{"id": 247958276, "updated": "2023-10-05 15:48:17.281", "metadata": {"title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization", "authors": "[{\"first\":\"Zihan\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Bingliang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.02246", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/0002FZW22", "doi": "10.48550/arxiv.2204.02246"}}, "content": {"source": {"pdf_hash": "89eed2e92270db2789cfb0cf00b387877809ad7a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.02246v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bba49736dede9e4dae2568548fc2a17b12b402d9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/89eed2e92270db2789cfb0cf00b387877809ad7a.txt", "contents": "\nPublished as a conference paper at ICLR 2022 CONTINUOUSLY DISCOVERING NOVEL STRATEGIES VIA REWARD-SWITCHING POLICY OPTIMIZATION\n\n\nZihan Zhou \nZ \nWei Fu \nIIIS\nTsinghua University\n3 Shanghai Qi Zhi Institute Z\n\nBingliang Zhang \nIIIS\nTsinghua University\n3 Shanghai Qi Zhi Institute Z\n\nYi Wu \n\nCS Department\nUniversity of Toronto\n\n\nPublished as a conference paper at ICLR 2022 CONTINUOUSLY DISCOVERING NOVEL STRATEGIES VIA REWARD-SWITCHING POLICY OPTIMIZATION\n\nWe present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufficiently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges. -rl: Efficient mixing of quality and diversity in reinforcement learning. arXiv preprint arXiv:\n\nINTRODUCTION\n\nThe foundation of deep learning successes is the use of stochastic gradient descent methods to obtain a local minimum for a highly non-convex learning objective. It has been a popular consensus with theoretical justifications that most local optima are very close to the global optimum (Ma, 2020). Consequently, algorithms for most classical deep learning applications only focus on the final performance of the learned local solution rather than which local minimum is discovered.\n\nHowever, this assumption can be problematic in reinforcement learning (RL), where different local optima in the policy space can correspond to substantially different strategies. Therefore, discovering a diverse set of policies can be critical for many RL applications, such as producing natural dialogues in chatbot (Li et al., 2016), improving the chance of finding a targeted molecule (Pereira et al., 2021), generating novel designs (Wang et al., 2019) or training a specialist robot for fast adaptation (Cully et al., 2015). Moreover, in the multi-agent setting, a collection of diverse local optima could further result in interesting emergent behaviors (Liu et al., 2019;Zheng et al., 2020;Baker et al., 2020) and discovery of multiple Nash equilibria (Tang et al., 2021), which further help build strong policies that can adapt to unseen participating agents in a zero-shot manner in competitive (Jaderberg et al., 2019;Vinyals et al., 2019) and cooperative games (Lupu et al., 2021).\n\nIn order to obtain diverse strategies in RL, most existing works train a large population of policies in parallel (Pugh et al., 2016;Cully et al., 2015;Parker-Holder et al., 2020b). These methods often adopt a soft learning objective by introducing additional diversity intrinsic rewards or auxiliary losses. However, when the underlying reward landscape in the RL problem is particularly non-uniform, policies obtained by population-based methods often lead to visually identical strategies (Omidshafiei et al., 2020;Tang et al., 2021). Therefore, population-based methods may require a substantially large population size in order to fully explore the policy space, which can be computationally infeasible. Moreover, the use of soft objective also results in non-trivial and subtle hyper-parameter tuning to balance diversity and the actual performance in the environment, which largely prevents these existing methods from discovering both diverse and high-quality policies in practice (Parker-Holder et al., 2020b;Lupu et al., 2021;Masood & Doshi-Velez, 2019). Another type of methods directly explores diverse strategies in the reward space by performing multi-objective optimization over human-designed behavior characterizations (Pugh et al., 2016;Cully et al., 2015) or random search over linear combinations of the predefined objectives (Tang et al., 2021;Zheng et al., 2020;Ma et al., 2020). Although these multi-objective methods are particularly successful, a set of well-defined and informative behavior objectives may not be accessible in most scenarios.\n\nWe propose a simple, generic and effective iterative learning algorithm, Reward-Switching Policy Optimization (RSPO), for continuously discovering novel strategies under a single reward function without the need of any environment-specific inductive bias. RSPO discovers novel strategies by solving a filtering-based objective, which restricts the RL policy to converge to a solution that is sufficiently different from a set of locally optimal reference policies. After a novel strategy is obtained, it becomes another reference policy for future RL optimization. Therefore, by repeatedly running RSPO, we can quickly derive diverse strategies in just a few iterations. In order to strictly enforce the novelty constraints in policy optimization, we adopt rejection sampling instead of optimizing a soft objective, which is adopted by many existing methods by converting the constraints as Lagrangian penalties or intrinsic rewards. Specifically, RSPO only optimizes extrinsic rewards over trajectories that have sufficiently low likelihood w.r.t. the reference policies. Meanwhile, to further utilize those rejected trajectories that are not distinct enough, RSPO ignores the environment rewards on these trajectories and only optimizes diversity rewards to promote effective exploration. Intuitively, this process adaptively switches the training objective between extrinsic rewards and diversity rewards w.r.t. the novelty of each sampled trajectory, so we call it the Reward Switching technique.\n\nWe empirically validate RSPO on a collection of highly multi-modal RL problems, ranging from particle-world multi-target navigation (Mordatch & Abbeel, 2018) and MuJoCo control (Todorov et al., 2012) in the single-agent domain, to the stag-hunt games (Tang et al., 2021) and StarCraftII challenges (Rashid et al., 2019) in the multi-agent domain. Experiments demonstrate that RSPO can reliably and efficiently discover surprisingly diverse strategies in all these challenging scenarios and substantially outperform existing baselines. The contributions can be summarized as follows:\n\n1. We propose a novel algorithm, Reward-Switching Policy Optimization, for continuously discovering diverse policies. The iterative learning scheme and reward-switching technique both significantly benefit the efficiency of discovering strategically different policies. 2. We propose to use cross-entropy-based diversity metric for policy optimization and two additional diversity-driven intrinsic rewards for promoting diversity-driven exploration. 3. Our algorithm is both general and effective across a variety of single-agent and multi-agent domains. Specifically, our algorithm is the first to learn the optimal policy in the staghunt games without any domain knowledge, and successfully discovers 6 visually distinct winning strategies via merely 6 iterations on a hard map in SMAC.\n\n\nRELATED WORK\n\nSearching for diverse solutions in a highly multi-modal optimization problem has a long history and various block-box methods have been proposed (Miller & Shaw, 1996;Deb & Saha, 2010;Kroese et al., 2006). In reinforcement learning, one of the most popular paradigms is population-based training with multi-objective optimization. Representative works include the family of qualitative diversity (QD) (Pugh et al., 2016) algorithms, such as MAP-Elites (Cully et al., 2015), which are based on genetic methods and assume a set of human-defined behavior characterizations, and policygradient methods (Ma et al., 2020;Tang et al., 2021), which typically assume a distribution of reward function is accessible. There are also some recent works that combine QD algorithms and policy gradient algorithms (Cideron et al., 2020;Nilsson & Cully, 2021). The DvD algorithm (Parker-Holder et al., 2020b) improves QD by optimizing population diversity (PD), a KL-divergence-based diversity metric, without the need of hand-designed behavior characterizations. Similarly, Lupu et al. (2021) proposes to maximize trajectory diversity, i.e., the approximated Jensen-Shannon divergence with action-discounting kernel, to train a diversified population.\n\nThere are also works aiming to learn policies iteratively. PSRO (Lanctot et al., 2017) focuses on learning Nash equilibrium strategies in zero-sum games by maintaining a strategy oracle and repeatedly adding best responses to it. Various improvements have been made upon PSRO by using different metrics to promote diverse oracle strategies (Liu et al., 2021;Nieves et al., 2021). Hong et al. (2018) utilizes the KL-divergence between the current policy and a past policy version as an exploration bonus, while we are maximizing the diversity w.r.t a fixed set of reference policies, which is more stable and will not incur a cyclic training process. Diversity-Inducing Policy Gradient (DIPG) (Masood & Doshi-Velez, 2019) utilizes maximum mean discrepancy (MMD) between policies as a soft learning objective to iteratively find novel policies. By contrast, our method utilizes a filtering-based objective via reward switching to strictly enforce all the diversity constraints. Sun et al. (2020) adopts a conceptually similar objective by early terminating episodes that do not incur sufficient novelty. However, Sun et al. (2020) does not leverage any exploration technique for those rejected samples and may easily suffer from low sample efficiency in challenging RL tasks we consider in this paper. There is another concurrent work with an orthogonal focus, which directly optimizes diversity with reward constraints (Zahavy et al., 2021). We remark that enforcing a reward constraint can be problematic in multi-agent scenarios where different Nash Equilibrium can have substantially different pay-offs. In addition, the ridge rider algorithm (Parker-Holder et al., 2020a) proposes to follow the eigenvectors of the Hessian matrix to discover diverse local optima with theoretical guarantees, but Hessian estimates can be extremely inaccurate in complex RL problems.\n\nAnother stream of work uses unsupervised RL to discover diverse skills without the use of environment rewards, such as DIYAN (Eysenbach et al., 2019) and DDLUS (Hartikainen et al., 2020). However, ignoring the reward signal can substantially limit the capability of discovering strategic behaviors. SMERL (Kumar et al., 2020) augments DIYAN with extrinsic rewards to induce diverse solutions for robust generalization. These methods primarily focus on learning low-level locomotion while we tackle a much harder problem of discovering strategically and visually different policies.\n\nFinally, our algorithm is also conceptually related to exploration methods (Zheng et al., 2018;Burda et al., 2019;Simmons-Edler et al., 2019), since it can even bypass inescapable local optima in challenging RL environments. Empirical comparisons can be found in Section 4. However, we emphasize that our paper tackles a much more challenging problem than standard RL exploration: we aim to discover as many distinct local optima as possible. That is, even if the global optimal solution is discovered, we still want to continuously seek for sufficiently distinct local-optimum strategies.\n\nWe remark that such an objective is particularly important for multi-agent games where finding all the Nash equilibria can be necessary for analyzing rational multi-agent behaviors (Tang et al., 2021).\n\n\nMETHOD\n\n\nPRELIMINARY\n\nWe consider environments that can be modeled as a Markov decision process (MDP) (Puterman, 1994) M = (S, A, R, P, \u03b3) where S and A are the state and action space respectively, R(s, a) is the reward function, P (s |s, a) is the transition dynamics and \u03b3 is the discount factor. We consider a stochastic policy \u03c0 \u03b8 paramterized by \u03b8. Reinforcement learning optimizes the policy w.r.t. the expected return J(\u03c0) = E \u03c4 \u223c\u03c0 [ t \u03b3 t r t ] over the sampled trajectories from \u03c0, where a trajectory \u03c4 denotes a sequence of state-action-reward triplets, i.e., \u03c4 = {(s t , a t , r t )}. Note that this formulation can be naturally applied to multi-agent scenarios with homogeneous agents with shared state and action space, where learning a shared policy for all the agents will be sufficient.\n\nRather than learning a single solution for J(\u03b8), we aim to discover a diverse set of M policies, i.e, {\u03c0 \u03b8 k |1 \u2264 k \u2264 M }, such that all of these polices are locally optimized under J(\u03b8) and mutually distinct w.r.t. some distance measure D(\u03c0 \u03b8 i , \u03c0 \u03b8 j ), i.e., max\n\u03b8 k J(\u03b8 k ) \u22001 \u2264 k \u2264 M, subject to D(\u03c0 \u03b8 i , \u03c0 \u03b8 j ) \u2265 \u03b4, \u22001 \u2264 i < j \u2264 M.(1)\nHere D(\u00b7, \u00b7) measures how different two policies are and \u03b4 is the novelty threshold. For conciseness, in the following content, we omit \u03b8 and use \u03c0 k to denote the policy with parameter \u03b8 k .\n\n\nITERATIVE CONSTRAINED POLICY OPTIMIZATION\n\nDirectly solving Eq. (1) suggests a population-based training paradigm, which requires a non-trivial optimization technique for the pairwise constraints and typically needs a large population size M . Herein, we adopt an iterative process to discover novel policies: in the k-th iteration, we optimize a single policy \u03c0 k with the constraint that \u03c0 k is sufficiently distinct from previously discovered policies \u03c0 1 , . . . , \u03c0 k\u22121 . Here, the term \"iteration\" is used to denote the process of learning a new policy. Formally, we solve the following iterative constrained optimization problem for iteration 1 \u2264 k \u2264 M : \u03b8 k = arg max \u03b8 J(\u03b8), subject to D(\u03c0 \u03b8 , \u03c0 j ) \u2265 \u03b4, \u22001 \u2264 j < k.\n\n(2) Eq. (2) reduces the population-based objective to a standard constrained optimization problem for a single policy, which is much easier to solve. Such an iterative procedure does not require a large population size M as is typically necessary in population-based methods. And, in practice, only a few iterations could result in a sufficiently diverse collection of policies. We remark that, in theory, directly solving the constraint problem in Eq. (2) may lead to a solution that is not a local optimum w.r.t. the unconstrained objective J(\u03b8). It is because a solution in Eq. (2) can be located on the boundary of the constraint space (i.e., D(\u03c0 \u03b8 , \u03c0 j ) = \u03b4), which is undesirable according to our original goal. However, this issue can be often alleviated by properly setting the novelty threshold \u03b4.\n\nThe natural choice for measuring the policy difference is KL divergence, as done in the trust-region constraint (Schulman et al., 2015;2017). However, in our setting where the difference between policies should be maximized, using KL as the diversity measure would inherently encourage learning a policy with small entropy, which is typically undesirable in RL problems (see App. F for a detailed derivation). Therefore, we adopt the accumulative cross-entropy as our diversity measure, i.e.,\nD(\u03c0 i , \u03c0 j ) :=H(\u03c0 i , \u03c0 j ) = E \u03c4 \u223c\u03c0i \u2212 t log \u03c0 j (a t | s t )(3)\n\nTRAJECTORY FILTERING FOR ENFORCING DIVERSITY CONSTRAINTS\n\nA popular approach to solve the constrained optimization problem in Eq.\n\n(2) is to use Lagrangian multipliers to convert the constraints to penalties in the learning objective. Formally, let \u03b2 1 , . . . , \u03b2 k\u22121 be a set of hyperparameters, the soft objective for Eq.\n\n(2) is defined by\nJ soft (\u03b8) := J(\u03c0 \u03b8 ) + k\u22121 j=1 \u03b2 j D(\u03c0 \u03b8 , \u03c0 j ).(4)\nSuch a soft objective substantially simplifies optimization and is widely adopted in RL applications. However, in our setting, since cross-entropy is a particularly dense function, including the diversity bonus as part of the objective may largely change the reward landscape of the original RL problem, which could make the final solution diverge from a locally optimal solution w.r.t J(\u03b8). Therefore, it is often necessary to anneal the Lagrangian multipliers \u03b2 j , which is particularly challenging in our setting with a large number of reference policies. Moreover, since D(\u03c0 \u03b8 , \u03c0 j ) is estimated over the trajectory samples, it introduces substantially high variance to the learning objective, which becomes even more severe as more policies are discovered.\n\nConsequently, we propose a Trajectory Filtering objective to alleviate the issues of the soft objective. Let's use NLL(\u03c4 ; \u03c0) to denote the negative log-likelihood of a trajectory \u03c4 w.r.t. a policy \u03c0, i.e., NLL(\u03c4 ; \u03c0) = \u2212 (st,at)\u223c\u03c4 log \u03c0(a t |s t ). We apply rejection sampling over the sampled trajectories \u03c4 \u223c \u03c0 \u03b8 such that we train on those trajectories satisfying all the constraints, i.e., NLL(\u03c4 ; \u03c0 j ) \u2265 \u03b4 for each reference policy \u03c0 j . Formally, for each sampled trajectory \u03c4 , we define a filtering function \u03c6(\u03c4 ), which indicates whether we want to reject the sample \u03c4 , and use I[\u00b7] to denote the indicator function, and then the trajectory filtering objective J filter (\u03b8) can be expressed as\nJ filter (\u03b8) = E \u03c4 \u223c\u03c0 \u03b8 \u03c6(\u03c4 ) t \u03b3 t r t , where \u03c6(\u03c4 ) := k\u22121 j=1 I[NLL(\u03c4 ; \u03c0 j ) \u2265 \u03b4].(5)\nWe call the objective in Eq. (5) a filtering objective. We show in App. G that solving Eq. (5) is equivalent to solving Eq. (2) with an even stronger diversity constraint. In addition, we also remark that trajectory filtering shares a conceptually similar motivation with the clipping term in Proximal Policy Optimization (Schulman et al., 2017).\n\n\nINTRINSIC REWARDS FOR DIVERSITY EXPLORATION\n\nThe main issue in Eq. (5) is that trajectory filtering may reject a significant number of trajectories, especially in the early stage of policy learning since the policy is typically initialized to the a random policy. Hence, it is often the case that most of the data in a batch are abandoned, which leads to a severe wasting of samples and may even break learning due to the lack of feasible trajectories.\n\nCan we make use of those rejected trajectories? We propose to additionally apply a novelty-driven objective on those rejected samples. Formally, we use \u03c6 j (\u03c4 ) to denote whether \u03c4 violates the constraint of \u03c0 j , i.e., \u03c6 j (\u03c4 ) = I[NLL(\u03c4, \u03c0 j ) \u2265 \u03b4]. Then we have the following switching objective:\nJ switch = E \u03c4 \u223c\u03c0 \u03b8 \uf8ee \uf8f0 \u03c6(\u03c4 ) t \u03b3 t r t + \u03bb j (1 \u2212 \u03c6 j (\u03c4 )) NLL(\u03c4, \u03c0 j ) \uf8f9 \uf8fb (6)\nThe above objective simultaneously maximizes the extrinsic return on accepted trajectories and the cross-entropy on rejected trajectories. It can be proved that solving Eq. (6) is also equivalent to solving Eq. (2) with a stronger diversity constraint (see App. G).\n\nFurthermore, Eq. (6) can be also interpreted as introducing additional cross-entropy intrinsic rewards on rejected trajectories (i.e., \u03c6 j (\u03c4 ) = 0). More specifically, given NLL(\u03c4 ; \u03c0) = \u2212 (st,at)\u2208\u03c4 log \u03c0(a t |s t ), an intrinsic reward r int (s t , a t ; \u03c0 j ) = \u2212 log \u03c0 j (a t | s t ) is applied to each state-action pair (s t , a t ) from every rejected trajectory \u03c4 . Conceptually, this suggests an even more general paradigm for diversity exploration: we can optimize extrinsic rewards on accepted trajectories while utilizing novelty-driven intrinsic rewards on rejected trajectories for more effective exploration, i.e., by encouraging the learning policy \u03c0 \u03b8 to be distinct from a reference policy \u03c0 j .\n\nHence, we propose two different types of intrinsic rewards to promote diversity exploration: one is likelihood-based, which directly follows Eq. (6) and focuses more on behavior novelty, and the other is reward-prediction-based, which focuses more on achieving novel states and reward signals.\n\nBehavior-driven exploration. The behavior-driven intrinsic reward r int B is defined by r int B (a, s; \u03c0 j ) = \u2212 log \u03c0 j (a | s).\n\n(7)\n\nr int B encourages the learning policy to output different actions from those reference policies and therefore to be more likely to be accepted. Note that r int B can be directly interpreted as the Lagrangian penalty utilized in the soft objective J soft (\u03b8).\n\nReward-driven exploration. A possible limitation of behavior-driven exploration is that it may overly focus on visually indistinguishable action changes rather than high-level strategies. Note that in RL problems with diverse reward signals, it is usually preferred to discover policies that can achieve different types of rewards (Simmons-Edler et al., 2020;Wang* et al., 2020). Inspired by the curiosity-driven exploration method (Pathak et al., 2017), we adopt a model-based approach for predicting novel reward signals. In particular, after obtaining each reference policy \u03c0 j , we learn a reward prediction function f (s, a; \u03c8 j ) trained by minimizing the expected MSE loss L(\u03c8 j ) = E \u03c4 \u223c\u03c0j ,t |f (s t , a t ; \u03c8 j ) \u2212 r t | 2 over the trajectories generated by \u03c0 j . The reward prediction function f (s, a; \u03c8 j ) is expected to predict the extrinsic environment reward more accurately on state-action pairs that are more frequently visited by \u03c0 j and less accurately on rarely visited pairs. To encourage policy exploration, we adopt the reward prediction error as our reward-driven intrinsic reward r int R (a, s; \u03c0 j ). Formally, given the transition triplet (s t , a t , r t ), r int R (a, s; \u03c0 j ) is defined by r int R (s t , a t ; \u03c0 j ) = |f (s t , a t ; \u03c8 j ) \u2212 r t | 2 .\n\nWe remark that reward-driven exploration can be also interpreted as approximately maximizing the f -divergence of joint state occupancy measure between policies (Liu et al., 2021). By combining these two intrinsic rewards together, we approximately maximize the divergence of both actions and states between policies to effectively promote diversity. By default, we use behavior-driven intrinsic reward for computational simplicity and optionally augment it with reward-driven intrinsic reward in more challenging scenarios (see examples in Section 4.2).\n\n\nREWARD-SWITCHING POLICY OPTIMIZATION\n\nWe define the RSPO function r RSPO t by\nr RSPO t = \u03c6(\u03c4 )r t + \u03bb j (1 \u2212 \u03c6 j (\u03c4 ))r int (a t , s t ; \u03c0 j ),(9)\nwhere \u03bb is a scaling hyper-parameter. Note that extrinsic rewards and intrinsic rewards are mutually exclusive, i.e., a trajectory \u03c4 may be either included in J filtering or be rejected to produce exploration bonuses. Conceptually, our method is adaptively \"switching\" between extrinsic and intrinsic rewards during policy gradients, which is so-called Reward-Switching Policy Optimization (RSPO). We also remark that the intrinsic reward will constantly push the learning policy towards the feasible policy space and the optimization objective will eventually converge to J(\u03b8) when no trajectory is rejected.\n\nIn addition to the aforementioned RSPO algorithm, we also introduce two implementation enhancements for better empirical performances, especially in some performance-sensitive scenarios.\n\nAutomatic threshold selection. We provide an empirical way of adjusting \u03b4. In some environments, \u03b4 is sensitive to each reference policy. Instead of tuning \u03b4 for each reference policy, we choose its corresponding threshold by \u03b4 j = \u03b1 \u00b7 D(\u03c0 rnd , \u03c0 j ), where \u03c0 rnd is a fully random policy and \u03b1 is a task-specific hyperparameter. We remark that \u03b1 is a constant parameter across training iterations and is much easier to choose than manually tuning \u03b4, which requires subtle variation throughout multiple training iterations. We use automatic threshold selection by default. Detailed values of \u03b1 and the methodology of tuning \u03b1 can be found in App. D.1 and App. B.3 respectively.\n\nSmoothed-switching for intrinsic rewards. Intrinsic rewards have multiple switching indicators, i.e., \u03c6 1 , . . . , \u03c6 k\u22121 . Moreover, for different trajectories, different subsets of indicators will be turned on and off, which may result in a varying scale of intrinsic rewards and hurt training stability. Therefore, in some constraint-sensitive cases, we propose a smoothed switching mechanism which could further improve practical performance. Specifically, we maintain a running average\u03c6 j over all the sampled trajectories for each indicator \u03c6 j (\u03c4 ), and use these smoothed indicators to compute intrinsic rewards defined in Eq. (9). Smoothed-switching empirically improves training stability when a large number of reference policies exist, such as in stag-hunt games (see Section 4.2).\n\n\nEXPERIMENTS\n\nTo illustrate that our method can be applied to general RL applications, we experiment on 4 domains that feature multi-modality of solutions, including a single-agent navigation problem in the particle-world ( In particle world and stag-hunt games, all the local optima can be precisely calculated, so we can quantitatively evaluate the effectiveness of different algorithms by measuring how many distinct strategy modes are discovered. In MuJoCo control and SMAC, we qualitatively demonstrate that our method can discover a large collection of visually distinguishable strategies. Notably, we primarily present results from purely RL-based methods which do not require prior knowledge over possible local optima for a fair comparison. We also remark that when a precise feature descriptor of local optima is feasible, it is also possible to apply evolutionary methods (Nilsson & Cully, 2021) to a subset of the scenarios we considered. For readers of further interest, a thorough study with discussions can be found in App. B.4.\n\nOur implementation is based on PPO (Schulman et al., 2017) on a desktop machine with one CPU and one NVIDIA RTX3090 GPU. All the algorithms are run for the same number of total environment steps and the same number of iterations (or population size). More details can be found in appendix. We consider a sparse-reward navigation scenario called 4-Goals (Fig. 1). The agent starts from the center and will receive a reward when reaching a landmark. We set up 3 difficulty levels. In the easy mode, the landmark locations are fixed. In the medium mode, the landmarks are randomly placed. In the hard mode, landmarks are not only placed randomly but also have different sizes and rewards. Specifically, the sizes and rewards of each landmark are 2\u00d7, 1\u00d7, 0.5\u00d7, 0.25\u00d7 and 1\u00d7, 1.1\u00d7, 1.2\u00d7, 1.3\u00d7 of the normal one respectively. We remark that in the hard mode, the landmark size decreases at an exponential rate while the reward gain is only marginal, making it exponentially harder to discover policies towards those smaller landmarks. We compare RSPO with several baselines, including PPO with restarts ( The number of distinct local optima discovered by different methods is presented in Fig. 2a. RSPO consistently discovers all the 4 modes within 4 iterations even without the use of any intrinsic rewards over rejected trajectories (i.e., r int t = 0). DIPG finds 4 strategies in 4 out of the 5 runs in the easy mode but performs no better than PG in the two harder modes. Fig. 2b shows the highest expected return achieved over the policy population in the hard mode. RSPO is the only algorithm that   We plot the heatmap of the two agents' meeting point to indicate the type of the found strategies.\n\n\nSINGLE-AGENT\nPARTICLE-WORLD ENVIRONMENT (a) Easy (b) Medium (c) Hard\nsuccessfully learns the optimal policy towards the smallest ball. We also report the performance of RSPO optimizing the soft objective in Eq. (4) with the default behavior-driven intrinsic reward r int B (no-switch), which was able to discover the policy towards the second largest landmark. Comparing no-switch with r int t = 0, we could conclude that the filtering-based objective can be critical for RSPO to discover sufficiently different modes. We also remark that the no-switch variant only differs from DIPG by the used diversity metric. This suggests that in environments with high state variance (e.g. landmarks with random sizes and positions), state-based diversity metric may be less effective. We further show the effectiveness of RSPO on two grid-world stag-hunt games developed in Tang et al. (2021), Monster-Hunt and Escalation, both of which have very distinct Nash Equilibria (NEs) for self-play RL methods to converge to. Moreover, the optimal NE with the highest rewards for both agents in these games are risky cooperation, i.e., a big penalty will be given to an agent if the other agent stops cooperation. This makes most self-play RL algorithms converge to the safe non-cooperative NE strategies with lower rewards. It has been shown that none of the state-of-the-art exploration methods can discover the global optimal solution without knowing the underlying reward structure (Tang et al., 2021). We remark that since there are enormous NEs in these environments as shown in Fig. 4 and 7b, population-based methods (PBT) require a significantly large population size for meaningful performances, which is computationally too expensive to run. Therefore, we do not include the results of PBT baselines. We also apply the smoothed-switching heuristic for RSPO in this domain. Environment details can be found in App. C.2.\n\n\n2-AGENT MARKOV STAG-HUNT GAMES\n\nThe Monster-Hunt game. The Monster-Hunt game (Fig. 3) contains a monster and two apples. When a single agent meets the monster, it gets a penalty of \u22122. When both agents meet the monster at the same time, they \"catch\" the monster and both get a bonus of 5. When a player meets an apple, it gets a bonus of 2. The optimal strategy, i.e., both agents move towards the monster, is a risky cooperative NE since an agent will receive a penalty if the other agent deceives. The non-cooperative NE for eating apples is a safe NE and easy to discover but has lower rewards.\n\nWe adopt both behavior-driven and reward-driven intrinsic rewards in RSPO to tackle Monster-Hunt. Fig. 4 illustrates all the discovered strategies by RSPO over 20 iterations, which covers a wide range of human-interpretable strategies, including the non-cooperative apple-eating strategy as well as the   optimal strategy, where both two agents stay together and chase the monster actively. By visualizing the heatmap of where both agents meet, we observe a surprisingly diverse sub-optimal cooperation NEs, where both agents move to a corner or an edge simultaneously, keep staying there, and wait for the monster coming. We remark that due to the existence of such a great number of passive waiting strategies, which all have similar accumulative environment rewards and states, it becomes critical to include the reward-driven intrinsic reward to quickly bypass them and discover the optimal solution.\n\nWe perform ablation studies on RSPO by turning off reward switching (No Switch) or intrinsic reward (No r int ) or only using the behavior-driven intrinsic reward (r int B only), and evaluate the performances of many baseline methods, including vanilla PG with restarts (PG), DIPG, RND, a popular multiagent exploration method MAVEN (Mahajan et al., 2019) and reward-randomized policy gradient (RPG) (Tang et al., 2021). We summarize the categories of discovered strategies by all these baselines in Table 1. Apple denotes the non-cooperative apple-eating NE; Chase denotes the optimal NE where both agents actively chase the monster; Corner and Edge denote the sub-optimal cooperative NE where both agents passively wait for the monster at a corner or an edge respectively. Regarding the baselines, PG, DIPG, and RND never discover any strategy beyond the non-cooperative Apple NE. For RPG, even using the domain knowledge to change the reward structure of the game, it never discovers the Edge NE. Regarding the RSPO variants, both reward switching and intrinsic rewards are necessary. Fig. 5 shows that when the intrinsic reward is turned off, the proportion of accepted trajectories per batch stays low throughout training. This implies that the learning policy failed to escape the infeasible subspace. Besides, as shown in Table 1, using behavior-driven exploration alone fails to discover the optimal NE, which suggests the necessity of reward-driven exploration to maximize the divergence of both states and actions in problems with massive equivalent local optima.\n\nThe Escalation game. Escalation (Fig. 6) requires the two players to interact with a static light. When both players step on the light simultaneously, they both receive a bonus of 1. Then the light moves to a random adjacent grid. The game continues only if both players choose to follow the light. If only one player steps on the light, it receives a penalty of \u22120.9L, where L is the number of previous cooperation steps. For each integer L, there is a corresponding NE where both players follow the light for L steps then simultaneously stop cooperation. We run RSPO with both diversity-driven intrinsic rewards and compare it with PG, DIPG, RND and RPG. Except for RPG, none of the baseline methods discover any cooperative NEs while RSPO directly learns the optimal cooperative NE (i.e., always cooperate) in the second iteration as shown in Fig. 7a. We also measure the total number of discovered NEs by different methods over 10 iterations in Fig. 7b. Due to the existence of many spiky local optima, the smoothed-switching technique can be crucial here to stabilize the  training process. We remark that even without the smoothed-switching technique, RSPO achieves comparable performance with RPG -note that RPG requires a known reward function while RSPO does not assume any environment-specific domain knowledge.\n\n\nCONTINUOUS CONTROL IN MuJoCo\n\nWe  Table 2, where RSPO achieves comparable performance in Hopper and Humanoid and substantially outperforms all the baselines in Half-Cheetah and Walker2d.\n\nWe remark that even with the same intrinsic reward, population-based training (PBT-CE) cannot discover sufficiently novel policies compared with iterative learning (RSPO). In Humanoid, SMERL achieves substantially lower return than other baseline methods and we don't report the population diversity score (more details can be found in App. B.6). We also visualize some interesting emergent behaviors RSPO discovered for Half-Cheetah and Hopper in App. B.1, where different strategy modes discovered by RSPO are visually distinguishable while baselines methods often converge to very similar behaviors despite of the non-zero diversity score.\n\n\nSTACRAFT MULTI-AGENT CHALLENGE\n\nWe further apply RSPO to the StarCraftII Multi-Agent Challenge (SMAC) (Rashid et al., 2019), which is substantially more difficult due to partial observability, long horizon, and complex state/action space. We conduct experiments on 2 maps, an easy map 2m_vs_1z and a hard map 2c_vs_64zg, both of which have heterogeneous unit types leading to a multi-modal solution space. Baseline methods include PG, DIPG, PBT-CE and trajectory diversity (TrajDiv) (Lupu et al., 2021). SMERL algorithm and DvD algorithm are not included because they were originally designed for continuous control domain and not suitable for SMAC (see App. B.6 and App. B.2.2). Instead, we include the TrajDiv algorithm (Lupu et al., 2021) as an additional baseline, which was designed for cooperative multi-agent games. We compare the number of visually distinct policies by training a population of 4 (or for 4 iterations), as shown in Table 3. While PBT-based algorithms tend to discover policies with slight distinctions, RSPO can effectively discover different winning strategies demonstrating intelligent behaviors in just a few iterations consistently across repetitions. We remark that there may not exist an appropriate quantitative diversity metric for such a sophisticated MARL game in the existing literature (see App. B.2.2). Visualizations and discussions can be found in App. B.1.\n\n\nCONCLUSION\n\nWe propose Reward-Switching Policy Optimization (RSPO), a simple, generic, and effective iterative learning algorithm that can continuously discover novel strategies. RSPO tackles a noveltyconstrained optimization problem via adaptive switching between extrinsic and intrinsic rewards used for policy learning. Empirically, RSPO can successfully tackle a wide range of challenging RL domains under both single-agent and multi-agent settings. We leave further theoretical justifications and sample efficiency improvements as future work.  \n\n\nB.1.2 SMAC\n\nWe present screenshots of emergent strategies in the SMAC environment in Fig. 14 and Fig. 15 for 2c_vs_64zg and 2m_vs_1z respectively. We expect different emergent strategies to be both visually distinguishable and human-interpretable. The strategies induced by baseline methods and RSPO are summarized in Table 4 and Table 5.\n\nOn the hard map 2c_vs_64zg, agents need to control the 2 colossi to fight against 64 zergs. The colossi have a wider attack range and can step over the cliff. Fig. 14a shows an aggressive strategy where the two colossi keep staying together on the left side of the cliff to fire at all the coming enemies. Fig. 14b shows a particularly intelligent strategy where the colossi make use of the terrain to play hit-and-run. When the game starts, the colossi stand on the cliff to snipe distant enemies to make them keep wandering around under the cliff. Hence, as the game proceeds, we can observe that the enemies are clearly partitioned while the colossi always maintain a smart fire position on the cliff. Fig. 14c shows a mirror strategy similar to Fig. 14a to aggressively clean up incoming enemies from the right side. Fig. 14d shows a conservative strategy that the colossi stand still in the corner to keep minimal contact with the enemies, and thus minimize damages received from enemies. Fig. 14e shows another smart strategy: one colossi blocks all incoming enemies on the mountain pass as a fire attractor, while the other one hides behind the fire attractor and snipes enemies distantly. We can see from the last two frames that the distant sniper does not lose any health points in late stages. In Fig. 14f, one colossi (#1) actively take advantage of the terrain to walk along the cliff, such that enemies on the plateau must run around to attack it. In the mean time, the colossi helps its teammate by sniping distant enemies. Finally, they separately clean up all the remaining enemies for the win.\n\nOn the easy map 2m_vs_1z, agents need to control 2 marines to defeat a Zealot. The marines can shoot the Zealot in a distant position but the Zealot can only perform close-range attacks. In Fig. 15a, the marines swing horizontally to keep an appropriate fire distance from the Zealot. In Fig. 15b, the marines perform a parallel hit-and-run from top to bottom as the Zealot approaches. In Fig. 15c, the right-side marine stands still, and the left-side marine swings vertically to distract the Zealot. In Fig. 15d, the two marines alternatively perform hit-and-run from bottom to top to distract the Zealot.  The final performance of MuJoCo environments is presented in Table 6. As mentioned in Appendix C.3, in our implementation we fix the episode length to 512 so that the diverse intrinsic rewards can be easily computed, which may harm sample efficiency and the evaluation score. Moreover, we use a hidden size of 64 which is usually smaller than previous works (which is 256 in the SAC paper (Haarnoja et al., 2018)). Hence, these results may not be directly compared with other numbers in the existing literature. However, the policy in Iter #1 is obtained by vanilla PPO and can therefore be used to assess the relative performances of other runs. Note that even if the presented scores are not the state-of-the-art, visualization results show that our algorithm successfully learns the basic locomotion and diverse gaits in these environments. The results indeed demonstrate diverse local optima that are properly discovered by RSPO, including many interesting emergent behaviors  \n\n\nB.2.2 SMAC\n\nFinal evaluation winning rate of SMAC is presented in Table 7. The final performance of RSPO on the easy map 2m_vs_1z matches the state-of-the-art (Yu et al., 2021). The median evaluation winning rate and standard deviation on the hard map 2c_vs_64zg is 98.4%(6.4%), which is slightly lower than the state-of-the-art 100%(0). We note that the policies discovered by our algorithm are both diverse and high-quality winning strategies (local optima) showing intelligent emergent behaviors.\n\nWe note that population diversity, which we use for quantitative evaluation for MuJoCo, may not be an appropriate metric for such a sophisticated MARL game with a large state space and a long horizon. Diversity via Determinant or Population Diversity (Parker-Holder et al., 2020b) is originally designed for the continuous control domain. It mainly focuses on the continuous control domain and directly adopts the action as action embedding, while it remains unclear how to embed a discrete action space. For SMAC, we adopt the logarithm probability of categorical distribution as the action embedding and evaluate RSPO and selected baselines using Population Diversity on the hard map 2c_vs_64zg. We further train DvD with a population size of 4 as an additional baseline. The results are shown in Table 8. The population diversity scores of baseline methods and RSPO both reach the maximum value of 1.000. However, if we visualize all the learned policies, actually many policies induced by PBT-CE or DvD cannot be visually distinguished by humans. Moreover, policies induced by PG are visually identical but still achieve a population diversity score of 0.981. This indicates that high population diversity scores might not necessarily imply a diverse strategy pool in complex environments like SMAC. We hypothesize that this is due to complex game dynamics in SMAC. For example, a unit performing micro-strategies of attack-then-move and move-then-attack are visually the same for humans but will have very different action probabilities in each timestep. Such subtle changes in policy outputs can significantly increase the population diversity scores. Since high scores may not directly reflect more diverse policies, it may not be reasonable to explicitly optimize population diversity as an auxiliary loss in SMAC. Therefore, we omit the results of DvD in SMAC in the main body of our paper.   Table 9: Population Diversity scores of the first 2 policies with different hyperparameters in Humanoid.\n\nWe have scaled the denominator of the RBF kernel in the Population Diversity matrix by a factor 10, such that the difference can be demonstrated more clearly.\n\nTo the best of our knowledge, a commonly accepted policy diversity metric for complex MARL games remains an open question in the existing literature. In our practice, rendering and visualizing the evaluation trajectories remains the best approach to distinguish different learned strategies. We emphasize that qualitatively, we are so far the first paper that ever reports such a visually diverse collection of winning strategies on a hard map in SMAC. Please check our website for policy visualizations (see Appendix A).\n\n\nB.3 SENSITIVITY ANALYSIS\n\nWe have performed a sensitivity analysis over \u03b1, \u03bb int B and \u03bb int R since they are the critical to the performance RSPO. The default values used in our experiments can be found in Table 12.\n\n\u03b1 is the most important hyperparameter in RSPO because it determines what trajectories in a batch to be accepted. We focus on the data efficiency, i.e., the proportion of accepted trajectories in a batch. In the sensitivity analysis, we run the second iteration of RSPO in Humanoid with \u03b1 = 0.5, 1.0, 1.5, 2 respectively and compute the population diversity score of the 2 resulting policies. The result is shown in Fig. 12 and the left part of Table 9. The result accords with our heuristic to adjust \u03b1: with a small \u03b1 (\u03b1 = 0.5), RSPO may accept all the trajectories at the beginning and lead to a similar policy after convergence, which is no better than the PG baseline; with a large \u03b1 (\u03b1 = 1.5 and \u03b1 = 2), RSPO may reject too many trajectories at the early stage of training and spend quite a lot of time on exploration, which sacrifices training time for the gain in the diversity score. In practice, we suggest starting with \u03b1 = 1 and adjusting it such that the acceptance rate can drop at the start of training and then quickly and smoothly converge to 1, as shown in Fig. 12 (\u03b1 = 1) and Fig. 5. \u03b1 should be decreased if too much data is rejected at the beginning of training and increased if data efficiency always stays high in the early stage of training.\n\n\u03bb int B and \u03bb int R determines the scale of intrinsic rewards. In our sensitivity analysis, \u03bb int B is analyzed in the Humanoid environment and \u03bb int R is analyzed on the 2m_vs_1z map in SMAC. Similarly, we run the second iteration of RSPO with \u03bb int B = 0.5, 1, 5, 10 in Humanoid and with \u03bb int R = 0, 0.05, 0.2 on 2m_vs_1z. The results are shown in Fig. 13 and the right part of Table 9. With value and advantage normalization in PPO, the scale of intrinsic rewards may not significantly affect performance. Specifically, the diversity scores in Humanoid do not vary too much, and the induced policies on 2m_vs_1z are all visually distinct from the reference one. However, if the scale is much larger than extrinsic rewards, it may cause learning instability, as shown in Fig. 13. On the opposite side, if the intrinsic rewards are turned off, RSPO may slow down convergence (Fig. 13), fail to discover non-trivial local optima due to lack of exploration (Table. 1) or get stuck during exploration due to low data efficiency (Fig. 5). We suggest starting with \u03bb int B = 1 and \u03bb int R = 0, and adjusting them such that the intrinsic rewards lead to a fast and smooth convergence.\n\n\nB.4 ADDITIONAL STUDY WITH AN EVOLUTIONARY METHOD\n\nNote that the main paper focuses on the discussion of RL-based solutions which require minimal domain knowledge of the solution structure. Evolutionary methods, as another popular line of research, have also shown promising results in a variety of domains and are also able to discover interesting diverse behaviors (Cully et al., 2015;Hong et al., 2018). However, evolutionary methods typically assume an effective human-designed set of characteristic features for effective learning.\n\nHere, for complete empirical study, we also conduct additional experiments w.r.t. a very recent evolutionary-based algorithm, PGA-MAP-Elites (Nilsson & Cully, 2021) which integrates MAP-Elites (Cully et al., 2015) into a policy gradient algorithm TD3 (Fujimoto et al., 2018). The PGA-MAP-Elites algorithm requires a human-defined behavioral descriptor (BD) to map a neural policy into a low-dimensional (discretized) space for behavior clustering. We run PGA-MAP-Elites on the 4-goals and the Escalation environment, where the behavioral descriptors (BDs) can be precisely defined to the best of our efforts. We remark that for the remaining cases, including the Monster-Hunt environment, MuJoCo control domain, and SMAC scenarios, behaviors of interests always involve strong temporal characteristics, which makes the design of a good BD particularly non-trivial and remain a challenging open question for the existing literature.\n\nIn particular, we define a 4-dimensional descriptor for the 4-Goals environment, i.e., a one-hot vector indicating the ID of the nearest landmark. For the Escalation environment, we use a 1-dimensional descriptor for the Escalation environment, which is a 0-1-normalized value of the cooperation steps within the episode. We set the number of behavior cells (niches) equal to the iteration number in RSPO, specifically 7 for 4-Goals and 10 for Escalation. Table 10, where PGA-MAP-Elites performs much worse on the 4-Goals scenario while outperforms RSPO on the Escalation environment due to the informative BD. Based on the results, we would like to discuss some characteristics of evolutionary algorithms and RSPO below:\n\n\nResults are shown in\n\n1. We empirically observe that many policies produced by PGA-MAP-Elites are immediately archived without becoming converged, particularly in 4-Goals (Hard) and Escalation. When measuring population diversity, these unconverged policies would contribute a lot even though many of them may have unsatisfying behaviors/returns. By contrast, the objective of RSPO aims to find diverse local optima. This also suggests a further research direction to bridge such a convergence-diversity gap.\n\n2. The quality of BD can strongly influence the performance of evolutionary methods. Note that the BD in Escalation provides a particularly clear signal on whether a policy reaches a local optimum or not (i.e., each BD niche precisely corresponds to a policy mode) while RSPO directly works on the deceptive extrinsic reward structure without knowing the structure of NEs. This suggests the importance of BD design, which, however, remains an open challenge in general.\n\n3. An improper BD may lead to a largely constrained behavior space. The success of PGA-MAP-Elites largely depends on the fact that the BD is known to be able to effectively cover all the local optima of interest. However, for complex environments like SMAC, we do not even know in advance what kind of behaviors will emerge after training. Therefore, an open-ended BD would be desired, which becomes an even more challenging problem -note that there even has not been any effective diversity measurement on SMAC yet. Therefore, a purely RL-based solution would be preferred.\n\n\n4.\n\nWithout an informative BD, evolutionary methods typically require a large population size introduced, which can cause practical issues. For example, maintaining a large unstructured archive can be computationally expensive. It can be also challenging to visually evaluate learned behaviors given a large population.\n\nTo sum up, when an effective and informative BD is available, evolutionary methods can be a strong candidate to consider, although it may not fit every scenario of interest, while RSPO can be a generally applicable solution with minimal domain knowledge. It could be also beneficial to investigate how to incorporate informative domain prior into RSPO framework, which we leave as our future work.\n\n\nB.5 THE POINT-V0 ENVIRONMENT\n\nParker-Holder et al. (2020b) develops a continuous control environment which requires the agent to bypass a wall to reach the goal. A big penalty will be given to the agent if it directly runs towards the goal and hits the wall. This environment indeed has a local optimum which can be overcome by diversity-driven exploration. While the authors of the DvD paper argued that ES and NSR will get stuck in the wall, the naive PPO algorithm can directly learns to bypass the wall and escape the local optimum. Hence in our experiment section, we consider much more challenging environments where a much larger number of local optima exist, such as stag-hunt games and SMAC.\n\n\nB.6 SMERL\n\nIn SMERL (Kumar et al., 2020), if the agent can achieve sufficiently high return in the trajectory, the trajectory will be augmented with intrinsic rewards of DIAYN (Eysenbach et al., 2019) for skill differentiation and policy diversification. In Humanoid, it may be challenging for SMERL to achieve such a high return, which turns off the intrinsic reward for promoting diversity. Hence, we do not report the population diversity score in the main body of our paper.\n\nIn stag-hunt games, SMERL keeps producing low returns. Note that since the SMERL algorithm only starts promoting diversity after sufficiently high reward is achieved, all the produced policies by SMERL are visually identical non-cooperative strategies. What's more, DIAYN (Eysenbach et al., 2019), which has the same intrinsic reward as SMERL, has been evaluated in Tang et al. (2021) and proved to perform worse than RPG (Tang et al., 2021), while the performance of RPG is further surpassed by RSPO. Hence, we omit the results of SMERL in the stag-hunt games.\n\nWe also evaluate SMERL in SMAC with a latent dimension of 5, which only induces 1 strategy on each map across all possible latent variables. We hypothesize the reason is that in such a complex MARL environment with a large state space and long horizon, the skill latent variable may be particularly challenging to be determined from game states. Moreover, the latent dimension is usually dominated by the state dimension, which makes latent variables less effective. \n\n\nC.2 GRIDWORLD STAG-HUNT GAMES\n\nBoth Monster-Hunt and Escalation are based on a 5 \u00d7 5 grid. In both games, the action space contains 5 discrete actions, including moving up, moving down, moving up, moving down and stay. The policy output is a softmax distribution over these actions. Each movement action moves the agent to one neighboring grid while the stay action makes the agent stay still. Each episode lasts for 50 timesteps. We used vector representations with continuous values for observations, i.e., two real numbers to represent the 2-dimensional positions for each entity. The detailed state representations are presented in the following sub-sections respectively. (Note: the dimension of each feature is shown in the parentheses)\n\n\nC.2.1 MONSTER-HUNT\n\nMonster-Hunt is based on a 5 \u00d7 5 grid. There are three types of entities: 2 agents, 1 monster and 2 apples. All of the entities spawn uniformly randomly on the grid without overlapping. At each step, both agents can choose to move up, down, left, right or stay; The monster always moves for one step towards its closest agent (ties break randomly). If one agent ends up in the same grid with the monster, it receives a penalty of -2; If both agents end up in the same grid with the monster, they both receive of bonus of 5; If any agent ends up in the same grid with an apple, it receives a bonus of 1 (ties break randomly if both agents enter the apple grid at the same time). Whenever any agent ends up in the same grid with a non-agent entity, the entity respawns randomly on the grid. The observation vector of an agent consists of the absolute position of itself and the relative positions of the other entities. Specifically, total dimension is 10, consisting of self absolute position (2), other agent's relative position (2), monster's relative position (2), apple 1's relative position (2), apple 2's relative position (2).\n\n\nC.2.2 ESCALATION\n\nSame as Monster-Hunt, Escalation is also based on a 5 \u00d7 5 grid. There are two types of entities: 2 agents and 1 light. All of the entities spawn uniformly randomly on the grid without overlapping. When the two agents and the light ends up in the same grid for the first time, they enter cooperation stage and the cooperation length L is set to 0. At each step of cooperation stage, they first both receive a bonus of 1, then L increases by 1, the light entity moves to a random adjacent grid and the agents move again. The cooperation stage continues only if both agents move the the same grid as the light. If only one agent moves to the light, it receives a penalty of \u22120.9L. The game ends if the cooperation stage ends. The observation vector of an agent consists of the current cooperation length L, the absolute position of itself and the relative positions of the other entities. Specifically, total dimension is 7, consisting of current cooperation length (1), self absolute position (2), other agent's relative position (2), light's relative position (2).\n\n\nC.3 MUJOCO\n\nWe use the MuJoCo environments from Gym (version 0.17.3) with a moving forward reward. The only modification is that we set the episode length to be 512 so that the diverse intrinsic rewards can be easily computed.  \n\n\nD IMPLEMENTATION DETAILS\n\nFor all experiments, baselines are re-initialized and re-run multiple times with the same number of iterations as RSPO except for population-based methods. Population-based baselines are run for a single trial with a population of policies trained in parallel. We also make sure the population size is the same as the iteration number of RSPO. All the baselines are built upon PPO with the same hyperparameters and run for the same number of total environment steps.\n\n\nD.1 RSPO\n\nWe use PPO with separate policy and value networks as the algorithm backbone of RSPO in all experiments. Both policy and value networks are multi-layer perceptrons (MLP) with a single hidden layer. Specifically for SMAC, we add an additional GRU (Chung et al., 2014) layer with the same hidden size and adopt the paradigm of centralized-training-with-decentralized-execution (CTDE), under which agents share a policy conditioned on local observations and a value function conditioned on global states. The PPO hyperparameters we use for each experiment is shown in Table 11. Even though the trajectory filtering technique induces an unbiased policy gradient estimator, it introduces unnegligible variance in the early stage of training. Therefore, we typically use a large batch size for PPO training to alleviate the defect of trajectory filtering. What's more, by using a large batch size, the advantage normalization and value normalization tricks could empirically improve learning stability. In the late stage of training, more trajectories are accepted and learning smoothly converges to a locally optimal solution as shown in Fig .5. We also note that the hidden size is 64 across the 4 domains, which may hurt final performance especially in the MuJoCo environments.\n\nThere are three additional hyperparameters for RSPO: the automatic threshold coefficient \u03b1 mentioned in Section 3.5, weight of the behavior-driven intrinsic reward \u03bb int B and weight of the reward-driven intrinsic reward \u03bb int R . These hyperparameters are shown in Table 12.\n\nSmoothed-switching We mention the smoothed-switching technique in Section 3.5 and remark that it helps to stabilize training when there are a large number of reference policies. Therefore, we apply smoothed-switching in experiments with 10+ iterations, specifically in Fig. 4 and Fig. 7b. We use the average of switching indicators in a batch as the smoothed indicator. All baseline methods are integrated with PPO and utilize the same hyperparameters as presented in Table 11. For population-based methods, all workers have independent data collection processes and buffers. RSPO and all baseline methods except for PG are initialized with the same weights. If not specially mentioned, the algorithm-specific hyperparameters are inherited from other papers which proposed the algorithm or utilized the algorithm as a baseline.\n\nPG Policy gradient with restarts (PG) utilize different random seeds for different policies. Samples are collected and consumed independently during training.\n\nPBT-CE Population-based training with our cross entropy objective (PBT-CE) utilizes the intrinsic reward in Eq. 3 as a soft objective without trajectory filtering and reward switching. By comparing PBT-CE with RSPO, we can validate the significance of the iterative learning and reward-switching scheme. We performed a grid search over 1e0, 5e-1, 1e-2, 5e-2, 5e-3, 2e-3, and 1e-3 on the Lagrange multiplier in each domain, and only 1e-3 converges.\n\nDIPG DIPG (Masood & Doshi-Velez, 2019) utilizes the Maximum Mean Discrepancy (MMD) of the state distribution between the learning policy and reference policies as a diversity driven objective. By contrast, our algorithm mainly focuses on the divergence of action distribution and optionally augment with approximate divergence of state distribution. We performed a grid search over 2e0, 1e0, 5e-1, 1e-1, and 1e-2 on the coefficient of MMD loss in each domain. The MMD loss coefficient is fixed to 0.1 in SMAC and 1.0 in other environments. We refer to the public implementation 2 .\n\n\nMAVEN\n\nWe use the open-source implementation 3 .\n\n\nPGA-MAP-Elites\n\nWe use the open-source implementation 4 .\n\nDvD Diversity via Determinant (DvD) (Parker-Holder et al., 2020b) algorithm aims to maximize the determinant of the RBF kernel matrix of action embedding. The original paper mainly focuses on the continuous control domain and directly adopts the action as action embedding, while it remains unclear how to embed a discrete action space. Regarding implementation, we refer to the public codebase 5 . We also utilize the Bayesian Bandits module to automatically adjust the coefficient of the auxiliary loss from {0, 0.5}, same as the original paper.\n\nSMERL In SMERL (Kumar et al., 2020), if the agent can achieve sufficiently high return in the trajectory, the trajectory will be augmented with intrinsic rewards of DIAYN (Eysenbach et al., 2019) for skill differentiation. SMERL is mainly designed for tackling perturbation in the continuous control domain. We use the score obtained in the first iteration of RSPO as the expert return for SMERL. We train SMERL with a latent dimension of 5 with intrinsic reward coefficient \u03b1 = 10.0 and trajectory threshold raio = 0.1, same as the original paper. For a fair comparison, we train SMERL for latent-dim\u00d7 more environment frames.\n\nTrajDiv Trajectory Diversity (TrajDiv) (Lupu et al., 2021) is designed for tackling zero-shot coordination problem in cooperative multi-agent games, specifically Hanabi (Bard et al., 2020). Trajdiv utilizes the approximate Jensen-Shannon divergence with action discounting kernel as an auxiliary loss in population-based training. We performed a grid search over 1e-2, 1e-1, 1e0 on the coefficient on the TrajDiv loss and over 1e-1, 5e-1, 9e-1 on the action-kernel discounting factor. We set the coefficient of trajectory diversity loss to be 0.1 and action discount factor to be 0.9. \n\n\nRidge Rider\nDiv DvD (\u03a0) := det(K SE (\u03c6(\u00b5 i ), \u03c6(\u00b5 j )) M i,j=1 )(10)\nwhere \u03c6 is a Behavior Embedding of \u03c0 defined as a concatenation of the actions taken on N randomly sampled states and K : where l is a length scale.\nR d \u00d7 R d \u2192 R is\nIn Parker-Holder et al. (2020b), only deterministic policies were considered. In our work, we design an modified version of PD that suits a set of stochastic policies \u03a0 = {\u03c0 1 , . . . , \u03c0 M }, defined as follows:\nDiv(\u03a0) := det(K JSD (\u03c0 i , \u03c0 j ) M i,j=1 )(11)\nwhere K JSD (x, y) := exp \u2212 JSD(x y) 2p 2 and JSD is the Jensen-Shannon divergence. We provide the following theorem that connects our definition of PD with the original PD in continuous action space.\n\nTheorem E.1. Let \u03a0 = {\u00b5 1 , . . . , \u00b5 M } be a set of deterministic policies in continuous action space. Let\u03a0 = {\u03c0 1 , . . . , \u03c0 M } be the randomized policies of \u03a0 defined as \u03c0 i (a|s) \u223c N (\u00b5 i (s), \u03c3 2 ) where \u03c3 > 0 is an arbitrary deviation. With a suitable choice of p, Div(\u03a0) = lim N \u2192\u221e Div DvD (\u03a0) if the states in \u03c6 is sampled with the randomized policies.\n\nProof. We first observe that for any i, j \u2208 {1, . . . , M },\nlim N \u2192\u221e \u03c6(\u00b5 i ) \u2212 \u03c6(\u00b5 j ) 2 N = lim N \u2192\u221e 1 N N k=1 \u00b5 i (s k ) \u2212 \u00b5 j (s k ) 2 =E s\u223c\u00b5i\u222a\u00b5j \u03c0 i (s) \u2212 \u03c0 j (s) 2(12)\nIn the case of stochastic policies, we have D KL (\u03c0 i \u03c0 j ) =E s,a\u223c\u03c0i log \u03c0 i (a|s) \u03c0 j (a|s)\n=E s,a\u223c\u03c0i \u2212 1 2\u03c3 2 a \u2212 \u00b5 i (s) 2 \u2212 a \u2212 \u00b5 j (s) 2 (Since \u03c0 i (a|s) \u223c N (\u00b5(s), \u03c3 2 )) = 1 2\u03c3 2 E s\u223c\u03c0i ( \u00b5 i (s) \u2212 \u00b5 j (s) 2 + \u03c3 2 ) \u2212 \u03c3 2 (Since a \u223c \u03c0 i (s)) = 1 2\u03c3 2 E s\u223c\u03c0i \u00b5 i (s) \u2212 \u00b5 j (s) 2(13)\nThen,\nJSD(\u03c0 i \u03c0 j ) = 1 2 D KL (\u03c0 i , \u03c0 j ) + 1 2 D KL (\u03c0 j , \u03c0 i ) = 1 4\u03c3 2 E s\u223c\u03c0i \u00b5 i (s) \u2212 \u00b5 j (s) 2 + 1 4\u03c3 2 E s\u223c\u03c0j \u00b5 j (s) \u2212 \u00b5 i (s) 2 = 1 2\u03c3 2 E s\u223c\u03c0i\u222a\u03c0j \u00b5 i (s) \u2212 \u00b5 j (s) 2(14)\nTherefore, if we choose p = l \u03c3 ,\nDiv(\u03a0) = det(K JSD (\u03c0 i , \u03c0 j ) M i,j=1 ) = det exp \u2212 JSD(\u03c0 i \u03c0 j ) 2p 2 M i,j=1 = det exp \u2212 \u03c3 2 JSD(\u03c0 i \u03c0 j ) 2l 2 M i,j=1 = det exp lim N \u2192\u221e \u2212 \u03c6(\u00b5 i ) \u2212 \u03c6(\u00b5 j ) 2 2l 2 N M i,j=1 = lim N \u2192\u221e det exp \u2212 \u03c6(\u00b5 i ) \u2212 \u03c6(\u00b5 j ) 2 2l 2 N M i,j=1 = lim N \u2192\u221e Div DvD (\u03a0)(15)\n\nF MOTIVATION OF USING CROSS-ENTROPY DIVERSITY MEASURE\n\nMaximizing KL-divergence between policies could be problematic in RL problems. The accumulative KL-divergence of a trajectory is defined as\nD KL (\u03c0 i , \u03c0 j ) = E \u03c4 \u223c\u03c0i t log \u03c0 i (a t |s t ) \u03c0 j (a t |s t ) = E \u03c4 \u223c\u03c0i \u2212 t log \u03c0 j (a t |s t ) cross entropy \u2212 E \u03c4 \u223c\u03c0i \u2212 t log \u03c0 i (a t |s t ) entropy of \u03c0i .\nThe above derivation suggests that optimizing KL-divergence would inherently encourages learning a policy with small entropy. This is usually undesirable. Therefore, we choose to use cross-entropy instead of KL-divergence.\n\n\nG DERIVATION OF TRAJECTORY FILTERING AND REWARD-SWITCHING\n\nIn this section, we want to validate our claim that solving the trajectory filtering objective in Eq. (5) and the reward switching objective in Eq. (6) is equivalent to solving Eq. (2) with an even stronger diversity measure D filter (\u03c0 i , \u03c0 j ) defined by\nD filter (\u03c0 i , \u03c0 j ) = inf \u03c4 \u223c\u03c0i \u2212 t log \u03c0 j (a t |s t ) .\nNote that D filter (\u03c0 i , \u03c0 j ) measures lowest possible trajectory-wise negative likelihood rather than the average value (i.e., cross-entropy) in Eq. (3).\n\nFor simplicity, in the following discussions, we omit the discounted factor \u03b3 for conciseness. Before presenting the theoretical justifications, we focus on a simplified setting with the following assumptions. Assumption 1. (Non-negative Reward and Fixed Horizon) The MDP has finite states and actions with a fixed horizon length and non-negative rewards, i.e., r t \u2265 0 at each timestep t.\n\nWe remark that this is a general condition since for any fixed-horizon MDP with bounded rewards, we can shape the reward function without changing the optimal policies by adding a big constant to each possible reward. Assumption 2. (Multiple Distinct Global Optima) There exist M distinct global optimal policies \u03c0 * 1 , \u03c0 * 2 , . . . , \u03c0 * M , namely,\n\u03c0 * i \u2208 arg max \u03c0 E \u03c4 \u223c\u03c0 [ t r t ] for 1 \u2264 i \u2264 M , and D filter (\u03c0 * i , \u03c0 * j ) \u2265 \u03b4 for 1 \u2264 i = j \u2264 M .\nThis assumption ensures that the problem is solvable. Assumption 3. (Non-trivial Optimum) For each global optimal policy \u03c0 * , we have rt\u2208\u03c4 r t > 0 for each possible trajectory \u03c4 under the policy \u03c0 .\n\nSince rewards are non-negative, Assumption 3 suggests that the optimal policy will not generate any trivial trajectories. We also remark that similar to Assumption 1, this assumption can be generally satisfied by properly shaping the underlying reward function without changing the optimal policies.\n\nWith all the assumptions presented, we first derive the filtering objective in Eq. (5) via Theorem G.1 and then the switching objective in Eq. (6) via Theorem G.2. Theorem G.1. (Filtering Objective) Consider the constrained optimization problem\n\u03c0 k+1 = arg max \u03c0 E \u03c4 \u223c\u03c0 t r t , subject to inf \u03c4 \u223c\u03c0 \u2212 t log \u03c0 i (a t |s t ) \u2265 \u03b4, \u22001 \u2264 i \u2264 k < M.\n(16) Given assumption 1, 2 and 3, solving the following unconstrained optimization problem\n\u03c0 k+1 = arg max \u03c0 E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 ) t r t(17)\nis equivalent to solving Eq. (16)\n, where \u03a6 k (\u03c4 ) := k i=1 \u03c6 i (\u03c4 ) and \u03c6 i (\u03c4 ) is defined by \u03c6 i (\u03c4 ) = 1, if \u2212 t log \u03c0 i (a t |s t ) \u2265 \u03b4, (s t , a t ) \u223c \u03c4 0, otherwise .(18)\nProof. When k = 1, the optimization problems in Eq. (16) and Eq. (17) are trivially equivalent. We assume the optimization in the first iteration leads to a global optimum. For any k > 1, given previously discovered optimal policies {\u03c0 i |1 \u2264 i \u2264 k}, we will show that Eq. (17) must have an equivalent solution to Eq. (16), which is a new global optimum. By induction, the proposition holds.\n\nConsider a fixed k, we use S to denote the subspace of feasible policies satisfying all the distance constraints, i.e., S = {\u03c0 | \u22001 \u2264 i \u2264 k, D filter (\u03c0, \u03c0 i ) \u2265 \u03b4}. Note that by Assumption 2, We would like to prove Theorem. G.1 by showing that the optimal value of Eq. (17) over \u03c0 \u2208 S is greater than the value over \u03c0 / \u2208 S. Consequently, the solution of Eq. (17) should satisfy the constraints in Eq. (16).\n\nPublished as a conference paper at ICLR 2022\nFor \u03c0 \u2208 S, max \u03c0\u2208S E \u03c4 \u223c\u03c0[\u03a6 k (\u03c4 ) t rt] = max \u03c0\u2208S E \u03c4 \u223c\u03c0 t r t = max \u03c0 E\u03c4\u223c\u03c0[ t rt] .(19)\nFor \u03c0 / \u2208 S, we define\nT = \u03c4 \u223c \u03c0 \u2212 t log \u03c0 i (a t |s t ) \u2265 \u03b4 , \u22001 \u2264 i \u2264 k(20)\nas the constraint-satisfying set of \u03c4 under policy \u03c0. The right-hand side of Eq. (17) can be written as\nmax \u03c0 / \u2208S RHS = max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 ) t r t = max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 ) t r t \u03c4 \u2208 T + P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 ) t r t \u03c4 / \u2208 T = max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 1 \u00b7 t r t \u03c4 \u2208 T + P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 0 \u00b7 t r t \u03c4 / \u2208 T = max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 \u2208 T .(21)\nHere E \u03c4 \u223c\u03c0 [\u00b7 |\u03c4 \u2208 T ] denotes the expectation conditioned on the event {\u03c4 \u2208 T } and P \u03c0 (\u03c4 \u2208 T ) = E \u03c4 \u223c\u03c0 [\u03a6 k (\u03c4 )].\n\nNext, we want to prove that\nmax \u03c0 / \u2208S RHS of Eq. (17) < max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 t r t .(22)\nOn one side, if the optimal value of the right-hand side in Eq. (17) is not a global optimum, Eq. (22) holds because r t \u2265 0 by Assumption 1. On the other side, we consider the solution of the righthand side in Eq. (17) is a globally optimal policy \u03c0 * / \u2208 S. According to the definition of T , P \u03c0 * (\u03c4 / \u2208 T ) > 0. By Assumption 3, for \u2200\u03c4 \u223c \u03c0 * , \u03c4 r t > 0. Further, for \u03c0 * / \u2208 S,\nP \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 [ t r t | \u03c4 / \u2208 T ] > 0. Thus, max \u03c0 / \u2208S RHS of Eq. (17) < max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 \u2208 T + P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 / \u2208 T = max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 t r t = max \u03c0 E \u03c4 \u223c\u03c0 t r t = max \u03c0\u2208S E \u03c4 \u223c\u03c0 t r t .(23)\nTherefore, we can conclude that the maximum value of the objective function in Eq. (17) should be obtained when \u03c0 \u2208 S, because for \u03c0 \u2208 S, \u03c6 i (\u03c4 ) = 1 for all 1 \u2264 i \u2264 k, the constrained objective and the unconstrained objective have the same form. Finally, by induction, the solution of Eq. (17) must be a solution of Eq. (16).\n\nTheorem G.2. (Switching Objective) Consider the constrained optimization problem in Eq. (16). Given Assumption. 1, 2 and 3, for any \u03b4 > 0, there exists some \u03bb > 0 such that solving the following unconstrained optimization problem\n\u03c0 k+1 = arg max \u03c0 E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 ) t r t + \u03bb k i=1\n(1 \u2212 \u03c6 i (\u03c4 )) \u2212 t log \u03c0 i (a t |s t )\n\nis equivalent to solving Eq. (16), where \u03a6 k (\u03c4 ) := k i=1 \u03c6 i (\u03c4 ) and \u03c6 i (\u03c4 ) is defined by \u03c6 i (\u03c4 ) = 1, if \u2212 t log \u03c0 i (a t |s t ) \u2265 \u03b4, (s t , a t ) \u223c \u03c4 0, otherwise .\n\nProof. Following the same induction process and definition of S, the critical part is again to show that the optimal value of Eq. (24) w.r.t. a particular iteration k over \u03c0 \u2208 S is greater than the value over \u03c0 / \u2208 S. Consequently, the solution of Eq. (24) should satisfy the constraints in Eq. (16) and the overall proposition holds by induction.\n\nFor \u03c0 \u2208 S, the unconstrained optimization problem in Eq. (24) changes t\u00f4\n\u03c0 k+1 = arg max \u03c0\u2208S E \u03c4 \u223c\u03c0 t r t .(26)\nFor \u03c0 / \u2208 S, we define\nT = \u03c4 \u223c \u03c0 \u2212 t log \u03c0 i (a t |s t ) \u2265 \u03b4 , \u22001 \u2264 i \u2264 k(27)\nas the constraint-satisfying set of \u03c4 given a policy \u03c0. First we observe that when \u03c4 / \u2208 T ,\nk i=1 (1 \u2212 \u03c6 i (\u03c4 )) \u2212 t log \u03c0 i (a t |s t ) = k i=1 1 \u2212 t log \u03c0 i (a t |s t ) < \u03b4 \u2212 t log \u03c0 i (a t |s t ) < k i=1 \u03b4 = k\u03b4.(28)\nThen we can write the right-hand side of Eq. (24) as: = max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 [g(\u03c4 ) |\u03c4 \u2208 T ] + P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 [g(\u03c4 ) |\u03c4 / \u2208 T ] = max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 )\nt r t + \u03bb k i=1\n(1 \u2212 \u03c6 i (\u03c4 )) \u2212 t log \u03c0 i (a t |s t ) \u03c4 \u2208 T\n+ P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 \u03a6 k (\u03c4 ) t r t + \u03bb k i=1\n(1 \u2212 \u03c6 i (\u03c4 )) \u2212 t log \u03c0 i (a t |s t ) \u03c4 / \u2208 T = max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 1 \u00b7 t r t \u03c4 \u2208 T + P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 0 \u00b7 t r t \u03c4 / \u2208 T\n+ \u03bbP \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 k i=1 (1 \u2212 \u03c6 i (\u03c4 )) \u2212 t log \u03c0 i (a t |s t ) \u03c4 / \u2208 T < max \u03c0 / \u2208S P \u03c0 (\u03c4 \u2208 T )E \u03c4 \u223c\u03c0 1 \u00b7 t r t \u03c4 \u2208 T + P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 0 \u00b7 t r t \u03c4 / \u2208 T + \u03bbk\u03b4 = max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 t r t \u2212 P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 / \u2208 T + \u03bbk\u03b4.(29)\nHere E \u03c4 \u223c\u03c0 [\u00b7 |\u03c4 \u2208 T ] denotes the expectation conditioned on the event {\u03c4 \u2208 T } and P \u03c0 (\u03c4 \u2208 T ) = E \u03c4 \u223c\u03c0 [\u03a6 k (\u03c4 )].\n\nNext, we would like to show that max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 t r t \u2212 P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 / \u2208 T < max \u03c0 E \u03c4 \u223c\u03c0 t r t .\n\nNote that it is trivial that max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 [ t r t ] \u2212 P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 [ t r t | \u03c4 / \u2208 T ] \u2264 max \u03c0 E \u03c4 \u223c\u03c0 [ t r t ] by Assumption 1 (i.e., r t \u2265 0). We just need to verify that the equality condition is infeasible. We prove by contradiction. In the case of equality, the left-hand side yields a global optimum \u03c0 * / \u2208 S. However, by Assumption 3 (i.e., \u2200\u03c4 \u223c \u03c0 * , rt\u223c\u03c4 r t > 0), we have E \u03c4 \u223c\u03c0 [ t r t |\u03c4 \u2208 T ] > 0. Since P \u03c0 * (\u03c4 \u2208 T ) > 0, a contradiction yields. Thus, Eq. (30) holds.\n\nAccordingly, let\n\u2206 = max \u03c0 E \u03c4 \u223c\u03c0 t r t \u2212 max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 t r t \u2212 P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 / \u2208 T(31)\nand we can choose \u03bb \u2264 \u2206 k\u03b4 . With such a \u03bb, we can continue the derivation in Eq. (29) by\nmax \u03c0 / \u2208S RHS < max \u03c0 / \u2208S E \u03c4 \u223c\u03c0 t r t \u2212 P \u03c0 (\u03c4 / \u2208 T )E \u03c4 \u223c\u03c0 t r t \u03c4 / \u2208 T + \u03bbk\u03b4 \u2264 max \u03c0 E \u03c4 \u223c\u03c0 t r t = max \u03c0\u2208S E \u03c4 \u223c\u03c0 t r t .(32)\nTherefore, we can conclude that the maximum value of the objective function in Eq. (24) can be only obtained from \u03c0 \u2208 S. Finally, by induction, the solution of Eq. (24) must be a solution of Eq. (16).\n\nCorollary G.3. Providing Assumption 1, 2, and 3, the filtering objective and the switching objective can find M distinct global optima in M iterations.\n\nIntuition Remark: We want to emphasize that the constraint by D filter in Eq. (16) does not mean the infimum over every possible trajectories, but the infimum over a subspace of trajectories generated by the learning policy \u03c0 k+1 . From the perspective of functional analysis, \u03c0 k+1 can produce 0 probability w.r.t. some state-action pairs, which restricts the space of possible trajectories sampled from \u03c0 k+1 .\n\nPractical Remark 1: Note that even though in practice, a neural network policy may never produce an action probability of zero due to approximation error, we would like to remark that the constraintviolated trajectories will have a very low probability to be sampled, that is, we typically have P \u03c0 (\u2212 t log \u03c0(a t |s t ) \u2265 \u03b4) \u2248 1 when the policy converges. In this case, the diversity constraints will be rarely violated given a limited number of trajectory samples. This is empirically justified as shown in Fig. 5 and Fig. 12 where the trajectory acceptance rate consistently stays at 1 in the later stage of training.\n\nPractical Remark 2: Although the theorems only justify our algorithm when the optimal solutions are found, we empirically notice that our algorithm can even effectively discover a surprisingly diverse set of local optimal policies as shown in the experiment section. We believe there will be still huge room for further theoretical analysis, which we leave as future work.\n\n\nH ADDITIONAL DISCUSSIONS H.1 COMPARISON WITH CLASSICAL EXPLORATION METHODS\n\nRSPO is beyond an exploration method. In the classical exploration literature, the goal is to discover a single policy that can approach the global optimal solution to produce the highest reward. By contrast, the goal of RSPO is not to just find a policy with high reward. Instead, RSPO aims to find as many distinct local optima as possible. In our experiments, a standard PPO trial in iteration 1 of RSPO can directly solve the task with the highest rewards. However, there are still a large number of distinct local optima with novel emergent behaviors other than this high-reward one. This is a particularly challenging constrained optimization problem as more local optima are discovered.\n\n\nH.2 SUBOPTIMALITY OF DISCOVERED POLICIES\n\nIt is possible that a policy produced by RSPO reaches a suboptimal solution if nearly-optimal solutions have been all discovered, such as in the MuJoCo domain. Meanwhile, RSPO could also possibly find an optimal solution if the previously discovered solutions are all suboptimal, such as in the stag-hunt games. Overall, as a general solution, RSPO can definitely be applied as an exploration method by escaping sub-optimal strategies.\n\nH.3 MAY RSPO FAIL TO PRODUCE DIVERSE SOLUTIONS?\n\nAs more strategy modes are discovered, RSPO is solving an increasingly challenging constrained optimization problem, so it is indeed possible that an iteration \"fails\", e.g., some constraints may be violated or the policies may simply converge to previous modes leading. We empirically observe that these \"failure\" iterations typically lead to visually indistinguishable behaviors, which would not affect our final evaluation metric.\n\n\nI LICENSES\n\nWe use MuJoCo under a personal license. \n\n\nMordatch & Abbeel, 2018), 2-agent Markov stag-hunt games (Tang et al., 2021), continuous control in MuJoCo (Todorov et al., 2012), and the StarCraftII Multi-Agent Challenge (SMAC) (Vinyals et al., 2017; Rashid et al., 2019).\n\nFigure 1 :\n1The agent (orange) and landmarks (blue) in 4-Goals.\n\n\nPG), Diversity-Inducing Policy Gradient (DIPG) (Masood & Doshi-Velez, 2019), population-based training with cross-entropy objective (PBT-CE), DvD (Parker-Holder et al., 2020b), SMERL (Kumar et al., 2020), and Random Network Distillation (RND) (Burda et al., 2019). RND is designed to explore the policy with the highest reward, so we only evaluate RND in the hard mode.\n\n\nRewards on hard mode.\n\nFigure 2 :\n2Experiment results on 4-Goals for M = 7 iterations averaged over 5 random seeds. Error bars are 95% confidence intervals.\n\nFigure 4 :\n4Different strategies found in a run of 20 iterations diversity setting RSPO in Monster-Hunt.\n\nFigure 3 :\n3Monster-Hunt\n\nFigure 5 :Figure 6 :\n56Sample acceptance ratio when learning a policy distinct from Apple NE. The intrinsic reward is critical. Escalationof distinct strategies found.\n\nFigure 7 :\n7Results on Escalation averaged over 3 random seeds. Shaded area and error bars are 95% confident intervals.\n\n\nevaluate RSPO in the continuous control domain, including Half-Cheetah, Hopper, Walker2d and Humanoid, and compare it with baseline methods including PG, DIPG, DvD (Parker-Holder et al., 2020b), SMERL (Kumar et al., 2020) and population-based training with our cross-entropy objective (PBT-CE). All the methods are run over 5 iterations (a population size of 5, or have a latent dimension of 5) across 3 seeds. We adopt Population Diversity, a determinant-based diversity criterion proposed in Parker-Holder et al. (2020b), to evaluate the diversity of derived policies by different methods. Results are summarized in\n\nFigure 8 :Figure 9 :Figure 10 :Figure 11 :\n891011Half-Cheetah behaviors. Hopper Walker Homanoid behaviors.\n\nFigure 12 :Figure 13 :\n1213Data efficiency with different \u03b1 in Humanoid. Learning curve with different \u03bb int R on the 2m_vs_1z map in SMAC.\n\n\nof 4-Goals is based on Multi-Agent Particle Environments 1 (Mordatch & Abbeel, 2018). The size of the agent is 0.02 and the size of the goals are 0.04 (in easy and medium mode). The agent spawns at (0, 0). In the easy mode, the four goals spawn at {(0, 0.5), (0.5, 0), (0, \u22120.5), (\u22120.5, 0)} respectively; In the medium and hard mode, the four goals spawn uniformly randomly with x \u2208 [\u22121, 1] and y \u2208 [\u22121, 1]. At each step, the agent receives an observation vector of the relative positions of the four goals to itself. The game ends when the agent touches any of the goals or when the timestep limit of 16 is reached.\n\nC. 4\n4STARCRAFT MULTI-AGENT CHALLENGE SMAC concentrates on decentralized micromanagement scenarios, where each unit of the game is controlled by an individual RL agent. Detailed description of state/action space and reward function can be found in Rashid et al. (2019).\n\n\nThe Ridge Rider method(Parker-Holder et al., 2020a)  attempts to search for local optima via Hessian information. However, due to the high variance of policy gradients, the Hessian information can not be accurately estimated in complex RL problems. The Ridge Rider method basically diverges across the 4 domains we considered. Hence, we exclude the results of it.E POPULATION DIVERSITYPopulation Diversity (PD) is originally proposed in Parker-Holder et al. (2020b) as a metric to measure the group diversity of a population of deterministic policies \u03a0 = {\u00b5 1 , . . . , \u00b5 M }. The original PD is defined as:\n\n\na kernel function. In the case of Parker-Holder et al. (2020b), K is the Squared Exponential (or RBF) kernel, defined as K SE (x, y) := exp \u2212 x\u2212y 2 2l 2 N\n\nFigure 14 :Figure 15 :\n1415Diverse strategies discovered by RSPO on the 2c_vs_64zg map in SMAC. Diverse strategies discovered by RSPO on the 2m_vs_1z map in SMAC. The movement direction is indicated by yellow arrows.\n\nTable 1 :\n1Types of strategies discovered by each methods inMonster-Hunt over 20 iterations.Apple Corner Edge Chase \n\nAblation \n\nRSPO \n-No switch \n-No r int \n-r int \nB only \n\nBaseline \n\nRPG \nMAVEN \nPG/DIPG/RND \n\n\n\nTable 2 :\n2Population Diversity scores in MuJoCo.H.-Cheetah \nHopper \nWalker2d \nHumanoid \n\nPG \n0.033 (0.013) 0.418 (0.125) 0.188 (0.079) 0.965 (0.006) \nDIPG \n0.051 (0.009) 0.468 (0.054) 0.179 (0.056) 0.996 (0.000) \nPBT-CE 0.160 (0.078) 0.620 (0.294) 0.512 (0.032) 0.999 (0.000) \nDvD \n0.275 (0.164) 0.656 (0.523) 0.542 (0.103) 1.000 (0.000) \nSMERL 0.003 (0.002) 0.674 (0.389) 0.669 (0.152) \n\nN/A \n\nRSPO \n0.359 (0.058) 0.989 (0.009) 0.955 (0.039) 0.999 (0.000) \n\n\n\nTable 3 :\n3Number of visually distinct policies over 4 iterations in SMAC.2c64zg \n2m1z \n\nPG \n2 \n2 \nDIPG \n2 \n2 \nPBT-CE \n2 \n3 \nTrajDiv \n3 \n1 \nRSPO \n4 \n4 \n\n\n\n\nXiangyu Liu, Hangtian Jia, Ying Wen, Yaodong Yang, Yujing Hu, Yingfeng Chen, Changjie Fan, and Zhipeng Hu. Unifying behavioral and response diversity for open-ended learning in zero-sum games. arXiv preprint arXiv:2106.04958, 2021. Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot coordination. In International Conference on Machine Learning, pp. 7204-7213. PMLR, 2021. Nicolas Perez Nieves, Yaodong Yang, Oliver Slumbers, David Mguni, and Jun Wang. Modelling behavioural diversity for learning in open-ended games. ICML, 2021. Olle Nilsson and Antoine Cully. Policy gradient assisted map-elites. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 866-875, 2021. Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Effective diversity in population-based reinforcement learning. NeurIPS, 2020b. Riley Simmons-Edler, Ben Eisner, Daniel Yang, Anthony Bisulco, Eric Mitchell, Sebastian Seung, and Daniel Lee. Qxplore: Q-learning exploration by maximizing temporal difference error. 2019. David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019. Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Poet: open-ended coevolution of environments and their optimized solutions. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 142-151, 2019.Stephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes, and Richard Socher. The AI economist: Improving equality and productivity with AI-driven tax policies. arXiv preprint arXiv:2004.13332, 2020.Pingchuan Ma, Tao Du, and Wojciech Matusik. Efficient continuous pareto exploration in multi-task \nlearning. In International Conference on Machine Learning, pp. 6522-6531. PMLR, 2020. \n\nTengyu Ma. Why do local methods solve nonconvex problems? Beyond the Worst-Case Analysis of \nAlgorithms, pp. 465, 2020. \n\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and S. Whiteson. Maven: Multi-agent variational \nexploration. In NeurIPS, 2019. \n\nM. A. Masood and Finale Doshi-Velez. Diversity-inducing policy gradient: Using maximum mean \ndiscrepancy to find a set of diverse policies. 2019. \n\nB. Miller and Michael J. Shaw. Genetic algorithms with dynamic niche sharing for multimodal func-\ntion optimization. Proceedings of IEEE International Conference on Evolutionary Computation, \npp. 786-791, 1996. \n\nIgor Mordatch and P. Abbeel. Emergence of grounded compositional language in multi-agent \npopulations. In AAAI, 2018. \n\nShayegan Omidshafiei, Karl Tuyls, Wojciech M Czarnecki, Francisco C Santos, Mark Rowland, \nJerome Connor, Daniel Hennes, Paul Muller, Julien P\u00e9rolat, Bart De Vylder, et al. Navigating the \nlandscape of multiplayer games. Nature communications, 11(1):1-17, 2020. \n\nJack Parker-Holder, Luke Metz, Cinjon Resnick, H. Hu, A. Lerer, Alistair Letcher, Alexander \nPeysakhovich, Aldo Pacchiano, and Jakob Foerster. Ridge rider: Finding diverse solutions by \nfollowing eigenvectors of the hessian. NeurIPS, 2020a. \n\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration \nby self-supervised prediction. In International Conference on Machine Learning, pp. 2778-2787. \nPMLR, 2017. \n\nTiago Pereira, Maryam Abbasi, Bernardete Ribeiro, and Joel P. Arrais. Diversity oriented deep \nreinforcement learning for targeted molecule generation. Journal of Cheminformatics, 13(1):21, \nMar 2021. ISSN 1758-2946. doi: 10.1186/s13321-021-00498-z. URL https://doi.org/ \n10.1186/s13321-021-00498-z. \n\nJustin K. Pugh, L. B. Soros, and K. Stanley. Quality diversity: A new frontier for evolutionary \ncomputation. Frontiers Robotics AI, 3:40, 2016. \n\nM. Puterman. Markov decision processes: Discrete stochastic dynamic programming. In Wiley Series \nin Probability and Statistics, 1994. \n\nTabish Rashid, Philip HS Torr, Gregory Farquhar, Chia-Man Hung, Tim GJ Rudner, Nantas Nardelli, \nShimon Whiteson, Christian Schroeder de Witt, Jakob Foerster, and Mikayel Samvelyan. The Star-\ncraft multi-agent challenge. volume 4, pp. 2186-2188. International Foundation for Autonomous \nAgents and Multiagent Systems, 2019. \n\nJohn Schulman, Sergey Levine, P. Abbeel, Michael I. Jordan, and P. Moritz. Trust region policy \noptimization. ICML, 2015. \nJohn Schulman, F. Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy \noptimization algorithms. ArXiv, abs/1707.06347, 2017. \n\nRiley Simmons-Edler, Ben Eisner, Daniel Yang, Anthony Bisulco, Eric Mitchell, Sebastian Seung, \nand Daniel Lee. Reward prediction error as an exploration objective in deep rl. In Proceedings of the \nTwenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 2816-2823, 7 \n2020. \n\nHao Sun, Zhenghao Peng, Bo Dai, Jian Guo, Dahua Lin, and Bolei Zhou. Novel policy seeking with \nconstrained optimization. arXiv preprint arXiv:2005.10696, 2020. \n\nZhenggang Tang, C. Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, S. Du, Yu Wang, and \nYi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. ICLR, 2021. \n\nE. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. 2012 IEEE/RSJ \nInternational Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012. \n\nOriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle \nYeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A \nnew challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017. \n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung \nChung, Tonghan Wang*, Jianhao Wang*, Yi Wu, and Chongjie Zhang. Influence-based multi-agent explo-\nration. In International Conference on Learning Representations, 2020. \n\nChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising \neffectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021. \n\nTom Zahavy, Brendan O'Donoghue, Andre Barreto, Volodymyr Mnih, Sebastian Flennerhag, and \nSatinder Singh. Discovering diverse nearly optimal policies with successor features. arXiv preprint \narXiv:2106.00669, 2021. \n\nZeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient \nmethods. Advances in Neural Information Processing Systems, 31:4644-4654, 2018. \nA GIF DEMONSTRATIONS ON MUJOCO AND SMAC \n\nSee https://sites.google.com/view/rspo-iclr-2022. \n\nB ADDITIONAL RESULTS \n\nB.1 VISUALIZATION OF DISCOVERED STRATEGIES \n\nB.1.1 MUJOCO \n\n(a) Normal running \n\n(b) Handstand running \n\n(c) Upside-down running \n\n\n\nTable 4 :\n4Strategies induced by baseline methods and RSPO in SMAC map 2c_vs_64zg.Algorithm \nStrategies \n\nPG \nleft wave cleanup, right wave cleanup \nDIPG \ncliff walk, corner \nPBT-CE \nleft wave cleanup, right wave cleanup \nTrajDiv \nfire attractor and distant sniper, cliff walk, right wave cleanup \n\nRSPO \nleft wave cleanup, cliff sniping and smart blocking, right wave cleanup \ncorner, fire attractor and distant sniper, cliff walk \n\n\n\nTable 5 :\n5Strategies induced by baseline methods and RSPO in SMAC map 2m_vs_1z.Algorithm \nStrategies \n\nPG \nparallel hit-and-run, alterative distraction \nDIPG \nparallel hit-and-run, one-sided swinging \nPBT-CE \none-sided swinging, parallel-hit-and-run, swinging \nTrajDiv \nparallel hit-and-run \n\nRSPO \none-sided swinging, parallel-hit-and-run, swinging, alterative distraction \n\nB.2 QUANTITATIVE EVALUATION \n\nB.2.1 MUJOCO \n\n\n\nTable 6 :\n6Final evaluation performance of RSPO averaged over 32 episodes in MuJoCo continuous control domain. Averaged over 3 random seeds with standard deviation in the brackets.Environment \nIter #1 \nIter #2 \nIter #3 \nIter #4 \nIter #5 \n\nHalf-Cheetah 2343 (882) 1790 (950) 1710 (1394) 859 (498) 1194 (640) \nHopper \n1741 (56) \n1690 (29) \n1428 (322) 1349 (275) 841 (296) \nWalker2d \n1668 (284) 1627 (228) 1258 (538) 1275 (342) 1040 (318) \nHomanoid \n3146 (26) 3148 (140) \n3118 (84) \n3072 (76) 2707 (860) \n\n\n\nTable 7 :\n7Final evaluation winning rate of RSPO averaged over 32 episodes in SMAC.Map \nIter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter#6 \n\n2c_vs_64zg 100% 84.4% 96.9% 100% 87.5% 100% \n2m_vs_1z \n100% \n100% \n100% \n100% \nN/A \nN/A \n\nthat may have never been reported in the literature (check the website in Appendix A for details), \nwhich accords with our initial motivation. \n\n\n\nTable 8 :\n8Population Diversity on the hard map 2c_vs_64zg in SMAC.Algorithm # Distinct Strategies Population Diversity \n\nPG \n1 \n0.981 \nPBT-CE \n2 \n1.000 \nDvD \n3 \n1.000 \n\nRSPO \n4 \n1.000 \n\n\nTable 10 :\n10The number of distinct strategies discovered by PGA-MAP-Elites and RSPO and the highest achieved rewards in 4-Goals Hard. Numbers are averaged over 3 seeds.Easy Medium Hard Escalation Reward in 4-Goals HardPGA-MAP-Elites \n4 \n1.6 \n1.2 \n10 \n0.98 \nRSPO \n4 \n4 \n4 \n7.7 \n1.30 \n\n\n\nTable 11 :\n11PPO hyperparameters for different experiments.Hyperparameters \n4-Goals \nStag-Hunt Games \nMuJoCo \nSMAC \n\nNetwork structure \nMLP \nMLP \nMLP \nMLP+RNN 1 \nHidden size \n64 \n64 \n64 \n64 \nInitial learning rate \n3e-4 \n1e-3 \n3e-4 \n5e-4 \n\nBatch size \n8192 \nMonster-Hunt: 12800 \nEscalation: 6400 \n16384 \n1600 2 \n\nMinibatch size \n8192 \nMonster-Hunt: 12800 \nEscalation: 6400 \n512 \n1600 2 \n\nAdam stepsize ( ) \n1e-5 \n1e-5 \n1e-5 \n1e-5 \nDiscount rate (\u03b3) \n0.99 \n0.99 \n0.99 \n0.99 \nGAE parameter (\u03bb) \n0.95 \n0.95 \n0.95 \n0.95 \nValue loss coefficient \n0.5 \n1.0 \n0.5 \n0.5 \nEntropy coefficient \n0.05 \n0.01 \n0.01 \n0.01 \nGradient clipping \n0.5 \n0.5 \n0.5 \n10.0 \nPPO clipping parameter \n0.2 \n0.2 \n0.2 \n0.2 \n\nPPO epochs \n4 \n4 \n10 \n2m_vs_1z: 1 \n2c_vs_64zg: 5 \n\n1 Gated Recurrent Unit (GRU) (Chung et al., 2014) \n2 1600 chunks with length 10. \n\n\n\nTable 12 :\n12RSPO hyperparameters for different experiments.Hyperparameters 4-Goals Stag-Hunt Games \nMuJoCo \nSMAC \n\n\u03bb int \n\nB \n\nN/A \n0.2 \n1 \n2m_vs_1z: 1 \n2c_vs_64zg: 5 \n\n\u03bb int \n\nR \n\nN/A \n1 \n0 \n2m_vs_1z: 0.05 \n2c_vs_64zg: 0 \n\n\u03b1 \n0.5 \n0.6 \n\nH.Cheetah: 1.1 \nHopper & Walker2d: 0.9 \nHomanoid: 1.0 \n\n2m_vs_1z: 0.5 \n2c_vs_64zg: 0.3 \n\nD.2 BASELINE METHODS \n\n\nhttps://github.com/openai/multiagent-particle-envs\nhttps://github.com/dtak/DIPG-public 3 https://github.com/AnujMahajanOxf/MAVEN 4 https://github.com/ollenilsson19/PGA-MAP-Elites 5 https://github.com/jparkerholder/DvD_ES\n", "annotations": {"author": "[{\"end\":142,\"start\":131},{\"end\":145,\"start\":143},{\"end\":209,\"start\":146},{\"end\":282,\"start\":210},{\"end\":289,\"start\":283},{\"end\":328,\"start\":290}]", "publisher": null, "author_last_name": "[{\"end\":141,\"start\":137},{\"end\":152,\"start\":150},{\"end\":225,\"start\":220},{\"end\":288,\"start\":286}]", "author_first_name": "[{\"end\":136,\"start\":131},{\"end\":144,\"start\":143},{\"end\":149,\"start\":146},{\"end\":219,\"start\":210},{\"end\":285,\"start\":283}]", "author_affiliation": "[{\"end\":208,\"start\":154},{\"end\":281,\"start\":227},{\"end\":327,\"start\":291}]", "title": "[{\"end\":128,\"start\":1},{\"end\":456,\"start\":329}]", "venue": null, "abstract": "[{\"end\":1518,\"start\":458}]", "bib_ref": "[{\"end\":1830,\"start\":1820},{\"end\":2351,\"start\":2334},{\"end\":2427,\"start\":2405},{\"end\":2473,\"start\":2454},{\"end\":2545,\"start\":2525},{\"end\":2695,\"start\":2677},{\"end\":2714,\"start\":2695},{\"end\":2733,\"start\":2714},{\"end\":2795,\"start\":2776},{\"end\":2945,\"start\":2921},{\"end\":2966,\"start\":2945},{\"end\":3008,\"start\":2989},{\"end\":3144,\"start\":3125},{\"end\":3163,\"start\":3144},{\"end\":3191,\"start\":3163},{\"end\":3529,\"start\":3503},{\"end\":3547,\"start\":3529},{\"end\":4029,\"start\":4000},{\"end\":4047,\"start\":4029},{\"end\":4074,\"start\":4047},{\"end\":4266,\"start\":4247},{\"end\":4285,\"start\":4266},{\"end\":4376,\"start\":4357},{\"end\":4395,\"start\":4376},{\"end\":4411,\"start\":4395},{\"end\":6240,\"start\":6215},{\"end\":6282,\"start\":6260},{\"end\":6353,\"start\":6334},{\"end\":6402,\"start\":6381},{\"end\":7638,\"start\":7617},{\"end\":7655,\"start\":7638},{\"end\":7675,\"start\":7655},{\"end\":7891,\"start\":7872},{\"end\":7943,\"start\":7923},{\"end\":8086,\"start\":8069},{\"end\":8104,\"start\":8086},{\"end\":8291,\"start\":8269},{\"end\":8313,\"start\":8291},{\"end\":8362,\"start\":8333},{\"end\":8547,\"start\":8529},{\"end\":8794,\"start\":8767},{\"end\":9066,\"start\":9048},{\"end\":9086,\"start\":9066},{\"end\":9106,\"start\":9088},{\"end\":9836,\"start\":9819},{\"end\":10147,\"start\":10126},{\"end\":10382,\"start\":10353},{\"end\":10727,\"start\":10697},{\"end\":10764,\"start\":10732},{\"end\":10903,\"start\":10877},{\"end\":11256,\"start\":11236},{\"end\":11275,\"start\":11256},{\"end\":11302,\"start\":11275},{\"end\":14970,\"start\":14947},{\"end\":14975,\"start\":14970},{\"end\":20574,\"start\":20546},{\"end\":20593,\"start\":20574},{\"end\":20668,\"start\":20647},{\"end\":28715,\"start\":28696},{\"end\":31066,\"start\":31047},{\"end\":34881,\"start\":34862},{\"end\":46125,\"start\":46105},{\"end\":46143,\"start\":46125},{\"end\":46550,\"start\":46523},{\"end\":50955,\"start\":50929},{\"end\":51691,\"start\":51661},{\"end\":51779,\"start\":51761},{\"end\":51836,\"start\":51813},{\"end\":56414,\"start\":56390},{\"end\":59898,\"start\":59869},{\"end\":60417,\"start\":60397},{\"end\":60577,\"start\":60547},{\"end\":61069,\"start\":61050},{\"end\":61199,\"start\":61173},{\"end\":61865,\"start\":61837}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":76074,\"start\":75848},{\"attributes\":{\"id\":\"fig_1\"},\"end\":76139,\"start\":76075},{\"attributes\":{\"id\":\"fig_2\"},\"end\":76511,\"start\":76140},{\"attributes\":{\"id\":\"fig_3\"},\"end\":76535,\"start\":76512},{\"attributes\":{\"id\":\"fig_4\"},\"end\":76670,\"start\":76536},{\"attributes\":{\"id\":\"fig_5\"},\"end\":76776,\"start\":76671},{\"attributes\":{\"id\":\"fig_6\"},\"end\":76802,\"start\":76777},{\"attributes\":{\"id\":\"fig_7\"},\"end\":76971,\"start\":76803},{\"attributes\":{\"id\":\"fig_8\"},\"end\":77092,\"start\":76972},{\"attributes\":{\"id\":\"fig_9\"},\"end\":77712,\"start\":77093},{\"attributes\":{\"id\":\"fig_10\"},\"end\":77820,\"start\":77713},{\"attributes\":{\"id\":\"fig_11\"},\"end\":77961,\"start\":77821},{\"attributes\":{\"id\":\"fig_12\"},\"end\":78580,\"start\":77962},{\"attributes\":{\"id\":\"fig_13\"},\"end\":78851,\"start\":78581},{\"attributes\":{\"id\":\"fig_14\"},\"end\":79461,\"start\":78852},{\"attributes\":{\"id\":\"fig_15\"},\"end\":79618,\"start\":79462},{\"attributes\":{\"id\":\"fig_17\"},\"end\":79836,\"start\":79619},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":80050,\"start\":79837},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":80512,\"start\":80051},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":80667,\"start\":80513},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":87432,\"start\":80668},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":87868,\"start\":87433},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":88292,\"start\":87869},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":88797,\"start\":88293},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":89171,\"start\":88798},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":89359,\"start\":89172},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":89646,\"start\":89360},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":90472,\"start\":89647},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":90825,\"start\":90473}]", "paragraph": "[{\"end\":2015,\"start\":1534},{\"end\":3009,\"start\":2017},{\"end\":4579,\"start\":3011},{\"end\":6081,\"start\":4581},{\"end\":6665,\"start\":6083},{\"end\":7455,\"start\":6667},{\"end\":8706,\"start\":7472},{\"end\":10576,\"start\":8708},{\"end\":11159,\"start\":10578},{\"end\":11750,\"start\":11161},{\"end\":11953,\"start\":11752},{\"end\":12758,\"start\":11978},{\"end\":13026,\"start\":12760},{\"end\":13295,\"start\":13104},{\"end\":14023,\"start\":13341},{\"end\":14833,\"start\":14025},{\"end\":15327,\"start\":14835},{\"end\":15526,\"start\":15455},{\"end\":15721,\"start\":15528},{\"end\":15740,\"start\":15723},{\"end\":16559,\"start\":15795},{\"end\":17266,\"start\":16561},{\"end\":17703,\"start\":17357},{\"end\":18158,\"start\":17751},{\"end\":18459,\"start\":18160},{\"end\":18807,\"start\":18542},{\"end\":19521,\"start\":18809},{\"end\":19816,\"start\":19523},{\"end\":19947,\"start\":19818},{\"end\":19952,\"start\":19949},{\"end\":20213,\"start\":19954},{\"end\":21500,\"start\":20215},{\"end\":22056,\"start\":21502},{\"end\":22136,\"start\":22097},{\"end\":22815,\"start\":22206},{\"end\":23003,\"start\":22817},{\"end\":23683,\"start\":23005},{\"end\":24478,\"start\":23685},{\"end\":25523,\"start\":24494},{\"end\":27223,\"start\":25525},{\"end\":29139,\"start\":27295},{\"end\":29739,\"start\":29174},{\"end\":30645,\"start\":29741},{\"end\":32220,\"start\":30647},{\"end\":33543,\"start\":32222},{\"end\":33732,\"start\":33576},{\"end\":34376,\"start\":33734},{\"end\":35776,\"start\":34411},{\"end\":36329,\"start\":35791},{\"end\":36670,\"start\":36344},{\"end\":38283,\"start\":36672},{\"end\":39875,\"start\":38285},{\"end\":40377,\"start\":39890},{\"end\":42385,\"start\":40379},{\"end\":42545,\"start\":42387},{\"end\":43068,\"start\":42547},{\"end\":43287,\"start\":43097},{\"end\":44554,\"start\":43289},{\"end\":45736,\"start\":44556},{\"end\":46274,\"start\":45789},{\"end\":47207,\"start\":46276},{\"end\":47930,\"start\":47209},{\"end\":48441,\"start\":47955},{\"end\":48912,\"start\":48443},{\"end\":49488,\"start\":48914},{\"end\":49810,\"start\":49495},{\"end\":50209,\"start\":49812},{\"end\":50912,\"start\":50242},{\"end\":51393,\"start\":50926},{\"end\":51956,\"start\":51395},{\"end\":52425,\"start\":51958},{\"end\":53170,\"start\":52459},{\"end\":54325,\"start\":53193},{\"end\":55409,\"start\":54346},{\"end\":55640,\"start\":55424},{\"end\":56135,\"start\":55669},{\"end\":57422,\"start\":56148},{\"end\":57699,\"start\":57424},{\"end\":58528,\"start\":57701},{\"end\":58688,\"start\":58530},{\"end\":59137,\"start\":58690},{\"end\":59720,\"start\":59139},{\"end\":59771,\"start\":59730},{\"end\":59831,\"start\":59790},{\"end\":60380,\"start\":59833},{\"end\":61009,\"start\":60382},{\"end\":61596,\"start\":61011},{\"end\":61816,\"start\":61668},{\"end\":62046,\"start\":61834},{\"end\":62294,\"start\":62094},{\"end\":62659,\"start\":62296},{\"end\":62721,\"start\":62661},{\"end\":62928,\"start\":62835},{\"end\":63130,\"start\":63125},{\"end\":63341,\"start\":63308},{\"end\":63800,\"start\":63661},{\"end\":64187,\"start\":63965},{\"end\":64506,\"start\":64249},{\"end\":64723,\"start\":64567},{\"end\":65114,\"start\":64725},{\"end\":65468,\"start\":65116},{\"end\":65773,\"start\":65574},{\"end\":66074,\"start\":65775},{\"end\":66320,\"start\":66076},{\"end\":66509,\"start\":66419},{\"end\":66587,\"start\":66554},{\"end\":67123,\"start\":66732},{\"end\":67533,\"start\":67125},{\"end\":67579,\"start\":67535},{\"end\":67692,\"start\":67670},{\"end\":67851,\"start\":67748},{\"end\":68259,\"start\":68140},{\"end\":68288,\"start\":68261},{\"end\":68731,\"start\":68348},{\"end\":69295,\"start\":68968},{\"end\":69526,\"start\":69297},{\"end\":69615,\"start\":69577},{\"end\":69789,\"start\":69617},{\"end\":70138,\"start\":69791},{\"end\":70212,\"start\":70140},{\"end\":70274,\"start\":70252},{\"end\":70422,\"start\":70330},{\"end\":70733,\"start\":70550},{\"end\":70794,\"start\":70750},{\"end\":70978,\"start\":70843},{\"end\":71344,\"start\":71225},{\"end\":71462,\"start\":71346},{\"end\":71960,\"start\":71464},{\"end\":71978,\"start\":71962},{\"end\":72158,\"start\":72069},{\"end\":72493,\"start\":72293},{\"end\":72646,\"start\":72495},{\"end\":73060,\"start\":72648},{\"end\":73682,\"start\":73062},{\"end\":74056,\"start\":73684},{\"end\":74828,\"start\":74135},{\"end\":75308,\"start\":74873},{\"end\":75357,\"start\":75310},{\"end\":75792,\"start\":75359},{\"end\":75847,\"start\":75807}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13103,\"start\":13027},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15395,\"start\":15328},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15794,\"start\":15741},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17356,\"start\":17267},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18541,\"start\":18460},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22205,\"start\":22137},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27294,\"start\":27239},{\"attributes\":{\"id\":\"formula_8\"},\"end\":61667,\"start\":61611},{\"attributes\":{\"id\":\"formula_9\"},\"end\":61833,\"start\":61817},{\"attributes\":{\"id\":\"formula_10\"},\"end\":62093,\"start\":62047},{\"attributes\":{\"id\":\"formula_11\"},\"end\":62834,\"start\":62722},{\"attributes\":{\"id\":\"formula_12\"},\"end\":63124,\"start\":62929},{\"attributes\":{\"id\":\"formula_13\"},\"end\":63307,\"start\":63131},{\"attributes\":{\"id\":\"formula_14\"},\"end\":63604,\"start\":63342},{\"attributes\":{\"id\":\"formula_15\"},\"end\":63964,\"start\":63801},{\"attributes\":{\"id\":\"formula_16\"},\"end\":64566,\"start\":64507},{\"attributes\":{\"id\":\"formula_17\"},\"end\":65573,\"start\":65469},{\"attributes\":{\"id\":\"formula_18\"},\"end\":66418,\"start\":66321},{\"attributes\":{\"id\":\"formula_19\"},\"end\":66553,\"start\":66510},{\"attributes\":{\"id\":\"formula_20\"},\"end\":66731,\"start\":66588},{\"attributes\":{\"id\":\"formula_21\"},\"end\":67669,\"start\":67580},{\"attributes\":{\"id\":\"formula_22\"},\"end\":67747,\"start\":67693},{\"attributes\":{\"id\":\"formula_23\"},\"end\":68139,\"start\":67852},{\"attributes\":{\"id\":\"formula_24\"},\"end\":68347,\"start\":68289},{\"attributes\":{\"id\":\"formula_25\"},\"end\":68967,\"start\":68732},{\"attributes\":{\"id\":\"formula_26\"},\"end\":69576,\"start\":69527},{\"attributes\":{\"id\":\"formula_29\"},\"end\":70251,\"start\":70213},{\"attributes\":{\"id\":\"formula_30\"},\"end\":70329,\"start\":70275},{\"attributes\":{\"id\":\"formula_31\"},\"end\":70549,\"start\":70423},{\"attributes\":{\"id\":\"formula_32\"},\"end\":70749,\"start\":70734},{\"attributes\":{\"id\":\"formula_33\"},\"end\":70842,\"start\":70795},{\"attributes\":{\"id\":\"formula_34\"},\"end\":71224,\"start\":70979},{\"attributes\":{\"id\":\"formula_36\"},\"end\":72068,\"start\":71979},{\"attributes\":{\"id\":\"formula_37\"},\"end\":72292,\"start\":72159}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31154,\"start\":31147},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31983,\"start\":31976},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33587,\"start\":33580},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35326,\"start\":35319},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36669,\"start\":36650},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38962,\"start\":38955},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39951,\"start\":39944},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":41185,\"start\":41178},{\"end\":42288,\"start\":42281},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43286,\"start\":43278},{\"end\":43741,\"start\":43734},{\"end\":44944,\"start\":44937},{\"end\":45520,\"start\":45513},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":47673,\"start\":47665},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":56721,\"start\":56713},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":57698,\"start\":57690},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":58177,\"start\":58169}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1532,\"start\":1520},{\"attributes\":{\"n\":\"2\"},\"end\":7470,\"start\":7458},{\"attributes\":{\"n\":\"3\"},\"end\":11962,\"start\":11956},{\"attributes\":{\"n\":\"3.1\"},\"end\":11976,\"start\":11965},{\"attributes\":{\"n\":\"3.2\"},\"end\":13339,\"start\":13298},{\"attributes\":{\"n\":\"3.3\"},\"end\":15453,\"start\":15397},{\"attributes\":{\"n\":\"3.4\"},\"end\":17749,\"start\":17706},{\"attributes\":{\"n\":\"3.5\"},\"end\":22095,\"start\":22059},{\"attributes\":{\"n\":\"4\"},\"end\":24492,\"start\":24481},{\"attributes\":{\"n\":\"4.1\"},\"end\":27238,\"start\":27226},{\"attributes\":{\"n\":\"4.2\"},\"end\":29172,\"start\":29142},{\"attributes\":{\"n\":\"4.3\"},\"end\":33574,\"start\":33546},{\"attributes\":{\"n\":\"4.4\"},\"end\":34409,\"start\":34379},{\"attributes\":{\"n\":\"5\"},\"end\":35789,\"start\":35779},{\"end\":36342,\"start\":36332},{\"end\":39888,\"start\":39878},{\"end\":43095,\"start\":43071},{\"end\":45787,\"start\":45739},{\"end\":47953,\"start\":47933},{\"end\":49493,\"start\":49491},{\"end\":50240,\"start\":50212},{\"end\":50924,\"start\":50915},{\"end\":52457,\"start\":52428},{\"end\":53191,\"start\":53173},{\"end\":54344,\"start\":54328},{\"end\":55422,\"start\":55412},{\"end\":55667,\"start\":55643},{\"end\":56146,\"start\":56138},{\"end\":59728,\"start\":59723},{\"end\":59788,\"start\":59774},{\"end\":61610,\"start\":61599},{\"end\":63659,\"start\":63606},{\"end\":64247,\"start\":64190},{\"end\":74133,\"start\":74059},{\"end\":74871,\"start\":74831},{\"end\":75805,\"start\":75795},{\"end\":76086,\"start\":76076},{\"end\":76547,\"start\":76537},{\"end\":76682,\"start\":76672},{\"end\":76788,\"start\":76778},{\"end\":76824,\"start\":76804},{\"end\":76983,\"start\":76973},{\"end\":77756,\"start\":77714},{\"end\":77844,\"start\":77822},{\"end\":78586,\"start\":78582},{\"end\":79642,\"start\":79620},{\"end\":79847,\"start\":79838},{\"end\":80061,\"start\":80052},{\"end\":80523,\"start\":80514},{\"end\":87443,\"start\":87434},{\"end\":87879,\"start\":87870},{\"end\":88303,\"start\":88294},{\"end\":88808,\"start\":88799},{\"end\":89182,\"start\":89173},{\"end\":89371,\"start\":89361},{\"end\":89658,\"start\":89648},{\"end\":90484,\"start\":90474}]", "table": "[{\"end\":80050,\"start\":79930},{\"end\":80512,\"start\":80101},{\"end\":80667,\"start\":80588},{\"end\":87432,\"start\":82388},{\"end\":87868,\"start\":87516},{\"end\":88292,\"start\":87950},{\"end\":88797,\"start\":88474},{\"end\":89171,\"start\":88882},{\"end\":89359,\"start\":89240},{\"end\":89646,\"start\":89580},{\"end\":90472,\"start\":89707},{\"end\":90825,\"start\":90534}]", "figure_caption": "[{\"end\":76074,\"start\":75850},{\"end\":76139,\"start\":76088},{\"end\":76511,\"start\":76142},{\"end\":76535,\"start\":76514},{\"end\":76670,\"start\":76549},{\"end\":76776,\"start\":76684},{\"end\":76802,\"start\":76790},{\"end\":76971,\"start\":76827},{\"end\":77092,\"start\":76985},{\"end\":77712,\"start\":77095},{\"end\":77820,\"start\":77763},{\"end\":77961,\"start\":77849},{\"end\":78580,\"start\":77964},{\"end\":78851,\"start\":78588},{\"end\":79461,\"start\":78854},{\"end\":79618,\"start\":79464},{\"end\":79836,\"start\":79647},{\"end\":79930,\"start\":79849},{\"end\":80101,\"start\":80063},{\"end\":80588,\"start\":80525},{\"end\":82388,\"start\":80670},{\"end\":87516,\"start\":87445},{\"end\":87950,\"start\":87881},{\"end\":88474,\"start\":88305},{\"end\":88882,\"start\":88810},{\"end\":89240,\"start\":89184},{\"end\":89580,\"start\":89374},{\"end\":89707,\"start\":89661},{\"end\":90534,\"start\":90487}]", "figure_ref": "[{\"end\":24703,\"start\":24702},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25885,\"start\":25878},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26715,\"start\":26708},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27002,\"start\":26995},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28801,\"start\":28795},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29227,\"start\":29219},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29845,\"start\":29839},{\"end\":31741,\"start\":31735},{\"end\":32261,\"start\":32254},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33075,\"start\":33068},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33178,\"start\":33171},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36436,\"start\":36417},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36839,\"start\":36831},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36986,\"start\":36978},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37385,\"start\":37377},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37429,\"start\":37421},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37501,\"start\":37493},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37674,\"start\":37666},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37988,\"start\":37980},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38483,\"start\":38475},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38581,\"start\":38573},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38682,\"start\":38674},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38798,\"start\":38790},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39306,\"start\":39283},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43712,\"start\":43705},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44379,\"start\":44364},{\"end\":44390,\"start\":44384},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44914,\"start\":44907},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45337,\"start\":45330},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45442,\"start\":45433},{\"end\":45591,\"start\":45583},{\"end\":57287,\"start\":57281},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":57976,\"start\":57970},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":57988,\"start\":57981},{\"end\":73577,\"start\":73571},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":73589,\"start\":73582}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}