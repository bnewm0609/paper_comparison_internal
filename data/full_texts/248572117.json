{"id": 248572117, "updated": "2023-10-05 14:41:43.093", "metadata": {"title": "Unified Chinese License Plate Detection and Recognition with High Efficiency", "authors": "[{\"first\":\"Yanxiang\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Linjie\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Shuai\",\"last\":\"Tao\",\"middle\":[]},{\"first\":\"Xinchen\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Peicheng\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Zhiwei\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Mei\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recently, deep learning-based methods have reached an excellent performance on License Plate (LP) detection and recognition tasks. However, it is still challenging to build a robust model for Chinese LPs since there are not enough large and representative datasets. In this work, we propose a new dataset named Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP images as a supplement to the existing public benchmarks. The images are mainly captured with electronic monitoring systems with detailed annotations. To our knowledge, CRPD is the largest public multi-objective Chinese LP dataset with annotations of vertices. With CRPD, a unified detection and recognition network with high efficiency is presented as the baseline. The network is end-to-end trainable with totally real-time inference efficiency (30 fps with 640p). The experiments on several public benchmarks demonstrate that our method has reached competitive performance. The code and dataset will be publicly available at https://github.com/yxgong0/CRPD.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.03582", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jvcir/GongDTLWXMX22", "doi": "10.1016/j.jvcir.2022.103541"}}, "content": {"source": {"pdf_hash": "a86c6a69c27f73abab31e3694b6e908d2f39e59d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.03582v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "49df1fc02c6ef8ccc4eb7ebe5886d92378c920b1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a86c6a69c27f73abab31e3694b6e908d2f39e59d.txt", "contents": "\nUnified Chinese License Plate Detection and Recognition with High Efficiency\n7 May 2022\n\nYanxiang Gong \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nLinjie Deng \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nShuai Tao \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nXinchen Lu \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nPeicheng Wu \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nZhiwei Xie mxie@uestc.edu.cnmeixie \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nZheng Ma \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nMei Xie \nSchool of Information and Communication Engineering\nUniversity of Electronic Science\nTechnology of China\n\nUnified Chinese License Plate Detection and Recognition with High Efficiency\n7 May 2022Preprint submitted to Journal of L A T E X Templates May 10, 2022Chinese license plate datasetLicense plate detection and recognitionEnd-to-endReal-time * Corresponding author\nRecently, deep learning-based methods have reached an excellent performance on License Plate (LP) detection and recognition tasks. However, it is still challenging to build a robust model for Chinese LPs since there are not enough large and representative datasets. In this work, we propose a new dataset named Chinese Road Plate Dataset (CRPD) that contains multi-objective Chinese LP images as a supplement to the existing public benchmarks. The images are mainly captured with electronic monitoring systems with detailed annotations. To our knowledge, CRPD is the largest public multi-objective Chinese LP dataset with annotations of vertices. With CRPD, a unified detection and recognition network with high efficiency is presented as the baseline. The network is end-to-end trainable with totally real-time inference efficiency (30 fps with 640p). The experiments on several public benchmarks demonstrate that our method has reached competitive performance. The code and dataset will be publicly available at https://github.com/yxgong0/CRPD.\n\nIntroduction\n\nLicense Plate (LP) detection and recognition are the key parts of intelligent transportation systems because it is the unique identification of vehicles. The relevant methods are widely used on electronic toll payment, parking managing, and traffic monitoring systems. In order to achieve effective detection and recognition, researchers proposed a variety of techniques that are capable of handling the task in most conditions. Before the deep learning era, most of the methods were on the basis of artificial designs [1,2,3]. They utilized handcrafted features such as colors, shadows and textures, and integrated them by a cascaded strategy with license plate detection, segmentation, and recognition. Although they reached promising performance, the robustness may not be enough for some uncontrolled circumstances like weather, illumination, and rotation since the scheme relies on manually designed features. Then with the explosion of deep learning methods in recent years, most researchers turned their attention to this framework that is able to learn features automatically.\n\nThe convenient and efficient technique quickly became popular, and networks for detection and recognition also sprung up [4,5,6]. In these works, great successes have been achieved. However, for Chinese LPs, the problem of data scarcity gradually emerges. Existing annotated Chinese LP data that is representative of most scenarios cannot meet the huge demands. Thus, to alleviate the issue, we present our Chinese Road Plate Dataset (CRPD).\n\nAdmittedly, there are already some excellent public datasets with LPs [4,7,8,9,10,11,12,13,14,15]. These public benchmarks lay the foundation of various LP processing methods, and our CRPD is an effective supplement to existing Chinese LP datasets, which is more challenging. Images of CRPD are collected from electronic monitoring systems in most provinces of mainland China in different periods and weather conditions. The images contain cars with different statuses and types, and quite a part of the data contains more than one LP in one image. Each image has annotations of (i) LP content. (ii) Locations of four vertices. (iii) LP type. More details will be introduced in Section 3.\n\nAs for detection and recognition tasks, most prestigious methods designed the two branches separately. Zhou et al. [4] proposed a scheme for LP detection with Principal Visual Word (PVW) generation and applied bag-of-words in partial-duplicate image search. Chen et al. [5] put forward a method to detect the vehicles and the LPs simultaneously, where the results can be used for further recognition. The two-stage framework is effective, but the error accumulation problems hinder further progresses. Therefore, end-to-end frameworks are increasingly prevalent. In this paper, we propose an end-to-end trainable Chinese LP detection and recognition network with both high efficiency and satisfactory performance as the baseline of our CRPD. Our method is a unified network that consists of two branches. The branch for detection is based on STELA [16], which is a learned anchor-based detector. It only associates one reference box at each spatial position, which highly reduces the computation to reach a fast running speed. In the recognition branch, we abandon the recognition by segmentation pipeline and utilize a sequence-to-sequence method to accomplish the recognition task. The region-wise features are extracted by the RRoIAlign [17] operator and then will be fed into components for recognition.\n\nThis scheme blurs the line between detection and recognition, which strongly alleviates the error accumulation issues.\n\nIn summary, there are three main contributions in this work:\n\n\u2022 We publish a new Chinese LP dataset with more than 30k images, which covers more scenes and administrative regions of mainland China. We argue that this new dataset is more difficult than existing datasets and is also a supplement to the Chinese LP research field.\n\n\u2022 We propose an end-to-end trainable network for Chinese LP detection and recognition, which almost reaches a trade-off between accuracy and efficiency as a baseline. Through utilizing the common feature extraction branch and the RRoIAlign [17] operator, end-to-end training is achieved, and the error accumulation problems are alleviated with a real-time efficiency kept.\n\n\u2022 Our code and dataset will be publicly available soon. To facilitate the reference of researchers and get more progress, we will upload related materials.\n\n\nRelated Work\n\n\nLP Datasets\n\nDue to the importance of LP detection and recognition, researchers built and published a number of LP datasets. ReId [7] is a dataset for license plate recognition with 76k images gathered from surveillance cameras on highway toll gates.\n\nCaltech [8] and Zemris [9] collected over 600 images from the road and freeways with high-resolution cameras. Hsu et al. [11] presented a dataset for applications of access control, traffic law enforcement and road patrol. Gon\u00e7alves et al. [12] proposed Sense SegPlate Database to evaluate license plate character segmentation problem. Laroca et al. [13] provided a dataset that includes 4,500 fully annotated images from 150 vehicles in real-world scenarios. These datasets strongly support the researches of LP detection and recognition methods.\n\nHowever, LPs in different countries and regions are usually not the same. \n\n\nLP Detection and Recognition\n\nOwing to the successes of text detection and recognition, LP processing is also well developed. There are methods with end-to-end frameworks, which achieved excellent performance. Zhang et al. [18] integrated LP detection, tracking, and recognition into a unified framework via deep learning. Silva et al. [19] proposed to identify the vehicle and the LP region using two passes on the same CNN and then to recognize the characters using a second CNN. Kessentini et al. [20] presented a two-stage network to achieve the detection and recognition of LPs. In [21], a light CNN was proposed for detection and recognition, which achieved real-time efficiency.\n\nAs for Chinese LPs, there are also excellent end-to-end frameworks. Laroca et al. [22] proposed a unified approach for LP detection and layout classification to improve the recognition results. Qin et al. [23] proposed a unified method that can recognize both single-line and double-line LPs in an end-to-end way without line segmentation and character segmentation. However, Chinese LP processing is still challenging due to the large number of categories of Chinese characters.\n\nMeanwhile, LP processing under unconstrained scenarios also faces many problems. Therefore, it is still valuable to propose a new end-to-end framework for Chinese LP detection and recognition.\n\n\nCRPD Overview\n\n\nConstitution of Data\n\nBecause CRPD is presented as a supplement for existing datasets, special attention is paid to the diversity of data. The images are mainly captured on electronic monitoring systems, including vehicles that are running, turning, parked, or far away which may cause blur and rotation. The scene includes day and night and different weathers. Quite a part of the images contains more than There are totally about 25k images for training, 6.25k for validating, and 2.3k for testing. In CRPD-single, there are 20k images for training, 5k for validating, and 1k for testing. In CRPD-double, there are 4k images for training, 1k for validating, and 1k for testing. In CRPD-multi, there are 1k images for training, 0.25k for validating, and 0.3k for testing.\n\nThe annotations consist of three parts. The first is LP content which includes numbers, Chinese and English characters. There are some LPs that are too small or seriously blurred whose content is unidentifiable, and they are also annotated while the unrecognizable characters are replaced with a special one. The second is the coordinate of four vertices of the LPs. The last is the LP type, including blue (small cars), yellow and single line (front of large cars), yellow and double lines (back of large cars), and white (police cars).\n\n\nData Analysis\n\nCRPD provides more than 30k LP images with annotations. Though it is not the largest dataset in current frequently used Chinese datasets, some characteristics of CRPD will be helpful for training a robust model. Existing datasets mostly contain single and focused LP in one image, as shown in Figure 2. In some cases, such as electronic toll payment or parking managing systems, the data is highly effective. But when dealing with data for traffic monitoring systems, suspect car tracking, or vehicle flow measuring, images with more LPs will be required. CRPD aims to fill up this deficiency, so we paid special attention to the number of LPs, as shown in Figure 3. Also, CRPD has some other advantages for building a robust model. To illustrate them, we compare with CCPD [10], EasyPR [24], ChineseLP [4] and CLPD [15] in some aspects, which are shown in Tables 1, 2 and 3.\n\nThe first is the number of LPs. Images in CCPD [10] and CLPD [15] contain one LP, which can better indicate the detection Precision of a network. But as noted above, this may restrict the scenarios where it can be used. In comparison, EasyPR [24], ChineseLP [4] and our CRPD have better compatibility in this aspect.\n\nThe second is the status of vehicles. CCPD [10] contains images captured from parking lots, and they are more interested in LPs in various circumstances   Figure 4. Altogether, our CRPD covers most a Status of Vehicles CCPD EasyPR ChineseLP CLPD CRPD \nParked \u2714 \u2714 \u2714 \u2714 \u2714 Running \u2717 \u2714 \u2714 \u2714 \u2714 Turning \u2717 \u2714 \u2714 \u2714 \u2714 Far away \u2717 \u2717 \u2717 \u2714 \u2714\n\nType of Vehicles CCPD EasyPR ChineseLP CLPD CRPD\n\nCoach Vehicles  recognition of these LPs are also useful.\n\u2717 \u2714 \u2717 \u2714 \u2714 Police Vehicles \u2717 \u2717 \u2717 \u2714 \u2714 Trailers \u2717 \u2717 \u2717 \u2717 \u2714\n\nMethodology\n\nIn this section, we will describe the proposed method in detail, and the pipeline is shown in Figure 5.\n\n\nDetection Branch\n\nThe first stage is the LP detection step. As noted above, there are some obstacles for LP detection networks to reach a balance between accuracy and efficiency. Considering the trade-off, we utilize our previous STELA [16], a totally real-time detector, as the basis of our detection branch. The detection network is implemented on RetinaNet [25] and utilizes Feature Pyramid Network (FPN) [26] to construct a rich, multi-scale feature pyramid from a single resolution input image. It consists of three portions: anchor classification, rotated bounding box regression, and anchor refining.\n\n\nAnchor Classification\n\nAs we do not generate region proposals, class imbalance problems still exist in our scheme. That means there are only a few anchors that are annotated as positive (the object), while the others are negative. Therefore, Focal Loss [25] are utilized to calculate the loss of classification, as it is designed to deal with the problem. Firstly, we define p t with\np t = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 p if y = 1 1 \u2212 p otherwise(1)\nwhere p \u2208 [0, 1] is the predicted probability and y = 1 specifies the ground-truth class. Then the loss is defined as\nL cls = F L(p t ) = \u2212\u03b1 t (1 \u2212 p t ) \u03b3 log(p t )(2)\n\u03b1 t is a balanced weighting factor and \u03b3 is a focusing parameter. They are set to 0.25 and 2.0 respectively, which is the same as the original Focal Loss [25].\n\n\nRotated Bounding Box Regression\n\nFor the detection of tilted LP, we utilize rotated bounding boxes to match the instances. The box can be represented by a five tuple (x, y, w, h, \u03b8), in which x and y are the coordinate of the center point, w and h are the width and height of the box, and \u03b8 is the angle to horizontal, as shown in Figure 6. For the regression operation, the distance vector \u2206 = (\u03b4 x , \u03b4 y , \u03b4 w , \u03b4 h , \u03b4 \u03b8 ) is defined as\n\u03b4 x = (g x \u2212 b x )/b w , \u03b4 y = (g y \u2212 b y )/b h (3) \u03b4 w = log(g w /b w ), \u03b4 h = log(g h /b h ) (4) \u03b4 \u03b8 = tan(g \u03b8 ) \u2212 tan(b \u03b8 )(5)\nwhere b and g represent a bounding box and the corresponding target groundtruth respectively. The loss of the regression can be calculated by\nL loc = smooth L1 (\u2206 t \u2212 \u2206 p )(6)\nwhere smooth L1 is the smooth L1 loss [27], \u2206 t is the target and \u2206 p is the predicted tuple.\n\n\nLearned Anchor\n\nAs depicted in [16], the most important part of the proposed scheme in twostage is that the selected proposals are chosen by learning. The manually-defined original anchors with fixed scales and aspect ratios may not be the optimal designs, so an extra regression branch for anchor refining is added. The final classification and regression task will be reached on the learned anchors, which brings an improvement in accuracy with a little increment of computation. The original anchor, learned anchor, and output boxes are illustrated in Figure 7. It is obvious that the center should be well aligned with the pixel in feature maps.\n\nThus, the offsets are only regressed within \u2206 \u2032 = (\u03b4 \u2032 w , \u03b4 \u2032 h , \u03b4 \u2032 \u03b8 ), which means that only the shapes are adjusted. The loss can be calculated with\nL ref = smooth L1 (\u2206 \u2032 t \u2212 \u2206 \u2032 p )(7)\nin which \u2206 \u2032 t is the target and \u2206 \u2032 p is the predicted tuple. And finally, the total loss is\nL det = \u03bb ref L ref + \u03bb loc L loc + \u03bb cls L cls(8)\nin which \u03bb ref , \u03bb loc and \u03bb cls are the weights which are set to 0.5, 0.5, and 1 which have been proven to be effective. \n\n\nRecognition Branch\n\nThe recognition step is the second stage of the LP processing tasks. Because most of the networks for recognition achieved high efficiency, we simply utilize modules based on CRNN [28] to achieve the recognition. There are three parts in the modules: the convolutional layers, the recurrent layers, and the transcription layer.\n\nIn consideration that the backbone of our network has already extracted critical features of the input images, to avoid redundant computation, we utilize the processed feature maps as the input of the recognition branch. In order to effectively deal with rotated plates, RRoIAlign [17] that can crop the feature map with a rotated box, is applied to crop the maps according to the groundtruth boxes of LPs, and the cropped size is 8 \u00d7 25. While training, we consider that the feature maps may not be processed well when the predicted boxes are not accurate, so the feature maps will also be cropped with both groundtruth boxes and predicted boxes with a score higher than 0.9.\n\nAnd because the input is feature maps, we remove the first three convolutions of the original convolutional layers of CRNN [28] to avoid the over-fitting problems. And in order to reach better accuracy, we refer to our another previous work [29], and replace two convolution layers with deformable convolution layers and add four residual blocks in this branch. The deformable convolution layers have an adaptive receptive field that can better cover the text area, as shown in Figure 8.\n\nThe architecture of the convolutional layers is shown in Figure 5. The re- current layers are based on bidirectional LSTM [30], and the transcription component is based on CTCLoss [31]. The total loss function is a weighted loss L = \u03bb det L det + \u03bb rec L rec (9) where \u03bb det and \u03bb rec are constants that indicate the strength of the detection and recognition modules. And in our training, the value of L rec is onemagnitude-order larger than L det , so to keep a balance, we set them to 1 and 0.1, respectively.\n\n\nExperiments\n\n\nTraining Details\n\nThe training and testing datasets are from CCPD [10], EasyPR [24], and our CRPD. The input images are resized to 640 \u00d7 640 with three channels.\n\nIn the training stage, the optimizer of the network is Adam [18], the batch size is set to 32, and the learning rate is 1e-4. The network is trained for 35000\n\niterations which consume about 10 hours. The proposed method is implemented by PyTorch [32]. The experiments are carried on a platform with Intel Xeon(R)\n\nE5-2630 v3 CPU and a single NVIDIA TITAN RTX GPU.\n\n\nEvaluation Metrics\n\nTo demonstrate the effectiveness of the methods, we utilize the protocols described in [10] to evaluate the models. The detection will be considered as a match if it overlaps a ground truth bounding box by more than 60% and the words match exactly. Then Recall, Precision, and F-score are calculated with\nRecall = T P T P + F N(10)\nP recision = T P T P + F P (11)\nF \u2212 score = 2 \u00d7 P recision \u00d7 Recall P recision + Recall(12)\nwhere T P represents the number of positive objects that are predicted as positive and the words match, F P represents the number of negative objects that are predicted as positive, and F N represents the number of negative objects that are predicted as negative.\n\n\nAblations\n\nTo demonstrate that the unified architecture brings some improvements and our scheme reaches the best performance, we evaluate the proposed components.\n\nFirstly, we test the effectiveness of our unified network. As comparisons, cascaded STELA [16] and CRNN [28] models are utilized. The two networks are trained respectively, and the CRNN [28] model will process the original images cropped in the light of the boxes predicted by STELA [16]. All the models are trained on CRPD and evaluated on CCPD [10], EasyPR [24], and CRPD.\n\nFrom Table 4, we see that our network achieves a few improvements. A better result is reached on CRPD because the training and testing data have the same distribution. The LPs in CCPD [10] have a larger size, so the model trained on our CRPD, which contains LPs with small sizes, is not able to reach the best performance.\n\nThen as we utilize the predicted boxes of the detection modules to crop the feature maps in the training stage, we evaluate the effectiveness of them with  Table 5, it is obvious that utilizing predicted boxes will bring an improvement on the Recall because more incomplete LPs will be detected, with a little descend on Precision. Considering the recognition branch will also become more robust with this mechanism, it will make the network able to better deal with inaccurate boxes. But because Chinese LPs usually have seven characters, a box with a score lower than 0.86 may cut a whole character off.\n\nThus, there must be a deterioration of the performance when using boxes with scores only higher than 0.85. And the number of boxes with scores higher than 0.95 is less, which causes a smaller batch size for recognition modules, so we choose 0.9 as the threshold finally. The way to crop the feature maps may also bring different results of the recognition modules. We compare RoIPool [27], RoIAlign [33], and RRoIAlign [17], and the results are shown in Table 6. There is no obvious difference between the first two methods, and we think the reason is that LPs are not small objects, so some slight errors will not influence much. RRoIAlign [17] can better handle the work because the recognition branch will have great progress with consideration of the rotation degrees.\n\nIn the recognition branch, we crop the feature maps yield by different layers of the FPN [26] components to explain why we utilize those from the third layer.\n\nThough it seems to utilize the feature maps from the third layer is optimal in Table 7, but we consider it is because most of the LPs in the images of CRPD have sizes which match the anchors in the third layer. In the fifth or deeper layers, the anchors are mappings of some huge boxes in the original images, but most of the LPs have a small size. Finally, to demonstrate that the deformable layers and residual blocks in the recognition branch are effective, ablations are involved, as shown in Table 8.\n\nIn our metrics, only when the content matches exactly, the result will be regarded as correct. Therefore, deformable convolution brings an improvement in recognition, and the Recall and Precision are both improved.  To demonstrate that our method has reached a satisfactory performance, we make some comparisons with other methods. We involve EasyPR [24], SSD [34], YOLO-v3 [35], YOLO-v4 [36], Scaled YOLO-v4 [37] and Faster-RCNN [38]. A CRNN [28] model is appended to achieve recognition. We utilize cascaded STELA [16] and CRNN [28] as our baseline in experiments. From Table 9, it can be observed that EasyPR [24] cannot treat LPs in CRPD well because it depends on manually designed features. SSD [34], YOLOv3 [35], YOLO-v4 [36],\n\n\nComparisons\n\nScaled YOLO-v4 [37] and Faster-RCNN [38] are not specially designed for LPs or text, but the results are still competitive. Our method reaches the best on all the sub-datasets, which proves the effectiveness.  [41] and RPNet [10] are utilized for comparisons. We also involve YOLO-v4 [36] and Scaled YOLO-v4 [37] with CRNN [28] to report more results.\n\nOur baseline which consists of STELA [16] and CRNN [28] are also involved.\n\nFrom Table 10, it can be observed that our method has reached the best in most circumstances, and the efficiency is also competitive.   incorrectly are also hard to distinguish by humans. Our proposed method is still effective in most cases, and the performance is competitive. \n\n\nDiscussion and Conclusion\n\nIn this paper, we present a dataset with Chinese LP images, which is named CRPD. As a supplement to multi-LP datasets, CRPD includes three sub-datasets, CRPD-single, CRPD-double, and CRPD-multi, which are able to deal with a variety of application scenarios. And CRPD covers many kinds of vehicles and a number of environments that will be helpful to build a robust model. We also propose an end-to-end trainable network to detect and recognize LPs with high efficiency as the baseline of the dataset. The experiments demonstrate the effectiveness of our proposed components, and the performance of the network is satisfactory. In the future, we hope CRPD will become a new benchmark on multi-LP detection and recognition tasks. We also consider utilizing the network for end-to-end scene text spotting and integrating more advanced techniques to achieve better portability and adaptation capability.\n\nFigure 1 :\n1The constitution of CRPD dataset.one LP from different provinces and cities, and there are LPs of special vehicles involved, such as coach cars, police cars, and trailers, whose LPs will contain some special characters. The dataset includes three sub-datasets according to the numbers of LPs: CRPD-single, CRPD-double, and CRPD-multi, as shown inFigure 1. CRPD-single contains images with only one LP, CRPD-double contains images with two LPs, and CRPD-multi contains images with three or more LPs.\n\nFigure 2 :\n2Examples of LP images in existing public datasets.\n\nFigure 3 :\n3Examples of LP images in CRPD-single (the first row), CRPD-double (the second row), and CRPD-multi (the third row).\n\nFigure 4 :\n4LPs of special vehicles with their annotations in CRPD. The green rectangles are the annotated boxes and the text on the top left corner of the zooming rectangle is the annotated LP content. variety of common scenes. It is a strong supplement to Chinese LP datasets either used as training or evaluating data. However, there are also some limitations of CRPD. First, the location of each LP character is not annotated, which restricts the applications of object detection and data augmentation. Meanwhile, it is also difficult to detect each character because it is very small in the perspective of electronic monitoring systems. Second, the LPs are all on the vehicles. The actual LPs can be held in hand or placed on the ground in some circumstances, and the detection and\n\nFigure 5 :\n5The framework of our network. Conv, DeConv, Bn, MaxPooling, and ResBlock stand for convolution layer, deformable convolution layer, batch normalization layer, max pooling layer, and residual block, respectively. k, s, p, and c stand for kernel size, stride, padding size, and output channel number, respectively, with the size behind each of them.\n\nFigure 6 :\n6Illustration of the parameters. The red dotted box represents b which is a box and the green solid box represents g which is the groundtruth.\n\nFigure 7 :\n7The illustration of anchors and boxes. The red, yellow, and green boxes represent the original anchor, the learned anchor, and the final output boxes respectively. The original anchor (red) and learned anchor (yellow) have the same center point.\n\nFigure 8 :\n8Indication of fixed receptive fields in standard convolution (the first row) and adaptive receptive fields in deformable convolution (the second row). In each image triplet, the left shows the sampling locations of two levels of 3 \u00d7 3 filters on the preceding feature map, the middle shows the sampling locations of a 3 \u00d7 3 filter, and the right shows two activation units. Two sets of locations are highlighted according to the activation units.\n\nFinally\n, some detection and recognition results on CRPD are shown in Figure 9 and 10. In the illustrations, we see that most LPs that are recognized\n\nFigure 9 :\n9Spotting results of our method on CRPD. The green rectangles are the predicted rotated bounding boxes and the text on the top left corner of the zooming rectangle is the predicted text.\n\nFigure 10 :\n10Examples of failed recognition on CRPD. The green rectangles are the predicted rotated bounding boxes and the text on the top left corner of the zooming rectangle is the description of the LP, and the red characters are the ones that are recognized incorrectly. For simplicity, only the failed examples are zoomed in.\n\nTable 1 :\n1A comparison on the number of images with different numbers of LPs between current public Chinese LP datasets. with different illumination, rotation, and blur. Therefore, the vehicle status is not focused. Our CRPD concentrates on the capability to deal with a variety of vehicles, so there is better coverage on vehicle status. The last is the vehicle types. Though there are not a number of special vehicles on the road, the detection and recognition of them are still of great importance. Thus, we paid attention to the LPs of special vehicles to ensure the capability of the network trained by CRPD to deal with these LPs. Some images of them are shown inLP Number CCPD EasyPR ChineseLP CLPD CRPD \n\n= 1 \n224001 \n225 \n392 \n1200 \n26659 \n\n= 2 \n0 \n21 \n16 \n0 \n6242 \n\n= 3 \n0 \n10 \n1 \n0 \n1232 \n\n\u2265 4 \n0 \n0 \n2 \n0 \n371 \n\n\n\nTable 2 :\n2A comparison of the coverage of different vehicle statuses between current public Chinese LP datasets.\n\nTable 3 :\n3A comparison of the coverage of different vehicle types between current public Chinese LP datasets.\n\nTable 4 :\n4Ablations of the unified scheme on different datasets.Dataset \nSTELA+CRNN \nOur Method \n\nR \nP \nF \nR \nP \nF \n\nCCPD \n79.1 67.8 73.0 \n75.6 72.1 73.8 \n\nEasyPR 90.2 72.8 80.6 \n89.9 73.0 80.6 \n\nCRPD \n88.3 82.9 85.5 \n95.4 84.1 89.4 \n\nR: Recall; P: Precision; F: F-score \n\ndifferent scores. In \n\nTable 5 :\n5Ablations on CRPD-all between different thresholds of scores of the predicted boxes utilized to train the model.Threshold Recall Precision F-score \n\nNone \n91.2 \n85.4 \n88.2 \n\n>0.95 \n91.7 \n84.6 \n88.0 \n\n>0.90 \n95.4 \n84.1 \n89.4 \n\n>0.85 \n94.5 \n82.5 \n88.1 \n\n\n\nTable 6 :\n6Ablations on CRPD-all between different region feature extracting approaches.Approach \nRecall Precision F-score \n\nRoIPooling \n95.0 \n80.2 \n87.0 \n\nRoIAlign \n95.7 \n80.1 \n87.2 \n\nRRoIAlign \n95.4 \n84.1 \n89.4 \n\n\n\nTable 7 :\n7Ablations on CRPD-all with feature maps from different FPN layers to train the model.Feature Map Layer Recall Precision F-score \n\nP3 \n95.4 \n84.1 \n89.4 \n\nP4 \n82.4 \n70.1 \n75.8 \n\nP5 \n2.6 \n2.9 \n2.7 \n\n\n\nTable 8 :\n8Ablations on CRPD-all with feature maps from different FPN layers to train the model.Deformable Conv Recall Precision F-score \n\n\u2717 \n93.0 \n83.7 \n88.1 \n\n\u2714 \n95.4 \n84.1 \n89.4 \n\n\n\nTable 9 :\n9Comparisons on CRPD between our method and other methods. The methods that are based on deep-learning are trained on CRPD.Method \nCRPD-all \nCRPD-single \n\nRecall Precision F-score \nFPS \nRecall Precision F-score \nFPS \n\nEasyPR \n2.0 \n1.3 \n1.6 \n6 \n2.3 \n1.3 \n1.7 \n6 \n\nSSD512 + CRNN \n97.8 \n27.2 \n42.6 \n66 \n98.9 \n28.7 \n44.4 \n71 \n\nYOLOv3 + CRNN \n73.0 \n61.0 \n66.5 \n18 \n73.7 \n59.4 \n65.8 \n18 \n\nYOLOv4 + CRNN \n84.4 \n60.5 \n70.5 \n40 \n87.3 \n68.4 \n76.7 \n40 \n\nSYOLOv4 + CRNN \n86.8 \n71.0 \n78.2 \n35 \n90.1 \n72.4 \n80.3 \n35 \n\nFaster-RCNN + CRNN \n79.9 \n73.7 \n76.7 \n19 \n81.4 \n71.7 \n76.3 \n20 \n\nSTELA + CRNN \n88.3 \n82.9 \n85.5 \n36 \n83.1 \n73.3 \n77.9 \n36 \n\nOurs \n95.4 \n84.1 \n89.4 \n30 \n96.3 \n83.6 \n89.5 \n35 \n\nMethod \nCRPD-double \nCRPD-multi \n\nRecall Precision F-score \nFPS \nRecall Precision F-score \nFPS \n\nEasyPR \n1.5 \n1.2 \n1.3 \n6 \n1.8 \n1.7 \n1.8 \n6 \n\nSSD512 + CRNN \n97.5 \n27.5 \n42.9 \n66 \n93.5 \n21.1 \n34.5 \n63 \n\nYOLOv3 + CRNN \n74.6 \n64.4 \n69.1 \n17 \n66.2 \n61.3 \n63.6 \n17 \n\nYOLOv4 + CRNN \n90.4 \n41.2 \n56.6 \n40 \n88.9 \n36.8 \n52.0 \n39 \n\nSYOLOv4 + CRNN \n91.5 \n75.9 \n83.0 \n35 \n91.5 \n75.2 \n82.5 \n35 \n\nFaster-RCNN + CRNN \n81.1 \n77.9 \n79.5 \n19 \n69.3 \n75.2 \n72.1 \n17 \n\nSTELA + CRNN \n84.0 \n80.6 \n82.3 \n34 \n77.6 \n82.8 \n80.1 \n33 \n\nOurs \n95.8 \n84.5 \n89.8 \n30 \n90.8 \n85.0 \n87.7 \n26 \n\nSYOLOv4: Scaled YOLOv4 \n\n\n\n\nBecause Xu et al. [10] evaluated their method on different circumstances, for fair comparisons, experiments of our method on these datasets are also utilized. And as the other methods are trained on CCPD [10], we also utilize it as the training data of our method in this comparison. Because images in CCPD [10] only contain one LP in one image, only Precision is involved when the Recall is not considered. Following the experiments of Xu et al. [10], Cascade classifier[39], SSD300[34], YOLO9000[40], Faster-RCNN[38] are involved as the detector with Holistic-CNN[7] as the recognition model. And end-to-end methods TE2E\n\nTable 10 :\n10Comparisons of Precision on CCPD between our method and others. Size AP Base DB FN Rotate Tilt Weather Challenge FPS SYOLOv4: Scaled YOLOv4; HC: Holistic-CNN; AP: average percent of all the circumstancesMethod \nCascade classifier + HC \n480 58.9 69.7 67.2 69.7 \n0.1 \n3.1 \n52.3 \n30.9 \n29 \n\nSSD300 + HC \n300 95.2 98.3 96.6 95.9 \n88.4 \n91.5 \n87.3 \n83.8 \n35 \n\nYOLO9000 + HC \n480 93.7 98.1 96.0 88.2 \n84.5 \n88.5 \n87.0 \n80.5 \n36 \n\nYOLOv4 + CRNN \n512 94.7 97.8 94.6 87.3 \n82.9 \n89.9 \n83.3 \n75.7 \n40 \n\nSYOLOv4 + CRNN \n640 95.3 97.8 95.0 88.9 \n84.9 \n91.5 \n90.4 \n77.1 \n34 \n\nFaster-RCNN + HC \n600 92.8 97.2 94.4 90.9 \n82.9 \n87.3 \n85.5 \n76.3 \n13 \n\nTE2E \n600 94.4 97.8 94.8 94.5 \n87.9 \n92.1 \n86.8 \n81.2 \n3 \n\nRPNet \n480 95.5 98.5 96.9 94.3 \n90.8 \n92.5 \n87.9 \n85.1 \n61 \n\nSTELA + CRNN \n640 97.8 97.9 98.3 94.5 \n90.1 \n91.3 \n89.5 \n83.6 \n41 \n\nOurs \n640 97.9 98.3 98.0 97.2 92.5 \n93.7 \n90.7 \n87.9 \n30 \n\n\nAcknowledgmentsThis work was partly supported by the National Key Research and Development Program of China with ID 2018AAA0103203.\nEnd-to-end system of license plate localization and recognition. S Zhu, S A Dianat, L K Mestha, Journal of Electronic Imaging. 242S. Zhu, S. A. Dianat, L. K. Mestha, End-to-end system of license plate localization and recognition, Journal of Electronic Imaging 24 (2) (2015) 1 -18.\n\nAn algorithm for license plate recognition applied to intelligent transportation system. Y Wen, Y Lu, J Yan, Z Zhou, K M Deneen, P Shi, IEEE Transactions on Intelligent Transportation Systems. 123Y. Wen, Y. Lu, J. Yan, Z. Zhou, K. M. von Deneen, P. Shi, An algorithm for license plate recognition applied to intelligent transportation system, IEEE Transactions on Intelligent Transportation Systems 12 (3) (2011) 830-845.\n\nAutomatic license plate recognition (alpr): A state-of-the-art review. S Du, M Ibrahim, M Shehata, W Badawy, Technology. 232S. Du, M. Ibrahim, M. Shehata, W. Badawy, Automatic license plate recog- nition (alpr): A state-of-the-art review, IEEE Transactions on Circuits and Systems for Video Technology 23 (2) (2012) 311-325.\n\nPrincipal visual word discovery for automatic license plate detection. W Zhou, H Li, Y Lu, Q Tian, IEEE Transactions on Image Processing. 219W. Zhou, H. Li, Y. Lu, Q. Tian, Principal visual word discovery for au- tomatic license plate detection, IEEE Transactions on Image Processing 21 (9) (2012) 4269-4279.\n\nSimultaneous end-toend vehicle and license plate detection with multi-branch attention neural network. S.-L Chen, C Yang, J.-W Ma, F Chen, X.-C Yin, IEEE Transactions on Intelligent Transportation Systems. 219S.-L. Chen, C. Yang, J.-W. Ma, F. Chen, X.-C. Yin, Simultaneous end-to- end vehicle and license plate detection with multi-branch attention neural network, IEEE Transactions on Intelligent Transportation Systems 21 (9) (2020) 3686-3695.\n\nUsing synthetic images for deep learning recognition process on automatic license plate recognition. S C Barreto, J A Lambert, F De Barros, Vidal, Mexican Conference on Pattern Recognition (MCPR). S. C. Barreto, J. A. Lambert, F. de Barros Vidal, Using synthetic images for deep learning recognition process on automatic license plate recognition, in: Mexican Conference on Pattern Recognition (MCPR), 2019, pp. 115-126.\n\nHolistic recognition of low quality license plates by cnn using track annotated data. J \u0160pa\u0148hel, J Sochor, R Jur\u00e1nek, A Herout, L Mar\u0161\u00edk, P Zem\u010d\u00edk, 14th IEEE International Conference on Advanced Video and Signal Based Surveillance. AVSSJ.\u0160pa\u0148hel, J. Sochor, R. Jur\u00e1nek, A. Herout, L. Mar\u0161\u00edk, P. Zem\u010d\u00edk, Holistic recognition of low quality license plates by cnn using track annotated data, in: 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), 2017, pp. 1-6.\n\nCaltech, Caltech license plate dataset. Caltech, Caltech license plate dataset, http://www.vision.caltech.edu/html-files/archive.html (2005).\n\nZemris, Zemris license plate dataset. Zemris, Zemris license plate dataset, http://www.zemris.fer.hr/projects/LicensePlates/hrvatski/r (2003).\n\nTowards end-to-end license plate detection and recognition: A large dataset and baseline. Z Xu, W Yang, A Meng, N Lu, H Huang, C Ying, L Huang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Z. Xu, W. Yang, A. Meng, N. Lu, H. Huang, C. Ying, L. Huang, Towards end-to-end license plate detection and recognition: A large dataset and baseline, in: Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 255-271.\n\nApplication-oriented license plate recognition. G.-S Hsu, J.-C Chen, Y.-Z Chung, IEEE Transactions on Vehicular Technology. 622G.-S. Hsu, J.-C. Chen, Y.-Z. Chung, Application-oriented license plate recognition, IEEE Transactions on Vehicular Technology 62 (2) (2012) 552- 561.\n\nBenchmark for license plate character segmentation. G R Gon\u00e7alves, S P G Silva, D Menotti, W R Schwartz, Journal of Electronic Imaging. 25553034G. R. Gon\u00e7alves, S. P. G. da Silva, D. Menotti, W. R. Schwartz, Bench- mark for license plate character segmentation, Journal of Electronic Imag- ing 25 (5) (2016) 053034.\n\nA robust real-time automatic license plate recognition based on the yolo detector. R Laroca, E Severo, L A Zanlorensi, L S Oliveira, G R Gon\u00e7alves, W R Schwartz, D Menotti, 2018 International Joint Conference on Neural Networks (IJCNN). R. Laroca, E. Severo, L. A. Zanlorensi, L. S. Oliveira, G. R. Gon\u00e7alves, W. R. Schwartz, D. Menotti, A robust real-time automatic license plate recognition based on the yolo detector, in: 2018 International Joint Con- ference on Neural Networks (IJCNN), 2018, pp. 1-10.\n\nA robust and efficient approach to license plate detection. Y Yuan, W Zou, Y Zhao, X Wang, X Hu, N Komodakis, IEEE Transactions on Image Processing. 263Y. Yuan, W. Zou, Y. Zhao, X. Wang, X. Hu, N. Komodakis, A robust and efficient approach to license plate detection, IEEE Transactions on Image Processing 26 (3) (2017) 1102-1114.\n\nA robust attentional framework for license plate recognition in the wild. L Zhang, P Wang, H Li, Z Li, C Shen, Y Zhang, IEEE Transactions on Intelligent Transportation Systems. 2211L. Zhang, P. Wang, H. Li, Z. Li, C. Shen, Y. Zhang, A robust attentional framework for license plate recognition in the wild, IEEE Transactions on Intelligent Transportation Systems 22 (11) (2021) 6967-6976.\n\nStela: A real-time scene text detector with learned anchor. L Deng, Y Gong, X Lu, Y Lin, Z Ma, M Xie, IEEE Access. 7L. Deng, Y. Gong, X. Lu, Y. Lin, Z. Ma, M. Xie, Stela: A real-time scene text detector with learned anchor, IEEE Access 7 (2019) 153400-153407.\n\nFots: Fast oriented text spotting with a unified network. X Liu, D Liang, S Yan, D Chen, Y Qiao, J Yan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao, J. Yan, Fots: Fast oriented text spotting with a unified network, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 5676-5685.\n\nV-lpdr: Towards a unified framework for license plate detection, tracking, and recognition in real-world traffic videos. C Zhang, Q Wang, X Li, Neurocomputing. 449C. Zhang, Q. Wang, X. Li, V-lpdr: Towards a unified framework for li- cense plate detection, tracking, and recognition in real-world traffic videos, Neurocomputing 449 (2021) 189-206.\n\nReal-time license plate detection and recognition using deep convolutional neural networks. S M Silva, C R Jung, Journal of Visual Communication and Image Representation. 71102773S. M. Silva, C. R. Jung, Real-time license plate detection and recognition using deep convolutional neural networks, Journal of Visual Communica- tion and Image Representation 71 (2020) 102773.\n\nA two-stage deep neural network for multi-norm license plate detection and recognition, Expert Systems with. Y Kessentini, M D Besbes, S Ammar, A Chabbouh, Applications. 136Y. Kessentini, M. D. Besbes, S. Ammar, A. Chabbouh, A two-stage deep neural network for multi-norm license plate detection and recognition, Ex- pert Systems with Applications 136 (2019) 159-170.\n\nA light cnn for end-to-end car license plates detection and recognition. W Wang, J Yang, M Chen, P Wang, IEEE Access. 7W. Wang, J. Yang, M. Chen, P. Wang, A light cnn for end-to-end car license plates detection and recognition, IEEE Access 7 (2019) 173875-173883.\n\nAn efficient and layout-independent automatic license plate recognition system based on the yolo detector. R Laroca, L A Zanlorensi, G R Gon\u00e7alves, E Todt, W R Schwartz, D Menotti, IET Intelligent Transport Systems. 154R. Laroca, L. A. Zanlorensi, G. R. Gon\u00e7alves, E. Todt, W. R. Schwartz, D. Menotti, An efficient and layout-independent automatic license plate recognition system based on the yolo detector, IET Intelligent Transport Systems 15 (4) (2021) 483-503.\n\nEfficient and unified license plate recognition via lightweight deep neural network. S Qin, S Liu, IET Image Processing. 1416S. Qin, S. Liu, Efficient and unified license plate recognition via lightweight deep neural network, IET Image Processing 14 (16) (2020) 4102-4109.\n\n. R Liu, M Li, Easypr , R. Liu, M. Li, Easypr, https://github.com/liuruoze/EasyPR (2014).\n\nFocal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, Proceedings of the IEEE International Conference on Computer Vision (ICCV. the IEEE International Conference on Computer Vision (ICCVT.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Doll\u00e1r, Focal loss for dense object detection, in: Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2980-2988.\n\nFeature pyramid networks for object detection. T.-Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionT.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, S. Belongie, Fea- ture pyramid networks for object detection, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2117-2125.\n\nFast r-cnn. R Girshick, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)R. Girshick, Fast r-cnn, in: Proceedings of the IEEE International Confer- ence on Computer Vision (ICCV), 2015, pp. 1440-1448.\n\nAn end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition. B Shi, X Bai, C Yao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3911B. Shi, X. Bai, C. Yao, An end-to-end trainable neural network for image- based sequence recognition and its application to scene text recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (11) (2016) 2298-2304.\n\nFocus-enhanced scene text recognition with deformable convolutions. L Deng, Y Gong, X Lu, X Yi, Z Ma, M Xie, IEEE 5th International Conference on Computer and Communications (ICCC). L. Deng, Y. Gong, X. Lu, X. Yi, Z. Ma, M. Xie, Focus-enhanced scene text recognition with deformable convolutions, in: IEEE 5th International Conference on Computer and Communications (ICCC), 2019, pp. 1685- 1689.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Compu- tation 9 (8) (1997) 1735-1780.\n\nConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. A Graves, S Fern\u00e1ndez, F Gomez, J Schmidhuber, Proceedings of the 23rd International Conference on Machine Learning (ICML). the 23rd International Conference on Machine Learning (ICML)A. Graves, S. Fern\u00e1ndez, F. Gomez, J. Schmidhuber, Connectionist tem- poral classification: Labelling unsegmented sequence data with recurrent neural networks, in: Proceedings of the 23rd International Conference on Machine Learning (ICML), 2006, pp. 369-376.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in Neural Information Processing Systems (NIPS). A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-performance deep learning library, Advances in Neural Informa- tion Processing Systems (NIPS) 32 (2019) 8026-8037.\n\nK He, G Gkioxari, P Doll\u00e1r, R Girshick, Proceedings of the IEEE International Conference on Computer Vision (ICCV. the IEEE International Conference on Computer Vision (ICCVK. He, G. Gkioxari, P. Doll\u00e1r, R. Girshick, Mask r-cnn, in: Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2961-2969.\n\nW Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, European Conference on Computer Vision (ECCV). Ssd: Single shot multibox detectorW. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg, Ssd: Single shot multibox detector, in: European Conference on Computer Vision (ECCV), 2016, pp. 21-37.\n\nJ Redmon, A Farhadi, arXiv:1804.02767Yolov3: An incremental improvement. arXiv preprintJ. Redmon, A. Farhadi, Yolov3: An incremental improvement, arXiv preprint arXiv:1804.02767.\n\nA Bochkovskiy, C.-Y Wang, H.-Y M Liao, arXiv:2004.10934Yolov4: Optimal speed and accuracy of object detection. arXiv preprintA. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: Optimal speed and accuracy of object detection, arXiv preprint arXiv:2004.10934.\n\nScaled-yolov4: Scaling cross stage partial network. C.-Y Wang, A Bochkovskiy, H.-Y M Liao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)C.-Y. Wang, A. Bochkovskiy, H.-Y. M. Liao, Scaled-yolov4: Scaling cross stage partial network, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 13029- 13038.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in Neural Information Processing Systems (NIPS). S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, in: Advances in Neural Informa- tion Processing Systems (NIPS), 2015, pp. 91-99.\n\nA cascade framework for a real-time statistical plate recognition system. S.-Z Wang, H.-J Lee, IEEE Transactions on Information Forensics and Security. 22S.-Z. Wang, H.-J. Lee, A cascade framework for a real-time statistical plate recognition system, IEEE Transactions on Information Forensics and Se- curity 2 (2) (2007) 267-282.\n\nYolo9000: Better, faster, stronger. J Redmon, A Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR. the IEEE Conference on Computer Vision and Pattern Recognition (CVPRJ. Redmon, A. Farhadi, Yolo9000: Better, faster, stronger, in: Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 7263-7271.\n\nToward end-to-end car license plate detection and recognition with deep neural networks. H Li, P Wang, C Shen, IEEE Transactions on Intelligent Transportation Systems. 203H. Li, P. Wang, C. Shen, Toward end-to-end car license plate detection and recognition with deep neural networks, IEEE Transactions on Intelligent Transportation Systems 20 (3) (2019) 1126-1136.\n", "annotations": {"author": "[{\"end\":210,\"start\":90},{\"end\":329,\"start\":211},{\"end\":446,\"start\":330},{\"end\":564,\"start\":447},{\"end\":683,\"start\":565},{\"end\":825,\"start\":684},{\"end\":941,\"start\":826},{\"end\":1056,\"start\":942}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":99},{\"end\":222,\"start\":218},{\"end\":339,\"start\":336},{\"end\":457,\"start\":455},{\"end\":576,\"start\":574},{\"end\":694,\"start\":691},{\"end\":834,\"start\":832},{\"end\":949,\"start\":946}]", "author_first_name": "[{\"end\":98,\"start\":90},{\"end\":217,\"start\":211},{\"end\":335,\"start\":330},{\"end\":454,\"start\":447},{\"end\":573,\"start\":565},{\"end\":690,\"start\":684},{\"end\":831,\"start\":826},{\"end\":945,\"start\":942}]", "author_affiliation": "[{\"end\":209,\"start\":105},{\"end\":328,\"start\":224},{\"end\":445,\"start\":341},{\"end\":563,\"start\":459},{\"end\":682,\"start\":578},{\"end\":824,\"start\":720},{\"end\":940,\"start\":836},{\"end\":1055,\"start\":951}]", "title": "[{\"end\":77,\"start\":1},{\"end\":1133,\"start\":1057}]", "venue": null, "abstract": "[{\"end\":2366,\"start\":1320}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2904,\"start\":2901},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2906,\"start\":2904},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2908,\"start\":2906},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3592,\"start\":3589},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3594,\"start\":3592},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3596,\"start\":3594},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3984,\"start\":3981},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3986,\"start\":3984},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3988,\"start\":3986},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3990,\"start\":3988},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3993,\"start\":3990},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3996,\"start\":3993},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3999,\"start\":3996},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4002,\"start\":3999},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4005,\"start\":4002},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4008,\"start\":4005},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4719,\"start\":4716},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4874,\"start\":4871},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5453,\"start\":5449},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5845,\"start\":5841},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6604,\"start\":6600},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7040,\"start\":7037},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7170,\"start\":7167},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7185,\"start\":7182},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7284,\"start\":7280},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7403,\"start\":7399},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7513,\"start\":7509},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8012,\"start\":8008},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8125,\"start\":8121},{\"end\":8280,\"start\":8267},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8289,\"start\":8285},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8376,\"start\":8372},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8681,\"start\":8677},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11271,\"start\":11267},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11284,\"start\":11280},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11299,\"start\":11296},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11313,\"start\":11309},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11421,\"start\":11417},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11435,\"start\":11431},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11616,\"start\":11612},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11631,\"start\":11628},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11735,\"start\":11731},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12536,\"start\":12532},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12660,\"start\":12656},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12708,\"start\":12704},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13163,\"start\":13159},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13663,\"start\":13659},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14455,\"start\":14451},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14544,\"start\":14540},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15827,\"start\":15823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16257,\"start\":16253},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16777,\"start\":16773},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16895,\"start\":16891},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17265,\"start\":17261},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17323,\"start\":17319},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17401,\"start\":17398},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17737,\"start\":17733},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17750,\"start\":17746},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17894,\"start\":17890},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18081,\"start\":18077},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18308,\"start\":18304},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19165,\"start\":19161},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19179,\"start\":19175},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19358,\"start\":19354},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19421,\"start\":19417},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19434,\"start\":19430},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19635,\"start\":19631},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20766,\"start\":20762},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20781,\"start\":20777},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20801,\"start\":20797},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21023,\"start\":21019},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21245,\"start\":21241},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22173,\"start\":22169},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22183,\"start\":22179},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22197,\"start\":22193},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22211,\"start\":22207},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22232,\"start\":22228},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22253,\"start\":22249},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22266,\"start\":22262},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22339,\"start\":22335},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22353,\"start\":22349},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22435,\"start\":22431},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22524,\"start\":22520},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22537,\"start\":22533},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22551,\"start\":22547},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22587,\"start\":22583},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22608,\"start\":22604},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22782,\"start\":22778},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22797,\"start\":22793},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22856,\"start\":22852},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22880,\"start\":22876},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22895,\"start\":22891},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22962,\"start\":22958},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22976,\"start\":22972},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31595,\"start\":31591},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31607,\"start\":31603},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31621,\"start\":31617},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31638,\"start\":31634},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31688,\"start\":31685}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24717,\"start\":24206},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24781,\"start\":24718},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24910,\"start\":24782},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25698,\"start\":24911},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26059,\"start\":25699},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26214,\"start\":26060},{\"attributes\":{\"id\":\"fig_6\"},\"end\":26473,\"start\":26215},{\"attributes\":{\"id\":\"fig_7\"},\"end\":26933,\"start\":26474},{\"attributes\":{\"id\":\"fig_8\"},\"end\":27084,\"start\":26934},{\"attributes\":{\"id\":\"fig_9\"},\"end\":27283,\"start\":27085},{\"attributes\":{\"id\":\"fig_10\"},\"end\":27616,\"start\":27284},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28443,\"start\":27617},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28558,\"start\":28444},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28670,\"start\":28559},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28967,\"start\":28671},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":29232,\"start\":28968},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":29449,\"start\":29233},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29658,\"start\":29450},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":29843,\"start\":29659},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":31117,\"start\":29844},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":31742,\"start\":31118},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":32639,\"start\":31743}]", "paragraph": "[{\"end\":3466,\"start\":2382},{\"end\":3909,\"start\":3468},{\"end\":4599,\"start\":3911},{\"end\":5908,\"start\":4601},{\"end\":6028,\"start\":5910},{\"end\":6090,\"start\":6030},{\"end\":6358,\"start\":6092},{\"end\":6732,\"start\":6360},{\"end\":6889,\"start\":6734},{\"end\":7157,\"start\":6920},{\"end\":7706,\"start\":7159},{\"end\":7782,\"start\":7708},{\"end\":8470,\"start\":7815},{\"end\":8951,\"start\":8472},{\"end\":9145,\"start\":8953},{\"end\":9936,\"start\":9186},{\"end\":10475,\"start\":9938},{\"end\":11368,\"start\":10493},{\"end\":11686,\"start\":11370},{\"end\":11939,\"start\":11688},{\"end\":12120,\"start\":12063},{\"end\":12293,\"start\":12190},{\"end\":12903,\"start\":12314},{\"end\":13289,\"start\":12929},{\"end\":13453,\"start\":13336},{\"end\":13664,\"start\":13505},{\"end\":14106,\"start\":13700},{\"end\":14378,\"start\":14237},{\"end\":14506,\"start\":14413},{\"end\":15158,\"start\":14525},{\"end\":15314,\"start\":15160},{\"end\":15446,\"start\":15353},{\"end\":15620,\"start\":15498},{\"end\":15970,\"start\":15643},{\"end\":16648,\"start\":15972},{\"end\":17137,\"start\":16650},{\"end\":17650,\"start\":17139},{\"end\":17828,\"start\":17685},{\"end\":17988,\"start\":17830},{\"end\":18143,\"start\":17990},{\"end\":18194,\"start\":18145},{\"end\":18521,\"start\":18217},{\"end\":18580,\"start\":18549},{\"end\":18904,\"start\":18641},{\"end\":19069,\"start\":18918},{\"end\":19445,\"start\":19071},{\"end\":19769,\"start\":19447},{\"end\":20376,\"start\":19771},{\"end\":21150,\"start\":20378},{\"end\":21310,\"start\":21152},{\"end\":21817,\"start\":21312},{\"end\":22552,\"start\":21819},{\"end\":22919,\"start\":22568},{\"end\":22995,\"start\":22921},{\"end\":23275,\"start\":22997},{\"end\":24205,\"start\":23305}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12011,\"start\":11940},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12175,\"start\":12121},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13335,\"start\":13290},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13504,\"start\":13454},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14236,\"start\":14107},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14412,\"start\":14379},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15352,\"start\":15315},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15497,\"start\":15447},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18548,\"start\":18522},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18640,\"start\":18581}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19459,\"start\":19452},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19934,\"start\":19927},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":20839,\"start\":20832},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":21398,\"start\":21391},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":21816,\"start\":21809},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":22398,\"start\":22391},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23010,\"start\":23002}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2380,\"start\":2368},{\"attributes\":{\"n\":\"2.\"},\"end\":6904,\"start\":6892},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6918,\"start\":6907},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7813,\"start\":7785},{\"attributes\":{\"n\":\"3.\"},\"end\":9161,\"start\":9148},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9184,\"start\":9164},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10491,\"start\":10478},{\"end\":12061,\"start\":12013},{\"attributes\":{\"n\":\"4.\"},\"end\":12188,\"start\":12177},{\"attributes\":{\"n\":\"4.1.\"},\"end\":12312,\"start\":12296},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":12927,\"start\":12906},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":13698,\"start\":13667},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":14523,\"start\":14509},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15641,\"start\":15623},{\"attributes\":{\"n\":\"5.\"},\"end\":17664,\"start\":17653},{\"attributes\":{\"n\":\"5.1.\"},\"end\":17683,\"start\":17667},{\"attributes\":{\"n\":\"5.2.\"},\"end\":18215,\"start\":18197},{\"attributes\":{\"n\":\"5.3.\"},\"end\":18916,\"start\":18907},{\"attributes\":{\"n\":\"5.4.\"},\"end\":22566,\"start\":22555},{\"attributes\":{\"n\":\"6.\"},\"end\":23303,\"start\":23278},{\"end\":24217,\"start\":24207},{\"end\":24729,\"start\":24719},{\"end\":24793,\"start\":24783},{\"end\":24922,\"start\":24912},{\"end\":25710,\"start\":25700},{\"end\":26071,\"start\":26061},{\"end\":26226,\"start\":26216},{\"end\":26485,\"start\":26475},{\"end\":26942,\"start\":26935},{\"end\":27096,\"start\":27086},{\"end\":27296,\"start\":27285},{\"end\":27627,\"start\":27618},{\"end\":28454,\"start\":28445},{\"end\":28569,\"start\":28560},{\"end\":28681,\"start\":28672},{\"end\":28978,\"start\":28969},{\"end\":29243,\"start\":29234},{\"end\":29460,\"start\":29451},{\"end\":29669,\"start\":29660},{\"end\":29854,\"start\":29845},{\"end\":31754,\"start\":31744}]", "table": "[{\"end\":28443,\"start\":28288},{\"end\":28967,\"start\":28737},{\"end\":29232,\"start\":29092},{\"end\":29449,\"start\":29322},{\"end\":29658,\"start\":29547},{\"end\":29843,\"start\":29756},{\"end\":31117,\"start\":29978},{\"end\":32639,\"start\":31960}]", "figure_caption": "[{\"end\":24717,\"start\":24219},{\"end\":24781,\"start\":24731},{\"end\":24910,\"start\":24795},{\"end\":25698,\"start\":24924},{\"end\":26059,\"start\":25712},{\"end\":26214,\"start\":26073},{\"end\":26473,\"start\":26228},{\"end\":26933,\"start\":26487},{\"end\":27084,\"start\":26943},{\"end\":27283,\"start\":27098},{\"end\":27616,\"start\":27299},{\"end\":28288,\"start\":27629},{\"end\":28558,\"start\":28456},{\"end\":28670,\"start\":28571},{\"end\":28737,\"start\":28683},{\"end\":29092,\"start\":28980},{\"end\":29322,\"start\":29245},{\"end\":29547,\"start\":29462},{\"end\":29756,\"start\":29671},{\"end\":29978,\"start\":29856},{\"end\":31742,\"start\":31120},{\"end\":31960,\"start\":31757}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10794,\"start\":10786},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11158,\"start\":11150},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11851,\"start\":11843},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12292,\"start\":12284},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14006,\"start\":13998},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":15072,\"start\":15064},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":17136,\"start\":17128},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17204,\"start\":17196}]", "bib_author_first_name": "[{\"end\":32838,\"start\":32837},{\"end\":32845,\"start\":32844},{\"end\":32847,\"start\":32846},{\"end\":32857,\"start\":32856},{\"end\":32859,\"start\":32858},{\"end\":33145,\"start\":33144},{\"end\":33152,\"start\":33151},{\"end\":33158,\"start\":33157},{\"end\":33165,\"start\":33164},{\"end\":33173,\"start\":33172},{\"end\":33175,\"start\":33174},{\"end\":33185,\"start\":33184},{\"end\":33550,\"start\":33549},{\"end\":33556,\"start\":33555},{\"end\":33567,\"start\":33566},{\"end\":33578,\"start\":33577},{\"end\":33876,\"start\":33875},{\"end\":33884,\"start\":33883},{\"end\":33890,\"start\":33889},{\"end\":33896,\"start\":33895},{\"end\":34221,\"start\":34217},{\"end\":34229,\"start\":34228},{\"end\":34240,\"start\":34236},{\"end\":34246,\"start\":34245},{\"end\":34257,\"start\":34253},{\"end\":34663,\"start\":34662},{\"end\":34665,\"start\":34664},{\"end\":34676,\"start\":34675},{\"end\":34678,\"start\":34677},{\"end\":34689,\"start\":34688},{\"end\":35070,\"start\":35069},{\"end\":35081,\"start\":35080},{\"end\":35091,\"start\":35090},{\"end\":35102,\"start\":35101},{\"end\":35112,\"start\":35111},{\"end\":35122,\"start\":35121},{\"end\":35861,\"start\":35860},{\"end\":35867,\"start\":35866},{\"end\":35875,\"start\":35874},{\"end\":35883,\"start\":35882},{\"end\":35889,\"start\":35888},{\"end\":35898,\"start\":35897},{\"end\":35906,\"start\":35905},{\"end\":36322,\"start\":36318},{\"end\":36332,\"start\":36328},{\"end\":36343,\"start\":36339},{\"end\":36601,\"start\":36600},{\"end\":36603,\"start\":36602},{\"end\":36616,\"start\":36615},{\"end\":36620,\"start\":36617},{\"end\":36629,\"start\":36628},{\"end\":36640,\"start\":36639},{\"end\":36642,\"start\":36641},{\"end\":36949,\"start\":36948},{\"end\":36959,\"start\":36958},{\"end\":36969,\"start\":36968},{\"end\":36971,\"start\":36970},{\"end\":36985,\"start\":36984},{\"end\":36987,\"start\":36986},{\"end\":36999,\"start\":36998},{\"end\":37001,\"start\":37000},{\"end\":37014,\"start\":37013},{\"end\":37016,\"start\":37015},{\"end\":37028,\"start\":37027},{\"end\":37434,\"start\":37433},{\"end\":37442,\"start\":37441},{\"end\":37449,\"start\":37448},{\"end\":37457,\"start\":37456},{\"end\":37465,\"start\":37464},{\"end\":37471,\"start\":37470},{\"end\":37780,\"start\":37779},{\"end\":37789,\"start\":37788},{\"end\":37797,\"start\":37796},{\"end\":37803,\"start\":37802},{\"end\":37809,\"start\":37808},{\"end\":37817,\"start\":37816},{\"end\":38156,\"start\":38155},{\"end\":38164,\"start\":38163},{\"end\":38172,\"start\":38171},{\"end\":38178,\"start\":38177},{\"end\":38185,\"start\":38184},{\"end\":38191,\"start\":38190},{\"end\":38415,\"start\":38414},{\"end\":38422,\"start\":38421},{\"end\":38431,\"start\":38430},{\"end\":38438,\"start\":38437},{\"end\":38446,\"start\":38445},{\"end\":38454,\"start\":38453},{\"end\":38959,\"start\":38958},{\"end\":38968,\"start\":38967},{\"end\":38976,\"start\":38975},{\"end\":39278,\"start\":39277},{\"end\":39280,\"start\":39279},{\"end\":39289,\"start\":39288},{\"end\":39291,\"start\":39290},{\"end\":39669,\"start\":39668},{\"end\":39683,\"start\":39682},{\"end\":39685,\"start\":39684},{\"end\":39695,\"start\":39694},{\"end\":39704,\"start\":39703},{\"end\":40002,\"start\":40001},{\"end\":40010,\"start\":40009},{\"end\":40018,\"start\":40017},{\"end\":40026,\"start\":40025},{\"end\":40301,\"start\":40300},{\"end\":40311,\"start\":40310},{\"end\":40313,\"start\":40312},{\"end\":40327,\"start\":40326},{\"end\":40329,\"start\":40328},{\"end\":40342,\"start\":40341},{\"end\":40350,\"start\":40349},{\"end\":40352,\"start\":40351},{\"end\":40364,\"start\":40363},{\"end\":40746,\"start\":40745},{\"end\":40753,\"start\":40752},{\"end\":40937,\"start\":40936},{\"end\":40944,\"start\":40943},{\"end\":40955,\"start\":40949},{\"end\":41068,\"start\":41064},{\"end\":41075,\"start\":41074},{\"end\":41084,\"start\":41083},{\"end\":41096,\"start\":41095},{\"end\":41102,\"start\":41101},{\"end\":41488,\"start\":41484},{\"end\":41495,\"start\":41494},{\"end\":41505,\"start\":41504},{\"end\":41517,\"start\":41516},{\"end\":41523,\"start\":41522},{\"end\":41536,\"start\":41535},{\"end\":41931,\"start\":41930},{\"end\":42329,\"start\":42328},{\"end\":42336,\"start\":42335},{\"end\":42343,\"start\":42342},{\"end\":42724,\"start\":42723},{\"end\":42732,\"start\":42731},{\"end\":42740,\"start\":42739},{\"end\":42746,\"start\":42745},{\"end\":42752,\"start\":42751},{\"end\":42758,\"start\":42757},{\"end\":43077,\"start\":43076},{\"end\":43091,\"start\":43090},{\"end\":43336,\"start\":43335},{\"end\":43346,\"start\":43345},{\"end\":43359,\"start\":43358},{\"end\":43368,\"start\":43367},{\"end\":43851,\"start\":43850},{\"end\":43861,\"start\":43860},{\"end\":43870,\"start\":43869},{\"end\":43879,\"start\":43878},{\"end\":43888,\"start\":43887},{\"end\":43900,\"start\":43899},{\"end\":43910,\"start\":43909},{\"end\":43921,\"start\":43920},{\"end\":43928,\"start\":43927},{\"end\":43942,\"start\":43941},{\"end\":44280,\"start\":44279},{\"end\":44286,\"start\":44285},{\"end\":44298,\"start\":44297},{\"end\":44308,\"start\":44307},{\"end\":44611,\"start\":44610},{\"end\":44618,\"start\":44617},{\"end\":44630,\"start\":44629},{\"end\":44639,\"start\":44638},{\"end\":44650,\"start\":44649},{\"end\":44661,\"start\":44657},{\"end\":44667,\"start\":44666},{\"end\":44669,\"start\":44668},{\"end\":44937,\"start\":44936},{\"end\":44947,\"start\":44946},{\"end\":45117,\"start\":45116},{\"end\":45135,\"start\":45131},{\"end\":45146,\"start\":45142},{\"end\":45148,\"start\":45147},{\"end\":45430,\"start\":45426},{\"end\":45438,\"start\":45437},{\"end\":45456,\"start\":45452},{\"end\":45458,\"start\":45457},{\"end\":45923,\"start\":45922},{\"end\":45930,\"start\":45929},{\"end\":45936,\"start\":45935},{\"end\":45948,\"start\":45947},{\"end\":46288,\"start\":46284},{\"end\":46299,\"start\":46295},{\"end\":46579,\"start\":46578},{\"end\":46589,\"start\":46588},{\"end\":47015,\"start\":47014},{\"end\":47021,\"start\":47020},{\"end\":47029,\"start\":47028}]", "bib_author_last_name": "[{\"end\":32842,\"start\":32839},{\"end\":32854,\"start\":32848},{\"end\":32866,\"start\":32860},{\"end\":33149,\"start\":33146},{\"end\":33155,\"start\":33153},{\"end\":33162,\"start\":33159},{\"end\":33170,\"start\":33166},{\"end\":33182,\"start\":33176},{\"end\":33189,\"start\":33186},{\"end\":33553,\"start\":33551},{\"end\":33564,\"start\":33557},{\"end\":33575,\"start\":33568},{\"end\":33585,\"start\":33579},{\"end\":33881,\"start\":33877},{\"end\":33887,\"start\":33885},{\"end\":33893,\"start\":33891},{\"end\":33901,\"start\":33897},{\"end\":34226,\"start\":34222},{\"end\":34234,\"start\":34230},{\"end\":34243,\"start\":34241},{\"end\":34251,\"start\":34247},{\"end\":34261,\"start\":34258},{\"end\":34673,\"start\":34666},{\"end\":34686,\"start\":34679},{\"end\":34699,\"start\":34690},{\"end\":34706,\"start\":34701},{\"end\":35078,\"start\":35071},{\"end\":35088,\"start\":35082},{\"end\":35099,\"start\":35092},{\"end\":35109,\"start\":35103},{\"end\":35119,\"start\":35113},{\"end\":35129,\"start\":35123},{\"end\":35490,\"start\":35483},{\"end\":35632,\"start\":35626},{\"end\":35864,\"start\":35862},{\"end\":35872,\"start\":35868},{\"end\":35880,\"start\":35876},{\"end\":35886,\"start\":35884},{\"end\":35895,\"start\":35890},{\"end\":35903,\"start\":35899},{\"end\":35912,\"start\":35907},{\"end\":36326,\"start\":36323},{\"end\":36337,\"start\":36333},{\"end\":36349,\"start\":36344},{\"end\":36613,\"start\":36604},{\"end\":36626,\"start\":36621},{\"end\":36637,\"start\":36630},{\"end\":36651,\"start\":36643},{\"end\":36956,\"start\":36950},{\"end\":36966,\"start\":36960},{\"end\":36982,\"start\":36972},{\"end\":36996,\"start\":36988},{\"end\":37011,\"start\":37002},{\"end\":37025,\"start\":37017},{\"end\":37036,\"start\":37029},{\"end\":37439,\"start\":37435},{\"end\":37446,\"start\":37443},{\"end\":37454,\"start\":37450},{\"end\":37462,\"start\":37458},{\"end\":37468,\"start\":37466},{\"end\":37481,\"start\":37472},{\"end\":37786,\"start\":37781},{\"end\":37794,\"start\":37790},{\"end\":37800,\"start\":37798},{\"end\":37806,\"start\":37804},{\"end\":37814,\"start\":37810},{\"end\":37823,\"start\":37818},{\"end\":38161,\"start\":38157},{\"end\":38169,\"start\":38165},{\"end\":38175,\"start\":38173},{\"end\":38182,\"start\":38179},{\"end\":38188,\"start\":38186},{\"end\":38195,\"start\":38192},{\"end\":38419,\"start\":38416},{\"end\":38428,\"start\":38423},{\"end\":38435,\"start\":38432},{\"end\":38443,\"start\":38439},{\"end\":38451,\"start\":38447},{\"end\":38458,\"start\":38455},{\"end\":38965,\"start\":38960},{\"end\":38973,\"start\":38969},{\"end\":38979,\"start\":38977},{\"end\":39286,\"start\":39281},{\"end\":39296,\"start\":39292},{\"end\":39680,\"start\":39670},{\"end\":39692,\"start\":39686},{\"end\":39701,\"start\":39696},{\"end\":39713,\"start\":39705},{\"end\":40007,\"start\":40003},{\"end\":40015,\"start\":40011},{\"end\":40023,\"start\":40019},{\"end\":40031,\"start\":40027},{\"end\":40308,\"start\":40302},{\"end\":40324,\"start\":40314},{\"end\":40339,\"start\":40330},{\"end\":40347,\"start\":40343},{\"end\":40361,\"start\":40353},{\"end\":40372,\"start\":40365},{\"end\":40750,\"start\":40747},{\"end\":40757,\"start\":40754},{\"end\":40941,\"start\":40938},{\"end\":40947,\"start\":40945},{\"end\":41072,\"start\":41069},{\"end\":41081,\"start\":41076},{\"end\":41093,\"start\":41085},{\"end\":41099,\"start\":41097},{\"end\":41109,\"start\":41103},{\"end\":41492,\"start\":41489},{\"end\":41502,\"start\":41496},{\"end\":41514,\"start\":41506},{\"end\":41520,\"start\":41518},{\"end\":41533,\"start\":41524},{\"end\":41545,\"start\":41537},{\"end\":41940,\"start\":41932},{\"end\":42333,\"start\":42330},{\"end\":42340,\"start\":42337},{\"end\":42347,\"start\":42344},{\"end\":42729,\"start\":42725},{\"end\":42737,\"start\":42733},{\"end\":42743,\"start\":42741},{\"end\":42749,\"start\":42747},{\"end\":42755,\"start\":42753},{\"end\":42762,\"start\":42759},{\"end\":43088,\"start\":43078},{\"end\":43103,\"start\":43092},{\"end\":43343,\"start\":43337},{\"end\":43356,\"start\":43347},{\"end\":43365,\"start\":43360},{\"end\":43380,\"start\":43369},{\"end\":43858,\"start\":43852},{\"end\":43867,\"start\":43862},{\"end\":43876,\"start\":43871},{\"end\":43885,\"start\":43880},{\"end\":43897,\"start\":43889},{\"end\":43907,\"start\":43901},{\"end\":43918,\"start\":43911},{\"end\":43925,\"start\":43922},{\"end\":43939,\"start\":43929},{\"end\":43949,\"start\":43943},{\"end\":44283,\"start\":44281},{\"end\":44295,\"start\":44287},{\"end\":44305,\"start\":44299},{\"end\":44317,\"start\":44309},{\"end\":44615,\"start\":44612},{\"end\":44627,\"start\":44619},{\"end\":44636,\"start\":44631},{\"end\":44647,\"start\":44640},{\"end\":44655,\"start\":44651},{\"end\":44664,\"start\":44662},{\"end\":44674,\"start\":44670},{\"end\":44944,\"start\":44938},{\"end\":44955,\"start\":44948},{\"end\":45129,\"start\":45118},{\"end\":45140,\"start\":45136},{\"end\":45153,\"start\":45149},{\"end\":45435,\"start\":45431},{\"end\":45450,\"start\":45439},{\"end\":45463,\"start\":45459},{\"end\":45927,\"start\":45924},{\"end\":45933,\"start\":45931},{\"end\":45945,\"start\":45937},{\"end\":45952,\"start\":45949},{\"end\":46293,\"start\":46289},{\"end\":46303,\"start\":46300},{\"end\":46586,\"start\":46580},{\"end\":46597,\"start\":46590},{\"end\":47018,\"start\":47016},{\"end\":47026,\"start\":47022},{\"end\":47034,\"start\":47030}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":38909775},\"end\":33053,\"start\":32772},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7943689},\"end\":33476,\"start\":33055},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206661467},\"end\":33802,\"start\":33478},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14703515},\"end\":34112,\"start\":33804},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":201248120},\"end\":34559,\"start\":34114},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":195063967},\"end\":34981,\"start\":34561},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3425112},\"end\":35481,\"start\":34983},{\"attributes\":{\"id\":\"b7\"},\"end\":35624,\"start\":35483},{\"attributes\":{\"id\":\"b8\"},\"end\":35768,\"start\":35626},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52846606},\"end\":36268,\"start\":35770},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7979740},\"end\":36546,\"start\":36270},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9775285},\"end\":36863,\"start\":36548},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206919813},\"end\":37371,\"start\":36865},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17045780},\"end\":37703,\"start\":37373},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":219530802},\"end\":38093,\"start\":37705},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":202583801},\"end\":38354,\"start\":38095},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9858530},\"end\":38835,\"start\":38356},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233523956},\"end\":39183,\"start\":38837},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":216445864},\"end\":39557,\"start\":39185},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":196209230},\"end\":39926,\"start\":39559},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":209322612},\"end\":40191,\"start\":39928},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":202121269},\"end\":40658,\"start\":40193},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232272232},\"end\":40932,\"start\":40660},{\"attributes\":{\"id\":\"b23\"},\"end\":41023,\"start\":40934},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":47252984},\"end\":41435,\"start\":41025},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10716717},\"end\":41916,\"start\":41437},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206770307},\"end\":42204,\"start\":41918},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":24139},\"end\":42653,\"start\":42206},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":201670529},\"end\":43050,\"start\":42655},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1915014},\"end\":43226,\"start\":43052},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9901844},\"end\":43778,\"start\":43228},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":202786778},\"end\":44277,\"start\":43780},{\"attributes\":{\"id\":\"b32\"},\"end\":44608,\"start\":44279},{\"attributes\":{\"id\":\"b33\"},\"end\":44934,\"start\":44610},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b34\"},\"end\":45114,\"start\":44936},{\"attributes\":{\"doi\":\"arXiv:2004.10934\",\"id\":\"b35\"},\"end\":45372,\"start\":45116},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":226964445},\"end\":45840,\"start\":45374},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":10328909},\"end\":46208,\"start\":45842},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14498782},\"end\":46540,\"start\":46210},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":786357},\"end\":46923,\"start\":46542},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1277107},\"end\":47290,\"start\":46925}]", "bib_title": "[{\"end\":32835,\"start\":32772},{\"end\":33142,\"start\":33055},{\"end\":33547,\"start\":33478},{\"end\":33873,\"start\":33804},{\"end\":34215,\"start\":34114},{\"end\":34660,\"start\":34561},{\"end\":35067,\"start\":34983},{\"end\":35858,\"start\":35770},{\"end\":36316,\"start\":36270},{\"end\":36598,\"start\":36548},{\"end\":36946,\"start\":36865},{\"end\":37431,\"start\":37373},{\"end\":37777,\"start\":37705},{\"end\":38153,\"start\":38095},{\"end\":38412,\"start\":38356},{\"end\":38956,\"start\":38837},{\"end\":39275,\"start\":39185},{\"end\":39666,\"start\":39559},{\"end\":39999,\"start\":39928},{\"end\":40298,\"start\":40193},{\"end\":40743,\"start\":40660},{\"end\":41062,\"start\":41025},{\"end\":41482,\"start\":41437},{\"end\":41928,\"start\":41918},{\"end\":42326,\"start\":42206},{\"end\":42721,\"start\":42655},{\"end\":43074,\"start\":43052},{\"end\":43333,\"start\":43228},{\"end\":43848,\"start\":43780},{\"end\":45424,\"start\":45374},{\"end\":45920,\"start\":45842},{\"end\":46282,\"start\":46210},{\"end\":46576,\"start\":46542},{\"end\":47012,\"start\":46925}]", "bib_author": "[{\"end\":32844,\"start\":32837},{\"end\":32856,\"start\":32844},{\"end\":32868,\"start\":32856},{\"end\":33151,\"start\":33144},{\"end\":33157,\"start\":33151},{\"end\":33164,\"start\":33157},{\"end\":33172,\"start\":33164},{\"end\":33184,\"start\":33172},{\"end\":33191,\"start\":33184},{\"end\":33555,\"start\":33549},{\"end\":33566,\"start\":33555},{\"end\":33577,\"start\":33566},{\"end\":33587,\"start\":33577},{\"end\":33883,\"start\":33875},{\"end\":33889,\"start\":33883},{\"end\":33895,\"start\":33889},{\"end\":33903,\"start\":33895},{\"end\":34228,\"start\":34217},{\"end\":34236,\"start\":34228},{\"end\":34245,\"start\":34236},{\"end\":34253,\"start\":34245},{\"end\":34263,\"start\":34253},{\"end\":34675,\"start\":34662},{\"end\":34688,\"start\":34675},{\"end\":34701,\"start\":34688},{\"end\":34708,\"start\":34701},{\"end\":35080,\"start\":35069},{\"end\":35090,\"start\":35080},{\"end\":35101,\"start\":35090},{\"end\":35111,\"start\":35101},{\"end\":35121,\"start\":35111},{\"end\":35131,\"start\":35121},{\"end\":35492,\"start\":35483},{\"end\":35634,\"start\":35626},{\"end\":35866,\"start\":35860},{\"end\":35874,\"start\":35866},{\"end\":35882,\"start\":35874},{\"end\":35888,\"start\":35882},{\"end\":35897,\"start\":35888},{\"end\":35905,\"start\":35897},{\"end\":35914,\"start\":35905},{\"end\":36328,\"start\":36318},{\"end\":36339,\"start\":36328},{\"end\":36351,\"start\":36339},{\"end\":36615,\"start\":36600},{\"end\":36628,\"start\":36615},{\"end\":36639,\"start\":36628},{\"end\":36653,\"start\":36639},{\"end\":36958,\"start\":36948},{\"end\":36968,\"start\":36958},{\"end\":36984,\"start\":36968},{\"end\":36998,\"start\":36984},{\"end\":37013,\"start\":36998},{\"end\":37027,\"start\":37013},{\"end\":37038,\"start\":37027},{\"end\":37441,\"start\":37433},{\"end\":37448,\"start\":37441},{\"end\":37456,\"start\":37448},{\"end\":37464,\"start\":37456},{\"end\":37470,\"start\":37464},{\"end\":37483,\"start\":37470},{\"end\":37788,\"start\":37779},{\"end\":37796,\"start\":37788},{\"end\":37802,\"start\":37796},{\"end\":37808,\"start\":37802},{\"end\":37816,\"start\":37808},{\"end\":37825,\"start\":37816},{\"end\":38163,\"start\":38155},{\"end\":38171,\"start\":38163},{\"end\":38177,\"start\":38171},{\"end\":38184,\"start\":38177},{\"end\":38190,\"start\":38184},{\"end\":38197,\"start\":38190},{\"end\":38421,\"start\":38414},{\"end\":38430,\"start\":38421},{\"end\":38437,\"start\":38430},{\"end\":38445,\"start\":38437},{\"end\":38453,\"start\":38445},{\"end\":38460,\"start\":38453},{\"end\":38967,\"start\":38958},{\"end\":38975,\"start\":38967},{\"end\":38981,\"start\":38975},{\"end\":39288,\"start\":39277},{\"end\":39298,\"start\":39288},{\"end\":39682,\"start\":39668},{\"end\":39694,\"start\":39682},{\"end\":39703,\"start\":39694},{\"end\":39715,\"start\":39703},{\"end\":40009,\"start\":40001},{\"end\":40017,\"start\":40009},{\"end\":40025,\"start\":40017},{\"end\":40033,\"start\":40025},{\"end\":40310,\"start\":40300},{\"end\":40326,\"start\":40310},{\"end\":40341,\"start\":40326},{\"end\":40349,\"start\":40341},{\"end\":40363,\"start\":40349},{\"end\":40374,\"start\":40363},{\"end\":40752,\"start\":40745},{\"end\":40759,\"start\":40752},{\"end\":40943,\"start\":40936},{\"end\":40949,\"start\":40943},{\"end\":40958,\"start\":40949},{\"end\":41074,\"start\":41064},{\"end\":41083,\"start\":41074},{\"end\":41095,\"start\":41083},{\"end\":41101,\"start\":41095},{\"end\":41111,\"start\":41101},{\"end\":41494,\"start\":41484},{\"end\":41504,\"start\":41494},{\"end\":41516,\"start\":41504},{\"end\":41522,\"start\":41516},{\"end\":41535,\"start\":41522},{\"end\":41547,\"start\":41535},{\"end\":41942,\"start\":41930},{\"end\":42335,\"start\":42328},{\"end\":42342,\"start\":42335},{\"end\":42349,\"start\":42342},{\"end\":42731,\"start\":42723},{\"end\":42739,\"start\":42731},{\"end\":42745,\"start\":42739},{\"end\":42751,\"start\":42745},{\"end\":42757,\"start\":42751},{\"end\":42764,\"start\":42757},{\"end\":43090,\"start\":43076},{\"end\":43105,\"start\":43090},{\"end\":43345,\"start\":43335},{\"end\":43358,\"start\":43345},{\"end\":43367,\"start\":43358},{\"end\":43382,\"start\":43367},{\"end\":43860,\"start\":43850},{\"end\":43869,\"start\":43860},{\"end\":43878,\"start\":43869},{\"end\":43887,\"start\":43878},{\"end\":43899,\"start\":43887},{\"end\":43909,\"start\":43899},{\"end\":43920,\"start\":43909},{\"end\":43927,\"start\":43920},{\"end\":43941,\"start\":43927},{\"end\":43951,\"start\":43941},{\"end\":44285,\"start\":44279},{\"end\":44297,\"start\":44285},{\"end\":44307,\"start\":44297},{\"end\":44319,\"start\":44307},{\"end\":44617,\"start\":44610},{\"end\":44629,\"start\":44617},{\"end\":44638,\"start\":44629},{\"end\":44649,\"start\":44638},{\"end\":44657,\"start\":44649},{\"end\":44666,\"start\":44657},{\"end\":44676,\"start\":44666},{\"end\":44946,\"start\":44936},{\"end\":44957,\"start\":44946},{\"end\":45131,\"start\":45116},{\"end\":45142,\"start\":45131},{\"end\":45155,\"start\":45142},{\"end\":45437,\"start\":45426},{\"end\":45452,\"start\":45437},{\"end\":45465,\"start\":45452},{\"end\":45929,\"start\":45922},{\"end\":45935,\"start\":45929},{\"end\":45947,\"start\":45935},{\"end\":45954,\"start\":45947},{\"end\":46295,\"start\":46284},{\"end\":46305,\"start\":46295},{\"end\":46588,\"start\":46578},{\"end\":46599,\"start\":46588},{\"end\":47020,\"start\":47014},{\"end\":47028,\"start\":47020},{\"end\":47036,\"start\":47028}]", "bib_venue": "[{\"end\":36029,\"start\":35980},{\"end\":38615,\"start\":38546},{\"end\":41244,\"start\":41186},{\"end\":41688,\"start\":41626},{\"end\":42077,\"start\":42018},{\"end\":43519,\"start\":43459},{\"end\":44452,\"start\":44394},{\"end\":45628,\"start\":45555},{\"end\":46752,\"start\":46684},{\"end\":32897,\"start\":32868},{\"end\":33246,\"start\":33191},{\"end\":33597,\"start\":33587},{\"end\":33940,\"start\":33903},{\"end\":34318,\"start\":34263},{\"end\":34756,\"start\":34708},{\"end\":35213,\"start\":35131},{\"end\":35521,\"start\":35492},{\"end\":35662,\"start\":35634},{\"end\":35978,\"start\":35914},{\"end\":36392,\"start\":36351},{\"end\":36682,\"start\":36653},{\"end\":37100,\"start\":37038},{\"end\":37520,\"start\":37483},{\"end\":37880,\"start\":37825},{\"end\":38208,\"start\":38197},{\"end\":38544,\"start\":38460},{\"end\":38995,\"start\":38981},{\"end\":39354,\"start\":39298},{\"end\":39727,\"start\":39715},{\"end\":40044,\"start\":40033},{\"end\":40407,\"start\":40374},{\"end\":40779,\"start\":40759},{\"end\":41184,\"start\":41111},{\"end\":41624,\"start\":41547},{\"end\":42016,\"start\":41942},{\"end\":42411,\"start\":42349},{\"end\":42835,\"start\":42764},{\"end\":43123,\"start\":43105},{\"end\":43457,\"start\":43382},{\"end\":44007,\"start\":43951},{\"end\":44392,\"start\":44319},{\"end\":44721,\"start\":44676},{\"end\":45007,\"start\":44973},{\"end\":45225,\"start\":45171},{\"end\":45553,\"start\":45465},{\"end\":46010,\"start\":45954},{\"end\":46360,\"start\":46305},{\"end\":46682,\"start\":46599},{\"end\":47091,\"start\":47036}]"}}}, "year": 2023, "month": 12, "day": 17}