{"id": 235368051, "updated": "2023-12-14 08:27:26.907", "metadata": {"title": "Conversational Fashion Image Retrieval via Multiturn Natural Language Feedback", "authors": "[{\"first\":\"Yifei\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Wai\",\"last\":\"Lam\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We study the task of conversational fashion image retrieval via multiturn natural language feedback. Most previous studies are based on single-turn settings. Existing models on multiturn conversational fashion image retrieval have limitations, such as employing traditional models, and leading to ineffective performance. We propose a novel framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts. One characteristic of the framework is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history. Furthermore, the image fashion attribute information is leveraged via a mutual attention strategy. Since there is no existing fashion dataset suitable for the multiturn setting of our task, we derive a large-scale multiturn fashion dataset via additional manual annotation efforts on an existing single-turn dataset. The experiments show that our proposed model significantly outperforms existing state-of-the-art methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3167936079", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/YuanL21", "doi": "10.1145/3404835.3462881"}}, "content": {"source": {"pdf_hash": "9a52054cd21a86a158f343cb075641d727be3595", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.04128v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2106.04128", "status": "GREEN"}}, "grobid": {"id": "0e73a165ef6699fedf37ebb2ce3445430a9db85c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9a52054cd21a86a158f343cb075641d727be3595.txt", "contents": "\nConversational Fashion Image Retrieval via Multiturn Natural Language Feedback\nACMCopyright ACM2021. July 11-15, 2021\n\nYifei Yuan yfyuan@se.cuhk.edu.hk \nThe Chinese University of Hong Kong\nThe Chinese University of Hong\nKong\n\nWai Lam wlam@se.cuhk.edu.hk \nThe Chinese University of Hong Kong\nThe Chinese University of Hong\nKong\n\nYifei Yuan \nThe Chinese University of Hong Kong\nThe Chinese University of Hong\nKong\n\nWai Lam \nThe Chinese University of Hong Kong\nThe Chinese University of Hong\nKong\n\nConversational Fashion Image Retrieval via Multiturn Natural Language Feedback\n\nCanada\nthe 44th Interna-tional ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21)New York, NY, USAACM102021. July 11-15, 202110.1145/3404835.3462881\nWe study the task of conversational fashion image retrieval via multiturn natural language feedback. Most previous studies are based on single-turn settings. Existing models on multiturn conversational fashion image retrieval have limitations, such as employing traditional models, and leading to ineffective performance. We propose a novel framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts. One characteristic of the framework is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history. Furthermore, the image fashion attribute information is leveraged via a mutual attention strategy. Since there is no existing fashion dataset suitable for the multiturn setting of our task, we derive a large-scale multiturn fashion dataset via additional manual annotation efforts on an existing single-turn dataset. The experiments show that our proposed model significantly outperforms existing state-of-the-art methods.CCS CONCEPTS\u2022 Information systems \u2192 Collaborative search.KEYWORDSMultiturn interactive image retrieval, cross-modal retrieval, multimodal embedding, natural language feedback ACM Reference Format:\n\nINTRODUCTION\n\nFashion is one of the most glamorous industries of the modern society, making great contributions to the global economy. With the development of image processing and information retrieval The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Codes: 14200719).\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  techniques, recently some investigation has been conducted in this domain, including fashion design [27,33], fashion product recommendation [12,18,23,26], and conversational fashion image retrieval [9,10,46]. Building an interactive conversational fashion image retrieval system based on user feedback has drawn increasing research interests in the past years. One important task in interactive conversational fashion image retrieval is target image selection, which aims to find the best-matched fashion image, called target image, from a set of candidate fashion product images via interactions of intermediate retrieved images, called reference images, and user natural language feedback texts. Consider a collection of fashion product images as shown in the top part of Figure 1, each image is associated with some fashion attributes. Suppose that a user has an information need of a fashion product, after an initial interaction with the system, a fashion product image is retrieved and presented to the user. Based on this intermediate reference image, the user typically wishes to refine the retrieval by providing natural language feedback texts, which describe the relative difference between the current retrieved reference image and the desired one. Such process is defined as a turn. If the user is not satisfied with the retrieved image, more turns are conducted until the desired product is retrieved. This multiturn process, consisting of several reference images, feedback texts as well as the final target image, is named as a session.\n\nStudies on multimodal feature composition have shown great promise in single-turn conversational image retrieval. Treating the query as a composition of an image and a text, these methods tackle the task by combining the visual and language representations [3,29,37,41]. These works, though having reasonable performance, are limited to single-turn feedback and cannot handle the multiturn conversational image retrieval task in our setting.\n\nRecently some existing multiturn conversational image retrieval methods have been investigated, however, their architecture and techniques are quite simple. [21] proposes a unique mode of feedback for image search, where users are allowed to give some property related binary feedback attempting to match his/her mental model of the target image(s). Others try to retrieve the target image based on multiple relevance levels [7] or relative attributes [21]. [24] proposes a knowledge-aware multimodal dialogue model which gives special consideration to the semantics and domain knowledge revealed in visual content. [9] first introduces a deep learning based approach to interactive image search which enables users to provide feedback via natural language. Based on this work, [45] proposed a novel constraint augmented reinforcement learning (RL) framework to efficiently incorporate user preferences over time. These works have several drawbacks: 1) Simply conducting attribute matching is far from understanding users' needs and thus leading to a bad performance. Existence of synonyms or metaphor makes it even harder to retrieve the target image on semantic level.\n\n2) The neural network models employed are not effective. Neural models which gain a lot of research interest such as attention mechanism [4] and Transformer [35] are not leveraged. 3) Fashion attribute information associated with products such as fabric, shape, etc is not fully used in these models. However, these attributes show great potential in the related tasks such as fashion image modeling, style prediction and so on.\n\nWe propose a framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts mentioned above. One characteristic is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history via a novel neural framework. It facilitates better predictions based on the information from all previous turns within the concerned session. Another unique component is a comparative analysis module which establishes a relationship between the differential representation derived from the reference image and the candidate image, and the feedback texts contributing to a matching score for the candidate image. To leverage the fashion attribute information of candidate images, a mutual attention mechanism is designed containing both the attention from the candidate image to the feedback texts and the other way round. The former attention helps to obtain a flexible feedback text representation according to each candidate image fashion attribute, forming one matching vector for each fashion attribute. The later one aims to adjust the corresponding weights with respect to the matching vectors obtained in the first stage, which contribute to the final matching score.\n\nSince there is no existing suitable fashion product image dataset that can appropriately capture user search scenario in multiturn settings, we derive a large-scale dataset based on an existing dataset which supports conversational fashion image retrieval with a singleturn natural language feedback. We integrate multiple single-turn data followed by additional manual efforts on a scrutinizing and consistency verification process ensuring that a multiturn session can consistently capture a particular user's fashion product search need.\n\nTo sum up, the main contributions of this paper are listed as follows:\n\n\u2022 Our model interacts with the dialog history from previous turns via a novel neural framework. It also establishes a relationship between the differential representation derived from the reference image and candidate image, and the feedback text contributing to a matching score for the candidate image. \u2022 Fashion attribute information of candidate images is leveraged via a mutual attention consisting of attention from both candidate image to feedback texts of each turn and the other way round. \u2022 We derive a new conversational fashion product image retrieval dataset supporting multiturn settings from an existing dataset. \u2022 Our model outperforms all existing state-of-the-art methods in the experiment.\n\n\nRELATED WORK 2.1 Conversational Image Retrieval\n\nRecent developments in computer vision and natural language processing methods have led to the considerable interest in image retrieval related tasks including image captioning [32,36], visual question-answering (VQA) [2,6,8], cross-modal image retrieval [11,29,37]. Among these tasks, some research focuses on the topic of image retrieval with natural language feedback. Aiming at selecting the desired image according to natural language feedback, efforts are made by incorporating users' feedback to the reference image and retrieving the image having the highest similarity score. Performance on this task has been enhanced in many works. For instance, some methods use a predefined set of attribute values to facilitate product retrieval application [11]. Some seek to fuse the image & text features producing a more precise representation of the image-text pair ranging from simple techniques (e.g. concatenation, simple feed-forward networks) to advanced techniques such as conducting parameter hashing [29], using a composition classifier [3,41] or through residual connection [37]. Some methods improve the performance by adding more features such as textonly, image-only, attribute-only features, etc [22,34]. Notably, [44] gains quite good performance in single-turn conversational image retrieval by adding a correction module which takes the difference between the reference and target image embedding into account.\n\n\nFashion Search and Recommendation\n\nWhen making fashion search decisions, people usually show different preferences for product attributes (e.g. a dress with short sleeves and floral prints), which can correspond to the fashion attributes the target image contain (e.g. sleeves, prints, etc). Fashion recommendation task aims to choose the best-matched product from a large number of fashion products satisfying personalized demands. Along this line, some studies have been proposed to improve the performance of fashion recommendation. In order to model visual characters and user preferences, some methods utilize pretrained CNN to generate the image representation [13,14,28,38]. Some methods attempt to get a better understanding of products by leveraging aesthetics and style features [25,43]. While some methods manage to provide recommendations following the purpose of explaining the recommendation reason through intuitive fashion attribute semantic highlights in a personalized manner [15]. Analyzing fashion attributes is essential in fashion retrieval. Fashion attributes, which include texture, fabric, style, are used to drive the learning of retrieval representations [1,16,17]. Typically, fashion attributes extracted from the side information are always informative. In some works, attribute-guided learning is a key factor for retrieval accuracy improvement [39,40,42]. Relative attributes, first introduced by [30], can be seen of a supplement of user feedback. Following this concept, a system named 'Whit-tleSearch' is proposed for fashion image retrieval [21]. When a user states a query, the system calculates the relative strength of attributes to provide the retrieval result. [19] leverages attributes for guiding relevance feedback information in image search. [20] aims to discover semantic visual attributes to assist downstream tasks such as image retrieval.\n\n\nOUR FRAMEWORK 3.1 Problem Definition\n\nAs described in the Introduction section, our goal is to find the most relevant image, called the target image, from a collection of candidate images satisfying the user's information need. Typically, each candidate image is associated with some fashion attributes. In each session, the user refines the retrieval result by providing natural language feedback texts. The initial retrieved image is treated as the first reference image, denoted as 1 , and the first natural language feedback text is denoted as 1 . Given a dialog context composed of turns of a session, represented as ( 1 , 1 , ..., , ) where denotes the -th reference image and represents the corresponding feedback texts. In each turn , the feedback texts describes the relative difference between the current reference image , and the desired image. The aim is to retrieve the target image via computing a matching score with each candidate image in the fashion dataset.\n\nThe training dataset consisting of sessions is denoted as = {[ , ] } =1 , where the reference information = ( 1 , 1 , ... , , ). The -th session contains turns and each turn is represented as ( , ) =1 where denotes a reference image and denotes a user feedback text. Figure 2 depicts the overview of our proposed framework, which consists of three modules, namely, composite representation module, comparative analysis module, and fashion attribute module. The composite representation module aims to extract and integrate the reference image and feedback text features of each turn and form a composite feature representation. Then it makes an association between the composite feature with the candidate image representation. Specifically, the image representation is extracted using residual networks. The natural language feedback text of each turn is embedded using pre-trained word embedding. The embedding of each text is then fed into a Transformer-based self-attention module. By making sentences attend to itself, dependencies of different level are captured. The image and text representation of each turn is composed into one representation and fed into a recurrent network following the turn order with a pooling layer. Finally a partial matching score for the candidate image, which can be treated as a matching score from the perspective of composite features, is obtained.\n\n\nFramework Overview\n\nThe comparative analysis module comes up with a differential representation in order to compare the difference between the reference image and the candidate image. The representation is acquired by feeding the reference image representation, the candidate image representation, as well as the difference of them into a fully connected layer. It then establishes a relationship between the differential representation and the feedback text by matching the representation with the feedback representation at each turn. After that, the matched vectors of each turn are recorded via a recurrent network with a pooling layer contributing to a partial matching score for the candidate image.\n\nThe fashion attribute module exploits the attribute information of the candidate image and calculates the mutual attention between candidate image and feedback texts. We first embed the fashion attribute texts using the same pre-trained word embedding method as above. Then a recurrent network is applied for the feedback text embedding at each turn. A mutual attention matching method is designed to handle the output of the pooling layer and the candidate image embedding. In the first attention stage, the feedback texts of all turns are embedded according to different fashion attributes forming one matching vector result for each attribute. In the second attention stage, the corresponding weights are learned with respect to the matching vectors obtained in the first stage. As a result, a partial matching score due to fashion attributes is obtained from the attentive sum of the weighted vectors.\n\nIn addition, in order to get a more precise text representation, we employ a self-attention mechanism. By making the textual feedback at each turn attend to itself, intra word-level dependencies are captured. This helps to learn hierarchical representations such as word-level, phrase-level as well as sentence-level representations.\n\n\nPreprocessing and Encoding\n\n3.3.1 Image Transformation. Before encoding the images into vectors, we first use some techniques to perform image transformation on product images. Image transformation aims to help make some desired information more obvious or explicit and augment the original data. These techniques include random horizontal flip, random rotation, random translation and random scale, which horizontally flips, rotates, translates, resizes the given image randomly with a given probability.\n\n\nImage Encoder.\n\nAfter the image transformation, we encode the image representation using ResNet-101 and ResNet-152. We share the parameters for image representation between reference and candidate images. The encoder embeds the -th image from a multiturn session into a vector representation = ( ) \u2208 R . The image encoder is trained from scratch without pretraining on any external data.\n\n\nSpell Checker.\n\nSince the dataset contains some misspelled words (e.g. stripe\u2192strip, colorful\u2192colorfull), we correct the spelling using the pyspellchecker 1 python package if the word cannot be found in the vocabulary. We further manually collect some commonly misspelled fashion-related words to ensure the probability of misspelling is minimized to the lowest. In the experiment, the collection contains 855 fashion-related misspelled words.\n\n\nText Encoder.\n\nTo encode the textual feedback of each turn, the texts are firstly tokenized, and the word embeddings are initialized 1 https://pypi.org/project/pyspellchecker/ with GloVe [31]. For example, the -th feedback of one session , after embedded into a vector, is represented as \u2208 R \u00d7 , where is the number of words in the sentence and is the embedding dimension. The word embeddings are then fed into a self-attention stack encoder. This stack has three inputs: the query sentence \u2208 R \u00d7 , the key sentence \u2208 R \u00d7 , and the value sentence \u2208 R \u00d7 . In our case, both of them are equal to the input embedding .\n\nSpecifically, the self-attention encoder is made up of several attention blocks. These blocks have the same structure and are stacked together. Each block takes the output of the former block as input. Inside the block, each word in the query sentence is attended to words in the key sentence via Scaled Dot-Product Attention. Then the block multiplies the results to the value input and calculates the weighted sum. Finally it adds the vector to the query sentence and feeds it into fully connected network [35].\n\nOur feedback text encoding result can be represented as the pooling result of the hierarchical self-attention stack. We use the average pooling, which takes the average of different granularity embedding, as the result.\n= ( ) = ( , ) =1 \u2208 R (1) , = ( , \u22121 , , \u22121 , , \u22121 )(2)\n,0 =\n\nwhere is the number of self-attention blocks, represents the encoding result of the feedback text , , is the embedding output vector at the -th attention block.\n\n\nComposite Representation Module\n\n3.4.1 Multimodal composer. At the -th turn, the multimodal composer composes the reference image embedding and the feedback text into a joint semantic representation . Here we use ComposeAE , which is an effective and state-of-the-art method for combining image and text through autoencoding [3]. It takes the image and text embeddings as input, and outputs the composed feature between them. This method shows promising result on image retrieval tasks where texts are used to express the difference between reference and target images. The composition process can be formulated as follows:\n= ( , )(4)\n\nMultiturn\n\nAnalyzer. The multiturn analyzer is used to aggregate the encoded multimodal representation with the conversation context history from previous turns. In order to better memorize the history information and make decisions based on all turns, a Gated Recurrent Unit (GRU) network with a pooling layer is employed [5]. Each time the composed feature is generated, it is then fed into the network following chronological order. We take the output of the pooling layer as the final representation of all multiturn reference image & text information. The forward formulas of the multiturn analyzer are:\n\u210e = (\u210e \u22121 , )(5)= 3 \u210e + 3(6)\nwhere \u210e is the hidden representation multimodal feature at the time step , is the final representation of all multiturn information, 3 is the parameter we wish to optimize.\n\n\nCandidate Generator.\n\nGiven the final representation of all multiturn reference information , we aim to search the target image from a set of candidate fashion images. Precisely, we calculate the cosine similarity between the reference embedding \u2208 R and the candidate image embedding \u2208 R , which can be obtained from the image encoder described in Section 3.3.2. The cosine similarity then serves as the partial matching score, denoted as 1 ( , ), derived from the composition representation module.\n\n\nComparative Analysis Module\n\n3.5.1 Differential Representation. Aiming at deriving the differential representation between encoded candidate and reference image information, this module first takes the difference between the reference image and candidate image as input. Let denote the differential representation, the representation is concatenated by three vectors:\n= ( [ \u2299 ; ]) \u2212 ([ \u2299 ; ]) (7) = ([ ; ; ])(8)\nwhere is the reference image representation at -th turn, is the candidate image representation, is a fully connected layer.\n\n\nText\n\nMatching. In order to detect the relationship between the differential representation mentioned above and the feedback text, at each turn, we match the differential representation with the feedback text representation via element-wise multiplication. The matched vectors of each turn are later fed into a GRU network:\n= * (9) \u210e = (\u210e \u22121 , )(10)\nwhere denotes the text encoding at -th turn, \u210e is the -th hidden layer in the GRU network.\n\nAfter that, an average pooling layer is employed to record the information of all GRU cells. Finally, the output of the pooling layer is fed into a fully connected layer with RELU activation. The output is served as the partial matching score 2 ( , ):\n2 ( , ) = ([h ] =1 )(11)\nwhere [h ] =1 is the average pooling output of all hidden layers, and (\u00b7) is a RELU activation function.\n\n\nFashion Attribute Module\n\n3.6.1 Image Fashion Attribute Embedding. Recall that each candidate image has five fashion attributes, namely, texture, fabric, shape, part, and style. Each fashion attribute consists of some words describing the corresponding aspect of it. To encode the candidate images, we also use GloVe (same as in Section 3.3) to embed each fashion attribute into a dimension vector. For a candidate image, the embedding is denoted as ( , , \u210e , , ). Each part of the embedding represents the embedding of one fashion attribute. It is worth noting that some fashion attributes may contain more than one words (e.g. part: long sleeve, button front). We first acquire the embedding of each word, then calculate the average of them as the final fashion attribute embedding.\n\n\nFeedback Encoder.\n\nThe feedback text is initiated using the GloVe embedding matrix. The embedding of the feedback text at the -th turn is denoted as as mentioned in Section 3.3.4. After that, each feedback is converted into a dimension vector.\n\nSimilar as mentioned in Section 3.4.2, the feedback text information of each turn is recorded chronologically. A bidirectional GRU network is leveraged and takes the embedding of feedback at each turn as input, which can be denoted as:\n\u210e = (\u210e \u22121 , \u210e +1 , )(12)\nwhere \u210e \u22121 is the forward encoded feedback text representation with the history from -1 previous turns, \u210e +1 is the backward encoded feedback text representation with the history from +1 previous turns, is the text representation at -th turn.\n\n\nCandidate-to-feedback Attention.\n\nThe key element of the fashion attribute module is the mutual attention mechanism. It is composed of mutual attention from the candidate image to the feedback text and the other way round.\n\nThe candidate-to-feedback attention mainly calculates the attentive embedding of the feedback text according to each candidate image attribute. When we wish to determine if a candidate image is suitable or not, we first take a look at one of its fashion attributes, fabric for example, then we reread the multiturn feedback to find out which part of the feedback dialog context should be more focused. Based on this strategy, each candidate fashion attribute should focus on different parts of the feedback dialog texts. Weights are introduced measuring the relevance between each image fashion attribute and the feedback text at each time step. The following formulas are designed for calculating the weights:\n= ( ) =1 ( ) (13) = ( 4 [ ; \u210e ] + 4 )(14)\nwhere denotes the attention weight from the -th turn feedback text hidden layer to the -th fashion attribute in the candidate image. Note that \u2208 { , , \u210e , , }. 4 , 4 are the parameters to optimize, (\u00b7) is an non-linear activation function.\n\nAccording to each image fashion attribute , the attention weights are then employed to calculate the weighted sum of the hidden representation in the feedback aspect. The detailed formula is shown as follows:\n= \u2211\ufe01 =1 \u210e (15)\nwhere is the attentive feedback representation according to the attribute .\n\nHaving the feedback representation , we can calculate the similarity between each attentive feedback text representation and candidate attribute representation :\n( , ) = ( , )(16)\n3.6.4 Feedback-to-candidate Attention. Intuitively, for the same feedback information, different values should be put to different image fashion attributes. That is, for the five similarity scores mentioned above, we compute the weight of each one to form the partial similarity score. This leads to the idea of feedback-to-candidate attention, which means that the weights are trained between feedback representation mentioned in the last section and the candidate attribute representation:\n3 ( , ) = \u2211\ufe01 ( , )(17)= ( ) \u2208 { , , \u210e , , } ( ) (18) = ( 5 [h ; ] + 5 )(19)\nwhereh is the average pooling result of the GRU multiturn analyzer hidden states.\n\nis the correlation weight of feedback-tocandidate aspect, indicating which candidate attribute should be more focused in the data.\n\n\nPartial Matching Score Combination\n\nGiven the three partial matching scores 1 ( , ), 2 ( , ), 3 ( , ) mentioned above, the final matching score is computed as the weighted sum of them:\n( , ) = 1 1 ( , )+ 2 2 ( , )+ 3 3 ( , )(20)\nwhere 1 , 2 , 3 are the parameters that need to be optimized.\n\n\nTraining\n\nDuring training, we use the max-margin triple loss as the loss function:\n= (0, \u2212 ( , \u2032 ) + ( , ))(21)\nwhere is a hyperparameter which records the gap between positive and negative examples.\n\nis the true positive target image while \u2032 is the false negative target image. It is worth noting that instead of building a negative sample of the target set, we use batch hard triplet loss. For each anchor, we get the hardest positive and negative to form a triplet and build the triplet loss over a batch of embeddings.\n\nWe train three modules separately and record the best score matrix of each module. Then, an iterative process is utilized to ensemble the modules. In the process, the previous best score matrix serves as the new candidate in the next iteration. We use hyperopt 2 Bayesian optimization to handle the process, which aims to find optimal weights between partial matching scores and maximize the overall score.\n\n\nEXPERIMENT 4.1 Dataset\n\nSince there is no existing suitable fashion product dataset which can appropriately capture user modeling in multiturn settings, we derive a large-scale multiturn fashion dataset based on the existing FashionIQ dataset which originally only supports single-turn feedback [10]. Every fashion product image in the dataset has five fashion attributes, namely, texture, fabric, shape, part, and style. Each fashion attribute includes some words describing the corresponding aspect of the image.\n\nThe original FashionIQ dataset contains single-turn sessions, and each session is represented by a triplet having the form of (reference image, feedback text, target image). Many reference and target images in FashionIQ dataset are highly relevant and have duplications. To derive multiturn sessions, we concatenate single-turn sessions in FashionIQ by matching the target image of one triplet with the reference image of another triplet in an automatic manner. For example, the original triplets (img1, txt1, img2), (img2, txt2, img3), (img3, txt3, img4) in FashionIQ dataset can be concatenated into a session having the form of (img1, txt1, img2, txt2, img3, txt3, img4). This process can derive a large number of multiturn sessions. A session with turns implies that the session has \u2212 1 feedback and images which is composed of \u2212 1 reference images and 1 target image.\n\nHowever, not all the sessions obtained are reasonable. Therefore, we manually select and filter out the problematic sessions which belong to several kinds of cases. The first kind refers to duplicates. For example, the current reference image is exactly the same as the next image. The second kind refers to inconsistency. For example, associated with a particular reference image, the feedback is 'white color'. Then a white colored reference image is retrieved. The user continues to write the feedback 'shorter sleeve'. The next reference image retrieved has shorter sleeves than the last one, but is not white colored, which is obviously unreasonable. The third kind refers to conflicts. For example, associated with the first image, Category Sessions  with 3  turns   Sessions  with 4  turns   Sessions  with 5  turns   Total  Sessions   Total  Images   Dress  3264  763  311  4338  5115  Shirt  2821  687  167  3675  4130  Toptee  2565  766  162  3493  4410   Total  8650  2216  640 11506 13655 Table 1: Detailed Information of Our Dataset the feedback is 'long sleeves'. After the second image is retrieved, the user gives another feedback 'sleeveless'. The last kind is circle. For example, the first reference image is followed by the feedback 'longer sleeves'. After the second reference image is retrieved, the user gives the feedback 'shorter sleeves', the system repeatedly retrieves the reference image same as the first one. Moreover, although a fashion attribute dataset is contained in the FashionIQ dataset, the size of the dataset is limited and cannot cover every target image. We expand the fashion attribute dataset to make sure every target image is associated with some fashion attributes following the same way as in [10]. Specifically, image fashion attributes were extracted from the product titles, the product summaries, and detailed product descriptions on product websites. As a result, we gather 11506 multiturn sessions. Table 1 shows the detailed information of our derived dataset. Furthermore, the multiturn sessions are grouped into three categories, namely, dress, toptee, and shirt. Each category includes sessions ranging from 3 to 5 turns. The average length of the feedback text in each turn is 5.02 words.\n\n\nExperimental Setting\n\nWe prepare 5 runs for the experiments. For each run, we randomly choose 70% multiturn sessions from the dataset as the training set and 30% as the testing set. We conduct our experiments using PyTorch. We use ResNet-152 as our image encoder without pretraining it on any external information. As for the text encoder, we use 3 stacks of self-attention blocks. By default, training is run for 150 epochs with a start learning rate 0.0001. We will release the code and the dataset to the public.\n\nWe use similar evaluation metrics as in previous works [10]. Each model outputs K best-matched products having the highest output score for the given session. Since we assume that each session in our dataset corresponds to one true positive image, which is the target image, we calculate the recall rate of the target image among the K selected ones as the main evaluation metric. The metric is denoted as recall at K (R@K), representing the proportion of target image found in the top-K retrieval results. In our experiment, K is selected to be 5 and 8. We also use MRR (mean reciprocal rank) as another evaluation metric in our experiment. It is the average of the multiplicative inverse of the rank of the target image in all retrieved images.\n\n\nComparison Models\n\nIn order to evaluate the effectiveness of our proposed model, we compare our model with several baselines and existing state-of-theart models. Since most of existing models are originally designed for single-turn setting. For fair comparison, we extend these models by adding a recurrent network to aggregate the encoded results with the dialog context from previous turns so that they can handle multiturn settings.\n\nText-Only. This model ignores the reference images and retrieves the target image according to user feedback only. The texts are encoded by a LSTM-GRU network and the images are encoded by ResNet-152.\n\nImage-Only. The image-only model only utilizes the image information in the reference image session. It utilizes the information of visual similarity between the information of the visual similarity between candidate and reference images.\n\nAttribute-Only. The attribute-only model treats every image as a combination of fashion attributes. Each fashion attribute is encoded by LSTM-GRU and is concatenated to form the image representation.\n\n\nTIRG.\n\nA recent method based on concatenation of visual and textual features with an additional gating connection to pass the image features directly to the learned joint feature space [37].\n\nRITC. It was first proposed by [34]. The residual text and image composer is a method that learns the residual between the features of target and reference images.\n\n\nComposeAE.\n\nA state-of-the art model proposed by [3]. It is an autoencoder based model which aims to learn the composition of image and text query for retrieving images.\n\nAttribute-aware User Simulator (AUS). It is a model where attribute features are incorporated to augment image representation [10].\n\nCorrection Network (CCNet). It is a model that finds the difference between reference and target images and checks its validity with a relative caption [44].\n\nDialog Manager. It was first proposed in [9], which is a framework that considers a user interacting with a retrieval agent via iterative dialog turns. The texts are encoded by a simple LSTM network, and the images are encoded by ResNet. It composes the image and text features by summing them up and feeds them into a memory network. Table 2 shows the experiment results of our model as well as all comparison models. Our Model, denoted as Ours, has gained the best R@5, R@8 and MRR score on the whole dataset as well as on all categories.\n\n\nExperiment Results\n\nGenerally for most of the models, the R@5, R@8 and MRR rates on the shirt category are higher than the other two categories. One reason is that the appearance of different shirts is less diverse, which assists the models in better capturing the difference between  \n\n\nFurther Analysis\n\nWe study how our model performs under different turn lengths. We also conduct similar investigation for three baselines. Recall that our dataset contains sessions ranging from 3 to 5 turns, Figure  4 shows the analysis on R@5 rate with respect to different turn lengths.\n\nFor attribute-only model, according to Figure 4(c), the longer the session is, the better the performance will be. The reason is that the fashion attribute information is insufficient in short sessions. While for text-only models, according to 4(d), the performance on 5-turn sessions is worse compared to the other two types. For the reason that on one hand, the number of 5-turn length sessions is much smaller than the other two types. On the other hand, without image information for reference, it is rather hard for the model to understand the target image the user wishes to retrieve, especially when it comes to long conversations. According to Figure 4(b), on all categories, the 4-turn sessions have the best performance in the ComposeAE model, while the 3-turn sessions have the worst. However, in our model, the performances on 4 and 5-turn sessions are similar. The 4 and 5-turn sessions have improved performance in the two models because they have richer information. Through interacting with the system, more hints are provided in the user feedback, which implies that if the session is too short, the information provided in each turn may not be rich enough for accurately retrieving the target image. However, for the ComposeAE model,  if the session is too long, it may give rise to the problem of information lost. Information of previous turns is easier to be forgotten in the former time steps. This problem gets tackled by the fashion attribute module in our model where the mutual attention between candidate image and feedback captures the information of each turn according to image attributes. The information buried in long sessions can be fully utilized by the model.\n\n\nCase Study\n\nWe collect some cases to analyze our retrieval results compared with a comparison model, namely Dialog Manager as shown in Figure 5. Five products are retrieved by our model and Dialog Manager, respectively. It can be observed that the retrieved image by our model can satisfy the user's need, which shows the effectiveness of the proposed model. Specifically, the desired target images in the first and third case are ranked the first among all candidate images, while they do not appear in the top-5 list in the comparison model result. Particularly, in the second case, the target image is ranked the third in our model while is ranked the ninth in the comparison model. Interestingly, general requirements such as 'blue colored','lighter' can be recognized by both models. However, detailed requirements such as 'has front bow', 'numbered slots', 'black spots' are challenging. One reason for the efficacy of our model is that it exploits the utilization of the fashion attribute module. Detailed information such as 'bow', 'number print', 'dots' can often be found clues in image fashion attributes, thus facilitating the interaction between target image and user feedback text. Another reason is the adoption of the self-attention feedback encoder. By making feedback text attend to itself, richer information of different granularities is learned. Compared with Dialog Manager, our model is also better at capturing the information from previous turns. In the first case, the feedback 'is shorter' seems to be forgotten in the comparison model result. The retrieved images do not have the 'shorter' aspect compared with the first target image. In the third case, 'sleeveless' is a very important clue in retrieving target images. However, not all the images retrieved by Dialog Manager top-5 list are sleeveless. One of the reasons that our model has better performance is that in our model, the fashion attribute module containing mutual attention is employed to connect the information from previous turns to the present task. Furthermore, due to the comparative analysis module, the retrieval results of our model is more similar to the reference images. In cases where there are many candidate images fulfilling the feedback requirements, such as the second case, choosing the one which is more similar to the reference image will produce the results with higher quality.\n\n\nCONCLUSIONS\n\nIn our paper, we investigate multiturn conversational fashion image retrieval with natural language feedback. A new framework is proposed which can effectively handle the task. Our model searches for target images based on the aggregation of the encoded reference image and text information with the conversation history via a novel neural framework. Moreover, it utilizes fashion attribute to improve the performance. We also derive a dataset suitable for multiturn conversational fashion image retrieval. Empirical results demonstrate the effectiveness of our model. The results show that our approach outperforms all the baselines and state-of-the-art models. In our future work, we wish to expand the generality of the approach on other fashion items having more wholistic fashion attributes and explore the usability of the work in real-world fashion domain applications.\n\n\nSIGIR '21, July 11-15, 2021, Virtual Event, Canada \u00a9 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8037-9/21/07. . . $15.00 https://doi.org/10.1145/3404835.3462881\n\nFigure 1 :\n1The top part is an example of three fashion product images. Each image has some fashion attributes. The bottom part is a sample session of multiturn conversational fashion image retrieval with natural language feedback.\n\nFigure 2 :\n2The overall architecture of our framework. It consists of three modules, namely, composite representation module, comparative analysis module, and fashion attribute module. The input of the framework is a conversation context composed of several turns of a session. The output is a matching score corresponding to the candidate image.\n\nFigure 3 :\n3The mutual attention between the feedback text and the candidate image.\n\nFigure 4 :\n4The average R@5 performance concerning different lengths of turns on different models.\n\nFigure 5 :\n5Some case studies.\n\n\nDialog Manager [9] 13.1 15.2 11.6 12.7 16.7 10.8 13.9 17.7 11.6 11.6 15.8 10.3Method \n\nOverall \nDress \nShirt \nToptee \nR@5 R@8 MRR R@5 R@8 MRR R@5 R@8 MRR R@5 R@8 MRR \n\nText-only \n7.8 \n12.3 \n6.9 \n7.4 \n10.8 \n5.4 \n7.5 \n12.5 \n6.2 \n8.2 \n13.4 \n6.3 \nImage-only \n10.7 14.5 \n6.9 \n10.2 15.0 \n6.3 \n9.3 \n15.7 \n5.5 \n9.1 \n13.9 \n7.6 \nAttribute-only \n9.8 \n12.8 \n7.7 \n11.1 13.9 \n8.2 \n9.7 \n11.5 \n6.8 \n8.9 \n11.8 \n6.3 \n\nTIRG [37] \n12.4 15.9 11.5 12.5 14.2 11.6 13.8 16.7 12.9 12.0 15.6 10.9 \nRITC [34] \n13.2 18.7 12.4 11.8 18.8 10.2 14.0 20.6 12.2 13.2 18.9 11.7 \nComposeAE [3] \n19.2 25.7 13.4 18.5 26.4 14.4 19.8 25.2 14.5 19.2 26.6 14.8 \nCCNet [44] \n13.5 15.5 12.2 12.7 17.2 10.5 15.2 18.5 13.3 13.6 16.2 12.1 \nAUS [10] \n11.3 16.2 \n9.2 \n13.4 15.3 10.5 14.7 16.6 11.3 12.4 13.3 10.6 \nOurs \n30.3 33.4 26.5 29.8 33.5 25.6 30.5 34.1 27.4 29.4 33.6 26.1 \nw/o CA \n25.5 29.2 21.3 24.1 28.4 18.5 25.6 28.6 22.1 24.5 25.2 21.7 \nw/o FA \n21.3 28.8 17.6 20.5 29.4 15.0 22.9 29.0 18.3 22.0 28.5 17.9 \n\n\n\nTable 2 :\n2Experimental results on Ours and some comparison methods. The metrics are in percentage. CA denotes the comparative analysis module. FA denotes the fashion attribute module. different products. Another reason is that the fashion attribute information of shirts is richer than other two categories. We also conduct the ablation study in our experiment. Without comparative analysis module, the performance of our model decreases by 4.8% in R@5, 4.2% in R@8, and 5.2% in MRR on average. This result reflects the difference between candidate and reference images can be exploited and reflected from the feedback text of each turn. Taken fashion attribute module into account, the performance increases by 9.0% in R@5, 4.6% in R@8, 8.9% in MRR on average. This verifies the effectiveness of fashion attributes for improving retrieval results. Compared with image-only and text-only models, the remaining models utilizing the multimodal feature composer have a better performance (Dialog Manager, RITC, TIRG, ComposeAE, CCNet, AUS), which verifies the necessity of composing the image & text at each turn into one representation. Compared with models without attention (Text-only, Image-only, Attribute-only, TIRG, ComposeAE, CCNet, AUS, Dialog Manager), attentive models outperform them on the whole, which demonstrates the superior power of attention mechanism in multiturn image retrieval.\nhttps://github.com/hyperopt/hyperopt\n\nFashion Forward: Forecasting Visual Style in Fashion. Ziad Al-Halah, Rainer Stiefelhagen, Kristen Grauman, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionZiad Al-Halah, Rainer Stiefelhagen, and Kristen Grauman. 2017. Fashion Forward: Forecasting Visual Style in Fashion. In Proceedings of the IEEE international conference on computer vision. 388-397.\n\nVqa: Visual Question Answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual Question Answering. In Proceedings of the IEEE international conference on computer vision. 2425-2433.\n\nCompositional Learning of Image-Text Query for Image Retrieval. Egor Muhammad Umer Anwaar, Martin Labintcev, Kleinsteuber, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionMuhammad Umer Anwaar, Egor Labintcev, and Martin Kleinsteuber. 2021. Com- positional Learning of Image-Text Query for Image Retrieval. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 1140-1149.\n\nDzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473Neural Machine Translation by Jointly Learning to Align and Translate. cs.CLDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2016. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv:1409.0473 [cs.CL]\n\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.3555Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv PreprintJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. arXiv Preprint arXiv:1412.3555 (2014).\n\nEmbodied Question Answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsAbhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2054-2063.\n\nImage Retrieval: Ideas, Influences, and Trends of the New Age. Ritendra Datta, Dhiraj Joshi, Jia Li, James Z Wang, Comput. Surveys. 40Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2008. Image Retrieval: Ideas, Influences, and Trends of the New Age. Comput. Surveys 40, 2 (2008), 1-60.\n\nMaking the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6904-6913.\n\nDialog-Based Interactive Image Retrieval. Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Feris, Advances in neural information processing systems. Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. 2018. Dialog-Based Interactive Image Retrieval. In Advances in neural information processing systems. 678-688.\n\nXiaoxiao Guo, Hui Wu, Yupeng Gao, Steven Rennie, Rogerio Feris, arXiv:1905.12794Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. arXiv PreprintXiaoxiao Guo, Hui Wu, Yupeng Gao, Steven Rennie, and Rogerio Feris. 2019. Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. arXiv Preprint arXiv:1905.12794 (2019).\n\nAutomatic Spatially-Aware Fashion Concept Discovery. Xintong Han, Zuxuan Wu, X Phoenix, Xiao Huang, Menglong Zhang, Yuan Zhu, Yang Li, Larry S Zhao, Davis, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry S Davis. 2017. Automatic Spatially-Aware Fashion Con- cept Discovery. In Proceedings of the IEEE International Conference on Computer Vision. 1463-1471.\n\nLearning Fashion Compatibility With Bidirectional LSTMS. Xintong Han, Zuxuan Wu, Yu-Gang Jiang, Larry S Davis, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaXintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S Davis. 2017. Learning Fashion Compatibility With Bidirectional LSTMS. In Proceedings of the 25th ACM international conference on Multimedia. 1078-1086.\n\nRuining He, Julian Mcauley, arXiv:1510.01784VBPR: Visual Bayesian Personalized Ranking From Implicit Feedback. arXiv PreprintRuining He and Julian McAuley. 2015. VBPR: Visual Bayesian Personalized Ranking From Implicit Feedback. arXiv Preprint arXiv:1510.01784 (2015).\n\nUps and Downs: Modeling the Visual Evolution of Fashion Trends With One-Class Collaborative Filtering. Ruining He, Julian Mcauley, proceedings of the 25th international conference on world wide web. the 25th international conference on world wide webRuining He and Julian McAuley. 2016. Ups and Downs: Modeling the Visual Evo- lution of Fashion Trends With One-Class Collaborative Filtering. In proceedings of the 25th international conference on world wide web. 507-517.\n\nExplainable Fashion Recommendation: A Semantic Attribute Region Guided Approach. Min Hou, Le Wu, Enhong Chen, Zhi Li, Vincent W Zheng, Qi Liu, arXiv:1905.12862cs.IRMin Hou, Le Wu, Enhong Chen, Zhi Li, Vincent W. Zheng, and Qi Liu. 2019. Explainable Fashion Recommendation: A Semantic Attribute Region Guided Approach. arXiv:1905.12862 [cs.IR]\n\nLearning the Latent \"Look\": Unsupervised Discovery of a Style-Coherent Embedding From Fashion Images. Wei-Lin Hsiao, Kristen Grauman, 2017 IEEE International Conference on Computer Vision (ICCV). IEEEWei-Lin Hsiao and Kristen Grauman. 2017. Learning the Latent \"Look\": Unsuper- vised Discovery of a Style-Coherent Embedding From Fashion Images. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 4213-4222.\n\nCreating Capsule Wardrobes From Fashion Images. Wei-Lin Hsiao, Kristen Grauman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWei-Lin Hsiao and Kristen Grauman. 2018. Creating Capsule Wardrobes From Fashion Images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7161-7170.\n\nCollaborative Fashion Recommendation: A Functional Tensor Factorization Approach. Yang Hu, Xi Yi, Larry S Davis, Proceedings of the 23rd ACM international conference on Multimedia. the 23rd ACM international conference on MultimediaYang Hu, Xi Yi, and Larry S Davis. 2015. Collaborative Fashion Recommendation: A Functional Tensor Factorization Approach. In Proceedings of the 23rd ACM international conference on Multimedia. 129-138.\n\nAttribute Pivots for Guiding Relevance Feedback in Image Search. Adriana Kovashka, Kristen Grauman, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAdriana Kovashka and Kristen Grauman. 2013. Attribute Pivots for Guiding Relevance Feedback in Image Search. In Proceedings of the IEEE International Conference on Computer Vision. 297-304.\n\nAttributes for Image Retrieval. Adriana Kovashka, Kristen Grauman, Visual Attributes. SpringerAdriana Kovashka and Kristen Grauman. 2017. Attributes for Image Retrieval. In Visual Attributes. Springer, 89-117.\n\nWhittlesearch: Image Search With Relative Attribute Feedback. Adriana Kovashka, Devi Parikh, Kristen Grauman, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEEAdriana Kovashka, Devi Parikh, and Kristen Grauman. 2012. Whittlesearch: Image Search With Relative Attribute Feedback. In 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2973-2980.\n\nDesignovel's System Description for Fashion-Iq Challenge. Jianri Li, Jae Whan Lee, arXiv:1910.11119Woo sang Song, Ki young Shin, and Byung hyun Go. cs.CVJianri Li, Jae whan Lee, Woo sang Song, Ki young Shin, and Byung hyun Go. 2019. Designovel's System Description for Fashion-Iq Challenge 2019. arXiv:1910.11119 [cs.CV]\n\nMining Fashion Outfit Composition Using an End-to-End Deep Learning Approach on Set Data. Yuncheng Li, Liangliang Cao, Jiang Zhu, Jiebo Luo, IEEE Transactions on Multimedia. 19Yuncheng Li, Liangliang Cao, Jiang Zhu, and Jiebo Luo. 2017. Mining Fashion Outfit Composition Using an End-to-End Deep Learning Approach on Set Data. IEEE Transactions on Multimedia 19, 8 (2017), 1946-1955.\n\nXiangnan He, Richang Hong, and Tat-seng Chua. Lizi Liao, Yunshan Ma, Proceedings of the 26th ACM international conference on Multimedia. the 26th ACM international conference on MultimediaKnowledge-Aware Multimodal Dialogue SystemsLizi Liao, Yunshan Ma, Xiangnan He, Richang Hong, and Tat-seng Chua. 2018. Knowledge-Aware Multimodal Dialogue Systems. In Proceedings of the 26th ACM international conference on Multimedia. 801-809.\n\nDeepStyle: Learning User Preferences for Visual Recommendation. Qiang Liu, Shu Wu, Liang Wang, Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information RetrievalQiang Liu, Shu Wu, and Liang Wang. 2017. DeepStyle: Learning User Preferences for Visual Recommendation. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 841-844.\n\nHi, Magic Closet, Tell Me What to Wear!. Si Liu, Jiashi Feng, Zheng Song, Tianzhu Zhang, Hanqing Lu, Changsheng Xu, Shuicheng Yan, Proceedings of the 20th ACM international conference on Multimedia. the 20th ACM international conference on MultimediaSi Liu, Jiashi Feng, Zheng Song, Tianzhu Zhang, Hanqing Lu, Changsheng Xu, and Shuicheng Yan. 2012. Hi, Magic Closet, Tell Me What to Wear!. In Proceedings of the 20th ACM international conference on Multimedia. 619-628.\n\nTowards Better Understanding the Clothing Fashion Styles: A Multimodal Deep Learning Approach. Yihui Ma, Jia Jia, Suping Zhou, Jingtian Fu, Yejun Liu, Zijian Tong, Thirty-First AAAI Conference on Artificial Intelligence. Yihui Ma, Jia Jia, Suping Zhou, Jingtian Fu, Yejun Liu, and Zijian Tong. 2017. Towards Better Understanding the Clothing Fashion Styles: A Multimodal Deep Learning Approach. In Thirty-First AAAI Conference on Artificial Intelligence.\n\nImage-Based Recommendations on Styles and Substitutes. Julian Mcauley, Christopher Targett, Qinfeng Shi, Anton Van Den, Hengel, Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. the 38th international ACM SIGIR conference on research and development in information retrievalJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 43-52.\n\nImage Question Answering Using Convolutional Neural Network With Dynamic Parameter Prediction. Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. 2016. Image Ques- tion Answering Using Convolutional Neural Network With Dynamic Parameter Prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition. 30-38.\n\nRelative Attributes. Devi Parikh, Kristen Grauman, 2011 International Conference on Computer Vision. IEEE. Devi Parikh and Kristen Grauman. 2011. Relative Attributes. In 2011 International Conference on Computer Vision. IEEE, 503-510.\n\nGlove: Global Vectors for Word Representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP. the 2014 conference on empirical methods in natural language processing (EMNLPJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1532-1543.\n\nSelf-Critical Sequence Training for Image Captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-Critical Sequence Training for Image Captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7008-7024.\n\nNegar Rostamzadeh, Seyedarian Hosseini, Thomas Boquet, Wojciech Stokowiec, Ying Zhang, Christian Jauvin, Chris Pal, arXiv:1806.08317Fashion-Gen: The Generative Fashion Dataset and Challenge. arXiv PreprintNegar Rostamzadeh, Seyedarian Hosseini, Thomas Boquet, Wojciech Stokowiec, Ying Zhang, Christian Jauvin, and Chris Pal. 2018. Fashion-Gen: The Generative Fashion Dataset and Challenge. arXiv Preprint arXiv:1806.08317 (2018).\n\nFashion-Iq 2020 Challenge 2nd Place Team's Solution. Minchul Shin, Yoonjae Cho, Seongwuk Hong, arXiv:2007.06404cs.CVMinchul Shin, Yoonjae Cho, and Seongwuk Hong. 2020. Fashion-Iq 2020 Chal- lenge 2nd Place Team's Solution. arXiv:2007.06404 [cs.CV]\n\nAttention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In Advances in neural information processing systems. 5998-6008.\n\nShow and Tell: A Neural Image Caption Generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and Tell: A Neural Image Caption Generator. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3156-3164.\n\nComposing Text and Image for Image Retrieval-an Empirical Odyssey. Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionNam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. 2019. Composing Text and Image for Image Retrieval-an Empirical Odyssey. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6439-6448.\n\nA Hierarchical Attention Model for Social Contextual Image Recommendation. Le Wu, Lei Chen, Richang Hong, Yanjie Fu, Xing Xie, Meng Wang, IEEE Transactions on Knowledge and Data Engineering. Le Wu, Lei Chen, Richang Hong, Yanjie Fu, Xing Xie, and Meng Wang. 2019. A Hierarchical Attention Model for Social Contextual Image Recommendation. IEEE Transactions on Knowledge and Data Engineering (2019).\n\nImage Captioning and Visual Question Answering Based on Attributes and External Knowledge. Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick, Anton Van Den, Hengel, IEEE Transactions on Pattern Analysis and Machine Intelligence. 40Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick, and Anton van den Hengel. 2017. Image Captioning and Visual Question Answering Based on Attributes and External Knowledge. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 6 (2017), 1367-1381.\n\nBoosting Image Captioning With Attributes. Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionTing Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. 2017. Boosting Image Captioning With Attributes. In Proceedings of the IEEE International Conference on Computer Vision. 4894-4902.\n\nRelevance Feedback: A Power Tool for Interactive Content-Based Image Retrieval. Yong Rui, T S Huang, M Ortega, S Mehrotra, IEEE Transactions on Circuits and Systems for Video Technology. 8Yong Rui, T. S. Huang, M. Ortega, and S. Mehrotra. 1998. Relevance Feedback: A Power Tool for Interactive Content-Based Image Retrieval. IEEE Transactions on Circuits and Systems for Video Technology 8, 5 (1998), 644-655.\n\nImage Captioning With Semantic Attention. Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. 2016. Image Captioning With Semantic Attention. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4651-4659.\n\nAesthetic-Based Clothing Recommendation. Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, Zheng Qin, Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceWenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, and Zheng Qin. 2018. Aesthetic-Based Clothing Recommendation. In Proceedings of the 2018 World Wide Web Conference. 649-658.\n\nYoungjae Yu, Seunghwan Lee, Yuncheol Choi, Gunhee Kim, arXiv:2003.12299CurlingNet: Compositional Learning Between Images and Text for Fashion IQ Data. cs.CVYoungjae Yu, Seunghwan Lee, Yuncheol Choi, and Gunhee Kim. 2020. CurlingNet: Compositional Learning Between Images and Text for Fashion IQ Data. arXiv:2003.12299 [cs.CV]\n\nText-Based Interactive Recommendation With Constraint-Augmented Reinforcement Learning. Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen, Lawrence Carin, Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen, and Lawrence Carin. 2019. Text-Based Interactive Recommendation With Constraint- Augmented Reinforcement Learning. (2019).\n\nRuiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen, Lawrence Carin, arXiv:2005.01618Reward Constrained Interactive Recommendation With Natural Language Feedback. arXiv PreprintRuiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen, and Lawrence Carin. 2020. Reward Constrained Interactive Recommendation With Natural Language Feedback. arXiv Preprint arXiv:2005.01618 (2020).\n", "annotations": {"author": "[{\"end\":226,\"start\":120},{\"end\":328,\"start\":227},{\"end\":413,\"start\":329},{\"end\":495,\"start\":414}]", "publisher": "[{\"end\":83,\"start\":80},{\"end\":712,\"start\":709}]", "author_last_name": "[{\"end\":130,\"start\":126},{\"end\":234,\"start\":231},{\"end\":339,\"start\":335},{\"end\":421,\"start\":418}]", "author_first_name": "[{\"end\":125,\"start\":120},{\"end\":230,\"start\":227},{\"end\":334,\"start\":329},{\"end\":417,\"start\":414}]", "author_affiliation": "[{\"end\":225,\"start\":154},{\"end\":327,\"start\":256},{\"end\":412,\"start\":341},{\"end\":494,\"start\":423}]", "title": "[{\"end\":79,\"start\":1},{\"end\":574,\"start\":496}]", "venue": "[{\"end\":582,\"start\":576}]", "abstract": "[{\"end\":2044,\"start\":760}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3114,\"start\":3110},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3117,\"start\":3114},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3154,\"start\":3150},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3157,\"start\":3154},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3160,\"start\":3157},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3163,\"start\":3160},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3211,\"start\":3208},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3214,\"start\":3211},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3217,\"start\":3214},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4824,\"start\":4821},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4827,\"start\":4824},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4830,\"start\":4827},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4833,\"start\":4830},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5168,\"start\":5164},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5435,\"start\":5432},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5463,\"start\":5459},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5469,\"start\":5465},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5626,\"start\":5623},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5789,\"start\":5785},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6319,\"start\":6316},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6340,\"start\":6336},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9477,\"start\":9473},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9480,\"start\":9477},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9517,\"start\":9514},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9519,\"start\":9517},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9521,\"start\":9519},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9555,\"start\":9551},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9558,\"start\":9555},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9561,\"start\":9558},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10055,\"start\":10051},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10310,\"start\":10306},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10346,\"start\":10343},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10349,\"start\":10346},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10385,\"start\":10381},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10511,\"start\":10507},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10514,\"start\":10511},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10529,\"start\":10525},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11398,\"start\":11394},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11401,\"start\":11398},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11404,\"start\":11401},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11407,\"start\":11404},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11520,\"start\":11516},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11523,\"start\":11520},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11725,\"start\":11721},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11912,\"start\":11909},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11915,\"start\":11912},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11918,\"start\":11915},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12106,\"start\":12102},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12109,\"start\":12106},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12112,\"start\":12109},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12159,\"start\":12155},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12307,\"start\":12303},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12432,\"start\":12428},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12518,\"start\":12514},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18472,\"start\":18468},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19410,\"start\":19406},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20185,\"start\":20182},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20819,\"start\":20816},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21265,\"start\":21264},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27288,\"start\":27287},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28718,\"start\":28714},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31555,\"start\":31551},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32636,\"start\":32632},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34596,\"start\":34592},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34634,\"start\":34630},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34817,\"start\":34814},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35066,\"start\":35062},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35225,\"start\":35221},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35272,\"start\":35269}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41515,\"start\":41334},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41748,\"start\":41516},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42096,\"start\":41749},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42181,\"start\":42097},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42281,\"start\":42182},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42313,\"start\":42282},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43287,\"start\":42314},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44687,\"start\":43288}]", "paragraph": "[{\"end\":2431,\"start\":2060},{\"end\":4562,\"start\":2433},{\"end\":5005,\"start\":4564},{\"end\":6177,\"start\":5007},{\"end\":6607,\"start\":6179},{\"end\":7920,\"start\":6609},{\"end\":8462,\"start\":7922},{\"end\":8534,\"start\":8464},{\"end\":9244,\"start\":8536},{\"end\":10724,\"start\":9296},{\"end\":12614,\"start\":10762},{\"end\":13594,\"start\":12655},{\"end\":14984,\"start\":13596},{\"end\":15692,\"start\":15007},{\"end\":16599,\"start\":15694},{\"end\":16934,\"start\":16601},{\"end\":17442,\"start\":16965},{\"end\":17832,\"start\":17461},{\"end\":18278,\"start\":17851},{\"end\":18896,\"start\":18296},{\"end\":19411,\"start\":18898},{\"end\":19632,\"start\":19413},{\"end\":19692,\"start\":19688},{\"end\":19854,\"start\":19694},{\"end\":20480,\"start\":19890},{\"end\":21101,\"start\":20504},{\"end\":21303,\"start\":21131},{\"end\":21805,\"start\":21328},{\"end\":22175,\"start\":21837},{\"end\":22343,\"start\":22220},{\"end\":22669,\"start\":22352},{\"end\":22786,\"start\":22696},{\"end\":23039,\"start\":22788},{\"end\":23169,\"start\":23065},{\"end\":23956,\"start\":23198},{\"end\":24202,\"start\":23978},{\"end\":24439,\"start\":24204},{\"end\":24707,\"start\":24465},{\"end\":24932,\"start\":24744},{\"end\":25644,\"start\":24934},{\"end\":25926,\"start\":25687},{\"end\":26136,\"start\":25928},{\"end\":26227,\"start\":26152},{\"end\":26390,\"start\":26229},{\"end\":26900,\"start\":26409},{\"end\":27058,\"start\":26977},{\"end\":27190,\"start\":27060},{\"end\":27377,\"start\":27229},{\"end\":27483,\"start\":27422},{\"end\":27568,\"start\":27496},{\"end\":27685,\"start\":27598},{\"end\":28008,\"start\":27687},{\"end\":28416,\"start\":28010},{\"end\":28933,\"start\":28443},{\"end\":29807,\"start\":28935},{\"end\":32057,\"start\":29809},{\"end\":32575,\"start\":32082},{\"end\":33323,\"start\":32577},{\"end\":33761,\"start\":33345},{\"end\":33963,\"start\":33763},{\"end\":34203,\"start\":33965},{\"end\":34404,\"start\":34205},{\"end\":34597,\"start\":34414},{\"end\":34762,\"start\":34599},{\"end\":34934,\"start\":34777},{\"end\":35067,\"start\":34936},{\"end\":35226,\"start\":35069},{\"end\":35768,\"start\":35228},{\"end\":36056,\"start\":35791},{\"end\":36347,\"start\":36077},{\"end\":38044,\"start\":36349},{\"end\":40441,\"start\":38059},{\"end\":41333,\"start\":40457}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19687,\"start\":19633},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20491,\"start\":20481},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21118,\"start\":21102},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21130,\"start\":21118},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22219,\"start\":22176},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22695,\"start\":22670},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23064,\"start\":23040},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24464,\"start\":24440},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25686,\"start\":25645},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26151,\"start\":26137},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26408,\"start\":26391},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26923,\"start\":26901},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26976,\"start\":26923},{\"attributes\":{\"id\":\"formula_14\"},\"end\":27421,\"start\":27378},{\"attributes\":{\"id\":\"formula_15\"},\"end\":27597,\"start\":27569}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30797,\"start\":30547},{\"end\":30817,\"start\":30810},{\"end\":31770,\"start\":31763},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35570,\"start\":35563}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2058,\"start\":2046},{\"attributes\":{\"n\":\"2\"},\"end\":9294,\"start\":9247},{\"attributes\":{\"n\":\"2.2\"},\"end\":10760,\"start\":10727},{\"attributes\":{\"n\":\"3\"},\"end\":12653,\"start\":12617},{\"attributes\":{\"n\":\"3.2\"},\"end\":15005,\"start\":14987},{\"attributes\":{\"n\":\"3.3\"},\"end\":16963,\"start\":16937},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":17459,\"start\":17445},{\"attributes\":{\"n\":\"3.3.3\"},\"end\":17849,\"start\":17835},{\"attributes\":{\"n\":\"3.3.4\"},\"end\":18294,\"start\":18281},{\"attributes\":{\"n\":\"3.4\"},\"end\":19888,\"start\":19857},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":20502,\"start\":20493},{\"attributes\":{\"n\":\"3.4.3\"},\"end\":21326,\"start\":21306},{\"attributes\":{\"n\":\"3.5\"},\"end\":21835,\"start\":21808},{\"attributes\":{\"n\":\"3.5.2\"},\"end\":22350,\"start\":22346},{\"attributes\":{\"n\":\"3.6\"},\"end\":23196,\"start\":23172},{\"attributes\":{\"n\":\"3.6.2\"},\"end\":23976,\"start\":23959},{\"attributes\":{\"n\":\"3.6.3\"},\"end\":24742,\"start\":24710},{\"attributes\":{\"n\":\"3.7\"},\"end\":27227,\"start\":27193},{\"attributes\":{\"n\":\"3.8\"},\"end\":27494,\"start\":27486},{\"attributes\":{\"n\":\"4\"},\"end\":28441,\"start\":28419},{\"attributes\":{\"n\":\"4.2\"},\"end\":32080,\"start\":32060},{\"attributes\":{\"n\":\"4.3\"},\"end\":33343,\"start\":33326},{\"end\":34412,\"start\":34407},{\"end\":34775,\"start\":34765},{\"attributes\":{\"n\":\"4.4\"},\"end\":35789,\"start\":35771},{\"attributes\":{\"n\":\"4.5\"},\"end\":36075,\"start\":36059},{\"attributes\":{\"n\":\"4.6\"},\"end\":38057,\"start\":38047},{\"attributes\":{\"n\":\"5\"},\"end\":40455,\"start\":40444},{\"end\":41527,\"start\":41517},{\"end\":41760,\"start\":41750},{\"end\":42108,\"start\":42098},{\"end\":42193,\"start\":42183},{\"end\":42293,\"start\":42283},{\"end\":43298,\"start\":43289}]", "table": "[{\"end\":43287,\"start\":42394}]", "figure_caption": "[{\"end\":41515,\"start\":41336},{\"end\":41748,\"start\":41529},{\"end\":42096,\"start\":41762},{\"end\":42181,\"start\":42110},{\"end\":42281,\"start\":42195},{\"end\":42313,\"start\":42295},{\"end\":42394,\"start\":42316},{\"end\":44687,\"start\":43300}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3792,\"start\":3784},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13871,\"start\":13863},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36276,\"start\":36267},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36396,\"start\":36388},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37009,\"start\":37001},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38190,\"start\":38182}]", "bib_author_first_name": "[{\"end\":44784,\"start\":44780},{\"end\":44801,\"start\":44795},{\"end\":44823,\"start\":44816},{\"end\":45194,\"start\":45185},{\"end\":45211,\"start\":45202},{\"end\":45227,\"start\":45221},{\"end\":45240,\"start\":45232},{\"end\":45256,\"start\":45251},{\"end\":45272,\"start\":45264},{\"end\":45286,\"start\":45282},{\"end\":45722,\"start\":45718},{\"end\":45751,\"start\":45745},{\"end\":46163,\"start\":46156},{\"end\":46183,\"start\":46174},{\"end\":46195,\"start\":46189},{\"end\":46457,\"start\":46449},{\"end\":46471,\"start\":46465},{\"end\":46491,\"start\":46482},{\"end\":46503,\"start\":46497},{\"end\":46847,\"start\":46839},{\"end\":46859,\"start\":46853},{\"end\":46874,\"start\":46867},{\"end\":46891,\"start\":46885},{\"end\":46901,\"start\":46897},{\"end\":46915,\"start\":46910},{\"end\":47382,\"start\":47374},{\"end\":47396,\"start\":47390},{\"end\":47407,\"start\":47404},{\"end\":47419,\"start\":47412},{\"end\":47711,\"start\":47707},{\"end\":47724,\"start\":47719},{\"end\":47738,\"start\":47731},{\"end\":47758,\"start\":47753},{\"end\":47770,\"start\":47766},{\"end\":48246,\"start\":48238},{\"end\":48255,\"start\":48252},{\"end\":48262,\"start\":48260},{\"end\":48276,\"start\":48270},{\"end\":48291,\"start\":48285},{\"end\":48308,\"start\":48301},{\"end\":48569,\"start\":48561},{\"end\":48578,\"start\":48575},{\"end\":48589,\"start\":48583},{\"end\":48601,\"start\":48595},{\"end\":48617,\"start\":48610},{\"end\":48994,\"start\":48987},{\"end\":49006,\"start\":49000},{\"end\":49012,\"start\":49011},{\"end\":49026,\"start\":49022},{\"end\":49042,\"start\":49034},{\"end\":49054,\"start\":49050},{\"end\":49064,\"start\":49060},{\"end\":49076,\"start\":49069},{\"end\":49526,\"start\":49519},{\"end\":49538,\"start\":49532},{\"end\":49550,\"start\":49543},{\"end\":49565,\"start\":49558},{\"end\":49903,\"start\":49896},{\"end\":49914,\"start\":49908},{\"end\":50276,\"start\":50269},{\"end\":50287,\"start\":50281},{\"end\":50723,\"start\":50720},{\"end\":50731,\"start\":50729},{\"end\":50742,\"start\":50736},{\"end\":50752,\"start\":50749},{\"end\":50764,\"start\":50757},{\"end\":50766,\"start\":50765},{\"end\":50776,\"start\":50774},{\"end\":51092,\"start\":51085},{\"end\":51107,\"start\":51100},{\"end\":51466,\"start\":51459},{\"end\":51481,\"start\":51474},{\"end\":51901,\"start\":51897},{\"end\":51908,\"start\":51906},{\"end\":51920,\"start\":51913},{\"end\":52323,\"start\":52316},{\"end\":52341,\"start\":52334},{\"end\":52702,\"start\":52695},{\"end\":52720,\"start\":52713},{\"end\":52943,\"start\":52936},{\"end\":52958,\"start\":52954},{\"end\":52974,\"start\":52967},{\"end\":53323,\"start\":53317},{\"end\":53331,\"start\":53328},{\"end\":53679,\"start\":53671},{\"end\":53694,\"start\":53684},{\"end\":53705,\"start\":53700},{\"end\":53716,\"start\":53711},{\"end\":54016,\"start\":54012},{\"end\":54030,\"start\":54023},{\"end\":54467,\"start\":54462},{\"end\":54476,\"start\":54473},{\"end\":54486,\"start\":54481},{\"end\":54976,\"start\":54974},{\"end\":54988,\"start\":54982},{\"end\":55000,\"start\":54995},{\"end\":55014,\"start\":55007},{\"end\":55029,\"start\":55022},{\"end\":55044,\"start\":55034},{\"end\":55058,\"start\":55049},{\"end\":55505,\"start\":55500},{\"end\":55513,\"start\":55510},{\"end\":55525,\"start\":55519},{\"end\":55540,\"start\":55532},{\"end\":55550,\"start\":55545},{\"end\":55562,\"start\":55556},{\"end\":55922,\"start\":55916},{\"end\":55943,\"start\":55932},{\"end\":55960,\"start\":55953},{\"end\":55971,\"start\":55966},{\"end\":56562,\"start\":56554},{\"end\":56572,\"start\":56568},{\"end\":56581,\"start\":56573},{\"end\":56594,\"start\":56587},{\"end\":57009,\"start\":57005},{\"end\":57025,\"start\":57018},{\"end\":57274,\"start\":57267},{\"end\":57294,\"start\":57287},{\"end\":57316,\"start\":57303},{\"end\":57781,\"start\":57780},{\"end\":57797,\"start\":57790},{\"end\":57813,\"start\":57806},{\"end\":57831,\"start\":57825},{\"end\":57848,\"start\":57840},{\"end\":58245,\"start\":58240},{\"end\":58269,\"start\":58259},{\"end\":58286,\"start\":58280},{\"end\":58303,\"start\":58295},{\"end\":58319,\"start\":58315},{\"end\":58336,\"start\":58327},{\"end\":58350,\"start\":58345},{\"end\":58731,\"start\":58724},{\"end\":58745,\"start\":58738},{\"end\":58759,\"start\":58751},{\"end\":58953,\"start\":58947},{\"end\":58967,\"start\":58963},{\"end\":58981,\"start\":58977},{\"end\":58995,\"start\":58990},{\"end\":59012,\"start\":59007},{\"end\":59025,\"start\":59020},{\"end\":59027,\"start\":59026},{\"end\":59041,\"start\":59035},{\"end\":59055,\"start\":59050},{\"end\":59397,\"start\":59392},{\"end\":59416,\"start\":59407},{\"end\":59429,\"start\":59425},{\"end\":59445,\"start\":59438},{\"end\":59878,\"start\":59875},{\"end\":59885,\"start\":59883},{\"end\":59897,\"start\":59893},{\"end\":59908,\"start\":59903},{\"end\":59923,\"start\":59917},{\"end\":59930,\"start\":59928},{\"end\":59945,\"start\":59940},{\"end\":60418,\"start\":60416},{\"end\":60426,\"start\":60423},{\"end\":60440,\"start\":60433},{\"end\":60453,\"start\":60447},{\"end\":60462,\"start\":60458},{\"end\":60472,\"start\":60468},{\"end\":60834,\"start\":60832},{\"end\":60846,\"start\":60839},{\"end\":60857,\"start\":60853},{\"end\":60871,\"start\":60864},{\"end\":60883,\"start\":60878},{\"end\":61272,\"start\":61268},{\"end\":61285,\"start\":61278},{\"end\":61296,\"start\":61291},{\"end\":61308,\"start\":61301},{\"end\":61317,\"start\":61314},{\"end\":61720,\"start\":61716},{\"end\":61727,\"start\":61726},{\"end\":61729,\"start\":61728},{\"end\":61738,\"start\":61737},{\"end\":61748,\"start\":61747},{\"end\":62097,\"start\":62089},{\"end\":62109,\"start\":62103},{\"end\":62122,\"start\":62115},{\"end\":62133,\"start\":62129},{\"end\":62145,\"start\":62140},{\"end\":62547,\"start\":62541},{\"end\":62557,\"start\":62552},{\"end\":62573,\"start\":62565},{\"end\":62580,\"start\":62578},{\"end\":62589,\"start\":62587},{\"end\":62602,\"start\":62597},{\"end\":62883,\"start\":62875},{\"end\":62897,\"start\":62888},{\"end\":62911,\"start\":62903},{\"end\":62924,\"start\":62918},{\"end\":63295,\"start\":63290},{\"end\":63307,\"start\":63303},{\"end\":63317,\"start\":63312},{\"end\":63331,\"start\":63324},{\"end\":63345,\"start\":63337},{\"end\":63360,\"start\":63352},{\"end\":63559,\"start\":63554},{\"end\":63571,\"start\":63567},{\"end\":63581,\"start\":63576},{\"end\":63595,\"start\":63588},{\"end\":63609,\"start\":63601},{\"end\":63624,\"start\":63616}]", "bib_author_last_name": "[{\"end\":44793,\"start\":44785},{\"end\":44814,\"start\":44802},{\"end\":44831,\"start\":44824},{\"end\":45200,\"start\":45195},{\"end\":45219,\"start\":45212},{\"end\":45230,\"start\":45228},{\"end\":45249,\"start\":45241},{\"end\":45262,\"start\":45257},{\"end\":45280,\"start\":45273},{\"end\":45293,\"start\":45287},{\"end\":45743,\"start\":45723},{\"end\":45761,\"start\":45752},{\"end\":45775,\"start\":45763},{\"end\":46172,\"start\":46164},{\"end\":46187,\"start\":46184},{\"end\":46202,\"start\":46196},{\"end\":46463,\"start\":46458},{\"end\":46480,\"start\":46472},{\"end\":46495,\"start\":46492},{\"end\":46510,\"start\":46504},{\"end\":46851,\"start\":46848},{\"end\":46865,\"start\":46860},{\"end\":46883,\"start\":46875},{\"end\":46895,\"start\":46892},{\"end\":46908,\"start\":46902},{\"end\":46921,\"start\":46916},{\"end\":47388,\"start\":47383},{\"end\":47402,\"start\":47397},{\"end\":47410,\"start\":47408},{\"end\":47424,\"start\":47420},{\"end\":47717,\"start\":47712},{\"end\":47729,\"start\":47725},{\"end\":47751,\"start\":47739},{\"end\":47764,\"start\":47759},{\"end\":47777,\"start\":47771},{\"end\":48250,\"start\":48247},{\"end\":48258,\"start\":48256},{\"end\":48268,\"start\":48263},{\"end\":48283,\"start\":48277},{\"end\":48299,\"start\":48292},{\"end\":48314,\"start\":48309},{\"end\":48573,\"start\":48570},{\"end\":48581,\"start\":48579},{\"end\":48593,\"start\":48590},{\"end\":48608,\"start\":48602},{\"end\":48623,\"start\":48618},{\"end\":48998,\"start\":48995},{\"end\":49009,\"start\":49007},{\"end\":49020,\"start\":49013},{\"end\":49032,\"start\":49027},{\"end\":49048,\"start\":49043},{\"end\":49058,\"start\":49055},{\"end\":49067,\"start\":49065},{\"end\":49081,\"start\":49077},{\"end\":49088,\"start\":49083},{\"end\":49530,\"start\":49527},{\"end\":49541,\"start\":49539},{\"end\":49556,\"start\":49551},{\"end\":49571,\"start\":49566},{\"end\":49906,\"start\":49904},{\"end\":49922,\"start\":49915},{\"end\":50279,\"start\":50277},{\"end\":50295,\"start\":50288},{\"end\":50727,\"start\":50724},{\"end\":50734,\"start\":50732},{\"end\":50747,\"start\":50743},{\"end\":50755,\"start\":50753},{\"end\":50772,\"start\":50767},{\"end\":50780,\"start\":50777},{\"end\":51098,\"start\":51093},{\"end\":51115,\"start\":51108},{\"end\":51472,\"start\":51467},{\"end\":51489,\"start\":51482},{\"end\":51904,\"start\":51902},{\"end\":51911,\"start\":51909},{\"end\":51926,\"start\":51921},{\"end\":52332,\"start\":52324},{\"end\":52349,\"start\":52342},{\"end\":52711,\"start\":52703},{\"end\":52728,\"start\":52721},{\"end\":52952,\"start\":52944},{\"end\":52965,\"start\":52959},{\"end\":52982,\"start\":52975},{\"end\":53326,\"start\":53324},{\"end\":53340,\"start\":53332},{\"end\":53682,\"start\":53680},{\"end\":53698,\"start\":53695},{\"end\":53709,\"start\":53706},{\"end\":53720,\"start\":53717},{\"end\":54021,\"start\":54017},{\"end\":54033,\"start\":54031},{\"end\":54471,\"start\":54468},{\"end\":54479,\"start\":54477},{\"end\":54491,\"start\":54487},{\"end\":54980,\"start\":54977},{\"end\":54993,\"start\":54989},{\"end\":55005,\"start\":55001},{\"end\":55020,\"start\":55015},{\"end\":55032,\"start\":55030},{\"end\":55047,\"start\":55045},{\"end\":55062,\"start\":55059},{\"end\":55508,\"start\":55506},{\"end\":55517,\"start\":55514},{\"end\":55530,\"start\":55526},{\"end\":55543,\"start\":55541},{\"end\":55554,\"start\":55551},{\"end\":55567,\"start\":55563},{\"end\":55930,\"start\":55923},{\"end\":55951,\"start\":55944},{\"end\":55964,\"start\":55961},{\"end\":55979,\"start\":55972},{\"end\":55987,\"start\":55981},{\"end\":56566,\"start\":56563},{\"end\":56585,\"start\":56582},{\"end\":56598,\"start\":56595},{\"end\":57016,\"start\":57010},{\"end\":57033,\"start\":57026},{\"end\":57285,\"start\":57275},{\"end\":57301,\"start\":57295},{\"end\":57324,\"start\":57317},{\"end\":57788,\"start\":57782},{\"end\":57804,\"start\":57798},{\"end\":57823,\"start\":57814},{\"end\":57838,\"start\":57832},{\"end\":57853,\"start\":57849},{\"end\":57859,\"start\":57855},{\"end\":58257,\"start\":58246},{\"end\":58278,\"start\":58270},{\"end\":58293,\"start\":58287},{\"end\":58313,\"start\":58304},{\"end\":58325,\"start\":58320},{\"end\":58343,\"start\":58337},{\"end\":58354,\"start\":58351},{\"end\":58736,\"start\":58732},{\"end\":58749,\"start\":58746},{\"end\":58764,\"start\":58760},{\"end\":58961,\"start\":58954},{\"end\":58975,\"start\":58968},{\"end\":58988,\"start\":58982},{\"end\":59005,\"start\":58996},{\"end\":59018,\"start\":59013},{\"end\":59033,\"start\":59028},{\"end\":59048,\"start\":59042},{\"end\":59066,\"start\":59056},{\"end\":59405,\"start\":59398},{\"end\":59423,\"start\":59417},{\"end\":59436,\"start\":59430},{\"end\":59451,\"start\":59446},{\"end\":59881,\"start\":59879},{\"end\":59891,\"start\":59886},{\"end\":59901,\"start\":59898},{\"end\":59915,\"start\":59909},{\"end\":59926,\"start\":59924},{\"end\":59938,\"start\":59931},{\"end\":59950,\"start\":59946},{\"end\":60421,\"start\":60419},{\"end\":60431,\"start\":60427},{\"end\":60445,\"start\":60441},{\"end\":60456,\"start\":60454},{\"end\":60466,\"start\":60463},{\"end\":60477,\"start\":60473},{\"end\":60837,\"start\":60835},{\"end\":60851,\"start\":60847},{\"end\":60862,\"start\":60858},{\"end\":60876,\"start\":60872},{\"end\":60891,\"start\":60884},{\"end\":60899,\"start\":60893},{\"end\":61276,\"start\":61273},{\"end\":61289,\"start\":61286},{\"end\":61299,\"start\":61297},{\"end\":61312,\"start\":61309},{\"end\":61321,\"start\":61318},{\"end\":61724,\"start\":61721},{\"end\":61735,\"start\":61730},{\"end\":61745,\"start\":61739},{\"end\":61757,\"start\":61749},{\"end\":62101,\"start\":62098},{\"end\":62113,\"start\":62110},{\"end\":62127,\"start\":62123},{\"end\":62138,\"start\":62134},{\"end\":62149,\"start\":62146},{\"end\":62550,\"start\":62548},{\"end\":62563,\"start\":62558},{\"end\":62576,\"start\":62574},{\"end\":62585,\"start\":62581},{\"end\":62595,\"start\":62590},{\"end\":62606,\"start\":62603},{\"end\":62886,\"start\":62884},{\"end\":62901,\"start\":62898},{\"end\":62916,\"start\":62912},{\"end\":62928,\"start\":62925},{\"end\":63301,\"start\":63296},{\"end\":63310,\"start\":63308},{\"end\":63322,\"start\":63318},{\"end\":63335,\"start\":63332},{\"end\":63350,\"start\":63346},{\"end\":63366,\"start\":63361},{\"end\":63565,\"start\":63560},{\"end\":63574,\"start\":63572},{\"end\":63586,\"start\":63582},{\"end\":63599,\"start\":63596},{\"end\":63614,\"start\":63610},{\"end\":63630,\"start\":63625}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7280528},\"end\":45151,\"start\":44726},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3180429},\"end\":45652,\"start\":45153},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":219955754},\"end\":46154,\"start\":45654},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b3\"},\"end\":46447,\"start\":46156},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b4\"},\"end\":46808,\"start\":46449},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":35985986},\"end\":47309,\"start\":46810},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7060187},\"end\":47605,\"start\":47311},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8081284},\"end\":48194,\"start\":47607},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13911587},\"end\":48559,\"start\":48196},{\"attributes\":{\"doi\":\"arXiv:1905.12794\",\"id\":\"b9\"},\"end\":48932,\"start\":48561},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3342857},\"end\":49460,\"start\":48934},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1537017},\"end\":49894,\"start\":49462},{\"attributes\":{\"doi\":\"arXiv:1510.01784\",\"id\":\"b12\"},\"end\":50164,\"start\":49896},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1964279},\"end\":50637,\"start\":50166},{\"attributes\":{\"doi\":\"arXiv:1905.12862\",\"id\":\"b14\"},\"end\":50981,\"start\":50639},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2214571},\"end\":51409,\"start\":50983},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4865281},\"end\":51813,\"start\":51411},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10031766},\"end\":52249,\"start\":51815},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7047321},\"end\":52661,\"start\":52251},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":63484879},\"end\":52872,\"start\":52663},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15277627},\"end\":53257,\"start\":52874},{\"attributes\":{\"doi\":\"arXiv:1910.11119\",\"id\":\"b21\"},\"end\":53579,\"start\":53259},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17203466},\"end\":53964,\"start\":53581},{\"attributes\":{\"id\":\"b23\"},\"end\":54396,\"start\":53966},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":35609905},\"end\":54931,\"start\":54398},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14989386},\"end\":55403,\"start\":54933},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10225808},\"end\":55859,\"start\":55405},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1012652},\"end\":56457,\"start\":55861},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14420812},\"end\":56982,\"start\":56459},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2633340},\"end\":57218,\"start\":56984},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1957433},\"end\":57724,\"start\":57220},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206594923},\"end\":58238,\"start\":57726},{\"attributes\":{\"doi\":\"arXiv:1806.08317\",\"id\":\"b32\"},\"end\":58669,\"start\":58240},{\"attributes\":{\"doi\":\"arXiv:2007.06404\",\"id\":\"b33\"},\"end\":58918,\"start\":58671},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13756489},\"end\":59341,\"start\":58920},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1169492},\"end\":59806,\"start\":59343},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":56173957},\"end\":60339,\"start\":59808},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":118683953},\"end\":60739,\"start\":60341},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":18548166},\"end\":61223,\"start\":60741},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1868294},\"end\":61634,\"start\":61225},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3888393},\"end\":62045,\"start\":61636},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3120635},\"end\":62498,\"start\":62047},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4531813},\"end\":62873,\"start\":62500},{\"attributes\":{\"doi\":\"arXiv:2003.12299\",\"id\":\"b43\"},\"end\":63200,\"start\":62875},{\"attributes\":{\"id\":\"b44\"},\"end\":63552,\"start\":63202},{\"attributes\":{\"doi\":\"arXiv:2005.01618\",\"id\":\"b45\"},\"end\":63945,\"start\":63554}]", "bib_title": "[{\"end\":44778,\"start\":44726},{\"end\":45183,\"start\":45153},{\"end\":45716,\"start\":45654},{\"end\":46837,\"start\":46810},{\"end\":47372,\"start\":47311},{\"end\":47705,\"start\":47607},{\"end\":48236,\"start\":48196},{\"end\":48985,\"start\":48934},{\"end\":49517,\"start\":49462},{\"end\":50267,\"start\":50166},{\"end\":51083,\"start\":50983},{\"end\":51457,\"start\":51411},{\"end\":51895,\"start\":51815},{\"end\":52314,\"start\":52251},{\"end\":52693,\"start\":52663},{\"end\":52934,\"start\":52874},{\"end\":53669,\"start\":53581},{\"end\":54010,\"start\":53966},{\"end\":54460,\"start\":54398},{\"end\":54972,\"start\":54933},{\"end\":55498,\"start\":55405},{\"end\":55914,\"start\":55861},{\"end\":56552,\"start\":56459},{\"end\":57003,\"start\":56984},{\"end\":57265,\"start\":57220},{\"end\":57778,\"start\":57726},{\"end\":58945,\"start\":58920},{\"end\":59390,\"start\":59343},{\"end\":59873,\"start\":59808},{\"end\":60414,\"start\":60341},{\"end\":60830,\"start\":60741},{\"end\":61266,\"start\":61225},{\"end\":61714,\"start\":61636},{\"end\":62087,\"start\":62047},{\"end\":62539,\"start\":62500}]", "bib_author": "[{\"end\":44795,\"start\":44780},{\"end\":44816,\"start\":44795},{\"end\":44833,\"start\":44816},{\"end\":45202,\"start\":45185},{\"end\":45221,\"start\":45202},{\"end\":45232,\"start\":45221},{\"end\":45251,\"start\":45232},{\"end\":45264,\"start\":45251},{\"end\":45282,\"start\":45264},{\"end\":45295,\"start\":45282},{\"end\":45745,\"start\":45718},{\"end\":45763,\"start\":45745},{\"end\":45777,\"start\":45763},{\"end\":46174,\"start\":46156},{\"end\":46189,\"start\":46174},{\"end\":46204,\"start\":46189},{\"end\":46465,\"start\":46449},{\"end\":46482,\"start\":46465},{\"end\":46497,\"start\":46482},{\"end\":46512,\"start\":46497},{\"end\":46853,\"start\":46839},{\"end\":46867,\"start\":46853},{\"end\":46885,\"start\":46867},{\"end\":46897,\"start\":46885},{\"end\":46910,\"start\":46897},{\"end\":46923,\"start\":46910},{\"end\":47390,\"start\":47374},{\"end\":47404,\"start\":47390},{\"end\":47412,\"start\":47404},{\"end\":47426,\"start\":47412},{\"end\":47719,\"start\":47707},{\"end\":47731,\"start\":47719},{\"end\":47753,\"start\":47731},{\"end\":47766,\"start\":47753},{\"end\":47779,\"start\":47766},{\"end\":48252,\"start\":48238},{\"end\":48260,\"start\":48252},{\"end\":48270,\"start\":48260},{\"end\":48285,\"start\":48270},{\"end\":48301,\"start\":48285},{\"end\":48316,\"start\":48301},{\"end\":48575,\"start\":48561},{\"end\":48583,\"start\":48575},{\"end\":48595,\"start\":48583},{\"end\":48610,\"start\":48595},{\"end\":48625,\"start\":48610},{\"end\":49000,\"start\":48987},{\"end\":49011,\"start\":49000},{\"end\":49022,\"start\":49011},{\"end\":49034,\"start\":49022},{\"end\":49050,\"start\":49034},{\"end\":49060,\"start\":49050},{\"end\":49069,\"start\":49060},{\"end\":49083,\"start\":49069},{\"end\":49090,\"start\":49083},{\"end\":49532,\"start\":49519},{\"end\":49543,\"start\":49532},{\"end\":49558,\"start\":49543},{\"end\":49573,\"start\":49558},{\"end\":49908,\"start\":49896},{\"end\":49924,\"start\":49908},{\"end\":50281,\"start\":50269},{\"end\":50297,\"start\":50281},{\"end\":50729,\"start\":50720},{\"end\":50736,\"start\":50729},{\"end\":50749,\"start\":50736},{\"end\":50757,\"start\":50749},{\"end\":50774,\"start\":50757},{\"end\":50782,\"start\":50774},{\"end\":51100,\"start\":51085},{\"end\":51117,\"start\":51100},{\"end\":51474,\"start\":51459},{\"end\":51491,\"start\":51474},{\"end\":51906,\"start\":51897},{\"end\":51913,\"start\":51906},{\"end\":51928,\"start\":51913},{\"end\":52334,\"start\":52316},{\"end\":52351,\"start\":52334},{\"end\":52713,\"start\":52695},{\"end\":52730,\"start\":52713},{\"end\":52954,\"start\":52936},{\"end\":52967,\"start\":52954},{\"end\":52984,\"start\":52967},{\"end\":53328,\"start\":53317},{\"end\":53342,\"start\":53328},{\"end\":53684,\"start\":53671},{\"end\":53700,\"start\":53684},{\"end\":53711,\"start\":53700},{\"end\":53722,\"start\":53711},{\"end\":54023,\"start\":54012},{\"end\":54035,\"start\":54023},{\"end\":54473,\"start\":54462},{\"end\":54481,\"start\":54473},{\"end\":54493,\"start\":54481},{\"end\":54982,\"start\":54974},{\"end\":54995,\"start\":54982},{\"end\":55007,\"start\":54995},{\"end\":55022,\"start\":55007},{\"end\":55034,\"start\":55022},{\"end\":55049,\"start\":55034},{\"end\":55064,\"start\":55049},{\"end\":55510,\"start\":55500},{\"end\":55519,\"start\":55510},{\"end\":55532,\"start\":55519},{\"end\":55545,\"start\":55532},{\"end\":55556,\"start\":55545},{\"end\":55569,\"start\":55556},{\"end\":55932,\"start\":55916},{\"end\":55953,\"start\":55932},{\"end\":55966,\"start\":55953},{\"end\":55981,\"start\":55966},{\"end\":55989,\"start\":55981},{\"end\":56568,\"start\":56554},{\"end\":56587,\"start\":56568},{\"end\":56600,\"start\":56587},{\"end\":57018,\"start\":57005},{\"end\":57035,\"start\":57018},{\"end\":57287,\"start\":57267},{\"end\":57303,\"start\":57287},{\"end\":57326,\"start\":57303},{\"end\":57790,\"start\":57780},{\"end\":57806,\"start\":57790},{\"end\":57825,\"start\":57806},{\"end\":57840,\"start\":57825},{\"end\":57855,\"start\":57840},{\"end\":57861,\"start\":57855},{\"end\":58259,\"start\":58240},{\"end\":58280,\"start\":58259},{\"end\":58295,\"start\":58280},{\"end\":58315,\"start\":58295},{\"end\":58327,\"start\":58315},{\"end\":58345,\"start\":58327},{\"end\":58356,\"start\":58345},{\"end\":58738,\"start\":58724},{\"end\":58751,\"start\":58738},{\"end\":58766,\"start\":58751},{\"end\":58963,\"start\":58947},{\"end\":58977,\"start\":58963},{\"end\":58990,\"start\":58977},{\"end\":59007,\"start\":58990},{\"end\":59020,\"start\":59007},{\"end\":59035,\"start\":59020},{\"end\":59050,\"start\":59035},{\"end\":59068,\"start\":59050},{\"end\":59407,\"start\":59392},{\"end\":59425,\"start\":59407},{\"end\":59438,\"start\":59425},{\"end\":59453,\"start\":59438},{\"end\":59883,\"start\":59875},{\"end\":59893,\"start\":59883},{\"end\":59903,\"start\":59893},{\"end\":59917,\"start\":59903},{\"end\":59928,\"start\":59917},{\"end\":59940,\"start\":59928},{\"end\":59952,\"start\":59940},{\"end\":60423,\"start\":60416},{\"end\":60433,\"start\":60423},{\"end\":60447,\"start\":60433},{\"end\":60458,\"start\":60447},{\"end\":60468,\"start\":60458},{\"end\":60479,\"start\":60468},{\"end\":60839,\"start\":60832},{\"end\":60853,\"start\":60839},{\"end\":60864,\"start\":60853},{\"end\":60878,\"start\":60864},{\"end\":60893,\"start\":60878},{\"end\":60901,\"start\":60893},{\"end\":61278,\"start\":61268},{\"end\":61291,\"start\":61278},{\"end\":61301,\"start\":61291},{\"end\":61314,\"start\":61301},{\"end\":61323,\"start\":61314},{\"end\":61726,\"start\":61716},{\"end\":61737,\"start\":61726},{\"end\":61747,\"start\":61737},{\"end\":61759,\"start\":61747},{\"end\":62103,\"start\":62089},{\"end\":62115,\"start\":62103},{\"end\":62129,\"start\":62115},{\"end\":62140,\"start\":62129},{\"end\":62151,\"start\":62140},{\"end\":62552,\"start\":62541},{\"end\":62565,\"start\":62552},{\"end\":62578,\"start\":62565},{\"end\":62587,\"start\":62578},{\"end\":62597,\"start\":62587},{\"end\":62608,\"start\":62597},{\"end\":62888,\"start\":62875},{\"end\":62903,\"start\":62888},{\"end\":62918,\"start\":62903},{\"end\":62930,\"start\":62918},{\"end\":63303,\"start\":63290},{\"end\":63312,\"start\":63303},{\"end\":63324,\"start\":63312},{\"end\":63337,\"start\":63324},{\"end\":63352,\"start\":63337},{\"end\":63368,\"start\":63352},{\"end\":63567,\"start\":63554},{\"end\":63576,\"start\":63567},{\"end\":63588,\"start\":63576},{\"end\":63601,\"start\":63588},{\"end\":63616,\"start\":63601},{\"end\":63632,\"start\":63616}]", "bib_venue": "[{\"end\":44900,\"start\":44833},{\"end\":45362,\"start\":45295},{\"end\":45857,\"start\":45777},{\"end\":46288,\"start\":46219},{\"end\":46603,\"start\":46527},{\"end\":47010,\"start\":46923},{\"end\":47441,\"start\":47426},{\"end\":47856,\"start\":47779},{\"end\":48365,\"start\":48316},{\"end\":48721,\"start\":48641},{\"end\":49157,\"start\":49090},{\"end\":49639,\"start\":49573},{\"end\":50005,\"start\":49940},{\"end\":50363,\"start\":50297},{\"end\":50718,\"start\":50639},{\"end\":51177,\"start\":51117},{\"end\":51568,\"start\":51491},{\"end\":51994,\"start\":51928},{\"end\":52418,\"start\":52351},{\"end\":52747,\"start\":52730},{\"end\":53047,\"start\":52984},{\"end\":53315,\"start\":53259},{\"end\":53753,\"start\":53722},{\"end\":54101,\"start\":54035},{\"end\":54604,\"start\":54493},{\"end\":55130,\"start\":55064},{\"end\":55624,\"start\":55569},{\"end\":56100,\"start\":55989},{\"end\":56677,\"start\":56600},{\"end\":57089,\"start\":57035},{\"end\":57419,\"start\":57326},{\"end\":57938,\"start\":57861},{\"end\":58429,\"start\":58372},{\"end\":58722,\"start\":58671},{\"end\":59117,\"start\":59068},{\"end\":59530,\"start\":59453},{\"end\":60029,\"start\":59952},{\"end\":60530,\"start\":60479},{\"end\":60963,\"start\":60901},{\"end\":61390,\"start\":61323},{\"end\":61821,\"start\":61759},{\"end\":62228,\"start\":62151},{\"end\":62657,\"start\":62608},{\"end\":63024,\"start\":62946},{\"end\":63288,\"start\":63202},{\"end\":63724,\"start\":63648},{\"end\":44954,\"start\":44902},{\"end\":45416,\"start\":45364},{\"end\":45924,\"start\":45859},{\"end\":47084,\"start\":47012},{\"end\":47920,\"start\":47858},{\"end\":49211,\"start\":49159},{\"end\":49692,\"start\":49641},{\"end\":50416,\"start\":50365},{\"end\":51632,\"start\":51570},{\"end\":52047,\"start\":51996},{\"end\":52472,\"start\":52420},{\"end\":54154,\"start\":54103},{\"end\":54702,\"start\":54606},{\"end\":55183,\"start\":55132},{\"end\":56198,\"start\":56102},{\"end\":56741,\"start\":56679},{\"end\":57499,\"start\":57421},{\"end\":58002,\"start\":57940},{\"end\":59594,\"start\":59532},{\"end\":60093,\"start\":60031},{\"end\":61444,\"start\":61392},{\"end\":62292,\"start\":62230},{\"end\":62693,\"start\":62659}]"}}}, "year": 2023, "month": 12, "day": 17}