{"id": 238207913, "updated": "2023-01-01 21:32:45.977", "metadata": {"title": "Recognizing human violent action using drone surveillance within real-time proximity", "authors": "[{\"first\":\"Anugrah\",\"last\":\"Srivastava\",\"middle\":[]},{\"first\":\"Tapas\",\"last\":\"Badal\",\"middle\":[]},{\"first\":\"Apar\",\"last\":\"Garg\",\"middle\":[]},{\"first\":\"Ankit\",\"last\":\"Vidyarthi\",\"middle\":[]},{\"first\":\"Rishav\",\"last\":\"Singh\",\"middle\":[]}]", "venue": "Journal of Real-Time Image Processing", "journal": "Journal of Real-Time Image Processing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Nowadays, the world is witnessing a significant rise in the cases of both reported and unnoticed violations. As an answer to this rising menace, video surveillance can fill the gap of covering untapped actions which lead to violence, while also ensuring a secure life. In our everyday life, surveillance can be accomplished efficiently by activity classification from drone videos. The prominent fields that have employed this technology are police work, video categorization, biometrics, and human\u2013computer interaction. So far, no public dataset is available for violent activity classification using drone surveillance. Hence, this work aims to look into the domain of machine-driven recognition and classification of human actions from drone videos. In this study, the dataset is created using drones from different heights for an unconstrained environment. The study begins by performing key-point extraction and generate 2D skeletons for the persons in the frame. These extracted key points are given as features in the classification module to recognize the actions. The classification models used in the proposed method are SVM (support vector machine) and Random Forest. Experimental results show that the SVM model with RBF (radial basis function) kernel for activity classification is more efficient when compared to the prior proposed approaches and other experimented models. The research work has also analyzed the run time performance of the proposed system and achieve its real-time performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jrtip/SrivastavaBGVS21", "doi": "10.1007/s11554-021-01171-2"}}, "content": {"source": {"pdf_hash": "71a658e0cef804a4192d2cc016621247700f1081", "pdf_src": "SpringerNature", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2e887209d5f0e5b7d7bb2cea04c0eb198d8fe6a7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/71a658e0cef804a4192d2cc016621247700f1081.txt", "contents": "\nRecognizing human violent action using drone surveillance within real-time proximity\n2021\n\nAnugrah Srivastava \n\u00b7 Tapas Badal \n\u00b7 Apar Garg \nAnkit Vidyarthi \n\u00b7 Rishav Singh \nRecognizing human violent action using drone surveillance within real-time proximity\n\n(0123456789) 1 3 Journal of Real-Time Image Processing\n18202110.1007/s11554-021-01171-2Received: 31 January 2021 / Accepted: 27 August 2021 / Published online: 12 September 2021SPECIAL ISSUE PAPERVideo surveillance \u00b7 Unconstrained environment \u00b7 Drone videos \u00b7 Key-point extraction \u00b7 Activity classification\nNowadays, the world is witnessing a significant rise in the cases of both reported and unnoticed violations. As an answer to this rising menace, video surveillance can fill the gap of covering untapped actions which lead to violence, while also ensuring a secure life. In our everyday life, surveillance can be accomplished efficiently by activity classification from drone videos. The prominent fields that have employed this technology are police work, video categorization, biometrics, and human-computer interaction. So far, no public dataset is available for violent activity classification using drone surveillance. Hence, this work aims to look into the domain of machine-driven recognition and classification of human actions from drone videos. In this study, the dataset is created using drones from different heights for an unconstrained environment. The study begins by performing key-point extraction and generate 2D skeletons for the persons in the frame. These extracted key points are given as features in the classification module to recognize the actions. The classification models used in the proposed method are SVM (support vector machine) and Random Forest. Experimental results show that the SVM model with RBF (radial basis function) kernel for activity classification is more efficient when compared to the prior proposed approaches and other experimented models. The research work has also analyzed the run time performance of the proposed system and achieve its real-time performance.\n\nIntroduction\n\nHuman action is about the real-world illustration of a person's behavior, intentions, and thoughts. Classifying human actions finds applications in various domains like video surveillance, behavior analysis, etc. The domain of this research work is related to surveillance systems. There are many unnoticed, suspicious/violent activities happening in a large crowd these days, and a better surveillance system is needed which can detect those activities and alert the responsible authorities. Static CCTV cameras are not efficient in such situations as their field of view is restricted. In such cases, a drone can hover over the area and can capture the video footage of the parts which are inaccessible to a static CCTV camera. Activity Recognition is an operative research area of computer vision and image processing. Convolution networks [12] have seen incredible achievement in image-based object detection and video-based activity classification [33,35].\n\nThe proposed research work has applications in several domains such as identifying criminal activities and in detecting individuals engaging in violent activities in public meetings or in other large crowds. Overall, it will be helpful in public safety systems and surveillance systems. It can also be used by event managers to manage large crowds during public events and check for any suspicious activity, thereby saving time and reducing crimes. Many security agencies have been persuaded to utilize unmanned aerial vehicles to keep an eye on extensive regions. Government officials or police can administer an aerial check in public places to capture violent activities such as chain riots, street fighting, etc. At the high end, this can be used by the army at the border to identify suspicious individuals. Governments have already started to deploy drones to monitor border activities [32] as well as finding crime in urban and provincial territories [13].\n\nIt is common to view anti-social elements in meetings and large gatherings. The rise in frequency of crime, vandalism, and terrorism in public areas has encouraged security agencies to develop sturdy video surveillance systems. Many researchers have made great efforts to develop video surveillance systems to detect squabbles [7], theft/robberies [4], abandoned objects (bags) in busy public areas [16], and fire [26].\n\nThe need for violent action classification is necessary, because if the machines are trained to automatically do this, then the violent activity can be identified immediately from surveillance cameras and corresponding action can be initiated within no time, so that security can be improved and simplified. However, accurate recognition of violent actions is a complex task. The circumstances under which the video was captured should be taken into consideration, the environment, and viewpoint variations. Another issue is that the human body is articulated to support advanced movements with several degrees of freedom. Therefore, making a comprehensive model that represents exploitation-proof movements from pictures is difficult, since it poses a retardant in an exceedingly high-dimensional area. This paper, therefore, attempts to introduce a human pose-based machine-driven model for a drone surveillance system to identify different violent actions. Many researchers have developed surveillance models which are mainly focused on ground-level object detection with fixed cameras where the surveillance cameras have a restricted field of view. Consequently, the area these systems can monitor is limited. Hence, there is a need for an aerial surveillance system for the surveillance of large areas.\n\nThe available datasets of aerial images are not relevant for pose estimation and action recognition as datasets have been taken from very far heights. The Hockey dataset [21] is a benchmarks dataset for violent activity recognition, but contains abrupt camera motion in recording non-fight scenes with only two classes namely fight and non-fight. The dataset is relatively uniform both in format and content. Similarly, the work [21] which introduced the movie dataset is explicitly designed for assessing fight detection. UCF101 Action Recognition Data Set [30] contains the diversity in terms of actions, but does not include violent activities like kicking, fall, throwing, strangling, pushing, SOS. Therefore, the dataset is created for an unconstrained environment. The created dataset consists of eight violent activities including punching, kicking, fall, strangling, throwing, swinging pipe, pushing, and SOS. Fall and SOS are not violent activities, but they are an indication of violence. These activities are performed by single/multiple persons in varied views taken from different heights. Figure 1 shows the sample images from the dataset collected. In the first step, the pose-estimation algorithm [2] is applied to proposed video data sequences to extract the human key points in the frame. Due to the overlapping or invisibility of some human joints, some key-point coordinates in frames are missed. The missing key points are handled, so that the performance of the model can be improved, and then, the frames are selected accordingly for classification purposes. Next, input to the classification network is the key-point coordinates to recognize the different violent actions. The overview of the proposed method is shown in Fig. 2.\n\nThe contribution of this paper is summarized in the following points: 1. A noteworthy challenge of violent activity recognition from aerial images for an unconstrained environment is accomplished by creating our dataset which is taken using a drone from different heights in diversified views. 2. The missing body key-point problem is caused due to invisibility and overlapping of human joints during the work of key-point extraction. The efficiency of classification can be affected by the missing key points in the frames. Therefore, the missing values are handled by replacing them with the corresponding non-negative values from the previous frame. The frames containing more than five missing key points are eliminated from the CSV file. 3. The dataset annotation that is done in this work consists of eight challenging violent activities performed by single/multiple persons.\n\nThe rest of the paper is assembled as follows: in Sect. 2, the related work regarding the recognition of human activity is discussed. In Sect. 3, the methodology and violent action recognition algorithm proposed is introduced briefly. In Sect. 4, the experimental results are discussed. Finally, in Sect. 5, the paper is concluded with the scope of the project.\n\n\nRelated work\n\nThis paper deals with the classification of violent actions for aerial data that are created. Classification of violent actions is essentially a task of activity recognition. Human activity recognition has seen rapid progress with the emergence of CNN-based models [9,22,33,35]. Manoj Ramanathan et al. [24] have discussed different approaches and challenging factors for action recognition based on video data. Many researchers have made great attempts for violence detection and fight detection. Goya et al. [8] developed a System for Public Safety to recognize illegal actions such as child abduction, purse snatching, and violence. To determine human behavior, their proposed system used three primary features (area, distance, and velocity). This system was capable of detecting criminal actions with approximately 85% accuracy. Particularly for violence detection, Deniz et al. [5] introduced a great technique using the Radon transform on the power spectrum of successive frames to identify violent sequences and extraordinary acceleration patterns, which was utilized as the primary feature. Additionally, a model proposed by Gracia et al. [27] used additional features derived from the motion blobs between consecutive frames for the classification of fight non-fight sequences. Comparable research like [7,10] has suggested a reliable and intuitive approach focused on the statistical motion characteristics of optical flow pictures. Furthermore, Zhang et al. [7] used a Gaussian optical flow model (GMOF) to isolate candidateviolent areas and classify areas of violence by linear SVM, where the orientation histogram of optical flow (OHOF) was used to generate vectors of input.\n\nDeep learning (DL) has been applied considerably over recent years in the analysis of images and video-based action. Zhou et al. [37] concluded that a video only shows and does not identify a violent incident. First of all, in their study, every video was framed as RGB pictures. The acceleration field was determined according to the optical flow field and the optical flow field was obtained by the video frames. Finally, with various input modalities, they trained the FightNet model. On the other hand, Kim et al. [11] used one of the deep learning architectures, a widely used convolutional neural networks for human activity recognition. The work in [17] used Bayes classifier and convolutional neural network to detect four activities, including walking, running, punching, and tripping. To recognize abusive human actions, Mumtaz et al. [20] implemented a deep representational model using the principle of transfer learning for the identification of violent scenes. This work suggested the transition of deep CNN learning for the identification of aggression in video streams. In [19], the study presented an activity recognition method based on posture cluster detection and was used to train a set of classifiers. Skeleton-based action recognition has become a widespread method and, in particular, makes use of graph convolutional networks (GCNs) and graph neural networks (GNNs), since they can constructively capture temporal and spatial information [25,36]. Spatial-temporal graph convolutional networks (STGCNs) model introduced by Yan et al. [34] processed spatial information in the body along with space by operating on bone connections using GCN. ST-GCN models despite working effectively on skeleton data have some limitations [3,18,28]. The path of information flow is predefined and graph links being directed, and the relevant extraction for the skeleton movements representation during time is prevented, since all layers and the actions have fixed graph representation topology. This, thereby, results in underestimation of body joints correlations irrespective of the action relevance.\n\nThe work in [6,14] has used LSTM and was successful at identifying the activity, describing the image and video. Li et al. [15] introduced ReHAR scheme for handling both single and group activities. In ReHAR, the LSTM was The paper [31] proposed an architecture based on CNN and LSTM for recognizing activity in surveillance videos recorded from industrial systems. A survey on public opinion regarding drones and their usage in surveillance and other fields was done by Aydin [1]. Surya et al. [23] suggested a visual surveillance device for the detection of suspicious behavior for autonomous unmanned air vehicles (UAV). The system was used to locate the top body parts (arms, head, and torso) of the person, where the primary stage was to detect and evaluate the aspect of the individual. According to them, the upper body provided ample information to identify a person's suspicious acts. Later, it performed human-pose estimation considering poor constraints on location, image parsing, and body appearance.\n\nThe human position was compared to the positions in the suspicious activity dataset and the activity that matched the best was flagged. With an accuracy of 76%, multiple suspicious acts such as punching, slapping, and so on were detected by the machine. To improve performance, a better autonomous drone surveillance device in real-time for detecting violent individuals in the public spaces was introduced in the research suggested in [29]. The proposed approach was to detect humans from aerial imagery using the feature pyramid network (FPN) and the pose for each human being detected was appraised using the network ScatterNet hybrid deep learning (SHDL). Eventually, the individuals engaging in the aggressive activity were identified by utilizing the orientations between the limbs of the projected pose by support vector machine(SVM).\n\nThe proposed video data are collected and captured by drones to actuate real-world data. Datasets contain eight violent activities (more than previously proposed works [29]). However, it is an extremely challenging task to recognize violent action as drone-captured videos can be affected by poor resolution, illumination changes, blurring, and scale variation, and conflicts can be generated as more acts occur. This paper also highlights data for classification on imbalanced violent action data. The proposed work first estimates the human pose using key points. Later, the missing key points are managed. In the last step, a multi-class classification for violent action recognition is performed.\n\n\nMethodology\n\nThis section presents the approach to detect violent actions using Drone surveillance in detail is proposed. Figure 4 shows the framework of the proposed model for violent action recognition and the section workflow is as follows:\n\n1. A detailed overview is discussed of the created dataset. 2. Next, human-pose estimation and techniques to handle the missing key points are discussed. 3. Finally, the classification of violent actions using extracted key points is presented.\n\n\nDataset\n\nThe dataset required for the proposed research work was created by initially recording videos of different violent activities, namely, punching, kicking, falling, strangling, throwing, swinging pipe, pushing, and SOS, from a height of 2-12 m using the drone, as shown in Table 1, However, for the paperwork, activities captured from a height of 2-8 Table 1 Details of sample image from each category in dataset used in this paper 1 3 m were considered. SOS signal detection capability is also an important feature of our work. Thus, when an incident of violence occurs, the victim or passerby can signal SOS by waving both his hands above his head for help. The dataset used in this paper was composed of video sequences of 610 s where each frame contained 1-10 humans involved in eight different violent activities. These violent activities were carried out by 32 subjects of both genders between 17 and 30 age groups with varying heights. The capturing angle varied with height and the subjects in the videos were captured from each and every view. The videos were captured from a drone (DJI Mavic Pro) which was placed at a height of 2-12 m from the ground, as shown in Fig. 3. The video resolution was 1080p at 60 fps. The information regarding the time in seconds of each activity and the number of extracted frames used after the extraction of key points is shown in Table 2.\n\nViolent activity recognition from aerial images is a very significant challenge, since many factors, such as the variation of the scale, shadows, positions, illumination, blurring, and poor resolution, can influence the data. Therefore, to account for this, data with people at different scales, locations, illumination, brightness, etc. are collected.\n\n\nKey-point extraction\n\nA multi-person 2D pose estimation for extracting human key points [2] was performed. This is a very efficient approach to detect multiple people's 2D poses from images or videos. This system uses video or pictures as input and generates  The framework first uses input video captured by drone to extract the human key points. The model extract 18 key points from the videos frame by frame. Next missing key points are handled and later the study has treated these processed 18 key points for the purpose of multi-class classification the 2D positions for each individual within the frame for the output. The approach uses a non-parametric representation to learn how to combine parts of the body with an individual in the frame. The architecture encodes the global context, facilitating a greedy bottom-up parsing method that can be carried out in high precision while achieving output in real time, despite the people's count in the frame. The method uses twobranch multistage CNN with the same sequential approach for learning the locations of components along with their association. The top branch predicts the set of body part locations in the form of confidence maps and the bottom branch is for affinity fields that encode the degree of association between these body parts. Before passing input to this two-branch network, the method uses auxiliary CNN to extract an input feature map and this prediction is processed by both branches [2]. This technique was effectively used to extract 18 key points including body, foot, hand, and facial key points from the videos frame by frame. The x, y coordinates of all key points were saved. The key-point coordinates of each frame were collected in a CSV file and labeled accordingly.\n\n\nHandling missing key points\n\nThe work of key-point extraction caused an issue of missing key points. The accuracy of the results can be affected if body key points, foot key points, hand key points, and some facial key points are missed. Due to the invisibility and overlapping of human joints, the pose-estimation algorithm was unable to detect some key points, and these missing key points were labeled using zero. These missing key points were handled by checking the key points value frame by frame. If any frame contained more than five missing key points, those frames were removed from the CSV file as they were insignificant for training and would lead to inefficiency. If the number of missing key points was from four to one in the frame, the values of those missing key points were replaced with the corresponding non-negative values in the previous frame as an approximation to the actual values.\n\nThus, we got an updated CSV with processed frames. The key points of these frames were further used for the classification of violent activities. The supercomputer Nvidia DGX V-100 is used for the complete processing of videos.\n\n\nActivity recognition\n\nTo attain a physical action, the person should interact and give feedback to the environment using his/her head, hands, arms, legs, and bodies. Different joints' positions express different actions. Hence, several joints movements of the human body can be considered as human activities. Therefore, the coordinates of extracted key point after handling the missing key points are collected as detailed in the previous section. These key points were further treated as features and fed into the classification model to recognize the different violent activities. The classification model SVM and Random Forest were chosen to predict the multi-class classification of eight classes-punching, kicking, fall, strangling, throwing, swinging pipe, pushing, and SOS. Moreover, the SVM model was trained with three different kernels, namely, RBF kernel, linear kernel, and poly kernel, to perform the violent activities classification. The models to recognize eight violent activities were successfully developed and trained on the feature vectors of the activities. After a comparison of the used classification models, it was found that the SVM model in combination with the RBF kernel was the most accurate model for classification, since it classified human violent activities efficiently. Table 3 represents the classification results of the models.\n\n\nExperimental results and analysis\n\nThe experimental details and the achievement of the classification model used for recognizing violent actions are discussed in this section. Section 4 describes the creation of our dataset using a drone. 610 s video sequences having a video resolution of 1080 p at 60 fps were created and experimented with the same dataset.\n\nThe 2-D pose estimation [2] is performed on our dataset to detect multiple people's 2D pose efficiently, as shown in Fig. 5. It was a real-time approach for extracting human key points regardless of the number of individuals in images or videos. The model extracted 18 human key points (x-y coordinates) from the videos frame by frame.\n\nThe 18 key points generated by the pose-estimation algorithm for each frame were saved to a CSV file and labeled. Due to overlapping and invisibility, the pose-estimation algorithm did not detect some of the human key points. The frames with more than five key points missing from the CSV file were removed, because they were not significant for training. In the frames which had less than five missing key points, the values of the key points were replaced with the corresponding non-negative values in the previous frame as an approximation to the actual values. These coordinates of key points were treated as features and further used in the classification model. This processing of videos was done on Nvidia DGX V-100 with specifications as follows: 8X NVIDIA Tesla V100 16 GB/GPU 40,960, 5,120 Tensor Cores, 4X 1.92 TB SSDs, 512 GB RAM, and 20 Core Intel Xeon E5-2698 v4 2.2 GHz. This paper's main objective was to classify violent actions, particularly taking into account the unbalanced data classes, as shown in Table 2. The 36 coordinates of 18 key points were considered as feature vectors for the classification model.\n\n\nModel analysis\n\nA support vector machine (SVM) with different kernels and random forest classifier was trained on the feature vector for each class of violent activity for 9086 randomly selected human poses (70%) to perform the multi-class classification.\n\nRandom Forest and SVM are intrinsically suited for multi-class problems. For multi-class problems, it has to be reduced into multiple binary classification problems. Random Forest and SVM work well with a mixture of numerical and categorical features. It works fine even when features are on various scales. For this experiment, a support vector machine with an RBF kernel was trained on a feature vector for eight violent classes to perform multi-class classification. The SVM parameter (c) was selected as 10, while the gamma parameter value was set to 0.0001 using fivefold cross-validation on the training set. The classification accuracy of SVM tested on our other 30% of the dataset is shown in Table 3. SVM with linear kernel and poly kernel was also trained on the same feature vector for multi-class classification and classification accuracy of the model, as shown in Table 3.\n\nIn Random Forest, the n-estimators parameter was selected as 40, min-samples-split as 10, min-samples-leaf as 3, max-features as 1, and max-depth as 10. The classification accuracy of Random Forest tested on our other 30% of the dataset is shown in Table 3. Table 4 shows the comparison of our action recognition approach with convolutional neural networks (CNN) on our dataset so as to conduct classification [11,17]. The proposed method has an end-to-end framework that provides feature extraction and classifiers. The table also provides information regarding the comparison of the proposed work with CNN combined with LSTM [15,31].\n\n\nExperiments and analysis\n\nTo demonstrate the classification report for each class, the following metrics have been used: Precision, Recall/Sensitivity, and F1 score is shown in Fig. 6. Additionally, the ROC curve of each violent class using different models is presented in Fig. 15. The performance of the proposed technique was analyzed using ROC curve. The AUC value of Random Forest model and SVM (with poly and RBF kernel) model for different violent classes ranged from 0.94 to 1.0 which proved that the proposed technique was performing well. ROC curves for Random Forest classifier and SVM classifier with poly, RBF kernel, and linear kernel for each activity class are shown in Figs. 7, 8, 9, 10, 11, 12, 13, and 14.\n\nThe superiority of the classification method was manifested using the following metrics: precision, sensitivity/ recall, specificity, F1 score, G-means, and accuracy of the used models. It was outlined as follows:\n(1) Precision = TP TP + FP ,(2)Sensitivity = TP TP + FN ,(3)Specificity = TN TN + FP (4) F1 score = 2 \u00d7 Recall \u00d7 Precision Recall + Precision ,(5)G-Mean = \u221a Sensitivity \u00d7 Specificity,\n\n3\n\nThe above-mentioned standard metrics are used in this paper to compare the performance of used classification models. \n\n\nRuntime analysis\n\nIn this research work, human key-point extraction was done and those values were considered as features. After that, during training, these features were passed to the classification models. Thus, the runtime of the proposed work consists of two major parts: (1) feature extraction time-the time taken for key-point extraction per frame and (2) classification   Table 5 shows the mean average runtime per frame with the standard deviation of the experimented proposed models. The time is shown in milliseconds. It includes the per frame runtime taken for feature extraction and classification.\n\n\nConclusion and future work\n\nAn efficient violent action recognition model which classifies eight different violent activities is presented in this paper. The task was complicated, because the drone-captured videos suffered irregularities like scale variation, poor resolution, brightness, and shadows. The framework initially used the pose estimation algorithm on video data captured by drones to extracts the key points. Next, the missing key points of each frame were handled. The updated key points of frames were used by classification models to recognize violent activities. In this paper, an aerial dataset of violent activities is also been created that can be utilized for other       surveillance-related applications. This paper presents an efficient autonomous drone surveillance system that can recognize violent actions in public space. Ultimately, the results show that the SVM in combination with RBF kernel yield the best performance with an accuracy of 97.741% and thereby, proving its supremacy over other experimented models. Also, the runtime analysis of the proposed work achieves its real-time performance. Furthermore, this paper   shows that the proposed method not only maintains accuracy but also preserves efficiency.\n\nIn the near future, the developed dataset can influence research in areas such as violence detection (that may include some non-violent actions) in humans of different heights and postures. This can thereby lead to a more efficient system that can be used for drone surveillance to detect activities in a crowd.\n\nFig. 1\n1Eight violent actions from the introduced dataset, namely (clockwise from top) (i) punching, (ii) kicking, (iii) fall, (iv) strangling, (v) throwing, (vi) swinging pipe, (vii) pushing, and (viii) SOS 1 3\n\nFig. 2\n2Overview of the proposed method used to predict the final activities based on the generated representations.\n\nFig. 3\n3Observed height of droneFig. 4 Framework of proposed model for violent action recognition.\n\n( 6 )\n6Accuracy = TP + TN TP + TN + FP + FN . Where TP and TN represent true-positive and true-negative values, and FP and FN are false-positive and false-negative values, respectively.\n\nFig. 5\n5Extracted human's key points along with presents the performance of the proposed system on our aerial dataset recorded using a drone (DJI Mavic Pro) which was placed at a height of 2-8 m from the ground. The illustration also presents the eight violent activities. Row 1 illustrates activities namely: fall, swinging pipe, throwing, and SOS (clockwise from top) having only one person. Row 2 illustrates activities, namely: punching, kicking, strangling, and pushing with two persons and row 3 illustrates multi-person involved in different violent activities\n\nFig. 6\n6It shows the classification report of each violent class using different models (clockwise from top): (i) SVM with linear kernel, (ii) Random forest, (iii) SVM with poly kernel, and (iv) SVM with RBF kernel 1 3\n\nFig. 7\n7ROC curves for different classifiers in comparison for activity fall\n\nFig. 8\n8ROC curves for different classifiers in comparison for activity kick\n\nFig. 9\n9ROC curves for different classifiers in comparison for activity throwing\n\nFig. 10\n10ROC curves for different classifiers in comparison for activity punch\n\nFig. 11\n11ROC curves for different classifiers in comparison for activity pushing\n\nFig. 12\n12ROC curves for different classifiers in comparison for activity SOS 1 3\n\nFig. 13\n13ROC curves for different classifiers in comparison for activity strangling\n\nFig. 14\n14ROC curves for different classifiers in comparison for activity swinging pipe\n\nFig. 15\n15ROC Curve of each violent class using different models (clockwise from top): (i) SVM with linear kernel, (ii) Random forest, (iii) SVM with poly kernel, and (iv) SVM with RBF kernel 1 3\n\nTable 2\n2Number of frames used in each sub-categoryAction \nTime in seconds \nUsed frames \n\nPunching \n75 \n1773 \nKicking \n71 \n1233 \nFall \n59 \n316 \nStrangling \n68 \n1405 \nThrowing \n90 \n2170 \nSwining Pipe \n101 \n2890 \nPushing \n63 \n1566 \nSOS \n83 \n1628 \n\n\n\nTable 3\n3Performance evaluation of proposed work with different classifiers time-the time taken by Classification models to classify action per frame. The runtime analysis is performed on Xeon(R) CPU E3-1240 v5, NVIDIA GeForce GTX-1080, and Nvidia DGX V-100. It is observed that classification of the action took very little time compared to key-point extraction per frame. Also, the runtime of the proposed model achieved its real-time performance.Methods \nPrecision \nSensitivity \nSpecificity \nF1 score \nG-Mean \nAccuracy \n\nRandom Forest \n0.95548 \n0.94582 \n0.99222 \n0.95005 \n0.96861 \n0.94763 \nSVM (with linear kernel) \n0.71892 \n0.70855 \n0.95465 \n0.71213 \n0.82080 \n0.69345 \nSVM (with poly kernel) \n0.91894 \n0.90937 \n0.98658 \n0.91691 \n0.95007 \n0.90937 \nSVM (with RBF kernel) \n0.98834 \n0.97336 \n0.99638 \n0.98028 \n0.98471 \n0.97741 \n\nTable 4 Comparison of \nthe human violent action \nrecognition of the proposed \nsystem against the CNN \ntechniques \n\nMethods \nAccuracy \n\nProposed method \n0.9774 \nCNN \n0.9170 \nCNN + LSTM \n0.9336 \n\nJournal of Real-Time Image Processing (2021) 18:1851-1863\nAcknowledgementsThe authors of the manuscript would like to thank\nPublic acceptance of drones: knowledge, attitudes, and practice. B Aydin, Technol. Soc. 59101180Aydin, B.: Public acceptance of drones: knowledge, attitudes, and practice. Technol. Soc. 59(101), 180 (2019)\n\nRealtime multi-person 2d pose estimation using part affinity fields. Z Cao, T Simon, S E Wei, Y Sheikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation using part affinity fields. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7291-7299 (2017)\n\nSkeleton-based action recognition with shift graph convolutional network. K Cheng, Y Zhang, X He, W Chen, J Cheng, H Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionCheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., Lu, H.: Skel- eton-based action recognition with shift graph convolutional net- work. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 183-192 (2020)\n\nCarried object detection using ratio histogram and its application to suspicious event analysis. C H Chuang, J W Hsieh, L W Tsai, S Y Chen, K C Fan, IEEE Trans. Circuits Syst. Video Technol. 196Chuang, C.H., Hsieh, J.W., Tsai, L.W., Chen, S.Y., Fan, K.C.: Carried object detection using ratio histogram and its application to suspicious event analysis. IEEE Trans. Circuits Syst. Video Technol. 19(6), 911-916 (2009)\n\nFast violence detection in video. O Deniz, I Serrano, G Bueno, T K Kim, 2014 International Conference on Computer Vision Theory and Applications (VISAPP). 2Deniz, O., Serrano, I., Bueno, G., Kim, T.K.: Fast violence detec- tion in video. In: 2014 International Conference on Computer Vision Theory and Applications (VISAPP), IEEE, vol. 2, pp. 478-485 (2014)\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, Anne Hendricks, L Guadarrama, S Rohrbach, M Venugopalan, S Saenko, K Darrell, T , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDonahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual recognition and description. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2625-2634 (2015)\n\nAutomatic fight detection in surveillance videos. E Y Fu, H V Leong, G Ngai, S C Chan, Proceedings of the 14th International Conference on Advances in Mobile Computing and Multi Media (MoMM '16). the 14th International Conference on Advances in Mobile Computing and Multi Media (MoMM '16)New York, NY, USAAssociation for Computing MachineryFu, E.Y., Leong, H.V., Ngai, G., Chan, S.C.: Automatic fight detection in surveillance videos. In: Proceedings of the 14th Inter- national Conference on Advances in Mobile Computing and Multi Media (MoMM '16). Association for Computing Machinery, New York, NY, USA, PP. 225-234 (2016)\n\nA method for automatic detection of crimes for public security by using motion analysis. K Goya, X Zhang, K Kitayama, I Nagayama, 2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing. IEEEGoya, K., Zhang, X., Kitayama, K., Nagayama, I.: A method for automatic detection of crimes for public security by using motion analysis. In: 2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing, IEEE, pp. 736-741 (2009)\n\nConvolutional neural networks for human activity recognition using multiple accelerometer and gyroscope sensors. S Ha, S Choi, 2016 International Joint Conference on Neural Networks (IJCNN). IEEEHa, S., Choi, S.: Convolutional neural networks for human activ- ity recognition using multiple accelerometer and gyroscope sen- sors. In: 2016 International Joint Conference on Neural Networks (IJCNN), IEEE, pp. 381-388 (2016)\n\n3d convolutional neural networks for human action recognition. S Ji, W Xu, M Yang, K Yu, IEEE Trans. Pattern Anal. Mach. Intell. 351Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action recognition. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 221-231 (2012)\n\nHuman activity recognition by using convolutional neural network. H Kim, S Lee, H Jung, Int. J. Electr. Comput. Eng. 965270Kim, H., Lee, S., Jung, H.: Human activity recognition by using convolutional neural network. Int. J. Electr. Comput. Eng. 9(6), 5270 (2019)\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. IEEE. 8611LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proc. IEEE 86(11), 2278-2324 (1998)\n\nCctv in the sky: police plan to use military-style spy drones. P Lewis, Guardian. 231Lewis, P.: Cctv in the sky: police plan to use military-style spy drones. Guardian 23, 1 (2010)\n\nSbgar: semantics based group activity recognition. X Li, M Choo Chuah, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionLi, X., Choo Chuah, M.: Sbgar: semantics based group activity recognition. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2876-2885 (2017)\n\nRehar: robust and efficient human activity recognition. X Li, M C Chuah, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEELi, X., Chuah, M.C.: Rehar: robust and efficient human activity recognition. In: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), IEEE, pp. 362-371 (2018)\n\nAbandoned objects detection using double illumination invariant foreground masks. X Li, C Zhang, D Zhang, 2010 20th International Conference on Pattern Recognition. IEEELi, X., Zhang, C., Zhang, D.: Abandoned objects detection using double illumination invariant foreground masks. In: 2010 20th International Conference on Pattern Recognition, IEEE, pp. 436- 439 (2010)\n\nTable 5 Runtime evaluation of proposed work with different classifiers running on a Xeon(R) CPU E3-1240 v5 @ 3.50 GHz, NVIDIA GeForce GTX-1080, and Nvidia DGX V-100. Table 5 Runtime evaluation of proposed work with different classifiers running on a Xeon(R) CPU E3-1240 v5 @ 3.50 GHz, NVIDIA GeForce GTX-1080, and Nvidia DGX V-100\n\n+ C) represents the mean average run time per frame for both feature extraction and classification. Here, ( F E Mean, Similarly, Mean (C) indi. Here, Mean (F.E. + C) represents the mean average run time per frame for both feature extraction and classification. Similarly, Mean (C) indi-\n\nAbnormal human activity recognition using Bayes classifier and convolutional neural network. C Liu, J Ying, F Han, M Ruan, IEEE 3rd International Conference on Signal and Image Processing. InIEEELiu, C., Ying, J., Han, F., Ruan, M.: Abnormal human activity rec- ognition using Bayes classifier and convolutional neural network. In: 2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP), IEEE, pp. 33-37 (2018)\n\nDisentangling and unifying graph convolutions for skeleton-based action recognition. Z Liu, H Zhang, Z Chen, Z Wang, W Ouyang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: Disentan- gling and unifying graph convolutions for skeleton-based action recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 143-152 (2020)\n\nTwoperson activity recognition using skeleton data. A Manzi, L Fiorini, R Limosani, P Dario, F Cavallo, IET Comput. Vis. 121Manzi, A., Fiorini, L., Limosani, R., Dario, P., Cavallo, F.: Two- person activity recognition using skeleton data. IET Comput. Vis. 12(1), 27-35 (2018)\n\nViolence detection in surveillance videos with deep network using transfer learning. A Mumtaz, A B Sargano, Z Habib, 2018 2nd European Conference on Electrical Engineering and Computer Science (EECS). IEEEMumtaz, A., Sargano, A.B., Habib, Z.: Violence detection in sur- veillance videos with deep network using transfer learning. In: 2018 2nd European Conference on Electrical Engineering and Computer Science (EECS), IEEE, pp. 558-563 (2018)\n\nViolence detection in video using computer vision techniques. E B Nievas, O D Suarez, G B Garc\u00eda, R Sukthankar, International Conference on Computer Analysis of Images and Patterns. SpringerNievas, E.B., Suarez, O.D., Garc\u00eda, G.B., Sukthankar, R.: Vio- lence detection in video using computer vision techniques. In: International Conference on Computer Analysis of Images and Patterns, pp. 332-339. Springer (2011)\n\nDeep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition. F J Ord\u00f3\u00f1ez, D Roggen, Sensors. 161115Ord\u00f3\u00f1ez, F.J., Roggen, D.: Deep convolutional and lstm recur- rent neural networks for multimodal wearable activity recognition. Sensors 16(1), 115 (2016)\n\nAutonomous uav for suspicious action detection using pictorial human pose estimation and classification. S Penmetsa, F Minhuj, A Singh, S Omkar, ELCVIA Electron. Lett. Comput. Vis. Image Anal. 131Penmetsa, S., Minhuj, F., Singh, A., Omkar, S.: Autonomous uav for suspicious action detection using pictorial human pose esti- mation and classification. ELCVIA Electron. Lett. Comput. Vis. Image Anal. 13(1), 0018-0032 (2014)\n\nHuman action recognition with video data: research and evaluation challenges. M Ramanathan, W Y Yau, E K Teoh, IEEE Trans. Hum. Mach. Syst. 445Ramanathan, M., Yau, W.Y., Teoh, E.K.: Human action recogni- tion with video data: research and evaluation challenges. IEEE Trans. Hum. Mach. Syst. 44(5), 650-663 (2014)\n\nA survey on 3d skeletonbased action recognition using learning method. B Ren, M Liu, R Ding, H Liu, arXiv:2002.05907Ren, B., Liu, M., Ding, R., Liu, H.: A survey on 3d skeleton- based action recognition using learning method (2020). arXiv: 2002. 05907\n\nFire detection in the buildings using image processing. J Seebamrungsat, S Praising, P Riyamongkol, 2014 Third ICT International Student Project Conference (ICT-ISPC). IEEESeebamrungsat, J., Praising, S., Riyamongkol, P.: Fire detection in the buildings using image processing. In: 2014 Third ICT Interna- tional Student Project Conference (ICT-ISPC), IEEE, pp. 95-98 (2014)\n\nFast fight detection. I Serrano Gracia, O Suarez, G Bueno Garcia, T K Kim, PLoS One. 104120448Serrano Gracia, I., Deniz Suarez, O., Bueno Garcia, G., Kim, T.K.: Fast fight detection. PLoS One 10(4), e0120448 (2015)\n\nSkeleton-based action recognition with directed graph neural networks. L Shi, Y Zhang, J Cheng, H Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShi, L., Zhang, Y., Cheng, J., Lu, H.: Skeleton-based action rec- ognition with directed graph neural networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7912-7921 (2019)\n\nEye in the sky: real-time drone surveillance system (dss) for violent individuals identification using scatternet hybrid deep learning network. A Singh, D Patil, S Omkar, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsSingh, A., Patil, D., Omkar, S.: Eye in the sky: real-time drone surveillance system (dss) for violent individuals identification using scatternet hybrid deep learning network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion Workshops, pp. 1629-1637 (2018)\n\nA dataset of 101 human action classes from videos in the wild. K Soomro, A R Zamir, M Shah, 2Center for Research in Computer VisionSoomro, K., Zamir, A.R., Shah, M.: A dataset of 101 human action classes from videos in the wild. Center for Research in Computer Vision 2(11) (2012)\n\nActivity recognition using temporal optical flow convolutional features and multilayer lstm. A Ullah, K Muhammad, J Del Ser, S W Baik, V H C De Albuquerque, IEEE Trans. Ind. Electron. 6612Ullah, A., Muhammad, K., Del Ser, J., Baik, S.W., de Albuquer- que, V.H.C.: Activity recognition using temporal optical flow con- volutional features and multilayer lstm. IEEE Trans. Ind. Electron. 66(12), 9692-9702 (2018)\n\nUcav surveillance, high-tech masculinities and oriental others. W Walters, J Weber, Presentation to A Global Surveillance Society. Walters, W., Weber, J.: Ucav surveillance, high-tech masculini- ties and oriental others. In: Presentation to A Global Surveillance Society (2010)\n\nTemporal segment networks: towards good practices for deep action recognition. L Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang, L Van Gool, European Conference on Computer Vision. SpringerWang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks: towards good prac- tices for deep action recognition. In: European Conference on Computer Vision, pp. 20-36. Springer (2016)\n\nSpatial temporal graph convolutional networks for skeleton-based action recognition. S Yan, Y Xiong, D Lin, Thirty-second AAAI Conference on Artificial Intelligence. Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based action recognition. In: Thirty-second AAAI Conference on Artificial Intelligence (2018)\n\nReal-time action recognition with enhanced motion vector cnns. B Zhang, L Wang, Z Wang, Y Qiao, H Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhang, B., Wang, L., Wang, Z., Qiao, Y., Wang, H.: Real-time action recognition with enhanced motion vector cnns. In: Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2718-2726 (2016)\n\nA comprehensive survey of vision-based human action recognition methods. H B Zhang, Y X Zhang, B Zhong, Q Lei, L Yang, J X Du, D S Chen, Sensors. 1951005Zhang, H.B., Zhang, Y.X., Zhong, B., Lei, Q., Yang, L., Du, J.X., Chen, D.S.: A comprehensive survey of vision-based human action recognition methods. Sensors 19(5), 1005 (2019)\n\nViolent interaction detection in video based on deep learning. P Zhou, Q Ding, H Luo, X Hou, Journal of Physics: Conference Series. 84412044IOP PublishingZhou, P., Ding, Q., Luo, H., Hou, X.: Violent interaction detection in video based on deep learning. In: Journal of Physics: Confer- ence Series, vol 844, p. 012044. IOP Publishing (2017)\n\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n", "annotations": {"author": "[{\"end\":111,\"start\":92},{\"end\":126,\"start\":112},{\"end\":139,\"start\":127},{\"end\":156,\"start\":140},{\"end\":172,\"start\":157}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":100},{\"end\":125,\"start\":114},{\"end\":138,\"start\":134},{\"end\":155,\"start\":146},{\"end\":171,\"start\":166}]", "author_first_name": "[{\"end\":99,\"start\":92},{\"end\":113,\"start\":112},{\"end\":128,\"start\":127},{\"end\":133,\"start\":129},{\"end\":145,\"start\":140},{\"end\":158,\"start\":157},{\"end\":165,\"start\":159}]", "author_affiliation": null, "title": "[{\"end\":85,\"start\":1},{\"end\":257,\"start\":173}]", "venue": "[{\"end\":313,\"start\":259}]", "abstract": "[{\"end\":2076,\"start\":566}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2939,\"start\":2935},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3049,\"start\":3045},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3052,\"start\":3049},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3951,\"start\":3947},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4017,\"start\":4013},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4350,\"start\":4347},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4371,\"start\":4368},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4423,\"start\":4419},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4438,\"start\":4434},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5924,\"start\":5920},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6183,\"start\":6179},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6312,\"start\":6308},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6966,\"start\":6963},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9033,\"start\":9030},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9036,\"start\":9033},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9039,\"start\":9036},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9042,\"start\":9039},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9072,\"start\":9068},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9278,\"start\":9275},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9652,\"start\":9649},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9917,\"start\":9913},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10081,\"start\":10078},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10084,\"start\":10081},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10238,\"start\":10235},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10589,\"start\":10585},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10978,\"start\":10974},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11116,\"start\":11112},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11305,\"start\":11301},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11549,\"start\":11545},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11924,\"start\":11920},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11927,\"start\":11924},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12019,\"start\":12015},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12207,\"start\":12204},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12210,\"start\":12207},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12213,\"start\":12210},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12585,\"start\":12582},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12588,\"start\":12585},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12697,\"start\":12693},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12806,\"start\":12802},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13050,\"start\":13047},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13069,\"start\":13065},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14025,\"start\":14021},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14600,\"start\":14596},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17461,\"start\":17458},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18838,\"start\":18835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22029,\"start\":22026},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25031,\"start\":25027},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25034,\"start\":25031},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25248,\"start\":25244},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25251,\"start\":25248},{\"end\":25978,\"start\":25947}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28888,\"start\":28676},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29006,\"start\":28889},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29106,\"start\":29007},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29293,\"start\":29107},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29862,\"start\":29294},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30082,\"start\":29863},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30160,\"start\":30083},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30238,\"start\":30161},{\"attributes\":{\"id\":\"fig_8\"},\"end\":30320,\"start\":30239},{\"attributes\":{\"id\":\"fig_9\"},\"end\":30401,\"start\":30321},{\"attributes\":{\"id\":\"fig_10\"},\"end\":30484,\"start\":30402},{\"attributes\":{\"id\":\"fig_11\"},\"end\":30567,\"start\":30485},{\"attributes\":{\"id\":\"fig_12\"},\"end\":30653,\"start\":30568},{\"attributes\":{\"id\":\"fig_13\"},\"end\":30742,\"start\":30654},{\"attributes\":{\"id\":\"fig_14\"},\"end\":30939,\"start\":30743},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31187,\"start\":30940},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32211,\"start\":31188}]", "paragraph": "[{\"end\":3053,\"start\":2092},{\"end\":4018,\"start\":3055},{\"end\":4439,\"start\":4020},{\"end\":5748,\"start\":4441},{\"end\":7502,\"start\":5750},{\"end\":8385,\"start\":7504},{\"end\":8748,\"start\":8387},{\"end\":10454,\"start\":8765},{\"end\":12568,\"start\":10456},{\"end\":13583,\"start\":12570},{\"end\":14426,\"start\":13585},{\"end\":15128,\"start\":14428},{\"end\":15374,\"start\":15144},{\"end\":15620,\"start\":15376},{\"end\":17013,\"start\":15632},{\"end\":17367,\"start\":17015},{\"end\":19127,\"start\":17392},{\"end\":20038,\"start\":19159},{\"end\":20267,\"start\":20040},{\"end\":21638,\"start\":20292},{\"end\":22000,\"start\":21676},{\"end\":22337,\"start\":22002},{\"end\":23469,\"start\":22339},{\"end\":23727,\"start\":23488},{\"end\":24615,\"start\":23729},{\"end\":25252,\"start\":24617},{\"end\":25979,\"start\":25281},{\"end\":26194,\"start\":25981},{\"end\":26501,\"start\":26383},{\"end\":27115,\"start\":26522},{\"end\":28362,\"start\":27146},{\"end\":28675,\"start\":28364}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":26226,\"start\":26195},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26255,\"start\":26226},{\"attributes\":{\"id\":\"formula_2\"},\"end\":26341,\"start\":26255},{\"attributes\":{\"id\":\"formula_3\"},\"end\":26378,\"start\":26341}]", "table_ref": "[{\"end\":15910,\"start\":15903},{\"end\":15988,\"start\":15981},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17012,\"start\":17005},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21585,\"start\":21578},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23367,\"start\":23360},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24437,\"start\":24430},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24614,\"start\":24607},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24873,\"start\":24866},{\"end\":24882,\"start\":24875},{\"end\":26891,\"start\":26884}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2090,\"start\":2078},{\"attributes\":{\"n\":\"2\"},\"end\":8763,\"start\":8751},{\"attributes\":{\"n\":\"3\"},\"end\":15142,\"start\":15131},{\"attributes\":{\"n\":\"3.1\"},\"end\":15630,\"start\":15623},{\"attributes\":{\"n\":\"3.2\"},\"end\":17390,\"start\":17370},{\"attributes\":{\"n\":\"3.3\"},\"end\":19157,\"start\":19130},{\"attributes\":{\"n\":\"3.4\"},\"end\":20290,\"start\":20270},{\"attributes\":{\"n\":\"4\"},\"end\":21674,\"start\":21641},{\"attributes\":{\"n\":\"4.1\"},\"end\":23486,\"start\":23472},{\"attributes\":{\"n\":\"4.2\"},\"end\":25279,\"start\":25255},{\"attributes\":{\"n\":\"1\"},\"end\":26381,\"start\":26380},{\"attributes\":{\"n\":\"4.3\"},\"end\":26520,\"start\":26504},{\"attributes\":{\"n\":\"5\"},\"end\":27144,\"start\":27118},{\"end\":28683,\"start\":28677},{\"end\":28896,\"start\":28890},{\"end\":29014,\"start\":29008},{\"end\":29113,\"start\":29108},{\"end\":29301,\"start\":29295},{\"end\":29870,\"start\":29864},{\"end\":30090,\"start\":30084},{\"end\":30168,\"start\":30162},{\"end\":30246,\"start\":30240},{\"end\":30329,\"start\":30322},{\"end\":30410,\"start\":30403},{\"end\":30493,\"start\":30486},{\"end\":30576,\"start\":30569},{\"end\":30662,\"start\":30655},{\"end\":30751,\"start\":30744},{\"end\":30948,\"start\":30941},{\"end\":31196,\"start\":31189}]", "table": "[{\"end\":31187,\"start\":30992},{\"end\":32211,\"start\":31638}]", "figure_caption": "[{\"end\":28888,\"start\":28685},{\"end\":29006,\"start\":28898},{\"end\":29106,\"start\":29016},{\"end\":29293,\"start\":29115},{\"end\":29862,\"start\":29303},{\"end\":30082,\"start\":29872},{\"end\":30160,\"start\":30092},{\"end\":30238,\"start\":30170},{\"end\":30320,\"start\":30248},{\"end\":30401,\"start\":30332},{\"end\":30484,\"start\":30413},{\"end\":30567,\"start\":30496},{\"end\":30653,\"start\":30579},{\"end\":30742,\"start\":30665},{\"end\":30939,\"start\":30754},{\"end\":30992,\"start\":30950},{\"end\":31638,\"start\":31198}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6861,\"start\":6853},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7501,\"start\":7495},{\"end\":15261,\"start\":15253},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16811,\"start\":16805},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22125,\"start\":22119},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25438,\"start\":25432},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25536,\"start\":25529}]", "bib_author_first_name": "[{\"end\":32402,\"start\":32401},{\"end\":32613,\"start\":32612},{\"end\":32620,\"start\":32619},{\"end\":32629,\"start\":32628},{\"end\":32631,\"start\":32630},{\"end\":32638,\"start\":32637},{\"end\":33080,\"start\":33079},{\"end\":33089,\"start\":33088},{\"end\":33098,\"start\":33097},{\"end\":33104,\"start\":33103},{\"end\":33112,\"start\":33111},{\"end\":33121,\"start\":33120},{\"end\":33617,\"start\":33616},{\"end\":33619,\"start\":33618},{\"end\":33629,\"start\":33628},{\"end\":33631,\"start\":33630},{\"end\":33640,\"start\":33639},{\"end\":33642,\"start\":33641},{\"end\":33650,\"start\":33649},{\"end\":33652,\"start\":33651},{\"end\":33660,\"start\":33659},{\"end\":33662,\"start\":33661},{\"end\":33972,\"start\":33971},{\"end\":33981,\"start\":33980},{\"end\":33992,\"start\":33991},{\"end\":34001,\"start\":34000},{\"end\":34003,\"start\":34002},{\"end\":34380,\"start\":34379},{\"end\":34394,\"start\":34390},{\"end\":34407,\"start\":34406},{\"end\":34421,\"start\":34420},{\"end\":34433,\"start\":34432},{\"end\":34448,\"start\":34447},{\"end\":34458,\"start\":34457},{\"end\":34469,\"start\":34468},{\"end\":34957,\"start\":34956},{\"end\":34959,\"start\":34958},{\"end\":34965,\"start\":34964},{\"end\":34967,\"start\":34966},{\"end\":34976,\"start\":34975},{\"end\":34984,\"start\":34983},{\"end\":34986,\"start\":34985},{\"end\":35622,\"start\":35621},{\"end\":35630,\"start\":35629},{\"end\":35639,\"start\":35638},{\"end\":35651,\"start\":35650},{\"end\":36156,\"start\":36155},{\"end\":36162,\"start\":36161},{\"end\":36530,\"start\":36529},{\"end\":36536,\"start\":36535},{\"end\":36542,\"start\":36541},{\"end\":36550,\"start\":36549},{\"end\":36825,\"start\":36824},{\"end\":36832,\"start\":36831},{\"end\":36839,\"start\":36838},{\"end\":37081,\"start\":37080},{\"end\":37090,\"start\":37089},{\"end\":37100,\"start\":37099},{\"end\":37110,\"start\":37109},{\"end\":37342,\"start\":37341},{\"end\":37512,\"start\":37511},{\"end\":37518,\"start\":37517},{\"end\":37879,\"start\":37878},{\"end\":37885,\"start\":37884},{\"end\":37887,\"start\":37886},{\"end\":38231,\"start\":38230},{\"end\":38237,\"start\":38236},{\"end\":38246,\"start\":38245},{\"end\":38958,\"start\":38957},{\"end\":38962,\"start\":38959},{\"end\":39233,\"start\":39232},{\"end\":39240,\"start\":39239},{\"end\":39248,\"start\":39247},{\"end\":39255,\"start\":39254},{\"end\":39660,\"start\":39659},{\"end\":39667,\"start\":39666},{\"end\":39676,\"start\":39675},{\"end\":39684,\"start\":39683},{\"end\":39692,\"start\":39691},{\"end\":40149,\"start\":40148},{\"end\":40158,\"start\":40157},{\"end\":40169,\"start\":40168},{\"end\":40181,\"start\":40180},{\"end\":40190,\"start\":40189},{\"end\":40460,\"start\":40459},{\"end\":40470,\"start\":40469},{\"end\":40472,\"start\":40471},{\"end\":40483,\"start\":40482},{\"end\":40881,\"start\":40880},{\"end\":40883,\"start\":40882},{\"end\":40893,\"start\":40892},{\"end\":40895,\"start\":40894},{\"end\":40905,\"start\":40904},{\"end\":40907,\"start\":40906},{\"end\":40917,\"start\":40916},{\"end\":41335,\"start\":41334},{\"end\":41337,\"start\":41336},{\"end\":41348,\"start\":41347},{\"end\":41634,\"start\":41633},{\"end\":41646,\"start\":41645},{\"end\":41656,\"start\":41655},{\"end\":41665,\"start\":41664},{\"end\":42031,\"start\":42030},{\"end\":42045,\"start\":42044},{\"end\":42047,\"start\":42046},{\"end\":42054,\"start\":42053},{\"end\":42056,\"start\":42055},{\"end\":42338,\"start\":42337},{\"end\":42345,\"start\":42344},{\"end\":42352,\"start\":42351},{\"end\":42360,\"start\":42359},{\"end\":42576,\"start\":42575},{\"end\":42593,\"start\":42592},{\"end\":42605,\"start\":42604},{\"end\":42918,\"start\":42917},{\"end\":42936,\"start\":42935},{\"end\":42946,\"start\":42945},{\"end\":42962,\"start\":42961},{\"end\":42964,\"start\":42963},{\"end\":43183,\"start\":43182},{\"end\":43190,\"start\":43189},{\"end\":43199,\"start\":43198},{\"end\":43208,\"start\":43207},{\"end\":43728,\"start\":43727},{\"end\":43737,\"start\":43736},{\"end\":43746,\"start\":43745},{\"end\":44273,\"start\":44272},{\"end\":44283,\"start\":44282},{\"end\":44285,\"start\":44284},{\"end\":44294,\"start\":44293},{\"end\":44585,\"start\":44584},{\"end\":44594,\"start\":44593},{\"end\":44606,\"start\":44605},{\"end\":44617,\"start\":44616},{\"end\":44619,\"start\":44618},{\"end\":44627,\"start\":44626},{\"end\":44631,\"start\":44628},{\"end\":44968,\"start\":44967},{\"end\":44979,\"start\":44978},{\"end\":45262,\"start\":45261},{\"end\":45270,\"start\":45269},{\"end\":45279,\"start\":45278},{\"end\":45287,\"start\":45286},{\"end\":45295,\"start\":45294},{\"end\":45302,\"start\":45301},{\"end\":45310,\"start\":45309},{\"end\":45682,\"start\":45681},{\"end\":45689,\"start\":45688},{\"end\":45698,\"start\":45697},{\"end\":46009,\"start\":46008},{\"end\":46018,\"start\":46017},{\"end\":46026,\"start\":46025},{\"end\":46034,\"start\":46033},{\"end\":46042,\"start\":46041},{\"end\":46485,\"start\":46484},{\"end\":46487,\"start\":46486},{\"end\":46496,\"start\":46495},{\"end\":46498,\"start\":46497},{\"end\":46507,\"start\":46506},{\"end\":46516,\"start\":46515},{\"end\":46523,\"start\":46522},{\"end\":46531,\"start\":46530},{\"end\":46533,\"start\":46532},{\"end\":46539,\"start\":46538},{\"end\":46541,\"start\":46540},{\"end\":46807,\"start\":46806},{\"end\":46815,\"start\":46814},{\"end\":46823,\"start\":46822},{\"end\":46830,\"start\":46829}]", "bib_author_last_name": "[{\"end\":32408,\"start\":32403},{\"end\":32617,\"start\":32614},{\"end\":32626,\"start\":32621},{\"end\":32635,\"start\":32632},{\"end\":32645,\"start\":32639},{\"end\":33086,\"start\":33081},{\"end\":33095,\"start\":33090},{\"end\":33101,\"start\":33099},{\"end\":33109,\"start\":33105},{\"end\":33118,\"start\":33113},{\"end\":33124,\"start\":33122},{\"end\":33626,\"start\":33620},{\"end\":33637,\"start\":33632},{\"end\":33647,\"start\":33643},{\"end\":33657,\"start\":33653},{\"end\":33666,\"start\":33663},{\"end\":33978,\"start\":33973},{\"end\":33989,\"start\":33982},{\"end\":33998,\"start\":33993},{\"end\":34007,\"start\":34004},{\"end\":34388,\"start\":34381},{\"end\":34404,\"start\":34395},{\"end\":34418,\"start\":34408},{\"end\":34430,\"start\":34422},{\"end\":34445,\"start\":34434},{\"end\":34455,\"start\":34449},{\"end\":34466,\"start\":34459},{\"end\":34962,\"start\":34960},{\"end\":34973,\"start\":34968},{\"end\":34981,\"start\":34977},{\"end\":34991,\"start\":34987},{\"end\":35627,\"start\":35623},{\"end\":35636,\"start\":35631},{\"end\":35648,\"start\":35640},{\"end\":35660,\"start\":35652},{\"end\":36159,\"start\":36157},{\"end\":36167,\"start\":36163},{\"end\":36533,\"start\":36531},{\"end\":36539,\"start\":36537},{\"end\":36547,\"start\":36543},{\"end\":36553,\"start\":36551},{\"end\":36829,\"start\":36826},{\"end\":36836,\"start\":36833},{\"end\":36844,\"start\":36840},{\"end\":37087,\"start\":37082},{\"end\":37097,\"start\":37091},{\"end\":37107,\"start\":37101},{\"end\":37118,\"start\":37111},{\"end\":37348,\"start\":37343},{\"end\":37515,\"start\":37513},{\"end\":37529,\"start\":37519},{\"end\":37882,\"start\":37880},{\"end\":37893,\"start\":37888},{\"end\":38234,\"start\":38232},{\"end\":38243,\"start\":38238},{\"end\":38252,\"start\":38247},{\"end\":38955,\"start\":38951},{\"end\":38967,\"start\":38963},{\"end\":39237,\"start\":39234},{\"end\":39245,\"start\":39241},{\"end\":39252,\"start\":39249},{\"end\":39260,\"start\":39256},{\"end\":39664,\"start\":39661},{\"end\":39673,\"start\":39668},{\"end\":39681,\"start\":39677},{\"end\":39689,\"start\":39685},{\"end\":39699,\"start\":39693},{\"end\":40155,\"start\":40150},{\"end\":40166,\"start\":40159},{\"end\":40178,\"start\":40170},{\"end\":40187,\"start\":40182},{\"end\":40198,\"start\":40191},{\"end\":40467,\"start\":40461},{\"end\":40480,\"start\":40473},{\"end\":40489,\"start\":40484},{\"end\":40890,\"start\":40884},{\"end\":40902,\"start\":40896},{\"end\":40914,\"start\":40908},{\"end\":40928,\"start\":40918},{\"end\":41345,\"start\":41338},{\"end\":41355,\"start\":41349},{\"end\":41643,\"start\":41635},{\"end\":41653,\"start\":41647},{\"end\":41662,\"start\":41657},{\"end\":41671,\"start\":41666},{\"end\":42042,\"start\":42032},{\"end\":42051,\"start\":42048},{\"end\":42061,\"start\":42057},{\"end\":42342,\"start\":42339},{\"end\":42349,\"start\":42346},{\"end\":42357,\"start\":42353},{\"end\":42364,\"start\":42361},{\"end\":42590,\"start\":42577},{\"end\":42602,\"start\":42594},{\"end\":42617,\"start\":42606},{\"end\":42933,\"start\":42919},{\"end\":42943,\"start\":42937},{\"end\":42959,\"start\":42947},{\"end\":42968,\"start\":42965},{\"end\":43187,\"start\":43184},{\"end\":43196,\"start\":43191},{\"end\":43205,\"start\":43200},{\"end\":43211,\"start\":43209},{\"end\":43734,\"start\":43729},{\"end\":43743,\"start\":43738},{\"end\":43752,\"start\":43747},{\"end\":44280,\"start\":44274},{\"end\":44291,\"start\":44286},{\"end\":44299,\"start\":44295},{\"end\":44591,\"start\":44586},{\"end\":44603,\"start\":44595},{\"end\":44614,\"start\":44607},{\"end\":44624,\"start\":44620},{\"end\":44646,\"start\":44632},{\"end\":44976,\"start\":44969},{\"end\":44985,\"start\":44980},{\"end\":45267,\"start\":45263},{\"end\":45276,\"start\":45271},{\"end\":45284,\"start\":45280},{\"end\":45292,\"start\":45288},{\"end\":45299,\"start\":45296},{\"end\":45307,\"start\":45303},{\"end\":45319,\"start\":45311},{\"end\":45686,\"start\":45683},{\"end\":45695,\"start\":45690},{\"end\":45702,\"start\":45699},{\"end\":46015,\"start\":46010},{\"end\":46023,\"start\":46019},{\"end\":46031,\"start\":46027},{\"end\":46039,\"start\":46035},{\"end\":46047,\"start\":46043},{\"end\":46493,\"start\":46488},{\"end\":46504,\"start\":46499},{\"end\":46513,\"start\":46508},{\"end\":46520,\"start\":46517},{\"end\":46528,\"start\":46524},{\"end\":46536,\"start\":46534},{\"end\":46546,\"start\":46542},{\"end\":46812,\"start\":46808},{\"end\":46820,\"start\":46816},{\"end\":46827,\"start\":46824},{\"end\":46834,\"start\":46831}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":202275391},\"end\":32541,\"start\":32336},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16224674},\"end\":33003,\"start\":32543},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":219964813},\"end\":33517,\"start\":33005},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6273421},\"end\":33935,\"start\":33519},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3155161},\"end\":34294,\"start\":33937},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5736847},\"end\":34904,\"start\":34296},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16372098},\"end\":35530,\"start\":34906},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15321096},\"end\":36040,\"start\":35532},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2281930},\"end\":36464,\"start\":36042},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1923924},\"end\":36756,\"start\":36466},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":208088208},\"end\":37021,\"start\":36758},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14542261},\"end\":37276,\"start\":37023},{\"attributes\":{\"id\":\"b12\"},\"end\":37458,\"start\":37278},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9204710},\"end\":37820,\"start\":37460},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3595245},\"end\":38146,\"start\":37822},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14501230},\"end\":38517,\"start\":38148},{\"attributes\":{\"id\":\"b16\"},\"end\":38849,\"start\":38519},{\"attributes\":{\"id\":\"b17\"},\"end\":39137,\"start\":38851},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":57377479},\"end\":39572,\"start\":39139},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":214728271},\"end\":40094,\"start\":39574},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":46874933},\"end\":40372,\"start\":40096},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208281389},\"end\":40816,\"start\":40374},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15752581},\"end\":41232,\"start\":40818},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6377626},\"end\":41526,\"start\":41234},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59391220},\"end\":41950,\"start\":41528},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":34777537},\"end\":42264,\"start\":41952},{\"attributes\":{\"doi\":\"arXiv:2002.05907\",\"id\":\"b26\"},\"end\":42517,\"start\":42266},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6457255},\"end\":42893,\"start\":42519},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5588768},\"end\":43109,\"start\":42895},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":195446390},\"end\":43581,\"start\":43111},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":46933128},\"end\":44207,\"start\":43583},{\"attributes\":{\"id\":\"b31\"},\"end\":44489,\"start\":44209},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":115450000},\"end\":44901,\"start\":44491},{\"attributes\":{\"id\":\"b33\"},\"end\":45180,\"start\":44903},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5711057},\"end\":45594,\"start\":45182},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":19167105},\"end\":45943,\"start\":45596},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6234666},\"end\":46409,\"start\":45945},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":73503740},\"end\":46741,\"start\":46411},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":64752361},\"end\":47084,\"start\":46743},{\"attributes\":{\"id\":\"b39\"},\"end\":47357,\"start\":47086}]", "bib_title": "[{\"end\":32399,\"start\":32336},{\"end\":32610,\"start\":32543},{\"end\":33077,\"start\":33005},{\"end\":33614,\"start\":33519},{\"end\":33969,\"start\":33937},{\"end\":34377,\"start\":34296},{\"end\":34954,\"start\":34906},{\"end\":35619,\"start\":35532},{\"end\":36153,\"start\":36042},{\"end\":36527,\"start\":36466},{\"end\":36822,\"start\":36758},{\"end\":37078,\"start\":37023},{\"end\":37339,\"start\":37278},{\"end\":37509,\"start\":37460},{\"end\":37876,\"start\":37822},{\"end\":38228,\"start\":38148},{\"end\":38949,\"start\":38851},{\"end\":39230,\"start\":39139},{\"end\":39657,\"start\":39574},{\"end\":40146,\"start\":40096},{\"end\":40457,\"start\":40374},{\"end\":40878,\"start\":40818},{\"end\":41332,\"start\":41234},{\"end\":41631,\"start\":41528},{\"end\":42028,\"start\":41952},{\"end\":42573,\"start\":42519},{\"end\":42915,\"start\":42895},{\"end\":43180,\"start\":43111},{\"end\":43725,\"start\":43583},{\"end\":44582,\"start\":44491},{\"end\":44965,\"start\":44903},{\"end\":45259,\"start\":45182},{\"end\":45679,\"start\":45596},{\"end\":46006,\"start\":45945},{\"end\":46482,\"start\":46411},{\"end\":46804,\"start\":46743}]", "bib_author": "[{\"end\":32410,\"start\":32401},{\"end\":32619,\"start\":32612},{\"end\":32628,\"start\":32619},{\"end\":32637,\"start\":32628},{\"end\":32647,\"start\":32637},{\"end\":33088,\"start\":33079},{\"end\":33097,\"start\":33088},{\"end\":33103,\"start\":33097},{\"end\":33111,\"start\":33103},{\"end\":33120,\"start\":33111},{\"end\":33126,\"start\":33120},{\"end\":33628,\"start\":33616},{\"end\":33639,\"start\":33628},{\"end\":33649,\"start\":33639},{\"end\":33659,\"start\":33649},{\"end\":33668,\"start\":33659},{\"end\":33980,\"start\":33971},{\"end\":33991,\"start\":33980},{\"end\":34000,\"start\":33991},{\"end\":34009,\"start\":34000},{\"end\":34390,\"start\":34379},{\"end\":34406,\"start\":34390},{\"end\":34420,\"start\":34406},{\"end\":34432,\"start\":34420},{\"end\":34447,\"start\":34432},{\"end\":34457,\"start\":34447},{\"end\":34468,\"start\":34457},{\"end\":34472,\"start\":34468},{\"end\":34964,\"start\":34956},{\"end\":34975,\"start\":34964},{\"end\":34983,\"start\":34975},{\"end\":34993,\"start\":34983},{\"end\":35629,\"start\":35621},{\"end\":35638,\"start\":35629},{\"end\":35650,\"start\":35638},{\"end\":35662,\"start\":35650},{\"end\":36161,\"start\":36155},{\"end\":36169,\"start\":36161},{\"end\":36535,\"start\":36529},{\"end\":36541,\"start\":36535},{\"end\":36549,\"start\":36541},{\"end\":36555,\"start\":36549},{\"end\":36831,\"start\":36824},{\"end\":36838,\"start\":36831},{\"end\":36846,\"start\":36838},{\"end\":37089,\"start\":37080},{\"end\":37099,\"start\":37089},{\"end\":37109,\"start\":37099},{\"end\":37120,\"start\":37109},{\"end\":37350,\"start\":37341},{\"end\":37517,\"start\":37511},{\"end\":37531,\"start\":37517},{\"end\":37884,\"start\":37878},{\"end\":37895,\"start\":37884},{\"end\":38236,\"start\":38230},{\"end\":38245,\"start\":38236},{\"end\":38254,\"start\":38245},{\"end\":38957,\"start\":38951},{\"end\":38969,\"start\":38957},{\"end\":39239,\"start\":39232},{\"end\":39247,\"start\":39239},{\"end\":39254,\"start\":39247},{\"end\":39262,\"start\":39254},{\"end\":39666,\"start\":39659},{\"end\":39675,\"start\":39666},{\"end\":39683,\"start\":39675},{\"end\":39691,\"start\":39683},{\"end\":39701,\"start\":39691},{\"end\":40157,\"start\":40148},{\"end\":40168,\"start\":40157},{\"end\":40180,\"start\":40168},{\"end\":40189,\"start\":40180},{\"end\":40200,\"start\":40189},{\"end\":40469,\"start\":40459},{\"end\":40482,\"start\":40469},{\"end\":40491,\"start\":40482},{\"end\":40892,\"start\":40880},{\"end\":40904,\"start\":40892},{\"end\":40916,\"start\":40904},{\"end\":40930,\"start\":40916},{\"end\":41347,\"start\":41334},{\"end\":41357,\"start\":41347},{\"end\":41645,\"start\":41633},{\"end\":41655,\"start\":41645},{\"end\":41664,\"start\":41655},{\"end\":41673,\"start\":41664},{\"end\":42044,\"start\":42030},{\"end\":42053,\"start\":42044},{\"end\":42063,\"start\":42053},{\"end\":42344,\"start\":42337},{\"end\":42351,\"start\":42344},{\"end\":42359,\"start\":42351},{\"end\":42366,\"start\":42359},{\"end\":42592,\"start\":42575},{\"end\":42604,\"start\":42592},{\"end\":42619,\"start\":42604},{\"end\":42935,\"start\":42917},{\"end\":42945,\"start\":42935},{\"end\":42961,\"start\":42945},{\"end\":42970,\"start\":42961},{\"end\":43189,\"start\":43182},{\"end\":43198,\"start\":43189},{\"end\":43207,\"start\":43198},{\"end\":43213,\"start\":43207},{\"end\":43736,\"start\":43727},{\"end\":43745,\"start\":43736},{\"end\":43754,\"start\":43745},{\"end\":44282,\"start\":44272},{\"end\":44293,\"start\":44282},{\"end\":44301,\"start\":44293},{\"end\":44593,\"start\":44584},{\"end\":44605,\"start\":44593},{\"end\":44616,\"start\":44605},{\"end\":44626,\"start\":44616},{\"end\":44648,\"start\":44626},{\"end\":44978,\"start\":44967},{\"end\":44987,\"start\":44978},{\"end\":45269,\"start\":45261},{\"end\":45278,\"start\":45269},{\"end\":45286,\"start\":45278},{\"end\":45294,\"start\":45286},{\"end\":45301,\"start\":45294},{\"end\":45309,\"start\":45301},{\"end\":45321,\"start\":45309},{\"end\":45688,\"start\":45681},{\"end\":45697,\"start\":45688},{\"end\":45704,\"start\":45697},{\"end\":46017,\"start\":46008},{\"end\":46025,\"start\":46017},{\"end\":46033,\"start\":46025},{\"end\":46041,\"start\":46033},{\"end\":46049,\"start\":46041},{\"end\":46495,\"start\":46484},{\"end\":46506,\"start\":46495},{\"end\":46515,\"start\":46506},{\"end\":46522,\"start\":46515},{\"end\":46530,\"start\":46522},{\"end\":46538,\"start\":46530},{\"end\":46548,\"start\":46538},{\"end\":46814,\"start\":46806},{\"end\":46822,\"start\":46814},{\"end\":46829,\"start\":46822},{\"end\":46836,\"start\":46829}]", "bib_venue": "[{\"end\":32422,\"start\":32410},{\"end\":32724,\"start\":32647},{\"end\":33207,\"start\":33126},{\"end\":33708,\"start\":33668},{\"end\":34090,\"start\":34009},{\"end\":34549,\"start\":34472},{\"end\":35100,\"start\":34993},{\"end\":35764,\"start\":35662},{\"end\":36231,\"start\":36169},{\"end\":36593,\"start\":36555},{\"end\":36873,\"start\":36846},{\"end\":37130,\"start\":37120},{\"end\":37358,\"start\":37350},{\"end\":37598,\"start\":37531},{\"end\":37964,\"start\":37895},{\"end\":38311,\"start\":38254},{\"end\":38683,\"start\":38519},{\"end\":38993,\"start\":38969},{\"end\":39326,\"start\":39262},{\"end\":39782,\"start\":39701},{\"end\":40215,\"start\":40200},{\"end\":40573,\"start\":40491},{\"end\":40998,\"start\":40930},{\"end\":41364,\"start\":41357},{\"end\":41719,\"start\":41673},{\"end\":42090,\"start\":42063},{\"end\":42335,\"start\":42266},{\"end\":42685,\"start\":42619},{\"end\":42978,\"start\":42970},{\"end\":43294,\"start\":43213},{\"end\":43841,\"start\":43754},{\"end\":44270,\"start\":44209},{\"end\":44673,\"start\":44648},{\"end\":45032,\"start\":44987},{\"end\":45359,\"start\":45321},{\"end\":45760,\"start\":45704},{\"end\":46126,\"start\":46049},{\"end\":46555,\"start\":46548},{\"end\":46873,\"start\":46836},{\"end\":47220,\"start\":47086},{\"end\":32788,\"start\":32726},{\"end\":33275,\"start\":33209},{\"end\":34613,\"start\":34551},{\"end\":35211,\"start\":35102},{\"end\":37652,\"start\":37600},{\"end\":39330,\"start\":39328},{\"end\":39850,\"start\":39784},{\"end\":43362,\"start\":43296},{\"end\":43915,\"start\":43843},{\"end\":46190,\"start\":46128}]"}}}, "year": 2023, "month": 12, "day": 17}