{"id": 198183644, "updated": "2022-02-05 22:50:17.611", "metadata": {"title": "End-to-End Speech-Driven Realistic Facial Animation with Temporal GANs", "authors": "[{\"middle\":[],\"last\":\"Vougioukas\",\"first\":\"Konstantinos\"}]", "venue": "CVPR Workshops", "journal": "37-40", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Speech-driven facial animation is the process which uses speech signals to automatically synthesize a talking character. The majority of work in this domain creates a mapping from audio features to visual features. This often requires postprocessing using computer graphics techniques to produce realistic albeit subject dependent results. We present a system for generating videos of a talking head, using a still image of a person and an audio clip containing speech, that does not rely on any handcrafted intermediate features. To the best of our knowledge, this is the first method capable of generating subject independent realistic videos directly from raw audio. Our method can generate videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements. We achieve this by using a temporal GAN with 2 discriminators, which are capable of capturing different aspects of the video. The generated videos are evaluated based on their sharpness, reconstruction quality, and lip-reading accuracy. Finally, a user study is conducted, confirming that temporal GANs lead to more natural sequences than a static GAN-based approach.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2959650063", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/VougioukasPP19", "doi": null}}, "content": {"source": {"pdf_hash": "aed99738a2ad6d99bad710bdf4938de3403221be", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "436f0afae492f3124b2e14251f0fcd3ef5322410", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aed99738a2ad6d99bad710bdf4938de3403221be.txt", "contents": "\nEnd-to-End Speech-Driven Realistic Facial Animation with Temporal GANs\n\n\nKonstantinos Vougioukas k.vougioukas@imperial.ac.uk \nImperial College London Samsung AI Center\nImperial College London Samsung AI Center\nImperial College London Samsung AI Center\n\n\nStavros Petridis stavros.petridis04@imperial.ac.uk \nImperial College London Samsung AI Center\nImperial College London Samsung AI Center\nImperial College London Samsung AI Center\n\n\nMaja Pantic m.pantic@imperial.ac.uk \nImperial College London Samsung AI Center\nImperial College London Samsung AI Center\nImperial College London Samsung AI Center\n\n\nEnd-to-End Speech-Driven Realistic Facial Animation with Temporal GANs\n\nSpeech-driven facial animation is the process which uses speech signals to automatically synthesize a talking character. The majority of work in this domain creates a mapping from audio features to visual features. This often requires postprocessing using computer graphics techniques to produce realistic albeit subject dependent results. We present a system for generating videos of a talking head, using a still image of a person and an audio clip containing speech, that does not rely on any handcrafted intermediate features. To the best of our knowledge, this is the first method capable of generating subject independent realistic videos directly from raw audio. Our method can generate videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements. We achieve this by using a temporal GAN with 2 discriminators, which are capable of capturing different aspects of the video. The generated videos are evaluated based on their sharpness, reconstruction quality, and lip-reading accuracy. Finally, a user study is conducted, confirming that temporal GANs lead to more natural sequences than a static GAN-based approach.\n\nIntroduction\n\nThe problem of generating realistic talking heads is multifaceted, requiring high-quality faces, lip movements synchronized with the audio, and plausible facial expressions. Such systems could simplify the film animation process through automatic generation from the voice acting and generating occluded parts of the face. Additionally, this technology can improve band-limited visual telecommunications by either generating the entire visual content based on the audio or filling in dropped frames.\n\nThe majority of research in this domain has focused on mapping audio features (e.g. MFCCs) to visual features (e.g. landmarks, visemes) and using computer graphics (CG) methods to generate realistic faces [7]. Some methods avoid the use of CG by selecting frames from a person-specific database and combining them to form a video [11].\n\nSubject independent approaches have also been proposed that transform audio features to video frames [3] but there is still no method to directly transform raw audio to video. Furthermore, many methods restrict the problem to generating only the mouth. Even techniques that generate the entire face are primar-Facial Sythesizer Figure 1: The proposed end-to-end face synthesis model, capable of producing realistic sequences of faces using one still image and an audio track containing speech. The generated sequences exhibit smoothness and natural expressions such as blinks and frowns.\n\nily focused on obtaining realistic lip movements, and typically neglect the importance of generating natural facial expressions.\n\nSome methods generate frames based solely on present information [3], without taking into account the facial dynamics. This makes generating natural sequences, which are characterized by a seamless transition between frames, challenging. Some video generation methods have dealt with this problem by generating the entire sequence at once [13] or in small batches [10]. However, this introduces a lag in the generation process, prohibiting their use in real-time applications and requiring fixed length sequences for training.\n\nWe propose a temporal generative adversarial network (GAN), capable of generating a video of a talking head from an audio signal and a single still image 1 (see Fig. 1). First, our model captures the dynamics of the entire face producing not only synchronized mouth movements but also natural facial expressions, such as eyebrow raises, frowns and blinks. Facial gestures are very important since their absence is a telltale sign that can be used to detect synthesized videos [8]. Our model is able to produce such expressions thanks to the use of a sequence discriminator.\n\nSecondly, our method is subject independent, does not rely on handcrafted audio or visual features, and requires no postprocessing. To the best of our knowledge, this is the first endto-end technique that generates talking faces directly from the raw audio waveform.\n\nEvaluation is performed in a subject independent way on the GRID [4] and TCD TIMIT [6] datasets, where our model achieves truly natural results. We measure the image quality using popular reconstruction and sharpness metrics, and compare it to a non-temporal approach. Additionally, we propose  using lip reading techniques to verify the accuracy of the spoken words and face verification to ensure that the identity of the speaker is maintained throughout the sequence.\n\n\nEnd-to-End Speech-Driven Facial Synthesis\n\nThe proposed architecture for speech-driven facial synthesis is shown in Fig. 2. The system is made up of a generator and 2 discriminators, each of which evaluates the generated sequence from a different perspective. The capability of the generator to capture various aspects of natural sequences is directly proportional to the ability of each discriminator to discern videos based on them.\n\n\nGenerator\n\nThe inputs to the generator networks consist of a single image and an audio signal, which is divided into overlapping frames each corresponding to 0.16 seconds. The generator can be conceptually divided into subnetworks as shown in Fig. 3. Using an RNN-based generator allows us to synthesize videos frame-by-frame, which is necessary for real-time applications.\n\n\nIdentity Encoder\n\nThe speaker's identity is encoded using a 6 layer CNN. Each layer uses strided 2D convolutions, followed by batch normalization and ReLU activation functions. The Identity Encoder network reduces the input image to a 50 dimensional encoding z id .\n\n\nContext Encoder\n\nAudio frames are encoded using a network comprising of 1D convolutions followed by batch normalization and ReLU activations. The initial convolutional layer starts with a large kernel which helps limit the depth of the network while ensuring that the low-level features are meaningful. Subsequent layers use smaller kernels until an embedding of the desired size is achieved. The audio frame encodings are input into a 2 layer GRU, which produces a context encoding z c with 256 elements.\n\n\nFrame Decoder\n\nThe identity encoding z id is concatenated to the context encoding z c and a noise component z n to form the latent represen-  tation. The 10-dimensional z n vector is obtained from a Noise Generator, which is a 1-layer GRU that takes Gaussian noise as input. The Frame Decoder is a CNN that uses strided transposed convolutions to produce the video frames from the latent representation. A U-Net architecture is used with skip connections between the Identity Encoder and the Frame Decoder to help preserve the identity of the subject.\n\n\nDiscriminators\n\nOur system has two different types of discriminator. The Frame Discriminator helps achieve a high-quality reconstruction of the speakers' face throughout the video. The Sequence Discriminator ensures that the frames form a cohesive video which exhibits natural movements and is synchronized with the audio.\n\n\nFrame Discriminator\n\nThe Frame Discriminator is a 6-layer CNN that determines whether a frame is real or not. Adversarial training with this discriminator ensures that the generated frames are realistic. The original still frame is used as a condition in this network, concatenated channel-wise to the target frame to form the input as shown in Fig. 3. This enforces the person's identity on the frame.\n\n\nSequence Discriminator\n\nThe Sequence Discriminator presented in Fig. 3 distinguishes between real and synthetic videos. The discriminator receives a frame at every time step, which is encoded using a CNN and then fed into a 2-layer GRU. A small (2-layer) classifier is used at the end of the sequence to determine if the sequence is real or not. The audio is added as a conditional input to the network, allowing this discriminator to classify speech-video pairs.\n\n\nTraining\n\nThe Frame discriminator (D img ) is trained on frames that are sampled uniformly from a video x using a sampling function S(x). The Sequence discriminator (D seq ), classifies based on the entire sequence x and audio a. The loss for each discriminator contributes to the total loss shown in eq. 1.\nL adv (Dimg, DSeq, G) = Ex\u223cP d [log Dimg(S(x), x1)] + Ez\u223cP z [log(1 \u2212 Dimg(S(G(z)), x1))] + Ex\u223cP d [log Dseq(x, a)] + Ez\u223cP z [log(1 \u2212 Dseq(G(z), a))](1)\nAn L 1 reconstruction loss is also used to improve the synchronization of the mouth movements. However we only apply the reconstruction loss to the lower half of the image since it discourages the generation of facial expressions. For a ground truth frame F and a generated frame G with dimensions W \u00d7H the reconstruction loss at the pixel level is:\nL L1 = p\u2208[0,W ]\u00d7[ H 2 ,H] |F p \u2212 G p | (2)\nThe final objective is to obtain the optimal generator G * , which satisfies eq. 3. The model is trained until no improvement is observed on the reconstruction metrics on the validation set for 10 epochs. The \u03bb hyperparameter controls the contribution of each loss factor and was set to 400 following a tuning procedure on the validation set.\n\n\narg min\nG max D L adv + \u03bbL L1(3)\nWe used Adam for all the networks with a learning rate of 0.0002 for the Generator and 0.001 Frame Discriminator which decay after epoch 20 with a rate of 10%. The Sequence Discriminator uses a smaller fixed learning rate of 5 \u00b7 10 \u22125 .\n\n\nExperiments\n\n\nDatasets\n\nThe GRID dataset has 33 speakers each uttering 1000 short phrases, containing 6 words taken from a limited dictionary. The TCD TIMIT dataset has 59 speakers uttering approximately 100 phonetically rich sentences each. We use the recommended data split for the TCD TIMIT dataset but exclude some of the test speakers and use them as a validation set. For the GRID dataset speakers are divided into training, validation and test sets with a 50% \u2212 20% \u2212 30% split respectively. As part of our preprocessing all faces are aligned to the canonical face and images are normalized. We increase the size of the training set by mirroring the training videos.\n\n\nMetrics\n\nWe use common reconstruction metrics such as the peak signal-to-noise ratio (PSNR) and the structural similarity (SSIM) index to evaluate the generated videos. However, it is important to note that reconstruction metrics penalize videos for any spontaneous expression. Frame sharpness is evaluated using the cumulative probability blur detection (CPBD) measure [9], which determines blur based on the presence of edges in the image and the frequency domain blurriness measure (FDBM) proposed in [5], which is based on the spectrum of the image. For these metrics larger values imply better quality.\n\nThe content of the videos is evaluated based on how well the video captures identity of the target and on the accuracy of the spoken words. We verify the identity of the speaker using the average content distance (ACD) [12], which measures the average Euclidean distance of the still image representation, obtained using OpenFace [1], from the representation of the generated frames. The accuracy of the spoken message is measured using the word error rate (WER) achieved by a pretrained lip-reading model (LipNet [2]). For both content metrics lower values indicate better accuracy.\n\n\nQualitative Results\n\nOur method is capable of producing realistic videos of previously unseen faces and audio clips taken from the test set. The same audio used on different identities is shown in Fig. 4. From visual inspection it is evident that the lips are consistently moving similarly to the ground truth video. Our method not only produces accurate lip movements but also natural videos that display characteristic human expressions such as frowns and blinks, examples of which are shown in Fig. 5. We compare our model to a static method that produces video frames using a sliding window of audio samples like that used in [3]. This is a GAN-based method that uses a combination of an L 1 loss and an adversarial loss on individual frames. We use this method as the baseline for our quantitative assessment in the following section. This baseline produces sequences characterized by jitter, which becomes worse in cases where the audio is silent. This is likely due to the fact that there are multiple mouth shapes that correspond to silence and since the model has no knowledge of its past state generates them at random.\n\n\nQuantitative Results\n\nWe measure the performance of our model on the GRID and TCD TIMIT datasets using the metrics proposed in section 3.2 and compare it to the static baseline. Additionally, we present the results of a 30-person survey, where users were shown 30 videos from each method and were asked to pick the more natural ones. The results in Table 1 show that our method outperforms the static baseline in both frame quality and content accuracy. Although the difference in performance is slight for frame-based measures (e.g. PSNR, ACD) it is substantial in terms of user preference and lipreading WER, where temporal smoothness of the video and natural expressions play a significant role.    We further evaluate the realism of the generated videos through an online Turing test. In this test users are shown 10 videos randomly chosen from GRID and TCD-TIMIT databases and are asked to label them as real or fake. Responses from 316 users were collected with the average user labeling correctly 63% of the videos.\n\n\nConclusion and Future Work\n\nIn this work we have presented an end-to-end model using temporal GANs for speech-driven facial animation. Our method produces more coherent sequences and more accurate mouth movements compared to the static approach and also produces facial expressions like blinks and frowns. We believe that these improvements are not only a result of using a temporal generator but also due to the use of the conditional Sequence Discriminator which encourages spontaneous facial gestures. Moving forward, we would like to capture and reflect the mood of the speaker in the facial expressions.\n\nFigure 2 :\n2The deep model for speech-driven facial synthesis. This uses 2 discriminators to incorporate the different aspects of a realistic video. Details about the architecture are presented in the supplementary material.\n\nFigure 3 :\n3The architecture of the (a) Generator which consists of a Context Encoder (audio encoder and RNN), an Identity Encoder, a Frame Decoder and Noise Generator (b) Sequence Discriminator, consisting of an audio encoder, an image encoder, GRUs and a small classifier.\n\nFigure 4 :\n4Animation of different faces using the same audio. The movement of the mouth is similar for both faces as well as for the ground truth sequence. Both audio and still image are unseen during training.\n\nFigure 5 :\n5Facial expressions generated using our framework include (a) frowns and (b) blinks.\n\nTable 1 :\n1Performance comparison of the proposed method against the baseline. The pretrained LipNet model is not available for the TCD TIMIT so the WER metric is omitted.\nVideos are available here: https://sites.google.com/view/ facialsynthesis/home\n\nOpenFace: A general-purpose face recognition library with mobile applications. B Amos, B Ludwiczuk, M Satyanarayanan, 118Technical ReportB. Amos, B. Ludwiczuk, and M. Satyanarayanan. OpenFace: A general-purpose face recognition library with mobile applica- tions. Technical Report 118, 2016. 3\n\nY M Assael, B Shillingford, S Whiteson, N De Freitas, arXiv:1611.01599LipNet: End-to-End Sentence-level Lipreading. arXiv preprintY. M. Assael, B. Shillingford, S. Whiteson, and N. de Freitas. LipNet: End-to-End Sentence-level Lipreading. arXiv preprint arXiv:1611.01599, 2016. 3\n\nYou said that? In BMVC. J S Chung, A Jamaludin, A Zisserman, 13J. S. Chung, A. Jamaludin, and A. Zisserman. You said that? In BMVC, pages 1-12, 2017. 1, 3\n\nAn audiovisual corpus for speech perception and automatic speech recognition. M Cooke, J Barker, S Cunningham, X Shao, The Journal of the Acoustical Society of America. 1205M. Cooke, J. Barker, S. Cunningham, and X. Shao. An audio- visual corpus for speech perception and automatic speech recog- nition. The Journal of the Acoustical Society of America, 120(5):2421-2424, 2006. 1\n\nImage Sharpness Measure for Blurred Images in Frequency Domain. K De, V Masilamani, Procedia Engineering. 643K. De and V. Masilamani. Image Sharpness Measure for Blurred Images in Frequency Domain. Procedia Engineering, 64:149- 158, 1 2013. 3\n\nTCD-TIMIT: An audio-visual corpus of continuous speech. N Harte, E Gillen, IEEE Transactions on Multimedia. 175N. Harte and E. Gillen. TCD-TIMIT: An audio-visual cor- pus of continuous speech. IEEE Transactions on Multimedia, 17(5):603-615, 2015. 1\n\nAudiodriven facial animation by joint end-to-end learning of pose and emotion. T Karras, T Aila, S Laine, A Herva, J Lehtinen, 2017. 1ACM TOG36T. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen. Audio- driven facial animation by joint end-to-end learning of pose and emotion. ACM TOG, 36(94), 2017. 1\n\nY Li, M Chang, S Lyu, arXiv:1806.02877Ictu Oculi : Exposing AI Generated Fake Face Videos by Detecting Eye Blinking. arXiv preprintY. Li, M. Chang, and S. Lyu. In Ictu Oculi : Exposing AI Gener- ated Fake Face Videos by Detecting Eye Blinking. arXiv preprint arXiv:1806.02877, 2018. 1\n\nA no-reference perceptual image sharpness metric based on a cumulative probability of blur detection. N D Narvekar, L J Karam, International Workshop on Quality of Multimedia Experience (QoMEx). 20N. D. Narvekar and L. J. Karam. A no-reference perceptual im- age sharpness metric based on a cumulative probability of blur detection. International Workshop on Quality of Multimedia Ex- perience (QoMEx), 20(9):87-91, 2009. 3\n\nTemporal Generative Adversarial Nets with Singular Value Clipping. M Saito, E Matsumoto, S Saito, ICCV. M. Saito, E. Matsumoto, and S. Saito. Temporal Generative Ad- versarial Nets with Singular Value Clipping. In ICCV, pages 2830-2839, 2017. 1\n\nSynthesizing Obama: Learning Lip Sync from Audio Output Obama Video. S Suwajanakorn, S Seitz, I Kemelmacher-Shlizerman, 2017. 1ACM TOG36S. Suwajanakorn, S. Seitz, and I. Kemelmacher-Shlizerman. Syn- thesizing Obama: Learning Lip Sync from Audio Output Obama Video. ACM TOG, 36(95), 2017. 1\n\nMoCoGAN: Decomposing Motion and Content for Video Generation. S Tulyakov, M Liu, X Yang, J Kautz, arXiv:1707.04993arXiv preprintS. Tulyakov, M. Liu, X. Yang, and J. Kautz. MoCoGAN: Decom- posing Motion and Content for Video Generation. arXiv preprint arXiv:1707.04993, 2017. 3\n\nGenerating Videos with Scene Dynamics. C Vondrick, H Pirsiavash, A Torralba, NIPS. C. Vondrick, H. Pirsiavash, and A. Torralba. Generating Videos with Scene Dynamics. In NIPS, pages 613-621, 2016. 1\n", "annotations": {"author": "[{\"start\":\"74\",\"end\":\"254\"},{\"start\":\"255\",\"end\":\"434\"},{\"start\":\"435\",\"end\":\"599\"}]", "publisher": null, "author_last_name": "[{\"start\":\"87\",\"end\":\"97\"},{\"start\":\"263\",\"end\":\"271\"},{\"start\":\"440\",\"end\":\"446\"}]", "author_first_name": "[{\"start\":\"74\",\"end\":\"86\"},{\"start\":\"255\",\"end\":\"262\"},{\"start\":\"435\",\"end\":\"439\"}]", "author_affiliation": "[{\"start\":\"127\",\"end\":\"253\"},{\"start\":\"307\",\"end\":\"433\"},{\"start\":\"472\",\"end\":\"598\"}]", "title": "[{\"start\":\"1\",\"end\":\"71\"},{\"start\":\"600\",\"end\":\"670\"}]", "venue": null, "abstract": "[{\"start\":\"672\",\"end\":\"1874\"}]", "bib_ref": "[{\"start\":\"2596\",\"end\":\"2599\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"2721\",\"end\":\"2725\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"2829\",\"end\":\"2832\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3512\",\"end\":\"3515\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3786\",\"end\":\"3790\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"3811\",\"end\":\"3815\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4451\",\"end\":\"4454\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"4883\",\"end\":\"4886\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"4901\",\"end\":\"4904\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"11147\",\"end\":\"11150\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"11281\",\"end\":\"11284\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"11605\",\"end\":\"11609\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"11716\",\"end\":\"11719\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"11900\",\"end\":\"11903\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"12602\",\"end\":\"12605\",\"attributes\":{\"ref_id\":\"b2\"}}]", "figure": "[{\"start\":\"14738\",\"end\":\"14963\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"14964\",\"end\":\"15239\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"15240\",\"end\":\"15452\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"15453\",\"end\":\"15549\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"15550\",\"end\":\"15722\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1890\",\"end\":\"2389\"},{\"start\":\"2391\",\"end\":\"2726\"},{\"start\":\"2728\",\"end\":\"3315\"},{\"start\":\"3317\",\"end\":\"3445\"},{\"start\":\"3447\",\"end\":\"3973\"},{\"start\":\"3975\",\"end\":\"4548\"},{\"start\":\"4550\",\"end\":\"4816\"},{\"start\":\"4818\",\"end\":\"5288\"},{\"start\":\"5334\",\"end\":\"5725\"},{\"start\":\"5739\",\"end\":\"6101\"},{\"start\":\"6122\",\"end\":\"6369\"},{\"start\":\"6389\",\"end\":\"6877\"},{\"start\":\"6895\",\"end\":\"7431\"},{\"start\":\"7450\",\"end\":\"7756\"},{\"start\":\"7780\",\"end\":\"8161\"},{\"start\":\"8188\",\"end\":\"8627\"},{\"start\":\"8640\",\"end\":\"8937\"},{\"start\":\"9091\",\"end\":\"9440\"},{\"start\":\"9484\",\"end\":\"9826\"},{\"start\":\"9862\",\"end\":\"10098\"},{\"start\":\"10125\",\"end\":\"10774\"},{\"start\":\"10786\",\"end\":\"11384\"},{\"start\":\"11386\",\"end\":\"11969\"},{\"start\":\"11993\",\"end\":\"13101\"},{\"start\":\"13126\",\"end\":\"14126\"},{\"start\":\"14157\",\"end\":\"14737\"}]", "formula": "[{\"start\":\"8938\",\"end\":\"9090\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"9441\",\"end\":\"9483\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"9837\",\"end\":\"9861\",\"attributes\":{\"id\":\"formula_2\"}}]", "table_ref": "[{\"start\":\"13453\",\"end\":\"13460\",\"attributes\":{\"ref_id\":\"tab_2\"}}]", "section_header": "[{\"start\":\"1876\",\"end\":\"1888\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"5291\",\"end\":\"5332\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"5728\",\"end\":\"5737\",\"attributes\":{\"n\":\"2.1.\"}},{\"start\":\"6104\",\"end\":\"6120\",\"attributes\":{\"n\":\"2.1.1\"}},{\"start\":\"6372\",\"end\":\"6387\",\"attributes\":{\"n\":\"2.1.2\"}},{\"start\":\"6880\",\"end\":\"6893\",\"attributes\":{\"n\":\"2.1.3\"}},{\"start\":\"7434\",\"end\":\"7448\",\"attributes\":{\"n\":\"2.2.\"}},{\"start\":\"7759\",\"end\":\"7778\",\"attributes\":{\"n\":\"2.2.1\"}},{\"start\":\"8164\",\"end\":\"8186\",\"attributes\":{\"n\":\"2.2.2\"}},{\"start\":\"8630\",\"end\":\"8638\",\"attributes\":{\"n\":\"2.3.\"}},{\"start\":\"9829\",\"end\":\"9836\"},{\"start\":\"10101\",\"end\":\"10112\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"10115\",\"end\":\"10123\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"10777\",\"end\":\"10784\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"11972\",\"end\":\"11991\",\"attributes\":{\"n\":\"3.3.\"}},{\"start\":\"13104\",\"end\":\"13124\",\"attributes\":{\"n\":\"3.4.\"}},{\"start\":\"14129\",\"end\":\"14155\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"14739\",\"end\":\"14749\"},{\"start\":\"14965\",\"end\":\"14975\"},{\"start\":\"15241\",\"end\":\"15251\"},{\"start\":\"15454\",\"end\":\"15464\"},{\"start\":\"15551\",\"end\":\"15560\"}]", "table": null, "figure_caption": "[{\"start\":\"14751\",\"end\":\"14963\"},{\"start\":\"14977\",\"end\":\"15239\"},{\"start\":\"15253\",\"end\":\"15452\"},{\"start\":\"15466\",\"end\":\"15549\"},{\"start\":\"15562\",\"end\":\"15722\"}]", "figure_ref": "[{\"start\":\"3056\",\"end\":\"3064\"},{\"start\":\"4136\",\"end\":\"4142\"},{\"start\":\"5407\",\"end\":\"5413\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"5971\",\"end\":\"5977\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"8104\",\"end\":\"8110\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"8228\",\"end\":\"8234\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"12169\",\"end\":\"12175\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"12469\",\"end\":\"12475\",\"attributes\":{\"ref_id\":\"fig_4\"}}]", "bib_author_first_name": "[{\"start\":\"15882\",\"end\":\"15883\"},{\"start\":\"15890\",\"end\":\"15891\"},{\"start\":\"15903\",\"end\":\"15904\"},{\"start\":\"16098\",\"end\":\"16099\"},{\"start\":\"16100\",\"end\":\"16101\"},{\"start\":\"16110\",\"end\":\"16111\"},{\"start\":\"16126\",\"end\":\"16127\"},{\"start\":\"16138\",\"end\":\"16139\"},{\"start\":\"16403\",\"end\":\"16404\"},{\"start\":\"16405\",\"end\":\"16406\"},{\"start\":\"16414\",\"end\":\"16415\"},{\"start\":\"16427\",\"end\":\"16428\"},{\"start\":\"16613\",\"end\":\"16614\"},{\"start\":\"16622\",\"end\":\"16623\"},{\"start\":\"16632\",\"end\":\"16633\"},{\"start\":\"16646\",\"end\":\"16647\"},{\"start\":\"16980\",\"end\":\"16981\"},{\"start\":\"16986\",\"end\":\"16987\"},{\"start\":\"17216\",\"end\":\"17217\"},{\"start\":\"17225\",\"end\":\"17226\"},{\"start\":\"17489\",\"end\":\"17490\"},{\"start\":\"17499\",\"end\":\"17500\"},{\"start\":\"17507\",\"end\":\"17508\"},{\"start\":\"17516\",\"end\":\"17517\"},{\"start\":\"17525\",\"end\":\"17526\"},{\"start\":\"17717\",\"end\":\"17718\"},{\"start\":\"17723\",\"end\":\"17724\"},{\"start\":\"17732\",\"end\":\"17733\"},{\"start\":\"18105\",\"end\":\"18106\"},{\"start\":\"18107\",\"end\":\"18108\"},{\"start\":\"18119\",\"end\":\"18120\"},{\"start\":\"18121\",\"end\":\"18122\"},{\"start\":\"18495\",\"end\":\"18496\"},{\"start\":\"18504\",\"end\":\"18505\"},{\"start\":\"18517\",\"end\":\"18518\"},{\"start\":\"18743\",\"end\":\"18744\"},{\"start\":\"18759\",\"end\":\"18760\"},{\"start\":\"18768\",\"end\":\"18769\"},{\"start\":\"19027\",\"end\":\"19028\"},{\"start\":\"19039\",\"end\":\"19040\"},{\"start\":\"19046\",\"end\":\"19047\"},{\"start\":\"19054\",\"end\":\"19055\"},{\"start\":\"19282\",\"end\":\"19283\"},{\"start\":\"19294\",\"end\":\"19295\"},{\"start\":\"19308\",\"end\":\"19309\"}]", "bib_author_last_name": "[{\"start\":\"15884\",\"end\":\"15888\"},{\"start\":\"15892\",\"end\":\"15901\"},{\"start\":\"15905\",\"end\":\"15919\"},{\"start\":\"16102\",\"end\":\"16108\"},{\"start\":\"16112\",\"end\":\"16124\"},{\"start\":\"16128\",\"end\":\"16136\"},{\"start\":\"16140\",\"end\":\"16150\"},{\"start\":\"16407\",\"end\":\"16412\"},{\"start\":\"16416\",\"end\":\"16425\"},{\"start\":\"16429\",\"end\":\"16438\"},{\"start\":\"16615\",\"end\":\"16620\"},{\"start\":\"16624\",\"end\":\"16630\"},{\"start\":\"16634\",\"end\":\"16644\"},{\"start\":\"16648\",\"end\":\"16652\"},{\"start\":\"16982\",\"end\":\"16984\"},{\"start\":\"16988\",\"end\":\"16998\"},{\"start\":\"17218\",\"end\":\"17223\"},{\"start\":\"17227\",\"end\":\"17233\"},{\"start\":\"17491\",\"end\":\"17497\"},{\"start\":\"17501\",\"end\":\"17505\"},{\"start\":\"17509\",\"end\":\"17514\"},{\"start\":\"17518\",\"end\":\"17523\"},{\"start\":\"17527\",\"end\":\"17535\"},{\"start\":\"17719\",\"end\":\"17721\"},{\"start\":\"17725\",\"end\":\"17730\"},{\"start\":\"17734\",\"end\":\"17737\"},{\"start\":\"18109\",\"end\":\"18117\"},{\"start\":\"18123\",\"end\":\"18128\"},{\"start\":\"18497\",\"end\":\"18502\"},{\"start\":\"18506\",\"end\":\"18515\"},{\"start\":\"18519\",\"end\":\"18524\"},{\"start\":\"18745\",\"end\":\"18757\"},{\"start\":\"18761\",\"end\":\"18766\"},{\"start\":\"18770\",\"end\":\"18792\"},{\"start\":\"19029\",\"end\":\"19037\"},{\"start\":\"19041\",\"end\":\"19044\"},{\"start\":\"19048\",\"end\":\"19052\"},{\"start\":\"19056\",\"end\":\"19061\"},{\"start\":\"19284\",\"end\":\"19292\"},{\"start\":\"19296\",\"end\":\"19306\"},{\"start\":\"19310\",\"end\":\"19318\"}]", "bib_entry": "[{\"start\":\"15803\",\"end\":\"16096\",\"attributes\":{\"id\":\"b0\",\"doi\":\"118\"}},{\"start\":\"16098\",\"end\":\"16377\",\"attributes\":{\"id\":\"b1\",\"doi\":\"arXiv:1611.01599\"}},{\"start\":\"16379\",\"end\":\"16533\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"16535\",\"end\":\"16914\",\"attributes\":{\"id\":\"b3\"}},{\"start\":\"16916\",\"end\":\"17158\",\"attributes\":{\"matched_paper_id\":\"62637609\",\"id\":\"b4\"}},{\"start\":\"17160\",\"end\":\"17408\",\"attributes\":{\"matched_paper_id\":\"9479308\",\"id\":\"b5\"}},{\"start\":\"17410\",\"end\":\"17715\",\"attributes\":{\"id\":\"b6\",\"doi\":\"2017. 1\"}},{\"start\":\"17717\",\"end\":\"18001\",\"attributes\":{\"id\":\"b7\",\"doi\":\"arXiv:1806.02877\"}},{\"start\":\"18003\",\"end\":\"18426\",\"attributes\":{\"matched_paper_id\":\"23547663\",\"id\":\"b8\"}},{\"start\":\"18428\",\"end\":\"18672\",\"attributes\":{\"matched_paper_id\":\"6945308\",\"id\":\"b9\"}},{\"start\":\"18674\",\"end\":\"18963\",\"attributes\":{\"id\":\"b10\",\"doi\":\"2017. 1\"}},{\"start\":\"18965\",\"end\":\"19241\",\"attributes\":{\"id\":\"b11\",\"doi\":\"arXiv:1707.04993\"}},{\"start\":\"19243\",\"end\":\"19441\",\"attributes\":{\"matched_paper_id\":\"9933254\",\"id\":\"b12\"}}]", "bib_title": "[{\"start\":\"16535\",\"end\":\"16611\"},{\"start\":\"16916\",\"end\":\"16978\"},{\"start\":\"17160\",\"end\":\"17214\"},{\"start\":\"18003\",\"end\":\"18103\"},{\"start\":\"18428\",\"end\":\"18493\"},{\"start\":\"19243\",\"end\":\"19280\"}]", "bib_author": "[{\"start\":\"15882\",\"end\":\"15890\"},{\"start\":\"15890\",\"end\":\"15903\"},{\"start\":\"15903\",\"end\":\"15921\"},{\"start\":\"16098\",\"end\":\"16110\"},{\"start\":\"16110\",\"end\":\"16126\"},{\"start\":\"16126\",\"end\":\"16138\"},{\"start\":\"16138\",\"end\":\"16152\"},{\"start\":\"16403\",\"end\":\"16414\"},{\"start\":\"16414\",\"end\":\"16427\"},{\"start\":\"16427\",\"end\":\"16440\"},{\"start\":\"16613\",\"end\":\"16622\"},{\"start\":\"16622\",\"end\":\"16632\"},{\"start\":\"16632\",\"end\":\"16646\"},{\"start\":\"16646\",\"end\":\"16654\"},{\"start\":\"16980\",\"end\":\"16986\"},{\"start\":\"16986\",\"end\":\"17000\"},{\"start\":\"17216\",\"end\":\"17225\"},{\"start\":\"17225\",\"end\":\"17235\"},{\"start\":\"17489\",\"end\":\"17499\"},{\"start\":\"17499\",\"end\":\"17507\"},{\"start\":\"17507\",\"end\":\"17516\"},{\"start\":\"17516\",\"end\":\"17525\"},{\"start\":\"17525\",\"end\":\"17537\"},{\"start\":\"17717\",\"end\":\"17723\"},{\"start\":\"17723\",\"end\":\"17732\"},{\"start\":\"17732\",\"end\":\"17739\"},{\"start\":\"18105\",\"end\":\"18119\"},{\"start\":\"18119\",\"end\":\"18130\"},{\"start\":\"18495\",\"end\":\"18504\"},{\"start\":\"18504\",\"end\":\"18517\"},{\"start\":\"18517\",\"end\":\"18526\"},{\"start\":\"18743\",\"end\":\"18759\"},{\"start\":\"18759\",\"end\":\"18768\"},{\"start\":\"18768\",\"end\":\"18794\"},{\"start\":\"19027\",\"end\":\"19039\"},{\"start\":\"19039\",\"end\":\"19046\"},{\"start\":\"19046\",\"end\":\"19054\"},{\"start\":\"19054\",\"end\":\"19063\"},{\"start\":\"19282\",\"end\":\"19294\"},{\"start\":\"19294\",\"end\":\"19308\"},{\"start\":\"19308\",\"end\":\"19320\"}]", "bib_venue": "[{\"start\":\"15803\",\"end\":\"15880\"},{\"start\":\"16168\",\"end\":\"16212\"},{\"start\":\"16379\",\"end\":\"16401\"},{\"start\":\"16654\",\"end\":\"16702\"},{\"start\":\"17000\",\"end\":\"17020\"},{\"start\":\"17235\",\"end\":\"17266\"},{\"start\":\"17410\",\"end\":\"17487\"},{\"start\":\"17755\",\"end\":\"17832\"},{\"start\":\"18130\",\"end\":\"18196\"},{\"start\":\"18526\",\"end\":\"18530\"},{\"start\":\"18674\",\"end\":\"18741\"},{\"start\":\"18965\",\"end\":\"19025\"},{\"start\":\"19320\",\"end\":\"19324\"}]"}}}, "year": 2023, "month": 12, "day": 17}