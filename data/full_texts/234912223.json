{"id": 234912223, "updated": "2022-05-02 03:52:15.656", "metadata": {"title": "MulTa-HDC: A Multi-Task Learning Framework For Hyperdimensional Computing", "authors": "[{\"first\":\"Cheng-Yang\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Yu-Chuan\",\"last\":\"Chuang\",\"middle\":[]},{\"first\":\"En-Jui\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"An-Yeu\",\"last\":\"Wu\",\"middle\":[\"Andy\"]}]", "venue": "IEEE Transactions on Computers", "journal": "IEEE Transactions on Computers", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": ", Abstract\u2014 Brain-inspired Hyperdimensional computing (HDC) has shown its effectiveness in low-power/energy designs for edge computing in the Internet of Things (IoT). Due to limited resources available on edge devices, multi-task learning (MTL), which accommodates multiple cognitive tasks in one model, is considered a more ef\ufb01cient deployment of HDC. However, as the number of tasks increases, MTL-based HDC (MTL-HDC) suffers from the huge overhead of associative memory (AM) and performance degradation. This hinders MTL-HDC from the practical realization on edge devices. This article aims to establish an MTL framework for HDC to achieve a \ufb02exible and ef\ufb01cient trade-off between memory overhead and performance degradation. For the shared-AM approach, we propose Dimension Ranking for Effective AM Sharing (DREAMS) to effectively merge multiple AMs while preserving as much information of each task as possible. For the independent-AM approach, we propose Dimension Ranking for Independent MEmory Retrieval (DRIMER) to extract and concatenate informative components of AMs while mitigating interferences among tasks. By leveraging both mechanisms, we propose a hybrid framework of Multi-Tasking HDC, called MulTa-HDC. To adapt an MTL-HDC system to an edge device given a memory resource budget, MulTa-HDC utilizes three parameters to \ufb02exibly adjust the proportion of the shared AM and independent AMs. The proposed MulTa-HDC is widely evaluated across three common benchmarks under two standard task protocols. The simulation results of ISOLET, UCIHAR, and MNIST datasets demonstrate that the proposed MulTa-HDC outperforms other state-of-the-art compressed HD models, including SparseHD and CompHD, by up to 8.23% in terms of classi\ufb01cation accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3155563699", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tc/ChangCCW21", "doi": "10.1109/tc.2021.3073409"}}, "content": {"source": {"pdf_hash": "2badc24319ea1bf3b27d0ec41380ee73f075bb60", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "92ff68e644efae5084368958d887077dfaa3eb00", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2badc24319ea1bf3b27d0ec41380ee73f075bb60.txt", "contents": "\nMulTa-HDC: A Multi-Task Learning Framework For Hyperdimensional Computing\n\n\nStudent Member, IEEECheng-Yang Chang \nStudent Member, IEEEYu-Chuan Chuang \nMember, IEEEEn-Jui Chang \nAn-Yeu \nFellow, IEEEAndy Wu \nMulTa-HDC: A Multi-Task Learning Framework For Hyperdimensional Computing\n10.1109/TC.2021.3073409Index Terms-Brain-inspired computinghyperdimensional computingmachine learningmulti-task learning\nBrain-inspired Hyperdimensional computing (HDC) has shown its effectiveness in low-power/energy designs for edge computing in the Internet of Things (IoT). Due to limited resources available on edge devices, multi-task learning (MTL), which accommodates multiple cognitive tasks in one model, is considered a more efficient deployment of HDC. However, as the number of tasks increases, MTL-based HDC (MTL-HDC) suffers from the huge overhead of associative memory (AM) and performance degradation. This hinders MTL-HDC from the practical realization on edge devices. This article aims to establish an MTL framework for HDC to achieve a flexible and efficient trade-off between memory overhead and performance degradation. For the shared-AM approach, we propose Dimension Ranking for Effective AM Sharing (DREAMS) to effectively merge multiple AMs while preserving as much information of each task as possible. For the independent-AM approach, we propose Dimension Ranking for Independent MEmory Retrieval (DRIMER) to extract and concatenate informative components of AMs while mitigating interferences among tasks. By leveraging both mechanisms, we propose a hybrid framework of Multi-Tasking HDC, called MulTa-HDC. To adapt an MTL-HDC system to an edge device given a memory resource budget, MulTa-HDC utilizes three parameters to flexibly adjust the proportion of the shared AM and independent AMs. The proposed MulTa-HDC is widely evaluated across three common benchmarks under two standard task protocols. The simulation results of ISOLET, UCIHAR, and MNIST datasets demonstrate that the proposed MulTa-HDC outperforms other state-of-the-art compressed HD models, including SparseHD and CompHD, by up to 8.23% in terms of classification accuracy.\n\n\u00c7\n\n\nINTRODUCTION\n\nW ITH the emergence of the Internet of Things (IoT), massive data streams are produced daily. Machine learning (ML)-on-edge provides a low-bandwidth and speedy way to process the vast amounts of data on the front. This also indicates that edge devices may need to handle various tasks, such as speech recognition, human activity recognition, and image classification, in a single inference engine. Due to limited resources available on edge devices, multi-task learning (MTL), where a single ML engine simultaneously executes multiple tasks, receives much attention for future IoT applications [1]. Instead of training models separately, MTL aims to develop an efficient multi-task model and mitigates the hardware overhead for resource-constrained devices. Currently, most works mainly focus on achieving MTL in deep neural networks (DNN) [2], [3], [4], [5]. However, the high complexity of DNN limits its deployability for practical applications where devices have limited resources. To overcome the limited-resource issue, the authors of [33] proposed a task offloading scheme to offload computation-intensive applications to remote edge nodes. The authors of [34] also proposed on-device multi-task transfer learning to tackle data scarcity on edge devices. Nevertheless, varying network conditions and limited computation resources of edge nodes cause complex offloading decisions and resource allocation problems [35]. Therefore, it is highly desired to devise an alternative computing scheme that can efficiently handle multiple cognitive tasks for IoT applications on devices.\n\nBrain-inspired hyperdimensional computing (HDC) emulates the patterns of neural activities in brains and handles cognitive tasks with high-dimensional (e.g., thousands) vectors, called hypervectors (HVs) [8]. With a simple set of welldefined vector operations, HDC exhibits many technical merits, including high energy efficiency [7], [11], robustness against hardware failures [6], and fast/few-shot learning capability [9], [10]. Moreover, HDC has shown great success in various real-world tasks, such as emotion recognition [12], seizure detection [13], DNA sequencing [14], speech recognition [15], human activity recognition [16], and image classification [17]. In some physiological applications, HDC even surpasses the state-of-the-art classification methods [9], [10], [38]. These properties and advantages both make HDC suitable for efficient inference on edge devices. For application of HDC to MTL, the authors of [32] utilized the concept of HDC to superimpose many subject-specific trained convolutional neural networks (CNNs) into one single model, reducing the memory requirement for storing CNN parameters. However, the computation of CNN is still unaffordable for many edge devices.\n\nIn HDC, training samples based on their corresponding classes are transformed and merged into a set of representative HVs, called class HVs. Class HVs are stored in the associative memory (AM) and are used to determine the predicted label in the inference phase.\n\nDespite the superior computational efficiency of HDC, under the scenario of multi-task learning-based HDC (MTL-HDC), the overall size of HDC model considerably grows with the number of tasks since HDC stores a separate AM for each task. As the number of tasks increases, the huge memory overhead of AMs becomes a significant problem. Although AMs of different tasks can be directly merged into one shared by all tasks to reduce memory overhead; however, this approach causes unacceptable performance degradation due to severe interference between AMs. Namely, MTL-HDC confronts a trade-off between memory overhead and performance degradation, as illustrated in Fig. 1.\n\nTo mitigate the interference while reducing the size of AMs, TP-HDC [18] randomly projects AMs into separate subspaces for learning. Meanwhile, IP-HDC [19] exploits redundant dimensionality in the high-dimensional (HD) space to extract informative dimensions of each task and merge them. Both of the prior works demonstrate the feasibility of applying MTL to HDC. However, there are still some open issues:\n\n1) The trade-off between information and interference should be further improved: In IP-HDC, informative dimensions of each task are extracted and merged into a shared AM, as shown in Fig. 2a. Specifically, a reservation ratio determines the proportion of dimensions reserved for each task. However, the reservation ratio ignores the trade-off between the reserved information for each task and interference among them. This issue causes performance degradation of the shared AM. 2) Existing model compression-based approaches to MTL-HDC ignore label information: Both TP-HDC and IP-HDC merge multiple AMs into a shared AM. Each dimension contains information from multiple tasks. SparseHD [20] observes that dropping some dimensions in the HD space does not pose a significant impact on accuracy. Intuitively, this indicates that we may establish an independent AM with a smaller size for each task, as shown in Fig. 2b. While SparseHD demonstrates a dramatic improvement in memory efficiency for many real-world tasks, we argue that this work ignores label information in the training dataset, which may lead to a suboptimal solution. This paper aims to establish an MTL-HDC framework that achieves a flexible and efficient trade-off between memory overhead and performance degradation. Our main contributions are summarized as follows:\n\n1) For a shared-AM-based MTL-HDC system, we propose Dimension Ranking for Effective AM Sharing (DREAMS) to merge multiple AMs while striking the best balance between information reserved for each task and interference among them. To rank each dimension's importance, we propose supervised dimension ranking (SDR) to consider label information in the training set. Moreover, different from IP-HDC, which solely uses the reservation ratio, we introduce an additional degree of freedom to DREAMS and improve its flexibility. Compared with TP-HDC and IP-HDC, DREAMS shows better accuracy of 11.74% on average. 2) For an independent-AM-based MTL-HDC system, we propose Dimension Ranking for Independent MEmory Retrieval (DRIMER) to store small-sized AM for each task independently. Moreover, we investigate the pros and cons of the shared AM and the independent AMs under two kinds of application scenarios. When the storage space on edge devices is large enough, independent AMs perform better than the shared AM since there is no interference between tasks. On the contrary, with insufficient storage space, the shared AM can reserve more discriminative information for each task and achieve higher performance. 3) By leveraging the combined strengths of the shared-AM and the independent-AM approaches, we propose a hybrid framework of Multi-Tasking HDC, called MulTa-HDC. Depending on the memory capacity of edge devices, MulTa-HDC can flexibly adjust the proportion of the shared AM and independent AMs by parameter tuning. To evaluate the effectiveness of MulTa-HDC, we extensively experiment on three benchmark datasets, including ISO-LET [21], UCIHAR [22], and MNIST [23], under two  task protocols for MTL [26]. Simulation results show that MulTa-HDC outperforms prior works and achieves an efficient trade-off between memory overhead and performance degradation. The rest of the paper is organized as follows. In Section 2, we review HDC algorithm and describe existing approaches that handle the problem of MTL-HDC. Section 3 illustrates the proposed hybrid framework of MulTa-HDC. The proposed DREAMS and DRIMER are presented in Section 4. Section 5 presents simulation results and evaluations. Finally, we conclude this paper in Section 6.\n\n\nPRELIMINARIES\n\n\nHDC Algorithm\n\nHDC operates on high-dimensional HVs, whose elements are with equally probable (-1)s and 1s. Fig. 3 shows the processing flow of a general HDC model. In the training phase, training data are first mapped into HVs and encoded. All encoded HVs corresponding to the same class are combined to generate a class HV. In the inference phase, HDC transforms a testing sample into a query HV. HDC computes the similarity between the query HV and all class HVs. The class with the highest similarity is selected as the prediction. In the following, we briefly introduce the procedure of training and inference in HDC. The details can be found in the recent review paper [24].\n\n\nMapping to the HD Space\n\nThe mapping step projects a feature vector F \u00bc \u00bdf 1 ; f 2 ; . . . ; f m to D-dimensional HVs, where m is the input dimension. Each entry of F is a feature component defined by a feature ID and a feature value, which are projected to the HD space through the Item Memory (IM) and the Continuous item Memory (CiM), respectively. IM \u00bc fID 1 ; ID 2 ; . . . ; ID m g maps the feature ID of f i to a randomly generated HV ID i 2 f\u00c01; 1g D . Note that any two HVs in IM are nearly orthogonal, i.e., Hamming\u00f0ID j ; ID k \u00de % 0:5; j 6 \u00bc k, where Hamming\u00f0\u00c1; \u00c1\u00de is the normalized Hamming distance.\n\nTo map continuous feature values into the HD space, CiM quantizes the range of values into ' levels. Each HV in CiM \u00bc fL 1 ; L 2 ; . . . ; L ' g; where L j 2 f\u00c01; 1g D , corresponds to the HV of the j th quantization level. To construct the CiM while preserving the spatial relation between quantization levels, two randomly generated HVs L 1 ; L ' 2 f\u00c01; 1g D are first assigned to the minimum value and the maximum value of each feature component, respectively. Then, D=l randomly selected bits of L 1 are flipped to generate an HV for the next level, and so on. The bit-flipping approach ensures a high correlation between adjacent levels. Therefore, the procedure for mapping the i th feature value of f i comprises discretization and looking up the corresponding vector S i from the CiM. After mapping each feature component of F , a set of two-vector pairs P \u00bc f\u00f0ID 1 ; S 1 \u00de; \u00f0ID 2 ; S 2 \u00de; . . . ; \u00f0ID m ; S m \u00deg is obtained.\n\n\nHD Encoding\n\nIn HD encoding, HDC utilizes two dimension-wise vector operations to encode the pairs in P to a single representative vector H 2 f\u00c01; 1g D :\nH \u00bc < X m i\u00bc1 S i \u00c8 ID i >;(1)\nwhere \u00c8 is defined as a binding operation to perform dimension-wise XOR between two HVs, \u00fe is defined as a bundling operation to compute dimension-wise addition, and h\u00c1i is defined as the majority function to bipolarize each component of HVs with the threshold 0.\n\n\nTraining Phase\n\nIn the training phase, all the same class's encoded training data are summed up to form a class HV. Assume that the j th class has n j samples associated with a set of encoded HVs fH 1 ; H 2 ; . . . ; H n j g, a class HV AM j 2 R D is computed as:\nAM j \u00bc X n j i\u00bc1 H i :(2)\nFor a C-class classification task, C class HVs denoted as fAM 1 ; AM 2 ; . . . ; AM C g are generated and stored in the AM.\n\n\nInference Phase\n\nIn the inference phase, a testing sample is encoded as a query vector Q 2 f\u00c01; 1g D by (1). Next, HDC evaluates the similarity between Q and all class HVs stored in AM. In this work, to achieve efficient similarity checking, class HVs are bipolarized to simplify the similarity metric as Hamming distance computation. The class with the highest similarity is the prediction:\nPrediction \u00bc argmax j Hamming Q;< AM j > \u00c0 \u00c1 \u00c0 \u00c1 :(3)\n2.2 Multi-task Learning-Based HDC (MTL-HDC)\n\n\nProblem Description of MTL-HDC\n\nAssume there are T tasks to be solved in the MTL-HDC system, and each of them contains C classes. As shown in Fig. 4, an AM table stores T isolated AMs. A AM M i i;j j represents the j th class HV of the i th task. However, this approach suffers from huge memory overhead when T increases. On the other hand, one can directly bundle all of the j th class HVs from T tasks fA AM M 1;j j ; A AM M 2;j j ; . . . ; A AM M T T ;j j g to get AM j , which is shared across tasks. However, the performance dramatically degrades due to interference between tasks. Hence, the problem of MTL-HDC is how to strike a balance between memory storage and performance degradation while transforming the AM table into a single AM whose size is independent of the number of tasks T .\n\n\nShared-AM Approach to MTL-HDC\n\nThe shared-AM approach manages to merge multiple AMs from different tasks into a shared one, as shown in Fig. 5a. To achieve AM sharing, TP-HDC [18] and IP-HDC [19] have been proposed to reduce the interference while merging AMs. TP-HDC exploits the model capacity by randomly projecting AMs into mutually orthogonal HD subspaces for merging. Meanwhile, IP-HDC analytically performs dimension ranking to filter out relatively inconsequential dimensions before AM sharing, leading to interference mitigation between tasks.\n\n\nIndependent-AM Approach to MTL-HDC\n\nThe independent-AM approach mitigates AM table's memory overhead by model compression, as shown in Fig. 5b. It reduces the required number of dimensions in each AM. With fewer dimensions brought by each AM, the total size of the compressed AMs is equal to that of a single uncompressed AM. Furthermore, no interference between tasks occurs due to isolation between the compressed AMs. Two prior works realize model compression for the independent-AM approach, including CompHD [25] and SparseHD [20]. CompHD compresses an HV into shorter segments and leverages orthogonality to decompose them with noise mitigated. SparseHD analytically measures variation in the class HVs for each dimension and discards inconsequential dimensions without significant impacts on classification results.\n\n\nComparisons of the Two Approaches\n\nThe shared-AM and independent-AM approaches utilize different properties of HDC to realize an MTL-HDC system, and we summarize them in Table 1. These two approaches are intrinsically the same, which are related to exploiting the model capacity of HDC efficiently. However, for the shared-AM approach, prior works [18] [19] do not fully consider the balance between the reserved information for each task and interference among them. For the independent-AM approach, the compression methods can be further improved by leveraging the label information in the training set. Moreover, we observe that the two approaches have different and complementary behaviors under two application scenarios, corresponding to different memory resources on edge devices. This means that neither approach is complete without the other. Therefore, a framework for MTL-HDC that can handle both issues is desirable. In the next section, we first analyze the strengths and weaknesses of shared-AM and independent-AM approaches. Next, we propose a hybrid framework for MTL-HDC based on both approaches to achieve efficient memory scalability, as shown in Fig. 5c.\n\n\nMULTI-TASK LEARNING FRAMEWORK FOR HDC\n\n\nObservation of Shared and Independent AMs\n\nWe experiment with both approaches on the Permuted UCI-HAR dataset, which is explained in Section 5.1. Five tasks are in the MTL-HDC system with HD dimensionality D ranging from 1K to 9K. Different HD dimensionalities imply various amounts of available memory capacity, corresponding to different application scenarios. Fig. 6 presents the classification accuracy of MTL-HDC based on shared-AM, independent-AM, and hybrid approaches. The independent-AM approach realized by our proposed DRIMER, which is discussed in Section 4.3, has stable performance in high HD dimensionality. Therefore, when designing an MTL system on edge devices with large memory capacity, the independent-AM approach is better than the shared one since there is no interference between independent AMs. However, we observe a critical point where a smaller HD dimensionality leads to a significant accuracy drop for the independent-AM approach. Specifically, when D < 4000, the compressed AMs can only retain an insufficient amount of discriminative information for classification. When devices only allow a maximum HD dimensionality lower than the critical point, the independent-AM approach is not reliable. On the contrary, the shared-AM approach based on our proposed DREAMS, which is discussed in Section 4.2, can mitigate the accuracy drop. The shared AM accommodates more information at the costs of interference between tasks. The positive effect brought by more reserved information is larger than the negative impact caused by interference.\n\nBased on the investigation, these two approaches have complementary behaviors under different application scenarios, as shown in Table 2. The independent-AM approach is more suitable for devices with large memory capacity since the independent AMs introduce no interference between tasks. The shared-AM approach is better for memory-constrained devices since they can reserve more discriminative information for each task, even if interference between tasks exists.\n\n\nProposed Hybrid Framework -MulTa-HDC\n\nWe propose MulTa-HDC to integrate two approaches for MTL-HDC into one hybrid framework to exploit their strengths under different application scenarios. Moreover, DREAMS and DRIMER are proposed to improve the performance of the shared-AM and independent-AM approaches, respectively. First, we introduce three parameters in the hybrid framework. We define the independent ratio a as:\na \u00bc D ind D ;(4)\nwhere D ind is the dimensionality assigned to independent AMs. In other words, each task in the system can store and retrieve its independent AM with D ind =T dimensionality. The remaining D sha dimensions in the system are assigned to the shared AMs:\nD sha \u00bc 1 \u00c0 a \u00f0 \u00deD:(5)\nThe proposed DRIMER takes a as input and generates independent AMs for each task. For the remaining D\u00f01 \u00c0 a\u00de dimensions, the proposed DREAMS takes the screening ratio b and the reservation ratio g as inputs to merge   \n\nwhere AM AM denotes the AM table. D sha is AM AM-dependent and related to both ratios b and g. Acc i \u00f0AM AM ind ind i i ; AM sha \u00de is the classification accuracy of the i th task based on the independent AMs AM AM ind ind i i and the shared AM AM sha . Throughout this paper, there are various kinds of AMs in MulTa-HDC. We summarize the notations for them in Table 3. A normal-faced and italic type AM denotes an ordinary AM, where AM j denotes the j th class HV in AM. A bold-faced and italic type AM AM denotes an AM table, where A AM M i i;j j denotes the j th class HV of the i th task in AM AM. Besides, mask HVs are binary vectors necessary for MulTa-HDC, which are introduced in Section 4. The second column of Table 3 implies whether an instance should be stored in devices for on-line inference. Fig. 7 shows how MulTa-HDC transforms an AM table into a hybrid AM. The processing flow is summarized in Algorithm 1. In the off-line phase, each task in the MTL-HDC system evaluates each dimension's importance in the HD space by the proposed SDR, which is introduced in Section 4.1. Based on SDR, DREAMS and DRIMER extract two disjoint sets of dimensions from the AM table and generate the shared AM AM sha and independent\nAMs {AM AM ind ind i i for i \u00bc 1; 2; . . . ; T }, respectively.\nIn the on-line inference phase, an unseen query vector Q i of the i th task comes in. MulTa-HDC checks the similarity between Q i and the hybrid AM to obtain similarity scores s ind i ;s sha i from the independent AMs and the shared AM, respectively. Finally, the class with the highest accumulated similarity score is the prediction.\n\nCompared with the case using either the shared-AM or the independent-AM approach, MulTa-HDC can flexibly adjust the proportion of two kinds of AMs to strike the right balance between memory storage and performance degradation. Note that the shared-AM and the independent-AM approach can be viewed as the subsets of MulTa-HDC when a is set as zero and one, respectively. In the following section, we present how to effectively generate the shared AM and the independent AMs by the proposed DREAMS and DRIMER, respectively.\n\n\nPROPOSED DREAMS AND DRIMER\n\nIn this section, we introduce DREAMS and DRIMER. DREAMS comprises two techniques: 1) Dimension Ranking, which evaluates the importance of each dimension, and 2) AM Sharing, which merges AMs from different tasks into a shared one. DRIMER also leverages two techniques: 1) Dimension Ranking, which performs the same functionality as in DREAMS, and 2) AM Retrieval, which independently stores AM of each task for retrieval. We first present how to rank dimensions while considering label information, followed by the details of DREAMS and DRIMER.  \n\n\nAlgorithm 1. MulTa-HDC Framework\nInput: AM table AM AM, a, b; g Output: {AM AM ind ind i i for i \u00bc 1; 2; . . . ; T }, AM sha ,K sha ; K dec , K ind Off-line phase: 1: A AM M ind ind ; K ind \u00bc DRIMER .Train( AM AM , a) 2: AM sha ; K sha ; K dec \u00bc DREAMS.Train( AM AM , a, b; g)\nOn-line phase for the query vector Q i of the i th task:\n3: s ind i \u00bc DRIMER.Test(Q i , AM AM ind ind i i ; K ind ) 4: s sha i \u00bc DREAMS.Test(Q i , AM sha ; K sha ; K dec ) 5: Prediction \u00bc arg max j \u00f0s ind i \u00bdj \u00fe s sha i \u00bdj\u00de 4.1 Supervised Dimension Ranking (SDR)\nSDR ranks the importance of each dimension in the HD space by leveraging label information, as shown in Fig. 8a and summarized in Algorithm 2. For the i th task in the MTL-HDC system, SDR takes as input A AM M i i in non-binarized representation and all encoded HVs from the training dataset, denoted H H i i \u00bc fH 1 ; H 2 ; . . . ; H N i g. To evaluate the importance of each dimension in a supervised manner, we define a score HV t 2 R D for each training data as:\nt \u00bc H A AM M i i;p p \u00c0 X j6 \u00bcp H A AM M i i;j j ;(7)\nwhere H is an encoded HV of a training sample belonging to the p th class. The operation denotes element-wise multiplication between two HVs. From the viewpoint of a specific dimension d, variation between AM j \u00bdd implies the change in class elements, which can be viewed as the statistics of AM. On the other hand, H\u00bdd can be regarded as a scaling factor that decides the influence of the d th dimension. Therefore, larger values in the score HV indicate more discriminative information useful for the HD model to correctly classify H. After aggregating N score HVs, an information HV I 2 R D is obtained as: if H n belongs to the j th class 6: t n n \u00bc t n n \u00fe H n A AM M i i;j j 7: else 8:\nI \u00bc X N n\u00bc1 t n ;(8)\nt n n \u00bc t n n \u00c0 H n A AM M i i;j j 9: end if 10: end for 11:\n\nI \u00bc I \u00fe t n n 12: end for where t n denotes the score HV derived from the n th training data. The information HV reveals the importance of each dimension in the HD space over the whole training dataset. Fig. 8b shows the histogram of the distribution of values in. We observe that each dimension contains imbalanced amounts of information, implying a chance to extract important information from the given task systematically. Therefore, those extracted dimensions can be further exploited to obtain a shared AM or independent AMs by DREAMS and DRIMER, respectively. The details are elaborated in Section 4.2 and 4.3, respectively.\n\n\nDREAMS for Shared AM\n\nThe goal of AM sharing is to alleviate the huge memory overhead brought by the AM table. However, interference between tasks in the shared AM leads to potential performance degradation. Hence, we propose DREAMS to generate a shared AM while striking the right balance between the reserved information for each task and interference among them, as shown in Fig. 9 and summarized in Algorithm 3. In the off-line phase, we propose a two-stage AM sharing mechanism by utilizing the screening ratio b and the reservation ratio g to improve the utilization of the model capacity of HDC. In the on-line inference phase, the task-specific AM from the shared AM is decoded for classification.\n\n\nTwo-Stage AM Sharing System\n\nIn the off-line phase, DREAMS first evaluates the importance of each dimension by SDR and generates the information HV I i for i \u00bc 1; 2; . . . ; T . Rather than directly merging multiple AMs into a shared one with severe interference among tasks, we retain the best representative dimensions with greater values in I i for each task. Those dimensions with smaller values in I i are considered as noise and thus should be screened out. To achieve this, we introduce several mask HVs generated by Algorithm 4 (MG) to indicate which dimensions are reserved in the shared AM and activated in the on-line phase. Based on the screening ratio b , reservation ratio g, and information HVs I i , DREAMS decomposes the AM merging process into two stages: 1) AM screening, and 2) AM sharing.\n\n1) AM screening. In the AM screening stage, the screening mask HVs, denoted as {K scr i for i \u00bc 1; 2; . . . ; T }, are obtained based on the information HV I i , the independent ratio a, and the screening ratio b, as shown in Fig. 9a. If a 6  2) AM sharing. \nAM mer \u00bc X T i\u00bc1 AM AM scr scr i i K mer i ;(9)\nwhere every row of AB corresponds to the element-wise multiplication of the vector B and the corresponding row in the matrix A. In the merging process described by (9), the merging mask HV K mer i only activates the d th dimensions in the i th screened AM AM AM scr scr i i if K mer i \u00bdd \u00bc 1: This mitigates the interference between tasks while merging the screened AMs. Furthermore, g imposes sparsity over dimensions in the merged AM AM mer . We define a redundancy mask HV K RD 2 \u00f00; 1\u00de Db as:\nK RD d \u00bd \u00bc 0; P T i\u00bc1 K mer i d \u00bd \u00bc 0 1; Otherwise ; &(10)\nwhich records the indices where none of the tasks in the MTL-HDC system stores information in AM mer . Since the d th dimension of AM mer , where K RD \u00bdd \u00bc 0, is always inactivated in the inference phase, it can be removed from AM mer by Algorithm 5 (EAC). A more compact shared AM AM sha with less memory overhead is obtained. Note that D sha \u00bc jjK RD jj 0 is exactly the number of dimensions assigned to the shared AM AM sha in MulTa-HDC.\n\n\nAM Decoding for Inference\n\nIn the off-line phase, a set of {K dec i for i \u00bc 1; 2; . . . ; T } is already generated based on K RD and {K mer i for i \u00bc 1; 2; . . . ; T }, as shown in Fig. 9c. Assume that the MTL-HDC system is aware of which task should be inferenced at runtime [36], [37]. In the on-line inference phase, since multiple AMs are merged into a shared one AM sha , DREAMS uses K dec i to decode the task-specific AM of the i th task AM AM dec dec i i , as shown in Fig. 9e.\n\n\nAlgorithm 4. Mask Generator (MG)\n\nInput: Information HV I, r start ; r end Output: Mask HV K 1: Initialize D length(I); K 0 D ; 2: Sort I in ascending order to obtain I sort 3: Derive start threshold value \" start : \" start \u00bc I sort \u00bdD \u00c2 \u00f01 \u00c0 r start \u00de 4: Derive end threshold value \" end :\n\" end \u00bc I sort \u00bdD \u00c2 \u00f01 \u00c0 r end \u00de 5: for ( d d \u00bc 1: D D ) do 6:\nif \" start ! I\u00bdd ! \" end 7:\n\nK\u00bdd \u00bc 1 8: end if 9: end for As for the query HV Q i of the i th task, K sha i is also generated in the off-line phase to activate the reserved dimensions in Q i , as shown in Fig. 9d \n\n\nThe Flexibility of the Two-Stage AM Sharing\n\nThe two-stage AM sharing mechanism provides two degrees of freedom to improve the flexibility of DREAMS. We observe that D sha \u00bc jjK RD jj 0 of DREAMS depends on the \u00f0b; g\u00de pair, but not \u00f0b \u00c2 g\u00de only. Specifically, different combinations of \u00f0b; g\u00de pairs may result in the same D sha , but with different numbers of reserved dimensions for each task, which is Dbg. Compared with the merging method in IP-HDC [19] as the particular case of DREAMS with b \u00bc 1, the flexibility brought by the two-stage mechanism enable DREAMS to achieve a better balance between interference and information by searching the optimal \u00f0b; g\u00de pair. We validate the effectiveness of DREAMS in Section 5.2.\n\n\nMTL Scenario With Varied Number of Classes\n\nIn a real-world MTL-HDC system, AM's size in each task depends on the number of classes it contains. DREAMS retains important dimensions in each AM and merge them into one. However, the merging procedure corresponding to Line 8 of Algorithm 3 is not feasible when AM's size differs. Specifically, the screened AM in DREAMS has the size of ( C i \u00c2 Db ), where C i and b denote the number of classes of the i th task and the screening ratio, respectively. When C i 6 \u00bc C j for any i 6 \u00bc j, the screened AM cannot be merged directly.\n\nSuppose that in an MTL-HDC system, the i th task does not contain the j th class (i.e., AM AM scr scr i i;j j does not exist). As an intuitive approach, a pseudo class HV which corresponds to the j th class of the i th task can be a \"zero\" HV, which contains zeros in all HD dimensions. However, this approach causes imbalanced noises of each class from the i th task, leading to severe performance degradation. The imbalanced noises are in favor of the j th class in other tasks, since they are not affected by the noise from the i th task. Instead, to simulate the noise caused by naturally generated HVs, we add pseudo classes to the i th task by randomly-weighted averaging the inherent class HVs in its AM. For example, in Fig. 10, the MTL system originally has two tasks, where task 1 has three classes, task 2 has only two classes. The pseudo class HV of task 2, which is denoted as AM 2;3 , is the randomly-weighted average of the other two class HVs. Note that pseudo classes are excluded from the similarity checking process in the inference phase, based on the prior knowledge on the number of classes. The method seems Fig. 10. Illustration of how to create a pseudo class HV by randomlyweighted averaging the inherent class HVs in AM. na\u00efve, but we found that it empirically works well, as illustrated in Section 5.4.3.\n\n\nDRIMER for Independent AMs\n\nDRIMER obtains an independent and compressed AM by discarding inconsequential dimensions for each task. Different from the shared AM, the independent AMs have no interference between tasks. To distinguish from the screened AMs AM AM scr scr i i , the independently stored AMs is referred to as the independent AMs AM AM ind ind i i , for i \u00bc 1; 2; . . . ; T . Fig. 11 illustrates the processing flow of DRIMER, which is summarized in Algorithm 6.\n\n\nIndependent AMs Based on SDR\n\nTo drop inconsequential dimensions in AM, DRIMER uses SDR to obtain the information HVs {I i for i \u00bc 1; 2; . . . ; T } in off-line phase. Then, the first aD dimensions with the highest score values in I i are selected to be retained in the compressed AM. The independent mask HV K ind i is obtained based on the independent ratio a and I i . As shown in Fig. 11a, K ind i decides which dimensions are retained to form the i th independent AM AM AM ind ind i i , reducing the memory size by a factor of \u00f01 \u00c0 a\u00de.\n\nIn the on-line inference phase, Fig. 11b shows the retrieval of the compressed AM for the specific task. \n\nBy considering the label information in the training set, SDR discovers more representative dimensions than prior works. This makes DRIMER reduce information loss after compressing independent AMs. We validate the effectiveness of DRIMER in Section 5.3.\n\n\nRetraining Independent AMs\n\nAfter compression, the performance degrades due to information loss. We refer to the retraining mechanism in [31] with some modifications to compensate for the quality loss. The retraining procedure first checks the predictions of the independent AMs on the training set by (12). (i) If a query vector Q i , which belongs to the j th class of the i th task, is correctly classified, nothing changed. (ii) If Q i is wrongly predicted as the k th class, we adjust the class HV in the independent AM AM AM ind ind i i . The adjustment process subtracts Q ind i from AM AM ind ind i i;k k and adds it to AM AM ind ind i i;j j :\nAM AM ind ind i i;k k \u00bc AM AM ind ind i i;k k \u00c0 Q ind i AM AM ind ind i i;j j \u00bc AM AM ind ind i i;j j \u00fe Q ind i ( :(13)\nThe procedure iterates over the training set until the model converges, which is defined as the fact that the validation accuracy does not improve in the last five iterations.\n\n\nEXPERIMENTS AND EVALUATIONS\n\n\nExperimental Setup\n\nThis section evaluates the performance of proposed DREAMS, DRIMER, and MulTa-HDC by using three datasets, including image classification, voice recognition, and activity recognition tasks. \"Permuted MNIST\" [23] is a standard task protocol for MTL [26] [28] [29]. Permuting means that the input representation is randomly permuted for each task, but all MTL system tasks are standard (10-class) MNIST classification tasks. Permuted ISOLET [21] and Permuted UCIHAR [22] share the same concept as Permuted MNIST. In ISOLET, 150 subjects speak the name of 26 letters of the alphabets, and 617 features of voice signals are extracted. UCIHAR recognizes six human activities based on 3-axial linear acceleration and angular velocity at a constant rate of 50 Hz.\n\nFor hyperparameter settings, the HD dimension D and quantization level l is set as 10K and 16, respectively, where the performances of all tasks saturate. Mapping and encoding modules are shared across tasks. Besides, accommodating multiple tasks in the MTL-HDC system leads to an accuracy drop. Therefore, all models are retrained for 50  for (i \u00bc 1:T) do 2:\nI i \u00bc SDR(A AM M i i , H H i i ) 3: K ind i \u00bc MG\u00f0I i ; 0; a=T \u00de 4: AM AM ind ind i i \u00bc EAC\u00f0K ind i ; A AM M i i \u00de 5:\nend for On-line phase for the query vector Q i of the i th task: 6:\nQ ind i \u00bc EAC\u00f0K ind i ; Q i \u00de 7:\nSimilarity Checking (Q ind , AM AM ind ind i i ) iterations before evaluation. The evaluation metric used in this paper is the averaged classification accuracy defined as:\nAccuracy \u00bc 1 T X T i\u00bc1 Acc i AM AM ind ind i i ; AM sha \u00c0 \u00c1 :(14)\nThe hyperparameter settings for HDC and evaluated datasets are summarized in Table 4.\n\n\nEvaluation of DREAMS\n\nThe proposed DREAMS strikes a balance between information reserved for each task and interference in the shared AM, improving the utilization of the model capacity. To validate the effectiveness of DREAMS, we experiment it on the Permuted ISOLET dataset. The number of tasks and classes are set as T \u00bc 5 and C \u00bc 26, respectively.\n\nFirst, the utilization of the model capacity is investigated. We fix the shared dimensions D sha at 7K. Fig. 12 shows the results, where the horizontal axis is the screening ratio b. Specifically, for each b value in Fig. 12, we exhaustively search for a reservation ratio g such that the specified (b; g) pair result in D sha being 7K with small variances. To quantize the amount of reserved information from each task and interference, we split up the dot product in (11) into two parts, including signal and noise. From the viewpoint of a query vector Q i of the i th task, signal and noise are defined as:  \nSignal \u00bc max j EAC K sha i ; Q i \u00c0 \u00c1 \u00c1 EAC\nSignal measures information reserved in the shared AM, while noise measures the remaining part. Noise results from interference from other tasks. In Fig. 12, the case with b \u00bc 1 can be viewed as a particular case of DREAMS, corresponding to IP-HDC [19]. Meanwhile, the other cases drop D \u00c2 \u00f01 \u00c0 b\u00de inconsequential dimensions before AM sharing. The results show that IP-HDC reserves the most information; however, interference is also significant, leading to noisy similarity checking. On the other hand, two degrees of freedom make DREAMS strike the right balance between information and interference. Therefore, DREAMS is more flexible than IP-HDC in terms of utilization of the model capacity, leading to a 6.91% of accuracy improvement (the case with b \u00bc 0:75).  Fig. 12. Illustration of the effectiveness of DREAMS in balancing information (signal) and interference (noise) when D sha \u00bc 7K:   13 compares the performance of DREAMS and IP-HDC at different D sha . They have negligible difference for D sha < 4K. However, for IP-HDC, when the size of the HD space D sha grows, interference grows faster than information. The severe interference hinders IP-HDC from efficiently exploiting model capacity. Therefore, a larger HD space does not improve the performance of IP-HDC. Instead, it leads to worse performance than a smaller HD space, as shown in Fig. 13a.\n\nIn contrast, DREAMS is more robust when D sha > 4K. For DREAMS, information and interference both grow slower, and the two-stage mechanism keeps information larger than interference steadily. Therefore, DREAMS utilizes the model capacity more efficiently, outperforming IP-HDC by 11.74% in accuracy (D sha \u00bc 8.5 K).\n\n\nEvaluation of DRIMER\n\nWe compare DRIMER with the state-of-the-art compression methods, including CompHD [25] and SparseHD [20], on the Permuted UCIHAR dataset. The number of tasks and classes are set as T \u00bc 5 and C \u00bc 6, respectively. While D is fixed at 10K, these methods aim to minimize the retained number of dimensions D retain in each compressed AM while preserving accuracy.\n\nThe results are shown in Fig. 14. First, the performance of CompHD degrades drastically with lower D retain , since its effectiveness for noise mitigation is based on highdimensional spaces. Next, DRIMER and SparseHD have a negligible difference for D retain > 3K, but DRIMER shows better robustness in low D retain . For D retain \u00bc 1K, DRIMER provides a 3.3% of accuracy improvement. Note that there is a critical point (D retain \u00bc 1K) for DRIMER, where lower D retain leads to a severe accuracy drop since the retained information is not enough for accurate classification.\n\n\nEvaluation of MulTa-HDC Framework\n\nIn this section, we demonstrate the scalability of MulTa-HDC, we conduct experiments over the \"Permuted\" task protocol. For comparison purposes, apart from the shared-AM and independent-AM approaches, we introduce the ideal benchmark and baseline method described in [19]. For the ideal benchmark, the complete AM table is stored, leading to the highest accuracy at the highest storage space costs. On the contrary, the baseline method directly averages class HVs of the same class in the AM table:\nAM j \u00bc X T i\u00bc1 A AM M i i;j j(17)\nwhere j \u00bc 1; . . . ; C. The baseline method causes severe interference between tasks, leading to the lowest accuracy.\n\nTo evaluate the effectiveness of MulTa-HDC in accommodating a growing number of tasks T , we set T \u00bc 1; 2; . . . ; 20, corresponding to the application scenarios of edge devices with memory resources ranging from abundant to highly-constrained. For each method, the storage requirement for AM is fixed at CD, where C is the number of classes and D is the dimensions in HV, by parameter tuning. This fixed storage requirement is exactly the memory overhead of the baseline method. The MTL system aims at accommodating more tasks while preserving classification accuracy. We present experimental results in Fig. 15.  [20] and CompHD [25]. \n\n\nApplication Scenario 1: Resource-Abundant Devices\n\nIn the scenario of edge devices with abundant memory resources, we consider an MTL system with a small number of tasks \u00f0T < 12\u00de. As shown in Fig. 15, DRIMER consistently outperforms other two independent-AM approaches.\n\nFor the Permuted UCIHAR in T \u00bc 11, DRIMER provides 3.83% and 6.67% accuracy improvements compared with SparseHD and CompHD, respectively. This is because DRIMER can retain more significant dimensions for classification by considering the label information in the training set. The performance of CompHD drastically degrades when T grows. Since larger T results in fewer dimensions retained by each independent AM, the orthogonality in HD space does not guarantee the noise reduction. In this scenario, MulTa-HDC sets a \u00bc 1 and assign all of the dimensions to independent AMs, corresponding to DRIMER.\n\n\nApplication Scenario 2: Resource-Limited Devices\n\nThe performances of DRIMER and SparseHD are stable in the previous scenario. However, in real-world scenarios, each task would not be granted many memory resources. MulTa-HDC can leverage DREAMS to accommodate more discriminative information to deploy models more reliably and cost-effectively. As shown in Figs. 15a and 15b, the gap between the accuracy of DREAMS and DRIMER is closed as T increases. For example, in the Permuted MNIST, gaps between the accuracy provided by DRIMER and DREAMS are 2.89% (T \u00bc 5) and 1.36% (T \u00bc 20), respectively. DREAMS even outperforms DRIMER on the Permuted UCIHAR dataset when T > 16, implying that the degree of positive effects brought by more information reserved for each task is higher than that of negative impacts brought by interference. By leveraging both AM approaches, MulTa-HDC can adaptively adjust the three ratios a; b; and g to achieve the best performance in both scenarios. When memory resources are abundant, corresponding to the application scenario 1, higher a prevents interference between tasks, leading to stable classification accuracy. Lower a in the system implies more dimensions are reserved for each task in the shared AM. Therefore, MulTa-HDC can leverage the combined strengths of independent and shared AMs. Based on the experimental results, MulTa-HDC mitigates the performance degradation when the resources on edge devices are highly constrained. In the Permuted UCIHAR tasks when T \u00bc 20, MulTa-HDC uses the configuration of \u00f0a; b; g\u00de \u00bc \u00f00:4; 0:95; 0:05\u00de to achieve 6.95% and 8.23% of accuracy improvements, compared with SparseHD and CompHD, respectively.\n\n\nMTL-HDC System With Datasets From Heterogeneous Domains\n\nTo further demonstrate the generality of MulTa-HDC, we conduct experiments over \"Split MNIST\" task protocol, which is adopted in many MTL-related works [2], [26], [27]. It requires learning to classify MNIST digits sequentially. In Split MNIST, we split 10 digits into 2 disjoint sets, each of which corresponds to a subtask that contains 5 labels. Split ISOLET and Split UCIHAR share the same concept as Split MNIST.\n\nIn Split ISOLET, 26 labels are split into 8 disjoint sets, each of which corresponds to a subtask that contains 3 labels. Split ISOLET randomly drops two labels since the number of classes is not divisible by 3. Similarly, in Split UCIHAR, there are 3 subtasks, and each subtask contains 2 labels. The MTL-HDC system starts with learning 8 subtasks in Split ISOLET, corresponding to the section with blue background color in Fig. 16. After that, the system sequentially accommodates the subtasks in Split MNIST (green section in Fig. 16) and Split UCIHAR (brown section in Fig. 16). The HDC setup and evaluated datasets are summarized in Table 5. Fig. 16 presents the experimental results. The x-axis denotes the number of subtasks in the MTL-HDC system. Note that the dotted black line indicates the ideal benchmark, which is the accuracy upper bound. Fig. 16 shows that MulTa-HDC performs slightly worse than the ideal benchmark, but better than other methods. While SparseHD and CompHD demonstrate dramatic improvements in memory efficiency of the MTL-HDC system, they still suffer from significant drop in accuracy. Instead, MulTa-HDC consistently outperforms prior works and shows scalability when the number of subtasks grows. Note that even in the transitions of training procedure of subtasks from heterogeneous domains (speech, image, and human activity), MulTa-HDC still preserves classification accuracy. The results imply that MulTa-HDC can not only accommodate datasets from multiple domains, but also handle multi-task scenarios where the subtasks do not have the same number of labels. Based on these experiments, MulTa-HDC can efficiently handle both protocols defined in [26], including the \"Split\" and the \"Permuted\" task protocols.\n\n\nANALYSIS OF COMPLEXITY\n\n\nMemory Complexity\n\nThis section provides quantitative evidence that MulTa-HDC achieves an efficient trade-off between memory overhead and performance degradation. In the training phase of MulTa-HDC, it finds task-specific mask HVs to encode task information and store the AMs compactly. In the inference phase, mask HVs act as auxiliary HVs to decode task-specific information from AMs with EAC operations, as shown in Algorithm 3 and 6, before Hamming distance computation. Apart from AMs, we should consider the extra storage space for the mask HVs.\n\nTo analyze the memory complexity of MulTa-HDC, we conduct experiments on Permuted UCIHAR with 12 tasks and calculate the storage requirement. The experimental setups are summarized in Table 6. Except for the ideal benchmark, we fine-tuned the parameters of all methods to fix the storage requirement for AMs at 60Kb (10K bits for each of the 6 classes in UCIHAR, not including the storage space for auxiliary HVs). Specifically, sparsity ratio (s) of SparseHD equals \u00f01 \u00c0 1 T \u00de, since SparseHD reduces the storage overhead of a single task from CD to CD\u00f01 \u00c0 s\u00de. Similarly, compression factor (r) of CompHD is exactly equal to T , since CompHD reduces the storage overhead of a single task from CD to CD T . As for parameters settings of Multa-HDC, we follow the optimization problem described in Equation (6) of Section 3.1. Table 7 shows the memory breakdown analysis, including the memory complexity of AM and auxiliary HVs for additional processing before computing the Hamming distance. For MulTa-HDC and SparseHD, these auxiliary HVs are mask HVs. As for CompHD, these auxiliary HVs are mutually orthogonal modulating HVs. In Table 7, S and r denote the sparsity ratio of SparseHD and the compression factor of CompHD, respectively. Table 7 shows that while the ideal benchmark achieves the best performance at the highest memory overhead cost, the baseline method shows the opposite behavior. Neither of them provides satisfactory performance in real-world applications. Conversely, SparseHD and CompHD demonstrate dramatic improvements in memory efficiency. However, there is still an over 9% accuracy drop compared to the ideal benchmark. Meanwhile, MulTa-HDC can trade less than 5% accuracy drop for saving 77% memory overhead, achieving an efficient trade-off. Note that the mask HVs of MulTa-HDC are highly sparse, and thus many offthe-shelf compression techniques are applicable. In this case, the ratio of 1's in mask HVs is around 5%, and we estimate the required storage of mask HVs can be reduced from 302 Kb to 86.4 Kb [30].\n\n\nDiscussion of Computational Complexity\n\nWhen analyzing the computational complexity of MulTa-HDC, the extra processing for decoding should be considered. Table 8 shows the approximated estimation of the computational complexity in terms of bit-level operation counts for one classification. Note that the computational analysis of EAC operations in MulTa-HDC is not straightforward due to its architecture-dependent implementation. Therefore, we use EAC(x) to denote the bit-level operation count for EAC with a x-dimension input HV. The execution time is also reported based on an AMD Ryzen Threadripper 2990WX 32-core processor.\n\nAll methods in Table 8 have the same computational requirement for HDC encoding, as shown in Equation (1). Meanwhile, the encoding process accounts for more than 95% energy consumption in a binary HDC model during inference [31]. To present a clear comparison, we only   z The sparse binary mask HVs are compressed [30].\n\nconsider the computation required after the query HV has been encoded. Table 8 shows that the execution time of MulTa-HDC is the highest. However, the crucial idea here is that the encoding module is the main computational bottleneck. Therefore, the computational overhead caused by the decoding in MulTa-HDC is negligible.\n\n\nCONCLUSION\n\nThis paper proposes a hybrid MulTa-HDC framework for MTL-HDC based on shared-AM and independent-AM approaches. Proposed DREAMS shows higher utilization of model capacity for the shared-AM approach while outperforming the prior work by 11.74% of accuracy. For the independent-AM approach, proposed DRIMER compresses AMs in a supervised manner and provides a 3.3% of accuracy improvement compared with SparseHD. Moreover, MulTa-HDC helps develop a more flexible framework for MTL-HDC by adjusting the proportion of the independent AMs and the shared AM depending on the memory capacity on device. Simulation results show that MulTa-HDC can realize the best trade-off between memory overhead and performance degradation, making the MTL-HDC system suitable for the future of IoT. \u00c3 EAC(x) denotes the operation count for EAC with a x-dimension input. y popcount(x) denotes the bit-level operation count required for counting the number of 1's in a x-bit sequence.\n\nFig. 1 .\n1The trade-off between memory overhead and performance degradation under the scenario of MTL-HDC.\n\nFig. 2 .\n2Illustration of two types of AMs used in MTL-HDC. (a) The shared AM merges multiple AMs into one AM. (b) The independent AMs apply model compression to the original AMs.\n\nFig. 3 .\n3The processing flow of a general HDC model.\n\nFig. 4 .\n4Problem description of MTL-HDC. Fig. 5. Approaches to MTL-HDC. (a) Shared-AM approach based on merging multiple AMs into one. (b) Independent-AM approach based on applying model compression to AMs. (c) The proposed hybrid framework of MTL-HDC based on both approaches.\n\n\nt: aD \u00fe D sha b; g; AM AM \u00f0 \u00de\u00bcD;\n\nFig. 7 .\n7Processing flow of the proposed MulTa-HDC framework.\n\nAlgorithm 2 .\n2Supervised Dimension Ranking (SDR) Input: AM of the i th task A AM M i i , Encoded HVs from the training set of the i th task H H i i \u00bc fH 1 ; H 2 ; . . . ; H N i g Output: Information HV I 1: Initialize I 0 D ; 2: for ( n n \u00bc 1:N N ) do 3:Initialize t n n \u00bc 0 D 4: for ( j j \u00bc 1:C C ) do 5:\n\nFig. 8 .\n8(a) Processing flow of SDR. (b) An example of the distribution of values in the information HV I from SDR.\n\n\u00bc 0 ,\n0then a total of D \u00c2 a=T dimensions in A AM M i i are retained in the independent AM for the i th task. Meanwhile, K scr i indicates the most significant Db dimensions in the remaining part of A AM M i i . The screening ratio b determines the number of dimensions that are retained for the next stage. In other words, we only extract and concatenate Db important dimensions by Algorithm 5 (EAC) and discard others. The screened AMs {AM AM scr scr i i for i \u00bc 1; 2; . . . ; T } and the screened information HVs {I scr i for i \u00bc 1; 2; . . . ; T } are obtained based on K scr i and are transferred to the next stage.\n\n\nFig. 9b illustrates the AM sharing stage. DREAMS uses the reservation ratio g to determine the number of important dimensions that are reserved in the merged AM AM mer . The merging mask HVs, denoted as {K mer i for i \u00bc 1; 2; . . . ; T }, are obtained based on g and the screened information HVs from the previous stage {I scr i for i \u00bc 1; 2; . . . ; T }. They help merge the screened AMs {AM AM scr scr i i for i \u00bc 1; 2; . . . ; T } into AM mer :\n\nFig. 9 .\n9The processing flow of DREAMS. (a) The AM screening stage, corresponding to lines 2-6 in Algorithm 3. (b) The AM sharing stage, corresponding to line 7-8 and line 15 in Algorithm 3. (c) Derive K dec i , corresponding to lines 9-13 and line 16-17 in Algorithm 3. (d) Derive K sha i , corresponding to line 18 in Algorithm 3. (e) AM decoding for the on-line inference, corresponding to lines 20-22 in Algorithm 3.\n\n\ndimensions in the query HV Q i that are activated for inference. Those activated dimensions are extracted and become Q ind i . The similarity value s ; for j \u00bc 1; . . . ; C:\n\nFig. 11 .\n11The processing flow of DRIMER. (a) Generate the independent AMs in the off-line phase, corresponding to lines 1-5 in Algorithm 6. (b) Retrieve task-specific AM for inference, corresponding to lines 6-7 in Algorithm 6.\n\nAlgorithm 6 .\n6Dimension Ranking for Independent Memory Retrieval (DRIMER) Input: AM table AM AM, a, T Encoded HVs from the training set of each task {H H i i for i \u00bc 1; 2; . . . ; T } Output: {AM AM ind ind i i for i \u00bc 1; 2; . . . ; T } {K ind i for i \u00bc 1; 2; . . . ; T } Off-line phase: 1:\n\nFig. 13 .\n13Illustration of the effectiveness of DREAMS in improving utilization of the model capacity. (a) IP-HDC [19]. (b) DREAMS.\n\nFig.\nFig. 13 compares the performance of DREAMS and IP-HDC at different D sha . They have negligible difference for D sha < 4K. However, for IP-HDC, when the size of the HD space D sha grows, interference grows faster than information. The severe interference hinders IP-HDC from efficiently exploiting model capacity. Therefore, a larger HD space does not improve the performance of IP-HDC. Instead, it leads to worse performance than a smaller HD space, as shown in Fig. 13a. In contrast, DREAMS is more robust when D sha > 4K. For DREAMS, information and interference both grow slower, and the two-stage mechanism keeps information larger than interference steadily. Therefore, DREAMS utilizes the model capacity more efficiently, outperforming IP-HDC by 11.74% in accuracy (D sha \u00bc 8.5 K).\n\nFig. 14 .\n14Illustration of the effectiveness of DRIMER, compared with Spar-seHD\n\nFig. 15 .\n15Averaged classification accuracy between MulTa-HDC and prior works on different Permuted datasets: (a) MNIST (b) ISOLET, and (c) UCI-HAR. The storage space for AM is fixed at C D , where C and D denote the number of classes and the HD dimensionality, respectively.\n\nFig. 16 .\n16Compare MulTa-HDC with prior works under the \"Split\" task protocol. The top plot shows the averaged accuracy on subtasks in Split ISOLET. The middle plot and bottom plot indicate the accuracy on subtasks in Split MNIST and Split UCIHAR, respectively. Note that MulTa-HDC with zero pseudo class HVs mentioned in Section 4.2.4 leads to severe performance degradation in the Split ISOLET and Split MNIST subtasks. The Split UCIHAR subtasks are unaffected by the pseudo class since they have the minimum number of classes in this MTL system.\n\nTABLE 1\n1Comparisons of Existing Approaches to MTL-HDC Fig. 6. Averaged accuracy of different MTL-HDC approaches to the Permuted UCIHAR dataset.\n\nTABLE 2\n2Comparisons of Shared-AM and Independent-AM Approaches to MTL-HDCApproaches \nLow HD Dimensionality \nHigh HD \nDimensionality \nShared AM \nMore reserved \ninformation \n\nSevere interference \n\nIndependent \nAM \n\nInsufficient retained \ninformation \n\nStable due to no \ninterference \n\n\nTABLE 3\n3Notations for MulTa-HDC\n\n\n. The activated and concatenated dimensions Q sha i is obtained by Algorithm 5 (EAC). Note that jjK sha i jj 0 \u00bc Dbg, meaning that a total of Dbg dimensions from the i th task are reserved in the shared AM AM sha and decoded as AM AM dec dec i i . With Q sha for i \u00bc 1; 2; . . . ; T}, AM sha , {K dec i for i \u00bc 1; 2; . . . ; T }, D sha \u00bc kK RD k 0 Input: Mask HV K, AM or HV Q Output: AM EAC or Q EAC 1: Initialize AM EACi \n\nand AM AM dec dec \ni i , \nDREAMS evaluates the similarity values s sha \n\ni \n\n2 R C between \nQ sha \n\ni \n\nand the decoded class HVs stored in AM AM dec dec \ni i : \n\ns sha \n\ni \n\nj \n\u00bd \u00bc \nX Dbg \n\nd\u00bc1 \n\nQ sha \n\ni \n\nd \n\u00bd <AM AM dec dec \ni i;j j d \n\u00bd > for j \u00bc 1; . . . ; C: \n\n(11) \n\nAlgorithm 3. Dimension Ranking for Efficient AM Shar-\ning (DREAMS) \n\nInput: AM table AM AM, a, b; g, T \nEncoded HVs from the training set of each task \n{H H i i for i \u00bc 1; 2; . . . ; T } \nOutput: {K sha \n\ni \n\n1: Initialize K RD \n0 Db ; AM mer 0 \u00bdC;Db \nOff-line phase: \n2: \nfor ( i i \u00bc 1:T T ) do \n3: \nI i \u00bc SDR(A AM M i i , H H i i ) \n4: \nK scr \n\ni \n\n\u00bc MG\u00f0I i ; a=T ; a=T \u00fe b\u00de \n5: \nAM AM scr scr \ni i \u00bc EAC\u00f0K scr \ni ; A AM M i i \u00de \n6: \nI scr \n\ni \n\n\u00bc EAC\u00f0K scr \ni ; I i \u00de \n7: \nK mer \n\ni \n\n\u00bc MG\u00f0I scr \ni ; 0; g\u00de \n8: \nAM mer \u00bc AM mer \u00fe A AM M scr \ni e \nK mer \n\ni \n\n9: \nfor (d \u00bc 1: Db Db ) do \n10: \nif K mer \n\ni \n\n\u00bdd 6 \u00bc 0 \n11: \nK RD \u00bdd \u00bc 1 \n12: \nend if \n13: \nend for \n14: \nend for \n15: \nAM sha \u00bc EAC\u00f0K RD ; AM mer \u00de \n16: \nfor ( i i \u00bc 1:T T ) do \n17: \nK dec \n\ni \n\n\u00bc EAC\u00f0K RD ; K mer \n\ni \n\n\u00de \n18: \nK sha \n\ni \n\n\u00bc MG\u00f0I i ; 0; bg\u00de \n19: \nend for \nOn-line phase for the query HV Q i of the i th task: \n20: \nQ sha \n\ni \n\n\u00bc EAC\u00f0K sha \ni ; Q i \u00de \n21: \nAM AM dec dec \n\ni i \n\n\u00bc EAC\u00f0K dec \ni ; AM sha \u00de \n22: \nSimilarity Checking (Q sha \ni , AM AM dec dec \ni i ) \nAlgorithm 5. Extract And Concatenate (EAC) \n\n0 \u00bdC;jjKjj 0 ; Q EAC \n0 jjKjj 0 ; count \u00bc 1 \n2: if input is AM \n3: \nfor ( j j \u00bc 1: C) do \n4: \nAM EAC \n\nj \n\n\u00bc EAC\u00f0K; AM j \u00de \n5: \nend for \n6: else \n7: \nfor ( d d \u00bc 1: length(K)) do \n8: \nif K[d] 6 \u00bc 0 \n9: \nQ EAC \u00bdcount \u00bc Q\u00bdd \n10: \ncount \u00bc count \u00fe 1 \n11: \nend for \n12: end if \n\n\n\nTABLE 4 Experimental\n4Setups\n\nTABLE 5 Experimental\n5Setups Under Split Task Protocol Hyperparameter Settings for HDC Evaluated Datasets under \"Split\" Task ProtocolHD Dimension (D D) \n10,000 \n\nLevel (') \n16 \n\nRetraining Times \n50 \n\nDatasets \nNumber \nof \nSubtasks \n\nNumber of Labels \nin each Subtask \n\nSplit ISOLET \n8 \n3 \n\nSplit MNIST \n2 \n5 \n\nSplit UCIHAR \n3 \n2 \n\n\n\nTABLE 6 Experimental\n6Setups for Complexity Analysis\n\nTABLE 7 Storage\n7Requirement of MulTa-HDC and Prior Works MulTa-HDC 90.35 C\u00f0aD \u00fe D sha \u00de T \u00f02D \u00fe D sha \u00de 146.4 zFramework \nAccuracy \n[%] \n\nAM \nComplexity \n\nAuxiliary HVs \nComplexity \n\nTotal \n[Kb] \n\nIdeal \nBenchmark \n\n94.92 \nCTD \n-\n720 \n\nSparseHD \n85.67 \nCTD\u00f01 \u00c0 S\u00de \nTD \n117.1 z \n\nCompHD \n83.02 \n\nCTD \nr \n\nD \n70 \n\nBaseline \n51.63 \nCD \n-\n6 0 \n\n\n\nTABLE 8\n8Bit-Level Operation Counts and Execution Time Per Inference\nIEEE TRANSACTIONS ON COMPUTERS, VOL. 70, NO. 8, AUGUST 2021 Authorized licensed use limited to: National Taiwan University. Downloaded on August 05,2021 at 07:07:40 UTC from IEEE Xplore. Restrictions apply.\nAuthorized licensed use limited to: National Taiwan University. Downloaded on August 05,2021 at 07:07:40 UTC from IEEE Xplore. Restrictions apply.\n\" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.\nACKNOWLEDGMENT This work was supported in part by the Ministry of Science and Technology of Taiwan under Grant MOST 109-2221-E-002-175. Cheng-Yang Chang and Yu-Chuan Chuang contributed equally to this work.Cheng-Yang Chang (Student Member, IEEE) received the BS degree in electronic engineering from National Taiwan University, Taipei, Taiwan, in 2019, where he is currently working toward the PhD degree at the Graduate Institute of Electronics Engineering. His research interests include hyperdimensional computing, multi-task learning, and collaborative learning.Yu-Chuan Chuang (Student Member, IEEE) received the BS degree in electronic engineering from National Taiwan University, Taipei, Taiwan, in 2018 , where he is currently working toward the PhD degree at the Graduate Institute of Electronics Engineering. His research interests include hyperdimensional computing, low-power design, and VLSI implementation. An-Yeu (Andy) Wu (Fellow, IEEE) received the BS degree in electrical engineering from NTU in 1987 and the MS and PhD degrees in electrical engineering from the University of Maryland, College Park, in 1992 and 1995, respectively. In August 2000, he was with the faculty of the Department of Electrical Engineering and the Graduate Institute of Electronics Engineering, NTU, where he is currently a distinguished professor. His research interests include low-power or high-performance VLSI architectures and IC designs for DSP or communication or AI applications, and adaptive or bio-medical signal processing. In 2015, he was promoted to IEEE fellow for his contributions to DSP algorithms and VLSI designs for communication IC/SoC. From 2016 to 2019, he was the director of the Graduate Institute of Electronics Engineering, National Taiwan University. He is currently the editor-in-chief of the IEEE Journal on Emerging and Selected Topics in Circuits and Systems.\nMulti-task learning of deep neural network-based keyword spotting for IoT devices. S.-G Leem, I Yoo, D Yook, IEEE Trans. Consum. Electron. 652S.-G. Leem, I. Yoo, and D. Yook, \"Multi-task learning of deep neu- ral network-based keyword spotting for IoT devices,\" IEEE Trans. Consum. Electron., vol. 65, no. 2, pp. 188-194, May 2019.\n\nContinual learning through synaptic intelligence. F Zenke, B Poole, S Ganguli, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnF. Zenke, B. Poole, and S. Ganguli, \"Continual learning through synaptic intelligence,\" in Proc. Int. Conf. Mach. Learn., 2017, pp. 3987-3995.\n\nDeep elastic networks with model selection for multi-task learning. C Ahn, E Kim, S Oh, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisC. Ahn, E. Kim, and S. Oh, \"Deep elastic networks with model selection for multi-task learning,\" in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 6528-6537.\n\nContinual learning with deep generative replay. H Shin, J K Lee, J Kim, J Kim, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystH. Shin, J. K. Lee, J. Kim, and J. Kim, \"Continual learning with deep generative replay,\" in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 2994-3003.\n\nContinual lifelong learning with neural networks: A review. G I Parisi, R Kemker, J L Part, C Kanan, S Wermter, Neural Netw. 113G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, \"Continual lifelong learning with neural networks: A review,\" Neural Netw vol. 113, pp. 54-71, 2019.\n\nHyperdimensional computing with 3D VRRAM inmemory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition. H Li, Proc. IEEE Int . Electron Devices Meeting. IEEE Int . Electron Devices Meeting16.1.1-16.1.4.H. Li et al., \"Hyperdimensional computing with 3D VRRAM in- memory kernels: Device-architecture co-design for energy-effi- cient, error-resilient language recognition,\" in Proc. IEEE Int . Elec- tron Devices Meeting, 2016, pp. 16.1.1-16.1.4.\n\nA robust and energyefficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proc. Int. Symp. Low Power Electron. Des. Int. Symp. Low Power Electron. DesA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy- efficient classifier using brain-inspired hyperdimensional compu- ting,\" in Proc. Int. Symp. Low Power Electron. Des., 2016, pp. 64-69.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cogn. Comput. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cogn. Comput., vol. 1, no. 2, pp. 139-159, 2009.\n\nHyperdimensional biosignal processing: A case study for EMGbased hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Proc. IEEE Int. Conf. Rebooting Comput. IEEE Int. Conf. Rebooting ComputA. Rahimi, S. Benatti, P. Kanerva, L. Benini, and J. M. Rabaey, \"Hyperdimensional biosignal processing: A case study for EMG- based hand gesture recognition,\" in Proc. IEEE Int. Conf. Rebooting Comput., 2016, pp. 1-8.\n\nPULP-HD: Accelerating brain-inspired high-dimensional computing on a parallel ultra-low power platform. F Montagna, A Rahimi, S Benatti, D Rossi, L Benini, Proc. nullF. Montagna, A. Rahimi, S. Benatti, D. Rossi, and L. Benini, \"PULP-HD: Accelerating brain-inspired high-dimensional com- puting on a parallel ultra-low power platform,\" in Proc. IEEE/ ACM Des. Automat. Conf., 2018, pp. 1-6.\n\nHigh-dimensional computing as a nanoscalable paradigm. A Rahimi, IEEE Trans. Circuits Syst. I: Regular Papers. 649A. Rahimi et al., \"High-dimensional computing as a nanoscalable paradigm,\" IEEE Trans. Circuits Syst. I: Regular Papers, vol. 64, no. 9, pp. 2508-2521, Sep. 2017.\n\nHyperdimensional computing-based multimodality emotion recognition with physiological signals. E.-J Chang, A Rahimi, L Benini, A.-Y A Wu, Proc. IEEE Int . Symp. AI Circuits Syst. IEEE Int . Symp. AI Circuits SystE.-J. Chang, A. Rahimi, L. Benini, and A.-Y. A. Wu, \"Hyperdimensional computing-based multimodality emotion rec- ognition with physiological signals,\" in Proc. IEEE Int . Symp. AI Circuits Syst., 2019, pp. 137-141.\n\nLaelaps: An energy-efficient seizure detection algorithm from long-term human iEEG re-cordings without false alarms. A Burrello, L Cavigelli, K Schindler, L Benini, A Rahimi, Proc. Des., Autom. Test Eur. Conf. Exhib. Des., Autom. Test Eur. Conf. ExhibA. Burrello, L. Cavigelli, K. Schindler, L. Benini, and A. Rahimi, \"Laelaps: An energy-efficient seizure detection algorithm from long-term human iEEG re-cordings without false alarms,\" in Proc. Des., Autom. Test Eur. Conf. Exhib., 2019, pp. 752-757.\n\nGenieHD: Efficient DNA pattern matching accelerator using hyperdimensional computing. Y Kim, M Imani, N Moshiri, T Rosing, Proc. Des. Automat. Test Eur. Conf. Exhib., 2020. Des. Automat. Test Eur. Conf. Exhib., 2020Y. Kim, M. Imani, N. Moshiri, and T. Rosing, \"GenieHD: Efficient DNA pattern matching accelerator using hyperdimensional computing,\" in Proc. Des. Automat. Test Eur. Conf. Exhib., 2020, pp. 115-120.\n\nVoiceHD: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, Proc. IEEE Int . Conf. Rebooting Comput. IEEE Int . Conf. Rebooting ComputM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"VoiceHD: Hyper- dimensional computing for efficient speech recognition,\" in Proc. IEEE Int . Conf. Rebooting Comput ., 2017, pp. 1-8.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, M Imani, T Rosing, Proc. 8th Int. Conf. Internet Things. 8th Int. Conf. Internet ThingsY. Kim, M. Imani, and T. Rosing, \"Efficient human activity recog- nition using hyperdimensional computing,\" in Proc. 8th Int. Conf. Internet Things, 2018, pp. 1-6.\n\nDynamic hyperdimensional computing for improving accuracy-energy efficiency trade-offs. Y.-C Chuang, C Y Chang, A.-Y A Wu, Proc. IEEE Int . Workshop Signal Process . Syst. IEEE Int . Workshop Signal ess . SystY.-C. Chuang, C. Y. Chang, and A.-Y. A. Wu, \"Dynamic hyperdi- mensional computing for improving accuracy-energy efficiency trade-offs,\" in Proc. IEEE Int . Workshop Signal Process . Syst ., 2020, pp. 1-5.\n\nTask-projected hyperdimensional computing for multi-task learning. C.-Y Chang, Y.-C Chuang, A.-Y A Wu, Proc. nullC.-Y. Chang, Y.-C. Chuang, and A.-Y. A. Wu, \"Task-projected hyperdimensional computing for multi-task learning,\" in Proc. Artif. Intell. Appl. Innovations, 2020, pp. 241-251.\n\nIP-HDC: Information-presevered hyperdimensional computing for multi-task learning. C.-Y Chang, Y.-C Chuang, A.-Y A Wu, Proc. IEEE Int . Workshop .Signal Process . Syst. IEEE Int . Workshop .Signal ess . SystC.-Y. Chang, Y.-C. Chuang, and A.-Y. A. Wu, \"IP-HDC: Informa- tion-presevered hyperdimensional computing for multi-task learning,\" in Proc. IEEE Int . Workshop .Signal Process . Syst ., 2020, pp. 1-6.\n\nSparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing. M Imani, S Salamat, B Khaleghi, M Samragh, F Koushanfar, T Rosing, Proc. IEEE 27th. IEEE 27thM. Imani, S. Salamat, B. Khaleghi, M. Samragh, F. Koushanfar, and T. Rosing, \"SparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing,\" in Proc. IEEE 27th\n\n. Annu. Int. Symp. Field-Programm . Custom Comput. Mach. Annu. Int. Symp. Field-Programm . Custom Comput. Mach., 2019, pp. 190-198.\n\n. Isolet Data, Set, ISOLET Data Set. Accessed: Feb. 1, 2021. [Online]. Available: http:// archive.ics.uci.edu/ml/datasets/ISOLET.\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, Proc. Int. Workshop Ambient Assist. Living. Int. Workshop Ambient Assist. LivingD. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, \"Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine,\" in Proc. Int. Workshop Ambient Assist. Living, 2012, pp. 216-223.\n\nMNIST handwritten digit database. Y Lecun, C Cortes, C J Burges, Y. LeCun, C. Cortes, and C. J. Burges, \"MNIST handwritten digit database,\" 2010. [Online]. Available: http://yann. lecun.com/ exdb/mnist\n\nClassification using hyperdimensional computing: A review. L Ge, K K Parhi, IEEE Circuits Syst. Mag. 202L. Ge, and K. K. Parhi, \"Classification using hyperdimensional computing: A review,\" IEEE Circuits Syst. Mag., vol. 20, no. 2, pp. 30-47, Sep. 2020.\n\nCompHD: Efficient hyperdimensional computing using model compression. J Morris, M Imani, S Bosch, A Thomas, H Shu, T Rosing, Proc. nullJ. Morris, M. Imani, S. Bosch, A. Thomas, H. Shu, and T. Rosing, \"CompHD: Efficient hyperdimensional computing using model compression,\" in Proc. IEEE/ACM Int . Symp. Low Power Electron. Des ., 2019, pp. 1-6.\n\nThree scenarios for continual learning. G M Van De Ven, A S Tolias, arXiv:1904.07734G. M. van de Ven, and A. S. Tolias, \"Three scenarios for continual learning,\" 2019, arXiv:1904.07734.\n\nStream-51: Streaming classification and novelty detection from videos. R Roady, T L Hayes, H Vaidya, C Kanan, Proc . IEEE/CVF Conf. Comput. Vis. Pattern Recognit . Workshops, 2020. IEEE/CVF Conf. Comput. Vis. Pattern Recognit . Workshops, 2020R. Roady, T. L. Hayes, H. Vaidya, and C. Kanan, \"Stream-51: Streaming classification and novelty detection from videos,\" in Proc . IEEE/CVF Conf. Comput. Vis. Pattern Recognit . Workshops, 2020, pp. 925-934.\n\nAn empirical investigation of catastrophic forgetting in gradientbased neural networks. I J Goodfellow, M Mirza, D Xiao, A Courville, Y Bengio, Proc. Int. Conf. Learn. Representations. Int. Conf. Learn. RepresentationsI. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, \"An empirical investigation of catastrophic forgetting in gradient- based neural networks,\" in Proc. Int. Conf. Learn. Representations, 2014.\n\nMeasuring catastrophic forgetting in neural networks. R Kemker, A Abitino, M Mcclure, C Kanan, arXiv:1708.02072R. Kemker, A. Abitino, M. McClure, and C. Kanan, \"Measuring cat- astrophic forgetting in neural networks,\" 2017, arXiv:1708.02072.\n\nRAKE: A simple and efficient lossless compression algorithm for the internet of things. G Campobello, A Segreto, S Zanafi, S Serrano, Proc. 25th Eur. Signal Process. Conf. 25th Eur. Signal ess. ConfG. Campobello, A. Segreto, S. Zanafi, and S. Serrano, \"RAKE: A simple and efficient lossless compression algorithm for the inter- net of things,\" in Proc. 25th Eur. Signal Process. Conf., 2017, pp. 2581-2585.\n\nQuantHD: A quantization framework for hyperdimensional computing. M Imani, IEEE Trans . Comput.-Aided Des . Integr . Circuits Syst. 3910M. Imani et al., \"QuantHD: A quantization framework for hyper- dimensional computing,\" IEEE Trans . Comput.-Aided Des . Integr . Circuits Syst., vol. 39, no. 10, pp. 2268-2278, Oct. 2020.\n\nCompressing subject-specific brain-computer interface models into one model by superposition in hyperdimensional space. M Hersche, P Rupp, L Benini, A Rahimi, Proc. Des. Automat. Test. Eur. Conf. Exhib., 2020. Des. Automat. Test. Eur. Conf. Exhib., 2020M. Hersche, P. Rupp, L. Benini, and A. Rahimi, \"Compressing subject-specific brain-computer interface models into one model by superposition in hyperdimensional space,\" in Proc. Des. Auto- mat. Test. Eur. Conf. Exhib., 2020, pp. 246-251.\n\nTask offloading and resource allocation for mobile edge computing by deep reinforcement learning based on SARSA. T Alfakih, M M Hassan, A Gumaei, C Savaglio, G Fortino, IEEE Access. 8T. Alfakih, M. M. Hassan, A. Gumaei, C. Savaglio, and G. Fortino, \"Task offloading and resource allocation for mobile edge comput- ing by deep reinforcement learning based on SARSA,\" IEEE Access, vol. 8, pp. 54074-54084, 2020.\n\nOn-edge multitask transfer learning: Model and practice with data-driven task allocation. Q Chen, Z Zheng, C Hu, D Wang, F Liu, IEEE Trans. Parallel. Distrib. Syst. 316Q. Chen, Z. Zheng, C. Hu, D. Wang, and F. Liu, \"On-edge multi- task transfer learning: Model and practice with data-driven task allocation,\" IEEE Trans. Parallel. Distrib. Syst., vol. 31, no. 6, pp. 1357-1371, Jun. 2020.\n\nComputation offloading in multi-access edge computing networks: A multi-task learning approach. B Yang, X Cao, J Bassey, X Li, T Kroecker, L Qian, Proc. IEEE Int . Conf. Commun. IEEE Int . Conf. CommunB. Yang, X. Cao, J. Bassey, X. Li, T. Kroecker, and L. Qian, \"Computation offloading in multi-access edge computing net- works: A multi-task learning approach,\" in Proc. IEEE Int . Conf. Commun., 2019, pp. 1-6.\n\nDeep virtual networks for memory efficient inference of multiple tasks. E Kim, C Ahn, P H S Torr, S Oh, Proc . IEEE Conf . Comput . Vis. Pattern Recognit. IEEE Conf . Comput . Vis. Pattern RecognitE. Kim, C. Ahn, P. H. S. Torr, and S. Oh, \"Deep virtual networks for memory efficient inference of multiple tasks,\" in Proc . IEEE Conf . Comput . Vis. Pattern Recognit., 2019, pp. 2705-2714.\n\nPackNet: Adding multiple tasks to a single network by iterative pruning. A Mallya, S Lazebnik, Proc. IEEE Conf. Comput . Vis. IEEE Conf. Comput . VisA. Mallya, and S. Lazebnik, \"PackNet: Adding multiple tasks to a single network by iterative pruning,\" in Proc. IEEE Conf. Comput . Vis. Pattern Recognit., 2018, pp. 7765-7773.\n\nHyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of EEG error-related potentials. A Rahimi, P Kanerva, J Del R. Millan, J M Rabaey, Proc. 10th EAI Int. Conf. Bio-Inspired Inf. 10th EAI Int. Conf. Bio-Inspired InfA. Rahimi, P. Kanerva, J. del R. Millan, and J. M. Rabaey, \"Hyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of EEG error-related potentials,\" in Proc. 10th EAI Int. Conf. Bio-Inspired Inf. Commun. Technol., 2017, pp. 19-26.\n", "annotations": {"author": "[{\"end\":114,\"start\":77},{\"end\":151,\"start\":115},{\"end\":177,\"start\":152},{\"end\":185,\"start\":178},{\"end\":206,\"start\":186}]", "publisher": null, "author_last_name": "[{\"end\":113,\"start\":108},{\"end\":150,\"start\":144},{\"end\":176,\"start\":171},{\"end\":205,\"start\":203}]", "author_first_name": "[{\"end\":107,\"start\":97},{\"end\":143,\"start\":135},{\"end\":170,\"start\":164},{\"end\":184,\"start\":178},{\"end\":202,\"start\":198}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":280,\"start\":207}]", "venue": null, "abstract": "[{\"end\":2151,\"start\":402}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2768,\"start\":2765},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3014,\"start\":3011},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3019,\"start\":3016},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3024,\"start\":3021},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3029,\"start\":3026},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3216,\"start\":3212},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3338,\"start\":3334},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3594,\"start\":3590},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3964,\"start\":3961},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4090,\"start\":4087},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4096,\"start\":4092},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4138,\"start\":4135},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4181,\"start\":4178},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4187,\"start\":4183},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4288,\"start\":4284},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4312,\"start\":4308},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4333,\"start\":4329},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4358,\"start\":4354},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4391,\"start\":4387},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4422,\"start\":4418},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4526,\"start\":4523},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4532,\"start\":4528},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4538,\"start\":4534},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4686,\"start\":4682},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5964,\"start\":5960},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6047,\"start\":6043},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6994,\"start\":6990},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9285,\"start\":9281},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9298,\"start\":9294},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9314,\"start\":9310},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9354,\"start\":9350},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10585,\"start\":10581},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14474,\"start\":14470},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14490,\"start\":14486},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15367,\"start\":15363},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15385,\"start\":15381},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16027,\"start\":16023},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16032,\"start\":16028},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27053,\"start\":27050},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28165,\"start\":28161},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28171,\"start\":28167},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29399,\"start\":29395},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33105,\"start\":33101},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33270,\"start\":33266},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34174,\"start\":34170},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34215,\"start\":34211},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34225,\"start\":34221},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34406,\"start\":34402},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34431,\"start\":34427},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36885,\"start\":36881},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38425,\"start\":38421},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38443,\"start\":38439},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39583,\"start\":39579},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":40583,\"start\":40579},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40599,\"start\":40595},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":43371,\"start\":43368},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":43377,\"start\":43373},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43383,\"start\":43379},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":45327,\"start\":45323},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48006,\"start\":48002},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":48870,\"start\":48866},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48961,\"start\":48957}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50369,\"start\":50262},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50550,\"start\":50370},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50605,\"start\":50551},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50885,\"start\":50606},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50920,\"start\":50886},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50984,\"start\":50921},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51292,\"start\":50985},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51410,\"start\":51293},{\"attributes\":{\"id\":\"fig_8\"},\"end\":52031,\"start\":51411},{\"attributes\":{\"id\":\"fig_9\"},\"end\":52481,\"start\":52032},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52904,\"start\":52482},{\"attributes\":{\"id\":\"fig_11\"},\"end\":53080,\"start\":52905},{\"attributes\":{\"id\":\"fig_12\"},\"end\":53311,\"start\":53081},{\"attributes\":{\"id\":\"fig_13\"},\"end\":53604,\"start\":53312},{\"attributes\":{\"id\":\"fig_15\"},\"end\":53738,\"start\":53605},{\"attributes\":{\"id\":\"fig_16\"},\"end\":54533,\"start\":53739},{\"attributes\":{\"id\":\"fig_17\"},\"end\":54615,\"start\":54534},{\"attributes\":{\"id\":\"fig_18\"},\"end\":54893,\"start\":54616},{\"attributes\":{\"id\":\"fig_19\"},\"end\":55444,\"start\":54894},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55590,\"start\":55445},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55875,\"start\":55591},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55909,\"start\":55876},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57978,\"start\":55910},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":58008,\"start\":57979},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":58342,\"start\":58009},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58396,\"start\":58343},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58740,\"start\":58397},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":58810,\"start\":58741}]", "paragraph": "[{\"end\":3755,\"start\":2171},{\"end\":4956,\"start\":3757},{\"end\":5220,\"start\":4958},{\"end\":5890,\"start\":5222},{\"end\":6298,\"start\":5892},{\"end\":7638,\"start\":6300},{\"end\":9887,\"start\":7640},{\"end\":10586,\"start\":9921},{\"end\":11199,\"start\":10614},{\"end\":12134,\"start\":11201},{\"end\":12290,\"start\":12150},{\"end\":12585,\"start\":12322},{\"end\":12851,\"start\":12604},{\"end\":13001,\"start\":12878},{\"end\":13395,\"start\":13021},{\"end\":13493,\"start\":13450},{\"end\":14292,\"start\":13528},{\"end\":14847,\"start\":14326},{\"end\":15672,\"start\":14886},{\"end\":16849,\"start\":15710},{\"end\":18459,\"start\":16935},{\"end\":18926,\"start\":18461},{\"end\":19349,\"start\":18967},{\"end\":19618,\"start\":19367},{\"end\":19860,\"start\":19642},{\"end\":21091,\"start\":19862},{\"end\":21490,\"start\":21156},{\"end\":22013,\"start\":21492},{\"end\":22589,\"start\":22044},{\"end\":22925,\"start\":22869},{\"end\":23597,\"start\":23132},{\"end\":24342,\"start\":23651},{\"end\":24424,\"start\":24364},{\"end\":25057,\"start\":24426},{\"end\":25765,\"start\":25082},{\"end\":26577,\"start\":25797},{\"end\":26837,\"start\":26579},{\"end\":27382,\"start\":26886},{\"end\":27882,\"start\":27442},{\"end\":28370,\"start\":27912},{\"end\":28663,\"start\":28407},{\"end\":28754,\"start\":28727},{\"end\":28940,\"start\":28756},{\"end\":29668,\"start\":28988},{\"end\":30245,\"start\":29715},{\"end\":31579,\"start\":30247},{\"end\":32056,\"start\":31610},{\"end\":32599,\"start\":32089},{\"end\":32706,\"start\":32601},{\"end\":32961,\"start\":32708},{\"end\":33615,\"start\":32992},{\"end\":33911,\"start\":33736},{\"end\":34719,\"start\":33964},{\"end\":35080,\"start\":34721},{\"end\":35265,\"start\":35198},{\"end\":35470,\"start\":35299},{\"end\":35622,\"start\":35537},{\"end\":35976,\"start\":35647},{\"end\":36589,\"start\":35978},{\"end\":37997,\"start\":36633},{\"end\":38314,\"start\":37999},{\"end\":38697,\"start\":38339},{\"end\":39274,\"start\":38699},{\"end\":39810,\"start\":39312},{\"end\":39962,\"start\":39845},{\"end\":40601,\"start\":39964},{\"end\":40873,\"start\":40655},{\"end\":41475,\"start\":40875},{\"end\":43156,\"start\":41528},{\"end\":43633,\"start\":43216},{\"end\":45385,\"start\":43635},{\"end\":45964,\"start\":45432},{\"end\":48007,\"start\":45966},{\"end\":48640,\"start\":48050},{\"end\":48962,\"start\":48642},{\"end\":49287,\"start\":48964},{\"end\":50261,\"start\":49302}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12321,\"start\":12291},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12877,\"start\":12852},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13449,\"start\":13396},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19366,\"start\":19350},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19641,\"start\":19619},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21155,\"start\":21092},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22868,\"start\":22625},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23131,\"start\":22926},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23650,\"start\":23598},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24363,\"start\":24343},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26885,\"start\":26838},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27441,\"start\":27383},{\"attributes\":{\"id\":\"formula_13\"},\"end\":28726,\"start\":28664},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33735,\"start\":33616},{\"attributes\":{\"id\":\"formula_16\"},\"end\":35197,\"start\":35081},{\"attributes\":{\"id\":\"formula_17\"},\"end\":35298,\"start\":35266},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35536,\"start\":35471},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36632,\"start\":36590},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39844,\"start\":39811}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15852,\"start\":15845},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18597,\"start\":18590},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20229,\"start\":20222},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20588,\"start\":20581},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35621,\"start\":35614},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44280,\"start\":44273},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":46157,\"start\":46150},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":46798,\"start\":46791},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":47104,\"start\":47097},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":47211,\"start\":47204},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":48171,\"start\":48164},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":48664,\"start\":48657},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":49042,\"start\":49035}]", "section_header": "[{\"end\":2154,\"start\":2153},{\"attributes\":{\"n\":\"1\"},\"end\":2169,\"start\":2157},{\"attributes\":{\"n\":\"2\"},\"end\":9903,\"start\":9890},{\"attributes\":{\"n\":\"2.1\"},\"end\":9919,\"start\":9906},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":10612,\"start\":10589},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":12148,\"start\":12137},{\"attributes\":{\"n\":\"2.1.3\"},\"end\":12602,\"start\":12588},{\"attributes\":{\"n\":\"2.1.4\"},\"end\":13019,\"start\":13004},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":13526,\"start\":13496},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":14324,\"start\":14295},{\"attributes\":{\"n\":\"2.2.3\"},\"end\":14884,\"start\":14850},{\"attributes\":{\"n\":\"2.2.4\"},\"end\":15708,\"start\":15675},{\"attributes\":{\"n\":\"3\"},\"end\":16889,\"start\":16852},{\"attributes\":{\"n\":\"3.1\"},\"end\":16933,\"start\":16892},{\"attributes\":{\"n\":\"3.2\"},\"end\":18965,\"start\":18929},{\"attributes\":{\"n\":\"4\"},\"end\":22042,\"start\":22016},{\"end\":22624,\"start\":22592},{\"attributes\":{\"n\":\"4.2\"},\"end\":25080,\"start\":25060},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":25795,\"start\":25768},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":27910,\"start\":27885},{\"end\":28405,\"start\":28373},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":28986,\"start\":28943},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":29713,\"start\":29671},{\"attributes\":{\"n\":\"4.3\"},\"end\":31608,\"start\":31582},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":32087,\"start\":32059},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":32990,\"start\":32964},{\"attributes\":{\"n\":\"5\"},\"end\":33941,\"start\":33914},{\"attributes\":{\"n\":\"5.1\"},\"end\":33962,\"start\":33944},{\"attributes\":{\"n\":\"5.2\"},\"end\":35645,\"start\":35625},{\"attributes\":{\"n\":\"5.3\"},\"end\":38337,\"start\":38317},{\"attributes\":{\"n\":\"5.4\"},\"end\":39310,\"start\":39277},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":40653,\"start\":40604},{\"attributes\":{\"n\":\"5.4.2\"},\"end\":41526,\"start\":41478},{\"attributes\":{\"n\":\"5.4.3\"},\"end\":43214,\"start\":43159},{\"attributes\":{\"n\":\"6\"},\"end\":45410,\"start\":45388},{\"attributes\":{\"n\":\"6.1\"},\"end\":45430,\"start\":45413},{\"attributes\":{\"n\":\"6.2\"},\"end\":48048,\"start\":48010},{\"attributes\":{\"n\":\"7\"},\"end\":49300,\"start\":49290},{\"end\":50271,\"start\":50263},{\"end\":50379,\"start\":50371},{\"end\":50560,\"start\":50552},{\"end\":50615,\"start\":50607},{\"end\":50930,\"start\":50922},{\"end\":50999,\"start\":50986},{\"end\":51302,\"start\":51294},{\"end\":51417,\"start\":51412},{\"end\":52491,\"start\":52483},{\"end\":53091,\"start\":53082},{\"end\":53326,\"start\":53313},{\"end\":53615,\"start\":53606},{\"end\":53744,\"start\":53740},{\"end\":54544,\"start\":54535},{\"end\":54626,\"start\":54617},{\"end\":54904,\"start\":54895},{\"end\":55453,\"start\":55446},{\"end\":55599,\"start\":55592},{\"end\":55884,\"start\":55877},{\"end\":58000,\"start\":57980},{\"end\":58030,\"start\":58010},{\"end\":58364,\"start\":58344},{\"end\":58413,\"start\":58398},{\"end\":58749,\"start\":58742}]", "table": "[{\"end\":55875,\"start\":55666},{\"end\":57978,\"start\":56333},{\"end\":58342,\"start\":58143},{\"end\":58740,\"start\":58510}]", "figure_caption": "[{\"end\":50369,\"start\":50273},{\"end\":50550,\"start\":50381},{\"end\":50605,\"start\":50562},{\"end\":50885,\"start\":50617},{\"end\":50920,\"start\":50888},{\"end\":50984,\"start\":50932},{\"end\":51292,\"start\":51001},{\"end\":51410,\"start\":51304},{\"end\":52031,\"start\":51419},{\"end\":52481,\"start\":52034},{\"end\":52904,\"start\":52493},{\"end\":53080,\"start\":52907},{\"end\":53311,\"start\":53094},{\"end\":53604,\"start\":53328},{\"end\":53738,\"start\":53618},{\"end\":54533,\"start\":53745},{\"end\":54615,\"start\":54547},{\"end\":54893,\"start\":54629},{\"end\":55444,\"start\":54907},{\"end\":55590,\"start\":55455},{\"end\":55666,\"start\":55601},{\"end\":55909,\"start\":55886},{\"end\":56333,\"start\":55912},{\"end\":58008,\"start\":58002},{\"end\":58143,\"start\":58032},{\"end\":58396,\"start\":58366},{\"end\":58510,\"start\":58415},{\"end\":58810,\"start\":58751}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5889,\"start\":5883},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6491,\"start\":6484},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7220,\"start\":7213},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10020,\"start\":10014},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13644,\"start\":13638},{\"end\":14438,\"start\":14431},{\"end\":14992,\"start\":14985},{\"end\":16848,\"start\":16841},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":17261,\"start\":17255},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20674,\"start\":20668},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23243,\"start\":23236},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24636,\"start\":24629},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":25444,\"start\":25438},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":26820,\"start\":26805},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":28073,\"start\":28066},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":28369,\"start\":28362},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":28939,\"start\":28932},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30982,\"start\":30975},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31385,\"start\":31378},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31977,\"start\":31970},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32451,\"start\":32443},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32641,\"start\":32633},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36089,\"start\":36082},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36202,\"start\":36195},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36789,\"start\":36782},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37406,\"start\":37399},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37532,\"start\":37530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37996,\"start\":37988},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38731,\"start\":38724},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40576,\"start\":40569},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40803,\"start\":40796},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41852,\"start\":41835},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44067,\"start\":44060},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44171,\"start\":44164},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44215,\"start\":44208},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44289,\"start\":44282},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44495,\"start\":44488}]", "bib_author_first_name": "[{\"end\":61260,\"start\":61256},{\"end\":61268,\"start\":61267},{\"end\":61275,\"start\":61274},{\"end\":61557,\"start\":61556},{\"end\":61566,\"start\":61565},{\"end\":61575,\"start\":61574},{\"end\":61850,\"start\":61849},{\"end\":61857,\"start\":61856},{\"end\":61864,\"start\":61863},{\"end\":62138,\"start\":62137},{\"end\":62146,\"start\":62145},{\"end\":62148,\"start\":62147},{\"end\":62155,\"start\":62154},{\"end\":62162,\"start\":62161},{\"end\":62447,\"start\":62446},{\"end\":62449,\"start\":62448},{\"end\":62459,\"start\":62458},{\"end\":62469,\"start\":62468},{\"end\":62471,\"start\":62470},{\"end\":62479,\"start\":62478},{\"end\":62488,\"start\":62487},{\"end\":62829,\"start\":62828},{\"end\":63259,\"start\":63258},{\"end\":63269,\"start\":63268},{\"end\":63280,\"start\":63279},{\"end\":63282,\"start\":63281},{\"end\":63693,\"start\":63692},{\"end\":64000,\"start\":63999},{\"end\":64010,\"start\":64009},{\"end\":64021,\"start\":64020},{\"end\":64032,\"start\":64031},{\"end\":64042,\"start\":64041},{\"end\":64044,\"start\":64043},{\"end\":64449,\"start\":64448},{\"end\":64461,\"start\":64460},{\"end\":64471,\"start\":64470},{\"end\":64482,\"start\":64481},{\"end\":64491,\"start\":64490},{\"end\":64791,\"start\":64790},{\"end\":65112,\"start\":65108},{\"end\":65121,\"start\":65120},{\"end\":65131,\"start\":65130},{\"end\":65144,\"start\":65140},{\"end\":65146,\"start\":65145},{\"end\":65559,\"start\":65558},{\"end\":65571,\"start\":65570},{\"end\":65584,\"start\":65583},{\"end\":65597,\"start\":65596},{\"end\":65607,\"start\":65606},{\"end\":66031,\"start\":66030},{\"end\":66038,\"start\":66037},{\"end\":66047,\"start\":66046},{\"end\":66058,\"start\":66057},{\"end\":66430,\"start\":66429},{\"end\":66439,\"start\":66438},{\"end\":66447,\"start\":66446},{\"end\":66457,\"start\":66456},{\"end\":66793,\"start\":66792},{\"end\":66800,\"start\":66799},{\"end\":66809,\"start\":66808},{\"end\":67143,\"start\":67139},{\"end\":67153,\"start\":67152},{\"end\":67155,\"start\":67154},{\"end\":67167,\"start\":67163},{\"end\":67169,\"start\":67168},{\"end\":67537,\"start\":67533},{\"end\":67549,\"start\":67545},{\"end\":67562,\"start\":67558},{\"end\":67564,\"start\":67563},{\"end\":67842,\"start\":67838},{\"end\":67854,\"start\":67850},{\"end\":67867,\"start\":67863},{\"end\":67869,\"start\":67868},{\"end\":68252,\"start\":68251},{\"end\":68261,\"start\":68260},{\"end\":68272,\"start\":68271},{\"end\":68284,\"start\":68283},{\"end\":68295,\"start\":68294},{\"end\":68309,\"start\":68308},{\"end\":68898,\"start\":68897},{\"end\":68909,\"start\":68908},{\"end\":68917,\"start\":68916},{\"end\":68926,\"start\":68925},{\"end\":68935,\"start\":68934},{\"end\":68937,\"start\":68936},{\"end\":69302,\"start\":69301},{\"end\":69311,\"start\":69310},{\"end\":69321,\"start\":69320},{\"end\":69323,\"start\":69322},{\"end\":69530,\"start\":69529},{\"end\":69536,\"start\":69535},{\"end\":69538,\"start\":69537},{\"end\":69795,\"start\":69794},{\"end\":69805,\"start\":69804},{\"end\":69814,\"start\":69813},{\"end\":69823,\"start\":69822},{\"end\":69833,\"start\":69832},{\"end\":69840,\"start\":69839},{\"end\":70110,\"start\":70109},{\"end\":70112,\"start\":70111},{\"end\":70126,\"start\":70125},{\"end\":70128,\"start\":70127},{\"end\":70328,\"start\":70327},{\"end\":70337,\"start\":70336},{\"end\":70339,\"start\":70338},{\"end\":70348,\"start\":70347},{\"end\":70358,\"start\":70357},{\"end\":70797,\"start\":70796},{\"end\":70799,\"start\":70798},{\"end\":70813,\"start\":70812},{\"end\":70822,\"start\":70821},{\"end\":70830,\"start\":70829},{\"end\":70843,\"start\":70842},{\"end\":71190,\"start\":71189},{\"end\":71200,\"start\":71199},{\"end\":71211,\"start\":71210},{\"end\":71222,\"start\":71221},{\"end\":71467,\"start\":71466},{\"end\":71481,\"start\":71480},{\"end\":71492,\"start\":71491},{\"end\":71502,\"start\":71501},{\"end\":71853,\"start\":71852},{\"end\":72232,\"start\":72231},{\"end\":72243,\"start\":72242},{\"end\":72251,\"start\":72250},{\"end\":72261,\"start\":72260},{\"end\":72717,\"start\":72716},{\"end\":72728,\"start\":72727},{\"end\":72730,\"start\":72729},{\"end\":72740,\"start\":72739},{\"end\":72750,\"start\":72749},{\"end\":72762,\"start\":72761},{\"end\":73105,\"start\":73104},{\"end\":73113,\"start\":73112},{\"end\":73122,\"start\":73121},{\"end\":73128,\"start\":73127},{\"end\":73136,\"start\":73135},{\"end\":73501,\"start\":73500},{\"end\":73509,\"start\":73508},{\"end\":73516,\"start\":73515},{\"end\":73526,\"start\":73525},{\"end\":73532,\"start\":73531},{\"end\":73544,\"start\":73543},{\"end\":73890,\"start\":73889},{\"end\":73897,\"start\":73896},{\"end\":73904,\"start\":73903},{\"end\":73908,\"start\":73905},{\"end\":73916,\"start\":73915},{\"end\":74281,\"start\":74280},{\"end\":74291,\"start\":74290},{\"end\":74672,\"start\":74671},{\"end\":74682,\"start\":74681},{\"end\":74693,\"start\":74692},{\"end\":74710,\"start\":74709},{\"end\":74712,\"start\":74711}]", "bib_author_last_name": "[{\"end\":61265,\"start\":61261},{\"end\":61272,\"start\":61269},{\"end\":61280,\"start\":61276},{\"end\":61563,\"start\":61558},{\"end\":61572,\"start\":61567},{\"end\":61583,\"start\":61576},{\"end\":61854,\"start\":61851},{\"end\":61861,\"start\":61858},{\"end\":61867,\"start\":61865},{\"end\":62143,\"start\":62139},{\"end\":62152,\"start\":62149},{\"end\":62159,\"start\":62156},{\"end\":62166,\"start\":62163},{\"end\":62456,\"start\":62450},{\"end\":62466,\"start\":62460},{\"end\":62476,\"start\":62472},{\"end\":62485,\"start\":62480},{\"end\":62496,\"start\":62489},{\"end\":62832,\"start\":62830},{\"end\":63266,\"start\":63260},{\"end\":63277,\"start\":63270},{\"end\":63289,\"start\":63283},{\"end\":63701,\"start\":63694},{\"end\":64007,\"start\":64001},{\"end\":64018,\"start\":64011},{\"end\":64029,\"start\":64022},{\"end\":64039,\"start\":64033},{\"end\":64051,\"start\":64045},{\"end\":64458,\"start\":64450},{\"end\":64468,\"start\":64462},{\"end\":64479,\"start\":64472},{\"end\":64488,\"start\":64483},{\"end\":64498,\"start\":64492},{\"end\":64798,\"start\":64792},{\"end\":65118,\"start\":65113},{\"end\":65128,\"start\":65122},{\"end\":65138,\"start\":65132},{\"end\":65149,\"start\":65147},{\"end\":65568,\"start\":65560},{\"end\":65581,\"start\":65572},{\"end\":65594,\"start\":65585},{\"end\":65604,\"start\":65598},{\"end\":65614,\"start\":65608},{\"end\":66035,\"start\":66032},{\"end\":66044,\"start\":66039},{\"end\":66055,\"start\":66048},{\"end\":66065,\"start\":66059},{\"end\":66436,\"start\":66431},{\"end\":66444,\"start\":66440},{\"end\":66454,\"start\":66448},{\"end\":66464,\"start\":66458},{\"end\":66797,\"start\":66794},{\"end\":66806,\"start\":66801},{\"end\":66816,\"start\":66810},{\"end\":67150,\"start\":67144},{\"end\":67161,\"start\":67156},{\"end\":67172,\"start\":67170},{\"end\":67543,\"start\":67538},{\"end\":67556,\"start\":67550},{\"end\":67567,\"start\":67565},{\"end\":67848,\"start\":67843},{\"end\":67861,\"start\":67855},{\"end\":67872,\"start\":67870},{\"end\":68258,\"start\":68253},{\"end\":68269,\"start\":68262},{\"end\":68281,\"start\":68273},{\"end\":68292,\"start\":68285},{\"end\":68306,\"start\":68296},{\"end\":68316,\"start\":68310},{\"end\":68676,\"start\":68665},{\"end\":68681,\"start\":68678},{\"end\":68906,\"start\":68899},{\"end\":68914,\"start\":68910},{\"end\":68923,\"start\":68918},{\"end\":68932,\"start\":68927},{\"end\":68949,\"start\":68938},{\"end\":69308,\"start\":69303},{\"end\":69318,\"start\":69312},{\"end\":69330,\"start\":69324},{\"end\":69533,\"start\":69531},{\"end\":69544,\"start\":69539},{\"end\":69802,\"start\":69796},{\"end\":69811,\"start\":69806},{\"end\":69820,\"start\":69815},{\"end\":69830,\"start\":69824},{\"end\":69837,\"start\":69834},{\"end\":69847,\"start\":69841},{\"end\":70123,\"start\":70113},{\"end\":70135,\"start\":70129},{\"end\":70334,\"start\":70329},{\"end\":70345,\"start\":70340},{\"end\":70355,\"start\":70349},{\"end\":70364,\"start\":70359},{\"end\":70810,\"start\":70800},{\"end\":70819,\"start\":70814},{\"end\":70827,\"start\":70823},{\"end\":70840,\"start\":70831},{\"end\":70850,\"start\":70844},{\"end\":71197,\"start\":71191},{\"end\":71208,\"start\":71201},{\"end\":71219,\"start\":71212},{\"end\":71228,\"start\":71223},{\"end\":71478,\"start\":71468},{\"end\":71489,\"start\":71482},{\"end\":71499,\"start\":71493},{\"end\":71510,\"start\":71503},{\"end\":71859,\"start\":71854},{\"end\":72240,\"start\":72233},{\"end\":72248,\"start\":72244},{\"end\":72258,\"start\":72252},{\"end\":72268,\"start\":72262},{\"end\":72725,\"start\":72718},{\"end\":72737,\"start\":72731},{\"end\":72747,\"start\":72741},{\"end\":72759,\"start\":72751},{\"end\":72770,\"start\":72763},{\"end\":73110,\"start\":73106},{\"end\":73119,\"start\":73114},{\"end\":73125,\"start\":73123},{\"end\":73133,\"start\":73129},{\"end\":73140,\"start\":73137},{\"end\":73506,\"start\":73502},{\"end\":73513,\"start\":73510},{\"end\":73523,\"start\":73517},{\"end\":73529,\"start\":73527},{\"end\":73541,\"start\":73533},{\"end\":73549,\"start\":73545},{\"end\":73894,\"start\":73891},{\"end\":73901,\"start\":73898},{\"end\":73913,\"start\":73909},{\"end\":73919,\"start\":73917},{\"end\":74288,\"start\":74282},{\"end\":74300,\"start\":74292},{\"end\":74679,\"start\":74673},{\"end\":74690,\"start\":74683},{\"end\":74707,\"start\":74694},{\"end\":74719,\"start\":74713}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":86693451},\"end\":61504,\"start\":61173},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10409742},\"end\":61779,\"start\":61506},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":202558580},\"end\":62087,\"start\":61781},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1888776},\"end\":62384,\"start\":62089},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":73497737},\"end\":62677,\"start\":62386},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":25209638},\"end\":63167,\"start\":62679},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9812826},\"end\":63565,\"start\":63169},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":733980},\"end\":63906,\"start\":63567},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12008695},\"end\":64342,\"start\":63908},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49291228},\"end\":64733,\"start\":64344},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10569020},\"end\":65011,\"start\":64735},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":155774532},\"end\":65439,\"start\":65013},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":68046116},\"end\":65942,\"start\":65441},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219858990},\"end\":66357,\"start\":65944},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21351739},\"end\":66719,\"start\":66359},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52978766},\"end\":67049,\"start\":66721},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":224882796},\"end\":67464,\"start\":67051},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":216642177},\"end\":67753,\"start\":67466},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":224887152},\"end\":68162,\"start\":67755},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":189824904},\"end\":68528,\"start\":68164},{\"attributes\":{\"id\":\"b20\"},\"end\":68661,\"start\":68530},{\"attributes\":{\"id\":\"b21\"},\"end\":68792,\"start\":68663},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13178535},\"end\":69265,\"start\":68794},{\"attributes\":{\"id\":\"b23\"},\"end\":69468,\"start\":69267},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":216080530},\"end\":69722,\"start\":69470},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":197618493},\"end\":70067,\"start\":69724},{\"attributes\":{\"doi\":\"arXiv:1904.07734\",\"id\":\"b26\"},\"end\":70254,\"start\":70069},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":220891274},\"end\":70706,\"start\":70256},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":12730344},\"end\":71133,\"start\":70708},{\"attributes\":{\"doi\":\"arXiv:1708.02072\",\"id\":\"b29\"},\"end\":71376,\"start\":71135},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3967864},\"end\":71784,\"start\":71378},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":211016154},\"end\":72109,\"start\":71786},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":214047809},\"end\":72601,\"start\":72111},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":214692846},\"end\":73012,\"start\":72603},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":210994829},\"end\":73402,\"start\":73014},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":198169111},\"end\":73815,\"start\":73404},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":104292349},\"end\":74205,\"start\":73817},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":35249701},\"end\":74532,\"start\":74207},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8996877},\"end\":75080,\"start\":74534}]", "bib_title": "[{\"end\":61254,\"start\":61173},{\"end\":61554,\"start\":61506},{\"end\":61847,\"start\":61781},{\"end\":62135,\"start\":62089},{\"end\":62444,\"start\":62386},{\"end\":62826,\"start\":62679},{\"end\":63256,\"start\":63169},{\"end\":63690,\"start\":63567},{\"end\":63997,\"start\":63908},{\"end\":64446,\"start\":64344},{\"end\":64788,\"start\":64735},{\"end\":65106,\"start\":65013},{\"end\":65556,\"start\":65441},{\"end\":66028,\"start\":65944},{\"end\":66427,\"start\":66359},{\"end\":66790,\"start\":66721},{\"end\":67137,\"start\":67051},{\"end\":67531,\"start\":67466},{\"end\":67836,\"start\":67755},{\"end\":68249,\"start\":68164},{\"end\":68895,\"start\":68794},{\"end\":69527,\"start\":69470},{\"end\":69792,\"start\":69724},{\"end\":70325,\"start\":70256},{\"end\":70794,\"start\":70708},{\"end\":71464,\"start\":71378},{\"end\":71850,\"start\":71786},{\"end\":72229,\"start\":72111},{\"end\":72714,\"start\":72603},{\"end\":73102,\"start\":73014},{\"end\":73498,\"start\":73404},{\"end\":73887,\"start\":73817},{\"end\":74278,\"start\":74207},{\"end\":74669,\"start\":74534}]", "bib_author": "[{\"end\":61267,\"start\":61256},{\"end\":61274,\"start\":61267},{\"end\":61282,\"start\":61274},{\"end\":61565,\"start\":61556},{\"end\":61574,\"start\":61565},{\"end\":61585,\"start\":61574},{\"end\":61856,\"start\":61849},{\"end\":61863,\"start\":61856},{\"end\":61869,\"start\":61863},{\"end\":62145,\"start\":62137},{\"end\":62154,\"start\":62145},{\"end\":62161,\"start\":62154},{\"end\":62168,\"start\":62161},{\"end\":62458,\"start\":62446},{\"end\":62468,\"start\":62458},{\"end\":62478,\"start\":62468},{\"end\":62487,\"start\":62478},{\"end\":62498,\"start\":62487},{\"end\":62834,\"start\":62828},{\"end\":63268,\"start\":63258},{\"end\":63279,\"start\":63268},{\"end\":63291,\"start\":63279},{\"end\":63703,\"start\":63692},{\"end\":64009,\"start\":63999},{\"end\":64020,\"start\":64009},{\"end\":64031,\"start\":64020},{\"end\":64041,\"start\":64031},{\"end\":64053,\"start\":64041},{\"end\":64460,\"start\":64448},{\"end\":64470,\"start\":64460},{\"end\":64481,\"start\":64470},{\"end\":64490,\"start\":64481},{\"end\":64500,\"start\":64490},{\"end\":64800,\"start\":64790},{\"end\":65120,\"start\":65108},{\"end\":65130,\"start\":65120},{\"end\":65140,\"start\":65130},{\"end\":65151,\"start\":65140},{\"end\":65570,\"start\":65558},{\"end\":65583,\"start\":65570},{\"end\":65596,\"start\":65583},{\"end\":65606,\"start\":65596},{\"end\":65616,\"start\":65606},{\"end\":66037,\"start\":66030},{\"end\":66046,\"start\":66037},{\"end\":66057,\"start\":66046},{\"end\":66067,\"start\":66057},{\"end\":66438,\"start\":66429},{\"end\":66446,\"start\":66438},{\"end\":66456,\"start\":66446},{\"end\":66466,\"start\":66456},{\"end\":66799,\"start\":66792},{\"end\":66808,\"start\":66799},{\"end\":66818,\"start\":66808},{\"end\":67152,\"start\":67139},{\"end\":67163,\"start\":67152},{\"end\":67174,\"start\":67163},{\"end\":67545,\"start\":67533},{\"end\":67558,\"start\":67545},{\"end\":67569,\"start\":67558},{\"end\":67850,\"start\":67838},{\"end\":67863,\"start\":67850},{\"end\":67874,\"start\":67863},{\"end\":68260,\"start\":68251},{\"end\":68271,\"start\":68260},{\"end\":68283,\"start\":68271},{\"end\":68294,\"start\":68283},{\"end\":68308,\"start\":68294},{\"end\":68318,\"start\":68308},{\"end\":68678,\"start\":68665},{\"end\":68683,\"start\":68678},{\"end\":68908,\"start\":68897},{\"end\":68916,\"start\":68908},{\"end\":68925,\"start\":68916},{\"end\":68934,\"start\":68925},{\"end\":68951,\"start\":68934},{\"end\":69310,\"start\":69301},{\"end\":69320,\"start\":69310},{\"end\":69332,\"start\":69320},{\"end\":69535,\"start\":69529},{\"end\":69546,\"start\":69535},{\"end\":69804,\"start\":69794},{\"end\":69813,\"start\":69804},{\"end\":69822,\"start\":69813},{\"end\":69832,\"start\":69822},{\"end\":69839,\"start\":69832},{\"end\":69849,\"start\":69839},{\"end\":70125,\"start\":70109},{\"end\":70137,\"start\":70125},{\"end\":70336,\"start\":70327},{\"end\":70347,\"start\":70336},{\"end\":70357,\"start\":70347},{\"end\":70366,\"start\":70357},{\"end\":70812,\"start\":70796},{\"end\":70821,\"start\":70812},{\"end\":70829,\"start\":70821},{\"end\":70842,\"start\":70829},{\"end\":70852,\"start\":70842},{\"end\":71199,\"start\":71189},{\"end\":71210,\"start\":71199},{\"end\":71221,\"start\":71210},{\"end\":71230,\"start\":71221},{\"end\":71480,\"start\":71466},{\"end\":71491,\"start\":71480},{\"end\":71501,\"start\":71491},{\"end\":71512,\"start\":71501},{\"end\":71861,\"start\":71852},{\"end\":72242,\"start\":72231},{\"end\":72250,\"start\":72242},{\"end\":72260,\"start\":72250},{\"end\":72270,\"start\":72260},{\"end\":72727,\"start\":72716},{\"end\":72739,\"start\":72727},{\"end\":72749,\"start\":72739},{\"end\":72761,\"start\":72749},{\"end\":72772,\"start\":72761},{\"end\":73112,\"start\":73104},{\"end\":73121,\"start\":73112},{\"end\":73127,\"start\":73121},{\"end\":73135,\"start\":73127},{\"end\":73142,\"start\":73135},{\"end\":73508,\"start\":73500},{\"end\":73515,\"start\":73508},{\"end\":73525,\"start\":73515},{\"end\":73531,\"start\":73525},{\"end\":73543,\"start\":73531},{\"end\":73551,\"start\":73543},{\"end\":73896,\"start\":73889},{\"end\":73903,\"start\":73896},{\"end\":73915,\"start\":73903},{\"end\":73921,\"start\":73915},{\"end\":74290,\"start\":74280},{\"end\":74302,\"start\":74290},{\"end\":74681,\"start\":74671},{\"end\":74692,\"start\":74681},{\"end\":74709,\"start\":74692},{\"end\":74721,\"start\":74709}]", "bib_venue": "[{\"end\":61310,\"start\":61282},{\"end\":61613,\"start\":61585},{\"end\":61902,\"start\":61869},{\"end\":62204,\"start\":62168},{\"end\":62509,\"start\":62498},{\"end\":62875,\"start\":62834},{\"end\":63331,\"start\":63291},{\"end\":63715,\"start\":63703},{\"end\":64091,\"start\":64053},{\"end\":64504,\"start\":64500},{\"end\":64844,\"start\":64800},{\"end\":65190,\"start\":65151},{\"end\":65656,\"start\":65616},{\"end\":66115,\"start\":66067},{\"end\":66505,\"start\":66466},{\"end\":66854,\"start\":66818},{\"end\":67221,\"start\":67174},{\"end\":67573,\"start\":67569},{\"end\":67922,\"start\":67874},{\"end\":68333,\"start\":68318},{\"end\":68585,\"start\":68532},{\"end\":68993,\"start\":68951},{\"end\":69299,\"start\":69267},{\"end\":69569,\"start\":69546},{\"end\":69853,\"start\":69849},{\"end\":70107,\"start\":70069},{\"end\":70435,\"start\":70366},{\"end\":70891,\"start\":70852},{\"end\":71187,\"start\":71135},{\"end\":71548,\"start\":71512},{\"end\":71916,\"start\":71861},{\"end\":72319,\"start\":72270},{\"end\":72783,\"start\":72772},{\"end\":73177,\"start\":73142},{\"end\":73580,\"start\":73551},{\"end\":73970,\"start\":73921},{\"end\":74331,\"start\":74302},{\"end\":74763,\"start\":74721},{\"end\":61637,\"start\":61615},{\"end\":61931,\"start\":61904},{\"end\":62232,\"start\":62206},{\"end\":62912,\"start\":62877},{\"end\":63367,\"start\":63333},{\"end\":64125,\"start\":64093},{\"end\":64510,\"start\":64506},{\"end\":65225,\"start\":65192},{\"end\":65692,\"start\":65658},{\"end\":66159,\"start\":66117},{\"end\":66540,\"start\":66507},{\"end\":66886,\"start\":66856},{\"end\":67260,\"start\":67223},{\"end\":67579,\"start\":67575},{\"end\":67962,\"start\":67924},{\"end\":68344,\"start\":68335},{\"end\":69031,\"start\":68995},{\"end\":69859,\"start\":69855},{\"end\":70499,\"start\":70437},{\"end\":70926,\"start\":70893},{\"end\":71576,\"start\":71550},{\"end\":72364,\"start\":72321},{\"end\":73605,\"start\":73582},{\"end\":74014,\"start\":73972},{\"end\":74356,\"start\":74333},{\"end\":74801,\"start\":74765}]"}}}, "year": 2023, "month": 12, "day": 17}