{"id": 59292011, "updated": "2023-10-04 08:02:45.234", "metadata": {"title": "FaceForensics++: Learning to Detect Manipulated Facial Images", "authors": "[{\"first\":\"Andreas\",\"last\":\"Rossler\",\"middle\":[]},{\"first\":\"Davide\",\"last\":\"Cozzolino\",\"middle\":[]},{\"first\":\"Luisa\",\"last\":\"Verdoliva\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Riess\",\"middle\":[]},{\"first\":\"Justus\",\"last\":\"Thies\",\"middle\":[]},{\"first\":\"Matthias\",\"last\":\"Niessner\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 1, "day": 25}, "abstract": "The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns on the implication on the society. At best, this leads to a loss of trust in digital content, but it might even cause further harm by spreading false information and the creation of fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them - either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, and FaceSwap as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of about 1.5 million manipulated images. Thus, this database is at least an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression and clearly outperforms human observers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1901.08971", "mag": "2982058372", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/RosslerCVRTN19", "doi": "10.1109/iccv.2019.00009"}}, "content": {"source": {"pdf_hash": "b65e757f52635714911f37c330a87ab44fb66cfe", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1901.08971v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1901.08971", "status": "GREEN"}}, "grobid": {"id": "5369e555fda18115802a337d6a37b09979573286", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b65e757f52635714911f37c330a87ab44fb66cfe.txt", "contents": "\nFaceForensics++: Learning to Detect Manipulated Facial Images\n\n\nAndreas R\u00f6ssler \nTechnical University of Munich\n\n\nDavide Cozzolino \nUniversity Federico II of Naples\n\n\nLuisa Verdoliva \nUniversity Federico II of Naples\n\n\nChristian Riess \nUniversity of Erlangen-Nuremberg\n\n\nJustus Thies \nTechnical University of Munich\n\n\nMatthias Nie\u00dfner \nTechnical University of Munich\n\n\nFaceForensics++: Learning to Detect Manipulated Facial Images\n\nFigure 1: FaceForensics++ is a database of facial forgeries that enables researchers to train deep-learning-based approaches in a supervised fashion. The database contains manipulations created with three state-of-the-art methods, namely, Face2Face, FaceSwap, and DeepFakes.AbstractThe rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns on the implication on the society. At best, this leads to a loss of trust in digital content, but it might even cause further harm by spreading false information and the creation of fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them -either automatically or by humans.To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection 1 . In particular, the benchmark is based on Deep-Fakes [1], Face2Face [58], and FaceSwap [2] as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available 2 and contains a hidden test set as well as a database of about 1.5 million manipulated images. Thus, this database is at least an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression and clearly outperforms human observers.\n\nIntroduction\n\nNowadays, manipulation of visual content is omnipresent and one of the most critical topics in our digital society. DeepFakes [1], for instance, showed how computer graphics and visualization techniques can be used to defame persons by replacing their face by the face of a different person. Faces are of special interest to current manipulation methods for various reasons: firstly, the reconstruction and tracking of human faces is a well-examined field in computer vision [67], which is the foundation of these editing approaches. Secondly, faces play a central role in human communication, as the face of a person can emphasize a message or it can even convey a message in its own right [28].\n\nCurrent facial manipulation methods can be separated into two categories: facial expression manipulation and facial identity manipulation (see Fig. 2). One of the most prominent facial expression manipulation techniques is the method of Thies et al. [58] called Face2Face. It enables the transfer of facial expressions of one person to another person in real time using only commodity hardware. Followup work such as \"Synthesizing Obama\" [55] is able to ani- Figure 2: Advances in the digitization of human faces are the basis for modern facial image editing tools. The editing tools can be split in two main categories: identity modification and expression modification. Besides manually editing the face using tools like Photoshop, many automatic approaches have been proposed in the last few years. The most prominent and most spread identity editing technique is face swapping. The popularity of these techniques stems from the fact that lightweight systems can run on mobile phones. Facial reenactment is a technique to alter the expressions of a person by transferring the expressions of a source person to the target. mate the face of a person based on an audio input sequence. \"Bringing Portraits to Life\" [8] enables the editing of expressions in a photograph.\n\nIdentity manipulation is the second category of facial forgeries. Instead of changing expressions, these methods replace the face of a person with the face of another person. This category is known as face swapping. It became popular with wide-spread consumer-level applications like Snapchat. DeepFakes also performs face swapping, but via deep learning. While face swapping based on simple computer graphics techniques is running in real time, Deep-Fakes need to be trained for each pair of videos, which is a time-consuming task.\n\nIn this work, we show that we can automatically and reliably detect such manipulations, and thereby outperform human observers by a significant margin. We leverage the recent advances in deep learning, especially, the ability to learn extremely powerful image features with convolutional neural networks (CNNs). We tackle the detection problem by training a neural network in a supervised fashion. To this end, we generated a large-scale dataset of manipulations based on Face2Face, the classical computer graphicsbased FaceSwap [2] and DeepFakes.\n\nAs the digital media forensics field lacks a benchmark for forgery detection, we propose an automated benchmark that considers the three manipulation methods in a realistic scenario, i.e., with random compression and random dimensions. Using this benchmark, we evaluate the current state-of-the-art detection methods as well as our forgery detection pipeline that considers the restricted field of facial manipulation methods.\n\nTo summarize, our paper makes the following contributions:\n\n\u2022 an automated benchmark for facial manipulation detection under random compression for a standardized comparison, including a human baseline,\n\n\u2022 a novel dataset of manipulated facial imagery composed of more than 1.5 million images from 1,000 videos with pristine (i.e., real) sources and target ground truth to enable supervised learning,\n\n\u2022 an extensive evaluation of state-of-the-art hand-crafted and learned forgery detectors in various scenarios,\n\n\u2022 a state-of-the-art forgery detection method tailored to facial manipulations.\n\n\nRelated Work\n\nThe paper intersects several fields in computer vision and digital multimedia forensics. We cover the most important related papers in the following paragraphs.\n\nFace Manipulation Methods: In the last two decades, interest in virtual face manipulation has rapidly increased. A comprehensive state-of-the-art report has been published by Zollh\u00f6fer et al. [67]. Bregler et al. [13] presented an imagebased approach called Video Rewrite to automatically create a new video of a person with generated mouth movements. With Video Face Replacement [20], Dale et al. presented one of the first automatic face swap methods. Using single-camera videos, they reconstruct a 3D model of both faces and exploit the corresponding 3D geometry to warp the source face to the target face. Garrido et al. [29] presented a similar system that replaces the face of an actor while preserving the original expressions. VDub [30] uses high-quality 3D face capturing techniques to photorealistically alter the face of an actor to match the mouth movements of a dubber. Thies et al. [57] demonstrated the first real-time expression transfer for facial reenactment. Based on a consumer level RGB-D camera, they reconstruct and track a 3D model of the source and the target actor. The tracked deformations of the source face are applied to the target face model. As a final step, they blend the altered face on top of the original target video. Face2Face, proposed by Thies et al. [58], is an advanced real-time facial reenactment system, capable of altering facial movements in commodity video streams, e.g., videos from the internet. They combine 3D model reconstruction and image-based rendering techniques to generate their output. The same principle can be also applied in Virtual Reality in combination with eye-tracking and reenactment [59] or be extended to the full body [60]. Kim et al. [38] learn an image-toimage translation network to convert computer graphic renderings of faces to real images. Suwajanakorn et al. [55] learned the mapping between audio and lip motions, while their compositing approach builds on similar techniques to Face2Face [58]. Averbuch-Elor et al. [8] present a reenactment method, Bringing Portraits to Life, which employs 2D warps to deform the image to match the expressions of a source actor. They also compare to the Face2Face technique and achieve similar quality.\n\nRecently, several face image synthesis approaches using deep learning techniques have been proposed. Lu et al. [47] provide an overview. Generative adversarial networks (GANs) are used to apply Face Aging [7], to generate new viewpoints [34], or to alter face attributes like skin color [46]. Deep Feature Interpolation [61] shows impressive results on altering face attributes like age, mustache, smiling etc. Similar results of attribute interpolations are achieved by Fader Networks [43]. Most of these deep learning based image synthesis techniques suffer from low image resolutions. Karras et al. [36] improve the image quality using progressively growing of GANs. Their results include high-quality synthesis of faces.\n\nMultimedia Forensics: Multimedia forensics aims to ensure authenticity, origin, and provenance of an image or video without the help of an embedded security scheme. Focusing on integrity, early methods are driven by handcrafted features that capture expected statistical or physicsbased artifacts that occur during image formation. Surveys on these methods can be found in [26,53]. More recent literature is indeed concentrated on CNN-based solutions both through supervised and unsupervised learning [10,17,12,9,35,66]. For videos, the main body of work focuses on detecting manipulations that can be created with relatively low effort, such as dropped or duplicated frames [62,31,45], varying interpolation types [25], copy-move manipulations [11,21], or chroma-key compositions [48].\n\nSome other papers explicitly refer to detecting manipulations related to faces, like distinguishing computer generated faces from natural ones [22,15,51], morphed faces [50], face splicing [24,23], face swapping [65,37] and DeepFakes [5,44,33]. For face manipulation detection some approaches exploit specific artifacts, arising from the synthesis process, like eye blinking [44], or color, texture and shape cues [24,23]. Other papers are more general and propose a deep network trained to capture the subtle inconsistencies arising from low-level and/or high level features [50,65,37,5,33]. These approaches show impressive results, however robustness issues are addressed only in very few works, even though it is of paramount importance for practical applications. For example, operations like compression and resizing are known for laundering manipulation traces from the data. In a real scenario, these basic operations are standard when images and videos are for example uploaded to social networks, which is one of the most important application field for forensic analysis. To this end, our dataset is designed to cover a realistic scenario, i.e., videos from the wild, manipulated and compressed with different quality level (see Section 3). The availability of a such large and various dataset can help researchers to benchmark their approaches and develop better forgery detectors for facial imagery.\n\nForensic Analysis Datasets: Classical forensics datasets have been created with significant manual effort under very controlled conditions, to isolate specific properties of the data like camera artifacts. While several datasets were proposed that include image manipulations, only a few of them address also the important case of video footage. MICC F2000, for example, is an image copy-move manipulation dataset consisting of a collection of 700 forged images from various sources [6]. The First IEEE Image Forensics Challenge Dataset 3 comprises a total of 1176 forged images; the Wild Web Dataset [63] with 90 real cases of manipulations coming from the web and the Realistic Tampering dataset [42] including 220 forged images. A database of 2010 FaceSwap-and SwapMe-generated images has been proposed by Zhou et al. [65]. Recently, Korshunov and Marcel [41] made a database of 620 Deepfakes videos publicly available which they created from multiple videos for each of 43 subjects. The National Institute of Standards and Technology (NIST) released the most extensive dataset for generic image manipulation comprising about 50, 000 forged images (both local and global manipulations) and around 500 forged videos [32].\n\nIn contrast to these datasets, we generated a database containing more than 1.5 million images from 3000 fake videos containing 1000 identities which is a magnitude more than existing databases. We evaluate the importance of a large training corpus in Section 4. \n\n\nLarge-Scale Facial Forgery Database\n\nA core contribution of this paper is our FaceForensics++ dataset extending the preliminary FaceForensics dataset [52] , which is already actively used by the forensic community.\n\nThis novel large-scale dataset enables us to train a stateof-the-art forgery detector for facial image manipulation in a supervised fashion (see Section 4). To this end, we used three automated state-of-the-art face manipulation methods that are applied to 1,000 pristine videos downloaded from the Internet (see Fig. 3 for some statistics). For a realistic scenario, we chose to collect videos in the wild, more specifically from YouTube. However, early experiments with all manipulation methods showed that the target face has to be nearly front-facing, to prevent the methods from failing or producing strong artifacts (see Fig. 4). Therefore, we perform a manual screening of the resulting clips to ensure a high quality video selection and to avoid videos with face occlusions. We selected 1,000 video sequences containing 509, 914 images which we use as our pristine data. For more details, we refer to Appendix A.\n\nTo generate a large scale manipulation database, we adapted state-of-the-art video editing methods to work fully automatically. In the following paragraphs, we describe the modifications of these methods.\n\nFor our dataset, we chose two computer graphics-based approaches (Face2Face and FaceSwap) and a learningbased approach (DeepFakes). The computer graphics-based approaches are both 3D editing techniques that reconstruct a 3D model of a face and apply edits in 3D. FaceSwap is a lightweight editing tool that copies the face region from one image to another by using sparse face marker positions. In contrast, Face2Face is a more sophisticated technique with superior face tracking and modeling that enables reenactment of facial expressions. Similar to FaceSwap, DeepFakes is an identity manipulation tool which is a deeplearning-based method that uses an auto-encoder to replace the face of a target video by the face of a source video. All three methods require source and a target actor video pairs as input. The final output of each method is a video composed of generated images. Besides the manipulation output, we can also access ground truth masks that indicate whether a pixel has been modified or not. These binary masks can be used to train forgery localization methods.\n\nFaceSwap FaceSwap is a graphics-based approach to transfer the face region from a source video to a target video. Based on sparse detected facial landmarks the face region is extracted. Using these landmarks, the method fits a 3D template model using blendshapes. This model is backprojected to the target image by minimizing the difference between the projected shape and the localized landmarks using the textures of the input image. Finally, the rendered model is blended with the image and color correction is applied. We perform these steps for all pairs of source and target frames until one video end. The implementation is computationally light-weight and can be computed quite fast with CPU only.\n\nDeepFakes DeepFakes is a synonym for face replacement that is based on deep learning. A face in a target sequence is replaced by a face that has been observed in a source video or image collection. There are various public implementations of DeepFakes available, most notably FakeApp [3] and the faceswap github [1]. The method is based on two autoencoders with a shared encoder that are trained to reconstruct training images of the source and the target face, respectively. They are using a face detector to crop and to align the images. To create a fake image they apply the trained encoder and the decoder of the source face on the target face. The autoencoder output is then blended with the rest of the image using Poisson image editing [49].\n\nFor our dataset, we use the faceswap github implementation. We slightly modify the implementation by replacing the manual training data selection with a fully automated data loader. We used the default parameters to train the video-pair models. Since the training of these models is very time-consuming, we also publish the models as part of the dataset. This allows to easily generate additional manipulations of these persons with different post-processing.\n\nFace2Face Face2Face [58] is a facial reenactment system that transfers the expressions of a source video to a target video while maintaining the identity of the target person. The original implementation is based on two video input streams with a manual key-frame selection. These frames are used to generate a dense reconstruction of the face which can be used to re-synthesize the face under different illumination and expressions. To process our video database, we adapt the Face2Face approach to fullyautomatically create reenactment manipulations. We process each video in a preprocessing pass; here, we use the first frames in order to obtain a temporary face identity (i.e., a 3D model), and track the expressions over the remaining frames. In order to select the keyframes that are used to improve the identity fitting and the static texture, we select the frames with the left-and rightmost angle of the face. Based on this identity reconstruction, we track the whole video to compute per frame the expression, rigid pose, and lighting parameters as done in the original implementation of Face2Face. We generate the reenactment video outputs by transferring the source expression parameters of each frame (i.e., 76 Blendshape coefficients) to the target video. More details of the reenactment process can be found in the original paper [58].\n\nPostprocessing -Video Quality To create a realistic scenario of manipulated videos, we generate output videos with different quality levels as they are used in many social networks. Since raw videos are rarely found on the Internet, we compress the videos using the H.264 codec, which is used by social networks or video-sharing websites. To generate high quality videos, we use a light compression denoted by HQ (constant rate quantization parameter equal to 23) which is visually nearly loss-less. Low quality videos are produced using a quantization of 40 (LQ ).\n\n\nForgery Detection\n\nWe cast the forgery detection as a binary classification problem per frame of the manipulated videos. The following sections show the results of manual and automatic forgery detection. For all experiments, we split the dataset into a fixed training, validation, and test set, consisting of 720, 140, and 140 videos respectively. All evaluations are reported using videos from the test set.\n\n\nForgery Detection of Human Observers\n\nTo evaluate the performance of humans in the task of forgery detection, we conducted a user study with 143 participants consisting mostly of computer science university students. This forms the baseline for the automated forgery detection methods.\n\nLayout of the User Study: After a short introduction to the binary task, users are told to classify randomly selected images from our test set. The selected images vary in image quality as well as manipulation method and we used a 50:50 split of pristine and fake images. Since the time for inspection of an image may be important, we randomly set a time limit of 2, 4 or 6 seconds after which we hide the image. Afterwards, the users were asked whether the displayed image is 'real' or 'fake'. To ensure that the users spend the available time on inspection, the question is asked after the image has been displayed and not during the observation time. We designed the study to only take a few minutes per participant showing 60 images per attendee which results in a collection of 8580 human decisions.\n\nEvaluation: In Fig. 5 we show the results of our study on all quality levels. It shows the dependency between the ability to detect fakes and video quality. With a lower video quality, the human performance decreases in average from 71% to 61%. The graph shows the numbers averaged across all time intervals since the different time constraints did not result in significantly different observations. Figure 5: Forgery detection results of our user study with 143 participants. The accuracy is dependent on the video quality and results in a decreasing accuracy rate that is 72% in average on raw videos, 71% on high quality, and 61% on low quality videos.\n\nNote that the user study contained fake images of all three manipulation methods and pristine images. In this setting Face2Face is particularly harder to detect by human observers since Face2Face does not introduce a strong semantic change, and thus, introducing only subtle visual artifacts in comparison to the face replacement methods. Figure 6: Our domain specific forgery detection pipeline: the input image is processed by a robust face tracking method, we use the information to extract the region of the image that is covered by the face, this region is fed into a learned classification network that outputs the prediction.\n\n\nAutomatic Forgery Detection Methods\n\nWe analyze the detection accuracy of our forgery detection pipeline depicted in Fig. 6. Since our goal is to detect forgeries of facial imagery, we use additional domainspecific information that we can extract from input sequences. To this end, we use the state-of-the-art face tracking method by Thies et al. [58] to track the face in the video and to extract the face region of the image. We use a conservative crop (enlarged by a factor of 1.3) around the center of the tracked face, enclosing the reconstructed face. This incorporation of domain knowledge improves the overall performance of a forgery detector in comparison to a na\u00efve approach that uses the whole image as input (see Sec. 4.2.2). We evaluated various variants of our approach by using different state-of-the-art classification methods. We are considering learning-based methods used in the forensic community for generic manipulation detection [10,17], computer-generated vs natural image detection [51] and face tampering detection [5]. In addition, we show that the classification based on XceptionNet [14] outperforms all other variants in detecting fakes.\n\n\nDetection based on Steganalysis Features:\n\nFollowing the method by Fridrich et al. [27], we are using hand-crafted features to detect forgeries. The features are co-occurrences on 4 pixels patterns along the horizontal and vertical direction on the high-pass images for a total feature length of 162. These features are then used to train a linear Support Vector Machine (SVM) classifier. This technique was the winning approach in the first IEEE Image forensic Challenge [16]. We provide a 128 \u00d7 128 central crop-out of the face as input to the method. While the handcrafted method outperforms human accuracy on raw images by a large margin, it struggles to cope with compression, which leads to an accuracy below human performance for low quality videos (see Fig. 7 and Table 1). \n\n\nDetection based on Learned Features:\n\nWe evaluate five network architectures known from the literature to solve the classification task:\n\n(1) Cozzolino et al. [17] cast the hand-crafted Steganalysis features from the previous section in a CNN-based network. We fine-tune this network on our large scale dataset.\n\n(2) We use our dataset to train the convolutional neural network proposed by Bayar and Stamm [10] that uses a constrained convolutional layer followed by two convolutional, two max-pooling and three fully-connected layer. The constrained convolutional layer is specifically designed to suppress the high-level content of the image.\n\n(3) Rahmouni et al. [51] adopt different CNN architectures with a global pooling layer that computes four statistics (mean, variance, maximum and minimum). We consider the Stats-2L network that had the best performance.\n\n(4) MesoInception-4 [5] is a CNN-based network inspired by InceptionNet [56] to detect face tampering in videos. The network has two inception modules and two classic convolution layers interlaced by max-pooling layers. Afterwards, there are two fully-connected layers. Instead of the classic cross-entropy loss, the authors propose the mean squared error between true and predicted labels. We resize the face images to 256 \u00d7 256, the input of the network.\n\n(5) XceptionNet [14] is a traditional CNN trained on ImageNet based on separable convolutions with residual  Table 1 for the average accuracy values. Besides the Full Image XceptionNet, we use the proposed preextraction of the face region as input to the approaches.. connections. We transfer it to our task by replacing the final fully connected layer with two outputs. The other layers are initialized with the ImageNet weights. To set up the newly inserted fully connected layer, we fix all weights up to the final layers and pre-train the network for 12 epochs. We use hyper-parameter optimization to find the best performing model based on validation accuracy. After this step, we train the best performing network for 60 more epochs where one epoch equals one-fourth of the actual dataset size.\n\nAll five methods are trained with the Adam optimizer using the default values for the moments (\u03b2 1 = 0.9, \u03b2 2 = 0.999, = 10 \u22128 ). We stop the training process if the validation accuracy doesn't change for 10 consecutive epochs. Validation accuracies are computed on 100 images per video to decrease training time. Finally, we solve the imbalance between real and fake images in the binary task (i.e., the number of fake images being roughly three times as large as the number of pristine images) by weighting the training images correspondingly.\n\nComparison of our Forgery Detection Variants: Fig. 7 shows the results of a binary forgery detection task using all network architectures evaluated separately on all three manipulation methods and at different video quality levels. All approaches achieve very high performance on raw input data. Performance drops for compressed videos, particularly for hand-crafted features and for shallow CNN architectures [10,17]. The neural networks are better at handling these situations, with XceptionNet being able to achieve compelling results on weak compression while still maintaining reasonable performance on low quality images, as it benefits from its pre-training on ImageNet as well as larger network capacity.\n\nTo compare the results of our user study to the performance of our automatic detectors, we also tested the detection variants on a dataset containing images from all manipulation methods. Fig. 8 and Table 1 show the results on the full dataset. Here, our automated detectors outperform human performance by a large margin (cf. Fig. 5). We also evaluate a na\u00efve forgery detector operating on the full image (resized to the XceptionNet input) instead of using face tracking information (see Fig. 8, rightmost column). Due to the lack of domain-specific information, the XceptionNet classifier has in this scenario a significantly lower accuracy. To summarize, domain-specific information in combination with a XceptionNet classifier shows the best performance in each test. We use this network to further understand the influence of the training corpus size and its ability to distinguish between the different manipulation methods.\n\nEvaluation of the Training Corpus Size: Fig. 9 shows the importance of the training corpus size. To this end, we trained the XceptionNet classifier with different training corpus sizes on all three video quality level separately. The overall performance increases with the number of training images which is particularly important for low quality video footage, as can be seen in the bottom of the figure. Figure 9: The detection performance of our approach using XceptionNet depends on the training corpus size. Especially, for low quality video data, a large database is needed.\n\n\nBenchmark\n\nBesides, our large-scale manipulation database, we publish a competitive benchmark for facial forgery detection. To this end, we collected 700 additional videos and manipulated a subset of those in a similar fashion as in Section 3 for each of our three manipulation methods. As uploaded videos, e.g., to social networks, will be post-processed in various ways, we obscure all selected videos multiple times by, e.g., unknown re-sizing, compression method as well as bit-rate to ensure realistic conditions. This processing is directly applied on raw videos. Finally, we manually select a single challenging frame from each video based on visual inspection. Specifically, we get a set of 700 images, each image randomly taken from either the manipulation methods or the original footage. Note, we do not necessarily have an equal split of pristine and fake images nor an equal split of the used manipulation methods. The ground truth labels are hidden and are used on our host server to evaluate the classification accuracy of the submitted models. The automated benchmark allows submissions every two weeks from a single submitter to prevent overfitting (similar to existing benchmarks [19]).\n\nAs baselines, we evaluate all previously trained models on the benchmark and report the numbers of the best performing model for each detection method separately (see Table 2). Besides the Full Image XceptionNet, we use the proposed pre-extraction of the face region as input to the  Table 2: Results of the best performing model of each detection method, i.e., trained on either raw, low or high quality. We report precision results for DeepFakes (DF), Face2Face (F2F), FaceSwap (FS), and pristine images (Real) as well as the overall total accuracy.\n\napproaches. The relative performance of the classification models is similar to our database test set (see Table 1). However, since the benchmark scenario deviates from the training database, the overall performance of the models is lower, especially, for the pristine image detection precision. The benchmark is already publicly available to the community and we hope that it leads to a standardized comparison of follow-up work.\n\n\nDiscussion & Conclusion\n\nWhile current state-of-the-art facial image manipulation methods exhibit visually stunning results, we demonstrate that they can be detected by trained forgery detectors. It is particularly encouraging that also the challenging case of low-quality video can be tackled by learning-based approaches, where humans and hand-crafted features exhibit difficulties. To train detectors using domain-specific knowledge, we introduce a novel dataset of videos of manipulated faces that exceeds all existing publicly available forensic datasets by an order of magnitude.\n\nIn this paper we focus on the influence of compression to the detectability of state-of-the-art manipulation methods and propose a standardized benchmark for follow-up work. All image data, trained models as well as our benchmark are publicly available and are already used by other researchers. Especially, transfer learning is of high interest in the forensic community. As new manipulation methods appear by the day, methods have to be developed that are able to detect fakes with no or little training data. Our database is already used for this forensic transfer learning task, where knowledge of one source manipulation domain is transferred to another target domain, as shown by Cozzolino et al. [18]. We hope that the dataset and benchmark becomes a stepping stone for even more researchers to work in the field of digital media forensics, and in particular with a focus on facial forgeries.\n\n\nAcknowledgement\n\nWe gratefully acknowledge the support of this research by the AI Foundation, a TUM-IAS Rudolf M\u00f6\u00dfbauer Fellowship, the and the ERC Starting Grant Scan2CAD (804724), and Google Faculty Award. We would also like to thank Google's Chris Bregler for help with the cloud computing. In addition, this material is based on research sponsored by the Air Force Research Laboratory and the Defense Advanced Research Projects Agency under agreement number FA8750-16-2-0204. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory and the Defense Advanced Research Projects Agency or the U.S. Government.\n\n\nMethods\n\n\nTrain\n\nValidation Test  \n\n\nA. Pristine Data Acquisition\n\nIn this section we give details on the selection of videos that we use as input to state-of-the-art manipulation methods. The videos have to fulfill certain criteria. Early experiments with all manipulation methods showed that the target face has to be nearly front-facing, to prevent the methods from failing or producing strong artifacts (see Fig. 4 in main paper). In addition, the manipulation methods exhibit difficulties in handling occlusions. Thus, we ensure that the face is visible in all frames to generate the best possible results.\n\nFor a realistic scenario, we chose to collect videos in the wild, more specifically from YouTube. As the YouTube search engine limits search hits, we made use of the YouTube-8m dataset [4] to collect videos with the tags \"face\", \"newscaster\" or \"newsprogram\" and also included videos which we collected from the YouTube search interface with the same tags and additional tags like \"interview\", \"blog\" or \"video blog\". To ensure adequate video quality, we only downloaded videos that offer a resolution of 480p or higher. For every video, we save its metadata to sort them by properties later on. In order to not change any image information and also to prevent the introduction of additional artifacts, we extract all video sequences into a lossless image format.\n\nIn order to match the above requirements, we first process all downloaded videos with a standard CPU Dlib face detector [39], which is based on Histograms of Oriented Gradients (HOG). During this step, we track the largest detected face by ensuring that the centers of two detections of consecutive frames are pixel-wise close. The histogrambased face tracker was chosen to ensure that the resulting video sequences contain little occlusions and thus contain easy-to-manipulate faces.\n\nMethods like Face2Face or DeepFakes rely on many training images for each video. We therefore select all sequences with a minimal length of 280 after removal of the first and last 10 to omit transition effects like fading overlays in TV shows. In addition, we perform a manual screening of the resulting clips to ensure a high quality video selection and to avoid videos with face occlusions. We selected 1,000 video sequences containing 509, 914 images which we use as our pristine data. All examined manipulation methods need a source and a target video. In case of facial reenactment, the expressions of the source video are transferred to the target video while retaining the identity of the target person. In contrast, the face swapping methods replace the face in the target video with the face in the source video. For the face swap methods, one has also to select source and target videos with similar resolutions to avoid strong visual artifacts. To this end, we compare the bounding box sizes that are computed by the DLib face detector. Further, we ensure that a video pair contains persons of the same gender and that the frame rates are similar.\n\n\nB. Classification of Manipulation Method\n\nTo train the XceptionNet classification network to distinguish between all three manipulation methods and the pristine images, we adapted the final output layer to return four class probabilities. The network is trained on the full dataset containing all pristine and manipulated images. On raw data the network is able to achieve a 99.08% accuracy, which slightly decreases for the high quality compression to 97.33% and to 86.69% on low quality images.\n\n\nC. Forgery Localization\n\nForgery localization is an important task in the field of digital multimedia forensics. It is the term for forgery segmentation, i.e., predicting a probability map with the size of the input image indicating whether a pixel has been altered or not. We built our database to also include such ground truth segmentation masks for each manipulation method which are published alongside the image data and can be used to train neural networks in a supervised fashion. We evaluate the performance of our method described in the main paper to detect forgeries on the task of forgery localization. I.e., instead of a binary decision on the image, we predict per-pixel classifications. To this end, we use our method in combination with the XceptionNet, as it performed better than the other classification networks for the forgery detection task. In the following, we detail the network architecture and show an evaluation of our approach.\n\nNetwork Architecture: To enable per-pixel classification we adapt the architecture presented in the main paper as follows. We designed the network to estimate the probability of a single patch being manipulated. To this end, we are using a sliding window with a size of 128 \u00d7 128 pixels with stride 16. Thus, resulting in a probability map with the size of a sixteenth part of the original image. We are using a linear interpolation to distribute the likelihoods to all pixels of the frames.  : Our forgery localization network is able to reliably detect manipulated pixels of a face in an image. In this experiment, we manipulate only the lower part of the face using the Face2Face algorithm [58]. As can be seen, the trained network does not learn a simple face detector, but instead correctly identifies the manipulated regions.  Figure 12: Examples of forgery localization results on three real and manipulated images on our three quality levels.\n\nTraining: The network is trained on randomly extracted patches from the frames. We compute smooth labels for each patch in a 32 \u00d7 32 neighborhood around the center of the patch using a box filter assuming manipulated pixels having a label of class 1 and pristine pixels having the class label 0. Based on these smooth labels that are in the range of [0, 1], we employ a cross-entropy loss to train the network.\n\nWe are using ADAM [40] to optimize the network, with a learning rate of 0.0001 and a mini-batch size of 96 patches. The 96 patches are extracted from 16 forged frames and the corresponding 16 pristine ones. Convergence of the training is measured by the change in accuracy. If the accuracy does not improve for 10 epochs (each 1000 iterations), we stop the training process.\n\nEvaluation of the Localization Performance: Fig. 10 shows an evaluation of our approach applied to our test set. As can be seen our approach is able to reliably localize the manipulated region in a raw image. Similar to the detection accuracy reported in the main paper, the performance is lowered by a reduced quality level but is still at a high level. In Fig. 12, we show qualitative results on randomly picked images from the test set.\n\nWe also conducted an experiment on a face manipulation that is not influencing the whole face region (see Fig. 11). Specifically, we want to highlight that the forgery localization in the case of facial manipulations is not equivalent to a face segmentation. As can be seen, our detection approach is able to detect the region of the manipulation when half of the face is untouched.\n\nConclusion As shown in the main paper, our dataset can be used for various tasks in the field of digital multimedia forensics. Specifically, we addressed the task of forgery localization, the segmentation of the altered region. Our approach enables the forgery localization in high-quality as well as in low-quality video footage.\n\nFigure 3 :\n3Statistics of our extracted sequences. VGA denotes 480p, HD denotes 720p, and FHD denotes 1080p resolution of our videos. The last graphs shows the number of sequences (y-axis) with given bounding box pixel height (x-axis).\n\nFigure 4 :\n4Automatic face editing tools rely on the ability to track the face in the target video. State-of-the-art tracking methods like Thies et al.[58] fail in cases of profile imagery of a face (left). Rotations larger than 45 \u2022 lead to tracking errors (middle). Also, occlusions lead to tracking errors (right).\n\nFigure 7 :\n7Binary detection accuracy of all evaluated architectures on the different manipulation methods using face tracking when trained on our different manipulation methods separately.\n\nFigure 8 :\n8Binary precision values of our baselines when trained on all three manipulation methods at the same time. Consult\n\nFigure 10 :\n10Precision vs. recall curve of the localization task evaluated on the three examined manipulation methods separately and on all methods. The training considered all manipulation methods at full resolution. The test set comprises both forged and pristine images.\n\nFigure 11\n11Figure 11: Our forgery localization network is able to reliably detect manipulated pixels of a face in an image. In this experiment, we manipulate only the lower part of the face using the Face2Face algorithm [58]. As can be seen, the trained network does not learn a simple face detector, but instead correctly identifies the manipulated regions.\n\nTable 3 :\n3Number of unique images of the dataset w.r.t. the \nmethod and the split in train validation and test used for our \ntests. \n\n\n. http://ifc.recod.ic.unicamp.br/fc.website/ index.py?sec=0\nFor reproducibility, we detail the hyperparameters used for the methods in the main paper. We structured this section into two parts, one for the manipulation methods and the second part for the classification approaches used for forgery detection.D.1. Manipulation MethodsOnly DeepFakes is learning-based, for the other manipulation methods we used the default parameters of the approaches. Our DeepFakes implementation is based on the deepfakes faceswap github project[1]. We keep most of the default parameters of this implementation. MTCNN ([64]) is used to extract and align the images for each video. Specifically, the largest face in the first frame of a sequence is detected and tracked throughout the whole video. This tracking information is used to extract the training data for DeepFakes. The auto-encoder takes input images of 64 (default). It uses a shared encoder consisting of four convolutional layers which downsizes the image to a bottleneck of 4 \u00d7 4, where we flatten the input, apply a fully connected layer, reshape the dense layer and apply a single upscaling using a convolutional layer as well as a pixel shuffle layer (see[54]). The two decoders use three identical up-scaling layers to attain full input image resolution. All layers use Leaky ReLus as non-linearities. The network is trained using Adam with a learning rate of 10 \u22125 , \u03b2 1 = 0.5 and \u03b2 2 = 0.999 as well as a batch size of 64. In our experiments we run the training for 200000 iterations on a cloud platform. Since the training procedure of the auto-encoder networks is very time-consuming, we also publish the corresponding models.By exchanging the decoder of one person to another, we can generate a identity-swapped face region. To insert the face into the target image we chose Poisson Image Editing[49]to achieve a seamless blending result.D.2. Classification MethodsFor our forgery detection pipeline proposed in the main paper, we conducted studies with five classification approaches based on convolutional neural networks. The networks are trained using the Adam optimizer with different parameters for learning-rate and batch-size. In particular, for the network proposed in Cozzolino at al.[17]the used learning-rate is 10 \u22125 with batch-size 16. For the proposal of Bayar and Stamm[10], we use a learning-rate equal to 10 \u22125 with a batch-size of 64. The network proposed byRahmouni [51]is trained with a learning-rate of 10 \u22124 and a batch-size equal to 64. MesoNet[5]uses a batch-size of 76. The learning-rate is initially set to 10 \u22123 and is consecutively reduced by a factor of ten for each epoch to 10 \u22126 . Our XceptionNet[14]-based approach is trained with a learningrate of 0.0002 and a batch-size of 32.The moments of Adam are set to the default values (\u03b2 1 = 0.9, \u03b2 2 = 0.999) for all experiments. The stop criterion of the training process is triggered when the validation accuracy does not improve for 10 consecutive epochs (an epoch equals to a fourth of the training set). We keep the network weights relative to the epoch with the best validation accuracy.\n. Deepfakes Github, 14Deepfakes github. https://github.com/ deepfakes/faceswap. Accessed: 2018-10-29. 1, 4, 14\n\nFaceswap. Faceswap. https://github.com/\n\n. / Marekkowalski/Faceswap, Accessed, MarekKowalski/FaceSwap/. Accessed: 2018-10-29. 1, 2\n\n2018-09-01. 4Fakeapp. Fakeapp. https://www.fakeapp.com/. Accessed: 2018-09-01. 4\n\nYouTube-8m: A large-scale video classification benchmark. S Abu-El-Haija, N Kothari, J Lee, P Natsev, G Toderici, B Varadarajan, S Vijayanarasimhan, arXiv:1609.0867512arXiv preprintS. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici, B. Varadarajan, and S. Vijayanarasimhan. YouTube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016. 12\n\nMesonet: a compact facial video forgery detection network. D Afchar, V Nozick, J Yamagishi, I Echizen, arXiv:1809.00888714arXiv preprintD. Afchar, V. Nozick, J. Yamagishi, and I. Echizen. Mesonet: a compact facial video forgery detection network. arXiv preprint arXiv:1809.00888, 2018. 3, 6, 7, 14\n\nA SIFT-based forensic method for copy-move attack detection and transformation recovery. I Amerini, L Ballan, R Caldelli, A Del Bimbo, G Serra, IEEE Transactions on Information Forensics and Security. 63I. Amerini, L. Ballan, R. Caldelli, A. Del Bimbo, and G. Serra. A SIFT-based forensic method for copy-move attack detection and transformation recovery. IEEE Trans- actions on Information Forensics and Security, 6(3):1099- 1110, Mar. 2011. 3\n\nFace aging with conditional generative adversarial networks. G Antipov, M Baccouche, J Dugelay, abs/1702.01983CoRRG. Antipov, M. Baccouche, and J. Dugelay. Face aging with conditional generative adversarial networks. CoRR, abs/1702.01983, 2017. 3\n\nBringing portraits to life. H Averbuch-Elor, D Cohen-Or, J Kopf, M F Cohen, ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia. 363H. Averbuch-Elor, D. Cohen-Or, J. Kopf, and M. F. Cohen. Bringing portraits to life. ACM Transactions on Graph- ics (Proceeding of SIGGRAPH Asia 2017), 36(4):to appear, 2017. 2, 3\n\nExploiting spatial structure for localizing manipulated image regions. J Bappy, A Roy-Chowdhury, J Bunk, L Nataraj, B Manjunath, IEEE International Conference on Computer Vision. J. Bappy, A. Roy-Chowdhury, J. Bunk, L. Nataraj, and B. Manjunath. Exploiting spatial structure for localizing ma- nipulated image regions. In IEEE International Conference on Computer Vision, pages 4970-4979, 2017. 3\n\nA deep learning approach to universal image manipulation detection using a new convolutional layer. B Bayar, M Stamm, ACM Workshop on Information Hiding and Multimedia Security. 714B. Bayar and M. Stamm. A deep learning approach to univer- sal image manipulation detection using a new convolutional layer. In ACM Workshop on Information Hiding and Multi- media Security, pages 5-10, 2016. 3, 6, 7, 14\n\nLocal tampering detection in video sequences. P Bestagini, S Milani, M Tagliasacchi, S Tubaro, IEEE International Workshop on Multimedia Signal Processing. P. Bestagini, S. Milani, M. Tagliasacchi, and S. Tubaro. Lo- cal tampering detection in video sequences. In IEEE Inter- national Workshop on Multimedia Signal Processing, pages 488-493, October 2013. 3\n\nTampering Detection and Localization through Clustering of Camera-Based CNN Features. L Bondi, S Lameri, D G\u00fcera, P Bestagini, E Delp, S Tubaro, IEEE Computer Vision and Pattern Recognition Workshops. L. Bondi, S. Lameri, D. G\u00fcera, P. Bestagini, E. Delp, and S. Tubaro. Tampering Detection and Localization through Clustering of Camera-Based CNN Features. In IEEE Com- puter Vision and Pattern Recognition Workshops, 2017. 3\n\nVideo rewrite: Driving visual speech with audio. C Bregler, M Covell, M Slaney, Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '97. the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '97New York, NY, USAACM Press/Addison-Wesley Publishing CoC. Bregler, M. Covell, and M. Slaney. Video rewrite: Driving visual speech with audio. In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Tech- niques, SIGGRAPH '97, pages 353-360, New York, NY, USA, 1997. ACM Press/Addison-Wesley Publishing Co. 2\n\nXception: Deep Learning with Depthwise Separable Convolutions. F Chollet, IEEE Conference on Computer Vision and Pattern Recognition. 714F. Chollet. Xception: Deep Learning with Depthwise Sepa- rable Convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 6, 7, 14\n\nPhysiologically-based detection of computer generated faces in video. V Conotter, E Bodnari, G Boato, H Farid, IEEE International Conference on Image Processing. V. Conotter, E. Bodnari, G. Boato, and H. Farid. Physiologically-based detection of computer generated faces in video. In IEEE International Conference on Image Pro- cessing, pages 1-5, Oct 2014. 3\n\nImage forgery detection through residual-based local descriptors and block-matching. D Cozzolino, D Gragnaniello, L Verdoliva, IEEE International Conference on Image Processing. D. Cozzolino, D. Gragnaniello, and L.Verdoliva. Image forgery detection through residual-based local descriptors and block-matching. In IEEE International Conference on Image Processing, pages 5297-5301, October 2014. 6\n\nRecasting residual-based local descriptors as convolutional neural networks: an application to image forgery detection. D Cozzolino, G Poggi, L Verdoliva, ACM Workshop on Information Hiding and Multimedia Security. 714D. Cozzolino, G. Poggi, and L. Verdoliva. Recasting residual-based local descriptors as convolutional neural net- works: an application to image forgery detection. In ACM Workshop on Information Hiding and Multimedia Security, pages 1-6, 2017. 3, 6, 7, 14\n\nForensicTransfer: Weakly-supervised domain adaptation for forgery detection. D Cozzolino, J Thies, A R\u00f6ssler, C Riess, M Nie\u00dfner, L Verdoliva, arXivD. Cozzolino, J. Thies, A. R\u00f6ssler, C. Riess, M. Nie\u00dfner, and L. Verdoliva. ForensicTransfer: Weakly-supervised domain adaptation for forgery detection. arXiv, 2018. 8\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEEA. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner. Scannet: Richly-annotated 3d reconstruc- tions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 8\n\nVideo face replacement. K Dale, K Sunkavalli, M K Johnson, D Vlasic, W Matusik, H Pfister, 130:1-130:10ACM Trans. Graph. 306K. Dale, K. Sunkavalli, M. K. Johnson, D. Vlasic, W. Ma- tusik, and H. Pfister. Video face replacement. ACM Trans. Graph., 30(6):130:1-130:10, Dec. 2011. 2\n\nA PatchMatch-based Dense-field Algorithm for Video Copy-Move Detection and Localization. L D&apos;amiano, D Cozzolino, G Poggi, L Verdoliva, IEEE Transactions on Circuits and Systems for Video Technology. in pressL. D'Amiano, D. Cozzolino, G. Poggi, and L. Verdoliva. A PatchMatch-based Dense-field Algorithm for Video Copy- Move Detection and Localization. IEEE Transactions on Circuits and Systems for Video Technology, in press, 2018. 3\n\nIdentify computer generated characters by analysing facial expressions variation. D.-T Dang-Nguyen, G Boato, F. De Natale, IEEE International Workshop on Information Forensics and Security. D.-T. Dang-Nguyen, G. Boato, and F. De Natale. Identify computer generated characters by analysing facial expres- sions variation. In IEEE International Workshop on Infor- mation Forensics and Security, pages 252-257, 2012. 3\n\nIlluminant-Based Transformed Spaces for Image Forensics. T Carvalho, F Faria, H Pedrini, R Torres, A Rocha, IEEE Transactions on Information Forensics and Security. 114T. de Carvalho, F. Faria, H. Pedrini, R. Torres, and A. Rocha. Illuminant-Based Transformed Spaces for Image Forensics. IEEE Transactions on Information Forensics and Security, 11(4):720-733, 2016. 3\n\nExposing digital image forgeries by illumination color classification. T Carvalho, C Riessa, E Angelopoulou, H Pedrini, A Rocha, IEEE Transactions on Information Forensics and Security. 87T. de Carvalho, C. Riessa, E. Angelopoulou, H. Pedrini, and A. Rocha. Exposing digital image forgeries by illumina- tion color classification. IEEE Transactions on Information Forensics and Security, 8(7):1182-1194, 2013. 3\n\nIdentification of Motion-Compensated Frame Rate Up-Conversion Based on Residual Signal. X Ding, Y Gaobo, R Li, L Zhang, Y Li, X Sun, IEEE Transactions on Circuits and Systems for Video Technology. in pressX. Ding, Y. Gaobo, R. Li, L. Zhang, Y. Li, and X. Sun. Identification of Motion-Compensated Frame Rate Up- Conversion Based on Residual Signal. IEEE Transactions on Circuits and Systems for Video Technology, in press, 2017. 3\n\nPhoto Forensics. H Farid, The MIT PressH. Farid. Photo Forensics. The MIT Press, 2016. 3\n\nRich Models for Steganalysis of Digital Images. J Fridrich, J Kodovsk\u00fd, IEEE Transactions on Information Forensics and Security. 737J. Fridrich and J. Kodovsk\u00fd. Rich Models for Steganalysis of Digital Images. IEEE Transactions on Information Forensics and Security, 7(3):868-882, June 2012. 6, 7\n\nRole of facial expressions in social interactions. C Frith, Philosophical Transactions of the Royal Society B: Biological Sciences. 364C. Frith. Role of facial expressions in social interactions. Philosophical Transactions of the Royal Society B: Biologi- cal Sciences, 364(1535), Dec. 2009. 1\n\nAutomatic face reenactment. P Garrido, L Valgaerts, O Rehmsen, T Thormaehlen, P Perez, C Theobalt, IEEE Conference on Computer Vision and Pattern Recognition. P. Garrido, L. Valgaerts, O. Rehmsen, T. Thormaehlen, P. Perez, and C. Theobalt. Automatic face reenactment. In IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4217-4224, 2014. 2\n\nVdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. P Garrido, L Valgaerts, H Sarmadi, I Steiner, K Varanasi, P Perez, C Theobalt, Computer Graphics Forum. 342P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. Varanasi, P. Perez, and C. Theobalt. Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. Computer Graphics Forum, 34(2):193-204, 2015. 2\n\nA video forensic technique for detection frame deletion and insertion. A Gironi, M Fontani, T Bianchi, A Piva, M Barni, IEEE International Conference on Acoustics, Speech and Signal Processing. A. Gironi, M. Fontani, T. Bianchi, A. Piva, and M. Barni. A video forensic technique for detection frame deletion and insertion. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6226-6230, 2014. 3\n\nMfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. H Guan, M Kozak, E Robertson, Y Lee, A N Yates, A Delgado, D Zhou, T Kheyrkhah, J Smith, J Fiscus, IEEE Winter Applications of Computer Vision Workshops. H. Guan, M. Kozak, E. Robertson, Y. Lee, A. N. Yates, A. Delgado, D. Zhou, T. Kheyrkhah, J. Smith, and J. Fis- cus. Mfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. In IEEE Winter Applications of Computer Vision Workshops, pages 63-72, Jan 2019. 3\n\nFake face detection methods: Can they be generalized. D G\u00fcera, E Delp, IEEE International Conference on Advanced Video and Signal Based Surveillance. D. G\u00fcera and E. Delp. Fake face detection methods: Can they be generalized? In IEEE International Conference on Advanced Video and Signal Based Surveillance, 2018. 3\n\nBeyond face rotation: Global and local perception GAN for photorealistic and identity preserving frontal view synthesis. R Huang, S Zhang, T Li, R He, abs/1704.04086CoRRR. Huang, S. Zhang, T. Li, and R. He. Beyond face ro- tation: Global and local perception GAN for photorealis- tic and identity preserving frontal view synthesis. CoRR, abs/1704.04086, 2017. 3\n\nFighting fake news: Image splice detection via learned self-consistency. M Huh, A Liu, A Owens, A A Efros, In ECCV. 3M. Huh, A. Liu, A. Owens, and A. A. Efros. Fighting fake news: Image splice detection via learned self-consistency. In ECCV, 2018. 3\n\nProgressive Growing of GANs for Improved Quality, Stability, and Variation. T Karras, T Aila, S Laine, J Lehtinen, abs/1710.10196CoRRT. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive Growing of GANs for Improved Quality, Stability, and Vari- ation. CoRR, abs/1710.10196, 2017. 3\n\nFake face detection methods: Can they be generalized?. A Khodabakhsh, R Ramachandra, K Raja, P Wasnik, C Busch, International Conference of the. A. Khodabakhsh, R. Ramachandra, K. Raja, P. Wasnik, and C. Busch. Fake face detection methods: Can they be general- ized? In International Conference of the Biometrics Special Interest Group, 2018. 3\n\nDeep Video Portraits. H Kim, P Garrido, A Tewari, W Xu, J Thies, N Nie\u00dfner, P P\u00e9rez, C Richardt, M Zollh\u00f6fer, C Theobalt, ACM Transactions on Graphics. 3TOG)H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, N. Nie\u00dfner, P. P\u00e9rez, C. Richardt, M. Zollh\u00f6fer, and C. Theobalt. Deep Video Portraits. ACM Transactions on Graphics 2018 (TOG), 2018. 3\n\nDlib-ml: A machine learning toolkit. D E King, Journal of Machine Learning Research. 1012D. E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research, 10:1755-1758, 2009. 12\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 13\n\nDeepfakes: a new threat to face recognition? assessment and detection. P Korshunov, S Marcel, arXiv:1812.08685arXiv preprintP. Korshunov and S. Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018. 3\n\nMulti-scale Analysis Strategies in PRNU-based Tampering Localization. P Korus, J Huang, IEEE Transactions on Information Forensics and Security. 124P. Korus and J. Huang. Multi-scale Analysis Strategies in PRNU-based Tampering Localization. IEEE Transactions on Information Forensics and Security, 12(4):809-824, Apr. 2017. 3\n\nFader networks: Manipulating images by sliding attributes. G Lample, N Zeghidour, N Usunier, A Bordes, L Denoyer, M Ranzato, abs/1706.00409CoRRG. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. De- noyer, and M. Ranzato. Fader networks: Manipulating im- ages by sliding attributes. CoRR, abs/1706.00409, 2017. 3\n\nIn ictu oculi: Exposing ai created fake videos by detecting eye blinking. Y Li, M Chang, S Lyu, IEEE WIFS. 3Y. Li, M. Chang, and S. Lyu. In ictu oculi: Exposing ai created fake videos by detecting eye blinking. In IEEE WIFS, 2018. 3\n\nA C3D-based Convolutional Neural Network for Frame Dropping Detection in a Single Video Shot. C Long, E Smith, A Basharat, A Hoogs, IEEE Computer Vision and Pattern Recognition Workshops. C. Long, E. Smith, A. Basharat, and A. Hoogs. A C3D-based Convolutional Neural Network for Frame Dropping Detec- tion in a Single Video Shot. In IEEE Computer Vision and Pattern Recognition Workshops, pages 1898-1906, 2017. 3\n\nConditional cyclegan for attribute guided face image generation. CoRR, abs/1705.09966. Y Lu, Y Tai, C Tang, Y. Lu, Y. Tai, and C. Tang. Conditional cyclegan for attribute guided face image generation. CoRR, abs/1705.09966, 2017. 3\n\nRecent progress of face image synthesis. Z Lu, Z Li, J Cao, R He, Z Sun, abs/1706.04717CoRRZ. Lu, Z. Li, J. Cao, R. He, and Z. Sun. Recent progress of face image synthesis. CoRR, abs/1706.04717, 2017. 3\n\nResidual-based forensic comparison of video sequences. P Mullan, D Cozzolino, L Verdoliva, C Riess, IEEE International Conference on Image Processing. P. Mullan, D. Cozzolino, L. Verdoliva, and C. Riess. Residual-based forensic comparison of video sequences. In IEEE International Conference on Image Processing, 2017. 3\n\nPoisson image editing. P P\u00e9rez, M Gangnet, A Blake, ACM Transactions on graphics (TOG). 22314P. P\u00e9rez, M. Gangnet, and A. Blake. Poisson image edit- ing. ACM Transactions on graphics (TOG), 22(3):313-318, 2003. 4, 14\n\nTransferable Deep-CNN features for detecting digital and printscanned morphed face images. R Raghavendra, K Raja, S Venkatesh, C Busch, IEEE Computer Vision and Pattern Recognition Workshops. R. Raghavendra, K. Raja, S. Venkatesh, and C. Busch. Trans- ferable Deep-CNN features for detecting digital and print- scanned morphed face images. In IEEE Computer Vision and Pattern Recognition Workshops, 2017. 3\n\nDistinguishing computer graphics from natural images using convolution neural networks. N Rahmouni, V Nozick, J Yamagishi, I Echizen, IEEE Workshop on Information Forensics and Security. 714N. Rahmouni, V. Nozick, J. Yamagishi, and I. Echizen. Dis- tinguishing computer graphics from natural images using convolution neural networks. In IEEE Workshop on Infor- mation Forensics and Security, pages 1-6, 2017. 3, 6, 7, 14\n\nA R\u00f6ssler, D Cozzolino, L Verdoliva, C Riess, J Thies, M Nie\u00dfner, FaceForensics: A large-scale video dataset for forgery detection in human faces. arXiv. A. R\u00f6ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nie\u00dfner. FaceForensics: A large-scale video dataset for forgery detection in human faces. arXiv, 2018. 4\n\nDigital Image Forensics -There is More to a Picture than Meets the Eye. H T Sencar, N Memon, SpringerH. T. Sencar and N. Memon. Digital Image Forensics - There is More to a Picture than Meets the Eye. Springer, 2013. 3\n\nReal-time single image and video super-resolution using an efficient subpixel convolutional neural network. W Shi, J Caballero, F Husz\u00e1r, J Totz, A P Aitken, R Bishop, D Rueckert, Z Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition14W. Shi, J. Caballero, F. Husz\u00e1r, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub- pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 1874-1883, 2016. 14\n\nSynthesizing Obama: learning lip sync from audio. S Suwajanakorn, S M Seitz, I Kemelmacher-Shlizerman, ACM Transactions on Graphics (TOG). 3643S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher- Shlizerman. Synthesizing Obama: learning lip sync from audio. ACM Transactions on Graphics (TOG), 36(4), 2017. 1, 3\n\nInception-v4, inception-resnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, A A , C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. 2017. 6\n\nReal-time expression transfer for facial reenactment. J Thies, M Zollh\u00f6fer, M Nie\u00dfner, L Valgaerts, M Stamminger, C Theobalt, Proceedings of ACM SIGGRAPH Asia. ACM SIGGRAPH Asia34J. Thies, M. Zollh\u00f6fer, M. Nie\u00dfner, L. Valgaerts, M. Stam- minger, and C. Theobalt. Real-time expression transfer for facial reenactment. ACM Transactions on Graphics (TOG) - Proceedings of ACM SIGGRAPH Asia 2015, 34(6):Art. No. 183, 2015. 2\n\nFace2Face: Real-Time Face Capture and Reenactment of RGB Videos. J Thies, M Zollh\u00f6fer, M Stamminger, C Theobalt, M Nie\u00dfner, IEEE Conference on Computer Vision and Pattern Recognition. 613J. Thies, M. Zollh\u00f6fer, M. Stamminger, C. Theobalt, and M. Nie\u00dfner. Face2Face: Real-Time Face Capture and Reen- actment of RGB Videos. In IEEE Conference on Com- puter Vision and Pattern Recognition, pages 2387-2395, June 2016. 1, 3, 4, 5, 6, 13\n\nFacevr: Real-time gaze-aware facial reenactment in virtual reality. J Thies, M Zollh\u00f6fer, M Stamminger, C Theobalt, M Nie\u00dfner, ACM Transactions on Graphics. 3TOG)J. Thies, M. Zollh\u00f6fer, M. Stamminger, C. Theobalt, and M. Nie\u00dfner. Facevr: Real-time gaze-aware facial reenact- ment in virtual reality. ACM Transactions on Graphics 2018 (TOG), 2018. 3\n\nJ Thies, M Zollh\u00f6fer, C Theobalt, M Stamminger, M Nie\u00dfner, Headon, arXiv:1805.11729Real-time reenactment of human portrait videos. arXiv preprintJ. Thies, M. Zollh\u00f6fer, C. Theobalt, M. Stamminger, and M. Nie\u00dfner. Headon: Real-time reenactment of human por- trait videos. arXiv preprint arXiv:1805.11729, 2018. 3\n\nDeep feature interpolation for image content changes. P Upchurch, J R Gardner, K Bala, R Pless, N Snavely, K Q Weinberger, abs/1611.05507CoRRP. Upchurch, J. R. Gardner, K. Bala, R. Pless, N. Snavely, and K. Q. Weinberger. Deep feature interpolation for image content changes. CoRR, abs/1611.05507, 2016. 3\n\nExposing Digital Forgeries in Interlaced and Deinterlaced Video. W Wang, H Farid, IEEE Transactions on Information Forensics and Security. 23W. Wang and H. Farid. Exposing Digital Forgeries in In- terlaced and Deinterlaced Video. IEEE Transactions on In- formation Forensics and Security, 2(3):438-449, Sept. 2007. 3\n\nDetecting image splicing in the wild (Web). M Zampoglou, S Papadopoulos, Y Kompatsiaris, IEEE International Conference on Multimedia & Expo Workshops (ICMEW). M. Zampoglou, S. Papadopoulos, , and Y. Kompatsiaris. Detecting image splicing in the wild (Web). In IEEE In- ternational Conference on Multimedia & Expo Workshops (ICMEW), 2015. 3\n\nJoint face detection and alignment using multitask cascaded convolutional networks. K Zhang, Z Zhang, Z Li, Y Qiao, IEEE Signal Processing Letters. 2310K. Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection and alignment using multitask cascaded convolutional net- works. IEEE Signal Processing Letters, 23(10):1499-1503, Oct 2016. 14\n\nTwo-stream neural networks for tampered face detection. P Zhou, X Han, V Morariu, L Davis, IEEE Computer Vision and Pattern Recognition Workshops. P. Zhou, X. Han, V. Morariu, and L. Davis. Two-stream neu- ral networks for tampered face detection. In IEEE Computer Vision and Pattern Recognition Workshops, pages 1831- 1839, 2017. 3\n\nLearning rich features for image manipulation detection. P Zhou, X Han, V Morariu, L Davis, CVPR. P. Zhou, X. Han, V. Morariu, and L. Davis. Learning rich features for image manipulation detection. In CVPR, 2018. 3\n\nState of the art on monocular 3d face reconstruction, tracking, and applications. M Zollh\u00f6fer, J Thies, D Bradley, P Garrido, T Beeler, P P\u00e9erez, M Stamminger, M Nie\u00dfner, C Theobalt, 1M. Zollh\u00f6fer, J. Thies, D. Bradley, P. Garrido, T. Beeler, P. P\u00e9erez, M. Stamminger, M. Nie\u00dfner, and C. Theobalt. State of the art on monocular 3d face reconstruction, track- ing, and applications. 2018. 1, 2\n", "annotations": {"author": "[{\"end\":114,\"start\":65},{\"end\":167,\"start\":115},{\"end\":219,\"start\":168},{\"end\":271,\"start\":220},{\"end\":318,\"start\":272},{\"end\":369,\"start\":319}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":73},{\"end\":131,\"start\":122},{\"end\":183,\"start\":174},{\"end\":235,\"start\":230},{\"end\":284,\"start\":279},{\"end\":335,\"start\":328}]", "author_first_name": "[{\"end\":72,\"start\":65},{\"end\":121,\"start\":115},{\"end\":173,\"start\":168},{\"end\":229,\"start\":220},{\"end\":278,\"start\":272},{\"end\":327,\"start\":319}]", "author_affiliation": "[{\"end\":113,\"start\":82},{\"end\":166,\"start\":133},{\"end\":218,\"start\":185},{\"end\":270,\"start\":237},{\"end\":317,\"start\":286},{\"end\":368,\"start\":337}]", "title": "[{\"end\":62,\"start\":1},{\"end\":431,\"start\":370}]", "venue": null, "abstract": "[{\"end\":2021,\"start\":433}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2166,\"start\":2163},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2516,\"start\":2512},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2732,\"start\":2728},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2989,\"start\":2985},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3177,\"start\":3173},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3952,\"start\":3949},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5072,\"start\":5069},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":6485,\"start\":6481},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6506,\"start\":6502},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6673,\"start\":6669},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6918,\"start\":6914},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7033,\"start\":7029},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7189,\"start\":7185},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7585,\"start\":7581},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7947,\"start\":7943},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7984,\"start\":7980},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8001,\"start\":7997},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8133,\"start\":8129},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8264,\"start\":8260},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8290,\"start\":8287},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8626,\"start\":8622},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8719,\"start\":8716},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8752,\"start\":8748},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8802,\"start\":8798},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8835,\"start\":8831},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9001,\"start\":8997},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9117,\"start\":9113},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9614,\"start\":9610},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9617,\"start\":9614},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9742,\"start\":9738},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9745,\"start\":9742},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9748,\"start\":9745},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9750,\"start\":9748},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9753,\"start\":9750},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9756,\"start\":9753},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9916,\"start\":9912},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9919,\"start\":9916},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9922,\"start\":9919},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9956,\"start\":9952},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9986,\"start\":9982},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9989,\"start\":9986},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10022,\"start\":10018},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10172,\"start\":10168},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10175,\"start\":10172},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10178,\"start\":10175},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10198,\"start\":10194},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10218,\"start\":10214},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10221,\"start\":10218},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10241,\"start\":10237},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10244,\"start\":10241},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10262,\"start\":10259},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10265,\"start\":10262},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10268,\"start\":10265},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10404,\"start\":10400},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10443,\"start\":10439},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10446,\"start\":10443},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10605,\"start\":10601},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10608,\"start\":10605},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10611,\"start\":10608},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10613,\"start\":10611},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10616,\"start\":10613},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11925,\"start\":11922},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":12044,\"start\":12040},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12141,\"start\":12137},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":12264,\"start\":12260},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12301,\"start\":12297},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12661,\"start\":12657},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13084,\"start\":13080},{\"end\":13301,\"start\":13291},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16350,\"start\":16347},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16378,\"start\":16375},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16810,\"start\":16806},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17298,\"start\":17294},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18623,\"start\":18619},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":22342,\"start\":22338},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22948,\"start\":22944},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22951,\"start\":22948},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23003,\"start\":22999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23036,\"start\":23033},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23108,\"start\":23104},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23249,\"start\":23245},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23638,\"start\":23634},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24110,\"start\":24106},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24357,\"start\":24353},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24617,\"start\":24613},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24837,\"start\":24834},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24890,\"start\":24886},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25292,\"start\":25288},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27035,\"start\":27031},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27038,\"start\":27035},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30052,\"start\":30048},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32336,\"start\":32332},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34264,\"start\":34261},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34965,\"start\":34961},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":38643,\"start\":38639},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39332,\"start\":39328},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":41235,\"start\":41231}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41078,\"start\":40842},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41397,\"start\":41079},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41588,\"start\":41398},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41715,\"start\":41589},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41991,\"start\":41716},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42352,\"start\":41992},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42489,\"start\":42353}]", "paragraph": "[{\"end\":2733,\"start\":2037},{\"end\":4004,\"start\":2735},{\"end\":4538,\"start\":4006},{\"end\":5087,\"start\":4540},{\"end\":5515,\"start\":5089},{\"end\":5575,\"start\":5517},{\"end\":5719,\"start\":5577},{\"end\":5917,\"start\":5721},{\"end\":6029,\"start\":5919},{\"end\":6110,\"start\":6031},{\"end\":6287,\"start\":6127},{\"end\":8509,\"start\":6289},{\"end\":9235,\"start\":8511},{\"end\":10023,\"start\":9237},{\"end\":11437,\"start\":10025},{\"end\":12662,\"start\":11439},{\"end\":12927,\"start\":12664},{\"end\":13144,\"start\":12967},{\"end\":14066,\"start\":13146},{\"end\":14272,\"start\":14068},{\"end\":15354,\"start\":14274},{\"end\":16061,\"start\":15356},{\"end\":16811,\"start\":16063},{\"end\":17272,\"start\":16813},{\"end\":18624,\"start\":17274},{\"end\":19191,\"start\":18626},{\"end\":19602,\"start\":19213},{\"end\":19890,\"start\":19643},{\"end\":20696,\"start\":19892},{\"end\":21354,\"start\":20698},{\"end\":21988,\"start\":21356},{\"end\":23159,\"start\":22028},{\"end\":23944,\"start\":23205},{\"end\":24083,\"start\":23985},{\"end\":24258,\"start\":24085},{\"end\":24591,\"start\":24260},{\"end\":24812,\"start\":24593},{\"end\":25270,\"start\":24814},{\"end\":26072,\"start\":25272},{\"end\":26619,\"start\":26074},{\"end\":27333,\"start\":26621},{\"end\":28265,\"start\":27335},{\"end\":28847,\"start\":28267},{\"end\":30054,\"start\":28861},{\"end\":30607,\"start\":30056},{\"end\":31039,\"start\":30609},{\"end\":31627,\"start\":31067},{\"end\":32528,\"start\":31629},{\"end\":33460,\"start\":32548},{\"end\":33497,\"start\":33480},{\"end\":34074,\"start\":33530},{\"end\":34839,\"start\":34076},{\"end\":35325,\"start\":34841},{\"end\":36485,\"start\":35327},{\"end\":36984,\"start\":36530},{\"end\":37944,\"start\":37012},{\"end\":38896,\"start\":37946},{\"end\":39308,\"start\":38898},{\"end\":39684,\"start\":39310},{\"end\":40125,\"start\":39686},{\"end\":40509,\"start\":40127},{\"end\":40841,\"start\":40511}]", "formula": null, "table_ref": "[{\"end\":23941,\"start\":23934},{\"end\":25388,\"start\":25381},{\"end\":27541,\"start\":27534},{\"end\":30230,\"start\":30223},{\"end\":30347,\"start\":30340},{\"end\":30723,\"start\":30716}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2035,\"start\":2023},{\"attributes\":{\"n\":\"2.\"},\"end\":6125,\"start\":6113},{\"attributes\":{\"n\":\"3.\"},\"end\":12965,\"start\":12930},{\"attributes\":{\"n\":\"4.\"},\"end\":19211,\"start\":19194},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19641,\"start\":19605},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22026,\"start\":21991},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":23203,\"start\":23162},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":23983,\"start\":23947},{\"attributes\":{\"n\":\"5.\"},\"end\":28859,\"start\":28850},{\"attributes\":{\"n\":\"6.\"},\"end\":31065,\"start\":31042},{\"attributes\":{\"n\":\"7.\"},\"end\":32546,\"start\":32531},{\"end\":33470,\"start\":33463},{\"end\":33478,\"start\":33473},{\"end\":33528,\"start\":33500},{\"end\":36528,\"start\":36488},{\"end\":37010,\"start\":36987},{\"end\":40853,\"start\":40843},{\"end\":41090,\"start\":41080},{\"end\":41409,\"start\":41399},{\"end\":41600,\"start\":41590},{\"end\":41728,\"start\":41717},{\"end\":42002,\"start\":41993},{\"end\":42363,\"start\":42354}]", "table": "[{\"end\":42489,\"start\":42365}]", "figure_caption": "[{\"end\":41078,\"start\":40855},{\"end\":41397,\"start\":41092},{\"end\":41588,\"start\":41411},{\"end\":41715,\"start\":41602},{\"end\":41991,\"start\":41731},{\"end\":42352,\"start\":42005}]", "figure_ref": "[{\"end\":2884,\"start\":2878},{\"end\":3202,\"start\":3194},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13465,\"start\":13459},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13779,\"start\":13773},{\"end\":20719,\"start\":20713},{\"end\":21107,\"start\":21099},{\"end\":21703,\"start\":21695},{\"end\":22114,\"start\":22108},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23929,\"start\":23923},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26673,\"start\":26667},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27529,\"start\":27523},{\"end\":27669,\"start\":27662},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27830,\"start\":27824},{\"end\":28313,\"start\":28307},{\"end\":28681,\"start\":28673},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33881,\"start\":33875},{\"end\":38788,\"start\":38779},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39737,\"start\":39730},{\"end\":40051,\"start\":40044},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40240,\"start\":40233}]", "bib_author_first_name": "[{\"end\":45633,\"start\":45624},{\"end\":45778,\"start\":45777},{\"end\":46007,\"start\":46006},{\"end\":46023,\"start\":46022},{\"end\":46034,\"start\":46033},{\"end\":46041,\"start\":46040},{\"end\":46051,\"start\":46050},{\"end\":46063,\"start\":46062},{\"end\":46078,\"start\":46077},{\"end\":46392,\"start\":46391},{\"end\":46402,\"start\":46401},{\"end\":46412,\"start\":46411},{\"end\":46425,\"start\":46424},{\"end\":46721,\"start\":46720},{\"end\":46732,\"start\":46731},{\"end\":46742,\"start\":46741},{\"end\":46754,\"start\":46753},{\"end\":46758,\"start\":46755},{\"end\":46767,\"start\":46766},{\"end\":47139,\"start\":47138},{\"end\":47150,\"start\":47149},{\"end\":47163,\"start\":47162},{\"end\":47354,\"start\":47353},{\"end\":47371,\"start\":47370},{\"end\":47383,\"start\":47382},{\"end\":47391,\"start\":47390},{\"end\":47393,\"start\":47392},{\"end\":47716,\"start\":47715},{\"end\":47725,\"start\":47724},{\"end\":47742,\"start\":47741},{\"end\":47750,\"start\":47749},{\"end\":47761,\"start\":47760},{\"end\":48143,\"start\":48142},{\"end\":48152,\"start\":48151},{\"end\":48491,\"start\":48490},{\"end\":48504,\"start\":48503},{\"end\":48514,\"start\":48513},{\"end\":48530,\"start\":48529},{\"end\":48890,\"start\":48889},{\"end\":48899,\"start\":48898},{\"end\":48909,\"start\":48908},{\"end\":48918,\"start\":48917},{\"end\":48931,\"start\":48930},{\"end\":48939,\"start\":48938},{\"end\":49279,\"start\":49278},{\"end\":49290,\"start\":49289},{\"end\":49300,\"start\":49299},{\"end\":49901,\"start\":49900},{\"end\":50201,\"start\":50200},{\"end\":50213,\"start\":50212},{\"end\":50224,\"start\":50223},{\"end\":50233,\"start\":50232},{\"end\":50577,\"start\":50576},{\"end\":50590,\"start\":50589},{\"end\":50606,\"start\":50605},{\"end\":51011,\"start\":51010},{\"end\":51024,\"start\":51023},{\"end\":51033,\"start\":51032},{\"end\":51443,\"start\":51442},{\"end\":51456,\"start\":51455},{\"end\":51465,\"start\":51464},{\"end\":51476,\"start\":51475},{\"end\":51485,\"start\":51484},{\"end\":51496,\"start\":51495},{\"end\":51746,\"start\":51745},{\"end\":51753,\"start\":51752},{\"end\":51755,\"start\":51754},{\"end\":51764,\"start\":51763},{\"end\":51773,\"start\":51772},{\"end\":51783,\"start\":51782},{\"end\":51797,\"start\":51796},{\"end\":52146,\"start\":52145},{\"end\":52154,\"start\":52153},{\"end\":52168,\"start\":52167},{\"end\":52170,\"start\":52169},{\"end\":52181,\"start\":52180},{\"end\":52191,\"start\":52190},{\"end\":52202,\"start\":52201},{\"end\":52492,\"start\":52491},{\"end\":52509,\"start\":52508},{\"end\":52522,\"start\":52521},{\"end\":52531,\"start\":52530},{\"end\":52929,\"start\":52925},{\"end\":52944,\"start\":52943},{\"end\":52957,\"start\":52952},{\"end\":53318,\"start\":53317},{\"end\":53330,\"start\":53329},{\"end\":53339,\"start\":53338},{\"end\":53350,\"start\":53349},{\"end\":53360,\"start\":53359},{\"end\":53701,\"start\":53700},{\"end\":53713,\"start\":53712},{\"end\":53723,\"start\":53722},{\"end\":53739,\"start\":53738},{\"end\":53750,\"start\":53749},{\"end\":54131,\"start\":54130},{\"end\":54139,\"start\":54138},{\"end\":54148,\"start\":54147},{\"end\":54154,\"start\":54153},{\"end\":54163,\"start\":54162},{\"end\":54169,\"start\":54168},{\"end\":54492,\"start\":54491},{\"end\":54613,\"start\":54612},{\"end\":54625,\"start\":54624},{\"end\":54913,\"start\":54912},{\"end\":55185,\"start\":55184},{\"end\":55196,\"start\":55195},{\"end\":55209,\"start\":55208},{\"end\":55220,\"start\":55219},{\"end\":55235,\"start\":55234},{\"end\":55244,\"start\":55243},{\"end\":55609,\"start\":55608},{\"end\":55620,\"start\":55619},{\"end\":55633,\"start\":55632},{\"end\":55644,\"start\":55643},{\"end\":55655,\"start\":55654},{\"end\":55667,\"start\":55666},{\"end\":55676,\"start\":55675},{\"end\":56019,\"start\":56018},{\"end\":56029,\"start\":56028},{\"end\":56040,\"start\":56039},{\"end\":56051,\"start\":56050},{\"end\":56059,\"start\":56058},{\"end\":56460,\"start\":56459},{\"end\":56468,\"start\":56467},{\"end\":56477,\"start\":56476},{\"end\":56490,\"start\":56489},{\"end\":56497,\"start\":56496},{\"end\":56499,\"start\":56498},{\"end\":56508,\"start\":56507},{\"end\":56519,\"start\":56518},{\"end\":56527,\"start\":56526},{\"end\":56540,\"start\":56539},{\"end\":56549,\"start\":56548},{\"end\":56954,\"start\":56953},{\"end\":56963,\"start\":56962},{\"end\":57338,\"start\":57337},{\"end\":57347,\"start\":57346},{\"end\":57356,\"start\":57355},{\"end\":57362,\"start\":57361},{\"end\":57653,\"start\":57652},{\"end\":57660,\"start\":57659},{\"end\":57667,\"start\":57666},{\"end\":57676,\"start\":57675},{\"end\":57678,\"start\":57677},{\"end\":57907,\"start\":57906},{\"end\":57917,\"start\":57916},{\"end\":57925,\"start\":57924},{\"end\":57934,\"start\":57933},{\"end\":58175,\"start\":58174},{\"end\":58190,\"start\":58189},{\"end\":58205,\"start\":58204},{\"end\":58213,\"start\":58212},{\"end\":58223,\"start\":58222},{\"end\":58488,\"start\":58487},{\"end\":58495,\"start\":58494},{\"end\":58506,\"start\":58505},{\"end\":58516,\"start\":58515},{\"end\":58522,\"start\":58521},{\"end\":58531,\"start\":58530},{\"end\":58542,\"start\":58541},{\"end\":58551,\"start\":58550},{\"end\":58563,\"start\":58562},{\"end\":58576,\"start\":58575},{\"end\":58846,\"start\":58845},{\"end\":58848,\"start\":58847},{\"end\":59053,\"start\":59052},{\"end\":59055,\"start\":59054},{\"end\":59065,\"start\":59064},{\"end\":59281,\"start\":59280},{\"end\":59294,\"start\":59293},{\"end\":59545,\"start\":59544},{\"end\":59554,\"start\":59553},{\"end\":59861,\"start\":59860},{\"end\":59871,\"start\":59870},{\"end\":59884,\"start\":59883},{\"end\":59895,\"start\":59894},{\"end\":59905,\"start\":59904},{\"end\":59916,\"start\":59915},{\"end\":60189,\"start\":60188},{\"end\":60195,\"start\":60194},{\"end\":60204,\"start\":60203},{\"end\":60443,\"start\":60442},{\"end\":60451,\"start\":60450},{\"end\":60460,\"start\":60459},{\"end\":60472,\"start\":60471},{\"end\":60851,\"start\":60850},{\"end\":60857,\"start\":60856},{\"end\":60864,\"start\":60863},{\"end\":61037,\"start\":61036},{\"end\":61043,\"start\":61042},{\"end\":61049,\"start\":61048},{\"end\":61056,\"start\":61055},{\"end\":61062,\"start\":61061},{\"end\":61255,\"start\":61254},{\"end\":61265,\"start\":61264},{\"end\":61278,\"start\":61277},{\"end\":61291,\"start\":61290},{\"end\":61545,\"start\":61544},{\"end\":61554,\"start\":61553},{\"end\":61565,\"start\":61564},{\"end\":61831,\"start\":61830},{\"end\":61846,\"start\":61845},{\"end\":61854,\"start\":61853},{\"end\":61867,\"start\":61866},{\"end\":62236,\"start\":62235},{\"end\":62248,\"start\":62247},{\"end\":62258,\"start\":62257},{\"end\":62271,\"start\":62270},{\"end\":62570,\"start\":62569},{\"end\":62581,\"start\":62580},{\"end\":62594,\"start\":62593},{\"end\":62607,\"start\":62606},{\"end\":62616,\"start\":62615},{\"end\":62625,\"start\":62624},{\"end\":62969,\"start\":62968},{\"end\":62971,\"start\":62970},{\"end\":62981,\"start\":62980},{\"end\":63225,\"start\":63224},{\"end\":63232,\"start\":63231},{\"end\":63245,\"start\":63244},{\"end\":63255,\"start\":63254},{\"end\":63263,\"start\":63262},{\"end\":63265,\"start\":63264},{\"end\":63275,\"start\":63274},{\"end\":63285,\"start\":63284},{\"end\":63297,\"start\":63296},{\"end\":63812,\"start\":63811},{\"end\":63828,\"start\":63827},{\"end\":63830,\"start\":63829},{\"end\":63839,\"start\":63838},{\"end\":64155,\"start\":64154},{\"end\":64166,\"start\":64165},{\"end\":64175,\"start\":64174},{\"end\":64188,\"start\":64187},{\"end\":64190,\"start\":64189},{\"end\":64393,\"start\":64392},{\"end\":64402,\"start\":64401},{\"end\":64415,\"start\":64414},{\"end\":64426,\"start\":64425},{\"end\":64439,\"start\":64438},{\"end\":64453,\"start\":64452},{\"end\":64826,\"start\":64825},{\"end\":64835,\"start\":64834},{\"end\":64848,\"start\":64847},{\"end\":64862,\"start\":64861},{\"end\":64874,\"start\":64873},{\"end\":65263,\"start\":65262},{\"end\":65272,\"start\":65271},{\"end\":65285,\"start\":65284},{\"end\":65299,\"start\":65298},{\"end\":65311,\"start\":65310},{\"end\":65545,\"start\":65544},{\"end\":65554,\"start\":65553},{\"end\":65567,\"start\":65566},{\"end\":65579,\"start\":65578},{\"end\":65593,\"start\":65592},{\"end\":65912,\"start\":65911},{\"end\":65924,\"start\":65923},{\"end\":65926,\"start\":65925},{\"end\":65937,\"start\":65936},{\"end\":65945,\"start\":65944},{\"end\":65954,\"start\":65953},{\"end\":65965,\"start\":65964},{\"end\":65967,\"start\":65966},{\"end\":66230,\"start\":66229},{\"end\":66238,\"start\":66237},{\"end\":66527,\"start\":66526},{\"end\":66540,\"start\":66539},{\"end\":66556,\"start\":66555},{\"end\":66908,\"start\":66907},{\"end\":66917,\"start\":66916},{\"end\":66926,\"start\":66925},{\"end\":66932,\"start\":66931},{\"end\":67222,\"start\":67221},{\"end\":67230,\"start\":67229},{\"end\":67237,\"start\":67236},{\"end\":67248,\"start\":67247},{\"end\":67557,\"start\":67556},{\"end\":67565,\"start\":67564},{\"end\":67572,\"start\":67571},{\"end\":67583,\"start\":67582},{\"end\":67798,\"start\":67797},{\"end\":67811,\"start\":67810},{\"end\":67820,\"start\":67819},{\"end\":67831,\"start\":67830},{\"end\":67842,\"start\":67841},{\"end\":67852,\"start\":67851},{\"end\":67862,\"start\":67861},{\"end\":67876,\"start\":67875},{\"end\":67887,\"start\":67886}]", "bib_author_last_name": "[{\"end\":45640,\"start\":45634},{\"end\":45801,\"start\":45779},{\"end\":45811,\"start\":45803},{\"end\":46020,\"start\":46008},{\"end\":46031,\"start\":46024},{\"end\":46038,\"start\":46035},{\"end\":46048,\"start\":46042},{\"end\":46060,\"start\":46052},{\"end\":46075,\"start\":46064},{\"end\":46095,\"start\":46079},{\"end\":46399,\"start\":46393},{\"end\":46409,\"start\":46403},{\"end\":46422,\"start\":46413},{\"end\":46433,\"start\":46426},{\"end\":46729,\"start\":46722},{\"end\":46739,\"start\":46733},{\"end\":46751,\"start\":46743},{\"end\":46764,\"start\":46759},{\"end\":46773,\"start\":46768},{\"end\":47147,\"start\":47140},{\"end\":47160,\"start\":47151},{\"end\":47171,\"start\":47164},{\"end\":47368,\"start\":47355},{\"end\":47380,\"start\":47372},{\"end\":47388,\"start\":47384},{\"end\":47399,\"start\":47394},{\"end\":47722,\"start\":47717},{\"end\":47739,\"start\":47726},{\"end\":47747,\"start\":47743},{\"end\":47758,\"start\":47751},{\"end\":47771,\"start\":47762},{\"end\":48149,\"start\":48144},{\"end\":48158,\"start\":48153},{\"end\":48501,\"start\":48492},{\"end\":48511,\"start\":48505},{\"end\":48527,\"start\":48515},{\"end\":48537,\"start\":48531},{\"end\":48896,\"start\":48891},{\"end\":48906,\"start\":48900},{\"end\":48915,\"start\":48910},{\"end\":48928,\"start\":48919},{\"end\":48936,\"start\":48932},{\"end\":48946,\"start\":48940},{\"end\":49287,\"start\":49280},{\"end\":49297,\"start\":49291},{\"end\":49307,\"start\":49301},{\"end\":49909,\"start\":49902},{\"end\":50210,\"start\":50202},{\"end\":50221,\"start\":50214},{\"end\":50230,\"start\":50225},{\"end\":50239,\"start\":50234},{\"end\":50587,\"start\":50578},{\"end\":50603,\"start\":50591},{\"end\":50616,\"start\":50607},{\"end\":51021,\"start\":51012},{\"end\":51030,\"start\":51025},{\"end\":51043,\"start\":51034},{\"end\":51453,\"start\":51444},{\"end\":51462,\"start\":51457},{\"end\":51473,\"start\":51466},{\"end\":51482,\"start\":51477},{\"end\":51493,\"start\":51486},{\"end\":51506,\"start\":51497},{\"end\":51750,\"start\":51747},{\"end\":51761,\"start\":51756},{\"end\":51770,\"start\":51765},{\"end\":51780,\"start\":51774},{\"end\":51794,\"start\":51784},{\"end\":51805,\"start\":51798},{\"end\":52151,\"start\":52147},{\"end\":52165,\"start\":52155},{\"end\":52178,\"start\":52171},{\"end\":52188,\"start\":52182},{\"end\":52199,\"start\":52192},{\"end\":52210,\"start\":52203},{\"end\":52506,\"start\":52493},{\"end\":52519,\"start\":52510},{\"end\":52528,\"start\":52523},{\"end\":52541,\"start\":52532},{\"end\":52941,\"start\":52930},{\"end\":52950,\"start\":52945},{\"end\":52964,\"start\":52958},{\"end\":53327,\"start\":53319},{\"end\":53336,\"start\":53331},{\"end\":53347,\"start\":53340},{\"end\":53357,\"start\":53351},{\"end\":53366,\"start\":53361},{\"end\":53710,\"start\":53702},{\"end\":53720,\"start\":53714},{\"end\":53736,\"start\":53724},{\"end\":53747,\"start\":53740},{\"end\":53756,\"start\":53751},{\"end\":54136,\"start\":54132},{\"end\":54145,\"start\":54140},{\"end\":54151,\"start\":54149},{\"end\":54160,\"start\":54155},{\"end\":54166,\"start\":54164},{\"end\":54173,\"start\":54170},{\"end\":54498,\"start\":54493},{\"end\":54622,\"start\":54614},{\"end\":54634,\"start\":54626},{\"end\":54919,\"start\":54914},{\"end\":55193,\"start\":55186},{\"end\":55206,\"start\":55197},{\"end\":55217,\"start\":55210},{\"end\":55232,\"start\":55221},{\"end\":55241,\"start\":55236},{\"end\":55253,\"start\":55245},{\"end\":55617,\"start\":55610},{\"end\":55630,\"start\":55621},{\"end\":55641,\"start\":55634},{\"end\":55652,\"start\":55645},{\"end\":55664,\"start\":55656},{\"end\":55673,\"start\":55668},{\"end\":55685,\"start\":55677},{\"end\":56026,\"start\":56020},{\"end\":56037,\"start\":56030},{\"end\":56048,\"start\":56041},{\"end\":56056,\"start\":56052},{\"end\":56065,\"start\":56060},{\"end\":56465,\"start\":56461},{\"end\":56474,\"start\":56469},{\"end\":56487,\"start\":56478},{\"end\":56494,\"start\":56491},{\"end\":56505,\"start\":56500},{\"end\":56516,\"start\":56509},{\"end\":56524,\"start\":56520},{\"end\":56537,\"start\":56528},{\"end\":56546,\"start\":56541},{\"end\":56556,\"start\":56550},{\"end\":56960,\"start\":56955},{\"end\":56968,\"start\":56964},{\"end\":57344,\"start\":57339},{\"end\":57353,\"start\":57348},{\"end\":57359,\"start\":57357},{\"end\":57365,\"start\":57363},{\"end\":57657,\"start\":57654},{\"end\":57664,\"start\":57661},{\"end\":57673,\"start\":57668},{\"end\":57684,\"start\":57679},{\"end\":57914,\"start\":57908},{\"end\":57922,\"start\":57918},{\"end\":57931,\"start\":57926},{\"end\":57943,\"start\":57935},{\"end\":58187,\"start\":58176},{\"end\":58202,\"start\":58191},{\"end\":58210,\"start\":58206},{\"end\":58220,\"start\":58214},{\"end\":58229,\"start\":58224},{\"end\":58492,\"start\":58489},{\"end\":58503,\"start\":58496},{\"end\":58513,\"start\":58507},{\"end\":58519,\"start\":58517},{\"end\":58528,\"start\":58523},{\"end\":58539,\"start\":58532},{\"end\":58548,\"start\":58543},{\"end\":58560,\"start\":58552},{\"end\":58573,\"start\":58564},{\"end\":58585,\"start\":58577},{\"end\":58853,\"start\":58849},{\"end\":59062,\"start\":59056},{\"end\":59068,\"start\":59066},{\"end\":59291,\"start\":59282},{\"end\":59301,\"start\":59295},{\"end\":59551,\"start\":59546},{\"end\":59560,\"start\":59555},{\"end\":59868,\"start\":59862},{\"end\":59881,\"start\":59872},{\"end\":59892,\"start\":59885},{\"end\":59902,\"start\":59896},{\"end\":59913,\"start\":59906},{\"end\":59924,\"start\":59917},{\"end\":60192,\"start\":60190},{\"end\":60201,\"start\":60196},{\"end\":60208,\"start\":60205},{\"end\":60448,\"start\":60444},{\"end\":60457,\"start\":60452},{\"end\":60469,\"start\":60461},{\"end\":60478,\"start\":60473},{\"end\":60854,\"start\":60852},{\"end\":60861,\"start\":60858},{\"end\":60869,\"start\":60865},{\"end\":61040,\"start\":61038},{\"end\":61046,\"start\":61044},{\"end\":61053,\"start\":61050},{\"end\":61059,\"start\":61057},{\"end\":61066,\"start\":61063},{\"end\":61262,\"start\":61256},{\"end\":61275,\"start\":61266},{\"end\":61288,\"start\":61279},{\"end\":61297,\"start\":61292},{\"end\":61551,\"start\":61546},{\"end\":61562,\"start\":61555},{\"end\":61571,\"start\":61566},{\"end\":61843,\"start\":61832},{\"end\":61851,\"start\":61847},{\"end\":61864,\"start\":61855},{\"end\":61873,\"start\":61868},{\"end\":62245,\"start\":62237},{\"end\":62255,\"start\":62249},{\"end\":62268,\"start\":62259},{\"end\":62279,\"start\":62272},{\"end\":62578,\"start\":62571},{\"end\":62591,\"start\":62582},{\"end\":62604,\"start\":62595},{\"end\":62613,\"start\":62608},{\"end\":62622,\"start\":62617},{\"end\":62633,\"start\":62626},{\"end\":62978,\"start\":62972},{\"end\":62987,\"start\":62982},{\"end\":63229,\"start\":63226},{\"end\":63242,\"start\":63233},{\"end\":63252,\"start\":63246},{\"end\":63260,\"start\":63256},{\"end\":63272,\"start\":63266},{\"end\":63282,\"start\":63276},{\"end\":63294,\"start\":63286},{\"end\":63302,\"start\":63298},{\"end\":63825,\"start\":63813},{\"end\":63836,\"start\":63831},{\"end\":63862,\"start\":63840},{\"end\":64163,\"start\":64156},{\"end\":64172,\"start\":64167},{\"end\":64185,\"start\":64176},{\"end\":64399,\"start\":64394},{\"end\":64412,\"start\":64403},{\"end\":64423,\"start\":64416},{\"end\":64436,\"start\":64427},{\"end\":64450,\"start\":64440},{\"end\":64462,\"start\":64454},{\"end\":64832,\"start\":64827},{\"end\":64845,\"start\":64836},{\"end\":64859,\"start\":64849},{\"end\":64871,\"start\":64863},{\"end\":64882,\"start\":64875},{\"end\":65269,\"start\":65264},{\"end\":65282,\"start\":65273},{\"end\":65296,\"start\":65286},{\"end\":65308,\"start\":65300},{\"end\":65319,\"start\":65312},{\"end\":65551,\"start\":65546},{\"end\":65564,\"start\":65555},{\"end\":65576,\"start\":65568},{\"end\":65590,\"start\":65580},{\"end\":65601,\"start\":65594},{\"end\":65609,\"start\":65603},{\"end\":65921,\"start\":65913},{\"end\":65934,\"start\":65927},{\"end\":65942,\"start\":65938},{\"end\":65951,\"start\":65946},{\"end\":65962,\"start\":65955},{\"end\":65978,\"start\":65968},{\"end\":66235,\"start\":66231},{\"end\":66244,\"start\":66239},{\"end\":66537,\"start\":66528},{\"end\":66553,\"start\":66541},{\"end\":66569,\"start\":66557},{\"end\":66914,\"start\":66909},{\"end\":66923,\"start\":66918},{\"end\":66929,\"start\":66927},{\"end\":66937,\"start\":66933},{\"end\":67227,\"start\":67223},{\"end\":67234,\"start\":67231},{\"end\":67245,\"start\":67238},{\"end\":67254,\"start\":67249},{\"end\":67562,\"start\":67558},{\"end\":67569,\"start\":67566},{\"end\":67580,\"start\":67573},{\"end\":67589,\"start\":67584},{\"end\":67808,\"start\":67799},{\"end\":67817,\"start\":67812},{\"end\":67828,\"start\":67821},{\"end\":67839,\"start\":67832},{\"end\":67849,\"start\":67843},{\"end\":67859,\"start\":67853},{\"end\":67873,\"start\":67863},{\"end\":67884,\"start\":67877},{\"end\":67896,\"start\":67888}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45732,\"start\":45622},{\"attributes\":{\"id\":\"b1\"},\"end\":45773,\"start\":45734},{\"attributes\":{\"id\":\"b2\"},\"end\":45864,\"start\":45775},{\"attributes\":{\"doi\":\"2018-09-01. 4\",\"id\":\"b3\"},\"end\":45946,\"start\":45866},{\"attributes\":{\"doi\":\"arXiv:1609.08675\",\"id\":\"b4\"},\"end\":46330,\"start\":45948},{\"attributes\":{\"doi\":\"arXiv:1809.00888\",\"id\":\"b5\"},\"end\":46629,\"start\":46332},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14160588},\"end\":47075,\"start\":46631},{\"attributes\":{\"doi\":\"abs/1702.01983\",\"id\":\"b7\"},\"end\":47323,\"start\":47077},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13012110},\"end\":47642,\"start\":47325},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7901560},\"end\":48040,\"start\":47644},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207237303},\"end\":48442,\"start\":48042},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11285357},\"end\":48801,\"start\":48444},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":32096248},\"end\":49227,\"start\":48803},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2341707},\"end\":49835,\"start\":49229},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2375110},\"end\":50128,\"start\":49837},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17627354},\"end\":50489,\"start\":50130},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7405987},\"end\":50888,\"start\":50491},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18800070},\"end\":51363,\"start\":50890},{\"attributes\":{\"id\":\"b18\"},\"end\":51680,\"start\":51365},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7684883},\"end\":52119,\"start\":51682},{\"attributes\":{\"doi\":\"130:1-130:10\",\"id\":\"b20\",\"matched_paper_id\":8692593},\"end\":52400,\"start\":52121},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":18665389},\"end\":52841,\"start\":52402},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15965841},\"end\":53258,\"start\":52843},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1819181},\"end\":53627,\"start\":53260},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15898368},\"end\":54040,\"start\":53629},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":49555906},\"end\":54472,\"start\":54042},{\"attributes\":{\"id\":\"b26\"},\"end\":54562,\"start\":54474},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9751504},\"end\":54859,\"start\":54564},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7369055},\"end\":55154,\"start\":54861},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2505656},\"end\":55513,\"start\":55156},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18087669},\"end\":55945,\"start\":55515},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10299788},\"end\":56371,\"start\":55947},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":61807685},\"end\":56897,\"start\":56373},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":54443881},\"end\":57214,\"start\":56899},{\"attributes\":{\"doi\":\"abs/1704.04086\",\"id\":\"b34\"},\"end\":57577,\"start\":57216},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":13684145},\"end\":57828,\"start\":57579},{\"attributes\":{\"doi\":\"abs/1710.10196\",\"id\":\"b36\"},\"end\":58117,\"start\":57830},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":54443881},\"end\":58463,\"start\":58119},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":44073530},\"end\":58806,\"start\":58465},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6155330},\"end\":59006,\"start\":58808},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b40\"},\"end\":59207,\"start\":59008},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b41\"},\"end\":59472,\"start\":59209},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":438533},\"end\":59799,\"start\":59474},{\"attributes\":{\"doi\":\"abs/1706.00409\",\"id\":\"b43\"},\"end\":60112,\"start\":59801},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":53317989},\"end\":60346,\"start\":60114},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":26639677},\"end\":60761,\"start\":60348},{\"attributes\":{\"id\":\"b46\"},\"end\":60993,\"start\":60763},{\"attributes\":{\"doi\":\"abs/1706.04717\",\"id\":\"b47\"},\"end\":61197,\"start\":60995},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3461376},\"end\":61519,\"start\":61199},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":6541990},\"end\":61737,\"start\":61521},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":8805770},\"end\":62145,\"start\":61739},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2460828},\"end\":62567,\"start\":62147},{\"attributes\":{\"id\":\"b52\"},\"end\":62894,\"start\":62569},{\"attributes\":{\"id\":\"b53\"},\"end\":63114,\"start\":62896},{\"attributes\":{\"id\":\"b54\"},\"end\":63759,\"start\":63116},{\"attributes\":{\"id\":\"b55\"},\"end\":64069,\"start\":63761},{\"attributes\":{\"id\":\"b56\"},\"end\":64336,\"start\":64071},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":14348821},\"end\":64758,\"start\":64338},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":52858569},\"end\":65192,\"start\":64760},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":49563706},\"end\":65542,\"start\":65194},{\"attributes\":{\"doi\":\"arXiv:1805.11729\",\"id\":\"b60\"},\"end\":65855,\"start\":65544},{\"attributes\":{\"doi\":\"abs/1611.05507\",\"id\":\"b61\"},\"end\":66162,\"start\":65857},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":5789620},\"end\":66480,\"start\":66164},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":25372915},\"end\":66821,\"start\":66482},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":10585115},\"end\":67163,\"start\":66823},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":4533859},\"end\":67497,\"start\":67165},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":44149078},\"end\":67713,\"start\":67499},{\"attributes\":{\"id\":\"b67\"},\"end\":68107,\"start\":67715}]", "bib_title": "[{\"end\":46718,\"start\":46631},{\"end\":47351,\"start\":47325},{\"end\":47713,\"start\":47644},{\"end\":48140,\"start\":48042},{\"end\":48488,\"start\":48444},{\"end\":48887,\"start\":48803},{\"end\":49276,\"start\":49229},{\"end\":49898,\"start\":49837},{\"end\":50198,\"start\":50130},{\"end\":50574,\"start\":50491},{\"end\":51008,\"start\":50890},{\"end\":51743,\"start\":51682},{\"end\":52143,\"start\":52121},{\"end\":52489,\"start\":52402},{\"end\":52923,\"start\":52843},{\"end\":53315,\"start\":53260},{\"end\":53698,\"start\":53629},{\"end\":54128,\"start\":54042},{\"end\":54610,\"start\":54564},{\"end\":54910,\"start\":54861},{\"end\":55182,\"start\":55156},{\"end\":55606,\"start\":55515},{\"end\":56016,\"start\":55947},{\"end\":56457,\"start\":56373},{\"end\":56951,\"start\":56899},{\"end\":57650,\"start\":57579},{\"end\":58172,\"start\":58119},{\"end\":58485,\"start\":58465},{\"end\":58843,\"start\":58808},{\"end\":59542,\"start\":59474},{\"end\":60186,\"start\":60114},{\"end\":60440,\"start\":60348},{\"end\":61252,\"start\":61199},{\"end\":61542,\"start\":61521},{\"end\":61828,\"start\":61739},{\"end\":62233,\"start\":62147},{\"end\":63222,\"start\":63116},{\"end\":63809,\"start\":63761},{\"end\":64390,\"start\":64338},{\"end\":64823,\"start\":64760},{\"end\":65260,\"start\":65194},{\"end\":66227,\"start\":66164},{\"end\":66524,\"start\":66482},{\"end\":66905,\"start\":66823},{\"end\":67219,\"start\":67165},{\"end\":67554,\"start\":67499}]", "bib_author": "[{\"end\":45642,\"start\":45624},{\"end\":45803,\"start\":45777},{\"end\":45813,\"start\":45803},{\"end\":46022,\"start\":46006},{\"end\":46033,\"start\":46022},{\"end\":46040,\"start\":46033},{\"end\":46050,\"start\":46040},{\"end\":46062,\"start\":46050},{\"end\":46077,\"start\":46062},{\"end\":46097,\"start\":46077},{\"end\":46401,\"start\":46391},{\"end\":46411,\"start\":46401},{\"end\":46424,\"start\":46411},{\"end\":46435,\"start\":46424},{\"end\":46731,\"start\":46720},{\"end\":46741,\"start\":46731},{\"end\":46753,\"start\":46741},{\"end\":46766,\"start\":46753},{\"end\":46775,\"start\":46766},{\"end\":47149,\"start\":47138},{\"end\":47162,\"start\":47149},{\"end\":47173,\"start\":47162},{\"end\":47370,\"start\":47353},{\"end\":47382,\"start\":47370},{\"end\":47390,\"start\":47382},{\"end\":47401,\"start\":47390},{\"end\":47724,\"start\":47715},{\"end\":47741,\"start\":47724},{\"end\":47749,\"start\":47741},{\"end\":47760,\"start\":47749},{\"end\":47773,\"start\":47760},{\"end\":48151,\"start\":48142},{\"end\":48160,\"start\":48151},{\"end\":48503,\"start\":48490},{\"end\":48513,\"start\":48503},{\"end\":48529,\"start\":48513},{\"end\":48539,\"start\":48529},{\"end\":48898,\"start\":48889},{\"end\":48908,\"start\":48898},{\"end\":48917,\"start\":48908},{\"end\":48930,\"start\":48917},{\"end\":48938,\"start\":48930},{\"end\":48948,\"start\":48938},{\"end\":49289,\"start\":49278},{\"end\":49299,\"start\":49289},{\"end\":49309,\"start\":49299},{\"end\":49911,\"start\":49900},{\"end\":50212,\"start\":50200},{\"end\":50223,\"start\":50212},{\"end\":50232,\"start\":50223},{\"end\":50241,\"start\":50232},{\"end\":50589,\"start\":50576},{\"end\":50605,\"start\":50589},{\"end\":50618,\"start\":50605},{\"end\":51023,\"start\":51010},{\"end\":51032,\"start\":51023},{\"end\":51045,\"start\":51032},{\"end\":51455,\"start\":51442},{\"end\":51464,\"start\":51455},{\"end\":51475,\"start\":51464},{\"end\":51484,\"start\":51475},{\"end\":51495,\"start\":51484},{\"end\":51508,\"start\":51495},{\"end\":51752,\"start\":51745},{\"end\":51763,\"start\":51752},{\"end\":51772,\"start\":51763},{\"end\":51782,\"start\":51772},{\"end\":51796,\"start\":51782},{\"end\":51807,\"start\":51796},{\"end\":52153,\"start\":52145},{\"end\":52167,\"start\":52153},{\"end\":52180,\"start\":52167},{\"end\":52190,\"start\":52180},{\"end\":52201,\"start\":52190},{\"end\":52212,\"start\":52201},{\"end\":52508,\"start\":52491},{\"end\":52521,\"start\":52508},{\"end\":52530,\"start\":52521},{\"end\":52543,\"start\":52530},{\"end\":52943,\"start\":52925},{\"end\":52952,\"start\":52943},{\"end\":52966,\"start\":52952},{\"end\":53329,\"start\":53317},{\"end\":53338,\"start\":53329},{\"end\":53349,\"start\":53338},{\"end\":53359,\"start\":53349},{\"end\":53368,\"start\":53359},{\"end\":53712,\"start\":53700},{\"end\":53722,\"start\":53712},{\"end\":53738,\"start\":53722},{\"end\":53749,\"start\":53738},{\"end\":53758,\"start\":53749},{\"end\":54138,\"start\":54130},{\"end\":54147,\"start\":54138},{\"end\":54153,\"start\":54147},{\"end\":54162,\"start\":54153},{\"end\":54168,\"start\":54162},{\"end\":54175,\"start\":54168},{\"end\":54500,\"start\":54491},{\"end\":54624,\"start\":54612},{\"end\":54636,\"start\":54624},{\"end\":54921,\"start\":54912},{\"end\":55195,\"start\":55184},{\"end\":55208,\"start\":55195},{\"end\":55219,\"start\":55208},{\"end\":55234,\"start\":55219},{\"end\":55243,\"start\":55234},{\"end\":55255,\"start\":55243},{\"end\":55619,\"start\":55608},{\"end\":55632,\"start\":55619},{\"end\":55643,\"start\":55632},{\"end\":55654,\"start\":55643},{\"end\":55666,\"start\":55654},{\"end\":55675,\"start\":55666},{\"end\":55687,\"start\":55675},{\"end\":56028,\"start\":56018},{\"end\":56039,\"start\":56028},{\"end\":56050,\"start\":56039},{\"end\":56058,\"start\":56050},{\"end\":56067,\"start\":56058},{\"end\":56467,\"start\":56459},{\"end\":56476,\"start\":56467},{\"end\":56489,\"start\":56476},{\"end\":56496,\"start\":56489},{\"end\":56507,\"start\":56496},{\"end\":56518,\"start\":56507},{\"end\":56526,\"start\":56518},{\"end\":56539,\"start\":56526},{\"end\":56548,\"start\":56539},{\"end\":56558,\"start\":56548},{\"end\":56962,\"start\":56953},{\"end\":56970,\"start\":56962},{\"end\":57346,\"start\":57337},{\"end\":57355,\"start\":57346},{\"end\":57361,\"start\":57355},{\"end\":57367,\"start\":57361},{\"end\":57659,\"start\":57652},{\"end\":57666,\"start\":57659},{\"end\":57675,\"start\":57666},{\"end\":57686,\"start\":57675},{\"end\":57916,\"start\":57906},{\"end\":57924,\"start\":57916},{\"end\":57933,\"start\":57924},{\"end\":57945,\"start\":57933},{\"end\":58189,\"start\":58174},{\"end\":58204,\"start\":58189},{\"end\":58212,\"start\":58204},{\"end\":58222,\"start\":58212},{\"end\":58231,\"start\":58222},{\"end\":58494,\"start\":58487},{\"end\":58505,\"start\":58494},{\"end\":58515,\"start\":58505},{\"end\":58521,\"start\":58515},{\"end\":58530,\"start\":58521},{\"end\":58541,\"start\":58530},{\"end\":58550,\"start\":58541},{\"end\":58562,\"start\":58550},{\"end\":58575,\"start\":58562},{\"end\":58587,\"start\":58575},{\"end\":58855,\"start\":58845},{\"end\":59064,\"start\":59052},{\"end\":59070,\"start\":59064},{\"end\":59293,\"start\":59280},{\"end\":59303,\"start\":59293},{\"end\":59553,\"start\":59544},{\"end\":59562,\"start\":59553},{\"end\":59870,\"start\":59860},{\"end\":59883,\"start\":59870},{\"end\":59894,\"start\":59883},{\"end\":59904,\"start\":59894},{\"end\":59915,\"start\":59904},{\"end\":59926,\"start\":59915},{\"end\":60194,\"start\":60188},{\"end\":60203,\"start\":60194},{\"end\":60210,\"start\":60203},{\"end\":60450,\"start\":60442},{\"end\":60459,\"start\":60450},{\"end\":60471,\"start\":60459},{\"end\":60480,\"start\":60471},{\"end\":60856,\"start\":60850},{\"end\":60863,\"start\":60856},{\"end\":60871,\"start\":60863},{\"end\":61042,\"start\":61036},{\"end\":61048,\"start\":61042},{\"end\":61055,\"start\":61048},{\"end\":61061,\"start\":61055},{\"end\":61068,\"start\":61061},{\"end\":61264,\"start\":61254},{\"end\":61277,\"start\":61264},{\"end\":61290,\"start\":61277},{\"end\":61299,\"start\":61290},{\"end\":61553,\"start\":61544},{\"end\":61564,\"start\":61553},{\"end\":61573,\"start\":61564},{\"end\":61845,\"start\":61830},{\"end\":61853,\"start\":61845},{\"end\":61866,\"start\":61853},{\"end\":61875,\"start\":61866},{\"end\":62247,\"start\":62235},{\"end\":62257,\"start\":62247},{\"end\":62270,\"start\":62257},{\"end\":62281,\"start\":62270},{\"end\":62580,\"start\":62569},{\"end\":62593,\"start\":62580},{\"end\":62606,\"start\":62593},{\"end\":62615,\"start\":62606},{\"end\":62624,\"start\":62615},{\"end\":62635,\"start\":62624},{\"end\":62980,\"start\":62968},{\"end\":62989,\"start\":62980},{\"end\":63231,\"start\":63224},{\"end\":63244,\"start\":63231},{\"end\":63254,\"start\":63244},{\"end\":63262,\"start\":63254},{\"end\":63274,\"start\":63262},{\"end\":63284,\"start\":63274},{\"end\":63296,\"start\":63284},{\"end\":63304,\"start\":63296},{\"end\":63827,\"start\":63811},{\"end\":63838,\"start\":63827},{\"end\":63864,\"start\":63838},{\"end\":64165,\"start\":64154},{\"end\":64174,\"start\":64165},{\"end\":64187,\"start\":64174},{\"end\":64193,\"start\":64187},{\"end\":64401,\"start\":64392},{\"end\":64414,\"start\":64401},{\"end\":64425,\"start\":64414},{\"end\":64438,\"start\":64425},{\"end\":64452,\"start\":64438},{\"end\":64464,\"start\":64452},{\"end\":64834,\"start\":64825},{\"end\":64847,\"start\":64834},{\"end\":64861,\"start\":64847},{\"end\":64873,\"start\":64861},{\"end\":64884,\"start\":64873},{\"end\":65271,\"start\":65262},{\"end\":65284,\"start\":65271},{\"end\":65298,\"start\":65284},{\"end\":65310,\"start\":65298},{\"end\":65321,\"start\":65310},{\"end\":65553,\"start\":65544},{\"end\":65566,\"start\":65553},{\"end\":65578,\"start\":65566},{\"end\":65592,\"start\":65578},{\"end\":65603,\"start\":65592},{\"end\":65611,\"start\":65603},{\"end\":65923,\"start\":65911},{\"end\":65936,\"start\":65923},{\"end\":65944,\"start\":65936},{\"end\":65953,\"start\":65944},{\"end\":65964,\"start\":65953},{\"end\":65980,\"start\":65964},{\"end\":66237,\"start\":66229},{\"end\":66246,\"start\":66237},{\"end\":66539,\"start\":66526},{\"end\":66555,\"start\":66539},{\"end\":66571,\"start\":66555},{\"end\":66916,\"start\":66907},{\"end\":66925,\"start\":66916},{\"end\":66931,\"start\":66925},{\"end\":66939,\"start\":66931},{\"end\":67229,\"start\":67221},{\"end\":67236,\"start\":67229},{\"end\":67247,\"start\":67236},{\"end\":67256,\"start\":67247},{\"end\":67564,\"start\":67556},{\"end\":67571,\"start\":67564},{\"end\":67582,\"start\":67571},{\"end\":67591,\"start\":67582},{\"end\":67810,\"start\":67797},{\"end\":67819,\"start\":67810},{\"end\":67830,\"start\":67819},{\"end\":67841,\"start\":67830},{\"end\":67851,\"start\":67841},{\"end\":67861,\"start\":67851},{\"end\":67875,\"start\":67861},{\"end\":67886,\"start\":67875},{\"end\":67898,\"start\":67886}]", "bib_venue": "[{\"end\":45742,\"start\":45734},{\"end\":45886,\"start\":45879},{\"end\":46004,\"start\":45948},{\"end\":46389,\"start\":46332},{\"end\":46830,\"start\":46775},{\"end\":47136,\"start\":47077},{\"end\":47458,\"start\":47401},{\"end\":47821,\"start\":47773},{\"end\":48218,\"start\":48160},{\"end\":48598,\"start\":48539},{\"end\":49002,\"start\":48948},{\"end\":49412,\"start\":49309},{\"end\":49969,\"start\":49911},{\"end\":50290,\"start\":50241},{\"end\":50667,\"start\":50618},{\"end\":51103,\"start\":51045},{\"end\":51440,\"start\":51365},{\"end\":51859,\"start\":51807},{\"end\":52240,\"start\":52224},{\"end\":52605,\"start\":52543},{\"end\":53031,\"start\":52966},{\"end\":53423,\"start\":53368},{\"end\":53813,\"start\":53758},{\"end\":54237,\"start\":54175},{\"end\":54489,\"start\":54474},{\"end\":54691,\"start\":54636},{\"end\":54991,\"start\":54921},{\"end\":55313,\"start\":55255},{\"end\":55710,\"start\":55687},{\"end\":56139,\"start\":56067},{\"end\":56611,\"start\":56558},{\"end\":57047,\"start\":56970},{\"end\":57335,\"start\":57216},{\"end\":57693,\"start\":57686},{\"end\":57904,\"start\":57830},{\"end\":58262,\"start\":58231},{\"end\":58615,\"start\":58587},{\"end\":58891,\"start\":58855},{\"end\":59050,\"start\":59008},{\"end\":59278,\"start\":59209},{\"end\":59617,\"start\":59562},{\"end\":59858,\"start\":59801},{\"end\":60219,\"start\":60210},{\"end\":60534,\"start\":60480},{\"end\":60848,\"start\":60763},{\"end\":61034,\"start\":60995},{\"end\":61348,\"start\":61299},{\"end\":61607,\"start\":61573},{\"end\":61929,\"start\":61875},{\"end\":62332,\"start\":62281},{\"end\":62721,\"start\":62635},{\"end\":62966,\"start\":62896},{\"end\":63381,\"start\":63304},{\"end\":63898,\"start\":63864},{\"end\":64152,\"start\":64071},{\"end\":64496,\"start\":64464},{\"end\":64942,\"start\":64884},{\"end\":65349,\"start\":65321},{\"end\":65673,\"start\":65627},{\"end\":65909,\"start\":65857},{\"end\":66301,\"start\":66246},{\"end\":66639,\"start\":66571},{\"end\":66969,\"start\":66939},{\"end\":67310,\"start\":67256},{\"end\":67595,\"start\":67591},{\"end\":67795,\"start\":67715},{\"end\":49519,\"start\":49414},{\"end\":51907,\"start\":51861},{\"end\":63445,\"start\":63383},{\"end\":64515,\"start\":64498}]"}}}, "year": 2023, "month": 12, "day": 17}