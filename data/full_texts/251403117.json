{"id": 251403117, "updated": "2023-10-05 12:01:49.345", "metadata": {"title": "AWEncoder: Adversarial Watermarking Pre-trained Encoders in Contrastive Learning", "authors": "[{\"first\":\"Tianxing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Hanzhou\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xiaofeng\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Guangling\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "Applied Sciences (2023)", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "As a self-supervised learning paradigm, contrastive learning has been widely used to pre-train a powerful encoder as an effective feature extractor for various downstream tasks. This process requires numerous unlabeled training data and computational resources, which makes the pre-trained encoder become valuable intellectual property of the owner. However, the lack of a priori knowledge of downstream tasks makes it non-trivial to protect the intellectual property of the pre-trained encoder by applying conventional watermarking methods. To deal with this problem, in this paper, we introduce AWEncoder, an adversarial method for watermarking the pre-trained encoder in contrastive learning. First, as an adversarial perturbation, the watermark is generated by enforcing the training samples to be marked to deviate respective location and surround a randomly selected key image in the embedding space. Then, the watermark is embedded into the pre-trained encoder by further optimizing a joint loss function. As a result, the watermarked encoder not only performs very well for downstream tasks, but also enables us to verify its ownership by analyzing the discrepancy of output provided using the encoder as the backbone under both white-box and black-box conditions. Extensive experiments demonstrate that the proposed work enjoys pretty good effectiveness and robustness on different contrastive learning algorithms and downstream tasks, which has verified the superiority and applicability of the proposed work.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2208.03948", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2208-03948", "doi": "10.3390/app13063531"}}, "content": {"source": {"pdf_hash": "55b18e6de5b279cb6472ace549e5f238402f8479", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.03948v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c22e16ca1eb00bee9ab99ddfc521d577abedec6e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/55b18e6de5b279cb6472ace549e5f238402f8479.txt", "contents": "\nAWEncoder: Adversarial Watermarking Pre-trained Encoders in Contrastive Learning\n\n\nTianxing Zhang \nHanzhou Wu \nXiaofeng Lu \nGuangling Sun \nAWEncoder: Adversarial Watermarking Pre-trained Encoders in Contrastive Learning\n1\nAs a self-supervised learning paradigm, contrastive learning has been widely used to pre-train a powerful encoder as an effective feature extractor for various downstream tasks. This process requires numerous unlabeled training data and computational resources, which makes the pre-trained encoder become valuable intellectual property of the owner. However, the lack of a priori knowledge of downstream tasks makes it non-trivial to protect the intellectual property of the pre-trained encoder by applying conventional watermarking methods. To deal with this problem, in this paper, we introduce AWEncoder, an adversarial method for watermarking the pre-trained encoder in contrastive learning. First, as an adversarial perturbation, the watermark is generated by enforcing the training samples to be marked to deviate respective location and surround a randomly selected key image in the embedding space. Then, the watermark is embedded into the pre-trained encoder by further optimizing a joint loss function. As a result, the watermarked encoder not only performs very well for downstream tasks, but also enables us to verify its ownership by analyzing the discrepancy of output provided using the encoder as the backbone under both white-box and black-box conditions. Extensive experiments demonstrate that the proposed work enjoys pretty good effectiveness and robustness on different contrastive learning algorithms and downstream tasks, which has verified the superiority and applicability of the proposed work.Index Terms-Model watermarking, contrastive learning, security, deep neural networks.\n\nI. INTRODUCTION\n\nT HE rapid development of deep learning (DL) has benefited a lot from a large number of diverse labeled datasets, which, however, simultaneously has become a factor hindering the further development of DL. It is due to the reason that the cost of collecting sufficient high-quality datasets with correct labels is prohibitively expensive. Fortunately, the emergence of self-supervised learning (SSL) [1] to train a powerful encoder with the unlabeled samples can effectively overcome the above obstacle. Contrastive learning [2], [3], as one mainstream selfsupervised learning technique, has achieved great success in various tasks. For example, SimCLR [4] and MoCo [5] have already enabled the SSL encoders to outperform traditional supervised learning based encoders in several downstream tasks. However, training the encoders still requires lots of unlabeled data and consumes a great deal of computing resources. Many malicious users are likely to steal the pre-trained encoders to obtain illegal income and even bring serious security risks [6], [7]. Therefore, we need to find reliable solutions to protect the intellectual property of the pre-trained encoders. All  As a major technique to protect the intellectual property of DL models, model watermarking [8], [9], [10], [11] is widely studied recently. It can be roughly divided into two categories: white-box watermarking and black-box watermarking. The former [12], [13] embeds a watermark into the internal parameters [14], feature maps [15] or structures [16] of a DL model, which needs white-box access to the watermarked model. The latter [17] usually uses backdooring [18], [19] or similar techniques such as adversarial attack [20] to mark a model. The ownership can be verified by analyzing the classification results of the marked model in correspondence to a set of carefully crafted samples. Compared with white-box watermarking, black-box watermarking is more desirable for applications since in most cases the model owner has no access to the internal details of the target model. However, for protecting the aforementioned encoders, the lack of a priori knowledge of downstream tasks makes it quite difficult to craft the special samples for blackbox watermarking. It indicates that we cannot simply extend the existing black-box watermarking schemes to the encoders. As a result, we urgently need to develop novel watermarking schemes specifically for the pre-trained encoders.\n\nIn this paper, we introduce AWEncoder, a copyright protection method for the pre-trained encoders in contrastive learning via an adversarial watermark. In the proposed method, through optimizing an adversarial perturbation [21], we determine such a perturbation according to the pre-trained encoder that it can cluster the perturbed training samples around a randomly selected key image in the embedding space and can also be used as the secret watermark. The watermark is then embedded into the pre-trained encoder via contrastive learning by further optimizing a joint loss involving watermark embedding. In this way, even if the downstream task of the watermarked encoder is unknown to us, we can still verify its ownership under the black-box condition. Experimental results demonstrate that the ownership can be reliably verified under both white-box and black-box conditions, which is quite helpful for applications.\n\nIn summary, the main contributions of this paper include:\n\n\u2022 We propose a novel adversarial watermarking strategy for the pre-trained encoders in the embedding space, which is more effective for watermark verification compared with previous black-box watermarking methods. \u2022 Unlike conventional watermarking methods assuming that the downstream task of the watermarked model is same as the original one, the proposed method does not require the priori knowledge of the downstream task. As a result, the proposed method enables us to verify the ownership under white-box and stricter black-box settings, which is more applicable to practice compared with previous arts.\n\n\u2022 Extensive experimental results demonstrate that the proposed method has satisfactory ability to verify the ownership of the target model and resist common watermark removal attacks such as model fine-tuning [22] and model pruning [23], which has good application prospects. The rest structure is organized as follows. We first provide preliminaries in Section II, followed by the proposed method in Section III. Experimental results and analysis are provided in Section IV. Finally, we conclude this paper in Section V.\n\n\nII. PRELIMINARIES\n\n\nA. Contrastive Learning\n\nContrastive learning is one of the most popular SSL techniques. SSL learns from unlabeled data, which can be regarded as an intermediate form between supervised and unsupervised learning. In this paper, we watermark a pre-trained encoder by contrastive learning. To this end, we consider two representative contrastive learning algorithms SimCLR [4] and MoCo v2 [24]. We briefly describe them in the following. It is pointed that the proposed method is not subject to this two algorithms.\n\n1) SimCLR [4]: SimCLR aims to learn representations by maximizing agreement between differently augmented views of the same sample via a contrastive loss function in the latent space. Briefly, SimCLR consists of three modules that are data augmentation, feature encoder f and projection head g. Data augmentation transforms a sample x \u2208 X into two augmented views x i and x j treated as a positive pair. The feature encoder f extracts representation vectors from augmented examples, e.g., h i = f (x i ) for the augmented sample x i . The projection head g maps representations to the space where the contrastive loss is applied, e.g., z i = g(h i ) = g(f (x i )). With N > 0 samples in a mini-batch, we are able to collect 2N augmented samples. Two augmented samples determined from the same sample constitute a positive pair. Otherwise, it is treated as a negative pair. Let sim(u, v) = u T v/||u||||v|| represent the dot product between 2 normalized u and v (i.e., cosine similarity). The loss function for a positive pair (z i , z j ) and a negative pair (z i , z k ) is then defined as [4]:\nL i,j = \u2212 log exp (sim (z i , z j ) /\u03c4 ) 2N k=1 I[k = i] \u00b7 exp (sim (z i , z k ) /\u03c4 ) ,(1)\nwhere \u03c4 denotes a temperature parameter and\nI[k = i] \u2208 {0, 1}\nis an indicator function equal to 1 if and only if k = i.\n\n2) MoCo v2 [24]: As an improved version of MoCo, MoCo v2 is consisting of a query encoder f q (x; \u03b8 q ) and a momentum key encoder f k (x; \u03b8 k ). The better performance of MoCo v2 is attributed to a large dictionary K = {k 1 , k 2 , ..., k |K| } which is the feature vectors of previous batches as the negative samples and it is continuously updating. A batch of N samples will be encoded by f q as feature vectors and simultaneously encoded by f k as different augmentations. Suppose that there is a single key k + in the dictionary that an encoded query q matches, the contrastive loss function is defined as [24]:\nL = \u2212 log exp q T k + /\u03c4 |K| i=1 exp (q T k i /\u03c4 ) .(2)\nBy minimizing the above contrastive loss function, the parameters of f q , i.e., denoted by \u03b8 q , are updated by back-propagation, but the parameters of f k , i.e., \u03b8 k , are updated by:\n\u03b8 k \u2190 \u03bb\u03b8 k + (1 \u2212 \u03bb)\u03b8 q ,(3)\nwhere \u03bb \u2208 [0, 1) is a momentum coefficient.\n\n\nB. Problem and Threats\n\nMainstream black-box watermarking methods mainly focus on classification based models such as [25], [26]. The zero-bit watermark is often embedded into the model by enforcing the model to learn the mapping relationship between the carefully crafted samples and the pre-determined labels. However, with the rise of contrastive learning, the pre-trained encoders are treated as feature extractors for various downstream tasks and the pre-training process is relying on the SSL strategy rather than label-based supervised learning [27], [28]. It indicates that traditional black-box watermarking techniques are not suitable for the pre-trained encoders in contrastive learning.\n\nOn the other hand, from the viewpoint of the adversary, he steals the encoder by an unauthorized way and add new layers to build the specific downstream task. The adversary may not train the encoder from scratch, but he is able to modify the encoder such as fine-tuning and pruning. In this case, even the owner of the encoder does not know what datasets and tasks will be used in downstream. As a result, when to watermark an encoder, it is very necessary for the embedded watermark to be transferable and robust. Moreover, it is quite desirable that the ownership of the encoder can be verified under both whitebox condition and black-box condition. This has motivated the authors to propose a novel adversarial watermarking method. Fig. 1 shows the general framework of the proposed method. In the following, we are to provide the technical details.\n\n\nIII. PROPOSED METHOD\n\n\nA. Watermark Generation\n\nThe first step is to generate an adversarial perturbation w adv as the watermark using the pre-trained encoder E \u03b8 . Since our task is encoder oriented, the traditional perturbation optimization strategy based on classification labels is inapplicable. To deal with this problem, we use the embedding of a randomly selected image x tar (called key image) extracted by E \u03b8 , i.e., E \u03b8 (x tar ), to generate w adv with a clean dataset D. This enables the embedding of a perturbed image denoted by E \u03b8 (x i +w adv ) (where x i \u2208 D, i \u2208 {1, 2, ..., |D|}) cluster around E \u03b8 (x tar ). In other words, the distance between E \u03b8 (x tar ) and E \u03b8 (x i + w adv ) is expected to be as low as possible. To achieve this goal, we minimize the following loss during perturbation optimization:\nL adv = E xi\u223cD [1 \u2212 sim(E \u03b8 (x i + w adv ), E \u03b8 (x tar ))] . (4)\nBy back-propagation, w adv can be generated without changing the parameters of E \u03b8 . It is obvious that different x tar result in different w adv . By using x tar as a key, it is difficult for the adversary to forge the watermark. Some watermark generation methods by linearly superimposed fixed patterns [29] may also perturb the embedding of an image, but their effect of changing the behavior of the encoder is weaker than our method.  Fig. 1. General framework for the proposed method. We use the same dataset for Phase I, Phase II and Phase III (a) so that the watermark can be successfully embedded into E \u03b8 while avoiding degrading the encoding performance of the encoder. In Phase II, each clean image is augmented into two images, only one randomly selected from which is used for generating the corresponding adversarial image that will be used for watermark embedding.\n\n\nB. Watermark Embedding\n\nAfter generating w adv , the next step is to embed w adv into the pre-trained E \u03b8 . It is realized by further training E \u03b8 according to a combined loss L comb , which consists of two components, i.e., the contrastive loss L con and the watermarking loss L wat . In other words, we have L comb = L con + \u03b1L wat , where \u03b1 is a parameter balancing the loss and we use \u03b1 = 40 by default. On one hand, L con has already been defined in Section II, referring to Eq. (1) and Eq. (2). Notice that, L con needs to be adjusted when other contrastive learning algorithms are applied. On the other hand, L wat is defined as the KL divergence [30] between the adversarial embedding and the non-adversarial embedding processed with the softmax function \u03c3, i.e.,\nL wat = E x i \u223cD [KL(\u03c3(E \u03b8 (x i )), \u03c3(E \u03b8 (x i + w adv )))] ,(5)\nwhere x i is sampled from the augmented dataset D . Thus, E \u03b8 can be watermarked by updating its parameters during training. We will use E + \u03b8 to represent the marked version of E \u03b8 .\n\n\nC. Watermark Verification\n\nWhen the defender verifies whether the suspicious encoder infringes the intellectual property of the watermarked encoder, white-box scenario and black-box scenario can be considered. In the white-box scenario, the defender has the access to the target encoder E \u2212 \u03b8 \u2248 E + \u03b8 . Namely, the defender can directly obtain the output of E \u2212 \u03b8 for watermark verification. We use the average KL divergence between a set of clean images and the corresponding adversarial images for similarity analysis:\nT sim = 1 |D | |D | i=1 KL(\u03c3(E \u2212 \u03b8 (x i )), \u03c3(E \u2212 \u03b8 (x i + w adv ))),(6)\nwhere D is the clean dataset used for watermark verification. The watermark will be successfully verified if T sim is less than a threshold t s , namely, we hope to keep T sim as low as possible.\n\nIn our experiments, |D | is set to 1000 by default.\n\nIn black-box scenario, given a suspicious downstream model M, the defender wants to verify whether M is developed from E + \u03b8 . The defender should build a clean dataset D * related to the downstream task. Then, the classification performance for the downstream task is analyzed by:\nT cls = 1 |D * | |D * | i=1 I[M(x * i ) = M(x * i + w adv )].(7)\nThe watermark will be successfully verified if T cls is less than a threshold t c , namely, we hope to keep T cls as low as possible.\n\nIn our experiments, |D * | is set to 1000 by default.  Remark: Same as many existing adversarial attacking methods, w adv is generated by using back-propagation. The strength of w adv is constrained by a threshold . To ensure that w adv \u221e \u2264 , w adv is projected on the \u221e norm-ball around x i with radius . We set \u221e -norm and = 15 by default. With this perturbation strength, it can effectively cluster the perturbed images around the key image x tar in the embedding space. The implementation can be found in the released source code.\n(a) (b) (c) (d) (e) (f) (g) (h) (i)\n\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\n\nIn our experiments, two benchmark datasets CIFAR-10 [31] and ImageNet [32] are used to pre-train the encoder. And, three benchmark datasets STL-10 [33], GTSRB [34] and ImageNet are used for the downstream task of the encoder. For ImageNet, we randomly select 30 semantic categories out for training and another 10 categories different from training out for testing. For the watermark generation phase, the pre-trained encoders are ResNet-18 [35] as the base model for SimCLR and MoCo v2 and the strength of the adversarial watermark is set as = 15. For the watermark embedding phase, we set the batch size to 50 for SimCLR and 32 for MoCo v2. Meanwhile, the total number of epochs is set to 50 and the learning rate is set to 0.003. Additionally, the key image is randomly selected from ImageNet which belongs to the semantic category \"Plane\" (see Fig. 1) and this image does not appear in the training set. We conduct simulation with PyTorch, accelerated by a single RTX 3080 GPU. To validate and reproduce our experiments, code will be released via https://github.com/fc88zhang/AWEncoder.   \n\n\nA. Effectiveness\n\nWe first provide examples for the adversarial image in Fig.  1 Phase III. As shown in Fig. 2, due to the diversity, different images result in different degradation of the adversarial image, but the visual quality are all satisfactory, verifying the applicability of adversarial perturbation. It is admitted that one may use other perturbation strategy which is not our main focus.\n\nThen, we evaluate the effectiveness of watermarked encoder in both black-box and white-box scenarios. The encoders are pre-trained on two kinds of training datasets and contrastive learning algorithms. We compare WPE [27] and the proposed AWEncoder in three downstream tasks with black-box access. The results are shown in Table I, where \"CE\" is short for clean encoder and \"WE\" is short for watermarked encoder. The fifth column shows the classification accuracy on the downstream task before and after watermarking. The last column gives T cls before/after watermarking. It is inferred that although there is a slight decrease in classification accuracy for the downstream task after watermarking, by assessing the difference between T cls for CE and WE, AWEncoder is more discriminative.\n\nIn white-box scenario, we verify the ownership by contrasting the embedding similarity of clean images and watermarked images. Table II presents that the similarity score of WE is much lower than that of CE, which indicates that the similarity score can be used for reliable verification. When the watermark is generated with an incorrect key image (\"dog\" in Fig. 1 is used in our experiment), T sim is much higher than the correct one. It indicates that AWEncoder has high security.\n\n\nB. Uniqueness\n\nTo evaluate uniqueness, we generate forged watermarks by different methods including replacing Plane with Dog as the key image (see Fig. 1), changing the value of , and replacing the pre-trained encoder based on ResNet-18 with a surrogate  pre-trained encoder based on ResNet-50 [35]. Table III shows the results, in which the similarity score and the classification score of the incorrect watermark are much higher than that of the correct one. It indicates that the proposed method provides superior performance on watermarking uniqueness.\n\n\nC. Robustness\n\nIn practice, adversaries may perform removal attacks such as fine-tuning and pruning to erase the watermark. To quantify the robustness of AWEncoder, we consider two common removal methods, i.e., fine-tuning all layers (FTAL) and retraining all layers (RTAL). FTAL fine-tunes the entire encoder with the training dataset and RTAL retrains the entire encoder with the downstream training dataset. In addition, we remove the parameters with the smallest L1-norms to prune the encoder. We also verify the robustness of AWEncoder in both white-box and black-box settings. The results in Table IV and Table V demonstrate that in white-box setting, pruning does not affect the watermarked encoder. Although fine-tuning increases T sim to a certain extent, AWEncoder can still effectively verify the ownership by applying a suitable threshold. In brief summary, AWEncoder is capable of resisting common attacks.\n\nFor black-box scenario, we compare AWEncoder with WPE by applying different attacks. The results are shown in Table  VI and Table VII. It can be easily inferred that both fine-tuning and pruning will compromise the watermarking performance, however, AWEncoder is more robust than WPE against the removal attacks, which verifies the superiority of AWEncoder.\n\n\nV. CONCLUSION\n\nIn this paper, we propose AWEncoder, an effective copyright protection technique for the pre-trained encoder in contrastive learning, which can be not only applied in both white-box and black-box scenarios but also transferred to several downstream tasks. Compared to the existing encoder watermarking, AWEncoder significantly improves the effectiveness and robustness. In the future, we will extend the proposed method to federated learning and ensemble learning, which are another two popular learning strategies widely applied in deep learning.\n\nFig. 2 .\n2Some examples for the adversarial image: (a, d, g) are clean images randomly selected from GTSRB, ImageNet and STL-10, respectively; (b, e, h) are adversarial with SimCLR; (c, f, i) are adversarial with MoCo v2.\n\n\nthe authors in this manuscript are from the School of Communication and Information Engineering, Shanghai University, Shanghai 200444, China; E-mails: zhang tx@shu.edu.cn, h.wu.phd@ieee.org, luxiaofeng@shu.edu.cn, sunguangling@shu.edu.cn. Corresponding author: Guangling Sun Hanzhou Wu improved and proofread this manuscript seriously.\n\nTABLE I EFFECTIVENESS\nIEVALUATION UNDER THE BLACK-BOX CONDITION.Pre-trained \ndataset \nEncoder \nDownstream \ndataset \nMethod \nAccuracy \nCE / WE \n\nTcls \nCE / WE / |CE-WE| \n\nImageNet \nSimCLR \n\nImageNet \nWPE \n74.5% \n73.4% \n25.5% \n86.4% \n60.9% \nAWEncoder \n74.5% \n71.1% \n69.5% \n3.9% \n65.6% \n\nGTSRB \nWPE \n77.8% \n77.5% \n31.7% \n90.4% \n58.7% \nAWEncoder \n77.8% \n75.5% \n83.2% \n11.1% 72.1% \n\nSTL-10 \nWPE \n64.7% \n64.1% \n27.3% \n91.6% \n64.3% \nAWEncoder \n64.7% \n61.2% \n86.3% \n6.1% \n80.2% \n\nCIFAR-10 \nMoCo v2 \n\nImageNet \nWPE \n72.3% \n72.0% \n21.3% \n81.5% \n60.2% \nAWEncoder \n72.3% \n69.9% \n67.6% \n5.7% \n61.9% \n\nGTSRB \nWPE \n82.9% \n80.7% \n16.5% \n84.6% \n68.1% \nAWEncoder \n82.9% \n82.7% \n84.3% \n10.9% 73.4% \n\nSTL-10 \nWPE \n64.2% \n63.9% \n14.0% \n82.8% \n68.8% \nAWEncoder \n64.2% \n63.4% \n80.5% \n6.9% \n73.6% \n\n\n\nTABLE II EFFECTIVENESS\nIIEVALUATION UNDER THE WHITE-BOX CONDITION.Model \nTsim \nCorrect watermark \nIncorrect watermark \n\nSimCLR \nCE \n101.0 \u2191 \n186.2 \u2191 \nWE \n0.4 \n34.8 \u2191 \n\nMoCo v2 \nCE \n1.6 \u2191 \n2.4 \u2191 \nWE \n0.003 \n0.1 \u2191 \n\nTABLE III \nUNIQUENESS EVALUATION DUE TO DIFFERENT SETTINGS. \n\nDownstream \ndataset \nEncoder \nSetting \nSimCLR \nMoCo v2 \nTcls \nTsim \nTcls \nTsim \n\nGTSRB \n\nPre-trained \nencoder \n\nPlane, = 15 \n11.1% \n0.4 \n10.9% \n0.003 \nPlane, = 20 \n80.6% \u2191 \n39.5 \u2191 \n72.9% \u2191 \n0.15 \u2191 \nDog, = 15 \n70.6% \u2191 \n34.8 \u2191 \n68.4% \u2191 \n0.1 \u2191 \nSurrogate \nencoder \nPlane, = 15 \n66.7% \u2191 \n22.9 \u2191 \n60.3% \u2191 \n0.04 \u2191 \n\n\n\nTABLE IV\nIVROBUSTNESS AGAINST FINE-TUNING \n(UNDER WHITE-BOX CONDITION). \n\nFine-tuning \nTsim \n\nSimCLR \nMoCo v2 \n\n-\n0.4 \n0.003 \n\nFTAL \n15.7 \n0.01 \n\nRTAL \n25.0 \n0.02 \n\n\n\nTABLE V\nVROBUSTNESS AGAINST PRUNING \n(UNDER WHITE-BOX CONDITION). \n\nPruning ratio \nTsim \nSimCLR \nMoCo v2 \n-\n0.40 \n0.003 \n0.2 \n0.66 \n0.005 \n0.4 \n0.66 \n0.008 \n0.6 \n0.68 \n0.01 \n0.8 \n0.72 \n0.04 \n\n\n\nTABLE VI ROBUSTNESS\nVIIN BLACK-BOX VERIFICATION AGAINST FINE-TUNING.Fine-\ntuning \n\nDownstream \ndataset \nMethod \n\nSimCLR \nMoCo v2 \nAccuracy \nCE / WE \n\nTcls \nCE / WE / |CE-WE| \n\nAccuracy \nCE / WE \n\nTcls \nCE / WE / |CE-WE| \n\nFTAL \n\nImageNet \nWPE \n74.8% 74.3% \n22.3% \n74.0% \n51.7% \n75.2% \n74.6% \n17.5% \n76.4% \n58.9% \nAWEncoder \n74.8% 72.1% \n69.4% \n7.6% \n61.8% \n75.2% \n73.1% \n69.5% \n8.7% \n60.8% \n\nGTSRB \nWPE \n65.7% 66.2% \n32.8% \n61.7% \n28.9% \n88.6% \n86.5% \n15.5% \n57.0% \n41.5% \nAWEncoder \n65.7% 63.2% \n76.1% \n17.6% 58.5% \n88.6% \n85.5% \n81.3% \n16.6% \n64.7% \n\nSTL-10 \nWPE \n63.4% 62.7% \n31.0% \n86.7% \n55.7% \n64.6% \n64.6% \n15.6% \n69.0% \n53.4% \nAWEncoder \n63.4% 60.8% \n85.4% \n10.7% 74.7% \n64.6% \n64.4% \n80.5% \n20.5% \n60.0% \n\nRTAL \n\nImageNet \nWPE \n94.5% 94.3% \n21.6% \n60.5% \n38.9% \n96.0% \n96.2% \n22.5% \n61.8% \n39.3% \nAWEncoder \n94.5% 92.7% \n60.2% \n18.6% 41.6% \n96.0% \n95.1% \n63.5% \n23.9% \n39.6% \n\nGTSRB \nWPE \n98.5% 98.9% \n29.7% \n55.0% \n25.3% \n98.1% \n97.3% \n16.3% \n48.0% \n31.7% \nAWEncoder \n98.5% 97.6% \n62.4% \n21.6% 40.8% \n98.1% \n97.1% \n64.8% \n18.3% \n46.5% \n\nSTL-10 \nWPE \n83.1% 82.2% \n24.6% \n50.3% \n25.7% \n87.8% \n87.5% \n17.2% \n41.7% \n24.5% \nAWEncoder \n83.1% 82.7% \n65.9% \n20.0% 45.9% \n87.8% \n85.2% \n61.3% \n23.4% \n37.9% \n\n\n\nTABLE VII ROBUSTNESS\nVIIIN BLACK-BOX VERIFICATION AGAINST PRUNING. Tcls CE / WE / |CE-WE| Accuracy CE / WE Tcls CE / WE / |CE-WE|Downstream \ndataset \n\nPruning \nratio \nMethod \n\nSimCLR \nMoCo v2 \nAccuracy \nCE / WE \n\nGTSRB \n\n0.2 \nWPE \n79.8% \n80.3% \n30.8% \n84.5% \n53.7% \n82.5% 80.5% \n17.3% \n80.4% \n63.1% \nAWEncoder \n79.8% \n78.0% \n80.1% \n14.6% \n65.5% \n82.5% 81.6% \n84.2% \n15.8% 68.4% \n\n0.4 \nWPE \n77.9% \n77.2% \n29.6% \n79.2% \n49.6% \n79.1% 78.8% \n19.6% \n72.8% \n53.2% \nAWEncoder \n77.9% \n76.3% \n80.5% \n19.2% \n61.3% \n79.1% 77.5% \n82.1% \n19.4% 62.7% \n\n0.6 \nWPE \n71.2% \n70.5% \n27.9% \n68.0% \n40.1% \n73.1% 72.1% \n18.9% \n62.6% \n43.7% \nAWEncoder \n71.2% \n68.5% \n79.5% \n20.4% \n59.1% \n73.1% 70.0% \n77.6% \n22.4% 55.2% \n\n0.8 \nWPE \n64.9% \n64.8% \n27.5% \n65.7% \n38.2% \n65.4% 65.7% \n17.0% \n52.1% \n35.1% \nAWEncoder \n64.9% \n63.8% \n77.9% \n21.6% \n56.3% \n65.4% 62.6% \n75.4% \n24.0% 51.4% \n\n\n\nSelf-supervised visual feature learning with deep neural networks: a survey. L Jing, Y Tian, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4311L. Jing, and Y. Tian, \"Self-supervised visual feature learning with deep neural networks: a survey,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4037-4058, 2021.\n\nBootstrap your own latent: a new approach to self-supervised learning. J.-B Grill, F Strub, F Altch, Proc. Neural Information Processing Systems. Neural Information essing SystemsJ.-B. Grill, F. Strub, and F.Altch et al., \"Bootstrap your own latent: a new approach to self-supervised learning,\" In: Proc. Neural Information Processing Systems, pp. 21271-21284, 2020.\n\nImproved baselines with momentum contrastive learning. X Chen, H Fan, R Girshick, K He, arXiv:2003.04297arXiv preprintX. Chen, H. Fan, R. Girshick, and K. He, \"Improved baselines with mo- mentum contrastive learning,\" arXiv preprint arXiv:2003.04297, 2020.\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, Proc. International Conference on Machine Learning. International Conference on Machine LearningT. Chen, S. Kornblith, M. Norouzi, and G. Hinton, \"A simple framework for contrastive learning of visual representations,\" In: Proc. International Conference on Machine Learning, pp. 1597-1607, 2020.\n\nMomentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionK. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \"Momentum contrast for unsupervised visual representation learning,\" In: Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 9726-9735, 2020.\n\nKnockoff Nets: Stealing functionality of black-box models. T Orekondy, B Schiele, M Fritz, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionT. Orekondy, B. Schiele, and M. Fritz, \"Knockoff Nets: Stealing func- tionality of black-box models,\" In: Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 4949-4958, 2019.\n\nExploring connections between active learning and model extraction. V Chandrasekaran, K Chaudhuri, I Giacomelli, S Jha, S Yan, Proc. 29th USENIX Security Symposium. 29th USENIX Security SymposiumV. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan, \"Exploring connections between active learning and model extraction,\" In: Proc. 29th USENIX Security Symposium, pp. 1309-1326, 2020.\n\nRethinking deep neural network ownership verification: Embedding passports to defeat ambiguity attacks. L Fan, K W Ng, C S Chan, Proc. Neural Information Processing Systems. Neural Information essing SystemsL. Fan, K. W. Ng, and C. S. Chan, \"Rethinking deep neural network ownership verification: Embedding passports to defeat ambiguity at- tacks,\" In: Proc. Neural Information Processing Systems, pp. 4714-4723, 2019.\n\nModel watermarking for image processing networks. J Zhang, D Chen, J Liao, H Fang, W Zhang, W Zhou, H Cui, N Yu, Proc. AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial IntelligenceJ. Zhang, D. Chen, J. Liao, H. Fang, W. Zhang, W. Zhou, H. Cui, and N. Yu, \"Model watermarking for image processing networks,\" In: Proc. AAAI Conference on Artificial Intelligence, pp. 12805-12812, 2020.\n\nCopy, Right? A testing framework for copyright protection of deep learning models. J Chen, J Wang, T Peng, Y Sun, P Cheng, S Ji, X Ma, B Li, D Song, arXiv:2112.05588arXiv preprintJ. Chen, J. Wang, T. Peng, Y. Sun, P. Cheng, S. Ji, X. Ma, B. Li, and D. Song, \"Copy, Right? A testing framework for copyright protection of deep learning models,\" arXiv preprint arXiv:2112.05588, 2021.\n\nWatermarking neural networks with watermarked images. H Wu, G Liu, Y Yao, X Zhang, IEEE Trans. Circuits Syst. Video Technol. 317H. Wu, G. Liu, Y. Yao, and X. Zhang, \"Watermarking neural networks with watermarked images,\" IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 7, pp. 2591-2601, 2021.\n\nEmbedding watermarks into deep neural networks. Y Uchida, Y Nagai, S Sakazawa, S Satoh, Proc. ACM International Conference on Multimedia Retrieval. ACM International Conference on Multimedia RetrievalY. Uchida, Y. Nagai, S. Sakazawa, and S. Satoh, \"Embedding wa- termarks into deep neural networks,\" In: Proc. ACM International Conference on Multimedia Retrieval, pp. 269-277, 2017.\n\nRobust watermarking of neural network with exponential weighting. R Namba, J Sakuma, Proc. ACM Asia Conference on Computer and Communications Security. ACM Asia Conference on Computer and Communications SecurityR. Namba and J. Sakuma, \"Robust watermarking of neural network with exponential weighting,\" In: Proc. ACM Asia Conference on Computer and Communications Security, pp. 228-240, 2019.\n\nWatermarking in deep neural networks via error back-propagation. J Wang, H Wu, X Zhang, Y Yao, Proc. IS&T Electronic Imaging, Media Watermarking, Security and Forensics. IS&T Electronic Imaging, Media Watermarking, Security and ForensicsJ. Wang, H. Wu, X. Zhang, and Y. Yao, \"Watermarking in deep neural networks via error back-propagation,\" In: Proc. IS&T Electronic Imaging, Media Watermarking, Security and Forensics, pp. 1-8, 2020.\n\nDeepsigns: An end-toend watermarking framework for ownership protection of deep neural networks. B D Rouhani, H Chen, F Koushanfar, Proc. International Conference on Architectural Support for Programming Languages and Operating Systems. International Conference on Architectural Support for Programming Languages and Operating SystemsB. D. Rouhani, H. Chen, and F. Koushanfar, \"Deepsigns: An end-to- end watermarking framework for ownership protection of deep neural networks,\" In: Proc. International Conference on Architectural Support for Programming Languages and Operating Systems, pp. 485-497, 2019.\n\nStructural watermarking to deep neural networks via network channel pruning. X Zhao, Y Yao, H Wu, X Zhang, Proc. IEEE Workshop on Information Forensics and Security. IEEE Workshop on Information Forensics and SecurityX. Zhao, Y. Yao, H. Wu, and X. Zhang, \"Structural watermarking to deep neural networks via network channel pruning,\" In: Proc. IEEE Workshop on Information Forensics and Security, pp. 1-6, 2021.\n\nTurning your weakness into a strength: Watermarking deep neural networks by backdooring. Y Adi, C Baum, M Cisse, B Pinkas, J Keshet, Proc. 27th USENIX Security Symposium. 27th USENIX Security SymposiumY. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, \"Turning your weakness into a strength: Watermarking deep neural networks by backdooring,\" In: Proc. 27th USENIX Security Symposium, pp. 1615- 1631, 2018.\n\nBadnets: Evaluating backdooring attacks on deep neural networks. T Gu, K Liu, B Dolan-Gavitt, S Garg, IEEE Access. 7T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, \"Badnets: Evaluating backdooring attacks on deep neural networks,\" IEEE Access, vol. 7, pp. 47230-47244, 2019.\n\nBadencoder: Backdoor attacks to pre-trained encoders in self-supervised learning. J Jia, Y Liu, N Z Gong, arXiv:2108.00352arXiv preprintJ. Jia, Y. Liu, and N. Z. Gong, \"Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning,\" arXiv preprint arXiv:2108.00352, 2021.\n\nAdversarial frontier stitching for remote neural network watermarking. E Le Merrer, P Perez, G Tr\u00e9dan, Neural Computing and Applications. 32E. Le Merrer, P. Perez, G. Tr\u00e9dan, \"Adversarial frontier stitching for remote neural network watermarking.\" Neural Computing and Applica- tions, vol. 32, no. 13, pp. 9233-9244, 2020.\n\nUniversal adversarial perturbations. S.-M Moosavi-Dezfooli, A Fawzi, O Fawzi, P Frossard, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionS.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, \"Univer- sal adversarial perturbations,\" In: Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 1765-1773, 2017.\n\nFine-pruning: Defending against backdooring attacks on deep neural networks. K Liu, B Dolan-Gavitt, S Garg, Proc. International Symposium on Research in Attacks, Intrusions, and Defenses. International Symposium on Research in Attacks, Intrusions, and DefensesK. Liu, B. Dolan-Gavitt, and S. Garg, \"Fine-pruning: Defending against backdooring attacks on deep neural networks,\" In: Proc. International Symposium on Research in Attacks, Intrusions, and Defenses, pp. 273- 294, 2018.\n\nPruning filters for efficient convnets. H Li, A Kadav, I Durdanovic, H Samet, H P Graf, arXiv:1608.08710arXiv preprintH. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, \"Pruning filters for efficient convnets,\" arXiv preprint arXiv:1608.08710, 2016.\n\nImproved baselines with momentum contrastive learning. X Chen, H Fan, R Girshick, K He, arXiv:2003.04297arXiv preprintX. Chen, H. Fan, R. Girshick, K. He, \"Improved baselines with momen- tum contrastive learning,\" arXiv preprint arXiv:2003.04297, 2020.\n\nEntangled watermarks as a defense against model extraction. H Jia, C A Choquette-Choo, V Chandrasekaran, N Papernot, Proc. 30th USENIX Security Symposium. 30th USENIX Security SymposiumH. Jia, C. A. Choquette-Choo, V. Chandrasekaran, and N. Papernot, \"Entangled watermarks as a defense against model extraction,\" In: Proc. 30th USENIX Security Symposium, pp. 1937-1954, 2021.\n\nProtecting intellectual property of deep neural networks with watermarking. J Zhang, Z Gu, J Jang, Proc. ACM Asia Conference on Computer and Communications Security. ACM Asia Conference on Computer and Communications SecurityJ. Zhang, Z. Gu, and J. Jang et al., \"Protecting intellectual property of deep neural networks with watermarking,\" In: Proc. ACM Asia Conference on Computer and Communications Security, pp. 159-172, 2018.\n\nWatermarking pre-trained encoders in contrastive learning. Y Wu, H Qiu, T Zhang, M Qiu, arXiv:2201.08217arXiv preprintY. Wu, H. Qiu, T. Zhang, M. Qiu, et al., \"Watermarking pre-trained encoders in contrastive learning,\" arXiv preprint arXiv:2201.08217, 2022.\n\nSSLGuard: A watermarking scheme for self-supervised learning pre-trained encoders. T Cong, X He, Y Zhang, arXiv:2201.11692arXiv preprintT. Cong, X. He, and Y. Zhang, \"SSLGuard: A watermarking scheme for self-supervised learning pre-trained encoders,\" arXiv preprint arXiv:2201.11692, 2022.\n\nTargeted backdoor attacks on deep learning systems using data poisoning. X Chen, C Liu, B Li, K Lu, D Song, arXiv:1712.05526arXiv preprintX. Chen, C. Liu, B. Li, K. Lu, and D. Song, \"Targeted backdoor attacks on deep learning systems using data poisoning,\" arXiv preprint arXiv:1712.05526, 2017.\n\nTheoretically principled trade-off between robustness and accuracy. H Zhang, Y Yu, J Jiao, E Xing, L El Ghaoui, M Jordan, Proc. International Conference on Machine Learning. International Conference on Machine LearningH. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan, \"Theoretically principled trade-off between robustness and accuracy,\" In: Proc. International Conference on Machine Learning, pp. 7472-7482, 2019.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Personal CommunicationA. Krizhevsky, G. Hinton, et al., \"Learning multiple layers of features from tiny images,\" Personal Communication, 2009.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, International Journal of Computer Vision. 1153O. Russakovsky, J. Deng, and H. Su et al., \"Imagenet large scale visual recognition challenge,\" International Journal of Computer Vision, vol. 115, no. 3, pp. 211-252, 2015.\n\nAn analysis of single-layer networks in unsupervised feature learning. A Coates, A Ng, H Lee, Proc. 14th International Conference on Artificial Intelligence and Statistics. 14th International Conference on Artificial Intelligence and StatisticsA. Coates, A. Ng, and H. Lee, \"An analysis of single-layer networks in unsupervised feature learning,\" In: Proc. 14th International Conference on Artificial Intelligence and Statistics, pp. 215-223, 2011.\n\nThe German traffic sign recognition benchmark: a multi-class classification competition. J Stallkamp, M Schlipsing, J Salmen, C Igel, Proc. International Joint Conference Neural Networks. International Joint Conference Neural NetworksJ. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, \"The German traffic sign recognition benchmark: a multi-class classification competition,\" In: Proc. International Joint Conference Neural Networks, pp. 1453-1460, 2011.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" In : Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016\n", "annotations": {"author": "[{\"end\":99,\"start\":84},{\"end\":111,\"start\":100},{\"end\":124,\"start\":112},{\"end\":139,\"start\":125}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":93},{\"end\":110,\"start\":108},{\"end\":123,\"start\":121},{\"end\":138,\"start\":135}]", "author_first_name": "[{\"end\":92,\"start\":84},{\"end\":107,\"start\":100},{\"end\":120,\"start\":112},{\"end\":134,\"start\":125}]", "author_affiliation": null, "title": "[{\"end\":81,\"start\":1},{\"end\":220,\"start\":140}]", "venue": null, "abstract": "[{\"end\":1827,\"start\":223}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2249,\"start\":2246},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2374,\"start\":2371},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2379,\"start\":2376},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2502,\"start\":2499},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2515,\"start\":2512},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2895,\"start\":2892},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2900,\"start\":2897},{\"end\":3017,\"start\":3014},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3113,\"start\":3110},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3118,\"start\":3115},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3124,\"start\":3120},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3130,\"start\":3126},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3272,\"start\":3268},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3278,\"start\":3274},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3331,\"start\":3327},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3350,\"start\":3346},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3369,\"start\":3365},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3455,\"start\":3451},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3485,\"start\":3481},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3491,\"start\":3487},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3545,\"start\":3541},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4528,\"start\":4524},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6108,\"start\":6104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6131,\"start\":6127},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6813,\"start\":6810},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6830,\"start\":6826},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6967,\"start\":6964},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8048,\"start\":8045},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8277,\"start\":8273},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8877,\"start\":8873},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9319,\"start\":9315},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9325,\"start\":9321},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9753,\"start\":9749},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9759,\"start\":9755},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11951,\"start\":11947},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13023,\"start\":13020},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13182,\"start\":13178},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15540,\"start\":15536},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15558,\"start\":15554},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15635,\"start\":15631},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15647,\"start\":15643},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15929,\"start\":15925},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17202,\"start\":17198},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18557,\"start\":18553}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20884,\"start\":20662},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":21222,\"start\":20885},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21999,\"start\":21223},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":22587,\"start\":22000},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":22754,\"start\":22588},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":22948,\"start\":22755},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":24159,\"start\":22949},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":25018,\"start\":24160}]", "paragraph": "[{\"end\":4299,\"start\":1846},{\"end\":5223,\"start\":4301},{\"end\":5282,\"start\":5225},{\"end\":5893,\"start\":5284},{\"end\":6416,\"start\":5895},{\"end\":6952,\"start\":6464},{\"end\":8049,\"start\":6954},{\"end\":8184,\"start\":8141},{\"end\":8260,\"start\":8203},{\"end\":8878,\"start\":8262},{\"end\":9121,\"start\":8935},{\"end\":9194,\"start\":9151},{\"end\":9895,\"start\":9221},{\"end\":10749,\"start\":9897},{\"end\":11576,\"start\":10800},{\"end\":12521,\"start\":11642},{\"end\":13295,\"start\":12548},{\"end\":13544,\"start\":13361},{\"end\":14067,\"start\":13574},{\"end\":14336,\"start\":14141},{\"end\":14389,\"start\":14338},{\"end\":14672,\"start\":14391},{\"end\":14871,\"start\":14738},{\"end\":15407,\"start\":14873},{\"end\":16577,\"start\":15484},{\"end\":16979,\"start\":16598},{\"end\":17771,\"start\":16981},{\"end\":18256,\"start\":17773},{\"end\":18815,\"start\":18274},{\"end\":19737,\"start\":18833},{\"end\":20096,\"start\":19739},{\"end\":20661,\"start\":20114}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8140,\"start\":8050},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8202,\"start\":8185},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8934,\"start\":8879},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9150,\"start\":9122},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11641,\"start\":11577},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13360,\"start\":13296},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14140,\"start\":14068},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14737,\"start\":14673},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15443,\"start\":15408}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17311,\"start\":17304},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17908,\"start\":17900},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18568,\"start\":18559},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19436,\"start\":19416},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19872,\"start\":19849}]", "section_header": "[{\"end\":1844,\"start\":1829},{\"end\":6436,\"start\":6419},{\"end\":6462,\"start\":6439},{\"end\":9219,\"start\":9197},{\"end\":10772,\"start\":10752},{\"end\":10798,\"start\":10775},{\"end\":12546,\"start\":12524},{\"end\":13572,\"start\":13547},{\"end\":15482,\"start\":15445},{\"end\":16596,\"start\":16580},{\"end\":18272,\"start\":18259},{\"end\":18831,\"start\":18818},{\"end\":20112,\"start\":20099},{\"end\":20671,\"start\":20663},{\"end\":21245,\"start\":21224},{\"end\":22023,\"start\":22001},{\"end\":22597,\"start\":22589},{\"end\":22763,\"start\":22756},{\"end\":22969,\"start\":22950},{\"end\":24181,\"start\":24161}]", "table": "[{\"end\":21999,\"start\":21288},{\"end\":22587,\"start\":22067},{\"end\":22754,\"start\":22600},{\"end\":22948,\"start\":22765},{\"end\":24159,\"start\":23018},{\"end\":25018,\"start\":24290}]", "figure_caption": "[{\"end\":20884,\"start\":20673},{\"end\":21222,\"start\":20887},{\"end\":21288,\"start\":21247},{\"end\":22067,\"start\":22026},{\"end\":23018,\"start\":22972},{\"end\":24290,\"start\":24185}]", "figure_ref": "[{\"end\":10638,\"start\":10632},{\"end\":12087,\"start\":12081},{\"end\":16338,\"start\":16332},{\"end\":16660,\"start\":16653},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16690,\"start\":16684},{\"end\":18138,\"start\":18132},{\"end\":18412,\"start\":18406}]", "bib_author_first_name": "[{\"end\":25098,\"start\":25097},{\"end\":25106,\"start\":25105},{\"end\":25460,\"start\":25456},{\"end\":25469,\"start\":25468},{\"end\":25478,\"start\":25477},{\"end\":25809,\"start\":25808},{\"end\":25817,\"start\":25816},{\"end\":25824,\"start\":25823},{\"end\":25836,\"start\":25835},{\"end\":26083,\"start\":26082},{\"end\":26091,\"start\":26090},{\"end\":26104,\"start\":26103},{\"end\":26115,\"start\":26114},{\"end\":26489,\"start\":26488},{\"end\":26495,\"start\":26494},{\"end\":26502,\"start\":26501},{\"end\":26508,\"start\":26507},{\"end\":26515,\"start\":26514},{\"end\":26918,\"start\":26917},{\"end\":26930,\"start\":26929},{\"end\":26941,\"start\":26940},{\"end\":27336,\"start\":27335},{\"end\":27354,\"start\":27353},{\"end\":27367,\"start\":27366},{\"end\":27381,\"start\":27380},{\"end\":27388,\"start\":27387},{\"end\":27769,\"start\":27768},{\"end\":27776,\"start\":27775},{\"end\":27778,\"start\":27777},{\"end\":27784,\"start\":27783},{\"end\":27786,\"start\":27785},{\"end\":28135,\"start\":28134},{\"end\":28144,\"start\":28143},{\"end\":28152,\"start\":28151},{\"end\":28160,\"start\":28159},{\"end\":28168,\"start\":28167},{\"end\":28177,\"start\":28176},{\"end\":28185,\"start\":28184},{\"end\":28192,\"start\":28191},{\"end\":28578,\"start\":28577},{\"end\":28586,\"start\":28585},{\"end\":28594,\"start\":28593},{\"end\":28602,\"start\":28601},{\"end\":28609,\"start\":28608},{\"end\":28618,\"start\":28617},{\"end\":28624,\"start\":28623},{\"end\":28630,\"start\":28629},{\"end\":28636,\"start\":28635},{\"end\":28932,\"start\":28931},{\"end\":28938,\"start\":28937},{\"end\":28945,\"start\":28944},{\"end\":28952,\"start\":28951},{\"end\":29228,\"start\":29227},{\"end\":29238,\"start\":29237},{\"end\":29247,\"start\":29246},{\"end\":29259,\"start\":29258},{\"end\":29630,\"start\":29629},{\"end\":29639,\"start\":29638},{\"end\":30023,\"start\":30022},{\"end\":30031,\"start\":30030},{\"end\":30037,\"start\":30036},{\"end\":30046,\"start\":30045},{\"end\":30492,\"start\":30491},{\"end\":30494,\"start\":30493},{\"end\":30505,\"start\":30504},{\"end\":30513,\"start\":30512},{\"end\":31079,\"start\":31078},{\"end\":31087,\"start\":31086},{\"end\":31094,\"start\":31093},{\"end\":31100,\"start\":31099},{\"end\":31504,\"start\":31503},{\"end\":31511,\"start\":31510},{\"end\":31519,\"start\":31518},{\"end\":31528,\"start\":31527},{\"end\":31538,\"start\":31537},{\"end\":31890,\"start\":31889},{\"end\":31896,\"start\":31895},{\"end\":31903,\"start\":31902},{\"end\":31919,\"start\":31918},{\"end\":32180,\"start\":32179},{\"end\":32187,\"start\":32186},{\"end\":32194,\"start\":32193},{\"end\":32196,\"start\":32195},{\"end\":32461,\"start\":32460},{\"end\":32464,\"start\":32462},{\"end\":32474,\"start\":32473},{\"end\":32483,\"start\":32482},{\"end\":32754,\"start\":32750},{\"end\":32774,\"start\":32773},{\"end\":32783,\"start\":32782},{\"end\":32792,\"start\":32791},{\"end\":33199,\"start\":33198},{\"end\":33206,\"start\":33205},{\"end\":33222,\"start\":33221},{\"end\":33644,\"start\":33643},{\"end\":33650,\"start\":33649},{\"end\":33659,\"start\":33658},{\"end\":33673,\"start\":33672},{\"end\":33682,\"start\":33681},{\"end\":33684,\"start\":33683},{\"end\":33917,\"start\":33916},{\"end\":33925,\"start\":33924},{\"end\":33932,\"start\":33931},{\"end\":33944,\"start\":33943},{\"end\":34176,\"start\":34175},{\"end\":34183,\"start\":34182},{\"end\":34185,\"start\":34184},{\"end\":34203,\"start\":34202},{\"end\":34221,\"start\":34220},{\"end\":34569,\"start\":34568},{\"end\":34578,\"start\":34577},{\"end\":34584,\"start\":34583},{\"end\":34983,\"start\":34982},{\"end\":34989,\"start\":34988},{\"end\":34996,\"start\":34995},{\"end\":35005,\"start\":35004},{\"end\":35267,\"start\":35266},{\"end\":35275,\"start\":35274},{\"end\":35281,\"start\":35280},{\"end\":35548,\"start\":35547},{\"end\":35556,\"start\":35555},{\"end\":35563,\"start\":35562},{\"end\":35569,\"start\":35568},{\"end\":35575,\"start\":35574},{\"end\":35840,\"start\":35839},{\"end\":35849,\"start\":35848},{\"end\":35855,\"start\":35854},{\"end\":35863,\"start\":35862},{\"end\":35871,\"start\":35870},{\"end\":35874,\"start\":35872},{\"end\":35884,\"start\":35883},{\"end\":36257,\"start\":36256},{\"end\":36271,\"start\":36270},{\"end\":36476,\"start\":36475},{\"end\":36491,\"start\":36490},{\"end\":36499,\"start\":36498},{\"end\":36797,\"start\":36796},{\"end\":36807,\"start\":36806},{\"end\":36813,\"start\":36812},{\"end\":37265,\"start\":37264},{\"end\":37278,\"start\":37277},{\"end\":37292,\"start\":37291},{\"end\":37302,\"start\":37301},{\"end\":37680,\"start\":37679},{\"end\":37686,\"start\":37685},{\"end\":37695,\"start\":37694},{\"end\":37702,\"start\":37701}]", "bib_author_last_name": "[{\"end\":25103,\"start\":25099},{\"end\":25111,\"start\":25107},{\"end\":25466,\"start\":25461},{\"end\":25475,\"start\":25470},{\"end\":25484,\"start\":25479},{\"end\":25814,\"start\":25810},{\"end\":25821,\"start\":25818},{\"end\":25833,\"start\":25825},{\"end\":25839,\"start\":25837},{\"end\":26088,\"start\":26084},{\"end\":26101,\"start\":26092},{\"end\":26112,\"start\":26105},{\"end\":26122,\"start\":26116},{\"end\":26492,\"start\":26490},{\"end\":26499,\"start\":26496},{\"end\":26505,\"start\":26503},{\"end\":26512,\"start\":26509},{\"end\":26524,\"start\":26516},{\"end\":26927,\"start\":26919},{\"end\":26938,\"start\":26931},{\"end\":26947,\"start\":26942},{\"end\":27351,\"start\":27337},{\"end\":27364,\"start\":27355},{\"end\":27378,\"start\":27368},{\"end\":27385,\"start\":27382},{\"end\":27392,\"start\":27389},{\"end\":27773,\"start\":27770},{\"end\":27781,\"start\":27779},{\"end\":27791,\"start\":27787},{\"end\":28141,\"start\":28136},{\"end\":28149,\"start\":28145},{\"end\":28157,\"start\":28153},{\"end\":28165,\"start\":28161},{\"end\":28174,\"start\":28169},{\"end\":28182,\"start\":28178},{\"end\":28189,\"start\":28186},{\"end\":28195,\"start\":28193},{\"end\":28583,\"start\":28579},{\"end\":28591,\"start\":28587},{\"end\":28599,\"start\":28595},{\"end\":28606,\"start\":28603},{\"end\":28615,\"start\":28610},{\"end\":28621,\"start\":28619},{\"end\":28627,\"start\":28625},{\"end\":28633,\"start\":28631},{\"end\":28641,\"start\":28637},{\"end\":28935,\"start\":28933},{\"end\":28942,\"start\":28939},{\"end\":28949,\"start\":28946},{\"end\":28958,\"start\":28953},{\"end\":29235,\"start\":29229},{\"end\":29244,\"start\":29239},{\"end\":29256,\"start\":29248},{\"end\":29265,\"start\":29260},{\"end\":29636,\"start\":29631},{\"end\":29646,\"start\":29640},{\"end\":30028,\"start\":30024},{\"end\":30034,\"start\":30032},{\"end\":30043,\"start\":30038},{\"end\":30050,\"start\":30047},{\"end\":30502,\"start\":30495},{\"end\":30510,\"start\":30506},{\"end\":30524,\"start\":30514},{\"end\":31084,\"start\":31080},{\"end\":31091,\"start\":31088},{\"end\":31097,\"start\":31095},{\"end\":31106,\"start\":31101},{\"end\":31508,\"start\":31505},{\"end\":31516,\"start\":31512},{\"end\":31525,\"start\":31520},{\"end\":31535,\"start\":31529},{\"end\":31545,\"start\":31539},{\"end\":31893,\"start\":31891},{\"end\":31900,\"start\":31897},{\"end\":31916,\"start\":31904},{\"end\":31924,\"start\":31920},{\"end\":32184,\"start\":32181},{\"end\":32191,\"start\":32188},{\"end\":32201,\"start\":32197},{\"end\":32471,\"start\":32465},{\"end\":32480,\"start\":32475},{\"end\":32490,\"start\":32484},{\"end\":32771,\"start\":32755},{\"end\":32780,\"start\":32775},{\"end\":32789,\"start\":32784},{\"end\":32801,\"start\":32793},{\"end\":33203,\"start\":33200},{\"end\":33219,\"start\":33207},{\"end\":33227,\"start\":33223},{\"end\":33647,\"start\":33645},{\"end\":33656,\"start\":33651},{\"end\":33670,\"start\":33660},{\"end\":33679,\"start\":33674},{\"end\":33689,\"start\":33685},{\"end\":33922,\"start\":33918},{\"end\":33929,\"start\":33926},{\"end\":33941,\"start\":33933},{\"end\":33947,\"start\":33945},{\"end\":34180,\"start\":34177},{\"end\":34200,\"start\":34186},{\"end\":34218,\"start\":34204},{\"end\":34230,\"start\":34222},{\"end\":34575,\"start\":34570},{\"end\":34581,\"start\":34579},{\"end\":34589,\"start\":34585},{\"end\":34986,\"start\":34984},{\"end\":34993,\"start\":34990},{\"end\":35002,\"start\":34997},{\"end\":35009,\"start\":35006},{\"end\":35272,\"start\":35268},{\"end\":35278,\"start\":35276},{\"end\":35287,\"start\":35282},{\"end\":35553,\"start\":35549},{\"end\":35560,\"start\":35557},{\"end\":35566,\"start\":35564},{\"end\":35572,\"start\":35570},{\"end\":35580,\"start\":35576},{\"end\":35846,\"start\":35841},{\"end\":35852,\"start\":35850},{\"end\":35860,\"start\":35856},{\"end\":35868,\"start\":35864},{\"end\":35881,\"start\":35875},{\"end\":35891,\"start\":35885},{\"end\":36268,\"start\":36258},{\"end\":36278,\"start\":36272},{\"end\":36488,\"start\":36477},{\"end\":36496,\"start\":36492},{\"end\":36502,\"start\":36500},{\"end\":36804,\"start\":36798},{\"end\":36810,\"start\":36808},{\"end\":36817,\"start\":36814},{\"end\":37275,\"start\":37266},{\"end\":37289,\"start\":37279},{\"end\":37299,\"start\":37293},{\"end\":37307,\"start\":37303},{\"end\":37683,\"start\":37681},{\"end\":37692,\"start\":37687},{\"end\":37699,\"start\":37696},{\"end\":37706,\"start\":37703}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":62841734},\"end\":25383,\"start\":25020},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":219687798},\"end\":25751,\"start\":25385},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b2\"},\"end\":26009,\"start\":25753},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":211096730},\"end\":26419,\"start\":26011},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207930212},\"end\":26856,\"start\":26421},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":54457412},\"end\":27265,\"start\":26858},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":67876983},\"end\":27662,\"start\":27267},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202583690},\"end\":28082,\"start\":27664},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":211296567},\"end\":28492,\"start\":28084},{\"attributes\":{\"doi\":\"arXiv:2112.05588\",\"id\":\"b9\"},\"end\":28875,\"start\":28494},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":229718542},\"end\":29177,\"start\":28877},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13060737},\"end\":29561,\"start\":29179},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":58028915},\"end\":29955,\"start\":29563},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":226159227},\"end\":30392,\"start\":29957},{\"attributes\":{\"id\":\"b14\"},\"end\":30999,\"start\":30394},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":236087683},\"end\":31412,\"start\":31001},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3322503},\"end\":31822,\"start\":31414},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":131777414},\"end\":32095,\"start\":31824},{\"attributes\":{\"doi\":\"arXiv:2108.00352\",\"id\":\"b18\"},\"end\":32387,\"start\":32097},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11008755},\"end\":32711,\"start\":32389},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11558223},\"end\":33119,\"start\":32713},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":44096776},\"end\":33601,\"start\":33121},{\"attributes\":{\"doi\":\"arXiv:1608.08710\",\"id\":\"b22\"},\"end\":33859,\"start\":33603},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b23\"},\"end\":34113,\"start\":33861},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211532649},\"end\":34490,\"start\":34115},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":44085059},\"end\":34921,\"start\":34492},{\"attributes\":{\"doi\":\"arXiv:2201.08217\",\"id\":\"b26\"},\"end\":35181,\"start\":34923},{\"attributes\":{\"doi\":\"arXiv:2201.11692\",\"id\":\"b27\"},\"end\":35472,\"start\":35183},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b28\"},\"end\":35769,\"start\":35474},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":59222747},\"end\":36199,\"start\":35771},{\"attributes\":{\"id\":\"b30\"},\"end\":36422,\"start\":36201},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2930547},\"end\":36723,\"start\":36424},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":308212},\"end\":37173,\"start\":36725},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15926837},\"end\":37631,\"start\":37175},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206594692},\"end\":38005,\"start\":37633}]", "bib_title": "[{\"end\":25095,\"start\":25020},{\"end\":25454,\"start\":25385},{\"end\":26080,\"start\":26011},{\"end\":26486,\"start\":26421},{\"end\":26915,\"start\":26858},{\"end\":27333,\"start\":27267},{\"end\":27766,\"start\":27664},{\"end\":28132,\"start\":28084},{\"end\":28929,\"start\":28877},{\"end\":29225,\"start\":29179},{\"end\":29627,\"start\":29563},{\"end\":30020,\"start\":29957},{\"end\":30489,\"start\":30394},{\"end\":31076,\"start\":31001},{\"end\":31501,\"start\":31414},{\"end\":31887,\"start\":31824},{\"end\":32458,\"start\":32389},{\"end\":32748,\"start\":32713},{\"end\":33196,\"start\":33121},{\"end\":34173,\"start\":34115},{\"end\":34566,\"start\":34492},{\"end\":35837,\"start\":35771},{\"end\":36473,\"start\":36424},{\"end\":36794,\"start\":36725},{\"end\":37262,\"start\":37175},{\"end\":37677,\"start\":37633}]", "bib_author": "[{\"end\":25105,\"start\":25097},{\"end\":25113,\"start\":25105},{\"end\":25468,\"start\":25456},{\"end\":25477,\"start\":25468},{\"end\":25486,\"start\":25477},{\"end\":25816,\"start\":25808},{\"end\":25823,\"start\":25816},{\"end\":25835,\"start\":25823},{\"end\":25841,\"start\":25835},{\"end\":26090,\"start\":26082},{\"end\":26103,\"start\":26090},{\"end\":26114,\"start\":26103},{\"end\":26124,\"start\":26114},{\"end\":26494,\"start\":26488},{\"end\":26501,\"start\":26494},{\"end\":26507,\"start\":26501},{\"end\":26514,\"start\":26507},{\"end\":26526,\"start\":26514},{\"end\":26929,\"start\":26917},{\"end\":26940,\"start\":26929},{\"end\":26949,\"start\":26940},{\"end\":27353,\"start\":27335},{\"end\":27366,\"start\":27353},{\"end\":27380,\"start\":27366},{\"end\":27387,\"start\":27380},{\"end\":27394,\"start\":27387},{\"end\":27775,\"start\":27768},{\"end\":27783,\"start\":27775},{\"end\":27793,\"start\":27783},{\"end\":28143,\"start\":28134},{\"end\":28151,\"start\":28143},{\"end\":28159,\"start\":28151},{\"end\":28167,\"start\":28159},{\"end\":28176,\"start\":28167},{\"end\":28184,\"start\":28176},{\"end\":28191,\"start\":28184},{\"end\":28197,\"start\":28191},{\"end\":28585,\"start\":28577},{\"end\":28593,\"start\":28585},{\"end\":28601,\"start\":28593},{\"end\":28608,\"start\":28601},{\"end\":28617,\"start\":28608},{\"end\":28623,\"start\":28617},{\"end\":28629,\"start\":28623},{\"end\":28635,\"start\":28629},{\"end\":28643,\"start\":28635},{\"end\":28937,\"start\":28931},{\"end\":28944,\"start\":28937},{\"end\":28951,\"start\":28944},{\"end\":28960,\"start\":28951},{\"end\":29237,\"start\":29227},{\"end\":29246,\"start\":29237},{\"end\":29258,\"start\":29246},{\"end\":29267,\"start\":29258},{\"end\":29638,\"start\":29629},{\"end\":29648,\"start\":29638},{\"end\":30030,\"start\":30022},{\"end\":30036,\"start\":30030},{\"end\":30045,\"start\":30036},{\"end\":30052,\"start\":30045},{\"end\":30504,\"start\":30491},{\"end\":30512,\"start\":30504},{\"end\":30526,\"start\":30512},{\"end\":31086,\"start\":31078},{\"end\":31093,\"start\":31086},{\"end\":31099,\"start\":31093},{\"end\":31108,\"start\":31099},{\"end\":31510,\"start\":31503},{\"end\":31518,\"start\":31510},{\"end\":31527,\"start\":31518},{\"end\":31537,\"start\":31527},{\"end\":31547,\"start\":31537},{\"end\":31895,\"start\":31889},{\"end\":31902,\"start\":31895},{\"end\":31918,\"start\":31902},{\"end\":31926,\"start\":31918},{\"end\":32186,\"start\":32179},{\"end\":32193,\"start\":32186},{\"end\":32203,\"start\":32193},{\"end\":32473,\"start\":32460},{\"end\":32482,\"start\":32473},{\"end\":32492,\"start\":32482},{\"end\":32773,\"start\":32750},{\"end\":32782,\"start\":32773},{\"end\":32791,\"start\":32782},{\"end\":32803,\"start\":32791},{\"end\":33205,\"start\":33198},{\"end\":33221,\"start\":33205},{\"end\":33229,\"start\":33221},{\"end\":33649,\"start\":33643},{\"end\":33658,\"start\":33649},{\"end\":33672,\"start\":33658},{\"end\":33681,\"start\":33672},{\"end\":33691,\"start\":33681},{\"end\":33924,\"start\":33916},{\"end\":33931,\"start\":33924},{\"end\":33943,\"start\":33931},{\"end\":33949,\"start\":33943},{\"end\":34182,\"start\":34175},{\"end\":34202,\"start\":34182},{\"end\":34220,\"start\":34202},{\"end\":34232,\"start\":34220},{\"end\":34577,\"start\":34568},{\"end\":34583,\"start\":34577},{\"end\":34591,\"start\":34583},{\"end\":34988,\"start\":34982},{\"end\":34995,\"start\":34988},{\"end\":35004,\"start\":34995},{\"end\":35011,\"start\":35004},{\"end\":35274,\"start\":35266},{\"end\":35280,\"start\":35274},{\"end\":35289,\"start\":35280},{\"end\":35555,\"start\":35547},{\"end\":35562,\"start\":35555},{\"end\":35568,\"start\":35562},{\"end\":35574,\"start\":35568},{\"end\":35582,\"start\":35574},{\"end\":35848,\"start\":35839},{\"end\":35854,\"start\":35848},{\"end\":35862,\"start\":35854},{\"end\":35870,\"start\":35862},{\"end\":35883,\"start\":35870},{\"end\":35893,\"start\":35883},{\"end\":36270,\"start\":36256},{\"end\":36280,\"start\":36270},{\"end\":36490,\"start\":36475},{\"end\":36498,\"start\":36490},{\"end\":36504,\"start\":36498},{\"end\":36806,\"start\":36796},{\"end\":36812,\"start\":36806},{\"end\":36819,\"start\":36812},{\"end\":37277,\"start\":37264},{\"end\":37291,\"start\":37277},{\"end\":37301,\"start\":37291},{\"end\":37309,\"start\":37301},{\"end\":37685,\"start\":37679},{\"end\":37694,\"start\":37685},{\"end\":37701,\"start\":37694},{\"end\":37708,\"start\":37701}]", "bib_venue": "[{\"end\":25175,\"start\":25113},{\"end\":25529,\"start\":25486},{\"end\":25806,\"start\":25753},{\"end\":26174,\"start\":26124},{\"end\":26590,\"start\":26526},{\"end\":27013,\"start\":26949},{\"end\":27430,\"start\":27394},{\"end\":27836,\"start\":27793},{\"end\":28245,\"start\":28197},{\"end\":28575,\"start\":28494},{\"end\":29000,\"start\":28960},{\"end\":29325,\"start\":29267},{\"end\":29713,\"start\":29648},{\"end\":30125,\"start\":30052},{\"end\":30629,\"start\":30526},{\"end\":31165,\"start\":31108},{\"end\":31583,\"start\":31547},{\"end\":31937,\"start\":31926},{\"end\":32177,\"start\":32097},{\"end\":32525,\"start\":32492},{\"end\":32867,\"start\":32803},{\"end\":33307,\"start\":33229},{\"end\":33641,\"start\":33603},{\"end\":33914,\"start\":33861},{\"end\":34268,\"start\":34232},{\"end\":34656,\"start\":34591},{\"end\":34980,\"start\":34923},{\"end\":35264,\"start\":35183},{\"end\":35545,\"start\":35474},{\"end\":35943,\"start\":35893},{\"end\":36254,\"start\":36201},{\"end\":36544,\"start\":36504},{\"end\":36896,\"start\":36819},{\"end\":37361,\"start\":37309},{\"end\":37772,\"start\":37708},{\"end\":25564,\"start\":25531},{\"end\":26220,\"start\":26176},{\"end\":26650,\"start\":26592},{\"end\":27073,\"start\":27015},{\"end\":27462,\"start\":27432},{\"end\":27871,\"start\":27838},{\"end\":28289,\"start\":28247},{\"end\":29379,\"start\":29327},{\"end\":29774,\"start\":29715},{\"end\":30194,\"start\":30127},{\"end\":30728,\"start\":30631},{\"end\":31218,\"start\":31167},{\"end\":31615,\"start\":31585},{\"end\":32927,\"start\":32869},{\"end\":33381,\"start\":33309},{\"end\":34300,\"start\":34270},{\"end\":34717,\"start\":34658},{\"end\":35989,\"start\":35945},{\"end\":36969,\"start\":36898},{\"end\":37409,\"start\":37363},{\"end\":37832,\"start\":37774}]"}}}, "year": 2023, "month": 12, "day": 17}