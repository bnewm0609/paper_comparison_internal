{"id": 254974187, "updated": "2023-10-05 06:34:18.375", "metadata": {"title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation", "authors": "[{\"first\":\"Jay\",\"last\":\"Wu\",\"middle\":[\"Zhangjie\"]},{\"first\":\"Yixiao\",\"last\":\"Ge\",\"middle\":[]},{\"first\":\"Xintao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Weixian\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Yuchao\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Yufei\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Wynne\",\"last\":\"Hsu\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Shan\",\"middle\":[]},{\"first\":\"Xiaohu\",\"last\":\"Qie\",\"middle\":[]},{\"first\":\"Mike\",\"last\":\"Shou\",\"middle\":[\"Zheng\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting$\\unicode{x2014}$One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.11565", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2212-11565", "doi": "10.48550/arxiv.2212.11565"}}, "content": {"source": {"pdf_hash": "1367dcff4ccb927a5e95c452041288b3f0dd0eff", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.11565v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "59e0254d60282c7ab11f20d66f54040be9a166f0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1367dcff4ccb927a5e95c452041288b3f0dd0eff.txt", "contents": "\nTune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation\n\n\nJay Zhangjie Wu \nShow Lab\nNational University of Singapore\n\n\nYixiao Ge \nARC Lab\n\n\nXintao Wang \nARC Lab\n\n\nStan Weixian Lei \nShow Lab\nNational University of Singapore\n\n\nYuchao Gu \nShow Lab\nNational University of Singapore\n\n\nYufei Shi \nShow Lab\nNational University of Singapore\n\n\nWynne Hsu \nSchool of Computing\nNational University of Singapore\n\n\nYing Shan \nARC Lab\n\n\nXiaohu Qie \nTencent PCG\n\n\nMike Zheng Shou \nShow Lab\nNational University of Singapore\n\n\nTune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation\nPretrained T2I\n\n\ntiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatiotemporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.\n\n\nIntroduction\n\nThe large-scale multimodal dataset [41], consisting of billions of text-image pairs crawled from the Internet, has enabled a breakthrough in Text-to-Image (T2I) generation [30,35,6,42,40]. To replicate this success in Text-to-Video (T2V) generation, recent works [42,15,18,53,47] have extended spatial-only T2I generation models to the spatio-temporal domain. These models generally adopt the standard paradigm of training on large-scale text-video datasets (e.g., WebVid-10M [2]). Although this paradigm produces promising results for T2V generation, it requires extensive training on large hardware accelerators, which is expensive and time-consuming.\n\nHumans possess the ability to create new concepts, ideas, or things by utilizing their existing knowledge and the information provided to them. For example, when presented a video with a textual description of \"a man skiing on snow\", we can imagine how a panda would ski on snow, drawing upon our knowledge of what a panda looks like. As T2I models pretrained with large-scale image-text data already capture knowledge of open-domain concepts, a intuitive question arises: can they infer other novel videos from a single video example, like humans? A new T2V generation setting is therefore introduced, namely, One-Shot Video Tuning, where only a single text-video pair is used to train a T2V generator. The generator is expected to capture essential motion information from the input video and synthesize novel videos with edited prompts.\n\nIntuitively, the key to successful video generation lies in preserving the continuous motion of consistent objects. So we make the following observations on state-of-the-art T2I diffusion models [37] that motivate our method accordingly.\n\n(1) Regarding motion: T2I models are able to generate images that align well with the text, including the verb terms. For example, given the text prompt \"a man is running on the beach\", the T2I models produce the snapshot where a man is running (not walking or jumping), albeit not necessarily in a continuous manner (the first row of Fig. 2). This serves as evidence that T2I models can properly attend to verbs via cross-modal attention for static motion generation. (2) Regarding consistent objects: Simply extending the spatial self-attention in the T2I model from one image to multiple images produces consistent content across frames. Taking \"A man is running on the beach\" ! spatial selfattention spatio-temporal attention Figure 2: Observations on pretrained T2I models: 1) They can generate still images that accurately represent the verb terms. 2) Extending spatial self-attention to spatio-temporal attention produces consistent content across frames.\n\" # $ ! \" # $ ! \" # $ ! \" # $ ! \" # $\nthe same example, when we generate consecutive frames in parallel with extended spatio-temporal attention, the same man and the same beach can be observed in the resultant sequence though the motion is still not continuous (the second row of Fig. 2). This implies that the self-attention layers in T2I models are only driven by spatial similarities rather than pixel positions. We implement our findings into a simple yet effective method called Tune-A-Video. Our method is based on a simple inflation of state-of-the-art T2I models over spatiotemporal dimension. However, using full attention in spacetime inevitably leads to quadratic growth in computation. It is thus infeasible for generating videos with increasing frames. Additionally, employing a naive fine-tuning strategy that updates all the parameters can jeopardize the preexisting knowledge of T2I models and hinder the generation of videos with new concepts. To tackle these problems, we introduce a sparse spatio-temporal attention mechanism that only visits the first and the former video frame, as well as an efficient tuning strategy that only updates the projection matrices in attention blocks. Empirically, these designs maintain consistent objects across all frames but lack continuous motion. Therefore, at inference, we further seek structure guidance from input video through DDIM inversion, which is a reverse process of DDIM sampling [43]. With the inverted latent as initial noise, we produce temporallycoherent videos featuring smooth movement. Notably, our method is inherently compatible with exiting personalized and conditional pretrained T2I models, such as Dream-Booth [39] and T2I-Adapter [29], providing a personalized and controllable user interface.\n\nWe showcase remarkable results of Tune-A-Video across a wide range of applications for text-driven video generation (see Fig. 1). We compare our method against the stateof-the-art baselines through extensive qualitative and quantitative experiments, demonstrating its superiority. In summary, our key contributions are as follows:\n\n\u2022 We introduce a new setting of One-Shot Video Tuning for T2V generation, which eliminates the burden of training with large-scale video datasets.\n\n\u2022 We present Tune-A-Video, which is the first framework for T2V generation using pretrained T2I models.\n\n\u2022 We propose efficient attention tuning and structural inversion that significant improve temporal consistency.\n\n\u2022 We demonstrate remarkable results of our method through extensive experiments.\n\n\nRelated Work\n\nOur work lies in the intersection of several fields: diffusion models and methods for image/video generation from text prompts, text-driven editing of a real image/video, and generative models trained on a single video. Here we provide a brief overview of the key accomplishments in each field, highlighting their connections and differences from our proposed method.\n\nText-to-Image diffusion models. Text-to-Image (T2I) generation has been studied extensively, in past years many of the models were based on transformers [36,51,50,6,9]. Several T2I generative models [30,40,10,37] have recently adopted diffusion models [16]. GLIDE [30] proposes classifier-free guidance [17] in the diffusion model to improve image quality, while DALLE-2 [35] improves text-image alignments using CLIP [34] feature space. Imagen [40] uses cascaded diffusion models for high definition video generation, and subsequent works like VQdiffusion [10] and Latent Diffusion Models (LDMs) [37] operate in the latent space of an autoencoder to improve training efficiency. Our method builds on LDMs, by inflating the 2D model to spatio-temporal domain in latent space.\n\nText-to-Video generative models. While there have been significant advancements in T2I generation, generating videos from text is still lagging behind due to the scarcity of high-quality, large-scale text-video datasets, and the inherent complexity of modeling temporal consistency and coherence. Early works [27,32,25,23,11,24] primarily focus on generating videos in simple domains, such as moving digits or specific human actions. Recently, GODIVA [45] is the first model to utilize 2D VQ-VAE and sparse attention for T2V generation, which allows for more realistic scenes. N\u00dcWA [48] expands upon GODIVA by presenting a unified representation for various generation tasks through a multitask learning approach. To further enhance T2V generation performance, CogVideo [19] is developed by incorporating additional temporal attention modules on top of a pre-trained T2I model, CogView2 [6].\n\nTo replicate the success of T2I diffusion models, Video Diffusion Models (VDM) [18] uses a space-time factorized U-Net with joint image and video data training. Imagen Video [15] improves VDM using cascaded diffusion models and v-prediction parameterization to generate high definition videos. Make-A-Video [42] and MagicVideo [53] share similar motivations and aim to transfer progress from T2I generation to T2V generation. Although current T2V generative models have shown impressive results, their success heavily rely on being trained using extensive video data. In contrast, we present a new framework for T2V generation via an efficient tuning of pre-trained T2I diffusion models on one text-video pair.\n\nText-driven video editing. Recent diffusion-based image editing models [26,14,5,49,21,44] can process each individual frame in a video, but this produces inconsistency between frames due to the lack of temporal awareness in the model. Text2Live [3] allows some texture-based video editing using text prompts, but struggles to accurately reflect the intended edits due to its dependence on Layered Neural Atlases [20]. Moreover, generating a neural atlas typically takes about 10 hours, whereas our approach only requires a 10-minute training per video and can sample a video in just 1 minute. Two concurrent works, Dreamix [28] and Gen-1 [7], both utilize the video diffusion model (VDM) for video editing purposes. Although their impressive outcomes, it is worth noting that the VDMs are computationally demanding and necessitate large-scale captioned images and videos for training. Additionally, their training data and pre-trained models are not publicly accessible.\n\nGeneration from a single video. Single-video GANs [1,12] generate new videos of similar appearance and dynamics to the input video. However, these GAN-based methods are limited in computation time (e.g., HPVAE-GAN [12] takes 8 days to train on a short video of 13 frames), and thus are impractical and unscalable to some extent. Patch nearest-neighbour methods [13] perform video generation of higher quality while reducing computation expense by orders of magnitude. However, they are limited in generalization, and therefore can only handle tasks where it is natural to \"copy\" parts of the input video. Lately, SinFusion [31] adapts diffusion models to single-video tasks, and enables autoregressive video generation with improved motion generalization capabilities; however, it is still incapable of producing videos that contains novel semantic contexts.\n\n\nMethod\n\nLet V = {v i |i \u2208 [1, m]} be a video containing m frames, P be the source prompt describing V. Our goal is to generate a novel video V * driven by an edited text prompt P * . For example, consider a video and a source prompt \"a man is skiing\", and assume that the user wants to alter the color of the clothes, incorporate a cowboy hat to the skier, or even replace the skier with Spider Man while preserving the motion of the original video. The user can directly modify the Inference \"Spider Man is skiing on the beach, cartoon style\"\n\nText-to-Image\n\n\nInput\n\n\nText-to-Video\n\nOutput \"A man is skiing\"\n\n\nFine-Tuning\n\n\nTune-A-Video\n\nText-to-Video Figure 3: High-level overview of Tune-A-Video. Given a captioned video, we finetune a pre-trained T2I model (e.g., Stable Diffusion) for T2V modeling. During inference, we generate novel videos that represent the edits in text prompt while preserving the temporal consistency of input video.\n\nsource prompt by further describing the appearance of the skier or replacing it with another word. An intuitive solution is to train a T2V model on largescale video datasets, but it is computationally expensive [42,15,7]. In this paper, we propose a new setting called One-Shot Video Tuning that achieves the same goal using a publicly available T2I model and a single text-video pair.\n\nNext, we provide a short background of diffusion models in Sec. 3.1, followed by a detailed description of our method in Sec. 3.2 and Sec. 3.3. An overview of our approach is depicted in Fig. 3.\n\n\nPreliminaries\n\nDenoising diffusion probabilistic models (DDPMs). DDPMs [16] are latent generative models trained to recreate a fixed forward Markov chain x 1 , . . . , x T . Given the data distribution x 0 \u223c q(x 0 ), the Markov transition q(x t |x t\u22121 ) is defined as a Gaussian distribution with a variance schedule \u03b2 t \u2208 (0, 1), that is,\nq(xt|xt\u22121) = N (xt; 1 \u2212 \u03b2txt\u22121, \u03b2tI), t = 1, . . . , T.\nBy the Bayes' rules and Markov property, one can explicitly express the conditional probabilities q(x t |x 0 ) and\nq(x t\u22121 |x t , x 0 ) as q(xt|x0) = N (xt; \u221a\u1fb1 tx0, (1 \u2212\u1fb1t)I), t = 1, . . . , T, q(xt\u22121|xt, x0) = N (xt\u22121;\u03bct(xt, x0),\u03b2tI), t = 1, . . . , T, w.r.t. \u03b1t = 1 \u2212 \u03b2t,\u1fb1t = t s=1 \u03b1s,\u03b2t = 1 \u2212\u1fb1t\u22121 1 \u2212\u1fb1t \u03b2t, \u00b5t(xt, x0) = \u221a\u1fb1 t\u03b2t 1 \u2212\u1fb1t x0 + \u221a \u03b1t(1 \u2212\u1fb1t\u22121) 1 \u2212\u1fb1t xt.\nTo generate the Markov chain x 1 , . . . , x T , DDPMs leverage the reverse process with a prior distribution p(x T ) = N (x T ; 0, I) and Gaussian transitions p \u03b8 (xt\u22121|xt) = N (xt\u22121; \u00b5 \u03b8 (xt, t), \u03a3 \u03b8 (xt, t)), t = T, . . . , 1.\n\nLearnable parameters \u03b8 are trained to guarantee that the generated reverse process is close to the forward process.\n\nTo this end, DDPMs follow the variational inference principle by maximizing the variational lower bound of the negative log-likelihood, which has a closed-form given the KL divergence among Guassian distributions. Empirically, these models can be interpreted as a sequence of weightsharing denoising autoencoders \u03b8 (x t , t), which are trained to predict a denoised variant of their input x t . The objective can be simplified as E x, \u223cN (0,1),t \u2212 \u03b8 (x t , t) 2 2 . Latent diffusion models (LDMs). LDMs [37] are newly introduced variants of DDPMs that operate in the latent space of an autoencoder. LDMs consist of two key components. First, an autoencoder [8,45] is trained with patch-wise losses on a large collection of images, where an encoder E learns to compress images x into latent representations z = E(x), and a decoder D learns to reconstruct the latent back to pixel space, such that D(E(x)) \u2248 x. The second component is a DDPM that is trained to remove the noise added to the sampled data. For a text-guided LDM, the objective is given by:\nE z, \u223cN (0,1),t,c \u2212 \u03b8 (z t , t, c) 2 2 ., where c = \u03c8(P * )\nis the embedding of textual condition P * .\n\n\nNetwork Inflation\n\nA T2I diffusion model (e.g., LDM [37]) typically employs a U-Net [38], which is a neural network architecture based on a spatial downsampling pass followed by an upsampling pass with skip connections. It is composed of stacked 2D convolutional residual blocks and transformer blocks. Each transformer block consists of a spatial self-attention layer, a cross-attention layer, and a feed-forward network (FFN). The spatial self-attention leverages pixel locations in feature maps for similar correlation, while the cross-attention considers correspondence between pixels and conditional inputs (e.g., text). Formally, given latent representation z vi of video frame v i , the spatial self-attention mechanism [46] implements\nAttention(Q, K, V ) = Softmax( QK T \u221a d ) \u00b7 V , with Q = W Q zv i , K = W K zv i , V = W V zv i ,\nwhere W Q , W K , and W V are learnable matrices that project the inputs to query, key and value, respectively, and and d is the output dimension of key and query features.\n\nWe extend a 2D LDM to the spatio-temporal domain. Similar to VDM [18], we inflate the 2D convolution layers to pseudo 3D convolution layers, with 3 \u00d7 3 kernels being replaced by 1 \u00d7 3 \u00d7 3 kernels and append a temporal self-attention layer in each transformer block for temporal modeling. To enhance the temporal coherence, Fine-Tuning Inference Figure 4: Pipeline of Tune-A-Video: Given a text-video pair (e.g., \"a man is skiing\") as input, our method leverages the pretrained T2I diffusion models for T2V generation. During fine-tuning, we update the projection matrices in attention blocks using the standard diffusion training loss. During inference, we sample a novel video from the latent noise inverted from the input video, guided by an edited prompt (e.g., \"Spider Man is surfing on the beach, cartoon style\"). we further extend the spatial self-attention mechanism to the spatio-temporal domain. There are alternative options for spatio-temporal attention (ST-Attn) mechanism, including full attention and causal attention which also capture spatio-temporal consistency. However, such straightforward choices are actually not feasible in generating videos with increasing frames due to their high computational complexity. Specifically, given m frames and N sequences for each frame, the complexity for both full attention and causal attention is O((mN ) 2 ). It is not affordable if we need to generate long videos with a large value of m.\n\nHere, we propose to use a sparse version of causal attention mechanism, where the attention matrix are computed between frame z vi and two previous frames z v1 and z vi\u22121 , remaining low computational complexity at O(2m(N ) 2 ). Specifically, we derive query feature from frame z vi , key and value features from the first frame z v1 and the former frame z vi\u22121 , and implement Attention(Q, K, V ) with\nQ = W Q zv i , K = W K zv 1 , zv i\u22121 , V = W V zv 1 , zv i\u22121 ,\nwhere [\u00b7] denotes concatenation operation. Note that the projection matrices W Q , W K , and W V are shared across space and time. See Fig. 5 for a visual depiction.\n\n\nFine-Tuning and Inference\n\nModel fine-tuning. We now finetune our network on the given input video for temporal modeling. The spatiotemporal attention (ST-Attn) is designed to model temporal consistency by querying relevant positions in previous frames. Therefore, we propose to fix parameters W K and W V , and only update W Q in ST-Attn layers. In contrast, we finetune the entire temporal self-attention (T-Attn) layers as they are newly added. Moreover, we propose to refine the text-video alignment by updating the query projection in cross-attention (Cross-Attn). In practice, finetuning the attention blocks is computationally efficient compared to full tuning [39], and meanwhile retains the original property of pre-trained T2I diffusion models. We use the same training objective in standard LDMs [37]. Fig. 4 illustrates the finetuning process with the trainable parameters highlighted.\n\nStructure guidance via DDIM inversion. Finetuning the attention layers is essential to ensure spatial consistency across all frames. However, it does not offer much control over pixel shifts, resulting in stagnant videos in the loop. To tackle this problem, we incorporate structure guidance from the source video during the inference stage. Specifically, we \"Spider Man is skiing on the beach, cartoon style\" \"A man is skiing\" \"A rabbit is eating a watermelon on the table\" \"A puppy is eating a cheeseburger on the table, comic style\" \"A rabbit is eating a watermelon on the table\"\n\n\"A cat with sunglasses is eating a watermelon on the beach\" \"Wonder Woman, wearing a cowboy hat, is skiing\" \"A man, wearing pink clothes, is skiing at sunset\" Note that for the same input video, we only need to perform DDIM inversion once. Our experiments demonstrate its effectiveness in accurately conveying the structural movements from the source video to the generated videos.\n\n\nApplications of Tune-A-Video\n\nWe showcase several applications of our Tune-A-Video for text-driven video generation and editing.\n\nObject editing. One of the major applications of our method is to modify the object through the editing of text prompts. This allows replacing, adding, or removing objects with ease. Fig. 6 shows some examples. We can replace \"a man\" with \"Spider Man\" or \"Wonder Woman\", \"a rabbit\" with \"a cat\" or \"a puppy\", or even switch out \"a watermelon\" for \"a cheeseburger\", simply by modifying the corresponding words. We can add an object such as \"a cowboy hat\" or \"sunglasses\" by further describing it in the prompt. To remove an object, we can easily delete the corresponding phrase-for example, the watermelon.\n\nBackground change. Our method also enables users to change the video background (i.e., the place where the object is), while preserving the consistency of the object's movements. For example, we can modify the background of the skiing man in Fig. 6 to be \"on the beach\" or \"at sunset\", by adding a new location/time description, and change the countryside road view in Fig. 7 to sea view, by replacing an existing location description.\n\n\nStyle transfer.\n\nThanks to the open-domain knowledge of pretrained T2I models, our method transfer videos into a variety of styles that are difficult to learn solely from video data [42]. For example, we transform real-world videos into comic styles (Fig. 6), or Van Gogh style (Fig. 10), by appending the global style descriptor to the prompt.\n\nPersonalized and controllable generation. Our method can be easily integrated with personalized T2I models (e.g., DreamBooth [39], which takes 3-5 images as input and returns a personalized T2I model), by directly finetuing on them. For instance, we can use a DreamBooth personalized for \"Modern Disney Style\" or \"Mr Potato Head\" to create videos of a specific style or subject (Fig. 11). Our method can also be integrated with conditional T2I models like T2I-Adapter [29] and ControlNet [52], to enable diverse controls on the generated videos at no extra training cost. For example, we can further edit the motion using a sequence of human pose as control (e.g., dancing in Fig. 1).\n\n\"A Porsche car is moving on the beach\"\n\n\nTune-A-Video\n\n\nText2LIVE\n\nPlug-and-Play CogVideo \"A jeep car is moving on road, cartoon style\" Input Video \"A car is moving on the road\" Note that the human pose sequence can be automatically detected from real-world videos using an off-the-shelf pose estimation model [4]. The compatibility of our method with personalized and conditional T2I models offers more possibilities for users to create the video content they desire.\n\n\nExperiments\n\n\nImplementation Details\n\nOur development is based on Latent Diffusion Models [37] (a.k.a Stable Diffusion) and the public pretrained weights 1 . We sample 32 uniform frames at resolution of 512 \u00d7 512 from input video, and finetune the models with our method for 500 steps on a learning rate 3 \u00d7 10 \u22125 and a batch size 1. At inference, we use DDIM sampler [43] with classifier-free guidance [17] in our experiments. For a single video, it takes about 10 minutes for finetuning, and about 1 minute for sampling on a NVIDIA A100 GPU.\n\n\nBaseline Comparisons\n\nDataset. To evaluate our approach, we use 42 representative videos taken from DAVIS dataset [33]. We auto-1 https://huggingface.co/CompVis/stable-diffusion-v1-4 matically produce the video footage using an off-the-shelf captioning model [22], and manually design 140 edited prompts across our applications in Sec. 4. More details on our benchmark are provided in Sec. A.\n\nBaselines. We compare our method against three baselines: 1) CogVideo [19]: a T2V model trained on a dataset of 5.4 million captioned videos, and is capable of generating videos directly from text prompts in a zero-shot manner. 2) Plug-and-Play [44]: a cutting-edge image editing model that can edit each frame of a video individually. 3) Text2LIVE [3]: a recent approach for text-guided video editing that employs layered neural atlases [20].\n\nQualitative results. We present a visual comparison of our approach against several baselines in Fig. 7. We observe that while CogVideo can produce videos that reflect the general concept in the text, the output videos varies a lot in quality and it cannot take a video as input. Plug-and-Play, on the other hand, successfully edits each video frame individually, but lacks frame consistency as the temporal context is neglected (e.g., the appearance of the Porsche car is not consistent across frames). Text2LIVE, while capable of producing temporally smooth videos, struggles to ac-  curately represent the edited prompt (e.g., the Porsche car still appears in the shape of the original jeep car). This may be due to its reliance on layered neural atlases, which restricts its editing ability. In contrast, our method generates temporally-coherent videos that preserve structural information from the input video and align well with edited words and details. Additional qualitative comparison can be found in Fig. 12.\n\nQuantitative results. We quantify our method against baselines through automatic metrics and user study, and report frame consistency and textual faithfulness in Tab. 1. Automatic metrics. For frame consistency, we compute CLIP [34] image embeddings on all frames of output videos and report the average cosine similarity between all pairs of video frames. To measure textual faithfulness, we compute average CLIP score between all frames of output videos and corresponding edited prompts. Our results indicate that CogVideo produces consistent video frames but struggle to represent the textual description, whereas Plug-and-Play achieves high textual faithfulness but failed to generate consistent content. In contrast, our method outperforms baselines in both metrics. \"lions\" \"pandas\" User study. For frame consistency, we present two videos generated by our method and a baseline in random order and ask the raters \"which one has better temporal consistency?\". For textual faithfulness, we additionally show the textual description and ask the raters \"which video better aligns with the textual description?\". We recruit 5 participants to annotate each example and use a majority vote for the final result. Additional details are provided in Appendix (Sec. B). We observe that CogVideo and Plug-and-Play are less preferred due to frame-wise and frame-text inconsistency, whereas our method achieves higher user preference in both aspects.\n\n\nAblation Study\n\nWe conduct an ablation study to assess the importance of the spatio-temporal attention (ST-Attn) mechanism, DDIM inversion, and finetuning in our Tune-A-Video. Each design is individually ablated to analyze its impact. The results, presented in Fig. 8, show that the model w/o ST-Attn displays significant content discrepancies (evident from the skier's clothing color). In contrast, the model w/o inversion maintains consistent content but fails to replicate the motion (i.e., skiing) in the input video. Thanks to the ST-Attn and inversion, model w/o finetuning still suffices consistent content across frames. However, the motion in consecutive frames is not smooth, resulting in flickering videos. Additional video examples of ablation study can be found in Fig. 13. These results indicate that all of our key designs contribute to the successful results of our method. Fig. 9 presents a failure case of our method when the input video contains multiple objects and exhibits occlusion. This may be due to the inherent limitation of the T2I model in handling multiple objects and object interactions. A potential solution is to use additional conditional information, such as depth, to enable the model to differentiate between different objects and their interactions. This avenue of research is left as future work.\n\n\nLimitations and Future Work\n\n\nConclusion\n\nIn this paper, we introduce a new task for T2V generation called One-Shot Video Tuning. This task involves training a T2V generator using only a single text-video pair and pretrained T2I models. We present Tune-A-Video, a simple yet effective framework for text-driven video generation and editing. To generate continuous videos, we propose an efficient tuning strategy and structural inversion that enable generating temporally-coherent videos. Extensive experiments demonstrate the remarkable results of our method spanning a wide range of applications.\n\n\"A bear is walking on the snow\" \"A lion is roaring\" \"A wolf is roaring in New York City\" \"A lion is roaring, Van Gogh style\" \"A bear is walking on some rocks, cartoon style\" \"A tiger is roaring\" \n\nFigure 5 :\n5Illustration of our ST-Attn: Latent features of frame v i , previous frames v i\u22121 and v 1 are projected to query Q, key K and value V . Output is a weighted sum of the values, weighted by the similarity between the query and key features. We highlight the updated parameter W Q .\n\nFigure 6 :\n6Sample results of our method.obtain a latent noise of source video V through DDIM inversion with no textual condition. This noise serves as the starting point for DDIM sampling, which is guided by an edited prompt T * . The output video V * is then given by V * = D(DDIM-samp(DDIM-inv(E(V)), T * )).\n\nFigure 7 :\n7Qualitative comparison between evaluated methods. Zoom in for best view.\n\nFigure 8 :\n8Ablation study. Input video is shown inFig .6.\n\nFigure 9 :\n9Limitations: Our method might produce unpleasant results when the input video contains multiple objects and exhibits occlusions. For example, the two pandas at the bottom being mixed together.\n\nFigure 10 :Figure 12 :Figure 13 :\n101213Additional sample results of our method (1/2). Additional qualitative comparsion between evaluated methods. \"A jeep car is moving on the beach\" Additional ablation study.\n\nTable 1 :\n1Quantitative comparison with evaluated baselines. * indicates Tune-A-Video vs. CogVideo, ** indicates Tune-A-Video vs. Plug-and-Play.\"An astronaut is skiing on the moon\"Method \nFrame Consisitency \nTextual alignment \nCLIP Score User Preference \nCLIP Score User Preference \nCogVideo \n90.64 \n12.14 \n23.91 \n15.00 \nPlug-and-Play \n88.89 \n37.86 \n27.56 \n23.57 \nTune-A-Video \n92.40 \n87.86* / 62.14** \n27.58 \n85.00* / 76.43** \n\n\nAppendix A. Dataset DetailsWe select 42 videos from the DAVIS dataset[33], covering a range of categories including animals, vehicles, and humans. The selected video items are listed in Tab. 2. To obtain video footage, we use BLIP-2[22]for automated captions. We then manually design three edited prompts for each video, resulting 140 edited prompts in total. These edited prompts include object editing, background changes, and style transfers, as described in Sec. 4.B. User Study DetailsWe conduct a user study on our dataset of 140 edited prompts to compare our method against two baselines: Plug-and-Play[44]and CogVideo[19]. The comparison results are shown in Tab. 1. The participants of the user study are mainly students and colleagues in university. We ask 5 raters to evaluate each edited prompt by comparing two videos generated by two different methods (shown in random order) and answering two following questions:1. Which video has higher consistency? Please select the one that looks more smooth as a video.2. Which video matches the text better? Please select the one that better represents the given text description.C. Additional Results\nSingan-gif: Learning a generative video model from a single gif. Rajat Arora, Yong Jae Lee, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionRajat Arora and Yong Jae Lee. Singan-gif: Learning a gen- erative video model from a single gif. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1310-1319, 2021. 3\n\nG\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. Max Bain, Arsha Nagrani, ICCV. Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisser- man. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, pages 1728-1738, 2021. 2\n\nText2live: Text-driven layered image and video editing. Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, Tali Dekel, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringer37Proceedings, Part XVOmer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas- ten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, pages 707-723. Springer, 2022. 3, 7\n\nRealtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7291-7299, 2017. 7\n\nDiffedit: Diffusion-based semantic image editing with mask guidance. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord, arXiv:2210.114272022arXiv preprintGuillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based seman- tic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. 3\n\nCogview2: Faster and better text-to-image generation via hierarchical transformers. Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang, arXiv:2204.1421723arXiv preprintMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hi- erarchical transformers. arXiv preprint arXiv:2204.14217, 2022. 2, 3\n\nStructure and content-guided video synthesis with diffusion models. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis, arXiv:2302.0301134arXiv preprintPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. 3, 4\n\nTaming transformers for high-resolution image synthesis. Patrick Esser, Robin Rombach, Bjorn Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021. 4\n\nMake-a-scene: Scenebased text-to-image generation with human priors. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringer2022Proceedings, Part XVOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene- based text-to-image generation with human priors. In Com- puter Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XV, pages 89-106. Springer, 2022. 3\n\nVector quantized diffusion model for text-to-image synthesis. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo, CVPR. 2022Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec- tor quantized diffusion model for text-to-image synthesis. In CVPR, pages 10696-10706, 2022. 3\n\nImagine this! scripts to compositions to videos. Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, Aniruddha Kembhavi, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to composi- tions to videos. In Proceedings of the European conference on computer vision (ECCV), pages 598-613, 2018. 3\n\nHierarchical patch vae-gan: Generating diverse videos from a single sample. Shir Gur, Sagie Benaim, Lior Wolf, Advances in Neural Information Processing Systems. 33Shir Gur, Sagie Benaim, and Lior Wolf. Hierarchical patch vae-gan: Generating diverse videos from a single sam- ple. Advances in Neural Information Processing Systems, 33:16761-16772, 2020. 3\n\nDiverse generation from a single video made possible. Niv Haim, Ben Feinstein, Niv Granot, Assaf Shocher, Shai Bagon, Tali Dekel, Michal Irani, European Conference on Computer Vision. Springer2022Niv Haim, Ben Feinstein, Niv Granot, Assaf Shocher, Shai Bagon, Tali Dekel, and Michal Irani. Diverse generation from a single video made possible. In European Conference on Computer Vision, pages 491-509. Springer, 2022. 3\n\nPrompt-to-prompt image editing with cross attention control. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or, arXiv:2208.016262022arXiv preprintAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- age editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3\n\nImagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Fleet, arXiv:2210.0230324arXiv preprintJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion mod- els. arXiv preprint arXiv:2210.02303, 2022. 2, 3, 4\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. 334Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020. 3, 4\n\nClassifier-free diffusion guidance. Jonathan Ho, Tim Salimans, arXiv:2207.1259837arXiv preprintJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3, 7\n\n. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet, arXiv:2204.0345824Video diffusion modelsJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video dif- fusion models. arXiv:2204.03458, 2022. 2, 3, 4\n\nCogvideo: Large-scale pretraining for text-to-video generation via transformers. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang, arXiv:2205.15868712arXiv preprintWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 3, 7, 12\n\nLayered neural atlases for consistent video editing. Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel, ACM Transactions on Graphics (TOG). 4067Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay- ered neural atlases for consistent video editing. ACM Trans- actions on Graphics (TOG), 40(6):1-12, 2021. 3, 7\n\nImagic: Text-based real image editing with diffusion models. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani, arXiv:2210.092762022arXiv preprintBahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276, 2022. 3\n\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597712arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 7, 12\n\nVideo generation from text. Yitong Li, Martin Min, Dinghan Shen, David Carlson, Lawrence Carin, AAAI. 32Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In AAAI, vol- ume 32, 2018. 3\n\nCrossmodal dual learning for sentence-to-video generation. Yue Liu, Xin Wang, Yitian Yuan, Wenwu Zhu, Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaYue Liu, Xin Wang, Yitian Yuan, and Wenwu Zhu. Cross- modal dual learning for sentence-to-video generation. In Proceedings of the 27th ACM International Conference on Multimedia, pages 1239-1247, 2019. 3\n\nAttentive semantic video generation using captions. Tanya Marwah, Gaurav Mittal, N Vineeth, Balasubramanian, ICCV. Tanya Marwah, Gaurav Mittal, and Vineeth N Balasubrama- nian. Attentive semantic video generation using captions. In ICCV, pages 1426-1434, 2017. 3\n\nSdedit: Guided image synthesis and editing with stochastic differential equations. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon, International Conference on Learning Representations. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equa- tions. In International Conference on Learning Representa- tions, 2021. 3\n\nSync-draw: Automatic video generation using deep recurrent attentive architectures. Gaurav Mittal, Tanya Marwah, N Vineeth, Balasubramanian, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaGaurav Mittal, Tanya Marwah, and Vineeth N Balasubrama- nian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In Proceedings of the 25th ACM international conference on Multimedia, pages 1096- 1104, 2017. 3\n\nDreamix: Video diffusion models are general. Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen, arXiv:2302.01329arXiv preprintEyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 3\n\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie, arXiv:2302.0845326arXiv preprintChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon- gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2, 6\n\nGlide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, arXiv:2112.1074123arXiv preprintAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2, 3\n\nSinfusion: Training diffusion models on a single image or video. Yaniv Nikankin, Niv Haim, Michal Irani, arXiv:2211.117432022arXiv preprintYaniv Nikankin, Niv Haim, and Michal Irani. Sinfusion: Training diffusion models on a single image or video. arXiv preprint arXiv:2211.11743, 2022. 3\n\nTo create what you tell: Generating videos from captions. Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, Tao Mei, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaYingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from cap- tions. In Proceedings of the 25th ACM international confer- ence on Multimedia, pages 1789-1798, 2017. 3\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, Luc Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. 712arXiv preprintJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00e1ez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 7, 12\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR, 2021. 3, 8Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 3, 8\n\nHierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.0612523arXiv preprintAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image gen- eration with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2, 3\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, PMLR, 2021. 3ICML. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821- 8831. PMLR, 2021. 3\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, CVPR. 57Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image syn- thesis with latent diffusion models. In CVPR, pages 10684- 10695, 2022. 2, 3, 4, 5, 7\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241.\n\n. Springer, Springer, 2015. 4\n\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman, arXiv:2208.122426arXiv preprintNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 2, 5, 6\n\nPhotorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, ; S Sara Mahdavi, Rapha Gontijo Lopes, arXiv:2205.11487Burcu Karagol Ayan. 23arXiv preprintSeyed Kamyar Seyed GhasemipourChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 2, 3\n\nLaion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, arXiv:2210.08402arXiv preprintChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 2\n\nMake-a-video: Text-to-video generation without text-video data. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, arXiv:2209.14792arXiv preprintUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2, 3, 4, 6\n\n. Jiaming Song, Chenlin Meng, Stefano Ermon, arXiv:2010.0250227Denoising diffusion implicit models. arXiv preprintJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2, 7\n\nPlug-and-play diffusion features for textdriven image-to-image translation. Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel, arXiv:2211.12572712arXiv preprintNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text- driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 3, 7, 12\n\nAdvances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 304Neural discrete representation learningAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information pro- cessing systems, 30, 2017. 3, 4\n\nAttention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4\n\nPhenaki: Variable length video generation from open domain textual description. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan, arXiv:2210.02399arXiv preprintRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin- dermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 2\n\nN\u00fcwa: Visual synthesis pretraining for neural visual world creation. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringer2022Proceedings, Part XVIChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00fcwa: Visual synthesis pre- training for neural visual world creation. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVI, pages 720-736. Springer, 2022. 3\n\nUnifying diffusion models' latent space, with applications to cyclediffusion and guidance. Chen Henry Wu, Fernando De La Torre, arXiv:2210.055592022arXiv preprintChen Henry Wu and Fernando De la Torre. Unifying diffu- sion models' latent space, with applications to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559, 2022. 3\n\nVector-quantized image modeling with improved vqgan. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu, arXiv:2110.04627arXiv preprintJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3\n\nScaling autoregressive models for content-rich text-to-image generation. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, arXiv:2206.107892022arXiv preprintJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun- jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin- fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres- sive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 3\n\nAdding conditional control to text-to-image diffusion models. Lvmin Zhang, Maneesh Agrawala, arXiv:2302.05543arXiv preprintLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 6\n\nMagicvideo: Efficient video generation with latent diffusion models. Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng, arXiv:2211.1101823arXiv preprintDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 2, 3\n\nA magic princess with sunglasses is playing guitar on the stage, modern disney style\" \"Mr. Potato Head, made of Lego, is playing guitar on the snow. Modern Disney Style\" \"Mr. Potato Head. Mr. Potato Head, wearing sunglasses, is playing guitar on the beach. A bear is playing guitar\" Figure 11: Additional sample results of our method (2/2\"A bear is playing guitar\" \"A rabbit is playing guitar, modern disney style\" \"A handsome prince is playing guitar, modern disney style\" \"Mr. Potato Head, wearing sunglasses, is playing guitar on the beach\" \"Mr. Potato Head is playing guitar in starry night, Van Gogh style\" \"A magic princess with sunglasses is playing guitar on the stage, modern disney style\" \"Mr. Potato Head, made of Lego, is playing guitar on the snow\" \"Modern Disney Style\" \"Mr. Potato Head\" \"A bear is playing guitar\" Figure 11: Additional sample results of our method (2/2).\n", "annotations": {"author": "[{\"end\":148,\"start\":88},{\"end\":169,\"start\":149},{\"end\":192,\"start\":170},{\"end\":254,\"start\":193},{\"end\":309,\"start\":255},{\"end\":364,\"start\":310},{\"end\":430,\"start\":365},{\"end\":451,\"start\":431},{\"end\":477,\"start\":452},{\"end\":538,\"start\":478}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":101},{\"end\":158,\"start\":156},{\"end\":181,\"start\":177},{\"end\":209,\"start\":206},{\"end\":264,\"start\":262},{\"end\":319,\"start\":316},{\"end\":374,\"start\":371},{\"end\":440,\"start\":436},{\"end\":462,\"start\":459},{\"end\":493,\"start\":489}]", "author_first_name": "[{\"end\":91,\"start\":88},{\"end\":100,\"start\":92},{\"end\":155,\"start\":149},{\"end\":176,\"start\":170},{\"end\":197,\"start\":193},{\"end\":205,\"start\":198},{\"end\":261,\"start\":255},{\"end\":315,\"start\":310},{\"end\":370,\"start\":365},{\"end\":435,\"start\":431},{\"end\":458,\"start\":452},{\"end\":482,\"start\":478},{\"end\":488,\"start\":483}]", "author_affiliation": "[{\"end\":147,\"start\":105},{\"end\":168,\"start\":160},{\"end\":191,\"start\":183},{\"end\":253,\"start\":211},{\"end\":308,\"start\":266},{\"end\":363,\"start\":321},{\"end\":429,\"start\":376},{\"end\":450,\"start\":442},{\"end\":476,\"start\":464},{\"end\":537,\"start\":495}]", "title": "[{\"end\":85,\"start\":1},{\"end\":623,\"start\":539}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b41\"},\"end\":1146,\"start\":1142},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1283,\"start\":1279},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1286,\"start\":1283},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1288,\"start\":1286},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1291,\"start\":1288},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1293,\"start\":1291},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1374,\"start\":1370},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1377,\"start\":1374},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1380,\"start\":1377},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1383,\"start\":1380},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1386,\"start\":1383},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1586,\"start\":1583},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2802,\"start\":2798},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5258,\"start\":5254},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5501,\"start\":5497},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5522,\"start\":5518},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6904,\"start\":6900},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6907,\"start\":6904},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6910,\"start\":6907},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6912,\"start\":6910},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6914,\"start\":6912},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6950,\"start\":6946},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6953,\"start\":6950},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6956,\"start\":6953},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6959,\"start\":6956},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7003,\"start\":6999},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7015,\"start\":7011},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7054,\"start\":7050},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7122,\"start\":7118},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7169,\"start\":7165},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7196,\"start\":7192},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7308,\"start\":7304},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7348,\"start\":7344},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7837,\"start\":7833},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7840,\"start\":7837},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7843,\"start\":7840},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7846,\"start\":7843},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7849,\"start\":7846},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7852,\"start\":7849},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7979,\"start\":7975},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8110,\"start\":8106},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8298,\"start\":8294},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8414,\"start\":8411},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8500,\"start\":8496},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8595,\"start\":8591},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8728,\"start\":8724},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8748,\"start\":8744},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9204,\"start\":9200},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9207,\"start\":9204},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9209,\"start\":9207},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9212,\"start\":9209},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9215,\"start\":9212},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9218,\"start\":9215},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9377,\"start\":9374},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9545,\"start\":9541},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9756,\"start\":9752},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9770,\"start\":9767},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10154,\"start\":10151},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10157,\"start\":10154},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10319,\"start\":10315},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10466,\"start\":10462},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10728,\"start\":10724},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12123,\"start\":12119},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12126,\"start\":12123},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12128,\"start\":12126},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12567,\"start\":12563},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14108,\"start\":14104},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14261,\"start\":14258},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14264,\"start\":14261},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14816,\"start\":14812},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14848,\"start\":14844},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15491,\"start\":15487},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15844,\"start\":15840},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18532,\"start\":18528},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18671,\"start\":18667},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21088,\"start\":21084},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21377,\"start\":21373},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21720,\"start\":21716},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21740,\"start\":21736},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22247,\"start\":22244},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22499,\"start\":22495},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22777,\"start\":22773},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22812,\"start\":22808},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23069,\"start\":23065},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23214,\"start\":23210},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23419,\"start\":23415},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23594,\"start\":23590},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23787,\"start\":23783},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25043,\"start\":25039}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":28683,\"start\":28391},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28996,\"start\":28684},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29082,\"start\":28997},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29142,\"start\":29083},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29348,\"start\":29143},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29560,\"start\":29349},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29991,\"start\":29561}]", "paragraph": "[{\"end\":1090,\"start\":641},{\"end\":1760,\"start\":1107},{\"end\":2601,\"start\":1762},{\"end\":2840,\"start\":2603},{\"end\":3804,\"start\":2842},{\"end\":5581,\"start\":3843},{\"end\":5913,\"start\":5583},{\"end\":6061,\"start\":5915},{\"end\":6166,\"start\":6063},{\"end\":6279,\"start\":6168},{\"end\":6361,\"start\":6281},{\"end\":6745,\"start\":6378},{\"end\":7522,\"start\":6747},{\"end\":8415,\"start\":7524},{\"end\":9127,\"start\":8417},{\"end\":10099,\"start\":9129},{\"end\":10959,\"start\":10101},{\"end\":11505,\"start\":10970},{\"end\":11520,\"start\":11507},{\"end\":11570,\"start\":11546},{\"end\":11906,\"start\":11601},{\"end\":12293,\"start\":11908},{\"end\":12489,\"start\":12295},{\"end\":12831,\"start\":12507},{\"end\":13002,\"start\":12888},{\"end\":13482,\"start\":13253},{\"end\":13599,\"start\":13484},{\"end\":14653,\"start\":13601},{\"end\":14757,\"start\":14714},{\"end\":15502,\"start\":14779},{\"end\":15773,\"start\":15601},{\"end\":17224,\"start\":15775},{\"end\":17628,\"start\":17226},{\"end\":17857,\"start\":17692},{\"end\":18757,\"start\":17887},{\"end\":19341,\"start\":18759},{\"end\":19724,\"start\":19343},{\"end\":19855,\"start\":19757},{\"end\":20462,\"start\":19857},{\"end\":20899,\"start\":20464},{\"end\":21246,\"start\":20919},{\"end\":21932,\"start\":21248},{\"end\":21972,\"start\":21934},{\"end\":22402,\"start\":22001},{\"end\":22948,\"start\":22443},{\"end\":23343,\"start\":22973},{\"end\":23788,\"start\":23345},{\"end\":24809,\"start\":23790},{\"end\":26254,\"start\":24811},{\"end\":27593,\"start\":26273},{\"end\":28193,\"start\":27638},{\"end\":28390,\"start\":28195}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3842,\"start\":3805},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12887,\"start\":12832},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13252,\"start\":13003},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14713,\"start\":14654},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15600,\"start\":15503},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17691,\"start\":17629}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1105,\"start\":1093},{\"attributes\":{\"n\":\"2.\"},\"end\":6376,\"start\":6364},{\"attributes\":{\"n\":\"3.\"},\"end\":10968,\"start\":10962},{\"end\":11528,\"start\":11523},{\"end\":11544,\"start\":11531},{\"end\":11584,\"start\":11573},{\"end\":11599,\"start\":11587},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12505,\"start\":12492},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14777,\"start\":14760},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17885,\"start\":17860},{\"attributes\":{\"n\":\"4.\"},\"end\":19755,\"start\":19727},{\"end\":20917,\"start\":20902},{\"end\":21987,\"start\":21975},{\"end\":21999,\"start\":21990},{\"attributes\":{\"n\":\"5.\"},\"end\":22416,\"start\":22405},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22441,\"start\":22419},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22971,\"start\":22951},{\"attributes\":{\"n\":\"5.3.\"},\"end\":26271,\"start\":26257},{\"attributes\":{\"n\":\"6.\"},\"end\":27623,\"start\":27596},{\"attributes\":{\"n\":\"7.\"},\"end\":27636,\"start\":27626},{\"end\":28402,\"start\":28392},{\"end\":28695,\"start\":28685},{\"end\":29008,\"start\":28998},{\"end\":29094,\"start\":29084},{\"end\":29154,\"start\":29144},{\"end\":29383,\"start\":29350},{\"end\":29571,\"start\":29562}]", "table": "[{\"end\":29991,\"start\":29742}]", "figure_caption": "[{\"end\":28683,\"start\":28404},{\"end\":28996,\"start\":28697},{\"end\":29082,\"start\":29010},{\"end\":29142,\"start\":29096},{\"end\":29348,\"start\":29156},{\"end\":29560,\"start\":29390},{\"end\":29742,\"start\":29573}]", "figure_ref": "[{\"end\":3183,\"start\":3177},{\"end\":3580,\"start\":3572},{\"end\":4091,\"start\":4085},{\"end\":5710,\"start\":5704},{\"end\":11623,\"start\":11615},{\"end\":12488,\"start\":12482},{\"end\":16128,\"start\":16120},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17833,\"start\":17827},{\"end\":18679,\"start\":18673},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20046,\"start\":20040},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20712,\"start\":20706},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20839,\"start\":20833},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21160,\"start\":21152},{\"end\":21189,\"start\":21180},{\"end\":21635,\"start\":21626},{\"end\":21930,\"start\":21924},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23893,\"start\":23887},{\"end\":24808,\"start\":24801},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26524,\"start\":26518},{\"end\":27042,\"start\":27035},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27153,\"start\":27147}]", "bib_author_first_name": "[{\"end\":31219,\"start\":31214},{\"end\":31231,\"start\":31227},{\"end\":31235,\"start\":31232},{\"end\":31706,\"start\":31703},{\"end\":31718,\"start\":31713},{\"end\":31963,\"start\":31959},{\"end\":31978,\"start\":31973},{\"end\":31996,\"start\":31990},{\"end\":32010,\"start\":32006},{\"end\":32023,\"start\":32019},{\"end\":32490,\"start\":32487},{\"end\":32501,\"start\":32496},{\"end\":32516,\"start\":32509},{\"end\":32527,\"start\":32522},{\"end\":32985,\"start\":32976},{\"end\":33001,\"start\":32996},{\"end\":33017,\"start\":33011},{\"end\":33035,\"start\":33027},{\"end\":33347,\"start\":33343},{\"end\":33359,\"start\":33354},{\"end\":33372,\"start\":33367},{\"end\":33382,\"start\":33379},{\"end\":33677,\"start\":33670},{\"end\":33694,\"start\":33685},{\"end\":33708,\"start\":33701},{\"end\":33731,\"start\":33723},{\"end\":33751,\"start\":33742},{\"end\":34071,\"start\":34064},{\"end\":34084,\"start\":34079},{\"end\":34099,\"start\":34094},{\"end\":34549,\"start\":34545},{\"end\":34561,\"start\":34557},{\"end\":34574,\"start\":34570},{\"end\":34589,\"start\":34583},{\"end\":34603,\"start\":34599},{\"end\":34617,\"start\":34612},{\"end\":35106,\"start\":35099},{\"end\":35115,\"start\":35111},{\"end\":35129,\"start\":35122},{\"end\":35139,\"start\":35135},{\"end\":35147,\"start\":35145},{\"end\":35163,\"start\":35155},{\"end\":35172,\"start\":35170},{\"end\":35186,\"start\":35179},{\"end\":35455,\"start\":35449},{\"end\":35469,\"start\":35463},{\"end\":35482,\"start\":35479},{\"end\":35497,\"start\":35492},{\"end\":35514,\"start\":35505},{\"end\":35944,\"start\":35940},{\"end\":35955,\"start\":35950},{\"end\":35968,\"start\":35964},{\"end\":36278,\"start\":36275},{\"end\":36288,\"start\":36285},{\"end\":36303,\"start\":36300},{\"end\":36317,\"start\":36312},{\"end\":36331,\"start\":36327},{\"end\":36343,\"start\":36339},{\"end\":36357,\"start\":36351},{\"end\":36707,\"start\":36703},{\"end\":36718,\"start\":36715},{\"end\":36730,\"start\":36727},{\"end\":36746,\"start\":36742},{\"end\":36760,\"start\":36756},{\"end\":36775,\"start\":36769},{\"end\":37090,\"start\":37082},{\"end\":37102,\"start\":37095},{\"end\":37116,\"start\":37109},{\"end\":37129,\"start\":37126},{\"end\":37142,\"start\":37137},{\"end\":37154,\"start\":37148},{\"end\":37167,\"start\":37166},{\"end\":37181,\"start\":37178},{\"end\":37198,\"start\":37190},{\"end\":37211,\"start\":37206},{\"end\":37213,\"start\":37212},{\"end\":37586,\"start\":37578},{\"end\":37595,\"start\":37591},{\"end\":37608,\"start\":37602},{\"end\":37879,\"start\":37871},{\"end\":37887,\"start\":37884},{\"end\":38051,\"start\":38043},{\"end\":38059,\"start\":38056},{\"end\":38076,\"start\":38070},{\"end\":38095,\"start\":38088},{\"end\":38110,\"start\":38102},{\"end\":38125,\"start\":38120},{\"end\":38127,\"start\":38126},{\"end\":38416,\"start\":38411},{\"end\":38427,\"start\":38423},{\"end\":38439,\"start\":38434},{\"end\":38454,\"start\":38447},{\"end\":38463,\"start\":38460},{\"end\":38753,\"start\":38749},{\"end\":38767,\"start\":38762},{\"end\":38780,\"start\":38774},{\"end\":38791,\"start\":38787},{\"end\":39077,\"start\":39071},{\"end\":39091,\"start\":39085},{\"end\":39102,\"start\":39098},{\"end\":39113,\"start\":39109},{\"end\":39125,\"start\":39119},{\"end\":39137,\"start\":39133},{\"end\":39150,\"start\":39145},{\"end\":39166,\"start\":39160},{\"end\":39528,\"start\":39522},{\"end\":39539,\"start\":39533},{\"end\":39550,\"start\":39544},{\"end\":39567,\"start\":39561},{\"end\":39845,\"start\":39839},{\"end\":39856,\"start\":39850},{\"end\":39869,\"start\":39862},{\"end\":39881,\"start\":39876},{\"end\":39899,\"start\":39891},{\"end\":40108,\"start\":40105},{\"end\":40117,\"start\":40114},{\"end\":40130,\"start\":40124},{\"end\":40142,\"start\":40137},{\"end\":40529,\"start\":40524},{\"end\":40544,\"start\":40538},{\"end\":40554,\"start\":40553},{\"end\":40826,\"start\":40819},{\"end\":40839,\"start\":40833},{\"end\":40848,\"start\":40844},{\"end\":40862,\"start\":40855},{\"end\":40875,\"start\":40869},{\"end\":40887,\"start\":40880},{\"end\":40900,\"start\":40893},{\"end\":41300,\"start\":41294},{\"end\":41314,\"start\":41309},{\"end\":41324,\"start\":41323},{\"end\":41763,\"start\":41759},{\"end\":41777,\"start\":41771},{\"end\":41791,\"start\":41787},{\"end\":41806,\"start\":41802},{\"end\":41810,\"start\":41807},{\"end\":41822,\"start\":41817},{\"end\":41835,\"start\":41831},{\"end\":41849,\"start\":41844},{\"end\":41866,\"start\":41861},{\"end\":42235,\"start\":42230},{\"end\":42247,\"start\":42241},{\"end\":42262,\"start\":42254},{\"end\":42272,\"start\":42268},{\"end\":42288,\"start\":42280},{\"end\":42297,\"start\":42293},{\"end\":42310,\"start\":42304},{\"end\":42687,\"start\":42683},{\"end\":42704,\"start\":42696},{\"end\":42721,\"start\":42715},{\"end\":42736,\"start\":42730},{\"end\":42750,\"start\":42744},{\"end\":42763,\"start\":42760},{\"end\":42776,\"start\":42772},{\"end\":42792,\"start\":42788},{\"end\":43160,\"start\":43155},{\"end\":43174,\"start\":43171},{\"end\":43187,\"start\":43181},{\"end\":43445,\"start\":43438},{\"end\":43458,\"start\":43451},{\"end\":43468,\"start\":43464},{\"end\":43482,\"start\":43474},{\"end\":43490,\"start\":43487},{\"end\":43841,\"start\":43836},{\"end\":43862,\"start\":43854},{\"end\":43877,\"start\":43872},{\"end\":43892,\"start\":43887},{\"end\":43907,\"start\":43903},{\"end\":43928,\"start\":43925},{\"end\":44312,\"start\":44308},{\"end\":44326,\"start\":44322},{\"end\":44331,\"start\":44327},{\"end\":44342,\"start\":44337},{\"end\":44358,\"start\":44352},{\"end\":44374,\"start\":44367},{\"end\":44388,\"start\":44380},{\"end\":44404,\"start\":44398},{\"end\":44419,\"start\":44413},{\"end\":44434,\"start\":44428},{\"end\":44448,\"start\":44444},{\"end\":44902,\"start\":44896},{\"end\":44919,\"start\":44911},{\"end\":44934,\"start\":44930},{\"end\":44948,\"start\":44943},{\"end\":44958,\"start\":44954},{\"end\":45225,\"start\":45219},{\"end\":45241,\"start\":45234},{\"end\":45257,\"start\":45250},{\"end\":45268,\"start\":45263},{\"end\":45282,\"start\":45275},{\"end\":45293,\"start\":45289},{\"end\":45307,\"start\":45303},{\"end\":45318,\"start\":45314},{\"end\":45609,\"start\":45604},{\"end\":45626,\"start\":45619},{\"end\":45645,\"start\":45638},{\"end\":45661,\"start\":45654},{\"end\":45674,\"start\":45669},{\"end\":45954,\"start\":45950},{\"end\":45975,\"start\":45968},{\"end\":45991,\"start\":45985},{\"end\":46440,\"start\":46432},{\"end\":46455,\"start\":46447},{\"end\":46465,\"start\":46460},{\"end\":46479,\"start\":46475},{\"end\":46495,\"start\":46488},{\"end\":46512,\"start\":46508},{\"end\":46868,\"start\":46861},{\"end\":46885,\"start\":46878},{\"end\":46899,\"start\":46892},{\"end\":46912,\"start\":46908},{\"end\":46920,\"start\":46917},{\"end\":46933,\"start\":46928},{\"end\":46950,\"start\":46942},{\"end\":46973,\"start\":46960},{\"end\":47460,\"start\":47451},{\"end\":47478,\"start\":47472},{\"end\":47496,\"start\":47489},{\"end\":47508,\"start\":47504},{\"end\":47521,\"start\":47517},{\"end\":47537,\"start\":47532},{\"end\":47550,\"start\":47546},{\"end\":47566,\"start\":47560},{\"end\":47581,\"start\":47574},{\"end\":47598,\"start\":47590},{\"end\":48003,\"start\":47998},{\"end\":48016,\"start\":48012},{\"end\":48031,\"start\":48025},{\"end\":48041,\"start\":48039},{\"end\":48050,\"start\":48047},{\"end\":48063,\"start\":48055},{\"end\":48077,\"start\":48071},{\"end\":48087,\"start\":48082},{\"end\":48098,\"start\":48094},{\"end\":48111,\"start\":48107},{\"end\":48401,\"start\":48394},{\"end\":48415,\"start\":48408},{\"end\":48429,\"start\":48422},{\"end\":48716,\"start\":48711},{\"end\":48733,\"start\":48727},{\"end\":48745,\"start\":48741},{\"end\":48757,\"start\":48753},{\"end\":49039,\"start\":49034},{\"end\":49054,\"start\":49049},{\"end\":49348,\"start\":49342},{\"end\":49362,\"start\":49358},{\"end\":49376,\"start\":49372},{\"end\":49390,\"start\":49385},{\"end\":49407,\"start\":49402},{\"end\":49420,\"start\":49415},{\"end\":49422,\"start\":49421},{\"end\":49436,\"start\":49430},{\"end\":49450,\"start\":49445},{\"end\":49768,\"start\":49763},{\"end\":49787,\"start\":49779},{\"end\":49811,\"start\":49801},{\"end\":49830,\"start\":49824},{\"end\":49843,\"start\":49840},{\"end\":49859,\"start\":49851},{\"end\":49865,\"start\":49860},{\"end\":49882,\"start\":49874},{\"end\":49897,\"start\":49891},{\"end\":49912,\"start\":49905},{\"end\":50311,\"start\":50304},{\"end\":50320,\"start\":50316},{\"end\":50331,\"start\":50328},{\"end\":50339,\"start\":50336},{\"end\":50353,\"start\":50346},{\"end\":50365,\"start\":50360},{\"end\":50376,\"start\":50373},{\"end\":50886,\"start\":50882},{\"end\":50892,\"start\":50887},{\"end\":50905,\"start\":50897},{\"end\":51187,\"start\":51181},{\"end\":51195,\"start\":51192},{\"end\":51204,\"start\":51200},{\"end\":51207,\"start\":51205},{\"end\":51216,\"start\":51213},{\"end\":51231,\"start\":51224},{\"end\":51243,\"start\":51238},{\"end\":51258,\"start\":51249},{\"end\":51272,\"start\":51263},{\"end\":51282,\"start\":51277},{\"end\":51301,\"start\":51294},{\"end\":51639,\"start\":51633},{\"end\":51653,\"start\":51644},{\"end\":51662,\"start\":51658},{\"end\":51676,\"start\":51671},{\"end\":51690,\"start\":51684},{\"end\":51702,\"start\":51697},{\"end\":51714,\"start\":51709},{\"end\":51735,\"start\":51726},{\"end\":51746,\"start\":51740},{\"end\":52142,\"start\":52137},{\"end\":52157,\"start\":52150},{\"end\":52411,\"start\":52405},{\"end\":52424,\"start\":52418},{\"end\":52437,\"start\":52431},{\"end\":52449,\"start\":52443},{\"end\":52459,\"start\":52454},{\"end\":52471,\"start\":52465}]", "bib_author_last_name": "[{\"end\":31225,\"start\":31220},{\"end\":31239,\"start\":31236},{\"end\":31711,\"start\":31707},{\"end\":31726,\"start\":31719},{\"end\":31971,\"start\":31964},{\"end\":31988,\"start\":31979},{\"end\":32004,\"start\":31997},{\"end\":32017,\"start\":32011},{\"end\":32029,\"start\":32024},{\"end\":32494,\"start\":32491},{\"end\":32507,\"start\":32502},{\"end\":32520,\"start\":32517},{\"end\":32534,\"start\":32528},{\"end\":32994,\"start\":32986},{\"end\":33009,\"start\":33002},{\"end\":33025,\"start\":33018},{\"end\":33040,\"start\":33036},{\"end\":33352,\"start\":33348},{\"end\":33365,\"start\":33360},{\"end\":33377,\"start\":33373},{\"end\":33387,\"start\":33383},{\"end\":33683,\"start\":33678},{\"end\":33699,\"start\":33695},{\"end\":33721,\"start\":33709},{\"end\":33740,\"start\":33732},{\"end\":33762,\"start\":33752},{\"end\":34077,\"start\":34072},{\"end\":34092,\"start\":34085},{\"end\":34105,\"start\":34100},{\"end\":34555,\"start\":34550},{\"end\":34568,\"start\":34562},{\"end\":34581,\"start\":34575},{\"end\":34597,\"start\":34590},{\"end\":34610,\"start\":34604},{\"end\":34625,\"start\":34618},{\"end\":35109,\"start\":35107},{\"end\":35120,\"start\":35116},{\"end\":35133,\"start\":35130},{\"end\":35143,\"start\":35140},{\"end\":35153,\"start\":35148},{\"end\":35168,\"start\":35164},{\"end\":35177,\"start\":35173},{\"end\":35190,\"start\":35187},{\"end\":35461,\"start\":35456},{\"end\":35477,\"start\":35470},{\"end\":35490,\"start\":35483},{\"end\":35503,\"start\":35498},{\"end\":35523,\"start\":35515},{\"end\":35948,\"start\":35945},{\"end\":35962,\"start\":35956},{\"end\":35973,\"start\":35969},{\"end\":36283,\"start\":36279},{\"end\":36298,\"start\":36289},{\"end\":36310,\"start\":36304},{\"end\":36325,\"start\":36318},{\"end\":36337,\"start\":36332},{\"end\":36349,\"start\":36344},{\"end\":36363,\"start\":36358},{\"end\":36713,\"start\":36708},{\"end\":36725,\"start\":36719},{\"end\":36740,\"start\":36731},{\"end\":36754,\"start\":36747},{\"end\":36767,\"start\":36761},{\"end\":36784,\"start\":36776},{\"end\":37093,\"start\":37091},{\"end\":37107,\"start\":37103},{\"end\":37124,\"start\":37117},{\"end\":37135,\"start\":37130},{\"end\":37146,\"start\":37143},{\"end\":37164,\"start\":37155},{\"end\":37176,\"start\":37168},{\"end\":37188,\"start\":37182},{\"end\":37204,\"start\":37199},{\"end\":37221,\"start\":37214},{\"end\":37228,\"start\":37223},{\"end\":37589,\"start\":37587},{\"end\":37600,\"start\":37596},{\"end\":37615,\"start\":37609},{\"end\":37882,\"start\":37880},{\"end\":37896,\"start\":37888},{\"end\":38054,\"start\":38052},{\"end\":38068,\"start\":38060},{\"end\":38086,\"start\":38077},{\"end\":38100,\"start\":38096},{\"end\":38118,\"start\":38111},{\"end\":38133,\"start\":38128},{\"end\":38421,\"start\":38417},{\"end\":38432,\"start\":38428},{\"end\":38445,\"start\":38440},{\"end\":38458,\"start\":38455},{\"end\":38468,\"start\":38464},{\"end\":38760,\"start\":38754},{\"end\":38772,\"start\":38768},{\"end\":38785,\"start\":38781},{\"end\":38797,\"start\":38792},{\"end\":39083,\"start\":39078},{\"end\":39096,\"start\":39092},{\"end\":39107,\"start\":39103},{\"end\":39117,\"start\":39114},{\"end\":39131,\"start\":39126},{\"end\":39143,\"start\":39138},{\"end\":39158,\"start\":39151},{\"end\":39172,\"start\":39167},{\"end\":39531,\"start\":39529},{\"end\":39542,\"start\":39540},{\"end\":39559,\"start\":39551},{\"end\":39571,\"start\":39568},{\"end\":39848,\"start\":39846},{\"end\":39860,\"start\":39857},{\"end\":39874,\"start\":39870},{\"end\":39889,\"start\":39882},{\"end\":39905,\"start\":39900},{\"end\":40112,\"start\":40109},{\"end\":40122,\"start\":40118},{\"end\":40135,\"start\":40131},{\"end\":40146,\"start\":40143},{\"end\":40536,\"start\":40530},{\"end\":40551,\"start\":40545},{\"end\":40562,\"start\":40555},{\"end\":40579,\"start\":40564},{\"end\":40831,\"start\":40827},{\"end\":40842,\"start\":40840},{\"end\":40853,\"start\":40849},{\"end\":40867,\"start\":40863},{\"end\":40878,\"start\":40876},{\"end\":40891,\"start\":40888},{\"end\":40906,\"start\":40901},{\"end\":41307,\"start\":41301},{\"end\":41321,\"start\":41315},{\"end\":41332,\"start\":41325},{\"end\":41349,\"start\":41334},{\"end\":41769,\"start\":41764},{\"end\":41785,\"start\":41778},{\"end\":41800,\"start\":41792},{\"end\":41815,\"start\":41811},{\"end\":41829,\"start\":41823},{\"end\":41842,\"start\":41836},{\"end\":41859,\"start\":41850},{\"end\":41873,\"start\":41867},{\"end\":42239,\"start\":42236},{\"end\":42252,\"start\":42248},{\"end\":42266,\"start\":42263},{\"end\":42278,\"start\":42273},{\"end\":42291,\"start\":42289},{\"end\":42302,\"start\":42298},{\"end\":42314,\"start\":42311},{\"end\":42694,\"start\":42688},{\"end\":42713,\"start\":42705},{\"end\":42728,\"start\":42722},{\"end\":42742,\"start\":42737},{\"end\":42758,\"start\":42751},{\"end\":42770,\"start\":42764},{\"end\":42786,\"start\":42777},{\"end\":42797,\"start\":42793},{\"end\":43169,\"start\":43161},{\"end\":43179,\"start\":43175},{\"end\":43193,\"start\":43188},{\"end\":43449,\"start\":43446},{\"end\":43462,\"start\":43459},{\"end\":43472,\"start\":43469},{\"end\":43485,\"start\":43483},{\"end\":43494,\"start\":43491},{\"end\":43852,\"start\":43842},{\"end\":43870,\"start\":43863},{\"end\":43885,\"start\":43878},{\"end\":43901,\"start\":43893},{\"end\":43923,\"start\":43908},{\"end\":43937,\"start\":43929},{\"end\":44320,\"start\":44313},{\"end\":44335,\"start\":44332},{\"end\":44350,\"start\":44343},{\"end\":44365,\"start\":44359},{\"end\":44378,\"start\":44375},{\"end\":44396,\"start\":44389},{\"end\":44411,\"start\":44405},{\"end\":44426,\"start\":44420},{\"end\":44442,\"start\":44435},{\"end\":44454,\"start\":44449},{\"end\":44909,\"start\":44903},{\"end\":44928,\"start\":44920},{\"end\":44941,\"start\":44935},{\"end\":44952,\"start\":44949},{\"end\":44963,\"start\":44959},{\"end\":45232,\"start\":45226},{\"end\":45248,\"start\":45242},{\"end\":45261,\"start\":45258},{\"end\":45273,\"start\":45269},{\"end\":45287,\"start\":45283},{\"end\":45301,\"start\":45294},{\"end\":45312,\"start\":45308},{\"end\":45328,\"start\":45319},{\"end\":45617,\"start\":45610},{\"end\":45636,\"start\":45627},{\"end\":45652,\"start\":45646},{\"end\":45667,\"start\":45662},{\"end\":45680,\"start\":45675},{\"end\":45966,\"start\":45955},{\"end\":45983,\"start\":45976},{\"end\":45996,\"start\":45992},{\"end\":46325,\"start\":46317},{\"end\":46445,\"start\":46441},{\"end\":46458,\"start\":46456},{\"end\":46473,\"start\":46466},{\"end\":46486,\"start\":46480},{\"end\":46506,\"start\":46496},{\"end\":46520,\"start\":46513},{\"end\":46876,\"start\":46869},{\"end\":46890,\"start\":46886},{\"end\":46906,\"start\":46900},{\"end\":46915,\"start\":46913},{\"end\":46926,\"start\":46921},{\"end\":46940,\"start\":46934},{\"end\":46958,\"start\":46951},{\"end\":46979,\"start\":46974},{\"end\":47470,\"start\":47461},{\"end\":47487,\"start\":47479},{\"end\":47502,\"start\":47497},{\"end\":47515,\"start\":47509},{\"end\":47530,\"start\":47522},{\"end\":47544,\"start\":47538},{\"end\":47558,\"start\":47551},{\"end\":47572,\"start\":47567},{\"end\":47588,\"start\":47582},{\"end\":47607,\"start\":47599},{\"end\":48010,\"start\":48004},{\"end\":48023,\"start\":48017},{\"end\":48037,\"start\":48032},{\"end\":48045,\"start\":48042},{\"end\":48053,\"start\":48051},{\"end\":48069,\"start\":48064},{\"end\":48080,\"start\":48078},{\"end\":48092,\"start\":48088},{\"end\":48105,\"start\":48099},{\"end\":48117,\"start\":48112},{\"end\":48406,\"start\":48402},{\"end\":48420,\"start\":48416},{\"end\":48435,\"start\":48430},{\"end\":48725,\"start\":48717},{\"end\":48739,\"start\":48734},{\"end\":48751,\"start\":48746},{\"end\":48763,\"start\":48758},{\"end\":49047,\"start\":49040},{\"end\":49059,\"start\":49055},{\"end\":49068,\"start\":49061},{\"end\":49356,\"start\":49349},{\"end\":49370,\"start\":49363},{\"end\":49383,\"start\":49377},{\"end\":49400,\"start\":49391},{\"end\":49413,\"start\":49408},{\"end\":49428,\"start\":49423},{\"end\":49443,\"start\":49437},{\"end\":49461,\"start\":49451},{\"end\":49777,\"start\":49769},{\"end\":49799,\"start\":49788},{\"end\":49822,\"start\":49812},{\"end\":49838,\"start\":49831},{\"end\":49849,\"start\":49844},{\"end\":49872,\"start\":49866},{\"end\":49889,\"start\":49883},{\"end\":49903,\"start\":49898},{\"end\":49918,\"start\":49913},{\"end\":50314,\"start\":50312},{\"end\":50326,\"start\":50321},{\"end\":50334,\"start\":50332},{\"end\":50344,\"start\":50340},{\"end\":50358,\"start\":50354},{\"end\":50371,\"start\":50366},{\"end\":50381,\"start\":50377},{\"end\":50895,\"start\":50893},{\"end\":50917,\"start\":50906},{\"end\":51190,\"start\":51188},{\"end\":51198,\"start\":51196},{\"end\":51211,\"start\":51208},{\"end\":51222,\"start\":51217},{\"end\":51236,\"start\":51232},{\"end\":51247,\"start\":51244},{\"end\":51261,\"start\":51259},{\"end\":51275,\"start\":51273},{\"end\":51292,\"start\":51283},{\"end\":51304,\"start\":51302},{\"end\":51642,\"start\":51640},{\"end\":51656,\"start\":51654},{\"end\":51669,\"start\":51663},{\"end\":51682,\"start\":51677},{\"end\":51695,\"start\":51691},{\"end\":51707,\"start\":51703},{\"end\":51724,\"start\":51715},{\"end\":51738,\"start\":51736},{\"end\":51751,\"start\":51747},{\"end\":51771,\"start\":51753},{\"end\":52148,\"start\":52143},{\"end\":52166,\"start\":52158},{\"end\":52416,\"start\":52412},{\"end\":52429,\"start\":52425},{\"end\":52441,\"start\":52438},{\"end\":52452,\"start\":52450},{\"end\":52463,\"start\":52460},{\"end\":52476,\"start\":52472}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":230095451},\"end\":31594,\"start\":31149},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":232478955},\"end\":31901,\"start\":31596},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":247996703},\"end\":32416,\"start\":31903},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16224674},\"end\":32905,\"start\":32418},{\"attributes\":{\"doi\":\"arXiv:2210.11427\",\"id\":\"b4\"},\"end\":33257,\"start\":32907},{\"attributes\":{\"doi\":\"arXiv:2204.14217\",\"id\":\"b5\"},\"end\":33600,\"start\":33259},{\"attributes\":{\"doi\":\"arXiv:2302.03011\",\"id\":\"b6\"},\"end\":34005,\"start\":33602},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":229297973},\"end\":34474,\"start\":34007},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":247628171},\"end\":35035,\"start\":34476},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":244714856},\"end\":35398,\"start\":35037},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4752763},\"end\":35862,\"start\":35400},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219966229},\"end\":36219,\"start\":35864},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":237563160},\"end\":36640,\"start\":36221},{\"attributes\":{\"doi\":\"arXiv:2208.01626\",\"id\":\"b13\"},\"end\":37010,\"start\":36642},{\"attributes\":{\"doi\":\"arXiv:2210.02303\",\"id\":\"b14\"},\"end\":37534,\"start\":37012},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219955663},\"end\":37833,\"start\":37536},{\"attributes\":{\"doi\":\"arXiv:2207.12598\",\"id\":\"b16\"},\"end\":38039,\"start\":37835},{\"attributes\":{\"doi\":\"arXiv:2204.03458\",\"id\":\"b17\"},\"end\":38328,\"start\":38041},{\"attributes\":{\"doi\":\"arXiv:2205.15868\",\"id\":\"b18\"},\"end\":38694,\"start\":38330},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":237605410},\"end\":39008,\"start\":38696},{\"attributes\":{\"doi\":\"arXiv:2210.09276\",\"id\":\"b20\"},\"end\":39416,\"start\":39010},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b21\"},\"end\":39809,\"start\":39418},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8672818},\"end\":40044,\"start\":39811},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204837030},\"end\":40470,\"start\":40046},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":26547274},\"end\":40734,\"start\":40472},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":245704504},\"end\":41208,\"start\":40736},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13423527},\"end\":41712,\"start\":41210},{\"attributes\":{\"doi\":\"arXiv:2302.01329\",\"id\":\"b27\"},\"end\":42124,\"start\":41714},{\"attributes\":{\"doi\":\"arXiv:2302.08453\",\"id\":\"b28\"},\"end\":42587,\"start\":42126},{\"attributes\":{\"doi\":\"arXiv:2112.10741\",\"id\":\"b29\"},\"end\":43088,\"start\":42589},{\"attributes\":{\"doi\":\"arXiv:2211.11743\",\"id\":\"b30\"},\"end\":43378,\"start\":43090},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5039505},\"end\":43834,\"start\":43380},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b32\"},\"end\":44235,\"start\":43836},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":231591445},\"end\":44828,\"start\":44237},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b34\"},\"end\":45181,\"start\":44830},{\"attributes\":{\"doi\":\"PMLR, 2021. 3\",\"id\":\"b35\",\"matched_paper_id\":232035663},\"end\":45540,\"start\":45183},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":245335280},\"end\":45884,\"start\":45542},{\"attributes\":{\"id\":\"b37\"},\"end\":46313,\"start\":45886},{\"attributes\":{\"id\":\"b38\"},\"end\":46344,\"start\":46315},{\"attributes\":{\"doi\":\"arXiv:2208.12242\",\"id\":\"b39\"},\"end\":46779,\"start\":46346},{\"attributes\":{\"doi\":\"arXiv:2205.11487\",\"id\":\"b40\",\"matched_paper_id\":248986576},\"end\":47363,\"start\":46781},{\"attributes\":{\"doi\":\"arXiv:2210.08402\",\"id\":\"b41\"},\"end\":47932,\"start\":47365},{\"attributes\":{\"doi\":\"arXiv:2209.14792\",\"id\":\"b42\"},\"end\":48390,\"start\":47934},{\"attributes\":{\"doi\":\"arXiv:2010.02502\",\"id\":\"b43\"},\"end\":48633,\"start\":48392},{\"attributes\":{\"doi\":\"arXiv:2211.12572\",\"id\":\"b44\"},\"end\":48981,\"start\":48635},{\"attributes\":{\"id\":\"b45\"},\"end\":49262,\"start\":48983},{\"attributes\":{\"id\":\"b46\"},\"end\":49681,\"start\":49264},{\"attributes\":{\"doi\":\"arXiv:2210.02399\",\"id\":\"b47\"},\"end\":50233,\"start\":49683},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":244527261},\"end\":50789,\"start\":50235},{\"attributes\":{\"doi\":\"arXiv:2210.05559\",\"id\":\"b49\"},\"end\":51126,\"start\":50791},{\"attributes\":{\"doi\":\"arXiv:2110.04627\",\"id\":\"b50\"},\"end\":51558,\"start\":51128},{\"attributes\":{\"doi\":\"arXiv:2206.10789\",\"id\":\"b51\"},\"end\":52073,\"start\":51560},{\"attributes\":{\"doi\":\"arXiv:2302.05543\",\"id\":\"b52\"},\"end\":52334,\"start\":52075},{\"attributes\":{\"doi\":\"arXiv:2211.11018\",\"id\":\"b53\"},\"end\":52699,\"start\":52336},{\"attributes\":{\"id\":\"b54\"},\"end\":53587,\"start\":52701}]", "bib_title": "[{\"end\":31212,\"start\":31149},{\"end\":31701,\"start\":31596},{\"end\":31957,\"start\":31903},{\"end\":32485,\"start\":32418},{\"end\":34062,\"start\":34007},{\"end\":34543,\"start\":34476},{\"end\":35097,\"start\":35037},{\"end\":35447,\"start\":35400},{\"end\":35938,\"start\":35864},{\"end\":36273,\"start\":36221},{\"end\":37576,\"start\":37536},{\"end\":38747,\"start\":38696},{\"end\":39837,\"start\":39811},{\"end\":40103,\"start\":40046},{\"end\":40522,\"start\":40472},{\"end\":40817,\"start\":40736},{\"end\":41292,\"start\":41210},{\"end\":43436,\"start\":43380},{\"end\":44306,\"start\":44237},{\"end\":45217,\"start\":45183},{\"end\":45602,\"start\":45542},{\"end\":45948,\"start\":45886},{\"end\":46859,\"start\":46781},{\"end\":50302,\"start\":50235},{\"end\":52848,\"start\":52701}]", "bib_author": "[{\"end\":31227,\"start\":31214},{\"end\":31241,\"start\":31227},{\"end\":31713,\"start\":31703},{\"end\":31728,\"start\":31713},{\"end\":31973,\"start\":31959},{\"end\":31990,\"start\":31973},{\"end\":32006,\"start\":31990},{\"end\":32019,\"start\":32006},{\"end\":32031,\"start\":32019},{\"end\":32496,\"start\":32487},{\"end\":32509,\"start\":32496},{\"end\":32522,\"start\":32509},{\"end\":32536,\"start\":32522},{\"end\":32996,\"start\":32976},{\"end\":33011,\"start\":32996},{\"end\":33027,\"start\":33011},{\"end\":33042,\"start\":33027},{\"end\":33354,\"start\":33343},{\"end\":33367,\"start\":33354},{\"end\":33379,\"start\":33367},{\"end\":33389,\"start\":33379},{\"end\":33685,\"start\":33670},{\"end\":33701,\"start\":33685},{\"end\":33723,\"start\":33701},{\"end\":33742,\"start\":33723},{\"end\":33764,\"start\":33742},{\"end\":34079,\"start\":34064},{\"end\":34094,\"start\":34079},{\"end\":34107,\"start\":34094},{\"end\":34557,\"start\":34545},{\"end\":34570,\"start\":34557},{\"end\":34583,\"start\":34570},{\"end\":34599,\"start\":34583},{\"end\":34612,\"start\":34599},{\"end\":34627,\"start\":34612},{\"end\":35111,\"start\":35099},{\"end\":35122,\"start\":35111},{\"end\":35135,\"start\":35122},{\"end\":35145,\"start\":35135},{\"end\":35155,\"start\":35145},{\"end\":35170,\"start\":35155},{\"end\":35179,\"start\":35170},{\"end\":35192,\"start\":35179},{\"end\":35463,\"start\":35449},{\"end\":35479,\"start\":35463},{\"end\":35492,\"start\":35479},{\"end\":35505,\"start\":35492},{\"end\":35525,\"start\":35505},{\"end\":35950,\"start\":35940},{\"end\":35964,\"start\":35950},{\"end\":35975,\"start\":35964},{\"end\":36285,\"start\":36275},{\"end\":36300,\"start\":36285},{\"end\":36312,\"start\":36300},{\"end\":36327,\"start\":36312},{\"end\":36339,\"start\":36327},{\"end\":36351,\"start\":36339},{\"end\":36365,\"start\":36351},{\"end\":36715,\"start\":36703},{\"end\":36727,\"start\":36715},{\"end\":36742,\"start\":36727},{\"end\":36756,\"start\":36742},{\"end\":36769,\"start\":36756},{\"end\":36786,\"start\":36769},{\"end\":37095,\"start\":37082},{\"end\":37109,\"start\":37095},{\"end\":37126,\"start\":37109},{\"end\":37137,\"start\":37126},{\"end\":37148,\"start\":37137},{\"end\":37166,\"start\":37148},{\"end\":37178,\"start\":37166},{\"end\":37190,\"start\":37178},{\"end\":37206,\"start\":37190},{\"end\":37223,\"start\":37206},{\"end\":37230,\"start\":37223},{\"end\":37591,\"start\":37578},{\"end\":37602,\"start\":37591},{\"end\":37617,\"start\":37602},{\"end\":37884,\"start\":37871},{\"end\":37898,\"start\":37884},{\"end\":38056,\"start\":38043},{\"end\":38070,\"start\":38056},{\"end\":38088,\"start\":38070},{\"end\":38102,\"start\":38088},{\"end\":38120,\"start\":38102},{\"end\":38135,\"start\":38120},{\"end\":38423,\"start\":38411},{\"end\":38434,\"start\":38423},{\"end\":38447,\"start\":38434},{\"end\":38460,\"start\":38447},{\"end\":38470,\"start\":38460},{\"end\":38762,\"start\":38749},{\"end\":38774,\"start\":38762},{\"end\":38787,\"start\":38774},{\"end\":38799,\"start\":38787},{\"end\":39085,\"start\":39071},{\"end\":39098,\"start\":39085},{\"end\":39109,\"start\":39098},{\"end\":39119,\"start\":39109},{\"end\":39133,\"start\":39119},{\"end\":39145,\"start\":39133},{\"end\":39160,\"start\":39145},{\"end\":39174,\"start\":39160},{\"end\":39533,\"start\":39522},{\"end\":39544,\"start\":39533},{\"end\":39561,\"start\":39544},{\"end\":39573,\"start\":39561},{\"end\":39850,\"start\":39839},{\"end\":39862,\"start\":39850},{\"end\":39876,\"start\":39862},{\"end\":39891,\"start\":39876},{\"end\":39907,\"start\":39891},{\"end\":40114,\"start\":40105},{\"end\":40124,\"start\":40114},{\"end\":40137,\"start\":40124},{\"end\":40148,\"start\":40137},{\"end\":40538,\"start\":40524},{\"end\":40553,\"start\":40538},{\"end\":40564,\"start\":40553},{\"end\":40581,\"start\":40564},{\"end\":40833,\"start\":40819},{\"end\":40844,\"start\":40833},{\"end\":40855,\"start\":40844},{\"end\":40869,\"start\":40855},{\"end\":40880,\"start\":40869},{\"end\":40893,\"start\":40880},{\"end\":40908,\"start\":40893},{\"end\":41309,\"start\":41294},{\"end\":41323,\"start\":41309},{\"end\":41334,\"start\":41323},{\"end\":41351,\"start\":41334},{\"end\":41771,\"start\":41759},{\"end\":41787,\"start\":41771},{\"end\":41802,\"start\":41787},{\"end\":41817,\"start\":41802},{\"end\":41831,\"start\":41817},{\"end\":41844,\"start\":41831},{\"end\":41861,\"start\":41844},{\"end\":41875,\"start\":41861},{\"end\":42241,\"start\":42230},{\"end\":42254,\"start\":42241},{\"end\":42268,\"start\":42254},{\"end\":42280,\"start\":42268},{\"end\":42293,\"start\":42280},{\"end\":42304,\"start\":42293},{\"end\":42316,\"start\":42304},{\"end\":42696,\"start\":42683},{\"end\":42715,\"start\":42696},{\"end\":42730,\"start\":42715},{\"end\":42744,\"start\":42730},{\"end\":42760,\"start\":42744},{\"end\":42772,\"start\":42760},{\"end\":42788,\"start\":42772},{\"end\":42799,\"start\":42788},{\"end\":43171,\"start\":43155},{\"end\":43181,\"start\":43171},{\"end\":43195,\"start\":43181},{\"end\":43451,\"start\":43438},{\"end\":43464,\"start\":43451},{\"end\":43474,\"start\":43464},{\"end\":43487,\"start\":43474},{\"end\":43496,\"start\":43487},{\"end\":43854,\"start\":43836},{\"end\":43872,\"start\":43854},{\"end\":43887,\"start\":43872},{\"end\":43903,\"start\":43887},{\"end\":43925,\"start\":43903},{\"end\":43939,\"start\":43925},{\"end\":44322,\"start\":44308},{\"end\":44337,\"start\":44322},{\"end\":44352,\"start\":44337},{\"end\":44367,\"start\":44352},{\"end\":44380,\"start\":44367},{\"end\":44398,\"start\":44380},{\"end\":44413,\"start\":44398},{\"end\":44428,\"start\":44413},{\"end\":44444,\"start\":44428},{\"end\":44456,\"start\":44444},{\"end\":44911,\"start\":44896},{\"end\":44930,\"start\":44911},{\"end\":44943,\"start\":44930},{\"end\":44954,\"start\":44943},{\"end\":44965,\"start\":44954},{\"end\":45234,\"start\":45219},{\"end\":45250,\"start\":45234},{\"end\":45263,\"start\":45250},{\"end\":45275,\"start\":45263},{\"end\":45289,\"start\":45275},{\"end\":45303,\"start\":45289},{\"end\":45314,\"start\":45303},{\"end\":45330,\"start\":45314},{\"end\":45619,\"start\":45604},{\"end\":45638,\"start\":45619},{\"end\":45654,\"start\":45638},{\"end\":45669,\"start\":45654},{\"end\":45682,\"start\":45669},{\"end\":45968,\"start\":45950},{\"end\":45985,\"start\":45968},{\"end\":45998,\"start\":45985},{\"end\":46327,\"start\":46317},{\"end\":46447,\"start\":46432},{\"end\":46460,\"start\":46447},{\"end\":46475,\"start\":46460},{\"end\":46488,\"start\":46475},{\"end\":46508,\"start\":46488},{\"end\":46522,\"start\":46508},{\"end\":46878,\"start\":46861},{\"end\":46892,\"start\":46878},{\"end\":46908,\"start\":46892},{\"end\":46917,\"start\":46908},{\"end\":46928,\"start\":46917},{\"end\":46942,\"start\":46928},{\"end\":46960,\"start\":46942},{\"end\":46981,\"start\":46960},{\"end\":47472,\"start\":47451},{\"end\":47489,\"start\":47472},{\"end\":47504,\"start\":47489},{\"end\":47517,\"start\":47504},{\"end\":47532,\"start\":47517},{\"end\":47546,\"start\":47532},{\"end\":47560,\"start\":47546},{\"end\":47574,\"start\":47560},{\"end\":47590,\"start\":47574},{\"end\":47609,\"start\":47590},{\"end\":48012,\"start\":47998},{\"end\":48025,\"start\":48012},{\"end\":48039,\"start\":48025},{\"end\":48047,\"start\":48039},{\"end\":48055,\"start\":48047},{\"end\":48071,\"start\":48055},{\"end\":48082,\"start\":48071},{\"end\":48094,\"start\":48082},{\"end\":48107,\"start\":48094},{\"end\":48119,\"start\":48107},{\"end\":48408,\"start\":48394},{\"end\":48422,\"start\":48408},{\"end\":48437,\"start\":48422},{\"end\":48727,\"start\":48711},{\"end\":48741,\"start\":48727},{\"end\":48753,\"start\":48741},{\"end\":48765,\"start\":48753},{\"end\":49049,\"start\":49034},{\"end\":49061,\"start\":49049},{\"end\":49070,\"start\":49061},{\"end\":49358,\"start\":49342},{\"end\":49372,\"start\":49358},{\"end\":49385,\"start\":49372},{\"end\":49402,\"start\":49385},{\"end\":49415,\"start\":49402},{\"end\":49430,\"start\":49415},{\"end\":49445,\"start\":49430},{\"end\":49463,\"start\":49445},{\"end\":49779,\"start\":49763},{\"end\":49801,\"start\":49779},{\"end\":49824,\"start\":49801},{\"end\":49840,\"start\":49824},{\"end\":49851,\"start\":49840},{\"end\":49874,\"start\":49851},{\"end\":49891,\"start\":49874},{\"end\":49905,\"start\":49891},{\"end\":49920,\"start\":49905},{\"end\":50316,\"start\":50304},{\"end\":50328,\"start\":50316},{\"end\":50336,\"start\":50328},{\"end\":50346,\"start\":50336},{\"end\":50360,\"start\":50346},{\"end\":50373,\"start\":50360},{\"end\":50383,\"start\":50373},{\"end\":50897,\"start\":50882},{\"end\":50919,\"start\":50897},{\"end\":51192,\"start\":51181},{\"end\":51200,\"start\":51192},{\"end\":51213,\"start\":51200},{\"end\":51224,\"start\":51213},{\"end\":51238,\"start\":51224},{\"end\":51249,\"start\":51238},{\"end\":51263,\"start\":51249},{\"end\":51277,\"start\":51263},{\"end\":51294,\"start\":51277},{\"end\":51306,\"start\":51294},{\"end\":51644,\"start\":51633},{\"end\":51658,\"start\":51644},{\"end\":51671,\"start\":51658},{\"end\":51684,\"start\":51671},{\"end\":51697,\"start\":51684},{\"end\":51709,\"start\":51697},{\"end\":51726,\"start\":51709},{\"end\":51740,\"start\":51726},{\"end\":51753,\"start\":51740},{\"end\":51773,\"start\":51753},{\"end\":52150,\"start\":52137},{\"end\":52168,\"start\":52150},{\"end\":52418,\"start\":52405},{\"end\":52431,\"start\":52418},{\"end\":52443,\"start\":52431},{\"end\":52454,\"start\":52443},{\"end\":52465,\"start\":52454},{\"end\":52478,\"start\":52465}]", "bib_venue": "[{\"end\":31388,\"start\":31323},{\"end\":32100,\"start\":32084},{\"end\":32677,\"start\":32615},{\"end\":34256,\"start\":34190},{\"end\":34696,\"start\":34680},{\"end\":35640,\"start\":35591},{\"end\":40267,\"start\":40216},{\"end\":41470,\"start\":41419},{\"end\":43615,\"start\":43564},{\"end\":50452,\"start\":50436},{\"end\":31321,\"start\":31241},{\"end\":31732,\"start\":31728},{\"end\":32082,\"start\":32031},{\"end\":32613,\"start\":32536},{\"end\":32974,\"start\":32907},{\"end\":33341,\"start\":33259},{\"end\":33668,\"start\":33602},{\"end\":34188,\"start\":34107},{\"end\":34678,\"start\":34627},{\"end\":35196,\"start\":35192},{\"end\":35589,\"start\":35525},{\"end\":36024,\"start\":35975},{\"end\":36403,\"start\":36365},{\"end\":36701,\"start\":36642},{\"end\":37080,\"start\":37012},{\"end\":37666,\"start\":37617},{\"end\":37869,\"start\":37835},{\"end\":38409,\"start\":38330},{\"end\":38833,\"start\":38799},{\"end\":39069,\"start\":39010},{\"end\":39520,\"start\":39418},{\"end\":39911,\"start\":39907},{\"end\":40214,\"start\":40148},{\"end\":40585,\"start\":40581},{\"end\":40960,\"start\":40908},{\"end\":41417,\"start\":41351},{\"end\":41757,\"start\":41714},{\"end\":42228,\"start\":42126},{\"end\":42681,\"start\":42589},{\"end\":43153,\"start\":43090},{\"end\":43562,\"start\":43496},{\"end\":44008,\"start\":43955},{\"end\":44500,\"start\":44456},{\"end\":44894,\"start\":44830},{\"end\":45347,\"start\":45343},{\"end\":45686,\"start\":45682},{\"end\":46084,\"start\":45998},{\"end\":46430,\"start\":46346},{\"end\":47015,\"start\":46997},{\"end\":47449,\"start\":47365},{\"end\":47996,\"start\":47934},{\"end\":48709,\"start\":48635},{\"end\":49032,\"start\":48983},{\"end\":49340,\"start\":49264},{\"end\":49761,\"start\":49683},{\"end\":50434,\"start\":50383},{\"end\":50880,\"start\":50791},{\"end\":51179,\"start\":51128},{\"end\":51631,\"start\":51560},{\"end\":52135,\"start\":52075},{\"end\":52403,\"start\":52336},{\"end\":52887,\"start\":52850}]"}}}, "year": 2023, "month": 12, "day": 17}