{"id": 146121106, "updated": "2023-10-07 06:49:21.937", "metadata": {"title": "WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving", "authors": "[{\"first\":\"Senthil\",\"last\":\"Yogamani\",\"middle\":[]},{\"first\":\"Ciaran\",\"last\":\"Hughes\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Horgan\",\"middle\":[]},{\"first\":\"Ganesh\",\"last\":\"Sistu\",\"middle\":[]},{\"first\":\"Padraig\",\"last\":\"Varley\",\"middle\":[]},{\"first\":\"Derek\",\"last\":\"O'Dea\",\"middle\":[]},{\"first\":\"Michal\",\"last\":\"Uricar\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Milz\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Simon\",\"middle\":[]},{\"first\":\"Karl\",\"last\":\"Amende\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Witt\",\"middle\":[]},{\"first\":\"Hazem\",\"last\":\"Rashed\",\"middle\":[]},{\"first\":\"Sumanth\",\"last\":\"Chennupati\",\"middle\":[]},{\"first\":\"Sanjaya\",\"last\":\"Nayak\",\"middle\":[]},{\"first\":\"Saquib\",\"last\":\"Mansoor\",\"middle\":[]},{\"first\":\"Xavier\",\"last\":\"Perroton\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Perez\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 5, "day": 4}, "abstract": "Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1905.01489", "mag": "3004734937", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/YogamaniWRNMVPO19", "doi": "10.1109/iccv.2019.00940"}}, "content": {"source": {"pdf_hash": "52e76738a5c6b751c738cf599fffa28f30b75c25", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1905.01489v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1905.01489", "status": "GREEN"}}, "grobid": {"id": "2c0d3c88404d920ea5fa87a6e912e28f078d219a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/52e76738a5c6b751c738cf599fffa28f30b75c25.txt", "contents": "\nWoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving\n\n\nSenthil Yogamani \nCiar\u00e1n Hughes \nJonathan Horgan \nGanesh Sistu \nPadraig Varley \nDerek O&apos;dea \nMichal U\u0159i\u010d\u00e1\u0159 \nStefan Milz \nMartin Simon \nKarl Amende \nChristian Witt \nHazem Rashed \nSumanth Chennupati \nSanjaya Nayak \nSaquib Mansoor \nXavier Perrotton \nPatrick P\u00e9rez \nWoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving\nFront Camera & Semantic Segmentation Front Camera Left Camera Right Camera Rear Camera Lidar 3D View 3D Box Lidar View Lidar Bird-View\nFigure 1: We introduce WoodScape, the first fisheye image dataset dedicated to autonomous driving. It contains four cameras covering 360\u00b0accompanied by a HD laser scanner, IMU and GNSS. Annotations are made available for nine tasks, notably 3D object detection, depth estimation (overlaid on front camera) and semantic segmentation as illustrated here.AbstractFisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, Wood-Scape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With Wood-Scape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using na\u00efve rectification.\n\nIntroduction\n\nFisheye lenses provide a large field of view (FOV) using a highly non-linear mapping instead of the standard perspective projection. However, it comes at the cost of strong radial distortion. Fisheye cameras are so-named because they relate to the 180\u00b0view of the world that a fish has observing the water surface from below, a phenomenon known as Snell's window. Robert Wood originally coined the term in 1906 [58], and constructed a basic fisheye camera by taking a pin-hole camera and filling it with water. It was later replaced with a hemispherical lens [3]. To pay homage to the original inventor and coiner of the term \"fisheye\", we have named our dataset WoodScape.\n\nLarge FOV cameras are necessary for various computer vision application domains, including video surveillance [28] and augmented reality [46], and have been of particular interest in autonomous driving [23]. In automotive, rearview fisheye cameras are commonly deployed in existing vehicles for dashboard viewing and reverse parking. While commercial autonomous driving systems typically make use of narrow FOV forward facing cameras at present, full 360\u00b0perception is now investigated for handling more complex use cases. In spite of this growing interest, there is relatively little literature and datasets available. Some examples of the few datasets that have fisheye are: Visual SLAM ground truth for indoor scenes with omnidirectional cameras in [7], SphereNet [9] containing 1200 labelled images of parked cars using 360\u00b0cameras (not strictly fisheye) and, in automotive, the Oxford Robotcar dataset [37] containing a large scale relocalization dataset.\n\nWoodScape is a comprehensive dataset for 360\u00b0sensing around a vehicle using the four fisheye cameras shown in Figure 2. It aims at complementing the range of already existing automotive datasets where only narrow FOV image data is present: among those, KITTI [17] was the first pioneering dataset with a variety of tasks, which drove a lot of research for autonomous driving; Cityscapes [10] provided the first comprehensive semantic segmentation dataset and Mapillary [39] provided a significantly larger dataset; Apolloscape [24] and BDD100k [59] are more recent datasets that push the annotation scale further. WoodScape is unique in that it provides fisheye image data, along with a comprehensive range of annotation types. A comparative summary of these different datasets is provided in Table 1. The main contributions of WoodScape are as follows:\n\n1. First fisheye dataset comprising of over 10,000 images containing instance level semantic annotation. 2. Four-camera nine-task dataset designed to encourage unified multi-task and multi-camera models. 3. Introduction of a novel soiling detection task and release of first dataset of its kind. 4. Proposal of an efficient metric for the 3D box detection task which improves training time by 95x.\n\nThe paper is organized as follows. Section 2 provides an overview of fisheye camera model, undistortion methods and fisheye adaption of vision algorithms. Section 3 discusses the details of the dataset including goals, capture infrastructure and dataset design. Section 4 presents the list of supported tasks and baseline experiments. Finally, Section 5 summarizes and concludes the paper.\n\n\nOverview of Fisheye Camera Projections\n\nFisheye cameras offer a distinct advantage for automotive applications. Given their extremely wide field of view, they can observe the full surrounding of a vehicle with a minimal number of sensors, with just four cameras typi- cally being required for full 360 \u2022 coverage ( Figure 2). This advantage comes with some drawbacks in the significantly more complex projection geometry that fisheye cameras exhibit. That is, images from fisheye cameras display severe distortion.\n\nTypical camera datasets consist of narrow FOV camera data where a simple pinhole projection model is commonly employed. In case of fisheye camera images, it is imperative that the appropriate camera model is well understood either to handle distortion in the algorithm or to warp the image prior to processing. This section is intended to highlight to the reader that the fisheye camera model requires specific attention. We provide a brief overview and references for further details, and discuss the merits of operating on the raw fisheye versus undistortion of the image.\n\n\nFisheye Camera Models\n\nFisheye distortion is modelled by a radial mapping function r(\u03b8), where r(\u03b8) is the distance on the image from the centre of distortion, and is a function of the angle \u03b8 of the incident ray against the optical axis of the camera system. The centre of distortion is the intersection of the optical axis with the image plane, and is the origin of the radial mapping function r(\u03b8). Stereographic projection [22] is the simplest model which uses a mapping from a sphere to a plane. More recent projection models are Unified Camera Model (UCM) [1,7] and eUCM (Enhanced UCM) [27]. More detailed analysis of accuracy of various projection models is discussed in [25]. These models are not a perfect fit for fisheye cameras as they encode a specific geometry (e.g. spherical projection), and errors arising in the model are compensated by using an added distortion correction component.\n\nIn WoodScape, we provide model parameters for a more generic fisheye intrinsic calibration that is independent of any specific projection model, and does not require the added step of distortion correction. Our model is based on a fourth order polynomial mapping incident angle to image radius in pixels (r(\u03b8) = a 1 \u03b8 + a 2 \u03b8 2 + a 3 \u03b8 3 + a 4 \u03b8 4 ). In our experience, higher orders provide no additional accuracy. Each video sequence in the dataset is provided with parameters for the fourth order polynomial model of fisheye intrinsics.\n\nAs a comparison, to give the reader an understanding of how different models behave, Figure 3 shows the mapping function r(\u03b8) for five different projection models, which are Polynomial, Rectilinear, Stereographic, UCM and eUCM. The parameters of the fourth order polynomial are taken from a calibration of our fisheye lens. We optimized the parameters for the other models to match this model in a range of 0 \u2022 to 120 \u2022 (i.e. up to FOV of 240 \u2022 ). The plot indicates that the difference to the original fourth order polynomial is about four pixels for UCM and one pixel for eUCM for low incident angles. For larger incident angles, these models are less precise.\n\n\nImage Undistortion vs. Model Adaptation\n\nStandard computer vision models do not generalize easily to fisheye cameras because of large non-linear distortion. For example, translation invariance is lost for a standard convolutional neural net (CNN). The na\u00efve way to develop algorithms for fisheye cameras is to perform rectilinear correction so that standard models can be applied. The simplest undistortion is to re-warp pixels to a rectilinear image as shown in Figure 4 (a). But there are two major issues. Firstly, the FOV is greater than 180 \u2022 , hence there are rays incident from behind the camera and it is not possible to establish a complete mapping to a rectilinear viewport. This leads to a loss of FOV, this is seen via the missing yellow pillars in the corrected image. Secondly, there is an issue of resampling distortion, which is more pronounced near the periphery of the image where a smaller region gets mapped to a larger region.\n\nThe missing FOV can be resolved by multiple linear viewports as shown in Figure 4 (b). However there are issues in the transition region from one plane to another. This can be viewed as a piecewise linear approximation of the fisheye lens manifold. Figure 4 (c) demonstrates a quasilinear correction using a cylindrical viewport, where it is linear in vertical direction and straight vertical objects like pedestrians are preserved. However, there is a quadratic distortion along the horizontal axis. In many scenarios, it provides a reasonable trade-off but it still has limitations. In case of learning algorithms, a parametric transform can be optimized for optimal performance of the target application accuracy.\n\nBecause of fundamental limitations of undistortion, an alternate approach of adapting the algorithm incorporating fisheye model discussed in previous section could be an optimal solution. In case of classical geometric algorithms, an analytical version of non-linear projection can be incorporated. For example, Kukelova et al. [32] extend homography estimation by incorporating radial distortion model. In case of deep learning algorithms, a possible solution could be to train the CNN model to learn the distortion. However, the translation invariance assumption of CNN fundamentally breaks down due to spatially variant distortion and thus it is not efficient to let the network learn it implicitly. This had led to several adaptations of CNN to handle spherical images such as [52] and [9]. However, spherical models do not provide an accurate fit for fisheye lenses and it is an open problem.\n\n\nOverview of WoodScape Dataset\n\n\nHigh-Level Goals\n\nFisheye: One of the main goals of this dataset is to encourage the research community to develop vision algorithms natively on fisheye images without undistortion. There are very few public fisheye datasets and none of them provide semantic segmentation annotation. Fisheye is particularly beneficial to automotive low speed manoeuvring scenarios such as parking [21] where accurate full coverage near field sensing can be achieved with just four cameras.\n\nMulti-camera: Surround view systems have at least four cameras rigidly connected to the body of the car. Pless [42] did pioneering work in deriving a framework for modeling a network of cameras as one, this approach is useful for geometric vision algorithms like visual odometry. However, for semantic segmentation algorithms, there is no literature on joint modeling of rigidly connected cameras.\n\nMulti-task: Autonomous driving has various vision tasks and most of the work has been focused on solving individual tasks independently. However, there is a recent trend [30,53,51,8] to solve tasks using a single multi-task model to enable efficient reuse of encoder features and also Height of the objects is color coded (green for high value, blue for medium value and grayscape for low value).\n\nprovide regularization while learning multiple tasks. However, in these cases, only the encoder is shared and there is no synergy among decoders. Existing datasets are primarily designed to facilitate task-specific learning and they don't provide simultaneous annotation for all the tasks. We have designed our dataset so that simultaneous annotation is provided for various tasks with some exceptions due to practical limitations of optimal dataset design for each task.\n\n\nDataset Acquisition\n\nOur diverse dataset originates from three distinct geographical locations: USA, Europe, and China. While the majority of data was obtained from saloon vehicles there is a significant subset from a sports utility vehicle ensuring a strong mix in sensor mechanical configurations. Driving scenarios are divided across the highway, urban driving and parking use cases. Intrinsic and extrinsic calibrations are provided for all sensors as well as timestamp files to allow synchronization of the data. Relevant vehicle's mechanical data (e.g. wheel circumference, wheel base) are included. High-quality data is ensured via quality checks at all stages of the data collection process. Annotation data undergoes a rigorous quality assurance by highly skilled reviewers. The sensors recorded for this dataset are listed below:\n\u2022 4x 1MPx RGB fisheye cameras (190 \u2022 horizontal FOV) \u2022 1x LiDAR rotating at 20Hz (Velodyne HDL-64E) \u2022 1x GNSS/IMU (NovAtel Propak6 & SPAN-IGM-A1) \u2022 1x GNSS Positioning with SPS (Garmin 18x)\n\u2022 Odometry signals from the vehicle bus.\n\nOur WoodScape dataset provides labels for several autonomous driving tasks including semantic segmentation, monocular depth estimation, object detection (2D & 3D bounding boxes), visual odometry, visual SLAM, motion segmentation, soiling detection and end-to-end driving (driving controls). In Table 1, we compare several properties of popular datasets against WoodScape. In addition to providing fisheye data, we provide data for many more tasks than is typical (nine in total), providing completely novel tasks such as soiled lens detection. Images are provided at 1MPx 24-bit resolution and videos are uncompressed at 30fps ranging in duration from 30s to 120s. The dataset also provides a set of synthetic data using accurate models of the real cameras, enabling investigations of additional tasks. The camera has a HDR sensor with a rolling shutter and a dynamic range of 120 dB. It has features including black level correction, auto-exposure control, auto-gain control, lens shading (optical vignetting) compensation, gamma correction and automatic white balance for color correction. The laser scanner point cloud provided in our data set is accurately preprocessed using a commercial SLAM algorithm to provide a denser point cloud ground truth for tasks such as depth estimation and visual SLAM, as shown in Figure 5. In terms of recognition tasks, we provide labels for forty classes, the distribution of the main classes is shown in Figure 6. Note, that for the purposes of display in this paper, we have merged some of the classes in Figure 6 (e.g. 'two wheelers' is a merge of 'bicycle' and 'motorcycle').\n\n\nDataset Design\n\nThe design of a dataset for machine learning is a very complex task. Unfortunately, due to the overwhelming success of deep learning, recently it does not get as much attention as it still deserves in our opinion. However, at the same time, it was shown that careful inspection of the training sets for outliers improves the robustness of deep neural networks [36], especially with regards to the adversarial examples. Therefore, we believe that whenever a new dataset is released, there should be a significant effort spent not only on the data acquisition but also on the careful consistency check and on the database splitting for the needs of training, model selection and testing.\n\nSampling strategy: Let us define some notation and nam- ing conventions, which we will refer to first (we follow the definitions provided in [4]). A population is a set of all existing feature vectors. A subset of the population collected during some process is called a sample set S. A representative set S * is significantly smaller than S, while capturing most of the information from S (compared to any different subset of the same size), and has low redundancy among the representatives it contains.\n\nIn an ideal world, we would like our training set to be equal to S * . This is extremely difficult to achieve in practice. One approach to approximate this is the concept of the minimal consistent subset of a training set, where, given a training set T , we are interested in a subset T * , being the smallest set such that Acc(T * ) = Acc(T ), where Acc(\u00b7) denotes the selected accuracy measure (e.g. the Jaccard index). Note, that computation of accuracy implies having the ground truth labels. The purpose is to reduce the size of the training set by removing non-informative samples, which do not contribute to improving the learned model, and therefore put some ease on the annotation efforts.\n\nThere are several ways of obtaining T * . One frequently used approach is instance selection [40,35,26]. There are two main groups of instance selection: wrappers and filters. The wrapper based methods use a selection criterion based on the constructed classifier's accuracy. Filter based methods, on the other hand, use a selection criterion which is based on an unrelated selection function. The concept of a minimal consistent subset is crucial for our setup, where we record image data from video cameras. Collecting frames at a frame rate of 30fps, particularly at low speeds, ultimately leads to significant image overlap, therefore, having an effective sampling strategy to distill the dataset is critical. We used a combination of a wrapper method using selection criterion based on the classifier's accuracy [40] and a simple filter based on the image similarity measurement.\n\nData splitting and class balancing: The dataset is split into three chunks in ratio of 6 : 1 : 3, namely training, validation, and testing. For classical algorithms, all the data can be used for testing. As the names suggest, the training part will serve for training purposes only, the validation part can be either joined with the training set (e.g. when the sought model does not require hyper-parameter selection) or be used for model selection, and finally, the testing set is used for model evaluation purposes only. The dataset supports correct hypothesis evaluation [55], therefore multiple splits are provided (5 in total). Depending on the particular task (see Section 4, for the full list), the class imbalance may be an issue [19], therefore, task-specific splits are also provided. Full control of the splitting mechanism is provided allowing for each class to be represented equally within each split (i.e. stratified sampling).\n\nGDPR challenges: The recent General Data Protection Regulation (GDPR) regulation in Europe has given rise to challenges in making our data publicly available. More than one third of our dataset is recorded in Europe and is therefore GDPR sensitive due to visible faces of pedestrians and license plates. There are three primary ways to handle privacy namely (1) Manual blurring, (2) GAN based retargeting and (3) Stringent data-handling license agreement. Blurring is the commonly used approach wherein privacy sensitive regions in the image are manually blurred. There is also the possibility of using GAN based re-targeting wherein faces are exchanged by automatically generated ones [31]. In the recent EuroCity persons dataset [5], the authors argued that any anonymization measure will introduce a bias. Thus they released their dataset with original data and a license agreement which enforces the user to strictly adhere to GDPR. We will follow a similar approach.\n\n\nTasks, Metrics and Baseline experiments\n\nDue to limited space, we briefly describe the metrics and baseline experiments for each task and they are summarized in Table 2. Test dataset for each task consists of 30% of the respective number of annotated samples listed in Table 1. Code is available on WoodScape GitHub and sample video results are shared in supplementary material.\n\n\nSemantic Segmentation\n\nSemantic segmentation networks for autonomous driving [47] have been successfully trained directly on fisheye images in [12,45]. Due to absence of fisheye datasets, they make use of artificially warped images of Cityscapes for training and testing was performed on fisheye images. However, the artificial images cannot increase the originally captured FOV. Our semantic segmentation dataset provides pixel-wise labels for 40 object categories, comparatively Cityscapes dataset [10] provides 30 for example. Figure 6 illustrates the distribution of main classes. We use ENet [41] to generate our baseline results. We fine-tune their model for our dataset by training with categorical cross entropy loss and Adam [29] optimizer. We chose Intersection over Union (IoU) metric [16] to report the baseline results shown in Table 2. We acheive a mean IoU of 51.4 on this test set. Figure 7 shows sample results of segmentation on fisheye images from our test set. The four camera images are treated the same, however it would be interesting to explore customization of the model for each camera. The dataset also provides instance segmentation labels to explore panoptic segmentation models [34].\n\n\n2D Bounding Box Detection\n\nOur 2D object detection dataset is obtained by extracting bounding boxes from instance segmentation labels for 7 different object categories including pedestrians, vehicles, cyclist and motorcyclist. We use Faster R-CNN [43] with ResNet101 [20] as encoder. We initialize the network with ImageNet [11] pre-trained weights. We fine-tune our detection network by training on both KITTI [18] and our object detection datasets. Performance of 2D object detection is reported in terms of mean average precision (mAP) when IoU\u2265 0.5 between predicted and ground truth bounding boxes. We achieve a mAP score of 31 which is significantly less than the accuracy achieved in other datasets. This was expected as bounding box detection is a difficult task on fisheye (the orientation of objects in the periphery of images being very different from central region). To quantify this better, we tested a pre-trained network for person class, and a poor mAP score of 12 was achieved compared to our dataset trained value of 45. Sample results of the fisheye trained model are illustrated in Figure 7. We observe that it is necessary to incorporate the fisheye geometry explicitly, which is an open research problem.\n\n\nCamera Soiling Detection\n\nThe task of soiling detection was to our best knowledge first defined in [56]. Unlike the front camera which is behind the windshield, the surround view cameras are usually directly exposed to the adverse environmental conditions, and thus prone to becoming soiled or water drops forming on the lens. As the functionality of visual perception degrades significantly, detection of soiled cameras is necessary for achieving higher levels of automated driving. As it is a novel task, we discuss it in more detail below.\n\nWe treat the camera soiling detection task as a mixed multilabel-categorical classification problem, i.e. we are interested in a classifier, which jointly classifies a single image with a binary indicator array, where each 0 or 1 corresponds to missing or present class, respectively and simultaneously assigns a categorical label. The classes to detect are {opaque, transparent}. Typically, opaque soiling arises from mud and dust (Figure 8 right image), and transparent soiling arises from water and ice (Figure 8 left image). However, in practice it is common to see water producing \"opaque\" regions in the camera image.\n\nAnnotation for 5k images is performed by drawing polygons to separate soiled from unsoiled regions, so that it can be modeled as a segmentation task if necessary. We evaluate the soiling classifier's performance via an example-based accuracy measure for each task separately, i.e. the average Jaccard index of the testing set: 1 n n i=1 |Yi\u2229Zi| |Yi\u222aZi| , where Y i \u2208 Y = {0, 1} k denotes the label for the i-th testing sample, Z i denotes the classifier's prediction and n denotes the cardinality of the testing set and k the length of the label vector. We use a small baseline network (ResNet10 encoder + 3-layer decoder) and achieved a precision of 84.5% for the multilabel classification. \n\n\n3D Bounding Box Detection\n\n3D box annotation is provided for 10k frames with 3 classes namely 'pedestrian', 'vehicle' and 'cyclist'. In general, 3D IoU [18] is used to evaluate 3D bounding box predictions, but there are drawbacks, especially for rotated objects. Two boxes can reach a good 3D IoU score, while overlapping in total with an opposite heading. Additionally, an exact calculation in 3D space is a time consuming task. To avoid those problems, we introduce a new evaluation metric called Scaling-Rotation-Translation score (SRTs). SRT is based on the idea that two non-overlapping 3D boxes can easily be transformed with respect to each other by using independent rigid transformations: translation S t , rotation S r and scaling S s . Hence, S srt is composed by:\nS s = 1 \u2212 min |1 \u2212 s x | + |1 \u2212 s y | + |1 \u2212 s z | w s , 1 S r = max 0, 1 \u2212 \u03b8 w r \u03c0 S t = max 0, r 1 + r 2 \u2212 t r 1 + r 2 r 1/2 = d 1/2 \u00b7 w t 2 w t , w r , w s \u2208 (0, 1]\nwhere s x,y,z denotes size ratios in x, y, z directions, \u03b8 determines the difference of the yaw angles and t defines the Euclidean distance between the two box centers. S t is calculated with respect to the size of the two objects based on the length of the diagonals d 1/2 of both objects that are used to calculate two radii r 1/2 . Based on the penalty term p t we define the full metric by: S srt = p t \u00b7 (\u03b1 S s + \u03b2 S t + \u03b3 S r ) \u03b1 + \u03b2 + \u03b3 = 1 p t = 0, if r 1 + r 2 < t 1, otherwise w s , w t and w r can be used to prioritize individual properties (e.g. w s \u2192 size, w t \u2192 angle). For our baseline experiments we used w s = 0.3, w t = 1, w r = 0.5, \u03b3 = 0.4 and \u03b1 = \u03b2 = 0.3 to add more weight to the angle, because our experiments have shown that translation or scaling is easier to learn. For baseline, we trained Complex-YOLO [50] for a single class ('car'). We repeated training two times, first optimized on 3D-IoU [18] and second optimized on S srt using a fixed 50:50 split for training and validation. For comparison, we present 3D-IoU, orientation and runtime following [18] on moderate difficulty, see Table 2. Runtime is the average runtime of all box comparisons for each input during training. Even though this comparison uses 3D-IoU, we achieve similar performance for average precision (3D-IoU), with better angle orientation similarity (AOS) and much faster computation time.\n\n\nMonocular Depth Estimation\n\nMonocular Depth estimation is an important task for detecting generic obstacles. We provide more than 100k images of all four cameras (totaling 400k) using ground truth provided by LiDAR. Figure 1 shows a colored example where blue to red indicates the distance for the front camera. As the depth obtained is sparse, we also provide denser point cloud based on SLAM'd static scenes as shown in Figure 5. The ground truth 3D points are projected onto the camera images using our proposed model discussed in Section 2.1. We also apply occlusion correction to handle difference in perspective of LiDAR and camera similar to the method proposed in [33]. We run the semi-supervised approach from [33] using the model proposed by Eigen [14] as baseline on our much larger dataset and obtained an RMSE (Root Mean Square Error) value of 7.7.\n\n\nMotion Segmentation\n\nIn automotive, motion is a strong cue due to ego-motion of the cameras on the moving vehicle and dynamic objects around the vehicle are the critical interacting agents. Additionally, it is helpful to detect generic objects based on  Figure 10: Synthetic images modelling fisheye optics motion cues rather than appearance cues as there will always be rare objects like kangaroos or construction trucks. This has been explored in [49,57,48] for narrow angle cameras. In our dataset, we provide motion masks annotation for moving classes such as vehicles, pedestrians and cyclists for over 10k images. We also provide previous and next images for exploring multi-stream models like MOD-Net [49]. Motion segmentation is treated as a binary segmentation problem and IoU is used as the metric. Using MODNet as baseline network, we achieve an IoU of 45.\n\n\nVisual Odometry/SLAM\n\nVisual Odometry (VO) is necessary for creating a map from the objects detected [38]. We make use of our GNSS and IMU to provide annotation in centimetre level accuracy. The ground truth contains all the six degrees of freedom upto scale and the metric used is percentage of frames within a tolerance level of translation and rotation error. Robustness could be added to the visual odometry by performing a joint estimation from all four cameras. We provide 50 video sequences comprising of over 100k frames with ground truth. The video sequences can also be used for Visual SLAM where we focus on relocalization of a mapped trajectory and the metric is same as VO. We use a fisheye adapted LSD-SLAM [15] as our baseline model as illus-trated in Figure 9 and accuracies are provided in Table 2.\n\n\nSynthetic Data Domain Transfer\n\nSynthetic data is crucial for autonomous driving for many reasons. Firstly, it provides a mechanism to do rigorous corner case testing for diverse scenarios. Secondly, there are legal restrictions like recording videos of a child. Finally, synthetic data is the only way to obtain dense depth and optical flow annotation. There are several popular synthetic datasets like SYNTHIA [44] and CARLA [13]. We will provide a synthetic version of our fisheye surround view dataset, as shown in Figure 10. The main goal is to explore domain transfer from synthetic to real domain for semantic segmentation and depth estimation tasks.\n\n\nEnd-to-End Steering/Braking\n\nBojarski et al. demonstrated end-to-end learning [2] for steering and recently it was applied to fisheye cameras [54]. Although this approach is currently not mature for deployment, it can be either used as a parallel model for redundancy or as an auxiliary task to improve accuracy of other tasks. In the traditional approach, perception is independently designed and it is probably a more complex intermediate problem to solve than what is needed for a small action space driving task. Thus we have added end-to-end steering and braking tasks to encourage modular end-to-end architectures and to explore optimized perception for the control task. The latter is analogous to hand-eye co-ordination of human drivers where perception is optimized for driving.\n\n\nConclusions\n\nIn this paper, we provide an extensive multi-camera fisheye dataset for autonomous driving with annotation for nine tasks. We hope that the release of the dataset encourages development of native fisheye models instead of undistorting fisheye images and applying standard models. In case of deep learning algorithms, it can help understand whether spatial distortion can be learned or it has to be explicitly modeled. In future work, we plan to explore and compare various methods of undistortion and explicit incorporation of fisheye geometry in CNN models. We also plan to design a unified multi-task model for all the listed tasks.\n\nFigure 2 :\n2Sample images from the surround-view camera network showing wide field of view and 360 \u2022 coverage.\n\nFigure 3 :\n3Comparison of fisheye models.\n\nFigure 4 :\n4Undistorting the fisheye image: (a) Rectilinear correction; (b) Piecewise linear correction; (c) Cylindrical correction. Left: raw image; Right: undistorted image.\n\nFigure 5 :\n5SLAM point cloud top-view of a parking lot.\n\nFigure 6 :\n6fence four_wheelers traffic_signs poles and other objects ground_markings lanemarks sidewalk road sky vegetation person four_wheelers_groups four_wheelers_heavy animals rider two_wheelers Distribution of instances of semantic segmentation classes in WoodScape. Minimum size is 300 pixels.\n\nFigure 7 :\n7Qualitative results of Segmentation using ENet[41] (top) and Object detection using Faster RCNN[43] (bottom)\n\nFigure 8 :Figure 9 :\n89Soiling Visual SLAM baseline results (left) based on raw fisheye images (right)\n\nTable 1 :\n1Summary of various autonomous driving datasets containing semantic annotationTask/Info \nQuantity \nKITTI \n[17] \n\nCityscapes \n[10] \n\nMapillary \n[39] \n\nnuScenes \n[6] \n\nApolloScape \n[24] \n\nBDD100k \n[59] \n\nWoodScape \nOurs \n\nCapture Information \n\nYear \n2012/14/15 \n2016 \n2017 \n2018 \n2018 \n2018 \n2018/19 \nState/cities \n1/1 \n2/50 \n50+/100+ \n2/2 \n1/4 \n1/4 \n5+/10+ \n\nOther sensors \n1 LiDAR \nGPS \n-\n-\n\n1 LiDAR \nGPS, IMU \n5 RADAR \n\n2 LiDAR \nGNSS \nIMU \n\n1 GPS \nIMU \n\n1 LiDAR \nGNSS \nIMU \n\nCamera Information \n\nCameras \n4 \n2 \n-\n6 \n6 \n1 \n4 \nTasks \n6 \n1 \n1 \n1 \n4 \n2 \n9 \n\nSegmentation \nClasses \n8 \n30 \n66 \n-\n25 \n40 \n40 \nFrames \n400 \n5k \n25k \n-\n140k \n5.7k \n10k \n\n2D Bounding Box 1 \nClasses \n3 \n-\n-\n-\n-\n10 \n7 \nFrames \n15k \n-\n-\n-\n-\n5.7k \n10k \n\n3D Bounding Box \nClasses \n3 \n-\n-\n25 \n1 \n-\n3 \nFrames \n15k \n-\n-\n40k \n5k+ \n-\n10k \nDepth Estimation \nFrames \n93k \n-\n-\n-\n-\n-\n400k \nMotion Segmentation \nFrames \n1.6k \n-\n-\n-\n-\n-\n10k \nSoiling Detection \nFrames \n-\n-\n-\n-\n-\n-\n5k \nVisual SLAM/Odometry Videos \n33 \n-\n-\n-\n-\n-\n50 \nEnd-to-end Driving \nVideos \n-\n-\n-\n-\n-\n-\n500 \nSynthetic Data \nFrames \n-\n-\n-\n-\n-\n-\n10k \n\n1 2D box annotation can be obtained for other datasets from instance segmentation. \n\n\n\nTable 2 :\n2Summary of results of baseline experiments.Task \nModel \nMetric \nValue \n\nSegmentation \nENet [41] \nIoU \n51.4 \n2D Bounding Box \nFaster R-CNN [43] \nmAP (IoU>0.5) \n31 \nSoiling Detection \nResNet10 [20] \nCategory (%) \n84.5 \nDepth Estimation \nEigen [14] \nRMSE \n7.7 \nMotion Segmentation \nMODNet [49] \nIoU \n45 \n\nVisual Odometry \nResNet50 [20] \nTranslation (<5mm) \n51 \nRotation (<0.1\u00b0) \n71 \nVisual SLAM \nLSD SLAM [15] \nRelocalization (%) \n61 \n3D Bounding Box Detection -Complex YOLO [50] \nMetric for Training \nAP (%) \nAOS (%) \nRuntime (ms) \n\n3D-IoU \n64.38 \n85.60 \n95 \nS srt \n62.46 \n88.43 \n1 \n\n\nAcknowledgementWe would like to thank our colleagues Nivedita Tripathi, Mihai Ilie, Philippe Lafon, Marie Yahiaoui, Sugirtha Thayalan, Jose Luis Fernandez and Pantelis Ermilios for supporting the creation of the dataset, and to thank our partners MightyAI for providing high-quality semantic segmentation annotation services and Next Limit for providing synthetic data using ANYVERSE platform.\nUnifying image plane liftings for central catadioptric and dioptric cameras. P Joao, Barreto, Imaging Beyond the Pinhole CameraJoao P Barreto. Unifying image plane liftings for central catadioptric and dioptric cameras. Imaging Beyond the Pin- hole Camera, pages 21--38, 2006. 2\n\nEnd to end learning for self-driving cars. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, D Lawrence, Mathew Jackel, Urs Monfort, Jiakai Muller, Zhang, arXiv:1604.07316arXiv preprintMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016. 8\n\nA wide angle lens for cloud recording. The London. Wn, Bond, Journal of Science. 44263WN Bond. A wide angle lens for cloud recording. The Lon- don, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 44(263):999-1001, 1922. 1\n\nSelecting representative data sets. Tomas Borovicka, Marcel Jirina Jr, Pavel Kordik, Marcel Jirina, Advances in Data Mining Knowledge Discovery and Applications. Adem KarahocaIntechOpen, RijekaTomas Borovicka, Marcel Jirina Jr., Pavel Kordik, and Mar- cel Jirina. Selecting representative data sets. In Adem Kara- hoca, editor, Advances in Data Mining Knowledge Discovery and Applications, chapter 2. IntechOpen, Rijeka, 2012. 5\n\nEuroCity persons: A novel benchmark for person detection in traffic scenes. Markus Braun, Sebastian Krebs, Fabian Flohr, Dariu Gavrila, IEEE transactions. 6Markus Braun, Sebastian Krebs, Fabian Flohr, and Dariu Gavrila. EuroCity persons: A novel benchmark for person detection in traffic scenes. IEEE transactions on pattern analysis and machine intelligence, 2019. 6\n\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom, arXiv:1903.11027nuScenes: A multimodal dataset for autonomous driving. arXiv preprintHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A mul- timodal dataset for autonomous driving. arXiv preprint arXiv:1903.11027, 2019. 5\n\nLargescale direct SLAM for omnidirectional cameras. David Caruso, Jakob Engel, Daniel Cremers, Proceedinsg of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). eedinsg of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)IEEE1David Caruso, Jakob Engel, and Daniel Cremers. Large- scale direct SLAM for omnidirectional cameras. In Proceed- insg of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 141-148. IEEE, 2015. 1, 2\n\nAuxnet: Auxiliary tasks enhanced semantic segmentation for automated driving. Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, Samir Rawashdeh, Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP). the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP)Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, and Samir Rawashdeh. Auxnet: Auxiliary tasks enhanced se- mantic segmentation for automated driving. In Proceedings of the 14th International Joint Conference on Computer Vi- sion, Imaging and Computer Graphics Theory and Applica- tions (VISAPP), pages 645-652, 2019. 3\n\nSphereNet: Learning spherical representations for detection and classification in omnidirectional images. Benjamin Coors, Alexandru Paul Condurache, Andreas Geiger, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)13Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. SphereNet: Learning spherical representations for detection and classification in omnidirectional images. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 518-533, 2018. 1, 3\n\nThe Cityscapes Dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)6Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for semantic urban scene understanding. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3213-3223, 2016. 2, 5, 6\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 248- 255. Ieee, 2009. 6\n\nCNN based semantic segmentation for urban traffic scenes using fisheye camera. Liuyuan Deng, Ming Yang, Yeqiang Qian, Chunxiang Wang, Bing Wang, 2017 IEEE Intelligent Vehicles Symposium (IV). IEEELiuyuan Deng, Ming Yang, Yeqiang Qian, Chunxiang Wang, and Bing Wang. CNN based semantic segmentation for ur- ban traffic scenes using fisheye camera. In 2017 IEEE Intel- ligent Vehicles Symposium (IV), pages 231-236. IEEE, 2017. 6\n\nCARLA: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Proceedings of the 1st Annual Conference on Robot Learning. the 1st Annual Conference on Robot LearningAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 1-16, 2017. 8\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision67David Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale con- volutional architecture. In Proceedings of the IEEE inter- national conference on computer vision, pages 2650-2658, 2015. 6, 7\n\nLSD-SLAM: Large-scale direct monocular SLAM. Jakob Engel, Thomas Sch\u00f6ps, Daniel Cremers, European conference on computer vision. Springer6Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. LSD- SLAM: Large-scale direct monocular SLAM. In European conference on computer vision, pages 834-849. Springer, 2014. 6, 8\n\nThe PASCAL visual object classes (VOC) challenge. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International Journal of Computer Vision. 882Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The PASCAL visual ob- ject classes (VOC) challenge. International Journal of Com- puter Vision, 88(2):303-338, 2010. 6\n\nVision meets robotics: The KITTI dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, The International Journal of Robotics Research. 32115Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, 32(11):1231- 1237, 2013. 2, 5\n\nAre we ready for Autonomous Driving? The KITTI Vision Benchmark Suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)67Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for Autonomous Driving? The KITTI Vision Bench- mark Suite. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2012. 6, 7\n\nOn the class imbalance problem. Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, Guang-Tong Zhou, Proceedings of the Fourth International Conference on Natural Computation (ICNC). the Fourth International Conference on Natural Computation (ICNC)Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, and Guang-Tong Zhou. On the class imbalance problem. In Proceedings of the Fourth International Conference on Nat- ural Computation (ICNC), pages 192-201, 2008. 5\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016. 6\n\nComputer vision in automated parking systems: Design, implementation and challenges. Markus Heimberger, Jonathan Horgan, Ciar\u00e1n Hughes, John Mcdonald, Senthil Yogamani, Image and Vision Computing. 683Markus Heimberger, Jonathan Horgan, Ciar\u00e1n Hughes, John McDonald, and Senthil Yogamani. Computer vision in au- tomated parking systems: Design, implementation and chal- lenges. Image and Vision Computing, 68:88-101, 2017. 3\n\nArea projections of fisheye photographic lenses. J Thomas, Herbert, Agricultural and Forest Meteorology. 392-3Thomas J Herbert. Area projections of fisheye photographic lenses. Agricultural and Forest Meteorology, 39(2-3):215- 223, 1987. 2\n\nVision-based driver assistance systems: Survey, taxonomy and advances. Jonathan Horgan, Ciar\u00e1n Hughes, John Mcdonald, Senthil Yogamani, 2015 IEEE 18th International Conference on Intelligent Transportation Systems. IEEEJonathan Horgan, Ciar\u00e1n Hughes, John McDonald, and Senthil Yogamani. Vision-based driver assistance systems: Survey, taxonomy and advances. In 2015 IEEE 18th Inter- national Conference on Intelligent Transportation Systems, pages 2032-2039. IEEE, 2015. 1\n\nThe ApolloScape dataset for autonomous driving. Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu Zhou, Peng Wang, Yuanqing Lin, Ruigang Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops25Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang. The ApolloScape dataset for autonomous driving. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 954-960, 2018. 2, 5\n\nAccuracy of fish-eye lens models. Ciar\u00e1n Hughes, Patrick Denny, Edward Jones, Martin Glavin, Applied Optics. 4917Ciar\u00e1n Hughes, Patrick Denny, Edward Jones, and Martin Glavin. Accuracy of fish-eye lens models. Applied Optics, 49(17):3338-3347, 2010. 2\n\nComparison of instances selection algorithms I. Algorithms survey. Norbert Jankowski, Marek Grochowski, Proceedings of the 7th International Conference on Artificial Intelligence and Soft Computing ICAISC. the 7th International Conference on Artificial Intelligence and Soft Computing ICAISCNorbert Jankowski and Marek Grochowski. Comparison of instances selection algorithms I. Algorithms survey. In Pro- ceedings of the 7th International Conference on Artificial Intelligence and Soft Computing ICAISC, pages 598-603, 2004. 5\n\nAn enhanced unified camera model. Bogdan Khomutenko, Gaetan Garcia, Philippe Martinet, IEEE Robotics and Automation Letters. 11Bogdan Khomutenko, Gaetan Garcia, and Philippe Martinet. An enhanced unified camera model. IEEE Robotics and Au- tomation Letters, 1(1):137-144, 2016. 2\n\nFisheye lens camera based surveillance system for wide field of view monitoring. Hyungtae Kim, Jaehoon Jung, Joonki Paik, Optik. 12714Hyungtae Kim, Jaehoon Jung, and Joonki Paik. Fisheye lens camera based surveillance system for wide field of view monitoring. Optik, 127(14):5636-5646, 2016. 1\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. 6\n\nUbernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. Iasonas Kokkinos, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIasonas Kokkinos. Ubernet: Training a universal convolu- tional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6129-6138, 2017. 3\n\nDeepfakes: a new threat to face recognition? assessment and detection. Pavel Korshunov, S\u00e9bastien Marcel, arXiv:1812.08685arXiv preprintPavel Korshunov and S\u00e9bastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018. 6\n\nRadial distortion homography. Zuzana Kukelova, Jan Heller, Martin Bujnak, Tomas Pajdla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Zuzana Kukelova, Jan Heller, Martin Bujnak, and Tomas Pa- jdla. Radial distortion homography. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 639-647, 2015. 3\n\nNear-field depth estimation using monocular fisheye camera: A semi-supervised learning approach using sparse LiDAR data. Stefan Varun Ravi Kumar, Christian Milz, Martin Witt, Karl Simon, Johannes Amende, Senthil Petzold, Timo Yogamani, Pech, CVPR Workshop. Varun Ravi Kumar, Stefan Milz, Christian Witt, Martin Si- mon, Karl Amende, Johannes Petzold, Senthil Yogamani, and Timo Pech. Near-field depth estimation using monoc- ular fisheye camera: A semi-supervised learning approach using sparse LiDAR data. In CVPR Workshop, 2018. 7\n\nWeaklyand semi-supervised panoptic segmentation. Qizhu Li, Anurag Arnab, Philip Hs Torr, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Qizhu Li, Anurag Arnab, and Philip HS Torr. Weakly- and semi-supervised panoptic segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 102-118, 2018. 6\n\nOn issues of instance selection. Huan Liu, Hiroshi Motoda, Data Mining and Knowledge Discovery. 62Huan Liu and Hiroshi Motoda. On issues of instance selec- tion. Data Mining and Knowledge Discovery, 6(2):115-130, 2002. 5\n\nLess is more: Culling the training set to improve robustness of deep neural networks. Yongshuai Liu, Jiyu Chen, Hao Chen, Proceedings of the 9th International Conference on Decision and Game Theory for Security (GameSec). the 9th International Conference on Decision and Game Theory for Security (GameSec)Yongshuai Liu, Jiyu Chen, and Hao Chen. Less is more: Culling the training set to improve robustness of deep neural networks. In In Proceedings of the 9th International Confer- ence on Decision and Game Theory for Security (GameSec), pages 102-114, 2018. 4\n\n1 year, 1000 km: The Oxford RobotCar dataset. Will Maddern, Geoffrey Pascoe, Chris Linegar, Paul Newman, The International Journal of Robotics Research. 361Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford RobotCar dataset. The International Journal of Robotics Research, 36(1):3-15, 2017. 2\n\nBassam Abdallah, and Senthil Yogamani. Visual slam for automated driving: Exploring the applications of deep learning. Stefan Milz, Georg Arbeiter, Christian Witt, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsStefan Milz, Georg Arbeiter, Christian Witt, Bassam Abdal- lah, and Senthil Yogamani. Visual slam for automated driv- ing: Exploring the applications of deep learning. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 247-257, 2018. 8\n\nThe Mapillary Vistas dataset for semantic understanding of street scenes. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, Peter Kontschieder, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)25Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The Mapillary Vistas dataset for seman- tic understanding of street scenes. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4990-4999, 2017. 2, 5\n\nA review of instance selection methods. Arturo Jos\u00e9, Jes\u00fas Ariel Olvera-L\u00f3pez, Jos&apos;e Francisco Mart\u00ednez Carrasco-Ochoa, Josef Trinidad, Kittler, Artificial Intelligence Review. 342Jos\u00e9 Arturo Olvera-L\u00f3pez, Jes\u00fas Ariel Carrasco-Ochoa, Jos'e Francisco Mart\u00ednez Trinidad, and Josef Kittler. A re- view of instance selection methods. Artificial Intelligence Review, 34(2):133-143, 2010. 5\n\nENet: A deep neural network architecture for real-time semantic segmentation. Adam Paszke, Abhishek Chaurasia, Sangpil Kim, Eugenio Culurciello, 67Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Euge- nio Culurciello. ENet: A deep neural network architecture for real-time semantic segmentation, 2016. 6, 7\n\nUsing many cameras as one. Robert Pless, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Robert Pless. Using many cameras as one. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2003. 3\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in Neural Information Processing Systems. 67Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with re- gion proposal networks. In Advances in Neural Information Processing Systems, pages 91-99, 2015. 6, 7\n\nThe SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M Lopez, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmenta- tion of urban scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3234-3243, 2016. 8\n\nCNN-based fisheye image real-time semantic segmentation. Alvaro S\u00e1ez, M Luis, Eduardo Bergasa, Elena Romeral, Rafael L\u00f3pez, Rafael Barea, Sanz, IEEE Intelligent Vehicles Symposium (IV). IEEEAlvaro S\u00e1ez, Luis M Bergasa, Eduardo Romeral, Elena L\u00f3pez, Rafael Barea, and Rafael Sanz. CNN-based fisheye image real-time semantic segmentation. In 2018 IEEE In- telligent Vehicles Symposium (IV), pages 1039-1044. IEEE, 2018. 6\n\nAugmented reality: principles and practice. Dieter Schmalstieg, Tobias Hollerer, Addison-Wesley ProfessionalDieter Schmalstieg and Tobias Hollerer. Augmented reality: principles and practice. Addison-Wesley Professional, 2016. 1\n\nDeep semantic segmentation for automated driving: Taxonomy, roadmap and challenges. Mennatullah Siam, Sara Elkerdawy, Martin Jagersand, Senthil Yogamani, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEEMennatullah Siam, Sara Elkerdawy, Martin Jagersand, and Senthil Yogamani. Deep semantic segmentation for auto- mated driving: Taxonomy, roadmap and challenges. In 2017 IEEE 20th International Conference on Intelligent Trans- portation Systems (ITSC), pages 1-8. IEEE, 2017. 6\n\nRTSeg: Realtime semantic segmentation comparative study. Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand, 25th IEEE International Conference on Image Processing (ICIP. IEEEMennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, and Martin Jagersand. RTSeg: Real- time semantic segmentation comparative study. In 2018 25th IEEE International Conference on Image Processing (ICIP). IEEE, 2018. 8\n\nMODNet: Motion and appearance based moving object detection network for autonomous driving. Mennatullah Siam, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani, Martin Jagersand, Ahmad El-Sallab, Proceedings of the 21st International Conference on Intelligent Transportation Systems (ITSC). the 21st International Conference on Intelligent Transportation Systems (ITSC)6Mennatullah Siam, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani, Martin Jagersand, and Ahmad El-Sallab. MODNet: Motion and appearance based moving object de- tection network for autonomous driving. In Proceedings of the 21st International Conference on Intelligent Transporta- tion Systems (ITSC), pages 2859-2864, 2018. 6, 8\n\nComplex-YOLO: An Euler-region-proposal for real-time 3D object detection on point clouds. Martin Simon, Stefan Milz, Karl Amende, Horst-Michael Gross, The European Conference on Computer Vision (ECCV) Workshops. 67Martin Simon, Stefan Milz, Karl Amende, and Horst- Michael Gross. Complex-YOLO: An Euler-region-proposal for real-time 3D object detection on point clouds. In The Eu- ropean Conference on Computer Vision (ECCV) Workshops, September 2018. 6, 7\n\nNeurAll: Towards a unified model for visual perception in automated driving. Ganesh Sistu, Isabelle Leang, Sumanth Chennupati, Stefan Milz, Senthil Yogamani, Samir Rawashdeh, International Conference on Intelligent Transportation Systems (ITSC). IEEEGanesh Sistu, Isabelle Leang, Sumanth Chennupati, Stefan Milz, Senthil Yogamani, and Samir Rawashdeh. NeurAll: Towards a unified model for visual perception in automated driving. In International Conference on Intelligent Trans- portation Systems (ITSC). IEEE, 2019. 3\n\nKernel transformer networks for compact spherical convolution. Yu-Chuan Su, Kristen Grauman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYu-Chuan Su and Kristen Grauman. Kernel transformer net- works for compact spherical convolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9442-9451, 2019. 3\n\nMultiNet: Real-time joint semantic reasoning for autonomous driving. M Teichmann, M Weber, M Z\u00f6llner, R Cipolla, R Urtasun, Proceedings of the IEEE Intelligent Vehicles Symposium (IV). the IEEE Intelligent Vehicles Symposium (IV)M. Teichmann, M. Weber, M. Z\u00f6llner, R. Cipolla, and R. Urtasun. MultiNet: Real-time joint semantic reasoning for autonomous driving. In Proceedings of the IEEE Intelligent Vehicles Symposium (IV), pages 1013-1020, June 2018. 3\n\nEnd to end vehicle lateral control using a single fisheye camera. Marin Toromanoff, Emilie Wirbel, Fr\u00e9d\u00e9ric Wilhelm, Camilo Vejarano, Xavier Perrotton, Fabien Moutarde, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)Marin Toromanoff, Emilie Wirbel, Fr\u00e9d\u00e9ric Wilhelm, Camilo Vejarano, Xavier Perrotton, and Fabien Moutarde. End to end vehicle lateral control using a single fisheye cam- era. In Proceedings of the IEEE/RSJ International Confer- ence on Intelligent Robots and Systems (IROS), pages 3613- 3619, 2018. 8\n\nChallenges in designing datasets and validation for autonomous driving. Michal U\u0159i\u010d\u00e1\u0159, David Hurych, Pavel K\u0159\u00ed\u017eek, Senthil Yogamani, Proceedings of the 14th International Joint Conference on Computer Vision. the 14th International Joint Conference on Computer VisionSciTePress5INSTICCMichal U\u0159i\u010d\u00e1\u0159, David Hurych, Pavel K\u0159\u00ed\u017eek, and Senthil Yo- gamani. Challenges in designing datasets and validation for autonomous driving. In Proceedings of the 14th Interna- tional Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications -Volume 5: VISAPP,, pages 653-659. INSTICC, SciTePress, 2019. 5\n\nMichal U\u0159i\u010d\u00e1\u0159, Pavel K\u0159\u00ed\u017eek, David Hurych, Ibrahim Sobh, Senthil Yogamani, Patrick Denny Yes, Gan, applying adversarial techniques for autonomous driving. Electronic Imaging. Michal U\u0159i\u010d\u00e1\u0159, Pavel K\u0159\u00ed\u017eek, David Hurych, Ibrahim Sobh, Senthil Yogamani, and Patrick Denny. Yes, we GAN: ap- plying adversarial techniques for autonomous driving. Elec- tronic Imaging, 2019(15):48-1-48-17, 2019. 6\n\nSmsnet: Semantic motion segmentation using deep convolutional neural networks. Johan Vertens, Abhinav Valada, Wolfram Burgard, Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS). the IEEE International Conference on Intelligent Robots and Systems (IROS)Vancouver, CanadaJohan Vertens, Abhinav Valada, and Wolfram Burgard. Sm- snet: Semantic motion segmentation using deep convolu- tional neural networks. In Proceedings of the IEEE Interna- tional Conference on Intelligent Robots and Systems (IROS), Vancouver, Canada, 2017. 8\n\nFish-eye views, and vision under water. The London, Edinburgh, and Dublin Philosophical Magazine and. W Robert, Wood, Journal of Science. 1268Robert W Wood. Fish-eye views, and vision under water. The London, Edinburgh, and Dublin Philosophical Maga- zine and Journal of Science, 12(68):159-162, 1906. 1\n\nBDD100K: A diverse driving video database with scalable annotation tooling. Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, Trevor Darrell, arXiv:1805.0468725arXiv preprintFisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, and Trevor Darrell. BDD100K: A diverse driving video database with scalable annotation tool- ing. arXiv preprint arXiv:1805.04687, 2018. 2, 5\n", "annotations": {"author": "[{\"end\":97,\"start\":80},{\"end\":112,\"start\":98},{\"end\":129,\"start\":113},{\"end\":143,\"start\":130},{\"end\":159,\"start\":144},{\"end\":177,\"start\":160},{\"end\":192,\"start\":178},{\"end\":205,\"start\":193},{\"end\":219,\"start\":206},{\"end\":232,\"start\":220},{\"end\":248,\"start\":233},{\"end\":262,\"start\":249},{\"end\":282,\"start\":263},{\"end\":297,\"start\":283},{\"end\":313,\"start\":298},{\"end\":331,\"start\":314},{\"end\":346,\"start\":332}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":88},{\"end\":111,\"start\":105},{\"end\":128,\"start\":122},{\"end\":142,\"start\":137},{\"end\":158,\"start\":152},{\"end\":176,\"start\":166},{\"end\":191,\"start\":185},{\"end\":204,\"start\":200},{\"end\":218,\"start\":213},{\"end\":231,\"start\":225},{\"end\":247,\"start\":243},{\"end\":261,\"start\":255},{\"end\":281,\"start\":271},{\"end\":296,\"start\":291},{\"end\":312,\"start\":305},{\"end\":330,\"start\":321},{\"end\":345,\"start\":340}]", "author_first_name": "[{\"end\":87,\"start\":80},{\"end\":104,\"start\":98},{\"end\":121,\"start\":113},{\"end\":136,\"start\":130},{\"end\":151,\"start\":144},{\"end\":165,\"start\":160},{\"end\":184,\"start\":178},{\"end\":199,\"start\":193},{\"end\":212,\"start\":206},{\"end\":224,\"start\":220},{\"end\":242,\"start\":233},{\"end\":254,\"start\":249},{\"end\":270,\"start\":263},{\"end\":290,\"start\":283},{\"end\":304,\"start\":298},{\"end\":320,\"start\":314},{\"end\":339,\"start\":332}]", "author_affiliation": null, "title": "[{\"end\":77,\"start\":1},{\"end\":423,\"start\":347}]", "venue": null, "abstract": "[{\"end\":1802,\"start\":559}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2233,\"start\":2229},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2380,\"start\":2377},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2607,\"start\":2603},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2634,\"start\":2630},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2699,\"start\":2695},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3248,\"start\":3245},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3263,\"start\":3260},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3404,\"start\":3400},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3718,\"start\":3714},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3846,\"start\":3842},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3928,\"start\":3924},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3986,\"start\":3982},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4003,\"start\":3999},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6625,\"start\":6621},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6759,\"start\":6756},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6761,\"start\":6759},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6790,\"start\":6786},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6876,\"start\":6872},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10302,\"start\":10298},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10755,\"start\":10751},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10763,\"start\":10760},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11287,\"start\":11283},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11492,\"start\":11488},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11950,\"start\":11946},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11953,\"start\":11950},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11956,\"start\":11953},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11958,\"start\":11956},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15721,\"start\":15717},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16188,\"start\":16185},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17347,\"start\":17343},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17350,\"start\":17347},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17353,\"start\":17350},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18071,\"start\":18067},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18714,\"start\":18710},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18878,\"start\":18874},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19770,\"start\":19766},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19814,\"start\":19811},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20516,\"start\":20512},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20582,\"start\":20578},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20585,\"start\":20582},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20939,\"start\":20935},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21036,\"start\":21032},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21173,\"start\":21169},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21235,\"start\":21231},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21647,\"start\":21643},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21902,\"start\":21898},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21922,\"start\":21918},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21979,\"start\":21975},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22066,\"start\":22062},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":22984,\"start\":22980},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24901,\"start\":24897},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":26524,\"start\":26520},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26615,\"start\":26611},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26774,\"start\":26770},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27761,\"start\":27757},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27808,\"start\":27804},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27847,\"start\":27843},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28402,\"start\":28398},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28405,\"start\":28402},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28408,\"start\":28405},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28661,\"start\":28657},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28924,\"start\":28920},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29544,\"start\":29540},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30053,\"start\":30049},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30068,\"start\":30064},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30378,\"start\":30375},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30443,\"start\":30439},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32489,\"start\":32485},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32538,\"start\":32534}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31846,\"start\":31735},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31889,\"start\":31847},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32066,\"start\":31890},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32123,\"start\":32067},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32425,\"start\":32124},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32547,\"start\":32426},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32651,\"start\":32548},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33825,\"start\":32652},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34420,\"start\":33826}]", "paragraph": "[{\"end\":2491,\"start\":1818},{\"end\":3453,\"start\":2493},{\"end\":4308,\"start\":3455},{\"end\":4707,\"start\":4310},{\"end\":5098,\"start\":4709},{\"end\":5615,\"start\":5141},{\"end\":6191,\"start\":5617},{\"end\":7095,\"start\":6217},{\"end\":7636,\"start\":7097},{\"end\":8300,\"start\":7638},{\"end\":9250,\"start\":8344},{\"end\":9968,\"start\":9252},{\"end\":10867,\"start\":9970},{\"end\":11375,\"start\":10920},{\"end\":11774,\"start\":11377},{\"end\":12172,\"start\":11776},{\"end\":12645,\"start\":12174},{\"end\":13487,\"start\":12669},{\"end\":13718,\"start\":13678},{\"end\":15338,\"start\":13720},{\"end\":16042,\"start\":15357},{\"end\":16548,\"start\":16044},{\"end\":17248,\"start\":16550},{\"end\":18134,\"start\":17250},{\"end\":19078,\"start\":18136},{\"end\":20051,\"start\":19080},{\"end\":20432,\"start\":20095},{\"end\":21648,\"start\":20458},{\"end\":22878,\"start\":21678},{\"end\":23423,\"start\":22907},{\"end\":24048,\"start\":23425},{\"end\":24742,\"start\":24050},{\"end\":25520,\"start\":24772},{\"end\":27082,\"start\":25689},{\"end\":27946,\"start\":27113},{\"end\":28816,\"start\":27970},{\"end\":29634,\"start\":28841},{\"end\":30294,\"start\":29669},{\"end\":31084,\"start\":30326},{\"end\":31734,\"start\":31100}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13677,\"start\":13488},{\"attributes\":{\"id\":\"formula_1\"},\"end\":25688,\"start\":25521}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4255,\"start\":4248},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14021,\"start\":14014},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20222,\"start\":20215},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20330,\"start\":20323},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21283,\"start\":21276},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26810,\"start\":26803},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29633,\"start\":29626}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1816,\"start\":1804},{\"attributes\":{\"n\":\"2.\"},\"end\":5139,\"start\":5101},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6215,\"start\":6194},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8342,\"start\":8303},{\"attributes\":{\"n\":\"3.\"},\"end\":10899,\"start\":10870},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10918,\"start\":10902},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12667,\"start\":12648},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15355,\"start\":15341},{\"attributes\":{\"n\":\"4.\"},\"end\":20093,\"start\":20054},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20456,\"start\":20435},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21676,\"start\":21651},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22905,\"start\":22881},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24770,\"start\":24745},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27111,\"start\":27085},{\"attributes\":{\"n\":\"4.6.\"},\"end\":27968,\"start\":27949},{\"attributes\":{\"n\":\"4.7.\"},\"end\":28839,\"start\":28819},{\"attributes\":{\"n\":\"4.8.\"},\"end\":29667,\"start\":29637},{\"attributes\":{\"n\":\"4.9.\"},\"end\":30324,\"start\":30297},{\"attributes\":{\"n\":\"5.\"},\"end\":31098,\"start\":31087},{\"end\":31746,\"start\":31736},{\"end\":31858,\"start\":31848},{\"end\":31901,\"start\":31891},{\"end\":32078,\"start\":32068},{\"end\":32135,\"start\":32125},{\"end\":32437,\"start\":32427},{\"end\":32569,\"start\":32549},{\"end\":32662,\"start\":32653},{\"end\":33836,\"start\":33827}]", "table": "[{\"end\":33825,\"start\":32741},{\"end\":34420,\"start\":33881}]", "figure_caption": "[{\"end\":31846,\"start\":31748},{\"end\":31889,\"start\":31860},{\"end\":32066,\"start\":31903},{\"end\":32123,\"start\":32080},{\"end\":32425,\"start\":32137},{\"end\":32547,\"start\":32439},{\"end\":32651,\"start\":32572},{\"end\":32741,\"start\":32664},{\"end\":33881,\"start\":33838}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3573,\"start\":3565},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5424,\"start\":5416},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7731,\"start\":7723},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8774,\"start\":8766},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9333,\"start\":9325},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9509,\"start\":9501},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15045,\"start\":15037},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15172,\"start\":15164},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15274,\"start\":15266},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20973,\"start\":20965},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21341,\"start\":21333},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22762,\"start\":22754},{\"end\":23866,\"start\":23857},{\"end\":23952,\"start\":23931},{\"end\":27309,\"start\":27301},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27515,\"start\":27507},{\"end\":28212,\"start\":28203},{\"end\":29594,\"start\":29586},{\"end\":30165,\"start\":30156}]", "bib_author_first_name": "[{\"end\":34893,\"start\":34892},{\"end\":35145,\"start\":35138},{\"end\":35162,\"start\":35156},{\"end\":35166,\"start\":35163},{\"end\":35180,\"start\":35174},{\"end\":35202,\"start\":35194},{\"end\":35215,\"start\":35211},{\"end\":35230,\"start\":35223},{\"end\":35239,\"start\":35238},{\"end\":35256,\"start\":35250},{\"end\":35268,\"start\":35265},{\"end\":35284,\"start\":35278},{\"end\":35868,\"start\":35863},{\"end\":35886,\"start\":35880},{\"end\":35893,\"start\":35887},{\"end\":35903,\"start\":35898},{\"end\":35918,\"start\":35912},{\"end\":36339,\"start\":36333},{\"end\":36356,\"start\":36347},{\"end\":36370,\"start\":36364},{\"end\":36383,\"start\":36378},{\"end\":36632,\"start\":36626},{\"end\":36646,\"start\":36641},{\"end\":36660,\"start\":36656},{\"end\":36662,\"start\":36661},{\"end\":36676,\"start\":36669},{\"end\":36689,\"start\":36683},{\"end\":36694,\"start\":36690},{\"end\":36707,\"start\":36702},{\"end\":36717,\"start\":36712},{\"end\":36730,\"start\":36728},{\"end\":36745,\"start\":36736},{\"end\":36759,\"start\":36754},{\"end\":37157,\"start\":37152},{\"end\":37171,\"start\":37166},{\"end\":37185,\"start\":37179},{\"end\":37703,\"start\":37696},{\"end\":37722,\"start\":37716},{\"end\":37737,\"start\":37730},{\"end\":37753,\"start\":37748},{\"end\":38463,\"start\":38455},{\"end\":38480,\"start\":38471},{\"end\":38505,\"start\":38498},{\"end\":38967,\"start\":38961},{\"end\":38983,\"start\":38976},{\"end\":39000,\"start\":38991},{\"end\":39012,\"start\":39008},{\"end\":39028,\"start\":39022},{\"end\":39047,\"start\":39040},{\"end\":39061,\"start\":39058},{\"end\":39076,\"start\":39070},{\"end\":39088,\"start\":39083},{\"end\":39637,\"start\":39634},{\"end\":39647,\"start\":39644},{\"end\":39661,\"start\":39654},{\"end\":39676,\"start\":39670},{\"end\":39684,\"start\":39681},{\"end\":39691,\"start\":39689},{\"end\":40192,\"start\":40185},{\"end\":40203,\"start\":40199},{\"end\":40217,\"start\":40210},{\"end\":40233,\"start\":40224},{\"end\":40244,\"start\":40240},{\"end\":40581,\"start\":40575},{\"end\":40601,\"start\":40595},{\"end\":40613,\"start\":40607},{\"end\":40632,\"start\":40625},{\"end\":40647,\"start\":40640},{\"end\":41081,\"start\":41076},{\"end\":41092,\"start\":41089},{\"end\":41517,\"start\":41512},{\"end\":41531,\"start\":41525},{\"end\":41546,\"start\":41540},{\"end\":41833,\"start\":41829},{\"end\":41849,\"start\":41846},{\"end\":41861,\"start\":41860},{\"end\":41863,\"start\":41862},{\"end\":41881,\"start\":41877},{\"end\":41898,\"start\":41892},{\"end\":42219,\"start\":42212},{\"end\":42234,\"start\":42228},{\"end\":42250,\"start\":42241},{\"end\":42266,\"start\":42260},{\"end\":42596,\"start\":42589},{\"end\":42611,\"start\":42605},{\"end\":42624,\"start\":42618},{\"end\":43055,\"start\":43048},{\"end\":43067,\"start\":43061},{\"end\":43080,\"start\":43073},{\"end\":43095,\"start\":43087},{\"end\":43112,\"start\":43102},{\"end\":43537,\"start\":43530},{\"end\":43549,\"start\":43542},{\"end\":43565,\"start\":43557},{\"end\":43575,\"start\":43571},{\"end\":44043,\"start\":44037},{\"end\":44064,\"start\":44056},{\"end\":44079,\"start\":44073},{\"end\":44092,\"start\":44088},{\"end\":44110,\"start\":44103},{\"end\":44427,\"start\":44426},{\"end\":44697,\"start\":44689},{\"end\":44712,\"start\":44706},{\"end\":44725,\"start\":44721},{\"end\":44743,\"start\":44736},{\"end\":45146,\"start\":45141},{\"end\":45161,\"start\":45154},{\"end\":45176,\"start\":45169},{\"end\":45189,\"start\":45183},{\"end\":45201,\"start\":45195},{\"end\":45212,\"start\":45208},{\"end\":45227,\"start\":45219},{\"end\":45240,\"start\":45233},{\"end\":45750,\"start\":45744},{\"end\":45766,\"start\":45759},{\"end\":45780,\"start\":45774},{\"end\":45794,\"start\":45788},{\"end\":46037,\"start\":46030},{\"end\":46054,\"start\":46049},{\"end\":46532,\"start\":46526},{\"end\":46551,\"start\":46545},{\"end\":46568,\"start\":46560},{\"end\":46862,\"start\":46854},{\"end\":46875,\"start\":46868},{\"end\":46888,\"start\":46882},{\"end\":47113,\"start\":47112},{\"end\":47129,\"start\":47124},{\"end\":47375,\"start\":47368},{\"end\":47871,\"start\":47866},{\"end\":47892,\"start\":47883},{\"end\":48118,\"start\":48112},{\"end\":48132,\"start\":48129},{\"end\":48147,\"start\":48141},{\"end\":48161,\"start\":48156},{\"end\":48661,\"start\":48655},{\"end\":48689,\"start\":48680},{\"end\":48702,\"start\":48696},{\"end\":48713,\"start\":48709},{\"end\":48729,\"start\":48721},{\"end\":48745,\"start\":48738},{\"end\":48759,\"start\":48755},{\"end\":49122,\"start\":49117},{\"end\":49133,\"start\":49127},{\"end\":49150,\"start\":49141},{\"end\":49497,\"start\":49493},{\"end\":49510,\"start\":49503},{\"end\":49777,\"start\":49768},{\"end\":49787,\"start\":49783},{\"end\":49797,\"start\":49794},{\"end\":50295,\"start\":50291},{\"end\":50313,\"start\":50305},{\"end\":50327,\"start\":50322},{\"end\":50341,\"start\":50337},{\"end\":50704,\"start\":50698},{\"end\":50716,\"start\":50711},{\"end\":50736,\"start\":50727},{\"end\":51271,\"start\":51264},{\"end\":51287,\"start\":51281},{\"end\":51303,\"start\":51297},{\"end\":51308,\"start\":51304},{\"end\":51320,\"start\":51315},{\"end\":51777,\"start\":51771},{\"end\":51789,\"start\":51784},{\"end\":51795,\"start\":51790},{\"end\":51839,\"start\":51810},{\"end\":51861,\"start\":51856},{\"end\":52204,\"start\":52200},{\"end\":52221,\"start\":52213},{\"end\":52240,\"start\":52233},{\"end\":52253,\"start\":52246},{\"end\":52465,\"start\":52459},{\"end\":52856,\"start\":52849},{\"end\":52875,\"start\":52871},{\"end\":52884,\"start\":52880},{\"end\":53278,\"start\":53272},{\"end\":53289,\"start\":53284},{\"end\":53305,\"start\":53299},{\"end\":53324,\"start\":53319},{\"end\":53343,\"start\":53334},{\"end\":53872,\"start\":53866},{\"end\":53880,\"start\":53879},{\"end\":53894,\"start\":53887},{\"end\":53909,\"start\":53904},{\"end\":53925,\"start\":53919},{\"end\":53939,\"start\":53933},{\"end\":54280,\"start\":54274},{\"end\":54300,\"start\":54294},{\"end\":54555,\"start\":54544},{\"end\":54566,\"start\":54562},{\"end\":54584,\"start\":54578},{\"end\":54603,\"start\":54596},{\"end\":55049,\"start\":55038},{\"end\":55063,\"start\":55056},{\"end\":55077,\"start\":55071},{\"end\":55098,\"start\":55091},{\"end\":55115,\"start\":55109},{\"end\":55534,\"start\":55523},{\"end\":55545,\"start\":55541},{\"end\":55562,\"start\":55555},{\"end\":55578,\"start\":55571},{\"end\":55595,\"start\":55589},{\"end\":55612,\"start\":55607},{\"end\":56222,\"start\":56216},{\"end\":56236,\"start\":56230},{\"end\":56247,\"start\":56243},{\"end\":56269,\"start\":56256},{\"end\":56667,\"start\":56661},{\"end\":56683,\"start\":56675},{\"end\":56698,\"start\":56691},{\"end\":56717,\"start\":56711},{\"end\":56731,\"start\":56724},{\"end\":56747,\"start\":56742},{\"end\":57175,\"start\":57167},{\"end\":57187,\"start\":57180},{\"end\":57614,\"start\":57613},{\"end\":57627,\"start\":57626},{\"end\":57636,\"start\":57635},{\"end\":57647,\"start\":57646},{\"end\":57658,\"start\":57657},{\"end\":58072,\"start\":58067},{\"end\":58091,\"start\":58085},{\"end\":58108,\"start\":58100},{\"end\":58124,\"start\":58118},{\"end\":58141,\"start\":58135},{\"end\":58159,\"start\":58153},{\"end\":58723,\"start\":58717},{\"end\":58737,\"start\":58732},{\"end\":58751,\"start\":58746},{\"end\":58767,\"start\":58760},{\"end\":59273,\"start\":59267},{\"end\":59287,\"start\":59282},{\"end\":59301,\"start\":59296},{\"end\":59317,\"start\":59310},{\"end\":59331,\"start\":59324},{\"end\":59349,\"start\":59342},{\"end\":59355,\"start\":59350},{\"end\":59743,\"start\":59738},{\"end\":59760,\"start\":59753},{\"end\":59776,\"start\":59769},{\"end\":60330,\"start\":60329},{\"end\":60614,\"start\":60608},{\"end\":60624,\"start\":60619},{\"end\":60639,\"start\":60631},{\"end\":60654,\"start\":60646},{\"end\":60664,\"start\":60660},{\"end\":60679,\"start\":60671},{\"end\":60696,\"start\":60690}]", "bib_author_last_name": "[{\"end\":34898,\"start\":34894},{\"end\":34907,\"start\":34900},{\"end\":35154,\"start\":35146},{\"end\":35172,\"start\":35167},{\"end\":35192,\"start\":35181},{\"end\":35209,\"start\":35203},{\"end\":35221,\"start\":35216},{\"end\":35236,\"start\":35231},{\"end\":35248,\"start\":35240},{\"end\":35263,\"start\":35257},{\"end\":35276,\"start\":35269},{\"end\":35291,\"start\":35285},{\"end\":35298,\"start\":35293},{\"end\":35636,\"start\":35634},{\"end\":35642,\"start\":35638},{\"end\":35878,\"start\":35869},{\"end\":35910,\"start\":35904},{\"end\":35925,\"start\":35919},{\"end\":36345,\"start\":36340},{\"end\":36362,\"start\":36357},{\"end\":36376,\"start\":36371},{\"end\":36391,\"start\":36384},{\"end\":36639,\"start\":36633},{\"end\":36654,\"start\":36647},{\"end\":36667,\"start\":36663},{\"end\":36681,\"start\":36677},{\"end\":36700,\"start\":36695},{\"end\":36710,\"start\":36708},{\"end\":36726,\"start\":36718},{\"end\":36734,\"start\":36731},{\"end\":36752,\"start\":36746},{\"end\":36767,\"start\":36760},{\"end\":37164,\"start\":37158},{\"end\":37177,\"start\":37172},{\"end\":37193,\"start\":37186},{\"end\":37714,\"start\":37704},{\"end\":37728,\"start\":37723},{\"end\":37746,\"start\":37738},{\"end\":37763,\"start\":37754},{\"end\":38469,\"start\":38464},{\"end\":38496,\"start\":38481},{\"end\":38512,\"start\":38506},{\"end\":38974,\"start\":38968},{\"end\":38989,\"start\":38984},{\"end\":39006,\"start\":39001},{\"end\":39020,\"start\":39013},{\"end\":39038,\"start\":39029},{\"end\":39056,\"start\":39048},{\"end\":39068,\"start\":39062},{\"end\":39081,\"start\":39077},{\"end\":39096,\"start\":39089},{\"end\":39642,\"start\":39638},{\"end\":39652,\"start\":39648},{\"end\":39668,\"start\":39662},{\"end\":39679,\"start\":39677},{\"end\":39687,\"start\":39685},{\"end\":39699,\"start\":39692},{\"end\":40197,\"start\":40193},{\"end\":40208,\"start\":40204},{\"end\":40222,\"start\":40218},{\"end\":40238,\"start\":40234},{\"end\":40249,\"start\":40245},{\"end\":40593,\"start\":40582},{\"end\":40605,\"start\":40602},{\"end\":40623,\"start\":40614},{\"end\":40638,\"start\":40633},{\"end\":40654,\"start\":40648},{\"end\":41087,\"start\":41082},{\"end\":41099,\"start\":41093},{\"end\":41523,\"start\":41518},{\"end\":41538,\"start\":41532},{\"end\":41554,\"start\":41547},{\"end\":41844,\"start\":41834},{\"end\":41858,\"start\":41850},{\"end\":41875,\"start\":41864},{\"end\":41890,\"start\":41882},{\"end\":41903,\"start\":41899},{\"end\":41914,\"start\":41905},{\"end\":42226,\"start\":42220},{\"end\":42239,\"start\":42235},{\"end\":42258,\"start\":42251},{\"end\":42274,\"start\":42267},{\"end\":42603,\"start\":42597},{\"end\":42616,\"start\":42612},{\"end\":42632,\"start\":42625},{\"end\":43059,\"start\":43056},{\"end\":43071,\"start\":43068},{\"end\":43085,\"start\":43081},{\"end\":43100,\"start\":43096},{\"end\":43117,\"start\":43113},{\"end\":43540,\"start\":43538},{\"end\":43555,\"start\":43550},{\"end\":43569,\"start\":43566},{\"end\":43579,\"start\":43576},{\"end\":44054,\"start\":44044},{\"end\":44071,\"start\":44065},{\"end\":44086,\"start\":44080},{\"end\":44101,\"start\":44093},{\"end\":44119,\"start\":44111},{\"end\":44434,\"start\":44428},{\"end\":44443,\"start\":44436},{\"end\":44704,\"start\":44698},{\"end\":44719,\"start\":44713},{\"end\":44734,\"start\":44726},{\"end\":44752,\"start\":44744},{\"end\":45152,\"start\":45147},{\"end\":45167,\"start\":45162},{\"end\":45181,\"start\":45177},{\"end\":45193,\"start\":45190},{\"end\":45206,\"start\":45202},{\"end\":45217,\"start\":45213},{\"end\":45231,\"start\":45228},{\"end\":45245,\"start\":45241},{\"end\":45757,\"start\":45751},{\"end\":45772,\"start\":45767},{\"end\":45786,\"start\":45781},{\"end\":45801,\"start\":45795},{\"end\":46047,\"start\":46038},{\"end\":46065,\"start\":46055},{\"end\":46543,\"start\":46533},{\"end\":46558,\"start\":46552},{\"end\":46577,\"start\":46569},{\"end\":46866,\"start\":46863},{\"end\":46880,\"start\":46876},{\"end\":46893,\"start\":46889},{\"end\":47122,\"start\":47114},{\"end\":47136,\"start\":47130},{\"end\":47140,\"start\":47138},{\"end\":47384,\"start\":47376},{\"end\":47881,\"start\":47872},{\"end\":47899,\"start\":47893},{\"end\":48127,\"start\":48119},{\"end\":48139,\"start\":48133},{\"end\":48154,\"start\":48148},{\"end\":48168,\"start\":48162},{\"end\":48678,\"start\":48662},{\"end\":48694,\"start\":48690},{\"end\":48707,\"start\":48703},{\"end\":48719,\"start\":48714},{\"end\":48736,\"start\":48730},{\"end\":48753,\"start\":48746},{\"end\":48768,\"start\":48760},{\"end\":48774,\"start\":48770},{\"end\":49125,\"start\":49123},{\"end\":49139,\"start\":49134},{\"end\":49155,\"start\":49151},{\"end\":49501,\"start\":49498},{\"end\":49517,\"start\":49511},{\"end\":49781,\"start\":49778},{\"end\":49792,\"start\":49788},{\"end\":49802,\"start\":49798},{\"end\":50303,\"start\":50296},{\"end\":50320,\"start\":50314},{\"end\":50335,\"start\":50328},{\"end\":50348,\"start\":50342},{\"end\":50709,\"start\":50705},{\"end\":50725,\"start\":50717},{\"end\":50741,\"start\":50737},{\"end\":51279,\"start\":51272},{\"end\":51295,\"start\":51288},{\"end\":51313,\"start\":51309},{\"end\":51333,\"start\":51321},{\"end\":51782,\"start\":51778},{\"end\":51808,\"start\":51796},{\"end\":51854,\"start\":51840},{\"end\":51870,\"start\":51862},{\"end\":51879,\"start\":51872},{\"end\":52211,\"start\":52205},{\"end\":52231,\"start\":52222},{\"end\":52244,\"start\":52241},{\"end\":52265,\"start\":52254},{\"end\":52471,\"start\":52466},{\"end\":52869,\"start\":52857},{\"end\":52878,\"start\":52876},{\"end\":52893,\"start\":52885},{\"end\":52898,\"start\":52895},{\"end\":53282,\"start\":53279},{\"end\":53297,\"start\":53290},{\"end\":53317,\"start\":53306},{\"end\":53332,\"start\":53325},{\"end\":53349,\"start\":53344},{\"end\":53877,\"start\":53873},{\"end\":53885,\"start\":53881},{\"end\":53902,\"start\":53895},{\"end\":53917,\"start\":53910},{\"end\":53931,\"start\":53926},{\"end\":53945,\"start\":53940},{\"end\":53951,\"start\":53947},{\"end\":54292,\"start\":54281},{\"end\":54309,\"start\":54301},{\"end\":54560,\"start\":54556},{\"end\":54576,\"start\":54567},{\"end\":54594,\"start\":54585},{\"end\":54612,\"start\":54604},{\"end\":55054,\"start\":55050},{\"end\":55069,\"start\":55064},{\"end\":55089,\"start\":55078},{\"end\":55107,\"start\":55099},{\"end\":55125,\"start\":55116},{\"end\":55539,\"start\":55535},{\"end\":55553,\"start\":55546},{\"end\":55569,\"start\":55563},{\"end\":55587,\"start\":55579},{\"end\":55605,\"start\":55596},{\"end\":55622,\"start\":55613},{\"end\":56228,\"start\":56223},{\"end\":56241,\"start\":56237},{\"end\":56254,\"start\":56248},{\"end\":56275,\"start\":56270},{\"end\":56673,\"start\":56668},{\"end\":56689,\"start\":56684},{\"end\":56709,\"start\":56699},{\"end\":56722,\"start\":56718},{\"end\":56740,\"start\":56732},{\"end\":56757,\"start\":56748},{\"end\":57178,\"start\":57176},{\"end\":57195,\"start\":57188},{\"end\":57624,\"start\":57615},{\"end\":57633,\"start\":57628},{\"end\":57644,\"start\":57637},{\"end\":57655,\"start\":57648},{\"end\":57666,\"start\":57659},{\"end\":58083,\"start\":58073},{\"end\":58098,\"start\":58092},{\"end\":58116,\"start\":58109},{\"end\":58133,\"start\":58125},{\"end\":58151,\"start\":58142},{\"end\":58168,\"start\":58160},{\"end\":58730,\"start\":58724},{\"end\":58744,\"start\":58738},{\"end\":58758,\"start\":58752},{\"end\":58776,\"start\":58768},{\"end\":59280,\"start\":59274},{\"end\":59294,\"start\":59288},{\"end\":59308,\"start\":59302},{\"end\":59322,\"start\":59318},{\"end\":59340,\"start\":59332},{\"end\":59359,\"start\":59356},{\"end\":59364,\"start\":59361},{\"end\":59751,\"start\":59744},{\"end\":59767,\"start\":59761},{\"end\":59784,\"start\":59777},{\"end\":60337,\"start\":60331},{\"end\":60343,\"start\":60339},{\"end\":60617,\"start\":60615},{\"end\":60629,\"start\":60625},{\"end\":60644,\"start\":60640},{\"end\":60658,\"start\":60655},{\"end\":60669,\"start\":60665},{\"end\":60688,\"start\":60680},{\"end\":60704,\"start\":60697}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35093,\"start\":34815},{\"attributes\":{\"doi\":\"arXiv:1604.07316\",\"id\":\"b1\"},\"end\":35581,\"start\":35095},{\"attributes\":{\"id\":\"b2\"},\"end\":35825,\"start\":35583},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":55755812},\"end\":36255,\"start\":35827},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":73443633},\"end\":36624,\"start\":36257},{\"attributes\":{\"doi\":\"arXiv:1903.11027\",\"id\":\"b5\"},\"end\":37098,\"start\":36626},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":749979},\"end\":37616,\"start\":37100},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":58014148},\"end\":38347,\"start\":37618},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":51883314},\"end\":38896,\"start\":38349},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":502946},\"end\":39579,\"start\":38898},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57246310},\"end\":40104,\"start\":39581},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":21057270},\"end\":40533,\"start\":40106},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5550767},\"end\":40966,\"start\":40535},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":102496818},\"end\":41465,\"start\":40968},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14547347},\"end\":41777,\"start\":41467},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4246903},\"end\":42168,\"start\":41779},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9455111},\"end\":42516,\"start\":42170},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6724907},\"end\":43014,\"start\":42518},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14787746},\"end\":43482,\"start\":43016},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594692},\"end\":43950,\"start\":43484},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1198561},\"end\":44375,\"start\":43952},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":84270139},\"end\":44616,\"start\":44377},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15528373},\"end\":45091,\"start\":44618},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3943983},\"end\":45708,\"start\":45093},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8267891},\"end\":45961,\"start\":45710},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6629049},\"end\":46490,\"start\":45963},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2197839},\"end\":46771,\"start\":46492},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":123833570},\"end\":47066,\"start\":46773},{\"attributes\":{\"id\":\"b28\"},\"end\":47226,\"start\":47068},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8070108},\"end\":47793,\"start\":47228},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b30\"},\"end\":48080,\"start\":47795},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15805629},\"end\":48532,\"start\":48082},{\"attributes\":{\"id\":\"b32\"},\"end\":49066,\"start\":48534},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":51967555},\"end\":49458,\"start\":49068},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12824352},\"end\":49680,\"start\":49460},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":24496639},\"end\":50243,\"start\":49682},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":22556995},\"end\":50577,\"start\":50245},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":53535741},\"end\":51188,\"start\":50579},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5753855},\"end\":51729,\"start\":51190},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14530050},\"end\":52120,\"start\":51731},{\"attributes\":{\"id\":\"b40\"},\"end\":52430,\"start\":52122},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9984768},\"end\":52767,\"start\":52432},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10328909},\"end\":53167,\"start\":52769},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206594095},\"end\":53807,\"start\":53169},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":53029837},\"end\":54228,\"start\":53809},{\"attributes\":{\"id\":\"b45\"},\"end\":54458,\"start\":54230},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3935721},\"end\":54979,\"start\":54460},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3738885},\"end\":55429,\"start\":54981},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":54462208},\"end\":56124,\"start\":55431},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":53512298},\"end\":56582,\"start\":56126},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":60440409},\"end\":57102,\"start\":56584},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":54457562},\"end\":57542,\"start\":57104},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5064446},\"end\":57999,\"start\":57544},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":52056830},\"end\":58643,\"start\":58001},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":59316638},\"end\":59265,\"start\":58645},{\"attributes\":{\"id\":\"b55\"},\"end\":59657,\"start\":59267},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":7580580},\"end\":60225,\"start\":59659},{\"attributes\":{\"id\":\"b57\"},\"end\":60530,\"start\":60227},{\"attributes\":{\"doi\":\"arXiv:1805.04687\",\"id\":\"b58\"},\"end\":60961,\"start\":60532}]", "bib_title": "[{\"end\":35632,\"start\":35583},{\"end\":35861,\"start\":35827},{\"end\":36331,\"start\":36257},{\"end\":37150,\"start\":37100},{\"end\":37694,\"start\":37618},{\"end\":38453,\"start\":38349},{\"end\":38959,\"start\":38898},{\"end\":39632,\"start\":39581},{\"end\":40183,\"start\":40106},{\"end\":40573,\"start\":40535},{\"end\":41074,\"start\":40968},{\"end\":41510,\"start\":41467},{\"end\":41827,\"start\":41779},{\"end\":42210,\"start\":42170},{\"end\":42587,\"start\":42518},{\"end\":43046,\"start\":43016},{\"end\":43528,\"start\":43484},{\"end\":44035,\"start\":43952},{\"end\":44424,\"start\":44377},{\"end\":44687,\"start\":44618},{\"end\":45139,\"start\":45093},{\"end\":45742,\"start\":45710},{\"end\":46028,\"start\":45963},{\"end\":46524,\"start\":46492},{\"end\":46852,\"start\":46773},{\"end\":47366,\"start\":47228},{\"end\":48110,\"start\":48082},{\"end\":48653,\"start\":48534},{\"end\":49115,\"start\":49068},{\"end\":49491,\"start\":49460},{\"end\":49766,\"start\":49682},{\"end\":50289,\"start\":50245},{\"end\":50696,\"start\":50579},{\"end\":51262,\"start\":51190},{\"end\":51769,\"start\":51731},{\"end\":52457,\"start\":52432},{\"end\":52847,\"start\":52769},{\"end\":53270,\"start\":53169},{\"end\":53864,\"start\":53809},{\"end\":54542,\"start\":54460},{\"end\":55036,\"start\":54981},{\"end\":55521,\"start\":55431},{\"end\":56214,\"start\":56126},{\"end\":56659,\"start\":56584},{\"end\":57165,\"start\":57104},{\"end\":57611,\"start\":57544},{\"end\":58065,\"start\":58001},{\"end\":58715,\"start\":58645},{\"end\":59736,\"start\":59659},{\"end\":60327,\"start\":60227}]", "bib_author": "[{\"end\":34900,\"start\":34892},{\"end\":34909,\"start\":34900},{\"end\":35156,\"start\":35138},{\"end\":35174,\"start\":35156},{\"end\":35194,\"start\":35174},{\"end\":35211,\"start\":35194},{\"end\":35223,\"start\":35211},{\"end\":35238,\"start\":35223},{\"end\":35250,\"start\":35238},{\"end\":35265,\"start\":35250},{\"end\":35278,\"start\":35265},{\"end\":35293,\"start\":35278},{\"end\":35300,\"start\":35293},{\"end\":35638,\"start\":35634},{\"end\":35644,\"start\":35638},{\"end\":35880,\"start\":35863},{\"end\":35898,\"start\":35880},{\"end\":35912,\"start\":35898},{\"end\":35927,\"start\":35912},{\"end\":36347,\"start\":36333},{\"end\":36364,\"start\":36347},{\"end\":36378,\"start\":36364},{\"end\":36393,\"start\":36378},{\"end\":36641,\"start\":36626},{\"end\":36656,\"start\":36641},{\"end\":36669,\"start\":36656},{\"end\":36683,\"start\":36669},{\"end\":36702,\"start\":36683},{\"end\":36712,\"start\":36702},{\"end\":36728,\"start\":36712},{\"end\":36736,\"start\":36728},{\"end\":36754,\"start\":36736},{\"end\":36769,\"start\":36754},{\"end\":37166,\"start\":37152},{\"end\":37179,\"start\":37166},{\"end\":37195,\"start\":37179},{\"end\":37716,\"start\":37696},{\"end\":37730,\"start\":37716},{\"end\":37748,\"start\":37730},{\"end\":37765,\"start\":37748},{\"end\":38471,\"start\":38455},{\"end\":38498,\"start\":38471},{\"end\":38514,\"start\":38498},{\"end\":38976,\"start\":38961},{\"end\":38991,\"start\":38976},{\"end\":39008,\"start\":38991},{\"end\":39022,\"start\":39008},{\"end\":39040,\"start\":39022},{\"end\":39058,\"start\":39040},{\"end\":39070,\"start\":39058},{\"end\":39083,\"start\":39070},{\"end\":39098,\"start\":39083},{\"end\":39644,\"start\":39634},{\"end\":39654,\"start\":39644},{\"end\":39670,\"start\":39654},{\"end\":39681,\"start\":39670},{\"end\":39689,\"start\":39681},{\"end\":39701,\"start\":39689},{\"end\":40199,\"start\":40185},{\"end\":40210,\"start\":40199},{\"end\":40224,\"start\":40210},{\"end\":40240,\"start\":40224},{\"end\":40251,\"start\":40240},{\"end\":40595,\"start\":40575},{\"end\":40607,\"start\":40595},{\"end\":40625,\"start\":40607},{\"end\":40640,\"start\":40625},{\"end\":40656,\"start\":40640},{\"end\":41089,\"start\":41076},{\"end\":41101,\"start\":41089},{\"end\":41525,\"start\":41512},{\"end\":41540,\"start\":41525},{\"end\":41556,\"start\":41540},{\"end\":41846,\"start\":41829},{\"end\":41860,\"start\":41846},{\"end\":41877,\"start\":41860},{\"end\":41892,\"start\":41877},{\"end\":41905,\"start\":41892},{\"end\":41916,\"start\":41905},{\"end\":42228,\"start\":42212},{\"end\":42241,\"start\":42228},{\"end\":42260,\"start\":42241},{\"end\":42276,\"start\":42260},{\"end\":42605,\"start\":42589},{\"end\":42618,\"start\":42605},{\"end\":42634,\"start\":42618},{\"end\":43061,\"start\":43048},{\"end\":43073,\"start\":43061},{\"end\":43087,\"start\":43073},{\"end\":43102,\"start\":43087},{\"end\":43119,\"start\":43102},{\"end\":43542,\"start\":43530},{\"end\":43557,\"start\":43542},{\"end\":43571,\"start\":43557},{\"end\":43581,\"start\":43571},{\"end\":44056,\"start\":44037},{\"end\":44073,\"start\":44056},{\"end\":44088,\"start\":44073},{\"end\":44103,\"start\":44088},{\"end\":44121,\"start\":44103},{\"end\":44436,\"start\":44426},{\"end\":44445,\"start\":44436},{\"end\":44706,\"start\":44689},{\"end\":44721,\"start\":44706},{\"end\":44736,\"start\":44721},{\"end\":44754,\"start\":44736},{\"end\":45154,\"start\":45141},{\"end\":45169,\"start\":45154},{\"end\":45183,\"start\":45169},{\"end\":45195,\"start\":45183},{\"end\":45208,\"start\":45195},{\"end\":45219,\"start\":45208},{\"end\":45233,\"start\":45219},{\"end\":45247,\"start\":45233},{\"end\":45759,\"start\":45744},{\"end\":45774,\"start\":45759},{\"end\":45788,\"start\":45774},{\"end\":45803,\"start\":45788},{\"end\":46049,\"start\":46030},{\"end\":46067,\"start\":46049},{\"end\":46545,\"start\":46526},{\"end\":46560,\"start\":46545},{\"end\":46579,\"start\":46560},{\"end\":46868,\"start\":46854},{\"end\":46882,\"start\":46868},{\"end\":46895,\"start\":46882},{\"end\":47124,\"start\":47112},{\"end\":47138,\"start\":47124},{\"end\":47142,\"start\":47138},{\"end\":47386,\"start\":47368},{\"end\":47883,\"start\":47866},{\"end\":47901,\"start\":47883},{\"end\":48129,\"start\":48112},{\"end\":48141,\"start\":48129},{\"end\":48156,\"start\":48141},{\"end\":48170,\"start\":48156},{\"end\":48680,\"start\":48655},{\"end\":48696,\"start\":48680},{\"end\":48709,\"start\":48696},{\"end\":48721,\"start\":48709},{\"end\":48738,\"start\":48721},{\"end\":48755,\"start\":48738},{\"end\":48770,\"start\":48755},{\"end\":48776,\"start\":48770},{\"end\":49127,\"start\":49117},{\"end\":49141,\"start\":49127},{\"end\":49157,\"start\":49141},{\"end\":49503,\"start\":49493},{\"end\":49519,\"start\":49503},{\"end\":49783,\"start\":49768},{\"end\":49794,\"start\":49783},{\"end\":49804,\"start\":49794},{\"end\":50305,\"start\":50291},{\"end\":50322,\"start\":50305},{\"end\":50337,\"start\":50322},{\"end\":50350,\"start\":50337},{\"end\":50711,\"start\":50698},{\"end\":50727,\"start\":50711},{\"end\":50743,\"start\":50727},{\"end\":51281,\"start\":51264},{\"end\":51297,\"start\":51281},{\"end\":51315,\"start\":51297},{\"end\":51335,\"start\":51315},{\"end\":51784,\"start\":51771},{\"end\":51810,\"start\":51784},{\"end\":51856,\"start\":51810},{\"end\":51872,\"start\":51856},{\"end\":51881,\"start\":51872},{\"end\":52213,\"start\":52200},{\"end\":52233,\"start\":52213},{\"end\":52246,\"start\":52233},{\"end\":52267,\"start\":52246},{\"end\":52473,\"start\":52459},{\"end\":52871,\"start\":52849},{\"end\":52880,\"start\":52871},{\"end\":52895,\"start\":52880},{\"end\":52900,\"start\":52895},{\"end\":53284,\"start\":53272},{\"end\":53299,\"start\":53284},{\"end\":53319,\"start\":53299},{\"end\":53334,\"start\":53319},{\"end\":53351,\"start\":53334},{\"end\":53879,\"start\":53866},{\"end\":53887,\"start\":53879},{\"end\":53904,\"start\":53887},{\"end\":53919,\"start\":53904},{\"end\":53933,\"start\":53919},{\"end\":53947,\"start\":53933},{\"end\":53953,\"start\":53947},{\"end\":54294,\"start\":54274},{\"end\":54311,\"start\":54294},{\"end\":54562,\"start\":54544},{\"end\":54578,\"start\":54562},{\"end\":54596,\"start\":54578},{\"end\":54614,\"start\":54596},{\"end\":55056,\"start\":55038},{\"end\":55071,\"start\":55056},{\"end\":55091,\"start\":55071},{\"end\":55109,\"start\":55091},{\"end\":55127,\"start\":55109},{\"end\":55541,\"start\":55523},{\"end\":55555,\"start\":55541},{\"end\":55571,\"start\":55555},{\"end\":55589,\"start\":55571},{\"end\":55607,\"start\":55589},{\"end\":55624,\"start\":55607},{\"end\":56230,\"start\":56216},{\"end\":56243,\"start\":56230},{\"end\":56256,\"start\":56243},{\"end\":56277,\"start\":56256},{\"end\":56675,\"start\":56661},{\"end\":56691,\"start\":56675},{\"end\":56711,\"start\":56691},{\"end\":56724,\"start\":56711},{\"end\":56742,\"start\":56724},{\"end\":56759,\"start\":56742},{\"end\":57180,\"start\":57167},{\"end\":57197,\"start\":57180},{\"end\":57626,\"start\":57613},{\"end\":57635,\"start\":57626},{\"end\":57646,\"start\":57635},{\"end\":57657,\"start\":57646},{\"end\":57668,\"start\":57657},{\"end\":58085,\"start\":58067},{\"end\":58100,\"start\":58085},{\"end\":58118,\"start\":58100},{\"end\":58135,\"start\":58118},{\"end\":58153,\"start\":58135},{\"end\":58170,\"start\":58153},{\"end\":58732,\"start\":58717},{\"end\":58746,\"start\":58732},{\"end\":58760,\"start\":58746},{\"end\":58778,\"start\":58760},{\"end\":59282,\"start\":59267},{\"end\":59296,\"start\":59282},{\"end\":59310,\"start\":59296},{\"end\":59324,\"start\":59310},{\"end\":59342,\"start\":59324},{\"end\":59361,\"start\":59342},{\"end\":59366,\"start\":59361},{\"end\":59753,\"start\":59738},{\"end\":59769,\"start\":59753},{\"end\":59786,\"start\":59769},{\"end\":60339,\"start\":60329},{\"end\":60345,\"start\":60339},{\"end\":60619,\"start\":60608},{\"end\":60631,\"start\":60619},{\"end\":60646,\"start\":60631},{\"end\":60660,\"start\":60646},{\"end\":60671,\"start\":60660},{\"end\":60690,\"start\":60671},{\"end\":60706,\"start\":60690}]", "bib_venue": "[{\"end\":36020,\"start\":36002},{\"end\":37379,\"start\":37290},{\"end\":38026,\"start\":37904},{\"end\":38629,\"start\":38580},{\"end\":39253,\"start\":39184},{\"end\":39856,\"start\":39787},{\"end\":40759,\"start\":40716},{\"end\":41222,\"start\":41170},{\"end\":42789,\"start\":42720},{\"end\":43266,\"start\":43201},{\"end\":43736,\"start\":43667},{\"end\":45422,\"start\":45343},{\"end\":46254,\"start\":46169},{\"end\":47527,\"start\":47465},{\"end\":48325,\"start\":48256},{\"end\":49272,\"start\":49223},{\"end\":49987,\"start\":49904},{\"end\":50904,\"start\":50832},{\"end\":51470,\"start\":51411},{\"end\":52628,\"start\":52559},{\"end\":53506,\"start\":53437},{\"end\":55797,\"start\":55719},{\"end\":57338,\"start\":57276},{\"end\":57773,\"start\":57729},{\"end\":58343,\"start\":58265},{\"end\":58911,\"start\":58853},{\"end\":59968,\"start\":59877},{\"end\":34890,\"start\":34815},{\"end\":35136,\"start\":35095},{\"end\":35662,\"start\":35644},{\"end\":35987,\"start\":35927},{\"end\":36410,\"start\":36393},{\"end\":36838,\"start\":36785},{\"end\":37288,\"start\":37195},{\"end\":37902,\"start\":37765},{\"end\":38578,\"start\":38514},{\"end\":39182,\"start\":39098},{\"end\":39785,\"start\":39701},{\"end\":40296,\"start\":40251},{\"end\":40714,\"start\":40656},{\"end\":41168,\"start\":41101},{\"end\":41594,\"start\":41556},{\"end\":41956,\"start\":41916},{\"end\":42322,\"start\":42276},{\"end\":42718,\"start\":42634},{\"end\":43199,\"start\":43119},{\"end\":43665,\"start\":43581},{\"end\":44147,\"start\":44121},{\"end\":44480,\"start\":44445},{\"end\":44831,\"start\":44754},{\"end\":45341,\"start\":45247},{\"end\":45817,\"start\":45803},{\"end\":46167,\"start\":46067},{\"end\":46615,\"start\":46579},{\"end\":46900,\"start\":46895},{\"end\":47110,\"start\":47068},{\"end\":47463,\"start\":47386},{\"end\":47864,\"start\":47795},{\"end\":48254,\"start\":48170},{\"end\":48789,\"start\":48776},{\"end\":49221,\"start\":49157},{\"end\":49554,\"start\":49519},{\"end\":49902,\"start\":49804},{\"end\":50396,\"start\":50350},{\"end\":50830,\"start\":50743},{\"end\":51409,\"start\":51335},{\"end\":51911,\"start\":51881},{\"end\":52198,\"start\":52122},{\"end\":52557,\"start\":52473},{\"end\":52949,\"start\":52900},{\"end\":53435,\"start\":53351},{\"end\":53993,\"start\":53953},{\"end\":54272,\"start\":54230},{\"end\":54698,\"start\":54614},{\"end\":55187,\"start\":55127},{\"end\":55717,\"start\":55624},{\"end\":56336,\"start\":56277},{\"end\":56828,\"start\":56759},{\"end\":57274,\"start\":57197},{\"end\":57727,\"start\":57668},{\"end\":58263,\"start\":58170},{\"end\":58851,\"start\":58778},{\"end\":59440,\"start\":59366},{\"end\":59875,\"start\":59786},{\"end\":60363,\"start\":60345},{\"end\":60606,\"start\":60532}]"}}}, "year": 2023, "month": 12, "day": 17}