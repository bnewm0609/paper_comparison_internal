{"id": 257404900, "updated": "2023-10-05 03:21:12.245", "metadata": {"title": "Gradient-Free Structured Pruning with Unlabeled Data", "authors": "[{\"first\":\"Azade\",\"last\":\"Nova\",\"middle\":[]},{\"first\":\"Hanjun\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Dale\",\"last\":\"Schuurmans\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accuracy loss across all tasks considered.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.04185", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/NovaDS23", "doi": "10.48550/arxiv.2303.04185"}}, "content": {"source": {"pdf_hash": "3d60a54a47b346608430344ff37935d897a14c09", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.04185v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7304f9e0d35348efa231cd7d6fb3426a43adaefd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3d60a54a47b346608430344ff37935d897a14c09.txt", "contents": "\nGradient-Free Structured Pruning with Unlabeled Data\n\n\nAzade Nova \nHanjun Dai \nDale Schuurmans \nGradient-Free Structured Pruning with Unlabeled Data\n\nLarge Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERT BASE and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40% of the original FLOP count can be reduced with less than a 4% accuracy loss across all tasks considered.\n\nIntroduction\n\nLarge Language Models (LLMs) have made great strides in solving difficult tasks across many domains, but this has come at the cost of high parameter counts and significant computational overhead. Developers and third parties can now employ these trained models and create custom versions tailored to their particular applications. Customization makes these models applicable to a wider variety of use cases, but this, even more, highlights the need for efficient inference models.\n\nMany efforts have been being made to reduce computational cost through model compression techniques specialized for Transformers, including structured pruning (Xia et al., 2022; Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). Hou et al., 2020;Sajjad et al., 2023;Liu et al., 2021a;Xia et al., 2022), efficient architecture design (Kitaev et al., 2020;Iandola et al., 2020;Sun et al., 2020;Wang et al., 2020b), neural architecture search (So et al., 2021;Xu et al., 2021;Yin et al., 2021), knowledge distillation (Sun et al., 2020;Jiao et al., 2019;Sanh et al., 2019), quantization Shen et al., 2020;Zadeh et al., 2020;Zafrir et al., 2019), and hardware-software co-design (Gu et al., 2022;Ham et al., 2021).\n\nAmong these techniques, structured pruning shows promising results in reducing model size, while also improving inference time because the resulting model remains compatible with the underlying hardware. However, most existing approaches are quite complex and require significant engineering effort to implement. Moreover, the process of compression is time-consuming and requires retraining the compressed model to regain accuracy. These limitations make effective compression difficult to realize in practice. Recently, Kwon et al. (2022) proposed a post-training pruning for Transformers that does not require any retraining of the model. Even though this approach avoids expensive retraining, it requires labeled data in the pruning pipeline.\n\nLLMs mainly utilize unlabeled data for training and with increased use of pre-trained LLMs by developers and third parties, access to the labeled data is questionable. Especially with the popularity of in-context learning, where the user only provides prompts, the purpose of the task is not necessarily known at compression time. In this scenario, none of the existing pruning techniques can be applied for model compression since they all require labeled data. Even though knowledge distillation (Sun et al., 2020;Jiao et al., 2019;Sanh et al., 2019) trains a student model with unlabeled data, it still requires a large amount of unlabeled data and is expensive to train. This motivates us to investigate whether one can design a structured pruning method that does not require retraining nor labeled data, while avoiding adverse effects on performance.\n\nIn this work, we propose Kernelized Convex Masking (KCM), a gradient-free framework (Figure 1) that only requires the trained model and sampled raw data to compress the model. We introduce R2D2 as the core of this framework that combines two ranking techniques Representative Ranking (R2) and Data-Driven (D2) to estimate the impor-tance of individual neurons. R2 maps our structured pruning goals into a representative selection problem (Huang et al., 2018), where the goal is to find a small subset of data points that can well represent a large dataset. Specifically, R2 considers the filters of a Feed-Forward Network (FFN) in the trained model as data points in a high-dimensional space, and ranks these by how well a filter can be represented by others. D2, on the other hand, ranks the filters based on statistics gathered from layer-wise model outputs using the raw sampled data. KCM decides which filter to remove by merging the R2D2 rankings across all layers. Since removing filters may still affect accuracy, we apply an existing scaling transformation method in Kwon et al. (2022) to mitigate the effect of their removal.\n\nOur main contributions are as follows:\n\n\u2022 We Propose Kernelized Convex Masking (KCM) pruning framework, a gradient-free structured pruning approach that neither requires labeled data nor retraining. \u2022 As the core of KCM, we propose R2D2 that combines two ranking techniques Representative Ranking (R2) and Data-Driven (D2). R2D2 only requires the weights of the trained model and sampled raw data to rank the neurons. An ablation study confirms the importance of combining the two proposed ranking techniques. \u2022 Our evaluation on GLUE and SQuAD benchmarks using BERT BASE and DistilBERT confirms the effectiveness of the proposed approach. Compared to when the labeled data is available, KCM is able to reduce up to 40% of the original FLOPs with less than 4% accuracy loss, in a matter of a few minutes on a single GPU.\n\n\nProblem Definition\n\n\nPreliminary\n\nIn this paper, we focus on pruning the BERT  architecture. BERT is a stack of L homogeneous Transformer encoder blocks (Vaswani et al., 2017), each of which consists of a multi-head attention (MHA) layer followed by a Feed-Forward Network (FFN) layer. Due to the fact that FFN layers have a huge impact on model size and inference latency (Ganesh et al., 2021), we focus on the pruning the filters of the FFN layers. In every transformer encoder layer , the F F N (x) with N filters is parame-\nterized with W (1) \u2208 R d\u00d7N , W (2) \u2208 R N \u00d7d , b (1) \u2208 R N , b\n(2) \u2208 R d , and activation function \u03c3:\nF F N (x) = N i=1 (\u03c3(xW (1) [:, i] + b (1) )W (2) [i, :]) + b(2)\n(1) For example, BERT BASE has 12 transformer encoder blocks (L = 12), where the number of filters (N ) is 3072, \n\n\nW\n\n(1) \u2208 R 768\u00d73072 with the GELU activation (Hendrycks & Gimpel, 2016), and W (2) \u2208 R 3072\u00d7768 .\n\n\nStructured Pruning by Masking\n\nMasking: Given an integer n < N , reducing the number of filters from N to n can be considered as introducing a mask variable m \u2208 R N (with n non-zero elements) associated with the outputs of the filters.\nF F N (x) = N i=1 (\u03c3(xW (1) [:, i] + b (1) )W (2) [i, :] \u2022 m i ) + b(2)\n(2) where \u2022 is Hadamard product.\n\nObjective: Transformer pruning can be formulated as a constrained optimization problem on the mask M \u2208 R L\u00d7N that represents the mask m of all layers L. There are LN filter mask variables which is much less than the total number of parameters in the model. For example BERT BASE with 110M parameters needs only 36k mask variables (0.03%).\n\nOptimal structural pruning is usually defined (Kwon et al., 2022) in the supervised setting with respect to minimizing the accuracy loss of the original model:\nargmin M L(M) s.t. Cost(M) \u2264 C(3)\nCost(M) is the floating point operations (FLOPs) of the pruned model determined by the mask M. In this work, since only unlabeled data is available, the supervised loss L(M) can not be evaluated. Similar to distillation, we consider minimizing the Feature Map Loss (Sun et al., 2020) L F M T for each FFN in layer . \nL ( ) F M T (m) = F F N ( ) (x) \u2212 F F N ( ) (x) 2(4)W (2) \u2208 R N \u00d7d of F F N 5: Initialize coefficient matrix: C0 \u2208 R N \u00d7N = 1 N 6: repeat 7: Ci+1 = Ci \u2022 K(W (2) ,W (2) ) K(W (2) ,W (2) )C i 8: \u03b4 = (C i+1 \u2212C i ).sum() C i .sum 9: Ci = Ci+1 10:\nuntil convergence i.e. \u03b4 \u2264 \u03b1 11:\n\nSR2[ ]= diagonal(Ci) 12: end for 13: return SR2 filters of all L transformer layers such that Cost(M) be less than C and the loss\nL F M T (M) = L =1 L ( ) F M T (M ,: ) is minimized. argmin M L F M T (M) s.t. Cost(M) \u2264 C(5)\nOne way to tackle this problem would be to consider it as a version of the distillation problem, where the goal is to find the optimal mask under the sparsity constraint. However, distillation methods require large amounts of unlabeled data and are very expensive to train (Xia et al., 2022).\n\n\nProposed Approach\n\nInstead, in this work, we propose a gradient-free approach that only uses the weights of the trained model and statistics on layer-wise outputs using the unlabeled data to implicitly minimizes the feature map loss in each layer. Figure 1 shows the overview of our framework, called Kernelized Convex Masking (KCM). KCM takes the trained model Model, sampled unlabeled dataset D and a cost constraint C, and returns a mask M \u2208 R L\u00d7N that represents the mask of the N filters of all layers L.\n\n\nFramework Overview\n\nWe introduce R2D2 that combines ranking techniques Representative Ranking (R2) and Data-Driven (D2) to estimate the importance of the filters. As shown in Figure 1, these two approaches independently rank N filters based on the weights and output of the activation function of the FFNs in all layers L. Then KCM merges the results of R2D2 across all layers. The top k filters are selected and the rest will be masked to zero. Note that given a FLOPs constraint C, k is the total number of filters that satisfies constraint C. Finally, we apply a scaling transformation in Kwon et al. (2022) over the selected filters to recover the accuracy drop and reduce the feature map loss in Equation 4. Next, we discuss our framework in more detail.\n\n\nKernelized Convex Masking(KCM)\n\nAlgorithm 1 illustrates the end-to-end approach. We first present the details of the proposed R2D2. Then, we discuss how we use these rankings for the final masking.\n\n\nREPRESENTATIVE RANKING (R2)\n\nBy considering\nH (1) = \u03c3(xW (1) + b (1) ), Equation 1 can be written as F F N (H (1) ) = H (1) W (2) + b (2) . Our filter\nRepresentative Ranking assumes H (1) is unknown and only uses the weights W (2) to rank the N filters.\n\nFrom the computational geometry perspective the filters in W (2) \u2208 R N \u00d7d can be considered as N data points in a d dimensional space. The structured pruning goal can be translated as selecting a subset of data points (filters) to be used as representatives that can describe any data point (filter) in the dataset. There has been a lot of work on finding such a representative set (Kazemi et al., 2022;Killamsetty et al., 2021;You et al., 2020). However, for linear functions, this problem can be reduced to finding a convex hull. The convex hull is a subset of data points that can be used to find the maxima of any linear function.\nSince F F N (H (1) )\nis, in fact, a linear function, the convex hull of W (2) can be considered as a representative of the filters that produce the maxima of F F N regardless of the input H (1) .\n\nThe challenge is that in a d dimensional space finding the exact convex hull is in order of O(N d/2 ) time, which can be very expensive (e.g. in BERT BASE , d=768). Moreover, the number of convex hull data points radically increases with the number of dimensions. To address these limitations, Table 1. Comparison of the different structured pruning methods studied in this work. , and show if a method has the specific feature or not. N/A means not applicable. To simplify notation, we show gradient-free with (!\u2207). Supervision-free indicates not using labeled data.\n\n\nMethod\n\nGradient-free (!\u2207) Retrain/Finetune-free Supervision-free Pruning time \u2264 7min FLOP  SLIP (Lin et al., 2020) Sajjad et al. (Sajjad et al., 2023) DynaBERT (Hou et al., 2020) EBERT (Liu et al., 2021b) Mask-Tuning (Kwon et al., 2022) Weight-Magnitude (Li et al., 2016) N/A Weight-Magnitude-Scale KCM (ours) rather than finding the exact solution, we propose to assign a ranking over the filters that represents how well a filter is representing others.\n\nConvex hull approximation is well-studied area. Among existing methods Kernelized Convex Hull Approximation (KCHA) (Huang et al., 2018) is one of the approaches that can be applied to our problem. Algorithm 2 shows the proposed Representative Ranking based on the KCHA. Specifically for each layer , we seek a positive coefficient matrix\nC \u2208 R N \u00d7N that minimizes W (2) \u2212 W (2) C 2 . The diago-\nnal elements of C indicate whether the corresponding data instances are extreme points. Huang et al. (2018) solves this problem as a Semi-NMF problem (Ding et al., 2008), rather than a Non-negative Least Square problem, and adopts a multiplicative updating rule as the solver:\nC i+1 = C i \u2022 [W (2) T W (2) ] + + [W (2) T W (2) ] \u2212 C i [W (2) T W (2) ] \u2212 + [W (2) T W (2) ] + C i (6) where [A] + = A+|A| 2 , [A] \u2212 = A\u2212|A| 2\n, and |A| is the absolute values of A. Please refer to Huang et al. (2018) for more detail.\nW (2) T W\n(2) can be considered as K(W (2) , W (2) ). In this paper, we use a Gaussian kernel. Since the kernel value is positive and K(W (2) , W (2) ) = 1, the updating rule of Semi-NMF algorithm can be modified as:\nC i+1 = C i \u2022 K(W (2) , W (2) ) K(W (2) , W (2) )C i(7)\nAlgorithm 2 illustrates the steps of the Representative Ranking, where for each layer the coefficient matrix C is independently calculated using Equation 7. The algorithm then returns the diagonal of C as the ranking score of the filters. The width of the Gaussian kernel \u03c3, and the convergence rate \u03b1 are hyperparameters. In our experiments, we observe that setting \u03c3 = 1.0 and \u03b1 = 0.01 works for all tasks considered. Moreover, on average it takes less than 20 iterations to converge.\n\n\nDATA-DRIVEN RANKING (D2)\n\nRepresentative Ranking (R2) assumes H (1) is unknown and ranks the filters solely based on the weights W (2) . One could imagine using a similar ranking approach over W\n\n(1) T to rank the N filters. However, as mentioned, the convex hull is only a good representative for finding the maxima of any linear function, and the activation function \u03c3 makes H (1) nonlinear.\n\nTherefore, to incorporate the nonlinearity introduced by the activation function, Data-Driven (D2) performs a forward pass using sampled unlabeled data, and gathers statistics on the output results of each layer H (1) . It then uses the normalized average of these outputs to rank filters in each layer (Algorithm 1 lines (5 to 7)).\n\n\nMERGE AND SCALE\n\nThus far, KCM ranks N filters of each layer independently by the filter Representative Ranking (R2) and Data-Driven Ranking (D2). In every layer, R2D2 combines the scores of R2 and D2 to capture the importance of filters based on the model weights and the layer outputs of the raw data (Algorithm 1 line 9). In our experiments, we run an ablation study to present the importance of these rankings.\n\nGiven a FLOPs constraint C, let k be the total number of filters that satisfy C. In other words, the pruned model only should have k active filters across all layers and the rest should be removed. As shown in Algorithm 1, KCM merges the R2D2 scores (S R2D2 ) across layers, and the top k filters are selected to be active in the pruned model (Algorithm 1 lines 10-12).\n\nSince after masking some accuracy drop is inevitable, existing structured pruning methods have shown that scaling can be helpful. Thus, similar to Kwon et al. (2022) as shown in Figure 1, we apply a scaling transformation to the selected filters. Such scaling uses only the unlabeled data and, based on the generated mask, aims to reconstruct the layer-wise outputs by scaling the outputs of the active filters. This, in fact, reduces the feature map loss in Equation 4. Table 2. Accuracy degradation of pruning BERTBASE using our method and the prior structured pruning methods with different relative FLOPs. Note that our method is gradient-free (!\u2207), does not use label of the data and not require retraining (more detail in Table 1)  (Li et al., 2016) with the scaling approach from (Li et al., 2016). Mask-Tuning uses labeled data but KCM and Weight-Magnitude-Scale are gradient-free with unlabeled data (Table 1). KCM outperforms Weight-Magnitude and Weight-Magnitude-Scale which highlights the effectiveness of our approach in the absence of labeled data. For 70% and 60% FLOPs constraints, Mask-Tuning that uses labeled data performs slightly better than KCM. Table 3 shows this gap more clearly.\n!\u2207 Method QQP QNLI SST-2 MRPC \u223c60% \u223c65% 75% \u223c60% \u223c65% 75% \u223c60% \u223c65% 75% \u223c60% \u223c65% 75% FLOP \u2212 \u2212 \u2212 \u2212 -2.6 \u2212 \u2212 -0.6 \u2212 \u2212 -2.3 \u2212 SLIP -1.7 -0.9 \u2212 -2.1 -0.9 \u2212 -1.0 -0.9 \u2212 -1.0 -2.8 \u2212 Sajjad \u2212 -0.4 \u2212 \u2212 -1.4 \u2212 \u2212 -1.8 \u2212 \u2212 -8.6 \u2212 DynaBERT \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 -0.6 \u2212 \u2212 -1.7 EBERT -0.4 \u2212 \u2212 -1.3 \u2212 -1.0 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Mask-\n\nEvaluation\n\n\nExperimental Setup\n\nWe implemented our framework with PyTorch (Paszke et al., 2019) using the HuggingFace Transformers  library. We evaluate the effectiveness of the proposed approach using BERT BASE  and Distil-BERT (Sanh et al., 2019) on GLUE  and SQuAD  benchmarks.\n\nFor the Data-Driven ranking we use 2K raw data from the training sets. Note that we only use raw input and no label is used. Figure 6, in Appendix B, shows how sample size affects our performance. For the Representative Ranking cal-culation in Algorithm 2, the width of the Gaussian kernel, \u03c3, and the convergence rate, \u03b1, are the hyperparameters. In our experiments, we set \u03c3 = 1.0 and \u03b1 = 0.01. Moreover, on average it takes less than 20 iterations to converge. All results are averaged over the runs with 10 different seeds. Please refer to Appendix A for more detail on experimental setup.\n\nBaselines from structured pruning methods: Table 1, shows the comparison of the different structured pruning methods specialized for Transformers studied in this work. We compare these methods by 4 important features, including gradient-free (no backward pass), retrain/finetune-free (no retrain/finetune), supervision-free (no use of labeled data), and fast pruning-time. We compare our proposed  (Sajjad et al., 2023), DynaBERT (Hou et al., 2020), and EBERT (Liu et al., 2021b). All of these techniques require retraining of the pruned model and/or jointly learning the pruning configurations during training, which leads to high training time, and they are not gradientfree. Specifically, as shown in Kwon et al. (2022) these methods require 5 to 33 hours of retraining.\n\nMask-Tuning (Kwon et al., 2022) is a recent work that does not need retraining but still relies on the labeled data and uses gradient computation to evaluate the importance of each filter. We also compare our method with Weight-Magnitude (Li et al., 2016), which is a light-structured pruning method that is gradient-free, does not retrain the pruned model, and does not use the data at all. We introduce Weight-Magnitude-scale that combines Li et al. (2016) with the scaling approach from Li et al. (2016). Note that the scaling step in Li et al. (2016) only needs unlabeled data, so Weight-Magnitude-scale will have the exact problem setup as our method 1. We would like to highlight that our method KCM, Mask-Tuning, Weight-Magnitude, and Weight-Magnitude-scale finish in less than 7 minutes across all tasks, which is 2 to 3 orders of magnitude faster than the other baselines. We evaluate the performance of our method against all these baselines by the FLOPs-accuracy trade-off of BERT BASE on the GLUE and SQuAD benchmarks. In the experimental results, to simplify the notation, we will indicate gradient-free with (!\u2207). Table 2 compares the accuracy drop of KCM against prior structured pruning methods in Table 1. Since the baseline accuracy differs slightly from paper to paper, we compare the amount of the accuracy drop from the baseline instead of the absolute accuracy. Similar to Kwon et al. (2022), we use the results without knowledge distillation and data augmentation reported in each paper since these add extra overhead. As one can see, the highest accuracy drop of KCM across all task is \u22123.62 which reduces 40% of the original FLOPs. Worth mentioning that, while all the baselines require labeled data and leverage the backward pass, our proposed method is gradient-free with unlabeled data.\n\n\nExperimental Results\n\nNext, we perform a more thorough evaluation against Mask-Tuning (Kwon et al., 2022), Weight-Magnitude (Li et al., 2016) and Weight-Magnitude-Scale, since their problem setup is closer to ours (Table 1). Figure 2 shows the results on BERT BASE as we vary the FLOPs constraint from 90% to 60%, i.e, reducing 10% to 40% of the original FLOPs. Clearly, KCM outperforms Weight-Magnitude and Weight-Magnitude-Scale, highlighting the effectiveness of our approach in the absence of labeled data. For the 70% and 60% FLOPs constraints, Mask-Tuning performs slightly better than ours. Table 3 shows the gap more clearly. This gap can be explained by the fact that, unlike Mask-Tuning, KCM is gradient-free with unlabeled data.\n\nWe further evaluate the performance of KCM against Mask-Tuning (Kwon et al., 2022) on DistilBERT for the 70% and 60% FLOPs constraints. As shown in Table 4, interestingly, even though Mask-Tuning leverages the backward pass and labeled data, the proposed KCM performs better than Mask-Tuning on the SQUAD 2.0 benchmark. Moreover the results of both approaches on QQP, and STS-B are quite comparable, showing that even without labeled data and no backward pass the accuracy loss of the pruned model by our KCM method can be minimal.\n\n\nAblation Studies\n\nImportance of our ranking techniques: R2D2 is the core component of our proposed approach KCM that combines the ranking of Representative Ranking (R2) and Data-Driven (D2) (Section 3). We run an ablation study to investigate the importance of these ranking techniques. Recall that R2 ranks N filters based on the weights of the FFNs, while D2 ranks them by the output of the activation function. Figure 3 illustrates how the performance changes if we only use one of these in our framework. While D2-only is our KCM without the R2, R2-only only uses the R2 ranking. As one can see, except in STS-B where D2-only slightly outperforms KCM, using R2-only performs better than D2-only. More importantly, when R2D2 combines them it allows our KCM to leverage both rankings and demonstrates improve-ment across all tasks. Note that the results of D2-only also confirm that using only the output of the activation functions is not always sufficient for pruning and highlights the impact of using the trained model weights (More results in Figure 5).\n\nDynamic neuron selection: Another important feature of our KCM is the fact that it dynamically decides how many neurons from each layer to prune. This feature is an outcome of merging the result of R2D2 across all L layers. Figure 4 illustrates how KCM affects different layers of the BERT BASE . Clearly more pruning occurs over the last three layers, and more than half of the filters in the first two layers are pruned. From KCM point of view, the middle layers seem to be more important across all tasks.\n\n\nDiscussion\n\nOur KCM is a gradient-free structured pruning framework that neither requires retraining nor labeled data. Here we would like to discuss what if we have a limited labeled data and how our approach can be extended to leverage that.\n\nRecall that R2D2 uses the statistics from the unlabeled data to rank filters based on layer-wise output. Thus a simple add-on would be to freeze the trained model, use the limited labeled data and only do one forward-backward pass and gather the gradient over the mask variables. Note that unlike Mask-Tuning (Kwon et al., 2022), we do not calculate the Fisher information since we just want to use the gradient as the new signal for the pruning. To do so, for example layer_1 layer_2 layer_3 layer_4 layer_5 layer_6 layer_7 layer_8 layer_9 layer_10 layer_11 layer_12 1) the gradient information can be used as a new ranking criteria that can be combined into our R2D2 or 2) one can use it to refine the top-k results of our KCM. Specifically, let us assume f i be the least important filter in top-k result of the KCM, and f j be the most important one from the gradient scores. If f j is not already in the top-k results, we can switch f i with f j if the total gradient of the top-k results increases.\n\nWe implemented this simple greedy solution as an add-on to our KCM and show that indeed having a limited labeled data contributes to improve the accuracy drop. Table 5 shows the result on DistilBERT where only 512 sampled label data is available. Since this is out of the scope of this work, We leave a more thorough investigation as a future work.\n\n\nRelated Work\n\nThere has been a lot of work on efficient transformers that improve inference speed and reduce memory usage, including efficient architecture design (Kitaev et al., 2020;Iandola et al., 2020;Sun et al., 2020;Wang et al., 2020b;Wu et al., 2020;Fakoor et al., 2020a;Lan et al., 2019;Lee et al., 2022), neural architecture search (So et al., 2021;Chen et al., 2020a;So et al., 2019;Wang et al., 2020a;Xu et al., 2021;Yin et al., 2021), knowledge distillation (Sun et al., 2020;Jiao et al., 2019;Sanh et al., 2019;Sun et al., 2019;Fakoor et al., 2020b), quantization Shen et al., 2020;Zadeh et al., 2020;Zafrir et al., 2019), and hardwaresoftware co-design (Ham et al., 2021;Tambe et al., 2021;Wang et al., 2021;Gu et al., 2022;Shi et al., 2018).\n\nPruning is an important area of research for model sparsity that removes insignificant weights in neural networks. While Kurtic et al. (2022); Sanh et al. (2020); Gale et al. (2019); Zhang et al. (2022) proposed second-order, first-order, and magnitude-based pruning methods for Transformers, Chen et al. (2020b;; Prasanna et al. (2020) explored the lottery ticket hypothesis. These methods can significantly reduce the model size; however, they might not offer significant inference speedup since the hardware and cannot efficiently utilize the unstructured sparse patterns.\n\nStructured pruning methods, on the other hand, target removing groups of parameters. For example, low-rank factorization (Gu et al., 2022;Wang et al., 2019), corsets based techniques (Mussay et al., 2021;Liebenwein et al., 2019;Baykal et al., 2018), block-wise sparsity , and tile-wise sparsity (Guo et al., 2020a) prune structured sets of parameters in weight matrices. Additionally, attention head pruning (Michel et al., 2019;Voita et al., 2019) and layer dropping (Fan et al., 2019;Sajjad et al., 2023;Peer et al., 2022) have been commonly used as more coarse-grained methods. Recent research has also explored combining different pruning granularity and principles to maximize model efficiency in all dimensions Khetan & Karnin, 2020;Lagunas et al., 2021;Lin et al., 2020;Liu et al., 2021a;Xia et al., 2022;Yao et al., 2021). Another approach is to dynamically prune Transformers during inference time (Fan et al., 2019;Hou et al., 2020;Liu et al., 2021b;Xin et al., 2020;Zhou et al., 2020).\n\nEven though structured pruning methods can be effective for compression and speedup, they can be difficult to implement in practice due to the high computational cost and complexity of the process. Additional training during or after pruning can be up to 10 times more expensive than original model training (Lagunas et al., 2021;Xia et al., 2022), and the pruning pipeline often requires rewriting the training code and involves many additional hyperparameters to adjust (Hou et al., 2020;Lan et al., 2019;Liu et al., 2021a;Yao et al., 2021).  Pruning in an unsupervised setting has been studied in Guo et al. (2020b); Browne et al. (2020; for spiking neural networks and fully-connected layers; however, the pruning either happens during training or still requires retraining of the pruned model. In contrast, our structured pruning method neither requires retraining nor labeled data. nsupervised: (Guo et al., 2020b) proposed an unsupervised online adaptive weight pruning method that dynamically removes non-critical weights from a spiking neural network (SNN). (Browne et al., 2020; uses unsupervised kmeans clustering to detect clusters of similar filters, and nodes in fully-connected layers, and prunes those that are redundant. (Aghasi et al., 2020): convex post-processing module, which prunes (sparsifies) a trained network layer by layer, while preserving the internal responses.\n\nrepresentative selection problem, low rank decomposition: There has been a lot of work on finding such a representative set (Kazemi et al., 2022;Killamsetty et al., 2021;You et al., 2020) that require training. However since W\n\n(\n2) H (1) + b (2)\nis a linear function this problem can be reduced to finding convex hull over W (2) .\n\n\nConclusion\n\nIn this work, we studied the problem of structured pruning with unlabeled data and no backward pass. We proposed a gradient-free structured pruning framework that prunes the filters with the help of our proposed R2D2 that combines two ranking techniques called Representative Ranking (R2) and Data-Driven (D2). We empirically evaluated our frame-work on GLUE and SQuAD benchmarks using BERT BASE and DistilBERT. Compared to when the labeled data is available, our approach achieved up to 40% FLOPs reduction with less than 4% accuracy loss over all tasks considered. \n\n\nB.3. Speedup\n\nWe evaluated the latency on real hardware and obtained the speedup of KCM on BERT BASE on a single NVIDIA V100 GPU for 60% Flops constraint: We conducted additional experiments on a larger-scale model, BERT LARGE over SQuAD 1.1 . The results in Table 7 indicate that our method outperforms unsupervised baselines, providing further evidence of its efficacy. \n\n\nD. Train-Test Data Discrepancy\n\nWe ran new experiments with a new dataset called new-Wiki to further evaluate the effectiveness of the proposed method under training-test data discrepancy. As outlined in Miller et al. (2020), new-Wiki is different from the original SQuAD 1.1 dataset and was generated using the Wikipedia dataset.\n\nWe explored various scenarios involving the sampling of unlabeled data from datasets that differ from the evaluation dataset. In particular, we sample unlabeled data from 1) SQuAD 1.1 -train 2) SQuAD 1.1 -val or 3) new-Wiki and evaluate on SQuAD 1.1 -val. As evident from the results in Table 9, sampling unlabeled data from new-Wiki (and evaluating on SQuAD 1.1 -val) yielded improved performance compared to sampling from SQuAD 1.1 -train and evaluating on SQuAD 1.1 -val. This finding further supports our assertion regarding the applicability and effectiveness of our approach. We further evaluate the effectiveness of our approach on using the finetuned model on SQuAD 1.1 but evaluate on new-Wiki and results are as follows: \n\nFigure 1 .\n1Kernelized Convex Masking (KCM): A gradient-free structured pruning framework with R2D2 as a core component. R2D2 combines the ranks of the Representative Ranking (R2) and Data-Driven (D2) rank.\n\n\nGiven a trained model Model, unlabeled dataset D, and a cost constraint C, find the mask M \u2208 R L\u00d7N for every N Algorithm 1 Kernelized Convex Masking (KCM) 1: Input: Trained model: Model, FLOPs constraint C, Gaussian Kernel K, convergence rate \u03b1 2: Output: Mask M 3: Initialize mask M as 0 //Call Representative Ranking (R2) Algorithm 2 4: SR2 = R2(Model, K, \u03b1) // Data-Driven (D2) Ranking 5: for batch in sample-data do 6: for each layer in Model collect H (1) 7: SD2[ ] = average over H (1) for each filter 8: end for 9: SR2D2[ ] = SR2[ ] * normalized(SD2[ ]) 10: k = Number of neurons to satisfy FLOPs constraint C 11: Candidates = top-k filters of the sorted SR2D2 12: M[Candidates] = 1.0 13: return M Algorithm 2 Representative Ranking (R2) 1: Input: Trained model: Model, Gaussian Kernel K with width \u03c3, convergence rate \u03b1 2: Output: SR2 that represents importance of Filters in all layers. 3: for layer in layers of the Model do 4:\n\nFigure 2 .\n2Performance of our pruning framework KCM against Mask-Tuning (Kwon et al., 2022), Weight-Magnitude (Li et al., 2016), and Weight-Magnitude-Scale on BERTBASE. Weight-Magnitude-Scale combines\n\nFigure 3 .\n3Ablation Study to investigate the importance of our ranking techniques. D2-only, and R2-only are our KCM that either uses Data-Driven or Representative Ranking. R2-only performs better than D2-only in all tasks except STS-B. But KCM that combines them by R2D2 is able to leverage both of these rankings and shows improvement across all tasks considered.\n\nFigure 4 .\n4Ablation Study to investigate how many filters from each layer are pruned by KCM on BERTBASE. While middle layers seems to less get affected by KCM, many filters from the last three layers and first layer are pruned.\n\n\nTo tackle this, post-training model compression has been recently studied in Kwon et al. (2022); Hubara et al. (2021); Frantar & Alistarh (2022). While Hubara et al. (2021; 2020); Banner et al. (2019) improves post training neural quantization, Kwon et al. (2022) proposed a fast post-training structured pruning framework for Transformers. Even though this approach avoids expensive retraining, it requires labeled data in the pruning pipeline.\n\nFigure 6 .\n6Performance of KCM on DistilBERT by varying unlabeled data sample size.\n\nTable 3 .Table 4 .\n34Performance of our pruning framework KCM against Mask-Tuning(Kwon et al., 2022) on BERTBASE, for 70% and 60% FLOPs constraints. Mask-Tuning that uses labeled data performs slightly better than KCM. Unlike Mask-Tuning, KCM is gradient-free(!\u2207) with unlabeled data. Performance of our pruning framework KCM against Mask-Tuning(Kwon et al., 2022) on DistilBERT, for 70% and 60% FLOPs constraints. Even though Mask-Tuning uses labeled data, for SQUAD2.0 task, KCM performs better than Mask-Tuning, and the results of both approaches on QQP, and STS-B are comparable. Tuning 90.44 \u00b1 0.41 90.93 \u00b1 0.24 85.73 \u00b1 0.07 85.96 \u00b1 0.10 83.20 \u00b1 0.16 84.64 \u00b1 0.09 62.36 \u00b1 1.40 65.32 \u00b1 0.48 KCM (Ours) 88.38 \u00b1 0.25 90.61 \u00b1 0.25 85.26 \u00b1 0.02 85.55 \u00b1 0.03 76.92 \u00b1 0.11 82.65 \u00b1 0.06 64.56 \u00b1 0.11 68.19 \u00b1 0.06 method with Flop (Wang et al., 2019), SLIP (Lin et al., 2020), Sajjad et al.!\u2207 \nMethod \nQQP \nMNLI \nMRPC \nQNLI \n60% \n70% \n60% \n70% \n60% \n70% \n60% \n70% \nbaseline \n91.00 \n84.53 \n86.27 \n91.41 \n\nMask-Tuning 90.38 \u00b1 0.07 90.74 \u00b1 0.07 82.26 \u00b1 0.21 83.24 \u00b1 0.16 84.51 \u00b1 0.63 85.91 \u00b1 0.40 90.00 \u00b1 0.26 90.83 \u00b1 0.16 \nKCM (Ours) 89.15 \u00b1 0.04 90.39 \u00b1 0.04 77.24 \u00b1 0.10 81.18 \u00b1 0.10 84.19 \u00b1 0.44 84.46 \u00b1 0.29 87.79 \u00b1 0.15 90.58 \u00b1 0.08 \n\n!\u2207 \nMethod \nSST-2 \nSTS-B \nSQuAD 1.1 \nSQuAD 2.0 \n60% \n70% \n60% \n70% \n60% \n70% \n60% \n70% \nbaseline \n93.57 \n88.59 \n88.48 \n76.82 \n\nMask-Tuning 92.47 \u00b1 0.41 92.92 \u00b1 0.26 87.95 \u00b1 0.12 88.40 \u00b1 0.05 85.77 \u00b1 0.41 87.57 \u00b1 0.11 73.86 \u00b1 0.55 76.00 \u00b1 0.29 \nKCM (Ours) 91.11 \u00b1 0.23 92.26 \u00b1 0.09 85.72 \u00b1 0.12 86.66 \u00b1 0.05 81.29 \u00b1 0.06 85.89 \u00b1 0.04 70.30 \u00b1 0.13 75.24 \u00b1 0.10 \n\n!\u2207 \nMethod \nQQP \nMNLI \nMRPC \nQNLI \n60% \n70% \n60% \n70% \n60% \n70% \n60% \n70% \nbaseline \n89.99 \n82.11 \n84.80 \n88.56 \n\nMask-Tuning 88.71 \u00b1 0.22 89.66 \u00b1 0.06 80.51 \u00b1 0.19 81.65 \u00b1 0.09 84.73 \u00b1 0.71 84.83 \u00b1 0.35 87.72 \u00b1 0.38 88.43 \u00b1 0.07 \nKCM (Ours) 88.16 \u00b1 0.03 89.28 \u00b1 0.03 78.05 \u00b1 0.08 80.60 \u00b1 0.05 79.66 \u00b1 0.27 83.01 \u00b1 0.16 85.93 \u00b1 0.09 86.93 \u00b1 0.13 \n\n!\u2207 \nMethod \nSST-2 \nSTS-B \nSQuAD 1.1 \nSQuAD 2.0 \n60% \n70% \n60% \n70% \n60% \n70% \n60% \n70% \nbaseline \n91.40 \n86.12 \n85.73 \n68.84 \n\nMask-\n\nTable 5 .\n5How few labeled data improves accuracy of our pruning framework KCM on DistilBERT. Tuning 88.71 \u00b1 0.22 89.66 \u00b1 0.06 80.51 \u00b1 0.19 81.65 \u00b1 0.09 84.73 \u00b1 0.71 84.83 \u00b1 0.35 87.72 \u00b1 0.38 88.43 \u00b1 0.07 KCM 88.16 \u00b1 0.03 89.28 \u00b1 0.03 78.05 \u00b1 0.08 80.60 \u00b1 0.05 79.66 \u00b1 0.27 83.01 \u00b1 0.16 85.93 \u00b1 0.09 86.93 \u00b1 0.13 Extension(512 labeled data) 88.76 \u00b1 0.25 89.45 \u00b1 0.07 80.02 \u00b1 0.25 81.37 \u00b1 0.11 83.70 \u00b1 1.40 84.49 \u00b1 0.49 87.21 \u00b1 0.54 88.21 \u00b1 0.15 Extension(1k labeled data) 88.92 \u00b1 0.20 89.53 \u00b1 0.08 80.41 \u00b1 0.11 81.50 \u00b1 0.12 84.17 \u00b1 0.45 84.68 \u00b1 0.49 87.60 \u00b1 0.31 88.29 \u00b1 0.16 Extension(512 labeled data) 89.32 \u00b1 0.54 90.38 \u00b1 0.35 85.83 \u00b1 0.11 86.02 \u00b1 0.07 81.41 \u00b1 0.26 83.30 \u00b1 0.09 66.51 \u00b1 0.24 67.72 \u00b1 0.18 Extension(1k labeled data) 89.86 \u00b1 0.56 90.62 \u00b1 0.40 85.90 \u00b1 0.11 86.04 \u00b1 0.06 81.16 \u00b1 0.22 83.34 \u00b1 0.11 66.35 \u00b1 0.32 67.74 \u00b1 0.18!\u2207 \nMethod \nQQP \nMNLI \nMRPC \nQNLI \n60% \n70% \n60% \n70% \n60% \n70% \n60% \n70% \nbaseline \n89.99 \n82.11 \n84.80 \n88.56 \n\nMask-!\u2207 \nMethod \nSST-2 \nSTS-B \nSQuAD 1.1 \nSQuad 2.1 \n60% \n70% \n60% \n70% \n60% \n70% \n60% \n70% \nbaseline \n91.40 \n86.12 \n85.73 \n68.84 \n\nMask-Tuning \n90.44 \u00b1 0.41 90.93 \u00b1 0.24 85.73 \u00b1 0.07 85.96 \u00b1 0.10 83.20 \u00b1 0.16 84.64 \u00b1 0.09 62.36 \u00b1 1.40 65.32 \u00b1 0.48 \nKCM \n88.38 \u00b1 0.25 90.61 \u00b1 0.25 85.26 \u00b1 0.02 85.55 \u00b1 0.03 76.92 \u00b1 0.11 82.65 \u00b1 0.06 64.56 \u00b1 0.11 68.19 \u00b1 0.06 \n\n\nTable 6 .\n6Speedup of KCM on BERTBASE on a single NVIDIA V100 GPU for 60% Flops constraint. Method QQP MNLI MRPC QNLI SST-2 STS-B SQuAD 1.1 SQuAD 2.0 C. SQuAD 1.1 Task on BERT LARGEspeedup 1.58x 1.46x \n1.53x \n1.57x 1.58x \n1.59x \n1.47x \n1.44x \n\n\n\nTable 7 .\n7KCM outperforms Weight-magnitude, and Weight-magnitude-Scale on BERTLARGE over SQuAD1.1.Table 8. How few labeled data improves accuracy of our pruning framework KCM on BERTLARGE for SQuAD1.1. 86 \u00b1 0.14 88.57 \u00b1 0.06 91.40 \u00b1 0.04 92.72 \u00b1 0.03 Extension(512 labeled data) 89.44 \u00b1 0.1 91.85 \u00b1 0.06 92.40 \u00b1 0.05 93.00 \u00b1 0.01 Extension(1k labeled data) 89.48 \u00b1 0.16 91.92 \u00b1 0.07 92.69 \u00b1 0.02 93.17 \u00b1 0.04Method \n60% \n70% \n80% \n90% \nWeight-Magnitude (Li et al., 2016) \n2.3029 \n2.8365 \n4.5763 \n52.5013 \nWeight-Magnitude-Scale \n2.3178 \n24.6811 \n83.6422 \n91.4438 \nKCM (ours) \n85.86 \u00b1 0.14 88.57 \u00b1 0.06 91.40 \u00b1 0.04 92.72 \u00b1 0.03 \n\nMethod \n60% \n70% \n80% \n90% \nKCM \n85.\n\nTable 9 .\n9Train-Test data discrepancy SQuAD 1.1 -train SQuAD 1.1 -val 76.92 \u00b1 0.11 82.65 \u00b1 0.06 SQuAD 1.1 -val SQuAD 1.1 -val 77.17 \u00b1 0.073 82.75 \u00b1 0.074 new-Wiki SQuAD 1.1 -val 77.45 \u00b1 0.076 82.80 \u00b1 0.031Unlabeled Sample \nEvaluation \n60% \n70% \n\n\nTable 10 .\n10Performance of Finetuned model on SQuAD1.1 on new-Wiki dataset. SQuAD 1.1 -train new-Wiki 75.26 \u00b1 0.08 81.14 \u00b1 0.096 new-Wiki new-Wiki 75.72 \u00b1 0.09 81.08 \u00b1 0.13Unlabeled Sample \nEvaluation \n60% \n70% \n\nGoogle DeepMind 2 University of Alberta. Correspondence to: Azade Nova <azade@google.com>.\n\nFast convex pruning of deep neural networks. A Aghasi, A Abdi, J Romberg, SIAM Journal on Mathematics of Data Science. 21Aghasi, A., Abdi, A., and Romberg, J. Fast convex pruning of deep neural networks. SIAM Journal on Mathematics of Data Science, 2(1):158-188, 2020.\n\nPost training 4-bit quantization of convolutional networks for rapiddeployment. R Banner, Y Nahshan, D Soudry, Advances in Neural Information Processing Systems. 32Banner, R., Nahshan, Y., and Soudry, D. Post training 4-bit quantization of convolutional networks for rapid- deployment. Advances in Neural Information Processing Systems, 32, 2019.\n\nData-dependent coresets for compressing neural networks with applications to generalization bounds. C Baykal, L Liebenwein, I Gilitschenski, D Feldman, D Rus, arXiv:1804.05345arXiv preprintBaykal, C., Liebenwein, L., Gilitschenski, I., Feldman, D., and Rus, D. Data-dependent coresets for compress- ing neural networks with applications to generalization bounds. arXiv preprint arXiv:1804.05345, 2018.\n\nPulsenetone: fast unsupervised pruning of convolutional neural networks for remote sensing. D Browne, M Giering, S Prestwich, Remote Sensing. 1271092Browne, D., Giering, M., and Prestwich, S. Pulsenetone: fast unsupervised pruning of convolutional neural net- works for remote sensing. Remote Sensing, 12(7):1092, 2020.\n\nUnsupervised pulsenet: Automated pruning of convolutional neural networks by k-means clustering. D Browne, M Giering, S Prestwich, International Conference on Machine Learning, Optimization, and Data Science. SpringerBrowne, D., Giering, M., and Prestwich, S. Unsupervised pulsenet: Automated pruning of convolutional neural net- works by k-means clustering. In International Conference on Machine Learning, Optimization, and Data Science, pp. 172-184. Springer, 2021.\n\nSemantic textual similarity-multilingual and crosslingual focused evaluation. D Cer, M Diab, E Agirre, I Lopez-Gazpio, L Specia, 10.18653/v1/s17-2001Proceedings of the 2017 SEMVAL International Workshop on Semantic Evaluation. the 2017 SEMVAL International Workshop on Semantic EvaluationCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. Semantic textual similarity-multilingual and cross- lingual focused evaluation. In Proceedings of the 2017 SEMVAL International Workshop on Semantic Evaluation (2017). https://doi. org/10.18653/v1/s17-2001, 2017.\n\nD Chen, Y Li, M Qiu, Z Wang, B Li, B Ding, H Deng, J Huang, W Lin, J Zhou, Adabert, arXiv:2001.04246Task-adaptive bert compression with differentiable neural architecture search. arXiv preprintChen, D., Li, Y., Qiu, M., Wang, Z., Li, B., Ding, B., Deng, H., Huang, J., Lin, W., and Zhou, J. Adabert: Task-adaptive bert compression with differentiable neural architecture search. arXiv preprint arXiv:2001.04246, 2020a.\n\nThe lottery ticket hypothesis for pretrained bert networks. T Chen, J Frankle, S Chang, S Liu, Y Zhang, Z Wang, M Carbin, Advances in neural information processing systems. 33Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre- trained bert networks. Advances in neural information processing systems, 33:15834-15846, 2020b.\n\nChasing sparsity in vision transformers: An end-to-end exploration. T Chen, Y Cheng, Z Gan, L Yuan, L Zhang, Wang , Z , Advances in Neural Information Processing Systems. 34Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., and Wang, Z. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems, 34:19974-19988, 2021.\n\nX Chen, Y Cheng, S Wang, Z Gan, Z Wang, J Liu, Earlybert, arXiv:2101.00063Efficient bert training via early-bird lottery tickets. arXiv preprintChen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., and Liu, J. Earlybert: Efficient bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063, 2020c.\n\nThe pascal recognising textual entailment challenge. I Dagan, O Glickman, B Magnini, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop. Southampton, UKSpringerRevised Selected PapersDagan, I., Glickman, O., and Magnini, B. The pascal recog- nising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entail- ment: First PASCAL Machine Learning Challenges Work- shop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, pp. 177-190. Springer, 2006.\n\nJ Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nConvex and seminonnegative matrix factorizations. C H Ding, T Li, Jordan , M I , IEEE transactions on pattern analysis and machine intelligence. 32Ding, C. H., Li, T., and Jordan, M. I. Convex and semi- nonnegative matrix factorizations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45-55, 2008.\n\nAutomatically constructing a corpus of sentential paraphrases. B Dolan, C Brockett, Third International Workshop on Paraphrasing (IWP2005). Dolan, B. and Brockett, C. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005), 2005.\n\nR Fakoor, P Chaudhari, J Mueller, A J Smola, Trade, arXiv:2004.02441Transformers for density estimation. arXiv preprintFakoor, R., Chaudhari, P., Mueller, J., and Smola, A. J. Trade: Transformers for density estimation. arXiv preprint arXiv:2004.02441, 2020a.\n\nFast, accurate, and simple models for tabular data via augmented distillation. R Fakoor, J W Mueller, N Erickson, P Chaudhari, A J Smola, Advances in Neural Information Processing Systems. 33Fakoor, R., Mueller, J. W., Erickson, N., Chaudhari, P., and Smola, A. J. Fast, accurate, and simple models for tabu- lar data via augmented distillation. Advances in Neural Information Processing Systems, 33:8671-8681, 2020b.\n\nA Fan, E Grave, Joulin , A , arXiv:1909.11556Reducing transformer depth on demand with structured dropout. arXiv preprintFan, A., Grave, E., and Joulin, A. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.\n\nOptimal brain compression: A framework for accurate post-training quantization and pruning. E Frantar, D Alistarh, arXiv:2208.11580arXiv preprintFrantar, E. and Alistarh, D. Optimal brain compression: A framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022.\n\nT Gale, E Elsen, S Hooker, arXiv:1902.09574The state of sparsity in deep neural networks. arXiv preprintGale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.\n\nCompressing large-scale transformer-based models: A case study on bert. P Ganesh, Y Chen, X Lou, M A Khan, Y Yang, H Sajjad, P Nakov, D Chen, M Winslett, Transactions of the Association for Computational Linguistics. 9Ganesh, P., Chen, Y., Lou, X., Khan, M. A., Yang, Y., Sajjad, H., Nakov, P., Chen, D., and Winslett, M. Compressing large-scale transformer-based models: A case study on bert. Transactions of the Association for Computational Linguistics, 9:1061-1080, 2021.\n\nHeat: Hardware-efficient automatic tensor decomposition for transformer compression. J Gu, B Keller, J Kossaifi, A Anandkumar, B Khailany, D Z Pan, arXiv:2211.16749arXiv preprintGu, J., Keller, B., Kossaifi, J., Anandkumar, A., Khailany, B., and Pan, D. Z. Heat: Hardware-efficient automatic tensor decomposition for transformer compression. arXiv preprint arXiv:2211.16749, 2022.\n\nAccelerating sparse dnn models without hardware-support via tile-wise sparsity. C Guo, B Y Hsueh, J Leng, Y Qiu, Y Guan, Z Wang, X Jia, X Li, M Guo, Y Zhu, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEEGuo, C., Hsueh, B. Y., Leng, J., Qiu, Y., Guan, Y., Wang, Z., Jia, X., Li, X., Guo, M., and Zhu, Y. Accelerating sparse dnn models without hardware-support via tile-wise sparsity. In SC20: International Conference for High Per- formance Computing, Networking, Storage and Analysis, pp. 1-15. IEEE, 2020a.\n\nUnsupervised adaptive weight pruning for energy-efficient neuromorphic systems. W Guo, M E Fouda, H E Yantir, A M Eltawil, K N Salama, Frontiers in Neuroscience. 14598876Guo, W., Fouda, M. E., Yantir, H. E., Eltawil, A. M., and Salama, K. N. Unsupervised adaptive weight pruning for energy-efficient neuromorphic systems. Frontiers in Neuroscience, 14:598876, 2020b.\n\nHardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. T J Ham, Y Lee, S H Seo, S Kim, H Choi, S J Jung, J W Lee, Elsa, 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEEHam, T. J., Lee, Y., Seo, S. H., Kim, S., Choi, H., Jung, S. J., and Lee, J. W. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. In 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pp. 692- 705. IEEE, 2021.\n\nGaussian error linear units (gelus). D Hendrycks, K Gimpel, arXiv:1606.08415arXiv preprintHendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\n\nL Hou, Z Huang, L Shang, X Jiang, X Chen, Q Liu, Dynabert, Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems. 33Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X., and Liu, Q. Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33:9782-9793, 2020.\n\nKernelized convex hull approximation and its applications in data description tasks. C Huang, Y Wu, G Min, Ying , Y , 2018 International Joint Conference on Neural Networks (IJCNN). IEEEHuang, C., Wu, Y., Min, G., and Ying, Y. Kernelized convex hull approximation and its applications in data description tasks. In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018.\n\nImproving post training neural quantization: Layerwise calibration and integer programming. I Hubara, Y Nahshan, Y Hanani, R Banner, D Soudry, arXiv:2006.10518arXiv preprintHubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D. Improving post training neural quantization: Layer- wise calibration and integer programming. arXiv preprint arXiv:2006.10518, 2020.\n\nAccelerated sparse neural training: A provable and efficient method to find n: m transposable masks. I Hubara, B Chmiel, M Island, R Banner, J Naor, D Soudry, Advances in Neural Information Processing Systems. 34Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, J., and Soudry, D. Accelerated sparse neural training: A provable and efficient method to find n: m transposable masks. Advances in Neural Information Processing Systems, 34: 21099-21111, 2021.\n\nF N Iandola, A E Shaw, R Krishna, K W Keutzer, Squeezebert, arXiv:2006.11316What can computer vision teach nlp about efficient neural networks. arXiv preprintIandola, F. N., Shaw, A. E., Krishna, R., and Keutzer, K. W. Squeezebert: What can computer vision teach nlp about efficient neural networks? arXiv preprint arXiv:2006.11316, 2020.\n\nX Jiao, Y Yin, L Shang, X Jiang, X Chen, L Li, F Wang, Q Liu, Tinybert, arXiv:1909.10351Distilling bert for natural language understanding. arXiv preprintJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.\n\nS M Kazemi, A Tsitsulin, H Esfandiari, M Bateni, D Ramachandran, B Perozzi, Mirrokni , arXiv:2205.10403Tackling provably hard representative selection via graph neural networks. arXiv preprintKazemi, S. M., Tsitsulin, A., Esfandiari, H., Bateni, M., Ra- machandran, D., Perozzi, B., and Mirrokni, V. Tackling provably hard representative selection via graph neural networks. arXiv preprint arXiv:2205.10403, 2022.\n\nA Khetan, Z Karnin, Schubert, arXiv:2005.06628Optimizing elements of bert. arXiv preprintKhetan, A. and Karnin, Z. schubert: Optimizing elements of bert. arXiv preprint arXiv:2005.06628, 2020.\n\nRetrieve: Coreset selection for efficient and robust semi-supervised learning. K Killamsetty, X Zhao, F Chen, R Iyer, Advances in Neural Information Processing Systems. 34Killamsetty, K., Zhao, X., Chen, F., and Iyer, R. Retrieve: Coreset selection for efficient and robust semi-supervised learning. Advances in Neural Information Processing Systems, 34:14488-14501, 2021.\n\nInteger-only bert quantization. S Kim, A Gholami, Z Yao, M W Mahoney, K Keutzer, International conference on machine learning. PMLRKim, S., Gholami, A., Yao, Z., Mahoney, M. W., and Keutzer, K. I-bert: Integer-only bert quantization. In International conference on machine learning, pp. 5506- 5518. PMLR, 2021.\n\nReformer: The efficient transformer. N Kitaev, \u0141 Kaiser, A Levskaya, arXiv:2001.04451arXiv preprintKitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.\n\nThe optimal bert surgeon: Scalable and accurate second-order pruning for large language models. E Kurtic, D Campos, T Nguyen, E Frantar, M Kurtz, B Fineran, M Goin, Alistarh , D , arXiv:2203.07259arXiv preprintKurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B., Goin, M., and Alistarh, D. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.\n\nA fast post-training pruning framework for transformers. W Kwon, S Kim, M W Mahoney, J Hassoun, K Keutzer, A Gholami, arXiv:2204.09656arXiv preprintKwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer, K., and Gholami, A. A fast post-training pruning frame- work for transformers. arXiv preprint arXiv:2204.09656, 2022.\n\nBlock pruning for faster transformers. F Lagunas, E Charlaix, V Sanh, A M Rush, arXiv:2109.04838arXiv preprintLagunas, F., Charlaix, E., Sanh, V., and Rush, A. M. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838, 2021.\n\nAlbert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.11942arXiv preprintLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n\nM Lee, K Han, M C Shin, Littlebird, arXiv:2210.11870Efficient faster & longer transformer for question answering. arXiv preprintLee, M., Han, K., and Shin, M. C. Littlebird: Efficient faster & longer transformer for question answering. arXiv preprint arXiv:2210.11870, 2022.\n\nThe winograd schema challenge. H Levesque, E Davis, L Morgenstern, Thirteenth international conference on the principles of knowledge representation and reasoning. Levesque, H., Davis, E., and Morgenstern, L. The winograd schema challenge. In Thirteenth international confer- ence on the principles of knowledge representation and reasoning, 2012.\n\nB Li, Z Kong, T Zhang, J Li, Z Li, H Liu, C Ding, arXiv:2009.08065Efficient transformer-based large scale language representations using hardware-friendly block structured pruning. arXiv preprintLi, B., Kong, Z., Zhang, T., Li, J., Li, Z., Liu, H., and Ding, C. Efficient transformer-based large scale language representations using hardware-friendly block structured pruning. arXiv preprint arXiv:2009.08065, 2020.\n\nPruning filters for efficient convnets. H Li, A Kadav, I Durdanovic, H Samet, H P Graf, arXiv:1608.08710arXiv preprintLi, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n\nProvable filter pruning for efficient neural networks. L Liebenwein, C Baykal, H Lang, D Feldman, D Rus, arXiv:1911.07412arXiv preprintLiebenwein, L., Baykal, C., Lang, H., Feldman, D., and Rus, D. Provable filter pruning for efficient neural networks. arXiv preprint arXiv:1911.07412, 2019.\n\nPruning redundant mappings in transformer models via spectral-normalized identity prior. Z Lin, J Z Liu, Z Yang, N Hua, Roth , D , arXiv:2010.01791arXiv preprintLin, Z., Liu, J. Z., Yang, Z., Hua, N., and Roth, D. Pruning redundant mappings in transformer models via spectral-normalized identity prior. arXiv preprint arXiv:2010.01791, 2020.\n\nRefined bert compression with integrated techniques. Y Liu, Z Lin, F Yuan, Rosita, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Liu, Y., Lin, Z., and Yuan, F. Rosita: Refined bert compres- sion with integrated techniques. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8715-8722, 2021a.\n\nEfficient bert inference with dynamic structured pruning. Z Liu, F Li, G Li, J Cheng, Ebert, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Liu, Z., Li, F., Li, G., and Cheng, J. Ebert: Efficient bert inference with dynamic structured pruning. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021, pp. 4814-4823, 2021b.\n\nAre sixteen heads really better than one?. P Michel, O Levy, G Neubig, Advances in neural information processing systems. 32Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019.\n\nThe effect of natural distribution shift on question answering models. J Miller, K Krauth, B Recht, L Schmidt, International Conference on Machine Learning. PMLRMiller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question answering models. In International Conference on Machine Learning, pp. 6905-6916. PMLR, 2020.\n\nData-independent neural pruning via coresets. B Mussay, M Osadchy, V Braverman, S Zhou, D Feldman, arXiv:1907.04018arXiv preprintMussay, B., Osadchy, M., Braverman, V., Zhou, S., and Feldman, D. Data-independent neural pruning via core- sets. arXiv preprint arXiv:1907.04018, 2019.\n\nData-independent structured pruning of neural networks via coresets. B Mussay, D Feldman, S Zhou, V Braverman, M Osadchy, IEEE Transactions on Neural Networks and Learning Systems. 3312Mussay, B., Feldman, D., Zhou, S., Braverman, V., and Osadchy, M. Data-independent structured pruning of neural networks via coresets. IEEE Transactions on Neu- ral Networks and Learning Systems, 33(12):7829-7841, 2021.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 32Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nGreedy-layer pruning: Speeding up transformer models for natural language processing. D Peer, S Stabinger, S Engl, A Rodr\u00edguez-S\u00e1nchez, Pattern Recognition Letters. 157Peer, D., Stabinger, S., Engl, S., and Rodr\u00edguez-S\u00e1nchez, A. Greedy-layer pruning: Speeding up transformer models for natural language processing. Pattern Recognition Letters, 157:76-82, 2022.\n\nWhen bert plays the lottery, all tickets are winning. S Prasanna, A Rogers, A Rumshisky, arXiv:2005.00561arXiv preprintPrasanna, S., Rogers, A., and Rumshisky, A. When bert plays the lottery, all tickets are winning. arXiv preprint arXiv:2005.00561, 2020.\n\nSquad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, arXiv:1606.05250arXiv preprintRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n\nP Rajpurkar, R Jia, P Liang, arXiv:1806.03822Know what you don't know: Unanswerable questions for squad. arXiv preprintRajpurkar, P., Jia, R., and Liang, P. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\n\nOn the effect of dropping layers of pre-trained transformer models. H Sajjad, F Dalvi, N Durrani, P Nakov, Computer Speech & Language. 77101429Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the ef- fect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023.\n\nV Sanh, L Debut, J Chaumond, T Wolf, Distilbert, arXiv:1910.01108a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprintSanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n\nMovement pruning: Adaptive sparsity by fine-tuning. V Sanh, T Wolf, A Rush, Advances in Neural Information Processing Systems. 33Sanh, V., Wolf, T., and Rush, A. Movement pruning: Adap- tive sparsity by fine-tuning. Advances in Neural Informa- tion Processing Systems, 33:20378-20389, 2020.\n\nFirst quora dataset release: question pairs. I Shankar, D Nikhil, C Kornel, Shankar, I., Nikhil, D., and Kornel, C. First quora dataset release: question pairs (2017). URL https://www. quora. com/q/quoradata/First-Quora-Dataset-Release- Question-Pairs, 2017.\n\nHessian based ultra low precision quantization of bert. S Shen, Z Dong, J Ye, L Ma, Z Yao, A Gholami, M W Mahoney, K Keutzer, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 34, pp. 8815-8821, 2020.\n\nNeuroinspired unsupervised learning and pruning with subquantum cbram arrays. Y Shi, L Nguyen, S Oh, X Liu, F Koushan, J R Jameson, D Kuzum, Nature communications. 91Shi, Y., Nguyen, L., Oh, S., Liu, X., Koushan, F., Jameson, J. R., and Kuzum, D. Neuroinspired unsupervised learn- ing and pruning with subquantum cbram arrays. Nature communications, 9(1):1-11, 2018.\n\nThe evolved transformer. D So, Q Le, C Liang, International Conference on Machine Learning. PMLRSo, D., Le, Q., and Liang, C. The evolved transformer. In International Conference on Machine Learning, pp. 5877-5886. PMLR, 2019.\n\nSearching for efficient transformers for language modeling. D So, W Ma\u0144ke, H Liu, Z Dai, N Shazeer, Q V Le, Advances in Neural Information Processing Systems. 34So, D., Ma\u0144ke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Searching for efficient transformers for language modeling. Advances in Neural Information Processing Systems, 34:6010-6022, 2021.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. R Socher, A Perelygin, J Wu, J Chuang, C D Manning, A Y Ng, C Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631-1642, 2013.\n\nS Sun, Y Cheng, Z Gan, J Liu, arXiv:1908.09355Patient knowledge distillation for bert model compression. arXiv preprintSun, S., Cheng, Y., Gan, Z., and Liu, J. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.\n\nZ Sun, H Yu, X Song, R Liu, Y Yang, D Zhou, arXiv:2004.02984Mobilebert: a compact task-agnostic bert for resourcelimited devices. arXiv preprintSun, Z., Yu, H., Song, X., Liu, R., Yang, Y., and Zhou, D. Mobilebert: a compact task-agnostic bert for resource- limited devices. arXiv preprint arXiv:2004.02984, 2020.\n\nEdgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference. T Tambe, C Hooper, L Pentecost, T Jia, E.-Y Yang, M Donato, V Sanh, P Whatmough, A M Rush, D Brooks, MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture. Tambe, T., Hooper, C., Pentecost, L., Jia, T., Yang, E.- Y., Donato, M., Sanh, V., Whatmough, P., Rush, A. M., Brooks, D., et al. Edgebert: Sentence-level energy op- timizations for latency-aware multi-task nlp inference. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 830-844, 2021.\n\nAttention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, 30Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At- tention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAnalyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. E Voita, D Talbot, F Moiseev, R Sennrich, I Titov, arXiv:1905.09418arXiv preprintVoita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418, 2019.\n\nA Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, Glue, arXiv:1804.07461A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n\nHat: Hardware-aware transformers for efficient natural language processing. H Wang, Z Wu, Z Liu, H Cai, L Zhu, C Gan, S Han, arXiv:2005.14187arXiv preprintWang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., and Han, S. Hat: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187, 2020a.\n\nEfficient sparse attention architecture with cascade token and head pruning. H Wang, Z Zhang, S Han, Spatten, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture with cascade token and head prun- ing. In 2021 IEEE International Symposium on High- Performance Computer Architecture (HPCA), pp. 97-110.\n\n. IEEE. IEEE, 2021.\n\nS Wang, B Z Li, M Khabsa, H Fang, H Ma, Linformer, arXiv:2006.04768Self-attention with linear complexity. arXiv preprintWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020b.\n\nStructured pruning of large language models. Z Wang, J Wohlwend, T Lei, arXiv:1910.04732arXiv preprintWang, Z., Wohlwend, J., and Lei, T. Structured pruning of large language models. arXiv preprint arXiv:1910.04732, 2019.\n\nNeural network acceptability judgments. A Warstadt, A Singh, S R Bowman, Transactions of the Association for Computational Linguistics. 7Warstadt, A., Singh, A., and Bowman, S. R. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625-641, 2019.\n\nA broadcoverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S R Bowman, arXiv:1704.05426arXiv preprintWilliams, A., Nangia, N., and Bowman, S. R. A broad- coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.\n\nTransformers: State-of-theart natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T Le Scao, S Gugger, M Drame, Q Lhoest, A Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing: System Demonstrations, pp. 38-45. Association for Computational Linguistics, 2020.\n\nLite transformer with long-short range attention. Z Wu, Z Liu, J Lin, Y Lin, S Han, arXiv:2004.11886arXiv preprintWu, Z., Liu, Z., Lin, J., Lin, Y., and Han, S. Lite trans- former with long-short range attention. arXiv preprint arXiv:2004.11886, 2020.\n\nStructured pruning learns compact and accurate models. M Xia, Z Zhong, Chen , D , arXiv:2204.00408arXiv preprintXia, M., Zhong, Z., and Chen, D. Structured pruning learns compact and accurate models. arXiv preprint arXiv:2204.00408, 2022.\n\nJ Xin, R Tang, J Lee, Y Yu, J Lin, Deebert, arXiv:2004.12993Dynamic early exiting for accelerating bert inference. arXiv preprintXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. Deebert: Dynamic early exiting for accelerating bert inference. arXiv preprint arXiv:2004.12993, 2020.\n\nNas-bert: task-agnostic and adaptive-size bert compression with neural architecture search. J Xu, X Tan, R Luo, K Song, J Li, T Qin, T.-Y Liu, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery & Data MiningXu, J., Tan, X., Luo, R., Song, K., Li, J., Qin, T., and Liu, T.-Y. Nas-bert: task-agnostic and adaptive-size bert com- pression with neural architecture search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1933-1943, 2021.\n\nZ Yao, L Ma, S Shen, K Keutzer, M W Mahoney, Mlpruning, arXiv:2105.14636A multilevel structured pruning framework for transformer-based models. arXiv preprintYao, Z., Ma, L., Shen, S., Keutzer, K., and Mahoney, M. W. Mlpruning: A multilevel structured pruning framework for transformer-based models. arXiv preprint arXiv:2105.14636, 2021.\n\nAutotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. Y Yin, C Chen, L Shang, X Jiang, X Chen, Q Liu, arXiv:2107.13686arXiv preprintYin, Y., Chen, C., Shang, L., Jiang, X., Chen, X., and Liu, Q. Autotinybert: Automatic hyper-parameter optimization for efficient pre-trained language models. arXiv preprint arXiv:2107.13686, 2021.\n\nSelfrepresentation based unsupervised exemplar selection in a union of subspaces. C You, C Li, D Robinson, R Vidal, IEEE Transactions on Pattern Analysis and Machine Intelligence. You, C., Li, C., Robinson, D., and Vidal, R. Self- representation based unsupervised exemplar selection in a union of subspaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\nQuantizing attention-based nlp models for low latency and energy efficient inference. A H Zadeh, I Edo, O M Awad, A Moshovos, Gobo, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). IEEEZadeh, A. H., Edo, I., Awad, O. M., and Moshovos, A. Gobo: Quantizing attention-based nlp models for low la- tency and energy efficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitec- ture (MICRO), pp. 811-824. IEEE, 2020.\n\nQuantized 8bit bert. O Zafrir, G Boudoukh, P Izsak, M Wasserblat, Q8bert, 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pp. 36-39.\n\n. IEEE. IEEE, 2019.\n\nPlaton: Pruning large transformer models with upper confidence bound of weight importance. Q Zhang, S Zuo, C Liang, A Bukharin, P He, W Chen, T Zhao, International Conference on Machine Learning. PMLRZhang, Q., Zuo, S., Liang, C., Bukharin, A., He, P., Chen, W., and Zhao, T. Platon: Pruning large transformer mod- els with upper confidence bound of weight importance. In International Conference on Machine Learning, pp. 26809-26823. PMLR, 2022.\n\nBert loses patience: Fast and robust inference with early exit. W Zhou, C Xu, T Ge, J Mcauley, K Xu, Wei , F , Advances in Neural Information Processing Systems. 33Zhou, W., Xu, C., Ge, T., McAuley, J., Xu, K., and Wei, F. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33:18330-18341, 2020.\n\n2019) using the HuggingFace Transformers (Wolf et al., 2020) library. We fine-tuned the pre-trained checkpoints of the BERT BASE. Paszke, 2018) and DistilBERT (Sanh et al., 2019) downloaded from the HuggingFace repository on GLUE. A. Experimental Details We implemented our framework with PyTorchA. Experimental Details We implemented our framework with PyTorch (Paszke et al., 2019) using the HuggingFace Transformers (Wolf et al., 2020) library. We fine-tuned the pre-trained checkpoints of the BERT BASE (Devlin et al., 2018) and DistilBERT (Sanh et al., 2019) downloaded from the HuggingFace repository on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018; 2016) benchmarks.\n\n2017)) with 364K, 4k and 6k training examples. 2) Sentiment classification (SST-2 (Socher et al., 2013)) with 67K training example, 3)Textual entailment. Glue (wang, with 3K training examples. 4) Natural language inference. 1) are question and answering tasks, each of which contains 88K and 130K training examples. SQuAD 2.0 is an extension of SQuAD 1.1 by including unanswerable questions whose answers are not stated in the given contextsGLUE (Wang et al., 2018) includes following tasks. 1)Sentence similarity (QQP (Shankar et al., 2017), MRPC (Dolan & Brockett, 2005), STS-B (Cer et al., 2017)) with 364K, 4k and 6k training examples. 2) Sentiment classification (SST- 2 (Socher et al., 2013)) with 67K training example, 3)Textual entailment (RTE (Dagan et al., 2006)) with 3K training examples. 4) Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al., 2016)) with 392K, 105K training examples. We exclude CoLA (Warstadt et al., 2019) and WLNI (Levesque et al., 2012) due to their unstable behaviors. SQuAD 1.1 (Rajpurkar et al., 2016) and SQuAD 2.0 (Rajpurkar et al., 2018) are question and answering tasks, each of which contains 88K and 130K training examples. SQuAD 2.0 is an extension of SQuAD 1.1 by including unanswerable questions whose answers are not stated in the given contexts.\n\nAll results are the averaged over the runs with 10 different seeds. For SQuAD tasks we report F1 score and for all GLUE tasks except STS-B we report accuracy. For STS-B we report Spearman Correlation. All results are the averaged over the runs with 10 different seeds. For SQuAD tasks we report F1 score and for all GLUE tasks except STS-B we report accuracy. For STS-B we report Spearman Correlation.\n\nNote that we only use the input features and not their labels. For our Representative Ranking calculation, in Algorithm 2, the width of Gaussian kernel \u03c3, and convergence rate \u03b1 are the hyperparameters. For Pruning we only use 2K raw data from the training sets. In our experiments, we set \u03c3 = 1.0 and \u03b1 = 0.01. On the average it only takes less than 20 iterations to convergeFor Pruning we only use 2K raw data from the training sets. Note that we only use the input features and not their labels. For our Representative Ranking calculation, in Algorithm 2, the width of Gaussian kernel \u03c3, and convergence rate \u03b1 are the hyperparameters. In our experiments, we set \u03c3 = 1.0 and \u03b1 = 0.01. On the average it only takes less than 20 iterations to converge.\n\nMore Experimental Results B.1. Comparison with Gradient-free, Retrain-free, Supervision-free Baselines. B , B. More Experimental Results B.1. Comparison with Gradient-free, Retrain-free, Supervision-free Baselines\n\nAll these methods are gradient-free (no backward pass), retrain-free (no retrain), supervision-free (no labeled data), and run in a matter of minutes. Clearly. Figure 5 shows the performance of KCM against Weight-magnitude, and layer-wise Output-magnitude using BERT BASE on GLUE and SQuAD tasks. KCM outperforms across all tasks consideredFigure 5 shows the performance of KCM against Weight-magnitude, and layer-wise Output-magnitude using BERT BASE on GLUE and SQuAD tasks. All these methods are gradient-free (no backward pass), retrain-free (no retrain), supervision-free (no labeled data), and run in a matter of minutes. Clearly, KCM outperforms across all tasks considered.\n\noutperforms Weight-magnitude, and outputs-magnitude across all GLUE and SQuAD tasks. outperforms Weight-magnitude, and outputs-magnitude across all GLUE and SQuAD tasks.\n", "annotations": {"author": "[{\"end\":67,\"start\":56},{\"end\":79,\"start\":68},{\"end\":96,\"start\":80}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":62},{\"end\":78,\"start\":75},{\"end\":95,\"start\":85}]", "author_first_name": "[{\"end\":61,\"start\":56},{\"end\":74,\"start\":68},{\"end\":84,\"start\":80}]", "author_affiliation": null, "title": "[{\"end\":53,\"start\":1},{\"end\":149,\"start\":97}]", "venue": null, "abstract": "[{\"end\":1184,\"start\":151}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b81\"},\"end\":1859,\"start\":1841},{\"end\":1968,\"start\":1921},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2020,\"start\":2003},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2040,\"start\":2020},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2058,\"start\":2040},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":2075,\"start\":2058},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2128,\"start\":2107},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2149,\"start\":2128},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2166,\"start\":2149},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":2185,\"start\":2166},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2231,\"start\":2214},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":2247,\"start\":2231},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":2264,\"start\":2247},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2307,\"start\":2289},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2325,\"start\":2307},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2343,\"start\":2325},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2376,\"start\":2358},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":2395,\"start\":2376},{\"end\":2415,\"start\":2395},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2466,\"start\":2449},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2483,\"start\":2466},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3026,\"start\":3008},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3750,\"start\":3732},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3768,\"start\":3750},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3785,\"start\":3768},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4550,\"start\":4530},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5185,\"start\":5167},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":6226,\"start\":6204},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6445,\"start\":6424},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6932,\"start\":6906},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7708,\"start\":7689},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8120,\"start\":8102},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":8946,\"start\":8928},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10072,\"start\":10054},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11082,\"start\":11061},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11107,\"start\":11082},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":11124,\"start\":11107},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":12232,\"start\":12211},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12260,\"start\":12242},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12285,\"start\":12267},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12318,\"start\":12299},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12353,\"start\":12336},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12674,\"start\":12654},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13041,\"start\":13022},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13103,\"start\":13084},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13431,\"start\":13412},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15893,\"start\":15875},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16483,\"start\":16466},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16532,\"start\":16515},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":17334,\"start\":17313},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17487,\"start\":17468},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18535,\"start\":18514},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18564,\"start\":18546},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18595,\"start\":18576},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18838,\"start\":18820},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18921,\"start\":18903},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19146,\"start\":19129},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19349,\"start\":19333},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19397,\"start\":19381},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19445,\"start\":19429},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20304,\"start\":20286},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20813,\"start\":20794},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20849,\"start\":20832},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21531,\"start\":21512},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24128,\"start\":24109},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25341,\"start\":25320},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25362,\"start\":25341},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":25379,\"start\":25362},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":25398,\"start\":25379},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":25414,\"start\":25398},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25435,\"start\":25414},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25452,\"start\":25435},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25469,\"start\":25452},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":25515,\"start\":25498},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25534,\"start\":25515},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":25550,\"start\":25534},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":25569,\"start\":25550},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":25585,\"start\":25569},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":25602,\"start\":25585},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":25645,\"start\":25627},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25663,\"start\":25645},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":25681,\"start\":25663},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":25698,\"start\":25681},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25719,\"start\":25698},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":25752,\"start\":25734},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":25771,\"start\":25752},{\"end\":25791,\"start\":25771},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25842,\"start\":25824},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":25861,\"start\":25842},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25879,\"start\":25861},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25895,\"start\":25879},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":25912,\"start\":25895},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26056,\"start\":26036},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":26076,\"start\":26058},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26096,\"start\":26078},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":26117,\"start\":26098},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26227,\"start\":26208},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":26251,\"start\":26229},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26630,\"start\":26613},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":26648,\"start\":26630},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26696,\"start\":26675},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26720,\"start\":26696},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26740,\"start\":26720},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26806,\"start\":26787},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26921,\"start\":26900},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":26940,\"start\":26921},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26978,\"start\":26960},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26998,\"start\":26978},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27016,\"start\":26998},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27231,\"start\":27209},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27252,\"start\":27231},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27269,\"start\":27252},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27287,\"start\":27269},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":27304,\"start\":27287},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":27321,\"start\":27304},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27417,\"start\":27399},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27434,\"start\":27417},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27452,\"start\":27434},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":27469,\"start\":27452},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":27487,\"start\":27469},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27820,\"start\":27798},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":27837,\"start\":27820},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27980,\"start\":27962},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27997,\"start\":27980},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28015,\"start\":27997},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":28032,\"start\":28015},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28108,\"start\":28090},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28130,\"start\":28110},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28410,\"start\":28391},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28578,\"start\":28557},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28749,\"start\":28728},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29029,\"start\":29008},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29054,\"start\":29029},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":29070,\"start\":29054},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30399,\"start\":30379},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33821,\"start\":33802},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34085,\"start\":34066}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31446,\"start\":31239},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32386,\"start\":31447},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32589,\"start\":32387},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32956,\"start\":32590},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33186,\"start\":32957},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33634,\"start\":33187},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33719,\"start\":33635},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35796,\"start\":33720},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37110,\"start\":35797},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37356,\"start\":37111},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38025,\"start\":37357},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38273,\"start\":38026},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38488,\"start\":38274}]", "paragraph": "[{\"end\":1680,\"start\":1200},{\"end\":2484,\"start\":1682},{\"end\":3232,\"start\":2486},{\"end\":4090,\"start\":3234},{\"end\":5226,\"start\":4092},{\"end\":5266,\"start\":5228},{\"end\":6048,\"start\":5268},{\"end\":6578,\"start\":6085},{\"end\":6679,\"start\":6641},{\"end\":6858,\"start\":6745},{\"end\":6958,\"start\":6864},{\"end\":7196,\"start\":6992},{\"end\":7301,\"start\":7269},{\"end\":7641,\"start\":7303},{\"end\":7802,\"start\":7643},{\"end\":8153,\"start\":7837},{\"end\":8429,\"start\":8397},{\"end\":8560,\"start\":8431},{\"end\":8947,\"start\":8655},{\"end\":9459,\"start\":8969},{\"end\":10221,\"start\":9482},{\"end\":10421,\"start\":10256},{\"end\":10467,\"start\":10453},{\"end\":10677,\"start\":10575},{\"end\":11313,\"start\":10679},{\"end\":11509,\"start\":11335},{\"end\":12078,\"start\":11511},{\"end\":12537,\"start\":12089},{\"end\":12876,\"start\":12539},{\"end\":13210,\"start\":12934},{\"end\":13448,\"start\":13357},{\"end\":13665,\"start\":13459},{\"end\":14208,\"start\":13722},{\"end\":14405,\"start\":14237},{\"end\":14604,\"start\":14407},{\"end\":14938,\"start\":14606},{\"end\":15355,\"start\":14958},{\"end\":15726,\"start\":15357},{\"end\":16932,\"start\":15728},{\"end\":17519,\"start\":17271},{\"end\":18114,\"start\":17521},{\"end\":18889,\"start\":18116},{\"end\":20705,\"start\":18891},{\"end\":21447,\"start\":20730},{\"end\":21980,\"start\":21449},{\"end\":23043,\"start\":22001},{\"end\":23553,\"start\":23045},{\"end\":23798,\"start\":23568},{\"end\":24804,\"start\":23800},{\"end\":25154,\"start\":24806},{\"end\":25913,\"start\":25171},{\"end\":26490,\"start\":25915},{\"end\":27488,\"start\":26492},{\"end\":28882,\"start\":27490},{\"end\":29110,\"start\":28884},{\"end\":29113,\"start\":29112},{\"end\":29215,\"start\":29131},{\"end\":29797,\"start\":29230},{\"end\":30172,\"start\":29814},{\"end\":30505,\"start\":30207},{\"end\":31238,\"start\":30507}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6640,\"start\":6579},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6744,\"start\":6680},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7268,\"start\":7197},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7836,\"start\":7803},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8206,\"start\":8154},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8396,\"start\":8206},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8654,\"start\":8561},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10574,\"start\":10468},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11334,\"start\":11314},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12933,\"start\":12877},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13356,\"start\":13211},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13458,\"start\":13449},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13721,\"start\":13666},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17236,\"start\":16933},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29130,\"start\":29114}]", "table_ref": "[{\"end\":11812,\"start\":11805},{\"end\":16206,\"start\":16199},{\"end\":16463,\"start\":16456},{\"end\":16646,\"start\":16637},{\"end\":16903,\"start\":16896},{\"end\":18166,\"start\":18159},{\"end\":20026,\"start\":20019},{\"end\":20112,\"start\":20105},{\"end\":20930,\"start\":20922},{\"end\":21313,\"start\":21306},{\"end\":21604,\"start\":21597},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24973,\"start\":24966},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30066,\"start\":30059},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30801,\"start\":30794}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1198,\"start\":1186},{\"attributes\":{\"n\":\"2.\"},\"end\":6069,\"start\":6051},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6083,\"start\":6072},{\"end\":6862,\"start\":6861},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6990,\"start\":6961},{\"attributes\":{\"n\":\"3.\"},\"end\":8967,\"start\":8950},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9480,\"start\":9462},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10254,\"start\":10224},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":10451,\"start\":10424},{\"end\":12087,\"start\":12081},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":14235,\"start\":14211},{\"attributes\":{\"n\":\"3.2.3.\"},\"end\":14956,\"start\":14941},{\"attributes\":{\"n\":\"4.\"},\"end\":17248,\"start\":17238},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17269,\"start\":17251},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20728,\"start\":20708},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21999,\"start\":21983},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23566,\"start\":23556},{\"attributes\":{\"n\":\"5.\"},\"end\":25169,\"start\":25157},{\"attributes\":{\"n\":\"6.\"},\"end\":29228,\"start\":29218},{\"end\":29812,\"start\":29800},{\"end\":30205,\"start\":30175},{\"end\":31250,\"start\":31240},{\"end\":32398,\"start\":32388},{\"end\":32601,\"start\":32591},{\"end\":32968,\"start\":32958},{\"end\":33646,\"start\":33636},{\"end\":33739,\"start\":33721},{\"end\":35807,\"start\":35798},{\"end\":37121,\"start\":37112},{\"end\":37367,\"start\":37358},{\"end\":38036,\"start\":38027},{\"end\":38285,\"start\":38275}]", "table": "[{\"end\":35796,\"start\":34607},{\"end\":37110,\"start\":36636},{\"end\":37356,\"start\":37293},{\"end\":38025,\"start\":37767},{\"end\":38273,\"start\":38233},{\"end\":38488,\"start\":38448}]", "figure_caption": "[{\"end\":31446,\"start\":31252},{\"end\":32386,\"start\":31449},{\"end\":32589,\"start\":32400},{\"end\":32956,\"start\":32603},{\"end\":33186,\"start\":32970},{\"end\":33634,\"start\":33189},{\"end\":33719,\"start\":33648},{\"end\":34607,\"start\":33742},{\"end\":36636,\"start\":35809},{\"end\":37293,\"start\":37123},{\"end\":37767,\"start\":37369},{\"end\":38233,\"start\":38038},{\"end\":38448,\"start\":38288}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4185,\"start\":4176},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9206,\"start\":9198},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9645,\"start\":9637},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15914,\"start\":15906},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":17654,\"start\":17646},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20941,\"start\":20933},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22405,\"start\":22397},{\"end\":23041,\"start\":23033},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23277,\"start\":23269}]", "bib_author_first_name": "[{\"end\":38627,\"start\":38626},{\"end\":38637,\"start\":38636},{\"end\":38645,\"start\":38644},{\"end\":38932,\"start\":38931},{\"end\":38942,\"start\":38941},{\"end\":38953,\"start\":38952},{\"end\":39300,\"start\":39299},{\"end\":39310,\"start\":39309},{\"end\":39324,\"start\":39323},{\"end\":39341,\"start\":39340},{\"end\":39352,\"start\":39351},{\"end\":39695,\"start\":39694},{\"end\":39705,\"start\":39704},{\"end\":39716,\"start\":39715},{\"end\":40021,\"start\":40020},{\"end\":40031,\"start\":40030},{\"end\":40042,\"start\":40041},{\"end\":40472,\"start\":40471},{\"end\":40479,\"start\":40478},{\"end\":40487,\"start\":40486},{\"end\":40497,\"start\":40496},{\"end\":40513,\"start\":40512},{\"end\":40960,\"start\":40959},{\"end\":40968,\"start\":40967},{\"end\":40974,\"start\":40973},{\"end\":40981,\"start\":40980},{\"end\":40989,\"start\":40988},{\"end\":40995,\"start\":40994},{\"end\":41003,\"start\":41002},{\"end\":41011,\"start\":41010},{\"end\":41020,\"start\":41019},{\"end\":41027,\"start\":41026},{\"end\":41440,\"start\":41439},{\"end\":41448,\"start\":41447},{\"end\":41459,\"start\":41458},{\"end\":41468,\"start\":41467},{\"end\":41475,\"start\":41474},{\"end\":41484,\"start\":41483},{\"end\":41492,\"start\":41491},{\"end\":41839,\"start\":41838},{\"end\":41847,\"start\":41846},{\"end\":41856,\"start\":41855},{\"end\":41863,\"start\":41862},{\"end\":41871,\"start\":41870},{\"end\":41883,\"start\":41879},{\"end\":41887,\"start\":41886},{\"end\":42150,\"start\":42149},{\"end\":42158,\"start\":42157},{\"end\":42167,\"start\":42166},{\"end\":42175,\"start\":42174},{\"end\":42182,\"start\":42181},{\"end\":42190,\"start\":42189},{\"end\":42517,\"start\":42516},{\"end\":42526,\"start\":42525},{\"end\":42538,\"start\":42537},{\"end\":43162,\"start\":43161},{\"end\":43175,\"start\":43171},{\"end\":43184,\"start\":43183},{\"end\":43191,\"start\":43190},{\"end\":43543,\"start\":43542},{\"end\":43545,\"start\":43544},{\"end\":43553,\"start\":43552},{\"end\":43564,\"start\":43558},{\"end\":43568,\"start\":43567},{\"end\":43570,\"start\":43569},{\"end\":43878,\"start\":43877},{\"end\":43887,\"start\":43886},{\"end\":44111,\"start\":44110},{\"end\":44121,\"start\":44120},{\"end\":44134,\"start\":44133},{\"end\":44145,\"start\":44144},{\"end\":44147,\"start\":44146},{\"end\":44451,\"start\":44450},{\"end\":44461,\"start\":44460},{\"end\":44463,\"start\":44462},{\"end\":44474,\"start\":44473},{\"end\":44486,\"start\":44485},{\"end\":44499,\"start\":44498},{\"end\":44501,\"start\":44500},{\"end\":44791,\"start\":44790},{\"end\":44798,\"start\":44797},{\"end\":44812,\"start\":44806},{\"end\":44816,\"start\":44815},{\"end\":45141,\"start\":45140},{\"end\":45152,\"start\":45151},{\"end\":45355,\"start\":45354},{\"end\":45363,\"start\":45362},{\"end\":45372,\"start\":45371},{\"end\":45654,\"start\":45653},{\"end\":45664,\"start\":45663},{\"end\":45672,\"start\":45671},{\"end\":45679,\"start\":45678},{\"end\":45681,\"start\":45680},{\"end\":45689,\"start\":45688},{\"end\":45697,\"start\":45696},{\"end\":45707,\"start\":45706},{\"end\":45716,\"start\":45715},{\"end\":45724,\"start\":45723},{\"end\":46144,\"start\":46143},{\"end\":46150,\"start\":46149},{\"end\":46160,\"start\":46159},{\"end\":46172,\"start\":46171},{\"end\":46186,\"start\":46185},{\"end\":46198,\"start\":46197},{\"end\":46200,\"start\":46199},{\"end\":46521,\"start\":46520},{\"end\":46528,\"start\":46527},{\"end\":46530,\"start\":46529},{\"end\":46539,\"start\":46538},{\"end\":46547,\"start\":46546},{\"end\":46554,\"start\":46553},{\"end\":46562,\"start\":46561},{\"end\":46570,\"start\":46569},{\"end\":46577,\"start\":46576},{\"end\":46583,\"start\":46582},{\"end\":46590,\"start\":46589},{\"end\":47084,\"start\":47083},{\"end\":47091,\"start\":47090},{\"end\":47093,\"start\":47092},{\"end\":47102,\"start\":47101},{\"end\":47104,\"start\":47103},{\"end\":47114,\"start\":47113},{\"end\":47116,\"start\":47115},{\"end\":47127,\"start\":47126},{\"end\":47129,\"start\":47128},{\"end\":47472,\"start\":47471},{\"end\":47474,\"start\":47473},{\"end\":47481,\"start\":47480},{\"end\":47488,\"start\":47487},{\"end\":47490,\"start\":47489},{\"end\":47497,\"start\":47496},{\"end\":47504,\"start\":47503},{\"end\":47512,\"start\":47511},{\"end\":47514,\"start\":47513},{\"end\":47522,\"start\":47521},{\"end\":47524,\"start\":47523},{\"end\":47960,\"start\":47959},{\"end\":47973,\"start\":47972},{\"end\":48119,\"start\":48118},{\"end\":48126,\"start\":48125},{\"end\":48135,\"start\":48134},{\"end\":48144,\"start\":48143},{\"end\":48153,\"start\":48152},{\"end\":48161,\"start\":48160},{\"end\":48550,\"start\":48549},{\"end\":48559,\"start\":48558},{\"end\":48565,\"start\":48564},{\"end\":48575,\"start\":48571},{\"end\":48579,\"start\":48578},{\"end\":48958,\"start\":48957},{\"end\":48968,\"start\":48967},{\"end\":48979,\"start\":48978},{\"end\":48989,\"start\":48988},{\"end\":48999,\"start\":48998},{\"end\":49338,\"start\":49337},{\"end\":49348,\"start\":49347},{\"end\":49358,\"start\":49357},{\"end\":49368,\"start\":49367},{\"end\":49378,\"start\":49377},{\"end\":49386,\"start\":49385},{\"end\":49698,\"start\":49697},{\"end\":49700,\"start\":49699},{\"end\":49711,\"start\":49710},{\"end\":49713,\"start\":49712},{\"end\":49721,\"start\":49720},{\"end\":49732,\"start\":49731},{\"end\":49734,\"start\":49733},{\"end\":50038,\"start\":50037},{\"end\":50046,\"start\":50045},{\"end\":50053,\"start\":50052},{\"end\":50062,\"start\":50061},{\"end\":50071,\"start\":50070},{\"end\":50079,\"start\":50078},{\"end\":50085,\"start\":50084},{\"end\":50093,\"start\":50092},{\"end\":50375,\"start\":50374},{\"end\":50377,\"start\":50376},{\"end\":50387,\"start\":50386},{\"end\":50400,\"start\":50399},{\"end\":50414,\"start\":50413},{\"end\":50424,\"start\":50423},{\"end\":50440,\"start\":50439},{\"end\":50458,\"start\":50450},{\"end\":50790,\"start\":50789},{\"end\":50800,\"start\":50799},{\"end\":51063,\"start\":51062},{\"end\":51078,\"start\":51077},{\"end\":51086,\"start\":51085},{\"end\":51094,\"start\":51093},{\"end\":51390,\"start\":51389},{\"end\":51397,\"start\":51396},{\"end\":51408,\"start\":51407},{\"end\":51415,\"start\":51414},{\"end\":51417,\"start\":51416},{\"end\":51428,\"start\":51427},{\"end\":51707,\"start\":51706},{\"end\":51717,\"start\":51716},{\"end\":51727,\"start\":51726},{\"end\":51983,\"start\":51982},{\"end\":51993,\"start\":51992},{\"end\":52003,\"start\":52002},{\"end\":52013,\"start\":52012},{\"end\":52024,\"start\":52023},{\"end\":52033,\"start\":52032},{\"end\":52044,\"start\":52043},{\"end\":52059,\"start\":52051},{\"end\":52063,\"start\":52062},{\"end\":52390,\"start\":52389},{\"end\":52398,\"start\":52397},{\"end\":52405,\"start\":52404},{\"end\":52407,\"start\":52406},{\"end\":52418,\"start\":52417},{\"end\":52429,\"start\":52428},{\"end\":52440,\"start\":52439},{\"end\":52696,\"start\":52695},{\"end\":52707,\"start\":52706},{\"end\":52719,\"start\":52718},{\"end\":52727,\"start\":52726},{\"end\":52729,\"start\":52728},{\"end\":52977,\"start\":52976},{\"end\":52984,\"start\":52983},{\"end\":52992,\"start\":52991},{\"end\":53003,\"start\":53002},{\"end\":53013,\"start\":53012},{\"end\":53023,\"start\":53022},{\"end\":53254,\"start\":53253},{\"end\":53261,\"start\":53260},{\"end\":53268,\"start\":53267},{\"end\":53270,\"start\":53269},{\"end\":53561,\"start\":53560},{\"end\":53573,\"start\":53572},{\"end\":53582,\"start\":53581},{\"end\":53879,\"start\":53878},{\"end\":53885,\"start\":53884},{\"end\":53893,\"start\":53892},{\"end\":53902,\"start\":53901},{\"end\":53908,\"start\":53907},{\"end\":53914,\"start\":53913},{\"end\":53921,\"start\":53920},{\"end\":54336,\"start\":54335},{\"end\":54342,\"start\":54341},{\"end\":54351,\"start\":54350},{\"end\":54365,\"start\":54364},{\"end\":54374,\"start\":54373},{\"end\":54376,\"start\":54375},{\"end\":54611,\"start\":54610},{\"end\":54625,\"start\":54624},{\"end\":54635,\"start\":54634},{\"end\":54643,\"start\":54642},{\"end\":54654,\"start\":54653},{\"end\":54938,\"start\":54937},{\"end\":54945,\"start\":54944},{\"end\":54947,\"start\":54946},{\"end\":54954,\"start\":54953},{\"end\":54962,\"start\":54961},{\"end\":54972,\"start\":54968},{\"end\":54976,\"start\":54975},{\"end\":55245,\"start\":55244},{\"end\":55252,\"start\":55251},{\"end\":55259,\"start\":55258},{\"end\":55638,\"start\":55637},{\"end\":55645,\"start\":55644},{\"end\":55651,\"start\":55650},{\"end\":55657,\"start\":55656},{\"end\":55999,\"start\":55998},{\"end\":56009,\"start\":56008},{\"end\":56017,\"start\":56016},{\"end\":56292,\"start\":56291},{\"end\":56302,\"start\":56301},{\"end\":56312,\"start\":56311},{\"end\":56321,\"start\":56320},{\"end\":56627,\"start\":56626},{\"end\":56637,\"start\":56636},{\"end\":56648,\"start\":56647},{\"end\":56661,\"start\":56660},{\"end\":56669,\"start\":56668},{\"end\":56933,\"start\":56932},{\"end\":56943,\"start\":56942},{\"end\":56954,\"start\":56953},{\"end\":56962,\"start\":56961},{\"end\":56975,\"start\":56974},{\"end\":57340,\"start\":57339},{\"end\":57350,\"start\":57349},{\"end\":57359,\"start\":57358},{\"end\":57368,\"start\":57367},{\"end\":57377,\"start\":57376},{\"end\":57389,\"start\":57388},{\"end\":57399,\"start\":57398},{\"end\":57410,\"start\":57409},{\"end\":57417,\"start\":57416},{\"end\":57431,\"start\":57430},{\"end\":57840,\"start\":57839},{\"end\":57848,\"start\":57847},{\"end\":57861,\"start\":57860},{\"end\":57869,\"start\":57868},{\"end\":58170,\"start\":58169},{\"end\":58182,\"start\":58181},{\"end\":58192,\"start\":58191},{\"end\":58434,\"start\":58433},{\"end\":58447,\"start\":58446},{\"end\":58456,\"start\":58455},{\"end\":58467,\"start\":58466},{\"end\":58660,\"start\":58659},{\"end\":58673,\"start\":58672},{\"end\":58680,\"start\":58679},{\"end\":58985,\"start\":58984},{\"end\":58995,\"start\":58994},{\"end\":59004,\"start\":59003},{\"end\":59015,\"start\":59014},{\"end\":59226,\"start\":59225},{\"end\":59234,\"start\":59233},{\"end\":59243,\"start\":59242},{\"end\":59255,\"start\":59254},{\"end\":59591,\"start\":59590},{\"end\":59599,\"start\":59598},{\"end\":59607,\"start\":59606},{\"end\":59876,\"start\":59875},{\"end\":59887,\"start\":59886},{\"end\":59897,\"start\":59896},{\"end\":60147,\"start\":60146},{\"end\":60155,\"start\":60154},{\"end\":60163,\"start\":60162},{\"end\":60169,\"start\":60168},{\"end\":60175,\"start\":60174},{\"end\":60182,\"start\":60181},{\"end\":60193,\"start\":60192},{\"end\":60195,\"start\":60194},{\"end\":60206,\"start\":60205},{\"end\":60661,\"start\":60660},{\"end\":60668,\"start\":60667},{\"end\":60678,\"start\":60677},{\"end\":60684,\"start\":60683},{\"end\":60691,\"start\":60690},{\"end\":60702,\"start\":60701},{\"end\":60704,\"start\":60703},{\"end\":60715,\"start\":60714},{\"end\":60976,\"start\":60975},{\"end\":60982,\"start\":60981},{\"end\":60988,\"start\":60987},{\"end\":61239,\"start\":61238},{\"end\":61245,\"start\":61244},{\"end\":61254,\"start\":61253},{\"end\":61261,\"start\":61260},{\"end\":61268,\"start\":61267},{\"end\":61279,\"start\":61278},{\"end\":61281,\"start\":61280},{\"end\":61615,\"start\":61614},{\"end\":61625,\"start\":61624},{\"end\":61638,\"start\":61637},{\"end\":61644,\"start\":61643},{\"end\":61654,\"start\":61653},{\"end\":61656,\"start\":61655},{\"end\":61667,\"start\":61666},{\"end\":61669,\"start\":61668},{\"end\":61675,\"start\":61674},{\"end\":62123,\"start\":62122},{\"end\":62130,\"start\":62129},{\"end\":62139,\"start\":62138},{\"end\":62146,\"start\":62145},{\"end\":62382,\"start\":62381},{\"end\":62389,\"start\":62388},{\"end\":62395,\"start\":62394},{\"end\":62403,\"start\":62402},{\"end\":62410,\"start\":62409},{\"end\":62418,\"start\":62417},{\"end\":62787,\"start\":62786},{\"end\":62796,\"start\":62795},{\"end\":62806,\"start\":62805},{\"end\":62819,\"start\":62818},{\"end\":62829,\"start\":62825},{\"end\":62837,\"start\":62836},{\"end\":62847,\"start\":62846},{\"end\":62855,\"start\":62854},{\"end\":62868,\"start\":62867},{\"end\":62870,\"start\":62869},{\"end\":62878,\"start\":62877},{\"end\":63365,\"start\":63364},{\"end\":63376,\"start\":63375},{\"end\":63387,\"start\":63386},{\"end\":63397,\"start\":63396},{\"end\":63410,\"start\":63409},{\"end\":63419,\"start\":63418},{\"end\":63421,\"start\":63420},{\"end\":63430,\"start\":63429},{\"end\":63440,\"start\":63439},{\"end\":63757,\"start\":63756},{\"end\":63766,\"start\":63765},{\"end\":63776,\"start\":63775},{\"end\":63787,\"start\":63786},{\"end\":63799,\"start\":63798},{\"end\":64043,\"start\":64042},{\"end\":64051,\"start\":64050},{\"end\":64060,\"start\":64059},{\"end\":64071,\"start\":64070},{\"end\":64079,\"start\":64078},{\"end\":64087,\"start\":64086},{\"end\":64089,\"start\":64088},{\"end\":64493,\"start\":64492},{\"end\":64501,\"start\":64500},{\"end\":64507,\"start\":64506},{\"end\":64514,\"start\":64513},{\"end\":64521,\"start\":64520},{\"end\":64528,\"start\":64527},{\"end\":64535,\"start\":64534},{\"end\":64832,\"start\":64831},{\"end\":64840,\"start\":64839},{\"end\":64849,\"start\":64848},{\"end\":65192,\"start\":65191},{\"end\":65200,\"start\":65199},{\"end\":65202,\"start\":65201},{\"end\":65208,\"start\":65207},{\"end\":65218,\"start\":65217},{\"end\":65226,\"start\":65225},{\"end\":65502,\"start\":65501},{\"end\":65510,\"start\":65509},{\"end\":65522,\"start\":65521},{\"end\":65720,\"start\":65719},{\"end\":65732,\"start\":65731},{\"end\":65741,\"start\":65740},{\"end\":65743,\"start\":65742},{\"end\":66060,\"start\":66059},{\"end\":66072,\"start\":66071},{\"end\":66082,\"start\":66081},{\"end\":66084,\"start\":66083},{\"end\":66348,\"start\":66347},{\"end\":66356,\"start\":66355},{\"end\":66365,\"start\":66364},{\"end\":66373,\"start\":66372},{\"end\":66385,\"start\":66384},{\"end\":66397,\"start\":66396},{\"end\":66404,\"start\":66403},{\"end\":66414,\"start\":66413},{\"end\":66423,\"start\":66422},{\"end\":66431,\"start\":66430},{\"end\":66444,\"start\":66443},{\"end\":66455,\"start\":66454},{\"end\":66467,\"start\":66466},{\"end\":66481,\"start\":66480},{\"end\":66487,\"start\":66486},{\"end\":66498,\"start\":66497},{\"end\":66505,\"start\":66504},{\"end\":66511,\"start\":66510},{\"end\":66522,\"start\":66521},{\"end\":66532,\"start\":66531},{\"end\":66541,\"start\":66540},{\"end\":66551,\"start\":66550},{\"end\":67351,\"start\":67350},{\"end\":67357,\"start\":67356},{\"end\":67364,\"start\":67363},{\"end\":67371,\"start\":67370},{\"end\":67378,\"start\":67377},{\"end\":67609,\"start\":67608},{\"end\":67616,\"start\":67615},{\"end\":67628,\"start\":67624},{\"end\":67632,\"start\":67631},{\"end\":67794,\"start\":67793},{\"end\":67801,\"start\":67800},{\"end\":67809,\"start\":67808},{\"end\":67816,\"start\":67815},{\"end\":67822,\"start\":67821},{\"end\":68167,\"start\":68166},{\"end\":68173,\"start\":68172},{\"end\":68180,\"start\":68179},{\"end\":68187,\"start\":68186},{\"end\":68195,\"start\":68194},{\"end\":68201,\"start\":68200},{\"end\":68211,\"start\":68207},{\"end\":68640,\"start\":68639},{\"end\":68647,\"start\":68646},{\"end\":68653,\"start\":68652},{\"end\":68661,\"start\":68660},{\"end\":68672,\"start\":68671},{\"end\":68674,\"start\":68673},{\"end\":69076,\"start\":69075},{\"end\":69083,\"start\":69082},{\"end\":69091,\"start\":69090},{\"end\":69100,\"start\":69099},{\"end\":69109,\"start\":69108},{\"end\":69117,\"start\":69116},{\"end\":69435,\"start\":69434},{\"end\":69442,\"start\":69441},{\"end\":69448,\"start\":69447},{\"end\":69460,\"start\":69459},{\"end\":69819,\"start\":69818},{\"end\":69821,\"start\":69820},{\"end\":69830,\"start\":69829},{\"end\":69837,\"start\":69836},{\"end\":69839,\"start\":69838},{\"end\":69847,\"start\":69846},{\"end\":70228,\"start\":70227},{\"end\":70238,\"start\":70237},{\"end\":70250,\"start\":70249},{\"end\":70259,\"start\":70258},{\"end\":70713,\"start\":70712},{\"end\":70722,\"start\":70721},{\"end\":70729,\"start\":70728},{\"end\":70738,\"start\":70737},{\"end\":70750,\"start\":70749},{\"end\":70756,\"start\":70755},{\"end\":70764,\"start\":70763},{\"end\":71134,\"start\":71133},{\"end\":71142,\"start\":71141},{\"end\":71148,\"start\":71147},{\"end\":71154,\"start\":71153},{\"end\":71165,\"start\":71164},{\"end\":71173,\"start\":71170},{\"end\":71177,\"start\":71176},{\"end\":74707,\"start\":74706}]", "bib_author_last_name": "[{\"end\":38634,\"start\":38628},{\"end\":38642,\"start\":38638},{\"end\":38653,\"start\":38646},{\"end\":38939,\"start\":38933},{\"end\":38950,\"start\":38943},{\"end\":38960,\"start\":38954},{\"end\":39307,\"start\":39301},{\"end\":39321,\"start\":39311},{\"end\":39338,\"start\":39325},{\"end\":39349,\"start\":39342},{\"end\":39356,\"start\":39353},{\"end\":39702,\"start\":39696},{\"end\":39713,\"start\":39706},{\"end\":39726,\"start\":39717},{\"end\":40028,\"start\":40022},{\"end\":40039,\"start\":40032},{\"end\":40052,\"start\":40043},{\"end\":40476,\"start\":40473},{\"end\":40484,\"start\":40480},{\"end\":40494,\"start\":40488},{\"end\":40510,\"start\":40498},{\"end\":40520,\"start\":40514},{\"end\":40965,\"start\":40961},{\"end\":40971,\"start\":40969},{\"end\":40978,\"start\":40975},{\"end\":40986,\"start\":40982},{\"end\":40992,\"start\":40990},{\"end\":41000,\"start\":40996},{\"end\":41008,\"start\":41004},{\"end\":41017,\"start\":41012},{\"end\":41024,\"start\":41021},{\"end\":41032,\"start\":41028},{\"end\":41041,\"start\":41034},{\"end\":41445,\"start\":41441},{\"end\":41456,\"start\":41449},{\"end\":41465,\"start\":41460},{\"end\":41472,\"start\":41469},{\"end\":41481,\"start\":41476},{\"end\":41489,\"start\":41485},{\"end\":41499,\"start\":41493},{\"end\":41844,\"start\":41840},{\"end\":41853,\"start\":41848},{\"end\":41860,\"start\":41857},{\"end\":41868,\"start\":41864},{\"end\":41877,\"start\":41872},{\"end\":42155,\"start\":42151},{\"end\":42164,\"start\":42159},{\"end\":42172,\"start\":42168},{\"end\":42179,\"start\":42176},{\"end\":42187,\"start\":42183},{\"end\":42194,\"start\":42191},{\"end\":42205,\"start\":42196},{\"end\":42523,\"start\":42518},{\"end\":42535,\"start\":42527},{\"end\":42546,\"start\":42539},{\"end\":43169,\"start\":43163},{\"end\":43181,\"start\":43176},{\"end\":43188,\"start\":43185},{\"end\":43201,\"start\":43192},{\"end\":43207,\"start\":43203},{\"end\":43550,\"start\":43546},{\"end\":43556,\"start\":43554},{\"end\":43884,\"start\":43879},{\"end\":43896,\"start\":43888},{\"end\":44118,\"start\":44112},{\"end\":44131,\"start\":44122},{\"end\":44142,\"start\":44135},{\"end\":44153,\"start\":44148},{\"end\":44160,\"start\":44155},{\"end\":44458,\"start\":44452},{\"end\":44471,\"start\":44464},{\"end\":44483,\"start\":44475},{\"end\":44496,\"start\":44487},{\"end\":44507,\"start\":44502},{\"end\":44795,\"start\":44792},{\"end\":44804,\"start\":44799},{\"end\":45149,\"start\":45142},{\"end\":45161,\"start\":45153},{\"end\":45360,\"start\":45356},{\"end\":45369,\"start\":45364},{\"end\":45379,\"start\":45373},{\"end\":45661,\"start\":45655},{\"end\":45669,\"start\":45665},{\"end\":45676,\"start\":45673},{\"end\":45686,\"start\":45682},{\"end\":45694,\"start\":45690},{\"end\":45704,\"start\":45698},{\"end\":45713,\"start\":45708},{\"end\":45721,\"start\":45717},{\"end\":45733,\"start\":45725},{\"end\":46147,\"start\":46145},{\"end\":46157,\"start\":46151},{\"end\":46169,\"start\":46161},{\"end\":46183,\"start\":46173},{\"end\":46195,\"start\":46187},{\"end\":46204,\"start\":46201},{\"end\":46525,\"start\":46522},{\"end\":46536,\"start\":46531},{\"end\":46544,\"start\":46540},{\"end\":46551,\"start\":46548},{\"end\":46559,\"start\":46555},{\"end\":46567,\"start\":46563},{\"end\":46574,\"start\":46571},{\"end\":46580,\"start\":46578},{\"end\":46587,\"start\":46584},{\"end\":46594,\"start\":46591},{\"end\":47088,\"start\":47085},{\"end\":47099,\"start\":47094},{\"end\":47111,\"start\":47105},{\"end\":47124,\"start\":47117},{\"end\":47136,\"start\":47130},{\"end\":47478,\"start\":47475},{\"end\":47485,\"start\":47482},{\"end\":47494,\"start\":47491},{\"end\":47501,\"start\":47498},{\"end\":47509,\"start\":47505},{\"end\":47519,\"start\":47515},{\"end\":47528,\"start\":47525},{\"end\":47534,\"start\":47530},{\"end\":47970,\"start\":47961},{\"end\":47980,\"start\":47974},{\"end\":48123,\"start\":48120},{\"end\":48132,\"start\":48127},{\"end\":48141,\"start\":48136},{\"end\":48150,\"start\":48145},{\"end\":48158,\"start\":48154},{\"end\":48165,\"start\":48162},{\"end\":48175,\"start\":48167},{\"end\":48556,\"start\":48551},{\"end\":48562,\"start\":48560},{\"end\":48569,\"start\":48566},{\"end\":48965,\"start\":48959},{\"end\":48976,\"start\":48969},{\"end\":48986,\"start\":48980},{\"end\":48996,\"start\":48990},{\"end\":49006,\"start\":49000},{\"end\":49345,\"start\":49339},{\"end\":49355,\"start\":49349},{\"end\":49365,\"start\":49359},{\"end\":49375,\"start\":49369},{\"end\":49383,\"start\":49379},{\"end\":49393,\"start\":49387},{\"end\":49708,\"start\":49701},{\"end\":49718,\"start\":49714},{\"end\":49729,\"start\":49722},{\"end\":49742,\"start\":49735},{\"end\":49755,\"start\":49744},{\"end\":50043,\"start\":50039},{\"end\":50050,\"start\":50047},{\"end\":50059,\"start\":50054},{\"end\":50068,\"start\":50063},{\"end\":50076,\"start\":50072},{\"end\":50082,\"start\":50080},{\"end\":50090,\"start\":50086},{\"end\":50097,\"start\":50094},{\"end\":50107,\"start\":50099},{\"end\":50384,\"start\":50378},{\"end\":50397,\"start\":50388},{\"end\":50411,\"start\":50401},{\"end\":50421,\"start\":50415},{\"end\":50437,\"start\":50425},{\"end\":50448,\"start\":50441},{\"end\":50797,\"start\":50791},{\"end\":50807,\"start\":50801},{\"end\":50817,\"start\":50809},{\"end\":51075,\"start\":51064},{\"end\":51083,\"start\":51079},{\"end\":51091,\"start\":51087},{\"end\":51099,\"start\":51095},{\"end\":51394,\"start\":51391},{\"end\":51405,\"start\":51398},{\"end\":51412,\"start\":51409},{\"end\":51425,\"start\":51418},{\"end\":51436,\"start\":51429},{\"end\":51714,\"start\":51708},{\"end\":51724,\"start\":51718},{\"end\":51736,\"start\":51728},{\"end\":51990,\"start\":51984},{\"end\":52000,\"start\":51994},{\"end\":52010,\"start\":52004},{\"end\":52021,\"start\":52014},{\"end\":52030,\"start\":52025},{\"end\":52041,\"start\":52034},{\"end\":52049,\"start\":52045},{\"end\":52395,\"start\":52391},{\"end\":52402,\"start\":52399},{\"end\":52415,\"start\":52408},{\"end\":52426,\"start\":52419},{\"end\":52437,\"start\":52430},{\"end\":52448,\"start\":52441},{\"end\":52704,\"start\":52697},{\"end\":52716,\"start\":52708},{\"end\":52724,\"start\":52720},{\"end\":52734,\"start\":52730},{\"end\":52981,\"start\":52978},{\"end\":52989,\"start\":52985},{\"end\":53000,\"start\":52993},{\"end\":53010,\"start\":53004},{\"end\":53020,\"start\":53014},{\"end\":53031,\"start\":53024},{\"end\":53258,\"start\":53255},{\"end\":53265,\"start\":53262},{\"end\":53275,\"start\":53271},{\"end\":53287,\"start\":53277},{\"end\":53570,\"start\":53562},{\"end\":53579,\"start\":53574},{\"end\":53594,\"start\":53583},{\"end\":53882,\"start\":53880},{\"end\":53890,\"start\":53886},{\"end\":53899,\"start\":53894},{\"end\":53905,\"start\":53903},{\"end\":53911,\"start\":53909},{\"end\":53918,\"start\":53915},{\"end\":53926,\"start\":53922},{\"end\":54339,\"start\":54337},{\"end\":54348,\"start\":54343},{\"end\":54362,\"start\":54352},{\"end\":54371,\"start\":54366},{\"end\":54381,\"start\":54377},{\"end\":54622,\"start\":54612},{\"end\":54632,\"start\":54626},{\"end\":54640,\"start\":54636},{\"end\":54651,\"start\":54644},{\"end\":54658,\"start\":54655},{\"end\":54942,\"start\":54939},{\"end\":54951,\"start\":54948},{\"end\":54959,\"start\":54955},{\"end\":54966,\"start\":54963},{\"end\":55249,\"start\":55246},{\"end\":55256,\"start\":55253},{\"end\":55264,\"start\":55260},{\"end\":55272,\"start\":55266},{\"end\":55642,\"start\":55639},{\"end\":55648,\"start\":55646},{\"end\":55654,\"start\":55652},{\"end\":55663,\"start\":55658},{\"end\":55670,\"start\":55665},{\"end\":56006,\"start\":56000},{\"end\":56014,\"start\":56010},{\"end\":56024,\"start\":56018},{\"end\":56299,\"start\":56293},{\"end\":56309,\"start\":56303},{\"end\":56318,\"start\":56313},{\"end\":56329,\"start\":56322},{\"end\":56634,\"start\":56628},{\"end\":56645,\"start\":56638},{\"end\":56658,\"start\":56649},{\"end\":56666,\"start\":56662},{\"end\":56677,\"start\":56670},{\"end\":56940,\"start\":56934},{\"end\":56951,\"start\":56944},{\"end\":56959,\"start\":56955},{\"end\":56972,\"start\":56963},{\"end\":56983,\"start\":56976},{\"end\":57347,\"start\":57341},{\"end\":57356,\"start\":57351},{\"end\":57365,\"start\":57360},{\"end\":57374,\"start\":57369},{\"end\":57386,\"start\":57378},{\"end\":57396,\"start\":57390},{\"end\":57407,\"start\":57400},{\"end\":57414,\"start\":57411},{\"end\":57428,\"start\":57418},{\"end\":57438,\"start\":57432},{\"end\":57845,\"start\":57841},{\"end\":57858,\"start\":57849},{\"end\":57866,\"start\":57862},{\"end\":57887,\"start\":57870},{\"end\":58179,\"start\":58171},{\"end\":58189,\"start\":58183},{\"end\":58202,\"start\":58193},{\"end\":58444,\"start\":58435},{\"end\":58453,\"start\":58448},{\"end\":58464,\"start\":58457},{\"end\":58473,\"start\":58468},{\"end\":58670,\"start\":58661},{\"end\":58677,\"start\":58674},{\"end\":58686,\"start\":58681},{\"end\":58992,\"start\":58986},{\"end\":59001,\"start\":58996},{\"end\":59012,\"start\":59005},{\"end\":59021,\"start\":59016},{\"end\":59231,\"start\":59227},{\"end\":59240,\"start\":59235},{\"end\":59252,\"start\":59244},{\"end\":59260,\"start\":59256},{\"end\":59272,\"start\":59262},{\"end\":59596,\"start\":59592},{\"end\":59604,\"start\":59600},{\"end\":59612,\"start\":59608},{\"end\":59884,\"start\":59877},{\"end\":59894,\"start\":59888},{\"end\":59904,\"start\":59898},{\"end\":60152,\"start\":60148},{\"end\":60160,\"start\":60156},{\"end\":60166,\"start\":60164},{\"end\":60172,\"start\":60170},{\"end\":60179,\"start\":60176},{\"end\":60190,\"start\":60183},{\"end\":60203,\"start\":60196},{\"end\":60214,\"start\":60207},{\"end\":60665,\"start\":60662},{\"end\":60675,\"start\":60669},{\"end\":60681,\"start\":60679},{\"end\":60688,\"start\":60685},{\"end\":60699,\"start\":60692},{\"end\":60712,\"start\":60705},{\"end\":60721,\"start\":60716},{\"end\":60979,\"start\":60977},{\"end\":60985,\"start\":60983},{\"end\":60994,\"start\":60989},{\"end\":61242,\"start\":61240},{\"end\":61251,\"start\":61246},{\"end\":61258,\"start\":61255},{\"end\":61265,\"start\":61262},{\"end\":61276,\"start\":61269},{\"end\":61284,\"start\":61282},{\"end\":61622,\"start\":61616},{\"end\":61635,\"start\":61626},{\"end\":61641,\"start\":61639},{\"end\":61651,\"start\":61645},{\"end\":61664,\"start\":61657},{\"end\":61672,\"start\":61670},{\"end\":61681,\"start\":61676},{\"end\":62127,\"start\":62124},{\"end\":62136,\"start\":62131},{\"end\":62143,\"start\":62140},{\"end\":62150,\"start\":62147},{\"end\":62386,\"start\":62383},{\"end\":62392,\"start\":62390},{\"end\":62400,\"start\":62396},{\"end\":62407,\"start\":62404},{\"end\":62415,\"start\":62411},{\"end\":62423,\"start\":62419},{\"end\":62793,\"start\":62788},{\"end\":62803,\"start\":62797},{\"end\":62816,\"start\":62807},{\"end\":62823,\"start\":62820},{\"end\":62834,\"start\":62830},{\"end\":62844,\"start\":62838},{\"end\":62852,\"start\":62848},{\"end\":62865,\"start\":62856},{\"end\":62875,\"start\":62871},{\"end\":62885,\"start\":62879},{\"end\":63373,\"start\":63366},{\"end\":63384,\"start\":63377},{\"end\":63394,\"start\":63388},{\"end\":63407,\"start\":63398},{\"end\":63416,\"start\":63411},{\"end\":63427,\"start\":63422},{\"end\":63437,\"start\":63431},{\"end\":63451,\"start\":63441},{\"end\":63763,\"start\":63758},{\"end\":63773,\"start\":63767},{\"end\":63784,\"start\":63777},{\"end\":63796,\"start\":63788},{\"end\":63805,\"start\":63800},{\"end\":64048,\"start\":64044},{\"end\":64057,\"start\":64052},{\"end\":64068,\"start\":64061},{\"end\":64076,\"start\":64072},{\"end\":64084,\"start\":64080},{\"end\":64096,\"start\":64090},{\"end\":64102,\"start\":64098},{\"end\":64498,\"start\":64494},{\"end\":64504,\"start\":64502},{\"end\":64511,\"start\":64508},{\"end\":64518,\"start\":64515},{\"end\":64525,\"start\":64522},{\"end\":64532,\"start\":64529},{\"end\":64539,\"start\":64536},{\"end\":64837,\"start\":64833},{\"end\":64846,\"start\":64841},{\"end\":64853,\"start\":64850},{\"end\":64862,\"start\":64855},{\"end\":65197,\"start\":65193},{\"end\":65205,\"start\":65203},{\"end\":65215,\"start\":65209},{\"end\":65223,\"start\":65219},{\"end\":65229,\"start\":65227},{\"end\":65240,\"start\":65231},{\"end\":65507,\"start\":65503},{\"end\":65519,\"start\":65511},{\"end\":65526,\"start\":65523},{\"end\":65729,\"start\":65721},{\"end\":65738,\"start\":65733},{\"end\":65750,\"start\":65744},{\"end\":66069,\"start\":66061},{\"end\":66079,\"start\":66073},{\"end\":66091,\"start\":66085},{\"end\":66353,\"start\":66349},{\"end\":66362,\"start\":66357},{\"end\":66370,\"start\":66366},{\"end\":66382,\"start\":66374},{\"end\":66394,\"start\":66386},{\"end\":66401,\"start\":66398},{\"end\":66411,\"start\":66405},{\"end\":66420,\"start\":66415},{\"end\":66428,\"start\":66424},{\"end\":66441,\"start\":66432},{\"end\":66452,\"start\":66445},{\"end\":66464,\"start\":66456},{\"end\":66478,\"start\":66468},{\"end\":66484,\"start\":66482},{\"end\":66495,\"start\":66488},{\"end\":66502,\"start\":66499},{\"end\":66508,\"start\":66506},{\"end\":66519,\"start\":66512},{\"end\":66529,\"start\":66523},{\"end\":66538,\"start\":66533},{\"end\":66548,\"start\":66542},{\"end\":66556,\"start\":66552},{\"end\":67354,\"start\":67352},{\"end\":67361,\"start\":67358},{\"end\":67368,\"start\":67365},{\"end\":67375,\"start\":67372},{\"end\":67382,\"start\":67379},{\"end\":67613,\"start\":67610},{\"end\":67622,\"start\":67617},{\"end\":67798,\"start\":67795},{\"end\":67806,\"start\":67802},{\"end\":67813,\"start\":67810},{\"end\":67819,\"start\":67817},{\"end\":67826,\"start\":67823},{\"end\":67835,\"start\":67828},{\"end\":68170,\"start\":68168},{\"end\":68177,\"start\":68174},{\"end\":68184,\"start\":68181},{\"end\":68192,\"start\":68188},{\"end\":68198,\"start\":68196},{\"end\":68205,\"start\":68202},{\"end\":68215,\"start\":68212},{\"end\":68644,\"start\":68641},{\"end\":68650,\"start\":68648},{\"end\":68658,\"start\":68654},{\"end\":68669,\"start\":68662},{\"end\":68682,\"start\":68675},{\"end\":68693,\"start\":68684},{\"end\":69080,\"start\":69077},{\"end\":69088,\"start\":69084},{\"end\":69097,\"start\":69092},{\"end\":69106,\"start\":69101},{\"end\":69114,\"start\":69110},{\"end\":69121,\"start\":69118},{\"end\":69439,\"start\":69436},{\"end\":69445,\"start\":69443},{\"end\":69457,\"start\":69449},{\"end\":69466,\"start\":69461},{\"end\":69827,\"start\":69822},{\"end\":69834,\"start\":69831},{\"end\":69844,\"start\":69840},{\"end\":69856,\"start\":69848},{\"end\":69862,\"start\":69858},{\"end\":70235,\"start\":70229},{\"end\":70247,\"start\":70239},{\"end\":70256,\"start\":70251},{\"end\":70270,\"start\":70260},{\"end\":70278,\"start\":70272},{\"end\":70719,\"start\":70714},{\"end\":70726,\"start\":70723},{\"end\":70735,\"start\":70730},{\"end\":70747,\"start\":70739},{\"end\":70753,\"start\":70751},{\"end\":70761,\"start\":70757},{\"end\":70769,\"start\":70765},{\"end\":71139,\"start\":71135},{\"end\":71145,\"start\":71143},{\"end\":71151,\"start\":71149},{\"end\":71162,\"start\":71155},{\"end\":71168,\"start\":71166},{\"end\":71566,\"start\":71560},{\"end\":72283,\"start\":72273}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":49298061},\"end\":38849,\"start\":38581},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59292009},\"end\":39197,\"start\":38851},{\"attributes\":{\"doi\":\"arXiv:1804.05345\",\"id\":\"b2\"},\"end\":39600,\"start\":39199},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":216224067},\"end\":39921,\"start\":39602},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":246483142},\"end\":40391,\"start\":39923},{\"attributes\":{\"doi\":\"10.18653/v1/s17-2001\",\"id\":\"b5\",\"matched_paper_id\":4421747},\"end\":40957,\"start\":40393},{\"attributes\":{\"doi\":\"arXiv:2001.04246\",\"id\":\"b6\"},\"end\":41377,\"start\":40959},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220768628},\"end\":41768,\"start\":41379},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":235367934},\"end\":42147,\"start\":41770},{\"attributes\":{\"doi\":\"arXiv:2101.00063\",\"id\":\"b9\"},\"end\":42461,\"start\":42149},{\"attributes\":{\"id\":\"b10\"},\"end\":43159,\"start\":42463},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b11\"},\"end\":43490,\"start\":43161},{\"attributes\":{\"id\":\"b12\"},\"end\":43812,\"start\":43492},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16639476},\"end\":44108,\"start\":43814},{\"attributes\":{\"doi\":\"arXiv:2004.02441\",\"id\":\"b14\"},\"end\":44369,\"start\":44110},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220055739},\"end\":44788,\"start\":44371},{\"attributes\":{\"doi\":\"arXiv:1909.11556\",\"id\":\"b16\"},\"end\":45046,\"start\":44790},{\"attributes\":{\"doi\":\"arXiv:2208.11580\",\"id\":\"b17\"},\"end\":45352,\"start\":45048},{\"attributes\":{\"doi\":\"arXiv:1902.09574\",\"id\":\"b18\"},\"end\":45579,\"start\":45354},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":211532645},\"end\":46056,\"start\":45581},{\"attributes\":{\"doi\":\"arXiv:2211.16749\",\"id\":\"b20\"},\"end\":46438,\"start\":46058},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":221376946},\"end\":47001,\"start\":46440},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":226300993},\"end\":47369,\"start\":47003},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":235414966},\"end\":47920,\"start\":47371},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b24\"},\"end\":48116,\"start\":47922},{\"attributes\":{\"id\":\"b25\"},\"end\":48462,\"start\":48118},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52988769},\"end\":48863,\"start\":48464},{\"attributes\":{\"doi\":\"arXiv:2006.10518\",\"id\":\"b27\"},\"end\":49234,\"start\":48865},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231934142},\"end\":49695,\"start\":49236},{\"attributes\":{\"doi\":\"arXiv:2006.11316\",\"id\":\"b29\"},\"end\":50035,\"start\":49697},{\"attributes\":{\"doi\":\"arXiv:1909.10351\",\"id\":\"b30\"},\"end\":50372,\"start\":50037},{\"attributes\":{\"doi\":\"arXiv:2205.10403\",\"id\":\"b31\"},\"end\":50787,\"start\":50374},{\"attributes\":{\"doi\":\"arXiv:2005.06628\",\"id\":\"b32\"},\"end\":50981,\"start\":50789},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":235436029},\"end\":51355,\"start\":50983},{\"attributes\":{\"id\":\"b34\"},\"end\":51667,\"start\":51357},{\"attributes\":{\"doi\":\"arXiv:2001.04451\",\"id\":\"b35\"},\"end\":51884,\"start\":51669},{\"attributes\":{\"doi\":\"arXiv:2203.07259\",\"id\":\"b36\"},\"end\":52330,\"start\":51886},{\"attributes\":{\"doi\":\"arXiv:2204.09656\",\"id\":\"b37\"},\"end\":52654,\"start\":52332},{\"attributes\":{\"doi\":\"arXiv:2109.04838\",\"id\":\"b38\"},\"end\":52896,\"start\":52656},{\"attributes\":{\"doi\":\"arXiv:1909.11942\",\"id\":\"b39\"},\"end\":53251,\"start\":52898},{\"attributes\":{\"doi\":\"arXiv:2210.11870\",\"id\":\"b40\"},\"end\":53527,\"start\":53253},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15710851},\"end\":53876,\"start\":53529},{\"attributes\":{\"doi\":\"arXiv:2009.08065\",\"id\":\"b42\"},\"end\":54293,\"start\":53878},{\"attributes\":{\"doi\":\"arXiv:1608.08710\",\"id\":\"b43\"},\"end\":54553,\"start\":54295},{\"attributes\":{\"doi\":\"arXiv:1911.07412\",\"id\":\"b44\"},\"end\":54846,\"start\":54555},{\"attributes\":{\"doi\":\"arXiv:2010.01791\",\"id\":\"b45\"},\"end\":55189,\"start\":54848},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232307646},\"end\":55577,\"start\":55191},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":236477807},\"end\":55953,\"start\":55579},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":166227946},\"end\":56218,\"start\":55955},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":216867120},\"end\":56578,\"start\":56220},{\"attributes\":{\"doi\":\"arXiv:1907.04018\",\"id\":\"b50\"},\"end\":56861,\"start\":56580},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":221172670},\"end\":57267,\"start\":56863},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":202786778},\"end\":57751,\"start\":57269},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":247771234},\"end\":58113,\"start\":57753},{\"attributes\":{\"doi\":\"arXiv:2005.00561\",\"id\":\"b54\"},\"end\":58370,\"start\":58115},{\"attributes\":{\"doi\":\"arXiv:1606.05250\",\"id\":\"b55\"},\"end\":58657,\"start\":58372},{\"attributes\":{\"doi\":\"arXiv:1806.03822\",\"id\":\"b56\"},\"end\":58914,\"start\":58659},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":251005814},\"end\":59223,\"start\":58916},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b58\"},\"end\":59536,\"start\":59225},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":218665313},\"end\":59828,\"start\":59538},{\"attributes\":{\"id\":\"b60\"},\"end\":60088,\"start\":59830},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":202565587},\"end\":60580,\"start\":60090},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":54632098},\"end\":60948,\"start\":60582},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":59523610},\"end\":61176,\"start\":60950},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":248497882},\"end\":61533,\"start\":61178},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":990233},\"end\":62120,\"start\":61535},{\"attributes\":{\"doi\":\"arXiv:1908.09355\",\"id\":\"b66\"},\"end\":62379,\"start\":62122},{\"attributes\":{\"doi\":\"arXiv:2004.02984\",\"id\":\"b67\"},\"end\":62694,\"start\":62381},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":237421361},\"end\":63284,\"start\":62696},{\"attributes\":{\"id\":\"b69\"},\"end\":63653,\"start\":63286},{\"attributes\":{\"doi\":\"arXiv:1905.09418\",\"id\":\"b70\"},\"end\":64040,\"start\":63655},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b71\"},\"end\":64414,\"start\":64042},{\"attributes\":{\"doi\":\"arXiv:2005.14187\",\"id\":\"b72\"},\"end\":64752,\"start\":64416},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":229298088},\"end\":65168,\"start\":64754},{\"attributes\":{\"id\":\"b74\"},\"end\":65189,\"start\":65170},{\"attributes\":{\"doi\":\"arXiv:2006.04768\",\"id\":\"b75\"},\"end\":65454,\"start\":65191},{\"attributes\":{\"doi\":\"arXiv:1910.04732\",\"id\":\"b76\"},\"end\":65677,\"start\":65456},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":44072099},\"end\":65978,\"start\":65679},{\"attributes\":{\"doi\":\"arXiv:1704.05426\",\"id\":\"b78\"},\"end\":66286,\"start\":65980},{\"attributes\":{\"id\":\"b79\"},\"end\":67298,\"start\":66288},{\"attributes\":{\"doi\":\"arXiv:2004.11886\",\"id\":\"b80\"},\"end\":67551,\"start\":67300},{\"attributes\":{\"doi\":\"arXiv:2204.00408\",\"id\":\"b81\"},\"end\":67791,\"start\":67553},{\"attributes\":{\"doi\":\"arXiv:2004.12993\",\"id\":\"b82\"},\"end\":68072,\"start\":67793},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":235253768},\"end\":68637,\"start\":68074},{\"attributes\":{\"doi\":\"arXiv:2105.14636\",\"id\":\"b84\"},\"end\":68977,\"start\":68639},{\"attributes\":{\"doi\":\"arXiv:2107.13686\",\"id\":\"b85\"},\"end\":69350,\"start\":68979},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":219531562},\"end\":69730,\"start\":69352},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":218571099},\"end\":70204,\"start\":69732},{\"attributes\":{\"id\":\"b88\"},\"end\":70598,\"start\":70206},{\"attributes\":{\"id\":\"b89\"},\"end\":70619,\"start\":70600},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":250072480},\"end\":71067,\"start\":70621},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":219531455},\"end\":71428,\"start\":71069},{\"attributes\":{\"id\":\"b92\"},\"end\":72117,\"start\":71430},{\"attributes\":{\"id\":\"b93\"},\"end\":73442,\"start\":72119},{\"attributes\":{\"id\":\"b94\"},\"end\":73845,\"start\":73444},{\"attributes\":{\"id\":\"b95\"},\"end\":74600,\"start\":73847},{\"attributes\":{\"id\":\"b96\"},\"end\":74815,\"start\":74602},{\"attributes\":{\"id\":\"b97\"},\"end\":75498,\"start\":74817},{\"attributes\":{\"id\":\"b98\"},\"end\":75669,\"start\":75500}]", "bib_title": "[{\"end\":38624,\"start\":38581},{\"end\":38929,\"start\":38851},{\"end\":39692,\"start\":39602},{\"end\":40018,\"start\":39923},{\"end\":40469,\"start\":40393},{\"end\":41437,\"start\":41379},{\"end\":41836,\"start\":41770},{\"end\":42514,\"start\":42463},{\"end\":43540,\"start\":43492},{\"end\":43875,\"start\":43814},{\"end\":44448,\"start\":44371},{\"end\":45651,\"start\":45581},{\"end\":46518,\"start\":46440},{\"end\":47081,\"start\":47003},{\"end\":47469,\"start\":47371},{\"end\":48547,\"start\":48464},{\"end\":49335,\"start\":49236},{\"end\":51060,\"start\":50983},{\"end\":51387,\"start\":51357},{\"end\":53558,\"start\":53529},{\"end\":55242,\"start\":55191},{\"end\":55635,\"start\":55579},{\"end\":55996,\"start\":55955},{\"end\":56289,\"start\":56220},{\"end\":56930,\"start\":56863},{\"end\":57337,\"start\":57269},{\"end\":57837,\"start\":57753},{\"end\":58982,\"start\":58916},{\"end\":59588,\"start\":59538},{\"end\":60144,\"start\":60090},{\"end\":60658,\"start\":60582},{\"end\":60973,\"start\":60950},{\"end\":61236,\"start\":61178},{\"end\":61612,\"start\":61535},{\"end\":62784,\"start\":62696},{\"end\":64829,\"start\":64754},{\"end\":65717,\"start\":65679},{\"end\":66345,\"start\":66288},{\"end\":68164,\"start\":68074},{\"end\":69432,\"start\":69352},{\"end\":69816,\"start\":69732},{\"end\":70225,\"start\":70206},{\"end\":70710,\"start\":70621},{\"end\":71131,\"start\":71069},{\"end\":71558,\"start\":71430},{\"end\":72271,\"start\":72119},{\"end\":74966,\"start\":74817}]", "bib_author": "[{\"end\":38636,\"start\":38626},{\"end\":38644,\"start\":38636},{\"end\":38655,\"start\":38644},{\"end\":38941,\"start\":38931},{\"end\":38952,\"start\":38941},{\"end\":38962,\"start\":38952},{\"end\":39309,\"start\":39299},{\"end\":39323,\"start\":39309},{\"end\":39340,\"start\":39323},{\"end\":39351,\"start\":39340},{\"end\":39358,\"start\":39351},{\"end\":39704,\"start\":39694},{\"end\":39715,\"start\":39704},{\"end\":39728,\"start\":39715},{\"end\":40030,\"start\":40020},{\"end\":40041,\"start\":40030},{\"end\":40054,\"start\":40041},{\"end\":40478,\"start\":40471},{\"end\":40486,\"start\":40478},{\"end\":40496,\"start\":40486},{\"end\":40512,\"start\":40496},{\"end\":40522,\"start\":40512},{\"end\":40967,\"start\":40959},{\"end\":40973,\"start\":40967},{\"end\":40980,\"start\":40973},{\"end\":40988,\"start\":40980},{\"end\":40994,\"start\":40988},{\"end\":41002,\"start\":40994},{\"end\":41010,\"start\":41002},{\"end\":41019,\"start\":41010},{\"end\":41026,\"start\":41019},{\"end\":41034,\"start\":41026},{\"end\":41043,\"start\":41034},{\"end\":41447,\"start\":41439},{\"end\":41458,\"start\":41447},{\"end\":41467,\"start\":41458},{\"end\":41474,\"start\":41467},{\"end\":41483,\"start\":41474},{\"end\":41491,\"start\":41483},{\"end\":41501,\"start\":41491},{\"end\":41846,\"start\":41838},{\"end\":41855,\"start\":41846},{\"end\":41862,\"start\":41855},{\"end\":41870,\"start\":41862},{\"end\":41879,\"start\":41870},{\"end\":41886,\"start\":41879},{\"end\":41890,\"start\":41886},{\"end\":42157,\"start\":42149},{\"end\":42166,\"start\":42157},{\"end\":42174,\"start\":42166},{\"end\":42181,\"start\":42174},{\"end\":42189,\"start\":42181},{\"end\":42196,\"start\":42189},{\"end\":42207,\"start\":42196},{\"end\":42525,\"start\":42516},{\"end\":42537,\"start\":42525},{\"end\":42548,\"start\":42537},{\"end\":43171,\"start\":43161},{\"end\":43183,\"start\":43171},{\"end\":43190,\"start\":43183},{\"end\":43203,\"start\":43190},{\"end\":43209,\"start\":43203},{\"end\":43552,\"start\":43542},{\"end\":43558,\"start\":43552},{\"end\":43567,\"start\":43558},{\"end\":43573,\"start\":43567},{\"end\":43886,\"start\":43877},{\"end\":43898,\"start\":43886},{\"end\":44120,\"start\":44110},{\"end\":44133,\"start\":44120},{\"end\":44144,\"start\":44133},{\"end\":44155,\"start\":44144},{\"end\":44162,\"start\":44155},{\"end\":44460,\"start\":44450},{\"end\":44473,\"start\":44460},{\"end\":44485,\"start\":44473},{\"end\":44498,\"start\":44485},{\"end\":44509,\"start\":44498},{\"end\":44797,\"start\":44790},{\"end\":44806,\"start\":44797},{\"end\":44815,\"start\":44806},{\"end\":44819,\"start\":44815},{\"end\":45151,\"start\":45140},{\"end\":45163,\"start\":45151},{\"end\":45362,\"start\":45354},{\"end\":45371,\"start\":45362},{\"end\":45381,\"start\":45371},{\"end\":45663,\"start\":45653},{\"end\":45671,\"start\":45663},{\"end\":45678,\"start\":45671},{\"end\":45688,\"start\":45678},{\"end\":45696,\"start\":45688},{\"end\":45706,\"start\":45696},{\"end\":45715,\"start\":45706},{\"end\":45723,\"start\":45715},{\"end\":45735,\"start\":45723},{\"end\":46149,\"start\":46143},{\"end\":46159,\"start\":46149},{\"end\":46171,\"start\":46159},{\"end\":46185,\"start\":46171},{\"end\":46197,\"start\":46185},{\"end\":46206,\"start\":46197},{\"end\":46527,\"start\":46520},{\"end\":46538,\"start\":46527},{\"end\":46546,\"start\":46538},{\"end\":46553,\"start\":46546},{\"end\":46561,\"start\":46553},{\"end\":46569,\"start\":46561},{\"end\":46576,\"start\":46569},{\"end\":46582,\"start\":46576},{\"end\":46589,\"start\":46582},{\"end\":46596,\"start\":46589},{\"end\":47090,\"start\":47083},{\"end\":47101,\"start\":47090},{\"end\":47113,\"start\":47101},{\"end\":47126,\"start\":47113},{\"end\":47138,\"start\":47126},{\"end\":47480,\"start\":47471},{\"end\":47487,\"start\":47480},{\"end\":47496,\"start\":47487},{\"end\":47503,\"start\":47496},{\"end\":47511,\"start\":47503},{\"end\":47521,\"start\":47511},{\"end\":47530,\"start\":47521},{\"end\":47536,\"start\":47530},{\"end\":47972,\"start\":47959},{\"end\":47982,\"start\":47972},{\"end\":48125,\"start\":48118},{\"end\":48134,\"start\":48125},{\"end\":48143,\"start\":48134},{\"end\":48152,\"start\":48143},{\"end\":48160,\"start\":48152},{\"end\":48167,\"start\":48160},{\"end\":48177,\"start\":48167},{\"end\":48558,\"start\":48549},{\"end\":48564,\"start\":48558},{\"end\":48571,\"start\":48564},{\"end\":48578,\"start\":48571},{\"end\":48582,\"start\":48578},{\"end\":48967,\"start\":48957},{\"end\":48978,\"start\":48967},{\"end\":48988,\"start\":48978},{\"end\":48998,\"start\":48988},{\"end\":49008,\"start\":48998},{\"end\":49347,\"start\":49337},{\"end\":49357,\"start\":49347},{\"end\":49367,\"start\":49357},{\"end\":49377,\"start\":49367},{\"end\":49385,\"start\":49377},{\"end\":49395,\"start\":49385},{\"end\":49710,\"start\":49697},{\"end\":49720,\"start\":49710},{\"end\":49731,\"start\":49720},{\"end\":49744,\"start\":49731},{\"end\":49757,\"start\":49744},{\"end\":50045,\"start\":50037},{\"end\":50052,\"start\":50045},{\"end\":50061,\"start\":50052},{\"end\":50070,\"start\":50061},{\"end\":50078,\"start\":50070},{\"end\":50084,\"start\":50078},{\"end\":50092,\"start\":50084},{\"end\":50099,\"start\":50092},{\"end\":50109,\"start\":50099},{\"end\":50386,\"start\":50374},{\"end\":50399,\"start\":50386},{\"end\":50413,\"start\":50399},{\"end\":50423,\"start\":50413},{\"end\":50439,\"start\":50423},{\"end\":50450,\"start\":50439},{\"end\":50461,\"start\":50450},{\"end\":50799,\"start\":50789},{\"end\":50809,\"start\":50799},{\"end\":50819,\"start\":50809},{\"end\":51077,\"start\":51062},{\"end\":51085,\"start\":51077},{\"end\":51093,\"start\":51085},{\"end\":51101,\"start\":51093},{\"end\":51396,\"start\":51389},{\"end\":51407,\"start\":51396},{\"end\":51414,\"start\":51407},{\"end\":51427,\"start\":51414},{\"end\":51438,\"start\":51427},{\"end\":51716,\"start\":51706},{\"end\":51726,\"start\":51716},{\"end\":51738,\"start\":51726},{\"end\":51992,\"start\":51982},{\"end\":52002,\"start\":51992},{\"end\":52012,\"start\":52002},{\"end\":52023,\"start\":52012},{\"end\":52032,\"start\":52023},{\"end\":52043,\"start\":52032},{\"end\":52051,\"start\":52043},{\"end\":52062,\"start\":52051},{\"end\":52066,\"start\":52062},{\"end\":52397,\"start\":52389},{\"end\":52404,\"start\":52397},{\"end\":52417,\"start\":52404},{\"end\":52428,\"start\":52417},{\"end\":52439,\"start\":52428},{\"end\":52450,\"start\":52439},{\"end\":52706,\"start\":52695},{\"end\":52718,\"start\":52706},{\"end\":52726,\"start\":52718},{\"end\":52736,\"start\":52726},{\"end\":52983,\"start\":52976},{\"end\":52991,\"start\":52983},{\"end\":53002,\"start\":52991},{\"end\":53012,\"start\":53002},{\"end\":53022,\"start\":53012},{\"end\":53033,\"start\":53022},{\"end\":53260,\"start\":53253},{\"end\":53267,\"start\":53260},{\"end\":53277,\"start\":53267},{\"end\":53289,\"start\":53277},{\"end\":53572,\"start\":53560},{\"end\":53581,\"start\":53572},{\"end\":53596,\"start\":53581},{\"end\":53884,\"start\":53878},{\"end\":53892,\"start\":53884},{\"end\":53901,\"start\":53892},{\"end\":53907,\"start\":53901},{\"end\":53913,\"start\":53907},{\"end\":53920,\"start\":53913},{\"end\":53928,\"start\":53920},{\"end\":54341,\"start\":54335},{\"end\":54350,\"start\":54341},{\"end\":54364,\"start\":54350},{\"end\":54373,\"start\":54364},{\"end\":54383,\"start\":54373},{\"end\":54624,\"start\":54610},{\"end\":54634,\"start\":54624},{\"end\":54642,\"start\":54634},{\"end\":54653,\"start\":54642},{\"end\":54660,\"start\":54653},{\"end\":54944,\"start\":54937},{\"end\":54953,\"start\":54944},{\"end\":54961,\"start\":54953},{\"end\":54968,\"start\":54961},{\"end\":54975,\"start\":54968},{\"end\":54979,\"start\":54975},{\"end\":55251,\"start\":55244},{\"end\":55258,\"start\":55251},{\"end\":55266,\"start\":55258},{\"end\":55274,\"start\":55266},{\"end\":55644,\"start\":55637},{\"end\":55650,\"start\":55644},{\"end\":55656,\"start\":55650},{\"end\":55665,\"start\":55656},{\"end\":55672,\"start\":55665},{\"end\":56008,\"start\":55998},{\"end\":56016,\"start\":56008},{\"end\":56026,\"start\":56016},{\"end\":56301,\"start\":56291},{\"end\":56311,\"start\":56301},{\"end\":56320,\"start\":56311},{\"end\":56331,\"start\":56320},{\"end\":56636,\"start\":56626},{\"end\":56647,\"start\":56636},{\"end\":56660,\"start\":56647},{\"end\":56668,\"start\":56660},{\"end\":56679,\"start\":56668},{\"end\":56942,\"start\":56932},{\"end\":56953,\"start\":56942},{\"end\":56961,\"start\":56953},{\"end\":56974,\"start\":56961},{\"end\":56985,\"start\":56974},{\"end\":57349,\"start\":57339},{\"end\":57358,\"start\":57349},{\"end\":57367,\"start\":57358},{\"end\":57376,\"start\":57367},{\"end\":57388,\"start\":57376},{\"end\":57398,\"start\":57388},{\"end\":57409,\"start\":57398},{\"end\":57416,\"start\":57409},{\"end\":57430,\"start\":57416},{\"end\":57440,\"start\":57430},{\"end\":57847,\"start\":57839},{\"end\":57860,\"start\":57847},{\"end\":57868,\"start\":57860},{\"end\":57889,\"start\":57868},{\"end\":58181,\"start\":58169},{\"end\":58191,\"start\":58181},{\"end\":58204,\"start\":58191},{\"end\":58446,\"start\":58433},{\"end\":58455,\"start\":58446},{\"end\":58466,\"start\":58455},{\"end\":58475,\"start\":58466},{\"end\":58672,\"start\":58659},{\"end\":58679,\"start\":58672},{\"end\":58688,\"start\":58679},{\"end\":58994,\"start\":58984},{\"end\":59003,\"start\":58994},{\"end\":59014,\"start\":59003},{\"end\":59023,\"start\":59014},{\"end\":59233,\"start\":59225},{\"end\":59242,\"start\":59233},{\"end\":59254,\"start\":59242},{\"end\":59262,\"start\":59254},{\"end\":59274,\"start\":59262},{\"end\":59598,\"start\":59590},{\"end\":59606,\"start\":59598},{\"end\":59614,\"start\":59606},{\"end\":59886,\"start\":59875},{\"end\":59896,\"start\":59886},{\"end\":59906,\"start\":59896},{\"end\":60154,\"start\":60146},{\"end\":60162,\"start\":60154},{\"end\":60168,\"start\":60162},{\"end\":60174,\"start\":60168},{\"end\":60181,\"start\":60174},{\"end\":60192,\"start\":60181},{\"end\":60205,\"start\":60192},{\"end\":60216,\"start\":60205},{\"end\":60667,\"start\":60660},{\"end\":60677,\"start\":60667},{\"end\":60683,\"start\":60677},{\"end\":60690,\"start\":60683},{\"end\":60701,\"start\":60690},{\"end\":60714,\"start\":60701},{\"end\":60723,\"start\":60714},{\"end\":60981,\"start\":60975},{\"end\":60987,\"start\":60981},{\"end\":60996,\"start\":60987},{\"end\":61244,\"start\":61238},{\"end\":61253,\"start\":61244},{\"end\":61260,\"start\":61253},{\"end\":61267,\"start\":61260},{\"end\":61278,\"start\":61267},{\"end\":61286,\"start\":61278},{\"end\":61624,\"start\":61614},{\"end\":61637,\"start\":61624},{\"end\":61643,\"start\":61637},{\"end\":61653,\"start\":61643},{\"end\":61666,\"start\":61653},{\"end\":61674,\"start\":61666},{\"end\":61683,\"start\":61674},{\"end\":62129,\"start\":62122},{\"end\":62138,\"start\":62129},{\"end\":62145,\"start\":62138},{\"end\":62152,\"start\":62145},{\"end\":62388,\"start\":62381},{\"end\":62394,\"start\":62388},{\"end\":62402,\"start\":62394},{\"end\":62409,\"start\":62402},{\"end\":62417,\"start\":62409},{\"end\":62425,\"start\":62417},{\"end\":62795,\"start\":62786},{\"end\":62805,\"start\":62795},{\"end\":62818,\"start\":62805},{\"end\":62825,\"start\":62818},{\"end\":62836,\"start\":62825},{\"end\":62846,\"start\":62836},{\"end\":62854,\"start\":62846},{\"end\":62867,\"start\":62854},{\"end\":62877,\"start\":62867},{\"end\":62887,\"start\":62877},{\"end\":63375,\"start\":63364},{\"end\":63386,\"start\":63375},{\"end\":63396,\"start\":63386},{\"end\":63409,\"start\":63396},{\"end\":63418,\"start\":63409},{\"end\":63429,\"start\":63418},{\"end\":63439,\"start\":63429},{\"end\":63453,\"start\":63439},{\"end\":63765,\"start\":63756},{\"end\":63775,\"start\":63765},{\"end\":63786,\"start\":63775},{\"end\":63798,\"start\":63786},{\"end\":63807,\"start\":63798},{\"end\":64050,\"start\":64042},{\"end\":64059,\"start\":64050},{\"end\":64070,\"start\":64059},{\"end\":64078,\"start\":64070},{\"end\":64086,\"start\":64078},{\"end\":64098,\"start\":64086},{\"end\":64104,\"start\":64098},{\"end\":64500,\"start\":64492},{\"end\":64506,\"start\":64500},{\"end\":64513,\"start\":64506},{\"end\":64520,\"start\":64513},{\"end\":64527,\"start\":64520},{\"end\":64534,\"start\":64527},{\"end\":64541,\"start\":64534},{\"end\":64839,\"start\":64831},{\"end\":64848,\"start\":64839},{\"end\":64855,\"start\":64848},{\"end\":64864,\"start\":64855},{\"end\":65199,\"start\":65191},{\"end\":65207,\"start\":65199},{\"end\":65217,\"start\":65207},{\"end\":65225,\"start\":65217},{\"end\":65231,\"start\":65225},{\"end\":65242,\"start\":65231},{\"end\":65509,\"start\":65501},{\"end\":65521,\"start\":65509},{\"end\":65528,\"start\":65521},{\"end\":65731,\"start\":65719},{\"end\":65740,\"start\":65731},{\"end\":65752,\"start\":65740},{\"end\":66071,\"start\":66059},{\"end\":66081,\"start\":66071},{\"end\":66093,\"start\":66081},{\"end\":66355,\"start\":66347},{\"end\":66364,\"start\":66355},{\"end\":66372,\"start\":66364},{\"end\":66384,\"start\":66372},{\"end\":66396,\"start\":66384},{\"end\":66403,\"start\":66396},{\"end\":66413,\"start\":66403},{\"end\":66422,\"start\":66413},{\"end\":66430,\"start\":66422},{\"end\":66443,\"start\":66430},{\"end\":66454,\"start\":66443},{\"end\":66466,\"start\":66454},{\"end\":66480,\"start\":66466},{\"end\":66486,\"start\":66480},{\"end\":66497,\"start\":66486},{\"end\":66504,\"start\":66497},{\"end\":66510,\"start\":66504},{\"end\":66521,\"start\":66510},{\"end\":66531,\"start\":66521},{\"end\":66540,\"start\":66531},{\"end\":66550,\"start\":66540},{\"end\":66558,\"start\":66550},{\"end\":67356,\"start\":67350},{\"end\":67363,\"start\":67356},{\"end\":67370,\"start\":67363},{\"end\":67377,\"start\":67370},{\"end\":67384,\"start\":67377},{\"end\":67615,\"start\":67608},{\"end\":67624,\"start\":67615},{\"end\":67631,\"start\":67624},{\"end\":67635,\"start\":67631},{\"end\":67800,\"start\":67793},{\"end\":67808,\"start\":67800},{\"end\":67815,\"start\":67808},{\"end\":67821,\"start\":67815},{\"end\":67828,\"start\":67821},{\"end\":67837,\"start\":67828},{\"end\":68172,\"start\":68166},{\"end\":68179,\"start\":68172},{\"end\":68186,\"start\":68179},{\"end\":68194,\"start\":68186},{\"end\":68200,\"start\":68194},{\"end\":68207,\"start\":68200},{\"end\":68217,\"start\":68207},{\"end\":68646,\"start\":68639},{\"end\":68652,\"start\":68646},{\"end\":68660,\"start\":68652},{\"end\":68671,\"start\":68660},{\"end\":68684,\"start\":68671},{\"end\":68695,\"start\":68684},{\"end\":69082,\"start\":69075},{\"end\":69090,\"start\":69082},{\"end\":69099,\"start\":69090},{\"end\":69108,\"start\":69099},{\"end\":69116,\"start\":69108},{\"end\":69123,\"start\":69116},{\"end\":69441,\"start\":69434},{\"end\":69447,\"start\":69441},{\"end\":69459,\"start\":69447},{\"end\":69468,\"start\":69459},{\"end\":69829,\"start\":69818},{\"end\":69836,\"start\":69829},{\"end\":69846,\"start\":69836},{\"end\":69858,\"start\":69846},{\"end\":69864,\"start\":69858},{\"end\":70237,\"start\":70227},{\"end\":70249,\"start\":70237},{\"end\":70258,\"start\":70249},{\"end\":70272,\"start\":70258},{\"end\":70280,\"start\":70272},{\"end\":70721,\"start\":70712},{\"end\":70728,\"start\":70721},{\"end\":70737,\"start\":70728},{\"end\":70749,\"start\":70737},{\"end\":70755,\"start\":70749},{\"end\":70763,\"start\":70755},{\"end\":70771,\"start\":70763},{\"end\":71141,\"start\":71133},{\"end\":71147,\"start\":71141},{\"end\":71153,\"start\":71147},{\"end\":71164,\"start\":71153},{\"end\":71170,\"start\":71164},{\"end\":71176,\"start\":71170},{\"end\":71180,\"start\":71176},{\"end\":71568,\"start\":71560},{\"end\":72285,\"start\":72273},{\"end\":74710,\"start\":74706}]", "bib_venue": "[{\"end\":38698,\"start\":38655},{\"end\":39011,\"start\":38962},{\"end\":39297,\"start\":39199},{\"end\":39742,\"start\":39728},{\"end\":40130,\"start\":40054},{\"end\":40618,\"start\":40542},{\"end\":41136,\"start\":41059},{\"end\":41550,\"start\":41501},{\"end\":41939,\"start\":41890},{\"end\":42277,\"start\":42223},{\"end\":42727,\"start\":42548},{\"end\":43299,\"start\":43225},{\"end\":43635,\"start\":43573},{\"end\":43952,\"start\":43898},{\"end\":44213,\"start\":44178},{\"end\":44558,\"start\":44509},{\"end\":44895,\"start\":44835},{\"end\":45138,\"start\":45048},{\"end\":45442,\"start\":45397},{\"end\":45796,\"start\":45735},{\"end\":46141,\"start\":46058},{\"end\":46691,\"start\":46596},{\"end\":47163,\"start\":47138},{\"end\":47617,\"start\":47536},{\"end\":47957,\"start\":47922},{\"end\":48270,\"start\":48177},{\"end\":48644,\"start\":48582},{\"end\":48955,\"start\":48865},{\"end\":49444,\"start\":49395},{\"end\":49839,\"start\":49773},{\"end\":50175,\"start\":50125},{\"end\":50550,\"start\":50477},{\"end\":50862,\"start\":50835},{\"end\":51150,\"start\":51101},{\"end\":51482,\"start\":51438},{\"end\":51704,\"start\":51669},{\"end\":51980,\"start\":51886},{\"end\":52387,\"start\":52332},{\"end\":52693,\"start\":52656},{\"end\":52974,\"start\":52898},{\"end\":53365,\"start\":53305},{\"end\":53691,\"start\":53596},{\"end\":54057,\"start\":53944},{\"end\":54333,\"start\":54295},{\"end\":54608,\"start\":54555},{\"end\":54935,\"start\":54848},{\"end\":55335,\"start\":55274},{\"end\":55746,\"start\":55672},{\"end\":56075,\"start\":56026},{\"end\":56375,\"start\":56331},{\"end\":56624,\"start\":56580},{\"end\":57042,\"start\":56985},{\"end\":57489,\"start\":57440},{\"end\":57916,\"start\":57889},{\"end\":58167,\"start\":58115},{\"end\":58431,\"start\":58372},{\"end\":58762,\"start\":58704},{\"end\":59049,\"start\":59023},{\"end\":59355,\"start\":59290},{\"end\":59663,\"start\":59614},{\"end\":59873,\"start\":59830},{\"end\":60277,\"start\":60216},{\"end\":60744,\"start\":60723},{\"end\":61040,\"start\":60996},{\"end\":61335,\"start\":61286},{\"end\":61769,\"start\":61683},{\"end\":62225,\"start\":62168},{\"end\":62509,\"start\":62441},{\"end\":62962,\"start\":62887},{\"end\":63362,\"start\":63286},{\"end\":63754,\"start\":63655},{\"end\":64199,\"start\":64120},{\"end\":64490,\"start\":64416},{\"end\":64946,\"start\":64864},{\"end\":65176,\"start\":65172},{\"end\":65295,\"start\":65258},{\"end\":65499,\"start\":65456},{\"end\":65813,\"start\":65752},{\"end\":66057,\"start\":65980},{\"end\":66667,\"start\":66558},{\"end\":67348,\"start\":67300},{\"end\":67606,\"start\":67553},{\"end\":67906,\"start\":67853},{\"end\":68299,\"start\":68217},{\"end\":68781,\"start\":68711},{\"end\":69073,\"start\":68979},{\"end\":69530,\"start\":69468},{\"end\":69942,\"start\":69864},{\"end\":70388,\"start\":70280},{\"end\":70606,\"start\":70602},{\"end\":70815,\"start\":70771},{\"end\":71229,\"start\":71180},{\"end\":71659,\"start\":71568},{\"end\":72341,\"start\":72285},{\"end\":73643,\"start\":73444},{\"end\":74048,\"start\":73847},{\"end\":74704,\"start\":74602},{\"end\":74975,\"start\":74968},{\"end\":75583,\"start\":75500},{\"end\":40681,\"start\":40620},{\"end\":42744,\"start\":42729},{\"end\":55383,\"start\":55337},{\"end\":60325,\"start\":60279},{\"end\":61842,\"start\":61771},{\"end\":66763,\"start\":66669},{\"end\":68368,\"start\":68301}]"}}}, "year": 2023, "month": 12, "day": 17}