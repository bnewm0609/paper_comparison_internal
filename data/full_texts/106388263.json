{"id": 106388263, "updated": "2022-02-22 09:00:22.621", "metadata": {"title": "Learning Non-Volumetric Depth Fusion Using Successive Reprojections", "authors": "[{\"first\":\"Simon\",\"last\":\"Donn\u00e9\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Geiger\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Given a set of input views, multi-view stereopsis techniques estimate depth maps to represent the 3D reconstruction of the scene; these are fused into a single, consistent, reconstruction -- most often a point cloud. In this work we propose to learn an auto-regressive depth refinement directly from data. While deep learning has improved the accuracy and speed of depth estimation significantly, learned MVS techniques remain limited to the planesweeping paradigm. We refine a set of input depth maps by successively reprojecting information from neighbouring views to leverage multi-view constraints. Compared to learning-based volumetric fusion techniques, an image-based representation allows significantly more detailed reconstructions; compared to traditional point-based techniques, our method learns noise suppression and surface completion in a data-driven fashion. Due to the limited availability of high-quality reconstruction datasets with ground truth, we introduce two novel synthetic datasets to (pre-)train our network. Our approach is able to improve both the output depth maps and the reconstructed point cloud, for both learned and traditional depth estimation front-ends, on both synthetic and real data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2982446936", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/DonneG19", "doi": "10.1109/cvpr.2019.00782"}}, "content": {"source": {"pdf_hash": "f89499c199cb0abb8b62563b8437a0469939cc08", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4de589c93f23a70a3b83ba8f836389d422cb5271", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f89499c199cb0abb8b62563b8437a0469939cc08.txt", "contents": "\nLearning Non-volumetric Depth Fusion using Successive Reprojections\n\n\nSimon Donn\u00e9 simon.donne@tue.mpg.de \nAutonomous Vision Group MPI for Intelligent Systems\nUniversity of T\u00fcbingen\n\n\nAndreas Geiger andreas.geiger@tue.mpg.de \nAutonomous Vision Group MPI for Intelligent Systems\nUniversity of T\u00fcbingen\n\n\nLearning Non-volumetric Depth Fusion using Successive Reprojections\n10.1109/CVPR.2019.00782\nGiven a set of input views, multi-view stereopsis techniques estimate depth maps to represent the 3D reconstruction of the scene; these are fused into a single, consistent, reconstruction -most often a point cloud. In this work we propose to learn an auto-regressive depth refinement directly from data. While deep learning has improved the accuracy and speed of depth estimation significantly, learned MVS techniques remain limited to the planesweeping paradigm. We refine a set of input depth maps by successively reprojecting information from neighbouring views to leverage multiview constraints. Compared to learning-based volumetric fusion techniques, an image-based representation allows significantly more detailed reconstructions; compared to traditional point-based techniques, our method learns noise suppression and surface completion in a data-driven fashion. Due to the limited availability of high-quality reconstruction datasets with ground truth, we introduce two novel synthetic datasets to (pre-)train our network. Our approach is able to improve both the output depth maps and the reconstructed point cloud, for both learned and traditional depth estimation front-ends, on both synthetic and real data.\n\nIntroduction\n\nMulti-view stereopsis techniques constitute the current state-of-the-art in 3D point cloud reconstruction [21]. Given a set of images and camera matrices, MVS techniques estimate depth maps for all input views and subsequently merge them into a consistent 3D reconstruction. While the deep learning paradigm has led to drastic improvements in the depth estimation step itself, the existing learned MVS approaches [11,38] consist of plane-sweeping, followed by classical depth fusion approaches [7,30] (which mainly filter out invalid estimates). Instead, we propose to learn depth map fusion from data; by incorporating and fusing information from neighbouring views we refine the depth map estimate of a central view. We dub our approach DeFuSR: Depth Fusion Through Successive Reprojections.  Figure 1: The center view depth estimate is inaccurate around the center of the Buddha, even though the neighbouring view has a confident estimate for these areas. By reprojecting the neighbour's information onto the center image, we efficiently encode this information for the refinement network to resolve the uncertain areas. Iteratively performing this refinement further improves the estimates.\n\nIn volumetric space, learned approaches [2,16,17,29] that fuse depth information from multiple views have shown great promise but are inherently limited because of computation time and memory requirements. Working in the image domain bypasses these scaling issues [8,30,38], but existing image-based fusing techniques focus on filtering out bad estimates in the fusion step rather than improving them. However, neighbouring views often contain information that is missing in the current view, as illustrated in Figure 1. We show that there is still a significant margin for improvement of the depth estimates by auto-regressively incorporating information from neighbouring views.\n\nAs the absence of large-scale high-quality ground truth depth maps is a potential hurdle in training our model, we introduce two novel synthetic MVS datasets for pre-training. The first is similar to Flying Chairs [4] and Flying Things 3D [24]. To close the domain gap between this dataset and the DTU [14] dataset, we also use Unreal Engine to render the unrealDTU dataset, a drop-in replacement for DTU.\n\nTo summarize our contributions: we propose autoregressive learned depth fusion in the image domain, we create two synthetic MVS datasets for pretraining, we empirically motivate the design choices with ablation studies, and we compare our approach with state-of-the-art depth fusion baselines. Code and datasets are available via https://github.com/simon-donne/defusr/.\n\n\nRelated Work\n\nWe first discuss the estimation of the depth maps before we go on to discuss their fusion. We give only an overview of the most influential recent work in both areas; an exhaustive historical overview can be found in [6].\n\n\nMulti-view Depth Estimation\n\nTraditional MVS: Based on the popular PatchMatch algorithm for rectified stereomatching [1], Gipuma [8] and COLMAP [30] are popular state-of-the-art approaches [21,31]; they propagate best fitting planes to estimate per-pixel depth. While Gipuma selects neighbouring views on a perview basis, COLMAP does so per-pixel for better results.\n\nDeep Stereo: Initial learning-based depth estimation considered only the binocular task, using Siamese Networks to learn patch-based descriptors that are aggregated using winner-takes all [40] or global optimization [41]. By combining patch description, matching cost and cost volume processing in a single network, disparity estimation can be learned end-to-end [19,20,23]. Finally, Ummenhofer et al. [35] demonstrate a model which jointly predicts depth, camera motion, normals and optical flow from two views.\n\nDeep MVS: Hartmann et al. succesfully showed the generalization of two-view matching-based costs to multiple views [10]. While the end-to-end approaches for disparity estimation mentioned above were restricted to the binocular case, Leroy et al. [22], DeepMVS [11] and MVSNet [38] show that depth map prediction can benefit from multiple input views. Similarly, Xu et al. recently proposed AMH-MVS [36], a learning-based version of Gipuma [8]. Paschalidou et al. [27] exploit the combination of deep learning and Markov random fields for highly accurate depth maps, but are restricted to relatively low resolutions. All of these methods have focused on the depth estimation problem. However, we show that fusing and incorporating depth from multiple views is a viable avenue for improvements.\n\n\nDepth Map Fusion\n\nDepth-based stereopsis techniques are subsequently faced with the task of fusing a set of depth maps into a consistent reconstruction. This part, too, can be split up into volumetric and image-based fusion approaches. Intuitively, volumetric fusion can better leverage spatial information, but image-domain techniques such as ours are more efficient and lightweight, enabling higher output resolutions.\n\nVolumetric Fusion, initially proposed by Curless et al. [3], was made increasingly popular by Zach et al. [39] and Kinect-Fusion [26], fusing various depth maps into a single truncated signed distance field (TSDF). Leroy et al. have recently integrated this TSDF-based fusion into an end-to-end pipeline [22]. Fuhrmann et al. discuss how to handle the case of varying viewing distances and at the same time do away with the volumetric grid in favour of a point list [5] which scales better. Other non-learning-based techniques have been proposed to counter this scaling behaviour, such as a hybrid Delaunay-volumetric approach [25] and octrees [34]. The first does not lean itself well for learning-based approaches, but three concurrent works have leveraged hierarchical surface representations (i.e., octrees) to improve execution speed [9,29,32]. However, even such approaches have issues scaling beyond 512 3 voxels: eventually, they hit a computational ceiling. By working in the image domain, we largely sidestep the scaling issue and can additionally lean on the large amount of work and understanding available for image-based deep learning.\n\nImage-based Fusion promises quadratic rather than cubic scaling. Traditionally, it only discards reconstructed points that are not supported by multiple views. This is implemented in Gipuma as the Fusibile algorithm [8], and Xu et al. [36] use the same fusion technique in AMHMVS, their learning-based version of Gipuma. In COLMAP [30], the accepted pixels are clustered in \"consistent pixel clusters\" that are combined into a single reconstructed point cloud: clusters not supported by a minimum number of views are discarded. Similarly, Poggi et al. [28] and Tosi et al. [33] have leveraged deep learning to yield confidence estimates.\n\nWhile the former techniques filter out bad depth estimates, they do not attempt to improve the estimates. We argue that the depth maps can still be significantly improved by incorporating information from neighbouring views. To the best of our knowledge, learning-based refinement of depth maps was only done in a single-view setting [15,37].\n\nWe aim to learn depth map fusion and refinement from a variable number of input views -the zero-neighbour variant of our approach serves as the single-image baseline similar to [15] and [37]. Our combined approach notably improves the quality of the fused point clouds, quantified in terms of the Chamfer distance (see Section 5), and at the same time yields improved depth maps for all input views.  \n\n\nDatasets\n\nFor evaluation and training, we consider the DTU MVS dataset [14]. Unfortunately, DTU lacks perfect ground truth: a potential hurdle for the learning task. To tackle this, we have constructed two new synthetic datasets for pre-training; see the supplementary material for more details.\n\nThe first, as seen in Figure 2, is similar to Flying Chairs [4] and Flying Things 3D [24]. We create ten observations of a static scene rather than only two views of a non-rigid scene. Rendered with Blender, each scene consists of 10-20 ShapeNet objects randomly placed in front of a slanted background plane with an arbitrary image texture.\n\nSecondly, we also introduce a more realistic dataset to close the domain gap between the above dataset and real imagery. This second dataset is a drop-in replacement for the DTU dataset, rendered within Unreal Engine (see Figure 3), sporting perfect ground truth and more realistic rendering.\n\n\nMethod\n\nWe now outline the various aspects of our approach. We assume a set of depth map estimates as input, in this work from one of two front-ends: the traditional COLMAP [30] or the learning-based MVSNet [38]. The photometric depth map estimates from COLMAP are extremely noisy in poorly constrained areas (see Figure 2); although this would interfere with our reprojection step (see later), it proves to be straightforward to filter such areas out. We bootstrap our method by estimating the confidence for the input estimates -we discuss this in more detail at the end of this section.\n\n\nNetwork overview\n\nOur network is summarized in Figure 4. We first outline the entire process before discussing each aspect in detail. For an exhaustive listing, we refer to the supplementary.\n\nAs mentioned before, the depth fusion step happens entirely in the image domain. To encode the information from neighbouring views, we project their depth maps and image features (obtained from a image-local network) onto the center view. After pooling the information of all neighbours, we have a two-sided approach: one head to residually refine the input depth values (with limited spatial support), and another head that is intended to inpaint large unknown areas (with much larger spatial support). A third network weights the two options to yield the output estimate. A final network predicts the confidence of the refined estimate. We do not use any normalization layers in the refinement parts of the network, as absolute depth values need to be preserved.\n\n\nNeighbour reprojection\n\nConsider the set of N images I n (u) with corresponding camera matrices P n = K n [R n |t n ], and estimated depth values d n (u) for each input pixel u = [u, v, 1]\n\nT . The 3D point corresponding with a given pixel is then given by\nx n (u) = R T n K \u22121 n (d n (u)u \u2212 K n t n ) .(1)\nThose x n (u) for which the input confidence is larger than 0.5 are then projected onto the center view 0. Call u n\u2192m (u) = P m x n (u) the projection of x n (u) onto neighbour m, and z n\u2192m (u) = [0 0 1] u n\u2192m (u) its depth.  The z-buffer in view 0 based on neighbour n is then\nz n (u) = min u n \u2208\u03a9 n (u) z n\u21920 (u n ),(2)\nwhere \u03a9 n (u) = {u \u2208 \u03a9|P 0 x n (u n ) \u223c u} is the collection of pixels in view n that reproject onto u in view 0. We call u n (u) the pixel in view n for which z n (u) = z n\u2192m (u n (u)), i.e. the pixel responsible for the entry in the z-buffer. We now construct the reprojected image as I n (u) = I n (u n (u)).\n\n(\n\nNote the apparent similarity with Spatial Transformer Networks [13], at least as far as the projection of the image features goes; the geometry involved in the reprojection of the depth maps is not captured by a Spatial Transformer.\n\nThe reprojected depth and features have two major issues (as a result of the aliasing inherent in the reprojection, which is essentially a form of resampling). First of all, sharp depth edges in the neighbour view are often smeared over a large area in the center view, due to the difference in vantage points. While they do not constitute a evidence-supported surface, they do imply evidence-supported free space which contains information valuable to the network; we encode this into a lower bound image. Secondly, because of the aliasing in the reprojection step, background surfaces bleed through foreground surfaces. We now detail how to address both issues, as visualized in Figure 5.\n\nMinimum depth bound To encode the free space implied by a neighbour's depth estimate, we calculate a lower bound depth map b n (u) for the center view with respect to neighbour n as the lowest depth hypotheses along each pixel's ray that is no longer observed as free space by the neighbour image (see Figure 6). As far as the reference view is concerned, this is the lower bound on the depth of that pixel as implied by that neighbour's depth map estimate. In terms of the previous notations, we can express the lower bound g n (u) from neighbour n as\ng n (u) = min {d > 0 | d m (u 0\u2192n (u)) > z 0\u2192n (u)} . (4)\nCulling invisible surfaces We now cull invisible surfaces in the reprojected depth z n (u): any pixel significantly beyond the lower bound b n (u) is considered to be invisible to the center view and is discarded in the culled versionz n (u). The threshold we use here is 1/1000 th of the maximum depth in the scene (determined experimentally). Value of initial confidence estimation MVSNet yields a confidence for its estimates, but for COLMAP, we bootstrap our method with a confidence estimation for the input depth. Figure 7 illustrates the necessity of this confidence filtering: the bounds image and culled reprojection become noticeably cleaner. This confidence estimation is described below.\n\n\nNeighbour pooling\n\nWe perform three types of per-pixel pooling over the reprojected neighbour information. First of all, we calculate the mean and maximum depth bound and culled reprojected depth, as well as the average reprojected feature and the feature corresponding to the maximum culled reprojected depth. We also extract the culled reprojected depth that is closest to the center view estimate, and its feature. These are passed to the refinement and classification heads, along with the input depth estimate and image features.\n\n\nDepth refinement\n\nThe depth refinement step consists of two steps. In a first step, the center view depth estimate and features, as well as the pooled reprojected neighbour information (we will refer to these as the \"shared features\"), are processed by two networks: a local residual depth refinement module (a UNet block with depth 1 whose output is added to the input depth estimate) and a depth inpainting module (a UNet block with depth 5). Finally, a scoring network takes the output of the two other heads as input in addition to the shared features and outputs scores for both the residual refinement and inpainting alternatives. These are softmaxed and used to weight both outputs.\n\n\nConfidence classification\n\nFinally, the last part of the network takes both the shared features and the final depth estimate as input to yield a confidence classification of the output depth. This network is a UNet with depth 4, outputting a single channel on which a sigmoid is applied to acquire the final confidence prediction. Figure 7: The necessity of the initial confidence estimate for the depth map inputs. While the noisy input depth map (a) yields noticeable artifacts in the bounds image (b) and culled reprojected image (c), the confidence-filtered estimate (d) yields cleaner bounds (e) and culled reprojection (f). The final row shows the same steps after one iteration of refinement: the refined neighbour estimate (g), and its implied bounds (h) and reprojection (i).\n(a) (b) (c) (d) (e) (f) (g) (h) (i)\n\nTraining\n\nAs the feature reprojection is differentiable in terms of the features themselves (but not in terms of the depth values), we train the entire architecture end-to-end. However, training the different refinement/inpainting/head scoring networks is challenging and training the confidence network from the get-go causes it to degenerate into estimating zero confidence everywhere (which is correct in the initial epochs).\n\nTo mitigate this, we apply curriculum learning. In an initial phase, only the inpainting head is used. After a while, we enable the weighting network between the two heads, but keep the residual refinement disabled. Once the classification network makes valid choices between inpainted and input depth, we enable the residual refinement and the confidence classification: utilizing the entirety of our architecture.\n\nThe refined depth output is supervised with an L1 loss on both the depth values and their gradients. The confidence is supervised with a binary cross-entropy loss, where pixels are assumed to be trusted if they are within a given threshold of the ground truth value. The confidence loss is only backpropagated through the confidence classification block to prevent it from degenerating the depth estimate to make its own optimization easier; as a result we require no weighting between both losses as they affect distinct sets of weights. To unify the world scales of the different datasets, we scale all scenes such that the used depth range is roughly between 1 and 5. To reduce the sensitivity to this scale factor, as well as augment the training set, we scale each minibatch element by a random value between 0.5 and 2.0. After inference, we undo this scaling.\n\n\nSupervision\n\nOur approach requires ground truth depth maps for supervision. In the synthetic case, flawless ground truth depth is provided by the rendering engine. For the DTU dataset, however, ground truth consists of point clouds reconstructed by a structured light scanner [14]. Creating depth maps out of these point clouds, as before, faces the issue of bleedthrough as in Figure 5c. To address this issue, we perform Poisson surface reconstruction of the point cloud to yield a watertight mesh [18]; any point in the point cloud which projects behind this surface is rejected from the ground truth depth maps. While this method works well, it was not viable for use inside the network because of its relatively low speed. Finally, we also reject points on the surface of the white table -this cannot be reconstructed by photometric methods and our network should not be penalized for incorrect depth estimates here: instead, we supervise those areas with zero confidence. These issues are illustrated in Figure 8.\n\n\nExperimental Evaluation\n\nIn what follows we empirically show the benefit of our approach. To quantify performance, we recall the accuracy and completeness metrics from the DTU dataset [14]: accuracy(u, n) = min g x g \u2212 x n (u) 2 , and (5)\ncompleteness(g) = min u\u2208\u03a9,n x g \u2212 x n (u) 2 .(6)\nAccuracy indicates how close a reconstructed point lies to the ground truth, while completeness expresses how close a reference point lies to our reconstruction (for both metrics, lower is better); the Chamfer distance is defined as the algebraic mean of accuracy and completeness.\n\nWe are mostly interested in the percentage of points whose accuracy or completeness is lower than \u03c4 = 2.0mm, which we consider more indicative than the average accuracy or completeness metrics: it matters little whether an erroneous surface is 10 or 20 cm offset -yet this strongly affects the global averages. We report on both per-view, to quantify individual depth map quality, and for the final point cloud. More results, including results on the new synthetic datasets for pre-training and the absolute distances for DTU, are provided in the supplementary.\n\nTo create the fused point cloud for our network, we simultaneously thin the point clouds and remove outliers (similar to Fusibile). The initial point cloud from the output of our network is given by all pixels for which the predicted confidence is larger than a given threshold -the choice for this threshold is empirically decided below. For each point, we count the reconstructed points within a given threshold \u03c4 :\nc \u03c4 (u, n) = u 2 \u2208\u03a9,n 2 I( x n (u) \u2212 x n 2 (u 2 ) 2 < \u03c4),(7)\nwhere I(\u00b7) is the indicator function. x n (u) is accepted in the final cloud with probability I(c \u03c4 (u, n) > 1)/c \u03c4 /5 (u, n). Obvious outliers, with no points closer than \u03c4 , are rejected. For the other points, rejection probability is inversely proportional to the number of points closer than \u03c4 /5, mitigating the effect of non-uniform density of the reconstruction.\n\nAll evaluations are at a resolution of 480 \u00d7 270, for feasability and because MVSNet is also restricted to this.\n\n\nSelecting the confidence thresholds\n\nThe confidence classification of our approach is binary; whether or not a depth estimate lies within a given threshold \u03c4 d of the ground truth. Only depth estimates for which this predicted probability is above \u03c4 prob are considered for the final point cloud. Figure 9 illustrates that training the confidence classification for various \u03c4 d yield the same tradeoff between accuracy and completeness percentages. Based on these curves, we select \u03c4 d = 2 and \u03c4 prob = 0.5 for the evaluation to maximize the sums of both percentages. \n\n\nSelecting and leveraging neighbours\n\nWe consider three strategies for neighbour selection: selecting the closest views, selecting the most far-away views, and selecting a mix of both. We evaluate three separate networks with these strategies to perform refinement of COLMAP depth estimates with twelve neighbouring views. Table 1 shows the performance of these strategies, after three iterations of refinement, compared to the result of COLMAP's fusion. The mixed strategy proved to be the most efficient: while far-away views clearly contain valuable information, close-by neighbours should not be neglected.\n\nIn following experiments, we have always used the mixed strategy. While there is a practical limit (we have limited ourselves to twelve neighbours in this work), Table 2 shows that, as expected, using more neighbours leads to better results.\n\n\nRefining MVSNet estimates\n\nFinally, we evaluate the use of our network architecture for refining the MVSNet depth estimates. As shown in Table 3, our approach is not able to refine the MVSNet estimates much; while the per-view accuracy increases noticeably, the other metrics remain roughly the same.\n\nThe raw MVSNet estimates perform better than the COLMAP depth estimates. Refining the COLMAP estimates with our approach, however, significantly improves over the MVSNet results (refined or otherwise). We observe (e.g. Figure 10) that the COLMAP estimates are much more view-point dependent: surface patches observed by many neighbours are reconstructed more accurately than others. MVSNet, as a learned technique, was trained to optimize the L1 error and appears to smooth these errors out over the surface. Intuitively, the former case indeed allows for more improvements by our approach, by propagating these accurate areas to neighbouring views.    Figure 10 illustrates that performing more than one iteration is paramount to a good result but as the gains level out quickly, we have settled on three refinement steps.\n\n\nQualitative results\n\nNeighbouring views are crucial to filling in missing areas of individual views, as Figure 11 illustrates: here, the entire right side of the box is missing in the input estimate. Single-view refinement is not able to fill in this missing area and does not benefit from multiple refinement iterations. Refinement based on 12 neighbours, however, propagates information from other views and further improves the estimate over the next iteration, leveraging the now-improved information in those neighbours.\n\nHaving focused on structure throughout this work, rather than appearance, our reconstructed point clouds look more noisy due to lighting changes (see Figure 12), while COLMAP fusion averages colour over the input views.  Figure 10: Visualization of the depth map error over multiple iterations, for two elements of the DTU test set (darker blue is better). The first two iterations are the most significant, after that the improvements level out. Object elements not available in any view (the middle part of the buildings on the bottom right) cannot be recovered.  Figure 11: Output depth errors (middle) and confidence (bottom) for our approach (darker blue is better). Without leveraging neighbouring views, additional iterations yield little benefit. Neighbour information leads to better depth estimates and confidence, further improving over iterations.\n\nFinally, we also provided a qualitative comparison on a small handheld capture. Without retraining the network, we process the COLMAP depth estimates resulting from 12 images taken with a smartphone, both for a DTU-style object on a table and a less familiar setting of a large couch. While the confidence network trained on DTU has issues around depth discontinuities, Figure 13 shows that the surfaces are well reconstructed.\n\n\nConclusion and Future Work\n\nWe have introduced a novel learning-based approach to depth fusion, DeFuSR: by iteratively propagating information from neighbouring views, we refine the input depth maps. We have shown the importance of both neighbourhood information and successive refinement for this problem, resulting in significantly more accurate and complete perview and overall reconstructions.\n\n\nGT\n\nCOLMAP Ours Figure 12: Example reconstructed cloud for an element of the test set. Note the significant imperfections in the reference point cloud (left). A visual drawback of our approach is that we focus solely on structure; the fusion step used by COLMAP innately also averages out the difference in color appearance between various views. COLMAP Proposed Figure 13: Two scenes captured with a smartphone (12 images). Note that the surfaces are well estimated (the green pillow and the gnome's nose) and holes are completed correctly (the black pillow, the gnome's hat).\n\nWe mention two directions for future work. First of all, our training loss is the L1 loss, which is known to have a tendency towards smooth outputs; alternative loss functions, such as the PatchGAN [12] can help mitigate this. Secondly, we have selected neighbours on the image level. Ideally, the selection of neighbours would happen more finegrained, and integrated into the learning pipeline, e.g. in the form of attention networks.\n\nFigure 2 :\n2Example from our synthetic dataset: an input image (a) with the corresponding ground truth depth map (b). The depth map estimates from COLMAP (c) and MVSNet (d) show the issues in poorly constrained areas, usually because of occlusions and homogeneous areas. While MVSNet also returns a confidence estimate for its estimate, we bootstrap our method with single-view confidence estimation in the case of COLMAP inputs.\n\nFigure 3 :\n3Examples from our unrealDTU dataset: similar in set-up to DTU, we observe a series of objects on a table from a set of cameras scattered across one octant of a sphere.\n\nFigure 4 :Figure 5 :\n45Overview of our proposed fusion network. A difference in coloring represents a difference in vantage point, i.e. the information is represented in different image planes. As outlined in Section 4.1, neighbouring views are first reprojected, and then passed alongside the center depth estimate and the observed image. The output of the network is an improved version of the input depth of the reference view as well as a confidence map for this output. Example of the depth reprojection, bound calculation, and culling results. A neighbour image (a) is reprojected onto the center view (b), resulting in unfiltered reprojection (c). Calculating the lower bound as in Section 4.1 yields (d). Finally, we filter (c) with (d) as outlined in Section 4.1 to result in the culled depth map (e). Note in the crop-outs how the bound calculation completes depth edges, while the culling step removes bleed-through.\n\nFigure 6 :\n6Visualization of the lower bound calculation. For each pixel in the center view, we find the lowest depth value for which the unprojected point is no longer viewed as empty space by the neighbouring camera.\n\nFigure 8 :\n8The depth maps used for supervision on the DTU dataset. For a given input image (a), the dataset contains a reference point cloud (b) obtained with a structured light scanner. After creating a watertight mesh (c) from this point cloud and projecting that, we reject any point in the point cloud which gets projected behind this surface from the supervision depth map (d). Additionally, we reject the points of the white table.\n\nFigure 9 :\n9The percentage of points with accuracy respectively completeness below 2.0, over the DTU validation set, for varying values of \u03c4 d . The curves result from varying \u03c4 prob ; note that the evaluated options essentially lead to a continuation of the same curve. We select \u03c4 d = 2 and \u03c4 prob = 0.5 as the values that optimize the sum of both percentages.\n\nTable 1 :\n1Quantitative evaluation of the neighbour selection. Using twelve neighbours for three iterations of refinement, leveraging information from both close-by and far-away neighbours yields the best results, mostly improving completeness compared to the COLMAP fusion result.COLMAP nearest mixed furthest \n\nper-view \n\nacc. (%) \n66 \n91 \n89 \n86 \ncomp. (%) \n40 \n38 \n45 \n37 \nmean (%) \n52 \n64 \n67 \n62 \n\nfull \n\nacc. (%) \n73 \n83 \n80 \n74 \ncomp. (%) \n72 \n72 \n84 \n76 \nmean (%) \n72 \n78 \n82 \n75 \n\n\n\nTable 2 :\n2Quantitative evaluation of the number of neighbours. Using zero, four, or twelve neighbours for three iterations of refinement. As expected, more neighbours results in better performance, and too few neighbours performs worse than COLMAP's fusion approach (which essentially uses all 48).neighbours \nCOLMAP 0 \n4 12 \n\nper-view \n\nacc. (%) \n66 \n91 92 89 \ncomp. (%) \n40 \n28 31 45 \nmean (%) \n52 \n59 62 67 \n\nfull \n\nacc. (%) \n73 \n81 84 80 \ncomp. (%) \n72 \n66 64 84 \nmean (%) \n72 \n74 74 82 \n\n\n\nTable 3 :\n3Refining the MVSNet output depth estimates using three iterations of our approach. The per-view accuracy noticeably increases, while other metrics see a slight drop.MVSNet Refined (it. 3) \n\nper-view \n\nacc. (%) \n76 \n92 \ncomp. (%) \n35 \n34 \nmean (%) \n55 \n63 \n\nfull \n\nacc. (%) \n88 \n86 \ncomp. (%) \n66 \n65 \nmean (%) \n77 \n76 \n\n\n\nPatchmatch stereo -stereo matching with slanted support windows. M Bleyer, C Rhemann, C Rother, Proc. of the British Machine Vision Conf. (BMVC). of the British Machine Vision Conf. (BMVC)M. Bleyer, C. Rhemann, and C. Rother. Patchmatch stereo -stereo matching with slanted support windows. In Proc. of the British Machine Vision Conf. (BMVC), 2011. 2\n\n3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. C B Choy, D Xu, J Gwak, K Chen, S Savarese, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d- r2n2: A unified approach for single and multi-view 3d object reconstruction. In Proc. of the European Conf. on Computer Vision (ECCV), 2016. 1\n\nA volumetric method for building complex models from range images. B Curless, M Levoy, In ACM Trans. on Graphics. 2B. Curless and M. Levoy. A volumetric method for build- ing complex models from range images. In ACM Trans. on Graphics, 1996. 2\n\nLearning optical flow with convolutional networks. A Dosovitskiy, P Fischer, E Ilg, P Haeusser, C Hazirbas, V Golkov, P Smagt, D Cremers, T Brox, Flownet, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)23A. Dosovitskiy, P. Fischer, E. Ilg, P. Haeusser, C. Hazirbas, V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2015. 2, 3\n\nFloating scale surface reconstruction. S Fuhrmann, M Goesele, TG. 2S. Fuhrmann and M. Goesele. Floating scale surface recon- struction. TG, 2014. 2\n\nMulti-view stereo: A tutorial. Foundations and Trends in Computer Graphics and Vision. Y Furukawa, C Hern\u00e1ndez, Y. Furukawa, C. Hern\u00e1ndez, and al. Multi-view stereo: A tutorial. Foundations and Trends in Computer Graphics and Vision, 2015. 2\n\nMassively parallel multiview stereopsis by surface normal diffusion. S Galliani, K Lasinger, K Schindler, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)S. Galliani, K. Lasinger, and K. Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2015. 1\n\nGipuma: Massively parallel multi-view stereo reconstruction. Publikationen der Deutschen Gesellschaft f\u00fcr Photogrammetrie, Fernerkundung und Geoinformation e. S Galliani, K Lasinger, K Schindler, 25S. Galliani, K. Lasinger, and K. Schindler. Gipuma: Mas- sively parallel multi-view stereo reconstruction. Publikatio- nen der Deutschen Gesellschaft f\u00fcr Photogrammetrie, Fern- erkundung und Geoinformation e. V, 25:361-369, 2016. 1, 2\n\nHierarchical surface prediction for 3d object reconstruction. C H\u00e4ne, S Tulsiani, J Malik, arXiv.org, 1704.00710C. H\u00e4ne, S. Tulsiani, and J. Malik. Hierarchical surface pre- diction for 3d object reconstruction. arXiv.org, 1704.00710, 2017. 2\n\nLearned multi-patch similarity. W Hartmann, S Galliani, M Havlena, L Van Gool, K Schindler, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)W. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and K. Schindler. Learned multi-patch similarity. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2017. 2\n\nDeepmvs: Learning multi-view stereopsis. P Huang, K Matzen, J Kopf, N Ahuja, J Huang, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)P. Huang, K. Matzen, J. Kopf, N. Ahuja, and J. Huang. Deep- mvs: Learning multi-view stereopsis. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 2\n\nImage-to-image translation with conditional adversarial networks. P Isola, J Zhu, T Zhou, A A Efros, Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionP. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 8\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, K Kavukcuoglu, Advances in Neural Information Processing Systems (NIPS). M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. In Advances in Neural Information Processing Systems (NIPS), 2015. 4\n\nLarge scale multi-view stereopsis evaluation. R R Jensen, A L Dahl, G Vogiatzis, E Tola, H Aanaes, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)36R. R. Jensen, A. L. Dahl, G. Vogiatzis, E. Tola, and H. Aanaes. Large scale multi-view stereopsis evaluation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. 2, 3, 6\n\nReconstruction-based pairwise depth dataset for depth image enhancement using cnn. J Jeon, S Lee, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)J. Jeon and S. Lee. Reconstruction-based pairwise depth dataset for depth image enhancement using cnn. In Proc. of the European Conf. on Computer Vision (ECCV), 2018. 2\n\nSurfaceNet: an end-to-end 3d neural network for multiview stereopsis. M Ji, J Gall, H Zheng, Y Liu, L Fang, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang. SurfaceNet: an end-to-end 3d neural network for multiview stereopsis. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2017. 1\n\nLearning a multi-view stereo machine. A Kar, C H\u00e4ne, J Malik, Advances in Neural Information Processing Systems (NIPS). A. Kar, C. H\u00e4ne, and J. Malik. Learning a multi-view stereo machine. In Advances in Neural Information Processing Systems (NIPS), 2017. 1\n\nScreened poisson surface reconstruction. M M Kazhdan, H Hoppe, ACM Trans. on Graphics. 32329M. M. Kazhdan and H. Hoppe. Screened poisson surface reconstruction. ACM Trans. on Graphics, 32(3):29, 2013. 6\n\nEndto-end learning of geometry and context for deep stereo regression. A Kendall, H Martirosyan, S Dasgupta, P Henry, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)A. Kendall, H. Martirosyan, S. Dasgupta, and P. Henry. End- to-end learning of geometry and context for deep stereo regres- sion. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2017. 2\n\nStereonet: Guided hierarchical refinement for real. S Khamis, S Fanello, C Rhemann, A Kowdle, J Valentin, S Izadi, time edge-aware depth prediction. arXiv.orgS. Khamis, S. Fanello, C. Rhemann, A. Kowdle, J. Valentin, and S. Izadi. Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction. arXiv.org, 2018. 2\n\nTanks and temples: Benchmarking large-scale scene reconstruction. A Knapitsch, J Park, Q.-Y Zhou, V Koltun, ACM Trans. on Graphics. 364A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Trans. on Graphics, 36(4), 2017. 1, 2\n\nShape reconstruction using volume sweeping and learned photoconsistency. V Leroy, J.-S Franco, E Boyer, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)V. Leroy, J.-S. Franco, and E. Boyer. Shape reconstruction using volume sweeping and learned photoconsistency. In Proc. of the European Conf. on Computer Vision (ECCV), 2018. 2\n\nEfficient deep learning for stereo matching. W Luo, A Schwing, R Urtasun, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)W. Luo, A. Schwing, and R. Urtasun. Efficient deep learning for stereo matching. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 2\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. N Mayer, E Ilg, P Haeusser, P Fischer, D Cremers, A Dosovitskiy, T Brox, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)23N. Mayer, E. Ilg, P. Haeusser, P. Fischer, D. Cremers, A. Doso- vitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estima- tion. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 2, 3\n\nScalable surface reconstruction from point clouds with extreme scale and density diversity. C Mostegel, R Prettenthaler, F Fraundorfer, H Bischof, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)C. Mostegel, R. Prettenthaler, F. Fraundorfer, and H. Bischof. Scalable surface reconstruction from point clouds with ex- treme scale and density diversity. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nKinectfusion: Real-time dense surface mapping and tracking. R A Newcombe, S Izadi, O Hilliges, D Molyneaux, D Kim, A J Davison, P Kohli, J Shotton, S Hodges, A Fitzgibbon, Proc. of the International Symposium on Mixed and Augmented Reality (ISMAR). of the International Symposium on Mixed and Augmented Reality (ISMAR)R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface map- ping and tracking. In Proc. of the International Symposium on Mixed and Augmented Reality (ISMAR), 2011. 2\n\nRaynet: Learning volumetric 3d reconstruction with ray potentials. D Paschalidou, A O Ulusoy, C Schmitt, L Van Gool, A Geiger, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)D. Paschalidou, A. O. Ulusoy, C. Schmitt, L. van Gool, and A. Geiger. Raynet: Learning volumetric 3d reconstruction with ray potentials. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nLearning from scratch a confidence measure. M Poggi, S Mattoccia, BMVC. M. Poggi and S. Mattoccia. Learning from scratch a confi- dence measure. In BMVC, 2016. 2\n\nOct-NetFusion: Learning depth fusion from data. G Riegler, A O Ulusoy, H Bischof, A Geiger, Proc. of the International Conf. on 3D Vision (3DV. of the International Conf. on 3D Vision (3DVG. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. Oct- NetFusion: Learning depth fusion from data. In Proc. of the International Conf. on 3D Vision (3DV), 2017. 1, 2\n\nPixelwise view selection for unstructured multi-view stereo. J L Sch\u00f6nberger, E Zheng, M Pollefeys, J.-M Frahm, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)23J. L. Sch\u00f6nberger, E. Zheng, M. Pollefeys, and J.-M. Frahm. Pixelwise view selection for unstructured multi-view stereo. In Proc. of the European Conf. on Computer Vision (ECCV), 2016. 1, 2, 3\n\nA multi-view stereo benchmark with high-resolution images and multi-camera videos. T Sch\u00f6ps, J Sch\u00f6nberger, S Galliani, T Sattler, K Schindler, M Pollefeys, A Geiger, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)T. Sch\u00f6ps, J. Sch\u00f6nberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys, and A. Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nOctree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. M Tatarchenko, A Dosovitskiy, T Brox, Proc. of the IEEE International Conf. on Computer Vision (ICCV. of the IEEE International Conf. on Computer Vision (ICCVM. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen- erating networks: Efficient convolutional architectures for high-resolution 3d outputs. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2017. 2\n\nBeyond local reasoning for stereo confidence estimation with deep learning. F Tosi, M Poggi, A Benincasa, S Mattoccia, In ECCV. 2F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia. Beyond local reasoning for stereo confidence estimation with deep learning. In ECCV, 2018. 2\n\nGlobal, dense multiscale reconstruction for a billion points. B Ummenhofer, T Brox, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)B. Ummenhofer and T. Brox. Global, dense multiscale recon- struction for a billion points. In Proc. of the IEEE Interna- tional Conf. on Computer Vision (ICCV), 2015. 2\n\nDemon: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Doso- vitskiy, and T. Brox. Demon: Depth and motion network for learning monocular stereo. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 2\n\nMulti-view stereo with asymmetric checkerboard propagation and multi-hypothesis joint view selection. Q Xu, W Tao, arXiv.org. 2Q. Xu and W. Tao. Multi-view stereo with asymmetric checkerboard propagation and multi-hypothesis joint view selection. arXiv.org, 2018. 2\n\nDdrnet: Depth map denoising and refinement for consumer depth cameras using cascaded cnns. S Yan, C Wu, L Wang, F Xu, L An, K Guo, Y Liu, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)S. Yan, C. Wu, L. Wang, F. Xu, L. An, K. Guo, and Y. Liu. Ddrnet: Depth map denoising and refinement for consumer depth cameras using cascaded cnns. In Proc. of the European Conf. on Computer Vision (ECCV), 2018. 2\n\nMvsnet: Depth inference for unstructured multi-view stereo. Y Yao, Z Luo, S Li, T Fang, L Quan, arXiv.org, abs/1804.0250513Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan. Mvsnet: Depth inference for unstructured multi-view stereo. arXiv.org, abs/1804.02505, 2018. 1, 2, 3\n\nA globally optimal algorithm for robust tv-l1 range image integration. C Zach, T Pock, H Bischof, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)C. Zach, T. Pock, and H. Bischof. A globally optimal algo- rithm for robust tv-l1 range image integration. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2007. 2\n\nLearning to compare image patches via convolutional neural networks. S Zagoruyko, N Komodakis, S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. 2015. 2\n\nStereo matching by training a convolutional neural network to compare image patches. J \u017dbontar, Y Lecun, Journal of Machine Learning Research. 1765JMLRJ.\u017dbontar and Y. LeCun. Stereo matching by training a con- volutional neural network to compare image patches. Journal of Machine Learning Research (JMLR), 17(65):1-32, 2016. 2\n", "annotations": {"author": "[{\"end\":183,\"start\":71},{\"end\":302,\"start\":184}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":77},{\"end\":198,\"start\":192}]", "author_first_name": "[{\"end\":76,\"start\":71},{\"end\":191,\"start\":184}]", "author_affiliation": "[{\"end\":182,\"start\":107},{\"end\":301,\"start\":226}]", "title": "[{\"end\":68,\"start\":1},{\"end\":370,\"start\":303}]", "venue": null, "abstract": "[{\"end\":1616,\"start\":395}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1742,\"start\":1738},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2049,\"start\":2045},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2052,\"start\":2049},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2129,\"start\":2126},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2132,\"start\":2129},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2871,\"start\":2868},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2874,\"start\":2871},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2877,\"start\":2874},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2879,\"start\":2877},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3095,\"start\":3092},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3098,\"start\":3095},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3101,\"start\":3098},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3727,\"start\":3724},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3753,\"start\":3749},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3816,\"start\":3812},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4523,\"start\":4520},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4647,\"start\":4644},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4659,\"start\":4656},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4675,\"start\":4671},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4720,\"start\":4716},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4723,\"start\":4720},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5087,\"start\":5083},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5115,\"start\":5111},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5262,\"start\":5258},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5265,\"start\":5262},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5268,\"start\":5265},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5301,\"start\":5297},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5528,\"start\":5524},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5659,\"start\":5655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5673,\"start\":5669},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5689,\"start\":5685},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5811,\"start\":5807},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5851,\"start\":5848},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5876,\"start\":5872},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6685,\"start\":6682},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6736,\"start\":6732},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6759,\"start\":6755},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6934,\"start\":6930},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7095,\"start\":7092},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7257,\"start\":7253},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7274,\"start\":7270},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7468,\"start\":7465},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7471,\"start\":7468},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7474,\"start\":7471},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7996,\"start\":7993},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8016,\"start\":8012},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8112,\"start\":8108},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8333,\"start\":8329},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8354,\"start\":8350},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8754,\"start\":8750},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8757,\"start\":8754},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8941,\"start\":8937},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8950,\"start\":8946},{\"end\":9089,\"start\":9079},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9239,\"start\":9235},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9524,\"start\":9521},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9550,\"start\":9546},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10276,\"start\":10272},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10310,\"start\":10306},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12663,\"start\":12659},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18881,\"start\":18877},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19105,\"start\":19101},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19811,\"start\":19807},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27275,\"start\":27271}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":27939,\"start\":27509},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28120,\"start\":27940},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29049,\"start\":28121},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29269,\"start\":29050},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29709,\"start\":29270},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30073,\"start\":29710},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30566,\"start\":30074},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31062,\"start\":30567},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31395,\"start\":31063}]", "paragraph": "[{\"end\":2826,\"start\":1632},{\"end\":3508,\"start\":2828},{\"end\":3915,\"start\":3510},{\"end\":4286,\"start\":3917},{\"end\":4524,\"start\":4303},{\"end\":4893,\"start\":4556},{\"end\":5407,\"start\":4895},{\"end\":6201,\"start\":5409},{\"end\":6624,\"start\":6222},{\"end\":7775,\"start\":6626},{\"end\":8414,\"start\":7777},{\"end\":8758,\"start\":8416},{\"end\":9161,\"start\":8760},{\"end\":9459,\"start\":9174},{\"end\":9802,\"start\":9461},{\"end\":10096,\"start\":9804},{\"end\":10688,\"start\":10107},{\"end\":10882,\"start\":10709},{\"end\":11648,\"start\":10884},{\"end\":11839,\"start\":11675},{\"end\":11907,\"start\":11841},{\"end\":12235,\"start\":11958},{\"end\":12591,\"start\":12280},{\"end\":12594,\"start\":12593},{\"end\":12828,\"start\":12596},{\"end\":13520,\"start\":12830},{\"end\":14074,\"start\":13522},{\"end\":14832,\"start\":14133},{\"end\":15369,\"start\":14854},{\"end\":16061,\"start\":15390},{\"end\":16848,\"start\":16091},{\"end\":17314,\"start\":16896},{\"end\":17731,\"start\":17316},{\"end\":18598,\"start\":17733},{\"end\":19620,\"start\":18614},{\"end\":19861,\"start\":19648},{\"end\":20192,\"start\":19911},{\"end\":20755,\"start\":20194},{\"end\":21174,\"start\":20757},{\"end\":21605,\"start\":21236},{\"end\":21719,\"start\":21607},{\"end\":22290,\"start\":21759},{\"end\":22902,\"start\":22330},{\"end\":23145,\"start\":22904},{\"end\":23448,\"start\":23175},{\"end\":24273,\"start\":23450},{\"end\":24801,\"start\":24297},{\"end\":25662,\"start\":24803},{\"end\":26091,\"start\":25664},{\"end\":26491,\"start\":26122},{\"end\":27071,\"start\":26498},{\"end\":27508,\"start\":27073}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11957,\"start\":11908},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12279,\"start\":12236},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14132,\"start\":14075},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16884,\"start\":16849},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19910,\"start\":19862},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21235,\"start\":21175}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22622,\"start\":22615},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23073,\"start\":23066},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23292,\"start\":23285}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1630,\"start\":1618},{\"attributes\":{\"n\":\"2.\"},\"end\":4301,\"start\":4289},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4554,\"start\":4527},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6220,\"start\":6204},{\"attributes\":{\"n\":\"3.\"},\"end\":9172,\"start\":9164},{\"attributes\":{\"n\":\"4.\"},\"end\":10105,\"start\":10099},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10707,\"start\":10691},{\"end\":11673,\"start\":11651},{\"end\":14852,\"start\":14835},{\"end\":15388,\"start\":15372},{\"end\":16089,\"start\":16064},{\"end\":16894,\"start\":16886},{\"end\":18612,\"start\":18601},{\"attributes\":{\"n\":\"5.\"},\"end\":19646,\"start\":19623},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21757,\"start\":21722},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22328,\"start\":22293},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23173,\"start\":23148},{\"attributes\":{\"n\":\"5.4.\"},\"end\":24295,\"start\":24276},{\"attributes\":{\"n\":\"6.\"},\"end\":26120,\"start\":26094},{\"end\":26496,\"start\":26494},{\"end\":27520,\"start\":27510},{\"end\":27951,\"start\":27941},{\"end\":28142,\"start\":28122},{\"end\":29061,\"start\":29051},{\"end\":29281,\"start\":29271},{\"end\":29721,\"start\":29711},{\"end\":30084,\"start\":30075},{\"end\":30577,\"start\":30568},{\"end\":31073,\"start\":31064}]", "table": "[{\"end\":30566,\"start\":30356},{\"end\":31062,\"start\":30867},{\"end\":31395,\"start\":31240}]", "figure_caption": "[{\"end\":27939,\"start\":27522},{\"end\":28120,\"start\":27953},{\"end\":29049,\"start\":28145},{\"end\":29269,\"start\":29063},{\"end\":29709,\"start\":29283},{\"end\":30073,\"start\":29723},{\"end\":30356,\"start\":30086},{\"end\":30867,\"start\":30579},{\"end\":31240,\"start\":31075}]", "figure_ref": "[{\"end\":2435,\"start\":2427},{\"end\":3347,\"start\":3339},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9491,\"start\":9483},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10034,\"start\":10026},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10421,\"start\":10413},{\"end\":10746,\"start\":10738},{\"end\":13519,\"start\":13511},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13832,\"start\":13824},{\"end\":14661,\"start\":14653},{\"end\":16403,\"start\":16395},{\"end\":18988,\"start\":18979},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19619,\"start\":19611},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22027,\"start\":22019},{\"end\":23678,\"start\":23669},{\"end\":24112,\"start\":24103},{\"end\":24389,\"start\":24380},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24962,\"start\":24953},{\"end\":25033,\"start\":25024},{\"end\":25378,\"start\":25369},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26043,\"start\":26034},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26519,\"start\":26510},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26866,\"start\":26857}]", "bib_author_first_name": "[{\"end\":31463,\"start\":31462},{\"end\":31473,\"start\":31472},{\"end\":31484,\"start\":31483},{\"end\":31831,\"start\":31830},{\"end\":31833,\"start\":31832},{\"end\":31841,\"start\":31840},{\"end\":31847,\"start\":31846},{\"end\":31855,\"start\":31854},{\"end\":31863,\"start\":31862},{\"end\":32246,\"start\":32245},{\"end\":32257,\"start\":32256},{\"end\":32475,\"start\":32474},{\"end\":32490,\"start\":32489},{\"end\":32501,\"start\":32500},{\"end\":32508,\"start\":32507},{\"end\":32520,\"start\":32519},{\"end\":32532,\"start\":32531},{\"end\":32542,\"start\":32541},{\"end\":32551,\"start\":32550},{\"end\":32562,\"start\":32561},{\"end\":32995,\"start\":32994},{\"end\":33007,\"start\":33006},{\"end\":33192,\"start\":33191},{\"end\":33204,\"start\":33203},{\"end\":33417,\"start\":33416},{\"end\":33429,\"start\":33428},{\"end\":33441,\"start\":33440},{\"end\":33925,\"start\":33924},{\"end\":33937,\"start\":33936},{\"end\":33949,\"start\":33948},{\"end\":34262,\"start\":34261},{\"end\":34270,\"start\":34269},{\"end\":34282,\"start\":34281},{\"end\":34476,\"start\":34475},{\"end\":34488,\"start\":34487},{\"end\":34500,\"start\":34499},{\"end\":34511,\"start\":34510},{\"end\":34523,\"start\":34522},{\"end\":34877,\"start\":34876},{\"end\":34886,\"start\":34885},{\"end\":34896,\"start\":34895},{\"end\":34904,\"start\":34903},{\"end\":34913,\"start\":34912},{\"end\":35296,\"start\":35295},{\"end\":35305,\"start\":35304},{\"end\":35312,\"start\":35311},{\"end\":35320,\"start\":35319},{\"end\":35322,\"start\":35321},{\"end\":35665,\"start\":35664},{\"end\":35678,\"start\":35677},{\"end\":35690,\"start\":35689},{\"end\":35703,\"start\":35702},{\"end\":35983,\"start\":35982},{\"end\":35985,\"start\":35984},{\"end\":35995,\"start\":35994},{\"end\":35997,\"start\":35996},{\"end\":36005,\"start\":36004},{\"end\":36018,\"start\":36017},{\"end\":36026,\"start\":36025},{\"end\":36445,\"start\":36444},{\"end\":36453,\"start\":36452},{\"end\":36802,\"start\":36801},{\"end\":36808,\"start\":36807},{\"end\":36816,\"start\":36815},{\"end\":36825,\"start\":36824},{\"end\":36832,\"start\":36831},{\"end\":37194,\"start\":37193},{\"end\":37201,\"start\":37200},{\"end\":37209,\"start\":37208},{\"end\":37456,\"start\":37455},{\"end\":37458,\"start\":37457},{\"end\":37469,\"start\":37468},{\"end\":37690,\"start\":37689},{\"end\":37701,\"start\":37700},{\"end\":37716,\"start\":37715},{\"end\":37728,\"start\":37727},{\"end\":38118,\"start\":38117},{\"end\":38128,\"start\":38127},{\"end\":38139,\"start\":38138},{\"end\":38150,\"start\":38149},{\"end\":38160,\"start\":38159},{\"end\":38172,\"start\":38171},{\"end\":38468,\"start\":38467},{\"end\":38481,\"start\":38480},{\"end\":38492,\"start\":38488},{\"end\":38500,\"start\":38499},{\"end\":38769,\"start\":38768},{\"end\":38781,\"start\":38777},{\"end\":38791,\"start\":38790},{\"end\":39125,\"start\":39124},{\"end\":39132,\"start\":39131},{\"end\":39143,\"start\":39142},{\"end\":39547,\"start\":39546},{\"end\":39556,\"start\":39555},{\"end\":39563,\"start\":39562},{\"end\":39575,\"start\":39574},{\"end\":39586,\"start\":39585},{\"end\":39597,\"start\":39596},{\"end\":39612,\"start\":39611},{\"end\":40117,\"start\":40116},{\"end\":40129,\"start\":40128},{\"end\":40146,\"start\":40145},{\"end\":40161,\"start\":40160},{\"end\":40597,\"start\":40596},{\"end\":40599,\"start\":40598},{\"end\":40611,\"start\":40610},{\"end\":40620,\"start\":40619},{\"end\":40632,\"start\":40631},{\"end\":40645,\"start\":40644},{\"end\":40652,\"start\":40651},{\"end\":40654,\"start\":40653},{\"end\":40665,\"start\":40664},{\"end\":40674,\"start\":40673},{\"end\":40685,\"start\":40684},{\"end\":40695,\"start\":40694},{\"end\":41201,\"start\":41200},{\"end\":41216,\"start\":41215},{\"end\":41218,\"start\":41217},{\"end\":41228,\"start\":41227},{\"end\":41239,\"start\":41238},{\"end\":41251,\"start\":41250},{\"end\":41650,\"start\":41649},{\"end\":41659,\"start\":41658},{\"end\":41817,\"start\":41816},{\"end\":41828,\"start\":41827},{\"end\":41830,\"start\":41829},{\"end\":41840,\"start\":41839},{\"end\":41851,\"start\":41850},{\"end\":42188,\"start\":42187},{\"end\":42190,\"start\":42189},{\"end\":42205,\"start\":42204},{\"end\":42214,\"start\":42213},{\"end\":42230,\"start\":42226},{\"end\":42620,\"start\":42619},{\"end\":42630,\"start\":42629},{\"end\":42645,\"start\":42644},{\"end\":42657,\"start\":42656},{\"end\":42668,\"start\":42667},{\"end\":42681,\"start\":42680},{\"end\":42694,\"start\":42693},{\"end\":43188,\"start\":43187},{\"end\":43203,\"start\":43202},{\"end\":43218,\"start\":43217},{\"end\":43644,\"start\":43643},{\"end\":43652,\"start\":43651},{\"end\":43661,\"start\":43660},{\"end\":43674,\"start\":43673},{\"end\":43904,\"start\":43903},{\"end\":43918,\"start\":43917},{\"end\":44281,\"start\":44280},{\"end\":44295,\"start\":44294},{\"end\":44303,\"start\":44302},{\"end\":44312,\"start\":44311},{\"end\":44321,\"start\":44320},{\"end\":44328,\"start\":44327},{\"end\":44343,\"start\":44342},{\"end\":44807,\"start\":44806},{\"end\":44813,\"start\":44812},{\"end\":45063,\"start\":45062},{\"end\":45070,\"start\":45069},{\"end\":45076,\"start\":45075},{\"end\":45084,\"start\":45083},{\"end\":45090,\"start\":45089},{\"end\":45096,\"start\":45095},{\"end\":45103,\"start\":45102},{\"end\":45488,\"start\":45487},{\"end\":45495,\"start\":45494},{\"end\":45502,\"start\":45501},{\"end\":45508,\"start\":45507},{\"end\":45516,\"start\":45515},{\"end\":45769,\"start\":45768},{\"end\":45777,\"start\":45776},{\"end\":45785,\"start\":45784},{\"end\":46171,\"start\":46170},{\"end\":46184,\"start\":46183},{\"end\":46391,\"start\":46390},{\"end\":46402,\"start\":46401}]", "bib_author_last_name": "[{\"end\":31470,\"start\":31464},{\"end\":31481,\"start\":31474},{\"end\":31491,\"start\":31485},{\"end\":31838,\"start\":31834},{\"end\":31844,\"start\":31842},{\"end\":31852,\"start\":31848},{\"end\":31860,\"start\":31856},{\"end\":31872,\"start\":31864},{\"end\":32254,\"start\":32247},{\"end\":32263,\"start\":32258},{\"end\":32487,\"start\":32476},{\"end\":32498,\"start\":32491},{\"end\":32505,\"start\":32502},{\"end\":32517,\"start\":32509},{\"end\":32529,\"start\":32521},{\"end\":32539,\"start\":32533},{\"end\":32548,\"start\":32543},{\"end\":32559,\"start\":32552},{\"end\":32567,\"start\":32563},{\"end\":32576,\"start\":32569},{\"end\":33004,\"start\":32996},{\"end\":33015,\"start\":33008},{\"end\":33201,\"start\":33193},{\"end\":33214,\"start\":33205},{\"end\":33426,\"start\":33418},{\"end\":33438,\"start\":33430},{\"end\":33451,\"start\":33442},{\"end\":33934,\"start\":33926},{\"end\":33946,\"start\":33938},{\"end\":33959,\"start\":33950},{\"end\":34267,\"start\":34263},{\"end\":34279,\"start\":34271},{\"end\":34288,\"start\":34283},{\"end\":34485,\"start\":34477},{\"end\":34497,\"start\":34489},{\"end\":34508,\"start\":34501},{\"end\":34520,\"start\":34512},{\"end\":34533,\"start\":34524},{\"end\":34883,\"start\":34878},{\"end\":34893,\"start\":34887},{\"end\":34901,\"start\":34897},{\"end\":34910,\"start\":34905},{\"end\":34919,\"start\":34914},{\"end\":35302,\"start\":35297},{\"end\":35309,\"start\":35306},{\"end\":35317,\"start\":35313},{\"end\":35328,\"start\":35323},{\"end\":35675,\"start\":35666},{\"end\":35687,\"start\":35679},{\"end\":35700,\"start\":35691},{\"end\":35715,\"start\":35704},{\"end\":35992,\"start\":35986},{\"end\":36002,\"start\":35998},{\"end\":36015,\"start\":36006},{\"end\":36023,\"start\":36019},{\"end\":36033,\"start\":36027},{\"end\":36450,\"start\":36446},{\"end\":36457,\"start\":36454},{\"end\":36805,\"start\":36803},{\"end\":36813,\"start\":36809},{\"end\":36822,\"start\":36817},{\"end\":36829,\"start\":36826},{\"end\":36837,\"start\":36833},{\"end\":37198,\"start\":37195},{\"end\":37206,\"start\":37202},{\"end\":37215,\"start\":37210},{\"end\":37466,\"start\":37459},{\"end\":37475,\"start\":37470},{\"end\":37698,\"start\":37691},{\"end\":37713,\"start\":37702},{\"end\":37725,\"start\":37717},{\"end\":37734,\"start\":37729},{\"end\":38125,\"start\":38119},{\"end\":38136,\"start\":38129},{\"end\":38147,\"start\":38140},{\"end\":38157,\"start\":38151},{\"end\":38169,\"start\":38161},{\"end\":38178,\"start\":38173},{\"end\":38478,\"start\":38469},{\"end\":38486,\"start\":38482},{\"end\":38497,\"start\":38493},{\"end\":38507,\"start\":38501},{\"end\":38775,\"start\":38770},{\"end\":38788,\"start\":38782},{\"end\":38797,\"start\":38792},{\"end\":39129,\"start\":39126},{\"end\":39140,\"start\":39133},{\"end\":39151,\"start\":39144},{\"end\":39553,\"start\":39548},{\"end\":39560,\"start\":39557},{\"end\":39572,\"start\":39564},{\"end\":39583,\"start\":39576},{\"end\":39594,\"start\":39587},{\"end\":39609,\"start\":39598},{\"end\":39617,\"start\":39613},{\"end\":40126,\"start\":40118},{\"end\":40143,\"start\":40130},{\"end\":40158,\"start\":40147},{\"end\":40169,\"start\":40162},{\"end\":40608,\"start\":40600},{\"end\":40617,\"start\":40612},{\"end\":40629,\"start\":40621},{\"end\":40642,\"start\":40633},{\"end\":40649,\"start\":40646},{\"end\":40662,\"start\":40655},{\"end\":40671,\"start\":40666},{\"end\":40682,\"start\":40675},{\"end\":40692,\"start\":40686},{\"end\":40706,\"start\":40696},{\"end\":41213,\"start\":41202},{\"end\":41225,\"start\":41219},{\"end\":41236,\"start\":41229},{\"end\":41248,\"start\":41240},{\"end\":41258,\"start\":41252},{\"end\":41656,\"start\":41651},{\"end\":41669,\"start\":41660},{\"end\":41825,\"start\":41818},{\"end\":41837,\"start\":41831},{\"end\":41848,\"start\":41841},{\"end\":41858,\"start\":41852},{\"end\":42202,\"start\":42191},{\"end\":42211,\"start\":42206},{\"end\":42224,\"start\":42215},{\"end\":42236,\"start\":42231},{\"end\":42627,\"start\":42621},{\"end\":42642,\"start\":42631},{\"end\":42654,\"start\":42646},{\"end\":42665,\"start\":42658},{\"end\":42678,\"start\":42669},{\"end\":42691,\"start\":42682},{\"end\":42701,\"start\":42695},{\"end\":43200,\"start\":43189},{\"end\":43215,\"start\":43204},{\"end\":43223,\"start\":43219},{\"end\":43649,\"start\":43645},{\"end\":43658,\"start\":43653},{\"end\":43671,\"start\":43662},{\"end\":43684,\"start\":43675},{\"end\":43915,\"start\":43905},{\"end\":43923,\"start\":43919},{\"end\":44292,\"start\":44282},{\"end\":44300,\"start\":44296},{\"end\":44309,\"start\":44304},{\"end\":44318,\"start\":44313},{\"end\":44325,\"start\":44322},{\"end\":44340,\"start\":44329},{\"end\":44348,\"start\":44344},{\"end\":44810,\"start\":44808},{\"end\":44817,\"start\":44814},{\"end\":45067,\"start\":45064},{\"end\":45073,\"start\":45071},{\"end\":45081,\"start\":45077},{\"end\":45087,\"start\":45085},{\"end\":45093,\"start\":45091},{\"end\":45100,\"start\":45097},{\"end\":45107,\"start\":45104},{\"end\":45492,\"start\":45489},{\"end\":45499,\"start\":45496},{\"end\":45505,\"start\":45503},{\"end\":45513,\"start\":45509},{\"end\":45521,\"start\":45517},{\"end\":45774,\"start\":45770},{\"end\":45782,\"start\":45778},{\"end\":45793,\"start\":45786},{\"end\":46181,\"start\":46172},{\"end\":46194,\"start\":46185},{\"end\":46399,\"start\":46392},{\"end\":46408,\"start\":46403}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1798946},\"end\":31748,\"start\":31397},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6325059},\"end\":32176,\"start\":31750},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12358833},\"end\":32421,\"start\":32178},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12552176},\"end\":32953,\"start\":32423},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3348029},\"end\":33102,\"start\":32955},{\"attributes\":{\"id\":\"b5\"},\"end\":33345,\"start\":33104},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9067666},\"end\":33763,\"start\":33347},{\"attributes\":{\"id\":\"b7\"},\"end\":34197,\"start\":33765},{\"attributes\":{\"doi\":\"arXiv.org, 1704.00710\",\"id\":\"b8\"},\"end\":34441,\"start\":34199},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9353335},\"end\":34833,\"start\":34443},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4559916},\"end\":35227,\"start\":34835},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6200260},\"end\":35632,\"start\":35229},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6099034},\"end\":35934,\"start\":35634},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18412989},\"end\":36359,\"start\":35936},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52953885},\"end\":36729,\"start\":36361},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9035204},\"end\":37153,\"start\":36731},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":19285959},\"end\":37412,\"start\":37155},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1371704},\"end\":37616,\"start\":37414},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2658860},\"end\":38063,\"start\":37618},{\"attributes\":{\"id\":\"b19\"},\"end\":38399,\"start\":38065},{\"attributes\":{\"id\":\"b20\"},\"end\":38693,\"start\":38401},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52958567},\"end\":39077,\"start\":38695},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10845625},\"end\":39440,\"start\":39079},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":206594275},\"end\":40022,\"start\":39442},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6423226},\"end\":40534,\"start\":40024},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11830123},\"end\":41131,\"start\":40536},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":43992332},\"end\":41603,\"start\":41133},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6372947},\"end\":41766,\"start\":41605},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17104434},\"end\":42124,\"start\":41768},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":977535},\"end\":42534,\"start\":42126},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":20603040},\"end\":43087,\"start\":42536},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":60945},\"end\":43565,\"start\":43089},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52955180},\"end\":43839,\"start\":43567},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5570837},\"end\":44215,\"start\":43841},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6159584},\"end\":44702,\"start\":44217},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":29153470},\"end\":44969,\"start\":44704},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52954401},\"end\":45425,\"start\":44971},{\"attributes\":{\"doi\":\"arXiv.org, abs/1804.02505\",\"id\":\"b37\"},\"end\":45695,\"start\":45427},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2428913},\"end\":46099,\"start\":45697},{\"attributes\":{\"id\":\"b39\"},\"end\":46303,\"start\":46101},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6913648},\"end\":46632,\"start\":46305}]", "bib_title": "[{\"end\":31460,\"start\":31397},{\"end\":31828,\"start\":31750},{\"end\":32243,\"start\":32178},{\"end\":32472,\"start\":32423},{\"end\":32992,\"start\":32955},{\"end\":33414,\"start\":33347},{\"end\":34473,\"start\":34443},{\"end\":34874,\"start\":34835},{\"end\":35293,\"start\":35229},{\"end\":35662,\"start\":35634},{\"end\":35980,\"start\":35936},{\"end\":36442,\"start\":36361},{\"end\":36799,\"start\":36731},{\"end\":37191,\"start\":37155},{\"end\":37453,\"start\":37414},{\"end\":37687,\"start\":37618},{\"end\":38465,\"start\":38401},{\"end\":38766,\"start\":38695},{\"end\":39122,\"start\":39079},{\"end\":39544,\"start\":39442},{\"end\":40114,\"start\":40024},{\"end\":40594,\"start\":40536},{\"end\":41198,\"start\":41133},{\"end\":41647,\"start\":41605},{\"end\":41814,\"start\":41768},{\"end\":42185,\"start\":42126},{\"end\":42617,\"start\":42536},{\"end\":43185,\"start\":43089},{\"end\":43641,\"start\":43567},{\"end\":43901,\"start\":43841},{\"end\":44278,\"start\":44217},{\"end\":44804,\"start\":44704},{\"end\":45060,\"start\":44971},{\"end\":45766,\"start\":45697},{\"end\":46388,\"start\":46305}]", "bib_author": "[{\"end\":31472,\"start\":31462},{\"end\":31483,\"start\":31472},{\"end\":31493,\"start\":31483},{\"end\":31840,\"start\":31830},{\"end\":31846,\"start\":31840},{\"end\":31854,\"start\":31846},{\"end\":31862,\"start\":31854},{\"end\":31874,\"start\":31862},{\"end\":32256,\"start\":32245},{\"end\":32265,\"start\":32256},{\"end\":32489,\"start\":32474},{\"end\":32500,\"start\":32489},{\"end\":32507,\"start\":32500},{\"end\":32519,\"start\":32507},{\"end\":32531,\"start\":32519},{\"end\":32541,\"start\":32531},{\"end\":32550,\"start\":32541},{\"end\":32561,\"start\":32550},{\"end\":32569,\"start\":32561},{\"end\":32578,\"start\":32569},{\"end\":33006,\"start\":32994},{\"end\":33017,\"start\":33006},{\"end\":33203,\"start\":33191},{\"end\":33216,\"start\":33203},{\"end\":33428,\"start\":33416},{\"end\":33440,\"start\":33428},{\"end\":33453,\"start\":33440},{\"end\":33936,\"start\":33924},{\"end\":33948,\"start\":33936},{\"end\":33961,\"start\":33948},{\"end\":34269,\"start\":34261},{\"end\":34281,\"start\":34269},{\"end\":34290,\"start\":34281},{\"end\":34487,\"start\":34475},{\"end\":34499,\"start\":34487},{\"end\":34510,\"start\":34499},{\"end\":34522,\"start\":34510},{\"end\":34535,\"start\":34522},{\"end\":34885,\"start\":34876},{\"end\":34895,\"start\":34885},{\"end\":34903,\"start\":34895},{\"end\":34912,\"start\":34903},{\"end\":34921,\"start\":34912},{\"end\":35304,\"start\":35295},{\"end\":35311,\"start\":35304},{\"end\":35319,\"start\":35311},{\"end\":35330,\"start\":35319},{\"end\":35677,\"start\":35664},{\"end\":35689,\"start\":35677},{\"end\":35702,\"start\":35689},{\"end\":35717,\"start\":35702},{\"end\":35994,\"start\":35982},{\"end\":36004,\"start\":35994},{\"end\":36017,\"start\":36004},{\"end\":36025,\"start\":36017},{\"end\":36035,\"start\":36025},{\"end\":36452,\"start\":36444},{\"end\":36459,\"start\":36452},{\"end\":36807,\"start\":36801},{\"end\":36815,\"start\":36807},{\"end\":36824,\"start\":36815},{\"end\":36831,\"start\":36824},{\"end\":36839,\"start\":36831},{\"end\":37200,\"start\":37193},{\"end\":37208,\"start\":37200},{\"end\":37217,\"start\":37208},{\"end\":37468,\"start\":37455},{\"end\":37477,\"start\":37468},{\"end\":37700,\"start\":37689},{\"end\":37715,\"start\":37700},{\"end\":37727,\"start\":37715},{\"end\":37736,\"start\":37727},{\"end\":38127,\"start\":38117},{\"end\":38138,\"start\":38127},{\"end\":38149,\"start\":38138},{\"end\":38159,\"start\":38149},{\"end\":38171,\"start\":38159},{\"end\":38180,\"start\":38171},{\"end\":38480,\"start\":38467},{\"end\":38488,\"start\":38480},{\"end\":38499,\"start\":38488},{\"end\":38509,\"start\":38499},{\"end\":38777,\"start\":38768},{\"end\":38790,\"start\":38777},{\"end\":38799,\"start\":38790},{\"end\":39131,\"start\":39124},{\"end\":39142,\"start\":39131},{\"end\":39153,\"start\":39142},{\"end\":39555,\"start\":39546},{\"end\":39562,\"start\":39555},{\"end\":39574,\"start\":39562},{\"end\":39585,\"start\":39574},{\"end\":39596,\"start\":39585},{\"end\":39611,\"start\":39596},{\"end\":39619,\"start\":39611},{\"end\":40128,\"start\":40116},{\"end\":40145,\"start\":40128},{\"end\":40160,\"start\":40145},{\"end\":40171,\"start\":40160},{\"end\":40610,\"start\":40596},{\"end\":40619,\"start\":40610},{\"end\":40631,\"start\":40619},{\"end\":40644,\"start\":40631},{\"end\":40651,\"start\":40644},{\"end\":40664,\"start\":40651},{\"end\":40673,\"start\":40664},{\"end\":40684,\"start\":40673},{\"end\":40694,\"start\":40684},{\"end\":40708,\"start\":40694},{\"end\":41215,\"start\":41200},{\"end\":41227,\"start\":41215},{\"end\":41238,\"start\":41227},{\"end\":41250,\"start\":41238},{\"end\":41260,\"start\":41250},{\"end\":41658,\"start\":41649},{\"end\":41671,\"start\":41658},{\"end\":41827,\"start\":41816},{\"end\":41839,\"start\":41827},{\"end\":41850,\"start\":41839},{\"end\":41860,\"start\":41850},{\"end\":42204,\"start\":42187},{\"end\":42213,\"start\":42204},{\"end\":42226,\"start\":42213},{\"end\":42238,\"start\":42226},{\"end\":42629,\"start\":42619},{\"end\":42644,\"start\":42629},{\"end\":42656,\"start\":42644},{\"end\":42667,\"start\":42656},{\"end\":42680,\"start\":42667},{\"end\":42693,\"start\":42680},{\"end\":42703,\"start\":42693},{\"end\":43202,\"start\":43187},{\"end\":43217,\"start\":43202},{\"end\":43225,\"start\":43217},{\"end\":43651,\"start\":43643},{\"end\":43660,\"start\":43651},{\"end\":43673,\"start\":43660},{\"end\":43686,\"start\":43673},{\"end\":43917,\"start\":43903},{\"end\":43925,\"start\":43917},{\"end\":44294,\"start\":44280},{\"end\":44302,\"start\":44294},{\"end\":44311,\"start\":44302},{\"end\":44320,\"start\":44311},{\"end\":44327,\"start\":44320},{\"end\":44342,\"start\":44327},{\"end\":44350,\"start\":44342},{\"end\":44812,\"start\":44806},{\"end\":44819,\"start\":44812},{\"end\":45069,\"start\":45062},{\"end\":45075,\"start\":45069},{\"end\":45083,\"start\":45075},{\"end\":45089,\"start\":45083},{\"end\":45095,\"start\":45089},{\"end\":45102,\"start\":45095},{\"end\":45109,\"start\":45102},{\"end\":45494,\"start\":45487},{\"end\":45501,\"start\":45494},{\"end\":45507,\"start\":45501},{\"end\":45515,\"start\":45507},{\"end\":45523,\"start\":45515},{\"end\":45776,\"start\":45768},{\"end\":45784,\"start\":45776},{\"end\":45795,\"start\":45784},{\"end\":46183,\"start\":46170},{\"end\":46196,\"start\":46183},{\"end\":46401,\"start\":46390},{\"end\":46410,\"start\":46401}]", "bib_venue": "[{\"end\":31541,\"start\":31493},{\"end\":31927,\"start\":31874},{\"end\":32290,\"start\":32265},{\"end\":32641,\"start\":32578},{\"end\":33019,\"start\":33017},{\"end\":33189,\"start\":33104},{\"end\":33516,\"start\":33453},{\"end\":33922,\"start\":33765},{\"end\":34259,\"start\":34199},{\"end\":34598,\"start\":34535},{\"end\":34987,\"start\":34921},{\"end\":35389,\"start\":35330},{\"end\":35773,\"start\":35717},{\"end\":36101,\"start\":36035},{\"end\":36512,\"start\":36459},{\"end\":36902,\"start\":36839},{\"end\":37273,\"start\":37217},{\"end\":37499,\"start\":37477},{\"end\":37799,\"start\":37736},{\"end\":38115,\"start\":38065},{\"end\":38531,\"start\":38509},{\"end\":38852,\"start\":38799},{\"end\":39219,\"start\":39153},{\"end\":39685,\"start\":39619},{\"end\":40237,\"start\":40171},{\"end\":40783,\"start\":40708},{\"end\":41326,\"start\":41260},{\"end\":41675,\"start\":41671},{\"end\":41910,\"start\":41860},{\"end\":42291,\"start\":42238},{\"end\":42769,\"start\":42703},{\"end\":43287,\"start\":43225},{\"end\":43693,\"start\":43686},{\"end\":43988,\"start\":43925},{\"end\":44416,\"start\":44350},{\"end\":44828,\"start\":44819},{\"end\":45162,\"start\":45109},{\"end\":45485,\"start\":45427},{\"end\":45858,\"start\":45795},{\"end\":46168,\"start\":46101},{\"end\":46446,\"start\":46410},{\"end\":31585,\"start\":31543},{\"end\":31976,\"start\":31929},{\"end\":32700,\"start\":32643},{\"end\":33575,\"start\":33518},{\"end\":34657,\"start\":34600},{\"end\":35049,\"start\":34989},{\"end\":35444,\"start\":35391},{\"end\":36163,\"start\":36103},{\"end\":36561,\"start\":36514},{\"end\":36961,\"start\":36904},{\"end\":37858,\"start\":37801},{\"end\":38901,\"start\":38854},{\"end\":39281,\"start\":39221},{\"end\":39747,\"start\":39687},{\"end\":40299,\"start\":40239},{\"end\":40854,\"start\":40785},{\"end\":41388,\"start\":41328},{\"end\":41956,\"start\":41912},{\"end\":42340,\"start\":42293},{\"end\":42831,\"start\":42771},{\"end\":43345,\"start\":43289},{\"end\":44047,\"start\":43990},{\"end\":44478,\"start\":44418},{\"end\":45211,\"start\":45164},{\"end\":45917,\"start\":45860}]"}}}, "year": 2023, "month": 12, "day": 17}