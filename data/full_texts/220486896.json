{"id": 220486896, "updated": "2023-10-06 16:26:16.418", "metadata": {"title": "Weakly Supervised Deep Nuclei Segmentation Using Partial Points Annotation in Histopathology Images", "authors": "[{\"first\":\"Hui\",\"last\":\"Qu\",\"middle\":[]},{\"first\":\"Pengxiang\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Qiaoying\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jingru\",\"last\":\"Yi\",\"middle\":[]},{\"first\":\"Zhennan\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Kang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Gregory\",\"last\":\"Riedlinger\",\"middle\":[\"M.\"]},{\"first\":\"Subhajyoti\",\"last\":\"De\",\"middle\":[]},{\"first\":\"Shaoting\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Dimitris\",\"last\":\"Metaxas\",\"middle\":[\"N.\"]}]", "venue": "IEEE transactions on medical imaging", "journal": "IEEE transactions on medical imaging", "publication_date": {"year": 2020, "month": 7, "day": 10}, "abstract": "Nuclei segmentation is a fundamental task in histopathology image analysis. Typically, such segmentation tasks require significant effort to manually generate accurate pixel-wise annotations for fully supervised training. To alleviate such tedious and manual effort, in this paper we propose a novel weakly supervised segmentation framework based on partial points annotation, i.e., only a small portion of nuclei locations in each image are labeled. The framework consists of two learning stages. In the first stage, we design a semi-supervised strategy to learn a detection model from partially labeled nuclei locations. Specifically, an extended Gaussian mask is designed to train an initial model with partially labeled data. Then, selftraining with background propagation is proposed to make use of the unlabeled regions to boost nuclei detection and suppress false positives. In the second stage, a segmentation model is trained from the detected nuclei locations in a weakly-supervised fashion. Two types of coarse labels with complementary information are derived from the detected points and are then utilized to train a deep neural network. The fully-connected conditional random field loss is utilized in training to further refine the model without introducing extra computational complexity during inference. The proposed method is extensively evaluated on two nuclei segmentation datasets. The experimental results demonstrate that our method can achieve competitive performance compared to the fully supervised counterpart and the state-of-the-art methods while requiring significantly less annotation effort.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.05448", "mag": "3098637688", "acl": null, "pubmed": "32746112", "pubmedcentral": null, "dblp": "journals/tmi/QuWHYYLRDZM20", "doi": "10.1109/tmi.2020.3002244"}}, "content": {"source": {"pdf_hash": "f9c3bff1d29bde9c7f533dd6b819aeb5c113fa19", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.05448v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.05448", "status": "GREEN"}}, "grobid": {"id": "03e41498c96b1328cdf52757bb403529c102bd78", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f9c3bff1d29bde9c7f533dd6b819aeb5c113fa19.txt", "contents": "\nWeakly Supervised Deep Nuclei Segmentation Using Partial Points Annotation in Histopathology Images\n\n\nHui Qu \nPengxiang Wu \nQiaoying Huang \nJingru Yi \nZhennan Yan \nKang Li \nGregory M Riedlinger \nSubhajyoti De \nShaoting Zhang \nFellow, IEEEDimitris N Metaxas \nWeakly Supervised Deep Nuclei Segmentation Using Partial Points Annotation in Histopathology Images\n\nIEEE TRANSACTIONS ON MEDICAL IMAGING\nXX110.1109/TMI.2020.3002244This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication. The final version of record is available at http://dx.\nNuclei segmentation is a fundamental task in histopathology image analysis. Typically, such segmentation tasks require significant effort to manually generate accurate pixel-wise annotations for fully supervised training. To alleviate such tedious and manual effort, in this paper we propose a novel weakly supervised segmentation framework based on partial points annotation, i.e., only a small portion of nuclei locations in each image are labeled. The framework consists of two learning stages. In the first stage, we design a semi-supervised strategy to learn a detection model from partially labeled nuclei locations. Specifically, an extended Gaussian mask is designed to train an initial model with partially labeled data. Then, selftraining with background propagation is proposed to make use of the unlabeled regions to boost nuclei detection and suppress false positives. In the second stage, a segmentation model is trained from the detected nuclei locations in a weakly-supervised fashion. Two types of coarse labels with complementary information are derived from the detected points and are then utilized to train a deep neural network. The fully-connected conditional random field loss is utilized in training to further refine the model without introducing extra computational complexity during inference. The proposed method is extensively evaluated on two nuclei segmentation datasets. The experimental results demonstrate that our method can achieve competitive performance compared to the fully supervised counterpart and the state-of-the-art methods while requiring significantly less annotation effort.Index Terms-Nuclei detection, nuclei segmentation, semi-supervised learning, weakly-supervised learning, deep learning, Voronoi diagram, k-means clustering, conditional random field.\n\nH ISTOPATHOLOGY plays a vital role in cancer diagnosis, prognosis, and treatment decisions. Histopathology slides are created from formalin-fixed paraffin-embedded (FFPE) tissue containing both tumor and surrounding normal tissue. These slides are then stained with agents such as hematoxylin and eosin (H&E) and immunohistochemical stains that permit the pathologist to ascertain important features. For instance, pathologists routinely determine the type of cancer, stage of cancer, cancer's grade, presence of infiltrating immune cells, and potential treatment options based on histopathology slides. Whole slide imaging allows the pathologist to view the slides digitally as opposed to what was traditionally viewed under a microscope. With improvements in computational power and image analysis algorithms, computational methods [1]- [4] have been developed for the quantitative and objective analyses of histopathology images, which can reduce the intensive labor and improve the efficiency for pathologists compared with manual examinations. Nuclei segmentation is a critical step in the automatic analysis of histopathology images, because the nuclear features such as average size, density and nucleusto-cytoplasm ratio are related to the clinical diagnosis and management of cancer. Besides, clinical sequencing of cancer specimens is becoming routine and nuclei segmentation algorithms will play a key role in the proper interpretation of these sequencing results.\n\nTraditional nuclei segmentation algorithms [5], [6] utilize techniques such as watershed segmentation, physics-based deformable models, level sets and graph cuts. They are designed for a certain type of histopathology images, and cannot work well when there are large variations in tissue types, colors, and nuclear appearances. Learning-based approaches can achieve better performance when dealing with the above variations, as well as some challenging cases like separating touching nuclei. Early learning-based methods, for example Kong et al. [7], trained models using handcrafted features such as color, texture, and other image-level features to segment nuclear regions. Zhang et al. [8] developed a robust segmentation method to delineate cells accurately using Gaussian-based hierarchical voting and repulsive balloon model. Modern deep learning based algorithms [9]- [18] focus on training deep convolutional neural networks (CNNs) for segmentation, and are more effective than previous methods. However, the fully supervised learning of deep neural networks in these methods requires a large amount of training data, which are pixelwise annotated. It is difficult to collect such datasets because assigning a nucleus/background class label to every pixel in the image is very time-consuming and requires expert domain knowledge.\n\nIn this paper, we describe a novel weakly supervised nuclei segmentation framework for histopathology images using only a portion of annotated nuclear locations. It achieves comparable performance as the fully-supervised methods and about 60\u00d7 speed-up (10% points) in the annotation time. Our method consists of two stages: (1) semi-supervised nuclei detection and (2) weakly supervised nuclei segmentation. The goal of the first stage is to train a detector from partial points annotation to predict the locations of all nuclei in training images. A challenge is that there is no clear background information because only part of the nuclei are labeled in an image. To obtain a good initial detector, we first design an extended Gaussian mask to supervise the training with the labeled nuclear locations and ignore most unlabeled areas. Then, we propose a self-training strategy to make use of the unlabeled areas in images, which refines the background information in an iterative fashion and suppresses false positives.\n\nThe detection stage produces central points of all detected nuclei. However, these detected points cannot be directly used to supervise the training of a segmentation model in the second stage. To address this problem, we take advantage of the original image and the shape prior of the nuclei to derive two types of coarse labels from the nuclei points using the Voronoi diagram and the k-means clustering algorithm. The coarse labels are used to train a deep convolutional neural network for the segmentation task. A common problem in most weakly supervised segmentation tasks is inaccurate object boundaries due to missing information. Therefore, postprocessing like the dense conditional random field (CRF) [19] or graph search [20] is needed to refine the object boundaries, at the expense of extra processing time. Inspired by Tang et al.'s work [21], we utilize the dense CRF in the loss function to fine-tune the trained model rather than a postprocessing step. This efficient inference is more effective in nuclei segmentation from large Whole Slide Images (WSIs).\n\nThis paper is an extension of our previous work [22] with the following contributions:\n\n\u2022 A unified framework for nuclei segmentation using only a small portion of nuclear locations (e.g., 10%). \u2022 A novel self-training strategy is proposed in semisupervised nuclei detection. \u2022 More extensive analyses of learning strategies in weakly supervised segmentation.\n\n\nII. RELATED WORK\n\nIn this section, we provide an overview of the related work in two aspects: (1) deep learning-based nuclei detection and segmentation methods in histopathology images and (2) weakly supervised algorithms in natural and medical images.\n\nA. Nuclei detection and segmentation using deep learning 1) Nuclei detection: Deep CNNs have been applied in nuclei detection in recent years. Ciresan et al. [23] proposed a mitosis detection method by classifying each pixel using a patch centered on it. Xie et al. [24] developed a structured regression CNN model (SR-CNN) to utilize topological structure information. Sirinukunwattana et al. [25] improved SR-CNN by using a spatial-constrained layer. Xu et al. [26] proposed a stacked sparse autoencoder to learn high-level structure information from unlabeled image patches and trained a classifier using the extracted features of the encoder. These methods depend on patch-based classification or regression, and thus are computationally expensive for large microscopy images. Later on, the fully convolutional neural network (FCN) [27] and its variants were applied in the nuclei detection, which can significantly improve the efficiency in inference by eliminating repeated computations for overlapping patches in the patchbased approaches. A structured regression model for nuclei detection was developed in [28] to produce better distinctive peaks at cell centroids. Zhou et al. [29] presented a Sibling FCN which detects nuclei and classifies them into subcategories simultaneously. It takes advantage of the mutual information of both tasks to improve performance. Aside from these supervised training methods, Li et al. [30] proposed a semi-supervised learning framework for signet ring cell detection to cope with incomplete annotation and make use of unlabeled images. What is more challenging in our case is that we only have a small portion of nuclei annotated.\n\n2) Nuclei segmentation: Similar to nuclei detection, current deep-learning-based methods can be roughly divided into patch-based and FCN-based categories. In the patch-based category, Su et al. [31] utilized sparse denoising autoencoder to segment nuclei. Xing et al. [9] obtained initial shape probability maps of nuclei by CNN and then incorporated a topdown shape prior model and a bottom-up deformable model for segmentation. Kumar et al. [10] formulated the problem as a three-class segmentation task and performed region growing as post-processing based on the initial segmentation results. In the FCN-based category, Naylor et al. [13] solved the problem as a regression task of estimating the nuclei distance map which is beneficial to separate touching or overlapping nuclei. Qu et al. [32] combined the tasks of nuclei segmentation and fine-grained classification into one framework. To solve the problem of insufficient training data, Mahmood et al. [12] synthesized additional training images using CycleGAN [33]. And Hou et al. [15] generated images of different tissue types and adopted an importance sampling loss during segmentation according to the quality of synthesized images. We also use the FCN-based framework, but with weak labels (central points).\n\n\nB. Weakly supervised image segmentation using deep learning\n\nCompared to fully supervised methods, weakly supervised approaches have the advantage of reducing manual annotation effort. In natural image segmentation, Papandreou et al. [  proposed Expectation-Maximization (EM) method for training with image-level or bounding-box annotation. Pathak et al. [35] added a set of linear constraints on the output space in loss function to exploit the information from image-level labels. Compared to image-level annotation, points annotation has better location information for each object. Bearman et al. [36] incorporated an objectness prior in the loss to guide the training of a CNN, which helps separate objects from background. Scribbles annotation, which requires at least one scribble for every object, is a more informative type of weak label. Lin et al. [37] adopted scribbles annotation to train a graphical model that propagates the information from the scribbles to the unmarked pixels. The most widely used weak annotation is the bounding box, both in natural images [38], [39] and in medical images [20], [40]. Kervadec et al. [41] used a small fraction of full labels and imposed a size constraint in their loss function, which achieved good performance but is not applicable for multiple objects of the same class. Although existing weakly supervised methods have achieved good performance in natural and medical image segmentation, most weak annotations are not suitable for nuclei segmentation task. Image-level annotation cannot be used in medical image segmentation where object classes in images are usually fixed (e.g., nuclei and background in our task). Scribbles annotation is also not suitable for our task due to the small size and large number of nuclei. It is difficult and time-consuming to label an image using bounding boxes for hundreds of nuclei, especially when the density is high. Points annotation is a good choice in terms of preserving information and saving annotation effort, but the objectiveness prior in the points supervision work [36] is not working here since nuclei are small and thus the prior is inaccurate. Different from existing weakly supervised methods, we propose to employ partial points annotation for nuclei segmentation.\n\n\nIII. DETECTION WITH PARTIAL POINTS\n\nIn this section, we describe the semi-supervised nuclei detection algorithm ( Fig. 1(a)), which consists of two steps: initial training with extended Gaussian masks and self-training with background propagation.\n\n\nA. Initial training with extended Gaussian masks\n\nThe first step of our detection method aims to train an initial detector using the labeled nuclei in each image. However, the points indicating nuclear locations cannot be directly applied for training. They are often used to generate binary masks for pixel classification [29], or structured proximity masks for regression [28]. In our case, it is not possible to follow these methods because most areas in an image are unlabeled. In order to tackle this issue, we define an extended Gaussian mask M according to the labeled points:\nM i = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 exp \u2212 D 2 i 2\u03c3 2 if D i < r 1 , 0 if r 1 < D i < r 2 , \u22121 otherwise,(1)\nwhere D i is the distance from pixel i to the closest labeled point, r 1 is average radius of the nuclei and can be calculated using the validation set, r 2 is a parameter to control the range of background area and is set to r 2 = 2r 1 . \u03c3 is the Gaussian bandwidth. In M , 0 means the background pixel and \u22121 means unlabeled pixel which will be ignored during training. The underlying assumption is that pixels in the annular area r 1 < D i < r 2 belong to the background, which is reasonable because most nuclei are surrounded by background pixels.\n\nWith the extended Gaussian masks, we are able to train a regression model for nuclei detection. We replace the encoder part of U-net [42] with the convolution layers of ResNet-34 [43] (shown in Fig. 1), which is more powerful in representation ability and can be initialized with pretrained parameters. The network is trained with a mean squared loss L mse with respect to the corresponding extended Gaussian mask:\nL mse = 1 |\u2126| i\u2208\u2126 w i (p i \u2212 M i ) 2 ,(2)\nwhere \u2126 is the set consisting of non-ignored pixels, p i is the predicted probability of being nucleus by the network, and w i is the weight of pixel i. Considering the imbalance between the labeled points and background pixels, we set w i = 10 for pixels with mask value greater than 0 and w i = 1 for background pixels. The detection results are obtained by thresholding the probability map and finding the centroids of connected components.\n\n\nB. Self-training with background propagation\n\nThe detection performance of the initial model is not good enough because of the small number of labeled nuclei and large ignored areas. The unlabeled regions in images can be utilized to improve the performance by semi-supervised learning methods such as self-training. Intuitively, the initial model could predict the nuclei locations on the unlabeled regions. The predicted nuclei are then used to supervise the model training along with the originally labeled nuclei, like what the authors did on cell detection in [30]. However, we find that there are too many false positives among the newly detected nuclei because the trained model with the small number of labeled nuclei is not good. The false positives mislead the training during iterations, resulting in worse detection performance. Therefore, we propose an iterative learning strategy to refine the background map during self-training. Because the background tissue and blank areas are easier to be distinguished, producing less false positives.\n\nThe process of self-training is shown in Fig. 1(a). In each round of self-training, the background map is firstly obtained from the trained model of the previous round (or the initial model for the first round). Then the background map is combined with the original labeled points to generate a new mask for training. In background map generation, we select background pixels in the probability map if p i < 0.1 or p i > 0.7. The first criterion is straightforward since p i is the probability of being nuclei. The second one is considered because many background pixels get predicted values close to 1, especially in the first stage. This behavior is expected because most background pixels are ignored during training. If the initialized model predicts them as nuclei pixels, the predictions will remain unchanged. In order to prevent from adding true positive into background, for p i > 0.7 case we only take into account the large connected components of areas greater than the average nuclei area, i.e., \u03c0r 2 1 . The updated maskM is finally generated by adding the new background information to the original extended Gaussian mask (See Eqn. 3).\nM i = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 M i if D i < r 2 , 0 if D i > r 2 and (p i < 0.1 or p i > 0.7) \u22121 otherwise,(3)\nAn example is shown in Fig. 2 to illustrate the masks in different rounds of self-training. The foreground nuclei annotation (green pixels) is kept unchanged during the iterations while the background area (pixels in black) grows up gradually. In the third round, the background has high accuracy and the ignored pixels (orange) are almost all nuclei.\n\n\nIV. SEGMENTATION WITH POINTS\n\nAfter obtaining a good detection model, we can predict the nuclei locations on all training images. Although the detected points are not 100% accurate, we have much more information for segmentation compared to the initial partial points. In this section, we describe our weakly supervised method to segment nuclei from the detected points. In particular, our pointlevel supervision for training a nuclei segmentation model consists of three steps: (1) coarse pixel-level labels generation using the detected points from the nuclei detection stage;\n\n(2) segmentation network training with the generated coarse labels; (3) model refinement using the dense CRF loss.\n\n\nA. From point-level to pixel-level labels\n\nThe point-level labels (detected points) cannot be used directly for the training of a CNN with the cross entropy loss due to the lack of (negative) background labels since all annotated points belong to the (positive) nuclei category. To solve this issue, the first step is to exploit the information we have to generate useful pixel-level labels for both classes. We have the following observations:\n\n\u2022 Each point is expected to be located or close to the center of a nucleus, and the shapes of most nuclei are nearly ellipses, i.e., they are convex. \u2022 The colors of nuclei pixels are often different from the surrounding background pixels. Based on these observations, we propose to utilize the Voronoi diagram and k-means clustering methods to produce two types of pixel-level labels.\n\n1) Voronoi labels: A Voronoi diagram is a partitioning of a plane into convex polygons (Voronoi cells) according to the distance to a set of points in the plane. There is exactly one point (seed point) in each cell and all points in a cell are closer to its seed point than other seed points. In our task, the detected points in an image can be treated as seed points to calculate the Voronoi diagram as shown in Fig. 1(b). For each Voronoi cell, assuming that the corresponding nucleus is located within the cell, then the Voronoi edges separate all nuclei well and the edge pixels belong to the background. This assumption holds for most of the nuclei because the detected points are around the centers and nuclear shapes are nearly convex ( Fig. 3(b)).\n\nAssigning the Voronoi edges as background pixels and the detected points (dilated with a disk kernel of radius 2) as nuclei pixels, we obtain the Voronoi point-edge label (Fig. 3). All other pixels are ignored during training. Note that although the pixels on the Voronoi edge between two touching nuclei may not necessarily be background, the edges are still helpful in guiding the network to separate the nuclei. The Voronoi labels aim to segment the central parts of nuclei and are not able to extract the full masks, because they lack the information of nuclear boundaries and shapes. To overcome this weakness, we generate another kind of labels that contain this complementary information.\n\n2) Cluster labels: Considering the color difference between nuclei and background pixels, it is feasible to perform a rough segmentation using clustering methods. We choose k-means clustering to extract both nuclei and background pixels from the image, and generate the cluster labels. Given an image x with N pixels (x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x N ), k-means clustering aims to partition the N pixels into k clusters S = (S 1 , S 2 , \u00b7 \u00b7 \u00b7 , S k ) according to the feature vector f xi of each pixel x i , such that the sum of within-cluster variances is minimized:\narg min S k i=1 x\u2208Si f x \u2212 c i 2 .(4)\nWe use k-means to divide all pixels into k = 3 clusters: nuclei, background and ignored. The cluster that has maximum overlap with points label is considered as nuclei, and the cluster that has minimum overlap with the dilated points label is considered as background. The remaining one is the ignored class. The pixels of the ignored class are often located around the nuclear boundaries, which are hard for a clustering method to assign correct labels.\n\nFor the feature vector f , color is a straightforward choice. However, clustering with color will result in wrong assignments for pixels inside some nuclei that have non-uniform colors. To cope with this issue, we add a distance feature. In a distance map (Fig. 3(c)), each value indicates the distance of that pixel to the closest nuclear point and therefore incorporates the spatial information. In particular, the pixels that belong to nuclei should be close enough to points in the label while background pixels are relatively far from those points. The distance map can be calculated by the distance transform of the complement image of detected points. Combining the distance value d i with the RGB color values (r i , g i , b i ) as the feature vector f xi = (d i ,r i ,\u011d i ,b i ) in k-means clustering, we obtain the initial cluster labels.d i is the clipped value by truncating large values to 20 andr i ,\u011d i ,b i are normalized values such that every feature has similar value range.\n\nThe cluster label ( Fig. 1(b)) is generated by refining the clustering result with morphological dilation and erosion, which are done separately in each Voronoi cell to avoid connecting close nuclei. The cluster labels have more shape information about the nuclei compared with the Voronoi labels, at the expense of more errors and uncertainties. We argue that these two types of labels are complementary to each other and will jointly lead to better results.\n\n\nB. Training deep neural networks with pixel-level labels\n\nOnce we have the two types of pixel-level labels, we are able to train a deep convolutional neural network for nuclei segmentation. The network structure is the same as that in nuclei detection. It outputs two probability maps of background and nuclei, which are used to calculate two cross entropy losses with respect to the cluster label L cluster and Voronoi label L vor :\nL cluster/vor (y, t) = \u2212 1 |\u2126| i\u2208\u2126 [t i log y i + (1 \u2212 t i ) log(1 \u2212 y i )] ,(5)\nwhere y is the probability map, t is the cluster label or Voronoi label, and \u2126 is the set consisting of non-ignored pixels. The final loss is\nL ce = \u03b1L vor + (1 \u2212 \u03b1)L cluster ,(6)\nwhere \u03b1 is a balancing parameter.\n\n\nC. Model refinement using dense CRF loss\n\nThe model trained using the two types of labels is able to predict the mask of individual nucleus with high accuracy. To further improve the performance, we refine the nuclear boundaries with the dense CRF loss. Previously, post-processing such as region growing [10], graph search [20] or dense CRF [19] is often utilized to refine the segmentation results. These algorithms introduce more computational complexity, making them unsuitable for the processing of high resolution Whole Slide Images. To solve this problem, similar to [21], we embed the dense CRF into the loss function during training to improve the accuracy. The loss function is not calculated during inference, and therefore will not introduce additional computational cost after training. Let\u1ef9 = (\u1ef9 1 ,\u1ef9 2 , \u00b7 \u00b7 \u00b7 ,\u1ef9 N ) denote the predicted label (0 for background and 1 for nuclei) from probability maps y and t be the label. The dense CRF is to minimize the energy function:\nE(\u1ef9, t) = i \u03c6(\u1ef9 i , t i ) + i,j \u03c8(\u1ef9 i ,\u1ef9 j ),(7)\nwhere \u03c6 is the unary potential that measures how likely a pixel belongs to a certain class, and \u03c8 is the pairwise potential that measures how different a pixel's label is from all other pixels' in the image. The unary term is replaced with the cross entropy loss L ce . The pairwise potential usually has the form:\n\u03c8(\u1ef9 i ,\u1ef9 j ) = \u00b5(\u1ef9 i ,\u1ef9 j )W ij = \u00b5(\u1ef9 i ,\u1ef9 j ) K m=1 w m k m (f i ,f j ),(8)\nwhere \u00b5 is a label compatibility function, W ij is the affinity between pixels i, j and is often calculated by the sum of Gaussian kernels k m . Here we choose \u00b5 as the Potts model, i.e., \u00b5(\u1ef9 i ,\u1ef9 j ) = [\u1ef9 i =\u1ef9 j ], and bilateral feature vectorf i = pi \u03c3pq , qi \u03c3pq , ri \u03c3 rgb , gi \u03c3 rgb , bi \u03c3 rgb that contains both location and color information. \u03c3 pq and \u03c3 rgb are Gaussian bandwidth.\n\nTo adapt the energy function to a differentiable loss function, we relax the pairwise potential as [21]:\n\u03c8(\u1ef9 i ,\u1ef9 j ) =\u1ef9 i (1 \u2212\u1ef9 j )W ij .(9)\nTherefore, the dense CRF loss can be expressed as:\n\nL crf (y, t cluster , t vor ) = L ce (y, t cluster , t vor ) + \u03b2L pair (y), (10) where L pair (y) = i,j y i (1\u2212y j )W ij is the pairwise potential loss and \u03b2 is the weighting factor. The CRF loss is used to fine-tune the trained model. Due to the large number of pixels in an image, the cost of directly computing the affinity matrix W = [W ij ] is prohibitive. For instance, there are N 2 = 1.6\u00d710 9 elements in W for an image of size 200\u00d7200 that has N = 40000 pixels. We adopt fast mean-field inference based on high-dimensional filtering [44] to compute the pairwise potential term.\n\n\nV. EXPERIMENTS\n\nTo validate the proposed framework, we conduct experiments on two datasets of H&E stained histopathology images.\n\nA. Datasets 1) Lung Cancer (LC) dataset: We generated this dataset by extracting 40 images of size 900 \u00d7 900 from 8 lung adenocarcinoma or lung squamous cell carcinoma cases, i.e., H&E stained WSIs with 20x magnification. They are split into the training, validation and test sets, consisting of 24, 8 and 8 images, respectively. 24401 nuclei are annotated with masks.\n\n2) Multi-Organ (MO) dataset: It is a public dataset released by Kumar et al. [10], and consists of 30 images of size 1000\u00d71000 which are taken from multiple hospitals including a diversity of nuclear appearances from seven organs [10]. The variability in this dataset is large because of the heterogeneity between organs and cancer types. There are 12, 4 and 14 images in training, validation and test sets.\n\nBoth datasets have full mask annotation. We use the bounding box centers of the nuclear masks as ground-truth for the detection. To generate the partial points annotation in the training set, we randomly sample a certain ratio of points.\n\n\nB. Detection using partial points annotation\n\nThe aim of this experiment is to detect all nuclei in an image using the model trained with partial points annotation.\n\n1) Evaluation metrics: We adopt the common metrics for detection tasks: precision (P), recall (R) and F1 score. They are defined as: P = T P/(T P + F P ), R = T P/(T P + F N ),  in a circle centered at a nuclear centroid with r-pixel radius, otherwise it is a false positive. The ground-truth points which have no corresponding detection are false negatives. If there are multiple detected points for the same ground-truth point, only the closest one is considered as a true positive. r is the rough average nuclear radius computed using the validation set. We set r = 8 for the LC dataset and r = 11 for the MO dataset. We also adopt the mean (\u00b5 d ) and standard deviation (\u03c3 d ) of the detection distance error to evaluate the localization accuracy. They are defined as\n\u00b5 d = 1 N T P N T P i=1 d i , \u03c3 d = 1 N T P N T P i=1 (d i \u2212 \u00b5 d ) 2(11)\nwhere N T P is the total number of true positive detected nuclei in all test images, d i is the Euclidean distance between the i-th groundtruth point and the true positive detection.\n\n2) Implementation details: Color normalization [45] is applied to all images to remove color variations caused by staining. We extract 16 image patches of size 250\u00d7250 from each training image, and randomly crop 224\u00d7224 patches as network inputs. Other data augmentations are conducted including random crop, scale, rotation, flipping, and affine transformations. The encoder part of the network is initialized with the pre-trained parameters. The model is trained using the Adam optimizer [46] for 80 epochs in initial training and each round of self-training. The batch size is 16. The learning rate is 1e-4 for LC dataset and 1e-3 for MO dataset. The parameters in the extended Gaussian mask in Eqn. (1) are r 1 = 8, r 2 = 16, \u03c3 = 2 for LC dataset and r 1 = 11, r 2 = 22, \u03c3 = 2.75 for MO dataset. During inference, models from round 3 of self-training are used to evaluate on the test data, since it is already converged.\n\n3) Results and discussion: We compare different strategies mentioned in Section III:\n\n\u2022 Full: fully-supervised training using all annotated nuclei.\n\n\u2022 GM: initial training using simple Gaussian masks from partial points annotation, i.e., no ignored pixels. \u2022 ext-GM: initial training using our proposed extended Gaussian masks from partial points annotation. \u2022 ST-nu: updating the label by adding detected nuclei in the self-training step of our method. \u2022 ST-bg: updating the label by propagating background pixels in the self-training step of our method. For both ST-nu and ST-bg, ext-GM is used in the initial training. The detection results using 10% points in each training image are reported in Table I. a) Initial training strategies: In the two initial training strategies, the ext-GM achieves better performance on both datasets. For GM, all the unlabeled nuclei are treated as background, which biases the training and guides the network to predict pixels as background more aggressively. Our extended Gaussian masks force the model to focus on the areas around the labeled points. As a result, the trained model is able to make correct predictions in similar unlabeled regions. b) Self-training strategies: In the self-training stage, compared with the results of the first stage (ext-GM), updating the nuclei (ST-nu) decreases the performance. The reason is that the number of false positives in the newly added nuclei is comparable to that of the labeled nuclei, resulting in a negative effect on training. In contrast, our background propagation strategy (ST-bg) keeps the labeled nuclei unchanged and gradually increases the background area (as shown in Fig. 2), which doesn't introduce false positives during training. Therefore, it can improve both recall and precision, resulting in a much higher F1 score and localization accuracy. c) Comparison to fully-supervised case: Compared with the results of full annotation (Full), our method (ST-bg) can achieve comparable performance while using much fewer annotation data. On the LC dataset, the precision, recall and F1 are 98.2%, 100.3%, 99.2% of the fully-supervised results, respectively. And these numbers are 97.8%, 96.1%, 97.0% respectively on the MO dataset. Besides, the localization error \u00b5 d \u00b1 \u03c3 d is also very close to that using full annotation on both datasets.\n\nThe typical qualitative results of different training strategies from both datasets using 10% points are shown in Fig. 4. 4) Different ratios of annotation: To explore how the proposed nuclei detection method behaves when the ratio of points changes, we trained with 5%, 10%, 25% and 50% partial points annotation. The results are shown in Table II. The detection accuracy (F1-score) increases and localization error (\u00b5 d \u00b1 \u03c3 d ) decreases as more annotations are available. With 50% points annotated, the performance is nearly the same as that using full annotation. Even with only 5% annotation, the F1 score of our method can reach 99.0% of that using full annotation on LC dataset and 96.3% on the MO dataset, which substantiates the effectiveness of our algorithm. The results of MO dataset are slightly worse compared with those on the LC dataset because the MO dataset is more challenging due to the diversity in nuclear size and appearance.\n\n\nC. Segmentation using ground-truth points\n\nIn this subsection, we present the experimental results of our segmentation method based on all ground-truth points. We first try to obtain the optimal value of \u03b1 in Eqn. (6) and discuss the effects of two types of labels, then show the effects of the dense CRF loss, and finally compare our results with fully supervised ones.\n\n1) Evaluation metrics: Four metrics are used to evaluate the segmentation performance. At pixel-level, we use pixel accuracy and pixel-level F1 score. Because nuclei segmentation is an instance segmentation task, two object-level metrics are also used: object-level Dice coefficient [47] (Dice obj ) and the Aggregated Jaccard Index (AJI) [10]. Dice obj is defined as\nDice obj (G, S) = 1 2 n G i=1 \u03b3 i Dice(G i , S * (G i )) + 1 2 n S j=1 \u03b7 j Dice(G * (S j ), S j )(12)\nwhere \u03b3 i , \u03b7 j are the weights related to object areas, G, S are the set of ground-truth objects and segmented objects, S * (G i ), G * (S i ) are the segmented object that has maximum overlapping area with G i and ground-truth object that has maximum overlapping area with S i , respectively. The correspondence is established if the overlapping area of two objects are more than 50%. The AJI is defined as\nAJI = n G i=1 |G i \u2229 S(G i )| n G i=1 |G i \u222a S(G i )| + k\u2208K |S k |(13)\nwhere S(G i ) is the segmented object that has maximum overlap with G i based on the Jaccard index, K is the set containing segmentation objects that have not been assigned to any ground-truth object.\n\n2) Implementation details: In weakly supervised settings we train a model for 100 epochs with a learning rate of 1e-4, and fine-tune the model using the dense CRF loss for 20 epochs with a learning rate of 1e-5. In fully supervised settings, we train 200 epochs using binary masks with a learning rate of 1e-4. The validation set is used to select the best model for testing.\n\n\n3) Results and discussion:\n\na) The effects of two types of labels: In order to explore how the two types of generated labels work on the model training, we change the values of \u03b1 in Eqn. (6). As \u03b1 changes from 0 to 1, all four metrics increase in the beginning and then decrease (shown in Fig. 6). Compared to the results using only the cluster labels (\u03b1 = 0), those with Voronoi labels (\u03b1 = 1) are better in the object-level metrics, but worse in pixellevel metrics. This is because the model trained with Voronoi labels predicts the central parts of nuclei, resulting in small separated instances (Fig. 5(g)). While lacking the Voronoi edge information, the model using cluster labels is not able to separate close nuclei (Fig. 5(c)). In contrast, segmentation     results using both labels (Fig. 5(d)-(f)) are better than those with either label alone, because they have both the shape information from the cluster label and the nuclei/background information from the Voronoi label. The best performance is achieved when \u03b1 is around 0.5, thus we set \u03b1 = 0.5 for all subsequent experiments.\n\nb) The effects of dense CRF loss: In the dense CRF loss, the Gaussian bandwidth parameters \u03c3 pq and \u03c3 rgb control the affinity between pixel pairs, thus having an impact on the effect of the loss along with the weight \u03b2. We perform an ablation study on the three parameters to show how their values affect the segmentation performance. The ranges are \u03c3 pq \u2208 {3, 6, 9, 12, 15}, \u03c3 rgb \u2208 {0.05, 0.1, 0.2, 0.3} and \u03b2 \u2208 {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}. Among different value combinations, the best for LC dataset is \u03c3 pq = 9, \u03c3 rgb = 0.2, \u03b2 = 0.001, and that of MO dataset is \u03c3 pq = 9, \u03c3 rgb = 0.1, \u03b2 = 0.005.\n\nThe results of varying the value of one parameter based on the best combination are shown in Fig. 7. For simplicity, we plot the improvements of the finetuned models over the baseline. When \u03c3 pq is small, only pixel pairs that are close enough have a large affinity. As a result, the current pixel's prediction is affected by local pixels, which leads to good results for small objects. On the contrary, a large \u03c3 pq is  good for large objects. Taking all nuclei into account, \u03c3 pq should not be too small or too large, which is revealed by the results in Fig. 7. The rule is the same for \u03c3 rgb . The weight \u03b2 adjusts the importance of the unary and pairwise potentials in the loss (Eqn. (10)). A large \u03b2 emphasizes the pairwise relationship obtained from the image during the finetuning process, biasing the baseline model trained on the labels. Therefore, the performance degrades a lot, especially on the LC dataset. While a small \u03b2 makes the fine-tuning less effective.\n\nc) Results using ground-truth points: With the above parameter settings, the results using all ground-truth points are shown in Table III. The segmentation performance of our weakly supervised method using all ground-truth points is close to that of the fully supervised models with the same network structure. On the Lung Cancer dataset, the gaps for accuracy, F1 score, Dice and AJI are 2.0%, 7.2%, 5.9%, 6.9%, respectively. On the MultiOrgan dataset, the gaps for accuracy and F1 score are 1.1% and 4.7%, respectively. However, the fully supervised model has very low Dice and AJI, since for fair comparison we didn't perform post-processing to separate the touching nuclei for any of the methods. The weakly supervised model is able to separate most of them due to the Voronoi labels while the fully supervised model failed to achieve this. Compared to the CNN3 method in [10], our method achieved a similar accuracy in terms of the AJI value. Compared to the state-of-the-art DIST method [13], our approach has a higher pixel-level F1 score, but still has room for improvement on the nuclear shapes, as indicated by the AJI values.\n\n\nD. Segmentation results using detected points\n\nThe settings, including parameters in the loss function and training details, are the same as those using groundtruth points. The only difference is that compared to the ground-truth points there are errors in the detected points, i.e., false positives, false negatives and localization errors. As a result, the errors in the generated Voronoi labels and cluster labels using detected points increase, which will degrade the performance.\n\nWe report the results using detected points from different ratios of initial points annotation in Table III. Even with only 5% annotated points, the proposed framework can achieve satisfactory segmentation performance compared to the fullysupervised ones. As the annotated points increase from 5% to 50%, the overall segmentation performance becomes better. This is quite reasonable because the performance is affected by the detection results and the detection error decreases for higher annotated points ratio, as shown in Table II.\n\nTo explore the underlying factors that affect the performance, we compute two statistical metrics of the datasets. One is the average difference between the pixel values of nuclei and its surrounding background (nuclei-bg-diff ) and defined as:\nnuclei-bg-diff = N i=1 A i A total (\u00b5 nucleus i \u2212 \u00b5 bg i ),(14)\n(a) image (b) true mask (c) 5% (d) 10% (e) 25% (f) 50% (g) gt points  where N is the number of nuclei in the image, A i and A total are the areas of the i-th nucleus and all nuclei, \u00b5 nucleus i and \u00b5 bg i are the average pixel values of the i-th nucleus and its surrounding background. We treat the annular area with radius 3 around the nucleus as its background area. The larger the nuclei-bg-diff is, the better the algorithm recognizes each nucleus. The other metric is the standard deviation of pixel values within each nucleus (nuclei-std), and defined as:\nnuclei-std = N i=1 A i A total \u03c3 nucleus i ,(15)\nwhere \u03c3 i is the standard deviation of pixel values within the i-th nucleus. The smaller the nuclei-std is, the more uniform color in each nucleus, which improves segmentation of the entire nucleus. We show the segmentation results of all test images of LC and MO datasets in Fig. 9, respectively. It can be observed that images with large nuclei-bg-diff and small nuclei-std have much better segmentation performance, e.g., KZ-5 in LC dataset and Kidney2 in MO dataset. Besides, for those with small nuclei-bg-diff and large nuclei-std, the performance gap between partial points and ground-truth points are larger than other images, e.g., MV-5 in LC dataset and Prostate1 in MO dataset. Because the nuclei in these images have similar appearance as background pixels and large color variance, thus are hard to be accurately extracted and more sensitive to the errors when using partial points. Typical segmentation results of both datasets are shown in Fig. 8 and Fig. 10.\n\n\nE. Sensitivity and generalization analyses\n\nTwo more experiments are conducted to analyze the sensitivity of our method and the generalization performance of the trained models. 1) Sensitivity: To explore how the initial selected points will affect the final performance of our method, we randomly select ten different sets of 10% initial points, and perform detection and segmentation. The results are reported in Table IV and  Table V. The small variances in the metrics indicate that our method is not sensitive to the choice of initial points.   2) Generalization: Generalization performance is an important aspect when applying a method to other datasets. We use the best segmentation model trained on one dataset (LC/MO) to test its performance on the other dataset (MO/LC). The results are shown in Table VI. When applying the MO models to LC test set (MO \u2192 LC), they can achieve 92% to 99% performance compared to the models trained on the LC dataset. On the MO test set, the models trained on the LC dataset can achieve 95% to 99% performance compared to the models trained on the MO dataset. The results illustrate the good generalization performance of our model on different nuclei datasets.\n\n\nF. Annotation time\n\nDr. Riedlinger, a board-certified pathologist annotated eight images (one per case) in the LC dataset using points, bounding boxes and full masks, respectively. The average time spent on each image (about 600 nuclei on average) for full masks is 115 minutes, while for bounding boxes 67 minutes. However, it only takes about 14 minutes for all points annotation and less than 2 minutes for 10% points annotation.\n\n\nVI. CONCLUSION\n\nIn this paper, we present a new weakly supervised nuclei segmentation method using only a small portion of nuclei locations. In the first stage, a semi-supervised nuclei detection algorithm is proposed to obtain the locations of all nuclei from the partial annotation. In the second stage, we perform nuclei segmentation using the detected points as weak labels. We generate the Voronoi label and cluster label from the detected points and take advantage of the dense CRF loss to refine the trained model. Our method achieves comparable performance as fully supervised methods while requiring much less annotation effort which in turn allows us to analyze large amounts of data.\n\nFig. 1 .\n1Overview of the proposed method. (a) Semi-supervised nuclei detection module: (b1) initial training step in which an initial detector is trained using the partial points annotation, and (b2) self-training step using background propagation. (b) Weakly supervised nuclei segmentation module: (b1) label generation in which the Voronoi label and cluster label are generated using the detected points and original image, (b2) model training using the cross entropy loss, and (b3) model refinement using the CRF loss.\n\nFig. 2 .\n2Training masks. The orange color indicates ignored pixels, black is background and green is Gaussian masks centered at labeled points. (a) image, (b)-(d) masks used for training in round 1, 2 3 of self-training, respectively.\n\nFig. 3 .\n3Label generation. (a) image, (b) ground-truth nuclei masks (in green) and Voronoi edges (in red), (c) distance map.\n\nF 1 =\n12T P/(2T P + F P + F N ), where T P, F P, F N are the number of true positives, false positives and false negatives, respectively. A detected nucleus is a true positive if it locates\n\nFig. 4 .\n4Typical detection results of different strategies from Lung Cancer dataset (first row) and Multi-Organ dataset (second row) using 10% points. (a) Fully supervised training, (b) initial training using simple Gaussian mask, (c) initial training using extended Gaussian mask, (d) selftraining using nuclei prediction, (e) self training using background propagation. Yellow dots are the detected nuclei. Green, blue and red circles represent ground-truth with correct detection (TP), ground-truth without correct detection (FN) and false positive detection (FP), respectively.\n\nFig. 5 .\n5Typical results using different weights (\u03b1) of the cluster and Voronoi labels on LC dataset (top row) and MO dataset (bottom row). (a) image, (b) ground-truth full mask, (c)-(g) are results using different \u03b1 values. \u03b1 = 0 means using only the cluster label and \u03b1 = 1 means using only the Voronoi label.\n\nFig. 6 .\n6The results of using different \u03b1 values on LC and MO datasets.\n\nFig. 7 .\n7The improvements over the baseline using different \u03c3pq, \u03c3 rgb and \u03b2 values in the CRF loss for LC and MO datasets. In each subfigure, the horizontal black dash line indicates the baseline's performance, and the vertical red dash line indicates the parameter value of the best combination.\n\nFig. 8 .\n8Typical segmentation results using different ratios of points annotation on LC dataset (top row) and MO dataset (bottom row). (a) image, (b) ground-truth full mask, (c)-(f) are results using different ratios of points, (g) the results of using all ground-truth (100%) points without the detection step. Distinct colors represent different nuclei.\n\nFig. 9 .\n9The results of all test images using partial points and groundtruth points (100%) in the LC dataset (first row) and MO dataset (second row). nuclei-bg-diff is the difference between pixel values of nuclei and background. nuclei-std is the standard deviation of the pixel values within nuclei.\n\nFig. 10 .\n10Typical bad segmentation results using different ratios of points annotation on LC dataset (top row) and MO dataset (bottom row). (a) image, (b) ground-truth full mask, (c)-(f) are results using different ratios of points, (g) the results of using all ground-truth (100%) points without the detection step. Distinct colors represent different nuclei.\n\nTABLE I NUCLEI\nIDETECTION RESULTS OF DIFFERENT STRATEGIES ON LC AND MO DATASETS USING 10% PARTIAL POINTS ANNOTATION.TABLE II NUCLEI DETECTION RESULTS ON LC AND MO DATASETS USING DIFFERENT RATIOS OF ANNOTATION.Dataset Method \nP \nR \nF1 \n\u00b5 d \n\u03c3 d \n\nLC \n\nFull \n0.8767 \n0.9141 \n0.8950 \n1.14 1.03 \nGM \n0.6322 0.6337 0.6329 2.34 2.00 \next-GM 0.7483 \n0.9306 \n0.8296 1.74 1.45 \nST-nu \n0.7505 0.9016 0.8192 1.70 1.49 \nST-bg \n0.8605 \n0.9171 \n0.8879 1.42 1.22 \n\nMO \n\nFull \n0.8420 \n0.8665 \n0.8541 \n2.68 2.00 \nGM \n0.5574 0.7650 0.6449 3.85 2.68 \next-GM 0.7932 \n0.8471 \n0.8193 \n2.87 \n2.18 \nST-nu \n0.7754 0.8138 0.7941 2.95 2.17 \nST-bg \n0.8238 \n0.8328 \n0.8282 \n2.90 \n2.07 \n\nDataset Method \nP \nR \nF1 \n\u00b5 d \n\u03c3 d \n\nLC \n\nFull \n0.8767 0.9141 \n0.8950 \n1.14 \n1.03 \n5% \n0.8564 0.9171 0.8857 1.53 1.30 \n10% \n0.8605 \n0.9171 0.8879 1.42 1.22 \n25% \n0.8517 0.9399 \n0.8936 \n1.35 1.17 \n50% \n0.8502 0.9414 \n0.8935 \n1.30 1.12 \n\nMO \n\nFull \n0.8420 0.8665 \n0.8541 \n2.68 \n2.00 \n5% \n0.8021 0.8441 0.8226 3.04 2.13 \n10% \n0.8238 0.8328 0.8282 2.90 2.07 \n25% \n0.8259 \n0.8440 0.8349 2.97 2.05 \n50% \n0.8237 0.8821 0.8519 2.76 2.03 \n\n\n\nTABLE III NUCLEI\nIIISEGMENTATION RESULTS ON LC AND MO DATASETS FOR OUR PARTIAL POINTS ANNOTATION, FULLY-SUPERVISED AND STATE-OF-THE-ARTS.Dataset Method \nPixel-level \nObject-level \n\nAcc \nF1 \nDice obj \nAJI \n\nLC \n\nFully-sup \n0.9615 0.8771 \n0.8521 \n0.6979 \nGT points \n0.9427 0.8143 \n0.8021 \n0.6497 \n\n5% \n0.9262 0.7612 \n0.7470 \n0.5742 \n10% \n0.9312 0.7700 \n0.7574 \n0.5754 \n25% \n0.9331 0.7768 \n0.7653 \n0.6003 \n50% \n0.9332 0.7819 \n0.7704 \n0.6120 \n\nMO \n\nCNN3 [10] \n-\n-\n-\n0.5083 \nDIST [11] \n-\n0.7623 \n-\n0.5598 \nFully-sup \n0.9194 0.8100 \n0.6763 \n0.3919 \nGT points \n0.9097 0.7716 \n0.7242 \n0.5174 \n\n5% \n0.8951 0.7540 \n0.7015 \n0.4941 \n10% \n0.8997 0.7490 \n0.7033 \n0.5031 \n25% \n0.8966 0.7511 \n0.7087 \n0.5120 \n50% \n0.8999 0.7566 \n0.7157 \n0.5160 \n\n\n\nTABLE IV NUCLEI\nIVDETECTION RESULTS ON LC AND MO DATASETS USING 10 DIFFERENT SETS OF 10% INITIAL POINTS.TABLE V NUCLEI SEGMENTATION RESULTS ON LC AND MO DATASETS USING 10 DIFFERENT SETS OF 10% INITIAL POINTS.Dataset \nP \nR \nF1 \n\u00b5 d \n\u03c3 d \n\nLC \nmean \n0.8544 \n0.9225 \n0.8868 \n1.43 \n1.23 \nstd \n0.02057 0.02012 0.00423 0.068 0.043 \n\nMO \nmean \n0.8227 \n0.8453 \n0.8330 \n2.92 \n2.11 \nstd \n0.03744 0.02148 0.01041 0.079 0.066 \n\nDataset \nPixel-level \nObject-level \n\nAcc \nF1 \nDice obj \nAJI \n\nLC \nmean \n0.9278 \n0.7695 \n0.7571 \n0.5880 \nstd \n0.00264 0.00381 0.00443 0.00782 \n\nMO \nmean \n0.8982 \n0.7484 \n0.7089 \n0.5158 \nstd \n0.00472 0.00753 0.00666 0.00413 \n\n\n\nTABLE VI GENERALIZATION\nVIPERFORMANCE OF OUR METHOD ON LC AND MO DATASETS.Train \u2192 Test Ratio \nPixel-level \nObject-level \n\nAcc \nF1 \nDice obj \nAJI \n\nMO \u2192 LC \n\n5% \n0.9271 0.7589 \n0.7418 \n0.5608 \n10% \n0.9213 0.7518 \n0.7297 \n0.5555 \n25% \n0.9222 0.7551 \n0.7320 \n0.5588 \n50% \n0.9226 0.7559 \n0.7336 \n0.5608 \n\nLC \u2192 MO \n\n5% \n0.9004 0.7419 \n0.7028 \n0.4884 \n10% \n0.8964 0.7338 \n0.6913 \n0.4971 \n25% \n0.8974 0.7234 \n0.6886 \n0.4870 \n50% \n0.8970 0.7232 \n0.6986 \n0.5030 \n\n\nCopyright (c) 2020 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\nThis is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.The final version of record is available at http://dx.doi.org/10.1109/TMI.2020.3002244Copyright (c) 2020 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.\n\nHistopathological image analysis: A review. M N Gurcan, L Boucheron, A Can, A Madabhushi, N Rajpoot, B Yener, IEEE reviews in Biomedical Engineering. 2147M. N. Gurcan, L. Boucheron, A. Can, A. Madabhushi, N. Rajpoot, and B. Yener, \"Histopathological image analysis: A review,\" IEEE reviews in Biomedical Engineering, vol. 2, p. 147, 2009.\n\nDeep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases. A Janowczyk, A Madabhushi, Journal of Pathology Informatics. 7A. Janowczyk and A. Madabhushi, \"Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases,\" Journal of Pathology Informatics, vol. 7, 2016.\n\nRobust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review. F Xing, L Yang, IEEE Reviews in Biomedical Engineering. 9F. Xing and L. Yang, \"Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review,\" IEEE Reviews in Biomedical Engineering, vol. 9, pp. 234-263, 2016.\n\nTowards largescale histopathological image analysis: Hashing-based image retrieval. X Zhang, W Liu, M Dundar, S Badve, S Zhang, IEEE Transactions on Medical Imaging. 342X. Zhang, W. Liu, M. Dundar, S. Badve, and S. Zhang, \"Towards large- scale histopathological image analysis: Hashing-based image retrieval,\" IEEE Transactions on Medical Imaging, vol. 34, no. 2, pp. 496-506, 2014.\n\nAutomatic nuclei segmentation in h&e stained breast cancer histopathology images. M Veta, P J Van Diest, R Kornegoor, A Huisman, M A Viergever, J P Pluim, PloS one. 87M. Veta, P. J. Van Diest, R. Kornegoor, A. Huisman, M. A. Viergever, and J. P. Pluim, \"Automatic nuclei segmentation in h&e stained breast cancer histopathology images,\" PloS one, vol. 8, no. 7, 2013.\n\nImproved automatic detection and segmentation of cell nuclei in histopathology images. Y Al-Kofahi, W Lassoued, W Lee, B Roysam, IEEE Transactions on Biomedical Engineering. 574Y. Al-Kofahi, W. Lassoued, W. Lee, and B. Roysam, \"Improved au- tomatic detection and segmentation of cell nuclei in histopathology images,\" IEEE Transactions on Biomedical Engineering, vol. 57, no. 4, pp. 841-852, 2010.\n\nPartitioning histopathological images: an integrated framework for supervised colortexture segmentation and cell splitting. H Kong, M Gurcan, K Belkacem-Boussaid, IEEE Transactions on Medical Imaging. 309H. Kong, M. Gurcan, and K. Belkacem-Boussaid, \"Partitioning histopathological images: an integrated framework for supervised color- texture segmentation and cell splitting,\" IEEE Transactions on Medical Imaging, vol. 30, no. 9, pp. 1661-1677, 2011.\n\nHigh-throughput histopathological image analysis via robust cell segmentation and hashing. X Zhang, F Xing, H Su, L Yang, S Zhang, Medical image analysis. 261X. Zhang, F. Xing, H. Su, L. Yang, and S. Zhang, \"High-throughput histopathological image analysis via robust cell segmentation and hash- ing,\" Medical image analysis, vol. 26, no. 1, pp. 306-315, 2015.\n\nAn automatic learning-based framework for robust nucleus segmentation. F Xing, Y Xie, L Yang, IEEE transactions on Medical Imaging. 352F. Xing, Y. Xie, and L. Yang, \"An automatic learning-based framework for robust nucleus segmentation,\" IEEE transactions on Medical Imag- ing, vol. 35, no. 2, pp. 550-566, 2016.\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, IEEE Transactions on Medical Imaging. 367N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Vahadane, and A. Sethi, \"A dataset and a technique for generalized nuclear segmen- tation for computational pathology,\" IEEE Transactions on Medical Imaging, vol. 36, no. 7, pp. 1550-1560, 2017.\n\nNuclei segmentation in histopathology images using deep neural networks. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE 14th International Symposium on Biomedical Imaging. IEEEP. Naylor, M. La\u00e9, F. Reyal, and T. Walter, \"Nuclei segmentation in histopathology images using deep neural networks,\" in IEEE 14th International Symposium on Biomedical Imaging. IEEE, 2017.\n\nDeep adversarial training for multi-organ nuclei segmentation in histopathology images. F Mahmood, D Borders, R Chen, G N Mckay, K J Salimian, A Baras, IEEE transactions on medical imaging. F. Mahmood, D. Borders, R. Chen, G. N. McKay, K. J. Salimian, and A. Baras, et al., \"Deep adversarial training for multi-organ nuclei segmentation in histopathology images,\" IEEE transactions on medical imaging, 2019.\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE Transactions on Medical Imaging. P. Naylor, M. La\u00e9, F. Reyal, and T. Walter, \"Segmentation of nuclei in histopathology images by deep regression of the distance map,\" IEEE Transactions on Medical Imaging, 2018.\n\nMicro-net: A unified model for segmentation of various objects in microscopy images. S E A Raza, L Cheung, M Shaban, S Graham, D Epstein, S Pelengaris, Medical Image Analysis. 52S. E. A. Raza, L. Cheung, M. Shaban, S. Graham, D. Epstein, and S. Pelengaris, et al., \"Micro-net: A unified model for segmentation of various objects in microscopy images,\" Medical Image Analysis, vol. 52, pp. 160-173, 2019.\n\nRobust histopathology image analysis: To label or to synthesize?. L Hou, A Agarwal, D Samaras, T M Kurc, R R Gupta, J H Saltz, IEEE Conference on Computer Vision and Pattern Recognition. L. Hou, A. Agarwal, D. Samaras, T. M. Kurc, R. R. Gupta, and J. H. Saltz, \"Robust histopathology image analysis: To label or to synthesize?\" in IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nAccurate nuclear segmentation with center vector encoding. J Li, Z Hu, S Yang, International Conference on Information Processing in Medical Imaging. SpringerJ. Li, Z. Hu, and S. Yang, \"Accurate nuclear segmentation with center vector encoding,\" in International Conference on Information Processing in Medical Imaging. Springer, 2019, pp. 394-404.\n\nImproving nuclei/gland instance segmentation in histopathology images by full resolution neural network and spatial constrained loss. H Qu, Z Yan, G M Riedlinger, S De, D N Metaxas, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerH. Qu, Z. Yan, G. M. Riedlinger, S. De, and D. N. Metaxas, \"Improving nuclei/gland instance segmentation in histopathology images by full resolution neural network and spatial constrained loss,\" in International Conference on Medical Image Computing and Computer-Assisted Inter- vention. Springer, 2019, pp. 378-386.\n\nNuclei segmentation using mixed points and masks selected from uncertainty. H Qu, J Yi, Q Huang, P Wu, D Metaxas, 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEEH. Qu, J. Yi, Q. Huang, P. Wu, and D. Metaxas, \"Nuclei segmentation using mixed points and masks selected from uncertainty,\" in 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI). IEEE, 2020, pp. 973-976.\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. L Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, International Conference on Learning Representations. L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, \"Semantic image segmentation with deep convolutional nets and fully connected crfs,\" in International Conference on Learning Representa- tions, 2015.\n\nBoxnet: Deep learning based biomedical image segmentation using boxes only annotation. L Yang, Y Zhang, Z Zhao, H Zheng, P Liang, M T Ying, arXiv:1806.00593arXiv preprintL. Yang, Y. Zhang, Z. Zhao, H. Zheng, P. Liang, and M. T. Ying, et al., \"Boxnet: Deep learning based biomedical image segmentation using boxes only annotation,\" arXiv preprint arXiv:1806.00593, 2018.\n\nOn regularized losses for weakly-supervised cnn segmentation. M Tang, F Perazzi, A Djelouah, I Ben Ayed, C Schroers, Y Boykov, European Conference on Computer Vision. M. Tang, F. Perazzi, A. Djelouah, I. Ben Ayed, C. Schroers, and Y. Boykov, \"On regularized losses for weakly-supervised cnn segmen- tation,\" in European Conference on Computer Vision, 2018.\n\nWeakly supervised deep nuclei segmentation using points annotation in histopathology images. H Qu, P Wu, Q Huang, J Yi, G M Riedlinger, S De, International Conference on Medical Imaging with Deep Learning. H. Qu, P. Wu, Q. Huang, J. Yi, G. M. Riedlinger, and S. De, et al., \"Weakly supervised deep nuclei segmentation using points annotation in histopathology images,\" in International Conference on Medical Imaging with Deep Learning, 2019.\n\nMitosis detection in breast cancer histology images with deep neural networks. D C Cire\u015fan, A Giusti, L M Gambardella, J Schmidhuber, International Conference on Medical Image Computing and Computer-Assisted Intervention. D. C. Cire\u015fan, A. Giusti, L. M. Gambardella, and J. Schmidhuber, \"Mitosis detection in breast cancer histology images with deep neural networks,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2013.\n\nBeyond classification: structured regression for robust cell detection using convolutional neural network. Y Xie, F Xing, X Kong, H Su, L Yang, International Conference on Medical Image Computing and Computer-Assisted Intervention. Y. Xie, F. Xing, X. Kong, H. Su, and L. Yang, \"Beyond classification: structured regression for robust cell detection using convolutional neural network,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015.\n\nLocality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. K Sirinukunwattana, S Ahmed Raza, Y.-W Tsang, D R Snead, I A Cree, N M Rajpoot, IEEE Trans. Med. Imaging. 355K. Sirinukunwattana, S. e Ahmed Raza, Y.-W. Tsang, D. R. Snead, I. A. Cree, and N. M. Rajpoot, \"Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images.\" IEEE Trans. Med. Imaging, vol. 35, no. 5, pp. 1196-1206, 2016.\n\nStacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images. J Xu, L Xiang, Q Liu, H Gilmore, J Wu, J Tang, IEEE transactions on medical imaging. 351J. Xu, L. Xiang, Q. Liu, H. Gilmore, J. Wu, and J. Tang, et al., \"Stacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images,\" IEEE transactions on medical imaging, vol. 35, no. 1, pp. 119-130, 2015.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431-3440.\n\nEfficient and robust cell detection: A structured regression approach. Y Xie, F Xing, X Shi, X Kong, H Su, L Yang, Medical image analysis. 44Y. Xie, F. Xing, X. Shi, X. Kong, H. Su, and L. Yang, \"Efficient and robust cell detection: A structured regression approach,\" Medical image analysis, vol. 44, pp. 245-254, 2018.\n\nSfcn-opi: Detection and fine-grained classification of nuclei using sibling fcn with objectness prior interaction. Y Zhou, Q Dou, H Chen, J Qin, P.-A Heng, Thirty-Second AAAI Conference on Artificial Intelligence. Y. Zhou, Q. Dou, H. Chen, J. Qin, and P.-A. Heng, \"Sfcn-opi: Detection and fine-grained classification of nuclei using sibling fcn with object- ness prior interaction,\" in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nSignet ring cell detection with a semi-supervised learning framework. J Li, S Yang, X Huang, Q Da, X Yang, Z Hu, International Conference on Information Processing in Medical Imaging. SpringerJ. Li, S. Yang, X. Huang, Q. Da, X. Yang, and Z. Hu, et al., \"Signet ring cell detection with a semi-supervised learning framework,\" in In- ternational Conference on Information Processing in Medical Imaging. Springer, 2019, pp. 842-854.\n\nRobust cell detection and segmentation in histopathological images using sparse reconstruction and stacked denoising autoencoders. H Su, F Xing, X Kong, Y Xie, S Zhang, L Yang, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerH. Su, F. Xing, X. Kong, Y. Xie, S. Zhang, and L. Yang, \"Robust cell detection and segmentation in histopathological images using sparse reconstruction and stacked denoising autoencoders,\" in International Conference on Medical Image Computing and Computer-Assisted In- tervention. Springer, 2015, pp. 383-390.\n\nJoint segmentation and fine-grained classification of nuclei in histopathology images. H Qu, G Riedlinger, P Wu, Q Huang, J Yi, S De, International Symposium on Biomedical Imaging. IEEEH. Qu, G. Riedlinger, P. Wu, Q. Huang, J. Yi, and S. De, et al., \"Joint segmentation and fine-grained classification of nuclei in histopathology images,\" in International Symposium on Biomedical Imaging. IEEE, 2019, pp. 900-904.\n\nUnpaired image-toimage translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, IEEE International Conference on Computer Vision. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \"Unpaired image-to- image translation using cycle-consistent adversarial networks,\" in IEEE International Conference on Computer Vision, 2017.\n\nWeaklyand semi-supervised learning of a deep convolutional network for semantic image segmentation. G Papandreou, L.-C Chen, K P Murphy, A L Yuille, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionG. Papandreou, L.-C. Chen, K. P. Murphy, and A. L. Yuille, \"Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation,\" in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1742-1750.\n\nConstrained convolutional neural networks for weakly supervised segmentation. D Pathak, P Krahenbuhl, T Darrell, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionD. Pathak, P. Krahenbuhl, and T. Darrell, \"Constrained convolutional neural networks for weakly supervised segmentation,\" in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1796- 1804.\n\nWhats the point: Semantic segmentation with point supervision. A Bearman, O Russakovsky, V Ferrari, L Fei-Fei, European Conference on Computer Vision. SpringerA. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei, \"Whats the point: Semantic segmentation with point supervision,\" in European Conference on Computer Vision. Springer, 2016, pp. 549-565.\n\nScribblesup: Scribblesupervised convolutional networks for semantic segmentation. D Lin, J Dai, J Jia, K He, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionD. Lin, J. Dai, J. Jia, K. He, and J. Sun, \"Scribblesup: Scribble- supervised convolutional networks for semantic segmentation,\" in Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3159-3167.\n\nBoxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation. J Dai, K He, J Sun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJ. Dai, K. He, and J. Sun, \"Boxsup: Exploiting bounding boxes to super- vise convolutional networks for semantic segmentation,\" in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1635-1643.\n\nDeepcut: Object segmentation from bounding box annotations using convolutional neural networks. M Rajchl, M C Lee, O Oktay, K Kamnitsas, J Passerat-Palmbach, W Bai, IEEE transactions on medical imaging. 362M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach, and W. Bai, et al., \"Deepcut: Object segmentation from bounding box annotations using convolutional neural networks,\" IEEE transactions on medical imaging, vol. 36, no. 2, pp. 674-683, 2017.\n\nDeep learning based instance segmentation in 3d biomedical images using weak annotation. Z Zhao, L Yang, H Zheng, I H Guldner, S Zhang, D Z Chen, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerZ. Zhao, L. Yang, H. Zheng, I. H. Guldner, S. Zhang, and D. Z. Chen, \"Deep learning based instance segmentation in 3d biomedical images using weak annotation,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2018, pp. 352-360.\n\nConstrained-cnn losses for weakly supervised segmentation. H Kervadec, J Dolz, M Tang, E Granger, Y Boykov, I B Ayed, Medical image analysis. 54H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, and I. B. Ayed, \"Constrained-cnn losses for weakly supervised segmentation,\" Medical image analysis, vol. 54, pp. 88-99, 2019.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerO. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional net- works for biomedical image segmentation,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2015, pp. 234-241.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nFast high-dimensional filtering using the permutohedral lattice. A Adams, J Baek, M A Davis, Computer Graphics Forum. 292Wiley Online LibraryA. Adams, J. Baek, and M. A. Davis, \"Fast high-dimensional filtering using the permutohedral lattice,\" in Computer Graphics Forum, vol. 29, no. 2. Wiley Online Library, 2010, pp. 753-762.\n\nColor transfer between images. E Reinhard, M Adhikhmin, B Gooch, P Shirley, IEEE Computer Graphics and Applications. 215E. Reinhard, M. Adhikhmin, B. Gooch, and P. Shirley, \"Color transfer between images,\" IEEE Computer Graphics and Applications, vol. 21, no. 5, pp. 34-41, 2001.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nA stochastic polygons model for glandular structures in colon histology images. K Sirinukunwattana, D R Snead, N M Rajpoot, IEEE transactions on Medical Imaging. 3411K. Sirinukunwattana, D. R. Snead, and N. M. Rajpoot, \"A stochastic polygons model for glandular structures in colon histology images,\" IEEE transactions on Medical Imaging, vol. 34, no. 11, pp. 2366-2378, 2015.\n", "annotations": {"author": "[{\"end\":110,\"start\":103},{\"end\":124,\"start\":111},{\"end\":140,\"start\":125},{\"end\":151,\"start\":141},{\"end\":164,\"start\":152},{\"end\":173,\"start\":165},{\"end\":195,\"start\":174},{\"end\":210,\"start\":196},{\"end\":226,\"start\":211},{\"end\":258,\"start\":227}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":107},{\"end\":123,\"start\":121},{\"end\":139,\"start\":134},{\"end\":150,\"start\":148},{\"end\":163,\"start\":160},{\"end\":172,\"start\":170},{\"end\":194,\"start\":184},{\"end\":209,\"start\":207},{\"end\":225,\"start\":220},{\"end\":257,\"start\":250}]", "author_first_name": "[{\"end\":106,\"start\":103},{\"end\":120,\"start\":111},{\"end\":133,\"start\":125},{\"end\":147,\"start\":141},{\"end\":159,\"start\":152},{\"end\":169,\"start\":165},{\"end\":181,\"start\":174},{\"end\":183,\"start\":182},{\"end\":206,\"start\":196},{\"end\":219,\"start\":211},{\"end\":247,\"start\":239},{\"end\":249,\"start\":248}]", "author_affiliation": null, "title": "[{\"end\":100,\"start\":1},{\"end\":358,\"start\":259}]", "venue": "[{\"end\":396,\"start\":360}]", "abstract": "[{\"end\":2442,\"start\":636}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3281,\"start\":3278},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3286,\"start\":3283},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3967,\"start\":3964},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3972,\"start\":3969},{\"end\":4463,\"start\":4456},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4471,\"start\":4468},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4614,\"start\":4611},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4795,\"start\":4792},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4801,\"start\":4797},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6999,\"start\":6995},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7020,\"start\":7016},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7140,\"start\":7136},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7411,\"start\":7407},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8137,\"start\":8133},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8245,\"start\":8241},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8373,\"start\":8369},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8442,\"start\":8438},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8815,\"start\":8811},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9094,\"start\":9090},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9166,\"start\":9162},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9410,\"start\":9406},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9851,\"start\":9847},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9924,\"start\":9921},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10100,\"start\":10096},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10295,\"start\":10291},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10452,\"start\":10448},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10618,\"start\":10614},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10677,\"start\":10673},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10698,\"start\":10694},{\"end\":11163,\"start\":11162},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11287,\"start\":11283},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11533,\"start\":11529},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11791,\"start\":11787},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12008,\"start\":12004},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12014,\"start\":12010},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12041,\"start\":12037},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12047,\"start\":12043},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12069,\"start\":12065},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13005,\"start\":13001},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13785,\"start\":13781},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13836,\"start\":13832},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14824,\"start\":14820},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14870,\"start\":14866},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16159,\"start\":16155},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24781,\"start\":24777},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24800,\"start\":24796},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24818,\"start\":24814},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25050,\"start\":25046},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26395,\"start\":26391},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27032,\"start\":27028},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27656,\"start\":27652},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27809,\"start\":27805},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":29470,\"start\":29466},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":29913,\"start\":29909},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33854,\"start\":33851},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34296,\"start\":34292},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34352,\"start\":34348},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35729,\"start\":35726},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39105,\"start\":39101},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39222,\"start\":39218}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45142,\"start\":44619},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45379,\"start\":45143},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45506,\"start\":45380},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45697,\"start\":45507},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46281,\"start\":45698},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46595,\"start\":46282},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46669,\"start\":46596},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46969,\"start\":46670},{\"attributes\":{\"id\":\"fig_11\"},\"end\":47327,\"start\":46970},{\"attributes\":{\"id\":\"fig_12\"},\"end\":47631,\"start\":47328},{\"attributes\":{\"id\":\"fig_14\"},\"end\":47995,\"start\":47632},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49086,\"start\":47996},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49818,\"start\":49087},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50460,\"start\":49819},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50917,\"start\":50461}]", "paragraph": "[{\"end\":3919,\"start\":2444},{\"end\":5259,\"start\":3921},{\"end\":6283,\"start\":5261},{\"end\":7357,\"start\":6285},{\"end\":7445,\"start\":7359},{\"end\":7718,\"start\":7447},{\"end\":7973,\"start\":7739},{\"end\":9651,\"start\":7975},{\"end\":10925,\"start\":9653},{\"end\":13205,\"start\":10989},{\"end\":13455,\"start\":13244},{\"end\":14041,\"start\":13508},{\"end\":14685,\"start\":14134},{\"end\":15101,\"start\":14687},{\"end\":15587,\"start\":15144},{\"end\":16644,\"start\":15636},{\"end\":17796,\"start\":16646},{\"end\":18244,\"start\":17893},{\"end\":18825,\"start\":18277},{\"end\":18941,\"start\":18827},{\"end\":19388,\"start\":18987},{\"end\":19775,\"start\":19390},{\"end\":20532,\"start\":19777},{\"end\":21229,\"start\":20534},{\"end\":21789,\"start\":21231},{\"end\":22282,\"start\":21828},{\"end\":23277,\"start\":22284},{\"end\":23738,\"start\":23279},{\"end\":24174,\"start\":23799},{\"end\":24397,\"start\":24256},{\"end\":24469,\"start\":24436},{\"end\":25460,\"start\":24514},{\"end\":25824,\"start\":25510},{\"end\":26290,\"start\":25902},{\"end\":26396,\"start\":26292},{\"end\":26484,\"start\":26434},{\"end\":27072,\"start\":26486},{\"end\":27203,\"start\":27091},{\"end\":27573,\"start\":27205},{\"end\":27982,\"start\":27575},{\"end\":28221,\"start\":27984},{\"end\":28388,\"start\":28270},{\"end\":29161,\"start\":28390},{\"end\":29417,\"start\":29235},{\"end\":30343,\"start\":29419},{\"end\":30429,\"start\":30345},{\"end\":30492,\"start\":30431},{\"end\":32684,\"start\":30494},{\"end\":33634,\"start\":32686},{\"end\":34007,\"start\":33680},{\"end\":34376,\"start\":34009},{\"end\":34887,\"start\":34479},{\"end\":35159,\"start\":34959},{\"end\":35536,\"start\":35161},{\"end\":36631,\"start\":35567},{\"end\":37248,\"start\":36633},{\"end\":38223,\"start\":37250},{\"end\":39361,\"start\":38225},{\"end\":39848,\"start\":39411},{\"end\":40384,\"start\":39850},{\"end\":40630,\"start\":40386},{\"end\":41256,\"start\":40695},{\"end\":42280,\"start\":41306},{\"end\":43486,\"start\":42327},{\"end\":43921,\"start\":43509},{\"end\":44618,\"start\":43940}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14133,\"start\":14042},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15143,\"start\":15102},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17892,\"start\":17797},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21827,\"start\":21790},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24255,\"start\":24175},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24435,\"start\":24398},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25509,\"start\":25461},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25901,\"start\":25825},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26433,\"start\":26397},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29234,\"start\":29162},{\"attributes\":{\"id\":\"formula_10\"},\"end\":34478,\"start\":34377},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34958,\"start\":34888},{\"attributes\":{\"id\":\"formula_12\"},\"end\":40694,\"start\":40631},{\"attributes\":{\"id\":\"formula_13\"},\"end\":41305,\"start\":41257}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31052,\"start\":31045},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33034,\"start\":33026},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38362,\"start\":38353},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39957,\"start\":39948},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40383,\"start\":40375},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42719,\"start\":42698},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43097,\"start\":43089}]", "section_header": "[{\"end\":7737,\"start\":7721},{\"end\":10987,\"start\":10928},{\"end\":13242,\"start\":13208},{\"end\":13506,\"start\":13458},{\"end\":15634,\"start\":15590},{\"end\":18275,\"start\":18247},{\"end\":18985,\"start\":18944},{\"end\":23797,\"start\":23741},{\"end\":24512,\"start\":24472},{\"end\":27089,\"start\":27075},{\"end\":28268,\"start\":28224},{\"end\":33678,\"start\":33637},{\"end\":35565,\"start\":35539},{\"end\":39409,\"start\":39364},{\"end\":42325,\"start\":42283},{\"end\":43507,\"start\":43489},{\"end\":43938,\"start\":43924},{\"end\":44628,\"start\":44620},{\"end\":45152,\"start\":45144},{\"end\":45389,\"start\":45381},{\"end\":45513,\"start\":45508},{\"end\":45707,\"start\":45699},{\"end\":46291,\"start\":46283},{\"end\":46605,\"start\":46597},{\"end\":46679,\"start\":46671},{\"end\":46979,\"start\":46971},{\"end\":47337,\"start\":47329},{\"end\":47642,\"start\":47633},{\"end\":48011,\"start\":47997},{\"end\":49104,\"start\":49088},{\"end\":49835,\"start\":49820},{\"end\":50485,\"start\":50462}]", "table": "[{\"end\":49086,\"start\":48206},{\"end\":49818,\"start\":49225},{\"end\":50460,\"start\":50028},{\"end\":50917,\"start\":50536}]", "figure_caption": "[{\"end\":45142,\"start\":44630},{\"end\":45379,\"start\":45154},{\"end\":45506,\"start\":45391},{\"end\":45697,\"start\":45515},{\"end\":46281,\"start\":45709},{\"end\":46595,\"start\":46293},{\"end\":46669,\"start\":46607},{\"end\":46969,\"start\":46681},{\"end\":47327,\"start\":46981},{\"end\":47631,\"start\":47339},{\"end\":47995,\"start\":47645},{\"end\":48206,\"start\":48013},{\"end\":49225,\"start\":49108},{\"end\":50028,\"start\":49838},{\"end\":50536,\"start\":50488}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13331,\"start\":13322},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14887,\"start\":14881},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16693,\"start\":16687},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17922,\"start\":17916},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20199,\"start\":20190},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20530,\"start\":20521},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20713,\"start\":20705},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22550,\"start\":22540},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23308,\"start\":23299},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32020,\"start\":32013},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32806,\"start\":32800},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":35834,\"start\":35828},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36148,\"start\":36138},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36273,\"start\":36263},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36339,\"start\":36332},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":37349,\"start\":37343},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":37812,\"start\":37806},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":41588,\"start\":41582},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":42267,\"start\":42261},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42279,\"start\":42272}]", "bib_author_first_name": "[{\"end\":51520,\"start\":51519},{\"end\":51522,\"start\":51521},{\"end\":51532,\"start\":51531},{\"end\":51545,\"start\":51544},{\"end\":51552,\"start\":51551},{\"end\":51566,\"start\":51565},{\"end\":51577,\"start\":51576},{\"end\":51918,\"start\":51917},{\"end\":51931,\"start\":51930},{\"end\":52280,\"start\":52279},{\"end\":52288,\"start\":52287},{\"end\":52627,\"start\":52626},{\"end\":52636,\"start\":52635},{\"end\":52643,\"start\":52642},{\"end\":52653,\"start\":52652},{\"end\":52662,\"start\":52661},{\"end\":53009,\"start\":53008},{\"end\":53017,\"start\":53016},{\"end\":53019,\"start\":53018},{\"end\":53032,\"start\":53031},{\"end\":53045,\"start\":53044},{\"end\":53056,\"start\":53055},{\"end\":53058,\"start\":53057},{\"end\":53071,\"start\":53070},{\"end\":53073,\"start\":53072},{\"end\":53383,\"start\":53382},{\"end\":53396,\"start\":53395},{\"end\":53408,\"start\":53407},{\"end\":53415,\"start\":53414},{\"end\":53819,\"start\":53818},{\"end\":53827,\"start\":53826},{\"end\":53837,\"start\":53836},{\"end\":54240,\"start\":54239},{\"end\":54249,\"start\":54248},{\"end\":54257,\"start\":54256},{\"end\":54263,\"start\":54262},{\"end\":54271,\"start\":54270},{\"end\":54582,\"start\":54581},{\"end\":54590,\"start\":54589},{\"end\":54597,\"start\":54596},{\"end\":54917,\"start\":54916},{\"end\":54926,\"start\":54925},{\"end\":54935,\"start\":54934},{\"end\":54945,\"start\":54944},{\"end\":54957,\"start\":54956},{\"end\":54969,\"start\":54968},{\"end\":55335,\"start\":55334},{\"end\":55345,\"start\":55344},{\"end\":55352,\"start\":55351},{\"end\":55361,\"start\":55360},{\"end\":55712,\"start\":55711},{\"end\":55723,\"start\":55722},{\"end\":55734,\"start\":55733},{\"end\":55742,\"start\":55741},{\"end\":55744,\"start\":55743},{\"end\":55753,\"start\":55752},{\"end\":55755,\"start\":55754},{\"end\":55767,\"start\":55766},{\"end\":56121,\"start\":56120},{\"end\":56131,\"start\":56130},{\"end\":56138,\"start\":56137},{\"end\":56147,\"start\":56146},{\"end\":56459,\"start\":56458},{\"end\":56463,\"start\":56460},{\"end\":56471,\"start\":56470},{\"end\":56481,\"start\":56480},{\"end\":56491,\"start\":56490},{\"end\":56501,\"start\":56500},{\"end\":56512,\"start\":56511},{\"end\":56845,\"start\":56844},{\"end\":56852,\"start\":56851},{\"end\":56863,\"start\":56862},{\"end\":56874,\"start\":56873},{\"end\":56876,\"start\":56875},{\"end\":56884,\"start\":56883},{\"end\":56886,\"start\":56885},{\"end\":56895,\"start\":56894},{\"end\":56897,\"start\":56896},{\"end\":57236,\"start\":57235},{\"end\":57242,\"start\":57241},{\"end\":57248,\"start\":57247},{\"end\":57661,\"start\":57660},{\"end\":57667,\"start\":57666},{\"end\":57674,\"start\":57673},{\"end\":57676,\"start\":57675},{\"end\":57690,\"start\":57689},{\"end\":57696,\"start\":57695},{\"end\":57698,\"start\":57697},{\"end\":58199,\"start\":58198},{\"end\":58205,\"start\":58204},{\"end\":58211,\"start\":58210},{\"end\":58220,\"start\":58219},{\"end\":58226,\"start\":58225},{\"end\":58616,\"start\":58615},{\"end\":58624,\"start\":58623},{\"end\":58638,\"start\":58637},{\"end\":58650,\"start\":58649},{\"end\":58660,\"start\":58659},{\"end\":58662,\"start\":58661},{\"end\":59030,\"start\":59029},{\"end\":59038,\"start\":59037},{\"end\":59047,\"start\":59046},{\"end\":59055,\"start\":59054},{\"end\":59064,\"start\":59063},{\"end\":59073,\"start\":59072},{\"end\":59075,\"start\":59074},{\"end\":59376,\"start\":59375},{\"end\":59384,\"start\":59383},{\"end\":59395,\"start\":59394},{\"end\":59407,\"start\":59406},{\"end\":59419,\"start\":59418},{\"end\":59431,\"start\":59430},{\"end\":59765,\"start\":59764},{\"end\":59771,\"start\":59770},{\"end\":59777,\"start\":59776},{\"end\":59786,\"start\":59785},{\"end\":59792,\"start\":59791},{\"end\":59794,\"start\":59793},{\"end\":59808,\"start\":59807},{\"end\":60194,\"start\":60193},{\"end\":60196,\"start\":60195},{\"end\":60207,\"start\":60206},{\"end\":60217,\"start\":60216},{\"end\":60219,\"start\":60218},{\"end\":60234,\"start\":60233},{\"end\":60688,\"start\":60687},{\"end\":60695,\"start\":60694},{\"end\":60703,\"start\":60702},{\"end\":60711,\"start\":60710},{\"end\":60717,\"start\":60716},{\"end\":61184,\"start\":61183},{\"end\":61204,\"start\":61203},{\"end\":61221,\"start\":61217},{\"end\":61230,\"start\":61229},{\"end\":61232,\"start\":61231},{\"end\":61241,\"start\":61240},{\"end\":61243,\"start\":61242},{\"end\":61251,\"start\":61250},{\"end\":61253,\"start\":61252},{\"end\":61667,\"start\":61666},{\"end\":61673,\"start\":61672},{\"end\":61682,\"start\":61681},{\"end\":61689,\"start\":61688},{\"end\":61700,\"start\":61699},{\"end\":61706,\"start\":61705},{\"end\":62047,\"start\":62046},{\"end\":62055,\"start\":62054},{\"end\":62068,\"start\":62067},{\"end\":62492,\"start\":62491},{\"end\":62499,\"start\":62498},{\"end\":62507,\"start\":62506},{\"end\":62514,\"start\":62513},{\"end\":62522,\"start\":62521},{\"end\":62528,\"start\":62527},{\"end\":62857,\"start\":62856},{\"end\":62865,\"start\":62864},{\"end\":62872,\"start\":62871},{\"end\":62880,\"start\":62879},{\"end\":62890,\"start\":62886},{\"end\":63263,\"start\":63262},{\"end\":63269,\"start\":63268},{\"end\":63277,\"start\":63276},{\"end\":63286,\"start\":63285},{\"end\":63292,\"start\":63291},{\"end\":63300,\"start\":63299},{\"end\":63755,\"start\":63754},{\"end\":63761,\"start\":63760},{\"end\":63769,\"start\":63768},{\"end\":63777,\"start\":63776},{\"end\":63784,\"start\":63783},{\"end\":63793,\"start\":63792},{\"end\":64296,\"start\":64295},{\"end\":64302,\"start\":64301},{\"end\":64316,\"start\":64315},{\"end\":64322,\"start\":64321},{\"end\":64331,\"start\":64330},{\"end\":64337,\"start\":64336},{\"end\":64707,\"start\":64703},{\"end\":64714,\"start\":64713},{\"end\":64722,\"start\":64721},{\"end\":64731,\"start\":64730},{\"end\":64733,\"start\":64732},{\"end\":65083,\"start\":65082},{\"end\":65100,\"start\":65096},{\"end\":65108,\"start\":65107},{\"end\":65110,\"start\":65109},{\"end\":65120,\"start\":65119},{\"end\":65122,\"start\":65121},{\"end\":65588,\"start\":65587},{\"end\":65598,\"start\":65597},{\"end\":65612,\"start\":65611},{\"end\":66024,\"start\":66023},{\"end\":66035,\"start\":66034},{\"end\":66050,\"start\":66049},{\"end\":66061,\"start\":66060},{\"end\":66396,\"start\":66395},{\"end\":66403,\"start\":66402},{\"end\":66410,\"start\":66409},{\"end\":66417,\"start\":66416},{\"end\":66423,\"start\":66422},{\"end\":66903,\"start\":66902},{\"end\":66910,\"start\":66909},{\"end\":66916,\"start\":66915},{\"end\":67362,\"start\":67361},{\"end\":67372,\"start\":67371},{\"end\":67374,\"start\":67373},{\"end\":67381,\"start\":67380},{\"end\":67390,\"start\":67389},{\"end\":67403,\"start\":67402},{\"end\":67424,\"start\":67423},{\"end\":67821,\"start\":67820},{\"end\":67829,\"start\":67828},{\"end\":67837,\"start\":67836},{\"end\":67846,\"start\":67845},{\"end\":67848,\"start\":67847},{\"end\":67859,\"start\":67858},{\"end\":67868,\"start\":67867},{\"end\":67870,\"start\":67869},{\"end\":68314,\"start\":68313},{\"end\":68326,\"start\":68325},{\"end\":68334,\"start\":68333},{\"end\":68342,\"start\":68341},{\"end\":68353,\"start\":68352},{\"end\":68363,\"start\":68362},{\"end\":68365,\"start\":68364},{\"end\":68646,\"start\":68645},{\"end\":68661,\"start\":68660},{\"end\":68672,\"start\":68671},{\"end\":69053,\"start\":69052},{\"end\":69059,\"start\":69058},{\"end\":69068,\"start\":69067},{\"end\":69075,\"start\":69074},{\"end\":69475,\"start\":69474},{\"end\":69484,\"start\":69483},{\"end\":69492,\"start\":69491},{\"end\":69494,\"start\":69493},{\"end\":69771,\"start\":69770},{\"end\":69783,\"start\":69782},{\"end\":69796,\"start\":69795},{\"end\":69805,\"start\":69804},{\"end\":70065,\"start\":70064},{\"end\":70067,\"start\":70066},{\"end\":70077,\"start\":70076},{\"end\":70301,\"start\":70300},{\"end\":70321,\"start\":70320},{\"end\":70323,\"start\":70322},{\"end\":70332,\"start\":70331},{\"end\":70334,\"start\":70333}]", "bib_author_last_name": "[{\"end\":51529,\"start\":51523},{\"end\":51542,\"start\":51533},{\"end\":51549,\"start\":51546},{\"end\":51563,\"start\":51553},{\"end\":51574,\"start\":51567},{\"end\":51583,\"start\":51578},{\"end\":51928,\"start\":51919},{\"end\":51942,\"start\":51932},{\"end\":52285,\"start\":52281},{\"end\":52293,\"start\":52289},{\"end\":52633,\"start\":52628},{\"end\":52640,\"start\":52637},{\"end\":52650,\"start\":52644},{\"end\":52659,\"start\":52654},{\"end\":52668,\"start\":52663},{\"end\":53014,\"start\":53010},{\"end\":53029,\"start\":53020},{\"end\":53042,\"start\":53033},{\"end\":53053,\"start\":53046},{\"end\":53068,\"start\":53059},{\"end\":53079,\"start\":53074},{\"end\":53393,\"start\":53384},{\"end\":53405,\"start\":53397},{\"end\":53412,\"start\":53409},{\"end\":53422,\"start\":53416},{\"end\":53824,\"start\":53820},{\"end\":53834,\"start\":53828},{\"end\":53855,\"start\":53838},{\"end\":54246,\"start\":54241},{\"end\":54254,\"start\":54250},{\"end\":54260,\"start\":54258},{\"end\":54268,\"start\":54264},{\"end\":54277,\"start\":54272},{\"end\":54587,\"start\":54583},{\"end\":54594,\"start\":54591},{\"end\":54602,\"start\":54598},{\"end\":54923,\"start\":54918},{\"end\":54932,\"start\":54927},{\"end\":54942,\"start\":54936},{\"end\":54954,\"start\":54946},{\"end\":54966,\"start\":54958},{\"end\":54975,\"start\":54970},{\"end\":55342,\"start\":55336},{\"end\":55349,\"start\":55346},{\"end\":55358,\"start\":55353},{\"end\":55368,\"start\":55362},{\"end\":55720,\"start\":55713},{\"end\":55731,\"start\":55724},{\"end\":55739,\"start\":55735},{\"end\":55750,\"start\":55745},{\"end\":55764,\"start\":55756},{\"end\":55773,\"start\":55768},{\"end\":56128,\"start\":56122},{\"end\":56135,\"start\":56132},{\"end\":56144,\"start\":56139},{\"end\":56154,\"start\":56148},{\"end\":56468,\"start\":56464},{\"end\":56478,\"start\":56472},{\"end\":56488,\"start\":56482},{\"end\":56498,\"start\":56492},{\"end\":56509,\"start\":56502},{\"end\":56523,\"start\":56513},{\"end\":56849,\"start\":56846},{\"end\":56860,\"start\":56853},{\"end\":56871,\"start\":56864},{\"end\":56881,\"start\":56877},{\"end\":56892,\"start\":56887},{\"end\":56903,\"start\":56898},{\"end\":57239,\"start\":57237},{\"end\":57245,\"start\":57243},{\"end\":57253,\"start\":57249},{\"end\":57664,\"start\":57662},{\"end\":57671,\"start\":57668},{\"end\":57687,\"start\":57677},{\"end\":57693,\"start\":57691},{\"end\":57706,\"start\":57699},{\"end\":58202,\"start\":58200},{\"end\":58208,\"start\":58206},{\"end\":58217,\"start\":58212},{\"end\":58223,\"start\":58221},{\"end\":58234,\"start\":58227},{\"end\":58621,\"start\":58617},{\"end\":58635,\"start\":58625},{\"end\":58647,\"start\":58639},{\"end\":58657,\"start\":58651},{\"end\":58669,\"start\":58663},{\"end\":59035,\"start\":59031},{\"end\":59044,\"start\":59039},{\"end\":59052,\"start\":59048},{\"end\":59061,\"start\":59056},{\"end\":59070,\"start\":59065},{\"end\":59080,\"start\":59076},{\"end\":59381,\"start\":59377},{\"end\":59392,\"start\":59385},{\"end\":59404,\"start\":59396},{\"end\":59416,\"start\":59408},{\"end\":59428,\"start\":59420},{\"end\":59438,\"start\":59432},{\"end\":59768,\"start\":59766},{\"end\":59774,\"start\":59772},{\"end\":59783,\"start\":59778},{\"end\":59789,\"start\":59787},{\"end\":59805,\"start\":59795},{\"end\":59811,\"start\":59809},{\"end\":60204,\"start\":60197},{\"end\":60214,\"start\":60208},{\"end\":60231,\"start\":60220},{\"end\":60246,\"start\":60235},{\"end\":60692,\"start\":60689},{\"end\":60700,\"start\":60696},{\"end\":60708,\"start\":60704},{\"end\":60714,\"start\":60712},{\"end\":60722,\"start\":60718},{\"end\":61201,\"start\":61185},{\"end\":61215,\"start\":61205},{\"end\":61227,\"start\":61222},{\"end\":61238,\"start\":61233},{\"end\":61248,\"start\":61244},{\"end\":61261,\"start\":61254},{\"end\":61670,\"start\":61668},{\"end\":61679,\"start\":61674},{\"end\":61686,\"start\":61683},{\"end\":61697,\"start\":61690},{\"end\":61703,\"start\":61701},{\"end\":61711,\"start\":61707},{\"end\":62052,\"start\":62048},{\"end\":62065,\"start\":62056},{\"end\":62076,\"start\":62069},{\"end\":62496,\"start\":62493},{\"end\":62504,\"start\":62500},{\"end\":62511,\"start\":62508},{\"end\":62519,\"start\":62515},{\"end\":62525,\"start\":62523},{\"end\":62533,\"start\":62529},{\"end\":62862,\"start\":62858},{\"end\":62869,\"start\":62866},{\"end\":62877,\"start\":62873},{\"end\":62884,\"start\":62881},{\"end\":62895,\"start\":62891},{\"end\":63266,\"start\":63264},{\"end\":63274,\"start\":63270},{\"end\":63283,\"start\":63278},{\"end\":63289,\"start\":63287},{\"end\":63297,\"start\":63293},{\"end\":63303,\"start\":63301},{\"end\":63758,\"start\":63756},{\"end\":63766,\"start\":63762},{\"end\":63774,\"start\":63770},{\"end\":63781,\"start\":63778},{\"end\":63790,\"start\":63785},{\"end\":63798,\"start\":63794},{\"end\":64299,\"start\":64297},{\"end\":64313,\"start\":64303},{\"end\":64319,\"start\":64317},{\"end\":64328,\"start\":64323},{\"end\":64334,\"start\":64332},{\"end\":64340,\"start\":64338},{\"end\":64711,\"start\":64708},{\"end\":64719,\"start\":64715},{\"end\":64728,\"start\":64723},{\"end\":64739,\"start\":64734},{\"end\":65094,\"start\":65084},{\"end\":65105,\"start\":65101},{\"end\":65117,\"start\":65111},{\"end\":65129,\"start\":65123},{\"end\":65595,\"start\":65589},{\"end\":65609,\"start\":65599},{\"end\":65620,\"start\":65613},{\"end\":66032,\"start\":66025},{\"end\":66047,\"start\":66036},{\"end\":66058,\"start\":66051},{\"end\":66069,\"start\":66062},{\"end\":66400,\"start\":66397},{\"end\":66407,\"start\":66404},{\"end\":66414,\"start\":66411},{\"end\":66420,\"start\":66418},{\"end\":66427,\"start\":66424},{\"end\":66907,\"start\":66904},{\"end\":66913,\"start\":66911},{\"end\":66920,\"start\":66917},{\"end\":67369,\"start\":67363},{\"end\":67378,\"start\":67375},{\"end\":67387,\"start\":67382},{\"end\":67400,\"start\":67391},{\"end\":67421,\"start\":67404},{\"end\":67428,\"start\":67425},{\"end\":67826,\"start\":67822},{\"end\":67834,\"start\":67830},{\"end\":67843,\"start\":67838},{\"end\":67856,\"start\":67849},{\"end\":67865,\"start\":67860},{\"end\":67875,\"start\":67871},{\"end\":68323,\"start\":68315},{\"end\":68331,\"start\":68327},{\"end\":68339,\"start\":68335},{\"end\":68350,\"start\":68343},{\"end\":68360,\"start\":68354},{\"end\":68370,\"start\":68366},{\"end\":68658,\"start\":68647},{\"end\":68669,\"start\":68662},{\"end\":68677,\"start\":68673},{\"end\":69056,\"start\":69054},{\"end\":69065,\"start\":69060},{\"end\":69072,\"start\":69069},{\"end\":69079,\"start\":69076},{\"end\":69481,\"start\":69476},{\"end\":69489,\"start\":69485},{\"end\":69500,\"start\":69495},{\"end\":69780,\"start\":69772},{\"end\":69793,\"start\":69784},{\"end\":69802,\"start\":69797},{\"end\":69813,\"start\":69806},{\"end\":70074,\"start\":70068},{\"end\":70080,\"start\":70078},{\"end\":70318,\"start\":70302},{\"end\":70329,\"start\":70324},{\"end\":70342,\"start\":70335}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8575576},\"end\":51813,\"start\":51475},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":27391290},\"end\":52162,\"start\":51815},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6538247},\"end\":52540,\"start\":52164},{\"attributes\":{\"id\":\"b3\"},\"end\":52924,\"start\":52542},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2092326},\"end\":53293,\"start\":52926},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":10186071},\"end\":53692,\"start\":53295},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10035420},\"end\":54146,\"start\":53694},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7630503},\"end\":54508,\"start\":54148},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":21593159},\"end\":54822,\"start\":54510},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5162860},\"end\":55259,\"start\":54824},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":24493631},\"end\":55621,\"start\":55261},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":52901563},\"end\":56030,\"start\":55623},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":59601271},\"end\":56371,\"start\":56032},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5078022},\"end\":56776,\"start\":56373},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":119102145},\"end\":57174,\"start\":56778},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":162169303},\"end\":57524,\"start\":57176},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204027903},\"end\":58120,\"start\":57526},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":210173022},\"end\":58530,\"start\":58122},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1996665},\"end\":58940,\"start\":58532},{\"attributes\":{\"doi\":\"arXiv:1806.00593\",\"id\":\"b19\"},\"end\":59311,\"start\":58942},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4796040},\"end\":59669,\"start\":59313},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":85461718},\"end\":60112,\"start\":59671},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15222519},\"end\":60578,\"start\":60114},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":22350096},\"end\":61063,\"start\":60580},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":54556210},\"end\":61569,\"start\":61065},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2612052},\"end\":61988,\"start\":61571},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1629541},\"end\":62418,\"start\":61990},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":24964897},\"end\":62739,\"start\":62420},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":19177412},\"end\":63190,\"start\":62741},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":162169250},\"end\":63621,\"start\":63192},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9524314},\"end\":64206,\"start\":63623},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":85464685},\"end\":64621,\"start\":64208},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":233404466},\"end\":64980,\"start\":64623},{\"attributes\":{\"id\":\"b33\"},\"end\":65507,\"start\":64982},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2359761},\"end\":65958,\"start\":65509},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1356654},\"end\":66311,\"start\":65960},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3121011},\"end\":66803,\"start\":66313},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1613420},\"end\":67263,\"start\":66805},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10726144},\"end\":67729,\"start\":67265},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":49558518},\"end\":68252,\"start\":67731},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":44093271},\"end\":68578,\"start\":68254},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3719281},\"end\":69004,\"start\":68580},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206594692},\"end\":69407,\"start\":69006},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2772063},\"end\":69737,\"start\":69409},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":14088925},\"end\":70018,\"start\":69739},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b45\"},\"end\":70218,\"start\":70020},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":5621496},\"end\":70596,\"start\":70220}]", "bib_title": "[{\"end\":51517,\"start\":51475},{\"end\":51915,\"start\":51815},{\"end\":52277,\"start\":52164},{\"end\":52624,\"start\":52542},{\"end\":53006,\"start\":52926},{\"end\":53380,\"start\":53295},{\"end\":53816,\"start\":53694},{\"end\":54237,\"start\":54148},{\"end\":54579,\"start\":54510},{\"end\":54914,\"start\":54824},{\"end\":55332,\"start\":55261},{\"end\":55709,\"start\":55623},{\"end\":56118,\"start\":56032},{\"end\":56456,\"start\":56373},{\"end\":56842,\"start\":56778},{\"end\":57233,\"start\":57176},{\"end\":57658,\"start\":57526},{\"end\":58196,\"start\":58122},{\"end\":58613,\"start\":58532},{\"end\":59373,\"start\":59313},{\"end\":59762,\"start\":59671},{\"end\":60191,\"start\":60114},{\"end\":60685,\"start\":60580},{\"end\":61181,\"start\":61065},{\"end\":61664,\"start\":61571},{\"end\":62044,\"start\":61990},{\"end\":62489,\"start\":62420},{\"end\":62854,\"start\":62741},{\"end\":63260,\"start\":63192},{\"end\":63752,\"start\":63623},{\"end\":64293,\"start\":64208},{\"end\":64701,\"start\":64623},{\"end\":65080,\"start\":64982},{\"end\":65585,\"start\":65509},{\"end\":66021,\"start\":65960},{\"end\":66393,\"start\":66313},{\"end\":66900,\"start\":66805},{\"end\":67359,\"start\":67265},{\"end\":67818,\"start\":67731},{\"end\":68311,\"start\":68254},{\"end\":68643,\"start\":68580},{\"end\":69050,\"start\":69006},{\"end\":69472,\"start\":69409},{\"end\":69768,\"start\":69739},{\"end\":70298,\"start\":70220}]", "bib_author": "[{\"end\":51531,\"start\":51519},{\"end\":51544,\"start\":51531},{\"end\":51551,\"start\":51544},{\"end\":51565,\"start\":51551},{\"end\":51576,\"start\":51565},{\"end\":51585,\"start\":51576},{\"end\":51930,\"start\":51917},{\"end\":51944,\"start\":51930},{\"end\":52287,\"start\":52279},{\"end\":52295,\"start\":52287},{\"end\":52635,\"start\":52626},{\"end\":52642,\"start\":52635},{\"end\":52652,\"start\":52642},{\"end\":52661,\"start\":52652},{\"end\":52670,\"start\":52661},{\"end\":53016,\"start\":53008},{\"end\":53031,\"start\":53016},{\"end\":53044,\"start\":53031},{\"end\":53055,\"start\":53044},{\"end\":53070,\"start\":53055},{\"end\":53081,\"start\":53070},{\"end\":53395,\"start\":53382},{\"end\":53407,\"start\":53395},{\"end\":53414,\"start\":53407},{\"end\":53424,\"start\":53414},{\"end\":53826,\"start\":53818},{\"end\":53836,\"start\":53826},{\"end\":53857,\"start\":53836},{\"end\":54248,\"start\":54239},{\"end\":54256,\"start\":54248},{\"end\":54262,\"start\":54256},{\"end\":54270,\"start\":54262},{\"end\":54279,\"start\":54270},{\"end\":54589,\"start\":54581},{\"end\":54596,\"start\":54589},{\"end\":54604,\"start\":54596},{\"end\":54925,\"start\":54916},{\"end\":54934,\"start\":54925},{\"end\":54944,\"start\":54934},{\"end\":54956,\"start\":54944},{\"end\":54968,\"start\":54956},{\"end\":54977,\"start\":54968},{\"end\":55344,\"start\":55334},{\"end\":55351,\"start\":55344},{\"end\":55360,\"start\":55351},{\"end\":55370,\"start\":55360},{\"end\":55722,\"start\":55711},{\"end\":55733,\"start\":55722},{\"end\":55741,\"start\":55733},{\"end\":55752,\"start\":55741},{\"end\":55766,\"start\":55752},{\"end\":55775,\"start\":55766},{\"end\":56130,\"start\":56120},{\"end\":56137,\"start\":56130},{\"end\":56146,\"start\":56137},{\"end\":56156,\"start\":56146},{\"end\":56470,\"start\":56458},{\"end\":56480,\"start\":56470},{\"end\":56490,\"start\":56480},{\"end\":56500,\"start\":56490},{\"end\":56511,\"start\":56500},{\"end\":56525,\"start\":56511},{\"end\":56851,\"start\":56844},{\"end\":56862,\"start\":56851},{\"end\":56873,\"start\":56862},{\"end\":56883,\"start\":56873},{\"end\":56894,\"start\":56883},{\"end\":56905,\"start\":56894},{\"end\":57241,\"start\":57235},{\"end\":57247,\"start\":57241},{\"end\":57255,\"start\":57247},{\"end\":57666,\"start\":57660},{\"end\":57673,\"start\":57666},{\"end\":57689,\"start\":57673},{\"end\":57695,\"start\":57689},{\"end\":57708,\"start\":57695},{\"end\":58204,\"start\":58198},{\"end\":58210,\"start\":58204},{\"end\":58219,\"start\":58210},{\"end\":58225,\"start\":58219},{\"end\":58236,\"start\":58225},{\"end\":58623,\"start\":58615},{\"end\":58637,\"start\":58623},{\"end\":58649,\"start\":58637},{\"end\":58659,\"start\":58649},{\"end\":58671,\"start\":58659},{\"end\":59037,\"start\":59029},{\"end\":59046,\"start\":59037},{\"end\":59054,\"start\":59046},{\"end\":59063,\"start\":59054},{\"end\":59072,\"start\":59063},{\"end\":59082,\"start\":59072},{\"end\":59383,\"start\":59375},{\"end\":59394,\"start\":59383},{\"end\":59406,\"start\":59394},{\"end\":59418,\"start\":59406},{\"end\":59430,\"start\":59418},{\"end\":59440,\"start\":59430},{\"end\":59770,\"start\":59764},{\"end\":59776,\"start\":59770},{\"end\":59785,\"start\":59776},{\"end\":59791,\"start\":59785},{\"end\":59807,\"start\":59791},{\"end\":59813,\"start\":59807},{\"end\":60206,\"start\":60193},{\"end\":60216,\"start\":60206},{\"end\":60233,\"start\":60216},{\"end\":60248,\"start\":60233},{\"end\":60694,\"start\":60687},{\"end\":60702,\"start\":60694},{\"end\":60710,\"start\":60702},{\"end\":60716,\"start\":60710},{\"end\":60724,\"start\":60716},{\"end\":61203,\"start\":61183},{\"end\":61217,\"start\":61203},{\"end\":61229,\"start\":61217},{\"end\":61240,\"start\":61229},{\"end\":61250,\"start\":61240},{\"end\":61263,\"start\":61250},{\"end\":61672,\"start\":61666},{\"end\":61681,\"start\":61672},{\"end\":61688,\"start\":61681},{\"end\":61699,\"start\":61688},{\"end\":61705,\"start\":61699},{\"end\":61713,\"start\":61705},{\"end\":62054,\"start\":62046},{\"end\":62067,\"start\":62054},{\"end\":62078,\"start\":62067},{\"end\":62498,\"start\":62491},{\"end\":62506,\"start\":62498},{\"end\":62513,\"start\":62506},{\"end\":62521,\"start\":62513},{\"end\":62527,\"start\":62521},{\"end\":62535,\"start\":62527},{\"end\":62864,\"start\":62856},{\"end\":62871,\"start\":62864},{\"end\":62879,\"start\":62871},{\"end\":62886,\"start\":62879},{\"end\":62897,\"start\":62886},{\"end\":63268,\"start\":63262},{\"end\":63276,\"start\":63268},{\"end\":63285,\"start\":63276},{\"end\":63291,\"start\":63285},{\"end\":63299,\"start\":63291},{\"end\":63305,\"start\":63299},{\"end\":63760,\"start\":63754},{\"end\":63768,\"start\":63760},{\"end\":63776,\"start\":63768},{\"end\":63783,\"start\":63776},{\"end\":63792,\"start\":63783},{\"end\":63800,\"start\":63792},{\"end\":64301,\"start\":64295},{\"end\":64315,\"start\":64301},{\"end\":64321,\"start\":64315},{\"end\":64330,\"start\":64321},{\"end\":64336,\"start\":64330},{\"end\":64342,\"start\":64336},{\"end\":64713,\"start\":64703},{\"end\":64721,\"start\":64713},{\"end\":64730,\"start\":64721},{\"end\":64741,\"start\":64730},{\"end\":65096,\"start\":65082},{\"end\":65107,\"start\":65096},{\"end\":65119,\"start\":65107},{\"end\":65131,\"start\":65119},{\"end\":65597,\"start\":65587},{\"end\":65611,\"start\":65597},{\"end\":65622,\"start\":65611},{\"end\":66034,\"start\":66023},{\"end\":66049,\"start\":66034},{\"end\":66060,\"start\":66049},{\"end\":66071,\"start\":66060},{\"end\":66402,\"start\":66395},{\"end\":66409,\"start\":66402},{\"end\":66416,\"start\":66409},{\"end\":66422,\"start\":66416},{\"end\":66429,\"start\":66422},{\"end\":66909,\"start\":66902},{\"end\":66915,\"start\":66909},{\"end\":66922,\"start\":66915},{\"end\":67371,\"start\":67361},{\"end\":67380,\"start\":67371},{\"end\":67389,\"start\":67380},{\"end\":67402,\"start\":67389},{\"end\":67423,\"start\":67402},{\"end\":67430,\"start\":67423},{\"end\":67828,\"start\":67820},{\"end\":67836,\"start\":67828},{\"end\":67845,\"start\":67836},{\"end\":67858,\"start\":67845},{\"end\":67867,\"start\":67858},{\"end\":67877,\"start\":67867},{\"end\":68325,\"start\":68313},{\"end\":68333,\"start\":68325},{\"end\":68341,\"start\":68333},{\"end\":68352,\"start\":68341},{\"end\":68362,\"start\":68352},{\"end\":68372,\"start\":68362},{\"end\":68660,\"start\":68645},{\"end\":68671,\"start\":68660},{\"end\":68679,\"start\":68671},{\"end\":69058,\"start\":69052},{\"end\":69067,\"start\":69058},{\"end\":69074,\"start\":69067},{\"end\":69081,\"start\":69074},{\"end\":69483,\"start\":69474},{\"end\":69491,\"start\":69483},{\"end\":69502,\"start\":69491},{\"end\":69782,\"start\":69770},{\"end\":69795,\"start\":69782},{\"end\":69804,\"start\":69795},{\"end\":69815,\"start\":69804},{\"end\":70076,\"start\":70064},{\"end\":70082,\"start\":70076},{\"end\":70320,\"start\":70300},{\"end\":70331,\"start\":70320},{\"end\":70344,\"start\":70331}]", "bib_venue": "[{\"end\":51623,\"start\":51585},{\"end\":51976,\"start\":51944},{\"end\":52333,\"start\":52295},{\"end\":52706,\"start\":52670},{\"end\":53089,\"start\":53081},{\"end\":53467,\"start\":53424},{\"end\":53893,\"start\":53857},{\"end\":54301,\"start\":54279},{\"end\":54640,\"start\":54604},{\"end\":55013,\"start\":54977},{\"end\":55425,\"start\":55370},{\"end\":55811,\"start\":55775},{\"end\":56192,\"start\":56156},{\"end\":56547,\"start\":56525},{\"end\":56963,\"start\":56905},{\"end\":57324,\"start\":57255},{\"end\":57794,\"start\":57708},{\"end\":58303,\"start\":58236},{\"end\":58723,\"start\":58671},{\"end\":59027,\"start\":58942},{\"end\":59478,\"start\":59440},{\"end\":59875,\"start\":59813},{\"end\":60334,\"start\":60248},{\"end\":60810,\"start\":60724},{\"end\":61287,\"start\":61263},{\"end\":61749,\"start\":61713},{\"end\":62155,\"start\":62078},{\"end\":62557,\"start\":62535},{\"end\":62953,\"start\":62897},{\"end\":63374,\"start\":63305},{\"end\":63886,\"start\":63800},{\"end\":64387,\"start\":64342},{\"end\":64789,\"start\":64741},{\"end\":65198,\"start\":65131},{\"end\":65689,\"start\":65622},{\"end\":66109,\"start\":66071},{\"end\":66506,\"start\":66429},{\"end\":66989,\"start\":66922},{\"end\":67466,\"start\":67430},{\"end\":67963,\"start\":67877},{\"end\":68394,\"start\":68372},{\"end\":68765,\"start\":68679},{\"end\":69158,\"start\":69081},{\"end\":69525,\"start\":69502},{\"end\":69854,\"start\":69815},{\"end\":70062,\"start\":70020},{\"end\":70380,\"start\":70344},{\"end\":62219,\"start\":62157},{\"end\":65252,\"start\":65200},{\"end\":65743,\"start\":65691},{\"end\":66570,\"start\":66508},{\"end\":67043,\"start\":66991},{\"end\":69222,\"start\":69160}]"}}}, "year": 2023, "month": 12, "day": 17}