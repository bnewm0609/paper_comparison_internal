{"id": 250459072, "updated": "2023-07-29 14:03:26.118", "metadata": {"title": "\ud835\udda7\ud835\uddd2\ud835\udda3\ud835\uddb1\ud835\udda4\ud835\udda0: Utilizing Hyperdimensional Computing for a More Robust and Efficient Machine Learning System", "authors": "[{\"first\":\"Justin\",\"last\":\"Morris\",\"middle\":[]},{\"first\":\"Kazim\",\"last\":\"Ergun\",\"middle\":[]},{\"first\":\"Behnam\",\"last\":\"Khaleghi\",\"middle\":[]},{\"first\":\"Mohen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Baris\",\"last\":\"Aksanli\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Simunic\",\"middle\":[]}]", "venue": "ACM Transactions on Embedded Computing Systems", "journal": "ACM Transactions on Embedded Computing Systems", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Today\u2019s systems rely on sending all the data to the cloud and then using complex algorithms, such as Deep Neural Networks, which require billions of parameters and many hours to train a model. In contrast, the human brain can do much of this learning effortlessly. Hyperdimensional (HD) Computing aims to mimic the behavior of the human brain by utilizing high-dimensional representations. This leads to various desirable properties that other Machine Learning (ML) algorithms lack, such as robustness to noise in the system and simple, highly parallel operations. In this article, we propose \ud835\udda7\ud835\uddd2\ud835\udda3\ud835\uddb1\ud835\udda4\ud835\udda0, a HyperDimensional Computing system that is Robust, Efficient, and Accurate. We propose a Processing-in-Memory (PIM) architecture that works in a federated learning environment with challenging communication scenarios that cause errors in the transmitted data. \ud835\udda7\ud835\uddd2\ud835\udda3\ud835\uddb1\ud835\udda4\ud835\udda0 adaptively changes the bitwidth of the model based on the signal-to-noise ratio (SNR) of the incoming sample to maintain the accuracy of the HD model while achieving significant speedup and energy efficiency. Our PIM architecture is able to achieve a speedup of 28\u00d7 and 255\u00d7 better energy efficiency compared to the baseline PIM architecture for Classification and achieves 32 \u00d7 speed up and 289 \u00d7 higher energy efficiency than the baseline architecture for Clustering. \ud835\udda7\ud835\uddd2\ud835\udda3\ud835\uddb1\ud835\udda4\ud835\udda0 is able to achieve this by relaxing hardware parameters to gain energy efficiency and speedup while introducing computational errors. We show experimentally, HD Computing is able to handle the errors without a significant drop in accuracy due to its unique robustness property. For wireless noise, we found that \ud835\udda7\ud835\uddd2\ud835\udda3\ud835\uddb1\ud835\udda4\ud835\udda0 is 48 \u00d7 more robust to noise than other comparable ML algorithms. Our results indicate that our proposed system loses less than 1% Classification accuracy, even in scenarios with an SNR of 6.64. We additionally test the robustness of using HD Computing for Clustering applications and found that our proposed system also looses less than 1% in the mutual information score, even in scenarios with an SNR under 7 dB, which is 57 \u00d7 more robust to noise than K-means.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tecs/MorrisEKIAS22", "doi": "10.1145/3524067"}}, "content": {"source": {"pdf_hash": "d201d01b06a774f7520035712d18d50f51711f58", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3524067", "status": "BRONZE"}}, "grobid": {"id": "72abf07a75cf0e4df6719259ddf0d20c7fe8a681", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d201d01b06a774f7520035712d18d50f51711f58.txt", "contents": "\nHyDREA: Utilizing Hyperdimensional Computing for a More Robust and Efficient Machine Learning System Additional Key Words and Phrases: Machine Learning, brain-insipred hyperdimensional computing, processing-in-memory ACM Reference format: HyDREA: Utilizing Hyperdimensional Computing for a More Robust and Efficient Machine Learning Sys- tem\nOctober 2022\n\nJustin Morris justinmorris@ucsd.edu \nKazim Ergun s:kergun@eng.ucsd.edu \nBehnam Khaleghi bkhalegh@eng.ucsd.edu \n; K Ergun \nB Khaleghi \nT Rosing \n; M Imani \nJustin Morris \nKazim Ergun \nBehnam Khaleghi \nMohen Imani m.imani@uci.edu \nBaris Aksanli baksanli@sdsu.edu. \nTajana 2022 Simunic tajana@ucsd.edu \n\nUniversity of California San Diego\nLa Jolla\n\n\nCA and San Diego State University\nSan DiegoCA\n\n\nUniversity of California San Diego\nLa JollaCA MOHEN\n\n\nIMANI\nUniversity of California Irvine\nIrvineCA\n\n\nBARIS AKSANLI\nSan Diego State University\nSan DiegoCA\n\n\nTAJANA SIMUNIC\nUniversity of California San Diego\nLa JollaCA\n\n\nUniversity of California San Diego\nLa Jolla92093CAUSA,\n\n\nSan Diego State University\n92182San DiegoCAUSA\n\n\nUniversity of California San Diego\nLa Jolla92093CAUSA\n\n\nUniversity of California Irvine\n92697IrvineCAUSA\n\n\nSan Diego State University\n92182San DiegoCAUSA\n\nHyDREA: Utilizing Hyperdimensional Computing for a More Robust and Efficient Machine Learning System Additional Key Words and Phrases: Machine Learning, brain-insipred hyperdimensional computing, processing-in-memory ACM Reference format: HyDREA: Utilizing Hyperdimensional Computing for a More Robust and Efficient Machine Learning Sys- tem\n\nACM Transactions on Embedded Computing Systems\n216October 202210.1145/352406778. Publication date: October 2022.78 This work is licensed under a Creative Commons Attribution International 4.0 License. 78:2 J. Morris et al.CCS Concepts:Computing methodologies \u2192 Machine learning algorithms;Hardware \u2192 Fault tolerance;\nToday's systems rely on sending all the data to the cloud and then using complex algorithms, such as Deep Neural Networks, which require billions of parameters and many hours to train a model. In contrast, the human brain can do much of this learning effortlessly. Hyperdimensional (HD) Computing aims to mimic the behavior of the human brain by utilizing high-dimensional representations. This leads to various desirable properties that other Machine Learning (ML) algorithms lack, such as robustness to noise in the system and simple, highly parallel operations. In this article, we propose HyDREA, a HyperDimensional Computing system that is Robust, Efficient, and Accurate. We propose a Processing-in-Memory (PIM) architecture that works in a federated learning environment with challenging communication scenarios that cause errors in the transmitted data. HyDREA adaptively changes the bitwidth of the model based on the signal-to-noise ratio (SNR) of the incoming sample to maintain the accuracy of the HD model while achieving significant speedup and energy efficiency. Our PIM architecture is able to achieve a speedup of 28\u00d7 and 255\u00d7 better energy efficiency compared to the baseline PIM architecture for Classification and achieves 32\u00d7 speed up and 289\u00d7 higher energy efficiency than the baseline architecture for Clustering. HyDREA is able to achieve this by relaxing hardware parameters to gain energy efficiency and speedup while introducing computational errors. We show experimentally, HD Computing is able to handle the errors without a significant drop in accuracy due to its unique robustness property. For wireless noise, we found that HyDREA is 48\u00d7 more robust to noise than other comparable ML algorithms. Our results indicate that our proposed system loses less than 1% Classification accuracy, even in scenarios with an SNR of 6.64. We additionally test the robustness of using HD Computing for Clustering applications and found that our proposed system also looses less than 1% in the mutual information score, even in scenarios with an SNR under 7 dB, which is 57\u00d7 more robust to noise than K-means.\n\nINTRODUCTION\n\n\"Federated learning\" [1] is a popular model for distributed model training in which a centralized model stored on a server is \"cloned\" to some set of devices that all collect the same features. Each device then updates its local copy of the model and periodically transmits weights to the server, which are used to update the global model via an averaging operation. Intuitively, federated learning reduces communication costs by transmitting only model weights instead of raw training data.\n\nIn \"Federated learning, \" Hyperdimensional (HD) computing offers three benefits [2]. First, an HD \"model\" is simply a collection of bitvectors, which may be less burdensome for communication than other state-of-the-art methods (especially deep neural networks) where the weights are typically floating point values and are non-negligible in size [3,4]. While a line of deep neural networks research tries to reduce the parameters of these models [5], the number of parameters are still higher than HD. Second, local training of the HD model is extremely simple and more energy efficient than many existing ML techniques [6]. Third, transmitting faulty model weights in classical ML algorithms may lead to slower training or convergence to a worse local optimum compared to HD.\n\nThe third point is particularly helpful for \"Federated learning. \" Transmitting model parameters to the central learning system is done mostly through wireless communication. The noise in a wireless channel can incur bit-level errors in the transmitted signal and without error correction, could lead to faulty models due to the noisy data. This is especially true in urban areas where distance is not the only factor adding noise to the wireless channel but also large buildings and multiple obstacles in the way that degrade the wireless signal.\n\nWe additionally take advantage of the simple and highly parallelizable operations in HD to create an analog PIM accelerator with adaptable model bitwidths to achieve the best energy and execution time, while maintaining high accuracy based on the SNR of the wireless channel. This characteristic has made HD the target of various hardware acceleration frameworks, particularly FPGAs [7], and PIM architectures [6,8,9]. Although GPUs and FPGAs provide a suitable degree of parallelism that makes them amenable to machine learning algorithms such as deep neural network [10], the complexity of their resources, e.g., floating point units or DSP blocks, is far beyond the HD requirements, making such devices inefficient for HD. Analog PIM architectures tackle this problem as they comprise memresistive arrays with intrinsically non-complex computational capability, which is sufficient for HD operations. Besides block-level parallelism, another remarkable feature of PIM is eliminating the high cost data movement in the traditional von Neumann architectures as, in PIM, data resides where computation is performed. Adding a PIM accelerator for HD computing to perform cognitive tasks provides significant speed up over utilizing the on-board CPU and saves energy with analog computations and less data movement. Our contributions in this article are as follows:\n\n\u2022 We propose a PIM architecture that adaptively changes the bitwidth of the model based on the SNR of the incoming sample to maintain the accuracy of the HD model while achieving high speedup and energy efficiency. Our PIM architecture is able to achieve 255\u00d7 better energy efficiency and speed up execution time by 28\u00d7 compared to the baseline PIM architecture. \u2022 We take advantage of HD Computing's robustness to errors and relax the precision of ADCs in ISAAC [11], which introduces errors, but improves area and energy efficiency. Our architecture also utilizes quantized values to different bitwidths. \u2022 We additionally evaluate utilizing our accelerator in a federated learning environment, by utilizing a popular network simulator-ns-3 [12]-to model the communication between devices and simulate wireless noise. We compared HyDREA with other light-weight ML algorithms in the same noisy environment. Our results demonstrate that HyDREA is 48\u00d7 more robust to noise than other comparable ML algorithms. Our results indicate that our proposed system loses less than 1% Classification accuracy, even in scenarios with an SNR under 7 dB. \u2022 We additionally evaluate HD Clustering to the same wireless communication errors and found that our proposed system also looses less than 1% in the mutual information score, even in scenarios with an SNR under 7 dB, which is 57\u00d7 more robust to noise than K-means. \u2022 Finally, we extend our architecture to support HD Clustering and our results show that our PIM architecture achieves 289\u00d7 higher energy efficiency and 32\u00d7 speed up compared to the baseline architecture during Clustering.\n\n\nPRELIMINARY\n\nIn this section, we first explain the procedures involved in HD algorithm and then review the related work on HD acceleration and HD robustness to noise.\n\n\nHyperdimensional Computing Classification\n\nWithout loss of generality, we explain the steps of HD computing for Classification tasks, though other algorithms, e.g., Clustering, follow the same procedure, as well. These steps are illustrated by Figure 1.\n\n(1) Encoding: There are multiple different types of encoding for HD Computing [13][14][15]. In this article, we evaluate two different types. The first is Random Projection and the second is ID-Level. Let us assume a feature vector F = { f 1 , f 2 , . . . , f n }, with n features (f i \u2208 N) in original domain. The goal of the encoding stage is to map this feature vector to a D-dimensional space vector:\nH = {h 1 , h 2 , . . . , h D }.\nRandom Projection: This encoding was first proposed in Reference [14]. This encoding first generates D dense bipolar vectors with the same dimensionality as original domain, P = {p 1 , p 2 , . . . , p D }, where p i \u2208 {\u22121, 1} n . The inner product of a feature vector with each randomly generated vector gives us a single dimension of a hypervector in high-dimensional space. For example, we can compute the ith dimension of the encoded data as\nh i = si\u0434n(p i \u00b7 F),\nwhere si\u0434n is a sign function that maps the result of the dot product to +1 or \u22121. Thus, to encode a feature vector into a hypervector, we perform a matrix vector multiplication between the projection matrix and the feature vector using:\nH = si\u0434n(PF)\nID-Level: This encoding was first proposed in Reference [13]. The encoding is performed in three steps, which we describe below. The first step is to create two sets of HVs, ID HVs and level HVs. Both ID HVs and level HVs are D-dimensional HVs where each element is either \u22121 or 1. The encoding scheme assigns a unique channel ID HV to each feature position. IDs are hypervectors that are randomly generated such that all features will have orthogonal channel IDs, i.e., \u03b4 (ID i , ID j ) < 5,000) for D = 10,000 and i j; where the \u03b4 measures the element-wise similarity between the vectors. The HD computing encoder also generates a set of level HVs to consider the impact of each feature value. To create these level hypervectors, we compute the minimum and maximum feature values among all data points, v min and v max , then quantize the range of [v min , v max ] into m levels. Each level is then assigned a corresponding level HV: LV = {LV 1 , . . . , LV m }. To encode a feature vector, the encoder looks at each position of the feature vector and element-wise multiplies the channel ID (ID i ) with the corresponding level hypervector (hv i ). The following equation shows how an n-length feature vector is mapped into the HD space with this encoding scheme:\nH = [hv 1 * ID 1 + hv 2 * ID 2 + \u00b7 \u00b7 \u00b7 + hv n * ID n ], hv j \u2208 {LV 1 , LV 2 , . . . , LV m }, 1 j m, ID i \u2208 {\u22121, 1} D , LV j \u2208 {\u22121, 1} D .\n(2) Training: The simplicity of HD training makes it distinguished from conventional learning algorithms. Consider hypervector H i as the encoded hypervector of input i with the procedure explained above, which required the inner-product of D bit hypervectors followed by dimensionwise addition of n 1 bit values, where n is the number of features. Each input i belongs to a class j, so we further annotate H j i to show the class j of input i, as well. HD training simply adds all hypervectors of the same class to generate the final model hypervector. Therefore, the class hypervector of label j, denoted by C j , is\nC j = H j 0 + H j 1 + \u00b7 \u00b7 \u00b7 = k H j .(1)\nMeaning that we simply accumulate the encoded hypervectors for which their original input belongs to class j. Another advantage of HD over DNNs is HD supports efficient one-pass training, i.e., visiting each input just once and adding the H i s to create the model yields acceptable accuracy, while DNN training requires hundreds of iterations over the whole data set to converge to the final accuracy. HD accuracy can also be improved by retraining the model. During retraining, the encoded hypervector of each input is created again, and its similarity with the existing class (model) hypervectors is checked (see step 3). If a misprediction is observed, say that encoded H j belonging to class C j is predicted as class C k , then the model is updated as follows, which means the information of H j causing (mis)-similarity to C k is discarded:\nC j = C j + H j , C k = C k \u2212 H j .(2)\n(3) Similarity checking: The inference step as well as the retraining step need to find out the most similar class hypervector to the encoded one. Most commonly, this is performed by cosine similarity while other metrics (e.g., Hamming distance) could be appropriate depending on the problem:\ncos ( H , C j ) = H \u00b7 C j H \u00b7 C j .(3)\nEquation (3) shows the similarity checking of encoded hypervector H with class hypervector C j . Since classes are constant, C j can be pre-calculated. H can be factored out as it is common for all candidate classes to be compared with H . Hence, cosine similarity reduces to a simple dot-product between H and C j s. These vectors are not in binary, they are the results of accumulating several other binary vectors.\n\n\nHyperdimensional Computing Clustering\n\nThe HD Clustering algorithm is very similar to the popular K-means algorithm [16]. The first step of HD Clustering, like Classification is to first encode the data into high-dimensional space. In this article, we evaluate two encodings, both covered in Section 2.1. HD Clustering then operates on the encoded HVs as the main datatype. HD Clustering, like K-means, then selects random centers to start. HD Clustering then iterates through all of the encoded data points while comparing them with the cluster centers using a similarity metric and assigning each point to the center it is most similar to. In K-means, that similarity metric is the Euclidean distance. In HD, we utilize cosine similarity for non-binary values, but Euclidean distance could also be used. However, HD maps data into high-dimensional space, D = 10,000, so calculating cosine similarity is much more efficient. After all the points are labeled, the new centers are chosen and the process is repeated until convergence or the maximum number of iterations is reached. Convergence occurs when no point is assigned to a different cluster compared to the previous iteration. The main difference is that HD Clustering adds a pre-processing step to the Clustering algorithm that maps the data into high-dimensional space, or hypervectors.\n\n\nRelated Work\n\nHD computing is light-weight enough to run with acceptable performance on CPUs [17]. However, utilizing a parallel architecture can significantly speed up HD execution time. Imani et al. showed two orders of magnitude speed up when HD runs on GPU [6]. Salamat et al. proposed a framework that facilitates fast implementation of HD algorithms on FPGA [7]. Due to the bit-level operations in HD, which is more suitable for FPGAs than GPUs, they claimed up to 12\u00d7 energy and 1.7\u00d7 speed up over GPUs. HD requires much less memory than DNNs, but the required memory capacity is still beyond the local cache of many devices. Thus, an excessive amount of energy and time is spent moving data between these devices and their main memory (off-chip memory in the case of FPGAs). To resolve this, prior work used PIM architectures, where processing occurs in memory, eliminating the time and energy of data movement [18][19][20]. In FELIX [8], a digital PIM architecture was proposed. However, digital PIM operations are significantly slower than equivalent analog PIM operations. Prior work accelerated the inference phase of HD computing in analog PIM with an associative memory [6]. However, the associative memory only stored the trained class hypervectors, so the input data needed to be encoded elsewhere and then moved into the associative memory, negating the benefit of less data movement. Also, the associative memory only supports inference in HD. In this article, we implement HD Computing in an analog PIM ReRAM architecture based on ISAAC [11]. This architecture allows us to fully implement HD Computing operations end-to- end from encoding to inference unlike prior work. Our architecture differs in that we further take advantage of HD Computing's robustness to noise and relax the precision of the ADCs. We target the ADCs as they are the highest energy overhead in the architecture [11,21].\n\nSeveral works claimed that HD signal representations are inherently robust to various forms of noise [22][23][24][25]. Work in Reference [23] investigated the robustness of HD to RTL level errors (e.g., bit-flips) during computation and found an HD-based approach tolerating an 8.8\u00d7 higher probability of bit-level errors. Similar results are reported in Reference [26].\n\nWork in Reference [23] presented preliminary evidence showing that HD delivered superior performance to conventional data representations in the presence of bit-level errors during processing. Similarly, bit-level errors occur during data transmission as a result of channel noise and interference from multiple users. To the best of our knowledge, there has been no systematic empirical (or theoretical) evaluation of HD as an avenue for achieving robust learning when data must be communicated over noisy channels. This article compares HD computing with a \"Federated learning\" approach for training other ML models and proposes a new analog PIM architecture to accelerate the whole HD computing algorithm from training to inference.\n\n3 HyDREA ANALOG PIM ARCHITECTURE Combining the energy savings by eliminating data movement and a parallel architecture suitable for dimension-wise parallelism of HD algorithms, analog PIM, with its simple arithmetic support, appears as a promising solution for HD computing. A PIM architecture needs to support three classes of in-memory operations; (1) dot-product for the matrix multiplication in encoding and the similarity metric in inference, i.e., the H \u00b7 C j part in Equation (3), in which each dimension of H and C j is fixed-point (results of binary vector additions), (2) addition and subtraction for training and retraining where, as explained by Equation (1), we add H j i s to produce C j , which denotes the final class hypervector of inputs with label j, and (3) search operation to find the best matched class in inference, by finding the maximum of cosine similarity scores between the encoded query H and all class hypervectors. The baseline architecture provided by ISAAC [11] is perfect for mapping HD Computing to an analog PIM architecture, because it supports all three of the above operations. This can be seen in Figure 2(c).\n\n(1) Dot Product: The top half shows how the dot product operation is implemented in our analog PIM crossbar. Assume each resistive cell in the first (i.e., the shown one) column is programmed to resistances R 11 and R 21 where R i j belongs to row i and column j. Voltages V 1 and V 2 are applied to the first and second rows. The corresponding generated current flows through the column is I 1 , which shows the result of dot-product. A larger I shows larger number, and since I = V /R, the resistance of memristive cells need to be proportional to the inverse of the value they represent. For 2D vectors, A and B, the first set of inputs, A, is programmed into the resistances R 11 and R 21 having the conductances of (A 11 = 1 /R11 = C 11 and A 21 = 1 /R21 = C 21 ). Afterwards, the second set of inputs, B, is applied as the voltages at each row (B 11 = V 1 and B 21 = V 21 ). As the figure shows, by applying input values as the voltages to the rows and storing values as conductances, Ohm's law dictates that the current flowing through each resistor is the product of the conductance and applied voltage. Following Kirchhoff's law, the current accumulated at each column is equal to the sum of all the currents flowing through resistors of the column. That is, the total current is\nI 1 = C 11 \u00b7 V 1 + C 21 \u00b7 V 2 .\nFor our design, we store the class hypervectors as the conductances of the ReRam matrix and the query HV is sent as the DAC input voltages.\n\n(2) Addition: The bottom half of Figure 2(c) shows how the addition is implemented in a crossbar analog PIM architecture. Addition works analogous to the dot product, except all the input voltages are set to logical 1 (i.e., V hi\u0434h ). This, the aggregate current of passing through the first column is I 1 = C 11 + C 21 .\n\n(3) Search: Upon performing dot-product between the query hypervector with all class hypervectors, the search operation needs to find the class with the maximum similarity score. In analog PIM, search is implemented using nearest distance search, which finds the most similar value for a given reference. However, we desire a search for the maximum value (so the reference is unknown). But we know the maximum value of the cosine similarity metric is 1, hence we can implement our maximum value search with the already supported nearest distance search by searching for the value that has the highest similarity to reference 1. Hence, the returned value will be maximum score. Note that similarity check returns the closest value (absolute difference) by prioritizing MSB bits.\n\nHyDREA takes advantage of HD computing's robustness to noise to reduce computational complexity without losing a significant amount of accuracy. By reducing the bitwidth of the ADCs in analog PIM, HyDREA is able to achieve significant energy savings. However, it comes at a cost of inaccurate computations. However, HD computing is robust to hardware failures and inaccurate computations, making it a perfect candidate to be accelerated by our design. With our bitwidth reduction optimization, HyDREA is able to achieve the energy efficiency of digital PIM with the speed of analog PIM. Figure 2(a) shows the architecture HyDREA constituting of multiple In Situ Multiply Accumulate (IMA) blocks. In our implementation, HyDREA comprises 24 IMA blocks. The design choice of using 24 IMA blocks was to ensure that our architecture can fit the largest dataset tested. This is critical, because if all the data does not fit, data would need to be offloaded and stored off chip. The load and store operations in our ReRAM array are very costly and would incur a significant amount of latency to our design. IMA blocks are memory crossbars with the capability of performing analog addition and dot-product operations. Each IMA block consists of 8 crossbar arrays, each of which contains 128 rows and 128 columns of memory cells. There are 8 \u00d7 128 Digital-to-Analog (DAC) blocks per IMA, i.e., 128 per each crossbar arrays, allocated to the rows to convert the incoming digital signal (voltage) to analog (current) to perform computation. There is also a shared Sample and Hold (S+H) block, and shared Analog-to-Digital (ADC) blocks in each IMA. Figure 2(b) shows an example of a crossbar memory array. Each bitline is connected to all the wordlines through memresistive cells, which have stored the information (e.g., values of class dimensions) by changing the resistance level of each cell. Each memresistive cell in our configuration is a 2 bit MLC, i.e., it has four resistance states to be able to represent 2 bits. Storing the HD model, i.e., the values of classes dimensions, needs to program the NVMs, which is a slow write operation. However, it is only done one time before beginning the inference step, so the overhead is amortized in the entire course of inference. Figure 3 shows an example of how inference is performed in HyDREA. The first step is to encode the input. The input is stored in the eDRAM buffer of the encoder tile. When a new input shows up, it allows the current input to proceed with its next operation. This operation is itself pipelined (shown in Figure 3). In the first cycle, an eDRAM read is performed to read the input. These values are sent over the shared bus to the IMA for the encoder and recorded in the input register (IR). After the input values have been moved, the IMA will perform the matrix multiplication during the next 16 cycles.\n\n\nArchitecture\n\nIn the next 16 cycles, the eDRAM is ready to receive other inputs and deal with other IMAs. Over the next 16 cycles, the IR feeds 1 bit at a time for each of the input values to the crossbar arrays. The first 128 bits are sent to crossbars 0 and 1, and the next 128 bits are sent to crossbars 2 and 3. At the end of each cycle, the outputs are latched in the Sample and Hold circuits. In the next cycle, these outputs are fed to the ADC units. The results of the ADCs are then fed to the shift-and-add units, where the results are merged with the output register (OR) in the IMA.\n\nAs shown in Figure 3, at the end of cycle 19, the OR in the IMA has its final output value. This is sent over the shared bus to the central units in the tile. The central OR contains the final results for encoding at the end of cycle 20. During this time, the IMA for the next input has already begun processing to maintain utilization. Finally, in cycle 21, contents of the central OR are written to the eDRAM that will provide the inputs for the similarity check. The similarity check is then performed with the same pipeline as it too is a matrix multiplication.\n\n\nChallenges\n\nTo perform the computation in analog, PIM needs to convert the signals into analog domain. For this, it requires to employ DAC and ADC converters at the inputs and outputs, respectively. As shown in previous work, these signal domain converters contribute to a significant overhead in the residing architecture [11,21], which reaches up to 89% of the system power consumption. However, the overhead of these converters can be significantly alleviated as it is exponentially tied in the precision of converters. This, obviously, increases the error as the signal levels are quantized. Fortunately, it is less problematic in the context of HD computing thanks to its remarkable tolerance to error, as information is spread over all the independent and identically distributed dimensions of vectors, so failing the computation on a certain portion of dimensions (bits) should not affect the overall result noticeably. Furthermore, the addition of ADCs for conversion is the largest overhead of using analog PIM for computation. The ADCs take up a huge amount of area as with each bit of resolution added, their area doubles. Prior work tried to alleviate this by sharing the large ADC across multiple blocks [11]. This approach can slow down computation. However, in this article, we significantly reduce this overhead by using extremely low precision ADCs (as low as 2-bits), which our application, HD Computing, can handle.\n\n\n3.3\n\nHyDREA: Analog PIM Architecture Optimiztions ADC Reduction: As in Section 3.2, the energy overhead of conversion from the digital domain to the analog domain and back dominates the energy usage of analog PIM, and this is handled by the ADC blocks. Thus, our task to improve the energy efficiency of analog PIM focuses on improving the energy efficiency of the ADC blocks. We achieve this by reducing the precision of the ADC blocks. Figure 4 shows the expected energy and area savings of reducing the bitwidth of an ADC. The results from the energy breakdown of ISAAC shows 89% of energy is used on ADC conversion. Then, knowing that each bit we drop from the ADC reduces ADC energy by approximately half, we can extrapolate the expected savings. As the figure shows, for each reduction in the bitwidth of an ADC, we expect the area and energy consumption to halve. This is because to add support for each additional bit, the amount of circuit area doubles and therefore, the energy usage approximately doubles. Instead of using 8-bit ADC blocks in analog PIM that achieve full precision conversion to the digital domain, if we reduce the bitwidth of the ADCs, then we can reduce the energy usage by half for every bit of the ADC we drop. This will save a significant amount of energy during the analog to digital conversion step in analog PIM. However, as mentioned, our computations will lose accuracy, and as we drop more bits, our computations will become more inaccurate as we sacrifice precision for energy efficiency.\n\nWe can reduce our ADC blocks from 8 bits to n bits. By doing this, we will convert the first n most significant bits and omit the 8 \u2212 n least significant bits. For example, if we use a 6 bit ADC block to convert 167, then we would lose the last two bits and output 164 instead. This leads to good approximate conversions with large numbers but very poor approximation with smaller numbers. If we use a 6 bit ADC block to convert 7, then we would get 4, which is almost 50% off. Furthermore, we do not produce inaccurate conversions every time. If we convert 172 with a 6 bit ADC block, then we wold get 172, because the last two bits of 172 are both 0. Therefore, we produce exact computations when the bits we would drop are all zero. Our ADC block conversions fall into three categories: exact conversions, slightly inaccurate conversions, and highly inaccurate conversions. Since HD computing utilizes dot product as the similarity check, the larger computations dominate the dot product operation and therefore, the highly inaccurate conversions of smaller operations do not effect the accuracy of the HD model. Therefore, we are able to take advantage of reducing the bitwidth of ADCs to create an analog PIM architecture for accelerating HD computing that does not incur a significant loss in accuracy. \n\n\nDAC Reduction:\n\nWe additionally reduce the energy and execution time overhead of analog PIM by reducing the number of DACs and IMA blocks needed. We achieve this by reducing the precision of the HD model bitwidth.\n\nDue to HD computing's robustness to noise, we could simply reduce the bitwidth of the HD model and achieve efficiency gains without a significant drop in accuracy. When reducing the bitwidth further, training the HD model becomes unstable and the accuracy does not converge. Figure 5 compares training an HD model with 4 bits of precision and training the same model with a full 8 bits of precision. The details of the setup and software used to obtain these results can be found in Section 5. The top line shows that training an 8 bit model is much smoother and clearly improves in each iteration compared to training with reduced bitwidth. This is because, as HVs are added up and adjusted with retraining, some dimensions may saturate the available bitwidth. Any additional change to dimensions with saturated bitwidths that attempt to change the dimension in the direction of the bitwidth saturation does not improve the model further. For instance, when using a bitwidth of 4, the maximum positive value a dimension can represent is 7. If during retraining the dimension would be increased further, then it would instead stay at 7. In contrast, if the dimension is adjusted with subtraction, then it would decrease normally despite any previous attempts to increase the dimension further. This causes over-adjustments in the HD model during retraining when an abnormal change is applied. This is why the accuracy does not converge during retraining with greatly reduced bitwidths. HyDREA is able to improve upon the naive design of simply reducing the bitwidths by additionally modifying the HD algorithm to complement the bitwidth reduction.\n\nAs explained in Section 2, the HD model is initially trained by adding up all of encoded data points into one class HV for each class. When reducing the bitwidth of the HD model from 8 bits to 4 bits, 4 bits may not provide enough precision for model convergence during retraining, preventing the HD model from performing effectively at lower bitwidths. To subvert this problem, we propose to analyze the initial HD model to identify key dimensions that need to utilize the full bitwidth available. HyDREA then locks these dimensions to either the maximum or minimum value to ensure the the HD model does not drastically change during retraining.\n\nWe propose that the largest dimensions in both the positive and negative directions that saturate the desired bitwidth are key dimensions, as dot product is used as the similarity metric. Hence, the largest dimensions in both positive or negative direction contribute the most to the resulting dot product. Dimensions with the largest values in either direction show that most data points from that class agree in that dimension, i.e., a class HV that represents the class well should ensure these dimensions are not over-adjusted.\n\n\nHyDREA\n\n\n78:11\n\nTo support bitwidth reduction, we propose to modify the initial training algorithm of HD. To identify key dimensions in the HD model to lock, our design first performs the initial training with a full 8 bit representation. HyDREA copies the initial class HV and takes the absolute value of all the dimensions in the class HV and finds the indices of the largest \u03b1 dimensions that would saturate the desired bitwidth. They are set to the maximum (minimum) value if they saturated in the positive (negative) direction. The other dimensions are scaled down to the desired bitwidth. This is done for all k class HVs. The initial model is then loaded into our PIM architecture. The dimensions that were previously set to the maximum or minimum value are locked from changes during retraining to prevent the HD model from over adjustments. HyDREA only locks dimensions that would saturate the desired bitwidth. If the dimensions do not saturate the desired bitwidth, then the bitwidth is sufficient and no change is needed. This lock is achieved by not enabling the write bits at locked dimensions. Figure 5 compares training an HD model with the naive approach of simply reducing the bitwidth to 4 and training the same model with HyDREA using the same bitwidth. The graph shows how HyDREA improves upon the naive design, as during retraining the model is clearly improving and increasing in accuracy like the full 8 bit model. Meanwhile, the naive design's accuracy fluctuates greatly and does not converge.\n\n\nHyDREA: Supporting HD Clustering\n\nHere, we discuss how we can use HyDREA to support HD Clustering. As described in Section 2, the HD Clustering algorithm is very similar to the K-means algorithm with a different similarity metric. For HD Clustering, instead of using Euclidean distance, we use cosine similarity to measure the distance between the samples and the cluster centers. This makes mapping HD Clustering onto our existing architecture relatively simple as for Classification, HyDREA already accelerates the similarity checking part of HD inference. Additionally, we use the same encodings for Clustering and Classification, so that accelerator can be reused as well. Therefore, to map HD Clustering to HyDREA, we feed the samples in the original feature domain into our encoding block. Then, to update the distances between the samples and the cluster centers, we feed the cluster centers into the inference accelerator as the class HVs and the samples as the query HVs. This then gives us both the distance in cosine similarity between each sample and all the cluster centers as well as the cluster that each sample is most similar to. The next step of the HD Clustering algorithm, which is to chose the next cluster centers is too complex to accelerate in PIM. However, 98% of the time is spent on encoding and similarity checking. Therefore, offloading updating the cluster centers to the host CPU does not incur a significant amount of overhead. Figure 6 shows an overview of our federated learning framework and how devices communicate. There are two kinds of devices in our network edge devices and the central node. Edge devices are where local samples are generated. During training, they use a cut down version of our accelerator for HyDREA that just implements encoding to map the data into HD space. The sample is then sent to the central node, where on its way there, the encoded sample is subject to wireless communication noise. The central node's purpose is to collect all encoded samples from all of the edge nodes, train a global model, and perform inference. It too uses our accelerator, except it has full training and inference functionality. Once the global model is sufficiently trained, it can be used for inference. Upon inference, the edge device again encodes the input sample to HD space. The sample is then sent to the central node wirelessly incurring a varying degree of noise. The central node then performs inference on the trained HD model and sends the resulting label back to the edge device. We evaluate the feasibility of HyDREA in a \"Federated learning\" environment, by utilizing a popular network simulator-ns-3 [12]-to model the communication between devices and simulate wireless noise. In the Results section, we compare HyDREA with other ML algorithms in the same noisy environment. The ns-3 physical layer model calculates bit error rates (BER) taking into account the Forward Error Correction (FEC) present in WiFi standards such as IEEE 802.11a/g/n. The model first calculates the received signal-to-noise ratio (SNR) based on parameters used in the simulation model and then calculates a packet error rate (PER) based on the mode of operation (e.g., modulation, coding rate) to determine the probability of successfully receiving a frame (packet success rate (PSR)). The received signal SNR depends on the following parameters:\n\n\nNETWORK SIMULATION\n\n\u2022 Transmission powers of devices: Since noise power is usually constant, increasing the transmission power results in a higher SNR, thus lower BER. However, since energy efficiency is crucial in many applications, IoT devices usually operate in low power modes, resulting in low SNR. \u2022 Distance between communicating nodes: As two communicating nodes get further away, the received signal strength decreases, resulting in low SNR. \u2022 Propagation loss: The loss in the communication channel is different for different topologies. For example, if two devices are in the line-of-sight of each other, this scenario would incur much less loss compared to them communicating in a dense downtown with buildings blocking the view. \u2022 Interference: When many devices communicate at the same time, each other's signals act as an interfering signal, which degrades the demodulation and decoding performance at the receiving end. In this case, we have to calculate signal-to-interference-plus-noise ratio (SINR).\n\nWe study how HD Classification and Clustering performance changes with varying transmission power levels, distance, different propagation loss scenarios, and under different number of interfering devices. Additionally, the error rate depends on the modulation, coding and error correction mechanism adopted by the WiFi technology. Ns-3 allows us to study the error rates for modulation schemes such as BPSK, QPSK, 16-1024 QAM, under binary convolutional coding for rates 1 2 , 2 3 , 3 4 , and 5 6 . We can both enable or disable forward error correction (FEC) in all of these  cases. Our experiments use the WiFi protocol stack (802.11n), which is the most matured communication standard implementation in ns-3. There are efforts on modeling low-rate and low-power standards for IoT, but they are not fully developed yet. Hence, we modify the 802.11 PHY and MAC layer parameters and scale data rate and power values to imitate communication in an IoT environment. The modulation techniques and coding schemes of 802.11n, namely, BPSK, M-ary QAM, and Direct-Sequence Spread Spectrum (DSSS), are common with many low-power wireless protocols [27]. Different techniques have different SNR versus BER (Bit error rates) curves, but these curves are the same across protocols [28][29][30]. Since we adjust the parameters of 802.11n, we can simulate the characteristics of low power IoT protocols by operating at the low SNR regions of the SNR-BER curve. We vary the distance between the transmitter and the receiver to collect data at various SNRs. We evaluate with the Friis propagation loss model. Figure 7 shows the BER versus distance curve between transmitter and receiver. We additionally test error rates from other sources of noise. Such as a downtown scenario with buildings in between the nodes shown in Figure 8 or a highly congested network. We use the hybrid building propagation loss model consisting of Okumura-Hata [31], ITU-R 1411 and ITU-R 1238 [32] loss models. The model includes the multi-path fading loss through building walls for both line-of-sight (LoS) and no LoS cases. There are also random communication attempts between other nodes in the network resulting in dynamic BER and packet losses. We compare HD with two baseline approaches. In the first, we assume that corrupted data packets are discarded and must be re-transmitted. This ensures the accuracy of the resulting model, but increases latency and energy consumption-especially in congested networks. Second, we train on the corrupted data. This eliminates the need to re-transmit packets but may slow model convergence or cause the model to converge to a worse local optimum (recall that Neural Networks are a non-convex optimization problem). Due to the robustness of HD Computing to noise, the HD model is able to learn more effectively from corrupted packets than other ML models, eliminating the need to re-transmit data while ensuring a high-quality result. Low-power networks such as LoRaWAN and LPWAN usually operate at very low SNRs [33], which can result in error rates ranging from 10 \u22125 to 10 \u22121 . Many applications require perfect data reconstruction at the receiver, so it is often aimed for networks by design to have an error rate at upper levels of this range. We show that HD is very resilient to errors, such that one can deliberately use very low-power for communication and operate at extremely low SNRs, going beyond the error rates that of standard network configurations, while still getting acceptable accuracy for the learning tasks. This comes with large energy savings that is crucial for resource-constrained IoT devices. We additionally compare HD Computing robustness to Error Correction Codes (ECC) in wireless communication in Section 5.10.\n\n\nEVALUATION\n\n\nExperimental Setup\n\nWe verified the functionality of HyDREA using both software and hardware implementations. In software, we implemented HD Classification and Clustering on an Intel Core i7 7600 CPU using an optimized C++ implementation. For the hardware implementation, we used an analog-based PIM architecture proposed in Reference [11]. We modify the ISAAC architecture to more efficiently run for HD Computing by relaxing the bitwidth resolution of the ADCs. Our PIM design works at 1.2 GHz and uses n bit ADCs, 1 bit DACs, and 128\u00d7128 arrays, where each memresistor cell stores 2 bits. To estimate the energy consumption and execution time of HyDREA, we utilize the detailed energy and execution time breakdown of an ISAAC tile found in the original ISAAC paper [11]. We then calculate the estimated execution time and energy by summing up the required operations for HD Computing. We tested our approach for HD Classification on four practical Classification applications and for HD Clustering on six datasets from the Fundamental Clustering Problem Suite [38], shown in Table 1.\n\n\nHyDREA and Dimensionality\n\nTo test the impact of dimensionality on HD Classification and Clustering robustness, we utilized the 6.64 SNR test with all datasets. Table 2 summarizes the results, where each entry in the table is  the average accuracy for all datasets at that dimensionality. There is a clear relationship between HD robustness to errors and dimensionality. One may think that we can achieve faster execution and lower energy consumption with lower dimensionality; but due to our PIM's highly parallel nature, as long as the HD model fits into the PIM arrays, execution time and energy does not change. Since our design requires a highly robust HD model, the rest of our tests utilize a dimensionality of D = 10,000. Additionally, the table shows that the data representation highly impacts the robustness of HD. Binary values are the most robust, because each individual bit flip impact the correctness of the end result the same. However, with other representations such as floating point, depending on the bit flipped, the error can increase significantly. For instance, if an exponent bit is flipped, that would incur significantly more error than if a mantissa bit was flipped. For the most robust models, one should transmit binary encoded HVs.\n\n\n5.3\n\nHyDREA and the Impact of our Analog PIM Architecture on HD Classification Figure 9 shows the impact of ADC bitwidth reduction on HD model accuracy for four practical applications. The accuracy of each model reduces as the bitwidth drops, but not significantly. When the ADC bitwidth is 4, the average accuracy drop across all applications is 1.5%. This is because our ADC blocks provide highly accurate approximations for high value conversions, and the high value numbers dominate the dot product output. Thus, the resulting dot product closely approximates the exact version. Also, the resulting dot product does not need to be exact, owing to HD's robustness to hardware inaccuracies. Despite inaccurate results, the classes are separated enough that slight variations still result in the HD model selecting the same output class. Overall, HyDREA reduces bitwidth to 2 while only losing 1.8% in accuracy. Figure 10 shows the impact of our analog PIM architecture with 2 bit ADCs and varying model bitwidths on energy consumption and execution time. Our proposed architectural changes drastically improve the energy efficiency and execution time of HD. Our proposed architecture uses 2 bit ADCs and 1 bit models, and achieves 32\u00d7 (29\u00d7) speed up and 232\u00d7 (267\u00d7) higher energy efficiency than the baseline architecture during inference (retraining). Also, in high SNR cases, these models achieve comparable accuracy to full precision models.   Figure 11 compares HyDREA execution time during the training process to THRIFTY [39]. The results show that due to the slower digital operations in THRIFTY, as well as the higher latency of computing near flash storage, HyDREA is on average 180\u00d7 faster during training than in storage computing. Furthermore, Figure 11 also compares the impact of high bandwidth memory, or specifically NVME storage, on HD Computing latency. We perform this test on the same machine where the only difference is for HDD, we store all data on a slow spinning hard drive and for NVME, we use a PCIe generation 4.0 NVME storage drive. The results clearly show that the higher bandwidth does not impact the overall latency of HD Computing. Therefore, in storage computing solutions such as THRIFTY do not have much to gain from utilizing NVME technologies. Thus, analog processing in memory architectures such as HyDREA are more capable of delivering faster execution times than digital processing in memory architectures.\n\n\nHyDREA Versus Processing in Storage and Digital Processing in Memory\n\nIn Table 3, we also compare HyDREA with a FELIX [8] digital PIM-based implementation of HD Computing. We compare using the same model bitwidths and memory area. Our results show that HyDREA is 111\u00d7 faster than the digital PIM design during retraining and 136\u00d7 faster than the digital PIM design during inference on average, because the individual operations in analog PIM are much faster than they are in digital PIM. HyDREA achieves better speed up during inference Fig. 11. Execution time comparison of HyDREA with THRIFTY, a processing in storage architecture for HD Computing and the impact of higher bandwidth memories such as NVME on HD Computing. than retraining when compared to digital PIM, because inference only involves the dot product operation while retraining includes addition operations to adjust the HD model. Due to relying on nor-based operations in digital PIM, execution time scales quadratically for multiplications. Therefore, because analog PIM directly implements multiply and accumulate, HyDREA achieves better speed up during inference and retraining. Figure 12 shows the impact of SNR on model accuracy in our analog PIM architecture. We can load in low bitwidth models when the channel has a high SNR to achieve the best energy consumption and execution time. However, during high network traffic, longer communication distance, or other factors that incur a high amount of noise on the wireless channel, we need to load in the higher bitwidth models to maintain accuracy. This is because our highly quantized models are taking advantage of HD's robustness to noise by effectively adding more noise to the computation. Therefore, if the environment, in this case wireless communication, is also adding noise, the robust property of HD does not hold up. However, if we adaptively switch which model is loaded based on the SNR, then we can maintain high accuracy and achieve significant energy and execution time savings when possible. Figure 13 shows the impact of SNR on model accuracy for two different encodings as well as different datatype representations. Results from all datasets show a similar pattern with increasing bit error rate. HD using integer and binary hypervectors is much more robust to noise as compared to floating-point representations. Since floating-point numbers are represented with mantissa and exponent, if the exponent bits are flipped because of an error, then the number itself changes significantly. We additionally compare to a DNN for the ISOLET dataset [40]. The DNN model uses a 16bit floating point representation for its weights, so we can observe the same problem with robustness in DNNs. The data also demonstrates that the random projection (RP) encoding offers similar robustness to noise as the ID-level encoding with binary values. This is likely because our implementation of random projection also encodes hypervectors to binary values (through a final sign function), so both the random projection and quantized ID-level encodings lead to similarly robust binary hypervectors. Last, random projection achieves on average, the same accuracy as ID-level, but beats ID-level in some datasets, such as ISOLET, while loses in accuracy to others, such as CARDIO and EMG, as both of them are time-series signals, which random projection does not classify well.\n\n\nHyDREA and the Impact of SNR on HD Classification\n\n\nHD Versus Other Classifiers\n\nWe also compared HD to state-of-the-art classifiers (Linear Regression (LR), MultiLayer Perceptron (MLP), Perceptron, Support Vector Classification (SVC)) and evaluated its robustness to noise on our four datasets. Figure 14 shows the results for (1) data with no noise, and (2) data corrupted with SNR of 2.21. We choose an SNR or 2.21, because it is the worst practical scenario in our ns-3 setup. All classifiers have comparable accuracy with no noise. While HD stays robust with a significant amount of noise, the other classifiers become very inaccurate. The high-dimensional  nature of the hypervectors used in HD leads to significant redundancy in representation, which improves its robustness to noise by 48\u00d7 compared to other classifiers at 2.21 SNR. In other words, HD loses 48\u00d7 less accuracy compared to the other classifiers. This gives us a metric where noise robustness is defined by how well the model maintains accuracy with the added wireless noise.\n\n\nHyDREA Versus State-of-the-Art PIM DNN Accelerator\n\nIn Table 4, we compare HyDREA with a State-of-the-Art DNN PIM accelerator Q-PIM [41]. The results show that State-of-the-Art DNNs are able to achieve higher accuracy on more complex datasets such as MNIST. However, in the presence of wireless communication errors, HD Computing is able to maintain its accuracy, while traditional DNNs become unreliable and return random classification results. Furthermore, due to HD Computing's light weight operation, HyDREA achieves a 411\u00d7 speedup and 498\u00d7 energy efficiency improvement over Q-PIM. Figure 15 shows the impact of our analog PIM architecture with 2 bit ADCs and varying model bitwidths on energy consumption and execution time for HD Clustering. Our proposed architectural changes drastically improve the energy efficiency and execution time of HD Clustering. Our proposed architecture uses 2 bit ADCs and 1 bit models, and achieves 32\u00d7 speed up and 289\u00d7 higher energy efficiency than the baseline architecture during Clustering. Also, in high SNR cases, just like for Classification, these models achieve comparable accuracy to full precision models.\n\n\nHyDREA Architecture Impact on Clustering Energy Consumption and Execution Time\n\n\nHD Clustering Accuracy and Robustness Versus K-means\n\nWe also compared HD to a state of the art Clustering algorithm, K-means, and evaluated its robustness to noise. As can be seen from Figure 16, K-means has a comparable accuracy to HD when there are no bit errors in the dataset. To measure Clustering accuracy, we use a metric based on the mutual information between the cluster assignments returned by our algorithm and ground truth cluster labels. The metric is one when the predicted labels are perfectly correlated with the ground truth and zero when they are totally uncorrelated. Although accuracy is similar without  errors, when we introduce errors HD Clustering is significantly more robust. Our proposed system also looses less than 1% in the mutual information score, even in scenarios with an SNR under 7 dB, which is 57\u00d7 more robust to noise than K-means. Figure 17 compares HD Clustering vs K-means Robustness to bit error rates. K-means has similar robustness to bit error rates as HD using integer and floating point representations, until a breaking point around 10 \u22123 bit error rate for most datasets. This is especially clear with the Isolet dataset, which is the biggest dataset we use. HD Clustering is able to maintain accuracy for much larger bit error rates than K-means when running Isolet. HD gains this additional robust property from the high-dimensional nature of the hypervectors used in HD computing leading to significant redundancy in the representation, which improves robustness to noise similar to our Classification results.\n\nAdditionally, similar to our Classification results, the results from all datasets show a similar pattern where HD using integer and binary hypervectors is much more robust to noise as compared to floating-point. Since floating-point numbers are represented with mantissa bits and exponent bits, if the exponent bits are flipped because of an error, the number itself changes significantly, thus incurring more noise. Integer representation performs closer to binary. Random projection provides similar accuracy to binarized ID-Level as random projection encodes hypervectors to binary values as well. Binary representation is the most robust as each individual bit flip incurs the same proportion of noise. \n\n\nImpact of Bit Error Rates on Decoding\n\nSome HD Computing encoding methods have the property where the encoded HV can be decoded back into the original feature vector. For instance, with access to the ID and LV HV banks used to encode the HV in ID-Level, one can decode the encoded HV to get back the original feature vector with some errors [42]. In Figure 18, we show the impact of dimensionality on the quality of the recovered feature vector using the ID-Level encoding. The y-axis shows the mean-squarederror of the original feature vector with the decoded one. We test against a range of bit error rates that could be seen in wireless communication as well as across different dimensions. The results indicate that with higher dimensionality, we are able to recover a better quality sample in the original feature space. Additionally, as the bit error rate increases, our decoding quality decreases. The decoded feature vectors become drastically different after bit error rates of around 0.001 for both 5,000 and 10,000 dimensions.\n\n\nHD Computing Versus Error Correcting Codes (ECC)\n\nIn conventional systems, the transmitter performs three steps to generate the wireless signal from data: source coding, channel coding, and modulation. First, a source encoder removes the redundancies and compresses the data. Then, to protect the compressed bitstream against the impairments introduced by the channel, a channel code is applied. The coded bitstream is finally modulated with a modulation scheme that maps the bits to complex-valued samples (symbols), transmitted over the communication link. The receiver inverts the above operations, but in the reverse order. A demodulator first maps the received complex-valued channel output to a sequence of bits. This bitstream is then decoded with a channel decoder to obtain the original compressed data; however, it might be possibly corrupted due to the channel impairments. Last, the source decoder provides a (usually inexact) reconstruction of the transmitted data by applying a decompression algorithm.\n\nIn this work, we deal with robust learning over unreliable communication channels, so we focus only on the channel coding techniques from this pipeline for our comparison. Error correcting codes (ECC) are used in channel coding for controlling errors in data over unreliable and noisy communication channels. The central idea is the sender encodes the message with redundant information in the form of an ECC. This redundancy allows the receiver to detect a limited number of errors that may occur anywhere in the message, and often to correct these errors without retransmission. We implement the setups depicted in Figure 19 and compare channel codes to our method. We refer to the framework shown in Figure 6 with the evaluation setup described in Section 4, and evaluate the inference robustness of the different communcation systems. For all experiments, we have an Additive White Gaussian Noise (AWGN) channel, over a range of SNR values, and the modulation type is QAM. In the first setup, there is no channel coding and raw data samples are transmitted over the channel. The HD classifier at the receiver side uses these raw data samples corrupted by bit errors to do inference. In the second setup, we add channel coding to the configuration. In the third setup, we apply HD encoding to data at the transmitter side and transmit hypervectors. In this case, we do not need to do encoding at the receiver, only a simple similarity check for HD Inference on the corrupted hypervectors suffices. In the fourth setup, we add channel coding on top of HD encoded hypervectors to further add redundancy.\n\nIn Figure 20(a), we compare a rate 1 2 convolutional channel code with HD encoding. Viterbi decoder is used to decode the transmitted bitstreams at the centralized receiver. Both channel codes and HD encoding are applied directly to raw data samples, as illustrated in second and third communication setups, respectively. The results show that HD encoding has better performance at similar coding rates than convolutional codes. At 35% BER, HD still has around 90% accuracy with 10k dimension hypervectors whereas convolutional code quickly loses accuracy then completely fails. In Figure 20(b), we compare HD encoding with high-dimension hypervectors to using  channel codes combined with lower dimension hypervectors. HD encoding alone performs better at the same overall coding rate, meaning that channel codes do not provide extra protection to the hypervectors. The above results can be explained by Figure 20(c), for which we refer to Reference [43]. All the plotted coding methods are rate 1 2 as the convolutional code used in the previous experiments. We show the SNR versus BER curves for both the exact (dashed) and approximate (solid) decoding algorithms of the considered methods. As implied by the plots, channel coding gains are significant at moderate to high SNRs. However, BER performance of channel coding converges to that of uncoded communication at low SNRs, for which we perform our experiments. In such cases, particularly where BER is greater than 10%, HD encoding is more robust. Moreover, channel codes aim at correcting the errors and reconstructing the original data. Since we are only interested in using the received data for classification or clustering, the exact reconstructions are not necessarily needed. HD encodings are more suitable for this purpose, as the holographic representation property allows to maintain as much information as possible when part of the data is lost.\n\n\nCONCLUSION\n\nIn this article, we proposed HyDREA, an HD computing system that is Robust, Efficient, and Accurate. We proposed a PIM architecture that adaptively changes the bitwidth of the model based on the SNR of the incoming sample to maintain the robustness of the HD model while achieving high accuracy and energy efficiency. Our results indicate that our proposed system loses less than 1% Classification accuracy even in scenarios with an SNR under 7 dB. Our PIM architecture is also able to achieve 255\u00d7 better energy efficiency and speed up execution time by 28\u00d7 compared to the baseline PIM architecture. We evaluated the feasibility of HyDREA in a \"Federated learning\" environment, by utilizing a popular network simulator, ns-3, to model the communication between devices and simulate wireless noise. We compared HyDREA with other light-weight ML algorithms in the same noisy environment. Our results demonstrated that HyDREA is 48\u00d7 more robust to noise than other comparable ML algorithms. We additionally tested the robustness of HD Clustering in the same network simulation scenarios and found that our proposed system also looses less than 1% in the mutual information score, even in scenarios with an SNR under 7 dB, which is 57\u00d7 more robust to noise than K-means. Finally, we extended our PIM architecture to support Clustering and our results show that we are able to achieves 289\u00d7 higher energy efficiency and 32\u00d7 speed up compared to the baseline architecture during Clustering.\n\nFig. 2 .\n2Overview of the PIM architecture used by HyDREA.\n\nFig. 3 .\n3Example of inference in HyDREA.\n\nFig. 4 .\n4Area savings (a) and energy consumption savings (b) as the bitwidth of the ADC is dropped.\n\nFig. 5 .\n5Impact of HyDREA using a 4 bit model on training compared to training a naive bitwidth reduction 4 bit model and training a 8 bit model.\n\nFig. 6 .\n6An Overview of our framework for communicating in the federated learning environment.\n\nFig. 8 .\n8Model of a downtown topology represented in ns-3, where buildings have higher signal attenuation compared to open-air and they block the line-of-sight when they are placed between the transmitters (blue) and the receiver (green).\n\nFig. 9 .\n9Impact of bitwidth reduction on accuracy of HyDREA.\n\nFig. 10 .\n10Energy consumption and execution time of HyDREA using different model bitwidths during training and inference with an ADC bitwidth of 2.\n\nFig. 12 .\n12Accuracy of Design as the SNR varies with an ADC bitwidth of 2 and varying model bitwidth.\n\nFig. 13 .\n13Accuracy of HD Classification as the SNR varies with different encodings and data representations.\n\nFig. 14 .\n14Comparison of the robustness of HD to other classifiers.\n\nFig. 15 .\n15Energy consumption and execution time of HyDREA for one Clustering iteration using different model bitwidths with an ADC bitwidth of 2.\n\nFig. 16 .\n16Comparison of HD clustering with K-means accuracy with no bit errors.\n\nFig. 17 .\n17Accuracy of HD clustering as the SNR varies with different encodings and data representations vs. K-means.\n\nFig. 18 .\n18Impact of dimensionality on decoding quality.\n\n\nFig. 19. Simulated communication setups.\n\nFig. 20 .\n20(a) Comparison of HD encoding to channel coding (setup 1, 2, and 3), (b) combined HD encoding and channel coding (setup 3 and 4), (c) channel coding performance at low SNRs, exact (dashed) and approximate (solid) decoding algorithms.\n\nTable 1 .\n1Dataset InformationDataset \nType \n# Classes # Train Data # Test Data # Features \n\nUCIHAR [34] \nClassification \n6 \n6,213 \n1,554 \n561 \n\nCARDIO [35] \nClassification \n2 \n1,913 \n213 \n21 \n\nFACE [36] \nClassification \n2 \n22,441 \n2,494 \n608 \n\nISOLET [37] \nClassification and Clustering \n26 \n6,238 \n1,559 \n617 \n\nHepta [38] \nClustering \n7 \nN/A \n212 \n3 \n\nTetra [38] \nClustering \n4 \nN/A \n400 \n3 \n\nTwo Diamonds [38] \nClustering \n2 \nN/A \n800 \n2 \n\nWingnut [38] \nClustering \n2 \nN/A \n1,016 \n2 \n\nIris [38] \nClustering \n3 \nN/A \n135 \n3 \n\n\n\nTable 2 .\n2Impact of Dimensionality and Data Representation on the Robustness of HD ComputingClassification and Clustering AccuracyDimensionality \n10,000 8,000 \n6,000 \n4,000 \n2,000 \n\nRP Binary (Classification) \n0.58% \n0.82% \n1.44% \n1.89% \n2.39% \n\nID-Level Binary (Classification) 0.56% \n0.79% \n1.52% \n1.78% \n2.42% \n\nRP (Clustering) \n0.58% \n2.31% \n2.65% \n2.86% \n3.24% \n\nID-Level Binary (Clustering) \n0.66% \n2.48% \n2.52% \n2.79% \n3.13% \n\nID-Level Int (Clustering) \n44.89% 46.60% 64.71% 72.82% 72.13% \n\nID-Level Float (Clustering) \n85.17% 85.19% 85.23% 85.43% 85.55% \n\n\n\nTable 3 .\n3Speedup of HyDREA Over a Digital PIMImplementation with the Same Bitwidth as \nHyDREA with the Same Area \n\nDataset \nISOLET UCIHAR CARDIO FACE \n\nRetraining \n110.4\u00d7 \n111.8\u00d7 \n105.6\u00d7 \n115.2\u00d7 \n\nInference Same Bit Digital 128.9\u00d7 \n137.3\u00d7 \n139.9\u00d7 \n136.1\u00d7 \n\n\n\nTable 4 .\n4Comparison of HyDREA with the State-of-the-Art DNN PIM Accelerator Q-PIM[41] Design \nExact Accuracy 2.21 SNR Accuracy Latency(s) \nEnergy(J) \n\nHyDREA \n93.4% \n92.1% \n9.98 \u00d7 10 \u22126 s 8.02 \u00d7 10 \u22127 J \n\nQ-PIM [41] \n98.5% \n10% \n4.1 \u00d7 10 \u22123 s \n4 \u00d7 10 \u22124 J \n\n\nACM Transactions on Embedded Computing Systems, Vol. 21, No. 6, Article 78. Publication date: October 2022.\n\nFederated learning: Strategies for improving communication efficiency. Jakub Kone\u010dn\u1ef3, H Brendan Mcmahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, Dave Bacon, Jakub Kone\u010dn\u1ef3, H. Brendan McMahan, Felix X. Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategies for improving communication efficiency. Retrieved from https://arXiv:1610.05492.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cogn. Comput. 1Pentti Kanerva. 2009. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cogn. Comput. 1, 2 (2009), 139-159.\n\nRevisiting hyperdimensional learning for FPGA and low-power architectures. Mohsen Imani, Proceedings of the IEEE Symposium on High-Performance Computer Architecture (HPCA'21). the IEEE Symposium on High-Performance Computer Architecture (HPCA'21)IEEEMohsen Imani et al. 2021. Revisiting hyperdimensional learning for FPGA and low-power architectures. In Proceedings of the IEEE Symposium on High-Performance Computer Architecture (HPCA'21). IEEE.\n\nHierarchical hyperdimensional computing for energy efficient classification. Mohsen Imani, Chenyu Huang, Deqian Kong, Tajana Rosing, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACM108Mohsen Imani, Chenyu Huang, Deqian Kong, and Tajana Rosing. 2018. Hierarchical hyperdimensional computing for energy efficient classification. In Proceedings of the 55th Annual Design Automation Conference. ACM, 108.\n\nGeneCAI: Genetic evolution for acquiring compact AI. Mojan Javaheripi, Mohammad Samragh, Tara Javidi, Farinaz Koushanfar, 10.1145/3377930.3390226Proceedings of the Genetic and Evolutionary Computation Conference (GECCO'20). the Genetic and Evolutionary Computation Conference (GECCO'20)ACMMojan Javaheripi, Mohammad Samragh, Tara Javidi, and Farinaz Koushanfar. 2020. GeneCAI: Genetic evolution for acquiring compact AI. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO'20). ACM, 350- 358. DOI:http://dx.doi.org/10.1145/3377930.3390226\n\nExploring hyperdimensional associative memory. Mohsen Imani, Abbas Rahimi, Deqian Kong, Tajana Rosing, Jan M Rabaey, Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA'17). the IEEE International Symposium on High Performance Computer Architecture (HPCA'17)IEEEMohsen Imani, Abbas Rahimi, Deqian Kong, Tajana Rosing, and Jan M. Rabaey. 2017. Exploring hyperdimensional associative memory. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA'17). IEEE, 445-456.\n\nF5-HD: Fast flexible FPGA-based framework for refreshing hyperdimensional computing. Sahand Salamat, Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA'19). the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA'19)ACMSahand Salamat et al. 2019. F5-HD: Fast flexible FPGA-based framework for refreshing hyperdimensional computing. In Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA'19). ACM, 53-62.\n\nFELIX: Fast and energy-efficient logic in memory. Saransh Gupta, Proceedings of the International Conference on Computer-Aided Design (ICCAD'18). the International Conference on Computer-Aided Design (ICCAD'18)ACM55Saransh Gupta et al. 2018. FELIX: Fast and energy-efficient logic in memory. In Proceedings of the International Con- ference on Computer-Aided Design (ICCAD'18). ACM, 55.\n\nDUAL: Acceleration of clustering algorithms using digital-based processing in-memory. Mohsen Imani, Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO'20). the IEEE/ACM International Symposium on Microarchitecture (MICRO'20)IEEEMohsen Imani et al. 2020. DUAL: Acceleration of clustering algorithms using digital-based processing in-memory. In Proceedings of the IEEE/ACM International Symposium on Microarchitecture (MICRO'20). IEEE, 356-371.\n\nEncodeep: Realizing bit-flexible encoding for deep neural networks. Mohammad Samragh, Mojan Javaheripi, Farinaz Koushanfar, ACM Trans. Embed. Comput. Syst. 19Mohammad Samragh, Mojan Javaheripi, and Farinaz Koushanfar. 2020. Encodeep: Realizing bit-flexible encoding for deep neural networks. ACM Trans. Embed. Comput. Syst. 19, 6 (2020), 1-29.\n\nISAAC: A convolutional neural network accelerator with in situ analog arithmetic in crossbars. Ali Shafiee, Proceedings of the International Symposium on Computer Architecture (ISCA'16). the International Symposium on Computer Architecture (ISCA'16)IEEEAli Shafiee et al. 2016. ISAAC: A convolutional neural network accelerator with in situ analog arithmetic in crossbars. In Proceedings of the International Symposium on Computer Architecture (ISCA'16). IEEE, 14-26.\n\nNetwork simulations with the ns-3 simulator. Thomas R Henderson, Mathieu Lacage, George F Riley, Craig Dowell, Joseph Kopena, SIGCOMM Demonst. 14527Thomas R. Henderson, Mathieu Lacage, George F. Riley, Craig Dowell, and Joseph Kopena. 2008. Network simulations with the ns-3 simulator. SIGCOMM Demonst. 14, 14 (2008), 527.\n\nVoiceHD: Hyperdimensional computing for efficient speech recognition. Mohsen Imani, Deqian Kong, Abbas Rahimi, Tajana Rosing, Proceedings of the International Conference on Rebooting Computing (ICRC'17). the International Conference on Rebooting Computing (ICRC'17)IEEEMohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. 2017. VoiceHD: Hyperdimensional computing for efficient speech recognition. In Proceedings of the International Conference on Rebooting Computing (ICRC'17). IEEE, 1-6.\n\nBRIC: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. Mohsen Imani, Proceedings of the ACM/IEEE Design Automation Conference (DAC'19). the ACM/IEEE Design Automation Conference (DAC'19)IEEEMohsen Imani et al. 2019. BRIC: Locality-based encoding for energy-efficient brain-inspired hyperdimensional com- puting. In Proceedings of the ACM/IEEE Design Automation Conference (DAC'19). IEEE, 1-6.\n\nHyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. Abbas Rahimi, Simone Benatti, Pentti Kanerva, Proceedings of the IEEE International Conference on Rebooting Computing (ICRC'16). the IEEE International Conference on Rebooting Computing (ICRC'16)IEEEAbbas Rahimi, Simone Benatti, Pentti Kanerva, et al. 2016. Hyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. In Proceedings of the IEEE International Conference on Rebooting Computing (ICRC'16). IEEE, 1-8.\n\nHDCluster: An accurate clustering using brain-inspired high-dimensional computing. Mohsen Imani, Proceedings of the Design, Automation and Test in Europe Conference and Exhibition (DATE'19. the Design, Automation and Test in Europe Conference and Exhibition (DATE'19Mohsen Imani et al. 2019. HDCluster: An accurate clustering using brain-inspired high-dimensional computing. In Proceedings of the Design, Automation and Test in Europe Conference and Exhibition (DATE'19). IEEE/ACM.\n\nA binary learning framework for hyperdimensional computing. Mohsen Imani, Proceedings of the Design, Automation and Test in Europe Conference and Exhibition (DATE'19). the Design, Automation and Test in Europe Conference and Exhibition (DATE'19)Mohsen Imani et al. 2019. A binary learning framework for hyperdimensional computing. In Proceedings of the Design, Automation and Test in Europe Conference and Exhibition (DATE'19). IEEE/ACM.\n\nA scalable design of multi-bit ferroelectric content addressable memory for data-centric computing. Chao Li, Proceedings of the IEEE International Electron Devices Meeting (IEDM'20). the IEEE International Electron Devices Meeting (IEDM'20)IEEEChao Li et al. 2020. A scalable design of multi-bit ferroelectric content addressable memory for data-centric computing. In Proceedings of the IEEE International Electron Devices Meeting (IEDM'20). IEEE.\n\nFloatpim: In-memory acceleration of deep neural network training with high precision. Mohsen Imani, Proceedings of the International Symposium on Computer Architecture (ISCA'19). the International Symposium on Computer Architecture (ISCA'19)IEEEMohsen Imani et al. 2019. Floatpim: In-memory acceleration of deep neural network training with high precision. In Proceedings of the International Symposium on Computer Architecture (ISCA'19). IEEE, 802-815.\n\nDeep learning acceleration with neuron-to-memory transformation. Mohsen Imani, Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA'20). the IEEE International Symposium on High-Performance Computer Architecture (HPCA'20)IEEEMohsen Imani et al. 2020. Deep learning acceleration with neuron-to-memory transformation. In Proceedings of the IEEE International Symposium on High-Performance Computer Architecture (HPCA'20). IEEE, 1-14.\n\nMixed-signal charge-domain acceleration of deep neural networks through interleaved bit-partitioned arithmetic. Soroush Ghodrati, Hardik Sharma, Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques. the ACM International Conference on Parallel Architectures and Compilation TechniquesSoroush Ghodrati, Hardik Sharma, et al. 2020. Mixed-signal charge-domain acceleration of deep neural networks through interleaved bit-partitioned arithmetic. In Proceedings of the ACM International Conference on Parallel Archi- tectures and Compilation Techniques. 399-411.\n\nEfficient biosignal processing using hyperdimensional computing: Network templates for combined learning and classification of ExG signals. Abbas Rahimi, Pentti Kanerva, Luca Benini, Jan M Rabaey, Proc. IEEE. 107Abbas Rahimi, Pentti Kanerva, Luca Benini, and Jan M. Rabaey. 2018. Efficient biosignal processing using hyperdi- mensional computing: Network templates for combined learning and classification of ExG signals. Proc. IEEE 107, 1 (2018), 123-143.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. Abbas Rahimi, Pentti Kanerva, Jan M Rabaey, Proceedings of the International Symposium on Low Power Electronics and Design. the International Symposium on Low Power Electronics and DesignACMAbbas Rahimi, Pentti Kanerva, and Jan M. Rabaey. 2016. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the International Symposium on Low Power Electronics and Design. ACM, 64-69.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. Mohsen Imani, Deqian Kong, Abbas Rahimi, Tajana Rosing, Proceedings of the IEEE International Conference on Rebooting Computing (ICRC'17). the IEEE International Conference on Rebooting Computing (ICRC'17)IEEEMohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. 2017. Voicehd: Hyperdimensional computing for efficient speech recognition. In Proceedings of the IEEE International Conference on Rebooting Computing (ICRC'17). IEEE, 1-8.\n\nQuantHD: A quantization framework for hyperdimensional computing. Mohsen Imani, IEEE Trans. Comput.-Aided Design Integr. Circ. Syst. 39Mohsen Imani et al. 2019. QuantHD: A quantization framework for hyperdimensional computing. IEEE Trans. Comput.-Aided Design Integr. Circ. Syst. 39, 10 (2019), 2268-2278.\n\nHyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture codesign for energy-efficient, error-resilient language recognition. Haitong Li, Proceedings of the IEEE International Electron Devices Meeting (IEDM'16). the IEEE International Electron Devices Meeting (IEDM'16)IEEEHaitong Li et al. 2016. Hyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture co- design for energy-efficient, error-resilient language recognition. In Proceedings of the IEEE International Electron De- vices Meeting (IEDM'16). IEEE, 16-1.\n\nLow power wide area networks: An overview. Usman Raza, Parag Kulkarni, Mahesh Sooriyabandara, IEEE Commun. Surveys Tutor. 19Usman Raza, Parag Kulkarni, and Mahesh Sooriyabandara. 2017. Low power wide area networks: An overview. IEEE Commun. Surveys Tutor. 19, 2 (2017), 855-873.\n\n802.11n-2009-IEEE Standard for Information technology-Local and metropolitan area networks. Ieee, IEEE. 2009. 802.11n-2009-IEEE Standard for Information technology-Local and metropolitan area networks. Re- trieved from https://standards.ieee.org/standard/802_11n-2009.html.\n\nTheodore S Rappaport, Wireless Communications: Principles and Practice. New JerseyPrentice Hall PTR2Theodore S. Rappaport et al. 1996. Wireless Communications: Principles and Practice. Vol. 2. Prentice Hall PTR, New Jersey.\n\nOn the use of the universal Okumura-Hata propagation prediction model in rural areas. Arturas Medeisis, Algimantas Kajackas, Proceedings of the IEEE 51st Vehicular Technology Conference Proceedings (VTC'00). the IEEE 51st Vehicular Technology Conference Proceedings (VTC'00)3Arturas Medeisis and Algimantas Kajackas. 2000. On the use of the universal Okumura-Hata propagation prediction model in rural areas. In Proceedings of the IEEE 51st Vehicular Technology Conference Proceedings (VTC'00), Vol. 3. IEEE, 1815-1818.\n\nInternational Telecommunication Union. n.d.International Telecommunication Union. [n.d.]. Retrieved from https://www.itu.int//.\n\nOn the error rate of the LoRa modulation with interference. Matthieu Cotting, Andreas Burg, and Alexios Balatsoukas-Stimming. 19Orion AfisiadisOrion Afisiadis, Matthieu Cotting, Andreas Burg, and Alexios Balatsoukas-Stimming. 2019. On the error rate of the LoRa modulation with interference. IEEE Trans. Wireless Commun. 19, 2 (2019), 1292-1304.\n\nUCI Machine Learning Repository. n.d.UCI Machine Learning Repository. [n.d.]. Retrieved from https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+ Activities.\n\nUCI Machine Learning Repository. n.d.UCI Machine Learning Repository. [n.d.]. Retrieved from https://archive.ics.uci.edu/ml/datasets/cardiotocography.\n\nCaltech-256 object category dataset. Gregory Griffin, Alex Holub, Pietro Perona, California Institute of TechnologyGregory Griffin, Alex Holub, and Pietro Perona. 2007. Caltech-256 object category dataset. California Institute of Technology.\n\nUCI Machine Learning Repository. n.d.UCI Machine Learning Repository. [n.d.]. Retrieved from http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nU* C: Self-organized clustering with emergent feature maps. Alfred Ultsch, Proceedings of the Lernen, Wissensentdeckung und Adaptivit GI Workshops (LWA'05). the Lernen, Wissensentdeckung und Adaptivit GI Workshops (LWA'05)CiteseerAlfred Ultsch. 2005. U* C: Self-organized clustering with emergent feature maps. In Proceedings of the Lernen, Wis- sensentdeckung und Adaptivit GI Workshops (LWA'05). Citeseer, 240-244.\n\nSaransh Gupta, Justin Morris, Mohsen Imani, Ranganathan Ramkumar, Jeffrey Yu, Aniket Tiwari, Baris Aksanli, and Tajana \u0160imuni\u0107 Rosing. 2020. THRIFTY: Training with hyperdimensional computing across flash hierarchy. Saransh Gupta, Justin Morris, Mohsen Imani, Ranganathan Ramkumar, Jeffrey Yu, Aniket Tiwari, Baris Aksanli, and Tajana \u0160imuni\u0107 Rosing. 2020. THRIFTY: Training with hyperdimensional computing across flash hierarchy.\n\nCustomizing neural networks for efficient FPGA implementation. Mohammad Samragh, Mohammad Ghasemzadeh, Farinaz Koushanfar, Proceedings of the IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'17. the IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'17IEEEMohammad Samragh, Mohammad Ghasemzadeh, and Farinaz Koushanfar. 2017. Customizing neural networks for efficient FPGA implementation. In Proceedings of the IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'17). IEEE, 85-92.\n\nQ-pim: A genetic algorithm based flexible dnn quantization method and application to processing-in-memory platform. Yun Long, Edward Lee, Daehyun Kim, Saibal Mukhopadhyay, Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC'20). the 57th ACM/IEEE Design Automation Conference (DAC'20)IEEEYun Long, Edward Lee, Daehyun Kim, and Saibal Mukhopadhyay. 2020. Q-pim: A genetic algorithm based flexible dnn quantization method and application to processing-in-memory platform. In Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC'20). IEEE, 1-6.\n\nFarinaz Koushanfar, and Tajana Rosing. 2019. A framework for collaborative learning in secure high-dimensional space. Mohsen Imani, Yeseong Kim, Sadegh Riazi, John Messerly, Patric Liu, Proceedings of the IEEE 12th International Conference on Cloud Computing (CLOUD'19). the IEEE 12th International Conference on Cloud Computing (CLOUD'19)IEEEMohsen Imani, Yeseong Kim, Sadegh Riazi, John Messerly, Patric Liu, Farinaz Koushanfar, and Tajana Rosing. 2019. A framework for collaborative learning in secure high-dimensional space. In Proceedings of the IEEE 12th International Conference on Cloud Computing (CLOUD'19). IEEE, 435-446.\n\nBER comparison between convolutional, turbo, LDPC, and polar codes. Bashar Tahir, Stefan Schwarz, Markus Rupp, Proceedings of the 24th International Conference on Telecommunications (ICT'17). the 24th International Conference on Telecommunications (ICT'17)IEEEBashar Tahir, Stefan Schwarz, and Markus Rupp. 2017. BER comparison between convolutional, turbo, LDPC, and polar codes. In Proceedings of the 24th International Conference on Telecommunications (ICT'17). IEEE, 1-7.\n", "annotations": {"author": "[{\"end\":393,\"start\":357},{\"end\":428,\"start\":394},{\"end\":467,\"start\":429},{\"end\":478,\"start\":468},{\"end\":490,\"start\":479},{\"end\":500,\"start\":491},{\"end\":511,\"start\":501},{\"end\":526,\"start\":512},{\"end\":539,\"start\":527},{\"end\":556,\"start\":540},{\"end\":585,\"start\":557},{\"end\":619,\"start\":586},{\"end\":656,\"start\":620},{\"end\":702,\"start\":657},{\"end\":750,\"start\":703},{\"end\":804,\"start\":751},{\"end\":853,\"start\":805},{\"end\":908,\"start\":854},{\"end\":971,\"start\":909},{\"end\":1028,\"start\":972},{\"end\":1077,\"start\":1029},{\"end\":1133,\"start\":1078},{\"end\":1184,\"start\":1134},{\"end\":1233,\"start\":1185},{\"end\":393,\"start\":357},{\"end\":428,\"start\":394},{\"end\":467,\"start\":429},{\"end\":478,\"start\":468},{\"end\":490,\"start\":479},{\"end\":500,\"start\":491},{\"end\":511,\"start\":501},{\"end\":526,\"start\":512},{\"end\":539,\"start\":527},{\"end\":556,\"start\":540},{\"end\":585,\"start\":557},{\"end\":619,\"start\":586},{\"end\":656,\"start\":620},{\"end\":702,\"start\":657},{\"end\":750,\"start\":703},{\"end\":804,\"start\":751},{\"end\":853,\"start\":805},{\"end\":908,\"start\":854},{\"end\":971,\"start\":909},{\"end\":1028,\"start\":972},{\"end\":1077,\"start\":1029},{\"end\":1133,\"start\":1078},{\"end\":1184,\"start\":1134},{\"end\":1233,\"start\":1185}]", "publisher": null, "author_last_name": "[{\"end\":370,\"start\":364},{\"end\":405,\"start\":400},{\"end\":444,\"start\":436},{\"end\":477,\"start\":472},{\"end\":489,\"start\":481},{\"end\":499,\"start\":493},{\"end\":510,\"start\":505},{\"end\":525,\"start\":519},{\"end\":538,\"start\":533},{\"end\":555,\"start\":547},{\"end\":568,\"start\":563},{\"end\":599,\"start\":592},{\"end\":639,\"start\":632},{\"end\":370,\"start\":364},{\"end\":405,\"start\":400},{\"end\":444,\"start\":436},{\"end\":477,\"start\":472},{\"end\":489,\"start\":481},{\"end\":499,\"start\":493},{\"end\":510,\"start\":505},{\"end\":525,\"start\":519},{\"end\":538,\"start\":533},{\"end\":555,\"start\":547},{\"end\":568,\"start\":563},{\"end\":599,\"start\":592},{\"end\":639,\"start\":632}]", "author_first_name": "[{\"end\":363,\"start\":357},{\"end\":399,\"start\":394},{\"end\":435,\"start\":429},{\"end\":469,\"start\":468},{\"end\":471,\"start\":470},{\"end\":480,\"start\":479},{\"end\":492,\"start\":491},{\"end\":502,\"start\":501},{\"end\":504,\"start\":503},{\"end\":518,\"start\":512},{\"end\":532,\"start\":527},{\"end\":546,\"start\":540},{\"end\":562,\"start\":557},{\"end\":591,\"start\":586},{\"end\":626,\"start\":620},{\"end\":631,\"start\":627},{\"end\":363,\"start\":357},{\"end\":399,\"start\":394},{\"end\":435,\"start\":429},{\"end\":469,\"start\":468},{\"end\":471,\"start\":470},{\"end\":480,\"start\":479},{\"end\":492,\"start\":491},{\"end\":502,\"start\":501},{\"end\":504,\"start\":503},{\"end\":518,\"start\":512},{\"end\":532,\"start\":527},{\"end\":546,\"start\":540},{\"end\":562,\"start\":557},{\"end\":591,\"start\":586},{\"end\":626,\"start\":620},{\"end\":631,\"start\":627}]", "author_affiliation": "[{\"end\":701,\"start\":658},{\"end\":749,\"start\":704},{\"end\":803,\"start\":752},{\"end\":852,\"start\":806},{\"end\":907,\"start\":855},{\"end\":970,\"start\":910},{\"end\":1027,\"start\":973},{\"end\":1076,\"start\":1030},{\"end\":1132,\"start\":1079},{\"end\":1183,\"start\":1135},{\"end\":1232,\"start\":1186},{\"end\":701,\"start\":658},{\"end\":749,\"start\":704},{\"end\":803,\"start\":752},{\"end\":852,\"start\":806},{\"end\":907,\"start\":855},{\"end\":970,\"start\":910},{\"end\":1027,\"start\":973},{\"end\":1076,\"start\":1030},{\"end\":1132,\"start\":1079},{\"end\":1183,\"start\":1135},{\"end\":1232,\"start\":1186}]", "title": "[{\"end\":342,\"start\":1},{\"end\":1575,\"start\":1234},{\"end\":342,\"start\":1},{\"end\":1575,\"start\":1234}]", "venue": "[{\"end\":1623,\"start\":1577},{\"end\":1623,\"start\":1577}]", "abstract": "[{\"end\":4019,\"start\":1894},{\"end\":4019,\"start\":1894}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4059,\"start\":4056},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4611,\"start\":4608},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4877,\"start\":4874},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4879,\"start\":4877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4977,\"start\":4974},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5151,\"start\":5148},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6241,\"start\":6238},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6268,\"start\":6265},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6270,\"start\":6268},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6272,\"start\":6270},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6427,\"start\":6423},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7686,\"start\":7682},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7966,\"start\":7962},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9357,\"start\":9353},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9361,\"start\":9357},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9365,\"start\":9361},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9781,\"start\":9777},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10489,\"start\":10485},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14253,\"start\":14249},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15579,\"start\":15575},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15746,\"start\":15743},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15849,\"start\":15846},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16405,\"start\":16401},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16409,\"start\":16405},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16413,\"start\":16409},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16427,\"start\":16424},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16669,\"start\":16666},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17042,\"start\":17038},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17390,\"start\":17386},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17393,\"start\":17390},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17501,\"start\":17497},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17505,\"start\":17501},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17509,\"start\":17505},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17513,\"start\":17509},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17537,\"start\":17533},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17765,\"start\":17761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17790,\"start\":17786},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19500,\"start\":19496},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26588,\"start\":26584},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26591,\"start\":26588},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27482,\"start\":27478},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37774,\"start\":37770},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39988,\"start\":39987},{\"end\":40002,\"start\":39999},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40660,\"start\":40656},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40790,\"start\":40786},{\"end\":40794,\"start\":40790},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":40798,\"start\":40794},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41445,\"start\":41441},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":41477,\"start\":41473},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":42543,\"start\":42539},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43625,\"start\":43621},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44058,\"start\":44054},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":44353,\"start\":44349},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":47174,\"start\":47170},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48215,\"start\":48212},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":50686,\"start\":50682},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":52683,\"start\":52679},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":56408,\"start\":56404},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":60682,\"start\":60678},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":60725,\"start\":60724},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":66480,\"start\":66476},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4059,\"start\":4056},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4611,\"start\":4608},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4877,\"start\":4874},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4879,\"start\":4877},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4977,\"start\":4974},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5151,\"start\":5148},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6241,\"start\":6238},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6268,\"start\":6265},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6270,\"start\":6268},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6272,\"start\":6270},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6427,\"start\":6423},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7686,\"start\":7682},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7966,\"start\":7962},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9357,\"start\":9353},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9361,\"start\":9357},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9365,\"start\":9361},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9781,\"start\":9777},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10489,\"start\":10485},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14253,\"start\":14249},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15579,\"start\":15575},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15746,\"start\":15743},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15849,\"start\":15846},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16405,\"start\":16401},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16409,\"start\":16405},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16413,\"start\":16409},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16427,\"start\":16424},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16669,\"start\":16666},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17042,\"start\":17038},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17390,\"start\":17386},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17393,\"start\":17390},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17501,\"start\":17497},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17505,\"start\":17501},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17509,\"start\":17505},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17513,\"start\":17509},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17537,\"start\":17533},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17765,\"start\":17761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17790,\"start\":17786},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19500,\"start\":19496},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26588,\"start\":26584},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26591,\"start\":26588},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27482,\"start\":27478},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37774,\"start\":37770},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39988,\"start\":39987},{\"end\":40002,\"start\":39999},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40660,\"start\":40656},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40790,\"start\":40786},{\"end\":40794,\"start\":40790},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":40798,\"start\":40794},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41445,\"start\":41441},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":41477,\"start\":41473},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":42543,\"start\":42539},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43625,\"start\":43621},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44058,\"start\":44054},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":44353,\"start\":44349},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":47174,\"start\":47170},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48215,\"start\":48212},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":50686,\"start\":50682},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":52683,\"start\":52679},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":56408,\"start\":56404},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":60682,\"start\":60678},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":60725,\"start\":60724},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":66480,\"start\":66476}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":63202,\"start\":63143},{\"attributes\":{\"id\":\"fig_1\"},\"end\":63245,\"start\":63203},{\"attributes\":{\"id\":\"fig_2\"},\"end\":63347,\"start\":63246},{\"attributes\":{\"id\":\"fig_3\"},\"end\":63495,\"start\":63348},{\"attributes\":{\"id\":\"fig_4\"},\"end\":63592,\"start\":63496},{\"attributes\":{\"id\":\"fig_5\"},\"end\":63833,\"start\":63593},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63896,\"start\":63834},{\"attributes\":{\"id\":\"fig_7\"},\"end\":64046,\"start\":63897},{\"attributes\":{\"id\":\"fig_8\"},\"end\":64150,\"start\":64047},{\"attributes\":{\"id\":\"fig_9\"},\"end\":64262,\"start\":64151},{\"attributes\":{\"id\":\"fig_10\"},\"end\":64332,\"start\":64263},{\"attributes\":{\"id\":\"fig_11\"},\"end\":64481,\"start\":64333},{\"attributes\":{\"id\":\"fig_12\"},\"end\":64564,\"start\":64482},{\"attributes\":{\"id\":\"fig_13\"},\"end\":64684,\"start\":64565},{\"attributes\":{\"id\":\"fig_14\"},\"end\":64743,\"start\":64685},{\"attributes\":{\"id\":\"fig_15\"},\"end\":64786,\"start\":64744},{\"attributes\":{\"id\":\"fig_16\"},\"end\":65033,\"start\":64787},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":65563,\"start\":65034},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":66130,\"start\":65564},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":66391,\"start\":66131},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66653,\"start\":66392},{\"attributes\":{\"id\":\"fig_0\"},\"end\":63202,\"start\":63143},{\"attributes\":{\"id\":\"fig_1\"},\"end\":63245,\"start\":63203},{\"attributes\":{\"id\":\"fig_2\"},\"end\":63347,\"start\":63246},{\"attributes\":{\"id\":\"fig_3\"},\"end\":63495,\"start\":63348},{\"attributes\":{\"id\":\"fig_4\"},\"end\":63592,\"start\":63496},{\"attributes\":{\"id\":\"fig_5\"},\"end\":63833,\"start\":63593},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63896,\"start\":63834},{\"attributes\":{\"id\":\"fig_7\"},\"end\":64046,\"start\":63897},{\"attributes\":{\"id\":\"fig_8\"},\"end\":64150,\"start\":64047},{\"attributes\":{\"id\":\"fig_9\"},\"end\":64262,\"start\":64151},{\"attributes\":{\"id\":\"fig_10\"},\"end\":64332,\"start\":64263},{\"attributes\":{\"id\":\"fig_11\"},\"end\":64481,\"start\":64333},{\"attributes\":{\"id\":\"fig_12\"},\"end\":64564,\"start\":64482},{\"attributes\":{\"id\":\"fig_13\"},\"end\":64684,\"start\":64565},{\"attributes\":{\"id\":\"fig_14\"},\"end\":64743,\"start\":64685},{\"attributes\":{\"id\":\"fig_15\"},\"end\":64786,\"start\":64744},{\"attributes\":{\"id\":\"fig_16\"},\"end\":65033,\"start\":64787},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":65563,\"start\":65034},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":66130,\"start\":65564},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":66391,\"start\":66131},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66653,\"start\":66392}]", "paragraph": "[{\"end\":4526,\"start\":4035},{\"end\":5304,\"start\":4528},{\"end\":5853,\"start\":5306},{\"end\":7217,\"start\":5855},{\"end\":8848,\"start\":7219},{\"end\":9017,\"start\":8864},{\"end\":9273,\"start\":9063},{\"end\":9679,\"start\":9275},{\"end\":10156,\"start\":9712},{\"end\":10415,\"start\":10178},{\"end\":11694,\"start\":10429},{\"end\":12452,\"start\":11834},{\"end\":13341,\"start\":12494},{\"end\":13673,\"start\":13381},{\"end\":14130,\"start\":13713},{\"end\":15479,\"start\":14172},{\"end\":17394,\"start\":15496},{\"end\":17766,\"start\":17396},{\"end\":18503,\"start\":17768},{\"end\":19655,\"start\":18505},{\"end\":20945,\"start\":19657},{\"end\":21117,\"start\":20978},{\"end\":21440,\"start\":21119},{\"end\":22219,\"start\":21442},{\"end\":25095,\"start\":22221},{\"end\":25691,\"start\":25112},{\"end\":26258,\"start\":25693},{\"end\":27695,\"start\":26273},{\"end\":29227,\"start\":27703},{\"end\":30538,\"start\":29229},{\"end\":30754,\"start\":30557},{\"end\":32403,\"start\":30756},{\"end\":33051,\"start\":32405},{\"end\":33584,\"start\":33053},{\"end\":35106,\"start\":33603},{\"end\":38493,\"start\":35143},{\"end\":39514,\"start\":38516},{\"end\":43270,\"start\":39516},{\"end\":44372,\"start\":43306},{\"end\":45638,\"start\":44402},{\"end\":48091,\"start\":45646},{\"end\":51494,\"start\":48164},{\"end\":52544,\"start\":51578},{\"end\":53702,\"start\":52599},{\"end\":55350,\"start\":53840},{\"end\":56060,\"start\":55352},{\"end\":57100,\"start\":56102},{\"end\":58119,\"start\":57153},{\"end\":59725,\"start\":58121},{\"end\":61641,\"start\":59727},{\"end\":63142,\"start\":61656},{\"end\":4526,\"start\":4035},{\"end\":5304,\"start\":4528},{\"end\":5853,\"start\":5306},{\"end\":7217,\"start\":5855},{\"end\":8848,\"start\":7219},{\"end\":9017,\"start\":8864},{\"end\":9273,\"start\":9063},{\"end\":9679,\"start\":9275},{\"end\":10156,\"start\":9712},{\"end\":10415,\"start\":10178},{\"end\":11694,\"start\":10429},{\"end\":12452,\"start\":11834},{\"end\":13341,\"start\":12494},{\"end\":13673,\"start\":13381},{\"end\":14130,\"start\":13713},{\"end\":15479,\"start\":14172},{\"end\":17394,\"start\":15496},{\"end\":17766,\"start\":17396},{\"end\":18503,\"start\":17768},{\"end\":19655,\"start\":18505},{\"end\":20945,\"start\":19657},{\"end\":21117,\"start\":20978},{\"end\":21440,\"start\":21119},{\"end\":22219,\"start\":21442},{\"end\":25095,\"start\":22221},{\"end\":25691,\"start\":25112},{\"end\":26258,\"start\":25693},{\"end\":27695,\"start\":26273},{\"end\":29227,\"start\":27703},{\"end\":30538,\"start\":29229},{\"end\":30754,\"start\":30557},{\"end\":32403,\"start\":30756},{\"end\":33051,\"start\":32405},{\"end\":33584,\"start\":33053},{\"end\":35106,\"start\":33603},{\"end\":38493,\"start\":35143},{\"end\":39514,\"start\":38516},{\"end\":43270,\"start\":39516},{\"end\":44372,\"start\":43306},{\"end\":45638,\"start\":44402},{\"end\":48091,\"start\":45646},{\"end\":51494,\"start\":48164},{\"end\":52544,\"start\":51578},{\"end\":53702,\"start\":52599},{\"end\":55350,\"start\":53840},{\"end\":56060,\"start\":55352},{\"end\":57100,\"start\":56102},{\"end\":58119,\"start\":57153},{\"end\":59725,\"start\":58121},{\"end\":61641,\"start\":59727},{\"end\":63142,\"start\":61656}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9711,\"start\":9680},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10177,\"start\":10157},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10428,\"start\":10416},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11833,\"start\":11695},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12493,\"start\":12453},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13380,\"start\":13342},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13712,\"start\":13674},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20977,\"start\":20946},{\"attributes\":{\"id\":\"formula_0\"},\"end\":9711,\"start\":9680},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10177,\"start\":10157},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10428,\"start\":10416},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11833,\"start\":11695},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12493,\"start\":12453},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13380,\"start\":13342},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13712,\"start\":13674},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20977,\"start\":20946}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":44371,\"start\":44364},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44543,\"start\":44536},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":48174,\"start\":48167},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":52609,\"start\":52602},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":44371,\"start\":44364},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44543,\"start\":44536},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":48174,\"start\":48167},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":52609,\"start\":52602}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4033,\"start\":4021},{\"attributes\":{\"n\":\"2\"},\"end\":8862,\"start\":8851},{\"attributes\":{\"n\":\"2.1\"},\"end\":9061,\"start\":9020},{\"attributes\":{\"n\":\"2.2\"},\"end\":14170,\"start\":14133},{\"attributes\":{\"n\":\"2.3\"},\"end\":15494,\"start\":15482},{\"attributes\":{\"n\":\"3.1\"},\"end\":25110,\"start\":25098},{\"attributes\":{\"n\":\"3.2\"},\"end\":26271,\"start\":26261},{\"end\":27701,\"start\":27698},{\"end\":30555,\"start\":30541},{\"end\":33593,\"start\":33587},{\"end\":33601,\"start\":33596},{\"attributes\":{\"n\":\"3.4\"},\"end\":35141,\"start\":35109},{\"attributes\":{\"n\":\"4\"},\"end\":38514,\"start\":38496},{\"attributes\":{\"n\":\"5\"},\"end\":43283,\"start\":43273},{\"attributes\":{\"n\":\"5.1\"},\"end\":43304,\"start\":43286},{\"attributes\":{\"n\":\"5.2\"},\"end\":44400,\"start\":44375},{\"end\":45644,\"start\":45641},{\"attributes\":{\"n\":\"5.4\"},\"end\":48162,\"start\":48094},{\"attributes\":{\"n\":\"5.5\"},\"end\":51546,\"start\":51497},{\"attributes\":{\"n\":\"5.6\"},\"end\":51576,\"start\":51549},{\"attributes\":{\"n\":\"5.7\"},\"end\":52597,\"start\":52547},{\"attributes\":{\"n\":\"5.8\"},\"end\":53783,\"start\":53705},{\"attributes\":{\"n\":\"5.9\"},\"end\":53838,\"start\":53786},{\"attributes\":{\"n\":\"5.10\"},\"end\":56100,\"start\":56063},{\"attributes\":{\"n\":\"5.11\"},\"end\":57151,\"start\":57103},{\"attributes\":{\"n\":\"6\"},\"end\":61654,\"start\":61644},{\"end\":63152,\"start\":63144},{\"end\":63212,\"start\":63204},{\"end\":63255,\"start\":63247},{\"end\":63357,\"start\":63349},{\"end\":63505,\"start\":63497},{\"end\":63602,\"start\":63594},{\"end\":63843,\"start\":63835},{\"end\":63907,\"start\":63898},{\"end\":64057,\"start\":64048},{\"end\":64161,\"start\":64152},{\"end\":64273,\"start\":64264},{\"end\":64343,\"start\":64334},{\"end\":64492,\"start\":64483},{\"end\":64575,\"start\":64566},{\"end\":64695,\"start\":64686},{\"end\":64797,\"start\":64788},{\"end\":65044,\"start\":65035},{\"end\":65574,\"start\":65565},{\"end\":66141,\"start\":66132},{\"end\":66402,\"start\":66393},{\"attributes\":{\"n\":\"1\"},\"end\":4033,\"start\":4021},{\"attributes\":{\"n\":\"2\"},\"end\":8862,\"start\":8851},{\"attributes\":{\"n\":\"2.1\"},\"end\":9061,\"start\":9020},{\"attributes\":{\"n\":\"2.2\"},\"end\":14170,\"start\":14133},{\"attributes\":{\"n\":\"2.3\"},\"end\":15494,\"start\":15482},{\"attributes\":{\"n\":\"3.1\"},\"end\":25110,\"start\":25098},{\"attributes\":{\"n\":\"3.2\"},\"end\":26271,\"start\":26261},{\"end\":27701,\"start\":27698},{\"end\":30555,\"start\":30541},{\"end\":33593,\"start\":33587},{\"end\":33601,\"start\":33596},{\"attributes\":{\"n\":\"3.4\"},\"end\":35141,\"start\":35109},{\"attributes\":{\"n\":\"4\"},\"end\":38514,\"start\":38496},{\"attributes\":{\"n\":\"5\"},\"end\":43283,\"start\":43273},{\"attributes\":{\"n\":\"5.1\"},\"end\":43304,\"start\":43286},{\"attributes\":{\"n\":\"5.2\"},\"end\":44400,\"start\":44375},{\"end\":45644,\"start\":45641},{\"attributes\":{\"n\":\"5.4\"},\"end\":48162,\"start\":48094},{\"attributes\":{\"n\":\"5.5\"},\"end\":51546,\"start\":51497},{\"attributes\":{\"n\":\"5.6\"},\"end\":51576,\"start\":51549},{\"attributes\":{\"n\":\"5.7\"},\"end\":52597,\"start\":52547},{\"attributes\":{\"n\":\"5.8\"},\"end\":53783,\"start\":53705},{\"attributes\":{\"n\":\"5.9\"},\"end\":53838,\"start\":53786},{\"attributes\":{\"n\":\"5.10\"},\"end\":56100,\"start\":56063},{\"attributes\":{\"n\":\"5.11\"},\"end\":57151,\"start\":57103},{\"attributes\":{\"n\":\"6\"},\"end\":61654,\"start\":61644},{\"end\":63152,\"start\":63144},{\"end\":63212,\"start\":63204},{\"end\":63255,\"start\":63247},{\"end\":63357,\"start\":63349},{\"end\":63505,\"start\":63497},{\"end\":63602,\"start\":63594},{\"end\":63843,\"start\":63835},{\"end\":63907,\"start\":63898},{\"end\":64057,\"start\":64048},{\"end\":64161,\"start\":64152},{\"end\":64273,\"start\":64264},{\"end\":64343,\"start\":64334},{\"end\":64492,\"start\":64483},{\"end\":64575,\"start\":64566},{\"end\":64695,\"start\":64686},{\"end\":64797,\"start\":64788},{\"end\":65044,\"start\":65035},{\"end\":65574,\"start\":65565},{\"end\":66141,\"start\":66132},{\"end\":66402,\"start\":66393}]", "table": "[{\"end\":65563,\"start\":65065},{\"end\":66130,\"start\":65696},{\"end\":66391,\"start\":66179},{\"end\":66653,\"start\":66481},{\"end\":65563,\"start\":65065},{\"end\":66130,\"start\":65696},{\"end\":66391,\"start\":66179},{\"end\":66653,\"start\":66481}]", "figure_caption": "[{\"end\":63202,\"start\":63154},{\"end\":63245,\"start\":63214},{\"end\":63347,\"start\":63257},{\"end\":63495,\"start\":63359},{\"end\":63592,\"start\":63507},{\"end\":63833,\"start\":63604},{\"end\":63896,\"start\":63845},{\"end\":64046,\"start\":63910},{\"end\":64150,\"start\":64060},{\"end\":64262,\"start\":64164},{\"end\":64332,\"start\":64276},{\"end\":64481,\"start\":64346},{\"end\":64564,\"start\":64495},{\"end\":64684,\"start\":64578},{\"end\":64743,\"start\":64698},{\"end\":64786,\"start\":64746},{\"end\":65033,\"start\":64800},{\"end\":65065,\"start\":65046},{\"end\":65696,\"start\":65576},{\"end\":66179,\"start\":66143},{\"end\":66481,\"start\":66404},{\"end\":63202,\"start\":63154},{\"end\":63245,\"start\":63214},{\"end\":63347,\"start\":63257},{\"end\":63495,\"start\":63359},{\"end\":63592,\"start\":63507},{\"end\":63833,\"start\":63604},{\"end\":63896,\"start\":63845},{\"end\":64046,\"start\":63910},{\"end\":64150,\"start\":64060},{\"end\":64262,\"start\":64164},{\"end\":64332,\"start\":64276},{\"end\":64481,\"start\":64346},{\"end\":64564,\"start\":64495},{\"end\":64684,\"start\":64578},{\"end\":64743,\"start\":64698},{\"end\":64786,\"start\":64746},{\"end\":65033,\"start\":64800},{\"end\":65065,\"start\":65046},{\"end\":65696,\"start\":65576},{\"end\":66179,\"start\":66143},{\"end\":66481,\"start\":66404}]", "figure_ref": "[{\"end\":9272,\"start\":9264},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19651,\"start\":19643},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21160,\"start\":21152},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22816,\"start\":22808},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23867,\"start\":23859},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24500,\"start\":24492},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24803,\"start\":24795},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25713,\"start\":25705},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28144,\"start\":28136},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31039,\"start\":31031},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34704,\"start\":34696},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36577,\"start\":36569},{\"end\":41118,\"start\":41110},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":41332,\"start\":41324},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45728,\"start\":45720},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46563,\"start\":46554},{\"end\":47099,\"start\":47090},{\"end\":47408,\"start\":47399},{\"end\":48638,\"start\":48631},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49253,\"start\":49244},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":50137,\"start\":50128},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":51802,\"start\":51793},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":53144,\"start\":53135},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":53981,\"start\":53972},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":54667,\"start\":54658},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56422,\"start\":56413},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":58747,\"start\":58738},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":58832,\"start\":58824},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59739,\"start\":59730},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60318,\"start\":60309},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60641,\"start\":60632},{\"end\":9272,\"start\":9264},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19651,\"start\":19643},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21160,\"start\":21152},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22816,\"start\":22808},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23867,\"start\":23859},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24500,\"start\":24492},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24803,\"start\":24795},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25713,\"start\":25705},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28144,\"start\":28136},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31039,\"start\":31031},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34704,\"start\":34696},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36577,\"start\":36569},{\"end\":41118,\"start\":41110},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":41332,\"start\":41324},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45728,\"start\":45720},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46563,\"start\":46554},{\"end\":47099,\"start\":47090},{\"end\":47408,\"start\":47399},{\"end\":48638,\"start\":48631},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49253,\"start\":49244},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":50137,\"start\":50128},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":51802,\"start\":51793},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":53144,\"start\":53135},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":53981,\"start\":53972},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":54667,\"start\":54658},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56422,\"start\":56413},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":58747,\"start\":58738},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":58832,\"start\":58824},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59739,\"start\":59730},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60318,\"start\":60309},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60641,\"start\":60632}]", "bib_author_first_name": "[{\"end\":66839,\"start\":66834},{\"end\":66850,\"start\":66849},{\"end\":66858,\"start\":66851},{\"end\":66873,\"start\":66868},{\"end\":66875,\"start\":66874},{\"end\":66885,\"start\":66880},{\"end\":66903,\"start\":66897},{\"end\":66925,\"start\":66921},{\"end\":67288,\"start\":67282},{\"end\":67578,\"start\":67572},{\"end\":68028,\"start\":68022},{\"end\":68042,\"start\":68036},{\"end\":68056,\"start\":68050},{\"end\":68069,\"start\":68063},{\"end\":68465,\"start\":68460},{\"end\":68486,\"start\":68478},{\"end\":68500,\"start\":68496},{\"end\":68516,\"start\":68509},{\"end\":69025,\"start\":69019},{\"end\":69038,\"start\":69033},{\"end\":69053,\"start\":69047},{\"end\":69066,\"start\":69060},{\"end\":69078,\"start\":69075},{\"end\":69080,\"start\":69079},{\"end\":69617,\"start\":69611},{\"end\":70093,\"start\":70086},{\"end\":70516,\"start\":70510},{\"end\":70973,\"start\":70965},{\"end\":70988,\"start\":70983},{\"end\":71008,\"start\":71001},{\"end\":71340,\"start\":71337},{\"end\":71762,\"start\":71756},{\"end\":71764,\"start\":71763},{\"end\":71783,\"start\":71776},{\"end\":71798,\"start\":71792},{\"end\":71800,\"start\":71799},{\"end\":71813,\"start\":71808},{\"end\":71828,\"start\":71822},{\"end\":72111,\"start\":72105},{\"end\":72125,\"start\":72119},{\"end\":72137,\"start\":72132},{\"end\":72152,\"start\":72146},{\"end\":72633,\"start\":72627},{\"end\":73063,\"start\":73058},{\"end\":73078,\"start\":73072},{\"end\":73094,\"start\":73088},{\"end\":73595,\"start\":73589},{\"end\":74055,\"start\":74049},{\"end\":74532,\"start\":74528},{\"end\":74969,\"start\":74963},{\"end\":75403,\"start\":75397},{\"end\":75927,\"start\":75920},{\"end\":75944,\"start\":75938},{\"end\":76560,\"start\":76555},{\"end\":76575,\"start\":76569},{\"end\":76589,\"start\":76585},{\"end\":76601,\"start\":76598},{\"end\":76603,\"start\":76602},{\"end\":76968,\"start\":76963},{\"end\":76983,\"start\":76977},{\"end\":76996,\"start\":76993},{\"end\":76998,\"start\":76997},{\"end\":77470,\"start\":77464},{\"end\":77484,\"start\":77478},{\"end\":77496,\"start\":77491},{\"end\":77511,\"start\":77505},{\"end\":77979,\"start\":77973},{\"end\":78370,\"start\":78363},{\"end\":78825,\"start\":78820},{\"end\":78837,\"start\":78832},{\"end\":78854,\"start\":78848},{\"end\":79340,\"start\":79332},{\"end\":79342,\"start\":79341},{\"end\":79650,\"start\":79643},{\"end\":79671,\"start\":79661},{\"end\":80914,\"start\":80907},{\"end\":80928,\"start\":80924},{\"end\":80942,\"start\":80936},{\"end\":81320,\"start\":81314},{\"end\":81679,\"start\":81672},{\"end\":81693,\"start\":81687},{\"end\":81708,\"start\":81702},{\"end\":81727,\"start\":81716},{\"end\":81745,\"start\":81738},{\"end\":81756,\"start\":81750},{\"end\":82174,\"start\":82166},{\"end\":82192,\"start\":82184},{\"end\":82213,\"start\":82206},{\"end\":82837,\"start\":82834},{\"end\":82850,\"start\":82844},{\"end\":82863,\"start\":82856},{\"end\":82875,\"start\":82869},{\"end\":83414,\"start\":83408},{\"end\":83429,\"start\":83422},{\"end\":83441,\"start\":83435},{\"end\":83453,\"start\":83449},{\"end\":83470,\"start\":83464},{\"end\":83997,\"start\":83991},{\"end\":84011,\"start\":84005},{\"end\":84027,\"start\":84021},{\"end\":66839,\"start\":66834},{\"end\":66850,\"start\":66849},{\"end\":66858,\"start\":66851},{\"end\":66873,\"start\":66868},{\"end\":66875,\"start\":66874},{\"end\":66885,\"start\":66880},{\"end\":66903,\"start\":66897},{\"end\":66925,\"start\":66921},{\"end\":67288,\"start\":67282},{\"end\":67578,\"start\":67572},{\"end\":68028,\"start\":68022},{\"end\":68042,\"start\":68036},{\"end\":68056,\"start\":68050},{\"end\":68069,\"start\":68063},{\"end\":68465,\"start\":68460},{\"end\":68486,\"start\":68478},{\"end\":68500,\"start\":68496},{\"end\":68516,\"start\":68509},{\"end\":69025,\"start\":69019},{\"end\":69038,\"start\":69033},{\"end\":69053,\"start\":69047},{\"end\":69066,\"start\":69060},{\"end\":69078,\"start\":69075},{\"end\":69080,\"start\":69079},{\"end\":69617,\"start\":69611},{\"end\":70093,\"start\":70086},{\"end\":70516,\"start\":70510},{\"end\":70973,\"start\":70965},{\"end\":70988,\"start\":70983},{\"end\":71008,\"start\":71001},{\"end\":71340,\"start\":71337},{\"end\":71762,\"start\":71756},{\"end\":71764,\"start\":71763},{\"end\":71783,\"start\":71776},{\"end\":71798,\"start\":71792},{\"end\":71800,\"start\":71799},{\"end\":71813,\"start\":71808},{\"end\":71828,\"start\":71822},{\"end\":72111,\"start\":72105},{\"end\":72125,\"start\":72119},{\"end\":72137,\"start\":72132},{\"end\":72152,\"start\":72146},{\"end\":72633,\"start\":72627},{\"end\":73063,\"start\":73058},{\"end\":73078,\"start\":73072},{\"end\":73094,\"start\":73088},{\"end\":73595,\"start\":73589},{\"end\":74055,\"start\":74049},{\"end\":74532,\"start\":74528},{\"end\":74969,\"start\":74963},{\"end\":75403,\"start\":75397},{\"end\":75927,\"start\":75920},{\"end\":75944,\"start\":75938},{\"end\":76560,\"start\":76555},{\"end\":76575,\"start\":76569},{\"end\":76589,\"start\":76585},{\"end\":76601,\"start\":76598},{\"end\":76603,\"start\":76602},{\"end\":76968,\"start\":76963},{\"end\":76983,\"start\":76977},{\"end\":76996,\"start\":76993},{\"end\":76998,\"start\":76997},{\"end\":77470,\"start\":77464},{\"end\":77484,\"start\":77478},{\"end\":77496,\"start\":77491},{\"end\":77511,\"start\":77505},{\"end\":77979,\"start\":77973},{\"end\":78370,\"start\":78363},{\"end\":78825,\"start\":78820},{\"end\":78837,\"start\":78832},{\"end\":78854,\"start\":78848},{\"end\":79340,\"start\":79332},{\"end\":79342,\"start\":79341},{\"end\":79650,\"start\":79643},{\"end\":79671,\"start\":79661},{\"end\":80914,\"start\":80907},{\"end\":80928,\"start\":80924},{\"end\":80942,\"start\":80936},{\"end\":81320,\"start\":81314},{\"end\":81679,\"start\":81672},{\"end\":81693,\"start\":81687},{\"end\":81708,\"start\":81702},{\"end\":81727,\"start\":81716},{\"end\":81745,\"start\":81738},{\"end\":81756,\"start\":81750},{\"end\":82174,\"start\":82166},{\"end\":82192,\"start\":82184},{\"end\":82213,\"start\":82206},{\"end\":82837,\"start\":82834},{\"end\":82850,\"start\":82844},{\"end\":82863,\"start\":82856},{\"end\":82875,\"start\":82869},{\"end\":83414,\"start\":83408},{\"end\":83429,\"start\":83422},{\"end\":83441,\"start\":83435},{\"end\":83453,\"start\":83449},{\"end\":83470,\"start\":83464},{\"end\":83997,\"start\":83991},{\"end\":84011,\"start\":84005},{\"end\":84027,\"start\":84021}]", "bib_author_last_name": "[{\"end\":66847,\"start\":66840},{\"end\":66866,\"start\":66859},{\"end\":66878,\"start\":66876},{\"end\":66895,\"start\":66886},{\"end\":66919,\"start\":66904},{\"end\":66931,\"start\":66926},{\"end\":67296,\"start\":67289},{\"end\":67584,\"start\":67579},{\"end\":68034,\"start\":68029},{\"end\":68048,\"start\":68043},{\"end\":68061,\"start\":68057},{\"end\":68076,\"start\":68070},{\"end\":68476,\"start\":68466},{\"end\":68494,\"start\":68487},{\"end\":68507,\"start\":68501},{\"end\":68527,\"start\":68517},{\"end\":69031,\"start\":69026},{\"end\":69045,\"start\":69039},{\"end\":69058,\"start\":69054},{\"end\":69073,\"start\":69067},{\"end\":69087,\"start\":69081},{\"end\":69625,\"start\":69618},{\"end\":70099,\"start\":70094},{\"end\":70522,\"start\":70517},{\"end\":70981,\"start\":70974},{\"end\":70999,\"start\":70989},{\"end\":71019,\"start\":71009},{\"end\":71348,\"start\":71341},{\"end\":71774,\"start\":71765},{\"end\":71790,\"start\":71784},{\"end\":71806,\"start\":71801},{\"end\":71820,\"start\":71814},{\"end\":71835,\"start\":71829},{\"end\":72117,\"start\":72112},{\"end\":72130,\"start\":72126},{\"end\":72144,\"start\":72138},{\"end\":72159,\"start\":72153},{\"end\":72639,\"start\":72634},{\"end\":73070,\"start\":73064},{\"end\":73086,\"start\":73079},{\"end\":73102,\"start\":73095},{\"end\":73601,\"start\":73596},{\"end\":74061,\"start\":74056},{\"end\":74535,\"start\":74533},{\"end\":74975,\"start\":74970},{\"end\":75409,\"start\":75404},{\"end\":75936,\"start\":75928},{\"end\":75951,\"start\":75945},{\"end\":76567,\"start\":76561},{\"end\":76583,\"start\":76576},{\"end\":76596,\"start\":76590},{\"end\":76610,\"start\":76604},{\"end\":76975,\"start\":76969},{\"end\":76991,\"start\":76984},{\"end\":77005,\"start\":76999},{\"end\":77476,\"start\":77471},{\"end\":77489,\"start\":77485},{\"end\":77503,\"start\":77497},{\"end\":77518,\"start\":77512},{\"end\":77985,\"start\":77980},{\"end\":78373,\"start\":78371},{\"end\":78830,\"start\":78826},{\"end\":78846,\"start\":78838},{\"end\":78869,\"start\":78855},{\"end\":79153,\"start\":79149},{\"end\":79352,\"start\":79343},{\"end\":79659,\"start\":79651},{\"end\":79680,\"start\":79672},{\"end\":80922,\"start\":80915},{\"end\":80934,\"start\":80929},{\"end\":80949,\"start\":80943},{\"end\":81327,\"start\":81321},{\"end\":81685,\"start\":81680},{\"end\":81700,\"start\":81694},{\"end\":81714,\"start\":81709},{\"end\":81736,\"start\":81728},{\"end\":81748,\"start\":81746},{\"end\":81763,\"start\":81757},{\"end\":82182,\"start\":82175},{\"end\":82204,\"start\":82193},{\"end\":82224,\"start\":82214},{\"end\":82842,\"start\":82838},{\"end\":82854,\"start\":82851},{\"end\":82867,\"start\":82864},{\"end\":82888,\"start\":82876},{\"end\":83420,\"start\":83415},{\"end\":83433,\"start\":83430},{\"end\":83447,\"start\":83442},{\"end\":83462,\"start\":83454},{\"end\":83474,\"start\":83471},{\"end\":84003,\"start\":83998},{\"end\":84019,\"start\":84012},{\"end\":84032,\"start\":84028},{\"end\":66847,\"start\":66840},{\"end\":66866,\"start\":66859},{\"end\":66878,\"start\":66876},{\"end\":66895,\"start\":66886},{\"end\":66919,\"start\":66904},{\"end\":66931,\"start\":66926},{\"end\":67296,\"start\":67289},{\"end\":67584,\"start\":67579},{\"end\":68034,\"start\":68029},{\"end\":68048,\"start\":68043},{\"end\":68061,\"start\":68057},{\"end\":68076,\"start\":68070},{\"end\":68476,\"start\":68466},{\"end\":68494,\"start\":68487},{\"end\":68507,\"start\":68501},{\"end\":68527,\"start\":68517},{\"end\":69031,\"start\":69026},{\"end\":69045,\"start\":69039},{\"end\":69058,\"start\":69054},{\"end\":69073,\"start\":69067},{\"end\":69087,\"start\":69081},{\"end\":69625,\"start\":69618},{\"end\":70099,\"start\":70094},{\"end\":70522,\"start\":70517},{\"end\":70981,\"start\":70974},{\"end\":70999,\"start\":70989},{\"end\":71019,\"start\":71009},{\"end\":71348,\"start\":71341},{\"end\":71774,\"start\":71765},{\"end\":71790,\"start\":71784},{\"end\":71806,\"start\":71801},{\"end\":71820,\"start\":71814},{\"end\":71835,\"start\":71829},{\"end\":72117,\"start\":72112},{\"end\":72130,\"start\":72126},{\"end\":72144,\"start\":72138},{\"end\":72159,\"start\":72153},{\"end\":72639,\"start\":72634},{\"end\":73070,\"start\":73064},{\"end\":73086,\"start\":73079},{\"end\":73102,\"start\":73095},{\"end\":73601,\"start\":73596},{\"end\":74061,\"start\":74056},{\"end\":74535,\"start\":74533},{\"end\":74975,\"start\":74970},{\"end\":75409,\"start\":75404},{\"end\":75936,\"start\":75928},{\"end\":75951,\"start\":75945},{\"end\":76567,\"start\":76561},{\"end\":76583,\"start\":76576},{\"end\":76596,\"start\":76590},{\"end\":76610,\"start\":76604},{\"end\":76975,\"start\":76969},{\"end\":76991,\"start\":76984},{\"end\":77005,\"start\":76999},{\"end\":77476,\"start\":77471},{\"end\":77489,\"start\":77485},{\"end\":77503,\"start\":77497},{\"end\":77518,\"start\":77512},{\"end\":77985,\"start\":77980},{\"end\":78373,\"start\":78371},{\"end\":78830,\"start\":78826},{\"end\":78846,\"start\":78838},{\"end\":78869,\"start\":78855},{\"end\":79153,\"start\":79149},{\"end\":79352,\"start\":79343},{\"end\":79659,\"start\":79651},{\"end\":79680,\"start\":79672},{\"end\":80922,\"start\":80915},{\"end\":80934,\"start\":80929},{\"end\":80949,\"start\":80943},{\"end\":81327,\"start\":81321},{\"end\":81685,\"start\":81680},{\"end\":81700,\"start\":81694},{\"end\":81714,\"start\":81709},{\"end\":81736,\"start\":81728},{\"end\":81748,\"start\":81746},{\"end\":81763,\"start\":81757},{\"end\":82182,\"start\":82175},{\"end\":82204,\"start\":82193},{\"end\":82224,\"start\":82214},{\"end\":82842,\"start\":82838},{\"end\":82854,\"start\":82851},{\"end\":82867,\"start\":82864},{\"end\":82888,\"start\":82876},{\"end\":83420,\"start\":83415},{\"end\":83433,\"start\":83430},{\"end\":83447,\"start\":83442},{\"end\":83462,\"start\":83454},{\"end\":83474,\"start\":83471},{\"end\":84003,\"start\":83998},{\"end\":84019,\"start\":84012},{\"end\":84032,\"start\":84028}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":67155,\"start\":66763},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":733980},\"end\":67495,\"start\":67157},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":233376633},\"end\":67943,\"start\":67497},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49301394},\"end\":68405,\"start\":67945},{\"attributes\":{\"doi\":\"10.1145/3377930.3390226\",\"id\":\"b4\",\"matched_paper_id\":215548747},\"end\":68970,\"start\":68407},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1677864},\"end\":69524,\"start\":68972},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":67872077},\"end\":70034,\"start\":69526},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53235957},\"end\":70422,\"start\":70036},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":222305243},\"end\":70895,\"start\":70424},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":218517454},\"end\":71240,\"start\":70897},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6329628},\"end\":71709,\"start\":71242},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18689058},\"end\":72033,\"start\":71711},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":21351739},\"end\":72531,\"start\":72035},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":163164623},\"end\":72964,\"start\":72533},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12008695},\"end\":73504,\"start\":72966},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":155106744},\"end\":73987,\"start\":73506},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":155109576},\"end\":74426,\"start\":73989},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":232266194},\"end\":74875,\"start\":74428},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":189818835},\"end\":75330,\"start\":74877},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":208154666},\"end\":75806,\"start\":75332},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195750708},\"end\":76413,\"start\":75808},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57365377},\"end\":76871,\"start\":76415},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9812826},\"end\":77392,\"start\":76873},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":21351739},\"end\":77905,\"start\":77394},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211016154},\"end\":78212,\"start\":77907},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":25209638},\"end\":78775,\"start\":78214},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4427557},\"end\":79055,\"start\":78777},{\"attributes\":{\"id\":\"b27\"},\"end\":79330,\"start\":79057},{\"attributes\":{\"id\":\"b28\"},\"end\":79555,\"start\":79332},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":61521866},\"end\":80076,\"start\":79557},{\"attributes\":{\"id\":\"b30\"},\"end\":80205,\"start\":80078},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":166228598},\"end\":80552,\"start\":80207},{\"attributes\":{\"id\":\"b32\"},\"end\":80716,\"start\":80554},{\"attributes\":{\"id\":\"b33\"},\"end\":80868,\"start\":80718},{\"attributes\":{\"id\":\"b34\"},\"end\":81111,\"start\":80870},{\"attributes\":{\"id\":\"b35\"},\"end\":81252,\"start\":81113},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2402811},\"end\":81670,\"start\":81254},{\"attributes\":{\"id\":\"b37\"},\"end\":82101,\"start\":81672},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5968842},\"end\":82716,\"start\":82103},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":222297673},\"end\":83288,\"start\":82718},{\"attributes\":{\"id\":\"b40\"},\"end\":83921,\"start\":83290},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":23226786},\"end\":84398,\"start\":83923},{\"attributes\":{\"id\":\"b0\"},\"end\":67155,\"start\":66763},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":733980},\"end\":67495,\"start\":67157},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":233376633},\"end\":67943,\"start\":67497},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49301394},\"end\":68405,\"start\":67945},{\"attributes\":{\"doi\":\"10.1145/3377930.3390226\",\"id\":\"b4\",\"matched_paper_id\":215548747},\"end\":68970,\"start\":68407},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1677864},\"end\":69524,\"start\":68972},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":67872077},\"end\":70034,\"start\":69526},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53235957},\"end\":70422,\"start\":70036},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":222305243},\"end\":70895,\"start\":70424},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":218517454},\"end\":71240,\"start\":70897},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6329628},\"end\":71709,\"start\":71242},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18689058},\"end\":72033,\"start\":71711},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":21351739},\"end\":72531,\"start\":72035},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":163164623},\"end\":72964,\"start\":72533},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12008695},\"end\":73504,\"start\":72966},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":155106744},\"end\":73987,\"start\":73506},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":155109576},\"end\":74426,\"start\":73989},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":232266194},\"end\":74875,\"start\":74428},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":189818835},\"end\":75330,\"start\":74877},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":208154666},\"end\":75806,\"start\":75332},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195750708},\"end\":76413,\"start\":75808},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57365377},\"end\":76871,\"start\":76415},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9812826},\"end\":77392,\"start\":76873},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":21351739},\"end\":77905,\"start\":77394},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211016154},\"end\":78212,\"start\":77907},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":25209638},\"end\":78775,\"start\":78214},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4427557},\"end\":79055,\"start\":78777},{\"attributes\":{\"id\":\"b27\"},\"end\":79330,\"start\":79057},{\"attributes\":{\"id\":\"b28\"},\"end\":79555,\"start\":79332},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":61521866},\"end\":80076,\"start\":79557},{\"attributes\":{\"id\":\"b30\"},\"end\":80205,\"start\":80078},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":166228598},\"end\":80552,\"start\":80207},{\"attributes\":{\"id\":\"b32\"},\"end\":80716,\"start\":80554},{\"attributes\":{\"id\":\"b33\"},\"end\":80868,\"start\":80718},{\"attributes\":{\"id\":\"b34\"},\"end\":81111,\"start\":80870},{\"attributes\":{\"id\":\"b35\"},\"end\":81252,\"start\":81113},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2402811},\"end\":81670,\"start\":81254},{\"attributes\":{\"id\":\"b37\"},\"end\":82101,\"start\":81672},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5968842},\"end\":82716,\"start\":82103},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":222297673},\"end\":83288,\"start\":82718},{\"attributes\":{\"id\":\"b40\"},\"end\":83921,\"start\":83290},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":23226786},\"end\":84398,\"start\":83923}]", "bib_title": "[{\"end\":67280,\"start\":67157},{\"end\":67570,\"start\":67497},{\"end\":68020,\"start\":67945},{\"end\":68458,\"start\":68407},{\"end\":69017,\"start\":68972},{\"end\":69609,\"start\":69526},{\"end\":70084,\"start\":70036},{\"end\":70508,\"start\":70424},{\"end\":70963,\"start\":70897},{\"end\":71335,\"start\":71242},{\"end\":71754,\"start\":71711},{\"end\":72103,\"start\":72035},{\"end\":72625,\"start\":72533},{\"end\":73056,\"start\":72966},{\"end\":73587,\"start\":73506},{\"end\":74047,\"start\":73989},{\"end\":74526,\"start\":74428},{\"end\":74961,\"start\":74877},{\"end\":75395,\"start\":75332},{\"end\":75918,\"start\":75808},{\"end\":76553,\"start\":76415},{\"end\":76961,\"start\":76873},{\"end\":77462,\"start\":77394},{\"end\":77971,\"start\":77907},{\"end\":78361,\"start\":78214},{\"end\":78818,\"start\":78777},{\"end\":79641,\"start\":79557},{\"end\":80265,\"start\":80207},{\"end\":81312,\"start\":81254},{\"end\":82164,\"start\":82103},{\"end\":82832,\"start\":82718},{\"end\":83406,\"start\":83290},{\"end\":83989,\"start\":83923},{\"end\":67280,\"start\":67157},{\"end\":67570,\"start\":67497},{\"end\":68020,\"start\":67945},{\"end\":68458,\"start\":68407},{\"end\":69017,\"start\":68972},{\"end\":69609,\"start\":69526},{\"end\":70084,\"start\":70036},{\"end\":70508,\"start\":70424},{\"end\":70963,\"start\":70897},{\"end\":71335,\"start\":71242},{\"end\":71754,\"start\":71711},{\"end\":72103,\"start\":72035},{\"end\":72625,\"start\":72533},{\"end\":73056,\"start\":72966},{\"end\":73587,\"start\":73506},{\"end\":74047,\"start\":73989},{\"end\":74526,\"start\":74428},{\"end\":74961,\"start\":74877},{\"end\":75395,\"start\":75332},{\"end\":75918,\"start\":75808},{\"end\":76553,\"start\":76415},{\"end\":76961,\"start\":76873},{\"end\":77462,\"start\":77394},{\"end\":77971,\"start\":77907},{\"end\":78361,\"start\":78214},{\"end\":78818,\"start\":78777},{\"end\":79641,\"start\":79557},{\"end\":80265,\"start\":80207},{\"end\":81312,\"start\":81254},{\"end\":82164,\"start\":82103},{\"end\":82832,\"start\":82718},{\"end\":83406,\"start\":83290},{\"end\":83989,\"start\":83923}]", "bib_author": "[{\"end\":66849,\"start\":66834},{\"end\":66868,\"start\":66849},{\"end\":66880,\"start\":66868},{\"end\":66897,\"start\":66880},{\"end\":66921,\"start\":66897},{\"end\":66933,\"start\":66921},{\"end\":67298,\"start\":67282},{\"end\":67586,\"start\":67572},{\"end\":68036,\"start\":68022},{\"end\":68050,\"start\":68036},{\"end\":68063,\"start\":68050},{\"end\":68078,\"start\":68063},{\"end\":68478,\"start\":68460},{\"end\":68496,\"start\":68478},{\"end\":68509,\"start\":68496},{\"end\":68529,\"start\":68509},{\"end\":69033,\"start\":69019},{\"end\":69047,\"start\":69033},{\"end\":69060,\"start\":69047},{\"end\":69075,\"start\":69060},{\"end\":69089,\"start\":69075},{\"end\":69627,\"start\":69611},{\"end\":70101,\"start\":70086},{\"end\":70524,\"start\":70510},{\"end\":70983,\"start\":70965},{\"end\":71001,\"start\":70983},{\"end\":71021,\"start\":71001},{\"end\":71350,\"start\":71337},{\"end\":71776,\"start\":71756},{\"end\":71792,\"start\":71776},{\"end\":71808,\"start\":71792},{\"end\":71822,\"start\":71808},{\"end\":71837,\"start\":71822},{\"end\":72119,\"start\":72105},{\"end\":72132,\"start\":72119},{\"end\":72146,\"start\":72132},{\"end\":72161,\"start\":72146},{\"end\":72641,\"start\":72627},{\"end\":73072,\"start\":73058},{\"end\":73088,\"start\":73072},{\"end\":73104,\"start\":73088},{\"end\":73603,\"start\":73589},{\"end\":74063,\"start\":74049},{\"end\":74537,\"start\":74528},{\"end\":74977,\"start\":74963},{\"end\":75411,\"start\":75397},{\"end\":75938,\"start\":75920},{\"end\":75953,\"start\":75938},{\"end\":76569,\"start\":76555},{\"end\":76585,\"start\":76569},{\"end\":76598,\"start\":76585},{\"end\":76612,\"start\":76598},{\"end\":76977,\"start\":76963},{\"end\":76993,\"start\":76977},{\"end\":77007,\"start\":76993},{\"end\":77478,\"start\":77464},{\"end\":77491,\"start\":77478},{\"end\":77505,\"start\":77491},{\"end\":77520,\"start\":77505},{\"end\":77987,\"start\":77973},{\"end\":78375,\"start\":78363},{\"end\":78832,\"start\":78820},{\"end\":78848,\"start\":78832},{\"end\":78871,\"start\":78848},{\"end\":79155,\"start\":79149},{\"end\":79354,\"start\":79332},{\"end\":79661,\"start\":79643},{\"end\":79682,\"start\":79661},{\"end\":80924,\"start\":80907},{\"end\":80936,\"start\":80924},{\"end\":80951,\"start\":80936},{\"end\":81329,\"start\":81314},{\"end\":81687,\"start\":81672},{\"end\":81702,\"start\":81687},{\"end\":81716,\"start\":81702},{\"end\":81738,\"start\":81716},{\"end\":81750,\"start\":81738},{\"end\":81765,\"start\":81750},{\"end\":82184,\"start\":82166},{\"end\":82206,\"start\":82184},{\"end\":82226,\"start\":82206},{\"end\":82844,\"start\":82834},{\"end\":82856,\"start\":82844},{\"end\":82869,\"start\":82856},{\"end\":82890,\"start\":82869},{\"end\":83422,\"start\":83408},{\"end\":83435,\"start\":83422},{\"end\":83449,\"start\":83435},{\"end\":83464,\"start\":83449},{\"end\":83476,\"start\":83464},{\"end\":84005,\"start\":83991},{\"end\":84021,\"start\":84005},{\"end\":84034,\"start\":84021},{\"end\":66849,\"start\":66834},{\"end\":66868,\"start\":66849},{\"end\":66880,\"start\":66868},{\"end\":66897,\"start\":66880},{\"end\":66921,\"start\":66897},{\"end\":66933,\"start\":66921},{\"end\":67298,\"start\":67282},{\"end\":67586,\"start\":67572},{\"end\":68036,\"start\":68022},{\"end\":68050,\"start\":68036},{\"end\":68063,\"start\":68050},{\"end\":68078,\"start\":68063},{\"end\":68478,\"start\":68460},{\"end\":68496,\"start\":68478},{\"end\":68509,\"start\":68496},{\"end\":68529,\"start\":68509},{\"end\":69033,\"start\":69019},{\"end\":69047,\"start\":69033},{\"end\":69060,\"start\":69047},{\"end\":69075,\"start\":69060},{\"end\":69089,\"start\":69075},{\"end\":69627,\"start\":69611},{\"end\":70101,\"start\":70086},{\"end\":70524,\"start\":70510},{\"end\":70983,\"start\":70965},{\"end\":71001,\"start\":70983},{\"end\":71021,\"start\":71001},{\"end\":71350,\"start\":71337},{\"end\":71776,\"start\":71756},{\"end\":71792,\"start\":71776},{\"end\":71808,\"start\":71792},{\"end\":71822,\"start\":71808},{\"end\":71837,\"start\":71822},{\"end\":72119,\"start\":72105},{\"end\":72132,\"start\":72119},{\"end\":72146,\"start\":72132},{\"end\":72161,\"start\":72146},{\"end\":72641,\"start\":72627},{\"end\":73072,\"start\":73058},{\"end\":73088,\"start\":73072},{\"end\":73104,\"start\":73088},{\"end\":73603,\"start\":73589},{\"end\":74063,\"start\":74049},{\"end\":74537,\"start\":74528},{\"end\":74977,\"start\":74963},{\"end\":75411,\"start\":75397},{\"end\":75938,\"start\":75920},{\"end\":75953,\"start\":75938},{\"end\":76569,\"start\":76555},{\"end\":76585,\"start\":76569},{\"end\":76598,\"start\":76585},{\"end\":76612,\"start\":76598},{\"end\":76977,\"start\":76963},{\"end\":76993,\"start\":76977},{\"end\":77007,\"start\":76993},{\"end\":77478,\"start\":77464},{\"end\":77491,\"start\":77478},{\"end\":77505,\"start\":77491},{\"end\":77520,\"start\":77505},{\"end\":77987,\"start\":77973},{\"end\":78375,\"start\":78363},{\"end\":78832,\"start\":78820},{\"end\":78848,\"start\":78832},{\"end\":78871,\"start\":78848},{\"end\":79155,\"start\":79149},{\"end\":79354,\"start\":79332},{\"end\":79661,\"start\":79643},{\"end\":79682,\"start\":79661},{\"end\":80924,\"start\":80907},{\"end\":80936,\"start\":80924},{\"end\":80951,\"start\":80936},{\"end\":81329,\"start\":81314},{\"end\":81687,\"start\":81672},{\"end\":81702,\"start\":81687},{\"end\":81716,\"start\":81702},{\"end\":81738,\"start\":81716},{\"end\":81750,\"start\":81738},{\"end\":81765,\"start\":81750},{\"end\":82184,\"start\":82166},{\"end\":82206,\"start\":82184},{\"end\":82226,\"start\":82206},{\"end\":82844,\"start\":82834},{\"end\":82856,\"start\":82844},{\"end\":82869,\"start\":82856},{\"end\":82890,\"start\":82869},{\"end\":83422,\"start\":83408},{\"end\":83435,\"start\":83422},{\"end\":83449,\"start\":83435},{\"end\":83464,\"start\":83449},{\"end\":83476,\"start\":83464},{\"end\":84005,\"start\":83991},{\"end\":84021,\"start\":84005},{\"end\":84034,\"start\":84021}]", "bib_venue": "[{\"end\":66832,\"start\":66763},{\"end\":67310,\"start\":67298},{\"end\":67671,\"start\":67586},{\"end\":68137,\"start\":68078},{\"end\":68629,\"start\":68552},{\"end\":69188,\"start\":69089},{\"end\":69723,\"start\":69627},{\"end\":70180,\"start\":70101},{\"end\":70607,\"start\":70524},{\"end\":71051,\"start\":71021},{\"end\":71427,\"start\":71350},{\"end\":71852,\"start\":71837},{\"end\":72237,\"start\":72161},{\"end\":72706,\"start\":72641},{\"end\":73185,\"start\":73104},{\"end\":73694,\"start\":73603},{\"end\":74155,\"start\":74063},{\"end\":74609,\"start\":74537},{\"end\":75054,\"start\":74977},{\"end\":75510,\"start\":75411},{\"end\":76053,\"start\":75953},{\"end\":76622,\"start\":76612},{\"end\":77085,\"start\":77007},{\"end\":77601,\"start\":77520},{\"end\":78038,\"start\":77987},{\"end\":78447,\"start\":78375},{\"end\":78897,\"start\":78871},{\"end\":79147,\"start\":79057},{\"end\":79402,\"start\":79354},{\"end\":79763,\"start\":79682},{\"end\":80115,\"start\":80078},{\"end\":80331,\"start\":80267},{\"end\":80585,\"start\":80554},{\"end\":80749,\"start\":80718},{\"end\":80905,\"start\":80870},{\"end\":81144,\"start\":81113},{\"end\":81409,\"start\":81329},{\"end\":81885,\"start\":81765},{\"end\":82342,\"start\":82226},{\"end\":82960,\"start\":82890},{\"end\":83559,\"start\":83476},{\"end\":84113,\"start\":84034},{\"end\":66832,\"start\":66763},{\"end\":67310,\"start\":67298},{\"end\":67671,\"start\":67586},{\"end\":68137,\"start\":68078},{\"end\":68629,\"start\":68552},{\"end\":69188,\"start\":69089},{\"end\":69723,\"start\":69627},{\"end\":70180,\"start\":70101},{\"end\":70607,\"start\":70524},{\"end\":71051,\"start\":71021},{\"end\":71427,\"start\":71350},{\"end\":71852,\"start\":71837},{\"end\":72237,\"start\":72161},{\"end\":72706,\"start\":72641},{\"end\":73185,\"start\":73104},{\"end\":73694,\"start\":73603},{\"end\":74155,\"start\":74063},{\"end\":74609,\"start\":74537},{\"end\":75054,\"start\":74977},{\"end\":75510,\"start\":75411},{\"end\":76053,\"start\":75953},{\"end\":76622,\"start\":76612},{\"end\":77085,\"start\":77007},{\"end\":77601,\"start\":77520},{\"end\":78038,\"start\":77987},{\"end\":78447,\"start\":78375},{\"end\":78897,\"start\":78871},{\"end\":79147,\"start\":79057},{\"end\":79402,\"start\":79354},{\"end\":79763,\"start\":79682},{\"end\":80115,\"start\":80078},{\"end\":80331,\"start\":80267},{\"end\":80585,\"start\":80554},{\"end\":80749,\"start\":80718},{\"end\":80905,\"start\":80870},{\"end\":81144,\"start\":81113},{\"end\":81409,\"start\":81329},{\"end\":81885,\"start\":81765},{\"end\":82342,\"start\":82226},{\"end\":82960,\"start\":82890},{\"end\":83559,\"start\":83476},{\"end\":84113,\"start\":84034},{\"end\":67743,\"start\":67673},{\"end\":68183,\"start\":68139},{\"end\":68693,\"start\":68631},{\"end\":69274,\"start\":69190},{\"end\":69806,\"start\":69725},{\"end\":70246,\"start\":70182},{\"end\":70677,\"start\":70609},{\"end\":71491,\"start\":71429},{\"end\":72300,\"start\":72239},{\"end\":72758,\"start\":72708},{\"end\":73253,\"start\":73187},{\"end\":73772,\"start\":73696},{\"end\":74234,\"start\":74157},{\"end\":74668,\"start\":74611},{\"end\":75118,\"start\":75056},{\"end\":75596,\"start\":75512},{\"end\":76140,\"start\":76055},{\"end\":77150,\"start\":77087},{\"end\":77669,\"start\":77603},{\"end\":78506,\"start\":78449},{\"end\":79414,\"start\":79404},{\"end\":79831,\"start\":79765},{\"end\":81476,\"start\":81411},{\"end\":82445,\"start\":82344},{\"end\":83017,\"start\":82962},{\"end\":83629,\"start\":83561},{\"end\":84179,\"start\":84115},{\"end\":67743,\"start\":67673},{\"end\":68183,\"start\":68139},{\"end\":68693,\"start\":68631},{\"end\":69274,\"start\":69190},{\"end\":69806,\"start\":69725},{\"end\":70246,\"start\":70182},{\"end\":70677,\"start\":70609},{\"end\":71491,\"start\":71429},{\"end\":72300,\"start\":72239},{\"end\":72758,\"start\":72708},{\"end\":73253,\"start\":73187},{\"end\":73772,\"start\":73696},{\"end\":74234,\"start\":74157},{\"end\":74668,\"start\":74611},{\"end\":75118,\"start\":75056},{\"end\":75596,\"start\":75512},{\"end\":76140,\"start\":76055},{\"end\":77150,\"start\":77087},{\"end\":77669,\"start\":77603},{\"end\":78506,\"start\":78449},{\"end\":79414,\"start\":79404},{\"end\":79831,\"start\":79765},{\"end\":81476,\"start\":81411},{\"end\":82445,\"start\":82344},{\"end\":83017,\"start\":82962},{\"end\":83629,\"start\":83561},{\"end\":84179,\"start\":84115}]"}}}, "year": 2023, "month": 12, "day": 17}