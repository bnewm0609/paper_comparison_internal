{"id": 259165040, "updated": "2023-10-04 23:51:47.788", "metadata": {"title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", "authors": "[{\"first\":\"Peng\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Wenqi\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Kaipeng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Shuo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Meng\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Fanqing\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Siyuan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Qiao\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.09265", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-09265", "doi": "10.48550/arxiv.2306.09265"}}, "content": {"source": {"pdf_hash": "a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.09265v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d978b5287d22bc48c07ccc2288e4175944f9dc37", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9.txt", "contents": "\nLVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models\n\n\nPeng Xu \n\u20202 \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nThe University of Hong\nKong\n\nWenqi Shao \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nKaipeng Zhang \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nPeng Gao \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nShuo Liu \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nMeng Lei \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nPeking University\n\n\nFanqing Meng \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nSiyuan Huang \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nYu Qiao \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nPing Luo \nOpenGVLab\nShanghai\n\nAI Laboratory\n\n\nThe University of Hong\nKong\n\nLVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models\n\nLarge Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of 8 representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates 6 categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on 47 standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at vlarena page.\n\nIntroduction\n\nLarge Language Models (LLMs), such as LLaMA [1], GPT-3 [2], and Vicuna [3], have demonstrated remarkable progress in Natural Language Processing (NLP). These models leverage large-scale pretraining data and huge networks to achieve impressive results in NLP benchmarks. Recently, GPT-4 [4] further expanded the impact to the multimodal community, stimulating the rapid development of Large Vision-Language Models (LVLMs) and revolutionizing the landscape of artificial intelligence.\n\nLarge Vision-Language Models (LVLM) have achieved remarkable progress in multimodal visionlanguage learning for various multimodal tasks such as visual question answering and multimodal conversation. Specifically, LVLMs capitalize on the knowledge from LLMs and effectively align visual features with the textual space. Flamingo [5], a pioneering LVLM, integrates visual features into LLMs through cross-attention layers. Later studies proposed more efficient vision-text interactions [6], more efficient training methods [7,8], and employing instruction tuning [9,7,[10][11][12][13]8].\n\nHowever, despite the great success, few efforts have been made to provide systematic evaluations of LVLMs. But evaluation plays a critical role in understanding the strengths and weaknesses of LVLMs, thereby guiding their future development. Recent work [14] presents a systematic investigation of object hallucination of LVLMs by proposing a polling-based object probing evaluation method. Moreover, ImageNetVC [15] studies how well LVLMs can master visual commonsense knowledge. Liu et al. [16] comprehensively evaluate the performance of LVLMs in visual recognition with text recognition, such as optical character recognition. GVT [17] evaluates LVLM's visual semantic understanding and fine-grained perception capabilities. Nevertheless, these studies only evaluate a portion of LVLMs on specific tasks, lacking an overall understanding of LVLM's capabilities.\n\nIn pursuit of a comprehensive evaluation of LVLMs, we build an LVLM Evaluation hub (LVLM-eHub) consolidating 8 representative LVLMs such as InstrucBLIP [13] and MiniGPT-4 [10]. The details about model configuration and training data are listed in Table 1. Our LVLM-eHub consists of a quantitative capability evaluation and an online arena platform, providing a thorough investigation of the selected LVLMs. Specifically, the quantitative capability evaluation extensively evaluates 6 categories of multimodal capabilities of LVLMs including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence (see Fig. 1 (a)), by collecting 47 standard text-related visual benchmarks. On the other hand, the online arena platform features anonymous randomized pairwise battles in a crowd-sourced manner, providing a user-level model ranking in the open-world question-answering scenario (see Fig. 1 (b & c)).\n\nOur LVLM-eHub comprehensively evaluates LVLMs, revealing several innovative findings. (1) Instruction-tuned LVLM with massive in-domain data suffers from overfitting problem and generalizes poorly in open-world scenarios , such as InstructBLIP (see Fig. 1 (a)). (2) With moderate instructionfollowing data, instruction-tuned LVLM may cause object hallucination issues, generating objects that are inconsistent with target images in the descriptions. This leads to incorrect answers or renders current evaluation metrics, such as CIDEr for image captioning, ineffective. (3) We find that a multi-turn reasoning evaluation pipeline can mitigate the issue of object hallucination, indicating that developing an effective pipeline for LVLM evaluation is urgent.\n\nThe contributions of our work are summarized follows. (1) We propose LVLM-eHub which is the first comprehensive evaluation benchmark for large vision-language models, to our best knowledge.   Table 1: Comparison between Different LVLMs. 'VE', 'Adapt', 'ToP', 'TuP', and '# Token' represent the visual encoder, adaption module, number of total parameters, tuning parameters, and visual tokens fed into the LLM, respectively. \u2020 indicates that the model is frozen. CC * consists of COCO [18], CC3M [19], and CC12M [20]. CC, VG, CY, L400, and LC indicate Conceptual Caption [19,20], Visual Genome [21], COYO-700M [22], LAION 400M [23] and LAION COCO [24], respectively. SBU [25] contains 1 million images with captions. LLaVA-I represents 158K multimodal instruction-following data in LLaVA [9]. QA * denotes 13 question-answering datasets in InstructBLIP [13]. We count all the data and tuning parameters needed to convert the pretrained vision model and LLM into a visual instruction model. evaluation results reveal several innovative findings, providing a foundational framework for the assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques.\n\n\nLVLM Evaluation Hub\n\nIn this section, we introduce representative LVLMs, multimodal capabilities of interest, and evaluation methods. The whole LVLM Evaluation Hub is illustrated in Fig. 2. Our LVLM evaluation hub compromises 8 representative models including BLIP2 [6], LLaVa [9], LLaMA-Adapter V2 [7], MiniGPT-4 [10], mPLUG-Owl [11], Otter [12], InstructBLIP [13], and VPGTrans [8]. All models boost vision-language representation learning by utilizing pre-trained image encoders and large language models (LLM). But they differ in training data scale and model configuration as shown in Table 1, where the information is collected from their papers or provided by the authors. For a fair comparison between LVLMs, we collect their checkpoints with parameter sizes less than 10B. The detailed descriptions of these models are provided in Appendix Sec. A.\n\n\nQuantitative Capability Evaluation\n\nWe aim to evaluate LVLMs' capability comprehensively. In particular, we summarize 6 categories of capabilities and collect corresponding benchmarks for quantitative evaluation (see Fig. 2). Please see our Appendix Sec. D for more statistics and details of the collected benchmarks.\n\nVisual Perception. Visual perception is the ability to recognize the scene or objects in images, the preliminary ability of the human visual system. We evaluate this capability of models through image classification (ImgCLs) using the ImageNet1K [26], CIFAR10 [27], Pets37 [28] and Flowers102 [29] benchmarks, multi-class identification (MCI) and object counting (OC) using the GVT [30] benchmark. ImgCLs and MCI measure how well an LVLM grasps high-level semantic information, while OC assesses the recognition ability for fine-grained objects.  Figure 3: Illustration of our adopted evaluation methods. To evaluate the zero-shot performance of LVLMs on diverse downstream tasks, we employ four methods including question answering, prefix-based score, multi-turn reasoning, and user study.\n\n\nOnline Evaluation with LVLM Arena\n\nDesigning quantitative evaluations for LVLM to satisfy all capabilities is challenging, as evaluating LVLM responses constitutes an open-ended problem. Inspired by FastChat [62], we introduce the LVLM Arena, an online evaluation framework for LVLMs' pairwise battle with human judgment. Figure 2 illustrates the LVLM Arena, comprising three primary components: matchmaking, chat, and voting. Initially, two models are sampled from the model zoo. Users then converse side-by-side with the models, who remain anonymous. Subsequently, users vote for the superior model.\n\nMatchmaking. The matchmaking module samples two models in a tournament style based on their Elo rating. However, due to the currently limited size of the model hub, we employ random sampling.\n\nChat. Users chat side-by-side with two sampled models (which remain anonymous) using images or text inputs. Different from quantitative evaluation, users can chat about anything. Our existing online platform supports only single-round chats due to high computational and memory demands in multi-round chats. Future updates will address this constraint.\n\nVoting. After the chat session, users vote for their preferred model. Four options are available: Model A, Model B, Tie, and Both are bad. The Elo rating is subsequently updated using voting results.\n\nIn contrast to limited quantitative evaluations, the LVLM Arena provides an open-world evaluation framework that enables users to chat with models about anything, emulating real-world conditions. Besides, users serve as the judge for the battle, which brings more convincing evaluation results than traditional evaluation metrics.\n\n\nZero-shot Evaluation\n\nLVLMs are capable of capturing a wide range of multimodal patterns and relationships. We evaluate the aforementioned 6 categories of capabilities of LVLMs in Sec. 2.1 by investigating their zero-shot performance on various tasks. Zero-shot evaluation allows us to evaluate the LVLMs' ability to generalize to new tasks without training the model, which is competent for large-scale evaluation. To be specific, we treat the zero-shot evaluation as various forms of prompt engineering for different tasks (see Fig. 3) as presented in the following.\n\n\u2022 Question Answering. Prompting with visual question answering can be used to solve many downstream tasks, which assess how well an LVLM understands the underlying language and visual features. We design proper prompts to ensure that the LLM can produce meaningful results.  Table 2: Evaluation results of visual perception capability of LVLMs on tasks of Image Classification (Imgcls), Object Counting (OC), and Multi-class Identification (MCI). We use accuracy metric for all tasks. The best result is bold while the second is underlined. The average score is obtained by normalizing over each row and taking the average of each column. S-SOTA indicates the supervised state-of-the-art results.\n\nFor example, text prompts of OCR can be \"what is written in the image?\". Then, we evaluate the answers generated by the LLM using the corresponding metric such as accuracy. \u2022 Prefix-based Score. For multi-choice QA tasks, we can utilize a visual encoder to obtain visual prompts for a given image. Then, the visual prompts are prefixed into the text embeddings, which are fed into the LLM. The likelihood of image-text pair can be generated, which is referred to as a prefix-based score. We can obtain a prefix-based score for each text prompt of the candidate's answer. The answer with the largest prefix-based score is selected as the final answer. \u2022 Multi-turn Reasoning. Following IdealGPT [16], we use a multi-turn reasoning framework to evaluate complex visual reasoning tasks. Specifically, we utilize an LLM such as ChatGPT to generate sub-questions for a given question, an LVLM to provide corresponding sub-answers, and another LLM to evaluate sub-answers' quality. Such a pipeline iteratively proceeds until a satisfactory answer is obtained. \u2022 User Study. Evaluating the quality of the text generated by an LVLM requires a thorough understanding of the underlying language and context. In embodied artificial intelligence tasks, the LVLM generates a plan for the given instruction, which should be evaluated through various aspects such as recognition accuracy and conciseness in answers. It is hard to implement such an evaluation using an existing metric. Thus, user studies are conducted to assess the quality, relevance, and usefulness of the text generated by the LVLM in a specific context. To maintain evaluation fairness, we randomly shuffle the model's output order and anonymize outputs during evaluation.\n\n\nExperiment and Analysis\n\nIn this section, we perform a zero-shot evaluation to assess the 6 categories of capabilities of LVLMs. Specifically, visual perception ability, visual knowledge acquisition, visual reasoning, visual commonsense understanding, visual object hallucination, and embodied intelligence are assessed in Sec. 3.1 \u223c Sec.3.6, respectively. The LVLM arena evaluation result is presented in Sec.3.7. More evaluation findings, evaluation details, quantitative results, and details about evaluation datasets can be found in Appendix Sec. A, Sec. B, Sec. C and Sec. D, respectively.\n\n\nResults on Visual Perception\n\nVisual perception is an important ability of LVLMs. As presented in Sec. 2.1, we evaluate through image classification (ImgCls), multi-class identification (MCI), and object counting (OC). The evaluation details of tasks are provided in Appendix. B.1. The evaluation results are reported in Table 2. We have three observations. (1) mPLUG-Owl and LLaVA perform best on coarse-fined classification tasks (i.e., ImageNet1K and CIFAR10). The commonality is that they update LLM with 158K instruction-following data.\n\n(2) InstructBLIP presents good perception ability in fine-grained ImgCls, OC, and MCI tasks. The main reason is that InstructBLIP is fine-tuned on 1.6M VQA data, making it overfit on these tasks.   \n\n\nResults on Visual Knowledge Acquisition\n\nVisual knowledge acquisition involves going beyond image perception to acquire deeper understanding and knowledge. In our study, we evaluate the acquisition of visual knowledge through various tasks, namely Optical Character Recognition (OCR), Key Information Extraction (KIE), and Image Captioning, all performed in a Visual Question Answering (VQA) fashion. The evaluation details of tasks are demonstrated in Appendix.B.2. Table 3 shows the zero-shot performance in visual knowledge acquisition, and we have the following observations. First, BLIP2, InstructBLIP, and VPGTrans achieve dominant performance in all tasks. This may be because these models use a large visual encoder (i.e., ViT-g/14) and Q-Former updated with massive image-text pairs. A stronger visual encoder and adaption module can extract better tokens entailed with the global and local context, leading to remarkable improvement in visual knowledge acquisition. Second, InstructBLIP presents consistently the best results on almost all tasks. The main reason is that InstructBLIP overfits these tasks by fine-tuning with massive VQA data.\n\n\nResults on Visual Reasoning\n\nVisual reasoning encompasses the ability to comprehensively understand images and perform cognitive tasks. In this section, we evaluate the visual reasoning ability of LVLMs on various tasks, including Visual Question Answering (VQA), Knowledge-Grounded Image Description (KGID), and Visual Entailment (VE) tasks. The evaluation details of tasks are provided in Appendix.B.3. Table 4 shows the zero-shot performance in visual reasoning, and we have the following observations. First, compared with BLIP2, InstructBLIP again presents better results overall because it overfits many  tasks by fine-tuning massive VQA data. Second, compared with BLIP2, instruction-tuned LVLMs, except for InstructBLIP, generally perform worse than BLIP2. The common words in the instruction data often influence the generated content, which can not be evaluated by the current metrics (see Fig.  4). Third, instruction-tuned LVLMs consistently surpass BLIP2 on SNLI-VE where the final answer is obtained by multi-turn reasoning. It shows that instruction-following fine-tuning can produce promising content once a good evaluation scheme is employed. We provide more evidence in \n\n\nResults on Visual Commonsense\n\nThe visual commonsense evaluation aims to evaluate the model's comprehension of commonly shared human knowledge about generic visual concepts. We use two challenging visual commonsense benchmarks in a zero-shot setting, including ImageNetVC and Visual Commonsense Reasoning (VCR). The evaluation details of tasks are presented in Appendix.B.4.\n\nAs shown in Table 5, we can find that all those LVLMs can partly solve visual commonsense problems. First, InstructBLIP performs best among those LVLMs on the ImageNetVC and VCR dataset. The main reason is that it is fine-tuned on 1.6M fine-grained VQA data, making it adapt to answer visual common questions. Second, LLaVA also performs well in the visual commonsense task. The reason is that it updates LLM with instruction-following data. Third, instruction-tuned LVLMs again surpass BLIP2 on the two Visual Commonsense tasks. It shows that instruction-tuning can provide more effective clues than BLIP2 for visual commonsense. Note that the final answer of VCR is obtained by multi-turn reasoning. It also shows the significant role of a good evaluation scheme in producing promising content for instruction-tuned models.\n\n\nResults on Object Hallucination\n\nAlthough LVLMs have made significant progress, they still struggle with the issue of hallucination, which refers to their tendency to produce objects that do not align with the descriptions provided in the target images. In this section, we focus on evaluating such object hallucination problems on MSCOCO captioning dataset. Following POPE [14] evaluation pipeline which is a multi-step QA procedure, we prompt LVLMs with multiple Yes-or-No questions. Each image is prompted with 6 Yes-or-NO questions. For example, 'Is there a person in the image?'. We use accuracy, precision, recall, F1-Score and the ratio of answering 'Yes' as evaluation metrics. The evaluations are three datasets including MSCOCO-Random/Popular/Adversarial [14]. From Random to Adversarial, the questions become more and more difficult. From Table 6, we could come to the following conclusions. InstructBlip performs best in the hallucination problem, followed by BLIP2, whose average accuracy reacheS more than 80%. We find that instruction-tuned models, except for InstructBLIP, perform worse than BLIP2 because they tend to answer 'Yes' to the question, which shows that LVLMs are prone to generate objects which do not exist in the image. In sec. C.2, we show that such an object hallucination problem can be alleviated by a multi-turn reasoning pipeline, which can be also seen from the experiments on SNLI-VE and VCR.\n\n\nResults on Embodied Intelligence\n\nIn this section, we present the evaluation results focusing on embodied intelligence. To appraise the effectiveness of planning outputs using the given image, we conducted a user study involving Table 6: Detailed evaluation results of the zero-shot performance of LVLMs on MSCOCO using POPE evaluation pipeline [14], where accuracy represents the accuracy of prediction; precision represents how many of the predicted positive samples are true positive samples; recall represents how many of all true positive samples are correctly identified; and yes represents the probability that the model outputs a yes answer. The average score is calculated based on the metric of accuracy.\n\nCIDEr Score The image features a close-up view of a small, sparkling tiara .\n\nA button with a tiny encrusted tiara on it.\n\nThe studded crown sits on the turn table.\n\nA tiara is sitting on a grey surface.\n\nTiara with heart shaped pattern on black platform. The image shows a close-up view of a large, messy, and delicious-looking cheeseburger.\n\nHamburger with cheese and bacon from fast food.\n\nThis large cheese burger has bacon on it.\n\nA hand holding a big bacon and cheese hamburger.\n\nA person is holding a sandwich with his/her hand.  Figure 4: Limitations of CIDEr Score Evaluation in Image Captioning. SentenceTransformer [77] computes the similarity between generated and ground-truth text as \"Sim.\" \"GPT3. 5 Turbo\" indicates that we feed GPT with the most elaborate ground-truth caption and use GPT to evaluate the quality of the generated captions (left score) and other ground-truth captions (right score). The prompt we used is similar to Vicuna's GPT-4 evaluation template [3].\n\nSpecifically, the participants rated the generated plans from different LVLM models using a scoring system similar to [58]. The evaluation comprised five dimensions with scores ranging from 1 to 5. These dimensions included object recognition accuracy, spatial relationship understanding, level of conciseness in the response, reasonability of the planning, and executability of the planning. The resulting average scores for the different models among the participants are presented in Table 7 Table 7: Generated planning quality evaluation on embodied tasks.\n\n\nResults on Online Arena Evaluation\n\nThe arena features anonymous and randomized pairwise battles in a crowd-sourced manner. We have collected 634 and 1425 pieces of evaluation data up until June 3 and June 13 in 2023, respectively. The collected data shows almost the same number of battle outcomes for 'Model A wins' and 'Model B wins.' Moreover, 21.8% and 22.0% battle outcomes are voted as 'both are bad' in two copies of collected data, respectively, implying that the current LVLMs still struggle to generate good answers for open-world visual questions. Furthermore, we rank the selected 8 LVLMs with Elo rating [79] using two copies of the collected data by following Fastchat [62] and [80]. As shown in Fig. 1 (b) and (c), mPLUG-Owl, MiniGPT-4, LLaMA-Adapter V2, Otter, and VPGTrans, which are finetuned with amounts of instruction-following data with updating many parameters, are the top-3 best models in the open-world VQA scenario according to two ranking lists, indicating the significance of instruction-following tuning and effective parameter update. Moreover, InstructBLIP perform best on in-domain capability evaluation, while being much worse than many instruction-tuned models, implying severe overfitting issue, as shown in Fig. 1.\n\n\nDiscussion and Conlcusion\n\nNew Evaluation Metrics. Our quantitative evaluation mainly uses the CIDEr score and accuracy. The CIDEr score is widely used in image captioning and QA evaluation. It measures the similarities between generated and ground-truth answers. However, LVLMs' responses are diverse, in different styles with the ground truth. As such, the CIDEr score is unsuitable (see Fig. 4 for failure cases). We also tried model-based evaluation, which uses Sentence Transformer [77] to calculate the feature similarity between generated and the ground-truth answers. It is generally more robust but sometimes suffers due to model limitations. Recent studies use the powerful Chat-GPT or GPT-4 as a judge to evaluate LLMs' responses. However, in LVLM evaluation, GPT is blind to the image and is inaccurate in some cases. We propose LVLM Arena, an innovative evaluation framework that utilizes a 1v1 LVLM battle with human judgment for open-world evaluation, leading to more accurate and realistic evaluation results. However, it requires significant human effort to produce reliable rating results, especially when numerous models exist. Therefore, developing fast, accurate, and generalized evaluation metrics for LVLMs remains an open problem.\n\nA Platform for LVLM Evaluation. We have developed an evaluation framework aimed at comprehensively assessing the performance of LVLM models across six critical capabilities. Each capability encompasses multiple tasks, with several datasets incorporated into each task. Our user-friendly interface allows users to contribute their own datasets and models, facilitating a collaborative and inclusive environment. With just one click, users can effortlessly access a holistic assessment of their target LVLM model through our evaluation platform. We are dedicated to regularly updating the datasets and expanding our support for a wider range of LVLM models on our platform. Users are encouraged to contribute their LVLM models by utilizing our platform's model inference interface. Additionally, we offer free online inference services for the LVLM models supported by LVLM Arena. This arena not only allows users to vote for their preferred models but also provides an Elo rating rank system that incorporates valuable human feedback, ensuring continuous improvement and refinement.  A More details about our LVLM-eHub\n\n\n(a) Rank in Visual Perception (b) Rank in Visual Knowledge Acquisition (c) Rank in Visual Reasoning\n\n\n(d) Rank in Visual Commonsense (e) Rank in Object Hallucination (f) Rank in Embodied Intelligence\n\n\nA.1 Overall Evaluation Results\n\nOur Findings. We present our observations from extensive evaluation experiments in the following.\n\n\u2022 Instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. As shown in Fig. 1 and Fig. A.1, InstructBLIP achieves the best results in 5 categories of capabilities while lagging behind other instruction-tuned models such as LLaMA-Adapter V2 and mPLUG-Owl in embodied AI and LVLM arena platform. We see that InstructBLIP is fine-tuned on 16M visual question answering pairs (see Table 1, exhibiting in many in-domain tasks such as perception and reasoning tasks. However, Embodied AI tasks require that the model is capable of generating a step-by-step plan for instruction with a given image. Moreover, the arena platform evaluates LVLMs' ability of visual question answering in open-world scenarios. InstructBLIP overfits in-domain tasks, generalizing poorly in these two real-world tasks. \u2022 Instruction-tuned LVLM with moderate high-quality instruction-following data may result in object hallucination issues. The issue means that LVLMs would generate objects that are inconsistent with target images in the descriptions. It either makes the current evaluation metric such as CIDER for image captioning ineffective or generates wrong answers. For instance, LLaMA-Adapter V2 can generate high-quality image captions which yet present a low CIDEr score as shown in Fig. 4. But the high sentence similarities between the generated answer and ground-truth answers measured by Sentence Transformer [77] and GPT3.5 shows that the generated answer is relatively accurate. Therefore, the instruction-tuned models could generate content that cannot be evaluated by existing metrics. It also indicates that it is urgent to develop an effective metric for LVLM evaluation.\n\nIn addition, we also find that instruction-tuned LVLMs with moderate high-quality data are more likely to generate wrong answers. As shown in Table 6, LLaMA-Adapter V2, LLaVA, MiniGPT-4, mPLUG-Owl, Otter, and VPGTrans generally present higher accuracy and recall, and lower precision than BLIP2 and InstructBLIP. These models are tuned with moderate high-quality data such as LLaVA-158K or instruction-following data generated by LLM as shown in Table 1. This implies that instruction-tuned LVLMs with moderate high-quality data are prone to answer 'Yes' regardless of the accuracy of the answer to the underlying question.\n\n\u2022 Employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective metric for LVLM evaluation. In Table 4 and Table 5, we see that instruction-tuned LVLMs with moderate high-quality data can achieve better performance than BLIP on SNLI-VE and VCR tasks under a multi-turn reasoning evaluation pipeline in Sec. 2.3. We also provide more evidence to demonstrate the effectiveness of such an evaluation technique in mitigating object hallucination in Fig. A.3.\n\n\nA.2 Model Details in LVLM-eHub\n\n\u2022 BLIP2 [6] pre-trains a lightweight Q-Former on 129M image-text pairs. It follows a two-stage strategy to bridge the modality gap. The first stage bootstraps vision-language representation learning from a frozen image encoder ViT-g/14 in EVA-CLIP [81]. The second stage bootstraps vision-to-language generative learning from a frozen LLM FlanT5-XL [82], which enables zero-shot instructed image-to-text generation. \u2022 LLaVA [9] connects the visual encoder ViT-L/14 of CLIP [83] with the language decoder LLaMA [1] by a lightweight fully-connected (FC) layer. LLaVA first trains the FC layer with 595K imagetext pairs while freezing the visual encoder and LLM and then fine-tunes the FC layer and LLM on 158K instructional vision-language data. \u2022 LLaMA-Adapter V2 (LA-V2) [7] is a parameter-efficient visual instruction model. Although the visual encoder (ViT-L/14) and LLM are kept frozen, LLaMA-Adapter V2 distributes the instructionfollowing ability of the whole LLaMA through bias(B)-tuning. In this way, the scale, bias, norm, and prompt parameters are tuned on 200M image captioning data, 158K visual instruction-following data, and 52K language instruction-following data constructed by GPT-4 [84]. \u2022 MiniGPT-4 [10] connects the visual encoder and text encoder by an FC layer. It also first trains the FC layer with 5M image-text pairs and then fine-tunes it on 3.5K instructional vision-language data. Despite the simplicity, MiniGPT-4 needs to load a pretrained vision encoder of BLIP2 and Vicuna LLM [3]. \u2022 mPLUG-Owl [11] incorporates a visual abstractor, essentially the same as Perceiver Resampler in Flamingo [5], to bridge pretrained visual encoder ViT-L/14 and LLM (LLaMA) with a twostage finetuning procedure. It firstly fully finetunes both the visual encoder and visual abstractor on 204M image-text pairs. Then for the second stage, 158K LLaVA-Instruct data is utilized to parameter-efficiently finetune pretrained LLM via LoRA. \u2022 Otter [12] is a multimodal model with in-context instruction tuning based on OpenFlamingo [5] which comprises a LLaMA-7B language encoder and a CLIP ViT-L/14. Although the visual and text encoder are frozen, Otter trains extra 1.3B parameters coming from adaption modules on 158K instruction-following data. \u2022 InstructBLIP [13] is initialized from a pre-trained BLIP-2 model consisting of a ViT-g/14 image encoder, a Vicuna LLM and a Q-Former to bridge those two. During vision-language instruction tuning, only Q-Former is fine-tuned on 13 visual question-answering datasets. \u2022 VPGTrans [8] is a simple transferring technique that adapts a smaller LLM to a larger LLM. It transfers the VPG of BLIP-2 (i.e. ViT-g/14) from OPT6.7B to Vicuna7B by training Q-Former on 13.8M Image-Text pairs. In addition, the VPG and projector are further tuned on MiniGPT-4's 3.5K self-instruct data instances.\n\n\nB Evaluation Details B.1 Details of Visual Perception\n\nFor ImgCls, we test LVLMs on two coarse-grained benchmarks (i.e., ImageNet1K and CIFAR10) in top-1 accuracy and two fine-grained benchmarks (i.e., Pets37 and Flowers102) in per-class accuracy.\n\nFollowing KOSMOS-1 [85], the default prompt 'The photo of the' is used for all LVLMs for a fair comparison over coarse-grained benchmarks, while it is too general for fine-grained visual perception. When confronted with intricate fine-grained image classification tasks in a zero-shot manner, contemporary multi-modal models often encounter difficulties in accurately generating precise subclass names. To gain a deeper understanding of their capabilities and enable effective comparisons between them, we have heuristically designed prompts \"What is the specific category of the dog or cat in the image?\" for the Pets37 dataset, and \"What is the specific category of the flower in the image?\" for the Flowers102 dataset, respectively. Furthermore, the generated coherent sentence-style responses deviate from the standard image classification benchmark. To accommodate this discrepancy, we considered the prediction as correct if the model output contains the correct class name, which is inspired by MultiModal OCR [16]. . The generated answer is then compared with the ground truth. We report accuracy by treating OC as a classification problem.\n\nFor MCI task, we also test LVLMs on MSCOCO and VCR1.0 [57]. We ask the model to determine whether a certain object is present or absent by prompting 'Question: Does [obj] exist in the image. Answer:'. We also report the accuracy by treating MC as a Yes or No classification problem.\n\n\nB.2 Details of Visual Knowledge Acquisition\n\nFor OCR task, we test the selected LVLMs with twelve representative OCR datasets, which are inclusive of IIIT5K [31], ICDAR 2013(IC13) [32], ICDAR 2015 (IC15) [ For VE task, it evaluates the VLPM's capability to determine the logical relationship between image pairs. We employ the SNLI-VE [26] benchmark, which provides pairs of images along with corresponding textual premises and hypotheses. For efficient evaluation, we randomly select 500 samples from the dev split of the SNLI-VE dataset. We find that a naive QA pipeline is hard to give meaningful predictions. Therefore, as shown in Fig. 3, we employ multi-turn reasoning to solve SNLI-VE. There are three components in multi-turn reasoning pipeline: a Questioner, an Answerer, and a Reasoner. The Questioner first raises a set of sub-questions based on the main question, then Answerer produces the relative sub-answers, and Reasoner decide whether a confident answer is derived to its main question by analyzing the sub-questions and sub-answers. The \"Questioner-Answerer-Reasoner\" loop keeps iterating until the Reasoner derives a confident final answer or Generate a caption for this image.\n\nThe image features a brightly colored, pink and blue Kirby, a popular video game character, standing on a white surface. Kirby is known for his cheerful and friendly demeanor, and his bright colors make him an eye -catching and endearing character. The image captures Kirby's playful and energetic spirit, as he stands tall and ready to take on any challenge that comes his way.\n\nWrite me a caption for this image.\n\nThe image features a brightly colored, pink and blue Kirby, a popular video game character, standing on a white surface. Kirby is wearing a red bow tie and appears to be smiling, giving off a cheerful and friendly vibe. The Kirby character is known for its unique appearance and ability to inhale enemies and copy their abilities, making it a fan favorite in the gaming world.\n\nWhat's the caption of this image?\n\nThe caption of this image is \"Kirby, the pink puffball, is smiling and winking at the camera.\"\n\n\n[Input Image] [Prompts] [Model Response]\n\nFigure A.2: In this example, we present the variation in model responses of LA-V2 when provided with the same image but different prompts. All model responses mentioned here were generated using zero temperature and a top-p value of 0.75. The purpose is to showcase how the model's output changes based on the prompt input. the number of iterations reaches a predefined maximum. Following You et al [86], three simple prompts are also applied to generate better sub-answers and sub-questions. We use ChatGPT as the Reasoner and Questioner via \"gpt-3.5-turbo\" API. The eight studied pre-trained LVLMs are served as the Answerers and produce image captions respectively for comparing their abilities to solve visual-entailment problem.\n\n\nB.4 Details of Visual Commonsense\n\nFor ImageNetVC, we evaluate the zero-shot visual commonsense of LVLMs. It contains high-quality QA pairs for various commonsense, including color, shape, mater, comp, and others. Specifically, as shown in Fig. 3, QA pairs in ImageNetVC [15] \n\n\nare first transformed into prompts like '[Question]\n\nThe answer is [Answer].', and then each prompt is converted into a sequence of tokens. Secondly, the image and text tokens are transformed into a sequence of visual embeddings and a sequence of text embeddings, respectively. Finally, visual embeddings are prefixed into the text embeddings yielding the final embeddings which are put into a frozen pre-trained LVLM to calculate the score. The probability distribution over all answer candidates using softmax is calculated. We use the prefix-based score to choose the final answer with maximum likelihood. Following Xia et al [15], five similar prompts are utilized to take average values for final evaluation among eight LVLMs.\n\nFor VCR, it expects that the LVLMs can find the correct answer among four answer candidates. For efficient evaluation, we randomly select 500 samples from the val split of the VCR dataset. We find that a naive QA prompt cannot give meaningful output. Similar to the SNLI-VE evaluation (Section B.3), we adopt a multi-turn reasoning evaluation technique to solve the VCR task.\n\nC More Experiments C.1 Sensitivity to Prompts.\n\nThroughout our comprehensive evaluation, we discovered that LVLM models are highly sensitive to the choice of prompts. An illustrative example of this sensitivity is observed in the image captioning task, where altering the prompt employed for the VPGTrans model on the NoCaps dataset leads to a substantial variation in performance, ranging from 19.66 to 48.13. Fig. A.2 showcases some examples of the results generated based on different prompts. This sensitivity underscores the significance of carefully selecting and designing prompts to achieve optimal performance in LVLM-based tasks.\n\n\nC.2 More Results on Object Hallucination\n\nMulti-turn Reasoning Evaluation Can Alleviate Object Hallucination. We also show the hallucination problem by visualizing some examples in Fig. A.3. It can be seen that LVLMs except for BLIP2 and InstructBLIP are prone to generate objects which are inconsistent with the image. Fortunately, such an issue can be mitigated by a multi-turn reasoning evaluation framework. We believe that the reasoning procedure can encourage instruction-tuned models to re-organize the \n\n\nInstructBLIP\n\nYes, there is a tennis racket in the image, and it is being used by a man.\n\n\nLLaVA\n\nYes, there is a tennis racket in the image, and it is being used by a man.\n\n\nLLaMA-Adapter V2\n\nYes, there is a tennis racket in the image.\n\n\nMiniGPT-4\n\nYes, there is a Tennis racket near the man in the picture.\n\n\nOtter\n\nYes, there is a blue tennis racket in the image.\n\n\nmPLUG-Owl\n\nThere is no tennis racket in the image.\n\n\nVPGTrans\n\nNo, there is no tennis racket in the image.\n\n\nBLIP2\n\nNo, there is no tennis racket in the image.\n\n\nInstructBLIP\n\nNo, there is no tennis racket in the image.\n\n\nLLaVA\n\nNo, there is no tennis racket in the image.\n\n\nLLaMA-Adapter V2\n\nNo, there is no tennis racket in the image.\n\n\nMiniGPT-4\n\nNo, there is no tennis racket in the image.\n\n\nOtter\n\nNo, there is no tennis racket in the image.\n\n\nmPLUG-Owl\n\nNo, there is no tennis racket in the image. \n\n\nInstructBLIP\n\nYes, there is a carrot in the image, and it is being held by a child.\n\n\nLLaVA\n\nYes, there is a carrot in the image, which is being used as a prop for the children's craft activity.\n\n\nLLaMA-Adapter V2\n\nNo, there is no carrot in the image.\n\n\nMiniGPT-4\n\nYes, there is a carrot in the picture, it is a party centered around a specific activity involving carrots.\n\n\nOtter\n\nYes, there is a carrot in the image.\n\n\nmPLUG-Owl\n\nYes, there is a carrot in the image.\n\n\nVPGTrans\n\nNo, there is no carrot in the image.\n\n\nBLIP2\n\nNo, there is no carrot in the image.\n\n\nInstructBLIP\n\nNo, there is no carrot in the image.\n\n\nLLaVA\n\nNo, there is no carrot in the image.\n\n\nLLaMA-Adapter V2\n\nNo, there is no carrot in the image.\n\n\nMiniGPT-4\n\nNo, there is no carrot in the image.\n\n\nOtter\n\nNo, there is no carrot in the image.\n\n\nmPLUG-Owl\n\nNo, there is no carrot in the image.\n\n\nVPGTrans\n\nMulti-turn Reasoning Figure A.3: The Effect of Multi-turn Reasoning Evaluation Pipeline. We see that many LVLMs could generate content that does not exist in the given image, suffering from the object hallucination issue [14]. Moreover, a multi-turn reasoning evaluation pipeline can mitigate object hallucination issues.\n\nknowledge they grasp and finally generate the right answers. It is significant to investigate how to evaluate instruction-tuned LVLMs in the right way.\n\n\nC.3 More Results on Embodied Tasks.\n\nIn this section, we provide quantitative evaluation results for embodied tasks in addition to the user study discussed in Section 3.6. We selected some representative scenes from Minecraft, Franka Kitchen, and Meta-World benchmarks as shown in Figures A.4 through A.6, and the results for these tasks are provided in Sections C.3.1 to C.3.3.\n\nIn Figure A.4, the models were asked to generate feasible plans for the Minecraft agent to reach the opposite shore with a boat floating on the river. All eight models recognized the presence of the floating boat, but only LLaMA-Adapter V2, InstructBLIP, and MiniGPT-4 generated a plan that utilized the boat to help the agent reach the opposite shore more quickly.\n\nIn Figure A.5, the models needed to assist the robotic arm in moving the kettle to the top left burner, and we expected the models to analyze where the goal state was achieved from the image. Except for You are a helpful assistant to complete any Minecraft task specified by me.\n\nGive a detailed explanation of the image, describe the scene and the observed objects, and spatial relationships in this image. You should answer the following question based on the image.\n\nYou should reach the opposite shore as fast as you can.  BLIP, all seven models provided a feasible and reasonable plan. Notably, LLaVA recognized that the goal state had already been achieved from the given image. Meanwhile, mPLUG-Owl generated some steps to deal with the situation when the goal state was achieved before execution.\n\nFor the Meta-World examples shown in Figure A.6, some models struggled with visual reasoning partially due to the low-resolution image input. mPLUG-Owl detected the basketball in the robot's hand mistakenly, while others, e.g. LLaVA, MiniGPT-4, and Otter, mistook the robotic arm for a human.\n\n\nC.3.1 Minecraft\n\nBLIP: This is a Minecraft screenshot of a boat in the water.\n\nInstructBLIP: The image depicts a Minecraft scene where the player is standing on a small island in the middle of a large body of water. The player's goal is to reach the opposite shore as quickly as possible, which involves navigating through the water and possibly encountering various obstacles along the way. In the scene, there are several objects visible, including a boat, a tree, and a rock. The boat appears to be partially submerged in the water, suggesting that the player may need to navigate around it or use it as a means of transportation to reach the opposite shore. The tree serves as a landmark, providing a reference point for the player to orient themselves and determine their progress towards the opposite shore. The rock is also present, adding to the visual interest of the scene. The spatial relationship between the player, the boat, the tree, and the rock can be inferred from the image. The player is located on the small island in the middle of the water, while the boat is partially submerged nearby. The tree serves as a reference point, indicating the player's proximity to the opposite shore. The rock adds to the visual appeal of the scene, contributing to the overall sense of.\n\nLLaMA-Adapter V2: The image shows a Minecraft scene with a small wooden boat floating on a body of water. The boat is positioned near the shore, and there is a path leading to the opposite shore. The boat appears to be in a relatively calm body of water, and it seems to be the primary mode of transportation for the player. The player's goal is to reach the opposite shore as fast as possible, so they will likely need to use the boat to navigate the water and reach their destination.\n\nLLaVA: The image shows a Minecraft scene with a small boat floating in a body of water, surrounded by a sandy beach. There is also a small island in the background. The boat is positioned near the shore, and the player is standing on the beach, looking at the boat. The player's objective is to reach the opposite shore as fast as possible. The scene appears to be a part of a game, and the player's actions are likely to be controlled by the game's mechanics.\n\nMiniGPT-4: The image shows a small wooden boat floating in the water. The boat has a small cabin at the front and a mast with a sail at the rear. There is a rocky shore on one side of the boat and a sandy beach on the other. The water is calm and there are no other objects or people in the scene. The sun is shining from the top right corner of the image, casting a bright light on the water and the boat. The boat is positioned in the middle of the water, with the mast facing towards the shore. The goal is to reach the opposite shore as fast as possible, so the player would need to navigate the boat through the water, possibly using a paddle or oars, and avoid any obstacles or hazards in the way. mPLUG-Owl: In the image, a person, likely the user, is standing in the shallows near the water, looking for a way to cross over to a boat that is on the other side of the water. They appear to be in a desert biome, as evidenced by the surrounding landscape. The boat is located at a distance from the shore, and it seems to be partially underwater, suggesting that it could be a partially submerged boat. In addition to the person standing near the water, there are several rocks and trees scattered across the scene. One of the rocks is located closer to the shore, while the others are scattered farther away from it. The trees are also dispersed throughout, some closer to the shore and others further away. The presence of these objects and their spatial relationships suggest that the scene might be part of a desert-themed map or world in Minecraft.\n\nOtter: In this image, a person is standing on a raft in the middle of a body of water, with a small boat nearby on the shore. The scene is set in a game like Minecraft, where the player is trying to reach the other side of the water. The raft and the small boat are the primary objects that the player needs to utilize to cross the water safely. The person on the raft is standing near the boat, likely preparing to use it to continue their journey to the other shore.\n\nVPGTrans: The image shows a small wooden boat floating in the middle of a large body of water. There is a small island on the other side of the water, and the boat is headed towards it. The water is calm, and there are no visible obstacles in the way of the boat. The boat appears to be in good condition, and there are no visible signs of damage or wear and tear. The island appears to be relatively close, and it should be possible to reach it in a short amount of time.\n\n\nC.3.3 Meta World\n\nBLIP: A robot is dunking a basketball in a pool.\n\nInstructBLIP: The image depicts a 3D basketball game where a robot is attempting to dunk the ball. The scene features a basketball hoop, a basketball, and a robot. In order to successfully dunk the basketball, the robot needs to perform several actions. Here is a detailed plan for the robot to dunk the basketball: 1. Position the robot near the basketball hoop: The robot should be positioned close to the basketball hoop so that it can easily reach the ball when it falls through the hoop. 2. Wait for the ball to fall through the hoop: Once the ball falls through the hoop, the robot should wait for it to land on the ground. 3. Pick up the ball: The robot should pick up the ball as soon as it lands on the ground. 4. Dribble the ball towards the hoop: The robot should dribble the ball towards the hoop, aiming for the basket. 5. Shoot the ball into the hoop: The robot should shoot the ball into the hoop, attempting to dunk it. 6. Celebrate the successful dunk: If the robot successfully dunks the ball, it should celebrate its achievement.\n\nLLaMA-Adapter V2: The image shows a basketball hoop with a basketball inside it, and a robot is positioned near the hoop. The robot is likely programmed to dunk the basketball, so it needs to execute a series of steps to achieve this goal. Here is a possible plan for the robot to dunk the basketball: 1. The robot should first identify the basketball and determine its position relative to the hoop. 2. The robot should then move towards the basketball, adjusting its speed and direction to ensure it reaches the ball at the right time. 3. As the robot approaches the basketball, it should extend its arm and grab the ball with its hand. 4. The robot should then lift its arm and place the basketball into the hoop, aiming for the basket. 5. Once the basketball is inside the hoop, the robot should release it.\n\nLLaVA: The image shows a basketball court with a basketball hoop, a basketball, and a person holding a remote control. The scene appears to be a simulation or a video game, as the person is controlling the basketball with the remote. The basketball is positioned near the hoop, and the person is likely trying to dunk it. The remote control allows the person to simulate the dunking motion, which is not possible in real life.\n\nMiniGPT-4: In this image, there is a basketball player standing on a wooden floor in front of a hoop. The player is holding a basketball in one hand and is preparing to dunk it through the hoop. The player is wearing a basketball uniform, and the hoop is hanging from the ceiling. There is a small orange ball lying on the ground next to the hoop, and a second ball is sitting on the top of the hoop. To write a plan for the robot to dunk the basketball, you could use the following steps: 1. Program the robot to move to the center of the room and stand in front of the hoop. 2. Program the robot to pick up the ball on the ground with its arm. 3. Program the robot to hold the ball with one hand while it uses its other hand to dunk the ball through the hoop. 4. Program the robot to release the ball after it has gone through the hoop. 5. Program the robot to move away from the hoop after it has finished dunking the ball. Overall, this plan would involve the robot using its arm to pick up the ball on the ground, its other hand to dunk the ball through the hoop, and then releasing the ball after it has gone through.\n\nmPLUG-Owl: The image features a robot standing on a wooden floor and holding a basketball in its hand. It seems to be attempting to perform a dunk on a basketball hoop. To achieve this, the robot needs to first position itself properly and aim at the hoop. First, it should move towards the basketball hoop by walking on its wheels. It should maintain a straight path and avoid colliding with the walls or any obstacles in its way. Once it reaches the hoop, the robot should adjust its position and align its body with the rim. It should also maintain a stable posture while maintaining the basketball in its hand. Next, the robot should jump up and extend its arm and hand towards the hoop, while simultaneously bending its knees. This will generate enough momentum to launch the basketball toward the rim. As the basketball is in its grasp, the robot should release it with a quick flick of its wrist. The ball's trajectory should follow the path of the robot's arm, and it should enter the basket with a satisfying dunk.\n\nOtter: The image shows a small toy basketball on a basketball court, with a large toy head of a man, which appears to be a statue or a decoration, nearby. The scene suggests that the toy robot is designed to perform a dunk on the basketball using a ramp. To accomplish this, the robot would need to follow a plan that involves moving to the appropriate position on the court, using the ramp to gain elevation, and timing its jump to make contact with the basketball at the right moment. The plan would likely involve adjusting the robot's speed, angle, and trajectory to achieve a successful dunk while maintaining balance and coordination throughout the process. The specific details of the plan would vary depending on the design and programming of the robot, but the general idea would be to mimic the motion of a human dunking the ball.\n\nVPGTrans: The image shows a basketball hoop in the middle of a basketball court. The hoop is made of metal and has a net attached to it. There is a basketball on the ground next to the hoop. The hoop is at a height of about 10 feet off the ground. The basketball is at a height of about 5 feet off the ground. There is a robot on the left side of the hoop. The robot is facing the hoop and has a net attached to its arm. The robot is at a height of about 3 feet off the ground. The robot's arm is extended and ready to shoot the basketball into the hoop.  SNLI-VE SNLI-VE [89] extends the text entailment (TE) task into the visual domain and asks the model whether the image is semantically entailed, neutral, or contradicted to the next hypothesis. It is a three-category classification task based on Flicker30k [90].\n\n\nD Summary of Evaluation Datasets\n\n\n(dev)\n\nVisual Commonsense Datasets ImageNetVC ImageNetVC [15] is a fine-grained human-annotated dataset for zero-shot visual commonsense evaluation, containing high-quality QA pairs across diverse domains with sufficient image sources.\n\n\n(rank)\n\nVCR VCR [57] is a challenging multiple-choice VQA dataset that needs commonsense knowledge to understand the visual scenes and requires multiple-steps reasoning to answer the question.\n\n\n(val)\n\nObject Hallucination Datasets COCO-Random Following [14], we randomly select 500 images from the validation set of MSCOCO with more than three ground-truth objects in the annotations and construct 6 questions for each image. The probing objects in the questions that do not exist in the image are randomly sampled 3000(val) MSCOCO-Popular Similar to COCO-Random, we randomly select 500 images and construct 6 questions for each image. But the probing objects in the questions that do not exist in the image are selected from the top-50% most frequent objects in MSCOCO [14].\n\n\n3000(val)\n\nMSCOCO-Adversarial Similar to COCO-Random, we randomly select 500 images and construct 6 questions for each image. But the probing objects in the questions that do not exist in the image are selected from the ranked objects with their co-occurring frequency and the top-50% most frequent objects are sampled [14]. \n\nFigure 1 :\n1Comparison of LVLMs in the LVLM-eHub. (a) illustrates the variances in quantitative capability performance across six distinct aspects among LVLMs (see ranking list in Fig. A.1). (b) and (c) present the Elo rating ranking of LVLMs in the LVLM Arena according to the data collected up until June 3 and June 13, respectively. The arena ranking list changes as more evaluation samples are involved. We see that InstructBLIP performs well in existing in-domain tasks, while instruction-tuned models with moderate high-quality instruction-following data such as mPLUG-Owl, MiniGPT-4, Otter and LLaMA-Adapter V2 work well in the open-world arena. VPGTrans * means that VPGTrans adopts the prompt in the same form with MiniGPT-4.\n\n( 2 )\n2LVLM-eHub provides extensive evaluation on 6 categories of multimodal capabilities of LVLMs in 47 text-based visual tasks. (3) LVLM-eHub builds an online arena platform for LVLMs, which features anonymous randomized pairwise user-level comparison in a open-world scenario. (4) Our\n\n\nFig. A.3 in Sec. C of Appendix.\n\nFigure A. 1 :\n1Comparison of LVLMs in the LVLM-eHub. (a -f) present the ranking list of LVLMs in terms of 6 categories of multimodal capabilities, respectively, including visual perception, visual knowledge acquisition, visual reasoning, visual commonsense, object hallucination, and embodied intelligence.In the appendix, we provide more details of LVLM-eHub and task settings of evaluation in Sec. A and Sec. B, respectively. Additionally, more experiments are illustrated in Sec. C. The evaluation datasets are summarized in Sec. D.\n\n\nthere is no tennis racket in the image.\n\nFigure A. 4 :\n4Embodied image caption and plan generation example task in Minecraft simulator.Can you write a detailed plan for the task of the robot to move the kettle to the top left burner? You should give your step plan based on this input image.You should analyze from the image whether the action goal is achieved already, if achieved, no need to generate the plans.\n\nFigure A. 5 :\n5Embodied image caption and plan generation example task in Franka Kitchen benchmark.Give a detailed explanation of the image, describe the scene and the observed objects, and spatial relationships in this image.Can you write a detailed plan for the robot to dunk the basketball?Figure A.6: Embodied image caption and plan generation example task in Meta-World benchmark.\n\n\nmodel can accurately identify and extract text from images or scanned documents. The KIE task further poses challenges in extracting structured information from unstructured or semi-structured text. Finally, ImgCap assesses whether a model can generate a good natural language description of the content of an image.Visual Reasoning. Visual reasoning requires a comprehensive understanding of images and related texts. To evaluate the visual reasoning ability of LVLMs, we utilize three tasks including visualFigure 2: Our evaluation encompasses quantitative evaluation and online LVLM Arena. Plentiful benchmarks are employed to comprehensively evaluate the six critical capabilities of the models in the quantitative evaluation. In the LVLM Arena, an online platform, users can participate in an online evaluation by chatting with two anonymous models and choosing their preferred model. KGID), and visual entailment. For KGID, we use ScienceQA [55] and VizWiz [56]). For visual entailment task, we use SNLI-VE. These three tasks are in VQA form. A capable LVLM should be able to understand the objects and scenes in an image and can reason to generate answers that are semantically meaningful to the question asked.Visual Commonsense. Visual commonsense refers to the general visual knowledge commonly shared across the world, as opposed to the visual information specific to a single image. This evaluation tests the model's understanding of commonly shared human knowledge about generic visual concepts using ImageNetVC[15] and visual commonsense reasoning (VCR)[57]. Specifically, ImageNetVC is utilized for zero-shot visual commonsense evaluation, such as color and shape, while VCR covers various scenes, such as spatial, casual, and mental commonsense.Object Hallucination. LVLM suffers from the object hallucination problem, i.e., the generated results are inconsistent with the target images in the descriptions[14]. Evaluating object hallucination for different LVLMs help understand their respective weaknesses. To this end, we evaluate the object hallucination problem of LVLMs on the MSCOCO dataset[18] under POPE pipeline[14].MetricsIf you feel sleepy while looking at this image, it might be a good idea to take a break and rest for a while. The scene features a well-decorated bedroom with a bed, a chair, and a window. The room is \u2026\u2026 Analysis: Watching is a vague action, so we cannot determine it is romantic or not. We need more information about \u2026\u2026 Answer: We are not sure.Visual Knowledge Acquisition. Visual knowledge acquisition entails understanding images beyond \nperception to acquire knowledge. This evaluation is conducted through Optical Characters Recogni-\ntion (OCR) using twelve benchmarks (including IIIT5K [31], IC13 [32], IC15 [33], Total-Text [34], \nCUTE80 [35], SVT [36], SVTP [37], COCO-Text [38], WordArt [39], CTW [40], HOST [41], \nWOST [41]), Key Information Extraction (KIE) using the SROIE [42] and FUNSD [43], and Image \nCaptioning (ImgCap) using two benchmarks (including NoCaps [44] and Flickr30K [45]). The OCR \ntask measures whether a Hallucination \n\nObject Hallucination \n\nCOCO-Ransom/Popular/Adversarial \n\nQuantitative Evaluation \n\n\u2694 LVLM Arena \u2694 \n\nIs this a rabbit? \n\nNo, this is not \na rabbit. It is a dog \ndressed in a bunny \ncostume. \n\nNo, it is a dog \nwearing a bunny \ncostume. \n\nwins \n\nTie \n\nBoth are bad \n\nupdate \n\nElo Rating \n\n1000 \n\nMatchmaking \n\nwins \n\nChat \nVoting \n\nVisual Commonsense Reasoning \n\nVCR \n\n1000 \n1000 \n1000 \n1000 \n1000 \n\nUser \nLA-V2 \nMiniGPT-4 \nOtter \nLLaVA \nBLIP2 \nInstructBLIP \nmPLUG-Owl \nVPGTrans \n\n1000 \n1000 \n\nquestion answering (VQA) (including DocVQA[46], TextVQA[47], STVQA[48], OCR-VQA[49], \nOKVQA[50], GQA[51], IconQA[52], Visual Spatial Reasoning (VSR)[53], and Visual Dialog \n(Visdial)[54].), knowledge-grounded image description (Embodied Intelligence. Embodied intelligence aims to create agents, such as robots, which learn to \nsolve challenging tasks requiring environmental interaction. Recently, LLM and LVLM exhibited \nexceptional effectiveness in guiding the agent to complete a series of tasks. In this evaluation, \nwe utilize high-level tasks in EmbodiedGPT [58] and employ Minecraft [59], VirtualHome [60], \nMeta-World [61], and Franks Kitchen [61] as benchmarks. \nQuality \n\nRelevance \n\nUsefulness \n\nIs this a rabbit? \n\nLVLM \nLLM \nUser \nInput \nOutput \nGround truth \n\nWhat should I do \nif I feel sleepy? \n\nNo, it is a dog wearing a \nbunny costume. \n\nIs the person on \nleft and person \non right dating? \n\nYes, it is a rabbit. \n\nQuestion Answering \n\nPrefix-based Score \n\nMulti-turn Reasoning \n\n(A) Sheep \n\n(B) Rabbit \n\n(C) Dog \n\nIs this a rabbit? \n\nUser Study \n\nCaption \n\nQ1: What is the body \nlanguage of the man? \nQ2: What is the facial \nexpression of the man? \nQ3. What is the facial \nexpression of the nun? \n\nA1: He is looking at the nun \n\nA2: Sad \n\nA3: Sad \n\n0.1 \n\n0.7 \n\n0.9 \n\n\n\nTable 3 :\n3Comparison of Zero-shot Performance for Large-scale Vision and Language Models \n(LVLMs) on OCR, KIE, and Image Captioning Tasks. Evaluation metrics include word accuracy for \nOCR datasets, entity-level F1 score for KIE datasets, and CIDEr score for image captioning datasets. \n\nDatasets \nBLIP2 InstructBLIP LLaMA-Adapter-v2 LLaVA MiniGPT-4 mPLUG-Owl Otter VPGTrans \nS-SOTA \n\nVQA \n\nDocVQA \n4.75 \n5.89 \n8.13 \n6.26 \n2.65 \n2.24 \n3.44 \n3.53 \n54.48[46] \nTextVQA \n31.98 \n39.60 \n43.76 \n38.92 \n19.40 \n38.76 \n21.52 \n21.98 \n73.1[73] \nSTVQA \n20.98 \n28.30 \n32.33 \n28.40 \n13.55 \n12.10 \n15.23 \n17.13 \n-\nOCR-VQA \n38.85 \n60.20 \n38.12 \n23.40 \n16.85 \n8.84 \n19.50 \n21.71 \n-\nOKVQA \n44.93 \n60.52 \n55.93 \n54.36 \n37.48 \n22.89 \n49.01 \n44.51 \n-\nGQA \n45.53 \n49.96 \n43.93 \n41.30 \n30.82 \n14.02 \n38.12 \n32.99 \n72.1[74] \nVisdial \n10.73 \n45.20 \n12.92 \n14.66 \n10.31 \n13.34 \n11.67 \n9.70 \n68.92[75] \nIconQA \n62.82 \n56.25 \n41.83 \n42.95 \n37.59 \n11.64 \n26.77 \n38.22 \n83.62[52] \nVSR \n63.63 \n41.28 \n50.63 \n51.24 \n41.56 \n24.74 \n06.40 \n48.77 \n70.1[53] \nWHOOPS \n24.87 \n30.13 \n24.15 \n24.39 \n17.91 \n20.70 \n15.14 \n15.88 \n-\n\nKGID \nScienceQA IMG \n60.73 \n46.26 \n54.19 \n49.33 \n25.43 \n2.80 \n27.22 \n36.99 \n92.53[76] \nVizWiz \n65.44 \n65.31 \n62.07 \n62.42 \n47.48 \n38.99 \n50.04 \n53.23 \n73.3[73] \n\nVE \nSNLI-VE \n32.00 \n59.00 \n58.8 \n57.80 \n54.80 \n54.50 \n52.60 \n52.20 \n-\n\nAverage Score \n0.759 \n0.908 \n0.833 \n0.771 \n0.527 \n0.420 \n0.513 \n0.588 \n-\n\n\n\nTable 4 :\n4Comparison of Zero-shot Performance for LVLM Models on VQA, KGID, and VE Tasks.For VQA and KGID tasks, Mean Reciprocal Rank (MRR) is used for the Visdial, while top-1 accuracy \nis employed for the remaining tasks. \n\n\n\nTable 5 :\n5Comparisons of Zero-shot visual commonsense Performance for LVLM Models on VCR and ImageNetVC datasets. Top-1 accuracy is employed for the two datasets.\n\n\n15 participants. The study comprised 6 household scenarios carefully selected from VirtualHome [60].Datasets \n\nMetrics \nBLIP2 InstructBLIP LA-V2 LLaVA MiniGPT-4 mPLUG-Owl Otter VPGTrans \n\nMSCOCO-Random \n\nAccuracy \n82.21 \n88.83 \n74.44 \n51.52 \n52.58 \n61.37 \n61.40 \n48.28 \nPrecision \n97.48 \n96.01 \n68.24 \n51.54 \n68.63 \n57.89 \n57.82 \n74.17 \nRecall \n67.27 \n81.60 \n94.00 \n100.00 \n57.50 \n97.52 \n95.92 \n56.78 \nF1-Score \n79.61 \n88.23 \n79.08 \n68.03 \n62.57 \n72.65 \n72.15 \n64.32 \nYes \n35.58 \n43.99 \n70.99 \n100.00 \n44.25 \n87.15 \n85.76 \n47.38 \n\nMSCOCO-Popular \n\nAccuracy \n80.10 \n84.15 \n56.82 \n50.00 \n49.31 \n55.83 \n49.56 \n47.86 \nPrecision \n90.49 \n85.96 \n53.89 \n50.00 \n63.56 \n53.61 \n50.07 \n70.37 \nRecall \n67.27 \n81.60 \n94.20 \n100.00 \n58.03 \n97.13 \n95.92 \n55.90 \nF1-Score \n77.17 \n83.72 \n68.56 \n66.67 \n60.67 \n69.09 \n65.79 \n62.92 \nYes \n37.17 \n47.47 \n87.40 \n100.00 \n48.29 \n91.20 \n96.58 \n51.92 \n\nMSCOCO-Adversarial \n\nAccuracy \n78.52 \n81.95 \n60.52 \n50.00 \n49.62 \n54.43 \n50.68 \n47.86 \nPrecision \n86.83 \n82.05 \n54.58 \n50.00 \n62.55 \n52.73 \n50.56 \n69.76 \nRecall \n67.27 \n81.60 \n96.45 \n100.00 \n58.71 \n97.59 \n95.92 \n59.22 \nF1-Score \n75.81 \n81.82 \n69.12 \n66.67 \n68.47 \n72.09 \n66.22 \n64.06 \nYes \n38.73 \n49.77 \n88.23 \n100.00 \n48.54 \n92.95 \n95.31 \n52.27 \n\nAverage Score \n0.945 \n1.00 \n0.751 \n0.595 \n0.594 \n0.673 \n0.633 \n0.565 \n\n\n\n\nbelow. Furthermore, in the Appendix C, we present quantitative evaluation results for Franka Kitchen [61], Minecraft [59], and Meta-World[78]. Based on the findings, two deductions can be made. Firstly, the use of image-text pairs is consequential in aligning visual-text features. This is evident from the comparison demonstrated inTable 1. Otter and InstructBLIP lacked the training process on image-text pairs which necessitates the alignment of visual reasoning and text description, resulting in a degraded ability of spatial relationship analysis. Conversely, mPLUG-Owl outperformed Otter partially due to 204M image-text training pairs. Secondly, we observe that visual instruction data is essential for embodied tasks. BLIP2 lacked visual instruction tuning, which greatly affected its capability of producing reasonable and executable plans.Dataset \n\nBLIP2 InstructBLIP LA-V2 LLaVA MiniGPT-4 mPLUG-Owl Otter VPGTrans \n\nVirtualHome \n\nObject Recon.(\u2191) \n2.03 \n3.08 \n3.81 \n3.88 \n3.70 \n3.42 \n3.38 \n3.43 \nSpatial Relation.(\u2191) \n1.68 \n2.78 \n3.71 \n3.61 \n3.47 \n3.22 \n3.10 \n3.22 \nConciseness (\u2191) \n3.25 \n2.48 \n2.04 \n1.86 \n1.62 \n1.48 \n1.86 \n1.76 \nReasonability(\u2191) \n2.78 \n3.20 \n4.04 \n3.70 \n3.54 \n3.44 \n3.07 \n3.35 \nExecutability(\u2191) \n2.88 \n3.10 \n4.08 \n3.82 \n3.11 \n3.54 \n3.12 \n3.35 \n\nAverage Score \n0.674 \n0.772 \n0.922 \n0.879 \n0.805 \n0.785 \n0.761 \n0.789 \n\n\n\n\narena serves as an invaluable resource, providing an Elo rating rank that offers LVLMs ranking in the open-world scenario. Our evaluation results reveal several important findings, stimulating the future development of LVLMs. Yuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, and Yongdong Zhang. From two to one: A new scene text recognizer with visual language modeling network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14194-14203, 2021. Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507-2521, 2022. [56] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023. [59] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.[34] Chee Kheng Ch'ng and Chee Seng Chan. Total-text: A comprehensive dataset for scene text \ndetection and recognition. In 2017 14th IAPR International Conference on Document Analysis \nand Recognition (ICDAR), volume 01, pages 935-942, 2017. \n\n[35] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan. A robust \narbitrary text detection system for natural scene images. Expert Systems with Applications, \n41(18):8027-8048, 2014. \n\n[36] Cunzhao Shi, Chunheng Wang, Baihua Xiao, Song Gao, and Jinlong Hu. End-to-end scene text \nrecognition using tree-structured models. Pattern Recognition, 47(9):2853-2866, 2014. \n\n[37] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. Recognizing \ntext with perspective distortion in natural scenes. In 2013 IEEE International Conference on \nComputer Vision, pages 569-576, 2013. \n\n[38] Andreas Veit, Tomas Matera, Luk\u00e1s Neumann, Jiri Matas, and Serge J. Belongie. Coco-\ntext: Dataset and benchmark for text detection and recognition in natural images. ArXiv, \nabs/1601.07140, 2016. \n\n[39] Xudong Xie, Ling Fu, Zhifei Zhang, Zhaowen Wang, and Xiang Bai. Toward understanding \nwordart: Corner-guided transformer for scene text recognition. 2022. \n\n[40] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text \ndetection via transverse and longitudinal sequence connection. Pattern Recogn., 90(C):337-345, \njun 2019. \n\n[41] [42] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and \nCV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 \nInternational Conference on Document Analysis and Recognition (ICDAR), pages 1516-1520. \nIEEE, 2019. \n\n[43] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: A dataset for form \nunderstanding in noisy scanned documents. In 2019 International Conference on Document \nAnalysis and Recognition Workshops (ICDARW), volume 2, pages 1-6. IEEE, 2019. \n\n[44] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv \nBatra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. \nIn 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8947-8956, \n2019. \n\n[45] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions \nto visual denotations: New similarity metrics for semantic inference over event descriptions. \nTransactions of the Association for Computational Linguistics, 2:67-78, 02 2014. \n\n[46] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on \ndocument images. In Proceedings of the IEEE/CVF winter conference on applications of \ncomputer vision, pages 2200-2209, 2021. \n\n[47] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, \nand Marcus Rohrbach. Towards vqa models that can read. In 2019 IEEE/CVF Conference on \nComputer Vision and Pattern Recognition (CVPR), pages 8309-8318, 2019. \n\n[48] Ali Furkan Biten, Rub\u00e8n Tito, Andr\u00e9s Mafla, Lluis Gomez, Mar\u00e7al Rusi\u00f1ol, Minesh Mathew, \nC.V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Icdar 2019 competition on scene \ntext visual question answering. In 2019 International Conference on Document Analysis and \nRecognition (ICDAR), pages 1563-1570, 2019. \n[49] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: \nVisual question answering by reading text in images. In 2019 International Conference on \nDocument Analysis and Recognition (ICDAR), pages 947-952, 2019. \n\n[50] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual \nquestion answering benchmark requiring external knowledge. In 2019 IEEE/CVF Conference \non Computer Vision and Pattern Recognition (CVPR), pages 3190-3199, 2019. \n\n[51] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual \nreasoning and compositional question answering. In 2019 IEEE/CVF Conference on Computer \nVision and Pattern Recognition (CVPR), pages 6693-6702, 2019. \n\n[52] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, \nand Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and \nvisual language reasoning. In The 35th Conference on Neural Information Processing Systems \n(NeurIPS) Track on Datasets and Benchmarks, 2021. \n\n[53] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions \nof the Association for Computational Linguistics, 2023. \n\n[54] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M.F. Moura, Devi \nParikh, and Dhruv Batra. Visual Dialog. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), 2017. \n\n[55] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind \nTafjord, interface software and technology, pages 333-342, 2010. \n\n[57] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: \nVisual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer \nvision and pattern recognition, pages 6720-6731, 2019. \n\n[58] [60] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio \nTorralba. Virtualhome: Simulating household activities via programs. In Proceedings of the \nIEEE Conference on Computer Vision and Pattern Recognition, pages 8494-8502, 2018. \n\n[61] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay \npolicy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv \npreprint arXiv:1910.11956, 2019. \n\n[62] Wei-Lin Chiang Hao Zhang Joseph E. Gonzalez Lianmin Zheng, Ying Sheng and Ion Stoica. \nFastchat. https://github.com/lm-sys/FastChat, 2023. \n\n[63] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, \nXuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V. Le. Symbolic discovery \nof optimization algorithms, 2023. \n\n[64] H M Dipu Kabir. Reduction of class activation uncertainty with background information, 2023. \nRank Model \n\nScore \n\nInstructBLIP \n0.928 \n\nBLIP2 \n0.858 \n\nmPLUG-Owl \n0.831 \n\nLLaMA-Adapter V2 0.813 \n\nMiniGPT-4 \n0.727 \n\nOtter \n0.661 \n\nLLaVA \n0.615 \n\nVPGTrans \n0.563 \n\nRank Model \nScore \n\nInstructBLIP \n0.967 \n\nBLIP2 \n0.927 \n\nVPGTrans \n0.720 \n\nLLaMA-Adapter V2 0.443 \n\nLLaVA \n0.377 \n\nMiniGPT-4 \n0.346 \n\nmPLUG-Owl \n0.286 \n\nOtter \n0.237 \n\nRank Model \nScore \n\nInstructBLIP \n0.908 \n\nLLaMA-Adapter V2 0.833 \n\nLLaVA \n0.771 \n\nBLIP2 \n0.759 \n\nVPGTrans \n0.588 \n\nMiniGPT-4 \n0.527 \n\nOtter \n0.513 \n\nmPLUG-Owl \n0.420 \n\nRank Model \nScore \n\nInstructBLIP \n0.995 \n\nLLaVA \n0.791 \n\nLLaMA-Adapter V2 0.589 \n\nOtter \n0.582 \n\nmPLUG-Owl \n0.579 \n\nMiniGPT-4 \n0.565 \n\nBLIP2 \n0.535 \n\nVPGTrans \n0.522 \n\nRank Model \nScore \n\nInstructBLIP \n1.0 \n\nBLIP2 \n0.945 \n\nLLaMA-Adapter V2 0.751 \n\nmPLUG-Owl \n0.673 \n\nOtter \n0.633 \n\nLLaVA \n0.595 \n\nMiniGPT-4 \n0.594 \n\nVPGTrans \n0.565 \n\nRank Model \nScore \n\nLLaMA-Adapter V2 0.922 \n\nLLaVA \n0.879 \n\nMiniGPT-4 \n0.805 \n\nVPGTrans \n0.789 \n\nmPLUG-Owl \n0.785 \n\nInstructBLIP \n0.772 \n\nOtter \n0.761 \n\nBLIP2 \n0.674 \n\n\n\n\nFor OC task, we test LVLMs on MSCOCO and VCR1.0[57]. It involves querying the model about the number of objects belonging to an image's specific class of interest. To this end, we use the prompt 'Question: How many[obj]  are there in the image? Answer:'\n\n\nFor KIE task, we employ the SROIE[42] and FUNSD[43]  benchmarks to evaluate LVLMs, which encompass diverse documents like receipts and forms that require specific information extraction. The performance of LVLMs is evaluated using entity-level F1 scores. Additionally, we utilize informationspecific prompts for each piece of information that the model should extract. For instance, in the SROIE benchmark case, we use the prompt 'what is the name of the company that issued this invoice?' to extract company information and 'where was this invoice issued?' prompt for address information.For ImgCap task, we utilize two benchmarks, including NoCaps[44] and Flickr30K[45]. Each benchmark provides a collection of images with corresponding captions. In evaluation, CIDEr scores are used to evaluate these models with the prompt 'what is described in the image?'. These benchmarks offer a diverse set of question-image pairs, covering a wide range of topics. The task requires LVLMs to not only understand the visual content but also comprehend and reason about the posed questions. For specific evaluation, we employ the Mean Reciprocal Rank (MRR) metric for Visdial and top-1 accuracy for the remaining datasets. These metrics provide insights into the model's ability to accurately answer questions across the various VQA benchmarks.33], Total-Text[34], CUTE80[35], \nStreet View Text (SVT)[36], SVTP-Perspective (SVTP)[37], COCO-Text[38], WordArt[39], SCUT-\nCTW1500 (CTW)[40], heavily occluded scene text (HOST)[41], weakly occluded scene text \n(WOST)[41]. These benchmarks consist of a diverse range of images containing textual infor-\nmation which can make an adequate comparison between LVLMs. The performance of the LVLMs \nis compared with top-1 accuracy and the prompt we use is 'what is written in the image?'. \n\nB.3 Details of Visual Reasoning \n\nFor VQA task, we utilize nine benchmarks: DocVQA[46], TextVQA[47], STVQA[48], OCR-\nVQA[49], OKVQA[50], GQA[51], IconQA[52], Visual Spatial Reasoning (VSR)[53], and Visual \nDialog (Visdial)[54]. For KGID task, it evaluates the LVLM's capability to generate informative and accurate descrip-\ntions of images by incorporating external knowledge. To assess performance, we employ the \nScienceQA[55] and VizWiz[56] benchmarks, which consist of images accompanied by textual de-\nscriptions and knowledge-based information. Notably, in the case of ScienceQA, we only utilize the \nsamples that contain images. \n\n\n\nTable A .\nA1: Description of datasets used in our LVLM-eHub.ImageNet1K   The ImageNet1K dataset[87] consists of 1K object classes and contains 1,281,167 training images, 50 images per class for validation, and 100 images per class for testing.Pets37The Oxford-IIIT Pet dataset[28] comprises 37 categories (Pets37 for short) with 25 dog breeds and 12 cat ones and200 images per class. There are 7349 images in total, 3680 trainval images, and 3669 test images.Flowers102The Oxford 102 Flower dataset[29] includes 120 flower categories (Flowers102 for short) with 40 to 258 images for each class and 8189 images in total, namely 10 images per class for both train and val and the rest for a test.COCO-OC We ask the model to count the number of a certain object appearing in the image and attend to individual objects, which is decoupled from high-level semantics and thus a more appropriate test bed for fine-grained visual understanding evaluation. We construct the dataset of this problem with images from the validation set of MSCOCO10000 (val) COCO-MCI We ask the model if a certain object exists in the image and attend to individual objects, which is decoupled from high-level semantics and thus a more appropriate test bed for fine-grained visual understanding evaluation. We construct the dataset of this problem with images from the validation set of MSCOCO 10000 (val) VCR-OC Same as COCO-OC, but using images from the validation set of the VCR dataset 10000 (val) VCR-MCI Same as COCO-MCI, but using images from the validation set of the VCR dataset 10000 (val) Visual Knowledge Acquisition Datasets IIIT5K The IIIT5K [31] is an ocr dataset that contains words from street scenes and originally-digital images. It is split into 2k/3k for train/test set. IC13 The ICDAR 2013 dataset [32] consists of 229 training images and 233 testing images, with word-level annotations provided. Specifically, it contains 848 and 1095 cropped text instance images for the train and test sets respectively. IC15 The ICDAR 2015 dataset [33] contains 1500 images: 1000 for training and 500 for testing. Its train/test set contains 4468/2077 cropped text instance images. Total-Text The total-test dataset [34] contains 1555 images: 1255 for training and 300 for testing. It contains 2551 cropped text instance images in the test set. CUTE80 The CUTE80 dataset [35] contains 288 cropped text instance images getting from 80 high-resolution images. SVT The Street View Text (SVT) dataset [36] was harvested from google street view. It contains 350 images in total and 647 cropped text instance images for testing. SVTP The SVTP dataset [37] contains 645 cropped text instance images. It is specifically designed to evaluate perspective-distorted text recognition. No train/test split was provided. COCO-Text The COCO-Text dataset [38] we use is based on the v1.4 annotations, which contains 9896/42618 annotated words in val/train set. WordArt The WordArt dataset [39] consists of 6316 artistic text images with 4805 training images and 1511 testing images. CTW The SUCT-CTW1500 (CTW) dataset includes over 10,000 text annotations in 1500 images (1000 for training and 500 for testing) used in curved text detection. In our evaluation, we use 1572 rectangle-cropped images getting from the testing set. HOST The heavily occluded scene text (HOST) in Occlusion Scene Text (OST) dataset [41]. 2416 (HOST) WOST The weakly occluded scene text (WOST) in the OST dataset. 2416 (WOST) SROIE The SROIE dataset [42] contains 1000 complete scanned receipt images for OCR and KIE tasks. The dataset is split into 600/400 for the trainval/test set. In the KIE task, it is required to extract company, data, address, and total expenditure information from the receipt and there are 347 annotated receipts in the test set. FUNSD The FUNSD dataset [43] contains 199 real, fully annotated, scanned forms for the KIE task. It is split 50/149 for the test/train set.NoCapsThe NoCaps dataset contains 15100 images with 166100 human-written captions for novel object image captioning.Dataset Name \nDataset Description \nEvaluation Data \n\nVisual Perception Datasets \n\n50K (val.) \n\nCIFAR10 \nThe CIFAR10 dataset [27] has 10 classes and 6000 images per class with 5000 for \ntraining and 1000 for testing. \n\n10K (test) \n\n3669 (test) \n\n6149 (test) \n\n3k (test) \n\n848 (train) \n\n2077 (test) \n\n2551 (test) \n\n288 (all) \n\n647 (test) \n\n645 (all) \n\n9896 (val) \n\n1511 (test) \n\n1572 (test) \n\n347 (test) \n\n50 (test) \n\n4500 (val) \n\n\nTable A .\nA1 -continued from previous page Flickr-30k The Flickr30k dataset consists of 31K images collected from Flickr, each image has five ground truth captions. We use the test split which contains 1K images. WHOOPS The WHOOPS dataset [88] includes 500 synthetic and compositional images and 5 captions per image. TextVQA Notably, we use the latest v0.5.1 version of TextVQA [47] dataset. It contains 34602 questions based on 21953 images from OpenImages' training set. Its validation set contains 5000 questions based on 3166 images. STVQA Scene Text Visual Question Answering (STVQA) [48] consists of 31,000+ questions across 23,000+ images collected from various public datasets. It contains 26074 questions in the train set and we sample 4000 samples from the train set in default order with seed 0. OKVQA OKVQA [50] is a dataset about outside knowledge visual question answering. It contains 14055 open-ended question-answer pairs in total. GQA GQA [51] is a visual question-answering dataset with real images from the Visual Genome dataset. Visdial Visual Dialog (Visdial) [54] contain images sampled from COCO2014 and each dialog has 10 rounds. In our evaluation, we treat it as a VQA dataset by splitting each dialog sample into question-answer pairs by rounds. As there are 2064 dialog samples in the validation set, we have 20640 question-answer pairs collected from the validation set. IconQA IconQA dataset [52] provide diverse visual question-answering samples and we use the test set in its multi-text-choice task. VSR Visual Spatial Reasoning (VSR) dataset [53] contains a collection of caption-image pairs with true/false labels. We treat it as a VQA dataset by asking the model to answer True or False. WHOOPS The WHOOPS dataset [88] encompasses 500 synthetic and compositional images and 3662 question-answer pairs in total. Specifically there is only one answer for each question. ScienceQA IMG ScienceQA [55] is a multimodal benchmark containing multiple choice questions with a diverse set of science topics. In our evaluation, we only use the samples with images in the test set. VizWiz VizWiz [56] is a VQA dataset whose answers are got by asking blind people. 1131 (val)Dataset Name \nDataset Description \nEvaluation Data \n\n1K (test) \n\n2500 \n\nVisual Reasoning Datasets \n\nDocVQA \nDocVQA [46] contains 12K images and 50K manually annotated questions and an-\nswers. \n\n5349 (val) \n\n5000 (val) \n\n4000 (train) \n\nOCR-VQA \nOCRVQA [49] contains 100037 question-answer pairs spanning 207572 book cover \nimages. \n\n100037 (all) \n\n5046 (val) \n\n12578 (testdev) \n\n20640 (val) \n\n6316 (test) \n\n10972 (all) \n\n3662 \n\n2017 (test) \n\n\nConclusion. This paper proposes a comprehensive evaluation benchmark for large vision-language models called LVLM-eHub that incorporates both quantitative performance evaluation and human feedback evaluation. For the quantitative evaluation, we employ 16 tasks spanning over 40+ textrelated visual datasets to assess the six essential capabilities of LVLM models. Additionally, we have established an online LVLM Arena to gather human feedback on LVLM models continually. This\nMiniGPT-4:The robot is standing in the middle of a kitchen, facing a counter with two burners on top and a sink on the left. A kettle is on the counter in front of the right burner. The robot is holding a ladle in its right arm and is reaching towards the kettle with it. Here's a plan to move the kettle to the top left burner: 1. The robot will take the ladle from its right arm and place it on the counter next to the kettle. 2. The robot will use its left arm to pick up the kettle and place it on the top left burner. 3. The robot will then take the ladle and use it to pour hot water into the kettle. Overall, the action goal is achieved in three steps. mPLUG-Owl: 1. Obtain the position of the kettle on the countertop from the image. If the position of the kettle is not known, move the robot arm to scan the entire countertop area to find it. 3. After locating the kettle, the robot arm should grasp it using its gripper. 4. Using sensors such as cameras, the robot arm should navigate around other objects on the countertop and reach the desired location, which in this case is the left burner of a kitchen stove. Once the robot has arrived at the desired location, it should securely place the kettle on the top left burner. 6. The robot may need to adjust its grip on the kettle or the stove if necessary to maintain stability. 7. After the kettle is securely placed, the robot arm should release its grip and return to its original position or move back to the countertop to perform any additional tasks. Note: If it is already known that the kettle is on the top left burner, the robot may skip to step 4 or proceed to step 6 directly, based on its predefined task.Otter: In the image, the robot is already in the process of lifting the teapot with its arm and is positioning it on top of a burner on the stove. The plan would likely consist of the following steps: 1. Position the arm with the pot securely and safely on the burner. 2. Ensure that the pot is placed at the correct height and distance from the flame to avoid any accidents or damage to the pot. 3. Utilize the robot's vision system or other sensors to accurately position the pot at the desired location. 4. Once positioned, the arm should maintain a firm grip on the pot to prevent it from falling or shifting during the cooking process. 5. If the pot needs to be lowered or moved to a different burner, the same process would be repeated, with the robot carefully adjusting the position of the pot and maintaining a secure grip. The generated plan should be clear, step-by-step, and easy to follow, ensuring that the robot successfully places the pot on the desired burner while maintaining safety and efficiency.VPGTrans: To move the kettle to the top left burner, the robot should first identify the location of the kettle and the location of the top left burner. Then, the robot should use its arm to pick up the kettle and move it to the top left burner. Once the kettle is in place, the robot should use its arm to adjust the position of the kettle so that it is centered on the burner. Finally, the robot should turn on the burner to heat the water in the kettle.\nC.3.2 Franka KitchenBLIP: Move the kettle to the top left burner step.InstructBLIP: Here's a step plan: 1. Analyze the image to determine whether the action goal of moving the kettle to the top left burner has already been achieved or not. 2. If the action goal is not achieved, generate a plan for the robot to move the kettle to the top left burner. 3. Determine the initial position of the kettle on the stove. 4. Calculate the distance between the current position of the kettle and the desired position on the top left burner. 5. Based on the calculated distance, determine the necessary steps for the robot to move the kettle from its current position to the desired position on the top left burner. 6. Implement the necessary steps using the robot's movement capabilities. 7. Verify that the kettle has been successfully moved to the top left burner.LLaMA-Adapter V2: To move the kettle to the top left burner, the robot would need to follow these steps: 1. Detect the kettle's position: The robot should first identify the kettle's current location on the kitchen counter. This can be done by analyzing the image and recognizing the kettle's shape, size, and color.2. Plan the path: The robot should then calculate the most efficient path to move the kettle to the top left burner. This could involve moving the kettle to a specific location on the counter, then rotating or tilting the kettle to align it with the burner.3. Execute the plan: The robot should then execute the plan.LLaVA: To move the kettle to the top left burner, the robot should follow these steps: 1. Identify the kettle's current position on the stove. 2. Determine the appropriate path to reach the top left burner. 3. Approach the kettle from the side, ensuring it doesn't obstruct the path. Move the kettle towards the top left burner, making sure it doesn't touch any other burners or appliances. 5. Once the kettle is positioned on the top left burner, confirm that it is stable and secure. 6. If the kettle is not yet on the top left burner, repeat steps 2-5 until the kettle is successfully placed on the burner. In the image, the kettle is already on the top left burner, so there is no need to generate the plans. The robot has successfully moved the kettle to the desired location.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n\nLanguage models are few-shot learners. Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Gray, Ilya Sutskever, and Dario Amodei. Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n\n. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Reynolds, Advances in Neural Information Processing Systems. 35Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.\n\nBlip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, arXiv:2304.15010Parameter-efficient visual instruction model. 2arXiv preprintPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023.\n\nAo Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, Tat-Seng Chua, arXiv:2305.01278Transfer visual prompt generator across llms. arXiv preprintAo Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt generator across llms. arXiv preprint arXiv:2305.01278, 2023.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. arXiv preprintHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, arXiv:2304.14178Modularization empowers large language models with multimodality. arXiv preprintQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.\n\nOtter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, arXiv:2305.03726arXiv preprintBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\n\nInstructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, arXiv:2305.06500arXiv preprintWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n\nEvaluating object hallucination in large vision-language models. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355arXiv preprintYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.\n\nImagenetvc: Zero-shot visual commonsense evaluation on 1000 imagenet categories. Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei Qin, Zhifang Sui, arXiv:2305.15028arXiv preprintHeming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei Qin, and Zhifang Sui. Imagenetvc: Zero-shot visual commonsense evaluation on 1000 imagenet categories. arXiv preprint arXiv:2305.15028, 2023.\n\nOn the hidden mystery of ocr in large multimodal models. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, arXiv:2305.07895arXiv preprintYuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023.\n\nWhat makes for good visual tokenizers for large language models?. Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, Ying Shan, arXiv:2305.12223arXiv preprintGuangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023.\n\nMicrosoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, arXiv:1504.00325arXiv preprintXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018.\n\nConceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558-3568, 2021.\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 123Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32-73, 2017.\n\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, Saehoon Kim, Coyo-700m: Image-text pair dataset. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Sae- hoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/ coyo-dataset, 2022.\n\nLaion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114arXiv preprintChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n\nLaion coco: 600m synthetic captions from laion2b-en. Theo Coombes Richard Vencu Benjamin Trom Christoph Schuhmann, Andreas K\u00f6pf and Romain BeaumontTheo Coombes Richard Vencu Benjamin Trom Christoph Schuhmann, Andreas K\u00f6pf and Romain Beaumont. Laion coco: 600m synthetic captions from laion2b-en, Oct 2022.\n\nIm2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara Berg, Advances in neural information processing systems. 24Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 115Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211-252, 2015.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nCats and dogs. M Omkar, Andrea Parkhi, Andrew Vedaldi, C V Zisserman, Jawahar, IEEE Conference on Computer Vision and Pattern Recognition. Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\n\nAutomated flower classification over a large number of classes. Maria-Elena Nilsback, Andrew Zisserman, Indian Conference on Computer Vision, Graphics and Image Processing. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.\n\nWhat makes for good visual tokenizers for large language models?. Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, Ying Shan, arXiv:2305.12223arXiv preprintGuangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023.\n\nTop-down and bottom-up cues for scene text recognition. Anand Mishra, Karteek Alahari, C V Jawahar, 2012 IEEE Conference on Computer Vision and Pattern Recognition. Anand Mishra, Karteek Alahari, and C. V. Jawahar. Top-down and bottom-up cues for scene text recognition. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2687-2694, 2012.\n\nIcdar 2013 robust reading competition. Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez I Bigorda, Joan Sergi Robles Mestre, David Fernandez Mas, Mota, 12th International Conference on Document Analysis and Recognition. Jon Almaz\u00e0n Almaz\u00e0n, and Llu\u00eds Pere de las HerasDimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almaz\u00e0n Almaz\u00e0n, and Llu\u00eds Pere de las Heras. Icdar 2013 robust reading competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1484-1493, 2013.\n\nIcdar 2015 competition on robust reading. Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, Faisal Shafait, Seiichi Uchida, Ernest Valveny, 13th International Conference on Document Analysis and Recognition (ICDAR). Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, Faisal Shafait, Seiichi Uchida, and Ernest Valveny. Icdar 2015 competition on robust reading. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 1156-1160, 2015.\n\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Julien Mairal, Patrick LabatutMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.\n\nFine-grained visual classification via internal ensemble learning transformer. Qin Xu, Jiahui Wang, Bo Jiang, Bin Luo, IEEE Transactions on Multimedia. Qin Xu, Jiahui Wang, Bo Jiang, and Bin Luo. Fine-grained visual classification via internal ensemble learning transformer. IEEE Transactions on Multimedia, pages 1-14, 2023.\n\nClip4str: A simple baseline for scene text recognition with pre-trained vision-language model. Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang, Shuai Zhao, Xiaohan Wang, Linchao Zhu, and Yi Yang. Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model, 2023.\n\nScene text recognition with permuted autoregressive sequence models. Darwin Bautista, Rowel Atienza, European Conference on Computer Vision. ChamSpringer Nature SwitzerlandDarwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. In European Conference on Computer Vision, pages 178-196, Cham, 10 2022. Springer Nature Switzerland.\n\nCentripetaltext: An efficient text instance representation for scene text detection. Tao Sheng, Jie Chen, Zhouhui Lian, Thirty-Fifth Conference on Neural Information Processing Systems. Tao Sheng, Jie Chen, and Zhouhui Lian. Centripetaltext: An efficient text instance representation for scene text detection. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n\nLayoutlmv2: Multimodal pre-training for visually-rich document understanding. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou, ACL-IJCNLP 2021. Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi- modal pre-training for visually-rich document understanding. In ACL-IJCNLP 2021, January 2021.\n\nGeolayoutlm: Geometric pre-training for visual information extraction. Chuwei Luo, Changxu Cheng, Qi Zheng, Cong Yao, abs/2304.10759CoRRChuwei Luo, Changxu Cheng, Qi Zheng, and Cong Yao. Geolayoutlm: Geometric pre-training for visual information extraction. CoRR, abs/2304.10759, 2023.\n\nGIT: A generative image-to-text transformer for vision and language. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, Transactions on Machine Learning Research. Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language. Transactions on Machine Learning Research, 2022.\n\n. Xi Chen, Xiao Wang, Soravit Changpinyo, Piotr Piergiovanni, Daniel Padlewski, Sebastian Alexander Salz, Adam Goodman, Basil Grycner, Lucas Mustafa, Alexander Beyer, Joan Kolesnikov, Nan Puigcerver, Keran Ding, Hassan Rong, Gaurav Akbari, Linting Mishra, Ashish Xue, James Thapliyal, Weicheng Bradbury, Mojtaba Kuo, Chao Seyedhosseini, Jia, Burcu Karagol Ayan. Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu SoricutPali: A jointly-scaled multilingual language-image model. 2023Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebas- tian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. 2023.\n\nCoarse-to-fine reasoning for visual question answering. Tuong Binh X Nguyen, Huy Do, Erman Tran, Tjiputra, D Quang, Anh Tran, Nguyen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBinh X Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D Tran, and Anh Nguyen. Coarse-to-fine reasoning for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4558-4566, 2022.\n\nFactor graph attention. Idan Schwartz, Seunghak Yu, Tamir Hazan, Alexander G Schwing, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIdan Schwartz, Seunghak Yu, Tamir Hazan, and Alexander G Schwing. Factor graph attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2039-2048, 2019.\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n\nAugmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsNandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 296-310, Online, June 2021. Association for Computational Linguistics.\n\nMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine, Conference on robot learning. PMLRTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094-1100. PMLR, 2020.\n\nThe proposed uscf rating system. its development, theory, and applications. E Arpad, Elo, Chess Life. 228Arpad E Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242-247, 1967.\n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, arXiv:2204.05862arXiv preprintYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\n\nExploring the limits of masked visual representation learning at scale. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao, Eva, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358-19369, 2023.\n\nHyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprintHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLRAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.\n\nBaolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprintBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n\nLanguage is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Qiang Owais Khan Mohammed, Liu, arXiv:2302.14045arXiv preprintShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.\n\nIdealgpt: Iteratively decomposing vision and language reasoning via large language models. Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A Ayyubi, Kai-Wei Chang, Shih-Fu Chang, Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. Idealgpt: Iteratively decomposing vision and language reasoning via large language models, 2023.\n\nImagenet: A largescale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255, 2009.\n\nBreaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images. Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, Roy Schwartz, Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images, 2023.\n\nVisual entailment: A novel task for fine-grained image understanding. Ning Xie, Farley Lai, Derek Doran, Asim Kadav, arXiv:1901.06706arXiv preprintNing Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706, 2019.\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, J Hockenmaier, Transactions of the Association for Computational Linguistics. 2Peter Young, Alice Lai, Micah Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78, 2014.\n", "annotations": {"author": "[{\"end\":92,\"start\":84},{\"end\":161,\"start\":93},{\"end\":209,\"start\":162},{\"end\":260,\"start\":210},{\"end\":306,\"start\":261},{\"end\":352,\"start\":307},{\"end\":418,\"start\":353},{\"end\":468,\"start\":419},{\"end\":518,\"start\":469},{\"end\":563,\"start\":519},{\"end\":638,\"start\":564}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":89},{\"end\":95,\"start\":93},{\"end\":172,\"start\":168},{\"end\":223,\"start\":218},{\"end\":269,\"start\":266},{\"end\":315,\"start\":312},{\"end\":361,\"start\":358},{\"end\":431,\"start\":427},{\"end\":481,\"start\":476},{\"end\":526,\"start\":522},{\"end\":572,\"start\":569}]", "author_first_name": "[{\"end\":88,\"start\":84},{\"end\":167,\"start\":162},{\"end\":217,\"start\":210},{\"end\":265,\"start\":261},{\"end\":311,\"start\":307},{\"end\":357,\"start\":353},{\"end\":426,\"start\":419},{\"end\":475,\"start\":469},{\"end\":521,\"start\":519},{\"end\":568,\"start\":564}]", "author_affiliation": "[{\"end\":115,\"start\":97},{\"end\":131,\"start\":117},{\"end\":160,\"start\":133},{\"end\":192,\"start\":174},{\"end\":208,\"start\":194},{\"end\":243,\"start\":225},{\"end\":259,\"start\":245},{\"end\":289,\"start\":271},{\"end\":305,\"start\":291},{\"end\":335,\"start\":317},{\"end\":351,\"start\":337},{\"end\":381,\"start\":363},{\"end\":397,\"start\":383},{\"end\":417,\"start\":399},{\"end\":451,\"start\":433},{\"end\":467,\"start\":453},{\"end\":501,\"start\":483},{\"end\":517,\"start\":503},{\"end\":546,\"start\":528},{\"end\":562,\"start\":548},{\"end\":592,\"start\":574},{\"end\":608,\"start\":594},{\"end\":637,\"start\":610}]", "title": "[{\"end\":81,\"start\":1},{\"end\":719,\"start\":639}]", "venue": null, "abstract": "[{\"end\":2453,\"start\":721}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2516,\"start\":2513},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2527,\"start\":2524},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2543,\"start\":2540},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2758,\"start\":2755},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3285,\"start\":3282},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3441,\"start\":3438},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3478,\"start\":3475},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3480,\"start\":3478},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3518,\"start\":3515},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3520,\"start\":3518},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3524,\"start\":3520},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3528,\"start\":3524},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3532,\"start\":3528},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3536,\"start\":3532},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3538,\"start\":3536},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3799,\"start\":3795},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3957,\"start\":3953},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4037,\"start\":4033},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4180,\"start\":4176},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4564,\"start\":4560},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4583,\"start\":4579},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5474,\"start\":5471},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5650,\"start\":5647},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6201,\"start\":6198},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6632,\"start\":6628},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6643,\"start\":6639},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6659,\"start\":6655},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6718,\"start\":6714},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6721,\"start\":6718},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6741,\"start\":6737},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6757,\"start\":6753},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6774,\"start\":6770},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6794,\"start\":6790},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6818,\"start\":6814},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6934,\"start\":6931},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7000,\"start\":6996},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7590,\"start\":7587},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7601,\"start\":7598},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7623,\"start\":7620},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7639,\"start\":7635},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7655,\"start\":7651},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7667,\"start\":7663},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7686,\"start\":7682},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7704,\"start\":7701},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8749,\"start\":8745},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8763,\"start\":8759},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8776,\"start\":8772},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8796,\"start\":8792},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8885,\"start\":8881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12943,\"start\":12939},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19244,\"start\":19240},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19635,\"start\":19631},{\"end\":20528,\"start\":20519},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20649,\"start\":20645},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21646,\"start\":21642},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21729,\"start\":21728},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22002,\"start\":21999},{\"end\":22127,\"start\":22123},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23190,\"start\":23186},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23265,\"start\":23261},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24314,\"start\":24310},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28032,\"start\":28028},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29505,\"start\":29502},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29746,\"start\":29742},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29847,\"start\":29843},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29921,\"start\":29918},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":29971,\"start\":29967},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30268,\"start\":30265},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30697,\"start\":30693},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30715,\"start\":30711},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31006,\"start\":31003},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31024,\"start\":31020},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31118,\"start\":31115},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31453,\"start\":31449},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31536,\"start\":31533},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31770,\"start\":31766},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32034,\"start\":32031},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":32610,\"start\":32606},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33608,\"start\":33604},{\"end\":33795,\"start\":33791},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34183,\"start\":34179},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34206,\"start\":34202},{\"end\":34227,\"start\":34226},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34361,\"start\":34357},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":36592,\"start\":36588},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37200,\"start\":37196},{\"end\":37279,\"start\":37271},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37837,\"start\":37833},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41448,\"start\":41444},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":54224,\"start\":54220},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":54465,\"start\":54461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":54565,\"start\":54561},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":55000,\"start\":54996},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":55517,\"start\":55513},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":55844,\"start\":55840},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":59776,\"start\":59772},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":60174,\"start\":60170},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":60365,\"start\":60361},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":60389,\"start\":60385},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":66360,\"start\":66356},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":79064,\"start\":79060},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":79245,\"start\":79241},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":79467,\"start\":79463}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56582,\"start\":55847},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56871,\"start\":56583},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56905,\"start\":56872},{\"attributes\":{\"id\":\"fig_5\"},\"end\":57442,\"start\":56906},{\"attributes\":{\"id\":\"fig_6\"},\"end\":57484,\"start\":57443},{\"attributes\":{\"id\":\"fig_8\"},\"end\":57858,\"start\":57485},{\"attributes\":{\"id\":\"fig_9\"},\"end\":58245,\"start\":57859},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63127,\"start\":58246},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":64525,\"start\":63128},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":64754,\"start\":64526},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":64919,\"start\":64755},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":66216,\"start\":64920},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":67567,\"start\":66217},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":76246,\"start\":67568},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":76502,\"start\":76247},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":78963,\"start\":76503},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":83446,\"start\":78964},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":86087,\"start\":83447}]", "paragraph": "[{\"end\":2951,\"start\":2469},{\"end\":3539,\"start\":2953},{\"end\":4406,\"start\":3541},{\"end\":5383,\"start\":4408},{\"end\":6142,\"start\":5385},{\"end\":7318,\"start\":6144},{\"end\":8177,\"start\":7342},{\"end\":8497,\"start\":8216},{\"end\":9290,\"start\":8499},{\"end\":9894,\"start\":9328},{\"end\":10087,\"start\":9896},{\"end\":10441,\"start\":10089},{\"end\":10642,\"start\":10443},{\"end\":10974,\"start\":10644},{\"end\":11545,\"start\":10999},{\"end\":12243,\"start\":11547},{\"end\":13972,\"start\":12245},{\"end\":14569,\"start\":14000},{\"end\":15113,\"start\":14602},{\"end\":15313,\"start\":15115},{\"end\":16468,\"start\":15357},{\"end\":17659,\"start\":16500},{\"end\":18036,\"start\":17693},{\"end\":18863,\"start\":18038},{\"end\":20297,\"start\":18899},{\"end\":21014,\"start\":20334},{\"end\":21092,\"start\":21016},{\"end\":21137,\"start\":21094},{\"end\":21180,\"start\":21139},{\"end\":21219,\"start\":21182},{\"end\":21358,\"start\":21221},{\"end\":21407,\"start\":21360},{\"end\":21450,\"start\":21409},{\"end\":21500,\"start\":21452},{\"end\":22003,\"start\":21502},{\"end\":22565,\"start\":22005},{\"end\":23820,\"start\":22604},{\"end\":25077,\"start\":23850},{\"end\":26196,\"start\":25079},{\"end\":26530,\"start\":26433},{\"end\":28296,\"start\":26532},{\"end\":28921,\"start\":28298},{\"end\":29459,\"start\":28923},{\"end\":32335,\"start\":29494},{\"end\":32585,\"start\":32393},{\"end\":33735,\"start\":32587},{\"end\":34019,\"start\":33737},{\"end\":35219,\"start\":34067},{\"end\":35599,\"start\":35221},{\"end\":35635,\"start\":35601},{\"end\":36013,\"start\":35637},{\"end\":36048,\"start\":36015},{\"end\":36144,\"start\":36050},{\"end\":36922,\"start\":36189},{\"end\":37201,\"start\":36960},{\"end\":37935,\"start\":37257},{\"end\":38312,\"start\":37937},{\"end\":38360,\"start\":38314},{\"end\":38953,\"start\":38362},{\"end\":39466,\"start\":38998},{\"end\":39557,\"start\":39483},{\"end\":39641,\"start\":39567},{\"end\":39705,\"start\":39662},{\"end\":39777,\"start\":39719},{\"end\":39835,\"start\":39787},{\"end\":39888,\"start\":39849},{\"end\":39944,\"start\":39901},{\"end\":39997,\"start\":39954},{\"end\":40057,\"start\":40014},{\"end\":40110,\"start\":40067},{\"end\":40174,\"start\":40131},{\"end\":40231,\"start\":40188},{\"end\":40284,\"start\":40241},{\"end\":40342,\"start\":40298},{\"end\":40428,\"start\":40359},{\"end\":40539,\"start\":40438},{\"end\":40596,\"start\":40560},{\"end\":40717,\"start\":40610},{\"end\":40763,\"start\":40727},{\"end\":40813,\"start\":40777},{\"end\":40862,\"start\":40826},{\"end\":40908,\"start\":40872},{\"end\":40961,\"start\":40925},{\"end\":41007,\"start\":40971},{\"end\":41064,\"start\":41028},{\"end\":41114,\"start\":41078},{\"end\":41160,\"start\":41124},{\"end\":41210,\"start\":41174},{\"end\":41544,\"start\":41223},{\"end\":41697,\"start\":41546},{\"end\":42078,\"start\":41737},{\"end\":42445,\"start\":42080},{\"end\":42725,\"start\":42447},{\"end\":42915,\"start\":42727},{\"end\":43251,\"start\":42917},{\"end\":43545,\"start\":43253},{\"end\":43625,\"start\":43565},{\"end\":44839,\"start\":43627},{\"end\":45327,\"start\":44841},{\"end\":45789,\"start\":45329},{\"end\":47350,\"start\":45791},{\"end\":47820,\"start\":47352},{\"end\":48294,\"start\":47822},{\"end\":48363,\"start\":48315},{\"end\":49413,\"start\":48365},{\"end\":50226,\"start\":49415},{\"end\":50654,\"start\":50228},{\"end\":51779,\"start\":50656},{\"end\":52804,\"start\":51781},{\"end\":53646,\"start\":52806},{\"end\":54466,\"start\":53648},{\"end\":54739,\"start\":54511},{\"end\":54934,\"start\":54750},{\"end\":55518,\"start\":54944},{\"end\":55846,\"start\":55532}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":4662,\"start\":4655},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":6343,\"start\":6336},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":7918,\"start\":7911},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":11829,\"start\":11822},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":14900,\"start\":14893},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15790,\"start\":15783},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16883,\"start\":16876},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":18057,\"start\":18050},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":19723,\"start\":19716},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":20536,\"start\":20529},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":22499,\"start\":22492},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":22507,\"start\":22500},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":27017,\"start\":27010},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":28447,\"start\":28440},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":28751,\"start\":28744},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29107,\"start\":29100},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29119,\"start\":29112}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2467,\"start\":2455},{\"attributes\":{\"n\":\"2\"},\"end\":7340,\"start\":7321},{\"attributes\":{\"n\":\"2.1\"},\"end\":8214,\"start\":8180},{\"attributes\":{\"n\":\"2.2\"},\"end\":9326,\"start\":9293},{\"attributes\":{\"n\":\"2.3\"},\"end\":10997,\"start\":10977},{\"attributes\":{\"n\":\"3\"},\"end\":13998,\"start\":13975},{\"attributes\":{\"n\":\"3.1\"},\"end\":14600,\"start\":14572},{\"attributes\":{\"n\":\"3.2\"},\"end\":15355,\"start\":15316},{\"attributes\":{\"n\":\"3.3\"},\"end\":16498,\"start\":16471},{\"attributes\":{\"n\":\"3.4\"},\"end\":17691,\"start\":17662},{\"attributes\":{\"n\":\"3.5\"},\"end\":18897,\"start\":18866},{\"attributes\":{\"n\":\"3.6\"},\"end\":20332,\"start\":20300},{\"attributes\":{\"n\":\"3.7\"},\"end\":22602,\"start\":22568},{\"attributes\":{\"n\":\"4\"},\"end\":23848,\"start\":23823},{\"end\":26298,\"start\":26199},{\"end\":26398,\"start\":26301},{\"end\":26431,\"start\":26401},{\"end\":29492,\"start\":29462},{\"end\":32391,\"start\":32338},{\"end\":34065,\"start\":34022},{\"end\":36187,\"start\":36147},{\"end\":36958,\"start\":36925},{\"end\":37255,\"start\":37204},{\"end\":38996,\"start\":38956},{\"end\":39481,\"start\":39469},{\"end\":39565,\"start\":39560},{\"end\":39660,\"start\":39644},{\"end\":39717,\"start\":39708},{\"end\":39785,\"start\":39780},{\"end\":39847,\"start\":39838},{\"end\":39899,\"start\":39891},{\"end\":39952,\"start\":39947},{\"end\":40012,\"start\":40000},{\"end\":40065,\"start\":40060},{\"end\":40129,\"start\":40113},{\"end\":40186,\"start\":40177},{\"end\":40239,\"start\":40234},{\"end\":40296,\"start\":40287},{\"end\":40357,\"start\":40345},{\"end\":40436,\"start\":40431},{\"end\":40558,\"start\":40542},{\"end\":40608,\"start\":40599},{\"end\":40725,\"start\":40720},{\"end\":40775,\"start\":40766},{\"end\":40824,\"start\":40816},{\"end\":40870,\"start\":40865},{\"end\":40923,\"start\":40911},{\"end\":40969,\"start\":40964},{\"end\":41026,\"start\":41010},{\"end\":41076,\"start\":41067},{\"end\":41122,\"start\":41117},{\"end\":41172,\"start\":41163},{\"end\":41221,\"start\":41213},{\"end\":41735,\"start\":41700},{\"end\":43563,\"start\":43548},{\"end\":48313,\"start\":48297},{\"end\":54501,\"start\":54469},{\"attributes\":{\"n\":\"500\"},\"end\":54509,\"start\":54504},{\"attributes\":{\"n\":\"10000\"},\"end\":54748,\"start\":54742},{\"attributes\":{\"n\":\"500\"},\"end\":54942,\"start\":54937},{\"end\":55530,\"start\":55521},{\"end\":55858,\"start\":55848},{\"end\":56589,\"start\":56584},{\"end\":56920,\"start\":56907},{\"end\":57499,\"start\":57486},{\"end\":57873,\"start\":57860},{\"end\":63138,\"start\":63129},{\"end\":64536,\"start\":64527},{\"end\":64765,\"start\":64756},{\"end\":78974,\"start\":78965},{\"end\":83457,\"start\":83448}]", "table": "[{\"end\":63127,\"start\":60743},{\"end\":64525,\"start\":63140},{\"end\":64754,\"start\":64617},{\"end\":66216,\"start\":65022},{\"end\":67567,\"start\":67069},{\"end\":76246,\"start\":69089},{\"end\":78963,\"start\":77839},{\"end\":83446,\"start\":83018},{\"end\":86087,\"start\":85646}]", "figure_caption": "[{\"end\":56582,\"start\":55860},{\"end\":56871,\"start\":56591},{\"end\":56905,\"start\":56874},{\"end\":57442,\"start\":56922},{\"end\":57484,\"start\":57445},{\"end\":57858,\"start\":57501},{\"end\":58245,\"start\":57875},{\"end\":60743,\"start\":58248},{\"end\":64617,\"start\":64538},{\"end\":64919,\"start\":64767},{\"end\":65022,\"start\":64922},{\"end\":67069,\"start\":66219},{\"end\":69089,\"start\":67570},{\"end\":76502,\"start\":76249},{\"end\":77839,\"start\":76505},{\"end\":83018,\"start\":78976},{\"end\":85646,\"start\":83459}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5095,\"start\":5089},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5373,\"start\":5367},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5640,\"start\":5634},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7509,\"start\":7503},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8403,\"start\":8397},{\"end\":9054,\"start\":9046},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9623,\"start\":9615},{\"end\":11513,\"start\":11507},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":17378,\"start\":17371},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":21561,\"start\":21553},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23285,\"start\":23279},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23819,\"start\":23813},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":24219,\"start\":24213},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26710,\"start\":26704},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26723,\"start\":26715},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27904,\"start\":27898},{\"end\":29458,\"start\":29450},{\"end\":34664,\"start\":34658},{\"end\":37171,\"start\":37165},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38733,\"start\":38725},{\"end\":39145,\"start\":39137},{\"end\":41253,\"start\":41244},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41992,\"start\":41981},{\"end\":42091,\"start\":42083},{\"end\":42458,\"start\":42450},{\"end\":43298,\"start\":43290}]", "bib_author_first_name": "[{\"end\":91994,\"start\":91990},{\"end\":92011,\"start\":92004},{\"end\":92027,\"start\":92020},{\"end\":92043,\"start\":92037},{\"end\":92064,\"start\":92054},{\"end\":92082,\"start\":92074},{\"end\":92097,\"start\":92092},{\"end\":92120,\"start\":92116},{\"end\":92134,\"start\":92128},{\"end\":92495,\"start\":92492},{\"end\":92497,\"start\":92496},{\"end\":92513,\"start\":92505},{\"end\":92524,\"start\":92520},{\"end\":92539,\"start\":92532},{\"end\":92554,\"start\":92549},{\"end\":92571,\"start\":92563},{\"end\":92588,\"start\":92582},{\"end\":93062,\"start\":93055},{\"end\":93079,\"start\":93072},{\"end\":93095,\"start\":93084},{\"end\":93108,\"start\":93104},{\"end\":93120,\"start\":93116},{\"end\":93134,\"start\":93127},{\"end\":93148,\"start\":93143},{\"end\":93621,\"start\":93614},{\"end\":93637,\"start\":93630},{\"end\":93644,\"start\":93642},{\"end\":93654,\"start\":93650},{\"end\":93670,\"start\":93662},{\"end\":93678,\"start\":93675},{\"end\":93693,\"start\":93686},{\"end\":93707,\"start\":93701},{\"end\":93723,\"start\":93716},{\"end\":93738,\"start\":93732},{\"end\":93740,\"start\":93739},{\"end\":93754,\"start\":93751},{\"end\":93767,\"start\":93763},{\"end\":93769,\"start\":93768},{\"end\":94164,\"start\":94160},{\"end\":94195,\"start\":94188},{\"end\":94212,\"start\":94205},{\"end\":94222,\"start\":94218},{\"end\":94234,\"start\":94230},{\"end\":94246,\"start\":94241},{\"end\":94261,\"start\":94255},{\"end\":94277,\"start\":94268},{\"end\":94293,\"start\":94286},{\"end\":94768,\"start\":94762},{\"end\":94779,\"start\":94773},{\"end\":94790,\"start\":94784},{\"end\":94807,\"start\":94801},{\"end\":95047,\"start\":95043},{\"end\":95060,\"start\":95053},{\"end\":95072,\"start\":95066},{\"end\":95084,\"start\":95080},{\"end\":95096,\"start\":95090},{\"end\":95108,\"start\":95103},{\"end\":95118,\"start\":95115},{\"end\":95129,\"start\":95126},{\"end\":95141,\"start\":95134},{\"end\":95153,\"start\":95146},{\"end\":95465,\"start\":95463},{\"end\":95476,\"start\":95473},{\"end\":95486,\"start\":95482},{\"end\":95495,\"start\":95492},{\"end\":95502,\"start\":95500},{\"end\":95514,\"start\":95507},{\"end\":95528,\"start\":95520},{\"end\":95780,\"start\":95773},{\"end\":95794,\"start\":95786},{\"end\":95807,\"start\":95799},{\"end\":95816,\"start\":95812},{\"end\":95820,\"start\":95817},{\"end\":96100,\"start\":96095},{\"end\":96109,\"start\":96106},{\"end\":96124,\"start\":96116},{\"end\":96136,\"start\":96131},{\"end\":96148,\"start\":96141},{\"end\":96396,\"start\":96389},{\"end\":96408,\"start\":96401},{\"end\":96419,\"start\":96413},{\"end\":96429,\"start\":96424},{\"end\":96438,\"start\":96434},{\"end\":96450,\"start\":96444},{\"end\":96464,\"start\":96457},{\"end\":96476,\"start\":96471},{\"end\":96490,\"start\":96481},{\"end\":96500,\"start\":96496},{\"end\":96908,\"start\":96906},{\"end\":96920,\"start\":96913},{\"end\":96935,\"start\":96928},{\"end\":96949,\"start\":96942},{\"end\":96964,\"start\":96956},{\"end\":96976,\"start\":96971},{\"end\":97289,\"start\":97281},{\"end\":97301,\"start\":97295},{\"end\":97312,\"start\":97306},{\"end\":97334,\"start\":97317},{\"end\":97347,\"start\":97342},{\"end\":97362,\"start\":97354},{\"end\":97375,\"start\":97369},{\"end\":97387,\"start\":97380},{\"end\":97400,\"start\":97394},{\"end\":97761,\"start\":97756},{\"end\":97771,\"start\":97766},{\"end\":97779,\"start\":97776},{\"end\":97793,\"start\":97786},{\"end\":97805,\"start\":97800},{\"end\":97809,\"start\":97806},{\"end\":97823,\"start\":97816},{\"end\":98128,\"start\":98122},{\"end\":98141,\"start\":98134},{\"end\":98151,\"start\":98148},{\"end\":98164,\"start\":98156},{\"end\":98174,\"start\":98169},{\"end\":98187,\"start\":98180},{\"end\":98483,\"start\":98476},{\"end\":98494,\"start\":98489},{\"end\":98508,\"start\":98499},{\"end\":98519,\"start\":98513},{\"end\":98531,\"start\":98524},{\"end\":98544,\"start\":98539},{\"end\":98557,\"start\":98551},{\"end\":98570,\"start\":98563},{\"end\":98585,\"start\":98577},{\"end\":98597,\"start\":98590},{\"end\":98938,\"start\":98930},{\"end\":98951,\"start\":98945},{\"end\":98963,\"start\":98956},{\"end\":98975,\"start\":98970},{\"end\":98993,\"start\":98989},{\"end\":99279,\"start\":99273},{\"end\":99289,\"start\":99286},{\"end\":99304,\"start\":99296},{\"end\":99321,\"start\":99310},{\"end\":99339,\"start\":99332},{\"end\":99352,\"start\":99347},{\"end\":99371,\"start\":99361},{\"end\":99732,\"start\":99726},{\"end\":99744,\"start\":99741},{\"end\":99760,\"start\":99751},{\"end\":99774,\"start\":99770},{\"end\":100363,\"start\":100356},{\"end\":100382,\"start\":100376},{\"end\":100394,\"start\":100391},{\"end\":100405,\"start\":100401},{\"end\":100931,\"start\":100925},{\"end\":100945,\"start\":100941},{\"end\":100957,\"start\":100951},{\"end\":100971,\"start\":100965},{\"end\":100986,\"start\":100981},{\"end\":100999,\"start\":100993},{\"end\":101018,\"start\":101009},{\"end\":101031,\"start\":101025},{\"end\":101050,\"start\":101044},{\"end\":101060,\"start\":101055},{\"end\":101062,\"start\":101061},{\"end\":101425,\"start\":101419},{\"end\":101440,\"start\":101433},{\"end\":101455,\"start\":101447},{\"end\":101468,\"start\":101461},{\"end\":101482,\"start\":101474},{\"end\":101496,\"start\":101489},{\"end\":101796,\"start\":101787},{\"end\":101815,\"start\":101808},{\"end\":101829,\"start\":101823},{\"end\":101846,\"start\":101840},{\"end\":101867,\"start\":101860},{\"end\":101882,\"start\":101876},{\"end\":101894,\"start\":101890},{\"end\":101909,\"start\":101904},{\"end\":101922,\"start\":101918},{\"end\":102611,\"start\":102604},{\"end\":102627,\"start\":102621},{\"end\":102644,\"start\":102638},{\"end\":102938,\"start\":102934},{\"end\":102955,\"start\":102952},{\"end\":102965,\"start\":102962},{\"end\":102978,\"start\":102970},{\"end\":102994,\"start\":102987},{\"end\":103009,\"start\":103005},{\"end\":103021,\"start\":103014},{\"end\":103035,\"start\":103029},{\"end\":103052,\"start\":103046},{\"end\":103068,\"start\":103061},{\"end\":103450,\"start\":103446},{\"end\":103558,\"start\":103557},{\"end\":103572,\"start\":103566},{\"end\":103587,\"start\":103581},{\"end\":103598,\"start\":103597},{\"end\":103600,\"start\":103599},{\"end\":103911,\"start\":103900},{\"end\":103928,\"start\":103922},{\"end\":104273,\"start\":104265},{\"end\":104286,\"start\":104280},{\"end\":104298,\"start\":104291},{\"end\":104310,\"start\":104305},{\"end\":104328,\"start\":104324},{\"end\":104605,\"start\":104600},{\"end\":104621,\"start\":104614},{\"end\":104632,\"start\":104631},{\"end\":104634,\"start\":104633},{\"end\":104957,\"start\":104946},{\"end\":104974,\"start\":104968},{\"end\":104991,\"start\":104984},{\"end\":105008,\"start\":105000},{\"end\":105023,\"start\":105018},{\"end\":105045,\"start\":105041},{\"end\":105072,\"start\":105067},{\"end\":105082,\"start\":105073},{\"end\":105600,\"start\":105589},{\"end\":105616,\"start\":105611},{\"end\":105640,\"start\":105632},{\"end\":105656,\"start\":105651},{\"end\":105670,\"start\":105664},{\"end\":105689,\"start\":105681},{\"end\":105703,\"start\":105699},{\"end\":105716,\"start\":105711},{\"end\":105731,\"start\":105726},{\"end\":105742,\"start\":105732},{\"end\":105765,\"start\":105758},{\"end\":105776,\"start\":105770},{\"end\":105793,\"start\":105786},{\"end\":105808,\"start\":105802},{\"end\":106283,\"start\":106277},{\"end\":106299,\"start\":106291},{\"end\":106312,\"start\":106308},{\"end\":106328,\"start\":106325},{\"end\":106337,\"start\":106333},{\"end\":106355,\"start\":106350},{\"end\":106372,\"start\":106366},{\"end\":106390,\"start\":106384},{\"end\":106408,\"start\":106399},{\"end\":106425,\"start\":106416},{\"end\":106443,\"start\":106436},{\"end\":106459,\"start\":106452},{\"end\":106476,\"start\":106468},{\"end\":106492,\"start\":106485},{\"end\":106506,\"start\":106500},{\"end\":106523,\"start\":106514},{\"end\":106533,\"start\":106528},{\"end\":106548,\"start\":106541},{\"end\":106561,\"start\":106557},{\"end\":106577,\"start\":106570},{\"end\":106590,\"start\":106588},{\"end\":106600,\"start\":106595},{\"end\":107286,\"start\":107283},{\"end\":107297,\"start\":107291},{\"end\":107306,\"start\":107304},{\"end\":107317,\"start\":107314},{\"end\":107631,\"start\":107626},{\"end\":107645,\"start\":107638},{\"end\":107659,\"start\":107652},{\"end\":107667,\"start\":107665},{\"end\":107903,\"start\":107897},{\"end\":107919,\"start\":107914},{\"end\":108295,\"start\":108292},{\"end\":108306,\"start\":108303},{\"end\":108320,\"start\":108313},{\"end\":108675,\"start\":108671},{\"end\":108686,\"start\":108680},{\"end\":108699,\"start\":108691},{\"end\":108707,\"start\":108704},{\"end\":108717,\"start\":108713},{\"end\":108729,\"start\":108723},{\"end\":108742,\"start\":108736},{\"end\":108752,\"start\":108747},{\"end\":108767,\"start\":108764},{\"end\":108783,\"start\":108775},{\"end\":108792,\"start\":108789},{\"end\":108806,\"start\":108800},{\"end\":109168,\"start\":109162},{\"end\":109181,\"start\":109174},{\"end\":109191,\"start\":109189},{\"end\":109203,\"start\":109199},{\"end\":109455,\"start\":109447},{\"end\":109471,\"start\":109462},{\"end\":109485,\"start\":109478},{\"end\":109496,\"start\":109490},{\"end\":109506,\"start\":109501},{\"end\":109515,\"start\":109512},{\"end\":109528,\"start\":109521},{\"end\":109536,\"start\":109534},{\"end\":109548,\"start\":109542},{\"end\":109833,\"start\":109831},{\"end\":109844,\"start\":109840},{\"end\":109858,\"start\":109851},{\"end\":109876,\"start\":109871},{\"end\":109897,\"start\":109891},{\"end\":109918,\"start\":109909},{\"end\":109928,\"start\":109919},{\"end\":109939,\"start\":109935},{\"end\":109954,\"start\":109949},{\"end\":109969,\"start\":109964},{\"end\":109988,\"start\":109979},{\"end\":110000,\"start\":109996},{\"end\":110016,\"start\":110013},{\"end\":110034,\"start\":110029},{\"end\":110047,\"start\":110041},{\"end\":110060,\"start\":110054},{\"end\":110076,\"start\":110069},{\"end\":110091,\"start\":110085},{\"end\":110102,\"start\":110097},{\"end\":110122,\"start\":110114},{\"end\":110140,\"start\":110133},{\"end\":110150,\"start\":110146},{\"end\":110936,\"start\":110931},{\"end\":110955,\"start\":110952},{\"end\":110965,\"start\":110960},{\"end\":110983,\"start\":110982},{\"end\":110994,\"start\":110991},{\"end\":111433,\"start\":111429},{\"end\":111452,\"start\":111444},{\"end\":111462,\"start\":111457},{\"end\":111479,\"start\":111470},{\"end\":111481,\"start\":111480},{\"end\":111835,\"start\":111828},{\"end\":111849,\"start\":111841},{\"end\":111862,\"start\":111854},{\"end\":111871,\"start\":111867},{\"end\":111875,\"start\":111872},{\"end\":112110,\"start\":112104},{\"end\":112123,\"start\":112119},{\"end\":112141,\"start\":112133},{\"end\":112160,\"start\":112155},{\"end\":112983,\"start\":112977},{\"end\":112995,\"start\":112988},{\"end\":113013,\"start\":113005},{\"end\":113022,\"start\":113018},{\"end\":113036,\"start\":113031},{\"end\":113053,\"start\":113046},{\"end\":113066,\"start\":113060},{\"end\":113438,\"start\":113437},{\"end\":113686,\"start\":113680},{\"end\":113696,\"start\":113692},{\"end\":113709,\"start\":113704},{\"end\":113725,\"start\":113719},{\"end\":113738,\"start\":113734},{\"end\":113749,\"start\":113745},{\"end\":113764,\"start\":113760},{\"end\":113781,\"start\":113772},{\"end\":113792,\"start\":113788},{\"end\":113805,\"start\":113802},{\"end\":114197,\"start\":114192},{\"end\":114207,\"start\":114204},{\"end\":114220,\"start\":114214},{\"end\":114230,\"start\":114226},{\"end\":114242,\"start\":114236},{\"end\":114255,\"start\":114247},{\"end\":114268,\"start\":114262},{\"end\":114283,\"start\":114276},{\"end\":114293,\"start\":114290},{\"end\":114766,\"start\":114764},{\"end\":114780,\"start\":114774},{\"end\":114792,\"start\":114786},{\"end\":114804,\"start\":114802},{\"end\":114818,\"start\":114811},{\"end\":114828,\"start\":114824},{\"end\":114842,\"start\":114836},{\"end\":114854,\"start\":114847},{\"end\":115279,\"start\":115275},{\"end\":115293,\"start\":115289},{\"end\":115298,\"start\":115294},{\"end\":115309,\"start\":115304},{\"end\":115325,\"start\":115319},{\"end\":115341,\"start\":115334},{\"end\":115355,\"start\":115347},{\"end\":115371,\"start\":115365},{\"end\":115386,\"start\":115380},{\"end\":115401,\"start\":115395},{\"end\":115415,\"start\":115411},{\"end\":115784,\"start\":115778},{\"end\":115799,\"start\":115791},{\"end\":116135,\"start\":116128},{\"end\":116145,\"start\":116143},{\"end\":116158,\"start\":116152},{\"end\":116169,\"start\":116165},{\"end\":116182,\"start\":116175},{\"end\":116199,\"start\":116192},{\"end\":116212,\"start\":116204},{\"end\":116220,\"start\":116217},{\"end\":116231,\"start\":116226},{\"end\":116635,\"start\":116628},{\"end\":116644,\"start\":116641},{\"end\":116656,\"start\":116650},{\"end\":116667,\"start\":116663},{\"end\":116680,\"start\":116674},{\"end\":116693,\"start\":116687},{\"end\":116695,\"start\":116694},{\"end\":116711,\"start\":116704},{\"end\":116726,\"start\":116719},{\"end\":116998,\"start\":116995},{\"end\":117008,\"start\":117005},{\"end\":117022,\"start\":117015},{\"end\":117037,\"start\":117031},{\"end\":117045,\"start\":117042},{\"end\":117052,\"start\":117050},{\"end\":117450,\"start\":117444},{\"end\":117473,\"start\":117466},{\"end\":117486,\"start\":117482},{\"end\":117501,\"start\":117495},{\"end\":117516,\"start\":117511},{\"end\":117533,\"start\":117526},{\"end\":117548,\"start\":117545},{\"end\":117861,\"start\":117857},{\"end\":117873,\"start\":117867},{\"end\":117884,\"start\":117879},{\"end\":117896,\"start\":117892},{\"end\":118218,\"start\":118213},{\"end\":118231,\"start\":118226},{\"end\":118242,\"start\":118237},{\"end\":118252,\"start\":118251}]", "bib_author_last_name": "[{\"end\":92002,\"start\":91995},{\"end\":92018,\"start\":92012},{\"end\":92035,\"start\":92028},{\"end\":92052,\"start\":92044},{\"end\":92072,\"start\":92065},{\"end\":92090,\"start\":92083},{\"end\":92114,\"start\":92098},{\"end\":92126,\"start\":92121},{\"end\":92141,\"start\":92135},{\"end\":92148,\"start\":92143},{\"end\":92503,\"start\":92498},{\"end\":92518,\"start\":92514},{\"end\":92530,\"start\":92525},{\"end\":92547,\"start\":92540},{\"end\":92561,\"start\":92555},{\"end\":92580,\"start\":92572},{\"end\":92600,\"start\":92589},{\"end\":93070,\"start\":93063},{\"end\":93082,\"start\":93080},{\"end\":93102,\"start\":93096},{\"end\":93114,\"start\":93109},{\"end\":93125,\"start\":93121},{\"end\":93141,\"start\":93135},{\"end\":93155,\"start\":93149},{\"end\":93161,\"start\":93157},{\"end\":93628,\"start\":93622},{\"end\":93640,\"start\":93638},{\"end\":93648,\"start\":93645},{\"end\":93660,\"start\":93655},{\"end\":93673,\"start\":93671},{\"end\":93684,\"start\":93679},{\"end\":93699,\"start\":93694},{\"end\":93714,\"start\":93708},{\"end\":93730,\"start\":93724},{\"end\":93749,\"start\":93741},{\"end\":93761,\"start\":93755},{\"end\":93774,\"start\":93770},{\"end\":94040,\"start\":94034},{\"end\":94186,\"start\":94165},{\"end\":94203,\"start\":94196},{\"end\":94216,\"start\":94213},{\"end\":94228,\"start\":94223},{\"end\":94239,\"start\":94235},{\"end\":94253,\"start\":94247},{\"end\":94266,\"start\":94262},{\"end\":94284,\"start\":94278},{\"end\":94302,\"start\":94294},{\"end\":94312,\"start\":94304},{\"end\":94771,\"start\":94769},{\"end\":94782,\"start\":94780},{\"end\":94799,\"start\":94791},{\"end\":94811,\"start\":94808},{\"end\":95051,\"start\":95048},{\"end\":95064,\"start\":95061},{\"end\":95078,\"start\":95073},{\"end\":95088,\"start\":95085},{\"end\":95101,\"start\":95097},{\"end\":95113,\"start\":95109},{\"end\":95124,\"start\":95119},{\"end\":95132,\"start\":95130},{\"end\":95144,\"start\":95142},{\"end\":95157,\"start\":95154},{\"end\":95471,\"start\":95466},{\"end\":95480,\"start\":95477},{\"end\":95490,\"start\":95487},{\"end\":95498,\"start\":95496},{\"end\":95505,\"start\":95503},{\"end\":95518,\"start\":95515},{\"end\":95533,\"start\":95529},{\"end\":95784,\"start\":95781},{\"end\":95797,\"start\":95795},{\"end\":95810,\"start\":95808},{\"end\":95824,\"start\":95821},{\"end\":96104,\"start\":96101},{\"end\":96114,\"start\":96110},{\"end\":96129,\"start\":96125},{\"end\":96139,\"start\":96137},{\"end\":96158,\"start\":96149},{\"end\":96399,\"start\":96397},{\"end\":96411,\"start\":96409},{\"end\":96422,\"start\":96420},{\"end\":96432,\"start\":96430},{\"end\":96442,\"start\":96439},{\"end\":96455,\"start\":96451},{\"end\":96469,\"start\":96465},{\"end\":96479,\"start\":96477},{\"end\":96494,\"start\":96491},{\"end\":96504,\"start\":96501},{\"end\":96911,\"start\":96909},{\"end\":96926,\"start\":96921},{\"end\":96940,\"start\":96936},{\"end\":96954,\"start\":96950},{\"end\":96969,\"start\":96965},{\"end\":96980,\"start\":96977},{\"end\":97293,\"start\":97290},{\"end\":97304,\"start\":97302},{\"end\":97315,\"start\":97313},{\"end\":97340,\"start\":97335},{\"end\":97352,\"start\":97348},{\"end\":97367,\"start\":97363},{\"end\":97378,\"start\":97376},{\"end\":97392,\"start\":97388},{\"end\":97404,\"start\":97401},{\"end\":97764,\"start\":97762},{\"end\":97774,\"start\":97772},{\"end\":97784,\"start\":97780},{\"end\":97798,\"start\":97794},{\"end\":97814,\"start\":97810},{\"end\":97827,\"start\":97824},{\"end\":98132,\"start\":98129},{\"end\":98146,\"start\":98142},{\"end\":98154,\"start\":98152},{\"end\":98167,\"start\":98165},{\"end\":98178,\"start\":98175},{\"end\":98191,\"start\":98188},{\"end\":98487,\"start\":98484},{\"end\":98497,\"start\":98495},{\"end\":98511,\"start\":98509},{\"end\":98522,\"start\":98520},{\"end\":98537,\"start\":98532},{\"end\":98549,\"start\":98545},{\"end\":98561,\"start\":98558},{\"end\":98575,\"start\":98571},{\"end\":98588,\"start\":98586},{\"end\":98601,\"start\":98598},{\"end\":98943,\"start\":98939},{\"end\":98954,\"start\":98952},{\"end\":98968,\"start\":98964},{\"end\":98987,\"start\":98976},{\"end\":98998,\"start\":98994},{\"end\":99284,\"start\":99280},{\"end\":99294,\"start\":99290},{\"end\":99308,\"start\":99305},{\"end\":99330,\"start\":99322},{\"end\":99345,\"start\":99340},{\"end\":99359,\"start\":99353},{\"end\":99379,\"start\":99372},{\"end\":99739,\"start\":99733},{\"end\":99749,\"start\":99745},{\"end\":99768,\"start\":99761},{\"end\":99782,\"start\":99775},{\"end\":100374,\"start\":100364},{\"end\":100389,\"start\":100383},{\"end\":100399,\"start\":100395},{\"end\":100413,\"start\":100406},{\"end\":100939,\"start\":100932},{\"end\":100949,\"start\":100946},{\"end\":100963,\"start\":100958},{\"end\":100979,\"start\":100972},{\"end\":100991,\"start\":100987},{\"end\":101007,\"start\":101000},{\"end\":101023,\"start\":101019},{\"end\":101042,\"start\":101032},{\"end\":101053,\"start\":101051},{\"end\":101069,\"start\":101063},{\"end\":101431,\"start\":101426},{\"end\":101445,\"start\":101441},{\"end\":101459,\"start\":101456},{\"end\":101472,\"start\":101469},{\"end\":101487,\"start\":101483},{\"end\":101500,\"start\":101497},{\"end\":101806,\"start\":101797},{\"end\":101821,\"start\":101816},{\"end\":101838,\"start\":101830},{\"end\":101858,\"start\":101847},{\"end\":101874,\"start\":101868},{\"end\":101888,\"start\":101883},{\"end\":101902,\"start\":101895},{\"end\":101916,\"start\":101910},{\"end\":101934,\"start\":101923},{\"end\":102619,\"start\":102612},{\"end\":102636,\"start\":102628},{\"end\":102649,\"start\":102645},{\"end\":102950,\"start\":102939},{\"end\":102960,\"start\":102956},{\"end\":102968,\"start\":102966},{\"end\":102985,\"start\":102979},{\"end\":103003,\"start\":102995},{\"end\":103012,\"start\":103010},{\"end\":103027,\"start\":103022},{\"end\":103044,\"start\":103036},{\"end\":103059,\"start\":103053},{\"end\":103078,\"start\":103069},{\"end\":103461,\"start\":103451},{\"end\":103564,\"start\":103559},{\"end\":103579,\"start\":103573},{\"end\":103595,\"start\":103588},{\"end\":103610,\"start\":103601},{\"end\":103619,\"start\":103612},{\"end\":103920,\"start\":103912},{\"end\":103938,\"start\":103929},{\"end\":104278,\"start\":104274},{\"end\":104289,\"start\":104287},{\"end\":104303,\"start\":104299},{\"end\":104322,\"start\":104311},{\"end\":104333,\"start\":104329},{\"end\":104612,\"start\":104606},{\"end\":104629,\"start\":104622},{\"end\":104642,\"start\":104635},{\"end\":104966,\"start\":104958},{\"end\":104982,\"start\":104975},{\"end\":104998,\"start\":104992},{\"end\":105016,\"start\":105009},{\"end\":105039,\"start\":105024},{\"end\":105065,\"start\":105046},{\"end\":105086,\"start\":105083},{\"end\":105092,\"start\":105088},{\"end\":105609,\"start\":105601},{\"end\":105630,\"start\":105617},{\"end\":105649,\"start\":105641},{\"end\":105662,\"start\":105657},{\"end\":105679,\"start\":105671},{\"end\":105697,\"start\":105690},{\"end\":105709,\"start\":105704},{\"end\":105724,\"start\":105717},{\"end\":105756,\"start\":105743},{\"end\":105768,\"start\":105766},{\"end\":105784,\"start\":105777},{\"end\":105800,\"start\":105794},{\"end\":105816,\"start\":105809},{\"end\":106289,\"start\":106284},{\"end\":106306,\"start\":106300},{\"end\":106323,\"start\":106313},{\"end\":106331,\"start\":106329},{\"end\":106348,\"start\":106338},{\"end\":106364,\"start\":106356},{\"end\":106382,\"start\":106373},{\"end\":106397,\"start\":106391},{\"end\":106414,\"start\":106409},{\"end\":106434,\"start\":106426},{\"end\":106450,\"start\":106444},{\"end\":106466,\"start\":106460},{\"end\":106483,\"start\":106477},{\"end\":106498,\"start\":106493},{\"end\":106512,\"start\":106507},{\"end\":106526,\"start\":106524},{\"end\":106539,\"start\":106534},{\"end\":106555,\"start\":106549},{\"end\":106568,\"start\":106562},{\"end\":106586,\"start\":106578},{\"end\":106593,\"start\":106591},{\"end\":106606,\"start\":106601},{\"end\":107289,\"start\":107287},{\"end\":107302,\"start\":107298},{\"end\":107312,\"start\":107307},{\"end\":107321,\"start\":107318},{\"end\":107636,\"start\":107632},{\"end\":107650,\"start\":107646},{\"end\":107663,\"start\":107660},{\"end\":107672,\"start\":107668},{\"end\":107912,\"start\":107904},{\"end\":107927,\"start\":107920},{\"end\":108301,\"start\":108296},{\"end\":108311,\"start\":108307},{\"end\":108325,\"start\":108321},{\"end\":108678,\"start\":108676},{\"end\":108689,\"start\":108687},{\"end\":108702,\"start\":108700},{\"end\":108711,\"start\":108708},{\"end\":108721,\"start\":108718},{\"end\":108734,\"start\":108730},{\"end\":108745,\"start\":108743},{\"end\":108762,\"start\":108753},{\"end\":108773,\"start\":108768},{\"end\":108787,\"start\":108784},{\"end\":108798,\"start\":108793},{\"end\":108811,\"start\":108807},{\"end\":109172,\"start\":109169},{\"end\":109187,\"start\":109182},{\"end\":109197,\"start\":109192},{\"end\":109207,\"start\":109204},{\"end\":109460,\"start\":109456},{\"end\":109476,\"start\":109472},{\"end\":109488,\"start\":109486},{\"end\":109499,\"start\":109497},{\"end\":109510,\"start\":109507},{\"end\":109519,\"start\":109516},{\"end\":109532,\"start\":109529},{\"end\":109540,\"start\":109537},{\"end\":109553,\"start\":109549},{\"end\":109838,\"start\":109834},{\"end\":109849,\"start\":109845},{\"end\":109869,\"start\":109859},{\"end\":109889,\"start\":109877},{\"end\":109907,\"start\":109898},{\"end\":109933,\"start\":109929},{\"end\":109947,\"start\":109940},{\"end\":109962,\"start\":109955},{\"end\":109977,\"start\":109970},{\"end\":109994,\"start\":109989},{\"end\":110011,\"start\":110001},{\"end\":110027,\"start\":110017},{\"end\":110039,\"start\":110035},{\"end\":110052,\"start\":110048},{\"end\":110067,\"start\":110061},{\"end\":110083,\"start\":110077},{\"end\":110095,\"start\":110092},{\"end\":110112,\"start\":110103},{\"end\":110131,\"start\":110123},{\"end\":110144,\"start\":110141},{\"end\":110164,\"start\":110151},{\"end\":110169,\"start\":110166},{\"end\":110950,\"start\":110937},{\"end\":110958,\"start\":110956},{\"end\":110970,\"start\":110966},{\"end\":110980,\"start\":110972},{\"end\":110989,\"start\":110984},{\"end\":110999,\"start\":110995},{\"end\":111007,\"start\":111001},{\"end\":111442,\"start\":111434},{\"end\":111455,\"start\":111453},{\"end\":111468,\"start\":111463},{\"end\":111489,\"start\":111482},{\"end\":111839,\"start\":111836},{\"end\":111852,\"start\":111850},{\"end\":111865,\"start\":111863},{\"end\":111879,\"start\":111876},{\"end\":112117,\"start\":112111},{\"end\":112131,\"start\":112124},{\"end\":112153,\"start\":112142},{\"end\":112169,\"start\":112161},{\"end\":112986,\"start\":112984},{\"end\":113003,\"start\":112996},{\"end\":113016,\"start\":113014},{\"end\":113029,\"start\":113023},{\"end\":113044,\"start\":113037},{\"end\":113058,\"start\":113054},{\"end\":113073,\"start\":113067},{\"end\":113444,\"start\":113439},{\"end\":113449,\"start\":113446},{\"end\":113690,\"start\":113687},{\"end\":113702,\"start\":113697},{\"end\":113717,\"start\":113710},{\"end\":113732,\"start\":113726},{\"end\":113743,\"start\":113739},{\"end\":113758,\"start\":113750},{\"end\":113770,\"start\":113765},{\"end\":113786,\"start\":113782},{\"end\":113800,\"start\":113793},{\"end\":113814,\"start\":113806},{\"end\":114202,\"start\":114198},{\"end\":114212,\"start\":114208},{\"end\":114224,\"start\":114221},{\"end\":114234,\"start\":114231},{\"end\":114245,\"start\":114243},{\"end\":114260,\"start\":114256},{\"end\":114274,\"start\":114269},{\"end\":114288,\"start\":114284},{\"end\":114297,\"start\":114294},{\"end\":114302,\"start\":114299},{\"end\":114762,\"start\":114753},{\"end\":114772,\"start\":114767},{\"end\":114784,\"start\":114781},{\"end\":114800,\"start\":114793},{\"end\":114809,\"start\":114805},{\"end\":114822,\"start\":114819},{\"end\":114834,\"start\":114829},{\"end\":114845,\"start\":114843},{\"end\":114859,\"start\":114855},{\"end\":114869,\"start\":114861},{\"end\":115287,\"start\":115280},{\"end\":115302,\"start\":115299},{\"end\":115317,\"start\":115310},{\"end\":115332,\"start\":115326},{\"end\":115345,\"start\":115342},{\"end\":115363,\"start\":115356},{\"end\":115378,\"start\":115372},{\"end\":115393,\"start\":115387},{\"end\":115409,\"start\":115402},{\"end\":115421,\"start\":115416},{\"end\":115789,\"start\":115785},{\"end\":115802,\"start\":115800},{\"end\":116141,\"start\":116136},{\"end\":116150,\"start\":116146},{\"end\":116163,\"start\":116159},{\"end\":116173,\"start\":116170},{\"end\":116190,\"start\":116183},{\"end\":116202,\"start\":116200},{\"end\":116215,\"start\":116213},{\"end\":116224,\"start\":116221},{\"end\":116251,\"start\":116232},{\"end\":116256,\"start\":116253},{\"end\":116639,\"start\":116636},{\"end\":116648,\"start\":116645},{\"end\":116661,\"start\":116657},{\"end\":116672,\"start\":116668},{\"end\":116685,\"start\":116681},{\"end\":116702,\"start\":116696},{\"end\":116717,\"start\":116712},{\"end\":116732,\"start\":116727},{\"end\":117003,\"start\":116999},{\"end\":117013,\"start\":117009},{\"end\":117029,\"start\":117023},{\"end\":117040,\"start\":117038},{\"end\":117048,\"start\":117046},{\"end\":117060,\"start\":117053},{\"end\":117464,\"start\":117451},{\"end\":117480,\"start\":117474},{\"end\":117493,\"start\":117487},{\"end\":117509,\"start\":117502},{\"end\":117524,\"start\":117517},{\"end\":117543,\"start\":117534},{\"end\":117557,\"start\":117549},{\"end\":117865,\"start\":117862},{\"end\":117877,\"start\":117874},{\"end\":117890,\"start\":117885},{\"end\":117902,\"start\":117897},{\"end\":118224,\"start\":118219},{\"end\":118235,\"start\":118232},{\"end\":118249,\"start\":118243},{\"end\":118264,\"start\":118253}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b0\"},\"end\":92488,\"start\":91990},{\"attributes\":{\"id\":\"b1\"},\"end\":93014,\"start\":92490},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":93537,\"start\":93016},{\"attributes\":{\"id\":\"b3\"},\"end\":94030,\"start\":93539},{\"attributes\":{\"id\":\"b4\"},\"end\":94101,\"start\":94032},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":248476411},\"end\":94657,\"start\":94103},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b6\"},\"end\":95041,\"start\":94659},{\"attributes\":{\"doi\":\"arXiv:2304.15010\",\"id\":\"b7\"},\"end\":95461,\"start\":95043},{\"attributes\":{\"doi\":\"arXiv:2305.01278\",\"id\":\"b8\"},\"end\":95771,\"start\":95463},{\"attributes\":{\"doi\":\"arXiv:2304.08485\",\"id\":\"b9\"},\"end\":96005,\"start\":95773},{\"attributes\":{\"doi\":\"arXiv:2304.10592\",\"id\":\"b10\"},\"end\":96387,\"start\":96007},{\"attributes\":{\"doi\":\"arXiv:2304.14178\",\"id\":\"b11\"},\"end\":96841,\"start\":96389},{\"attributes\":{\"doi\":\"arXiv:2305.03726\",\"id\":\"b12\"},\"end\":97193,\"start\":96843},{\"attributes\":{\"doi\":\"arXiv:2305.06500\",\"id\":\"b13\"},\"end\":97689,\"start\":97195},{\"attributes\":{\"doi\":\"arXiv:2305.10355\",\"id\":\"b14\"},\"end\":98039,\"start\":97691},{\"attributes\":{\"doi\":\"arXiv:2305.15028\",\"id\":\"b15\"},\"end\":98417,\"start\":98041},{\"attributes\":{\"doi\":\"arXiv:2305.07895\",\"id\":\"b16\"},\"end\":98862,\"start\":98419},{\"attributes\":{\"doi\":\"arXiv:2305.12223\",\"id\":\"b17\"},\"end\":99207,\"start\":98864},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b18\"},\"end\":99625,\"start\":99209},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":51876975},\"end\":100256,\"start\":99627},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":231951742},\"end\":100833,\"start\":100258},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4492210},\"end\":101417,\"start\":100835},{\"attributes\":{\"id\":\"b22\"},\"end\":101713,\"start\":101419},{\"attributes\":{\"doi\":\"arXiv:2111.02114\",\"id\":\"b23\"},\"end\":102229,\"start\":101715},{\"attributes\":{\"id\":\"b24\"},\"end\":102536,\"start\":102231},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14579301},\"end\":102881,\"start\":102538},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2930547},\"end\":103389,\"start\":102883},{\"attributes\":{\"id\":\"b27\"},\"end\":103540,\"start\":103391},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":383200},\"end\":103834,\"start\":103542},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15193013},\"end\":104197,\"start\":103836},{\"attributes\":{\"doi\":\"arXiv:2305.12223\",\"id\":\"b30\"},\"end\":104542,\"start\":104199},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5728901},\"end\":104905,\"start\":104544},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206777226},\"end\":105545,\"start\":104907},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13322740},\"end\":106275,\"start\":105547},{\"attributes\":{\"id\":\"b34\"},\"end\":107202,\"start\":106277},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":256969669},\"end\":107529,\"start\":107204},{\"attributes\":{\"id\":\"b36\"},\"end\":107826,\"start\":107531},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":250526325},\"end\":108205,\"start\":107828},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":235829490},\"end\":108591,\"start\":108207},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":229923949},\"end\":109089,\"start\":108593},{\"attributes\":{\"doi\":\"abs/2304.10759\",\"id\":\"b40\"},\"end\":109376,\"start\":109091},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":249152323},\"end\":109827,\"start\":109378},{\"attributes\":{\"id\":\"b42\"},\"end\":110873,\"start\":109829},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":238407862},\"end\":111403,\"start\":110875},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":119115657},\"end\":111826,\"start\":111405},{\"attributes\":{\"id\":\"b45\"},\"end\":111997,\"start\":111828},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":223957053},\"end\":112888,\"start\":111999},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":204852201},\"end\":113359,\"start\":112890},{\"attributes\":{\"id\":\"b48\"},\"end\":113587,\"start\":113361},{\"attributes\":{\"doi\":\"arXiv:2204.05862\",\"id\":\"b49\"},\"end\":114118,\"start\":113589},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":253510587},\"end\":114751,\"start\":114120},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b51\"},\"end\":115202,\"start\":114753},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":231591445},\"end\":115776,\"start\":115204},{\"attributes\":{\"doi\":\"arXiv:2304.03277\",\"id\":\"b53\"},\"end\":116054,\"start\":115778},{\"attributes\":{\"doi\":\"arXiv:2302.14045\",\"id\":\"b54\"},\"end\":116535,\"start\":116056},{\"attributes\":{\"id\":\"b55\"},\"end\":116941,\"start\":116537},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":57246310},\"end\":117340,\"start\":116943},{\"attributes\":{\"id\":\"b57\"},\"end\":117785,\"start\":117342},{\"attributes\":{\"doi\":\"arXiv:1901.06706\",\"id\":\"b58\"},\"end\":118093,\"start\":117787},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":3104920},\"end\":118583,\"start\":118095}]", "bib_title": "[{\"end\":93053,\"start\":93016},{\"end\":94158,\"start\":94103},{\"end\":99724,\"start\":99627},{\"end\":100354,\"start\":100258},{\"end\":100923,\"start\":100835},{\"end\":102602,\"start\":102538},{\"end\":102932,\"start\":102883},{\"end\":103555,\"start\":103542},{\"end\":103898,\"start\":103836},{\"end\":104598,\"start\":104544},{\"end\":104944,\"start\":104907},{\"end\":105587,\"start\":105547},{\"end\":107281,\"start\":107204},{\"end\":107895,\"start\":107828},{\"end\":108290,\"start\":108207},{\"end\":108669,\"start\":108593},{\"end\":109445,\"start\":109378},{\"end\":110929,\"start\":110875},{\"end\":111427,\"start\":111405},{\"end\":112102,\"start\":111999},{\"end\":112975,\"start\":112890},{\"end\":113435,\"start\":113361},{\"end\":114190,\"start\":114120},{\"end\":115273,\"start\":115204},{\"end\":116993,\"start\":116943},{\"end\":118211,\"start\":118095}]", "bib_author": "[{\"end\":92004,\"start\":91990},{\"end\":92020,\"start\":92004},{\"end\":92037,\"start\":92020},{\"end\":92054,\"start\":92037},{\"end\":92074,\"start\":92054},{\"end\":92092,\"start\":92074},{\"end\":92116,\"start\":92092},{\"end\":92128,\"start\":92116},{\"end\":92143,\"start\":92128},{\"end\":92150,\"start\":92143},{\"end\":92505,\"start\":92492},{\"end\":92520,\"start\":92505},{\"end\":92532,\"start\":92520},{\"end\":92549,\"start\":92532},{\"end\":92563,\"start\":92549},{\"end\":92582,\"start\":92563},{\"end\":92602,\"start\":92582},{\"end\":93072,\"start\":93055},{\"end\":93084,\"start\":93072},{\"end\":93104,\"start\":93084},{\"end\":93116,\"start\":93104},{\"end\":93127,\"start\":93116},{\"end\":93143,\"start\":93127},{\"end\":93157,\"start\":93143},{\"end\":93163,\"start\":93157},{\"end\":93630,\"start\":93614},{\"end\":93642,\"start\":93630},{\"end\":93650,\"start\":93642},{\"end\":93662,\"start\":93650},{\"end\":93675,\"start\":93662},{\"end\":93686,\"start\":93675},{\"end\":93701,\"start\":93686},{\"end\":93716,\"start\":93701},{\"end\":93732,\"start\":93716},{\"end\":93751,\"start\":93732},{\"end\":93763,\"start\":93751},{\"end\":93776,\"start\":93763},{\"end\":94042,\"start\":94034},{\"end\":94188,\"start\":94160},{\"end\":94205,\"start\":94188},{\"end\":94218,\"start\":94205},{\"end\":94230,\"start\":94218},{\"end\":94241,\"start\":94230},{\"end\":94255,\"start\":94241},{\"end\":94268,\"start\":94255},{\"end\":94286,\"start\":94268},{\"end\":94304,\"start\":94286},{\"end\":94314,\"start\":94304},{\"end\":94773,\"start\":94762},{\"end\":94784,\"start\":94773},{\"end\":94801,\"start\":94784},{\"end\":94813,\"start\":94801},{\"end\":95053,\"start\":95043},{\"end\":95066,\"start\":95053},{\"end\":95080,\"start\":95066},{\"end\":95090,\"start\":95080},{\"end\":95103,\"start\":95090},{\"end\":95115,\"start\":95103},{\"end\":95126,\"start\":95115},{\"end\":95134,\"start\":95126},{\"end\":95146,\"start\":95134},{\"end\":95159,\"start\":95146},{\"end\":95473,\"start\":95463},{\"end\":95482,\"start\":95473},{\"end\":95492,\"start\":95482},{\"end\":95500,\"start\":95492},{\"end\":95507,\"start\":95500},{\"end\":95520,\"start\":95507},{\"end\":95535,\"start\":95520},{\"end\":95786,\"start\":95773},{\"end\":95799,\"start\":95786},{\"end\":95812,\"start\":95799},{\"end\":95826,\"start\":95812},{\"end\":96106,\"start\":96095},{\"end\":96116,\"start\":96106},{\"end\":96131,\"start\":96116},{\"end\":96141,\"start\":96131},{\"end\":96160,\"start\":96141},{\"end\":96401,\"start\":96389},{\"end\":96413,\"start\":96401},{\"end\":96424,\"start\":96413},{\"end\":96434,\"start\":96424},{\"end\":96444,\"start\":96434},{\"end\":96457,\"start\":96444},{\"end\":96471,\"start\":96457},{\"end\":96481,\"start\":96471},{\"end\":96496,\"start\":96481},{\"end\":96506,\"start\":96496},{\"end\":96913,\"start\":96906},{\"end\":96928,\"start\":96913},{\"end\":96942,\"start\":96928},{\"end\":96956,\"start\":96942},{\"end\":96971,\"start\":96956},{\"end\":96982,\"start\":96971},{\"end\":97295,\"start\":97281},{\"end\":97306,\"start\":97295},{\"end\":97317,\"start\":97306},{\"end\":97342,\"start\":97317},{\"end\":97354,\"start\":97342},{\"end\":97369,\"start\":97354},{\"end\":97380,\"start\":97369},{\"end\":97394,\"start\":97380},{\"end\":97406,\"start\":97394},{\"end\":97766,\"start\":97756},{\"end\":97776,\"start\":97766},{\"end\":97786,\"start\":97776},{\"end\":97800,\"start\":97786},{\"end\":97816,\"start\":97800},{\"end\":97829,\"start\":97816},{\"end\":98134,\"start\":98122},{\"end\":98148,\"start\":98134},{\"end\":98156,\"start\":98148},{\"end\":98169,\"start\":98156},{\"end\":98180,\"start\":98169},{\"end\":98193,\"start\":98180},{\"end\":98489,\"start\":98476},{\"end\":98499,\"start\":98489},{\"end\":98513,\"start\":98499},{\"end\":98524,\"start\":98513},{\"end\":98539,\"start\":98524},{\"end\":98551,\"start\":98539},{\"end\":98563,\"start\":98551},{\"end\":98577,\"start\":98563},{\"end\":98590,\"start\":98577},{\"end\":98603,\"start\":98590},{\"end\":98945,\"start\":98930},{\"end\":98956,\"start\":98945},{\"end\":98970,\"start\":98956},{\"end\":98989,\"start\":98970},{\"end\":99000,\"start\":98989},{\"end\":99286,\"start\":99273},{\"end\":99296,\"start\":99286},{\"end\":99310,\"start\":99296},{\"end\":99332,\"start\":99310},{\"end\":99347,\"start\":99332},{\"end\":99361,\"start\":99347},{\"end\":99381,\"start\":99361},{\"end\":99741,\"start\":99726},{\"end\":99751,\"start\":99741},{\"end\":99770,\"start\":99751},{\"end\":99784,\"start\":99770},{\"end\":100376,\"start\":100356},{\"end\":100391,\"start\":100376},{\"end\":100401,\"start\":100391},{\"end\":100415,\"start\":100401},{\"end\":100941,\"start\":100925},{\"end\":100951,\"start\":100941},{\"end\":100965,\"start\":100951},{\"end\":100981,\"start\":100965},{\"end\":100993,\"start\":100981},{\"end\":101009,\"start\":100993},{\"end\":101025,\"start\":101009},{\"end\":101044,\"start\":101025},{\"end\":101055,\"start\":101044},{\"end\":101071,\"start\":101055},{\"end\":101433,\"start\":101419},{\"end\":101447,\"start\":101433},{\"end\":101461,\"start\":101447},{\"end\":101474,\"start\":101461},{\"end\":101489,\"start\":101474},{\"end\":101502,\"start\":101489},{\"end\":101808,\"start\":101787},{\"end\":101823,\"start\":101808},{\"end\":101840,\"start\":101823},{\"end\":101860,\"start\":101840},{\"end\":101876,\"start\":101860},{\"end\":101890,\"start\":101876},{\"end\":101904,\"start\":101890},{\"end\":101918,\"start\":101904},{\"end\":101936,\"start\":101918},{\"end\":102621,\"start\":102604},{\"end\":102638,\"start\":102621},{\"end\":102651,\"start\":102638},{\"end\":102952,\"start\":102934},{\"end\":102962,\"start\":102952},{\"end\":102970,\"start\":102962},{\"end\":102987,\"start\":102970},{\"end\":103005,\"start\":102987},{\"end\":103014,\"start\":103005},{\"end\":103029,\"start\":103014},{\"end\":103046,\"start\":103029},{\"end\":103061,\"start\":103046},{\"end\":103080,\"start\":103061},{\"end\":103463,\"start\":103446},{\"end\":103566,\"start\":103557},{\"end\":103581,\"start\":103566},{\"end\":103597,\"start\":103581},{\"end\":103612,\"start\":103597},{\"end\":103621,\"start\":103612},{\"end\":103922,\"start\":103900},{\"end\":103940,\"start\":103922},{\"end\":104280,\"start\":104265},{\"end\":104291,\"start\":104280},{\"end\":104305,\"start\":104291},{\"end\":104324,\"start\":104305},{\"end\":104335,\"start\":104324},{\"end\":104614,\"start\":104600},{\"end\":104631,\"start\":104614},{\"end\":104644,\"start\":104631},{\"end\":104968,\"start\":104946},{\"end\":104984,\"start\":104968},{\"end\":105000,\"start\":104984},{\"end\":105018,\"start\":105000},{\"end\":105041,\"start\":105018},{\"end\":105067,\"start\":105041},{\"end\":105088,\"start\":105067},{\"end\":105094,\"start\":105088},{\"end\":105611,\"start\":105589},{\"end\":105632,\"start\":105611},{\"end\":105651,\"start\":105632},{\"end\":105664,\"start\":105651},{\"end\":105681,\"start\":105664},{\"end\":105699,\"start\":105681},{\"end\":105711,\"start\":105699},{\"end\":105726,\"start\":105711},{\"end\":105758,\"start\":105726},{\"end\":105770,\"start\":105758},{\"end\":105786,\"start\":105770},{\"end\":105802,\"start\":105786},{\"end\":105818,\"start\":105802},{\"end\":106291,\"start\":106277},{\"end\":106308,\"start\":106291},{\"end\":106325,\"start\":106308},{\"end\":106333,\"start\":106325},{\"end\":106350,\"start\":106333},{\"end\":106366,\"start\":106350},{\"end\":106384,\"start\":106366},{\"end\":106399,\"start\":106384},{\"end\":106416,\"start\":106399},{\"end\":106436,\"start\":106416},{\"end\":106452,\"start\":106436},{\"end\":106468,\"start\":106452},{\"end\":106485,\"start\":106468},{\"end\":106500,\"start\":106485},{\"end\":106514,\"start\":106500},{\"end\":106528,\"start\":106514},{\"end\":106541,\"start\":106528},{\"end\":106557,\"start\":106541},{\"end\":106570,\"start\":106557},{\"end\":106588,\"start\":106570},{\"end\":106595,\"start\":106588},{\"end\":106608,\"start\":106595},{\"end\":107291,\"start\":107283},{\"end\":107304,\"start\":107291},{\"end\":107314,\"start\":107304},{\"end\":107323,\"start\":107314},{\"end\":107638,\"start\":107626},{\"end\":107652,\"start\":107638},{\"end\":107665,\"start\":107652},{\"end\":107674,\"start\":107665},{\"end\":107914,\"start\":107897},{\"end\":107929,\"start\":107914},{\"end\":108303,\"start\":108292},{\"end\":108313,\"start\":108303},{\"end\":108327,\"start\":108313},{\"end\":108680,\"start\":108671},{\"end\":108691,\"start\":108680},{\"end\":108704,\"start\":108691},{\"end\":108713,\"start\":108704},{\"end\":108723,\"start\":108713},{\"end\":108736,\"start\":108723},{\"end\":108747,\"start\":108736},{\"end\":108764,\"start\":108747},{\"end\":108775,\"start\":108764},{\"end\":108789,\"start\":108775},{\"end\":108800,\"start\":108789},{\"end\":108813,\"start\":108800},{\"end\":109174,\"start\":109162},{\"end\":109189,\"start\":109174},{\"end\":109199,\"start\":109189},{\"end\":109209,\"start\":109199},{\"end\":109462,\"start\":109447},{\"end\":109478,\"start\":109462},{\"end\":109490,\"start\":109478},{\"end\":109501,\"start\":109490},{\"end\":109512,\"start\":109501},{\"end\":109521,\"start\":109512},{\"end\":109534,\"start\":109521},{\"end\":109542,\"start\":109534},{\"end\":109555,\"start\":109542},{\"end\":109840,\"start\":109831},{\"end\":109851,\"start\":109840},{\"end\":109871,\"start\":109851},{\"end\":109891,\"start\":109871},{\"end\":109909,\"start\":109891},{\"end\":109935,\"start\":109909},{\"end\":109949,\"start\":109935},{\"end\":109964,\"start\":109949},{\"end\":109979,\"start\":109964},{\"end\":109996,\"start\":109979},{\"end\":110013,\"start\":109996},{\"end\":110029,\"start\":110013},{\"end\":110041,\"start\":110029},{\"end\":110054,\"start\":110041},{\"end\":110069,\"start\":110054},{\"end\":110085,\"start\":110069},{\"end\":110097,\"start\":110085},{\"end\":110114,\"start\":110097},{\"end\":110133,\"start\":110114},{\"end\":110146,\"start\":110133},{\"end\":110166,\"start\":110146},{\"end\":110171,\"start\":110166},{\"end\":110952,\"start\":110931},{\"end\":110960,\"start\":110952},{\"end\":110972,\"start\":110960},{\"end\":110982,\"start\":110972},{\"end\":110991,\"start\":110982},{\"end\":111001,\"start\":110991},{\"end\":111009,\"start\":111001},{\"end\":111444,\"start\":111429},{\"end\":111457,\"start\":111444},{\"end\":111470,\"start\":111457},{\"end\":111491,\"start\":111470},{\"end\":111841,\"start\":111828},{\"end\":111854,\"start\":111841},{\"end\":111867,\"start\":111854},{\"end\":111881,\"start\":111867},{\"end\":112119,\"start\":112104},{\"end\":112133,\"start\":112119},{\"end\":112155,\"start\":112133},{\"end\":112171,\"start\":112155},{\"end\":112988,\"start\":112977},{\"end\":113005,\"start\":112988},{\"end\":113018,\"start\":113005},{\"end\":113031,\"start\":113018},{\"end\":113046,\"start\":113031},{\"end\":113060,\"start\":113046},{\"end\":113075,\"start\":113060},{\"end\":113446,\"start\":113437},{\"end\":113451,\"start\":113446},{\"end\":113692,\"start\":113680},{\"end\":113704,\"start\":113692},{\"end\":113719,\"start\":113704},{\"end\":113734,\"start\":113719},{\"end\":113745,\"start\":113734},{\"end\":113760,\"start\":113745},{\"end\":113772,\"start\":113760},{\"end\":113788,\"start\":113772},{\"end\":113802,\"start\":113788},{\"end\":113816,\"start\":113802},{\"end\":114204,\"start\":114192},{\"end\":114214,\"start\":114204},{\"end\":114226,\"start\":114214},{\"end\":114236,\"start\":114226},{\"end\":114247,\"start\":114236},{\"end\":114262,\"start\":114247},{\"end\":114276,\"start\":114262},{\"end\":114290,\"start\":114276},{\"end\":114299,\"start\":114290},{\"end\":114304,\"start\":114299},{\"end\":114764,\"start\":114753},{\"end\":114774,\"start\":114764},{\"end\":114786,\"start\":114774},{\"end\":114802,\"start\":114786},{\"end\":114811,\"start\":114802},{\"end\":114824,\"start\":114811},{\"end\":114836,\"start\":114824},{\"end\":114847,\"start\":114836},{\"end\":114861,\"start\":114847},{\"end\":114871,\"start\":114861},{\"end\":115289,\"start\":115275},{\"end\":115304,\"start\":115289},{\"end\":115319,\"start\":115304},{\"end\":115334,\"start\":115319},{\"end\":115347,\"start\":115334},{\"end\":115365,\"start\":115347},{\"end\":115380,\"start\":115365},{\"end\":115395,\"start\":115380},{\"end\":115411,\"start\":115395},{\"end\":115423,\"start\":115411},{\"end\":115791,\"start\":115778},{\"end\":115804,\"start\":115791},{\"end\":116143,\"start\":116128},{\"end\":116152,\"start\":116143},{\"end\":116165,\"start\":116152},{\"end\":116175,\"start\":116165},{\"end\":116192,\"start\":116175},{\"end\":116204,\"start\":116192},{\"end\":116217,\"start\":116204},{\"end\":116226,\"start\":116217},{\"end\":116253,\"start\":116226},{\"end\":116258,\"start\":116253},{\"end\":116641,\"start\":116628},{\"end\":116650,\"start\":116641},{\"end\":116663,\"start\":116650},{\"end\":116674,\"start\":116663},{\"end\":116687,\"start\":116674},{\"end\":116704,\"start\":116687},{\"end\":116719,\"start\":116704},{\"end\":116734,\"start\":116719},{\"end\":117005,\"start\":116995},{\"end\":117015,\"start\":117005},{\"end\":117031,\"start\":117015},{\"end\":117042,\"start\":117031},{\"end\":117050,\"start\":117042},{\"end\":117062,\"start\":117050},{\"end\":117466,\"start\":117444},{\"end\":117482,\"start\":117466},{\"end\":117495,\"start\":117482},{\"end\":117511,\"start\":117495},{\"end\":117526,\"start\":117511},{\"end\":117545,\"start\":117526},{\"end\":117559,\"start\":117545},{\"end\":117867,\"start\":117857},{\"end\":117879,\"start\":117867},{\"end\":117892,\"start\":117879},{\"end\":117904,\"start\":117892},{\"end\":118226,\"start\":118213},{\"end\":118237,\"start\":118226},{\"end\":118251,\"start\":118237},{\"end\":118266,\"start\":118251}]", "bib_venue": "[{\"end\":93273,\"start\":93197},{\"end\":99945,\"start\":99873},{\"end\":100564,\"start\":100498},{\"end\":105210,\"start\":105162},{\"end\":106736,\"start\":106706},{\"end\":107973,\"start\":107969},{\"end\":111158,\"start\":111092},{\"end\":111632,\"start\":111570},{\"end\":112448,\"start\":112315},{\"end\":114453,\"start\":114387},{\"end\":92211,\"start\":92166},{\"end\":93195,\"start\":93163},{\"end\":93612,\"start\":93539},{\"end\":94363,\"start\":94314},{\"end\":94760,\"start\":94659},{\"end\":95219,\"start\":95175},{\"end\":95595,\"start\":95551},{\"end\":95867,\"start\":95842},{\"end\":96093,\"start\":96007},{\"end\":96586,\"start\":96522},{\"end\":96904,\"start\":96843},{\"end\":97279,\"start\":97195},{\"end\":97754,\"start\":97691},{\"end\":98120,\"start\":98041},{\"end\":98474,\"start\":98419},{\"end\":98928,\"start\":98864},{\"end\":99271,\"start\":99209},{\"end\":99871,\"start\":99784},{\"end\":100496,\"start\":100415},{\"end\":101111,\"start\":101071},{\"end\":101536,\"start\":101502},{\"end\":101785,\"start\":101715},{\"end\":102282,\"start\":102231},{\"end\":102700,\"start\":102651},{\"end\":103120,\"start\":103080},{\"end\":103444,\"start\":103391},{\"end\":103679,\"start\":103621},{\"end\":104007,\"start\":103940},{\"end\":104263,\"start\":104199},{\"end\":104707,\"start\":104644},{\"end\":105160,\"start\":105094},{\"end\":105892,\"start\":105818},{\"end\":106704,\"start\":106608},{\"end\":107354,\"start\":107323},{\"end\":107624,\"start\":107531},{\"end\":107967,\"start\":107929},{\"end\":108391,\"start\":108327},{\"end\":108828,\"start\":108813},{\"end\":109160,\"start\":109091},{\"end\":109596,\"start\":109555},{\"end\":110189,\"start\":110171},{\"end\":111090,\"start\":111009},{\"end\":111568,\"start\":111491},{\"end\":111906,\"start\":111881},{\"end\":112313,\"start\":112171},{\"end\":113103,\"start\":113075},{\"end\":113461,\"start\":113451},{\"end\":113678,\"start\":113589},{\"end\":114385,\"start\":114304},{\"end\":114958,\"start\":114887},{\"end\":115467,\"start\":115423},{\"end\":115896,\"start\":115820},{\"end\":116126,\"start\":116056},{\"end\":116626,\"start\":116537},{\"end\":117125,\"start\":117062},{\"end\":117442,\"start\":117342},{\"end\":117855,\"start\":117787},{\"end\":118327,\"start\":118266}]"}}}, "year": 2023, "month": 12, "day": 17}