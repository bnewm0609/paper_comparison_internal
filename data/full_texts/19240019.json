{"id": 19240019, "updated": "2023-10-22 13:34:30.681", "metadata": {"title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph", "authors": "[{\"first\":\"Amrita\",\"last\":\"Saha\",\"middle\":[]},{\"first\":\"Vardaan\",\"last\":\"Pahuja\",\"middle\":[]},{\"first\":\"Mitesh\",\"last\":\"Khapra\",\"middle\":[\"M.\"]},{\"first\":\"Karthik\",\"last\":\"Sankaranarayanan\",\"middle\":[]},{\"first\":\"Sarath\",\"last\":\"Chandar\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2018, "month": 1, "day": 31}, "abstract": "While conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1801.10314", "mag": "2963376744", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/SahaPKSC18", "doi": "10.1609/aaai.v32i1.11332"}}, "content": {"source": {"pdf_hash": "2cea3b200f5ad6c643fdd3f8e11cb986328e2309", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1801.10314v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e6d14bc7eeafa0720e3657215f789f742fa1b7bf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2cea3b200f5ad6c643fdd3f8e11cb986328e2309.txt", "contents": "\nComplex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph\n\n\nAmrita Saha amrsaha4@in.ibm.com \nIBM Research AI\n\n\nI.I.T. Madras\nIndia\n\nVardaan Pahuja vardaanpahuja@gmail.com \nMILA\nUniversit\u00e9 de Montr\u00e9al\n\n\nMitesh M Khapra miteshk@cse.iitm.ac.in \nI.I.T. Madras\nIndia\n\nKarthik Sankaranarayanan \nIBM Research AI\n\n\nSarath Chandar apsarathchandar@gmail.com \nMILA\nUniversit\u00e9 de Montr\u00e9al\n\n\nComplex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph\n\nWhile conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.\n\nIntroduction\n\nIn recent years there has been an increased demand for AI driven personal assistants which are capable of conversing coherently with humans. Such personal assistants could benefit from large scale knowledge graphs which contain millions of facts stored as tuples of the form {predicate, subject, object} (for example, {director, Titanic, James Cameron}).\n\nSuch knowledge graphs can indeed be handy when the bot is used in specific domains such as education, entertainment, sports, etc. where it is often required to answer factual questions while being aware of the context of the conversation. While Question Answering (Voorhees and Tice 2000;Wang, Smith, and Mitamura 2007;Yang, Yih, and Meek 2015;Berant et al. 2013;Bordes et al. 2015;Rajpurkar et al. 2016;Nguyen et al. 2016;Onishi et al. 2016;Richardson, Burges, and Renshaw 2013; and Conversation Systems (Ritter, Cherry, and Dolan 2010;Lowe et al. 2015;Banchs 2012;Bordes and Weston 2016) have received a lot of attention in the recent past, we would like to focus on such real life settings encountered by chatbots which involve a combination of QA and dialog. Specifically, we are interested in building systems which can learn to converse over a series of coherently linked questions that can be answered from a large scale knowledge graph. We refer to this task as Complex Sequential Question Answering (CSQA).\n\nNeedless to say, CSQA is very different from the kind of conversations found in existing dialog datasets such as the Twitter (Ritter, Cherry, and Dolan 2010), Ubuntu (Lowe et al. 2015) and Movie Subtitles (Banchs 2012) datasets. Table 1 shows an example of one such conversation from our dataset containing a series of questions. Note that to answer the question in Turn 11, the bot needs to remember that the question involves the same predicate ('diplomatically related') as the previous question, but with a different subject ('Australia'). In other words, it is difficult to answer this question without retaining the context of the conversation. Further, in a natural conversation, some of the questions may require co-reference resolution (as in Turn 2), ellipsis resolution (as in Turn 11), etc. Finally, in some cases the question could be ambiguous (as in Turn 2) in which case the bot needs to ask for clarifications keeping in mind other entities and relations which were previously mentioned in the conversation.\n\nWhile the example in Table 1 already highlights some of the challenges involved in CSQA, we now discuss an orthogonal set of challenges which arise from the complexity of the questions. Existing datasets for Factual QA (Berant et al. 2013;Bordes et al. 2015;Yang, Yih, and Meek 2015)   a single tuple in the KG. However, in a real-life setting, a bot could encounter more complex questions requiring logical, quantitative and comparative reasoning. Table 2 shows some examples of such questions. It should be clear that unlike simple questions, which can be answered from a single tuple, these questions require a larger subgraph of the KG. For example, to answer the question \"Which rivers flow through India and China ?\" one needs to find (i) the set of rivers flowing through India (ii) the set of rivers flowing through China and finally (iii) the intersection between these two sets. Answering such questions requires models which can parse complex natural language questions, retrieve relevant subgraphs of the KG and then perform some logical, comparative and/or quantitative operations on this subgraph. Also the Knowledge Graph used in our work is orders of magnitude larger than those used in some existing works Bordes et al. 2015;Dodge et al. 2015;Fader, Soderland, and Etzioni 2011) which lie at the intersection of QA and dialog.\n\nHaving motivated the task of CSQA and highlighted its differences from existing work on dialog and QA, we now briefly describe the process used for creating our dataset. As mentioned earlier, a KG contains tuples of the form {predicate, subject, object}. For each of the 330 predicates in our Knowledge Graph, we first asked workers on Amazon Mechanical Turk to create questions containing the predicate and the subject (or object) such that the answer to that question is the object (or subject). These questions are complete in the sense that they do not have any ambiguity and can be answered in isolation without requiring any additional context. We then ask in-house annotators to create multiple templates for generating a conversation comprising of connected question answer pairs. Two question answer pairs are said to be connected if they contain the same predicate, subject or object. We then ask workers to make modifications to these questions so as to introduce challenges like co-references, ellipsis, incompleteness (or under specification) and contextual dependence. We also solicit templates and modifications to add logical, comparative and quantitative operators to the questions obtained above. This results in a dataset which contains conversations of the form shown in Table 1.\n\nThe objective of this work is twofold: (i) to introduce the task of Complex Sequential QA and (ii) to show the inadequacy of current state of the art QA and dialog methods to deal with such tasks. Towards the second objective, we propose a model for CSQA which is a cross between a state of the art hierarchical conversation model (Serban et al. 2016a) and a key value based memory network model for QA (Miller et al. 2016). Through our experiments, we demonstrate the inadequacy of these models and highlight specific challenges that need to be addressed. It is also worth mentioning that the unambiguous (context independent) questions which appear in our dataset (typically, at the start of the conversation) can also be used for studying Complex Question Answering (as opposed to Simple Question Answering) in isolation ignoring the dialog context. This will help to independently push the state of the art in Complex QA.\n\n\nRelated Work\n\nOur work lies at the intersection of Question Answering and Dialog Systems. Question Answering has always been of interest to the research community starting from early TREC evaluations (Voorhees and Tice 2000) . Over the years various datasets and tasks have been introduced to advance the state of the art in QA. These datasets can be divided into 5 main types (i) TREC style Open Domain QA (Voorhees and Tice 2000;Wang, Smith, and Mitamura 2007;Yang, Yih, and Meek 2015) where the aim is to answer a question from a collection of documents (ii) factoid QA over structured knowledge graphs (Berant et al. 2013;Bordes et al. 2015;Serban et al. 2016b) (iii) reading comprehension style QA (Rajpurkar et al. 2016;Nguyen et al. 2016), (iv) cloze style QA (Mostafazadeh et al. 2016;Onishi et al. 2016) (v) multiple choice QA (Richardson, Burges, and Renshaw 2013; Of the above QA tasks, factoid QA is most relevant to us as the questions in our CSQA dataset are factoid questions. Existing factoid QA datasets contain Simple Questions which can be answered from a single tuple in the knowledge graph. Specifically, unlike our dataset, none of the existing datasets contain complex questions requiring logical, quantitative and comparative reasoning involving larger subgraphs of the KG as opposed to a single tuple. Solutions to the simple QA task range from semantic parsing based methods (Berant and Liang 2014;Fader, Zettlemoyer, and Etzioni 2014) to embedding based methods Bordes, Chopra, and Weston 2014;Yang et al. 2014) and state of the art Memory Networks based architectures Miller et al. 2016;Kumar et al. 2016). In this work, we experiment with Memory Network based architectures and make a case for the need of better architectures when going beyond simple questions.\n\nSince we are interested in CSQA which contains a series of QA pairs over a coherent conversation, we also review some related work on dialog systems. Over the past few years three large scale dialog datasets, viz., Twitter-Conversations (Ritter, Cherry, and Dolan 2010), Ubuntu Dialogue  and Movie-Dic Corpus (Banchs 2012) have become very popular. However, none of these datasets have the flavor of CSQA and there is no explicit Knowledge Graph associated with the conversations. Here again, neural network based (hierarchical) sequence to sequence methods (Luong et al. 2015;Serban et al. 2016a;Serban et al. 2017) have become the de facto choice. Recently, (Bordes and Weston 2016) proposed a dataset which contains knowledge graph driven goal oriented dialogs for the task of restaurant reservation. However, the size of the KG here is very small (<10 cuisines, locations, ambience, etc.) and the dialog contains very few states. (Dodge et al. 2015) also uses a dataset for QA and recommendation but unlike our dataset, their dataset does not contain coherently linked question answer pairs. Further, the KG is again much smaller (75K entities and 11 relations). Recently (Neelakantan et al. 2016) have explored complex question answering over around 18.5K queries from the Wik-iTableQuestions dataset, but their tables have less than 100 rows and a handful of columns whereas our complex QAs are grounded in a KB of over 20 million tuples. Further, their dataset does not have a conversational aspect.\n\n\nDataset Creation\n\nOur aim is to create a dataset which contains a series of linked QA pairs forming a coherent conversation. Further, these questions should be answerable from a Knowledge Graph using logical, comparative and/or quantitative reasoning. We started by asking pairs of in-house annotators to converse with each other. One annotator in the pair acted as a user whose job was to ask questions and the other annotator acted as the system whose job was to answer the questions or ask for clarifications if required. Note that these annotators were Computer Science graduates who understood the concepts of knowledge graph, sub graph, tuples, subject, object, relation, etc. The idea was to use the in-house annotators to understand the types of simple and complex questions that can be asked over a knowledge graph. These could then be abstracted to templates and used to instantiate more questions involving different relations, subjects and objects. Similarly, we also wanted to understand the type of coreferences, ellipses etc used by users when asking linked questions over a coherent conversation. These could again be abstracted to templates and used to link individual QA pairs to form a coherent dialog. In the remainder of this section we describe (i) the knowledge graph supporting our CSQA (ii) simple question templates suggested by the in-house annotators (iii) complex question templates and finally (iv) the linked conversation templates and the process used to instantiate around 200K dialogs containing 1.6 million linked QA pairs.\n\n\nKnowledge Graph\n\nAs our KG, we used wikidata which stores facts in the form of tuples containing a relation, a subject and an object. For example, (rel: capital, subj: India, obj: New Delhi) is a tuple in wikidata. Each entity (subject or object) is associated with an entity type. For example, in the above tuple, India is an entity of type country and New Delhi is an entity of type city. We use the wikidata dump of 14-Nov-2016 which contains 5.2K relations, 12.8M entities and 52.3M facts. Of these 5.2K relations, we retain only 330 meaningful ones. Specifically, we ignore relations such as \"ISO 3166-1 alpha-2 code\", \"NDL Auth ID\" etc., as we do not expect users to ask questions about such obscure relations. Similarly, of the 30.8K unique entity types in wikidata, we selected 642 types (considering only immediate parents of entities) which appeared in the top 90 percentile of the tuples associated with atleast one of the retained meaningful relations. In effect, there were around 21.2M such tuples containing only the filtered relations and entity types. The total number of unique entities in these filtered tuples is 12.8M out of which 3.8M appear in atleast 3 tuples.\n\n\nSimple Questions\n\nFor discovering simple question templates, we asked the annotators to come up with questions which can be answered from a single tuple in the knowledge graph. The annotators suggested that for a given tuple (say, rel: CEO, subj: Google, obj: Sundar Pichai) there are mainly 3 types of simple questions that can be generated:\n\n\nObject based questions:\n\nHere the question contains the relation and the subject from a tuple and the answer is the tuple's object. For example, \"Q: Who is the CEO (relation) of Google (subject) ? A: Sundar Pichai (object)\".\n\n2. Subject based questions: Here the question contains the relation and the object from a tuple and the answer is the tuple's subject. For example, \"Q: Which company is Sundar Pichai (object) the CEO of (relation) ? A: Google (subject)\".\n\n3. Relation based questions: Here the question contains the subject and the object from a tuple and the answer is the tuple's relation. For example, \"Q: How is Sundar Pichai (object) related to Google (subject) ? A: CEO (relation)\". During our discussions, we found that in many cases, relation based questions do not make a lot of sense. For example, it is unnatural for someone to ask the question \"Q: How is Himalayas related to India? A: located in\". Hence, in this work we focus only on object based and subject based questions.\n\nNote that in some cases the question could have multiple correct answers. In other words, there are multiple tuples related to this question. For example, \"Q: Which rivers flow through India ? A: Ganga, Yamuna, Narmada, ....\". Note that even though these questions can be answered from multiple tuples, they are still simple questions because they do not require any joint reasoning over multiple tuples.  Crowdsourced question generation: Based on this initial pilot with in-house annotators we then requested workers on AMT to create subject based and object based questions for each of the 330 relations in our KG. For creating subject based questions the annotators were shown (i) the object, (ii) the relation (iii) the type of the subject associated with that tuple and (iv) a few sample tuples. Note that the subject type is important as the annotator will need to look at the subject type (city) to form the question \"Which city is the capital of India ?\". This is important because some relations (for example, the relation tributary) can have multiple subject and object types as shown below:\n\n1. subj: Spring Creek (type: river), obj: Lake Ilsanjo (type: lake)\n\n2. subj: Spring Creek (type: river), obj: Matanzas Creek (type: stream)\n\nIt should be obvious that even for the same relation different combinations of subjects and objects should result in different questions. For example, \"Which lake is a tributary of Spring Creek?\" v/s \"Which river is a tributary of Spring Creek?\". Note that, on an average each relation in our KG was associated with 5 subject types and 6 object types. We first asked a set of workers to create one subject based and object based question for each relation. We then asked a separate set of annotators to create paraphrases of these questions. In all, we collected 1531 subject based and 1450 object based question templates (including paraphrases) through this process.\n\nOnce we get a template we can instantiate it with different entity types and entities to create many questions. For example, given the template \"Which <water course> is located in <country> ?\" we can instantiate it by replacing water course by it's sub-types (river, lake, etc) and by replacing country by entities of that type (U.S., India, etc.). This gives us a semi-automatic way of creating many questions from the collected templates. Note that the question templates also contain paraphrases, so we have different ways of asking the same question.\n\n\nComplex Questions\n\nNext we wanted the annotators to help us identify types of questions which require logical, comparative and quantitative reasoning over a larger subgraph of the KG.\n\nLogical Reasoning: These are questions which require some logical inferencing over multiple tuples in the KG. For example, consider the question \"Which rivers flow through India and China ?\" To answer this question we first need to create two sets (i) a set A containing rivers appearing in tuples of the form (flows through, India, river) and (ii) a set B containing rivers appearing in tuples of the form (flows through, China, river). The final answer to the question is then an intersection of these two sets. It should be obvious that answering such questions is more difficult then the Simple Questions studied in literature so far (and as described in the previous section).\n\nThe annotators came up with questions involving different logical operators such as AND, OR, NOT, etc (see Table 2). They also suggested some templates for creating such logical reasoning questions from the simple questions that we had already collected (as described in Section 3.1). For example, one such template was to take a simple object based question such as \"Which rivers flow through India\" and augment it with another subject such as \"and China\". Similar templates and paraphrases were suggested for other operators such as OR, NOT, etc. for both subject based and object based questions. This allowed us to semi-automatically create many questions requiring logical reasoning. This process is semiautomatic because once a template is created, we instantiate it for multiple tuples (as explained earlier) and then manually verify a subset of these questions to check whether they are syntactically and semantically correct.\n\nNote that some of the logical reasoning questions suggested by the annotators contained multiple relations. For example, the question \"Which river flows through India and has its source in Himalayas?\" requires a logical operation over two relations, viz., flows through and source.\n\nQuantitative Reasoning: These questions require some quantitative reasoning involving standard aggregation functions like max, min, count, atleast / atmost / approximately / equal to N , etc. We refer the reader to Table 2 to see examples of different types of quantitative questions. Once again, with the help of in-house annotators we identified several templates for modifying the simple questions that we had already collected and creating quantitative reasoning questions involving different aggregation operators. For example, one such template was to take the object based question \"Which rivers flow through India\" and replace \"Which\" by \"How many\". In fact, we found this particular template to be so convenient that for every relation, we asked the workers to give us at least one simple question which starts with \"Which subject-type ... \". Some of these simple questions starting with \"Which subject-type ... \" look a bit unnatural but we made a conscious choice to allow this so that it simplifies the process of creating complex questions. We also created questions which require quantitative reasoning on top of logical reasoning. For example, \"How many rivers flow through India but not through China ?\".\n\nComparative Reasoning: These are questions which require a comparison between entities based on certain relations (predicates). For example, consider the question \"Which countries have more number of rivers than India ?\". This requires inference over multiple tuples in the KG. The model here essentially needs to learn the count, sort and more/less operations. Such questions could also involve multiple entity types. For example the question \"Which countries have more lakes and rivers than India ?\" involves two entity types (lakes, rivers). Finally, we could have questions which require a counting type quantitative reasoning on top of comparative reasoning. For example, \"How many countries have more rivers than India ?\" requires counting after comparing. These questions were created by modifying the simple questions, using the rules of transformation given by our annotators.\n\nNote that in all of the above cases, once the annotator suggests a modification, we can apply that modification and its paraphrases to multiple tuples to get many questions. Further, after instantiating we retain only those Qs which have less than 1000 answers.\n\nLinked Sequential QA So far we have described the process of collecting individual QA pairs containing various types of questions. We are now interested in creating coherent conversations involving such QA pairs. We can think of such a conversation as a walk over the Knowledge Graph using QA pairs such that subsequent questions refer to subjects, objects or relations which have appeared previously in the conversation. More specifically, such conversations should have the following properties (i) subsequent QA pairs should be linked and (ii) the conversation should contain typical elements of a dialog such as coreferences, ellipses, clarifications, confirmation, etc.\n\nThe process of connecting linked QA pairs in a coherent conversation can be thought of as performing a systematic walk over a Knowledge Graph. Simply stated, two questions can be placed next to each other in a conversation if they share a relation or an entity. However, bringing in factors such as ambiguity, underspecified or coreferenced questions into the conversation requires manual effort. For this, we again requested in-house annotators to create templates for converting simple or complex questions described above into conversational questions. For example, one such template was to take a simple question such as \"Which rivers flow through India ?\" and replace the subject by \"that subject-type\" or \"that country\" in this case. Multiple such templates were created and refined for different question types that we described in the earlier sections. This was a labor intensive tedious process requiring several iterations. Some templates were also collected using crowdsourcing on AMT. We refer to such questions as Indirect questions as opposed to Direct questions which are fully specified and do not indirectly refer to some entity or relation from the earlier conversation. The in-house annotators also suggested some clarification templates which involved asking questions containing coreferences which could resolve to more than one of the previously mentioned entities. Turn 2 in Table 1 shows one such example. The information in this question is not enough to answer the question and hence the system needs to ask for a clarification. Note that, whenever we use linking we only link consecutive questions and not arbitrary questions in the sequence (i.e., the i-th question can be linked to the next pr previous question but not to arbitrary questions appearing before or after it.)\n\nThrough the above processing involving a mix of manual work (crowdsourced and inhouse) and semi-automatic instantiation, we created a dataset containing 200 K dialogs and a total of 1.6 M turns. Table 2 shows the number of templates for each question type and some sample types. Table 3 shows various statistics about the dataset including the Train, Validation and Test splits . Note that we constructed the train, valid and test splits in such a way that the dialogs in the validation and test set do not contain questions corresponding to tuples for which questions were seen at train time.  \n\n\nSome peculiar characteristics of Wikidata\n\nWe found that Wikidata has some typical characteristics and predicates, subject types and object types which often leads to very unnatural questions. We list down some of these issues below:\n\n\u2022 Very generic predicates: Consider the relation lake outf low for which the annotators suggested the question \"Which object type outflows from the lake YYY ?\". This seems like a valid template but turns out that Wikidata also contains predicates of the form lake outf low(DalLake, evaporation) where evaporation is an outflow from the lake. Similarly, the relation f abrication method allows for methods used to grow, cook, weave, build, assemble, manufacture an item. Due to the presence of such very generic relations (which allow a wide range of object types) sometime the questions instantiated from these templates may look very unnatural. In many cases, we manually tried to filter out such questions but given the scale of the KB it was not always possible to do this. We expect some such noisy questions to be a part of the final dataset.\n\n\u2022 Overlapping predicate and subject types: The word religion is both a predicate and a subject type in Wikidata. Similarly, sport is both a predicate and a subject type in Wikidata. This often leads to some questions containing repitions (for example, \"Which religion (subject type) is the religion (predicate) practised by YYY ?\". Again, we filtered out many such cases by applying some rule based post-processing after instantiating the templates but we still expect a few of these to be present in the dataset.\n\n\u2022 Long tail of subject types and relations: There are a few subject types in Wikidata which are very dominant. For example, a large number of entities in Wikidata belong to the sub-class person and location. These subject types in turn are associated with a few dominant relations. For example, part-of is the predominant relations associated with almost entities of type location. Similarly, citizenof, birthdate, birthplace are common relations associated with almost all entities of type person. Other relations such as named-after are a bit rare. Hence, when creating complex or linked questions connecting multiple entities and relations some of the rarer relations do not show up frequently. Such long tail behavior wherein some relations and predicates dominate will be observed in any KB of a reasonable size and can't really be avoided.\n\n\u2022 Unnatural Peer Subject types: As per Wikidata, the subject types religion and social group are peers as they are both sub-classes of belief system. As a consequence of this we have logical questions of the form \"Which religions and social groups does YYY belong to?\". We found this is a bit odd and we are not sure if an average user would consider these to be peers. These are special cases and are expected to any such large scale KB.\n\n\nProposed Model\n\nSince CSQA involves a combination of dialog and QA, we propose a model which is a cross between (i) the HRED model (Serban et al. 2016a) which is one of the state of the art models for dialog systems and (ii) the key value memory network model (Miller et al. 2016) which is a state of the art QA system. Our model has the following components:\n\n1. Hierarchical Encoder: The model contains a lower level RNN encoder which goes over the words in an utterance and computes a representation for each utterance. This is followed by a higher level encoder which goes over these utterance representations and computes a representation q 1 for the context (current state of the dialog).\n\n2. Handling Large Vocabulary: As input to the above encoder, we provide pre-trained Glove embeddings (Pennington, Socher, and Manning 2014) of the words in the question. However, our questions contain many entities (names, locations, etc.) for which pre-trained word embeddings are not available. Since these entities are crucial for answering the questions we cannot treat them as unknown words. One option is to randomly initialize the embeddings of these entity words and then train them along with other parameters of the model. This would effectively lead to a very large vocabulary and blow up the number of parameters. To avoid this, we use a state of the art TransE method (Bordes et al. 2013) for learning embeddings of KG entities offline. More specifically, for entities such as India, China, Ganga, Himalayas, etc. which are present in the KG we learn embeddings using the TransE model. We refer to these embeddings as KG embeddings. The final embedding of every question word is then a concatenation of the Glove embedding (if available, 0s otherwise) and the KG embedding (if available, 0s otherwise).\n\n\nCandidate generation:\n\nState of the art memory network based methods (Miller et al. 2016) learn to compute an attention function over the tuples in the KG based on the given question (or dialog context in our case). For large sized KGs, it is infeasible to compute the attention over the entire KG. Instead, following (Miller et al. 2016) we filter out tuples from the KG using the longest possible n-gram matching. We essentially consider only the longest n-gram which corresponds to the name of a KG-entity and retain only those tuples where the entity appears as subject/object. We observed that even with this filtering, the average number of candidate tuples for a given question in our dataset can sometimes be very large. We return to this issue in the Discussions section.\n\n\nKey Value Memory Network:\n\nA key value memory network stores each of the N candidate tuples (as selected above) as a key-value pair where the key contains the concatenated embedding of the relation and the subject (denoted by \u03c6 K (k hi ) \u2208 R D for the i th memory entry) whereas the value contains the embedding of the object (denoted by \u03c6 V (v hi ) \u2208 R D for the i th memory entry). Here, the subject, object and relation embeddings are the TransE KG embeddings, as described above. The model makes multiple passes over the memory computing new attention weights over the keys of the memory at each pass and updating the contextual question representation (q) whose initial representation q 1 \u2208 R d is computed by the hierarchical encoder. The rationale behind making multiple passes over the question is that the model may learn to focus on different aspects of the question in each pass. This is especially important in the case of complex questions. The following equation shows how the query representation gets updated in the j th pass.\nq j+1 = R j (q + N i Softmax(q j A\u03c6 K (k hi ))A\u03c6 V (v hi )) (1)\nA \u2208 R d\u00d7D and R 1...H \u2208 R d\u00d7d are the parameters of the (for clarification questions) (iv) Ganga, Narmada, Yamuna, ... (list of KG entities satisfying the query) and so on. At a high level, we can say that the model always produces sequences and in most cases the sequences will contain KG entities whereas in some cases the sequences may contain counts, entity types (rivers, lakes, etc) and non-KG words. We thus model the decoder as an RNN based sequence generator which takes as input the modified query representation. At each time step it gives a softmax over a shortlisted vocabulary containing counts, yes/no and KG entity types amounting to 1500 words approximately. Note that even though the model has to produce KG entities, we cannot include all KG entities in this vocabulary (as it will blow up the number of parameters). Instead, we train the decoder to produce the token KG W ORD whenever a KG entity needs to be produced in the output. We then use a copy mechanism to replace the KG W ORD with relevant entities. For example, if the decoder produces n KG W ORD tokens then we use q H+1 to give a distribution over the entities in the candidate tuples and then replace each KG W ORD token in the output by these top n entities having the highest probability. The distribution over the candidate entities is computed as\nSoftmax(q H+1 B\u03c6 V (v hi )) where B \u2208 R d\u00d7D is a parameter.\nThe training loss is a sum of the cross-entropy loss over the tokens and the KG entities.\n\n\nResults\n\nWe used Adam as the optimization algorithm and tuned the following hyperparameters using the validation set; learning  We used Precision and Recall as the evaluation metrics which capture the percentage of entities in the final decoder output that were correct and the percentage of actual entities that were retrieved by the system respectively. For verification and count based questions which produce a sequence of Yes and/or No or counts we use accuracy as the evaluation metric (i.e., whether the count or boolean answer was exact or not). Finally for questions which need clarification, the system has to generate a natural language response which is usually a sequence of KG-entities and non-KG words, hence for that we separately report both Precision/Recall over the predicted KG-entities and BLEU for the overall utterance similarity. The results of our experiments are summarized in Table 4.\n\n\nDiscussions\n\nBased on the results in Table 4, we discuss some shortcomings of existing methods and suggest areas for future research.\n\n1. Simple v/s Complex Questions: It is obvious that the model performs very poorly on complex questions as compared to simple questions. There are multiple reasons for this. First, existing models do not really model an aggregate or logical function for handling quantitative, comparative and logical reasoning. Designing such aggregation functions for an end-to-end solution is non-trivial and needs further exploration. This dataset should provide a good benchmark for exploring such solutions for complex QA. Second, it is not clear if the existing encoders (HRED + KVmem, in this case) are capable of effectively parsing complex questions and feeding a good represetation to the decoder. For example, the encoder ideally needs to learn to break down the question \"Which rivers flow through India and China?\" into two parts (i) \"Which rivers flow through India?\" (ii) \"Which rivers flow through China?\" and then compute an attention over relevant tuples in the memory. Such kind of parsing is not explicitly modeled by existing encoders. There is clearly a need for revisiting some of the traditional parsing based methods for QA in the light of this dataset.\n\n\nDirect v/s Indirect Questions:\n\nComparing the third and fourth rows of Table 4 with the second row, we see that the performance of the model drops when dealing with indirect or incomplete questions which rely on the context for resolving ellipsis, coreferences, etc. Even though current dialog systems (HRED, in this case) do learn to capture the context, one key challenge w.r.t our dataset is that, here named entities and relations matter more than other words in the context. We need better models which can explicitly learn to give importance to relations and entities (for example, using an explicit supervised attention mechanism).\n\n\nCandidate Generation:\n\nThis step is required to prune the size of the KG and store only relevant steps in the memory. This step is a bit adhoc as it relies on n-gram matching and we saw specific issues while using this on our dataset. We had explicitly asked the annotators to create paraphrases of the same question. As a result simple n-gram matching does not work well resulting in low recall of the actual answer entity in the filtered candidate tuples. A better candidate matching algorithm which takes care of entity paraphrases (Leo, Leonardo, etc.) and relation paraphrases (director, directed by, direct, etc.) are needed. In some cases, we also have the reverse problem. For example, if the entity being referred to in the question is extremely popular then it will be involved in over 100K tuples in the KB (for example, an entity like U.S.A.). This causes the KV memory to blow up leading to poor and inefficient training and inference.\n\n4. Better organization of the memory: It is inevitable that for some questions, especially complex questions involving logical operators over multiple entities and relations, the number of tuples required to be stored in the memory would be large. For example, around 15% of the questions in our data require more than 100K candidate tuples. Current Key Value Memory Networks which are flat in their organization are not suitable for this for two reasons. First, the amount of memory required by the model increases and can go beyond the capacity of existing GPUs. Second the attention weights computed using equation 1 need a prohibitively expensive softmax computation which increases both training and test time. Better ways of organizing the memory along with approximate methods for computing the softmax function are needed to handle such complex questions. We hope that the dataset, results and discussions on the resources presented in this paper will convince the reader that CSQA has several challenges which are not encountered in previous datasets for dialog and QA. Some of them are listed above and there are a few more which we do not list due to space constraints. Addressing/solving all of these challenges is clearly beyond the scope of a single paper. The purpose of this paper was to introduce the task and propose a model based on existing state of the art models and thereby highlight the need for further research to address the inadequacies of these models. To facilitate research, this dataset will be made available at https://github.com/iitm-nlp-miteshk/ AmritaSaha/tree/master/CSQA (please copy paste the URL in a browser instead of clicking on it). This URL will contain the following resources:\n\n\u2022 the train/valid/test splits used in our experiments\n\n\u2022 the processed version of the WikiData dump of 14-Nov-2016 that was used to construct the dataset \u2022 scripts to extract the train/valid/test set for each of the different question types listed in Table 2 \u2022 scripts to evaluate the performance of the model\n\n\nConclusion\n\nIn this paper, we introduced the task of Complex Sequential Question Answering (CSQA) with a large scale dataset consisting of conversations over linked QA pairs. The dataset contains 200K dialogs with 1.6M turns and was collected through a manually intensive semi-automated process. To the best of our knowledge, this is the first dataset of its kind which contains complex questions which require logical, quantitative and/or comparative reasoning over a large Knowledge Graph containing millions of tuples. We propose a model for CSQA which is a cross between state of the art models for dialog and QA and highlight the inadequacies of this model in dealing with the task of CSQA. It should be obvious that CSQA has several challenges and addressing/solving all of them is beyond the scope of a single paper. We hope that the introduction of this task and dataset should excite the research community to develop models for Complex Sequential Question Answering.\n\nFigure 1 :\n1Proposed Model consisting of a (i) Hierarchical Encoder (ii) Key-Value Memory Network and (iii) Decoder key-value memory network and N is the number of candidate tuples.5. Decoder:For a truly end-to-end solution, the decoder should be generic enough to produce multiple types of answers. For example, here are some of the answer sequences that the decoder is expected to generate: (i) 5 rivers and 4 lakes (for count questions) (ii) Yes/No/Yes and No respectively etc. (for verification questions) (iii) Did you mean ...\n\n\ndeal with Simple Questions, each of which can be answered from arXiv:1801.10314v2 [cs.CL] 4 Oct 2018 T12 Comparative Q. USER : Which countries have lesser number of political relations with others than Australia ? Response SYSTEM : Denmark, Canada, GrenadaTurn State \n\nUtterance \n\nT1 \nSimple Q. \nUSER : Can you tell me which cities border Verderio In-\nferiore ? \nResponse \nSYSTEM : Cornate d'Adda, Bernareggio, Robbiate \n\nT2 \nCoreference Q. USER : And which cities flank that one ? \nClarification Q. SYSTEM : Did you mean Robbiate ? \n\nT3 \nClarification \nUSER : No, I meant Cornate d'Adda. \nResponse \nSYSTEM : Bottanuco, Busnago, Trezzo sull'Adda \n\nT4 \nLogical Q. \nUSER : Which ones share border with Robbiate and are \nnot part of Azerbaijan ? \nResponse \nSYSTEM : Trezzo sull'Adda, Busnago, Bottanuco \n\nT5 \nQuantitative Q. USER : Which city is the capital of min number of french \nadministrative divisions ? \nResponse \nSYSTEM : Riga \n\nT6 \nBoolean Q. \nUSER : Is Rot am See located adjacent to that one ? \nResponse \nSYSTEM : No \n\nT7 \nQuanti. Q. \nUSER : How many political territories have strategic rela-\ntion with around 22 other countries ? \nResponse \nSYSTEM : 11 \n\nT8 \nLogical Q. \nUSER : Which are the nations connected with Robbiate \nand are not located in Italy ? \nResponse \nSYSTEM : The answer count is 11. Do you want to see \nall possibilities? \n\nT9 \nClarification \nUSER : No, show only a few of them \nResponse \nSYSTEM : Papua New Guinea, Jamaica, Antigua and Bar-\nbuda, Austria, New Zealand, The Bahamas, Gabon \n\nT10 \nQuantitative Q. USER : How many countries are diplomatically related to \nItaly ? \nResponse \nSYSTEM : 74 \n\nT11 \nQuantitative Q. USER : And how many of them also with Australia ? \nResponse \nSYSTEM : 56 \n\n\n\nTable 1 :\n1A sample dialog from the dataset (More examples of generated dialogs are provided in the supplementary material)\n\n\nWhich country has at least N rivers and lakes combined ? Count over Atleast Single entity type How many rivers flow through at least N countries? /Atmost / Approx./Equal Mult. entity type How many countries have at least N rivers and lakes combined ? How many countries have more number of rivers than India ? Mult. entity type How many countries have more rivers and lakes than India ?Reaso-\nning \n\nType \nContaining \nExample \n\nLog-\nical \n\nUnion \nSingle \nRelation \n\nWhich rivers flow through India \nor China? \nIntersection \nWhich rivers flow through India \nand China? \nDifference \nWhich rivers flow through India \nbut not China? \nAny of the \nabove \n\nMultiple Rela-\ntions \n\nWhich river flows through In-\ndia but does not originate in Hi-\nmalayas? \nVerifi-\ncation \n\nBoolean \nSingle/Multi-\nple entities \n\nDoes Ganga flow through India ? \n\nQuant-\nitative \n\nCount \nSingle entity \ntype \n\nHow many rivers flow through In-\ndia ? \nMult. entity \ntype \n\nHow many rivers and lakes does \nIndia have ? \nLogical \noperators \nHow many rivers flow through In-\ndia and/or/but not China? \n\nMin/Max \nSingle entity \ntype \n\nWhich river flows through maxi-\nmum number of countries ? \nMult. entity \ntype \n\nWhich country has maximum \nnumber of rivers and lakes com-\nbined ? \nAtleast/Atmost Single entity \ntype \n\nWhich rivers flow through at least \nN countries ? \n/Approx./ \nEqual \n\nMult. entity \ntype \n\nComp-\narative \n\nMore/Less \nSingle entity \ntype \n\nWhich countries have more num-\nber of rivers than India ? \nMult. entity \ntype \n\nWhich countries have more rivers \nand lakes than India ? \nCount over \nMore/Less \n\nSingle entity \ntype \n\n\n\nTable 2 :\n2Types of questions in the dataset.\n\nTable 3 :\n3Overall Dataset Statistics\n\nTable 4 :\n4Performance of the proposed model on different types of questions in the dialog rate \u2208 {1e-3, 4e-4}, RNN hidden unit size, word embedding size, KG embedding size \u2208 {256, 512}, batch size \u2208 {32, 64} and dialog context size as 2. The bracketed numbers indicate the values of each hyperparameter considered. On average, we found that the candidate generation step produces 10K candidate tuples, hence we kept upto 10K key value pairs in the memory network. Following(Miller et al. 2016), we set H = 2.\n\nMovie-dic: a movie dialogue corpus for research and development. R E Banchs, ACL. Banchs, R. E. 2012. Movie-dic: a movie dialogue corpus for research and development. In ACL, 2012, 203-207.\n\nSemantic parsing via paraphrasing. J Berant, P Liang, ACL (1). Berant, J., and Liang, P. 2014. Semantic parsing via para- phrasing. In ACL (1), 1415-1425.\n\nSemantic parsing on freebase from question-answer pairs. J Berant, A Chou, R Frostig, P Liang, EMNLP. 26Berant, J.; Chou, A.; Frostig, R.; and Liang, P. 2013. Se- mantic parsing on freebase from question-answer pairs. In EMNLP, volume 2, 6.\n\nModeling biological processes for reading comprehension. J Berant, V Srikumar, P Chen, A V Linden, B Harding, B Huang, P Clark, C D Manning, Berant, J.; Srikumar, V.; Chen, P.; Linden, A. V.; Harding, B.; Huang, B.; Clark, P.; and Manning, C. D. 2014. Modeling biological processes for reading comprehension. In EMNLP 2014,.\n\nLearning end-to-end goaloriented dialog. A Bordes, Weston , J , CoRR abs/1605.07683Bordes, A., and Weston, J. 2016. Learning end-to-end goal- oriented dialog. CoRR abs/1605.07683.\n\nTranslating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garc\u00eda-Dur\u00e1n, J Weston, O Yakhnenko, Neural Information Processing Systems. Bordes, A.; Usunier, N.; Garc\u00eda-Dur\u00e1n, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for model- ing multi-relational data. In Neural Information Processing Systems 2013, 2787-2795.\n\nLarge-scale simple question answering with memory networks. A Bordes, N Usunier, S Chopra, J Weston, A Bordes, S Chopra, J Weston, arXiv:1406.3676Question answering with subgraph embeddings. arXiv preprintBordes, A.; Usunier, N.; Chopra, S.; and Weston, J. 2015. Large-scale simple question answering with memory net- works. CoRR abs/1506.02075. Bordes, A.; Chopra, S.; and Weston, J. 2014. Ques- tion answering with subgraph embeddings. arXiv preprint arXiv:1406.3676.\n\nOpen question answering with weakly supervised embedding models. A Bordes, J Weston, N Usunier, ECML PKDD 2014. Proceedings, Part I. Bordes, A.; Weston, J.; and Usunier, N. 2014. Open question answering with weakly supervised embedding models. In ECML PKDD 2014. Proceedings, Part I, 165-180.\n\nEvaluating prerequisite qualities for learning end-to-end dialog systems. J Dodge, A Gane, X Zhang, A Bordes, S Chopra, A H Miller, A Szlam, J Weston, CoRR abs/1511.06931Dodge, J.; Gane, A.; Zhang, X.; Bordes, A.; Chopra, S.; Miller, A. H.; Szlam, A.; and Weston, J. 2015. Evaluating prerequisite qualities for learning end-to-end dialog systems. CoRR abs/1511.06931.\n\nIdentifying relations for open information extraction. A Fader, S Soderland, O Etzioni, EMNLP 2011. Fader, A.; Soderland, S.; and Etzioni, O. 2011. Identifying relations for open information extraction. In EMNLP 2011, 1535-1545.\n\nOpen question answering over curated and extracted knowledge bases. A Fader, L Zettlemoyer, O Etzioni, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACMFader, A.; Zettlemoyer, L.; and Etzioni, O. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD interna- tional conference on Knowledge discovery and data mining, 1156-1165. ACM.\n\nAsk me anything: Dynamic memory networks for natural language processing. A Kumar, O Irsoy, P Ondruska, M Iyyer, J Bradbury, I Gulrajani, V Zhong, R Paulus, R Socher, ICML 2016. Kumar, A.; Irsoy, O.; Ondruska, P.; Iyyer, M.; Bradbury, J.; Gulrajani, I.; Zhong, V.; Paulus, R.; and Socher, R. 2016. Ask me anything: Dynamic memory networks for natural language processing. In ICML 2016, 1378-1387.\n\nThe ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. R Lowe, N Pow, I Serban, J Pineau, Lowe, R.; Pow, N.; Serban, I.; and Pineau, J. 2015. The ubuntu dialogue corpus: A large dataset for research in un- structured multi-turn dialogue systems. In SIGDIAL 2015,, 285-294.\n\nTraining end-to-end dialogue systems with the ubuntu dialogue corpus. R T Lowe, N Pow, I V Serban, L Charlin, C Liu, J Pineau, D&D. 81Lowe, R. T.; Pow, N.; Serban, I. V.; Charlin, L.; Liu, C.; and Pineau, J. 2017. Training end-to-end dialogue systems with the ubuntu dialogue corpus. D&D 8(1):31-65.\n\nMulti-task sequence to sequence learning. M.-T Luong, Q V Le, I Sutskever, O Vinyals, L Kaiser, arXiv:1511.06114arXiv preprintLuong, M.-T.; Le, Q. V.; Sutskever, I.; Vinyals, O.; and Kaiser, L. 2015. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114.\n\nKey-value memory networks for directly reading documents. A H Miller, A Fisch, J Dodge, A Karimi, A Bordes, J Weston, CoRR abs/1606.03126Miller, A. H.; Fisch, A.; Dodge, J.; Karimi, A.; Bordes, A.; and Weston, J. 2016. Key-value memory networks for di- rectly reading documents. CoRR abs/1606.03126.\n\nA corpus and evaluation framework for deeper understanding of commonsense stories. N Mostafazadeh, N Chambers, X He, D Parikh, D Batra, L Vanderwende, P Kohli, J F Allen, A Neelakantan, Q V Le, M Abadi, A Mccallum, D Amodei, CoRR abs/1604.01696Learning a natural language interface with neural programmer. CoRR abs/1611.08945Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. F. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. CoRR abs/1604.01696. Neelakantan, A.; Le, Q. V.; Abadi, M.; McCallum, A.; and Amodei, D. 2016. Learning a natural language interface with neural programmer. CoRR abs/1611.08945.\n\nMS MARCO: A human generated machine reading comprehension dataset. T Nguyen, M Rosenberg, X Song, J Gao, S Tiwary, R Majumder, L Deng, CoRR abs/1611.09268Nguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.; Majumder, R.; and Deng, L. 2016. MS MARCO: A human generated machine reading comprehension dataset. CoRR abs/1611.09268.\n\nWho did what: A large-scale personcentered cloze dataset. T Onishi, H Wang, M Bansal, K Gimpel, D Mcallester, arXiv:1608.05457arXiv preprintOnishi, T.; Wang, H.; Bansal, M.; Gimpel, K.; and McAllester, D. 2016. Who did what: A large-scale person- centered cloze dataset. arXiv preprint arXiv:1608.05457.\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Empirical Methods in Natural Language Processing. Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), 1532-1543.\n\nSquad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, arXiv:1606.05250arXiv preprintRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.\n\nMctest: A challenge dataset for the open-domain machine comprehension of text. M Richardson, C J C Burges, E Renshaw, EMNLP 2013. Richardson, M.; Burges, C. J. C.; and Renshaw, E. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP 2013, 193-203.\n\nUnsupervised modeling of twitter conversations. A Ritter, C Cherry, B Dolan, NAACL 2010. Ritter, A.; Cherry, C.; and Dolan, B. 2010. Unsupervised modeling of twitter conversations. In NAACL 2010, 172-180.\n\nBuilding end-to-end dialogue systems using generative hierarchical neural network models. I V Serban, A Sordoni, Y Bengio, A Courville, J Pineau, AAAI'. 16AAAI PressSerban, I. V.; Sordoni, A.; Bengio, Y.; Courville, A.; and Pineau, J. 2016a. Building end-to-end dialogue systems using generative hierarchical neural network models. AAAI'16, 3776-3783. AAAI Press.\n\nGenerating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. I V Serban, A Garc\u00eda-Dur\u00e1n, \u00c7 G\u00fcl\u00e7ehre, S Ahn, S Chandar, A C Courville, Y Bengio, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, GermanyLong Papers1Serban, I. V.; Garc\u00eda-Dur\u00e1n, A.; G\u00fcl\u00e7ehre, \u00c7 .; Ahn, S.; Chan- dar, S.; Courville, A. C.; and Bengio, Y. 2016b. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.\n\nA hierarchical latent variable encoder-decoder model for generating dialogues. I V Serban, A Sordoni, R Lowe, L Charlin, J Pineau, A C Courville, Y Bengio, AAAI. Serban, I. V.; Sordoni, A.; Lowe, R.; Charlin, L.; Pineau, J.; Courville, A. C.; and Bengio, Y. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI, 3295-3301.\n\nWhat is the jeopardy model? a quasi-synchronous grammar for qa. E M Voorhees, D M Tice, Acm, M Wang, N A Smith, T Mitamura, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. the 23rd annual international ACM SIGIR conference on Research and development in information retrieval7EMNLP-CoNLLVoorhees, E. M., and Tice, D. M. 2000. Building a ques- tion answering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, 200-207. ACM. Wang, M.; Smith, N. A.; and Mitamura, T. 2007. What is the jeopardy model? a quasi-synchronous grammar for qa. In EMNLP-CoNLL, volume 7, 22-32.\n\nJoint relational embeddings for knowledge-based question answering. M.-C Yang, N Duan, M Zhou, H.-C Rim, EMNLP. 14Yang, M.-C.; Duan, N.; Zhou, M.; and Rim, H.-C. 2014. Joint relational embeddings for knowledge-based question answering. In EMNLP, volume 14, 645-650.\n\nWikiqa: A challenge dataset for open-domain question answering. Y Yang, W Yih, C Meek, EMNLP 2015. Lisbon, PortugalYang, Y.; Yih, W.; and Meek, C. 2015. Wikiqa: A challenge dataset for open-domain question answering. In EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, 2013-2018.\n", "annotations": {"author": "[{\"end\":199,\"start\":128},{\"end\":269,\"start\":200},{\"end\":330,\"start\":270},{\"end\":374,\"start\":331},{\"end\":446,\"start\":375}]", "publisher": null, "author_last_name": "[{\"end\":139,\"start\":135},{\"end\":214,\"start\":208},{\"end\":285,\"start\":279},{\"end\":355,\"start\":339},{\"end\":389,\"start\":382}]", "author_first_name": "[{\"end\":134,\"start\":128},{\"end\":207,\"start\":200},{\"end\":276,\"start\":270},{\"end\":278,\"start\":277},{\"end\":338,\"start\":331},{\"end\":381,\"start\":375}]", "author_affiliation": "[{\"end\":177,\"start\":161},{\"end\":198,\"start\":179},{\"end\":268,\"start\":240},{\"end\":329,\"start\":310},{\"end\":373,\"start\":357},{\"end\":445,\"start\":417}]", "title": "[{\"end\":125,\"start\":1},{\"end\":571,\"start\":447}]", "venue": null, "abstract": "[{\"end\":2405,\"start\":573}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3065,\"start\":3041},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3096,\"start\":3065},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3121,\"start\":3096},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3140,\"start\":3121},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3159,\"start\":3140},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3181,\"start\":3159},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3200,\"start\":3181},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3219,\"start\":3200},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3256,\"start\":3219},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3314,\"start\":3282},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3331,\"start\":3314},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3343,\"start\":3331},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3366,\"start\":3343},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3951,\"start\":3919},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3978,\"start\":3960},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4011,\"start\":3999},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5059,\"start\":5039},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5078,\"start\":5059},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5103,\"start\":5078},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6062,\"start\":6043},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6080,\"start\":6062},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6115,\"start\":6080},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7817,\"start\":7797},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7889,\"start\":7869},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8618,\"start\":8594},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8825,\"start\":8801},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8856,\"start\":8825},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8881,\"start\":8856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9020,\"start\":9000},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9039,\"start\":9020},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9058,\"start\":9039},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9120,\"start\":9097},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9138,\"start\":9120},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9187,\"start\":9161},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9206,\"start\":9187},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9268,\"start\":9230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9818,\"start\":9795},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9855,\"start\":9818},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9915,\"start\":9883},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9931,\"start\":9915},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10009,\"start\":9990},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10027,\"start\":10009},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10456,\"start\":10424},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10509,\"start\":10496},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10764,\"start\":10745},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10784,\"start\":10764},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10802,\"start\":10784},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11140,\"start\":11121},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11387,\"start\":11363},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28837,\"start\":28817},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28966,\"start\":28946},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30082,\"start\":30063},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30589,\"start\":30569},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30838,\"start\":30818},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":45278,\"start\":45258}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41230,\"start\":40697},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42958,\"start\":41231},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43083,\"start\":42959},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44696,\"start\":43084},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44743,\"start\":44697},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44782,\"start\":44744},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45293,\"start\":44783}]", "paragraph": "[{\"end\":2775,\"start\":2421},{\"end\":3792,\"start\":2777},{\"end\":4818,\"start\":3794},{\"end\":6163,\"start\":4820},{\"end\":7464,\"start\":6165},{\"end\":8391,\"start\":7466},{\"end\":10185,\"start\":8408},{\"end\":11693,\"start\":10187},{\"end\":13254,\"start\":11714},{\"end\":14441,\"start\":13274},{\"end\":14786,\"start\":14462},{\"end\":15013,\"start\":14814},{\"end\":15252,\"start\":15015},{\"end\":15787,\"start\":15254},{\"end\":16891,\"start\":15789},{\"end\":16960,\"start\":16893},{\"end\":17033,\"start\":16962},{\"end\":17703,\"start\":17035},{\"end\":18259,\"start\":17705},{\"end\":18445,\"start\":18281},{\"end\":19128,\"start\":18447},{\"end\":20064,\"start\":19130},{\"end\":20347,\"start\":20066},{\"end\":21569,\"start\":20349},{\"end\":22456,\"start\":21571},{\"end\":22719,\"start\":22458},{\"end\":23395,\"start\":22721},{\"end\":25199,\"start\":23397},{\"end\":25796,\"start\":25201},{\"end\":26032,\"start\":25842},{\"end\":26881,\"start\":26034},{\"end\":27396,\"start\":26883},{\"end\":28243,\"start\":27398},{\"end\":28683,\"start\":28245},{\"end\":29045,\"start\":28702},{\"end\":29380,\"start\":29047},{\"end\":30497,\"start\":29382},{\"end\":31280,\"start\":30523},{\"end\":32325,\"start\":31310},{\"end\":33724,\"start\":32390},{\"end\":33874,\"start\":33785},{\"end\":34788,\"start\":33886},{\"end\":34924,\"start\":34804},{\"end\":36088,\"start\":34926},{\"end\":36729,\"start\":36123},{\"end\":37680,\"start\":36755},{\"end\":39406,\"start\":37682},{\"end\":39461,\"start\":39408},{\"end\":39717,\"start\":39463},{\"end\":40696,\"start\":39732}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":32389,\"start\":32326},{\"attributes\":{\"id\":\"formula_1\"},\"end\":33784,\"start\":33725}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4030,\"start\":4023},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4848,\"start\":4841},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":5276,\"start\":5269},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":7463,\"start\":7456},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19244,\"start\":19237},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20571,\"start\":20564},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24802,\"start\":24795},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25403,\"start\":25396},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25487,\"start\":25480},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":34787,\"start\":34780},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":34835,\"start\":34828},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36169,\"start\":36162},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":39666,\"start\":39659}]", "section_header": "[{\"end\":2419,\"start\":2407},{\"end\":8406,\"start\":8394},{\"end\":11712,\"start\":11696},{\"end\":13272,\"start\":13257},{\"end\":14460,\"start\":14444},{\"attributes\":{\"n\":\"1.\"},\"end\":14812,\"start\":14789},{\"end\":18279,\"start\":18262},{\"end\":25840,\"start\":25799},{\"end\":28700,\"start\":28686},{\"attributes\":{\"n\":\"3.\"},\"end\":30521,\"start\":30500},{\"attributes\":{\"n\":\"4.\"},\"end\":31308,\"start\":31283},{\"end\":33884,\"start\":33877},{\"end\":34802,\"start\":34791},{\"attributes\":{\"n\":\"2.\"},\"end\":36121,\"start\":36091},{\"attributes\":{\"n\":\"3.\"},\"end\":36753,\"start\":36732},{\"end\":39730,\"start\":39720},{\"end\":40708,\"start\":40698},{\"end\":42969,\"start\":42960},{\"end\":44707,\"start\":44698},{\"end\":44754,\"start\":44745},{\"end\":44793,\"start\":44784}]", "table": "[{\"end\":42958,\"start\":41489},{\"end\":44696,\"start\":43472}]", "figure_caption": "[{\"end\":41230,\"start\":40710},{\"end\":41489,\"start\":41233},{\"end\":43083,\"start\":42971},{\"end\":43472,\"start\":43086},{\"end\":44743,\"start\":44709},{\"end\":44782,\"start\":44756},{\"end\":45293,\"start\":44795}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":45361,\"start\":45360},{\"end\":45363,\"start\":45362},{\"end\":45522,\"start\":45521},{\"end\":45532,\"start\":45531},{\"end\":45700,\"start\":45699},{\"end\":45710,\"start\":45709},{\"end\":45718,\"start\":45717},{\"end\":45729,\"start\":45728},{\"end\":45942,\"start\":45941},{\"end\":45952,\"start\":45951},{\"end\":45964,\"start\":45963},{\"end\":45972,\"start\":45971},{\"end\":45974,\"start\":45973},{\"end\":45984,\"start\":45983},{\"end\":45995,\"start\":45994},{\"end\":46004,\"start\":46003},{\"end\":46013,\"start\":46012},{\"end\":46015,\"start\":46014},{\"end\":46252,\"start\":46251},{\"end\":46267,\"start\":46261},{\"end\":46271,\"start\":46270},{\"end\":46451,\"start\":46450},{\"end\":46461,\"start\":46460},{\"end\":46472,\"start\":46471},{\"end\":46488,\"start\":46487},{\"end\":46498,\"start\":46497},{\"end\":46809,\"start\":46808},{\"end\":46819,\"start\":46818},{\"end\":46830,\"start\":46829},{\"end\":46840,\"start\":46839},{\"end\":46850,\"start\":46849},{\"end\":46860,\"start\":46859},{\"end\":46870,\"start\":46869},{\"end\":47285,\"start\":47284},{\"end\":47295,\"start\":47294},{\"end\":47305,\"start\":47304},{\"end\":47588,\"start\":47587},{\"end\":47597,\"start\":47596},{\"end\":47605,\"start\":47604},{\"end\":47614,\"start\":47613},{\"end\":47624,\"start\":47623},{\"end\":47634,\"start\":47633},{\"end\":47636,\"start\":47635},{\"end\":47646,\"start\":47645},{\"end\":47655,\"start\":47654},{\"end\":47938,\"start\":47937},{\"end\":47947,\"start\":47946},{\"end\":47960,\"start\":47959},{\"end\":48181,\"start\":48180},{\"end\":48190,\"start\":48189},{\"end\":48205,\"start\":48204},{\"end\":48716,\"start\":48715},{\"end\":48725,\"start\":48724},{\"end\":48734,\"start\":48733},{\"end\":48746,\"start\":48745},{\"end\":48755,\"start\":48754},{\"end\":48767,\"start\":48766},{\"end\":48780,\"start\":48779},{\"end\":48789,\"start\":48788},{\"end\":48799,\"start\":48798},{\"end\":49142,\"start\":49141},{\"end\":49150,\"start\":49149},{\"end\":49157,\"start\":49156},{\"end\":49167,\"start\":49166},{\"end\":49431,\"start\":49430},{\"end\":49433,\"start\":49432},{\"end\":49441,\"start\":49440},{\"end\":49448,\"start\":49447},{\"end\":49450,\"start\":49449},{\"end\":49460,\"start\":49459},{\"end\":49471,\"start\":49470},{\"end\":49478,\"start\":49477},{\"end\":49707,\"start\":49703},{\"end\":49716,\"start\":49715},{\"end\":49718,\"start\":49717},{\"end\":49724,\"start\":49723},{\"end\":49737,\"start\":49736},{\"end\":49748,\"start\":49747},{\"end\":49996,\"start\":49995},{\"end\":49998,\"start\":49997},{\"end\":50008,\"start\":50007},{\"end\":50017,\"start\":50016},{\"end\":50026,\"start\":50025},{\"end\":50036,\"start\":50035},{\"end\":50046,\"start\":50045},{\"end\":50322,\"start\":50321},{\"end\":50338,\"start\":50337},{\"end\":50350,\"start\":50349},{\"end\":50356,\"start\":50355},{\"end\":50366,\"start\":50365},{\"end\":50375,\"start\":50374},{\"end\":50390,\"start\":50389},{\"end\":50399,\"start\":50398},{\"end\":50401,\"start\":50400},{\"end\":50410,\"start\":50409},{\"end\":50425,\"start\":50424},{\"end\":50427,\"start\":50426},{\"end\":50433,\"start\":50432},{\"end\":50442,\"start\":50441},{\"end\":50454,\"start\":50453},{\"end\":51007,\"start\":51006},{\"end\":51017,\"start\":51016},{\"end\":51030,\"start\":51029},{\"end\":51038,\"start\":51037},{\"end\":51045,\"start\":51044},{\"end\":51055,\"start\":51054},{\"end\":51067,\"start\":51066},{\"end\":51332,\"start\":51331},{\"end\":51342,\"start\":51341},{\"end\":51350,\"start\":51349},{\"end\":51360,\"start\":51359},{\"end\":51370,\"start\":51369},{\"end\":51626,\"start\":51625},{\"end\":51640,\"start\":51639},{\"end\":51650,\"start\":51649},{\"end\":51652,\"start\":51651},{\"end\":51947,\"start\":51946},{\"end\":51960,\"start\":51959},{\"end\":51969,\"start\":51968},{\"end\":51980,\"start\":51979},{\"end\":52252,\"start\":52251},{\"end\":52266,\"start\":52265},{\"end\":52270,\"start\":52267},{\"end\":52280,\"start\":52279},{\"end\":52511,\"start\":52510},{\"end\":52521,\"start\":52520},{\"end\":52531,\"start\":52530},{\"end\":52759,\"start\":52758},{\"end\":52761,\"start\":52760},{\"end\":52771,\"start\":52770},{\"end\":52782,\"start\":52781},{\"end\":52792,\"start\":52791},{\"end\":52805,\"start\":52804},{\"end\":53135,\"start\":53134},{\"end\":53137,\"start\":53136},{\"end\":53147,\"start\":53146},{\"end\":53163,\"start\":53162},{\"end\":53175,\"start\":53174},{\"end\":53182,\"start\":53181},{\"end\":53193,\"start\":53192},{\"end\":53195,\"start\":53194},{\"end\":53208,\"start\":53207},{\"end\":53882,\"start\":53881},{\"end\":53884,\"start\":53883},{\"end\":53894,\"start\":53893},{\"end\":53905,\"start\":53904},{\"end\":53913,\"start\":53912},{\"end\":53924,\"start\":53923},{\"end\":53934,\"start\":53933},{\"end\":53936,\"start\":53935},{\"end\":53949,\"start\":53948},{\"end\":54231,\"start\":54230},{\"end\":54233,\"start\":54232},{\"end\":54245,\"start\":54244},{\"end\":54247,\"start\":54246},{\"end\":54260,\"start\":54259},{\"end\":54268,\"start\":54267},{\"end\":54270,\"start\":54269},{\"end\":54279,\"start\":54278},{\"end\":54967,\"start\":54963},{\"end\":54975,\"start\":54974},{\"end\":54983,\"start\":54982},{\"end\":54994,\"start\":54990},{\"end\":55227,\"start\":55226},{\"end\":55235,\"start\":55234},{\"end\":55242,\"start\":55241}]", "bib_author_last_name": "[{\"end\":45370,\"start\":45364},{\"end\":45529,\"start\":45523},{\"end\":45538,\"start\":45533},{\"end\":45707,\"start\":45701},{\"end\":45715,\"start\":45711},{\"end\":45726,\"start\":45719},{\"end\":45735,\"start\":45730},{\"end\":45949,\"start\":45943},{\"end\":45961,\"start\":45953},{\"end\":45969,\"start\":45965},{\"end\":45981,\"start\":45975},{\"end\":45992,\"start\":45985},{\"end\":46001,\"start\":45996},{\"end\":46010,\"start\":46005},{\"end\":46023,\"start\":46016},{\"end\":46259,\"start\":46253},{\"end\":46458,\"start\":46452},{\"end\":46469,\"start\":46462},{\"end\":46485,\"start\":46473},{\"end\":46495,\"start\":46489},{\"end\":46508,\"start\":46499},{\"end\":46816,\"start\":46810},{\"end\":46827,\"start\":46820},{\"end\":46837,\"start\":46831},{\"end\":46847,\"start\":46841},{\"end\":46857,\"start\":46851},{\"end\":46867,\"start\":46861},{\"end\":46877,\"start\":46871},{\"end\":47292,\"start\":47286},{\"end\":47302,\"start\":47296},{\"end\":47313,\"start\":47306},{\"end\":47594,\"start\":47589},{\"end\":47602,\"start\":47598},{\"end\":47611,\"start\":47606},{\"end\":47621,\"start\":47615},{\"end\":47631,\"start\":47625},{\"end\":47643,\"start\":47637},{\"end\":47652,\"start\":47647},{\"end\":47662,\"start\":47656},{\"end\":47944,\"start\":47939},{\"end\":47957,\"start\":47948},{\"end\":47968,\"start\":47961},{\"end\":48187,\"start\":48182},{\"end\":48202,\"start\":48191},{\"end\":48213,\"start\":48206},{\"end\":48722,\"start\":48717},{\"end\":48731,\"start\":48726},{\"end\":48743,\"start\":48735},{\"end\":48752,\"start\":48747},{\"end\":48764,\"start\":48756},{\"end\":48777,\"start\":48768},{\"end\":48786,\"start\":48781},{\"end\":48796,\"start\":48790},{\"end\":48806,\"start\":48800},{\"end\":49147,\"start\":49143},{\"end\":49154,\"start\":49151},{\"end\":49164,\"start\":49158},{\"end\":49174,\"start\":49168},{\"end\":49438,\"start\":49434},{\"end\":49445,\"start\":49442},{\"end\":49457,\"start\":49451},{\"end\":49468,\"start\":49461},{\"end\":49475,\"start\":49472},{\"end\":49485,\"start\":49479},{\"end\":49713,\"start\":49708},{\"end\":49721,\"start\":49719},{\"end\":49734,\"start\":49725},{\"end\":49745,\"start\":49738},{\"end\":49755,\"start\":49749},{\"end\":50005,\"start\":49999},{\"end\":50014,\"start\":50009},{\"end\":50023,\"start\":50018},{\"end\":50033,\"start\":50027},{\"end\":50043,\"start\":50037},{\"end\":50053,\"start\":50047},{\"end\":50335,\"start\":50323},{\"end\":50347,\"start\":50339},{\"end\":50353,\"start\":50351},{\"end\":50363,\"start\":50357},{\"end\":50372,\"start\":50367},{\"end\":50387,\"start\":50376},{\"end\":50396,\"start\":50391},{\"end\":50407,\"start\":50402},{\"end\":50422,\"start\":50411},{\"end\":50430,\"start\":50428},{\"end\":50439,\"start\":50434},{\"end\":50451,\"start\":50443},{\"end\":50461,\"start\":50455},{\"end\":51014,\"start\":51008},{\"end\":51027,\"start\":51018},{\"end\":51035,\"start\":51031},{\"end\":51042,\"start\":51039},{\"end\":51052,\"start\":51046},{\"end\":51064,\"start\":51056},{\"end\":51072,\"start\":51068},{\"end\":51339,\"start\":51333},{\"end\":51347,\"start\":51343},{\"end\":51357,\"start\":51351},{\"end\":51367,\"start\":51361},{\"end\":51381,\"start\":51371},{\"end\":51637,\"start\":51627},{\"end\":51647,\"start\":51641},{\"end\":51660,\"start\":51653},{\"end\":51957,\"start\":51948},{\"end\":51966,\"start\":51961},{\"end\":51977,\"start\":51970},{\"end\":51986,\"start\":51981},{\"end\":52263,\"start\":52253},{\"end\":52277,\"start\":52271},{\"end\":52288,\"start\":52281},{\"end\":52518,\"start\":52512},{\"end\":52528,\"start\":52522},{\"end\":52537,\"start\":52532},{\"end\":52768,\"start\":52762},{\"end\":52779,\"start\":52772},{\"end\":52789,\"start\":52783},{\"end\":52802,\"start\":52793},{\"end\":52812,\"start\":52806},{\"end\":53144,\"start\":53138},{\"end\":53160,\"start\":53148},{\"end\":53172,\"start\":53164},{\"end\":53179,\"start\":53176},{\"end\":53190,\"start\":53183},{\"end\":53205,\"start\":53196},{\"end\":53215,\"start\":53209},{\"end\":53891,\"start\":53885},{\"end\":53902,\"start\":53895},{\"end\":53910,\"start\":53906},{\"end\":53921,\"start\":53914},{\"end\":53931,\"start\":53925},{\"end\":53946,\"start\":53937},{\"end\":53956,\"start\":53950},{\"end\":54242,\"start\":54234},{\"end\":54252,\"start\":54248},{\"end\":54257,\"start\":54254},{\"end\":54265,\"start\":54261},{\"end\":54276,\"start\":54271},{\"end\":54288,\"start\":54280},{\"end\":54972,\"start\":54968},{\"end\":54980,\"start\":54976},{\"end\":54988,\"start\":54984},{\"end\":54998,\"start\":54995},{\"end\":55232,\"start\":55228},{\"end\":55239,\"start\":55236},{\"end\":55247,\"start\":55243}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":244187},\"end\":45484,\"start\":45295},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1336493},\"end\":45640,\"start\":45486},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6401679},\"end\":45882,\"start\":45642},{\"attributes\":{\"id\":\"b3\"},\"end\":46208,\"start\":45884},{\"attributes\":{\"doi\":\"CoRR abs/1605.07683\",\"id\":\"b4\"},\"end\":46389,\"start\":46210},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14941970},\"end\":46746,\"start\":46391},{\"attributes\":{\"doi\":\"arXiv:1406.3676\",\"id\":\"b6\",\"matched_paper_id\":9605730},\"end\":47217,\"start\":46748},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1849689},\"end\":47511,\"start\":47219},{\"attributes\":{\"doi\":\"CoRR abs/1511.06931\",\"id\":\"b8\"},\"end\":47880,\"start\":47513},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10318045},\"end\":48110,\"start\":47882},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207214527},\"end\":48639,\"start\":48112},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2319779},\"end\":49037,\"start\":48641},{\"attributes\":{\"id\":\"b12\"},\"end\":49358,\"start\":49039},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13823999},\"end\":49659,\"start\":49360},{\"attributes\":{\"doi\":\"arXiv:1511.06114\",\"id\":\"b14\"},\"end\":49935,\"start\":49661},{\"attributes\":{\"id\":\"b15\"},\"end\":50236,\"start\":49937},{\"attributes\":{\"doi\":\"CoRR abs/1604.01696\",\"id\":\"b16\"},\"end\":50937,\"start\":50238},{\"attributes\":{\"doi\":\"CoRR abs/1611.09268\",\"id\":\"b17\"},\"end\":51271,\"start\":50939},{\"attributes\":{\"doi\":\"arXiv:1608.05457\",\"id\":\"b18\"},\"end\":51576,\"start\":51273},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1957433},\"end\":51883,\"start\":51578},{\"attributes\":{\"doi\":\"arXiv:1606.05250\",\"id\":\"b20\"},\"end\":52170,\"start\":51885},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2100831},\"end\":52460,\"start\":52172},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16322335},\"end\":52666,\"start\":52462},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6126582},\"end\":53031,\"start\":52668},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":12241221},\"end\":53800,\"start\":53033},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14857825},\"end\":54164,\"start\":53802},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10761261},\"end\":54893,\"start\":54166},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15262897},\"end\":55160,\"start\":54895},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1373518},\"end\":55445,\"start\":55162}]", "bib_title": "[{\"end\":45358,\"start\":45295},{\"end\":45519,\"start\":45486},{\"end\":45697,\"start\":45642},{\"end\":46448,\"start\":46391},{\"end\":46806,\"start\":46748},{\"end\":47282,\"start\":47219},{\"end\":47935,\"start\":47882},{\"end\":48178,\"start\":48112},{\"end\":48713,\"start\":48641},{\"end\":49428,\"start\":49360},{\"end\":51623,\"start\":51578},{\"end\":52249,\"start\":52172},{\"end\":52508,\"start\":52462},{\"end\":52756,\"start\":52668},{\"end\":53132,\"start\":53033},{\"end\":53879,\"start\":53802},{\"end\":54228,\"start\":54166},{\"end\":54961,\"start\":54895},{\"end\":55224,\"start\":55162}]", "bib_author": "[{\"end\":45372,\"start\":45360},{\"end\":45531,\"start\":45521},{\"end\":45540,\"start\":45531},{\"end\":45709,\"start\":45699},{\"end\":45717,\"start\":45709},{\"end\":45728,\"start\":45717},{\"end\":45737,\"start\":45728},{\"end\":45951,\"start\":45941},{\"end\":45963,\"start\":45951},{\"end\":45971,\"start\":45963},{\"end\":45983,\"start\":45971},{\"end\":45994,\"start\":45983},{\"end\":46003,\"start\":45994},{\"end\":46012,\"start\":46003},{\"end\":46025,\"start\":46012},{\"end\":46261,\"start\":46251},{\"end\":46270,\"start\":46261},{\"end\":46274,\"start\":46270},{\"end\":46460,\"start\":46450},{\"end\":46471,\"start\":46460},{\"end\":46487,\"start\":46471},{\"end\":46497,\"start\":46487},{\"end\":46510,\"start\":46497},{\"end\":46818,\"start\":46808},{\"end\":46829,\"start\":46818},{\"end\":46839,\"start\":46829},{\"end\":46849,\"start\":46839},{\"end\":46859,\"start\":46849},{\"end\":46869,\"start\":46859},{\"end\":46879,\"start\":46869},{\"end\":47294,\"start\":47284},{\"end\":47304,\"start\":47294},{\"end\":47315,\"start\":47304},{\"end\":47596,\"start\":47587},{\"end\":47604,\"start\":47596},{\"end\":47613,\"start\":47604},{\"end\":47623,\"start\":47613},{\"end\":47633,\"start\":47623},{\"end\":47645,\"start\":47633},{\"end\":47654,\"start\":47645},{\"end\":47664,\"start\":47654},{\"end\":47946,\"start\":47937},{\"end\":47959,\"start\":47946},{\"end\":47970,\"start\":47959},{\"end\":48189,\"start\":48180},{\"end\":48204,\"start\":48189},{\"end\":48215,\"start\":48204},{\"end\":48724,\"start\":48715},{\"end\":48733,\"start\":48724},{\"end\":48745,\"start\":48733},{\"end\":48754,\"start\":48745},{\"end\":48766,\"start\":48754},{\"end\":48779,\"start\":48766},{\"end\":48788,\"start\":48779},{\"end\":48798,\"start\":48788},{\"end\":48808,\"start\":48798},{\"end\":49149,\"start\":49141},{\"end\":49156,\"start\":49149},{\"end\":49166,\"start\":49156},{\"end\":49176,\"start\":49166},{\"end\":49440,\"start\":49430},{\"end\":49447,\"start\":49440},{\"end\":49459,\"start\":49447},{\"end\":49470,\"start\":49459},{\"end\":49477,\"start\":49470},{\"end\":49487,\"start\":49477},{\"end\":49715,\"start\":49703},{\"end\":49723,\"start\":49715},{\"end\":49736,\"start\":49723},{\"end\":49747,\"start\":49736},{\"end\":49757,\"start\":49747},{\"end\":50007,\"start\":49995},{\"end\":50016,\"start\":50007},{\"end\":50025,\"start\":50016},{\"end\":50035,\"start\":50025},{\"end\":50045,\"start\":50035},{\"end\":50055,\"start\":50045},{\"end\":50337,\"start\":50321},{\"end\":50349,\"start\":50337},{\"end\":50355,\"start\":50349},{\"end\":50365,\"start\":50355},{\"end\":50374,\"start\":50365},{\"end\":50389,\"start\":50374},{\"end\":50398,\"start\":50389},{\"end\":50409,\"start\":50398},{\"end\":50424,\"start\":50409},{\"end\":50432,\"start\":50424},{\"end\":50441,\"start\":50432},{\"end\":50453,\"start\":50441},{\"end\":50463,\"start\":50453},{\"end\":51016,\"start\":51006},{\"end\":51029,\"start\":51016},{\"end\":51037,\"start\":51029},{\"end\":51044,\"start\":51037},{\"end\":51054,\"start\":51044},{\"end\":51066,\"start\":51054},{\"end\":51074,\"start\":51066},{\"end\":51341,\"start\":51331},{\"end\":51349,\"start\":51341},{\"end\":51359,\"start\":51349},{\"end\":51369,\"start\":51359},{\"end\":51383,\"start\":51369},{\"end\":51639,\"start\":51625},{\"end\":51649,\"start\":51639},{\"end\":51662,\"start\":51649},{\"end\":51959,\"start\":51946},{\"end\":51968,\"start\":51959},{\"end\":51979,\"start\":51968},{\"end\":51988,\"start\":51979},{\"end\":52265,\"start\":52251},{\"end\":52279,\"start\":52265},{\"end\":52290,\"start\":52279},{\"end\":52520,\"start\":52510},{\"end\":52530,\"start\":52520},{\"end\":52539,\"start\":52530},{\"end\":52770,\"start\":52758},{\"end\":52781,\"start\":52770},{\"end\":52791,\"start\":52781},{\"end\":52804,\"start\":52791},{\"end\":52814,\"start\":52804},{\"end\":53146,\"start\":53134},{\"end\":53162,\"start\":53146},{\"end\":53174,\"start\":53162},{\"end\":53181,\"start\":53174},{\"end\":53192,\"start\":53181},{\"end\":53207,\"start\":53192},{\"end\":53217,\"start\":53207},{\"end\":53893,\"start\":53881},{\"end\":53904,\"start\":53893},{\"end\":53912,\"start\":53904},{\"end\":53923,\"start\":53912},{\"end\":53933,\"start\":53923},{\"end\":53948,\"start\":53933},{\"end\":53958,\"start\":53948},{\"end\":54244,\"start\":54230},{\"end\":54254,\"start\":54244},{\"end\":54259,\"start\":54254},{\"end\":54267,\"start\":54259},{\"end\":54278,\"start\":54267},{\"end\":54290,\"start\":54278},{\"end\":54974,\"start\":54963},{\"end\":54982,\"start\":54974},{\"end\":54990,\"start\":54982},{\"end\":55000,\"start\":54990},{\"end\":55234,\"start\":55226},{\"end\":55241,\"start\":55234},{\"end\":55249,\"start\":55241}]", "bib_venue": "[{\"end\":45375,\"start\":45372},{\"end\":45547,\"start\":45540},{\"end\":45742,\"start\":45737},{\"end\":45939,\"start\":45884},{\"end\":46249,\"start\":46210},{\"end\":46547,\"start\":46510},{\"end\":46937,\"start\":46894},{\"end\":47350,\"start\":47315},{\"end\":47585,\"start\":47513},{\"end\":47980,\"start\":47970},{\"end\":48313,\"start\":48215},{\"end\":48817,\"start\":48808},{\"end\":49139,\"start\":49039},{\"end\":49490,\"start\":49487},{\"end\":49701,\"start\":49661},{\"end\":49993,\"start\":49937},{\"end\":50319,\"start\":50238},{\"end\":51004,\"start\":50939},{\"end\":51329,\"start\":51273},{\"end\":51710,\"start\":51662},{\"end\":51944,\"start\":51885},{\"end\":52300,\"start\":52290},{\"end\":52549,\"start\":52539},{\"end\":52819,\"start\":52814},{\"end\":53314,\"start\":53217},{\"end\":53962,\"start\":53958},{\"end\":54408,\"start\":54290},{\"end\":55005,\"start\":55000},{\"end\":55259,\"start\":55249},{\"end\":48398,\"start\":48315},{\"end\":53413,\"start\":53316},{\"end\":54513,\"start\":54410},{\"end\":55277,\"start\":55261}]"}}}, "year": 2023, "month": 12, "day": 17}