{"id": 255999752, "updated": "2023-10-05 05:16:20.829", "metadata": {"title": "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture", "authors": "[{\"first\":\"Mahmoud\",\"last\":\"Assran\",\"middle\":[]},{\"first\":\"Quentin\",\"last\":\"Duval\",\"middle\":[]},{\"first\":\"Ishan\",\"last\":\"Misra\",\"middle\":[]},{\"first\":\"Piotr\",\"last\":\"Bojanowski\",\"middle\":[]},{\"first\":\"Pascal\",\"last\":\"Vincent\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Rabbat\",\"middle\":[]},{\"first\":\"Yann\",\"last\":\"LeCun\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Ballas\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2301.08243", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/AssranDMBVRLB23", "doi": "10.1109/cvpr52729.2023.01499"}}, "content": {"source": {"pdf_hash": "ee57e4d7a125f4ca8916284a857c3760d7d378d3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.08243v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e7a0c97e72380f130dfbee0af35f103f7faafb65", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ee57e4d7a125f4ca8916284a857c3760d7d378d3.txt", "contents": "\nSelf-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\n\n\nMahmoud Assran \nMeta AI (FAIR)\n\n\nMcGill University\n\n\nAI Institute\nMilaQuebec\n\nQuentin Duval \nMeta AI (FAIR)\n\n\nIshan Misra \nMeta AI (FAIR)\n\n\nPiotr Bojanowski \nMeta AI (FAIR)\n\n\nPascal Vincent \nMeta AI (FAIR)\n\n\nMichael Rabbat \nMeta AI (FAIR)\n\n\nAI Institute\nMilaQuebec\n\nYann Lecun \nMeta AI (FAIR)\n\n\nNew York University\n\n\nNicolas Ballas \nMeta AI (FAIR)\n\n\nSelf-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\n\nThis paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Imagebased Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.\n\nIntroduction\n\nIn computer vision, there are two common families of approaches for self-supervised learning from images: invariance-based methods [1,4,10,17,18,24,35,37,74] and generative methods [8,28,36,57].\n\nInvariance-based pretraining methods optimize an encoder to produce similar embeddings for two or more views of the same image [15,20], with image views typically constructed using a set of hand-crafted data augmentations, such as random scaling, cropping, and color jittering [20], amongst others [35]. These pretraining methods can produce representations of a high semantic level [4,18], but they also introduce strong biases that may be detrimental for certain downstream tasks or even for pretraining tasks with different data distributions [2]. Often, it is unclear * massran@meta.com  how to generalize these biases for tasks requiring different levels of abstraction. For example, image classification and instance segmentation do not require the same invariances [11]. Additionally, it is not straightforward to generalize these image-specific augmentations to other modalities such as audio.\n\nCognitive learning theories have suggested that a driving mechanism behind representation learning in biological systems is the adaptation of an internal model to predict sensory input responses [31,59]. This idea is at the core of self-supervised generative methods, which remove or corrupt portions of the input and learn to predict the corrupted content [9,36,57,67,68,71]. In particular, maskdenoising approaches learn representations by reconstructing randomly masked patches from an input, either at the pixel or token level. Masked pretraining tasks require less prior knowledge than view-invariance approaches and easily generalize beyond the image modality [8]. However, the  Common architectures for self-supervised learning, in which the system learns to capture the relationships between its inputs. The objective is to assign a high energy (large scaler value) to incompatible inputs, and to assign a low energy (low scaler value) to compatible inputs. (a) Joint-Embedding Architectures learn to output similar embeddings for compatible inputs x, y and dissimilar embeddings for incompatible inputs. (b) Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a decoder network that is conditioned on additional (possibly latent) variables z to facilitate reconstruction. (c) Joint-Embedding Predictive Architectures learn to predict the embeddings of a signal y from a compatible signal x, using a predictor network that is conditioned on additional (possibly latent) variables z to facilitate prediction.\n\nresulting representations are typically of a lower semantic level and underperform invariance-based pretraining in offthe-shelf evaluations (e.g., linear-probing) and in transfer settings with limited supervision for semantic classification tasks [4]. Consequently, a more involved adaptation mechanism (e.g., end-to-end fine-tuning) is required to reap the full advantage of these methods.\n\nIn this work, we explore how to improve the semantic level of self-supervised representations without using extra prior knowledge encoded through image transformations. To that end, we introduce a joint-embedding predictive architecture [48] for images (I-JEPA). An illustration of the method is provided in Figure 3. The idea behind I-JEPA is to predict missing information in an abstract representation space; e.g., given a single context block, predict the representations of various target blocks in the same image, where target representations are computed by a learned target-encoder network.\n\nCompared to generative methods that predict in pixel/token space, I-JEPA makes use of abstract prediction targets for which unnecessary pixel-level details are potentially eliminated, thereby leading the model to learn more semantic features. Another core design choice to guide I-JEPA towards producing semantic representations is the proposed multi-block masking strategy. Specifically, we demonstrate the importance of predicting sufficiently large target blocks in the image, using an informative (spatially distributed) context block.\n\nThrough an extensive empirical evaluation, we demonstrate that:\n\n\u2022 I-JEPA learns strong off-the-shelf representations without the use of hand-crafted view augmentations (cf. Fig.1). I-JEPA outperforms pixel-reconstruction methods such as MAE [36] on ImageNet-1K linear probing, semi-supervised 1% ImageNet-1K, and semantic transfer tasks.\n\n\u2022 I-JEPA is competitive with view-invariant pretraining approaches on semantic tasks and achieves better performance on low-level visions tasks such as object counting and depth prediction (Sections 5 and 6). By using a simpler model with less rigid inductive bias, I-JEPA is applicable to a wider set of tasks.\n\n\u2022 I-JEPA is also scalable and efficient (Section 7). Pretraining a ViT-H/14 on ImageNet requires less than 1200 GPU hours, which is over 2.5\u00d7 faster than a ViT-S/16 pretrained with iBOT [79] and over 10\u00d7 more efficient than a ViT-H/14 pretrained with MAE. Predicting in representation space significantly reduces the total computation needed for self-supervised pretraining.\n\n\nBackground\n\nSelf-supervised learning is an approach to representation learning in which a system learns to capture the relationships between its inputs. This objective can be readily described using the framework of Energy-Based Models (EBMs) [49] in which the self-supervised objective is to assign a high energy to incompatible inputs, and to assign a low energy to compatible inputs. Many existing generative and non-generative approaches to self-supervised learning can indeed be cast in this framework; see Figure 2.\n\nJoint-Embedding Architectures. Invariance-based pretraining can be cast in the framework of EBMs using a Joint-Embedding Architecture (JEA), which learns to output similar embeddings for compatible inputs, x, y, and dissimilar embeddings for incompatible inputs; see Figure 2a.\n\nIn the context of image-based pretraining, compatible x, y pairs are typically constructed by randomly applying handcrafted data augmentations to the same input image [20].\n\nThe main challenge with JEAs is representation collapse, wherein the energy landscape is flat (i.e., the encoder produces a constant output regardless of the input). During the past few years, several approaches have been investi-gated to prevent representation collapse, such as contrastive losses that explicitly push apart embeddings of negative examples [15,24,37], non-contrastive losses that minimize the informational redundancy across embeddings [10,74], and clustering-based approaches that maximize the entropy of the average embedding [4,5,18]. There are also heuristic approaches that leverage an asymmetric architectural design between the x-encoder and y-encoder to avoid collapse [8,24,35].\n\nGenerative Architectures. Reconstruction-based methods for self-supervised learning can also be cast in the framework of EBMs using Generative Architectures; see Figure 2b. Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a decoder network that is conditioned on an additional (possibly latent) variable z to facilitate reconstruction. In the context of image-based pretraining, one common approach in computer vision is to produce compatible x, y pairs using masking [9,38] where x is a copy of the image y, but with some of the patches masked. The conditioning variable z then corresponds to a set of (possibly learnable) mask and position tokens, that specifies to the decoder which image patches to reconstruct. Representation collapse is not a concern with these architectures as long as the informational capacity of z is low compared to the signal y.\n\nJoint-Embedding Predictive Architectures. As shown in Figure 2c, Joint-Embedding Predictive Architectures [48] are conceptually similar to Generative Architectures; however, a key difference is that the loss function is applied in embedding space, not input space. JEPAs learn to predict the embeddings of a signal y from a compatible signal x, using a predictor network that is conditioned on an additional (possibly latent) variable z to facilitate prediction. Our proposed I-JEPA provides an instantiation of this architecture in the context of images using masking; see Figure 3. In contrast to Joint-Embedding Architectures, JEPAs do not seek representations invariant to a set of hand-crafted data augmentations, but instead seek representations that are predictive of each other when conditioned on additional information z. However, as with Joint-Embedding Architectures, representation collapse is also a concern with JEPAs; we leverage an asymmetric architecture between the xand y-encoders to avoid representation collapse.\n\n\nMethod\n\nWe now describe the proposed Image-based Joint-Embedding Predictive Architecture (I-JEPA), illustrated in Figure 3. The overall objective is as follows: given a context block, predict the representations of various target blocks The context encoder is a Vision Transformer (ViT), which only processes the visible context patches. The predictor is a narrow ViT that takes the context encoder output and, conditioned on positional tokens (shown in color), predicts the representations of a target block at a specific location. The target representations correspond to the outputs of the target-encoder, the weights of which are updated at each iteration via an exponential moving average of the context encoder weights.\n\nin the same image. We use a Vision Transformer [29,63] (ViT) architecture for the context-encoder, target-encoder, and predictor. A ViT is composed of a stack of transformer layers, each consisting of a self-attention [66] operation followed by a fully-connected MLP. Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [36] method. However, one key difference is that the I-JEPA method is non-generative and the predictions are made in representation space.\n\nTargets. We first describe how we produce the targets in the I-JEPA framework: in I-JEPA, the targets correspond to the representations of image blocks. Given an input image y, we convert it into a sequence of N non-overlapping patches, and feed this through the target-encoder f\u03b8 to obtain a corresponding patch-level representation s y = {s y1 , . . . , s y N } where s y k is the representation associated with the k th patch. To obtain the targets for our loss, we randomly sample M (possibly overlapping) blocks from the target representations s y . We denote by B i the mask corresponding of the i th block and by s y (i) = {s yj } j\u2208Bi its original context targets Figure 4. Examples of our context and target-masking strategy.\n\nGiven an image, we randomly sample 4 target blocks with scale in the range (0.15, 0.2) and aspect ratio in the range (0.75, 1.5).\n\nNext, we randomly sample a context block with scale in the range (0.85, 1.0) and remove any overlapping target blocks. Under this strategy, the target-blocks are relatively semantic, and the contextblock is informative, yet sparse (efficient to process).\n\npatch-level representation. Typically, we set M equal to 4, and sample the blocks with a random aspect ratio in the range (0.75, 1.5) and random scale in the range (0.15, 0.2). Note that the target blocks are obtained by masking the output of the target-encoder, not the input. This distinction is crucial to ensure target representations of a high semantic level; see, e.g., [8].\n\nContext. Recall, the goal behind I-JEPA is to predict the target block representations from a single context block.\n\nTo obtain the context in I-JEPA, we first sample a single block x from the image with a random scale in the range (0.85, 1.0) and unit aspect ratio. We denote by B x the mask associated with the context block x. Since the target blocks are sampled independently from the context block, there may be significant overlap. To ensure a non-trivial prediction task, we remove any overlapping regions from the context block. Figure 4 shows examples of various context and target blocks in practice. Next, the masked context block, x, is fed through the context encoder f \u03b8 to obtain a corresponding patch-level representation s x = {s xj } j\u2208Bx .\n\nPrediction. Given the output of the context encoder, s x , we wish to predict the M target block representations s y (1), . . . , s y (M ). To that end, for a given target block s y (i) corresponding to a target mask B i , the predictor g \u03d5 (\u00b7, \u00b7) takes as input the output of the context encoder s x and a mask token for each patch we wish to predict, {m j } j\u2208Bi , and outputs a patch-level prediction\u015d y (i) = {\u015d yj } j\u2208Bi = g \u03d5 (s x , {m j } j\u2208Bi ). The mask tokens are parameterized by a shared learnable vector with an added positional embedding. Since we wish to make predictions for M target blocks, we apply our predictor M times, each time conditioning on the mask tokens corresponding to the target-block locations we wish to predict, and obtain predictions\u015d y (1), . . . ,\u015d y (M ).\n\nLoss. The loss is simply the average L 2 distance between the predicted patch-level representations\u015d y (i) and the target patch-level representation s y (i); i.e.,\n1 M M i=1 D (\u015d y (i), s y (i)) = 1 M M i=1 j\u2208Bi \u2225\u015d yj \u2212 s yj \u2225 2 2 .\nThe parameters of the predictor, \u03d5, and context encoder, \u03b8, are learned through gradient-based optimization, while the parameters of the target encoder\u03b8 are updated via an exponential moving average of the context-encoder parameters. The use of an exponential moving average target-encoder has proven essential for training JEAs with Vision Transformers [18,25,79], we find the same to be true for I-JEPA.\n\n\nRelated Work\n\nA long line of work has explored visual representation learning by predicting the values of missing or corrupted sensory inputs. Denoising autoencoders use random noise as input corruption [67]. Context encoders regress an entire image region based on its surrounding [57]. Other works cast image colorization as a denoising task [46,47,77].\n\nThe idea of image denoising has recently been revisited in the context of masked image modelling [9,36,71], where a Vision Transformer [29] is used to reconstruct missing input patches. The work on Masked Autoencoders (MAE) [36] proposed an efficient architecture that only requires the encoder to process visible image patches. By reconstructing missing patches in pixels space, MAE achieves strong performance when fine-tuned end-to-end on large labeled datasets and exhibits good scaling properties. BEiT [9] predicts the value of missing patches in a tokenized space; specifically, tokenizing image patches using a frozen discreteVAE, which is trained on a dataset containing 250 million images [58]. Yet, pixel-level pre-training has been shown to outperform BEiT for fine-tuning [36]. Another work, SimMIM [71], explores reconstruction targets based on the classic Histogram of Gradients [27] feature space, and demonstrates some advantage over pixel space reconstruction. Different from those works, our representation space is learned during training through a Joint-Embedding Predictive Architecture. Our goal is to learn semantic representations that do not require extensive finetuning on downstream tasks.\n\nClosest to our work is data2vec [8] and Context Autoencoders [25]. The data2vec method learns to predict the rep- resentation of missing patches computed through an online target encoder; by avoiding handcrafted augmentations, the method can be applied to diverse modalities with promising results in vision, text and speech. Context Autoencoders use an encoder/decoder architecture optimized via the sum of a reconstruction loss and an alignment constraint, which enforces predictability of missing patches in representation space. Compared to these methods, I-JEPA exhibits significant improvements in computational efficiency and learns more semantic off-the-shelf representations. Concurrent to our work, data2vec-v2 [7] explores efficient architectures for learning with various modalities.\n\nWe also compare I-JEPA with various methods based on joint-embedding architectures; e.g., DINO [18], MSN [4] and iBOT [79]. Theses methods rely on hand-crafted data augmentations during pretraining to learn semantic image representations. The work on MSN [4], uses masking as an additional data-augmentation during pretraining, while iBOT combines a data2vec-style patch-level reconstruction loss with the DINO view-invariance loss. Common to these approaches is the need to process multiple usergenerated views of each input image, thereby hindering scalability. By contrast, I-JEPA only requires processing a single view of each image. We find that a ViT-Huge/14 trained with I-JEPA requires less computational effort than a ViT-Small/16 trained with iBOT.  Table 2.\n\nImageNet-1%.\n\nSemi-supervised evaluation on ImageNet-1K using only 1% of the available labels. Models are adapted via fine-tuning or linear-probing, depending on whichever works best for each respective method. ViT-H/16448 is pretrained at at a resolution of 448 \u00d7 448. I-JEPA pretraining outperforms MAE which also does not rely on hand-crafted data-augmentations during pretraining. Moreover, I-JEPA benefits from scale. A ViT-H/16 trained at resolution 448 surpasses previous methods including methods that leverage extra hand-crafted data-augmentations.\n\n\nImage Classification\n\nTo demonstrate that I-JEPA learns high-level representations without relying on hand-crafted data-augmentations, we report results on various image classification tasks using the linear probing and partial fine-tuning protocols. In this section, we consider self-supervised models that have been pretrained on the ImageNet-1K dataset [60]. Pretraining and evaluation implementation details are described in the Appendix A. All I-JEPA models are trained at resolution 224 \u00d7 224 pixels, unless stated otherwise.\n\nImageNet-1K. Table 1 shows performance on the common ImageNet-1K linear-evaluation benchmark. After selfsupervised pretraining, the model weights are frozen and a linear classifier is trained on top using the full ImageNet-1K training set. Compared to popular methods such as Masked Autoencoders (MAE) [36], Context Autoencoders (CAE) [22], and data2vec [8], which also do not rely on extensive hand-crafted data-augmentations during pretraining, we see that I-JEPA significantly improves linear probing performance, while using less computational effort (see section 7). By leveraging the improved efficiency of I-JEPA, we can train larger models that outperform the best CAE model while using a fraction of the compute. I-JEPA also benefits from scale; in particular, a ViT-H/16 trained at resolution 448 \u00d7 448 pixels matches the performance of view-  Table 3. Linear-probe transfer for image classification. Linearevaluation on downstream image classification tasks. I-JEPA significantly outperforms previous methods that also do not use augmentations (MAE and data2vec), and decreases the gap with the best view-invariance-based methods that leverage hand-crafted data augmentations during pretraining.\n\ninvariant approaches such as iBOT [79], despite avoiding the use of hand-crafted data-augmentations.\n\nLow-Shot ImageNet-1K. Table 2 shows performance on the 1% ImageNet benchmark. Here the idea is to adapt the pretrained models for ImageNet classification using only 1% of the available ImageNet labels, corresponding to roughly 12 or 13 images per class. Models are adapted via fine-tuning or linear-probing, depending on whichever works best for each respective method. I-JEPA outperforms MAE while requiring less pretraining epochs when using a similar encoder architecture. I-JEPA, using a ViT-H/14 architecture, matches the performance of a ViT-L/16 pretrained with data2vec [8], while using significantly less computational effort (see Section 7). By increasing the image input resolution, I-JEPA outperforms previous methods including joint-embedding methods that do leverage extra hand-crafted data-augmentations during pretraining, such as MSN [4], DINO [17], and iBOT [79].\n\nTransfer learning. Table 3 shows performance on various downstream image classification tasks using a linear probe. I-JEPA significantly outperforms previous methods that do not use augmentations (MAE and data2vec), and decreases the gap with the best view-invariance-based methods, which leverage hand-crafted data augmentations during pretraining, even surpassing the popular DINO [18] on CIFAR100 and Place205 with a linear probe.\n\n\nLocal Prediction Tasks\n\nAs demonstrated in Section 5, I-JEPA learns semantic image representations that significantly improve the downstream image classification performance of previous methods, such as MAE and data2vec. Additionally, I-JEPA benefits from scale and can close the gap, and even surpass,  Table 4. Linear-probe transfer for low-level tasks. Linearevaluation on downstream low-level tasks consisting of object counting (Clevr/Count) and depth prediction (Clevr/Dist). The I-JEPA method effectively captures low-level image features during pretraining and outperforms view-invariance based methods on tasks such object counting and depth prediction.\n\nview-invariance based methods that leverage extra handcrafted data augmentations. In this section, we find that I-JEPA also learns local image features and surpasses viewinvariance based methods on low-level and dense prediction tasks, such as object counting and depth prediction. Table 4 shows performance on various low-level tasks using a linear probe. After pretraining, the encoder weights are frozen and a linear model is trained on top to perform object-counting and depth prediction on the Clevr dataset [43]. Compared to view-invariance methods such as DINO and iBOT, the I-JEPA method effectively captures low-level image features during pretraining and outperforms them in object counting (Clevr/Count) and (by a large margin) depth prediction (Clevr/Dist).\n\n\nScalability\n\nModel Efficiency. I-JEPA is highly scalable compared to previous approaches. Figure 5 shows semi-supervised evaluation on 1% ImageNet-1K as a function of GPU hours. I-JEPA requires less compute than previous methods and achieves strong performance without relying on handcrafted data-augmentations. Compared to reconstructionbased methods, such as MAE, which directly use pixels as targets, I-JEPA introduces extra overhead by computing targets in representation space (about 7% slower time per iteration). However, since I-JEPA converges in roughly 5\u00d7 fewer iterations, we still see significant compute savings in practice. Compared to view-invariance based methods, such as iBOT, which rely on hand-crafted data augmentations to create and process multiple views of each image, I-JEPA also runs significantly faster. In particular, a huge I-JEPA model (ViT-H/14) requires less compute than a small iBOT model (ViT-S/16).\n\nScaling data size. We also find I-JEPA to benefit from pretraining with larger datasets.  learning performance on semantic and low level tasks when increasing the size of the pretraining dataset (IN1K versus IN22K). Transfer learning performance on these conceptually different tasks improves when pretraining on a larger more diverse dataset.\n\nScaling model size. \n\n\nPredictor Visualizations\n\nThe role of the predictor in I-JEPA is to take the output of the context encoder and, conditioned on positional mask tokens, to predict the representations of a target black at the location specified by the mask tokens. One natural question is whether the predictor conditioned on the positional mask tokens is learning to correctly capture positional uncertainty in the target. To qualitatively investigate this question, we visualize the outputs of the predictor. We use the following visualization approach to enable the research community to independently reproduce our findings. After pretraining, we freeze the context-encoder and predictor weights, and train a decoder following the RCDM framework [13] to map the average-pool of the predictor outputs back to pixel space. Figure 6 shows decoder outputs for various random seeds. Qualities that are common across samples represent information that is contained in the average-pooled predictor representation. The I-JEPA predictor correctly captures positional uncertainty and produces high-level object parts with the correct pose (e.g., back of the bird and top of the car).\n\n\nAblations\n\nPredicting in representation space. Table 7 compares low-shot performance on 1% ImageNet-1K using a linear probe when the loss is computed in pixel-space versus representation space. We conjecture that a crucial component of I-JEPA is that the loss is computed entirely in representation space, thereby giving the target encoder the ability to produce abstract prediction targets, for which irrelevant pixel-level details are eliminated. From Table 7, it is clear that predicting in pixel-space leads to a significant degradation in the linear probing performance.\n\nMasking strategy. Table 6 compare our multi-block masking with other masking strategies such as rasterized masking, where the image is split into four large quadrants, and the goal is to use one quadrant as a context to predict the other three quadrants, and the traditional block and random masking typically used in reconstruction-based methods. In block masking, the target is a single image block and the context is the Figure 6. Visualization of I-JEPA predictor representations. For each image: first column contains the original image; second column contains the context image, which is processed by a pretrained I-JEPA ViT-H/14 encoder. Green bounding boxes in subsequent columns contain samples from a generative model decoding the output of the pretrained I-JEPA predictor, which is conditioned on positional mask tokens corresponding to the location of the green bounding box. Qualities that are common across samples represent information that contained is in the I-JEPA prediction. The I-JEPA predictor is correctly capturing positional uncertainty and producing high-level object parts with a correct pose (e.g., the back of the bird and top of a car). Qualities that vary across samples represent information that is not contained in the representation. In this case, the I-JEPA predictor discards the precise low-level details as well as background information.  Table 6. Ablating masking strategy. Linear evaluation on ImageNet-1K using only 1% of the available labels after I-JEPA pretraining of a ViT-B/16 for 300 epochs. Comparison of proposed multi-block masking strategy. In rasterized masking the image is split into four large quadrants; one quadrant is used as a context to predict the other three quadrants. In block masking, the target is a single image block and the context is the image complement. In random masking, the target is a set of random image patches and the context is the image complement. The proposed multi-block masking strategy is helpful for guiding I-JEPA to learn semantic representations.\n\n\nTargets\n\n\nArch. Epochs Top-1\n\nTarget-Encoder Output ViT-L/16 500\n\n\n66.9\n\nPixels ViT-L/16 800 40.7 Table 7. Ablating targets. Linear evaluation on ImageNet-1K using only 1% of the available labels. The semantic level of the I-JEPA representations degrades significantly when the loss is applied in pixel space, rather than representation space, highlighting the importance of the target-encoder during pretraining.\n\nimage complement. In random masking, the target is a set of random patches and the context is the image complement. Note that there is no overlap between the context and target blocks in all considered strategies. We find multi-block masking helpful for guiding I-JEPA to learning semantic representations. Additional ablations on multi-block masking can be found in Appendix C.\n\n\nConclusion\n\nWe proposed I-JEPA, a simple and efficient method for learning semantic image representations without relying on hand-crafted data augmentations. We show that by predicting in representation space, I-JEPA converges faster than pixel reconstruction methods and learns representations of a high semantic level. In contrast to view-invariance based methods, I-JEPA highlights a path for learning general representations with joint-embedding architectures, without relying on hand-crafted view augmentations.\n\n\nA. Implementation Details\n\n\nA.1. Pretraining\n\nArchitectures. For I-JEPA pretraining, we use Vision Transformer [29] (ViT) architectures for the context-encoder, targetencoder, and the predictor. While the context-encoders and target-encoders correspond to standard ViT architectures, the predictor is designed as a light-weight (narrow) ViT architecture. Specifically, we fix the embedding dimension of the predictor to 384, while keeping the number of self-attention heads equal to that of the backbone context-encoder. For the smaller ViT-B/16 context-encoder, we set the depth of the predictor to 6. For ViT-L/16, ViT-H/16, and ViT-H/14 contextencoders, we set the depth of the predictor to 12. Finally, the ViT-G/16 uses a predictor of depth 16. I-JEPA is pretrained without a [cls] token. We use the target-encoder for evaluation and average pool its output to produce a global image representation.\n\nOptimization. We use AdamW [51] to optimize the context-encoder and predictor weights. Our default batch-size is 2048, and the learning rate is linearly increased from 10 \u22124 to 10 \u22123 during the first 15 epochs of pretraining, and decayed to 10 \u22126 following a cosine schedule thereafter. Following [4,18], the weight-decay is linearly increased from 0.04 to 0.4 throughout pretraining. The target-encoder weights are identical to the context-encoder weights at initialization, and updated via an exponential moving average thereafter [4,18,23,35,37,61]. We use a momentum value of 0.996, and linearly increase this value to 1.0 throughout pretraining, following [4,18].\n\nMasking. By default, we sample 4 possibly overlapping target blocks masks with random scale in the range (015, 0.2) and aspect ratio in the range (0.75, 1.5). We sample 1 context block mask with random scale in the range (0.85, 1.0) and unit aspect ratio. We subsequently eliminate any regions in the context block mask that overlap with any of the 4 target block masks. The context-block mask and target-block masks are sampled independently for each image in the mini-batch. To ensure efficient batch processing, we restrict the size of all context masks on a co-located GPU to be identical. Similarly, we restrict the size of all target masks on a co-located GPU to be identical. The mask-sampler is efficiently implemented in only a few lines of code in PyTorch [56] using a batch-collator function, which runs in the data loader processes. In short, in each iteration, the data loader returns a mini-batch of images and a set of context and target masks for each image, identifying the patch indices to keep for the context and target views.\n\n\nA.2. Downstream Tasks\n\nLinear evaluation. When evaluating methods such as iBOT [79], DINO [18] or MAE [36], which leverage Vision Transformers [29] with an additional [cls] token, we use the default configurations of VISSL [34] to evaluate all the models on iNaturalist18 [65], CIFAR100 [45], Clevr/Count [42,75], Clevr/Dist [42,75], and Places205 [78]. We freeze the encoder and return the best number among the following representations: 1) the [cls] token representation of the last layer, 2) the concatenation of the last 4 layers of the [cls] token. For each representation, we try two different heads: 1) a linear head, or 2) a linear head preceded by a batch normalization, and return the best number. We use the default data augmentations of VISSL [34]: random resize cropping and horizontal flipping, with the exception of Clevr/Count and Clevr/Dist, where we only use center crop and horizontal flipping, as random cropping interferes with the capability of counting objects and estimating distance, removing critical objects from the scene. For CIFAR100, we resize the images to 224 \u00d7 224 pixels, so as to keep the number of patches equal to that used during pretraining.\n\nBecause our I-JEPA implementation uses Vision Transformer architectures without a [cls] token, we adapt the default VISSL evaluation recipe to utilize the average-pooled patch representation instead of the [cls] token. We therefore report the best linear evaluation number among the following representations: 1) the average-pooled patch representation of the last layer, 2) the concatenation of the last 4 layers of the average-pooled patch representations. We otherwise keep the linear-probing recipe identical.\n\nImageNet evaluations. To evaluate the I-JEPA on ImageNet [60], we adapt the VISSL recipe to use average pooled representations instead of the [cls] token. Following MAE [36], we use the LARS [72] optimizer with a batch-size of 16384, and train the linear probe for 50 epochs. We use a learning rate with a step-wise decay, dividing it by a factor of 10 every 15 epochs, and sweep three different reference learning rates [0.01, 0.05, 0.001], and two weight decay values [0.0005, 0.0].\n\nLow-shot evaluation. To evaluate our model on the ImageNet-1% low-shot task, we adapt the fine-tuning protocol of MAE [36].We fine-tune our ViT-L/H models for 50 epochs on ImageNet-1% with the AdamW optimizer and a cosine learning rate scheduler. We use a batch size of 512, a learning rate layer decay of 0.75 and 0.1 label smoothing. We use the default randaugment data-augmentations as in MAE. In contrast to the fine-tuning done with MAE, we do not use mixup, cutmix, random erasing or drop path. For the I-JEPA, we use a learning rate /weight decay of 3e \u22125 /5e \u22122 for the ViT-L/16, 3e \u22125 /4e \u22121 for the ViT-H/14 and 3e \u22125 /4e \u22121 for the ViT-H/16 448 . Similar fine-tuning strategy for low-shot learning has been explored by Semi-VIT in the context of semi-supervised learning [16].\n\n\nB. Broader Related Work\n\nSelf-supervised learning of visual representations with joint-embedding architectures is an active line of research [3,10,12,18,23,24,35,37,54,69,79]. These approaches train a pair of encoders to output similar embeddings for two or more views of the same image. To avoid pathological solution, many popular joint-embedding approaches use explicit regularization [5,10,18,20] or architectural constraints [24,35]. Collapse-prevention based on architectural constraints leverage specific network design choices to avoid collapse, for example, by stopping the gradient flow in one of the jointembedding branches [20], using a momentum encoder in one of the joint-embedding branches [35], or using an asymmetric prediction head [8,20,35]. Recent work [62] attempts to theoretically understand (in certain simplified settings) how jointembedding methods with architectural constraints avoid representation collapse without explicit regularization.\n\nTypical regularization-based approaches to collapse prevention in joint-embedding architectures try to maximize the volume of space occupied by the representations. This is often motivated through the InfoMax [52] principle. Indeed, a longstanding conviction in unsupervised representation learning is that the resulting representations should be both maximally informative about the inputs, while also satisfying certain simplicity constraints [33,50]. The former objective is often referred to as the information-maximization principle (InfoMax), while the latter is sometimes referred to as the parsimony principle [52]. Such approaches to representation learning have been proposed for decades (e.g., [14]), where, historically, simplicity constraints were enforced by encouraging the learned representations to be sparse, low-dimensional, or disentangled, i.e., the individual dimensions of the representation vector should be statistically independent [33]. Modern approaches enforce the simplicity constraints coupled with InfoMax regularization through self-supervised loss terms [6,40,41,44,55,64]. One example is the widespread view-invariance penalty [53], often coupled with with independence [10,74] or low-dimensionality constraints, e.g., by projecting representations on the unit hypersphere [20,35,37]. However, despite its proliferation, there have also been many criticisms of the InfoMax principle, especially since it is does not discriminate between different types of information (e.g, noise and semantics) [2]. Indeed, the sets of features we wish the model to capture are not always those with the highest marginal entropy (maximal information content).\n\nOrthogonal to the contributions of invariance-based pretraining, another line of work attempts to learn representations by artificially masking parts of the input and training a network to reconstruct the hidden content [67]. Autoregressive models, and denoising autoencoders in particular, predict clean visual inputs from noisy views [8,9,19,36,67]. Typically, the goal is to predict missing inputs at a pixel level [29,36,70], or at a patch token-level, using a tokenizer [9,68]. While these works demonstrate impressive scalability, they usually learn features at a low-level of semantic abstraction compared to joint-embedding approaches [4].\n\nMore recently, a set of approaches attempt to combine both joint-embedding architectures and reconstruction based approaches [30], wherein they combine an invariance pretraining loss with a patch-level reconstruction loss, as in the iBOT method [79]. Since view-invariance based approaches are typically biased towards learning global image representations, thereby limiting their applicability to other computer vision tasks, the idea is that adding local loss terms can improve performance on other popular tasks in computer vision [11,26,32]. The framework of contrastive predictive coding [55] is also closely related to this line of work on local loss terms. In the context of images [39], here the idea is to use a contrastive objective combined with a convolutional network to discriminate between overlapping image patch representations. Specifically, the goal is to encourage the representations of an image patch to be predictable of the image patches directly below it, while pushing away the representations of other patch views. In contrast to that work, the proposed I-JEPA method is non-contrastive and does not seek to discriminate between image patches. Rather, the goal is to predict the representations of various target blocks from a single context block. This is achieved with a Joint-Embedding Predictive Architecture, using a predictor network that is conditioned on positional embeddings corresponding to the location of the target block in the image. Qualitative experiments in Section 8 show that the predictor network in our architecture learns to correctly perform this local-to-local region feature mapping, and learns to correctly capture positional uncertainty in the image.  Table 8. Ablation of the target block size for multi-block masking. Linear evaluation on 1% ImageNet-1K (using only 1% of the available labels); ablating the multi-block target size during I-JEPA pretraining of a ViT-B/16 for 300 epochs. Predicting larger (semantic) blocks improves the low-shot accuracy as long as the context is sufficiently informative.  Table 9. Ablation of the context size for multi-block masking. Linear evaluation on 1% ImageNet-1K (using only 1% of the available labels); ablating the multi-block target size during I-JEPA pretraining of a ViT-B/16 for 300 epochs. Reducing the multi-block context size degrades the low-shot performance.  Table 10. Ablation of the targets number for multi-block masking. Linear evaluation on 1% ImageNet-1K (using only 1% of the available labels); ablating the multi-block number of targets during I-JEPA pretraining of a ViT-B/16 for 300 epochs. Increasing the number of target blocks improve the low-shot accuracy.\n\n\n54.2\n\n\n54.2\n\n\nC. Additional Ablations\n\nThis section follows the same experimental protocol as Section 9. We report the result of a linear probe with a frozen backbone, trained on the low-shot 1% ImageNet-1K benchmark.\n\nMultiblock masking strategy. We present an extended ablation of the multiblock masking strategy where we change the targets block scale (Table 8), the context scale (Table 9) and the number of target blocks (Table 10). We train a ViT-B/16 for 300 epochs using I-JEPA with various multi-block settings and compare performance on the 1% ImageNet-1K benchmark using a linear probe. In short, we find that it is important to predict several relatively large (semantic) target blocks, and to use a sufficiently informative (spatially distributed) context block.\n\nMasking at the output of the target-encoder. An important important design choice in I-JEPA is that the target blocks are obtained by masking the output of the target-encoder, not the input. Table 11 shows the effect of this design choice on the semantic level of the learned representations when pretraining a ViT-H/16 using I-JEPA for 300 epochs. In the case where masking is applied to the input, we forward-propagate through the target-encoder once for each target region. Masking the output of the target-encoder during pretraining results in more semantic prediction targets and improves linear probing performance.  Table 11. Ablating masking output of target encoder. Linear evaluation on ImageNet-1K using only 1% of the available labels; ablating the effect of masking the target-encoder output during I-JEPA pretraining of a ViT-H/16 for 300 epochs. Masking the output of the target-encoder during pretraining significantly improves the linear probing performance of the pretrained representations.\n\nPredictor depth. We examine the impact of the predictor depth on the downstream low-shot performance in Table 12.\n\nWe pretrain a ViT-L/16 for 500 epochs using either a 6-layer predictor network or a 12-layer predictor network. The model pretrained using a deeper predictor shows a significant improvement in downstream low-shot performance compared to the model pretrained with a shallower predictor. Weight decay. In Table 13, we evaluate the impact of weight-decay during pretraining. We explore two weight decay strategies: linearly increase the weight-decay from 0.04 to 0.4 or use a fix weight-decay of 0.05. Using a smaller weight decay during pretraining improves the downstream performance on ImageNet-1% when fine-tuning. However, this also leads to a degradation of performance in linear evaluation. In the main paper, we use the first weight decay strategy as it improves the performances in linear evaluation downstream tasks.  Table 13. Ablating the pretraining weight-decay. We compare our default pretraining weight decay strategy where we linearly increase the weight-decay from 0.04 to 0.4 to using a fix weight decay of 0.05. Using a smaller weight-decay during pretraining can improve the fine-tuning performance on ImageNet-1%, However, it also leads to a drop of performance in linear evaluation.\n\nPredictor width. We explore the impact of the predictor width in Table 14. We compare I-JEPA using a ViT-L encoder and a predictor with 386 channels to a similar model using a predictor with 1024 channels. Note that the ViT-L encoder has 1024 channels. Using a bottleneck in the predictor width improves the downstream performance on ImageNet 1%. \n\n\nD. Finetuning on the full ImageNet\n\nIn this section, we report performance on I-JEPA when fine-tuning on the full ImageNet dataset. We focus on the ViT-H/16 448 as this architecture achieves state-of-art performance with MAE [36].\n\nWe use a fine-tuning protocol similar to MAE. Specifically, we fine-tune our model for 50 epochs using AdamW and a cosine learning rate schedule. The base learning rate is set to 10 \u22124 and the batch size to 528. We train using mixup [76] set to 0.8, cutmix [73] set to 1.0, a drop path probability of 0.25 and a weight decay set to 0.04. We also use a layer decay of 0.75. Finally, we use the same rand-augment data-augmentations as MAE, Table 15 reports the fine-tuning results. I-JEPA achieves 87.1 top-1 accuracy. Its performance is less than 1% away from the best MAE model despite I-JEPA being trained for 5.3 times less epochs than MAE. This result demonstrates that I-JEPA is competitive when fine-tuning on the full ImageNet dataset.\n\n\nMethod\n\nArch. \n\n\nE. RCDM Visualizations\n\nTo visualize the representations of a pretrained neural network in pixel space, we use the RCDM framework [13]. The RCDM framework trains a decoder network h \u03c9 , comprising a generative diffusion model, to reconstruct an image x from the representation vector of that image s x and a noisy version of that imagex := x + \u03f5, where \u03f5 is an additive noise vector. Concretely, the decoder objective is to minimize the loss function \u2225h \u03c9 (x, s x )\u2212\u03f5\u2225. We train each RCDM network for 300,000 iterations using the default hyperparameters [13]. After training the decoder, one can subsequently feed the representation vector of an unseen test image s y into the decoder along with various random noise vectors to generate several pixel-level visualizations of the representation, thus providing insight into the features captured in the representations of the pretrained network. Qualities that are common across samples represent information that is contained in the representation. On the other hand, qualities that vary across samples represent information that is not contained in the representations In Figure 6, the visualizations are obtained by feeding the average-pooled output of the predictor, conditioned on a specific target region, into the decoder network, along with various random noise vectors. In Figures 7 and 8, the visualizations are obtained by feeding the average-pooled output of the target-encoder into the decoder network, along with various random noise vectors.\n\n\nE.1. Encoder Visualization\n\nIn Figure 7, we visualize the average-pooled I-JEPA representations at the output of our ViT-H/14 target-encoder. The first column contains the original image, while subsequent columns contain synthetic samples obtained by feeding the averagepooled representation of the image into the decoder along with various random noise vectors. Figure 7 suggests that the I-JEPA target-encoder is able to correctly capture the high-level information regarding objects and their poses, while discarding low-level image details and background information. Figure 8 shows similar visualizations, but when using an MSN [4] pretrained ViT-L/7 target-encoder to compute the image representations. The MSN method trains a context-and target-encoder using a Joint-Embedding Architecture to enforce invariance of global image representations to various hand crafted data augmentations and missing patches. While the MSN pretrained network is able to capture high level semantic information about the image in the first column, it also exhibits higher variability in the generated samples, e.g., variability in the object pose, object scale, and number of instances. In short, the MSN pretrained discards much of the local structure in the image, which is in stark contrast to I-JEPA, which retains information about much of the local structure in the input image.   [4]. Qualities that are common across samples represent information that is contained in the representation. Qualities that vary across samples represent information that is not captured by MSN. Compared to I-JEPA, MSN samples show higher variability. MSN retains less information from the input. In particular, it discards global structure information such as the object pose or even number of instances.\n\nFigure 1 .\n1ImageNet Linear Evaluation. The I-JEPA method learns semantic image representations without using any view data augmentations during pretraining. By predicting in representation space, I-JEPA produces semantic representations while using less compute than previous methods.\n\nFigure 2 .\n2Figure 2. Common architectures for self-supervised learning, in which the system learns to capture the relationships between its inputs. The objective is to assign a high energy (large scaler value) to incompatible inputs, and to assign a low energy (low scaler value) to compatible inputs. (a) Joint-Embedding Architectures learn to output similar embeddings for compatible inputs x, y and dissimilar embeddings for incompatible inputs. (b) Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a decoder network that is conditioned on additional (possibly latent) variables z to facilitate reconstruction. (c) Joint-Embedding Predictive Architectures learn to predict the embeddings of a signal y from a compatible signal x, using a predictor network that is conditioned on additional (possibly latent) variables z to facilitate prediction.\n\nFigure 3 .\n3I-JEPA. The Image-based Joint-Embedding Predictive Architecture uses a single context block to predict the representations of various target blocks originating from the same image.\n\nFigure 5 .\n5Scaling. Semi-supervised evaluation on ImageNet-1K 1% as a function of pretraining GPU hours. I-JEPA requires less compute than previous methods to achieve strong performance. Compared to MAE and data2vec, I-JEPA obtains a significant speedup by requiring fewer pretraining epochs. Compared to iBOT, which relies on hand-crafted data-augmentations, a huge I-JEPA model (ViT-H/14) requires less compute than their smallest model (ViT-S/16).\n\nFigure 7 .\n7Visualization of I-JEPA target-encoder representations. For each image: first column contains the original image; subsequent columns contain samples from a generative model decoding the average-pooled output of a pretrained I-JEPA target-encoder. Qualities that are common across samples represent information that contained is in the I-JEPA representation. I-JEPA is able to correctly capture the high-level information regarding objects and their poses. Qualities that vary across samples represent information that is not contained in the representation. I-JEPA encoder discards the precise low-level details as well as background information.\n\nFigure 8 .\n8Visualization of MSN target-encoder representations. For each image: first column contains the original image; subsequent columns contain samples from a generative model decoding the output of a frozen MSN encoder\n\nTable 5 shows transfer\n5Pretrain Arch. \n\nCIFAR100 Place205 INat18 Clevr/Count Clevr/Dist \n\nIN1k \nViT-H/14 \n\n87.5 \n58.4 \n47.6 \n86.7 \n72.4 \n\nIN22k \nViT-H/14 \n\n89.5 \n57.8 \n50.5 \n88.6 \n75.0 \n\nIN22k \nViT-G/16 \n\n89.5 \n59.1 \n55.3 \n86.7 \n73.0 \n\nTable 5. Ablating dataset and model size. Evaluating impact of pre-training dataset size and model size on transfer tasks. I-JEPA \nbenefits from larger more diverse datasets. When increasing the size of the pretraining dataset (IN1k versus IN22k) we see an performance \nimprovement for the ViT-H/14 model. We observe a further performance improvement on semantic tasks by training a larger model ViT-\nG/16 model on ImageNet-22k. The ViT-H/14 is trained for 300 epochs on IN1k and the equivalent of 900 IN1K epochs on IN22k. The \nViT-H/16 is trained for the equivalent of 600 IN1k epochs. \n\n\n\nTable 5\n5also shows that I-JEPA ben-\nefit from larger model size when pretraining on IN22K. \nPretraining a ViT-G/16 significantly improves the down-\nstream performances on image classification tasks such as \nPlace205 and INat18 compared to a ViT-H/14 model, but \ndoes not improve performance on low-level downstream \ntasks -the ViT-G/16 uses larger input patches, which can \nbe detrimental for the local prediction tasks. \n\n\n\n\nAvg. Ratio is the average number of patches in the context block relative to the total number of patches in the image.Targets \nContext \n\nMask \nType \nFreq. Type \nAvg. Ratio  *  Top-1 \n\nmulti-block Block(0.15, 0.2) \n\n4 \nBlock(0.85, 1.0) \u00d7 Complement \n\n0.25 \n54.2 \n\nrasterized \nQuadrant \n3 \nComplement \n\n0.25 \n15.5 \n\nblock \nBlock(0.6) \n1 \nComplement \n\n0.4 \n20.2 \n\nrandom \nRandom(0.6) \n1 \nComplement \n\n0.4 \n17.6 \n\n *  \n\n\nTarget Masking Arch.Epochs Top-1Output \nViT-H/16 \n\n300 \n67.3 \n\nInput \nViT-H/16 \n\n300 \n56.1 \n\n\n\n\nTable 12. Ablating the predictor depth. Linear evaluation on ImageNet-1K using only 1% of the available labels; ablating the effect of masking the predictor depth for a ViT-L/16 pretrained for 500 epochs. Increasing the predictor depth leads to significant improvement of the linear probe performance of the pretrained representations.Predictor Depth Arch. \nEpochs Top-1 \n\n6 \nViT-L/16 \n\n500 \n64.0 \n\n12 \nViT-L/16 \n\n500 \n66.9 \n\n\n\n\nWeight Decay Arch.Epochs ImageNet-1% ImageNet Linear-Eval \n\n0.04 \u2192 0.4 \nViT-L/16 \n\n600 \n69.4 \n77.8 \n\n0.05 \nViT-L/16 \n\n600 \n70.7 \n76.4 \n\n\n\n\nTable 14. Ablating the predictor width. We reports results on ImageNet-1K 1% using fine-tuning. We compare two predictors having a width of either 384 or 1024. Note the I-JEPA encoder is a ViT-L with 1024 channels. Having a width bottleneck in the predictor improves the downstream performances.Predictor Width Arch. \nEpochs Top-1 \n\n384 \nViT-L/16 \n\n600 \n70.7 \n\n1024 \nViT-L/16 \n\n600 \n68.4 \n\n\n\n\nTable 15. Finetuning on the full ImageNet dataset. I-JEPA achieves competitive performance. I-JEPA is close to MAE approach despite I-JEPA being trained for 5.3 times less epochs than MAE.Epochs Top-1 \n\nMAE [36] ViT-H/14448 \n\n1600 \n87.8 \n\nI-JEPA \nViT-H/16448 \n\n300 \n87.1 \n\n\n\nSelf-labelling via simultaneous clustering and representation learning. Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi, Internatinoal Conference on Learning Representations. 20201Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and rep- resentation learning. Internatinoal Conference on Learning Representations, 2020. 1\n\nThe hidden uniform cluster prior in self-supervised learning. International Conference on Learning Representations. Mahmoud Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Nicolas Ballas, 113Mahmoud Assran, Randall Balestriero, Quentin Duval, Flo- rian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, and Nicolas Ballas. The hidden uniform cluster prior in self-supervised learning. International Con- ference on Learning Representations, 2023. 1, 13\n\nSupervision accelerates pre-training in contrastive semi-supervised learning of visual representations. Mahmoud Assran, Nicolas Ballas, NeurIPS Workshop on Self-Supervised Learning. Lluis Castrejon, and Michael RabbatMahmoud Assran, Nicolas Ballas, Lluis Castrejon, and Michael Rabbat. Supervision accelerates pre-training in con- trastive semi-supervised learning of visual representations. NeurIPS Workshop on Self-Supervised Learning, 2020. 13\n\nMasked siamese networks for label-efficient learning. Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas, European Conference on Computer Vision. 1617Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo- janowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese net- works for label-efficient learning. European Conference on Computer Vision, 2022. 1, 2, 3, 5, 6, 12, 13, 16, 17\n\nNicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by nonparametrically predicting view assignments with support samples. Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, IEEE/CVF International Conference on Computer Vision. 313Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo- janowski, Armand Joulin, Nicolas Ballas, and Michael Rab- bat. Semi-supervised learning of visual features by non- parametrically predicting view assignments with support samples. IEEE/CVF International Conference on Computer Vision, 2021. 3, 13\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, Advances in neural information processing systems. 32Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019. 13\n\nEfficient self-supervised learning with contextualized target representations for vision, speech and language. Alexei Baevski, Arun Babu, Wei-Ning Hsu, Michael Auli, arXiv:2212.07525arXiv preprintAlexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. arXiv preprint arXiv:2212.07525, 2022. 5\n\nData2vec: A general framework for self-supervised learning in speech, vision and language. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli, arXiv:2202.03555613arXiv preprintAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Ji- atao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022. 1, 3, 4, 5, 6, 13\n\nBeit: Bert pre-training of image transformers. Hangbo Bao, Li Dong, Furu Wei, arXiv:2106.0825413arXiv preprintHangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 1, 3, 4, 13\n\nVicreg: Variance-invariance-covariance regularization for selfsupervised learning. Adrien Bardes, Jean Ponce, Yann Lecun, arXiv:2105.04906113arXiv preprintAdrien Bardes, Jean Ponce, and Yann LeCun. Vi- creg: Variance-invariance-covariance regularization for self- supervised learning. arXiv preprint arXiv:2105.04906, 2021. 1, 3, 13\n\nVicregl: Selfsupervised learning of local visual features. Adrien Bardes, Jean Ponce, Yann Lecun, arXiv:2210.01571113arXiv preprintAdrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Self- supervised learning of local visual features. arXiv preprint arXiv:2210.01571, 2022. 1, 13\n\nGuillotine regularization: Improving deep networks generalization by removing their head. Florian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, Pascal Vincent, arXiv:2206.13378arXiv preprintFlorian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, and Pascal Vincent. Guillotine regulariza- tion: Improving deep networks generalization by removing their head. arXiv preprint arXiv:2206.13378, 2022. 13\n\nHigh fidelity visualization of what your self-supervised representation knows about. Florian Bordes, Randall Balestriero, Pascal Vincent, Transactions on Machine Learning Research. 716Florian Bordes, Randall Balestriero, and Pascal Vincent. High fidelity visualization of what your self-supervised rep- resentation knows about. Transactions on Machine Learning Research, 2022. 7, 16\n\nUnsupervised classifiers, mutual information and'phantom targets. John Bridle, Anthony Heading, David Mackay, Advances in neural information processing systems. 4John Bridle, Anthony Heading, and David MacKay. Un- supervised classifiers, mutual information and'phantom tar- gets. Advances in neural information processing systems, 4, 1991. 13\n\nSignature verification using a \"siamese\" time delay neural network. Jane Bromley, W James, L\u00e9on Bentz, Isabelle Bottou, Yann Guyon, Cliff Lecun, Eduard Moore, Roopak S\u00e4ckinger, Shah, International Journal of Pattern Recognition and Artificial Intelligence. 7043Jane Bromley, James W Bentz, L\u00e9on Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard S\u00e4ckinger, and Roopak Shah. Signature verification using a \"siamese\" time delay neural network. International Journal of Pattern Recognition and Artificial Intelligence, 7(04):669-688, 1993. 1, 3\n\nSemi-supervised vision transformers at scale. Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, Stefano Soatto, arXiv:2208.05688arXiv preprintZhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, and Stefano Soatto. Semi-supervised vision transformers at scale. arXiv preprint arXiv:2208.05688, 2022. 13\n\nUnsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, arXiv:2006.0988216arXiv preprintMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020. 1, 6\n\nEmerging properties in self-supervised vision transformers. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin, arXiv:2104.142941213arXiv preprintMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021. 1, 3, 4, 5, 6, 12, 13\n\nGenerative pretraining from pixels. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, PMLR, 2020. 13International Conference on Machine Learning. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee- woo Jun, David Luan, and Ilya Sutskever. Generative pre- training from pixels. In International Conference on Ma- chine Learning, pages 1691-1703. PMLR, 2020. 13\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.0570913preprintTing Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. preprint arXiv:2002.05709, 2020. 1, 2, 13\n\nBig self-supervised models are strong semi-supervised learners. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, arXiv:2006.10029arXiv preprintTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised mod- els are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020. 5\n\nContext autoencoder for self-supervised representation learning. Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, Jingdong Wang, arXiv:2202.03026arXiv preprintXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022. 5\n\nXinlei Chen, Haoqi Fan, arXiv:2003.04297Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. 1213arXiv preprintXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. 12, 13\n\nExploring simple siamese representation learning. Xinlei Chen, Kaiming He, arXiv:2011.10566113arXiv preprintXinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint arXiv:2011.10566, 2020. 1, 3, 13\n\nAn empirical study of training self-supervised vision transformers. Xinlei Chen, Saining Xie, Kaiming He, arXiv:2104.02057arXiv preprintXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. 4\n\nIntra-instance vicreg: Bag of self-supervised image patch embedding. Yubei Chen, Adrien Bardes, Zengyi Li, Yann Lecun, arXiv:2206.08954arXiv preprintYubei Chen, Adrien Bardes, Zengyi Li, and Yann LeCun. Intra-instance vicreg: Bag of self-supervised image patch embedding. arXiv preprint arXiv:2206.08954, 2022. 13\n\nHistograms of oriented gradients for human detection. Navneet Dalal, Bill Triggs, 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05). Ieee1Navneet Dalal and Bill Triggs. Histograms of oriented gra- dients for human detection. In 2005 IEEE computer soci- ety conference on computer vision and pattern recognition (CVPR'05), volume 1, pages 886-893. Ieee, 2005. 4\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.11929Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. 1213arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, 4, 12, 13\n\nAre large-scale datasets necessary for self-supervised pre-training?. Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv\u00e9 Jegou, Edouard Grave, arXiv:2112.10740arXiv preprintAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Herv\u00e9 Jegou, and Edouard Grave. Are large-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740, 2021. 13\n\nA theory of cortical responses. Karl Friston, Philosophical transactions of the Royal Society B: Biological sciences. 3601Karl Friston. A theory of cortical responses. Philosophi- cal transactions of the Royal Society B: Biological sciences, 360(1456):815-836, 2005. 1\n\nLearning representations by predicting bags of visual words. Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, Matthieu Cord, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSpyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord. Learning representations by predicting bags of visual words. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6928-6938, 2020. 13\n\nDeep learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT pressIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. 13\n\n. Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu, Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr Bojanowski, Armand Joulin, Ishan Misra, 2021. 12Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu, Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Ishan Misra. Vissl. https://github.com/ facebookresearch/vissl, 2021. 12\n\nBootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, H Pierre, Elena Richemond, Carl Buchatskaya, Bernardo Doersch, Zhaohan Daniel Avila Pires, Mohammad Gheshlaghi Guo, Azar, arXiv:2006.077331213arXiv preprintJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do- ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham- mad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. 1, 3, 5, 12, 13\n\nMasked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1516Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 1, 2, 3, 4, 5, 6, 12, 13, 15, 16\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, arXiv:1911.057221213arXiv preprintKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre- sentation learning. arXiv preprint arXiv:1911.05722, 2019. 1, 3, 12, 13\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016. 3\n\nData-efficient image recognition with contrastive predictive coding. Olivier Henaff, PMLR, 2020. 13International conference on machine learning. Olivier Henaff. Data-efficient image recognition with con- trastive predictive coding. In International conference on machine learning, pages 4182-4192. PMLR, 2020. 13\n\nLearning deep representations by mutual information estimation and maximization. Alex R Devon Hjelm, Samuel Fedorov, Karan Lavoie-Marchildon, Phil Grewal, Adam Bachman, Yoshua Trischler, Bengio, arXiv:1808.06670arXiv preprintR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual in- formation estimation and maximization. arXiv preprint arXiv:1808.06670, 2018. 13\n\nLearning discrete representations via information maximizing self-augmented training. Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, Masashi Sugiyama, PMLR, 2017. 13In International conference on machine learning. Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. In In- ternational conference on machine learning, pages 1558- 1567. PMLR, 2017. 13\n\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition12Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901-2910, 2017. 12\n\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, CVPR. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elemen- tary visual reasoning. In CVPR, 2017. 6\n\nDiscriminative clustering by regularized information maximization. Andreas Krause, Pietro Perona, Ryan Gomes, Advances in neural information processing systems. 2313Andreas Krause, Pietro Perona, and Ryan Gomes. Dis- criminative clustering by regularized information maximiza- tion. Advances in neural information processing systems, 23, 2010. 13\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, 12Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 12\n\nLearning representations for automatic colorization. Gustav Larsson, Michael Maire, Gregory Shakhnarovich, Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. 2016. 4\n\nColorization as a proxy task for visual understanding. Gustav Larsson, Michael Maire, Gregory Shakhnarovich, Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as a proxy task for visual understanding. 2017. 4\n\nA path towards autonomous machine intelligence version 0.9. Yann Lecun, 23Yann LeCun. A path towards autonomous machine intelli- gence version 0.9. 2, 2022-06-27. 2022. 2, 3\n\n. Yann Lecun, Sumit Chopra, Raia Hadsell, M Ranzato, Fujie Huang, A tutorial on energy-based learning. Predicting structured data. 10Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006. 2\n\nSelf-organization in a perceptual network. Ralph Linsker, Computer. 213Ralph Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988. 13\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.0510112Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12\n\nOn the principles of parsimony and self-consistency for the emergence of intelligence. Yi Ma, Doris Tsao, Heung-Yeung Shum, Frontiers of Information Technology & Electronic Engineering. 13Yi Ma, Doris Tsao, and Heung-Yeung Shum. On the prin- ciples of parsimony and self-consistency for the emergence of intelligence. Frontiers of Information Technology & Elec- tronic Engineering, pages 1-26, 2022. 13\n\nSelf-supervised learning of pretext-invariant representations. Ishan Misra, Laurens Van Der Maaten, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6707-6717, 2020. 13\n\nRepresentation learning via invariant causal mechanisms. Jovana Mitrovic, Brian Mcwilliams, Jacob Walker, Lars Buesing, Charles Blundell, International Conference on Learning Representations. Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles Blundell. Representation learning via invariant causal mechanisms. International Conference on Learning Representations, 2021. 13\n\nRepresentation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 13\n\nPytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, 3212Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im- perative style, high-performance deep learning library. Ad- vances in neural information processing systems, 32, 2019. 12\n\nContext encoders: Feature learning by inpainting. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition14Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2536-2544, 2016. 1, 4\n\n. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\n\nZero-shot text-to-image generation. PMLR, 2021. 4International Conference on Machine Learning. Zero-shot text-to-image generation. In International Confer- ence on Machine Learning, pages 8821-8831. PMLR, 2021. 4\n\nPredictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. P N Rajesh, Rao, H Dana, Ballard, Nature neuroscience. 21Rajesh PN Rao and Dana H Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79-87, 1999. 1\n\n. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, International Journal of Computer Vision. 115312Imagenet large scale visual recognition challengeOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015. 5, 12\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, arXiv:1703.0178012arXiv preprintAntti Tarvainen and Harri Valpola. Mean teachers are bet- ter role models: Weight-averaged consistency targets im- prove semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017. 12\n\nUnderstanding self-supervised learning dynamics without contrastive pairs. Yuandong Tian, Xinlei Chen, Surya Ganguli, PMLR, 2021. 13International Conference on Machine Learning. Yuandong Tian, Xinlei Chen, and Surya Ganguli. Un- derstanding self-supervised learning dynamics without con- trastive pairs. In International Conference on Machine Learning, pages 10268-10278. PMLR, 2021. 13\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, PMLR, 2021. 3International Conference on Machine Learning. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through at- tention. In International Conference on Machine Learning, pages 10347-10357. PMLR, 2021. 3\n\nOn mutual information maximization for representation learning. Michael Tschannen, Josip Djolonga, Sylvain Paul K Rubenstein, Mario Gelly, Lucic, arXiv:1907.13625arXiv preprintMichael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019. 13\n\nThe inaturalist species classification and detection dataset. Oisin Mac Grant Van Horn, Yang Aodha, Yin Song, Chen Cui, Alex Sun, Hartwig Shepard, Pietro Adam, Serge Perona, Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition12Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and de- tection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8769-8778, 2018. 12\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 3\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, L\u00e9on Bottou, Journal of machine learning research. 111213Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L\u00e9on Bottou. Stacked denoising autoencoders: Learning useful represen- tations in a deep network with a local denoising criterion. Journal of machine learning research, 11(12), 2010. 1, 4, 13\n\nMasked feature prediction for self-supervised visual pre-training. Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, Christoph Feichtenhofer, arXiv:2112.09133113arXiv preprintChen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature predic- tion for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133, 2021. 1, 13\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733-3742, 2018. 13\n\n. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V Le, arXiv:1904.12848Unsupervised data augmentation. arXiv preprintQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019. 13\n\nSimmim: A simple framework for masked image modeling. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, Han Hu, arXiv:2111.0988614arXiv preprintZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A sim- ple framework for masked image modeling. arXiv preprint arXiv:2111.09886, 2021. 1, 4\n\nLarge batch training of convolutional networks. Yang You, Igor Gitman, Boris Ginsburg, 12Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks, 2017. 12\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision16Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular- ization strategy to train strong classifiers with localizable fea- tures. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 6023-6032, 2019. 16\n\nBarlow twins: Self-supervised learning via redundancy reduction. Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, St\u00e9phane Deny, arXiv:2103.03230113arXiv preprintJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021. 1, 3, 13\n\nSylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, 12Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andre Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark, 2019. 12\n\nHongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, mixup: Beyond empirical risk minimization. Internatinoal Conference on Learning Representations. 16Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. Internatinoal Conference on Learning Representations, 2018. 16\n\n. Richard Zhang, Phillip Isola, Alexei A Efros, Colorful image colorization. 4Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. 2016. 4\n\nLearning deep features for scene recognition using places database. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, Aude Oliva, Advances in neural information processing systems. 2712Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor- ralba, and Aude Oliva. Learning deep features for scene recognition using places database. Advances in neural in- formation processing systems, 27, 2014. 12\n\nIbot: Image bert pretraining with online tokenizer. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, Tao Kong, International Conference on Learning Representations. 1213Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Ibot: Image bert pre- training with online tokenizer. International Conference on Learning Representations, 2022. 2, 4, 5, 6, 12, 13\n", "annotations": {"author": "[{\"end\":164,\"start\":87},{\"end\":196,\"start\":165},{\"end\":226,\"start\":197},{\"end\":261,\"start\":227},{\"end\":294,\"start\":262},{\"end\":352,\"start\":295},{\"end\":403,\"start\":353},{\"end\":436,\"start\":404}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":95},{\"end\":178,\"start\":173},{\"end\":208,\"start\":203},{\"end\":243,\"start\":233},{\"end\":276,\"start\":269},{\"end\":309,\"start\":303},{\"end\":363,\"start\":358},{\"end\":418,\"start\":412}]", "author_first_name": "[{\"end\":94,\"start\":87},{\"end\":172,\"start\":165},{\"end\":202,\"start\":197},{\"end\":232,\"start\":227},{\"end\":268,\"start\":262},{\"end\":302,\"start\":295},{\"end\":357,\"start\":353},{\"end\":411,\"start\":404}]", "author_affiliation": "[{\"end\":118,\"start\":103},{\"end\":138,\"start\":120},{\"end\":163,\"start\":140},{\"end\":195,\"start\":180},{\"end\":225,\"start\":210},{\"end\":260,\"start\":245},{\"end\":293,\"start\":278},{\"end\":326,\"start\":311},{\"end\":351,\"start\":328},{\"end\":380,\"start\":365},{\"end\":402,\"start\":382},{\"end\":435,\"start\":420}]", "title": "[{\"end\":84,\"start\":1},{\"end\":520,\"start\":437}]", "venue": null, "abstract": "[{\"end\":1541,\"start\":522}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1691,\"start\":1688},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1693,\"start\":1691},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1696,\"start\":1693},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1699,\"start\":1696},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1702,\"start\":1699},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1705,\"start\":1702},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1708,\"start\":1705},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1711,\"start\":1708},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":1714,\"start\":1711},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1741,\"start\":1738},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1744,\"start\":1741},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1747,\"start\":1744},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":1750,\"start\":1747},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1884,\"start\":1880},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1887,\"start\":1884},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2034,\"start\":2030},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2055,\"start\":2051},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2139,\"start\":2136},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2142,\"start\":2139},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2302,\"start\":2299},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2529,\"start\":2525},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2855,\"start\":2851},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2858,\"start\":2855},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3016,\"start\":3013},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3019,\"start\":3016},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3022,\"start\":3019},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3025,\"start\":3022},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":3028,\"start\":3025},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":3031,\"start\":3028},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3325,\"start\":3322},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4471,\"start\":4468},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4854,\"start\":4850},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6000,\"start\":5996},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":6597,\"start\":6593},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7757,\"start\":7753},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8122,\"start\":8118},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8125,\"start\":8122},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8128,\"start\":8125},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8218,\"start\":8214},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8221,\"start\":8218},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8309,\"start\":8306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8311,\"start\":8309},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8314,\"start\":8311},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8458,\"start\":8455},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8461,\"start\":8458},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8464,\"start\":8461},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8989,\"start\":8986},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8992,\"start\":8989},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9487,\"start\":9483},{\"end\":9960,\"start\":9951},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11192,\"start\":11188},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11195,\"start\":11192},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":11363,\"start\":11359},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11507,\"start\":11503},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13145,\"start\":13142},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15293,\"start\":15289},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15296,\"start\":15293},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":15299,\"start\":15296},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":15550,\"start\":15546},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15629,\"start\":15625},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15691,\"start\":15687},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15694,\"start\":15691},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":15697,\"start\":15694},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15800,\"start\":15797},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15803,\"start\":15800},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":15806,\"start\":15803},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15839,\"start\":15835},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15928,\"start\":15924},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16211,\"start\":16208},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16403,\"start\":16399},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16489,\"start\":16485},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":16516,\"start\":16512},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16598,\"start\":16594},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16954,\"start\":16951},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16984,\"start\":16980},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17643,\"start\":17640},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17815,\"start\":17811},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17824,\"start\":17821},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":17838,\"start\":17834},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17974,\"start\":17971},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19406,\"start\":19402},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19885,\"start\":19881},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19918,\"start\":19914},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19936,\"start\":19933},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":20825,\"start\":20821},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21470,\"start\":21467},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21743,\"start\":21740},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21754,\"start\":21750},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":21769,\"start\":21765},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22159,\"start\":22155},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23389,\"start\":23385},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25684,\"start\":25680},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30158,\"start\":30154},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30980,\"start\":30976},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31249,\"start\":31246},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31252,\"start\":31249},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31485,\"start\":31482},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31488,\"start\":31485},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31491,\"start\":31488},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31494,\"start\":31491},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31497,\"start\":31494},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":31500,\"start\":31497},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31613,\"start\":31610},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31616,\"start\":31613},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":32389,\"start\":32385},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":32751,\"start\":32747},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32762,\"start\":32758},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32774,\"start\":32770},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32815,\"start\":32811},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32895,\"start\":32891},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":32944,\"start\":32940},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32959,\"start\":32955},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32977,\"start\":32973},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":32980,\"start\":32977},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32997,\"start\":32993},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":33000,\"start\":32997},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":33020,\"start\":33016},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33428,\"start\":33424},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":34428,\"start\":34424},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34540,\"start\":34536},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":34562,\"start\":34558},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34975,\"start\":34971},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35639,\"start\":35635},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35787,\"start\":35784},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35790,\"start\":35787},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35793,\"start\":35790},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35796,\"start\":35793},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35799,\"start\":35796},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35802,\"start\":35799},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":35805,\"start\":35802},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35808,\"start\":35805},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":35811,\"start\":35808},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":35814,\"start\":35811},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":35817,\"start\":35814},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36034,\"start\":36031},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36037,\"start\":36034},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36040,\"start\":36037},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36043,\"start\":36040},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36077,\"start\":36073},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36080,\"start\":36077},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36282,\"start\":36278},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36352,\"start\":36348},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36396,\"start\":36393},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36399,\"start\":36396},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36402,\"start\":36399},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":36420,\"start\":36416},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36826,\"start\":36822},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37062,\"start\":37058},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":37065,\"start\":37062},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":37235,\"start\":37231},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37322,\"start\":37318},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37575,\"start\":37571},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37704,\"start\":37701},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":37707,\"start\":37704},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37710,\"start\":37707},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37713,\"start\":37710},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":37716,\"start\":37713},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":37719,\"start\":37716},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":37779,\"start\":37775},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37822,\"start\":37818},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":37825,\"start\":37822},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37925,\"start\":37921},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37928,\"start\":37925},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37931,\"start\":37928},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38146,\"start\":38143},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":38517,\"start\":38513},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38632,\"start\":38629},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38634,\"start\":38632},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38637,\"start\":38634},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38640,\"start\":38637},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":38643,\"start\":38640},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38715,\"start\":38711},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38718,\"start\":38715},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":38721,\"start\":38718},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38771,\"start\":38768},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":38774,\"start\":38771},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38939,\"start\":38936},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39071,\"start\":39067},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":39191,\"start\":39187},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39480,\"start\":39476},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39483,\"start\":39480},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39486,\"start\":39483},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":39539,\"start\":39535},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":39635,\"start\":39631},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":45314,\"start\":45310},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":45554,\"start\":45550},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":45578,\"start\":45574},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46212,\"start\":46208},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46636,\"start\":46632},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48222,\"start\":48219},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48964,\"start\":48961}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":49653,\"start\":49367},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50555,\"start\":49654},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50749,\"start\":50556},{\"attributes\":{\"id\":\"fig_5\"},\"end\":51202,\"start\":50750},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51862,\"start\":51203},{\"attributes\":{\"id\":\"fig_9\"},\"end\":52089,\"start\":51863},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":52918,\"start\":52090},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":53344,\"start\":52919},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":53761,\"start\":53345},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":53857,\"start\":53762},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":54286,\"start\":53858},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":54425,\"start\":54287},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":54818,\"start\":54426},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":55094,\"start\":54819}]", "paragraph": "[{\"end\":1751,\"start\":1557},{\"end\":2654,\"start\":1753},{\"end\":4219,\"start\":2656},{\"end\":4611,\"start\":4221},{\"end\":5211,\"start\":4613},{\"end\":5752,\"start\":5213},{\"end\":5817,\"start\":5754},{\"end\":6092,\"start\":5819},{\"end\":6405,\"start\":6094},{\"end\":6781,\"start\":6407},{\"end\":7305,\"start\":6796},{\"end\":7584,\"start\":7307},{\"end\":7758,\"start\":7586},{\"end\":8465,\"start\":7760},{\"end\":9375,\"start\":8467},{\"end\":10411,\"start\":9377},{\"end\":11139,\"start\":10422},{\"end\":11641,\"start\":11141},{\"end\":12377,\"start\":11643},{\"end\":12508,\"start\":12379},{\"end\":12764,\"start\":12510},{\"end\":13146,\"start\":12766},{\"end\":13263,\"start\":13148},{\"end\":13905,\"start\":13265},{\"end\":14700,\"start\":13907},{\"end\":14865,\"start\":14702},{\"end\":15340,\"start\":14935},{\"end\":15698,\"start\":15357},{\"end\":16917,\"start\":15700},{\"end\":17714,\"start\":16919},{\"end\":18484,\"start\":17716},{\"end\":18498,\"start\":18486},{\"end\":19043,\"start\":18500},{\"end\":19577,\"start\":19068},{\"end\":20785,\"start\":19579},{\"end\":20887,\"start\":20787},{\"end\":21770,\"start\":20889},{\"end\":22205,\"start\":21772},{\"end\":22870,\"start\":22232},{\"end\":23641,\"start\":22872},{\"end\":24579,\"start\":23657},{\"end\":24924,\"start\":24581},{\"end\":24946,\"start\":24926},{\"end\":26107,\"start\":24975},{\"end\":26685,\"start\":26121},{\"end\":28725,\"start\":26687},{\"end\":28792,\"start\":28758},{\"end\":29141,\"start\":28801},{\"end\":29521,\"start\":29143},{\"end\":30040,\"start\":29536},{\"end\":30947,\"start\":30089},{\"end\":31617,\"start\":30949},{\"end\":32665,\"start\":31619},{\"end\":33850,\"start\":32691},{\"end\":34365,\"start\":33852},{\"end\":34851,\"start\":34367},{\"end\":35640,\"start\":34853},{\"end\":36611,\"start\":35668},{\"end\":38291,\"start\":36613},{\"end\":38940,\"start\":38293},{\"end\":41625,\"start\":38942},{\"end\":41845,\"start\":41667},{\"end\":42403,\"start\":41847},{\"end\":43414,\"start\":42405},{\"end\":43529,\"start\":43416},{\"end\":44733,\"start\":43531},{\"end\":45082,\"start\":44735},{\"end\":45315,\"start\":45121},{\"end\":46058,\"start\":45317},{\"end\":46075,\"start\":46069},{\"end\":47583,\"start\":46102},{\"end\":49366,\"start\":47614}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14934,\"start\":14866}]", "table_ref": "[{\"end\":18483,\"start\":18476},{\"end\":19599,\"start\":19592},{\"end\":20440,\"start\":20433},{\"end\":20918,\"start\":20911},{\"end\":21798,\"start\":21791},{\"end\":22519,\"start\":22512},{\"end\":23161,\"start\":23154},{\"end\":26164,\"start\":26157},{\"end\":26571,\"start\":26564},{\"end\":26712,\"start\":26705},{\"end\":28073,\"start\":28066},{\"end\":28833,\"start\":28826},{\"end\":40656,\"start\":40649},{\"end\":41014,\"start\":41007},{\"end\":41322,\"start\":41314},{\"end\":41992,\"start\":41983},{\"end\":42020,\"start\":42012},{\"end\":42064,\"start\":42054},{\"end\":42604,\"start\":42596},{\"end\":43036,\"start\":43028},{\"end\":43528,\"start\":43520},{\"end\":43842,\"start\":43834},{\"end\":44364,\"start\":44356},{\"end\":44808,\"start\":44800},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":45763,\"start\":45755}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1555,\"start\":1543},{\"attributes\":{\"n\":\"2.\"},\"end\":6794,\"start\":6784},{\"attributes\":{\"n\":\"3.\"},\"end\":10420,\"start\":10414},{\"attributes\":{\"n\":\"4.\"},\"end\":15355,\"start\":15343},{\"attributes\":{\"n\":\"5.\"},\"end\":19066,\"start\":19046},{\"attributes\":{\"n\":\"6.\"},\"end\":22230,\"start\":22208},{\"attributes\":{\"n\":\"7.\"},\"end\":23655,\"start\":23644},{\"attributes\":{\"n\":\"8.\"},\"end\":24973,\"start\":24949},{\"attributes\":{\"n\":\"9.\"},\"end\":26119,\"start\":26110},{\"end\":28735,\"start\":28728},{\"end\":28756,\"start\":28738},{\"end\":28799,\"start\":28795},{\"attributes\":{\"n\":\"10.\"},\"end\":29534,\"start\":29524},{\"end\":30068,\"start\":30043},{\"end\":30087,\"start\":30071},{\"end\":32689,\"start\":32668},{\"end\":35666,\"start\":35643},{\"end\":41632,\"start\":41628},{\"end\":41639,\"start\":41635},{\"end\":41665,\"start\":41642},{\"end\":45119,\"start\":45085},{\"end\":46067,\"start\":46061},{\"end\":46100,\"start\":46078},{\"end\":47612,\"start\":47586},{\"end\":49378,\"start\":49368},{\"end\":49665,\"start\":49655},{\"end\":50567,\"start\":50557},{\"end\":50761,\"start\":50751},{\"end\":51214,\"start\":51204},{\"end\":51874,\"start\":51864},{\"end\":52113,\"start\":52091},{\"end\":52927,\"start\":52920}]", "table": "[{\"end\":52918,\"start\":52115},{\"end\":53344,\"start\":52929},{\"end\":53761,\"start\":53465},{\"end\":53857,\"start\":53796},{\"end\":54286,\"start\":54195},{\"end\":54425,\"start\":54307},{\"end\":54818,\"start\":54723},{\"end\":55094,\"start\":55009}]", "figure_caption": "[{\"end\":49653,\"start\":49380},{\"end\":50555,\"start\":49667},{\"end\":50749,\"start\":50569},{\"end\":51202,\"start\":50763},{\"end\":51862,\"start\":51216},{\"end\":52089,\"start\":51876},{\"end\":53465,\"start\":53347},{\"end\":53796,\"start\":53764},{\"end\":54195,\"start\":53860},{\"end\":54307,\"start\":54289},{\"end\":54723,\"start\":54428},{\"end\":55009,\"start\":54821}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":4929,\"start\":4921},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5933,\"start\":5928},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":7304,\"start\":7296},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":7583,\"start\":7574},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8638,\"start\":8629},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9440,\"start\":9431},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":10536,\"start\":10528},{\"end\":12323,\"start\":12315},{\"end\":13692,\"start\":13684},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23742,\"start\":23734},{\"end\":25763,\"start\":25755},{\"end\":27119,\"start\":27111},{\"end\":47209,\"start\":47201},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":47424,\"start\":47409},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":47625,\"start\":47617},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":47957,\"start\":47949},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":48166,\"start\":48158}]", "bib_author_first_name": "[{\"end\":55172,\"start\":55168},{\"end\":55179,\"start\":55173},{\"end\":55196,\"start\":55187},{\"end\":55214,\"start\":55208},{\"end\":55603,\"start\":55596},{\"end\":55619,\"start\":55612},{\"end\":55640,\"start\":55633},{\"end\":55655,\"start\":55648},{\"end\":55669,\"start\":55664},{\"end\":55682,\"start\":55677},{\"end\":55701,\"start\":55695},{\"end\":55718,\"start\":55711},{\"end\":55734,\"start\":55727},{\"end\":56141,\"start\":56134},{\"end\":56157,\"start\":56150},{\"end\":56539,\"start\":56532},{\"end\":56556,\"start\":56548},{\"end\":56569,\"start\":56564},{\"end\":56582,\"start\":56577},{\"end\":56602,\"start\":56595},{\"end\":56617,\"start\":56611},{\"end\":56633,\"start\":56627},{\"end\":56649,\"start\":56642},{\"end\":56665,\"start\":56658},{\"end\":57157,\"start\":57150},{\"end\":57174,\"start\":57166},{\"end\":57187,\"start\":57182},{\"end\":57200,\"start\":57195},{\"end\":57219,\"start\":57213},{\"end\":57665,\"start\":57659},{\"end\":57680,\"start\":57675},{\"end\":57695,\"start\":57688},{\"end\":58070,\"start\":58064},{\"end\":58084,\"start\":58080},{\"end\":58099,\"start\":58091},{\"end\":58112,\"start\":58105},{\"end\":58458,\"start\":58452},{\"end\":58476,\"start\":58468},{\"end\":58490,\"start\":58482},{\"end\":58499,\"start\":58495},{\"end\":58512,\"start\":58506},{\"end\":58524,\"start\":58517},{\"end\":58851,\"start\":58845},{\"end\":58859,\"start\":58857},{\"end\":58870,\"start\":58866},{\"end\":59131,\"start\":59125},{\"end\":59144,\"start\":59140},{\"end\":59156,\"start\":59152},{\"end\":59441,\"start\":59435},{\"end\":59454,\"start\":59450},{\"end\":59466,\"start\":59462},{\"end\":59754,\"start\":59747},{\"end\":59770,\"start\":59763},{\"end\":59791,\"start\":59784},{\"end\":59807,\"start\":59801},{\"end\":59822,\"start\":59816},{\"end\":60178,\"start\":60171},{\"end\":60194,\"start\":60187},{\"end\":60214,\"start\":60208},{\"end\":60540,\"start\":60536},{\"end\":60556,\"start\":60549},{\"end\":60571,\"start\":60566},{\"end\":60886,\"start\":60882},{\"end\":60897,\"start\":60896},{\"end\":60909,\"start\":60905},{\"end\":60925,\"start\":60917},{\"end\":60938,\"start\":60934},{\"end\":60951,\"start\":60946},{\"end\":60965,\"start\":60959},{\"end\":60979,\"start\":60973},{\"end\":61415,\"start\":61408},{\"end\":61428,\"start\":61421},{\"end\":61448,\"start\":61443},{\"end\":61464,\"start\":61457},{\"end\":61477,\"start\":61471},{\"end\":61491,\"start\":61486},{\"end\":61508,\"start\":61501},{\"end\":61520,\"start\":61513},{\"end\":61858,\"start\":61850},{\"end\":61871,\"start\":61866},{\"end\":61885,\"start\":61879},{\"end\":61899,\"start\":61894},{\"end\":61912,\"start\":61907},{\"end\":61931,\"start\":61925},{\"end\":62258,\"start\":62250},{\"end\":62270,\"start\":62266},{\"end\":62285,\"start\":62280},{\"end\":62298,\"start\":62293},{\"end\":62312,\"start\":62306},{\"end\":62326,\"start\":62321},{\"end\":62345,\"start\":62339},{\"end\":62660,\"start\":62656},{\"end\":62671,\"start\":62667},{\"end\":62686,\"start\":62681},{\"end\":62701,\"start\":62694},{\"end\":62712,\"start\":62706},{\"end\":62723,\"start\":62718},{\"end\":62734,\"start\":62730},{\"end\":63099,\"start\":63095},{\"end\":63111,\"start\":63106},{\"end\":63131,\"start\":63123},{\"end\":63149,\"start\":63141},{\"end\":63435,\"start\":63431},{\"end\":63447,\"start\":63442},{\"end\":63464,\"start\":63459},{\"end\":63482,\"start\":63474},{\"end\":63500,\"start\":63492},{\"end\":63802,\"start\":63794},{\"end\":63815,\"start\":63809},{\"end\":63828,\"start\":63822},{\"end\":63839,\"start\":63835},{\"end\":63853,\"start\":63845},{\"end\":63864,\"start\":63858},{\"end\":63877,\"start\":63871},{\"end\":63887,\"start\":63883},{\"end\":63897,\"start\":63893},{\"end\":63912,\"start\":63904},{\"end\":64191,\"start\":64185},{\"end\":64203,\"start\":64198},{\"end\":64542,\"start\":64536},{\"end\":64556,\"start\":64549},{\"end\":64795,\"start\":64789},{\"end\":64809,\"start\":64802},{\"end\":64822,\"start\":64815},{\"end\":65083,\"start\":65078},{\"end\":65096,\"start\":65090},{\"end\":65111,\"start\":65105},{\"end\":65120,\"start\":65116},{\"end\":65385,\"start\":65378},{\"end\":65397,\"start\":65393},{\"end\":65732,\"start\":65727},{\"end\":65749,\"start\":65741},{\"end\":65763,\"start\":65757},{\"end\":65777,\"start\":65769},{\"end\":65787,\"start\":65778},{\"end\":66096,\"start\":66090},{\"end\":66115,\"start\":66110},{\"end\":66132,\"start\":66123},{\"end\":66149,\"start\":66145},{\"end\":66170,\"start\":66163},{\"end\":66183,\"start\":66177},{\"end\":66204,\"start\":66197},{\"end\":66223,\"start\":66215},{\"end\":66239,\"start\":66234},{\"end\":66774,\"start\":66765},{\"end\":66792,\"start\":66785},{\"end\":66806,\"start\":66802},{\"end\":66820,\"start\":66816},{\"end\":66834,\"start\":66829},{\"end\":66849,\"start\":66842},{\"end\":67131,\"start\":67127},{\"end\":67432,\"start\":67426},{\"end\":67448,\"start\":67442},{\"end\":67462,\"start\":67457},{\"end\":67481,\"start\":67474},{\"end\":67497,\"start\":67489},{\"end\":67927,\"start\":67924},{\"end\":67946,\"start\":67940},{\"end\":67960,\"start\":67955},{\"end\":68076,\"start\":68071},{\"end\":68091,\"start\":68084},{\"end\":68105,\"start\":68099},{\"end\":68126,\"start\":68119},{\"end\":68139,\"start\":68136},{\"end\":68152,\"start\":68144},{\"end\":68170,\"start\":68164},{\"end\":68186,\"start\":68178},{\"end\":68201,\"start\":68193},{\"end\":68214,\"start\":68209},{\"end\":68233,\"start\":68227},{\"end\":68247,\"start\":68242},{\"end\":68595,\"start\":68583},{\"end\":68610,\"start\":68603},{\"end\":68625,\"start\":68618},{\"end\":68642,\"start\":68634},{\"end\":68652,\"start\":68651},{\"end\":68666,\"start\":68661},{\"end\":68682,\"start\":68678},{\"end\":68704,\"start\":68696},{\"end\":68721,\"start\":68714},{\"end\":68728,\"start\":68722},{\"end\":68750,\"start\":68742},{\"end\":68761,\"start\":68751},{\"end\":69191,\"start\":69184},{\"end\":69202,\"start\":69196},{\"end\":69216,\"start\":69209},{\"end\":69229,\"start\":69222},{\"end\":69239,\"start\":69234},{\"end\":69252,\"start\":69248},{\"end\":69642,\"start\":69635},{\"end\":69652,\"start\":69647},{\"end\":69663,\"start\":69658},{\"end\":69675,\"start\":69668},{\"end\":69685,\"start\":69681},{\"end\":69970,\"start\":69963},{\"end\":69982,\"start\":69975},{\"end\":69998,\"start\":69990},{\"end\":70008,\"start\":70004},{\"end\":70440,\"start\":70433},{\"end\":70763,\"start\":70759},{\"end\":70785,\"start\":70779},{\"end\":70800,\"start\":70795},{\"end\":70824,\"start\":70820},{\"end\":70837,\"start\":70833},{\"end\":70853,\"start\":70847},{\"end\":71239,\"start\":71233},{\"end\":71250,\"start\":71244},{\"end\":71264,\"start\":71259},{\"end\":71278,\"start\":71272},{\"end\":71297,\"start\":71290},{\"end\":71715,\"start\":71709},{\"end\":71732,\"start\":71725},{\"end\":71751,\"start\":71744},{\"end\":71770,\"start\":71768},{\"end\":71788,\"start\":71780},{\"end\":71802,\"start\":71798},{\"end\":72357,\"start\":72351},{\"end\":72374,\"start\":72367},{\"end\":72393,\"start\":72386},{\"end\":72412,\"start\":72410},{\"end\":72430,\"start\":72422},{\"end\":72444,\"start\":72440},{\"end\":72753,\"start\":72746},{\"end\":72768,\"start\":72762},{\"end\":72781,\"start\":72777},{\"end\":73086,\"start\":73082},{\"end\":73107,\"start\":73099},{\"end\":73283,\"start\":73277},{\"end\":73300,\"start\":73293},{\"end\":73315,\"start\":73308},{\"end\":73512,\"start\":73506},{\"end\":73529,\"start\":73522},{\"end\":73544,\"start\":73537},{\"end\":73746,\"start\":73742},{\"end\":73863,\"start\":73859},{\"end\":73876,\"start\":73871},{\"end\":73889,\"start\":73885},{\"end\":73900,\"start\":73899},{\"end\":73915,\"start\":73910},{\"end\":74186,\"start\":74181},{\"end\":74308,\"start\":74304},{\"end\":74326,\"start\":74321},{\"end\":74611,\"start\":74609},{\"end\":74621,\"start\":74616},{\"end\":74639,\"start\":74628},{\"end\":74994,\"start\":74989},{\"end\":75009,\"start\":75002},{\"end\":75444,\"start\":75438},{\"end\":75460,\"start\":75455},{\"end\":75478,\"start\":75473},{\"end\":75491,\"start\":75487},{\"end\":75508,\"start\":75501},{\"end\":75844,\"start\":75839},{\"end\":75864,\"start\":75859},{\"end\":75874,\"start\":75869},{\"end\":76193,\"start\":76189},{\"end\":76205,\"start\":76202},{\"end\":76222,\"start\":76213},{\"end\":76234,\"start\":76230},{\"end\":76247,\"start\":76242},{\"end\":76265,\"start\":76258},{\"end\":76280,\"start\":76274},{\"end\":76296,\"start\":76290},{\"end\":76309,\"start\":76302},{\"end\":76326,\"start\":76322},{\"end\":76687,\"start\":76681},{\"end\":76703,\"start\":76696},{\"end\":76720,\"start\":76716},{\"end\":76736,\"start\":76730},{\"end\":76752,\"start\":76746},{\"end\":76754,\"start\":76753},{\"end\":77161,\"start\":77155},{\"end\":77177,\"start\":77170},{\"end\":77193,\"start\":77186},{\"end\":77204,\"start\":77199},{\"end\":77218,\"start\":77211},{\"end\":77229,\"start\":77225},{\"end\":77243,\"start\":77239},{\"end\":77254,\"start\":77250},{\"end\":77714,\"start\":77713},{\"end\":77716,\"start\":77715},{\"end\":77731,\"start\":77730},{\"end\":77969,\"start\":77965},{\"end\":77986,\"start\":77983},{\"end\":77996,\"start\":77993},{\"end\":78009,\"start\":78001},{\"end\":78025,\"start\":78018},{\"end\":78040,\"start\":78036},{\"end\":78052,\"start\":78045},{\"end\":78066,\"start\":78060},{\"end\":78083,\"start\":78077},{\"end\":78099,\"start\":78092},{\"end\":78120,\"start\":78111},{\"end\":78122,\"start\":78121},{\"end\":78131,\"start\":78129},{\"end\":78669,\"start\":78664},{\"end\":78686,\"start\":78681},{\"end\":79014,\"start\":79006},{\"end\":79027,\"start\":79021},{\"end\":79039,\"start\":79034},{\"end\":79400,\"start\":79396},{\"end\":79418,\"start\":79410},{\"end\":79433,\"start\":79425},{\"end\":79450,\"start\":79441},{\"end\":79467,\"start\":79458},{\"end\":79487,\"start\":79482},{\"end\":79890,\"start\":79883},{\"end\":79907,\"start\":79902},{\"end\":79925,\"start\":79918},{\"end\":79950,\"start\":79945},{\"end\":80255,\"start\":80250},{\"end\":80259,\"start\":80256},{\"end\":80280,\"start\":80276},{\"end\":80291,\"start\":80288},{\"end\":80302,\"start\":80298},{\"end\":80312,\"start\":80308},{\"end\":80325,\"start\":80318},{\"end\":80341,\"start\":80335},{\"end\":80353,\"start\":80348},{\"end\":80847,\"start\":80841},{\"end\":80861,\"start\":80857},{\"end\":80875,\"start\":80871},{\"end\":80889,\"start\":80884},{\"end\":80906,\"start\":80901},{\"end\":80919,\"start\":80914},{\"end\":80921,\"start\":80920},{\"end\":80935,\"start\":80929},{\"end\":80949,\"start\":80944},{\"end\":81369,\"start\":81363},{\"end\":81383,\"start\":81379},{\"end\":81404,\"start\":81396},{\"end\":81419,\"start\":81413},{\"end\":81442,\"start\":81428},{\"end\":81457,\"start\":81453},{\"end\":81868,\"start\":81864},{\"end\":81879,\"start\":81874},{\"end\":81892,\"start\":81885},{\"end\":81907,\"start\":81898},{\"end\":81916,\"start\":81912},{\"end\":81934,\"start\":81925},{\"end\":82269,\"start\":82262},{\"end\":82281,\"start\":82274},{\"end\":82290,\"start\":82289},{\"end\":82304,\"start\":82299},{\"end\":82700,\"start\":82695},{\"end\":82712,\"start\":82706},{\"end\":82724,\"start\":82718},{\"end\":82741,\"start\":82731},{\"end\":82755,\"start\":82749},{\"end\":83026,\"start\":83020},{\"end\":83037,\"start\":83032},{\"end\":83048,\"start\":83045},{\"end\":83060,\"start\":83054},{\"end\":83073,\"start\":83066},{\"end\":83087,\"start\":83079},{\"end\":83095,\"start\":83093},{\"end\":83104,\"start\":83101},{\"end\":83387,\"start\":83383},{\"end\":83397,\"start\":83393},{\"end\":83411,\"start\":83406},{\"end\":83619,\"start\":83612},{\"end\":83633,\"start\":83625},{\"end\":83647,\"start\":83639},{\"end\":83669,\"start\":83663},{\"end\":83685,\"start\":83676},{\"end\":84182,\"start\":84178},{\"end\":84194,\"start\":84192},{\"end\":84206,\"start\":84201},{\"end\":84218,\"start\":84214},{\"end\":84234,\"start\":84226},{\"end\":84585,\"start\":84578},{\"end\":84596,\"start\":84592},{\"end\":84618,\"start\":84609},{\"end\":84637,\"start\":84631},{\"end\":84653,\"start\":84647},{\"end\":84669,\"start\":84664},{\"end\":84682,\"start\":84677},{\"end\":84698,\"start\":84693},{\"end\":84705,\"start\":84699},{\"end\":84718,\"start\":84713},{\"end\":84734,\"start\":84728},{\"end\":84753,\"start\":84748},{\"end\":84768,\"start\":84761},{\"end\":84784,\"start\":84777},{\"end\":84802,\"start\":84796},{\"end\":84821,\"start\":84814},{\"end\":85231,\"start\":85225},{\"end\":85248,\"start\":85239},{\"end\":85261,\"start\":85256},{\"end\":85574,\"start\":85567},{\"end\":85589,\"start\":85582},{\"end\":85603,\"start\":85597},{\"end\":85605,\"start\":85604},{\"end\":85804,\"start\":85799},{\"end\":85816,\"start\":85811},{\"end\":85837,\"start\":85828},{\"end\":85851,\"start\":85844},{\"end\":85866,\"start\":85862},{\"end\":86204,\"start\":86197},{\"end\":86215,\"start\":86211},{\"end\":86226,\"start\":86221},{\"end\":86236,\"start\":86233},{\"end\":86249,\"start\":86243},{\"end\":86259,\"start\":86255},{\"end\":86271,\"start\":86268}]", "bib_author_last_name": "[{\"end\":55185,\"start\":55180},{\"end\":55206,\"start\":55197},{\"end\":55222,\"start\":55215},{\"end\":55610,\"start\":55604},{\"end\":55631,\"start\":55620},{\"end\":55646,\"start\":55641},{\"end\":55662,\"start\":55656},{\"end\":55675,\"start\":55670},{\"end\":55693,\"start\":55683},{\"end\":55709,\"start\":55702},{\"end\":55725,\"start\":55719},{\"end\":55741,\"start\":55735},{\"end\":56148,\"start\":56142},{\"end\":56164,\"start\":56158},{\"end\":56546,\"start\":56540},{\"end\":56562,\"start\":56557},{\"end\":56575,\"start\":56570},{\"end\":56593,\"start\":56583},{\"end\":56609,\"start\":56603},{\"end\":56625,\"start\":56618},{\"end\":56640,\"start\":56634},{\"end\":56656,\"start\":56650},{\"end\":56672,\"start\":56666},{\"end\":57164,\"start\":57158},{\"end\":57180,\"start\":57175},{\"end\":57193,\"start\":57188},{\"end\":57211,\"start\":57201},{\"end\":57226,\"start\":57220},{\"end\":57673,\"start\":57666},{\"end\":57686,\"start\":57681},{\"end\":57706,\"start\":57696},{\"end\":58078,\"start\":58071},{\"end\":58089,\"start\":58085},{\"end\":58103,\"start\":58100},{\"end\":58117,\"start\":58113},{\"end\":58466,\"start\":58459},{\"end\":58480,\"start\":58477},{\"end\":58493,\"start\":58491},{\"end\":58504,\"start\":58500},{\"end\":58515,\"start\":58513},{\"end\":58529,\"start\":58525},{\"end\":58855,\"start\":58852},{\"end\":58864,\"start\":58860},{\"end\":58874,\"start\":58871},{\"end\":59138,\"start\":59132},{\"end\":59150,\"start\":59145},{\"end\":59162,\"start\":59157},{\"end\":59448,\"start\":59442},{\"end\":59460,\"start\":59455},{\"end\":59472,\"start\":59467},{\"end\":59761,\"start\":59755},{\"end\":59782,\"start\":59771},{\"end\":59799,\"start\":59792},{\"end\":59814,\"start\":59808},{\"end\":59830,\"start\":59823},{\"end\":60185,\"start\":60179},{\"end\":60206,\"start\":60195},{\"end\":60222,\"start\":60215},{\"end\":60547,\"start\":60541},{\"end\":60564,\"start\":60557},{\"end\":60578,\"start\":60572},{\"end\":60894,\"start\":60887},{\"end\":60903,\"start\":60898},{\"end\":60915,\"start\":60910},{\"end\":60932,\"start\":60926},{\"end\":60944,\"start\":60939},{\"end\":60957,\"start\":60952},{\"end\":60971,\"start\":60966},{\"end\":60989,\"start\":60980},{\"end\":60995,\"start\":60991},{\"end\":61419,\"start\":61416},{\"end\":61441,\"start\":61429},{\"end\":61455,\"start\":61449},{\"end\":61469,\"start\":61465},{\"end\":61484,\"start\":61478},{\"end\":61499,\"start\":61492},{\"end\":61511,\"start\":61509},{\"end\":61527,\"start\":61521},{\"end\":61864,\"start\":61859},{\"end\":61877,\"start\":61872},{\"end\":61892,\"start\":61886},{\"end\":61905,\"start\":61900},{\"end\":61923,\"start\":61913},{\"end\":61938,\"start\":61932},{\"end\":62264,\"start\":62259},{\"end\":62278,\"start\":62271},{\"end\":62291,\"start\":62286},{\"end\":62304,\"start\":62299},{\"end\":62319,\"start\":62313},{\"end\":62337,\"start\":62327},{\"end\":62352,\"start\":62346},{\"end\":62665,\"start\":62661},{\"end\":62679,\"start\":62672},{\"end\":62692,\"start\":62687},{\"end\":62704,\"start\":62702},{\"end\":62716,\"start\":62713},{\"end\":62728,\"start\":62724},{\"end\":62744,\"start\":62735},{\"end\":63104,\"start\":63100},{\"end\":63121,\"start\":63112},{\"end\":63139,\"start\":63132},{\"end\":63156,\"start\":63150},{\"end\":63440,\"start\":63436},{\"end\":63457,\"start\":63448},{\"end\":63472,\"start\":63465},{\"end\":63490,\"start\":63483},{\"end\":63507,\"start\":63501},{\"end\":63807,\"start\":63803},{\"end\":63820,\"start\":63816},{\"end\":63833,\"start\":63829},{\"end\":63843,\"start\":63840},{\"end\":63856,\"start\":63854},{\"end\":63869,\"start\":63865},{\"end\":63881,\"start\":63878},{\"end\":63891,\"start\":63888},{\"end\":63902,\"start\":63898},{\"end\":63917,\"start\":63913},{\"end\":64196,\"start\":64192},{\"end\":64207,\"start\":64204},{\"end\":64547,\"start\":64543},{\"end\":64559,\"start\":64557},{\"end\":64800,\"start\":64796},{\"end\":64813,\"start\":64810},{\"end\":64825,\"start\":64823},{\"end\":65088,\"start\":65084},{\"end\":65103,\"start\":65097},{\"end\":65114,\"start\":65112},{\"end\":65126,\"start\":65121},{\"end\":65391,\"start\":65386},{\"end\":65404,\"start\":65398},{\"end\":65739,\"start\":65733},{\"end\":65755,\"start\":65750},{\"end\":65767,\"start\":65764},{\"end\":65792,\"start\":65788},{\"end\":66108,\"start\":66097},{\"end\":66121,\"start\":66116},{\"end\":66143,\"start\":66133},{\"end\":66161,\"start\":66150},{\"end\":66175,\"start\":66171},{\"end\":66195,\"start\":66184},{\"end\":66213,\"start\":66205},{\"end\":66232,\"start\":66224},{\"end\":66247,\"start\":66240},{\"end\":66783,\"start\":66775},{\"end\":66800,\"start\":66793},{\"end\":66814,\"start\":66807},{\"end\":66827,\"start\":66821},{\"end\":66840,\"start\":66835},{\"end\":66855,\"start\":66850},{\"end\":67139,\"start\":67132},{\"end\":67440,\"start\":67433},{\"end\":67455,\"start\":67449},{\"end\":67472,\"start\":67463},{\"end\":67487,\"start\":67482},{\"end\":67502,\"start\":67498},{\"end\":67938,\"start\":67928},{\"end\":67953,\"start\":67947},{\"end\":67970,\"start\":67961},{\"end\":68082,\"start\":68077},{\"end\":68097,\"start\":68092},{\"end\":68117,\"start\":68106},{\"end\":68134,\"start\":68127},{\"end\":68142,\"start\":68140},{\"end\":68162,\"start\":68153},{\"end\":68176,\"start\":68171},{\"end\":68191,\"start\":68187},{\"end\":68207,\"start\":68202},{\"end\":68225,\"start\":68215},{\"end\":68240,\"start\":68234},{\"end\":68253,\"start\":68248},{\"end\":68601,\"start\":68596},{\"end\":68616,\"start\":68611},{\"end\":68632,\"start\":68626},{\"end\":68649,\"start\":68643},{\"end\":68659,\"start\":68653},{\"end\":68676,\"start\":68667},{\"end\":68694,\"start\":68683},{\"end\":68712,\"start\":68705},{\"end\":68740,\"start\":68729},{\"end\":68765,\"start\":68762},{\"end\":68771,\"start\":68767},{\"end\":69194,\"start\":69192},{\"end\":69207,\"start\":69203},{\"end\":69220,\"start\":69217},{\"end\":69232,\"start\":69230},{\"end\":69246,\"start\":69240},{\"end\":69261,\"start\":69253},{\"end\":69645,\"start\":69643},{\"end\":69656,\"start\":69653},{\"end\":69666,\"start\":69664},{\"end\":69679,\"start\":69676},{\"end\":69694,\"start\":69686},{\"end\":69973,\"start\":69971},{\"end\":69988,\"start\":69983},{\"end\":70002,\"start\":69999},{\"end\":70012,\"start\":70009},{\"end\":70447,\"start\":70441},{\"end\":70777,\"start\":70764},{\"end\":70793,\"start\":70786},{\"end\":70818,\"start\":70801},{\"end\":70831,\"start\":70825},{\"end\":70845,\"start\":70838},{\"end\":70863,\"start\":70854},{\"end\":70871,\"start\":70865},{\"end\":71242,\"start\":71240},{\"end\":71257,\"start\":71251},{\"end\":71270,\"start\":71265},{\"end\":71288,\"start\":71279},{\"end\":71306,\"start\":71298},{\"end\":71723,\"start\":71716},{\"end\":71742,\"start\":71733},{\"end\":71766,\"start\":71752},{\"end\":71778,\"start\":71771},{\"end\":71796,\"start\":71789},{\"end\":71811,\"start\":71803},{\"end\":72365,\"start\":72358},{\"end\":72384,\"start\":72375},{\"end\":72408,\"start\":72394},{\"end\":72420,\"start\":72413},{\"end\":72438,\"start\":72431},{\"end\":72453,\"start\":72445},{\"end\":72760,\"start\":72754},{\"end\":72775,\"start\":72769},{\"end\":72787,\"start\":72782},{\"end\":73097,\"start\":73087},{\"end\":73114,\"start\":73108},{\"end\":73291,\"start\":73284},{\"end\":73306,\"start\":73301},{\"end\":73329,\"start\":73316},{\"end\":73520,\"start\":73513},{\"end\":73535,\"start\":73530},{\"end\":73558,\"start\":73545},{\"end\":73752,\"start\":73747},{\"end\":73869,\"start\":73864},{\"end\":73883,\"start\":73877},{\"end\":73897,\"start\":73890},{\"end\":73908,\"start\":73901},{\"end\":73921,\"start\":73916},{\"end\":74194,\"start\":74187},{\"end\":74319,\"start\":74309},{\"end\":74333,\"start\":74327},{\"end\":74614,\"start\":74612},{\"end\":74626,\"start\":74622},{\"end\":74644,\"start\":74640},{\"end\":75000,\"start\":74995},{\"end\":75024,\"start\":75010},{\"end\":75453,\"start\":75445},{\"end\":75471,\"start\":75461},{\"end\":75485,\"start\":75479},{\"end\":75499,\"start\":75492},{\"end\":75517,\"start\":75509},{\"end\":75857,\"start\":75845},{\"end\":75867,\"start\":75865},{\"end\":75882,\"start\":75875},{\"end\":76200,\"start\":76194},{\"end\":76211,\"start\":76206},{\"end\":76228,\"start\":76223},{\"end\":76240,\"start\":76235},{\"end\":76256,\"start\":76248},{\"end\":76272,\"start\":76266},{\"end\":76288,\"start\":76281},{\"end\":76300,\"start\":76297},{\"end\":76320,\"start\":76310},{\"end\":76333,\"start\":76327},{\"end\":76694,\"start\":76688},{\"end\":76714,\"start\":76704},{\"end\":76728,\"start\":76721},{\"end\":76744,\"start\":76737},{\"end\":76760,\"start\":76755},{\"end\":77168,\"start\":77162},{\"end\":77184,\"start\":77178},{\"end\":77197,\"start\":77194},{\"end\":77209,\"start\":77205},{\"end\":77223,\"start\":77219},{\"end\":77237,\"start\":77230},{\"end\":77248,\"start\":77244},{\"end\":77264,\"start\":77255},{\"end\":77723,\"start\":77717},{\"end\":77728,\"start\":77725},{\"end\":77736,\"start\":77732},{\"end\":77745,\"start\":77738},{\"end\":77981,\"start\":77970},{\"end\":77991,\"start\":77987},{\"end\":77999,\"start\":77997},{\"end\":78016,\"start\":78010},{\"end\":78034,\"start\":78026},{\"end\":78043,\"start\":78041},{\"end\":78058,\"start\":78053},{\"end\":78075,\"start\":78067},{\"end\":78090,\"start\":78084},{\"end\":78109,\"start\":78100},{\"end\":78127,\"start\":78123},{\"end\":78139,\"start\":78132},{\"end\":78679,\"start\":78670},{\"end\":78694,\"start\":78687},{\"end\":79019,\"start\":79015},{\"end\":79032,\"start\":79028},{\"end\":79047,\"start\":79040},{\"end\":79408,\"start\":79401},{\"end\":79423,\"start\":79419},{\"end\":79439,\"start\":79434},{\"end\":79456,\"start\":79451},{\"end\":79480,\"start\":79468},{\"end\":79493,\"start\":79488},{\"end\":79900,\"start\":79891},{\"end\":79916,\"start\":79908},{\"end\":79943,\"start\":79926},{\"end\":79956,\"start\":79951},{\"end\":79963,\"start\":79958},{\"end\":80274,\"start\":80260},{\"end\":80286,\"start\":80281},{\"end\":80296,\"start\":80292},{\"end\":80306,\"start\":80303},{\"end\":80316,\"start\":80313},{\"end\":80333,\"start\":80326},{\"end\":80346,\"start\":80342},{\"end\":80360,\"start\":80354},{\"end\":80370,\"start\":80362},{\"end\":80855,\"start\":80848},{\"end\":80869,\"start\":80862},{\"end\":80882,\"start\":80876},{\"end\":80899,\"start\":80890},{\"end\":80912,\"start\":80907},{\"end\":80927,\"start\":80922},{\"end\":80942,\"start\":80936},{\"end\":80960,\"start\":80950},{\"end\":81377,\"start\":81370},{\"end\":81394,\"start\":81384},{\"end\":81411,\"start\":81405},{\"end\":81426,\"start\":81420},{\"end\":81451,\"start\":81443},{\"end\":81464,\"start\":81458},{\"end\":81872,\"start\":81869},{\"end\":81883,\"start\":81880},{\"end\":81896,\"start\":81893},{\"end\":81910,\"start\":81908},{\"end\":81923,\"start\":81917},{\"end\":81948,\"start\":81935},{\"end\":82272,\"start\":82270},{\"end\":82287,\"start\":82282},{\"end\":82297,\"start\":82291},{\"end\":82307,\"start\":82305},{\"end\":82312,\"start\":82309},{\"end\":82704,\"start\":82701},{\"end\":82716,\"start\":82713},{\"end\":82729,\"start\":82725},{\"end\":82747,\"start\":82742},{\"end\":82758,\"start\":82756},{\"end\":83030,\"start\":83027},{\"end\":83043,\"start\":83038},{\"end\":83052,\"start\":83049},{\"end\":83064,\"start\":83061},{\"end\":83077,\"start\":83074},{\"end\":83091,\"start\":83088},{\"end\":83099,\"start\":83096},{\"end\":83107,\"start\":83105},{\"end\":83391,\"start\":83388},{\"end\":83404,\"start\":83398},{\"end\":83420,\"start\":83412},{\"end\":83623,\"start\":83620},{\"end\":83637,\"start\":83634},{\"end\":83661,\"start\":83648},{\"end\":83674,\"start\":83670},{\"end\":83690,\"start\":83686},{\"end\":83695,\"start\":83692},{\"end\":84190,\"start\":84183},{\"end\":84199,\"start\":84195},{\"end\":84212,\"start\":84207},{\"end\":84224,\"start\":84219},{\"end\":84239,\"start\":84235},{\"end\":84590,\"start\":84586},{\"end\":84607,\"start\":84597},{\"end\":84629,\"start\":84619},{\"end\":84645,\"start\":84638},{\"end\":84662,\"start\":84654},{\"end\":84675,\"start\":84670},{\"end\":84691,\"start\":84683},{\"end\":84711,\"start\":84706},{\"end\":84726,\"start\":84719},{\"end\":84746,\"start\":84735},{\"end\":84759,\"start\":84754},{\"end\":84775,\"start\":84769},{\"end\":84794,\"start\":84785},{\"end\":84812,\"start\":84803},{\"end\":84830,\"start\":84822},{\"end\":85237,\"start\":85232},{\"end\":85254,\"start\":85249},{\"end\":85276,\"start\":85262},{\"end\":85287,\"start\":85278},{\"end\":85580,\"start\":85575},{\"end\":85595,\"start\":85590},{\"end\":85611,\"start\":85606},{\"end\":85809,\"start\":85805},{\"end\":85826,\"start\":85817},{\"end\":85842,\"start\":85838},{\"end\":85860,\"start\":85852},{\"end\":85872,\"start\":85867},{\"end\":86209,\"start\":86205},{\"end\":86219,\"start\":86216},{\"end\":86231,\"start\":86227},{\"end\":86241,\"start\":86237},{\"end\":86253,\"start\":86250},{\"end\":86266,\"start\":86260},{\"end\":86276,\"start\":86272}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207930156},\"end\":55478,\"start\":55096},{\"attributes\":{\"id\":\"b1\"},\"end\":56028,\"start\":55480},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227247755},\"end\":56476,\"start\":56030},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":248178208},\"end\":56997,\"start\":56478},{\"attributes\":{\"id\":\"b4\"},\"end\":57585,\"start\":56999},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":173990164},\"end\":57951,\"start\":57587},{\"attributes\":{\"doi\":\"arXiv:2212.07525\",\"id\":\"b6\"},\"end\":58359,\"start\":57953},{\"attributes\":{\"doi\":\"arXiv:2202.03555\",\"id\":\"b7\"},\"end\":58796,\"start\":58361},{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b8\"},\"end\":59040,\"start\":58798},{\"attributes\":{\"doi\":\"arXiv:2105.04906\",\"id\":\"b9\"},\"end\":59374,\"start\":59042},{\"attributes\":{\"doi\":\"arXiv:2210.01571\",\"id\":\"b10\"},\"end\":59655,\"start\":59376},{\"attributes\":{\"doi\":\"arXiv:2206.13378\",\"id\":\"b11\"},\"end\":60084,\"start\":59657},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":245329541},\"end\":60468,\"start\":60086},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":46422691},\"end\":60812,\"start\":60470},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16394033},\"end\":61360,\"start\":60814},{\"attributes\":{\"doi\":\"arXiv:2208.05688\",\"id\":\"b15\"},\"end\":61771,\"start\":61362},{\"attributes\":{\"doi\":\"arXiv:2006.09882\",\"id\":\"b16\"},\"end\":62188,\"start\":61773},{\"attributes\":{\"doi\":\"arXiv:2104.14294\",\"id\":\"b17\"},\"end\":62618,\"start\":62190},{\"attributes\":{\"doi\":\"PMLR, 2020. 13\",\"id\":\"b18\",\"matched_paper_id\":219781060},\"end\":63022,\"start\":62620},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b19\"},\"end\":63365,\"start\":63024},{\"attributes\":{\"doi\":\"arXiv:2006.10029\",\"id\":\"b20\"},\"end\":63727,\"start\":63367},{\"attributes\":{\"doi\":\"arXiv:2202.03026\",\"id\":\"b21\"},\"end\":64183,\"start\":63729},{\"attributes\":{\"doi\":\"arXiv:2003.04297\",\"id\":\"b22\"},\"end\":64484,\"start\":64185},{\"attributes\":{\"doi\":\"arXiv:2011.10566\",\"id\":\"b23\"},\"end\":64719,\"start\":64486},{\"attributes\":{\"doi\":\"arXiv:2104.02057\",\"id\":\"b24\"},\"end\":65007,\"start\":64721},{\"attributes\":{\"doi\":\"arXiv:2206.08954\",\"id\":\"b25\"},\"end\":65322,\"start\":65009},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206590483},\"end\":65725,\"start\":65324},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b27\"},\"end\":66088,\"start\":65727},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b28\"},\"end\":66693,\"start\":66090},{\"attributes\":{\"doi\":\"arXiv:2112.10740\",\"id\":\"b29\"},\"end\":67093,\"start\":66695},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15399012},\"end\":67363,\"start\":67095},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":211532737},\"end\":67907,\"start\":67365},{\"attributes\":{\"id\":\"b32\"},\"end\":68067,\"start\":67909},{\"attributes\":{\"doi\":\"2021. 12\",\"id\":\"b33\"},\"end\":68510,\"start\":68069},{\"attributes\":{\"doi\":\"arXiv:2006.07733\",\"id\":\"b34\"},\"end\":69132,\"start\":68512},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":243985980},\"end\":69566,\"start\":69134},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b36\"},\"end\":69915,\"start\":69568},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206594692},\"end\":70362,\"start\":69917},{\"attributes\":{\"doi\":\"PMLR, 2020. 13\",\"id\":\"b38\",\"matched_paper_id\":162168848},\"end\":70676,\"start\":70364},{\"attributes\":{\"doi\":\"arXiv:1808.06670\",\"id\":\"b39\"},\"end\":71145,\"start\":70678},{\"attributes\":{\"doi\":\"PMLR, 2017. 13\",\"id\":\"b40\",\"matched_paper_id\":2853246},\"end\":71619,\"start\":71147},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15458100},\"end\":72261,\"start\":71621},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15458100},\"end\":72677,\"start\":72263},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2906843},\"end\":73025,\"start\":72679},{\"attributes\":{\"id\":\"b44\"},\"end\":73222,\"start\":73027},{\"attributes\":{\"id\":\"b45\"},\"end\":73449,\"start\":73224},{\"attributes\":{\"id\":\"b46\"},\"end\":73680,\"start\":73451},{\"attributes\":{\"id\":\"b47\"},\"end\":73855,\"start\":73682},{\"attributes\":{\"id\":\"b48\"},\"end\":74136,\"start\":73857},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1527671},\"end\":74300,\"start\":74138},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b50\"},\"end\":74520,\"start\":74302},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":250426024},\"end\":74924,\"start\":74522},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":208617491},\"end\":75379,\"start\":74926},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":222378250},\"end\":75777,\"start\":75381},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b54\"},\"end\":76066,\"start\":75779},{\"attributes\":{\"id\":\"b55\"},\"end\":76629,\"start\":76068},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":2202933},\"end\":77151,\"start\":76631},{\"attributes\":{\"id\":\"b57\"},\"end\":77380,\"start\":77153},{\"attributes\":{\"doi\":\"PMLR, 2021. 4\",\"id\":\"b58\",\"matched_paper_id\":232035663},\"end\":77594,\"start\":77382},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":221608503},\"end\":77961,\"start\":77596},{\"attributes\":{\"id\":\"b60\"},\"end\":78541,\"start\":77963},{\"attributes\":{\"doi\":\"arXiv:1703.01780\",\"id\":\"b61\"},\"end\":78929,\"start\":78543},{\"attributes\":{\"doi\":\"PMLR, 2021. 13\",\"id\":\"b62\",\"matched_paper_id\":231924910},\"end\":79317,\"start\":78931},{\"attributes\":{\"doi\":\"PMLR, 2021. 3\",\"id\":\"b63\",\"matched_paper_id\":229363322},\"end\":79817,\"start\":79319},{\"attributes\":{\"doi\":\"arXiv:1907.13625\",\"id\":\"b64\"},\"end\":80186,\"start\":79819},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":29156801},\"end\":80812,\"start\":80188},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":13756489},\"end\":81245,\"start\":80814},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":17804904},\"end\":81795,\"start\":81247},{\"attributes\":{\"doi\":\"arXiv:2112.09133\",\"id\":\"b68\"},\"end\":82186,\"start\":81797},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":4591284},\"end\":82691,\"start\":82188},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b70\"},\"end\":82964,\"start\":82693},{\"attributes\":{\"doi\":\"arXiv:2111.09886\",\"id\":\"b71\"},\"end\":83333,\"start\":82966},{\"attributes\":{\"id\":\"b72\"},\"end\":83523,\"start\":83335},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":152282661},\"end\":84111,\"start\":83525},{\"attributes\":{\"doi\":\"arXiv:2103.03230\",\"id\":\"b74\"},\"end\":84453,\"start\":84113},{\"attributes\":{\"id\":\"b75\"},\"end\":85223,\"start\":84455},{\"attributes\":{\"id\":\"b76\"},\"end\":85563,\"start\":85225},{\"attributes\":{\"id\":\"b77\"},\"end\":85729,\"start\":85565},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":1849990},\"end\":86143,\"start\":85731},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":244117494},\"end\":86553,\"start\":86145}]", "bib_title": "[{\"end\":55166,\"start\":55096},{\"end\":56132,\"start\":56030},{\"end\":56530,\"start\":56478},{\"end\":57148,\"start\":56999},{\"end\":57657,\"start\":57587},{\"end\":60169,\"start\":60086},{\"end\":60534,\"start\":60470},{\"end\":60880,\"start\":60814},{\"end\":62654,\"start\":62620},{\"end\":65376,\"start\":65324},{\"end\":67125,\"start\":67095},{\"end\":67424,\"start\":67365},{\"end\":69182,\"start\":69134},{\"end\":69961,\"start\":69917},{\"end\":70431,\"start\":70364},{\"end\":71231,\"start\":71147},{\"end\":71707,\"start\":71621},{\"end\":72349,\"start\":72263},{\"end\":72744,\"start\":72679},{\"end\":74179,\"start\":74138},{\"end\":74607,\"start\":74522},{\"end\":74987,\"start\":74926},{\"end\":75436,\"start\":75381},{\"end\":76679,\"start\":76631},{\"end\":77416,\"start\":77382},{\"end\":77711,\"start\":77596},{\"end\":79004,\"start\":78931},{\"end\":79394,\"start\":79319},{\"end\":80248,\"start\":80188},{\"end\":80839,\"start\":80814},{\"end\":81361,\"start\":81247},{\"end\":82260,\"start\":82188},{\"end\":83610,\"start\":83525},{\"end\":85797,\"start\":85731},{\"end\":86195,\"start\":86145}]", "bib_author": "[{\"end\":55187,\"start\":55168},{\"end\":55208,\"start\":55187},{\"end\":55224,\"start\":55208},{\"end\":55612,\"start\":55596},{\"end\":55633,\"start\":55612},{\"end\":55648,\"start\":55633},{\"end\":55664,\"start\":55648},{\"end\":55677,\"start\":55664},{\"end\":55695,\"start\":55677},{\"end\":55711,\"start\":55695},{\"end\":55727,\"start\":55711},{\"end\":55743,\"start\":55727},{\"end\":56150,\"start\":56134},{\"end\":56166,\"start\":56150},{\"end\":56548,\"start\":56532},{\"end\":56564,\"start\":56548},{\"end\":56577,\"start\":56564},{\"end\":56595,\"start\":56577},{\"end\":56611,\"start\":56595},{\"end\":56627,\"start\":56611},{\"end\":56642,\"start\":56627},{\"end\":56658,\"start\":56642},{\"end\":56674,\"start\":56658},{\"end\":57166,\"start\":57150},{\"end\":57182,\"start\":57166},{\"end\":57195,\"start\":57182},{\"end\":57213,\"start\":57195},{\"end\":57228,\"start\":57213},{\"end\":57675,\"start\":57659},{\"end\":57688,\"start\":57675},{\"end\":57708,\"start\":57688},{\"end\":58080,\"start\":58064},{\"end\":58091,\"start\":58080},{\"end\":58105,\"start\":58091},{\"end\":58119,\"start\":58105},{\"end\":58468,\"start\":58452},{\"end\":58482,\"start\":58468},{\"end\":58495,\"start\":58482},{\"end\":58506,\"start\":58495},{\"end\":58517,\"start\":58506},{\"end\":58531,\"start\":58517},{\"end\":58857,\"start\":58845},{\"end\":58866,\"start\":58857},{\"end\":58876,\"start\":58866},{\"end\":59140,\"start\":59125},{\"end\":59152,\"start\":59140},{\"end\":59164,\"start\":59152},{\"end\":59450,\"start\":59435},{\"end\":59462,\"start\":59450},{\"end\":59474,\"start\":59462},{\"end\":59763,\"start\":59747},{\"end\":59784,\"start\":59763},{\"end\":59801,\"start\":59784},{\"end\":59816,\"start\":59801},{\"end\":59832,\"start\":59816},{\"end\":60187,\"start\":60171},{\"end\":60208,\"start\":60187},{\"end\":60224,\"start\":60208},{\"end\":60549,\"start\":60536},{\"end\":60566,\"start\":60549},{\"end\":60580,\"start\":60566},{\"end\":60896,\"start\":60882},{\"end\":60905,\"start\":60896},{\"end\":60917,\"start\":60905},{\"end\":60934,\"start\":60917},{\"end\":60946,\"start\":60934},{\"end\":60959,\"start\":60946},{\"end\":60973,\"start\":60959},{\"end\":60991,\"start\":60973},{\"end\":60997,\"start\":60991},{\"end\":61421,\"start\":61408},{\"end\":61443,\"start\":61421},{\"end\":61457,\"start\":61443},{\"end\":61471,\"start\":61457},{\"end\":61486,\"start\":61471},{\"end\":61501,\"start\":61486},{\"end\":61513,\"start\":61501},{\"end\":61529,\"start\":61513},{\"end\":61866,\"start\":61850},{\"end\":61879,\"start\":61866},{\"end\":61894,\"start\":61879},{\"end\":61907,\"start\":61894},{\"end\":61925,\"start\":61907},{\"end\":61940,\"start\":61925},{\"end\":62266,\"start\":62250},{\"end\":62280,\"start\":62266},{\"end\":62293,\"start\":62280},{\"end\":62306,\"start\":62293},{\"end\":62321,\"start\":62306},{\"end\":62339,\"start\":62321},{\"end\":62354,\"start\":62339},{\"end\":62667,\"start\":62656},{\"end\":62681,\"start\":62667},{\"end\":62694,\"start\":62681},{\"end\":62706,\"start\":62694},{\"end\":62718,\"start\":62706},{\"end\":62730,\"start\":62718},{\"end\":62746,\"start\":62730},{\"end\":63106,\"start\":63095},{\"end\":63123,\"start\":63106},{\"end\":63141,\"start\":63123},{\"end\":63158,\"start\":63141},{\"end\":63442,\"start\":63431},{\"end\":63459,\"start\":63442},{\"end\":63474,\"start\":63459},{\"end\":63492,\"start\":63474},{\"end\":63509,\"start\":63492},{\"end\":63809,\"start\":63794},{\"end\":63822,\"start\":63809},{\"end\":63835,\"start\":63822},{\"end\":63845,\"start\":63835},{\"end\":63858,\"start\":63845},{\"end\":63871,\"start\":63858},{\"end\":63883,\"start\":63871},{\"end\":63893,\"start\":63883},{\"end\":63904,\"start\":63893},{\"end\":63919,\"start\":63904},{\"end\":64198,\"start\":64185},{\"end\":64209,\"start\":64198},{\"end\":64549,\"start\":64536},{\"end\":64561,\"start\":64549},{\"end\":64802,\"start\":64789},{\"end\":64815,\"start\":64802},{\"end\":64827,\"start\":64815},{\"end\":65090,\"start\":65078},{\"end\":65105,\"start\":65090},{\"end\":65116,\"start\":65105},{\"end\":65128,\"start\":65116},{\"end\":65393,\"start\":65378},{\"end\":65406,\"start\":65393},{\"end\":65741,\"start\":65727},{\"end\":65757,\"start\":65741},{\"end\":65769,\"start\":65757},{\"end\":65794,\"start\":65769},{\"end\":66110,\"start\":66090},{\"end\":66123,\"start\":66110},{\"end\":66145,\"start\":66123},{\"end\":66163,\"start\":66145},{\"end\":66177,\"start\":66163},{\"end\":66197,\"start\":66177},{\"end\":66215,\"start\":66197},{\"end\":66234,\"start\":66215},{\"end\":66249,\"start\":66234},{\"end\":66785,\"start\":66765},{\"end\":66802,\"start\":66785},{\"end\":66816,\"start\":66802},{\"end\":66829,\"start\":66816},{\"end\":66842,\"start\":66829},{\"end\":66857,\"start\":66842},{\"end\":67141,\"start\":67127},{\"end\":67442,\"start\":67426},{\"end\":67457,\"start\":67442},{\"end\":67474,\"start\":67457},{\"end\":67489,\"start\":67474},{\"end\":67504,\"start\":67489},{\"end\":67940,\"start\":67924},{\"end\":67955,\"start\":67940},{\"end\":67972,\"start\":67955},{\"end\":68084,\"start\":68071},{\"end\":68099,\"start\":68084},{\"end\":68119,\"start\":68099},{\"end\":68136,\"start\":68119},{\"end\":68144,\"start\":68136},{\"end\":68164,\"start\":68144},{\"end\":68178,\"start\":68164},{\"end\":68193,\"start\":68178},{\"end\":68209,\"start\":68193},{\"end\":68227,\"start\":68209},{\"end\":68242,\"start\":68227},{\"end\":68255,\"start\":68242},{\"end\":68603,\"start\":68583},{\"end\":68618,\"start\":68603},{\"end\":68634,\"start\":68618},{\"end\":68651,\"start\":68634},{\"end\":68661,\"start\":68651},{\"end\":68678,\"start\":68661},{\"end\":68696,\"start\":68678},{\"end\":68714,\"start\":68696},{\"end\":68742,\"start\":68714},{\"end\":68767,\"start\":68742},{\"end\":68773,\"start\":68767},{\"end\":69196,\"start\":69184},{\"end\":69209,\"start\":69196},{\"end\":69222,\"start\":69209},{\"end\":69234,\"start\":69222},{\"end\":69248,\"start\":69234},{\"end\":69263,\"start\":69248},{\"end\":69647,\"start\":69635},{\"end\":69658,\"start\":69647},{\"end\":69668,\"start\":69658},{\"end\":69681,\"start\":69668},{\"end\":69696,\"start\":69681},{\"end\":69975,\"start\":69963},{\"end\":69990,\"start\":69975},{\"end\":70004,\"start\":69990},{\"end\":70014,\"start\":70004},{\"end\":70449,\"start\":70433},{\"end\":70779,\"start\":70759},{\"end\":70795,\"start\":70779},{\"end\":70820,\"start\":70795},{\"end\":70833,\"start\":70820},{\"end\":70847,\"start\":70833},{\"end\":70865,\"start\":70847},{\"end\":70873,\"start\":70865},{\"end\":71244,\"start\":71233},{\"end\":71259,\"start\":71244},{\"end\":71272,\"start\":71259},{\"end\":71290,\"start\":71272},{\"end\":71308,\"start\":71290},{\"end\":71725,\"start\":71709},{\"end\":71744,\"start\":71725},{\"end\":71768,\"start\":71744},{\"end\":71780,\"start\":71768},{\"end\":71798,\"start\":71780},{\"end\":71813,\"start\":71798},{\"end\":72367,\"start\":72351},{\"end\":72386,\"start\":72367},{\"end\":72410,\"start\":72386},{\"end\":72422,\"start\":72410},{\"end\":72440,\"start\":72422},{\"end\":72455,\"start\":72440},{\"end\":72762,\"start\":72746},{\"end\":72777,\"start\":72762},{\"end\":72789,\"start\":72777},{\"end\":73099,\"start\":73082},{\"end\":73116,\"start\":73099},{\"end\":73293,\"start\":73277},{\"end\":73308,\"start\":73293},{\"end\":73331,\"start\":73308},{\"end\":73522,\"start\":73506},{\"end\":73537,\"start\":73522},{\"end\":73560,\"start\":73537},{\"end\":73754,\"start\":73742},{\"end\":73871,\"start\":73859},{\"end\":73885,\"start\":73871},{\"end\":73899,\"start\":73885},{\"end\":73910,\"start\":73899},{\"end\":73923,\"start\":73910},{\"end\":74196,\"start\":74181},{\"end\":74321,\"start\":74304},{\"end\":74335,\"start\":74321},{\"end\":74616,\"start\":74609},{\"end\":74628,\"start\":74616},{\"end\":74646,\"start\":74628},{\"end\":75002,\"start\":74989},{\"end\":75026,\"start\":75002},{\"end\":75455,\"start\":75438},{\"end\":75473,\"start\":75455},{\"end\":75487,\"start\":75473},{\"end\":75501,\"start\":75487},{\"end\":75519,\"start\":75501},{\"end\":75859,\"start\":75839},{\"end\":75869,\"start\":75859},{\"end\":75884,\"start\":75869},{\"end\":76202,\"start\":76189},{\"end\":76213,\"start\":76202},{\"end\":76230,\"start\":76213},{\"end\":76242,\"start\":76230},{\"end\":76258,\"start\":76242},{\"end\":76274,\"start\":76258},{\"end\":76290,\"start\":76274},{\"end\":76302,\"start\":76290},{\"end\":76322,\"start\":76302},{\"end\":76335,\"start\":76322},{\"end\":76696,\"start\":76681},{\"end\":76716,\"start\":76696},{\"end\":76730,\"start\":76716},{\"end\":76746,\"start\":76730},{\"end\":76762,\"start\":76746},{\"end\":77170,\"start\":77155},{\"end\":77186,\"start\":77170},{\"end\":77199,\"start\":77186},{\"end\":77211,\"start\":77199},{\"end\":77225,\"start\":77211},{\"end\":77239,\"start\":77225},{\"end\":77250,\"start\":77239},{\"end\":77266,\"start\":77250},{\"end\":77725,\"start\":77713},{\"end\":77730,\"start\":77725},{\"end\":77738,\"start\":77730},{\"end\":77747,\"start\":77738},{\"end\":77983,\"start\":77965},{\"end\":77993,\"start\":77983},{\"end\":78001,\"start\":77993},{\"end\":78018,\"start\":78001},{\"end\":78036,\"start\":78018},{\"end\":78045,\"start\":78036},{\"end\":78060,\"start\":78045},{\"end\":78077,\"start\":78060},{\"end\":78092,\"start\":78077},{\"end\":78111,\"start\":78092},{\"end\":78129,\"start\":78111},{\"end\":78141,\"start\":78129},{\"end\":78681,\"start\":78664},{\"end\":78696,\"start\":78681},{\"end\":79021,\"start\":79006},{\"end\":79034,\"start\":79021},{\"end\":79049,\"start\":79034},{\"end\":79410,\"start\":79396},{\"end\":79425,\"start\":79410},{\"end\":79441,\"start\":79425},{\"end\":79458,\"start\":79441},{\"end\":79482,\"start\":79458},{\"end\":79495,\"start\":79482},{\"end\":79902,\"start\":79883},{\"end\":79918,\"start\":79902},{\"end\":79945,\"start\":79918},{\"end\":79958,\"start\":79945},{\"end\":79965,\"start\":79958},{\"end\":80276,\"start\":80250},{\"end\":80288,\"start\":80276},{\"end\":80298,\"start\":80288},{\"end\":80308,\"start\":80298},{\"end\":80318,\"start\":80308},{\"end\":80335,\"start\":80318},{\"end\":80348,\"start\":80335},{\"end\":80362,\"start\":80348},{\"end\":80372,\"start\":80362},{\"end\":80857,\"start\":80841},{\"end\":80871,\"start\":80857},{\"end\":80884,\"start\":80871},{\"end\":80901,\"start\":80884},{\"end\":80914,\"start\":80901},{\"end\":80929,\"start\":80914},{\"end\":80944,\"start\":80929},{\"end\":80962,\"start\":80944},{\"end\":81379,\"start\":81363},{\"end\":81396,\"start\":81379},{\"end\":81413,\"start\":81396},{\"end\":81428,\"start\":81413},{\"end\":81453,\"start\":81428},{\"end\":81466,\"start\":81453},{\"end\":81874,\"start\":81864},{\"end\":81885,\"start\":81874},{\"end\":81898,\"start\":81885},{\"end\":81912,\"start\":81898},{\"end\":81925,\"start\":81912},{\"end\":81950,\"start\":81925},{\"end\":82274,\"start\":82262},{\"end\":82289,\"start\":82274},{\"end\":82299,\"start\":82289},{\"end\":82309,\"start\":82299},{\"end\":82314,\"start\":82309},{\"end\":82706,\"start\":82695},{\"end\":82718,\"start\":82706},{\"end\":82731,\"start\":82718},{\"end\":82749,\"start\":82731},{\"end\":82760,\"start\":82749},{\"end\":83032,\"start\":83020},{\"end\":83045,\"start\":83032},{\"end\":83054,\"start\":83045},{\"end\":83066,\"start\":83054},{\"end\":83079,\"start\":83066},{\"end\":83093,\"start\":83079},{\"end\":83101,\"start\":83093},{\"end\":83109,\"start\":83101},{\"end\":83393,\"start\":83383},{\"end\":83406,\"start\":83393},{\"end\":83422,\"start\":83406},{\"end\":83625,\"start\":83612},{\"end\":83639,\"start\":83625},{\"end\":83663,\"start\":83639},{\"end\":83676,\"start\":83663},{\"end\":83692,\"start\":83676},{\"end\":83697,\"start\":83692},{\"end\":84192,\"start\":84178},{\"end\":84201,\"start\":84192},{\"end\":84214,\"start\":84201},{\"end\":84226,\"start\":84214},{\"end\":84241,\"start\":84226},{\"end\":84592,\"start\":84578},{\"end\":84609,\"start\":84592},{\"end\":84631,\"start\":84609},{\"end\":84647,\"start\":84631},{\"end\":84664,\"start\":84647},{\"end\":84677,\"start\":84664},{\"end\":84693,\"start\":84677},{\"end\":84713,\"start\":84693},{\"end\":84728,\"start\":84713},{\"end\":84748,\"start\":84728},{\"end\":84761,\"start\":84748},{\"end\":84777,\"start\":84761},{\"end\":84796,\"start\":84777},{\"end\":84814,\"start\":84796},{\"end\":84832,\"start\":84814},{\"end\":85239,\"start\":85225},{\"end\":85256,\"start\":85239},{\"end\":85278,\"start\":85256},{\"end\":85289,\"start\":85278},{\"end\":85582,\"start\":85567},{\"end\":85597,\"start\":85582},{\"end\":85613,\"start\":85597},{\"end\":85811,\"start\":85799},{\"end\":85828,\"start\":85811},{\"end\":85844,\"start\":85828},{\"end\":85862,\"start\":85844},{\"end\":85874,\"start\":85862},{\"end\":86211,\"start\":86197},{\"end\":86221,\"start\":86211},{\"end\":86233,\"start\":86221},{\"end\":86243,\"start\":86233},{\"end\":86255,\"start\":86243},{\"end\":86268,\"start\":86255},{\"end\":86278,\"start\":86268}]", "bib_venue": "[{\"end\":67653,\"start\":67587},{\"end\":70155,\"start\":70093},{\"end\":71954,\"start\":71892},{\"end\":75167,\"start\":75105},{\"end\":76903,\"start\":76841},{\"end\":80513,\"start\":80451},{\"end\":82455,\"start\":82393},{\"end\":83826,\"start\":83770},{\"end\":55276,\"start\":55224},{\"end\":55594,\"start\":55480},{\"end\":56210,\"start\":56166},{\"end\":56712,\"start\":56674},{\"end\":57280,\"start\":57228},{\"end\":57757,\"start\":57708},{\"end\":58062,\"start\":57953},{\"end\":58450,\"start\":58361},{\"end\":58843,\"start\":58798},{\"end\":59123,\"start\":59042},{\"end\":59433,\"start\":59376},{\"end\":59745,\"start\":59657},{\"end\":60265,\"start\":60224},{\"end\":60629,\"start\":60580},{\"end\":61069,\"start\":60997},{\"end\":61406,\"start\":61362},{\"end\":61848,\"start\":61773},{\"end\":62248,\"start\":62190},{\"end\":62804,\"start\":62760},{\"end\":63093,\"start\":63024},{\"end\":63429,\"start\":63367},{\"end\":63792,\"start\":63729},{\"end\":64309,\"start\":64225},{\"end\":64534,\"start\":64486},{\"end\":64787,\"start\":64721},{\"end\":65076,\"start\":65009},{\"end\":65496,\"start\":65406},{\"end\":65884,\"start\":65810},{\"end\":66361,\"start\":66265},{\"end\":66763,\"start\":66695},{\"end\":67211,\"start\":67141},{\"end\":67585,\"start\":67504},{\"end\":67922,\"start\":67909},{\"end\":68581,\"start\":68512},{\"end\":69325,\"start\":69263},{\"end\":69633,\"start\":69568},{\"end\":70091,\"start\":70014},{\"end\":70507,\"start\":70463},{\"end\":70757,\"start\":70678},{\"end\":71369,\"start\":71322},{\"end\":71890,\"start\":71813},{\"end\":72459,\"start\":72455},{\"end\":72838,\"start\":72789},{\"end\":73080,\"start\":73027},{\"end\":73275,\"start\":73224},{\"end\":73504,\"start\":73451},{\"end\":73740,\"start\":73682},{\"end\":73986,\"start\":73923},{\"end\":74204,\"start\":74196},{\"end\":74706,\"start\":74646},{\"end\":75103,\"start\":75026},{\"end\":75571,\"start\":75519},{\"end\":75837,\"start\":75779},{\"end\":76187,\"start\":76068},{\"end\":76839,\"start\":76762},{\"end\":77475,\"start\":77431},{\"end\":77766,\"start\":77747},{\"end\":78181,\"start\":78141},{\"end\":78662,\"start\":78543},{\"end\":79107,\"start\":79063},{\"end\":79552,\"start\":79508},{\"end\":79881,\"start\":79819},{\"end\":80449,\"start\":80372},{\"end\":81011,\"start\":80962},{\"end\":81502,\"start\":81466},{\"end\":81862,\"start\":81797},{\"end\":82391,\"start\":82314},{\"end\":83018,\"start\":82966},{\"end\":83381,\"start\":83335},{\"end\":83768,\"start\":83697},{\"end\":84176,\"start\":84113},{\"end\":84576,\"start\":84455},{\"end\":85384,\"start\":85289},{\"end\":85640,\"start\":85613},{\"end\":85923,\"start\":85874},{\"end\":86330,\"start\":86278}]"}}}, "year": 2023, "month": 12, "day": 17}