{"id": 222379282, "updated": "2023-10-06 10:37:31.24", "metadata": {"title": "Improving Neural Network Verification through Spurious Region Guided Refinement", "authors": "[{\"first\":\"Pengfei\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Renjue\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jianlin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Cheng-Chao\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jingyi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Bai\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Lijun\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "Tools and Algorithms for the Construction and Analysis of Systems", "journal": "Tools and Algorithms for the Construction and Analysis of Systems", "publication_date": {"year": 2021, "month": 3, "day": 1}, "abstract": "We propose a spurious region guided refinement approach for robustness verification of deep neural networks. Our method starts with applying the DeepPoly abstract domain to analyze the network. If the robustness property cannot be verified, the result is inconclusive. Due to the over-approximation, the computed region in the abstraction may be spurious in the sense that it does not contain any true counterexample. Our goal is to identify such spurious regions and use them to guide the abstraction refinement. The core idea is to make use of the obtained constraints of the abstraction to infer new bounds for the neurons. This is achieved by linear programming techniques. With the new bounds, we iteratively apply DeepPoly, aiming to eliminate spurious regions. We have implemented our approach in a prototypical tool DeepSRGR. Experimental results show that a large amount of regions can be identified as spurious, and as a result, the precision of DeepPoly can be significantly improved. As a side contribution, we show that our approach can be applied to verify quantitative robustness properties.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.07722", "mag": "3092751871", "acl": null, "pubmed": null, "pubmedcentral": "7979199", "dblp": "conf/tacas/YangLLHWSXZ21", "doi": "10.1007/978-3-030-72016-2_21"}}, "content": {"source": {"pdf_hash": "43f043ec3bbb99ed901f1cad979fc4f2c06895c7", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://link.springer.com/content/pdf/10.1007/978-3-030-72016-2_21.pdf", "status": "HYBRID"}}, "grobid": {"id": "3c7bb325a20bf502c5f17ca8e057c71084ee21e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/43f043ec3bbb99ed901f1cad979fc4f2c06895c7.txt", "contents": "\nImproving Neural Network Verification through Spurious Region Guided Refinement\n\n\nPengfei Yang \nInstitute of Software\nSKLCS\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nRenjue Li \nInstitute of Software\nSKLCS\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nJianlin Li \nInstitute of Software\nSKLCS\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nCheng-Chao Huang \nInstitute of Intelligent Software\nGuangzhouChina\n\nCAS Software Testing (Guangzhou) Co., Ltd\nGuangzhouChina\n\nJingyi Wang \nZhejiang University NGICS Platform\nHangzhouChina\n\nJun Sun \nSingapore Management University\nSingaporeSingapore\n\nBai Xue \nInstitute of Software\nSKLCS\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nLijun Zhang zhanglj@ios.ac.cn \nInstitute of Software\nSKLCS\nChinese Academy of Sciences\nBeijingChina\n\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nInstitute of Intelligent Software\nGuangzhouChina\n\nImproving Neural Network Verification through Spurious Region Guided Refinement\n10.1007/978-3-030-72016-2\nWe propose a spurious region guided refinement approach for robustness verification of deep neural networks. Our method starts with applying the DeepPoly abstract domain to analyze the network. If the robustness property cannot be verified, the result is inconclusive. Due to the over-approximation, the computed region in the abstraction may be spurious in the sense that it does not contain any true counterexample. Our goal is to identify such spurious regions and use them to guide the abstraction refinement. The core idea is to make use of the obtained constraints of the abstraction to infer new bounds for the neurons. This is achieved by linear programming techniques. With the new bounds, we iteratively apply DeepPoly, aiming to eliminate spurious regions. We have implemented our approach in a prototypical tool DeepSRGR. Experimental results show that a large amount of regions can be identified as spurious, and as a result, the precision of DeepPoly can be significantly improved. As a side contribution, we show that our approach can be applied to verify quantitative robustness properties.\n\nIntroduction\n\nIn the seminal work [34], deep neural networks (DNN) have been successfully applied in Go to play against expert humans. Afterwards, they have achieved exceptional performance in many other applications such as image, speech and audio recognition, selfdriving cars, and malware detection. Despite the success of solving these problems, DNNs have also been shown to be often lack of robustness, and are vulnerable to adversarial samples [39]. Even for a well-trained DNN, a small (and even imperceptible) perturbation may fool the network. This is arguably one of the major obstacles when we deploy DNNs in safety-critical applications like self-driving cars [42], and medical systems [33].\n\nIt is thus important to guarantee the robustness of DNNs for safety-critical applications. In this work, we focus on (local) robustness, i.e., given an input and a manipulation region around the input (which is usually specified according to a certain norm), we verify that a given DNN never makes any mistake on any input in the region. The first work on DNN verification was published in [30], which focuses on DNNs with sigmoid activation functions with a partition-refinement approach. In 2017, Katz et al. [20] and Ehlers [10] independently implemented Reluplex and Planet, two SMT solvers to verify DNNs with the ReLU activation function on properties expressible with SMT constraints. Since 2018, abstract interpretation has been one of the most popular methods for DNN verification in the lead of AI 2 [13], and subsequent works like [36,37,23,1,35,28,24] have improved AI 2 in terms of efficiency, precision and more activation functions (like sigmoid and tanh) so that abstract interpretation based approach can be applied to DNNs of larger size and more complex structures.\n\nAmong the above methods, DeepPoly [37] is a most outstanding one regarding precision and scalability. DeepPoly is an abstract domain specially developed for DNN verification. It sufficiently considers the structures and the operators of a DNN, and it designs a polytope expression which not only fits for these structures and operators to control the loss of precision, but also works with a very small time overhead to achieve scalability. However, as an abstraction interpretation based method, it provides very little insight if it fails to verify the property. In this work, we propose a method to improve DeepPoly by eliminating spurious regions through abstraction refinement. A spurious region is a region computed using abstract semantics, conjuncted with the negation of the property to be verified. This region is spurious in the sense that if the property is satisfied, then this region, although not empty, does not contain any true counterexample which can be realized in the original program. In this case, we propose a refinement strategy to rule out the spurious region, i.e., to prove that this region does not contain any true counterexamples.\n\nOur approach is based on DeepPoly and improves it by refinement of the spurious region through linear programming. The core idea is to intersect the abstraction constructed by abstract interpretation with the negation of the property to generate a spurious region, and perform linear programming on the constraints of the spurious region so that the bounds of the ReLU neurons whose behaviors are uncertain can be tightened. As a result, some of these neurons can be determined to be definitely activated or deactivated, which significantly improves the precision of the abstraction given by abstract interpretation. This procedure can be performed iteratively and the precision of the abstraction are gradually improved, so that we are likely to rule out this spurious region in some iteration. If we successfully rule out all the possible spurious regions through such an iterative refinement, the property is soundly verified. Our method is similar in spirit to counterexample guided abstraction refinement (CEGAR) [6], i.e., we apply abstract interpretation for abstraction and linear programming for refinement. A fundamental difference is that we use the constraints of the spurious region, instead of a concrete counterexample (which is challenging to construct in our setting), as the guidance of refinement.\n\nThe same spurious region guided refinement approach is also effective in quantitative robustness verification. Instead of requiring that all inputs in the region should be correctly classified, a certain probability of error in the region is allowed. Quantitative robustness is more realistic and general compared to the ordinary robustness, and a DNN verified against quantitative robustness is useful in practice as well. The spurious region guided refinement approach naturally fits for this setting, since a comparatively precise over-approximation of the spurious region implies a sound robustness confidence. To the best of our knowledge, for DNNs, this is the first work to verify quantitative robustness with strict soundness guarantee, which distinguishes our approach from the previous sampling based methods like [45,46,3].\n\nIn summary, our main contributions are as follows:\n\n-We propose spurious region guided refinement to verify robustness properties of deep neural networks. This approach significantly improves the precision of Deep-Poly and it can verify more challenging properties than DeepPoly. -We implement the algorithms as a prototype and run them on networks trained on popular datasets like MNIST and ACAS Xu. The experimental results show that our approach significantly improves the precision of DeepPoly in successfully verifying much stronger robustness properties (larger maximum radius) and determining the behaviors of a great proportion of uncertain ReLU neurons. -We apply our approach to solve quantitative robustness verification problem with strict soundness guarantee. In the experiments, we observe that, comparing to using only DeepPoly, the bounds by our approach can be up to two orders of magnitudes better in the experiments.\n\nOrganisations of the paper. We provide preliminaries in Section 2. DeepPoly is recalled in Section 3. We present our overall verification framework and the algorithm in Section 4, and discuss quantitative robustness verification in Section 5. Section 6 evaluates our algorithms through experiments. Section 7 reviews related works and concludes the paper.\n\n\nPreliminaries\n\nIn this section we recall some basic notions on deep neural networks, local robustness verification, and abstract interpretation. Given a vector x \u2208 R m , we write x i to denote its i-th entry for 1 \u2264 i \u2264 m.\n\n\nRobustness verification of deep neural networks\n\nIn this work, we focus on deep feedforward neural networks (DNNs), which can be represented as a function f : R m \u2192 R n , mapping an input x \u2208 R m to its output y = f (x) \u2208 R n . A DNN f often classifies an input x by obtaining the maximum dimension of the output, i.e., arg max 1\u2264i\u2264n f (x) i . We denote such a DNN by\nC f : R m \u2192 C which is defined by C f (x) = arg max 1\u2264i\u2264n f (x) i where C = {1, .\n. . , n} is the set of classification classes. A DNN has a sequence of layers, including an input layer at the beginning, followed by several hidden layers, and an output layer in the end. The output of a layer is the input of the next layer. Each layer contains multiple neurons, the number of which is known as the dimension of the layer. The DNN f is the composition of the transformations between layers. Typically an affine transformation followed by a non-linear activation function is performed. For an affine transformation y = Ax + b, if the matrix A is not sparse, we call such a layer fully connected. A DNN with only fully connected layers and activation functions is a fully connected neural network (FNN). In this work, we focus on the rectified linear unit (ReLU) activation function, defined as ReLU(x) = max(x, 0) for x \u2208 R. Typically, a DNN verification problem is defined as follows:\nDefinition 1.\nGiven a DNN f : R m \u2192 R n , a set of inputs X \u2286 R m , and a property P \u2286 R n , we need to determine whether f (X) := {f (x) | x \u2208 X} \u2286 P holds.\n\nLocal robustness describes the stability of the behaviour of a normal input under a perturbation. The range of input under this perturbation is the robustness region. For a DNN C f (x) which performs classification tasks, a robustness property typically states that C f outputs the same class on the robustness region.\n\nThere are various ways to define a robustness region, and one of the most popular ways is to use the L p norm. For x \u2208 R m and 1 \u2264 p < \u221e, we define the L p norm of\nx to be x p = ( m i=1 |x i | p ) 1 p , and its L \u221e norm x \u221e = max 1\u2264i\u2264m |x i |. We writ\u0113 B p (x, r) := {x \u2208 R m | x\u2212x p \u2264 r}\nto represent a (closed) L p ball for x \u2208 R m and r > 0, which is a neighbourhood of x as its robustness region. If we set X =B p (x, r) and P = {y \u2208 R n | arg max i y i = C f (x)} in Def. 1, it is exactly the robustness verification problem. Hereafter, we set p = \u221e.\n\n\nAbstract interpretation for DNN verification\n\nAbstract interpretation [7] is a static analysis method and it is aimed to find an overapproximation of the semantics of programs and other complex systems so as to verify their correctness. Generally we have a function f : R m \u2192 R n representing the concrete program, a set X \u2286 R m representing the property that the input of the program satisfies, and a set P \u2286 R n representing the property to verify. The problem is to determine whether f (X) \u2286 P holds. However, in many cases it is difficult to calculate f (X) and to determine whether f (X) \u2286 P holds. Abstract interpretation uses abstract domains and abstract transformations to over-approximate sets and functions so that an overapproximation of the output can be obtained efficiently.\n\nNow we have a concrete domain C, which includes X as one of its elements. To make computation efficient, we need an abstract domain A to abstract elements in the concrete domain. We assume that there is a partial order \u2264 on C and A, which in our settings is the subset relation \u2286. We also have a concretization function \u03b3 : A \u2192 C which assigns an abstract element to its concrete semantics, and \u03b3(a) is the least upper bounds of the concrete elements that can be soundly abstracted by a \u2208 A. Naturally a \u2208 A is a sound abstraction of c \u2208 C if and only if c \u2264 \u03b3(a).\n\nThe design of an abstract domain is one of the most important problems in abstract interpretation because it determines the efficiency and precision. In practice, we use a certain type of constraints to represent the abstract elements in an abstract domain. Classical abstract domains for Euclidean spaces include Box, Zonotope [14,15], and Polyhedra [38].\n\nNot only do we need abstract domains to over-approximate sets, but we are also required to adopt over-approximation to functions. Here we consider the lifting of the function f :\nR m \u2192 R n defined as T f (X) : P(R m ) \u2192 P(R n ), T f (X) := f (X) = {f (x) | x \u2208 X}. Now we have an abstract domain A k for the k-dimension Euclidean space and the corresponding concretization \u03b3. A function T # f : A m \u2192 A n is a sound abstract transformer of T f , if T f \u2022 \u03b3 \u2286 \u03b3 \u2022 T #\nf . When we have a sound abstraction X # \u2208 A of X and a sound abstract transformer\nT # f , we can use the concretization of T # f (X # ) to over-approximate f (X) since we have f (X) = T f (X) \u2286 T f (\u03b3(X # )) \u2286 \u03b3 \u2022 T # f (X # ). If \u03b3 \u2022 T # f (X # )\n\u2286 P , the property P is successfully verified. Obviously, verification through abstract interpretation is sound but not complete. Hereafter, we write f # to represent T # f for simplicity. AI 2 [13] first adopted abstract interpretation to verify DNNs, and many subsequent works like [36,37,23] focused on improving its efficiency and precision through, e.g., defining new abstract domains. As a deep neural network, the function f : R m \u2192 R n can be regarded as a composition f = f l \u2022 \u00b7 \u00b7 \u00b7 \u2022 f 1 of its l + 1 layers, where f j performs the transformation between the j-th and the (j + 1)-th layer, i.e., it can be an affine transformation, or a ReLU operation. If we choose Box, Zonotope, or Polyhedra as the abstract domain, then for linear transformations and the ReLU functions, their abstract transformers have been developed in [13]. After we have abstract transformers f # j for these f j , we can conduct abstract interpretation layer by layer as\nf # l \u2022 \u00b7 \u00b7 \u00b7 \u2022 f # 1 (X # ).\n\nA Brief Introduction to DeepPoly\n\nOur approach relies on the abstract domain DeepPoly [37], which is the state-of-the-art abstract domain for DNN verification. It defines the abstract transformers of multiple activation functions and layers used in DNNs. The core idea of DeepPoly is to give every variable an upper and a lower bound in the form of an affine expression using only variables that appear before it. It can express a polyhedron globally. Moreover, experimentally, it often has better precision than Box and Zonotope domains. We denote the n-dimensional DeepPoly abstract domain with A n . Formally an abstract element a \u2208 A n is a tuple (a \u2264 , a \u2265 , l, u), where a \u2264 and a \u2265 give the i-th variable x i a lower bound and an upper bound, respectively, in the form of a linear combination of variables which appear before it, i.e. i\u22121 k=1 w k x k + w 0 , for i = 1, . . . , n, and l, u \u2208 R n give the lower bound and upper bound of each variable, respectively. The concretization of a is defined as\n\u03b3(a) = {x \u2208 R n | a \u2264 i \u2264 x i \u2264 a \u2265 i , i = 1, . . . , n}.(1)\nThe abstract domain A n also requests that its abstract elements a should satisfy the invariant \u03b3(a) \u2286 [l, u]. This invariant helps construct efficient abstract transformers. For an affine transformation\nx i = i\u22121 k=1 w k x k + w 0 , we set a \u2264 i = a \u2265 i = i\u22121 k=1 w k x k + w 0 .\nBy substituting the variables x j appearing in a \u2264 i with a \u2264 j or a \u2265 j according to its coefficient at most i \u2212 1 times, we can obtain a sound lower bound in the form of linear  -If l j \u2265 0 or u j \u2264 0, this ReLU neuron is definitely activated or deactivated, respectively. In this case, this ReLU transformation actually performs an affine transformation, and thus its abstract transformer can be defined as above.\n\n-If l j < 0 and u j > 0, the behavior of this ReLU neuron is uncertain, and we need to over-approximate this relation with a linear upper/lower bound. The best upper bound is\na \u2265 i = uj (xj \u2212lj )\nuj \u2212lj . For the lower bound, there are multiple choices\na \u2264 i = \u03bbx j where \u03bb \u2208 [0, 1].\nWe choose \u03bb \u2208 {0, 1} which minimizes the area of the constraints. Basically we have two abstraction modes here, corresponding to the two choices of \u03bb.\n\nNote that for a DNN with only ReLU as non-linear operators, over-approximation occurs only when there are uncertain ReLU neurons, which are over-approximated using a triangle. The key of improving the precision is thus to compute the bounds of the uncertain ReLU neurons as precisely as possible, and to determine the behaviors of the most uncertain ReLU neurons.\n\nDeepPoly also supports activation functions which are monotonically increasing, convex on (\u2212\u221e, 0] and concave on [0, +\u221e), like sigmoid and tanh, and it supports max pooling layers. Readers can refer to [37] for details.\n\n\nSpurious Region Guided Refinement\n\nWe explain the main steps of our algorithm, as depicted in Fig. 1. For the input property and network, we first employ DeepPoly as the initial step to compute f # (X # ). The concretization of f # (X # ) is the conjunction of many linear inequities given in Eq. 1, and for the robustness property P , the negation \u00acP is the disjunction of several linear\ninequities \u00acP = t =C f (x) (y C f (x) \u2212 y t \u2264 0). 1. We check whether f # (X # ) \u2229 # (y C f (x) \u2212 y t \u2264 0) = \u22a5 holds for each t, which\nfollows the same method as DeepPoly, i.e., we compute the lower bound of y C f (x) \u2212 y t and see whether it is larger than 0. In case of yes, it indicates that the label t cannot be classified, as it is dominated by C f (x). Otherwise, we have f # (X # ) \u2229 # \u00acP = \u22a5, we have the conjunction \u03b3(f # (X # ))\u2227\u00acP as a potential spurious region, which represents the intersection of the abstraction of the real semantics and the negation of the property to verify. We call such a region spurious because if the property is satisfied, then this region does not contain a true counterexample, i.e., a pair of input and output (x * , y * ) such that y * = f (x * ) and y * violates the property P . In this case, this region is spuriously constructed due to the abstraction of the real semantics, where the counterexamples cannot be realized, and thus we aim to rule out the spurious region. 2. If no potential spurious region is found, our algorithm safely returns yes. 3. Assume now that we have a the potential spurious region. The core idea is to use the constraints of the spurious region to refine this spurious region. Here a natural way to refine the spurious region is linear programming, since all the constraints here are linear inequities. If the linear programming is infeasible, it indicates that the region is spurious, and thus we can return an affirmative result. Otherwise, our refinement will tighten the bounds of variables involved in the DNN, especially the input variables and uncertain ReLU neurons, and these tightened bounds help further give a more precise abstraction. 4. As our approach is based on DeepPoly, similarly, we cannot guarantee completeness. We set a threshold N of the number of iterations as a simple termination condition. If the termination condition is not reached, we run DeepPoly again, and return to the first step.\n\nBelow we give an example, illustrating how refinement can help in robustness verification.\n\n\nExample 1.\n\nConsider the network f (x) = ReLU 1 \u22121 1 1\n\nx + 0 2.5 and the re-gionB \u221e ((0, 0) T , 1). The robustness property P here is y 2 \u2212 y 1 > 0. We invoke first DeepPoly: the lower bound of y 2 \u2212 y 1 given by DeepPoly is \u22120.5. As a result, the robustness property cannot be verified directly. Fig. 2(a) shows details of the example.\n\nWe fail to verify the property in Example 1 because for the uncertain ReLU relation y 1 = ReLU(x 3 ), the abstraction is imprecise, and the key to making the abstraction more precise here is to obtain as tight a bound as possible for x 3 .\n\n\nExample 2.\n\nWe use the constraints in Fig. 2(a) and additionally the constraint y 2 \u2212y 1 \u2264 0 (i.e., \u00acP ) as the input of linear programming. Our aim is to obtain a tighter bound of the input neurons x 1 and x 2 , as well as the uncertain ReLU neuron x 3 , so the objective functions of the linear programming are min x i and min \u2212x i for i = 1, 2, 3. All the three neurons have a tighter bound after the linear programming (see the red part in Fig. 2(b)). Fig. 2(b) shows the running of DeepPoly under these new bounds, where the input range and the abstraction of the uncertain ReLU neuron are both refined. Now the lower bound of y 2 \u2212 y 1 is 0.25, so DeepPoly successfully verifies the property.  \nl1 = \u22121 x1 \u2264 1 x1 \u2265 \u22121 u3 = 2 l3 = \u22122 x3 \u2264 x1 \u2212 x2 x3 \u2265 x1 \u2212 x2 u5 = 2 l5 = 0 y1 \u2264 0.5x3 + 1 y1 \u2265 0 x2 \u2265 \u22121 x2 \u2264 1 l2 = \u22121 u2 = 1 x4 \u2265 x1 + x2 + 2.5 x4 \u2264 x1 + x2 + 2.5 l4 = 0.5 u4 = 4.5 y2 \u2265 x4 y2 \u2264 x4 l6 = 0.5 u6 = 4.5 (a) x1 x2 x3 x4 y1 y2 u1 = 0 l1 = \u22121 x1 \u2264 0 x1 \u2265 \u22121 x2 \u2265 \u22121 x2 \u2264 \u22120.667 l2 = \u22121 u2 = \u22120.667 u3 = 1 l3 = \u22120.333 x3 \u2264 x1 \u2212 x2 x3 \u2265 x1 \u2212 x2 x4 \u2265 x1 + x2 + 2.5 x4 \u2264 x1 + x2 + 2.5 l4 = 0.5 u4 = 1.833 u5 = 1 l5 = 0 y1 \u2264 0.75x3 + 0.25 y1 \u2265 x3 y2 \u2265 x4 y2 \u2264 x4 l6 = 0.5 u6 = 1.833 (b) 1 1 \u22121 1 ReLU(x3) ReLU(x4) 1 1 \u22121 1 ReLU(x3) ReLU(x4)\n\nMain algorithm\n\nAlg. 1 presents our algorithm. First we run abstract interpretation to find the uncertain neurons and the spurious regions (Line 2-5). For each possible spurious region, we have a while loop which iteratively refines the abstraction. In each iteration we perform linear programming to renew the bounds of the input neurons and uncertain ReLU neurons; when we find that the bound of an uncertain ReLU neuron becomes definitely nonnegative or non-positive, then the ReLU behavior of this neuron is renewed (Line 14-20). We use them to guide abstract interpretation in the next step (Line 21-22). Here in Line 22, we make sure that during the abstract interpretation, the abstraction of previous uncertain neurons (namely the uncertain neurons before the linear programming step in the same iteration) compulsorily follows the new bounds and new ReLU behaviors given by the current C \u22650 , C \u22640 , l, and u, where these bounds will not be renewed by abstract interpretation, and the concretization of Y is defined as\n\u03b3(Y ) = {x | \u2200i. Y \u2264 i \u2264 x i \u2264 Y \u2265 i } \u2229 [l, u].(2)\nThe while loop ends when (i) either we find that the spurious region is infeasible (Line 11, 24) and we proceed to refine the next spurious region, with a label Verified True, (ii) or we reach the terminating condition and fail to rule out this spurious region, in which case we return UNKNOWN. If every while loop ends with the label Verified True, we successfully rule out all the spurious regions and return YES. An observation is that, if some spurious regions have been ruled out, we can add the constraints of their negation to make the current spurious region smaller so as to improve the precision (Line 9).\n\nHere we discuss the soundness of Alg. 1. We focus on the while loop and claim that it has the following loop invariant:\nInvariant 1\nThe abstract element Y over-approximates the intersection of the semantics of f onB \u221e (x, r) and the spurious region, i.e., f (B \u221e (x, r)) \u2229 Spu \u2286 \u03b3(Y ).\n\n\nAlgorithm 1 Spurious region guided robustness verification\n\nInput:\n\nDNN f , input x, radius r.\n\n\nOutput:\n\nReturn \"YES\" if verified, or \"UNKNOWN\" otherwise. 1: function VERIFY(f , x, r) 2:\n\nY0 \u2190 f # (B\u221e(x, r)) abstract interpretation with DeepPoly 3:\n\nVu \u2190 {v | v was marked as uncertain in Line 2} 4:\nA = {t | Y0 \u2229 # (y C f (x) \u2212 yt \u2264 0) = \u22a5} 5: if A = \u2205 then return YES otherwise A = {t1, . . . , t l } 6: for i \u2190 1 to l do 7: Verified \u2190 False, V \u2190 Vu, Y \u2190 Y0 denote Y = (Y \u2264 , Y \u2265 , l, u) 8: C \u22650 \u2190 \u2205, C \u22640 \u2190 \u2205 set of new activated/deactivated neurons 9: Spu \u2190 (y C f (x) \u2212 yt i \u2264 0) \u2227 i\u22121 j=1 (y C f (x) \u2212 yt j \u2265 0)\nspurious region 10:\n\nwhile terminating condition not satisfied do 11:\n\nif Y \u2227 Spu is infeasible then 12:\n\nVerified \u2190 True 13: break 14:\n\nfor\nv \u2208 V \u222a V 0 do V0: set of input neurons 15: (lv, uv) \u2190 LP(Y \u2227 Spu, v) 16:\nfor v \u2208 V do 17:\n\nif lv \u2265 0 then 18:\nC \u22650 \u2190 C \u22650 \u222a {v}, V \u2190 V \\ {v} 19: else if uv \u2264 0 then 20: C \u22640 \u2190 C \u22640 \u222a {v}, V \u2190 V \\ {v} 21: X \u2190 v\u2208V 0 {lv \u2264 v \u2264 uv} 22:\nY \u2190 f # (X) according to C \u22650 , C \u22640 , l, and u 23:\nV \u2190 {v | v was marked as uncertain in Line 22} \\ (C \u22650 \u222a C \u22640 ) 24: if Y \u2229 # (y C f (x) \u2212 yt i \u2264 0) = \u22a5 then 25:\nVerified \u2190 True 26: break 27:\n\nif Verified = False then return UNKNOWN 28:\n\nreturn YES\n\nThe initialization of Y is f # (B \u221e (x, r)) and it is naturally an over-approximation.\n\nThe box X is obtained by linear programming on Y \u2227 Spu, and f # (X) is calculated through abstract interpretation and the bounds given by linear programming on Y \u2227 Spu, and thus it remains an over-approximation. It is worth mentioning that, when we run DeepPoly in Line 22, we are using the bounds obtained by linear programming to guide DeepPoly, and this may violate the invariant \u03b3(a) \u2286 [l, u] mentioned in Sect. 3. Nonotheless, soundness still holds since the concretization of Y is newly defined in Eq. 2, where both items in the intersection over-approximate f (B \u221e (x, r)) \u2229 Spu. With Invarient 1, Alg. 1 returns YES if for any possible spurious region Spu, the overapproximation of f (B \u221e (x, r)) \u2229 Spu is infeasible, which implies the soundness of Alg. 1.\n\n\nIterative refinement of the spurious region\n\nHere we present more theoretical insight on the iterative refinement of the spurious region. An iteration of the while loop in Alg. 1 can be represented as a function L : A \u2192 A, where A is the DeepPoly domain. An interesting observation is that, the abstract transformer f # in the DeepPoly domain is not necessarily increasing, because different input ranges, even if they have inclusion relation, may lead to different choices of the abstraction mode of some uncertain ReLU neurons, which may violate the inclusion relation of abstraction. We have found such examples during our experiment, which is illustrated in the following example. .\nWe have f # (I 1 ) = {(x 1 , x 2 ) T \u2208 R 2 | \u22122 \u2264 x 1 \u2264 1, x 2 \u2265 0, x 2 \u2264 1 3 x 1 + 2 3 } and f # (I 2 ) = {(x 1 , x 2 ) T \u2208 R 2 | \u22122 \u2264 x 1 \u2264 3, x 2 \u2265 x 1 , x 2 \u2264 3 5 x 1 + 6 5 }. We observe (1, 0) T \u2208 f # (I 1 ) but (1, 0) T / \u2208 f # (I 2 )\n, which implies that the transformer f # is not increasing.\n\nThis fact also implies that L is not necessarily increasing, which violates the condition of Kleene's Theorem on fixed point [4]. Now we turn to the analysis of the sequence Lemma 1 implies that if our sequence {Y k } is decreasing, then the iterative refinement converges to an abstract element in DeepPoly, which is the greatest fixed point of L that is smaller than f # (B \u221e (x, r)). A sufficient condition for {Y k } being decreasing is that during the abstract interpretation in every Y k , every initial uncertain neuron maintains its abstraction mode, i.e. its corresponding \u03bb does not change, before its ReLU behavior is determined. A weaker sufficient condition for convergence is that change in abstraction mode of uncertain neurons never happens after finitely many iterations.\n{Y k = L k (f # (B \u221e (x, r)))} \u221e k=1 ,\nIf the abstraction mode of uncertain neurons changes infinitely often, generally the sequence {Y k } does not converge. In this case, we can consider its subsequence in which every Y k is obtained with the same abstraction mode. It is easy to see that such a subsequence must be decreasing and thus have a meet, as it is an accumulative point of the sequence {Y k }. Since there are only finitely many choices of abstraction modes, such a accumulative points exists in {Y k }, and there are only finitely many accumulative points. We conclude these results in the following theorem which describes the convergence behavior of our iterative refinement of the spurious region: Proof. Since the abstraction modes of uncertain ReLU neurons have only finitely many choices, there must be one which happens infinitely often in the computation of the sequence {Y k }, and we choose the subsequence {Y n k } in which every item is computed through this abstraction mode. Obviously {Y n k } is decreasing and thus has a meet. For a decreasing subsequence {Y n k }, we can find its subsequnce in which the abstraction mode of uncertain ReLU neurons does not change, and they have the same meet. Since there are only finitely many choices of abstraction modes of uncertain ReLU neurons, such accumulative points of {Y k } also have finitely many values. If exact one abstraction mode of uncertain ReLU neurons happens infinitely often, obviously there is only one accumulative point in {Y k }.\n\n\nOptimizations\n\nIn the implementation of our main algorithm, we propose the following optimizations to improve the precision of refinement.\n\nOptimization 1: More precise constraints in linear programming. In Line 15 of Alg. 1, it is not the best choice to take the linear constraints in the abstract element Y into linear programming, because the abstraction of uncertain ReLU neurons in DeepPoly is not the best. Planet [10] has a component which gives a more precise linear approximation for uncertain ReLU relations, where it uses the linear constraints y \u2264 u(x\u2212l) u\u2212l , y \u2265 x, y \u2265 0 to over-approximate the relation y = ReLU(x) with x \u2208 [l, u].\n\nOptimization 2: Priority to work on small spurious regions. In Line 6 of Alg. 1,we determine the order of refining the spurious regions based on their sizes, i.e., a smaller region is chosen earlier. This is based on the intuition that Alg. 1 works effectively if the spurious region is small. After the small spurious regions are ruled out, the constraints of large spurious regions can be tightened with the conjunction\ni\u22121 j=1 (y C f (x) \u2212 y tj \u2265 0).\nIt is difficult to strictly determine which spurious region is the smallest, and thus we refer to the lower bound of y C f (x) \u2212 y ti given by DeepPoly, i.e., the larger this lower bound is, the smaller the spurious region is likely to be, and we perform the for loop in Line 6 of Alg. 1 in this order.\n\n\nQuantitative Robustness Verification\n\nIn this section we recall the notion of quantitative robustness and show how to verify a quantitative robustness property of a DNN with spurious region guided refinement.\n\nIn practice, we may not need a strict condition of robustness to ensure that an input x is not an adversarial example. A notion of mutation testing is proposed in [44,43], which requires that an input x is normal if it has a low label change rate on its neighbourhood. They follow a statistical way to estimate the label change rate of an input, which motivates us to give a formal definition of the property showing a low label change rate, and to consider the verification problem for such a property. Below we recall the definition of quantitative robustness [27], where we have a parameter 0 < \u03b7 \u2264 1 representing the confidence of robustness.\nDefinition 2. Given a DNN C f : R m \u2192 C, an input x \u2208 R m , r > 0, 0 < \u03b7 \u2264 1, and a probability measure \u03bc onB \u221e (x, r), f is \u03b7-robust at x, if \u03bc({x \u2208B \u221e (x, r) | C f (x ) = C f (x)}) \u2265 \u03b7.\nDef. 2 has a tight association with label change rate, i.e., if x is \u03b7-robust, then the label change rate should be smaller than, or close to 1 \u2212 \u03b7. Hereafter, we set \u03bc to be the uniform distribution onB \u221e (x, r).\n\nIt is natural to adapt spurious region guided refinement to quantitative robustness verification. In Alg. 1, we do not return UNKNOWN when we cannot rule out a spurious region, but record the volume of the box X as an over-approximation of the Lebesgue measure of the spurious region. After we work on all the spurious regions, we calculate the sum of these volume, and obtain a sound robustness confidence. Here we do not calculate the volume of the spurious region because precise calculation of volume of a high-dimensional polytope remains open, and we do not choose to use randomized algorithms because it may not be sound.\n\nWe further improve the algorithm through the powerset technique [13]. Powerset technique is a classical and effective way to enhance the precision of abstract interpretation. We split the input region into several subsets, and run abstract interpretation on these subsets, In our quantitative robustness verification setting, powerset technique not only improves the precision, but also accelerates the algorithm in some situations: If the subsets have the same volume, and the percentage of the subsets on which we may fail to verify robustness is already smaller than 1 \u2212 \u03b7, then we have successfully verified the \u03b7-robustness property.\n\n\nExperimental Evaluation\n\nWe implement our approach as a prototype called DeepSRGR. The implementation is based on a re-implementation of the ReLU and the affine abstract transformers of DeepPoly in Python 3.7 and we amend it accordingly to implement Alg. 1. We use CVXPY [8] as our modeling language for convex optimization problems and CBC [18] as the LP solver. It is worth mentioning that we ignore the floating point error in our re-implementation of DeepPoly because sound linear programming currently does not scale in our experiments. In the terminating condition, we set N = 5. The two optimizations in Sect. 4.3 are adopted in all the experiments. All the experiments are conducted on a CentOS 7.7 server with 16 Intel Xeon Platinum 8153 @2.00GHz (16 cores) and 512G RAM, and they use 96 sub-processes concurrently at most. Readers can find all the source code and other experimental materials in https://iscasmc.ios.ac. cn/ToolDownload/?Tool=DeepSRGR.\n\nDatasets. We use MNIST [22] and ACAS Xu [12,17] as the datasets in our experiments. MNIST contains 60 000 grayscale handwritten digits of the size 28 \u00d7 28. We can train DNNs to classify the images by the written digits on them. The ACAS Xu system is aimed to avoid airborne collisions for unmanned aircrafts and it uses an observation table to make decisions for the aircraft. In [19], the observation table is realized by training DNNs instead of storing it.\n\nNetworks. On MNIST, we trained seven fully connected networks of the size 6 \u00d7 20, 3 \u00d7 50, 3 \u00d7 100, 6 \u00d7 100, 6 \u00d7 200, 9 \u00d7 200, and 6 \u00d7 500, where m \u00d7 n refers m hidden layers and n neurons in each hidden layer, and we name them from FNN2 to FNN8, respectively (we also have a small network FNN1 for testing). On ACAS Xu, we randomly choose three networks used in [20], all of the size 6 \u00d7 50.\n\n\nImprovement in precision\n\nFirst we compare DeepPoly and DeepSRGR in terms of their precision of robustness verification. We consider the following two indices: (i) the maximum radius that the two tools can verify, and (ii) the number of uncertain ReLU neurons whose behaviors can be further determined by DeepSRGR. For each network, we randomly choose three images from the MNIST dataset, and calculate their maximum radius that the two tools can verify through a binary search on the seven FNNs. In column \"# uncertin ReLU\" we record the number of the uncertain ReLU neurons when first applying DeepPoly, and also count how many of them are renewed, namely become definitely activated/deactivated in later iterations when applying DeepSRGR. Table 1 shows the results. We can see from Table 1 that DeepSRGR can verify much stronger (i.e., larger maximum radius) robustness properties than DeepPoly. The average number of iterations for ruling out a spurious region is 2.875, and about half of the spurious regions can be ruled out within 2 iterations. DeepSRGR sometimes determines behaviors of a large proportion of uncertain ReLU neurons on large networks: Considering the last picture of the most challenging network FNN8, more than ninety percent (92.6% \u2248 1269 1371 ) of the uncertain neurons are renewed. Improvement in precision evaluated in this experiment works for verification of both robustness and quantitative robustness, and this is why our method is effective in both tasks.\n\n\nRobustness verification performance\n\nIn this setting, we randomly choose 50 samples from the MNIST dataset. We fix four radii, 0.037, 0.026, 0.021, and 0.015 for the four networks FNN4 -FNN7 respectively, and verify the robustness property with the corresponding radius on the 50 inputs. The radius chosen here is very challenging for the corresponding network.   We observe that, by increasing the termination threshold N from 5 to 50, only two more properties out of 15 can be verified additionally. This suggests that our method can effectively identify these spurious regions which are relevant to verification of the property, in a small number of iterations.\n\n\nQuantitative robustness verification on ACAS Xu networks\n\nWe evaluate DeepSRGR for quantitative robustness verification on ACAS Xu networks. We randomly choose five inputs, and compute the maximum robustness radius for each input on the three networks with DeepPoly through a binary search. In our experiment, the radius for a running example is the maximum robustness radius plus 0.02, 0.03, 0.04, 0.05, and 0.06. We use the powerset technique and the number of splits is 32. For DeepPoly, the robustness confidence it gives is the proportion of the splits on which DeepPoly verifies the property. Fig. 4 shows the results. We can see that DeepSRGR gives significantly better overapproximation of 1\u2212\u03b7 than DeepPoly. That is, in more than 90% running examples, our over-approximation is no more than one half of that given by DeepPoly, and in more than 75% of the cases, our over-approximation is even smaller than one tenth of that given by DeepPoly. \n\n\nRelated Works and Conclusion\n\nWe have already discussed papers mostly related to our paper. Here we add some more new results. Marabou [21] has been developed as the next generation of Reluplex. Recently, verification approach based on abstraction of DNN models has been proposed in [11,2]. In addition, alternative approaches based on constraint-solving [26,29,5,25], layer-by-layer exhaustive search [16], global optimization [31,9,32], functional approximation [47], reduction to two-player games [48,49], and star set abstraction [41,40] have been proposed as well.\n\nIn this work, we propose a spurious region guided refinement approach for robustness and quantitative robustness verification of deep neural networks, where abstract interpretation calculates an abstraction, and linear programming performs refinement with the guidance of the spurious region. Our experimental results show that our tool can significantly improve the precision of DeepPoly, verify more robustness properties, and often provide a quantitative robustness with strict soundness guarantee.\n\nAbstraction interpretation based framework is quite extensive to different DNN models, different properties, and incorporate different verification methods. As future work, we will investigate how to increase the precision further by using more precise linear over-approximation like [35].\n\nFig. 1 .\n1Framework of spurious region guided refinement combination on input variables only, and l i can be computed immediately from the range of input variables. A similar procedure also works for computing u i .For a ReLU transformation x i = ReLU(x j ), we consider two cases:\n\nFig. 2 .\n2Example 1 (left) and Example 2 (right): where the red parts are introduced through linear programming based refinement and the blue parts are introduced by a second run of DeepPoly.\n\nExample 3 .\n3Let f (x) = ReLU(x) with input ranges I 1 = [\u22122, 1] and I 2 = [\u22122, 3]\n\n\nwhere L 1 := L and L k := L \u2022 L k\u22121 for k \u2265 2. First we have the following lemma showing that in our settings every decreasing chain S in the DeepPoly domain A has a meet # S \u2208 A. Lemma 1. Let A n be the n-dimensional DeepPoly domain and {a (k) } \u2286 A n a decreasing bounded sequence of non-empty abstract elements. If the coefficients in a (k),\u2264 i and a (k),\u2265 i are uniformly bounded, then there exists an abstract element a * \u2208 A n s.t. \u03b3(a * ) = \u221e k=1 \u03b3(a (k) ).Remark: The condition that the coefficients in a bounded are naturally satisfied in our setting, since in a DNN the coefficients and bounds involved have only finitely many values. Readers can refer to[50] for a formal proof.\n\nFig. 3 .\n3Number of renewed ReLU behaviors in the spurious regions newly ruled out.\n\nFig. 4 .\n4Quantitative robustness verification using DeepPoly and DeepSRGR\n\nTheorem 2 .\n2There exists a subsequence {Y n k } of {Y k } s.t. {Y n k } is decreasing and thus has a meet # {Y n k }. Moreover, the set # {Y n k } | {Y n k } is a decreasing subsequence of {Y k }is finite, and it is a singleton if exact one abstraction mode of uncertain ReLU neurons \nhappens infinitely often. \n\n\n\nTable 2 Table 2 .\n22presents the results. As we can see, DeepSRGR can verify significantly more properties than DeepPoly. Linear programming in DeepSRGR takes a large amount of time in the experiment, and thus DeepSRGR is less efficient (a DeepPoly run takes noTable 1. Maximum radius which can be verified by DeepPoly and DeepSRGR, and details of DeepSRGR running on its maximum radius, where in the number of renewed uncertain nuerons, we show the largest one among the spurious regions. MAX, AVG, and GT means the maximum, the average, and the grant total among the spurious regions, respectively. The indices of the three images are 414, 481, and 65 in the MNIST dataset. more than 100 seconds on FNN7). Furthermore, we again run the 15 running examples which are not verified by DeepSRGR on FNN4, by resetting the maximum number of iterations to 20 and 50. We have the following observations:-Two more properties (out of 15) are successfully verified when we change N to 20.No more properties can be verified even if we change N from 20 to 50.-In this experiment, 13 more spurious regions are ruled out, six of which takes 6 iterations, one takes 7, two takes 8, and the other four takes 13, 22, 27, and 32 iterations, respectively. In these running examples, the average number of renewed ReLU behaviors is 102.8, and a large proportion are renewed in the last iteration (47.4% on average). Fig. 3 shows the detailed results. -As for the 13 spurious regions which cannot be ruled out within 50 iterations, the average number of renewed ReLU behaviors is only 8.54, which is significantly lower than the average of the 13 spurious regions which are newly ruled out. In these running examples, changes in ReLU behaviors and ReLU abstraction modes do not happen after the 9th iteration, and the average number is 4.4. The number that DeepPoly and DeepSRGR verifies among the 50 inputs, and the maximum/average running time of DeepSRGR.Maximum radius # spurious \nregions \n\n# uncertain ReLU \n% renewed \n# iterations \nDeepPoly DeepSRGR \nOriginal Renewed MAX \nAVG MAX GT \n\n0.034 \n0.047 \n6 \n51 \n38 \n74.5% \n48.4% \n5 \n17 \nFNN2 0.017 \n0.023 \n3 \n47 \n37 \n78.7% \n51.8% \n4 \n9 \n0.017 \n0.023 \n1 \n34 \n25 \n73.5% \n73.5% \n4 \n4 \n\n0.049 \n0.066 \n6 \n88 \n69 \n78.4% \n60.9% \n5 \n15 \nFNN3 0.025 \n0.033 \n7 \n94 \n85 \n90.4% \n46.0% \n5 \n18 \n0.045 \n0.058 \n3 \n98 \n45 \n45.1% \n27.2% \n5 \n9 \n\n0.045 \n0.060 \n6 \n180 \n102 \n56.7% \n35.2% \n5 \n19 \nFNN4 0.024 \n0.030 \n6 \n199 \n144 \n72.4% \n36.5% \n4 \n15 \n0.035 \n0.046 \n2 \n155 \n103 \n66.5% \n42.9% \n5 \n7 \n\n0.034 \n0.042 \n7 \n305 \n245 \n80.3% \n37.8% \n5 \n20 \nFNN5 0.016 \n0.019 \n5 \n315 \n204 \n64.8% \n34.0% \n4 \n14 \n0.021 \n0.027 \n7 \n337 \n256 \n76.0% \n34.9% \n5 \n18 \n\n0.022 \n0.026 \n7 \n683 \n271 \n39.7% \n19.8% \n4 \n18 \nFNN6 0.011 \n0.013 \n6 \n657 \n483 \n73.5% \n36.7% \n3 \n14 \n0.021 \n0.025 \n8 \n723 \n169 \n23.4% \n12.2% \n5 \n21 \n\n0.021 \n0.023 \n9 \n987 \n297 \n30.1% \n10.0% \n5 \n29 \nFNN7 0.010 \n0.011 \n5 \n877 \n648 \n73.9% \n26.8% \n3 \n11 \n0.017 \n0.019 \n7 \n913 \n352 \n38.6% \n24.3% \n3 \n16 \n\n0.037 \n0.044 \n9 \n1 504 \n976 \n64.9% \n45.9% \n5 \n36 \nFNN8 0.020 \n0.022 \n9 \n1 213 \n818 \n67.4% \n33.3% \n3 \n21 \n0.033 \n0.040 \n9 \n1 371 \n1 269 \n92.6% \n51.1% \n5 \n37 \n\n\n\nOptimization and abstraction: a synergistic approach for analyzing neural network robustness. G Anderson, S Pailoor, I Dillig, S Chaudhuri, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. McKinley, K.S., Fisher, K.the 40th ACM SIGPLAN Conference on Programming Language Design and ImplementationPhoenix, AZ, USAACMAnderson, G., Pailoor, S., Dillig, I., Chaudhuri, S.: Optimization and abstraction: a synergis- tic approach for analyzing neural network robustness. In: McKinley, K.S., Fisher, K. (eds.) Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2019, Phoenix, AZ, USA, June 22-26, 2019. pp. 731-744. ACM (2019)\n\nDeepabstract: Neural network abstraction for accelerating verification. P Ashok, V Hashemi, J Kret\u00ednsk\u00fd, S Mohr, Automated Technology for Verification and Analysis -18th International Symposium. Hung, D.V., Sokolsky, O.Hanoi, VietnamSpringer2020Ashok, P., Hashemi, V., Kret\u00ednsk\u00fd, J., Mohr, S.: Deepabstract: Neural network abstraction for accelerating verification. In: Hung, D.V., Sokolsky, O. (eds.) Automated Technology for Verification and Analysis -18th International Symposium, ATVA 2020, Hanoi, Vietnam, October 19-23, 2020, Proceedings. Lecture Notes in Computer Science, vol. 12302, pp. 92- 107. Springer (2020)\n\nScalable quantitative verification for deep neural networks. T Baluta, Z L Chua, K S Meel, P Saxena, CoRR abs/2002.06864Baluta, T., Chua, Z.L., Meel, K.S., Saxena, P.: Scalable quantitative verification for deep neural networks. CoRR abs/2002.06864 (2020), https://arxiv.org/abs/2002.06864\n\nThe contraction principle as a particular case of kleene's fixed point theorem. A Baranga, Discret. Math. 981Baranga, A.: The contraction principle as a particular case of kleene's fixed point theorem. Discret. Math. 98(1), 75-79 (1991)\n\nBranch and bound for piecewise linear neural network verification. R Bunel, J Lu, I Turkaslan, P H S Torr, P Kohli, M P Kumar, J. Mach. Learn. Res. 2139Bunel, R., Lu, J., Turkaslan, I., Torr, P.H.S., Kohli, P., Kumar, M.P.: Branch and bound for piecewise linear neural network verification. J. Mach. Learn. Res. 21, 42:1-42:39 (2020)\n\nCounterexample-guided abstraction refinement. E M Clarke, O Grumberg, S Jha, Y Lu, H Veith, Computer Aided Verification, 12th International Conference. Emerson, E.A., Sistla, A.P.Chicago, IL, USASpringer1855Clarke, E.M., Grumberg, O., Jha, S., Lu, Y., Veith, H.: Counterexample-guided abstraction refinement. In: Emerson, E.A., Sistla, A.P. (eds.) Computer Aided Verification, 12th Inter- national Conference, CAV 2000, Chicago, IL, USA, July 15-19, 2000, Proceedings. Lecture Notes in Computer Science, vol. 1855, pp. 154-169. Springer (2000)\n\nAbstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. P Cousot, R Cousot, Fourth ACM Symposium on Principles of Programming Languages (POPL). Cousot, P., Cousot, R.: Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. In: Fourth ACM Symposium on Principles of Programming Languages (POPL). pp. 238-252 (1977)\n\nCVXPY: A Python-embedded modeling language for convex optimization. S Diamond, S Boyd, Journal of Machine Learning Research. 1783Diamond, S., Boyd, S.: CVXPY: A Python-embedded modeling language for convex opti- mization. Journal of Machine Learning Research 17(83), 1-5 (2016)\n\nOutput range analysis for deep feedforward neural networks. S Dutta, S Jha, S Sankaranarayanan, A Tiwari, NASA Formal Methods -10th International Symposium, NFM 2018. Dutle, A., Mu\u00f1oz, C.A., Narkawicz, A.Newport News, VA, USASpringer10811Dutta, S., Jha, S., Sankaranarayanan, S., Tiwari, A.: Output range analysis for deep feedfor- ward neural networks. In: Dutle, A., Mu\u00f1oz, C.A., Narkawicz, A. (eds.) NASA Formal Meth- ods -10th International Symposium, NFM 2018, Newport News, VA, USA, April 17-19, 2018, Proceedings. Lecture Notes in Computer Science, vol. 10811, pp. 121-138. Springer (2018)\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, 15th International Symposium on Automated Technology for Verification and Analysis (ATVA2017). Ehlers, R.: Formal verification of piece-wise linear feed-forward neural networks. In: 15th International Symposium on Automated Technology for Verification and Analysis (ATVA2017). pp. 269-286 (2017)\n\nAn abstraction-based framework for neural network verification. Y Y Elboher, J Gottschlich, G Katz, Computer Aided Verification -32nd International Conference, CAV 2020. Lahiri, S.K., Wang, C.Los Angeles, CA, USASpringer12224Elboher, Y.Y., Gottschlich, J., Katz, G.: An abstraction-based framework for neural network verification. In: Lahiri, S.K., Wang, C. (eds.) Computer Aided Verification -32nd Interna- tional Conference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part I. Lecture Notes in Computer Science, vol. 12224, pp. 43-65. Springer (2020)\n\nAnalyzing the next generation airborne collision avoidance system. C Von Essen, D Giannakopoulou, Tools and Algorithms for the Construction and Analysis of Systems -20th International Conference, TACAS 2014, Held as Part of the European Joint Conferences on Theory and Practice of Software. \u00c1brah\u00e1m, E., Havelund, K.Grenoble, FranceSpringer8413ETAPSvon Essen, C., Giannakopoulou, D.: Analyzing the next generation airborne collision avoid- ance system. In:\u00c1brah\u00e1m, E., Havelund, K. (eds.) Tools and Algorithms for the Construction and Analysis of Systems -20th International Conference, TACAS 2014, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2014, Grenoble, France, April 5-13, 2014. Proceedings. Lecture Notes in Computer Science, vol. 8413, pp. 620-635. Springer (2014)\n\nAI 2 : Safety and robustness certification of neural networks with abstract interpretation. T Gehr, M Mirman, D Drachsler-Cohen, P Tsankov, S Chaudhuri, M Vechev, 2018 IEEE Symposium on Security and Privacy. Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., Vechev, M.: AI 2 : Safety and robustness certification of neural networks with abstract interpretation. In: 2018 IEEE Symposium on Security and Privacy (S&P 2018). pp. 948-963 (2018)\n\nThe zonotope abstract domain taylor1+. K Ghorbal, E Goubault, S Putot, International Conference on Computer Aided Verification. SpringerGhorbal, K., Goubault, E., Putot, S.: The zonotope abstract domain taylor1+. In: International Conference on Computer Aided Verification. pp. 627-633. Springer (2009)\n\nA logical product approach to zonotope intersection. K Ghorbal, E Goubault, S Putot, Computer Aided Verification, 22nd International Conference, CAV 2010. Touili, T., Cook, B., Jackson, P.B.Edinburgh, UKSpringer6174Ghorbal, K., Goubault, E., Putot, S.: A logical product approach to zonotope intersection. In: Touili, T., Cook, B., Jackson, P.B. (eds.) Computer Aided Verification, 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings. Lecture Notes in Computer Science, vol. 6174, pp. 212-226. Springer (2010)\n\nSafety verification of deep neural networks. X Huang, M Kwiatkowska, S Wang, M Wu, 29th International Conference on Computer Aided Verification (CAV2017). Huang, X., Kwiatkowska, M., Wang, S., Wu, M.: Safety verification of deep neural networks. In: 29th International Conference on Computer Aided Verification (CAV2017). pp. 3-29 (2017)\n\nFormal verification of ACAS x, an industrial airborne collision avoidance system. J Jeannin, K Ghorbal, Y Kouskoulas, R Gardner, A Schmidt, E Zawadzki, A Platzer, 2015 International Conference on Embedded Software. Girault, A., Guan, N.Amsterdam, NetherlandsIEEEJeannin, J., Ghorbal, K., Kouskoulas, Y., Gardner, R., Schmidt, A., Zawadzki, E., Platzer, A.: Formal verification of ACAS x, an industrial airborne collision avoidance system. In: Girault, A., Guan, N. (eds.) 2015 International Conference on Embedded Software, EMSOFT 2015, Amsterdam, Netherlands, October 4-9, 2015. pp. 127-136. IEEE (2015)\n\n. S Vigerske, H G Santos, T Ralphs, L Hafer, B Kristjansson, M Lubin, M Saltzman, coin-or/cbc: Version 2.10.5johnjforrest, Vigerske, S., Santos, H.G., Ralphs, T., Hafer, L., Kristjansson, B., jpfasano, Ed- winStraver, Lubin, M., rlougee, jpgoncal1, h-i gassmann, Saltzman, M.: coin-or/cbc: Version 2.10.5 (Mar 2020)\n\nDeep neural network compression for aircraft collision avoidance systems. K D Julian, M J Kochenderfer, M P Owen, CoRR abs/1810.04240Julian, K.D., Kochenderfer, M.J., Owen, M.P.: Deep neural network compression for aircraft collision avoidance systems. CoRR abs/1810.04240 (2018), http://arxiv.org/abs/1810.04240\n\nReluplex: An efficient SMT solver for verifying deep neural networks. G Katz, C W Barrett, D L Dill, K Julian, M J Kochenderfer, 29th International Conference on Computer Aided Verification (CAV2017). Katz, G., Barrett, C.W., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex: An efficient SMT solver for verifying deep neural networks. In: 29th International Conference on Com- puter Aided Verification (CAV2017). pp. 97-117 (2017)\n\nThe marabou framework for verification and analysis of deep neural networks. G Katz, D A Huang, D Ibeling, K Julian, C Lazarus, R Lim, P Shah, S Thakoor, H Wu, A Zeljic, D L Dill, M J Kochenderfer, C W Barrett, Computer Aided Verification -31st International Conference, CAV 2019. Dillig, I., Tasiran, S.New York City, NY, USASpringer11561Katz, G., Huang, D.A., Ibeling, D., Julian, K., Lazarus, C., Lim, R., Shah, P., Thakoor, S., Wu, H., Zeljic, A., Dill, D.L., Kochenderfer, M.J., Barrett, C.W.: The marabou framework for verification and analysis of deep neural networks. In: Dillig, I., Tasiran, S. (eds.) Computer Aided Verification -31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I. Lecture Notes in Computer Science, vol. 11561, pp. 443-452. Springer (2019)\n\nGradient-based learning applied to document recognition. Y L\u00e9cun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611L\u00e9cun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278-2324 (1998)\n\nAnalyzing deep neural networks with symbolic propagation: Towards higher precision and faster verification. J Li, J Liu, P Yang, L Chen, X Huang, L Zhang, Static Analysis -26th International Symposium. Chang, B.E.Porto, PortugalSpringer11822SASLi, J., Liu, J., Yang, P., Chen, L., Huang, X., Zhang, L.: Analyzing deep neural networks with symbolic propagation: Towards higher precision and faster verification. In: Chang, B.E. (ed.) Static Analysis -26th International Symposium, SAS 2019, Porto, Portugal, October 8-11, 2019, Proceedings. Lecture Notes in Computer Science, vol. 11822, pp. 296-319. Springer (2019)\n\nProdeep: a platform for robustness verification of deep neural networks. R Li, J Li, C Huang, P Yang, X Huang, L Zhang, B Xue, H Hermanns, ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. Devanbu, P., Cohen, M.B., Zimmermann, T.USAACMLi, R., Li, J., Huang, C., Yang, P., Huang, X., Zhang, L., Xue, B., Hermanns, H.: Prodeep: a platform for robustness verification of deep neural networks. In: Devanbu, P., Cohen, M.B., Zimmermann, T. (eds.) ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020. pp. 1630-1634. ACM (2020)\n\nRobustness verification of classification deep neural networks via linear programming. W Lin, Z Yang, X Chen, Q Zhao, X Li, Z Liu, J He, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAIEEELin, W., Yang, Z., Chen, X., Zhao, Q., Li, X., Liu, Z., He, J.: Robustness verification of clas- sification deep neural networks via linear programming. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 11418-11427. Computer Vision Foundation / IEEE (2019)\n\nAn approach to reachability analysis for feed-forward ReLU neural networks. A Lomuscio, L Maganti, 2018Lomuscio, A., Maganti, L.: An approach to reachability analysis for feed-forward ReLU neural networks. In: KR2018 (2018)\n\nRobustness of neural networks: A probabilistic and practical approach. R Mangal, A V Nori, A Orso, CoRR abs/1902.05983Mangal, R., Nori, A.V., Orso, A.: Robustness of neural networks: A probabilistic and practi- cal approach. CoRR abs/1902.05983 (2019), http://arxiv.org/abs/1902.05983\n\nNeural network robustness verification on gpus. C M\u00fcller, G Singh, M P\u00fcschel, M T Vechev, abs/2007.10868M\u00fcller, C., Singh, G., P\u00fcschel, M., Vechev, M.T.: Neural network robustness verification on gpus. CoRR abs/2007.10868 (2020), https://arxiv.org/abs/2007.10868\n\nVerifying properties of binarized deep neural networks. N Narodytska, S P Kasiviswanathan, L Ryzhyk, M Sagiv, T Walsh, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). McIlraith, S.A., Weinberger, K.Q.the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressNarodytska, N., Kasiviswanathan, S.P., Ryzhyk, L., Sagiv, M., Walsh, T.: Verifying prop- erties of binarized deep neural networks. In: McIlraith, S.A., Weinberger, K.Q. (eds.) Pro- ceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Sympo- sium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. pp. 6615-6624. AAAI Press (2018)\n\nAn abstraction-refinement approach to verification of artificial neural networks. L Pulina, A Tacchella, Computer Aided Verification, 22nd International Conference, CAV 2010. Edinburgh, UKPulina, L., Tacchella, A.: An abstraction-refinement approach to verification of artificial neu- ral networks. In: Computer Aided Verification, 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings. pp. 243-257 (2010)\n\nReachability analysis of deep neural networks with provable guarantees. W Ruan, X Huang, M Kwiatkowska, IJCAI2018Ruan, W., Huang, X., Kwiatkowska, M.: Reachability analysis of deep neural networks with provable guarantees. In: IJCAI2018. pp. 2651-2659 (2018)\n\nGlobal robustness evaluation of deep neural networks with provable guarantees for the hamming distance. W Ruan, M Wu, Y Sun, X Huang, D Kroening, M Kwiatkowska, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. Kraus, S.the Twenty-Eighth International Joint Conference on Artificial IntelligenceMacao, China2019ijcai.orgRuan, W., Wu, M., Sun, Y., Huang, X., Kroening, D., Kwiatkowska, M.: Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance. In: Kraus, S. (ed.) Proceedings of the Twenty-Eighth International Joint Conference on Artifi- cial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019. pp. 5944-5952. ijcai.org (2019)\n\nDeveloping and using expert systems and neural networks in medicine: A review on benefits and challenges. A Sheikhtaheri, F Sadoughi, Z H Dehaghi, J. Medical Syst. 389110Sheikhtaheri, A., Sadoughi, F., Dehaghi, Z.H.: Developing and using expert systems and neural networks in medicine: A review on benefits and challenges. J. Medical Syst. 38(9), 110 (2014)\n\nMastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T P Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 5297587Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., Schrit- twieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.P., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D.: Mastering the game of go with deep neural networks and tree search. Nature 529(7587), 484-489 (2016)\n\nBeyond the single neuron convex barrier for neural network certification. G Singh, R Ganvir, M P\u00fcschel, M T Vechev, H M Wallach, H Larochelle, A Beygelzimer, F Buc, E B Fox, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Garnett, R.NeurIPS; Vancouver, BC, CanadaSingh, G., Ganvir, R., P\u00fcschel, M., Vechev, M.T.: Beyond the single neuron convex bar- rier for neural network certification. In: Wallach, H.M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E.B., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada. pp. 15072-15083 (2019)\n\nFast and effective robustness certification. G Singh, T Gehr, M Mirman, M P\u00fcschel, M T Vechev, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. NeurIPS; Montr\u00e9al, CanadaSingh, G., Gehr, T., Mirman, M., P\u00fcschel, M., Vechev, M.T.: Fast and effective robustness certification. In: Advances in Neural Information Processing Systems 31: Annual Confer- ence on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montr\u00e9al, Canada. pp. 10825-10836 (2018)\n\nAn abstract domain for certifying neural networks. G Singh, T Gehr, M P\u00fcschel, M T Vechev, 41:1-41:30PACMPL. 3Singh, G., Gehr, T., P\u00fcschel, M., Vechev, M.T.: An abstract domain for certifying neural networks. PACMPL 3(POPL), 41:1-41:30 (2019)\n\nFast polyhedra abstract domain. G Singh, M P\u00fcschel, M T Vechev, Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages. Castagna, G., Gordon, A.D.the 44th ACM SIGPLAN Symposium on Principles of Programming LanguagesParis, FranceACMSingh, G., P\u00fcschel, M., Vechev, M.T.: Fast polyhedra abstract domain. In: Castagna, G., Gordon, A.D. (eds.) Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages, POPL 2017, Paris, France, January 18-20, 2017. pp. 46-59. ACM (2017)\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, International Conference on Learning Representations (ICLR2014). Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R.: Intriguing properties of neural networks. In: International Conference on Learning Repre- sentations (ICLR2014) (2014)\n\nVerification of deep convolutional neural networks using imagestars. H Tran, S Bak, W Xiang, T T Johnson, Computer Aided Verification -32nd International Conference, CAV 2020. Lahiri, S.K., Wang, C.Los Angeles, CA, USASpringer12224Proceedings, Part ITran, H., Bak, S., Xiang, W., Johnson, T.T.: Verification of deep convolutional neural net- works using imagestars. In: Lahiri, S.K., Wang, C. (eds.) Computer Aided Verification - 32nd International Conference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Pro- ceedings, Part I. Lecture Notes in Computer Science, vol. 12224, pp. 18-42. Springer (2020)\n\nStarbased reachability analysis of deep neural networks. H Tran, D M Lopez, P Musau, X Yang, L V Nguyen, W Xiang, T T Johnson, Formal Methods -The Next 30 Years -Third World Congress, FM 2019. ter Beek, M.H., McIver, A., Oliveira, J.N.Porto, PortugalSpringer11800Tran, H., Lopez, D.M., Musau, P., Yang, X., Nguyen, L.V., Xiang, W., Johnson, T.T.: Star- based reachability analysis of deep neural networks. In: ter Beek, M.H., McIver, A., Oliveira, J.N. (eds.) Formal Methods -The Next 30 Years -Third World Congress, FM 2019, Porto, Portugal, October 7-11, 2019, Proceedings. Lecture Notes in Computer Science, vol. 11800, pp. 670-686. Springer (2019)\n\nSelf-driving cars and the urban challenge. C Urmson, W Whittaker, IEEE Intell. Syst. 232Urmson, C., Whittaker, W.: Self-driving cars and the urban challenge. IEEE Intell. Syst. 23(2), 66-68 (2008)\n\nAdversarial sample detection for deep neural network through model mutation testing. J Wang, G Dong, J Sun, X Wang, P Zhang, 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEEWang, J., Dong, G., Sun, J., Wang, X., Zhang, P.: Adversarial sample detection for deep neu- ral network through model mutation testing. In: 2019 IEEE/ACM 41st International Confer- ence on Software Engineering (ICSE). pp. 1245-1256. IEEE (2019)\n\nDetecting adversarial samples for deep neural networks through mutation testing. J Wang, J Sun, P Zhang, X Wang, CoRR abs/1805.05010Wang, J., Sun, J., Zhang, P., Wang, X.: Detecting adversarial samples for deep neural net- works through mutation testing. CoRR abs/1805.05010 (2018), http://arxiv.org/abs/1805. 05010\n\nA statistical approach to assessing neural network robustness. S Webb, T Rainforth, Y W Teh, M P Kumar, 7th International Conference on Learning Representations. New Orleans, LA, USAWebb, S., Rainforth, T., Teh, Y.W., Kumar, M.P.: A statistical approach to assessing neural network robustness. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net (2019)\n\nPROVEN: verifying robustness of neural networks with a probabilistic approach. L Weng, P Chen, L M Nguyen, M S Squillante, A Boopathy, I V Oseledets, L Daniel, Proceedings of the 36th International Conference on Machine Learning, ICML 2019. Chaudhuri, K., Salakhutdinov, R.the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USAPMLR97Proceedings of Machine Learning ResearchWeng, L., Chen, P., Nguyen, L.M., Squillante, M.S., Boopathy, A., Oseledets, I.V., Daniel, L.: PROVEN: verifying robustness of neural networks with a probabilistic approach. In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA. Proceedings of Machine Learning Research, vol. 97, pp. 6727-6736. PMLR (2019)\n\nTowards Fast Computation of Certified Robustness for ReLU Networks. T W Weng, H Zhang, H Chen, Z Song, C J Hsieh, D Boning, I S Dhillon, L Daniel, Weng, T.W., Zhang, H., Chen, H., Song, Z., Hsieh, C.J., Boning, D., Dhillon, I.S., Daniel, L.: Towards Fast Computation of Certified Robustness for ReLU Networks. In: ICML 2018 (Apr 2018)\n\nFeature-guided black-box safety testing of deep neural networks. M Wicker, X Huang, M Kwiatkowska, Tools and Algorithms for the Construction and Analysis of Systems -24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software. Beyer, D., Huisman, M.GreeceSpringer2018ThessalonikiWicker, M., Huang, X., Kwiatkowska, M.: Feature-guided black-box safety testing of deep neural networks. In: Beyer, D., Huisman, M. (eds.) Tools and Algorithms for the Construc- tion and Analysis of Systems -24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2018, Thessa- loniki, Greece, April 14-20, 2018, Proceedings, Part I. Lecture Notes in Computer Science, vol. 10805, pp. 408-426. Springer (2018)\n\nA game-based approximate verification of deep neural networks with provable guarantees. M Wu, M Wicker, W Ruan, X Huang, M Kwiatkowska, Theor. Comput. Sci. 807Wu, M., Wicker, M., Ruan, W., Huang, X., Kwiatkowska, M.: A game-based approximate verification of deep neural networks with provable guarantees. Theor. Comput. Sci. 807, 298-329 (2020)\n\nImproving neural network verification through spurious region guided refinement. P Yang, R Li, J Li, C Huang, J Wang, J Sun, B Xue, L Zhang, CoRR abs/2010.07722Yang, P., Li, R., Li, J., Huang, C., Wang, J., Sun, J., Xue, B., Zhang, L.: Improving neu- ral network verification through spurious region guided refinement. CoRR abs/2010.07722 (2020), https://arxiv.org/abs/2010.07722\n\n), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use. Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License. you will need to obtain permission directly from the copyright holderOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (https://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropri- ate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\n", "annotations": {"author": "[{\"end\":222,\"start\":83},{\"end\":359,\"start\":223},{\"end\":497,\"start\":360},{\"end\":623,\"start\":498},{\"end\":686,\"start\":624},{\"end\":747,\"start\":687},{\"end\":882,\"start\":748},{\"end\":1089,\"start\":883}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":91},{\"end\":232,\"start\":230},{\"end\":370,\"start\":368},{\"end\":514,\"start\":509},{\"end\":635,\"start\":631},{\"end\":694,\"start\":691},{\"end\":755,\"start\":752},{\"end\":894,\"start\":889}]", "author_first_name": "[{\"end\":90,\"start\":83},{\"end\":229,\"start\":223},{\"end\":367,\"start\":360},{\"end\":508,\"start\":498},{\"end\":630,\"start\":624},{\"end\":690,\"start\":687},{\"end\":751,\"start\":748},{\"end\":888,\"start\":883}]", "author_affiliation": "[{\"end\":165,\"start\":97},{\"end\":221,\"start\":167},{\"end\":302,\"start\":234},{\"end\":358,\"start\":304},{\"end\":440,\"start\":372},{\"end\":496,\"start\":442},{\"end\":564,\"start\":516},{\"end\":622,\"start\":566},{\"end\":685,\"start\":637},{\"end\":746,\"start\":696},{\"end\":825,\"start\":757},{\"end\":881,\"start\":827},{\"end\":982,\"start\":914},{\"end\":1038,\"start\":984},{\"end\":1088,\"start\":1040}]", "title": "[{\"end\":80,\"start\":1},{\"end\":1169,\"start\":1090}]", "venue": null, "abstract": "[{\"end\":2302,\"start\":1196}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2342,\"start\":2338},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2758,\"start\":2754},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2980,\"start\":2976},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3006,\"start\":3002},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3403,\"start\":3399},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3524,\"start\":3520},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3540,\"start\":3536},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3823,\"start\":3819},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3855,\"start\":3851},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3858,\"start\":3855},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3861,\"start\":3858},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3863,\"start\":3861},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3866,\"start\":3863},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3869,\"start\":3866},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3872,\"start\":3869},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4133,\"start\":4129},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6279,\"start\":6276},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7404,\"start\":7400},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7407,\"start\":7404},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7409,\"start\":7407},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11395,\"start\":11392},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13011,\"start\":13007},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13014,\"start\":13011},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13034,\"start\":13030},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13951,\"start\":13947},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14041,\"start\":14037},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14044,\"start\":14041},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14047,\"start\":14044},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14593,\"start\":14589},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14831,\"start\":14827},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17519,\"start\":17515},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19022,\"start\":19021},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27045,\"start\":27042},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29654,\"start\":29650},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31015,\"start\":31011},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":31018,\"start\":31015},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31414,\"start\":31410},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32596,\"start\":32592},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33443,\"start\":33440},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33514,\"start\":33510},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34159,\"start\":34155},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34176,\"start\":34172},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34179,\"start\":34176},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34516,\"start\":34512},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34959,\"start\":34955},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38240,\"start\":38236},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38388,\"start\":38384},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38390,\"start\":38388},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38460,\"start\":38456},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38463,\"start\":38460},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38465,\"start\":38463},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38468,\"start\":38465},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38507,\"start\":38503},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":38533,\"start\":38529},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38535,\"start\":38533},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38538,\"start\":38535},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":38569,\"start\":38565},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38605,\"start\":38601},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38608,\"start\":38605},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38639,\"start\":38635},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38642,\"start\":38639},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":39463,\"start\":39459},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":40696,\"start\":40692}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39747,\"start\":39465},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39940,\"start\":39748},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40024,\"start\":39941},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40716,\"start\":40025},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40801,\"start\":40717},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40877,\"start\":40802},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41193,\"start\":40878},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44309,\"start\":41194}]", "paragraph": "[{\"end\":3007,\"start\":2318},{\"end\":4093,\"start\":3009},{\"end\":5256,\"start\":4095},{\"end\":6574,\"start\":5258},{\"end\":7410,\"start\":6576},{\"end\":7462,\"start\":7412},{\"end\":8347,\"start\":7464},{\"end\":8704,\"start\":8349},{\"end\":8929,\"start\":8722},{\"end\":9299,\"start\":8981},{\"end\":10284,\"start\":9382},{\"end\":10442,\"start\":10299},{\"end\":10762,\"start\":10444},{\"end\":10927,\"start\":10764},{\"end\":11319,\"start\":11053},{\"end\":12111,\"start\":11368},{\"end\":12677,\"start\":12113},{\"end\":13035,\"start\":12679},{\"end\":13215,\"start\":13037},{\"end\":13586,\"start\":13504},{\"end\":14709,\"start\":13753},{\"end\":15750,\"start\":14775},{\"end\":16016,\"start\":15813},{\"end\":16510,\"start\":16094},{\"end\":16686,\"start\":16512},{\"end\":16764,\"start\":16708},{\"end\":16946,\"start\":16796},{\"end\":17311,\"start\":16948},{\"end\":17532,\"start\":17313},{\"end\":17923,\"start\":17570},{\"end\":19914,\"start\":18059},{\"end\":20006,\"start\":19916},{\"end\":20063,\"start\":20021},{\"end\":20346,\"start\":20065},{\"end\":20587,\"start\":20348},{\"end\":21290,\"start\":20602},{\"end\":22869,\"start\":21858},{\"end\":23537,\"start\":22922},{\"end\":23658,\"start\":23539},{\"end\":23824,\"start\":23671},{\"end\":23893,\"start\":23887},{\"end\":23921,\"start\":23895},{\"end\":24014,\"start\":23933},{\"end\":24076,\"start\":24016},{\"end\":24127,\"start\":24078},{\"end\":24465,\"start\":24446},{\"end\":24515,\"start\":24467},{\"end\":24550,\"start\":24517},{\"end\":24581,\"start\":24552},{\"end\":24586,\"start\":24583},{\"end\":24677,\"start\":24661},{\"end\":24697,\"start\":24679},{\"end\":24871,\"start\":24820},{\"end\":25014,\"start\":24985},{\"end\":25059,\"start\":25016},{\"end\":25071,\"start\":25061},{\"end\":25159,\"start\":25073},{\"end\":25925,\"start\":25161},{\"end\":26614,\"start\":25973},{\"end\":26915,\"start\":26856},{\"end\":27705,\"start\":26917},{\"end\":29227,\"start\":27745},{\"end\":29368,\"start\":29245},{\"end\":29877,\"start\":29370},{\"end\":30300,\"start\":29879},{\"end\":30635,\"start\":30333},{\"end\":30846,\"start\":30676},{\"end\":31494,\"start\":30848},{\"end\":31896,\"start\":31683},{\"end\":32526,\"start\":31898},{\"end\":33166,\"start\":32528},{\"end\":34130,\"start\":33194},{\"end\":34591,\"start\":34132},{\"end\":34984,\"start\":34593},{\"end\":36476,\"start\":35013},{\"end\":37143,\"start\":36516},{\"end\":38098,\"start\":37204},{\"end\":38670,\"start\":38131},{\"end\":39173,\"start\":38672},{\"end\":39464,\"start\":39175}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9381,\"start\":9300},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10298,\"start\":10285},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11052,\"start\":10928},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13503,\"start\":13216},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13752,\"start\":13587},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14739,\"start\":14710},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15812,\"start\":15751},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16093,\"start\":16017},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16707,\"start\":16687},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16795,\"start\":16765},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18058,\"start\":17924},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21840,\"start\":21291},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22921,\"start\":22870},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23670,\"start\":23659},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24445,\"start\":24128},{\"attributes\":{\"id\":\"formula_15\"},\"end\":24660,\"start\":24587},{\"attributes\":{\"id\":\"formula_16\"},\"end\":24819,\"start\":24698},{\"attributes\":{\"id\":\"formula_17\"},\"end\":24984,\"start\":24872},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26855,\"start\":26615},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27744,\"start\":27706},{\"attributes\":{\"id\":\"formula_20\"},\"end\":30332,\"start\":30301},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31682,\"start\":31495}]", "table_ref": "[{\"end\":35736,\"start\":35729},{\"end\":35779,\"start\":35772}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2316,\"start\":2304},{\"attributes\":{\"n\":\"2\"},\"end\":8720,\"start\":8707},{\"attributes\":{\"n\":\"2.1\"},\"end\":8979,\"start\":8932},{\"attributes\":{\"n\":\"2.2\"},\"end\":11366,\"start\":11322},{\"attributes\":{\"n\":\"3\"},\"end\":14773,\"start\":14741},{\"attributes\":{\"n\":\"4\"},\"end\":17568,\"start\":17535},{\"end\":20019,\"start\":20009},{\"end\":20600,\"start\":20590},{\"attributes\":{\"n\":\"4.1\"},\"end\":21856,\"start\":21842},{\"end\":23885,\"start\":23827},{\"end\":23931,\"start\":23924},{\"attributes\":{\"n\":\"4.2\"},\"end\":25971,\"start\":25928},{\"attributes\":{\"n\":\"4.3\"},\"end\":29243,\"start\":29230},{\"attributes\":{\"n\":\"5\"},\"end\":30674,\"start\":30638},{\"attributes\":{\"n\":\"6\"},\"end\":33192,\"start\":33169},{\"attributes\":{\"n\":\"6.1\"},\"end\":35011,\"start\":34987},{\"attributes\":{\"n\":\"6.2\"},\"end\":36514,\"start\":36479},{\"attributes\":{\"n\":\"6.3\"},\"end\":37202,\"start\":37146},{\"attributes\":{\"n\":\"7\"},\"end\":38129,\"start\":38101},{\"end\":39474,\"start\":39466},{\"end\":39757,\"start\":39749},{\"end\":39953,\"start\":39942},{\"end\":40726,\"start\":40718},{\"end\":40811,\"start\":40803},{\"end\":40890,\"start\":40879},{\"end\":41212,\"start\":41195}]", "table": "[{\"end\":41193,\"start\":41075},{\"end\":44309,\"start\":43133}]", "figure_caption": "[{\"end\":39747,\"start\":39476},{\"end\":39940,\"start\":39759},{\"end\":40024,\"start\":39955},{\"end\":40716,\"start\":40027},{\"end\":40801,\"start\":40728},{\"end\":40877,\"start\":40813},{\"end\":41075,\"start\":40892},{\"end\":43133,\"start\":41215}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17635,\"start\":17629},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20313,\"start\":20307},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20637,\"start\":20628},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21043,\"start\":21034},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21055,\"start\":21046},{\"end\":24034,\"start\":24025},{\"end\":27301,\"start\":27290},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37751,\"start\":37745}]", "bib_author_first_name": "[{\"end\":44406,\"start\":44405},{\"end\":44418,\"start\":44417},{\"end\":44429,\"start\":44428},{\"end\":44439,\"start\":44438},{\"end\":45106,\"start\":45105},{\"end\":45115,\"start\":45114},{\"end\":45126,\"start\":45125},{\"end\":45139,\"start\":45138},{\"end\":45717,\"start\":45716},{\"end\":45727,\"start\":45726},{\"end\":45729,\"start\":45728},{\"end\":45737,\"start\":45736},{\"end\":45739,\"start\":45738},{\"end\":45747,\"start\":45746},{\"end\":46027,\"start\":46026},{\"end\":46252,\"start\":46251},{\"end\":46261,\"start\":46260},{\"end\":46267,\"start\":46266},{\"end\":46280,\"start\":46279},{\"end\":46284,\"start\":46281},{\"end\":46292,\"start\":46291},{\"end\":46301,\"start\":46300},{\"end\":46303,\"start\":46302},{\"end\":46566,\"start\":46565},{\"end\":46568,\"start\":46567},{\"end\":46578,\"start\":46577},{\"end\":46590,\"start\":46589},{\"end\":46597,\"start\":46596},{\"end\":46603,\"start\":46602},{\"end\":47193,\"start\":47192},{\"end\":47203,\"start\":47202},{\"end\":47593,\"start\":47592},{\"end\":47604,\"start\":47603},{\"end\":47864,\"start\":47863},{\"end\":47873,\"start\":47872},{\"end\":47880,\"start\":47879},{\"end\":47900,\"start\":47899},{\"end\":48473,\"start\":48472},{\"end\":48844,\"start\":48843},{\"end\":48846,\"start\":48845},{\"end\":48857,\"start\":48856},{\"end\":48872,\"start\":48871},{\"end\":49420,\"start\":49419},{\"end\":49433,\"start\":49432},{\"end\":50264,\"start\":50263},{\"end\":50272,\"start\":50271},{\"end\":50282,\"start\":50281},{\"end\":50301,\"start\":50300},{\"end\":50312,\"start\":50311},{\"end\":50325,\"start\":50324},{\"end\":50674,\"start\":50673},{\"end\":50685,\"start\":50684},{\"end\":50697,\"start\":50696},{\"end\":50992,\"start\":50991},{\"end\":51003,\"start\":51002},{\"end\":51015,\"start\":51014},{\"end\":51529,\"start\":51528},{\"end\":51538,\"start\":51537},{\"end\":51553,\"start\":51552},{\"end\":51561,\"start\":51560},{\"end\":51905,\"start\":51904},{\"end\":51916,\"start\":51915},{\"end\":51927,\"start\":51926},{\"end\":51941,\"start\":51940},{\"end\":51952,\"start\":51951},{\"end\":51963,\"start\":51962},{\"end\":51975,\"start\":51974},{\"end\":52431,\"start\":52430},{\"end\":52443,\"start\":52442},{\"end\":52445,\"start\":52444},{\"end\":52455,\"start\":52454},{\"end\":52465,\"start\":52464},{\"end\":52474,\"start\":52473},{\"end\":52490,\"start\":52489},{\"end\":52499,\"start\":52498},{\"end\":52820,\"start\":52819},{\"end\":52822,\"start\":52821},{\"end\":52832,\"start\":52831},{\"end\":52834,\"start\":52833},{\"end\":52850,\"start\":52849},{\"end\":52852,\"start\":52851},{\"end\":53130,\"start\":53129},{\"end\":53138,\"start\":53137},{\"end\":53140,\"start\":53139},{\"end\":53151,\"start\":53150},{\"end\":53153,\"start\":53152},{\"end\":53161,\"start\":53160},{\"end\":53171,\"start\":53170},{\"end\":53173,\"start\":53172},{\"end\":53574,\"start\":53573},{\"end\":53582,\"start\":53581},{\"end\":53584,\"start\":53583},{\"end\":53593,\"start\":53592},{\"end\":53604,\"start\":53603},{\"end\":53614,\"start\":53613},{\"end\":53625,\"start\":53624},{\"end\":53632,\"start\":53631},{\"end\":53640,\"start\":53639},{\"end\":53651,\"start\":53650},{\"end\":53657,\"start\":53656},{\"end\":53667,\"start\":53666},{\"end\":53669,\"start\":53668},{\"end\":53677,\"start\":53676},{\"end\":53679,\"start\":53678},{\"end\":53695,\"start\":53694},{\"end\":53697,\"start\":53696},{\"end\":54375,\"start\":54374},{\"end\":54384,\"start\":54383},{\"end\":54394,\"start\":54393},{\"end\":54404,\"start\":54403},{\"end\":54707,\"start\":54706},{\"end\":54713,\"start\":54712},{\"end\":54720,\"start\":54719},{\"end\":54728,\"start\":54727},{\"end\":54736,\"start\":54735},{\"end\":54745,\"start\":54744},{\"end\":55289,\"start\":55288},{\"end\":55295,\"start\":55294},{\"end\":55301,\"start\":55300},{\"end\":55310,\"start\":55309},{\"end\":55318,\"start\":55317},{\"end\":55327,\"start\":55326},{\"end\":55336,\"start\":55335},{\"end\":55343,\"start\":55342},{\"end\":56019,\"start\":56018},{\"end\":56026,\"start\":56025},{\"end\":56034,\"start\":56033},{\"end\":56042,\"start\":56041},{\"end\":56050,\"start\":56049},{\"end\":56056,\"start\":56055},{\"end\":56063,\"start\":56062},{\"end\":56565,\"start\":56564},{\"end\":56577,\"start\":56576},{\"end\":56785,\"start\":56784},{\"end\":56795,\"start\":56794},{\"end\":56797,\"start\":56796},{\"end\":56805,\"start\":56804},{\"end\":57048,\"start\":57047},{\"end\":57058,\"start\":57057},{\"end\":57067,\"start\":57066},{\"end\":57078,\"start\":57077},{\"end\":57080,\"start\":57079},{\"end\":57320,\"start\":57319},{\"end\":57334,\"start\":57333},{\"end\":57336,\"start\":57335},{\"end\":57355,\"start\":57354},{\"end\":57365,\"start\":57364},{\"end\":57374,\"start\":57373},{\"end\":58525,\"start\":58524},{\"end\":58535,\"start\":58534},{\"end\":58954,\"start\":58953},{\"end\":58962,\"start\":58961},{\"end\":58971,\"start\":58970},{\"end\":59246,\"start\":59245},{\"end\":59254,\"start\":59253},{\"end\":59260,\"start\":59259},{\"end\":59267,\"start\":59266},{\"end\":59276,\"start\":59275},{\"end\":59288,\"start\":59287},{\"end\":59976,\"start\":59975},{\"end\":59992,\"start\":59991},{\"end\":60004,\"start\":60003},{\"end\":60006,\"start\":60005},{\"end\":60297,\"start\":60296},{\"end\":60307,\"start\":60306},{\"end\":60316,\"start\":60315},{\"end\":60318,\"start\":60317},{\"end\":60330,\"start\":60329},{\"end\":60338,\"start\":60337},{\"end\":60347,\"start\":60346},{\"end\":60368,\"start\":60367},{\"end\":60385,\"start\":60384},{\"end\":60399,\"start\":60398},{\"end\":60417,\"start\":60416},{\"end\":60428,\"start\":60427},{\"end\":60440,\"start\":60439},{\"end\":60449,\"start\":60448},{\"end\":60457,\"start\":60456},{\"end\":60473,\"start\":60472},{\"end\":60486,\"start\":60485},{\"end\":60488,\"start\":60487},{\"end\":60501,\"start\":60500},{\"end\":60510,\"start\":60509},{\"end\":60525,\"start\":60524},{\"end\":60536,\"start\":60535},{\"end\":61032,\"start\":61031},{\"end\":61041,\"start\":61040},{\"end\":61051,\"start\":61050},{\"end\":61062,\"start\":61061},{\"end\":61064,\"start\":61063},{\"end\":61074,\"start\":61073},{\"end\":61076,\"start\":61075},{\"end\":61087,\"start\":61086},{\"end\":61101,\"start\":61100},{\"end\":61116,\"start\":61115},{\"end\":61123,\"start\":61122},{\"end\":61125,\"start\":61124},{\"end\":61757,\"start\":61756},{\"end\":61766,\"start\":61765},{\"end\":61774,\"start\":61773},{\"end\":61784,\"start\":61783},{\"end\":61795,\"start\":61794},{\"end\":61797,\"start\":61796},{\"end\":62302,\"start\":62301},{\"end\":62311,\"start\":62310},{\"end\":62319,\"start\":62318},{\"end\":62330,\"start\":62329},{\"end\":62332,\"start\":62331},{\"end\":62527,\"start\":62526},{\"end\":62536,\"start\":62535},{\"end\":62547,\"start\":62546},{\"end\":62549,\"start\":62548},{\"end\":63062,\"start\":63061},{\"end\":63073,\"start\":63072},{\"end\":63084,\"start\":63083},{\"end\":63097,\"start\":63096},{\"end\":63106,\"start\":63105},{\"end\":63115,\"start\":63114},{\"end\":63129,\"start\":63128},{\"end\":63484,\"start\":63483},{\"end\":63492,\"start\":63491},{\"end\":63499,\"start\":63498},{\"end\":63508,\"start\":63507},{\"end\":63510,\"start\":63509},{\"end\":64081,\"start\":64080},{\"end\":64089,\"start\":64088},{\"end\":64091,\"start\":64090},{\"end\":64100,\"start\":64099},{\"end\":64109,\"start\":64108},{\"end\":64117,\"start\":64116},{\"end\":64119,\"start\":64118},{\"end\":64129,\"start\":64128},{\"end\":64138,\"start\":64137},{\"end\":64140,\"start\":64139},{\"end\":64720,\"start\":64719},{\"end\":64730,\"start\":64729},{\"end\":64960,\"start\":64959},{\"end\":64968,\"start\":64967},{\"end\":64976,\"start\":64975},{\"end\":64983,\"start\":64982},{\"end\":64991,\"start\":64990},{\"end\":65408,\"start\":65407},{\"end\":65416,\"start\":65415},{\"end\":65423,\"start\":65422},{\"end\":65432,\"start\":65431},{\"end\":65707,\"start\":65706},{\"end\":65715,\"start\":65714},{\"end\":65728,\"start\":65727},{\"end\":65730,\"start\":65729},{\"end\":65737,\"start\":65736},{\"end\":65739,\"start\":65738},{\"end\":66150,\"start\":66149},{\"end\":66158,\"start\":66157},{\"end\":66166,\"start\":66165},{\"end\":66168,\"start\":66167},{\"end\":66178,\"start\":66177},{\"end\":66180,\"start\":66179},{\"end\":66194,\"start\":66193},{\"end\":66206,\"start\":66205},{\"end\":66208,\"start\":66207},{\"end\":66221,\"start\":66220},{\"end\":66972,\"start\":66971},{\"end\":66974,\"start\":66973},{\"end\":66982,\"start\":66981},{\"end\":66991,\"start\":66990},{\"end\":66999,\"start\":66998},{\"end\":67007,\"start\":67006},{\"end\":67009,\"start\":67008},{\"end\":67018,\"start\":67017},{\"end\":67028,\"start\":67027},{\"end\":67030,\"start\":67029},{\"end\":67041,\"start\":67040},{\"end\":67305,\"start\":67304},{\"end\":67315,\"start\":67314},{\"end\":67324,\"start\":67323},{\"end\":68159,\"start\":68158},{\"end\":68165,\"start\":68164},{\"end\":68175,\"start\":68174},{\"end\":68183,\"start\":68182},{\"end\":68192,\"start\":68191},{\"end\":68498,\"start\":68497},{\"end\":68506,\"start\":68505},{\"end\":68512,\"start\":68511},{\"end\":68518,\"start\":68517},{\"end\":68527,\"start\":68526},{\"end\":68535,\"start\":68534},{\"end\":68542,\"start\":68541},{\"end\":68549,\"start\":68548}]", "bib_author_last_name": "[{\"end\":44415,\"start\":44407},{\"end\":44426,\"start\":44419},{\"end\":44436,\"start\":44430},{\"end\":44449,\"start\":44440},{\"end\":45112,\"start\":45107},{\"end\":45123,\"start\":45116},{\"end\":45136,\"start\":45127},{\"end\":45144,\"start\":45140},{\"end\":45724,\"start\":45718},{\"end\":45734,\"start\":45730},{\"end\":45744,\"start\":45740},{\"end\":45754,\"start\":45748},{\"end\":46035,\"start\":46028},{\"end\":46258,\"start\":46253},{\"end\":46264,\"start\":46262},{\"end\":46277,\"start\":46268},{\"end\":46289,\"start\":46285},{\"end\":46298,\"start\":46293},{\"end\":46309,\"start\":46304},{\"end\":46575,\"start\":46569},{\"end\":46587,\"start\":46579},{\"end\":46594,\"start\":46591},{\"end\":46600,\"start\":46598},{\"end\":46609,\"start\":46604},{\"end\":47200,\"start\":47194},{\"end\":47210,\"start\":47204},{\"end\":47601,\"start\":47594},{\"end\":47609,\"start\":47605},{\"end\":47870,\"start\":47865},{\"end\":47877,\"start\":47874},{\"end\":47897,\"start\":47881},{\"end\":47907,\"start\":47901},{\"end\":48480,\"start\":48474},{\"end\":48854,\"start\":48847},{\"end\":48869,\"start\":48858},{\"end\":48877,\"start\":48873},{\"end\":49430,\"start\":49421},{\"end\":49448,\"start\":49434},{\"end\":50269,\"start\":50265},{\"end\":50279,\"start\":50273},{\"end\":50298,\"start\":50283},{\"end\":50309,\"start\":50302},{\"end\":50322,\"start\":50313},{\"end\":50332,\"start\":50326},{\"end\":50682,\"start\":50675},{\"end\":50694,\"start\":50686},{\"end\":50703,\"start\":50698},{\"end\":51000,\"start\":50993},{\"end\":51012,\"start\":51004},{\"end\":51021,\"start\":51016},{\"end\":51535,\"start\":51530},{\"end\":51550,\"start\":51539},{\"end\":51558,\"start\":51554},{\"end\":51564,\"start\":51562},{\"end\":51913,\"start\":51906},{\"end\":51924,\"start\":51917},{\"end\":51938,\"start\":51928},{\"end\":51949,\"start\":51942},{\"end\":51960,\"start\":51953},{\"end\":51972,\"start\":51964},{\"end\":51983,\"start\":51976},{\"end\":52440,\"start\":52432},{\"end\":52452,\"start\":52446},{\"end\":52462,\"start\":52456},{\"end\":52471,\"start\":52466},{\"end\":52487,\"start\":52475},{\"end\":52496,\"start\":52491},{\"end\":52508,\"start\":52500},{\"end\":52829,\"start\":52823},{\"end\":52847,\"start\":52835},{\"end\":52857,\"start\":52853},{\"end\":53135,\"start\":53131},{\"end\":53148,\"start\":53141},{\"end\":53158,\"start\":53154},{\"end\":53168,\"start\":53162},{\"end\":53186,\"start\":53174},{\"end\":53579,\"start\":53575},{\"end\":53590,\"start\":53585},{\"end\":53601,\"start\":53594},{\"end\":53611,\"start\":53605},{\"end\":53622,\"start\":53615},{\"end\":53629,\"start\":53626},{\"end\":53637,\"start\":53633},{\"end\":53648,\"start\":53641},{\"end\":53654,\"start\":53652},{\"end\":53664,\"start\":53658},{\"end\":53674,\"start\":53670},{\"end\":53692,\"start\":53680},{\"end\":53705,\"start\":53698},{\"end\":54381,\"start\":54376},{\"end\":54391,\"start\":54385},{\"end\":54401,\"start\":54395},{\"end\":54412,\"start\":54405},{\"end\":54710,\"start\":54708},{\"end\":54717,\"start\":54714},{\"end\":54725,\"start\":54721},{\"end\":54733,\"start\":54729},{\"end\":54742,\"start\":54737},{\"end\":54751,\"start\":54746},{\"end\":55292,\"start\":55290},{\"end\":55298,\"start\":55296},{\"end\":55307,\"start\":55302},{\"end\":55315,\"start\":55311},{\"end\":55324,\"start\":55319},{\"end\":55333,\"start\":55328},{\"end\":55340,\"start\":55337},{\"end\":55352,\"start\":55344},{\"end\":56023,\"start\":56020},{\"end\":56031,\"start\":56027},{\"end\":56039,\"start\":56035},{\"end\":56047,\"start\":56043},{\"end\":56053,\"start\":56051},{\"end\":56060,\"start\":56057},{\"end\":56066,\"start\":56064},{\"end\":56574,\"start\":56566},{\"end\":56585,\"start\":56578},{\"end\":56792,\"start\":56786},{\"end\":56802,\"start\":56798},{\"end\":56810,\"start\":56806},{\"end\":57055,\"start\":57049},{\"end\":57064,\"start\":57059},{\"end\":57075,\"start\":57068},{\"end\":57087,\"start\":57081},{\"end\":57331,\"start\":57321},{\"end\":57352,\"start\":57337},{\"end\":57362,\"start\":57356},{\"end\":57371,\"start\":57366},{\"end\":57380,\"start\":57375},{\"end\":58532,\"start\":58526},{\"end\":58545,\"start\":58536},{\"end\":58959,\"start\":58955},{\"end\":58968,\"start\":58963},{\"end\":58983,\"start\":58972},{\"end\":59251,\"start\":59247},{\"end\":59257,\"start\":59255},{\"end\":59264,\"start\":59261},{\"end\":59273,\"start\":59268},{\"end\":59285,\"start\":59277},{\"end\":59300,\"start\":59289},{\"end\":59989,\"start\":59977},{\"end\":60001,\"start\":59993},{\"end\":60014,\"start\":60007},{\"end\":60304,\"start\":60298},{\"end\":60313,\"start\":60308},{\"end\":60327,\"start\":60319},{\"end\":60335,\"start\":60331},{\"end\":60344,\"start\":60339},{\"end\":60365,\"start\":60348},{\"end\":60382,\"start\":60369},{\"end\":60396,\"start\":60386},{\"end\":60414,\"start\":60400},{\"end\":60425,\"start\":60418},{\"end\":60437,\"start\":60429},{\"end\":60446,\"start\":60441},{\"end\":60454,\"start\":60450},{\"end\":60470,\"start\":60458},{\"end\":60483,\"start\":60474},{\"end\":60498,\"start\":60489},{\"end\":60507,\"start\":60502},{\"end\":60522,\"start\":60511},{\"end\":60533,\"start\":60526},{\"end\":60545,\"start\":60537},{\"end\":61038,\"start\":61033},{\"end\":61048,\"start\":61042},{\"end\":61059,\"start\":61052},{\"end\":61071,\"start\":61065},{\"end\":61084,\"start\":61077},{\"end\":61098,\"start\":61088},{\"end\":61113,\"start\":61102},{\"end\":61120,\"start\":61117},{\"end\":61129,\"start\":61126},{\"end\":61763,\"start\":61758},{\"end\":61771,\"start\":61767},{\"end\":61781,\"start\":61775},{\"end\":61792,\"start\":61785},{\"end\":61804,\"start\":61798},{\"end\":62308,\"start\":62303},{\"end\":62316,\"start\":62312},{\"end\":62327,\"start\":62320},{\"end\":62339,\"start\":62333},{\"end\":62533,\"start\":62528},{\"end\":62544,\"start\":62537},{\"end\":62556,\"start\":62550},{\"end\":63070,\"start\":63063},{\"end\":63081,\"start\":63074},{\"end\":63094,\"start\":63085},{\"end\":63103,\"start\":63098},{\"end\":63112,\"start\":63107},{\"end\":63126,\"start\":63116},{\"end\":63136,\"start\":63130},{\"end\":63489,\"start\":63485},{\"end\":63496,\"start\":63493},{\"end\":63505,\"start\":63500},{\"end\":63518,\"start\":63511},{\"end\":64086,\"start\":64082},{\"end\":64097,\"start\":64092},{\"end\":64106,\"start\":64101},{\"end\":64114,\"start\":64110},{\"end\":64126,\"start\":64120},{\"end\":64135,\"start\":64130},{\"end\":64148,\"start\":64141},{\"end\":64727,\"start\":64721},{\"end\":64740,\"start\":64731},{\"end\":64965,\"start\":64961},{\"end\":64973,\"start\":64969},{\"end\":64980,\"start\":64977},{\"end\":64988,\"start\":64984},{\"end\":64997,\"start\":64992},{\"end\":65413,\"start\":65409},{\"end\":65420,\"start\":65417},{\"end\":65429,\"start\":65424},{\"end\":65437,\"start\":65433},{\"end\":65712,\"start\":65708},{\"end\":65725,\"start\":65716},{\"end\":65734,\"start\":65731},{\"end\":65745,\"start\":65740},{\"end\":66155,\"start\":66151},{\"end\":66163,\"start\":66159},{\"end\":66175,\"start\":66169},{\"end\":66191,\"start\":66181},{\"end\":66203,\"start\":66195},{\"end\":66218,\"start\":66209},{\"end\":66228,\"start\":66222},{\"end\":66979,\"start\":66975},{\"end\":66988,\"start\":66983},{\"end\":66996,\"start\":66992},{\"end\":67004,\"start\":67000},{\"end\":67015,\"start\":67010},{\"end\":67025,\"start\":67019},{\"end\":67038,\"start\":67031},{\"end\":67048,\"start\":67042},{\"end\":67312,\"start\":67306},{\"end\":67321,\"start\":67316},{\"end\":67336,\"start\":67325},{\"end\":68162,\"start\":68160},{\"end\":68172,\"start\":68166},{\"end\":68180,\"start\":68176},{\"end\":68189,\"start\":68184},{\"end\":68204,\"start\":68193},{\"end\":68503,\"start\":68499},{\"end\":68509,\"start\":68507},{\"end\":68515,\"start\":68513},{\"end\":68524,\"start\":68519},{\"end\":68532,\"start\":68528},{\"end\":68539,\"start\":68536},{\"end\":68546,\"start\":68543},{\"end\":68555,\"start\":68550}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":127982021},\"end\":45031,\"start\":44311},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220042000},\"end\":45653,\"start\":45033},{\"attributes\":{\"doi\":\"CoRR abs/2002.06864\",\"id\":\"b2\"},\"end\":45944,\"start\":45655},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":36539154},\"end\":46182,\"start\":45946},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":202577669},\"end\":46517,\"start\":46184},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6913118},\"end\":47062,\"start\":46519},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207614632},\"end\":47522,\"start\":47064},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6298008},\"end\":47801,\"start\":47524},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3332132},\"end\":48399,\"start\":47803},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1931807},\"end\":48777,\"start\":48401},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207758160},\"end\":49350,\"start\":48779},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12586606},\"end\":50169,\"start\":49352},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206579396},\"end\":50632,\"start\":50171},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9119506},\"end\":50936,\"start\":50634},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1416687},\"end\":51481,\"start\":50938},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11626373},\"end\":51820,\"start\":51483},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":258848},\"end\":52426,\"start\":51822},{\"attributes\":{\"doi\":\"coin-or/cbc: Version 2.10.5\",\"id\":\"b17\"},\"end\":52743,\"start\":52428},{\"attributes\":{\"doi\":\"CoRR abs/1810.04240\",\"id\":\"b18\"},\"end\":53057,\"start\":52745},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":516928},\"end\":53494,\"start\":53059},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":196610895},\"end\":54315,\"start\":53496},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14542261},\"end\":54596,\"start\":54317},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":67855367},\"end\":55213,\"start\":54598},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":226274169},\"end\":55929,\"start\":55215},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":198161698},\"end\":56486,\"start\":55931},{\"attributes\":{\"id\":\"b25\"},\"end\":56711,\"start\":56488},{\"attributes\":{\"doi\":\"CoRR abs/1902.05983\",\"id\":\"b26\"},\"end\":56997,\"start\":56713},{\"attributes\":{\"doi\":\"abs/2007.10868\",\"id\":\"b27\"},\"end\":57261,\"start\":56999},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3290586},\"end\":58440,\"start\":57263},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1679726},\"end\":58879,\"start\":58442},{\"attributes\":{\"doi\":\"IJCAI2018\",\"id\":\"b30\"},\"end\":59139,\"start\":58881},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":197632508},\"end\":59867,\"start\":59141},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14044934},\"end\":60226,\"start\":59869},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":515925},\"end\":60955,\"start\":60228},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207796611},\"end\":61709,\"start\":60957},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53960414},\"end\":62248,\"start\":61711},{\"attributes\":{\"doi\":\"41:1-41:30\",\"id\":\"b36\",\"matched_paper_id\":57757287},\"end\":62492,\"start\":62250},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15378666},\"end\":63017,\"start\":62494},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":604334},\"end\":63412,\"start\":63019},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":215745203},\"end\":64021,\"start\":63414},{\"attributes\":{\"id\":\"b40\"},\"end\":64674,\"start\":64023},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":206468666},\"end\":64872,\"start\":64676},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":55702297},\"end\":65324,\"start\":64874},{\"attributes\":{\"doi\":\"CoRR abs/1805.05010\",\"id\":\"b43\"},\"end\":65641,\"start\":65326},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":53749372},\"end\":66068,\"start\":65643},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":56517535},\"end\":66901,\"start\":66070},{\"attributes\":{\"id\":\"b46\"},\"end\":67237,\"start\":66903},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3404328},\"end\":68068,\"start\":67239},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":49665251},\"end\":68414,\"start\":68070},{\"attributes\":{\"doi\":\"CoRR abs/2010.07722\",\"id\":\"b49\"},\"end\":68795,\"start\":68416},{\"attributes\":{\"id\":\"b50\"},\"end\":70410,\"start\":68797}]", "bib_title": "[{\"end\":44403,\"start\":44311},{\"end\":45103,\"start\":45033},{\"end\":46024,\"start\":45946},{\"end\":46249,\"start\":46184},{\"end\":46563,\"start\":46519},{\"end\":47190,\"start\":47064},{\"end\":47590,\"start\":47524},{\"end\":47861,\"start\":47803},{\"end\":48470,\"start\":48401},{\"end\":48841,\"start\":48779},{\"end\":49417,\"start\":49352},{\"end\":50261,\"start\":50171},{\"end\":50671,\"start\":50634},{\"end\":50989,\"start\":50938},{\"end\":51526,\"start\":51483},{\"end\":51902,\"start\":51822},{\"end\":53127,\"start\":53059},{\"end\":53571,\"start\":53496},{\"end\":54372,\"start\":54317},{\"end\":54704,\"start\":54598},{\"end\":55286,\"start\":55215},{\"end\":56016,\"start\":55931},{\"end\":57317,\"start\":57263},{\"end\":58522,\"start\":58442},{\"end\":59243,\"start\":59141},{\"end\":59973,\"start\":59869},{\"end\":60294,\"start\":60228},{\"end\":61029,\"start\":60957},{\"end\":61754,\"start\":61711},{\"end\":62299,\"start\":62250},{\"end\":62524,\"start\":62494},{\"end\":63059,\"start\":63019},{\"end\":63481,\"start\":63414},{\"end\":64078,\"start\":64023},{\"end\":64717,\"start\":64676},{\"end\":64957,\"start\":64874},{\"end\":65704,\"start\":65643},{\"end\":66147,\"start\":66070},{\"end\":67302,\"start\":67239},{\"end\":68156,\"start\":68070}]", "bib_author": "[{\"end\":44417,\"start\":44405},{\"end\":44428,\"start\":44417},{\"end\":44438,\"start\":44428},{\"end\":44451,\"start\":44438},{\"end\":45114,\"start\":45105},{\"end\":45125,\"start\":45114},{\"end\":45138,\"start\":45125},{\"end\":45146,\"start\":45138},{\"end\":45726,\"start\":45716},{\"end\":45736,\"start\":45726},{\"end\":45746,\"start\":45736},{\"end\":45756,\"start\":45746},{\"end\":46037,\"start\":46026},{\"end\":46260,\"start\":46251},{\"end\":46266,\"start\":46260},{\"end\":46279,\"start\":46266},{\"end\":46291,\"start\":46279},{\"end\":46300,\"start\":46291},{\"end\":46311,\"start\":46300},{\"end\":46577,\"start\":46565},{\"end\":46589,\"start\":46577},{\"end\":46596,\"start\":46589},{\"end\":46602,\"start\":46596},{\"end\":46611,\"start\":46602},{\"end\":47202,\"start\":47192},{\"end\":47212,\"start\":47202},{\"end\":47603,\"start\":47592},{\"end\":47611,\"start\":47603},{\"end\":47872,\"start\":47863},{\"end\":47879,\"start\":47872},{\"end\":47899,\"start\":47879},{\"end\":47909,\"start\":47899},{\"end\":48482,\"start\":48472},{\"end\":48856,\"start\":48843},{\"end\":48871,\"start\":48856},{\"end\":48879,\"start\":48871},{\"end\":49432,\"start\":49419},{\"end\":49450,\"start\":49432},{\"end\":50271,\"start\":50263},{\"end\":50281,\"start\":50271},{\"end\":50300,\"start\":50281},{\"end\":50311,\"start\":50300},{\"end\":50324,\"start\":50311},{\"end\":50334,\"start\":50324},{\"end\":50684,\"start\":50673},{\"end\":50696,\"start\":50684},{\"end\":50705,\"start\":50696},{\"end\":51002,\"start\":50991},{\"end\":51014,\"start\":51002},{\"end\":51023,\"start\":51014},{\"end\":51537,\"start\":51528},{\"end\":51552,\"start\":51537},{\"end\":51560,\"start\":51552},{\"end\":51566,\"start\":51560},{\"end\":51915,\"start\":51904},{\"end\":51926,\"start\":51915},{\"end\":51940,\"start\":51926},{\"end\":51951,\"start\":51940},{\"end\":51962,\"start\":51951},{\"end\":51974,\"start\":51962},{\"end\":51985,\"start\":51974},{\"end\":52442,\"start\":52430},{\"end\":52454,\"start\":52442},{\"end\":52464,\"start\":52454},{\"end\":52473,\"start\":52464},{\"end\":52489,\"start\":52473},{\"end\":52498,\"start\":52489},{\"end\":52510,\"start\":52498},{\"end\":52831,\"start\":52819},{\"end\":52849,\"start\":52831},{\"end\":52859,\"start\":52849},{\"end\":53137,\"start\":53129},{\"end\":53150,\"start\":53137},{\"end\":53160,\"start\":53150},{\"end\":53170,\"start\":53160},{\"end\":53188,\"start\":53170},{\"end\":53581,\"start\":53573},{\"end\":53592,\"start\":53581},{\"end\":53603,\"start\":53592},{\"end\":53613,\"start\":53603},{\"end\":53624,\"start\":53613},{\"end\":53631,\"start\":53624},{\"end\":53639,\"start\":53631},{\"end\":53650,\"start\":53639},{\"end\":53656,\"start\":53650},{\"end\":53666,\"start\":53656},{\"end\":53676,\"start\":53666},{\"end\":53694,\"start\":53676},{\"end\":53707,\"start\":53694},{\"end\":54383,\"start\":54374},{\"end\":54393,\"start\":54383},{\"end\":54403,\"start\":54393},{\"end\":54414,\"start\":54403},{\"end\":54712,\"start\":54706},{\"end\":54719,\"start\":54712},{\"end\":54727,\"start\":54719},{\"end\":54735,\"start\":54727},{\"end\":54744,\"start\":54735},{\"end\":54753,\"start\":54744},{\"end\":55294,\"start\":55288},{\"end\":55300,\"start\":55294},{\"end\":55309,\"start\":55300},{\"end\":55317,\"start\":55309},{\"end\":55326,\"start\":55317},{\"end\":55335,\"start\":55326},{\"end\":55342,\"start\":55335},{\"end\":55354,\"start\":55342},{\"end\":56025,\"start\":56018},{\"end\":56033,\"start\":56025},{\"end\":56041,\"start\":56033},{\"end\":56049,\"start\":56041},{\"end\":56055,\"start\":56049},{\"end\":56062,\"start\":56055},{\"end\":56068,\"start\":56062},{\"end\":56576,\"start\":56564},{\"end\":56587,\"start\":56576},{\"end\":56794,\"start\":56784},{\"end\":56804,\"start\":56794},{\"end\":56812,\"start\":56804},{\"end\":57057,\"start\":57047},{\"end\":57066,\"start\":57057},{\"end\":57077,\"start\":57066},{\"end\":57089,\"start\":57077},{\"end\":57333,\"start\":57319},{\"end\":57354,\"start\":57333},{\"end\":57364,\"start\":57354},{\"end\":57373,\"start\":57364},{\"end\":57382,\"start\":57373},{\"end\":58534,\"start\":58524},{\"end\":58547,\"start\":58534},{\"end\":58961,\"start\":58953},{\"end\":58970,\"start\":58961},{\"end\":58985,\"start\":58970},{\"end\":59253,\"start\":59245},{\"end\":59259,\"start\":59253},{\"end\":59266,\"start\":59259},{\"end\":59275,\"start\":59266},{\"end\":59287,\"start\":59275},{\"end\":59302,\"start\":59287},{\"end\":59991,\"start\":59975},{\"end\":60003,\"start\":59991},{\"end\":60016,\"start\":60003},{\"end\":60306,\"start\":60296},{\"end\":60315,\"start\":60306},{\"end\":60329,\"start\":60315},{\"end\":60337,\"start\":60329},{\"end\":60346,\"start\":60337},{\"end\":60367,\"start\":60346},{\"end\":60384,\"start\":60367},{\"end\":60398,\"start\":60384},{\"end\":60416,\"start\":60398},{\"end\":60427,\"start\":60416},{\"end\":60439,\"start\":60427},{\"end\":60448,\"start\":60439},{\"end\":60456,\"start\":60448},{\"end\":60472,\"start\":60456},{\"end\":60485,\"start\":60472},{\"end\":60500,\"start\":60485},{\"end\":60509,\"start\":60500},{\"end\":60524,\"start\":60509},{\"end\":60535,\"start\":60524},{\"end\":60547,\"start\":60535},{\"end\":61040,\"start\":61031},{\"end\":61050,\"start\":61040},{\"end\":61061,\"start\":61050},{\"end\":61073,\"start\":61061},{\"end\":61086,\"start\":61073},{\"end\":61100,\"start\":61086},{\"end\":61115,\"start\":61100},{\"end\":61122,\"start\":61115},{\"end\":61131,\"start\":61122},{\"end\":61765,\"start\":61756},{\"end\":61773,\"start\":61765},{\"end\":61783,\"start\":61773},{\"end\":61794,\"start\":61783},{\"end\":61806,\"start\":61794},{\"end\":62310,\"start\":62301},{\"end\":62318,\"start\":62310},{\"end\":62329,\"start\":62318},{\"end\":62341,\"start\":62329},{\"end\":62535,\"start\":62526},{\"end\":62546,\"start\":62535},{\"end\":62558,\"start\":62546},{\"end\":63072,\"start\":63061},{\"end\":63083,\"start\":63072},{\"end\":63096,\"start\":63083},{\"end\":63105,\"start\":63096},{\"end\":63114,\"start\":63105},{\"end\":63128,\"start\":63114},{\"end\":63138,\"start\":63128},{\"end\":63491,\"start\":63483},{\"end\":63498,\"start\":63491},{\"end\":63507,\"start\":63498},{\"end\":63520,\"start\":63507},{\"end\":64088,\"start\":64080},{\"end\":64099,\"start\":64088},{\"end\":64108,\"start\":64099},{\"end\":64116,\"start\":64108},{\"end\":64128,\"start\":64116},{\"end\":64137,\"start\":64128},{\"end\":64150,\"start\":64137},{\"end\":64729,\"start\":64719},{\"end\":64742,\"start\":64729},{\"end\":64967,\"start\":64959},{\"end\":64975,\"start\":64967},{\"end\":64982,\"start\":64975},{\"end\":64990,\"start\":64982},{\"end\":64999,\"start\":64990},{\"end\":65415,\"start\":65407},{\"end\":65422,\"start\":65415},{\"end\":65431,\"start\":65422},{\"end\":65439,\"start\":65431},{\"end\":65714,\"start\":65706},{\"end\":65727,\"start\":65714},{\"end\":65736,\"start\":65727},{\"end\":65747,\"start\":65736},{\"end\":66157,\"start\":66149},{\"end\":66165,\"start\":66157},{\"end\":66177,\"start\":66165},{\"end\":66193,\"start\":66177},{\"end\":66205,\"start\":66193},{\"end\":66220,\"start\":66205},{\"end\":66230,\"start\":66220},{\"end\":66981,\"start\":66971},{\"end\":66990,\"start\":66981},{\"end\":66998,\"start\":66990},{\"end\":67006,\"start\":66998},{\"end\":67017,\"start\":67006},{\"end\":67027,\"start\":67017},{\"end\":67040,\"start\":67027},{\"end\":67050,\"start\":67040},{\"end\":67314,\"start\":67304},{\"end\":67323,\"start\":67314},{\"end\":67338,\"start\":67323},{\"end\":68164,\"start\":68158},{\"end\":68174,\"start\":68164},{\"end\":68182,\"start\":68174},{\"end\":68191,\"start\":68182},{\"end\":68206,\"start\":68191},{\"end\":68505,\"start\":68497},{\"end\":68511,\"start\":68505},{\"end\":68517,\"start\":68511},{\"end\":68526,\"start\":68517},{\"end\":68534,\"start\":68526},{\"end\":68541,\"start\":68534},{\"end\":68548,\"start\":68541},{\"end\":68557,\"start\":68548}]", "bib_venue": "[{\"end\":44672,\"start\":44575},{\"end\":45266,\"start\":45252},{\"end\":46714,\"start\":46698},{\"end\":48028,\"start\":48007},{\"end\":48991,\"start\":48971},{\"end\":49684,\"start\":49668},{\"end\":51141,\"start\":51128},{\"end\":52080,\"start\":52058},{\"end\":53822,\"start\":53800},{\"end\":54826,\"start\":54811},{\"end\":55525,\"start\":55522},{\"end\":56158,\"start\":56139},{\"end\":57921,\"start\":57663},{\"end\":58630,\"start\":58617},{\"end\":59490,\"start\":59403},{\"end\":61286,\"start\":61256},{\"end\":61945,\"start\":61920},{\"end\":62752,\"start\":62670},{\"end\":63632,\"start\":63612},{\"end\":64273,\"start\":64258},{\"end\":65825,\"start\":65805},{\"end\":66434,\"start\":66343},{\"end\":67559,\"start\":67553},{\"end\":44547,\"start\":44451},{\"end\":45226,\"start\":45146},{\"end\":45714,\"start\":45655},{\"end\":46050,\"start\":46037},{\"end\":46330,\"start\":46311},{\"end\":46669,\"start\":46611},{\"end\":47278,\"start\":47212},{\"end\":47647,\"start\":47611},{\"end\":47968,\"start\":47909},{\"end\":48575,\"start\":48482},{\"end\":48947,\"start\":48879},{\"end\":49641,\"start\":49450},{\"end\":50377,\"start\":50334},{\"end\":50760,\"start\":50705},{\"end\":51091,\"start\":51023},{\"end\":51636,\"start\":51566},{\"end\":52035,\"start\":51985},{\"end\":52817,\"start\":52745},{\"end\":53258,\"start\":53188},{\"end\":53775,\"start\":53707},{\"end\":54437,\"start\":54414},{\"end\":54798,\"start\":54753},{\"end\":55480,\"start\":55354},{\"end\":56137,\"start\":56068},{\"end\":56562,\"start\":56488},{\"end\":56782,\"start\":56713},{\"end\":57045,\"start\":56999},{\"end\":57628,\"start\":57382},{\"end\":58615,\"start\":58547},{\"end\":58951,\"start\":58881},{\"end\":59392,\"start\":59302},{\"end\":60031,\"start\":60016},{\"end\":60553,\"start\":60547},{\"end\":61243,\"start\":61131},{\"end\":61918,\"start\":61806},{\"end\":62357,\"start\":62351},{\"end\":62642,\"start\":62558},{\"end\":63201,\"start\":63138},{\"end\":63588,\"start\":63520},{\"end\":64214,\"start\":64150},{\"end\":64759,\"start\":64742},{\"end\":65073,\"start\":64999},{\"end\":65405,\"start\":65326},{\"end\":65803,\"start\":65747},{\"end\":66309,\"start\":66230},{\"end\":66969,\"start\":66903},{\"end\":67529,\"start\":67338},{\"end\":68224,\"start\":68206},{\"end\":68495,\"start\":68416},{\"end\":69393,\"start\":68797}]"}}}, "year": 2023, "month": 12, "day": 17}