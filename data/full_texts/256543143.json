{"id": 256543143, "updated": "2023-10-05 05:19:54.875", "metadata": {"title": "Collaborative Discrepancy Optimization for Reliable Image Anomaly Localization", "authors": "[{\"first\":\"Yunkang\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Xiaohao\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Zhaoge\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Weiming\",\"last\":\"Shen\",\"middle\":[]}]", "venue": "IEEE Transactions on Industrial Informatics", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Most unsupervised image anomaly localization methods suffer from overgeneralization because of the high generalization abilities of convolutional neural networks, leading to unreliable predictions. To mitigate the overgeneralization, this study proposes to collaboratively optimize normal and abnormal feature distributions with the assistance of synthetic anomalies, namely collaborative discrepancy optimization (CDO). CDO introduces a margin optimization module and an overlap optimization module to optimize the two key factors determining the localization performance, i.e., the margin and the overlap between the discrepancy distributions (DDs) of normal and abnormal samples. With CDO, a large margin and a small overlap between normal and abnormal DDs are obtained, and the prediction reliability is boosted. Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the overgeneralization and achieves great anomaly localization performance with real-time computation efficiency. A real-world automotive plastic parts inspection application further demonstrates the capability of the proposed CDO. Code is available on https://github.com/caoyunkang/CDO.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.08769", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tii/CaoXLS23", "doi": "10.1109/tii.2023.3241579"}}, "content": {"source": {"pdf_hash": "fbc63971070e2dec714c94294302640325169170", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.08769v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4c4e39074eec8c1ca35f2ab4bae88cae8debfb1a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fbc63971070e2dec714c94294302640325169170.txt", "contents": "\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS \uf020Index Terms-Anomaly LocalizationDefect DetectionCollaborative Discrepancy OptimizationComputer Vision\nMost unsupervised image anomaly localization methods suffer from overgeneralization because of the high generalization abilities of convolutional neural networks, leading to unreliable predictions. To mitigate the overgeneralization, this study proposes to collaboratively optimize normal and abnormal feature distributions with the assistance of synthetic anomalies, namely collaborative discrepancy optimization (CDO). CDO introduces a margin optimization module and an overlap optimization module to optimize the two key factors determining the localization performance, i.e., the margin and the overlap between the discrepancy distributions (DDs) of normal and abnormal samples. With CDO, a large margin and a small overlap between normal and abnormal DDs are obtained, and the prediction reliability is boosted. Experiments on MVTec2D and MVTec3D show that CDO effectively mitigates the overgeneralization and achieves great anomaly localization performance with real-time computation efficiency. A realworld automotive plastic parts inspection application further demonstrates the capability of the proposed CDO. Code is available on https://github.com/caoyunkang/CDO.\n\nI. INTRODUCTION\n\nmage anomaly localization aims to localize anomalies or defects that significantly deviate from normal patterns. Its abnormalities localizing ability can potentially greatly improve the effectiveness and performance of product quality controls [1]- [4] and thermal image-based fault diagnosis [5], which makes itself crucial in intelligent manufacturing systems.\n\nGenerally, anomaly samples are hard to collect in real-world applications, especially in the early deployment stage. It is also costly to make precise pixel-level annotations for anomaly samples. The lack of precisely annotated samples poses a challenge to the application of broadly investigated supervised semantic segmentation methods in the anomaly localization task. To mitigate the requirement for large amounts of annotated data, unsupervised image anomaly localization methods that only require normal samples in the training phase have gained significant popularity.\n\nMost unsupervised image anomaly localization methods [6]- [11] localize anomalies in the same fashion. In the training stage, they try to train a model to describe the normal feature distribution (FD). In the inference stage, they localize abnormal features according to the response of the model to the testing features. Among existing unsupervised image anomaly localization methods, reconstruction-based [12]- [17] and knowledge distillation-based [18]- [23] methods describe the normal FD implicitly with a feature-to-feature discrepancy optimization scheme. This scheme contains an informative expert and an initially-awkward apprentice, and both can extract features for input data and construct respective domains. In the expert domain, features distribute discriminatively, and normal and abnormal features can be distinguished well. To describe the normal FD, the apprentice is trained to mimic the normal FD in the expert domain by minimizing the feature-tofeature discrepancies between the two normal FDs, as shown in Fig. 1 (a). In existing methods [12]- [21], it is commonly assumed that the discrepancy minimization between the normal FDs rarely affects the abnormal FDs, and abnormal FDs still distribute differently. Hence, the discrepancies between features corresponding to the same input data in the two domains indicate the anomaly degree.\n\nHowever, the assumption is only partially valid because of the high generalization ability of the apprentice. When minimizing discrepancies between normal FDs, the apprentice may generalize unexpectedly and produce a similar abnormal FD to that in the expert domain, as Fig. 1 (a) shows, resulting in some low discrepancies between the abnormal FDs. This unexpected generalization is defined as the overgeneralization problem in this paper. The overgeneralization problem seriously affects the discrepancy distribution (DD) between corresponding features in the expert and apprentice domains, and the margin and the overlap between DDs of normal and abnormal features are also damaged. The margin and the overlap between DDs of normal features and abnormal features are two key factors that influence anomaly localization performance. The margin can be roughly defined as the absolute difference between the average values of the two DDs. The overlap can be regarded as the percentage of parts having the same values. The overgeneralization problem causes an overall decrease in the discrepancies between abnormal FDs, which results in a small margin and a large overlap between the DDs of normal features and abnormal features and makes the anomaly localization predictions less reliable, as Fig. 1 (b) shows. Whereas existing unsupervised image anomaly localization methods have achieved comparable localization performance, they only minimize the discrepancies between normal FDs and rarely address the overgeneralization, which leaves considerable further improvements for image anomaly localization methods.\n\nTo address the overgeneralization of existing unsupervised anomaly localization methods and ensure their prediction reliability, this study proposes a method to optimize the discrepancies between the normal FDs and abnormal FDs collaboratively, namely Collaborative Discrepancy Optimization (CDO), as Fig. 1 (c) shows. With CDO, the apprentice is aware of not only mimicking the normal FD in the expert domain but also producing an easily-distinguished abnormal FD to that in the expert domain, thus mitigating the overgeneralization problem. Concretely, CDO alleviates the overgeneralization problem by minimizing the discrepancies between the normal FDs and maximizing the abnormal FDs in the expert and apprentice domains simultaneously. The simultaneous optimization avoids those low discrepancies between abnormal FDs and improves the prediction reliability. However, CDO cannot directly maximize the discrepancies between abnormal FDs due to the inaccessibility of abnormal data in the training phase. Hence, CDO proposes to generate synthetic abnormal inputs via random perturbation and maximize the discrepancies between the two synthetic abnormal FDs instead of the real abnormal FDs. The maximization implicitly pushes the real abnormal FDs away, and assists in acquiring large discrepancies between abnormal FDs, as Fig. 1 (c) shows, which empowers the apprentice with a stronger ability to localize anomalies. Specifically, CDO proposes to optimize the two critical factors, the margin and the overlap between the DDs, respectively, and consists of a margin optimization module (MOM) and an overlap optimization module (OOM). MOM minimizes the discrepancies of normal FDs and maximizes those of synthetic abnormal FDs simultaneously to optimize the margin. To optimize the overlap, OOM raises attention to the tailed hard samples that are the most significant ones influencing the overlap, namely normal samples with large discrepancies and abnormal ones with small discrepancies. OOM assigns weights to individual samples dynamically based on the statistical information of DDs. Those tailed samples are assigned larger weights to draw more attention in the training process for the overlap reduction. With the proposed MOM and OOM, a large margin and a small overlap between the DDs of normal features and abnormal features are obtained, achieving reliable anomaly localization performance, as Fig. 1 (d) shows. Fig. 1 (e) also shows that CDO achieves significantly better results than its strong baseline ST [20].\n\nThe contributions of this study can be summarized as follows: \uf06c This study proposes a general formulation that takes anomaly localization into a discrepancy optimization scheme. Based on this formulation, this study presents Collaborative Discrepancy Optimization (CDO), which produces reliable and excellent anomaly localization.\n\n\uf06c This study proposes a margin optimization module (MOM) and an overlap optimization module (OOM) to support the proposed CDO, obtaining a large margin and a small overlap between the normal and abnormal discrepancy distributions.\n\n\uf06c For further validation, this study collects a real-world automotive plastic part dataset with pixel-wise annotations. Compared to existing anomaly detection datasets, the collected dataset is more challenging because it has normal patches with larger inter-class variance and anomalies of extremely small size. We applied the proposed CDO to this dataset and achieved impressive anomaly localization performance.\n\nThe remainder of the paper is organized as follows. Section II comprehensively reviews the related work on unsupervised image anomaly localization. CDO is illustrated in detail in Section III. Section IV presents thorough experiments, ablation studies, results, and analyses. Finally, Section V concludes this paper and discusses future research directions.\n\n\nII. RELATED WORK\n\nUnsupervised image anomaly localization methods can be categorized into three types: distribution-based [6]- [10], reconstruction-based [12]- [17], and knowledge distillationbased methods [18]- [21].\n\n\nA. Distribution-based methods\n\nDistribution-based methods [6]- [10] directly leverage clustering models or generative models to describe the normal FD. For example, SPADE [10] built a feature gallery first and then dynamically retrieved the most similar features in the gallery to the tested one using KNN. The distances between those features are used to score anomalies. However, SPADE had an inefficient computation as two KNNs were needed. Instead, GCPF [8] used several Gaussian clusters to describe the normal FD, and the computation speed was significantly improved. Similarly, PaDiM [9] leveraged individual Gaussian distribution in every location and achieved better performance, but PaDiM still required large memory consumption and performed subpar when images were not aligned well. DifferNet [6] and CFLOW [7] were based on a normalizing flow framework, which can automatically describe the distribution of normal features and explicitly estimate the likelihood of the tested features. Despite some improvements in anomaly localization, they are often time-consuming since the pre-trained network and decoder typically work sequentially.\n\n\nB. Reconstruction-based methods\n\nReconstruction-based methods [12]- [17] attempt to reconstruct the corresponding normal features for arbitrary noisy input features. The reconstructed features act as the apprentice domain, mimicking the expert domain constructed by input features. AESSIM [17] reconstructed the normal RGB values under several similarity constraints. MemAE [16] reconstructed normal features from a gallery, offering stronger constraints to prevent the reconstruction of abnormal features.\n\nSince RGB values are not distinctive enough, DFR [13] used pre-trained high-dimension features as inputs and achieved better performance.\n\nMoreover, DRAEM [14], RIAD [12], and dual-Siamese network [15] also generated anomalies to achieve better localization performance, but their purpose is virtually different from the proposed CDO. They generated anomalies to explicitly endow the network with the capability to suppress anomalous for better reconstruction quality. Thus, they try to output normal features under arbitrary inputs. In other words, they try to minimize the discrepancies between the normal FD in the expert domain and the normal and abnormal FDs in the apprentice domain, which is quite difficult and still cannot guarantee low prediction uncertainty. By contrast, CDO generates anomalies to optimize the margin and overlap between the two DDs, effectively decreasing the prediction uncertainty and improving anomaly localization performance.\n\n\nC. Knowledge distillation-based methods\n\nKnowledge distillation-based methods [18]- [23] provide a concise scheme to implicitly model the normal FD. In this scheme, the features from the pre-trained teacher network compose the expert domain, and those from the student network construct the apprentice domain. During the training phase, the student network only learns to regress the normal features, so it would deviate under abnormal features, and the pixel-wise feature regression errors indicate the anomaly scores. US [19] and MRKD [18] were the first to detect anomalies based on knowledge distillation. US [19] investigated multi-resolution feature representation by cropping patches with different sizes. Instead, MRKD [18] leveraged the features from different hierarchies to capture the multi-resolution context. ST [20] developed further with feature pyramid matching and achieved effective yet powerful anomaly localization performance. IKD [21] found high overfitting risk existed in the knowledge distillation-based scheme because of the mismatch between network capacity and knowledge contained in homogeneous images. IKD [21] proposed to distill informative knowledge and alleviate the overfitting. RD4AD [22] noticed the overgeneralization problem and mitigated it by reverse distillation, which is less likely to be overgeneralized as the expert and apprentice networks have different architectures. SKD [23] concluded that some real anomaly samples contribute to better localization performance. The aforementioned methods have reached state-of-the-art localization performance with great efficiency. However, knowledge distillation-based methods only minimize the discrepancies between the normal FDs. Due to the overgeneralization problem, the discrepancies between abnormal FDs also decrease during the discrepancy minimization of normal FDs, leading to high prediction uncertainties.\n\nThis study proposes to optimize the normal and abnormal FDs collaboratively, and the proposed CDO enlarges the margin and decreases the overlap between the normal and abnormal DDs, producing reliable and excellent anomaly localization performance.\n\n\nIII. COLLABORATIVE DISCREPANCY OPTIMIZATION\n\n\nA. Problem Definition\n\nThe image anomaly localization task aims at localizing abnormal regions that deviate from normal patterns and can be formulated as follows. \u2119 = { , , \u2026 , } is a set of pixels of anomaly-free training images. Image anomaly localization methods localize abnormal regions by learning an anomaly scoring function: : \u2192 \u211d that assigns large anomaly scores to abnormal pixels and relatively low scores to normal pixels.\n\nIn the rest of this section, this paper first describes a general formulation of existing methods. Then, the details of the proposed CDO are elaborated. The process of anomaly score calculation is depicted in the end.\n\n\nB. General Formulation of Existing Methods\n\nDenoting the normal and abnormal FDs corresponding to respective pixels in the expert and the apprentice domains as ( ), ( ), ( ), and ( ), most existing unsupervised anomaly localization methods can be formulated as the process of minimizing the discrepancies between ( ) and ( ) , which can be formulated as follows:\n= ( ), ( ) (1) where (\u2022,\u2022)\ndenotes a function for feature-to-feature discrepancy calculation between two distributions. Previous work [12]- [21] assumed that the discrepancies between ( ) and ( ) would not be significantly affected and keep large during the minimization of discrepancies between ( ) and ( ) . However, because of the overgeneralization, the discrepancies between ( ) and ( ) may also decrease, resulting in high prediction uncertainty.\n\nTo tackle the overgeneralization problem, this study proposes CDO, which improves the prediction certainty by optimizing discrepancies between normal FDs and abnormal FDs collaboratively. As no abnormal pixels can be accessed in the training stage, CDO introduces to leverage synthetic abnormal pixels for the assistance of discrepancies maximization between ( ) and ( ) . As Fig. 2 shows, CDO contains three modules: discrepancy distribution generation (DDG), margin optimization module (MOM), and overlap optimization module (OOM). Firstly, CDO perturbs the input normal image and maps the perturbed image into the expert and apprentice domains with respective map functions. Normal and synthetic abnormal features in the two domains are also acquired. Secondly, with the discrepancy function, DDs of normal features and synthetic abnormal features are generated. Thirdly, OOM assigns weights to individual features according to their discrepancies to raise more attention to hard tailed samples. Finally, to enlarge the margin between DDs, MOM maximizes the discrepancies of synthetic abnormal FDs and minimizes those of normal FDs collaboratively under the weights produced by the OOM. DDG, MOM, and OOM will be described in the following subsections.\n\n\nC. DDG\n\nThe normal and abnormal discrepancies between the two domains are formulated as ( ) and ( ): ( ) = ( ; ), ( ; ) (2) where and denote normal and abnormal pixels; and denote the map functions that can map pixels into respective domains and extract features; and are the corresponding parameters for the expert and apprentice networks, and keeps frozen while is optimized during the training phase. This study implements CDO based on the knowledge distillation-based scheme [18]- [21] because this scheme has achieved excellent performance and high computation efficiency. Concretely, in our implementation, denotes a pre-trained network, and denotes the corresponding randomly initialized network. The features for respective pixels extracted by and are denoted as and ; and denote the normalized features. Pixel-wise mean square errors are used to evaluate the discrepancy between the two features, ( ) = ( , ) = \u2212 (3) This study proposes to synthesize anomalies via random perturbation and maximize the discrepancies between them to optimize the discrepancies between features of real anomalies implicitly. Specifically, several squares are randomly generated, and the corresponding regions are replaced with random values sampled from a Gaussian normal distribution. Some samples of The improved optimization objective can empower the apprentice to mimic the expert as well as to be aware of producing large discrepancies when the input pixels are out-ofdistribution. With MOM, the margin between the normal and abnormal DDs can be enlarged significantly, and better performance can be produced. However, simply optimizing the average of normal and abnormal discrepancies may not optimize some tail samples well, still resulting in a large overlap between DDs. Hence, OOM is developed to raise specific attention to those tailed samples and complement the drawbacks of solely using MOM.\n\n\nE. OOM\n\nWith MOM, the DDs of normal features and abnormal features will be endowed with a larger margin than existing methods. While increasing the margin can effectively improve the performance, tailed samples are not optimized well and may affect the overlap. To better reduce the overlap, inspired by the Focal Loss [24], this study introduces OOM to dynamically assign weights to individual pixels during the training stage. OOM focuses on optimizing tailed samples to achieve better overall optimization results. Technically, the ratio between individual discrepancies to the average is a great indicator of pixel-wise importance in the training stage. This study first calculates the averages of normal and abnormal discrepancies, respectively,  \n\nAs normal FDs should have significantly smaller discrepancies than those of abnormal FDs, for discrepancies between normal FDs, the larger the ratio to the average, the more attention should be paid. For the discrepancies between abnormal FDs, the smaller the ratio, the more attention should be paid. Therefore, the weights for discrepancies are designed as follows:\n( ) = ( ) , ( ) = ( )(7)\nwhere \u2265 0 is a hyper-parameter that adjusts the effect of the modulation. Notably, the exponent of normal samples is , and that of abnormal samples is \u2212 . Therefore, normal pixels with discrepancies larger than the average are enhanced, and the weights of abnormal pixels with discrepancies smaller than the average are increased. Then the loss function of CDO combined with OOM and MOM can be formulated as:\n\u2112 = \u2211 ( ) ( ) \u2212 \u2211 ( ) ( ) \u2211 ( ) + \u2211 ( )(8)\n\nF. Anomaly Score Calculation\n\nAfter optimizing with the proposed CDO, the expert domain ( ) and the apprentice domain ( ) are endowed with small discrepancies between normal FDs and large discrepancies between abnormal FDs. Then the anomaly scoring function for individual pixels can be defined as:\n\n( ) = ( ) = \u2212 (9) where and denote the normalized features extracted by the expert network and apprentice network . Moreover, it has been proved that multi-hierarchical representations extracted from hidden layers of convolutional neural networks (CNNs) can further improve anomaly localization performance [8], [9], [20]. Hence, ( ) and ( ) in different hierarchies are leveraged, and the anomaly scores can be further defined as:\n( ) = ( ) ( ) \u210b(10)\nwhere ( ) ( ) denotes the discrepancy between the hierarchy ( ) and ( ) , and \u210b represents the number of hierarchies. Algorithm 1 summarizes the framework of CDO.\n\n\nIV. EXPERIMENTS\n\nIn this section, several sets of experiments are conducted on MVTec2D [3] and MVTec3D [1] to evaluate the performance of CDO and illustrate the influence of individual components. Besides, CDO is applied to inspect automotive plastic parts to validate its effectiveness in real-world applications.\n\n\nA. Experiments Settings\n\nDataset Descriptions: MVTec2D [3] was released by MVTec Software and has been widely studied for unsupervised image anomaly localization. More recently, a new dataset MVTec3D [1] was published, creating a new task to localize mainly geometric anomalies in 3D objects. RGB images and 3D scans are provided in MVTec3D. MVTec2D contains 15 categories, of which five are texture and ten are object categories. MVTec3D contains ten categories, and all of them are object categories. Both MVTec2D and MVTec3D have various anomaly types, such as scratch, colored, and broken. When localizing geometry anomalies in MVTec3D, CDO leverages no 3D scan because CDO only requires RGB images as inputs.\n\nEvaluation Metrics: The commonly used area under the receiver operating characteristic curve (AU-ROC) [3] and the normalized area under the per-region overlap curve (AU-PRO) [19] with a threshold of 0.3 is calculated in this study. For both AU-ROC and AU-PRO, a higher value indicates better detection performance.\n\nImplementation Details: All images are normalized using the mean and variance of the ImageNet dataset and resized to 256 \u00d7 256.\n\nis a network pre-trained on the ImageNet [25] and is frozen during training.\n\nis a randomly initialized network and is optimized by the AdamW algorithm [26] with a learning rate of 0.0002 and a batch size of 8. Unless otherwise specified, HRNet32 [27] is used as the backbones of and by default. The coefficient in Eq. (7) is fixed to 2 by default. Moreover, the expert and apprentice domains from the first three hierarchies are leveraged. The number of training epochs  (7)  9:\n\nCompute \u2112 according to Equation (8)  10:\n\nPerform a gradient descent step to update 11: end for 12: end for 12: Compute according to Equation (2), (9), and (10) 14: return # Inference Stage Input: Unknown pixels \u2119 , Anomaly scoring function : \u2192 \u211d Output: Anomaly scores for \u2119 1: Calculate anomaly scores using 2: return anomaly scores for \u2119 is set as 50. All results reported below are taken from the average and variance of the last five epochs.\n\n\nB. Comparisons with State-of-the-art methods\n\nMVTec2D: Table I and Table II summarize per-class comparisons between CDO and other state-of-the-art models, namely, ST [20], IKD [21], SPADE [10], PaDiM [9], and CFLOW [7]. Although supervised methods can be directly applied using perturbed anomalies, they do not perform well as they are usually based on the closed-set assumption. Thus, they try to extract distinctive features for both normal and abnormal data and determine whether test data are more likely to be normal or abnormal. Therefore, they can only detect anomalies that occurred in the training set. However, the synthetic anomalies are visually different from real anomalies, and supervised methods perform weakly, so this study does not compare supervised segmentation methods. For the common setting, despite slight drops in some categories, CDO achieves 1.11% AU-ROC and 2.26% AU-PRO higher than its strong baseline ST, especially in the transistor category, with improvements of 12.8% AU-ROC and 11.14% AU-PRO. It may demonstrate that CDO better captures global context information than ST and effectively mitigates the overgeneralization problem. CDO also outperforms the existing best-performing method PaDiM by a large margin, particularly in AU-PRO (2.57%).\n\nFor comparison with CFLOW [7], we also report the performance with the same evaluation setting as CFLOW, which picks the best results of each category with various backbones and resolutions. With the CFLOW setting, CDO achieves 0.07% AU-ROC and 1.9% AU-PRO higher than CFLOW. Besides, the performance of CDO using CFLOW setting is further improved compared with using the common setting, indicating that the resolution and backbone should be cautiously selected in practical applications, as a suitable resolution and backbone performs better than a general one.    Moreover, in nine categories, CDO achieves nearly perfect AU-ROC (>99%). As shown in Table I and Table II, CDO achieves the highest performance in terms of AU-ROC and AU-PRO for both settings, demonstrating that CDO has the strongest ability for anomaly localization. Some qualitative results are shown in Fig. 4 (a), which shows that CDO can detect a variety of defects reliably. MVTec3D: Table III summarizes per-class comparisons between CDO and other methods, including baselines [1] and 3D-ST [28]. We also evaluate ST [18] for a fair comparison. Among these methods, ST and CDO use RGB information, while others use depth information in voxel, depth, or point cloud for anomaly localization. CDO outperforms the bestperforming depth information-based method 3D-ST by a large margin (10.72% AU-PRO), demonstrating that RGB information is also effective for geometric defect detection. The reason may be that geometric defects usually bring changes in RGB values, while surface texture defects sometimes do not result in depth changes, and depth-only methods may suffer from detecting these texture defects. CDO still outperforms its baseline ST in MVTec3D, illustrating the generality of CDO. Some qualitative results are shown in Fig. 4 (b). Whereas the depth and RGB information may complement each other well and produce better localization performance, a 3D sensor incurs additional costs. Therefore, it may be more appropriate to use a common RGB camera in some geometric defect localization tasks that do not require high precision.\n\n\nC. Ablation Studies\n\nIn this subsection, comprehensive experiments are conducted to show the influence of individual components of CDO. Different combinations of MOM and OOM are evaluated. Table IV shows the evaluated results on MVTec2D and MVTec3D of several instantiated cases. For cases without MOM, only discrepancies between normal FDs are optimized. Fig. 5 (a) shows the DDs of various combinations in the leather category of MVTec2D.\n\n\nInfluence of MOM:\n\nThe influence of MOM can be concluded through the comparisons between Cases 1 and 3, and between Cases 2 and 4. Both comparisons show an improvement with MOM, especially in MVTec2D, with 2.33% and 1.04% AU-PRO gain, respectively.\n\nFrom comparisons between Cases 1 and 3, and between Cases 2 and 4 as shown in Fig. 5 (a), the influence of the MOM can be deduced. Concretely, the margins in Cases 3 and 4 are significantly larger than those in Cases 1 and 2, as discrepancies between real abnormal FDs are implicitly maximized with the assistance of synthetic abnormal FDs in Cases 3 and 4. Fig. 5 (b) shows the optimizing processes of normal and abnormal average discrepancies of Cases 1 and 3 during training. Both average discrepancies between normal FDs and abnormal FDs in Case 1 fall off quickly and then settle at low values, and the final margins are small. In Case 3 with MOM, the average discrepancies between abnormal FDs drop first and then increase to high values, resulting in a much larger margin than that in Case 1. The curves in Fig. 5 (b) effectively prove that the overgeneralization problem exists in previous methods, while MOM can alleviate it and secure reliable predictions.\n\n\nInfluence of OOM:\n\nThe influence of OOM can be concluded through the comparisons between Cases 1 and 2, and between Cases 3 and 4. As shown in Table IV, significant improvements can be obtained with OOM.\n\nFrom the comparisons between Cases 1 and 2, and between Cases 3 and 4 as shown in Fig. 5 (a), the influence of OOM can be deduced. For the cases without OOM, small second peaks of discrepancies exist, as shown in Fig. 5 (a), which indicates that some tailed normal features are not aligned well between the expert and apprentice domains. OOM raises abundant attention on the tailed samples and removes those second peaks, decreasing the overlap between normal and abnormal DDs.\n\nAccording to the comparison between discrepancies and the average, samples can be categorized into easy and hard samples. Fig. 6 (a) shows that a larger in Eq. (8) will produce more significant suppression to easy samples and enhancement to hard samples. Fig. 6 (b) shows the heatmaps of weights calculated by Eq. (8) in the training procedure. OOM gradually assigns some pixels with significantly larger weights, i.e., weights of tailed samples are enhanced.\n\nTo evaluate the influence of , this study conducts several experiments with different . Fig. 7 reports the evaluation results regarding the average of corresponding categories.   When = 0 , OOM makes no effect, and the anomaly localization performance is subpar. The localization performance is improved first and then stabilizes as is continuously increased. A suitable can improve nearly 0.5% AU-ROC and 1.2% AU-PRO than = 0 . More significant improvements are achieved in the object categories than in the texture categories, as normal patterns are more complex in the object categories, and OOM helps to optimize those tailed hard samples better.\n\n\nComparison of different backbones and resolutions:\n\nBackbones and image resolutions significantly influence localization performance [7], [8]. This study compares different backbones, including HR18 (HRNet18), HR32, HR48 [27], Res18 (ResNet18), Res34, Res50 [29], and WRes50 [30] under two resolutions, 256 \u00d7 256 and 512 \u00d7 512. As Fig. 8 shows, ResNet under the resolution of 256 \u00d7 256 performs better in texture categories. By contrast, HRNet under the resolution of 256 \u00d7 256 performs better in object categories. The reason may be that anomaly localization on texture categories needs low-level structural information while high-level semantic information is needed on object categories, and HRNet can extract more semantic features owing to its efficient information transmission between its several branches. HR32 with a resolution of 512 \u00d7 512 has the highest average AU-PRO of 95.21%, and HN48 with a resolution of 256 \u00d7 256 has the highest average AU-ROC of 98.28%.\n\n\nD. Complexity Evaluation\n\nThis study evaluates the actual computation and memory complexity for the proposed CDO and other state-of-the-art methods, including SPADE [10], PaDiM [9], and CFLOW [7] in terms of inference speed and model size metrics.\n\nThe model size is defined as the size of all floating-point parameters in the corresponding model in Table V. SPADE needs to store a feature gallery, and PaDiM needs to store pixelwise covariance, so they require high memory consumption, and both CFLOW and CDO require less memory than SPADE and PaDiM. In addition, since the decoder in CFLOW is larger than the apprentice feature extractor in CDO, CDO has a smaller model size than CFLOW. Ultimately, CDO requires the smallest size and is 1.1\u00d7 to 1.7\u00d7 smaller than CFLOW [7].\n\nThe inference speed shown in Table V is measured using a computer with Intel i7@2.30GHz CPU, NVIDIA GTX 2060 GPU, and 16G RAM. All models are allocated to GPU to accelerate the inference. The data input time is taken into consideration for the evaluation of each method when measuring the inference speed. CDO can run in real-time and is 41.9 \uf0b4 faster than SPADE and 34.7 \uf0b4 faster than PaDiM.\n\nBesides, as the extractors and of CDO can work in parallel, while the encoder and the decoder work sequentially in CFLOW, CDO is 1.3 \uf0b4 to 1.7 \uf0b4 faster than CFLOW.\n\nNotably, the inference speed of CDO is almost unchanged under different backbones as the main bottleneck of the inference speed is the data input time. Excluding the data input time, our model can achieve a very high computational speed, around 80 fps with Res18. Overall, CDO with HR32 has the best localization performance and considerable performance on inference speed and memory consumption.\n\n\nE. Application to Automotive Plastic Parts Inspection\n\nTo further evaluate the practical performance of CDO, we apply CDO to the inspection of real-world automotive plastic parts. The inspection device is shown in Fig. 9 (a). An automotive plastic parts dataset was collected using the device, and the dataset contains 1500 normal patches for training, 500 normal patches, and 271 abnormal patches for testing. All samples have a resolution of 256 \u00d7 256. Compared to existing   anomaly localization datasets like MVTec2D [3], this dataset is much more challenging because the normal patches have large inter-class variance, and the anomalies are extremely small in the dataset. Some samples are shown in Fig. 9 (b). This study compares the performance of our proposed CDO with different backbones and its strong baseline ST [20]. As mentioned earlier, WRes50 performs better for texture images, and the dataset collected is more like texture images. Hence, the proposed CDO performs the best with WRes50 and has 7.7% higher AU-PRO than the baseline ST [20], as shown in Table VI. Some examples of the localization results can be found in Fig. 4 (c). Despite the hard-to-detect anomalies, CDO localizes them impressively.\n\nIn summary, all experiment results show that the proposed CDO achieves a state-of-the-art performance and performs excellently on the automotive plastic parts dataset.\n\n\nV. CONCLUSION\n\nMost existing unsupervised anomaly localization methods focus on optimizing the discrepancies between normal feature distributions (FDs) in the expert and apprentice domains and neglect the optimization of abnormal FDs, and the high generalization abilities of convolutional neural network may result in overgeneralization. This paper proposes a collaborative discrepancy optimization (CDO) method to mitigate the overgeneralization problem in two aspects. First, a margin optimization module is proposed to explicitly enlarge the margin between the discrepancy distributions (DDs) with the assistance of synthetic anomalies. Second, an overlap optimization module is developed to reduce the overlap between DDs by assigning weights to individual samples dynamically. The experiments on MVTec2D and MVTec3D demonstrate that the proposed CDO achieves a state-of-the-art anomaly localization performance with very fast computation speed and low memory consumption. The application in realworld automotive plastic parts inspection further proves its effectiveness.\n\nIn the future, better anomaly generation methods like generative adversarial networks can be investigated to replace random perturbation in order to generate more real synthetic abnormal FDs and provide better assistance to optimize real abnormal DDs. On the other hand, the proposed image anomaly localization methods can be deployed to other industrial applications, such as intelligent fault diagnosis with thermal images. \n\n\nperturbed images are shown in Fig. 3. With random perturbation, normal and synthetic abnormal pixel sets { } and { } can be collected, and the numbers of pixels are denoted as and , respectively. The generated DDs are denoted as { ( )} and { ( )}. D. MOM Existing methods only minimized { ( )} and neglected the optimization of { ( Despite their impressive performances, high prediction uncertainty exists because of the overgeneralization problem. To address the overgeneralization problem, MOM minimizes { ( )} and maximizes { ( )} simultaneously to enlarge the margin between the two DDs,\n\nFig. 2 .\n2The framework of CDO.\n\nFig. 3 .\n3Some\n\nFig. 4 .\n4Qualitative results of the proposed CDO on the MVTec2D (a), MVTec3D (b) and the collected dataset of plastic parts (c). Original images, ground truth masks and anomaly maps are listed from top to bottom.\n\nFig. 5 .\n5(a) Discrepancy distributions of the corresponding cases. Xaxis: ( ), Y-axis: Density (b) The curves of the discrepancies over epoch in cases 1 and 3.\n\nFig. 6 .\n6(a) Influence of on the modulating factor of normal samples. (b) The heatmaps of weights under different epochs.\n\nFig. 8 .\n8The AU-ROC and AU-PRO of the proposed method in MVTec2D with different backbones and resolutions.\n\nFig. 9 .\n9(a) The device for inspection of automotive plastic parts. 1: camera, 2: stand, 3: light source, 4: the inspected plastic part. (b) Some samples from the collected dataset.\n\n\nCollaborative Discrepancy Optimization for Reliable Image Anomaly Localization Manuscript received XXXX; revised XXXX; accepted XXXX. (Corresponding author: Weiming Shen.) The authors are with the State Key Laboratory of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail: cyk_hust@hust.edu.cn; xxh11102019@outlook.com; liuzhg@hust.edu.cn; wshen@ieee.org)Yunkang Cao, Student Member, IEEE, Xiaohao Xu, Zhaoge Liu, Weiming Shen, Fellow, IEEE \n\nI \n\nFig. 1. Comparison between previous methods and the proposed \nCDO. Previous methods only minimize the discrepancies between \nnormal FDs in the expert and apprentice domains, bringing small \ndiscrepancies for some abnormal samples and leading to high \nprediction uncertainty, thus the overgeneralization problem. \nInstead, the proposed CDO collaboratively optimizes the normal \nand abnormal discrepancy distributions with the assistance of \nsynthesized anomalies to mitigate the overgeneralization problem. \n\n\n\n\nAlgorithm 1: Framework of CDO # Training Stage Input: Training normal pixels \u2119 = { , , \u2026 , } ; Expert network and its parameters ,; Apprentice \nnetwork and its parameters , \n; \nOutput: Anomaly scoring function : \u2192 \u211d \n1: Initialize \nwith pre-trained weights \n2: Initialize \nwith random weights \n3: for \n\u210e = 1 to _ \n\u210e do \n4: \nfor \n\u210e = 1 to _ \n\u210e do \n5: \n{ } \u2190 Randomly sample several pixels from \n\u2119 \n6: \n{ }, { } \u2190 Randomly perturb { } to \ngenerate synthetic abnormal pixels \n7: \nCompute { ( )}, { ( )} according to \nEquation (3) \n8: \nCompute { }, { } according to Equation \n(6) and \n\nTABLE III .\nIIIQUANTITATIVE COMPARISON OF ANOMALY DETECTION METHODS ON THE MVTEC3D DATASET IN TERMS OF AU-PRO.Modality \nCategory \nBagel \nCable gland \nCarrot \nCookie \nDowel \nFoam \nPeach \nPotato \nRope \nTire \nAverage \n\nVoxel \n\nGAN [1] \n44.00 \n45.30 \n82.50 \n75.50 \n78.20 \n37.80 \n39.20 \n63.90 \n77.50 \n38.90 \n58.28 \nAE [1] \n26.00 \n34.10 \n58.10 \n35.10 \n50.20 \n23.40 \n35.10 \n65.80 \n1.50 \n18.50 \n34.78 \nVM [1] \n45.30 \n34.30 \n52.10 \n69.70 \n68.00 \n28.40 \n34.90 \n63.40 \n61.60 \n34.60 \n49.23 \n\nDepth \n\nGAN [1] \n11.10 \n7.20 \n21.20 \n17.40 \n16.00 \n12.80 \n0.30 \n4.20 \n44.60 \n7.50 \n14.23 \nAE [1] \n14.70 \n6.90 \n29.30 \n21.70 \n20.70 \n18.10 \n16.40 \n6.60 \n54.50 \n14.20 \n20.31 \nVM [1] \n28.00 \n37.40 \n24.30 \n52.60 \n48.50 \n31.40 \n19.90 \n38.80 \n54.30 \n38.50 \n37.37 \n\nPCD \n3D-ST_64 [25] \n93.90 \n44.00 \n98.40 \n90.40 \n87.60 \n63.30 \n93.70 \n98.90 \n96.70 \n50.70 \n81.76 \n3D-ST_128 [25] \n95.00 \n48.30 \n98.60 \n92.10 \n90.50 \n63.20 \n94.50 \n98.80 \n97.60 \n52.40 \n83.10 \n\nRGB \n\nST [19] \n93.22 \n90.95 \n97.48 \n91.73 \n89.80 \n69.82 \n93.30 \n95.49 \n94.54 \n88.41 \n90.47 \nCDO \n97.52 \n98.30 \n98.08 \n86.32 \n97.60 \n70.53 \n98.61 \n96.05 \n97.05 \n97.40 \n\n93.75\uf0b1 0.13 \n\n\n\nTABLE II .\nIIQUANTITATIVE COMPARISON OF ANOMALY DETECTION METHODS ON THE MVTEC2D DATASET IN TERMS OF AU-PRO. CDO* AND CFLOW* DENOTE THE EVALUATED RESULTS UNDER THE EVALUATION SETTING OF CFLOW.Category \nCarpet \nGrid \nLeather \nTile \nWood \nBottle \nCable \nCapsule \nHazelnut \nMetal Nut \nPill \nScrew \nToothbrush \nTransistor \nZipper \nAverage \nST [19] \n95.80 \n96.60 \n98.00 \n92.10 \n93.60 \n95.10 \n87.70 \n92.20 \n94.30 \n94.50 \n96.50 \n93.00 \n92.20 \n69.50 \n95.20 \n92.42 \nIKD [20] \n94.49 \n87.73 \n97.64 \n86.35 \n89.06 \n96.08 \n94.21 \n90.62 \n95.97 \n94.69 \n96.09 \n92.95 \n87.01 \n93.78 \n91.55 \n92.55 \nSPADE [9] \n94.70 \n86.70 \n97.20 \n75.90 \n87.40 \n95.50 \n90.90 \n93.70 \n95.40 \n95.40 \n94.60 \n96.00 \n93.50 \n87.40 \n92.60 \n91.79 \nPaDiM [8] \n96.20 \n94.60 \n97.80 \n86.00 \n91.10 \n94.80 \n88.80 \n93.50 \n92.60 \n85.60 \n92.70 \n94.40 \n93.10 \n84.50 \n95.90 \n92.11 \nCDO \n96.77 \n96.02 \n98.34 \n90.51 \n92.87 \n97.17 \n94.17 \n92.97 \n97.39 \n95.74 \n96.59 \n94.33 \n90.50 \n92.56 \n94.28 \n94.68\uf0b1 0.20 \nCFLOW* [6] \n97.70 \n96.08 \n99.35 \n94.34 \n95.79 \n96.80 \n93.53 \n93.40 \n96.68 \n91.65 \n95.39 \n95.30 \n95.06 \n81.40 \n96.60 \n94.60 \nCDO* \n96.77 \n97.83 \n99.10 \n94.63 \n96.22 \n97.66 \n94.17 \n96.63 \n98.98 \n96.69 \n97.01 \n96.99 \n95.98 \n92.54 \n96.24 \n96.50\uf0b1 0.31 \n\n\n\nTABLE I .\nIQUANTITATIVE COMPARISON OF ANOMALY DETECTION METHODS ON THE MVTEC2D DATASET IN TERMS OF AU-ROC. CDO* AND CFLOW* DENOTE THE EVALUATED RESULTS UNDER THE EVALUATION SETTING OF CFLOW.Category \nCarpet \nGrid \nLeather \nTile \nWood \nBottle \nCable \nCapsule \nHazelnut \nMetal Nut \nPill \nScrew \nToothbrush \nTransistor \nZipper \nAverage \nST [19] \n98.80 \n99.00 \n99.30 \n97.40 \n97.20 \n98.80 \n95.50 \n98.30 \n98.50 \n97.60 \n97.80 \n98.30 \n98.90 \n82.50 \n98.50 \n97.09 \nIKD [20] \n98.71 \n97.04 \n98.53 \n95.68 \n93.88 \n98.99 \n98.03 \n98.55 \n98.71 \n98.38 \n98.79 \n98.63 \n98.58 \n97.13 \n97.56 \n97.81 \nSPADE [9] \n97.50 \n93.70 \n97.60 \n87.40 \n88.50 \n98.40 \n97.20 \n99.00 \n99.10 \n98.10 \n96.50 \n98.90 \n97.90 \n94.10 \n96.50 \n96.03 \nPaDiM [8] \n99.10 \n97.30 \n99.20 \n94.10 \n94.90 \n98.30 \n96.70 \n98.50 \n98.20 \n97.20 \n95.70 \n98.50 \n98.80 \n97.50 \n98.50 \n97.50 \nCDO \n99.08 \n98.40 \n99.17 \n97.20 \n95.85 \n99.30 \n97.60 \n98.64 \n99.24 \n98.54 \n98.94 \n99.01 \n98.86 \n95.30 \n98.21 \n98.22\uf0b1 0.05 \nCFLOW* [6] \n99.25 \n98.99 \n99.66 \n98.01 \n96.65 \n98.98 \n97.64 \n98.98 \n98.89 \n98.56 \n98.95 \n98.86 \n98.93 \n97.99 \n99.08 \n98.63 \nCDO* \n99.11 \n99.36 \n99.73 \n98.48 \n97.66 \n99.30 \n97.60 \n98.64 \n99.42 \n98.59 \n99.21 \n99.40 \n99.24 \n95.53 \n99.19 \n98.70\uf0b1 0.16 \n\n\nTABLE IV .\nIVQUANTITATIVE COMPARISON OF SEVERAL INSTANTIATED CASES. IN EVERY DATASET, THE LEFT COLUMN REPORTS AVERAGE AU-ROC, AND THE RIGHT COLUMN REPORTS AVERAGE AU-PRO. M DENOTES THE MOM. O DENOTES THE OOM. BEST IN BOLD.N M O \nMVTec2D \nMVTec3D \n\u2460 \n96.94\uf0b1 0.12 \n91.37\uf0b1 0.29 97.72\uf0b1 0.05 93.11\uf0b1 0.19 \n\u2461 \n\uf050 \n97.88\uf0b1 0.03 \n93.71\uf0b1 0.16 97.88\uf0b1 0.05 93.52\uf0b1 0.21 \n\u2462 \uf050 \n97.94\uf0b1 0.08 \n93.66\uf0b1 0.28 97.78\uf0b1 0.03 93.18\uf0b1 0.18 \n\u2463 \uf050 \uf050 \n98.25\uf0b1 0.05 \n94.75\uf0b1 0.20 \n97.88\uf0b1 0.05 93.75\uf0b1 0.13 \n\n\n\nTABLE V .\nVCOMPLEXITY COMPARISON IN TERMS OF INFERENCE SPEED (FPS) AND MODEL SIZE (MB).Fig. 7. The AU-ROC and AU-PRO of the proposed method in MVTec2D under different .Method \nBackbone \nInference speed \nModel size \n\nCDO \n\nRes18 \n18.70 \n89 \nHR32 \n16.58 \n227 \nWRes50 \n18.35 \n526 \nPaDiM [8] \nWRes50 \n0.47 \n4624 \nSPADE [9] \nWRes50 \n0.39 \n1284 \n\nCFLOW [6] \nRes18 \n12.72 \n96 \nWRes50 \n10.02 \n902 \n\n\n\nTABLE VI .\nVIQUANTITATIVE COMPARISON OF ST AND THE PROPOSED CDO WITH DIFFERENT BACKBONES. BEST IN BOLD.Method \nBackbone \nAU-PRO \nST [19] \nRes18 \n82.60\uf0b11.30 \n\nCDO \n\nRes18 \n84.55\uf0b11.31 \nRes34 \n87.15\uf0b10.50 \nRes50 \n86.30\uf0b11.52 \nWRes50 \n90.30\uf0b10.32 \nHR18 \n85.14\uf0b10.73 \nHR32 \n87.86\uf0b10.54 \nHR48 \n87.94\uf0b10.78 \n\nHis work has been cited more than 16 000 times with an h-index of 61. He authored or coauthored several books and more than 560 articles in scientific journals and international conferences in related areas. His research interests include agent-based collaboration technologies and applications, collaborative intelligent manufacturing, the Internet of Things, and Big Data analytics\nThe MVTec 3D-AD Dataset for Unsupervised 3D Anomaly Detection and Localization. P Bergmann, D Sattlegger, International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. P. Bergmann and D. Sattlegger, \"The MVTec 3D-AD Dataset for Unsupervised 3D Anomaly Detection and Localization,\" in International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2022.\n\nPGA-Net: Pyramid Feature Fusion and Global Context Attention Network for Automated Surface Defect Detection. H Dong, K Song, Y He, J Xu, Y Yan, Q Meng, 10.1109/TII.2019.2958826IEEE Trans. Ind. Informatics. 1612H. Dong, K. Song, Y. He, J. Xu, Y. Yan, and Q. Meng, \"PGA-Net: Pyramid Feature Fusion and Global Context Attention Network for Automated Surface Defect Detection,\" IEEE Trans. Ind. Informatics, vol. 16, no. 12, pp. 7448-7458, 2020, doi: 10.1109/TII.2019.2958826.\n\nThe MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. P Bergmann, K Batzner, M Fauser, D Sattlegger, C Steger, 10.1007/s11263-020-01400-4Int. J. Comput. Vis. 1294P. Bergmann, K. Batzner, M. Fauser, D. Sattlegger, and C. Steger, \"The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection,\" Int. J. Comput. Vis., vol. 129, no. 4, pp. 1038-1059, 2021, doi: 10.1007/s11263-020-01400-4.\n\nKnowledge transfer in fault diagnosis of rotary machines. G Liu, W Shen, L Gao, A Kusiak, 10.1049/cim2.12047IET Collab. Intell. Manuf. 41G. Liu, W. Shen, L. Gao, and A. Kusiak, \"Knowledge transfer in fault diagnosis of rotary machines,\" IET Collab. Intell. Manuf., vol. 4, no. 1, pp. 17-34, 2022, doi: 10.1049/cim2.12047.\n\nFault diagnosis of electric impact drills using thermal imaging. A Glowacz, 10.1016/j.measurement.2020.108815Meas. J. Int. Meas. Confed. 171108815A. Glowacz, \"Fault diagnosis of electric impact drills using thermal imaging,\" Meas. J. Int. Meas. Confed., vol. 171, no. August 2020, p. 108815, 2021, doi: 10.1016/j.measurement.2020.108815.\n\nSame Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows. M Rudolph, B Wandt, B Rosenhahn, 10.1109/wacv48630.2021.00195Proceedings of the IEEE Winter Conference on Applications of Computer Vision. the IEEE Winter Conference on Applications of Computer VisionM. Rudolph, B. Wandt, and B. Rosenhahn, \"Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows,\" in Proceedings of the IEEE Winter Conference on Applications of Computer Vision, 2021, pp. 1906-1915, doi: 10.1109/wacv48630.2021.00195.\n\nCFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows. D Gudovskiy, S Ishizaka, K Kozuka, Proceedings of the IEEE Winter Conference on Applications of Computer Vision. the IEEE Winter Conference on Applications of Computer VisionD. Gudovskiy, S. Ishizaka, and K. Kozuka, \"CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows,\" in Proceedings of the IEEE Winter Conference on Applications of Computer Vision, 2021, pp. 98-107.\n\nIndustrial Image Anomaly Localization Based on Gaussian Clustering of Pre-trained Feature. Q Wan, L Gao, X Li, L Wen, 10.1109/tie.2021.3094452IEEE Trans. Ind. Electron. 00462021Q. Wan, L. Gao, X. Li, and L. Wen, \"Industrial Image Anomaly Localization Based on Gaussian Clustering of Pre-trained Feature,\" IEEE Trans. Ind. Electron., vol. 0046, 2021, doi: 10.1109/tie.2021.3094452.\n\nPaDiM: A Patch Distribution Modeling Framework for Anomaly Detection and Localization. T Defard, A Setkov, A Loesch, R Audigier, 10.1007/978-3-030-68799-1_35International Conference on Pattern Recognition. T. Defard, A. Setkov, A. Loesch, and R. Audigier, \"PaDiM: A Patch Distribution Modeling Framework for Anomaly Detection and Localization,\" in International Conference on Pattern Recognition, 2021, pp. 475-489, doi: 10.1007/978-3-030-68799-1_35.\n\nPANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation. T Reiss, N Cohen, L Bergman, Y Hoshen, 10.1109/cvpr46437.2021.00283Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionT. Reiss, N. Cohen, L. Bergman, and Y. Hoshen, \"PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 2805-2813, doi: 10.1109/cvpr46437.2021.00283.\n\nPatch SVDD: Patch-Level SVDD for Anomaly Detection and Segmentation. J Yi, S Yoon, 10.1007/978-3-030-69544-6_23Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer VisionJ. Yi and S. Yoon, \"Patch SVDD: Patch-Level SVDD for Anomaly Detection and Segmentation,\" in Proceedings of the Asian Conference on Computer Vision, 2020, pp. 375-390, doi: 10.1007/978-3-030-69544- 6_23.\n\nReconstruction by inpainting for visual anomaly detection. V Zavrtanik, M Kristan, D Sko\u010daj, 10.1016/j.patcog.2020.107706Pattern Recognit. 1122021V. Zavrtanik, M. Kristan, and D. Sko\u010daj, \"Reconstruction by inpainting for visual anomaly detection,\" Pattern Recognit., vol. 112, 2021, doi: 10.1016/j.patcog.2020.107706.\n\nDFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation. Y Shi, J Yang, Z Qi, 10.1016/j.neucom.2020.11.018Neurocomputing. 424Y. Shi, J. Yang, and Z. Qi, \"DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation,\" Neurocomputing, vol. 424, pp. 9-22, 2021, doi: 10.1016/j.neucom.2020.11.018.\n\nDRAEM --A discriminatively trained reconstruction embedding for surface anomaly detection. V Zavrtanik, M Kristan, D Sko\u010daj, 10.1109/ICCV48922.2021.00822Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionV. Zavrtanik, M. Kristan, and D. Sko\u010daj, \"DRAEM --A discriminatively trained reconstruction embedding for surface anomaly detection,\" in Proceedings of the IEEE International Conference on Computer Vision, 2021, pp. 8310-8319, doi: 10.1109/ICCV48922.2021.00822.\n\nUnsupervised Anomaly Detection for Surface Defects with Dual-Siamese Network. X Tao, D P Zhang, W Ma, Z Hou, Z Lu, C Adak, 10.1109/TII.2022.3142326IEEE Trans. Ind. Informatics. 32032022X. Tao, D. P. Zhang, W. Ma, Z. Hou, Z. Lu, and C. Adak, \"Unsupervised Anomaly Detection for Surface Defects with Dual-Siamese Network,\" IEEE Trans. Ind. Informatics, vol. 3203, no. c, 2022, doi: 10.1109/TII.2022.3142326.\n\nMemorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection. D Gong, 10.1109/ICCV.2019.00179Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionD. Gong et al., \"Memorizing normality to detect anomaly: Memory- augmented deep autoencoder for unsupervised anomaly detection,\" in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1705-1714, doi: 10.1109/ICCV.2019.00179.\n\nImproving unsupervised defect segmentation by applying structural similarity to autoencoders. P Bergmann, S L\u00f6we, M Fauser, D Sattlegger, C Steger, 10.5220/0007364503720380International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. 5P. Bergmann, S. L\u00f6we, M. Fauser, D. Sattlegger, and C. Steger, \"Improving unsupervised defect segmentation by applying structural similarity to autoencoders,\" in International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2019, vol. 5, pp. 372-380, doi: 10.5220/0007364503720380.\n\nMultiresolution Knowledge Distillation for Anomaly Detection. M Salehi, N Sadjadi, S Baselizadeh, M H Rohban, H R Rabiee, 10.1109/CVPR46437.2021.01466Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionM. Salehi, N. Sadjadi, S. Baselizadeh, M. H. Rohban, and H. R. Rabiee, \"Multiresolution Knowledge Distillation for Anomaly Detection,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021, pp. 14897-14907, doi: 10.1109/CVPR46437.2021.01466.\n\nUninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings. P Bergmann, M Fauser, D Sattlegger, C Steger, 10.1109/CVPR42600.2020.00424Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionP. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, \"Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 4182-4191, doi: 10.1109/CVPR42600.2020.00424.\n\nStudent-Teacher Feature Pyramid Matching for Anomaly Detection. G Wang, S Han, E Ding, D Huang, Proceedings of the British Machine Vision Conference. the British Machine Vision Conference2021G. Wang, S. Han, E. Ding, and D. Huang, \"Student-Teacher Feature Pyramid Matching for Anomaly Detection,\" in Proceedings of the British Machine Vision Conference, 2021\n\nInformative knowledge distillation for image anomaly segmentation. Y Cao, Q Wan, W Shen, L Gao, 10.1016/j.knosys.2022.1088462022Knowledge-Based SystY. Cao, Q. Wan, W. Shen, and L. Gao, \"Informative knowledge distillation for image anomaly segmentation,\" Knowledge-Based Syst., 2022, doi: 10.1016/j.knosys.2022.108846.\n\nAnomaly Detection via Reverse Distillation from One-Class Embedding. H Deng, X Li, 10.48550/arXiv.2201.10703IEEE Conference on Computer Vision and Pattern Recognition. H. Deng and X. Li, \"Anomaly Detection via Reverse Distillation from One-Class Embedding,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2022, pp. 9727--9736, doi: 10.48550/arXiv.2201.10703.\n\nSemi-supervised Knowledge Distillation for Tiny Defect Detection. Y Cao, Y Song, X Xu, S Li, Y Yu, W Shen, 10.1109/CSCWD54268.2022.9776026International Conference on Computer Supported Cooperative Work in Design. Y. Cao, Y. Song, X. Xu, S. Li, Y. Yu, and W. Shen, \"Semi-supervised Knowledge Distillation for Tiny Defect Detection,\" in International Conference on Computer Supported Cooperative Work in Design, 2022, pp. 1010-1015, doi: 10.1109/CSCWD54268.2022.9776026.\n\nFocal Loss for Dense Object Detection. T Y Lin, P Goyal, R Girshick, K He, P Dollar, 10.1109/TPAMI.2018.2858826IEEE Trans. Pattern Anal. Mach. Intell. 422T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, \"Focal Loss for Dense Object Detection,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 2, pp. 318-327, 2020, doi: 10.1109/TPAMI.2018.2858826.\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, R Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 10.1109/cvprw.2009.5206848Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, \"ImageNet: A large-scale hierarchical image database,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248-255, doi: 10.1109/cvprw.2009.5206848.\n\nDecoupled Weight Decay Regularization. F Loshchilov, Hutter, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsLoshchilov and F. Hutter, \"Decoupled Weight Decay Regularization,\" in Proceedings of the International Conference on Learning Representations, 2019.\n\nDeep High-Resolution Representation Learning for Visual Recognition. J Wang, 10.1109/tpami.2020.2983686IEEE Trans. Pattern Anal. Mach. Intell. 4310J. Wang et al., \"Deep High-Resolution Representation Learning for Visual Recognition,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 10, pp. 3349-3364, 2020, doi: 10.1109/tpami.2020.2983686.\n\nAnomaly Detection in 3D Point Clouds using Deep Geometric Descriptors. P Bergmann, D Sattlegger, ArXiv PreprP. Bergmann and D. Sattlegger, \"Anomaly Detection in 3D Point Clouds using Deep Geometric Descriptors,\" ArXiv Prepr., pp. 1-21.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.90Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.\n\nWide Residual Networks. B S Zagoruyko, N Komodakis, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceB. S. Zagoruyko and N. Komodakis, \"Wide Residual Networks,\" in Proceedings of the British Machine Vision Conference, 2016.\n", "annotations": {"author": null, "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": null, "title": null, "venue": null, "abstract": "[{\"end\":1324,\"start\":150}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1590,\"start\":1587},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1595,\"start\":1592},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1639,\"start\":1636},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2340,\"start\":2337},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2346,\"start\":2342},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2695,\"start\":2691},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2701,\"start\":2697},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2739,\"start\":2735},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2745,\"start\":2741},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3349,\"start\":3345},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3355,\"start\":3351},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7786,\"start\":7782},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9254,\"start\":9251},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9260,\"start\":9256},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9287,\"start\":9283},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9293,\"start\":9289},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9339,\"start\":9335},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9345,\"start\":9341},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9410,\"start\":9407},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9416,\"start\":9412},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9524,\"start\":9520},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9810,\"start\":9807},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9943,\"start\":9940},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10157,\"start\":10154},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10171,\"start\":10168},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10568,\"start\":10564},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10574,\"start\":10570},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10795,\"start\":10791},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10880,\"start\":10876},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11063,\"start\":11059},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11169,\"start\":11165},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11180,\"start\":11176},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11211,\"start\":11207},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12055,\"start\":12051},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12061,\"start\":12057},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12500,\"start\":12496},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12514,\"start\":12510},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12590,\"start\":12586},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12704,\"start\":12700},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12803,\"start\":12799},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12930,\"start\":12926},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13114,\"start\":13110},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13198,\"start\":13194},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13399,\"start\":13395},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15335,\"start\":15331},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15341,\"start\":15337},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17392,\"start\":17388},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17398,\"start\":17394},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19130,\"start\":19126},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20724,\"start\":20721},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21017,\"start\":21014},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21022,\"start\":21019},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21028,\"start\":21024},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21414,\"start\":21411},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21430,\"start\":21427},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21699,\"start\":21696},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21844,\"start\":21841},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22461,\"start\":22458},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22534,\"start\":22530},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22846,\"start\":22842},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22957,\"start\":22953},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23052,\"start\":23048},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23432,\"start\":23429},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23901,\"start\":23897},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23911,\"start\":23907},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23923,\"start\":23919},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23934,\"start\":23931},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23949,\"start\":23946},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25040,\"start\":25037},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26064,\"start\":26061},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26079,\"start\":26075},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26105,\"start\":26101},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29786,\"start\":29783},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30719,\"start\":30716},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30724,\"start\":30721},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30808,\"start\":30804},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30845,\"start\":30841},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30862,\"start\":30858},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31728,\"start\":31724},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31739,\"start\":31736},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31754,\"start\":31751},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32333,\"start\":32330},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33817,\"start\":33814},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34121,\"start\":34117},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34349,\"start\":34345}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36783,\"start\":36190},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36816,\"start\":36784},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36832,\"start\":36817},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37047,\"start\":36833},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37209,\"start\":37048},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37333,\"start\":37210},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37442,\"start\":37334},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37626,\"start\":37443},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38656,\"start\":37627},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39239,\"start\":38657},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40353,\"start\":39240},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41552,\"start\":40354},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42748,\"start\":41553},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43220,\"start\":42749},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43613,\"start\":43221},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":43910,\"start\":43614}]", "paragraph": "[{\"end\":1705,\"start\":1343},{\"end\":2282,\"start\":1707},{\"end\":3643,\"start\":2284},{\"end\":5257,\"start\":3645},{\"end\":7787,\"start\":5259},{\"end\":8119,\"start\":7789},{\"end\":8351,\"start\":8121},{\"end\":8767,\"start\":8353},{\"end\":9126,\"start\":8769},{\"end\":9346,\"start\":9147},{\"end\":10499,\"start\":9380},{\"end\":11008,\"start\":10535},{\"end\":11147,\"start\":11010},{\"end\":11970,\"start\":11149},{\"end\":13879,\"start\":12014},{\"end\":14128,\"start\":13881},{\"end\":14612,\"start\":14200},{\"end\":14831,\"start\":14614},{\"end\":15196,\"start\":14878},{\"end\":15649,\"start\":15224},{\"end\":16906,\"start\":15651},{\"end\":18804,\"start\":16917},{\"end\":19559,\"start\":18815},{\"end\":19928,\"start\":19561},{\"end\":20362,\"start\":19954},{\"end\":20705,\"start\":20437},{\"end\":21138,\"start\":20707},{\"end\":21321,\"start\":21159},{\"end\":21638,\"start\":21341},{\"end\":22354,\"start\":21666},{\"end\":22670,\"start\":22356},{\"end\":22799,\"start\":22672},{\"end\":22877,\"start\":22801},{\"end\":23280,\"start\":22879},{\"end\":23322,\"start\":23282},{\"end\":23728,\"start\":23324},{\"end\":25009,\"start\":23777},{\"end\":27120,\"start\":25011},{\"end\":27563,\"start\":27144},{\"end\":27814,\"start\":27585},{\"end\":28782,\"start\":27816},{\"end\":28988,\"start\":28804},{\"end\":29467,\"start\":28990},{\"end\":29928,\"start\":29469},{\"end\":30580,\"start\":29930},{\"end\":31556,\"start\":30635},{\"end\":31806,\"start\":31585},{\"end\":32334,\"start\":31808},{\"end\":32728,\"start\":32336},{\"end\":32892,\"start\":32730},{\"end\":33290,\"start\":32894},{\"end\":34513,\"start\":33348},{\"end\":34682,\"start\":34515},{\"end\":35761,\"start\":34700},{\"end\":36189,\"start\":35763}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15223,\"start\":15197},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19953,\"start\":19929},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20405,\"start\":20363},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21158,\"start\":21139}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23806,\"start\":23786},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25682,\"start\":25662},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25976,\"start\":25967},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27320,\"start\":27312},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28936,\"start\":28928},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31916,\"start\":31909},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32372,\"start\":32365},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34371,\"start\":34363}]", "section_header": "[{\"end\":1341,\"start\":1326},{\"end\":9145,\"start\":9129},{\"end\":9378,\"start\":9349},{\"end\":10533,\"start\":10502},{\"end\":12012,\"start\":11973},{\"end\":14174,\"start\":14131},{\"end\":14198,\"start\":14177},{\"end\":14876,\"start\":14834},{\"end\":16915,\"start\":16909},{\"end\":18813,\"start\":18807},{\"end\":20435,\"start\":20407},{\"end\":21339,\"start\":21324},{\"end\":21664,\"start\":21641},{\"end\":23775,\"start\":23731},{\"end\":27142,\"start\":27123},{\"end\":27583,\"start\":27566},{\"end\":28802,\"start\":28785},{\"end\":30633,\"start\":30583},{\"end\":31583,\"start\":31559},{\"end\":33346,\"start\":33293},{\"end\":34698,\"start\":34685},{\"end\":36793,\"start\":36785},{\"end\":36826,\"start\":36818},{\"end\":36842,\"start\":36834},{\"end\":37057,\"start\":37049},{\"end\":37219,\"start\":37211},{\"end\":37343,\"start\":37335},{\"end\":37452,\"start\":37444},{\"end\":39252,\"start\":39241},{\"end\":40365,\"start\":40355},{\"end\":41563,\"start\":41554},{\"end\":42760,\"start\":42750},{\"end\":43231,\"start\":43222},{\"end\":43625,\"start\":43615}]", "table": "[{\"end\":38656,\"start\":38056},{\"end\":39239,\"start\":38789},{\"end\":40353,\"start\":39351},{\"end\":41552,\"start\":40547},{\"end\":42748,\"start\":41744},{\"end\":43220,\"start\":42972},{\"end\":43613,\"start\":43390},{\"end\":43910,\"start\":43718}]", "figure_caption": "[{\"end\":36783,\"start\":36192},{\"end\":36816,\"start\":36795},{\"end\":36832,\"start\":36828},{\"end\":37047,\"start\":36844},{\"end\":37209,\"start\":37059},{\"end\":37333,\"start\":37221},{\"end\":37442,\"start\":37345},{\"end\":37626,\"start\":37454},{\"end\":38056,\"start\":37629},{\"end\":38789,\"start\":38659},{\"end\":39351,\"start\":39256},{\"end\":40547,\"start\":40368},{\"end\":41744,\"start\":41565},{\"end\":42972,\"start\":42763},{\"end\":43390,\"start\":43233},{\"end\":43718,\"start\":43628}]", "figure_ref": "[{\"end\":3323,\"start\":3313},{\"end\":3925,\"start\":3915},{\"end\":4948,\"start\":4938},{\"end\":5570,\"start\":5560},{\"end\":6596,\"start\":6586},{\"end\":7677,\"start\":7667},{\"end\":7691,\"start\":7685},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16033,\"start\":16027},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25893,\"start\":25883},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26823,\"start\":26813},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27489,\"start\":27479},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27904,\"start\":27894},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28180,\"start\":28174},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28636,\"start\":28630},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29082,\"start\":29072},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29213,\"start\":29203},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29601,\"start\":29591},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29734,\"start\":29724},{\"end\":30024,\"start\":30018},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30920,\"start\":30914},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33517,\"start\":33507},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34003,\"start\":33997},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34441,\"start\":34431}]", "bib_author_first_name": "[{\"end\":44376,\"start\":44375},{\"end\":44388,\"start\":44387},{\"end\":44846,\"start\":44845},{\"end\":44854,\"start\":44853},{\"end\":44862,\"start\":44861},{\"end\":44868,\"start\":44867},{\"end\":44874,\"start\":44873},{\"end\":44881,\"start\":44880},{\"end\":45319,\"start\":45318},{\"end\":45331,\"start\":45330},{\"end\":45342,\"start\":45341},{\"end\":45352,\"start\":45351},{\"end\":45366,\"start\":45365},{\"end\":45755,\"start\":45754},{\"end\":45762,\"start\":45761},{\"end\":45770,\"start\":45769},{\"end\":45777,\"start\":45776},{\"end\":46085,\"start\":46084},{\"end\":46441,\"start\":46440},{\"end\":46452,\"start\":46451},{\"end\":46461,\"start\":46460},{\"end\":47007,\"start\":47006},{\"end\":47020,\"start\":47019},{\"end\":47032,\"start\":47031},{\"end\":47520,\"start\":47519},{\"end\":47527,\"start\":47526},{\"end\":47534,\"start\":47533},{\"end\":47540,\"start\":47539},{\"end\":47898,\"start\":47897},{\"end\":47908,\"start\":47907},{\"end\":47918,\"start\":47917},{\"end\":47928,\"start\":47927},{\"end\":48339,\"start\":48338},{\"end\":48348,\"start\":48347},{\"end\":48357,\"start\":48356},{\"end\":48368,\"start\":48367},{\"end\":48880,\"start\":48879},{\"end\":48886,\"start\":48885},{\"end\":49281,\"start\":49280},{\"end\":49294,\"start\":49293},{\"end\":49305,\"start\":49304},{\"end\":49613,\"start\":49612},{\"end\":49620,\"start\":49619},{\"end\":49628,\"start\":49627},{\"end\":49952,\"start\":49951},{\"end\":49965,\"start\":49964},{\"end\":49976,\"start\":49975},{\"end\":50476,\"start\":50475},{\"end\":50483,\"start\":50482},{\"end\":50485,\"start\":50484},{\"end\":50494,\"start\":50493},{\"end\":50500,\"start\":50499},{\"end\":50507,\"start\":50506},{\"end\":50513,\"start\":50512},{\"end\":50914,\"start\":50913},{\"end\":51413,\"start\":51412},{\"end\":51425,\"start\":51424},{\"end\":51433,\"start\":51432},{\"end\":51443,\"start\":51442},{\"end\":51457,\"start\":51456},{\"end\":51987,\"start\":51986},{\"end\":51997,\"start\":51996},{\"end\":52008,\"start\":52007},{\"end\":52023,\"start\":52022},{\"end\":52025,\"start\":52024},{\"end\":52035,\"start\":52034},{\"end\":52037,\"start\":52036},{\"end\":52586,\"start\":52585},{\"end\":52598,\"start\":52597},{\"end\":52608,\"start\":52607},{\"end\":52622,\"start\":52621},{\"end\":53154,\"start\":53153},{\"end\":53162,\"start\":53161},{\"end\":53169,\"start\":53168},{\"end\":53177,\"start\":53176},{\"end\":53517,\"start\":53516},{\"end\":53524,\"start\":53523},{\"end\":53531,\"start\":53530},{\"end\":53539,\"start\":53538},{\"end\":53838,\"start\":53837},{\"end\":53846,\"start\":53845},{\"end\":54211,\"start\":54210},{\"end\":54218,\"start\":54217},{\"end\":54226,\"start\":54225},{\"end\":54232,\"start\":54231},{\"end\":54238,\"start\":54237},{\"end\":54244,\"start\":54243},{\"end\":54654,\"start\":54653},{\"end\":54656,\"start\":54655},{\"end\":54663,\"start\":54662},{\"end\":54672,\"start\":54671},{\"end\":54684,\"start\":54683},{\"end\":54690,\"start\":54689},{\"end\":55031,\"start\":55028},{\"end\":55041,\"start\":55038},{\"end\":55049,\"start\":55048},{\"end\":55064,\"start\":55058},{\"end\":55072,\"start\":55069},{\"end\":55079,\"start\":55077},{\"end\":55552,\"start\":55551},{\"end\":55922,\"start\":55921},{\"end\":56271,\"start\":56270},{\"end\":56283,\"start\":56282},{\"end\":56483,\"start\":56482},{\"end\":56489,\"start\":56488},{\"end\":56498,\"start\":56497},{\"end\":56505,\"start\":56504},{\"end\":56911,\"start\":56910},{\"end\":56913,\"start\":56912},{\"end\":56926,\"start\":56925}]", "bib_author_last_name": "[{\"end\":44385,\"start\":44377},{\"end\":44399,\"start\":44389},{\"end\":44851,\"start\":44847},{\"end\":44859,\"start\":44855},{\"end\":44865,\"start\":44863},{\"end\":44871,\"start\":44869},{\"end\":44878,\"start\":44875},{\"end\":44886,\"start\":44882},{\"end\":45328,\"start\":45320},{\"end\":45339,\"start\":45332},{\"end\":45349,\"start\":45343},{\"end\":45363,\"start\":45353},{\"end\":45373,\"start\":45367},{\"end\":45759,\"start\":45756},{\"end\":45767,\"start\":45763},{\"end\":45774,\"start\":45771},{\"end\":45784,\"start\":45778},{\"end\":46093,\"start\":46086},{\"end\":46449,\"start\":46442},{\"end\":46458,\"start\":46453},{\"end\":46471,\"start\":46462},{\"end\":47017,\"start\":47008},{\"end\":47029,\"start\":47021},{\"end\":47039,\"start\":47033},{\"end\":47524,\"start\":47521},{\"end\":47531,\"start\":47528},{\"end\":47537,\"start\":47535},{\"end\":47544,\"start\":47541},{\"end\":47905,\"start\":47899},{\"end\":47915,\"start\":47909},{\"end\":47925,\"start\":47919},{\"end\":47937,\"start\":47929},{\"end\":48345,\"start\":48340},{\"end\":48354,\"start\":48349},{\"end\":48365,\"start\":48358},{\"end\":48375,\"start\":48369},{\"end\":48883,\"start\":48881},{\"end\":48891,\"start\":48887},{\"end\":49291,\"start\":49282},{\"end\":49302,\"start\":49295},{\"end\":49312,\"start\":49306},{\"end\":49617,\"start\":49614},{\"end\":49625,\"start\":49621},{\"end\":49631,\"start\":49629},{\"end\":49962,\"start\":49953},{\"end\":49973,\"start\":49966},{\"end\":49983,\"start\":49977},{\"end\":50480,\"start\":50477},{\"end\":50491,\"start\":50486},{\"end\":50497,\"start\":50495},{\"end\":50504,\"start\":50501},{\"end\":50510,\"start\":50508},{\"end\":50518,\"start\":50514},{\"end\":50919,\"start\":50915},{\"end\":51422,\"start\":51414},{\"end\":51430,\"start\":51426},{\"end\":51440,\"start\":51434},{\"end\":51454,\"start\":51444},{\"end\":51464,\"start\":51458},{\"end\":51994,\"start\":51988},{\"end\":52005,\"start\":51998},{\"end\":52020,\"start\":52009},{\"end\":52032,\"start\":52026},{\"end\":52044,\"start\":52038},{\"end\":52595,\"start\":52587},{\"end\":52605,\"start\":52599},{\"end\":52619,\"start\":52609},{\"end\":52629,\"start\":52623},{\"end\":53159,\"start\":53155},{\"end\":53166,\"start\":53163},{\"end\":53174,\"start\":53170},{\"end\":53183,\"start\":53178},{\"end\":53521,\"start\":53518},{\"end\":53528,\"start\":53525},{\"end\":53536,\"start\":53532},{\"end\":53543,\"start\":53540},{\"end\":53843,\"start\":53839},{\"end\":53849,\"start\":53847},{\"end\":54215,\"start\":54212},{\"end\":54223,\"start\":54219},{\"end\":54229,\"start\":54227},{\"end\":54235,\"start\":54233},{\"end\":54241,\"start\":54239},{\"end\":54249,\"start\":54245},{\"end\":54660,\"start\":54657},{\"end\":54669,\"start\":54664},{\"end\":54681,\"start\":54673},{\"end\":54687,\"start\":54685},{\"end\":54697,\"start\":54691},{\"end\":55036,\"start\":55032},{\"end\":55046,\"start\":55042},{\"end\":55056,\"start\":55050},{\"end\":55067,\"start\":55065},{\"end\":55075,\"start\":55073},{\"end\":55087,\"start\":55080},{\"end\":55563,\"start\":55553},{\"end\":55571,\"start\":55565},{\"end\":55927,\"start\":55923},{\"end\":56280,\"start\":56272},{\"end\":56294,\"start\":56284},{\"end\":56486,\"start\":56484},{\"end\":56495,\"start\":56490},{\"end\":56502,\"start\":56499},{\"end\":56509,\"start\":56506},{\"end\":56923,\"start\":56914},{\"end\":56936,\"start\":56927}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":245218819},\"end\":44734,\"start\":44295},{\"attributes\":{\"doi\":\"10.1109/TII.2019.2958826\",\"id\":\"b1\",\"matched_paper_id\":213569314},\"end\":45208,\"start\":44736},{\"attributes\":{\"doi\":\"10.1007/s11263-020-01400-4\",\"id\":\"b2\",\"matched_paper_id\":230795562},\"end\":45694,\"start\":45210},{\"attributes\":{\"doi\":\"10.1049/cim2.12047\",\"id\":\"b3\",\"matched_paper_id\":247037312},\"end\":46017,\"start\":45696},{\"attributes\":{\"doi\":\"10.1016/j.measurement.2020.108815\",\"id\":\"b4\",\"matched_paper_id\":230555990},\"end\":46356,\"start\":46019},{\"attributes\":{\"doi\":\"10.1109/wacv48630.2021.00195\",\"id\":\"b5\",\"matched_paper_id\":221370646},\"end\":46900,\"start\":46358},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":236447794},\"end\":47426,\"start\":46902},{\"attributes\":{\"doi\":\"10.1109/tie.2021.3094452\",\"id\":\"b7\",\"matched_paper_id\":237984459},\"end\":47808,\"start\":47428},{\"attributes\":{\"doi\":\"10.1007/978-3-030-68799-1_35\",\"id\":\"b8\",\"matched_paper_id\":226976039},\"end\":48260,\"start\":47810},{\"attributes\":{\"doi\":\"10.1109/cvpr46437.2021.00283\",\"id\":\"b9\",\"matched_paper_id\":233872223},\"end\":48808,\"start\":48262},{\"attributes\":{\"doi\":\"10.1007/978-3-030-69544-6_23\",\"id\":\"b10\",\"matched_paper_id\":220250825},\"end\":49219,\"start\":48810},{\"attributes\":{\"doi\":\"10.1016/j.patcog.2020.107706\",\"id\":\"b11\",\"matched_paper_id\":225114154},\"end\":49538,\"start\":49221},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2020.11.018\",\"id\":\"b12\",\"matched_paper_id\":229153008},\"end\":49858,\"start\":49540},{\"attributes\":{\"doi\":\"10.1109/ICCV48922.2021.00822\",\"id\":\"b13\",\"matched_paper_id\":237142564},\"end\":50395,\"start\":49860},{\"attributes\":{\"doi\":\"10.1109/TII.2022.3142326\",\"id\":\"b14\",\"matched_paper_id\":245946729},\"end\":50802,\"start\":50397},{\"attributes\":{\"doi\":\"10.1109/ICCV.2019.00179\",\"id\":\"b15\",\"matched_paper_id\":102353587},\"end\":51316,\"start\":50804},{\"attributes\":{\"doi\":\"10.5220/0007364503720380\",\"id\":\"b16\",\"matched_paper_id\":49567058},\"end\":51922,\"start\":51318},{\"attributes\":{\"doi\":\"10.1109/CVPR46437.2021.01466\",\"id\":\"b17\",\"matched_paper_id\":227126845},\"end\":52489,\"start\":51924},{\"attributes\":{\"doi\":\"10.1109/CVPR42600.2020.00424\",\"id\":\"b18\",\"matched_paper_id\":207880670},\"end\":53087,\"start\":52491},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":240070818},\"end\":53447,\"start\":53089},{\"attributes\":{\"doi\":\"10.1016/j.knosys.2022.108846\",\"id\":\"b20\"},\"end\":53766,\"start\":53449},{\"attributes\":{\"doi\":\"10.48550/arXiv.2201.10703\",\"id\":\"b21\",\"matched_paper_id\":246285427},\"end\":54142,\"start\":53768},{\"attributes\":{\"doi\":\"10.1109/CSCWD54268.2022.9776026\",\"id\":\"b22\",\"matched_paper_id\":248922934},\"end\":54612,\"start\":54144},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2018.2858826\",\"id\":\"b23\",\"matched_paper_id\":47252984},\"end\":54973,\"start\":54614},{\"attributes\":{\"doi\":\"10.1109/cvprw.2009.5206848\",\"id\":\"b24\",\"matched_paper_id\":57246310},\"end\":55510,\"start\":54975},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53592270},\"end\":55850,\"start\":55512},{\"attributes\":{\"doi\":\"10.1109/tpami.2020.2983686\",\"id\":\"b26\",\"matched_paper_id\":201124533},\"end\":56197,\"start\":55852},{\"attributes\":{\"id\":\"b27\"},\"end\":56434,\"start\":56199},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b28\",\"matched_paper_id\":206594692},\"end\":56884,\"start\":56436},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15276198},\"end\":57151,\"start\":56886}]", "bib_title": "[{\"end\":44373,\"start\":44295},{\"end\":44843,\"start\":44736},{\"end\":45316,\"start\":45210},{\"end\":45752,\"start\":45696},{\"end\":46082,\"start\":46019},{\"end\":46438,\"start\":46358},{\"end\":47004,\"start\":46902},{\"end\":47517,\"start\":47428},{\"end\":47895,\"start\":47810},{\"end\":48336,\"start\":48262},{\"end\":48877,\"start\":48810},{\"end\":49278,\"start\":49221},{\"end\":49610,\"start\":49540},{\"end\":49949,\"start\":49860},{\"end\":50473,\"start\":50397},{\"end\":50911,\"start\":50804},{\"end\":51410,\"start\":51318},{\"end\":51984,\"start\":51924},{\"end\":52583,\"start\":52491},{\"end\":53151,\"start\":53089},{\"end\":53835,\"start\":53768},{\"end\":54208,\"start\":54144},{\"end\":54651,\"start\":54614},{\"end\":55026,\"start\":54975},{\"end\":55549,\"start\":55512},{\"end\":55919,\"start\":55852},{\"end\":56480,\"start\":56436},{\"end\":56908,\"start\":56886}]", "bib_author": "[{\"end\":44387,\"start\":44375},{\"end\":44401,\"start\":44387},{\"end\":44853,\"start\":44845},{\"end\":44861,\"start\":44853},{\"end\":44867,\"start\":44861},{\"end\":44873,\"start\":44867},{\"end\":44880,\"start\":44873},{\"end\":44888,\"start\":44880},{\"end\":45330,\"start\":45318},{\"end\":45341,\"start\":45330},{\"end\":45351,\"start\":45341},{\"end\":45365,\"start\":45351},{\"end\":45375,\"start\":45365},{\"end\":45761,\"start\":45754},{\"end\":45769,\"start\":45761},{\"end\":45776,\"start\":45769},{\"end\":45786,\"start\":45776},{\"end\":46095,\"start\":46084},{\"end\":46451,\"start\":46440},{\"end\":46460,\"start\":46451},{\"end\":46473,\"start\":46460},{\"end\":47019,\"start\":47006},{\"end\":47031,\"start\":47019},{\"end\":47041,\"start\":47031},{\"end\":47526,\"start\":47519},{\"end\":47533,\"start\":47526},{\"end\":47539,\"start\":47533},{\"end\":47546,\"start\":47539},{\"end\":47907,\"start\":47897},{\"end\":47917,\"start\":47907},{\"end\":47927,\"start\":47917},{\"end\":47939,\"start\":47927},{\"end\":48347,\"start\":48338},{\"end\":48356,\"start\":48347},{\"end\":48367,\"start\":48356},{\"end\":48377,\"start\":48367},{\"end\":48885,\"start\":48879},{\"end\":48893,\"start\":48885},{\"end\":49293,\"start\":49280},{\"end\":49304,\"start\":49293},{\"end\":49314,\"start\":49304},{\"end\":49619,\"start\":49612},{\"end\":49627,\"start\":49619},{\"end\":49633,\"start\":49627},{\"end\":49964,\"start\":49951},{\"end\":49975,\"start\":49964},{\"end\":49985,\"start\":49975},{\"end\":50482,\"start\":50475},{\"end\":50493,\"start\":50482},{\"end\":50499,\"start\":50493},{\"end\":50506,\"start\":50499},{\"end\":50512,\"start\":50506},{\"end\":50520,\"start\":50512},{\"end\":50921,\"start\":50913},{\"end\":51424,\"start\":51412},{\"end\":51432,\"start\":51424},{\"end\":51442,\"start\":51432},{\"end\":51456,\"start\":51442},{\"end\":51466,\"start\":51456},{\"end\":51996,\"start\":51986},{\"end\":52007,\"start\":51996},{\"end\":52022,\"start\":52007},{\"end\":52034,\"start\":52022},{\"end\":52046,\"start\":52034},{\"end\":52597,\"start\":52585},{\"end\":52607,\"start\":52597},{\"end\":52621,\"start\":52607},{\"end\":52631,\"start\":52621},{\"end\":53161,\"start\":53153},{\"end\":53168,\"start\":53161},{\"end\":53176,\"start\":53168},{\"end\":53185,\"start\":53176},{\"end\":53523,\"start\":53516},{\"end\":53530,\"start\":53523},{\"end\":53538,\"start\":53530},{\"end\":53545,\"start\":53538},{\"end\":53845,\"start\":53837},{\"end\":53851,\"start\":53845},{\"end\":54217,\"start\":54210},{\"end\":54225,\"start\":54217},{\"end\":54231,\"start\":54225},{\"end\":54237,\"start\":54231},{\"end\":54243,\"start\":54237},{\"end\":54251,\"start\":54243},{\"end\":54662,\"start\":54653},{\"end\":54671,\"start\":54662},{\"end\":54683,\"start\":54671},{\"end\":54689,\"start\":54683},{\"end\":54699,\"start\":54689},{\"end\":55038,\"start\":55028},{\"end\":55048,\"start\":55038},{\"end\":55058,\"start\":55048},{\"end\":55069,\"start\":55058},{\"end\":55077,\"start\":55069},{\"end\":55089,\"start\":55077},{\"end\":55565,\"start\":55551},{\"end\":55573,\"start\":55565},{\"end\":55929,\"start\":55921},{\"end\":56282,\"start\":56270},{\"end\":56296,\"start\":56282},{\"end\":56488,\"start\":56482},{\"end\":56497,\"start\":56488},{\"end\":56504,\"start\":56497},{\"end\":56511,\"start\":56504},{\"end\":56925,\"start\":56910},{\"end\":56938,\"start\":56925}]", "bib_venue": "[{\"end\":46640,\"start\":46579},{\"end\":47180,\"start\":47119},{\"end\":48546,\"start\":48484},{\"end\":49016,\"start\":48977},{\"end\":50134,\"start\":50082},{\"end\":51065,\"start\":51013},{\"end\":52215,\"start\":52153},{\"end\":52800,\"start\":52738},{\"end\":53276,\"start\":53239},{\"end\":55256,\"start\":55194},{\"end\":55702,\"start\":55646},{\"end\":56672,\"start\":56610},{\"end\":57029,\"start\":56992},{\"end\":44505,\"start\":44401},{\"end\":44940,\"start\":44912},{\"end\":45420,\"start\":45401},{\"end\":45829,\"start\":45804},{\"end\":46154,\"start\":46128},{\"end\":46577,\"start\":46501},{\"end\":47117,\"start\":47041},{\"end\":47595,\"start\":47570},{\"end\":48014,\"start\":47967},{\"end\":48482,\"start\":48405},{\"end\":48975,\"start\":48921},{\"end\":49358,\"start\":49342},{\"end\":49675,\"start\":49661},{\"end\":50080,\"start\":50013},{\"end\":50572,\"start\":50544},{\"end\":51011,\"start\":50944},{\"end\":51594,\"start\":51490},{\"end\":52151,\"start\":52074},{\"end\":52736,\"start\":52659},{\"end\":53237,\"start\":53185},{\"end\":53514,\"start\":53449},{\"end\":53934,\"start\":53876},{\"end\":54355,\"start\":54282},{\"end\":54763,\"start\":54725},{\"end\":55192,\"start\":55115},{\"end\":55644,\"start\":55573},{\"end\":55993,\"start\":55955},{\"end\":56268,\"start\":56199},{\"end\":56608,\"start\":56531},{\"end\":56990,\"start\":56938}]"}}}, "year": 2023, "month": 12, "day": 17}