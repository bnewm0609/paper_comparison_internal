{"id": 255658077, "updated": "2023-08-18 13:52:49.474", "metadata": {"title": "A Self-Supervised Learning-Based Channel Estimation for IRS-Aided Communication Without Ground Truth", "authors": "[{\"first\":\"Zhengming\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Taotao\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Haoqing\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Chunguo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yongming\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Luxi\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "IEEE Transactions on Wireless Communications", "journal": "IEEE Transactions on Wireless Communications", "publication_date": {"year": 2023, "month": 8, "day": 1}, "abstract": "Deep learning (DL) is an emerging paradigm for accurate channel estimation for intelligent reflecting surface (IRS)-aided wireless communication systems. It has been proven to be a promising way to achieve better channel estimation performance for the IRS-aided wireless communication system than traditional methods (e.g., least-square algorithm). However, existing DL-based methods rely on ground truth (labels of the true channels) which is difficult to obtain in real networks. In this paper, we propose a self-supervised learning (SSL) method for the IRS channel estimation problem. No ground truth channel is needed in the training, while a simple and novel self-supervised denoising formula without a clean reference signal is presented. Particularly, in the training phase, the self-supervised signal and the input are the received signal vector and its noisy version, respectively. While in the inference phase the input is the estimated channel by using the least-square method and the output is the refined channel estimation. That is, our neural network-based channel estimation algorithm is not reciprocal for training and testing. We demonstrate that the proposed SSL solution has good convergence performance and generalization ability through numerical simulations. Interestingly, we find a \u201cdouble descent\u201d phenomenon in the learning curve during the test phase, i.e., when we gradually increase the number of training epochs, the performance first gets better, then becomes worse, and further gets better again. Besides, we propose to analyze SSL using the loss landscape and centered kernel alignment method. The results show that the self-supervised model has a similar loss landscape and representational similarity to the supervised model. We explored the effects of different signal-to-noise ratios (SNRs), different neural network sizes, and different training data volumes on our algorithm through numerical simulations. Extensive numerical simulation results show that our SSL algorithm is still competitive without ground truth. We also show that the developed scheme exhibits robustness to SNR ratio mismatch.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/twc/ZhangJSLHY23", "doi": "10.1109/twc.2023.3233970"}}, "content": {"source": {"pdf_hash": "e6dcf9e72e90426d2abed34ad39fd0daa32f5bec", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f5ecc9d8a462191c4e56d6c71c616bd3ae3ffd5c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e6dcf9e72e90426d2abed34ad39fd0daa32f5bec.txt", "contents": "\nA Self-Supervised Learning-Based Channel Estimation for IRS-Aided Communication Without Ground Truth\nAUGUST 2023\n\nZhengming Zhang zmzhang@seu.edu.cn \nGraduate Student Member, IEEETaotao Ji jitaotao@seu.edu.cn \nHaoqing Shi shihaoqing619@seu.edu.cn \nSenior Member, IEEEChunguo Li chunguoli@seu.edu.cn \nSenior Member, IEEEYongming Huang huangym@seu.edu.cn \nSenior Member, IEEELuxi Yang lxyang@seu.edu.cn. \n\nFrontiers Science Center for Mobile Information Com-munication and Security\nSchool of Information Science and Engi-neering\nNational Mobile Communications Research Laboratory\nSoutheast University\n210096NanjingChina\n\n\nPervasive Communications Center\nPurple Mountain Laboratories\n211111NanjingChina\n\nA Self-Supervised Learning-Based Channel Estimation for IRS-Aided Communication Without Ground Truth\n\nIEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS\n228AUGUST 202310.1109/TWC.2023.3233970received 23 June 2022; revised 20 October 2022 and 5 December 2022; accepted 22 December 2022. Date of publication 10 January 2023; date of current version 14 August 2023.5446 The associate editor coordinating the review of this article and approving it for publication was B. Chalise. (Corresponding author: Luxi Yang.) The authors are with the Color versions of one or more figures in this article are available at\nDeep learning (DL) is an emerging paradigm for accurate channel estimation for intelligent reflecting surface (IRS)-aided wireless communication systems. It has been proven to be a promising way to achieve better channel estimation performance for the IRS-aided wireless communication system than traditional methods (e.g., least-square algorithm). However, existing DL-based methods rely on ground truth (labels of the true channels) which is difficult to obtain in real networks. In this paper, we propose a self-supervised learning (SSL) method for the IRS channel estimation problem. No ground truth channel is needed in the training, while a simple and novel self-supervised denoising formula without a clean reference signal is presented. Particularly, in the training phase, the self-supervised signal and the input are the received signal vector and its noisy version, respectively. While in the inference phase the input is the estimated channel by using the least-square method and the output is the refined channel estimation. That is, our neural network-based channel estimation algorithm is not reciprocal for training and testing. We demonstrate that the proposed SSL solution has good convergence performance and generalization ability through numerical simulations. Interestingly, we find a \"double descent\" phenomenon in the learning curve during the test phase, i.e., when we gradually increase the number of training epochs, the performance first gets better, then becomes worse, and further gets better again. Besides, we propose to analyze SSL using the loss landscape and centered kernel alignment method. The results show that the self-supervised model has a similar loss landscape and representational similarity to the supervised model. We explored the effects of different signal-tonoise ratios (SNRs), different neural network sizes, and different training data volumes on our algorithm through numerical Manuscript simulations. Extensive numerical simulation results show that our SSL algorithm is still competitive without ground truth. We also show that the developed scheme exhibits robustness to SNR ratio mismatch.Index Terms-Channel estimation, self-supervised learning, IRS, deep neural network, double descent. 1536-1276 in 2016. He is currently pursuing the Ph.D. degree in information and communication engineering with the School of Information Science and Engineering, Southeast University. His current research interests include wireless big data, machine learning, 5G mobile networks, IRS-aided communication, and resource management.Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nI. INTRODUCTION\n\nI NTELLIGENT reflecting surface (IRS) [1], [2], [3], [4] is one of the promising techniques for the beyond fifthgeneration (B5G) networks. It can enhance the coverage and capacity of wireless communication systems with low hardware costs and energy consumption. Usually, an IRS is composed of numerous passive reflection elements, which can obtain customized reflection signals by adjusting the electromagnetic properties such as the phase of the incident electromagnetic signal. Thus, IRS can act as an intelligent relay to increase the flexibility of the wireless network. Compared with relay-based wireless communication technology, IRS does not need to consume huge energy and does not introduce additional noise. A more detailed comparison of IRS and other communication techniques can refer to [2]. Because of the excellent properties of IRS, it has attracted a lot of research interest [5] and has been proven a promising way to enhance various wireless communication networks, e.g., non-orthogonal multiple access (NOMA) systems [6], unmanned aerial vehicles-based wireless communications [7], etc. To realize a high-performance IRS communication system, high-precision channel estimation is essential.\n\nThe related work [8] points out that only when perfect channel state information (CSI) is obtained, can IRS fully exert its excellent characteristics. Actually, according to [9], the acquisition of perfect CSI is quite challenging in practice. With precise CSI, advanced methods, such as the machine learning method proposed in [10], can perform well optimizing transmit and reflecting beamforming in an IRS-assisted wireless system. Different from traditional wireless systems, IRS as a kind of passive component cannot send, receive and process pilot signals, making the IRS-involved channel difficult to estimate. Thus, directly estimating the cascaded user-IRS-base station channel estimation is a typical way [11]. In addition, compared to the traditional systems, there are often a large number of passive components in IRS systems, and the dimension of the cascaded channels is large, which results in a large pilot overhead. To address these obstacles, efficient channel estimations are studied by many related works. For example, a leastsquare (LS)-based estimation method is used in [12] and [13]. In [12], the LS estimator is aimed at obtaining the unbiased cascaded channel for single-user multiple-input single-output (SU-MISO) systems. Binary reflection-controlled least-square channel estimation is proposed in [13] for IRS-aided energy transfer from a multi-antenna power beacon to a single-antenna user. This new LS-based channel estimation protocol suggests that IRS can only switch on one reflecting element and switch off the remaining reflecting elements at each time slot. The minimum mean squared error (MMSE)-based estimator is another solution to the IRS channel estimation problem. In [14], channel estimation in a coherence block is divided into two sub-stages. By turning off the entire IRS in the first sub-stage, the base station estimates the channel between itself and the user. Following this, the passive reflection elements of the IRS are turned on in turn to allow the base station to perform the estimation. MMSE method is used after estimation results of all sub-phases are taken together. The literature [14] indicates that the MMSE estimates are both analytically and numerically shown to achieve a much lower estimation error than the conventional LS estimates. As presented in [15], the performance of the available LS-based estimators still has a large gap compared with the optimal Bayesian MMSE estimator. While the optimal MMSE estimator needs to know the probability distribution of the channel (which is difficult to achieve in practice) and involves a multidimensional integration that has high complexity. The optimal MMSE estimator is also difficult to achieve in the real IRS network. Sparse matrix factorization, ambiguity elimination, and matrix completion are used in [16] to achieve a three-stage IRS channel estimation mechanism. However, a lot of iterative steps need to be performed to achieve the optimization of each stage. This limits its efficient execution in real networks. Thankfully, the development of deep learning (DL) provides support for low-complexity algorithmic inference and it promises to be a good tradeoff between channel estimation performance and complexity.\n\nDifferent from traditional methods, DL can learn the mapping from the input data to the desired output and get at least a locally optimal solution. The main reasons why it attracts a large number of researchers carrying out cutting-edge research are: 1) The neural network used in deep learning only uses simple matrix multiplication in the inference process, and does not require complex iterative optimization. 2) Deep learning has the generalization ability, that is, a trained neural network can perform well on samples that have never been seen before. Regarding the channel estimation problem as a supervised learning problem, several DL-based methods [17], [18], [19] have shown their remarkable performance in the IRS channel estimation task. The recent work [20] which proposes a model-driven deep unfolding scheme also shows that DL can achieve low-complexity online inference with a trained model.\n\nRelated works [17], [18] explore the combination of wireless communication models and deep learning to achieve channel estimation for IRS networks. In [17], the traditional multi-stage channel estimation method is used to construct the dataset to train neural networks. In [18], the compressed sensing technique is used to improve estimation accuracy. Both of the above research adopted a convolutional neural network (CNN) to solve the channel estimation problem for IRS-assisted communication systems. However, the method proposed in [17] requires turning on individual IRS elements in a sequence, which needs to deploy a hybrid passive/active IRS, thus may incur additional training overhead or hardware costs. Different from them, an end-to-end supervised learning method is proposed in [15], where a convolutional deep residual network (CDRN) is proposed to reconstruct the channel from the noisy pilot-based observations. It improves the channel estimation accuracy without the requirements of additional system deployment. Similarly, [19] proposes to use residual dense network structure to obtain better channel estimation generalization and fitting capabilities by leveraging the low-rank structure of IRS channels. In [21], enhanced residual neural networks, which leverage the rank-deficient structure of RIS channels, are proposed to obtain accurate channel state information. In [22], data-driven nonlinear solutions based on deep learning are proposed, which are two convolutional neural network-based methods to perform denoising and approximate optimal MMSE channel estimation solutions. Although these studies have shown the performance superiority of deep learning as a solution for channel estimation, these schemes all use supervised learning to complete the training of neural networks. A natural question is how the labels needed for supervised learning are obtained in practice. Obviously, if the labels are obtained by traditional channel estimation algorithms (e.g., LS estimator or multi-stage channel estimation proposed in [14]), the performance of supervised learning methods will be limited by the performance of traditional algorithms. And end-to-end training of the neural network, like the method used in [15], needs to use the ground truth channel label to calculate the loss function to update the neural network. However, it is difficult to obtain the ground truth channel in the real world. Thus, an effective and practical method that takes advantage of the good properties (generalization ability, competitive online inference ability) of DL without relying on ground truth is expected.\n\nThe motivation of our work is to propose a DL method that does not require ground-truth labels to efficiently solve the IRS channel estimation problem. In this work, we propose a self-supervised learning algorithm 1 to achieve the above goal. Self-supervised learning (SSL) can be considered a kind of unsupervised learning which learns from unlabeled samples [23]. Usually, unsupervised learning works towards clustering and dimensionality reduction, whereas SSL performs conclusive tasks like classification and regression like any supervised model. The main idea of SSL is to self-generate labeled data by defining a pretext task that guides us to a supervised loss function and empowers us to exploit a variety 1 The code to reproduce this algorithm is open-sourced: https://github.com/jhcknzzm/SSL-Based-IRS-Channel-Estimation of labels that come with the data for free. In this context, our main contributions are the following:\n\n1. We propose a novel SSL method for IRS-aided wireless network channel estimation. Specifically, first, a new neural network architecture is designed, which is inspired by existing research on residual neural networks for channel estimation. Through empirical experiments, we demonstrate that this neural network outperforms existing neural networks CDRN [15] on the considered task. Then, we design a simple denoising pretext task to train this neural network. This pretext task lets the neural network reconstruct the input from a noisy version of the received signal. It is worth emphasizing that we do not assume that the perfect channel state information or the received signal not polluted by wireless transmission noise can be obtained as training labels. Besides, our solution is an unconventional learning method where training and testing are not reciprocal, i.e., in the test phase, the input is different from that in the training phase. Specifically, the input of the neural network in the test phase is the imprecise channel estimation calculated by the LS while the input of the training is the received signal.\n\n2. An interesting double descent phenomenon is observed by using the proposed channel estimation where training and testing are not reciprocal. Specifically, we observe \"epochwise double descent\" when we increase the training time, with test performance following a classical U-like curve in the underfitting stage (the same as the fundamental biasvariance trade-off concept) and then improving with training time to be sufficiently larger (contrary to traditional statistical learning theory but in line with advanced double descent theory). This phenomenon, which has not been observed in channel estimation work before, implies that we can train neural networks without early stopping [24] to achieve good performance.\n\n3. We analyze the similarity of SSL-trained models and supervised learning-trained models through loss landscape and centered kernel alignment (CKA). The loss landscape results show that the self-supervised trained model has similar generalization performance to supervised learning can be exhibited, and the CKA results show that the feature representation by self-supervised learning is also similar to supervised trained models. The extensive empirical evaluations show the proposed self-supervised solution has competitive performance. Besides, we show that the developed scheme exhibits robustness to signal-to-noise ratio (SNR) mismatch, i.e., our algorithm is robust when there is a certain difference in the distribution of test and training data.\n\nThe novelty of our work lies in the realization of channel estimation without ground-truth labels by establishing a simple and effective self-supervised learning scheme. Although its performance cannot exceed supervised learning, it offers the possibility to apply the good generalization ability of DL in real-world situations where labels are not available.\n\nThe rest of this paper is organized as follows. In Section II, we introduce our considered system model and the cascaded channel estimation problem of the intelligent reflecting surface-aided wireless communication network. In Section III, we propose a self-supervised learning method. In this section, the loss landscape and centered kernel alignment for analyzing the similarity between models are also introduced. Finally, numerical results and discussions are presented in Section IV, and a conclusion is drawn in Section V.\n\n\nII. SYSTEM MODEL AND CHANNEL ESTIMATION\n\nIn this paper, as shown in Fig. 1, we consider an IRSaided multi-user communication system, where an IRS consisting of N programmable reflecting elements is deployed to assist in the communications from a BS equipped with M antennas to K single-antenna users over a given frequency band. We assume that a time division duplex (TDD) protocol is adopted in the system. By jointly optimizing the active beamforming at the BS and passive beamforming at the IRS, the total throughput of the investigated system can be greatly enhanced, which, however, critically depends on the availability of accurate involved CSI. In practice, the downlink CSI could be acquired by the estimation of the uplink CSI due to the channel reciprocity property in the TDD system. Denote the set of users as K = {1, 2, \u00b7 \u00b7 \u00b7 , K}. The baseband equivalent channels of the user k-IRS link, the IRS-BS link, and the direct user k-BS link are respectively denoted as h k,r \u2208 C N \u00d71 , G \u2208 C M\u00d7N , and h k,d \u2208 C M\u00d71 . We define the diagonal matrix \u0398 \u0394 = diag (\u03b8) \u2208 C N \u00d7N as the reflection-coefficients matrix of the IRS with \u03b8 = \u03b2 1 e j\u03d51 , \u03b2 2 e j\u03d52 , \u00b7 \u00b7 \u00b7 , \u03b2 N e j\u03d5N T , where 0 \u2264 \u03b2 n \u2264 1 and 0 \u2264 \u03d5 n < 2\u03c0 are the reflection amplitude and phase shift of the n-th element, respectively. The cascaded user k-IRS-BS channel can be expressed as G\u0398h k,r \u2208 C M\u00d71 . By applying the change of variables, the cascaded user k-IRS-BS channel can be reformulated as Gdiag (h k,r ) \u03b8. Accordingly, the effective channel from user k to the BS can be rep-\nresented ash k,d = h k,d + Gdiag (h k,r ) \u03b8 = H k\u03b8 , where H k = [Gdiag (h k,r ) , h k,d ] , \u2200k \u2208 K, and\u03b8 = \u03b8 T , 1 T .\nTherefore, estimating all the involved channels is equivalent to estimating the matrix H k , \u2200k \u2208 K. Next, we design an uplink pilot transmission scheme for channel estimation in the considered IRS-aided multi-user communication system. Specifically, in the channel estimation phase, the IRS generates P different reflection patterns via P different phase shift matrices with P \u2265 N + 1, which are denoted as \u03a6 = [\u0398 1 , \u0398 2 , \u00b7 \u00b7 \u00b7 , \u0398 P ] T , where \u0398 p is the p-th phase shift matrix for p \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , P }. For user k, a pilot sequence u k = [u k,1 , u k,2 , \u00b7 \u00b7 \u00b7 , u k,L ] T with a length L, where L \u2265 K is adopted for each IRS reflection pattern. It is worth noting that each two pilot sequences should be orthogonal for ease of distinguishing different users, i.e., u H m u n = 0, Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \u2200m = n, and u H k u k = P t L, \u2200k \u2208 K, where P t is the power of each pilot symbol. Therefore, the l-th (1 \u2264 l \u2264 L) received pilot signal vectors during the p-th sub-frame at the BS is given by\ns p,l = K k=1 (Gdiag (\u03b8 p ) h k,r + h k,d ) u p,l + n p,l = K k=1 H k\u03b8p u p,l + n p,l(1)\nwhere n p,l \u2208 C M\u00d71 is the l-th sampling noise vector at the BS in the p-th sub-frame with n p,l \u223c CN 0, \u03c3 2 n I , where \u03c3 2 n is the noise power of each antenna at the BS. Here, H k , \u2200k \u2208 K is the channel matrix needed to be estimated. By stacking the L received pilot signal vectors at the BS in the p-th sub-frame into a matrix form, we have\nS p = K k=1 H k\u03b8p u H k + N p ,(2)\nwhere S p = [s p,1 , s p,2 , \u00b7 \u00b7 \u00b7 , s p,L ] \u2208 C M\u00d7L , and N p = [n p,1 , n p,2 , \u00b7 \u00b7 \u00b7 , n p,L ] \u2208 C M\u00d7L . Due to the orthogonal property of the pilot sequences of different users, we can separate the received signal from the k-th user by multiplying u k with\nS p in (2), i.e., r p,k = H k\u03b8p + v p,k , where r p,k = 1 PtL S p u k \u2208 C M\u00d71 is the received signal vector at the BS from the k- th user in the p-th sub-frame. v p,k = 1 PtL N p u k \u2208 C M\u00d71 with v p,k \u223c CN 0, \u03c3 2 v I M , where \u03c3 2 v\nis the variance of each element of v p,k . Therefore, after receiving P sub-frames at the BS, we have\nR k = H k P + V k ,(3)\nwhere P = \u03b8 1 ,\u03b8 2 , \u00b7 \u00b7 \u00b7 ,\u03b8 P \u2208 C (N +1)\u00d7P , and V k = [v 1,k , v 2,k , \u00b7 \u00b7 \u00b7 v P,k ] \u2208 C M\u00d7P . According to [15], an optimal scheme of P in terms of improving received signal power at the BS can be designed as a discrete Fourier transform (DFT) form, i.e.,\nP = \u23a1 \u23a2 \u23a2 \u23a2 \u23a3 1 1 \u00b7 \u00b7 \u00b7 1 1 W P \u00b7 \u00b7 \u00b7 W P \u22121 P . . . . . . . . . . . . 1 W N P \u00b7 \u00b7 \u00b7 W N (P \u22121) P \u23a4 \u23a5 \u23a5 \u23a5 \u23a6 \u2208 C (N +1)\u00d7P ,(4)\nwhere W P = e j2\u03c0/P . Typically, with known R k , the channel estimation problem can be formulated as\nmin Fw E H k \u2212 F w (R k ) 2 F ,(5)\nwhere F w in (5) is a channel estimation method, e.g., the LS method. To estimate H k , the LS method is a practical technique if no prior channel knowledge is available. By applying the LS method, we have\u0124 LS k = R k P \u2020 , where\u0124 LS k denotes the estimated H k by adopting the LS estimator, and P \u2020 = P H PP H \u22121 is the pseudoinverse of P. However, the LS method treats the channel as a deterministic but unknown constant, which does not exploit the spatial features of channels compared with the data-driven DL-based approaches. Based on this, we will propose a self-supervised learning solution in the next section. \n\n\nIII. OUR SELF-SUPERVISED LEARNING SOLUTION\n\nIn this section, first, we introduce the neural network structure used in this paper. Then, we introduce the self-supervised loss used to train the designed neural network for IRS channel estimation. Moreover, we give an introduction to loss landscape and CKA for empirical analysis of supervised trained models versus our self-supervised models.\n\n\nA. Deep Neural Network Structure\n\nThere are many excellent neural network structures, e.g., DnCNN [25], [26], CDRN [15] and complex-valued DnCNN [18], for channel estimation. All of them use CNN structures which are widely used in (RGB) image processing. This is based on a natural intuition that the real and imaginary parts of a frequency-domain channel are similar to different color channels of an RGB image. That is, the real part and the imaginary part of the complex channel are usually separated and stacked into a picture for subsequent CNN processing. However, it is noted that when CNN processes different channels of input, there is no information exchange between channels, that is, a single channel cannot obtain the complete information of the entire input. We propose to fuse the real and imaginary information of the original channel in the middle layer of the neural network. The detailed neural network structure is shown in Fig. 2. The initiation behind this structure is that the skip connection, which divides the complete neural network into 2 sub-networks, could provide the real and imaginary parts of the original wireless channel for the hidden layers of the neural network to increase information flow within the network, and could alleviate the gradient vanishing problem of deep neural networks like ResNet [27].\n\nAs shown in Fig. 2, the neural network consists of 2 subnetwork modules, f 1 (\u00b7, w f 1 ) and f 2 (\u00b7, w f 2 ), each of which is followed by a residual module, g 1 (\u00b7, w g 1 ) and g 2 (\u00b7, w g 2 ), respectively. Each sub-network module contains N L convolutional layers, and each convolutional layer uses ReLu [28] activation function and a batch normalization (BN) [29]. The residual module uses the tanh activation function without BN. Similar to [15], the complex matrix A is decomposed into 2 channels with real and imaginary parts as input, i.e., X = [Re{A}, Im{A}], where A is the raw input of the channel estimation scheme. In our work, A is the received signal R in the training phase, and it is the LS channel estimation in the test phase. In order for the network to utilize the global information of the raw complex input matrix A to a great extent, we perform a linear transformation of the channel with the matrix U to exchange the real and imaginary parts of the input X, i.e., XU = [Im{A}, Re{A}], and average the result with the output of the first residual module, i.e.,\nz = g 1 f 1 (X, w f 1 ), w g 1 /2 + XU/2.(6)\nThe output of the second residual module is subtracted from the original input to obtain the final output, i.e., y =\nX \u2212 g 2 f 2 (z, w f 2 ), w g 2 ,\nwhere element-wise subtraction is adopted to exploit the additive nature of the noise for denoising the noisy input. The scale of the above neural network is elastic, a shallow or deep neural network can be obtained by setting a small N L or a large N L . It is worth mentioning that this kind of neural network only uses convolutional layers and no fully connected layers, since the convolutional layers have few parameters, the storage overhead of the entire network is small.\n\n\nB. Self-Supervised Learning\n\nTraining neural network regressors for the channel estimation problem follows the form of the typical training task for a set of input-target pairs (X, H):\narg min w E [L(F (X, w), H)] ,(7)\nwhere the network function F (X, w)\n= X \u2212 g 2 f 2 (z, w f 2 ), w g 2\nis parameterized by w, z is calculated by (6) and L(\u00b7) is a loss function, e.g., MSE. Equation (7) is exactly the loss function used by existing research for training denoising neural networks, where (X, H) are considered as random variables. However, it is a kind of noise-to-clean supervised learning strategy that requires ground-truth labels H for training. According to the joint probability formula, the above objective function can be transformed into arg min\nw E X E H|X [L(F (X, w), H)]\n. This means that the network F (X, w) can minimize this loss by solving the point estimation problem separately for each input sample and get the same result as the original problem (7). If we replace the label with random variables whose expect matches the target label, the estimate will remain the same [30], i.e., if both the inputs and the targets are now drawn from a corrupted distribution and E H| X = H, then given infinite data, the solution of the following problem (8) is the same as that of (7), i.e., arg min\nw lim N \u2192\u221e 1 N N i=1 L(F ( X i , w), y i ) ,(8)\nwhere X i and H i is the i-th samples sampled from the corrupted distribution. This motivates us to propose the following self-supervised learning method for channel estimation. Specifically, the training dataset required to train the neural network consists of the received signal data and its noisy version, i.e., D =\n{(X i , X i + \u03b5 i )} NX i=1 , where \u03b5 i \u223c N (0, \u03c3 2 \u03b5 I).\nThe pretext task for the self-supervised loss function is denoising for the noisy version of the received signal X i + \u03b5 i , i.e.,\narg min w E Xi,\u03b5i [L(F (X i + \u03b5 i , w), X i )] .(9)\nWe emphasize that the above optimization problem does not require obtaining ground-truth labels, which is easy to implement in practice. The purpose of adding noise to already noisy X i is to generate a self-supervised training task to obtain an estimator which implicitly uses channel statistics (shown in Theorem 1). In addition, since noise can be generated indefinitely by sampling from a pre-defined distribution, training samples are sufficient. Note that our goal is to recover the channel (while the goal of the training phase is to recover the received signal), thus, a natural way to achieve this with a trained network is to use the coarse estimation result of the LS estimator as the input in the online estimation phase, i.e., the test phase, where the LS estimation is\nH LS = RP H (PP H ) \u22121 .\nFramework for the self-supervised learning: Here we detail the framework of our self-supervised approach. The framework is divided into two phases, i.e., training and testing. In the training phase, the input is the received signal with extra noise, and the label is the corresponding original received signal. The training is performed using the following Algorithm 1. Specifically, during the training phase, we use stochastic gradient descent (SGD) to update the neural network with a learning rate \u03b7. The training performs E epochs (traversals of the training dataset) of SGD with a mini-batch size B, the iterations of each epoch is I = N X /B , where N X is the number of training data and \u00b7 is the floor function. Usually, the expected quadratic loss is used as the loss function in (11), e.g., the normalized MSE: (10) where \u00b7 F is the Frobenius norm. Note that each time a batch of data is taken during training, fresh noise \u03b5 is generated to perturb X to obtain the input of the neural network. In the testing phase, the parameters of the trained neural network model will no longer be updated, the input of the model is the estimation result of LS, that is, RP H PP H \u22121 , and the output of the neural network is regarded as the final channel estimation result.\nL(X, X + \u03b5, w) = F (X + \u03b5, w) \u2212 X 2 F / X 2 F ,\nBesides, the sketch of the whole training and test phase is shown in Fig. 3. It can be clearly seen that the training and testing are not reciprocal, both their input types and output types are different. In Fig. 3, the blue line represents the training pipeline, where the input is the received signal with additional noise perturbation, and the output is the deep neural network's estimation of the original received signal. The red line represents the test pipeline, where the input of this pipeline is the rough estimation structure of the channel by the LS estimator, and the output is the estimation of the IRS channel by the deep neural network. It is worth noting that our proposed learning strategy is different from existing works [15], [25] that use the same kind of input for training and testing.\n\nIn Algorithm 1, Gaussian noise is used to contaminate the received signal, because it is easier to handle mathematically and can serve as a good frame of reference. Actually, Algorithm 1 can obtain the explicit solution of the optimal Randomly selects a batch of data D B i=1 \u2286 D X , and use the following equations with SGD to update the neural network model F (\u00b7, w i ):\n\u23a7 \u23a8 \u23a9 w t+1 \u2190 w t \u2212 \u03b7\u2207 w L(w), L(w) = 1 B B i=1 L (F (X i + \u03b5 i , w t ) , X i ),(11)\nwhere X i \u2208 D B i=1 and \u03b5 i \u223c N (0, \u03c3 2 \u03b5 I). 4: end for 5: end for Ensure: The trained model F (\u00b7, w).\n\ndenoising learning problem (9) for additive Gaussian noise. We show this by the following Theorem 1.\n\nTheorem 1: In the considered IRS channel estimation problem, assume the covariance matrix of H is \u03a3 H . Then the optimal solution of the denoising neural network is:\nF (y, w * ) = I \u2212 \u03c3 2 \u03b5 P T \u03a3 H P + \u03c3 2 n I + \u03c3 2 \u03b5 I \u22121 y. (12)\nWhen the optimal model F (\u00b7, w * ) is used to estimate the channel in the test phase, the estimation error is bounded by:\nE H \u2212 H 2 F \u2264 4\u03c3 2 v 1 + \u03c3 4 \u03b5 \u03be 1 \u03be 2 + 2\u03c3 4 \u03b5 \u03be 1 T r (R HH ) ,(13)\nwhere Proof: Here we briefly introduce the sketch of the proof, please refer to the detailed proof in Appendix B. The main idea is to first use Tweedie's formula [31] to obtain the optimal posterior expectation of X with given X+\u03b5, then convolve the distribution of the received signal with the additionally added noise distribution, and finally obtain (12) as above.\n\u03be 1 = P T \u03a3 H P + \u03c3 2 v I + \u03c3 2 \u03b5 I \u22121 2 F , R HH is\nThe above Theorem 1 gives the following insights: 1. The optimal denoising model obtained by Algorithm 1 is a linear model F (y, w * ), that is, F (yP, w * ) = w * * y * p = F (y, w * ) * P . It can be seen that the noisy received signal R + \u03b5 is used to obtain F , and F is used to denoise R + \u03b5. However, we only need to perform a linear transformation on R to obtain RP H PP H \u22121 , and input it into F , then the output of F is to perform LS estimation after denoising the received signal. 2. Even if we do not directly estimate the channel during training, the estimation error of the channel in the testing phase is still bounded and the lower bound is related to the noise intensity \u03c3 2 \u03b5 . Although the input in the test phase is different from that in the training phase, in practice to ensure performance, we can adjust \u03c3 2 \u03b5 and observe the performance of the wireless transmission system (such as the user rate) to obtain the best \u03c3 2 \u03b5 for channel estimation (the empirical results are presented in Subsection C of Section IV). However, we emphasize that it is difficult to directly calculate the weight w * of the optimal linear model because it is necessary to know the covariance matrix of the channel and the noise power in the wireless environment, and fine-tuning \u03c3 2 \u03b5 also requires extra care. Design schemes to obtain the optimal linear model and optimal \u03c3 2 \u03b5 are beyond the scope of this paper, and they are issues worthy of further study in the future. Our work focuses on how to give a baseline for channel estimation of the IRS system that does not require ground truth.\n\n\nC. Similarity Analysis With Models Trained by Supervised Learning\n\nIn this subsection, we introduce the tools to compare and analyze quantities of interest for the SSL model and supervised learning (SL) model, namely, the loss-landscape and centered kernel alignment (CKA). The former is used to characterize the similarity in a global loss of the models obtained by the two training methods, and the latter focuses on the similarity with their representations for the same input.\n\nThe parametric loss-landscape is plotted by perturbing the neural network model parameters, w, along with the first eigenvector of the Hessian denoted as v. Then, we compute the loss of B data points with the following formula:\nL (w, \u03bb) = L(w + \u03bbv) = 1 B B i=1 L F R i P H i (P i P H i ) \u22121 , w + \u03bbv , H i .(14)\nFor a loss function L, the Hessian at a given point w in parameter space is represented by the matrix \u2207 2 w L(w) = \u2202g w /\u2202w, where g w = \u2202L(w)/\u2202w. Although calculating the full Hessian is hard for large neural networks, the top eigenvalue \u03bb max (\u2207 2 w L(w)) can be efficiently calculated by using methods from randomized numerical linear algebra [32], [33]. Specifically, according to the chain rule, for a given random vector v we have\n\u2202g T w v \u2202w = \u2202g T w \u2202w v + g T w \u2202v \u2202w = \u2202g T w \u2202w v = \u2207 2 w L(w)v.(15)\nAs can be seen from the first and last terms in (15), calculating the product of the Hessian matrix and the vector is equivalent to a gradient backpropagation. According to this oracle, we can easily compute the top Hessian eigenvalues by using power iteration [34]. In this paper, for a supervised trained model w sl and a self-supervised trained model w ssl , we compute their loss landscapes L (w sl , \u03bb) and L (w ssl , \u03bb) on the same test data to show their similarity. This measure of similarity of the changes in the loss landscape can indicate whether the trained self-supervised model could retain a similar generalization ability to a supervised model when the model parameters are changed. It can be found that the loss landscape is an indicator of the model on the entire test set and is a global feature of the trained neural network model. Here, CKA is further introduced to describe the similarity of input representations of different neural networks.\n\nLet x sl denote a matrix of the logistic output of the last hidden layer of F (\u00b7, w sl ) for n samples, and x ssl denotes the logistic output of the last hidden layer of F (\u00b7, w ssl ) for the same n examples. The CKA is the normalized Hilbert-Schmidt Independence Criterion (HSIC):\n\u23a7 \u23aa \u23a8 \u23aa \u23a9 CKA(x sl , x ssl ) = HSIC(x sl , x ssl ) HSIC(x sl , x sl )HSIC(x ssl , x ssl ) , x sl = [x sl i,j ] = k(x i sl , x j sl ), x ssl = [x ssl i,j ] = k(x i ssl , x j ssl ),(16)\nwhere HSIC(a, b) = tr(acbc)/(n \u2212 1) 2 , c = I \u2212 11 T /n is the centering matrix, k(\u00b7, \u00b7) is a kernel function, x i sl is i-th sample's logistic output of the last hidden layer of F (\u00b7, w sl ), x i ssl is i-th sample's logistic output of the last hidden layer of F (\u00b7, w ssl ). HSIC used in CKA (16) is equivalent to the maximum mean discrepancy between the joint distribution and the product of the marginal distribution. CKA=0 implies the independence of the two representations, and the larger the CKA, the higher the similarity between the two representations. We study the changes in CKA (CKA vs. epoch) of supervised or self-supervised training models to measure how similar different supervision modes make models become when they utilize the same input.\n\n\nIV. EMPIRICAL EVALUATION\n\nThe goal of our empirical study is to illustrate the proposed SSL method can still be competitive and perform well without any ground-truth label, i.e., ground-truth channel in the considered IRS system. We conduct numerical simulations on the channel estimation problem in the considered IRS system presented in Section II. Specifically, in this section, first, we show the convergence of the proposed SSL and intuitively see that there is a double descend phenomenon. Then, we show the effectiveness of our SSL-based solution under different SNRs in terms of NMSE of the channel estimation and spectral efficiency of the MU-MISO IRS system. In addition, we present the loss landscape comparison and CKA comparison of the models trained by SSL and SL to illustrate that SSL can achieve similar generalization performance and feature representation as SL even without any ground-truth labels. Besides, we perform SSL with neural networks of different depths or number of training data and compare the results with the baselines, e.g., LS and linear MMSE (LMMSE) to show that our SSL can achieve better performance than the baseline as the model size increases or the dataset size increases.\n\n\nA. Simulation Setup\n\nThe numerical simulation setup is as follows: the BS is equipped with M = 16 antennas and K = 6 users in the considered IRS-aided MU-MISO communication system. The IRS consists of N = 64 reflecting elements. The Rician channel model is assumed for all the channel links involved. In this case, the channel of the IRS-AP link is formulated as\nG = \u03b2 r,A \u03b2 r,A + 1\u1e20 + 1 \u03b2 r,A + 1G ,(17)\nwhere \u03b2 r,A is the Rician factor of the IRS-AP channel,\u1e20 andG are the line-of-sight (LOS) and non-LOS (NLOS) components, respectively. In addition, the Rician factor is set to \u03b2 r,A = 10. Note that the SNR in the simulation results is set as the transmit SNR, which is defined as SNR = P t /\u03c3 2 n . Besides, we perform multiple Monte Carlo simulations to generate the dataset. For each cascaded channel instance H, its shape is 65 \u00d7 16, a total of 96000 instances are used as training samples and 24000 instances are used as test samples.\n\nFor the neural network training, the batch size is set to 512, and the depth of the neural network is N L = 4 by default unless otherwise specified (we will explore the effect of depth on performance in later numerical simulation subsection). The optimizer for training is SGD with momentum and weight decay. The initial learning rate is 0.01 with a warmup [35] of 10 epochs (warmup guarantees the performance and training efficiency when the batch size is large), and then the learning rate is scheduled by the cosine learning rate decay [36].\n\n\nB. Convergence Behavior\n\nIn the numerical simulation, we show the convergence curves of the training loss and test loss of our SSL. We train the deep neural network which has 10 layers (each subnetwork has 4 layers) 100 epochs with the data generated under the IRS system with \u221210dB SNR and 0dB SNR, respectively. The results are shown in Fig. 4.\n\nAs illustrated in Fig. 4, a sufficiently large training epoch shows a \"double descent\" behavior where test loss first decreases then increases near a threshold, and then decreases again. This phenomenon has not been mentioned or described in the existing work on DL-aided channel estimation. It is different from traditional statistical machine learning cognition. The classical bias-variance tradeoff wisdom of the traditional statistical learning theory suggests the learning models (e.g., deep neural networks) learn a function with a small generalization error (small test loss) first. Then, the learning models start to overfit the training data and result in an increase in the generalization error. In other words, as we move to the right along the training epoch axis, we overfit and expect the test loss to get higher and higher even as the training loss continues to decrease. However, the convergence curves we observed in Fig. 4 are different from the conventional view above. The test loss does rise (blue area in Fig. 4), but it does so before falling to a new minimum. Even though the learning model is extremely overfitted, it achieves its best performance during the second descent (red area in Fig. 4). We call the blue region with a U-shape at the left of the second descent the classical regime where the bias-variance tradeoff behaves as expected, and the point of the peak error is the interpolation threshold. We call this red region the interpolation regime, where the model perfectly memorizes or interpolates the data.\n\nOur numerical simulations suggest that the classical bias-variance tradeoff wisdom is not the complete picture to some extent in the considered SSL channel estimation, the test loss decreases again and may achieve a lower value at the end of training as compared to the first minimum. This implies that when we perform SSL training to achieve channel estimation that does not require ground-truth labels, it may be possible to train for a sufficient time without early stopping [24], a technique often used to prevent overfitting. In practice, there should be a trade-off between performance and training overhead. Early stopping is a method that allows us to terminate training once the model performance stops improving on a hold-out validation dataset. However, since in this paper we observed a large number of experimental results that the proposed algorithm will have a double descent phenomenon, so if one wants to achieve better performance, it is worthwhile to train for a long period of time, but it does not need to train endlessly, because from the experiment results we also observed, in the region where the epoch larger than the threshold, the test loss drops rapidly until convergence. Determining an appropriate number of training epochs helps us to achieve a trade-off between the training overhead and performance. A validation set is often used, and the loss on the validation set is observed during the training phase. When the   TABLE II HYPERPARAMETERS USED FOR PERFORMANCE COMPARISON loss meets the requirements or has converged, the training will be terminated. It can also be seen from Fig. 4 that the training can obtain a good convergence curve, and the test curve does not diverge but also converges to a local solution. We also perform more general numerical simulations to illustrate the prevalence of the double descent phenomenon, please refer to Appendix A. As for its rationale, the current advanced work [37] points out that the epoch-wise double descent may be attributed to different features being learned at different time scales, resulting in a non-monotonous performance curve. Exploring a more detailed theoretical model behind it to fully understand this phenomenon is meaningful for future work.\n\n\nC. Performance With Different \u03c3 \u03b5\n\nHere, we show the performance of the SSL algorithm as the value of \u03c3 \u03b5 changes. Specifically, we show the test spectral efficiency (SE) of the SSL-trained estimator where \u03c3 \u03b5 \u2208 {0.1, 0.2, . . . , 1.0}, and the results are shown in Fig. 5. From Fig. 5, we can see that the value of \u03c3 \u03b5 that makes the test spectral efficiency highest (test error lowest) in different SNR settings is not the same. It can be observed that the value of the optimal \u03c3 \u03b5 gradually decreases as SNR increases. This is intuitive because the larger the SNR, the smaller the ambient noise power, and a smaller \u03c3 \u03b5 needs to be used. We can also see that the test NMSE first decreases and then increases with the increase of \u03c3 \u03b5 when the SNR takes different values. In practice, we can observe the performance (spectral efficiency of users) of the maximum ratio combining (MRC) transmission scheme [38] using an estimator for channel estimation to choose an appropriate \u03c3 \u03b5 which corresponds to the maximum spectral efficiency.\n\n\nD. Performance Comparison With Other Estimation Methods\n\nHere, we show the performance comparison of LS, LMMSE, SL, and SSL solutions in terms of NMSE and spectral efficiency. For the learning-based solutions, for a  fair comparison, we use the same neural network structure, learning rate scheduler, and optimizer, the difference is that the training input and test input for SL are both rough estimates of the channel by LS, while the training input of SSL is a noisy version of the received signal. The hyperparameters are shown in Tab. II. Similar to [39], the spectral efficiency, a metric that indirectly shows the quality of channel estimation, is obtained by adopting the maximum ratio combining [38] scheme, which is motivated by the uplink-downlink duality [40].\n\nWe first present the NMSE results of the cascaded IRS channel in Fig. 6 (a). It is shown that the proposed SSL method outperforms the LS method significantly. It can also be found that the NMSE of SSL is close to that of LMMSE when the SNR is low (e.g., \u221210dB), and the NMSE of SSL can be slightly lower than that of LMMSE when the SNR is greater than \u221210dB. Besides, supervised models achieve the best NMSE performance with different SNRs, which is in line with expectations and the conclusions of existing research [41], [42]. However, we can still see that the SSL solution is competitive because it does not use ground-truth labels at all. Fig. 6 (b) shows the spectral efficiency of these algorithms. It can be found that the spectral efficiencies of SSL and LMMSE are very close and significantly higher than the LS algorithm in a low SNR regime. Similar to [15], the rationale is that the non-linear estimators, SL and SSL, can further improve the estimation performance by intelligently exploiting the non-linear spatial features of channels in a datadriven approach. Due to the fact that compared to SL which requires a large number of labels, SSL has more application value and potential.\n\n\nE. Comparison With Other Neural Networks\n\nHere, we compare the proposed deep neural network with CDRN, [15] which can also be used for channel estimation. The results are shown in Fig. 7. Fig. 7 (a) shows the test loss of different neural networks with labeled data (test loss during supervised training). Note that the neural network proposed in [15] may also be used in our self-supervised training framework. Fig. 7 (b) shows the test loss curves for self-supervised training with different neural networks (without labeled data). From Fig. 7 (a), it can be found that the neural network proposed in our paper has higher learning efficiency than CDRN. It can be seen from Fig. 7 (b) that the neural network proposed in our paper can obtain lower test losses than CDRN.\n\n\nF. Loss Landscape and CKA\n\nIn this subsection, we first compute the top Hessian eigenvector and then perturb the model parameters along with that direction and measure the loss to plot the loss landscape in Fig. 8. The random model in this figure is a neural network model that has not been trained and is only initialized with a Gaussian distribution.\n\nIt can be seen from Fig. 8 that when the perturbation is 0, our solution and the supervised learning solution have close generalization performance, and this closeness is maintained as the perturbation changes. It can be clearly seen that the loss landscape of the SSL-trained model is closer to supervised learning than the random solution, that is, the change of the SSL-trained model along the direction of the top-1 eigenvalue can reflect the loss landscape of supervised learning to a certain extent. Therefore, the results of our proposed SSL solution could provide valuable insights into the training of deep learning-based IRS channel estimation solutions used in practice. Besides, we also see that the nadir of the loss landscape becomes lower as the SNR increases (from Fig. 8 (a) to (c)), which is intuitive since a higher SNR means that the channel estimation problem is easier (the received signal is less perturbed by noise).\n\nWe discuss representational similarities between different models with different training methods via CKA. In the formula (16) for calculating CKA, we use a linear kernel function, which is the inner product operation. Then, we calculate the CKA between the SL-trained model and the SSL-trained model or the random model without training for different epochs, and the heatmap results are shown in Fig. 9. In this figure, the x-axis is the number of training epochs of the SL model, and the y-axis is the number of training epochs of the supervised training model. The darker color in the figure indicates higher similarity, and the lighter color indicates lower similarity. Comparing the results in the first and second row of Fig. 9, it can be clearly seen that in different SNR regimes, the SSL-trained model and the SL-trained model always maintain a high CKA score compared to the random results. This means that even without the use of ground-truth labels, the feature representation of the input by the SSL-trained model maintains a certain similarity with the SL-trained model. This implies that our proposed SSL provides a training method for the initialized neural network that is closer to the optimal feature representation for channel estimation. Overall, the above results suggest that SSL is a good alternative to SL since SSL can show a similar generalization performance to SL after perturbing the neural network parameters and a similar feature representation during training.\n\n\nG. Performance With Different Number of Layers\n\nIn this subsection, we empirically validate neural networks with different depths. Specifically, we change the depth N L of each subnetwork of the deep neural network proposed in Section II. Then, we use the data in the environment with SNR of \u22125dB to complete the training of SSL, and test the NMSE and spectral efficiency of the trained model. The results are shown in Fig. 10 (a) and (b). In addition, we demonstrate the out-of-distribution generalization ability of SSL, i.e., the performance when test and training data distributions differ. We set the SNR of the test environment to 0dB and show the NMSE and spectral efficiency performance of SSL in Fig. 10 (c) and (d).\n\nIt can be clearly seen from Fig. 10 that with the increase of the neural network size, the performance of SSL is improved to varying degrees, and the performance of SL is also on the rise. However, when the depth of the neural network is too large, the performance of SL slightly decreases due to overfitting. And when the neural network size is large enough, SSL can outperform LMMSE without any ground truth labels. It is worth mentioning that from Fig. 10 (c) and (d), it can be seen that even if there are differences between the distributions of test and training data, the SSL-trained network can still obtain a very low NMSE and a high spectral efficiency compared with LS and LMMSE, which indicates that SSL has strong out-of-distribution generalization ability. However, the larger the scale of the neural network, the higher the complexity. Specifically, Tab. III shows total parameters and the floating-point operations (FLOPs) required for the inference of neural networks with different depths. One can see that larger-scale neural networks have more storage overhead and higher computational complexity. In practice, an appropriate scale needs to be selected to obtain a trade-off between performance and complexity.\n\n\nH. Performance With Different Number of Training Data\n\nHere we discuss the impact of the amount of training data on performance. In order to achieve a trade-off between complexity and performance, we set the layer number of each sub-network to N L = 4 layers. Using the data generated in the environment with an SNR of \u22125dB as the training data, the numerical results are shown in Fig. 11. Among them, the first behavior is the NMSE and spectral efficiency results of the in-distribution scenarios (the training and test data distributions are consistent), and the second behavior is the results of the out-of-distribution scenarios (the training and test data distributions are inconsistent).\n\nFrom Fig. 11, we see that with the increase in the amount of training data, the performance of the algorithm is improved first and then basically kept unchanged. This is in line with expectations. A larger amount of data can improve the generalization ability of the neural network, but the performance is also affected by parameters such as the neural network structure, and it will converge. From Fig. 11 (c) and (d) we can also draw a similar empirical conclusion to the previous subsection, i.e., SSL not only works in scenarios where the training and test data distributions are consistent but also generalizes competitively in out-of-distribution situations.  (14). The vertical axis is the test loss. Models with similar trends in test loss as perturbation change have similar generalization abilities.  \n\n\nI. Performance Under Other Settings\n\nAs shown in [43] and [44], integrated sensing and communications (ISAC) have been proposed as a way to further improve spectral efficiency and reduce hardware costs. There is potential value in considering a mobility scenario in an IRS-assisted wireless communication scenario to support ISAC. Here, we show that the proposed method still works under a mobility scenario where the mobility small-scale Ricean fading of the User-IRS and User-BS links are considered the same as [45].\n\nThe simulation results are shown in Tab. IV. From the results in Tab. IV, we can see that our proposed solution outperforms both LS and LMMSE in different SNR settings. This confirms that our self-supervised method can also work well in usermoving scenarios.\n\nTransfer learning is a useful way to guarantee test performance in the out-of-distribution case [46]. Here, we also verify the scalability of the proposed SSL in a transfer learning problem. We generate the test data in an environment with  0dB SNR, we assume that only a small number of labeled samples can be accessed, and SL will use these samples for training to obtain an estimator, while our method first works in the source domain (\u22125dB SNR) for pre-training and then fine-tune the trained SSL model with the labeled samples. The results are shown in Tab. V. From Tab. V, it can be found that the method of combining SSL and transfer learning has better performance than supervised learning. This corroborates the effectiveness of the combination of SSL and transfer learning.\n\n\nV. CONCLUSION\n\nWe have proposed a self-supervised learning-based channel estimation, i.e, SSL solution, for intelligent reflecting surface-aided wireless communication systems without ground truth labels. First, we presented a new neural network architecture to take full advantage of the global information of the complex domain input. Then, we proposed a simple and novel pretext task that resulted in a self-supervised loss. This loss forced the neural network to recover information from the noisy version of the received signal in the training phase. While in the test phase, the neural network promised directly inputs the rough channel estimation of the least-square (LS) estimator to obtain a more precise solution. Our empirical evaluations demonstrated an \"epoch-wise double descent\" phenomenon which has not been noticed in previous works. This phenomenon inspired researchers to obtain better performance by training longer and not using early stopping. We also performed extensive numerical simulations to demonstrate the performance of the proposed SSL solution. Our results showed that SSL can achieve performance that surpasses LS and linear minimum mean squared error (LMMSE), and that SSL-trained models have similar loss landscapes and feature representations as supervised learning-trained models. These results demonstrated that the proposed SSL is a good alternative to SL in practice.\n\nFor the broader impact, deep learning-assisted wireless communications have become a key enabler to fuel the development of wireless networks beyond 5G. However, in practice, labeling a large number of training samples to achieve deep learning requires huge efforts. We believe that it is important to inform the community about the importance of methods like our SSL that don't need ground truth labels and thus raise more inspiring and novel research. As for limitations, like [15], [18], [19] we only provide empirical results and analysis on synthetic channels, and the theoretical explanation for the double-descent phenomenon we found remains an open problem. Precise theoretical analysis is difficult, we envision that the existing advanced works [37], [47] to understand this phenomenon. However, [37] only fits linear models and [47] does not fit the \"epoch-wise\" case. Thus, we follow Ockham's Razor [48], and just present limited discussions rather than farfetched explanations. For future work, evaluating the proposed method on real-world data can better illustrate the practical value of the method. Besides, we think the SSL method may also have exploratory value in other wireless communication problems, e.g., CSI feedback reconstitution [49], channel tracking [50], etc. SSL without ground truth labels can also be applied mmWave [51], massive MIMO [52], NOMA [53], etc. Another meaningful future work is to explore detailed theoretical models behind the new SSL framework that exhibits the double-descent phenomenon, e.g., by analyzing specific self-supervised loss functions and using the results of stateof-the-art theories in transfer learning [54].\n\n\nAPPENDIX A ADDITIONAL RESULTS\n\nHere we show the results of the test loss vs. the number of subnetwork layers in Fig. 12 (a) and the results of the test loss as a function of the number of training data in Fig. 12 (b). From Fig. 12 (a), it can be found that when the number of sub-network layers is greater than 1, the double descent phenomenon will occur. This is consistent with the conclusion in machine learning pipelines that the double descent phenomenon usually occurs in overparameterized neural networks. And we can also find that increasing the number of neural network layers causes the interpolation threshold to move to the left, and results in a lower loss for the interpolation regime, which implies that we could use a larger network to obtain good performance. It can be found from Fig. 12 (b) that the increase in the number of training data also moves the interpolation threshold to the left, and the loss function in the interpolation regime has a lower value than that when the number of data is small. Collectively these results suggest that double descent is ubiquitous and has a guiding role in neural network architecture design and data size selection.\n\n\nAPPENDIX B PROOF OF THEOREM 1\n\nAccording to the definition of the denoising problem (9), we denote L = E X,\u03b5 F (X + \u03b5, w) \u2212 X 2 . According to the Tweedie's formula [31], we have: P (X)u(\u03b5) F (X + \u03b5, w) \u2212 X 2 / X 2 dXd\u03b5 = P (X)u(y \u2212X) F (y, w) \u2212 X 2 / X 2 dXd\u03b5 = P (X)u(y \u2212X) F (y, w) \u2212 X, \u2202F (y, w) / X 2 dXd\u03b5,\n\nwhere u(\u03b5) = exp(\u2212\u03b5 T \u03a3\u03b5\u03b5/2) \u221a (2\u03c0) d det (\u03a3\u03b5) and \u03a3 \u03b5 = \u03c3 2 \u03b5 I. Thus, \u2202L = 0 \u2192 P (X)u(y \u2212 X) (F (y, w) \u2212 X) dX = 0, then we have: F (y, w * ) = P (X)u(y \u2212 X)XdX P (X)u(y \u2212 X)dX .\n\nThus, we have F (y, w * ) = y + \u03c3 2 \u03b5 \u2207 y log [P * u] (y), where * is the convolution operation. Note that for a Rician channel whose combination matrix H consisting of real and imaginary parts has a covariance matrix \u03a3 H , X = HP + V is the linear transform of a random variable from the normal distribution. Because the convolution of two normal distributions is also a normal probability density distribution function. Thus, F (y, w * ) = y + \u03c3 2 \u03b5 1 log [P * u] (y)\nd log [P * u] (u) dy = y \u2212 \u03c3 2 \u03b5 P T \u03a3 H P + \u03c3 2 v I + \u03c3 2 \u03b5 I \u22121 y(20)\nFurther let \u03a3 = P T \u03a3 H P + \u03c3 2 v I + \u03c3 2 \u03b5 I , substituting into the estimation result of LS, and using the Cauchy-Schwarz inequation we have:\nE F (XP H (PP H ) \u22121 ) \u2212 H\nFig. 1 .\n1System model.\n\nFig. 2 .\n2Deep neural network structure for channel estimation.\n\nFig. 3 .\n3Self-supervised training and test sketch. Algorithm 1 The Training Pipeline of the SSL-Based IRS Channel Estimation Require: Learning rate \u03b7, local batch size B, number of training epochs E, initialized the neural network model with parameters w 0 , a dataset D X = {X} NX i=1 containing only received signal. 1: for Epoch e = 1, 2 . . . , E do 2: for Iteration t = 1, 2 . . . , I do 3:\n\n\nthe autocorrelation matrix of the channel H, T r(x) is the trace of x and \u03be 2 = P PP H \u22121 2 F .\n\nFig. 4 .\n4Convergence behaviors of the training loss and test loss of our SSL with different SNRs.\n\nFig. 5 .\n5Test NMSE and spectral efficiency (SE) performance of the SSL solution vs. \u03c3\u03b5.\n\nFig. 6 .\n6Comparison of different solutions in terms of NMSE and spectral efficiency.\n\nFig. 7 .\n7Convergence behaviors of test loss of different neural networks with (a) supervised learning and (b) self-supervised learning.\n\nFig. 8 .\n8Loss landscape perturbed based on the top Hessian eigenvector of the SSL-trained model and the SL-trained model with different SNR. The horizontal axis of these plots is perturbations, i.e., \u03bb in\n\nFig. 9 .\n9CKA reveals consistent relationships between the neural network trained with different methods. (Top) CKA vs. epoch between SSL and SL with different SNR. (Bottom) CKA vs. epoch between SL and random model with different SNR. In these heatmaps, the darker the pixels, the more similar the representations of the models. Scales (0-49) in the figure represent training epochs. The x-axis is the number of training epochs of the SL model, and the y-axis is the number of training epochs of the supervised training model.\n\nFig. 10 .\n10NMSE and spectral efficiency performance of the SSL solution with a different number of layers N L of each subnetwork. (a-b) Both training data and test data are derived from an environment with an SNR of \u22125dB. (c-d) The training data comes from an environment with an SNR of \u22125dB, and the test data comes from an environment with an SNR of 0dB.\n\nFig. 11 .\n11NMSE and spectral efficiency performance of SSL solution with a different number of training data N . (a-b) Both training data and test data are derived from an environment with an SNR of \u22125dB. (c-d) The training data comes from an environment with an SNR of \u22125dB, and the test data comes from an environment with an SNR of 0dB.\n\nFig. 12 .\n12Convergence behaviors of the test loss of our SSL with \u22125dB SNR. (Left) SSL test loss with a different number of layers of each subnetwork. (Right) SSL test loss with a different number of training data.\n\nTABLE I\nISIMULATION PARAMETERS\n\nTABLE III THE\nIIICOMPLEXITY OF NEURAL NETWORKS WITH DIFFERENT NUMBERS OF LAYERS OF EACH SUBNETWORK\n\nTABLE IV TESTTABLE V\nIVVNMSE OF DIFFERENT METHODS IN A MOBILITY SCENARIO TEST NMSE WITH DIFFERENT NUMBERS FOR LABELED DATA\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nwhere \u03a8 = P H (PP H ) \u22121 . Then one can get the bound presented in Theorem 1.\nToward smart wireless communications via intelligent reflecting surfaces: A contemporary survey. S Gong, IEEE Commun. Surveys Tuts. 2244th Quart.S. Gong et al., \"Toward smart wireless communications via intelligent reflecting surfaces: A contemporary survey,\" IEEE Commun. Surveys Tuts., vol. 22, no. 4, pp. 2283-2314, 4th Quart., 2020.\n\nIntelligent reflecting surface enhanced wireless network via joint active and passive beamforming. Q Wu, R Zhang, IEEE Trans. Wireless Commun. 1811Q. Wu and R. Zhang, \"Intelligent reflecting surface enhanced wireless network via joint active and passive beamforming,\" IEEE Trans. Wireless Commun., vol. 18, no. 11, pp. 5394-5409, Nov. 2019.\n\nReconfigurable intelligent surfaces for wireless communications: Principles, challenges, and opportunities. M A Mossallamy, H Zhang, L Song, K G Seddik, Z Han, G Y Li, IEEE Trans. Cogn. Commun. Netw. 63M. A. El Mossallamy, H. Zhang, L. Song, K. G. Seddik, Z. Han, and G. Y. Li, \"Reconfigurable intelligent surfaces for wireless communi- cations: Principles, challenges, and opportunities,\" IEEE Trans. Cogn. Commun. Netw., vol. 6, no. 3, pp. 990-1002, Sep. 2020.\n\nIntelligent reflecting surface-aided joint processing coordinated multipoint transmission. M Hua, Q Wu, D W K Ng, J Zhao, L Yang, IEEE Trans. Commun. 693M. Hua, Q. Wu, D. W. K. Ng, J. Zhao, and L. Yang, \"Intelligent reflect- ing surface-aided joint processing coordinated multipoint transmission,\" IEEE Trans. Commun., vol. 69, no. 3, pp. 1650-1665, Mar. 2021.\n\nMulticell MIMO communications relying on intelligent reflecting surfaces. C Pan, IEEE Trans. Wireless Commun. 198C. Pan et al., \"Multicell MIMO communications relying on intelligent reflecting surfaces,\" IEEE Trans. Wireless Commun., vol. 19, no. 8, pp. 5218-5233, May 2020.\n\nCovert communication in intelligent reflecting surface-assisted NOMA systems: Design, analysis, and optimization. L Lv, Q Wu, Z Li, Z Ding, N Al-Dhahir, J Chen, IEEE Trans. Wireless Commun. 213L. Lv, Q. Wu, Z. Li, Z. Ding, N. Al-Dhahir, and J. Chen, \"Covert communication in intelligent reflecting surface-assisted NOMA systems: Design, analysis, and optimization,\" IEEE Trans. Wireless Commun., vol. 21, no. 3, pp. 1735-1750, Mar. 2022.\n\nIntelligent reflecting surface assisted interference mitigation for cellular-connected UAV. X Pang, W Mei, N Zhao, R Zhang, IEEE Wireless Commun. Lett. 118X. Pang, W. Mei, N. Zhao, and R. Zhang, \"Intelligent reflecting surface assisted interference mitigation for cellular-connected UAV,\" IEEE Wireless Commun. Lett., vol. 11, no. 8, pp. 1708-1712, Aug. 2022.\n\nDeep compressed sensing-based cascaded channel estimation for RIS-aided communication systems. W Xie, J Xiao, P Zhu, C Yu, L Yang, IEEE Wireless Commun. Lett. 114W. Xie, J. Xiao, P. Zhu, C. Yu, and L. Yang, \"Deep compressed sensing-based cascaded channel estimation for RIS-aided communica- tion systems,\" IEEE Wireless Commun. Lett., vol. 11, no. 4, pp. 846-850, Apr. 2022.\n\nChannel estimation for intelligent reflecting surface assisted multiuser communications: Framework, algorithms, and analysis. Z Wang, L Liu, S Cui, IEEE Trans. Wireless Commun. 1910Z. Wang, L. Liu, and S. Cui, \"Channel estimation for intelligent reflecting surface assisted multiuser communications: Framework, algo- rithms, and analysis,\" IEEE Trans. Wireless Commun., vol. 19, no. 10, pp. 6607-6620, Oct. 2020.\n\nMachine learning-inspired algorithmic framework for intelligent reflecting surface-assisted wireless systems. J.-C Chen, IEEE Trans. Veh. Technol. 7010J.-C. Chen, \"Machine learning-inspired algorithmic framework for intel- ligent reflecting surface-assisted wireless systems,\" IEEE Trans. Veh. Technol., vol. 70, no. 10, pp. 10671-10685, Oct. 2021.\n\nChannel estimation for RIS-aided multiuser millimeter-wave systems. G Zhou, C Pan, H Ren, P Popovski, A L Swindlehurst, IEEE Trans. Signal Process. 70G. Zhou, C. Pan, H. Ren, P. Popovski, and A. L. Swindlehurst, \"Channel estimation for RIS-aided multiuser millimeter-wave systems,\" IEEE Trans. Signal Process., vol. 70, pp. 1478-1492, 2022.\n\nAn optimal channel estimation scheme for intelligent reflecting surfaces based on a minimum variance unbiased estimator. T L Jensen, E. De Carvalho, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)T. L. Jensen and E. De Carvalho, \"An optimal channel estimation scheme for intelligent reflecting surfaces based on a minimum variance unbiased estimator,\" in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2020, pp. 5000-5004.\n\nChannel estimation and low-complexity beamforming design for passive intelligent surface assisted MISO wireless energy transfer. D Mishra, H Johansson, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)D. Mishra and H. Johansson, \"Channel estimation and low-complexity beamforming design for passive intelligent surface assisted MISO wire- less energy transfer,\" in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2019, pp. 4659-4663.\n\nIntelligent reflecting surface-assisted multi-user MISO communication: Channel estimation and beamforming design. Q.-U.-A Nadeem, H Alwazani, A Kammoun, A Chaaban, M Debbah, M.-S Alouini, IEEE Open J. Commun. Soc. 1Q.-U.-A. Nadeem, H. Alwazani, A. Kammoun, A. Chaaban, M. Debbah, and M.-S. Alouini, \"Intelligent reflecting surface-assisted multi-user MISO communication: Channel estimation and beamforming design,\" IEEE Open J. Commun. Soc., vol. 1, pp. 661-680, 2020.\n\nDeep residual learning for channel estimation in intelligent reflecting surface-assisted multiuser communications. C Liu, X Liu, D W K Ng, J Yuan, IEEE Trans. Wireless Commun. 212C. Liu, X. Liu, D. W. K. Ng, and J. Yuan, \"Deep residual learning for channel estimation in intelligent reflecting surface-assisted multi- user communications,\" IEEE Trans. Wireless Commun., vol. 21, no. 2, pp. 898-912, Feb. 2021.\n\nCascaded channel estimation for large intelligent metasurface assisted massive MIMO. Z.-Q He, X Yuan, IEEE Wireless Commun. Lett. 92Z.-Q. He and X. Yuan, \"Cascaded channel estimation for large intelligent metasurface assisted massive MIMO,\" IEEE Wireless Commun. Lett., vol. 9, no. 2, pp. 210-214, Feb. 2019.\n\nDeep channel learning for large intelligent surfaces aided mm-Wave massive MIMO systems. A M Elbir, A Papazafeiropoulos, P Kourtessis, S Chatzinotas, IEEE Wireless Commun. Lett. 99A. M. Elbir, A. Papazafeiropoulos, P. Kourtessis, and S. Chatzinotas, \"Deep channel learning for large intelligent surfaces aided mm-Wave massive MIMO systems,\" IEEE Wireless Commun. Lett., vol. 9, no. 9, pp. 1447-1451, Sep. 2020.\n\nDeep denoising neural network assisted compressive channel estimation for mmWave intelligent reflecting surfaces. S Liu, Z Gao, J Zhang, M D Renzo, M.-S Alouini, IEEE Trans. Veh. Technol. 698S. Liu, Z. Gao, J. Zhang, M. D. Renzo, and M.-S. Alouini, \"Deep denoising neural network assisted compressive channel estimation for mmWave intelligent reflecting surfaces,\" IEEE Trans. Veh. Technol., vol. 69, no. 8, pp. 9223-9228, Aug. 2020.\n\nMultiple residual dense networks for reconfigurable intelligent surfaces cascaded channel estimation. Y Jin, IEEE Trans. Veh. Technol. 712Y. Jin et al., \"Multiple residual dense networks for reconfigurable intelligent surfaces cascaded channel estimation,\" IEEE Trans. Veh. Technol., vol. 71, no. 2, pp. 2134-2139, Feb. 2022.\n\nLearning to estimate RIS-aided mmWave channels. J He, H Wymeersch, M Di Renzo, M Juntti, IEEE Wireless Commun. Lett. 114J. He, H. Wymeersch, M. Di Renzo, and M. Juntti, \"Learning to estimate RIS-aided mmWave channels,\" IEEE Wireless Commun. Lett., vol. 11, no. 4, pp. 841-845, Apr. 2022.\n\nChannel estimation for semi-passive reconfigurable intelligent surfaces with enhanced deep residual networks. Y Jin, J Zhang, X Zhang, H Xiao, B Ai, D W K Ng, IEEE Trans. Veh. Technol. 7010Y. Jin, J. Zhang, X. Zhang, H. Xiao, B. Ai, and D. W. K. Ng, \"Channel estimation for semi-passive reconfigurable intelligent surfaces with enhanced deep residual networks,\" IEEE Trans. Veh. Technol., vol. 70, no. 10, pp. 11083-11088, Oct. 2021.\n\nChannel estimation for reconfigurable intelligent surface aided MISO communications: From LMMSE to deep learning solutions. N K Kundu, M R Mckay, IEEE Open J. Commun. Soc. 2N. K. Kundu and M. R. McKay, \"Channel estimation for reconfigurable intelligent surface aided MISO communications: From LMMSE to deep learning solutions,\" IEEE Open J. Commun. Soc., vol. 2, pp. 471-487, 2021.\n\nRepresentation learning with contrastive predictive coding. A Van Den Oord, Y Li, O Vinyals, arXiv:1807.03748A. van den Oord, Y. Li, and O. Vinyals, \"Representation learning with contrastive predictive coding,\" 2018, arXiv:1807.03748.\n\nOverfitting in neural nets: Backpropagation, conjugate gradient, and early stopping. R Caruana, S Lawrence, C Giles, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystR. Caruana, S. Lawrence, and C. Giles, \"Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping,\" in Proc. Adv. Neural Inf. Process. Syst., 2000, pp. 381-387.\n\nBeyond a Gaussian denoiser: Residual learning of deep CNN for image denoising. K Zhang, W Zuo, Y Chen, D Meng, L Zhang, IEEE Trans. Image Process. 267K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, \"Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising,\" IEEE Trans. Image Process., vol. 26, no. 7, pp. 3142-3155, Jul. 2017.\n\nDeep learning-based channel estimation for beamspace mmWave massive MIMO systems. H He, C.-K Wen, S Jin, G Y Li, IEEE Wireless Commun. Lett. 75H. He, C.-K. Wen, S. Jin, and G. Y. Li, \"Deep learning-based channel estimation for beamspace mmWave massive MIMO systems,\" IEEE Wireless Commun. Lett., vol. 7, no. 5, pp. 852-855, Oct. 2018.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2016, pp. 770-778.\n\nRectified linear units improve restricted Boltzmann machines. V Nair, G E Hinton, Proc. 27th Int. Conf. Int. Conf. 27th Int. Conf. Int. ConfV. Nair and G. E. Hinton, \"Rectified linear units improve restricted Boltzmann machines,\" in Proc. 27th Int. Conf. Int. Conf. Mach. Learn., 2010, pp. 807-814.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, Proc. 32nd Int. Conf. Int. Conf. Mach. Learn. 32nd Int. Conf. Int. Conf. Mach. LearnS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in Proc. 32nd Int. Conf. Int. Conf. Mach. Learn., 2015, pp. 448-456.\n\nNoise2Noise: Learning image restoration without clean data. J Lehtinen, Proc. 35th Int. Conf. 35th Int. ConfJ. Lehtinen et al., \"Noise2Noise: Learning image restoration without clean data,\" in Proc. 35th Int. Conf. Mach. Learn., 2018, pp. 2965-2974.\n\nAn empirical Bayes approach to statistics. H E Robbins, Breakthroughs in Statistics. New York, NY, USASpringerH. E. Robbins, \"An empirical Bayes approach to statistics,\" in Breakthroughs in Statistics. New York, NY, USA: Springer, 1992, pp. 388-394.\n\nRandomized Algorithms for Matrices and Data (Foundations and Trends in Machine Learning). M W Mahoney, NOWBoston, MA, USAM. W. Mahoney, Randomized Algorithms for Matrices and Data (Foun- dations and Trends in Machine Learning). Boston, MA, USA: NOW, 2011.\n\nRandNLA: Randomized numerical linear algebra. P Drineas, M W Mahoney, Commun. ACM. 596P. Drineas and M. W. Mahoney, \"RandNLA: Randomized numerical linear algebra,\" Commun. ACM, vol. 59, no. 6, pp. 80-90, 2016.\n\nHessian-based analysis of large batch training and robustness to adversaries. Z Yao, A Gholami, K Keutzer, M W Mahoney, Proc. 32nd Int. Conf. Neural Inf. 32nd Int. Conf. Neural InfZ. Yao, A. Gholami, K. Keutzer, and M. W. Mahoney, \"Hessian-based analysis of large batch training and robustness to adversaries,\" in Proc. 32nd Int. Conf. Neural Inf. Process. Syst., 2018, pp. 4954-4964.\n\nAccurate, large minibatch SGD: Training ImageNet in 1 hour. P Goyal, arXiv:1706.02677P. Goyal et al., \"Accurate, large minibatch SGD: Training ImageNet in 1 hour,\" 2017, arXiv:1706.02677.\n\nSGDR: Stochastic gradient descent with warm restarts. I Loshchilov, F Hutter, arXiv:1608.03983I. Loshchilov and F. Hutter, \"SGDR: Stochastic gradient descent with warm restarts,\" 2016, arXiv:1608.03983.\n\nMulti-scale feature learning dynamics: Insights for double descent. M Pezeshki, A Mitra, Y Bengio, G Lajoie, arXiv:2112.032152021M. Pezeshki, A. Mitra, Y. Bengio, and G. Lajoie, \"Multi-scale feature learning dynamics: Insights for double descent,\" 2021, arXiv:2112.03215.\n\nT L Marzetta, H Q Ngo, Fundamentals of Massive MIMO. Cambridge, U.K.Cambridge Univ. PressT. L. Marzetta and H. Q. Ngo, Fundamentals of Massive MIMO. Cambridge, U.K.: Cambridge Univ. Press, 2016.\n\nLearning the MMSE channel estimator. D Neumann, T Wiese, W Utschick, IEEE Trans. Signal Process. 6611D. Neumann, T. Wiese, and W. Utschick, \"Learning the MMSE channel estimator,\" IEEE Trans. Signal Process., vol. 66, no. 11, pp. 2905-2917, Jun. 2018.\n\nMassive MIMO networks: Spectral, energy, and hardware efficiency. E Bj\u00f6rnson, J Hoydis, L Sanguinetti, Found. Trends Signal Process. 11E. Bj\u00f6rnson, J. Hoydis, and L. Sanguinetti, \"Massive MIMO net- works: Spectral, energy, and hardware efficiency,\" Found. Trends Signal Process., vol. 11, nos. 3-4, pp. 154-655, Nov. 2017.\n\nPower of deep learning for channel estimation and signal detection in OFDM systems. H Ye, G Y Li, B.-H Juang, IEEE Wireless Commun. Lett. 71H. Ye, G. Y. Li, and B.-H. Juang, \"Power of deep learning for channel estimation and signal detection in OFDM systems,\" IEEE Wireless Commun. Lett., vol. 7, no. 1, pp. 114-117, Feb. 2018.\n\nDeep learning for channel estimation: Interpretation, performance, and comparison. Q Hu, F Gao, H Zhang, S Jin, G Y Li, IEEE Trans. Wireless Commun. 204Q. Hu, F. Gao, H. Zhang, S. Jin, and G. Y. Li, \"Deep learning for channel estimation: Interpretation, performance, and comparison,\" IEEE Trans. Wireless Commun., vol. 20, no. 4, pp. 2398-2412, Apr. 2021.\n\nLearning-based predictive beamforming for integrated sensing and communication in vehicular networks. C Liu, IEEE J. Sel. Areas Commun. 408C. Liu et al., \"Learning-based predictive beamforming for integrated sensing and communication in vehicular networks,\" IEEE J. Sel. Areas Commun., vol. 40, no. 8, pp. 2317-2334, Aug. 2022.\n\nIntegrated sensing and communications (ISAC) for vehicular communication networks (VCN). X Cheng, D Duan, S Gao, L Yang, IEEE Internet Things J. 923X. Cheng, D. Duan, S. Gao, and L. Yang, \"Integrated sensing and communications (ISAC) for vehicular communication networks (VCN),\" IEEE Internet Things J., vol. 9, no. 23, pp. 23441-23451, Dec. 2022.\n\nReconfigurable intelligent surface assisted multi-carrier wireless systems for doubly selective high-mobility Ricean channels. C Xu, IEEE Trans. Veh. Technol. 714C. Xu et al., \"Reconfigurable intelligent surface assisted multi-carrier wireless systems for doubly selective high-mobility Ricean channels,\" IEEE Trans. Veh. Technol., vol. 71, no. 4, pp. 4023-4041, Apr. 2022.\n\nDeep transfer learning for signal detection in ambient backscatter communications. C Liu, Z Wei, D W K Ng, J Yuan, Y.-C Liang, IEEE Trans. Wireless Commun. 203C. Liu, Z. Wei, D. W. K. Ng, J. Yuan, and Y.-C. Liang, \"Deep transfer learning for signal detection in ambient backscatter communi- cations,\" IEEE Trans. Wireless Commun., vol. 20, no. 3, pp. 1624-1638, Mar. 2021.\n\nOn the role of optimization in double descent: A least squares study. I Kuzborskij, C Szepesv\u00e1ri, O Rivasplata, A Rannen-Triki, R Pascanu, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystI. Kuzborskij, C. Szepesv\u00e1ri, O. Rivasplata, A. Rannen-Triki, and R. Pascanu, \"On the role of optimization in double descent: A least squares study,\" in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 29567-29577.\n\nFisher information and stochastic complexity. J J Rissanen, IEEE Trans. Inf. Theory. 421J. J. Rissanen, \"Fisher information and stochastic complexity,\" IEEE Trans. Inf. Theory, vol. 42, no. 1, pp. 40-47, Jan. 1996.\n\nUser-centric online gossip training for autoencoder-based CSI feedback. J Guo, Y Zuo, C.-K Wen, S Jin, IEEE J. Sel. Topics Signal Process. 163J. Guo, Y. Zuo, C.-K. Wen, and S. Jin, \"User-centric online gossip training for autoencoder-based CSI feedback,\" IEEE J. Sel. Topics Signal Process., vol. 16, no. 3, pp. 559-572, Apr. 2022.\n\nAngular-domain selective channel tracking and Doppler compensation for high-mobility mmWave massive MIMO. G Liu, A Liu, R Zhang, M Zhao, IEEE Trans. Wireless Commun. 205G. Liu, A. Liu, R. Zhang, and M. Zhao, \"Angular-domain selective channel tracking and Doppler compensation for high-mobility mmWave massive MIMO,\" IEEE Trans. Wireless Commun., vol. 20, no. 5, pp. 2902-2916, May 2021.\n\nAn overview of signal processing techniques for millimeter wave MIMO systems. R W Heath, N Gonz\u00e1lez-Prelcic, S Rangan, W Roh, A M Sayeed, IEEE J. Sel. Topics Signal Process. 103R. W. Heath, N. Gonz\u00e1lez-Prelcic, S. Rangan, W. Roh, and A. M. Sayeed, \"An overview of signal processing techniques for mil- limeter wave MIMO systems,\" IEEE J. Sel. Topics Signal Process., vol. 10, no. 3, pp. 436-453, Apr. 2016.\n\nMassive MIMO: Ten myths and one critical question. E Bj\u00f6rnson, E G Larsson, T L Marzetta, IEEE Commun. Mag. 542E. Bj\u00f6rnson, E. G. Larsson, and T. L. Marzetta, \"Massive MIMO: Ten myths and one critical question,\" IEEE Commun. Mag., vol. 54, no. 2, pp. 114-123, Feb. 2016.\n\nOn the performance of non-orthogonal multiple access in 5G systems with randomly deployed users. Z Ding, Z Yang, P Fan, H V Poor, IEEE Signal Process. Lett. 2112Z. Ding, Z. Yang, P. Fan, and H. V. Poor, \"On the performance of non-orthogonal multiple access in 5G systems with randomly deployed users,\" IEEE Signal Process. Lett., vol. 21, no. 12, pp. 1501-1505, Dec. 2014.\n\nWhat is being transferred in transfer learning?\" in Proc. B Neyshabur, H Sedghi, C Zhang, Adv. Neural Inf. Process. Syst. 33B. Neyshabur, H. Sedghi, and C. Zhang, \"What is being transferred in transfer learning?\" in Proc. Adv. Neural Inf. Process. Syst., vol. 33, 2020, pp. 512-523.\n", "annotations": {"author": "[{\"end\":150,\"start\":115},{\"end\":210,\"start\":151},{\"end\":248,\"start\":211},{\"end\":300,\"start\":249},{\"end\":354,\"start\":301},{\"end\":403,\"start\":355},{\"end\":619,\"start\":404},{\"end\":701,\"start\":620}]", "publisher": null, "author_last_name": "[{\"end\":130,\"start\":125},{\"end\":189,\"start\":187},{\"end\":222,\"start\":219},{\"end\":334,\"start\":329},{\"end\":383,\"start\":379}]", "author_first_name": "[{\"end\":124,\"start\":115},{\"end\":186,\"start\":180},{\"end\":218,\"start\":211},{\"end\":275,\"start\":268},{\"end\":278,\"start\":276},{\"end\":328,\"start\":320},{\"end\":378,\"start\":374}]", "author_affiliation": "[{\"end\":618,\"start\":405},{\"end\":700,\"start\":621}]", "title": "[{\"end\":101,\"start\":1},{\"end\":802,\"start\":702}]", "venue": "[{\"end\":848,\"start\":804}]", "abstract": "[{\"end\":3991,\"start\":1304}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4051,\"start\":4048},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4056,\"start\":4053},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4061,\"start\":4058},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4066,\"start\":4063},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4813,\"start\":4810},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4906,\"start\":4903},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5050,\"start\":5047},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5110,\"start\":5107},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5242,\"start\":5239},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5399,\"start\":5396},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5554,\"start\":5550},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5940,\"start\":5936},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6319,\"start\":6315},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6328,\"start\":6324},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6337,\"start\":6333},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6552,\"start\":6548},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6937,\"start\":6933},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7369,\"start\":7365},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7545,\"start\":7541},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8049,\"start\":8045},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9125,\"start\":9121},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9131,\"start\":9127},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9137,\"start\":9133},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9234,\"start\":9230},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9391,\"start\":9387},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9397,\"start\":9393},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9528,\"start\":9524},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9650,\"start\":9646},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9913,\"start\":9909},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10168,\"start\":10164},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10418,\"start\":10414},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10605,\"start\":10601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10769,\"start\":10765},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11428,\"start\":11424},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11615,\"start\":11611},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12364,\"start\":12360},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12716,\"start\":12715},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13296,\"start\":13292},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14757,\"start\":14753},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20413,\"start\":20409},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21938,\"start\":21934},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21944,\"start\":21940},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21955,\"start\":21951},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21985,\"start\":21981},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23177,\"start\":23173},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23491,\"start\":23487},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23547,\"start\":23543},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23630,\"start\":23626},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25274,\"start\":25271},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26036,\"start\":26032},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28492,\"start\":28488},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29732,\"start\":29728},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29738,\"start\":29734},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31052,\"start\":31048},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34050,\"start\":34046},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34056,\"start\":34052},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34262,\"start\":34258},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34475,\"start\":34471},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38932,\"start\":38928},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39114,\"start\":39110},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":41494,\"start\":41490},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":42956,\"start\":42952},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":44164,\"start\":44160},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":44851,\"start\":44847},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45000,\"start\":44996},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":45063,\"start\":45059},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":45587,\"start\":45583},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45593,\"start\":45589},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":45934,\"start\":45930},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":46374,\"start\":46370},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":46618,\"start\":46614},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":48463,\"start\":48459},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53158,\"start\":53154},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":53355,\"start\":53351},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":53364,\"start\":53360},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":53820,\"start\":53816},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":54183,\"start\":54179},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":56761,\"start\":56757},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":56767,\"start\":56763},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":56773,\"start\":56769},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":57036,\"start\":57032},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":57042,\"start\":57038},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":57087,\"start\":57083},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":57120,\"start\":57116},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":57192,\"start\":57188},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":57537,\"start\":57533},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":57560,\"start\":57556},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":57630,\"start\":57626},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":57649,\"start\":57645},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":57660,\"start\":57656},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":57948,\"start\":57944},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":59301,\"start\":59297},{\"end\":59491,\"start\":59487}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":60363,\"start\":60339},{\"attributes\":{\"id\":\"fig_1\"},\"end\":60428,\"start\":60364},{\"attributes\":{\"id\":\"fig_2\"},\"end\":60826,\"start\":60429},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60924,\"start\":60827},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61024,\"start\":60925},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61114,\"start\":61025},{\"attributes\":{\"id\":\"fig_6\"},\"end\":61201,\"start\":61115},{\"attributes\":{\"id\":\"fig_7\"},\"end\":61339,\"start\":61202},{\"attributes\":{\"id\":\"fig_8\"},\"end\":61546,\"start\":61340},{\"attributes\":{\"id\":\"fig_9\"},\"end\":62075,\"start\":61547},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62434,\"start\":62076},{\"attributes\":{\"id\":\"fig_11\"},\"end\":62776,\"start\":62435},{\"attributes\":{\"id\":\"fig_12\"},\"end\":62993,\"start\":62777},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":63025,\"start\":62994},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63125,\"start\":63026},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":63249,\"start\":63126}]", "paragraph": "[{\"end\":5220,\"start\":4010},{\"end\":8461,\"start\":5222},{\"end\":9371,\"start\":8463},{\"end\":11998,\"start\":9373},{\"end\":12934,\"start\":12000},{\"end\":14063,\"start\":12936},{\"end\":14786,\"start\":14065},{\"end\":15543,\"start\":14788},{\"end\":15904,\"start\":15545},{\"end\":16434,\"start\":15906},{\"end\":17991,\"start\":16478},{\"end\":19207,\"start\":18112},{\"end\":19642,\"start\":19297},{\"end\":19938,\"start\":19678},{\"end\":20274,\"start\":20173},{\"end\":20557,\"start\":20298},{\"end\":20785,\"start\":20684},{\"end\":21440,\"start\":20821},{\"end\":21833,\"start\":21487},{\"end\":23178,\"start\":21870},{\"end\":24264,\"start\":23180},{\"end\":24426,\"start\":24310},{\"end\":24938,\"start\":24460},{\"end\":25125,\"start\":24970},{\"end\":25195,\"start\":25160},{\"end\":25695,\"start\":25229},{\"end\":26248,\"start\":25725},{\"end\":26616,\"start\":26297},{\"end\":26805,\"start\":26675},{\"end\":27640,\"start\":26858},{\"end\":28938,\"start\":27666},{\"end\":29796,\"start\":28987},{\"end\":30170,\"start\":29798},{\"end\":30359,\"start\":30256},{\"end\":30461,\"start\":30361},{\"end\":30628,\"start\":30463},{\"end\":30815,\"start\":30694},{\"end\":31253,\"start\":30886},{\"end\":32903,\"start\":31307},{\"end\":33386,\"start\":32973},{\"end\":33615,\"start\":33388},{\"end\":34136,\"start\":33700},{\"end\":35176,\"start\":34210},{\"end\":35459,\"start\":35178},{\"end\":36404,\"start\":35644},{\"end\":37623,\"start\":36433},{\"end\":37988,\"start\":37647},{\"end\":38569,\"start\":38031},{\"end\":39115,\"start\":38571},{\"end\":39464,\"start\":39143},{\"end\":41010,\"start\":39466},{\"end\":43252,\"start\":41012},{\"end\":44289,\"start\":43290},{\"end\":45064,\"start\":44349},{\"end\":46264,\"start\":45066},{\"end\":47038,\"start\":46309},{\"end\":47393,\"start\":47068},{\"end\":48335,\"start\":47395},{\"end\":49830,\"start\":48337},{\"end\":50558,\"start\":49881},{\"end\":51790,\"start\":50560},{\"end\":52486,\"start\":51848},{\"end\":53299,\"start\":52488},{\"end\":53821,\"start\":53339},{\"end\":54081,\"start\":53823},{\"end\":54866,\"start\":54083},{\"end\":56276,\"start\":54884},{\"end\":57949,\"start\":56278},{\"end\":59129,\"start\":57983},{\"end\":59443,\"start\":59163},{\"end\":59625,\"start\":59445},{\"end\":60096,\"start\":59627},{\"end\":60312,\"start\":60169}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18111,\"start\":17992},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19296,\"start\":19208},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19677,\"start\":19643},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20172,\"start\":19939},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20297,\"start\":20275},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20683,\"start\":20558},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20820,\"start\":20786},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24309,\"start\":24265},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24459,\"start\":24427},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25159,\"start\":25126},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25228,\"start\":25196},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25724,\"start\":25696},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26296,\"start\":26249},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26674,\"start\":26617},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26857,\"start\":26806},{\"attributes\":{\"id\":\"formula_15\"},\"end\":27665,\"start\":27641},{\"attributes\":{\"id\":\"formula_16\"},\"end\":28986,\"start\":28939},{\"attributes\":{\"id\":\"formula_17\"},\"end\":30255,\"start\":30171},{\"attributes\":{\"id\":\"formula_18\"},\"end\":30693,\"start\":30629},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30885,\"start\":30816},{\"attributes\":{\"id\":\"formula_20\"},\"end\":31306,\"start\":31254},{\"attributes\":{\"id\":\"formula_21\"},\"end\":33699,\"start\":33616},{\"attributes\":{\"id\":\"formula_22\"},\"end\":34209,\"start\":34137},{\"attributes\":{\"id\":\"formula_23\"},\"end\":35643,\"start\":35460},{\"attributes\":{\"id\":\"formula_24\"},\"end\":38030,\"start\":37989},{\"attributes\":{\"id\":\"formula_27\"},\"end\":60168,\"start\":60097},{\"attributes\":{\"id\":\"formula_28\"},\"end\":60339,\"start\":60313}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42471,\"start\":42457}]", "section_header": "[{\"end\":4008,\"start\":3993},{\"end\":16476,\"start\":16437},{\"end\":21485,\"start\":21443},{\"end\":21868,\"start\":21836},{\"end\":24968,\"start\":24941},{\"end\":32971,\"start\":32906},{\"end\":36431,\"start\":36407},{\"end\":37645,\"start\":37626},{\"end\":39141,\"start\":39118},{\"end\":43288,\"start\":43255},{\"end\":44347,\"start\":44292},{\"end\":46307,\"start\":46267},{\"end\":47066,\"start\":47041},{\"end\":49879,\"start\":49833},{\"end\":51846,\"start\":51793},{\"end\":53337,\"start\":53302},{\"end\":54882,\"start\":54869},{\"end\":57981,\"start\":57952},{\"end\":59161,\"start\":59132},{\"end\":60348,\"start\":60340},{\"end\":60373,\"start\":60365},{\"end\":60438,\"start\":60430},{\"end\":60934,\"start\":60926},{\"end\":61034,\"start\":61026},{\"end\":61124,\"start\":61116},{\"end\":61211,\"start\":61203},{\"end\":61349,\"start\":61341},{\"end\":61556,\"start\":61548},{\"end\":62086,\"start\":62077},{\"end\":62445,\"start\":62436},{\"end\":62787,\"start\":62778},{\"end\":63002,\"start\":62995},{\"end\":63040,\"start\":63027},{\"end\":63147,\"start\":63127}]", "table": null, "figure_caption": "[{\"end\":60363,\"start\":60350},{\"end\":60428,\"start\":60375},{\"end\":60826,\"start\":60440},{\"end\":60924,\"start\":60829},{\"end\":61024,\"start\":60936},{\"end\":61114,\"start\":61036},{\"end\":61201,\"start\":61126},{\"end\":61339,\"start\":61213},{\"end\":61546,\"start\":61351},{\"end\":62075,\"start\":61558},{\"end\":62434,\"start\":62089},{\"end\":62776,\"start\":62448},{\"end\":62993,\"start\":62790},{\"end\":63025,\"start\":63004},{\"end\":63125,\"start\":63044},{\"end\":63249,\"start\":63151}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16511,\"start\":16505},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22786,\"start\":22780},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23198,\"start\":23192},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29062,\"start\":29056},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29201,\"start\":29195},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39463,\"start\":39457},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39490,\"start\":39484},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40406,\"start\":40400},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40499,\"start\":40493},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40685,\"start\":40678},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":42630,\"start\":42624},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":43527,\"start\":43521},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":43540,\"start\":43529},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45141,\"start\":45131},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":45720,\"start\":45710},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46465,\"start\":46447},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46689,\"start\":46679},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46816,\"start\":46806},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46952,\"start\":46942},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":47254,\"start\":47248},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":47421,\"start\":47415},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":48182,\"start\":48176},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":48740,\"start\":48734},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":49070,\"start\":49064},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50271,\"start\":50252},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50545,\"start\":50538},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50595,\"start\":50588},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51018,\"start\":51011},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52181,\"start\":52174},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52500,\"start\":52493},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52898,\"start\":52887},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58071,\"start\":58064},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58168,\"start\":58157},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58186,\"start\":58175},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58757,\"start\":58750}]", "bib_author_first_name": "[{\"end\":63538,\"start\":63537},{\"end\":63878,\"start\":63877},{\"end\":63884,\"start\":63883},{\"end\":64229,\"start\":64228},{\"end\":64231,\"start\":64230},{\"end\":64245,\"start\":64244},{\"end\":64254,\"start\":64253},{\"end\":64262,\"start\":64261},{\"end\":64264,\"start\":64263},{\"end\":64274,\"start\":64273},{\"end\":64281,\"start\":64280},{\"end\":64283,\"start\":64282},{\"end\":64676,\"start\":64675},{\"end\":64683,\"start\":64682},{\"end\":64689,\"start\":64688},{\"end\":64693,\"start\":64690},{\"end\":64699,\"start\":64698},{\"end\":64707,\"start\":64706},{\"end\":65021,\"start\":65020},{\"end\":65337,\"start\":65336},{\"end\":65343,\"start\":65342},{\"end\":65349,\"start\":65348},{\"end\":65355,\"start\":65354},{\"end\":65363,\"start\":65362},{\"end\":65376,\"start\":65375},{\"end\":65754,\"start\":65753},{\"end\":65762,\"start\":65761},{\"end\":65769,\"start\":65768},{\"end\":65777,\"start\":65776},{\"end\":66118,\"start\":66117},{\"end\":66125,\"start\":66124},{\"end\":66133,\"start\":66132},{\"end\":66140,\"start\":66139},{\"end\":66146,\"start\":66145},{\"end\":66525,\"start\":66524},{\"end\":66533,\"start\":66532},{\"end\":66540,\"start\":66539},{\"end\":66926,\"start\":66922},{\"end\":67231,\"start\":67230},{\"end\":67239,\"start\":67238},{\"end\":67246,\"start\":67245},{\"end\":67253,\"start\":67252},{\"end\":67265,\"start\":67264},{\"end\":67267,\"start\":67266},{\"end\":67626,\"start\":67625},{\"end\":67628,\"start\":67627},{\"end\":67642,\"start\":67637},{\"end\":68148,\"start\":68147},{\"end\":68158,\"start\":68157},{\"end\":68661,\"start\":68654},{\"end\":68671,\"start\":68670},{\"end\":68683,\"start\":68682},{\"end\":68694,\"start\":68693},{\"end\":68705,\"start\":68704},{\"end\":68718,\"start\":68714},{\"end\":69126,\"start\":69125},{\"end\":69133,\"start\":69132},{\"end\":69140,\"start\":69139},{\"end\":69144,\"start\":69141},{\"end\":69150,\"start\":69149},{\"end\":69510,\"start\":69506},{\"end\":69516,\"start\":69515},{\"end\":69821,\"start\":69820},{\"end\":69823,\"start\":69822},{\"end\":69832,\"start\":69831},{\"end\":69853,\"start\":69852},{\"end\":69867,\"start\":69866},{\"end\":70258,\"start\":70257},{\"end\":70265,\"start\":70264},{\"end\":70272,\"start\":70271},{\"end\":70281,\"start\":70280},{\"end\":70283,\"start\":70282},{\"end\":70295,\"start\":70291},{\"end\":70681,\"start\":70680},{\"end\":70954,\"start\":70953},{\"end\":70960,\"start\":70959},{\"end\":70973,\"start\":70972},{\"end\":70976,\"start\":70974},{\"end\":70985,\"start\":70984},{\"end\":71305,\"start\":71304},{\"end\":71312,\"start\":71311},{\"end\":71321,\"start\":71320},{\"end\":71330,\"start\":71329},{\"end\":71338,\"start\":71337},{\"end\":71344,\"start\":71343},{\"end\":71348,\"start\":71345},{\"end\":71754,\"start\":71753},{\"end\":71756,\"start\":71755},{\"end\":71765,\"start\":71764},{\"end\":71767,\"start\":71766},{\"end\":72073,\"start\":72072},{\"end\":72089,\"start\":72088},{\"end\":72095,\"start\":72094},{\"end\":72334,\"start\":72333},{\"end\":72345,\"start\":72344},{\"end\":72357,\"start\":72356},{\"end\":72697,\"start\":72696},{\"end\":72706,\"start\":72705},{\"end\":72713,\"start\":72712},{\"end\":72721,\"start\":72720},{\"end\":72729,\"start\":72728},{\"end\":73052,\"start\":73051},{\"end\":73061,\"start\":73057},{\"end\":73068,\"start\":73067},{\"end\":73075,\"start\":73074},{\"end\":73077,\"start\":73076},{\"end\":73352,\"start\":73351},{\"end\":73358,\"start\":73357},{\"end\":73367,\"start\":73366},{\"end\":73374,\"start\":73373},{\"end\":73693,\"start\":73692},{\"end\":73701,\"start\":73700},{\"end\":73703,\"start\":73702},{\"end\":74025,\"start\":74024},{\"end\":74034,\"start\":74033},{\"end\":74380,\"start\":74379},{\"end\":74614,\"start\":74613},{\"end\":74616,\"start\":74615},{\"end\":74912,\"start\":74911},{\"end\":74914,\"start\":74913},{\"end\":75125,\"start\":75124},{\"end\":75136,\"start\":75135},{\"end\":75138,\"start\":75137},{\"end\":75368,\"start\":75367},{\"end\":75375,\"start\":75374},{\"end\":75386,\"start\":75385},{\"end\":75397,\"start\":75396},{\"end\":75399,\"start\":75398},{\"end\":75736,\"start\":75735},{\"end\":75919,\"start\":75918},{\"end\":75933,\"start\":75932},{\"end\":76137,\"start\":76136},{\"end\":76149,\"start\":76148},{\"end\":76158,\"start\":76157},{\"end\":76168,\"start\":76167},{\"end\":76342,\"start\":76341},{\"end\":76344,\"start\":76343},{\"end\":76356,\"start\":76355},{\"end\":76358,\"start\":76357},{\"end\":76575,\"start\":76574},{\"end\":76586,\"start\":76585},{\"end\":76595,\"start\":76594},{\"end\":76856,\"start\":76855},{\"end\":76868,\"start\":76867},{\"end\":76878,\"start\":76877},{\"end\":77198,\"start\":77197},{\"end\":77204,\"start\":77203},{\"end\":77206,\"start\":77205},{\"end\":77215,\"start\":77211},{\"end\":77526,\"start\":77525},{\"end\":77532,\"start\":77531},{\"end\":77539,\"start\":77538},{\"end\":77548,\"start\":77547},{\"end\":77555,\"start\":77554},{\"end\":77557,\"start\":77556},{\"end\":77902,\"start\":77901},{\"end\":78218,\"start\":78217},{\"end\":78227,\"start\":78226},{\"end\":78235,\"start\":78234},{\"end\":78242,\"start\":78241},{\"end\":78605,\"start\":78604},{\"end\":78936,\"start\":78935},{\"end\":78943,\"start\":78942},{\"end\":78950,\"start\":78949},{\"end\":78954,\"start\":78951},{\"end\":78960,\"start\":78959},{\"end\":78971,\"start\":78967},{\"end\":79297,\"start\":79296},{\"end\":79311,\"start\":79310},{\"end\":79325,\"start\":79324},{\"end\":79339,\"start\":79338},{\"end\":79355,\"start\":79354},{\"end\":79692,\"start\":79691},{\"end\":79694,\"start\":79693},{\"end\":79934,\"start\":79933},{\"end\":79941,\"start\":79940},{\"end\":79951,\"start\":79947},{\"end\":79958,\"start\":79957},{\"end\":80301,\"start\":80300},{\"end\":80308,\"start\":80307},{\"end\":80315,\"start\":80314},{\"end\":80324,\"start\":80323},{\"end\":80661,\"start\":80660},{\"end\":80663,\"start\":80662},{\"end\":80672,\"start\":80671},{\"end\":80692,\"start\":80691},{\"end\":80702,\"start\":80701},{\"end\":80709,\"start\":80708},{\"end\":80711,\"start\":80710},{\"end\":81042,\"start\":81041},{\"end\":81054,\"start\":81053},{\"end\":81056,\"start\":81055},{\"end\":81067,\"start\":81066},{\"end\":81069,\"start\":81068},{\"end\":81360,\"start\":81359},{\"end\":81368,\"start\":81367},{\"end\":81376,\"start\":81375},{\"end\":81383,\"start\":81382},{\"end\":81385,\"start\":81384},{\"end\":81695,\"start\":81694},{\"end\":81708,\"start\":81707},{\"end\":81718,\"start\":81717}]", "bib_author_last_name": "[{\"end\":63543,\"start\":63539},{\"end\":63881,\"start\":63879},{\"end\":63890,\"start\":63885},{\"end\":64242,\"start\":64232},{\"end\":64251,\"start\":64246},{\"end\":64259,\"start\":64255},{\"end\":64271,\"start\":64265},{\"end\":64278,\"start\":64275},{\"end\":64286,\"start\":64284},{\"end\":64680,\"start\":64677},{\"end\":64686,\"start\":64684},{\"end\":64696,\"start\":64694},{\"end\":64704,\"start\":64700},{\"end\":64712,\"start\":64708},{\"end\":65025,\"start\":65022},{\"end\":65340,\"start\":65338},{\"end\":65346,\"start\":65344},{\"end\":65352,\"start\":65350},{\"end\":65360,\"start\":65356},{\"end\":65373,\"start\":65364},{\"end\":65381,\"start\":65377},{\"end\":65759,\"start\":65755},{\"end\":65766,\"start\":65763},{\"end\":65774,\"start\":65770},{\"end\":65783,\"start\":65778},{\"end\":66122,\"start\":66119},{\"end\":66130,\"start\":66126},{\"end\":66137,\"start\":66134},{\"end\":66143,\"start\":66141},{\"end\":66151,\"start\":66147},{\"end\":66530,\"start\":66526},{\"end\":66537,\"start\":66534},{\"end\":66544,\"start\":66541},{\"end\":66931,\"start\":66927},{\"end\":67236,\"start\":67232},{\"end\":67243,\"start\":67240},{\"end\":67250,\"start\":67247},{\"end\":67262,\"start\":67254},{\"end\":67280,\"start\":67268},{\"end\":67635,\"start\":67629},{\"end\":67651,\"start\":67643},{\"end\":68155,\"start\":68149},{\"end\":68168,\"start\":68159},{\"end\":68668,\"start\":68662},{\"end\":68680,\"start\":68672},{\"end\":68691,\"start\":68684},{\"end\":68702,\"start\":68695},{\"end\":68712,\"start\":68706},{\"end\":68726,\"start\":68719},{\"end\":69130,\"start\":69127},{\"end\":69137,\"start\":69134},{\"end\":69147,\"start\":69145},{\"end\":69155,\"start\":69151},{\"end\":69513,\"start\":69511},{\"end\":69521,\"start\":69517},{\"end\":69829,\"start\":69824},{\"end\":69850,\"start\":69833},{\"end\":69864,\"start\":69854},{\"end\":69879,\"start\":69868},{\"end\":70262,\"start\":70259},{\"end\":70269,\"start\":70266},{\"end\":70278,\"start\":70273},{\"end\":70289,\"start\":70284},{\"end\":70303,\"start\":70296},{\"end\":70685,\"start\":70682},{\"end\":70957,\"start\":70955},{\"end\":70970,\"start\":70961},{\"end\":70982,\"start\":70977},{\"end\":70992,\"start\":70986},{\"end\":71309,\"start\":71306},{\"end\":71318,\"start\":71313},{\"end\":71327,\"start\":71322},{\"end\":71335,\"start\":71331},{\"end\":71341,\"start\":71339},{\"end\":71351,\"start\":71349},{\"end\":71762,\"start\":71757},{\"end\":71773,\"start\":71768},{\"end\":72086,\"start\":72074},{\"end\":72092,\"start\":72090},{\"end\":72103,\"start\":72096},{\"end\":72342,\"start\":72335},{\"end\":72354,\"start\":72346},{\"end\":72363,\"start\":72358},{\"end\":72703,\"start\":72698},{\"end\":72710,\"start\":72707},{\"end\":72718,\"start\":72714},{\"end\":72726,\"start\":72722},{\"end\":72735,\"start\":72730},{\"end\":73055,\"start\":73053},{\"end\":73065,\"start\":73062},{\"end\":73072,\"start\":73069},{\"end\":73080,\"start\":73078},{\"end\":73355,\"start\":73353},{\"end\":73364,\"start\":73359},{\"end\":73371,\"start\":73368},{\"end\":73378,\"start\":73375},{\"end\":73698,\"start\":73694},{\"end\":73710,\"start\":73704},{\"end\":74031,\"start\":74026},{\"end\":74042,\"start\":74035},{\"end\":74389,\"start\":74381},{\"end\":74624,\"start\":74617},{\"end\":74922,\"start\":74915},{\"end\":75133,\"start\":75126},{\"end\":75146,\"start\":75139},{\"end\":75372,\"start\":75369},{\"end\":75383,\"start\":75376},{\"end\":75394,\"start\":75387},{\"end\":75407,\"start\":75400},{\"end\":75742,\"start\":75737},{\"end\":75930,\"start\":75920},{\"end\":75940,\"start\":75934},{\"end\":76146,\"start\":76138},{\"end\":76155,\"start\":76150},{\"end\":76165,\"start\":76159},{\"end\":76175,\"start\":76169},{\"end\":76353,\"start\":76345},{\"end\":76362,\"start\":76359},{\"end\":76583,\"start\":76576},{\"end\":76592,\"start\":76587},{\"end\":76604,\"start\":76596},{\"end\":76865,\"start\":76857},{\"end\":76875,\"start\":76869},{\"end\":76890,\"start\":76879},{\"end\":77201,\"start\":77199},{\"end\":77209,\"start\":77207},{\"end\":77221,\"start\":77216},{\"end\":77529,\"start\":77527},{\"end\":77536,\"start\":77533},{\"end\":77545,\"start\":77540},{\"end\":77552,\"start\":77549},{\"end\":77560,\"start\":77558},{\"end\":77906,\"start\":77903},{\"end\":78224,\"start\":78219},{\"end\":78232,\"start\":78228},{\"end\":78239,\"start\":78236},{\"end\":78247,\"start\":78243},{\"end\":78608,\"start\":78606},{\"end\":78940,\"start\":78937},{\"end\":78947,\"start\":78944},{\"end\":78957,\"start\":78955},{\"end\":78965,\"start\":78961},{\"end\":78977,\"start\":78972},{\"end\":79308,\"start\":79298},{\"end\":79322,\"start\":79312},{\"end\":79336,\"start\":79326},{\"end\":79352,\"start\":79340},{\"end\":79363,\"start\":79356},{\"end\":79703,\"start\":79695},{\"end\":79938,\"start\":79935},{\"end\":79945,\"start\":79942},{\"end\":79955,\"start\":79952},{\"end\":79962,\"start\":79959},{\"end\":80305,\"start\":80302},{\"end\":80312,\"start\":80309},{\"end\":80321,\"start\":80316},{\"end\":80329,\"start\":80325},{\"end\":80669,\"start\":80664},{\"end\":80689,\"start\":80673},{\"end\":80699,\"start\":80693},{\"end\":80706,\"start\":80703},{\"end\":80718,\"start\":80712},{\"end\":81051,\"start\":81043},{\"end\":81064,\"start\":81057},{\"end\":81078,\"start\":81070},{\"end\":81365,\"start\":81361},{\"end\":81373,\"start\":81369},{\"end\":81380,\"start\":81377},{\"end\":81390,\"start\":81386},{\"end\":81705,\"start\":81696},{\"end\":81715,\"start\":81709},{\"end\":81724,\"start\":81719}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":219473483},\"end\":63776,\"start\":63440},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52946522},\"end\":64118,\"start\":63778},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218487811},\"end\":64582,\"start\":64120},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":214727900},\"end\":64944,\"start\":64584},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":201667622},\"end\":65220,\"start\":64946},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227335740},\"end\":65659,\"start\":65222},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":248904269},\"end\":66020,\"start\":65661},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":246453712},\"end\":66396,\"start\":66022},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":222297089},\"end\":66810,\"start\":66398},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":238994243},\"end\":67160,\"start\":66812},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":235790778},\"end\":67502,\"start\":67162},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":202712921},\"end\":68016,\"start\":67504},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":146013685},\"end\":68538,\"start\":68018},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":174802553},\"end\":69008,\"start\":68540},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":225304503},\"end\":69419,\"start\":69010},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":159040601},\"end\":69729,\"start\":69421},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":210966290},\"end\":70141,\"start\":69731},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":219259605},\"end\":70576,\"start\":70143},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":245006186},\"end\":70903,\"start\":70578},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":236447568},\"end\":71192,\"start\":70905},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":238993374},\"end\":71627,\"start\":71194},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":232266474},\"end\":72010,\"start\":71629},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b22\"},\"end\":72246,\"start\":72012},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7365231},\"end\":72615,\"start\":72248},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":996788},\"end\":72967,\"start\":72617},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3616586},\"end\":73303,\"start\":72969},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206594692},\"end\":73628,\"start\":73305},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15539264},\"end\":73928,\"start\":73630},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5808102},\"end\":74317,\"start\":73930},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3846544},\"end\":74568,\"start\":74319},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":26161481},\"end\":74819,\"start\":74570},{\"attributes\":{\"id\":\"b31\"},\"end\":75076,\"start\":74821},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14994083},\"end\":75287,\"start\":75078},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3621851},\"end\":75673,\"start\":75289},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b34\"},\"end\":75862,\"start\":75675},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b35\"},\"end\":76066,\"start\":75864},{\"attributes\":{\"doi\":\"arXiv:2112.03215\",\"id\":\"b36\"},\"end\":76339,\"start\":76068},{\"attributes\":{\"id\":\"b37\"},\"end\":76535,\"start\":76341},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5034926},\"end\":76787,\"start\":76537},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":29591305},\"end\":77111,\"start\":76789},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3412710},\"end\":77440,\"start\":77113},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":233227711},\"end\":77797,\"start\":77442},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":237303970},\"end\":78126,\"start\":77799},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":250598495},\"end\":78475,\"start\":78128},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":246507705},\"end\":78850,\"start\":78477},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":221641014},\"end\":79224,\"start\":78852},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":236447720},\"end\":79643,\"start\":79226},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":43657306},\"end\":79859,\"start\":79645},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":247537331},\"end\":80192,\"start\":79861},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":208176423},\"end\":80580,\"start\":80194},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":8323234},\"end\":80988,\"start\":80582},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":8998398},\"end\":81260,\"start\":80990},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4764988},\"end\":81634,\"start\":81262},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":221319407},\"end\":81918,\"start\":81636}]", "bib_title": "[{\"end\":63535,\"start\":63440},{\"end\":63875,\"start\":63778},{\"end\":64226,\"start\":64120},{\"end\":64673,\"start\":64584},{\"end\":65018,\"start\":64946},{\"end\":65334,\"start\":65222},{\"end\":65751,\"start\":65661},{\"end\":66115,\"start\":66022},{\"end\":66522,\"start\":66398},{\"end\":66920,\"start\":66812},{\"end\":67228,\"start\":67162},{\"end\":67623,\"start\":67504},{\"end\":68145,\"start\":68018},{\"end\":68652,\"start\":68540},{\"end\":69123,\"start\":69010},{\"end\":69504,\"start\":69421},{\"end\":69818,\"start\":69731},{\"end\":70255,\"start\":70143},{\"end\":70678,\"start\":70578},{\"end\":70951,\"start\":70905},{\"end\":71302,\"start\":71194},{\"end\":71751,\"start\":71629},{\"end\":72331,\"start\":72248},{\"end\":72694,\"start\":72617},{\"end\":73049,\"start\":72969},{\"end\":73349,\"start\":73305},{\"end\":73690,\"start\":73630},{\"end\":74022,\"start\":73930},{\"end\":74377,\"start\":74319},{\"end\":74611,\"start\":74570},{\"end\":75122,\"start\":75078},{\"end\":75365,\"start\":75289},{\"end\":76572,\"start\":76537},{\"end\":76853,\"start\":76789},{\"end\":77195,\"start\":77113},{\"end\":77523,\"start\":77442},{\"end\":77899,\"start\":77799},{\"end\":78215,\"start\":78128},{\"end\":78602,\"start\":78477},{\"end\":78933,\"start\":78852},{\"end\":79294,\"start\":79226},{\"end\":79689,\"start\":79645},{\"end\":79931,\"start\":79861},{\"end\":80298,\"start\":80194},{\"end\":80658,\"start\":80582},{\"end\":81039,\"start\":80990},{\"end\":81357,\"start\":81262},{\"end\":81692,\"start\":81636}]", "bib_author": "[{\"end\":63545,\"start\":63537},{\"end\":63883,\"start\":63877},{\"end\":63892,\"start\":63883},{\"end\":64244,\"start\":64228},{\"end\":64253,\"start\":64244},{\"end\":64261,\"start\":64253},{\"end\":64273,\"start\":64261},{\"end\":64280,\"start\":64273},{\"end\":64288,\"start\":64280},{\"end\":64682,\"start\":64675},{\"end\":64688,\"start\":64682},{\"end\":64698,\"start\":64688},{\"end\":64706,\"start\":64698},{\"end\":64714,\"start\":64706},{\"end\":65027,\"start\":65020},{\"end\":65342,\"start\":65336},{\"end\":65348,\"start\":65342},{\"end\":65354,\"start\":65348},{\"end\":65362,\"start\":65354},{\"end\":65375,\"start\":65362},{\"end\":65383,\"start\":65375},{\"end\":65761,\"start\":65753},{\"end\":65768,\"start\":65761},{\"end\":65776,\"start\":65768},{\"end\":65785,\"start\":65776},{\"end\":66124,\"start\":66117},{\"end\":66132,\"start\":66124},{\"end\":66139,\"start\":66132},{\"end\":66145,\"start\":66139},{\"end\":66153,\"start\":66145},{\"end\":66532,\"start\":66524},{\"end\":66539,\"start\":66532},{\"end\":66546,\"start\":66539},{\"end\":66933,\"start\":66922},{\"end\":67238,\"start\":67230},{\"end\":67245,\"start\":67238},{\"end\":67252,\"start\":67245},{\"end\":67264,\"start\":67252},{\"end\":67282,\"start\":67264},{\"end\":67637,\"start\":67625},{\"end\":67653,\"start\":67637},{\"end\":68157,\"start\":68147},{\"end\":68170,\"start\":68157},{\"end\":68670,\"start\":68654},{\"end\":68682,\"start\":68670},{\"end\":68693,\"start\":68682},{\"end\":68704,\"start\":68693},{\"end\":68714,\"start\":68704},{\"end\":68728,\"start\":68714},{\"end\":69132,\"start\":69125},{\"end\":69139,\"start\":69132},{\"end\":69149,\"start\":69139},{\"end\":69157,\"start\":69149},{\"end\":69515,\"start\":69506},{\"end\":69523,\"start\":69515},{\"end\":69831,\"start\":69820},{\"end\":69852,\"start\":69831},{\"end\":69866,\"start\":69852},{\"end\":69881,\"start\":69866},{\"end\":70264,\"start\":70257},{\"end\":70271,\"start\":70264},{\"end\":70280,\"start\":70271},{\"end\":70291,\"start\":70280},{\"end\":70305,\"start\":70291},{\"end\":70687,\"start\":70680},{\"end\":70959,\"start\":70953},{\"end\":70972,\"start\":70959},{\"end\":70984,\"start\":70972},{\"end\":70994,\"start\":70984},{\"end\":71311,\"start\":71304},{\"end\":71320,\"start\":71311},{\"end\":71329,\"start\":71320},{\"end\":71337,\"start\":71329},{\"end\":71343,\"start\":71337},{\"end\":71353,\"start\":71343},{\"end\":71764,\"start\":71753},{\"end\":71775,\"start\":71764},{\"end\":72088,\"start\":72072},{\"end\":72094,\"start\":72088},{\"end\":72105,\"start\":72094},{\"end\":72344,\"start\":72333},{\"end\":72356,\"start\":72344},{\"end\":72365,\"start\":72356},{\"end\":72705,\"start\":72696},{\"end\":72712,\"start\":72705},{\"end\":72720,\"start\":72712},{\"end\":72728,\"start\":72720},{\"end\":72737,\"start\":72728},{\"end\":73057,\"start\":73051},{\"end\":73067,\"start\":73057},{\"end\":73074,\"start\":73067},{\"end\":73082,\"start\":73074},{\"end\":73357,\"start\":73351},{\"end\":73366,\"start\":73357},{\"end\":73373,\"start\":73366},{\"end\":73380,\"start\":73373},{\"end\":73700,\"start\":73692},{\"end\":73712,\"start\":73700},{\"end\":74033,\"start\":74024},{\"end\":74044,\"start\":74033},{\"end\":74391,\"start\":74379},{\"end\":74626,\"start\":74613},{\"end\":74924,\"start\":74911},{\"end\":75135,\"start\":75124},{\"end\":75148,\"start\":75135},{\"end\":75374,\"start\":75367},{\"end\":75385,\"start\":75374},{\"end\":75396,\"start\":75385},{\"end\":75409,\"start\":75396},{\"end\":75744,\"start\":75735},{\"end\":75932,\"start\":75918},{\"end\":75942,\"start\":75932},{\"end\":76148,\"start\":76136},{\"end\":76157,\"start\":76148},{\"end\":76167,\"start\":76157},{\"end\":76177,\"start\":76167},{\"end\":76355,\"start\":76341},{\"end\":76364,\"start\":76355},{\"end\":76585,\"start\":76574},{\"end\":76594,\"start\":76585},{\"end\":76606,\"start\":76594},{\"end\":76867,\"start\":76855},{\"end\":76877,\"start\":76867},{\"end\":76892,\"start\":76877},{\"end\":77203,\"start\":77197},{\"end\":77211,\"start\":77203},{\"end\":77223,\"start\":77211},{\"end\":77531,\"start\":77525},{\"end\":77538,\"start\":77531},{\"end\":77547,\"start\":77538},{\"end\":77554,\"start\":77547},{\"end\":77562,\"start\":77554},{\"end\":77908,\"start\":77901},{\"end\":78226,\"start\":78217},{\"end\":78234,\"start\":78226},{\"end\":78241,\"start\":78234},{\"end\":78249,\"start\":78241},{\"end\":78610,\"start\":78604},{\"end\":78942,\"start\":78935},{\"end\":78949,\"start\":78942},{\"end\":78959,\"start\":78949},{\"end\":78967,\"start\":78959},{\"end\":78979,\"start\":78967},{\"end\":79310,\"start\":79296},{\"end\":79324,\"start\":79310},{\"end\":79338,\"start\":79324},{\"end\":79354,\"start\":79338},{\"end\":79365,\"start\":79354},{\"end\":79705,\"start\":79691},{\"end\":79940,\"start\":79933},{\"end\":79947,\"start\":79940},{\"end\":79957,\"start\":79947},{\"end\":79964,\"start\":79957},{\"end\":80307,\"start\":80300},{\"end\":80314,\"start\":80307},{\"end\":80323,\"start\":80314},{\"end\":80331,\"start\":80323},{\"end\":80671,\"start\":80660},{\"end\":80691,\"start\":80671},{\"end\":80701,\"start\":80691},{\"end\":80708,\"start\":80701},{\"end\":80720,\"start\":80708},{\"end\":81053,\"start\":81041},{\"end\":81066,\"start\":81053},{\"end\":81080,\"start\":81066},{\"end\":81367,\"start\":81359},{\"end\":81375,\"start\":81367},{\"end\":81382,\"start\":81375},{\"end\":81392,\"start\":81382},{\"end\":81707,\"start\":81694},{\"end\":81717,\"start\":81707},{\"end\":81726,\"start\":81717}]", "bib_venue": "[{\"end\":67769,\"start\":67717},{\"end\":68286,\"start\":68234},{\"end\":72429,\"start\":72403},{\"end\":73468,\"start\":73428},{\"end\":73770,\"start\":73745},{\"end\":74128,\"start\":74090},{\"end\":74427,\"start\":74413},{\"end\":74672,\"start\":74655},{\"end\":75469,\"start\":75443},{\"end\":76409,\"start\":76394},{\"end\":79429,\"start\":79403},{\"end\":63570,\"start\":63545},{\"end\":63919,\"start\":63892},{\"end\":64318,\"start\":64288},{\"end\":64732,\"start\":64714},{\"end\":65054,\"start\":65027},{\"end\":65410,\"start\":65383},{\"end\":65811,\"start\":65785},{\"end\":66179,\"start\":66153},{\"end\":66573,\"start\":66546},{\"end\":66957,\"start\":66933},{\"end\":67308,\"start\":67282},{\"end\":67715,\"start\":67653},{\"end\":68232,\"start\":68170},{\"end\":68752,\"start\":68728},{\"end\":69184,\"start\":69157},{\"end\":69549,\"start\":69523},{\"end\":69907,\"start\":69881},{\"end\":70329,\"start\":70305},{\"end\":70711,\"start\":70687},{\"end\":71020,\"start\":70994},{\"end\":71377,\"start\":71353},{\"end\":71799,\"start\":71775},{\"end\":72070,\"start\":72012},{\"end\":72401,\"start\":72365},{\"end\":72762,\"start\":72737},{\"end\":73108,\"start\":73082},{\"end\":73426,\"start\":73380},{\"end\":73743,\"start\":73712},{\"end\":74088,\"start\":74044},{\"end\":74411,\"start\":74391},{\"end\":74653,\"start\":74626},{\"end\":74909,\"start\":74821},{\"end\":75159,\"start\":75148},{\"end\":75441,\"start\":75409},{\"end\":75733,\"start\":75675},{\"end\":75916,\"start\":75864},{\"end\":76134,\"start\":76068},{\"end\":76392,\"start\":76364},{\"end\":76632,\"start\":76606},{\"end\":76920,\"start\":76892},{\"end\":77249,\"start\":77223},{\"end\":77589,\"start\":77562},{\"end\":77933,\"start\":77908},{\"end\":78271,\"start\":78249},{\"end\":78634,\"start\":78610},{\"end\":79006,\"start\":78979},{\"end\":79401,\"start\":79365},{\"end\":79728,\"start\":79705},{\"end\":79998,\"start\":79964},{\"end\":80358,\"start\":80331},{\"end\":80754,\"start\":80720},{\"end\":81096,\"start\":81080},{\"end\":81417,\"start\":81392},{\"end\":81756,\"start\":81726}]"}}}, "year": 2023, "month": 12, "day": 17}