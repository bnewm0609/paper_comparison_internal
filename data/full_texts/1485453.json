{"id": 1485453, "updated": "2023-10-08 13:46:00.709", "metadata": {"title": "Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal", "authors": "[{\"first\":\"Jian\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Wenfei\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Zongben\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Jean\",\"last\":\"Ponce\",\"middle\":[]}]", "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2015, "month": 3, "day": 2}, "abstract": "In this paper, we address the problem of estimating and removing non-uniform motion blur from a single blurry image. We propose a deep learning approach to predicting the probabilistic distribution of motion blur at the patch level using a convolutional neural network (CNN). We further extend the candidate set of motion kernels predicted by the CNN using carefully designed image rotations. A Markov random field model is then used to infer a dense non-uniform motion blur field enforcing motion smoothness. Finally, motion blur is removed by a non-uniform deblurring model using patch-level image prior. Experimental evaluations show that our approach can effectively estimate and remove complex non-uniform motion blur that is not handled well by previous approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1503.00593", "mag": "2951529889", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/0001CXP15", "doi": "10.1109/cvpr.2015.7298677"}}, "content": {"source": {"pdf_hash": "f6eebe0ec5959dcaef97c4e52444b472a6d46f25", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1503.00593v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1503.00593", "status": "GREEN"}}, "grobid": {"id": "9aa73fa1ef26283e5875719c7cad71f6714e3a0e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f6eebe0ec5959dcaef97c4e52444b472a6d46f25.txt", "contents": "\nLearning a Convolutional Neural Network for Non-uniform Motion Blur Removal\n\n\nJian Sun \nXi'an Jiaotong University\n2\u00c9\n\ncole Normale Sup\u00e9rieure / PSL Research University\n\n\nWenfei Cao \nXi'an Jiaotong University\n2\u00c9\n\ncole Normale Sup\u00e9rieure / PSL Research University\n\n\nZongben Xu \nXi'an Jiaotong University\n2\u00c9\n\ncole Normale Sup\u00e9rieure / PSL Research University\n\n\nJean Ponce \nLearning a Convolutional Neural Network for Non-uniform Motion Blur Removal\n\nIn this paper, we address the problem of estimating and removing non-uniform motion blur from a single blurry image. We propose a deep learning approach to predicting the probabilistic distribution of motion blur at the patch level using a convolutional neural network (CNN). We further extend the candidate set of motion kernels predicted by the CNN using carefully designed image rotations. A Markov random field model is then used to infer a dense non-uniform motion blur field enforcing motion smoothness. Finally, motion blur is removed by a non-uniform deblurring model using patch-level image prior. Experimental evaluations show that our approach can effectively estimate and remove complex non-uniform motion blur that is not handled well by previous approaches.\n\nIntroduction\n\nImage deblurring [2,6,9,11,14,19,23,30] aims at recovering sharp image from a blurry image due to camera shake, object motion or out-of-focus. In this paper, we focus on estimating and removing spatially varying motion blur.\n\nNon-uniform deblurring [10,13,20] has attracted much attention in recent years. Methods in [7,8,26,31] work on non-uniform blur caused by camera rotations, in-plane translations or forward out-of-plane translations. They are effective for removing non-uniform blur consistent with these motion assumptions. Another category of approaches works on non-uniform motion blur caused by object motion. They estimate blur kernels by analyzing image statistics [17], blur spectrum [1], or with a learning approach using hand-crafted features [3]. Other approaches [13,29] jointly estimate the sharp image and blur kernels using a sparsity prior. It is still challenging today to remove strongly non-uniform motion blur captured in complex scenes.\n\nIn this work, we propose a novel deep learning-based approach to estimating non-uniform motion blur, followed by a patch statistics-based deblurring model adapted to nonuniform motion blur. We estimate the probabilities of motion kernels at the patch level using a convolutional neural network (CNN) [5,12,15,16], then fuse the patch-based estimations into a dense field of motion kernels using a Markov random field (MRF) model. To fully utilize the CNN, we propose to extend the candidate motion kernel set predicted by CNN using an image rotation technique, which significantly boost its performance for motion kernel estimation. Taking advantage of the strong feature learning power of CNNs, we can well predict the challenging nonuniform motion blur that can hardly be well estimated by the state-of-the-art approaches. Figure 1 illustrates our approach. Given a blurry image, we first estimate non-uniform motion blur field by a CNN model, then we deconvolve the blurry image. Our approach can effectively estimate the spatially varying motion kernels, which enable us to well remove the motion blur.\n\n\nRelated Work\n\nEstimating accurate motion blur kernels is essential to non-uniform image deblurring. In [7,8,25,26,31], nonuniform motion blur is modeled as a global camera motion, which basically estimates an uniform kernel in the camera motion space. Methods in [10,13,29] jointly estimate the motion kernels and sharp image. They rely on a sparsity prior to infer the latent sharp image for better motion kernel estimation. Different to them, we estimate motion blur kernels directly using the local patches, which does not require the estimation of camera motion or a latent sharp image.\n\nAnother category of approaches [1,4] estimates spatially varying motion blur based on local image features. The method in [1] estimates motion blur based on blur spectrum analysis of image patch in Fourier transform space. [17] predicts motion blur kernel using natural image statistics. [4] estimates motion blur by analyzing the alpha maps of image edges. [3] learns a regression function to predict motion blur kernel based on some hand-crafted features. Different to them, we estimate motion blur kernels using a convolutional neural network, followed by a carefully designed motion kernel extension method and MRF model to predict  a dense field of motion kernels. Our approach can well estimate complex and strong motion blur, which can hardly be well estimated by the previous approaches.\n\nRecently, there has been some related work on learningbased deblurring approaches. [21] proposes a discriminative deblurring approach using cascade of Gaussian CRF models for uniform blur removal. [22] proposes a neural network approach for learning a denoiser to suppress noises during deconvolution. [28] designs an image deconvolution neural network for non-blind deconvolution. These approaches above focus on designing better learning-based model for uniform blur removal. Our approach works on a more challenging task of non-uniform motion blur estimation and removal. Our CNN-based approach provides an effective method for solving this problem.\n\n\nLearning a CNN for Motion Blur Estimation\n\nWe propose to estimate spatially-varying motion blur kernels using a convolutional neural network. The basic idea is that we first predict the probabilities of different motion kernels for each image patch. Then we estimate dense motion blur kernels for the whole image using a Markov random field model enforcing motion smoothness. Before giving the details of our approach, let us first introduce our general formulation for non-uniform motion blur. We consider non-uniform image blur caused by object or camera motion. Given a blurry image I, we represent the local motion blur kernel at an image pixel p \u2208 \u2126 (\u2126 is the image region) by a motion vector m p = (l p , o p ), which characterizes the length and orientation of the motion field in p when the camera shutter is open. As shown in Fig. 2(a), each motion vector determines a motion kernel with nonzero values only along the motion trace. The blurry image can then be represented by I = k(M ) * I 0 , i.e., the convolution of a latent sharp image I 0 with the non-uniform motion blur kernels k(M ) determined by the motion field M = {m p } p\u2208\u2126 .\n\nIn the following paragraph, we also represent the motion vector m p as (u p , v p ) in Cartesian coordinate system based on the transform:\nu p = l p cos(o p ), v p = l p sin(o p ).(1)\nThe estimation of spatially-varying motion blur kernels is equivalent to estimating the motion field 1 from a single blurry image. In our approach, we do not make any global parametric assumptions (e.g., homography) on the motion, therefore the motion kernel estimation is challenging, and we only use local image regions for predicting these kernels.\n\n\nPatch-level Motion Kernel Estimation by CNN\n\nWe now present our approach to predicting motion blur kernels (or equivalently, the motion vector) at the patch level. We decompose the image into overlapping patches of size 30 \u00d7 30. Given a blurry patch \u03a8 p centered at pixel p, we aim to predict the probabilistic distribution of motion kernels:\nP (m = (l, o)|\u03a8 p )(2)\nfor all l \u2208 S l and o \u2208 S o , S l and S o are the sets of motion lengths and orientations respectively. In the followings, we  call this distribution as motion distribution.\n\nTaking the problem of motion kernel estimation as a learning problem, we utilize convolutional neural network to learn the effective features for predicting motion distributions in Eqn. (2). We generate a set of candidate motion kernels by discretizing the motion space, i.e., the ranges of length and orientation of the motion vectors. In our implementation, we discretize the range of motion length into 13 samples from l = 1 to 25 with interval of two, and discretize the range of motion orientation [0, 180 \u2022 ) into 6 samples from 0 \u2022 to 150 \u2022 with interval of 30 \u2022 . Note that when the motion length l = 1, all motion vectors correspond to the same blur kernel (i.e., identity kernel) on image grid regardless of the motion orientation. We therefore generate 73 candidate motion vectors (shown in Fig. 2(c)) in different combinations of motion lengths and orientations. We denote the above set of motion kernel candidates as S and the sets of motion lengths and motion orientations as S l and S o respectively. Obviously, these candidate motion vectors are far from dense in the continuous motion space. In Section 2.2 we will show how to extend the motion kernels of CNN to predict motion kernels outside the set S.\n\nGiven the candidate motion kernel set S, we next construct and learn CNN for predicting the motion distribution over S given a blurry patch. The convolutional neural network is constructed as follows. As shown in Fig. 3, the network has six layers:\nC1 \u2212 M 2 \u2212 C3 \u2212 M 4 \u2212 F 5 \u2212 S6. C1\nis a convolutional layer using filters (7 \u00d7 7 \u00d7 3) followed by ReLU (i.e., f (x) = max(x, 0) [15]) non-linear transform; M 2 is a max-pooling layer over 2 \u00d7 2 cells with stride 2; C3 is a convolutional layer using 256 filters (5 \u00d7 5 \u00d7 96); M 4 is a max-pooling layer same as M 2; F 5 is a fully connected layer with 1024 neurons; S6 is a soft-max layer with 73 labels, and each label corresponds to a candidate motion blur kernel in S as shown in Fig. 2(c).\n\nTo train the CNN model, we generate a large set of training data T = {\u03a8 k , m k } K k=1 , which are composed of blurry patch / motion kernel pairs. We synthetically generate blurry images by convolving clean natural images with the 73 possible motion kernels, then randomly crop 30 \u00d7 30 \u00d7 3 color patches from the blurry images as the training patches {\u03a8 k } K k=1 , and take the labels of corresponding ground-truth motion kernels as the training labels {m k } K k=1 . We gener-ate training data using 1000 images randomly sampled from PASCAL VOC 2010 database and finally construct a training set of around 1.4 million pairs of blurry patches and their ground-truth motion kernels. Using Caffe [5] 2 , we train the CNN model in one million iterations by stochastic gradient descent algorithm with batches of 64 patches in each iteration.\n\nBecause the final layer of the CNN is a soft-max layer, we can predict the probabilities of motion kernels given an observed blurry patch \u03a8 as\nP (m = (l, o)|\u03a8) = exp((w S6 c ) T \u03c6 F 5 (\u03a8)) n exp((w S6 n ) T \u03c6 F 5 (\u03a8)) ,(3)\nwhere w S6 c is the vector of weights on neuron connections from F5 layer to the neuron in S6 layer representing the motion kernel (l, o), c is the index of (l, o) in S. \u03c6 F 5 (\u03a8) is the output features of F 5 layer of a blurry patch \u03a8, which is a 1024-dimensional feature vector.\n\nIn our implementation, we also tried to learn more complex CNN structures (e.g., with one more convolutional layer or more filters in convolutional layers), but the learning speed is significantly slower while the final prediction results are not significantly improved. Figure 3 (right) shows examples of automatically learned filters by our CNN model for motion kernel prediction. These filters reflect diverse local structures in sharp or blurry patch instances. \n\u03a8 p (I) \u03a8 p (R \u03b8 I) \u03a8 p (I) \u03a8 p (R \u03b8 I) m = (l,o \u2212\u03b8) m = (l,o) I R \u03b8 I \u03b8 = \u221224 o\n\nExtending the Motion Kernel Set of CNN\n\nOur learned CNN model can predict the probabilities of 73 candidate motion kernels in S. Obviously, they are not sufficiently dense in the motion space. We next extend the motion kernel set predicted by the CNN to enable the prediction for motion kernels outside S.\n\nPredic'ng probabili'es of mo'on kernels by CNN We make the extension based on the following observation. As shown in Fig. 4, given a blurry image I, we rotate it by \u03b8 degrees (denoted as R \u03b8 I, R \u03b8 is a rotation operator). For a pair of patches \u03a8 p (I) and its rotated version \u03a8 p (R \u03b8 I) cropped from I and R \u03b8 I centered at pixel p respectively, if we can predict that the motion kernel of \u03a8 p (R \u03b8 I) is m = (l, o), then we can deduce directly that the motion kernel of corresponding patch \u03a8 p (I) in I is m = (l, o \u2212 \u03b8).\nP(m = (l,o)) P(m = (l,o + 24 o )) l \u2208S l ;o \u2208S o R \u22126 o I R \u221212 o I R \u221218 o I R \u221224 o I I \u03a8 p (I) \u03a8 p (R \u22126 o I) \u03a8 p (R \u221212 o I) \u03a8 p (R \u221218 o I) \u03a8 p (R \u221224 o I) P(m = (l,o + 6 o )) P(m = (l,o + 12 o )) P(m = (l,o + 18 o ))\nBased on the above observation, we can estimate the probabilities of motion kernels for patch \u03a8 p (I) using its rotated patch \u03a8 p (R \u03b8 I). By feeding the rotated patch into CNN, we can estimate the probabilities of motion kernels for the rotated patch: P (m = (l, o)|\u03a8 p (R \u03b8 I)), m \u2208 S, then we can deduce that the motion distribution of the original patch \u03a8 p (I) before rotation is: where l \u2208 S l , o \u2208 S o . By concatenating all the above estimations from one patch and its rotated versions, we can therefore predict the motion distribution in an extended motion kernel set of CNN:\nP (m = (l, o)|\u03a8 p (I)), o \u2208 S o ext = {0 \u2022 , 6 \u2022 , 12 \u2022 , \u00b7 \u00b7 \u00b7 , 174 \u2022 }, l \u2208 S l .\nAfter motion kernels extension, we can totally predict probabilities of 361 candidate motion kernels 3 for an image patch by CNN, which is almost 5 times of the number of candidate motion kernels in S predicted by CNN. Note that this process does not require the CNN retraining, but just feed this image and its rotated versions to our learned CNN.  Figure 6 shows an example of motion kernel estimation without and with CNN motion kernel set extension. In this example, we synthetically generate the motion blur using a camera motion. As shown in Fig. 6(b), the estimated motion kernels suffer from blocky artifacts in the blue rectangle because all pixels in it are predicted to have the same orientation due to the large quantization interval of motion orientations. By extending the motion kernel set of CNN, we can predict more accurate motion kernels shown in Fig. 6(c). The mean squared error (MSE) w.r.t. ground-truth motion kernels is reduced from 10.3 to 8.4.\n\n\nDense Motion Field Estimation by MRF\n\nThe CNN predicts distribution of motion kernels in an image at the patch level. We now discuss how to fuse these 3 There are totally 390 possible kernels by combining the motion lengths in S l (|S l | = 13) and motion orientations in   patch-level motion kernel estimations into a dense field of motion kernels for the image.\n\nGiven an image I, we sample 30 \u00d7 30 \u00d7 3 overlapping color patches with a spatial interval of 6 pixels over image I. Each patch \u03a8 p (I) produces motion kernel probabilities: P (m = (l, o)|\u03a8 p (I)) (l \u2208 S l , o \u2208 S o ext ) by applying the CNN. Figure 7(b) shows examples of motion distribution maps for four blurry patches, and each of them is sparse and composed of one local high probability region. We assume that the pixels in patch \u03a8 p (I) share the same motion distribution. Then each pixel has multiple estimates for each motion kernel probability from all patches containing it. For a pixel p, we perform weighted average over the multiple estimates of each motion kernel probability, and define the confidence of motion kernel m = (l, o) at pixel p as\nC(m p = (l, o)) = 1 Z q:p\u2208\u03a8q G \u03c3 (||x p \u2212 x q || 2 )P (m = (l, o)|\u03a8 q ),(5)\nfor all l \u2208 S l , o \u2208 S o ext . x p is the coordinate of pixel p. As a Gaussian function, G \u03c3 (||x p \u2212 x q || 2 ) imposes higher weights on the patch \u03a8 q in the summation if its center pixel q is closer to pixel p. \u03c3 is set to 10 in our implementation. This means that we trust more the motion prediction from the patch containing pixel p closer to its patch center. Z = q:p\u2208\u03a8q G \u03c3 (||x p \u2212 x q ||) is a normalization constant. We further assume that the motion kernels are spatially smooth. This is reasonable because the moving objects or camera are moving smoothly during capturing image, and nearby pixels should have similar motions. Then we estimate the dense motion field M = {m p = (l p , o p )} p\u2208\u2126 over image I by optimizing the following MRF model:\nmin M p\u2208\u2126 [\u2212C(m p = (l p , o p )) + q\u2208N (p) \u03bb[(u p \u2212 u q ) 2 + (v p \u2212 v q ) 2 ],(6)\nwhere l p \u2208 S l , o p \u2208 S o ext , (u p , v p ) and (u q , v q ) are motion vectors m p and m q in Cartesian coordinates that are related to (l p , o p ) and (l q , o q ) by Eqn. (1). N (p) is the neighborhood of p. By minimizing the energy function, the first term encourages to choose the motion kernel for each pixel with higher confidence estimated by CNN, and the second term enforces the smoothness of nearby motion kernels.\n\nFor each pixel, there are 361 motion kernel candidates, it is inefficient to optimize the MRF problem with such a large number of candidate labels. We therefore generate candidate motion kernels for each pixel by selecting the top 20 motion kernels with highest confidence values, together with 30 sampled motion candidates from the remaining candidates to make the motion kernel candidate set for each pixel both prominent and diverse. Since the candidate label sets are spatially varying, we cannot use the off-the-shelf graph cut toolbox [24], we therefore optimize the energy by max-product belief propagation algorithm. Predicting dense motion blur for an image of size 300 \u00d7 400 takes around 80 seconds using CPU including computing patchlevel motion distributions by CNN. Figure 8 shows an example of motion blur estimation. As shown in Fig. 8(b, c), the full MRF model can effectively remove the noisy estimates in Fig. 8(b) using smoothness term, and quantitative results are significantly improved.\n\n\nNon-Uniform Motion Deblurring\n\nWith the dense non-uniform motion kernels estimated by CNN, we now deconvolve the blurry image to estimate the sharp image. It is challenging to deconvolve the image blurred by non-uniform motion blur. We adapt the uniform deconvolution approach in [32] to the non-uniform deconvolution problem. The non-uniform deconvolution is modeled as optimizing:\nmin I \u03bb 2 ||k(M ) * I \u2212 O|| 2 2 \u2212 i\u2208\u2126 log(P (R i I)) (7)\nwhere O is the observed blurry image, R i is an operator to extract the patch located at i from an image. P (\u00b7) is the prior distribution of natural image patches, which is modeled as a Gaussian mixture model learned from natural image patches [32]. Different to uniform deblur in [32], the first term in Eqn. (7) is modeled for non-uniform motion blur. We optimize the above problem by half-quadratic splitting algorithm, i.e., optimizing: min I,{zi} \u03bb 2 ||k(M ) * I \u2212 O|| 2 2 + i\u2208\u2126 ( \u03b2 2 ||R i I \u2212 z i || 2 2 \u2212 log(P (z i ))), where auxiliary variables {z i } are introduced. We iteratively optimize I and {z i } by increasing \u03b2. In the iterations, we need to optimize the following two sub-problems. (1) By fixing {z i }, we optimize sharp image: min I \u03bb 2 ||k(M ) * I \u2212 O|| 2 2 + i\u2208\u2126 ( \u03b2 2 ||R i I \u2212 z i || 2 2 ). (2) By fixing I, we optimize {z i }: min zi \u03b2 2 ||R i I \u2212 z i || 2 2 \u2212 log(P (z i )), i \u2208 \u2126. For sub-problem (1), the blur kernels k(M ) are nonuniform and determined by spatially varying motion vectors M . By re-writing the non-uniform convolution as matrixvector multiplication (i.e., k(M ) * I = K M I ), we optimize sharp image by solving the linear equations deduced by setting the gradients of cost in sub-problem (1) to zeros:\n[\u03bbK T M K M + \u03b2 i\u2208\u2126 (R T i R i )]I = \u03bbK T M O + \u03b2( i\u2208\u2126 R T i z i ). (8)\nWe solve these linear equations using a conjugate gradient algorithm. In the implementation, all the involved matrixvector multiplications can be efficiently implemented by convolutions or local operations around each pixel. R T i z is an operation to put the patch z back to the region where it was extracted. The sub-problem (2) can be optimized following [32]. In implementation, we set the patch size to 8 \u00d7 8, \u03bb = 2 \u00d7 10 5 , and \u03b2 is increased from 50 to 3200 with a ratio of 2 in 7 iterations of alternative optimizations. Table 1. Comparison of motion kernel estimation on 15 test images with synthetic motion blur. \"BlurSpect\" is based on the approach in [1]. \"SLayerRegr\" is the extension of approach in [3]. \n\n\nExperiments\n\nTo evaluate the quantitative accuracy of our approach for non-uniform motion kernel estimation, we generate 15 synthetic blurred images with ground-truth nonuniform motion kernels caused by camera motions , d max = 25 is the maximum motion length. Figure 9 presents four examples with strongly nonuniform motion blur captured for scenes with complex depth layers. The first three examples are real-captured blurry images, and the final example is a synthetic blurry image. All these examples show that our CNN-based approach can effectively predict the spatially varying motion kernels.\n\nIn Table 1, we evaluate and compare our approach to the other approaches for non-uniform motion kernel estimation. \"DL noMRF\" is our approach using only the unary term in Eqn. (6). \"DL noLE\" is our MRF-based approach without using the motion kernel set extension. \"DL MRF\" is our full estimation approach. \"BlurSpect\" is the approach proposed in [1]. It was originally designed for estimating horizontal or vertical motion blur, and we extend it to estimate motion kernels with orientations in S o ext by the technique in Section 2.2. \"SLayerRegr\" is an extended version of approach in [3]. The original approach learns a logistic regressor to estimate discrete motion kernels in horizontal direction using hand-crafted features. To predict motion kernels in other directions, we implement [3] using the same features and learn SVMs for predicting 73 motion kernels in S, then extend motion kernel set by the method in Section 2.2. \"SLayerRegr\" can be seen as a learning machine with a single layer of hand-crafted features. As shown in Table 1, both \"BlurSpect\" and \"SLayerRegr\" perform poorly on estimating the challenging non-uniform motion blur with diverse motion lengths and orientations. Our approach can effectively estimate the motion kernels with average MSE motion 7.83 and PSNR motion 44.55. More- Figure 9. Examples on motion kernel estimation. The first three columns are real blurry images, the last column shows a synthetic picture with camera rotation (MSE motion = 9.9). Figure 10. Examples of non-uniform motion deblurring. The first and second columns show the blurry images and our results. The third and fourth columns show the results of methods in [18,26,27,29] using their source codes. These examples are challenging because the motion blur kernels are strongly non-uniform and the scenes are complex. Our estimated motion blur fields are shown in Figs. 9, 12 . over, the motion kernel set extension and motion smoothness constraint significantly improve the accuracy of motion kernel estimation. Figure 10 compares deblurring results of our approach, non-uniform deblurring approaches [26,29] and uniform deblurring approaches [18,27], for which the source codes are available. Except for ours, none of these methods handles the non-uniform blur in a satisfying manner for these examples. Our approach estimates more accurate motion blur kernels, which enables us to produce better final de- Table 2. Accuracies of motion kernel estimation and blur removal on 15 test images with synthetic motion blur. \"BlurSpect\" is based on the approach in [1]. \"SLayerRegr\" is the extension of approach in [3]. \"MSE ker\" is an error for non-uniform blur kernel estimation using the average MSE of blur kernels across image pixels. \"PSNR deblur\" is the PSNR of the final deblurred results. The number in each  Figure 11. Comparison to [13]. Our CNN can better predict the different motion layers. The deblurring result of [13] is over-sharpened and image details are removed, while our result is visually more natural.  Figure 12. Comparison of motion kernel estimation. \"BlurSpect\" and \"SLayerRegr\" are based on the methods in [1] and [3] respectively. blurring results. The method in [13] is an effective approach for motion deblurring. Because its source code is not available, we directly compare it on examples of [13] in Fig. 11. Our approach can better \"recognize\" the complex motions. The deblurring result of [13] is commonly over-sharpened, but our deblurring result is visually more natural.\n\nIn Table 2, we qualitatively compare our method to the state-of-the-art non-blind debluring approaches for both the motion blur kernel estimation and the final deblurred results. We define an error of \"MSE ker\" for non-uniform motion blur estimation using average MSE of blur kernels across pixels in an image, and the MSE of each pixel is defined by the mean per-element squared difference between the estimated and ground-truth kernels after aligning kernels by centers. Contrary to the \"MSE motion\" that measures the kernel error in the linear motion space, this error term directly measures the kernel differences in the spatial domain. We also evaluate the deblurring result by the PSNR of the deblurred image (denoted as \"PSNR deblur\") w.r.t. the ground-truth clean image. All the values in Table 2 are the mean values over the image set. We can not qualita-tively compare to the approach in [13] because the source codes are not available. These results clearly show that our approach can produce significantly better results both in the motion blur kernel estimation and the motion blur removal than the compared state-of-the-art approaches. Figure 12 shows an example of motion blur estimation by different non-uniform blur estimation approaches. Our approach can produce significantly better non-uniform motion blur field than the compared approaches.\n\n\nConclusion\n\nIn this paper, we have proposed a novel CNN-based nonuniform motion deblurring approach. We learn an effective CNN for estimating motion kernels from local patches. Using an MRF model, we are able to well predict the nonuniform motion blur field. This leads to state-of-the-art motion deblurring results. In the future, we are interested in designing a CNN for estimating the general non-uniform blur kernels. We are also interested in designing an CNN system that can estimate and remove general non-uniform blurs in a single framework.\n\nFigure 1 .\n1An example illustrating our approach. Given an image with non-uniform motion blur (left). We first estimate the field of non-uniform motion blur kernels by a convolutional neural network (middle), then deconvolve the blurred image (right).\n\nFigure 2 .\n2) Candidate mo'on kernel set for learning CNN (b) Discre'zing mo'on vector \u03b8 r Representation of motion blur kernel by motion vector and generation of motion kernel candidates.\n\nFigure 3 .\n3Structure of CNN for motion kernels prediction. It is composed of 6 layers of convolutional layers and fully connected layers. It outputs the probability of each candidate motion kernel using soft-max layer. The right sub-figure shows the learned filters in C1.\n\nFigure 4 .\n4Motion kernel estimation on a rotated patch. I is a blurry image, R \u03b8 I is the rotated image with \u03b8 (\u03b8 = \u221224 o in this case).\n\nFigure 5 .\n5Extension of motion kernel set predicted by CNN using rotated images. For an image I, we generate its rotated images R \u22126 \u2022 I, R \u221212 \u2022 I, R \u221218 \u2022 I, R \u221224 \u2022 I, then feed each patch and its rotated versions into the CNN to predict motion distributions. By concatenating all the motion distribution estimations, we can estimate the probabilities of more densely sampled motion kernels.\n\nP\n(m = (l, o \u2212 \u03b8)|\u03a8 p (I)) = P (m = (l, o)|\u03a8 p (R \u03b8 I)).(4) Note that motion m = (l, o \u2212 \u03b8) may not belong to the motion kernel set of CNN (i.e., S).By carefully designing the image rotations, we can extend the motion kernel set of CNN as follows. Remember that the original CNN can predict probabilities of 73 motion kernels in S with orientations inS o = {0 \u2022 , 30 \u2022 , 60 \u2022 , 90 \u2022 , 120 \u2022 , 150 \u2022 } with interval of 30 \u2022 .As shown in Fig. 5, given a blurry image I, we first generate its rotated images R \u22126 \u2022 I, R \u221212 \u2022 I, R \u221218 \u2022 I, R \u221224 \u2022 I with rotation angles within [0, 30 \u2022 ) and interval of 6 \u2022 . For each patch \u03a8 p (I) centered at pixel p, we extract its rotated versions \u03a8 p (R \u03b8 I) (\u03b8 \u2208 {\u22126 \u2022 , \u221212 \u2022 , \u221218 \u2022 , \u221224 \u2022 }) from the rotated images. By feeding these patches into CNN, we can predict the probabilities of motion kernels for patch \u03a8 p (I) using each patch based on Eqn. (4): P (m = (l, o)|\u03a8 p (I)) = P (m = (l, o)|\u03a8 p (I)), P (m = (l, o + 6 \u2022 )|\u03a8 p (I)) = P (m = (l, o)|\u03a8 p (R \u22126 \u2022 I)), P (m = (l, o + 12 \u2022 )|\u03a8 p (I)) = P (m = (l, o)|\u03a8 p (R \u221212 \u2022 I)), P (m = (l, o + 18 \u2022 )|\u03a8 p (I)) = P (m = (l, o)|\u03a8 p (R \u221218 \u2022 I)), P (m = (l, o + 24 \u2022 )|\u03a8 p (I)) = P (m = (l, o)|\u03a8 p (R \u221224 \u2022 I)),\n\nFigure 6 .\n6Effect of CNN motion kernel set extension.\n\nFigure 7 .\n7S o ext (|S o ext | = 30). But the motion kernels {m = (l, o)} o\u2208S o ext when l = 1 are all the same, we only retain one of them. Examples of mo(on kernels probabili(es es(mated by CNN (c) Final mo(on blur es(ma(on (d) Ground--truth mo(on blur Examples of motion kernel probabilities. The left of (b) show four blurry patches cropped from (a). Each color map on the right of (b) shows the probabilities of motion kernels in different motion lengths and orientations estimated for each blurry patch by CNN. Note that the high probability regions are local in each map. (c) shows our final motion kernel estimation.\n\nFigure 8 .\n8Example of non-uniform motion kernel estimation. (b) Estimation using the unary term of Eqn.(6), i.e., choosing the motion kernel with highest confidence for each pixel. (c) Estimation using the full model of Eqn.(6) with motion smoothness constraint. (d) Ground-truth motion blur. MSE motion is an accuracy measurement of motion blur defined in Section 5.\n\n\n(rotation and translation). The examples shown in Figs. 5-8 are from this synthetic image set. Given the estimated motion blur kernels M = {u p , v p } p\u2208\u2126 and ground-truth motion blur kernels M gt = {u gt p , v gt p } p\u2208\u2126 in the Cartesian coordinate system, we measure the accuracy of the estimated motion kernel by the meansquared-error (MSE motion): MSE motion(M, M gt ) = 1 2|\u2126| p\u2208\u2126 [(u p \u2212 u gt p ) 2 + (v p \u2212 v gt p ) 2 ] and peak signal-tonoise ratio (PSNR motion): PSNR motion(M, M gt ) = \u221210 log MSE motion(M,M gt ) d 2 max\n\n\ntable cell is the mean value over the image set.DL MRF \nDL noMRF DL noLE \nBlurSpect \nSLayerRegr \nTwoPhase [27] \nMargLike [18] \nNonUnif [26] \nUnNatural [29] \nMSE ker \n0.024 \n0.029 \n0.041 \n0.250 \n0.127 \n0.108 \n0.119 \n0.193 \n0.165 \nPSNR deblur \n24.81 \n24.66 \n24.61 \n21.72 \n19.04 \n21.26 \n18.49 \n20.65 \n21.33 \n\nMo$on blur by [13] \nOur result \nResult of [13] \nInput image \nOur es$mated mo$on blur \n\nOurs \n[13] \nOurs \n[13] \n\nOurs Ours \n\n[13] [13] \n\n\nNote that motions m = (l, o) and m = (l, o + 180 \u2022 ) generate the same motion blur kernel. We therefore only need to estimate the motions with o \u2208 [0, 180 \u2022 ).\nhttp://caffe.berkeleyvision.org\nAcknowledgementThis work was partially supported by NSFC projects (61472313, 11131006), the 973 program (2013CB329404) and NCET-12-0442. Ponce was supported in part by the Institut Universitaire de France and European Research Council (VideoWorld project).\nAnalyzing spatially-varying blur. A Chakrabarti, T Zickler, W Freeman, CVPR. A. Chakrabarti, T. Zickler, and W. Freeman. Ana- lyzing spatially-varying blur. In CVPR, pages 2512- 2519, 2010.\n\nRemoving nonuniform motion blur from images. S Cho, Y Matsushita, S Lee, ICCV. S. Cho, Y. Matsushita, and S. Lee. Removing non- uniform motion blur from images. In ICCV, pages 1-8, 2007.\n\nLearning to estimate and remove non-uniform image blur. F Couzini\u00e9-Devy, J Sun, K Alahari, J Ponce, CVPR. F. Couzini\u00e9-Devy, J. Sun, K. Alahari, and J. Ponce. Learning to estimate and remove non-uniform image blur. In CVPR, 2013.\n\nMotion from blur. S Dai, Y Wu, CVPR. S. Dai and Y. Wu. Motion from blur. In CVPR, 2008.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, arXiv:1310.1531J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolu- tional activation feature for generic visual recognition. arXiv:1310.1531, 2013.\n\nRemoving camera shake from a single photograph. R Fergus, B Singh, A Hertzmann, S Roweis, W Freeman, ACM Transactions on Graphics (TOG). 253R. Fergus, B. Singh, A. Hertzmann, S. Roweis, and W. Freeman. Removing camera shake from a single photograph. ACM Transactions on Graphics (TOG), 25(3):787-794, 2006.\n\nSingle image deblurring using motion density functions. A Gupta, N Joshi, C Lawrence Zitnick, M Cohen, B Curless, ECCV. A. Gupta, N. Joshi, C. Lawrence Zitnick, M. Cohen, and B. Curless. Single image deblurring using motion density functions. In ECCV, 2010.\n\nFast removal of non-uniform camera shake. M Hirsch, C Schuler, S Harmeling, B Scholkopf, ICCV. M. Hirsch, C. Schuler, S. Harmeling, and B. Scholkopf. Fast removal of non-uniform camera shake. In ICCV, 2011.\n\nGood regions to deblur. Z Hu, M Yang, ECCV. Z. Hu and M. Yang. Good regions to deblur. In ECCV, 2012.\n\nA two-stage approach to blind spatially-varying motion deblurring. H Ji, K Wang, CVPR. H. Ji and K. Wang. A two-stage approach to blind spatially-varying motion deblurring. In CVPR, 2012.\n\nPsf estimation using sharp edge prediction. N Joshi, R Szeliski, D Kriegman, CVPR. N. Joshi, R. Szeliski, and D. Kriegman. Psf estimation using sharp edge prediction. In CVPR, 2008.\n\nConvolutional neural networks for no-reference image quality assessment. L Kang, P Ye, Y Li, D Doermann, CVPR. L. Kang, P. Ye, Y. Li, and D. Doermann. Convolu- tional neural networks for no-reference image quality assessment. In CVPR, 2014.\n\nSegmentation-free dynamic scene deblurring. T H Kim, K M Lee, CVPR. T. H. Kim and K. M. Lee. Segmentation-free dynamic scene deblurring. In CVPR, 2014.\n\nFast image deconvolution using hyper-laplacian priors. D Krishnan, R Fergus, NIPS. D. Krishnan and R. Fergus. Fast image deconvolution using hyper-laplacian priors. In NIPS, 2009.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Im- agenet classification with deep convolutional neural networks. In NIPS, 2012.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. the IEEEY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog- nition. In Proceedings of the IEEE, 1998.\n\nBlind motion deblurring using image statistics. A Levin, NIPS. A. Levin. Blind motion deblurring using image statis- tics. In NIPS, 2007.\n\nEfficient marginal likelihood optimization in blind deconvolution. A Levin, Y Weiss, F Durand, W T Freeman, CVPR. A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Efficient marginal likelihood optimization in blind de- convolution. In CVPR, 2011.\n\nBlind deblurring using internal patch recurrence. T Michaeli, M Irani, ECCV. T. Michaeli and M. Irani. Blind deblurring using in- ternal patch recurrence. In ECCV, 2014.\n\nNon-uniform motion deblurring for bilayer scenes. C Paramanand, A N Rajagopalan, CVPR. C. Paramanand and A. N. Rajagopalan. Non-uniform motion deblurring for bilayer scenes. In CVPR, 2013.\n\nDiscriminative non-blind deblurring. U Schmidt, C Rother, S Nowozin, J Jancsary, S Roth, CVPR. U. Schmidt, C. Rother, S. Nowozin, J. Jancsary, and S. Roth. Discriminative non-blind deblurring. In CVPR, 2013.\n\nA machine learning approach for nonblind image deconvolution. C J Schuler, H C Burger, S Harmeling, B Scholkopf, CVPR. C. J. Schuler, H. C. Burger, S. Harmeling, and B. Scholkopf. A machine learning approach for non- blind image deconvolution. In CVPR, 2013.\n\nGood image priors for non-blind deconvolution. L Sun, S Cho, J Wang, J Hays, ECCV. L. Sun, S. Cho, J. Wang, and J. Hays. Good image priors for non-blind deconvolution. In ECCV. 2014.\n\nA comparative study of energy minimization methods for markov random fields with smoothness-based priors. R Szeliski, R Zabih, D Scharstein, O Veksler, Kolmogorov, IEEE T. PAMI. 306R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, Kol- mogorov, and et.al. A comparative study of energy minimization methods for markov random fields with smoothness-based priors. IEEE T. PAMI, 30(6):1068- 1080, 2008.\n\nRichardson-lucy deblurring for scenes under a projective motion path. Y Tai, P Tan, M Brown, IEEE T. PAMI. 338Y. Tai, P. Tan, and M. Brown. Richardson-lucy deblur- ring for scenes under a projective motion path. IEEE T. PAMI, 33(8):1603-1618, 2011.\n\nNon-uniform deblurring for shaken images. O Whyte, J Sivic, A Zisserman, J Ponce, IJCV. 982O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform deblurring for shaken images. IJCV, 98(2):168-186, 2012.\n\nTwo-phase kernel estimation for robust motion deblurring. L Xu, J Jia, ECCV. L. Xu and J. Jia. Two-phase kernel estimation for ro- bust motion deblurring. In ECCV, 2010.\n\nDeep convolutional neural network for image deconvolution. L Xu, J S Ren, C Liu, J Jia, NIPS. L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convo- lutional neural network for image deconvolution. In NIPS, 2014.\n\nUnnatural l0 sparse representation for natural image deblurring. L Xu, S Zheng, J Jia, CVPR. L. Xu, S. Zheng, and J. Jia. Unnatural l0 sparse rep- resentation for natural image deblurring. In CVPR, 2013.\n\nImage deblurring with blurred/noisy image pairs. L Yuan, J Sun, L Quan, H Shum, ACM TOG. 2631L. Yuan, J. Sun, L. Quan, and H. Shum. Image de- blurring with blurred/noisy image pairs. ACM TOG, 26(3):1, 2007.\n\nForward motion deblurring. S Zheng, L Xu, J Jia, ICCV. S. Zheng, L. Xu, and J. Jia. Forward motion deblur- ring. In ICCV, 2013.\n\nFrom learning models of natural image patches to whole image restoration. D Zoran, Y Weiss, ICCV. D. Zoran and Y. Weiss. From learning models of nat- ural image patches to whole image restoration. In ICCV, 2011.\n", "annotations": {"author": "[{\"end\":170,\"start\":79},{\"end\":264,\"start\":171},{\"end\":358,\"start\":265},{\"end\":370,\"start\":359}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":181,\"start\":178},{\"end\":275,\"start\":273},{\"end\":369,\"start\":364}]", "author_first_name": "[{\"end\":83,\"start\":79},{\"end\":177,\"start\":171},{\"end\":272,\"start\":265},{\"end\":363,\"start\":359}]", "author_affiliation": "[{\"end\":117,\"start\":89},{\"end\":169,\"start\":119},{\"end\":211,\"start\":183},{\"end\":263,\"start\":213},{\"end\":305,\"start\":277},{\"end\":357,\"start\":307}]", "title": "[{\"end\":76,\"start\":1},{\"end\":446,\"start\":371}]", "venue": null, "abstract": "[{\"end\":1219,\"start\":448}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1255,\"start\":1252},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1257,\"start\":1255},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1259,\"start\":1257},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1262,\"start\":1259},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1265,\"start\":1262},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1268,\"start\":1265},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1271,\"start\":1268},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1274,\"start\":1271},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1488,\"start\":1484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1491,\"start\":1488},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1494,\"start\":1491},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1555,\"start\":1552},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1557,\"start\":1555},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1560,\"start\":1557},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1563,\"start\":1560},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1918,\"start\":1914},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1937,\"start\":1934},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1998,\"start\":1995},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2021,\"start\":2017},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2024,\"start\":2021},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2504,\"start\":2501},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2507,\"start\":2504},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2510,\"start\":2507},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2513,\"start\":2510},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3416,\"start\":3413},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3418,\"start\":3416},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3421,\"start\":3418},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3424,\"start\":3421},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3427,\"start\":3424},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3577,\"start\":3573},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3580,\"start\":3577},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3583,\"start\":3580},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3936,\"start\":3933},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3938,\"start\":3936},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4027,\"start\":4024},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4129,\"start\":4125},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4193,\"start\":4190},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4263,\"start\":4260},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4786,\"start\":4782},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4900,\"start\":4896},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5005,\"start\":5001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7771,\"start\":7768},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9186,\"start\":9182},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14293,\"start\":14292},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16366,\"start\":16363},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17161,\"start\":17157},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17911,\"start\":17907},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18315,\"start\":18311},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18352,\"start\":18348},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19753,\"start\":19749},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20057,\"start\":20054},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20107,\"start\":20104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20892,\"start\":20889},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21062,\"start\":21059},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21302,\"start\":21299},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21506,\"start\":21503},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22389,\"start\":22385},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22392,\"start\":22389},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22395,\"start\":22392},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22398,\"start\":22395},{\"end\":22600,\"start\":22593},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22829,\"start\":22825},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22832,\"start\":22829},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22871,\"start\":22867},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22874,\"start\":22871},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23286,\"start\":23283},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23336,\"start\":23333},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23565,\"start\":23561},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23652,\"start\":23648},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23857,\"start\":23854},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23865,\"start\":23862},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23916,\"start\":23912},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24049,\"start\":24045},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24148,\"start\":24144},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25132,\"start\":25128},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27458,\"start\":27455}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":26396,\"start\":26144},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26586,\"start\":26397},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26861,\"start\":26587},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27000,\"start\":26862},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27397,\"start\":27001},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28604,\"start\":27398},{\"attributes\":{\"id\":\"fig_7\"},\"end\":28660,\"start\":28605},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29287,\"start\":28661},{\"attributes\":{\"id\":\"fig_10\"},\"end\":29657,\"start\":29288},{\"attributes\":{\"id\":\"fig_11\"},\"end\":30192,\"start\":29658},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30637,\"start\":30193}]", "paragraph": "[{\"end\":1459,\"start\":1235},{\"end\":2199,\"start\":1461},{\"end\":3307,\"start\":2201},{\"end\":3900,\"start\":3324},{\"end\":4697,\"start\":3902},{\"end\":5351,\"start\":4699},{\"end\":6501,\"start\":5397},{\"end\":6641,\"start\":6503},{\"end\":7038,\"start\":6687},{\"end\":7383,\"start\":7086},{\"end\":7580,\"start\":7407},{\"end\":8803,\"start\":7582},{\"end\":9053,\"start\":8805},{\"end\":9546,\"start\":9089},{\"end\":10387,\"start\":9548},{\"end\":10531,\"start\":10389},{\"end\":10892,\"start\":10612},{\"end\":11360,\"start\":10894},{\"end\":11748,\"start\":11483},{\"end\":12274,\"start\":11750},{\"end\":13083,\"start\":12498},{\"end\":14138,\"start\":13169},{\"end\":14504,\"start\":14179},{\"end\":15264,\"start\":14506},{\"end\":16100,\"start\":15341},{\"end\":16614,\"start\":16185},{\"end\":17624,\"start\":16616},{\"end\":18009,\"start\":17658},{\"end\":19318,\"start\":18067},{\"end\":20109,\"start\":19391},{\"end\":20711,\"start\":20125},{\"end\":24228,\"start\":20713},{\"end\":25591,\"start\":24230},{\"end\":26143,\"start\":25606}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6686,\"start\":6642},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7406,\"start\":7384},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9088,\"start\":9054},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10611,\"start\":10532},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11441,\"start\":11361},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12497,\"start\":12275},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13168,\"start\":13084},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15340,\"start\":15265},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16184,\"start\":16101},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18066,\"start\":18010},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19390,\"start\":19319}]", "table_ref": "[{\"end\":19927,\"start\":19920},{\"end\":20723,\"start\":20716},{\"end\":21757,\"start\":21750},{\"end\":23139,\"start\":23132},{\"end\":24240,\"start\":24233},{\"end\":25034,\"start\":25027}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1233,\"start\":1221},{\"attributes\":{\"n\":\"1.1.\"},\"end\":3322,\"start\":3310},{\"attributes\":{\"n\":\"2.\"},\"end\":5395,\"start\":5354},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7084,\"start\":7041},{\"attributes\":{\"n\":\"2.2.\"},\"end\":11481,\"start\":11443},{\"attributes\":{\"n\":\"3.\"},\"end\":14177,\"start\":14141},{\"attributes\":{\"n\":\"4.\"},\"end\":17656,\"start\":17627},{\"attributes\":{\"n\":\"5.\"},\"end\":20123,\"start\":20112},{\"attributes\":{\"n\":\"6.\"},\"end\":25604,\"start\":25594},{\"end\":26155,\"start\":26145},{\"end\":26408,\"start\":26398},{\"end\":26598,\"start\":26588},{\"end\":26873,\"start\":26863},{\"end\":27012,\"start\":27002},{\"end\":27400,\"start\":27399},{\"end\":28616,\"start\":28606},{\"end\":28672,\"start\":28662},{\"end\":29299,\"start\":29289}]", "table": "[{\"end\":30637,\"start\":30243}]", "figure_caption": "[{\"end\":26396,\"start\":26157},{\"end\":26586,\"start\":26410},{\"end\":26861,\"start\":26600},{\"end\":27000,\"start\":26875},{\"end\":27397,\"start\":27014},{\"end\":28604,\"start\":27401},{\"end\":28660,\"start\":28618},{\"end\":29287,\"start\":28674},{\"end\":29657,\"start\":29301},{\"end\":30192,\"start\":29660},{\"end\":30243,\"start\":30195}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3034,\"start\":3026},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6195,\"start\":6189},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8390,\"start\":8384},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9024,\"start\":9018},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9545,\"start\":9536},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11181,\"start\":11165},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11873,\"start\":11867},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":13527,\"start\":13519},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":13726,\"start\":13717},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":14044,\"start\":14035},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":14759,\"start\":14748},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":17403,\"start\":17395},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":17472,\"start\":17460},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":17548,\"start\":17539},{\"end\":20381,\"start\":20373},{\"end\":22031,\"start\":22023},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22211,\"start\":22202},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22745,\"start\":22736},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23545,\"start\":23536},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23755,\"start\":23746},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24060,\"start\":24053},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25389,\"start\":25380}]", "bib_author_first_name": "[{\"end\":31122,\"start\":31121},{\"end\":31137,\"start\":31136},{\"end\":31148,\"start\":31147},{\"end\":31324,\"start\":31323},{\"end\":31331,\"start\":31330},{\"end\":31345,\"start\":31344},{\"end\":31523,\"start\":31522},{\"end\":31540,\"start\":31539},{\"end\":31547,\"start\":31546},{\"end\":31558,\"start\":31557},{\"end\":31715,\"start\":31714},{\"end\":31722,\"start\":31721},{\"end\":31865,\"start\":31864},{\"end\":31876,\"start\":31875},{\"end\":31883,\"start\":31882},{\"end\":31894,\"start\":31893},{\"end\":31905,\"start\":31904},{\"end\":31914,\"start\":31913},{\"end\":31923,\"start\":31922},{\"end\":32182,\"start\":32181},{\"end\":32192,\"start\":32191},{\"end\":32201,\"start\":32200},{\"end\":32214,\"start\":32213},{\"end\":32224,\"start\":32223},{\"end\":32498,\"start\":32497},{\"end\":32507,\"start\":32506},{\"end\":32516,\"start\":32515},{\"end\":32525,\"start\":32517},{\"end\":32536,\"start\":32535},{\"end\":32545,\"start\":32544},{\"end\":32743,\"start\":32742},{\"end\":32753,\"start\":32752},{\"end\":32764,\"start\":32763},{\"end\":32777,\"start\":32776},{\"end\":32933,\"start\":32932},{\"end\":32939,\"start\":32938},{\"end\":33079,\"start\":33078},{\"end\":33085,\"start\":33084},{\"end\":33245,\"start\":33244},{\"end\":33254,\"start\":33253},{\"end\":33266,\"start\":33265},{\"end\":33457,\"start\":33456},{\"end\":33465,\"start\":33464},{\"end\":33471,\"start\":33470},{\"end\":33477,\"start\":33476},{\"end\":33670,\"start\":33669},{\"end\":33672,\"start\":33671},{\"end\":33679,\"start\":33678},{\"end\":33681,\"start\":33680},{\"end\":33834,\"start\":33833},{\"end\":33846,\"start\":33845},{\"end\":34025,\"start\":34024},{\"end\":34039,\"start\":34038},{\"end\":34052,\"start\":34051},{\"end\":34054,\"start\":34053},{\"end\":34257,\"start\":34256},{\"end\":34266,\"start\":34265},{\"end\":34276,\"start\":34275},{\"end\":34286,\"start\":34285},{\"end\":34520,\"start\":34519},{\"end\":34678,\"start\":34677},{\"end\":34687,\"start\":34686},{\"end\":34696,\"start\":34695},{\"end\":34706,\"start\":34705},{\"end\":34708,\"start\":34707},{\"end\":34910,\"start\":34909},{\"end\":34922,\"start\":34921},{\"end\":35081,\"start\":35080},{\"end\":35095,\"start\":35094},{\"end\":35097,\"start\":35096},{\"end\":35258,\"start\":35257},{\"end\":35269,\"start\":35268},{\"end\":35279,\"start\":35278},{\"end\":35290,\"start\":35289},{\"end\":35302,\"start\":35301},{\"end\":35492,\"start\":35491},{\"end\":35494,\"start\":35493},{\"end\":35505,\"start\":35504},{\"end\":35507,\"start\":35506},{\"end\":35517,\"start\":35516},{\"end\":35530,\"start\":35529},{\"end\":35737,\"start\":35736},{\"end\":35744,\"start\":35743},{\"end\":35751,\"start\":35750},{\"end\":35759,\"start\":35758},{\"end\":35980,\"start\":35979},{\"end\":35992,\"start\":35991},{\"end\":36001,\"start\":36000},{\"end\":36015,\"start\":36014},{\"end\":36345,\"start\":36344},{\"end\":36352,\"start\":36351},{\"end\":36359,\"start\":36358},{\"end\":36567,\"start\":36566},{\"end\":36576,\"start\":36575},{\"end\":36585,\"start\":36584},{\"end\":36598,\"start\":36597},{\"end\":36792,\"start\":36791},{\"end\":36798,\"start\":36797},{\"end\":36964,\"start\":36963},{\"end\":36970,\"start\":36969},{\"end\":36972,\"start\":36971},{\"end\":36979,\"start\":36978},{\"end\":36986,\"start\":36985},{\"end\":37179,\"start\":37178},{\"end\":37185,\"start\":37184},{\"end\":37194,\"start\":37193},{\"end\":37368,\"start\":37367},{\"end\":37376,\"start\":37375},{\"end\":37383,\"start\":37382},{\"end\":37391,\"start\":37390},{\"end\":37554,\"start\":37553},{\"end\":37563,\"start\":37562},{\"end\":37569,\"start\":37568},{\"end\":37730,\"start\":37729},{\"end\":37739,\"start\":37738}]", "bib_author_last_name": "[{\"end\":31134,\"start\":31123},{\"end\":31145,\"start\":31138},{\"end\":31156,\"start\":31149},{\"end\":31328,\"start\":31325},{\"end\":31342,\"start\":31332},{\"end\":31349,\"start\":31346},{\"end\":31537,\"start\":31524},{\"end\":31544,\"start\":31541},{\"end\":31555,\"start\":31548},{\"end\":31564,\"start\":31559},{\"end\":31719,\"start\":31716},{\"end\":31725,\"start\":31723},{\"end\":31873,\"start\":31866},{\"end\":31880,\"start\":31877},{\"end\":31891,\"start\":31884},{\"end\":31902,\"start\":31895},{\"end\":31911,\"start\":31906},{\"end\":31920,\"start\":31915},{\"end\":31931,\"start\":31924},{\"end\":32189,\"start\":32183},{\"end\":32198,\"start\":32193},{\"end\":32211,\"start\":32202},{\"end\":32221,\"start\":32215},{\"end\":32232,\"start\":32225},{\"end\":32504,\"start\":32499},{\"end\":32513,\"start\":32508},{\"end\":32533,\"start\":32526},{\"end\":32542,\"start\":32537},{\"end\":32553,\"start\":32546},{\"end\":32750,\"start\":32744},{\"end\":32761,\"start\":32754},{\"end\":32774,\"start\":32765},{\"end\":32787,\"start\":32778},{\"end\":32936,\"start\":32934},{\"end\":32944,\"start\":32940},{\"end\":33082,\"start\":33080},{\"end\":33090,\"start\":33086},{\"end\":33251,\"start\":33246},{\"end\":33263,\"start\":33255},{\"end\":33275,\"start\":33267},{\"end\":33462,\"start\":33458},{\"end\":33468,\"start\":33466},{\"end\":33474,\"start\":33472},{\"end\":33486,\"start\":33478},{\"end\":33676,\"start\":33673},{\"end\":33685,\"start\":33682},{\"end\":33843,\"start\":33835},{\"end\":33853,\"start\":33847},{\"end\":34036,\"start\":34026},{\"end\":34049,\"start\":34040},{\"end\":34061,\"start\":34055},{\"end\":34263,\"start\":34258},{\"end\":34273,\"start\":34267},{\"end\":34283,\"start\":34277},{\"end\":34294,\"start\":34287},{\"end\":34526,\"start\":34521},{\"end\":34684,\"start\":34679},{\"end\":34693,\"start\":34688},{\"end\":34703,\"start\":34697},{\"end\":34716,\"start\":34709},{\"end\":34919,\"start\":34911},{\"end\":34928,\"start\":34923},{\"end\":35092,\"start\":35082},{\"end\":35109,\"start\":35098},{\"end\":35266,\"start\":35259},{\"end\":35276,\"start\":35270},{\"end\":35287,\"start\":35280},{\"end\":35299,\"start\":35291},{\"end\":35307,\"start\":35303},{\"end\":35502,\"start\":35495},{\"end\":35514,\"start\":35508},{\"end\":35527,\"start\":35518},{\"end\":35540,\"start\":35531},{\"end\":35741,\"start\":35738},{\"end\":35748,\"start\":35745},{\"end\":35756,\"start\":35752},{\"end\":35764,\"start\":35760},{\"end\":35989,\"start\":35981},{\"end\":35998,\"start\":35993},{\"end\":36012,\"start\":36002},{\"end\":36023,\"start\":36016},{\"end\":36035,\"start\":36025},{\"end\":36349,\"start\":36346},{\"end\":36356,\"start\":36353},{\"end\":36365,\"start\":36360},{\"end\":36573,\"start\":36568},{\"end\":36582,\"start\":36577},{\"end\":36595,\"start\":36586},{\"end\":36604,\"start\":36599},{\"end\":36795,\"start\":36793},{\"end\":36802,\"start\":36799},{\"end\":36967,\"start\":36965},{\"end\":36976,\"start\":36973},{\"end\":36983,\"start\":36980},{\"end\":36990,\"start\":36987},{\"end\":37182,\"start\":37180},{\"end\":37191,\"start\":37186},{\"end\":37198,\"start\":37195},{\"end\":37373,\"start\":37369},{\"end\":37380,\"start\":37377},{\"end\":37388,\"start\":37384},{\"end\":37396,\"start\":37392},{\"end\":37560,\"start\":37555},{\"end\":37566,\"start\":37564},{\"end\":37573,\"start\":37570},{\"end\":37736,\"start\":37731},{\"end\":37745,\"start\":37740}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5821064},\"end\":31276,\"start\":31087},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11842732},\"end\":31464,\"start\":31278},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1307614},\"end\":31694,\"start\":31466},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":154438},\"end\":31783,\"start\":31696},{\"attributes\":{\"doi\":\"arXiv:1310.1531\",\"id\":\"b4\"},\"end\":32131,\"start\":31785},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":40060575},\"end\":32439,\"start\":32133},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18417477},\"end\":32698,\"start\":32441},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3884789},\"end\":32906,\"start\":32700},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6311049},\"end\":33009,\"start\":32908},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8567612},\"end\":33198,\"start\":33011},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14582767},\"end\":33381,\"start\":33200},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11654786},\"end\":33623,\"start\":33383},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15749523},\"end\":33776,\"start\":33625},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7894188},\"end\":33957,\"start\":33778},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195908774},\"end\":34197,\"start\":33959},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14542261},\"end\":34469,\"start\":34199},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11951016},\"end\":34608,\"start\":34471},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1212427},\"end\":34857,\"start\":34610},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16638962},\"end\":35028,\"start\":34859},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13943046},\"end\":35218,\"start\":35030},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1742103},\"end\":35427,\"start\":35220},{\"attributes\":{\"id\":\"b21\"},\"end\":35687,\"start\":35429},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9794287},\"end\":35871,\"start\":35689},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1210309},\"end\":36272,\"start\":35873},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":591674},\"end\":36522,\"start\":36274},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10233982},\"end\":36731,\"start\":36524},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8000561},\"end\":36902,\"start\":36733},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7036324},\"end\":37111,\"start\":36904},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1916689},\"end\":37316,\"start\":37113},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10878983},\"end\":37524,\"start\":37318},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7657380},\"end\":37653,\"start\":37526},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1726588},\"end\":37866,\"start\":37655}]", "bib_title": "[{\"end\":31119,\"start\":31087},{\"end\":31321,\"start\":31278},{\"end\":31520,\"start\":31466},{\"end\":31712,\"start\":31696},{\"end\":32179,\"start\":32133},{\"end\":32495,\"start\":32441},{\"end\":32740,\"start\":32700},{\"end\":32930,\"start\":32908},{\"end\":33076,\"start\":33011},{\"end\":33242,\"start\":33200},{\"end\":33454,\"start\":33383},{\"end\":33667,\"start\":33625},{\"end\":33831,\"start\":33778},{\"end\":34022,\"start\":33959},{\"end\":34254,\"start\":34199},{\"end\":34517,\"start\":34471},{\"end\":34675,\"start\":34610},{\"end\":34907,\"start\":34859},{\"end\":35078,\"start\":35030},{\"end\":35255,\"start\":35220},{\"end\":35489,\"start\":35429},{\"end\":35734,\"start\":35689},{\"end\":35977,\"start\":35873},{\"end\":36342,\"start\":36274},{\"end\":36564,\"start\":36524},{\"end\":36789,\"start\":36733},{\"end\":36961,\"start\":36904},{\"end\":37176,\"start\":37113},{\"end\":37365,\"start\":37318},{\"end\":37551,\"start\":37526},{\"end\":37727,\"start\":37655}]", "bib_author": "[{\"end\":31136,\"start\":31121},{\"end\":31147,\"start\":31136},{\"end\":31158,\"start\":31147},{\"end\":31330,\"start\":31323},{\"end\":31344,\"start\":31330},{\"end\":31351,\"start\":31344},{\"end\":31539,\"start\":31522},{\"end\":31546,\"start\":31539},{\"end\":31557,\"start\":31546},{\"end\":31566,\"start\":31557},{\"end\":31721,\"start\":31714},{\"end\":31727,\"start\":31721},{\"end\":31875,\"start\":31864},{\"end\":31882,\"start\":31875},{\"end\":31893,\"start\":31882},{\"end\":31904,\"start\":31893},{\"end\":31913,\"start\":31904},{\"end\":31922,\"start\":31913},{\"end\":31933,\"start\":31922},{\"end\":32191,\"start\":32181},{\"end\":32200,\"start\":32191},{\"end\":32213,\"start\":32200},{\"end\":32223,\"start\":32213},{\"end\":32234,\"start\":32223},{\"end\":32506,\"start\":32497},{\"end\":32515,\"start\":32506},{\"end\":32535,\"start\":32515},{\"end\":32544,\"start\":32535},{\"end\":32555,\"start\":32544},{\"end\":32752,\"start\":32742},{\"end\":32763,\"start\":32752},{\"end\":32776,\"start\":32763},{\"end\":32789,\"start\":32776},{\"end\":32938,\"start\":32932},{\"end\":32946,\"start\":32938},{\"end\":33084,\"start\":33078},{\"end\":33092,\"start\":33084},{\"end\":33253,\"start\":33244},{\"end\":33265,\"start\":33253},{\"end\":33277,\"start\":33265},{\"end\":33464,\"start\":33456},{\"end\":33470,\"start\":33464},{\"end\":33476,\"start\":33470},{\"end\":33488,\"start\":33476},{\"end\":33678,\"start\":33669},{\"end\":33687,\"start\":33678},{\"end\":33845,\"start\":33833},{\"end\":33855,\"start\":33845},{\"end\":34038,\"start\":34024},{\"end\":34051,\"start\":34038},{\"end\":34063,\"start\":34051},{\"end\":34265,\"start\":34256},{\"end\":34275,\"start\":34265},{\"end\":34285,\"start\":34275},{\"end\":34296,\"start\":34285},{\"end\":34528,\"start\":34519},{\"end\":34686,\"start\":34677},{\"end\":34695,\"start\":34686},{\"end\":34705,\"start\":34695},{\"end\":34718,\"start\":34705},{\"end\":34921,\"start\":34909},{\"end\":34930,\"start\":34921},{\"end\":35094,\"start\":35080},{\"end\":35111,\"start\":35094},{\"end\":35268,\"start\":35257},{\"end\":35278,\"start\":35268},{\"end\":35289,\"start\":35278},{\"end\":35301,\"start\":35289},{\"end\":35309,\"start\":35301},{\"end\":35504,\"start\":35491},{\"end\":35516,\"start\":35504},{\"end\":35529,\"start\":35516},{\"end\":35542,\"start\":35529},{\"end\":35743,\"start\":35736},{\"end\":35750,\"start\":35743},{\"end\":35758,\"start\":35750},{\"end\":35766,\"start\":35758},{\"end\":35991,\"start\":35979},{\"end\":36000,\"start\":35991},{\"end\":36014,\"start\":36000},{\"end\":36025,\"start\":36014},{\"end\":36037,\"start\":36025},{\"end\":36351,\"start\":36344},{\"end\":36358,\"start\":36351},{\"end\":36367,\"start\":36358},{\"end\":36575,\"start\":36566},{\"end\":36584,\"start\":36575},{\"end\":36597,\"start\":36584},{\"end\":36606,\"start\":36597},{\"end\":36797,\"start\":36791},{\"end\":36804,\"start\":36797},{\"end\":36969,\"start\":36963},{\"end\":36978,\"start\":36969},{\"end\":36985,\"start\":36978},{\"end\":36992,\"start\":36985},{\"end\":37184,\"start\":37178},{\"end\":37193,\"start\":37184},{\"end\":37200,\"start\":37193},{\"end\":37375,\"start\":37367},{\"end\":37382,\"start\":37375},{\"end\":37390,\"start\":37382},{\"end\":37398,\"start\":37390},{\"end\":37562,\"start\":37553},{\"end\":37568,\"start\":37562},{\"end\":37575,\"start\":37568},{\"end\":37738,\"start\":37729},{\"end\":37747,\"start\":37738}]", "bib_venue": "[{\"end\":31162,\"start\":31158},{\"end\":31355,\"start\":31351},{\"end\":31570,\"start\":31566},{\"end\":31731,\"start\":31727},{\"end\":31862,\"start\":31785},{\"end\":32268,\"start\":32234},{\"end\":32559,\"start\":32555},{\"end\":32793,\"start\":32789},{\"end\":32950,\"start\":32946},{\"end\":33096,\"start\":33092},{\"end\":33281,\"start\":33277},{\"end\":33492,\"start\":33488},{\"end\":33691,\"start\":33687},{\"end\":33859,\"start\":33855},{\"end\":34067,\"start\":34063},{\"end\":34319,\"start\":34296},{\"end\":34532,\"start\":34528},{\"end\":34722,\"start\":34718},{\"end\":34934,\"start\":34930},{\"end\":35115,\"start\":35111},{\"end\":35313,\"start\":35309},{\"end\":35546,\"start\":35542},{\"end\":35770,\"start\":35766},{\"end\":36049,\"start\":36037},{\"end\":36379,\"start\":36367},{\"end\":36610,\"start\":36606},{\"end\":36808,\"start\":36804},{\"end\":36996,\"start\":36992},{\"end\":37204,\"start\":37200},{\"end\":37405,\"start\":37398},{\"end\":37579,\"start\":37575},{\"end\":37751,\"start\":37747},{\"end\":34329,\"start\":34321}]"}}}, "year": 2023, "month": 12, "day": 17}