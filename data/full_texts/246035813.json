{"id": 246035813, "updated": "2023-10-05 17:45:43.16", "metadata": {"title": "Using Pre-Trained Models to Boost Code Review Automation", "authors": "[{\"first\":\"Rosalia\",\"last\":\"Tufano\",\"middle\":[]},{\"first\":\"Simone\",\"last\":\"Masiero\",\"middle\":[]},{\"first\":\"Antonio\",\"last\":\"Mastropaolo\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Pascarella\",\"middle\":[]},{\"first\":\"Denys\",\"last\":\"Poshyvanyk\",\"middle\":[]},{\"first\":\"Gabriele\",\"last\":\"Bavota\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.06850", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icse/TufanoMMPPB22", "doi": "10.1145/3510003.3510621"}}, "content": {"source": {"pdf_hash": "c7cbbbdba2c942180675c2a69035e506d9378679", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.06850v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c30790ef35c2962b8a1ecd6bfcbaeba41abdc369", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c7cbbbdba2c942180675c2a69035e506d9378679.txt", "contents": "\nUsing Pre-Trained Models to Boost Code Review Automation\n\n\nRosalia Tufano \nSimone Masiero \nAntonio Mastropaolo \nLuca Pascarella \nDenys Poshyvanyk \nGabriele Bavota \n\nSEART @ Software Institute\nSEART @ Software Institute\nSEART @ Software Institute\nUniversit\u00e0 della Svizzera italiana Switzerland\nUniversit\u00e0 della Svizzera italiana\nSwitzerland\n\n\nSEART @ Software Institute\nUniversit\u00e0 della Svizzera italiana\nSwitzerland\n\n\nSEMERU @ Computer Science Department William and Mary\nSEART @ Software Institute\nUniversit\u00e0 della Svizzera italiana\nSwitzerland, USA\n\n\nUniversit\u00e0 della Svizzera italiana\nSwitzerland\n\nUsing Pre-Trained Models to Boost Code Review Automation\nCode ReviewEmpirical StudyMachine Learning on Code\nCode review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.\n\nINTRODUCTION\n\nThe benefits of code reviews have been widely recognized, with several studies providing evidence of the higher quality of reviewed code [15,29,31]. Also, code reviews help in preventing bugs and foster knowledge transfer among developers [10,40]. However, studies on code reviews also highlighted an additional cost that such a process entails: Empirical evidence suggests that large software projects can undergo hundreds of code reviews per month. This applies to both open-source (e.g., \u223c500 reviews per month in Linux [39]) and industrial (e.g., \u223c3k reviews per month in Microsoft Bing [38]) projects. As a result, developers can spend many hours per week reviewing code [16].\n\nGiven the non-negligible cost of code review, we recently proposed the automation of specific code review tasks: The goal is not to replace developers, but to help them save time in two scenarios. The first is that of a contributor (i.e., the developer submitting the code for review) who wants to receive a rapid feedback about the code they wrote before submitting it for review. The feedback is provided by a Deep Learning (DL) model trained to take as input the code to submit for review and provide as output a revised version of (i.e., ) implementing code changes that are likely to be recommended by a reviewer.\n\nThe second scenario concerns the reviewer(s) involved in the process: a DL model is trained to take as input (i) the code submitted for review, and (ii) a comment written by the reviewer in natural language to request a specific change on . The output of the model is a revised version of (i.e., ) implementing the changes recommended in . The idea here is that the reviewer can use the model to provide the contributor with a concrete example of the code changes that they would like to see implemented.\n\nIn our previous work [46] we trained and experimented with the DL models on a dataset composed of \u223c17k triplets \u27e8 , , \u27e9 extracted from code reviews performed in GitHub [2] and Gerrit [1]. In particular, the model recommending code changes to the contributor is an encoder-decoder model with one encoder taking as input and one decoder generating . Our evaluation shows that this model can recommend a change as a reviewer would do in 3% (single prediction) to 16% of the cases (10 different predictions). The model employed in the second scenario (i.e., the automated implementation of a comment recommended by the reviewer), has instead two encoders taking as input and , respectively, and one decoder generating . This model can successfully implement a change recommended by a reviewer in 12% (single prediction) to 31% (10 different predictions) of the cases.\n\nWhile these results represent our first step towards automating code review tasks, our approach [46] as well as the conducted empirical study suffers of several limitations we try to overcome in this paper. First, we adopted a code abstraction process to reduce the vocabulary size and simplify the learning of the DL model. This means that the model did not work on the raw source code, but on an abstracted version of it in which, for example, variable identifiers were replaced with a special VAR_ID token, where ID is a progressive number (e.g., the second declared variable is represented by VAR_2). The possibility to go back to raw source code was guaranteed by keeping a map linking abstracted to raw tokens in (e.g., VAR_1 \u2192 i).\n\nWhile such a procedure simplifies the learning of the model, it poses a strong limitation on the variety of code review tasks that can be supported by such a model. Indeed, the abstraction process forces to exclude from the dataset of triplets \u27e8 , , \u27e9 all those in which introduces identifiers or literals that were not present in . This is necessary because the abstraction map is built on and, if a new variable VAR_2 is introduced in during the review process, such a variable cannot be mapped back to raw source code, making such an approach unusable in practice. This means that the triplets \u27e8 , , \u27e9 on which we evaluated our approach [46] were relatively simple changes implemented during code review, not requiring the introduction of new identifiers or literals.\n\nSecond, to simplify the learning, we only considered triplets \u27e8 , , \u27e9 in which both the code submitted for review ( ) and the revised code ( ) had no more than 100 tokens [46]. Again, this reduced the complexity of the tackled problem.\n\nBasically, the two above choices resulted in training and experimenting the proposed models on quite simple code review instances only representative of a minority of the code transformations actually implemented during code reviews.\n\nIn this paper, we build on top of our previous work [46] experimenting with DL models for code review automation in more realistic and challenging scenarios. We start by training the recently proposed Text-To-Text-Transfer Transformer (T5) model [35] on a dataset similar to the one used in [46]. However, we adopt a tokenizer (i.e., SentencePiece [26]) that allows us to work with raw source code, without the need for code abstraction. Also, we increase the maximum length of the considered code components from 100 \"abstracted\" tokens to 512 \"SentencePiece\" tokens (i.e., \u223c390 \"abstracted\" tokens). The absence of an abstraction mechanism and the increased upper bound for input/output length allowed us to build a substantially larger dataset as compared to the one used in [46] (168k instances vs. 17k) and, more importantly, to feature in such a dataset a wider variety of code transformations implemented in the code review process, including quite challenging instances such as those requiring the introduction of new identifiers and literals (accounting for 63% of the new dataset we built). Also, we experimented with the automation of a third task related to the code review process: Given the code submitted for review ( ), generating a natural language comment requesting to the contributor code changes as a reviewer would do (i.e., simulating a reviewer commenting on the submitted code).\n\nWe also compare the T5 model with the encoder-decoder model presented in our previous work on the original dataset used in [46]. Our results show the superior performance of T5, which represents a significant step forward in automating code review tasks. To summarize, the contributions of this work are:\n\n(i) A novel approach for code review automation overcoming several limitations of the state-of-the-art technique [46];\n\n(ii) A comprehensive empirical evaluation of such an approach, including a comparison with our previous technique [46];\n\n(iii) The automation of a third task: Given the code submitted for review, automatically generating natural language comments requesting changes as reviewers would do;\n\n(iv) A code review dataset to train and test DL models in more realistic scenarios as compared to the one used in [46];\n\n(v) A comprehensive replication package [8].\n\n\nT5 TO AUTOMATE CODE REVIEW\n\nWe describe the DL model we adopt, the construction process of the datasets needed for its training, and the procedure used for hyperparameter search, model training, and generation of predictions.\n\n\nText-to-Text Transfer Transformer (T5)\n\nThe Text-to-Text Transfer Transformer, or simply T5, is not merely a model. Raffel et al. [35] compare \"pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks\". The result of this exploration is the best combination of architectures and training techniques, namely T5. T5 is based on the Transformer [48] architecture. The proposed implementation differs only in some details (regarding the normalization layer and the embedding scheme) from its original form. Raffel et al. proposed several versions of T5, differing from each other in their size (e.g., number of layers) and, as a consequence, training complexity. In this work we adopt the small version of T5 consisting of: 8-headed attention, 6 layers in both the encoder and the decoder, each having a dimensionality of 512 and the output dimensionality of 2,048 (\u223c 60M parameters).\n\nThe model is subjected to a first training (pre-training) whose purpose is to provide it with a general knowledge useful to solve a set of related tasks. Suppose, for example, that we want to train a model able to (i) translate English to German, and (ii) summarize English text. Instead of starting by training the model for these two tasks, T5 can be pre-trained in an unsupervised manner by using the denoising objective (or masked language modeling): The model is fed with sentences having 15% of their tokens (e.g., words in English sentences or code tokens in Java statements) randomly masked and it is asked to predict them. By learning how to predict the masked tokens, the model can acquire general knowledge about the language of interest. In our example, we could pre-train the model on English and German sentences.\n\nOnce pre-trained, T5 is fine-tuned on the downstream tasks in a supervised fashion. Each task is formulated in a \"text-to-text\" format (i.e., both the input and the output of the model are represented as text). For example, for the translation task a dataset composed of pairs of English and German sentences allows to fine-tune the model. Similarly, the summarization task requires the input English text and a corresponding summary. In the next sections we explain how we pre-train and fine-tune T5 to support code review tasks.\n\n\nTraining Data\n\nWe describe the process used to build the datasets needed for the pre-training (Section 2.2.1) and fine-tuning (Section 2.2.2) of T5. Part of the fine-tuning dataset has been used for hyperparameter search (Section 2.3) and for testing the performance of T5 (Section 3).\n\n\nPre-training Dataset.\n\nGiven the goal of the pre-training phase (i.e., providing the model with general knowledge about the languages of the downstream tasks) we built a dataset allowing to train T5 on Java and technical English.\n\nIndeed, besides source code, technical English is instrumental in a code review process in which reviewers post natural language comments about code.\n\nWe start from two datasets featuring instances including both source code and technical English: the official Stack Overflow dump (SOD) [7] and CodeSearchNet (CSN) [25]. Stack Overflow is a Q&A website for programmers. The data dump we used collects all the questions and relative answers between 2006 and 2020 for a total of roughly 51M posts (where a post is a single question or answer). A post includes English text (as per the SO guidelines) and/or code snippets. Posts are usually accompanied by tags characterizing their topic (e.g., Java, Android) and can be rated with up-/down-votes and, for what concerns the answers, they can be marked as the \"accepted answer\" from the question's author.\n\nWe extracted from the SOD all the answers (i) having a Java tag; (ii) containing at least one <pre><code> HTML tag to ensure the presence of at least one code snippet in the answer; and (iii) having at least 5 up-votes and/or being the accepted answer. These filters are justified by the goal of our pre-training. Indeed, we want the model to acquire knowledge about technical English and Java: focusing on answers containing at least one code snippet increases the chances that their natural language text refers to an implementation task, similarly to what happens in code review. Also, the up-votes/accepted answer filter aims at discarding low-quality instances containing, for example, wrong code solutions. This is also the reason why we focused on high-quality answers likely to contain working solutions rather than on questions that, even if up-voted (e.g., because they are relevant for many users) may contain wrong implementations. From this step we obtained 1,018,163 candidate instances from the SOD.\n\nOn each selected answer , we performed the following cleaning steps: We remove emojis, non-latin characters, control characters, trailing spaces and multiple white spaces. Some special symbols are replaced using latin characters having the same meaning, e.g., \"\u2265\" is replaced with \">=\". Moreover, we replace any embedded link with a special tag \"<LINK_i>\", with being an integer ranging from 0 to \u2212 1, where is the number of links in . Finally, we removed all the instances having less than ten tokens or more than 512 (40,491). This left us with 977,379 valid instances.\n\nThe CSN [25] Java dataset features 1.5M unique Java methods, some of which containing their Javadoc. We filtered out all those in which a Javadoc was not available or it did not contain any letter, removing 1,034,755 of them. Unlike the SOD, CSN can contain instances in which the \"textual part\" (i.e., the method comment) is not in English. To partially address this issue, we exclude pairs in which no Latin characters were found. While this does not exclude all non-English comments, at least identifies and removes those written in specific languages (e.g., Russian, Chinese) (15,229). We decided to accept some level of noise in the pre-training dataset (e.g., comments written in French) since (i) given the size of this dataset, this little amount of noise should not substantially affect the model's performance, and (ii) the pre-training dataset is not used as test set to assess the performance of the approach. As we will explain later, a more fine-grained cleaning has been performed for the fine-tuning dataset that, instead, is used for performance evaluation. On the 519,905 remaining instances, we performed the same cleaning steps described for the SOD (e.g., remove emojis). Finally, from each pair we obtain a single string concatenating the Javadoc comment and the code, retaining the ones having more than ten and less than 512 tokens (507,947 instances left).\n\nBy putting together the instances collected from the SOD and CSN we obtained the pre-training dataset consisting of 1,485,326 instances. To perform the pre-training, we randomly mask in each instance 15% of its tokens. The masked tokens are replaced with sentinel tokens <extra_id_i>, where is an increasing number ranging from 0 up to \u2212 1, where is the number of tokens masked in a given instance. If several contiguous tokens are masked they are replaced by a single sentinel token. These \"masked instances\" represent the input of the model during the pre-training. The target (i.e., the string the model is expected to generate) is built concatenating the sentinel tokens and the token(s) they are masking. An extra sentinel token is added to indicate the end of the string.\n\nOur pre-training dataset is publicly available [8].\n\n\nFine-tuning Datasets.\n\nTo create the fine-tuning dataset we mined Java open source projects from GitHub using the web application by Dabic et al. [19]. Using the querying interface [5], we selected all Java projects having at least 50 pull requests (PRs), ten contributors, ten stars, and not being forks. The filters aim at (i) ensuring that enough \"code review\" material is contained in the projects (i.e., at least 50 PRs); (ii) discarding personal/toy projects (at least ten contributors and stars); and (iii) reducing the chance of mining duplicated code. This resulted in a list of 4,901 projects. We also mined the six Gerrit [1] installations used in [46] containing code review data about 6,388 projects. From both the GitHub and the Gerrit datasets we extract triplets < , , >, where is a method submitted for the review; is a single reviewer's comment suggesting code changes for ; and is the revised version of implementing the reviewer's recommendation . Note that (i) we only looked for PRs that are accepted at the end of the code review, since we want to learn how to recommend changes that, at the end, can lead to code considered good from a reviewer's perspective; and (ii) a single PR in GitHub and Gerrit can result in several triplets for our dataset. Indeed, we mine the different review rounds in each PR. For example, a method can be submitted for review, receiving a comment asking for changes (first round). The revised version of addressing is then resubmitted ( ), resulting in the second review round (possibly leading to additional comments and revisions of the method). We stop when the code is formally accepted.\n\nOverall, we mined 382,955 valid triplets from GitHub and Gerrit using the pipeline from [46] that we summarize in the following (see [46] for additional details). We target triplets in which a comment has been posted by a reviewer on a method . We can identify these cases since both GitHub and Gerrit (i) provide information about the developers submitting the code and posting comments in the review process; and (ii) allow to retrieve the specific code line(s) refers to (i.e., the code in that has been highlighted by the reviewer when posting the comment).\n\nWe exclude all the comments posted by the authors of the code (e.g., to reply to reviewers), since they do not represent a review of the code. Thus, the triplets in our dataset have being a single comment posted by a reviewer. Also, we exclude linked to inline comments (rather than code lines) in , since we target the fixing of code-related issues. To consider a triplet as valid, must be the only comment posted by a reviewer on in that specific review round.\n\nIn this way, we can be confident that the revised version submitted later on by the author ( ) actually aimed at implementing . Also, must differ from (i.e., a change must have been implemented in the code to address ). From the technical point of view, the parsing of the methods from the patches submitted for review has been done using the lizard library [4]. Note that, the removal of triplets in which include more than one comment has been done later in the processing pipeline (we will get back to this point). Indeed, before we had to clean comments possibly just representing noise.\n\nAs done for the pre-training dataset, we performed some cleaning steps. We replaced any link with the numbered token <LINK_i>, with being an integer ranging from 0 to \u2212 1, where is the total number of links in , and . If the same link appears in different parts (e.g., in and ), it is replaced with the same token. We also removed any emoji and non-ascii characters from the comments, extra spaces and control characters from both the comments and the methods, and inline comments from the methods (we are not interested in addressing issues related to internal comments).\n\nAfter the cleaning process we obtained some triplets in which became an empty string or where and became equal (e.g., they only differed for some spaces before the cleaning). We removed these instances (-33,005) as well as those having + or longer than 512 tokens (-61,233). We considered the sum of and in terms of length because, for one of the tasks (i.e., the automated implementation of a comment posted by a reviewer), they will be concatenated to form the input for the model.\n\nThen, we removed from our triplets non-relevant comments (-28,581), i.e., comments not recommending code change suggestions (e.g., \"looks good to me\"). In [46] we manually crafted a set of natural language patterns to spot non-relevant comments (e.g., single-word comments containing words such as \"thanks\", \"nice\", etc.). We have extended this set since we noticed that in our richer dataset several non-relevant comments were left by these patterns. Such analysis has been done by one of the authors by manually inspecting all the triplets having consisting of less than six words. The updated heuristics are available in our replication package [8].\n\nWe also excluded triplets including non-English comments (-4,815) through a pipeline composed by three language detector tools. A preliminary classification has been performed using the Python libraries langdetect [3] and pycld3 [6]. If both of these tools classify the comment as non-English, we relied on the Google language detection API for a final decision. Such a process was needed since we noticed that the Google API was the most accurate in detecting the language, especially when the comments also featured code constructs in them. In this scenario, the Python libraries often generated false negatives (i.e., classifying an English sentence as non-English). However, we had a limited number of requests available for the Google API. Thus, we performed a pre-filtering using the Python libraries and, when they both reported the comment as being not in English, we double checked using the Google API.\n\nAfter this cleaning process, we excluded all triplets featuring more than one comment in (-86,604). Finally, we removed all the duplicates from the fine-tuning dataset (-918). To be conservative, we identify as duplicates two triplets having the same (thus, even triplets having the same but different / have been removed).\n\nThe resulting dataset features 167,799 triplets that have been used to build the three fine-tuning datasets needed for the three tasks we aim at automating. In the first task (code-to-code) the model takes as input with the goal of automatically generating its revised version , implementing code changes that may be required in the code review process. Thus, the fine-tuning dataset is represented by pairs \u2192 . In the second task (code&comment-to-code) the model takes as input both and a comment posted by the reviewer and targets the generation of , the revised version of implementing the code changes recommended in .\n\nThe code contains two special tags <START>, <END> marking the portion of the code refers to. The fine-tuning dataset of this second task is represented by pairs < , >\u2192 . Finally, in the third task (code-to-comment) the model takes as input and aims at generating a natural language comment ( ) suggesting code changes as a reviewer would do. The fine-tuning dataset is represented by pairs \u2192 . All three fine-tuning datasets have been split into 80% training, 10% evaluation, and 10% test. Table 1 summarizes the number of instances in the datasets: The pre-training is only used for training, while the fine-tuning datasets are exploited also for the hyperparameter tuning (evaluation) and for assessing the performance of the model (test). In Table 1 we only report information for a single fine-tuning dataset (rather than for the three previously described), since all three fine-tuning datasets contain the same number of instances. Indeed, they are all derived from the same set of triplets.\n\n\nTraining and Hyperparameter Search\n\nRaffel et al. [35] showed the major role pre-training plays on the performance of T5 models. The importance of pre-training has also been confirmed (for other Transformer-based models) in the context of code-related tasks such as test case generation [44]. To further study this aspect, we decided to experiment with both a pre-trained and a non pre-trained model, both of which have been subject to a hyperparameter tuning process.\n\nSince we adopted the small version of T5 presented by Raffel et al. [35], we did not experiment with variations related to its architecture (e.g., changing the number of layers or the number of hidden units). Though, as also done by Mastropaolo et al. [28], we experimented with different learning rate configurations: (i) Costant Learning Rate (C-LR), in which the learning rate value is fixed during the training; (ii) Inverse Square Root Learning Rate (ISR-LR), in which the learning rate value decays as the inverse square root of the training step; (iii) Slanted Triangular Learning Rate (ST-LR) in which first the learning rate linearly increases and then it linearly decays returning to the starting value; (iv) Polynomial Decay Learning Rate (PD-LR), in which the learning rate polynomially decays to a fixed value in a given number of steps.\n\nThe hyperparameter tuning has been done for the fine-tuning phase only. Indeed, even though we just focus on one hyperparameter, such a process still remains quite expensive, requiring the training of eight different T5 models (i.e., pre-trained and non pre-trained each with four different learning rates).\n\nFor pre-training we use the same configuration proposed by Raffel et al. in [35]. We pre-trainied the model on the pre-training dataset (Table 1) for 200k steps (\u223c34 epochs). Starting from the pretrained model, we fine-tuned for 75k steps four different models, each using one of the experimented learning rates.\n\nSince the goal of this procedure is to find the best learning rate for the three code review tasks, we fine-tuned each of these models using a mixture of the three tasks: A single model is trained to support all three tasks using the union of their training sets. This is one of the characteristics of T5, the possibility to train a single model for multiple tasks. The same approach has been used for the non pre-trained model: In this case four T5 models (one per learning rate) have been directly fine-tuned.\n\nWe assessed the performance of the eight models on the evaluation set of each task in terms of \"perfect predictions\", namely cases in which the generated output was identical to the target (expected) string. Table 2 reports the achieved results. As it can be seen, no learning rate achieves the best results in all the tasks. Nevertheless, ST-LR shows better overall performance and, for this reason, is the one we adopt in our experiments. Given the best configuration for both the pre-trained and the non pre-trained models, we fine-tuned them for a maximum of 300k steps using an early stop strategy. This means that we saved a checkpoint of the model every 10k steps computing its performance in terms of \"perfect predictions\" on the evaluation set and stopped the training if the performance of the model did not increase for three consecutive checkpoints (to avoid overfitting).\n\n\nGenerating Predictions\n\nOnce the models are trained, they can be used to generate predictions. As done in previous work, we adopt a beam search strategy [36] to generate multiple predictions given a single input. For example, in the case of the code-to-code task, for a single method provided as input multiple candidates can be generated. When we ask the model to generate predictions, it generates the most probable sequences of tokens given the input sequence; is known as the beam size and we experiment with = 1, 3, 5, 10.\n\nFor each prediction generated by T5, we also exploited its score function to assess the model's confidence on the provided input.\n\nThe value returned by this function ranges from minus infinity to 0 and it is the log-likelihood ( ) of the prediction. Thus, if it is 0, it means that the likelihood of the prediction is 1 (i.e., the maximum confidence, since (1) = 0), while when it goes towards minus infinity, the confidence tends to be 0. In our empirical study (Section 3) we assess the reliability of the confidence level as a proxy for the quality of the predictions.\n\n\nSTUDY DESIGN\n\nThe goal of our evaluation is to empirically assess the performance of the T5 model in code review automation tasks. The context consists of (i) the datasets we presented in Section 2; and (ii) the dataset from our previous work [46]. From now on we refer to our previously presented approach as the baseline. The study aims at tackling five research questions (RQs).\n\nRQ 1 : To what extent is T5 able to automatically recommend code changes to developers as reviewers would do? We provide as input to T5 a Java method submitted for review and assess the extent to which the model is able to provide as output a revised version of ( ) implementing code changes that will be likely requested during the code review process. The idea here is that such a model could be used before the code is submitted for review as an automated check for the contributor.\n\nRQ 2 : To what extent is T5 able to automatically implement code changes recommended by reviewers? Given a Java method submitted for review ( ) and a natural language comment ( ) in which a reviewer asks to implement specific code changes in , we assess the ability of T5 to automatically revise to address (thus obtaining a revised method ).\n\nThe third RQ focuses on the novel code review-related task we introduce in this paper:\n\nRQ 3 : To what extent is T5 able to automatically recommend changes in natural language as reviewers would do? In this RQ T5 is provided as input with a Java method submitted for review ( ) and it is required to generate a natural language comment ( ) requesting code changes as reviewers would do.\n\nFor RQ 1 -RQ 3 , we experiment with different variants of the T5 model. In particular, we assess the quality of T5 predictions for all three tasks when (i) the model is pre-trained or not; and (ii) the predictions have different confidence levels. Thanks to these analyses, we can answer our fourth RQ:\n\nRQ 4 : What is the role played by the model pre-training on the performance of T5? How does the confidence of the predictions affects their quality? As explained in Section 2.3, we perform an ablation study in which T5 is fine-tuned without any pre-training (i.e., by starting from random weights in the neural network). This allows to assess the contribution of the pre-training to the performance of the model. As for the confidence of the predictions, we assess whether it can be used as a reliable proxy for the quality of the predictions (i.e., the higher the confidence, the higher the likelihood the prediction is correct). If this is the case, such a finding would have implications for the usage of the T5 model in practice: A developer using the model could decide to receive recommendations having confidence higher than , reducing the chances of receiving meaningless predictions.\n\nFinally, the last RQ compares the performance of the T5 model with that of the approach we presented in [46]: RQ 5 : What is the performance of T5 as compared to the state-of-the-art technique? We use the implementation and datasets from our previous work to compare the performance of the T5 model with the baseline [46].\n\n\nData Collection and Analysis\n\nTo answer the first four research questions, we experiment with the best configuration of both the pre-trained and non pre-trained T5 model on the test set of the fine-tuning dataset reported in Table 1.\n\nRemember that for each of the three tasks we support (i.e., the ones that map to RQ 1 , RQ 2 , and RQ 3 ) the 16,779 test set instances are the same triplets < , , >. The only difference is that: in RQ 1 the model has been trained (and is tested) to take as input and produce ; in RQ 2 it takes as input and and produces ; in RQ 3 it takes as input and produces . By running the models on the test sets, we report for each of the three tasks the percentage of \"perfect predictions\", namely the cases in which the output of the model is the expected one. For example, in the case of RQ 3 , this means that the model was able, given as input, to generate a comment identical to the one manually written by the reviewer who inspected .\n\nBesides computing the perfect predictions, in RQ 3 (i.e., the task in which the model is required to generate natural language text), we also compute the BLEU (Bilingual Evaluation Understudy) score of the predictions [32]. BLEU assesses the quality of the automatically generated text. The BLEU score ranges between 0 and 1, with 1 indicating, in our case, that the natural language comment generated by the model is identical to the one manually written by the reviewer. We use the BLEU-4 variant, that computes the overlap in terms of 4-grams between the generated and the reference text.\n\nIn RQ 1 and RQ 2 (i.e., in the tasks in which the model is required to generate code), we adopt instead the CodeBLEU [37], a recently proposed similarity metric inspired by the BLEU score but tailored to assess the quality of automatically generated code.\n\nDifferently from BLEU, CodeBLEU computes not only an \"ngram based similarity\" but it also considers how similar the abstract syntax tree and the data-flow of the generated and the reference code are. Ren et al. [37], who proposed the CodeBLEU, showed that their metric better correlates with developers' perception of code similarity as compared to the BLEU metric.\n\nConcerning RQ 4 , we compare the results (i.e., perfect predictions, BLEU, CodeBLEU) achieved by the T5 model with and without pre-training. We also statistically compare the two models (i.e., with/without pre-training) using the McNemar's test [30] and Odds Ratios (ORs) on the perfect predictions they can generate. As for the confidence of the predictions, we take the best performing model (i.e., the one with pre-training) and split its predictions into ten buckets based on their confidence going from 0.0 to 1.0 at steps of 0.1 (i.e., the first interval includes all predictions having a confidence with 0 < \u2264 0.1, the last interval has 0.9 < \u2264 1). Then, we report for each interval the percentage of perfect predictions.\n\nFinally, in RQ 5 , we compare T5 with the baseline [46] on the two tasks automated in our previous work (i.e., the ones related to our RQ 1 and RQ 2 ).\n\nAs metrics for the comparisons, we used the percentage of perfect predictions and the CodeBLEU of the predictions. We compared the two techniques in several scenarios. First, we used the dataset from [46] featuring 17,194 triplets < , , >. By performing some checks on this dataset, we noticed that a few instances (97) had comments ( ) not written in English or containing invalid unicode characters that did not allow our tokenizer to work. Thus, we excluded those instances from the training and the test sets shared by the authors. The training set has then been used to (i) train the baseline [46]; and (ii) fine-tune the T5 model without any pre-training. In this way, we can compare the performance of the two models on the test set when trained on exactly the same data. Important to notice is that the baseline has been trained and tested on abstracted code (as done in [46]), while T5 worked directly with the raw source code.\n\nOn top of this, we also report the performance of the pre-trained T5 model when run on the test set from [46]. This pre-trained model has been fine-tuned using the training dataset in [46]. Clearly, this analysis favors T5 since it has been trained on more data (i.e., the pre-training dataset). However, it provides additional hints into the role played by the pre-training and on the effectiveness of the T5 model in general. Besides reporting descriptive statistics, we statistically compare the two models using the McNemar's test [30] and Odds Ratios (ORs) on the perfect predictions they can generate. Since multiple comparisons are involved (e.g., comparing the pretrained and the non pre-trained model to the baseline), we adjust the -values using the Holm's correction [24].\n\n\nRESULTS DISCUSSION\n\nWe start by answering RQ 1 -RQ 3 (Section 4.1), presenting the performance of T5 in the three tasks we aim at automating. Then, we discuss the impact on the performance of the pre-training and the reliability of the confidence level as a proxy for the quality of the predictions (Section 4.2). Finally, we compare T5 with the baseline [46] (Section 4.3).\n\n4.1 RQ 1 -RQ 3 : Performance of T5 Fig. 1 reports two graphs for each task. The line chart on top shows the percentage of perfect predictions ( -axis) achieved by T5 for different beam sizes ( -axis); the continuous line represents the pre-trained version of the model, while the dashed line the non pretrained one. The boxplots at the bottom report the CodeBLEU for the two code-generation tasks (i.e., code-to-code and code&commentto-code) and the BLEU score for the code-to-comment task in which text is generated. Lighter blue represents the pre-trained model.\n\nWe start by commenting on the perfect predictions (line charts). At a first sight, the performance of the model might seem quite low. For example, in the case of code-to-code at = 1 (i.e., a single prediction is proposed by T5), both the pre-trained and the non pre-trained models achieve \u223c5% of perfect predictions (751 and 863 instances correctly predicted with and without pre-training, respectively). However, such a result should be considered in the context of what was reported by the state-of-the-art technique [46] that, on a much simpler test dataset, achieved for the same task and same beam size 2.91% of perfect predictions.  Similar observations can be made for the code&comment-to-code task, where at = 1 T5 can generate 14.08% (2,363 instances) and 12.06% (2,024) perfect predictions when pre-trained and not, respectively. For this task, in our previous work [46], we achieved on a simpler dataset 12.16% perfect predictions. We directly compare the two approaches in RQ 5 .\n\nInterestingly, increasing the beam size from 1 to 10 does only result in marginal improvements for all tasks. The largest improvement is obtained for the code&comment-to-code, where we move from 14.08% ( = 1) to 18.88% ( = 10) of perfect predictions for the pre-trained model. Given the goal of our approach, we believe that the most relevant performance are those achieved at = 1.\n\nIndeed, providing several recommendations to inspect to a developer might be counterproductive, especially considering that the recommendations are entire methods in the case of the two code-generation tasks.\n\nMoving to the code-to-comment task, T5 struggles in formulating natural language comments identical to the ones written by reviewers. The pre-trained model, at = 1, generates 356 correct comments (2.12%) against the 324 (1.93%) of the non pre-trained model. These numbers only slightly increase at = 10, with a maximum of 2.44% perfect predictions achieved with pre-training.\n\nThe top part of Fig. 2 shows two examples of perfect predictions generated by the model for each task. A dashed line separates the two examples within each task. For the code-to-code task, the first code in each example represents the input of the model, while the second its output. We highlighted in bold the parts of code changed by the model and replaced irrelevant parts of the methods with [...] to save space. In the first code-to-code example, T5 removes an unneeded instanceof check, since FileSystemDataset is a subclass of Dataset. Instead, the second example simplifies the checking for the existence of a cluster, providing a meaningful error message. This second case cannot be supported by the baseline [46], since it requires the introduction of new code tokens that were not present in the input code. Remember that, these being perfect predictions, the implemented changes are identical to those performed by developers during code review.\n\nFor the code&comment-to-code task, the input provided by the model includes the comment written by the reviewer and requiring a specific change to the part of code highlighted in orange. In the first example, the reviewer suggests to use a specific object to perform the null check and T5 correctly implements the change.\n\nThe second one is interesting because, despite the reviewer highlighting return null as the relevant code for their comment (\"else is redundant\"), the model correctly understands that the action to take is the removal of the unneeded else statement.\n\nFinally, for the code-to-comment task, we report the code provided as input to the model (first line) with the comment it generated as output (second line). In the first example, T5 suggests (as done by the real reviewer) to add a null check, also showing the code needed for its implementation. This code is not just a template, but it is suitable for the provided input code (it refers to the supplier object). In the second example, T5 suggests to rename an identifier, providing valid recommendations for the renaming.\n\nLooking at the bottom of Fig. 1, the results in terms of CodeBLEU show a median higher than 0.80 for all beam sizes and for both code-generation tasks. However, while we report these values for completeness and for being consistent with what done in similar works [45,46,50], they say little about the quality of the predictions and they are mostly useful for future work that wants to compare with our approach (complete distributions are available in our replication package [8]). Indeed, it is difficult to properly interpret these values for two reasons. First, there is no accepted threshold above which good performance can be claimed. Second, as also done in previous works proposing models taking as input a code snippet and providing as output the same code \"revised\" in some way (e.g., with a fixed bug [45], with a single statement added [50], or with review-related changes implemented [46]), we computed the Code-BLEU between the predicted and the target code (two methods in our case). However, the input provided to the model is already quite similar to the target output, which means that a model taking as input a method and not implementing any change on it, is likely to obtain high values of CodeBLEU. For this reason, we mostly focus our discussion on perfect predictions. Concerning the BLEU score achieved in the code-to-comment task, the median ranges around 0.10 (see Fig. 1). Such a result is expected given the low percentage of perfect predictions achieved for this task.\n\nGoing back to the perfect predictions, the results reported in the line charts in Fig. 1 represent a lower bound for the performance of our approach. Indeed, we consider a prediction as \"perfect\" only if it is identical to the reference one. For example, in the case of the code-to-comment task, the natural language comment generated by T5 is classified as correct only if it is equal to the reference one, including punctuation.  However, it is possible that a natural language comment generated by T5 is different but semantically equivalent to the one written by the developer (e.g., \"variable should be private\" vs \"change visibility to private\"). Similar observations hold for the two code-generation tasks (e.g., a reviewer's comment could be addressed in different but semantically equivalent ways).\n\n\ncode-to-comment \"Extract the building of the ResponseMessage to it's own variable (in eclipse, select the text, right-click > refactor > extract local variable / select code + shift+alt+L). This will make the code a bit more readable, especially when you'll be passing in other things besides the\n\nTo have an idea on the number of valuable predictions present among those classified as \"wrong\" (i.e., the non-perfect predictions), three authors manually analyzed a sample of 100 \"wrong\" predictions for each task (300 in total). The analysis was done in two meetings in which each instance was discussed by all three authors. The goal was to classify each instance into one of three categories: (i) \"semantically equivalent\" (i.e., the generated code/comment is different but semantically equivalent to the reference one); (ii) \"alternative solution\" (i.e., the generated code/comment is not semantically equivalent, but valuable); or (iii) \"wrong\" (i.e., the generated code/comment is not meaningful for the provided input). Since we also computed the confidence for each of the predictions generated by T5, rather than randomly selecting the 300 instances to inspect, we decided to target for each task the top-100 wrong predictions generated by the model in terms of confidence. Indeed, those cases are particularly interesting, since they represent wrong predictions for which, however, the model is quite confident.  Table 3 shows the results of our manual analysis. For the codeto-code we observed that, in most cases (89%) the model actually generates wrong predictions that are not inline with the changes implemented by the developer. There are few exceptions to these cases, mostly related to small changes in which the model made a decision different from that one of the developer but still valid (e.g., extracting a string into a variable and using a different name for the extracted variable). More interesting are the results for the other two tasks.\n\nIn the case of code&comment-to-code, we found that 62 out of the 100 \"wrong\" predictions we inspected were actually valid implementations of the change recommended by the reviewer. One example is presented at the bottom of Fig. 2 (black background), where we show the input provided to the model (i.e., the code in the first line and the reviewer's comment \"Inline this variable\") and the output of the model right below. T5 successfully addressed the reviewer's comment.\n\n\nPerfect Predictions (%)\n\n\nConfidence\n\nConfidence Confidence code-to-code code&comment-to-code code-to-comment Figure 3: Perfect predictions by confidence of the model However, the prediction is different from the target implementation, since the latter also includes another change that was not explicitly required in the code review. This case is representative of all 56 instances we classified as \"alternative solutions\" for this task and, given the goal of the code&comment-to-code, we believe they represent good predictions.\n\nFinally, also for the code-to-comment task, we found a large number of \"wrong\" predictions that are actually valuable, with 36 of them even being semantically equivalent (i.e., T5 formulated a comment asking the same changes required by the reviewer, but using a different wording). One example is reported at the very bottom of Fig. 2. While the model only received the code as input we also show the original reviewer's comment (i.e., \"Please make this one a variable as well\") to make it easier to assess the relevance of the comment generated by T5 (i.e., \"Extract the building ...\").\n\nOverall, our analysis showed that the perfect predictions really represent a lower bound for the performance of T5, especially for the two tasks in which natural language comments are involved.\n\n\nRQ 4 : Pre-training and confidence\n\nIn Fig. 1 we observed better performance for the pre-trained model in the code&comment-to-code and in the code-to-comment task, while the non pre-trained model performed better in the code-to-code task. The results of the McNemar's test on the predictions at =1, confirm such findings: besides the significant difference confirmed for all tasks ( -value < 0.01), the ORs indicate 85% and 59% higher odds of obtaining a perfect prediction using the pre-trained model in the code&comment-to-code (OR=1.85) and in the code-to-comment (OR=1.59) task, while odds are 34% lower in the code-to-code task (OR=0.66). Two observations are worth to be made. First, overall, the pre-trained model seems to represent a more valuable solution. Second, the lack of improvement in the code-to-code task can be explained by the pre-training and fine-tuning we performed. Indeed, the code-to-code task only focuses on source code, with no natural language in the input nor in the output. The fine-tuning stage, focused on source code, was probably sufficient to the model to learn about the code syntax and the possible transformations to perform. The additional pre-training, also including technical English, did not benefit the model for the code-to-code task. The other two tasks, instead, either include natural language as input (code&comment-to-code) or require its generation as output (code-tocomment), obtaining a boost of performance from the pre-training. Fig. 3 depicts the percentage of perfect predictions ( -axis) within each confidence interval (from 0.0-0.1 up to 0.9-1.0, -axis) when using the pre-trained model and =1. To better interpret the reported results, the gray line represents the overall performance of the model when considering all predictions (e.g., 4.48% of perfect predictions for the code-to-code task).\n\nIn all three tasks, we observe a clear trend, with the predictions in the highest confidence bucket (0.9-1.0) ensuring substantially better performance than the overall trend. When only considering the predictions in this bucket, the percentage of perfect predictions increases to: 14.24% for code-to-code (from an overall 4.48%), 28.23% for code&comment-to-code (overall=14.08%), and 22.23% for code-to-comment (overall=2.12%). Considering the complexity of the addressed tasks, the jump in performance is substantial and indicates the usability of the confidence level as a proxy for the prediction quality. Also, while the percentage of perfect predictions is quite limited, with seven out of ten predictions being wrong in the best-case scenario (28.23% for code&comment-to-code), it is worth considering what previously observed in our manual analysis, with \"valuable\" predictions which are classified as \"wrong\" in our quantitative analysis.\n\n\nRQ 5 : Comparison with the baseline [46]\n\nFig. 4 compares the performance achieved by the T5 model with those obtained by the baseline [46].\n\nIn the line charts the continuous lines represent the pre-trained T5, the dashed lines non pre-trained T5, and the dotted lines the baseline. Two important points are worth remembering: First, the results in Fig. 4 have been computed on the test set used in [46]. Indeed, the performance in terms of perfect predictions are substantially higher as compared to those in Fig. 1 (see values on the -axis), due to the simpler instances featured in this dataset. Second, the baseline has been trained and tested on abstracted code (as in the original paper), while T5 worked on raw source code.\n\nWhen =1, T5 achieves substantially better performance. The results of the statistical test in Table 4 always show a significant difference in favor of T5 (adjusted -value < 0.01), with ORs ranging from 1.69 (non pre-trained T5 vs [46] in the code-to-code task) to 11.48 (pre-trained T5 vs [46] in the code&comment-to-code task). The pre-trained T5 in this case performs better than the non pretrained one for both tasks. This is likely due to the limited size of the fine-tuning dataset used in this comparison. Indeed, to have a fair comparison with [46], we fine-tuned T5 on the training set we used in [46] and composed by \u223c13.5k instances (vs the \u223c134k we had in our fine-tuning dataset when answering RQ 1 -RQ 4 ). This is probably not sufficient to effectively train a large model such as T5, and makes the instances used in the pre-training fundamental to further learn about the language. Still, even without pre-training, T5 outperforms the baseline when =1. For example, in the code&comment-to-code task, the baseline achieves 9.48% perfect predictions, against the 15.46% of the non pre-trained T5, and the 29.74% of the pre-trained T5. The baseline observes a stronger improvement with the increasing of (i.e., the beam size) as compared to T5 (see Fig. 4). We believe this is due to usage of the abstraction. Indeed, when working with abstracted code the \"search space\" (i.e., the number of possible solutions that can be generated with the given vocabulary) is much more limited since the model does not deal with identifiers and literals. Attempting ten predictions in a smaller search space is more likely to result in correct predictions. The results of the CodeBLEU confirm the trend observed with the perfect predictions, with the pre-trained T5 being the best model.  Figure 4: T5 vs. baseline [46] We also looked at the union of perfect predictions generated by the two approaches on the test set to verify the complementarity of the techniques. On the code-to-code (code&comment-to-code) task we observed that 15% (24%) of perfect predictions are shared by both approaches (i.e., both succeed), 65% (70%) are perfect predictions only for T5, and 20% (6%) only for the baseline. \n\n\nTHREATS TO VALIDITY\n\nConstruct validity. As explained in Section 2 we took care of cleaning the datasets used in our study by removing duplicates and noisy data points to the extent possible. Still, we are aware that problematic instances may be present, especially in the new (large) dataset we built. This manifests, for example, in non-English comments, or in some wrong \"links\" between comments and implementation (e.g., we assume that implemented a change described in while, in fact, it implemented another change). Internal validity. We did not fully explore the role played by the T5 parameters on its performance. Indeed, our hyperparameter tuning was limited to variations in the learning rate, as done in previous work [28]. For the other parameters we relied on the best architecture identified by Raffel et al. [35]. We acknowledge that additional tuning can result in improved performance.\n\nExternal validity. RQ 1 -RQ 4 have been answered using a dataset being one order of magnitude larger as compared to our previous work on automating code review tasks [46]. However, our findings are limited to Java. Concerning RQ 5 in which we compare with the baseline [46], we only used the dataset presented in [46]. This is due to the fact that our previous approach [46] requires code abstraction and, as previously explained, cannot work on instances having new identifiers and literals inserted during the code review process. The new dataset used in this paper has not been built with such a constraint in mind and, thus, it is not suitable for direct comparison.\n\n\nRELATED WORK\n\nOur work relates to three research areas: (i) DL techniques to automate software-related tasks, (ii) empirical studies on code review, and (iii) works providing recommendations on how to optimize the code review process and/or presenting techniques to partially automate it. Here we focus on the third research area, while for the first two we point the reader to the systematic literature reviews by Watson et al. [49] (deep learning in software engineering) and by Davila and Nunes [20] (modern code review).\n\nOptimizing/automating the code review process. By studying tools and techniques supporting code review, Tymchuk et al. [47] concluded that popular code review platforms (e.g., Gerrit, Code Flow, Phabricator) mostly offer the same basic functionalities with little support for automating tasks. Such a finding has been confirmed by Pascarella et al. [34]. Also, in a study performed by Lewis et al. [27] at Google, the authors show that while developers are excited by the idea of embracing automated solutions for code review, they find current solutions not to be ready for daily use. Starting from these observations, researchers studied possible optimizations of the review process: Baum et al. [14] investigate the effect of ordering submitted changes in alternative ways rather than in alphabetical order that, as shown by Barnett et al. [12] and Baum and Schneider [13], is sub-optimal. Baum et al. [14] concluded that smarter ordering is needed as the size of the patch increases, and suggest to aggregate changed parts by relatedness.\n\nDi Biase et al. [21] studied the impact of the patch size on the review's effectiveness, finding that smaller patches, while not increasing the defects found, affect how reviewers approach their task. Spadini et al. [43] compared the effectiveness of a standard code review process with test-driven code review (TDR), i.e., the reviewer inspects the changed test code before the production code. They show that TDR does not boost the code review effectiveness.\n\nSeveral researchers [23,33,51] suggest exploiting defect prediction models during code review. Similarly, Balachandran [11] and Singh et al. [42] suggest the use of static analysis tools to automatically spot coding standard violations and common defects.\n\nConcerning the automation of specific code review tasks, authors proposed techniques to optimize the reviewers' assignment. For example, Al-Zubaidi et al. [9] in open source and Chouchen et al. [18] in industrial contexts show how a multi-objective search-based approach can simplify the code review triaging process.\n\nShi et al. [41] and Chouchen et al. [18] look at the automation of code review from a similar perspective. Shi et al. [41] present a DL model taking as input the code submitted for review and the revised code implementing the changes recommended by reviewers and providing as output whether the change can be accepted or not. Note that the change(s) required by the reviewer(s) are not considered by the model. Chouchen et al. [18] use instead a set of quality metrics as features for machine learning algorithms to classify the quality of the code submitted for review. Recently, Hellendoorn et al. [22] focus on the prediction of the location of a possible reviewer's comment, showing that even this simple task is challenging to automate.\n\nThe above discussed techniques [18,22,41] are complementary to the approach we presented in [46] (and, as a consequence, to the models experimented in this work).\n\nWhile Shi et al. [41] and Chouchen et al. [18] assess the code under review through a \"boolean answer\" (i.e., accepted/rejected or well-written/badly-written), we attempt the automation of code changes implemented in code review. Also, the approach by Hellendoorn et al. could be combined with the automation of the codeto-comment task we presented.\n\n\nCONCLUSION AND FUTURE WORK\n\nOur paper starts by discussing limitations in the approach we recently proposed to automate code review tasks [46]. We highlighted that the usage of code abstraction does not allow to support nontrivial code review scenarios requiring code changes resulting in the introduction of new identifiers/literals. Hence, we proposed the usage of a pre-trained T5 model [35] relying on a SentencePiece [26] tokenizer to overcome such a limitation and work directly on raw source code. Our empirical evaluation, performed on a much larger and realistic code review dataset, shows the improvements brought by the T5 model that represents a step forward as compared to the state-of-the-art [46] both in terms of applicability (i.e., scenarios in which it can be applied) and performance. Still, the level of actual performance observed makes these techniques far from being deployable in practice, calling for more research in code review automation.\n\nOur future research agenda will be focused on designing improved solutions to boost the prediction accuracy of these techniques (e.g., by combining different representations of code [17] and/or by exploiting the model's confidence as a possible filter to select only high-quality recommendations).\n\nThe code and data used in our study are publicly available [8].\n\n\nACKNOWLEDGMENT\n\nThis project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 851720). W&M has been supported in part by the NSF CCF-1955853 and CCF-2007246 grants. Any opinions, findings, and conclusions expressed herein are the authors' and do not necessarily reflect those of the sponsors.\n\nFigure 1 :\n1Results T5 dataset large\n\n\nreadFrom(View<?> view) { if (view instanceof Dataset && view instanceof FileSystemDataset) { FileSystemDataset dataset = (FileSystemDataset) view; [...] } public ConfigBuilder readFrom(View<?> view) { if (view instanceof FileSystemDataset) { FileSystemDataset dataset = (FileSystemDataset) view; [...] } public Response getCustomizedStateAggregationConfig(@PathParam(\"clusterId\") String clusterId) { HelixZkClient zkClient = getHelixZkClient(); if (!ZKUtil.isClusterSetup(clusterId, zkClient)) { return notFound();} [...] } public Response getCustomizedStateAggregationConfig(@PathParam(\"clusterId\") String clusterId) { if (!doesClusterExist(clusterId)) { return notFound(String.format(\"Cluster %s does not exist\", clusterId));} [...] } code&comment-to-code private String getBillingFrequencyDescription(Award award) { if (ObjectUtils.isNull(award) || ObjectUtils.isNull(award.getBillingFrequency())) { [...] } \"I suggest ObjectUtils check for nulls\" private String getBillingFrequencyDescription(Award award) { if (award == null || award.getBillingFrequency() == null) { [...] } public <T extends IRemoteConnection.Service> T getService([...]) { if ([...]) { return [...]; } return null; } public <T extends IRemoteConnection.Service> T getService([...]) { if ([...]) { return [...]; } else { return null; } } \"else is redundant\" code-to-comment static <E,T> Validation<E,T> valid(Supplier<? extends T> supplier) { return new Valid<>(supplier.get()); } \"Please add a check Objects.requireNonNull(supplier, \"supplier is null\");\" public List<[...]> getExecuteBefore() { Rules ann = this.getClass().getAnnotation(Rules.class); if(ann != null) [...] } \"Rename 'ann' to 'rules', 'rulesAnnotation' or something more descriptive.\" Alternative and valid predictions code&comment-to-code public UserDTO addUser(UserDTO userResource) { [...] return UserDTO.createInstanceWithPrivateData(user); } \"Inline this variable\" public UserDTO addUser(UserDTO userResource) { [...] UserDTO savedUser = UserDTO.createInstanceWithPrivateData(user); return savedUser; }\n\nFigure 2 :\n2ResponseMessage.\" \"Please make this one a variable as well\" public void handleSetDeviceLifecycleStatusByChannelResponse([...]) { [...] ResponseMessage.newResponseMessageBuilder().[...])} Examples of perfect and alternative predictions\n\nTable 1 :\n1Pre-training and fine-tuning datasets (# instances)Dataset \ntrain \nevaluation \ntest \n\nPre-training \nStack Overflow \n977,379 \n-\n-\nCodeSearchNet \n507,947 \n-\n-\nFine-tuning \n134,239 \n16,780 \n16,780 \n\n\n\nTable 2 :\n2Hyperparameter tuning resultsTask \nLearining Rate Strategy \nC-LR \nISR-LR \nST-LR \nPD-LR \n\nPre-Trained \ncode-to-code \n2.68% \n3.68% \n4.64% \n2.53% \ncode&comment-to-code \n10.39% \n9.23% \n8.46% \n9.89% \ncode-to-comment \n0.15% \n0.32% \n0.60% \n0.15% \n\nNon Pre-Trained \ncode-to-code \n1.23% \n3.71% \n4.16% \n1.22% \ncode&comment-to-code \n5.05% \n6.41% \n6.24% \n5.18% \ncode-to-comment \n0.09% \n0.44% \n0.49% \n0.03% \n\n\n\nTable 3 :\n3Manual analysis of 100 \"wrong\" predictions per taskTask \nSemantically Equivalent \nAlternative Solution \nWrong \n\ncode-to-code \n1 \n10 \n89 \ncode&comment-to-code \n6 \n56 \n38 \ncode-to-comment \n36 \n10 \n54 \n\n\n\n\ncode-to-codeBeam size \nBeam size \n\ncode&comment-to-code \n\nCode BLEU \n\nPerfect Predictions (%) \n\nT5 Pre-Trained \nT5 Non Pre-Trained \nBaseline \n\n\n\nTable 4 :\n4RQ 5 : McNemar's test (adj. -value and OR) T5 pre-trained vs [46] T5 non pre-trained vs [46] <0.01 1.69 T5 pre-trained vs T5 non pre-trained <0.01 2.50 code&comment-to-code T5 pre-trained vs [46] <0.01 11.48 T5 non pre-trained vs [46] <0.01 2.38 T5 pre-trained vs T5 non pre-trainedTask \nTest \np-value \nOR \n\ncode-to-code \n\n<0.01 \n\n2.90 \n<0.01 \n\n5.69 \n\n\n\n. Github, GitHub. https://github.com/.\n\nMSR mining platform. n.d.[n.d.]. MSR mining platform. https://seart-ghs.si.usi.ch.\n\nStack Exchange Dumps. n.d.[n.d.]. Stack Exchange Dumps. https://archive.org/details/stackexchange.\n\nReplication Package. 20212021. Replication Package. https://github.com/RosaliaTufano/code_review_ automation.\n\nHoa Khanh Dam, Chakkrit Tantithamthavorn, and Aditya Ghose. 2020. Workload-aware reviewer recommendation using a multi-objective search-based approach. Wisam Haitham Abbood Al-Zubaidi, Patanamon Thongtanunam, Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering. the 16th ACM International Conference on Predictive Models and Data Analytics in Software EngineeringWisam Haitham Abbood Al-Zubaidi, Patanamon Thongtanunam, Hoa Khanh Dam, Chakkrit Tantithamthavorn, and Aditya Ghose. 2020. Workload-aware reviewer recommendation using a multi-objective search-based approach. In Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering. 21-30.\n\nExpectations, outcomes, and challenges of modern code review. Alberto Bacchelli, Christian Bird, Proceedings of the 2013 international conference on software engineering. the 2013 international conference on software engineeringIEEE PressAlberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal- lenges of modern code review. In Proceedings of the 2013 international conference on software engineering. IEEE Press, 712-721.\n\nReducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. Vipin Balachandran, 10.1109/ICSE.2013.660664235th International Conference on Software Engineering (ICSE). 931-940. Vipin Balachandran. 2013. Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. In 2013 35th International Conference on Software Engineering (ICSE). 931-940. https://doi.org/10.1109/ICSE.2013.6606642\n\nHelping Developers Help Themselves: Automatic Decomposition of Code Review Changesets. Mike Barnett, Christian Bird, Jo\u00e3o Brunet, Shuvendu K Lahiri, Proceedings of the 37th International Conference on Software Engineering. the 37th International Conference on Software Engineering1Mike Barnett, Christian Bird, Jo\u00e3o Brunet, and Shuvendu K. Lahiri. 2015. Helping Developers Help Themselves: Automatic Decomposition of Code Review Change- sets. In Proceedings of the 37th International Conference on Software Engineering - Volume 1 (ICSE '15). 134-144.\n\nOn the need for a new generation of code review tools. Tobias Baum, Kurt Schneider, International Conference on Product-Focused Software Process Improvement. SpringerTobias Baum and Kurt Schneider. 2016. On the need for a new generation of code review tools. In International Conference on Product-Focused Software Process Improvement. Springer, 301-308.\n\nOn the optimal order of reading source code changes for review. Tobias Baum, Kurt Schneider, Alberto Bacchelli, 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEETobias Baum, Kurt Schneider, and Alberto Bacchelli. 2017. On the optimal order of reading source code changes for review. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 329-340.\n\nFour eyes are better than two: On the impact of code reviews on software quality. Gabriele Bavota, Barbara Russo, IEEE International Conference on Software Maintenance and Evolution. ICSMEGabriele Bavota and Barbara Russo. 2015. Four eyes are better than two: On the impact of code reviews on software quality. In IEEE International Conference on Software Maintenance and Evolution, (ICSME). 81-90.\n\nImpact of Peer Code Review on Peer Impression Formation: A Survey. A Bosu, J C Carver, ACM / IEEE International Symposium on Empirical Software Engineering and Measurement. A. Bosu and J. C. Carver. 2013. Impact of Peer Code Review on Peer Impression Formation: A Survey. In 2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement. 133-142.\n\nOn Multi-Modal Learning of Editing Source Code. Saikat Chakraborty, Baishakhi Ray, arXiv:2108.06645cs.SESaikat Chakraborty and Baishakhi Ray. 2021. On Multi-Modal Learning of Editing Source Code. arXiv:2108.06645 [cs.SE]\n\nWhoReview: A multi-objective search-based approach for code reviewers recommendation in modern code review. Moataz Chouchen, Ali Ouni, Mohamed Wiem Mkaouer, Raula Gaikovina Kula, Katsuro Inoue, Applied Soft Computing. 100106908Moataz Chouchen, Ali Ouni, Mohamed Wiem Mkaouer, Raula Gaikovina Kula, and Katsuro Inoue. 2021. WhoReview: A multi-objective search-based approach for code reviewers recommendation in modern code review. Applied Soft Com- puting 100 (2021), 106908.\n\nSampling Projects in GitHub for MSR Studies. Ozren Dabic, Emad Aghajani, Gabriele Bavota, 18th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021. IEEEOzren Dabic, Emad Aghajani, and Gabriele Bavota. 2021. Sampling Projects in GitHub for MSR Studies. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021. IEEE, 560-564.\n\nA systematic literature review and taxonomy of modern code review. Nicole Davila, Ingrid Nunes, Journal of Systems and Software. 110951Nicole Davila and Ingrid Nunes. 2021. A systematic literature review and taxon- omy of modern code review. Journal of Systems and Software (2021), 110951.\n\nThe effects of change decomposition on code review-a controlled experiment. Magiel Marco Di Biase, Arie Bruntink, Alberto Van Deursen, Bacchelli, PeerJ Computer Science. 5193Marco di Biase, Magiel Bruntink, Arie van Deursen, and Alberto Bacchelli. 2019. The effects of change decomposition on code review-a controlled experiment. PeerJ Computer Science 5 (2019), e193.\n\nTowards automating code review at scale. Jason Vincent J Hellendoorn, Manisha Tsay, Martin Mukherjee, Hirzel, Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringVincent J Hellendoorn, Jason Tsay, Manisha Mukherjee, and Martin Hirzel. 2021. Towards automating code review at scale. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1479-1482.\n\nDeepJIT: an end-to-end deep learning framework for just-intime defect prediction. Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, Naoyasu Ubayashi, 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEEThong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in- time defect prediction. In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR). IEEE, 34-45.\n\nA simple sequentially rejective multiple test procedure. Scandinavian journal of statistics. Sture Holm, Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scan- dinavian journal of statistics (1979), 65-70.\n\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc Brockschmidt, CoRR abs/1909.09436Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. CoRR abs/1909.09436 (2019). http://arxiv.org/abs/1909.09436\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. Taku Kudo, John Richardson, arXiv:1808.06226CoRRTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. CoRR (2018). arXiv:1808.06226\n\nDoes bug prediction support human developers? findings from a google case study. Chris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, E James Whitehead, 35th International Conference on Software Engineering (ICSE). IEEEChris Lewis, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, and E James Whitehead. 2013. Does bug prediction support human developers? find- ings from a google case study. In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 372-381.\n\nStudying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks. Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, Gabriele Bavota, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEEAntonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 336-347.\n\nThe Impact of Code Review Coverage and Code Review Participation on Software Quality: A Case Study of the Qt, VTK, and ITK Projects. Shane Mcintosh, Yasutaka Kamei, Bram Adams, Ahmed E Hassan, Proceedings of the 11th Working Conference on Mining Software Repositories (MSR. the 11th Working Conference on Mining Software Repositories (MSRShane McIntosh, Yasutaka Kamei, Bram Adams, and Ahmed E. Hassan. 2014. The Impact of Code Review Coverage and Code Review Participation on Software Quality: A Case Study of the Qt, VTK, and ITK Projects. In Proceedings of the 11th Working Conference on Mining Software Repositories (MSR 2014). 192-201.\n\nNote on the sampling error of the difference between correlated proportions or percentages. Quinn Mcnemar, Psychometrika. 12Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika 12, 2 (1947), 153-157.\n\nDo Code Review Practices Impact Design Quality? A Case Study of the Qt, VTK, and ITK Projects. Rodrigo Morales, Shane Mcintosh, Foutse Khomh, Proc. of the 22nd Int'l Conf. on Software Analysis, Evolution, and Reengineering (SANER). of the 22nd Int'l Conf. on Software Analysis, Evolution, and Reengineering (SANER)Rodrigo Morales, Shane McIntosh, and Foutse Khomh. 2015. Do Code Review Practices Impact Design Quality? A Case Study of the Qt, VTK, and ITK Projects. In Proc. of the 22nd Int'l Conf. on Software Analysis, Evolution, and Reengineering (SANER). 171-180.\n\nBLEU: A Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL '02. the 40th Annual Meeting on Association for Computational Linguistics (ACL '02Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL '02). 311-318.\n\nFine-grained justin-time defect prediction. Luca Pascarella, Fabio Palomba, Alberto Bacchelli, Journal of Systems and Software. 150Luca Pascarella, Fabio Palomba, and Alberto Bacchelli. 2019. Fine-grained just- in-time defect prediction. Journal of Systems and Software 150 (2019), 22-36.\n\nInformation needs in contemporary code review. Luca Pascarella, Davide Spadini, Fabio Palomba, Magiel Bruntink, Alberto Bacchelli, Proceedings of the ACM on Human-Computer Interaction. 2CSCWLuca Pascarella, Davide Spadini, Fabio Palomba, Magiel Bruntink, and Alberto Bacchelli. 2018. Information needs in contemporary code review. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1-27.\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 1-67. http://jmlr.org/papers/v21/20-074.html\n\nCode Completion with Statistical Language Models. Veselin Raychev, Martin Vechev, Eran Yahav, Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '14). the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '14)ACMVeselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code Completion with Statistical Language Models. In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI '14). ACM, 419-428.\n\nDaya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.10297CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. cs.SEShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundare- san, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU: a Method for Automatic Evaluation of Code Synthesis. arXiv:2009.10297 [cs.SE]\n\nConvergent Contemporary Software Peer Review Practices. C Peter, Christian Rigby, Bird, Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2013. the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2013Peter C. Rigby and Christian Bird. 2013. Convergent Contemporary Software Peer Review Practices. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2013). 202-212.\n\n. C Peter, Daniel M Rigby, Laura German, Margaret-Anne Cowen, Storey, Peer Review on Open-Source Software Projects: Parameters, Statistical Models, and Theory. ACM Trans. Softw. Eng. Methodol. 234Peter C. Rigby, Daniel M. German, Laura Cowen, and Margaret-Anne Storey. 2014. Peer Review on Open-Source Software Projects: Parameters, Statistical Models, and Theory. ACM Trans. Softw. Eng. Methodol. 23, 4 (2014).\n\nModern Code Review: A Case Study at Google. Caitlin Sadowski, Emma S\u00f6derberg, Luke Church, Michal Sipko, Alberto Bacchelli, Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP '18). the 40th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP '18)181Caitlin Sadowski, Emma S\u00f6derberg, Luke Church, Michal Sipko, and Alberto Bacchelli. 2018. Modern Code Review: A Case Study at Google. In Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP '18). 181?190.\n\nAutomatic code review by learning the revision of source code. Shu-Ting Shi, Ming Li, David Lo, Ferdian Thung, Xuan Huo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Shu-Ting Shi, Ming Li, David Lo, Ferdian Thung, and Xuan Huo. 2019. Automatic code review by learning the revision of source code. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 4910-4917.\n\nEvaluating how static analysis tools can reduce code review effort. Devarshi Singh, Kathryn T Varun Ramachandra Sekar, Brittany Stolee, Johnson, 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEEDevarshi Singh, Varun Ramachandra Sekar, Kathryn T Stolee, and Brittany John- son. 2017. Evaluating how static analysis tools can reduce code review effort. In 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEE, 101-105.\n\nTest-driven code review: an empirical study. Davide Spadini, Fabio Palomba, Tobias Baum, Stefan Hanenberg, Magiel Bruntink, Alberto Bacchelli, 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEEDavide Spadini, Fabio Palomba, Tobias Baum, Stefan Hanenberg, Magiel Bruntink, and Alberto Bacchelli. 2019. Test-driven code review: an empirical study. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 1061-1072.\n\nUnit Test Case Generation with Transformers. Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Shao Kun Deng, Sundaresan, abs/2009.05617Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. 2020. Unit Test Case Generation with Transformers. CoRR abs/2009.05617 (2020). https://arxiv.org/abs/2009.05617\n\nAn Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation. Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, Denys Poshyvanyk, ACM Trans. Softw. Eng. Methodol. 2829Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation. ACM Trans. Softw. Eng. Methodol. 28, 4 (2019), 19:1-19:29.\n\nTowards Automating Code Review Activities. Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, Gabriele Bavota, 43rd International Conference on Software Engineering, ICSE'21. Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele Bavota. 2021. Towards Automating Code Review Activities. In 43rd International Conference on Software Engineering, ICSE'21. https://arxiv.org/abs/ 2101.02518\n\nCode review: Veni, vidi, vici. Yuriy Tymchuk, Andrea Mocci, Michele Lanza, IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEEYuriy Tymchuk, Andrea Mocci, and Michele Lanza. 2015. Code review: Veni, vidi, vici. In 2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEE, 151-160.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.\n\nCody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran, arXiv:2009.06520and Denys Poshyvanyk. 2020. A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research. arXiv preprintCody Watson, Nathan Cooper, David Nader Palacio, Kevin Moran, and Denys Poshyvanyk. 2020. A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research. arXiv preprint arXiv:2009.06520 (2020).\n\nOn learning meaningful assert statements for unit test cases. Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, Denys Poshyvanyk, ICSE '20: 42nd International Conference on Software Engineering. Gregg Rothermel and Doo-Hwan BaeSeoul, South KoreaACMCody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshy- vanyk. 2020. On learning meaningful assert statements for unit test cases. In ICSE '20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June -19 July, 2020, Gregg Rothermel and Doo-Hwan Bae (Eds.). ACM, 1398-1409.\n\nPredicting Defective Lines Using a Model-Agnostic Technique. Supatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Hideaki Hata, Kenichi Matsumoto, CoRRSupatsara Wattanakriengkrai, Patanamon Thongtanunam, Chakkrit Tan- tithamthavorn, Hideaki Hata, and Kenichi Matsumoto. 2020. Predicting Defective Lines Using a Model-Agnostic Technique. CoRR (2020).\n", "annotations": {"author": "[{\"end\":75,\"start\":60},{\"end\":91,\"start\":76},{\"end\":112,\"start\":92},{\"end\":129,\"start\":113},{\"end\":147,\"start\":130},{\"end\":164,\"start\":148},{\"end\":341,\"start\":165},{\"end\":417,\"start\":342},{\"end\":552,\"start\":418},{\"end\":601,\"start\":553}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":68},{\"end\":90,\"start\":83},{\"end\":111,\"start\":100},{\"end\":128,\"start\":118},{\"end\":146,\"start\":136},{\"end\":163,\"start\":157}]", "author_first_name": "[{\"end\":67,\"start\":60},{\"end\":82,\"start\":76},{\"end\":99,\"start\":92},{\"end\":117,\"start\":113},{\"end\":135,\"start\":130},{\"end\":156,\"start\":148}]", "author_affiliation": "[{\"end\":340,\"start\":166},{\"end\":416,\"start\":343},{\"end\":551,\"start\":419},{\"end\":600,\"start\":554}]", "title": "[{\"end\":57,\"start\":1},{\"end\":658,\"start\":602}]", "venue": null, "abstract": "[{\"end\":1889,\"start\":710}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2046,\"start\":2042},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2049,\"start\":2046},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2052,\"start\":2049},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2148,\"start\":2144},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2151,\"start\":2148},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2432,\"start\":2428},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2500,\"start\":2496},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2585,\"start\":2581},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3739,\"start\":3735},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3885,\"start\":3882},{\"end\":3900,\"start\":3897},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4679,\"start\":4675},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5962,\"start\":5958},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6265,\"start\":6261},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6618,\"start\":6614},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6812,\"start\":6808},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6857,\"start\":6853},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6914,\"start\":6910},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7344,\"start\":7340},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8094,\"start\":8090},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8390,\"start\":8386},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8511,\"start\":8507},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8801,\"start\":8797},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8847,\"start\":8844},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9213,\"start\":9209},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9510,\"start\":9506},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12217,\"start\":12214},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12246,\"start\":12242},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14319,\"start\":14315},{\"end\":14323,\"start\":14319},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14381,\"start\":14377},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14953,\"start\":14949},{\"end\":14957,\"start\":14953},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16581,\"start\":16578},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16735,\"start\":16731},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16769,\"start\":16766},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17248,\"start\":17244},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18324,\"start\":18320},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18369,\"start\":18365},{\"end\":19620,\"start\":19617},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21070,\"start\":21066},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21562,\"start\":21559},{\"end\":21797,\"start\":21794},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24482,\"start\":24478},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24719,\"start\":24715},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24970,\"start\":24966},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25154,\"start\":25150},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26139,\"start\":26135},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27930,\"start\":27926},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29124,\"start\":29120},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31785,\"start\":31781},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31998,\"start\":31994},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33193,\"start\":33189},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33685,\"start\":33681},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34036,\"start\":34032},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34437,\"start\":34433},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34973,\"start\":34969},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35275,\"start\":35271},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35673,\"start\":35669},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35954,\"start\":35950},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36118,\"start\":36114},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36197,\"start\":36193},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36548,\"start\":36544},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36791,\"start\":36787},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37154,\"start\":37150},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":38260,\"start\":38256},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":38617,\"start\":38613},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38726,\"start\":38725},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":40422,\"start\":40418},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":42025,\"start\":42021},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":42028,\"start\":42025},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":42031,\"start\":42028},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42237,\"start\":42234},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":42574,\"start\":42570},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":42610,\"start\":42606},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":42659,\"start\":42655},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":50775,\"start\":50771},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":51040,\"start\":51036},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":51603,\"start\":51599},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":51662,\"start\":51658},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":51924,\"start\":51920},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":51978,\"start\":51974},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":53187,\"start\":53183},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":54306,\"start\":54302},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":54400,\"start\":54396},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54647,\"start\":54643},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54750,\"start\":54746},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54794,\"start\":54790},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54851,\"start\":54847},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":55583,\"start\":55579},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":55652,\"start\":55648},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":55799,\"start\":55795},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":56029,\"start\":56025},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":56078,\"start\":56074},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56378,\"start\":56374},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56523,\"start\":56519},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":56551,\"start\":56547},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56585,\"start\":56581},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":56740,\"start\":56736},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":56940,\"start\":56936},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":57206,\"start\":57202},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":57209,\"start\":57206},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":57212,\"start\":57209},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":57305,\"start\":57301},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":57327,\"start\":57323},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":57597,\"start\":57594},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57637,\"start\":57633},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":57773,\"start\":57769},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57798,\"start\":57794},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":57880,\"start\":57876},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58189,\"start\":58185},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":58362,\"start\":58358},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58536,\"start\":58532},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":58539,\"start\":58536},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":58542,\"start\":58539},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":58597,\"start\":58593},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":58686,\"start\":58682},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58711,\"start\":58707},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":59159,\"start\":59155},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":59411,\"start\":59407},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":59443,\"start\":59439},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":59728,\"start\":59724},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":60172,\"start\":60168},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":60347,\"start\":60344}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":60789,\"start\":60752},{\"attributes\":{\"id\":\"fig_1\"},\"end\":62839,\"start\":60790},{\"attributes\":{\"id\":\"fig_2\"},\"end\":63087,\"start\":62840},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":63296,\"start\":63088},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":63705,\"start\":63297},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":63918,\"start\":63706},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":64064,\"start\":63919},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":64429,\"start\":64065}]", "paragraph": "[{\"end\":2586,\"start\":1905},{\"end\":3206,\"start\":2588},{\"end\":3712,\"start\":3208},{\"end\":4577,\"start\":3714},{\"end\":5316,\"start\":4579},{\"end\":6088,\"start\":5318},{\"end\":6325,\"start\":6090},{\"end\":6560,\"start\":6327},{\"end\":7965,\"start\":6562},{\"end\":8271,\"start\":7967},{\"end\":8391,\"start\":8273},{\"end\":8512,\"start\":8393},{\"end\":8681,\"start\":8514},{\"end\":8802,\"start\":8683},{\"end\":8848,\"start\":8804},{\"end\":9076,\"start\":8879},{\"end\":10044,\"start\":9119},{\"end\":10873,\"start\":10046},{\"end\":11405,\"start\":10875},{\"end\":11693,\"start\":11423},{\"end\":11925,\"start\":11719},{\"end\":12076,\"start\":11927},{\"end\":12778,\"start\":12078},{\"end\":13794,\"start\":12780},{\"end\":14367,\"start\":13796},{\"end\":15750,\"start\":14369},{\"end\":16529,\"start\":15752},{\"end\":16582,\"start\":16531},{\"end\":18230,\"start\":16608},{\"end\":18793,\"start\":18232},{\"end\":19257,\"start\":18795},{\"end\":19850,\"start\":19259},{\"end\":20424,\"start\":19852},{\"end\":20909,\"start\":20426},{\"end\":21563,\"start\":20911},{\"end\":22477,\"start\":21565},{\"end\":22802,\"start\":22479},{\"end\":23426,\"start\":22804},{\"end\":24425,\"start\":23428},{\"end\":24896,\"start\":24464},{\"end\":25748,\"start\":24898},{\"end\":26057,\"start\":25750},{\"end\":26371,\"start\":26059},{\"end\":26884,\"start\":26373},{\"end\":27770,\"start\":26886},{\"end\":28300,\"start\":27797},{\"end\":28431,\"start\":28302},{\"end\":28874,\"start\":28433},{\"end\":29258,\"start\":28891},{\"end\":29745,\"start\":29260},{\"end\":30089,\"start\":29747},{\"end\":30177,\"start\":30091},{\"end\":30477,\"start\":30179},{\"end\":30781,\"start\":30479},{\"end\":31675,\"start\":30783},{\"end\":31999,\"start\":31677},{\"end\":32235,\"start\":32032},{\"end\":32969,\"start\":32237},{\"end\":33562,\"start\":32971},{\"end\":33819,\"start\":33564},{\"end\":34186,\"start\":33821},{\"end\":34916,\"start\":34188},{\"end\":35069,\"start\":34918},{\"end\":36007,\"start\":35071},{\"end\":36792,\"start\":36009},{\"end\":37169,\"start\":36815},{\"end\":37735,\"start\":37171},{\"end\":38728,\"start\":37737},{\"end\":39111,\"start\":38730},{\"end\":39321,\"start\":39113},{\"end\":39698,\"start\":39323},{\"end\":40657,\"start\":39700},{\"end\":40980,\"start\":40659},{\"end\":41231,\"start\":40982},{\"end\":41755,\"start\":41233},{\"end\":43256,\"start\":41757},{\"end\":44065,\"start\":43258},{\"end\":46033,\"start\":44366},{\"end\":46506,\"start\":46035},{\"end\":47039,\"start\":46547},{\"end\":47629,\"start\":47041},{\"end\":47824,\"start\":47631},{\"end\":49684,\"start\":47863},{\"end\":50633,\"start\":49686},{\"end\":50776,\"start\":50678},{\"end\":51367,\"start\":50778},{\"end\":53569,\"start\":51369},{\"end\":54475,\"start\":53593},{\"end\":55147,\"start\":54477},{\"end\":55674,\"start\":55164},{\"end\":56718,\"start\":55676},{\"end\":57180,\"start\":56720},{\"end\":57437,\"start\":57182},{\"end\":57756,\"start\":57439},{\"end\":58499,\"start\":57758},{\"end\":58663,\"start\":58501},{\"end\":59014,\"start\":58665},{\"end\":59984,\"start\":59045},{\"end\":60283,\"start\":59986},{\"end\":60348,\"start\":60285},{\"end\":60751,\"start\":60367}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23925,\"start\":23918},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24180,\"start\":24173},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26204,\"start\":26195},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27101,\"start\":27094},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32234,\"start\":32227},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":45497,\"start\":45490},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":51470,\"start\":51463}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1903,\"start\":1891},{\"attributes\":{\"n\":\"2\"},\"end\":8877,\"start\":8851},{\"attributes\":{\"n\":\"2.1\"},\"end\":9117,\"start\":9079},{\"attributes\":{\"n\":\"2.2\"},\"end\":11421,\"start\":11408},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":11717,\"start\":11696},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":16606,\"start\":16585},{\"attributes\":{\"n\":\"2.3\"},\"end\":24462,\"start\":24428},{\"attributes\":{\"n\":\"2.4\"},\"end\":27795,\"start\":27773},{\"attributes\":{\"n\":\"3\"},\"end\":28889,\"start\":28877},{\"attributes\":{\"n\":\"3.1\"},\"end\":32030,\"start\":32002},{\"attributes\":{\"n\":\"4\"},\"end\":36813,\"start\":36795},{\"end\":44364,\"start\":44068},{\"end\":46532,\"start\":46509},{\"end\":46545,\"start\":46535},{\"attributes\":{\"n\":\"4.2\"},\"end\":47861,\"start\":47827},{\"attributes\":{\"n\":\"4.3\"},\"end\":50676,\"start\":50636},{\"attributes\":{\"n\":\"5\"},\"end\":53591,\"start\":53572},{\"attributes\":{\"n\":\"6\"},\"end\":55162,\"start\":55150},{\"attributes\":{\"n\":\"7\"},\"end\":59043,\"start\":59017},{\"end\":60365,\"start\":60351},{\"end\":60763,\"start\":60753},{\"end\":62851,\"start\":62841},{\"end\":63098,\"start\":63089},{\"end\":63307,\"start\":63298},{\"end\":63716,\"start\":63707},{\"end\":64075,\"start\":64066}]", "table": "[{\"end\":63296,\"start\":63151},{\"end\":63705,\"start\":63338},{\"end\":63918,\"start\":63769},{\"end\":64064,\"start\":63933},{\"end\":64429,\"start\":64359}]", "figure_caption": "[{\"end\":60789,\"start\":60765},{\"end\":62839,\"start\":60792},{\"end\":63087,\"start\":62853},{\"end\":63151,\"start\":63100},{\"end\":63338,\"start\":63309},{\"end\":63769,\"start\":63718},{\"end\":63933,\"start\":63921},{\"end\":64359,\"start\":64077}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37212,\"start\":37206},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39722,\"start\":39716},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41788,\"start\":41782},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43156,\"start\":43150},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43346,\"start\":43340},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46283,\"start\":46258},{\"end\":46627,\"start\":46619},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":47376,\"start\":47370},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47872,\"start\":47866},{\"end\":49319,\"start\":49313},{\"end\":50992,\"start\":50986},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51153,\"start\":51147},{\"end\":52636,\"start\":52630},{\"end\":53165,\"start\":53157}]", "bib_author_first_name": "[{\"end\":64923,\"start\":64918},{\"end\":64960,\"start\":64951},{\"end\":65601,\"start\":65594},{\"end\":65622,\"start\":65613},{\"end\":66584,\"start\":66580},{\"end\":66603,\"start\":66594},{\"end\":66614,\"start\":66610},{\"end\":66631,\"start\":66623},{\"end\":66633,\"start\":66632},{\"end\":67106,\"start\":67100},{\"end\":67117,\"start\":67113},{\"end\":67471,\"start\":67465},{\"end\":67482,\"start\":67478},{\"end\":67501,\"start\":67494},{\"end\":67912,\"start\":67904},{\"end\":67928,\"start\":67921},{\"end\":68290,\"start\":68289},{\"end\":68298,\"start\":68297},{\"end\":68300,\"start\":68299},{\"end\":68652,\"start\":68646},{\"end\":68675,\"start\":68666},{\"end\":68934,\"start\":68928},{\"end\":68948,\"start\":68945},{\"end\":68962,\"start\":68955},{\"end\":68967,\"start\":68963},{\"end\":68982,\"start\":68977},{\"end\":69006,\"start\":68999},{\"end\":69347,\"start\":69342},{\"end\":69359,\"start\":69355},{\"end\":69378,\"start\":69370},{\"end\":69747,\"start\":69741},{\"end\":69762,\"start\":69756},{\"end\":70047,\"start\":70041},{\"end\":70068,\"start\":70064},{\"end\":70086,\"start\":70079},{\"end\":70381,\"start\":70376},{\"end\":70412,\"start\":70405},{\"end\":70425,\"start\":70419},{\"end\":71082,\"start\":71077},{\"end\":71093,\"start\":71090},{\"end\":71099,\"start\":71094},{\"end\":71113,\"start\":71105},{\"end\":71126,\"start\":71121},{\"end\":71138,\"start\":71131},{\"end\":71600,\"start\":71595},{\"end\":71811,\"start\":71806},{\"end\":71829,\"start\":71820},{\"end\":71841,\"start\":71834},{\"end\":71858,\"start\":71849},{\"end\":71874,\"start\":71870},{\"end\":72248,\"start\":72244},{\"end\":72259,\"start\":72255},{\"end\":72557,\"start\":72552},{\"end\":72574,\"start\":72565},{\"end\":72587,\"start\":72580},{\"end\":72605,\"start\":72598},{\"end\":72615,\"start\":72611},{\"end\":72627,\"start\":72620},{\"end\":73065,\"start\":73058},{\"end\":73085,\"start\":73079},{\"end\":73104,\"start\":73098},{\"end\":73118,\"start\":73113},{\"end\":73124,\"start\":73119},{\"end\":73139,\"start\":73134},{\"end\":73157,\"start\":73152},{\"end\":73175,\"start\":73167},{\"end\":73720,\"start\":73715},{\"end\":73739,\"start\":73731},{\"end\":73751,\"start\":73747},{\"end\":73764,\"start\":73759},{\"end\":73766,\"start\":73765},{\"end\":74321,\"start\":74316},{\"end\":74601,\"start\":74594},{\"end\":74616,\"start\":74611},{\"end\":74633,\"start\":74627},{\"end\":75139,\"start\":75132},{\"end\":75155,\"start\":75150},{\"end\":75168,\"start\":75164},{\"end\":75183,\"start\":75175},{\"end\":75647,\"start\":75643},{\"end\":75665,\"start\":75660},{\"end\":75682,\"start\":75675},{\"end\":75940,\"start\":75936},{\"end\":75959,\"start\":75953},{\"end\":75974,\"start\":75969},{\"end\":75990,\"start\":75984},{\"end\":76008,\"start\":76001},{\"end\":76384,\"start\":76379},{\"end\":76397,\"start\":76393},{\"end\":76411,\"start\":76407},{\"end\":76430,\"start\":76421},{\"end\":76442,\"start\":76436},{\"end\":76458,\"start\":76451},{\"end\":76472,\"start\":76467},{\"end\":76482,\"start\":76479},{\"end\":76492,\"start\":76487},{\"end\":76494,\"start\":76493},{\"end\":76911,\"start\":76904},{\"end\":76927,\"start\":76921},{\"end\":76940,\"start\":76936},{\"end\":77387,\"start\":77383},{\"end\":77403,\"start\":77398},{\"end\":77413,\"start\":77409},{\"end\":77424,\"start\":77418},{\"end\":77435,\"start\":77431},{\"end\":77445,\"start\":77441},{\"end\":77456,\"start\":77452},{\"end\":77477,\"start\":77469},{\"end\":77489,\"start\":77484},{\"end\":77863,\"start\":77862},{\"end\":77880,\"start\":77871},{\"end\":78282,\"start\":78281},{\"end\":78296,\"start\":78290},{\"end\":78298,\"start\":78297},{\"end\":78311,\"start\":78306},{\"end\":78333,\"start\":78320},{\"end\":78743,\"start\":78736},{\"end\":78758,\"start\":78754},{\"end\":78774,\"start\":78770},{\"end\":78789,\"start\":78783},{\"end\":78804,\"start\":78797},{\"end\":79392,\"start\":79384},{\"end\":79402,\"start\":79398},{\"end\":79412,\"start\":79407},{\"end\":79424,\"start\":79417},{\"end\":79436,\"start\":79432},{\"end\":79847,\"start\":79839},{\"end\":79862,\"start\":79855},{\"end\":79864,\"start\":79863},{\"end\":79898,\"start\":79890},{\"end\":80303,\"start\":80297},{\"end\":80318,\"start\":80313},{\"end\":80334,\"start\":80328},{\"end\":80347,\"start\":80341},{\"end\":80365,\"start\":80359},{\"end\":80383,\"start\":80376},{\"end\":80777,\"start\":80770},{\"end\":80790,\"start\":80786},{\"end\":80804,\"start\":80798},{\"end\":80823,\"start\":80819},{\"end\":81164,\"start\":81157},{\"end\":81177,\"start\":81173},{\"end\":81194,\"start\":81186},{\"end\":81215,\"start\":81203},{\"end\":81218,\"start\":81216},{\"end\":81232,\"start\":81226},{\"end\":81245,\"start\":81240},{\"end\":81610,\"start\":81603},{\"end\":81623,\"start\":81619},{\"end\":81643,\"start\":81636},{\"end\":81657,\"start\":81652},{\"end\":81678,\"start\":81670},{\"end\":82026,\"start\":82021},{\"end\":82042,\"start\":82036},{\"end\":82057,\"start\":82050},{\"end\":82401,\"start\":82395},{\"end\":82415,\"start\":82411},{\"end\":82429,\"start\":82425},{\"end\":82443,\"start\":82438},{\"end\":82460,\"start\":82455},{\"end\":82473,\"start\":82468},{\"end\":82475,\"start\":82474},{\"end\":82489,\"start\":82483},{\"end\":82503,\"start\":82498},{\"end\":82795,\"start\":82791},{\"end\":82810,\"start\":82804},{\"end\":82824,\"start\":82819},{\"end\":82830,\"start\":82825},{\"end\":82845,\"start\":82840},{\"end\":83294,\"start\":83290},{\"end\":83310,\"start\":83303},{\"end\":83324,\"start\":83319},{\"end\":83340,\"start\":83332},{\"end\":83354,\"start\":83349},{\"end\":83876,\"start\":83867},{\"end\":83905,\"start\":83896},{\"end\":83928,\"start\":83920},{\"end\":83954,\"start\":83947},{\"end\":83968,\"start\":83961}]", "bib_author_last_name": "[{\"end\":64439,\"start\":64433},{\"end\":64949,\"start\":64924},{\"end\":64973,\"start\":64961},{\"end\":65611,\"start\":65602},{\"end\":65627,\"start\":65623},{\"end\":66121,\"start\":66103},{\"end\":66592,\"start\":66585},{\"end\":66608,\"start\":66604},{\"end\":66621,\"start\":66615},{\"end\":66640,\"start\":66634},{\"end\":67111,\"start\":67107},{\"end\":67127,\"start\":67118},{\"end\":67476,\"start\":67472},{\"end\":67492,\"start\":67483},{\"end\":67511,\"start\":67502},{\"end\":67919,\"start\":67913},{\"end\":67934,\"start\":67929},{\"end\":68295,\"start\":68291},{\"end\":68307,\"start\":68301},{\"end\":68664,\"start\":68653},{\"end\":68679,\"start\":68676},{\"end\":68943,\"start\":68935},{\"end\":68953,\"start\":68949},{\"end\":68975,\"start\":68968},{\"end\":68997,\"start\":68983},{\"end\":69012,\"start\":69007},{\"end\":69353,\"start\":69348},{\"end\":69368,\"start\":69360},{\"end\":69385,\"start\":69379},{\"end\":69754,\"start\":69748},{\"end\":69768,\"start\":69763},{\"end\":70062,\"start\":70048},{\"end\":70077,\"start\":70069},{\"end\":70098,\"start\":70087},{\"end\":70109,\"start\":70100},{\"end\":70403,\"start\":70382},{\"end\":70417,\"start\":70413},{\"end\":70435,\"start\":70426},{\"end\":70443,\"start\":70437},{\"end\":71088,\"start\":71083},{\"end\":71103,\"start\":71100},{\"end\":71119,\"start\":71114},{\"end\":71129,\"start\":71127},{\"end\":71147,\"start\":71139},{\"end\":71605,\"start\":71601},{\"end\":71818,\"start\":71812},{\"end\":71832,\"start\":71830},{\"end\":71847,\"start\":71842},{\"end\":71868,\"start\":71859},{\"end\":71887,\"start\":71875},{\"end\":72253,\"start\":72249},{\"end\":72270,\"start\":72260},{\"end\":72563,\"start\":72558},{\"end\":72578,\"start\":72575},{\"end\":72596,\"start\":72588},{\"end\":72609,\"start\":72606},{\"end\":72618,\"start\":72616},{\"end\":72637,\"start\":72628},{\"end\":73077,\"start\":73066},{\"end\":73096,\"start\":73086},{\"end\":73111,\"start\":73105},{\"end\":73132,\"start\":73125},{\"end\":73150,\"start\":73140},{\"end\":73165,\"start\":73158},{\"end\":73182,\"start\":73176},{\"end\":73729,\"start\":73721},{\"end\":73745,\"start\":73740},{\"end\":73757,\"start\":73752},{\"end\":73773,\"start\":73767},{\"end\":74329,\"start\":74322},{\"end\":74609,\"start\":74602},{\"end\":74625,\"start\":74617},{\"end\":74639,\"start\":74634},{\"end\":75148,\"start\":75140},{\"end\":75162,\"start\":75156},{\"end\":75173,\"start\":75169},{\"end\":75187,\"start\":75184},{\"end\":75658,\"start\":75648},{\"end\":75673,\"start\":75666},{\"end\":75692,\"start\":75683},{\"end\":75951,\"start\":75941},{\"end\":75967,\"start\":75960},{\"end\":75982,\"start\":75975},{\"end\":75999,\"start\":75991},{\"end\":76018,\"start\":76009},{\"end\":76391,\"start\":76385},{\"end\":76405,\"start\":76398},{\"end\":76419,\"start\":76412},{\"end\":76434,\"start\":76431},{\"end\":76449,\"start\":76443},{\"end\":76465,\"start\":76459},{\"end\":76477,\"start\":76473},{\"end\":76485,\"start\":76483},{\"end\":76498,\"start\":76495},{\"end\":76919,\"start\":76912},{\"end\":76934,\"start\":76928},{\"end\":76946,\"start\":76941},{\"end\":77396,\"start\":77388},{\"end\":77407,\"start\":77404},{\"end\":77416,\"start\":77414},{\"end\":77429,\"start\":77425},{\"end\":77439,\"start\":77436},{\"end\":77450,\"start\":77446},{\"end\":77467,\"start\":77457},{\"end\":77482,\"start\":77478},{\"end\":77496,\"start\":77490},{\"end\":77500,\"start\":77498},{\"end\":77869,\"start\":77864},{\"end\":77886,\"start\":77881},{\"end\":77892,\"start\":77888},{\"end\":78288,\"start\":78283},{\"end\":78304,\"start\":78299},{\"end\":78318,\"start\":78312},{\"end\":78339,\"start\":78334},{\"end\":78347,\"start\":78341},{\"end\":78752,\"start\":78744},{\"end\":78768,\"start\":78759},{\"end\":78781,\"start\":78775},{\"end\":78795,\"start\":78790},{\"end\":78814,\"start\":78805},{\"end\":79396,\"start\":79393},{\"end\":79405,\"start\":79403},{\"end\":79415,\"start\":79413},{\"end\":79430,\"start\":79425},{\"end\":79440,\"start\":79437},{\"end\":79853,\"start\":79848},{\"end\":79888,\"start\":79865},{\"end\":79905,\"start\":79899},{\"end\":79914,\"start\":79907},{\"end\":80311,\"start\":80304},{\"end\":80326,\"start\":80319},{\"end\":80339,\"start\":80335},{\"end\":80357,\"start\":80348},{\"end\":80374,\"start\":80366},{\"end\":80393,\"start\":80384},{\"end\":80784,\"start\":80778},{\"end\":80796,\"start\":80791},{\"end\":80817,\"start\":80805},{\"end\":80837,\"start\":80824},{\"end\":80849,\"start\":80839},{\"end\":81171,\"start\":81165},{\"end\":81184,\"start\":81178},{\"end\":81201,\"start\":81195},{\"end\":81224,\"start\":81219},{\"end\":81238,\"start\":81233},{\"end\":81256,\"start\":81246},{\"end\":81617,\"start\":81611},{\"end\":81634,\"start\":81624},{\"end\":81650,\"start\":81644},{\"end\":81668,\"start\":81658},{\"end\":81685,\"start\":81679},{\"end\":82034,\"start\":82027},{\"end\":82048,\"start\":82043},{\"end\":82063,\"start\":82058},{\"end\":82409,\"start\":82402},{\"end\":82423,\"start\":82416},{\"end\":82436,\"start\":82430},{\"end\":82453,\"start\":82444},{\"end\":82466,\"start\":82461},{\"end\":82481,\"start\":82476},{\"end\":82496,\"start\":82490},{\"end\":82514,\"start\":82504},{\"end\":82802,\"start\":82796},{\"end\":82817,\"start\":82811},{\"end\":82838,\"start\":82831},{\"end\":82851,\"start\":82846},{\"end\":83301,\"start\":83295},{\"end\":83317,\"start\":83311},{\"end\":83330,\"start\":83325},{\"end\":83347,\"start\":83341},{\"end\":83365,\"start\":83355},{\"end\":83894,\"start\":83877},{\"end\":83918,\"start\":83906},{\"end\":83945,\"start\":83929},{\"end\":83959,\"start\":83955},{\"end\":83978,\"start\":83969}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":64469,\"start\":64431},{\"attributes\":{\"id\":\"b1\"},\"end\":64553,\"start\":64471},{\"attributes\":{\"id\":\"b2\"},\"end\":64653,\"start\":64555},{\"attributes\":{\"id\":\"b3\"},\"end\":64764,\"start\":64655},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":226263389},\"end\":65530,\"start\":64766},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220663293},\"end\":65975,\"start\":65532},{\"attributes\":{\"doi\":\"10.1109/ICSE.2013.6606642\",\"id\":\"b6\",\"matched_paper_id\":15823436},\"end\":66491,\"start\":65977},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8328637},\"end\":67043,\"start\":66493},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6982628},\"end\":67399,\"start\":67045},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10679897},\"end\":67820,\"start\":67401},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4499379},\"end\":68220,\"start\":67822},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2583854},\"end\":68596,\"start\":68222},{\"attributes\":{\"doi\":\"arXiv:2108.06645\",\"id\":\"b12\"},\"end\":68818,\"start\":68598},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":229401343},\"end\":69295,\"start\":68820},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":232147743},\"end\":69672,\"start\":69297},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":232240592},\"end\":69963,\"start\":69674},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":216070735},\"end\":70333,\"start\":69965},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":237205255},\"end\":70993,\"start\":70335},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195298550},\"end\":71500,\"start\":70995},{\"attributes\":{\"id\":\"b19\"},\"end\":71733,\"start\":71502},{\"attributes\":{\"doi\":\"CoRR abs/1909.09436\",\"id\":\"b20\"},\"end\":72131,\"start\":71735},{\"attributes\":{\"doi\":\"arXiv:1808.06226\",\"id\":\"b21\"},\"end\":72469,\"start\":72133},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14094962},\"end\":72969,\"start\":72471},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":231786586},\"end\":73580,\"start\":72971},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3257395},\"end\":74222,\"start\":73582},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":46226024},\"end\":74497,\"start\":74224},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14892409},\"end\":75066,\"start\":74499},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11080756},\"end\":75597,\"start\":75068},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":57869281},\"end\":75887,\"start\":75599},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53234683},\"end\":76294,\"start\":75889},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":204838007},\"end\":76852,\"start\":76296},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13040187},\"end\":77381,\"start\":76854},{\"attributes\":{\"doi\":\"arXiv:2009.10297\",\"id\":\"b32\"},\"end\":77804,\"start\":77383},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":11163811},\"end\":78277,\"start\":77806},{\"attributes\":{\"id\":\"b34\"},\"end\":78690,\"start\":78279},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":49217999},\"end\":79319,\"start\":78692},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":69522979},\"end\":79769,\"start\":79321},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":32599229},\"end\":80250,\"start\":79771},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":78088907},\"end\":80723,\"start\":80252},{\"attributes\":{\"doi\":\"abs/2009.05617\",\"id\":\"b39\"},\"end\":81061,\"start\":80725},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":56517510},\"end\":81558,\"start\":81063},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":230799433},\"end\":81988,\"start\":81560},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2145115},\"end\":82366,\"start\":81990},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":13756489},\"end\":82789,\"start\":82368},{\"attributes\":{\"doi\":\"arXiv:2009.06520\",\"id\":\"b44\"},\"end\":83226,\"start\":82791},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":211088142},\"end\":83804,\"start\":83228},{\"attributes\":{\"id\":\"b46\"},\"end\":84182,\"start\":83806}]", "bib_title": "[{\"end\":64916,\"start\":64766},{\"end\":65592,\"start\":65532},{\"end\":66101,\"start\":65977},{\"end\":66578,\"start\":66493},{\"end\":67098,\"start\":67045},{\"end\":67463,\"start\":67401},{\"end\":67902,\"start\":67822},{\"end\":68287,\"start\":68222},{\"end\":68926,\"start\":68820},{\"end\":69340,\"start\":69297},{\"end\":69739,\"start\":69674},{\"end\":70039,\"start\":69965},{\"end\":70374,\"start\":70335},{\"end\":71075,\"start\":70995},{\"end\":72550,\"start\":72471},{\"end\":73056,\"start\":72971},{\"end\":73713,\"start\":73582},{\"end\":74314,\"start\":74224},{\"end\":74592,\"start\":74499},{\"end\":75130,\"start\":75068},{\"end\":75641,\"start\":75599},{\"end\":75934,\"start\":75889},{\"end\":76377,\"start\":76296},{\"end\":76902,\"start\":76854},{\"end\":77860,\"start\":77806},{\"end\":78734,\"start\":78692},{\"end\":79382,\"start\":79321},{\"end\":79837,\"start\":79771},{\"end\":80295,\"start\":80252},{\"end\":81155,\"start\":81063},{\"end\":81601,\"start\":81560},{\"end\":82019,\"start\":81990},{\"end\":82393,\"start\":82368},{\"end\":83288,\"start\":83228}]", "bib_author": "[{\"end\":64441,\"start\":64433},{\"end\":64951,\"start\":64918},{\"end\":64975,\"start\":64951},{\"end\":65613,\"start\":65594},{\"end\":65629,\"start\":65613},{\"end\":66123,\"start\":66103},{\"end\":66594,\"start\":66580},{\"end\":66610,\"start\":66594},{\"end\":66623,\"start\":66610},{\"end\":66642,\"start\":66623},{\"end\":67113,\"start\":67100},{\"end\":67129,\"start\":67113},{\"end\":67478,\"start\":67465},{\"end\":67494,\"start\":67478},{\"end\":67513,\"start\":67494},{\"end\":67921,\"start\":67904},{\"end\":67936,\"start\":67921},{\"end\":68297,\"start\":68289},{\"end\":68309,\"start\":68297},{\"end\":68666,\"start\":68646},{\"end\":68681,\"start\":68666},{\"end\":68945,\"start\":68928},{\"end\":68955,\"start\":68945},{\"end\":68977,\"start\":68955},{\"end\":68999,\"start\":68977},{\"end\":69014,\"start\":68999},{\"end\":69355,\"start\":69342},{\"end\":69370,\"start\":69355},{\"end\":69387,\"start\":69370},{\"end\":69756,\"start\":69741},{\"end\":69770,\"start\":69756},{\"end\":70064,\"start\":70041},{\"end\":70079,\"start\":70064},{\"end\":70100,\"start\":70079},{\"end\":70111,\"start\":70100},{\"end\":70405,\"start\":70376},{\"end\":70419,\"start\":70405},{\"end\":70437,\"start\":70419},{\"end\":70445,\"start\":70437},{\"end\":71090,\"start\":71077},{\"end\":71105,\"start\":71090},{\"end\":71121,\"start\":71105},{\"end\":71131,\"start\":71121},{\"end\":71149,\"start\":71131},{\"end\":71607,\"start\":71595},{\"end\":71820,\"start\":71806},{\"end\":71834,\"start\":71820},{\"end\":71849,\"start\":71834},{\"end\":71870,\"start\":71849},{\"end\":71889,\"start\":71870},{\"end\":72255,\"start\":72244},{\"end\":72272,\"start\":72255},{\"end\":72565,\"start\":72552},{\"end\":72580,\"start\":72565},{\"end\":72598,\"start\":72580},{\"end\":72611,\"start\":72598},{\"end\":72620,\"start\":72611},{\"end\":72639,\"start\":72620},{\"end\":73079,\"start\":73058},{\"end\":73098,\"start\":73079},{\"end\":73113,\"start\":73098},{\"end\":73134,\"start\":73113},{\"end\":73152,\"start\":73134},{\"end\":73167,\"start\":73152},{\"end\":73184,\"start\":73167},{\"end\":73731,\"start\":73715},{\"end\":73747,\"start\":73731},{\"end\":73759,\"start\":73747},{\"end\":73775,\"start\":73759},{\"end\":74331,\"start\":74316},{\"end\":74611,\"start\":74594},{\"end\":74627,\"start\":74611},{\"end\":74641,\"start\":74627},{\"end\":75150,\"start\":75132},{\"end\":75164,\"start\":75150},{\"end\":75175,\"start\":75164},{\"end\":75189,\"start\":75175},{\"end\":75660,\"start\":75643},{\"end\":75675,\"start\":75660},{\"end\":75694,\"start\":75675},{\"end\":75953,\"start\":75936},{\"end\":75969,\"start\":75953},{\"end\":75984,\"start\":75969},{\"end\":76001,\"start\":75984},{\"end\":76020,\"start\":76001},{\"end\":76393,\"start\":76379},{\"end\":76407,\"start\":76393},{\"end\":76421,\"start\":76407},{\"end\":76436,\"start\":76421},{\"end\":76451,\"start\":76436},{\"end\":76467,\"start\":76451},{\"end\":76479,\"start\":76467},{\"end\":76487,\"start\":76479},{\"end\":76500,\"start\":76487},{\"end\":76921,\"start\":76904},{\"end\":76936,\"start\":76921},{\"end\":76948,\"start\":76936},{\"end\":77398,\"start\":77383},{\"end\":77409,\"start\":77398},{\"end\":77418,\"start\":77409},{\"end\":77431,\"start\":77418},{\"end\":77441,\"start\":77431},{\"end\":77452,\"start\":77441},{\"end\":77469,\"start\":77452},{\"end\":77484,\"start\":77469},{\"end\":77498,\"start\":77484},{\"end\":77502,\"start\":77498},{\"end\":77871,\"start\":77862},{\"end\":77888,\"start\":77871},{\"end\":77894,\"start\":77888},{\"end\":78290,\"start\":78281},{\"end\":78306,\"start\":78290},{\"end\":78320,\"start\":78306},{\"end\":78341,\"start\":78320},{\"end\":78349,\"start\":78341},{\"end\":78754,\"start\":78736},{\"end\":78770,\"start\":78754},{\"end\":78783,\"start\":78770},{\"end\":78797,\"start\":78783},{\"end\":78816,\"start\":78797},{\"end\":79398,\"start\":79384},{\"end\":79407,\"start\":79398},{\"end\":79417,\"start\":79407},{\"end\":79432,\"start\":79417},{\"end\":79442,\"start\":79432},{\"end\":79855,\"start\":79839},{\"end\":79890,\"start\":79855},{\"end\":79907,\"start\":79890},{\"end\":79916,\"start\":79907},{\"end\":80313,\"start\":80297},{\"end\":80328,\"start\":80313},{\"end\":80341,\"start\":80328},{\"end\":80359,\"start\":80341},{\"end\":80376,\"start\":80359},{\"end\":80395,\"start\":80376},{\"end\":80786,\"start\":80770},{\"end\":80798,\"start\":80786},{\"end\":80819,\"start\":80798},{\"end\":80839,\"start\":80819},{\"end\":80851,\"start\":80839},{\"end\":81173,\"start\":81157},{\"end\":81186,\"start\":81173},{\"end\":81203,\"start\":81186},{\"end\":81226,\"start\":81203},{\"end\":81240,\"start\":81226},{\"end\":81258,\"start\":81240},{\"end\":81619,\"start\":81603},{\"end\":81636,\"start\":81619},{\"end\":81652,\"start\":81636},{\"end\":81670,\"start\":81652},{\"end\":81687,\"start\":81670},{\"end\":82036,\"start\":82021},{\"end\":82050,\"start\":82036},{\"end\":82065,\"start\":82050},{\"end\":82411,\"start\":82395},{\"end\":82425,\"start\":82411},{\"end\":82438,\"start\":82425},{\"end\":82455,\"start\":82438},{\"end\":82468,\"start\":82455},{\"end\":82483,\"start\":82468},{\"end\":82498,\"start\":82483},{\"end\":82516,\"start\":82498},{\"end\":82804,\"start\":82791},{\"end\":82819,\"start\":82804},{\"end\":82840,\"start\":82819},{\"end\":82853,\"start\":82840},{\"end\":83303,\"start\":83290},{\"end\":83319,\"start\":83303},{\"end\":83332,\"start\":83319},{\"end\":83349,\"start\":83332},{\"end\":83367,\"start\":83349},{\"end\":83896,\"start\":83867},{\"end\":83920,\"start\":83896},{\"end\":83947,\"start\":83920},{\"end\":83961,\"start\":83947},{\"end\":83980,\"start\":83961}]", "bib_venue": "[{\"end\":64490,\"start\":64471},{\"end\":64575,\"start\":64555},{\"end\":64674,\"start\":64655},{\"end\":65091,\"start\":64975},{\"end\":65701,\"start\":65629},{\"end\":66217,\"start\":66148},{\"end\":66714,\"start\":66642},{\"end\":67201,\"start\":67129},{\"end\":67593,\"start\":67513},{\"end\":68003,\"start\":67936},{\"end\":68393,\"start\":68309},{\"end\":68644,\"start\":68598},{\"end\":69036,\"start\":69014},{\"end\":69467,\"start\":69387},{\"end\":69801,\"start\":69770},{\"end\":70133,\"start\":70111},{\"end\":70587,\"start\":70445},{\"end\":71230,\"start\":71149},{\"end\":71593,\"start\":71502},{\"end\":71804,\"start\":71735},{\"end\":72242,\"start\":72133},{\"end\":72699,\"start\":72639},{\"end\":73258,\"start\":73184},{\"end\":73854,\"start\":73775},{\"end\":74344,\"start\":74331},{\"end\":74729,\"start\":74641},{\"end\":75281,\"start\":75189},{\"end\":75725,\"start\":75694},{\"end\":76072,\"start\":76020},{\"end\":76536,\"start\":76500},{\"end\":77055,\"start\":76948},{\"end\":77579,\"start\":77518},{\"end\":77989,\"start\":77894},{\"end\":78470,\"start\":78349},{\"end\":78938,\"start\":78816},{\"end\":79503,\"start\":79442},{\"end\":79992,\"start\":79916},{\"end\":80469,\"start\":80395},{\"end\":80768,\"start\":80725},{\"end\":81289,\"start\":81258},{\"end\":81749,\"start\":81687},{\"end\":82158,\"start\":82065},{\"end\":82565,\"start\":82516},{\"end\":82988,\"start\":82869},{\"end\":83430,\"start\":83367},{\"end\":83865,\"start\":83806},{\"end\":65194,\"start\":65093},{\"end\":65760,\"start\":65703},{\"end\":66773,\"start\":66716},{\"end\":70716,\"start\":70589},{\"end\":73920,\"start\":73856},{\"end\":74813,\"start\":74731},{\"end\":75360,\"start\":75283},{\"end\":77149,\"start\":77057},{\"end\":78071,\"start\":77991},{\"end\":79047,\"start\":78940},{\"end\":79551,\"start\":79505},{\"end\":83482,\"start\":83464}]"}}}, "year": 2023, "month": 12, "day": 17}