{"id": 19187663, "updated": "2023-10-01 21:07:53.265", "metadata": {"title": "Improving Knowledge Graph Embedding Using Simple Constraints", "authors": "[{\"first\":\"Boyang\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Quan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Guo\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1805.02408", "mag": "2963469133", "acl": "P18-1011", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/WangWGD18", "doi": "10.18653/v1/p18-1011"}}, "content": {"source": {"pdf_hash": "75cefc96745eb0cb6be53556ba129f2bf2a7d898", "pdf_src": "ACL", "pdf_uri": "[\"https://github.com/iieir-km/ComplEx-NNE_AER\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P18-1011.pdf", "status": "HYBRID"}}, "grobid": {"id": "f35786486b2d60cc02ab90dd2e501e9a9e3a8ca4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/75cefc96745eb0cb6be53556ba129f2bf2a7d898.txt", "contents": "\nImproving Knowledge Graph Embedding Using Simple Constraints\nJuly 15 -20. 2018\n\nBoyang Ding dingboyang@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nQuan Wang wangquan@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nState Key Laboratory of Information Security\nChinese Academy of Sciences\n\n\nBin Wang wangbin@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nLi Guo guoli@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nImproving Knowledge Graph Embedding Using Simple Constraints\n\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)\nthe 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)Melbourne, AustraliaJuly 15 -20. 2018110\nEmbedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/i ieir-km/ComplEx-NNE_AER.\n\nIntroduction\n\nThe past decade has witnessed great achievements in building web-scale knowledge graphs (KGs), e.g., Freebase (Bollacker et al., 2008), DBpedia (Lehmann et al., 2015), and Google's Knowledge * Corresponding author: Quan Wang. Vault (Dong et al., 2014). A typical KG is a multirelational graph composed of entities as nodes and relations as different types of edges, where each edge is represented as a triple of the form (head entity, relation, tail entity). Such KGs contain rich structured knowledge, and have proven useful for many NLP tasks (Wasserman-Pritsker et al., 2015;Hoffmann et al., 2011;Yang and Mitchell, 2017).\n\nRecently, the concept of knowledge graph embedding has been presented and quickly become a hot research topic. The key idea there is to embed components of a KG (i.e., entities and relations) into a continuous vector space, so as to simplify manipulation while preserving the inherent structure of the KG. Early works on this topic learned such vectorial representations (i.e., embeddings) via just simple models developed over KG triples (Bordes et al., 2011(Bordes et al., , 2013Jenatton et al., 2012;Nickel et al., 2011). Recent attempts focused on either designing more complicated triple scoring models (Socher et al., 2013;Bordes et al., 2014;Wang et al., 2014;Lin et al., 2015b;Xiao et al., 2016;Nickel et al., 2016b;Trouillon et al., 2016;Liu et al., 2017), or incorporating extra information beyond KG triples (Chang et al., 2014;Zhong et al., 2015;Lin et al., 2015a;Neelakantan et al., 2015;Luo et al., 2015b;Xie et al., 2016a,b;Xiao et al., 2017). See (Wang et al., 2017) for a thorough review. This paper, by contrast, investigates the potential of using very simple constraints to improve the KG embedding task. Specifically, we examine two types of constraints: (i) non-negativity constraints on entity representations and (ii) approximate entailment constraints over relation representations. By using the former, we learn compact representations for entities, which would naturally induce sparsity and interpretability (Murphy et al., 2012). By using the latter, we further encode regularities of logical entailment between relations into their distributed representations, which might be advantageous to downstream tasks like link prediction and relation extraction (Rockt\u00e4schel et al., 2015;Guo et al., 2016). These constraints impose prior beliefs upon the structure of the embedding space, and will help us to learn more predictive embeddings, without significantly increasing the space or time complexity.\n\nOur work has some similarities to those which integrate logical background knowledge into KG embedding (Rockt\u00e4schel et al., 2015;Guo et al., 2016Guo et al., , 2018. Most of such works, however, need grounding of first-order logic rules. The grounding process could be time and space inefficient especially for complicated rules. To avoid grounding, Demeester et al. (2016) tried to model rules using only relation representations. But their work creates vector representations for entity pairs rather than individual entities, and hence fails to handle unpaired entities. Moreover, it can only incorporate strict, hard rules which usually require extensive manual effort to create. Minervini et al. (2017b) proposed adversarial training which can integrate first-order logic rules without grounding. But their work, again, focuses on strict, hard rules. Minervini et al. (2017a) tried to handle uncertainty of rules. But their work assigns to different rules a same confidence level, and considers only equivalence and inversion of relations, which might not always be available in a given KG.\n\nOur approach differs from the aforementioned works in that: (i) it imposes constraints directly on entity and relation representations without grounding, and can easily scale up to large KGs; (ii) the constraints, i.e., non-negativity and approximate entailment derived automatically from statistical properties, are quite universal, requiring no manual effort and applicable to almost all KGs; (iii) it learns an individual representation for each entity, and can successfully make predictions between unpaired entities.\n\nWe evaluate our approach on publicly available KGs of WordNet, Freebase, and DBpedia as well.\n\nExperimental results indicate that our approach is simple yet surprisingly effective, achieving significant and consistent improvements over competitive baselines, but without negative impacts on efficiency or scalability. The non-negativity and approximate entailment constraints indeed improve model interpretability, resulting in a substantially increased structuring of the embedding space.\n\nThe remainder of this paper is organized as follows. We first review related work in Section 2, and then detail our approach in Section 3. Experiments and results are reported in Section 4, followed by concluding remarks in Section 5.\n\n\nRelated Work\n\nRecent years have seen growing interest in learning distributed representations for entities and relations in KGs, a.k.a. KG embedding. Early works on this topic devised very simple models to learn such distributed representations, solely on the basis of triples observed in a given KG, e.g., TransE which takes relations as translating operations between head and tail entities (Bordes et al., 2013), and RESCAL which models triples through bilinear operations over entity and relation representations (Nickel et al., 2011). Later attempts roughly fell into two groups: (i) those which tried to design more complicated triple scoring models, e.g., the TransE extensions (Wang et al., 2014;Lin et al., 2015b;Ji et al., 2015), the RESCAL extensions (Yang et al., 2015;Nickel et al., 2016b;Trouillon et al., 2016;Liu et al., 2017), and the (deep) neural network models (Socher et al., 2013;Bordes et al., 2014;Shi and Weninger, 2017;Schlichtkrull et al., 2017;Dettmers et al., 2018); (ii) those which tried to integrate extra information beyond triples, e.g., entity types Xie et al., 2016b), relation paths (Neelakantan et al., 2015;Lin et al., 2015a), and textual descriptions (Xie et al., 2016a;Xiao et al., 2017). Please refer to (Nickel et al., 2016a;Wang et al., 2017) for a thorough review of these techniques. In this paper, we show the potential of using very simple constraints (i.e., nonnegativity constraints and approximate entailment constraints) to improve KG embedding, without significantly increasing the model complexity.\n\nA line of research related to ours is KG embedding with logical background knowledge incorporated (Rockt\u00e4schel et al., 2015;Guo et al., 2016Guo et al., , 2018. But most of such works require grounding of first-order logic rules, which is time and space inefficient especially for complicated rules. To avoid grounding, Demeester et al. Both works, however, can only handle strict, hard rules which usually require extensive effort to create. Minervini et al. (2017a) tried to handle uncertainty of background knowledge. But their work considers only equivalence and inversion between relations, which might not always be available in a given KG. Our approach, in contrast, imposes constraints directly on entity and relation representations without grounding. And the constraints used are quite universal, requiring no manual effort and applicable to almost all KGs.\n\nNon-negativity has long been a subject studied in various research fields. Previous studies reveal that non-negativity could naturally induce sparsity and, in most cases, better interpretability (Lee and Seung, 1999). In many NLP-related tasks, nonnegativity constraints are introduced to learn more interpretable word representations, which capture the notion of semantic composition (Murphy et al., 2012;Luo et al., 2015a;Fyshe et al., 2015). In this paper, we investigate the ability of non-negativity constraints to learn more accurate KG embeddings with good interpretability.\n\n\nOur Approach\n\nThis section presents our approach. We first introduce a basic embedding technique to model triples in a given KG ( \u00a7 3.1). Then we discuss the nonnegativity constraints over entity representations ( \u00a7 3.2) and the approximate entailment constraints over relation representations ( \u00a7 3.3). And finally we present the overall model ( \u00a7 3.4).\n\n\nA Basic Embedding Model\n\nWe choose ComplEx (Trouillon et al., 2016) as our basic embedding model, since it is simple and efficient, achieving state-of-the-art predictive performance. Specifically, suppose we are given a KG containing a set of triples O = {(e i , r k , e j )}, with each triple composed of two entities e i , e j \u2208 E and their relation r k \u2208 R. Here E is the set of entities and R the set of relations. ComplEx then represents each entity e \u2208 E as a complex-valued vector e \u2208 C d , and each relation r \u2208 R a complex-valued vector r \u2208 C d , where d is the dimensionality of the embedding space. Each x \u2208 C d consists of a real vector component Re(x) and an imaginary vector component Im(x), i.e., x = Re(x) + iIm(x). For any given triple (e i , r k , e j ) \u2208 E \u00d7 R \u00d7 E, a multilinear dot product is used to score that triple, i.e.,\n\u03c6(e i , r k , e j ) Re( e i , r k ,\u0113 j ) Re( [e i ] [r k ] [\u0113 j ] ),(1)\nwhere e i , r k , e j \u2208 C d are the vectorial representations associated with e i , r k , e j , respectively;\u0113 j is the conjugate of e j ; [\u00b7] is the -th entry of a vector; and Re(\u00b7) means taking the real part of a complex value. Triples with higher \u03c6(\u00b7, \u00b7, \u00b7) scores are more likely to be true. Owing to the asymmetry of this scoring function, i.e., \u03c6(e i , r k , e j ) = \u03c6(e j , r k , e i ), ComplEx can effectively handle asymmetric relations (Trouillon et al., 2016).\n\n\nNon-negativity of Entity Representations\n\nOn top of the basic ComplEx model, we further require entities to have non-negative (and bounded) vectorial representations. In fact, these distributed representations can be taken as feature vectors for entities, with latent semantics encoded in different dimensions. In ComplEx, as well as most (if not all) previous approaches, there is no limitation on the range of such feature values, which means that both positive and negative properties of an entity can be encoded in its representation. However, as pointed out by Murphy et al. (2012), it would be uneconomical to store all negative properties of an entity or a concept. For instance, to describe cats (a concept), people usually use positive properties such as cats are mammals, cats eat fishes, and cats have four legs, but hardly ever negative properties like cats are not vehicles, cats do not have wheels, or cats are not used for communication.\n\nBased on such intuition, this paper proposes to impose non-negativity constraints on entity representations, by using which only positive properties will be stored in these representations. To better compare different entities on the same scale, we further require entity representations to stay within the hypercube of [0, 1] d , as approximately Boolean embeddings (Kruszewski et al., 2015), i.e.,\n0 \u2264 Re(e), Im(e) \u2264 1, \u2200e \u2208 E,(2)\nwhere e \u2208 C d is the representation for entity e \u2208 E, with its real and imaginary components denoted by Re(e), Im(e) \u2208 R d ; 0 and 1 are d-dimensional vectors with all their entries being 0 or 1; and \u2265, \u2264 , = denote the entry-wise comparisons throughout the paper whenever applicable. As shown by Lee and Seung (1999), non-negativity, in most cases, will further induce sparsity and interpretability.\n\n\nApproximate Entailment for Relations\n\nBesides the non-negativity constraints over entity representations, we also study approximate entailment constraints over relation representations. By approximate entailment, we mean an ordered pair of relations that the former approximately entails the latter, e.g., BornInCountry and Nationality, stating that a person born in a country is very likely, but not necessarily, to have a nationality of that country. Each such relation pair is associated with a weight to indicate the confidence level of entailment. A larger weight stands for a higher level of confidence. We denote by r p \u03bb \u2212 \u2192 r q the approximate entailment between relations r p and r q , with confidence level \u03bb. This kind of entailment can be derived automatically from a KG by modern rule mining systems (Gal\u00e1rraga et al., 2015). Let T denote the set of all such approximate entailments derived beforehand.\n\nBefore diving into approximate entailment, we first explore the modeling of strict entailment, i.e., entailment with infinite confidence level \u03bb = +\u221e. The strict entailment r p \u2192 r q states that if relation r p holds then relation r q must also hold. This entailment can be roughly modelled by requiring\n\u03c6(e i , r p , e j ) \u2264 \u03c6(e i , r q , e j ), \u2200e i , e j \u2208 E, (3)\nwhere \u03c6(\u00b7, \u00b7, \u00b7) is the score for a triple predicted by the embedding model, defined by Eq. (1). Eq. (3) can be interpreted as follows: for any two entities e i and e j , if (e i , r p , e j ) is a true fact with a high score \u03c6(e i , r p , e j ), then the triple (e i , r q , e j ) with an even higher score should also be predicted as a true fact by the embedding model. Note that given the non-negativity constraints defined by Eq. (2), a sufficient condition for Eq. (3) to hold, is to further impose Re(r p ) \u2264 Re(r q ), Im(r p ) = Im(r q ), (4) where r p and r q are the complex-valued representations for r p and r q respectively, with the real and imaginary components denoted by Re(\u00b7), Im(\u00b7) \u2208 R d . That means, when the constraints of Eq. (4) (along with those of Eq. (2)) are satisfied, the requirement of Eq. (3) (or in other words r p \u2192 r q ) will always hold. We provide a proof of sufficiency as supplementary material.\n\nNext we examine the modeling of approximate entailment. To this end, we further introduce the confidence level \u03bb and allow slackness in Eq. (4), which yields\n\u03bb Re(r p ) \u2212 Re(r q ) \u2264 \u03b1,(5)\n\u03bb Im(r p ) \u2212 Im(r q ) 2 \u2264 \u03b2.\n\nHere \u03b1, \u03b2 \u2265 0 are slack variables, and (\u00b7) 2 means an entry-wise operation. Entailments with higher confidence levels show less tolerance for violating the constraints. When \u03bb = +\u221e, Eqs. (5) -(6) degenerate to Eq. (4). The above analysis indicates that our approach can model entailment simply by imposing constraints over relation representations, without traversing all possible (e i , e j ) entity pairs (i.e., grounding). In addition, different confidence levels are encoded in the constraints, making our approach moderately tolerant of uncertainty.\n\n\nThe Overall Model\n\nFinally, we combine together the basic embedding model of ComplEx, the non-negativity constraints on entity representations, and the approximate entailment constraints over relation representations.\n\nThe overall model is presented as follows:\nmin \u0398,{\u03b1,\u03b2} D + \u222aD \u2212 log 1 + exp(\u2212y ijk \u03c6(e i , r k , e j )) + \u00b5 T 1 (\u03b1 + \u03b2) + \u03b7 \u0398 2 2 , s.t. \u03bb Re(r p ) \u2212 Re(r q ) \u2264 \u03b1, \u03bb Im(r p ) \u2212 Im(r q ) 2 \u2264 \u03b2, \u03b1, \u03b2 \u2265 0, \u2200r p \u03bb \u2212 \u2192 r q \u2208 T , 0 \u2264 Re(e), Im(e) \u2264 1, \u2200e \u2208 E.(7)\nHere, \u0398 {e : e \u2208 E} \u222a {r : r \u2208 R} is the set of all entity and relation representations; D + and D \u2212 are the sets of positive and negative training triples respectively; a positive triple is directly observed in the KG, i.e., (e i , r k , e j ) \u2208 O; a negative triple can be generated by randomly corrupting the head or the tail entity of a positive triple, i.e., (e i , r k , e j ) or (e i , r k , e j ); y ijk = \u00b11 is the label (positive or negative) of triple (e i , r k , e j ). In this optimization, the first term of the objective function is a typical logistic loss, which enforces triples to have scores close to their labels. The second term is the sum of slack variables in the approximate entailment constraints, with a penalty coefficient \u00b5 \u2265 0. The motivation is, although we allow slackness in those constraints we hope the total slackness to be small, so that the constraints can be better satisfied. The last term is L 2 regularization to avoid over-fitting, and \u03b7 \u2265 0 is the regularization coefficient.\n\nTo solve this optimization problem, the approximate entailment constraints (as well as the corresponding slack variables) are converted into penalty terms and added to the objective function, while the non-negativity constraints remain as they are. As such, the optimization problem of Eq. (7) can be rewritten as:\nmin \u0398 D + \u222aD \u2212 log 1 + exp(\u2212y ijk \u03c6(e i , r k , e j )) + \u00b5 T \u03bb1 Re(r p )\u2212Re(r q ) + + \u00b5 T \u03bb1 Im(r p )\u2212Im(r q ) 2 + \u03b7 \u0398 2 2 , s.t. 0 \u2264 Re(e), Im(e) \u2264 1, \u2200e \u2208 E,(8)\nwhere [x] + = max(0, x) with max(\u00b7, \u00b7) being an entry-wise operation. The equivalence between Eq. (7) and Eq. (8) is shown in the supplementary material. We use SGD in mini-batch mode as our optimizer, with AdaGrad (Duchi et al., 2011) to tune the learning rate. After each gradient descent step, we project (by truncation) real and imaginary components of entity representations into the hypercube of [0, 1] d , to satisfy the non-negativity constraints.\n\nWhile favouring a better structuring of the embedding space, imposing the additional constraints will not substantially increase model complexity. Our approach has a space complexity of O(nd + md), which is the same as that of ComplEx. Here, n is the number of entities, m the number of relations, and O(nd + md) to store a d-dimensional complex-valued vector for each entity and each relation. The time complexity (per iteration) of our approach is O(sd+td+nd), where s is the average number of triples in a mini-batch,n the average number of entities in a mini-batch, and t the total number of approximate entailments in T . O(sd) is to handle triples in a mini-batch, O(td) penalty terms introduced by the approximate entailments, and O(nd) further the non-negativity constraints on entity representations. Usually there are much fewer entailments than triples, i.e., t s, and als\u014d n \u2264 2s. 1 So the time complexity of our approach is on a par with O(sd), i.e., the time complexity of ComplEx.\n\n\nExperiments and Results\n\nThis section presents our experiments and results. We first introduce the datasets used in our experiments ( \u00a7 4.1). Then we empirically evaluate our approach in the link prediction task ( \u00a7 4.2). After that, we conduct extensive analysis on both entity representations ( \u00a7 4.3) and relation representations ( \u00a7 4.4) to show the interpretability of our model. 1 There will be at most 2s entities contained in s triples.\n\nCode and data used in the experiments are available at https://github.com/iieir-km/ ComplEx-NNE_AER.\n\n\nDatasets\n\nThe first two datasets we used are WN18 and F-B15K, released by Bordes et al. (2013). 2 WN18 is a subset of WordNet containing 18 relations and 40,943 entities, and FB15K a subset of Freebase containing 1,345 relations and 14,951 entities. We create our third dataset from the mapping-based objects of core DBpedia. 3 We eliminate relations not included within the DBpedia ontology such as HomePage and Logo, and discard entities appearing less than 20 times. The final dataset, referred to as DB100K, is composed of 470 relations and 99,604 entities. Triples on each datasets are further divided into training, validation, and test sets, used for model training, hyperparameter tuning, and evaluation respectively. We follow the original split for WN18 and FB15K, and draw a split of 597,572/ 50,000/50,000 triples for DB100K.\n\nWe further use AMIE+ (Gal\u00e1rraga et al., 2015) 4 to extract approximate entailments automatically from the training set of each dataset. As suggested by Guo et al. (2018), we consider entailments with PCA confidence higher than 0.8. 5 As such, we extract 17 approximate entailments from WN18, 535 from FB15K, and 56 from DB100K. Table 1 gives some examples of these approximate entailments, along with their confidence levels. Table 2 further summarizes the statistics of the datasets.\n\n\nLink Prediction\n\nWe first evaluate our approach in the link prediction task, which aims to predict a triple (e i , r k , e j ) with e i or e j missing, i.e., predict e i given (r k , e j ) or predict e j given (e i , r k ).\n\nEvaluation Protocol: We follow the protocol introduced by Bordes et al. (2013). For each test triple (e i , r k , e j ), we replace its head entity e i with every entity e i \u2208 E, and calculate a score for the corrupted triple (e i , r k , e j ), e.g., \u03c6(e i , r k , e j ) defined by Eq. (1). Then we sort these scores in de-  scending order, and get the rank of the correct entity e i . During ranking, we remove corrupted triples that already exist in either the training, validation, or test set, i.e., the filtered setting as described in (Bordes et al., 2013). This whole procedure is repeated while replacing the tail entity e j . We report on the test set the mean reciprocal rank (MRR) and the proportion of correct entities ranked in the top n (HITS@N), with n = 1, 3, 10.\n\nComparison Settings: We compare the performance of our approach against a variety of KG embedding models developed in recent years. These models can be categorized into three groups:\n\n\u2022 Simple embedding models that utilize triples alone without integrating extra information, including TransE (Bordes et al., 2013), Dist-Mult (Yang et al., 2015), HolE (Nickel et al., 2016b), ComplEx (Trouillon et al., 2016), and ANALOGY (Liu et al., 2017). Our approach is developed on the basis of ComplEx.\n\n\u2022 Other extensions of ComplEx that integrate logical background knowledge in addition to triples, including RUGE (Guo et al., 2018) and ComplEx R (Minervini et al., 2017a). The former requires grounding of first-order logic rules. The latter is restricted to relation equiv-alence and inversion, and assigns an identical confidence level to all different rules.\n\n\u2022 Latest developments or implementations that achieve current state-of-the-art performance reported on the benchmarks of WN18 and F-B15K, including R-GCN (Schlichtkrull et al., 2017), ConvE (Dettmers et al., 2018), and Single DistMult (Kadlec et al., 2017). 6 The first two are built based on neural network architectures, which are, by nature, more complicated than the simple models. The last one is a re-implementation of DistMult, generating 1000 to 2000 negative training examples per positive one, which leads to better performance but requires significantly longer training time.\n\nWe further evaluate our approach in two different settings: (i) ComplEx-NNE that imposes only the Non-Negativity constraints on Entity representations, i.e., optimization Eq. (8) with \u00b5 = 0; and (ii) ComplEx-NNE+AER that further imposes the Approximate Entailment constraints over Relation representations besides those non-negativity ones, i.e., optimization Eq. (8) with \u00b5 > 0.\n\nImplementation Details: We compare our approach against all the three groups of baselines on the benchmarks of WN18 and FB15K. We directly report their original results on these two datasets to avoid re-implementation bias. On DB100K, the newly created dataset, we take the first two groups of baselines, i.e., those simple embedding models and ComplEx extensions with logical background knowledge incorporated. We do not use the third group of baselines due to efficiency and complexity issues. We use the code provided by Trouillon et al. (2016) 7 for TransE, DistMult, and ComplEx, and the code released by their authors for ANAL-OGY 8 and RUGE 9 . We re-implement HolE and ComplEx R so that all the baselines (as well as our approach) share the same optimization mode, i.e., SGD with AdaGrad and gradient normalization, to facilitate a fair comparison. 10 We follow Trouillon et al. (2016) to adopt a ranking loss for TransE and a logistic loss for all the other methods.   (Trouillon et al., 2016). Results for the other baselines are taken from the original papers.\n\nMissing scores not reported in the literature are indicated by \"-\". Best scores are highlighted in bold, and \" * \" indicates statistically significant improvements over ComplEx.  Table 4: Link prediction results on the test set of DB100K, with best scores highlighted in bold, statistically significant improvements marked by \" * \".\n\nAmong those baselines, RUGE and ComplEx R require additional logical background knowledge. RUGE makes use of soft rules, which are extracted by AMIE+ from the training sets. As suggested by Guo et al. (2018), length-1 and length-2 rules with PCA confidence higher than 0.8 are utilized. Note that our approach also makes use of AMIE+ rules with PCA confidence higher than 0.8. But it only considers entailments between a pair of relations, i.e., length-1 rules. ComplEx R takes into account equivalence and inversion between relations. We derive such axioms directly from our approximate entailments. If r p \u03bb 1 \u2212 \u2192 r q and r q \u03bb 2 \u2212 \u2192 r p with \u03bb 1 , \u03bb 2 > 0.8, we think relations r p and r q are equivalent.\n\nAnd similarly, if r \u22121 p \u03bb 1 \u2212 \u2192 r q and r \u22121 q \u03bb 2 \u2212 \u2192 r p with \u03bb 1 , \u03bb 2 > 0.8, we consider r p as an inverse of r q .\n\nFor all the methods, we create 100 mini-batches on each dataset, and conduct a grid search to find hyperparameters that maximize MRR on the validation set, with at most 1000 iterations over the training set. Specifically, we tune the embedding size d \u2208 {100, 150, 200}, the L 2 regularization coefficient \u03b7 \u2208 {0.001, 0.003, 0.01, 0.03, 0.1}, the ratio of negative over positive training examples \u03b1 \u2208 {2, 10}, and the initial learning rate \u03b3 \u2208 {0.01, 0.05, 0.1, 0.5, 1.0}. For TransE, we tune the margin of the ranking loss \u03b4 \u2208 {0.1, 0.2, 0.5, 1, 2, 5, 10}. Other hyperparameters of ANALOGY and RUGE are set or tuned according to the default settings suggested by their authors (Liu et al., 2017;Guo et al., 2018). After getting the best ComplEx model, we tune the relation constraint penalty of our approach ComplEx-NNE+AER (\u00b5 in Eq. (8)) in the range of {10 \u22125 , 10 \u22124 , \u00b7 \u00b7 \u00b7 , 10 4 , 10 5 }, with all its other hyperparameters fixed to their optimal configurations. We then directly set \u00b5 = 0 to get the optimal ComplEx-NNE model. The weight of soft constraints in ComplEx R is tuned in the same range as \u00b5. The optimal configurations for our approach are: d = 200, \u03b7 = 0.03, \u03b1 = 10, \u03b3 = 1.0, \u00b5 = 10 on WN18; d = 200, \u03b7 = 0.01, \u03b1 = 10, \u03b3 = 0.5, \u00b5 = 10 \u22123 on FB15K; and d = 150, \u03b7 = 0.03, \u03b1 = 10, \u03b3 = 0.1, \u00b5 = 10 \u22125 on DB100K.\n\nExperimental Results: Table 3 presents the results on the test sets of WN18 and FB15K, where the results for the baselines are taken directly from previous literature. Table 4 further provides the results on the test set of DB100K, with all the methods tuned and tested in (almost) the same setting. On all the datasets, we test statistical significance of the improvements achieved by ComplEx-NNE/ ComplEx-NNE+AER over ComplEx, by using a paired t-test. The reciprocal rank or HITS@N value with n = 1, 3, 10 for each test triple is used as paired data. The symbol \" * \" indicates a significance level of p < 0.05.\n\nThe results demonstrate that imposing the nonnegativity and approximate entailment constraints indeed improves KG embedding. ComplEx-NNE and ComplEx-NNE+AER perform better than (or at least equally well as) ComplEx in almost all the metrics on all the three datasets, and most of the improvements are statistically significant (except those on WN18). More interestingly, just by introducing these simple constraints, ComplEx-NNE+ AER can beat very strong baselines, including the best performing basic models like ANALOGY, those previous extensions of ComplEx like RUGE or ComplEx R , and even the complicated developments or implementations like ConvE or Single DistMult. This demonstrates the superiority of our approach.\n\n\nAnalysis on Entity Representations\n\nThis section inspects how the structure of the entity embedding space changes when the constraints are imposed. We first provide the visualization of entity representations on DB100K. On this dataset each entity is associated with a single type label. 11 We pick 4 types reptile, wine region, species, and programming language, and randomly select 30 entities from each type. Figure 1 visualizes the representations of these entities learned by Com-plEx and ComplEx-NNE+AER (real components only), with the optimal configurations determined by link prediction (see \u00a7 4.2 for details, applicable to all analysis hereafter). During the visualization, we normalize the real component of each entity by\n[x] = [x] \u2212min(x)\nmax(x)\u2212min(x) , where min(x) or max(x) is the minimum or maximum entry of x respectively. We observe that after imposing the non-negativity constraints, ComplEx-NNE+AER indeed obtains compact and interpretable representations for entities. Each entity is represented by only a relatively small number of \"active\" dimensions.  with the same type tend to activate the same set of dimensions, while entities with different types often get clearly different dimensions activated. Then we investigate the semantic purity of these dimensions. Specifically, we collect the representations of all the entities on DB100K (real components only). For each dimension of these representations, top K percent of entities with the highest activation values on this dimension are picked. We can calculate the entropy of the type distribution of the entities selected. This entropy reflects diversity of entity types, or in other words, semantic purity. If all the K percent of entities have the same type, we will get the lowest entropy of zero (the highest semantic purity). On the contrary, if each of them has a distinct type, we will get the highest entropy (the lowest semantic purity). Figure 2 shows the average entropy over all dimensions of entity representations (real components only) learned by ComplEx, ComplEx-NNE, and ComplEx-NNE+ AER, as K varies. We can see that after imposing the non-negativity constraints, ComplEx-NNE and ComplEx-NNE+AER can learn entity representations with latent dimensions of consistently higher semantic purity. We have conducted the same analyses on imaginary components of entity representations, and observed similar phenomena. The results are given as supplementary material.\n\n\nAnalysis on Relation Representations\n\nThis section further provides a visual inspection of the relation embedding space when the constraints are imposed. To this end, we group relation pairs involved in the DB100K entailment constraints into 3 classes: equivalence, inversion, and others. 12 We choose 2 pairs of relations from each class, and visualize these relation representations learned by ComplEx-NNE+AER in Figure 3, where for each relation we randomly pick 5 dimensions from both its real and imaginary components. By imposing the approximate entailment constraints, these relation representations can encode logical regularities quite well. Pairs of relations from the first class (equivalence) tend to have identical representations r p \u2248 r q , those from the second class (inversion) complex conjugate representations r p \u2248r q ; and the others representations that Re(r p ) \u2264 Re(r q ) and Im(r p ) \u2248 Im(r q ).\n\n12 Equivalence and inversion are detected using heuristics introduced in \u00a7 4.2 (implementation details). See the supplementary material for detailed properties of these three classes.\n\n\nConclusion\n\nThis paper investigates the potential of using very simple constraints to improve KG embedding. Two types of constraints have been studied: (i) the non-negativity constraints to learn compact, interpretable entity representations, and (ii) the approximate entailment constraints to further encode logical regularities into relation representations. Such constraints impose prior beliefs upon the structure of the embedding space, and will not significantly increase the space or time complexity. Experimental results on benchmark KGs demonstrate that our method is simple yet surprisingly effective, showing significant and consistent improvements over strong baselines. The constraints indeed improve model interpretability, yielding a substantially increased structuring of the embedding space. Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christopher Meek. 2014 \n\n\n(2016) proposed lifted rule injection, and Minervini et al. (2017b) investigated adversarial training.\n\nFigure 2 :\n2Average entropy over all dimensions of real components of entity representations learned by ComplEx (circles), ComplEx-NNE (squares), and ComplEx-NNE+AER (triangles) as K varies.\n\nFigure 3 :\n3Visualization of relation representations learned by ComplEx-NNE+AER, with the top 4 relations from the equivalence class, the middle 4 the inversion class, and the bottom 4 others.\n\n\nwhere r \u22121 means the inverse of relation r.hypernym \u22121 1.00 \n\u2212\u2212\u2192 hyponym \nsynset domain topic of \u22121 0.99 \n\u2212\u2212\u2192 member of domain topic \ninstance hypernym \u22121 0.98 \n\u2212\u2212\u2192 instance hyponym \n\n/people/place of birth \u22121 1.00 \n\u2212\u2212\u2192 /location/people born here \n/film/directed by \u22121 0.98 \n\u2212\u2212\u2192 /director/film \n/country/admin divisions \n\n0.91 \n\n\u2212\u2212\u2192 /country/1st level divisions \n\nowner \n\n0.95 \n\n\u2212\u2212\u2192 owning company \nchild \u22121 0.92 \n\u2212\u2212\u2192 parent \ndistributing company \n\n0.92 \n\n\u2212\u2212\u2192 distributing label \n\nTable 1: Approximate entailments extracted from \nWN18 (top), FB15K (middle), and DB100K (bot-\ntom), Dataset \n# Ent # Rel \n# Train/Valid/Test \n# Cons \n\nWN18 \n40,943 \n18 141,442 5,000 5,000 \n17 \nFB15K 14,951 1,345 483,142 50,000 59,071 \n535 \nDB100K 99,604 470 597,572 50,000 50,000 \n56 \n\n\n\nTable 2 :\n2Statistics of datasets, where the columns respectively indicate the number of entities, relations, training/validation/test triples, and approximate entailments.\n\nTable 3 :\n3Link prediction results on the test sets of WN18 and FB15K. Results for TransE and DistMult \nare taken from \n\n\nAnd entities 11 http://downloads.dbpedia.org/2016-10/ core-i18n/en/instance_types_wkd_uris_en. ttl.bz20 \n50 \n100 \n150 0 \n50 \n100 \n150 \n\nComplEx-NNE+AER \nComplEx \n\nFigure 1: Visualization of real components of en-\ntity representations (rows) learned by ComplEx-\nNNE+AER (left) and ComplEx (right). From top \nto bottom, entities belong to type reptile, wine \nregion, species, and programming language in \nturn. Values range from 0 (white) via 0.5 (orange) \nto 1 (black). Best viewed in color. \n\n0 \n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nk value \n\n3.8 \n\n4.0 \n\n4.2 \n\n4.4 \n\n4.6 \n\n4.8 \n\nAverage entropy \n\nComplEx \nComplEx-NNE \nComplEx-NNE+AER \n\n\n\n\n. Typed tensor decomposition of knowledge bases for relation extraction. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. pages 1568-1579. Thomas Demeester, Tim Rockt\u00e4schel, and Sebastian Riedel. 2016. Lifted rule injection for relation embeddings. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pages 1389-1399. Tim Dettmers, Minervini Pasquale, Stenetorp Pontus, and Sebastian Riedel. 2018. Convolutional 2D knowledge graph embeddings. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence. pages 1811-1818.\nhttps://everest.hds.utc.fr/doku.php? id=en:smemlj12 3 http://downloads.dbpedia.org/2016-10/ core/ 4 https://www.mpi-inf.mpg.de/departmen ts/databases-and-information-systems/res earch/yago-naga/amie/ 5 PCA confidence is the confidence under the partial completeness assumption. See(Gal\u00e1rraga et al., 2015) for details.\nWe do not consider EnsembleDistMult (Dettmers et al.,  2018)  which combines several different models together, to facilitate a fair comparison. 7 https://github.com/ttrouill/complex 8 https://github.com/quark0/ANALOGY 9 https://github.com/iieir-km/RUGE 10 An exception here is that ANALOGY uses asynchronous SGD with AdaGrad(Liu et al., 2017).\nAcknowledgmentsWe would like to thank all the anonymous reviewers for their insightful and valuable suggestions, which help to improve the quality of this paper.\nFreebase: A collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. the 2008 ACM SIGMOD International Conference on Management of DataKurt Bollacker, Colin Evans, Praveen Paritosh, Tim S- turge, and Jamie Taylor. 2008. Freebase: A collab- oratively created graph database for structuring hu- man knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. pages 1247-1250.\n\nA semantic matching energy function for learning with multi-relational data. Antoine Bordes, Xavier Glorot, Jason Weston, Yoshua Bengio, Machine Learning. 942Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic matching ener- gy function for learning with multi-relational data. Machine Learning 94(2):233-259.\n\nTranslating embeddings for modeling multirelational data. Antoine Bordes, Nicolas Usunier, Alberto Garc\u00eda-Dur\u00e1n, Jason Weston, Oksana Yakhnenko, Advances in Neural Information Processing Systems. Antoine Bordes, Nicolas Usunier, Alberto Garc\u00eda- Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In Advances in Neural Information Processing Systems. pages 2787-2795.\n\nLearning structured embeddings of knowledge bases. Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, Proceedings of the 25th AAAI Conference on Artificial Intelligence. the 25th AAAI Conference on Artificial IntelligenceAntoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured em- beddings of knowledge bases. In Proceedings of the 25th AAAI Conference on Artificial Intelligence. pages 301-306.\n\nKnowledge vault: A web-scale approach to probabilistic knowledge fusion. Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, Wei Zhang, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningXin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohman- n, Shaohua Sun, and Wei Zhang. 2014. Knowl- edge vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 601-610.\n\nAdaptive subgradient methods for online learning and stochastic optimization. John Duchi, Elad Hazan, Yoram Singer, Journal of Machine Learning Research. 12John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12(Jul):2121-2159.\n\nA compositional and interpretable semantic space. Alona Fyshe, Leila Wehbe, Partha P Talukdar, Brian Murphy, Tom M Mitchell, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAlona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian Murphy, and Tom M. Mitchell. 2015. A composi- tional and interpretable semantic space. In Proceed- ings of the 2015 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies. pages 32- 41.\n\nFast rule mining in ontological knowledge bases with AMIE+. Luis Antonio Gal\u00e1rraga, Christina Teflioudi, Katja Hose, Fabian M Suchanek, The VLDB Journal. 246Luis Antonio Gal\u00e1rraga, Christina Teflioudi, Katja Hose, and Fabian M. Suchanek. 2015. Fast rule min- ing in ontological knowledge bases with AMIE+. The VLDB Journal 24(6):707-730.\n\nSemantically smooth knowledge graph embedding. Shu Guo, Quan Wang, Bin Wang, Lihong Wang, Li Guo, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingShu Guo, Quan Wang, Bin Wang, Lihong Wang, and Li Guo. 2015. Semantically smooth knowledge graph embedding. In Proceedings of the 53rd An- nual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer- ence on Natural Language Processing. pages 84-94.\n\nJointly embedding knowledge graphs and logical rules. Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingShu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. 2016. Jointly embedding knowledge graphs and logical rules. In Proceedings of the 2016 Con- ference on Empirical Methods in Natural Language Processing. pages 192-202.\n\nKnowledge graph embedding with iterative guidance from soft rules. Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo, Proceedings of the 32nd AAAI Conference on Artificial Intelligence. the 32nd AAAI Conference on Artificial IntelligenceShu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. 2018. Knowledge graph embedding with iterative guidance from soft rules. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence. pages 4816-4823.\n\nKnowledge-based weak supervision for information extraction of overlapping relations. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S Weld, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. the 49th Annual Meeting of the Association for Computational LinguisticsRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. pages 541-550.\n\nA latent factor model for highly multi-relational data. Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, Guillaume R Obozinski, Advances in Neural Information Processing Systems. Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes, and Guillaume R. Obozinski. 2012. A latent fac- tor model for highly multi-relational data. In Ad- vances in Neural Information Processing Systems. pages 3167-3175.\n\nKnowledge graph embedding via dynamic mapping matrix. Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, Jun Zhao, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge graph embedding vi- a dynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association for Compu- tational Linguistics and the 7th International Joint Conference on Natural Language Processing. pages 687-696.\n\nKnowledge base completion: Baselines strike back. Rudolf Kadlec, Ondrej Bajgar, Proceedings of the 2nd Workshop on Representation Learning for NLP. the 2nd Workshop on Representation Learning for NLPRudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. 2017. Knowledge base completion: Baselines strike back. In Proceedings of the 2nd Workshop on Rep- resentation Learning for NLP. pages 69-74.\n\nDeriving Boolean structures from distributional vectors. German Kruszewski, Denis Paperno, Marco Baroni, Transactions of the Association for Computational Linguistics. 3German Kruszewski, Denis Paperno, and Marco Ba- roni. 2015. Deriving Boolean structures from distri- butional vectors. Transactions of the Association for Computational Linguistics 3:375-388.\n\nLearning the parts of objects by non-negative matrix factorization. D Daniel, H Sebastian Lee, Seung, Nature. 401Daniel D. Lee and H. Sebastian Seung. 1999. Learning the parts of objects by non-negative matrix factor- ization. Nature 401:788-791.\n\nDBpedia: A largescale, multilingual knowledge base extracted from Wikipedia. Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, S\u00f6ren Patrick Van Kleef, Auer, Semantic Web Journal. 62Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, S\u00f6ren Auer, et al. 2015. DBpedia: A large- scale, multilingual knowledge base extracted from Wikipedia. Semantic Web Journal 6(2):167-195.\n\nModeling relation paths for representation learning of knowledge bases. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, Song Liu, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingYankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. 2015a. Modeling rela- tion paths for representation learning of knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing. pages 705-714.\n\nLearning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, Proceedings of the 29th AAAI Conference on Artificial Intelligence. the 29th AAAI Conference on Artificial IntelligenceYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015b. Learning entity and relation em- beddings for knowledge graph completion. In Pro- ceedings of the 29th AAAI Conference on Artificial Intelligence. pages 2181-2187.\n\nAnalogical inference for multi-relational embeddings. Hanxiao Liu, Yuexin Wu, Yiming Yang, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningHanxiao Liu, Yuexin Wu, and Yiming Yang. 2017. Analogical inference for multi-relational embed- dings. In Proceedings of the 34th International Con- ference on Machine Learning. pages 2168-2178.\n\nOnline learning of interpretable word embeddings. Hongyin Luo, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingHongyin Luo, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2015a. Online learning of inter- pretable word embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1687-1692.\n\nContext-dependent knowledge graph embedding. Yuanfei Luo, Quan Wang, Bin Wang, Li Guo, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingYuanfei Luo, Quan Wang, Bin Wang, and Li Guo. 2015b. Context-dependent knowledge graph em- bedding. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing. pages 1656-1661.\n\nRegularizing knowledge graph embeddings via equivalence and inversion axioms. Pasquale Minervini, Luca Costabello, Emir Mu\u00f1oz, V\u00edt Nov\u00e1\u010dek, Pierre-Yves Vandenbussche, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Pasquale Minervini, Luca Costabello, Emir Mu\u00f1oz, V\u00edt Nov\u00e1\u010dek, and Pierre-Yves Vandenbussche. 2017a. Regularizing knowledge graph embeddings via e- quivalence and inversion axioms. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pages 668-683.\n\nAdversarial sets for regularising neural link predictors. Pasquale Minervini, Thomas Demeester, Tim Rockt\u00e4schel, Sebastian Riedel, Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence. the 33rd Conference on Uncertainty in Artificial IntelligencePasquale Minervini, Thomas Demeester, Tim Rock- t\u00e4schel, and Sebastian Riedel. 2017b. Adversarial sets for regularising neural link predictors. In Pro- ceedings of the 33rd Conference on Uncertainty in Artificial Intelligence.\n\nLearning effective and interpretable semantic models using non-negative sparse embedding. Brian Murphy, Partha Talukdar, Tom Mitchell, Proceedings of COLING 2012. COLING 2012Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012. Learning effective and interpretable seman- tic models using non-negative sparse embedding. In Proceedings of COLING 2012. pages 1933-1950.\n\nCompositional vector space models for knowledge base completion. Arvind Neelakantan, Benjamin Roth, Andrew M-Ccallum , Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingArvind Neelakantan, Benjamin Roth, and Andrew M- cCallum. 2015. Compositional vector space model- s for knowledge base completion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. pages 156-166.\n\nA review of relational machine learning for knowledge graphs. Proceedings of the. Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich, IEEE. 1041Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2016a. A review of relational machine learning for knowledge graphs. Proceed- ings of the IEEE 104(1):11-33.\n\nHolographic embeddings of knowledge graphs. Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio, Proceedings of the 30th AAAI Conference on Artificial Intelligence. the 30th AAAI Conference on Artificial IntelligenceMaximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016b. Holographic embeddings of knowl- edge graphs. In Proceedings of the 30th AAAI Con- ference on Artificial Intelligence. pages 1955-1961.\n\nA three-way model for collective learning on multi-relational data. Maximilian Nickel, Hans-Peter Volker Tresp, Kriegel, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning. pages 809-816.\n\nInjecting logical background knowledge into embeddings for relation extraction. Tim Rockt\u00e4schel, Sameer Singh, Sebastian Riedel, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesTim Rockt\u00e4schel, Sameer Singh, and Sebastian Riedel. 2015. Injecting logical background knowledge in- to embeddings for relation extraction. In Proceed- ings of the 2015 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies. pages 1119-1129.\n\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den, Ivan Berg, Max Titov, Welling, arXiv:1703.06103Modeling relational data with graph convolutional networks. Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. Modeling relational data with graph convolu- tional networks. arXiv:1703.06103 .\n\nProjE: Embedding projection for knowledge graph completion. Baoxu Shi, Tim Weninger, Proceedings of the 31st AAAI Conference on Artificial Intelligence. the 31st AAAI Conference on Artificial IntelligenceBaoxu Shi and Tim Weninger. 2017. ProjE: Embed- ding projection for knowledge graph completion. In Proceedings of the 31st AAAI Conference on Artifi- cial Intelligence. pages 1236-1242.\n\nReasoning with neural tensor networks for knowledge base completion. Richard Socher, Danqi Chen, Christopher D Manning, Andrew Y Ng, Advances in Neural Information Processing Systems. Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing System- s. pages 926-934.\n\nComplex embeddings for simple link prediction. Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, Guillaume Bouchard, Proceedings of the 33rd International Conference on Machine Learning. the 33rd International Conference on Machine LearningTh\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. 2016. Complex embeddings for simple link prediction. In Proceed- ings of the 33rd International Conference on Ma- chine Learning. pages 2071-2080.\n\nKnowledge graph embedding: A survey of approaches and applications. Quan Wang, Zhendong Mao, Bin Wang, Li Guo, IEEE Transactions on Knowledge and Data Engineering. 2912Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering 29(12):2724- 2743.\n\nKnowledge base completion using embeddings and rules. Quan Wang, Bin Wang, Li Guo, Proceedings of the 24th International Joint Conference on Artificial Intelligence. the 24th International Joint Conference on Artificial IntelligenceQuan Wang, Bin Wang, and Li Guo. 2015. Knowl- edge base completion using embeddings and rules. In Proceedings of the 24th International Joint Con- ference on Artificial Intelligence. pages 1859-1865.\n\nKnowledge graph embedding by translating on hyperplanes. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, Proceedings of the 28th AAAI Conference on Artificial Intelligence. the 28th AAAI Conference on Artificial IntelligenceZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by trans- lating on hyperplanes. In Proceedings of the 28th AAAI Conference on Artificial Intelligence. pages 1112-1119.\n\nLearning to identify the best contexts for knowledge-based WSD. Evgenia Wasserman-Pritsker, William W Cohen, Einat Minkov, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingEvgenia Wasserman-Pritsker, William W. Cohen, and Einat Minkov. 2015. Learning to identify the best contexts for knowledge-based WSD. In Proceed- ings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1662-1667.\n\nTransG: A generative model for knowledge graph embedding. Han Xiao, Minlie Huang, Xiaoyan Zhu, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsHan Xiao, Minlie Huang, and Xiaoyan Zhu. 2016. TransG: A generative model for knowledge graph embedding. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics. pages 2316-2325.\n\nSSP: Semantic space projection for knowledge graph embedding with text descriptions. Han Xiao, Minlie Huang, Xiaoyan Zhu, Proceedings of the 31st AAAI Conference on Artificial Intelligence. the 31st AAAI Conference on Artificial IntelligenceHan Xiao, Minlie Huang, and Xiaoyan Zhu. 2017. SSP: Semantic space projection for knowledge graph embedding with text descriptions. In Pro- ceedings of the 31st AAAI Conference on Artificial Intelligence. pages 3104-3110.\n\nRepresentation learning of knowledge graphs with entity descriptions. Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, Maosong Sun, Proceedings of the 30th AAAI Conference on Artificial Intelligence. the 30th AAAI Conference on Artificial IntelligenceRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016a. Representation learning of knowledge graphs with entity descriptions. In Pro- ceedings of the 30th AAAI Conference on Artificial Intelligence. pages 2659-2665.\n\nRepresentation learning of knowledge graphs with hierarchical types. Ruobing Xie, Zhiyuan Liu, Maosong Sun, Proceedings of the 25th International Joint Conference on Artificial Intelligence. the 25th International Joint Conference on Artificial IntelligenceRuobing Xie, Zhiyuan Liu, and Maosong Sun. 2016b. Representation learning of knowledge graphs with hierarchical types. In Proceedings of the 25th Inter- national Joint Conference on Artificial Intelligence. pages 2965-2971.\n\nLeveraging knowledge bases in LSTMs for improving machine reading. Bishan Yang, Tom Mitchell, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsBishan Yang and Tom Mitchell. 2017. Leveraging knowledge bases in LSTMs for improving machine reading. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistic- s. pages 1436-1446.\n\nEmbedding entities and relations for learning and inference in knowledge bases. Bishan Yang, Wen-Tau Yih, Xiaodong He, Jianfeng Gao, Li Deng, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the International Confer- ence on Learning Representations.\n\nAligning knowledge and text embeddings by entity descriptions. Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, Zheng Chen, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingHuaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. 2015. Aligning knowledge and text embeddings by entity descriptions. In Proceed- ings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 267-272.\n", "annotations": {"author": "[{\"end\":250,\"start\":81},{\"end\":491,\"start\":251},{\"end\":655,\"start\":492},{\"end\":815,\"start\":656}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":88},{\"end\":260,\"start\":256},{\"end\":500,\"start\":496},{\"end\":662,\"start\":659}]", "author_first_name": "[{\"end\":87,\"start\":81},{\"end\":255,\"start\":251},{\"end\":495,\"start\":492},{\"end\":658,\"start\":656}]", "author_affiliation": "[{\"end\":180,\"start\":115},{\"end\":249,\"start\":182},{\"end\":346,\"start\":281},{\"end\":415,\"start\":348},{\"end\":490,\"start\":417},{\"end\":585,\"start\":520},{\"end\":654,\"start\":587},{\"end\":745,\"start\":680},{\"end\":814,\"start\":747}]", "title": "[{\"end\":61,\"start\":1},{\"end\":876,\"start\":816}]", "venue": "[{\"end\":979,\"start\":878}]", "abstract": "[{\"end\":2380,\"start\":1107}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2530,\"start\":2506},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2562,\"start\":2540},{\"end\":2621,\"start\":2616},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2647,\"start\":2628},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2974,\"start\":2941},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2996,\"start\":2974},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3020,\"start\":2996},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3482,\"start\":3462},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3504,\"start\":3482},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3526,\"start\":3504},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3546,\"start\":3526},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3652,\"start\":3631},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3672,\"start\":3652},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3690,\"start\":3672},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3708,\"start\":3690},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3726,\"start\":3708},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3747,\"start\":3726},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3770,\"start\":3747},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3787,\"start\":3770},{\"end\":3862,\"start\":3842},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3881,\"start\":3862},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3899,\"start\":3881},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3924,\"start\":3899},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3942,\"start\":3924},{\"end\":3962,\"start\":3942},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3980,\"start\":3962},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4005,\"start\":3986},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4479,\"start\":4458},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4732,\"start\":4706},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4749,\"start\":4732},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5080,\"start\":5054},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5096,\"start\":5080},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5114,\"start\":5096},{\"end\":5323,\"start\":5300},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5657,\"start\":5633},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5829,\"start\":5805},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7711,\"start\":7690},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7835,\"start\":7814},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8001,\"start\":7982},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8019,\"start\":8001},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8035,\"start\":8019},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8078,\"start\":8059},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8099,\"start\":8078},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8122,\"start\":8099},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8139,\"start\":8122},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8199,\"start\":8178},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8219,\"start\":8199},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8242,\"start\":8219},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8269,\"start\":8242},{\"end\":8291,\"start\":8269},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8400,\"start\":8382},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8443,\"start\":8417},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8461,\"start\":8443},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8507,\"start\":8488},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8525,\"start\":8507},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8565,\"start\":8543},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8583,\"start\":8565},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8975,\"start\":8949},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8991,\"start\":8975},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9009,\"start\":8991},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9317,\"start\":9293},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9935,\"start\":9914},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10125,\"start\":10104},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10143,\"start\":10125},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10162,\"start\":10143},{\"end\":10605,\"start\":10597},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10727,\"start\":10703},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12049,\"start\":12025},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12639,\"start\":12619},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13399,\"start\":13374},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13757,\"start\":13737},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14681,\"start\":14657},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19048,\"start\":19028},{\"end\":20654,\"start\":20653},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20911,\"start\":20891},{\"end\":21144,\"start\":21143},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21825,\"start\":21808},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22446,\"start\":22426},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22931,\"start\":22910},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23464,\"start\":23443},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23495,\"start\":23476},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23524,\"start\":23502},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23558,\"start\":23534},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23590,\"start\":23572},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23775,\"start\":23757},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23815,\"start\":23790},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24189,\"start\":24161},{\"end\":24220,\"start\":24191},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24263,\"start\":24242},{\"end\":24266,\"start\":24265},{\"end\":25525,\"start\":25500},{\"end\":25835,\"start\":25833},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25869,\"start\":25846},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25978,\"start\":25954},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26590,\"start\":26573},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27910,\"start\":27892},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27927,\"start\":27910},{\"end\":30177,\"start\":30175},{\"end\":32640,\"start\":32638},{\"end\":34334,\"start\":34267},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37452,\"start\":37428},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37809,\"start\":37791}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34440,\"start\":34336},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34632,\"start\":34441},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34827,\"start\":34633},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35597,\"start\":34828},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35771,\"start\":35598},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35892,\"start\":35772},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36532,\"start\":35893},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":37146,\"start\":36533}]", "paragraph": "[{\"end\":3021,\"start\":2396},{\"end\":4949,\"start\":3023},{\"end\":6044,\"start\":4951},{\"end\":6567,\"start\":6046},{\"end\":6662,\"start\":6569},{\"end\":7058,\"start\":6664},{\"end\":7294,\"start\":7060},{\"end\":8849,\"start\":7311},{\"end\":9717,\"start\":8851},{\"end\":10300,\"start\":9719},{\"end\":10657,\"start\":10317},{\"end\":11506,\"start\":10685},{\"end\":12050,\"start\":11579},{\"end\":13005,\"start\":12095},{\"end\":13406,\"start\":13007},{\"end\":13840,\"start\":13440},{\"end\":14759,\"start\":13881},{\"end\":15064,\"start\":14761},{\"end\":16061,\"start\":15128},{\"end\":16220,\"start\":16063},{\"end\":16279,\"start\":16251},{\"end\":16835,\"start\":16281},{\"end\":17055,\"start\":16857},{\"end\":17099,\"start\":17057},{\"end\":18333,\"start\":17314},{\"end\":18649,\"start\":18335},{\"end\":19268,\"start\":18813},{\"end\":20265,\"start\":19270},{\"end\":20712,\"start\":20293},{\"end\":20814,\"start\":20714},{\"end\":21654,\"start\":20827},{\"end\":22140,\"start\":21656},{\"end\":22366,\"start\":22160},{\"end\":23148,\"start\":22368},{\"end\":23332,\"start\":23150},{\"end\":23642,\"start\":23334},{\"end\":24005,\"start\":23644},{\"end\":24593,\"start\":24007},{\"end\":24974,\"start\":24595},{\"end\":26047,\"start\":24976},{\"end\":26381,\"start\":26049},{\"end\":27091,\"start\":26383},{\"end\":27213,\"start\":27093},{\"end\":28543,\"start\":27215},{\"end\":29159,\"start\":28545},{\"end\":29884,\"start\":29161},{\"end\":30621,\"start\":29923},{\"end\":32346,\"start\":30640},{\"end\":33270,\"start\":32387},{\"end\":33455,\"start\":33272},{\"end\":34335,\"start\":33470}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11578,\"start\":11507},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13439,\"start\":13407},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15127,\"start\":15065},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16250,\"start\":16221},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17313,\"start\":17100},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18812,\"start\":18650},{\"attributes\":{\"id\":\"formula_7\"},\"end\":30639,\"start\":30622}]", "table_ref": "[{\"end\":21991,\"start\":21984},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22089,\"start\":22082},{\"end\":26235,\"start\":26228},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28574,\"start\":28567},{\"end\":28720,\"start\":28713}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2394,\"start\":2382},{\"attributes\":{\"n\":\"2\"},\"end\":7309,\"start\":7297},{\"attributes\":{\"n\":\"3\"},\"end\":10315,\"start\":10303},{\"attributes\":{\"n\":\"3.1\"},\"end\":10683,\"start\":10660},{\"attributes\":{\"n\":\"3.2\"},\"end\":12093,\"start\":12053},{\"attributes\":{\"n\":\"3.3\"},\"end\":13879,\"start\":13843},{\"attributes\":{\"n\":\"3.4\"},\"end\":16855,\"start\":16838},{\"attributes\":{\"n\":\"4\"},\"end\":20291,\"start\":20268},{\"attributes\":{\"n\":\"4.1\"},\"end\":20825,\"start\":20817},{\"attributes\":{\"n\":\"4.2\"},\"end\":22158,\"start\":22143},{\"attributes\":{\"n\":\"4.3\"},\"end\":29921,\"start\":29887},{\"attributes\":{\"n\":\"4.4\"},\"end\":32385,\"start\":32349},{\"attributes\":{\"n\":\"5\"},\"end\":33468,\"start\":33458},{\"end\":34452,\"start\":34442},{\"end\":34644,\"start\":34634},{\"end\":35608,\"start\":35599},{\"end\":35782,\"start\":35773}]", "table": "[{\"end\":35597,\"start\":34873},{\"end\":35892,\"start\":35784},{\"end\":36532,\"start\":35997}]", "figure_caption": "[{\"end\":34440,\"start\":34338},{\"end\":34632,\"start\":34454},{\"end\":34827,\"start\":34646},{\"end\":34873,\"start\":34830},{\"end\":35771,\"start\":35610},{\"end\":35997,\"start\":35895},{\"end\":37146,\"start\":36535}]", "figure_ref": "[{\"end\":30307,\"start\":30299},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31824,\"start\":31816},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32772,\"start\":32764}]", "bib_author_first_name": "[{\"end\":38061,\"start\":38057},{\"end\":38078,\"start\":38073},{\"end\":38093,\"start\":38086},{\"end\":38107,\"start\":38104},{\"end\":38121,\"start\":38116},{\"end\":38640,\"start\":38633},{\"end\":38655,\"start\":38649},{\"end\":38669,\"start\":38664},{\"end\":38684,\"start\":38678},{\"end\":38961,\"start\":38954},{\"end\":38977,\"start\":38970},{\"end\":38994,\"start\":38987},{\"end\":39014,\"start\":39009},{\"end\":39029,\"start\":39023},{\"end\":39380,\"start\":39373},{\"end\":39394,\"start\":39389},{\"end\":39408,\"start\":39403},{\"end\":39426,\"start\":39420},{\"end\":39842,\"start\":39839},{\"end\":39856,\"start\":39849},{\"end\":39876,\"start\":39870},{\"end\":39889,\"start\":39884},{\"end\":39898,\"start\":39896},{\"end\":39909,\"start\":39904},{\"end\":39924,\"start\":39918},{\"end\":39943,\"start\":39936},{\"end\":39952,\"start\":39949},{\"end\":40552,\"start\":40548},{\"end\":40564,\"start\":40560},{\"end\":40577,\"start\":40572},{\"end\":40864,\"start\":40859},{\"end\":40877,\"start\":40872},{\"end\":40891,\"start\":40885},{\"end\":40893,\"start\":40892},{\"end\":40909,\"start\":40904},{\"end\":40921,\"start\":40918},{\"end\":40923,\"start\":40922},{\"end\":41574,\"start\":41570},{\"end\":41603,\"start\":41594},{\"end\":41620,\"start\":41615},{\"end\":41633,\"start\":41627},{\"end\":41635,\"start\":41634},{\"end\":41899,\"start\":41896},{\"end\":41909,\"start\":41905},{\"end\":41919,\"start\":41916},{\"end\":41932,\"start\":41926},{\"end\":41941,\"start\":41939},{\"end\":42605,\"start\":42602},{\"end\":42615,\"start\":42611},{\"end\":42628,\"start\":42622},{\"end\":42638,\"start\":42635},{\"end\":42647,\"start\":42645},{\"end\":43106,\"start\":43103},{\"end\":43116,\"start\":43112},{\"end\":43129,\"start\":43123},{\"end\":43139,\"start\":43136},{\"end\":43148,\"start\":43146},{\"end\":43583,\"start\":43576},{\"end\":43600,\"start\":43594},{\"end\":43612,\"start\":43608},{\"end\":43623,\"start\":43619},{\"end\":43643,\"start\":43637},{\"end\":43645,\"start\":43644},{\"end\":44158,\"start\":44150},{\"end\":44176,\"start\":44169},{\"end\":44178,\"start\":44177},{\"end\":44192,\"start\":44185},{\"end\":44210,\"start\":44201},{\"end\":44212,\"start\":44211},{\"end\":44555,\"start\":44547},{\"end\":44566,\"start\":44560},{\"end\":44577,\"start\":44571},{\"end\":44586,\"start\":44582},{\"end\":44595,\"start\":44592},{\"end\":45272,\"start\":45266},{\"end\":45287,\"start\":45281},{\"end\":45672,\"start\":45666},{\"end\":45690,\"start\":45685},{\"end\":45705,\"start\":45700},{\"end\":46040,\"start\":46039},{\"end\":46050,\"start\":46049},{\"end\":46060,\"start\":46051},{\"end\":46300,\"start\":46296},{\"end\":46316,\"start\":46310},{\"end\":46327,\"start\":46324},{\"end\":46339,\"start\":46335},{\"end\":46358,\"start\":46350},{\"end\":46377,\"start\":46372},{\"end\":46379,\"start\":46378},{\"end\":46397,\"start\":46388},{\"end\":46415,\"start\":46408},{\"end\":46429,\"start\":46424},{\"end\":46845,\"start\":46839},{\"end\":46858,\"start\":46851},{\"end\":46870,\"start\":46864},{\"end\":46884,\"start\":46877},{\"end\":46895,\"start\":46890},{\"end\":46905,\"start\":46901},{\"end\":47414,\"start\":47408},{\"end\":47427,\"start\":47420},{\"end\":47440,\"start\":47433},{\"end\":47450,\"start\":47446},{\"end\":47460,\"start\":47456},{\"end\":47880,\"start\":47873},{\"end\":47892,\"start\":47886},{\"end\":47903,\"start\":47897},{\"end\":48286,\"start\":48279},{\"end\":48299,\"start\":48292},{\"end\":48311,\"start\":48305},{\"end\":48325,\"start\":48318},{\"end\":48766,\"start\":48759},{\"end\":48776,\"start\":48772},{\"end\":48786,\"start\":48783},{\"end\":48795,\"start\":48793},{\"end\":49257,\"start\":49249},{\"end\":49273,\"start\":49269},{\"end\":49290,\"start\":49286},{\"end\":49301,\"start\":49298},{\"end\":49322,\"start\":49311},{\"end\":49771,\"start\":49763},{\"end\":49789,\"start\":49783},{\"end\":49804,\"start\":49801},{\"end\":49827,\"start\":49818},{\"end\":50298,\"start\":50293},{\"end\":50313,\"start\":50307},{\"end\":50327,\"start\":50324},{\"end\":50644,\"start\":50638},{\"end\":50666,\"start\":50658},{\"end\":50689,\"start\":50673},{\"end\":51406,\"start\":51396},{\"end\":51420,\"start\":51415},{\"end\":51435,\"start\":51429},{\"end\":51450,\"start\":51443},{\"end\":51710,\"start\":51700},{\"end\":51726,\"start\":51719},{\"end\":51742,\"start\":51736},{\"end\":52147,\"start\":52137},{\"end\":52166,\"start\":52156},{\"end\":52616,\"start\":52613},{\"end\":52636,\"start\":52630},{\"end\":52653,\"start\":52644},{\"end\":53250,\"start\":53243},{\"end\":53272,\"start\":53266},{\"end\":53274,\"start\":53273},{\"end\":53286,\"start\":53281},{\"end\":53300,\"start\":53294},{\"end\":53314,\"start\":53310},{\"end\":53324,\"start\":53321},{\"end\":53672,\"start\":53667},{\"end\":53681,\"start\":53678},{\"end\":54074,\"start\":54067},{\"end\":54088,\"start\":54083},{\"end\":54106,\"start\":54095},{\"end\":54108,\"start\":54107},{\"end\":54124,\"start\":54118},{\"end\":54126,\"start\":54125},{\"end\":54450,\"start\":54446},{\"end\":54470,\"start\":54462},{\"end\":54487,\"start\":54478},{\"end\":54500,\"start\":54496},{\"end\":54520,\"start\":54511},{\"end\":54963,\"start\":54959},{\"end\":54978,\"start\":54970},{\"end\":54987,\"start\":54984},{\"end\":54996,\"start\":54994},{\"end\":55310,\"start\":55306},{\"end\":55320,\"start\":55317},{\"end\":55329,\"start\":55327},{\"end\":55746,\"start\":55742},{\"end\":55760,\"start\":55753},{\"end\":55775,\"start\":55768},{\"end\":55787,\"start\":55782},{\"end\":56194,\"start\":56187},{\"end\":56222,\"start\":56215},{\"end\":56224,\"start\":56223},{\"end\":56237,\"start\":56232},{\"end\":56711,\"start\":56708},{\"end\":56724,\"start\":56718},{\"end\":56739,\"start\":56732},{\"end\":57211,\"start\":57208},{\"end\":57224,\"start\":57218},{\"end\":57239,\"start\":57232},{\"end\":57664,\"start\":57657},{\"end\":57677,\"start\":57670},{\"end\":57686,\"start\":57683},{\"end\":57698,\"start\":57692},{\"end\":57712,\"start\":57705},{\"end\":58146,\"start\":58139},{\"end\":58159,\"start\":58152},{\"end\":58172,\"start\":58165},{\"end\":58625,\"start\":58619},{\"end\":58635,\"start\":58632},{\"end\":59110,\"start\":59104},{\"end\":59124,\"start\":59117},{\"end\":59138,\"start\":59130},{\"end\":59151,\"start\":59143},{\"end\":59159,\"start\":59157},{\"end\":59596,\"start\":59589},{\"end\":59611,\"start\":59604},{\"end\":59623,\"start\":59619},{\"end\":59633,\"start\":59630},{\"end\":59644,\"start\":59639}]", "bib_author_last_name": "[{\"end\":38071,\"start\":38062},{\"end\":38084,\"start\":38079},{\"end\":38102,\"start\":38094},{\"end\":38114,\"start\":38108},{\"end\":38128,\"start\":38122},{\"end\":38647,\"start\":38641},{\"end\":38662,\"start\":38656},{\"end\":38676,\"start\":38670},{\"end\":38691,\"start\":38685},{\"end\":38968,\"start\":38962},{\"end\":38985,\"start\":38978},{\"end\":39007,\"start\":38995},{\"end\":39021,\"start\":39015},{\"end\":39039,\"start\":39030},{\"end\":39387,\"start\":39381},{\"end\":39401,\"start\":39395},{\"end\":39418,\"start\":39409},{\"end\":39433,\"start\":39427},{\"end\":39847,\"start\":39843},{\"end\":39868,\"start\":39857},{\"end\":39882,\"start\":39877},{\"end\":39894,\"start\":39890},{\"end\":39902,\"start\":39899},{\"end\":39916,\"start\":39910},{\"end\":39934,\"start\":39925},{\"end\":39947,\"start\":39944},{\"end\":39958,\"start\":39953},{\"end\":40558,\"start\":40553},{\"end\":40570,\"start\":40565},{\"end\":40584,\"start\":40578},{\"end\":40870,\"start\":40865},{\"end\":40883,\"start\":40878},{\"end\":40902,\"start\":40894},{\"end\":40916,\"start\":40910},{\"end\":40932,\"start\":40924},{\"end\":41592,\"start\":41575},{\"end\":41613,\"start\":41604},{\"end\":41625,\"start\":41621},{\"end\":41644,\"start\":41636},{\"end\":41903,\"start\":41900},{\"end\":41914,\"start\":41910},{\"end\":41924,\"start\":41920},{\"end\":41937,\"start\":41933},{\"end\":41945,\"start\":41942},{\"end\":42609,\"start\":42606},{\"end\":42620,\"start\":42616},{\"end\":42633,\"start\":42629},{\"end\":42643,\"start\":42639},{\"end\":42651,\"start\":42648},{\"end\":43110,\"start\":43107},{\"end\":43121,\"start\":43117},{\"end\":43134,\"start\":43130},{\"end\":43144,\"start\":43140},{\"end\":43152,\"start\":43149},{\"end\":43592,\"start\":43584},{\"end\":43606,\"start\":43601},{\"end\":43617,\"start\":43613},{\"end\":43635,\"start\":43624},{\"end\":43650,\"start\":43646},{\"end\":44167,\"start\":44159},{\"end\":44183,\"start\":44179},{\"end\":44199,\"start\":44193},{\"end\":44222,\"start\":44213},{\"end\":44558,\"start\":44556},{\"end\":44569,\"start\":44567},{\"end\":44580,\"start\":44578},{\"end\":44590,\"start\":44587},{\"end\":44600,\"start\":44596},{\"end\":45279,\"start\":45273},{\"end\":45294,\"start\":45288},{\"end\":45683,\"start\":45673},{\"end\":45698,\"start\":45691},{\"end\":45712,\"start\":45706},{\"end\":46047,\"start\":46041},{\"end\":46064,\"start\":46061},{\"end\":46071,\"start\":46066},{\"end\":46308,\"start\":46301},{\"end\":46322,\"start\":46317},{\"end\":46333,\"start\":46328},{\"end\":46348,\"start\":46340},{\"end\":46370,\"start\":46359},{\"end\":46386,\"start\":46380},{\"end\":46406,\"start\":46398},{\"end\":46422,\"start\":46416},{\"end\":46447,\"start\":46430},{\"end\":46453,\"start\":46449},{\"end\":46849,\"start\":46846},{\"end\":46862,\"start\":46859},{\"end\":46875,\"start\":46871},{\"end\":46888,\"start\":46885},{\"end\":46899,\"start\":46896},{\"end\":46909,\"start\":46906},{\"end\":47418,\"start\":47415},{\"end\":47431,\"start\":47428},{\"end\":47444,\"start\":47441},{\"end\":47454,\"start\":47451},{\"end\":47464,\"start\":47461},{\"end\":47884,\"start\":47881},{\"end\":47895,\"start\":47893},{\"end\":47908,\"start\":47904},{\"end\":48290,\"start\":48287},{\"end\":48303,\"start\":48300},{\"end\":48316,\"start\":48312},{\"end\":48329,\"start\":48326},{\"end\":48770,\"start\":48767},{\"end\":48781,\"start\":48777},{\"end\":48791,\"start\":48787},{\"end\":48799,\"start\":48796},{\"end\":49267,\"start\":49258},{\"end\":49284,\"start\":49274},{\"end\":49296,\"start\":49291},{\"end\":49309,\"start\":49302},{\"end\":49336,\"start\":49323},{\"end\":49781,\"start\":49772},{\"end\":49799,\"start\":49790},{\"end\":49816,\"start\":49805},{\"end\":49834,\"start\":49828},{\"end\":50305,\"start\":50299},{\"end\":50322,\"start\":50314},{\"end\":50336,\"start\":50328},{\"end\":50656,\"start\":50645},{\"end\":50671,\"start\":50667},{\"end\":51413,\"start\":51407},{\"end\":51427,\"start\":51421},{\"end\":51441,\"start\":51436},{\"end\":51462,\"start\":51451},{\"end\":51717,\"start\":51711},{\"end\":51734,\"start\":51727},{\"end\":51749,\"start\":51743},{\"end\":52154,\"start\":52148},{\"end\":52179,\"start\":52167},{\"end\":52188,\"start\":52181},{\"end\":52628,\"start\":52617},{\"end\":52642,\"start\":52637},{\"end\":52660,\"start\":52654},{\"end\":53264,\"start\":53251},{\"end\":53279,\"start\":53275},{\"end\":53292,\"start\":53287},{\"end\":53308,\"start\":53301},{\"end\":53319,\"start\":53315},{\"end\":53330,\"start\":53325},{\"end\":53339,\"start\":53332},{\"end\":53676,\"start\":53673},{\"end\":53690,\"start\":53682},{\"end\":54081,\"start\":54075},{\"end\":54093,\"start\":54089},{\"end\":54116,\"start\":54109},{\"end\":54129,\"start\":54127},{\"end\":54460,\"start\":54451},{\"end\":54476,\"start\":54471},{\"end\":54494,\"start\":54488},{\"end\":54509,\"start\":54501},{\"end\":54529,\"start\":54521},{\"end\":54968,\"start\":54964},{\"end\":54982,\"start\":54979},{\"end\":54992,\"start\":54988},{\"end\":55000,\"start\":54997},{\"end\":55315,\"start\":55311},{\"end\":55325,\"start\":55321},{\"end\":55333,\"start\":55330},{\"end\":55751,\"start\":55747},{\"end\":55766,\"start\":55761},{\"end\":55780,\"start\":55776},{\"end\":55792,\"start\":55788},{\"end\":56213,\"start\":56195},{\"end\":56230,\"start\":56225},{\"end\":56244,\"start\":56238},{\"end\":56716,\"start\":56712},{\"end\":56730,\"start\":56725},{\"end\":56743,\"start\":56740},{\"end\":57216,\"start\":57212},{\"end\":57230,\"start\":57225},{\"end\":57243,\"start\":57240},{\"end\":57668,\"start\":57665},{\"end\":57681,\"start\":57678},{\"end\":57690,\"start\":57687},{\"end\":57703,\"start\":57699},{\"end\":57716,\"start\":57713},{\"end\":58150,\"start\":58147},{\"end\":58163,\"start\":58160},{\"end\":58176,\"start\":58173},{\"end\":58630,\"start\":58626},{\"end\":58644,\"start\":58636},{\"end\":59115,\"start\":59111},{\"end\":59128,\"start\":59125},{\"end\":59141,\"start\":59139},{\"end\":59155,\"start\":59152},{\"end\":59164,\"start\":59160},{\"end\":59602,\"start\":59597},{\"end\":59617,\"start\":59612},{\"end\":59628,\"start\":59624},{\"end\":59637,\"start\":59634},{\"end\":59649,\"start\":59645}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207167677},\"end\":38554,\"start\":37973},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9095914},\"end\":38894,\"start\":38556},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14941970},\"end\":39320,\"start\":38896},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":715463},\"end\":39764,\"start\":39322},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4557963},\"end\":40468,\"start\":39766},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":538820},\"end\":40807,\"start\":40470},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3095388},\"end\":41508,\"start\":40809},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207032678},\"end\":41847,\"start\":41510},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":205692},\"end\":42546,\"start\":41849},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7958862},\"end\":43034,\"start\":42548},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8535487},\"end\":43488,\"start\":43036},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16483125},\"end\":44092,\"start\":43490},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10854724},\"end\":44491,\"start\":44094},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11202498},\"end\":45214,\"start\":44493},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7557552},\"end\":45607,\"start\":45216},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16470003},\"end\":45969,\"start\":45609},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4428232},\"end\":46217,\"start\":45971},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1181640},\"end\":46765,\"start\":46219},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1969092},\"end\":47334,\"start\":46767},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2949428},\"end\":47817,\"start\":47336},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":19370455},\"end\":48227,\"start\":47819},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10705753},\"end\":48712,\"start\":48229},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14047545},\"end\":49169,\"start\":48714},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46129446},\"end\":49703,\"start\":49171},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3194306},\"end\":50201,\"start\":49705},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8348149},\"end\":50571,\"start\":50203},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":11171811},\"end\":51312,\"start\":50573},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12161567},\"end\":51654,\"start\":51314},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6071257},\"end\":52067,\"start\":51656},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1157792},\"end\":52531,\"start\":52069},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6488690},\"end\":53241,\"start\":52533},{\"attributes\":{\"doi\":\"arXiv:1703.06103\",\"id\":\"b31\"},\"end\":53605,\"start\":53243},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":18367155},\"end\":53996,\"start\":53607},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8429835},\"end\":54397,\"start\":53998},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15150247},\"end\":54889,\"start\":54399},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":19135805},\"end\":55250,\"start\":54891},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3143712},\"end\":55683,\"start\":55252},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15027084},\"end\":56121,\"start\":55685},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1598651},\"end\":56648,\"start\":56123},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11091552},\"end\":57121,\"start\":56650},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3150829},\"end\":57585,\"start\":57123},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":31606602},\"end\":58068,\"start\":57587},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1743664},\"end\":58550,\"start\":58070},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":19968363},\"end\":59022,\"start\":58552},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2768038},\"end\":59524,\"start\":59024},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":5440379},\"end\":60052,\"start\":59526}]", "bib_title": "[{\"end\":38055,\"start\":37973},{\"end\":38631,\"start\":38556},{\"end\":38952,\"start\":38896},{\"end\":39371,\"start\":39322},{\"end\":39837,\"start\":39766},{\"end\":40546,\"start\":40470},{\"end\":40857,\"start\":40809},{\"end\":41568,\"start\":41510},{\"end\":41894,\"start\":41849},{\"end\":42600,\"start\":42548},{\"end\":43101,\"start\":43036},{\"end\":43574,\"start\":43490},{\"end\":44148,\"start\":44094},{\"end\":44545,\"start\":44493},{\"end\":45264,\"start\":45216},{\"end\":45664,\"start\":45609},{\"end\":46037,\"start\":45971},{\"end\":46294,\"start\":46219},{\"end\":46837,\"start\":46767},{\"end\":47406,\"start\":47336},{\"end\":47871,\"start\":47819},{\"end\":48277,\"start\":48229},{\"end\":48757,\"start\":48714},{\"end\":49247,\"start\":49171},{\"end\":49761,\"start\":49705},{\"end\":50291,\"start\":50203},{\"end\":50636,\"start\":50573},{\"end\":51394,\"start\":51314},{\"end\":51698,\"start\":51656},{\"end\":52135,\"start\":52069},{\"end\":52611,\"start\":52533},{\"end\":53665,\"start\":53607},{\"end\":54065,\"start\":53998},{\"end\":54444,\"start\":54399},{\"end\":54957,\"start\":54891},{\"end\":55304,\"start\":55252},{\"end\":55740,\"start\":55685},{\"end\":56185,\"start\":56123},{\"end\":56706,\"start\":56650},{\"end\":57206,\"start\":57123},{\"end\":57655,\"start\":57587},{\"end\":58137,\"start\":58070},{\"end\":58617,\"start\":58552},{\"end\":59102,\"start\":59024},{\"end\":59587,\"start\":59526}]", "bib_author": "[{\"end\":38073,\"start\":38057},{\"end\":38086,\"start\":38073},{\"end\":38104,\"start\":38086},{\"end\":38116,\"start\":38104},{\"end\":38130,\"start\":38116},{\"end\":38649,\"start\":38633},{\"end\":38664,\"start\":38649},{\"end\":38678,\"start\":38664},{\"end\":38693,\"start\":38678},{\"end\":38970,\"start\":38954},{\"end\":38987,\"start\":38970},{\"end\":39009,\"start\":38987},{\"end\":39023,\"start\":39009},{\"end\":39041,\"start\":39023},{\"end\":39389,\"start\":39373},{\"end\":39403,\"start\":39389},{\"end\":39420,\"start\":39403},{\"end\":39435,\"start\":39420},{\"end\":39849,\"start\":39839},{\"end\":39870,\"start\":39849},{\"end\":39884,\"start\":39870},{\"end\":39896,\"start\":39884},{\"end\":39904,\"start\":39896},{\"end\":39918,\"start\":39904},{\"end\":39936,\"start\":39918},{\"end\":39949,\"start\":39936},{\"end\":39960,\"start\":39949},{\"end\":40560,\"start\":40548},{\"end\":40572,\"start\":40560},{\"end\":40586,\"start\":40572},{\"end\":40872,\"start\":40859},{\"end\":40885,\"start\":40872},{\"end\":40904,\"start\":40885},{\"end\":40918,\"start\":40904},{\"end\":40934,\"start\":40918},{\"end\":41594,\"start\":41570},{\"end\":41615,\"start\":41594},{\"end\":41627,\"start\":41615},{\"end\":41646,\"start\":41627},{\"end\":41905,\"start\":41896},{\"end\":41916,\"start\":41905},{\"end\":41926,\"start\":41916},{\"end\":41939,\"start\":41926},{\"end\":41947,\"start\":41939},{\"end\":42611,\"start\":42602},{\"end\":42622,\"start\":42611},{\"end\":42635,\"start\":42622},{\"end\":42645,\"start\":42635},{\"end\":42653,\"start\":42645},{\"end\":43112,\"start\":43103},{\"end\":43123,\"start\":43112},{\"end\":43136,\"start\":43123},{\"end\":43146,\"start\":43136},{\"end\":43154,\"start\":43146},{\"end\":43594,\"start\":43576},{\"end\":43608,\"start\":43594},{\"end\":43619,\"start\":43608},{\"end\":43637,\"start\":43619},{\"end\":43652,\"start\":43637},{\"end\":44169,\"start\":44150},{\"end\":44185,\"start\":44169},{\"end\":44201,\"start\":44185},{\"end\":44224,\"start\":44201},{\"end\":44560,\"start\":44547},{\"end\":44571,\"start\":44560},{\"end\":44582,\"start\":44571},{\"end\":44592,\"start\":44582},{\"end\":44602,\"start\":44592},{\"end\":45281,\"start\":45266},{\"end\":45296,\"start\":45281},{\"end\":45685,\"start\":45666},{\"end\":45700,\"start\":45685},{\"end\":45714,\"start\":45700},{\"end\":46049,\"start\":46039},{\"end\":46066,\"start\":46049},{\"end\":46073,\"start\":46066},{\"end\":46310,\"start\":46296},{\"end\":46324,\"start\":46310},{\"end\":46335,\"start\":46324},{\"end\":46350,\"start\":46335},{\"end\":46372,\"start\":46350},{\"end\":46388,\"start\":46372},{\"end\":46408,\"start\":46388},{\"end\":46424,\"start\":46408},{\"end\":46449,\"start\":46424},{\"end\":46455,\"start\":46449},{\"end\":46851,\"start\":46839},{\"end\":46864,\"start\":46851},{\"end\":46877,\"start\":46864},{\"end\":46890,\"start\":46877},{\"end\":46901,\"start\":46890},{\"end\":46911,\"start\":46901},{\"end\":47420,\"start\":47408},{\"end\":47433,\"start\":47420},{\"end\":47446,\"start\":47433},{\"end\":47456,\"start\":47446},{\"end\":47466,\"start\":47456},{\"end\":47886,\"start\":47873},{\"end\":47897,\"start\":47886},{\"end\":47910,\"start\":47897},{\"end\":48292,\"start\":48279},{\"end\":48305,\"start\":48292},{\"end\":48318,\"start\":48305},{\"end\":48331,\"start\":48318},{\"end\":48772,\"start\":48759},{\"end\":48783,\"start\":48772},{\"end\":48793,\"start\":48783},{\"end\":48801,\"start\":48793},{\"end\":49269,\"start\":49249},{\"end\":49286,\"start\":49269},{\"end\":49298,\"start\":49286},{\"end\":49311,\"start\":49298},{\"end\":49338,\"start\":49311},{\"end\":49783,\"start\":49763},{\"end\":49801,\"start\":49783},{\"end\":49818,\"start\":49801},{\"end\":49836,\"start\":49818},{\"end\":50307,\"start\":50293},{\"end\":50324,\"start\":50307},{\"end\":50338,\"start\":50324},{\"end\":50658,\"start\":50638},{\"end\":50673,\"start\":50658},{\"end\":50692,\"start\":50673},{\"end\":51415,\"start\":51396},{\"end\":51429,\"start\":51415},{\"end\":51443,\"start\":51429},{\"end\":51464,\"start\":51443},{\"end\":51719,\"start\":51700},{\"end\":51736,\"start\":51719},{\"end\":51751,\"start\":51736},{\"end\":52156,\"start\":52137},{\"end\":52181,\"start\":52156},{\"end\":52190,\"start\":52181},{\"end\":52630,\"start\":52613},{\"end\":52644,\"start\":52630},{\"end\":52662,\"start\":52644},{\"end\":53266,\"start\":53243},{\"end\":53281,\"start\":53266},{\"end\":53294,\"start\":53281},{\"end\":53310,\"start\":53294},{\"end\":53321,\"start\":53310},{\"end\":53332,\"start\":53321},{\"end\":53341,\"start\":53332},{\"end\":53678,\"start\":53667},{\"end\":53692,\"start\":53678},{\"end\":54083,\"start\":54067},{\"end\":54095,\"start\":54083},{\"end\":54118,\"start\":54095},{\"end\":54131,\"start\":54118},{\"end\":54462,\"start\":54446},{\"end\":54478,\"start\":54462},{\"end\":54496,\"start\":54478},{\"end\":54511,\"start\":54496},{\"end\":54531,\"start\":54511},{\"end\":54970,\"start\":54959},{\"end\":54984,\"start\":54970},{\"end\":54994,\"start\":54984},{\"end\":55002,\"start\":54994},{\"end\":55317,\"start\":55306},{\"end\":55327,\"start\":55317},{\"end\":55335,\"start\":55327},{\"end\":55753,\"start\":55742},{\"end\":55768,\"start\":55753},{\"end\":55782,\"start\":55768},{\"end\":55794,\"start\":55782},{\"end\":56215,\"start\":56187},{\"end\":56232,\"start\":56215},{\"end\":56246,\"start\":56232},{\"end\":56718,\"start\":56708},{\"end\":56732,\"start\":56718},{\"end\":56745,\"start\":56732},{\"end\":57218,\"start\":57208},{\"end\":57232,\"start\":57218},{\"end\":57245,\"start\":57232},{\"end\":57670,\"start\":57657},{\"end\":57683,\"start\":57670},{\"end\":57692,\"start\":57683},{\"end\":57705,\"start\":57692},{\"end\":57718,\"start\":57705},{\"end\":58152,\"start\":58139},{\"end\":58165,\"start\":58152},{\"end\":58178,\"start\":58165},{\"end\":58632,\"start\":58619},{\"end\":58646,\"start\":58632},{\"end\":59117,\"start\":59104},{\"end\":59130,\"start\":59117},{\"end\":59143,\"start\":59130},{\"end\":59157,\"start\":59143},{\"end\":59166,\"start\":59157},{\"end\":59604,\"start\":59589},{\"end\":59619,\"start\":59604},{\"end\":59630,\"start\":59619},{\"end\":59639,\"start\":59630},{\"end\":59651,\"start\":59639}]", "bib_venue": "[{\"end\":38279,\"start\":38213},{\"end\":39554,\"start\":39503},{\"end\":40143,\"start\":40060},{\"end\":41205,\"start\":41078},{\"end\":42256,\"start\":42110},{\"end\":42812,\"start\":42741},{\"end\":43273,\"start\":43222},{\"end\":43813,\"start\":43741},{\"end\":44911,\"start\":44765},{\"end\":45415,\"start\":45364},{\"end\":47070,\"start\":46999},{\"end\":47585,\"start\":47534},{\"end\":48033,\"start\":47980},{\"end\":48490,\"start\":48419},{\"end\":48960,\"start\":48889},{\"end\":49975,\"start\":49914},{\"end\":50377,\"start\":50366},{\"end\":51001,\"start\":50855},{\"end\":51870,\"start\":51819},{\"end\":52313,\"start\":52260},{\"end\":52933,\"start\":52806},{\"end\":53811,\"start\":53760},{\"end\":54654,\"start\":54601},{\"end\":55484,\"start\":55418},{\"end\":55913,\"start\":55862},{\"end\":56405,\"start\":56334},{\"end\":56906,\"start\":56834},{\"end\":57364,\"start\":57313},{\"end\":57837,\"start\":57786},{\"end\":58327,\"start\":58261},{\"end\":58807,\"start\":58735},{\"end\":59295,\"start\":59239},{\"end\":59810,\"start\":59739},{\"end\":38211,\"start\":38130},{\"end\":38709,\"start\":38693},{\"end\":39090,\"start\":39041},{\"end\":39501,\"start\":39435},{\"end\":40058,\"start\":39960},{\"end\":40622,\"start\":40586},{\"end\":41076,\"start\":40934},{\"end\":41662,\"start\":41646},{\"end\":42108,\"start\":41947},{\"end\":42739,\"start\":42653},{\"end\":43220,\"start\":43154},{\"end\":43739,\"start\":43652},{\"end\":44273,\"start\":44224},{\"end\":44763,\"start\":44602},{\"end\":45362,\"start\":45296},{\"end\":45775,\"start\":45714},{\"end\":46079,\"start\":46073},{\"end\":46475,\"start\":46455},{\"end\":46997,\"start\":46911},{\"end\":47532,\"start\":47466},{\"end\":47978,\"start\":47910},{\"end\":48417,\"start\":48331},{\"end\":48887,\"start\":48801},{\"end\":49420,\"start\":49338},{\"end\":49912,\"start\":49836},{\"end\":50364,\"start\":50338},{\"end\":50853,\"start\":50692},{\"end\":51468,\"start\":51464},{\"end\":51817,\"start\":51751},{\"end\":52258,\"start\":52190},{\"end\":52804,\"start\":52662},{\"end\":53415,\"start\":53357},{\"end\":53758,\"start\":53692},{\"end\":54180,\"start\":54131},{\"end\":54599,\"start\":54531},{\"end\":55053,\"start\":55002},{\"end\":55416,\"start\":55335},{\"end\":55860,\"start\":55794},{\"end\":56332,\"start\":56246},{\"end\":56832,\"start\":56745},{\"end\":57311,\"start\":57245},{\"end\":57784,\"start\":57718},{\"end\":58259,\"start\":58178},{\"end\":58733,\"start\":58646},{\"end\":59237,\"start\":59166},{\"end\":59737,\"start\":59651}]"}}}, "year": 2023, "month": 12, "day": 17}