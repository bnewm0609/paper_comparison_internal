{"id": 247315430, "updated": "2023-10-06 01:50:00.76", "metadata": {"title": "Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text", "authors": "[{\"first\":\"Yao\",\"last\":\"Dou\",\"middle\":[]},{\"first\":\"Maxwell\",\"last\":\"Forbes\",\"middle\":[]},{\"first\":\"Rik\",\"last\":\"Koncel-Kedziorski\",\"middle\":[]},{\"first\":\"Noah\",\"last\":\"Smith\",\"middle\":[]},{\"first\":\"Yejin\",\"last\":\"Choi\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow\u2014such as redundancy, commonsense errors, and incoherence\u2014are identified through several rounds of crowd annotation experiments without a predefined ontology.We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2107.01294", "mag": null, "acl": "2022.acl-long.501", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/DouFKSC22", "doi": "10.18653/v1/2022.acl-long.501"}}, "content": {"source": {"pdf_hash": "19d8c0f9cb115bf2b2405c3a193de67d12cc4a02", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.acl-long.501.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "781e50c88f5b18ff36714e1e0b0567503624f56a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/19d8c0f9cb115bf2b2405c3a193de67d12cc4a02.txt", "contents": "\nIs GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsMay 22-27, 2022 c 2022\n\nYao Dou douy@cs.washington.edu \nSchool of Computer Science & Engineering\nUniversity of Washington\nAllen Institute for AI\n\n\nMaxwell Forbes mbforbes@cs.washington.edu \nSchool of Computer Science & Engineering\nUniversity of Washington\nAllen Institute for AI\n\n\nRik Koncel-Kedziorski \nSchool of Computer Science & Engineering\nUniversity of Washington\nAllen Institute for AI\n\n\nNoah A Smith nasmith@cs.washington.edu \nSchool of Computer Science & Engineering\nUniversity of Washington\nAllen Institute for AI\n\n\nYejin Choi \nSchool of Computer Science & Engineering\nUniversity of Washington\nAllen Institute for AI\n\n\nPaul G Allen \nSchool of Computer Science & Engineering\nUniversity of Washington\nAllen Institute for AI\n\n\nIs GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text\n\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nthe 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1May 22-27, 2022 c 2022\nModern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al.  (2021)  has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and humanauthored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation.We propose a new framework called SCARE-CROW for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of SCARECROWsuch as redundancy , commonsense errors , and incoherence -are identified through several rounds of crowd annotation experiments without a predefined ontology.\n\nWe then use SCARECROW to collect over 41k error spans in human-written and machinegenerated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https: //yao-dou.github.io/scarecrow/. chine text and human text, but reveal the distributions of specific categories of issues, and pinpoint their occurrences in text written by several sizes of language models as well as humans.\n\nTo achieve this, we develop SCARECROW, a methodology for eliciting categorical judgements of errors in machine-generated text from crowd workers. One goal in natural language generation (NLG) is to produce fluent outputs which can be read by laypeople. As such, we propose that important errors to address are those which are recognized by readers without NLP expertise. Our framework allows crowd workers to annotate problems in model outputs at the span level. A single such annotation is shown in Figure 1.\n\nTo make this possible, we establish a categorization of shortcomings commonly found in machine generated text (Table 1). This error schema covers a broad scope of problems as identified by experts, but has been honed according to what is salient to non-expert readers through several pilot rounds of crowd annotation without a fixed label set. The result is a framework that is usable by everyday people with minimal training, but covers the error phenomena found in real machine-generated text. Labeling spans of text using specific error types creates a picture of contemporary model generations with an unprecedented level of detail. In contrast to judging text holistically (Celikyilmaz et al., 2021), insights from this method are specific and practical, as it measures exactly how and where problems arise.\n\nWe conduct a large-scale analysis of humanwritten and machine-generated text using SCARE-CROW, collecting 13k annotations of 1.3k paragraphs, amassing 41k spans labeled with error type, severity, and an explanation. Through this, we characterize in which ways GPT-3's generations are better than those of previous models, and which aspects do not improve with increased data and parameters. We also provide a rigorous error analysis of text generated by several other contemporary language models, examining the impact of model size, training data, and decoding strategy.\n\nWe provide our detailed annotator training system and task interface so that future researchers may employ and refine them for error analyses of machine-generated text. We hope this will contribute to the standardization of NLG human evaluation (Howcroft et al., 2020).\n\n\nKey Findings\n\nWe perform a large-scale annotation of errors in English news text generated by five sources (four 2 G P T-2 S G P T-2 X L G ro ve r G P T-3 H um an Technical Jargon Figure 2: Average portion of tokens annotated with each error type (y-axis) across models (x-axis), with 95% confidence intervals. We group the trends into several broad categories. Decreasing: fine-tuning and increasing model size improves performance.\n\nModel plateau: increasing model size to GPT-3 does not correlate with further improvements.\n\nRising and falling: errors become more prevalent with some models, then improve. Humans highest: these spans are labeled most on human-authored text; both are reader issues (distinct from errors; see Table 1). Details: all models, including GPT-3, use the same \"apples-to-apples\" decoding hyperparameters: top-p=0.96, temperature=1, and no frequency penalty. models and ground truth articles). We present Figures 2, 3, and 4 as summaries of our main results. As a reminder to readers, Grover (Zellers et al., 2019) is the same model size and architecture as GPT-2 XL (Radford et al., 2019), but trained indomain (on news text). As such, our results cover three increasing model sizes (GPT-2 Small, XL, and GPT-3 (Brown et al., 2020)), one change in domain (Grover), and ground-truth text (Human). For GPT-3, we also study a variety of decoding configurations ( Figure 4).\n\nThe main quantity we measure (on y-axes) is span coverage, which is the average portion of tokens that ends up covered by annotations of a particular error type. Since it is possible that multiple spans nest or overlap, there is no upper bound for this quantity. (See Figure 12 for a comparison of span coverage with other measurement alternatives.) Figure 2 measures span coverage for each type of span separately, Figure 3 stacks them, and Figure 4 removes non-error spans (reader issues) before adding them (as in Figure 3, but without showing the individual types).\n\nThe following are our key findings.\n\n1. Scaling pays off to improve Encyclopedic , Commonsense , and Incoherent errors (Fig.  2). These error categories decrease with in-domain training (Grover) and larger model size . Human text still shows the fewest of these kinds of errors.\n\n2. Scaling benefits plateau for Off-Prompt , Bad Math , and Grammar and Usage errors (Fig. 2). These three error categories see a model plateau in error reduction when scaling to GPT-3. Of these error types, humans still commit fewer Off-Prompt (more: \u00a7E.1) and Grammar and Usage errors, but Bad Math appears saturated for our domain.\n\n3. Self-Contradiction and Redundant errors exhibit more complex scaling behavior (Fig. 2). We roughly categorize these trends as rising and falling: increasing for medium or large-scale models, but dropping for human-authored text. Text generated by GPT-2 Small is so often incoherent that there is little possibility for Self-Contradiction (more: \u00a7E.2), and the increase in Redundant errors varies based on how errors are counted (more: \u00a7E.3).\n\n4. Human-authored text produces the most reader issues (Figs. 2 and 3). The Needs Google and Technical Jargon span categories both have a humans highest trend, and both fall under reader issues: problems that are not necessarily errors, but that still prevent full comprehension 3 GPT-2 S GPT-2 XL Grover-Mega GPT-3  Figure 3: Average portion of tokens covered by span annotations, broken down by error type. All models, including GPT-3, use the same apples-to-apples decoding hyperparameters: top-p=0.96, temperature=1, and no frequency penalty. We scale each span by its token length, normalize by generation token lengths, and remove severity-1 Grammar and Usage errors (see \u00a7C).  Figure 4: Taking the average span coverage ( Figure 3) and removing reader issues ( Technical Jargon and Needs Google ), we plot values and 95% confidence intervals for all models, including all decoding hyperparameters we tested for GPT-3. We find a surprisingly large change in annotated errors depending on the decoding setting used.\n\nor factual verification of the text (more: \u00a7E.4). Furthermore, human-authored text is not free from error annotations (Figure 3). This can serve either as a control for baseline error rates (more: \u00a7E.6), or as a mechanism for critiquing human writing.\n\n5. Decoding hyperparameters have a huge im-pact ( Figure 4). For the previous findings, we fix the sampling configuration for all models to an apples-to-apples setup for fair comparison: top-p = 0.96, (softmax) temperature = 1, and no frequency penalty (i.e., word repetition penalty; defined precisely in \u00a75.2, Equation 1). To study the effects of these decoding settings, we annotate text generated by GPT-3 using a variety of values for top-p and temperature, both with and without a frequency penalty.\n\nTo our surprise, the decoding hyperparameters considerably affected error rates (more: \u00a7E.5). As seen in Figure 4, the worst sampling procedure for GPT-3 (argmax sampling with no frequency penalty) performed even worse than GPT-2 XL. But the best sampling procedure (surprisingly, also argmax sampling, but with a frequency penalty) produced text with as few apparent SCARECROW error spans as those authored by humans (more: \u00a7E.6).\n\nAll of these findings are discussed in more detail in Appendix E.\n\n\nEvaluation of Natural Language Generation\n\nWe make our study in the area of open-ended natural language generation, a loose term for generating longer texts with an increased level of creative freedom. The common factor in all open-ended generation tasks such as story, blog, and dialog generation is the wide and diverse nature of target outputs. Lexically and even semantically dissimilar responses to the same prompt could be equally valid. For example, a model prompted with the blog title \"Recipes for success this Holiday season\" could describe how to roast a turkey or strategies for dealing with the stresses of holiday travel. This allowable variation poses a particular difficulty for the evaluation of generation systems.\n\nTraditionally, text generation quality for tasks like machine translation or graph-to-text generation has been measured by word overlap with humanauthored references (Papineni et al., 2002;Lin, 2004). Though measures like BLEU allow for multiple references, they break down when the space of allowable outputs is large, as in open-ended generation. Recently introduced metrics seek to remedy this problem (Hashimoto et al., 2019;Pillutla et al., 2021), but the gold standard for evaluating generated text is still human judgment.\n\nHowever, current approaches to eliciting human 4 judgement of generated text often do not provide detailed insight into where models are making progress, where they are failing, and the scope of these failures. A/B-style testing allows for directly comparing one system against others (Clark and Smith, 2021), but can only express relative improvements. Simple Likert scale judgements can assess text quality, but do not explain why a generated text receives a given rating, or which segment of the text is problematic. Insights into model failures often come instead from a small scale expert analysis of outputs. However, these \"error analyses,\" once a staple of NLP research, have become less common in recent years, perhaps due to their small size and high variance.\n\nA hypothesis of the current work is that a well designed error analysis annotation framework could be used by crowdworkers to annotate large amounts of text, thereby providing detailed information about model progress and failures as well as actionable directions for future research. Such a framework would be easy to learn, reusable, and independent of particular models or experimental conditions. In what follows, we outline the details of such a method.\n\n\nSCARECROW Annotation Methodology\n\nThis section describes the high-level annotation methodology for SCARECROW.\n\n\nPrompt and Generation\n\nOur annotations consider two segments of text: a one-sentence prompt, and a one-paragraph generation. The prompt is human-written. It provides both starting tokens for model generation, as well as context for humans to evaluate whether a model is able to stay on-prompt-both topically and factually. Annotators know that the prompt is written by a human.\n\nThe generation is either text sampled from a language model, or the human-authored continuation to the prompt. Annotators, who do not know whether the generation came from a model or humans, assess this text. A paragraph length (80-145 tokens) is chosen to balance expressiveness with scope. For expressiveness, models must be given a sufficient number of tokens to express their capabilities lexically, syntactically, and semantically. One paragraph allows for significantly more variation than a single sentence. On the other hand, assessing multiple paragraphs is challenging, both Inconsistent about how many moons Mars has. 1 2 3\n\n\nSelf-Contradiction\n\nInconsistent about how many moons Mars has.\n\n\nNeeds\n\n\nGoogle Bad Math\n\nReader Issues Factual Language Figure 5: SCARECROW interface for annotating a single span: (1) highlighting a span (and later, an antecedent); (2) completing the annotation, with the error type, explanation, and severity; (3) the error annotation is saved-interactive controls allow detailed viewing and editing of spans (not shown).\n\nas a crowdsourcing task itself, and because it broadens the kinds of errors to include larger narrative scope. We leave extensions of SCARECROW to longer narrative lengths for future work.\n\n\nSpan Labeling\n\nAnnotators select spans that contain problems in the generation. The spans are automatically snapped to word boundaries. We choose spans to balance specificity (i.e., vs. simply commenting on the text as a whole) with ease of use (vs. imposing a more structured annotation schema).\n\n\nSpan Selection\n\nWe instruct workers to select the smallest spanminimally a single word-that contains an issue. Sometimes this involves an entire phrase, sentence, 5 or multiple sentences. We aim for specificity because during aggregation, it is possible to \"back off\" annotations to larger spans, but not the inverse. Once they select a span, workers (1) label the error type, (2) choose a severity level, and (3) explain their reasoning behind the error. Workers use the annotation interface shown in Figure 5 to mark a span with these three steps. We describe each step in greater detail in the next three sections.\n\n\nError Types\n\nEach selected span is labeled with exactly one error type. Multiple errors may be marked with partially or fully overlapping spans in the case that one text segment contains multiple problems.\n\nWe chose ten error types to balance three criteria: linguistic analysis, observed errors in generated text, and capabilities of everyday people with one to two hours of training. 1 We developed the schema by starting with the first two criteria (linguistic analysis and observed errors), and refining it over several pilot annotation studies, with 30 crowd workers performing 750 total annotations of 60 paragraphs before beginning data collection.\n\nWe broadly group the errors into three categories: language errors, factual errors, and reader issues. Language errors are issues with internal and external structure of text: which ideas are expressed, and whether they are expressed coherently and consistently. Factual errors denote that the information presented is known to be incorrect. Reader issues, on the other hand, are cases where the text is too technical or obscure to assess its factuality. Hence, reader issues are not errors, per se, but regions where a reader would need assistance outside of the text itself for comprehension.\n\nWe present the ten error types in Table 1 (several pages back). Appendix A provides more details, examples, and explanations for all error types.\n\n\nSeverity\n\nErrors naturally vary in how jarring they are to a reader. We define three error severity levels, and ask annotators to pick one for each error.\n\nThe severity levels are as follows.\n\n(1) Almost no impact on quality; just a small problem. (2) Understandable, but difficult; what's written is still comprehensible, but there's clearly an issue.\n\n(3) Very difficult to understand; the error almost completely ruins the text. 1 The complete training material is available for download.\n\nWe provide examples of each severity in Appendix B.1. In this paper, we omit an analysis of the severity labels (except for an illustration in Figure 12), but include it in our data release for future work to explore.\n\n\nExplanation\n\nFinally, we ask annotators to explain their reasoning behind each error in natural language. We provide example explanations during training, but do not impose strict guidelines. This paper primarily focuses on quantitative error analysis, but we anticipate the error explanations may warrant future investigation.\n\n\nAnnotation Process\n\nWe use Amazon Mechanical Turk (AMT) for all data collection.\n\nTraining We first pay each worker $40 to take an extensive qualification task, which both trains them in the span categorization scheme and quizzes their understanding. We pass workers if they score \u2265 90 points out of 100 points (details in Appendix B.2).\n\nAnnotation Workers annotate each paragraph using a custom annotation interface (shown partially in Figure 5), for which we pay $3.50. We calculated $3.50 per annotation by aiming to pay workers at least $15/hour. After several annotation rounds, we observed considerable variation in time per annotation, 2 so this cost should not be necessarily seen as a requirement for SCARECROW annotations.\n\n\nData Collection\n\nWe collect 13k human annotations of 1.3k paragraphs using SCARECROW, resulting in over 41k spans.\n\n\nModels\n\nWe consider four model configurations to test recent state-of-the-art transformer-based (Vaswani et al., 2017) models. Grover-Mega (Zellers et al., 2019) The 1.5B parameter variant of Grover, a model with the same architecture and parameter count of GPT-2, trained on news articles and their metadata. GPT-3 DaVinci (Brown et al., 2020) The 175B parameter variant of GPT-3, which is trained on a version of the Common Crawl web scrape with additional filtering and deduplicating.\n\nIn addition, we also use the actual human-written text from the data sources we draw from, which we denote as Human.\n\n\nDecoding strategies\n\nWe consider three main hyperparameters when sampling from models: p for top-p or nucleus sampling (Holtzman et al., 2020), an alternative to top-k; 3 t for the softmax temperature; and f.p. for frequency penalty. The frequency penalty scales a token's likelihood based on how many times it was already generated by applying the following modification to the model's output:\ni (t) \u2190 i (t) \u2212 c <i (t) \u00b7 \u03b1 f(1)\nwhere i (t) is the model's output for token t at the i-th position, 4 c <i (t) is the count of token t's sampled occurrences prior to the i-th position, and \u03b1 f is the frequency penalty. We omit studying presence penalty, another hyperparameter offered for GPT-3, simply due to annotation budget constraints.\n\nTo compare models as consistently as possible, we set identical decoding strategies for our primary data collection. We refer to this as the \"apples-toapples\" decoding setup throughout the paper:\np = 0.96 t = 1.0 f.p. = 0\nHowever, we also wish to study the effects of these decoding strategies. We annotate generations from the strongest available model (currently, GPT-3) varying the following parameters: 3 We omit separate studies of top-k, due to results presented by Holtzman et al. (2020), and OpenAI's removal of top-k from the GPT-3 API. 4 While i(t) is defined to be \"logits (un-normalized logprobabilities),\" because it is un-normalized, we anticipate that it is simply the model's output before the log(softmax(\u00b7)) is applied. See OpenAI's description of frequency and presence penalties: https://beta.openai.com/docs/ api-reference/parameter-details p \u2208 {0.4, 0.7, 0.9, 0.96} t \u2208 {0.0 (argmax), 0.4, 0.7, 1.0} f.p. \u2208 {0 (none), 1 (full)} For budget reasons, we only vary p and t independently-i.e., we set p = 0.96 when varying t, and t = 1.0 when varying p.\n\n\nPrompt Selection\n\nWe use news articles as the sources of prompts for models to condition on for generation. Specifically, we use news articles found in the Common Crawl. We select the first sentence as the prompt.\n\nOur use of news text is constrained by two factors. First GPT-3 is trained on the Common Crawl, from 2016 through 2019. We wish to avoid testing GPT-3 by generating from articles it saw during training, due to the possibility of copying (Carlini et al., 2021). Second, news articles began heavily covering the COVID-19 pandemic beginning around February 2020. Though testing models' capabilities to generate text about unseen events is a valuable line of study, the distribution shift caused by COVID-19 in news writing about all aspects of life is difficult to overstate.\n\nAs such, to make the comparison more amenable to models' training data, we consider news articles from January 2020. We select articles where there is a known topic-such as Food or Sports-from the Common Crawl metadata, to allow for studying any effect of coarse-grained subject.\n\n\nGeneration\n\nWe generate between 80 and 145 tokens 5 from each model as a continuation to the first sentence of the news article. We stop generating when we heuristically detect the first sentence boundary after 80 tokens. If the model does not end a sentence between 80 and 145 tokens, we sample again. For the Human setting, we use the remainder of the article, similarly stopping after the first sentence boundary after 80 tokens.\n\n\nAnnotation\n\nCrowdsourcing Workers first complete training and qualification tasks. We provide more details in 4.7. From pilot studies, we discovered that each error, depending on its severity and clarity, has only a low to moderate chance of being identified by each worker. However, most worker-identified errors were truly problems. In other words, annotators labeled issues with high precision and low recall. To account for this, we have 10 workers annotate each paragraph. We examine the agreement and variability of annotations in Appendix C.\n\nDataset statistics We provide detailed dataset statistics in Appendix D.\n\n\nError Prediction\n\nA natural question is: using this data, can machines learn to detect and classify errors in machine generated text?\n\nTask We frame this problem as a span classification task. Given a span from a generated text, the goal is to classify its error type or output \"No Error\" if there is none. Positive examples for each error class are taken from our data. We sample random spans that were not labeled with any error type as negative examples. To ensure a breadth of span lengths, we sample 3 negative spans for every length of error span in the generated text. We split the generated texts into train, development, and test sets using 1063 texts (28029 error spans), 100 texts (2538 spans) and 100 texts (2677 spans) respectively.\n\nModel We use a standard span classification model inspired by Wadden et al. (2019). This model encodes every generated text using a pretrained language model (RoBERTa-large). Spans are represented with the final layer of this encoding. Following previous work, we concatenate the start and end tokens with a task-specific learned length embedding. The resulting vector is passed through a feedforward network which reduces its dimensionally to the number of error categories plus a \"No Error\" option. The resulting model has 357M trainable parameters. The model is trained to minimize the cross entropy of the correct span category. We train for 15 epochs using AdamW with a learning rate of 10 \u22126 . We validate after each epoch and use the checkpoint with the lowest validation loss (epoch 8).\n\nEvaluation To evaluate the error prediction model, we use per-token precision, recall, and F 1 score per error category. We classify every span up to length 30 in a generated text. We take as gold labels the aggregated human error spans collected  Table 2: Model prediction results against combined spans of 10 annotators, compared with humans scored as one-vs-rest (i.e., 1-vs-9). Bold F 1 scores denote the higher average; values marked \"-\" cannot be computed due to division by zero. Takeaway: Humans have higher precision in every error type except Commonsense , but relatively sparse annotations lead to lower computed recall. This allows the model to achieve higher F 1 scores for half of the span categories.\nError Model Human P R F 1 P R F 1 Bad Math - 0 - 0.\nin our data. In other words, models predict the combined spans of all 10 annotators. For comparison, we also report as Human the average metrics of one annotator versus the others (i.e., 1-vs-9). 6\n\nResults Table 2 shows the error prediction capability of this model in terms of precision and recall. As we noted earlier, a single human annotator can be thought of as a high precision, low recall judge. These results bear out this claim. For all but one category, humans have higher precision annotations. However, the models trained on the aggregation of human labels can achieve considerably higher recall. For half of the error categories, this leads to higher model F 1 scores than the human annotators. We see that the model is successful at identifying information that human's would have to manually verify ( Needs Google ), achieving nearly perfect recall with precision close to 0.6. The model can also identify Grammar and Usage , Incoherent , and Redundant errors with higher recall than an individual human annotator, though at the cost of precision (sometimes in the .20s).\n\n\nRelated Work\n\nAutomated evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), ME-TEOR (Banerjee and Lavie, 2005), and BERTScore (Zhang et al., 2019) compute a generation's score based on a (set of) reference(s). Their use is wellestablished in tasks like machine translation and summarization, but they are less helpful in openended text generation, where there is a vast diversity of possible high-quality continuations.\n\nRecent studies propose automated metrics for open-ended text generation evaluation such as: Perception Score (Gu et al., 2021), which diffuses evaluation onto a multidimensional space and assigns a single score; UNION (Guan and Huang, 2020), which learns to distinguish human-written stories from negative samples by generating perturbations of human-written stories; and MAUVE (Pillutla et al., 2021), which compares the distribution of machine-generated text to that of human language.\n\nAn alternate recent approach to assessing openended text generation was presented in TuringAdvice (Zellers et al., 2021), where crowd workers assess machine-generated advice in response to Reddit posts. In their error analysis, Zellers et al. connect problems in generated text to core NLP tasks, such as Self-Contradiction errors as instances of failed natural language inference (Monz and de Rijke, 2001), or Off-Prompt errors as cases of failed reading comprehension (Richardson et al., 2013). While past work has attempted to guide text generation using discriminative models trained for such tasks (Holtzman et al., 2018), it remains an open challenge.\n\nComparative human evaluations of natural language generations ask annotators to rank system outputs relative to each other. Text is typically evaluated using a few global criteria, such as fluency and relevance, using discrete (e.g., 5-point) (Sai et al., 2020) or continuous scales (Novikova et al., 2018). Recent work even automates this approach, running a human evaluation alongside automatic metrics on leaderboard submissions (Khashabi et al., 2021). In the RoFT system (Dugan et al., 2020), annotators attempt to detect the boundary between human-and machine-written text as a proxy for assessing quality. Table 3 summarizes the differences between these schemes and SCARECROW. See Celikyilmaz et al. (2021) for a recent survey of text generation evaluation techniques across both human and automatic metrics.\n\nWhile these approaches may be helpfulsometimes (Card et al., 2020)-at ranking systems, they do not give us insight into exactly which parts of a generation fall short, and why. One approach\n\n\nMethod GC SET DE RR EE RS SA\n\nLikert-Scale RankME RoFT SCARECROW \n\n\nConclusion\n\nWe present SCARECROW, a method for identifying and explaining issues in generated text. Along with the annotation framework, we present an analysis of the SCARECROW method applied to several large neural language models in an open-ended news generation task. We release our data and methodology to the community. \n\n\nA SCARECROW Annotation Schema\n\nHere, we present in greater detail the SCARECROW annotation error types. 7 A visual summary is shown in Figure 6. While we annotate using this schema, the essence of our study is to embrace language users' abilities to detect when something may be wrong with text. In other words, we do not wish for our span definitions to get in the way of humans describing problems with text. To this end, we encourage researchers to embrace label back off (to coarser categories), merging labels (based on empirical observations), and refining the annotation ontology over time. The central goal is to collect what people find wrong with text.\n\n\nA.1 Language Errors\n\nWe define five categories of language errors, which concern the selection of ideas in a text and how they are expressed. These range from grammar and syntax problems to issues of semantics and pragmatics.\n\n\nA.1.1 Grammar and Usage\n\nThis category of errors includes missing words, extra words, and incorrect or out of order words.\n\n\nEXAMPLE\n\nA PhD student from the University of Kent in the UK claims to have discovered a clever way to explain the positive emoticons in cats.\n\nExplanation: The word should probably be \"emotions.\"\n\nWe also label Grammar and Usage for inserted words or small phrases that could be deleted to resolve the issue:\n\nA couple is facing criticism for their extravagant birthday party. The bewitching pair had first stripped down to fishnets and backward.\n\nExplanation: This phrase can simply be deleted.\n\nWe avoid partitioning Grammar and Usage errors into more detailed categories based on the observation that large language models produce fewer issues of syntax and diction (aside from Redundant errors, described next). As such, we focus instead on semantic and pragmatic errors, captured by the upcoming error types. 7 All example annotations here are our own. Many are provided to annotators during training. Figure 6: A visualization of SCARECROW spans: three categories (reader, language, and factual) composed of ten types. Annotators choose directly from the ten error types.\n\n\nREADER\n\n\nSOMETHING IS WRONG\n\n\nBad math\n\n\nA.1.2 Redundant\n\nWhile \"redundant\" can also include extra unnecessary information, we specifically use the Redundant label to mark repetition. In identifying redundant text, our schema annotates both the antecedent (first mention) and the redundant text (when the repetition occurs). Sometimes the exact word or phrase will be repeated.\n\n\nEXAMPLE\n\nMany merchants worry about the possibility of poor service or service for certain categories of customers.\n\nOther times, generated text expresses the same idea repeatedly using different words.\n\n\nEXAMPLE\n\nThey then made decisions based on Kondo's instructions, to the extent that they created decluttered spaces and got rid of clutter and clutter-filled spaces .\n\n\nA.1.3 Off-Prompt\n\nThe prompt is a human-written sentence used as context from which the model generates a continuation. Models sometimes generate text that is unrelated to the prompt.\n\n\nEXAMPLE\n\nPrompt: Dogs are the new kids. Generation: Statistics suggest that most Americans would be happier with dogs than children.\n\nIn fact, four out of five don't even visit the dentist annually, much less every six months. Dog owners report much higher rates of happiness than non-dog owners.\n\nOther times, the text may be related, but it contradicts what is stated in the prompt. \n\n\nA.1.4 Self-Contradiction\n\nWhen a model generates text that contradicts the prompt, that is labeled as Off-Prompt . But when a model generates text that contradicts itself, that is labeled as Self-Contradiction . We also mark the antecedent (original statement).\n\n\nEXAMPLE\n\nMcDonald's is considering a design which will replace the cardboard packaging. Mr Gore-Cotter said: \"We recognise the concern around waste. We are now looking at a new design that minimises the plastic bag.\"\n\nExplanation: The idea of minimizing the plastic bag contradicts the stated goal of replacing cardboard packaging.\n\n\nEXAMPLE\n\nMall of America plans to lay off and furlough hundreds of its employees. It has no plans to restrict the number of hours workers can work.\n\nExplanation: Furloughed workers are explicitly restricted from working.\n\n\nA.1.5 Incoherent\n\nGenerated text is sometimes grammatical, not redundant, on prompt, and not contradictory, but still confusing. We provide the Incoherent label for such sentences.\n\n\nEXAMPLE\n\nMelody Mitsugi, 28, had never given her kids cheese toast before her husband drew a map of it on her toast.\n\nExplanation: One can't exactly draw a map of Cheese Toast, and one probably wouldn't draw it on toast itself.\n\n\nEXAMPLE\n\nCats naturally show anxiety and fear by at times breaking apart different parts of the brain in an attempt to keep the others from escaping.\n\nExplanation: It's difficult to even imagine what is happening in this passage.\n\n\nA.2 Factual Errors\n\nWe define three categories of factual errors, which encompass known incorrect statements.\n\n\nA.2.1 Bad Math\n\nGenerated text will sometimes have issues with basic mathematical operations of known quantities (e.g., \"half of ten apples is four\"), problems converting fixed units (e.g., m to cm).\n\n\nEXAMPLE\n\nOne account, @Iain_Rowling1, had over 500,000 followers at one point, but in just four days they fell by around half -some 4,000.\n\nWe also include problems converting currencies that are wildly implausible under modern assumptions (e.g., \u00a31 = $18 US ).\n\n\nEXAMPLE\n\n... compared with just over \u00a31,000 ($18,868) for previous versions of Samsung's flagship phone.\n\n\nA.2.2 Commonsense\n\nThese errors mark spans that violate our everyday basic understanding of the world. Though it is challenging to precisely define commonsense knowledge (Liu and Singh, 2004), we include nonencyclopedic knowledge and basic reasoning.\n\nThe following example concerns broadly sensible numerical ranges.\n\n\nEXAMPLE\n\nThe picture is from high above the South Pole, where close to 100,000 Astronauts live and work.\n\nExplanation: Even if we don't know the exact number of astronauts in space, it is common knowledge that 100k is far too many.\n\nThe next example involves world knowledge, akin to scripts (Schank and Abelson, 1977).\n\n\nEXAMPLE\n\nYou can get the dress custom-made and stitched at your favorite spa.\n\nExplanation: Spas don't offer stitching.\n\nThe following example involves lexical entailment.\n\n\nEXAMPLE\n\nThe thinness of our bodies isn't an answer to all common human health problems like obesity or diabetes Explanation: While most of the statement is acceptable, it's impossible to be \"thin\" and \"obese\" at the same time.\n\nThe final example involves time. \n\n\nA.2.3 Encyclopedic\n\nThese errors are ones that we know are factually wrong, and that we could look up in, say, Wikipedia.\n\n13 7262 EXAMPLE Japanese Prime Minister Justin Trudeau said he will be halting all imports and exports until the current situation can be contained.\n\nExplanation: Justin Trudeau is the Prime Minister of Canada, not Japan.\n\nThe distinction between Encyclopedic errors, and the upcoming Technical Jargon and Needs Google issues, depends on the reader's knowledge.\n\n\nEXAMPLE\n\nThe gas contains something known as phyto-romatic acid, a common chemical element in the periodic table.\n\nExplanation: Acids aren't elements.\n\n\nA.3 Reader Issues\n\nWe define two categories of reader issues. These are words or statements a reader cannot verify without using an external resource.\n\n\nA.3.1 Technical Jargon\n\nSometimes generated text includes specific words from a field that requires expertise to understand.\n\n\nEXAMPLE\n\nIn Chile, an 800-megawatt photovoltaic plant was built for a record low cost of $129 per megawatt-hour last year.\n\nWhich words are jargon depends on the reader's particular expertise. This means Technical Jargon spans are more accurately thought of as potential issues rather than known errors.\n\n\nEXAMPLE\n\nHe uses a spirit mash made from white corn and malted barley and a neutral grain , which he describes as a \"whiskey grain.\"\n\n\nA.3.2 Needs Google\n\nMany facts-especially those involving specific people, events, dates, or numbers-could be categorized as encyclopedic knowledge. However, whether the fact is accurate may require additional verification by the everyday reader. To make this distinction between known encyclopedic knowledge and trivia, we introduce this label to denote that a reader would need to search online to verify whether it is true.\n\nWe instruct annotators to not look up facts marked with the Needs Google span. We do this to keep the focus of the task on classification, rather than factuality detection. As a result, Needs Google spans mark statements that would need to be verified, rather than known errors. Explanation: In addition to potential Technical Jargon spans, there are at least two Needs Google spans: 1. whether such a plant can be roughly 800-megawatt, 2. whether $129/megawatt-hour is a sensible cost measure, and the value is reasonable.\n\nTo illustrate the annotation methodology and schema in practice, we present four complete example annotations in Figure 7. This figure also illustrates how much variation we see across models.\n\n\nB Annotation Details B.1 Error Severity\n\nWe provide here examples for each of the three error severity levels, which we also give to annotators during training.\n\n\nEXAMPLE\n\nPaul Campbell-Hughes, from the University of Aberdeen, explains how she managed to locate colonies of honey bees in Kent.\n\nSeverity: 1. Since Paul is usually a male name, the model should have used \"he.\" But this error is pretty minor.\n\n\nEXAMPLE\n\nPaul Campbell-Smith, a PhD student from the University of Kent in the UK, claims to have discovered a clever way to explain the positive emoticons in cats.\n\nSeverity: 2. The word should probably be \"emotions.\" We can guess what was being said, but it's definitely wrong. EXAMPLE Prompt: Whether you're on Facebook, Instagram, Snapchat or TikTok, many people make huge efforts to curate the best version of themselves online. Generation: This year we've got something for you: a Love Match Custom Size Poster featuring Mather, Phoenix, Kashun and all her friends, divided among six different covers, creating a beautiful custom size poster for your own personal high school reunion.\n\nSeverity: 3. Even ignoring the end of the generation (a poster for a personal high school reunion?), this whole generation is way off the prompt and does not make sense.\n\n\n14\n\n\nGPT-2 Small\n\n\nGPT-2 XL\n\n\nGPT-3 DaVinci Human\n\nOff-prompt (3): The prompt is about parents putting their children at risk of depression from ignoring them while on the smartphone.\n\n\nIncoherent (3): Children's personality and speech shouldn't be invaded by researchers. The rest doesn't really make any sense.\n\nIncoherent (3): What kind of classes taking place is a mystery. The items that percentages are given for make no sense.\n\n\nIncoherent (3): This doesn't make any sense either. Half-time reading and C-section check is nonsense.\n\nOff-prompt (3): This contradicts the prompt that says his gun and drugs were found.\n\nSelf-contradiction (2): This states that police were sent to the house following reports that someone was checking on the welfare of Coombes. It's more likely the police were sent to do a welfare check on him.\n\nOff-prompt (3): According to this the police didn't find or arrest Coombes on Thursday, but the prompt say he was arrested at the house he was staying at.\n\nSelf-contradiction (2): It says Coombes has a roommate and a house so the homeless shelter seems like a contradiction.\n\n\nNeeds Google (1): Is paracetamol used this way? Needs Google (1): Is Zylowska a psychologist there?\n\nNeeds Google (1): Is this drug used for these conditions?\n\n\nSelf-contradiction (3): I guess medicines can be used for different purposes, but these different conditions seem contradictory.\n\nNeeds Google (1): Is paracetamol a painkiller?\n\n\nSelf-contradiction (3): Testing the drug on people with depression and anxiety indicates it's used not that not as a painkiller.\n\n\nNeeds Google (1): Is Polar Bear Plunge what the New Years swim in La Jolla is called, and has it been going on for 30 years?\n\nFigure 7: Example SCARECROW annotations (for a single annotator) of three model generations and one ground truth continuation, demonstrating the shift in number, type, and severity of errors. The entirety of the GPT-2 Small generation is Off-Prompt and/or Incoherent , with high severity (3/3). GPT-2 XL is instead only about two-thirds covered by errors-still sometimes Off-Prompt , but also Self-Contradiction , and with high severity (2-3/3). In contrast, GPT-3 DaVinci receives several Needs Google marks-less severe than errors, as they only indicate that fact-checking is needed-though it also commits two high-severity Self-Contradiction errors by generating inconsistent claims. The Human (ground-truth) continuation only receives one Needs Google span.\n\n\nB.2 Grading Details\n\nIn the training material, there are 10 annotation exercises, 10 multiple choice questions, and 1 real task question to test workers' understanding.\n\nAnnotation Exercise After going through each error type, there is an annotation exercise. Workers are asked to mark the span with that particular error in a short text. Each exercise is worth 5 points.\n\nMultiple Choice Question After going through all language errors, and going through all factual errors and reader issues, there is a language error label quiz and a reader and factual error label quiz respectively. Each label quiz consists of 5 multiple choice questions, where workers are asked to choose the error type of a marked span in a short text. Each multiple choice question is worth 3 points.\n\nReal Task Question At the end of the whole training material, workers are asked to apply what they learn in an actual task where they annotate a given paragraph with full tool like ones shown in Figure 7. This question is worth 20 points. We mark 7 error spans as the solution. As long as they can mark 5 of 7 error spans, they get a full 20 15 points. Otherwise, 4 points will be deducted for each missing error span. In total, there are 100 points. We pass workers if they score \u2265 90 points, and then they are provided with the solution to review.\n\n\nC Data Quality\n\nIdentifying and classifying errors in potentially noisy machine-generated text is a challenging task. How consistent are the annotations collected from crowd workers? In this section, we examine the agreement and variability of the collected annotations.\n\nAt a high level, we observe either acceptable or high inter-annotator agreement across error categories. For rare error types such as Bad Math , high agreement stems from the prevalence of spans with no error. For such categories, we recommend treating each annotator as a high precision, low recall judge, and considering the information from their aggregate annotations. Figure 8 gives an example of the perspective gained by viewing all 10 annotations of a single generation.  Table 4: Per-token inter-annotator agreement metrics by error category. The >1 indicates that we omit severity-1 Grammar and Usage errors in all analyses in this paper due to higher variance; including them would drop the Krippendorf's \u03b1 to 0.56.\n\nAgreement Table 4 shows token-level interannotator agreement statistics aggregated over all collected data. Since a single annotator can label a single span with multiple errors, we break the agreement statistics down by error category. We report Krippendorff's \u03b1 coefficient, a chancecorrected measure of agreement for multiple annotators (Krippendorff, 2018). Due to computational constraints, we calculate this coefficient per generation and report the average across the dataset. The agreement shown here is high for most categories (>0.8) and acceptable (>0.6) for all error types. The Krippendorff measure may be deceptively high for some error types such as Bad Math , where 99% of tokens are not annotated with this error. The Two Agree measure in Table 4 gives a different characterization of this data. Two Agree for a given error label is the percentage of tokens labeled by at least one annotator that were also labeled by one or more additional annotators. This metric allows us to see where annotators agree that particular errors exist while ignoring the majority of tokens (for most error categories) which annotators agree are not errors. Two Agree shows significantly lower rates for sparse errors with high Krippendorff scores, such as Encyclopedic . However, it reveals stronger agreement among Incoherent and Off-Prompt errors than might be expected given the Krippendorff coefficient.\n\nA limitation for both metrics is the use of tokenbased overlap.\n\nBootstrap One issue we face is high variance of annotations. To determine the impact of this variance for lower-data settings, we perform a bootstrap analysis using largest subset of our data (GPT-3, top-p = 0.96, t = 1, f.p.= 0, for which we have annotations of 200+ generations). We choose 50 generations (roughly 500 annotations) and calculate the error statistics therein. We repeat this process 1000 times and report the mean, standard deviation, and coefficient of variation in Table 5. We also calculate the coefficient of variation for different numbers of samples, shown in Figure 9. We see that as the number of samples increases, the coefficient of variation decreases as expected, though less precipitously after 30 examples. These results show that with as few as 50 documents, the SCARE-CROW error analysis should yield relatively robust results. However, this varies by error type: rare errors like Bad Math and Encyclopedic show greater variance. Here, again we repeat our recommendation to treat annotations for these categories in aggregate. These results motivate our collection of at least 500 annotations per condition studied.\n\n\nD Dataset Statistics\n\nWe list the data collection quantities in Table 6, and plot visualizations of three aspects: prompt topic and annotated span proportions are shown in Figure 10, and average span lengths are shown in Figure 11.\n\n\nE Detailed Analysis\n\nIn this section we perform a detailed analysis of the trends of individual error types and decoding 16 Figure 8: A visual representation of the 10 annotations we collected for one paragraph. Each blue bar represents one annotator, where the width of the bar represents the text of the paragraph. Colored bars drawn on top of the blue bar represent spans marked as errors. We draw bars semi-transparently to show overlapping errors. We can see that some problematic spans (e.g., the Off-Prompt section) are marked by almost all workers and given the same label. Other spans are marked by only a subset of the workers (e.g., Commonsense and Incoherent spans on the right side), or have some label disagreement.   configurations.\n\nTo begin, we consider apples-to-apples model decoding configurations. To expand on these results, originally presented in Figure 2, we also present two additional ways of counting error spans, which we show in Figure 12. While our method for counting errors throughout the paper takes into account the number of tokens covered in each span (span coverage), we also show plots for scaling each span by its severity level (span coverage \u00d7 severity), and by ignoring both severity and token length (simply span counts). These changes in measurement further illuminate model error characters, which we discuss in the upcoming sections (refer to Figure  12).  \n\n\nE.1 Off-Prompt\n\nUnder initial analysis of span coverage, Off-Prompt errors show a model plateau at GPT-3. Measuring span counts offers barely perceptible improvement, indicating that scaling language models over more in-domain training does not guarantee topicality. This observation is consistent with growing work on prompt programming as a new technique for attempting to steer large pretrained models to complete the desired task (Branwen, 2020;Gao et al., 2020;Reynolds and McDonell, 2021). In practice, we observe that while GPT-3 will sometimes continue a prompt by writing an article, other times, it may elaborate on the prompt itself: PROMPT Do you prefer the idea of being outdoors in the fresh air to being stuck inside with phones ringing and messages pinging?\n\n\nGPT-3\n\nCan you leave work at work? Are you flexible enough to cover holidays or take on additional responsibilities? Can you prioritize tasks? If your boss comes to you on Tuesday to confirm the new social media strategy, are you able to pick up the ball and get the messaging hammered out by Thursday? ...   Figure 11: Average number of tokens covered by each annotated span. We observe span length correlates with how abstract the error category is, from word-level issues ( Technical Jargon ), through phrase-level semantics (e.g., Commonsense ), and into problems of pragmatics ( Off-Prompt ).\n\nOf course, this generation is not literally Off-Prompt , but it is out of place when other generations are continuations of the prompt, rather than further elaborations of it.\n\nWhile avoiding Off-Prompt errors for language models is worth exploring with prompt programming and other avenues, an investigation of these techniques is outside the scope of this work.\n\nFinally, we note that Off-Prompt spans are the most prevalent error (not reader issue) marked for human-authored text. We suggest that a higher rate of false positives for this error type, coupled with its prevalence in model-generated text, makes further refinement of this error a compelling avenue for further study.\n\n\nE.2 Self-Contradiction\n\nWhile changing from span coverage to span counts alters the relative order of GPT-2 XL and Grover (though still within confidence bounds), the puzzling question is why GPT-2 Small performs better than most (or all) other models. Why would the smallest model produce the fewest Self-Contra- (The top plot for each error type is identicial to the one shown in Figure 2.) The top method (span coverage) is used in the rest of the paper; we provide the comparisons here to illustrate how this decision affects analysis. Top subplots: span coverage, where the number of tokens annotated as the error span are divided by the length of each annotation. (Annotations with no spans count as 0.) Intuitively, this measures the expected portion of tokens that will be covered by an error span. Middle subplots: span coverage \u00d7 severity, like the top measure, but each span's token count is multiplied by its severity, more harshly penalizing errors intuitively marked as worse.\n\nBottom subplots: span counts, where each error span simply counts as 1, regardless of the span length. In all cases, model configurations are set as closely as possible (top-p = 0.96, t = 1.0, no frequency penalty), severity-1 grammar errors are removed (see \u00a7C), and 95% confidence intervals are shown as bands. Takeaways: Compared to the approach used in the rest of the paper (span coverage; top), scaling by severity (middle) does not affect the relative model ordering, primarily widening confidence intervals. However, ignoring span lengths (bottom) does affect the results in several cases. Grammar and Usage and Encyclopedic develop clearer decreasing shapes, previously suffering from various levels of model plateau at GPT-3. Furthermore, the relative model ordering is changed for Redundant , Self-Contradiction , and Technical Jargon spans.\n\ndiction errors?\n\nWe posit the reason is that GPT-2 generations are so Incoherent and Off-Prompt that there is little opportunity for relevant, comprehensible points to be made and then reversed. For example, see the GPT-2 Small annotated generation in the top left of Figure 7. The entire text is covered by Off-Prompt and Incoherent errors. 8 If we look at GPT-2 Small's error distribution in Figure 3, we see most of its added density comes from significantly more Off-Prompt and Incoherent tokens.\n\n\nE.3 Redundant\n\nThe different counting methods shown in Figure 12 reveal a change in the results for Redundant errors. Rather than repetition simply increasing as models grow larger, we observe that GPT-3 repeats in a similar number of cases (lower span counts), but for more tokens (higher span coverage). This matches the qualitative observation that GPT-3 produces larger topically repetitive blocks, rather than simple word or phrase repetitions generated by GPT-2-sized models:\n\nGPT-2 Small ... owners have started growing their own breeds and dogs are starting to start so there's really ...\n\n\nGPT-3\n\nThe focus of your thoughts should be on the task at hand, not on your productivity. You shouldn't be thinking about how you can be more productive. You should be thinking about how you can be productive right now. ... Such repetitions can be more difficult to clearly isolate, because even slight wording changes produce variations in tone and connotation. Rather than being identical semantically, we observe GPT-3 will seem stuck on a particular topic, elaborating on and rephrasing similar ideas more times than a human writer (hopefully) would.\n\n\nE.4 Reader Issues\n\nWe observe the highest number of Needs Google and Technical Jargon issues in humanauthored text.\n\nNeeds Google issues broadly represent any specific claim that could be fact-checked. In our domain (news articles), these are primarily whether an event happened on a particular day, whether a person holds a role, or whether a mechanism works as described (e.g., chemical or technical). As seen in Figure 13 (which shows GPT-3's span distribution), Needs Google issues happen roughly equally for all topics. We believe this trend is due to the news article domain, which is prone to a high density of specific information. As such, for other domains, this trend may be less prevalent, more difficult to label (e.g., subtle claims assumed to be true in long running text), or both.\n\nWe observe that Technical Jargon issues are influenced by topic (Figure 13, bottom), occurring significantly more frequently in Business, Health, Science, and Technology topics than in others. This trend displays a clear topic-dependence even within a single broader domain (news). These results indicate that both reader issues are characteristics of natural text. Of course, one might wish to measure or minimize potential reader issues for a particular application-for example, claim verification, or controlling for reading level.\n\n\nE.5 Decoding Hyperparameters\n\nWe discuss the effects of the decoding hyperparameters we consider-top-p, temperature, and frequency penalty-on generation quality. For the sake of annotation cost, we only vary these parameters for the strongest model available, GPT-3. First, we show the effect of varying top-p and temperature alone (i.e., with no frequency penalty) on different error types. Figure  intuitively expect the model to produce them, given the hyperparameter changes. The bottom-right corner of each subplot, where t = 1 and p = 0.96, is the configuration with the highest amount of randomness from sampling. As we move away from that corner-either left by lowering temperature, or up by lowering top-p-we lower the amount of randomness. We observe a positive correlation with randomness and Off-Prompt errors, and an inverse correlation with Redundant errors. In other words, sampling from a larger set of words makes the model more prone to changing topics, but less likely to repeat itself, and vice versa.\n\nAfter confirming these intuitive measures, we turn our attention to Figure 15, which investigates the overall error spans for GPT-3 both without (left) and with (right) the frequency penalty. (Note that unlike Figure 14, both heatmaps in Figure 15 have the same color scale.) We observe that introducing the frequency penalty lowers error rates for every value of temperature and top-p that we try. Furthermore, it appears to reverse the trend seen without a frequency penalty: that sampling from a larger set of words produces fewer errors.\n\nThe overall results for all decoding configurations were shown previously in Figure 4. In the next section, we focus on the GPT-3 decoding configuration that produced the fewest number of errors, and compare it to human authored text.  Figure 15: Comparison of frequency penalty off (left) and full (right) for GPT-3 (removing reader issues and severity-1 Grammar and Usage errors; argmax sampling is agnostic to the top-p value, so we simply plot it in the p = 0.96 cell). We observe the frequency penalty improves average span coverage for all values of topp and temperature. Furthermore its trend is reversed: with a frequency penalty, the least diverse sampling mechanisms (low temperature and low top-p) now produce text with the fewest error spans, rather than the most. (See Figure 4 for confidence intervals on each value.)\n\n\nE.6 Best GPT-3 vs. Humans\n\nThe best GPT-3 configuration shown in Figure  4-argmax sampling with frequency penalty = 1appears to match error rates seen in human text. Is the text generated by this model truly as error-free as news articles?\n\nWe first look at the error composition of both sets of annotations. To get a clear picture of the potential problems, we plot only error spans (ignoring reader issues), and we omit length scaling, instead plotting span counts. This breakdown is shown in the left plot of Figure 16. The error compositions are similar, the largest differences being more Redundant errors for GPT-3, and more Grammar and Usage errors for human-authored text.\n\nNext, we perform a manual analysis of 160 errors, sampling 10 at random from each of the 8 error types for each model (GPT-3 and humanauthored text). We show the results in the center plot of Figure 16. We notice that a greater portion of errors in human-authored text were due to artifacts present in the text-only format of the Common Crawl. For example, links to other articles or advertisements sometimes appear in the middle of an article's text. While annotators were quick to mark these spans, they reflect errors in formatting, not in writing. We partition these errors separately and exclude them from the subsequent calculations. 9  Figure 16: Analysis of the best GPT-3 configuration (argmax, freq. penalty = 1) vs. human-authored text. Left: A breakdown of errors by type. Center: Results of manually annotating 10 random spans from each type with whether the error was legitimate. For human-authored text, we also show errors marked on scraping artifacts that were present in the Common Crawl data. Right: Scaling each error type (left plot, now shown in black outline) by the portion of errors found to be legitimate (center plot), we estimate the true errors counts for each model (colorfilled portions). Takeaway: Humans have more difficulty spotting errors in higher quality text; accounting for this difference dramatically increases the gap between model-authored and human-authored text. For simplicity, all plots use error counts rather than error coverage-i.e., they count the number of error spans, rather than scaling by the number of tokens covered.\n\nFinally, we scale each error type's prevalence for each model (i.e., the left plot of Figure 16) by the portion of errors that we estimate to be legitimate based on our manual annotation (i.e., Figure 16, center) to produce the right plot of Figure 16. After taking into account each error type's frequency, we estimate that 48% of GPT-3's worker-annotated errors overall are legitimate, compared to 9% for human-written articles.\n\nThis analysis suggests two findings. First, human-authored news paragraphs contain many times fewer issues than text authored by GPT-3 using the best decoding configuration we tested. Second, the noise of error annotations may be as high as 90% when assessing high-quality text. Though it would require further manual annotation to verify, we conjecture that the trend of GPT-3's error spans being more reliable (only 50% noise) would continue, and that text generated by GPT-2 would contain even fewer false positives. We note that such rates are not fixed-after all, the manual annotations were done by one of the authors simply by reading carefully-but that more realistic text may require correspondingly more effort by human annotators.\n\ntext, though more rarely. For example, some generations contained Which? after vague noun phrases, which appear to be learned from Wikipedia, where under-specified information is tagged by an editor with this word. For fairness, we removed these errors from GPT-3's tally as well, though they were few enough we do not plot them separately. , with 95% confidence intervals. While the majority of topics display no significant trend, we observe that more technical topics such as Tech and Health are covered by a higher density of error spans than Style and Art.\n\n\nE.7 Topics\n\nAs noted in \u00a75.3, we collect data using prompts drawn primarily from 12-14 news topics. For conciseness, we show results only for GPT-3, and only for the standard apples-to-apples decoding configuration. Figure 17 plots, based on the prompt topics, the average portion of the generation that is covered by error spans. While there is no significant difference between most topics, the results do indicate that generating text in more technical domains leads to higher span counts. Figure 13 shows individual span prevalence by topic. The top heatmap normalizes each topic (col-22 umn) independently. Needs Google issues and Off-Prompt errors dominate the error types, with a few exceptions: for History, and Nature articles, Redundant trumps Off-Prompt as a source of errors.\n\nFor the bottom, if we instead normalize by error label (row), we can observe which topics are more prone to certain error types than others. For example, we can see Bad Math errors are most common in Business and Health generations; Entertainment causes the most Self-Contradiction errors; and Technical Jargon issues appears more frequently in articles about Business, Technology, or Health.\n\n\nE.8 Error explanations\n\nFigure 18 displays word clouds for common unigrams and bigrams found in the error explanations for each error type, and Figure 19 shows the average explanation lengths for each error type. For Technical Jargon , Redundant , and Needs Google error types, the prominent words do not provide much illumination and they have short average explanation length, indicating that the explanations are straightforward affirmations of the category (\"I think this is financial jargon,\" \"The information is repeated,\" or \"I would need Google to check this.\"). But for categories like Encyclopedic and Bad Math , we observe some coarse trends: \"year\" is prevalent in both, \"movie\" appears in Encyclopedic , and \"million\" is present in Bad Math , which suggests that the explanations are more likely from outside knowledge and needs some calculation (\"The iPhone uses a lightening connector not a L-shaped connector,\" or \"5000 feet is 1524 meters.\") Figure 20 presents a few representative explanations for four error types, taking particular note of their explanation lengths ( Figure 19). Both Self-Contradiction and Redundant errors have antecedents, but their explanations are markedly different. Explanations for Self-Contradiction contain more information describing the particular semantics that is reversed, which are less obvious at first glance than other errors. On the other hand, Redundant errors are more straightforward to spot, often involving simple lexical overlap, and so don't require elaboration.\n\nExplanations for Commonsense contain the true commonsense knowledge that the text violates, which may take several words to explain. But an explanation for a Grammar and Usage error simply corrects the error; as these errors are easier to fix, the explanation lengths are often short.\n\n\nF Future Work\n\nWe outline several further directions of study centering around the SCARECROW annotation framework, considering both natural implications and broader steps.\n\n\nF.1 SCARECROW Studies: Simple\n\nFind the best-performing GPT-3 decoding hyperparameters. We observed that for GPT-3, a frequency penalty value of 1 with argmax sampling produced fewer error spans than any other configuration (Fig. 4). We have not tried varying the frequency penalty to values between 0 and 1, or adding any presence penalty ( \u00a75.2), both of which then allow for fresh explorations of top-p and temperature.\n\nStudy decoding parameters in smaller models. How good can (a finetuned) GPT-2 get? We saw decoding parameters considerably impacted GPT-3's performance, moving it from edging out Grover to error rates close to humans (Fig. 4). Could such decoding changes have a similar effect on a GPT-2-sized model? Or might a smaller model favor different decoding hyperparameteres?\n\nBack-off annotations. We observed good annotator agreement given the complexity of the task, but the odds that two annotators agree exactly on each span's type and boundaries remains only moderate ( \u00a7C). We did not try backing-off (a) error types into coarser categories (e.g., language, factual, reader issue) or even to binary presence; (b) span boundaries into phrase or sentence-level annotations. Applying a type of back-off could also allow clustering methods to discover different error ontologies.\n\nImprove automatic error detection. While we present baseline results for automatic span error detection ( \u00a76), we anticipate that significant progress is still available in this new task.\n\n\nF.2 SCARECROW Studies: Complex\n\nAlign multiple annotations. In the current work, we largely treat annotators independently, with the exception of measuring their overlap to study agreement ( \u00a7C) or taking their union to train prediction model ( \u00a76). However, we might consider other 23 Figure 18: Common unigrams and bi-grams observed in the explanations written for each annotated span, grouped by error type.  Figure 19: Average number of tokens in explanation for each error type. We observe explanation length correlates with how obvious the error type is, where categories like Grammar and Usage and Technical Jargon are easier to find and explain than Self-Contradiction and Commonsense .\n\nways of viewing the 10 annotations for each generation together. For example, we might consider the aggregate decision of whether a token is labeled with any span a measure of how noticeable or jarring an error is. This measure may be related to error severity, but may be distinct from it.\n\nOne might also consider formal methods for computing annotation alignments. The Gamma measure, proposed by Mathet et al. (2015), satisfies the long list of criteria needed to align and measure SCARECROW annotations: spans of multiple types, with gaps, full and partial span overlap, more than three annotators, and the potential to merge or split annotations (which we have not addressed in this paper). While we performed experiments with this measure, we experienced difficulties producing intuitive alignments with the authors' software, which disallows configuring parameters of Commonsense Grammar / Usage\n\n\nSelf-Contradiction\n\n\nRedundant\n\nThere should be a period after 'video'.\n\nNeeds end quotation marks.\n\nWord usage. Correction: despite.\n\nIf he wasn't interested, he wouldn't be attracted to her 'for years'.\n\nThe span says the villagers rescued Rinku from her house, but the first span says that the villagers chased the kidnappers and found Rinku near a tea stall.\n\nHow can they end up with a title if they lost in the finale?\n\nThis was already stated.\n\n\nDuplication\n\nPhrase is repeated at the end of the paragraph It doesn't seem logical that a Sicilian restaurant would have Chinese take-out.\n\nIt's hard to believe 50,000 people were homeschooled by one person.\n\nIn a psych department of a hospital they would not call an ambulance nor would an ambulance have or give a lethal dose of a narcotic. the mixed-integer programming problem. 10 Emerging concurrent work (Titeux and Riad, 2021) offers a reimplementation of this measure that exposes additional parameters, which may be a promising avenue. However, it is possible that aligning annotations is a challenging task on its own that might require use of the explanations.\n\nCharacterize error nuance. Related to the previous point about error alignment, one might study whether model size affects span agreement. Anecdotally, errors from larger models like GPT-3even of the same type, like Commonsense errorsare more difficult to describe without careful consideration, and may also be more difficult to identify.\n\nCharacterize repetition. Our quantitative studies of Redundant errors (e.g., Figs. 14 and 12) point to semantic repetition as the major issue that emerges as models are scaled. Though this effect may be mitigated by changes to the decoding algorithm (like the frequency penalty), we still observe that models have difficulty striking a balance of repetition. With excessive paraphrasing, generated text seems stuck on an idea. But equally, if a generation moves too quickly between ideas without linking them together or to an overall theme, the text lacks coherence. We posit that the issue of Redundant text emerges as the shadow of encompassing issues of narrative structure and discourse.\n\n\nF.3 Broadening SCARECROW\n\nConstrained generation This paper focuses on open-ended generation, but a natural extension of this method would be to assessing constrained generation tasks, such as machine translation.\n\nNew error types Especially if considering a novel task setting, new error types may prove useful. For example, in constrained generation, one might consider an Adequacy error, which-as in machine translation-would indicate that the meaning of a span diverges from what is expected given the generation constraints. Furthermore, one might need to introduce annotations on the provided (not generated) text to account for desired semantic components that are missing from the generated text. Or, perhaps for a dialog setting, one might introduce a Generic label, which would indicate that a portion of the generation is otherwise coherent and correct, but offers a lack of new information. 11\n\nCorpus-level evaluation Other work has considered the evaluation of natural language generations 11 Such generic language may be seen as violating Grice's Maxims (Grice, 1975), for example, by providing a dearth of information quantity, or by flouting improper manner by lacking brevity. at-scale, looking at distributional properties of the text (Caccia et al., 2020;Pillutla et al., 2021). We suggest that these views are complementary to instance-based, human evaluation proposed here, and combining the approaches could lead towards a more holistic view of generative evaluation. For example, while all Self-Contradiction errors right now are within-document, one could similarly identify cross-document contradiction errors, where a model is inconsistent at a more global scale.\n\n\nF.4 Applications\n\nDetecting factuality One potential application of the SCARECROW data could be using the Needs Google spans as a dataset of its own. In addition to training models to identify spans that require verification, one could go a step further and consider evidence retrieval for each span, and even propose a classification task. 12\n\nEditing errors One errors can be detected, can they be fixed? The difficulty and scope of fixing SCARECROW-identified errors may depend on the error type, as error fixes may have cascading effects in the rest of the document.\n\nGPT- 2\n2Small (Radford et al., 2019)  The 117M parameter variant of GPT-2, which is pretrained on WebText, without additional fine-tuning.GPT-2 XL (Radford et al., 2019) The 1.5B parameter variant of GPT-2, (WebText, no finetuning).\n\n\nChina sets new record for Economic Growth Generation: The Chinese economy fell 10% this month, the third such loss this year.\n\nEXAMPLE\nNow in 2021, NASA is measuring California wildfire temperatures using an instrument on the International Space Station. This year's recordshattering heat has had global repercussions in 2017 , forcing sea level rise on California and increasing the risk of deadly wildfires.Explanation: Events in 2021 can't affect events in 2017.\n\n\n... an 800-megawatt photovoltaic plant was built for a record low cost of $129 per megawatt-hour last year.\n\nFigure 9 :\n9Change in coefficient of variation as number of bootstrap samples increases overall (top), and by error type (bottom), with 95% confidence intervals. Data shown for GPT-3 with apples-to-apples decoding configuration (top-p = 0.96, t = 1, no f.p.).\n\nFigure 10 :\n10Visual overviews of the distribution of prompt topics used for generating the 1.3k paragraphs used in the annotation (left), and the types of the 41k spans labeled during the annotation (right).\n\nFigure 12 :\n12Comparison of three different ways of measuring quantities of error span annotations, shown per label.\n\nFigure 13 :\n13Span coverage across both topic (x-axis) and span label (y-axis) for GPT-3 generated spans (apples-to-apples decoding: p = 0.96, t = 1, and no frequency penalty). Top: normalized by topic (column); bottom: normalized by error type (row).\n\nFigure 14 :\n1414 shows the effect on two salient spans: Off-Prompt and Redundant . (We omit others for space.) We observe that annotators naturally label errors the way we GPT-3 span coverage for Off-Prompt (left) and Redundant (right) for values of top-p and temperature (t = 0 is argmax; both plots with no frequency penalty; argmax sampling is agnostic to the top-p value, so we simply plot it in the p = 0.96 cell). Takeaway: Our annotation confirms intuitive expectations of the effect of sampling on two error categories. When sampling from a larger pool of words (higher p and t), a model is more likely to veer Off-Prompt , but less likely to produce Redundant text.\n\nFigure 17 :\n17p=0.96, t=1.0, no freq. penalty) Span Coverage by Topic Average span coverage for different topics (GPT-3 generations with apples-to-apples decoding configuration)\n\nFigure 20 :\n20Examples of error explanations from different error types that favor longer (top) and shorter (bottom) descriptions.\n\nTable 1 :\n1Error types in the SCARECROW framework, grouped into three categories. The categories are explained further in \u00a74.4, and detailed definitions and examples for each error type is provided in Appendix A.\n\nTable 3 :\n3Comparison of different natural language gen-\neration human evaluations. Here, GC : General Crite-\nria, SET : Specific Error Type, DE : Direct Evaluation, \nRR : Relative Ranking, EE : Error Explanation, RS : \nRating Scale, SA : Span Annotation. \n\nrelated to or annotation method is pursued by Wood \net al. (2018), who develop a collaborative mobile \napp where users draw \"graffiti\" commentary on \nnews articles. SCARECROW aims to assess model \ngenerations the way we would critique human-\nwritten text: by locating, coarsely categorizing, and \nexplaining problems. \n\n\n\n\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2021. Evaluation of text generation: A survey. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296, Online. Association for Computational Linguistics. Elizabeth Clark and Noah A. Smith. 2021. Choose your own adventure: Paired suggestions in collaborative writing for evaluating story generation models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3566-3575, Online. Association for Computational Linguistics. Liam Dugan, Daphne Ippolito, Arun Kirubarajan, and Chris Callison-Burch. 2020. Roft: A tool for evaluating human detection of machine-generated text. arXiv preprint arXiv:2010.03070. Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723. Herbert P Grice. 1975. Logic and conversation. In Speech acts, pages 41-58. Brill. Jing Gu, Qing yang Wu, and Zhou Yu. 2021. Perception score: A learned metric for open-ended text generation evaluation. In AAAI. Jian Guan and Minlie Huang. 2020. Union: An unreferenced metric for evaluating open-ended story generation. In EMNLP. Tatsunori Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying human and statistical evaluation for natural language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1689-1701, Minneapolis, Minnesota. Association for Computational Linguistics. David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169-182, Dublin, Ireland. Association for Computational Linguistics. Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, and Daniel S. Weld. 2021. Genie: A leaderboard for human-in-the-loop evaluation of text generation. Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology. Sage publications. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81. Christof Monz and Maarten de Rijke. 2001. Lightweight entailment checking for computational semantics. In Proc. of the third workshop on inference in computational semantics (ICoS-3). Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. 2018. Rankme: Reliable human ratings for natural language generation. In NAACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Human-machine divergence curves for evaluating open-ended text generation. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101-108, Online. Association for Computational Linguistics. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7. Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 193-203. Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. 2020. A survey of evaluation metrics used for nlg systems. arXiv preprint arXiv:2008.12009. Roger C Schank and Robert P Abelson. 1977. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology Press. Hadrien Titeux and Rachid Riad. 2021. pygammaagreement: Gamma \u03b3 measure for inter/intraannotator agreement in python. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762. David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5784-5789, Hong Kong, China. Association for Computational Linguistics. Gavin Wood, Kiel Long, Tom Feltwell, Scarlett Rowland, Phillip Brooker, Jamie Mahoney, John Vines, Julie Barnett, and Shaun Lawson. 2018. Rethinking engagement with online news through social and visual co-annotation. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 1-12. Rowan Zellers, Ari Holtzman, Elizabeth Clark, Lianhui Qin, Ali Farhadi, and Yejin Choi. 2021. TuringAdvice: A generative and dynamic evaluation of language use. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4856-4880, Online. Association for Computational Linguistics.Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, \nMatthew Jagielski, Ariel Herbert-Voss, Katherine \nLee, Adam Roberts, Tom B. Brown, Dawn Xi-\naodong Song, \u00dalfar Erlingsson, Alina Oprea, and \nColin Raffel. 2021. Extracting training data from \nlarge language models. \nIn USENIX Security \nSymposium. \n\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine \nBosselut, David Golub, and Yejin Choi. 2018. \nLearning to write with cooperative discriminators. \nIn Proceedings of the 56th Annual Meeting of the \nAssociation for Computational Linguistics (Volume \n1: Long Papers), pages 1638-1649, Melbourne, \nAustralia. Association for Computational Linguis-\ntics. \n\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin \nChoi. 2020. The curious case of neural text de-\ngeneration. International Conference on Learning \nRepresentations. \n\nHugo Liu and Push Singh. 2004. Conceptnet-a practi-\ncal commonsense reasoning tool-kit. BT technology \njournal, 22(4):211-226. \n\nYann Mathet, Antoine Widl\u00f6cher, and Jean-Philippe \nM\u00e9tivier. 2015. The unified and holistic method \ngamma (\u03b3) for inter-annotator agreement mea-\nsure and alignment. Computational Linguistics, \n41(3):437-479. \n\n10 \nOpenAI blog, 1(8):9. \n\nRowan Zellers, Ari Holtzman, Hannah Rashkin, \nYonatan Bisk, Ali Farhadi, Franziska Roesner, and \nYejin Choi. 2019. Defending against neural fake \nnews. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, \nAdvances in Neural Information Processing Systems \n32, pages 9054-9065. Curran Associates, Inc. \n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q \nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint \narXiv:1904.09675. \n\n11 \n\n7260 \n\n\n\nTable 5 :\n5Bootstrap analysis (sampling 50 generations) of error counts, by category (c.v. is the coefficient of variation).\n\nTable 6 :\n6Statistics of data annotated with SCARE-\nCROW. t is the (softmax) temperature, and F.P. is a fre-\nquency penalty for already-generated words (explained \nin  \u00a75.2). GENS, ANNS, and SPANS are then number \nof generations, annotations over those generations, and \nerror spans marked during the annotations, respectively. \nWe perform the most annotations on the strongest avail-\nable generative model (GPT-3). \n\n\n\n\nAverage Span Counts (Errors Per Token)Portion of Annotated Errors Which Were True ErrorsGPT-3 \n(argmax, f.p.=1) \nHuman \n0.000 \n\n0.001 \n\n0.002 \n\n0.003 \n\n0.004 \n\n0.005 \n\n0.006 \n\n0.007 \n\n0.008 \n\nSpan counts \n\nSelf-contradiction \nRedundant \nOff-prompt \nIncoherent \nGrammar_Usage \nEncyclopedic \nCommonsense \nBad_Math \n\n0.0 \n0.2 \n0.4 \n0.6 \n0.8 \n1.0 \n\nEncyclo-\npedic \nSelf-\nContradiction \n\nIncoherent \n\nBad Math \n\nGrammar / \nUsage \nCommon-\nsense \n\nRedundant \n\nOff-Prompt \n\nGPT-3 (argmax, f.p.=1) \nHuman: Including Scraping Artifacts \nHuman: Article Fault Only \n\nEstimated True Error Counts \n\n!\"\"#$%$&'()*( \n+#,-&,. \n\n/,0& \n\n/,0& \n\n123 \n\n43 \n\n\n Median: 212s, mean: 265s, std. dev.: 199s.    6\nCounted by Stanza tokenization (Qi et al., 2020), not byte-pair encoding (BPE) or whitespace-separated tokens.\nThe difference in available references (10 for models, 9 for humans) mean this setup makes it easier for models to score higher in precision, and for humans to score higher in recall. Despite this, humans still achieve higher precision, and models still achieve higher recall. 8\nThe high double-error coverage reveals another consideration: to what depth (i.e., number of overlapping spans) will annotators mark? By the design of our framework, Incoherent errors serve as a fall-back, but without it, we might imagine poor generations splatter-painted by other error types.\nGPT-3's generations also sometimes exhibited what appeared to be formatting errors due to training on web-scraped\nThe mixed-integer programming approach is also computationally intensive; e.g., memory alone prevented us from computing alignments for pilot studies with twenty annotators, even on a machine with 500GB of RAM.\nMinimally, Needs Google spans from human-authored reputable news text should (hopefully) all be factually correct.\nAcknowledgmentsThe authors thank members of xlab for their feedback on this work. This research is supported in part by NSF (IIS-1714566), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), DARPA SemaFor program, and Allen Institute for AI.", "annotations": {"author": "[{\"end\":341,\"start\":219},{\"end\":475,\"start\":342},{\"end\":589,\"start\":476},{\"end\":720,\"start\":590},{\"end\":823,\"start\":721},{\"end\":928,\"start\":824}]", "publisher": "[{\"end\":144,\"start\":103},{\"end\":1233,\"start\":1192}]", "author_last_name": "[{\"end\":226,\"start\":223},{\"end\":356,\"start\":350},{\"end\":497,\"start\":480},{\"end\":602,\"start\":597},{\"end\":731,\"start\":727},{\"end\":836,\"start\":831}]", "author_first_name": "[{\"end\":222,\"start\":219},{\"end\":349,\"start\":342},{\"end\":479,\"start\":476},{\"end\":594,\"start\":590},{\"end\":596,\"start\":595},{\"end\":726,\"start\":721},{\"end\":828,\"start\":824},{\"end\":830,\"start\":829}]", "author_affiliation": "[{\"end\":340,\"start\":251},{\"end\":474,\"start\":385},{\"end\":588,\"start\":499},{\"end\":719,\"start\":630},{\"end\":822,\"start\":733},{\"end\":927,\"start\":838}]", "title": "[{\"end\":102,\"start\":1},{\"end\":1030,\"start\":929}]", "venue": "[{\"end\":1119,\"start\":1032}]", "abstract": "[{\"end\":2078,\"start\":1257}]", "bib_ref": "[{\"end\":4365,\"start\":4339},{\"end\":5316,\"start\":5293},{\"end\":6361,\"start\":6340},{\"end\":6437,\"start\":6406},{\"end\":6580,\"start\":6532},{\"end\":11560,\"start\":11537},{\"end\":11570,\"start\":11560},{\"end\":11800,\"start\":11776},{\"end\":11822,\"start\":11800},{\"end\":16186,\"start\":16185},{\"end\":17633,\"start\":17632},{\"end\":19440,\"start\":19406},{\"end\":19846,\"start\":19823},{\"end\":20851,\"start\":20850},{\"end\":20937,\"start\":20915},{\"end\":20990,\"start\":20989},{\"end\":21990,\"start\":21968},{\"end\":24476,\"start\":24456},{\"end\":27127,\"start\":27104},{\"end\":27146,\"start\":27129},{\"end\":27619,\"start\":27602},{\"end\":27894,\"start\":27871},{\"end\":28102,\"start\":28080},{\"end\":28477,\"start\":28452},{\"end\":28608,\"start\":28585},{\"end\":28902,\"start\":28884},{\"end\":28947,\"start\":28924},{\"end\":29096,\"start\":29073},{\"end\":36263,\"start\":36237},{\"end\":45653,\"start\":45633},{\"end\":50007,\"start\":49992},{\"end\":50024,\"start\":50007},{\"end\":50052,\"start\":50024},{\"end\":68638,\"start\":68618},{\"end\":72369,\"start\":72356},{\"end\":72562,\"start\":72541},{\"end\":72584,\"start\":72562}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":73784,\"start\":73551},{\"attributes\":{\"id\":\"fig_3\"},\"end\":73912,\"start\":73785},{\"attributes\":{\"id\":\"fig_4\"},\"end\":74252,\"start\":73913},{\"attributes\":{\"id\":\"fig_5\"},\"end\":74362,\"start\":74253},{\"attributes\":{\"id\":\"fig_6\"},\"end\":74623,\"start\":74363},{\"attributes\":{\"id\":\"fig_7\"},\"end\":74833,\"start\":74624},{\"attributes\":{\"id\":\"fig_8\"},\"end\":74951,\"start\":74834},{\"attributes\":{\"id\":\"fig_9\"},\"end\":75204,\"start\":74952},{\"attributes\":{\"id\":\"fig_10\"},\"end\":75880,\"start\":75205},{\"attributes\":{\"id\":\"fig_12\"},\"end\":76059,\"start\":75881},{\"attributes\":{\"id\":\"fig_13\"},\"end\":76191,\"start\":76060},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":76405,\"start\":76192},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":76985,\"start\":76406},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":84939,\"start\":76986},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":85065,\"start\":84940},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":85485,\"start\":85066},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":86123,\"start\":85486}]", "paragraph": "[{\"end\":3148,\"start\":2080},{\"end\":3659,\"start\":3150},{\"end\":4473,\"start\":3661},{\"end\":5046,\"start\":4475},{\"end\":5317,\"start\":5048},{\"end\":5753,\"start\":5334},{\"end\":5846,\"start\":5755},{\"end\":6719,\"start\":5848},{\"end\":7290,\"start\":6721},{\"end\":7327,\"start\":7292},{\"end\":7570,\"start\":7329},{\"end\":7906,\"start\":7572},{\"end\":8352,\"start\":7908},{\"end\":9374,\"start\":8354},{\"end\":9627,\"start\":9376},{\"end\":10134,\"start\":9629},{\"end\":10567,\"start\":10136},{\"end\":10634,\"start\":10569},{\"end\":11369,\"start\":10680},{\"end\":11900,\"start\":11371},{\"end\":12672,\"start\":11902},{\"end\":13132,\"start\":12674},{\"end\":13244,\"start\":13169},{\"end\":13624,\"start\":13270},{\"end\":14260,\"start\":13626},{\"end\":14326,\"start\":14283},{\"end\":14687,\"start\":14354},{\"end\":14877,\"start\":14689},{\"end\":15176,\"start\":14895},{\"end\":15796,\"start\":15195},{\"end\":16004,\"start\":15812},{\"end\":16454,\"start\":16006},{\"end\":17050,\"start\":16456},{\"end\":17197,\"start\":17052},{\"end\":17354,\"start\":17210},{\"end\":17391,\"start\":17356},{\"end\":17552,\"start\":17393},{\"end\":17691,\"start\":17554},{\"end\":17910,\"start\":17693},{\"end\":18240,\"start\":17926},{\"end\":18323,\"start\":18263},{\"end\":18580,\"start\":18325},{\"end\":18976,\"start\":18582},{\"end\":19093,\"start\":18996},{\"end\":19583,\"start\":19104},{\"end\":19701,\"start\":19585},{\"end\":20098,\"start\":19725},{\"end\":20441,\"start\":20133},{\"end\":20638,\"start\":20443},{\"end\":21513,\"start\":20665},{\"end\":21729,\"start\":21534},{\"end\":22303,\"start\":21731},{\"end\":22584,\"start\":22305},{\"end\":23019,\"start\":22599},{\"end\":23570,\"start\":23034},{\"end\":23644,\"start\":23572},{\"end\":23780,\"start\":23665},{\"end\":24392,\"start\":23782},{\"end\":25188,\"start\":24394},{\"end\":25905,\"start\":25190},{\"end\":26155,\"start\":25958},{\"end\":27045,\"start\":26157},{\"end\":27491,\"start\":27062},{\"end\":27980,\"start\":27493},{\"end\":28639,\"start\":27982},{\"end\":29457,\"start\":28641},{\"end\":29648,\"start\":29459},{\"end\":29716,\"start\":29681},{\"end\":30044,\"start\":29731},{\"end\":30709,\"start\":30078},{\"end\":30937,\"start\":30733},{\"end\":31062,\"start\":30965},{\"end\":31207,\"start\":31074},{\"end\":31261,\"start\":31209},{\"end\":31374,\"start\":31263},{\"end\":31512,\"start\":31376},{\"end\":31561,\"start\":31514},{\"end\":32143,\"start\":31563},{\"end\":32523,\"start\":32204},{\"end\":32641,\"start\":32535},{\"end\":32728,\"start\":32643},{\"end\":32897,\"start\":32740},{\"end\":33083,\"start\":32918},{\"end\":33218,\"start\":33095},{\"end\":33382,\"start\":33220},{\"end\":33471,\"start\":33384},{\"end\":33735,\"start\":33500},{\"end\":33954,\"start\":33747},{\"end\":34069,\"start\":33956},{\"end\":34219,\"start\":34081},{\"end\":34292,\"start\":34221},{\"end\":34475,\"start\":34313},{\"end\":34594,\"start\":34487},{\"end\":34705,\"start\":34596},{\"end\":34857,\"start\":34717},{\"end\":34937,\"start\":34859},{\"end\":35049,\"start\":34960},{\"end\":35251,\"start\":35068},{\"end\":35392,\"start\":35263},{\"end\":35515,\"start\":35394},{\"end\":35622,\"start\":35527},{\"end\":35875,\"start\":35644},{\"end\":35942,\"start\":35877},{\"end\":36049,\"start\":35954},{\"end\":36176,\"start\":36051},{\"end\":36264,\"start\":36178},{\"end\":36344,\"start\":36276},{\"end\":36386,\"start\":36346},{\"end\":36438,\"start\":36388},{\"end\":36668,\"start\":36450},{\"end\":36703,\"start\":36670},{\"end\":36827,\"start\":36726},{\"end\":36977,\"start\":36829},{\"end\":37050,\"start\":36979},{\"end\":37190,\"start\":37052},{\"end\":37306,\"start\":37202},{\"end\":37343,\"start\":37308},{\"end\":37496,\"start\":37365},{\"end\":37623,\"start\":37523},{\"end\":37748,\"start\":37635},{\"end\":37929,\"start\":37750},{\"end\":38064,\"start\":37941},{\"end\":38493,\"start\":38087},{\"end\":39018,\"start\":38495},{\"end\":39212,\"start\":39020},{\"end\":39375,\"start\":39256},{\"end\":39508,\"start\":39387},{\"end\":39622,\"start\":39510},{\"end\":39789,\"start\":39634},{\"end\":40315,\"start\":39791},{\"end\":40486,\"start\":40317},{\"end\":40672,\"start\":40540},{\"end\":40922,\"start\":40803},{\"end\":41112,\"start\":41029},{\"end\":41323,\"start\":41114},{\"end\":41479,\"start\":41325},{\"end\":41599,\"start\":41481},{\"end\":41760,\"start\":41703},{\"end\":41939,\"start\":41893},{\"end\":42960,\"start\":42199},{\"end\":43131,\"start\":42984},{\"end\":43334,\"start\":43133},{\"end\":43739,\"start\":43336},{\"end\":44290,\"start\":43741},{\"end\":44563,\"start\":44309},{\"end\":45291,\"start\":44565},{\"end\":46699,\"start\":45293},{\"end\":46764,\"start\":46701},{\"end\":47914,\"start\":46766},{\"end\":48148,\"start\":47939},{\"end\":48898,\"start\":48172},{\"end\":49555,\"start\":48900},{\"end\":50331,\"start\":49574},{\"end\":50931,\"start\":50341},{\"end\":51108,\"start\":50933},{\"end\":51296,\"start\":51110},{\"end\":51617,\"start\":51298},{\"end\":52610,\"start\":51644},{\"end\":53464,\"start\":52612},{\"end\":53481,\"start\":53466},{\"end\":53966,\"start\":53483},{\"end\":54450,\"start\":53984},{\"end\":54565,\"start\":54452},{\"end\":55123,\"start\":54575},{\"end\":55241,\"start\":55145},{\"end\":55923,\"start\":55243},{\"end\":56459,\"start\":55925},{\"end\":57483,\"start\":56492},{\"end\":58026,\"start\":57485},{\"end\":58859,\"start\":58028},{\"end\":59101,\"start\":58889},{\"end\":59542,\"start\":59103},{\"end\":61118,\"start\":59544},{\"end\":61550,\"start\":61120},{\"end\":62293,\"start\":61552},{\"end\":62856,\"start\":62295},{\"end\":63646,\"start\":62871},{\"end\":64040,\"start\":63648},{\"end\":65569,\"start\":64067},{\"end\":65855,\"start\":65571},{\"end\":66029,\"start\":65873},{\"end\":66454,\"start\":66063},{\"end\":66824,\"start\":66456},{\"end\":67331,\"start\":66826},{\"end\":67520,\"start\":67333},{\"end\":68217,\"start\":67555},{\"end\":68509,\"start\":68219},{\"end\":69121,\"start\":68511},{\"end\":69195,\"start\":69156},{\"end\":69223,\"start\":69197},{\"end\":69257,\"start\":69225},{\"end\":69328,\"start\":69259},{\"end\":69486,\"start\":69330},{\"end\":69548,\"start\":69488},{\"end\":69574,\"start\":69550},{\"end\":69716,\"start\":69590},{\"end\":69785,\"start\":69718},{\"end\":70249,\"start\":69787},{\"end\":70590,\"start\":70251},{\"end\":71284,\"start\":70592},{\"end\":71500,\"start\":71313},{\"end\":72192,\"start\":71502},{\"end\":72977,\"start\":72194},{\"end\":73323,\"start\":72998},{\"end\":73550,\"start\":73325}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20132,\"start\":20099},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20664,\"start\":20639},{\"attributes\":{\"id\":\"formula_2\"},\"end\":25957,\"start\":25906}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3779,\"start\":3771},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6055,\"start\":6048},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17093,\"start\":17086},{\"end\":25445,\"start\":25438},{\"end\":26172,\"start\":26165},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29261,\"start\":29254},{\"end\":45052,\"start\":45045},{\"end\":45310,\"start\":45303},{\"end\":46056,\"start\":46049},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":47257,\"start\":47250},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":47988,\"start\":47981}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":5332,\"start\":5320},{\"attributes\":{\"n\":\"3\"},\"end\":10678,\"start\":10637},{\"attributes\":{\"n\":\"4\"},\"end\":13167,\"start\":13135},{\"attributes\":{\"n\":\"4.1\"},\"end\":13268,\"start\":13247},{\"end\":14281,\"start\":14263},{\"end\":14334,\"start\":14329},{\"end\":14352,\"start\":14337},{\"attributes\":{\"n\":\"4.2\"},\"end\":14893,\"start\":14880},{\"attributes\":{\"n\":\"4.3\"},\"end\":15193,\"start\":15179},{\"attributes\":{\"n\":\"4.4\"},\"end\":15810,\"start\":15799},{\"attributes\":{\"n\":\"4.5\"},\"end\":17208,\"start\":17200},{\"attributes\":{\"n\":\"4.6\"},\"end\":17924,\"start\":17913},{\"attributes\":{\"n\":\"4.7\"},\"end\":18261,\"start\":18243},{\"attributes\":{\"n\":\"5\"},\"end\":18994,\"start\":18979},{\"attributes\":{\"n\":\"5.1\"},\"end\":19102,\"start\":19096},{\"attributes\":{\"n\":\"5.2\"},\"end\":19723,\"start\":19704},{\"attributes\":{\"n\":\"5.3\"},\"end\":21532,\"start\":21516},{\"attributes\":{\"n\":\"5.4\"},\"end\":22597,\"start\":22587},{\"attributes\":{\"n\":\"5.5\"},\"end\":23032,\"start\":23022},{\"attributes\":{\"n\":\"6\"},\"end\":23663,\"start\":23647},{\"attributes\":{\"n\":\"7\"},\"end\":27060,\"start\":27048},{\"end\":29679,\"start\":29651},{\"attributes\":{\"n\":\"8\"},\"end\":29729,\"start\":29719},{\"end\":30076,\"start\":30047},{\"end\":30731,\"start\":30712},{\"end\":30963,\"start\":30940},{\"end\":31072,\"start\":31065},{\"end\":32152,\"start\":32146},{\"end\":32173,\"start\":32155},{\"end\":32184,\"start\":32176},{\"end\":32202,\"start\":32187},{\"end\":32533,\"start\":32526},{\"end\":32738,\"start\":32731},{\"end\":32916,\"start\":32900},{\"end\":33093,\"start\":33086},{\"end\":33498,\"start\":33474},{\"end\":33745,\"start\":33738},{\"end\":34079,\"start\":34072},{\"end\":34311,\"start\":34295},{\"end\":34485,\"start\":34478},{\"end\":34715,\"start\":34708},{\"end\":34958,\"start\":34940},{\"end\":35066,\"start\":35052},{\"end\":35261,\"start\":35254},{\"end\":35525,\"start\":35518},{\"end\":35642,\"start\":35625},{\"end\":35952,\"start\":35945},{\"end\":36274,\"start\":36267},{\"end\":36448,\"start\":36441},{\"end\":36724,\"start\":36706},{\"end\":37200,\"start\":37193},{\"end\":37363,\"start\":37346},{\"end\":37521,\"start\":37499},{\"end\":37633,\"start\":37626},{\"end\":37939,\"start\":37932},{\"end\":38085,\"start\":38067},{\"end\":39254,\"start\":39215},{\"end\":39385,\"start\":39378},{\"end\":39632,\"start\":39625},{\"end\":40491,\"start\":40489},{\"end\":40505,\"start\":40494},{\"end\":40516,\"start\":40508},{\"end\":40538,\"start\":40519},{\"end\":40801,\"start\":40675},{\"end\":41027,\"start\":40925},{\"end\":41701,\"start\":41602},{\"end\":41891,\"start\":41763},{\"end\":42070,\"start\":41942},{\"end\":42197,\"start\":42073},{\"end\":42982,\"start\":42963},{\"end\":44307,\"start\":44293},{\"end\":47937,\"start\":47917},{\"end\":48170,\"start\":48151},{\"end\":49572,\"start\":49558},{\"end\":50339,\"start\":50334},{\"end\":51642,\"start\":51620},{\"end\":53982,\"start\":53969},{\"end\":54573,\"start\":54568},{\"end\":55143,\"start\":55126},{\"end\":56490,\"start\":56462},{\"end\":58887,\"start\":58862},{\"end\":62869,\"start\":62859},{\"end\":64065,\"start\":64043},{\"end\":65871,\"start\":65858},{\"end\":66061,\"start\":66032},{\"end\":67553,\"start\":67523},{\"end\":69142,\"start\":69124},{\"end\":69154,\"start\":69145},{\"end\":69588,\"start\":69577},{\"end\":71311,\"start\":71287},{\"end\":72996,\"start\":72980},{\"end\":73558,\"start\":73552},{\"end\":73921,\"start\":73914},{\"end\":74374,\"start\":74364},{\"end\":74636,\"start\":74625},{\"end\":74846,\"start\":74835},{\"end\":74964,\"start\":74953},{\"end\":75217,\"start\":75206},{\"end\":75893,\"start\":75882},{\"end\":76072,\"start\":76061},{\"end\":76202,\"start\":76193},{\"end\":76416,\"start\":76407},{\"end\":84950,\"start\":84941},{\"end\":85076,\"start\":85067}]", "table": "[{\"end\":76985,\"start\":76418},{\"end\":84939,\"start\":83224},{\"end\":85485,\"start\":85078},{\"end\":86123,\"start\":85576}]", "figure_caption": "[{\"end\":73784,\"start\":73560},{\"end\":73912,\"start\":73787},{\"end\":74252,\"start\":73922},{\"end\":74362,\"start\":74255},{\"end\":74623,\"start\":74376},{\"end\":74833,\"start\":74639},{\"end\":74951,\"start\":74849},{\"end\":75204,\"start\":74967},{\"end\":75880,\"start\":75220},{\"end\":76059,\"start\":75896},{\"end\":76191,\"start\":76075},{\"end\":76405,\"start\":76204},{\"end\":83224,\"start\":76988},{\"end\":85065,\"start\":84952},{\"end\":85576,\"start\":85488}]", "figure_ref": "[{\"end\":3658,\"start\":3650},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5508,\"start\":5500},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6262,\"start\":6253},{\"end\":6718,\"start\":6709},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6998,\"start\":6989},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7079,\"start\":7071},{\"end\":7145,\"start\":7137},{\"end\":7171,\"start\":7163},{\"end\":7246,\"start\":7238},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7420,\"start\":7411},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7665,\"start\":7657},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7997,\"start\":7989},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8424,\"start\":8409},{\"end\":8679,\"start\":8671},{\"end\":9046,\"start\":9038},{\"end\":9091,\"start\":9083},{\"end\":9503,\"start\":9494},{\"end\":9687,\"start\":9679},{\"end\":10249,\"start\":10241},{\"end\":14393,\"start\":14385},{\"end\":15689,\"start\":15681},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17845,\"start\":17836},{\"end\":18689,\"start\":18681},{\"end\":30190,\"start\":30182},{\"end\":31981,\"start\":31973},{\"end\":39141,\"start\":39133},{\"end\":43944,\"start\":43936},{\"end\":44946,\"start\":44938},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47357,\"start\":47349},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":48098,\"start\":48089},{\"end\":48147,\"start\":48138},{\"end\":48283,\"start\":48275},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":49030,\"start\":49022},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":49119,\"start\":49110},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":49551,\"start\":49541},{\"end\":50652,\"start\":50643},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52010,\"start\":52002},{\"end\":53742,\"start\":53734},{\"end\":53868,\"start\":53860},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":54033,\"start\":54024},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":55550,\"start\":55541},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":56008,\"start\":55989},{\"end\":56860,\"start\":56854},{\"end\":57562,\"start\":57553},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":57704,\"start\":57695},{\"end\":57732,\"start\":57723},{\"end\":58113,\"start\":58105},{\"end\":58273,\"start\":58264},{\"end\":58818,\"start\":58810},{\"end\":58936,\"start\":58927},{\"end\":59383,\"start\":59374},{\"end\":59745,\"start\":59736},{\"end\":60196,\"start\":60187},{\"end\":61215,\"start\":61206},{\"end\":61323,\"start\":61314},{\"end\":61371,\"start\":61362},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":63084,\"start\":63075},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":63361,\"start\":63352},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":64196,\"start\":64187},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":65011,\"start\":65002},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":65140,\"start\":65131},{\"end\":66264,\"start\":66256},{\"end\":66681,\"start\":66673},{\"end\":67818,\"start\":67809},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":67944,\"start\":67935}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}