{"id": 71142966, "updated": "2023-04-05 03:55:16.819", "metadata": {"title": "FusionGAN: A generative adversarial network for infrared and visible image fusion", "authors": "[{\"first\":\"Jiayi\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Pengwei\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Chang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Junjun\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "Information Fusion", "journal": "Information Fusion", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": ".", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2912147220", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/inffus/MaYLLJ19", "doi": "10.1016/j.inffus.2018.09.004"}}, "content": {"source": {"pdf_hash": "d48a02f7493a506e6270e34238e3fb2bceeb3e73", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cba7207b5a86293176ac230dc343b5240a42840f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d48a02f7493a506e6270e34238e3fb2bceeb3e73.txt", "contents": "\nTo appear in: Information Fusion\n\n\nJiayi Ma \nWei Yu \nPengwei Liang \nChang Li \nJunjun Jiang \nJiayi Ma \nWei Yu \nPengwei Liang \nChang Li \nJunjun Jiang \nJiayi Ma \nElectronic Information School\nWuhan University\n430072WuhanChina\n\nWei Yu \nElectronic Information School\nWuhan University\n430072WuhanChina\n\nPengwei Liang \nElectronic Information School\nWuhan University\n430072WuhanChina\n\nChang Li \nDepartment of Biomedical Engineering\nHefei University of Technology\n230009HefeiChina\n\nJunjun Jiang \nSchool of Computer Science and Technology\nHarbin Institute of Technology\n150001HarbinChina\n\nTo appear in: Information Fusion\n10.1016/j.inffus.2018.09.004Received date: 10 February 2018 Revised date: 30 June 2018 Accepted date: 3 September 2018 Preprint submitted to Information Fusion September 4, 2018Please cite this article as: FusionGAN: A generative adversarial network for infrared and visible image fusion, Information Fusion (2018), doi: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain. ACCEPTED MANUSCRIPT A C C E P T E D M A N U S C R I P T 1 ACCEPTED MANUSCRIPT A C C E P T E D M A N U S C R I P T ACCEPTED MANUSCRIPT A C C E P T E D M A N U S C R I P TImage fusioninfrared imagevisible imagegenerative adversarial networkdeep learning\nHighlights\u2022 We propose a new IR/VIS fusion method based on Generative Adversarial Networks.\u2022 It can keep both the thermal radiation and the texture details in the source images.\u2022 It is an end-to-end model and does not need to design fusion rules manually.\u2022 Our results look like sharpened IR images with highlighted target and abundant textures.\u2022 We generalize it to fuse images with different resolutions like thermal pan-sharpening.AbstractInfrared images can distinguish targets from their backgrounds on the basis of difference in thermal radiation, which works well at all day/night time and under all weather conditions. By contrast, visible images can provide texture details with high spatial resolution and definition in a manner consistent with the human visual system. This paper proposes a novel method to fuse these two types of information using a generative adversarial network, termed as FusionGAN. Our method establishes an adversarial game between a generator and a discriminator, where the generator aims to generate a fused image with major infrared intensities together with additional visible gradients, and the discriminator aims to force the fused image to have more details existing in visible images. This enables that the final fused image simultaneously keeps the thermal radiation in a infrared image and the textures in a visible image. In addition, our FusionGAN is an end-to-end model, avoiding manually designing complicated activity level measurements and fusion rules as in traditional methods. Experiments on public datasets demonstrate the superiority of our strategy over state-of-the-arts, where our results look like sharpened infrared images with clear highlighted targets and abundant details. Moreover, we also generalize our FusionGAN to fuse images with different resolutions, say a low-resolution infrared image and a high-resolution visible image. Extensive results demonstrate that our strategy can generate clear and clean fused images which do not suffer from noise caused by upsampling of infrared information.\n\nIntroduction\n\nImage fusion is an enhancement technique that aims to combine images obtained by different kinds of sensors to generate a robust or informative image that can facilitate subsequent processing or help in decision making [1,2]. Particularly, multi-sensor data such as thermal infrared and visible images has been used to enhance the performance in terms of human visual perception, object detection, and target recognition [3]. For example, infrared images capture thermal radiation, whereas visible images capture reflected light. These two types of images can provide scene information from different aspects with complementary properties, and they are also inherent in nearly all objects [4].\n\nThe image fusion problem has been developed with different schemes including multiscale transform- [5,6,7], sparse representation- [8,9], neural network- [10,11], subspace- [12,13], and saliency-based [14,15] methods, hybrid models [16,17], and other methods [18,19].\n\nNevertheless, the major fusion framework involves three key components, including image transform, activity level measurement, and fusion rule designing [20]. Existing methods typically use the same transform or representation for different source images during the fusion process. However, it may not be appropriate for infrared and visible images, as the thermal radiation in infrared images and the appearance in visible images are manifestations of two different phenomena. In addition, the activity level measurement and fusion rule in most existing methods are designed in a manual way, and they have become more and more complex, having the limitations of implementation difficulty and computational cost [21].\n\nTo overcome the above mentioned issues, in this paper we propose an infrared and visible image fusion method from a novel perspective based on generative adversarial network (FusionGAN ), which formulates the fusion as an adversarial game between keeping the infrared thermal radiation information and preserving the visible appearance texture information. More specifically, it can be seen as a minimax problem between a generator and a discriminator. The generator attempts to generate a fused image with major infrared intensities together with additional visible gradients, while the discriminator aims to force the fused image to have more texture details. This enables our fused image to maintain the thermal radiation in a infrared image and the texture details in a visible image at the same time. In addition, the end-to-end property of generative adversarial networks (GANs) can avoid manually designing complicated activity level measurements and fusion rules.\n\nTo show the major superiority of our method, we give a representative example in Fig. 1.\n\nThe left two images are the infrared and visible images to be fused, where the visible image contains detailed background and the infrared image highlights the target, i.e. the water.\n\nThe third image is the fusion result by using a recent method [22]. Clearly, this traditional method is just able to keep more texture details in source images, and the property of high contrast between target and background in the infrared image cannot be preserved in the fused image. In fact, the key information in the infrared image (i.e., the thermal radiation distribution) is totally lost in the fused image. The rightmost image in Fig. 1 is the fusion result by our FusionGAN. In contrast, our result preserves the thermal radiation distribution in the infrared image, and hence the target can be easily detected. Meanwhile, the details of the background (i.e., the trees, road and water plants) in the visible image are also well retained.\n\nThe main contributions of this work lie in the following four folds. First, we propose a generative adversarial architecture and design a loss function specialized for infrared and visible image fusion. The feasibility and superiority of GANs used for image fusion are also A C C E P T E D M A N U S C R I P T discussed. To the best of our knowledge, it is the first time that the GANs are adopted for addressing the image fusion task. Second, the proposed FusionGAN is an end-to-end model, where the fused image can be generated automatically from input source images without manually designing the activity level measurement or fusion rule. Third, we conduct experiments on public infrared and visible image fusion datasets with qualitative and quantitative comparisons to state-of-the-art methods. Compared to previous methods, the proposed Fu-sionGAN can obtain results looking like sharpened infrared images with clear highlighted targets and abundant textures. Last but not the least, we generalize the proposed Fusion-GAN to fuse source images with different resolutions such as low-resolution infrared images and high-resolution visible images. It can generate high-resolution resulting images which do not suffer from noise caused by upsampling of infrared information.\n\nThe rest of this paper is arranged as follows. Section 2 describes background material and related work on GAN. In Section 3, we present our FusionGAN algorithm for infrared and visible image fusion. Section 4 illustrates the fusion performance of our method on various types of infrared and visible image/video pairs with comparisons to other approaches. We discuss the explainability of our FusionGAN in Section 5, followed by some concluding remarks in Section 6.\n\n\nRelated work\n\nIn this section, we briefly introduce the background material and relevant works, including traditional infrared and visible image fusion methods, deep learning based fusion techniques, as well as GANs and their variants.\n\n\nInfrared and visible image fusion\n\nWith the fast-growing demands of image representation methods, there are quantities of image fusion methods that have been proposed. They can be simply divided into seven categories including multi-scale transform- [5,6,7], sparse representation- [8,9], neural network- [10,11], subspace- [12,13], and saliency-based [14,15] methods, hybrid models [16,17], and other methods [18,19]. Next, we briefly discuss the main ideas of these methods.\n\n\nA C C E P T E D M A N U S C R I P T\n\nMulti-scale transform-based methods are the most popular in image fusion, and multiscale transform can decompose original images into components of different scales, where each component represents the sub-image at each scale and real-world objects typically comprise components at different scales [23]. In general, infrared and visible image fusion schemes based on multi-scale transforms comprise three steps [23]. First, each source image is decomposed into a series of multi-scale representations. Then, the multi-scale representations of the source image are fused according to a given fusion rule. Finally, the fused image is acquired using corresponding inverse multi-scale transforms on the fused representations.\n\nSparse representation image fusion methods aim to learn an over-complete dictionary from a large number of high-quality natural images. Then, the source images can be sparsely represented by the learned dictionary, thereby potentially enhancing the representation of meaningful and stable images [24]. Meanwhile, sparse representation-based fusion methods divide source images into several overlapping patches using a sliding window strategy, thereby potentially reducing visual artifacts and improving robustness to misregistration [16]. Neural network-based methods imitate the perception behavior of the human brain to deal with neural information, the interactions among neurons characterize the transmission and processing of neuron information, and the neural network has the advantages of strong adaptability and fault tolerance and anti-noise capabilities, most neural networkbased infrared and visible image fusion methods adopt the pulse-coupled neural network or its variants [10]. Subspace-based methods aim to project high-dimensional input images into low-dimensional spaces or subspaces. For most natural images, redundant information exists and low-dimensional subspaces can help capture the intrinsic structures of the original images. Thus, subspace-based methods, including principal component analysis, non-negative matrix factorization, and independent component analysis, have been successfully applied in infrared and visible image fusion [12]. Saliency-based methods are based on the fact that the attention is often captured by objects or pixels that are more significant than their neighbors, and saliency-based fusion methods can maintain the integrity of the salient object region and improve the visual quality of the fused image [14]. The above mentioned infrared and visible image fusion methods all have their advantages and disadvantages, and hybrid A C C E P T E D M A N U S C R I P T models combine their advantages to improve the image fusion performance [16]. Other infrared and visible image fusion methods can inspire new ideas and perspectives for image fusion, which are based on total variation [18], fuzzy theory [25], entropy [19] and so on.\n\n\nDeep learning based image fusion\n\nIn recent years, deep learning has also been successfully applied to image fusion, due to its strong ability of extracting image features. In multi-focus image fusion, Liu et al.\n\n[26] trained a deep convolutional neural network (CNN) to jointly generate activity level measurement and fusion rule, and they also applied their model to fuse infrared and visible images [27]. In multi-modality image fusion, Zhong et al. [28] proposed a joint image fusion and super-resolution method based on CNN. Besides, Liu et al. [29] introduced the convolution sparse representation for image fusion, in which deconvolutional networks intend to build a hierarchy of layers, and each layer consists of an encoder and a decoder. In remote sensing image fusion, Masi et al. [30] proposed an effective three-layer architecture to solve the pansharpening problem, where the input is augmented by adding several maps of nonlinear radiometric indices to promote the fusion performance.\n\nThe existing deep learning based image fusion techniques typically rely on the CNN model, which has a critical prerequisite that the ground truth should be available in advance.\n\nFor the multi-focus image fusion and pansharpening problems, the ground truth is well defined, for example, a clear image without blurred regions or a multispectral image with the same resolution as the corresponding panchromatic image. However, in the task of infrared and visible image fusion, defining a standard for fused images is unrealistic, and hence, establishing the ground truth is not considered. On this basis, rather than learning an end-to-end model which requires ground truth fused images, the existing techniques for infrared and visible image fusion just learn a deep model to determine the blurring degree of each patch in the source images, and then calculate a weight map accordingly to generate the final fused image [27]. In this paper, we formulate the fusion problem in the framework of GAN, which does not suffer from the aforementioned problem.\n\nA C C E P T E D M A N U S C R I P T\n\n\nGenerative adversarial networks and its variants\n\nGAN is a popular framework for estimating generative models via an adversarial process, and deep convolutional GANs (DCGANs) successfully introduce a class of CNNs into GANs, while the least squares generative adversarial networks (LSGANs) overcome the vanishing gradients problem in regular GANs, which are more stable during the learning process. Next, we will briefly introduce the above mentioned three related techniques.\n\n\nGenerative adversarial networks\n\nGoodfellow et al. [31] first proposed the concept of GAN, which has drawn substantial attention in the field of deep learning. The GAN is based on the minmax two-player game, which can provide a simple yet powerful way to estimate target distribution and generate new samples. The GAN framework consists of two adversarial models: a generative model G and a discriminative model D. The generative model G can capture the data distribution, and the discriminative model D can estimate the probability that a sample comes from the training data rather than G. More specifically, the GAN establishes an adversarial game between a discriminator and a generator, the generator takes the noise whose prior distribution is P z as input and tries to generate different samples to fool the discriminator, and the discriminator aims to determine whether a sample is from the model distribution or the data distribution, finally the generator generates samples that are not distinguishable by the discriminator.\n\nMathematically, a generative model G aims to generate samples, whose distribution (P G ) tries to approximate the distribution (P data ) of real training data, G and D play the minimax two-player game as follows: except the last activation layer, and the last layer is tanh activation. Last but not the least, all activation layers in discriminator are leaky ReLU activations. Thus, the training process becomes more steady, and the quality of generated results can be improved.\nmin G max D V GAN (G, D) = E x\u223cp data (x) [log D(x)] + E z\u223cpz(z) [log(1 \u2212 D(G(z)))].(1)\n\nLeast squares GANs\n\nDespite the great success GANs have achieved, there still exits two critical issues to be solved. The first is how to improve the quality of generated images. In recent years, many works have been proposed to solve this problem, such as DCGANs. The second is how to improve the stability of training process. A lot of works have been proposed to deal with this problem by exploring the objective functions of GANs, such as Wasserstein GANs (WGANs) [33], which converge much slowly than regular GANs [34]. In addition, the regular GANs adopt the sigmoid cross entropy loss function for the discriminator, which may lead to the gradient-vanishing problem during the learning process. To overcome the above mentioned two problems, Mao et al. [34] proposed the least squares generative adversarial networks (LSGANs), which adopt the least squares loss function for the discriminator, and\nA C C E P T E D M A N U S C R I P T\nthe objective functions for LSGANs are defined as follows\nmin D V LSGAN (D) = 1 2 E x\u223cp data (x) [(D(x) \u2212 b) 2 ] + 1 2 E z\u223cpz(z) [(D(G(z)) \u2212 a) 2 ], min G V LSGAN (G) = 1 2 E z\u223cpz(z) [(D(G(z)) \u2212 c) 2 ],(2)\nwhere the coding scheme is used for both the discriminator and generator, a and b denote the labels for fake data and real data, respectively, and c denotes the value that the generator wants the discriminator to believe for fake data. There are two methods to determine the values of a, b and c in Eq. (2). The first is to set the b -c = 1 and b -a = 2, thus minimizing\n\nEq.\n\n(2) yields minimizing the Pearson \u03c7 2 between P data +P g and P g . The second is to set c = b, which can make samples generated by generator as real as possible. The above mentioned two methods usually get similar performance.\n\nIn LSGANs, penalizing the samples that lie in a long way to the decision boundary make the samples generated by generator close to the decision boundary and generate more gradients. Thus, LSGANs have two advantages over regular GANs. On the one hand, LS-GANs can generate higher quality images than regular GANs. On the other hand, LSGANs perform more stable than regular GANs during the training process.\n\n\nMethod\n\nIn this section, we describe the proposed FusionGAN for infrared and visible image fusion.\n\nWe start by laying out the problem formulation with GANs, and then discuss the network architectures of the generator and the discriminator. Finally, we provide some details for the network training.\n\n\nProblem formulation\n\nTo keep the thermal radiation information of infrared image and the abundant texture information of visible image simultaneously, we propose a new fusion strategy from a novel perspective. We formulate the infrared and visible image fusion problem as an adversarial problem, as schematically illustrated in Fig. 2 (a). At the beginning, we concatenate the infrared image I r and the visible image I v in the channel dimension. Then, the concatenated\nA C C E P T E D M A N U S C R I P T Adversarial Infrared image ( ) Visible image ( ) Concat Discriminator Generator Fused image ( ) Trained Generator Final fused image ( ) Infrared image ( ) Visible image ( ) Concat\nTesting process Training process image is fed into the generator G \u03b8 G , and the output of G \u03b8 G is a fused image I f . Due to the loss function of the generator designed in this paper (explained later), without the discriminator D \u03b8 D , I f tends to keep the thermal radiation information of infrared image I r and preserve the gradient information of visible image I v . After that, we input the fused image I f and the visible image I v into the discriminator D \u03b8 D , which aims to distinguish I f from I v . The proposed FusionGAN establishes an adversarial game between the generator G \u03b8 G and the discriminator D \u03b8 D , and I f will gradually contain more and more detail information in visible image I v . During the training phase, once the generator G \u03b8 G generates samples (i.e., I f ) that cannot be distinguished by the discriminator D \u03b8 D , we can obtain the expected fused image I f . The testing process is shown in Fig. 2 (b), we only input the concatenating image of I f and I v into the trained generator G \u03b8 G , and the output of G \u03b8 G is our final fused result.\n\nLoss function. The loss function of our FusionGAN is consist of two parts, i.e., the loss function of generator G \u03b8 G and the loss function of discriminator D \u03b8 D . In the following, we will introduce them separately. First, the loss function of generator G \u03b8 G consists of two\nA C C E P T E D M A N U S C R I P T terms: L G = V FusionGAN (G) + \u03bbL content ,(3)\nwhere L G denotes the total loss, the first term V FusionGAN (G) on the right hand denotes the adversarial loss between generator G \u03b8 G and discriminator D \u03b8 D , which is defined as follows:\nV FusionGAN (G) = 1 N N n=1 D \u03b8 D (I n f ) \u2212 c 2 ,(4)\nwhere I n f denotes the fused image with n \u2208 II N N and N denoting the number of fused images, and c is the value that generator wants discriminator to believe for fake data.\n\nThe second term L content represents the content loss, and \u03bb is used to strike a balance between V FusionGAN (G) and L content . Due to that the thermal radiation information of infrared image is characterized by its pixel intensities and the texture detail information of visible image can be partly characterized by its gradients [18], we enforce the fused image I f to have similar intensities as I r and similar gradients as I v . Specifically, L content is defined as follows:\nL content = 1 HW ( I f \u2212 I r 2 F + \u03be \u2207I f \u2212 \u2207I v 2 F ),(5)\nwhere H and W represents the height and width of the input images, respectively, \u00b7 F stands for the matrix Frobenius norm, and \u2207 means the gradient operator. The first term of L content aims to keep the thermal radiation information of infrared image I r in the fused image I f , the second term of L content aims to preserve the gradient information contained in the visible image I v , and \u03be is a positive parameter controlling the trade-off between two terms.\n\nActually, without D \u03b8 D , we can also get a fused image, which can keep thermal radiation information in the infrared image and gradient information in the visible image. But that is often not enough, because the texture details in a visible image cannot be totally represented by using only gradient information (we will validate this issue in our experiments). Therefore, we establish an adversarial game between a generator G \u03b8 G and a discriminator D \u03b8 D to adjust the fused image I f based on the visible image I v . This can make I f contain more texture details. Formally, the loss function of discriminator D \u03b8 D is defined as follows:\nA C C E P T E D M A N U S C R I P TL D = 1 N N n=1 D \u03b8 D (I v ) \u2212 b 2 + 1 N N n=1 D \u03b8 D (I f ) \u2212 a 2 ,(6)\nwhere a and b denote the labels of fused image I f and visible image\nI v , respectively, D \u03b8 D (I v )\nand D \u03b8 D (I f ) denote the classification results of the visible and fused images, respectively.\n\nThe discriminator is designed to distinguish the fused images from the visible images based on the features extracted from them. We use the least square loss function, which obeys minimizing the Pearson \u03c7 2 divergence. It makes the training process more steady and the loss function of discriminator converge quickly.\n\n\nNetwork architecture\n\nOur network architecture consists of two parts, i.e., the generator and the discriminator.\n\nArchitectures of them are designed based on the convolution neural network.\n\nNetwork architecture of generator. Our network architecture of generator G \u03b8 G is presented in Fig. 3. As shown, G \u03b8 G is a simple five-layer convolution neural network, where 5 \u00d7 5 filters are used in the first and second layers, 3 \u00d7 3 filters in the third and fourth layers, and 1 \u00d7 1 filters in the last layer. The stride in each layer is set to 1, and there is no padding operation in convolution. Our generator's input is a concatenated image without noise. To improve the diversity of generated images, many works usually extract feature maps of input image by convolution layers, and then reconstruct an image to the same size of input image by transposed convolution layer. For infrared and visible image fusion, every down-sampling process will drop out some detail information in the source images, which is important for fusion [35]. Therefore, we only introduce the convolution layer without down-sampling. This can also keep the sizes of input and output the same, and hence, the transposed convolution layer is unnecessary in our network. In addition, to avoid the problem of vanishing gradient, we follow the rules of deep convolutional GAN [32] for batch normalization and activation function. To overcome the sensitivity to data initialization, we employ the batch normalization in the first four layers, and the batch normalization layer can make our model more stable and also can help the gradients to back propagate to every layer effectively. For activation function, we use leaky ReLU activation function in the first four layers, and the tanh activation function is used in the last layer.\nA C C E P T E D M A N U S C R I P T\nNetwork architecture of discriminator. Our network architecture of discriminator D \u03b8 D is a simple five-layer convolution neural network, which is shown in Fig. 4. From the first layer to the fourth layer, we use 3 \u00d7 3 filters in convolution layers and set the stride to 2 without padding. This is different from the generator network. The underlying reason is that the discriminator is a classifier, and it first extracts feature maps from the input images and then classifies them. Therefore, it works the same way as pooling layer by setting the stride to 2. In order not to introduce noise in our model, we only perform padding operation\nA C C E P T E D M A N U S C R I P T\non input images in the first layer, and no padding is performed in the rest three convolution layers. From the second layer to fourth layer, we use batch normalization layer. In addition, we use leaky ReLU activation function in the first four layers. The last layer is linear layer, which is mainly used for classification.\n\n\nTraining details\n\nWe select 45 pairs of infrared and visible images with different scenes from TNO database as our training data. However, it is insufficient to train a good model, so we crop each image by setting the stride to 14, and each patch is of the same size 120 \u00d7 120 1 . Thus, we can get 64, 381 pairs of infrared and visible patches, and make them centralized to [\u22121, 1]. We select m pairs of infrared and visible patches from the training data, and then pad them to the size of 132 \u00d7 132, which are used as the input of the generator. The fused image patch output by the generator is of size 120 \u00d7 120. Next, we use m pairs of visible and fused image patches as the input of discriminator. We first train the discriminator k times, and the optimizer solver is Adam [36], then we train the generator until reaching the maximum number of training iterations. The procedure is summarized in Alg. 1. In the testing process, we crop the testing data without overlapping, and input them into the generator G \u03b8 G as a batch.\n\nThen we connect the results of generator according to the sequence of cropping, thus we can get the final fused image. The parameter setting will be discussed in the next section.\n\n\nExperiments\n\nIn this section, we first briefly introduce the fusion metrics used in this paper and then demonstrate the efficacy of the proposed FusionGAN on public datasets, and compare it with eight state-of-the-art fusion methods including adaptive sparse representation (ASR) [37], curvelet transform (CVT) [38], dual-tree complex wavelet transform (DTCWT) [39], fourth order partial differential equation (FPDE) [12], guided filtering based fusion (GFF) 1 Note that using smaller image patch size will make training process time-saving. However, for infrared images, small patch size usually leads to few valid information. For example, the image patch may be just a black or white patch without textures. Clearly, this will adversely affect the training process. (1)\nf , \u00b7 \u00b7 \u00b7 , I (m) f } from G \u03b8 G ; 4 Select m visible patches {I (1) v , \u00b7 \u00b7 \u00b7 , I (m) v }; 5\nUpdate discriminator by AdamOptimizer: \n\u2207 \u03b8 D 1 N N n=1 D \u03b8 D (I v ) \u2212 b 2 + 1 N N n=1 D \u03b8 D (I f ) \u2212 a 2 ;\n\nFusion metrics\n\nSince it is difficult to get an accurate evaluation of the fusion performance only by subjective evaluation, we also need fusion metrics for objective evaluation. In recent years, many studies have proposed various fusion metrics, but it seems that none of them is definitely better than the others [41]. Therefore, it is necessary to choose multiple metrics to evaluate A C C E P T E D M A N U S C R I P T different fusion methods. We quantitatively evaluate the performances of different fusion methods using six metrics, i.e., entropy (EN) [42], standard deviation (SD) [43], structural similarity index measure (SSIM) [44], correlation coefficient (CC) [45], spatial frequency (SF) [46], and visual information fidelity (VIF) [47]. The definitions of these six metrics are as follows.\n\nEN is defined based on information theory, which measures the amount of information the fused image contains. Mathematically, EN is defined as follows:\nEN = \u2212 L\u22121 l=0 p l log 2 p l ,(7)\nwhere L denotes the number of gray level, we set it to 256 in our experiments. p l is the normalized histogram of corresponding gray level in the fused image. The larger entropy is, the more information fused image contains, and the better performance fusion method achieves.\n\nSD is defined based on the statistical concept, which reflects the extent to which the values of individual pixels in the image from the average value. Mathematically, SD is defined as follows:\nSD = M i=1 N j=1 (F (i, j) \u2212 \u00b5) 2 ,(8)\nwhere F is the fused image with dimensions of M \u00d7 N , and \u00b5 is the mean value of the fused image F . The regions with high contrast always attract the attention of human, and the fused image with higher contrast often leads to larger SD, which means that the fused image achieves better visual effect.\n\nSSIM is used to model the image loss and distortion, which measures the structural similarity between source images and fused image. SSIM mainly consist of three components:\n\nloss of correlation, luminance distortion, and contrast distortion. The product of three components is the assessment result of the fused image, and SSIM is defined as follows:\nSSIM X,F = x,f 2\u00b5 x \u00b5 f + C 1 \u00b5 2 x + \u00b5 2 f + C 1 \u00b7 2\u03c3 x \u03c3 f + C 2 \u03c3 2 x + \u03c3 2 f + C 2 \u00b7 \u03c3 xf + C 3 \u03c3 x \u03c3 f + C 3 ,(9)SSIM = SSIM A,F + SSIM B,F ,(10)\n\nA C C E P T E D M A N U S C R I P T\n\nwhere SSIM X,F denotes the structural similarity between source image X and fused image CC measures the degree of linear correlation of the fused image and source images and is defined as follows:\nCC = (r AF + r BF ) 2 ,(11)\nwhere\nr XF = M i=1 N j=1 (X(i,j)\u2212X)(F (i,j)\u2212\u00b5) M i=1 N j=1 (X(i,j)\u2212X) 2 ( M i=1 N j=1 (F (i,j)\u2212\u00b5) 2 )\nandX denotes the mean value of source image X. The larger the CC, the more similar the fused image is to the source images and the better the fusion performance.\n\nSF is designed to measure the gradient distribution of an image, which is defined as follows:\nSF = \u221a RF 2 + CF 2 ,(12)\nwhere RF is spatial row frequency defined as RF = Our FusionGAN is trained on the TNO database, from which we choose 45 infrared and visible images from different scenes, and our training parameters are set as follows: the size of batch images m is set to 32, the number of training iterations is set to 10, and the discriminator training step k is set to 2. \u03bb is set to 100, \u03be is set to 8, and the learning rate is set to 10 \u22124 . The label a of fused image is a random number ranging from 0 to 0.3, the label b of visible image is a random number ranging from 0.7 to 1.2, and the label c is also a random number ranging from 0.7 to 1.2. The labels a, b and c are not specific numbers, which are the so-called soft labels [34].  \n\n\nResults on TNO database\n\n\nA C C E P T E D M A N U S C R I P T\n\njudge which method is the best or worst. However, we also find that for the comparison methods except GTF, the targets (e.g., the building, human, or the car) in the fused images are not that obvious, which means that the thermal radiation information in the infrared images is not well preserved. This can be attributed to that the comparison methods all focus on exploiting the detail information in the source images.\n\nIn contrast, GTF and our method can highlight the target regions in the fused images better than those in the visible images, which is beneficial for automatic target detection and localization. Both our method and GTF can well preserve the thermal radiation. Nevertheless, compared with GTF, the fusion results of our FusionGAN obviously contain more abundant detail information, and our results are more appropriate for human visual perception. For example, in Kaptein 1123, the human is equally highlighted in two results, but the lamp on the street is much more clear in our result. In Marne 04, the tree is fused appropriately in our FusionGAN result; however, in the result of GTF, it is almost impossible to be recognized. Similar phenomenon can also be observed in the other five examples, as shown in the red boxes. This demonstrates that our FusionGAN has better performance than the other state-of-the-arts in terms of simultaneously preserving thermal radiation information and texture detail information.\n\nQuantitative comparisons. We further give quantitative comparisons of the nine methods on two sets of image pairs from the TNO database. The first set contains ten pairs of infrared and visible images including the seven pairs in Fig. 5 and three additional pairs. The second set is an infrared and visible image sequence pair, e.g., the Nato camp sequence.     The largest EN demonstrates that our fused image has more abundant information than the other eight comparison methods. The largest SD demonstrates that our fused image has the largest image contrast. The largest SSIM demonstrates that our fused image is more similar to the infrared and visible image on the structures. While the largest SF demonstrates that our fused image contains rich edges and textures. Although CC and VIF of our method are not the best, the comparable results still demonstrate that our fused image has great correlation with two source images and is also more consistent with the human visual system. We also provide the run time comparison of nine methods in Table 1.\n\nFrom the results, we see that our FusionGAN can achieve comparable efficiency compared with the other eight methods.\n\n\nResults on INO database\n\nWe further test our method and other eight compared methods on the INO Database, in which we select 31 visible and infrared image pairs every other 18 frames from the video named Trees and runner for both qualitative and quantitative comparisons. Figure 8 shows\nA C C E P T E D M A N U S C R I P T\nthe fused results of the 27th frame in the selected 31 image pairs. We see that the visible image involves rich textures, whereas the infrared image contains poor textures, as can be seen from the car and tree on the ground. However, the human in the infrared image is more highlighted than that in the visible image. Considering the fusion results, the texture information can be well retained by all the nine methods. However, only our FusionGan can keep the thermal radiation distribution in the infrared image, e.g., the pixel intensities of the human area. Figure 9 shows  Table 1, and our FusionGAN can achieve comparable efficiency compared with the other eight methods.\n\n\nExperimental validation of adversarial loss\n\nTo verify the importance of adversarial training in our FusionGAN, here we train two models on the TNO database based on whether to use adversarial loss, with all training settings the same as the first experiment. In Fig. 10 \n\n\nApplication to fusing images with different resolutions\n\nIn this experiment, we further apply our FusionGAN to fuse infrared and visible images with different spatial resolutions, say low-resolution infrared images and high-resolution visible images. To this end, we make the following three modifications. First, we downsample all infrared images to 1 c 2 (i.e., 1 c \u00d7 1 c , here we set c = 4) of the original image scale as the new low-resolution source infrared images, while the visible images keep the original scale.\n\nThe ten image pairs from the TNO database as described in Fig. 6 are used as testing data A C C E P T E D M A N U S C R I P T in this experiment. Second, due to the infrared and visible images are of different spatial resolutions, we cannot directly concatenate them for training or testing, as shown in Fig. 2.\n\nTherefore, we interpolate the low-resolution infrared images to the same resolution as the corresponding visible images before concatenating and putting them into the generator.\n\nThird, since the fused image and infrared image are also of different resolutions, we redesign the content loss L content in Eq. (5) as follows:\nL content = 1 HW ( \u03c6(I f ) \u2212 I r 2 F + \u03be \u2207I f \u2212 \u2207I v 2 F ),(13)\nwhere \u03c6 is a downsampling operation, which downsamples the fused image to the resolution of infrared image. All other training settings are the same as the first experiment. Note that in Eq. (13) we choose to downsample the fused image rather than upsampling the infrared image. This is because that upsampling infrared image will inevitably introduce additional noise which is likely to be transferred to the fused image, leading to an unsatisfying result.\n\nFor all the eight comparison methods, they have a prerequisite that the source images should share the same resolution. Therefore, we have to first eliminate resolution differences through downsampling visible images or upsampling infrared images. Clearly, downsampling visible images will cause texture information loss and upsampling infrared images will blur thermal radiation information. Nevertheless, to avoid loss of information, we choose to upsample the infrared images before fusion for all the comparison methods.\n\nWe select five typical image pairs for qualitative evaluation, including Bunker, Sandpath, Kaptein 1123, Kaptein 1654 and Marne 04. Figure 11 shows the fused image generated by our FusionGAN and other eight comparison methods. The first two rows in Fig. 11 \n\n\nA C C E P T E D M A N U S C R I P T\n\nto the other eight methods. This is due to that our FusionGAN does not need to downsample or upsample source images in the content loss (i.e., (13)), and hence does not suffer from noise caused by upsampling of infrared information.\n\nWe further give quantitative comparisons of the nine methods on the whole ten image pairs, and the results of six metrics are reported in Fig. 12. There are three metrics such as SSIM, CC and VIF that rely on the source images. More specifically, calculating these metrics requires the source images and the fused image to share the same resolution. It may be not appropriate to require the fused image to preserve as much information as possible in the upsampled infrared image, as it involves additional noise. Instead, it would be better to downsample the visible and fused images and require the downsampled fused image to preserve as much information as possible in the original infrared image and downsampled visible image. Therefore, in this paper we downsample the visible and fused images to the same resolution as the corresponding infrared image before calculating the metrics of SSIM, CC and VIF. From the results, we see that our FusionGAN has the best average values on all six metrics. This demonstrates the significant advantage of our FusionGAN over the existing fusion methods for fusing source images with different resolutions.\n\nIn conclusion, our FusionGAN can keep the thermal radiation information in infrared images and the rich texture details in visible images simultaneously, no matter fusing source images in the same spatial resolution or different resolutions. Compared with existing stateof-the-art fusion methods, our results look like sharpened infrared images with clear highlighted targets and abundant detail information. In addition, our FusionGAN has comparable efficiency compared with the state-of-the-arts.\n\n\nDiscussion\n\nThe deep learning based techniques usually have a common problem that they are regarded as black-box models and even if we understand the underlying mathematical principles of such models they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures [48]. In this section, we briefly discuss the explainability of our FusionGAN.\n\n\nA C C E P T E D M A N U S C R I P T\n\nThe essence of traditional GAN is to train a generator to capture the data distribution, so that the data generated by the generator has the same distribution as the original data.\n\nIn this procedure, the similarity of data distribution is measured by a discriminator. More specifically, a discriminator is trained to distinguish the generated data from the original data. When the discriminator cannot successfully distinguish these two types of data, we consider that the generated data has the same distribution as that of the original data.\n\nThe essence of our FusionGAN is to generate a fused image which selectively preserves the information in source images (i.e., the infrared and visible images), and the amount of information to be preserved is controlled by the parameters \u03bb and \u03be. In particular, the content loss aims to preserve the radiation information in the infrared image and the gradient information in the visible image, while the adversarial loss aims to preserve other important properties characterizing the detail information in visible image, such as image contrast, saturation and illumination changes. Therefore, during the adversarial process, the generator continually fits the distribution of detail information in the fused image to that in the visible image while simultaneously preserves the infrared radiation information. When the discriminator cannot distinguish the fused image from the visible image, the distribution of detail information in the fused image is then considered to be the same as that in the visible image, and hence the fused image visually possesses more textural details.\n\n\nConclusion\n\nIn this paper, we propose a novel infrared and visible image fusion method based on Our FusionGAN is a general framework for dealing with fusion tasks aiming at fusing pixel intensities in one source image together with texture details in anther source image. We have also generalized our FusionGAN to fuse source images with different resolutions. In our future work, we will further apply our FusionGAN to solve the well-known pansharpening problem from the remote sensing community, the goal of which is to fuse a low-resolution multispectral image and a high-resolution panchromatic image to generate a multispectral image with high spatial resolution [49]. \n\nFigure 1 :\n1Email addresses: jyma2010@gmail.com (Jiayi Ma), yuwei998@whu.edu.cn (Wei Yu), erfect@whu.edu.cn (Pengwei Liang), lichang1214@gmail.com (Chang Li), junjun0595@163.com (Junjun Jiang) Schematic illustration of image fusion. From left to right: the infrared image, the visible image, the fusion result of a classic method, and the fusion result of our proposed FusionGAN. Our result can simultaneously keep the thermal radiation distribution in the infrared image and the appearance textures in the visible image.\n\nFigure 2 :\n2Framework of the proposed FusionGAN for infrared and visible image fusion.\n\nFigure 3 :\n3Network architecture of generator G \u03b8 G . G \u03b8 G is a simple five-layer convolution neural network with 5 convolution layers, 4 batch normalization layers, 4 leaky ReLU activation layers, and 1 tanh activation layer.\n\nFigure 4 :\n4Network architecture of discriminator D \u03b8 D . D \u03b8 D is a simple five-layer convolution neural network with 4 convolution layers to extract feature maps of input, 1 linear layer to do the classification, 4 batch normalization layers, and 4 leaky ReLU activation layers.\n\n\nby AdamOptimizer: \u2207 \u03b8 G L G ; 9 end[22], ratio of low-pass pyramid (LPP)[3], two-scale image fusion based on visual saliency (TSIFVS)[40], and gradient transfer fusion (GTF)[18]. The implementation of all these eight methods are publicly available, and we set the parameters of competing methods according to the original papers. Subsequently, in order to verify the importance of adversarial training, we train two models based on whether to use adversarial loss and compare their fusion performances. Finally, we generalize our FusionGAN to fuse images with different resolutions such as low-resolution infrared images and high-resolution visible images, and we also compare it with the aforementioned eight state-of-the-art fusion methods. All the experiments are conducted on a desktop with 2.4 GHz Intel Xeon CPU E5-2673 v3, GeForce GTX 1080Ti, and 64 GB memory.\n\nF\n, x and f represent the image patch of the source image and fused image in a local window of size M \u00d7 N , \u03c3 x and \u03c3 f denote the standard deviation, \u03c3 xf is the standard covariance correlation of source and fused image, \u00b5 x and \u00b5 f denote the mean value of source image and fused image. C 1 and C 2 and C 3 are parameters to make the algorithm stable. And SSIM A,F and SSIM B,F denote the structural similarities between infrared/visible images and fused image. The larger value of SSIM indicates better performance.\n\n\nF (i, j) \u2212 F (i, j \u2212 1)) 2 , and CF is column frequency defined as CF = M i=1 N j=1 (F (i, j) \u2212 F (i \u2212 1, j)) 2 . The larger SF is, the richer edges and textures the fused image has.VIF measures the information fidelity of the fused image, and it is computed by four steps: firstly, the source images and fused image are divided into different blocks; then, evaluate the visual information of each block with and without distortion; subsequently, evaluate the VIF for each sub-band; finally, calculate the overall metric based on VIF.4.2. Experimental validation of fusion performance4.2.1. Databases and training settingsIn this experiment, we first focus on qualitative and quantitative comparisons on the fusion performance of different methods on the surveillance images from TNO Human Factors, which contain multispectral nighttime imagery of different military relevant scenarios, different multiband camera systems 2 . We choose seven typical pairs for qualitative illustration, i.e., Bunker, Bench, Sandpath, Kaptein 1123, Kaptein 1654, Marne 04, and Nato camp. The Nato camp is an image sequence, which contains 32 image pairs, and this sequence is also used for quantitative comparison. In addition, we also test our method on the INO database 3 , which is provided by the National Optics Institute of Canada, and contains several pairs of visible and infrared videos representing different scenarios captured under different weather conditions. Specifically, we capture 31 visible and infrared image pairs from the video named Trees and runner for both qualitative and quantitative comparisons.\n\n\nQualitative comparisons. To give some intuitive results on the fusion performance, we select seven typical image pairs for qualitative evaluation, Bunker, Bench, Sandpath, Kaptein 1123, Kaptein 1654, Marne 04, and Nato camp. The fusion results of the proposed FusionGAN and the other eight comparison methods are shown in Fig. 5. The first two rows in Fig. 5 present the original infrared images and visible images, the last row is the fusion results of our FusionGAN, and the rest eight rows correspond to the fusion results of eight comparison methods. From the results, we see that all methods can well fuse the information of visible image and infrared image to some extent. In this sense, we cannot\n\nFigure 5 :\n5Qualitative fusion results on seven typical infrared and visible image pairs from the TNO database. From left to right: Bunker, Bench, Sandpath, Kaptein 1123, Kaptein 1654, Marne 04, and Nato camp. From top to bottom: infrared images, visible images, results of ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS, GTF and our FusionGAN. Note that in the last two rows, for clear comparison we select a small region (i.e., the red box) in each fused image, and then zoom in it and put it in the bottom right corner.\n\n\nThe results of six metrics on the two datasets are shown in Figs. 6 and 7. On the first dataset, our FusionGAN is able to obtain the largest average values on the five evaluation metrics, i.e. EN, SD, SSIM, SF and VIF, and our FusionGAN only follows GTF by a narrow margin on the metric of CC. While on the second dataset, our FusionGAN clearly has the best EN, SD, SSIM and SF on most image pairs, and the average values of these evaluation metrics are also the largest compared to the rest eight methods. For the metric of CC, our FusionGAN follows behind FPDE and ASR by a narrow margin; for the metric of VIF, our FusionGAN also only follows behind GFF.\n\nFigure 6 :\n6Quantitative comparisons of the six metrics, i.e., EN, SD, SSIM, CC, SF and VIF, on ten image pairs from the TNO database. The eight state-of-the-art methods such as ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS and GTF are used for comparison.\n\nFigure 7 :\n7Quantitative comparisons of the six metrics, i.e., EN, SD, SSIM, CC, SF and VIF, on the Nato camp sequence from the TNO database. The eight state-of-the-art methods such as ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS and GTF are used for comparison.\n\n\nthe quantitative comparison results of four metrics, where our FusionGAN again has the best EN, SD, SSIM, CC and VIF on most image pairs, and the average values of these evaluation metrics are the largest compared to the eight compared methods. For the metric of SF, our FusionGAN only follows behind LPP. In addition, we also provide the run time comparison of different fusion methods in\n\nFigure 8 :\n8, we schematically illustrate the fusion results of two models on four typical pairs including Bunker, Sandpath, Kaptein 1123 and Marne 04. The first two rows present the infrared images and visible images, the third row is the fusion results of FusionGAN trained without adversarial loss, and the last row is the fusion results of our proposed method, i.e., using the adversarial loss. From the results, we can clearly see that the fusion results of FusionGAN by using adversarial loss contain much more detail information and are also more consistent with the human visual system. For example, the textures of vegetation in Bunker, stumps in Sandpath, trees in Kaptein 1123, as well as car window in Marne 04 are all fused appropriately in the last row. However, without using adversarial loss in the third row, the contrast of the resultant images is low and all the above mentioned details are difficult to distinguish. In fact, texture details cannot be totally represented by only gradient information. Other properties such as image contrast, saturation and illumination changes also play important Qualitative fusion results on Frame 27 of the selected Trees and runner from INO Dataset. The first row is the infrared image and visible image. The rest three rows (from left to right, top to bottom) are the fusion results of ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS, GTF and our FusionGAN.roles in reflecting the details in visible images. However, it is difficult to mathematically characterize these properties in an objective function to be optimized. Nevertheless, the results in this section demonstrate that the adversarial training can help to exploit more texture details in a visible image and transfer them to the fused image.\n\nFigure 9 :Figure 10 :\n910Quantitative comparisons of the six metrics, i.e., EN, SD, SSIM, CC, SF and VIF, on the Trees and runner sequence from the INO database. The eight state-of-the-art methods such as ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS and GTF are used for comparison. Qualitative fusion results on four typical infrared and visible image pairs from the TNO database. From left to right: Bunker, Sandpath, Kaptein 1123 and Marne 04. From top to bottom: infrared images, visible images, results of FusionGAN without adversarial loss and with adversarial loss. For clear comparison we select a small region (i.e., the red box) in each fused image, and then zoom in it and put it in the bottom right corner.\n\nFigure 12 :\n12Quantitative comparisons of the six metrics, i.e., EN, SD, SSIM, CC, SF and VIF on ten image pairs from the TNO database. The eight state-of-the-art methods such as ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS and GTF are used for comparison.\n\n\ngenerative adversarial network. It can simultaneously keep the thermal radiation information in infrared images and the texture detail information in visible images. The proposed FusionGAN is an end-to-end model, which can avoid designing complicated activity level measurement and fusion rule manually as in traditional fusion strategies. Experiments on public datasets demonstrate that our fusion results look like sharpened infrared images with clear highlighted targets and abundant detail information, which is beneficial for target detection and recognition systems based on image fusion. The quantitative comparisons with eight state-of-the-arts on four evaluation metrics reveal that our FusionGAN can not only visual effects, but also can keep the largest or approximately the largest amount of information in the source images.\n\n\nHowever, P G cannot be represented explicitly, and D must be synchronized well with G and avoid vanishing gradient in deeper models. Third, fully connected layers are removed in deeper models. Fourth, all activation layers in generator are rectified linear unit (ReLU)during training. Thus, regular GANs are unstable, and it is hard to train a good model by \n\nregular GANs. \nA C C E P T E D \n\nM \nA N U S C R I P T \n\n2.3.2. Deep convolutional GANs \n\nThe technique of Deep convolutional GANs (DCGANs) was first proposed by Radrord \n\net al. in 2015 [32]. DCGANs first successfully introduced CNNs, which can bridge the \n\ngap between CNNs for supervised learning and GANs for unsupervised learning. Since \n\ntraditional GANs are unstable to train a good model, so the architecture of CNNs should \n\nbe designed appropriately to make the traditional GANs more stable, and there are mainly \n\nfive differences compared with traditional CNNs. First, the pooling layers are not used \n\nin both the generator and the discriminator. Instead, strided convolutions are applied in \n\ndiscriminator to learn its own spatial downsampling, and fractional-strided convolutions are \n\nused in generator to realize upsampling. Second, batchnormalization layers are introduced \n\ninto both the generator and the discriminator. Since poor initialization always tends to \n\ncreate a lot of training problems, batchnormalization layers are able to solve these problems \n\n\n\nTable 1 :\n1Run time comparison of nine methods on the two datasets from the TNO database and one dataset from the INO database. Our method is performed on GPU while all the other methods are performed onCPU. Each value denotes the mean of run times of a certain method on a dataset (unit: second). \n\nMethod \nTNO1 \nTNO2 \nINO \n\nASR \n2.62 \u00d7 10 2 \n9.13 \u00d7 10 1 \n8.97 \u00d7 10 1 \n\nCVT \n1.46 \n6.53 \u00d7 10 \u22121 6.62 \u00d7 10 \u22121 \n\nDTCWT \n3.32 \u00d7 10 \u22121 1.30 \u00d7 10 \u22121 1.28 \u00d7 10 \u22121 \n\nFPDE \n5.02 \u00d7 10 \u22121 9.22 \u00d7 10 \u22122 1.09 \u00d7 10 \u22121 \n\nGFF \n3.18 \u00d710 \u22121 8.35 \u00d7 10 \u22122 1.04 \u00d7 10 \u22121 \n\nLPP \n9.60 \u00d7 10 \u22122 3.72 \u00d7 10 \u22122 4.28 \u00d7 10 \u22122 \n\nTSIFVS \n3.08 \u00d7 10 \u22122 1.16 \u00d7 10 \u22122 3.60 \u00d7 10 \u22122 \n\nGTF \n4.82 \n1.00 \n1.51 \n\nFusionGAN 2.54 \u00d7 10 \u22121 7.16 \u00d7 10 \u22122 3.50 \u00d7 10 \u22122 \n\n\n\n\npresent the original low-resolution infrared images and high-resolution visible images, the last row is the fusion results of our FusionGAN, and the rest eight rows correspond to the fusion results of eight comparison methods. From all these results, we can get the same conclusion as the first experiments, e.g., thermal radiation in the infrared images is not well preserved in the comparison methods except GTF, and compared with GTF, our results contain more abundant detail information and are more appropriate for human visual perception. In addition, the fused images generated by our FusionGAN are clearer and cleaner compared M A N U S C R I P T Figure 11: Qualitative fusion results on five typical infrared and visible image pairs from the TNO database. From left to right: Bunker, Sandpath, Kaptein 1123, Kaptein 1654, and Marne 04. From top to bottom: low-resolution infrared images, high-resolution visible images, results of ASR, CVT, DTCWT, FPDE, GFF, LPP, TSIFVS, GTF and our FusionGAN. Note that in the last two rows, for clear comparison we select a small region (i.e., the red box) in each fused image, and then zoom in it and put it in the bottom right corner.A C C E P T E D \n\nM \nA N U S C R I P T \n\n1 2 3 4 5 6 7 8 9 10 \n5 \n\n5.5 \n\n6 \n\n6.5 \n\n7 \n\n7.5 \n\n8 \nEN \n\nASR:6.1215 \nCVT:6.2959 \nDTCWT:6.2087 \nFPDE:6.1094 \nGFF:6.8210 \nLPP:6.3541 \nTSIFVS:6.3214 \nGTF:7.0180 \nOur:7.2984 \n\n1 2 3 4 5 6 7 8 9 10 \n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \nSD \n\nASR:19.3573 \nCVT:21.6509 \nDTCWT:20.3529 \nFPDE:19.2607 \nGFF:40.7484 \nLPP:22.7872 \nTSIFVS:21.9812 \nGTF:49.4693 \nOur:50.4729 \n\n1 2 3 4 5 6 7 8 9 10 \n0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \nSSIM \n\nASR:0.2902 \nCVT:0.2968 \nDTCWT:0.2958 \nFPDE:0.2923 \nGFF:0.2450 \nLPP:0.2957 \nTSIFVS:0.3033 \nGTF:0.2430 \nOur:0.3306 \n\n1 2 3 4 5 6 7 8 9 10 \n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\n0.9 \n\nCC \n\nASR:0.4057 \nCVT:0.3831 \nDTCWT:0.3738 \nFPDE:0.4094 \nGFF:0.6686 \nLPP:0.3491 \nTSIFVS:0.4014 \nGTF:0.8087 \nOur:0.8235 \n\n1 2 3 4 5 6 7 8 9 10 \n1 \n\n2 \n\n3 \n\n4 \n\n5 \n\n6 \n\n7 \nSF \n\nASR:3.4828 \nCVT:3.9567 \nDTCWT:3.7872 \nFPDE:3.1884 \nGFF:3.4775 \nLPP:5.0344 \nTSIFVS:3.0091 \nGTF:3.5409 \nOur:5.3454 \n\n1 2 3 4 5 6 7 8 9 10 \n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \nVIF \n\nASR:0.3880 \nCVT:0.3768 \nDTCWT:0.4000 \nFPDE:0.3738 \nGFF:0.4378 \nLPP:0.3450 \nTSIFVS:0.2794 \nGTF:0.1725 \nOur:0.4656 \n\n\nAvailable at https://figshare.com/articles/TNO_Image_Fusion_Dataset/1008029.3 Available at https://www.ino.ca/en/video-analytics-dataset/.\nACCEPTED MANUSCRIPT\nFrom multi-scale decomposition to non-multi-scale decomposition methods: A comprehensive survey of image fusion techniques and its applications. A Dogra, B Goyal, S , IEEE Access. 5A. Dogra, B. Goyal, S. Agrawal, From multi-scale decomposition to non-multi-scale decomposition methods: A comprehensive survey of image fusion techniques and its applications, IEEE Access 5 (2017) 16040-16067.\n\nInfrared and visible image fusion using total variation model. Y Ma, J Chen, C Chen, F Fan, J Ma, Neurocomputing. 202Y. Ma, J. Chen, C. Chen, F. Fan, J. Ma, Infrared and visible image fusion using total variation model, Neurocomputing 202 (2016) 12-19.\n\nImage fusion by a ratio of low-pass pyramid. A Toet, Pattern Recognition Letters. 94A. Toet, Image fusion by a ratio of low-pass pyramid, Pattern Recognition Letters 9 (4) (1989) 245-253.\n\nA survey of infrared and visual image fusion methods. X Jin, Q Jiang, S Yao, D Zhou, R Nie, J Hai, K He, Infrared Physics & Technology. 85X. Jin, Q. Jiang, S. Yao, D. Zhou, R. Nie, J. Hai, K. He, A survey of infrared and visual image fusion methods, Infrared Physics & Technology 85 (2017) 478-501.\n\nPerformance comparison of different multi-resolution transforms for image fusion. S Li, B Yang, J Hu, Information Fusion. 122S. Li, B. Yang, J. Hu, Performance comparison of different multi-resolution transforms for image fusion, Information Fusion 12 (2) (2011) 74-84.\n\nA wavelet-based image fusion tutorial. G Pajares, J M De La, Cruz, Pattern Recognition. 379G. Pajares, J. M. De La Cruz, A wavelet-based image fusion tutorial, Pattern Recognition 37 (9) (2004) 1855-1872.\n\nA categorization of multiscale-decomposition-based image fusion schemes with a performance study for a digital camera application. Z Zhang, R S Blum, Proceedings of the IEEE. 878Z. Zhang, R. S. Blum, A categorization of multiscale-decomposition-based image fusion schemes with a performance study for a digital camera application, Proceedings of the IEEE 87 (8) (1999) 1315-1326.\n\n. A C C E P T E D M A N U S C R I P T, A C C E P T E D M A N U S C R I P T\n\nFusion method for infrared and visible images by using non-negative sparse representation. J Wang, J Peng, X Feng, G He, J Fan, Infrared Physics & Technology. 67J. Wang, J. Peng, X. Feng, G. He, J. Fan, Fusion method for infrared and visible images by using non-negative sparse representation, Infrared Physics & Technology 67 (2014) 477-489.\n\nGroup-sparse representation with dictionary learning for medical image denoising and fusion. S Li, H Yin, L Fang, IEEE Transactions on Biomedical Engineering. 5912S. Li, H. Yin, L. Fang, Group-sparse representation with dictionary learning for medical image denoising and fusion, IEEE Transactions on Biomedical Engineering 59 (12) (2012) 3450-3459.\n\nA fusion algorithm for infrared and visible images based on adaptive dualchannel unit-linking pcnn in nsct domain. T Xiang, L Yan, R Gao, Infrared Physics & Technology. 69T. Xiang, L. Yan, R. Gao, A fusion algorithm for infrared and visible images based on adaptive dual- channel unit-linking pcnn in nsct domain, Infrared Physics & Technology 69 (2015) 53-61.\n\nNovel fusion method for visible light and infrared images based on nsstsf-pcnn. W Kong, L Zhang, Y Lei, Infrared Physics & Technology. 65W. Kong, L. Zhang, Y. Lei, Novel fusion method for visible light and infrared images based on nsst- sf-pcnn, Infrared Physics & Technology 65 (2014) 103-112.\n\nMulti-sensor image fusion based on fourth order partial differential equations. D P Bavirisetti, G Xiao, G Liu, International Conference on Information Fusion. D. P. Bavirisetti, G. Xiao, G. Liu, Multi-sensor image fusion based on fourth order partial differential equations, in: International Conference on Information Fusion, 2017, pp. 1-9.\n\nAdaptive fusion method of visible light and infrared images based on non-subsampled shearlet transform and fast non-negative matrix factorization. W Kong, Y Lei, H Zhao, Infrared Physics & Technology. 67W. Kong, Y. Lei, H. Zhao, Adaptive fusion method of visible light and infrared images based on non-subsampled shearlet transform and fast non-negative matrix factorization, Infrared Physics & Technology 67 (2014) 161-172.\n\nInfrared and visible image fusion via saliency analysis and local edge-preserving multi-scale decomposition. X Zhang, Y Ma, F Fan, Y Zhang, J Huang, JOSA A. 348X. Zhang, Y. Ma, F. Fan, Y. Zhang, J. Huang, Infrared and visible image fusion via saliency analysis and local edge-preserving multi-scale decomposition, JOSA A 34 (8) (2017) 1400-1410.\n\nInfrared image enhancement through saliency feature analysis based on multi-scale decomposition. J Zhao, Y Chen, H Feng, Z Xu, Q Li, Infrared Physics & Technology. 62J. Zhao, Y. Chen, H. Feng, Z. Xu, Q. Li, Infrared image enhancement through saliency feature analysis based on multi-scale decomposition, Infrared Physics & Technology 62 (2014) 86-93.\n\nA general framework for image fusion based on multi-scale transform and sparse representation. Y Liu, S Liu, Z Wang, Information Fusion. 24Y. Liu, S. Liu, Z. Wang, A general framework for image fusion based on multi-scale transform and sparse representation, Information Fusion 24 (2015) 147-164.\n\nInfrared and visible image fusion based on visual saliency map and weighted least square optimization. J Ma, Z Zhou, B Wang, H Zong, Infrared Physics & Technology. 82J. Ma, Z. Zhou, B. Wang, H. Zong, Infrared and visible image fusion based on visual saliency map and weighted least square optimization, Infrared Physics & Technology 82 (2017) 8-17.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. J Ma, C Chen, C Li, J Huang, Information Fusion. 31J. Ma, C. Chen, C. Li, J. Huang, Infrared and visible image fusion via gradient transfer and total variation minimization, Information Fusion 31 (2016) 100-109.\n\nFusion of visible and infrared images using global entropy and gradient constrained regularization. J Zhao, G Cui, X Gong, Y Zang, S Tao, D Wang, Infrared Physics & Technology. 81J. Zhao, G. Cui, X. Gong, Y. Zang, S. Tao, D. Wang, Fusion of visible and infrared images using global entropy and gradient constrained regularization, Infrared Physics & Technology 81 (2017) 201-209.\n\nPixel-level image fusion: A survey of the state of the art. S Li, X Kang, L Fang, J Hu, H Yin, Information Fusion. 33S. Li, X. Kang, L. Fang, J. Hu, H. Yin, Pixel-level image fusion: A survey of the state of the art, Information Fusion 33 (2017) 100-112.\n\nDeep learning for pixel-level image fusion: Recent advances and future prospects. Y Liu, X Chen, Z Wang, Z J Wang, R K Ward, X Wang, Information Fusion. 42Y. Liu, X. Chen, Z. Wang, Z. J. Wang, R. K. Ward, X. Wang, Deep learning for pixel-level image fusion: Recent advances and future prospects, Information Fusion 42 (2018) 158-173.\n\nImage fusion with guided filtering. S Li, X Kang, J Hu, IEEE Transactions on Image Processing. 227S. Li, X. Kang, J. Hu, Image fusion with guided filtering, IEEE Transactions on Image Processing 22 (7) (2013) 2864-2875.\n\nA general framework for multiresolution image fusion: from pixels to regions. G Piella, Information Fusion. 44G. Piella, A general framework for multiresolution image fusion: from pixels to regions, Information Fusion 4 (4) (2003) 259-280.\n\nSparse representation based multi-sensor image fusion -focus and multi-modality images: A review. Q Zhang, Y Liu, R S Blum, J Han, D Tao, Information Fusion. 40Q. Zhang, Y. Liu, R. S. Blum, J. Han, D. Tao, Sparse representation based multi-sensor image fusion -focus and multi-modality images: A review, Information Fusion 40 (2018) 57-75.\n\nInfrared and visible image fusion using entropy and neuro-fuzzy concepts. S Rajkumar, P C Mouli, Proceedings of the Annual Convention of Computer Society of India. the Annual Convention of Computer Society of IndiaS. Rajkumar, P. C. Mouli, Infrared and visible image fusion using entropy and neuro-fuzzy concepts, in: Proceedings of the Annual Convention of Computer Society of India, 2014, pp. 93-100.\n\nMulti-focus image fusion with a deep convolutional neural network. Y Liu, X Chen, H Peng, Z Wang, Information Fusion. 36Y. Liu, X. Chen, H. Peng, Z. Wang, Multi-focus image fusion with a deep convolutional neural network, Information Fusion 36 (2017) 191-207.\n\nInfrared and visible image fusion with convolutional neural networks. Y Liu, X Chen, J Cheng, H Peng, Z Wang, International Journal of Wavelets, Multiresolution and Information Processing. 1631850018Y. Liu, X. Chen, J. Cheng, H. Peng, Z. Wang, Infrared and visible image fusion with convolutional neural networks, International Journal of Wavelets, Multiresolution and Information Processing 16 (3) (2018) 1850018.\n\nImage fusion and super-resolution with convolutional neural network. J Zhong, B Yang, Y Li, F Zhong, Z Chen, Chinese Conference on Pattern Recognition. J. Zhong, B. Yang, Y. Li, F. Zhong, Z. Chen, Image fusion and super-resolution with convolutional neural network, in: Chinese Conference on Pattern Recognition, 2016, pp. 78-88.\n\nImage fusion with convolutional sparse representation. Y Liu, X Chen, R K Ward, Z J Wang, IEEE Signal Processing Letters. 2312Y. Liu, X. Chen, R. K. Ward, Z. J. Wang, Image fusion with convolutional sparse representation, IEEE Signal Processing Letters 23 (12) (2016) 1882-1886.\n\nG Masi, D Cozzolino, L Verdoliva, G Scarpa, Pansharpening by convolutional neural networks. 8594G. Masi, D. Cozzolino, L. Verdoliva, G. Scarpa, Pansharpening by convolutional neural networks, Remote Sensing 8 (7) (2016) 594.\n\nI Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in Neural Information Processing Systems. Generative adversarial netsI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in: Advances in Neural Information Processing Systems, 2014, pp. 2672- 2680.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. A Radford, L Metz, S Chintala, arXiv:1511.06434arXiv preprintA. Radford, L. Metz, S. Chintala, Unsupervised representation learning with deep convolutional gen- erative adversarial networks, arXiv preprint arXiv:1511.06434.\n\n. M Arjovsky, S Chintala, L Bottou, Wasserstein Gan, arXiv:1701.07875arXiv preprintM. Arjovsky, S. Chintala, L. Bottou, Wasserstein gan, arXiv preprint arXiv:1701.07875.\n\nX Mao, Q Li, H Xie, R Y Lau, Z Wang, S P Smolley, IEEE International Conference on Computer Vision. Least squares generative adversarial networksX. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, S. P. Smolley, Least squares generative adversarial networks, in: IEEE International Conference on Computer Vision, 2017, pp. 2813-2821.\n\nF Yu, V Koltun, arXiv:1511.07122Multi-scale context aggregation by dilated convolutions. arXiv preprintF. Yu, V. Koltun, Multi-scale context aggregation by dilated convolutions, arXiv preprint arXiv:1511.07122.\n\nD P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. arXiv preprintD. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980.\n\nVisual attention guided image fusion with sparse representation. B Yang, S Li, Optik-International Journal for Light and Electron Optics. 12517B. Yang, S. Li, Visual attention guided image fusion with sparse representation, Optik-International Journal for Light and Electron Optics 125 (17) (2014) 4881-4888.\n\nRemote sensing image fusion using the curvelet transform. F Nencini, A Garzelli, S Baronti, L Alparone, Information Fusion. 82F. Nencini, A. Garzelli, S. Baronti, L. Alparone, Remote sensing image fusion using the curvelet transform, Information Fusion 8 (2) (2007) 143-156.\n\nPixel-and region-based image fusion with complex wavelets. J J Lewis, R J O&apos;callaghan, S G Nikolov, D R Bull, N Canagarajah, Information fusion. 82J. J. Lewis, R. J. O'Callaghan, S. G. Nikolov, D. R. Bull, N. Canagarajah, Pixel-and region-based image fusion with complex wavelets, Information fusion 8 (2) (2007) 119-130.\n\nTwo-scale image fusion of visible and infrared images using saliency detection. D P Bavirisetti, R Dhuli, Infrared Physics & Technology. 76D. P. Bavirisetti, R. Dhuli, Two-scale image fusion of visible and infrared images using saliency detec- tion, Infrared Physics & Technology 76 (2016) 52-64.\n\nInfrared and visible image fusion methods and applications: a survey. J Ma, Y Ma, C Li, InformationJ. Ma, Y. Ma, C. Li, Infrared and visible image fusion methods and applications: a survey, Information\n\nAssessment of image fusion procedures using entropy, image quality, and multispectral classification. J W Roberts, J Van Aardt, F Ahmed, Journal of Applied Remote Sensing. 2123522J. W. Roberts, J. Van Aardt, F. Ahmed, Assessment of image fusion procedures using entropy, image quality, and multispectral classification, Journal of Applied Remote Sensing 2 (1) (2008) 023522.\n\nIn-fibre bragg grating sensors. Y.-J Rao, Measurement Science and Technology. 84355Y.-J. Rao, In-fibre bragg grating sensors, Measurement Science and Technology 8 (4) (1997) 355.\n\nA universal image quality index. Z Wang, A C Bovik, IEEE Signal Processing Letters. 93Z. Wang, A. C. Bovik, A universal image quality index, IEEE Signal Processing Letters 9 (3) (2002) 81-84.\n\nImage fusion and image quality assessment of fused images. M Deshmukh, U Bhosale, International Journal of Image Processing. 45M. Deshmukh, U. Bhosale, Image fusion and image quality assessment of fused images, International Journal of Image Processing 4 (5) (2010) 484-508.\n\nImage quality measures and their performance. A M Eskicioglu, P S Fisher, IEEE Transactions on Communications. 4312A. M. Eskicioglu, P. S. Fisher, Image quality measures and their performance, IEEE Transactions on Communications 43 (12) (1995) 2959-2965.\n\nA new image fusion performance metric based on visual information fidelity. Y Han, Y Cai, Y Cao, X Xu, Information fusion. 142Y. Han, Y. Cai, Y. Cao, X. Xu, A new image fusion performance metric based on visual information fidelity, Information fusion 14 (2) (2013) 127-135.\n\nWhat do we need to build explainable ai systems for the medical domain?. A Holzinger, C Biemann, C S Pattichis, D B Kell, arXiv:1712.09923arXiv preprintA. Holzinger, C. Biemann, C. S. Pattichis, D. B. Kell, What do we need to build explainable ai systems for the medical domain?, arXiv preprint arXiv:1712.09923.\n\nSirf: simultaneous satellite image registration and fusion in a unified framework. C Chen, Y Li, W Liu, J Huang, IEEE Transactions on Image Processing. 2411C. Chen, Y. Li, W. Liu, J. Huang, Sirf: simultaneous satellite image registration and fusion in a unified framework, IEEE Transactions on Image Processing 24 (11) (2015) 4213-4224.\n", "annotations": {"author": "[{\"end\":45,\"start\":36},{\"end\":53,\"start\":46},{\"end\":68,\"start\":54},{\"end\":78,\"start\":69},{\"end\":92,\"start\":79},{\"end\":102,\"start\":93},{\"end\":110,\"start\":103},{\"end\":125,\"start\":111},{\"end\":135,\"start\":126},{\"end\":149,\"start\":136},{\"end\":224,\"start\":150},{\"end\":297,\"start\":225},{\"end\":377,\"start\":298},{\"end\":473,\"start\":378},{\"end\":579,\"start\":474}]", "publisher": null, "author_last_name": "[{\"end\":44,\"start\":42},{\"end\":52,\"start\":50},{\"end\":67,\"start\":62},{\"end\":77,\"start\":75},{\"end\":91,\"start\":86},{\"end\":101,\"start\":99},{\"end\":109,\"start\":107},{\"end\":124,\"start\":119},{\"end\":134,\"start\":132},{\"end\":148,\"start\":143},{\"end\":158,\"start\":156},{\"end\":231,\"start\":229},{\"end\":311,\"start\":306},{\"end\":386,\"start\":384},{\"end\":486,\"start\":481}]", "author_first_name": "[{\"end\":41,\"start\":36},{\"end\":49,\"start\":46},{\"end\":61,\"start\":54},{\"end\":74,\"start\":69},{\"end\":85,\"start\":79},{\"end\":98,\"start\":93},{\"end\":106,\"start\":103},{\"end\":118,\"start\":111},{\"end\":131,\"start\":126},{\"end\":142,\"start\":136},{\"end\":155,\"start\":150},{\"end\":228,\"start\":225},{\"end\":305,\"start\":298},{\"end\":383,\"start\":378},{\"end\":480,\"start\":474}]", "author_affiliation": "[{\"end\":223,\"start\":160},{\"end\":296,\"start\":233},{\"end\":376,\"start\":313},{\"end\":472,\"start\":388},{\"end\":578,\"start\":488}]", "title": "[{\"end\":33,\"start\":1},{\"end\":612,\"start\":580}]", "venue": null, "abstract": "[{\"end\":3712,\"start\":1651}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3950,\"start\":3947},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3952,\"start\":3950},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4152,\"start\":4149},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4420,\"start\":4417},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4525,\"start\":4522},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4527,\"start\":4525},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4529,\"start\":4527},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4557,\"start\":4554},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4559,\"start\":4557},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4581,\"start\":4577},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4584,\"start\":4581},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4600,\"start\":4596},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4603,\"start\":4600},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4628,\"start\":4624},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4631,\"start\":4628},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4659,\"start\":4655},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4662,\"start\":4659},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4686,\"start\":4682},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4689,\"start\":4686},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4849,\"start\":4845},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5408,\"start\":5404},{\"end\":5597,\"start\":5585},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6725,\"start\":6721},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9650,\"start\":9647},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9652,\"start\":9650},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9654,\"start\":9652},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9682,\"start\":9679},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9684,\"start\":9682},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9706,\"start\":9702},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9709,\"start\":9706},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9725,\"start\":9721},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9728,\"start\":9725},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9753,\"start\":9749},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9756,\"start\":9753},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9784,\"start\":9780},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9787,\"start\":9784},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9811,\"start\":9807},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9814,\"start\":9811},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10216,\"start\":10212},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10329,\"start\":10325},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10937,\"start\":10933},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11174,\"start\":11170},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11628,\"start\":11624},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12103,\"start\":12099},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12400,\"start\":12396},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12632,\"start\":12628},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12778,\"start\":12774},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12797,\"start\":12793},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12811,\"start\":12807},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13232,\"start\":13228},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13283,\"start\":13279},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13380,\"start\":13376},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13622,\"start\":13618},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14750,\"start\":14746},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15452,\"start\":15448},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17472,\"start\":17468},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17523,\"start\":17519},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17763,\"start\":17759},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22349,\"start\":22345},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25323,\"start\":25319},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25640,\"start\":25636},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27916,\"start\":27912},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28632,\"start\":28628},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28663,\"start\":28659},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28713,\"start\":28709},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28769,\"start\":28765},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28808,\"start\":28807},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29643,\"start\":29639},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29887,\"start\":29883},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29917,\"start\":29913},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":29966,\"start\":29962},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30001,\"start\":29997},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":30030,\"start\":30026},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30074,\"start\":30070},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33005,\"start\":33001},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":39623,\"start\":39619},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41698,\"start\":41694},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":44115,\"start\":44111},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":45281,\"start\":45277},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45317,\"start\":45314},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":45379,\"start\":45375},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":45419,\"start\":45415},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":59159,\"start\":59158}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":44640,\"start\":44118},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44728,\"start\":44641},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44957,\"start\":44729},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45239,\"start\":44958},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46109,\"start\":45240},{\"attributes\":{\"id\":\"fig_7\"},\"end\":46629,\"start\":46110},{\"attributes\":{\"id\":\"fig_8\"},\"end\":48237,\"start\":46630},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48943,\"start\":48238},{\"attributes\":{\"id\":\"fig_10\"},\"end\":49460,\"start\":48944},{\"attributes\":{\"id\":\"fig_11\"},\"end\":50120,\"start\":49461},{\"attributes\":{\"id\":\"fig_12\"},\"end\":50372,\"start\":50121},{\"attributes\":{\"id\":\"fig_13\"},\"end\":50631,\"start\":50373},{\"attributes\":{\"id\":\"fig_14\"},\"end\":51023,\"start\":50632},{\"attributes\":{\"id\":\"fig_15\"},\"end\":52780,\"start\":51024},{\"attributes\":{\"id\":\"fig_16\"},\"end\":53495,\"start\":52781},{\"attributes\":{\"id\":\"fig_17\"},\"end\":53748,\"start\":53496},{\"attributes\":{\"id\":\"fig_18\"},\"end\":54588,\"start\":53749},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56031,\"start\":54589},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56753,\"start\":56032},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":59081,\"start\":56754}]", "paragraph": "[{\"end\":4421,\"start\":3728},{\"end\":4690,\"start\":4423},{\"end\":5409,\"start\":4692},{\"end\":6382,\"start\":5411},{\"end\":6472,\"start\":6384},{\"end\":6657,\"start\":6474},{\"end\":7408,\"start\":6659},{\"end\":8688,\"start\":7410},{\"end\":9156,\"start\":8690},{\"end\":9394,\"start\":9173},{\"end\":9873,\"start\":9432},{\"end\":10635,\"start\":9913},{\"end\":12822,\"start\":10637},{\"end\":13037,\"start\":12859},{\"end\":13825,\"start\":13039},{\"end\":14004,\"start\":13827},{\"end\":14878,\"start\":14006},{\"end\":14915,\"start\":14880},{\"end\":15394,\"start\":14968},{\"end\":16430,\"start\":15430},{\"end\":16910,\"start\":16432},{\"end\":17903,\"start\":17020},{\"end\":17997,\"start\":17940},{\"end\":18516,\"start\":18146},{\"end\":18521,\"start\":18518},{\"end\":18750,\"start\":18523},{\"end\":19157,\"start\":18752},{\"end\":19258,\"start\":19168},{\"end\":19459,\"start\":19260},{\"end\":19932,\"start\":19483},{\"end\":21229,\"start\":20149},{\"end\":21508,\"start\":21231},{\"end\":21782,\"start\":21592},{\"end\":22011,\"start\":21837},{\"end\":22494,\"start\":22013},{\"end\":23016,\"start\":22554},{\"end\":23661,\"start\":23018},{\"end\":23836,\"start\":23768},{\"end\":23967,\"start\":23870},{\"end\":24286,\"start\":23969},{\"end\":24401,\"start\":24311},{\"end\":24478,\"start\":24403},{\"end\":26093,\"start\":24480},{\"end\":26771,\"start\":26130},{\"end\":27132,\"start\":26808},{\"end\":28164,\"start\":27153},{\"end\":28345,\"start\":28166},{\"end\":29120,\"start\":28361},{\"end\":29254,\"start\":29215},{\"end\":30128,\"start\":29340},{\"end\":30281,\"start\":30130},{\"end\":30591,\"start\":30316},{\"end\":30786,\"start\":30593},{\"end\":31127,\"start\":30826},{\"end\":31302,\"start\":31129},{\"end\":31480,\"start\":31304},{\"end\":31866,\"start\":31670},{\"end\":31900,\"start\":31895},{\"end\":32158,\"start\":31997},{\"end\":32253,\"start\":32160},{\"end\":33008,\"start\":32279},{\"end\":33494,\"start\":33074},{\"end\":34513,\"start\":33496},{\"end\":35571,\"start\":34515},{\"end\":35689,\"start\":35573},{\"end\":35978,\"start\":35717},{\"end\":36692,\"start\":36015},{\"end\":36966,\"start\":36740},{\"end\":37491,\"start\":37026},{\"end\":37804,\"start\":37493},{\"end\":37983,\"start\":37806},{\"end\":38129,\"start\":37985},{\"end\":38651,\"start\":38194},{\"end\":39177,\"start\":38653},{\"end\":39436,\"start\":39179},{\"end\":39708,\"start\":39476},{\"end\":40857,\"start\":39710},{\"end\":41357,\"start\":40859},{\"end\":41772,\"start\":41372},{\"end\":41992,\"start\":41812},{\"end\":42356,\"start\":41994},{\"end\":43440,\"start\":42358},{\"end\":44117,\"start\":43455}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16998,\"start\":16911},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17939,\"start\":17904},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18145,\"start\":17998},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20148,\"start\":19933},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21591,\"start\":21509},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21836,\"start\":21783},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22553,\"start\":22495},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23697,\"start\":23662},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23767,\"start\":23697},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23869,\"start\":23837},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26129,\"start\":26094},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26807,\"start\":26772},{\"attributes\":{\"id\":\"formula_12\"},\"end\":29214,\"start\":29121},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29322,\"start\":29255},{\"attributes\":{\"id\":\"formula_14\"},\"end\":30315,\"start\":30282},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30825,\"start\":30787},{\"attributes\":{\"id\":\"formula_16\"},\"end\":31599,\"start\":31481},{\"attributes\":{\"id\":\"formula_17\"},\"end\":31631,\"start\":31599},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31894,\"start\":31867},{\"attributes\":{\"id\":\"formula_19\"},\"end\":31996,\"start\":31901},{\"attributes\":{\"id\":\"formula_20\"},\"end\":32278,\"start\":32254},{\"attributes\":{\"id\":\"formula_21\"},\"end\":36014,\"start\":35979},{\"attributes\":{\"id\":\"formula_22\"},\"end\":38193,\"start\":38130}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35570,\"start\":35563},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":36600,\"start\":36593}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3726,\"start\":3714},{\"attributes\":{\"n\":\"2.\"},\"end\":9171,\"start\":9159},{\"attributes\":{\"n\":\"2.1.\"},\"end\":9430,\"start\":9397},{\"end\":9911,\"start\":9876},{\"attributes\":{\"n\":\"2.2.\"},\"end\":12857,\"start\":12825},{\"attributes\":{\"n\":\"2.3.\"},\"end\":14966,\"start\":14918},{\"attributes\":{\"n\":\"2.3.1.\"},\"end\":15428,\"start\":15397},{\"attributes\":{\"n\":\"2.3.3.\"},\"end\":17018,\"start\":17000},{\"attributes\":{\"n\":\"3.\"},\"end\":19166,\"start\":19160},{\"attributes\":{\"n\":\"3.1.\"},\"end\":19481,\"start\":19462},{\"attributes\":{\"n\":\"3.2.\"},\"end\":24309,\"start\":24289},{\"attributes\":{\"n\":\"3.3.\"},\"end\":27151,\"start\":27135},{\"attributes\":{\"n\":\"4.\"},\"end\":28359,\"start\":28348},{\"attributes\":{\"n\":\"4.1.\"},\"end\":29338,\"start\":29324},{\"end\":31668,\"start\":31633},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":33034,\"start\":33011},{\"end\":33072,\"start\":33037},{\"attributes\":{\"n\":\"4.2.3.\"},\"end\":35715,\"start\":35692},{\"attributes\":{\"n\":\"4.3.\"},\"end\":36738,\"start\":36695},{\"attributes\":{\"n\":\"4.4.\"},\"end\":37024,\"start\":36969},{\"end\":39474,\"start\":39439},{\"attributes\":{\"n\":\"5.\"},\"end\":41370,\"start\":41360},{\"end\":41810,\"start\":41775},{\"attributes\":{\"n\":\"6.\"},\"end\":43453,\"start\":43443},{\"end\":44129,\"start\":44119},{\"end\":44652,\"start\":44642},{\"end\":44740,\"start\":44730},{\"end\":44969,\"start\":44959},{\"end\":46112,\"start\":46111},{\"end\":48955,\"start\":48945},{\"end\":50132,\"start\":50122},{\"end\":50384,\"start\":50374},{\"end\":51035,\"start\":51025},{\"end\":52803,\"start\":52782},{\"end\":53508,\"start\":53497},{\"end\":56042,\"start\":56033}]", "table": "[{\"end\":56031,\"start\":54859},{\"end\":56753,\"start\":56236},{\"end\":59081,\"start\":57937}]", "figure_caption": "[{\"end\":44640,\"start\":44131},{\"end\":44728,\"start\":44654},{\"end\":44957,\"start\":44742},{\"end\":45239,\"start\":44971},{\"end\":46109,\"start\":45242},{\"end\":46629,\"start\":46113},{\"end\":48237,\"start\":46632},{\"end\":48943,\"start\":48240},{\"end\":49460,\"start\":48957},{\"end\":50120,\"start\":49463},{\"end\":50372,\"start\":50134},{\"end\":50631,\"start\":50386},{\"end\":51023,\"start\":50634},{\"end\":52780,\"start\":51037},{\"end\":53495,\"start\":52807},{\"end\":53748,\"start\":53511},{\"end\":54588,\"start\":53751},{\"end\":54859,\"start\":54591},{\"end\":56236,\"start\":56044},{\"end\":57937,\"start\":56756}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6471,\"start\":6465},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7105,\"start\":7099},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19800,\"start\":19790},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21085,\"start\":21079},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24581,\"start\":24575},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26292,\"start\":26286},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":34751,\"start\":34745},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":35972,\"start\":35964},{\"end\":36585,\"start\":36577},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36965,\"start\":36958},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":37557,\"start\":37551},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37803,\"start\":37797},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39320,\"start\":39311},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39435,\"start\":39428},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39855,\"start\":39848}]", "bib_author_first_name": "[{\"end\":59387,\"start\":59386},{\"end\":59396,\"start\":59395},{\"end\":59405,\"start\":59404},{\"end\":59698,\"start\":59697},{\"end\":59704,\"start\":59703},{\"end\":59712,\"start\":59711},{\"end\":59720,\"start\":59719},{\"end\":59727,\"start\":59726},{\"end\":59934,\"start\":59933},{\"end\":60132,\"start\":60131},{\"end\":60139,\"start\":60138},{\"end\":60148,\"start\":60147},{\"end\":60155,\"start\":60154},{\"end\":60163,\"start\":60162},{\"end\":60170,\"start\":60169},{\"end\":60177,\"start\":60176},{\"end\":60460,\"start\":60459},{\"end\":60466,\"start\":60465},{\"end\":60474,\"start\":60473},{\"end\":60688,\"start\":60687},{\"end\":60699,\"start\":60698},{\"end\":60701,\"start\":60700},{\"end\":60986,\"start\":60985},{\"end\":60995,\"start\":60994},{\"end\":60997,\"start\":60996},{\"end\":61403,\"start\":61402},{\"end\":61411,\"start\":61410},{\"end\":61419,\"start\":61418},{\"end\":61427,\"start\":61426},{\"end\":61433,\"start\":61432},{\"end\":61749,\"start\":61748},{\"end\":61755,\"start\":61754},{\"end\":61762,\"start\":61761},{\"end\":62122,\"start\":62121},{\"end\":62131,\"start\":62130},{\"end\":62138,\"start\":62137},{\"end\":62449,\"start\":62448},{\"end\":62457,\"start\":62456},{\"end\":62466,\"start\":62465},{\"end\":62745,\"start\":62744},{\"end\":62747,\"start\":62746},{\"end\":62762,\"start\":62761},{\"end\":62770,\"start\":62769},{\"end\":63156,\"start\":63155},{\"end\":63164,\"start\":63163},{\"end\":63171,\"start\":63170},{\"end\":63544,\"start\":63543},{\"end\":63553,\"start\":63552},{\"end\":63559,\"start\":63558},{\"end\":63566,\"start\":63565},{\"end\":63575,\"start\":63574},{\"end\":63879,\"start\":63878},{\"end\":63887,\"start\":63886},{\"end\":63895,\"start\":63894},{\"end\":63903,\"start\":63902},{\"end\":63909,\"start\":63908},{\"end\":64229,\"start\":64228},{\"end\":64236,\"start\":64235},{\"end\":64243,\"start\":64242},{\"end\":64535,\"start\":64534},{\"end\":64541,\"start\":64540},{\"end\":64549,\"start\":64548},{\"end\":64557,\"start\":64556},{\"end\":64872,\"start\":64871},{\"end\":64878,\"start\":64877},{\"end\":64886,\"start\":64885},{\"end\":64892,\"start\":64891},{\"end\":65185,\"start\":65184},{\"end\":65193,\"start\":65192},{\"end\":65200,\"start\":65199},{\"end\":65208,\"start\":65207},{\"end\":65216,\"start\":65215},{\"end\":65223,\"start\":65222},{\"end\":65526,\"start\":65525},{\"end\":65532,\"start\":65531},{\"end\":65540,\"start\":65539},{\"end\":65548,\"start\":65547},{\"end\":65554,\"start\":65553},{\"end\":65804,\"start\":65803},{\"end\":65811,\"start\":65810},{\"end\":65819,\"start\":65818},{\"end\":65827,\"start\":65826},{\"end\":65829,\"start\":65828},{\"end\":65837,\"start\":65836},{\"end\":65839,\"start\":65838},{\"end\":65847,\"start\":65846},{\"end\":66093,\"start\":66092},{\"end\":66099,\"start\":66098},{\"end\":66107,\"start\":66106},{\"end\":66356,\"start\":66355},{\"end\":66617,\"start\":66616},{\"end\":66626,\"start\":66625},{\"end\":66633,\"start\":66632},{\"end\":66635,\"start\":66634},{\"end\":66643,\"start\":66642},{\"end\":66650,\"start\":66649},{\"end\":66934,\"start\":66933},{\"end\":66946,\"start\":66945},{\"end\":66948,\"start\":66947},{\"end\":67331,\"start\":67330},{\"end\":67338,\"start\":67337},{\"end\":67346,\"start\":67345},{\"end\":67354,\"start\":67353},{\"end\":67595,\"start\":67594},{\"end\":67602,\"start\":67601},{\"end\":67610,\"start\":67609},{\"end\":67619,\"start\":67618},{\"end\":67627,\"start\":67626},{\"end\":68010,\"start\":68009},{\"end\":68019,\"start\":68018},{\"end\":68027,\"start\":68026},{\"end\":68033,\"start\":68032},{\"end\":68042,\"start\":68041},{\"end\":68327,\"start\":68326},{\"end\":68334,\"start\":68333},{\"end\":68342,\"start\":68341},{\"end\":68344,\"start\":68343},{\"end\":68352,\"start\":68351},{\"end\":68354,\"start\":68353},{\"end\":68552,\"start\":68551},{\"end\":68560,\"start\":68559},{\"end\":68573,\"start\":68572},{\"end\":68586,\"start\":68585},{\"end\":68778,\"start\":68777},{\"end\":68792,\"start\":68791},{\"end\":68809,\"start\":68808},{\"end\":68818,\"start\":68817},{\"end\":68824,\"start\":68823},{\"end\":68840,\"start\":68839},{\"end\":68849,\"start\":68848},{\"end\":68862,\"start\":68861},{\"end\":69253,\"start\":69252},{\"end\":69264,\"start\":69263},{\"end\":69272,\"start\":69271},{\"end\":69480,\"start\":69479},{\"end\":69492,\"start\":69491},{\"end\":69504,\"start\":69503},{\"end\":69524,\"start\":69513},{\"end\":69649,\"start\":69648},{\"end\":69656,\"start\":69655},{\"end\":69662,\"start\":69661},{\"end\":69669,\"start\":69668},{\"end\":69671,\"start\":69670},{\"end\":69678,\"start\":69677},{\"end\":69686,\"start\":69685},{\"end\":69688,\"start\":69687},{\"end\":69975,\"start\":69974},{\"end\":69981,\"start\":69980},{\"end\":70187,\"start\":70186},{\"end\":70189,\"start\":70188},{\"end\":70199,\"start\":70198},{\"end\":70441,\"start\":70440},{\"end\":70449,\"start\":70448},{\"end\":70744,\"start\":70743},{\"end\":70755,\"start\":70754},{\"end\":70767,\"start\":70766},{\"end\":70778,\"start\":70777},{\"end\":71021,\"start\":71020},{\"end\":71023,\"start\":71022},{\"end\":71032,\"start\":71031},{\"end\":71034,\"start\":71033},{\"end\":71054,\"start\":71053},{\"end\":71056,\"start\":71055},{\"end\":71067,\"start\":71066},{\"end\":71069,\"start\":71068},{\"end\":71077,\"start\":71076},{\"end\":71370,\"start\":71369},{\"end\":71372,\"start\":71371},{\"end\":71387,\"start\":71386},{\"end\":71658,\"start\":71657},{\"end\":71664,\"start\":71663},{\"end\":71670,\"start\":71669},{\"end\":71893,\"start\":71892},{\"end\":71895,\"start\":71894},{\"end\":71906,\"start\":71905},{\"end\":71919,\"start\":71918},{\"end\":72202,\"start\":72198},{\"end\":72380,\"start\":72379},{\"end\":72388,\"start\":72387},{\"end\":72390,\"start\":72389},{\"end\":72599,\"start\":72598},{\"end\":72611,\"start\":72610},{\"end\":72862,\"start\":72861},{\"end\":72864,\"start\":72863},{\"end\":72878,\"start\":72877},{\"end\":72880,\"start\":72879},{\"end\":73148,\"start\":73147},{\"end\":73155,\"start\":73154},{\"end\":73162,\"start\":73161},{\"end\":73169,\"start\":73168},{\"end\":73421,\"start\":73420},{\"end\":73434,\"start\":73433},{\"end\":73445,\"start\":73444},{\"end\":73447,\"start\":73446},{\"end\":73460,\"start\":73459},{\"end\":73462,\"start\":73461},{\"end\":73745,\"start\":73744},{\"end\":73753,\"start\":73752},{\"end\":73759,\"start\":73758},{\"end\":73766,\"start\":73765}]", "bib_author_last_name": "[{\"end\":59393,\"start\":59388},{\"end\":59402,\"start\":59397},{\"end\":59701,\"start\":59699},{\"end\":59709,\"start\":59705},{\"end\":59717,\"start\":59713},{\"end\":59724,\"start\":59721},{\"end\":59730,\"start\":59728},{\"end\":59939,\"start\":59935},{\"end\":60136,\"start\":60133},{\"end\":60145,\"start\":60140},{\"end\":60152,\"start\":60149},{\"end\":60160,\"start\":60156},{\"end\":60167,\"start\":60164},{\"end\":60174,\"start\":60171},{\"end\":60180,\"start\":60178},{\"end\":60463,\"start\":60461},{\"end\":60471,\"start\":60467},{\"end\":60477,\"start\":60475},{\"end\":60696,\"start\":60689},{\"end\":60707,\"start\":60702},{\"end\":60713,\"start\":60709},{\"end\":60992,\"start\":60987},{\"end\":61002,\"start\":60998},{\"end\":61272,\"start\":61237},{\"end\":61408,\"start\":61404},{\"end\":61416,\"start\":61412},{\"end\":61424,\"start\":61420},{\"end\":61430,\"start\":61428},{\"end\":61437,\"start\":61434},{\"end\":61752,\"start\":61750},{\"end\":61759,\"start\":61756},{\"end\":61767,\"start\":61763},{\"end\":62128,\"start\":62123},{\"end\":62135,\"start\":62132},{\"end\":62142,\"start\":62139},{\"end\":62454,\"start\":62450},{\"end\":62463,\"start\":62458},{\"end\":62470,\"start\":62467},{\"end\":62759,\"start\":62748},{\"end\":62767,\"start\":62763},{\"end\":62774,\"start\":62771},{\"end\":63161,\"start\":63157},{\"end\":63168,\"start\":63165},{\"end\":63176,\"start\":63172},{\"end\":63550,\"start\":63545},{\"end\":63556,\"start\":63554},{\"end\":63563,\"start\":63560},{\"end\":63572,\"start\":63567},{\"end\":63581,\"start\":63576},{\"end\":63884,\"start\":63880},{\"end\":63892,\"start\":63888},{\"end\":63900,\"start\":63896},{\"end\":63906,\"start\":63904},{\"end\":63912,\"start\":63910},{\"end\":64233,\"start\":64230},{\"end\":64240,\"start\":64237},{\"end\":64248,\"start\":64244},{\"end\":64538,\"start\":64536},{\"end\":64546,\"start\":64542},{\"end\":64554,\"start\":64550},{\"end\":64562,\"start\":64558},{\"end\":64875,\"start\":64873},{\"end\":64883,\"start\":64879},{\"end\":64889,\"start\":64887},{\"end\":64898,\"start\":64893},{\"end\":65190,\"start\":65186},{\"end\":65197,\"start\":65194},{\"end\":65205,\"start\":65201},{\"end\":65213,\"start\":65209},{\"end\":65220,\"start\":65217},{\"end\":65228,\"start\":65224},{\"end\":65529,\"start\":65527},{\"end\":65537,\"start\":65533},{\"end\":65545,\"start\":65541},{\"end\":65551,\"start\":65549},{\"end\":65558,\"start\":65555},{\"end\":65808,\"start\":65805},{\"end\":65816,\"start\":65812},{\"end\":65824,\"start\":65820},{\"end\":65834,\"start\":65830},{\"end\":65844,\"start\":65840},{\"end\":65852,\"start\":65848},{\"end\":66096,\"start\":66094},{\"end\":66104,\"start\":66100},{\"end\":66110,\"start\":66108},{\"end\":66363,\"start\":66357},{\"end\":66623,\"start\":66618},{\"end\":66630,\"start\":66627},{\"end\":66640,\"start\":66636},{\"end\":66647,\"start\":66644},{\"end\":66654,\"start\":66651},{\"end\":66943,\"start\":66935},{\"end\":66954,\"start\":66949},{\"end\":67335,\"start\":67332},{\"end\":67343,\"start\":67339},{\"end\":67351,\"start\":67347},{\"end\":67359,\"start\":67355},{\"end\":67599,\"start\":67596},{\"end\":67607,\"start\":67603},{\"end\":67616,\"start\":67611},{\"end\":67624,\"start\":67620},{\"end\":67632,\"start\":67628},{\"end\":68016,\"start\":68011},{\"end\":68024,\"start\":68020},{\"end\":68030,\"start\":68028},{\"end\":68039,\"start\":68034},{\"end\":68047,\"start\":68043},{\"end\":68331,\"start\":68328},{\"end\":68339,\"start\":68335},{\"end\":68349,\"start\":68345},{\"end\":68359,\"start\":68355},{\"end\":68557,\"start\":68553},{\"end\":68570,\"start\":68561},{\"end\":68583,\"start\":68574},{\"end\":68593,\"start\":68587},{\"end\":68789,\"start\":68779},{\"end\":68806,\"start\":68793},{\"end\":68815,\"start\":68810},{\"end\":68821,\"start\":68819},{\"end\":68837,\"start\":68825},{\"end\":68846,\"start\":68841},{\"end\":68859,\"start\":68850},{\"end\":68869,\"start\":68863},{\"end\":69261,\"start\":69254},{\"end\":69269,\"start\":69265},{\"end\":69281,\"start\":69273},{\"end\":69489,\"start\":69481},{\"end\":69501,\"start\":69493},{\"end\":69511,\"start\":69505},{\"end\":69528,\"start\":69525},{\"end\":69653,\"start\":69650},{\"end\":69659,\"start\":69657},{\"end\":69666,\"start\":69663},{\"end\":69675,\"start\":69672},{\"end\":69683,\"start\":69679},{\"end\":69696,\"start\":69689},{\"end\":69978,\"start\":69976},{\"end\":69988,\"start\":69982},{\"end\":70196,\"start\":70190},{\"end\":70202,\"start\":70200},{\"end\":70446,\"start\":70442},{\"end\":70452,\"start\":70450},{\"end\":70752,\"start\":70745},{\"end\":70764,\"start\":70756},{\"end\":70775,\"start\":70768},{\"end\":70787,\"start\":70779},{\"end\":71029,\"start\":71024},{\"end\":71051,\"start\":71035},{\"end\":71064,\"start\":71057},{\"end\":71074,\"start\":71070},{\"end\":71089,\"start\":71078},{\"end\":71384,\"start\":71373},{\"end\":71393,\"start\":71388},{\"end\":71661,\"start\":71659},{\"end\":71667,\"start\":71665},{\"end\":71673,\"start\":71671},{\"end\":71903,\"start\":71896},{\"end\":71916,\"start\":71907},{\"end\":71925,\"start\":71920},{\"end\":72206,\"start\":72203},{\"end\":72385,\"start\":72381},{\"end\":72396,\"start\":72391},{\"end\":72608,\"start\":72600},{\"end\":72619,\"start\":72612},{\"end\":72875,\"start\":72865},{\"end\":72887,\"start\":72881},{\"end\":73152,\"start\":73149},{\"end\":73159,\"start\":73156},{\"end\":73166,\"start\":73163},{\"end\":73172,\"start\":73170},{\"end\":73431,\"start\":73422},{\"end\":73442,\"start\":73435},{\"end\":73457,\"start\":73448},{\"end\":73467,\"start\":73463},{\"end\":73750,\"start\":73746},{\"end\":73756,\"start\":73754},{\"end\":73763,\"start\":73760},{\"end\":73772,\"start\":73767}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":25291441},\"end\":59632,\"start\":59241},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207113667},\"end\":59886,\"start\":59634},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6965804},\"end\":60075,\"start\":59888},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":125717966},\"end\":60375,\"start\":60077},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13273826},\"end\":60646,\"start\":60377},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2352996},\"end\":60852,\"start\":60648},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2018638},\"end\":61233,\"start\":60854},{\"attributes\":{\"id\":\"b7\"},\"end\":61309,\"start\":61235},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":122945792},\"end\":61653,\"start\":61311},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3141171},\"end\":62004,\"start\":61655},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":120585341},\"end\":62366,\"start\":62006},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":122238377},\"end\":62662,\"start\":62368},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2180547},\"end\":63006,\"start\":62664},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":121695286},\"end\":63432,\"start\":63008},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":37093748},\"end\":63779,\"start\":63434},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":120097290},\"end\":64131,\"start\":63781},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":30096994},\"end\":64429,\"start\":64133},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":125122786},\"end\":64779,\"start\":64431},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":205432776},\"end\":65082,\"start\":64781},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":125527705},\"end\":65463,\"start\":65084},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9263669},\"end\":65719,\"start\":65465},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":46849537},\"end\":66054,\"start\":65721},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":493320},\"end\":66275,\"start\":66056},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46684442},\"end\":66516,\"start\":66277},{\"attributes\":{\"id\":\"b24\"},\"end\":66857,\"start\":66518},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":118427459},\"end\":67261,\"start\":66859},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":11925688},\"end\":67522,\"start\":67263},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":49379448},\"end\":67938,\"start\":67524},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":43352112},\"end\":68269,\"start\":67940},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":18649677},\"end\":68549,\"start\":68271},{\"attributes\":{\"id\":\"b30\"},\"end\":68775,\"start\":68551},{\"attributes\":{\"id\":\"b31\"},\"end\":69156,\"start\":68777},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b32\"},\"end\":69475,\"start\":69158},{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b33\"},\"end\":69646,\"start\":69477},{\"attributes\":{\"id\":\"b34\"},\"end\":69972,\"start\":69648},{\"attributes\":{\"doi\":\"arXiv:1511.07122\",\"id\":\"b35\"},\"end\":70184,\"start\":69974},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b36\"},\"end\":70373,\"start\":70186},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":119888433},\"end\":70683,\"start\":70375},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":17658730},\"end\":70959,\"start\":70685},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":29717271},\"end\":71287,\"start\":70961},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":123883362},\"end\":71585,\"start\":71289},{\"attributes\":{\"id\":\"b41\"},\"end\":71788,\"start\":71587},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":119424378},\"end\":72164,\"start\":71790},{\"attributes\":{\"id\":\"b43\"},\"end\":72344,\"start\":72166},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":14488670},\"end\":72537,\"start\":72346},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":128696536},\"end\":72813,\"start\":72539},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":9186472},\"end\":73069,\"start\":72815},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":41804650},\"end\":73345,\"start\":73071},{\"attributes\":{\"doi\":\"arXiv:1712.09923\",\"id\":\"b48\"},\"end\":73659,\"start\":73347},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":18599584},\"end\":73997,\"start\":73661}]", "bib_title": "[{\"end\":59384,\"start\":59241},{\"end\":59695,\"start\":59634},{\"end\":59931,\"start\":59888},{\"end\":60129,\"start\":60077},{\"end\":60457,\"start\":60377},{\"end\":60685,\"start\":60648},{\"end\":60983,\"start\":60854},{\"end\":61400,\"start\":61311},{\"end\":61746,\"start\":61655},{\"end\":62119,\"start\":62006},{\"end\":62446,\"start\":62368},{\"end\":62742,\"start\":62664},{\"end\":63153,\"start\":63008},{\"end\":63541,\"start\":63434},{\"end\":63876,\"start\":63781},{\"end\":64226,\"start\":64133},{\"end\":64532,\"start\":64431},{\"end\":64869,\"start\":64781},{\"end\":65182,\"start\":65084},{\"end\":65523,\"start\":65465},{\"end\":65801,\"start\":65721},{\"end\":66090,\"start\":66056},{\"end\":66353,\"start\":66277},{\"end\":66614,\"start\":66518},{\"end\":66931,\"start\":66859},{\"end\":67328,\"start\":67263},{\"end\":67592,\"start\":67524},{\"end\":68007,\"start\":67940},{\"end\":68324,\"start\":68271},{\"end\":70438,\"start\":70375},{\"end\":70741,\"start\":70685},{\"end\":71018,\"start\":70961},{\"end\":71367,\"start\":71289},{\"end\":71890,\"start\":71790},{\"end\":72196,\"start\":72166},{\"end\":72377,\"start\":72346},{\"end\":72596,\"start\":72539},{\"end\":72859,\"start\":72815},{\"end\":73145,\"start\":73071},{\"end\":73742,\"start\":73661}]", "bib_author": "[{\"end\":59395,\"start\":59386},{\"end\":59404,\"start\":59395},{\"end\":59408,\"start\":59404},{\"end\":59703,\"start\":59697},{\"end\":59711,\"start\":59703},{\"end\":59719,\"start\":59711},{\"end\":59726,\"start\":59719},{\"end\":59732,\"start\":59726},{\"end\":59941,\"start\":59933},{\"end\":60138,\"start\":60131},{\"end\":60147,\"start\":60138},{\"end\":60154,\"start\":60147},{\"end\":60162,\"start\":60154},{\"end\":60169,\"start\":60162},{\"end\":60176,\"start\":60169},{\"end\":60182,\"start\":60176},{\"end\":60465,\"start\":60459},{\"end\":60473,\"start\":60465},{\"end\":60479,\"start\":60473},{\"end\":60698,\"start\":60687},{\"end\":60709,\"start\":60698},{\"end\":60715,\"start\":60709},{\"end\":60994,\"start\":60985},{\"end\":61004,\"start\":60994},{\"end\":61274,\"start\":61237},{\"end\":61410,\"start\":61402},{\"end\":61418,\"start\":61410},{\"end\":61426,\"start\":61418},{\"end\":61432,\"start\":61426},{\"end\":61439,\"start\":61432},{\"end\":61754,\"start\":61748},{\"end\":61761,\"start\":61754},{\"end\":61769,\"start\":61761},{\"end\":62130,\"start\":62121},{\"end\":62137,\"start\":62130},{\"end\":62144,\"start\":62137},{\"end\":62456,\"start\":62448},{\"end\":62465,\"start\":62456},{\"end\":62472,\"start\":62465},{\"end\":62761,\"start\":62744},{\"end\":62769,\"start\":62761},{\"end\":62776,\"start\":62769},{\"end\":63163,\"start\":63155},{\"end\":63170,\"start\":63163},{\"end\":63178,\"start\":63170},{\"end\":63552,\"start\":63543},{\"end\":63558,\"start\":63552},{\"end\":63565,\"start\":63558},{\"end\":63574,\"start\":63565},{\"end\":63583,\"start\":63574},{\"end\":63886,\"start\":63878},{\"end\":63894,\"start\":63886},{\"end\":63902,\"start\":63894},{\"end\":63908,\"start\":63902},{\"end\":63914,\"start\":63908},{\"end\":64235,\"start\":64228},{\"end\":64242,\"start\":64235},{\"end\":64250,\"start\":64242},{\"end\":64540,\"start\":64534},{\"end\":64548,\"start\":64540},{\"end\":64556,\"start\":64548},{\"end\":64564,\"start\":64556},{\"end\":64877,\"start\":64871},{\"end\":64885,\"start\":64877},{\"end\":64891,\"start\":64885},{\"end\":64900,\"start\":64891},{\"end\":65192,\"start\":65184},{\"end\":65199,\"start\":65192},{\"end\":65207,\"start\":65199},{\"end\":65215,\"start\":65207},{\"end\":65222,\"start\":65215},{\"end\":65230,\"start\":65222},{\"end\":65531,\"start\":65525},{\"end\":65539,\"start\":65531},{\"end\":65547,\"start\":65539},{\"end\":65553,\"start\":65547},{\"end\":65560,\"start\":65553},{\"end\":65810,\"start\":65803},{\"end\":65818,\"start\":65810},{\"end\":65826,\"start\":65818},{\"end\":65836,\"start\":65826},{\"end\":65846,\"start\":65836},{\"end\":65854,\"start\":65846},{\"end\":66098,\"start\":66092},{\"end\":66106,\"start\":66098},{\"end\":66112,\"start\":66106},{\"end\":66365,\"start\":66355},{\"end\":66625,\"start\":66616},{\"end\":66632,\"start\":66625},{\"end\":66642,\"start\":66632},{\"end\":66649,\"start\":66642},{\"end\":66656,\"start\":66649},{\"end\":66945,\"start\":66933},{\"end\":66956,\"start\":66945},{\"end\":67337,\"start\":67330},{\"end\":67345,\"start\":67337},{\"end\":67353,\"start\":67345},{\"end\":67361,\"start\":67353},{\"end\":67601,\"start\":67594},{\"end\":67609,\"start\":67601},{\"end\":67618,\"start\":67609},{\"end\":67626,\"start\":67618},{\"end\":67634,\"start\":67626},{\"end\":68018,\"start\":68009},{\"end\":68026,\"start\":68018},{\"end\":68032,\"start\":68026},{\"end\":68041,\"start\":68032},{\"end\":68049,\"start\":68041},{\"end\":68333,\"start\":68326},{\"end\":68341,\"start\":68333},{\"end\":68351,\"start\":68341},{\"end\":68361,\"start\":68351},{\"end\":68559,\"start\":68551},{\"end\":68572,\"start\":68559},{\"end\":68585,\"start\":68572},{\"end\":68595,\"start\":68585},{\"end\":68791,\"start\":68777},{\"end\":68808,\"start\":68791},{\"end\":68817,\"start\":68808},{\"end\":68823,\"start\":68817},{\"end\":68839,\"start\":68823},{\"end\":68848,\"start\":68839},{\"end\":68861,\"start\":68848},{\"end\":68871,\"start\":68861},{\"end\":69263,\"start\":69252},{\"end\":69271,\"start\":69263},{\"end\":69283,\"start\":69271},{\"end\":69491,\"start\":69479},{\"end\":69503,\"start\":69491},{\"end\":69513,\"start\":69503},{\"end\":69530,\"start\":69513},{\"end\":69655,\"start\":69648},{\"end\":69661,\"start\":69655},{\"end\":69668,\"start\":69661},{\"end\":69677,\"start\":69668},{\"end\":69685,\"start\":69677},{\"end\":69698,\"start\":69685},{\"end\":69980,\"start\":69974},{\"end\":69990,\"start\":69980},{\"end\":70198,\"start\":70186},{\"end\":70204,\"start\":70198},{\"end\":70448,\"start\":70440},{\"end\":70454,\"start\":70448},{\"end\":70754,\"start\":70743},{\"end\":70766,\"start\":70754},{\"end\":70777,\"start\":70766},{\"end\":70789,\"start\":70777},{\"end\":71031,\"start\":71020},{\"end\":71053,\"start\":71031},{\"end\":71066,\"start\":71053},{\"end\":71076,\"start\":71066},{\"end\":71091,\"start\":71076},{\"end\":71386,\"start\":71369},{\"end\":71395,\"start\":71386},{\"end\":71663,\"start\":71657},{\"end\":71669,\"start\":71663},{\"end\":71675,\"start\":71669},{\"end\":71905,\"start\":71892},{\"end\":71918,\"start\":71905},{\"end\":71927,\"start\":71918},{\"end\":72208,\"start\":72198},{\"end\":72387,\"start\":72379},{\"end\":72398,\"start\":72387},{\"end\":72610,\"start\":72598},{\"end\":72621,\"start\":72610},{\"end\":72877,\"start\":72861},{\"end\":72889,\"start\":72877},{\"end\":73154,\"start\":73147},{\"end\":73161,\"start\":73154},{\"end\":73168,\"start\":73161},{\"end\":73174,\"start\":73168},{\"end\":73433,\"start\":73420},{\"end\":73444,\"start\":73433},{\"end\":73459,\"start\":73444},{\"end\":73469,\"start\":73459},{\"end\":73752,\"start\":73744},{\"end\":73758,\"start\":73752},{\"end\":73765,\"start\":73758},{\"end\":73774,\"start\":73765}]", "bib_venue": "[{\"end\":67073,\"start\":67023},{\"end\":59419,\"start\":59408},{\"end\":59746,\"start\":59732},{\"end\":59968,\"start\":59941},{\"end\":60211,\"start\":60182},{\"end\":60497,\"start\":60479},{\"end\":60734,\"start\":60715},{\"end\":61027,\"start\":61004},{\"end\":61468,\"start\":61439},{\"end\":61812,\"start\":61769},{\"end\":62173,\"start\":62144},{\"end\":62501,\"start\":62472},{\"end\":62822,\"start\":62776},{\"end\":63207,\"start\":63178},{\"end\":63589,\"start\":63583},{\"end\":63943,\"start\":63914},{\"end\":64268,\"start\":64250},{\"end\":64593,\"start\":64564},{\"end\":64918,\"start\":64900},{\"end\":65259,\"start\":65230},{\"end\":65578,\"start\":65560},{\"end\":65872,\"start\":65854},{\"end\":66149,\"start\":66112},{\"end\":66383,\"start\":66365},{\"end\":66674,\"start\":66656},{\"end\":67021,\"start\":66956},{\"end\":67379,\"start\":67361},{\"end\":67711,\"start\":67634},{\"end\":68090,\"start\":68049},{\"end\":68391,\"start\":68361},{\"end\":68641,\"start\":68595},{\"end\":68920,\"start\":68871},{\"end\":69250,\"start\":69158},{\"end\":69746,\"start\":69698},{\"end\":70061,\"start\":70006},{\"end\":70261,\"start\":70219},{\"end\":70511,\"start\":70454},{\"end\":70807,\"start\":70789},{\"end\":71109,\"start\":71091},{\"end\":71424,\"start\":71395},{\"end\":71655,\"start\":71587},{\"end\":71960,\"start\":71927},{\"end\":72242,\"start\":72208},{\"end\":72428,\"start\":72398},{\"end\":72662,\"start\":72621},{\"end\":72924,\"start\":72889},{\"end\":73192,\"start\":73174},{\"end\":73418,\"start\":73347},{\"end\":73811,\"start\":73774}]"}}}, "year": 2023, "month": 12, "day": 17}