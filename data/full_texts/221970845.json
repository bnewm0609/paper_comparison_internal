{"id": 221970845, "updated": "2023-10-06 10:54:00.466", "metadata": {"title": "Distribution Matching for Crowd Counting", "authors": "[{\"first\":\"Boyu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Huidong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Dimitris\",\"last\":\"Samaras\",\"middle\":[]},{\"first\":\"Minh\",\"last\":\"Hoai\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 9, "day": 28}, "abstract": "In crowd counting, each training image contains multiple people, where each person is annotated by a dot. Existing crowd counting methods need to use a Gaussian to smooth each annotated dot or to estimate the likelihood of every pixel given the annotated point. In this paper, we show that imposing Gaussians to annotations hurts generalization performance. Instead, we propose to use Distribution Matching for crowd COUNTing (DM-Count). In DM-Count, we use Optimal Transport (OT) to measure the similarity between the normalized predicted density map and the normalized ground truth density map. To stabilize OT computation, we include a Total Variation loss in our model. We show that the generalization error bound of DM-Count is tighter than that of the Gaussian smoothed methods. In terms of Mean Absolute Error, DM-Count outperforms the previous state-of-the-art methods by a large margin on two large-scale counting datasets, UCF-QNRF and NWPU, and achieves the state-of-the-art results on the ShanghaiTech and UCF-CC50 datasets. DM-Count reduced the error of the state-of-the-art published result by approximately 16%. Code is available at https://github.com/cvlab-stonybrook/DM-Count.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.13077", "mag": "3097994591", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/0001LSN20", "doi": null}}, "content": {"source": {"pdf_hash": "67b59c80f217eb169a473a695dcd91a06e0fd883", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.13077v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0da963dbe017d634fbaab404919225d14fa30842", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/67b59c80f217eb169a473a695dcd91a06e0fd883.txt", "contents": "\nDistribution Matching for Crowd Counting\n\n\nBoyu Wang boywang@cs.stonybrook.edu \nDepartment of Computer Science\nStony Brook University\n11790Stony BrookNY\n\nHuidong Liu \nDepartment of Computer Science\nStony Brook University\n11790Stony BrookNY\n\nDimitris Samaras samaras@cs.stonybrook.edu \nDepartment of Computer Science\nStony Brook University\n11790Stony BrookNY\n\nMinh Hoai minhhoai@cs.stonybrook.edu \nDepartment of Computer Science\nStony Brook University\n11790Stony BrookNY\n\nDistribution Matching for Crowd Counting\n* indicates equal contribution\nIn crowd counting, each training image contains multiple people, where each person is annotated by a dot. Existing crowd counting methods need to use a Gaussian to smooth each annotated dot or to estimate the likelihood of every pixel given the annotated point. In this paper, we show that imposing Gaussians to annotations hurts generalization performance. Instead, we propose to use Distribution Matching for crowd COUNTing (DM-Count). In DM-Count, we use Optimal Transport (OT) to measure the similarity between the normalized predicted density map and the normalized ground truth density map. To stabilize OT computation, we include a Total Variation loss in our model. We show that the generalization error bound of DM-Count is tighter than that of the Gaussian smoothed methods. In terms of Mean Absolute Error, DM-Count outperforms the previous state-of-the-art methods by a large margin on two large-scale counting datasets, UCF-QNRF and NWPU, and achieves the state-of-the-art results on the ShanghaiTech and UCF-CC50 datasets. DM-Count reduced the error of the state-of-the-art published result by approximately 16%. Code is available at https://github.com/cvlab-stonybrook/DM-Count.\n\nIntroduction\n\nImage-based crowd counting is an important research problem with various applications in many domains including journalism and surveillance. Current state-of-the-art methods [54,8,25,55,61,59,17,48,21,23,36] treat crowd counting as a density map estimation problem, where a deep neural network first produces a 2D crowd density map for a given input image and subsequently estimates the total size of the crowd by summing the density values across all spatial locations of the density map. For images of large crowds, this density map estimation approach has been shown to be more robust than the detection-then-counting approach [22,19,62,12] because the former is less sensitive to occlusion and it does not need to commit to binarized decisions at an early stage.\n\nA crucial step in the development of a density map estimation method is the training of a deep neural network that maps from an input image to the corresponding annotated density map. In all existing crowd counting datasets [15,60,14,51], the annotated density map for each training image is a sparse binary mask, where each individual person is marked with a single dot on their head or forehead. The spatial extent of each person is not provided, due to the laborious effort needed for delineating the spatial extent, especially when there is too much occlusion ambiguity. Given training images with dot annotation, training the density map estimation network is equivalent to optimizing the parameters of the network to minimize a differentiable loss function that measures the discrepancy between the predicted density map and the dot-annotation map. Notably, the former is a dense real-value matrix, while the later is a sparse binary matrix. Given the sparsity of the dots, a function that is defined based on the pixel-wise difference between the annotated and predicted density maps is hard to train because the reconstruction loss is heavily unbalanced between the 0s and 1s in the sparse binary matrix. One approach to alleviate this problem is to turn each annotated dot into a Gaussian blob such that the ground truth is more balanced and thus the network is easier to train. Almost all prior crowd density map estimation methods [56,57,60,38,20,33,35,4,28,50,27,40,29,26] have followed 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. this convention. Unfortunately, the performance of the resulting network is highly dependent on the quality of this \"pseudo ground truth\", but it is not trivial to set the right widths for the Gaussian blobs given huge variation in the sizes and shapes of people in a perspective image of a crowded scene.\n\nRecently, Ma et al. [31] proposed a Bayesian loss to measure the discrepancy between the predicted and the annotated density maps. This method transforms a binary ground truth annotation map into N \"smoothed ground truth\" density maps, where N is the count number. Each pixel value of a smoothed ground truth density map is the posterior probability of the corresponding annotation dot given the location of that pixel. Empirically, this method has been shown to outperform other aforementioned approaches [60,38,20,33,35,4]. However, there are two major problems with this loss function. First, it also requires a Gaussian kernel to construct the likelihood function for each annotated dot, which involves setting the kernel width. Second, this loss corresponds to an underdetermined system of equations with infinitely many solutions. The loss can be 0 for many density maps that are not similar to the ground truth density map. As a consequence, using this loss for training can lead to a predicted density map that is very different from the ground truth density map.\n\nIn this paper, we address the shortcomings in existing approaches with the following contributions.\n\n\u2022 We theoretically and empirically show that imposing Gaussians to annotations will hurt the generalization performance of a crowd counting network. \u2022 We propose DM-Count, a method that performs Distribution Matching for crowd COUNTing.\n\nUnlike previous works, DM-Count does not need any Gaussian smoothing ground truth annotations. Instead, we use Optimal Transport (OT) to measure the similarity between the normalized predicted density map and the normalized ground truth density map. To stabilize the OT computation, we further add a Total Variation (TV) loss. \u2022 We present the generalization error bounds for the counting loss, OT loss, TV loss and the overall loss in our method. All the bounds are tighter than those of the Gaussian smoothed methods. \u2022 Empirically, our method improved the state-of-the-art by a large margin on four challenging crowd counting datasets: UCF-QNRF, NWPU, ShanghaiTech, and UCF-CC50. Notably, our method reduced the published state-of-the-art MAE on the NWPU dataset by approximately 16%.\n\n\nPrevious Work\n\n\nCrowd Counting Methods\n\nCrowd counting methods can be divided into three categories: detection-then-count, direct count regression, and density map estimation. Early methods [22,19,62,12] detect people, heads, or upper bodies in the image. However, accurate detection is difficult for dense crowds. Besides, it also requires bounding box annotation, which is a laborious and ambiguous process due to heavy occlusion. Later methods [5,6,49,7] avoid the detection problem and directly learn to regress the count from a feature vector. But their results are less interpretable and the dot annotation maps are underutilized. Most recent works [20,35,15,4,31,50,27,40,29,26,54,8,25,55,61,59,17,48,21,23,30,47,53,43,37,56,18,39,24,44,42] are based on density map estimation, which has been shown to be more robust than detection-then-count and count regression approaches.\n\nDensity map estimation methods usually define the training loss based on the pixel-wise difference between the Gaussian smoothed density map and the predicted density map. Instead of using a single kernel width to smooth the dot annotation, [60,14,47] used adaptive kernel width. The kernel width is selected based on the distance to an annotated dot's nearest neighbors. Specifically, [15] generated multiple smoothed ground truth density maps on different density levels. The final loss combines the reconstruction errors from multiple density levels. However, these methods assume the crowd is evenly distributed; in reality crowd distribution is quite irregular. The Bayesian loss method [31] uses a Gaussian to construct a likelihood function for each annotated dot. However, it may not predict a correct density because the loss is underdetermined. Detailed analysis can be found in Sec 4.2.\n\n\nOptimal Transport\n\nWe propose a novel loss function based on Optimal Transport (OT) [46]. For a better understanding of the proposed method, we briefly review the Monge-Kantarovich OT formulation in this section.\n\nOptimal Transport refers to the optimal cost to transform one probability distribution to another. Let\nX = {x i |x i \u2208 R d } n i=1\nand Y = {y j |y j \u2208 R d } n j=1 be two sets of points on d-dimensional vector space. Let \u00b5 and \u03bd be two probability measures defined on X and Y, respectively; \u00b5, \u03bd \u2208 R n + and 1 T n \u00b5 = 1 T n \u03bd = 1 (1 n is a n-dimensional vector of all ones). Let c : X \u00d7 Y \u2192 R + be the cost function for moving from a point in X to a point in Y, and C be the corresponding n\u00d7n cost matrix for the two sets of points: C ij = c(x i , y j ). Let \u0393 be the set of all possible ways to transport probability mass from X to Y: \u0393 = {\u03b3 \u2208 R n\u00d7n + : \u03b31 = \u00b5, \u03b3 T 1 = \u03bd}. The Monge-Kantorovich's Optimal Transport (OT) cost between \u00b5 and \u03bd is defined as:\nW(\u00b5, \u03bd) = min \u03b3\u2208\u0393 C, \u03b3 .(1)\nIntuitively, if the probability distribution \u00b5 is viewed as a unit amount of \"dirt\" piled on X and \u03bd a unit amount of dirt piled on Y, the OT cost is the minimum \"cost\" of turning one pile into the other. The OT cost is a principal measurement to quantify the dissimilarity between two probability distributions, also taking into account the distance between \"dirt\" locations.\n\nThe OT cost can also be computed via the dual formulation:\nW(\u00b5, \u03bd) = max \u03b1,\u03b2\u2208R n \u03b1, \u00b5 + \u03b2, \u03bd , s.t. \u03b1 i + \u03b2 j \u2264 c(x i , y j ), \u2200i, j.(2)\n\nDM-Count: Distribution Matching for Crowd Counting\n\nWe consider crowd counting as a distribution matching problem. In this section, we propose DM-Count: Distribution matching for crowd counting. A network for crowd counting inputs an image and outputs a map of density values. The final count estimate can be obtained by summing over the predicted density map. DM-Count is agnostic to different network architectures. In our experiments, we use the same network as in the Bayesian loss paper [31]. Unlike all previous density map estimation methods which need to use Gaussians to smooth ground truth annotations, DM-Count does not need any Gaussian to preprocess ground truth annotations.\n\nLet z \u2208 R n + denote the vectorized binary map for dot-annotation and\u1e91 \u2208 R n + the vectorized predicted density map returned by a neural network. By viewing z and\u1e91 as unnormalized density functions, we formulate the loss function in DM-Count using three terms: the counting loss, the OT loss, and the Total Variation (TV) loss. The first term measures the difference between the total masses, while the last two measures the difference between the distributions of the normalized density functions.\n\nThe Counting Loss. Let \u00b7 1 denote the L 1 norm of a vector, and so z 1 , \u1e91 1 are the ground truth and predicted counts respectively. The goal of crowd counting is to make \u1e91 1 as close as possible to z 1 , and the counting loss is defined as the absolute difference between them:\nC (z,\u1e91) = | z 1 \u2212 \u1e91 1 |.(3)\nThe Optimal Transport Loss. Both z and\u1e91 are unnormalized density functions, but we can turn them into probability density functions (pdfs) by dividing them by the their respective total mass. Apart from OT, the Kullback-Leibler divergence and Jensen-Shannon divergence can also measure the similarity between two pdfs. However, these measurements do not provide valid gradients to train a network if the source distribution does not overlap with the target distribution [32]. Therefore, we propose the use of OT in this work. We define the OT loss as follows:\nOT (z,\u1e91) = W z z 1 ,\u1e91 \u1e91 1 = \u03b1 * , z z 1 + \u03b2 * ,\u1e91 \u1e91 1 ,(4)\nwhere \u03b1 * and \u03b2 * are the solutions of Problem (2). We use the quadratic transport cost, i.e., c (z(i),\u1e91(j)) = z(i) \u2212\u1e91(j) 2 2 , where z(i) and\u1e91(j) are 2D coordinates of locations i and j, respectively. To avoid the division-by-zero error, we add a machine precision to the denominator.\n\nSince the entries in\u1e91 are non-negative, the gradient of Eq. (4) with respect to\u1e91 is:\n\u2202 OT (z,\u1e91) \u2202\u1e91 = \u03b2 * \u1e91 1 \u2212 \u03b2 * ,\u1e91 \u1e91 2 1 .(5)\nThis gradient can be back-propagated to learn the parameters of the density estimation network.\n\nTotal Variation Loss. In each training iteration, we use the Sinkhorn algorithm [34] to approximate \u03b1 * and \u03b2 * . The time complexity is O(n 2 log n/ 2 ) [9], where is the desired optimality gap, i.e., the upper bound for the difference between the returned objective and the optimal objective. When optimizing with the Sinkhorn algorithm, the objective decreases dramatically at the beginning but only converges slowly to the optimal objective in later iterations. In practice, we set the maximum number of iterations, and the Sinkhorn algorithm only returns an approximate solution. As a result, when we optimize the OT loss with the Sinkhorn algorithm, the predicted density map ends up close to the ground truth density map, but not exactly the same. The OT loss will approximate well the dense areas of the crowd, but the approximation might be poorer for the low density areas of the crowd. To address this issue, we additionally use the Total Variation (TV) loss, defined as 1 :\nT V (z,\u1e91) = z z 1 \u2212\u1e91 \u1e91 1 T V = 1 2 z z 1 \u2212\u1e91 \u1e91 1 1 .(6)\nThe TV loss will also increase the stability of the training procedure. Optimizing the OT loss with the Sinkhorn algorithm is a min-max saddle point optimization procedure, which is similar to GAN optimization [13]. The stability of GAN training can be increased by adding a reconstruction loss, as shown in the Pix2Pix GAN [16]. To this end, the TV loss is similar to the reconstruction loss, and also increases the stability of the training procedure.\n\nThe gradient of the TV loss with respect to the predicted density map\u1e91 is:\n\u2202 T V (z,\u1e91) \u2202\u1e91 = \u2212 1 2 sign(v) \u1e91 1 \u2212 sign(v),\u1e91 \u1e91 2 1 ,(7)\nwhere v = z/ z 1 \u2212\u1e91/ \u1e91 1 , and sign(\u00b7) is the Sign function on each element of a vector.\n\nThe Overall Objective. The overall loss function is the combination of the counting loss, the OT loss, and the TV loss:\n(z,\u1e91) = C (z,\u1e91) + \u03bb 1 OT (z,\u1e91) + \u03bb 2 z 1 T V (z,\u1e91),(8)\nwhere \u03bb 1 and \u03bb 2 are tunable hyper-parameters for the OT and TV losses. To ensure that the TV loss has the same scale as the counting loss, we multiply this loss term with the total count.\n\nGiven K training images {I k } K k=1 with corresponding dot annotation maps {z k } K k=1 , we will learn a deep neural network f for density map estimation by minimizing: L(f ) = 1 K K k=1 (z k , f (I k )).\n\n\nGeneralization Bounds and Theoretical Analysis\n\nIn this section, we analyze the theoretical properties of the Gaussian smoothed methods, the Bayesian loss, and the proposed DM-Count. The proofs of the theorems in this section can be found in the supplementary material. First, we introduce some notations below.\n\nLet I denote the set of images and Z the set of dot annotation maps. Let D = {(I, z)} be the joint distribution of crowd images and corresponding dot annotation maps. Let H be a hypothesis space. Each h \u2208 H maps from I \u2208 I to each dimension of z \u2208 Z. Let F = H \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 H (n times) be the mapping space. Each f \u2208 F maps I \u2208 I to z \u2208 Z. Let t be the Gaussian smoothed density map of each z \u2208 D, and letD = {(I, t)} be the joint distribution of (I, t). \nLet S = {(I k , z k )} K k=1 , andS = {(I k , t k )} K\n\nGeneralization Error Bounds of Gaussian Smoothed Methods\n\nMany existing methods (e.g., [60,20,35]) use Gaussian-smoothed annotation maps for training. Below we give generalization error bounds when using the 1 loss on the density maps. 1 In the training loss context, Total Variation refers to the total variation distance of two probability measures. A formal definition can be found in [ Theorem 1 Assume that \u2200f \u2208 F and (I, t) \u223cD, we have (t, f (I)) \u2264 B. Then, for any 0 < \u03b4 < 1, with probability of at least 1 \u2212 \u03b4, a) the upper bound of the generalization error is R(D, fS 1 , 1 ) \u2264 R(D, fD 1 , 1 ) + 2nR S (H) + 5B 2 log (8/\u03b4)/K + E (I,z)\u223cD z \u2212 t 1 , b) the lower bound of the generalization error is\nR(D, fS 1 , 1 ) \u2265 E (I,z)\u223cD z \u2212 t 1 \u2212 R(D, fS 1 , 1 ) .\nIn this theorem, as the number of samples K grows to infinity, 2nR S (H) and 5B 2 log (8/\u03b4)/K decrease to 0. Theorem 1.a) shows that the upper bound (worst case) of the expected risk R (D, fS 1 , 1 ), which is evaluated on real ground truth data using an empirical minimizer trained on the Gaussian smoothed ground truth, does not exceed R(D, fD 1 , 1 ) + E (I,z)\u223cD z \u2212 t 1 given sufficient training data. Theorem 1.b) shows that the lower bound (best case) of R(D, fS 1 , 1 ) is not smaller than |E (I,z)\u223cD z \u2212 t 1 \u2212 R(D, fS 1 , 1 )|. This means that if R(D, fS 1 , 1 ) \u2264 E (I,z)\u223cD z \u2212 t 1 , then the smaller R(D, fS 1 , 1 ) is, the larger the expected risk R(D, fS 1 , 1 ) will be. In other words, the better a good model fS 1 performs on the Gaussian smoothed ground truthD, the poorer it generalizes on the real ground truth D. Furthermore, as long as R(D, fS 1 , 1 ) = E (I,z)\u223cD z \u2212 t 1 , we have R(D, fS 1 , 1 ) > 0. R(D, fS 1 , 1 ) can be as large as E (I,z)\u223cD z \u2212 t 1 when R(D, fS 1 , 1 ) = 0. This is undesirable because we want the risk R(D, fS 1 , 1 ) evaluated on the real ground truth to be 0 as well.\n\n\nThe Underdetermined Bayesian Loss\n\nThe Bayesian Loss [31] is:\nBayesian (z,\u1e91) = N i=1 |1 \u2212 p i ,\u1e91 |, where p i = N (q i , \u03c3 2 1 2\u00d72 ) N i=1 N (q i , \u03c3 2 1 2\u00d72 ) ,(9)\nand N is number of people of z, and N (q i , \u03c3 2 1 2\u00d72 ) is a Gaussian distribution centered at q i with variance \u03c3 2 1 2\u00d72 . q i is the i th annotated dot in z. The dimension of p i and z is n, the number of pixels of the density map. However, since the number of annotated dots N is less than n, the Bayesian loss is underdetermined. For a ground truth annotation z, there are infinitely many\u1e91 with Bayesian (z,\u1e91) = 0 and\u1e91 = z. Therefore, the predicted density map could be very different from the ground truth density map.\n\n\nThe Generalization Error Bounds of the Losses in DM-Count\n\nWe give the generalization error bounds of the losses in the proposed method in the following theorem.\n\nTheorem 2 Assume that \u2200f \u2208 F and (I, z) \u223c D, we have z 1 \u2265 1, f (I) 1 \u2265 1 (can be satisfied by adding a dummy dimension with value of 1 to both z and f (I)) and C (z, f (I)) \u2264 B. Then, for any 0 < \u03b4 < 1, with probability of at least 1 \u2212 \u03b4 a) the generalization error bound of the counting loss is   Figure 1: Comparison of different methods on toy data. The pixel-wise loss generates a blurry density map with a higher counting error. The Bayesian loss produces dissimilar density maps from the ground truth, with high values in many locations with no annotations. DM-Count is able to produce more accurate crowd count and localization than the other two methods.\nR(D, f S C , C ) \u2264 R(D, f D C , C ) +\nIn the above theorem, as K grows, R S (H) and 2 log (1/\u03b4)K decrease. All the expected risks R(D, f S \u2206 , \u2206 ) using the empirical minimizers f S \u2206 converge to the expected risks R(D, f D \u2206 , \u2206 ), \u2206 \u2208 {C, OT, T V, \u2205} using optimal minimizers f D \u2206 . This means that all the upper bounds are tight. In addition, all upper bounds are tighter than the upper bound of the Gaussian smoothed methods shown in Theorem 1.a). The bound of the OT loss in Theorem 2.b) is related to the maximum transport cost C \u221e . Therefore, we need to use a smaller transport cost in OT for better generalization performance. The coefficient of R S (H) for the counting loss is O(n), and for the OT loss and the TV loss is O(n 2 ). This means that for larger image size, we need more images to train. The number is linear to the size of z using solely the counting loss, and quadratic using solely the OT loss or the TV loss. When using all three losses, we need to set \u03bb 1 and \u03bb 2 to be small in order to balance the three losses.\n\n\nExperiments\n\nIn this section, we describe experiments on toy data and on benchmark crowd counting datasets. More detailed dataset descriptions, implementation details and experimental settings can be found in the supplementary material.\n\n\nResults on Toy Data\n\nTo understand the empirical behavior of different methods, we consider a toy problem where the task is to move a source density map\u1e91 to a target density map z using the Pixel-wise loss, the Bayesian loss and DM-Count. The source density map\u1e91 is initialized from a uniform distribution between 0 and 0.01, and the target density map is shown in the leftmost figure in Fig. 1. All three methods start from the same source density map. Fig. 1 visualizes the final\u1e91 at convergence. The Pixel-wise loss yields a blurry density map with a higher count. The Bayesian loss performs better than the Pixel-wise loss in terms of counting error, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity in Image (SSIM) [52], but the resulting density map is quite different from the target, with high values at many locations where no dots are annotated. This confirms our analysis that the Bayesian loss corresponds to an underdetermined system such that the output density map could be very different from the target density map. In contrast, DM-Count is able to produce a more accurate count and density map. DM-Count outperforms the Bayesian loss by a large margin in both PSNR and SSIM.\n\n\nResults on Benchmark Datasets\n\nWe perform experiments on four challenging crowd counting datasets: UCF-QNRF [15], NWPU [51], ShanghaiTech [60], and UCF-CC-50 [14]. It is worth noting that the NWPU dataset is the largestscale and most challenging crowd counting dataset publicly available today. The ground truth counts for test images are not released, and the results on the test set must be obtained by submitting to the evaluation server at https://www.crowdbenchmark.com/nwpucrowd.html. Following previous work [35,15,4,14,60]  all three metrics, the smaller the better. For a fair comparison, we use the same network as in the Bayesian loss paper [31]. In all experiments, we set \u03bb 1 = 0.1, \u03bb 2 = 0.01, and the Sinkhorn entropic regularization parameter to 10. The number of Sinkhorn iterations is set to 100. On average, the OT computation time is 25ms for each image.\n\nQuantitative Results. Tables 1 and 2  DM-Count outperforms the Pixel-wise loss and the Bayesian loss, when used in the same network architecture and training procedure as DM-Count, in all the experiments. This demonstrates the effectiveness of the proposed loss. The pixel-wise loss is much worse than DM-Count in Table 1. Additionally, even without using a multi-scale architecture as in [4,47], or a deeper network as in [2,50], DM-Count still achieves state-of-the-art performance on all four datasets. This indicates the importance of having a good loss function in crowd counting.\n\nOn the large-scale and challenging datasets UCF-QNRF and NWPU, DM-Count significantly outperforms the state-of-the-art methods.  Qualitative Results. Fig. 2 shows the predicted density maps of the Pixel-wise loss, the Bayesian loss and DM-Count. This figure demonstrates that: 1) DM-Count produces count numbers that are closer to the ground truth numbers, 2) DM-Count produces much sharper density maps than the   Pixel-wise and Bayesian losses. In Fig. 2, DM-Count produces much higher PSNRs and SSIMs than the Pixel-wise and Bayesian losses. The average PSNR and SSIM over the whole UCF-QNRF test set for the Pixel-wise loss are 34.79 and 0.43, for the Bayesian loss are 34.55 and 0.42, and for DM-Count are 40.65 and 0.55, respectively. Because the Pixel-wise loss uses the Gaussian smoothed ground truth, it produces blurrier density maps than the real ground truth. This empirically verifies our theoretical analysis of the generalization bound of Gaussian smoothed methods. As shown in the figure, the Pixel-wise and Bayesian losses are unable to localize people in dense regions. In contrast, DM-Count localizes people well in both dense and sparse regions. Fig. 3 shows predicted density maps by DM-Count. The predicted density maps correspond well to crowd densities in both sparse and dense areas, demonstrating the effectiveness of DM-Count in spatial density estimation.\n\n\nAblation Studies\n\nHyper-parameter study. We tune \u03bb 1 and \u03bb 2 in DM-Count on the UCF-QNRF dataset. First, we fix \u03bb 1 to 0.1 and tune \u03bb 2 from 0.01, 0.05 to 0. Effect of the number of Sinkhorn iterations.  Contribution of each component. The loss in DM-Count is composed of three components, the counting loss, the OT loss and the TV loss. We study the contribution of each component on the UCF-QNRF dataset. Results are listed in Table 5. As seen in the Table, all components are essential to the final performance. However, the OT loss is the most important component.\n\nRobustness to noisy annotations. Crowd annotation is performed by placing a single dot on a person. Such process is ambiguous and could lead to inevitable annotation errors. We study how different loss functions perform w.r.t. annotation errors. We add uniform random noise to the original annotation and train different models with the same noisy annotation. The noise is randomly generated between 0 and 5% of the image height, and is about 80 pixels on average. As shown in Table 4, the proposed DM-Count is more robust to annotation errors compared to the pixel-wise Bayesian losses.\n\n\nConclusion\n\nIn this paper, we have shown that using the Gaussian kernel to smooth the ground truth dot annotations can hurt the generalization bound of a model when testing on the real ground truth data. Instead, we consider crowd counting as a distribution matching problem and propose DM-Count, based on Optimal Transport, to address this problem. Unlike prior work, DM-Count does not need a Gaussian kernel to smooth the annotated dots. The generalization error bound of DM-Count is tighter than that of the Gaussian smoothed methods. Extensive experiments on four crowd counting benchmarks demonstrated that DM-Count significantly outperforms previous state-of-the-art methods.\n\n\nBroader Impact\n\nOur work is able to more accurately estimate the crowd size in images or videos, such that it can guide crowd control and improve public safety. The estimated crowd count results are interpretable, with better crowd localization, which will increase transparency of the results for critical applications.\n\nIn an age when the size of the crowd in various political events often becomes a point of heated dispute, having transparent, accurate and objective counting methods could help the historical record, as well a public acceptance of the estimates. Our method could potentially be used to protect public health by monitoring social distancing which is becoming increasingly important during the current epidemic. This method does not leverage biases in the data. The proposed method for counting is general, with possible applications to biomedical cell counting, live stock counting and etc. Our work can be adapted to count moving crowds.\n\n\nk=1 be the finite sets of K samples i.i.d. sampled from D andD, respectively. Let R S (H) denote the empirical Rademacher complexity [3] for H w.r.t S. Given a data set D \u2208 {D, S,D,S}, a mapping f \u2208 F and a loss function , let R(D, f, ) = E (I,s)\u223cD [ (s, f (I))] denote the expected risk. Let 1 (z,\u1e91) = z \u2212\u1e91 1 . Let f D \u2206 = argmin f \u2208F R(D, f, \u2206 ) be the minimizer of R(D, f, \u2206 ) over a data set D using the loss \u2206 , where D \u2208 {D, S,D,S}, and \u2206 \u2208 {1, C, OT, T V, \u2205}.\n\n\n2nR S (H) + 5B 2 log (8/\u03b4)/K, b) the generalization error bound of the OT loss isR(D, f S OT , OT ) \u2264 R(D, f D OT , OT ) + 4C \u221e n 2 R S (H) + 5C \u221e 2 log (8/\u03b4)/K, c) the generalization error bound of the TV loss is R(D, f S T V , T V ) \u2264 R(D, f D T V , T V ) + n 2 R S (H) + 5 2 log (8/\u03b4)K, d) the generalization error bound of the overall loss isR(D, f S , ) \u2264 R(D, f D , ) + (2n + 4\u03bb 1 C \u221e n 2 + \u03bb 2 N n 2 )R S (H) +5(B + \u03bb 1 C \u221e + \u03bb 2 N ) 2 log (8/\u03b4)K, where C \u221e is the maximum cost in the cost matrix in OT, and N = sup{ z 1 | \u2200(I, z) \u223c D} is the maximum count number over a dataset.\n\nFigure 2 :Figure 3 :\n23Density map visualization. Comparison between Pixel-wise loss, Bayesian loss and DM-Count. The pixel-wise and Bayesian losses fail to localize people well in dense regions. DM-Count is able to localize people both in dense and sparse regions. The Count number, PSNR and SSIM metrics suggest that DM-Count produces more accurate count numbers and better density maps. Density map visualization on the NWPU validation set.\n\n\n45, Definition 2.4, page 83]. Eq. (6) is [45, Lemma 2.1, page 84].\n\n\n, we use the following metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Normalized Absolute Error (NAE) as evaluation metrics. ForUCF-QNRF \n\nShanghaiTech A ShanghaiTech B \nUCF-CC-50 \nMAE RMSE MAE RMSE MAE RMSE MAE RMSE \n\nCrowd CNN [58] \n-\n-\n181.8 \n277.7 \n32.0 \n49.8 \n467.0 498.5 \nMCNN [60] \n277 \n426 \n110.2 \n173.2 \n26.4 \n41.3 \n377.6 509.1 \nCMTL [41] \n252 \n514 \n101.3 \n152.4 \n20.0 \n31.1 \n322.8 341.4 \nSwitch CNN [2] \n228 \n445 \n90.4 \n135.0 \n21.6 \n33.4 \n318.1 439.2 \nIG-CNN [1] \n-\n-\n72.5 \n118.2 \n13.6 \n21.1 \n291.4 349.4 \nic-CNN [35] \n-\n-\n68.5 \n116.2 \n10.7 \n16.0 \n260.9 365.5 \nCSR Net [20] \n-\n-\n68.2 \n115.0 \n10.6 \n16.0 \n266.1 397.5 \nSANet [4] \n-\n-\n67.0 \n104.5 \n8.4 \n13.6 \n258.4 334.9 \nCL-CNN [15] \n132 \n191 \n-\n-\n-\n-\n-\n-\nPACNN [40] \n-\n-\n62.4 \n102.0 \n7.6 \n11.8 \n241.7 320.7 \nCAN [27] \n107 \n183 \n62.3 \n100.0 \n7.8 \n12.2 \n212.2 243.7 \nSFCN [50] \n102 \n171 \n64.8 \n107.5 \n7.6 \n13.0 \n214.2 318.2 \nANF [57] \n110 \n174 \n63.9 \n99.4 \n8.3 \n13.2 \n250.2 340.0 \nWan et al. [47] \n101 \n176 \n64.7 \n97.1 \n8.1 \n13.6 \n-\n-\n\nPixel-wise Loss [31] \n106.8 183.7 \n68.6 \n110.1 \n8.5 \n13.9 \n251.6 331.3 \nBayesian Loss [31] \n88.7 \n154.8 \n62.8 \n101.8 \n7.7 \n12.7 \n229.3 308.2 \nDM-Count (proposed) 85.6 \n148.3 \n59.7 \n95.7 \n7.4 \n11.8 \n211.0 291.5 \n\nTable 1: Results on the UCF-QNRF, Shanghai Tech, and UCF-CC-50 datasets. \n\nBackbone \nValidation set \nTest set \n\nMAE RMSE MAE RMSE NAE \n\nMCNN [60] \nFS \n218.5 700.6 232.5 714.6 1.063 \nCSR net [20] \nVGG-16 \n104.8 433.4 121.3 387.8 0.604 \nPCC-Net-VGG [10] \nVGG-16 \n100.7 573.1 112.3 457.0 0.251 \nCAN [27] \nVGG-16 \n93.5 \n489.9 106.3 386.5 0.295 \nSCAR [11] \nVGG-16 \n81.5 \n397.9 110.0 495.3 0.288 \nBayesian Loss [31] \nVGG-19 \n93.6 \n470.3 105.4 454.2 0.203 \nSFCN [50] \nResNet-101 95.4 \n608.3 105.7 424.1 0.254 \n\nDM-Count (proposed) \nVGG-19 \n70.5 \n357.6 \n88.4 \n388.6 0.169 \n\n\n\nTable 2 :\n2Results of various methods on the NWPU validation and test sets.\n\n\n# Sinkhorn Iters MAE RMSE50 \n90.8 \n162.1 \n100 \n85.6 \n148.3 \n120 \n85.5 \n151.5 \n\n\n\nTable 3 :\n3Effect of # of Sinkhorn iterations.Method \nMAE RMSE \n\nPixel-wise loss 144.1 232.5 \nBayesian loss 108.4 187.2 \nDM-Count \n105.6 181.6 \n\n\n\nTable 4 :\n4Robustness to noisy annotations.\n\n\n1. The MAE varies from 85.6, 87.8 to 88.5. As \u03bb 2 = 0.01 achieves the best result, we fix \u03bb 2 to 0.01 and tune \u03bb 1 from 0.01, 0.05 to 0.1. The MAE varies from 87.2, 86.2 to 85.6. Thus, we set \u03bb 1 = 0.1, \u03bb 2 = 0.01 and use them on all the datasets.\n\nTable 3\n3lists the results of DM-Count on the UCF-QNRF dataset using different numbers of Sinkhorn iterations. As shown in this table, using a small number of iterations lowers the performance of DM-Count, which indicates that we obtain inaccurate OT solutions. When the number of iterations increases to 100, DM-Count outperforms the previous state-of-the-art. The performance plateaued after the number of iterations crossed 100. Therefore, in all of our experiments, we use 100 Sinkhorn iterations for DM-Count.Component \nCombinations \n\nCounting loss \nOT loss \nTV loss \nMAE \n103.1 94.9 89.3 85.6 \nRMSE \n175.9 167.4 161.3 148.3 \n\n\n\nTable 5 :\n5Component analysis\nAcknowledgementsThis research was partially supported by US National Science Foundation Award IIS-1763981, the SUNY2020 Infrastructure Transportation Security Center, and Air Force Research Laboratory (AFRL) DARPA FA8750-19-2-1003, the Partner University Fund, and a gift from Adobe.\nDivide and grow: capturing huge diversity in crowd images with incrementally growing cnn. Deepak Babu, Neeraj Sajjan, Mukundhan Vr Babu, Srinivasan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDeepak Babu, Neeraj Sajjan, VR Babu, and Mukundhan Srinivasan. Divide and grow: capturing huge diversity in crowd images with incrementally growing cnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nSwitching convolutional neural network for crowd counting. Shiv Deepak Babu Sam, R Surya, Venkatesh, Babu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDeepak Babu Sam, Shiv Surya, and R. Venkatesh Babu. Switching convolutional neural network for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\nRademacher and gaussian complexities: Risk bounds and structural results. L Peter, Shahar Bartlett, Mendelson, Journal of Machine Learning Research. 3Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.\n\nScale aggregation network for accurate and efficient crowd counting. Xinkun Cao, Zhipeng Wang, Yanyun Zhao, Fei Su, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionXinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and efficient crowd counting. In Proceedings of the European Conference on Computer Vision, 2018.\n\nBayesian poisson regression for crowd counting. B Antoni, Nuno Chan, Vasconcelos, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionAntoni B Chan and Nuno Vasconcelos. Bayesian poisson regression for crowd counting. In Proceedings of the International Conference on Computer Vision, 2009.\n\nFeature mining for localised crowd counting. Ke Chen, Chen Change Loy, Shaogang Gong, Tony Xiang, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceKe Chen, Chen Change Loy, Shaogang Gong, and Tony Xiang. Feature mining for localised crowd counting. In Proceedings of the British Machine Vision Conference, 2012.\n\nCumulative attribute space for age and crowd density estimation. Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKe Chen, Shaogang Gong, Tao Xiang, and Chen Change Loy. Cumulative attribute space for age and crowd density estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013.\n\nLearning spatial awareness to improve crowd counting. Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Alexander G Hauptmann, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionZhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, and Alexander G. Hauptmann. Learning spatial awareness to improve crowd counting. In Proceedings of the International Conference on Computer Vision, 2019.\n\nComputational optimal transport: Complexity by accelerated gradient descent is better than by sinkhorn's algorithm. Pavel Dvurechensky, Alexander Gasnikov, Alexey Kroshnin, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPavel Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational optimal transport: Complexity by accelerated gradient descent is better than by sinkhorn's algorithm. In Proceedings of the International Conference on Machine Learning, 2018.\n\nPcc net: Perspective crowd counting via spatial convolutional network. Junyu Gao, Qi Wang, Xuelong Li, IEEE Transactions on Circuits and Systems for Video Technology. Junyu Gao, Qi Wang, and Xuelong Li. Pcc net: Perspective crowd counting via spatial convolutional network. IEEE Transactions on Circuits and Systems for Video Technology, 2019.\n\nScar: Spatial-/channel-wise attention regression networks for crowd counting. Junyu Gao, Qi Wang, Yuan Yuan, Neurocomputing. 363Junyu Gao, Qi Wang, and Yuan Yuan. Scar: Spatial-/channel-wise attention regression networks for crowd counting. Neurocomputing, 363:1-8, 2019.\n\nMarked point processes for crowd counting. Weina Ge, T Robert, Collins, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWeina Ge and Robert T Collins. Marked point processes for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014.\n\nMulti-source multi-scale counting in extremely dense crowd images. Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHaroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source multi-scale counting in extremely dense crowd images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013.\n\nComposition loss for counting, density map estimation and localization in dense crowds. Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, Mubarak Shah, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionHaroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation and localization in dense crowds. In Proceedings of the European Conference on Computer Vision, 2018.\n\nImage-to-image translation with conditional adversarial networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\nCrowd counting and density estimation by trellis encoder-decoder networks. Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, Ling Shao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, and Ling Shao. Crowd counting and density estimation by trellis encoder-decoder networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nWhere are the blobs: Counting by localization with point supervision. H Issam, Negar Laradji, Pedro O Rostamzadeh, David Pinheiro, Mark Vazquez, Schmidt, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionIssam H Laradji, Negar Rostamzadeh, Pedro O Pinheiro, David Vazquez, and Mark Schmidt. Where are the blobs: Counting by localization with point supervision. In Proceedings of the European Conference on Computer Vision, 2018.\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. Min Li, Zhaoxiang Zhang, Kaiqi Huang, Tieniu Tan, Proceedings of the International Conference on Pattern Recognition. the International Conference on Pattern RecognitionMin Li, Zhaoxiang Zhang, Kaiqi Huang, and Tieniu Tan. Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. In Proceedings of the International Conference on Pattern Recognition, 2008.\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Yuhong Li, Xiaofan Zhang, Deming Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nDensity map regression guided detection network for rgb-d crowd counting and localization. Dongze Lian, Jing Li, Jia Zheng, Weixin Luo, Shenghua Gao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDongze Lian, Jing Li, Jia Zheng, Weixin Luo, and Shenghua Gao. Density map regression guided detection network for rgb-d crowd counting and localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nEstimation of number of people in crowded scenes using perspective transformation. Jaw-Yeh Sheng-Fuu Lin, Hung-Xin Chen, Chao, IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans. 31Sheng-Fuu Lin, Jaw-Yeh Chen, and Hung-Xin Chao. Estimation of number of people in crowded scenes using perspective transformation. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 31(6):645-654, 2001.\n\nRecurrent attentive zooming for joint crowd counting and precise localization. Chenchen Liu, Xinyu Weng, Yadong Mu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChenchen Liu, Xinyu Weng, and Yadong Mu. Recurrent attentive zooming for joint crowd counting and precise localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G Hauptmann, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJiang Liu, Chenqiang Gao, Deyu Meng, and Alexander G Hauptmann. Decidenet: Counting varying density crowds through attention guided detection and density estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nCrowd counting with deep structured scale integration network. Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, Liang Lin, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionLingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, and Liang Lin. Crowd counting with deep structured scale integration network. In Proceedings of the International Conference on Computer Vision, 2019.\n\nAdcrowdnet: An attentioninjective deformable convolutional network for crowd understanding. Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, Hefeng Wu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionNing Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu. Adcrowdnet: An attention- injective deformable convolutional network for crowd understanding. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, 2019.\n\nContext-aware crowd counting. Weizhe Liu, Mathieu Salzmann, Pascal Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWeizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nLeveraging unlabeled data for crowd counting by learning to rank. Xialei Liu, Joost Van De, Andrew D Weijer, Bagdanov, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXialei Liu, Joost Van De Weijer, and Andrew D Bagdanov. Leveraging unlabeled data for crowd counting by learning to rank. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7661-7669, 2018.\n\nPoint in, box out: Beyond counting persons in crowds. Yuting Liu, Miaojing Shi, Qijun Zhao, Xiaofang Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYuting Liu, Miaojing Shi, Qijun Zhao, and Xiaofang Wang. Point in, box out: Beyond counting persons in crowds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nClass-agnostic counting. Erika Lu, Weidi Xie, Andrew Zisserman, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer VisionErika Lu, Weidi Xie, and Andrew Zisserman. Class-agnostic counting. In Proceedings of the Asian Conference on Computer Vision, 2018.\n\nBayesian loss for crowd count estimation with point supervision. Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionZhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Proceedings of the International Conference on Computer Vision, 2019.\n\nWasserstein generative adversarial networks. Arjovsky Sc Martin, Leon Bottou, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningSC Martin Arjovsky and Leon Bottou. Wasserstein generative adversarial networks. In Proceedings of the International Conference on Machine Learning, 2017.\n\nTowards perspective-free object counting with deep learning. Daniel Onoro, - Rubio, Roberto J L\u00f3pez-Sastre , Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionDaniel Onoro-Rubio and Roberto J L\u00f3pez-Sastre. Towards perspective-free object counting with deep learning. In Proceedings of the European Conference on Computer Vision, 2016.\n\nComputational optimal transport. Foundations and Trends\u00ae in Machine Learning. Gabriel Peyr\u00e9, Marco Cuturi, 11Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355-607, 2019.\n\nIterative crowd counting. Hieu Viresh Ranjan, Minh Le, Hoai, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionViresh Ranjan, Hieu Le, and Minh Hoai. Iterative crowd counting. In Proceedings of the European Conference on Computer Vision, 2018.\n\nUncertainty estimation and sample selection for crowd counting. Boyu Viresh Ranjan, Mubarak Wang, Minh Shah, Hoai, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer VisionViresh Ranjan, Boyu Wang, Mubarak Shah, and Minh Hoai. Uncertainty estimation and sample selection for crowd counting. In Proceedings of the Asian Conference on Computer Vision, 2020.\n\nTop-down feedback for crowd counting convolutional neural network. Babu Deepak, R Venkatesh Sam, Babu, Proceedings of AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial IntelligenceDeepak Babu Sam and R Venkatesh Babu. Top-down feedback for crowd counting convolutional neural network. Proceedings of AAAI Conference on Artificial Intelligence, 2019.\n\nSwitching convolutional neural network for crowd counting. Shiv Deepak Babu Sam, R Venkatesh Surya, Babu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDeepak Babu Sam, Shiv Surya, and R Venkatesh Babu. Switching convolutional neural network for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\nCrowd counting via adversarial cross-scale consistency pursuit. Zan Shen, Yi Xu, Bingbing Ni, Minsi Wang, Jianguo Hu, Xiaokang Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZan Shen, Yi Xu, Bingbing Ni, Minsi Wang, Jianguo Hu, and Xiaokang Yang. Crowd counting via adversarial cross-scale consistency pursuit. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nRevisiting perspective information for efficient crowd counting. Miaojing Shi, Zhaohui Yang, Chao Xu, Qijun Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMiaojing Shi, Zhaohui Yang, Chao Xu, and Qijun Chen. Revisiting perspective information for efficient crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. A Vishwanath, Sindagi, M Vishal, Patel, IEEE International Conference on Advanced Video and Signal Based Surveillance. Vishwanath A Sindagi and Vishal M Patel. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. In IEEE International Conference on Advanced Video and Signal Based Surveillance, 2017.\n\nHa-ccn: Hierarchical attention-based crowd counting network. A Vishwanath, Sindagi, M Vishal, Patel, IEEE Transactions on Image Processing. 29Vishwanath A Sindagi and Vishal M Patel. Ha-ccn: Hierarchical attention-based crowd counting network. IEEE Transactions on Image Processing, 29:323-335, 2019.\n\nMulti-level bottom-top and top-bottom feature fusion for crowd counting. A Vishwanath, Sindagi, M Vishal, Patel, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionVishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom feature fusion for crowd counting. In Proceedings of the International Conference on Computer Vision, 2019.\n\nPadnet: Pan-density crowd counting. Yukun Tian, Yiming Lei, Junping Zhang, James Z Wang, IEEE Transactions on Image Processing. 29Yukun Tian, Yiming Lei, Junping Zhang, and James Z Wang. Padnet: Pan-density crowd counting. IEEE Transactions on Image Processing, 29:2714-2727, 2019.\n\nIntroduction to nonparametric estimation. B Alexandre, Tsybakov, Springer Science & Business MediaAlexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media, 2008.\n\nOptimal transport: old and new. C\u00e9dric Villani, Springer Science & Business Media338C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.\n\nAdaptive density map generation for crowd counting. Jia Wan, Antoni Chan, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionJia Wan and Antoni Chan. Adaptive density map generation for crowd counting. In Proceedings of the International Conference on Computer Vision, 2019.\n\nResidual regression with semantic prior for crowd counting. Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan, Wei Liu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJia Wan, Wenhan Luo, Baoyuan Wu, Antoni B. Chan, and Wei Liu. Residual regression with semantic prior for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nDeep people counting in extremely dense crowds. Chuan Wang, Hua Zhang, Liang Yang, Si Liu, Xiaochun Cao, Proceedings of the ACM Multimedia Conference. the ACM Multimedia ConferenceChuan Wang, Hua Zhang, Liang Yang, Si Liu, and Xiaochun Cao. Deep people counting in extremely dense crowds. In Proceedings of the ACM Multimedia Conference, 2015.\n\nLearning from synthetic data for crowd counting in the wild. Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nNwpu-crowd: A large-scale benchmark for crowd counting. Qi Wang, Junyu Gao, Wei Lin, Xuelong Li, arXiv:2001.03360arXiv preprintQi Wang, Junyu Gao, Wei Lin, and Xuelong Li. Nwpu-crowd: A large-scale benchmark for crowd counting. arXiv preprint arXiv:2001.03360, 2020.\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004.\n\nFrom open set to closed set: Counting objects by spatial divide-and-conquer. Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, Chunhua Shen, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionHaipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, and Chunhua Shen. From open set to closed set: Counting objects by spatial divide-and-conquer. In Proceedings of the International Conference on Computer Vision, 2019.\n\nLearn to scale: Generating multipolar normalized density maps for crowd counting. Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, Xiang Bai, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionChenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, and Xiang Bai. Learn to scale: Generating multipolar normalized density maps for crowd counting. In Proceedings of the International Conference on Computer Vision, 2019.\n\nPerspective-guided convolution networks for crowd counting. Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, Errui Ding, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionZhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, and Errui Ding. Perspective-guided convolution networks for crowd counting. In Proceedings of the International Confer- ence on Computer Vision, 2019.\n\nRelational attention network for crowd counting. Anran Zhang, Jiayi Shen, Zehao Xiao, Fan Zhu, Xiantong Zhen, Xianbin Cao, Ling Shao, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionAnran Zhang, Jiayi Shen, Zehao Xiao, Fan Zhu, Xiantong Zhen, Xianbin Cao, and Ling Shao. Relational attention network for crowd counting. In Proceedings of the International Conference on Computer Vision, 2019.\n\nAttentional neural fields for crowd counting. Anran Zhang, Lei Yue, Jiayi Shen, Fan Zhu, Xiantong Zhen, Xianbin Cao, Ling Shao, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionAnran Zhang, Lei Yue, Jiayi Shen, Fan Zhu, Xiantong Zhen, Xianbin Cao, and Ling Shao. Attentional neural fields for crowd counting. In Proceedings of the International Conference on Computer Vision, 2019.\n\nCross-scene crowd counting via deep convolutional neural networks. Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang Yang. Cross-scene crowd counting via deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015.\n\nWide-area crowd counting via ground-plane density maps and multi-view fusion cnns. Qi Zhang, Antoni B Chan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQi Zhang and Antoni B. Chan. Wide-area crowd counting via ground-plane density maps and multi-view fusion cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nSingle-image crowd counting via multi-column convolutional neural network. Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, Yi Ma, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\n\nLeveraging heterogeneous auxiliary tasks to assist crowd counting. Muming Zhao, Jian Zhang, Chongyang Zhang, Wenjun Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMuming Zhao, Jian Zhang, Chongyang Zhang, and Wenjun Zhang. Leveraging heterogeneous auxiliary tasks to assist crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nBayesian human segmentation in crowded situations. Tao Zhao, Ramakant Nevatia, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTao Zhao and Ramakant Nevatia. Bayesian human segmentation in crowded situations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2003.\n", "annotations": {"author": "[{\"end\":154,\"start\":44},{\"end\":241,\"start\":155},{\"end\":359,\"start\":242},{\"end\":471,\"start\":360}]", "publisher": null, "author_last_name": "[{\"end\":53,\"start\":49},{\"end\":166,\"start\":163},{\"end\":258,\"start\":251},{\"end\":369,\"start\":365}]", "author_first_name": "[{\"end\":48,\"start\":44},{\"end\":162,\"start\":155},{\"end\":250,\"start\":242},{\"end\":364,\"start\":360}]", "author_affiliation": "[{\"end\":153,\"start\":81},{\"end\":240,\"start\":168},{\"end\":358,\"start\":286},{\"end\":470,\"start\":398}]", "title": "[{\"end\":41,\"start\":1},{\"end\":512,\"start\":472}]", "venue": null, "abstract": "[{\"end\":1737,\"start\":544}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b53\"},\"end\":1931,\"start\":1927},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1933,\"start\":1931},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1936,\"start\":1933},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":1939,\"start\":1936},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":1942,\"start\":1939},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":1945,\"start\":1942},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1948,\"start\":1945},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1951,\"start\":1948},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1954,\"start\":1951},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1957,\"start\":1954},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1960,\"start\":1957},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2387,\"start\":2383},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2390,\"start\":2387},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2393,\"start\":2390},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2396,\"start\":2393},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2749,\"start\":2745},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2752,\"start\":2749},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2755,\"start\":2752},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2758,\"start\":2755},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3967,\"start\":3963},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3970,\"start\":3967},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3973,\"start\":3970},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3976,\"start\":3973},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3979,\"start\":3976},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3982,\"start\":3979},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3985,\"start\":3982},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3987,\"start\":3985},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3990,\"start\":3987},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3993,\"start\":3990},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3996,\"start\":3993},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3999,\"start\":3996},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4002,\"start\":3999},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4005,\"start\":4002},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4443,\"start\":4439},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4929,\"start\":4925},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4932,\"start\":4929},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4935,\"start\":4932},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4938,\"start\":4935},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4941,\"start\":4938},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4943,\"start\":4941},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6815,\"start\":6811},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6818,\"start\":6815},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":6821,\"start\":6818},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6824,\"start\":6821},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7071,\"start\":7068},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7073,\"start\":7071},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7076,\"start\":7073},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7078,\"start\":7076},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7280,\"start\":7276},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7283,\"start\":7280},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7286,\"start\":7283},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7288,\"start\":7286},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7291,\"start\":7288},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7294,\"start\":7291},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7297,\"start\":7294},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7300,\"start\":7297},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7303,\"start\":7300},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7306,\"start\":7303},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7309,\"start\":7306},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7311,\"start\":7309},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7314,\"start\":7311},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7317,\"start\":7314},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7320,\"start\":7317},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7323,\"start\":7320},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7326,\"start\":7323},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7329,\"start\":7326},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7332,\"start\":7329},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7335,\"start\":7332},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7338,\"start\":7335},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7341,\"start\":7338},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7344,\"start\":7341},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7347,\"start\":7344},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7350,\"start\":7347},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7353,\"start\":7350},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7356,\"start\":7353},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7359,\"start\":7356},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7362,\"start\":7359},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7365,\"start\":7362},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7368,\"start\":7365},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7750,\"start\":7746},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7753,\"start\":7750},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7756,\"start\":7753},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7895,\"start\":7891},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8201,\"start\":8197},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8493,\"start\":8489},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10416,\"start\":10412},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11891,\"start\":11887},{\"end\":12160,\"start\":12157},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12632,\"start\":12628},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12705,\"start\":12702},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13803,\"start\":13799},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13917,\"start\":13913},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":15755,\"start\":15751},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15758,\"start\":15755},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15761,\"start\":15758},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15901,\"start\":15900},{\"end\":16053,\"start\":16052},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17600,\"start\":17596},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":21082,\"start\":21078},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21665,\"start\":21661},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21676,\"start\":21672},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21695,\"start\":21691},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21715,\"start\":21711},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22072,\"start\":22068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22075,\"start\":22072},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22077,\"start\":22075},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22080,\"start\":22077},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22083,\"start\":22080},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22209,\"start\":22205},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22821,\"start\":22818},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22824,\"start\":22821},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22855,\"start\":22852},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22858,\"start\":22855}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27674,\"start\":27206},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28263,\"start\":27675},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28708,\"start\":28264},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28777,\"start\":28709},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30581,\"start\":28778},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30658,\"start\":30582},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":30740,\"start\":30659},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":30887,\"start\":30741},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":30932,\"start\":30888},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":31182,\"start\":30933},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":31816,\"start\":31183},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":31847,\"start\":31817}]", "paragraph": "[{\"end\":2519,\"start\":1753},{\"end\":4417,\"start\":2521},{\"end\":5490,\"start\":4419},{\"end\":5591,\"start\":5492},{\"end\":5829,\"start\":5593},{\"end\":6618,\"start\":5831},{\"end\":7503,\"start\":6661},{\"end\":8402,\"start\":7505},{\"end\":8617,\"start\":8424},{\"end\":8721,\"start\":8619},{\"end\":9375,\"start\":8750},{\"end\":9780,\"start\":9404},{\"end\":9840,\"start\":9782},{\"end\":10608,\"start\":9972},{\"end\":11108,\"start\":10610},{\"end\":11388,\"start\":11110},{\"end\":11976,\"start\":11417},{\"end\":12320,\"start\":12035},{\"end\":12406,\"start\":12322},{\"end\":12546,\"start\":12451},{\"end\":13533,\"start\":12548},{\"end\":14042,\"start\":13589},{\"end\":14118,\"start\":14044},{\"end\":14265,\"start\":14177},{\"end\":14386,\"start\":14267},{\"end\":14631,\"start\":14442},{\"end\":14839,\"start\":14633},{\"end\":15153,\"start\":14890},{\"end\":15607,\"start\":15155},{\"end\":16369,\"start\":15722},{\"end\":17540,\"start\":16426},{\"end\":17604,\"start\":17578},{\"end\":18233,\"start\":17708},{\"end\":18397,\"start\":18295},{\"end\":19062,\"start\":18399},{\"end\":20105,\"start\":19101},{\"end\":20344,\"start\":20121},{\"end\":21550,\"start\":20368},{\"end\":22427,\"start\":21584},{\"end\":23014,\"start\":22429},{\"end\":24399,\"start\":23016},{\"end\":24970,\"start\":24420},{\"end\":25559,\"start\":24972},{\"end\":26243,\"start\":25574},{\"end\":26566,\"start\":26262},{\"end\":27205,\"start\":26568}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8749,\"start\":8722},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9403,\"start\":9376},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9918,\"start\":9841},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11416,\"start\":11389},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12034,\"start\":11977},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12450,\"start\":12407},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13588,\"start\":13534},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14176,\"start\":14119},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14441,\"start\":14387},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15662,\"start\":15608},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16425,\"start\":16370},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17707,\"start\":17605},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19100,\"start\":19063}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22465,\"start\":22451},{\"end\":22750,\"start\":22743},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":24838,\"start\":24831},{\"end\":24861,\"start\":24855},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25456,\"start\":25449}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1751,\"start\":1739},{\"attributes\":{\"n\":\"2\"},\"end\":6634,\"start\":6621},{\"attributes\":{\"n\":\"2.1\"},\"end\":6659,\"start\":6637},{\"attributes\":{\"n\":\"2.2\"},\"end\":8422,\"start\":8405},{\"attributes\":{\"n\":\"3\"},\"end\":9970,\"start\":9920},{\"attributes\":{\"n\":\"4\"},\"end\":14888,\"start\":14842},{\"attributes\":{\"n\":\"4.1\"},\"end\":15720,\"start\":15664},{\"attributes\":{\"n\":\"4.2\"},\"end\":17576,\"start\":17543},{\"attributes\":{\"n\":\"4.3\"},\"end\":18293,\"start\":18236},{\"attributes\":{\"n\":\"5\"},\"end\":20119,\"start\":20108},{\"attributes\":{\"n\":\"5.1\"},\"end\":20366,\"start\":20347},{\"attributes\":{\"n\":\"5.2\"},\"end\":21582,\"start\":21553},{\"attributes\":{\"n\":\"5.3\"},\"end\":24418,\"start\":24402},{\"attributes\":{\"n\":\"6\"},\"end\":25572,\"start\":25562},{\"end\":26260,\"start\":26246},{\"end\":28285,\"start\":28265},{\"end\":30592,\"start\":30583},{\"end\":30751,\"start\":30742},{\"end\":30898,\"start\":30889},{\"end\":31191,\"start\":31184},{\"end\":31827,\"start\":31818}]", "table": "[{\"end\":30581,\"start\":28938},{\"end\":30740,\"start\":30686},{\"end\":30887,\"start\":30788},{\"end\":31816,\"start\":31698}]", "figure_caption": "[{\"end\":27674,\"start\":27208},{\"end\":28263,\"start\":27677},{\"end\":28708,\"start\":28288},{\"end\":28777,\"start\":28711},{\"end\":28938,\"start\":28780},{\"end\":30658,\"start\":30594},{\"end\":30686,\"start\":30661},{\"end\":30788,\"start\":30753},{\"end\":30932,\"start\":30900},{\"end\":31182,\"start\":30935},{\"end\":31698,\"start\":31193},{\"end\":31847,\"start\":31829}]", "figure_ref": "[{\"end\":16625,\"start\":16611},{\"end\":18706,\"start\":18698},{\"end\":20741,\"start\":20735},{\"end\":20807,\"start\":20801},{\"end\":23172,\"start\":23166},{\"end\":23472,\"start\":23466},{\"end\":24188,\"start\":24182}]", "bib_author_first_name": "[{\"end\":32228,\"start\":32222},{\"end\":32241,\"start\":32235},{\"end\":32259,\"start\":32250},{\"end\":32727,\"start\":32723},{\"end\":32746,\"start\":32745},{\"end\":33187,\"start\":33186},{\"end\":33201,\"start\":33195},{\"end\":33511,\"start\":33505},{\"end\":33524,\"start\":33517},{\"end\":33537,\"start\":33531},{\"end\":33547,\"start\":33544},{\"end\":33891,\"start\":33890},{\"end\":33904,\"start\":33900},{\"end\":34240,\"start\":34238},{\"end\":34251,\"start\":34247},{\"end\":34258,\"start\":34252},{\"end\":34272,\"start\":34264},{\"end\":34283,\"start\":34279},{\"end\":34615,\"start\":34613},{\"end\":34630,\"start\":34622},{\"end\":34640,\"start\":34637},{\"end\":34659,\"start\":34648},{\"end\":35076,\"start\":35070},{\"end\":35091,\"start\":35084},{\"end\":35098,\"start\":35096},{\"end\":35108,\"start\":35104},{\"end\":35122,\"start\":35113},{\"end\":35124,\"start\":35123},{\"end\":35567,\"start\":35562},{\"end\":35591,\"start\":35582},{\"end\":35608,\"start\":35602},{\"end\":36060,\"start\":36055},{\"end\":36068,\"start\":36066},{\"end\":36082,\"start\":36075},{\"end\":36412,\"start\":36407},{\"end\":36420,\"start\":36418},{\"end\":36431,\"start\":36427},{\"end\":36650,\"start\":36645},{\"end\":36656,\"start\":36655},{\"end\":37010,\"start\":37007},{\"end\":37027,\"start\":37023},{\"end\":37048,\"start\":37043},{\"end\":37060,\"start\":37056},{\"end\":37070,\"start\":37065},{\"end\":37092,\"start\":37085},{\"end\":37105,\"start\":37100},{\"end\":37123,\"start\":37117},{\"end\":37475,\"start\":37469},{\"end\":37489,\"start\":37484},{\"end\":37503,\"start\":37499},{\"end\":37520,\"start\":37513},{\"end\":37980,\"start\":37974},{\"end\":37996,\"start\":37989},{\"end\":38011,\"start\":38005},{\"end\":38024,\"start\":38020},{\"end\":38038,\"start\":38032},{\"end\":38056,\"start\":38051},{\"end\":38073,\"start\":38066},{\"end\":38521,\"start\":38514},{\"end\":38536,\"start\":38529},{\"end\":38549,\"start\":38542},{\"end\":38562,\"start\":38556},{\"end\":38564,\"start\":38563},{\"end\":39013,\"start\":39005},{\"end\":39026,\"start\":39021},{\"end\":39041,\"start\":39033},{\"end\":39057,\"start\":39049},{\"end\":39071,\"start\":39064},{\"end\":39082,\"start\":39077},{\"end\":39097,\"start\":39093},{\"end\":39583,\"start\":39582},{\"end\":39596,\"start\":39591},{\"end\":39611,\"start\":39606},{\"end\":39613,\"start\":39612},{\"end\":39632,\"start\":39627},{\"end\":39647,\"start\":39643},{\"end\":40112,\"start\":40109},{\"end\":40126,\"start\":40117},{\"end\":40139,\"start\":40134},{\"end\":40153,\"start\":40147},{\"end\":40625,\"start\":40619},{\"end\":40637,\"start\":40630},{\"end\":40651,\"start\":40645},{\"end\":41121,\"start\":41115},{\"end\":41132,\"start\":41128},{\"end\":41140,\"start\":41137},{\"end\":41154,\"start\":41148},{\"end\":41168,\"start\":41160},{\"end\":41648,\"start\":41641},{\"end\":41672,\"start\":41664},{\"end\":42085,\"start\":42077},{\"end\":42096,\"start\":42091},{\"end\":42109,\"start\":42103},{\"end\":42571,\"start\":42566},{\"end\":42586,\"start\":42577},{\"end\":42596,\"start\":42592},{\"end\":42612,\"start\":42603},{\"end\":42614,\"start\":42613},{\"end\":43091,\"start\":43085},{\"end\":43103,\"start\":43097},{\"end\":43116,\"start\":43109},{\"end\":43127,\"start\":43121},{\"end\":43138,\"start\":43133},{\"end\":43152,\"start\":43147},{\"end\":43579,\"start\":43575},{\"end\":43593,\"start\":43585},{\"end\":43609,\"start\":43600},{\"end\":43618,\"start\":43615},{\"end\":43626,\"start\":43624},{\"end\":43638,\"start\":43632},{\"end\":44077,\"start\":44071},{\"end\":44090,\"start\":44083},{\"end\":44107,\"start\":44101},{\"end\":44491,\"start\":44485},{\"end\":44502,\"start\":44497},{\"end\":44517,\"start\":44511},{\"end\":44519,\"start\":44518},{\"end\":44967,\"start\":44961},{\"end\":44981,\"start\":44973},{\"end\":44992,\"start\":44987},{\"end\":45007,\"start\":44999},{\"end\":45385,\"start\":45380},{\"end\":45395,\"start\":45390},{\"end\":45407,\"start\":45401},{\"end\":45720,\"start\":45713},{\"end\":45729,\"start\":45725},{\"end\":45743,\"start\":45735},{\"end\":45756,\"start\":45750},{\"end\":46120,\"start\":46112},{\"end\":46136,\"start\":46132},{\"end\":46481,\"start\":46475},{\"end\":46490,\"start\":46489},{\"end\":46520,\"start\":46498},{\"end\":46886,\"start\":46879},{\"end\":46899,\"start\":46894},{\"end\":47078,\"start\":47074},{\"end\":47098,\"start\":47094},{\"end\":47412,\"start\":47408},{\"end\":47435,\"start\":47428},{\"end\":47446,\"start\":47442},{\"end\":47810,\"start\":47806},{\"end\":47830,\"start\":47819},{\"end\":48177,\"start\":48173},{\"end\":48206,\"start\":48195},{\"end\":48627,\"start\":48624},{\"end\":48636,\"start\":48634},{\"end\":48649,\"start\":48641},{\"end\":48659,\"start\":48654},{\"end\":48673,\"start\":48666},{\"end\":48686,\"start\":48678},{\"end\":49133,\"start\":49125},{\"end\":49146,\"start\":49139},{\"end\":49157,\"start\":49153},{\"end\":49167,\"start\":49162},{\"end\":49625,\"start\":49624},{\"end\":49648,\"start\":49647},{\"end\":50037,\"start\":50036},{\"end\":50060,\"start\":50059},{\"end\":50351,\"start\":50350},{\"end\":50374,\"start\":50373},{\"end\":50730,\"start\":50725},{\"end\":50743,\"start\":50737},{\"end\":50756,\"start\":50749},{\"end\":50771,\"start\":50764},{\"end\":51015,\"start\":51014},{\"end\":51214,\"start\":51208},{\"end\":51417,\"start\":51414},{\"end\":51429,\"start\":51423},{\"end\":51761,\"start\":51758},{\"end\":51773,\"start\":51767},{\"end\":51786,\"start\":51779},{\"end\":51797,\"start\":51791},{\"end\":51799,\"start\":51798},{\"end\":51809,\"start\":51806},{\"end\":52220,\"start\":52215},{\"end\":52230,\"start\":52227},{\"end\":52243,\"start\":52238},{\"end\":52252,\"start\":52250},{\"end\":52266,\"start\":52258},{\"end\":52575,\"start\":52573},{\"end\":52587,\"start\":52582},{\"end\":52596,\"start\":52593},{\"end\":52606,\"start\":52602},{\"end\":53006,\"start\":53004},{\"end\":53018,\"start\":53013},{\"end\":53027,\"start\":53024},{\"end\":53040,\"start\":53033},{\"end\":53294,\"start\":53290},{\"end\":53305,\"start\":53301},{\"end\":53307,\"start\":53306},{\"end\":53316,\"start\":53315},{\"end\":53330,\"start\":53324},{\"end\":53676,\"start\":53669},{\"end\":53687,\"start\":53684},{\"end\":53700,\"start\":53692},{\"end\":53711,\"start\":53706},{\"end\":53723,\"start\":53717},{\"end\":53736,\"start\":53729},{\"end\":54173,\"start\":54165},{\"end\":54181,\"start\":54178},{\"end\":54195,\"start\":54187},{\"end\":54204,\"start\":54200},{\"end\":54218,\"start\":54210},{\"end\":54228,\"start\":54223},{\"end\":54640,\"start\":54634},{\"end\":54652,\"start\":54646},{\"end\":54667,\"start\":54659},{\"end\":54677,\"start\":54673},{\"end\":54689,\"start\":54683},{\"end\":54702,\"start\":54696},{\"end\":54713,\"start\":54708},{\"end\":55111,\"start\":55106},{\"end\":55124,\"start\":55119},{\"end\":55136,\"start\":55131},{\"end\":55146,\"start\":55143},{\"end\":55160,\"start\":55152},{\"end\":55174,\"start\":55167},{\"end\":55184,\"start\":55180},{\"end\":55565,\"start\":55560},{\"end\":55576,\"start\":55573},{\"end\":55587,\"start\":55582},{\"end\":55597,\"start\":55594},{\"end\":55611,\"start\":55603},{\"end\":55625,\"start\":55618},{\"end\":55635,\"start\":55631},{\"end\":56030,\"start\":56026},{\"end\":56047,\"start\":56038},{\"end\":56060,\"start\":56052},{\"end\":56075,\"start\":56067},{\"end\":56524,\"start\":56522},{\"end\":56538,\"start\":56532},{\"end\":56540,\"start\":56539},{\"end\":56972,\"start\":56964},{\"end\":56985,\"start\":56980},{\"end\":56997,\"start\":56992},{\"end\":57012,\"start\":57004},{\"end\":57020,\"start\":57018},{\"end\":57468,\"start\":57462},{\"end\":57479,\"start\":57475},{\"end\":57496,\"start\":57487},{\"end\":57510,\"start\":57504},{\"end\":57929,\"start\":57926},{\"end\":57944,\"start\":57936}]", "bib_author_last_name": "[{\"end\":32233,\"start\":32229},{\"end\":32248,\"start\":32242},{\"end\":32267,\"start\":32260},{\"end\":32279,\"start\":32269},{\"end\":32743,\"start\":32728},{\"end\":32752,\"start\":32747},{\"end\":32763,\"start\":32754},{\"end\":32769,\"start\":32765},{\"end\":33193,\"start\":33188},{\"end\":33210,\"start\":33202},{\"end\":33221,\"start\":33212},{\"end\":33515,\"start\":33512},{\"end\":33529,\"start\":33525},{\"end\":33542,\"start\":33538},{\"end\":33550,\"start\":33548},{\"end\":33898,\"start\":33892},{\"end\":33909,\"start\":33905},{\"end\":33922,\"start\":33911},{\"end\":34245,\"start\":34241},{\"end\":34262,\"start\":34259},{\"end\":34277,\"start\":34273},{\"end\":34289,\"start\":34284},{\"end\":34620,\"start\":34616},{\"end\":34635,\"start\":34631},{\"end\":34646,\"start\":34641},{\"end\":34663,\"start\":34660},{\"end\":35082,\"start\":35077},{\"end\":35094,\"start\":35092},{\"end\":35102,\"start\":35099},{\"end\":35111,\"start\":35109},{\"end\":35134,\"start\":35125},{\"end\":35580,\"start\":35568},{\"end\":35600,\"start\":35592},{\"end\":35617,\"start\":35609},{\"end\":36064,\"start\":36061},{\"end\":36073,\"start\":36069},{\"end\":36085,\"start\":36083},{\"end\":36416,\"start\":36413},{\"end\":36425,\"start\":36421},{\"end\":36436,\"start\":36432},{\"end\":36653,\"start\":36651},{\"end\":36663,\"start\":36657},{\"end\":36672,\"start\":36665},{\"end\":37021,\"start\":37011},{\"end\":37041,\"start\":37028},{\"end\":37054,\"start\":37049},{\"end\":37063,\"start\":37061},{\"end\":37083,\"start\":37071},{\"end\":37098,\"start\":37093},{\"end\":37115,\"start\":37106},{\"end\":37130,\"start\":37124},{\"end\":37482,\"start\":37476},{\"end\":37497,\"start\":37490},{\"end\":37511,\"start\":37504},{\"end\":37525,\"start\":37521},{\"end\":37987,\"start\":37981},{\"end\":38003,\"start\":37997},{\"end\":38018,\"start\":38012},{\"end\":38030,\"start\":38025},{\"end\":38049,\"start\":38039},{\"end\":38064,\"start\":38057},{\"end\":38078,\"start\":38074},{\"end\":38527,\"start\":38522},{\"end\":38540,\"start\":38537},{\"end\":38554,\"start\":38550},{\"end\":38570,\"start\":38565},{\"end\":39019,\"start\":39014},{\"end\":39031,\"start\":39027},{\"end\":39047,\"start\":39042},{\"end\":39062,\"start\":39058},{\"end\":39075,\"start\":39072},{\"end\":39091,\"start\":39083},{\"end\":39102,\"start\":39098},{\"end\":39589,\"start\":39584},{\"end\":39604,\"start\":39597},{\"end\":39625,\"start\":39614},{\"end\":39641,\"start\":39633},{\"end\":39655,\"start\":39648},{\"end\":39664,\"start\":39657},{\"end\":40115,\"start\":40113},{\"end\":40132,\"start\":40127},{\"end\":40145,\"start\":40140},{\"end\":40157,\"start\":40154},{\"end\":40628,\"start\":40626},{\"end\":40643,\"start\":40638},{\"end\":40656,\"start\":40652},{\"end\":41126,\"start\":41122},{\"end\":41135,\"start\":41133},{\"end\":41146,\"start\":41141},{\"end\":41158,\"start\":41155},{\"end\":41172,\"start\":41169},{\"end\":41662,\"start\":41649},{\"end\":41677,\"start\":41673},{\"end\":41683,\"start\":41679},{\"end\":42089,\"start\":42086},{\"end\":42101,\"start\":42097},{\"end\":42112,\"start\":42110},{\"end\":42575,\"start\":42572},{\"end\":42590,\"start\":42587},{\"end\":42601,\"start\":42597},{\"end\":42624,\"start\":42615},{\"end\":43095,\"start\":43092},{\"end\":43107,\"start\":43104},{\"end\":43119,\"start\":43117},{\"end\":43131,\"start\":43128},{\"end\":43145,\"start\":43139},{\"end\":43156,\"start\":43153},{\"end\":43583,\"start\":43580},{\"end\":43598,\"start\":43594},{\"end\":43613,\"start\":43610},{\"end\":43622,\"start\":43619},{\"end\":43630,\"start\":43627},{\"end\":43641,\"start\":43639},{\"end\":44081,\"start\":44078},{\"end\":44099,\"start\":44091},{\"end\":44111,\"start\":44108},{\"end\":44495,\"start\":44492},{\"end\":44509,\"start\":44503},{\"end\":44526,\"start\":44520},{\"end\":44536,\"start\":44528},{\"end\":44971,\"start\":44968},{\"end\":44985,\"start\":44982},{\"end\":44997,\"start\":44993},{\"end\":45012,\"start\":45008},{\"end\":45388,\"start\":45386},{\"end\":45399,\"start\":45396},{\"end\":45417,\"start\":45408},{\"end\":45723,\"start\":45721},{\"end\":45733,\"start\":45730},{\"end\":45748,\"start\":45744},{\"end\":45761,\"start\":45757},{\"end\":46130,\"start\":46121},{\"end\":46143,\"start\":46137},{\"end\":46487,\"start\":46482},{\"end\":46496,\"start\":46491},{\"end\":46892,\"start\":46887},{\"end\":46906,\"start\":46900},{\"end\":47092,\"start\":47079},{\"end\":47101,\"start\":47099},{\"end\":47107,\"start\":47103},{\"end\":47426,\"start\":47413},{\"end\":47440,\"start\":47436},{\"end\":47451,\"start\":47447},{\"end\":47457,\"start\":47453},{\"end\":47817,\"start\":47811},{\"end\":47834,\"start\":47831},{\"end\":47840,\"start\":47836},{\"end\":48193,\"start\":48178},{\"end\":48212,\"start\":48207},{\"end\":48218,\"start\":48214},{\"end\":48632,\"start\":48628},{\"end\":48639,\"start\":48637},{\"end\":48652,\"start\":48650},{\"end\":48664,\"start\":48660},{\"end\":48676,\"start\":48674},{\"end\":48691,\"start\":48687},{\"end\":49137,\"start\":49134},{\"end\":49151,\"start\":49147},{\"end\":49160,\"start\":49158},{\"end\":49172,\"start\":49168},{\"end\":49636,\"start\":49626},{\"end\":49645,\"start\":49638},{\"end\":49655,\"start\":49649},{\"end\":49662,\"start\":49657},{\"end\":50048,\"start\":50038},{\"end\":50057,\"start\":50050},{\"end\":50067,\"start\":50061},{\"end\":50074,\"start\":50069},{\"end\":50362,\"start\":50352},{\"end\":50371,\"start\":50364},{\"end\":50381,\"start\":50375},{\"end\":50388,\"start\":50383},{\"end\":50735,\"start\":50731},{\"end\":50747,\"start\":50744},{\"end\":50762,\"start\":50757},{\"end\":50776,\"start\":50772},{\"end\":51025,\"start\":51016},{\"end\":51035,\"start\":51027},{\"end\":51222,\"start\":51215},{\"end\":51421,\"start\":51418},{\"end\":51434,\"start\":51430},{\"end\":51765,\"start\":51762},{\"end\":51777,\"start\":51774},{\"end\":51789,\"start\":51787},{\"end\":51804,\"start\":51800},{\"end\":51813,\"start\":51810},{\"end\":52225,\"start\":52221},{\"end\":52236,\"start\":52231},{\"end\":52248,\"start\":52244},{\"end\":52256,\"start\":52253},{\"end\":52270,\"start\":52267},{\"end\":52580,\"start\":52576},{\"end\":52591,\"start\":52588},{\"end\":52600,\"start\":52597},{\"end\":52611,\"start\":52607},{\"end\":53011,\"start\":53007},{\"end\":53022,\"start\":53019},{\"end\":53031,\"start\":53028},{\"end\":53043,\"start\":53041},{\"end\":53299,\"start\":53295},{\"end\":53313,\"start\":53308},{\"end\":53322,\"start\":53317},{\"end\":53337,\"start\":53331},{\"end\":53349,\"start\":53339},{\"end\":53682,\"start\":53677},{\"end\":53690,\"start\":53688},{\"end\":53704,\"start\":53701},{\"end\":53715,\"start\":53712},{\"end\":53727,\"start\":53724},{\"end\":53741,\"start\":53737},{\"end\":54176,\"start\":54174},{\"end\":54185,\"start\":54182},{\"end\":54198,\"start\":54196},{\"end\":54208,\"start\":54205},{\"end\":54221,\"start\":54219},{\"end\":54232,\"start\":54229},{\"end\":54644,\"start\":54641},{\"end\":54657,\"start\":54653},{\"end\":54671,\"start\":54668},{\"end\":54681,\"start\":54678},{\"end\":54694,\"start\":54690},{\"end\":54706,\"start\":54703},{\"end\":54718,\"start\":54714},{\"end\":55117,\"start\":55112},{\"end\":55129,\"start\":55125},{\"end\":55141,\"start\":55137},{\"end\":55150,\"start\":55147},{\"end\":55165,\"start\":55161},{\"end\":55178,\"start\":55175},{\"end\":55189,\"start\":55185},{\"end\":55571,\"start\":55566},{\"end\":55580,\"start\":55577},{\"end\":55592,\"start\":55588},{\"end\":55601,\"start\":55598},{\"end\":55616,\"start\":55612},{\"end\":55629,\"start\":55626},{\"end\":55640,\"start\":55636},{\"end\":56036,\"start\":56031},{\"end\":56050,\"start\":56048},{\"end\":56065,\"start\":56061},{\"end\":56080,\"start\":56076},{\"end\":56530,\"start\":56525},{\"end\":56545,\"start\":56541},{\"end\":56978,\"start\":56973},{\"end\":56990,\"start\":56986},{\"end\":57002,\"start\":56998},{\"end\":57016,\"start\":57013},{\"end\":57023,\"start\":57021},{\"end\":57473,\"start\":57469},{\"end\":57485,\"start\":57480},{\"end\":57502,\"start\":57497},{\"end\":57516,\"start\":57511},{\"end\":57934,\"start\":57930},{\"end\":57952,\"start\":57945}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":50785961},\"end\":32662,\"start\":32132},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1089358},\"end\":33110,\"start\":32664},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":463216},\"end\":33434,\"start\":33112},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52955811},\"end\":33840,\"start\":33436},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":749620},\"end\":34191,\"start\":33842},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1910869},\"end\":34546,\"start\":34193},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8747356},\"end\":35014,\"start\":34548},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":202577235},\"end\":35444,\"start\":35016},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3298427},\"end\":35982,\"start\":35446},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":165163975},\"end\":36327,\"start\":35984},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":199543884},\"end\":36600,\"start\":36329},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13497941},\"end\":36976,\"start\":36602},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1033682},\"end\":37400,\"start\":36978},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9749221},\"end\":37884,\"start\":37402},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51901514},\"end\":38446,\"start\":37886},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6200260},\"end\":38928,\"start\":38448},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67856061},\"end\":39510,\"start\":38930},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":50787131},\"end\":39991,\"start\":39512},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5157313},\"end\":40524,\"start\":39993},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3645757},\"end\":41022,\"start\":40526},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":196194550},\"end\":41556,\"start\":41024},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":37318664},\"end\":41996,\"start\":41558},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195473446},\"end\":42462,\"start\":41998},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3740753},\"end\":43020,\"start\":42464},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":201645306},\"end\":43481,\"start\":43022},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53957027},\"end\":44039,\"start\":43483},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53783843},\"end\":44417,\"start\":44041},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3787969},\"end\":44905,\"start\":44419},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":91184269},\"end\":45353,\"start\":44907},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53179880},\"end\":45646,\"start\":45355},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":199543622},\"end\":46065,\"start\":45648},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2057420},\"end\":46412,\"start\":46067},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":40499053},\"end\":46799,\"start\":46414},{\"attributes\":{\"id\":\"b33\"},\"end\":47046,\"start\":46801},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":50785934},\"end\":47342,\"start\":47048},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":222066573},\"end\":47737,\"start\":47344},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":19200040},\"end\":48112,\"start\":47739},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1089358},\"end\":48558,\"start\":48114},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52860247},\"end\":49058,\"start\":48560},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":54461738},\"end\":49520,\"start\":49060},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3003101},\"end\":49973,\"start\":49522},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":198133773},\"end\":50275,\"start\":49975},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":201668945},\"end\":50687,\"start\":50277},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":53227020},\"end\":50970,\"start\":50689},{\"attributes\":{\"id\":\"b44\"},\"end\":51174,\"start\":50972},{\"attributes\":{\"id\":\"b45\"},\"end\":51360,\"start\":51176},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":204958589},\"end\":51696,\"start\":51362},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":197639806},\"end\":52165,\"start\":51698},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4509294},\"end\":52510,\"start\":52167},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":72941021},\"end\":52946,\"start\":52512},{\"attributes\":{\"doi\":\"arXiv:2001.03360\",\"id\":\"b50\"},\"end\":53214,\"start\":52948},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":207761262},\"end\":53590,\"start\":53216},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":201070309},\"end\":54081,\"start\":53592},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":198968173},\"end\":54572,\"start\":54083},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":202577232},\"end\":55055,\"start\":54574},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":207901201},\"end\":55512,\"start\":55057},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":207926635},\"end\":55957,\"start\":55514},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":2131202},\"end\":56437,\"start\":55959},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":197464061},\"end\":56887,\"start\":56439},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":4545310},\"end\":57393,\"start\":56889},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":198161572},\"end\":57873,\"start\":57395},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":3489431},\"end\":58264,\"start\":57875}]", "bib_title": "[{\"end\":32220,\"start\":32132},{\"end\":32721,\"start\":32664},{\"end\":33184,\"start\":33112},{\"end\":33503,\"start\":33436},{\"end\":33888,\"start\":33842},{\"end\":34236,\"start\":34193},{\"end\":34611,\"start\":34548},{\"end\":35068,\"start\":35016},{\"end\":35560,\"start\":35446},{\"end\":36053,\"start\":35984},{\"end\":36405,\"start\":36329},{\"end\":36643,\"start\":36602},{\"end\":37005,\"start\":36978},{\"end\":37467,\"start\":37402},{\"end\":37972,\"start\":37886},{\"end\":38512,\"start\":38448},{\"end\":39003,\"start\":38930},{\"end\":39580,\"start\":39512},{\"end\":40107,\"start\":39993},{\"end\":40617,\"start\":40526},{\"end\":41113,\"start\":41024},{\"end\":41639,\"start\":41558},{\"end\":42075,\"start\":41998},{\"end\":42564,\"start\":42464},{\"end\":43083,\"start\":43022},{\"end\":43573,\"start\":43483},{\"end\":44069,\"start\":44041},{\"end\":44483,\"start\":44419},{\"end\":44959,\"start\":44907},{\"end\":45378,\"start\":45355},{\"end\":45711,\"start\":45648},{\"end\":46110,\"start\":46067},{\"end\":46473,\"start\":46414},{\"end\":47072,\"start\":47048},{\"end\":47406,\"start\":47344},{\"end\":47804,\"start\":47739},{\"end\":48171,\"start\":48114},{\"end\":48622,\"start\":48560},{\"end\":49123,\"start\":49060},{\"end\":49622,\"start\":49522},{\"end\":50034,\"start\":49975},{\"end\":50348,\"start\":50277},{\"end\":50723,\"start\":50689},{\"end\":51412,\"start\":51362},{\"end\":51756,\"start\":51698},{\"end\":52213,\"start\":52167},{\"end\":52571,\"start\":52512},{\"end\":53288,\"start\":53216},{\"end\":53667,\"start\":53592},{\"end\":54163,\"start\":54083},{\"end\":54632,\"start\":54574},{\"end\":55104,\"start\":55057},{\"end\":55558,\"start\":55514},{\"end\":56024,\"start\":55959},{\"end\":56520,\"start\":56439},{\"end\":56962,\"start\":56889},{\"end\":57460,\"start\":57395},{\"end\":57924,\"start\":57875}]", "bib_author": "[{\"end\":32235,\"start\":32222},{\"end\":32250,\"start\":32235},{\"end\":32269,\"start\":32250},{\"end\":32281,\"start\":32269},{\"end\":32745,\"start\":32723},{\"end\":32754,\"start\":32745},{\"end\":32765,\"start\":32754},{\"end\":32771,\"start\":32765},{\"end\":33195,\"start\":33186},{\"end\":33212,\"start\":33195},{\"end\":33223,\"start\":33212},{\"end\":33517,\"start\":33505},{\"end\":33531,\"start\":33517},{\"end\":33544,\"start\":33531},{\"end\":33552,\"start\":33544},{\"end\":33900,\"start\":33890},{\"end\":33911,\"start\":33900},{\"end\":33924,\"start\":33911},{\"end\":34247,\"start\":34238},{\"end\":34264,\"start\":34247},{\"end\":34279,\"start\":34264},{\"end\":34291,\"start\":34279},{\"end\":34622,\"start\":34613},{\"end\":34637,\"start\":34622},{\"end\":34648,\"start\":34637},{\"end\":34665,\"start\":34648},{\"end\":35084,\"start\":35070},{\"end\":35096,\"start\":35084},{\"end\":35104,\"start\":35096},{\"end\":35113,\"start\":35104},{\"end\":35136,\"start\":35113},{\"end\":35582,\"start\":35562},{\"end\":35602,\"start\":35582},{\"end\":35619,\"start\":35602},{\"end\":36066,\"start\":36055},{\"end\":36075,\"start\":36066},{\"end\":36087,\"start\":36075},{\"end\":36418,\"start\":36407},{\"end\":36427,\"start\":36418},{\"end\":36438,\"start\":36427},{\"end\":36655,\"start\":36645},{\"end\":36665,\"start\":36655},{\"end\":36674,\"start\":36665},{\"end\":37023,\"start\":37007},{\"end\":37043,\"start\":37023},{\"end\":37056,\"start\":37043},{\"end\":37065,\"start\":37056},{\"end\":37085,\"start\":37065},{\"end\":37100,\"start\":37085},{\"end\":37117,\"start\":37100},{\"end\":37132,\"start\":37117},{\"end\":37484,\"start\":37469},{\"end\":37499,\"start\":37484},{\"end\":37513,\"start\":37499},{\"end\":37527,\"start\":37513},{\"end\":37989,\"start\":37974},{\"end\":38005,\"start\":37989},{\"end\":38020,\"start\":38005},{\"end\":38032,\"start\":38020},{\"end\":38051,\"start\":38032},{\"end\":38066,\"start\":38051},{\"end\":38080,\"start\":38066},{\"end\":38529,\"start\":38514},{\"end\":38542,\"start\":38529},{\"end\":38556,\"start\":38542},{\"end\":38572,\"start\":38556},{\"end\":39021,\"start\":39005},{\"end\":39033,\"start\":39021},{\"end\":39049,\"start\":39033},{\"end\":39064,\"start\":39049},{\"end\":39077,\"start\":39064},{\"end\":39093,\"start\":39077},{\"end\":39104,\"start\":39093},{\"end\":39591,\"start\":39582},{\"end\":39606,\"start\":39591},{\"end\":39627,\"start\":39606},{\"end\":39643,\"start\":39627},{\"end\":39657,\"start\":39643},{\"end\":39666,\"start\":39657},{\"end\":40117,\"start\":40109},{\"end\":40134,\"start\":40117},{\"end\":40147,\"start\":40134},{\"end\":40159,\"start\":40147},{\"end\":40630,\"start\":40619},{\"end\":40645,\"start\":40630},{\"end\":40658,\"start\":40645},{\"end\":41128,\"start\":41115},{\"end\":41137,\"start\":41128},{\"end\":41148,\"start\":41137},{\"end\":41160,\"start\":41148},{\"end\":41174,\"start\":41160},{\"end\":41664,\"start\":41641},{\"end\":41679,\"start\":41664},{\"end\":41685,\"start\":41679},{\"end\":42091,\"start\":42077},{\"end\":42103,\"start\":42091},{\"end\":42114,\"start\":42103},{\"end\":42577,\"start\":42566},{\"end\":42592,\"start\":42577},{\"end\":42603,\"start\":42592},{\"end\":42626,\"start\":42603},{\"end\":43097,\"start\":43085},{\"end\":43109,\"start\":43097},{\"end\":43121,\"start\":43109},{\"end\":43133,\"start\":43121},{\"end\":43147,\"start\":43133},{\"end\":43158,\"start\":43147},{\"end\":43585,\"start\":43575},{\"end\":43600,\"start\":43585},{\"end\":43615,\"start\":43600},{\"end\":43624,\"start\":43615},{\"end\":43632,\"start\":43624},{\"end\":43643,\"start\":43632},{\"end\":44083,\"start\":44071},{\"end\":44101,\"start\":44083},{\"end\":44113,\"start\":44101},{\"end\":44497,\"start\":44485},{\"end\":44511,\"start\":44497},{\"end\":44528,\"start\":44511},{\"end\":44538,\"start\":44528},{\"end\":44973,\"start\":44961},{\"end\":44987,\"start\":44973},{\"end\":44999,\"start\":44987},{\"end\":45014,\"start\":44999},{\"end\":45390,\"start\":45380},{\"end\":45401,\"start\":45390},{\"end\":45419,\"start\":45401},{\"end\":45725,\"start\":45713},{\"end\":45735,\"start\":45725},{\"end\":45750,\"start\":45735},{\"end\":45763,\"start\":45750},{\"end\":46132,\"start\":46112},{\"end\":46145,\"start\":46132},{\"end\":46489,\"start\":46475},{\"end\":46498,\"start\":46489},{\"end\":46523,\"start\":46498},{\"end\":46894,\"start\":46879},{\"end\":46908,\"start\":46894},{\"end\":47094,\"start\":47074},{\"end\":47103,\"start\":47094},{\"end\":47109,\"start\":47103},{\"end\":47428,\"start\":47408},{\"end\":47442,\"start\":47428},{\"end\":47453,\"start\":47442},{\"end\":47459,\"start\":47453},{\"end\":47819,\"start\":47806},{\"end\":47836,\"start\":47819},{\"end\":47842,\"start\":47836},{\"end\":48195,\"start\":48173},{\"end\":48214,\"start\":48195},{\"end\":48220,\"start\":48214},{\"end\":48634,\"start\":48624},{\"end\":48641,\"start\":48634},{\"end\":48654,\"start\":48641},{\"end\":48666,\"start\":48654},{\"end\":48678,\"start\":48666},{\"end\":48693,\"start\":48678},{\"end\":49139,\"start\":49125},{\"end\":49153,\"start\":49139},{\"end\":49162,\"start\":49153},{\"end\":49174,\"start\":49162},{\"end\":49638,\"start\":49624},{\"end\":49647,\"start\":49638},{\"end\":49657,\"start\":49647},{\"end\":49664,\"start\":49657},{\"end\":50050,\"start\":50036},{\"end\":50059,\"start\":50050},{\"end\":50069,\"start\":50059},{\"end\":50076,\"start\":50069},{\"end\":50364,\"start\":50350},{\"end\":50373,\"start\":50364},{\"end\":50383,\"start\":50373},{\"end\":50390,\"start\":50383},{\"end\":50737,\"start\":50725},{\"end\":50749,\"start\":50737},{\"end\":50764,\"start\":50749},{\"end\":50778,\"start\":50764},{\"end\":51027,\"start\":51014},{\"end\":51037,\"start\":51027},{\"end\":51224,\"start\":51208},{\"end\":51423,\"start\":51414},{\"end\":51436,\"start\":51423},{\"end\":51767,\"start\":51758},{\"end\":51779,\"start\":51767},{\"end\":51791,\"start\":51779},{\"end\":51806,\"start\":51791},{\"end\":51815,\"start\":51806},{\"end\":52227,\"start\":52215},{\"end\":52238,\"start\":52227},{\"end\":52250,\"start\":52238},{\"end\":52258,\"start\":52250},{\"end\":52272,\"start\":52258},{\"end\":52582,\"start\":52573},{\"end\":52593,\"start\":52582},{\"end\":52602,\"start\":52593},{\"end\":52613,\"start\":52602},{\"end\":53013,\"start\":53004},{\"end\":53024,\"start\":53013},{\"end\":53033,\"start\":53024},{\"end\":53045,\"start\":53033},{\"end\":53301,\"start\":53290},{\"end\":53315,\"start\":53301},{\"end\":53324,\"start\":53315},{\"end\":53339,\"start\":53324},{\"end\":53351,\"start\":53339},{\"end\":53684,\"start\":53669},{\"end\":53692,\"start\":53684},{\"end\":53706,\"start\":53692},{\"end\":53717,\"start\":53706},{\"end\":53729,\"start\":53717},{\"end\":53743,\"start\":53729},{\"end\":54178,\"start\":54165},{\"end\":54187,\"start\":54178},{\"end\":54200,\"start\":54187},{\"end\":54210,\"start\":54200},{\"end\":54223,\"start\":54210},{\"end\":54234,\"start\":54223},{\"end\":54646,\"start\":54634},{\"end\":54659,\"start\":54646},{\"end\":54673,\"start\":54659},{\"end\":54683,\"start\":54673},{\"end\":54696,\"start\":54683},{\"end\":54708,\"start\":54696},{\"end\":54720,\"start\":54708},{\"end\":55119,\"start\":55106},{\"end\":55131,\"start\":55119},{\"end\":55143,\"start\":55131},{\"end\":55152,\"start\":55143},{\"end\":55167,\"start\":55152},{\"end\":55180,\"start\":55167},{\"end\":55191,\"start\":55180},{\"end\":55573,\"start\":55560},{\"end\":55582,\"start\":55573},{\"end\":55594,\"start\":55582},{\"end\":55603,\"start\":55594},{\"end\":55618,\"start\":55603},{\"end\":55631,\"start\":55618},{\"end\":55642,\"start\":55631},{\"end\":56038,\"start\":56026},{\"end\":56052,\"start\":56038},{\"end\":56067,\"start\":56052},{\"end\":56082,\"start\":56067},{\"end\":56532,\"start\":56522},{\"end\":56547,\"start\":56532},{\"end\":56980,\"start\":56964},{\"end\":56992,\"start\":56980},{\"end\":57004,\"start\":56992},{\"end\":57018,\"start\":57004},{\"end\":57025,\"start\":57018},{\"end\":57475,\"start\":57462},{\"end\":57487,\"start\":57475},{\"end\":57504,\"start\":57487},{\"end\":57518,\"start\":57504},{\"end\":57936,\"start\":57926},{\"end\":57954,\"start\":57936}]", "bib_venue": "[{\"end\":32422,\"start\":32360},{\"end\":32912,\"start\":32850},{\"end\":33653,\"start\":33611},{\"end\":34035,\"start\":33988},{\"end\":34382,\"start\":34345},{\"end\":34806,\"start\":34744},{\"end\":35247,\"start\":35200},{\"end\":35732,\"start\":35684},{\"end\":36815,\"start\":36753},{\"end\":37668,\"start\":37606},{\"end\":38181,\"start\":38139},{\"end\":38713,\"start\":38651},{\"end\":39245,\"start\":39183},{\"end\":39767,\"start\":39725},{\"end\":40278,\"start\":40227},{\"end\":40799,\"start\":40737},{\"end\":41315,\"start\":41253},{\"end\":42255,\"start\":42193},{\"end\":42767,\"start\":42705},{\"end\":43269,\"start\":43222},{\"end\":43784,\"start\":43722},{\"end\":44254,\"start\":44192},{\"end\":44679,\"start\":44617},{\"end\":45155,\"start\":45093},{\"end\":45514,\"start\":45475},{\"end\":45874,\"start\":45827},{\"end\":46258,\"start\":46210},{\"end\":46624,\"start\":46582},{\"end\":47210,\"start\":47168},{\"end\":47554,\"start\":47515},{\"end\":47943,\"start\":47901},{\"end\":48361,\"start\":48299},{\"end\":48834,\"start\":48772},{\"end\":49315,\"start\":49253},{\"end\":50501,\"start\":50454},{\"end\":51547,\"start\":51500},{\"end\":51956,\"start\":51894},{\"end\":52347,\"start\":52318},{\"end\":52754,\"start\":52692},{\"end\":53854,\"start\":53807},{\"end\":54345,\"start\":54298},{\"end\":54831,\"start\":54784},{\"end\":55302,\"start\":55255},{\"end\":55753,\"start\":55706},{\"end\":56223,\"start\":56161},{\"end\":56688,\"start\":56626},{\"end\":57166,\"start\":57104},{\"end\":57659,\"start\":57597},{\"end\":58095,\"start\":58033},{\"end\":32358,\"start\":32281},{\"end\":32848,\"start\":32771},{\"end\":33259,\"start\":33223},{\"end\":33609,\"start\":33552},{\"end\":33986,\"start\":33924},{\"end\":34343,\"start\":34291},{\"end\":34742,\"start\":34665},{\"end\":35198,\"start\":35136},{\"end\":35682,\"start\":35619},{\"end\":36149,\"start\":36087},{\"end\":36452,\"start\":36438},{\"end\":36751,\"start\":36674},{\"end\":37181,\"start\":37132},{\"end\":37604,\"start\":37527},{\"end\":38137,\"start\":38080},{\"end\":38649,\"start\":38572},{\"end\":39181,\"start\":39104},{\"end\":39723,\"start\":39666},{\"end\":40225,\"start\":40159},{\"end\":40735,\"start\":40658},{\"end\":41251,\"start\":41174},{\"end\":41762,\"start\":41685},{\"end\":42191,\"start\":42114},{\"end\":42703,\"start\":42626},{\"end\":43220,\"start\":43158},{\"end\":43720,\"start\":43643},{\"end\":44190,\"start\":44113},{\"end\":44615,\"start\":44538},{\"end\":45091,\"start\":45014},{\"end\":45473,\"start\":45419},{\"end\":45825,\"start\":45763},{\"end\":46208,\"start\":46145},{\"end\":46580,\"start\":46523},{\"end\":46877,\"start\":46801},{\"end\":47166,\"start\":47109},{\"end\":47513,\"start\":47459},{\"end\":47899,\"start\":47842},{\"end\":48297,\"start\":48220},{\"end\":48770,\"start\":48693},{\"end\":49251,\"start\":49174},{\"end\":49741,\"start\":49664},{\"end\":50113,\"start\":50076},{\"end\":50452,\"start\":50390},{\"end\":50815,\"start\":50778},{\"end\":51012,\"start\":50972},{\"end\":51206,\"start\":51176},{\"end\":51498,\"start\":51436},{\"end\":51892,\"start\":51815},{\"end\":52316,\"start\":52272},{\"end\":52690,\"start\":52613},{\"end\":53002,\"start\":52948},{\"end\":53388,\"start\":53351},{\"end\":53805,\"start\":53743},{\"end\":54296,\"start\":54234},{\"end\":54782,\"start\":54720},{\"end\":55253,\"start\":55191},{\"end\":55704,\"start\":55642},{\"end\":56159,\"start\":56082},{\"end\":56624,\"start\":56547},{\"end\":57102,\"start\":57025},{\"end\":57595,\"start\":57518},{\"end\":58031,\"start\":57954}]"}}}, "year": 2023, "month": 12, "day": 17}