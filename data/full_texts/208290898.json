{"id": 208290898, "updated": "2023-09-28 06:27:21.222", "metadata": {"title": "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds", "authors": "[{\"first\":\"Qingyong\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Linhai\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Stefano\",\"last\":\"Rosa\",\"middle\":[]},{\"first\":\"Yulan\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Zhihua\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Markham\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 6, "day": 1}, "abstract": "We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200x faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "1911.11236", "mag": "3012494314", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Hu0XRGWTM20", "doi": "10.1109/cvpr42600.2020.01112"}}, "content": {"source": {"pdf_hash": "65f77c574139ee54bdd2d787dd1957b00798e28c", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"https://arxiv.org/pdf/1911.11236v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1911.11236", "status": "GREEN"}}, "grobid": {"id": "dda92c028a7163f941853f15922307c9b4e2b8ec", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/65f77c574139ee54bdd2d787dd1957b00798e28c.txt", "contents": "\nRandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds\n\n\nQingyong Hu \nUniversity of Oxford\n\n\nBo Yang \nUniversity of Oxford\n\n\nLinhai Xie \nUniversity of Oxford\n\n\nStefano Rosa \nUniversity of Oxford\n\n\nYulan Guo \nSun Yat-sen University\n\n\nNational University of Defense Technology\n\n\nZhihua Wang \nUniversity of Oxford\n\n\nNiki Trigoni \nUniversity of Oxford\n\n\nAndrew Markham \nUniversity of Oxford\n\n\nRandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds\n\nWe study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/postprocessing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200\u00d7 faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and Se-manticKITTI. * Corresponding author PointNet++ (2.4s) SPG (10.8s) Ours (0.04s) Ground Truth\n\nIntroduction\n\nEfficient semantic segmentation of large-scale 3D point clouds is a fundamental and essential capability for realtime intelligent systems, such as autonomous driving and augmented reality. A key challenge is that the raw point clouds acquired by depth sensors are typically irregularly sampled, unstructured and unordered. Although deep convolutional networks show excellent performance in structured 2D computer vision tasks, they cannot be directly applied to this type of unstructured data.\n\nRecently, the pioneering work PointNet [43] has emerged as a promising approach for directly processing 3D point clouds. It learns per-point features using shared Figure 1. Semantic segmentation results of PointNet++ [44], SPG [26] and our approach on SemanticKITTI [3]. Our RandLA-Net takes only 0.04s to directly process a large point cloud with 10 5 points over 150\u00d7130\u00d710 meters in 3D space, which is up to 200\u00d7 faster than SPG. Red circles highlight the superior segmentation accuracy of our approach. multilayer perceptrons (MLPs). This is computationally efficient but fails to capture wider context information for each point. To learn richer local structures, many dedicated neural modules have been subsequently and rapidly introduced. These modules can be generally categorized as: 1) neighbouring feature pooling [44,32,21,70,69], 2) graph message passing [57,48,55,56,5,22,34], 3) kernel-based convolution [49,20,60,29,23,24,54,38], and 4) attentionbased aggregation [61,68,66,42]. Although these approaches achieve impressive results for object recognition and semantic segmentation, almost all of them are limited to extremely small 3D point clouds (e.g., 4k points or 1\u00d71 meter blocks) and cannot be directly extended to larger point clouds (e.g., millions of points and up to 200\u00d7200 meters) without preprocessing steps such as block partition. The reasons for this limitation are three-fold. 1) The commonly used point-sampling methods of these networks are either computationally expensive or memory inefficient. For example, the widely employed farthest-point sampling [44] takes over 200 seconds to sample 10% of 1 million points.\n\n2) Most existing local feature learners usually rely on computationally expensive kernelisation or graph construction, thereby being unable to process massive number of points.\n\n3) For a large-scale point cloud, which usually consists of hundreds of objects, the existing local feature learners are either incapable of capturing complex structures, or do so inefficiently, due to their limited size of receptive fields.\n\nA handful of recent works have started to tackle the task of directly processing large-scale point clouds. SPG [26] preprocesses the large point clouds as super graphs before applying neural networks to learn per super-point semantics. Both FCPN [45] and PCT [7] combine voxelization and point-level networks to process massive point clouds. Although they achieve decent segmentation accuracy, the preprocessing and voxelization steps are too computationally heavy to be deployed in real-time applications.\n\nIn this paper, we aim to design a memory and computationally efficient neural architecture, which is able to directly process large-scale 3D point clouds in a single pass, without requiring any pre/post-processing steps such as voxelization, block partitioning or graph construction. However, this task is extremely challenging as it requires: 1) a memory and computationally efficient sampling approach to progressively downsample large-scale point clouds to fit in the limits of current GPUs, and 2) an effective local feature learner to progressively increase the receptive field size to preserve complex geometric structures. To this end, we first systematically demonstrate that random sampling is a key enabler for deep neural networks to efficiently process large-scale point clouds. However, random sampling can discard key information, especially for objects with sparse points. To counter the potentially detrimental impact of random sampling, we propose a new and efficient local feature aggregation module to capture complex local structures over progressively smaller point-sets.\n\nAmongst existing sampling methods, farthest point sampling and inverse density sampling are the most frequently used for small-scale point clouds [44,60,33,70,15]. As point sampling is such a fundamental step within these networks, we investigate the relative merits of different approaches in Section 3.2, where we see that the commonly used sampling methods limit scaling towards large point clouds, and act as a significant bottleneck to real-time processing. However, we identify random sampling as by far the most suitable component for large-scale point cloud processing as it is fast and scales efficiently. Random sampling is not without cost, because prominent point features may be dropped by chance and it cannot be used directly in existing networks without incurring a performance penalty. To overcome this issue, we design a new local feature aggregation module in Section 3.3, which is capable of effectively learning complex local structures by progressively increasing the receptive field size in each neural layer. In partic-ular, for each 3D point, we firstly introduce a local spatial encoding (LocSE) unit to explicitly preserve local geometric structures. Secondly, we leverage attentive pooling to automatically keep the useful local features. Thirdly, we stack multiple LocSE units and attentive poolings as a dilated residual block, greatly increasing the effective receptive field for each point. Note that all these neural components are implemented as shared MLPs, and are therefore remarkably memory and computational efficient.\n\nOverall, being built on the principles of simple random sampling and an effective local feature aggregator, our efficient neural architecture, named RandLA-Net, not only is up to 200\u00d7 faster than existing approaches on large-scale point clouds, but also surpasses the state-of-the-art semantic segmentation methods on both Semantic3D [17] and Se-manticKITTI [3] benchmarks. Figure 1 shows qualitative results of our approach. Our key contributions are:\n\n\u2022 We analyse and compare existing sampling approaches, identifying random sampling as the most suitable component for efficient learning on large-scale point clouds.\n\n\u2022 We propose an effective local feature aggregation module to preserve complex local structures by progressively increasing the receptive field for each point.\n\n\u2022 We demonstrate significant memory and computational gains over baselines, and surpass the state-of-the-art semantic segmentation methods on multiple large-scale benchmarks.\n\n\nRelated Work\n\nTo extract features from 3D point clouds, traditional approaches usually rely on hand-crafted features [11,47,25,18]. Recent learning based approaches [16,43,37] mainly include projection-based, voxel-based and pointbased schemes which are outlined here.\n\n(1) Projection and Voxel Based Networks. To leverage the success of 2D CNNs, many works [30,8,63,27] project/flatten 3D point clouds onto 2D images to address the task of object detection. However, geometric details may be lost during the projection. Alternatively, point clouds can be voxelized into 3D grids and then powerful 3D CNNs are applied in [14,28,10,39,9]. Although they achieve leading results on semantic segmentation and object detection, their primary limitation is the heavy computation cost, especially when processing large-scale point clouds.\n\n(2) Point Based Networks. Inspired by Point-Net/PointNet++ [43,44], many recent works introduced sophisticated neural modules to learn per-point local features. These modules can be generally classified as 1) neighbouring feature pooling [32,21,70,69], 2) graph message passing [57,48,55,56,5,22,34,31], 3) kernel-based convolution [49,20,60,29,23,24,54,38], and 4) attention-based aggregation [61,68,66,42]. Although these networks have shown promising results on small point clouds, most of them cannot directly scale up to large scenarios due to their high computational and memory costs. Compared with them, our proposed RandLA-Net is distinguished in three ways: 1) it only relies on random sampling within the network, thereby requiring much less memory and computation; 2) the proposed local feature aggregator can obtain successively larger receptive fields by explicitly considering the local spatial relationship and point features, thus being more effective and robust for learning complex local patterns; 3) the entire network only consists of shared MLPs without relying on any expensive operations such as graph construction and kernelisation, therefore being superbly efficient for large-scale point clouds.\n\n(3) Learning for Large-scale Point Clouds. SPG [26] preprocesses the large point clouds as superpoint graphs to learn per super-point semantics. The recent FCPN [45] and PCT [7] apply both voxel-based and point-based networks to process the massive point clouds. However, both the graph partitioning and voxelisation are computationally expensive. In constrast, our RandLA-Net is end-to-end trainable without requiring additional pre/post-processing steps.\n\n\nRandLA-Net\n\n\nOverview\n\nAs illustrated in Figure 2, given a large-scale point cloud with millions of points spanning up to hundreds of meters, to process it with a deep neural network inevitably requires those points to be progressively and efficiently downsampled in each neural layer, without losing the useful point features. In our RandLA-Net, we propose to use the simple and fast approach of random sampling to greatly decrease point density, whilst applying a carefully designed local feature aggregator to retain prominent features. This allows the entire network to achieve an excellent trade-off between efficiency and effectiveness.  \n\n\nThe quest for efficient sampling\n\nExisting point sampling approaches [44,33,15,12,1,60] can be roughly classified into heuristic and learningbased approaches. However, there is still no standard sampling strategy that is suitable for large-scale point clouds. Therefore, we analyse and compare their relative merits and complexity as follows.\n\n(1) Heuristic Sampling\n\n\u2022 Farthest Point Sampling (FPS): In order to sample K points from a large-scale point cloud P with N points, FPS returns a reordering of the metric space {p 1 \u00b7 \u00b7 \u00b7 p k \u00b7 \u00b7 \u00b7 p K }, such that each p k is the farthest point from the first k \u2212 1 points. FPS is widely used in [44,33,60] for semantic segmentation of small point sets. Although it has a good coverage of the entire point set, its computational complexity is O(N 2 ). For a largescale point cloud (N \u223c 10 6 ), FPS takes up to 200 seconds 1 to process on a single GPU. This shows that FPS is not suitable for large-scale point clouds.\n\n\u2022 Inverse Density Importance Sampling (IDIS): To sample K points from N points, IDIS reorders all N points according to the density of each point, after which the top K points are selected [15]. Its computational complexity is approximately O(N ). Empirically, it takes 10 seconds to process 10 6 points. Compared with FPS, IDIS is more efficient, but also more sensitive to outliers. However, it is still too slow for use in a real-time system.\n\n\u2022 Random Sampling (RS): Random sampling uniformly selects K points from the original N points. Its computational complexity is O(1), which is agnostic to the total number of input points, i.e., it is constant-time and hence inherently scalable. Compared with FPS and IDIS, random sampling has the highest computational efficiency, regardless of the scale of input point clouds. It only takes 0.004s to process 10 6 points.\n\n(2) Learning-based Sampling \u2022 Generator-based Sampling (GS): GS [12] learns to generate a small set of points to approximately represent the original large point set. However, FPS is usually used in order to match the generated subset with the original set at inference stage, incurring additional computation. In our experiments, it takes up to 1200 seconds to sample 10% of 10 6 points.\n\n\u2022 Continuous Relaxation based Sampling (CRS): CRS approaches [1,66] use the reparameterization trick to relax the sampling operation to a continuous domain for endto-end training. In particular, each sampled point is learnt based on a weighted sum over the full point clouds. It results in a large weight matrix when sampling all the new points simultaneously with a one-pass matrix multiplication, leading to an unaffordable memory cost. For example, it is estimated to take more than a 300 GB memory footprint to sample 10% of 10 6 points.   Figure 3. The proposed local feature aggregation module. The top panel shows the location spatial encoding block that extracts features, and the attentive pooling mechanism that weights the most important neighbouring features, based on the local context and geometry. The bottom panel shows how two of these components are chained together, to increase the receptive field size, within a residual block.\n\n\nLocal Spatial Encoding (LocSE)\n\uf0c5 S \uf0e5 Attentive Pooling ( , 3 + ) (1, 3 + ) ( , 3) ( , ) ( , ) ( , 2 ) ( , 2 ) ( , 2 ) (1, \u2032 ) ( \u1218 , ) ( , f i ) Input point features ( , 3 + ) K ( , ) ( , /2) ( , )( , ) ( , 2\n\u2022 Policy Gradient based Sampling (PGS): PGS formulates the sampling operation as a Markov decision process [62]. It sequentially learns a probability distribution to sample the points. However, the learnt probability has high variance due to the extremely large exploration space when the point cloud is large. For example, to sample 10% of 10 6 points, the exploration space is C 10 5 10 6 and it is unlikely to learn an effective sampling policy. We empirically find that the network is difficult to converge if PGS is used for large point clouds.\n\nOverall, FPS, IDIS and GS are too computationally expensive to be applied for large-scale point clouds. CRS approaches have an excessive memory footprint and PGS is hard to learn. By contrast, random sampling has the following two advantages: 1) it is remarkably computational efficient as it is agnostic to the total number of input points, 2) it does not require extra memory for computation. Therefore, we safely conclude that random sampling is by far the most suitable approach to process large-scale point clouds compared with all existing alternatives. However, random sampling may result in many useful point features being dropped. To overcome it, we propose a powerful local feature aggregation module as presented in the next section.\n\n\nLocal Feature Aggregation\n\nAs shown in Figure 3, our local feature aggregation module is applied to each 3D point in parallel and it consists of three neural units: 1) local spatial encoding (LocSE), 2) attentive pooling, and 3) dilated residual block.\n\n(1) Local Spatial Encoding Given a point cloud P together with per-point features (e.g., raw RGB, or intermediate learnt features), this local spatial encoding unit explicitly embeds the x-y-z coordinates of all neighbouring points, such that the corresponding point features are always aware of their relative spatial locations. This allows the LocSE unit to explicitly observe the local geometric patterns, thus eventually benefiting the entire network to effectively learn complex local structures. In particular, this unit includes the following steps:\n\nFinding Neighbouring Points. For the i th point, its neighbouring points are firstly gathered by the simple Knearest neighbours (KNN) algorithm for efficiency. The KNN is based on the point-wise Euclidean distances.\n\nRelative Point Position Encoding. For each of the nearest K points {p 1 i \u00b7 \u00b7 \u00b7 p k i \u00b7 \u00b7 \u00b7 p K i } of the center point p i , we explicitly encode the relative point position as follows:\nr k i = M LP p i \u2295 p k i \u2295 (p i \u2212 p k i ) \u2295 ||p i \u2212 p k i ||(1)\nwhere p i and p k i are the x-y-z positions of points, \u2295 is the concatenation operation, and || \u00b7 || calculates the Euclidean distance between the neighbouring and center points. It seems that r k i is encoded from redundant point positions. Interestingly, this tends to aid the network to learn local features and obtains good performance in practice.\n\nPoint Feature Augmentation. For each neighbouring point p k i , the encoded relative point positions r k i are concatenated with its corresponding point features f k i , obtaining an augmented feature vectorf k i . Eventually, the output of the LocSE unit is a new set of neighbouring featuresF i = {f 1 i \u00b7 \u00b7 \u00b7f k i \u00b7 \u00b7 \u00b7f K i }, which explicitly encodes the local geometric structures for the center point p i . We notice that the recent work [36] also uses point positions to improve semantic segmentation. However, the positions are used to learn point scores in [36], while our LocSE explicitly encodes the relative positions to augment the neighbouring point features.\n\n(2) Attentive Pooling This neural unit is used to aggregate the set of neighbouring point featuresF i . Existing works [44,33] typically use max/mean pooling to hard integrate the neighbouring features, resulting in the majority of the information being lost. By contrast, we turn to the powerful attention mechanism to automatically learn important local features. In particular, inspired by [65], our attentive pooling unit consists of the following steps.\n\nComputing Attention Scores. Given the set of local fea-\nturesF i = {f 1 i \u00b7 \u00b7 \u00b7f k i \u00b7 \u00b7 \u00b7f K i },\nwe design a shared function g() to learn a unique attention score for each feature. Basically, the function g() consists of a shared MLP followed by sof tmax. It is formally defined as follows:\ns k i = g(f k i , W )(2)\nwhere W is the learnable weights of a shared MLP. Weighted Summation. The learnt attention scores can be regarded as a soft mask which automatically selects the important features. Formally, these features are weighted summed as follows:f\ni = K k=1 (f k i \u00b7 s k i )(3)\nTo summarize, given the input point cloud P , for the i th point p i , our LocSE and Attentive Pooling units learn to aggregate the geometric patterns and features of its K nearest points, and finally generate an informative feature vectorf i .\n\n\n(3) Dilated Residual Block\n\nSince the large point clouds are going to be substantially downsampled, it is desirable to significantly increase the receptive field for each point, such that the geometric details of input point clouds are more likely to be reserved, even if some points are dropped. As shown in Figure 3, inspired by the successful ResNet [19] and the effective dilated networks [13], we stack multiple LocSE and Attentive Pooling units with a skip connection as a dilated residual block.\n\nTo further illustrate the capability of our dilated residual block, Figure 4 shows that the red 3D point observes K neighbouring points after the first LocSE/Attentive Pooling operation, and then is able to receive information from up to K 2 neighbouring points i.e. its two-hop neighbourhood after the second. This is a cheap way of dilating the receptive field and expanding the effective neighbourhood through feature propagation. Theoretically, the more units we stack, the more powerful this block as its sphere of reach becomes greater and greater. However, more units would inevitably sacrifice the overall computation efficiency. In addition, the entire network is likely to be over-fitted. In our RandLA-Net, we simply stack two sets of LocSE and Attentive Pooling as the standard residual block, achieving a satisfactory balance between efficiency and effectiveness. Overall, our local feature aggregation module is designed to effectively preserve complex local structures via explicitly considering neighbouring geometries and significantly increasing receptive fields. Moreover, this module only consists of feed-forward MLPs, thus being computationally efficient.\n\n\nImplementation\n\nWe implement RandLA-Net by stacking multiple local feature aggregation modules and random sampling layers. The detailed architecture is presented in the Appendix. We use the Adam optimizer with default parameters. The initial learning rate is set as 0.01 and decreases by 5% after each epoch. The number of nearest points K is set as 16. To train our RandLA-Net in parallel, we sample a fixed number of points (\u223c 10 5 ) from each point cloud as the input. During testing, the whole raw point cloud is fed into our network to infer per-point semantics without pre/post-processing such as geometrical or block partition. All experiments are conducted on an NVIDIA RTX2080Ti GPU.\n\n\nExperiments\n\n\nEfficiency of Random Sampling\n\nIn this section, we empirically evaluate the efficiency of existing sampling approaches including FPS, IDIS, RS, GS, CRS, and PGS, which have been discussed in Section 3.2. In particular, we conduct the following 4 groups of experiments.\n\n\u2022 Group 1. Given a small-scale point cloud (\u223c 10 3 points), we use each sampling approach to progressively downsample it. Specifically, the point cloud is downsampled by five steps with only 25% points being retained in each step on a single GPU i.e. a four-fold decimation ratio. This means that there are only \u223c (1/4) 5 \u00d7 10 3 points left in the end. This downsampling strategy emulates the procedure used in PointNet++ [44]. For each sampling approach, we sum up its time and memory consumption for comparison. \u2022 Group 2/3/4. The total number of points are increased towards large-scale, i.e., around 10 4 , 10 5 and 10 6 points respectively. We use the same five sampling steps as in Group 1.\n\nAnalysis. Figure 5 compares the total time and memory consumption of each sampling approach to process different scales of point clouds. It can be seen that: 1) For small-scale point clouds (\u223c 10 3 ), all sampling approaches tend to have similar time and memory consumption, and are unlikely to incur a heavy or limiting computation burden. 2) For largescale point clouds (\u223c 10 6 ), FPS/IDIS/GS/CRS/PGS are either extremely time-consuming or memory-costly. By contrast, random sampling has superior time and memory efficiency overall. This result clearly demonstrates that most existing networks [44,33,60,36,70,66] are only able to be optimized on small blocks of point clouds primarily because they rely on the expensive sampling approaches. Motivated by this, we use the efficient random sampling strategy in our RandLA-Net.\n\n\nEfficiency of RandLA-Net\n\nIn this section, we systematically evaluate the overall efficiency of our RandLA-Net on real-world large-scale point clouds for semantic segmentation. Particularly, we evaluate RandLA-Net on the SemanticKITTI [3] dataset, obtaining the total time consumption of our network on Sequence 08 which has 4071 scans of point clouds in total. We also evaluate the time consumption of recent representative works [43,44,33,26,54] on the same dataset. For a fair comparison, we feed the same number of points (i.e., 81920) from each scan into each neural network.\n\nIn addition, we also evaluate the memory consumption of RandLA-Net and the baselines. In particular, we not only report the total number of parameters of each network, but also measure the maximum number of 3D points each network can take as input in a single pass to infer per-point semantics. Note that, all experiments are conducted on the same machine with an AMD 3700X @3.6GHz CPU and an NVIDIA RTX2080Ti GPU.\n\nAnalysis. Table 1 quantitatively shows the total time and memory consumption of different approaches. It can be seen that, 1) SPG [26] has the lowest number of network parameters, but takes the longest time to process the point clouds due to the expensive geometrical partitioning and super-graph construction steps; 2) both PointNet++ [44] and PointCNN [33] are also computationally expensive mainly because of the FPS sampling operation; 3) PointNet [43] and KPConv [54] are unable to take extremely largescale point clouds (e.g. 10 6 points) in a single pass due to their memory inefficient operations. 4) Thanks to the simple random sampling together with the efficient MLP-based local feature aggregator, our RandLA-Net takes the shortest time (185 seconds averaged by 4071 frames \u2192 roughly 22 FPS) to infer the semantic labels for each large-scale point cloud (up to 10 6 points).\n\n\nSemantic Segmentation on Benchmarks\n\nIn this section, we evaluate the semantic segmentation of our RandLA-Net on three large-scale public datasets: the outdoor Semantic3D [17] and SemanticKITTI [3], and the indoor S3DIS [2].\n\n(1) Evaluation on Semantic3D. The Semantic3D dataset [17] consists of 15 point clouds for training and 15 for online testing. Each point cloud has up to 10 8 points, covering up to 160\u00d7240\u00d730 meters in real-world 3D space. The raw 3D points belong to 8 classes and contain 3D coordinates, RGB information, and intensity. We only use the 3D coordinates and color information to train and test our RandLA-Net. Mean Intersection-over-Union (mIoU) and Overall Accuracy (OA) of all classes are used as the standard metrics. For fair comparison, we only include the results of recently published strong baselines [4,52,53,46,69,56,26] and the current state-of-the-art approach KPConv [54].   (2) Evaluation on SemanticKITTI. SemanticKITTI [3] consists of 43552 densely annotated LIDAR scans belonging to 21 sequences. Each scan is a large-scale point cloud with \u223c 10 5 points and spanning up to 160\u00d7160\u00d720 meters in 3D space. Officially, the sequences 00\u223c07 and 09\u223c10 (19130 scans) are used for training, the sequence 08 (4071 scans) for validation, and the sequences 11\u223c21 (20351 scans) for online testing. The raw 3D points only have 3D coordinates without color information. The mIoU score over 19 categories is used as the standard metric. Table 3 shows a quantitative comparison of our RandLA-Net with two families of recent approaches, i.e. 1) pointbased methods [43,26,49,44,51] and 2) projection based approaches [58,59,3,40], and Figure 6 shows some qualitative results of RandLA-Net on the validation split. It can be seen that our RandLA-Net surpasses all point based approaches [43,26,49,44,51] by a large margin. We also outperform all projection based methods [58,59,3,40], but not significantly, primarily because RangeNet++ [40] achieves much better results on the small object category such as traffic-sign. However, our RandLA-Net has 40\u00d7 fewer net-work parameters than RangeNet++ [40] and is more computationally efficient as it does not require the costly steps of pre/post projection.\n\n(3) Evaluation on S3DIS. The S3DIS dataset [2] consists of 271 rooms belonging to 6 large areas. Each point cloud is a medium-sized single room (\u223c 20\u00d715\u00d75 meters) with dense 3D points. To evaluate the semantic segmentation of our RandLA-Net, we use the standard 6-fold crossvalidation in our experiments. The mean IoU (mIoU), mean class Accuracy (mAcc) and Overall Accuracy (OA) of the total 13 classes are compared.\n\nAs shown in Table 4, our RandLA-Net achieves on-par or better performance than state-of-the-art methods. Note that, most of these baselines [44,33,70,69,57,6] tend to use sophisticated but expensive operations or samplings to optimize the networks on small blocks (e.g., 1\u00d71 meter) of point clouds, and the relatively small rooms act in their favours to be divided into tiny blocks. By contrast, RandLA-Net takes the entire rooms as input and is able to efficiently infer per-point semantics in a single pass.\n\n\nOA(%) mAcc(%) mIoU(%)\n\nPointNet [43] 78. 6 \n\n\nAblation Study\n\nSince the impact of random sampling is fully studied in Section 4.1, we conduct the following ablation studies for our local feature aggregation module. All ablated networks are trained on sequences 00\u223c07 and 09\u223c10, and tested on the sequence 08 of SemanticKITTI dataset [3].\n\n(1) Removing local spatial encoding (LocSE). This unit enables each 3D point to explicitly observe its local geometry. After removing locSE, we directly feed the local point features into the subsequent attentive pooling.\n\n(2\u223c4) Replacing attentive pooling by max/mean/sum pooling. The attentive pooling unit learns to automatically combine all local point features. By comparison, the widely used max/mean/sum poolings tend to hard select or combine features, therefore their performance may be suboptimal.\n\n(5) Simplifying the dilated residual block. The dilated residual block stacks multiple LocSE units and attentive poolings, substantially dilating the receptive field for each 3D point. By simplifying this block, we use only one LocSE unit and attentive pooling per layer, i.e. we do not chain multiple blocks as in our original RandLA-Net. Table 5 compares the mIoU scores of all ablated networks. From this, we can see that: 1) The greatest impact is caused by the removal of the chained spatial embedding and attentive pooling blocks. This is highlighted in Figure  4, which shows how using two chained blocks allows information to be propagated from a wider neighbourhood, i.e. approximately K 2 points as opposed to just K. This is especially critical with random sampling, which is not guaranteed to preserve a particular set of points.\n\n2) The removal of the local spatial encoding unit shows the next greatest impact on performance, demonstrating that this module is necessary to effectively learn local and relative geometry context. 3) Removing the attention module diminishes performance by not being able to effectively retain useful features. From this ablation study, we can see how the proposed neural units complement each other to attain our state-of-the-art performance.  \n\n\nConclusion\n\nIn this paper, we demonstrated that it is possible to efficiently and effectively segment large-scale point clouds by using a lightweight network architecture. In contrast to most current approaches, that rely on expensive sampling strategies, we instead use random sampling in our framework to significantly reduce the memory footprint and computational cost. A local feature aggregation module is also introduced to effectively preserve useful features from a wide neighbourhood. Extensive experiments on multiple benchmarks demonstrate the high efficiency and the stateof-the-art performance of our approach. It would be interesting to extend our framework for the end-to-end 3D instance segmentation on large-scale point clouds by drawing on the recent work [64] and also for the real-time dynamic point cloud processing [35].\n\nEach sampled point feature vector y \u2208 R d+3 is calculated as follows:\ny = N i=1 exp ((log(s (i) ) + g (i) )/\u03c4 )P (i) N j=1 exp ((log(s (j) ) + g (j) )/\u03c4 ) ,(5)\nwhere s (i) and g (i) indicate the i th element in the vector s and g respectively, P (i) represents the i th row vector in the input matrix P . \u03c4 > 0 is the annealing temperature. When \u03c4 \u2192 0, Equation 5 approaches the discrete distribution and samples each row vector in P with the probability p(y = P (i) ) = s (i) .\n\n6. Policy Gradients based Sampling (PGS): Given a point feature set P \u2208 R N \u00d7(d+3) with 3D coordinates and per point features, we first predict a score s for each point, which is learnt by an MLP function, i.e., s = sof tmax(M LP (P )) + , where \u2208 R N is a zeromean Gaussian noise with the variance \u03a3 for random exploration. After that, we sample K vectors in P with the top K scores. Sampling each point/vector can be regarded as an independent action and a sequence of them form a sequential Markov Decision Process (MDP) with the following policy function \u03c0:\na i \u223c \u03c0(a|P (i) ; \u03b8, s)(6)\nwhere a i is the binary decision of whether to sample the i th vector in P and \u03b8 is the network parameter of the MLP. Hence to properly improve the sampling policy with an underivable sampling process, we apply REINFORCE algorithm [50] as the gradient estimator. The segmentation accuracy R is applied as the reward value for the entire sampling process as J = R. It is optimized with the following estimated gradients:\n\u2202J \u2202\u03b8 \u2248 1 M M m=1 N i=1 \u2202 \u2202\u03b8 log \u03c0(a i |P (i) ; \u03b8, \u03a3)\u00d7 (R \u2212 b c \u2212 b(P (i) )),(7)\nwhere M is the batch size, b c and b(P (i) ) are two control variates [41] for alleviating the high variance problem of policy gradients.   Network Input: The input is a large-scale point cloud with a size of N \u00d7 d in (the batch dimension is dropped for simplicity), where N is the number of points, d in is the feature dimension of each input point. For both S3DIS [2] and Semantic3D [17] datasets, each point is represented by its 3D coordinates and color information (i.e., x-y-z-R-G-B), while each point of the SemanticKITTI [3] dataset is only represented by 3D coordinates.\n\n\nRandLA-Net\n\nEncoding Layers: Four encoding layers are used in our network to progressively reduce the size of the point clouds and increase the per-point feature dimensions. Each encoding layer consists of a local feature aggregation module (Section 3.3) and a random sampling operation (Section 3.2). The point cloud is downsampled with a four-fold decimation ratio. In particular, only 25% of the point features are retained after each layer, i.e.,\n(N \u2192 N 4 \u2192 N 16 \u2192 N 64 \u2192 N 256 )\n. Meanwhile, the per-point feature dimension is gradually increased each layer to preserve more information, i.e., (8 \u2192 32 \u2192 128 \u2192 256 \u2192 512).\n\nDecoding Layers: Four decoding layers are used after the above encoding layers. For each layer in the decoder, we first use the KNN algorithm to find one nearest neighboring point for each query point, the point feature set is then upsampled through a nearest-neighbor interpolation. Next, the upsampled feature maps are concatenated with the intermediate feature maps produced by encoding layers through skip connections, after which a shared MLP is applied to the concatenated feature maps. \n\n\nC. Additional Ablation Studies on LocSE\n\nIn Section 3.3, we encode the relative point position based on the following equation:\nr k i = M LP p i \u2295 p k i \u2295 (p i \u2212 p k i ) \u2295 ||p i \u2212 p k i ||(8)\nWe further investigate the effects of different spatial information in our framework. Particularly, we conduct the following more ablative experiments for LocSE:\n\n\u2022 1) Encoding the coordinates of the point p i only.\n\n\u2022 2) Encoding the coordinates of neighboring points p k i only.\n\n\u2022 3) Encoding the coordinates of the point p i and its neighboring points p k i .\n\n\u2022 4) Encoding the coordinates of the point p i , the neighboring points p k i , and Euclidean distance ||p i \u2212 p k i ||.\n\n\u2022 5) Encoding the coordinates of the point p i , the neighboring points p k i , and the relative position p i \u2212 p k i . Table 6 compares the mIoU scores of all ablated networks on the SemanticKITTI [3] dataset. We can see that: 1) Explicitly encoding all spatial information leads to the best mIoU performance. 2) The relative position p i \u2212 p k i plays an important role in this component, primarily because the relative point position enables the network to be aware of the local geometric patterns. 3) Only encoding LocSE mIoU(%) the point position p i or p k i is unlikely to improve the performance, because the relative local geometric patterns are not explicitly encoded.\n(1) (p i ) 45.5 (2) (p k i ) 47.7 (3) (p i , p k i ) 49.1 (4) (p i , p k i , ||p i \u2212 p k i ||) 50.5 (5) (p i , p k i , p i \u2212 p k i ) 53.6 (6) (p i , p k i , p i \u2212 p k i , ||p i \u2212 p k i ||) (The Full Unit) 54.3\n\nD. Additional Ablation Studies on Dilated Residual Block\n\nIn our RandLA-Net, we stack two LocSE and Attentive Pooling units as the standard dilated residual block to gradually increase the receptive field. To further evaluate how the number of aggregation units in the dilated residual block impact the entire network, we conduct the following two more groups of experiments.\n\n\u2022 1) We simplify the dilated residual block by using only one LocSE unit and attentive pooling.\n\n\u2022 2) We add one more LocSE unit and attentive pooling, i.e., there are three aggregation units chained together.   Table 7 shows the mIoU scores of different ablated networks on the validation split of the SemanticKITTI [3] dataset. It can be seen that: 1) Only one aggregation unit in the dilated residual block leads to a significant drop in segmentation performance, due to the limited receptive field. 2) Three aggregation units in each block do not improve the accuracy as expected. This is because the significantly increased receptive fields and the large number of trainable parameters tend to be overfitted.\n\n\nE. Visualization of Attention Scores\n\nTo better understand the attentive pooling, it is desirable to visualize the learned attention scores. However, since the attentive pooling operates on a relatively small local point set (i.e., K=16), it is hardly able to recognize meaningful shapes from such small local regions. Alternatively, we visualize the learned attention weight matrix W defined in Equation 2 in each layer. As shown in Figure 8, the attention weights have large values in the first encoding layers, then gradually become smooth and stable in subsequent layers. This shows that the attentive pooling tends to choose prominent or key point features at the beginning. After the point cloud being significantly downsampled, the attentive pooling layer tends to retain the majority of those point features. \n\n\nF. Additional Results on Semantic3D\n\nMore qualitative results of RandLA-Net on the Seman-tic3D [17] dataset (reduced-8) are shown in Figure 9. Figure 10 shows more qualitative results of our RandLA-Net on the validation set of SemanticKITTI [3]. The red boxes showcase the failure cases. It can be seen that, the points belonging to other-vehicle are likely to be misclassified as car, mainly because the partial point clouds without colors are extremely difficult to be distinguished between the two similar classes. In addition, our approach tends to fail in several minority classes such as bicycle, motorcycle, bicyclist and motorcyclist, due to the extremely imbalanced point distribution in the dataset. For example, the number of points for vegetation is 7000 times more than that of motorcyclist. \n\n\nG. Additional Results on SemanticKITTI\n\n\nH. Additional Results on S3DIS\n\nWe report the detailed 6-fold cross validation results of our RandLA-Net on S3DIS [2] in Table 8. Figure 11 shows more qualitative results of our approach.\n\n\nI. Video Illustration\n\nWe provide a video to show qualitative results of our RandLA-Net on both indoor and outdoor datasets, which can be viewed at https://www.youtube.com/ watch?v=Ar3eY_lwzMk&t=9s.  \n\nFigure 2 .\n2In each layer of RandLA-Net, the large-scale point cloud is significantly downsampled, yet is capable of retaining features necessary for accurate segmentation.\n\nFigure 4 .\n4Illustration of the dilated residual block which significantly increases the receptive field (dotted circle) of each point, colored points represent the aggregated features. L: Local spatial encoding, A: Attentive pooling.\n\nFigure 5 .\n5Time and memory consumption of different sampling approaches. The dashed lines represent estimated values due to the limited GPU memory.\n\nFigure 6 .\n6Qualitative results of RandLA-Net on the validation set of SemanticKITTI[3]. Red circles show the failure cases. classes, except low vegetation and scanning art..\n\nFigure 7 .\n7The detailed architecture of our RandLA-Net. (N, D) represents the number of points and feature dimension respectively. FC: Fully Connected layer, LFA: Local Feature Aggregation, RS: Random Sampling, MLP: shared Multi-Layer Perceptron, US: Up-sampling, DP: Dropout. B. Details of the Network Architecture\n\nFigure 7\n7shows the detailed architecture of RandLA-Net. The network follows the widely-used encoder-decoder architecture with skip connections. The input point cloud is first fed to a shared MLP layer to extract per-point features. Four encoding and decoding layers are then used to learn features for each point. At last, three fully-connected layers and a dropout layer are used to predict the semantic label of each point. The details of each part are as follows:\n\nFinal\nSemantic Prediction: The final semantic label of each point is obtained through three shared fully-connected layers (N , 64) \u2192 (N , 32) \u2192 (N , n class ) and a dropout layer. The dropout ratio is 0.5. Network Output: The output of RandLA-Net is the predicted semantics of all points, with a size of N \u00d7 n class , where n class is the number of classes.\n\nFigure 8 .\n8Visualization of the learned attention matrix in different layers. From top left to bottom right: 16\u00d716 attention matrix, 64\u00d764 attention matrix, 128\u00d7128 attention matrix, 256\u00d7256 attention matrix. The yellow color represents higher attention scores.\n\nFigure 9 .\n9Qualitative results of RandLA-Net on the reduced-8 split of Semantic3D. From left to right: full RGB colored point clouds, predicted semantic labels of full point clouds, detailed view of colored point clouds, detailed view of predicted semantic labels. Note that the ground truth of the test set is not publicly available.\n\nFigure 10 .\n10Qualitative results of RandLA-Net on the validation split of SemanticKITTI[3]. Red boxes show the failure cases.\n\n\n)Dilated Residual Block \n\n( , \u2032 ) \n\nAggregated \nfeatures \n\n( , 3) \n\n3D coordinates \nPoint features \n\n\uf0c5 Concatenation \n\nK K nearest neighbor \nDot product \n\uf0e5 Sum \nS Softmax \n\nAttention scores \nAttention features \nAggregated feature \n\nShared \nMLP \n\nShared \nMLP \n\nShared \nMLP \n\n\u121a f \n\n{p , } \n\n{ } \n\n{ } \n\n{ } \n\n{ \u1218 } \n{s } \n\nRelative Point \nPosition Encoding \n\n\n\nTable 2\n2presents the quantitative results of different approaches. RandLA-Net clearly outperforms all existing methods in terms of both mIoU and OA. Notably, RandLA-Net also achieves superior performance on six of the eight mIoU (%) OA (%) man-made. natural. high veg. low veg. buildings hard scape scanning art. carsSnapNet [4] \n59.1 \n88.6 \n82.0 \n77.3 \n79.7 \n22.9 \n91.1 \n18.4 \n37.3 \n64.4 \nSEGCloud [52] \n61.3 \n88.1 \n83.9 \n66.0 \n86.0 \n40.5 \n91.1 \n30.9 \n27.5 \n64.3 \nRF MSSF [53] \n62.7 \n90.3 \n87.6 \n80.3 \n81.8 \n36.4 \n92.2 \n24.1 \n42.6 \n56.6 \nMSDeepVoxNet [46] \n65.3 \n88.4 \n83.0 \n67.2 \n83.8 \n36.7 \n92.4 \n31.3 \n50.0 \n78.2 \nShellNet [69] \n69.3 \n93.2 \n96.3 \n90.4 \n83.9 \n41.0 \n94.2 \n34.7 \n43.9 \n70.2 \nGACNet [56] \n70.8 \n91.9 \n86.4 \n77.7 \n88.5 \n60.6 \n94.2 \n37.3 \n43.5 \n77.8 \nSPG [26] \n73.2 \n94.0 \n97.4 \n92.6 \n87.9 \n44.0 \n83.2 \n31.0 \n63.5 \n76.2 \nKPConv [54] \n74.6 \n92.9 \n90.9 \n82.2 \n84.2 \n47.9 \n94.9 \n40.0 \n77.3 \n79.7 \nRandLA-Net (Ours) \n77.4 \n94.8 \n95.6 \n91.4 \n86.6 \n51.5 \n95.7 \n51.5 \n69.8 \n76.8 \n\nTable 2. Quantitative results of different approaches on Semantic3D (reduced-8) [17]. Only the recent published approaches are compared. \nAccessed on 31 March 2020. \n\nMethods \nSize \nmIoU(%) \nParams(M) \n\nroad \nsidewalk \nparking \nother-ground \n\nbuilding \n\ncar \ntruck \nbicycle \nmotorcycle \nother-vehicle \nvegetation \n\ntrunk \nterrain \nperson \nbicyclist \nmotorcyclist \n\nfence \npole \ntraffic-sign \n\nPointNet [43] \n\n50K pts \n\n14.6 \n3 \n61.6 35.7 15.8 1.4 41.4 46.3 0.1 \n1.3 \n0.3 \n0.8 31.0 4.6 17.6 0.2 \n0.2 0.0 12.9 2.4 \n3.7 \nSPG [26] \n17.4 0.25 45.0 28.5 0.6 \n0.6 64.3 49.3 0.1 \n0.2 \n0.2 \n0.8 48.9 27.2 24.6 0.3 \n2.7 0.1 20.8 15.9 0.8 \nSPLATNet [49] \n18.4 0.8 64.6 39.1 0.4 \n0.0 58.3 58.2 0.0 \n0.0 \n0.0 \n0.0 71.1 9.9 19.3 0.0 \n0.0 0.0 23.1 5.6 \n0.0 \nPointNet++ [44] \n20.1 \n6 \n72.0 41.8 18.7 5.6 62.3 53.7 0.9 \n1.9 \n0.2 \n0.2 46.5 13.8 30.0 0.9 \n1.0 0.0 16.9 6.0 \n8.9 \nTangentConv [51] \n40.9 0.4 83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5 \n\nSqueezeSeg [58] \n\n64*2048 \npixels \n\n29.5 \n1 \n85.4 54.3 26.9 4.5 57.4 68.8 3.3 16.0 4.1 \n3.6 60.0 24.3 53.7 12.9 13.1 0.9 29.0 17.5 24.5 \nSqueezeSegV2 [59] \n39.7 \n1 \n88.6 67.6 45.8 17.7 73.7 81.8 13.4 18.5 17.9 14.0 71.8 35.8 60.2 20.1 25.1 3.9 41.1 20.2 36.3 \nDarkNet21Seg [3] \n47.4 \n25 \n91.4 74.0 57.0 26.4 81.9 85.4 18.6 26.2 26.5 15.6 77.6 48.4 63.6 31.8 33.6 4.0 52.3 36.0 50.0 \nDarkNet53Seg [3] \n49.9 \n50 \n91.8 74.6 64.8 27.9 84.1 86.4 25.5 24.5 32.7 22.6 78.3 50.1 64.0 36.2 33.6 4.7 55.0 38.9 52.2 \nRangeNet53++ [40] \n52.2 \n50 \n91.8 75.2 65.0 27.8 87.4 91.4 25.7 25.7 34.4 23.0 80.5 55.1 64.6 38.3 38.8 4.8 58.6 47.9 55.9 \n\nRandLA-Net (Ours) 50K pts 53.9 1.24 90.7 73.7 60.3 20.4 86.9 94.2 40.1 26.0 25.8 38.9 81.4 61.3 66.8 49.2 48.2 7.2 56.3 49.2 47.7 \n\n\n\nTable 3 .\n3Quantitative results of different approaches on SemanticKITTI[3]. Only the recent published methods are compared and all scores are obtained from the online single scan evaluation track. Accessed on 31 March 2020.car \nroad \n\n\nTable 4 .\n4Quantitative results of different approaches on the S3DIS \ndataset [2] (6-fold cross validation). Only the recent published \nmethods are included. \n\n\n\nTable 5 .\n5The mean IoU scores of all ablated networks based on our full RandLA-Net.\n\nTable 6 .\n6The mIoU result of RandLA-Net by encoding different kinds of spatial information.\n\nTable 7 .\n7The mIoU scores of RandLA-Net regarding different number of aggregation units in a residual block.\nWe use the same hardware in Sec 3.4, unless specified otherwise.\nhttps://github.com/charlesq34/pointnet2 3 https://github.com/orendv/learning_to_sample\nAcknowledgments: This work was partially supported by a China Scholarship Council (CSC) scholarship. Yulan Guo was supported by the National Natural Science Foundation of China (No. 61972435), Natural Science Foundation of Guangdong Province (2019A1515011271), and Shenzhen Technology and Innovation Committee.AppendicesA. Details for the Evaluation of Sampling.We provide the implementation details of different sampling approaches evaluated in Section 4.1. To sample K points (point features) from a large-scale point cloud P with N points (point features):1. Farthest Point Sampling (FPS): We follow the implementation 2 provided by PointNet++[44], which is also widely used in[33,60,36,6,70]. In particular, FPS is implemented as an operator running on GPU.2. Inverse Density Importance Sampling (IDIS): Given a point p i , its density \u03c1 is approximated by calculating the summation of the distances between p i and its nearest t points[15]. Formally:where p j i represents the coordinates (i.e. x-y-z) of the j th point of the neighbour points set N (p i ), t is set to 16. All the points are ranked according to the inverse density 1 \u03c1 of points. Finally, the top K points are selected.Random Sampling (RS):We implement random sampling with the python numpy package. Specifically, we first use the numpy function numpy.random.choice() to generate K indices. We then gather the corresponding spatial coordinates and per-point features from point clouds by using these indices.Generator-based Sampling (GS):The implementation follows the code 3 provided by[12]. We first train a ProgressiveNet[12]to transform the raw point clouds into ordered point sets according to their relevance to the task. After that, the first K points are kept, while the rest is discarded.Continuous Relaxation based Sampling (CRS): CRSis implemented with the self-attended gumbel-softmax sampling [1][66]. Given a point feature set P \u2208 R N \u00d7(d+3) with 3D coordinates and per point features, we firstly estimate a probability score vector s \u2208 R N through a score function parameterized by a MLP layer, i.e., s = sof tmax(M LP (P )), which learns a categorical distribution. Then, with the Gumbel noise g \u2208 R N drawn from the distribution Gumbel(0, 1).\nConcrete autoencoders for differentiable feature selection and reconstruction. Abubakar Abid, Muhammad Fatih Balin, James Zou, ICML. Abubakar Abid, Muhammad Fatih Balin, and James Zou. Concrete autoencoders for differentiable feature selection and reconstruction. In ICML, 2019.\n\nJoint 2D-3D-semantic data for indoor scene understanding. Iro Armeni, Sasha Sax, Silvio Amir R Zamir, Savarese, CVPR. Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2D-3D-semantic data for indoor scene understanding. In CVPR, 2017.\n\nSe-manticKITTI: A dataset for semantic scene understanding of lidar sequences. Jens Behley, Martin Garbade, Andres Milioto, Sven Behnke, Cyrill Stachniss, and Juergen Gall. ICCVJens Behley, Martin Garbade, Andres Milioto, Jan Quen- zel, Sven Behnke, Cyrill Stachniss, and Juergen Gall. Se- manticKITTI: A dataset for semantic scene understanding of lidar sequences. In ICCV, 2019.\n\nUnstructured point cloud semantic labeling using deep segmentation networks. Alexandre Boulch, Bertrand Le Saux, Nicolas Audebert, 3DORAlexandre Boulch, Bertrand Le Saux, and Nicolas Audebert. Unstructured point cloud semantic labeling using deep seg- mentation networks. In 3DOR, 2017.\n\nClusterNet: Deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis. Chao Chen, Guanbin Li, Ruijia Xu, Tianshui Chen, Meng Wang, Liang Lin, CVPR. Chao Chen, Guanbin Li, Ruijia Xu, Tianshui Chen, Meng Wang, and Liang Lin. ClusterNet: Deep hierarchical cluster network with rigorously rotation-invariant representation for point cloud analysis. In CVPR, 2019.\n\nLSANet: Feature learning on point sets by local spatial attention. Lin-Zhuo Chen, Xuan-Yi Li, Deng-Ping Fan, Ming-Ming Cheng, Kai Wang, Shao-Ping Lu, arXiv:1905.05442arXiv preprintLin-Zhuo Chen, Xuan-Yi Li, Deng-Ping Fan, Ming-Ming Cheng, Kai Wang, and Shao-Ping Lu. LSANet: Feature learning on point sets by local spatial attention. arXiv preprint arXiv:1905.05442, 2019.\n\nPCT: Large-scale 3D point cloud representations via graph inception networks with applications to autonomous driving. Siheng Chen, Sufeng Niu, Tian Lan, Baoan Liu, ICIP. Siheng Chen, Sufeng Niu, Tian Lan, and Baoan Liu. PCT: Large-scale 3D point cloud representations via graph incep- tion networks with applications to autonomous driving. In ICIP, 2019.\n\nMulti-view 3D object detection network for autonomous driving. Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia, CVPR. Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3D object detection network for autonomous driving. In CVPR, 2017.\n\nFast point R-CNN. Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia, ICCV. Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast point R-CNN. In ICCV, 2019.\n\n4D spatio-temporal convnets: Minkowski convolutional neural networks. Christopher Choy, Junyoung Gwak, Silvio Savarese, CVPR. Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D spatio-temporal convnets: Minkowski convolutional neural networks. In CVPR, 2019.\n\nPoint signatures: A new representation for 3D object recognition. Chin Seng Chua, Ray Jarvis, Chin Seng Chua and Ray Jarvis. Point signatures: A new representation for 3D object recognition. IJCV, 1997.\n\nLearning to sample. Oren Dovrat, Itai Lang, Shai Avidan, CVPR. Oren Dovrat, Itai Lang, and Shai Avidan. Learning to sam- ple. In CVPR, 2019.\n\nDilated point convolutions: On the receptive field of point convolutions. Francis Engelmann, Theodora Kontogianni, Bastian Leibe, BMVC. Francis Engelmann, Theodora Kontogianni, and Bastian Leibe. Dilated point convolutions: On the receptive field of point convolutions. In BMVC, 2019.\n\n3D semantic segmentation with submanifold sparse convolutional networks. Benjamin Graham, Martin Engelcke, Laurens Van Der Maaten, CVPR. Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3D semantic segmentation with submanifold sparse convolutional networks. In CVPR, 2018.\n\nFlex-convolution (million-scale point-cloud learning beyond grid-worlds). Fabian Groh, Patrick Wieschollek, Hendrik P A Lensch, In ACCV. Fabian Groh, Patrick Wieschollek, and Hendrik P. A. Lensch. Flex-convolution (million-scale point-cloud learning beyond grid-worlds). In ACCV, 2018.\n\nYulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, Mohammed Bennamoun, arXiv:1912.12033Deep learning for 3d point clouds: A survey. arXiv preprintYulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3d point clouds: A survey. arXiv preprint arXiv:1912.12033, 2019.\n\nTimo Hackel, Nikolay Savinov, Lubor Ladicky, Jan D Wegner, Konrad Schindler, Marc Pollefeys, Semantic3d. net: A new large-scale point cloud classification benchmark. IS-PRS. Timo Hackel, Nikolay Savinov, Lubor Ladicky, Jan D Weg- ner, Konrad Schindler, and Marc Pollefeys. Semantic3d. net: A new large-scale point cloud classification benchmark. IS- PRS, 2017.\n\nFast semantic segmentation of 3d point clouds with strongly varying density. Timo Hackel, Jan D Wegner, Konrad Schindler, ISPRS. Timo Hackel, Jan D Wegner, and Konrad Schindler. Fast se- mantic segmentation of 3d point clouds with strongly vary- ing density. ISPRS, 2016.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nPointwise convolutional neural networks. Binh-Son, Minh-Khoi Hua, Sai-Kit Tran, Yeung, In CVPR. Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point- wise convolutional neural networks. In CVPR, 2018.\n\nRecurrent slice networks for 3D segmentation of point clouds. Qiangui Huang, Weiyue Wang, Ulrich Neumann, CVPR. Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recur- rent slice networks for 3D segmentation of point clouds. In CVPR, 2018.\n\nHierarchical point-edge interaction network for point cloud semantic segmentation. Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-Wing Fu, Jiaya Jia, ICCV. Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi- Wing Fu, and Jiaya Jia. Hierarchical point-edge interaction network for point cloud semantic segmentation. In ICCV, 2019.\n\nA-CNN: Annularly convolutional neural networks on point clouds. Artem Komarichev, Zichun Zhong, Jing Hua, CVPR. Artem Komarichev, Zichun Zhong, and Jing Hua. A-CNN: Annularly convolutional neural networks on point clouds. In CVPR, 2019.\n\nModeling local geometric structure of 3D point clouds using Geo-CNN. Shiyi Lan, Ruichi Yu, Gang Yu, Larry S Davis, CVPR. Shiyi Lan, Ruichi Yu, Gang Yu, and Larry S Davis. Model- ing local geometric structure of 3D point clouds using Geo- CNN. In CVPR, 2019.\n\nA structured regularization framework for spatially smoothing semantic labelings of 3d point clouds. Loic Landrieu, Hugo Raguet, Bruno Vallet, Cl\u00e9ment Mallet, Martin Weinmann, Loic Landrieu, Hugo Raguet, Bruno Vallet, Cl\u00e9ment Mallet, and Martin Weinmann. A structured regularization frame- work for spatially smoothing semantic labelings of 3d point clouds. ISPRS, 2017.\n\nLarge-scale point cloud semantic segmentation with superpoint graphs. Loic Landrieu, Martin Simonovsky, CVPR. Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In CVPR, 2018.\n\nPointPillars: Fast encoders for object detection from point clouds. Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom, CVPR. Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019.\n\nPointGrid: A deep network for 3D shape understanding. Truc Le, Ye Duan, CVPR. Truc Le and Ye Duan. PointGrid: A deep network for 3D shape understanding. In CVPR, 2018.\n\nOctree guided cnn with spherical kernels for 3D point clouds. Huan Lei, Naveed Akhtar, Ajmal Mian, CVPR. Huan Lei, Naveed Akhtar, and Ajmal Mian. Octree guided cnn with spherical kernels for 3D point clouds. In CVPR, 2019.\n\nVehicle detection from 3D lidar using fully convolutional network. Bo Li, Tianlei Zhang, Tian Xia, RSS. Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from 3D lidar using fully convolutional network. In RSS, 2016.\n\nDeepgcns: Can gcns go as deep as cnns? In ICCV. Guohao Li, Matthias Muller, Ali Thabet, Bernard Ghanem, Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as cnns? In ICCV, October 2019.\n\nSO-Net: Selforganizing network for point cloud analysis. Jiaxin Li, M Ben, Gim Hee Chen, Lee, CVPR. Jiaxin Li, Ben M Chen, and Gim Hee Lee. SO-Net: Self- organizing network for point cloud analysis. In CVPR, 2018.\n\nPointCNN: Convolution on Xtransformed points. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen, In NeurIPS. Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. PointCNN: Convolution on X- transformed points. In NeurIPS, 2018.\n\nDynamic points agglomeration for hierarchical point sets learning. Jinxian Liu, Bingbing Ni, Caiyuan Li, Jiancheng Yang, Qi Tian, ICCV. Jinxian Liu, Bingbing Ni, Caiyuan Li, Jiancheng Yang, and Qi Tian. Dynamic points agglomeration for hierarchical point sets learning. In ICCV, 2019.\n\nMeteor-Net: Deep learning on dynamic 3D point cloud sequences. Xingyu Liu, Mengyuan Yan, Jeannette Bohg, ICCV. Xingyu Liu, Mengyuan Yan, and Jeannette Bohg. Meteor- Net: Deep learning on dynamic 3D point cloud sequences. In ICCV, 2019.\n\nRelation-shape convolutional neural network for point cloud analysis. Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. CVPRYongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In CVPR, 2019.\n\nPointvoxel cnn for efficient 3d deep learning. Zhijian Liu, Haotian Tang, Yujun Lin, Song Han, NeurIPS. Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point- voxel cnn for efficient 3d deep learning. In NeurIPS, 2019.\n\nInterpolated convolutional networks for 3D point cloud understanding. Jiageng Mao, Xiaogang Wang, Hongsheng Li, ICCV. Jiageng Mao, Xiaogang Wang, and Hongsheng Li. Interpo- lated convolutional networks for 3D point cloud understand- ing. In ICCV, 2019.\n\nVV-net: Voxel vae net with group convolutions for point cloud segmentation. Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, Dinesh Manocha, ICCV. Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, and Dinesh Manocha. VV-net: Voxel vae net with group convolutions for point cloud segmentation. In ICCV, 2019.\n\nRangeNet++: Fast and accurate lidar semantic segmentation. Andres Milioto, Ignacio Vizzo, Jens Behley, Cyrill Stachniss, IROS. Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss. RangeNet++: Fast and accurate lidar semantic segmentation. In IROS, 2019.\n\nAndriy Mnih, Karol Gregor, arXiv:1402.0030Neural variational inference and learning in belief networks. arXiv preprintAndriy Mnih and Karol Gregor. Neural variational in- ference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014.\n\nAttentional pointnet for 3d-object detection in point clouds. Anshul Paigwar, Ozgur Erkent, Christian Wolf, Christian Laugier, CVPRWAnshul Paigwar, Ozgur Erkent, Christian Wolf, and Christian Laugier. Attentional pointnet for 3d-object detection in point clouds. In CVPRW, 2019.\n\nPointNet: Deep learning on point sets for 3D classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, CVPR. Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. PointNet: Deep learning on point sets for 3D classification and segmentation. In CVPR, 2017.\n\nPointNet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, NeurIPS. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, 2017.\n\nFully-convolutional point networks for large-scale point clouds. Dario Rethage, Johanna Wald, Jurgen Sturm, Nassir Navab, Federico Tombari, ECCV. Dario Rethage, Johanna Wald, Jurgen Sturm, Nassir Navab, and Federico Tombari. Fully-convolutional point networks for large-scale point clouds. In ECCV, 2018.\n\nClassification of point cloud scenes with multiscale voxel deep network. Xavier Roynard, Jean-Emmanuel Deschaud, Fran\u00e7ois Goulette, arXiv:1804.03583arXiv preprintXavier Roynard, Jean-Emmanuel Deschaud, and Fran\u00e7ois Goulette. Classification of point cloud scenes with multi- scale voxel deep network. arXiv preprint arXiv:1804.03583, 2018.\n\nFast point feature histograms (fpfh) for 3D registration. Nico Radu Bogdan Rusu, Michael Blodow, Beetz, ICRA. Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast point feature histograms (fpfh) for 3D registration. In ICRA, 2009.\n\nMining point cloud local structures by kernel correlation and graph pooling. Yiru Shen, Chen Feng, Yaoqing Yang, Dong Tian, CVPR. Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Min- ing point cloud local structures by kernel correlation and graph pooling. In CVPR, 2018.\n\nSPLATNet: sparse lattice networks for point cloud processing. Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, Jan Kautz, CVPR. Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz. SPLATNet: sparse lattice networks for point cloud process- ing. In CVPR, 2018.\n\nPolicy gradient methods for reinforcement learning with function approximation. S Richard, David A Sutton, Mcallester, P Satinder, Yishay Singh, Mansour, NeurIPS. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NeurIPS, 2000.\n\nTangent convolutions for dense prediction in 3D. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou, CVPR. Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian- Yi Zhou. Tangent convolutions for dense prediction in 3D. In CVPR, 2018.\n\nSegcloud: Semantic segmentation of 3D point clouds. Lyne Tchapmi, Christopher Choy, Iro Armeni, Junyoung Gwak, Silvio Savarese, 3Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmen- tation of 3D point clouds. In 3DV, 2017.\n\nSemantic classification of 3D point clouds with multiscale spherical neighborhoods. Hugues Thomas, Fran\u00e7ois Goulette, Jean-Emmanuel Deschaud, Beatriz Marcotegui, 3Hugues Thomas, Fran\u00e7ois Goulette, Jean-Emmanuel De- schaud, and Beatriz Marcotegui. Semantic classification of 3D point clouds with multiscale spherical neighborhoods. In 3DV, 2018.\n\nKPConv: Flexible and deformable convolution for point clouds. Hugues Thomas, R Charles, Jean-Emmanuel Qi, Beatriz Deschaud, Fran\u00e7ois Marcotegui, Leonidas J Goulette, Guibas, ICCV. Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J Guibas. KPConv: Flexible and deformable convolution for point clouds. In ICCV, 2019.\n\nLocal spectral graph convolution for point set feature learning. Chu Wang, Babak Samari, Kaleem Siddiqi, ECCV. Chu Wang, Babak Samari, and Kaleem Siddiqi. Local spec- tral graph convolution for point set feature learning. In ECCV, 2018.\n\nGraph attention convolution for point cloud semantic segmentation. Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, Jie Shan, CVPR. Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph attention convolution for point cloud seman- tic segmentation. In CVPR, 2019.\n\nDynamic graph cnn for learning on point clouds. Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, Justin M Solomon, ACM Transactions on Graphics. Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019.\n\nSqueezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3D lidar point cloud. Bichen Wu, Alvin Wan, Xiangyu Yue, Kurt Keutzer, In ICRA. Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3D lidar point cloud. In ICRA, 2018.\n\nSqueezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud. Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, Kurt Keutzer, ICRA. Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, and Kurt Keutzer. Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmenta- tion from a lidar point cloud. In ICRA, 2019.\n\nPointConv: Deep convolutional networks on 3D point clouds. Wenxuan Wu, Zhongang Qi, Li Fuxin, CVPR. Wenxuan Wu, Zhongang Qi, and Li Fuxin. PointConv: Deep convolutional networks on 3D point clouds. In CVPR, 2018.\n\nAttentional shapecontextnet for point cloud recognition. Saining Xie, Sainan Liu, Zeyu Chen, Zhuowen Tu, CVPR. Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. At- tentional shapecontextnet for point cloud recognition. In CVPR, 2018.\n\nRich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, ICML. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption gen- eration with visual attention. In ICML, 2015.\n\nPixor: Realtime 3D object detection from point clouds. Bin Yang, Wenjie Luo, Raquel Urtasun, CVPR. Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real- time 3D object detection from point clouds. In CVPR, 2018.\n\nLearning object bounding boxes for 3D instance segmentation on point clouds. Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, Niki Trigoni, NeurIPS. Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, and Niki Trigoni. Learning ob- ject bounding boxes for 3D instance segmentation on point clouds. In NeurIPS, 2019.\n\nRobust attentional aggregation of deep feature sets for multiview 3D reconstruction. Bo Yang, Sen Wang, Andrew Markham, Niki Trigoni, IJCVBo Yang, Sen Wang, Andrew Markham, and Niki Trigoni. Robust attentional aggregation of deep feature sets for multi- view 3D reconstruction. IJCV, 2019.\n\nModeling point clouds with self-attention and gumbel subset sampling. Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian, CVPR. Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds with self-attention and gumbel subset sampling. In CVPR, 2019.\n\n3D recurrent neural networks with context fusion for point cloud semantic segmentation. Xiaoqing Ye, Jiamao Li, Hexiao Huang, Liang Du, Xiaolin Zhang, ECCV. Xiaoqing Ye, Jiamao Li, Hexiao Huang, Liang Du, and Xi- aolin Zhang. 3D recurrent neural networks with context fu- sion for point cloud semantic segmentation. In ECCV, 2018.\n\nPCAN: 3D attention map learning using contextual information for point cloud based retrieval. Wenxiao Zhang, Chunxia Xiao, CVPR. Wenxiao Zhang and Chunxia Xiao. PCAN: 3D attention map learning using contextual information for point cloud based retrieval. In CVPR, 2019.\n\nShellnet: Efficient point cloud convolutional neural networks using concentric shells statistics. Zhiyuan Zhang, Binh-Son, Sai-Kit Hua, Yeung, ICCV. Zhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Shell- net: Efficient point cloud convolutional neural networks us- ing concentric shells statistics. In ICCV, 2019.\n\nPointweb: Enhancing local neighborhood features for point cloud processing. Hengshuang Zhao, Li Jiang, Chi-Wing Fu, Jiaya Jia, CVPR. Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In CVPR, 2019.\n\nOA(%) mAcc(%) mIoU(%) ceil. floor wall beam col. wind. door table chair sofa book. board clutOA(%) mAcc(%) mIoU(%) ceil. floor wall beam col. wind. door table chair sofa book. board clut.\n\nOverall Accuracy (OA, %), mean class Accuracy (mAcc, %), mean IoU (mIoU, %), and per-class. 8Quantitative results of different approaches on S3DIS [2] (6-fold cross-validation). IoU (%) are reportedTable 8. Quantitative results of different approaches on S3DIS [2] (6-fold cross-validation). Overall Accuracy (OA, %), mean class Accuracy (mAcc, %), mean IoU (mIoU, %), and per-class IoU (%) are reported.\n\nSemantic segmentation results of our RandLA-Net on the complete point clouds of Areas 1-6 in S3DIS. Left: full RGB input cloud. Figure 11. middle: predicted labels; right: ground truthFigure 11. Semantic segmentation results of our RandLA-Net on the complete point clouds of Areas 1-6 in S3DIS. Left: full RGB input cloud; middle: predicted labels; right: ground truth.\n", "annotations": {"author": "[{\"end\":110,\"start\":75},{\"end\":142,\"start\":111},{\"end\":177,\"start\":143},{\"end\":214,\"start\":178},{\"end\":294,\"start\":215},{\"end\":330,\"start\":295},{\"end\":367,\"start\":331},{\"end\":406,\"start\":368}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":84},{\"end\":118,\"start\":114},{\"end\":153,\"start\":150},{\"end\":190,\"start\":186},{\"end\":224,\"start\":221},{\"end\":306,\"start\":302},{\"end\":343,\"start\":336},{\"end\":382,\"start\":375}]", "author_first_name": "[{\"end\":83,\"start\":75},{\"end\":113,\"start\":111},{\"end\":149,\"start\":143},{\"end\":185,\"start\":178},{\"end\":220,\"start\":215},{\"end\":301,\"start\":295},{\"end\":335,\"start\":331},{\"end\":374,\"start\":368}]", "author_affiliation": "[{\"end\":109,\"start\":88},{\"end\":141,\"start\":120},{\"end\":176,\"start\":155},{\"end\":213,\"start\":192},{\"end\":249,\"start\":226},{\"end\":293,\"start\":251},{\"end\":329,\"start\":308},{\"end\":366,\"start\":345},{\"end\":405,\"start\":384}]", "title": "[{\"end\":72,\"start\":1},{\"end\":478,\"start\":407}]", "venue": null, "abstract": "[{\"end\":1696,\"start\":480}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2250,\"start\":2246},{\"end\":2378,\"start\":2370},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2428,\"start\":2424},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2438,\"start\":2434},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2476,\"start\":2473},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3036,\"start\":3032},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3039,\"start\":3036},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3042,\"start\":3039},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":3045,\"start\":3042},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":3048,\"start\":3045},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3079,\"start\":3075},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3082,\"start\":3079},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3085,\"start\":3082},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3088,\"start\":3085},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3090,\"start\":3088},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3093,\"start\":3090},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3096,\"start\":3093},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3130,\"start\":3126},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3133,\"start\":3130},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3136,\"start\":3133},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3139,\"start\":3136},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3142,\"start\":3139},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3145,\"start\":3142},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3148,\"start\":3145},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3151,\"start\":3148},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3191,\"start\":3187},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":3194,\"start\":3191},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":3197,\"start\":3194},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3200,\"start\":3197},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3800,\"start\":3796},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4396,\"start\":4392},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4531,\"start\":4527},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4543,\"start\":4540},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6033,\"start\":6029},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6036,\"start\":6033},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6039,\"start\":6036},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":6042,\"start\":6039},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6045,\"start\":6042},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7780,\"start\":7776},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7803,\"start\":7800},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8522,\"start\":8518},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8525,\"start\":8522},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8528,\"start\":8525},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8531,\"start\":8528},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8570,\"start\":8566},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8573,\"start\":8570},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8576,\"start\":8573},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8763,\"start\":8759},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8765,\"start\":8763},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8768,\"start\":8765},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8771,\"start\":8768},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9026,\"start\":9022},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9029,\"start\":9026},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9032,\"start\":9029},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9035,\"start\":9032},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9037,\"start\":9035},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9297,\"start\":9293},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9300,\"start\":9297},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9476,\"start\":9472},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9479,\"start\":9476},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9482,\"start\":9479},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":9485,\"start\":9482},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9516,\"start\":9512},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9519,\"start\":9516},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9522,\"start\":9519},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9525,\"start\":9522},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9527,\"start\":9525},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9530,\"start\":9527},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9533,\"start\":9530},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9536,\"start\":9533},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9570,\"start\":9566},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9573,\"start\":9570},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9576,\"start\":9573},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9579,\"start\":9576},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9582,\"start\":9579},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9585,\"start\":9582},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9588,\"start\":9585},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9591,\"start\":9588},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9632,\"start\":9628},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":9635,\"start\":9632},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9638,\"start\":9635},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9641,\"start\":9638},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10509,\"start\":10505},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10623,\"start\":10619},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10635,\"start\":10632},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11637,\"start\":11633},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11640,\"start\":11637},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11643,\"start\":11640},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11646,\"start\":11643},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11648,\"start\":11646},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11651,\"start\":11648},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12210,\"start\":12206},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12213,\"start\":12210},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":12216,\"start\":12213},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12722,\"start\":12718},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13468,\"start\":13464},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13854,\"start\":13851},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":13857,\"start\":13854},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15060,\"start\":15056},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18331,\"start\":18327},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18453,\"start\":18449},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18681,\"start\":18677},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18684,\"start\":18681},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18955,\"start\":18951},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20209,\"start\":20205},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20249,\"start\":20245},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22941,\"start\":22937},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23813,\"start\":23809},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23816,\"start\":23813},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":23819,\"start\":23816},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23822,\"start\":23819},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":23825,\"start\":23822},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":23828,\"start\":23825},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24281,\"start\":24278},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24478,\"start\":24474},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24481,\"start\":24478},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24484,\"start\":24481},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24487,\"start\":24484},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":24490,\"start\":24487},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25175,\"start\":25171},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25381,\"start\":25377},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25399,\"start\":25395},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25497,\"start\":25493},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":25513,\"start\":25509},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26105,\"start\":26101},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26127,\"start\":26124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26153,\"start\":26150},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26213,\"start\":26209},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26766,\"start\":26763},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26769,\"start\":26766},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26772,\"start\":26769},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26775,\"start\":26772},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":26778,\"start\":26775},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":26781,\"start\":26778},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26784,\"start\":26781},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26838,\"start\":26834},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26892,\"start\":26889},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27523,\"start\":27519},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27526,\"start\":27523},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27529,\"start\":27526},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27532,\"start\":27529},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27535,\"start\":27532},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27575,\"start\":27571},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27578,\"start\":27575},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27580,\"start\":27578},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27583,\"start\":27580},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27744,\"start\":27740},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27747,\"start\":27744},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27750,\"start\":27747},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27753,\"start\":27750},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27756,\"start\":27753},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27828,\"start\":27824},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27831,\"start\":27828},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27833,\"start\":27831},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27836,\"start\":27833},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27894,\"start\":27890},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28053,\"start\":28049},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28203,\"start\":28200},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28719,\"start\":28715},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28722,\"start\":28719},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":28725,\"start\":28722},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":28728,\"start\":28725},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28731,\"start\":28728},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28733,\"start\":28731},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29123,\"start\":29119},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29129,\"start\":29128},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29423,\"start\":29420},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":32005,\"start\":32001},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32068,\"start\":32064},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":33375,\"start\":33371},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33715,\"start\":33711},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34010,\"start\":34007},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34030,\"start\":34026},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34173,\"start\":34170},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36227,\"start\":36224},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37613,\"start\":37610},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38928,\"start\":38924},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39073,\"start\":39070},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39795,\"start\":39792},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40717,\"start\":40714},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":42644,\"start\":42641},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45853,\"start\":45850}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":40242,\"start\":40069},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40478,\"start\":40243},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40628,\"start\":40479},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40804,\"start\":40629},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41122,\"start\":40805},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41591,\"start\":41123},{\"attributes\":{\"id\":\"fig_9\"},\"end\":41950,\"start\":41592},{\"attributes\":{\"id\":\"fig_11\"},\"end\":42214,\"start\":41951},{\"attributes\":{\"id\":\"fig_12\"},\"end\":42551,\"start\":42215},{\"attributes\":{\"id\":\"fig_14\"},\"end\":42679,\"start\":42552},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43039,\"start\":42680},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45776,\"start\":43040},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46013,\"start\":45777},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46175,\"start\":46014},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46261,\"start\":46176},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":46355,\"start\":46262},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":46466,\"start\":46356}]", "paragraph": "[{\"end\":2205,\"start\":1712},{\"end\":3858,\"start\":2207},{\"end\":4036,\"start\":3860},{\"end\":4279,\"start\":4038},{\"end\":4787,\"start\":4281},{\"end\":5881,\"start\":4789},{\"end\":7440,\"start\":5883},{\"end\":7894,\"start\":7442},{\"end\":8061,\"start\":7896},{\"end\":8222,\"start\":8063},{\"end\":8398,\"start\":8224},{\"end\":8669,\"start\":8415},{\"end\":9232,\"start\":8671},{\"end\":10456,\"start\":9234},{\"end\":10914,\"start\":10458},{\"end\":11561,\"start\":10940},{\"end\":11906,\"start\":11598},{\"end\":11930,\"start\":11908},{\"end\":12527,\"start\":11932},{\"end\":12974,\"start\":12529},{\"end\":13398,\"start\":12976},{\"end\":13788,\"start\":13400},{\"end\":14738,\"start\":13790},{\"end\":15498,\"start\":14949},{\"end\":16245,\"start\":15500},{\"end\":16500,\"start\":16275},{\"end\":17058,\"start\":16502},{\"end\":17275,\"start\":17060},{\"end\":17463,\"start\":17277},{\"end\":17880,\"start\":17528},{\"end\":18556,\"start\":17882},{\"end\":19016,\"start\":18558},{\"end\":19073,\"start\":19018},{\"end\":19310,\"start\":19117},{\"end\":19574,\"start\":19336},{\"end\":19849,\"start\":19605},{\"end\":20354,\"start\":19880},{\"end\":21533,\"start\":20356},{\"end\":22228,\"start\":21552},{\"end\":22513,\"start\":22276},{\"end\":23211,\"start\":22515},{\"end\":24040,\"start\":23213},{\"end\":24623,\"start\":24069},{\"end\":25039,\"start\":24625},{\"end\":25927,\"start\":25041},{\"end\":26154,\"start\":25967},{\"end\":28155,\"start\":26156},{\"end\":28573,\"start\":28157},{\"end\":29084,\"start\":28575},{\"end\":29130,\"start\":29110},{\"end\":29424,\"start\":29149},{\"end\":29647,\"start\":29426},{\"end\":29933,\"start\":29649},{\"end\":30776,\"start\":29935},{\"end\":31224,\"start\":30778},{\"end\":32069,\"start\":31239},{\"end\":32140,\"start\":32071},{\"end\":32549,\"start\":32231},{\"end\":33112,\"start\":32551},{\"end\":33559,\"start\":33140},{\"end\":34220,\"start\":33641},{\"end\":34673,\"start\":34235},{\"end\":34849,\"start\":34707},{\"end\":35344,\"start\":34851},{\"end\":35474,\"start\":35388},{\"end\":35700,\"start\":35539},{\"end\":35754,\"start\":35702},{\"end\":35819,\"start\":35756},{\"end\":35902,\"start\":35821},{\"end\":36024,\"start\":35904},{\"end\":36704,\"start\":36026},{\"end\":37291,\"start\":36974},{\"end\":37388,\"start\":37293},{\"end\":38006,\"start\":37390},{\"end\":38826,\"start\":38047},{\"end\":39634,\"start\":38866},{\"end\":39865,\"start\":39710},{\"end\":40068,\"start\":39891}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14948,\"start\":14772},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17527,\"start\":17464},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19116,\"start\":19074},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19335,\"start\":19311},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19604,\"start\":19575},{\"attributes\":{\"id\":\"formula_5\"},\"end\":32230,\"start\":32141},{\"attributes\":{\"id\":\"formula_6\"},\"end\":33139,\"start\":33113},{\"attributes\":{\"id\":\"formula_7\"},\"end\":33640,\"start\":33560},{\"attributes\":{\"id\":\"formula_8\"},\"end\":34706,\"start\":34674},{\"attributes\":{\"id\":\"formula_9\"},\"end\":35538,\"start\":35475},{\"attributes\":{\"id\":\"formula_10\"},\"end\":36914,\"start\":36705}]", "table_ref": "[{\"end\":25058,\"start\":25051},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27401,\"start\":27394},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":28594,\"start\":28587},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30282,\"start\":30275},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":36153,\"start\":36146},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":37512,\"start\":37505},{\"end\":39806,\"start\":39799}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1710,\"start\":1698},{\"attributes\":{\"n\":\"2.\"},\"end\":8413,\"start\":8401},{\"attributes\":{\"n\":\"3.\"},\"end\":10927,\"start\":10917},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10938,\"start\":10930},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11596,\"start\":11564},{\"end\":14771,\"start\":14741},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16273,\"start\":16248},{\"end\":19878,\"start\":19852},{\"attributes\":{\"n\":\"3.4.\"},\"end\":21550,\"start\":21536},{\"attributes\":{\"n\":\"4.\"},\"end\":22242,\"start\":22231},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22274,\"start\":22245},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24067,\"start\":24043},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25965,\"start\":25930},{\"end\":29108,\"start\":29087},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29147,\"start\":29133},{\"attributes\":{\"n\":\"5.\"},\"end\":31237,\"start\":31227},{\"end\":34233,\"start\":34223},{\"end\":35386,\"start\":35347},{\"end\":36972,\"start\":36916},{\"end\":38045,\"start\":38009},{\"end\":38864,\"start\":38829},{\"end\":39675,\"start\":39637},{\"end\":39708,\"start\":39678},{\"end\":39889,\"start\":39868},{\"end\":40080,\"start\":40070},{\"end\":40254,\"start\":40244},{\"end\":40490,\"start\":40480},{\"end\":40640,\"start\":40630},{\"end\":40816,\"start\":40806},{\"end\":41132,\"start\":41124},{\"end\":41598,\"start\":41593},{\"end\":41962,\"start\":41952},{\"end\":42226,\"start\":42216},{\"end\":42564,\"start\":42553},{\"end\":43048,\"start\":43041},{\"end\":45787,\"start\":45778},{\"end\":46024,\"start\":46015},{\"end\":46186,\"start\":46177},{\"end\":46272,\"start\":46263},{\"end\":46366,\"start\":46357}]", "table": "[{\"end\":43039,\"start\":42683},{\"end\":45776,\"start\":43359},{\"end\":46013,\"start\":46002},{\"end\":46175,\"start\":46026}]", "figure_caption": "[{\"end\":40242,\"start\":40082},{\"end\":40478,\"start\":40256},{\"end\":40628,\"start\":40492},{\"end\":40804,\"start\":40642},{\"end\":41122,\"start\":40818},{\"end\":41591,\"start\":41134},{\"end\":41950,\"start\":41599},{\"end\":42214,\"start\":41964},{\"end\":42551,\"start\":42228},{\"end\":42679,\"start\":42567},{\"end\":42683,\"start\":42682},{\"end\":43359,\"start\":43050},{\"end\":46002,\"start\":45789},{\"end\":46261,\"start\":46188},{\"end\":46355,\"start\":46274},{\"end\":46466,\"start\":46368}]", "figure_ref": "[{\"end\":7824,\"start\":7816},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10966,\"start\":10958},{\"end\":14342,\"start\":14334},{\"end\":16295,\"start\":16287},{\"end\":20169,\"start\":20161},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20432,\"start\":20424},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23231,\"start\":23223},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27597,\"start\":27589},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30504,\"start\":30495},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38451,\"start\":38443},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":38970,\"start\":38962},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":38981,\"start\":38972},{\"end\":39817,\"start\":39808}]", "bib_author_first_name": "[{\"end\":48939,\"start\":48931},{\"end\":48954,\"start\":48946},{\"end\":48960,\"start\":48955},{\"end\":48973,\"start\":48968},{\"end\":49193,\"start\":49190},{\"end\":49207,\"start\":49202},{\"end\":49219,\"start\":49213},{\"end\":49465,\"start\":49461},{\"end\":49480,\"start\":49474},{\"end\":49496,\"start\":49490},{\"end\":49850,\"start\":49841},{\"end\":49867,\"start\":49859},{\"end\":49870,\"start\":49868},{\"end\":49884,\"start\":49877},{\"end\":50178,\"start\":50174},{\"end\":50192,\"start\":50185},{\"end\":50203,\"start\":50197},{\"end\":50216,\"start\":50208},{\"end\":50227,\"start\":50223},{\"end\":50239,\"start\":50234},{\"end\":50539,\"start\":50531},{\"end\":50553,\"start\":50546},{\"end\":50567,\"start\":50558},{\"end\":50582,\"start\":50573},{\"end\":50593,\"start\":50590},{\"end\":50609,\"start\":50600},{\"end\":50962,\"start\":50956},{\"end\":50975,\"start\":50969},{\"end\":50985,\"start\":50981},{\"end\":50996,\"start\":50991},{\"end\":51264,\"start\":51257},{\"end\":51277,\"start\":51271},{\"end\":51284,\"start\":51282},{\"end\":51292,\"start\":51290},{\"end\":51301,\"start\":51297},{\"end\":51469,\"start\":51464},{\"end\":51479,\"start\":51476},{\"end\":51493,\"start\":51485},{\"end\":51505,\"start\":51500},{\"end\":51683,\"start\":51672},{\"end\":51698,\"start\":51690},{\"end\":51711,\"start\":51705},{\"end\":51938,\"start\":51934},{\"end\":51953,\"start\":51950},{\"end\":52096,\"start\":52092},{\"end\":52109,\"start\":52105},{\"end\":52120,\"start\":52116},{\"end\":52295,\"start\":52288},{\"end\":52315,\"start\":52307},{\"end\":52336,\"start\":52329},{\"end\":52581,\"start\":52573},{\"end\":52596,\"start\":52590},{\"end\":52614,\"start\":52607},{\"end\":52868,\"start\":52862},{\"end\":52882,\"start\":52875},{\"end\":52903,\"start\":52896},{\"end\":52907,\"start\":52904},{\"end\":53080,\"start\":53075},{\"end\":53092,\"start\":53086},{\"end\":53107,\"start\":53099},{\"end\":53115,\"start\":53112},{\"end\":53123,\"start\":53121},{\"end\":53137,\"start\":53129},{\"end\":53391,\"start\":53387},{\"end\":53407,\"start\":53400},{\"end\":53422,\"start\":53417},{\"end\":53435,\"start\":53432},{\"end\":53437,\"start\":53436},{\"end\":53452,\"start\":53446},{\"end\":53468,\"start\":53464},{\"end\":53830,\"start\":53826},{\"end\":53842,\"start\":53839},{\"end\":53844,\"start\":53843},{\"end\":53859,\"start\":53853},{\"end\":54075,\"start\":54068},{\"end\":54087,\"start\":54080},{\"end\":54103,\"start\":54095},{\"end\":54113,\"start\":54109},{\"end\":54302,\"start\":54293},{\"end\":54315,\"start\":54308},{\"end\":54515,\"start\":54508},{\"end\":54529,\"start\":54523},{\"end\":54542,\"start\":54536},{\"end\":54771,\"start\":54769},{\"end\":54789,\"start\":54779},{\"end\":54799,\"start\":54796},{\"end\":54813,\"start\":54805},{\"end\":54828,\"start\":54820},{\"end\":54838,\"start\":54833},{\"end\":55098,\"start\":55093},{\"end\":55117,\"start\":55111},{\"end\":55129,\"start\":55125},{\"end\":55341,\"start\":55336},{\"end\":55353,\"start\":55347},{\"end\":55362,\"start\":55358},{\"end\":55374,\"start\":55367},{\"end\":55631,\"start\":55627},{\"end\":55646,\"start\":55642},{\"end\":55660,\"start\":55655},{\"end\":55676,\"start\":55669},{\"end\":55691,\"start\":55685},{\"end\":55972,\"start\":55968},{\"end\":55989,\"start\":55983},{\"end\":56205,\"start\":56199},{\"end\":56219,\"start\":56212},{\"end\":56232,\"start\":56226},{\"end\":56247,\"start\":56241},{\"end\":56259,\"start\":56254},{\"end\":56271,\"start\":56266},{\"end\":56515,\"start\":56511},{\"end\":56522,\"start\":56520},{\"end\":56692,\"start\":56688},{\"end\":56704,\"start\":56698},{\"end\":56718,\"start\":56713},{\"end\":56919,\"start\":56917},{\"end\":56931,\"start\":56924},{\"end\":56943,\"start\":56939},{\"end\":57126,\"start\":57120},{\"end\":57139,\"start\":57131},{\"end\":57151,\"start\":57148},{\"end\":57167,\"start\":57160},{\"end\":57362,\"start\":57356},{\"end\":57368,\"start\":57367},{\"end\":57381,\"start\":57374},{\"end\":57567,\"start\":57560},{\"end\":57575,\"start\":57572},{\"end\":57588,\"start\":57580},{\"end\":57597,\"start\":57594},{\"end\":57608,\"start\":57602},{\"end\":57620,\"start\":57613},{\"end\":57851,\"start\":57844},{\"end\":57865,\"start\":57857},{\"end\":57877,\"start\":57870},{\"end\":57891,\"start\":57882},{\"end\":57900,\"start\":57898},{\"end\":58132,\"start\":58126},{\"end\":58146,\"start\":58138},{\"end\":58161,\"start\":58152},{\"end\":58379,\"start\":58370},{\"end\":58628,\"start\":58621},{\"end\":58641,\"start\":58634},{\"end\":58653,\"start\":58648},{\"end\":58663,\"start\":58659},{\"end\":58875,\"start\":58868},{\"end\":58889,\"start\":58881},{\"end\":58905,\"start\":58896},{\"end\":59136,\"start\":59128},{\"end\":59146,\"start\":59143},{\"end\":59158,\"start\":59152},{\"end\":59170,\"start\":59164},{\"end\":59399,\"start\":59393},{\"end\":59416,\"start\":59409},{\"end\":59428,\"start\":59424},{\"end\":59443,\"start\":59437},{\"end\":59608,\"start\":59602},{\"end\":59620,\"start\":59615},{\"end\":59921,\"start\":59915},{\"end\":59936,\"start\":59931},{\"end\":59954,\"start\":59945},{\"end\":59970,\"start\":59961},{\"end\":60214,\"start\":60211},{\"end\":60236,\"start\":60229},{\"end\":60249,\"start\":60241},{\"end\":60251,\"start\":60250},{\"end\":60503,\"start\":60501},{\"end\":60531,\"start\":60528},{\"end\":60544,\"start\":60536},{\"end\":60546,\"start\":60545},{\"end\":60799,\"start\":60794},{\"end\":60816,\"start\":60809},{\"end\":60829,\"start\":60823},{\"end\":60843,\"start\":60837},{\"end\":60859,\"start\":60851},{\"end\":61114,\"start\":61108},{\"end\":61137,\"start\":61124},{\"end\":61156,\"start\":61148},{\"end\":61437,\"start\":61433},{\"end\":61463,\"start\":61456},{\"end\":61690,\"start\":61686},{\"end\":61701,\"start\":61697},{\"end\":61715,\"start\":61708},{\"end\":61726,\"start\":61722},{\"end\":61951,\"start\":61947},{\"end\":61961,\"start\":61956},{\"end\":61977,\"start\":61971},{\"end\":61992,\"start\":61983},{\"end\":62008,\"start\":61999},{\"end\":62032,\"start\":62022},{\"end\":62042,\"start\":62039},{\"end\":62324,\"start\":62323},{\"end\":62339,\"start\":62334},{\"end\":62341,\"start\":62340},{\"end\":62363,\"start\":62362},{\"end\":62380,\"start\":62374},{\"end\":62635,\"start\":62630},{\"end\":62655,\"start\":62649},{\"end\":62669,\"start\":62662},{\"end\":62685,\"start\":62678},{\"end\":62886,\"start\":62882},{\"end\":62907,\"start\":62896},{\"end\":62917,\"start\":62914},{\"end\":62934,\"start\":62926},{\"end\":62947,\"start\":62941},{\"end\":63198,\"start\":63192},{\"end\":63215,\"start\":63207},{\"end\":63239,\"start\":63226},{\"end\":63257,\"start\":63250},{\"end\":63522,\"start\":63516},{\"end\":63532,\"start\":63531},{\"end\":63555,\"start\":63542},{\"end\":63567,\"start\":63560},{\"end\":63586,\"start\":63578},{\"end\":63607,\"start\":63599},{\"end\":63609,\"start\":63608},{\"end\":63895,\"start\":63892},{\"end\":63907,\"start\":63902},{\"end\":63922,\"start\":63916},{\"end\":64135,\"start\":64132},{\"end\":64148,\"start\":64142},{\"end\":64162,\"start\":64156},{\"end\":64175,\"start\":64168},{\"end\":64186,\"start\":64183},{\"end\":64400,\"start\":64397},{\"end\":64414,\"start\":64407},{\"end\":64425,\"start\":64420},{\"end\":64437,\"start\":64431},{\"end\":64439,\"start\":64438},{\"end\":64454,\"start\":64447},{\"end\":64456,\"start\":64455},{\"end\":64474,\"start\":64468},{\"end\":64476,\"start\":64475},{\"end\":64832,\"start\":64826},{\"end\":64842,\"start\":64837},{\"end\":64855,\"start\":64848},{\"end\":64865,\"start\":64861},{\"end\":65211,\"start\":65205},{\"end\":65222,\"start\":65216},{\"end\":65236,\"start\":65229},{\"end\":65250,\"start\":65243},{\"end\":65260,\"start\":65256},{\"end\":65558,\"start\":65551},{\"end\":65571,\"start\":65563},{\"end\":65578,\"start\":65576},{\"end\":65770,\"start\":65763},{\"end\":65782,\"start\":65776},{\"end\":65792,\"start\":65788},{\"end\":65806,\"start\":65799},{\"end\":66059,\"start\":66053},{\"end\":66069,\"start\":66064},{\"end\":66078,\"start\":66074},{\"end\":66095,\"start\":66086},{\"end\":66106,\"start\":66101},{\"end\":66124,\"start\":66118},{\"end\":66416,\"start\":66413},{\"end\":66429,\"start\":66423},{\"end\":66441,\"start\":66435},{\"end\":66651,\"start\":66649},{\"end\":66664,\"start\":66658},{\"end\":66677,\"start\":66671},{\"end\":66693,\"start\":66685},{\"end\":66701,\"start\":66698},{\"end\":66714,\"start\":66708},{\"end\":66728,\"start\":66724},{\"end\":67025,\"start\":67023},{\"end\":67035,\"start\":67032},{\"end\":67048,\"start\":67042},{\"end\":67062,\"start\":67058},{\"end\":67308,\"start\":67299},{\"end\":67320,\"start\":67315},{\"end\":67336,\"start\":67328},{\"end\":67347,\"start\":67341},{\"end\":67359,\"start\":67352},{\"end\":67372,\"start\":67365},{\"end\":67381,\"start\":67379},{\"end\":67669,\"start\":67661},{\"end\":67680,\"start\":67674},{\"end\":67691,\"start\":67685},{\"end\":67704,\"start\":67699},{\"end\":67716,\"start\":67709},{\"end\":68006,\"start\":67999},{\"end\":68021,\"start\":68014},{\"end\":68281,\"start\":68274},{\"end\":68306,\"start\":68299},{\"end\":68577,\"start\":68567},{\"end\":68586,\"start\":68584},{\"end\":68602,\"start\":68594},{\"end\":68612,\"start\":68607}]", "bib_author_last_name": "[{\"end\":48944,\"start\":48940},{\"end\":48966,\"start\":48961},{\"end\":48977,\"start\":48974},{\"end\":49200,\"start\":49194},{\"end\":49211,\"start\":49208},{\"end\":49232,\"start\":49220},{\"end\":49242,\"start\":49234},{\"end\":49472,\"start\":49466},{\"end\":49488,\"start\":49481},{\"end\":49504,\"start\":49497},{\"end\":49857,\"start\":49851},{\"end\":49875,\"start\":49871},{\"end\":49893,\"start\":49885},{\"end\":50183,\"start\":50179},{\"end\":50195,\"start\":50193},{\"end\":50206,\"start\":50204},{\"end\":50221,\"start\":50217},{\"end\":50232,\"start\":50228},{\"end\":50243,\"start\":50240},{\"end\":50544,\"start\":50540},{\"end\":50556,\"start\":50554},{\"end\":50571,\"start\":50568},{\"end\":50588,\"start\":50583},{\"end\":50598,\"start\":50594},{\"end\":50612,\"start\":50610},{\"end\":50967,\"start\":50963},{\"end\":50979,\"start\":50976},{\"end\":50989,\"start\":50986},{\"end\":51000,\"start\":50997},{\"end\":51269,\"start\":51265},{\"end\":51280,\"start\":51278},{\"end\":51288,\"start\":51285},{\"end\":51295,\"start\":51293},{\"end\":51305,\"start\":51302},{\"end\":51474,\"start\":51470},{\"end\":51483,\"start\":51480},{\"end\":51498,\"start\":51494},{\"end\":51509,\"start\":51506},{\"end\":51688,\"start\":51684},{\"end\":51703,\"start\":51699},{\"end\":51720,\"start\":51712},{\"end\":51948,\"start\":51939},{\"end\":51960,\"start\":51954},{\"end\":52103,\"start\":52097},{\"end\":52114,\"start\":52110},{\"end\":52127,\"start\":52121},{\"end\":52305,\"start\":52296},{\"end\":52327,\"start\":52316},{\"end\":52342,\"start\":52337},{\"end\":52588,\"start\":52582},{\"end\":52605,\"start\":52597},{\"end\":52629,\"start\":52615},{\"end\":52873,\"start\":52869},{\"end\":52894,\"start\":52883},{\"end\":52914,\"start\":52908},{\"end\":53084,\"start\":53081},{\"end\":53097,\"start\":53093},{\"end\":53110,\"start\":53108},{\"end\":53119,\"start\":53116},{\"end\":53127,\"start\":53124},{\"end\":53147,\"start\":53138},{\"end\":53398,\"start\":53392},{\"end\":53415,\"start\":53408},{\"end\":53430,\"start\":53423},{\"end\":53444,\"start\":53438},{\"end\":53462,\"start\":53453},{\"end\":53478,\"start\":53469},{\"end\":53837,\"start\":53831},{\"end\":53851,\"start\":53845},{\"end\":53869,\"start\":53860},{\"end\":54078,\"start\":54076},{\"end\":54093,\"start\":54088},{\"end\":54107,\"start\":54104},{\"end\":54117,\"start\":54114},{\"end\":54291,\"start\":54283},{\"end\":54306,\"start\":54303},{\"end\":54320,\"start\":54316},{\"end\":54327,\"start\":54322},{\"end\":54521,\"start\":54516},{\"end\":54534,\"start\":54530},{\"end\":54550,\"start\":54543},{\"end\":54777,\"start\":54772},{\"end\":54794,\"start\":54790},{\"end\":54803,\"start\":54800},{\"end\":54818,\"start\":54814},{\"end\":54831,\"start\":54829},{\"end\":54842,\"start\":54839},{\"end\":55109,\"start\":55099},{\"end\":55123,\"start\":55118},{\"end\":55133,\"start\":55130},{\"end\":55345,\"start\":55342},{\"end\":55356,\"start\":55354},{\"end\":55365,\"start\":55363},{\"end\":55380,\"start\":55375},{\"end\":55640,\"start\":55632},{\"end\":55653,\"start\":55647},{\"end\":55667,\"start\":55661},{\"end\":55683,\"start\":55677},{\"end\":55700,\"start\":55692},{\"end\":55981,\"start\":55973},{\"end\":56000,\"start\":55990},{\"end\":56210,\"start\":56206},{\"end\":56224,\"start\":56220},{\"end\":56239,\"start\":56233},{\"end\":56252,\"start\":56248},{\"end\":56264,\"start\":56260},{\"end\":56279,\"start\":56272},{\"end\":56518,\"start\":56516},{\"end\":56527,\"start\":56523},{\"end\":56696,\"start\":56693},{\"end\":56711,\"start\":56705},{\"end\":56723,\"start\":56719},{\"end\":56922,\"start\":56920},{\"end\":56937,\"start\":56932},{\"end\":56947,\"start\":56944},{\"end\":57129,\"start\":57127},{\"end\":57146,\"start\":57140},{\"end\":57158,\"start\":57152},{\"end\":57174,\"start\":57168},{\"end\":57365,\"start\":57363},{\"end\":57372,\"start\":57369},{\"end\":57386,\"start\":57382},{\"end\":57391,\"start\":57388},{\"end\":57570,\"start\":57568},{\"end\":57578,\"start\":57576},{\"end\":57592,\"start\":57589},{\"end\":57600,\"start\":57598},{\"end\":57611,\"start\":57609},{\"end\":57625,\"start\":57621},{\"end\":57855,\"start\":57852},{\"end\":57868,\"start\":57866},{\"end\":57880,\"start\":57878},{\"end\":57896,\"start\":57892},{\"end\":57905,\"start\":57901},{\"end\":58136,\"start\":58133},{\"end\":58150,\"start\":58147},{\"end\":58166,\"start\":58162},{\"end\":58383,\"start\":58380},{\"end\":58632,\"start\":58629},{\"end\":58646,\"start\":58642},{\"end\":58657,\"start\":58654},{\"end\":58667,\"start\":58664},{\"end\":58879,\"start\":58876},{\"end\":58894,\"start\":58890},{\"end\":58908,\"start\":58906},{\"end\":59141,\"start\":59137},{\"end\":59150,\"start\":59147},{\"end\":59162,\"start\":59159},{\"end\":59178,\"start\":59171},{\"end\":59407,\"start\":59400},{\"end\":59422,\"start\":59417},{\"end\":59435,\"start\":59429},{\"end\":59453,\"start\":59444},{\"end\":59613,\"start\":59609},{\"end\":59627,\"start\":59621},{\"end\":59929,\"start\":59922},{\"end\":59943,\"start\":59937},{\"end\":59959,\"start\":59955},{\"end\":59978,\"start\":59971},{\"end\":60227,\"start\":60215},{\"end\":60239,\"start\":60237},{\"end\":60254,\"start\":60252},{\"end\":60262,\"start\":60256},{\"end\":60526,\"start\":60504},{\"end\":60534,\"start\":60532},{\"end\":60549,\"start\":60547},{\"end\":60557,\"start\":60551},{\"end\":60807,\"start\":60800},{\"end\":60821,\"start\":60817},{\"end\":60835,\"start\":60830},{\"end\":60849,\"start\":60844},{\"end\":60867,\"start\":60860},{\"end\":61122,\"start\":61115},{\"end\":61146,\"start\":61138},{\"end\":61165,\"start\":61157},{\"end\":61454,\"start\":61438},{\"end\":61470,\"start\":61464},{\"end\":61477,\"start\":61472},{\"end\":61695,\"start\":61691},{\"end\":61706,\"start\":61702},{\"end\":61720,\"start\":61716},{\"end\":61731,\"start\":61727},{\"end\":61954,\"start\":61952},{\"end\":61969,\"start\":61962},{\"end\":61981,\"start\":61978},{\"end\":61997,\"start\":61993},{\"end\":62020,\"start\":62009},{\"end\":62037,\"start\":62033},{\"end\":62048,\"start\":62043},{\"end\":62332,\"start\":62325},{\"end\":62348,\"start\":62342},{\"end\":62360,\"start\":62350},{\"end\":62372,\"start\":62364},{\"end\":62386,\"start\":62381},{\"end\":62395,\"start\":62388},{\"end\":62647,\"start\":62636},{\"end\":62660,\"start\":62656},{\"end\":62676,\"start\":62670},{\"end\":62690,\"start\":62686},{\"end\":62894,\"start\":62887},{\"end\":62912,\"start\":62908},{\"end\":62924,\"start\":62918},{\"end\":62939,\"start\":62935},{\"end\":62956,\"start\":62948},{\"end\":63205,\"start\":63199},{\"end\":63224,\"start\":63216},{\"end\":63248,\"start\":63240},{\"end\":63268,\"start\":63258},{\"end\":63529,\"start\":63523},{\"end\":63540,\"start\":63533},{\"end\":63558,\"start\":63556},{\"end\":63576,\"start\":63568},{\"end\":63597,\"start\":63587},{\"end\":63618,\"start\":63610},{\"end\":63626,\"start\":63620},{\"end\":63900,\"start\":63896},{\"end\":63914,\"start\":63908},{\"end\":63930,\"start\":63923},{\"end\":64140,\"start\":64136},{\"end\":64154,\"start\":64149},{\"end\":64166,\"start\":64163},{\"end\":64181,\"start\":64176},{\"end\":64191,\"start\":64187},{\"end\":64405,\"start\":64401},{\"end\":64418,\"start\":64415},{\"end\":64429,\"start\":64426},{\"end\":64445,\"start\":64440},{\"end\":64466,\"start\":64457},{\"end\":64484,\"start\":64477},{\"end\":64835,\"start\":64833},{\"end\":64846,\"start\":64843},{\"end\":64859,\"start\":64856},{\"end\":64873,\"start\":64866},{\"end\":65214,\"start\":65212},{\"end\":65227,\"start\":65223},{\"end\":65241,\"start\":65237},{\"end\":65254,\"start\":65251},{\"end\":65268,\"start\":65261},{\"end\":65561,\"start\":65559},{\"end\":65574,\"start\":65572},{\"end\":65584,\"start\":65579},{\"end\":65774,\"start\":65771},{\"end\":65786,\"start\":65783},{\"end\":65797,\"start\":65793},{\"end\":65809,\"start\":65807},{\"end\":66062,\"start\":66060},{\"end\":66072,\"start\":66070},{\"end\":66084,\"start\":66079},{\"end\":66099,\"start\":66096},{\"end\":66116,\"start\":66107},{\"end\":66137,\"start\":66125},{\"end\":66421,\"start\":66417},{\"end\":66433,\"start\":66430},{\"end\":66449,\"start\":66442},{\"end\":66656,\"start\":66652},{\"end\":66669,\"start\":66665},{\"end\":66683,\"start\":66678},{\"end\":66696,\"start\":66694},{\"end\":66706,\"start\":66702},{\"end\":66722,\"start\":66715},{\"end\":66736,\"start\":66729},{\"end\":67030,\"start\":67026},{\"end\":67040,\"start\":67036},{\"end\":67056,\"start\":67049},{\"end\":67070,\"start\":67063},{\"end\":67313,\"start\":67309},{\"end\":67326,\"start\":67321},{\"end\":67339,\"start\":67337},{\"end\":67350,\"start\":67348},{\"end\":67363,\"start\":67360},{\"end\":67377,\"start\":67373},{\"end\":67386,\"start\":67382},{\"end\":67672,\"start\":67670},{\"end\":67683,\"start\":67681},{\"end\":67697,\"start\":67692},{\"end\":67707,\"start\":67705},{\"end\":67722,\"start\":67717},{\"end\":68012,\"start\":68007},{\"end\":68026,\"start\":68022},{\"end\":68287,\"start\":68282},{\"end\":68297,\"start\":68289},{\"end\":68310,\"start\":68307},{\"end\":68317,\"start\":68312},{\"end\":68582,\"start\":68578},{\"end\":68592,\"start\":68587},{\"end\":68605,\"start\":68603},{\"end\":68616,\"start\":68613}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":59316873},\"end\":49130,\"start\":48852},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2730848},\"end\":49380,\"start\":49132},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":199441943},\"end\":49762,\"start\":49382},{\"attributes\":{\"doi\":\"3DOR\",\"id\":\"b3\"},\"end\":50050,\"start\":49764},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":150368751},\"end\":50462,\"start\":50052},{\"attributes\":{\"doi\":\"arXiv:1905.05442\",\"id\":\"b5\"},\"end\":50836,\"start\":50464},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195699956},\"end\":51192,\"start\":50838},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":707161},\"end\":51444,\"start\":51194},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":199501855},\"end\":51600,\"start\":51446},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":121123422},\"end\":51866,\"start\":51602},{\"attributes\":{\"id\":\"b10\"},\"end\":52070,\"start\":51868},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":54458618},\"end\":52212,\"start\":52072},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":198968335},\"end\":52498,\"start\":52214},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10154243},\"end\":52786,\"start\":52500},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53029867},\"end\":53073,\"start\":52788},{\"attributes\":{\"doi\":\"arXiv:1912.12033\",\"id\":\"b15\"},\"end\":53385,\"start\":53075},{\"attributes\":{\"id\":\"b16\"},\"end\":53747,\"start\":53387},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52061886},\"end\":54020,\"start\":53749},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":54240,\"start\":54022},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4445385},\"end\":54444,\"start\":54242},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3603383},\"end\":54684,\"start\":54446},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":202237019},\"end\":55027,\"start\":54686},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":119185126},\"end\":55265,\"start\":55029},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53733117},\"end\":55524,\"start\":55267},{\"attributes\":{\"id\":\"b24\"},\"end\":55896,\"start\":55526},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4396837},\"end\":56129,\"start\":55898},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":55701967},\"end\":56455,\"start\":56131},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52829846},\"end\":56624,\"start\":56457},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":67855534},\"end\":56848,\"start\":56626},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15201663},\"end\":57070,\"start\":56850},{\"attributes\":{\"id\":\"b30\"},\"end\":57297,\"start\":57072},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3934562},\"end\":57512,\"start\":57299},{\"attributes\":{\"id\":\"b32\"},\"end\":57775,\"start\":57514},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":208005024},\"end\":58061,\"start\":57777},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":204800955},\"end\":58298,\"start\":58063},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":118621697},\"end\":58572,\"start\":58300},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":195833620},\"end\":58796,\"start\":58574},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":199551973},\"end\":59050,\"start\":58798},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":53279704},\"end\":59332,\"start\":59052},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":199478000},\"end\":59600,\"start\":59334},{\"attributes\":{\"doi\":\"arXiv:1402.0030\",\"id\":\"b40\"},\"end\":59851,\"start\":59602},{\"attributes\":{\"id\":\"b41\"},\"end\":60131,\"start\":59853},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5115938},\"end\":60419,\"start\":60133},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1745976},\"end\":60727,\"start\":60421},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":52059096},\"end\":61033,\"start\":60729},{\"attributes\":{\"doi\":\"arXiv:1804.03583\",\"id\":\"b45\"},\"end\":61373,\"start\":61035},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15022990},\"end\":61607,\"start\":61375},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":4578850},\"end\":61883,\"start\":61609},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3525655},\"end\":62241,\"start\":61885},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1211821},\"end\":62579,\"start\":62243},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4592390},\"end\":62828,\"start\":62581},{\"attributes\":{\"id\":\"b51\"},\"end\":63106,\"start\":62830},{\"attributes\":{\"id\":\"b52\"},\"end\":63452,\"start\":63108},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":121328056},\"end\":63825,\"start\":63454},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3847166},\"end\":64063,\"start\":63827},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":198164185},\"end\":64347,\"start\":64065},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":94822},\"end\":64701,\"start\":64349},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":206853127},\"end\":65074,\"start\":64703},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":52815788},\"end\":65490,\"start\":65076},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":53720607},\"end\":65704,\"start\":65492},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":4486619},\"end\":65942,\"start\":65706},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":1055111},\"end\":66356,\"start\":65944},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":52238978},\"end\":66570,\"start\":66358},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":174798057},\"end\":66936,\"start\":66572},{\"attributes\":{\"id\":\"b64\"},\"end\":67227,\"start\":66938},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":102351184},\"end\":67571,\"start\":67229},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":52955695},\"end\":67903,\"start\":67573},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":128001892},\"end\":68174,\"start\":67905},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":201070399},\"end\":68489,\"start\":68176},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":199012203},\"end\":68769,\"start\":68491},{\"attributes\":{\"id\":\"b70\"},\"end\":68958,\"start\":68771},{\"attributes\":{\"id\":\"b71\"},\"end\":69364,\"start\":68960},{\"attributes\":{\"id\":\"b72\"},\"end\":69735,\"start\":69366}]", "bib_title": "[{\"end\":48929,\"start\":48852},{\"end\":49188,\"start\":49132},{\"end\":49459,\"start\":49382},{\"end\":50172,\"start\":50052},{\"end\":50954,\"start\":50838},{\"end\":51255,\"start\":51194},{\"end\":51462,\"start\":51446},{\"end\":51670,\"start\":51602},{\"end\":52090,\"start\":52072},{\"end\":52286,\"start\":52214},{\"end\":52571,\"start\":52500},{\"end\":52860,\"start\":52788},{\"end\":53824,\"start\":53749},{\"end\":54066,\"start\":54022},{\"end\":54281,\"start\":54242},{\"end\":54506,\"start\":54446},{\"end\":54767,\"start\":54686},{\"end\":55091,\"start\":55029},{\"end\":55334,\"start\":55267},{\"end\":55966,\"start\":55898},{\"end\":56197,\"start\":56131},{\"end\":56509,\"start\":56457},{\"end\":56686,\"start\":56626},{\"end\":56915,\"start\":56850},{\"end\":57354,\"start\":57299},{\"end\":57558,\"start\":57514},{\"end\":57842,\"start\":57777},{\"end\":58124,\"start\":58063},{\"end\":58368,\"start\":58300},{\"end\":58619,\"start\":58574},{\"end\":58866,\"start\":58798},{\"end\":59126,\"start\":59052},{\"end\":59391,\"start\":59334},{\"end\":60209,\"start\":60133},{\"end\":60499,\"start\":60421},{\"end\":60792,\"start\":60729},{\"end\":61431,\"start\":61375},{\"end\":61684,\"start\":61609},{\"end\":61945,\"start\":61885},{\"end\":62321,\"start\":62243},{\"end\":62628,\"start\":62581},{\"end\":63514,\"start\":63454},{\"end\":63890,\"start\":63827},{\"end\":64130,\"start\":64065},{\"end\":64395,\"start\":64349},{\"end\":64824,\"start\":64703},{\"end\":65203,\"start\":65076},{\"end\":65549,\"start\":65492},{\"end\":65761,\"start\":65706},{\"end\":66051,\"start\":65944},{\"end\":66411,\"start\":66358},{\"end\":66647,\"start\":66572},{\"end\":67297,\"start\":67229},{\"end\":67659,\"start\":67573},{\"end\":67997,\"start\":67905},{\"end\":68272,\"start\":68176},{\"end\":68565,\"start\":68491}]", "bib_author": "[{\"end\":48946,\"start\":48931},{\"end\":48968,\"start\":48946},{\"end\":48979,\"start\":48968},{\"end\":49202,\"start\":49190},{\"end\":49213,\"start\":49202},{\"end\":49234,\"start\":49213},{\"end\":49244,\"start\":49234},{\"end\":49474,\"start\":49461},{\"end\":49490,\"start\":49474},{\"end\":49506,\"start\":49490},{\"end\":49859,\"start\":49841},{\"end\":49877,\"start\":49859},{\"end\":49895,\"start\":49877},{\"end\":50185,\"start\":50174},{\"end\":50197,\"start\":50185},{\"end\":50208,\"start\":50197},{\"end\":50223,\"start\":50208},{\"end\":50234,\"start\":50223},{\"end\":50245,\"start\":50234},{\"end\":50546,\"start\":50531},{\"end\":50558,\"start\":50546},{\"end\":50573,\"start\":50558},{\"end\":50590,\"start\":50573},{\"end\":50600,\"start\":50590},{\"end\":50614,\"start\":50600},{\"end\":50969,\"start\":50956},{\"end\":50981,\"start\":50969},{\"end\":50991,\"start\":50981},{\"end\":51002,\"start\":50991},{\"end\":51271,\"start\":51257},{\"end\":51282,\"start\":51271},{\"end\":51290,\"start\":51282},{\"end\":51297,\"start\":51290},{\"end\":51307,\"start\":51297},{\"end\":51476,\"start\":51464},{\"end\":51485,\"start\":51476},{\"end\":51500,\"start\":51485},{\"end\":51511,\"start\":51500},{\"end\":51690,\"start\":51672},{\"end\":51705,\"start\":51690},{\"end\":51722,\"start\":51705},{\"end\":51950,\"start\":51934},{\"end\":51962,\"start\":51950},{\"end\":52105,\"start\":52092},{\"end\":52116,\"start\":52105},{\"end\":52129,\"start\":52116},{\"end\":52307,\"start\":52288},{\"end\":52329,\"start\":52307},{\"end\":52344,\"start\":52329},{\"end\":52590,\"start\":52573},{\"end\":52607,\"start\":52590},{\"end\":52631,\"start\":52607},{\"end\":52875,\"start\":52862},{\"end\":52896,\"start\":52875},{\"end\":52916,\"start\":52896},{\"end\":53086,\"start\":53075},{\"end\":53099,\"start\":53086},{\"end\":53112,\"start\":53099},{\"end\":53121,\"start\":53112},{\"end\":53129,\"start\":53121},{\"end\":53149,\"start\":53129},{\"end\":53400,\"start\":53387},{\"end\":53417,\"start\":53400},{\"end\":53432,\"start\":53417},{\"end\":53446,\"start\":53432},{\"end\":53464,\"start\":53446},{\"end\":53480,\"start\":53464},{\"end\":53839,\"start\":53826},{\"end\":53853,\"start\":53839},{\"end\":53871,\"start\":53853},{\"end\":54080,\"start\":54068},{\"end\":54095,\"start\":54080},{\"end\":54109,\"start\":54095},{\"end\":54119,\"start\":54109},{\"end\":54293,\"start\":54283},{\"end\":54308,\"start\":54293},{\"end\":54322,\"start\":54308},{\"end\":54329,\"start\":54322},{\"end\":54523,\"start\":54508},{\"end\":54536,\"start\":54523},{\"end\":54552,\"start\":54536},{\"end\":54779,\"start\":54769},{\"end\":54796,\"start\":54779},{\"end\":54805,\"start\":54796},{\"end\":54820,\"start\":54805},{\"end\":54833,\"start\":54820},{\"end\":54844,\"start\":54833},{\"end\":55111,\"start\":55093},{\"end\":55125,\"start\":55111},{\"end\":55135,\"start\":55125},{\"end\":55347,\"start\":55336},{\"end\":55358,\"start\":55347},{\"end\":55367,\"start\":55358},{\"end\":55382,\"start\":55367},{\"end\":55642,\"start\":55627},{\"end\":55655,\"start\":55642},{\"end\":55669,\"start\":55655},{\"end\":55685,\"start\":55669},{\"end\":55702,\"start\":55685},{\"end\":55983,\"start\":55968},{\"end\":56002,\"start\":55983},{\"end\":56212,\"start\":56199},{\"end\":56226,\"start\":56212},{\"end\":56241,\"start\":56226},{\"end\":56254,\"start\":56241},{\"end\":56266,\"start\":56254},{\"end\":56281,\"start\":56266},{\"end\":56520,\"start\":56511},{\"end\":56529,\"start\":56520},{\"end\":56698,\"start\":56688},{\"end\":56713,\"start\":56698},{\"end\":56725,\"start\":56713},{\"end\":56924,\"start\":56917},{\"end\":56939,\"start\":56924},{\"end\":56949,\"start\":56939},{\"end\":57131,\"start\":57120},{\"end\":57148,\"start\":57131},{\"end\":57160,\"start\":57148},{\"end\":57176,\"start\":57160},{\"end\":57367,\"start\":57356},{\"end\":57374,\"start\":57367},{\"end\":57388,\"start\":57374},{\"end\":57393,\"start\":57388},{\"end\":57572,\"start\":57560},{\"end\":57580,\"start\":57572},{\"end\":57594,\"start\":57580},{\"end\":57602,\"start\":57594},{\"end\":57613,\"start\":57602},{\"end\":57627,\"start\":57613},{\"end\":57857,\"start\":57844},{\"end\":57870,\"start\":57857},{\"end\":57882,\"start\":57870},{\"end\":57898,\"start\":57882},{\"end\":57907,\"start\":57898},{\"end\":58138,\"start\":58126},{\"end\":58152,\"start\":58138},{\"end\":58168,\"start\":58152},{\"end\":58385,\"start\":58370},{\"end\":58634,\"start\":58621},{\"end\":58648,\"start\":58634},{\"end\":58659,\"start\":58648},{\"end\":58669,\"start\":58659},{\"end\":58881,\"start\":58868},{\"end\":58896,\"start\":58881},{\"end\":58910,\"start\":58896},{\"end\":59143,\"start\":59128},{\"end\":59152,\"start\":59143},{\"end\":59164,\"start\":59152},{\"end\":59180,\"start\":59164},{\"end\":59409,\"start\":59393},{\"end\":59424,\"start\":59409},{\"end\":59437,\"start\":59424},{\"end\":59455,\"start\":59437},{\"end\":59615,\"start\":59602},{\"end\":59629,\"start\":59615},{\"end\":59931,\"start\":59915},{\"end\":59945,\"start\":59931},{\"end\":59961,\"start\":59945},{\"end\":59980,\"start\":59961},{\"end\":60229,\"start\":60211},{\"end\":60241,\"start\":60229},{\"end\":60256,\"start\":60241},{\"end\":60264,\"start\":60256},{\"end\":60528,\"start\":60501},{\"end\":60536,\"start\":60528},{\"end\":60551,\"start\":60536},{\"end\":60559,\"start\":60551},{\"end\":60809,\"start\":60794},{\"end\":60823,\"start\":60809},{\"end\":60837,\"start\":60823},{\"end\":60851,\"start\":60837},{\"end\":60869,\"start\":60851},{\"end\":61124,\"start\":61108},{\"end\":61148,\"start\":61124},{\"end\":61167,\"start\":61148},{\"end\":61456,\"start\":61433},{\"end\":61472,\"start\":61456},{\"end\":61479,\"start\":61472},{\"end\":61697,\"start\":61686},{\"end\":61708,\"start\":61697},{\"end\":61722,\"start\":61708},{\"end\":61733,\"start\":61722},{\"end\":61956,\"start\":61947},{\"end\":61971,\"start\":61956},{\"end\":61983,\"start\":61971},{\"end\":61999,\"start\":61983},{\"end\":62022,\"start\":61999},{\"end\":62039,\"start\":62022},{\"end\":62050,\"start\":62039},{\"end\":62334,\"start\":62323},{\"end\":62350,\"start\":62334},{\"end\":62362,\"start\":62350},{\"end\":62374,\"start\":62362},{\"end\":62388,\"start\":62374},{\"end\":62397,\"start\":62388},{\"end\":62649,\"start\":62630},{\"end\":62662,\"start\":62649},{\"end\":62678,\"start\":62662},{\"end\":62692,\"start\":62678},{\"end\":62896,\"start\":62882},{\"end\":62914,\"start\":62896},{\"end\":62926,\"start\":62914},{\"end\":62941,\"start\":62926},{\"end\":62958,\"start\":62941},{\"end\":63207,\"start\":63192},{\"end\":63226,\"start\":63207},{\"end\":63250,\"start\":63226},{\"end\":63270,\"start\":63250},{\"end\":63531,\"start\":63516},{\"end\":63542,\"start\":63531},{\"end\":63560,\"start\":63542},{\"end\":63578,\"start\":63560},{\"end\":63599,\"start\":63578},{\"end\":63620,\"start\":63599},{\"end\":63628,\"start\":63620},{\"end\":63902,\"start\":63892},{\"end\":63916,\"start\":63902},{\"end\":63932,\"start\":63916},{\"end\":64142,\"start\":64132},{\"end\":64156,\"start\":64142},{\"end\":64168,\"start\":64156},{\"end\":64183,\"start\":64168},{\"end\":64193,\"start\":64183},{\"end\":64407,\"start\":64397},{\"end\":64420,\"start\":64407},{\"end\":64431,\"start\":64420},{\"end\":64447,\"start\":64431},{\"end\":64468,\"start\":64447},{\"end\":64486,\"start\":64468},{\"end\":64837,\"start\":64826},{\"end\":64848,\"start\":64837},{\"end\":64861,\"start\":64848},{\"end\":64875,\"start\":64861},{\"end\":65216,\"start\":65205},{\"end\":65229,\"start\":65216},{\"end\":65243,\"start\":65229},{\"end\":65256,\"start\":65243},{\"end\":65270,\"start\":65256},{\"end\":65563,\"start\":65551},{\"end\":65576,\"start\":65563},{\"end\":65586,\"start\":65576},{\"end\":65776,\"start\":65763},{\"end\":65788,\"start\":65776},{\"end\":65799,\"start\":65788},{\"end\":65811,\"start\":65799},{\"end\":66064,\"start\":66053},{\"end\":66074,\"start\":66064},{\"end\":66086,\"start\":66074},{\"end\":66101,\"start\":66086},{\"end\":66118,\"start\":66101},{\"end\":66139,\"start\":66118},{\"end\":66423,\"start\":66413},{\"end\":66435,\"start\":66423},{\"end\":66451,\"start\":66435},{\"end\":66658,\"start\":66649},{\"end\":66671,\"start\":66658},{\"end\":66685,\"start\":66671},{\"end\":66698,\"start\":66685},{\"end\":66708,\"start\":66698},{\"end\":66724,\"start\":66708},{\"end\":66738,\"start\":66724},{\"end\":67032,\"start\":67023},{\"end\":67042,\"start\":67032},{\"end\":67058,\"start\":67042},{\"end\":67072,\"start\":67058},{\"end\":67315,\"start\":67299},{\"end\":67328,\"start\":67315},{\"end\":67341,\"start\":67328},{\"end\":67352,\"start\":67341},{\"end\":67365,\"start\":67352},{\"end\":67379,\"start\":67365},{\"end\":67388,\"start\":67379},{\"end\":67674,\"start\":67661},{\"end\":67685,\"start\":67674},{\"end\":67699,\"start\":67685},{\"end\":67709,\"start\":67699},{\"end\":67724,\"start\":67709},{\"end\":68014,\"start\":67999},{\"end\":68028,\"start\":68014},{\"end\":68289,\"start\":68274},{\"end\":68299,\"start\":68289},{\"end\":68312,\"start\":68299},{\"end\":68319,\"start\":68312},{\"end\":68584,\"start\":68567},{\"end\":68594,\"start\":68584},{\"end\":68607,\"start\":68594},{\"end\":68618,\"start\":68607}]", "bib_venue": "[{\"end\":48983,\"start\":48979},{\"end\":49248,\"start\":49244},{\"end\":49553,\"start\":49506},{\"end\":49839,\"start\":49764},{\"end\":50249,\"start\":50245},{\"end\":50529,\"start\":50464},{\"end\":51006,\"start\":51002},{\"end\":51311,\"start\":51307},{\"end\":51515,\"start\":51511},{\"end\":51726,\"start\":51722},{\"end\":51932,\"start\":51868},{\"end\":52133,\"start\":52129},{\"end\":52348,\"start\":52344},{\"end\":52635,\"start\":52631},{\"end\":52923,\"start\":52916},{\"end\":53208,\"start\":53165},{\"end\":53559,\"start\":53480},{\"end\":53876,\"start\":53871},{\"end\":54123,\"start\":54119},{\"end\":54336,\"start\":54329},{\"end\":54556,\"start\":54552},{\"end\":54848,\"start\":54844},{\"end\":55139,\"start\":55135},{\"end\":55386,\"start\":55382},{\"end\":55625,\"start\":55526},{\"end\":56006,\"start\":56002},{\"end\":56285,\"start\":56281},{\"end\":56533,\"start\":56529},{\"end\":56729,\"start\":56725},{\"end\":56952,\"start\":56949},{\"end\":57118,\"start\":57072},{\"end\":57397,\"start\":57393},{\"end\":57637,\"start\":57627},{\"end\":57911,\"start\":57907},{\"end\":58172,\"start\":58168},{\"end\":58425,\"start\":58385},{\"end\":58676,\"start\":58669},{\"end\":58914,\"start\":58910},{\"end\":59184,\"start\":59180},{\"end\":59459,\"start\":59455},{\"end\":59704,\"start\":59644},{\"end\":59913,\"start\":59853},{\"end\":60268,\"start\":60264},{\"end\":60566,\"start\":60559},{\"end\":60873,\"start\":60869},{\"end\":61106,\"start\":61035},{\"end\":61483,\"start\":61479},{\"end\":61737,\"start\":61733},{\"end\":62054,\"start\":62050},{\"end\":62404,\"start\":62397},{\"end\":62696,\"start\":62692},{\"end\":62880,\"start\":62830},{\"end\":63190,\"start\":63108},{\"end\":63632,\"start\":63628},{\"end\":63936,\"start\":63932},{\"end\":64197,\"start\":64193},{\"end\":64514,\"start\":64486},{\"end\":64882,\"start\":64875},{\"end\":65274,\"start\":65270},{\"end\":65590,\"start\":65586},{\"end\":65815,\"start\":65811},{\"end\":66143,\"start\":66139},{\"end\":66455,\"start\":66451},{\"end\":66745,\"start\":66738},{\"end\":67021,\"start\":66938},{\"end\":67392,\"start\":67388},{\"end\":67728,\"start\":67724},{\"end\":68032,\"start\":68028},{\"end\":68323,\"start\":68319},{\"end\":68622,\"start\":68618},{\"end\":68852,\"start\":68771},{\"end\":69050,\"start\":68960},{\"end\":69492,\"start\":69366}]"}}}, "year": 2023, "month": 12, "day": 17}