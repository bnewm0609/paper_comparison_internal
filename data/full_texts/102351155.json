{"id": 102351155, "updated": "2023-10-01 22:42:43.404", "metadata": {"title": "Beyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry", "authors": "[{\"first\":\"Fei\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shunkai\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Qiuyuan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Junqiu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Hongbin\",\"last\":\"Zha\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 4, "day": 3}, "abstract": "Most previous learning-based visual odometry (VO) methods take VO as a pure tracking problem. In contrast, we present a VO framework by incorporating two additional components called Memory and Refining. The Memory component preserves global information by employing an adaptive and efficient selection strategy. The Refining component ameliorates previous results with the contexts stored in the Memory by adopting a spatial-temporal attention mechanism for feature distilling. Experiments on the KITTI and TUM-RGBD benchmark datasets demonstrate that our method outperforms state-of-the-art learning-based methods by a large margin and produces competitive results against classic monocular VO approaches. Especially, our model achieves outstanding performance in challenging scenarios such as texture-less regions and abrupt motions, where classic VO algorithms tend to fail.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.01892", "mag": "2958731371", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/XueWLWWZ19", "doi": "10.1109/cvpr.2019.00877"}}, "content": {"source": {"pdf_hash": "a63f36407a3ee9ed5c6608983974ed793626eb08", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.01892v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.01892", "status": "GREEN"}}, "grobid": {"id": "7b254b88dfa9c76bd6133cfb548def1d7a387765", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a63f36407a3ee9ed5c6608983974ed793626eb08.txt", "contents": "\nBeyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry\n\n\nFei Xue feixue@pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nXin Wang xinwangcis@pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nShunkai Li lishunkai@pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nQiuyuan Wang wangqiuyuan@pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nJunqiu Wang \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nHongbin Zha zha@cis.pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nBeyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry\n2 Beijing Changcheng Aviation Measurement and Control Institute, AVIC 3 PKU-SenseTime Machine Vision Joint Lab\nMost previous learning-based visual odometry (VO) methods take VO as a pure tracking problem. In contrast, we present a VO framework by incorporating two additional components called Memory and Refining. The Memory component preserves global information by employing an adaptive and efficient selection strategy. The Refining component ameliorates previous results with the contexts stored in the Memory by adopting a spatial-temporal attention mechanism for feature distilling. Experiments on the KITTI and TUM-RGBD benchmark datasets demonstrate that our method outperforms state-of-the-art learning-based methods by a large margin and produces competitive results against classic monocular VO approaches. Especially, our model achieves outstanding performance in challenging scenarios such as texture-less regions and abrupt motions, where classic VO algorithms tend to fail.\n\nIntroduction\n\nVisual Odometry (VO) and Visual Simultaneous Localization And Mapping (V-SLAM) estimate camera poses from image sequences by exploiting the consistency between consecutive frames. As an essential task in autonomous driving and robotics, VO has been studied for decades and many outstanding algorithms have been developed [7,8,10,20,30]. Recently, as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) achieve impressive performance in many computer vision tasks [4,6,12,34], a number of end-to-end models have been proposed for VO estimation. These methods either learn depth and ego-motion jointly with CNNs [16,19,36,37,39], or leverage RNNs to introduce temporal information [14, Figure 1. Overview of our framework. Compared with existing learning-based methods which formulate VO task as a pure tracking problem, we introduce two useful components called Memory and Refining. The Memory module preserves longer time information by adopting an adaptive context selection strategy. The Refining module ameliorates previous outputs by employing a spatialtemporal feature reorganization mechanism. 22,[31][32][33]. Due to the high dimensionality of depth maps, the number of frames is commonly limited to no more than 5. Although temporal information is aggregated through recurrent units, RNNs are incapable of remembering previous observations for long time [27], leading to the limited usage of historical information. Besides, the above methods pay little attention to the significance of incoming observations for refining previous results, which is crucial for VO tasks.\n\nDirect estimation of camera motions from image snippets is prone to large errors due to the geometric uncertainty caused by small baselines (especially for handheld devices). Consequently, error accumulation is getting increasingly severe over time, as global poses are integrated from framewise poses. In classic VO/SLAM systems [20], a local map is established according to the co-visibility graph over up to hundreds of frames, on which bundle adjustment is executed to jointly optimize all corresponding poses. Therefore, both previous and new observations are incorporated for optimization, and accumulated errors are thus alleviated.\n\nInspired by classic VO/SLAM systems [7,20], we introduce an effective component, called Memory, which explicitly preserves the accumulated information adaptively.\n\nOwing to the high sample frequency, contents between consecutive frames are much overlapped. Rather than keeping accumulated information per time step with brute force, an intuitive and efficient strategy is utilized to reduce the redundancy. As errors of previous poses will propagate over time to current estimation, refining previous results becomes necessary. The Memory contains much global information, which can be leveraged naturally to refine previous results. Therefore, a Refining component is introduced. The Refining module takes the global pose estimation as a registration problem by aligning each view with the Memory. A spatial-temporal attention mechanism is applied to the contexts stored in the Memory for feature selection.\n\nThe overview of our framework is illustrated in Fig. 1. The encoder encodes paired images into high-level features. The Tracking module accepts sequential features as input, fuses current observation into accumulated information using convolution LSTMs [25] for preserving spatial connections, and produces relative poses. Hidden states of the Tracking RNN are adaptively preserved in the Memory slots. The Refining module ameliorates previous results using another convolutional LSTM, enabling refined results passing through recurrent units to improve the following outputs. Our contributions can be summarized as follows:\n\n\u2022 We propose a novel end-to-end VO framework consisting of the Tracking, Memory and Refining components;\n\n\u2022 An adaptive and efficient strategy is adopted for the Memory component to preserve accumulated information;\n\n\u2022 A spatial-temporal attention mechanism is employed for the Refining component to distill valuable features.\n\nOur method outperforms state-of-the-art learning-based methods and produces competitive results against classic algorithms. Additionally, it works well in challenging conditions where classic algorithms tend to fail due to insufficient textures or abrupt motions. The rest of this paper is organized as follows. In Sec.2, related works on monocular odometry are discussed. In Sec.3, our architecture is described in detail. The performance of the proposed approach is compared with current state-of-the-art methods in Sec.4. We conclude the paper in Sec.5.\n\n\nRelated Works\n\nVisual odometry has been studied for decades, and many excellent approaches have been proposed. Traditionally, VO is tackled by minimizing geometric reprojection errors [10,18,20] or photometric errors [7,8,30]. These methods mostly work in regular environments, but will fail in challenging scenarios such as textureless scenes or abrupt motions. After the surge of CNNs and RNNs, the VO task has been explored with deep learning techniques. A number of approaches have been proposed to deal with the challenges in classic monocular VO/SLAM systems such as feature detection [1], depth initialization [28,34], scale correction [35], depth representation [2] and data association [3,17]. Despite their promising performance, they utilize the classic framework as backend, and thus cannot been deployed in an end-to-end fashion. In this paper, we mainly focus on learning-based end-to-end monocular VO works.\n\nUnsupervised methods Mimicking the conventional structure from motion, SfmLearner [39] learns the single view depth and ego-motion from monocular image snippets using photometric errors as supervisory signals. Following the same scenario, Vid2Depth [19] adopts a differential ICP (Iterative Closest Point) loss executed on estimated 3D point clouds to enforce the consistency of predicted depth maps of two consecutive frames. GeoNet [36] estimates the depth, optical flow and ego-motion jointly from monocular views. To cope with the scale ambiguity of motions recovered from monocular image sequences, Depth-VO-Feat [37] and Un-DeepVO [16] extend the work of SfmLearner to accept stereo image pairs as input and recover the absolute scale with the known baseline.\n\nAlthough these unsupervised methods break the limitation of requiring massive labeled data for training, only limited number of consecutive frames can be processed in a sequence due to the fragility of photometric losses, leading to high geometric uncertainty and severe error accumulation.\n\nSupervised methods DeMoN [29] jointly estimates the depth and camera poses in an end-to-end fashion by formulating structure from motion as a supervised learning problem. DeepTAM [38] extends DTAM [21] via two individual subnetworks indicating tracking and mapping for the pose and depth estimation respectively. Both DeMoN and DeepTAM achieve promising results, yet require highly labeled data (depth, optical flow and camera pose) for training. MapNet [12] presents an allocentric spatial memory for localization, but only discrete directions and positions can be obtained in synthetic environments.\n\nVO can be formulated as a sequential learning problem via RNNs. DeepVO [31] harnesses the LSTM [13] to intro-duce historical knowledge for current relative motion prediction. Based on DeepVO, ESP-VO [32] infers poses and uncertainties in a unified framework. GFS-VO [33] considers the discriminability of features to different motion patterns and estimates the rotation and translation separately with a dual-branch LSTM. In addition, the ConvLSTM unit [25] is adopted to retain the spatial formulation of features. There are some other works focusing reducing localization errors by imposing constraints on relative poses [4,14,22].\n\nGeometric uncertainty can be partially reduced by aggregating more temporal information using RNNs or LSTMs. Unfortunately, RNNs or LSTMs are limited for remembering long historical knowledge [27]. Instead, we extend the field of view by adaptively preserving hidden states of recurrent units as memories. Therefore, the previous valuable information can be inherited longer than being kept in only the single current hidden state. Besides, all these methods ignore the importance of new observations in refining previous poses, which is essential for VO tasks. By incorporating the Refining module, previous poses can be updated by aligning filtered features with the Memory. Therefore, error accumulation is further mitigated.\n\n\nApproach\n\nThe encoder extracts high-level features from consecutive RGB images in Sec.3.1. The Tracking module accepts sequential features as input, aggregates temporal information, and produces relative poses in Sec \n\n\nEncoder\n\nWe harness CNNs to encode images into high-level features. Optical flow has been proved useful for estimating frame-to-frame ego-motion by lots of current works [22,[31][32][33]38]. We design the encoder based on the Flownet [6] which predicts optical flow between two images. The encoder retains the first 9 convolutional layers of Flownet encoding a pair of images, concatenated along RGB channels, into a 1024-channel 2D feature-map. The process can be described as:\nX t = F(I t\u22121 , I t ) .\n(1)\n\nX t \u2208 R C\u00d7H\u00d7W denotes the encoded feature-map at time t by function F from two consecutive images I t\u22121 and I t .\n\nH, W and C represent the height, width and channel of obtained feature maps.\n\n\nTracking\n\nThe Tracking module fuses current observations into accumulated information and calculates relative camera mo- Figure 2. The Tracking module of our framework is implemented on a convolutional LSTM [25]. Relative camera poses are produced by the SE (3) layer [5] from the outputs of recurrent units. Temporal information is preserved in the hidden states.\nEncoder ConvLSTM SE3 layer , , \u2212 , \u2212 , \u2212\ntions between two consecutive views as shown in Fig. 2. Sequence modeling We adopt the prevent LSTM [13] to model the image sequence. In this case, the feature flow passing through recurrent units carries rich accumulated information of previous inputs to infer the current output. Note that the standard LSTM unit used by DeepVO [31] and ESP-VO [32] requires 1D vector as input in which the spatial structure of features is ignored. The ConvLSTM unit [25], an extension of LSTM with convolution underneath, is adopted in the Tracking RNN for preserving the spatial formulation of visual cues and expanding the capacity of recurrent units for remembering more knowledge. The recurrent process can be controlled by\nO t , H t = U(X t , H t\u22121 ) .(2)\nO t denotes the output at time t. H t and H t\u22121 are the hidden states at current and the last time step.\n\nRelative pose estimation Relative motions can be directly recovered from paired images. Unfortunately, direct estimation is prone to error accumulation due to the geometric uncertainty brought by short baselines. The problem can be mitigated by introducing more historical information. Inheriting accumulated knowledge, the output of recurrent unit at each time step is naturally used for pose estimation. The SE (3) [5] layer generates the 6-DoF motion P t,t\u22121 from the output at time t.\n\nTheoretically, the global pose of each view can be recovered by integrating predicted relative poses as P t = t i=1 P i,i\u22121 P 0 (P 0 denotes the origin pose of the world coordinate) just as DeepVO [31] and ESP-VO [32]. The accumulated error, however, will get increasingly severe, and thus degrades the performance of the entire system. Due to the lack of explicit geometric representation of the 3D environments, neural networks, however, are incapable of building a global map to assist tracking. Fortunately, the temporal information is recorded in the hidden states of recurrent units. Although the information is short-time, these hidden states at different time points can be gathered and reorganized as parts of an implicit map (discussed in Sec.3.3).\n1 \u2032 2 \u2032 2 \u2032 \u2032 \u2032 1 2 (a) \u22121 1 2 \u2026 \u2032 \u2032 (b)\n\nRemembering\n\nThe Memory module is a neural analogue of the local map commonly used in classic VO/SLAM systems [20]. Considering the LSTM cannot remember knowledge for long time [27], we explicitly store hidden states of recurrent units at different time points to lengthen the time span.\n\nA vanilla choice is to take each time step into account via storing all hidden states over the whole sequence as M = {m 1 , m 2 , ..., m N \u22121 , m N }, where m i denotes the ith hidden state in the sequence, and N is the size of the memory buffer. Since contents of two consecutive images are much overlapped, it's redundant to remember each hidden state. Instead, only key states are selected. As the difference between two frames coincides with the poses, we utilize the motion distance as a metric to decide if current hidden state should be stored.\n\nSpecifically, the current hidden state would not be put into the Memory, unless the parallax between the current and the latest view in the slot is large enough. Here, the rotational and translational distances are utilized:\n||Rot mi \u2212 Rot mi\u22121 || 2 \u2265 \u03b8 Rot , (3) ||T rans mi \u2212 T rans mi\u22121 || 2 \u2265 \u03b8 T rans .(4)\nThis strategy guarantees both the co-visibility of different views and the existence of global information. As both previous and new observations are gathered, the Memory can be used to optimize previous poses.\n\n\nRefining\n\nOnce the Memory is constructed, the Refining module estimates the absolute pose of each view by aligning corre- sponding observation with the Memory, as shown in Fig. 3. We adopt another recurrent branch using ConvLSTM, enabling previously refined outputs passing through recurrent units to improve the next estimation, as:\n0 0 \u2032 \u2032 \u2032 \u22121 \u2026 \u2026 (a) \u00d7 1 \u00d7 1 \u2032 \u22121 (b)O A t , H A t = U A (X A t , H A t\u22121 ) . (5) X A t , O A t\nand H A t are the input, output and hidden state at time t. H A t\u22121 denotes the hidden state at time t \u2212 1. The U A indicates the recurrent branch for the Absolute pose estimation. All these variables are 3D tensors to be discussed in the following sections.\n\nSpatial-temporal attention Although all observations are fused and distributed in N hidden states, each hidden state stored in the Memory contributes discriminatively to different views. In order to distinguish related information, an attention mechanism is adopted. We utilize the last output O A t\u22121 as guidance, since motions between two consecutive views in a sequence are very small.\n\nIn specific, we generate selected memories M t for current view t with the function G as:\nM t = G(O A t\u22121 , M ) .(6)\nThe temporal attention aims to re-weight elements in the Memory considering the contribution of each m i to the pose estimation of specific views. Therefore, as shown in Fig. 4(a), M t can be defined as the linear averaging of all el-\nements in M as M t = N i=1 \u03b1 i m i . The \u03b1 i = exp(wi) N k=1 exp(wi) denotes the normalized weight. The w i = S(O A t\u22121 , m i )\nis the weight computed according to the cosine similarity function denoted as S.\n\nAs all elements in the Memory are formulated as 3D tensors, spatial connections are retained. In this framework, we focus on not only which element in the Memory plays a more important role but also where each element influences the final results more significantly. We try to find corresponding co-visible contents at the feature level. Hence, we extend the attention mechanism from the temporal domain to the spatial-temporal domain incorporating an additional channel favored feature attention mechanism. Feature-map at each channel is taken as a unit and re-weighted for each view according to the last output. As shown in Fig. 4(b), the process is described as:\nM t = N i=1 \u03b1 i C(\u03b2 i1 m i1 , \u03b2 i2 m i2 , ..., \u03b2 iC m iC ) . (7)\nThe m ij \u2208 R H\u00d7W denotes the jth channel of the ith element in the Memory. The \u03b2 ij is the normalized weight defined on the correlation between the jth channel of O t\u22121 and m i . C concatenates all reweighted feature maps along the channel dimension. We calculate the cosine similarity between two vectorized feature-maps to assign the correlation weights.\n\nAbsolute pose estimation The guidance is also executed on the observations encoded as high-level features to distill related visual cues, denoted as X t . Both reorganized memories and observations are stacked along channels and passed through two convolutional layers with kernel size of 3 for fusion. The fused feature denoted as X A t is the final input to be fed into convolutional recurrent units. Then the SE 3 layer calculates the absolute pose from the output O A t . Note that, through recurrent units, the hidden state propagating refined results to next time point further improves the following prediction.\n\n\nLoss Function\n\nOur model learns relative and absolute poses in the Tracking and Refining modules separately. Therefore, consisting of both relative and absolute pose errors, the loss functions are defined as:\nL local = 1 t t i=1 ||p i\u22121,i \u2212 p i\u22121,i || 2 + k||\u03c6 i\u22121,i \u2212 \u03c6 i\u22121,i || 2 ,(8)L global = t i=1 1 i (||p 0,i \u2212 p 0,i || 2 + k||\u03c6 0,i \u2212 \u03c6 0,i || 2 ),(9)L total = L local + L global ,(10)\nwherep i\u22121,i , p i\u22121,i ,\u03c6 i\u22121,i , and \u03c6 i\u22121,i respectively represent the predicted and ground-truth relative translations and rotations in three directions;p 0,i , p 0,i ,\u03c6 0,i , and \u03c6 0,i represent the predicted and ground-truth absolute translations and rotations. L local , L global and L total denote the local, global, and total losses respectively. t is the current frame index in a sequence. k is a fixed parameter for balancing the rotational and translational errors.\n\n\nExperiment\n\nWe first discuss the implementation details of our framework in Sec.4.1. Next, we compare our method with state-of-the-art approaches on the KITTI [9] and TUM-RGBD [26] \n\n\nImplementation\n\nTraining Our network takes monocular RGB image sequences as input. The image size can be arbitrary because our model has no requirement of compressing features into vectors as DeepVO [31] and ESP-VO [32]. We use 11 consecutive images to construct a sequence, yet our model can accept dynamic lengths of inputs. The parameter k is set to 100 and 1 for the KITTI and TUM-RGBD dataset. The \u03b8 Rot and \u03b8 T rans are set to 0.005 (rad) and 0.6 (m) for the KITTI dataset. While for the TUM-RGBD dataset, the values are 0.01 (rad) and 0.01 (m). The buffer size N is initialized with the sequence length, yet the buffer can be used without being fully occupied.\n\nNetwork The encoder is pretrained on the FlyingChairs dataset [6], while other parts of the network are initialized with MSRA [11]. Our model is implemented by PyTorch [23] on an NVIDIA 1080Ti GPU. Adam [15] with \u03b2 1 = 0.9, \u03b2 2 = 0.99 is used as the optimizer. The network is trained with a batch size of 4, a weight decay of 4 \u00d7 10 \u22124 for 150,000 iterations in total. The initial learning rate is set to 10 \u22124 and reduced by half every 60,000 iterations.\n\n\nResults on the KITTI Dataset\n\nThe KITTI dataset [9], one of the most influential outdoor VO/SLAM benchmark datasets, is widely used in both classic [10,20] and learning-based works [16,19,31,32,36,37,39]. It consists of 22 sequences captured in urban and highway environments at a relatively low sample frequency (10 fps) at the speed up to 90km/h. Seq 00-10 provide raw data with ground-truth represented as 6-DoF motion parameters considering the complicated urban environments, while Seq 11-21 provide only raw data. In our experiments, the left RGB images are resized to 1280 x 384 for training and testing. We adopt the same train/test split as DeepVO [31] and GFS-VO [33] by using Seq 00, 02, 08, 09 for training and Seq 03, 04, 05, 06, 07, 10 for evaluation.\n\nBaseline methods The learning-based baselines include supervised approaches such as DeepVO [31], ESP-VO [32], GFS-VO [33], and unsupervised approaches such as SfmLearner [39], Depth-VO-Feat [37], GeoNet [36], Vid2Depth [19] and UndeepVO [16]. Monocular VISO2 [10] (VISO2-M) and ORB-SLAM2 [20] are used as classic baselines. The error metrics, i.e., averaged Root Mean Square Errors (RMSE) of the translational and rotational errors, are adopted for all the test sequences of the lengths ranging from 100, 200 to 800 meters.\n\nComparison with learning-based methods As shown in Table 1, our method outperforms DeepVO [31], ESP-VO [32] and GFS-VO-RNN [33] (without motion decou-Sequence  Method  03  04  05  06  07  10 Avg t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel UnDeepVO [16] 5.00 6. 17    pling) on all of the test sequences by a large margin. Since DeepVO, ESP-VO and GFS-VO only consider historical knowledge stored in a single hidden state, error accumulates severely. The problem is partially mitigated by considering the discriminative ability of features to different motion patterns in GFS-VO, while our method is more effective.\n\nMeanwhile, we provide the results of unsupervised approaches in Table 1. As monocular VO methods including SfmLearner [39], GeoNet [36], Vid2Depth [19] suffer from scale ambiguity, frame-to-frame motions of short sequence snippets are aligned individually with ground-truths Method Seq Ours VISO2-M [10] ORB-SLAM2 [20] ORB-SLAM2 (LC) [20] t rel r rel t rel r rel t rel r rel t rel r rel 03 3.32 2. 10  to fix scales. Although they achieve promising performance on sequences consisting of 5 (SfmLearner, GeoNet) or 3 (Vid2Depth) frames, they suffer from heavy error accumulation when integrating poses over the entire sequence. Benefited from stereo images in scale recovery, UnDeepVO [16] and Depth-VO-Feat [37] obtain competitive results against DeepVO, ESP-VO, and GFS-VO, while our results are still much better. Note that only monocular images are used in our model.\n\nWe further evaluate the average rotation and translation errors for different path lengths and speeds in Fig. 5. The accumulated errors over long path lengths are effectively mitigated by our method owing to the new information for refining previous results. Moreover, this advantage of our algorithm can also be seen in handling high speed situations. GFS-VO [33] also achieves promising rotation estimation by decoupling the motions. Unfortunately, it does not provide robust translation results.\n\nComparison with classic methods The results of VISO2-M [10], ORB-SLAM2 [20] (with and without loop closure), and our method are shown in Table 2. VISO2-M is a pure monocular VO algorithm recovering framewise poses. ORB-SLAM2, however, is a strong baseline, because both versions utilize local bundle adjustment for jointly optimizing poses and a global map. Our model outperforms VISO2-M consistently. ORB-SLAM2 [20] achieves superior performance in terms of rotation estimation owing to the global explicit geometric constraints. However, it suffers more from error accumulation in translation on long sequences (Seq 05, 06, 07) than our approach, which is reduced by global bundle adjustment. While for short sequences (Seq 03, 04, 10), performances of the two versions and our method are very close. The small differences between the results of ORB-SLAM2 with loop close and our method suggest that global information is retained and effectively used by our novel framework.\n\nA visualization of the trajectories estimated by Depth-VO-Feat, GFS-VO, ORB-SLAM2 and our method is illustrated in Fig. 6. Depth-VO-Feat suffers from sever error accumulation though trained on stereo images. GFS-VO and ORB-SLAM2 produces close results with our model in simple environments (Seq 03, 10), while our method outperforms them in complicated scenes (Seq 05, 07).\n\n\nResults on the TUM-RGBD Dataset\n\nWe test the generalization ability of our model on the TUM-RGBD dataset [26], a prevalent public benchmark used by a number of VO/SLAM algorithms [8,20,38]. The dataset was collected by handheld cameras in indoor environments with various conditions including dynamic objects, textureless regions and abrupt motions. The dataset provides both color and depth images, while only the monocular RGB images are used in our experiments. Different from datasets captured by moving cars, motions in this benchmark contain complicated patterns due to the handheld capture mode. We select some sequences for training and others for testing (The details can be found in the supplementary material), and evaluate the performance in both regular and challenging conditions using the averaged Absolute Trajectory Errors (ATE).\n\nComparison with classic methods Since few monocular learning-based VO algorithms have attempted to handle complicated motions recorded by handheld cameras, we alternatively compare our approach against current state-ofthe-art classic methods including ORB-SLAM2 [20] and DSO [7]. As shown in Table 3, they yield promising results on scenes with rich textures (fr2/desk, fr2/360 kidnap, fr3/sitting static, fr3/nstr tex near loop, fr3/str tex far), yet our results are comparable.\n\nAs ORB-SLAM2 [20] relies on ORB [24] features to establish correspondences, it fails in scenes with- out rich textures (fr3/nstr ntex near loop, fr3/str ntex far, fr2/large cabinet). Utilizing pixels with large gradients for tracking, DSO [7] works well in scenes with structures or edges (fr3/str ntex far, fr3/str tex far). It cannot achieve good performance when textures are insufficient. Both ORB-SLAM2 and DSO can hardly work in scenes without texture and structure (fr2/large cabinet, fr3/nstr ntex near loop) and tend to fail when facing abrupt motions (fr2 pioneer 360, fr2/ pioneer slam3). In contrast, our method is capable of dealing with these challenges owing to the ability of deep learning in extracting high-level features, and the efficacy of our proposal for error reduction. A visualization of trajectories is shown in Fig. 7. Table 3 also shows an ablation study illustrating the importance of each component in our framework. The baseline is our model removing the Memory and Refining modules, similar to [31][32][33]. The Tracking model works poorly in both regular and challenging conditions, because historical knowledge in a single hidden state is inefficient to reduce accumulated errors. Fortunately, the Memory component mitigates the problem by explicitly introducing more global information and considerably improves results of the Tracking model both on regular and challenging sequences.\n\n\nAblation Study\n\nWe further test the spatial-temporal attention strategy adopted for selecting features from memories and observations by removing the temporal attention and spatial attention progressively. We observe that both of the two attention techniques are crucial to improve the results, especially in challenging conditions (fr2/pioneer 360, fr2/pioneer slam3, fr3/nstr ntex near loop).  [26] (from left to right: fr3/str tex far, fr2/poineer 360, fr3/str ntex far, fr3/nstr ntex near loop). Trajectories are aligned with ground-truths for scale recovery.\n\n\nSequence\n\nDesc  Table 3. Evaluation on the TUM-RGBD dataset [26]. The values describe the translational RMSE in [m/s]. Results of ORB-SLAM2 [20] and DSO [7] are generated from the officially released source code with recommended parameters. Ours (tracking) is a network which contains only the tracking component. Ours (w/o temp atten) indicates the model averaging the all memories as input without temporal attention. Ours (w/o spat atten) is the model removing the spatial attention yet retaining the temporal attention.\n\n\nConclusion\n\nIn this paper, we present a novel framework for learning monocular visual odometry in an end-to-end fashion. In the framework, we incorporate another two helpful components called Memory and Refining, which focus on introducing more global information and ameliorating previous results with these information respectively. We utilize an adaptive and efficient selection strategy to construct the Memory. Besides, a spatial-temporal attention mechanism is employed for feature selection when recovering the absolute poses in the Refining module. The refined results propagating information through recurrent units, further improve the following estimation. Experiments demonstrate that our model outperforms previous learning-based monocular VO methods and gives competitive results against classic VO approaches on the KITTI and TUM-RGBD benchmarks re-spectively. Moreover, our model obtains outstanding results under challenging conditions including texture-less regions and abrupt motions, where classic methods tend to fail.\n\nIn the future, we consider to extend the work to a full SLAM system consisting tracking, mapping and global bundle adjustment. Moreover, auxiliary information, such as IMU (Inertial Measurement Units) and GPS data will also be introduced to enhance the system.\n\n\nAbstract\n\nIn the supplementary material, we provide additional technical details and results for our paper. We first give the details of the training/testing splits of TUM-RGBD dataset used in our experiments. Next, we test the influence of the length of the training sequence on the performance of our model. Finally, we test the generalization ability of our network on the extra sequences of the KITTI benchmark.\n\n\nTraining Data in the TUM-RGBD dataset\n\nWe select 19 sequences for training (see Table 4) and 10 sequences for testing (see Table 5). Both the training and testing sequences include various conditions such as regular scenes, textureless regions, or abrupt motions. This dataset was collected by handheld cameras at up to 30 fps, resulting in much overlap between consecutive frames. Training our model directly on these raw sequences will lead to over-fitting. To cope with problem, we randomly select 16578 short sequences as the training sequences, yet the overlapped frames are reduced.\n\n\nThe Influence of Sequence Length\n\nIn this section, We test the influence of sequence length on the results. Theoretically, the better performance can be achieved by our model when given more frames. We compare the results with sequences constructing of 5, 7, 9, and 11 frames on the KITTI (see Table 6) and the TUM-RGBD dataset (see Table 7) respectively.\n\nAs we can see from Table 6 and 7, results of our model are improved considerably by introducing more frames. Since our model preserves information explicitly over the whole sequence and refines previous outputs with new observations, the performance can be intuitively improved by giving more observations. Fig. 8 and 9 illustrate the qualitative comparison.\n\n\nGeneralization\n\nWe tentatively test the generalization ability of our model on Seq 11-19 of the KITTI dataset [9]. Since the groundtruths of these sequences are unavailable, similar with GFS-VO, we utilize the results of stereo VISO2 (VISO2-S) [10] as references. This time, our model is trained on the Seq 00-10 as GFS-VO [33] to avoid over-fitting and maximize the ability of generalization.\n\nQualitative comparison is illustrated in Fig. 10. VISO2-M [10] suffers from severe error accumulation, because it is a classic tracking VO by integrating frame-wise poses as the entire trajectories. ORB-SLAM2 [20] partially alleviates the problem with a global map to assist tracking. Although achieving promising performance in regular environments  Table 5. Training sequences in the TUM-RGBD dataset [26].\n\n( Seq 11,15), it bears large scale drift in complicated scenes (Seq 13,14,16,18,19). The requirement of sophisticate map initialization degrades its ability to handle situations such as high speeds (Seq 12, 17). In contrast, owing to the introduced the Memory component for global information gathering and the Refining component for ameliorating previous outputs, the scale drift is significantly alleviated in contrast to GFS-VO [33]. Sequence  Method  03  04  05  06  07  10 Avg t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel t rel r rel 5 frames 4.01 2.82 3.31 2. 28 Table 6. Results of our method using different lengths of sequences on the KITTI dataset. The best results are highlighted.   Table 7. Evaluation on the TUM-RGBD dataset [26]. The values describe the translational RMSE in [m/s]. Results of our model with the Tracking, Memory and Refining components on sequence lengths of 5, 7, 9,11. The best results are highlighted. Figure 9. Trajectories of our model trained with sequences consisting of different frames on the TUM-RGBD dataset [26] (from left to right: fr3/nstr tex near loop, fr2/pioneer 360, fr3/str ntex far, fr3/nstr ntex near loop, fr3/str tex far, fr3/large cabinet).  Figure 10. The trajectories of stereo VISO2 (VISO2-S) [10], monocular VISO2 (VISO2-M) [10], monocular ORB-SLAM2 [20] without loop closure , GFS-VO [33], and our model on Seq 11-19 of the KITTI benchmark dataset [9]. This time, GFS-VO and our model are trained on the Seq 00-10. As the ground-truths of Seq 11-19 are available, we the results of VISO2-S for references, which is similar to GFS-VO. ORB-SLAM2 fails to initialization in Seq 12 and 17 due to high speeds in highway environments.\n\n\n.3.2. Hidden states of the Tracking RNN are adaptively selected to construct the Memory (Sec.3.3) for further Refining previous results in Sec.3.4. We design the loss function considering both relative and absolute pose errors in Sec.3.5.\n\nFigure 3 .\n3(a) The Refining module aligns current observation with the contexts stored in the Memory module for absolute pose estimation. (b) Both contexts and the current observation are reorganized utilizing the last output as guidance.\n\nFigure 4 .\n4Extracting features from Memory using the last output as guidance. We consider the correlation of both each context stored in the Memory in (a) and every channel of the context in (b).\n\n( a )\naTranslation against path length. (b) Rotation against path length.(c) Translation against speed.(d) Rotation against speed.\n\nFigure 5 .\n5Average errors on translation and rotation against different path lengths and speeds.\n\nFigure 6 .\n6The trajectories of ground-truth, ORB-SLAM2 [20], Depth-VO-Feat [37], GFS-VO [33] and our model on Seq 03, 05, 07 and 10 (from left to right) of the KITTI benchmark.\n\nFigure 7 .\n7The raw images (top) and trajectories (bottom) recovered by our method on the TUM-RGBD dataset\n\nFigure 8 .\n8The trajectories of models trained on sequences with 5, 7, 9, and 11 frames.\n\n\ndatasets in Sec.4.2 and Sec.4.3, respectively. Finally, an ablation study is performed in Sec.4.4.\n\nTable 2 .\n2Results of VISO2-M[10], ORB-SLAM2 (with and without loop closure)[20] and our method on the KITTI dataset.8.47 8.82 2.28 0.40 2.17 0.39 \n04 2.96 1.76 4.69 4.49 1.41 0.14 1.07 0.17 \n05 2.59 1.25 19.22 17.58 13.21 0.22 1.86 0.24 \n06 4.93 1.90 7.30 6.14 18.68 0.26 4.96 0.18 \n07 3.07 1.76 23.61 19.11 10.96 0.37 1.87 0.39 \n10 3.94 1.72 41.56 32.99 3.71 0.30 3.76 0.29 \nAvg 3.47 1.75 17.48 16.52 8.38 0.28 2.62 0.28 \n\n\n\n\nTable 4. Training sequences used in the TUM-RGBD dataset[26].Sequence \nDesc. \nstr/tex/brute motion \nFrames \n\nfr1/360 \nY/Y/N \n756 \nfr1/floor \nY/N/N \n1242 \nfr1/room \nY/Y/N \n1362 \nfr1/desk \nY/Y/Y \n613 \nfr1/desk2 \nY/Y/Y \n640 \nfr2/xyz \nY/Y/N \n3669 \nfr1/plant \nN/Y/N \n1141 \nfr1/teddy \nN/N/N \n1419 \nfr1/coke \nN/Y/N \n2521 \nfr3/teddy \nN/Y/N \n2409 \nfr2/flowerbouquet \nY/N/N \n2972 \nfr3/sitting xyz \nY/Y/N \n1261 \nfr1/sitting helfsphere \nY/Y/N \n1110 \nfr2/pioneer slam \nY/Y/Y \n2921 \nfr2/pioneer slam2 \nY/Y/Y \n2113 \nfr3/nstr ntex far \nN/N/N \n474 \nfr3/nstr tex far \nN/Y/N \n465 \nfr3/str ntex near \nY/N/N \n1082 \nfr3/str tex near \nY/Y/N \n1099 \n\nSequence \nDesc. \nstr/tex/brute motion \nFrames \n\nfr2/desk \nY/Y/N \n2965 \nfr2/360 kidnap \nY/Y/N \n1431 \nfr2/pioneer 360 \nY/Y/Y \n1225 \nfr2/pioneer slam3 \nY/Y/Y \n2113 \nfr2/large cabinet \nY/N/N \n1011 \nfr3/sitting static \nY/Y/N \n707 \nfr3/nstr ntex near loop \nN/N/N \n1125 \nfr3/nstr tex near loop \nN/Y/N \n1682 \nfr3/str ntex far \nY/N/N \n814 \nfr3/str tex far \nY/Y/N \n938 \n\n\n\n\n3.54 1.69 7.27 2.61 5.67 3.35 5.25 3.16 4.84 2.65 7 frames 3.83 2.81 3.34 2.51 3.33 1.64 6.56 2.32 2.60 1.71 5.02 2.77 4.11 2.29 9 frames 3.34 2.16 3.18 1.46 3.31 1.51 6.30 2.08 3.24 1.97 4.16 2.16 3.92 1.89 11 frames 3.32 2.10 2.96 1.76 2.59 1.25 4.93 1.90 3.07 1.76 3.94 1.72 3.47 1.75\nt rel : average translational RMSE drift (%) on length from 100, 200 to 800 m. r rel : average rotational RMSE drift ( \u2022 /100m) on length from 100, 200 to 800 m.\nAcknowledgementThe work is supported by the National Key Research and Development Program of China (2017YFB1002601) and National Natural Science Foundation of China (61632003, 61771026).\nLearning to See by Moving. P Agrawal, J Carreira, J Malik, ICCV. P. Agrawal, J. Carreira, and J. Malik. Learning to See by Moving. In ICCV, 2015. 2\n\nCodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. M Bloesch, J Czarnowski, R Clark, S Leutenegger, A J Davison, CVPR. M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison. CodeSLAM-Learning a Compact, Optimis- able Representation for Dense Visual SLAM. In CVPR, 2018. 2\n\nPappas. Probabilistic Data Association for Semantic SLAM. S L Bowman, N Atanasov, K Daniilidis, G J , ICRA. S. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pap- pas. Probabilistic Data Association for Semantic SLAM. In ICRA, 2017. 2\n\nMap-Net: Geometry-aware Learning of Maps for Camera Localization. S Brahmbhatt, J Gu, K Kim, J Hays, J Kautz, CVPR. 13S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz. Map- Net: Geometry-aware Learning of Maps for Camera Local- ization. In CVPR, 2018. 1, 3\n\nVidLoc: A Deep Spatio-temporal Model for 6-DoF Videoclip Relocalization. R Clark, S Wang, A Markham, N Trigoni, H Wen, CVPR. R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen. VidLoc: A Deep Spatio-temporal Model for 6-DoF Video- clip Relocalization. In CVPR, 2017. 3\n\nFlownet: Learning Optical Flow with Convolutional Networks. A Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas, V Golkov, P Van Der Smagt, D Cremers, T Brox, ICCV. 15A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. van der Smagt, D. Cremers, and T. Brox. Flownet: Learning Optical Flow with Convolutional Net- works. In ICCV, 2015. 1, 3, 5\n\nDirect Sparse Odometry. TPAMI. J Engel, V Koltun, D Cremers, J. Engel, V. Koltun, and D. Cremers. Direct Sparse Odome- try. TPAMI, 2018. 1, 2, 7, 8\n\nLSD-SLAM: Largescale Direct Monocular SLAM. J Engel, T Sch\u00f6ps, D Cremers, ECCV. J. Engel, T. Sch\u00f6ps, and D. Cremers. LSD-SLAM: Large- scale Direct Monocular SLAM. In ECCV, 2014. 1, 2, 7\n\nAre We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite. A Geiger, P Lenz, R Urtasun, CVPR. 1013A. Geiger, P. Lenz, and R. Urtasun. Are We Ready for Au- tonomous Driving? The KITTI Vision Benchmark Suite. In CVPR, 2012. 5, 10, 13\n\nStereoscan: Dense 3D Reconstruction in Real-time. A Geiger, J Ziegler, C Stiller, IV. 13A. Geiger, J. Ziegler, and C. Stiller. Stereoscan: Dense 3D Reconstruction in Real-time. In IV, 2011. 1, 2, 5, 6, 7, 10, 13\n\nDelving Deep into Rectifiers: Surpassing Human-level Performance on Imagenet Classification. K He, X Zhang, S Ren, J Sun, ICCV. K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rec- tifiers: Surpassing Human-level Performance on Imagenet Classification. In ICCV, 2015. 5\n\nMapNet: An Allocentric Spatial Memory for Mapping Environments. J F Henriques, A Vedaldi, CVPR. 1J. F. Henriques and A. Vedaldi. MapNet: An Allocentric Spatial Memory for Mapping Environments. In CVPR, 2018. 1, 2\n\nLong Short-term Memory. S Hochreiter, J Schmidhuber, Neural Computation. 23S. Hochreiter and J. Schmidhuber. Long Short-term Mem- ory. Neural Computation, 1997. 2, 3\n\nGeometric Consistency for Self-supervised End-to-end Visual Odometry. G Iyer, J K Murthy, K Gunshi Gupta, L Paull, CVPR Workshops. 13G. Iyer, J. K. Murthy, K. Gunshi Gupta, and L. Paull. Ge- ometric Consistency for Self-supervised End-to-end Visual Odometry. In CVPR Workshops, 2018. 1, 3\n\nAdam: A Method for Stochastic Optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015. 5\n\nUnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning. R Li, S Wang, Z Long, D Gu, ICRA. 6R. Li, S. Wang, Z. Long, and D. Gu. UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning. In ICRA, 2018. 1, 2, 5, 6\n\nVSO: Visual Semantic Odometry. K.-N Lianos, J L Sch\u00f6nberger, M Pollefeys, T Sattler, ECCV. K.-N. Lianos, J. L. Sch\u00f6nberger, M. Pollefeys, and T. Sattler. VSO: Visual Semantic Odometry. In ECCV, 2018. 2\n\nICE-BA: Incremental, Consistent and Efficient Bundle Adjustment for Visual-Inertial SLAM. H Liu, M Chen, G Zhang, H Bao, Y Bao, CVPR. H. Liu, M. Chen, G. Zhang, H. Bao, and Y. Bao. ICE-BA: Incremental, Consistent and Efficient Bundle Adjustment for Visual-Inertial SLAM. In CVPR, 2018. 2\n\nUnsupervised Learning of Depth and Ego-motion from Monocular Video Using 3D Geometric Constraints. R Mahjourian, M Wicke, A Angelova, CVPR. 6R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised Learning of Depth and Ego-motion from Monocular Video Using 3D Geometric Constraints. In CVPR, 2018. 1, 2, 5, 6\n\nORB-SLAM2: An Opensource SLAM System for Monocular, Stereo, and RGB-D Cameras. R Mur-Artal, J D Tard\u00f3s, 1013R. Mur-Artal and J. D. Tard\u00f3s. ORB-SLAM2: An Open- source SLAM System for Monocular, Stereo, and RGB-D Cameras. T-RO, 2017. 1, 2, 4, 5, 6, 7, 8, 10, 13\n\nDTAM: Dense Tracking and Mapping in Real-time. R A Newcombe, S J Lovegrove, A J Davison, ICCV. R. A. Newcombe, S. J. Lovegrove, and A. J. Davison. DTAM: Dense Tracking and Mapping in Real-time. In ICCV, 2011. 2\n\nGlobal Pose Estimation with an Attention-based Recurrent Network. E Parisotto, D Singh Chaplot, J Zhang, R Salakhutdinov, CVPR Workshops. 13E. Parisotto, D. Singh Chaplot, J. Zhang, and R. Salakhutdi- nov. Global Pose Estimation with an Attention-based Recur- rent Network. In CVPR Workshops, 2018. 1, 3\n\n. A Paszke, S Gross, S Chintala, G Chanan, Pytorch, A. Paszke, S. Gross, S. Chintala, and G. Chanan. Pytorch. https://github.com/pytorch/pytorch, 2017. 5\n\nORB: An Efficient Alternative to SIFT or SURF. E Rublee, V Rabaud, K Konolige, G Bradski, ICCV. E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. ORB: An Efficient Alternative to SIFT or SURF. In ICCV, 2011. 7\n\nX Shi, Z Chen, H Wang, D Yeung, W Wong, W Woo, Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. In NIPS. 23X. Shi, Z. Chen, H. Wang, D. Yeung, W. Wong, and W. Woo. Convolutional LSTM Network: A Machine Learning Ap- proach for Precipitation Nowcasting. In NIPS, 2015. 2, 3\n\nA Benchmark for the Evaluation of RGB-D SLAM Systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, IROS. 1012J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre- mers. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IROS, 2012. 5, 7, 8, 10, 12\n\nEnd-toend Memory Networks. S Sukhbaatar, J Weston, R Fergus, NIPS. 14S. Sukhbaatar, a. szlam, J. Weston, and R. Fergus. End-to- end Memory Networks. In NIPS, 2015. 1, 3, 4\n\nCNN-SLAM: Real-time Dense Monocular SLAM with Learned Depth Prediction. K Tateno, F Tombari, I Laina, N Navab, CVPR. K. Tateno, F. Tombari, I. Laina, and N. Navab. CNN-SLAM: Real-time Dense Monocular SLAM with Learned Depth Pre- diction. In CVPR, 2017. 2\n\nDeMoN: Depth and Motion Network for Learning Monocular Stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, CVPR. B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. DeMoN: Depth and Motion Network for Learning Monocular Stereo. In CVPR, 2017. 2\n\nStereo DSO: Largescale Direct Sparse Visual Odometry with Stereo Cameras. R Wang, M Schworer, D Cremers, ICCV. 1R. Wang, M. Schworer, and D. Cremers. Stereo DSO: Large- scale Direct Sparse Visual Odometry with Stereo Cameras. In ICCV, 2017. 1, 2\n\nDeepVO: Towards End-to-end Visual Odometry with Deep Recurrent Convolutional Neural Networks. S Wang, R Clark, H Wen, N Trigoni, ICRA. 67S. Wang, R. Clark, H. Wen, and N. Trigoni. DeepVO: Towards End-to-end Visual Odometry with Deep Recurrent Convolutional Neural Networks. In ICRA, 2017. 1, 2, 3, 5, 6, 7\n\nEnd-toend, Sequence-to-sequence Probabilistic Visual Odometry through Deep Neural Networks. IJRR. S Wang, R Clark, H Wen, N Trigoni, 67S. Wang, R. Clark, H. Wen, and N. Trigoni. End-to- end, Sequence-to-sequence Probabilistic Visual Odometry through Deep Neural Networks. IJRR, 2018. 1, 3, 5, 6, 7\n\nGuided Feature Selection for Deep Visual Odometry. F Xue, Q Wang, X Wang, W Dong, J Wang, H Zha, ACCV. 13F. Xue, Q. Wang, X. Wang, W. Dong, J. Wang, and H. Zha. Guided Feature Selection for Deep Visual Odometry. In ACCV, 2018. 1, 3, 5, 6, 7, 10, 13\n\nDeep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry. N Yang, R Wang, J St\u00fcckler, D Cremers, ECCV. 6N. Yang, R. Wang, J. St\u00fcckler, and D. Cremers. Deep Vir- tual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry. In ECCV, 2018. 1, 2, 6\n\nScale Recovery for Monocular Visual Odometry Using Depth Estimated with Deep Convolutional Neural Fields. X Yin, X Wang, X Du, Q Chen, ICCV. X. Yin, X. Wang, X. Du, and Q. Chen. Scale Recovery for Monocular Visual Odometry Using Depth Estimated with Deep Convolutional Neural Fields. In ICCV, 2017. 2\n\nGeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose. Z Yin, J Shi, CVPR. 6Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose. In CVPR, 2018. 1, 2, 5, 6\n\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction. H Zhan, R Garg, C Weerasekera, K Li, H Agarwal, I Reid, CVPR. 67H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agarwal, and I. Reid. Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Recon- struction. In CVPR, 2018. 1, 2, 5, 6, 7\n\nDeepTAM: Deep Tracking and Mapping. H Zhou, B Ummenhofer, T Brox, ECCV. 7H. Zhou, B. Ummenhofer, and T. Brox. DeepTAM: Deep Tracking and Mapping. In ECCV, 2018. 2, 3, 7\n\nUnsupervised Learning of Depth and Ego-motion from Video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. 6T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu- pervised Learning of Depth and Ego-motion from Video. In CVPR, 2017. 1, 2, 5, 6\n", "annotations": {"author": "[{\"end\":185,\"start\":81},{\"end\":295,\"start\":186},{\"end\":406,\"start\":296},{\"end\":521,\"start\":407},{\"end\":612,\"start\":522},{\"end\":722,\"start\":613}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":85},{\"end\":194,\"start\":190},{\"end\":306,\"start\":304},{\"end\":419,\"start\":415},{\"end\":533,\"start\":529},{\"end\":624,\"start\":621}]", "author_first_name": "[{\"end\":84,\"start\":81},{\"end\":189,\"start\":186},{\"end\":303,\"start\":296},{\"end\":414,\"start\":407},{\"end\":528,\"start\":522},{\"end\":620,\"start\":613}]", "author_affiliation": "[{\"end\":184,\"start\":108},{\"end\":294,\"start\":218},{\"end\":405,\"start\":329},{\"end\":520,\"start\":444},{\"end\":611,\"start\":535},{\"end\":721,\"start\":645}]", "title": "[{\"end\":78,\"start\":1},{\"end\":800,\"start\":723}]", "venue": null, "abstract": "[{\"end\":1790,\"start\":912}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2130,\"start\":2127},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2132,\"start\":2130},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2135,\"start\":2132},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2138,\"start\":2135},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2141,\"start\":2138},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2294,\"start\":2291},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2296,\"start\":2294},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2299,\"start\":2296},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2302,\"start\":2299},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2442,\"start\":2438},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2445,\"start\":2442},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2448,\"start\":2445},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2451,\"start\":2448},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2454,\"start\":2451},{\"end\":2511,\"start\":2507},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2931,\"start\":2928},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2935,\"start\":2931},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2939,\"start\":2935},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2943,\"start\":2939},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3194,\"start\":3190},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3742,\"start\":3738},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4088,\"start\":4085},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4091,\"start\":4088},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5216,\"start\":5212},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6660,\"start\":6656},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6663,\"start\":6660},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6666,\"start\":6663},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6692,\"start\":6689},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6694,\"start\":6692},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6697,\"start\":6694},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7066,\"start\":7063},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7093,\"start\":7089},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7096,\"start\":7093},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7119,\"start\":7115},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7145,\"start\":7142},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7170,\"start\":7167},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7173,\"start\":7170},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7482,\"start\":7478},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7649,\"start\":7645},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7834,\"start\":7830},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8018,\"start\":8014},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8037,\"start\":8033},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8484,\"start\":8480},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8638,\"start\":8634},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8656,\"start\":8652},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8913,\"start\":8909},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9133,\"start\":9129},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9157,\"start\":9153},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9261,\"start\":9257},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9328,\"start\":9324},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9515,\"start\":9511},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9684,\"start\":9681},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9687,\"start\":9684},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9690,\"start\":9687},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9889,\"start\":9885},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10818,\"start\":10814},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10822,\"start\":10818},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10826,\"start\":10822},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10830,\"start\":10826},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10833,\"start\":10830},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10881,\"start\":10878},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11557,\"start\":11553},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11617,\"start\":11614},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11856,\"start\":11852},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12086,\"start\":12082},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12102,\"start\":12098},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12208,\"start\":12204},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13025,\"start\":13022},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13296,\"start\":13292},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13312,\"start\":13308},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14010,\"start\":14006},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14077,\"start\":14073},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19649,\"start\":19646},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19667,\"start\":19663},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19874,\"start\":19870},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19890,\"start\":19886},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20405,\"start\":20402},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20470,\"start\":20466},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20512,\"start\":20508},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20547,\"start\":20543},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20849,\"start\":20846},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20950,\"start\":20946},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20953,\"start\":20950},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20983,\"start\":20979},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20986,\"start\":20983},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20989,\"start\":20986},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20992,\"start\":20989},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20995,\"start\":20992},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20998,\"start\":20995},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21001,\"start\":20998},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21459,\"start\":21455},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21475,\"start\":21471},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21660,\"start\":21656},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21673,\"start\":21669},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21686,\"start\":21682},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21739,\"start\":21735},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21759,\"start\":21755},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21772,\"start\":21768},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21788,\"start\":21784},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21806,\"start\":21802},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21828,\"start\":21824},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21857,\"start\":21853},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22184,\"start\":22180},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22197,\"start\":22193},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22217,\"start\":22213},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22382,\"start\":22378},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22393,\"start\":22391},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22868,\"start\":22864},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22881,\"start\":22877},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22897,\"start\":22893},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23049,\"start\":23045},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23064,\"start\":23060},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23084,\"start\":23080},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23146,\"start\":23144},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23434,\"start\":23430},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23457,\"start\":23453},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23982,\"start\":23978},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24177,\"start\":24173},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24193,\"start\":24189},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24534,\"start\":24530},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25582,\"start\":25578},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25655,\"start\":25652},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25658,\"start\":25655},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25661,\"start\":25658},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26587,\"start\":26583},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26599,\"start\":26596},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26819,\"start\":26815},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26838,\"start\":26834},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27044,\"start\":27041},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27833,\"start\":27829},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27837,\"start\":27833},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27841,\"start\":27837},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28625,\"start\":28621},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28855,\"start\":28851},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28935,\"start\":28931},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28947,\"start\":28944},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32461,\"start\":32458},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32596,\"start\":32592},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32675,\"start\":32671},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32805,\"start\":32801},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32956,\"start\":32952},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33150,\"start\":33146},{\"end\":33162,\"start\":33155},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33165,\"start\":33162},{\"end\":33224,\"start\":33216},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33227,\"start\":33224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33230,\"start\":33227},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33233,\"start\":33230},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33236,\"start\":33233},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33588,\"start\":33584},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33748,\"start\":33746},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33923,\"start\":33919},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34236,\"start\":34232},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34438,\"start\":34434},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34470,\"start\":34466},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34496,\"start\":34492},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34531,\"start\":34527},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34594,\"start\":34591},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36295,\"start\":36291},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36342,\"start\":36338},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36750,\"start\":36746}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":35112,\"start\":34872},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35353,\"start\":35113},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35551,\"start\":35354},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35683,\"start\":35552},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35782,\"start\":35684},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35961,\"start\":35783},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36069,\"start\":35962},{\"attributes\":{\"id\":\"fig_8\"},\"end\":36159,\"start\":36070},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36260,\"start\":36160},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36687,\"start\":36261},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37677,\"start\":36688},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37967,\"start\":37678}]", "paragraph": "[{\"end\":3406,\"start\":1806},{\"end\":4047,\"start\":3408},{\"end\":4211,\"start\":4049},{\"end\":4957,\"start\":4213},{\"end\":5583,\"start\":4959},{\"end\":5689,\"start\":5585},{\"end\":5800,\"start\":5691},{\"end\":5911,\"start\":5802},{\"end\":6469,\"start\":5913},{\"end\":7394,\"start\":6487},{\"end\":8161,\"start\":7396},{\"end\":8453,\"start\":8163},{\"end\":9056,\"start\":8455},{\"end\":9691,\"start\":9058},{\"end\":10421,\"start\":9693},{\"end\":10641,\"start\":10434},{\"end\":11122,\"start\":10653},{\"end\":11150,\"start\":11147},{\"end\":11265,\"start\":11152},{\"end\":11343,\"start\":11267},{\"end\":11710,\"start\":11356},{\"end\":12465,\"start\":11752},{\"end\":12603,\"start\":12499},{\"end\":13093,\"start\":12605},{\"end\":13853,\"start\":13095},{\"end\":14183,\"start\":13909},{\"end\":14736,\"start\":14185},{\"end\":14962,\"start\":14738},{\"end\":15259,\"start\":15049},{\"end\":15595,\"start\":15272},{\"end\":15950,\"start\":15692},{\"end\":16340,\"start\":15952},{\"end\":16431,\"start\":16342},{\"end\":16693,\"start\":16459},{\"end\":16902,\"start\":16822},{\"end\":17570,\"start\":16904},{\"end\":17992,\"start\":17636},{\"end\":18612,\"start\":17994},{\"end\":18823,\"start\":18630},{\"end\":19484,\"start\":19008},{\"end\":19668,\"start\":19499},{\"end\":20338,\"start\":19687},{\"end\":20795,\"start\":20340},{\"end\":21563,\"start\":20828},{\"end\":22088,\"start\":21565},{\"end\":22744,\"start\":22090},{\"end\":23616,\"start\":22746},{\"end\":24116,\"start\":23618},{\"end\":25095,\"start\":24118},{\"end\":25470,\"start\":25097},{\"end\":26319,\"start\":25506},{\"end\":26800,\"start\":26321},{\"end\":28222,\"start\":26802},{\"end\":28788,\"start\":28241},{\"end\":29314,\"start\":28801},{\"end\":30356,\"start\":29329},{\"end\":30618,\"start\":30358},{\"end\":31036,\"start\":30631},{\"end\":31627,\"start\":31078},{\"end\":31985,\"start\":31664},{\"end\":32345,\"start\":31987},{\"end\":32741,\"start\":32364},{\"end\":33151,\"start\":32743},{\"end\":34871,\"start\":33153}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11146,\"start\":11123},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11751,\"start\":11711},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12498,\"start\":12466},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13894,\"start\":13854},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15048,\"start\":14963},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15633,\"start\":15596},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15691,\"start\":15633},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16458,\"start\":16432},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16821,\"start\":16694},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17635,\"start\":17571},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18901,\"start\":18824},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18973,\"start\":18901},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19007,\"start\":18973}]", "table_ref": "[{\"end\":22148,\"start\":22141},{\"end\":22280,\"start\":22218},{\"end\":22817,\"start\":22810},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24262,\"start\":24255},{\"end\":26620,\"start\":26613},{\"end\":27656,\"start\":27649},{\"end\":28814,\"start\":28807},{\"end\":31126,\"start\":31119},{\"end\":31169,\"start\":31162},{\"end\":31931,\"start\":31924},{\"end\":31970,\"start\":31963},{\"end\":32013,\"start\":32006},{\"end\":33101,\"start\":33094},{\"end\":33630,\"start\":33590},{\"end\":33756,\"start\":33749},{\"end\":33882,\"start\":33875}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1804,\"start\":1792},{\"attributes\":{\"n\":\"2.\"},\"end\":6485,\"start\":6472},{\"attributes\":{\"n\":\"3.\"},\"end\":10432,\"start\":10424},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10651,\"start\":10644},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11354,\"start\":11346},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13907,\"start\":13896},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15270,\"start\":15262},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18628,\"start\":18615},{\"attributes\":{\"n\":\"4.\"},\"end\":19497,\"start\":19487},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19685,\"start\":19671},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20826,\"start\":20798},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25504,\"start\":25473},{\"attributes\":{\"n\":\"4.4.\"},\"end\":28239,\"start\":28225},{\"end\":28799,\"start\":28791},{\"attributes\":{\"n\":\"5.\"},\"end\":29327,\"start\":29317},{\"end\":30629,\"start\":30621},{\"end\":31076,\"start\":31039},{\"end\":31662,\"start\":31630},{\"end\":32362,\"start\":32348},{\"end\":35124,\"start\":35114},{\"end\":35365,\"start\":35355},{\"end\":35558,\"start\":35553},{\"end\":35695,\"start\":35685},{\"end\":35794,\"start\":35784},{\"end\":35973,\"start\":35963},{\"end\":36081,\"start\":36071},{\"end\":36271,\"start\":36262}]", "table": "[{\"end\":36687,\"start\":36379},{\"end\":37677,\"start\":36751}]", "figure_caption": "[{\"end\":35112,\"start\":34874},{\"end\":35353,\"start\":35126},{\"end\":35551,\"start\":35367},{\"end\":35683,\"start\":35560},{\"end\":35782,\"start\":35697},{\"end\":35961,\"start\":35796},{\"end\":36069,\"start\":35975},{\"end\":36159,\"start\":36083},{\"end\":36260,\"start\":36162},{\"end\":36379,\"start\":36273},{\"end\":36751,\"start\":36690},{\"end\":37967,\"start\":37680}]", "figure_ref": "[{\"end\":2520,\"start\":2512},{\"end\":5013,\"start\":5007},{\"end\":11475,\"start\":11467},{\"end\":11806,\"start\":11800},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15440,\"start\":15434},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16638,\"start\":16629},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17540,\"start\":17531},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23729,\"start\":23723},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24855,\"start\":24839},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25218,\"start\":25212},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27648,\"start\":27641},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32306,\"start\":32294},{\"end\":32791,\"start\":32784},{\"end\":34126,\"start\":34118},{\"end\":34389,\"start\":34380}]", "bib_author_first_name": "[{\"end\":38345,\"start\":38344},{\"end\":38356,\"start\":38355},{\"end\":38368,\"start\":38367},{\"end\":38546,\"start\":38545},{\"end\":38557,\"start\":38556},{\"end\":38571,\"start\":38570},{\"end\":38580,\"start\":38579},{\"end\":38595,\"start\":38594},{\"end\":38597,\"start\":38596},{\"end\":38843,\"start\":38842},{\"end\":38845,\"start\":38844},{\"end\":38855,\"start\":38854},{\"end\":38867,\"start\":38866},{\"end\":38881,\"start\":38880},{\"end\":38883,\"start\":38882},{\"end\":39089,\"start\":39088},{\"end\":39103,\"start\":39102},{\"end\":39109,\"start\":39108},{\"end\":39116,\"start\":39115},{\"end\":39124,\"start\":39123},{\"end\":39357,\"start\":39356},{\"end\":39366,\"start\":39365},{\"end\":39374,\"start\":39373},{\"end\":39385,\"start\":39384},{\"end\":39396,\"start\":39395},{\"end\":39617,\"start\":39616},{\"end\":39632,\"start\":39631},{\"end\":39643,\"start\":39642},{\"end\":39650,\"start\":39649},{\"end\":39661,\"start\":39660},{\"end\":39673,\"start\":39672},{\"end\":39683,\"start\":39682},{\"end\":39700,\"start\":39699},{\"end\":39711,\"start\":39710},{\"end\":39959,\"start\":39958},{\"end\":39968,\"start\":39967},{\"end\":39978,\"start\":39977},{\"end\":40121,\"start\":40120},{\"end\":40130,\"start\":40129},{\"end\":40140,\"start\":40139},{\"end\":40335,\"start\":40334},{\"end\":40345,\"start\":40344},{\"end\":40353,\"start\":40352},{\"end\":40559,\"start\":40558},{\"end\":40569,\"start\":40568},{\"end\":40580,\"start\":40579},{\"end\":40815,\"start\":40814},{\"end\":40821,\"start\":40820},{\"end\":40830,\"start\":40829},{\"end\":40837,\"start\":40836},{\"end\":41064,\"start\":41063},{\"end\":41066,\"start\":41065},{\"end\":41079,\"start\":41078},{\"end\":41238,\"start\":41237},{\"end\":41252,\"start\":41251},{\"end\":41451,\"start\":41450},{\"end\":41459,\"start\":41458},{\"end\":41461,\"start\":41460},{\"end\":41471,\"start\":41470},{\"end\":41478,\"start\":41472},{\"end\":41487,\"start\":41486},{\"end\":41715,\"start\":41714},{\"end\":41717,\"start\":41716},{\"end\":41727,\"start\":41726},{\"end\":41897,\"start\":41896},{\"end\":41903,\"start\":41902},{\"end\":41911,\"start\":41910},{\"end\":41919,\"start\":41918},{\"end\":42101,\"start\":42097},{\"end\":42111,\"start\":42110},{\"end\":42113,\"start\":42112},{\"end\":42128,\"start\":42127},{\"end\":42141,\"start\":42140},{\"end\":42360,\"start\":42359},{\"end\":42367,\"start\":42366},{\"end\":42375,\"start\":42374},{\"end\":42384,\"start\":42383},{\"end\":42391,\"start\":42390},{\"end\":42658,\"start\":42657},{\"end\":42672,\"start\":42671},{\"end\":42681,\"start\":42680},{\"end\":42947,\"start\":42946},{\"end\":42960,\"start\":42959},{\"end\":42962,\"start\":42961},{\"end\":43176,\"start\":43175},{\"end\":43178,\"start\":43177},{\"end\":43190,\"start\":43189},{\"end\":43192,\"start\":43191},{\"end\":43205,\"start\":43204},{\"end\":43207,\"start\":43206},{\"end\":43407,\"start\":43406},{\"end\":43420,\"start\":43419},{\"end\":43437,\"start\":43436},{\"end\":43446,\"start\":43445},{\"end\":43648,\"start\":43647},{\"end\":43658,\"start\":43657},{\"end\":43667,\"start\":43666},{\"end\":43679,\"start\":43678},{\"end\":43848,\"start\":43847},{\"end\":43858,\"start\":43857},{\"end\":43868,\"start\":43867},{\"end\":43880,\"start\":43879},{\"end\":44013,\"start\":44012},{\"end\":44020,\"start\":44019},{\"end\":44028,\"start\":44027},{\"end\":44036,\"start\":44035},{\"end\":44045,\"start\":44044},{\"end\":44053,\"start\":44052},{\"end\":44377,\"start\":44376},{\"end\":44386,\"start\":44385},{\"end\":44399,\"start\":44398},{\"end\":44409,\"start\":44408},{\"end\":44420,\"start\":44419},{\"end\":44619,\"start\":44618},{\"end\":44633,\"start\":44632},{\"end\":44643,\"start\":44642},{\"end\":44837,\"start\":44836},{\"end\":44847,\"start\":44846},{\"end\":44858,\"start\":44857},{\"end\":44867,\"start\":44866},{\"end\":45084,\"start\":45083},{\"end\":45098,\"start\":45097},{\"end\":45106,\"start\":45105},{\"end\":45115,\"start\":45114},{\"end\":45124,\"start\":45123},{\"end\":45131,\"start\":45130},{\"end\":45146,\"start\":45145},{\"end\":45396,\"start\":45395},{\"end\":45404,\"start\":45403},{\"end\":45416,\"start\":45415},{\"end\":45663,\"start\":45662},{\"end\":45671,\"start\":45670},{\"end\":45680,\"start\":45679},{\"end\":45687,\"start\":45686},{\"end\":45974,\"start\":45973},{\"end\":45982,\"start\":45981},{\"end\":45991,\"start\":45990},{\"end\":45998,\"start\":45997},{\"end\":46226,\"start\":46225},{\"end\":46233,\"start\":46232},{\"end\":46241,\"start\":46240},{\"end\":46249,\"start\":46248},{\"end\":46257,\"start\":46256},{\"end\":46265,\"start\":46264},{\"end\":46526,\"start\":46525},{\"end\":46534,\"start\":46533},{\"end\":46542,\"start\":46541},{\"end\":46554,\"start\":46553},{\"end\":46852,\"start\":46851},{\"end\":46859,\"start\":46858},{\"end\":46867,\"start\":46866},{\"end\":46873,\"start\":46872},{\"end\":47124,\"start\":47123},{\"end\":47131,\"start\":47130},{\"end\":47373,\"start\":47372},{\"end\":47381,\"start\":47380},{\"end\":47389,\"start\":47388},{\"end\":47404,\"start\":47403},{\"end\":47410,\"start\":47409},{\"end\":47421,\"start\":47420},{\"end\":47683,\"start\":47682},{\"end\":47691,\"start\":47690},{\"end\":47705,\"start\":47704},{\"end\":47875,\"start\":47874},{\"end\":47883,\"start\":47882},{\"end\":47892,\"start\":47891},{\"end\":47903,\"start\":47902},{\"end\":47905,\"start\":47904}]", "bib_author_last_name": "[{\"end\":38353,\"start\":38346},{\"end\":38365,\"start\":38357},{\"end\":38374,\"start\":38369},{\"end\":38554,\"start\":38547},{\"end\":38568,\"start\":38558},{\"end\":38577,\"start\":38572},{\"end\":38592,\"start\":38581},{\"end\":38605,\"start\":38598},{\"end\":38852,\"start\":38846},{\"end\":38864,\"start\":38856},{\"end\":38878,\"start\":38868},{\"end\":39100,\"start\":39090},{\"end\":39106,\"start\":39104},{\"end\":39113,\"start\":39110},{\"end\":39121,\"start\":39117},{\"end\":39130,\"start\":39125},{\"end\":39363,\"start\":39358},{\"end\":39371,\"start\":39367},{\"end\":39382,\"start\":39375},{\"end\":39393,\"start\":39386},{\"end\":39400,\"start\":39397},{\"end\":39629,\"start\":39618},{\"end\":39640,\"start\":39633},{\"end\":39647,\"start\":39644},{\"end\":39658,\"start\":39651},{\"end\":39670,\"start\":39662},{\"end\":39680,\"start\":39674},{\"end\":39697,\"start\":39684},{\"end\":39708,\"start\":39701},{\"end\":39716,\"start\":39712},{\"end\":39965,\"start\":39960},{\"end\":39975,\"start\":39969},{\"end\":39986,\"start\":39979},{\"end\":40127,\"start\":40122},{\"end\":40137,\"start\":40131},{\"end\":40148,\"start\":40141},{\"end\":40342,\"start\":40336},{\"end\":40350,\"start\":40346},{\"end\":40361,\"start\":40354},{\"end\":40566,\"start\":40560},{\"end\":40577,\"start\":40570},{\"end\":40588,\"start\":40581},{\"end\":40818,\"start\":40816},{\"end\":40827,\"start\":40822},{\"end\":40834,\"start\":40831},{\"end\":40841,\"start\":40838},{\"end\":41076,\"start\":41067},{\"end\":41087,\"start\":41080},{\"end\":41249,\"start\":41239},{\"end\":41264,\"start\":41253},{\"end\":41456,\"start\":41452},{\"end\":41468,\"start\":41462},{\"end\":41484,\"start\":41479},{\"end\":41493,\"start\":41488},{\"end\":41724,\"start\":41718},{\"end\":41730,\"start\":41728},{\"end\":41900,\"start\":41898},{\"end\":41908,\"start\":41904},{\"end\":41916,\"start\":41912},{\"end\":41922,\"start\":41920},{\"end\":42108,\"start\":42102},{\"end\":42125,\"start\":42114},{\"end\":42138,\"start\":42129},{\"end\":42149,\"start\":42142},{\"end\":42364,\"start\":42361},{\"end\":42372,\"start\":42368},{\"end\":42381,\"start\":42376},{\"end\":42388,\"start\":42385},{\"end\":42395,\"start\":42392},{\"end\":42669,\"start\":42659},{\"end\":42678,\"start\":42673},{\"end\":42690,\"start\":42682},{\"end\":42957,\"start\":42948},{\"end\":42969,\"start\":42963},{\"end\":43187,\"start\":43179},{\"end\":43202,\"start\":43193},{\"end\":43215,\"start\":43208},{\"end\":43417,\"start\":43408},{\"end\":43434,\"start\":43421},{\"end\":43443,\"start\":43438},{\"end\":43460,\"start\":43447},{\"end\":43655,\"start\":43649},{\"end\":43664,\"start\":43659},{\"end\":43676,\"start\":43668},{\"end\":43686,\"start\":43680},{\"end\":43695,\"start\":43688},{\"end\":43855,\"start\":43849},{\"end\":43865,\"start\":43859},{\"end\":43877,\"start\":43869},{\"end\":43888,\"start\":43881},{\"end\":44017,\"start\":44014},{\"end\":44025,\"start\":44021},{\"end\":44033,\"start\":44029},{\"end\":44042,\"start\":44037},{\"end\":44050,\"start\":44046},{\"end\":44057,\"start\":44054},{\"end\":44383,\"start\":44378},{\"end\":44396,\"start\":44387},{\"end\":44406,\"start\":44400},{\"end\":44417,\"start\":44410},{\"end\":44428,\"start\":44421},{\"end\":44630,\"start\":44620},{\"end\":44640,\"start\":44634},{\"end\":44650,\"start\":44644},{\"end\":44844,\"start\":44838},{\"end\":44855,\"start\":44848},{\"end\":44864,\"start\":44859},{\"end\":44873,\"start\":44868},{\"end\":45095,\"start\":45085},{\"end\":45103,\"start\":45099},{\"end\":45112,\"start\":45107},{\"end\":45121,\"start\":45116},{\"end\":45128,\"start\":45125},{\"end\":45143,\"start\":45132},{\"end\":45151,\"start\":45147},{\"end\":45401,\"start\":45397},{\"end\":45413,\"start\":45405},{\"end\":45424,\"start\":45417},{\"end\":45668,\"start\":45664},{\"end\":45677,\"start\":45672},{\"end\":45684,\"start\":45681},{\"end\":45695,\"start\":45688},{\"end\":45979,\"start\":45975},{\"end\":45988,\"start\":45983},{\"end\":45995,\"start\":45992},{\"end\":46006,\"start\":45999},{\"end\":46230,\"start\":46227},{\"end\":46238,\"start\":46234},{\"end\":46246,\"start\":46242},{\"end\":46254,\"start\":46250},{\"end\":46262,\"start\":46258},{\"end\":46269,\"start\":46266},{\"end\":46531,\"start\":46527},{\"end\":46539,\"start\":46535},{\"end\":46551,\"start\":46543},{\"end\":46562,\"start\":46555},{\"end\":46856,\"start\":46853},{\"end\":46864,\"start\":46860},{\"end\":46870,\"start\":46868},{\"end\":46878,\"start\":46874},{\"end\":47128,\"start\":47125},{\"end\":47135,\"start\":47132},{\"end\":47378,\"start\":47374},{\"end\":47386,\"start\":47382},{\"end\":47401,\"start\":47390},{\"end\":47407,\"start\":47405},{\"end\":47418,\"start\":47411},{\"end\":47426,\"start\":47422},{\"end\":47688,\"start\":47684},{\"end\":47702,\"start\":47692},{\"end\":47710,\"start\":47706},{\"end\":47880,\"start\":47876},{\"end\":47889,\"start\":47884},{\"end\":47900,\"start\":47893},{\"end\":47910,\"start\":47906}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1637703},\"end\":38464,\"start\":38317},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4624670},\"end\":38782,\"start\":38466},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1657925},\"end\":39020,\"start\":38784},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4547677},\"end\":39281,\"start\":39022},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":30370275},\"end\":39554,\"start\":39283},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12552176},\"end\":39925,\"start\":39556},{\"attributes\":{\"id\":\"b6\"},\"end\":40074,\"start\":39927},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14547347},\"end\":40261,\"start\":40076},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6724907},\"end\":40506,\"start\":40263},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16284071},\"end\":40719,\"start\":40508},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13740328},\"end\":40997,\"start\":40721},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4651026},\"end\":41211,\"start\":40999},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1915014},\"end\":41378,\"start\":41213},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4854033},\"end\":41668,\"start\":41380},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6628106},\"end\":41822,\"start\":41670},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206853077},\"end\":42064,\"start\":41824},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52967509},\"end\":42267,\"start\":42066},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52089080},\"end\":42556,\"start\":42269},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3645349},\"end\":42865,\"start\":42558},{\"attributes\":{\"id\":\"b19\"},\"end\":43126,\"start\":42867},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1336659},\"end\":43338,\"start\":43128},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3425782},\"end\":43643,\"start\":43340},{\"attributes\":{\"id\":\"b22\"},\"end\":43798,\"start\":43645},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":206769866},\"end\":44010,\"start\":43800},{\"attributes\":{\"id\":\"b24\"},\"end\":44320,\"start\":44012},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206942855},\"end\":44589,\"start\":44322},{\"attributes\":{\"id\":\"b26\"},\"end\":44762,\"start\":44591},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206596482},\"end\":45018,\"start\":44764},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6159584},\"end\":45319,\"start\":45020},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8515741},\"end\":45566,\"start\":45321},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9114952},\"end\":45873,\"start\":45568},{\"attributes\":{\"id\":\"b31\"},\"end\":46172,\"start\":45875},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53776191},\"end\":46422,\"start\":46174},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":49658377},\"end\":46743,\"start\":46424},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8176169},\"end\":47045,\"start\":46745},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3714620},\"end\":47264,\"start\":47047},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4578162},\"end\":47644,\"start\":47266},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":51929808},\"end\":47814,\"start\":47646},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11977588},\"end\":48051,\"start\":47816}]", "bib_title": "[{\"end\":38342,\"start\":38317},{\"end\":38543,\"start\":38466},{\"end\":38840,\"start\":38784},{\"end\":39086,\"start\":39022},{\"end\":39354,\"start\":39283},{\"end\":39614,\"start\":39556},{\"end\":40118,\"start\":40076},{\"end\":40332,\"start\":40263},{\"end\":40556,\"start\":40508},{\"end\":40812,\"start\":40721},{\"end\":41061,\"start\":40999},{\"end\":41235,\"start\":41213},{\"end\":41448,\"start\":41380},{\"end\":41712,\"start\":41670},{\"end\":41894,\"start\":41824},{\"end\":42095,\"start\":42066},{\"end\":42357,\"start\":42269},{\"end\":42655,\"start\":42558},{\"end\":43173,\"start\":43128},{\"end\":43404,\"start\":43340},{\"end\":43845,\"start\":43800},{\"end\":44374,\"start\":44322},{\"end\":44616,\"start\":44591},{\"end\":44834,\"start\":44764},{\"end\":45081,\"start\":45020},{\"end\":45393,\"start\":45321},{\"end\":45660,\"start\":45568},{\"end\":46223,\"start\":46174},{\"end\":46523,\"start\":46424},{\"end\":46849,\"start\":46745},{\"end\":47121,\"start\":47047},{\"end\":47370,\"start\":47266},{\"end\":47680,\"start\":47646},{\"end\":47872,\"start\":47816}]", "bib_author": "[{\"end\":38355,\"start\":38344},{\"end\":38367,\"start\":38355},{\"end\":38376,\"start\":38367},{\"end\":38556,\"start\":38545},{\"end\":38570,\"start\":38556},{\"end\":38579,\"start\":38570},{\"end\":38594,\"start\":38579},{\"end\":38607,\"start\":38594},{\"end\":38854,\"start\":38842},{\"end\":38866,\"start\":38854},{\"end\":38880,\"start\":38866},{\"end\":38886,\"start\":38880},{\"end\":39102,\"start\":39088},{\"end\":39108,\"start\":39102},{\"end\":39115,\"start\":39108},{\"end\":39123,\"start\":39115},{\"end\":39132,\"start\":39123},{\"end\":39365,\"start\":39356},{\"end\":39373,\"start\":39365},{\"end\":39384,\"start\":39373},{\"end\":39395,\"start\":39384},{\"end\":39402,\"start\":39395},{\"end\":39631,\"start\":39616},{\"end\":39642,\"start\":39631},{\"end\":39649,\"start\":39642},{\"end\":39660,\"start\":39649},{\"end\":39672,\"start\":39660},{\"end\":39682,\"start\":39672},{\"end\":39699,\"start\":39682},{\"end\":39710,\"start\":39699},{\"end\":39718,\"start\":39710},{\"end\":39967,\"start\":39958},{\"end\":39977,\"start\":39967},{\"end\":39988,\"start\":39977},{\"end\":40129,\"start\":40120},{\"end\":40139,\"start\":40129},{\"end\":40150,\"start\":40139},{\"end\":40344,\"start\":40334},{\"end\":40352,\"start\":40344},{\"end\":40363,\"start\":40352},{\"end\":40568,\"start\":40558},{\"end\":40579,\"start\":40568},{\"end\":40590,\"start\":40579},{\"end\":40820,\"start\":40814},{\"end\":40829,\"start\":40820},{\"end\":40836,\"start\":40829},{\"end\":40843,\"start\":40836},{\"end\":41078,\"start\":41063},{\"end\":41089,\"start\":41078},{\"end\":41251,\"start\":41237},{\"end\":41266,\"start\":41251},{\"end\":41458,\"start\":41450},{\"end\":41470,\"start\":41458},{\"end\":41486,\"start\":41470},{\"end\":41495,\"start\":41486},{\"end\":41726,\"start\":41714},{\"end\":41732,\"start\":41726},{\"end\":41902,\"start\":41896},{\"end\":41910,\"start\":41902},{\"end\":41918,\"start\":41910},{\"end\":41924,\"start\":41918},{\"end\":42110,\"start\":42097},{\"end\":42127,\"start\":42110},{\"end\":42140,\"start\":42127},{\"end\":42151,\"start\":42140},{\"end\":42366,\"start\":42359},{\"end\":42374,\"start\":42366},{\"end\":42383,\"start\":42374},{\"end\":42390,\"start\":42383},{\"end\":42397,\"start\":42390},{\"end\":42671,\"start\":42657},{\"end\":42680,\"start\":42671},{\"end\":42692,\"start\":42680},{\"end\":42959,\"start\":42946},{\"end\":42971,\"start\":42959},{\"end\":43189,\"start\":43175},{\"end\":43204,\"start\":43189},{\"end\":43217,\"start\":43204},{\"end\":43419,\"start\":43406},{\"end\":43436,\"start\":43419},{\"end\":43445,\"start\":43436},{\"end\":43462,\"start\":43445},{\"end\":43657,\"start\":43647},{\"end\":43666,\"start\":43657},{\"end\":43678,\"start\":43666},{\"end\":43688,\"start\":43678},{\"end\":43697,\"start\":43688},{\"end\":43857,\"start\":43847},{\"end\":43867,\"start\":43857},{\"end\":43879,\"start\":43867},{\"end\":43890,\"start\":43879},{\"end\":44019,\"start\":44012},{\"end\":44027,\"start\":44019},{\"end\":44035,\"start\":44027},{\"end\":44044,\"start\":44035},{\"end\":44052,\"start\":44044},{\"end\":44059,\"start\":44052},{\"end\":44385,\"start\":44376},{\"end\":44398,\"start\":44385},{\"end\":44408,\"start\":44398},{\"end\":44419,\"start\":44408},{\"end\":44430,\"start\":44419},{\"end\":44632,\"start\":44618},{\"end\":44642,\"start\":44632},{\"end\":44652,\"start\":44642},{\"end\":44846,\"start\":44836},{\"end\":44857,\"start\":44846},{\"end\":44866,\"start\":44857},{\"end\":44875,\"start\":44866},{\"end\":45097,\"start\":45083},{\"end\":45105,\"start\":45097},{\"end\":45114,\"start\":45105},{\"end\":45123,\"start\":45114},{\"end\":45130,\"start\":45123},{\"end\":45145,\"start\":45130},{\"end\":45153,\"start\":45145},{\"end\":45403,\"start\":45395},{\"end\":45415,\"start\":45403},{\"end\":45426,\"start\":45415},{\"end\":45670,\"start\":45662},{\"end\":45679,\"start\":45670},{\"end\":45686,\"start\":45679},{\"end\":45697,\"start\":45686},{\"end\":45981,\"start\":45973},{\"end\":45990,\"start\":45981},{\"end\":45997,\"start\":45990},{\"end\":46008,\"start\":45997},{\"end\":46232,\"start\":46225},{\"end\":46240,\"start\":46232},{\"end\":46248,\"start\":46240},{\"end\":46256,\"start\":46248},{\"end\":46264,\"start\":46256},{\"end\":46271,\"start\":46264},{\"end\":46533,\"start\":46525},{\"end\":46541,\"start\":46533},{\"end\":46553,\"start\":46541},{\"end\":46564,\"start\":46553},{\"end\":46858,\"start\":46851},{\"end\":46866,\"start\":46858},{\"end\":46872,\"start\":46866},{\"end\":46880,\"start\":46872},{\"end\":47130,\"start\":47123},{\"end\":47137,\"start\":47130},{\"end\":47380,\"start\":47372},{\"end\":47388,\"start\":47380},{\"end\":47403,\"start\":47388},{\"end\":47409,\"start\":47403},{\"end\":47420,\"start\":47409},{\"end\":47428,\"start\":47420},{\"end\":47690,\"start\":47682},{\"end\":47704,\"start\":47690},{\"end\":47712,\"start\":47704},{\"end\":47882,\"start\":47874},{\"end\":47891,\"start\":47882},{\"end\":47902,\"start\":47891},{\"end\":47912,\"start\":47902}]", "bib_venue": "[{\"end\":38380,\"start\":38376},{\"end\":38611,\"start\":38607},{\"end\":38890,\"start\":38886},{\"end\":39136,\"start\":39132},{\"end\":39406,\"start\":39402},{\"end\":39722,\"start\":39718},{\"end\":39956,\"start\":39927},{\"end\":40154,\"start\":40150},{\"end\":40367,\"start\":40363},{\"end\":40592,\"start\":40590},{\"end\":40847,\"start\":40843},{\"end\":41093,\"start\":41089},{\"end\":41284,\"start\":41266},{\"end\":41509,\"start\":41495},{\"end\":41736,\"start\":41732},{\"end\":41928,\"start\":41924},{\"end\":42155,\"start\":42151},{\"end\":42401,\"start\":42397},{\"end\":42696,\"start\":42692},{\"end\":42944,\"start\":42867},{\"end\":43221,\"start\":43217},{\"end\":43476,\"start\":43462},{\"end\":43894,\"start\":43890},{\"end\":44152,\"start\":44059},{\"end\":44434,\"start\":44430},{\"end\":44656,\"start\":44652},{\"end\":44879,\"start\":44875},{\"end\":45157,\"start\":45153},{\"end\":45430,\"start\":45426},{\"end\":45701,\"start\":45697},{\"end\":45971,\"start\":45875},{\"end\":46275,\"start\":46271},{\"end\":46568,\"start\":46564},{\"end\":46884,\"start\":46880},{\"end\":47141,\"start\":47137},{\"end\":47432,\"start\":47428},{\"end\":47716,\"start\":47712},{\"end\":47916,\"start\":47912}]"}}}, "year": 2023, "month": 12, "day": 17}