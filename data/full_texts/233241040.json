{"id": 233241040, "updated": "2023-10-06 04:57:29.447", "metadata": {"title": "Image Super-Resolution via Iterative Refinement", "authors": "[{\"first\":\"Chitwan\",\"last\":\"Saharia\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Ho\",\"middle\":[]},{\"first\":\"William\",\"last\":\"Chan\",\"middle\":[]},{\"first\":\"Tim\",\"last\":\"Salimans\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Fleet\",\"middle\":[\"J.\"]},{\"first\":\"Mohammad\",\"last\":\"Norouzi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 4, "day": 15}, "abstract": "We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2104.07636", "mag": null, "acl": null, "pubmed": "36094974", "pubmedcentral": null, "dblp": "journals/pami/SahariaHCSFN23", "doi": "10.1109/tpami.2022.3204461"}}, "content": {"source": {"pdf_hash": "bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.07636v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6d268fdd64deeee55cb316ebea483dabdee8cbfa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bc7e6165b00f0c39d40ca2c7a4eb33fcc0e3200d.txt", "contents": "\nImage Super-Resolution via Iterative Refinement\n\n\nChitwan Saharia sahariac@google.com \nGoogle Research\nBrain Team\n\nJonathan Ho jonathanho@google.com \nGoogle Research\nBrain Team\n\nWilliam Chan williamchan@google.com \nGoogle Research\nBrain Team\n\nTim Salimans salimans@google.com \nGoogle Research\nBrain Team\n\nDavid J Fleet davidfleet@google.com \nGoogle Research\nBrain Team\n\nMohammad Norouzi mnorouzi@google.com \nGoogle Research\nBrain Team\n\nImage Super-Resolution via Iterative Refinement\n\nWe present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models[17,48]to conditional image generation and performs super-resolution through a stochastic iterative denoising process. Output generation starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8\u00d7 face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.\n\nIntroduction\n\nSingle-image super-resolution is the process of generating a high-resolution image that is consistent with an input low-resolution image. It falls under the broad family of image-to-image translation tasks, including colorization, in-painting, and de-blurring. Like many such inverse problems, image super-resolution is challenging because multiple output images may be consistent with a single input image, and the conditional distribution of output images given the input typically does not conform well to simple parametric distributions, e.g., a multivariate Gaussian. Accordingly, while simple regression-based methods with feedforward convolutional nets may work for super-resolution at low magnification ratios, they often lack the high-fidelity details needed for high magnification ratios.\n\nDeep generative models have seen success in learning complex empirical distributions of images (e.g., [52,57]). Autoregressive models [31,32], variational autoencoders (VAEs) [24,54], Normalizing Flows (NFs) [10,23], and \u2020 Work done as part of the Google AI Residency.\n\n\nInput\n\nSR3 output Reference GANs [14,19,36] have shown convincing image generation results and have been applied to conditional tasks such as image super-resolution [7,8,25,28,33]. However, these approaches often suffer from various limitations; e.g., autoregressive models are prohibitively expensive for highresolution image generation, NFs and VAEs often yield sub-optimal sample quality, and GANs require carefully designed regularization and optimization tricks to tame optimization instability [2,15] and mode collapse [29,38]. We propose SR3 (Super-Resolution via Repeated Refinement), a new approach to conditional image generation, inspired by recent work on Denoising Diffusion Probabilistic Models (DDPM) [17,47], and denoising score matching [17,49]. SR3 works by learning to transform a standard normal distribution into an empirical data distribution through a sequence of refinement steps, resembling Langevin dynamics. The key is a U-Net architecture [42] that is trained with a denoising objective to iteratively remove various levels of noise from the output. We adapt DDPMs to conditional image generation by proposing a simple and effective modification to the U-Net architecture. In contrast to GANs that require inner-loop maximization, arXiv:2104.07636v2 [eess.IV] 30 Jun 2021 we minimize a well-defined loss function. Unlike autoregressive models, SR3 uses a constant number of inference steps regardless of output resolution.\n\nSR3 works well across a range of magnification factors and input resolutions. SR3 models can also be cascaded, e.g., going from 64\u00d764 to 256\u00d7256, and then to 1024\u00d71024. Cascading models allows one to independently train a few small models rather than a single large model with a high magnification factor. We find that chained models enable more efficient inference, since directly generating a highresolution image requires more iterative refinement steps for the same quality. We also find that one can chain an unconditional generative model with SR3 models to unconditionally generate high-fidelity images. Unlike existing work that focuses on specific domains (e.g., faces), we show that SR3 is effective on both faces and natural images.\n\nAutomated image quality scores like PSNR and SSIM do not reflect human preference well when the input resolution is low and the magnification ratio is large (e.g., [3,7,8,28]). These quality scores often penalize synthetic high-frequency details, such as hair texture, because synthetic details do not perfectly align with the reference details. We resort to human evaluation to compare the quality of super-resolution methods. We adopt a 2-alternative forced-choice (2AFC) paradigm in which human subjects are shown a low-resolution input and are required to select between a model output and a ground truth image (cf. [63]). Based on this study, we calculate fool rate scores that capture both image quality and the consistency of model outputs with low-resolution inputs. Experiments demonstrate that SR3 achieves a significantly higher fool rate than SOTA GAN methods [7,28] and a strong regression baseline.\n\nOur key contributions are summarized as: \u2022 We adapt denoising diffusion models to conditional image generation. Our method, SR3, is an approach to image super-resolution via iterative refinement. \u2022 SR3 proves effective on face and natural image superresolution at different magnification factors. On a standard 8\u00d7 face super-resolution task, SR3 achieves a human fool rate close to 50%, outperforming FSRGAN [7] and PULSE [28] that achieve fool rates of at most 34%. \u2022 We demonstrate unconditional and class-conditional generation by cascading a 64\u00d764 image synthesis model with SR3 models to progressively generate 1024\u00d71024 unconditional faces in 3 stages, and 256\u00d7256 class-conditional ImageNet samples in 2 stages. Our class conditional Im-ageNet samples attain competitive FID scores.\n\n\nConditional Denoising Diffusion Model\n\nWe are given a dataset of input-output image pairs, de-\nnoted D = {x i , y i } N i=1\n, which represent samples drawn from an unknown conditional distribution p(y | x). This is a one-to-many mapping in which many target images may Figure 2: The forward diffusion process q (left to right) gradually adds Gaussian noise to the target image. The reverse inference process p (right to left) iteratively denoises the target image conditioned on a source image x. Source image x is not shown here.\ny 0 \u223c p(y | x) y t\u22121 yt y T \u223c N (0, I) q(yt|yt\u22121) p \u03b8 (yt\u22121|yt, x)\nbe consistent with a single source image. We are interested in learning a parametric approximation to p(y | x) through a stochastic iterative refinement process that maps a source image x to a target image y \u2208 R d . We approach this problem by adapting the denoising diffusion probabilistic (DDPM) model of [17,47] to conditional image generation. The conditional DDPM model generates a target image y 0 in T refinement steps. Starting with a pure noise image y T \u223c N (0, I), the model iteratively refines the image through successive iterations (y T \u22121 , y T \u22122 , . . . , y 0 ) according to learned conditional transition distributions p \u03b8 (y t\u22121 | y t , x) such that y 0 \u223c p(y | x) (see Figure 2).\n\nThe distributions of intermediate images in the inference chain are defined in terms of a forward diffusion process that gradually adds Gaussian noise to the signal via a fixed Markov chain, denoted q(y t | y t\u22121 ). The goal of our model is to reverse the Gaussian diffusion process by iteratively recovering signal from noise through a reverse Markov chain conditioned on x. In principle, each forward process step can be conditioned on x too, but we leave that to future work. We learn the reverse chain using a neural denoising model f \u03b8 that takes as input a source image and a noisy target image and estimates the noise. We first give an overview of the forward diffusion process, and then discuss how our denoising model f \u03b8 is trained and used for inference.\n\n\nGaussian Diffusion Process\n\nFollowing [17,47], we first define a forward Markovian diffusion process q that gradually adds Gaussian noise to a high-resolution image y 0 over T iterations:\nq(y 1:T | y 0 ) = T t=1 q(y t | y t\u22121 ) ,(1)q(y t | y t\u22121 ) = N (y t | \u221a \u03b1 t y t\u22121 , (1 \u2212 \u03b1 t )I) ,(2)\nwhere the scalar parameters \u03b1 1:T are hyper-parameters, subject to 0 < \u03b1 t < 1, which determine the variance of the noise added at each iteration. Note that y t\u22121 is attenuated by \u221a \u03b1 t to ensure that the variance of the random variables remains bounded as t \u2192 \u221e. For instance, if the variance of y t\u22121 is 1, then the variance of y t is also 1. Importantly, one can characterize the distribution of y t given y 0 by marginalizing out the intermediate steps as \u223c N (0, I) 5: Take a gradient descent step on\nq(y t | y 0 ) = N (y t | \u221a \u03b3 t y 0 , (1 \u2212 \u03b3 t )I) ,(3)\u2207 \u03b8 f \u03b8 (x, \u221a \u03b3y0 + \u221a 1 \u2212 \u03b3 , \u03b3) \u2212 p p 6: until converged where \u03b3 t = t i=1 \u03b1 i .\nFurthermore, with some algebraic manipulation and completing the square, one can derive the posterior distribution of y t\u22121 given (y 0 , y t ) as\nq(y t\u22121 | y 0 , y t ) = N (y t\u22121 | \u00b5, \u03c3 2 I) \u00b5 = \u221a \u03b3 t\u22121 (1 \u2212 \u03b1 t ) 1 \u2212 \u03b3 t y 0 + \u221a \u03b1 t (1 \u2212 \u03b3 t\u22121 ) 1 \u2212 \u03b3 t y t \u03c3 2 = (1 \u2212 \u03b3 t\u22121 )(1 \u2212 \u03b1 t ) 1 \u2212 \u03b3 t .(4)\nThis posterior distribution is helpful when parameterizing the reverse chain and formulating a variational lower bound on the log-likelihood of the reverse chain. We next discuss how one can learn a neural network to reverse this Gaussian diffusion process.\n\n\nOptimizing the Denoising Model\n\nTo help reverse the diffusion process, we take advantage of additional side information in the form of a source image x and optimize a neural denoising model f \u03b8 that takes as input this source image x and a noisy target image y,\ny = \u221a \u03b3 y 0 + 1 \u2212 \u03b3 , \u223c N (0, I) ,(5)\nand aims to recover the noiseless target image y 0 . This definition of a noisy target image y is compatible with the marginal distribution of noisy images at different steps of the forward diffusion process in (3). In addition to a source image x and a noisy target image y, the denoising model f \u03b8 (x, y, \u03b3) takes as input the sufficient statistics for the variance of the noise \u03b3, and is trained to predict the noise vector . We make the denoising model aware of the level of noise through conditioning on a scalar \u03b3, similar to [49,6]. The proposed objective function for training f \u03b8 is\nE (x,y) E ,\u03b3 f \u03b8 (x, \u221a \u03b3 y 0 + 1 \u2212 \u03b3 y , \u03b3) \u2212 p p ,(6)\nwhere \u223c N (0, I), (x, y) is sampled from the training dataset, p \u2208 {1, 2}, and \u03b3 \u223c p(\u03b3). The distribution of \u03b3 has a big impact on the quality of the model and the generated outputs. We discuss our choice of p(\u03b3) in Section 2.4. Instead of regressing the output of f \u03b8 to , as in (6), one can also regress the output of f \u03b8 to y 0 . Given \u03b3 and y, the Algorithm 2 Inference in T iterative refinement steps 1: yT \u223c N (0, I) 2: for t = T, . . . , 1 do 3: z \u223c N (0, I) if t > 1, else z = 0\n4: yt\u22121 = 1 \u221a \u03b1 t yt \u2212 1\u2212\u03b1 t \u221a 1\u2212\u03b3 t f \u03b8 (x, yt, \u03b3t) + \u221a 1 \u2212 \u03b1tz 5: end for 6: return y0\nvalues of and y 0 can be derived from each other deterministically, but changing the regression target has an impact on the scale of the loss function. We expect both of these variants to work reasonably well if p(\u03b3) is modified to account for the scale of the loss function. Further investigation of the loss function used for training the denoising model is an interesting avenue for future research in this area.\n\n\nInference via Iterative Refinement\n\nInference under our model is defined as a reverse Markovian process, which goes in the reverse direction of the forward diffusion process, starting from Gaussian noise y T :\np \u03b8 (y 0:T |x) = p(y T ) T t=1 p \u03b8 (y t\u22121 |y t , x) (7) p(y T ) = N (y T | 0, I) (8) p \u03b8 (y t\u22121 |y t , x) = N (y t\u22121 | \u00b5 \u03b8 (x, y t , \u03b3 t ), \u03c3 2 t I) . (9)\nWe define the inference process in terms of isotropic Gaussian conditional distributions, p \u03b8 (y t\u22121 |y t , x), which are learned. If the noise variance of the forward process steps are set as small as possible, i.e., \u03b1 1:T \u2248 1, the optimal reverse process p(y t\u22121 |y t , x) will be approximately Gaussian [47]. Accordingly, our choice of Gaussian conditionals in the inference process (9) can provide a reasonable fit to the true reverse process. Meanwhile, 1 \u2212 \u03b3 T should be large enough so that y T is approximately distributed according to the prior p(y T ) = N (y T |0, I), allowing the sampling process to start at pure Gaussian noise.\n\nRecall that the denoising model f \u03b8 is trained to estimate , given any noisy image y including y t . Thus, given y t , we approximate y 0 by rearranging the terms in (5) a\u015d\ny 0 = 1 \u221a \u03b3 t y t \u2212 1 \u2212 \u03b3 t f \u03b8 (x, y t , \u03b3 t ) .(10)\nFollowing the formulation of [17], we substitute our estimate\u0177 0 into the posterior distribution of q(y t\u22121 |y 0 , y t ) in (4) to parameterize the mean of p \u03b8 (y t\u22121 |y t , x) as\n\u00b5 \u03b8 (x, y t , \u03b3 t ) = 1 \u221a \u03b1 t y t \u2212 1 \u2212 \u03b1 t \u221a 1 \u2212 \u03b3 t f \u03b8 (x, y t , \u03b3 t ) ,(11)\nand we set the variance of p \u03b8 (y t\u22121 |y t , x) to (1 \u2212 \u03b1 t ), a default given by the variance of the forward process [17].\n\nFollowing this parameterization, each iteration of iterative refinement under our model takes the form,\ny t\u22121 \u2190 1 \u221a \u03b1 t y t \u2212 1 \u2212 \u03b1 t \u221a 1 \u2212 \u03b3 t f \u03b8 (x, y t , \u03b3 t ) + \u221a 1 \u2212 \u03b1 t t ,\nwhere t \u223c N (0, I). This resembles one step of Langevin dynamics with f \u03b8 providing an estimate of the gradient of the data log-density. We justify the choice of the training objective in (6) for the probabilistic model outlined in (9) from a variational lower bound perspective and a denoising score-matching perspective in Appendix B.\n\n\nSR3 Model Architecture and Noise Schedule\n\nThe SR3 architecture is similar to the U-Net found in DDPM [17], with modifications adapted from [51]; we replace the original DDPM residual blocks with residual blocks from BigGAN [4], and we re-scale skip connections by 1 \u221a 2 . We also increase the number of residual blocks, and the channel multipliers at different resolutions (see Appendix A for details). To condition the model on the input x, we up-sample the low-resolution image to the target resolution using bicubic interpolation. The result is concatenated with y t along the channel dimension. We experimented with more sophisticated methods of conditioning, such as using FiLM [34], but we found that the simple concatenation yielded similar generation quality.\n\nFor our training noise schedule, we follow [6], and use a piece wise distribution for \u03b3,\np(\u03b3) = T t=1 1 T U (\u03b3 t\u22121 , \u03b3 t ).\nSpecifically, during training, we first uniformly sample a time step t \u223c {0, ..., T } followed by sampling \u03b3 \u223c U (\u03b3 t\u22121 , \u03b3 t ). We set T = 2000 in all our experiments.\n\nPrior work of diffusion models [17,51] require 1-2k diffusion steps during inference, making generation slow for large target resolution tasks. We adapt techniques from [6] to enable more efficient inference. Our model conditions on \u03b3 directly (vs t as in [17]), which allows us flexibility in choosing number of diffusion steps, and the noise schedule during inference. This has been demonstrated to work well for speech synthesis [6], but has not been explored for images. For efficient inference we set the maximum inference budget to 100 diffusion steps, and hyper-parameter search over the inference noise schedule. This search is inexpensive as we only need to train the model once [6]. We use FID on held out data to choose the best noise schedule, as we found PSNR did not correlate well with image quality.\n\n\nRelated Work\n\nSR3 is inspired by recent work on deep generative models and recent learning-based approaches to super-resolution. Generative Models. Autoregressive models (ARs) [55,45] can model exact data log likelihood, capturing rich distributions. However, their sequential generation of pix-els is expensive, limiting application to low-resolution images. Normalizing flows [40,10,23] improve on sampling speed while modelling the exact data likelihood, but the need for invertible parameterized transformations with a tractable Jacobian determinant limits their expressiveness. VAEs [24,41] offer fast sampling, but tend to underperform GANs and ARs in image quality [54]. Generative Adversarial Networks (GANs) [14] are popular for class conditional image generation and super-resolution. Nevertheless, the inner-outer loop optimization often requires tricks to stabilize training [2,15], and conditional tasks like super-resolution usually require an auxiliary consistencybased loss to avoid mode collapse [25]. Cascades of GAN models have been used to generate higher resolution images [9].\n\nScore matching [18] models the gradient of the data logdensity with respect to the image. Score matching on noisy data, called denoising score matching [58], is equivalent to training a denoising autoencoder, and to DDPMs [17]. Denoising score matching over multiple noise scales with Langevin dynamics sampling from the learned score functions has recently been shown to be effective for high quality unconditional image generation [49,17]. These models have also been generalized to continuous time [51]. Denoising score matching and diffusion models have also found success in shape generation [5], and speech synthesis [6]. We extend this method to super-resolution, with a simple learning objective, a constant number of inference generation steps, and high quality generation.\n\nSuper-Resolution. Numerous super-resolution methods have been proposed in the computer vision community [11,1,22,53,25,44]. Much of the early work on superresolution is regression based and trained with an MSE loss [11,1,60,12,21]. As such, they effectively estimate the posterior mean, yielding blurry images when the posterior is multi-modal [25,44,28]. Our regression baseline defined below is also a one-step regression model trained with MSE (cf. [1,21]), but with a large U-Net architecture. SR3, by comparison, relies on a series of iterative refinement steps, each of which is trained with a regression loss. This difference permits our iterative approach to capture richer distributions. Further, rather than estimating the posterior mean, SR3 generates samples from the target posterior.\n\nAutoregressive models have been used successfully for super-resolution and cascaded up-sampling [8,27,56,33]. Nevertheless, the expensive of inference limits their applicability to low-resolution images. SR3 can generate highresolution images, e.g., 1024\u00d71024, but with a constant number of refinement steps (often no more than 100).\n\nNormalizing flows have been used for super-resolution with a multi-scale approach [62]. They are capable of generating 1024\u00d71024 images due in part to their efficient inference process. But SR3 uses a series of reverse diffusion steps to transform a Gaussian distribution to an image distribution while flows require a deep and invertible network.\n\nGAN-based super-resolution methods have also found considerable success [19,25,28,61,44]. FSRGAN [7] and PULSE [28] in particular have demonstrated high quality face super-resolution results. However, many such GAN based methods are generally difficult to optimize, and often require auxiliary objective functions to ensure consistency with the low resolution inputs.\n\n\nExperiments\n\nWe assess the effectiveness of SR3 models in superresolution on faces, natural images, and synthetic images obtained from a low-resolution generative model. The latter enables high-resolution image synthesis using model cascades. We compare SR3 with recent methods such as FSR-GAN [7] and PULSE [28] using human evaluation 1 , and report FID for various tasks. We also compare to a regression 1 Samples generously provided by the authors of [28] baseline model that shares the same architecture as SR3, but is trained with a MSE loss. Our experiments include: \u2022 Face super-resolution at 16\u00d716 \u2192 128\u00d7128 and 64\u00d764 \u2192 512 \u00d7 1512 trained on FFHQ and evaluated on CelebA-HQ. \u2022 Natural image super-resolution at 64 \u00d7 64 \u2192 256 \u00d7 256 pixels on ImageNet [43]. \u2022 Unconditional 1024\u00d71024 face generation by a cascade of 3 models, and class-conditional 256 \u00d7 256 ImageNet image generation by a cascade of 2 models.\n\n\nDatasets:\n\nWe follow previous work [28], training face super-resolution models on Flickr-Faces-HQ (FFHQ) [20] and evaluating on CelebA-HQ [19]. For natural image super-resolution, we train on ImageNet 1K [43] and use the dev split for evaluation. We train unconditional face and class-conditional ImageNet generative models using DDPM on the same datasets discussed above. For training and testing, we use low-resolution images that are downsampled using bicubic interpolation with anti-aliasing en- abled. For ImageNet, we discard images where the shorter side is less than the target resolution. We use the largest central crop like [4], which is then resized to the target resolution using area resampling as our high resolution image.\n\nTraining Details: We train all of our SR3 and regression models for 1M training steps with a batch size of 256. We choose a checkpoint for the regression baseline based on peak-PSNR on the held out set. We do not perform any checkpoint selection on SR3 models and simply select the latest checkpoint. Consistent with [17], we use the Adam optimizer with a linear warmup schedule over 10k training steps, followed by a fixed learning rate of 1e-4 for SR3 models and 1e-5 for regression models. We use 625M parameters for our 64 \u00d7 64 \u2192 {256 \u00d7 256, 512 \u00d7 512} models, 550M parameters for the 16\u00d716 \u2192 128\u00d7128 models, and 150M parameters for 256\u00d7256 \u2192 1024\u00d71024 model. We use a dropout rate of 0.2 for 16\u00d716 \u2192 128\u00d7128 models super-resolution, but otherwise, we do not use dropout.\n\n(See Appendix A for task specific architectural details.)\n\n\nQualitative Results\n\nNatural Images: Figure 3 gives examples of superresolution natural images for 64\u00d764 \u2192 256\u00d7256 on the ImageNet dev set, along with enlarged patches for finer inspection. The baseline Regression model generates images that are faithful to the inputs, but are blurry and lack detail. By comparison, SR3 produces sharp images with more detail; this is most evident in the enlarged patches. For more samples see Appendix C.3 and C.4.\n\nFace Images: Figure 4 shows outputs of a face superresolution model (64\u00d764 \u2192 512\u00d7512) on two test images, again with selected patches enlarged. With the 8\u00d7 magnification factor one can clearly see the detailed structure inferred. Note that, because of the large magnification factor, there are many plausible outputs, so we do not expect the output to exactly match the reference image. This is evident in the regions highlighted in the faces. For more samples see Appendix C.1 and C.2. Table 1 shows the PSNR, SSIM [59] and Consistency scores for 16\u00d716 \u2192 128\u00d7128 face super-resolution. SR3 outperforms PULSE and FSRGAN on PSNR and SSIM while underperforming the regression baseline. Previous work [7,8,28] observed that these conventional automated evaluation measures do not correlate well with human perception when the input resolution is low and the magnification factor is large. This is not surprising because these metrics tend to penalize any synthetic high-frequency detail that is not perfectly aligned with the target image. Since generating perfectly aligned high-frequency details, e.g., the exact same hair strands in Figure 4 and identical leopard spots in Figure 3, is almost impossible, PSNR and SSIM tend to prefer MSE regression-based techniques that are extremely conservative with high-frequency details. This is further confirmed in Table 2 for ImageNet super-resolution (64 \u00d7 64 \u2192 256 \u00d7 256) where the outputs of SR3 achieve higher sample quality scores (FID and IS), but worse PSNR and SSIM than regression.\n\n\nBenchmark Comparison\n\n\nAutomated metrics\n\nConsistency: As a measure of the consistentcy of the superresolution outputs, we compute MSE between the downsampled outputs and the low resolution inputs. Table 1 shows that SR3 achieves the best consistency error beating PULSE and FSRGAN by a significant margin slightly outperforming even the regression baseline. This result demonstrates the key advantage of SR3 over state of the art GAN based methods as they do not require any auxiliary objective function in order to ensure consistency with the low resolution inputs.\n\nClassification Accuracy: Table 3 compares our 4\u00d7 natural image super-resolution models with previous work in terms of object classification on low-resolution images. We mirror the evaluation setup of [44,64] and apply 4\u00d7 superresolution models to 56\u00d756 center crops from the validation set of ImageNet. Then, we report classification error based on a pre-trained ResNet-50 [16]. Since, our super-resolution models are trained on the task of 64\u00d764 \u2192 256\u00d7256, we use bicubic interpolation to resize the input 56\u00d756 to 64\u00d764, then we apply 4\u00d7 super-resolution, followed by resizing back to 224\u00d7224. SR3 outperforms existing methods by a large margin on top-1 and top-5 classification errors, demonstrating high perceptual quality of SR3 outputs. The Regression model achieves strong performance compared to existing methods demonstrating the strength of our baseline model. However, SR3 significantly outperforms Regression re-affirming the limitation of conventional metrics such as PSNR and SSIM.   \n\n\nHuman Evaluation (2AFC)\n\nIn this work, we are primarily interested in photo-realistic super-resolution with large magnification factors. Accordingly, we resort to direct human evaluation. While mean opinion score (MOS) is commonly used to measure image quality in this context, forced choice pairwise comparison has been found to be a more reliable method for such subjective quality assessments [26]. Furthermore, standard MOS studies do not capture consistency between lowresolution inputs and high-resolution outputs. We use a 2alternative forced-choice (2AFC) paradigm to measure how well humans can discriminate true images from those generated from a model. In Task-1 subjects were shown a low resolution input in between two high-resolution images, one being the real image (ground truth), and the other generated from the model. Subjects were asked \"Which of the two images is a better high quality version of the low resolution image in the middle?\" This task takes into account both image quality and consistency with the low resolution input. Task-2 is similar to Task-1, except that the low-resolution  image was not shown, so subjects only had to select the image that was more photo-realistic. They were asked \"Which image would you guess is from a camera?\" Subjects viewed images for 3 seconds before responding, in both tasks. The source code for human evaluation can be found here 2 . The subject fool rate is the fraction of trials on which a subject selects the model output over ground truth. Our fool rates for each model are based on 50 subjects, each of whom were shown 50 of the 100 images in the test set. Figure 6 shows the fool rates for Task-1 (top), and for Task-2 (bottom). In both experiments, the fool rate of SR3 is close to 50%, indicating that SR3 produces images that are both photo-realistic and faithful to the low-resolution inputs. We find similar fool rates over a wide range of viewing durations up to 12 seconds.\n\nThe fool rates for FSRGAN and PULSE in Task-1 are lower than the Regression baseline and SR3. We speculate that the PULSE optimization has failed to converge to high resolution images sufficiently close to the inputs. Indeed, 2 https://tinyurl.com/sr3-human-eval-code when asked solely about image quality in Task-2 ( Fig. 6 (bottom)), the PULSE fool rate increases significantly. The fool rate for the Regression baseline is lower in Task-2 ( Fig. 6 (bottom)) than Task-1. The regression model tends to generate images that are blurry, but nevertheless faithful to the low resolution input. We speculate that in Task-1, given the inputs, subjects are influenced by consistency, while in Task-2, ignoring consistency, they instead focus on image sharpness. SR3 and Regression samples used for human evaluation are provided here 3 .\n\nWe conduct similar human evaluation studies on natural images comparing SR3 and the regression baseline on Ima-geNet. Figure 7 shows the results for Task-1 (top) and task-2 (bottom). In both tasks with natural images, SR3 achieves a human subject fool rate is close to 40%. Like the face image experiments in Fig. 6, here again we find that the Regression baseline yields a lower fool rate in Task-2, where the low resolution image is not shown. Again we speculate that this is a result of a somewhat simpler task (looking at 2 rather than 3 images), and the fact that subjects can focus solely on image artifacts, such as blurriness, without having to worry about consistency between model output and the low resolution input.\n\nTo further appreciate the experimental results it is use-\nModel FID-50k\nPrior Work VQ-VAE-2 [39] 38.1 BigGAN (Truncation 1.0) [4] 7.4 BigGAN (Truncation 1.5) [4] 11.8 Our Work SR3 (Two Stage) 11.3 Table 4: FID scores for class-conditional 256\u00d7256 ImageNet.\n\nful to visually compare outputs of different models on the same inputs, as in Figure 5. FSRGAN exhibits distortion in face region and struggles with generating glasses properly (e.g., top row). It also fails to recover texture details in the hair region (see bottom row). PULSE often produces images that differ significantly from the input image, both in the shape of the face and the background, and sometimes in gender too (see bottom row) presumably due to failure of the optimization to find a sufficiently good minima. As noted above, our Regression baseline produces results consistent to the input, however they are typically quite blurry. By comparison, the SR3 results are consistent with the input and contain more detailed image structure.\n\n\nCascaded High-Resolution Image Synthesis\n\nWe study cascaded image generation, where SR3 models at different scales are chained together with unconditional generative models, enabling high-resolution image synthesis. Cascaded generation allows one to train different models in parallel, and each model in the cascade solves a simpler task, requiring fewer parameters and less computation for training. Inference with cascaded models is also more efficient, especially for iterative refinement models. With cascaded generation we found it effective to use more refinement steps at low-resolutions, and fewer steps at higher resolutions. This was much more efficient than generating directly at high resolution without sacrificing image quality.\n\nWe train a DDPM [17] model for unconditional 64\u00d764 face generation. Samples from this model are then fed to two 4\u00d7 SR3 models, up-sampling to 256 2 and then to 1024 2 pixels. Synthetic high-resolution face samples are shown in Figure 8. In addition, we train an Improved DDPM [30] model on class-conditional 64\u00d764 ImageNet, and we pass its generated samples to a 4\u00d7 SR3 model yielding 256 2 pixels. The 4\u00d7 SR3 model is not conditioned on the class label. See Figure 9 for representative samples. Table 4 reports FID scores for the resulting classconditional ImageNet samples. Our 2-stage model improves on VQ-VAE-2 [39], is comparable to deep BigGANs [4] at truncation factor of 1.5 but underperforms them a truncation factor of 1.0. Unlike BigGAN, our diffusion models do not provide a knob to control sample quality vs. sample diversity, and finding ways to do so is interesting avenue for future research. Nichol and Dhariwal [30] con- currently trained cascaded generation models using superresolution conditioned on class labels (our super-resolution is not conditioned on class labels), and observed a similar trend in FID scores. The effectiveness of cascaded image generation indicates that SR3 models are robust to the precise distribution of inputs (i.e., the specific form of antialiasing and downsampling).\n\nAblation Studies: Table 5 shows ablation studies on our 64 \u00d7 64 \u2192 256 \u00d7 256 Imagenet SR3 model. In order to improve the robustness of the SR3 model, we experiment with use of data augmentation while training. Specifically, we trained the model with varying amounts of Gaussian Blurring noise added to the low resolution input image. No blurring is applied during inference. We find that this has a siginificant impact, improving the FID score roughly by 2 points. We also explore the choice of L p norm for the denoising objective (Equation 6). We find that L 1 norm gives slightly better FID scores than L 2 . Figure 9: Synthetic 256\u00d7256 ImageNet images. We first draw a random label, then sample a 64\u00d764 image from a classconditional diffusion model, and apply a 4\u00d7 SR3 model to obtain 256\u00d7256 images. Additional samples in Appendix C.10 and C.11.\n\n\nModel\n\nFID-50k\n\nTraining with Augmentation SR3 13.1 SR3 (w/ Gaussian Blur) 11.3\n\nObjective Lp Norm SR3 (L2) 11.8 SR3 (L1) 11.3 Table 5: Ablation study on SR3 model for class-conditional 256\u00d7256 ImageNet.\n\n\nDiscussion and Conclusion\n\nBias is an important problem in all generative models. SR3 is no different, and suffers from bias issues. While in theory, our log-likelihood based objective is mode covering (e.g., unlike some GAN-based objectives), we believe it is likely our diffusion-based models drop modes. We observed some evidence of mode dropping, the model consistently generates nearly the same image output during sampling (when conditioned on the same input). We also observed the model to generate very continuous skin texture in face super-resolution, dropping moles, pimples and piercings found in the reference. SR3 should not be used for any real world super-resolution tasks, until these biases are thoroughly understood and mitigated.\n\nIn conclusion, SR3 is an approach to image superresolution via iterative refinement. SR3 can be used in a cascaded fashion to generate high resolution super-resolution images, as well as unconditional samples when cascaded with a unconditional model. We demonstrate SR3 on face and natural image super-resolution at high resolution and high magnification ratios (e.g., 64\u00d764\u2192256\u00d7256 and 256\u00d7256\u21921024\u00d71024). SR3 achieves a human fool rate close to 50%, suggesting photo-realistic outputs.   \n\n\n. A Variational Bound Perspective\n\nFollowing Ho et al. [17], we justify the choice of the training objective in (6) for the probabilistic model outlined in (9) from a variational lower bound perspective. If the forward diffusion process is viewed as a fixed approximate posterior to the inference process, one can derive the following variational lower bound on the marginal log-likelihood:\n\nE (x,y0) log p \u03b8 (y 0 |x) \u2265 E x,y0 E q(y 1:T |y0) log p(y T ) + t\u22651 log p \u03b8 (y t\u22121 |y t , x) q(y t |y t\u22121 ) .\n\nGiven the particular parameterization of the inference process outlined above, one can show [17] that the negative variational lower bound can be expressed as the following simplified loss, up to a constant weighting of each term for each time step:\nE x,y0, T t=1 1 T \u2212 \u03b8 (x, \u221a \u03b3 t y 0 + 1 \u2212 \u03b3 t , \u03b3 t ) 2 2(13)\nwhere \u223c N (0, I). Note that this objective function corresponds to L 2 norm in (6), and a characterization of p(\u03b3) in terms of a uniform distribution over {\u03b3 1 , . . . , \u03b3 T }.\n\n\nB.2. A Denoising Score-Matching Perspective\n\nOur approach is also linked to denoising score matching [18,58,37,46] for training unnormalized energy functions for density estimation. These methods learn a parametric score function to approximate the gradient of the empirical data logdensity. To make sure that the gradient of the data log-density is well-defined, one often replaces each data point with a Gaussian distribution with a small variance. Song and Ermon [50] advocate for the use of a Multi-scale Guassian mixture as the target density, where each data point is perturbed with different amounts of Gaussian noise, so that Langevin dynamics starting from pure noise can still yield reasonable samples.\n\nOne can view our approach as a variant of denoising score matching in which the target density is given by a mixture of q( y|y 0 , \u03b3) = N ( y | \u221a \u03b3y 0 , 1 \u2212 \u03b3) for different values of y 0 and \u03b3. Accordingly, the gradient of data log-density is given by\nd log q( y | y 0 , \u03b3) d y = \u2212 y \u2212 \u221a \u03b3y 0 \u221a 1 \u2212 \u03b3 = \u2212 ,(14)\nwhich is used as the regression target of our model.\n\n\nC. Additional Experimental Results\n\nThe \n\n\nC.1. Failure Cases of SR3\n\nWhile SR3 generates high quality super-resolution natural images as well as aligned face images, we do observe certain instances where the model falls short. In this section, we highlight some of these examples.\n\nSR3 struggles with generating certain complex, regular hair patterns, an example of which is the finely braided hair in the top row of Figure C.2. Since the FFHQ dataset used for training is relatively small, it is possible that the model is not exposed to enough examples of such structures. Long range correlations of fine details such as the consistency of highlights in eyes can also be challenging. As shown in the 2nd row of Figure C.2, the model also struggles with generating eye-glasses in certain cases, especially when the glasses are frameless. In such cases the structure of the glasses is much harder to discern from the low resolution inputs.\n\nSR3 also fails to generate natural looking text in certain ImageNet images. While it has learned to capture some properties of text, it has not learned common alphabets. So while SR3 is able to generate much sharper characters compared to the regression baseline, the lack of meaningful structure of its generated text makes it easier for the human subjects to distinguish between real and generated images. The last row in Figure C.3 shows one such instance.\n\nThe bottom row in Figure C.4 shows an instance where the model has not correctly inferred the fine structure on the side of the building. When the low resolution input does not reflect many of the fine details in the high resolution original, SR3 will infer structure. In some cases it will introduce texture (e.g., the collar of the shirt in Figure 1). In others, like the building here it may infer a more uniform texture. As such, while the SR3 output in the bottom row of Figure C.4 is much sharper than the regression baseline, it also lacks many perceptually relevant details when compared with the reference image.\n\n\nD. Images with the Lowest and Highest Fool Rates\n\nIn interpreting the fool rate results in Figure 6, it is interesting to inspect those images that maximize the fool rates for a given technique, as well as those images that minimize the fool rate. This provides insight into the nature of the problems that models exhibit, as well as cases in which the model outputs are good enough to regularly fool people.\n\nIn Figure D.1 we display the images with the lowest fool rates generated by PULSE [28] and SR3 for both Task-1 (the conditional task), and Task-2, (the unconditional task). In order to be consistent with our human study interface, we show the corresponding low resolution image only for Task-1. Notice that images from PULSE for which the fool rate is low have obvious distortions, and the fool rates are lower than 10% for both tasks. For SR3, by comparison, the images with the lowest fool rates are still reasonably good, with much higher fool rates of 14% and 19% in Task-1, and 21% and 26% in Task-2. Figure D.2 shows images that best fool human subjects. In this case, it is interesting to note that the best fool rates for SR3 are 84% and 88%. The corresponding original images are somewhat noisy, and as a consequence, many subjects refer the SR3 outputs.  Examples with lowest and highest fool rates for PULSE and SR3 based on Task-2. Task-2 involves comparing the outputs of each algorithm with reference high-resolution images in the absence of low-resolution inputs, but for privacy reasons reference images are not included. Instead, we show the corresponding outputs from PULSE and SR3 for each input image and report the Mean Fool Rate for each image right below it.\n\n\nTask-1: Human Evaluation given low-resolution inputs\n\nFigure 1 :\n1Two representative SR3 outputs: (top) 8\u00d7 face superresolution at 16\u00d716\u2192128\u00d7128 pixels (bottom) 4\u00d7 natural image super-resolution at 64\u00d764\u2192256\u00d7256 pixels.\n\nFigure 3 :\n3Results of a SR3 model (64\u00d764 \u2192 256\u00d7256), trained on ImageNet and evaluated on two ImageNet test images. For each we also show an enlarged patch in which finer details are more apparent. Additional samples are shown in Appendix C.3 and C.4.\n\nFigure 4 :\n4Results of a SR3 model (64\u00d764 \u2192 512\u00d7512), trained on FFHQ, and applied to images outside of the training set, along with enlarged patches to show finer details. Additional results are shown in Appendix C.1 and C.2.\n\nFigure 6 :\n6Face super-resolution human fool rates (higher is better, photo-realistic samples yield a fool rate of 50%). Outputs of 4 models are compared against ground truth. (top) Subjects are shown low-resolution inputs. (bottom) Inputs are not shown.\n\nFigure 7 :\n7ImageNet super-resolution fool rates (higher is better, photo-realistic samples yield a fool rate of 50%). SR3 and Regression outputs are compared against ground truth. (top) Subjects are shown low-resolution inputs. (bottom) Inputs are not shown.\n\nFigure 8 :\n8Synthetic 1024\u00d71024 face images. We first sample from an unconditional 64\u00d764 diffusion model, then pass the samples through two 4\u00d7 SR3 models, i.e., 64\u00d764 \u2192 256\u00d7256 \u2192 1024\u00d71024. Additional samples in Appendix C.7, C.8 and C.9.\n\nFigure A. 1 :\n1Description of the U-Net architecture with skip connections. The low resolution input image x is interpolated to the target high resolution, and concatenated with the noisy high resolution image yt. We show the activation dimensions for the example task of 16\u00d716 \u2192 128\u00d7128 super resolution. B. Justification of the Training Objective B.1\n\nFigure D. 2 :\n2Figure D.2: Examples with lowest and highest fool rates for PULSE and SR3 based on Task-2. Task-2 involves comparing the outputs of each algorithm with reference high-resolution images in the absence of low-resolution inputs, but for privacy reasons reference images are not included. Instead, we show the corresponding outputs from PULSE and SR3 for each input image and report the Mean Fool Rate for each image right below it.\n\nTable 2 :\n2Performance comparison between SR3 and Regression baseline on natural image super-resolution using standard metrics computed on the ImageNet validation set.Method \nTop-1 Error Top-5 Error \n\nBaseline \n0.252 \n0.080 \n\nDRCN [22] \n0.477 \n0.242 \nFSRCNN [13] \n0.437 \n0.196 \nPsyCo [35] \n0.454 \n0.224 \nENet-E [44] \n0.449 \n0.214 \nRCAN [64] \n0.393 \n0.167 \n\nRegression \n0.383 \n0.173 \nSR3 \n0.317 \n0.120 \n\n\n\nTable 3 :\n3Comparison of classification accuracy scores for 4\u00d7 natural image super-resolution on the first 1K images from the Ima-geNet Validation set.\n\nTable A .\nA1 summarizes the primary architecture details for each super-resolution task. For a particular task, we use the same architecture for both SR3 and Regression models.Figure A.1 describes our method of conditioning the diffusion model on the low resolution image. We first interpolate the low resolution image to the target high resolution, and then simply concatenate it with the input noisy high resolution image.Task \nChannel Dim \nDepth Multipliers \n# ResNet Blocks # Parameters \n16 \u00d7 16 \u2192 128 \u00d7 128 \n128 \n{1, 2, 4, 8, 8} \n3 \n550M \n64 \u00d7 64 \u2192 256 \u00d7 256 \n128 \n{1, 2, 4, 4, 8, 8} \n3 \n625M \n64 \u00d7 64 \u2192 512 \u00d7 512 \n64 \n{1, 2, 4, 8, 8, 16, 16} \n3 \n625M \n256 \u00d7 256 \u2192 1024 \u00d7 1024 \n16 \n{1, 2, 4, 8, 16, 32, 32, 32} \n2 \n150M \n\n\n\nTable A . 1 :\nA1Task specific architecture hyper-parameters for the U-Net model. Channel Dim is the dimension of the first U-Net layer, while the depth multipliers are the multipliers for subsequent resolutions.x \u00b7 y t 128 2 , 128 \n\n64 2 , 256 \n\n8 2 , 1024 \n8 2 , 1024 \n\n64 2 , 256 \n\n128 2 , 128 y t\u22121 \n\n\n\n\nfollowing figures show more examples of SR3 on faces, natural images, and samples from unconditional generativeFigure C.11: Classwise Synthetic 256\u00d7256 ImageNet images. Each row represents a specific ImageNet class. Classes from top to bottom -Goldfish, Indigo Bird, Red Fox, Monarch Butterfly, African Elephant, Balloon, Church, Fire Truck. For a given label, we sample a 64\u00d764 image from a class-conditional diffusion model, and apply a 4\u00d7 SR3 model to obtain 256\u00d7256 images.\n\n\nLowest Mean Fool Rates for PULSE Lowest Mean Fool Rates for SR3 Highest Mean Fool Rates for PULSE Highest Mean Fool Rates for SR3 Figure D.1: Examples with lowest and highest fool rates for PULSE and SR3 based on Task-1. Task-1 involves comparing the outputs of each algorithm with reference high-resolution images in the presence of low-resolution inputs, but for privacy reasons reference images are not included. Instead, we show the corresponding outputs from PULSE and SR3 for each input image and report the Mean Fool Rate for each image right below it. Task-2: Human Evaluation without low-resolution inputs Lowest Mean Fool Rates for PULSE Lowest Mean Fool Rates for SR3 Highest Mean Fool Rates for PULSE Highest Mean Fool Rates for SR3PULSE \nInput \nSR3 \nSR3 \nInput \nPULSE \n\nFool Rate: 0% \nFool Rate: 53.4% \nFool Rate: 14% \nFool Rate: 4.5% \n\nFool Rate: 2.2% \nFool Rate: 60.4% \nFool Rate: 18.6% \nFool Rate: 11.4% \n\nPULSE \nInput \nSR3 \nSR3 \nInput \nPULSE \n\nFool Rate: 63.4% \nFool Rate: 62.7% \nFool Rate: 88.3% \nFool Rate: 38.6% \n\nFool Rate: 63.4% \nFool Rate: 69.7% \nFool Rate: 83.7% \nFool Rate: 54.5% \n\nPULSE \nSR3 \nSR3 \nPULSE \n\nFool Rate: 8.9% \nFool Rate: 19% \nFool Rate: 21.4% \nFool Rate: 15.5% \n\nFool Rate: 8.9% \nFool Rate: 54.8% \nFool Rate: 26.2% \nFool Rate: 31.1% \n\nPULSE \nSR3 \nSR3 \nPULSE \n\nFool Rate: 75.5% \nFool Rate: 61.9% \nFool Rate: 78.5% \nFool Rate: 35.5% \n\nFool Rate: 66.7% \nFool Rate: 47.6% \nFool Rate: 66.7% \nFool Rate: 55.6% \n\n\nhttps://tinyurl.com/sr3-outputs\nAcknowledgementsWe thank Jimmy Ba, Adji Bousso Dieng, Chelsea Finn, Geoffrey Hinton, Natacha Mainville, Shingai Manjengwa, and Ali Punjani for providing their face images on which we demonstrate the face SR3 results. We thank Ben Poole, Samy Bengio and the Google Brain team for research discussions and technical assistance. We also thank authors of[28]for generously providing us with baseline superresolution samples for human evaluation.AppendixThis appendix includes further details about the architecture of the models used for super-resolution. It also formulates the training objective in terms of a variation bound and in terms of denoising score-matching. We then provide additional experimental results to complement those in the main body of the paper.\nImage Super-resolution via Progressive Cascading Residual Network. Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn, CVPR. Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Im- age Super-resolution via Progressive Cascading Residual Network. In CVPR, 2018.\n\n. Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou, Gan Wasserstein, Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein GAN. In arXiv, 2017.\n\nCreating high resolution images with a latent adversarial generator. David Berthelot, Peyman Milanfar, Ian Goodfellow, arXiv preprintDavid Berthelot, Peyman Milanfar, and Ian Goodfellow. Creating high resolution images with a latent adversarial generator. arXiv preprint 2003.02365, 2020.\n\nLarge scale GAN training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, arXiv:1809.11096arXiv preprintAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nLearning Gradient Fields for Shape Generation. Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, Bharath Hariharan, ECCV. Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning Gradient Fields for Shape Generation. In ECCV, 2020.\n\nWaveGrad: Estimating Gradients for Waveform Generation. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, William Chan, ICLR. 2021Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Moham- mad Norouzi, and William Chan. WaveGrad: Estimating Gradients for Waveform Generation. In ICLR, 2021.\n\nFsrnet: End-to-end learning face super-resolution with facial priors. Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionYu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. Fsrnet: End-to-end learning face super-resolution with facial priors. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages 2492-2501, 2018.\n\nPixel recursive super resolution. Ryan Dahl, Mohammad Norouzi, Jonathon Shlens, ICCV. Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In ICCV, 2017.\n\nDeep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus, NIPS. Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. In NIPS, 2015.\n\nLaurent Dinh, arXiv:1605.08803Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv:1605.08803, 2016.\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, European conference on computer vision. SpringerChao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In European conference on computer vi- sion, pages 184-199. Springer, 2014.\n\nImage super-resolution using deep convolutional networks. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, IEEE transactions on pattern analysis and machine intelligence. 38Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional net- works. IEEE transactions on pattern analysis and machine intelligence, 38(2):295-307, 2015.\n\nAccelerating the super-resolution convolutional neural network. Chao Dong, Chen Change Loy, Xiaoou Tang, European conference on computer vision. SpringerChao Dong, Chen Change Loy, and Xiaoou Tang. Acceler- ating the super-resolution convolutional neural network. In European conference on computer vision, pages 391-407. Springer, 2016.\n\n. J Ian, Jean Goodfellow, Mehdi Pouget-Abadie, Bing Mirza, David Xu, Sherjil Warde-Farley, Aaron Ozair, Yoshua Courville, Bengio, Generative Adversarial Networks. NIPS. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. NIPS, 2014.\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville, arXiv:1704.00028Improved training of wasserstein gans. arXiv preprintIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, NeurIPS. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. In NeurIPS, 2020.\n\nEstimation of nonnormalized statistical models by score matching. Aapo Hyv\u00e4rinen, Peter Dayan, JMLR. 64Aapo Hyv\u00e4rinen and Peter Dayan. Estimation of non- normalized statistical models by score matching. JMLR, 6(4), 2005.\n\nProgressive growing of gans for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, ICLR. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018.\n\nA Style-Based Generator Architecture for Generative Adversarial Networks. Tero Karras, Samuli Laine, Timo Aila, CVPR. Tero Karras, Samuli Laine, and Timo Aila. A Style- Based Generator Architecture for Generative Adversarial Networks. In CVPR, 2019.\n\nAccurate image super-resolution using very deep convolutional networks. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, CVPR. Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional net- works. In CVPR, 2016.\n\nDeeplyrecursive convolutional network for image super-resolution. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply- recursive convolutional network for image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1637-1645, 2016.\n\nGlow: Generative Flow with Invertible 1x1 Convolutions. P Diederik, Prafulla Kingma, Dhariwal, NIPS. Diederik P. Kingma and Prafulla Dhariwal. Glow: Genera- tive Flow with Invertible 1x1 Convolutions. In NIPS, 2018.\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, ICLR. Diederik P Kingma and Max Welling. Auto-Encoding Vari- ational Bayes. In ICLR, 2013.\n\nPhotorealistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, ICCV. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative ad- versarial network. In ICCV, 2017.\n\nComparison of four subjective methods for image quality assessment. K Rafa\u0142, Anna Mantiuk, Rados\u0142aw Tomaszewska, Mantiuk, Computer graphics forum. Wiley Online Library31Rafa\u0142 K Mantiuk, Anna Tomaszewska, and Rados\u0142aw Man- tiuk. Comparison of four subjective methods for image qual- ity assessment. In Computer graphics forum, volume 31, pages 2478-2491. Wiley Online Library, 2012.\n\nGenerating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling. Jacob Menick, Nal Kalchbrenner, ICLR. Jacob Menick and Nal Kalchbrenner. Generating High Fi- delity Images with Subscale Pixel Networks and Multidi- mensional Upscaling. In ICLR, 2019.\n\nPULSE: Self-supervised photo upsampling via latent space exploration of generative models. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin, CVPR. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsam- pling via latent space exploration of generative models. In CVPR, 2020.\n\nLuke Metz, Ben Poole, David Pfau, Jascha Sohl-Dickstein, arXiv:1611.02163Unrolled generative adversarial networks. arXiv preprintLuke Metz, Ben Poole, David Pfau, and Jascha Sohl- Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.\n\nImproved denoising diffusion probabilistic models. Alex Nichol, Prafulla Dhariwal, arXiv:2102.09672arXiv preprintAlex Nichol and Prafulla Dhariwal. Improved de- noising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021.\n\nA\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, arXiv:1609.03499Andrew Senior, and Koray Kavukcuoglu. WaveNet: A Generative Model for Raw Audio. arXiv preprintNal KalchbrennerA\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbren- ner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A Generative Model for Raw Audio. arXiv preprint arXiv:1609.03499, 2016.\n\nConditional Image Generation with PixelCNN Decoders. A\u00e4ron Van Den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu, NIPS. A\u00e4ron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Condi- tional Image Generation with PixelCNN Decoders. In NIPS, 2016.\n\nImage transformer. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran, International Conference on Machine Learning. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im- age transformer. In International Conference on Machine Learning, 2018.\n\nFiLM: Visual Reasoning with a General Conditioning Layer. Ethan Perez, Florian Strub, Harm De, Vincent Vries, Aaron Dumoulin, Courville, AAAI. Ethan Perez, Florian Strub, Harm De Vries, Vincent Du- moulin, and Aaron Courville. FiLM: Visual Reasoning with a General Conditioning Layer . In AAAI, 2018.\n\nPsyco: Manifold span reduction for super resolution. Eduardo P\u00e9rez-Pellitero, Jordi Salvador, J Hidalgo, B Rosenhahn, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Eduardo P\u00e9rez-Pellitero, Jordi Salvador, J. Hidalgo, and B. Rosenhahn. Psyco: Manifold span reduction for super reso- lution. 2016 IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 1837-1845, 2016.\n\nUnsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, arXiv:1511.06434arXiv preprintAlec Radford, Luke Metz, and Soumith Chintala. Un- supervised representation learning with deep convolu- tional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n\nLeast squares estimation without priors or supervision. Martin Raphan, P Eero, Simoncelli, Neural computation. 232Martin Raphan and Eero P Simoncelli. Least squares esti- mation without priors or supervision. Neural computation, 23(2):374-420, 2011.\n\nClassification accuracy score for conditional generative models. Suman Ravuri, Oriol Vinyals, arXiv:1905.10887arXiv preprintSuman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. arXiv preprint arXiv:1905.10887, 2019.\n\nGenerating diverse high-fidelity images with vq-vae-2. Ali Razavi, Aaron Van Den Oord, Oriol Vinyals, arXiv:1906.00446arXiv preprintAli Razavi, Aaron van den Oord, and Oriol Vinyals. Gen- erating diverse high-fidelity images with vq-vae-2. arXiv preprint arXiv:1906.00446, 2019.\n\nVariational inference with normalizing flows. Danilo Rezende, Shakir Mohamed, International Conference on Machine Learning. PMLRDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Ma- chine Learning, pages 1530-1538. PMLR, 2015.\n\nStochastic backpropagation and approximate inference in deep generative models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, International conference on machine learning. PMLRDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wier- stra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278-1286. PMLR, 2014.\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, 2015.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International journal of computer vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.\n\nEnhancenet: Single image super-resolution through automated texture synthesis. S M Mehdi, Bernhard Sajjadi, Michael Scholkopf, Hirsch, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionMehdi SM Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. In Proceedings of the IEEE International Conference on Computer Vision, pages 4491- 4500, 2017.\n\nPixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P Kingma, International Conference on Learning Representations. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with dis- cretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2017.\n\n. Saeed Saremi, Arash Mehrjou, Bernhard Sch\u00f6lkopf, Aapo Hyv\u00e4rinen, arXiv:1805.08306Deep Energy Estimator Networks. arXiv preprintSaeed Saremi, Arash Mehrjou, Bernhard Sch\u00f6lkopf, and Aapo Hyv\u00e4rinen. Deep Energy Estimator Networks. arXiv preprint arXiv:1805.08306, 2018.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, ICML. PMLRJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pages 2256- 2265. PMLR, 2015.\n\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics. Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, Surya Ganguli, ICML. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah- eswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In ICML, 2015.\n\nGenerative Modeling by Estimating Gradients of the Data Distribution. Yang Song, Stefano Ermon, NeurIPS. Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. In NeurIPS, 2019.\n\nImproved Techniques for Training Score-Based Generative Models. Yang Song, Stefano Ermon, arXiv:2006.09011arXiv preprintYang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. arXiv preprint arXiv:2006.09011, 2020.\n\nScore-Based Generative Modeling through Stochastic Differential Equations. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole, ICLR. 2021Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab- hishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equa- tions. In ICLR, 2021.\n\nSequence to Sequence Learning with Neural Networks. Ilya Sutskever, Oriol Vinyals, Quoc Le, NIPS. Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to Sequence Learning with Neural Networks. In NIPS, 2014.\n\nImage superresolution via deep recursive residual network. Ying Tai, Jian Yang, Xiaoming Liu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYing Tai, Jian Yang, and Xiaoming Liu. Image super- resolution via deep recursive residual network. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 3147-3155, 2017.\n\nNVAE: A deep hierarchical variational autoencoder. Arash Vahdat, Jan Kautz, NeurIPS. Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020.\n\nAaron Van Den Oord, Nal Kalchbrenner, Koray Kavukcuoglu, Pixel recurrent neural networks. International Conference on Machine Learning. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. International Conference on Machine Learning, 2016.\n\nConditional image generation with PixelCNN decoders. Aaron Van Den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu, Advances in Neural Information Processing Systems. Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Condi- tional image generation with PixelCNN decoders. In Ad- vances in Neural Information Processing Systems, pages 4790-4798, 2016.\n\nAttention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In NIPS, 2017.\n\nA connection between score matching and denoising autoencoders. Pascal Vincent, Neural Computation. 237Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661- 1674, 2011.\n\nImage quality assessment: from error visibility to structural similarity. A C Zhou Wang, H R Bovik, E P Sheikh, Simoncelli, IEEE Transactions on Image Processing. 134Zhou Wang, A.C. Bovik, H.R. Sheikh., and E.P. Simon- celli. Image quality assessment: from error visibility to struc- tural similarity. IEEE Transactions on Image Processing, 13(4):600-612, 2004.\n\nDeep networks for image super-resolution with sparse prior. Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, Thomas Huang, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionZhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, and Thomas Huang. Deep networks for image super-resolution with sparse prior. In Proceedings of the IEEE international conference on computer vision, pages 370-378, 2015.\n\nHiFaceGAN: Face Renovation via Collaborative Suppression and Replenishment. Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siwei Ma, Wen Gao, arXivLingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siwei Ma, and Wen Gao. HiFaceGAN: Face Renova- tion via Collaborative Suppression and Replenishment. In arXiv, 2020.\n\nJason J Yu, Konstantinos G Derpanis, Marcus A Brubaker, Wavelet Flow: Fast Training of High Resolution Normalizing Flows. In arXiv. Jason J. Yu, Konstantinos G. Derpanis, and Marcus A. Brubaker. Wavelet Flow: Fast Training of High Resolution Normalizing Flows. In arXiv, 2020.\n\nColorful image colorization. Richard Zhang, Phillip Isola, Alexei A Efros, ECCV. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European conference on computer vision (ECCV), pages 286-301, 2018.\n", "annotations": {"author": "[{\"end\":115,\"start\":51},{\"end\":178,\"start\":116},{\"end\":243,\"start\":179},{\"end\":305,\"start\":244},{\"end\":370,\"start\":306},{\"end\":436,\"start\":371}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":59},{\"end\":127,\"start\":125},{\"end\":191,\"start\":187},{\"end\":256,\"start\":248},{\"end\":319,\"start\":314},{\"end\":387,\"start\":380}]", "author_first_name": "[{\"end\":58,\"start\":51},{\"end\":124,\"start\":116},{\"end\":186,\"start\":179},{\"end\":247,\"start\":244},{\"end\":311,\"start\":306},{\"end\":313,\"start\":312},{\"end\":379,\"start\":371}]", "author_affiliation": "[{\"end\":114,\"start\":88},{\"end\":177,\"start\":151},{\"end\":242,\"start\":216},{\"end\":304,\"start\":278},{\"end\":369,\"start\":343},{\"end\":435,\"start\":409}]", "title": "[{\"end\":48,\"start\":1},{\"end\":484,\"start\":437}]", "venue": null, "abstract": "[{\"end\":1444,\"start\":486}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2366,\"start\":2362},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2369,\"start\":2366},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2398,\"start\":2394},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2401,\"start\":2398},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2439,\"start\":2435},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2442,\"start\":2439},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2472,\"start\":2468},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2475,\"start\":2472},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2568,\"start\":2564},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2571,\"start\":2568},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2574,\"start\":2571},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2699,\"start\":2696},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2701,\"start\":2699},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2704,\"start\":2701},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2707,\"start\":2704},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2710,\"start\":2707},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3034,\"start\":3031},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3037,\"start\":3034},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3060,\"start\":3056},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3063,\"start\":3060},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3251,\"start\":3247},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3254,\"start\":3251},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3289,\"start\":3285},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3292,\"start\":3289},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3502,\"start\":3498},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4895,\"start\":4892},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4897,\"start\":4895},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4899,\"start\":4897},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4902,\"start\":4899},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":5352,\"start\":5348},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5603,\"start\":5600},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5606,\"start\":5603},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6053,\"start\":6050},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6068,\"start\":6064},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7343,\"start\":7339},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7346,\"start\":7343},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8543,\"start\":8539},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8546,\"start\":8543},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10509,\"start\":10506},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10831,\"start\":10827},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10833,\"start\":10831},{\"end\":11350,\"start\":11348},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12611,\"start\":12607},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13204,\"start\":13200},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13553,\"start\":13549},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14181,\"start\":14177},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14219,\"start\":14215},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14302,\"start\":14299},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14763,\"start\":14759},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14891,\"start\":14888},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15174,\"start\":15170},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15177,\"start\":15174},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15311,\"start\":15308},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15399,\"start\":15395},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15574,\"start\":15571},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15830,\"start\":15827},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16137,\"start\":16133},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16140,\"start\":16137},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16339,\"start\":16335},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16342,\"start\":16339},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16345,\"start\":16342},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16549,\"start\":16545},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16552,\"start\":16549},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16633,\"start\":16629},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16678,\"start\":16674},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16847,\"start\":16844},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16850,\"start\":16847},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16974,\"start\":16970},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17054,\"start\":17051},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17076,\"start\":17072},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":17213,\"start\":17209},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17283,\"start\":17279},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17494,\"start\":17490},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17497,\"start\":17494},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":17562,\"start\":17558},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17657,\"start\":17654},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17683,\"start\":17680},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17949,\"start\":17945},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17951,\"start\":17949},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17954,\"start\":17951},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":17957,\"start\":17954},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17960,\"start\":17957},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17963,\"start\":17960},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18060,\"start\":18056},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18062,\"start\":18060},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":18065,\"start\":18062},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18068,\"start\":18065},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18071,\"start\":18068},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18189,\"start\":18185},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18192,\"start\":18189},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18195,\"start\":18192},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18296,\"start\":18293},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18299,\"start\":18296},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18739,\"start\":18736},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18742,\"start\":18739},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18745,\"start\":18742},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18748,\"start\":18745},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":19061,\"start\":19057},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19400,\"start\":19396},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19403,\"start\":19400},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19406,\"start\":19403},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19409,\"start\":19406},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19412,\"start\":19409},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19424,\"start\":19421},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19439,\"start\":19435},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19991,\"start\":19988},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20006,\"start\":20002},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20101,\"start\":20100},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20152,\"start\":20148},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20456,\"start\":20452},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20651,\"start\":20647},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20721,\"start\":20717},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20754,\"start\":20750},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20820,\"start\":20816},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21250,\"start\":21247},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21673,\"start\":21669},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23160,\"start\":23156},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23341,\"start\":23338},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23343,\"start\":23341},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23346,\"start\":23343},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24948,\"start\":24944},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":24951,\"start\":24948},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25121,\"start\":25117},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26145,\"start\":26141},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28531,\"start\":28530},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29360,\"start\":29356},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29393,\"start\":29390},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29425,\"start\":29422},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31040,\"start\":31036},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31300,\"start\":31296},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31639,\"start\":31635},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31674,\"start\":31671},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31953,\"start\":31949},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34700,\"start\":34696},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35240,\"start\":35236},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35740,\"start\":35736},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":35743,\"start\":35740},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":35746,\"start\":35743},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35749,\"start\":35746},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36105,\"start\":36101},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39239,\"start\":39235}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40656,\"start\":40490},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40910,\"start\":40657},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41138,\"start\":40911},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41394,\"start\":41139},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41655,\"start\":41395},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41895,\"start\":41656},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42249,\"start\":41896},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42694,\"start\":42250},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43099,\"start\":42695},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43252,\"start\":43100},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43981,\"start\":43253},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44287,\"start\":43982},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44767,\"start\":44288},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":46215,\"start\":44768}]", "paragraph": "[{\"end\":2258,\"start\":1460},{\"end\":2528,\"start\":2260},{\"end\":3981,\"start\":2538},{\"end\":4726,\"start\":3983},{\"end\":5640,\"start\":4728},{\"end\":6431,\"start\":5642},{\"end\":6528,\"start\":6473},{\"end\":6964,\"start\":6558},{\"end\":7731,\"start\":7032},{\"end\":8498,\"start\":7733},{\"end\":8688,\"start\":8529},{\"end\":9297,\"start\":8792},{\"end\":9579,\"start\":9434},{\"end\":9992,\"start\":9735},{\"end\":10256,\"start\":10027},{\"end\":10886,\"start\":10295},{\"end\":11428,\"start\":10942},{\"end\":11933,\"start\":11518},{\"end\":12145,\"start\":11972},{\"end\":12942,\"start\":12301},{\"end\":13116,\"start\":12944},{\"end\":13350,\"start\":13171},{\"end\":13554,\"start\":13431},{\"end\":13659,\"start\":13556},{\"end\":14072,\"start\":13736},{\"end\":14843,\"start\":14118},{\"end\":14933,\"start\":14845},{\"end\":15137,\"start\":14969},{\"end\":15954,\"start\":15139},{\"end\":17055,\"start\":15971},{\"end\":17839,\"start\":17057},{\"end\":18638,\"start\":17841},{\"end\":18973,\"start\":18640},{\"end\":19322,\"start\":18975},{\"end\":19691,\"start\":19324},{\"end\":20609,\"start\":19707},{\"end\":21350,\"start\":20623},{\"end\":22127,\"start\":21352},{\"end\":22186,\"start\":22129},{\"end\":22638,\"start\":22210},{\"end\":24172,\"start\":22640},{\"end\":24742,\"start\":24217},{\"end\":25742,\"start\":24744},{\"end\":27700,\"start\":25770},{\"end\":28533,\"start\":27702},{\"end\":29262,\"start\":28535},{\"end\":29321,\"start\":29264},{\"end\":29520,\"start\":29336},{\"end\":30273,\"start\":29522},{\"end\":31018,\"start\":30318},{\"end\":32338,\"start\":31020},{\"end\":33189,\"start\":32340},{\"end\":33206,\"start\":33199},{\"end\":33271,\"start\":33208},{\"end\":33395,\"start\":33273},{\"end\":34146,\"start\":33425},{\"end\":34638,\"start\":34148},{\"end\":35031,\"start\":34676},{\"end\":35142,\"start\":35033},{\"end\":35393,\"start\":35144},{\"end\":35632,\"start\":35456},{\"end\":36347,\"start\":35680},{\"end\":36601,\"start\":36349},{\"end\":36713,\"start\":36661},{\"end\":36756,\"start\":36752},{\"end\":36997,\"start\":36786},{\"end\":37656,\"start\":36999},{\"end\":38117,\"start\":37658},{\"end\":38740,\"start\":38119},{\"end\":39151,\"start\":38793},{\"end\":40434,\"start\":39153}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6557,\"start\":6529},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7031,\"start\":6965},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8733,\"start\":8689},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8791,\"start\":8733},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9352,\"start\":9298},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9433,\"start\":9352},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9734,\"start\":9580},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10294,\"start\":10257},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10941,\"start\":10887},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11517,\"start\":11429},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12300,\"start\":12146},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13170,\"start\":13117},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13430,\"start\":13351},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13735,\"start\":13660},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14968,\"start\":14934},{\"attributes\":{\"id\":\"formula_15\"},\"end\":29335,\"start\":29322},{\"attributes\":{\"id\":\"formula_17\"},\"end\":35455,\"start\":35394},{\"attributes\":{\"id\":\"formula_18\"},\"end\":36660,\"start\":36602}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23134,\"start\":23127},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24003,\"start\":23996},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24380,\"start\":24373},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24776,\"start\":24769},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29468,\"start\":29461},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31523,\"start\":31516},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32365,\"start\":32358},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33326,\"start\":33319}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1458,\"start\":1446},{\"end\":2536,\"start\":2531},{\"attributes\":{\"n\":\"2.\"},\"end\":6471,\"start\":6434},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8527,\"start\":8501},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10025,\"start\":9995},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11970,\"start\":11936},{\"attributes\":{\"n\":\"2.4.\"},\"end\":14116,\"start\":14075},{\"attributes\":{\"n\":\"3.\"},\"end\":15969,\"start\":15957},{\"attributes\":{\"n\":\"4.\"},\"end\":19705,\"start\":19694},{\"end\":20621,\"start\":20612},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22208,\"start\":22189},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24195,\"start\":24175},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":24215,\"start\":24198},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":25768,\"start\":25745},{\"attributes\":{\"n\":\"4.3.\"},\"end\":30316,\"start\":30276},{\"end\":33197,\"start\":33192},{\"attributes\":{\"n\":\"5.\"},\"end\":33423,\"start\":33398},{\"end\":34674,\"start\":34641},{\"end\":35678,\"start\":35635},{\"end\":36750,\"start\":36716},{\"end\":36784,\"start\":36759},{\"end\":38791,\"start\":38743},{\"end\":40489,\"start\":40437},{\"end\":40501,\"start\":40491},{\"end\":40668,\"start\":40658},{\"end\":40922,\"start\":40912},{\"end\":41150,\"start\":41140},{\"end\":41406,\"start\":41396},{\"end\":41667,\"start\":41657},{\"end\":41910,\"start\":41897},{\"end\":42264,\"start\":42251},{\"end\":42705,\"start\":42696},{\"end\":43110,\"start\":43101},{\"end\":43263,\"start\":43254},{\"end\":43996,\"start\":43983}]", "table": "[{\"end\":43099,\"start\":42863},{\"end\":43981,\"start\":43678},{\"end\":44287,\"start\":44194},{\"end\":46215,\"start\":45514}]", "figure_caption": "[{\"end\":40656,\"start\":40503},{\"end\":40910,\"start\":40670},{\"end\":41138,\"start\":40924},{\"end\":41394,\"start\":41152},{\"end\":41655,\"start\":41408},{\"end\":41895,\"start\":41669},{\"end\":42249,\"start\":41912},{\"end\":42694,\"start\":42266},{\"end\":42863,\"start\":42707},{\"end\":43252,\"start\":43112},{\"end\":43678,\"start\":43265},{\"end\":44194,\"start\":43999},{\"end\":44767,\"start\":44290},{\"end\":45514,\"start\":44770}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":6711,\"start\":6703},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":7729,\"start\":7721},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22234,\"start\":22226},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22661,\"start\":22653},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23781,\"start\":23773},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23821,\"start\":23813},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27384,\"start\":27376},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28026,\"start\":28020},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28161,\"start\":28146},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28661,\"start\":28653},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28850,\"start\":28844},{\"end\":29608,\"start\":29600},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31255,\"start\":31247},{\"end\":31487,\"start\":31479},{\"end\":32959,\"start\":32951},{\"end\":37143,\"start\":37134},{\"end\":37438,\"start\":37430},{\"end\":38091,\"start\":38082},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":38147,\"start\":38137},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38470,\"start\":38462},{\"end\":38603,\"start\":38595},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38842,\"start\":38834},{\"end\":39165,\"start\":39156},{\"end\":39768,\"start\":39759}]", "bib_author_first_name": "[{\"end\":47087,\"start\":47080},{\"end\":47101,\"start\":47093},{\"end\":47116,\"start\":47108},{\"end\":47269,\"start\":47263},{\"end\":47287,\"start\":47280},{\"end\":47302,\"start\":47298},{\"end\":47314,\"start\":47311},{\"end\":47488,\"start\":47483},{\"end\":47506,\"start\":47500},{\"end\":47520,\"start\":47517},{\"end\":47778,\"start\":47772},{\"end\":47790,\"start\":47786},{\"end\":47805,\"start\":47800},{\"end\":48055,\"start\":48049},{\"end\":48068,\"start\":48061},{\"end\":48080,\"start\":48075},{\"end\":48101,\"start\":48096},{\"end\":48112,\"start\":48107},{\"end\":48127,\"start\":48123},{\"end\":48144,\"start\":48137},{\"end\":48398,\"start\":48392},{\"end\":48407,\"start\":48405},{\"end\":48420,\"start\":48415},{\"end\":48429,\"start\":48426},{\"end\":48431,\"start\":48430},{\"end\":48447,\"start\":48439},{\"end\":48464,\"start\":48457},{\"end\":48711,\"start\":48709},{\"end\":48722,\"start\":48718},{\"end\":48736,\"start\":48728},{\"end\":48749,\"start\":48742},{\"end\":48760,\"start\":48756},{\"end\":49154,\"start\":49150},{\"end\":49169,\"start\":49161},{\"end\":49187,\"start\":49179},{\"end\":49387,\"start\":49382},{\"end\":49403,\"start\":49396},{\"end\":49420,\"start\":49414},{\"end\":49431,\"start\":49428},{\"end\":49611,\"start\":49604},{\"end\":49893,\"start\":49889},{\"end\":49904,\"start\":49900},{\"end\":49911,\"start\":49905},{\"end\":49924,\"start\":49917},{\"end\":49935,\"start\":49929},{\"end\":50252,\"start\":50248},{\"end\":50263,\"start\":50259},{\"end\":50270,\"start\":50264},{\"end\":50283,\"start\":50276},{\"end\":50294,\"start\":50288},{\"end\":50638,\"start\":50634},{\"end\":50649,\"start\":50645},{\"end\":50656,\"start\":50650},{\"end\":50668,\"start\":50662},{\"end\":50912,\"start\":50911},{\"end\":50922,\"start\":50918},{\"end\":50940,\"start\":50935},{\"end\":50960,\"start\":50956},{\"end\":50973,\"start\":50968},{\"end\":50985,\"start\":50978},{\"end\":51005,\"start\":51000},{\"end\":51019,\"start\":51013},{\"end\":51261,\"start\":51255},{\"end\":51278,\"start\":51273},{\"end\":51292,\"start\":51286},{\"end\":51310,\"start\":51303},{\"end\":51326,\"start\":51321},{\"end\":51626,\"start\":51619},{\"end\":51638,\"start\":51631},{\"end\":51654,\"start\":51646},{\"end\":51664,\"start\":51660},{\"end\":52068,\"start\":52060},{\"end\":52077,\"start\":52073},{\"end\":52090,\"start\":52084},{\"end\":52284,\"start\":52280},{\"end\":52301,\"start\":52296},{\"end\":52516,\"start\":52512},{\"end\":52529,\"start\":52525},{\"end\":52542,\"start\":52536},{\"end\":52556,\"start\":52550},{\"end\":52802,\"start\":52798},{\"end\":52817,\"start\":52811},{\"end\":52829,\"start\":52825},{\"end\":53052,\"start\":53047},{\"end\":53062,\"start\":53058},{\"end\":53067,\"start\":53063},{\"end\":53082,\"start\":53073},{\"end\":53300,\"start\":53295},{\"end\":53310,\"start\":53306},{\"end\":53315,\"start\":53311},{\"end\":53330,\"start\":53321},{\"end\":53753,\"start\":53752},{\"end\":53772,\"start\":53764},{\"end\":53947,\"start\":53946},{\"end\":53961,\"start\":53958},{\"end\":54165,\"start\":54156},{\"end\":54178,\"start\":54173},{\"end\":54192,\"start\":54186},{\"end\":54205,\"start\":54201},{\"end\":54223,\"start\":54217},{\"end\":54245,\"start\":54236},{\"end\":54260,\"start\":54254},{\"end\":54276,\"start\":54269},{\"end\":54293,\"start\":54285},{\"end\":54305,\"start\":54300},{\"end\":54655,\"start\":54654},{\"end\":54667,\"start\":54663},{\"end\":54685,\"start\":54677},{\"end\":55067,\"start\":55062},{\"end\":55079,\"start\":55076},{\"end\":55345,\"start\":55339},{\"end\":55362,\"start\":55353},{\"end\":55377,\"start\":55371},{\"end\":55388,\"start\":55382},{\"end\":55402,\"start\":55395},{\"end\":55604,\"start\":55600},{\"end\":55614,\"start\":55611},{\"end\":55627,\"start\":55622},{\"end\":55640,\"start\":55634},{\"end\":55928,\"start\":55924},{\"end\":55945,\"start\":55937},{\"end\":56119,\"start\":56114},{\"end\":56140,\"start\":56134},{\"end\":56156,\"start\":56151},{\"end\":56167,\"start\":56162},{\"end\":56596,\"start\":56591},{\"end\":56614,\"start\":56611},{\"end\":56634,\"start\":56629},{\"end\":56649,\"start\":56644},{\"end\":56664,\"start\":56660},{\"end\":56678,\"start\":56673},{\"end\":56897,\"start\":56893},{\"end\":56912,\"start\":56906},{\"end\":56927,\"start\":56922},{\"end\":56945,\"start\":56939},{\"end\":56958,\"start\":56954},{\"end\":56977,\"start\":56968},{\"end\":56988,\"start\":56982},{\"end\":57287,\"start\":57282},{\"end\":57302,\"start\":57295},{\"end\":57326,\"start\":57319},{\"end\":57339,\"start\":57334},{\"end\":57586,\"start\":57579},{\"end\":57609,\"start\":57604},{\"end\":57621,\"start\":57620},{\"end\":57632,\"start\":57631},{\"end\":58033,\"start\":58029},{\"end\":58047,\"start\":58043},{\"end\":58061,\"start\":58054},{\"end\":58349,\"start\":58343},{\"end\":58359,\"start\":58358},{\"end\":58608,\"start\":58603},{\"end\":58622,\"start\":58617},{\"end\":58857,\"start\":58854},{\"end\":58871,\"start\":58866},{\"end\":58891,\"start\":58886},{\"end\":59131,\"start\":59125},{\"end\":59147,\"start\":59141},{\"end\":59455,\"start\":59449},{\"end\":59479,\"start\":59473},{\"end\":59493,\"start\":59489},{\"end\":59842,\"start\":59838},{\"end\":59863,\"start\":59856},{\"end\":59879,\"start\":59873},{\"end\":60249,\"start\":60245},{\"end\":60266,\"start\":60263},{\"end\":60276,\"start\":60273},{\"end\":60289,\"start\":60281},{\"end\":60305,\"start\":60298},{\"end\":60320,\"start\":60316},{\"end\":60332,\"start\":60325},{\"end\":60346,\"start\":60340},{\"end\":60363,\"start\":60357},{\"end\":60379,\"start\":60372},{\"end\":60788,\"start\":60787},{\"end\":60790,\"start\":60789},{\"end\":60806,\"start\":60798},{\"end\":60823,\"start\":60816},{\"end\":61306,\"start\":61303},{\"end\":61323,\"start\":61317},{\"end\":61336,\"start\":61334},{\"end\":61353,\"start\":61343},{\"end\":61657,\"start\":61652},{\"end\":61671,\"start\":61666},{\"end\":61689,\"start\":61681},{\"end\":61705,\"start\":61701},{\"end\":61990,\"start\":61984},{\"end\":62011,\"start\":62007},{\"end\":62023,\"start\":62019},{\"end\":62046,\"start\":62041},{\"end\":62316,\"start\":62310},{\"end\":62337,\"start\":62333},{\"end\":62339,\"start\":62338},{\"end\":62351,\"start\":62347},{\"end\":62374,\"start\":62369},{\"end\":62625,\"start\":62621},{\"end\":62639,\"start\":62632},{\"end\":62842,\"start\":62838},{\"end\":62856,\"start\":62849},{\"end\":63106,\"start\":63102},{\"end\":63119,\"start\":63113},{\"end\":63144,\"start\":63136},{\"end\":63146,\"start\":63145},{\"end\":63163,\"start\":63155},{\"end\":63178,\"start\":63171},{\"end\":63189,\"start\":63186},{\"end\":63458,\"start\":63454},{\"end\":63475,\"start\":63470},{\"end\":63489,\"start\":63485},{\"end\":63675,\"start\":63671},{\"end\":63685,\"start\":63681},{\"end\":63700,\"start\":63692},{\"end\":64111,\"start\":64106},{\"end\":64123,\"start\":64120},{\"end\":64243,\"start\":64238},{\"end\":64261,\"start\":64258},{\"end\":64281,\"start\":64276},{\"end\":64579,\"start\":64574},{\"end\":64597,\"start\":64594},{\"end\":64617,\"start\":64612},{\"end\":64632,\"start\":64627},{\"end\":64647,\"start\":64643},{\"end\":64661,\"start\":64656},{\"end\":64999,\"start\":64993},{\"end\":65013,\"start\":65009},{\"end\":65027,\"start\":65023},{\"end\":65041,\"start\":65036},{\"end\":65058,\"start\":65053},{\"end\":65071,\"start\":65066},{\"end\":65073,\"start\":65072},{\"end\":65087,\"start\":65081},{\"end\":65101,\"start\":65096},{\"end\":65361,\"start\":65355},{\"end\":65594,\"start\":65593},{\"end\":65596,\"start\":65595},{\"end\":65609,\"start\":65608},{\"end\":65611,\"start\":65610},{\"end\":65620,\"start\":65619},{\"end\":65622,\"start\":65621},{\"end\":65949,\"start\":65942},{\"end\":65960,\"start\":65956},{\"end\":65974,\"start\":65966},{\"end\":65984,\"start\":65981},{\"end\":65996,\"start\":65990},{\"end\":66427,\"start\":66421},{\"end\":66439,\"start\":66434},{\"end\":66448,\"start\":66445},{\"end\":66462,\"start\":66455},{\"end\":66475,\"start\":66469},{\"end\":66486,\"start\":66481},{\"end\":66494,\"start\":66491},{\"end\":66688,\"start\":66683},{\"end\":66690,\"start\":66689},{\"end\":66707,\"start\":66695},{\"end\":66709,\"start\":66708},{\"end\":66726,\"start\":66720},{\"end\":66728,\"start\":66727},{\"end\":66997,\"start\":66990},{\"end\":67012,\"start\":67005},{\"end\":67026,\"start\":67020},{\"end\":67028,\"start\":67027},{\"end\":67218,\"start\":67213},{\"end\":67233,\"start\":67226},{\"end\":67241,\"start\":67238},{\"end\":67252,\"start\":67246},{\"end\":67265,\"start\":67259},{\"end\":67276,\"start\":67273}]", "bib_author_last_name": "[{\"end\":47091,\"start\":47088},{\"end\":47106,\"start\":47102},{\"end\":47121,\"start\":47117},{\"end\":47278,\"start\":47270},{\"end\":47296,\"start\":47288},{\"end\":47309,\"start\":47303},{\"end\":47326,\"start\":47315},{\"end\":47498,\"start\":47489},{\"end\":47515,\"start\":47507},{\"end\":47531,\"start\":47521},{\"end\":47784,\"start\":47779},{\"end\":47798,\"start\":47791},{\"end\":47814,\"start\":47806},{\"end\":48059,\"start\":48056},{\"end\":48073,\"start\":48069},{\"end\":48094,\"start\":48081},{\"end\":48105,\"start\":48102},{\"end\":48121,\"start\":48113},{\"end\":48135,\"start\":48128},{\"end\":48154,\"start\":48145},{\"end\":48403,\"start\":48399},{\"end\":48413,\"start\":48408},{\"end\":48424,\"start\":48421},{\"end\":48437,\"start\":48432},{\"end\":48455,\"start\":48448},{\"end\":48469,\"start\":48465},{\"end\":48716,\"start\":48712},{\"end\":48726,\"start\":48723},{\"end\":48740,\"start\":48737},{\"end\":48754,\"start\":48750},{\"end\":48765,\"start\":48761},{\"end\":49159,\"start\":49155},{\"end\":49177,\"start\":49170},{\"end\":49194,\"start\":49188},{\"end\":49394,\"start\":49388},{\"end\":49412,\"start\":49404},{\"end\":49426,\"start\":49421},{\"end\":49438,\"start\":49432},{\"end\":49616,\"start\":49612},{\"end\":49898,\"start\":49894},{\"end\":49915,\"start\":49912},{\"end\":49927,\"start\":49925},{\"end\":49940,\"start\":49936},{\"end\":50257,\"start\":50253},{\"end\":50274,\"start\":50271},{\"end\":50286,\"start\":50284},{\"end\":50299,\"start\":50295},{\"end\":50643,\"start\":50639},{\"end\":50660,\"start\":50657},{\"end\":50673,\"start\":50669},{\"end\":50916,\"start\":50913},{\"end\":50933,\"start\":50923},{\"end\":50954,\"start\":50941},{\"end\":50966,\"start\":50961},{\"end\":50976,\"start\":50974},{\"end\":50998,\"start\":50986},{\"end\":51011,\"start\":51006},{\"end\":51029,\"start\":51020},{\"end\":51037,\"start\":51031},{\"end\":51271,\"start\":51262},{\"end\":51284,\"start\":51279},{\"end\":51301,\"start\":51293},{\"end\":51319,\"start\":51311},{\"end\":51336,\"start\":51327},{\"end\":51629,\"start\":51627},{\"end\":51644,\"start\":51639},{\"end\":51658,\"start\":51655},{\"end\":51668,\"start\":51665},{\"end\":52071,\"start\":52069},{\"end\":52082,\"start\":52078},{\"end\":52097,\"start\":52091},{\"end\":52294,\"start\":52285},{\"end\":52307,\"start\":52302},{\"end\":52523,\"start\":52517},{\"end\":52534,\"start\":52530},{\"end\":52548,\"start\":52543},{\"end\":52565,\"start\":52557},{\"end\":52809,\"start\":52803},{\"end\":52823,\"start\":52818},{\"end\":52834,\"start\":52830},{\"end\":53056,\"start\":53053},{\"end\":53071,\"start\":53068},{\"end\":53086,\"start\":53083},{\"end\":53304,\"start\":53301},{\"end\":53319,\"start\":53316},{\"end\":53334,\"start\":53331},{\"end\":53762,\"start\":53754},{\"end\":53779,\"start\":53773},{\"end\":53789,\"start\":53781},{\"end\":53956,\"start\":53948},{\"end\":53968,\"start\":53962},{\"end\":53977,\"start\":53970},{\"end\":54171,\"start\":54166},{\"end\":54184,\"start\":54179},{\"end\":54199,\"start\":54193},{\"end\":54215,\"start\":54206},{\"end\":54234,\"start\":54224},{\"end\":54252,\"start\":54246},{\"end\":54267,\"start\":54261},{\"end\":54283,\"start\":54277},{\"end\":54298,\"start\":54294},{\"end\":54310,\"start\":54306},{\"end\":54661,\"start\":54656},{\"end\":54675,\"start\":54668},{\"end\":54697,\"start\":54686},{\"end\":54706,\"start\":54699},{\"end\":55074,\"start\":55068},{\"end\":55092,\"start\":55080},{\"end\":55351,\"start\":55346},{\"end\":55369,\"start\":55363},{\"end\":55380,\"start\":55378},{\"end\":55393,\"start\":55389},{\"end\":55408,\"start\":55403},{\"end\":55609,\"start\":55605},{\"end\":55620,\"start\":55615},{\"end\":55632,\"start\":55628},{\"end\":55655,\"start\":55641},{\"end\":55935,\"start\":55929},{\"end\":55954,\"start\":55946},{\"end\":56132,\"start\":56120},{\"end\":56149,\"start\":56141},{\"end\":56160,\"start\":56157},{\"end\":56176,\"start\":56168},{\"end\":56609,\"start\":56597},{\"end\":56627,\"start\":56615},{\"end\":56642,\"start\":56635},{\"end\":56658,\"start\":56650},{\"end\":56671,\"start\":56665},{\"end\":56690,\"start\":56679},{\"end\":56904,\"start\":56898},{\"end\":56920,\"start\":56913},{\"end\":56937,\"start\":56928},{\"end\":56952,\"start\":56946},{\"end\":56966,\"start\":56959},{\"end\":56980,\"start\":56978},{\"end\":56993,\"start\":56989},{\"end\":57293,\"start\":57288},{\"end\":57308,\"start\":57303},{\"end\":57317,\"start\":57310},{\"end\":57332,\"start\":57327},{\"end\":57348,\"start\":57340},{\"end\":57359,\"start\":57350},{\"end\":57602,\"start\":57587},{\"end\":57618,\"start\":57610},{\"end\":57629,\"start\":57622},{\"end\":57642,\"start\":57633},{\"end\":58041,\"start\":58034},{\"end\":58052,\"start\":58048},{\"end\":58070,\"start\":58062},{\"end\":58356,\"start\":58350},{\"end\":58364,\"start\":58360},{\"end\":58376,\"start\":58366},{\"end\":58615,\"start\":58609},{\"end\":58630,\"start\":58623},{\"end\":58864,\"start\":58858},{\"end\":58884,\"start\":58872},{\"end\":58899,\"start\":58892},{\"end\":59139,\"start\":59132},{\"end\":59155,\"start\":59148},{\"end\":59471,\"start\":59456},{\"end\":59487,\"start\":59480},{\"end\":59502,\"start\":59494},{\"end\":59854,\"start\":59843},{\"end\":59871,\"start\":59864},{\"end\":59884,\"start\":59880},{\"end\":60261,\"start\":60250},{\"end\":60271,\"start\":60267},{\"end\":60279,\"start\":60277},{\"end\":60296,\"start\":60290},{\"end\":60314,\"start\":60306},{\"end\":60323,\"start\":60321},{\"end\":60338,\"start\":60333},{\"end\":60355,\"start\":60347},{\"end\":60370,\"start\":60364},{\"end\":60389,\"start\":60380},{\"end\":60796,\"start\":60791},{\"end\":60814,\"start\":60807},{\"end\":60833,\"start\":60824},{\"end\":60841,\"start\":60835},{\"end\":61315,\"start\":61307},{\"end\":61332,\"start\":61324},{\"end\":61341,\"start\":61337},{\"end\":61360,\"start\":61354},{\"end\":61664,\"start\":61658},{\"end\":61679,\"start\":61672},{\"end\":61699,\"start\":61690},{\"end\":61715,\"start\":61706},{\"end\":62005,\"start\":61991},{\"end\":62017,\"start\":62012},{\"end\":62039,\"start\":62024},{\"end\":62054,\"start\":62047},{\"end\":62331,\"start\":62317},{\"end\":62345,\"start\":62340},{\"end\":62367,\"start\":62352},{\"end\":62382,\"start\":62375},{\"end\":62630,\"start\":62626},{\"end\":62645,\"start\":62640},{\"end\":62847,\"start\":62843},{\"end\":62862,\"start\":62857},{\"end\":63111,\"start\":63107},{\"end\":63134,\"start\":63120},{\"end\":63153,\"start\":63147},{\"end\":63169,\"start\":63164},{\"end\":63184,\"start\":63179},{\"end\":63195,\"start\":63190},{\"end\":63468,\"start\":63459},{\"end\":63483,\"start\":63476},{\"end\":63492,\"start\":63490},{\"end\":63679,\"start\":63676},{\"end\":63690,\"start\":63686},{\"end\":63704,\"start\":63701},{\"end\":64118,\"start\":64112},{\"end\":64129,\"start\":64124},{\"end\":64256,\"start\":64244},{\"end\":64274,\"start\":64262},{\"end\":64293,\"start\":64282},{\"end\":64592,\"start\":64580},{\"end\":64610,\"start\":64598},{\"end\":64625,\"start\":64618},{\"end\":64641,\"start\":64633},{\"end\":64654,\"start\":64648},{\"end\":64673,\"start\":64662},{\"end\":65007,\"start\":65000},{\"end\":65021,\"start\":65014},{\"end\":65034,\"start\":65028},{\"end\":65051,\"start\":65042},{\"end\":65064,\"start\":65059},{\"end\":65079,\"start\":65074},{\"end\":65094,\"start\":65088},{\"end\":65112,\"start\":65102},{\"end\":65369,\"start\":65362},{\"end\":65606,\"start\":65597},{\"end\":65617,\"start\":65612},{\"end\":65629,\"start\":65623},{\"end\":65641,\"start\":65631},{\"end\":65954,\"start\":65950},{\"end\":65964,\"start\":65961},{\"end\":65979,\"start\":65975},{\"end\":65988,\"start\":65985},{\"end\":66002,\"start\":65997},{\"end\":66432,\"start\":66428},{\"end\":66443,\"start\":66440},{\"end\":66453,\"start\":66449},{\"end\":66467,\"start\":66463},{\"end\":66479,\"start\":66476},{\"end\":66489,\"start\":66487},{\"end\":66498,\"start\":66495},{\"end\":66693,\"start\":66691},{\"end\":66718,\"start\":66710},{\"end\":66737,\"start\":66729},{\"end\":67003,\"start\":66998},{\"end\":67018,\"start\":67013},{\"end\":67034,\"start\":67029},{\"end\":67224,\"start\":67219},{\"end\":67236,\"start\":67234},{\"end\":67244,\"start\":67242},{\"end\":67257,\"start\":67253},{\"end\":67271,\"start\":67266},{\"end\":67279,\"start\":67277}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52860102},\"end\":47259,\"start\":47013},{\"attributes\":{\"id\":\"b1\"},\"end\":47412,\"start\":47261},{\"attributes\":{\"id\":\"b2\"},\"end\":47702,\"start\":47414},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b3\"},\"end\":48000,\"start\":47704},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":221139756},\"end\":48334,\"start\":48002},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221447287},\"end\":48637,\"start\":48336},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4564405},\"end\":49114,\"start\":48639},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3625846},\"end\":49300,\"start\":49116},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1282515},\"end\":49602,\"start\":49302},{\"attributes\":{\"doi\":\"arXiv:1605.08803\",\"id\":\"b9\"},\"end\":49821,\"start\":49604},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18874645},\"end\":50188,\"start\":49823},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6593498},\"end\":50568,\"start\":50190},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13271756},\"end\":50907,\"start\":50570},{\"attributes\":{\"id\":\"b13\"},\"end\":51253,\"start\":50909},{\"attributes\":{\"doi\":\"arXiv:1704.00028\",\"id\":\"b14\"},\"end\":51571,\"start\":51255},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":52016,\"start\":51573},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219955663},\"end\":52212,\"start\":52018},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1152227},\"end\":52434,\"start\":52214},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3568073},\"end\":52722,\"start\":52436},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":54482423},\"end\":52973,\"start\":52724},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9971732},\"end\":53227,\"start\":52975},{\"attributes\":{\"id\":\"b21\"},\"end\":53694,\"start\":53229},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":49657329},\"end\":53911,\"start\":53696},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":216078090},\"end\":54069,\"start\":53913},{\"attributes\":{\"id\":\"b24\"},\"end\":54584,\"start\":54071},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5667874},\"end\":54967,\"start\":54586},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":54458552},\"end\":55246,\"start\":54969},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":212634162},\"end\":55598,\"start\":55248},{\"attributes\":{\"doi\":\"arXiv:1611.02163\",\"id\":\"b28\"},\"end\":55871,\"start\":55600},{\"attributes\":{\"doi\":\"arXiv:2102.09672\",\"id\":\"b29\"},\"end\":56112,\"start\":55873},{\"attributes\":{\"doi\":\"arXiv:1609.03499\",\"id\":\"b30\"},\"end\":56536,\"start\":56114},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14989939},\"end\":56872,\"start\":56538},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3353110},\"end\":57222,\"start\":56874},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":19119291},\"end\":57524,\"start\":57224},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15257950},\"end\":57933,\"start\":57526},{\"attributes\":{\"doi\":\"arXiv:1511.06434\",\"id\":\"b35\"},\"end\":58285,\"start\":57935},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10309443},\"end\":58536,\"start\":58287},{\"attributes\":{\"doi\":\"arXiv:1905.10887\",\"id\":\"b37\"},\"end\":58797,\"start\":58538},{\"attributes\":{\"doi\":\"arXiv:1906.00446\",\"id\":\"b38\"},\"end\":59077,\"start\":58799},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12554042},\"end\":59367,\"start\":59079},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":16895865},\"end\":59772,\"start\":59369},{\"attributes\":{\"id\":\"b41\"},\"end\":60192,\"start\":59774},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2930547},\"end\":60706,\"start\":60194},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206771333},\"end\":61196,\"start\":60708},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12663716},\"end\":61648,\"start\":61198},{\"attributes\":{\"doi\":\"arXiv:1805.08306\",\"id\":\"b45\"},\"end\":61918,\"start\":61650},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14888175},\"end\":62244,\"start\":61920},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":14888175},\"end\":62549,\"start\":62246},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":196470871},\"end\":62772,\"start\":62551},{\"attributes\":{\"doi\":\"arXiv:2006.09011\",\"id\":\"b49\"},\"end\":63025,\"start\":62774},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":227209335},\"end\":63400,\"start\":63027},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":7961699},\"end\":63610,\"start\":63402},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":21618854},\"end\":64053,\"start\":63612},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":220403493},\"end\":64236,\"start\":64055},{\"attributes\":{\"id\":\"b54\"},\"end\":64519,\"start\":64238},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":14989939},\"end\":64964,\"start\":64521},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":13756489},\"end\":65289,\"start\":64966},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":5560643},\"end\":65517,\"start\":65291},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":207761262},\"end\":65880,\"start\":65519},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":6594838},\"end\":66343,\"start\":65882},{\"attributes\":{\"doi\":\"arXiv\",\"id\":\"b60\"},\"end\":66681,\"start\":66345},{\"attributes\":{\"id\":\"b61\"},\"end\":66959,\"start\":66683},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":50698},\"end\":67135,\"start\":66961},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":49657846},\"end\":67633,\"start\":67137}]", "bib_title": "[{\"end\":47078,\"start\":47013},{\"end\":48047,\"start\":48002},{\"end\":48390,\"start\":48336},{\"end\":48707,\"start\":48639},{\"end\":49148,\"start\":49116},{\"end\":49380,\"start\":49302},{\"end\":49887,\"start\":49823},{\"end\":50246,\"start\":50190},{\"end\":50632,\"start\":50570},{\"end\":51617,\"start\":51573},{\"end\":52058,\"start\":52018},{\"end\":52278,\"start\":52214},{\"end\":52510,\"start\":52436},{\"end\":52796,\"start\":52724},{\"end\":53045,\"start\":52975},{\"end\":53293,\"start\":53229},{\"end\":53750,\"start\":53696},{\"end\":53944,\"start\":53913},{\"end\":54154,\"start\":54071},{\"end\":54652,\"start\":54586},{\"end\":55060,\"start\":54969},{\"end\":55337,\"start\":55248},{\"end\":56589,\"start\":56538},{\"end\":56891,\"start\":56874},{\"end\":57280,\"start\":57224},{\"end\":57577,\"start\":57526},{\"end\":58341,\"start\":58287},{\"end\":59123,\"start\":59079},{\"end\":59447,\"start\":59369},{\"end\":59836,\"start\":59774},{\"end\":60243,\"start\":60194},{\"end\":60785,\"start\":60708},{\"end\":61301,\"start\":61198},{\"end\":61982,\"start\":61920},{\"end\":62308,\"start\":62246},{\"end\":62619,\"start\":62551},{\"end\":63100,\"start\":63027},{\"end\":63452,\"start\":63402},{\"end\":63669,\"start\":63612},{\"end\":64104,\"start\":64055},{\"end\":64572,\"start\":64521},{\"end\":64991,\"start\":64966},{\"end\":65353,\"start\":65291},{\"end\":65591,\"start\":65519},{\"end\":65940,\"start\":65882},{\"end\":66988,\"start\":66961},{\"end\":67211,\"start\":67137}]", "bib_author": "[{\"end\":47093,\"start\":47080},{\"end\":47108,\"start\":47093},{\"end\":47123,\"start\":47108},{\"end\":47280,\"start\":47263},{\"end\":47298,\"start\":47280},{\"end\":47311,\"start\":47298},{\"end\":47328,\"start\":47311},{\"end\":47500,\"start\":47483},{\"end\":47517,\"start\":47500},{\"end\":47533,\"start\":47517},{\"end\":47786,\"start\":47772},{\"end\":47800,\"start\":47786},{\"end\":47816,\"start\":47800},{\"end\":48061,\"start\":48049},{\"end\":48075,\"start\":48061},{\"end\":48096,\"start\":48075},{\"end\":48107,\"start\":48096},{\"end\":48123,\"start\":48107},{\"end\":48137,\"start\":48123},{\"end\":48156,\"start\":48137},{\"end\":48405,\"start\":48392},{\"end\":48415,\"start\":48405},{\"end\":48426,\"start\":48415},{\"end\":48439,\"start\":48426},{\"end\":48457,\"start\":48439},{\"end\":48471,\"start\":48457},{\"end\":48718,\"start\":48709},{\"end\":48728,\"start\":48718},{\"end\":48742,\"start\":48728},{\"end\":48756,\"start\":48742},{\"end\":48767,\"start\":48756},{\"end\":49161,\"start\":49150},{\"end\":49179,\"start\":49161},{\"end\":49196,\"start\":49179},{\"end\":49396,\"start\":49382},{\"end\":49414,\"start\":49396},{\"end\":49428,\"start\":49414},{\"end\":49440,\"start\":49428},{\"end\":49618,\"start\":49604},{\"end\":49900,\"start\":49889},{\"end\":49917,\"start\":49900},{\"end\":49929,\"start\":49917},{\"end\":49942,\"start\":49929},{\"end\":50259,\"start\":50248},{\"end\":50276,\"start\":50259},{\"end\":50288,\"start\":50276},{\"end\":50301,\"start\":50288},{\"end\":50645,\"start\":50634},{\"end\":50662,\"start\":50645},{\"end\":50675,\"start\":50662},{\"end\":50918,\"start\":50911},{\"end\":50935,\"start\":50918},{\"end\":50956,\"start\":50935},{\"end\":50968,\"start\":50956},{\"end\":50978,\"start\":50968},{\"end\":51000,\"start\":50978},{\"end\":51013,\"start\":51000},{\"end\":51031,\"start\":51013},{\"end\":51039,\"start\":51031},{\"end\":51273,\"start\":51255},{\"end\":51286,\"start\":51273},{\"end\":51303,\"start\":51286},{\"end\":51321,\"start\":51303},{\"end\":51338,\"start\":51321},{\"end\":51631,\"start\":51619},{\"end\":51646,\"start\":51631},{\"end\":51660,\"start\":51646},{\"end\":51670,\"start\":51660},{\"end\":52073,\"start\":52060},{\"end\":52084,\"start\":52073},{\"end\":52099,\"start\":52084},{\"end\":52296,\"start\":52280},{\"end\":52309,\"start\":52296},{\"end\":52525,\"start\":52512},{\"end\":52536,\"start\":52525},{\"end\":52550,\"start\":52536},{\"end\":52567,\"start\":52550},{\"end\":52811,\"start\":52798},{\"end\":52825,\"start\":52811},{\"end\":52836,\"start\":52825},{\"end\":53058,\"start\":53047},{\"end\":53073,\"start\":53058},{\"end\":53088,\"start\":53073},{\"end\":53306,\"start\":53295},{\"end\":53321,\"start\":53306},{\"end\":53336,\"start\":53321},{\"end\":53764,\"start\":53752},{\"end\":53781,\"start\":53764},{\"end\":53791,\"start\":53781},{\"end\":53958,\"start\":53946},{\"end\":53970,\"start\":53958},{\"end\":53979,\"start\":53970},{\"end\":54173,\"start\":54156},{\"end\":54186,\"start\":54173},{\"end\":54201,\"start\":54186},{\"end\":54217,\"start\":54201},{\"end\":54236,\"start\":54217},{\"end\":54254,\"start\":54236},{\"end\":54269,\"start\":54254},{\"end\":54285,\"start\":54269},{\"end\":54300,\"start\":54285},{\"end\":54312,\"start\":54300},{\"end\":54663,\"start\":54654},{\"end\":54677,\"start\":54663},{\"end\":54699,\"start\":54677},{\"end\":54708,\"start\":54699},{\"end\":55076,\"start\":55062},{\"end\":55094,\"start\":55076},{\"end\":55353,\"start\":55339},{\"end\":55371,\"start\":55353},{\"end\":55382,\"start\":55371},{\"end\":55395,\"start\":55382},{\"end\":55410,\"start\":55395},{\"end\":55611,\"start\":55600},{\"end\":55622,\"start\":55611},{\"end\":55634,\"start\":55622},{\"end\":55657,\"start\":55634},{\"end\":55937,\"start\":55924},{\"end\":55956,\"start\":55937},{\"end\":56134,\"start\":56114},{\"end\":56151,\"start\":56134},{\"end\":56162,\"start\":56151},{\"end\":56178,\"start\":56162},{\"end\":56611,\"start\":56591},{\"end\":56629,\"start\":56611},{\"end\":56644,\"start\":56629},{\"end\":56660,\"start\":56644},{\"end\":56673,\"start\":56660},{\"end\":56692,\"start\":56673},{\"end\":56906,\"start\":56893},{\"end\":56922,\"start\":56906},{\"end\":56939,\"start\":56922},{\"end\":56954,\"start\":56939},{\"end\":56968,\"start\":56954},{\"end\":56982,\"start\":56968},{\"end\":56995,\"start\":56982},{\"end\":57295,\"start\":57282},{\"end\":57310,\"start\":57295},{\"end\":57319,\"start\":57310},{\"end\":57334,\"start\":57319},{\"end\":57350,\"start\":57334},{\"end\":57361,\"start\":57350},{\"end\":57604,\"start\":57579},{\"end\":57620,\"start\":57604},{\"end\":57631,\"start\":57620},{\"end\":57644,\"start\":57631},{\"end\":58043,\"start\":58029},{\"end\":58054,\"start\":58043},{\"end\":58072,\"start\":58054},{\"end\":58358,\"start\":58343},{\"end\":58366,\"start\":58358},{\"end\":58378,\"start\":58366},{\"end\":58617,\"start\":58603},{\"end\":58632,\"start\":58617},{\"end\":58866,\"start\":58854},{\"end\":58886,\"start\":58866},{\"end\":58901,\"start\":58886},{\"end\":59141,\"start\":59125},{\"end\":59157,\"start\":59141},{\"end\":59473,\"start\":59449},{\"end\":59489,\"start\":59473},{\"end\":59504,\"start\":59489},{\"end\":59856,\"start\":59838},{\"end\":59873,\"start\":59856},{\"end\":59886,\"start\":59873},{\"end\":60263,\"start\":60245},{\"end\":60273,\"start\":60263},{\"end\":60281,\"start\":60273},{\"end\":60298,\"start\":60281},{\"end\":60316,\"start\":60298},{\"end\":60325,\"start\":60316},{\"end\":60340,\"start\":60325},{\"end\":60357,\"start\":60340},{\"end\":60372,\"start\":60357},{\"end\":60391,\"start\":60372},{\"end\":60798,\"start\":60787},{\"end\":60816,\"start\":60798},{\"end\":60835,\"start\":60816},{\"end\":60843,\"start\":60835},{\"end\":61317,\"start\":61303},{\"end\":61334,\"start\":61317},{\"end\":61343,\"start\":61334},{\"end\":61362,\"start\":61343},{\"end\":61666,\"start\":61652},{\"end\":61681,\"start\":61666},{\"end\":61701,\"start\":61681},{\"end\":61717,\"start\":61701},{\"end\":62007,\"start\":61984},{\"end\":62019,\"start\":62007},{\"end\":62041,\"start\":62019},{\"end\":62056,\"start\":62041},{\"end\":62333,\"start\":62310},{\"end\":62347,\"start\":62333},{\"end\":62369,\"start\":62347},{\"end\":62384,\"start\":62369},{\"end\":62632,\"start\":62621},{\"end\":62647,\"start\":62632},{\"end\":62849,\"start\":62838},{\"end\":62864,\"start\":62849},{\"end\":63113,\"start\":63102},{\"end\":63136,\"start\":63113},{\"end\":63155,\"start\":63136},{\"end\":63171,\"start\":63155},{\"end\":63186,\"start\":63171},{\"end\":63197,\"start\":63186},{\"end\":63470,\"start\":63454},{\"end\":63485,\"start\":63470},{\"end\":63494,\"start\":63485},{\"end\":63681,\"start\":63671},{\"end\":63692,\"start\":63681},{\"end\":63706,\"start\":63692},{\"end\":64120,\"start\":64106},{\"end\":64131,\"start\":64120},{\"end\":64258,\"start\":64238},{\"end\":64276,\"start\":64258},{\"end\":64295,\"start\":64276},{\"end\":64594,\"start\":64574},{\"end\":64612,\"start\":64594},{\"end\":64627,\"start\":64612},{\"end\":64643,\"start\":64627},{\"end\":64656,\"start\":64643},{\"end\":64675,\"start\":64656},{\"end\":65009,\"start\":64993},{\"end\":65023,\"start\":65009},{\"end\":65036,\"start\":65023},{\"end\":65053,\"start\":65036},{\"end\":65066,\"start\":65053},{\"end\":65081,\"start\":65066},{\"end\":65096,\"start\":65081},{\"end\":65114,\"start\":65096},{\"end\":65371,\"start\":65355},{\"end\":65608,\"start\":65593},{\"end\":65619,\"start\":65608},{\"end\":65631,\"start\":65619},{\"end\":65643,\"start\":65631},{\"end\":65956,\"start\":65942},{\"end\":65966,\"start\":65956},{\"end\":65981,\"start\":65966},{\"end\":65990,\"start\":65981},{\"end\":66004,\"start\":65990},{\"end\":66434,\"start\":66421},{\"end\":66445,\"start\":66434},{\"end\":66455,\"start\":66445},{\"end\":66469,\"start\":66455},{\"end\":66481,\"start\":66469},{\"end\":66491,\"start\":66481},{\"end\":66500,\"start\":66491},{\"end\":66695,\"start\":66683},{\"end\":66720,\"start\":66695},{\"end\":66739,\"start\":66720},{\"end\":67005,\"start\":66990},{\"end\":67020,\"start\":67005},{\"end\":67036,\"start\":67020},{\"end\":67226,\"start\":67213},{\"end\":67238,\"start\":67226},{\"end\":67246,\"start\":67238},{\"end\":67259,\"start\":67246},{\"end\":67273,\"start\":67259},{\"end\":67281,\"start\":67273}]", "bib_venue": "[{\"end\":48891,\"start\":48833},{\"end\":51811,\"start\":51749},{\"end\":53477,\"start\":53415},{\"end\":60964,\"start\":60912},{\"end\":63847,\"start\":63785},{\"end\":66125,\"start\":66073},{\"end\":67396,\"start\":67347},{\"end\":47127,\"start\":47123},{\"end\":47481,\"start\":47414},{\"end\":47770,\"start\":47704},{\"end\":48160,\"start\":48156},{\"end\":48475,\"start\":48471},{\"end\":48831,\"start\":48767},{\"end\":49200,\"start\":49196},{\"end\":49444,\"start\":49440},{\"end\":49707,\"start\":49634},{\"end\":49980,\"start\":49942},{\"end\":50363,\"start\":50301},{\"end\":50713,\"start\":50675},{\"end\":51076,\"start\":51039},{\"end\":51391,\"start\":51354},{\"end\":51747,\"start\":51670},{\"end\":52106,\"start\":52099},{\"end\":52313,\"start\":52309},{\"end\":52571,\"start\":52567},{\"end\":52840,\"start\":52836},{\"end\":53092,\"start\":53088},{\"end\":53413,\"start\":53336},{\"end\":53795,\"start\":53791},{\"end\":53983,\"start\":53979},{\"end\":54316,\"start\":54312},{\"end\":54731,\"start\":54708},{\"end\":55098,\"start\":55094},{\"end\":55414,\"start\":55410},{\"end\":55713,\"start\":55673},{\"end\":55922,\"start\":55873},{\"end\":56273,\"start\":56194},{\"end\":56696,\"start\":56692},{\"end\":57039,\"start\":56995},{\"end\":57365,\"start\":57361},{\"end\":57709,\"start\":57644},{\"end\":58027,\"start\":57935},{\"end\":58396,\"start\":58378},{\"end\":58601,\"start\":58538},{\"end\":58852,\"start\":58799},{\"end\":59201,\"start\":59157},{\"end\":59548,\"start\":59504},{\"end\":59972,\"start\":59886},{\"end\":60431,\"start\":60391},{\"end\":60910,\"start\":60843},{\"end\":61414,\"start\":61362},{\"end\":62060,\"start\":62056},{\"end\":62388,\"start\":62384},{\"end\":62654,\"start\":62647},{\"end\":62836,\"start\":62774},{\"end\":63201,\"start\":63197},{\"end\":63498,\"start\":63494},{\"end\":63783,\"start\":63706},{\"end\":64138,\"start\":64131},{\"end\":64372,\"start\":64295},{\"end\":64724,\"start\":64675},{\"end\":65118,\"start\":65114},{\"end\":65389,\"start\":65371},{\"end\":65680,\"start\":65643},{\"end\":66071,\"start\":66004},{\"end\":66419,\"start\":66345},{\"end\":66813,\"start\":66739},{\"end\":67040,\"start\":67036},{\"end\":67345,\"start\":67281}]"}}}, "year": 2023, "month": 12, "day": 17}