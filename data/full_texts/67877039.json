{"id": 67877039, "updated": "2023-10-04 14:00:33.83", "metadata": {"title": "Selective Sensor Fusion for Neural Visual-Inertial Odometry", "authors": "[{\"first\":\"Changhao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Stefano\",\"last\":\"Rosa\",\"middle\":[]},{\"first\":\"Yishu\",\"last\":\"Miao\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Lu\",\"middle\":[\"Xiaoxuan\"]},{\"first\":\"Wei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Markham\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 3, "day": 4}, "abstract": "Deep learning approaches for Visual-Inertial Odometry (VIO) have proven successful, but they rarely focus on incorporating robust fusion strategies for dealing with imperfect input sensory data. We propose a novel end-to-end selective sensor fusion framework for monocular VIO, which fuses monocular images and inertial measurements in order to estimate the trajectory whilst improving robustness to real-life issues, such as missing and corrupted data or bad sensor synchronization. In particular, we propose two fusion modalities based on different masking strategies: deterministic soft fusion and stochastic hard fusion, and we compare with previously proposed direct fusion baselines. During testing, the network is able to selectively process the features of the available sensor modalities and produce a trajectory at scale. We present a thorough investigation on the performances on three public autonomous driving, Micro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate the effectiveness of the fusion strategies, which offer better performances compared to direct fusion, particularly in presence of corrupted data. In addition, we study the interpretability of the fusion networks by visualising the masking layers in different scenarios and with varying data corruption, revealing interesting correlations between the fusion networks and imperfect sensory input data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.01534", "mag": "2949878620", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ChenRMLWMT19", "doi": "10.1109/cvpr.2019.01079"}}, "content": {"source": {"pdf_hash": "afd254c020722f62f1ca34206d3405558e77c821", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.01534v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1903.01534", "status": "GREEN"}}, "grobid": {"id": "0e90ebbafcbfd209d07973fe069cfc9e0746f4e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/afd254c020722f62f1ca34206d3405558e77c821.txt", "contents": "\nSelective Sensor Fusion for Neural Visual-Inertial Odometry\n\n\nChanghao Chen \nDepartment of Computer Science\nUniversity of Oxford\n\n\nStefano Rosa \nDepartment of Computer Science\nUniversity of Oxford\n\n\nYishu Miao \nMO Intelligence 3 Tencent\n\n\nChris Xiaoxuan Lu \nDepartment of Computer Science\nUniversity of Oxford\n\n\nWei Wu \nAndrew Markham \nDepartment of Computer Science\nUniversity of Oxford\n\n\nNiki Trigoni \nDepartment of Computer Science\nUniversity of Oxford\n\n\nSelective Sensor Fusion for Neural Visual-Inertial Odometry\n\nDeep learning approaches for Visual-Inertial Odometry (VIO) have proven successful, but they rarely focus on incorporating robust fusion strategies for dealing with imperfect input sensory data. We propose a novel end-to-end selective sensor fusion framework for monocular VIO, which fuses monocular images and inertial measurements in order to estimate the trajectory whilst improving robustness to real-life issues, such as missing and corrupted data or bad sensor synchronization. In particular, we propose two fusion modalities based on different masking strategies: deterministic soft fusion and stochastic hard fusion, and we compare with previously proposed direct fusion baselines. During testing, the network is able to selectively process the features of the available sensor modalities and produce a trajectory at scale. We present a thorough investigation on the performances on three public autonomous driving, Micro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate the effectiveness of the fusion strategies, which offer better performances compared to direct fusion, particularly in presence of corrupted data. In addition, we study the interpretability of the fusion networks by visualising the masking layers in different scenarios and with varying data corruption, revealing interesting correlations between the fusion networks and imperfect sensory input data.\n\nIntroduction\n\nHumans are able to perceive their self-motion through space via multimodal perceptions. Optical flow (visual cues) and vestibular signals (inertial motion sense) are the two most sensitive cues for determining self-motion [9].\n\nIn the fields of computer vision and robotics, integrating visual and inertial information in the form of Visual-Inertial Odometry (VIO) is a well researched topic [17,20,19,11,29], as it enables ubiquitous mobility for mobile agents by providing robust and accurate pose information. Moreover, cameras and inertial sensors are relatively low-cost, power-efficient and widely found in ground robots, smartphones, and unmanned aerial vehicles (UAVs). Existing VIO approaches generally follow a standard pipeline that involves fine-tuning of both feature detection and tracking, and of the sensor fusion strategy. These models rely on handcrafted features, and fuse the information based on filtering [20] or nonlinear optimization [19,11,29]. However, naively using all features before fusion will lead to unreliable state estimation, as incorrect feature extraction or matching cripples the entire system. Real issues which can and do occur include camera occlusion or operation in low-light conditions [40], excess noise or drift within the inertial sensor [26], time-synchronization between the two streams or spatial misalignment [21].\n\nRecent studies on applying deep neural networks (DNNs) to solving visual-inertial odometry [30] or visual odometry [18,7] showed competitive performance in terms of both accuracy and robustness. Although DNNs excel at extracting high-level features representative of egomotion, these learning-based methods are not explicitly modelling the sources of degradation in real-world usages. Without considering possible sensor errors, all features are directly fed into other modules for further pose regression in [4,7,18], or simply concatenated as in [30]. These factors can possibly cause troubles to the accuracy and safety of VIO systems, when the input data are corrupted or missing.\n\nFor this reason, we present a generic framework that models feature selection for robust sensor fusion. The selection process is conditioned on the measurement reliability and the dynamics of both egomotion and environment. Two alternative feature weighting strategies are presented: soft fusion, implemented in a deterministic fashion; and hard fusion, which introduces stochastic noise and intuitively learns to keep the most relevant feature representations, while discarding useless or misleading information. Both architectures are trained in an end-to-end fashion.\n\nBy explicitly modelling the selection process, we are able to demonstrate the strong correlation between the selected features and the environmental/measurement dynamics by visualizing the sensor fusion masks, as illus-  Figure 1: Visualization of the learned hard and soft fusion masks under different conditions (left: normal data; middle and right: corrupted data). The number (hard) or weights (soft) of selected features in the visual and inertial sides can reflect the self-motion dynamics (increasing importance of inertial features during turning), and data corruption conditions. trated in Figure 1. Our results show that features extracted from different modalities (i.e., vision and inertial motion) are complementary in various conditions: the inertial features contribute more in presence of fast rotation, while visual features are preferred during large translations (Figure 6). Thus, the selective sensor fusion provides insight into the underlying strengths of each sensor modality. We also demonstrate how incorporating selective sensor fusion makes VIO robust to data corruption typically encountered in real-world scenarios.\n\nThe main contributions of this work are as follows:\n\n\u2022 We present a generic framework to learn selective sensor fusion enabling more robust and accurate egomotion estimation in real world scenarios.\n\n\u2022 Our selective sensor fusion masks can be visualized and interpreted, providing deeper insight into the relative strengths of each stream, and guiding further system design.\n\n\u2022 We create challenging datasets on top of current public VIO datasets by considering seven different sources of sensor degradation, and conduct a new and complete study on the accuracy and robustness of deep sensor fusion in presence of corrupted data.\n\n\nNeural VIO Models with Selective Fusion\n\nIn this section, we introduce the end-to-end architecture for neural visual-inertial odometry, which is the foundation for our proposed framework. Figure 2(top) shows a modular overview of the architecture, consisting of visual and inertial encoders, feature fusion, temporal modelling and pose regression. Our model takes in a sequence of raw images and IMU measurements, and generates their corresponding pose transformation. With the exception of our novel feature fusion, the pipeline can be any generic deep VIO technique. In the Feature Fusion component we propose two different selection mechanisms (soft and hard) and compare them with direct (i.e. a uniform/unweighted mask) fusion, as shown in Figure 2(bottom).\n\n\nFeature Encoder\n\nVisual Feature Encoder The Visual Encoder extracts a latent representation from a set of two consecutive monocular images x V . Ideally, we want the Visual Encoder f vision to learn geometrically meaningful features rather than features related with appearance or context. For this reason, instead of using a PoseNet model [18], as commonly found in other DL-based VO approaches [43,42,41], we use FlowNetSimple [10] as our feature encoder. Flownet provides features that are suited for optical flow prediction. The network consists of nine convolutional layers. The size of the receptive fields gradually reduces from 7\u00d77 to 5\u00d75 and finally 3\u00d73, with stride two for the first six. Each layer is followed by a ReLU nonlinearity except for the last one, and we use the features from the last convolutional layer a V as our visual feature:  Figure 2: An overview of our neural visual-inertial odometry architecture with proposed selective sensor fusion, consisting of visual and inertial encoders, feature fusion, temporal modelling and pose regression. In the feature fusion component, we compare our proposed soft and hard selective sensor fusion strategies with direct fusion.\na V = f vision (x V ).(1)\nInertial Feature Encoder: Inertial data streams have a strong temporal component, and are generally available at higher frequency (\u223c100 Hz) than images (\u223c10 Hz). Inspired by IONet [6], we use a two-layer Bi-directional LSTM with 128 hidden states as the Inertial Feature Encoder f inertial . As shown in Figure 2, a window of inertial measurements x I between each two images is fed to the inertial feature encoder in order to extract the dimensional feature vector a I :\na I = f inertial (x I ).(2)\n\nFusion Function\n\nWe now combine the high-level features produced by the two encoders from raw data sequences, with a fusion function g that combines information from the visual a V and inertial a I channels to extract the useful combined feature z for future pose regression task:\nz = g(a V , a I ).(3)\nThere are several different ways to implement this fusion function. The current approach is to directly concatenate the two features together into one feature space (we call this method direct fusion g direct ). However, in order to learn a robust sensor fusion model, we propose two fusion schemes -deterministic soft fusion g soft and stochastic hard fusion g hard , which explicitly model the feature selection process according to the current environment dynamics and the reliability of the data input. Our selective fusion mechanisms re-weights the concatenated inertial-visual features, guided by the concatenated features themselves. The fusion network is another deep neural network and is end-to-end trainable. Details will be discussed in Section 3.\n\n\nTemporal Modelling and Pose Regression\n\nThe fundamental tenet of ego-motion estimation requires modeling temporal dependencies to derive accurate pose regression. Hence, a recurrent neural network (a two-layer Bi-directional LSTM) takes in input the combined feature representation z t at time step t and its previous hidden states h t\u22121 and models the dynamics and connections between a sequence of features. After the recurrent network, a fullyconnected layer serves as the pose regressor, mapping the features to a pose transformation y t , representing the motion transformation over a time window.\ny t = RNN(z t , h t\u22121 )(4)\n\nSelective Sensor Fusion\n\nIntuitively, the features from each modality offer different strengths for the task of regressing pose transformations. This is particularly true in the case of visualinertial odometry (VIO), where the monocular visual input is capable of estimating the appearance and geometry of a 3D scene, but is unable to determine the metric scale [11]. Moreover, changes in illumination, textureless areas and motion blur can lead to bad data association. Meanwhile, inertial data is interoceptive/egocentric and generally environment-agnostic, and can still be reliable when visual tracking fails [6]. However, measurements from low-cost MEMS inertial sensors are corrupted by inevitable noise and bias, which leads to higher long-term drift than a wellfunctioning visual-odometry chain.\n\nOur perspective is that simply considering all features as though they are correct, without any consideration of degradation, is unwise and will lead to unrecoverable er-rors. In this section, we propose two different selective sensor fusion schemes for explicitly learning the feature selection process: soft (deterministic) fusion, and hard (stochastic) fusion, as illustrated in Figure 3. In addition, we also present a straightforward sensor fusion scheme -direct fusion -as a baseline model for comparison.\n\n\nDirect Fusion\n\nA straightforward approach for implementing sensor fusion in a VIO framework consists in the use of Multi-Layer Perceptrons (MLPs) to combine the features from the visual and inertial channels. Ideally, the system learns to perform feature selection and prediction in an end-to-end fashion. Hence, direct fusion is modelled as:\ng direct (a V , a I ) = [a V ; a I ](5)\nwhere [a V ; a I ] denotes an MLP function that concatenates a V and a I .\n\n\nSoft Fusion (Deterministic)\n\nWe now propose a soft fusion scheme that explicitly and deterministically models feature selection. Similar to the widely applied attention mechanism [33,39,14], this function re-weights each feature by conditioning on both the visual and inertial channels, which allows the feature selection process to be jointly trained with other modules. The function is deterministic and differentiable.\n\nHere, a pair of continuous masks s V and s I is introduced to implement soft selection of the extracted feature representations, before these features are passed to temporal modelling and pose regression:\ns V = Sigmoid V ([a V ; a I ])(6)s I = Sigmoid I ([a V ; a I ])(7)\nwhere s V and s I are the masks applied to visual features and inertial features respectively, and which are deterministically parameterised by the neural networks, conditioned on both the visual a V and inertial features a I . The sigmoid function makes sure that each of the features will be re-weighted in the range [0, 1]. Then, the visual and inertial features are element-wise multiplied with their corresponding soft masks as the new re-weighted vectors. The selective soft fusion function is modelled as\ng soft (a V , a I ) = [a V s V ; a I s I ].(8)\n\nHard Fusion (Stochastic)\n\nIn addition to the soft fusion introduced above, we propose a variant of the fusion scheme -hard fusion. Instead of re-weighting each feature by a continuous value, hard fusion learns a stochastic function that generates a binary mask that either propagates the feature or blocks it. This mechanism can be viewed as a switcher for each component of the feature map, which is stochastic neural implemented by a parameterised Bernoulli distributions. However, the stochastic layer cannot be trained directly by back-propagation, as gradients will not propagate through discrete latent variables. To tackle this, the REIN-FORCE algorithm [38,24] is generally used to construct the gradient estimator. In our case, we employ a more lightweight method -Gumbel-Softmax resampling [16,22] to infer the stochastic layer, so that the hard fusion can be trained in an end-to-end fashion as well.\n\nInstead of learning masks deterministically from features, hard masks s V and s I are re-sampled from a Bernoulli distribution, parameterised by \u03b1, which is conditioned on features but with the addition of stochastic noise:\ns V \u223c p(s V |a V , a I ) = Bernoulli(\u03b1 V )(9)s I \u223c p(s I |a V , a I ) = Bernoulli(\u03b1 I ).(10)\nSimilar to soft fusion, features are element-wise multiplied with their corresponding hard masks as the new reweighted vectors. The stochastic hard fusion function is modelled as Figure 3 (b) shows the detailed workflow of proposed Gumbel-Softmax resampling based hard fusion. A pair of probability variables \u03b1 V and \u03b1 I is conditioned on the concatenated visual and inertial feature vectors [a V ; a I ]:\ng hard (a V , a I ) = [a V s V ; a I s I ].(11)\u03b1 V = Sigmoid V ([a V ; a I ])(12)\u03b1 I = Sigmoid I ([a V ; a I ]),(13)\nwhere the probability variables are n-dimensional vectors \u03b1 = [\u03c0 1 , ..., \u03c0 n ], representing the probability of each fea-ture at location n to be selected or not. Sigmoid function enables each vector to be re-weighted in the range [0, 1]. The Gumbel-max trick [23] allows efficiently to draw samples s from a categorical distribution given the class probabilities \u03c0 i and a random variable i , and then the onehot encoding performs \"binarization\" of the category:\ns = one hot(arg max i [ i + log \u03c0 i ]).(14)\nThis is due to the fact that for any B \u2286 [1, ..., n] [13]:\narg max i [ i + log \u03c0 i ] \u223c \u03c0 i i\u2208B \u03c0 i(15)\nIt could be viewed as a process of adding independent Gumbel perturbations i to the discrete probability variable. In practice, the random variable i is sampled from a Gumbel distribution, which is a continuous distribution on the simplex that can approximate categorical samples:\n= \u2212 log(\u2212 log(u)), u \u223c Uniform(0, 1).(16)\nIn Equation 14 the argmax operation is not differentiable, so Softmax function is instead used as an approximate:\nh i = exp((log(\u03c0 i ) + i )/\u03c4 ) n i=1 exp((log(\u03c0 j ) + j )/\u03c4 ) , i = 1, ..., n,(17)\nwhere \u03c4 > 0 is the temperature that modulates the resampling process.\n\n\nDiscussions on Neural and classical VIOs\n\nBasically, soft fusion gently re-weights each feature in a deterministic way, while hard fusion directly blocks features according to the environment and its reliability. In general, soft fusion is a simple extension of direct fusion that is good for dealing with the uncertainties in the input sensory data. By comparison, the inference in hard fusion is more difficult, but it offers a more intuitive representation. The stochasticity gives the VIO system better generalisation ability and higher tolerance to imperfect sensory data. The stochastic mask of hard fusion acts as an inductive bias, separating the feature selection process from prediction, which can also be easily interpreted by corresponding to uncertainties of the input sensory data.\n\nFiltering methods update their belief based on the past state and current observations of visual and inertial modalities [25,20,15,2]. \"Learning\" within these methods is usually constrained to gain and covariances [1]. This is a deterministic process, and noise parameters are hand-tuned beforehand. Deep leaning methods are instead fully learned from data and the hidden recurrent state only contains information relevant to the regressor. Our approach models the feature selection process explicitly with the use of soft and hard masks. Loosely, the proposed soft mask can be viewed as similar to tuning the gain and covariance matrix in classical filtering methods, but based on the latent data representation instead. \n\n\nExperiments\n\nWe evaluate our proposed approaches on three wellknown datasets: the KITTI Odometry dataset for autonomous driving [12], the EuRoC dataset for micro aerial vehicle [5], and the PennCOSYVIO dataset for hand-held devices [28]. A demonstration video and other details can be found at our project website 1 .\n\n\nExperimental Setup and Baselines\n\nThe architecture was implemented with PyTorch and trained on a NVIDIA Titan X GPU.\n\nWe chose the neural vision-only model and the neural visual-inertial model with direct fusion as our baselines, termed Vision-Only (DeepVO) and VIO-Direct (VINet) respectively in our experiments. The neural vision-only model uses the visual encoder, temporal modelling and pose regression as in our proposed framework in Figure 2. Neural visual inertial model with direct fusion uses the same framework as in our proposed selective fusion except the feature fusion component. All of the networks including baselines were trained with a batch size of 8 using the Adam optimizer, with a learning rate lr = 1e \u22124 . The hyper-parameters inside the networks were identical for a fair comparison.  [12] We used Sequences 00, 01, 02, 04, 06, 08, 09 for training and tested the network on Sequences 05, 07, and 10, excluding sequence 03 as the corresponding raw file is unavailable. The images and ground-truth provided by GPS are collected at 10 Hz, while the IMU data is at 100 Hz. EuRoC Micro Aerial Vehicle dataset [5] It contains tightly synchronized video streams from a Micro Aerial Vehicle (MAV), carrying a stereo camera and an IMU, and is composed by 11 flight trajectories in two environments, exhibiting complex motion. We used Sequence MH 04 difficult for testing, and left the other sequences for training. We downsampled the images and IMUs to 10 Hz and 100 Hz respectively. PennCOSYVIO dataset [28] It is composed by four sequences where the user is carrying multiple visual and inertial sensors rigidly attached. We used Sequences bs, as and bf for training, and af for testing. The images and IMUs were downsampled to 10 Hz and 100 Hz respectively.\n\n\nData Degradation\n\nIn order to provide an extensive study of the effects of sensor data degradation and to evaluate the performances of the proposed approach, we generate three categories of degraded datasets, by adding various types of noise and occlusion to the original data, as described in the following subsections.\n\n\nVision Degradation\n\nOcclusions: we overlay a mask of dimensions 128\u00d7128 pixels on top of the sample images, at random locations for each sample. Occlusions can happen due to dust or dirt on the sensor or stationary objects close to the sensor [37]. Blur+noise: we apply Gaussian blur with \u03c3=15 pixels to the input images, with additional salt-and-pepper noise. Motion blur and noise can happen when the camera or the light condition changes substantially [8].\n\nMissing data: we randomly remove 10% of the input images. This can occur when packets are dropped from the bus due to excess load or temporary sensor disconnection. It can also occur if we pass through an area of very poor illumination e.g. a tunnel or underpass.\n\n\nIMU Degradation\n\nNoise+bias: on top of the already noisy sensor data we add additive white noise to the accelerometer data and a fixed bias on the gyroscope data. This can occur due to increased sensor temperature and mechanical shocks, causing inevitable thermo-mechanical white noise and random walking noise [26]. Missing data: we randomly remove windows of inertial samples between two consecutive random visual frames. This can occur when the IMU measuring is unstable or packets are dropped from the bus.\n\n\nCross-Sensor Degradation\n\nSpatial misalignment: we randomly alter the relative rotation between the camera and the IMU, compared to the initial extrinsic calibration. This can occur due to axis misalignment and the incorrect sensor calibration [20]. We uniformly model up to 10 degrees of misalignment . Temporal misalignment: we apply a time shift between windows of input images and windows of inertial measurements. This can happen due to relative drifts in clocks between independent sensor subsystems [21]. Table 1 shows the relative performance of the proposed data fusion strategies, compared with the baselines. In particular, we compare with a DeepVO [36] (Vision-Only) implementation, and finally with an implementation of VINet [30] (VIO Direct), which uses a na\u00efve fusion strategy by concatenating visual and inertial features. Figure  4 shows a visual comparison of the resulting test trajectories in presence of visual and combined degradations. In the vision degraded set the input images are randomly degraded by adding occlusion, blurring+noise and removing images, with 10% probability for each degradation. In the full degradation set, images and IMU sequences from the dataset are corrupted by all seven degradations with a probability of 5% each. As a metric, we always report the average absolute error on relative translation and rotation estimates   over the trajectory, in order to avoid the shortcomings of approaches using global reference frames to compute errors. Some interesting behaviours emerge from Table 1. Firstly, as expected, both the proposed fusion approaches outperform VO and the baseline VIO fusion approaches when subject to degradation. Our intuition is that the visual features are likely to be local and discrete, and as such, erroneous regions can be blanked out, which would benefit the fusion network when it is predominantly relying on vision. Conversely, inertial data is continuous and thus a more gradual reweighting as performed by the soft fusion approach would preserve these features better. As inertial data is more important for rotation, this could explain this observation. More interestingly, the soft fusion always improves the angle component estimation, while the hard fusion always improves the translation component estimation.  Table 2 shows the aggregate results on the KITTI dataset in presence of normal data, all combined visual degradation and all combined visual+inertial degradation. In particular, we compare with two deep approaches: DeepVO (Vision-Only) and an implementation of VINet (VIO Direct). We can see the same fusion behavior as in Table 1. Table 3 reports the error results on EuRoC. Similar to KITTI, the soft fusion strategy consistently improves the angle estimation, while the hard fusion always improves the translation estimation. Interestingly, in the hand-held sce- nario (Table 4) there is less marked difference between the different fusion strategies regarding the translation component. This can be due to the small size of the dataset and the nature of motion, leading the network to slightly overfit on linear translations. However, hard fusion still improves both errors in presence of both visual and inertial degradation. This could be ascribed to the direct fusion method overfitting on visual data, while a few transitions from outdoor to indoor introduce illumination changes and occlusion.\n\n\nDetailed Investigation on Robustness to Data Corruption\n\n\nComparison with Classical VIOs\n\nFor KITTI, due to the lack of time synchronization between IMUs and images, both OKVIS [19] and VINS-Mono [29] cannot work. We instead provide results from an implementation of MSCKF [15] 2 . For EuRoC MAV we compare with OKVIS [19] 3 .\n\nAs shown in Table 5, on KITTI, MSCKF fails with full degradation due to the missing images; on EuRoc OKVIS handles missing images instead but both baselines fail with full sensor degradation due to the temporal misalignment. Learning-based methods reach comparable position/translation errors, but the orientation error is always lower for traditional methods. Because DNNs shine at extracting features and regressing translation from raw images, while IMUs improve filtering methods to get better orientation results on normal data. Interestingly, the performance of learning-based fusion strategies degrade gracefully in the presence of corrupted data, while filtering methods fail abruptly with the presence of large sensor noise and misalignment issues.\n\n\nInterpretation of Selective Fusion\n\nIncorporating hard mask into our framework enables us to quantitatively and qualitatively interpret the fusion process. Firstly, we analyse the contribution of each individual modality in different scenarios. Since hard fusion blocks Correlations between the number of inertial/visual features and amount of rotation/translation show that the inertial features contribute more with rotation rates, e.g. turning, while more visual features are selected with increasing linear velocity.\n\nsome features according to their reliability, in order to interpret the \"feature selection\" mechanism we simply compare the ratio of the non-blocked features for each modality. Figure 5 shows that visual features dominate compared with inertial features in most scenarios. Non-blocked visual features are more than 60%, underlining the importance of this modality. We see no obvious change when facing small visual degradation, such as image blur, because the FlowNet extractor can deal with such disturbances. However, when the visual degradation becomes stronger the role of inertial features becomes significant. Notably, the two modalities contribute equally in presence of occlusion. Inertial features dominate with missing images by more than 90%.\n\nIn Figure 6 we analyze the correlation between amount of linear and angular velocity and the selected features. These results also show how the belief on inertial features is stronger in presence of large rotations, e.g. turning, while visual features are more reliable with increasing linear translations. It is interesting to see that at low translational velocity (0.5m / 0.1s) only 50% to 60% visual features are activated, while at high speed (1.5m / 0.1s) 60 % to 75 % visual features are used.\n\n\nRelated Work\n\nVisual Inertial Odometry Traditionally, visual-inertial approaches can be roughly segmented into three different classes according to their information fusion methods: filtering approaches [17], fixed-lag smoothers [19] and full smoothing methods [11]. In classical VIO models, their features are handcrafted, as OKVIS [19] presented a keyframebased approach that jointly optimizes visual feature reprojections and inertial error terms. Semi-direct [32] and direct [34] methods have been proposed in an effort to move towards feature-less approaches, removing the feature extraction pipeline for increased speed. Recent VINet [30] used neural network to learn visual-inertial navigation, but only fused two modalities in a naive concatenation way. We provide a generic framework for deep features fusion, and outperformed the direct fusion in different scenarios.\n\nDeep Neural Networks for Localization Recent datadriven approaches to visual odometry have gained a lot of attention. The advantage of learned methods is their potential robustness to lack of features, dynamic lightning conditions, motion blur, accurate camera calibration, which are hard to model by hard [31]. Posenet [18] used Convolutional Neural Networks (CNNs) for 6-DoF pose regression from monocular images. The combination of CNNs and Long-Short Term Memory (LSTM) networks was reported in [7,36], showing comparable results to traditional methods. Several approaches [43,41,42] used the view synthesis as unsupervisory signal to train and estimate both egomotion and depth. Other DL-based methods can be found on learning representations for dense visual SLAM [3], general map [4], global pose [27], deep Localization and segmentation [35]. We study the contribution of multimodal data to robust deep localization in degraded scenarios.\n\nMultimodal Sensor fusion and Attention Our proposed selective sensor fusion is related with the attention mechanisms, widely applied in neural machine translation [33], image caption generation [39], and video description [14]. Limited by the fixed-length vector in embedding space, these attention mechanisms compute a focus map to help the decoder, when generating a sequence of words. This is different from our design intention that the features selection works to fuse multimodal sensor fusion for visual inertial odometry, and cope with more complex error resources, and self-motion dynamics.\n\n\nConclusion\n\nIn this work, we presented a novel study of end-to-end sensor fusion for visual-inertial navigation. Two feature selection strategies are proposed: deterministic soft fusion, in which a soft mask is learned from the concatenated visual and inertial features, and a stochastic hard fusion, in which Gumbel-softmax resampling is used to learn a stochastic binary mask. Based on the extensive experiments, we also provided insightful interpretations of selective sensor fusion and investigate the influence of different modalities under different degradation and self-motion circumstances. their timestamps. We used Sequences 00, 01, 02, 04, 06, 08, 09 for training and tested the network on Sequences 05, 07, and 10, excluding sequence 03 as the corresponding raw file is unavailable. Correspondences between the KITTI Odometry Number, the raw data file names and the corresponding number of sequences are reported in Table 2. The images are resized to 512\u00d7256.\n\nEuRoC For the EuRoC MAV dataset [1] we use grayscale video images from CAM1 at 20fps of the VI-Sensor and the IMU measurements from the same sensors at 200Hz. The data is tightly synchronized. Doubling the sensor rates compared to the ones in KITTI helps to deal with the faster, jerky MAV movements. As with KITTI, images are resized to 512\u00d7256. We train on all sequences, minus MH 04 difficult, which is used for testing.\n\nPennCOSYVIO A number of sensors are included in the PennCOSYVIO dataset [7]. For our experiment we select the video images from the Tango Bottom camera, running at 30fps, and inertial measurements from the VI-Sensor, running at 200Hz. Images are subsampled to 10Hz, and IMU measurements are subsampled to 100Hz, similarly to KITTI. Images are cropped and resized to 512\u00d7256. We train on sequences as, bf, bs and test on sequence af. Small time-synchronization errors between the Tango camera and the VI-Sensor are likely present in the data, leading to worse results in all scenarios. Figure 1 shows the global RMSE position errors on the three test KITTI trajectories (Seq 05, Seq 07, Seq 10) as a function of travelled distance, for both the normal dataset and the fully degraded dataset (visual degradation + sensor degradation). We compare the two VO and vanilla VIO baselines with the proposed soft and hard fusion strategies. It can be noticed how, while at start VIO performs as well as soft and hard fusion, on average, over time the proposed selective fusion strategies outperform the vanilla fusion, since the increased robustness reduces error accumulation. This is particularly visible in the most challenging Seq 05. As expected, a VO approach heavily underperforms in presence of large amounts of angular rotations (Seq 05, Seq 07).\n\n\nEvaluation of global trajectories on KITTI\n\nAnother interesting result is how VO performs slightly better in presence of IMU degradation and camera-IMU synchronization ( Figure 1, bottom row). That shows how a vanilla VIO fusion is unable to deal with these issues, to the point of underperforming compared to vision-only approaches. This result further corroborates the fact that in deep learning-based approaches explicitly learning the belief on the different components makes the estimation more robust, while stacking sensors without a sensible fusion strategy can lead to catastrophic fusion, similarly to traditional approaches. Catastrophic fusion happens when the single components of the system before fusion significantly outperform the overall system after fusion [6].\n\n\nSupplementary Video\n\nThe supplementary video shows an evaluation of soft fusion and hard fusion on Seq 05 of the KITTI dataset, with vision degradation (10% occlusion, 10% blur+noise, 10% missing data), compared with two deep VO and VIO baselines. The degraded images and corresponding soft/hard masks are shown in the top-left and bottom-left respectively. The trajectories from the four methods are shown on the right side.\n\nFigure 3 :\n3An illustration of our proposed soft (deterministic) and hard (stochastic) feature selection process.\n\nFigure 4 :\n4Estimated trajectories on the KITTI dataset. Top row: dataset with vision degradation (10% occlussion, 10% blur, and 10% missing data); bottom row: data with all degradation (5% for each). Here, GT, VO, VIO, Soft and Hard mean the ground truth, neural vision-only model, neural visual inertial models with direct, soft, and hard fusion.\n\nFigure 5 :\n5A comparison of visual and inertial features selection rate in seven data degradation scenarios.\n\n\nFigure 6: Correlations between the number of inertial/visual features and amount of rotation/translation show that the inertial features contribute more with rotation rates, e.g. turning, while more visual features are selected with increasing linear velocity.\n\nFigure 1 :\n1Global position errors (in meters, Y axis) on the KITTI dataset, over travelled distance (in meters, X axis).\n\nTable 1 :\n1Effectiveness of different sensor fusion strategies in presence of different kinds of sensor data corruption. For each case we report absolute translational error (m) and rotational error (degrees).Vision Degradation \nIMU Degradation \nSensor Degradation \nModel \nOcclusion \nBlur \nMissing \nNoise and bias \nMissing \nSpatial \nTemporal \nVision Only 0.117,0.148 0.117,0.153 0.213,0.456 \n0.116,0.136 \n0.116,0.136 0.116,0.136 0.116,0.136 \nVIO Direct 0.116,0.110 0.117,0.107 0.191,0.155 \n0.118,0.115 \n0.118,0.163 0.119,0.137 0.120,0.111 \nVIO Soft \n0.116,0.105 0.119,0.104 0.198,0.149 \n0.119, 0.105 \n0.118,0.129 0.119,0.128 0.119,0.108 \nVIO Hard \n0.112,0.126 0.114,0.110 0.187,0.159 \n0.114,0.120 \n0.115,0.140 0.111,0.146 0.113,0.133 \n\n4.2. Datasets \n\nKITTI Odometry dataset \n\nTable 2 :\n2Results on autonomous driving scenario[12].Normal Data Vision Degradation All Degradation \nVision Only 0.116,0.136 \n0.177,0.355 \n0.142,0.281 \nVIO Direct \n0.116,0.106 \n0.175,0.164 \n0.148,0.139 \nVIO Soft \n0.118,0.098 \n0.173,0.150 \n0.152,0.134 \nVIO Hard \n0.112,0.110 \n0.172,0.151 \n0.145,0.150 \n\n\n\nTable 3 :\n3Results on UAV scenario[5].Normal Data \nVision Degradation All Degradation \nVision Only 0.00976,0.0867 \n0.0222,0.268 \n0.0190,0.213 \nVIO Direct 0.00765,0.0540 \n0.0181,0.0696 \n0.0162,0.0935 \nVIO Soft \n0.00848,0.0564 \n0.0170,0.0533 \n0.0152,0.0860 \nVIO Hard \n0.00795,0.0589 \n0.0177,0.0565 \n0.0157,0.0823 \n\n\n\nTable 4 :\n4Results on handheld scenario[28].Normal Data Vision Degradation All Degradation \nVision Only 0.0379,1.755 \n0.0446,1.849 \n0.0414,1.875 \nVIO Direct 0.0377,1.350 \n0.0396, 1.223 \n0.0407,1.353 \nVIO Soft \n0.0381,1.252 \n0.0399,1.166 \n0.0405,1.296 \nVIO Hard \n0.0387,1.296 \n0.0410,1.206 \n0.0400,1.232 \n\n\n\nTable 5 :\n5Comparison with classical methodsNormal data \nFull visual degr. \nOccl.+blur \nFull sensor degr. \nKITTI \n0.116,0.044 \nFail \n2.4755,0.0726 \nFail \nEuRoC 0.0283,0.0402 \n0.0540,0.0591 \n0.0198,0.0400 \nFail \n\n4.5. Results on autonomous driving, UAV scenario \nand hand-held scenario \n\n\nhttps://changhaoc.github.io/selective sensor fusion/\nThe code can be found at: https://uk.mathworks.com/matlabcentral/ fileexchange/43218-visual-inertial-odometry3  The code can be found at: https://github.com/ethz-asl/okvis\nhttps://lmb.informatik.uni-freiburg.de/ resources/datasets/FlyingChairs.en.html\nSupplementary MaterialAbstract In this supplementary document we provide additional implementation details, including the detailed network architecture, and data preparation/training details. We also provide additional results and discussion on predicted global trajectories.Network ArchitectureThe implementation details for the architecture that was used in our experiments are reported inTable 1. We use the following abbreviations: Conv: convolution, FC: fully-connected, activ.: activation function, \u2295: concatenation; : element-wise multiplication; B: batch size.The visual encoder follows the encoder structure of the FlowNetS architecture[3]. We initialize the visual encoder weights with the weights of a model that was pre-trained on the FlyingChairs dataset 1 , since training from scratch would require larger amounts of data compared to our dataset sizes. For this reason, training the encoder from scratch experimentally gave worse results.Bi-directional LSTMs were used for the inertial encoder (as in[2]) as well as in the recurrent part of the pose regressor. Bi-directional RNNs provide higher data efficiency when dealing with small datasets. The LSTM layers include dropout regularization on the recurrent connections.Dataset Preprocessing DetailsFew publicly available datasets provide camera images, high-frequency IMU and ground-truth for training visual inertial odometry (i.e., inertial data is missing in the Oxford Robotcar Dataset[5]and 7-Scenes Dataset[9]; groundtruth for the full trajectories is not available in TUM VIO[8]). Therefore, we use the KITTI[4], EuRoC[1], and Pen-nCOSYVIO[7]datasets for training and evaluation.KITTI High-frequency inertial data (100Hz) is only available in the raw unsynced data packages. Hence, we manually synchronize inertial data and images according to\nC Bishop, Pattern Recognition and Machine Learning. SpringerC. Bishop. Pattern Recognition and Machine Learning. Springer, 2006. 5\n\nIterated extended kalman filter visual-inertial odometry using direct photometric feedback. M Bloesch, M Burri, S Omari, M Hutter, R Siegwart, The International Journal of Robotics Research. 3610M. Bloesch, M. Burri, S. Omari, M. Hutter, and R. Siegwart. Iterated extended kalman filter visual-inertial odometry us- ing direct photometric feedback. The International Journal of Robotics Research, 36(10):1053-1072, 2017. 5\n\nCodeSLAM Learning a Compact, Optimisable Representation for Dense Visual SLAM. M Bloesch, J Czarnowski, R Clark, S Leutenegger, A J Davison, In CVPR. 8M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison. CodeSLAM Learning a Compact, Opti- misable Representation for Dense Visual SLAM. In CVPR, 2018. 8\n\nGeometry-Aware Learning of Maps for Camera Localization. S Brahmbhatt, J Gu, K Kim, J Hays, J Kautz, CVPR. 1S. Brahmbhatt, J. Gu, K. Kim, J. Hays, and J. Kautz. Geometry-Aware Learning of Maps for Camera Localiza- tion. In CVPR, pages 2616-2625, 2018. 1, 8\n\nThe euroc micro aerial vehicle datasets. M Burri, J Nikolic, P Gohl, T Schneider, J Rehder, S Omari, M W Achtelik, R Siegwart, The International Journal of Robotics Research. 657M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart. The euroc micro aerial vehicle datasets. The International Journal of Robotics Research, 2016. 5, 6, 7\n\nIonet: Learning to cure the curse of drift in inertial odometry. C Chen, C X Lu, A Markham, N Trigoni, AAAI Conference on Artificial Intelligence (AAAI). C. Chen, C. X. Lu, A. Markham, and N. Trigoni. Ionet: Learning to cure the curse of drift in inertial odometry. In AAAI Conference on Artificial Intelligence (AAAI), 2018. 3\n\nVidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization. R Clark, S Wang, A Markham, N Trigoni, H Wen, CVPR. R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen. VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video- Clip Relocalization. In CVPR, 2017. 1, 8\n\nLearning to estimate and remove non-uniform image blur. F Couzinie-Devy, J Sun, K Alahari, J Ponce, CVPR. F. Couzinie-Devy, J. Sun, K. Alahari, and J. Ponce. Learning to estimate and remove non-uniform image blur. In CVPR, pages 1075-1082, 2013. 6\n\nAngelaki. Dynamic Reweighting of Visual and Vestibular Cues during Self-Motion Perception. C R Fetsch, A H Turner, G C Deangelis, D E , Journal of Neuroscience. 2949C. R. Fetsch, A. H. Turner, G. C. DeAngelis, and D. E. An- gelaki. Dynamic Reweighting of Visual and Vestibular Cues during Self-Motion Perception. Journal of Neuroscience, 29(49):15601-15612, 2009. 1\n\nFlowNet: Learning Optical Flow with Convolutional Networks. P Fischer, E Ilg, H Philip, C Hazrbas, P V D Smagt, D Cremers, T Brox, International Conference on Computer Vision, ICCV. P. Fischer, E. Ilg, H. Philip, C. Hazrbas, P. V. D. Smagt, D. Cremers, and T. Brox. FlowNet: Learning Optical Flow with Convolutional Networks. In International Conference on Computer Vision, ICCV, 2015. 2\n\nOnmanifold preintegration for real-time visual-inertial odometry. C Forster, L Carlone, F Dellaert, D Scaramuzza, IEEE Transactions on Robotics. 331C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza. On- manifold preintegration for real-time visual-inertial odome- try. IEEE Transactions on Robotics, 33(1):1-21, 2017. 1, 3, 8\n\nVision meets robotics: The KITTI dataset. A Geiger, P Lenz, C Stiller, R Urtasun, The International Journal of Robotics Research. 32117A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013. 5, 6, 7\n\nStatistical theory of extreme values and some practical applications: a series of lectures. E J Gumbel, U. S. Govt. Print. Office. 5E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures. U. S. Govt. Print. Office, 1954. 5\n\nAttention-Based Multimodal Fusion for Video Description. C Hori, T Hori, T Y Lee, Z Zhang, B Harsham, J R Hershey, T K Marks, K Sumi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision4C. Hori, T. Hori, T. Y. Lee, Z. Zhang, B. Harsham, J. R. Hershey, T. K. Marks, and K. Sumi. Attention-Based Mul- timodal Fusion for Video Description. Proceedings of the IEEE International Conference on Computer Vision, 2017- October:4203-4212, 2017. 4, 8\n\nA sliding-window visual-IMU odometer based on tri-focal tensor geometry. J S Hu, M Y Chen, ICRA. IEEE57J. S. Hu and M. Y. Chen. A sliding-window visual-IMU odometer based on tri-focal tensor geometry. In ICRA, pages 3963-3968. IEEE, 2014. 5, 7\n\nE Jang, S Gu, B Poole, arXiv:1611.01144Categorical reparameterization with gumbel-softmax. arXiv preprintE. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. 4\n\nVisual-inertial navigation, mapping and localization: A scalable real-time causal approach. E S Jones, S Soatto, The International Journal of Robotics Research. 304E. S. Jones and S. Soatto. Visual-inertial navigation, map- ping and localization: A scalable real-time causal approach. The International Journal of Robotics Research, 30(4):407- 430, 2011. 1, 8\n\nPosenet: A convolutional network for real-time 6-dof camera relocalization. A Kendall, M Grimes, R Cipolla, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionA. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu- tional network for real-time 6-dof camera relocalization. In Proceedings of the IEEE international conference on com- puter vision, pages 2938-2946, 2015. 1, 2, 8\n\nKeyframe-based visual-inertial odometry using nonlinear optimization. S Leutenegger, S Lynen, M Bosse, R Siegwart, P Furgale, The International Journal of Robotics Research. 343S. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Fur- gale. Keyframe-based visual-inertial odometry using non- linear optimization. The International Journal of Robotics Research, 34(3):314-334, 2015. 1, 7, 8\n\nHigh-precision, Consistent EKFbased Visual-Inertial Odometry. M Li, A I Mourikis, The International Journal of Robotics Research. 3266M. Li and A. I. Mourikis. High-precision, Consistent EKF- based Visual-Inertial Odometry. The International Journal of Robotics Research, 32(6):690-711, 2013. 1, 5, 6\n\nModeling Varying Camera-IMU Time Offset in Optimization-Based Visual-Inertial Odometry. Y Ling, L Bao, Z Jie, F Zhu, Z Li, S Tang, Y Liu, W Liu, T Zhang, The European Conference on Computer Vision (ECCV). 6Y. Ling, L. Bao, Z. Jie, F. Zhu, Z. Li, S. Tang, Y. Liu, W. Liu, and T. Zhang. Modeling Varying Camera-IMU Time Offset in Optimization-Based Visual-Inertial Odometry. In The Eu- ropean Conference on Computer Vision (ECCV), 2018. 1, 6\n\nThe concrete distribution: A continuous relaxation of discrete random variables. C J Maddison, A Mnih, Y W Teh, arXiv:1611.00712arXiv preprintC. J. Maddison, A. Mnih, and Y. W. Teh. The concrete dis- tribution: A continuous relaxation of discrete random vari- ables. arXiv preprint arXiv:1611.00712, 2016. 4\n\nC J Maddison, D Tarlow, T Minka, Sampling, NIPS. C. J. Maddison, D. Tarlow, and T. Minka. A* Sampling. In NIPS, pages 1-9, 2014. 5\n\nA Mnih, K Gregor, arXiv:1402.0030Neural variational inference and learning in belief networks. arXiv preprintA. Mnih and K. Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030, 2014. 4\n\nA multi-state constraint Kalman filter for vision-aided inertial navigation. A I Mourikis, S I Roumeliotis, Proceedings -IEEE International Conference on Robotics and Automation. -IEEE International Conference on Robotics and AutomationA. I. Mourikis and S. I. Roumeliotis. A multi-state constraint Kalman filter for vision-aided inertial navigation. In Pro- ceedings -IEEE International Conference on Robotics and Automation, pages 3565-3572, 2007. 5\n\n. N Naser, El-SheimyN. Naser, El-Sheimy;\n\nAnalysis and Modeling of Inertial Sensors Using Allan Variance. Hou ; Haiying, Xiaojii, IEEE Transactions on Instrumentation and Measurement. 576Haiying, Hou; Xiaojii. Analysis and Modeling of Inertial Sensors Using Allan Variance. IEEE Transactions on Instrumentation and Measurement, 57(JANUARY):684-694, 2008. 1, 6\n\nGlobal Pose Estimation with an Attention-based Recurrent Network. E Parisotto, D S Chaplot, J Zhang, R Salakhutdinov, CVPR. E. Parisotto, D. S. Chaplot, J. Zhang, and R. Salakhutdinov. Global Pose Estimation with an Attention-based Recurrent Network. In CVPR, 2018. 8\n\nPenncosyvio: A challenging visual inertial odometry benchmark. B Pfrommer, N Sanket, K Daniilidis, J Cleveland, 2017 IEEE International Conference on Robotics and Automation. Singapore, Singapore67B. Pfrommer, N. Sanket, K. Daniilidis, and J. Cleveland. Penncosyvio: A challenging visual inertial odometry bench- mark. In 2017 IEEE International Conference on Robotics and Automation, ICRA 2017, Singapore, Singapore, May 29 -June 3, 2017, pages 3847-3854, 2017. 5, 6, 7\n\nVins-mono: A robust and versatile monocular visual-inertial state estimator. T Qin, P Li, S Shen, IEEE Transactions on Robotics. 344T. Qin, P. Li, and S. Shen. Vins-mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics, 34(4):1004-1020, Aug 2018. 1, 7\n\nVinet: Visualinertial odometry as a sequence-to-sequence learning problem. H W A M N T Ronald Clark, Sen Wang, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. AAAI. the Thirty-First AAAI Conference on Artificial Intelligence. AAAI6H. W. A. M. N. T. Ronald Clark, Sen Wang. Vinet: Visual- inertial odometry as a sequence-to-sequence learning prob- lem. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. AAAI, 2017. 1, 6, 8\n\nThe limits and potentials of deep learning for robotics. N S\u00fcnderhauf, O Brock, W Scheirer, R Hadsell, D Fox, J Leitner, B Upcroft, P Abbeel, W Burgard, M Milford, P Corke, International Journal of Robotics Research. 374-5N. S\u00fcnderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B. Upcroft, P. Abbeel, W. Burgard, M. Milford, and P. Corke. The limits and potentials of deep learning for robotics. International Journal of Robotics Research, 37(4- 5):405-420, 2018. 8\n\nSemi-direct ekf-based monocular visual-inertial odometry. P Tanskanen, T Naegeli, M Pollefeys, O Hilliges, Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on. IEEEP. Tanskanen, T. Naegeli, M. Pollefeys, and O. Hilliges. Semi-direct ekf-based monocular visual-inertial odometry. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 6073-6078. IEEE, 2015. 8\n\nAttention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 4A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention Is All You Need. In NIPS, 2017. 4, 8\n\nDirect sparse visual-inertial odometry using dynamic marginalization. L Stumberg, V Usenko, D Cremers, L. von Stumberg, V. Usenko, and D. Cremers. Direct sparse visual-inertial odometry using dynamic marginaliza- tion, 2018. 8\n\nDeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map. P Wang, R Yang, B Cao, W Xu, Y Lin, In CVPR. 8P. Wang, R. Yang, B. Cao, W. Xu, and Y. Lin. DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map. In CVPR, 2018. 8\n\nDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, International Conference on Robotics and Automation. 6S. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo: To- wards end-to-end visual odometry with deep recurrent con- volutional neural networks. International Conference on Robotics and Automation, 2017. 6, 8\n\nOcclusionaware depth estimation using light-field cameras. T C Wang, A A Efros, R Ramamoorthi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionT. C. Wang, A. A. Efros, and R. Ramamoorthi. Occlusion- aware depth estimation using light-field cameras. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 3487-3495, 2015. 6\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4R. J. Williams. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992. 4\n\nAttend and Tell: Neural Image Caption Generation with Visual Attention. K Xu, J Ba, R Kiros, K Cho, A Courville, R Salakhutdinov, R Zemel, Y Bengio, Show, ICML. 4K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdi- nov, R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML, 2015. 4, 8\n\nChallenges in Monocular Visual Odometry: Photometric Calibration, Motion Bias and Rolling Shutter Effect. N Yang, R Wang, X Gao, D Cremers, IEEE ROBOTICS AND AUTOMATION LETTERS. 1N. Yang, R. Wang, X. Gao, and D. Cremers. Challenges in Monocular Visual Odometry: Photometric Calibration, Mo- tion Bias and Rolling Shutter Effect. IEEE ROBOTICS AND AUTOMATION LETTERS, pages 1-8, 2018. 1\n\nGeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose. Z Yin, J Shi, CVPR. 2Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose. In CVPR, 2018. 2, 8\n\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction. H Zhan, R Garg, C S Weerasekera, K Li, H Agarwal, I Reid, CVPR. 2H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. Reid. Unsupervised Learning of Monocular Depth Esti- mation and Visual Odometry with Deep Feature Reconstruc- tion. In CVPR, pages 340-349, 2018. 2, 8\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. 2T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu- pervised learning of depth and ego-motion from video. In CVPR, volume 2, page 7, 2017. 2, 8\n\nThe euroc micro aerial vehicle datasets. M Burri, J Nikolic, P Gohl, T Schneider, J Rehder, S Omari, M W Achtelik, R Siegwart, The International Journal of Robotics Research. 1M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart. The euroc micro aerial vehicle datasets. The International Journal of Robotics Research, 2016. 1\n\nIonet: Learning to cure the curse of drift in inertial odometry. C Chen, C X Lu, A Markham, N Trigoni, AAAI Conference on Artificial Intelligence (AAAI). C. Chen, C. X. Lu, A. Markham, and N. Trigoni. Ionet: Learn- ing to cure the curse of drift in inertial odometry. In AAAI Conference on Artificial Intelligence (AAAI), 2018. 1\n\nFlowNet: Learning Optical Flow with Convolutional Networks. P Fischer, E Ilg, H Philip, C Hazrbas, P V D Smagt, D Cremers, T Brox, International Conference on Computer Vision, ICCV. P. Fischer, E. Ilg, H. Philip, C. Hazrbas, P. V. D. Smagt, D. Cremers, and T. Brox. FlowNet: Learning Optical Flow with Convolutional Networks. In International Conference on Computer Vision, ICCV, 2015. 1\n\nVision meets robotics: The KITTI dataset. A Geiger, P Lenz, C Stiller, R Urtasun, The International Journal of Robotics Research. 3211A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013. 1\n\n1 Year, 1000km: The Oxford RobotCar Dataset. W Maddern, G Pascoe, C Linegar, P Newman, The International Journal of Robotics Research (IJRR). 361W. Maddern, G. Pascoe, C. Linegar, and P. Newman. 1 Year, 1000km: The Oxford RobotCar Dataset. The International Journal of Robotics Research (IJRR), 36(1):3-15, 2016. 1\n\nRobust sensor fusion: Analysis and application to audio visual speech recognition. J R Movellan, P Mineiro, Machine Learning. 32J. R. Movellan and P. Mineiro. Robust sensor fusion: Analysis and application to audio visual speech recognition. Machine Learning, 32(2):85-100, 1998. 2\n\nPenncosyvio: A challenging visual inertial odometry benchmark. B Pfrommer, N Sanket, K Daniilidis, J Cleveland, 2017 IEEE International Conference on Robotics and Automation. Singapore, SingaporeB. Pfrommer, N. Sanket, K. Daniilidis, and J. Cleveland. Pen- ncosyvio: A challenging visual inertial odometry benchmark. In 2017 IEEE International Conference on Robotics and Au- tomation, ICRA 2017, Singapore, Singapore, May 29 -June 3, 2017, pages 3847-3854, 2017. 1\n\nThe TUM VI Benchmark for Evaluating Visual-Inertial Odometry. D Schubert, T Goll, N Demmel, V Usenko, J St\u00fcckler, D Cremers, ICRA. D. Schubert, T. Goll, N. Demmel, V. Usenko, J. St\u00fcckler, and D. Cremers. The TUM VI Benchmark for Evaluating Visual- Inertial Odometry. In ICRA, 2018. 1\n\nScene coordinate regression forests for camera relocalization in RGB-D images. J Shotton, B Glocker, C Zach, S Izadi, A Criminisi, A Fitzgibbon, CVPR. J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR, pages 2930-2937, 2013. 1\n\nStride 2 2 , Padding 3, LeakyReLU activ. [ layer 2 ] Conv. 5 2 , Stride 2 2 , Padding 2, LeakyReLU activ. Stride 2 2 , Padding 2, LeakyReLU activ. layer 3 1 ] Conv. 3 2 , Stride 1 2 , Padding 1 [ layer 4 ] Conv. 3 2 , Stride 2 2 , Padding 2, LeakyReLU activ. layer 4 1 ] Conv. 3 2 , Stride 1 2 , Padding 1 [ layer 5 ] Conv. 3 2 , Stride 2 2 , Padding 2, LeakyReLU activ. layer 5 1 ] Conv. 3 2 , Stride 1 2 , Padding 1Visual Encoder [ input ] Two stacked images: B \u00d7 512 \u00d7 256 \u00d7 6 [ layer 1 ] Conv. 7 2 , Stride 2 2 , Padding 3, LeakyReLU activ. [ layer 2 ] Conv. 5 2 , Stride 2 2 , Padding 2, LeakyReLU activ. [ layer 3 ] Conv. 5 2 , Stride 2 2 , Padding 2, LeakyReLU activ. [ layer 3 1 ] Conv. 3 2 , Stride 1 2 , Padding 1 [ layer 4 ] Conv. 3 2 , Stride 2 2 , Padding 2, LeakyReLU activ. [ layer 4 1 ] Conv. 3 2 , Stride 1 2 , Padding 1 [ layer 5 ] Conv. 3 2 , Stride 2 2 , Padding 2, LeakyReLU activ. [ layer 5 1 ] Conv. 3 2 , Stride 1 2 , Padding 1\n\n. B \u00d7 256, B \u00d7 256\n\nFC 128. FC 128\n\n. B-Lstm, 2-layers, hidden size 128, Dropout 0.2B-LSTM, 2-layers, hidden size 128, Dropout 0.2\n\n. B \u00d7 256, B \u00d7 256\n\nDirect Fusion Module. Direct Fusion Module\n\n. B \u00d7, B \u00d7 (256 \u2295 256)\n\n. B \u00d7 512, B \u00d7 512\n\n. B \u00d7 512, B \u00d7 512\n\nHard Fusion Module. Hard Fusion Module\n\n. B \u00d7, layer 1 ] FC 1024. Sigmoid activ.. inputB \u00d7 (256 \u2295 256) [ layer 1 ] FC 1024, Sigmoid activ. (input)\n\n. Gumbel-Softmax Sampling, 512layer 2 ] [ (input) (layer 2Gumbel-Softmax sampling, 512 \u00d7 2 [ layer 2 ] [ (input) (layer 2)\n\n. B \u00d7 512, B \u00d7 512\n\nPose Regressor. Pose Regressor\n\n. B \u00d7 512, B \u00d7 512\n\n. B-Lstm, 2-layers, hidden size 512, Dropout 0.2B-LSTM, 2-layers, hidden size 512, Dropout 0.2\n\n. B \u00d7 6, B \u00d7 6\n\nTable 1: Implementation details for the proposed architecture. \u2295 denotes a concatenation operation; indicates an element-wise product between two tensors. Table 2: Correspondences between the KITTI Odometry Number, the raw data file names and the corresponding number of sequences Odometry Nr. Raw Data Filename Number of Sequences 00Table 1: Implementation details for the proposed architec- ture. \u2295 denotes a concatenation operation; indicates an element-wise product between two tensors. Table 2: Correspondences between the KITTI Odometry Number, the raw data file names and the corresponding number of sequences Odometry Nr. Raw Data Filename Number of Sequences 00\n", "annotations": {"author": "[{\"end\":131,\"start\":63},{\"end\":199,\"start\":132},{\"end\":239,\"start\":200},{\"end\":312,\"start\":240},{\"end\":320,\"start\":313},{\"end\":390,\"start\":321},{\"end\":458,\"start\":391}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":72},{\"end\":144,\"start\":140},{\"end\":210,\"start\":206},{\"end\":257,\"start\":255},{\"end\":319,\"start\":317},{\"end\":335,\"start\":328},{\"end\":403,\"start\":396}]", "author_first_name": "[{\"end\":71,\"start\":63},{\"end\":139,\"start\":132},{\"end\":205,\"start\":200},{\"end\":245,\"start\":240},{\"end\":254,\"start\":246},{\"end\":316,\"start\":313},{\"end\":327,\"start\":321},{\"end\":395,\"start\":391}]", "author_affiliation": "[{\"end\":130,\"start\":78},{\"end\":198,\"start\":146},{\"end\":238,\"start\":212},{\"end\":311,\"start\":259},{\"end\":389,\"start\":337},{\"end\":457,\"start\":405}]", "title": "[{\"end\":60,\"start\":1},{\"end\":518,\"start\":459}]", "venue": null, "abstract": "[{\"end\":1923,\"start\":520}]", "bib_ref": "[{\"end\":2164,\"start\":2161},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2335,\"start\":2331},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2338,\"start\":2335},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2341,\"start\":2338},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2344,\"start\":2341},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2347,\"start\":2344},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2870,\"start\":2866},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2901,\"start\":2897},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2904,\"start\":2901},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2907,\"start\":2904},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3174,\"start\":3170},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3229,\"start\":3225},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3304,\"start\":3300},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3402,\"start\":3398},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3426,\"start\":3422},{\"end\":3428,\"start\":3426},{\"end\":3819,\"start\":3816},{\"end\":3821,\"start\":3819},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3824,\"start\":3821},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3859,\"start\":3855},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7508,\"start\":7504},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7511,\"start\":7508},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7514,\"start\":7511},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7541,\"start\":7537},{\"end\":8512,\"start\":8509},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10892,\"start\":10888},{\"end\":11142,\"start\":11139},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12488,\"start\":12484},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12491,\"start\":12488},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12494,\"start\":12491},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":14225,\"start\":14221},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14228,\"start\":14225},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14364,\"start\":14360},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14367,\"start\":14364},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15578,\"start\":15574},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15879,\"start\":15875},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17439,\"start\":17435},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17442,\"start\":17439},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17445,\"start\":17442},{\"end\":17447,\"start\":17445},{\"end\":17531,\"start\":17528},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18171,\"start\":18167},{\"end\":18219,\"start\":18216},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18275,\"start\":18271},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19173,\"start\":19169},{\"end\":19491,\"start\":19488},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19883,\"start\":19879},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20708,\"start\":20704},{\"end\":20919,\"start\":20916},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21503,\"start\":21499},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21949,\"start\":21945},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22211,\"start\":22207},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22365,\"start\":22361},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22444,\"start\":22440},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25284,\"start\":25280},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25303,\"start\":25299},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25380,\"start\":25376},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28178,\"start\":28174},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28204,\"start\":28200},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28236,\"start\":28232},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28308,\"start\":28304},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28438,\"start\":28434},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28454,\"start\":28450},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28615,\"start\":28611},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29160,\"start\":29156},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29174,\"start\":29170},{\"end\":29352,\"start\":29349},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29355,\"start\":29352},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29431,\"start\":29427},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29434,\"start\":29431},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29437,\"start\":29434},{\"end\":29623,\"start\":29620},{\"end\":29640,\"start\":29637},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29658,\"start\":29654},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29699,\"start\":29695},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29965,\"start\":29961},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29996,\"start\":29992},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30024,\"start\":30020},{\"end\":31407,\"start\":31404},{\"end\":31872,\"start\":31869},{\"end\":33925,\"start\":33922},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36147,\"start\":36143},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36757,\"start\":36753}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34469,\"start\":34355},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34819,\"start\":34470},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34929,\"start\":34820},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35192,\"start\":34930},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35315,\"start\":35193},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36092,\"start\":35316},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36397,\"start\":36093},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36712,\"start\":36398},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37019,\"start\":36713},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":37308,\"start\":37020}]", "paragraph": "[{\"end\":2165,\"start\":1939},{\"end\":3305,\"start\":2167},{\"end\":3991,\"start\":3307},{\"end\":4563,\"start\":3993},{\"end\":5709,\"start\":4565},{\"end\":5762,\"start\":5711},{\"end\":5909,\"start\":5764},{\"end\":6085,\"start\":5911},{\"end\":6340,\"start\":6087},{\"end\":7105,\"start\":6384},{\"end\":8302,\"start\":7125},{\"end\":8800,\"start\":8329},{\"end\":9110,\"start\":8847},{\"end\":9892,\"start\":9133},{\"end\":10497,\"start\":9935},{\"end\":11329,\"start\":10551},{\"end\":11842,\"start\":11331},{\"end\":12187,\"start\":11860},{\"end\":12302,\"start\":12228},{\"end\":12726,\"start\":12334},{\"end\":12932,\"start\":12728},{\"end\":13511,\"start\":13000},{\"end\":14471,\"start\":13586},{\"end\":14696,\"start\":14473},{\"end\":15195,\"start\":14790},{\"end\":15777,\"start\":15313},{\"end\":15880,\"start\":15822},{\"end\":16205,\"start\":15925},{\"end\":16361,\"start\":16248},{\"end\":16514,\"start\":16445},{\"end\":17312,\"start\":16559},{\"end\":18036,\"start\":17314},{\"end\":18356,\"start\":18052},{\"end\":18475,\"start\":18393},{\"end\":20135,\"start\":18477},{\"end\":20458,\"start\":20156},{\"end\":20920,\"start\":20481},{\"end\":21185,\"start\":20922},{\"end\":21698,\"start\":21205},{\"end\":25100,\"start\":21727},{\"end\":25429,\"start\":25193},{\"end\":26188,\"start\":25431},{\"end\":26711,\"start\":26227},{\"end\":27466,\"start\":26713},{\"end\":27968,\"start\":27468},{\"end\":28848,\"start\":27985},{\"end\":29796,\"start\":28850},{\"end\":30396,\"start\":29798},{\"end\":31370,\"start\":30411},{\"end\":31795,\"start\":31372},{\"end\":33143,\"start\":31797},{\"end\":33926,\"start\":33190},{\"end\":34354,\"start\":33950}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8328,\"start\":8303},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8828,\"start\":8801},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9132,\"start\":9111},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10524,\"start\":10498},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12227,\"start\":12188},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12966,\"start\":12933},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12999,\"start\":12966},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13558,\"start\":13512},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14742,\"start\":14697},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14789,\"start\":14742},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15243,\"start\":15196},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15277,\"start\":15243},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15312,\"start\":15277},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15821,\"start\":15778},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15924,\"start\":15881},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16247,\"start\":16206},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16444,\"start\":16362}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22220,\"start\":22213},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23241,\"start\":23234},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24005,\"start\":23998},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24328,\"start\":24321},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24337,\"start\":24330},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24578,\"start\":24570},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25450,\"start\":25443},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31334,\"start\":31327}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1937,\"start\":1925},{\"attributes\":{\"n\":\"2.\"},\"end\":6382,\"start\":6343},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7123,\"start\":7108},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8845,\"start\":8830},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9933,\"start\":9895},{\"attributes\":{\"n\":\"3.\"},\"end\":10549,\"start\":10526},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11858,\"start\":11845},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12332,\"start\":12305},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13584,\"start\":13560},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16557,\"start\":16517},{\"attributes\":{\"n\":\"4.\"},\"end\":18050,\"start\":18039},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18391,\"start\":18359},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20154,\"start\":20138},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":20479,\"start\":20461},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":21203,\"start\":21188},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":21725,\"start\":21701},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25158,\"start\":25103},{\"attributes\":{\"n\":\"4.6.\"},\"end\":25191,\"start\":25161},{\"attributes\":{\"n\":\"4.7.\"},\"end\":26225,\"start\":26191},{\"attributes\":{\"n\":\"5.\"},\"end\":27983,\"start\":27971},{\"attributes\":{\"n\":\"6.\"},\"end\":30409,\"start\":30399},{\"attributes\":{\"n\":\"3.\"},\"end\":33188,\"start\":33146},{\"attributes\":{\"n\":\"4.\"},\"end\":33948,\"start\":33929},{\"end\":34366,\"start\":34356},{\"end\":34481,\"start\":34471},{\"end\":34831,\"start\":34821},{\"end\":35204,\"start\":35194},{\"end\":35326,\"start\":35317},{\"end\":36103,\"start\":36094},{\"end\":36408,\"start\":36399},{\"end\":36723,\"start\":36714},{\"end\":37030,\"start\":37021}]", "table": "[{\"end\":36092,\"start\":35526},{\"end\":36397,\"start\":36148},{\"end\":36712,\"start\":36437},{\"end\":37019,\"start\":36758},{\"end\":37308,\"start\":37065}]", "figure_caption": "[{\"end\":34469,\"start\":34368},{\"end\":34819,\"start\":34483},{\"end\":34929,\"start\":34833},{\"end\":35192,\"start\":34932},{\"end\":35315,\"start\":35206},{\"end\":35526,\"start\":35328},{\"end\":36148,\"start\":36105},{\"end\":36437,\"start\":36410},{\"end\":36758,\"start\":36725},{\"end\":37065,\"start\":37032}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":4794,\"start\":4786},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":5172,\"start\":5164},{\"end\":5456,\"start\":5447},{\"end\":6539,\"start\":6531},{\"end\":7096,\"start\":7088},{\"end\":7972,\"start\":7964},{\"end\":8641,\"start\":8633},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11721,\"start\":11713},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14977,\"start\":14969},{\"end\":18806,\"start\":18798},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22550,\"start\":22541},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26898,\"start\":26890},{\"end\":27479,\"start\":27471},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32390,\"start\":32382},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33337,\"start\":33316}]", "bib_author_first_name": "[{\"end\":39434,\"start\":39433},{\"end\":39658,\"start\":39657},{\"end\":39669,\"start\":39668},{\"end\":39678,\"start\":39677},{\"end\":39687,\"start\":39686},{\"end\":39697,\"start\":39696},{\"end\":40069,\"start\":40068},{\"end\":40080,\"start\":40079},{\"end\":40094,\"start\":40093},{\"end\":40103,\"start\":40102},{\"end\":40118,\"start\":40117},{\"end\":40120,\"start\":40119},{\"end\":40369,\"start\":40368},{\"end\":40383,\"start\":40382},{\"end\":40389,\"start\":40388},{\"end\":40396,\"start\":40395},{\"end\":40404,\"start\":40403},{\"end\":40611,\"start\":40610},{\"end\":40620,\"start\":40619},{\"end\":40631,\"start\":40630},{\"end\":40639,\"start\":40638},{\"end\":40652,\"start\":40651},{\"end\":40662,\"start\":40661},{\"end\":40671,\"start\":40670},{\"end\":40673,\"start\":40672},{\"end\":40685,\"start\":40684},{\"end\":41016,\"start\":41015},{\"end\":41024,\"start\":41023},{\"end\":41026,\"start\":41025},{\"end\":41032,\"start\":41031},{\"end\":41043,\"start\":41042},{\"end\":41354,\"start\":41353},{\"end\":41363,\"start\":41362},{\"end\":41371,\"start\":41370},{\"end\":41382,\"start\":41381},{\"end\":41393,\"start\":41392},{\"end\":41613,\"start\":41612},{\"end\":41630,\"start\":41629},{\"end\":41637,\"start\":41636},{\"end\":41648,\"start\":41647},{\"end\":41897,\"start\":41896},{\"end\":41899,\"start\":41898},{\"end\":41909,\"start\":41908},{\"end\":41911,\"start\":41910},{\"end\":41921,\"start\":41920},{\"end\":41923,\"start\":41922},{\"end\":41936,\"start\":41935},{\"end\":41938,\"start\":41937},{\"end\":42233,\"start\":42232},{\"end\":42244,\"start\":42243},{\"end\":42251,\"start\":42250},{\"end\":42261,\"start\":42260},{\"end\":42272,\"start\":42271},{\"end\":42276,\"start\":42273},{\"end\":42285,\"start\":42284},{\"end\":42296,\"start\":42295},{\"end\":42628,\"start\":42627},{\"end\":42639,\"start\":42638},{\"end\":42650,\"start\":42649},{\"end\":42662,\"start\":42661},{\"end\":42936,\"start\":42935},{\"end\":42946,\"start\":42945},{\"end\":42954,\"start\":42953},{\"end\":42965,\"start\":42964},{\"end\":43292,\"start\":43291},{\"end\":43294,\"start\":43293},{\"end\":43531,\"start\":43530},{\"end\":43539,\"start\":43538},{\"end\":43547,\"start\":43546},{\"end\":43549,\"start\":43548},{\"end\":43556,\"start\":43555},{\"end\":43565,\"start\":43564},{\"end\":43576,\"start\":43575},{\"end\":43578,\"start\":43577},{\"end\":43589,\"start\":43588},{\"end\":43591,\"start\":43590},{\"end\":43600,\"start\":43599},{\"end\":44060,\"start\":44059},{\"end\":44062,\"start\":44061},{\"end\":44068,\"start\":44067},{\"end\":44070,\"start\":44069},{\"end\":44232,\"start\":44231},{\"end\":44240,\"start\":44239},{\"end\":44246,\"start\":44245},{\"end\":44553,\"start\":44552},{\"end\":44555,\"start\":44554},{\"end\":44564,\"start\":44563},{\"end\":44898,\"start\":44897},{\"end\":44909,\"start\":44908},{\"end\":44919,\"start\":44918},{\"end\":45344,\"start\":45343},{\"end\":45359,\"start\":45358},{\"end\":45368,\"start\":45367},{\"end\":45377,\"start\":45376},{\"end\":45389,\"start\":45388},{\"end\":45730,\"start\":45729},{\"end\":45736,\"start\":45735},{\"end\":45738,\"start\":45737},{\"end\":46058,\"start\":46057},{\"end\":46066,\"start\":46065},{\"end\":46073,\"start\":46072},{\"end\":46080,\"start\":46079},{\"end\":46087,\"start\":46086},{\"end\":46093,\"start\":46092},{\"end\":46101,\"start\":46100},{\"end\":46108,\"start\":46107},{\"end\":46115,\"start\":46114},{\"end\":46492,\"start\":46491},{\"end\":46494,\"start\":46493},{\"end\":46506,\"start\":46505},{\"end\":46514,\"start\":46513},{\"end\":46516,\"start\":46515},{\"end\":46720,\"start\":46719},{\"end\":46722,\"start\":46721},{\"end\":46734,\"start\":46733},{\"end\":46744,\"start\":46743},{\"end\":46852,\"start\":46851},{\"end\":46860,\"start\":46859},{\"end\":47164,\"start\":47163},{\"end\":47166,\"start\":47165},{\"end\":47178,\"start\":47177},{\"end\":47180,\"start\":47179},{\"end\":47542,\"start\":47541},{\"end\":47648,\"start\":47645},{\"end\":47650,\"start\":47649},{\"end\":47967,\"start\":47966},{\"end\":47980,\"start\":47979},{\"end\":47982,\"start\":47981},{\"end\":47993,\"start\":47992},{\"end\":48002,\"start\":48001},{\"end\":48233,\"start\":48232},{\"end\":48245,\"start\":48244},{\"end\":48255,\"start\":48254},{\"end\":48269,\"start\":48268},{\"end\":48719,\"start\":48718},{\"end\":48726,\"start\":48725},{\"end\":48732,\"start\":48731},{\"end\":49018,\"start\":49017},{\"end\":49028,\"start\":49019},{\"end\":49046,\"start\":49043},{\"end\":49480,\"start\":49479},{\"end\":49494,\"start\":49493},{\"end\":49503,\"start\":49502},{\"end\":49515,\"start\":49514},{\"end\":49526,\"start\":49525},{\"end\":49533,\"start\":49532},{\"end\":49544,\"start\":49543},{\"end\":49555,\"start\":49554},{\"end\":49565,\"start\":49564},{\"end\":49576,\"start\":49575},{\"end\":49587,\"start\":49586},{\"end\":49962,\"start\":49961},{\"end\":49975,\"start\":49974},{\"end\":49986,\"start\":49985},{\"end\":49999,\"start\":49998},{\"end\":50356,\"start\":50355},{\"end\":50367,\"start\":50366},{\"end\":50378,\"start\":50377},{\"end\":50388,\"start\":50387},{\"end\":50401,\"start\":50400},{\"end\":50410,\"start\":50409},{\"end\":50412,\"start\":50411},{\"end\":50421,\"start\":50420},{\"end\":50431,\"start\":50430},{\"end\":50666,\"start\":50665},{\"end\":50678,\"start\":50677},{\"end\":50688,\"start\":50687},{\"end\":50892,\"start\":50891},{\"end\":50900,\"start\":50899},{\"end\":50908,\"start\":50907},{\"end\":50915,\"start\":50914},{\"end\":50921,\"start\":50920},{\"end\":51163,\"start\":51162},{\"end\":51171,\"start\":51170},{\"end\":51180,\"start\":51179},{\"end\":51187,\"start\":51186},{\"end\":51517,\"start\":51516},{\"end\":51519,\"start\":51518},{\"end\":51527,\"start\":51526},{\"end\":51529,\"start\":51528},{\"end\":51538,\"start\":51537},{\"end\":51971,\"start\":51970},{\"end\":51973,\"start\":51972},{\"end\":52231,\"start\":52230},{\"end\":52237,\"start\":52236},{\"end\":52243,\"start\":52242},{\"end\":52252,\"start\":52251},{\"end\":52259,\"start\":52258},{\"end\":52272,\"start\":52271},{\"end\":52289,\"start\":52288},{\"end\":52298,\"start\":52297},{\"end\":52617,\"start\":52616},{\"end\":52625,\"start\":52624},{\"end\":52633,\"start\":52632},{\"end\":52640,\"start\":52639},{\"end\":52974,\"start\":52973},{\"end\":52981,\"start\":52980},{\"end\":53217,\"start\":53216},{\"end\":53225,\"start\":53224},{\"end\":53233,\"start\":53232},{\"end\":53235,\"start\":53234},{\"end\":53250,\"start\":53249},{\"end\":53256,\"start\":53255},{\"end\":53267,\"start\":53266},{\"end\":53555,\"start\":53554},{\"end\":53563,\"start\":53562},{\"end\":53572,\"start\":53571},{\"end\":53583,\"start\":53582},{\"end\":53585,\"start\":53584},{\"end\":53787,\"start\":53786},{\"end\":53796,\"start\":53795},{\"end\":53807,\"start\":53806},{\"end\":53815,\"start\":53814},{\"end\":53828,\"start\":53827},{\"end\":53838,\"start\":53837},{\"end\":53847,\"start\":53846},{\"end\":53849,\"start\":53848},{\"end\":53861,\"start\":53860},{\"end\":54184,\"start\":54183},{\"end\":54192,\"start\":54191},{\"end\":54194,\"start\":54193},{\"end\":54200,\"start\":54199},{\"end\":54211,\"start\":54210},{\"end\":54510,\"start\":54509},{\"end\":54521,\"start\":54520},{\"end\":54528,\"start\":54527},{\"end\":54538,\"start\":54537},{\"end\":54549,\"start\":54548},{\"end\":54553,\"start\":54550},{\"end\":54562,\"start\":54561},{\"end\":54573,\"start\":54572},{\"end\":54881,\"start\":54880},{\"end\":54891,\"start\":54890},{\"end\":54899,\"start\":54898},{\"end\":54910,\"start\":54909},{\"end\":55183,\"start\":55182},{\"end\":55194,\"start\":55193},{\"end\":55204,\"start\":55203},{\"end\":55215,\"start\":55214},{\"end\":55537,\"start\":55536},{\"end\":55539,\"start\":55538},{\"end\":55551,\"start\":55550},{\"end\":55800,\"start\":55799},{\"end\":55812,\"start\":55811},{\"end\":55822,\"start\":55821},{\"end\":55836,\"start\":55835},{\"end\":56265,\"start\":56264},{\"end\":56277,\"start\":56276},{\"end\":56285,\"start\":56284},{\"end\":56295,\"start\":56294},{\"end\":56305,\"start\":56304},{\"end\":56317,\"start\":56316},{\"end\":56567,\"start\":56566},{\"end\":56578,\"start\":56577},{\"end\":56589,\"start\":56588},{\"end\":56597,\"start\":56596},{\"end\":56606,\"start\":56605},{\"end\":56619,\"start\":56618},{\"end\":58205,\"start\":58191}]", "bib_author_last_name": "[{\"end\":39441,\"start\":39435},{\"end\":39666,\"start\":39659},{\"end\":39675,\"start\":39670},{\"end\":39684,\"start\":39679},{\"end\":39694,\"start\":39688},{\"end\":39706,\"start\":39698},{\"end\":40077,\"start\":40070},{\"end\":40091,\"start\":40081},{\"end\":40100,\"start\":40095},{\"end\":40115,\"start\":40104},{\"end\":40128,\"start\":40121},{\"end\":40380,\"start\":40370},{\"end\":40386,\"start\":40384},{\"end\":40393,\"start\":40390},{\"end\":40401,\"start\":40397},{\"end\":40410,\"start\":40405},{\"end\":40617,\"start\":40612},{\"end\":40628,\"start\":40621},{\"end\":40636,\"start\":40632},{\"end\":40649,\"start\":40640},{\"end\":40659,\"start\":40653},{\"end\":40668,\"start\":40663},{\"end\":40682,\"start\":40674},{\"end\":40694,\"start\":40686},{\"end\":41021,\"start\":41017},{\"end\":41029,\"start\":41027},{\"end\":41040,\"start\":41033},{\"end\":41051,\"start\":41044},{\"end\":41360,\"start\":41355},{\"end\":41368,\"start\":41364},{\"end\":41379,\"start\":41372},{\"end\":41390,\"start\":41383},{\"end\":41397,\"start\":41394},{\"end\":41627,\"start\":41614},{\"end\":41634,\"start\":41631},{\"end\":41645,\"start\":41638},{\"end\":41654,\"start\":41649},{\"end\":41906,\"start\":41900},{\"end\":41918,\"start\":41912},{\"end\":41933,\"start\":41924},{\"end\":42241,\"start\":42234},{\"end\":42248,\"start\":42245},{\"end\":42258,\"start\":42252},{\"end\":42269,\"start\":42262},{\"end\":42282,\"start\":42277},{\"end\":42293,\"start\":42286},{\"end\":42301,\"start\":42297},{\"end\":42636,\"start\":42629},{\"end\":42647,\"start\":42640},{\"end\":42659,\"start\":42651},{\"end\":42673,\"start\":42663},{\"end\":42943,\"start\":42937},{\"end\":42951,\"start\":42947},{\"end\":42962,\"start\":42955},{\"end\":42973,\"start\":42966},{\"end\":43301,\"start\":43295},{\"end\":43536,\"start\":43532},{\"end\":43544,\"start\":43540},{\"end\":43553,\"start\":43550},{\"end\":43562,\"start\":43557},{\"end\":43573,\"start\":43566},{\"end\":43586,\"start\":43579},{\"end\":43597,\"start\":43592},{\"end\":43605,\"start\":43601},{\"end\":44065,\"start\":44063},{\"end\":44075,\"start\":44071},{\"end\":44237,\"start\":44233},{\"end\":44243,\"start\":44241},{\"end\":44252,\"start\":44247},{\"end\":44561,\"start\":44556},{\"end\":44571,\"start\":44565},{\"end\":44906,\"start\":44899},{\"end\":44916,\"start\":44910},{\"end\":44927,\"start\":44920},{\"end\":45356,\"start\":45345},{\"end\":45365,\"start\":45360},{\"end\":45374,\"start\":45369},{\"end\":45386,\"start\":45378},{\"end\":45397,\"start\":45390},{\"end\":45733,\"start\":45731},{\"end\":45747,\"start\":45739},{\"end\":46063,\"start\":46059},{\"end\":46070,\"start\":46067},{\"end\":46077,\"start\":46074},{\"end\":46084,\"start\":46081},{\"end\":46090,\"start\":46088},{\"end\":46098,\"start\":46094},{\"end\":46105,\"start\":46102},{\"end\":46112,\"start\":46109},{\"end\":46121,\"start\":46116},{\"end\":46503,\"start\":46495},{\"end\":46511,\"start\":46507},{\"end\":46520,\"start\":46517},{\"end\":46731,\"start\":46723},{\"end\":46741,\"start\":46735},{\"end\":46750,\"start\":46745},{\"end\":46760,\"start\":46752},{\"end\":46857,\"start\":46853},{\"end\":46867,\"start\":46861},{\"end\":47175,\"start\":47167},{\"end\":47192,\"start\":47181},{\"end\":47548,\"start\":47543},{\"end\":47658,\"start\":47651},{\"end\":47667,\"start\":47660},{\"end\":47977,\"start\":47968},{\"end\":47990,\"start\":47983},{\"end\":47999,\"start\":47994},{\"end\":48016,\"start\":48003},{\"end\":48242,\"start\":48234},{\"end\":48252,\"start\":48246},{\"end\":48266,\"start\":48256},{\"end\":48279,\"start\":48270},{\"end\":48723,\"start\":48720},{\"end\":48729,\"start\":48727},{\"end\":48737,\"start\":48733},{\"end\":49041,\"start\":49029},{\"end\":49051,\"start\":49047},{\"end\":49491,\"start\":49481},{\"end\":49500,\"start\":49495},{\"end\":49512,\"start\":49504},{\"end\":49523,\"start\":49516},{\"end\":49530,\"start\":49527},{\"end\":49541,\"start\":49534},{\"end\":49552,\"start\":49545},{\"end\":49562,\"start\":49556},{\"end\":49573,\"start\":49566},{\"end\":49584,\"start\":49577},{\"end\":49593,\"start\":49588},{\"end\":49972,\"start\":49963},{\"end\":49983,\"start\":49976},{\"end\":49996,\"start\":49987},{\"end\":50008,\"start\":50000},{\"end\":50364,\"start\":50357},{\"end\":50375,\"start\":50368},{\"end\":50385,\"start\":50379},{\"end\":50398,\"start\":50389},{\"end\":50407,\"start\":50402},{\"end\":50418,\"start\":50413},{\"end\":50428,\"start\":50422},{\"end\":50442,\"start\":50432},{\"end\":50675,\"start\":50667},{\"end\":50685,\"start\":50679},{\"end\":50696,\"start\":50689},{\"end\":50897,\"start\":50893},{\"end\":50905,\"start\":50901},{\"end\":50912,\"start\":50909},{\"end\":50918,\"start\":50916},{\"end\":50925,\"start\":50922},{\"end\":51168,\"start\":51164},{\"end\":51177,\"start\":51172},{\"end\":51184,\"start\":51181},{\"end\":51195,\"start\":51188},{\"end\":51524,\"start\":51520},{\"end\":51535,\"start\":51530},{\"end\":51550,\"start\":51539},{\"end\":51982,\"start\":51974},{\"end\":52234,\"start\":52232},{\"end\":52240,\"start\":52238},{\"end\":52249,\"start\":52244},{\"end\":52256,\"start\":52253},{\"end\":52269,\"start\":52260},{\"end\":52286,\"start\":52273},{\"end\":52295,\"start\":52290},{\"end\":52305,\"start\":52299},{\"end\":52311,\"start\":52307},{\"end\":52622,\"start\":52618},{\"end\":52630,\"start\":52626},{\"end\":52637,\"start\":52634},{\"end\":52648,\"start\":52641},{\"end\":52978,\"start\":52975},{\"end\":52985,\"start\":52982},{\"end\":53222,\"start\":53218},{\"end\":53230,\"start\":53226},{\"end\":53247,\"start\":53236},{\"end\":53253,\"start\":53251},{\"end\":53264,\"start\":53257},{\"end\":53272,\"start\":53268},{\"end\":53560,\"start\":53556},{\"end\":53569,\"start\":53564},{\"end\":53580,\"start\":53573},{\"end\":53590,\"start\":53586},{\"end\":53793,\"start\":53788},{\"end\":53804,\"start\":53797},{\"end\":53812,\"start\":53808},{\"end\":53825,\"start\":53816},{\"end\":53835,\"start\":53829},{\"end\":53844,\"start\":53839},{\"end\":53858,\"start\":53850},{\"end\":53870,\"start\":53862},{\"end\":54189,\"start\":54185},{\"end\":54197,\"start\":54195},{\"end\":54208,\"start\":54201},{\"end\":54219,\"start\":54212},{\"end\":54518,\"start\":54511},{\"end\":54525,\"start\":54522},{\"end\":54535,\"start\":54529},{\"end\":54546,\"start\":54539},{\"end\":54559,\"start\":54554},{\"end\":54570,\"start\":54563},{\"end\":54578,\"start\":54574},{\"end\":54888,\"start\":54882},{\"end\":54896,\"start\":54892},{\"end\":54907,\"start\":54900},{\"end\":54918,\"start\":54911},{\"end\":55191,\"start\":55184},{\"end\":55201,\"start\":55195},{\"end\":55212,\"start\":55205},{\"end\":55222,\"start\":55216},{\"end\":55548,\"start\":55540},{\"end\":55559,\"start\":55552},{\"end\":55809,\"start\":55801},{\"end\":55819,\"start\":55813},{\"end\":55833,\"start\":55823},{\"end\":55846,\"start\":55837},{\"end\":56274,\"start\":56266},{\"end\":56282,\"start\":56278},{\"end\":56292,\"start\":56286},{\"end\":56302,\"start\":56296},{\"end\":56314,\"start\":56306},{\"end\":56325,\"start\":56318},{\"end\":56575,\"start\":56568},{\"end\":56586,\"start\":56579},{\"end\":56594,\"start\":56590},{\"end\":56603,\"start\":56598},{\"end\":56616,\"start\":56607},{\"end\":56630,\"start\":56620},{\"end\":57790,\"start\":57783},{\"end\":57825,\"start\":57819},{\"end\":57922,\"start\":57915},{\"end\":57982,\"start\":57979},{\"end\":58010,\"start\":58003},{\"end\":58030,\"start\":58023},{\"end\":58086,\"start\":58083},{\"end\":58214,\"start\":58206},{\"end\":58322,\"start\":58315},{\"end\":58374,\"start\":58367},{\"end\":58393,\"start\":58387},{\"end\":58488,\"start\":58483}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":39563,\"start\":39433},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":29967714},\"end\":39987,\"start\":39565},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4624670},\"end\":40309,\"start\":39989},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4547677},\"end\":40567,\"start\":40311},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9999787},\"end\":40948,\"start\":40569},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3641788},\"end\":41277,\"start\":40950},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":30370275},\"end\":41554,\"start\":41279},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1307614},\"end\":41803,\"start\":41556},{\"attributes\":{\"id\":\"b8\"},\"end\":42170,\"start\":41805},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12552176},\"end\":42559,\"start\":42172},{\"attributes\":{\"id\":\"b10\"},\"end\":42891,\"start\":42561},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9455111},\"end\":43197,\"start\":42893},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":125881359},\"end\":43471,\"start\":43199},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6388927},\"end\":43984,\"start\":43473},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15990054},\"end\":44229,\"start\":43986},{\"attributes\":{\"doi\":\"arXiv:1611.01144\",\"id\":\"b15\"},\"end\":44458,\"start\":44231},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17834704},\"end\":44819,\"start\":44460},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12888763},\"end\":45271,\"start\":44821},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206500609},\"end\":45665,\"start\":45273},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3778469},\"end\":45967,\"start\":45667},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52956886},\"end\":46408,\"start\":45969},{\"attributes\":{\"doi\":\"arXiv:1611.00712\",\"id\":\"b21\"},\"end\":46717,\"start\":46410},{\"attributes\":{\"id\":\"b22\"},\"end\":46849,\"start\":46719},{\"attributes\":{\"doi\":\"arXiv:1402.0030\",\"id\":\"b23\"},\"end\":47084,\"start\":46851},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":12751695},\"end\":47537,\"start\":47086},{\"attributes\":{\"id\":\"b25\"},\"end\":47579,\"start\":47539},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16418752},\"end\":47898,\"start\":47581},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3425782},\"end\":48167,\"start\":47900},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":37400798},\"end\":48639,\"start\":48169},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7334757},\"end\":48940,\"start\":48641},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":17112447},\"end\":49420,\"start\":48942},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4956314},\"end\":49901,\"start\":49422},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1403696},\"end\":50326,\"start\":49903},{\"attributes\":{\"id\":\"b33\"},\"end\":50593,\"start\":50328},{\"attributes\":{\"id\":\"b34\"},\"end\":50821,\"start\":50595},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":44100533},\"end\":51066,\"start\":50823},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9114952},\"end\":51455,\"start\":51068},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":438575},\"end\":51877,\"start\":51457},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2332513},\"end\":52156,\"start\":51879},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1055111},\"end\":52508,\"start\":52158},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6761996},\"end\":52895,\"start\":52510},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3714620},\"end\":53108,\"start\":52897},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4578162},\"end\":53494,\"start\":53110},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":11977588},\"end\":53743,\"start\":53496},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":9999787},\"end\":54116,\"start\":53745},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3641788},\"end\":54447,\"start\":54118},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":12552176},\"end\":54836,\"start\":54449},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9455111},\"end\":55135,\"start\":54838},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":22556995},\"end\":55451,\"start\":55137},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":8209918},\"end\":55734,\"start\":55453},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":37400798},\"end\":56200,\"start\":55736},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4884375},\"end\":56485,\"start\":56202},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":8632684},\"end\":56826,\"start\":56487},{\"attributes\":{\"id\":\"b53\"},\"end\":57779,\"start\":56828},{\"attributes\":{\"id\":\"b54\"},\"end\":57799,\"start\":57781},{\"attributes\":{\"id\":\"b55\"},\"end\":57815,\"start\":57801},{\"attributes\":{\"id\":\"b56\"},\"end\":57911,\"start\":57817},{\"attributes\":{\"id\":\"b57\"},\"end\":57931,\"start\":57913},{\"attributes\":{\"id\":\"b58\"},\"end\":57975,\"start\":57933},{\"attributes\":{\"id\":\"b59\"},\"end\":57999,\"start\":57977},{\"attributes\":{\"id\":\"b60\"},\"end\":58019,\"start\":58001},{\"attributes\":{\"id\":\"b61\"},\"end\":58039,\"start\":58021},{\"attributes\":{\"id\":\"b62\"},\"end\":58079,\"start\":58041},{\"attributes\":{\"id\":\"b63\"},\"end\":58187,\"start\":58081},{\"attributes\":{\"id\":\"b64\"},\"end\":58311,\"start\":58189},{\"attributes\":{\"id\":\"b65\"},\"end\":58331,\"start\":58313},{\"attributes\":{\"id\":\"b66\"},\"end\":58363,\"start\":58333},{\"attributes\":{\"id\":\"b67\"},\"end\":58383,\"start\":58365},{\"attributes\":{\"id\":\"b68\"},\"end\":58479,\"start\":58385},{\"attributes\":{\"id\":\"b69\"},\"end\":58495,\"start\":58481},{\"attributes\":{\"id\":\"b70\"},\"end\":59167,\"start\":58497}]", "bib_title": "[{\"end\":39655,\"start\":39565},{\"end\":40066,\"start\":39989},{\"end\":40366,\"start\":40311},{\"end\":40608,\"start\":40569},{\"end\":41013,\"start\":40950},{\"end\":41351,\"start\":41279},{\"end\":41610,\"start\":41556},{\"end\":41894,\"start\":41805},{\"end\":42230,\"start\":42172},{\"end\":42625,\"start\":42561},{\"end\":42933,\"start\":42893},{\"end\":43289,\"start\":43199},{\"end\":43528,\"start\":43473},{\"end\":44057,\"start\":43986},{\"end\":44550,\"start\":44460},{\"end\":44895,\"start\":44821},{\"end\":45341,\"start\":45273},{\"end\":45727,\"start\":45667},{\"end\":46055,\"start\":45969},{\"end\":47161,\"start\":47086},{\"end\":47643,\"start\":47581},{\"end\":47964,\"start\":47900},{\"end\":48230,\"start\":48169},{\"end\":48716,\"start\":48641},{\"end\":49015,\"start\":48942},{\"end\":49477,\"start\":49422},{\"end\":49959,\"start\":49903},{\"end\":50889,\"start\":50823},{\"end\":51160,\"start\":51068},{\"end\":51514,\"start\":51457},{\"end\":51968,\"start\":51879},{\"end\":52228,\"start\":52158},{\"end\":52614,\"start\":52510},{\"end\":52971,\"start\":52897},{\"end\":53214,\"start\":53110},{\"end\":53552,\"start\":53496},{\"end\":53784,\"start\":53745},{\"end\":54181,\"start\":54118},{\"end\":54507,\"start\":54449},{\"end\":54878,\"start\":54838},{\"end\":55180,\"start\":55137},{\"end\":55534,\"start\":55453},{\"end\":55797,\"start\":55736},{\"end\":56262,\"start\":56202},{\"end\":56564,\"start\":56487}]", "bib_author": "[{\"end\":39443,\"start\":39433},{\"end\":39668,\"start\":39657},{\"end\":39677,\"start\":39668},{\"end\":39686,\"start\":39677},{\"end\":39696,\"start\":39686},{\"end\":39708,\"start\":39696},{\"end\":40079,\"start\":40068},{\"end\":40093,\"start\":40079},{\"end\":40102,\"start\":40093},{\"end\":40117,\"start\":40102},{\"end\":40130,\"start\":40117},{\"end\":40382,\"start\":40368},{\"end\":40388,\"start\":40382},{\"end\":40395,\"start\":40388},{\"end\":40403,\"start\":40395},{\"end\":40412,\"start\":40403},{\"end\":40619,\"start\":40610},{\"end\":40630,\"start\":40619},{\"end\":40638,\"start\":40630},{\"end\":40651,\"start\":40638},{\"end\":40661,\"start\":40651},{\"end\":40670,\"start\":40661},{\"end\":40684,\"start\":40670},{\"end\":40696,\"start\":40684},{\"end\":41023,\"start\":41015},{\"end\":41031,\"start\":41023},{\"end\":41042,\"start\":41031},{\"end\":41053,\"start\":41042},{\"end\":41362,\"start\":41353},{\"end\":41370,\"start\":41362},{\"end\":41381,\"start\":41370},{\"end\":41392,\"start\":41381},{\"end\":41399,\"start\":41392},{\"end\":41629,\"start\":41612},{\"end\":41636,\"start\":41629},{\"end\":41647,\"start\":41636},{\"end\":41656,\"start\":41647},{\"end\":41908,\"start\":41896},{\"end\":41920,\"start\":41908},{\"end\":41935,\"start\":41920},{\"end\":41941,\"start\":41935},{\"end\":42243,\"start\":42232},{\"end\":42250,\"start\":42243},{\"end\":42260,\"start\":42250},{\"end\":42271,\"start\":42260},{\"end\":42284,\"start\":42271},{\"end\":42295,\"start\":42284},{\"end\":42303,\"start\":42295},{\"end\":42638,\"start\":42627},{\"end\":42649,\"start\":42638},{\"end\":42661,\"start\":42649},{\"end\":42675,\"start\":42661},{\"end\":42945,\"start\":42935},{\"end\":42953,\"start\":42945},{\"end\":42964,\"start\":42953},{\"end\":42975,\"start\":42964},{\"end\":43303,\"start\":43291},{\"end\":43538,\"start\":43530},{\"end\":43546,\"start\":43538},{\"end\":43555,\"start\":43546},{\"end\":43564,\"start\":43555},{\"end\":43575,\"start\":43564},{\"end\":43588,\"start\":43575},{\"end\":43599,\"start\":43588},{\"end\":43607,\"start\":43599},{\"end\":44067,\"start\":44059},{\"end\":44077,\"start\":44067},{\"end\":44239,\"start\":44231},{\"end\":44245,\"start\":44239},{\"end\":44254,\"start\":44245},{\"end\":44563,\"start\":44552},{\"end\":44573,\"start\":44563},{\"end\":44908,\"start\":44897},{\"end\":44918,\"start\":44908},{\"end\":44929,\"start\":44918},{\"end\":45358,\"start\":45343},{\"end\":45367,\"start\":45358},{\"end\":45376,\"start\":45367},{\"end\":45388,\"start\":45376},{\"end\":45399,\"start\":45388},{\"end\":45735,\"start\":45729},{\"end\":45749,\"start\":45735},{\"end\":46065,\"start\":46057},{\"end\":46072,\"start\":46065},{\"end\":46079,\"start\":46072},{\"end\":46086,\"start\":46079},{\"end\":46092,\"start\":46086},{\"end\":46100,\"start\":46092},{\"end\":46107,\"start\":46100},{\"end\":46114,\"start\":46107},{\"end\":46123,\"start\":46114},{\"end\":46505,\"start\":46491},{\"end\":46513,\"start\":46505},{\"end\":46522,\"start\":46513},{\"end\":46733,\"start\":46719},{\"end\":46743,\"start\":46733},{\"end\":46752,\"start\":46743},{\"end\":46762,\"start\":46752},{\"end\":46859,\"start\":46851},{\"end\":46869,\"start\":46859},{\"end\":47177,\"start\":47163},{\"end\":47194,\"start\":47177},{\"end\":47550,\"start\":47541},{\"end\":47660,\"start\":47645},{\"end\":47669,\"start\":47660},{\"end\":47979,\"start\":47966},{\"end\":47992,\"start\":47979},{\"end\":48001,\"start\":47992},{\"end\":48018,\"start\":48001},{\"end\":48244,\"start\":48232},{\"end\":48254,\"start\":48244},{\"end\":48268,\"start\":48254},{\"end\":48281,\"start\":48268},{\"end\":48725,\"start\":48718},{\"end\":48731,\"start\":48725},{\"end\":48739,\"start\":48731},{\"end\":49043,\"start\":49017},{\"end\":49053,\"start\":49043},{\"end\":49493,\"start\":49479},{\"end\":49502,\"start\":49493},{\"end\":49514,\"start\":49502},{\"end\":49525,\"start\":49514},{\"end\":49532,\"start\":49525},{\"end\":49543,\"start\":49532},{\"end\":49554,\"start\":49543},{\"end\":49564,\"start\":49554},{\"end\":49575,\"start\":49564},{\"end\":49586,\"start\":49575},{\"end\":49595,\"start\":49586},{\"end\":49974,\"start\":49961},{\"end\":49985,\"start\":49974},{\"end\":49998,\"start\":49985},{\"end\":50010,\"start\":49998},{\"end\":50366,\"start\":50355},{\"end\":50377,\"start\":50366},{\"end\":50387,\"start\":50377},{\"end\":50400,\"start\":50387},{\"end\":50409,\"start\":50400},{\"end\":50420,\"start\":50409},{\"end\":50430,\"start\":50420},{\"end\":50444,\"start\":50430},{\"end\":50677,\"start\":50665},{\"end\":50687,\"start\":50677},{\"end\":50698,\"start\":50687},{\"end\":50899,\"start\":50891},{\"end\":50907,\"start\":50899},{\"end\":50914,\"start\":50907},{\"end\":50920,\"start\":50914},{\"end\":50927,\"start\":50920},{\"end\":51170,\"start\":51162},{\"end\":51179,\"start\":51170},{\"end\":51186,\"start\":51179},{\"end\":51197,\"start\":51186},{\"end\":51526,\"start\":51516},{\"end\":51537,\"start\":51526},{\"end\":51552,\"start\":51537},{\"end\":51984,\"start\":51970},{\"end\":52236,\"start\":52230},{\"end\":52242,\"start\":52236},{\"end\":52251,\"start\":52242},{\"end\":52258,\"start\":52251},{\"end\":52271,\"start\":52258},{\"end\":52288,\"start\":52271},{\"end\":52297,\"start\":52288},{\"end\":52307,\"start\":52297},{\"end\":52313,\"start\":52307},{\"end\":52624,\"start\":52616},{\"end\":52632,\"start\":52624},{\"end\":52639,\"start\":52632},{\"end\":52650,\"start\":52639},{\"end\":52980,\"start\":52973},{\"end\":52987,\"start\":52980},{\"end\":53224,\"start\":53216},{\"end\":53232,\"start\":53224},{\"end\":53249,\"start\":53232},{\"end\":53255,\"start\":53249},{\"end\":53266,\"start\":53255},{\"end\":53274,\"start\":53266},{\"end\":53562,\"start\":53554},{\"end\":53571,\"start\":53562},{\"end\":53582,\"start\":53571},{\"end\":53592,\"start\":53582},{\"end\":53795,\"start\":53786},{\"end\":53806,\"start\":53795},{\"end\":53814,\"start\":53806},{\"end\":53827,\"start\":53814},{\"end\":53837,\"start\":53827},{\"end\":53846,\"start\":53837},{\"end\":53860,\"start\":53846},{\"end\":53872,\"start\":53860},{\"end\":54191,\"start\":54183},{\"end\":54199,\"start\":54191},{\"end\":54210,\"start\":54199},{\"end\":54221,\"start\":54210},{\"end\":54520,\"start\":54509},{\"end\":54527,\"start\":54520},{\"end\":54537,\"start\":54527},{\"end\":54548,\"start\":54537},{\"end\":54561,\"start\":54548},{\"end\":54572,\"start\":54561},{\"end\":54580,\"start\":54572},{\"end\":54890,\"start\":54880},{\"end\":54898,\"start\":54890},{\"end\":54909,\"start\":54898},{\"end\":54920,\"start\":54909},{\"end\":55193,\"start\":55182},{\"end\":55203,\"start\":55193},{\"end\":55214,\"start\":55203},{\"end\":55224,\"start\":55214},{\"end\":55550,\"start\":55536},{\"end\":55561,\"start\":55550},{\"end\":55811,\"start\":55799},{\"end\":55821,\"start\":55811},{\"end\":55835,\"start\":55821},{\"end\":55848,\"start\":55835},{\"end\":56276,\"start\":56264},{\"end\":56284,\"start\":56276},{\"end\":56294,\"start\":56284},{\"end\":56304,\"start\":56294},{\"end\":56316,\"start\":56304},{\"end\":56327,\"start\":56316},{\"end\":56577,\"start\":56566},{\"end\":56588,\"start\":56577},{\"end\":56596,\"start\":56588},{\"end\":56605,\"start\":56596},{\"end\":56618,\"start\":56605},{\"end\":56632,\"start\":56618},{\"end\":57792,\"start\":57783},{\"end\":57827,\"start\":57819},{\"end\":57924,\"start\":57915},{\"end\":57984,\"start\":57979},{\"end\":58012,\"start\":58003},{\"end\":58032,\"start\":58023},{\"end\":58088,\"start\":58083},{\"end\":58216,\"start\":58191},{\"end\":58324,\"start\":58315},{\"end\":58376,\"start\":58367},{\"end\":58395,\"start\":58387},{\"end\":58490,\"start\":58483}]", "bib_venue": "[{\"end\":43728,\"start\":43676},{\"end\":45050,\"start\":44998},{\"end\":47322,\"start\":47265},{\"end\":48364,\"start\":48344},{\"end\":49200,\"start\":49135},{\"end\":51673,\"start\":51621},{\"end\":55931,\"start\":55911},{\"end\":39483,\"start\":39443},{\"end\":39754,\"start\":39708},{\"end\":40137,\"start\":40130},{\"end\":40416,\"start\":40412},{\"end\":40742,\"start\":40696},{\"end\":41102,\"start\":41053},{\"end\":41403,\"start\":41399},{\"end\":41660,\"start\":41656},{\"end\":41964,\"start\":41941},{\"end\":42352,\"start\":42303},{\"end\":42704,\"start\":42675},{\"end\":43021,\"start\":42975},{\"end\":43328,\"start\":43303},{\"end\":43674,\"start\":43607},{\"end\":44081,\"start\":44077},{\"end\":44320,\"start\":44270},{\"end\":44619,\"start\":44573},{\"end\":44996,\"start\":44929},{\"end\":45445,\"start\":45399},{\"end\":45795,\"start\":45749},{\"end\":46172,\"start\":46123},{\"end\":46489,\"start\":46410},{\"end\":46766,\"start\":46762},{\"end\":46944,\"start\":46884},{\"end\":47263,\"start\":47194},{\"end\":47721,\"start\":47669},{\"end\":48022,\"start\":48018},{\"end\":48342,\"start\":48281},{\"end\":48768,\"start\":48739},{\"end\":49133,\"start\":49053},{\"end\":49637,\"start\":49595},{\"end\":50090,\"start\":50010},{\"end\":50353,\"start\":50328},{\"end\":50663,\"start\":50595},{\"end\":50934,\"start\":50927},{\"end\":51248,\"start\":51197},{\"end\":51619,\"start\":51552},{\"end\":52000,\"start\":51984},{\"end\":52317,\"start\":52313},{\"end\":52686,\"start\":52650},{\"end\":52991,\"start\":52987},{\"end\":53278,\"start\":53274},{\"end\":53596,\"start\":53592},{\"end\":53918,\"start\":53872},{\"end\":54270,\"start\":54221},{\"end\":54629,\"start\":54580},{\"end\":54966,\"start\":54920},{\"end\":55277,\"start\":55224},{\"end\":55577,\"start\":55561},{\"end\":55909,\"start\":55848},{\"end\":56331,\"start\":56327},{\"end\":56636,\"start\":56632},{\"end\":56932,\"start\":56828},{\"end\":57807,\"start\":57801},{\"end\":57953,\"start\":57933},{\"end\":58059,\"start\":58041},{\"end\":58347,\"start\":58333},{\"end\":58789,\"start\":58497}]"}}}, "year": 2023, "month": 12, "day": 17}