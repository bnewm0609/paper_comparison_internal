{"id": 211296708, "updated": "2023-10-06 18:36:58.14", "metadata": {"title": "An LSTM Based Architecture to Relate Speech Stimulus to EEG", "authors": "[{\"first\":\"Mohammad\",\"last\":\"Monesi\",\"middle\":[\"Jalilpour\"]},{\"first\":\"Bernd\",\"last\":\"Accou\",\"middle\":[]},{\"first\":\"Jair\",\"last\":\"Montoya-Martinez\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Francart\",\"middle\":[]},{\"first\":\"Hugo\",\"last\":\"Hamme\",\"middle\":[\"Van\"]}]", "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "journal": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "publication_date": {"year": 2020, "month": 2, "day": 25}, "abstract": "Modeling the relationship between natural speech and a recorded electroencephalogram (EEG) helps us understand how the brain processes speech and has various applications in neuroscience and brain-computer interfaces. In this context, so far mainly linear models have been used. However, the decoding performance of the linear model is limited due to the complex and highly non-linear nature of the auditory processing in the human brain. We present a novel Long Short-Term Memory (LSTM)-based architecture as a non-linear model for the classification problem of whether a given pair of (EEG, speech envelope) correspond to each other or not. The model maps short segments of the EEG and the envelope to a common embedding space using a CNN in the EEG path and an LSTM in the speech path. The latter also compensates for the brain response delay. In addition, we use transfer learning to fine-tune the model for each subject. The mean classification accuracy of the proposed model reaches 85%, which is significantly higher than that of a state of the art Convolutional Neural Network (CNN)-based model (73%) and the linear model (69%).", "fields_of_study": "[\"Engineering\"]", "external_ids": {"arxiv": "2002.10988", "mag": "3016174996", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/Jalilpour-Monesi20", "doi": "10.1109/icassp40776.2020.9054000"}}, "content": {"source": {"pdf_hash": "bc971f33e7f5944433c913b683564875d2845356", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.10988v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2002.10988", "status": "GREEN"}}, "grobid": {"id": "5fc43dfb1da24c1304947a4bc88b8a8571761cf5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bc971f33e7f5944433c913b683564875d2845356.txt", "contents": "\nAN LSTM BASED ARCHITECTURE TO RELATE SPEECH STIMULUS TO EEG\n\n\nMohammad Jalilpour Monesi \nDept. Neurosciences\nLeuven, ExpORL, LeuvenBelgium\n\nBernd Accou \nDept. Neurosciences\nLeuven, ExpORL, LeuvenBelgium\n\nJair Montoya-Martinez \nDept. Neurosciences\nLeuven, ExpORL, LeuvenBelgium\n\nTom Francart \nDept. Neurosciences\nLeuven, ExpORL, LeuvenBelgium\n\nHugo Van Hamme \nK U Leuven \nPsi \n\nDept. of Electrical engineering (ESAT)\nLeuvenBelgium\n\nAN LSTM BASED ARCHITECTURE TO RELATE SPEECH STIMULUS TO EEG\nIndex Terms-LSTMCNNspeech decodingauditory systemEEG\nModeling the relationship between natural speech and a recorded electroencephalogram (EEG) helps us understand how the brain processes speech and has various applications in neuroscience and brain-computer interfaces. In this context, so far mainly linear models have been used. However, the decoding performance of the linear model is limited due to the complex and highly non-linear nature of the auditory processing in the human brain. We present a novel Long Short-Term Memory (LSTM)-based architecture as a nonlinear model for the classification problem of whether a given pair of (EEG, speech envelope) correspond to each other or not. The model maps short segments of the EEG and the envelope to a common embedding space using a CNN in the EEG path and an LSTM in the speech path. The latter also compensates for the brain response delay. In addition, we use transfer learning to fine-tune the model for each subject. The mean classification accuracy of the proposed model reaches 85%, which is significantly higher than that of a state of the art Convolutional Neural Network (CNN)-based model (73%) and the linear model (69%).\n\nINTRODUCTION\n\nOver the past two decades, researchers have tried to model how natural running speech is encoded in the human brain [e.g. 1,2,3,4,5]. In an experimental paradigm where natural running speech is presented to a listener while the EEG is recorded, linear regression is used to either decode features of the speech signal from the EEG signal (backward model), or to predict the EEG signal from the speech stimulus (forward model). Then, the correlation between the actual and the predicted signal is computed and used as a measure of neural tracking of speech [e.g. 3,6,4,5]. This method has applications in domains such as audiology, as part of an objective measure of speech intelligibility [5,7], as well as other potential applications in neuroscience such as brain computer interfaces (BCIs).\n\nUnfortunately, the correlations between actual and predicted signal with either technique are small (in the order of 0.1), limiting its applicability. This is partly due to the use of simple linear models, which cannot model the complex and dynamic nature of the brain. Another problem of linear models is the delay (lag) between speech and EEG [2,8]. Usually this delay varies across subjects which necessitates the use of subject-specific decoders. But importantly, the delay also varies within-subject during a recording [e.g. 9], depending on their state of mind (attention, arousal, effort, etc), which cannot be modeled with a linear model. Hence, linear models have a high variance in performance, both within and across subjects. Furthermore, linear models need long segments of test data (between 30-60 seconds) [8,10,9,11] which is too long for some online applications.\n\nConsidering the complex and highly non-linear nature of auditory processing in the human brain, and with the recent success of deep learning methods in vision related tasks and in automatic speech recognition, using artificial neural networks (ANNs) in this context seems a worthwhile exploration. In [12], a simple feedforward neural network with hyperbolic tangent activation functions was used to reconstruct the envelope from the EEG. Recently, CNNs have also been applied for auditory attention decoding (AAD), in which case the subject attends to one of two concurrent speakers, and the system decodes the attended speaker [13,14].\n\nIn this work, inspired by recent advances in AAD [14,13], we have redefined the more difficult regression problem of reconstructing the speech stimulus from the EEG as a classification problem. The goal is to design a model that can determine whether a given pair of EEG and speech envelope correspond to each other or not. To this end, we have designed a novel LSTM-based deep learning model [15]. When the average performance of the network (% correct) on this task is high, the performance can be used as a proxy for neural coding of the speech envelope.\n\n\nMETHODOLOGY\n\nIn this section, first, we will explain our data collection and preprocessing step. Then, we will explain the our classification task in detail. Finally, we will present the proposed LSTM model to be used in the classification task. The linear model and the CNN model presented in [14] will be used as baseline and state of the art, respectively, for comparisons.\n\n\nData collection and preprocessing\n\nIn our protocol, we present natural running speech, which are stories in quiet, and record the EEG signal simultaneously. 90 normal hearing native Flemish subjects participated in this study. All subjects went through screening for normal hearing test with pure tone audiometry and the Flemish MATRIX-test [16].\n\nStories were chosen from a set of ten unique stories of roughly the same length (14 minutes and 30 seconds). However, the number of stories presented to each subject varies between a minimum of 1 and a maximum of 8. The presentation order of the stories was randomized for each subject. These stories were presented binaurally at 62 dBA with Etymotic ER-3A insert phones. After each stimulus (story), a comprehension question was asked to the subjects to ensure they paid attention. EEG data was recorded using a 64 channel Biosemi Active-Two EEG system at 8 kHz sampling rate. The stimuli were presented using the APEX 4 software platform [17] developed at ExpORL. The experiments took place in an electromagnetically shielded and soundproofed cabin.\n\nAfter each recording, the EEG signal is synchronized with the corresponding stimulus. The envelope of the speech stimulus is extracted using the powerlaw subbands method from [18]. A multi channel Wiener filter [19] is used to remove artefacts from the EEG recordings. Both EEG and envelope are bandpass filtered between 0.5 Hz and 32 Hz. Then, both EEG and envelope are downsampled to 64 Hz. Finally, mean and variance normalization is applied to the EEG and envelope for each recording. Furthermore, we divided each subject's recorded data into training, validation and test sets. The training set contains the first and the last 40% of each recording( 80% in total) and the remaining 20% from the middle of the recording is equally divided between the validation and the test sets in this order.\n\n\nClassification task\n\nWe use a 10 seconds time window with 90% overlap to cut the recorded EEG and envelope into several segments. Each of these segments is considered a sample to the classifiers (models). The positive samples include all the (EEG, envelope) pairs in which the speech envelope corresponds to the recorded EEG. Similarly, the negative samples are pairs of (EEG, envelope), where the speech envelope does not correspond to the recorded EEG. To generate these negative sam-ples (mismatched envelopes), we extract 10 seconds of envelope from the same dataset. This mismatched envelope is chosen randomly either to start one second after the end of matched envelope or to end one second before beginning of the matched envelope.\n\nGiven a pair of (EEG, envelope), we want to have a model (classifier) that can correctly determine whether it is a matched (positive sample) or mismatched (negative sample). Note that compared to a two envelope setup where the task is to choose the matched envelope between the two given candidate envelope (similar to AAD), our one envelope setup is more challenging. We will use classification accuracy as the evaluation measure to compare performance of the models.\n\n\nModels\n\nOur proposed model to classify a pair of (EEG, envelope) as matched or mismatched is shown in Figure 1. It is composed of two separate networks mapping either the EEG or the speech envelope to a sequence of embedding vectors of unit length. The network training is organised to obtain a common embedding space for EEG and speech envelope. This common space ideally should have a very similar representation (i.e. aligned embedding vectors) when speech envelope and EEG are matched, while dissimilar representation (opposite embedding vectors) are expected for mismatched samples.\n\nInstead of mapping the whole 10 seconds of EEG (640 time samples) and envelope into the common embedding space, using a CNN layer we divide the whole 10 second segment into shorter sections which are each mapped to the embedding space (columns of matrices A and B in figure 1). Our hypothesis is that shorter segments of the input data contain enough information, so it should be possible for the network to align EEG and envelope of these shorter segments to each other in the new common space. As a result, in our loss function, we will have more than one sample (i.e. 211) per 10 second segment. We use binary cross entropy (BCE) as our loss function immediately after the cosine similarity scores.\n\nWe have opted for an LSTM layer for speech envelope processing (lower part of the network) to also give our network the ability to compensate for the brain response delay and hence synchronize with the recorded EEG. Furthermore, the LSTM can try to mimic the brain's response to speech input. However, an LSTM's memory capability, expressed in recurrence steps, is limited. To address this, we use a CNN preprocessing layer with a stride of 3 to decrease the number of recurrence steps the LSTM has to apply to model the typical brain response delay to less than 10. The EEG signal is processed by a CNN followed by two dense layers. The CNN layer uses the same temporal stride of 3 to maintain the same sampling rate for both of the EEG and the speech envelope.\n\nOur proposed network contains several important hyperparameters including learning rate, number of LSTM units,  Fig. 1. Our proposed LSTM-based model for classification of match/mismatch. TD refers to time distributed which applies a dense layer to every temporal slice of the input. Dot is a layer that applies dot product (cosine similarity) to its two norm one input vectors. Figure 1), size of the CNN kernels, etc. We have used our train and validation sets to find the optimal values for these hyper parameters. We used 30 epochs with early stopping in the training procedure. The optimal values for the network's hyperparameters are shown in Figure 1. After hyper parameter tuning, our network has around 8000 trainable parameters. The code for the proposed model is provided in https://github.com/jalilpour-m/match-mismatch icassp2020.\n\n\nnumber of units in the time distributed layers (TD in\n\nWe used the linear backward model [20,4] as our linear baseline. To do so, first, we use the linear backward model to reconstruct the envelope from the recorded EEG. Then, we calculate the Spearman correlation between this reconstructed envelope and the candidate envelope. Finally, if the correlation score is above a threshold (tuned for equal false positives and false negatives on the validation set) then it is classified as matched otherwise it is classified as mismatched.\n\nAdditionally, we used a CNN-based model proposed in [14] as a state of the art architecture. We will refer to this network as the SoA (state of the art) network. We tuned the hyperparameters of the SoA network for our dataset the same way we did for our LSTM model.\n\n\nRESULTS\n\n\nSubject dependence\n\nThe models are trained in three different scenarios. First, we can train one specific model per subject, i.e. the training data of just one subject is used. This is repeated for each subject and results in as many models as there are subjects. We will call this scenario subject dependent (SD). Second, we use all the data available from all the subjects to train one generic model, and test it separately for each individual subject. We will use the term subject independent (SI) to refer to this scenario. In the third case, again we train one specific model for each subject, but this time, we initialize the parameters of each SD model with the SI model. Then, we use the train set of each subject to fine tune the model's layers in the EEG path (upper layers in Figure 1). The term transfer learning (TL) will be used to refer to this scenario. It is worth noting that in practical applications, we are interested mostly in the SI and TL scenarios, because the TL scenario is a better alternative for the SD scenario. But we may also report the performance of models in the SD scenario for comparison reasons.\n\n\nEffect of data size\n\nIn this section, we analyze the effect of data size on the classification accuracy of the models. Therefore, we applied all three models to three different data sizes. First, we evaluated methods in the SD scenario (least amount of data). Then, we applied the models in the SI scenario with 20 subjects. Finally, models were applied to the SI scenario again, but this time we used all the data available (90 subjects) in the training. Note that we still use the same 20 subjects in the test set. In addition, in order to evaluate how well the models generalize, we used data of 20 different subjects as holdout data.\n\nBox plots over the 20 subjects are shown in Figure 2. For the subject dependent case, we see that the simple linear model has the same or even better performance than the two deep-learning-based models. But when the size of the data increases in the SI scenario, the performance of the deep learning models increases while the classification accuracy of linear model decreases. These results are in line with our expectation from deep learning literature that more data helps to obtain an ANN model that generalizes better. In contrast, we see that the linear model cannot benefit from more data with subject variation (20 times more data). The proposed LSTM model's average accuracy reaches 80% in the SI scenario which outperforms the linear model (66%) (W = 9, p < 0.001) and the SoA model (72%) (W = 42.5, p = 0.01888). The statistics are reported from a Wilcoxon signed-rank test. Furthermore, we do not see further increase in accuracy when we increase the number of training subjects from 20 to 90. Our results for the holdout data suggest that the models are well generalized over unseen data. sub dependent sub independent (20 subjects) sub independent (90 subjects) holdout data Fig. 2. Effect of data size on classification accuracy: in 'sub dependent', models are trained on one subject only. In 'sub independent', one generic model is trained using data of all subjects. Numbers inside parentheses in the legend indicate the number of subjects used in training. Box plots are shown over a fixed test set of 20 subjects for the first three scenarios. In 'holdout data', the generic model is evaluated on data of 20 new subjects.\n\n\nTransfer learning\n\nHere, we try to investigate the idea of transferring knowledge between ANN models. While more data clearly improves our deep learning models, it is also known that an EEG can be very idiosyncratic, so individualisation of the models may improve performance. However, it is not possible to collect large amounts of data for one individual subject. Therefore we train a generic model using all the subjects, then we fine tune it for each subject. Since the speech signal is the same for all subjects, we only retrain layers in the EEG path (the upper layers in our network). The results are shown in Figure 3. For input frames of 10 seconds long, performance significantly improved with TL (85%) (z = \u22126.97, p < 0.001) compared to the classical SD setting (71%). As performance now approaches 100%, to avoid ceiling effects, we decreased the input frame length to 5 seconds. Overall, this reduction decreases performance, as expected. In this case we also obtained the best performance in the TL scenario (80%), which was again significantly better than the SD scenario (66%) ( z = \u22127.50 , p < 0.001) and the SI scenario (74%) ( z = \u22123.72, p < 0.001). Our results confirm that TL is clearly a better choice than the classical SD scenario when we want to train one model for each subject.\n\n\nCONCLUSION\n\nThe aim of this work was to propose a deep learning architecture to model the relationship between speech stimulus input length = 5s input length = 10s and EEG. More specifically, the goal was to design a model (classifier) that can determine whether a given pair of (EEG, speech envelope) correspond to each other or not. To this end, we introduced an LSTM-based architecture which transforms small temporal segments of EEG and speech envelope to a common embedding space. In this common space, vectors of matched (EEG, envelope) should ideally be similar (aligned), while vectors of mismatched (EEG, envelope) should be dissimilar (not aligned). The reason behind using LSTM in the path of speech envelope is to give our network the ability to compensate for the brain response delay and hence synchronize with the recorded EEG. By doing this, our network finds the appropriate time delay in an automatic way from the training data, hence solving the fixed time delay problem that exists in linear models.\n\n\nThe work is funded byKU  Leuven Special Research Fund C24/18/099 (C2 project to Tom Francart and Hugo Van hamme). This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No 637424, ERC starting Grant to Tom Francart).\n\nFig. 3 .\n3Classification accuracies of the proposed LSTM model in SD, SI, and TL scenarios for two different lengths of input segments. Box plots are shown over 90 subjects.\nWe compared the classification performance of the proposed LSTM-based model with that of the linear baseline and the state of the art (SoA) using data of 90 subjects. Our results show that the LSTM model significantly outperforms the other two models. We found that transferring knowledge from the generic model and fine tuning it on each subject (TL scenario) significantly increased the classification accuracy (up to 85%) compared to the subject dependent scenario. As a result, if one intends to train one model per subject as in the SD scenario, one better uses the TL scenario instead. We conclude that the proposed LSTM architecture can be used to model the relationship between speech stimulus and EEG (also in AAD tasks) as a better alternative for the linear model. Furthermore, the learned embedded representations of EEG and speech envelope could be used in other studies as a rich representation of EEG and speech.\nSpeech comprehension is correlated with temporal response patterns recorded from auditory cortex. E Ahissar, S Nagarajan, M Ahissar, A Protopapas, H Mahncke, M M Merzenich, Proceedings of the National Academy of Sciences. the National Academy of Sciences98E. Ahissar, S. Nagarajan, M. Ahissar, A. Protopapas, H. Mahncke, and M. M. Merzenich, \"Speech compre- hension is correlated with temporal response patterns recorded from auditory cortex,\" Proceedings of the Na- tional Academy of Sciences, vol. 98, no. 23, pp. 13367- 13372, Nov. 2001.\n\nHuman Cortical Responses to the Speech Envelope. J Steven, Terence W Aiken, Picton, Ear and Hearing. 29139Steven J. Aiken and Terence W. Picton, \"Human Corti- cal Responses to the Speech Envelope,\" Ear and Hear- ing, vol. 29, no. 2, pp. 139, Apr. 2008.\n\nEmergence of neural encoding of auditory objects while listening to competing speakers. Nai Ding, Jonathan Z Simon, Proceedings of the National Academy of Sciences. 10929Nai Ding and Jonathan Z. Simon, \"Emergence of neural encoding of auditory objects while listening to compet- ing speakers,\" Proceedings of the National Academy of Sciences, vol. 109, no. 29, pp. 11854-11859, July 2012.\n\nThe Multivariate Temporal Response Function (mTRF) Toolbox: A MAT-LAB Toolbox for Relating Neural Signals to Continuous Stimuli. Michael J Crosse, Giovanni M Di Liberto, Adam Bednar, Edmund C Lalor, Frontiers in Human Neuroscience. 10Michael J. Crosse, Giovanni M. Di Liberto, Adam Bed- nar, and Edmund C. Lalor, \"The Multivariate Tem- poral Response Function (mTRF) Toolbox: A MAT- LAB Toolbox for Relating Neural Signals to Continu- ous Stimuli,\" Frontiers in Human Neuroscience, vol. 10, 2016.\n\nJonas Vanthornhout, Lien Decruy, Jan Wouters, Jonathan Z Simon, Tom Francart, Speech Intelligibility Predicted from Neural Entrainment of the Speech Envelope. 19Jonas Vanthornhout, Lien Decruy, Jan Wouters, Jonathan Z. Simon, and Tom Francart, \"Speech In- telligibility Predicted from Neural Entrainment of the Speech Envelope,\" Journal of the Association for Re- search in Otolaryngology, vol. 19, no. 2, pp. 181-191, Apr. 2018.\n\nLow-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing. Giovannim, Jamesa Diliberto, Ed-Mundc Osullivan, Lalor, Current Biology. 2519GiovanniM. DiLiberto, JamesA. OSullivan, and Ed- mundC. Lalor, \"Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing,\" Cur- rent Biology, vol. 25, no. 19, pp. 2457-2465, Oct. 2015.\n\nPredicting individual speech intelligibility from the cortical tracking of acoustic-and phonetic-level speech representations. D Lesenfants, J Vanthornhout, E Verschueren, L Decruy, T Francart, Hearing Research. 380D. Lesenfants, J. Vanthornhout, E. Verschueren, L. De- cruy, and T. Francart, \"Predicting individual speech in- telligibility from the cortical tracking of acoustic-and phonetic-level speech representations,\" Hearing Re- search, vol. 380, pp. 1-9, Sept. 2019.\n\nDecoding the attended speech stream with multi-channel EEG: implications for online, dailylife applications. Bojana Mirkovic, Stefan Debener, Manuela Jaeger, Maarten De Vos, Journal of Neural Engineering. 12446007Bojana Mirkovic, Stefan Debener, Manuela Jaeger, and Maarten De Vos, \"Decoding the attended speech stream with multi-channel EEG: implications for online, daily- life applications,\" Journal of Neural Engineering, vol. 12, no. 4, pp. 046007, June 2015.\n\nAdaptive Temporal Encoding Leads to a Background-Insensitive Cortical Representation of Speech. Nai Ding, Jonathan Z Simon, Journal of Neuroscience. 3313Nai Ding and Jonathan Z. Simon, \"Adaptive Tempo- ral Encoding Leads to a Background-Insensitive Corti- cal Representation of Speech,\" Journal of Neuroscience, vol. 33, no. 13, pp. 5728-5735, Mar. 2013.\n\nDecoding the auditory brain with canonical component analysis. Alain De Cheveign, D E Daniel, Giovanni M Wong, Jens Di Liberto, Malcolm Hjortkjr, Edmund Slaney, Lalor, NeuroImage. 172Alain de Cheveign, Daniel D. E. Wong, Giovanni M. Di Liberto, Jens Hjortkjr, Malcolm Slaney, and Edmund Lalor, \"Decoding the auditory brain with canonical component analysis,\" NeuroImage, vol. 172, pp. 206- 216, May 2018.\n\nNoise-robust cortical tracking of attended speech in real-world acoustic scenes. Torsten Sren Asp Fuglsang, Jens Dau, Hjortkjr, NeuroImage. 156Sren Asp Fuglsang, Torsten Dau, and Jens Hjortkjr, \"Noise-robust cortical tracking of attended speech in real-world acoustic scenes,\" NeuroImage, vol. 156, pp. 435-444, Aug. 2017.\n\nMachine learning for decoding listeners attention from electroencephalography evoked by continuous speech. Birger Tobias De Taillez, Bernd T Kollmeier, Meyer, European Journal of Neuroscience. 00Tobias de Taillez, Birger Kollmeier, and Bernd T. Meyer, \"Machine learning for decoding listeners atten- tion from electroencephalography evoked by continuous speech,\" European Journal of Neuroscience, vol. 0, no. 0, Dec. 2017.\n\nEEG-based detection of the attended speaker and the locus of auditory attention with convolutional neural networks. Lucas Deckers, Neetha Das, Alexander Amir Hossein Ansari, Tom Bertrand, Francart, 475673Lucas Deckers, Neetha Das, Amir Hossein Ansari, Alexander Bertrand, and Tom Francart, \"EEG-based de- tection of the attended speaker and the locus of auditory attention with convolutional neural networks,\" bioRxiv, p. 475673, Dec. 2018.\n\nComparison of Two-Talker Attention Decoding from EEG with Nonlinear Neural Networks and Linear Methods. Gregory Ciccarelli, Michael Nolan, Joseph Perricone, Paul T Calamia, Stephanie Haro, James Osullivan, Nima Mesgarani, Thomas F Quatieri, Christopher J Smalt, Scientific Reports. 9111538Gregory Ciccarelli, Michael Nolan, Joseph Perricone, Paul T. Calamia, Stephanie Haro, James OSullivan, Nima Mesgarani, Thomas F. Quatieri, and Christo- pher J. Smalt, \"Comparison of Two-Talker Attention Decoding from EEG with Nonlinear Neural Networks and Linear Methods,\" Scientific Reports, vol. 9, no. 1, pp. 11538, Dec. 2019.\n\nLong Short-Term Memory. Sepp Hochreiter, Jrgen Schmidhuber, Neural Computation. 98Sepp Hochreiter and Jrgen Schmidhuber, \"Long Short- Term Memory,\" Neural Computation, vol. 9, no. 8, pp. 1735-1780, Nov. 1997.\n\nDevelopment and normative data for the flemish/dutch matrix test. Heleen Luts, Sofie Jansen, Wouter Dreschler, Jan Wouters, Heleen Luts, Sofie Jansen, Wouter Dreschler, and Jan Wouters, \"Development and normative data for the flemish/dutch matrix test,\" 2014.\n\nAPEX 3: a multi-purpose test platform for auditory psychophysical experiments. Tom Francart, Astrid Van Wieringen, Jan Wouters, Journal of Neuroscience Methods. 1722Tom Francart, Astrid van Wieringen, and Jan Wouters, \"APEX 3: a multi-purpose test platform for auditory psychophysical experiments,\" Journal of Neuroscience Methods, vol. 172, no. 2, pp. 283-293, July 2008.\n\nAuditory-Inspired Speech Envelope Extraction Methods for Improved EEG-Based Auditory Attention Detection in a Cocktail Party Scenario. W Biesmans, N Das, T Francart, A Bertrand, IEEE Transactions on Neural Systems and Rehabilitation Engineering. 255W. Biesmans, N. Das, T. Francart, and A. Bertrand, \"Auditory-Inspired Speech Envelope Extraction Meth- ods for Improved EEG-Based Auditory Attention De- tection in a Cocktail Party Scenario,\" IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 5, pp. 402-412, May 2017.\n\nA generic EEG artifact removal algorithm based on the multi-channel Wiener filter. Ben Somers, Tom Francart, Alexander Bertrand, Journal of Neural Engineering. 15336007Ben Somers, Tom Francart, and Alexander Bertrand, \"A generic EEG artifact removal algorithm based on the multi-channel Wiener filter,\" Journal of Neural Engi- neering, vol. 15, no. 3, pp. 036007, Feb. 2018.\n\nAttentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG. James A O&apos;sullivan, Alan J Power, Nima Mesgarani, Siddharth Rajaram, John J Foxe, Barbara G Shinn-Cunningham, Malcolm Slaney, A Shihab, Edmund C Shamma, Lalor, Cerebral Cortex. 257James A. O'Sullivan, Alan J. Power, Nima Mesgarani, Siddharth Rajaram, John J. Foxe, Barbara G. Shinn- Cunningham, Malcolm Slaney, Shihab A. Shamma, and Edmund C. Lalor, \"Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG,\" Cerebral Cortex (New York, N.Y.: 1991), vol. 25, no. 7, pp. 1697-1706, July 2015.\n", "annotations": {"author": "[{\"end\":140,\"start\":63},{\"end\":204,\"start\":141},{\"end\":278,\"start\":205},{\"end\":343,\"start\":279},{\"end\":359,\"start\":344},{\"end\":371,\"start\":360},{\"end\":376,\"start\":372},{\"end\":431,\"start\":377}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":82},{\"end\":152,\"start\":147},{\"end\":226,\"start\":210},{\"end\":291,\"start\":283},{\"end\":358,\"start\":353},{\"end\":370,\"start\":364},{\"end\":375,\"start\":372}]", "author_first_name": "[{\"end\":71,\"start\":63},{\"end\":81,\"start\":72},{\"end\":146,\"start\":141},{\"end\":209,\"start\":205},{\"end\":282,\"start\":279},{\"end\":348,\"start\":344},{\"end\":352,\"start\":349},{\"end\":361,\"start\":360},{\"end\":363,\"start\":362}]", "author_affiliation": "[{\"end\":139,\"start\":90},{\"end\":203,\"start\":154},{\"end\":277,\"start\":228},{\"end\":342,\"start\":293},{\"end\":430,\"start\":378}]", "title": "[{\"end\":60,\"start\":1},{\"end\":491,\"start\":432}]", "venue": null, "abstract": "[{\"end\":1680,\"start\":545}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1820,\"start\":1818},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1822,\"start\":1820},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1824,\"start\":1822},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1826,\"start\":1824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1828,\"start\":1826},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2260,\"start\":2258},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2262,\"start\":2260},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2264,\"start\":2262},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2266,\"start\":2264},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2388,\"start\":2385},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2390,\"start\":2388},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2839,\"start\":2836},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2841,\"start\":2839},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3315,\"start\":3312},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3318,\"start\":3315},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3320,\"start\":3318},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3323,\"start\":3320},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3678,\"start\":3674},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4006,\"start\":4002},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4009,\"start\":4006},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4065,\"start\":4061},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4068,\"start\":4065},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4409,\"start\":4405},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4870,\"start\":4866},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5296,\"start\":5292},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5943,\"start\":5939},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6231,\"start\":6227},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6267,\"start\":6263},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11060,\"start\":11056},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11062,\"start\":11060},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11559,\"start\":11555}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17855,\"start\":17527},{\"attributes\":{\"id\":\"fig_3\"},\"end\":18030,\"start\":17856}]", "paragraph": "[{\"end\":2489,\"start\":1696},{\"end\":3371,\"start\":2491},{\"end\":4010,\"start\":3373},{\"end\":4569,\"start\":4012},{\"end\":4948,\"start\":4585},{\"end\":5297,\"start\":4986},{\"end\":6050,\"start\":5299},{\"end\":6850,\"start\":6052},{\"end\":7592,\"start\":6874},{\"end\":8062,\"start\":7594},{\"end\":8652,\"start\":8073},{\"end\":9355,\"start\":8654},{\"end\":10119,\"start\":9357},{\"end\":10964,\"start\":10121},{\"end\":11501,\"start\":11022},{\"end\":11768,\"start\":11503},{\"end\":12915,\"start\":11801},{\"end\":13555,\"start\":12939},{\"end\":15197,\"start\":13557},{\"end\":16504,\"start\":15219},{\"end\":17526,\"start\":16519}]", "formula": null, "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1694,\"start\":1682},{\"attributes\":{\"n\":\"2.\"},\"end\":4583,\"start\":4572},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4984,\"start\":4951},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6872,\"start\":6853},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8071,\"start\":8065},{\"end\":11020,\"start\":10967},{\"attributes\":{\"n\":\"3.\"},\"end\":11778,\"start\":11771},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11799,\"start\":11781},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12937,\"start\":12918},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15217,\"start\":15200},{\"attributes\":{\"n\":\"4.\"},\"end\":16517,\"start\":16507},{\"end\":17865,\"start\":17857}]", "table": null, "figure_caption": "[{\"end\":17855,\"start\":17529},{\"end\":18030,\"start\":17867}]", "figure_ref": "[{\"end\":8175,\"start\":8167},{\"end\":8929,\"start\":8921},{\"end\":10239,\"start\":10233},{\"end\":10509,\"start\":10500},{\"end\":10778,\"start\":10770},{\"end\":12576,\"start\":12568},{\"end\":13609,\"start\":13601},{\"end\":14752,\"start\":14746},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15825,\"start\":15817}]", "bib_author_first_name": "[{\"end\":19058,\"start\":19057},{\"end\":19069,\"start\":19068},{\"end\":19082,\"start\":19081},{\"end\":19093,\"start\":19092},{\"end\":19107,\"start\":19106},{\"end\":19118,\"start\":19117},{\"end\":19120,\"start\":19119},{\"end\":19551,\"start\":19550},{\"end\":19567,\"start\":19560},{\"end\":19569,\"start\":19568},{\"end\":19846,\"start\":19843},{\"end\":19861,\"start\":19853},{\"end\":19863,\"start\":19862},{\"end\":20281,\"start\":20274},{\"end\":20283,\"start\":20282},{\"end\":20300,\"start\":20292},{\"end\":20305,\"start\":20301},{\"end\":20319,\"start\":20315},{\"end\":20334,\"start\":20328},{\"end\":20336,\"start\":20335},{\"end\":20648,\"start\":20643},{\"end\":20667,\"start\":20663},{\"end\":20679,\"start\":20676},{\"end\":20697,\"start\":20689},{\"end\":20699,\"start\":20698},{\"end\":20710,\"start\":20707},{\"end\":21171,\"start\":21165},{\"end\":21191,\"start\":21183},{\"end\":21567,\"start\":21566},{\"end\":21581,\"start\":21580},{\"end\":21597,\"start\":21596},{\"end\":21612,\"start\":21611},{\"end\":21622,\"start\":21621},{\"end\":22030,\"start\":22024},{\"end\":22047,\"start\":22041},{\"end\":22064,\"start\":22057},{\"end\":22083,\"start\":22073},{\"end\":22480,\"start\":22477},{\"end\":22495,\"start\":22487},{\"end\":22497,\"start\":22496},{\"end\":22805,\"start\":22800},{\"end\":22820,\"start\":22819},{\"end\":22822,\"start\":22821},{\"end\":22839,\"start\":22831},{\"end\":22841,\"start\":22840},{\"end\":22852,\"start\":22848},{\"end\":22872,\"start\":22865},{\"end\":22889,\"start\":22883},{\"end\":23231,\"start\":23224},{\"end\":23255,\"start\":23251},{\"end\":23580,\"start\":23574},{\"end\":23605,\"start\":23600},{\"end\":23607,\"start\":23606},{\"end\":24012,\"start\":24007},{\"end\":24028,\"start\":24022},{\"end\":24043,\"start\":24034},{\"end\":24068,\"start\":24065},{\"end\":24444,\"start\":24437},{\"end\":24464,\"start\":24457},{\"end\":24478,\"start\":24472},{\"end\":24494,\"start\":24490},{\"end\":24496,\"start\":24495},{\"end\":24515,\"start\":24506},{\"end\":24527,\"start\":24522},{\"end\":24543,\"start\":24539},{\"end\":24561,\"start\":24555},{\"end\":24563,\"start\":24562},{\"end\":24585,\"start\":24574},{\"end\":24587,\"start\":24586},{\"end\":24981,\"start\":24977},{\"end\":24999,\"start\":24994},{\"end\":25235,\"start\":25229},{\"end\":25247,\"start\":25242},{\"end\":25262,\"start\":25256},{\"end\":25277,\"start\":25274},{\"end\":25506,\"start\":25503},{\"end\":25523,\"start\":25517},{\"end\":25542,\"start\":25539},{\"end\":25934,\"start\":25933},{\"end\":25946,\"start\":25945},{\"end\":25953,\"start\":25952},{\"end\":25965,\"start\":25964},{\"end\":26433,\"start\":26430},{\"end\":26445,\"start\":26442},{\"end\":26465,\"start\":26456},{\"end\":26820,\"start\":26815},{\"end\":26822,\"start\":26821},{\"end\":26844,\"start\":26840},{\"end\":26846,\"start\":26845},{\"end\":26858,\"start\":26854},{\"end\":26879,\"start\":26870},{\"end\":26893,\"start\":26889},{\"end\":26895,\"start\":26894},{\"end\":26909,\"start\":26902},{\"end\":26911,\"start\":26910},{\"end\":26937,\"start\":26930},{\"end\":26947,\"start\":26946},{\"end\":26962,\"start\":26956},{\"end\":26964,\"start\":26963}]", "bib_author_last_name": "[{\"end\":19066,\"start\":19059},{\"end\":19079,\"start\":19070},{\"end\":19090,\"start\":19083},{\"end\":19104,\"start\":19094},{\"end\":19115,\"start\":19108},{\"end\":19130,\"start\":19121},{\"end\":19558,\"start\":19552},{\"end\":19575,\"start\":19570},{\"end\":19583,\"start\":19577},{\"end\":19851,\"start\":19847},{\"end\":19869,\"start\":19864},{\"end\":20290,\"start\":20284},{\"end\":20313,\"start\":20306},{\"end\":20326,\"start\":20320},{\"end\":20342,\"start\":20337},{\"end\":20661,\"start\":20649},{\"end\":20674,\"start\":20668},{\"end\":20687,\"start\":20680},{\"end\":20705,\"start\":20700},{\"end\":20719,\"start\":20711},{\"end\":21163,\"start\":21154},{\"end\":21181,\"start\":21172},{\"end\":21201,\"start\":21192},{\"end\":21208,\"start\":21203},{\"end\":21578,\"start\":21568},{\"end\":21594,\"start\":21582},{\"end\":21609,\"start\":21598},{\"end\":21619,\"start\":21613},{\"end\":21631,\"start\":21623},{\"end\":22039,\"start\":22031},{\"end\":22055,\"start\":22048},{\"end\":22071,\"start\":22065},{\"end\":22087,\"start\":22084},{\"end\":22485,\"start\":22481},{\"end\":22503,\"start\":22498},{\"end\":22817,\"start\":22806},{\"end\":22829,\"start\":22823},{\"end\":22846,\"start\":22842},{\"end\":22863,\"start\":22853},{\"end\":22881,\"start\":22873},{\"end\":22896,\"start\":22890},{\"end\":22903,\"start\":22898},{\"end\":23249,\"start\":23232},{\"end\":23259,\"start\":23256},{\"end\":23269,\"start\":23261},{\"end\":23598,\"start\":23581},{\"end\":23617,\"start\":23608},{\"end\":23624,\"start\":23619},{\"end\":24020,\"start\":24013},{\"end\":24032,\"start\":24029},{\"end\":24063,\"start\":24044},{\"end\":24077,\"start\":24069},{\"end\":24087,\"start\":24079},{\"end\":24455,\"start\":24445},{\"end\":24470,\"start\":24465},{\"end\":24488,\"start\":24479},{\"end\":24504,\"start\":24497},{\"end\":24520,\"start\":24516},{\"end\":24537,\"start\":24528},{\"end\":24553,\"start\":24544},{\"end\":24572,\"start\":24564},{\"end\":24593,\"start\":24588},{\"end\":24992,\"start\":24982},{\"end\":25011,\"start\":25000},{\"end\":25240,\"start\":25236},{\"end\":25254,\"start\":25248},{\"end\":25272,\"start\":25263},{\"end\":25285,\"start\":25278},{\"end\":25515,\"start\":25507},{\"end\":25537,\"start\":25524},{\"end\":25550,\"start\":25543},{\"end\":25943,\"start\":25935},{\"end\":25950,\"start\":25947},{\"end\":25962,\"start\":25954},{\"end\":25974,\"start\":25966},{\"end\":26440,\"start\":26434},{\"end\":26454,\"start\":26446},{\"end\":26474,\"start\":26466},{\"end\":26838,\"start\":26823},{\"end\":26852,\"start\":26847},{\"end\":26868,\"start\":26859},{\"end\":26887,\"start\":26880},{\"end\":26900,\"start\":26896},{\"end\":26928,\"start\":26912},{\"end\":26944,\"start\":26938},{\"end\":26954,\"start\":26948},{\"end\":26971,\"start\":26965},{\"end\":26978,\"start\":26973}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8253817},\"end\":19499,\"start\":18959},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":43037341},\"end\":19753,\"start\":19501},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15570759},\"end\":20143,\"start\":19755},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12165026},\"end\":20641,\"start\":20145},{\"attributes\":{\"id\":\"b4\"},\"end\":21072,\"start\":20643},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15888974},\"end\":21437,\"start\":21074},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":167221605},\"end\":21913,\"start\":21439},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":22076362},\"end\":22379,\"start\":21915},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11814655},\"end\":22735,\"start\":22381},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3531854},\"end\":23141,\"start\":22737},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5319132},\"end\":23465,\"start\":23143},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":5275290},\"end\":23889,\"start\":23467},{\"attributes\":{\"id\":\"b12\"},\"end\":24331,\"start\":23891},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":91320213},\"end\":24951,\"start\":24333},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1915014},\"end\":25161,\"start\":24953},{\"attributes\":{\"id\":\"b15\"},\"end\":25422,\"start\":25163},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":41858699},\"end\":25796,\"start\":25424},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":27835175},\"end\":26345,\"start\":25798},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10736717},\"end\":26721,\"start\":26347},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2934551},\"end\":27345,\"start\":26723}]", "bib_title": "[{\"end\":19055,\"start\":18959},{\"end\":19548,\"start\":19501},{\"end\":19841,\"start\":19755},{\"end\":20272,\"start\":20145},{\"end\":21152,\"start\":21074},{\"end\":21564,\"start\":21439},{\"end\":22022,\"start\":21915},{\"end\":22475,\"start\":22381},{\"end\":22798,\"start\":22737},{\"end\":23222,\"start\":23143},{\"end\":23572,\"start\":23467},{\"end\":24435,\"start\":24333},{\"end\":24975,\"start\":24953},{\"end\":25501,\"start\":25424},{\"end\":25931,\"start\":25798},{\"end\":26428,\"start\":26347},{\"end\":26813,\"start\":26723}]", "bib_author": "[{\"end\":19068,\"start\":19057},{\"end\":19081,\"start\":19068},{\"end\":19092,\"start\":19081},{\"end\":19106,\"start\":19092},{\"end\":19117,\"start\":19106},{\"end\":19132,\"start\":19117},{\"end\":19560,\"start\":19550},{\"end\":19577,\"start\":19560},{\"end\":19585,\"start\":19577},{\"end\":19853,\"start\":19843},{\"end\":19871,\"start\":19853},{\"end\":20292,\"start\":20274},{\"end\":20315,\"start\":20292},{\"end\":20328,\"start\":20315},{\"end\":20344,\"start\":20328},{\"end\":20663,\"start\":20643},{\"end\":20676,\"start\":20663},{\"end\":20689,\"start\":20676},{\"end\":20707,\"start\":20689},{\"end\":20721,\"start\":20707},{\"end\":21165,\"start\":21154},{\"end\":21183,\"start\":21165},{\"end\":21203,\"start\":21183},{\"end\":21210,\"start\":21203},{\"end\":21580,\"start\":21566},{\"end\":21596,\"start\":21580},{\"end\":21611,\"start\":21596},{\"end\":21621,\"start\":21611},{\"end\":21633,\"start\":21621},{\"end\":22041,\"start\":22024},{\"end\":22057,\"start\":22041},{\"end\":22073,\"start\":22057},{\"end\":22089,\"start\":22073},{\"end\":22487,\"start\":22477},{\"end\":22505,\"start\":22487},{\"end\":22819,\"start\":22800},{\"end\":22831,\"start\":22819},{\"end\":22848,\"start\":22831},{\"end\":22865,\"start\":22848},{\"end\":22883,\"start\":22865},{\"end\":22898,\"start\":22883},{\"end\":22905,\"start\":22898},{\"end\":23251,\"start\":23224},{\"end\":23261,\"start\":23251},{\"end\":23271,\"start\":23261},{\"end\":23600,\"start\":23574},{\"end\":23619,\"start\":23600},{\"end\":23626,\"start\":23619},{\"end\":24022,\"start\":24007},{\"end\":24034,\"start\":24022},{\"end\":24065,\"start\":24034},{\"end\":24079,\"start\":24065},{\"end\":24089,\"start\":24079},{\"end\":24457,\"start\":24437},{\"end\":24472,\"start\":24457},{\"end\":24490,\"start\":24472},{\"end\":24506,\"start\":24490},{\"end\":24522,\"start\":24506},{\"end\":24539,\"start\":24522},{\"end\":24555,\"start\":24539},{\"end\":24574,\"start\":24555},{\"end\":24595,\"start\":24574},{\"end\":24994,\"start\":24977},{\"end\":25013,\"start\":24994},{\"end\":25242,\"start\":25229},{\"end\":25256,\"start\":25242},{\"end\":25274,\"start\":25256},{\"end\":25287,\"start\":25274},{\"end\":25517,\"start\":25503},{\"end\":25539,\"start\":25517},{\"end\":25552,\"start\":25539},{\"end\":25945,\"start\":25933},{\"end\":25952,\"start\":25945},{\"end\":25964,\"start\":25952},{\"end\":25976,\"start\":25964},{\"end\":26442,\"start\":26430},{\"end\":26456,\"start\":26442},{\"end\":26476,\"start\":26456},{\"end\":26840,\"start\":26815},{\"end\":26854,\"start\":26840},{\"end\":26870,\"start\":26854},{\"end\":26889,\"start\":26870},{\"end\":26902,\"start\":26889},{\"end\":26930,\"start\":26902},{\"end\":26946,\"start\":26930},{\"end\":26956,\"start\":26946},{\"end\":26973,\"start\":26956},{\"end\":26980,\"start\":26973}]", "bib_venue": "[{\"end\":19213,\"start\":19181},{\"end\":19179,\"start\":19132},{\"end\":19600,\"start\":19585},{\"end\":19918,\"start\":19871},{\"end\":20375,\"start\":20344},{\"end\":20800,\"start\":20721},{\"end\":21225,\"start\":21210},{\"end\":21649,\"start\":21633},{\"end\":22118,\"start\":22089},{\"end\":22528,\"start\":22505},{\"end\":22915,\"start\":22905},{\"end\":23281,\"start\":23271},{\"end\":23658,\"start\":23626},{\"end\":24005,\"start\":23891},{\"end\":24613,\"start\":24595},{\"end\":25031,\"start\":25013},{\"end\":25227,\"start\":25163},{\"end\":25583,\"start\":25552},{\"end\":26042,\"start\":25976},{\"end\":26505,\"start\":26476},{\"end\":26995,\"start\":26980}]"}}}, "year": 2023, "month": 12, "day": 17}