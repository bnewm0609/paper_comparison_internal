{"id": 2899486, "updated": "2023-10-01 03:59:49.401", "metadata": {"title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward", "authors": "[{\"first\":\"Zhou\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Xiaoyu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Ning\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xutao\",\"last\":\"Lv\",\"middle\":[]},{\"first\":\"Li-Jia\",\"last\":\"Li\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": 4, "day": 12}, "abstract": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a\"policy network\"and a\"value network\"to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1704.03899", "mag": "2952591111", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/RenWZLL17", "doi": "10.1109/cvpr.2017.128"}}, "content": {"source": {"pdf_hash": "0041afaf2b17f1a33bd514db27b17ce34670fdb8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1704.03899v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1704.03899", "status": "GREEN"}}, "grobid": {"id": "249b2cdb3a0270ff1e0866b5280e6305ec0b4141", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0041afaf2b17f1a33bd514db27b17ce34670fdb8.txt", "contents": "\nDeep Reinforcement Learning-based Image Captioning with Embedding Reward\n\n\nZhou Ren zhou.ren@snap.com \nSnap Inc\n\n\nXiaoyu Wang xiaoyu.wang@snap.com \nSnap Inc\n\n\nNing Zhang ning.zhang@snap.com \nSnap Inc\n\n\nXutao Lv xutao.lv@snap.com \nSnap Inc\n\n\nLi-Jia Li lijiali@cs.stanford.edu \nGoogle Inc\n\n\nDeep Reinforcement Learning-based Image Captioning with Embedding Reward\n\nImage captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a \"policy network\" and a \"value network\" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-ofthe-art approaches across different evaluation metrics.\n\nIntroduction\n\nImage captioning, the task of automatically describing the content of an image with natural language, has attracted increasingly interests in computer vision. It is interesting because it aims at endowing machines with one of the core human intelligence to understand the huge amount of visual information and to express it in natural language.\n\nRecent state-of-the-art approaches [3,44,30,17,7,46,15,48,43] follow an encoder-decoder framework to generate captions for the images. They generally employ convolutional neural networks to encode the visual information and utilize recurrent neural networks to decode that infor- * This work was done when the author was at Snap Inc.  Figure 1. Illustration of the proposed decision-making framework. During our lookahead inference procedure, we utilize a \"policy network\" and a \"value network\" to collaboratively predict the word for each time step. The policy network provides an action prediction that locally predicts the next word according to current state. The value network provides a reward prediction that globally evaluates all possible extensions of the current state. mation to coherent sentences. During training and inference, they try to maximize the probability of the next word based on recurrent hidden state.\n\nIn this paper, we introduce a novel decision-making framework for image captioning. Instead of learning a sequential recurrent model to greedily look for the next correct word, we utilize a \"policy network\" and a \"value network\" to jointly determine the next best word at each time step. The policy network, which provides the confidence of predicting the next word according to current state, serves as a local guidance. The value network, that evaluates the reward value of all possible extensions of the current state, serves as a global and lookahead guidance. Such value network adjusts the goal of predicting the correct words towards the goal of generating captions that are similar to ground truth captions. Our framework is able to include the good words that are with low probability to be drawn by using the policy network alone. Figure 1 shows an example to illustrate the proposed framework. The word holding is not among the top choices of our policy network at current step. But our value network goes forward for one step to the state supposing holding is generated and evaluates how good such state is for the goal of generating a good caption in the end. The two networks complement each other and are able to choose the word holding.\n\nTo learn the policy and value networks, we use deep reinforcement learning with embedding reward. We begin by pretraining a policy network using standard supervised learning with cross entropy loss, and by pretraining a value network with mean squared loss. Then, we improve the policy and value networks by deep reinforcement learning. Reinforcement learning has been widely used in gaming [38], control theory [32], etc. The problems in control or gaming have concrete targets to optimize by nature, whereas defining an appropriate optimization goal is nontrivial for image captioning. In this paper, we propose to train using an actor-critic model [21] with reward driven by visualsemantic embedding [11,19,36,37]. Visual-semantic embedding, which provides a measure of similarity between images and sentences, can measure the correctness of generated captions and serve as a reasonable global target to optimize for image captioning in reinforcement learning.\n\nWe conduct detailed analyses on our framework to understand its merits and properties. Extensive experiments on the Microsoft COCO dataset [29] show that the proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics, including BLEU [34], Meteor [25], Rouge [28] and CIDEr [42]. The contributions of this paper are summarized as follows:\n\n\u2022 We present a novel decision-making framework for image captioning utilizing a policy network and a value network. Our method achieves state-of-the-art performance on the MS COCO dataset. To our best knowledge, this is the first work that applies decisionmaking framework to image captioning.\n\n\u2022 To learn our policy and value networks, we introduce an actor-critic reinforcement learning algorithm driven by visual-semantic embedding. Our experiments suggest that the supervision from embedding generalizes well across different evaluation metrics.\n\n\nRelated Work\n\n\nImage captioning\n\nMany image captioning approaches have been proposed in the literature. Early approaches tackled this problem using bottom-up paradigm [10,23,27,47,24,8,26,9], which first generated descriptive words of an image by object recognition and attribute prediction, and then combined them by language models. Recently, inspired by the successful use of neural networks in machine translation [4], the encoder-decoder framework [3,44,30,17,7,46,15,48,43] has been brought to image captioning. Researchers adopted such framework because \"translating\" an image to a sentence was analogous to the task in machine translation. Approaches following this framework generally encoded an image as a single feature vector by convolutional neural networks [22,6,39,41], and then fed such vector into recurrent neural networks [14,5] to generate captions. On top of it, various modeling strategies have been developed. Karpathy and Fei-Fei [17], Fang et al. [9] presented methods to enhance their models by detecting objects in images. To mimic the visual system of humans [20], spatial attention [46] and semantic attention [48] were proposed to direct the model to attend to the meaningful fine details. Dense captioning [16] was proposed to handle the localization and captioning tasks simultaneously. Ranzato et al. [35] proposed a sequence-level training algorithm.\n\nDuring inference, most state-of-the-art methods employ a common decoder mechanism using greedy search or beam search. Words are sequentially drawn according to local confidence. Since they always predict the words with top local confidence, such mechanism can miss good words at early steps which may lead to bad captions. In contrast, in addition to the local guidance, our method also utilizes a global and lookahead guidance to compensate such errors.\n\n\nDecision-making\n\nDecision-making is the core problem in computer gaming [38], control theory [32], navigation and path planning [49], etc. In those problems, there exist agents that interact with the environment, execute a series of actions, and aim to fulfill some pre-defined goals. Reinforcement learning [45,21,40,31], known as \"a machine learning technique concerning how software agent ought to take actions in an environment so as to maximize some notion of cumulative reward\", is well suited for the task of decisionmaking. Recently, professional-level computer Go program was designed by Silver et al. [38] using deep neural networks and Monte Carlo Tree Search. Human-level gaming control [32] was achieved through deep Q-learning. A visual navigation system [49] was proposed recently based on actor-critic reinforcement learning model.\n\nDecision-making framework has not been applied to image captioning. One previous work in text generation [35] has used REINFORCE [45] to train its model by directly optimizing a user-specified evaluation metric. Such metricdriven approach [35] is hard to generalize to other metrics. To perform well across different metrics, it needs to be re-trained for each one in isolation. In this paper, we propose a training method using actor-critic reinforcement learning [21] driven by visual-semantic embedding [11,19], which performs well across different evaluation metrics without re-training. Our approach shows significant performance improvement over [35]. Moreover, we use a decision-making framework to generate captions, while [35] follows the existing encoder-decoder framework. \n\n\nDeep Reinforcement Learning-based Image Captioning\n\nIn this section, we first define our formulation for deep reinforcement learning-based image captioning and propose a novel reward function defined by visual-semantic embedding. Then we introduce our training procedure as well as our inference mechanism.\n\n\nProblem formulation\n\nWe formulate image captioning as a decision-making process. In decision-making, there is an agent that interacts with the environment, and executes a series of actions, so as to optimize a goal. In image captioning, the goal is, given an image I, to generate a sentence S = {w 1 , w 2 , ..., w T } which correctly describes the image content, where w i is a word in sentence S and T is the length. Our model, including the policy network p \u03c0 and value network v \u03b8 , can be viewed as the agent; the environment is the given image I and the words predicted so far {w 1 , ..., w t }; and an action is to predict the next word w t+1 .\n\n\nState and action space\n\nA decision-making process consists of a series of actions. After each action a, a state s is observed. In our problem, state s t at time step t consists of the image I and the words predicted until t, {w 1 , ..., w t }. The action space is the dictionary Y that the words are drawn from, i.e., a t \u2282 Y.\n\n\nPolicy network\n\nThe policy network p \u03c0 provides the probability for the agent to take actions at each state, p \u03c0 (a t |s t ), where the current state s t = {I, w 1 , ..., w t } and action a t = w t+1 . In this paper, we use a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN) to construct our policy network, denoted as CNN p and RNN p . It is similar to the basic image captioning model [44] used in the encoderdecoder framework. As shown in Figure 2, firstly we use CNN p to encode the visual information of image I. The visual information is then fed into the initial input node x 0 \u2208 R n of RNN p . As the hidden state h t \u2208 R m of RNN p evolves over time t, the policy at each time step to take an action a t is provided. The generated word w t at t will be fed back into RNN p in the next time step as the network input x t+1 , which drives the RNN p state transition from h t to h t+1 . Specifically, the main working flow of p \u03c0 is governed by the following equations:\nCNN v \"A dog sits on a\" RNN v concatenation layer s t MLP v v \u2713 (s t )x 0 = W x,v CNN p (I) (1) h t = RNN p (h t\u22121 , x t ) (2) x t = \u03c6(w t\u22121 ), t > 0 (3) p \u03c0 (a t |s t ) = \u03d5(h t )(4)\nwhere W x,v is the weight of the linear embedding model of visual information, \u03c6 and \u03d5 denote the input and output models of RNN p .\n\n\nValue network\n\nBefore we introduce our value network v \u03b8 , we first define the value function v p of a policy p. v p is defined as the prediction of the total reward r (will be defined later in Section 3.2) from the observed state s t , assuming the decisionmaking process is following a policy p, i.e.,\nv p (s) = E[r|s t = s, a t...T \u223c p](5)\nWe approximate the value function using a value network, v \u03b8 (s) \u2248 v p (s). It serves as an evaluation of state s t = {I, w 1 , ..., w t }. As shown in Figure 3, our value network is comprised of a CNN, a RNN, and a Multilayer Perceptron (MLP), denoted as CNN v , RNN v and MLP v . Our value network takes the raw image and sentence inputs. CNN v is utilized to encode the visual information of I, RNN v is designed to encode the semantic information of a partially generated sentence {w 1 , ..., w t }. All the components are trained simultaneously to regress the scalar reward from s t . We investigate our value network architecture in Section 4.4.\n\n\nReward defined by visual-semantic embedding\n\nIn our decision-making framework, it is important to define a concrete and reasonable optimization goal, i.e., the reward for reinforcement learning. We propose to utilize visual-semantic embedding similarities as the reward.\n\nVisual-semantic embedding has been successfully applied to image classification [11,37], retrieval [19,36,33], etc. Our embedding model is comprised of a CNN, a RNN and a linear mapping layer, denoted as CNN e , RNN e and f e . By learning the mapping of images and sentences into one semantic embedding space, it provides a measure of similarity between images and sentences. Given a sentence S, its embedding feature is represented using the last hidden state of RNN e , i.e., h T (S). Let v denote the feature vector of image I extracted by CNN e , and f e (\u00b7) is the mapping function from image features to the embedding space. We train the embedding model using the same image-sentence pairs as in image captioning. We fix the CNN e weight, and learn the RNN e weights as well as f e (\u00b7) using a bidirectional ranking loss defined as follows:\nLe = v S \u2212 max(0, \u03b2 \u2212fe(v) \u00b7 h T (S)+fe(v) \u00b7 h T (S \u2212 )) + S v \u2212 max(0, \u03b2 \u2212h T (S) \u00b7 fe(v)+h T (S) \u00b7 fe(v \u2212 )) (6)\nwhere \u03b2 is the margin cross-validated, every (v, S) are a ground truth image-sentence pair, S \u2212 denotes a negative description for the image corresponding to v, and vice-versa with v \u2212 .\n\nGiven an image with feature v * , we define the reward of a generated sentence S to be the embedding similarity between S and v * :\nr = f e (v * ) \u00b7 h T ( S) f e (v * ) h T ( S)(7)\n\nTraining using deep reinforcement learning\n\nFollowing [38], we learn p \u03c0 and v \u03b8 in two steps. In the first step, we train the policy network p \u03c0 using standard supervised learning with cross entropy loss, where the loss function is defined as L p = \u2212log p(w 1 , ..., w T |I; \u03c0) = \u2212 T t=1 log p \u03c0 (a t |s t ). And we train the value network by minimizing the mean squared loss, ||v \u03b8 (s i ) \u2212 r|| 2 where r is the final reward of the generated sentence and s i denotes a randomly selected state in the generating process. For one generated sentence, successive states are strongly correlated, differing by just one word, but the regression target is shared for each entire captioning process. Thus, we randomly sample one single state from each distinct sentence, to prevent overfitting.\n\nIn the second step, we train p \u03c0 and v \u03b8 jointly using deep reinforcement learning (RL). The parameters of our agent are represented by \u0398 = {\u03c0, \u03b8}, and we learn \u0398 by maximizing the total reward the agent can expect when interacting with the environment:\nJ(\u0398) = E s 1...T \u223cp\u03c0 ( T t=1 r t ). As r t = 0 \u2200 0 < t < T and r T = r, J(\u0398) = E s 1...T \u223cp\u03c0 (r).\nMaximizing J exactly is non-trivial since it involves an expectation over the high-dimensional interaction sequences which may involve unknown environment dynamics in turn. Viewing the problem as a partially observable Markov decision process, however, allows us to bring techniques from the RL literature to bear: As shown in [45,40,31], a sample approximation to the gradient is:\n\u2207 \u03c0 J \u2248 T t=1 \u2207 \u03c0 log p \u03c0 (a t |s t ) (r \u2212 v \u03b8 (s t ))(8)\u2207 \u03b8 J = \u2207 \u03b8 v \u03b8 (s t ) (r \u2212 v \u03b8 (s t ))(9)\nHere the value network v \u03b8 serves as a moving baseline. The subtraction with the evaluation of value network leads to a much lower variance estimate of the policy gradient. The quantity r \u2212 v \u03b8 (s t ) used to scale the gradient can be seen as an estimate of the advantage of action a t in state s t . This approach can be viewed as an actor-critic architecture where the policy p \u03c0 is the actor and v \u03b8 is the critic.\n\nHowever, reinforcement learning in image captioning is hard to train, because of the large action space comparing to other decision-making problems. The action space of image captioning is in the order of 10 3 which equals the vocabulary size, while that of visual navigation in [49] is only 4, which indicates four directions to go. To handle this problem, we follow [35] to apply curriculum learning [1] to train our actor-critic model. In order to gradually teach the model to produce stable sentences, we provide training samples with gradually more difficulty: iteratively we fix the first (T \u2212 i \u00d7 \u2206) words with cross entropy loss and let the actor-critic model train with the remaining i \u00d7 \u2206 words, for i = 1, 2, ..., until reinforcement learning is used to train the whole sentence. Please refer to [35] for details.\n\n\nLookahead inference with policy network and value network\n\nOne of the key contributions of the proposed decisionmaking framework over existing framework lies in the inference mechanism. For decision-making problems, the inference is guided by a local guidance and a global guidance, e.g., AlphaGo [38] utilized MCTS to combine both guidances. For image captioning, we propose a novel lookahead inference mechanism that combines the local guidance of policy network and the global guidance of value network. The learned value network provides a lookahead evaluation for each decision, which can complement the policy network and collaboratively generate captions.\n\nBeam Search (BS) is the most prevalent method for decoding in existing image captioning approaches, which stores the top-B highly scoring candidates at each time step. Here B is the beam width. Let us denote the set of B sequences held by BS at time t as W t = {w 1, t , ..., w B, t }, where each sequence are the generated words until then, w b, t = {w b,1 , ..., w b,t }. At each time step t, BS considers all possible single word extensions of these beams, given by the set W t+1 = W t \u00d7 Y, and selects the top-B most scoring extensions as the new beam sequences W t+1 :\nW t+1 = argtopB w b, t+1 \u2208Wt+1 S(w b, t+1 ), s.t. w i, t+1 = w j, t+1\nwhere operator argtopB denotes the obtaining top-B operation that is implemented by sorting the B \u00d7 |Y| members of W t+1 , and S(\u00b7) denotes the scoring function of a generated sequence. In existing BS of image captioning, S(\u00b7) is the log-probability of the generated sequence. However, such scoring function may miss good captions because it assumes that the log-probability of every word in a good caption must be among top choices. This is not necessarily true. Analogously, in AlphaGo not every move is with top probability. It is beneficial to sometimes allow some actions with low probability to be selected as long as the final reward is optimized.\n\nTo this end, we propose a lookahead inference that combines the policy network and value network to consider all options in W t+1 . It executes each action by taking both the current policy and the lookahead reward evaluation into consideration, i.e.,\nS(w b, t+1 ) = S({w b, t , w b,t+1 }) (10) = S(w b, t ) + \u03bb log p \u03c0 (a t |s t ) + (1 \u2212 \u03bb) v \u03b8 ({s t , w b,t+1 })\nwhere S(w b, t+1 ) is the score of extending the current sequence w b, t with a word w b,t+1 , log p \u03c0 (a t |s t ) denotes the confidence of policy network to predict w b,t+1 as extension, and v \u03b8 ({s t , w b,t+1 }) denotes the evaluation of value network for the state supposing w b,t+1 is generated. 0 \u2264 \u03bb \u2264 1 is a hyperparameter combining policy and value network that we will analyze experimentally in Section 4.5.\n\n\nExperiments\n\nIn this section, we perform extensive experiments to evaluate the proposed framework. All the reported results are computed using Microsoft COCO caption evaluation tool [2], including the metrics BLEU, Meteor, Rouge-L and CIDEr, which are commonly used together for fair and thorough performance measure. Firstly, we discuss the dataset and implementation details. Then we compare the proposed method with state-of-the-art approaches on image captioning. Finally, we conduct detailed analyses on our method.\n\n\nDataset and implementation details\n\nDataset We evaluate our method on the widely used MS COCO dataset [29] for the image captioning task. For fair comparison, we adopt the commonly used splits proposed in [17], which use 82,783 images for training, 5,000 images for validation, and 5,000 images for testing. Each image is given at least five captions by different AMT workers. We follow [17] to preprocess the captions (i.e. building dictionaries, tokenizing the captions).\n\nNetwork architecture As shown in Figure 2 and 3, our policy network, value network both contain a CNN and a RNN. We adopt the same CNN and RNN architectures for them, but train them independently. We use VGG-16 [39] as our CNN architecture and LSTM [14] as our RNN architecture. The input node dimension and the hidden state dimension of LSTM are both set to be 512, i.e., m = n = 512. There are many CNN, RNN architectures in the literature, e.g., ResNet [12], GRU [5], etc. Some of them have reported better performance than the ones we use. We do not use the latest architecture for fair comparison with existing methods. In our value network, we use a three-layer MLP that regresses to a scalar reward value, with a 1024-dim and a 512-dim hidden layers in between. In Figure 3, a state s t is represented by concatenating the visual and semantic features. The visual feature is a 512-dim embedded feature, mapped from the 4096-dim CNN v output. The semantic feature is the 512-dim hidden state of RNN v at the last time step. Thus, the dimension of s t is 1024.\n\nVisual-semantic embedding Visual-semantic embedding can measure the similarity between images and sentences by mapping them to the same space. We followed [19] to use VGG-16 [39] as CNN e and GRU [5] as RNN e . The image feature v in Equation 6 is extracted from the last 4096-dim layer of VGG-16. The input node dimension and the hidden state dimension of GRU are set as 300 and 1024. f e (\u00b7) is a 4096\u00d71024 linear mapping layer. The margin \u03b2 in Equation 6 is set as 0.2.\n\nTraining details In training, we use Adam [18] algorithm to do model updating. It is worth noting that, other than using the pretrained VGG-16 model, we only use the images and captions provided in the dataset to train our networks and embedding, without any external data. We set \u2206 in curriculum learning as 2. In testing, a caption is formed by drawing words sequentially until a special end token is reached, using the proposed lookahead inference mechanism. We do not use ensemble of models.\n\n\nComparing with state-of-the-art methods\n\nIn Table 1, we provide a summary of the results of our method and existing methods. We obtain state-of-theart performance on MS COCO in most evaluation metrics. Note that Semantic ATT [48] utilized rich extra data from social media to train their visual attribute predictor, and  Table 2. Performance of the variants of our method on MS COCO dataset, with beam size = 10. SL: supervised learning baseline. SL-Embed: SL with embedding. SL-RawVN: SL plus pretrained raw value network. hid-VN: value network directly utilizes policy hidden state. hid-Im-VN: value network utilizes policy hidden state and policy image feature. Full-model: our full model. DCC [13] utilized external data to prove its unique transfer capacity. It makes their results incomparable to other methods that do not use extra training data. Surprisingly, even without external training data, our method outperforms [48,13]. Comparing to methods other than [48,13], our approach shows significant improvements in all the metrics except Bleu-1 in which our method ranks the second. Bleu-1 is related to single word accuracy, the performance gap of Bleu-1 between our method and [46] may be due to different preprocessing for word vocabularies. MIXER [35] is a metric-driven trained method. A model trained with Bleu-4 using [35] is hard to generalize to other metrics. Our embedding-driven decision-making approach performs well in all metrics. Especially, considering our policy network shown in Figure 2 is based on a mechanism similar to the very basic image captioning model similar to Google NIC [44], such significant improvement over [44] validates the effectiveness of the proposed decision-making framework that utilizes both policy and value networks. Moreover, the proposed framework is modular w.r.t. the network design. Other powerful mechanisms such as spatial attention, semantic attention can be directly integrated into our policy network and further improve our performance.\n\nSince the proposed embedding-driven decision-making framework is very different from existing methods, we want to perform insightful analyses and answer the following questions: 1) How powerful is embedding? Is the performance gain more because of the framework or embedding alone? 2) How important is lookahead inference? 3) How important is reinforcement learning in the framework? 4) Why the value network is designed as in Figure 3? 5) How sensitive is the method to hyperparameter \u03bb and beam size? To answer those questions, we conduct detailed analyses in the following three sections.\n\n\nHow much each component contributes?\n\nIn this section, we answer questions 1) 2) 3) above. As discussed in Section 3.3, we train our policy and value networks in two steps: pretraining and then reinforcement learning. We name the initial policy network pretrained with supervised learning as (SL). We name the initial value network pretrained with mean squared loss as (RawVN). The SL model can be served as our baseline, which does not use value network or lookahead inference. To evaluate the impact of embedding, we incorporate SL with embedding as follows: in the last step of beam search of SL, when a beam of candidate captions are generated, we rank those candidates according to their embedding similarities with the test image other than their log-probabilities, and finally output the one with highest embedding score. This baseline is named as (SL-Embed). To validate the contribution of lookahead inference and reinforcement learning, we construct a baseline that use SL and RawVN with the proposed lookahead inference, which is named as (SL-RawVN). Finally our full model is named as (Full-model).\n\nAccording to the results of those variants of our method  shown in Table 2, we can answer the questions 1)-3) above: 1. Using embedding alone, SL-Embed performs slightly better than the SL baseline. However, the gap between SL-Embed and Full-model is very big. Therefore, we conclude that using embedding alone is not powerful. The proposed embedding-driven decision-making framework is the merit of our method. 2. By using lookahead inference, SL-RawVN is much better than the SL baseline. This validates the importance of the proposed lookahead inference that utilizes both local and global guidance. 3. After reinforcement learning, our Full-model performs better than the SL-RawVN. This validates the importance of using embedding-driven actor-critic learning for model training. We show some qualitative captioning results of our method and the SL baseline in Figure 4. GT stands for ground truth caption. In the first three columns, we compare our method and SL baseline. As we see, our method is better at recognizing key objects that are easily missed by SL, e.g., the snowboard and umbrellas in the first column images. Besides, our method can reduce the chance of generating incorrect word and accumulating errors, e.g., we generate the word eating other than sitting for the image in the lower second column. Moreover, thanks to the global guidance, our method is better at generating correct captions at global level, e.g., we can recognize the airplane and painting for the images in the third column. Finally, we show two failure cases of our method in the last column, in which cases we fail to understand some important visual contents that only take small portions of the images. This may be due to our policy network architecture. Adding more detailed visual modeling techniques such as detection and attention can alleviate such problem in the future.\n\n\nValue network architecture analysis\n\nIn this paper we propose a novel framework that involves value network, whose architecture is worth looking into. As in Figure 3, we use CNN v and RNN v to extract visual and semantic information from the raw image and sentence inputs. Since the hidden state in policy network at each time step is a representation of each state as well, a natural question is \"can we directly utilize the policy hidden state?\". To answer this question, we construct two variants of our value network: the first one, named as (hid-VN), is comprised of a MLP v on top of the policy hidden state of RNN p ; the second variant, (hid-Im-VN), is comprised of a MLP v on top of the concatenation of the policy hidden state of RNN p and the visual input x 0 of policy RNN p . The results are shown in Table 2. As we see, both variants that utilize policy hidden state do not work well, comparing to our Full-model. The problem of the policy hidden state is that it compresses and also loses lots of information. Thus, it is reasonable and better to train independent CNN, RNN for value network it- self with raw image and sentence inputs, as in Figure 3.\n\n\nParameter sensitivity analysis\n\nThere are two major hyperparameters in our method, \u03bb in Equation 10 and the beam size. In this section, we analyze their sensitivity to answer question 5) above.\n\nIn Table 3, we show the evaluation of \u03bb's impact on our method. As in Equation 10, \u03bb is a hyperparameter combining policy and value networks in lookahead inference, 0 \u2264 \u03bb \u2264 1. \u03bb = 0 means we only use value network to guide our lookahead inference; while \u03bb = 1 means we only use policy network, which is identical to beam search. As shown in Table 3, the best performance is when \u03bb = 0.4. As \u03bb goes down from 0.4 to 0 or goes up from 0.4 to 1, overall the performance drops monotonically. This validates the importance of both networks; we should not emphasize too much on either network in lookahead inference. Besides, \u03bb = 0 performs much worse than \u03bb = 1. This is because policy network provides local guidance, which is very important in sequential prediction. Thus, in lookahead inference, it is too weak if we only use global guidance, i.e. value network in our approach.\n\nIn Table 4, we provide the evaluation of different beam sizes' impact on SL baseline and our full model. As discovered in previous work such as [17], the image captioning performance becomes worse as the beam size gets larger. We validate such discovery for existing encoder-decoder framework. As shown in the upper half of Table 4, we test our SL baseline with 5 different beam sizes from 5 to 100.\n\nNote that SL is based on beam search, which follows the encoder-decoder framework as most existing approaches. As we see, the impact of beam size on SL is relatively big. It's mainly because that as we increase the beam size, bad word candidates are more likely to be drawn into the beam, since the confidence provided by the sequential word generator is only consider local information.\n\nOn the other hand, as shown in the lower part of Table 4, our method is less sensitive to beam sizes. The performance variations between different beam sizes are fairly small. We argue that this is because of the proposed lookahead inference that considers both policy and value networks. With local and global guidances, our framework is more robust and stable to policy mistakes.\n\n\nConclusion\n\nIn this work, we present a novel decision-making framework for image captioning, which achieves state-of-the-art performance on standard benchmark. Different from previous encoder-decoder framework, our method utilizes a policy network and a value network to generate captions. The policy network serves as a local guidance and the value network serves as a global and lookahead guidance. To learn both networks, we use an actor-critic reinforcement learning approach with novel visual-semantic embedding rewards. We conduct detailed analyses on our framework to understand its merits and properties. Our future works include improving network architectures and investigating the reward design by considering other embedding measures.\n\nFigure 2 .\n2An illustration of our policy network p\u03c0 that is comprised of a CNN and a RNN. The CNNp output is fed as the initial input of RNNp. The policy network computes the probability of executing an action at at a certain state st, by p\u03c0(at|st).\n\nFigure 3 .\n3An illustration of our value network v \u03b8 that is comprised of a CNN, a RNN and a MLP. Given a state st which contains raw image input I and a partially generated raw sentence until t, the value network v \u03b8 (st) evaluates its value.\n\nFigure 4 .\n4Qualitative results of our method and the supervised learning (SL) baseline. In the first three columns, our method generates better captions than SL. We show two failure cases in the last column. GT stands for ground truth caption.\n\n\nPerformance of our method on MS COCO dataset, comparing with state-of-the-art methods. Our beam size is set to 10. For those competing methods, we show the results from their latest version of paper. The numbers in bold face are the best known results and (\u2212) indicates unknown scores. ( * ) indicates that external data was used for training in these methods.Methods \n\nBleu-1 Bleu-2 Bleu-3 Bleu-4 METEOR Rouge-L CIDEr \nGoogle NIC [44] \n0.666 \n0.461 \n0.329 \n0.246 \n\u2212 \n\u2212 \n\u2212 \nm-RNN [30] \n0.67 \n0.49 \n0.35 \n0.25 \n\u2212 \n\u2212 \n\u2212 \nBRNN [17] \n0.642 \n0.451 \n0.304 \n0.203 \n\u2212 \n\u2212 \n\u2212 \nLRCN [7] \n0.628 \n0.442 \n0.304 \n0.21 \n\u2212 \n\u2212 \n\u2212 \nMSR/CMU [3] \n\u2212 \n\u2212 \n\u2212 \n0.19 \n0.204 \n\u2212 \n\u2212 \nSpatial ATT [46] \n0.718 \n0.504 \n0.357 \n0.25 \n0.23 \n\u2212 \n\u2212 \ngLSTM [15] \n0.67 \n0.491 \n0.358 \n0.264 \n0.227 \n\u2212 \n0.813 \nMIXER [35] \n\u2212 \n\u2212 \n\u2212 \n0.29 \n\u2212 \n\u2212 \n\u2212 \nSemantic ATT [48]  *  \n0.709 \n0.537 \n0.402 \n0.304 \n0.243 \n\u2212 \n\u2212 \nDCC [13]  *  \n0.644 \n\u2212 \n\u2212 \n\u2212 \n0.21 \n\u2212 \n\u2212 \nOurs \n0.713 \n0.539 \n0.403 \n0.304 \n0.251 \n0.525 \n0.937 \nTable 1. Methods \nBleu-1 Bleu-2 Bleu-3 Bleu-4 METEOR Rouge-L CIDEr \nSL \n0.692 \n0.519 \n0.384 \n0.289 \n0.237 \n0.512 \n0.872 \nSL-Embed \n0.7 \n0.523 \n0.383 \n0.280 \n0.241 \n0.514 \n0.888 \nSL-RawVN \n0.706 \n0.533 \n0.395 \n0.298 \n0.243 \n0.52 \n0.916 \nhid-VN \n0.603 \n0.429 \n0.292 \n0.197 \n0.2 \n0.467 \n0.69 \nhid-Im-VN \n0.611 \n0.435 \n0.297 \n0.201 \n0.202 \n0.468 \n0.701 \nFull-model \n0.713 \n0.539 \n0.403 \n0.304 \n0.251 \n0.525 \n0.937 \n\n\n\nBleu-1 Bleu-2 Bleu-3 Bleu-4 METEOR Rouge-L CIDEr . Evaluation of hyperparameter \u03bb's impact on our method.Method Beam size Bleu-1 Bleu-2 Bleu-3 Bleu-4 METEOR Rouge-L CIDEr . Evaluation of different beam sizes' impact on SL baseline and our method.\u03bb \n\n0 \n0.638 \n0.471 \n0.34 \n0.247 \n0.233 \n0.501 \n0.8 \n0.1 \n0.683 \n0.51 \n0.373 \n0.274 \n0.248 \n0.516 \n0.894 \n0.2 \n0.701 \n0.527 \n0.389 \n0.288 \n0.248 \n0.521 \n0.922 \n0.3 \n0.71 \n0.535 \n0.398 \n0.298 \n0.251 \n0.524 \n0.934 \n0.4 \n0.713 \n0.539 \n0.403 \n0.304 \n0.247 \n0.525 \n0.937 \n0.5 \n0.71 \n0.538 \n0.402 \n0.304 \n0.246 \n0.524 \n0.934 \n0.6 \n0.708 \n0.535 \n0.399 \n0.301 \n0.245 \n0.522 \n0.923 \n0.7 \n0.704 \n0.531 \n0.395 \n0.297 \n0.243 \n0.52 \n0.912 \n0.8 \n0.7 \n0.526 \n0.392 \n0.295 \n0.241 \n0.518 \n0.903 \n0.9 \n0.698 \n0.524 \n0.389 \n0.293 \n0.24 \n0.516 \n0.895 \n1 \n0.694 \n0.52 \n0.385 \n0.289 \n0.238 \n0.513 \n0.879 \nTable 3SL \n\n5 \n0.696 \n0.522 \n0.388 \n0.29 \n0.238 \n0.513 \n0.876 \n10 \n0.692 \n0.519 \n0.384 \n0.289 \n0.237 \n0.512 \n0.872 \n25 \n0.683 \n0.508 \n0.374 \n0.281 \n0.234 \n0.505 \n0.853 \n50 \n0.680 \n0.505 \n0.372 \n0.279 \n0.233 \n0.503 \n0.850 \n100 \n0.679 \n0.504 \n0.372 \n0.279 \n0.233 \n0.503 \n0.849 \n\nOurs \n\n5 \n0.711 \n0.538 \n0.403 \n0.302 \n0.251 \n0.524 \n0.934 \n10 \n0.713 \n0.539 \n0.403 \n0.304 \n0.251 \n0.525 \n0.937 \n25 \n0.709 \n0.534 \n0.398 \n0.299 \n0.248 \n0.522 \n0.928 \n50 \n0.708 \n0.533 \n0.397 \n0.298 \n0.247 \n0.52 \n0.924 \n100 \n0.707 \n0.531 \n0.395 \n0.297 \n0.244 \n0.52 \n0.92 \nTable 4\n\nCurriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, ICML. Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009. 4\n\nMicrosoft coco captions: Data collection and evaluation server. X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Dollar, C L Zitnick, arXiv:1504.00325X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick. Microsoft coco captions: Data collection and evalua- tion server. In arXiv:1504.00325, 2015. 5\n\nMind's eye: A recurrent visual representation for image caption generation. X Chen, C L Zitnick, CVPR. 6X. Chen and C. L. Zitnick. Mind's eye: A recurrent visual represen- tation for image caption generation. In CVPR, 2015. 1, 2, 6\n\nLearning phrase representations using rnn encoderdecoder for statistical machine translation. K Cho, B Van Merrienboer, C Gulcehre, F Bougares, H Schwenk, Y Bengio, EMNLP. 2K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder- decoder for statistical machine translation. In EMNLP, 2014. 2\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.355525J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evalua- tion of gated recurrent neural networks on sequence modeling. In arXiv:1412.3555, 2014. 2, 5\n\nImagenet: a large-scale hierachical image database. J Deng, W Dong, R Socher, L Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: a large-scale hierachical image database. In CVPR, 2009. 2\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L A Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, CVPR. 16J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venu- gopalan, K. Saenko, and T. Darrell. Long-term recurrent convolu- tional networks for visual recognition and description. In CVPR, 2015. 1, 2, 6\n\nImage description using visual dependency representations. D Elliott, F Keller, EMNLP. 2D. Elliott and F. Keller. Image description using visual dependency representations. In EMNLP, 2013. 2\n\nFrom from captions to visual concepts and back. H Fang, S Gupta, F Iandola, R K Srivastava, L Deng, P Dollar, J Gao, X He, M Mitchell, J C Platt, C L Zitnick, G Zweig, CVPR. H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zitnick, and G. Zweig. From from captions to visual concepts and back. In CVPR, 2015. 2\n\nEvery picture tells a story: Generating sentences from images. A Farhadi, M Hejrati, M A Sadeghi, P Young, C Rashtchian, J Hockenmaier, D Forsyth, ECCV. A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Gen- erating sentences from images. In ECCV, 2010. 2\n\nDevise: A deep visual-semantic embedding model. A Frome, G Corrado, J Shlens, S Bengio, J Dean, M Ranzato, T Mikolov, NIPS. 24A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. Devise: A deep visual-semantic embedding model. In NIPS, 2013. 2, 4\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 5\n\nDeep compositional captioning: Describing novel object categories without paired training data. L A Hendricks, S Venugopalan, M Rohrbach, R Mooney, K Saenko, T Darrell, CVPR. L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and T. Darrell. Deep compositional captioning: Describ- ing novel object categories without paired training data. In CVPR, 2016. 6\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 95S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9:1735-1780, 1997. 2, 5\n\nGuiding the longshort term memory model for image caption generation. X Jia, E Gavves, B Fernando, T Tuytelaars, ICCV. 6X. Jia, E. Gavves, B. Fernando, and T. Tuytelaars. Guiding the long- short term memory model for image caption generation. In ICCV, 2015. 1, 2, 6\n\nDensecap: Fully convolutional localization networks for dense captioning. J Johnson, A Karpathy, L Fei-Fei, CVPR. J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully convolu- tional localization networks for dense captioning. In CVPR, 2016. 2\n\nDeep visual-semantic alignments for generating image descriptions. A Karpathy, L Fei-Fei, CVPR. 6A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. 1, 2, 5, 6, 8\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, ICLR. D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5\n\nUnifying visualsemantic embeddings with multimodal neural language models. R Kiros, R Salakhutdinov, R S Zemel, TACL. 5R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying visual- semantic embeddings with multimodal neural language models. In TACL, 2015. 2, 4, 5\n\nShifts in selective visual attention: towards the underlying neural circuitry. Matters of intelligence. C Koch, S Ullman, C. Koch and S. Ullman. Shifts in selective visual attention: towards the underlying neural circuitry. Matters of intelligence, pages 115- 141, 1987. 2\n\nActor-critic algorithms. V Konda, J Tsitsiklis, NIPS. V. Konda and J. Tsitsiklis. Actor-critic algorithms. In NIPS, 1999. 2\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 2\n\nBaby talk: Understanding and generating image descriptions. G Kulkarni, V Premraj, S Dhar, S Li, Y Choi, A C Berg, T L Berg, CVPR. G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating image descriptions. In CVPR, 2011. 2\n\nCollective generation of natural image descriptions. P Kuznetsova, V Ordonez, A C Berg, T L Berg, Y Choi, ACL. P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi. Collective generation of natural image descriptions. In ACL, 2012. 2\n\nThe meteor metric for automatic evaluation of machine translation. Machine Translation. A Lavie, M Denkowski, A. Lavie and M. Denkowski. The meteor metric for automatic eval- uation of machine translation. Machine Translation, 2010. 2\n\nSimple image description generator via a linear phrase-based approach. R Lebret, P O Pinheiro, R Collobert, ICLR. R. Lebret, P. O. Pinheiro, and R. Collobert. Simple image description generator via a linear phrase-based approach. In ICLR, 2015. 2\n\nComposing simple image descriptions using web-scale n-grams. S Li, G Kulkarni, T L Berg, A C Berg, Y Choi, CoNLL. S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Compos- ing simple image descriptions using web-scale n-grams. In CoNLL, 2011. 2\n\nRouge: A package for automatic evaluation of summaries. C.-Y. Lin, WAS. C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In WAS, 2004. 2\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, L Bourdev, R Girshick, J Hays, P Perona, D Ramanan, C L Zitnick, P Dollar, ECCV. 25T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar. Microsoft coco: Common objects in context. In ECCV, 2014. 2, 5\n\nDeep captioning with multimodal recurrent neural networks. J Mao, W Xu, Y Yang, J Wang, A Yuille, ICLR. 6J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille. Deep captioning with multimodal recurrent neural networks. In ICLR, 2015. 1, 2, 6\n\nAsynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, ICML. 24V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016. 2, 4\n\nHuman-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, Nature. 5182V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. G. Belle- mare, A. Graves, M. Riedmiller, A. Fidjeland, G. Ostrovski, S. Pe- tersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015. 2\n\nJointly modeling embedding and translation to bridge video and language. Y Pan, T Mei, T Yao, H Li, Y Rui, CVPR. Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. Jointly modeling embed- ding and translation to bridge video and language. In CVPR, 2016. 4\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W J Zhu, ACL. K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 2\n\nSequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, ICLR. 6M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016. 2, 4, 6\n\nMulti-instance visualsemantic embedding. Z Ren, H Jin, Z Lin, C Fang, A Yuille, arXiv:1512.0696324Z. Ren, H. Jin, Z. Lin, C. Fang, and A. Yuille. Multi-instance visual- semantic embedding. In arXiv:1512.06963, 2015. 2, 4\n\nJoint image-text representation by gaussian visual-semantic embedding. Z Ren, H Jin, Z Lin, C Fang, A Yuille, ACM Multimedia. 24Z. Ren, H. Jin, Z. Lin, C. Fang, and A. Yuille. Joint image-text rep- resentation by gaussian visual-semantic embedding. In ACM Multi- media, 2016. 2, 4\n\nMastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, D Grewe, J Nham, N Kalchbrenner, I Sutskever, T Lillicrap, M Leach, K Kavukcuoglu, T Graepel, D Hassabis, Nature. 5294D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484-489, 2016. 2, 4\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. 25K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 2, 5\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D Mcallester, S Singh, Y Mansour, NIPS. 24R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradi- ent methods for reinforcement learning with function approximation. In NIPS, 2000. 2, 4\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er- han, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu- tions. In CVPR, 2015. 2\n\nCider: Consensus-based image description evaluation. R Vedantam, C L Zitnick, D Parikh, CVPR. R. Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 2\n\nDiverse beam search: Decoding diverse solutions from neural sequence models. A Vijayakumar, M Cogswell, R R Selvaraju, Q Sun, S Lee, D Crandall, D Batra, arXiv:1610.024241A. Vijayakumar, M. Cogswell, R. R. Selvaraju, Q. Sun, S. Lee, D. Crandall, and D. Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. In arXiv:1610.02424, 2016. 1, 2\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, CVPR. 6O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015. 1, 2, 3, 6\n\nsimple statistical gradient-following algorithms for connectionist reinforcement learning. R Williams, Machine Learning. 84R. Williams. simple statistical gradient-following algorithms for con- nectionist reinforcement learning. Machine Learning, 8:229-256, 1992. 2, 4\n\nShow, attend and tell: Neural image caption generation with visual attention. K Xu, J L Ba, R Kiros, K Cho, A Courville, R Salakhutdinov, R S Zemel, Y Bengio, ICML. 16K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015. 1, 2, 6\n\nCorpus-guided sentence generation of natural images. Y Yang, C L Teo, H Daume, Iii , Y Aloimonos, EMNLP. 2Y. Yang, C. L. Teo, H. Daume III, and Y. Aloimonos. Corpus-guided sentence generation of natural images. In EMNLP, 2011. 2\n\nImage captioning with semantic attention. Q You, H Jin, Z Wang, C Fang, J Luo, CVPR. 6Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image captioning with semantic attention. In CVPR, 2016. 1, 2, 5, 6\n\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, arXiv:1609.0514324Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, and A. Gupta. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In arXiv:1609.05143, 2016. 2, 4\n", "annotations": {"author": "[{\"end\":114,\"start\":76},{\"end\":159,\"start\":115},{\"end\":202,\"start\":160},{\"end\":241,\"start\":203},{\"end\":289,\"start\":242}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":126,\"start\":122},{\"end\":170,\"start\":165},{\"end\":211,\"start\":209},{\"end\":251,\"start\":249}]", "author_first_name": "[{\"end\":80,\"start\":76},{\"end\":121,\"start\":115},{\"end\":164,\"start\":160},{\"end\":208,\"start\":203},{\"end\":248,\"start\":242}]", "author_affiliation": "[{\"end\":113,\"start\":104},{\"end\":158,\"start\":149},{\"end\":201,\"start\":192},{\"end\":240,\"start\":231},{\"end\":288,\"start\":277}]", "title": "[{\"end\":73,\"start\":1},{\"end\":362,\"start\":290}]", "venue": null, "abstract": "[{\"end\":1659,\"start\":364}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2059,\"start\":2056},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2062,\"start\":2059},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2065,\"start\":2062},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2068,\"start\":2065},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2070,\"start\":2068},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2073,\"start\":2070},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2076,\"start\":2073},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2079,\"start\":2076},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2082,\"start\":2079},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4600,\"start\":4596},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4621,\"start\":4617},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4860,\"start\":4856},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4912,\"start\":4908},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4915,\"start\":4912},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4918,\"start\":4915},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4921,\"start\":4918},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5313,\"start\":5309},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5453,\"start\":5449},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5466,\"start\":5462},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5478,\"start\":5474},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5493,\"start\":5489},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6278,\"start\":6274},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6281,\"start\":6278},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6284,\"start\":6281},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6287,\"start\":6284},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6290,\"start\":6287},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6292,\"start\":6290},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6295,\"start\":6292},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6297,\"start\":6295},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6528,\"start\":6525},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6563,\"start\":6560},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6566,\"start\":6563},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6569,\"start\":6566},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6572,\"start\":6569},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6574,\"start\":6572},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6577,\"start\":6574},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6580,\"start\":6577},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6583,\"start\":6580},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6586,\"start\":6583},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6882,\"start\":6878},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6884,\"start\":6882},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6887,\"start\":6884},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6890,\"start\":6887},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6952,\"start\":6948},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6954,\"start\":6952},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7065,\"start\":7061},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7198,\"start\":7194},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7222,\"start\":7218},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7250,\"start\":7246},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7348,\"start\":7344},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7445,\"start\":7441},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8026,\"start\":8022},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8047,\"start\":8043},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8082,\"start\":8078},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8262,\"start\":8258},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8265,\"start\":8262},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8268,\"start\":8265},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8271,\"start\":8268},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8565,\"start\":8561},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8653,\"start\":8649},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8723,\"start\":8719},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8908,\"start\":8904},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8932,\"start\":8928},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9042,\"start\":9038},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9268,\"start\":9264},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9309,\"start\":9305},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9312,\"start\":9309},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9455,\"start\":9451},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9534,\"start\":9530},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11292,\"start\":11288},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13548,\"start\":13544},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13551,\"start\":13548},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13567,\"start\":13563},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13570,\"start\":13567},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13573,\"start\":13570},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14855,\"start\":14851},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16269,\"start\":16265},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16272,\"start\":16269},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16275,\"start\":16272},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17122,\"start\":17118},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17211,\"start\":17207},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17244,\"start\":17241},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17650,\"start\":17646},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17967,\"start\":17963},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20601,\"start\":20598},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21045,\"start\":21041},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21148,\"start\":21144},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21330,\"start\":21326},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21629,\"start\":21625},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21667,\"start\":21663},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21874,\"start\":21870},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21883,\"start\":21880},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22640,\"start\":22636},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22659,\"start\":22655},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22680,\"start\":22677},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23001,\"start\":22997},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23682,\"start\":23678},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24154,\"start\":24150},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24385,\"start\":24381},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24388,\"start\":24385},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24426,\"start\":24422},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24429,\"start\":24426},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24646,\"start\":24642},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24718,\"start\":24714},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24792,\"start\":24788},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25069,\"start\":25065},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25109,\"start\":25105},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31428,\"start\":31424}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":33452,\"start\":33201},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33697,\"start\":33453},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33943,\"start\":33698},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35321,\"start\":33944},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36706,\"start\":35322}]", "paragraph": "[{\"end\":2019,\"start\":1675},{\"end\":2949,\"start\":2021},{\"end\":4203,\"start\":2951},{\"end\":5168,\"start\":4205},{\"end\":5553,\"start\":5170},{\"end\":5848,\"start\":5555},{\"end\":6104,\"start\":5850},{\"end\":7491,\"start\":6140},{\"end\":7947,\"start\":7493},{\"end\":8797,\"start\":7967},{\"end\":9583,\"start\":8799},{\"end\":9892,\"start\":9638},{\"end\":10546,\"start\":9916},{\"end\":10875,\"start\":10573},{\"end\":11876,\"start\":10894},{\"end\":12192,\"start\":12060},{\"end\":12498,\"start\":12210},{\"end\":13189,\"start\":12538},{\"end\":13462,\"start\":13237},{\"end\":14311,\"start\":13464},{\"end\":14613,\"start\":14427},{\"end\":14746,\"start\":14615},{\"end\":15584,\"start\":14841},{\"end\":15839,\"start\":15586},{\"end\":16319,\"start\":15938},{\"end\":16837,\"start\":16420},{\"end\":17663,\"start\":16839},{\"end\":18328,\"start\":17725},{\"end\":18903,\"start\":18330},{\"end\":19628,\"start\":18974},{\"end\":19881,\"start\":19630},{\"end\":20413,\"start\":19995},{\"end\":20936,\"start\":20429},{\"end\":21412,\"start\":20975},{\"end\":22479,\"start\":21414},{\"end\":22953,\"start\":22481},{\"end\":23450,\"start\":22955},{\"end\":25456,\"start\":23494},{\"end\":26049,\"start\":25458},{\"end\":27162,\"start\":26090},{\"end\":29034,\"start\":27164},{\"end\":30204,\"start\":29074},{\"end\":30400,\"start\":30239},{\"end\":31278,\"start\":30402},{\"end\":31679,\"start\":31280},{\"end\":32068,\"start\":31681},{\"end\":32451,\"start\":32070},{\"end\":33200,\"start\":32466}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11947,\"start\":11877},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12059,\"start\":11947},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12537,\"start\":12499},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14426,\"start\":14312},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14795,\"start\":14747},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15937,\"start\":15840},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16377,\"start\":16320},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16419,\"start\":16377},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18973,\"start\":18904},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19994,\"start\":19882}]", "table_ref": "[{\"end\":23504,\"start\":23497},{\"end\":23781,\"start\":23774},{\"end\":27238,\"start\":27231},{\"end\":29858,\"start\":29851},{\"end\":30412,\"start\":30405},{\"end\":30750,\"start\":30743},{\"end\":31290,\"start\":31283},{\"end\":31611,\"start\":31604},{\"end\":32126,\"start\":32119}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1673,\"start\":1661},{\"attributes\":{\"n\":\"2.\"},\"end\":6119,\"start\":6107},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6138,\"start\":6122},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7965,\"start\":7950},{\"attributes\":{\"n\":\"3.\"},\"end\":9636,\"start\":9586},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9914,\"start\":9895},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":10571,\"start\":10549},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":10892,\"start\":10878},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":12208,\"start\":12195},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13235,\"start\":13192},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14839,\"start\":14797},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17723,\"start\":17666},{\"attributes\":{\"n\":\"4.\"},\"end\":20427,\"start\":20416},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20973,\"start\":20939},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23492,\"start\":23453},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26088,\"start\":26052},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29072,\"start\":29037},{\"attributes\":{\"n\":\"4.5.\"},\"end\":30237,\"start\":30207},{\"attributes\":{\"n\":\"5.\"},\"end\":32464,\"start\":32454},{\"end\":33212,\"start\":33202},{\"end\":33464,\"start\":33454},{\"end\":33709,\"start\":33699}]", "table": "[{\"end\":35321,\"start\":34306},{\"end\":36706,\"start\":35570}]", "figure_caption": "[{\"end\":33452,\"start\":33214},{\"end\":33697,\"start\":33466},{\"end\":33943,\"start\":33711},{\"end\":34306,\"start\":33946},{\"end\":35570,\"start\":35324}]", "figure_ref": "[{\"end\":2364,\"start\":2356},{\"end\":3800,\"start\":3792},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11351,\"start\":11343},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12698,\"start\":12690},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21455,\"start\":21447},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22194,\"start\":22186},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24969,\"start\":24961},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25893,\"start\":25885},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28037,\"start\":28029},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29202,\"start\":29194},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30203,\"start\":30195}]", "bib_author_first_name": "[{\"end\":36730,\"start\":36729},{\"end\":36740,\"start\":36739},{\"end\":36753,\"start\":36752},{\"end\":36766,\"start\":36765},{\"end\":36939,\"start\":36938},{\"end\":36947,\"start\":36946},{\"end\":36958,\"start\":36954},{\"end\":36965,\"start\":36964},{\"end\":36977,\"start\":36976},{\"end\":36986,\"start\":36985},{\"end\":36996,\"start\":36995},{\"end\":36998,\"start\":36997},{\"end\":37279,\"start\":37278},{\"end\":37287,\"start\":37286},{\"end\":37289,\"start\":37288},{\"end\":37530,\"start\":37529},{\"end\":37537,\"start\":37536},{\"end\":37556,\"start\":37555},{\"end\":37568,\"start\":37567},{\"end\":37580,\"start\":37579},{\"end\":37591,\"start\":37590},{\"end\":37883,\"start\":37882},{\"end\":37892,\"start\":37891},{\"end\":37904,\"start\":37903},{\"end\":37911,\"start\":37910},{\"end\":38148,\"start\":38147},{\"end\":38156,\"start\":38155},{\"end\":38164,\"start\":38163},{\"end\":38174,\"start\":38173},{\"end\":38180,\"start\":38179},{\"end\":38186,\"start\":38185},{\"end\":38415,\"start\":38414},{\"end\":38426,\"start\":38425},{\"end\":38428,\"start\":38427},{\"end\":38441,\"start\":38440},{\"end\":38455,\"start\":38454},{\"end\":38467,\"start\":38466},{\"end\":38482,\"start\":38481},{\"end\":38492,\"start\":38491},{\"end\":38781,\"start\":38780},{\"end\":38792,\"start\":38791},{\"end\":38962,\"start\":38961},{\"end\":38970,\"start\":38969},{\"end\":38979,\"start\":38978},{\"end\":38990,\"start\":38989},{\"end\":38992,\"start\":38991},{\"end\":39006,\"start\":39005},{\"end\":39014,\"start\":39013},{\"end\":39024,\"start\":39023},{\"end\":39031,\"start\":39030},{\"end\":39037,\"start\":39036},{\"end\":39049,\"start\":39048},{\"end\":39051,\"start\":39050},{\"end\":39060,\"start\":39059},{\"end\":39062,\"start\":39061},{\"end\":39073,\"start\":39072},{\"end\":39356,\"start\":39355},{\"end\":39367,\"start\":39366},{\"end\":39378,\"start\":39377},{\"end\":39380,\"start\":39379},{\"end\":39391,\"start\":39390},{\"end\":39400,\"start\":39399},{\"end\":39414,\"start\":39413},{\"end\":39429,\"start\":39428},{\"end\":39673,\"start\":39672},{\"end\":39682,\"start\":39681},{\"end\":39693,\"start\":39692},{\"end\":39703,\"start\":39702},{\"end\":39713,\"start\":39712},{\"end\":39721,\"start\":39720},{\"end\":39732,\"start\":39731},{\"end\":39947,\"start\":39946},{\"end\":39953,\"start\":39952},{\"end\":39962,\"start\":39961},{\"end\":39969,\"start\":39968},{\"end\":40179,\"start\":40178},{\"end\":40181,\"start\":40180},{\"end\":40194,\"start\":40193},{\"end\":40209,\"start\":40208},{\"end\":40221,\"start\":40220},{\"end\":40231,\"start\":40230},{\"end\":40241,\"start\":40240},{\"end\":40482,\"start\":40481},{\"end\":40496,\"start\":40495},{\"end\":40706,\"start\":40705},{\"end\":40713,\"start\":40712},{\"end\":40723,\"start\":40722},{\"end\":40735,\"start\":40734},{\"end\":40977,\"start\":40976},{\"end\":40988,\"start\":40987},{\"end\":41000,\"start\":40999},{\"end\":41219,\"start\":41218},{\"end\":41231,\"start\":41230},{\"end\":41418,\"start\":41417},{\"end\":41428,\"start\":41427},{\"end\":41598,\"start\":41597},{\"end\":41607,\"start\":41606},{\"end\":41624,\"start\":41623},{\"end\":41626,\"start\":41625},{\"end\":41892,\"start\":41891},{\"end\":41900,\"start\":41899},{\"end\":42087,\"start\":42086},{\"end\":42096,\"start\":42095},{\"end\":42252,\"start\":42251},{\"end\":42266,\"start\":42265},{\"end\":42279,\"start\":42278},{\"end\":42482,\"start\":42481},{\"end\":42494,\"start\":42493},{\"end\":42505,\"start\":42504},{\"end\":42513,\"start\":42512},{\"end\":42519,\"start\":42518},{\"end\":42527,\"start\":42526},{\"end\":42529,\"start\":42528},{\"end\":42537,\"start\":42536},{\"end\":42539,\"start\":42538},{\"end\":42762,\"start\":42761},{\"end\":42776,\"start\":42775},{\"end\":42787,\"start\":42786},{\"end\":42789,\"start\":42788},{\"end\":42797,\"start\":42796},{\"end\":42799,\"start\":42798},{\"end\":42807,\"start\":42806},{\"end\":43042,\"start\":43041},{\"end\":43051,\"start\":43050},{\"end\":43261,\"start\":43260},{\"end\":43271,\"start\":43270},{\"end\":43273,\"start\":43272},{\"end\":43285,\"start\":43284},{\"end\":43499,\"start\":43498},{\"end\":43505,\"start\":43504},{\"end\":43517,\"start\":43516},{\"end\":43519,\"start\":43518},{\"end\":43527,\"start\":43526},{\"end\":43529,\"start\":43528},{\"end\":43537,\"start\":43536},{\"end\":43751,\"start\":43746},{\"end\":43893,\"start\":43889},{\"end\":43900,\"start\":43899},{\"end\":43909,\"start\":43908},{\"end\":43921,\"start\":43920},{\"end\":43932,\"start\":43931},{\"end\":43944,\"start\":43943},{\"end\":43952,\"start\":43951},{\"end\":43962,\"start\":43961},{\"end\":43973,\"start\":43972},{\"end\":43975,\"start\":43974},{\"end\":43986,\"start\":43985},{\"end\":44248,\"start\":44247},{\"end\":44255,\"start\":44254},{\"end\":44261,\"start\":44260},{\"end\":44269,\"start\":44268},{\"end\":44277,\"start\":44276},{\"end\":44479,\"start\":44478},{\"end\":44487,\"start\":44486},{\"end\":44489,\"start\":44488},{\"end\":44498,\"start\":44497},{\"end\":44507,\"start\":44506},{\"end\":44517,\"start\":44516},{\"end\":44519,\"start\":44518},{\"end\":44532,\"start\":44531},{\"end\":44542,\"start\":44541},{\"end\":44552,\"start\":44551},{\"end\":44809,\"start\":44808},{\"end\":44817,\"start\":44816},{\"end\":44832,\"start\":44831},{\"end\":44842,\"start\":44841},{\"end\":44850,\"start\":44849},{\"end\":44860,\"start\":44859},{\"end\":44862,\"start\":44861},{\"end\":44875,\"start\":44874},{\"end\":44885,\"start\":44884},{\"end\":44899,\"start\":44898},{\"end\":44912,\"start\":44911},{\"end\":44925,\"start\":44924},{\"end\":44937,\"start\":44936},{\"end\":44948,\"start\":44947},{\"end\":44957,\"start\":44956},{\"end\":44971,\"start\":44970},{\"end\":44979,\"start\":44978},{\"end\":44990,\"start\":44989},{\"end\":45002,\"start\":45001},{\"end\":45010,\"start\":45009},{\"end\":45435,\"start\":45434},{\"end\":45442,\"start\":45441},{\"end\":45449,\"start\":45448},{\"end\":45456,\"start\":45455},{\"end\":45462,\"start\":45461},{\"end\":45675,\"start\":45674},{\"end\":45687,\"start\":45686},{\"end\":45697,\"start\":45696},{\"end\":45705,\"start\":45704},{\"end\":45707,\"start\":45706},{\"end\":45904,\"start\":45903},{\"end\":45915,\"start\":45914},{\"end\":45925,\"start\":45924},{\"end\":45933,\"start\":45932},{\"end\":46120,\"start\":46119},{\"end\":46127,\"start\":46126},{\"end\":46134,\"start\":46133},{\"end\":46141,\"start\":46140},{\"end\":46149,\"start\":46148},{\"end\":46372,\"start\":46371},{\"end\":46379,\"start\":46378},{\"end\":46386,\"start\":46385},{\"end\":46393,\"start\":46392},{\"end\":46401,\"start\":46400},{\"end\":46651,\"start\":46650},{\"end\":46661,\"start\":46660},{\"end\":46670,\"start\":46669},{\"end\":46672,\"start\":46671},{\"end\":46684,\"start\":46683},{\"end\":46692,\"start\":46691},{\"end\":46701,\"start\":46700},{\"end\":46722,\"start\":46721},{\"end\":46739,\"start\":46738},{\"end\":46753,\"start\":46752},{\"end\":46771,\"start\":46770},{\"end\":46782,\"start\":46781},{\"end\":46794,\"start\":46793},{\"end\":46803,\"start\":46802},{\"end\":46811,\"start\":46810},{\"end\":46827,\"start\":46826},{\"end\":46840,\"start\":46839},{\"end\":46853,\"start\":46852},{\"end\":46862,\"start\":46861},{\"end\":46877,\"start\":46876},{\"end\":46888,\"start\":46887},{\"end\":47355,\"start\":47354},{\"end\":47367,\"start\":47366},{\"end\":47587,\"start\":47586},{\"end\":47589,\"start\":47588},{\"end\":47599,\"start\":47598},{\"end\":47613,\"start\":47612},{\"end\":47622,\"start\":47621},{\"end\":47831,\"start\":47830},{\"end\":47842,\"start\":47841},{\"end\":47849,\"start\":47848},{\"end\":47856,\"start\":47855},{\"end\":47868,\"start\":47867},{\"end\":47876,\"start\":47875},{\"end\":47888,\"start\":47887},{\"end\":47897,\"start\":47896},{\"end\":47910,\"start\":47909},{\"end\":48143,\"start\":48142},{\"end\":48155,\"start\":48154},{\"end\":48157,\"start\":48156},{\"end\":48168,\"start\":48167},{\"end\":48375,\"start\":48374},{\"end\":48390,\"start\":48389},{\"end\":48402,\"start\":48401},{\"end\":48404,\"start\":48403},{\"end\":48417,\"start\":48416},{\"end\":48424,\"start\":48423},{\"end\":48431,\"start\":48430},{\"end\":48443,\"start\":48442},{\"end\":48717,\"start\":48716},{\"end\":48728,\"start\":48727},{\"end\":48738,\"start\":48737},{\"end\":48748,\"start\":48747},{\"end\":48979,\"start\":48978},{\"end\":49236,\"start\":49235},{\"end\":49242,\"start\":49241},{\"end\":49244,\"start\":49243},{\"end\":49250,\"start\":49249},{\"end\":49259,\"start\":49258},{\"end\":49266,\"start\":49265},{\"end\":49279,\"start\":49278},{\"end\":49296,\"start\":49295},{\"end\":49298,\"start\":49297},{\"end\":49307,\"start\":49306},{\"end\":49575,\"start\":49574},{\"end\":49583,\"start\":49582},{\"end\":49585,\"start\":49584},{\"end\":49592,\"start\":49591},{\"end\":49603,\"start\":49600},{\"end\":49607,\"start\":49606},{\"end\":49794,\"start\":49793},{\"end\":49801,\"start\":49800},{\"end\":49808,\"start\":49807},{\"end\":49816,\"start\":49815},{\"end\":49824,\"start\":49823},{\"end\":50037,\"start\":50036},{\"end\":50044,\"start\":50043},{\"end\":50056,\"start\":50055},{\"end\":50065,\"start\":50064},{\"end\":50067,\"start\":50066},{\"end\":50074,\"start\":50073}]", "bib_author_last_name": "[{\"end\":36737,\"start\":36731},{\"end\":36750,\"start\":36741},{\"end\":36763,\"start\":36754},{\"end\":36773,\"start\":36767},{\"end\":36944,\"start\":36940},{\"end\":36952,\"start\":36948},{\"end\":36962,\"start\":36959},{\"end\":36974,\"start\":36966},{\"end\":36983,\"start\":36978},{\"end\":36993,\"start\":36987},{\"end\":37006,\"start\":36999},{\"end\":37284,\"start\":37280},{\"end\":37297,\"start\":37290},{\"end\":37534,\"start\":37531},{\"end\":37553,\"start\":37538},{\"end\":37565,\"start\":37557},{\"end\":37577,\"start\":37569},{\"end\":37588,\"start\":37581},{\"end\":37598,\"start\":37592},{\"end\":37889,\"start\":37884},{\"end\":37901,\"start\":37893},{\"end\":37908,\"start\":37905},{\"end\":37918,\"start\":37912},{\"end\":38153,\"start\":38149},{\"end\":38161,\"start\":38157},{\"end\":38171,\"start\":38165},{\"end\":38177,\"start\":38175},{\"end\":38183,\"start\":38181},{\"end\":38194,\"start\":38187},{\"end\":38423,\"start\":38416},{\"end\":38438,\"start\":38429},{\"end\":38452,\"start\":38442},{\"end\":38464,\"start\":38456},{\"end\":38479,\"start\":38468},{\"end\":38489,\"start\":38483},{\"end\":38500,\"start\":38493},{\"end\":38789,\"start\":38782},{\"end\":38799,\"start\":38793},{\"end\":38967,\"start\":38963},{\"end\":38976,\"start\":38971},{\"end\":38987,\"start\":38980},{\"end\":39003,\"start\":38993},{\"end\":39011,\"start\":39007},{\"end\":39021,\"start\":39015},{\"end\":39028,\"start\":39025},{\"end\":39034,\"start\":39032},{\"end\":39046,\"start\":39038},{\"end\":39057,\"start\":39052},{\"end\":39070,\"start\":39063},{\"end\":39079,\"start\":39074},{\"end\":39364,\"start\":39357},{\"end\":39375,\"start\":39368},{\"end\":39388,\"start\":39381},{\"end\":39397,\"start\":39392},{\"end\":39411,\"start\":39401},{\"end\":39426,\"start\":39415},{\"end\":39437,\"start\":39430},{\"end\":39679,\"start\":39674},{\"end\":39690,\"start\":39683},{\"end\":39700,\"start\":39694},{\"end\":39710,\"start\":39704},{\"end\":39718,\"start\":39714},{\"end\":39729,\"start\":39722},{\"end\":39740,\"start\":39733},{\"end\":39950,\"start\":39948},{\"end\":39959,\"start\":39954},{\"end\":39966,\"start\":39963},{\"end\":39973,\"start\":39970},{\"end\":40191,\"start\":40182},{\"end\":40206,\"start\":40195},{\"end\":40218,\"start\":40210},{\"end\":40228,\"start\":40222},{\"end\":40238,\"start\":40232},{\"end\":40249,\"start\":40242},{\"end\":40493,\"start\":40483},{\"end\":40508,\"start\":40497},{\"end\":40710,\"start\":40707},{\"end\":40720,\"start\":40714},{\"end\":40732,\"start\":40724},{\"end\":40746,\"start\":40736},{\"end\":40985,\"start\":40978},{\"end\":40997,\"start\":40989},{\"end\":41008,\"start\":41001},{\"end\":41228,\"start\":41220},{\"end\":41239,\"start\":41232},{\"end\":41425,\"start\":41419},{\"end\":41431,\"start\":41429},{\"end\":41604,\"start\":41599},{\"end\":41621,\"start\":41608},{\"end\":41632,\"start\":41627},{\"end\":41897,\"start\":41893},{\"end\":41907,\"start\":41901},{\"end\":42093,\"start\":42088},{\"end\":42107,\"start\":42097},{\"end\":42263,\"start\":42253},{\"end\":42276,\"start\":42267},{\"end\":42286,\"start\":42280},{\"end\":42491,\"start\":42483},{\"end\":42502,\"start\":42495},{\"end\":42510,\"start\":42506},{\"end\":42516,\"start\":42514},{\"end\":42524,\"start\":42520},{\"end\":42534,\"start\":42530},{\"end\":42544,\"start\":42540},{\"end\":42773,\"start\":42763},{\"end\":42784,\"start\":42777},{\"end\":42794,\"start\":42790},{\"end\":42804,\"start\":42800},{\"end\":42812,\"start\":42808},{\"end\":43048,\"start\":43043},{\"end\":43061,\"start\":43052},{\"end\":43268,\"start\":43262},{\"end\":43282,\"start\":43274},{\"end\":43295,\"start\":43286},{\"end\":43502,\"start\":43500},{\"end\":43514,\"start\":43506},{\"end\":43524,\"start\":43520},{\"end\":43534,\"start\":43530},{\"end\":43542,\"start\":43538},{\"end\":43755,\"start\":43752},{\"end\":43897,\"start\":43894},{\"end\":43906,\"start\":43901},{\"end\":43918,\"start\":43910},{\"end\":43929,\"start\":43922},{\"end\":43941,\"start\":43933},{\"end\":43949,\"start\":43945},{\"end\":43959,\"start\":43953},{\"end\":43970,\"start\":43963},{\"end\":43983,\"start\":43976},{\"end\":43993,\"start\":43987},{\"end\":44252,\"start\":44249},{\"end\":44258,\"start\":44256},{\"end\":44266,\"start\":44262},{\"end\":44274,\"start\":44270},{\"end\":44284,\"start\":44278},{\"end\":44484,\"start\":44480},{\"end\":44495,\"start\":44490},{\"end\":44504,\"start\":44499},{\"end\":44514,\"start\":44508},{\"end\":44529,\"start\":44520},{\"end\":44539,\"start\":44533},{\"end\":44549,\"start\":44543},{\"end\":44564,\"start\":44553},{\"end\":44814,\"start\":44810},{\"end\":44829,\"start\":44818},{\"end\":44839,\"start\":44833},{\"end\":44847,\"start\":44843},{\"end\":44857,\"start\":44851},{\"end\":44872,\"start\":44863},{\"end\":44882,\"start\":44876},{\"end\":44896,\"start\":44886},{\"end\":44909,\"start\":44900},{\"end\":44922,\"start\":44913},{\"end\":44934,\"start\":44926},{\"end\":44945,\"start\":44938},{\"end\":44954,\"start\":44949},{\"end\":44968,\"start\":44958},{\"end\":44976,\"start\":44972},{\"end\":44987,\"start\":44980},{\"end\":44999,\"start\":44991},{\"end\":45007,\"start\":45003},{\"end\":45019,\"start\":45011},{\"end\":45439,\"start\":45436},{\"end\":45446,\"start\":45443},{\"end\":45453,\"start\":45450},{\"end\":45459,\"start\":45457},{\"end\":45466,\"start\":45463},{\"end\":45684,\"start\":45676},{\"end\":45694,\"start\":45688},{\"end\":45702,\"start\":45698},{\"end\":45711,\"start\":45708},{\"end\":45912,\"start\":45905},{\"end\":45922,\"start\":45916},{\"end\":45930,\"start\":45926},{\"end\":45941,\"start\":45934},{\"end\":46124,\"start\":46121},{\"end\":46131,\"start\":46128},{\"end\":46138,\"start\":46135},{\"end\":46146,\"start\":46142},{\"end\":46156,\"start\":46150},{\"end\":46376,\"start\":46373},{\"end\":46383,\"start\":46380},{\"end\":46390,\"start\":46387},{\"end\":46398,\"start\":46394},{\"end\":46408,\"start\":46402},{\"end\":46658,\"start\":46652},{\"end\":46667,\"start\":46662},{\"end\":46681,\"start\":46673},{\"end\":46689,\"start\":46685},{\"end\":46698,\"start\":46693},{\"end\":46719,\"start\":46702},{\"end\":46736,\"start\":46723},{\"end\":46750,\"start\":46740},{\"end\":46768,\"start\":46754},{\"end\":46779,\"start\":46772},{\"end\":46791,\"start\":46783},{\"end\":46800,\"start\":46795},{\"end\":46808,\"start\":46804},{\"end\":46824,\"start\":46812},{\"end\":46837,\"start\":46828},{\"end\":46850,\"start\":46841},{\"end\":46859,\"start\":46854},{\"end\":46874,\"start\":46863},{\"end\":46885,\"start\":46878},{\"end\":46897,\"start\":46889},{\"end\":47364,\"start\":47356},{\"end\":47377,\"start\":47368},{\"end\":47596,\"start\":47590},{\"end\":47610,\"start\":47600},{\"end\":47619,\"start\":47614},{\"end\":47630,\"start\":47623},{\"end\":47839,\"start\":47832},{\"end\":47846,\"start\":47843},{\"end\":47853,\"start\":47850},{\"end\":47865,\"start\":47857},{\"end\":47873,\"start\":47869},{\"end\":47885,\"start\":47877},{\"end\":47894,\"start\":47889},{\"end\":47907,\"start\":47898},{\"end\":47921,\"start\":47911},{\"end\":48152,\"start\":48144},{\"end\":48165,\"start\":48158},{\"end\":48175,\"start\":48169},{\"end\":48387,\"start\":48376},{\"end\":48399,\"start\":48391},{\"end\":48414,\"start\":48405},{\"end\":48421,\"start\":48418},{\"end\":48428,\"start\":48425},{\"end\":48440,\"start\":48432},{\"end\":48449,\"start\":48444},{\"end\":48725,\"start\":48718},{\"end\":48735,\"start\":48729},{\"end\":48745,\"start\":48739},{\"end\":48754,\"start\":48749},{\"end\":48988,\"start\":48980},{\"end\":49239,\"start\":49237},{\"end\":49247,\"start\":49245},{\"end\":49256,\"start\":49251},{\"end\":49263,\"start\":49260},{\"end\":49276,\"start\":49267},{\"end\":49293,\"start\":49280},{\"end\":49304,\"start\":49299},{\"end\":49314,\"start\":49308},{\"end\":49580,\"start\":49576},{\"end\":49589,\"start\":49586},{\"end\":49598,\"start\":49593},{\"end\":49617,\"start\":49608},{\"end\":49798,\"start\":49795},{\"end\":49805,\"start\":49802},{\"end\":49813,\"start\":49809},{\"end\":49821,\"start\":49817},{\"end\":49828,\"start\":49825},{\"end\":50041,\"start\":50038},{\"end\":50053,\"start\":50045},{\"end\":50062,\"start\":50057},{\"end\":50071,\"start\":50068},{\"end\":50080,\"start\":50075}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":873046},\"end\":36872,\"start\":36708},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b1\"},\"end\":37200,\"start\":36874},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6785090},\"end\":37433,\"start\":37202},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5590763},\"end\":37802,\"start\":37435},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b4\"},\"end\":38093,\"start\":37804},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":57246310},\"end\":38329,\"start\":38095},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5736847},\"end\":38719,\"start\":38331},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10282227},\"end\":38911,\"start\":38721},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9254582},\"end\":39290,\"start\":38913},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13272863},\"end\":39622,\"start\":39292},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":261138},\"end\":39898,\"start\":39624},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":40080,\"start\":39900},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6918332},\"end\":40455,\"start\":40082},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1915014},\"end\":40633,\"start\":40457},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7502269},\"end\":40900,\"start\":40635},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14521054},\"end\":41149,\"start\":40902},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8517067},\"end\":41371,\"start\":41151},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":41520,\"start\":41373},{\"attributes\":{\"id\":\"b18\"},\"end\":41785,\"start\":41522},{\"attributes\":{\"id\":\"b19\"},\"end\":42059,\"start\":41787},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207779694},\"end\":42184,\"start\":42061},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":195908774},\"end\":42419,\"start\":42186},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18124397},\"end\":42706,\"start\":42421},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10315654},\"end\":42951,\"start\":42708},{\"attributes\":{\"id\":\"b24\"},\"end\":43187,\"start\":42953},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6108541},\"end\":43435,\"start\":43189},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10702193},\"end\":43688,\"start\":43437},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":964287},\"end\":43844,\"start\":43690},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14113767},\"end\":44186,\"start\":43846},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3509328},\"end\":44422,\"start\":44188},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6875312},\"end\":44749,\"start\":44424},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":205242740},\"end\":45359,\"start\":44751},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14432549},\"end\":45608,\"start\":45361},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":11080756},\"end\":45845,\"start\":45610},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7147309},\"end\":46076,\"start\":45847},{\"attributes\":{\"doi\":\"arXiv:1512.06963\",\"id\":\"b35\"},\"end\":46298,\"start\":46078},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6631177},\"end\":46580,\"start\":46300},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":515925},\"end\":47284,\"start\":46582},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14124313},\"end\":47504,\"start\":47286},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1211821},\"end\":47796,\"start\":47506},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206592484},\"end\":48087,\"start\":47798},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9026666},\"end\":48295,\"start\":48089},{\"attributes\":{\"doi\":\"arXiv:1610.02424\",\"id\":\"b42\"},\"end\":48665,\"start\":48297},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1169492},\"end\":48885,\"start\":48667},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2332513},\"end\":49155,\"start\":48887},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1055111},\"end\":49519,\"start\":49157},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1539668},\"end\":49749,\"start\":49521},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3120635},\"end\":49950,\"start\":49751},{\"attributes\":{\"doi\":\"arXiv:1609.05143\",\"id\":\"b48\"},\"end\":50271,\"start\":49952}]", "bib_title": "[{\"end\":36727,\"start\":36708},{\"end\":37276,\"start\":37202},{\"end\":37527,\"start\":37435},{\"end\":38145,\"start\":38095},{\"end\":38412,\"start\":38331},{\"end\":38778,\"start\":38721},{\"end\":38959,\"start\":38913},{\"end\":39353,\"start\":39292},{\"end\":39670,\"start\":39624},{\"end\":39944,\"start\":39900},{\"end\":40176,\"start\":40082},{\"end\":40479,\"start\":40457},{\"end\":40703,\"start\":40635},{\"end\":40974,\"start\":40902},{\"end\":41216,\"start\":41151},{\"end\":41415,\"start\":41373},{\"end\":41595,\"start\":41522},{\"end\":42084,\"start\":42061},{\"end\":42249,\"start\":42186},{\"end\":42479,\"start\":42421},{\"end\":42759,\"start\":42708},{\"end\":43258,\"start\":43189},{\"end\":43496,\"start\":43437},{\"end\":43744,\"start\":43690},{\"end\":43887,\"start\":43846},{\"end\":44245,\"start\":44188},{\"end\":44476,\"start\":44424},{\"end\":44806,\"start\":44751},{\"end\":45432,\"start\":45361},{\"end\":45672,\"start\":45610},{\"end\":45901,\"start\":45847},{\"end\":46369,\"start\":46300},{\"end\":46648,\"start\":46582},{\"end\":47352,\"start\":47286},{\"end\":47584,\"start\":47506},{\"end\":47828,\"start\":47798},{\"end\":48140,\"start\":48089},{\"end\":48714,\"start\":48667},{\"end\":48976,\"start\":48887},{\"end\":49233,\"start\":49157},{\"end\":49572,\"start\":49521},{\"end\":49791,\"start\":49751}]", "bib_author": "[{\"end\":36739,\"start\":36729},{\"end\":36752,\"start\":36739},{\"end\":36765,\"start\":36752},{\"end\":36775,\"start\":36765},{\"end\":36946,\"start\":36938},{\"end\":36954,\"start\":36946},{\"end\":36964,\"start\":36954},{\"end\":36976,\"start\":36964},{\"end\":36985,\"start\":36976},{\"end\":36995,\"start\":36985},{\"end\":37008,\"start\":36995},{\"end\":37286,\"start\":37278},{\"end\":37299,\"start\":37286},{\"end\":37536,\"start\":37529},{\"end\":37555,\"start\":37536},{\"end\":37567,\"start\":37555},{\"end\":37579,\"start\":37567},{\"end\":37590,\"start\":37579},{\"end\":37600,\"start\":37590},{\"end\":37891,\"start\":37882},{\"end\":37903,\"start\":37891},{\"end\":37910,\"start\":37903},{\"end\":37920,\"start\":37910},{\"end\":38155,\"start\":38147},{\"end\":38163,\"start\":38155},{\"end\":38173,\"start\":38163},{\"end\":38179,\"start\":38173},{\"end\":38185,\"start\":38179},{\"end\":38196,\"start\":38185},{\"end\":38425,\"start\":38414},{\"end\":38440,\"start\":38425},{\"end\":38454,\"start\":38440},{\"end\":38466,\"start\":38454},{\"end\":38481,\"start\":38466},{\"end\":38491,\"start\":38481},{\"end\":38502,\"start\":38491},{\"end\":38791,\"start\":38780},{\"end\":38801,\"start\":38791},{\"end\":38969,\"start\":38961},{\"end\":38978,\"start\":38969},{\"end\":38989,\"start\":38978},{\"end\":39005,\"start\":38989},{\"end\":39013,\"start\":39005},{\"end\":39023,\"start\":39013},{\"end\":39030,\"start\":39023},{\"end\":39036,\"start\":39030},{\"end\":39048,\"start\":39036},{\"end\":39059,\"start\":39048},{\"end\":39072,\"start\":39059},{\"end\":39081,\"start\":39072},{\"end\":39366,\"start\":39355},{\"end\":39377,\"start\":39366},{\"end\":39390,\"start\":39377},{\"end\":39399,\"start\":39390},{\"end\":39413,\"start\":39399},{\"end\":39428,\"start\":39413},{\"end\":39439,\"start\":39428},{\"end\":39681,\"start\":39672},{\"end\":39692,\"start\":39681},{\"end\":39702,\"start\":39692},{\"end\":39712,\"start\":39702},{\"end\":39720,\"start\":39712},{\"end\":39731,\"start\":39720},{\"end\":39742,\"start\":39731},{\"end\":39952,\"start\":39946},{\"end\":39961,\"start\":39952},{\"end\":39968,\"start\":39961},{\"end\":39975,\"start\":39968},{\"end\":40193,\"start\":40178},{\"end\":40208,\"start\":40193},{\"end\":40220,\"start\":40208},{\"end\":40230,\"start\":40220},{\"end\":40240,\"start\":40230},{\"end\":40251,\"start\":40240},{\"end\":40495,\"start\":40481},{\"end\":40510,\"start\":40495},{\"end\":40712,\"start\":40705},{\"end\":40722,\"start\":40712},{\"end\":40734,\"start\":40722},{\"end\":40748,\"start\":40734},{\"end\":40987,\"start\":40976},{\"end\":40999,\"start\":40987},{\"end\":41010,\"start\":40999},{\"end\":41230,\"start\":41218},{\"end\":41241,\"start\":41230},{\"end\":41427,\"start\":41417},{\"end\":41433,\"start\":41427},{\"end\":41606,\"start\":41597},{\"end\":41623,\"start\":41606},{\"end\":41634,\"start\":41623},{\"end\":41899,\"start\":41891},{\"end\":41909,\"start\":41899},{\"end\":42095,\"start\":42086},{\"end\":42109,\"start\":42095},{\"end\":42265,\"start\":42251},{\"end\":42278,\"start\":42265},{\"end\":42288,\"start\":42278},{\"end\":42493,\"start\":42481},{\"end\":42504,\"start\":42493},{\"end\":42512,\"start\":42504},{\"end\":42518,\"start\":42512},{\"end\":42526,\"start\":42518},{\"end\":42536,\"start\":42526},{\"end\":42546,\"start\":42536},{\"end\":42775,\"start\":42761},{\"end\":42786,\"start\":42775},{\"end\":42796,\"start\":42786},{\"end\":42806,\"start\":42796},{\"end\":42814,\"start\":42806},{\"end\":43050,\"start\":43041},{\"end\":43063,\"start\":43050},{\"end\":43270,\"start\":43260},{\"end\":43284,\"start\":43270},{\"end\":43297,\"start\":43284},{\"end\":43504,\"start\":43498},{\"end\":43516,\"start\":43504},{\"end\":43526,\"start\":43516},{\"end\":43536,\"start\":43526},{\"end\":43544,\"start\":43536},{\"end\":43757,\"start\":43746},{\"end\":43899,\"start\":43889},{\"end\":43908,\"start\":43899},{\"end\":43920,\"start\":43908},{\"end\":43931,\"start\":43920},{\"end\":43943,\"start\":43931},{\"end\":43951,\"start\":43943},{\"end\":43961,\"start\":43951},{\"end\":43972,\"start\":43961},{\"end\":43985,\"start\":43972},{\"end\":43995,\"start\":43985},{\"end\":44254,\"start\":44247},{\"end\":44260,\"start\":44254},{\"end\":44268,\"start\":44260},{\"end\":44276,\"start\":44268},{\"end\":44286,\"start\":44276},{\"end\":44486,\"start\":44478},{\"end\":44497,\"start\":44486},{\"end\":44506,\"start\":44497},{\"end\":44516,\"start\":44506},{\"end\":44531,\"start\":44516},{\"end\":44541,\"start\":44531},{\"end\":44551,\"start\":44541},{\"end\":44566,\"start\":44551},{\"end\":44816,\"start\":44808},{\"end\":44831,\"start\":44816},{\"end\":44841,\"start\":44831},{\"end\":44849,\"start\":44841},{\"end\":44859,\"start\":44849},{\"end\":44874,\"start\":44859},{\"end\":44884,\"start\":44874},{\"end\":44898,\"start\":44884},{\"end\":44911,\"start\":44898},{\"end\":44924,\"start\":44911},{\"end\":44936,\"start\":44924},{\"end\":44947,\"start\":44936},{\"end\":44956,\"start\":44947},{\"end\":44970,\"start\":44956},{\"end\":44978,\"start\":44970},{\"end\":44989,\"start\":44978},{\"end\":45001,\"start\":44989},{\"end\":45009,\"start\":45001},{\"end\":45021,\"start\":45009},{\"end\":45441,\"start\":45434},{\"end\":45448,\"start\":45441},{\"end\":45455,\"start\":45448},{\"end\":45461,\"start\":45455},{\"end\":45468,\"start\":45461},{\"end\":45686,\"start\":45674},{\"end\":45696,\"start\":45686},{\"end\":45704,\"start\":45696},{\"end\":45713,\"start\":45704},{\"end\":45914,\"start\":45903},{\"end\":45924,\"start\":45914},{\"end\":45932,\"start\":45924},{\"end\":45943,\"start\":45932},{\"end\":46126,\"start\":46119},{\"end\":46133,\"start\":46126},{\"end\":46140,\"start\":46133},{\"end\":46148,\"start\":46140},{\"end\":46158,\"start\":46148},{\"end\":46378,\"start\":46371},{\"end\":46385,\"start\":46378},{\"end\":46392,\"start\":46385},{\"end\":46400,\"start\":46392},{\"end\":46410,\"start\":46400},{\"end\":46660,\"start\":46650},{\"end\":46669,\"start\":46660},{\"end\":46683,\"start\":46669},{\"end\":46691,\"start\":46683},{\"end\":46700,\"start\":46691},{\"end\":46721,\"start\":46700},{\"end\":46738,\"start\":46721},{\"end\":46752,\"start\":46738},{\"end\":46770,\"start\":46752},{\"end\":46781,\"start\":46770},{\"end\":46793,\"start\":46781},{\"end\":46802,\"start\":46793},{\"end\":46810,\"start\":46802},{\"end\":46826,\"start\":46810},{\"end\":46839,\"start\":46826},{\"end\":46852,\"start\":46839},{\"end\":46861,\"start\":46852},{\"end\":46876,\"start\":46861},{\"end\":46887,\"start\":46876},{\"end\":46899,\"start\":46887},{\"end\":47366,\"start\":47354},{\"end\":47379,\"start\":47366},{\"end\":47598,\"start\":47586},{\"end\":47612,\"start\":47598},{\"end\":47621,\"start\":47612},{\"end\":47632,\"start\":47621},{\"end\":47841,\"start\":47830},{\"end\":47848,\"start\":47841},{\"end\":47855,\"start\":47848},{\"end\":47867,\"start\":47855},{\"end\":47875,\"start\":47867},{\"end\":47887,\"start\":47875},{\"end\":47896,\"start\":47887},{\"end\":47909,\"start\":47896},{\"end\":47923,\"start\":47909},{\"end\":48154,\"start\":48142},{\"end\":48167,\"start\":48154},{\"end\":48177,\"start\":48167},{\"end\":48389,\"start\":48374},{\"end\":48401,\"start\":48389},{\"end\":48416,\"start\":48401},{\"end\":48423,\"start\":48416},{\"end\":48430,\"start\":48423},{\"end\":48442,\"start\":48430},{\"end\":48451,\"start\":48442},{\"end\":48727,\"start\":48716},{\"end\":48737,\"start\":48727},{\"end\":48747,\"start\":48737},{\"end\":48756,\"start\":48747},{\"end\":48990,\"start\":48978},{\"end\":49241,\"start\":49235},{\"end\":49249,\"start\":49241},{\"end\":49258,\"start\":49249},{\"end\":49265,\"start\":49258},{\"end\":49278,\"start\":49265},{\"end\":49295,\"start\":49278},{\"end\":49306,\"start\":49295},{\"end\":49316,\"start\":49306},{\"end\":49582,\"start\":49574},{\"end\":49591,\"start\":49582},{\"end\":49600,\"start\":49591},{\"end\":49606,\"start\":49600},{\"end\":49619,\"start\":49606},{\"end\":49800,\"start\":49793},{\"end\":49807,\"start\":49800},{\"end\":49815,\"start\":49807},{\"end\":49823,\"start\":49815},{\"end\":49830,\"start\":49823},{\"end\":50043,\"start\":50036},{\"end\":50055,\"start\":50043},{\"end\":50064,\"start\":50055},{\"end\":50073,\"start\":50064},{\"end\":50082,\"start\":50073}]", "bib_venue": "[{\"end\":36779,\"start\":36775},{\"end\":36936,\"start\":36874},{\"end\":37303,\"start\":37299},{\"end\":37605,\"start\":37600},{\"end\":37880,\"start\":37804},{\"end\":38200,\"start\":38196},{\"end\":38506,\"start\":38502},{\"end\":38806,\"start\":38801},{\"end\":39085,\"start\":39081},{\"end\":39443,\"start\":39439},{\"end\":39746,\"start\":39742},{\"end\":39979,\"start\":39975},{\"end\":40255,\"start\":40251},{\"end\":40528,\"start\":40510},{\"end\":40752,\"start\":40748},{\"end\":41014,\"start\":41010},{\"end\":41245,\"start\":41241},{\"end\":41437,\"start\":41433},{\"end\":41638,\"start\":41634},{\"end\":41889,\"start\":41787},{\"end\":42113,\"start\":42109},{\"end\":42292,\"start\":42288},{\"end\":42550,\"start\":42546},{\"end\":42817,\"start\":42814},{\"end\":43039,\"start\":42953},{\"end\":43301,\"start\":43297},{\"end\":43549,\"start\":43544},{\"end\":43760,\"start\":43757},{\"end\":43999,\"start\":43995},{\"end\":44290,\"start\":44286},{\"end\":44570,\"start\":44566},{\"end\":45027,\"start\":45021},{\"end\":45472,\"start\":45468},{\"end\":45716,\"start\":45713},{\"end\":45947,\"start\":45943},{\"end\":46117,\"start\":46078},{\"end\":46424,\"start\":46410},{\"end\":46905,\"start\":46899},{\"end\":47383,\"start\":47379},{\"end\":47636,\"start\":47632},{\"end\":47927,\"start\":47923},{\"end\":48181,\"start\":48177},{\"end\":48372,\"start\":48297},{\"end\":48760,\"start\":48756},{\"end\":49006,\"start\":48990},{\"end\":49320,\"start\":49316},{\"end\":49624,\"start\":49619},{\"end\":49834,\"start\":49830},{\"end\":50034,\"start\":49952}]"}}}, "year": 2023, "month": 12, "day": 17}