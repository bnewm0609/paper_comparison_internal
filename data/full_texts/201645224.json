{"id": 201645224, "updated": "2023-11-11 00:47:55.707", "metadata": {"title": "Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry", "authors": "[{\"first\":\"Shunkai\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zike\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Hongbin\",\"last\":\"Zha\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 10, "day": 1}, "abstract": "We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2983619469", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/LiXWYZ19", "doi": "10.1109/iccv.2019.00294"}}, "content": {"source": {"pdf_hash": "267ddda5a5a14ba584123ef1c344eb4a7214eff6", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1908.08704", "status": "GREEN"}}, "grobid": {"id": "95b65c1f619e8ea77efae6528fa5402c854d2c22", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/267ddda5a5a14ba584123ef1c344eb4a7214eff6.txt", "contents": "\nSequential Adversarial Learning for Self-Supervised Deep Visual Odometry\n\n\nShunkai Li \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nSenseTime Machine Vision Joint Lab\nPeking University PKU\n\n\nFei Xue \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nSenseTime Machine Vision Joint Lab\nPeking University PKU\n\n\nXin Wang \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nSenseTime Machine Vision Joint Lab\nPeking University PKU\n\n\nZike Yan zike.yan@pku.edu.cnzha@cis.pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nSenseTime Machine Vision Joint Lab\nPeking University PKU\n\n\nHongbin Zha \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nSenseTime Machine Vision Joint Lab\nPeking University PKU\n\n\nSequential Adversarial Learning for Self-Supervised Deep Visual Odometry\n10.1109/ICCV.2019.00294\nWe propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-ofthe-art self-supervised methods significantly.\n\nIntroduction\n\nThe ability for an agent to understand 3D environment and infer ego-motion is crucial for many real-world applications, such as autonomous driving [7], robotics [14], and virtual/augmented reality [30]. As the problem of simultaneous localization and mapping (SLAM) and visual odometry (VO) has a clear meaning in 3D geometry, VO/SLAM has been studied as a multi-view geometric problem for decades. These classic methods [11,12,15,25,29] perfor- * equal contribution Figure 1. Overview of our method. The network extracts optical flow into a compact code, which is incorporated by LSTM to aggregate historical information and refine previous estimations. Depth and pose estimation is regarded as an image conditioned generative task, and the refined code is provided as input signal. The geometric inference is used to reconstruct a warped image by view synthesis and evaluated by a discriminator. m well in regular scenes, but fail in challenging conditions due to their inherent reliance on low-level feature correspondences. Since deep learning captures structural perception by extracting high-level features, a number of learningbased VO methods have been applied to break through the limitations of classic approaches [19,33,34,36,37,41].\n\nHowever, supervised learning requires substantial labeled data, which is either tedious or impractical to obtain. Recent work has been trying to address this problem by coupling depth and pose estimation in a self-supervised manner [39,42]. As image sequence is the only input, all the estimations should be mapped to image space for selfsupervision. The mapping is typically made by view synthesis and photometric loss is defined to minimize the difference between synthesized image and the real one.\n\nIn self-supervised VO, estimation of depth and pose are simultaneously learned in a coupled way, accurate depth contributes to precise pose estimation and vice versa. Previous works on self-supervised VO estimate depth from single view. As an ill-posed problem, the output depth is vague, hence the predicted pose is also inaccurate. However, uncertainties in depth estimation can be eliminated by exploiting correlations between consecutive frames. Nonetheless, due to the data redundancy of image sequence, it is inefficient to integrate the information of multiple frames by stacking them along the RGB channel. In this paper, we propose to learn a compact representation (referred to as 'code') of the correlation between frames, and sequential information is accumulated by integrating codes via Long Short-Term Memory (LSTM). The code provides correlations of consecutive frames that help generate clear depth maps and reduce accumulated error over a long sequence.\n\nOn the other hand, inaccurate depth and pose leads to distortion artifacts in synthesized images (Fig. 3), which are difficult to be eliminated by photometric loss due to the pixel-level correspondence. A new evaluation criterion with structural perception is needed for accurate depth estimation. In this paper, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN) [18]. The generator learns to estimate depth and pose to synthesize a warped image, while the discriminator evaluates the quality of synthesized image with structural perception and higher-level understandings. This two-player game impels the generator to estimate more accurate depth and pose, while the discriminator is able to distinguish distortion artifacts with structural perception.\n\nThe overview of our method is shown in Fig. 1. Different from single-view estimation, our method generates clear depth with additional information which cannot be retrieved from a single image. The information is obtained by encoding optical flow into a compact code, and codes of multiple frames are incorporated and refined by LSTM. The overall framework is treated as a generative model with adversarial learning. During training, the spatial-temporal consistency is enforced as self-supervision. The main contributions of our paper can be summarized as follows:\n\n\u2022 We propose to exploit spatial-temporal correlations over long sequence to significantly reduce estimation errors and scale drift for self-supervised VO.\n\n\u2022 We treat self-supervised VO as a generative model and take the advantage of adversarial learning for selfsupervised pose and depth estimation.\n\nOur method outperforms state-of-the-art self-supervised approaches significantly and gives comparable results with supervised manners. Extensive experiments manifest the advantages of our model. Besides, the idea of selfsupervised adversarial learning with spatial-temporal consistency may also bring insight into VO/SLAM and videobased computer vision researches.\n\n\nRelated works\n\nHumans are capable of perceiving 3D environment and inferring ego-motion in a short time, but it is hard for an agent to be equipped with similar capabilities. VO/SLAM has been considered as a multi-view geometric problem for decades. It is traditionally solved by minimizing photometric [12] or geometric [29] reprojection errors and works well in regular environments, but fails in challenging conditions like dynamic objects and abrupt motions. In light of these limitations, VO has been studied with learning techniques in recent years and many approaches with promising performance have been proposed.\n\nSupervised methods formulate VO as a supervised learning problem and many methods with good results have been proposed. DeMoN [33] jointly estimates pose and depth in an end-to-end manner. Inspired by the practice of parallel tracking and mapping in classic VO/SLAM, Deep-TAM [41] utilizes two networks for pose and depth estimation. DeepVO [34] treats VO as a sequence-to-sequence learning problem by estimating poses recurrently. The limitation of supervised learning is that it requires a large amount of labeled data. The acquisition of ground truth often requires expensive equipment or highly manual labeling, and some gathered data are inaccurate. Depth obtained by LIDAR is sparse, and the output depth of Kinect contains a lot of noise. Furthermore, some ground truth is unable to obtain (e.g. optical flow). Previous works have tried to address these problems with synthetic datasets [9], but there is always a gap between synthetic and real-world data.\n\nSelf-supervised methods In order to alleviate the reliance on ground truth, recently many self-supervised methods have been proposed for VO. The key to self-supervised learning is to find the internal correlations and constraints in the training data. SfMLearner [42] leverages the geometric correlation of depth and pose to learn both of them in a coupled way, with a learned mask to mask out regions that don't meet static scene assumption. As the first selfsupervised approach for VO, SfMLearner couples depth and pose estimations with image warping, which becomes the problem of minimizing photometric loss. Inherited from this idea, many self-supervised VO have been proposed, including modifications on loss functions [22,26], network architectures [3,4,22,28,40], predicted contents [39], and combination with classic VO/SLAM [5,38]. For example, GeoNet [39] extends the framework to jointly estimate optical flow with forward-backward consistency to infer unstable regions and achieves state-of-the-art performance among self-supervised VO methods.\n\nDespite its feasibility, self-supervised VO still underperforms supervised ones. Apart from the effectiveness of direct supervision, a key reason is that they focus mainly on geometric properties [42] but pay little attention to the sequential nature of the problem. In these methods, only a few frames (no more than 5) are processed in the network, while previous estimations are discarded and the current estimation is made from scratch. Instead, the performance can be enhanced by taking geometric relations of sequential observations into account.\n\nOur approach differs from previous art in formulating self-supervised VO as a sequential learning problem. The frame-to-frame correlation is represented as a compact code, and sequential information are integrated via LSTM. In contrast to prevalent single-view depth estimation, our framework estimates depth with the code conditioned on a single image and treat VO as a generative task. By means of adversarial learning, our method provides sharper depth and more accurate pose estimations.\n\n\nMethod\n\nIn this section, we will introduce our method in detail. The entire framework consists of four components ( \n\n\nEncoder\n\nVisual odometry estimates camera motion between consecutive image pairs. This estimation is computed by feature correspondence or photometric consistency in classic VO/SLAM. Different from previous self-supervised methods that estimate directly from raw images, we provide the network with a representation of frame-to-frame correspondence for depth and pose estimation.\n\nAs a way of frame-to-frame correspondence, parallax and motion of each pixel can be obtained by computing optical flow between consecutive images. In our framework, we compute optical flow [13] and extract it into a compact representation (referred to as 'code') c t with a size of 128\nc t = C(F(I t\u22121 , I t )).(1)\nThe extracted c t will be incorporated with historical information and used as side input for depth and pose estimation.\n\n\nSequential information aggregation\n\nEstimating depth and pose from only a few frames is prone to error accumulation and scale drift. The problem can be mitigated by exploiting correlations over long sequence. This formulation is appealing for self-supervised sequential estimations since it utilizes incoming observations and spatial-temporal consistency as self-supervision.\n\nIn our framework, we use LSTM [20] to model VO as a self-supervised sequential learning problem. As an extension of recurrent neural networks (RNN), LSTM introduces a cell to remember and forget information adaptively. LSTM fuses the code c t of current frame I t into accumulated information. Intuitively, the long-term information is remembered as a prior, and short-term memory is used to infer the current state. The feature flow passing through recurrent units carries rich information of previous states, enabling refined outputs to improve the current estimation\nc t , h t = U(c t , h t\u22121 ),(2)\nwhere c t denotes the refined code that incorporates historical information, and h t\u22121 , h t are hidden states at time t \u2212 1, t, respectively.\n\n\nDepth estimation\n\nIn the existing literature, depth is estimated from a single image ID = D(I).\n\nAs an ill-posed problem, the estimated depth is reasonable on the whole but vague in details. On the other hand, simply stacking multiple frames does not improve the result of depth estimation [42]. In order to obtain a clear depth, correlations of multiple views should be provided as additional information which cannot be retrieved from a single image. Because of the high degree of order and regularity of 3D scenes, depth can be effectively represented by a compact feature with a single image [6]. As motion parallax of two frames reflects the distance of each part of the scene, we provide the refined code c t as side input for depth estimation\nD t = D(I t , c t ).(4)\nAs an image conditioned depth generation process, I t is extracted into a feature map by convolutional layers, which is further concatenated with c t in the network. It is then followed by up-sampling layers with skip connections.\n\n\nPose and mask estimation\n\nMost self-supervised VO methods regress pose directly from images but fail to exploit the depth of two views. In classic methods, pose regression from images and depth is solved by RGBD registration, such as using image feature detection for initial guess and robust 3D correspondence for pose refinement [23,31]. In order to exploit both color and depth information, we stack images and depth maps into 2 RGBD images for pose estimation from t \u2212 1 to t\nT t t\u22121 = P((I t\u22121 ,D t\u22121 ), (I t ,D t )).(5)\nAfter the acquisition of pose and depth, image warping is used for view synthesis. The homogeneous coordinate of Figure 2. Illustration of our framework. The encoder compresses optical flow of two consecutive images into a compact code, which is aggregated and refined by LSTM. The DepthNet estimates depth conditioned on the refined code and input image. The estimated depth is concatenated with image for pose and mask prediction, while the authenticity of the warped image is judged by the discriminator. The discriminator is excluded during the test phase. a pixel in the target view p t and the source view p t\u22121 are correlated by [42] \np t\u22121 \u223c KT t\u22121 tDt (p t )K \u22121 p t ,(6)\nwhere K denotes camera intrinsics. We use differentiable bilinear sampling as [42]. In this way, the synthesized im-age\u00ce t and I t can be used for self-supervision. Nonetheless, view synthesis builds on the assumption that the scene is static without illumination change and occlusions, which is often violated in practice. To overcome this problem, our framework learns to predict a per-pixel maskM t as a belief in how successful a target pixel is rendered during view synthesis [42]. Consequently, the weighted photometric loss is\nL pho = <I1,...,I N > pM t (p) \u00ce t (p) \u2212 I t (p) 1 . (7)\n\nDiscriminator\n\nPhotometric loss is widely used in self-supervised VO and the warped results are shown in Fig. 3. Despite convolutional neural networks (CNN) extract high-level features that prevent low-level feature problem in classic VO/SLAM, the loss function is still based on pixel-level instead of evaluating on a larger receptive field with higherlevel understandings. Due to the pixel-level correspondence and photometric consistency assumption, photometric loss is not robust to occlusion, texture-less regions, dynamic objects and illumination change. In these challenging conditions, there are multiple local minima with similar magnitudes. The network tends to trap into any of them during training with vague depth and wrong pose, leading to inaccurate reconstruction (Fig. 3). Some of previous research have realized this problem [39,40] and try to eliminate this disturbing factor by explicitly modeling motion segmentation and optical flow, but achieve limited improvement.\n\nInstead, the distortion artifacts are easily detectable by a discriminator. The compelling results achieved by GAN have been successfully demonstrated in many image generation tasks [1,21,43]. The adversarial learning impels the network to learn more flexible distributions to tackle underfitting issues and overcome gradient locality. In the selfsupervised paradigm, VO can be regarded as a conditional image generation task\nI t = G(c t\u22121 , c t |I t\u22121 , I t ).(8)\nI t is a sample from distribution p real , and\u00ce t is generated from c t\u22121 , c t on the latent space p code . During training, the generator tries to fool the discriminator by generating better pose and depth. Meanwhile, given I t as side information, the discriminator tries to distinguish the fake\u00ce t by predicting a probability of authenticity D(\u00ce t |I t ). The adversarial training overcomes the problem of Eq. (7) to produce accurate depth and pose without explicit modeling of motion segmentations and optical flow.  [42], bottom row: warped images of our method. It can be seen that inaccurate predictions will lead to distortion artifacts on the warped image. Compared to the existing literature, our method synthesizes more accurate warped images.\n\nThe value function of this min-max game can be formulated according to [21] \nL GAN = min G max D V (G, D) =E It\u223cp real [log(D(I t |I t ))]+ E c t\u22121 ,c t \u223cp code [log(1 \u2212 D(\u00ce t |I t ))].(9)\n\nLoss functions\n\nAppearance loss In order to overcome the pixel-level correspondence problem, we measure the reconstructed images from both weighted photometric loss and structural similarity metric (SSIM) [35] L ap =L reg (M ) + (1 \u2212 \u03b1)L pho\n+ 1 N x,y \u03b1 SSIM (\u00ce(x, y), I(x, y)) 2 ,(10)\nL reg (M ) is a regularization term to prevent the network converges to a trivial solution, which is detailed in [42]. N is the number of images in the training minibatch. The filter size of SSIM is set 10\u00d710 and \u03b1 is set 0.85. Depth regularization Discontinuity of depth usually happens where strong image gradients are present. Similar to [4,40], we introduce an edge-aware smoothness loss to enforce discontinuity and local smoothness in depth\nL smo = 1 N x,y \u2207 xD (x, y) e \u2212 \u2207xI(x,y) + \u2207 yD (x, y) e \u2212 \u2207yI(x,y) .(11)\nTrajectory consistency Although LSTM-based framework is suffice to provide more accurate poses by filtering out the noise between consecutive transformations, the es-timatedT t\u22121 t are still relative poses. There are no relations and no geometric consistency among them. Actually, these relative poses can be transformed into a unified coordinate by accumulating them along the trajectory. According to rigid-body transformation, given a set of transformations such as A \u2192 B \u2192 C \u2192 D, the relative poses T B A , T C B , T D C satisfies the following constraints [22] T\nB A \u00b7 T C B \u00b7 T D C = T D A , T B A \u00b7 T C B = T C A , T C B \u00b7 T D C = T D B ,(12)\nIn order to enforce trajectory consistency, we compute the following loss on three scales for every eight frames GAN loss in Eq. (9) acts as an auxiliary self-supervision for the synthesized image. The final loss function becomes\nL T C = 1 N N i=1 t\u2208[2,4,8] p d i+t i \u2212p r i+t i 1 ,(13)L f inal = \u03bb a L ap + \u03bb s L smo + \u03bb t L T C + \u03bb g L GAN . (14)\n\nExperiments\n\nIn this section, we will introduce the implementation details and show both qualitative and quantitative results compared with other methods. In the end, an ablation study is employed to test the effectiveness of each component in our framework.\n\n\nImplementation details\n\nAs shown in Fig. 2, our framework includes 4 subnetworks. Both DepthNet and PoseMaskNet consist of encoding and decoding parts. The encoders are made up of 6 convolutional downsampling layers with stride 2, and decoders transform the extracted features into depth or masks with deconvolutional layers. Both depth and masks are predicted in 4 scales. In order to preserve both high-level and detailed information of the image, skip connections are used between encoders and decoders at corresponding resolutions [42]. Meanwhile, the encoding part of PoseMaskNet is also followed by 2 fully-connected layers to regress Euler angles and translations of 6-DoF pose, respectively. The Encoder and discriminator follow the same architecture as the encoding part of DepthNet. The extracted feature from Encoder then passes through an average pooling layer to output a 128-channel vector. Batch normalization and Re-LUs are adopted in each layer except for the output layers.\n\nOur  Table 1. Monodular depth estimation results on KITTI dataset by the split of Eigen et al. [10]. K and CS refer to KITTI and Cityscapes datasets, respectively. As for supervision, 'Depth' means the ground truth depth is used during training, 'Stereo' means stereo image sequences with known baselines between two cameras are used during training, and '-' means no supervision is provided. The results are capped at 80m and 50m, respectively. As for error metrics Abs Rel, Seq Rel, RMSE and RMSE log, lower value is better; as for accuracy metrics \u03b4 < 1.25, \u03b4 < 1.25 2 and \u03b4 < 1.25 3 , higher value is better.\n\ntogether in an self-supervised manner. During training, images are resized to 128\u00d7416 and data augmentation (random rotation, zoom, color jitter) is applied to prevent overfitting. As suggested in WGAN [2], the stochastic gradient descent is used for the discriminator, and Adam [24] optimizer with \u03b2 1 = 0.9, \u03b2 2 = 0.99 is used for all the other networks. The length of LSTM is set 15, and weighting factors \u03bb a , \u03bb s , \u03bb t , \u03bb g are set 0.75, 0.1, 0.14 and 0.01, respectively. The training batch size is set 4 with a weight decay of 3 \u00d7 10 \u22124 for 100,000 iterations. The initial learning rate is set 10 \u22124 and reduced by half for every 15,000 iterations. The network infers depth and pose at the speed of 18ms per frame during the test.\n\n\nDepth estimation\n\nWe take the split of Eigen et al. [10] and use monocular images to train and test depth estimation. Ground truth depth is obtained by projecting sparse laser-scanned depth points into images, and depth predictions are interpolated to be the same size as ground truth for evaluation. In order to solve the scale ambiguity problem, the predicted depth is multiplied by a scaling factor to match the median with ground truth. Following the evaluation protocol in [17], both 50m and 80m thresholds of maximum depth are used for evaluation. As with previous methods, we also pre-train the network on Cityscapes dataset [8] and fine-tune on KIT-TI to test its adaptability across different environments.\n\nWe provide a comparison with related works which have depth supervision [10] or calibrated stereo images with known camera baseline for self-supervision. As shown in Table 1, our method outperforms all self-supervised methods and achieves comparable results with supervised ones. In particular, KITTI and Cityscapes datasets differ not only in scene contents but also in camera intrinsics. Results in the bottom rows of Table 1 show that our method generalizes well in different environments. Since enhanced edges and details only take up a small proportion of depth maps, the improvement on depth accuracy is therefore limited. Fig. 4 shows the qualitative examples of depth estimated by different methods. It can be seen that some methods have difficulty in recovering the depth of cars and mistake the depth of several objects. As the code provides frame-toframe correspondence, our method produces clearer depth compared with single-view depth estimation approaches. Additionally, benefited from adversarial learning, the estimated depth preserves boundaries and thin structures, which is more accurate in details.\n\n\nPose estimation\n\nIn addition, we apply our method to KITTI odometry dataset for pose estimation. The dataset contains 11 driving scenes with ground truth poses. In order to make fair comparison, we follow the same train/test split as [39,42] by using sequences 00-08 for training and 09-10 for test.\n\nThe performance of pose estimation is evaluated using Absolute Trajectory Error (ATE) for both translation and rotation. Our method is compared with SfMLearner [42], GeoNet [39], Vid2Dpeth [28], Zhan et al. [40] and ORB-SLAM, a representative framework in classic SLAM. ORB-SLAM (short) is emplemented by tracking module with lo-  cal bundle adjustment, and ORB-SLAM (full) processes the entire sequence with loop closure and global bundle adjustment. Both versions of ORB-SLAM use a single scale map which is beneficial to an accurate trajectory with consistent scale. In order to solve the scale ambiguity problem in monocular VO, a scaling factor is used to align the trajectories with ground truth [40].\n\nAs shown in Table 2, our method significantly outperforms all the other baselines, and trajectories of sequences 09-10 are plotted in Fig. 5. In addition, although only a limited number of frames can be processed by LSTM, our method still performs better than ORB-SLAM (full) without any need of global optimization (such as loop closure, bundle adjustment and re-localization) [29]. This reveals that our method is able to produce accurate pose estimations by incorporating short-term correspondences and long-term \n\n\nAblation studies\n\nIn order to study the importance of each component, we perform ablation studies on various versions of our method. The baseline is our framework removing code, LSTM, trajectory consistency loss and discriminator. All the experiments are conducted on KITTI dataset and results are shown in Table 3, 4 and Fig. 6.\n\nAs shown in Fig. 6 (b), single view depth estimation is prone to be misled by the texture and color distributions in RGB images. The depth of poles is not recovered, and the depth of the sky is regarded the same as the white wall due to similar colors. In contrast, our method avoids these problems by taking additional information into account. The code encodes frame-to-frame correspondence which provides a significant improvement in depth estimation. The recovered depth is much sharper in contours and preserves tiny objects in both close and distant areas. In addition, adversarial learning gives the performance a further boost, and  Table 3. Ablation study on depth estimation for various versions of our method. Baseline denotes our framework without code, LSTM, discriminator (i.e. GAN) and trajectory consistency (TC) loss.  Table 4. Ablation study on pose estimation for various versions of our method on KITTI sequence 09 and 10. B denotes baseline. the temporal information actually improves depth.\n\nAs for pose estimation in Table 4, our baseline method performs much better than the other self-supervised VO approaches in literature (Table 2). This may mainly because of the joint use of depth and image for pose estimation (Eq. (5)). In addition, the accuracy is significantly improved by LSTM which incorporates historical information of multiple frames. The enforcement of trajectory consistency also brings about promising improvements in that it enforces geometric consistency among multiple pose estimations. Since depth is improved mainly on edges and details which takes up a small proportion, the accuracy gain is therefore limited. Yet the improved details are very important to RGBD matching for pose regression. Therefore, a slight increase in depth accuracy causes a big improvement in pose estimation.\n\n\nConclusions\n\nWe proposed an self-supervised VO framework that reduces accumulated errors over long sequence to achieve accurate pose and depth estimation. Benefited from spatialtemporal consistency among consecutive frames, the proposed framework incorporates historical information to reduce estimation errors in a self-supervised manner. In addition, we proposed to tackle VO as an self-supervised image generation task by means of a GAN paradigm. Our method outperforms both self-supervised and traditional VO baselines in literature, and ablation studies validate the effectiveness of each component of our framework.\n\nIn the future, we will extend our framework to unsupervised end-to-end SLAM. It is also worthwhile to investigate the code learned by our framework, which may help semantic segmentation, surface normal estimation and dense 3D reconstruction. In addition, developing an self-supervised online refinement technique to adaptively learn new environments on the fly is also an interesting issue of VO/SLAM and other 3D computer vision researches.\n\n\nFig. 2). The encoder extracts high-level features from optical flow into a compact code in Sec. 3.1, and the codes are aggregated and further refined by LSTM in Sec. 3.2. The generator estimates depth and pose conditioned on refined codes and images in Sec. 3.3-3.4. The discriminator in Sec. 3.5 judges the authenticity of a synthesized view. Finally, loss functions used in training are defined in Sec. 3.6.\n\nFigure 3 .\n3Example of warped images according to the estimated depth and pose. Top row: captured images, medium row: warped images of SfMLearner\n\nthe 6 -\n6DoF pose directly estimated from (I i , c i ) and (I i+t , c i+t ), andp r i+t i is the concatenated 6-DoF pose of successive relative transformations.\n\nFigure 4 .\n4Selected depth estimations from the test on KITTI dataset. Our method shows better prediction on detailed structures, low texture regions and shaded areas than the other self-supervised VO approaches. The estimated depth is clear in both close and distant areas.\n\nFigure 5 .\n5Trajectories of different methods on KITTI dataset. Our method shows a better odometry in both rotation and translation.\n\nFigure 6 .\n6Ablation study on depth estimation of our method. B denotes our baseline method, which is our framework without code, LSTM, discriminator (i.e. GAN) and trajectory consistency (TC) loss.\n\n\nmodel is implemented by PyTorch [32] on a single NVIDIA GTX 1080Ti GPU. All sub-networks are trainedMethod \n\nSupervision Dataset \nCap \nAbs Rel \nSq Rel RMSE \nRMSE log \u03b4 < 1.25 \u03b4 < 1.25 2 \n\u03b4 < 1.25 3 \nTrain set mean \n-\nK \n80m \n0.361 \n4.826 \n8.102 \n0.377 \n0.638 \n0.804 \n0.894 \nEigen et al. [10] Coarse \nDepth \nK \n80m \n0.214 \n1.605 \n6.563 \n0.292 \n0.673 \n0.884 \n0.957 \nEigen et al. [10] Fine \nDepth \nK \n80m \n0.203 \n1.548 \n6.307 \n0.282 \n0.702 \n0.890 \n0.958 \nLiu et al. [27] \nDepth \nK \n80m \n0.201 \n1.584 \n6.471 \n0.273 \n0.680 \n0.898 \n0.967 \nSfMLearner [42] \n-\nK \n80m \n0.208 \n1.768 \n6.856 \n0.283 \n0.678 \n0.885 \n0.957 \nVid2Depth [28] \n-\nK \n80m \n0.163 \n1.240 \n6.220 \n0.250 \n0.762 \n0.916 \n0.968 \nGeoNet [39] \n-\nK \n80m \n0.155 \n1.296 \n5.857 \n0.233 \n0.793 \n0.931 \n0.973 \nZhan et al. [40] \nStereo \nK \n80m \n0.135 \n1.132 \n5.585 \n0.229 \n0.820 \n0.933 \n0.971 \nOurs \n-\nK \n80m \n0.150 \n1.127 \n5.564 \n0.229 \n0.823 \n0.936 \n0.974 \nGarg et al. [16] \nStereo \nK \n50m \n0.169 \n1.080 \n5.104 \n0.273 \n0.740 \n0.904 \n0.962 \nSfMLearner [42] \n-\nK \n50m \n0.201 \n1.391 \n5.181 \n0.264 \n0.696 \n0.900 \n0.966 \nVid2Depth [28] \n-\nK \n50m \n0.155 \n0.927 \n4.549 \n0.231 \n0.781 \n0.931 \n0.975 \nGeoNet [39] \n-\nK \n50m \n0.147 \n0.936 \n4.348 \n0.218 \n0.810 \n0.941 \n0.977 \nZhan et al. [40] \nStereo \nK \n50m \n0.128 \n0.815 \n4.204 \n0.216 \n0.835 \n0.941 \n0.975 \nOurs \n-\nK \n50m \n0.146 \n0.927 \n4.107 \n0.216 \n0.819 \n0.943 \n0.981 \nSfMLearner [42] \n-\nCS+K \n80m \n0.198 \n1.836 \n6.565 \n0.275 \n0.718 \n0.901 \n0.960 \nVid2Depth [28] \n-\nCS+K \n80m \n0.159 \n1.231 \n5.912 \n0.243 \n0.784 \n0.923 \n0.970 \nGeoNet [39] \n-\nCS+K \n80m \n0.153 \n1.328 \n5.737 \n0.232 \n0.802 \n0.934 \n0.972 \nOurs \n-\nCS+K \n80m \n0.136 \n1.064 \n5.176 \n0.289 \n0.830 \n0.942 \n0.976 \n\n\n\n\nTable 2. Absolute Trajectory Error (ATE) on sequence 09 and 10 in KITTI odometry dataset. Our method outperforms all the other baselines by a large margin. dependences in odometry.Method \nSeq.09 \nSeq.10 \nORB-SLAM [29] (short) \n0.064\u00b10.141 \n0.064\u00b10.130 \nORB-SLAM [29] (full) \n0.014\u00b10.008 \n0.012\u00b10.011 \nSfMLearner [42] \n0.021\u00b10.017 \n0.020\u00b10.015 \nSfMLearner [42] modified \n0.016\u00b10.009 \n0.013\u00b10.009 \nZhan et al. [40] \n0.013\u00b10.009 \n0.013\u00b10.008 \nVid2Depth [28] \n0.013\u00b10.010 \n0.012\u00b10.011 \nGeoNet [39] \n0.012\u00b10.007 \n0.012\u00b10.009 \nOurs \n0.0030\u00b10.0014 \n0.0029\u00b10.0012 \n\n\nAcknowledgments. The work is supported by the National Key Research and Development Program of China (2017YFB1002601) and National Natural Science Foundation of China (61632003, 61771026).\nGenerative Adversarial Networks for Unsupervised Monocular Depth Prediction. Filippo Aleotti, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, ECCV. Filippo Aleotti, Fabio Tosi, Matteo Poggi, and Stefano Mat- toccia. Generative Adversarial Networks for Unsupervised Monocular Depth Prediction. In ECCV, 2018.\n\nWasserstein Generative Adversarial Networks. Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou, ICML. Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein Generative Adversarial Networks. In ICML, 2017.\n\nA Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation. V Babu, Anima Majumder, Kaushik Das, Swagat Kumar, arXiv:1809.00969arXiv preprintV Babu, Anima Majumder, Kaushik Das, Swagat Kumar, et al. A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation. arXiv preprint arXiv:1809.00969, 2018.\n\nUnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation. Kaushik V Madhu Babu, Anima Das, Swagat Majumdar, Kumar, In IROS. V Madhu Babu, Kaushik Das, Anima Majumdar, and Swagat Kumar. UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation. In IROS, 2018.\n\nDriven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments. Dan Barnes, Will Maddern, Geoffrey Pascoe, Ingmar Posner, In ICRA. Dan Barnes, Will Maddern, Geoffrey Pascoe, and Ingmar Posner. Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments. In ICRA, 2018.\n\nCodeSLAM: Learning a Compact, Optimisable Representation for Dense Visual SLAM. Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J Davison, CVPR. Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. CodeSLAM: Learn- ing a Compact, Optimisable Representation for Dense Visual SLAM. In CVPR, 2018.\n\nDeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. Chenyi Chen, Ari Seff, Alain Kornhauser, Jianxiong Xiao, ICCV. Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xi- ao. DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving. In ICCV, 2015.\n\nUwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, CVPR. Marius Cordts, Mohamed Omran, Sebastian Ramos, Tim- o Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. In CVPR, 2016.\n\nFlowNet: Learning Optical Flow with Convolutional Networks. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, ICCV. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hauss- er, Caner Hazirbas, Vladimir Golkov, Patrick Van Der S- magt, Daniel Cremers, and Thomas Brox. FlowNet: Learn- ing Optical Flow with Convolutional Networks. In ICCV, 2015.\n\nDepth Map Prediction from a Single Image Using a Multi-Scale Deep Network. David Eigen, Christian Puhrsch, Rob Fergus, NIPS. David Eigen, Christian Puhrsch, and Rob Fergus. Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network. In NIPS, 2014.\n\nDirect Sparse Odometry. Jakob Engel, Vladlen Koltun, Daniel Cremers, IEEE Transactions on Pattern Analysis and Machine Intelligence. 403Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct Sparse Odometry. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(3):611-625, 2018.\n\nLSD-SLAM: Large-Scale Direct Monocular SLAM. Jakob Engel, Thomas Sch\u00f6ps, Daniel Cremers, ECCV. Jakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. LSD- SLAM: Large-Scale Direct Monocular SLAM. In ECCV, 2014.\n\nTwo-Frame Motion Estimation Based on Polynomial Expansion. Gunnar Farnebck, Scandinavian Conference on Image Analysis. Gunnar Farnebck. Two-Frame Motion Estimation Based on Polynomial Expansion. In Scandinavian Conference on Im- age Analysis, 2003.\n\nCollaborative Monocular SLAM with Multiple Micro Aerial Vehicles. Christian Forster, Simon Lynen, Laurent Kneip, Davide Scaramuzza, IROS. Christian Forster, Simon Lynen, Laurent Kneip, and Davide Scaramuzza. Collaborative Monocular SLAM with Multiple Micro Aerial Vehicles. In IROS, 2013.\n\nFast Semi-Direct Monocular Visual Odometry. Christian Forster, Matia Pizzoli, Davide Scaramuzza, . S- Vo, ICRA. Christian Forster, Matia Pizzoli, and Davide Scaramuzza. S- VO: Fast Semi-Direct Monocular Visual Odometry. In ICRA, 2014.\n\nUnsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. Ravi Garg, Vijay Kumar, B G , Gustavo Carneiro, Ian Reid, ECCV. Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Rei- d. Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. In ECCV, 2016.\n\nUnsupervised Monocular Depth Estimation with Left-Right Consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, CVPR. Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised Monocular Depth Estimation with Left-Right Consistency. In CVPR, 2017.\n\n. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative Adversarial Nets. In NIPS. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing X- u, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In NIPS, 2014.\n\nMapNet: An Allocentric Spatial Memory for Mapping Environments. F Joao, Andrea Henriques, Vedaldi, CVPR. Joao F Henriques and Andrea Vedaldi. MapNet: An Allocen- tric Spatial Memory for Mapping Environments. In CVPR, 2018.\n\nLong Short-Term Memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural Computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997.\n\nImage-to-Image Translation with Conditional Adversarial Networks. Phillip Isola, Junyan Zhu, Tinghui Zhou, Alexei A Efros, CVPR. Phillip Isola, Junyan Zhu, Tinghui Zhou, and Alexei A E- fros. Image-to-Image Translation with Conditional Adver- sarial Networks. In CVPR, 2017.\n\nGeometric Consistency for Self-Supervised End-to-End Visual Odometry. Ganesh Iyer, Krishna Murthy, Gunshi Gupta, Madhava Krishna, Liam Paull, CVPR Workshops. Ganesh Iyer, J Krishna Murthy, Gunshi Gupta, Madhava Krishna, and Liam Paull. Geometric Consistency for Self- Supervised End-to-End Visual Odometry. In CVPR Work- shops, 2018.\n\nDense visual SLAM for RGB-D cameras. Christian Kerl, Jurgen Sturm, Daniel Cremers, IROS. Christian Kerl, Jurgen Sturm, and Daniel Cremers. Dense visual SLAM for RGB-D cameras. In IROS, 2014.\n\nAdam: A method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for Stochastic Optimization. In ICLR, 2015.\n\nParallel Tracking and Mapping on a Camera Phone. Georg Klein, David Murray, ISMAR. Georg Klein and David Murray. Parallel Tracking and Map- ping on a Camera Phone. In ISMAR, 2009.\n\nUndeepVO: Monocular Visual Odometry through Unsupervised Deep Learning. Ruihao Li, Sen Wang, Zhiqiang Long, Dongbing Gu, In ICRA. Ruihao Li, Sen Wang, Zhiqiang Long, and Dongbing Gu. UndeepVO: Monocular Visual Odometry through Unsuper- vised Deep Learning. In ICRA, 2018.\n\nFayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence. 38Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields. IEEE Transactions on Pat- tern Analysis and Machine Intelligence, 38(10):2024-2039, 2016.\n\nUnsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints. Reza Mahjourian, Martin Wicke, Anelia Angelova, CVPR. Reza Mahjourian, Martin Wicke, and Anelia Angelova. Un- supervised Learning of Depth and Ego-Motion from Monoc- ular Video Using 3D Geometric Constraints. In CVPR, 2018.\n\nORB-SLAM: A Versatile and Accurate Monocular SLAM System. Raul Mur-Artal, Jose Maria Martinez Montiel, Juan D Tardos, IEEE Transactions on Robotics. 315Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. ORB-SLAM: A Versatile and Accurate Monoc- ular SLAM System. IEEE Transactions on Robotics, 31(5):1147-1163, 2015.\n\nKinectFusion: Real-Time Dense Surface Mapping and Tracking. Shahram Richard A Newcombe, Otmar Izadi, David Hilliges, David Molyneaux, Kim, J Andrew, Pushmeet Davison, Kohi, ISMAR. Jamie Shotton, Steve Hodges, and Andrew FitzgibbonRichard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J Davison, Pushmeet Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgib- bon. KinectFusion: Real-Time Dense Surface Mapping and Tracking. In ISMAR, 2011.\n\nColored Point Cloud Registration Revisited. Jaesik Park, Yi Zhou, Vladlen Koltun, ICCV. Jaesik Park, Qian Yi Zhou, and Vladlen Koltun. Colored Point Cloud Registration Revisited. In ICCV, 2017.\n\n. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Pytorch, Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. PyTorch. https://github.com/pytorch/ pytorch, 2017.\n\nDeMoN: Depth and Motion Network for Learning Monocular Stereo. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox, CVPR. Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Niko- laus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMoN: Depth and Motion Network for Learning Monocular Stereo. In CVPR, 2017.\n\nDeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks. Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni, ICRA. Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks. In ICRA, 2017.\n\nImage Quality Assessment: from Error Visibility to Structural Similarity. Zhou Wang, Alan C Bovik, R Hamid, Sheikh, P Eero, Simoncelli, IEEE Transactions on Image Processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simon- celli, et al. Image Quality Assessment: from Error Visibility to Structural Similarity. IEEE Transactions on Image Pro- cessing, 13(4):600-612, 2004.\n\nGuided Feature Selection for Deep Visual Odometry. Fei Xue, Qiuyuan Wang, Xin Wang, Wei Dong, Junqiu Wang, Hongbin Zha, ACCV. Fei Xue, Qiuyuan Wang, Xin Wang, Wei Dong, Junqiu Wang, and Hongbin Zha. Guided Feature Selection for Deep Visual Odometry. In ACCV, 2018.\n\nBeyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry. Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, Hongbin Zha, CVPR. Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, and Hongbin Zha. Beyond Tracking: Selecting Mem- ory and Refining Poses for Deep Visual Odometry. In CVPR, 2019.\n\nDeep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry. Nan Yang, Rui Wang, Jorg Stuckler, Daniel Cremers, ECCV. Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers. Deep Virtual Stereo Odometry: Leveraging Deep Depth Pre- diction for Monocular Direct Sparse Odometry. In ECCV, 2018.\n\nGeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose. Zhichao Yin, Jianping Shi, CVPR. Zhichao Yin and Jianping Shi. GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose. In CVPR, 2018.\n\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction. Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, Ian Reid, CVPR. Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised Learn- ing of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction. In CVPR, 2018.\n\nDeepTAM: Deep Tracking and Mapping. Huizhong Zhou, Benjamin Ummenhofer, Thomas Brox, ECCV. Huizhong Zhou, Benjamin Ummenhofer, and Thomas Brox. DeepTAM: Deep Tracking and Mapping. In ECCV, 2018.\n\nUnsupervised Learning of Depth and Ego-Motion from Video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, CVPR. Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised Learning of Depth and Ego-Motion from Video. In CVPR, 2017.\n\nUnpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. Taesung Jun Yan Zhu, Phillip Park, Alexei A Isola, Efros, Jun Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-Image Translation Using Cycle- Consistent Adversarial Networks. In ICCV, 2017.\n", "annotations": {"author": "[{\"end\":204,\"start\":76},{\"end\":330,\"start\":205},{\"end\":457,\"start\":331},{\"end\":622,\"start\":458},{\"end\":752,\"start\":623}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":84},{\"end\":212,\"start\":209},{\"end\":339,\"start\":335},{\"end\":466,\"start\":463},{\"end\":634,\"start\":631}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":208,\"start\":205},{\"end\":334,\"start\":331},{\"end\":462,\"start\":458},{\"end\":630,\"start\":623}]", "author_affiliation": "[{\"end\":203,\"start\":88},{\"end\":329,\"start\":214},{\"end\":456,\"start\":341},{\"end\":621,\"start\":506},{\"end\":751,\"start\":636}]", "title": "[{\"end\":73,\"start\":1},{\"end\":825,\"start\":753}]", "venue": null, "abstract": "[{\"end\":2252,\"start\":850}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2418,\"start\":2415},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2433,\"start\":2429},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2469,\"start\":2465},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2693,\"start\":2689},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2696,\"start\":2693},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2699,\"start\":2696},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2702,\"start\":2699},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2705,\"start\":2702},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3496,\"start\":3492},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3502,\"start\":3499},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3505,\"start\":3502},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3508,\"start\":3505},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3511,\"start\":3508},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3750,\"start\":3746},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3753,\"start\":3750},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5423,\"start\":5419},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7354,\"start\":7350},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7372,\"start\":7368},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7800,\"start\":7796},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7950,\"start\":7946},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8015,\"start\":8011},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8567,\"start\":8564},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8902,\"start\":8898},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9363,\"start\":9359},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9366,\"start\":9363},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9393,\"start\":9390},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9395,\"start\":9393},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9398,\"start\":9395},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9401,\"start\":9398},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9404,\"start\":9401},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9429,\"start\":9425},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9471,\"start\":9468},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9474,\"start\":9471},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9500,\"start\":9496},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9893,\"start\":9889},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11433,\"start\":11429},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12089,\"start\":12085},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13096,\"start\":13092},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13401,\"start\":13398},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14144,\"start\":14140},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14147,\"start\":14144},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14975,\"start\":14971},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15098,\"start\":15094},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15501,\"start\":15497},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16455,\"start\":16451},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16458,\"start\":16455},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16783,\"start\":16780},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16786,\"start\":16783},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16789,\"start\":16786},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17589,\"start\":17585},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17895,\"start\":17891},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18219,\"start\":18215},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18413,\"start\":18409},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18640,\"start\":18637},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18643,\"start\":18640},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19382,\"start\":19378},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20617,\"start\":20613},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21170,\"start\":21166},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21890,\"start\":21887},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21968,\"start\":21964},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22482,\"start\":22478},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22908,\"start\":22904},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23061,\"start\":23058},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23219,\"start\":23215},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24502,\"start\":24498},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24505,\"start\":24502},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24729,\"start\":24725},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24742,\"start\":24738},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24758,\"start\":24754},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24776,\"start\":24772},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25271,\"start\":25267},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25656,\"start\":25652}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29434,\"start\":29023},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29581,\"start\":29435},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29743,\"start\":29582},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30019,\"start\":29744},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30153,\"start\":30020},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30353,\"start\":30154},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32015,\"start\":30354},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32576,\"start\":32016}]", "paragraph": "[{\"end\":3512,\"start\":2268},{\"end\":4015,\"start\":3514},{\"end\":4988,\"start\":4017},{\"end\":5809,\"start\":4990},{\"end\":6376,\"start\":5811},{\"end\":6532,\"start\":6378},{\"end\":6678,\"start\":6534},{\"end\":7044,\"start\":6680},{\"end\":7668,\"start\":7062},{\"end\":8633,\"start\":7670},{\"end\":9691,\"start\":8635},{\"end\":10244,\"start\":9693},{\"end\":10737,\"start\":10246},{\"end\":10856,\"start\":10748},{\"end\":11238,\"start\":10868},{\"end\":11525,\"start\":11240},{\"end\":11675,\"start\":11555},{\"end\":12053,\"start\":11714},{\"end\":12624,\"start\":12055},{\"end\":12799,\"start\":12657},{\"end\":12897,\"start\":12820},{\"end\":13551,\"start\":12899},{\"end\":13806,\"start\":13576},{\"end\":14288,\"start\":13835},{\"end\":14976,\"start\":14335},{\"end\":15549,\"start\":15016},{\"end\":16596,\"start\":15623},{\"end\":17023,\"start\":16598},{\"end\":17818,\"start\":17063},{\"end\":17896,\"start\":17820},{\"end\":18251,\"start\":18026},{\"end\":18742,\"start\":18296},{\"end\":19384,\"start\":18817},{\"end\":19696,\"start\":19467},{\"end\":20075,\"start\":19830},{\"end\":21069,\"start\":20102},{\"end\":21683,\"start\":21071},{\"end\":22423,\"start\":21685},{\"end\":23141,\"start\":22444},{\"end\":24261,\"start\":23143},{\"end\":24563,\"start\":24281},{\"end\":25272,\"start\":24565},{\"end\":25790,\"start\":25274},{\"end\":26122,\"start\":25811},{\"end\":27136,\"start\":26124},{\"end\":27955,\"start\":27138},{\"end\":28579,\"start\":27971},{\"end\":29022,\"start\":28581}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11554,\"start\":11526},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12656,\"start\":12625},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13575,\"start\":13552},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14334,\"start\":14289},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15015,\"start\":14977},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15606,\"start\":15550},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17062,\"start\":17024},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18008,\"start\":17897},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18295,\"start\":18252},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18816,\"start\":18743},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19466,\"start\":19385},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19753,\"start\":19697},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19815,\"start\":19753}]", "table_ref": "[{\"end\":21083,\"start\":21076},{\"end\":23316,\"start\":23309},{\"end\":23570,\"start\":23563},{\"end\":25293,\"start\":25286},{\"end\":26107,\"start\":26100},{\"end\":26772,\"start\":26765},{\"end\":26967,\"start\":26960},{\"end\":27171,\"start\":27164},{\"end\":27281,\"start\":27273}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2266,\"start\":2254},{\"attributes\":{\"n\":\"2.\"},\"end\":7060,\"start\":7047},{\"attributes\":{\"n\":\"3.\"},\"end\":10746,\"start\":10740},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10866,\"start\":10859},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11712,\"start\":11678},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12818,\"start\":12802},{\"attributes\":{\"n\":\"3.4.\"},\"end\":13833,\"start\":13809},{\"attributes\":{\"n\":\"3.5.\"},\"end\":15621,\"start\":15608},{\"attributes\":{\"n\":\"3.6.\"},\"end\":18024,\"start\":18010},{\"attributes\":{\"n\":\"4.\"},\"end\":19828,\"start\":19817},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20100,\"start\":20078},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22442,\"start\":22426},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24279,\"start\":24264},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25809,\"start\":25793},{\"attributes\":{\"n\":\"5.\"},\"end\":27969,\"start\":27958},{\"end\":29446,\"start\":29436},{\"end\":29590,\"start\":29583},{\"end\":29755,\"start\":29745},{\"end\":30031,\"start\":30021},{\"end\":30165,\"start\":30155}]", "table": "[{\"end\":32015,\"start\":30456},{\"end\":32576,\"start\":32198}]", "figure_caption": "[{\"end\":29434,\"start\":29025},{\"end\":29581,\"start\":29448},{\"end\":29743,\"start\":29592},{\"end\":30019,\"start\":29757},{\"end\":30153,\"start\":30033},{\"end\":30353,\"start\":30167},{\"end\":30456,\"start\":30356},{\"end\":32198,\"start\":32018}]", "figure_ref": "[{\"end\":2743,\"start\":2735},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5095,\"start\":5087},{\"end\":5856,\"start\":5850},{\"end\":14456,\"start\":14448},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15719,\"start\":15713},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16396,\"start\":16388},{\"end\":20120,\"start\":20114},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23778,\"start\":23772},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25414,\"start\":25408},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26121,\"start\":26115},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26146,\"start\":26136}]", "bib_author_first_name": "[{\"end\":32850,\"start\":32843},{\"end\":32865,\"start\":32860},{\"end\":32878,\"start\":32872},{\"end\":32893,\"start\":32886},{\"end\":33123,\"start\":33117},{\"end\":33141,\"start\":33134},{\"end\":33156,\"start\":33152},{\"end\":33383,\"start\":33382},{\"end\":33395,\"start\":33390},{\"end\":33413,\"start\":33406},{\"end\":33425,\"start\":33419},{\"end\":33738,\"start\":33731},{\"end\":33758,\"start\":33753},{\"end\":33770,\"start\":33764},{\"end\":34068,\"start\":34065},{\"end\":34081,\"start\":34077},{\"end\":34099,\"start\":34091},{\"end\":34114,\"start\":34108},{\"end\":34416,\"start\":34409},{\"end\":34429,\"start\":34426},{\"end\":34448,\"start\":34442},{\"end\":34462,\"start\":34456},{\"end\":34484,\"start\":34476},{\"end\":34771,\"start\":34765},{\"end\":34781,\"start\":34778},{\"end\":34793,\"start\":34788},{\"end\":34815,\"start\":34806},{\"end\":35098,\"start\":35092},{\"end\":35114,\"start\":35107},{\"end\":35131,\"start\":35122},{\"end\":35143,\"start\":35139},{\"end\":35159,\"start\":35153},{\"end\":35178,\"start\":35171},{\"end\":35483,\"start\":35477},{\"end\":35504,\"start\":35497},{\"end\":35518,\"start\":35514},{\"end\":35530,\"start\":35524},{\"end\":35545,\"start\":35540},{\"end\":35564,\"start\":35556},{\"end\":35580,\"start\":35573},{\"end\":35596,\"start\":35590},{\"end\":35610,\"start\":35604},{\"end\":35946,\"start\":35941},{\"end\":35963,\"start\":35954},{\"end\":35976,\"start\":35973},{\"end\":36159,\"start\":36154},{\"end\":36174,\"start\":36167},{\"end\":36189,\"start\":36183},{\"end\":36475,\"start\":36470},{\"end\":36489,\"start\":36483},{\"end\":36504,\"start\":36498},{\"end\":36695,\"start\":36689},{\"end\":36955,\"start\":36946},{\"end\":36970,\"start\":36965},{\"end\":36985,\"start\":36978},{\"end\":36999,\"start\":36993},{\"end\":37223,\"start\":37214},{\"end\":37238,\"start\":37233},{\"end\":37254,\"start\":37248},{\"end\":37271,\"start\":37267},{\"end\":37485,\"start\":37481},{\"end\":37497,\"start\":37492},{\"end\":37506,\"start\":37505},{\"end\":37508,\"start\":37507},{\"end\":37518,\"start\":37511},{\"end\":37532,\"start\":37529},{\"end\":37773,\"start\":37766},{\"end\":37787,\"start\":37782},{\"end\":37806,\"start\":37799},{\"end\":37808,\"start\":37807},{\"end\":37970,\"start\":37967},{\"end\":37987,\"start\":37983},{\"end\":38008,\"start\":38003},{\"end\":38020,\"start\":38016},{\"end\":38030,\"start\":38025},{\"end\":38052,\"start\":38045},{\"end\":38065,\"start\":38060},{\"end\":38083,\"start\":38077},{\"end\":38371,\"start\":38370},{\"end\":38384,\"start\":38378},{\"end\":38558,\"start\":38554},{\"end\":38577,\"start\":38571},{\"end\":38793,\"start\":38786},{\"end\":38807,\"start\":38801},{\"end\":38820,\"start\":38813},{\"end\":38833,\"start\":38827},{\"end\":38835,\"start\":38834},{\"end\":39072,\"start\":39066},{\"end\":39086,\"start\":39079},{\"end\":39101,\"start\":39095},{\"end\":39116,\"start\":39109},{\"end\":39130,\"start\":39126},{\"end\":39377,\"start\":39368},{\"end\":39390,\"start\":39384},{\"end\":39404,\"start\":39398},{\"end\":39568,\"start\":39567},{\"end\":39584,\"start\":39579},{\"end\":39749,\"start\":39744},{\"end\":39762,\"start\":39757},{\"end\":39954,\"start\":39948},{\"end\":39962,\"start\":39959},{\"end\":39977,\"start\":39969},{\"end\":39992,\"start\":39984},{\"end\":40154,\"start\":40149},{\"end\":40167,\"start\":40160},{\"end\":40182,\"start\":40174},{\"end\":40191,\"start\":40188},{\"end\":40679,\"start\":40675},{\"end\":40698,\"start\":40692},{\"end\":40712,\"start\":40706},{\"end\":40962,\"start\":40958},{\"end\":40993,\"start\":40974},{\"end\":41007,\"start\":41003},{\"end\":41009,\"start\":41008},{\"end\":41298,\"start\":41291},{\"end\":41324,\"start\":41319},{\"end\":41337,\"start\":41332},{\"end\":41353,\"start\":41348},{\"end\":41371,\"start\":41370},{\"end\":41388,\"start\":41380},{\"end\":41754,\"start\":41748},{\"end\":41763,\"start\":41761},{\"end\":41777,\"start\":41770},{\"end\":41905,\"start\":41901},{\"end\":41917,\"start\":41914},{\"end\":41932,\"start\":41925},{\"end\":41950,\"start\":41943},{\"end\":42154,\"start\":42146},{\"end\":42175,\"start\":42167},{\"end\":42187,\"start\":42182},{\"end\":42203,\"start\":42195},{\"end\":42215,\"start\":42211},{\"end\":42227,\"start\":42221},{\"end\":42247,\"start\":42241},{\"end\":42550,\"start\":42547},{\"end\":42563,\"start\":42557},{\"end\":42578,\"start\":42571},{\"end\":42588,\"start\":42584},{\"end\":42847,\"start\":42843},{\"end\":42858,\"start\":42854},{\"end\":42860,\"start\":42859},{\"end\":42869,\"start\":42868},{\"end\":42886,\"start\":42885},{\"end\":43207,\"start\":43204},{\"end\":43220,\"start\":43213},{\"end\":43230,\"start\":43227},{\"end\":43240,\"start\":43237},{\"end\":43253,\"start\":43247},{\"end\":43267,\"start\":43260},{\"end\":43501,\"start\":43498},{\"end\":43510,\"start\":43507},{\"end\":43524,\"start\":43517},{\"end\":43536,\"start\":43529},{\"end\":43549,\"start\":43543},{\"end\":43563,\"start\":43556},{\"end\":43851,\"start\":43848},{\"end\":43861,\"start\":43858},{\"end\":43872,\"start\":43868},{\"end\":43889,\"start\":43883},{\"end\":44162,\"start\":44155},{\"end\":44176,\"start\":44168},{\"end\":44425,\"start\":44416},{\"end\":44436,\"start\":44432},{\"end\":44450,\"start\":44443},{\"end\":44475,\"start\":44470},{\"end\":44485,\"start\":44480},{\"end\":44498,\"start\":44495},{\"end\":44772,\"start\":44764},{\"end\":44787,\"start\":44779},{\"end\":44806,\"start\":44800},{\"end\":44989,\"start\":44982},{\"end\":45003,\"start\":44996},{\"end\":45015,\"start\":45011},{\"end\":45032,\"start\":45025},{\"end\":45268,\"start\":45261},{\"end\":45289,\"start\":45282},{\"end\":45302,\"start\":45296},{\"end\":45304,\"start\":45303}]", "bib_author_last_name": "[{\"end\":32858,\"start\":32851},{\"end\":32870,\"start\":32866},{\"end\":32884,\"start\":32879},{\"end\":32903,\"start\":32894},{\"end\":33132,\"start\":33124},{\"end\":33150,\"start\":33142},{\"end\":33163,\"start\":33157},{\"end\":33388,\"start\":33384},{\"end\":33404,\"start\":33396},{\"end\":33417,\"start\":33414},{\"end\":33431,\"start\":33426},{\"end\":33751,\"start\":33739},{\"end\":33762,\"start\":33759},{\"end\":33779,\"start\":33771},{\"end\":33786,\"start\":33781},{\"end\":34075,\"start\":34069},{\"end\":34089,\"start\":34082},{\"end\":34106,\"start\":34100},{\"end\":34121,\"start\":34115},{\"end\":34424,\"start\":34417},{\"end\":34440,\"start\":34430},{\"end\":34454,\"start\":34449},{\"end\":34474,\"start\":34463},{\"end\":34492,\"start\":34485},{\"end\":34776,\"start\":34772},{\"end\":34786,\"start\":34782},{\"end\":34804,\"start\":34794},{\"end\":34820,\"start\":34816},{\"end\":35105,\"start\":35099},{\"end\":35120,\"start\":35115},{\"end\":35137,\"start\":35132},{\"end\":35151,\"start\":35144},{\"end\":35169,\"start\":35160},{\"end\":35187,\"start\":35179},{\"end\":35495,\"start\":35484},{\"end\":35512,\"start\":35505},{\"end\":35522,\"start\":35519},{\"end\":35538,\"start\":35531},{\"end\":35554,\"start\":35546},{\"end\":35571,\"start\":35565},{\"end\":35588,\"start\":35581},{\"end\":35602,\"start\":35597},{\"end\":35618,\"start\":35611},{\"end\":35624,\"start\":35620},{\"end\":35952,\"start\":35947},{\"end\":35971,\"start\":35964},{\"end\":35983,\"start\":35977},{\"end\":36165,\"start\":36160},{\"end\":36181,\"start\":36175},{\"end\":36197,\"start\":36190},{\"end\":36481,\"start\":36476},{\"end\":36496,\"start\":36490},{\"end\":36512,\"start\":36505},{\"end\":36704,\"start\":36696},{\"end\":36963,\"start\":36956},{\"end\":36976,\"start\":36971},{\"end\":36991,\"start\":36986},{\"end\":37010,\"start\":37000},{\"end\":37231,\"start\":37224},{\"end\":37246,\"start\":37239},{\"end\":37265,\"start\":37255},{\"end\":37274,\"start\":37272},{\"end\":37490,\"start\":37486},{\"end\":37503,\"start\":37498},{\"end\":37527,\"start\":37519},{\"end\":37537,\"start\":37533},{\"end\":37780,\"start\":37774},{\"end\":37797,\"start\":37788},{\"end\":37816,\"start\":37809},{\"end\":37981,\"start\":37971},{\"end\":38001,\"start\":37988},{\"end\":38014,\"start\":38009},{\"end\":38023,\"start\":38021},{\"end\":38043,\"start\":38031},{\"end\":38058,\"start\":38053},{\"end\":38075,\"start\":38066},{\"end\":38090,\"start\":38084},{\"end\":38376,\"start\":38372},{\"end\":38394,\"start\":38385},{\"end\":38403,\"start\":38396},{\"end\":38569,\"start\":38559},{\"end\":38589,\"start\":38578},{\"end\":38799,\"start\":38794},{\"end\":38811,\"start\":38808},{\"end\":38825,\"start\":38821},{\"end\":38841,\"start\":38836},{\"end\":39077,\"start\":39073},{\"end\":39093,\"start\":39087},{\"end\":39107,\"start\":39102},{\"end\":39124,\"start\":39117},{\"end\":39136,\"start\":39131},{\"end\":39382,\"start\":39378},{\"end\":39396,\"start\":39391},{\"end\":39412,\"start\":39405},{\"end\":39577,\"start\":39569},{\"end\":39591,\"start\":39585},{\"end\":39595,\"start\":39593},{\"end\":39755,\"start\":39750},{\"end\":39769,\"start\":39763},{\"end\":39957,\"start\":39955},{\"end\":39967,\"start\":39963},{\"end\":39982,\"start\":39978},{\"end\":39995,\"start\":39993},{\"end\":40158,\"start\":40155},{\"end\":40172,\"start\":40168},{\"end\":40186,\"start\":40183},{\"end\":40196,\"start\":40192},{\"end\":40690,\"start\":40680},{\"end\":40704,\"start\":40699},{\"end\":40721,\"start\":40713},{\"end\":40972,\"start\":40963},{\"end\":41001,\"start\":40994},{\"end\":41016,\"start\":41010},{\"end\":41317,\"start\":41299},{\"end\":41330,\"start\":41325},{\"end\":41346,\"start\":41338},{\"end\":41363,\"start\":41354},{\"end\":41368,\"start\":41365},{\"end\":41378,\"start\":41372},{\"end\":41396,\"start\":41389},{\"end\":41402,\"start\":41398},{\"end\":41759,\"start\":41755},{\"end\":41768,\"start\":41764},{\"end\":41784,\"start\":41778},{\"end\":41912,\"start\":41906},{\"end\":41923,\"start\":41918},{\"end\":41941,\"start\":41933},{\"end\":41957,\"start\":41951},{\"end\":41966,\"start\":41959},{\"end\":42165,\"start\":42155},{\"end\":42180,\"start\":42176},{\"end\":42193,\"start\":42188},{\"end\":42209,\"start\":42204},{\"end\":42219,\"start\":42216},{\"end\":42239,\"start\":42228},{\"end\":42252,\"start\":42248},{\"end\":42555,\"start\":42551},{\"end\":42569,\"start\":42564},{\"end\":42582,\"start\":42579},{\"end\":42596,\"start\":42589},{\"end\":42852,\"start\":42848},{\"end\":42866,\"start\":42861},{\"end\":42875,\"start\":42870},{\"end\":42883,\"start\":42877},{\"end\":42891,\"start\":42887},{\"end\":42903,\"start\":42893},{\"end\":43211,\"start\":43208},{\"end\":43225,\"start\":43221},{\"end\":43235,\"start\":43231},{\"end\":43245,\"start\":43241},{\"end\":43258,\"start\":43254},{\"end\":43271,\"start\":43268},{\"end\":43505,\"start\":43502},{\"end\":43515,\"start\":43511},{\"end\":43527,\"start\":43525},{\"end\":43541,\"start\":43537},{\"end\":43554,\"start\":43550},{\"end\":43567,\"start\":43564},{\"end\":43856,\"start\":43852},{\"end\":43866,\"start\":43862},{\"end\":43881,\"start\":43873},{\"end\":43897,\"start\":43890},{\"end\":44166,\"start\":44163},{\"end\":44180,\"start\":44177},{\"end\":44430,\"start\":44426},{\"end\":44441,\"start\":44437},{\"end\":44468,\"start\":44451},{\"end\":44478,\"start\":44476},{\"end\":44493,\"start\":44486},{\"end\":44503,\"start\":44499},{\"end\":44777,\"start\":44773},{\"end\":44798,\"start\":44788},{\"end\":44811,\"start\":44807},{\"end\":44994,\"start\":44990},{\"end\":45009,\"start\":45004},{\"end\":45023,\"start\":45016},{\"end\":45037,\"start\":45033},{\"end\":45280,\"start\":45269},{\"end\":45294,\"start\":45290},{\"end\":45310,\"start\":45305},{\"end\":45317,\"start\":45312}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":59159133},\"end\":33070,\"start\":32766},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2057420},\"end\":33282,\"start\":33072},{\"attributes\":{\"doi\":\"arXiv:1809.00969\",\"id\":\"b2\"},\"end\":33657,\"start\":33284},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52153565},\"end\":33944,\"start\":33659},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3681715},\"end\":34327,\"start\":33946},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4624670},\"end\":34685,\"start\":34329},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15693605},\"end\":34983,\"start\":34687},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":502946},\"end\":35415,\"start\":34985},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12552176},\"end\":35864,\"start\":35417},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2255738},\"end\":36128,\"start\":35866},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3299195},\"end\":36423,\"start\":36130},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14547347},\"end\":36628,\"start\":36425},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15601477},\"end\":36878,\"start\":36630},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12227},\"end\":37168,\"start\":36880},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206850490},\"end\":37404,\"start\":37170},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":299085},\"end\":37695,\"start\":37406},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206596513},\"end\":37963,\"start\":37697},{\"attributes\":{\"id\":\"b17\"},\"end\":38304,\"start\":37965},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4651026},\"end\":38528,\"start\":38306},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1915014},\"end\":38718,\"start\":38530},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6200260},\"end\":38994,\"start\":38720},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4854033},\"end\":39329,\"start\":38996},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10038098},\"end\":39521,\"start\":39331},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6628106},\"end\":39693,\"start\":39523},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13136001},\"end\":39874,\"start\":39695},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206853077},\"end\":40147,\"start\":39876},{\"attributes\":{\"id\":\"b26\"},\"end\":40574,\"start\":40149},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3645349},\"end\":40898,\"start\":40576},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206775100},\"end\":41229,\"start\":40900},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11830123},\"end\":41702,\"start\":41231},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":605652},\"end\":41897,\"start\":41704},{\"attributes\":{\"id\":\"b31\"},\"end\":42081,\"start\":41899},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6159584},\"end\":42451,\"start\":42083},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":9114952},\"end\":42767,\"start\":42453},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":207761262},\"end\":43151,\"start\":42769},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53776191},\"end\":43417,\"start\":43153},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":102351155},\"end\":43745,\"start\":43419},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":49658377},\"end\":44077,\"start\":43747},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3714620},\"end\":44308,\"start\":44079},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4578162},\"end\":44726,\"start\":44310},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51929808},\"end\":44922,\"start\":44728},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":11977588},\"end\":45178,\"start\":44924},{\"attributes\":{\"id\":\"b42\"},\"end\":45478,\"start\":45180}]", "bib_title": "[{\"end\":32841,\"start\":32766},{\"end\":33115,\"start\":33072},{\"end\":33729,\"start\":33659},{\"end\":34063,\"start\":33946},{\"end\":34407,\"start\":34329},{\"end\":34763,\"start\":34687},{\"end\":35090,\"start\":34985},{\"end\":35475,\"start\":35417},{\"end\":35939,\"start\":35866},{\"end\":36152,\"start\":36130},{\"end\":36468,\"start\":36425},{\"end\":36687,\"start\":36630},{\"end\":36944,\"start\":36880},{\"end\":37212,\"start\":37170},{\"end\":37479,\"start\":37406},{\"end\":37764,\"start\":37697},{\"end\":38368,\"start\":38306},{\"end\":38552,\"start\":38530},{\"end\":38784,\"start\":38720},{\"end\":39064,\"start\":38996},{\"end\":39366,\"start\":39331},{\"end\":39565,\"start\":39523},{\"end\":39742,\"start\":39695},{\"end\":39946,\"start\":39876},{\"end\":40673,\"start\":40576},{\"end\":40956,\"start\":40900},{\"end\":41289,\"start\":41231},{\"end\":41746,\"start\":41704},{\"end\":42144,\"start\":42083},{\"end\":42545,\"start\":42453},{\"end\":42841,\"start\":42769},{\"end\":43202,\"start\":43153},{\"end\":43496,\"start\":43419},{\"end\":43846,\"start\":43747},{\"end\":44153,\"start\":44079},{\"end\":44414,\"start\":44310},{\"end\":44762,\"start\":44728},{\"end\":44980,\"start\":44924}]", "bib_author": "[{\"end\":32860,\"start\":32843},{\"end\":32872,\"start\":32860},{\"end\":32886,\"start\":32872},{\"end\":32905,\"start\":32886},{\"end\":33134,\"start\":33117},{\"end\":33152,\"start\":33134},{\"end\":33165,\"start\":33152},{\"end\":33390,\"start\":33382},{\"end\":33406,\"start\":33390},{\"end\":33419,\"start\":33406},{\"end\":33433,\"start\":33419},{\"end\":33753,\"start\":33731},{\"end\":33764,\"start\":33753},{\"end\":33781,\"start\":33764},{\"end\":33788,\"start\":33781},{\"end\":34077,\"start\":34065},{\"end\":34091,\"start\":34077},{\"end\":34108,\"start\":34091},{\"end\":34123,\"start\":34108},{\"end\":34426,\"start\":34409},{\"end\":34442,\"start\":34426},{\"end\":34456,\"start\":34442},{\"end\":34476,\"start\":34456},{\"end\":34494,\"start\":34476},{\"end\":34778,\"start\":34765},{\"end\":34788,\"start\":34778},{\"end\":34806,\"start\":34788},{\"end\":34822,\"start\":34806},{\"end\":35107,\"start\":35092},{\"end\":35122,\"start\":35107},{\"end\":35139,\"start\":35122},{\"end\":35153,\"start\":35139},{\"end\":35171,\"start\":35153},{\"end\":35189,\"start\":35171},{\"end\":35497,\"start\":35477},{\"end\":35514,\"start\":35497},{\"end\":35524,\"start\":35514},{\"end\":35540,\"start\":35524},{\"end\":35556,\"start\":35540},{\"end\":35573,\"start\":35556},{\"end\":35590,\"start\":35573},{\"end\":35604,\"start\":35590},{\"end\":35620,\"start\":35604},{\"end\":35626,\"start\":35620},{\"end\":35954,\"start\":35941},{\"end\":35973,\"start\":35954},{\"end\":35985,\"start\":35973},{\"end\":36167,\"start\":36154},{\"end\":36183,\"start\":36167},{\"end\":36199,\"start\":36183},{\"end\":36483,\"start\":36470},{\"end\":36498,\"start\":36483},{\"end\":36514,\"start\":36498},{\"end\":36706,\"start\":36689},{\"end\":36965,\"start\":36946},{\"end\":36978,\"start\":36965},{\"end\":36993,\"start\":36978},{\"end\":37012,\"start\":36993},{\"end\":37233,\"start\":37214},{\"end\":37248,\"start\":37233},{\"end\":37267,\"start\":37248},{\"end\":37276,\"start\":37267},{\"end\":37492,\"start\":37481},{\"end\":37505,\"start\":37492},{\"end\":37511,\"start\":37505},{\"end\":37529,\"start\":37511},{\"end\":37539,\"start\":37529},{\"end\":37782,\"start\":37766},{\"end\":37799,\"start\":37782},{\"end\":37818,\"start\":37799},{\"end\":37983,\"start\":37967},{\"end\":38003,\"start\":37983},{\"end\":38016,\"start\":38003},{\"end\":38025,\"start\":38016},{\"end\":38045,\"start\":38025},{\"end\":38060,\"start\":38045},{\"end\":38077,\"start\":38060},{\"end\":38092,\"start\":38077},{\"end\":38378,\"start\":38370},{\"end\":38396,\"start\":38378},{\"end\":38405,\"start\":38396},{\"end\":38571,\"start\":38554},{\"end\":38591,\"start\":38571},{\"end\":38801,\"start\":38786},{\"end\":38813,\"start\":38801},{\"end\":38827,\"start\":38813},{\"end\":38843,\"start\":38827},{\"end\":39079,\"start\":39066},{\"end\":39095,\"start\":39079},{\"end\":39109,\"start\":39095},{\"end\":39126,\"start\":39109},{\"end\":39138,\"start\":39126},{\"end\":39384,\"start\":39368},{\"end\":39398,\"start\":39384},{\"end\":39414,\"start\":39398},{\"end\":39579,\"start\":39567},{\"end\":39593,\"start\":39579},{\"end\":39597,\"start\":39593},{\"end\":39757,\"start\":39744},{\"end\":39771,\"start\":39757},{\"end\":39959,\"start\":39948},{\"end\":39969,\"start\":39959},{\"end\":39984,\"start\":39969},{\"end\":39997,\"start\":39984},{\"end\":40160,\"start\":40149},{\"end\":40174,\"start\":40160},{\"end\":40188,\"start\":40174},{\"end\":40198,\"start\":40188},{\"end\":40692,\"start\":40675},{\"end\":40706,\"start\":40692},{\"end\":40723,\"start\":40706},{\"end\":40974,\"start\":40958},{\"end\":41003,\"start\":40974},{\"end\":41018,\"start\":41003},{\"end\":41319,\"start\":41291},{\"end\":41332,\"start\":41319},{\"end\":41348,\"start\":41332},{\"end\":41365,\"start\":41348},{\"end\":41370,\"start\":41365},{\"end\":41380,\"start\":41370},{\"end\":41398,\"start\":41380},{\"end\":41404,\"start\":41398},{\"end\":41761,\"start\":41748},{\"end\":41770,\"start\":41761},{\"end\":41786,\"start\":41770},{\"end\":41914,\"start\":41901},{\"end\":41925,\"start\":41914},{\"end\":41943,\"start\":41925},{\"end\":41959,\"start\":41943},{\"end\":41968,\"start\":41959},{\"end\":42167,\"start\":42146},{\"end\":42182,\"start\":42167},{\"end\":42195,\"start\":42182},{\"end\":42211,\"start\":42195},{\"end\":42221,\"start\":42211},{\"end\":42241,\"start\":42221},{\"end\":42254,\"start\":42241},{\"end\":42557,\"start\":42547},{\"end\":42571,\"start\":42557},{\"end\":42584,\"start\":42571},{\"end\":42598,\"start\":42584},{\"end\":42854,\"start\":42843},{\"end\":42868,\"start\":42854},{\"end\":42877,\"start\":42868},{\"end\":42885,\"start\":42877},{\"end\":42893,\"start\":42885},{\"end\":42905,\"start\":42893},{\"end\":43213,\"start\":43204},{\"end\":43227,\"start\":43213},{\"end\":43237,\"start\":43227},{\"end\":43247,\"start\":43237},{\"end\":43260,\"start\":43247},{\"end\":43273,\"start\":43260},{\"end\":43507,\"start\":43498},{\"end\":43517,\"start\":43507},{\"end\":43529,\"start\":43517},{\"end\":43543,\"start\":43529},{\"end\":43556,\"start\":43543},{\"end\":43569,\"start\":43556},{\"end\":43858,\"start\":43848},{\"end\":43868,\"start\":43858},{\"end\":43883,\"start\":43868},{\"end\":43899,\"start\":43883},{\"end\":44168,\"start\":44155},{\"end\":44182,\"start\":44168},{\"end\":44432,\"start\":44416},{\"end\":44443,\"start\":44432},{\"end\":44470,\"start\":44443},{\"end\":44480,\"start\":44470},{\"end\":44495,\"start\":44480},{\"end\":44505,\"start\":44495},{\"end\":44779,\"start\":44764},{\"end\":44800,\"start\":44779},{\"end\":44813,\"start\":44800},{\"end\":44996,\"start\":44982},{\"end\":45011,\"start\":44996},{\"end\":45025,\"start\":45011},{\"end\":45039,\"start\":45025},{\"end\":45282,\"start\":45261},{\"end\":45296,\"start\":45282},{\"end\":45312,\"start\":45296},{\"end\":45319,\"start\":45312}]", "bib_venue": "[{\"end\":32909,\"start\":32905},{\"end\":33169,\"start\":33165},{\"end\":33380,\"start\":33284},{\"end\":33795,\"start\":33788},{\"end\":34130,\"start\":34123},{\"end\":34498,\"start\":34494},{\"end\":34826,\"start\":34822},{\"end\":35193,\"start\":35189},{\"end\":35630,\"start\":35626},{\"end\":35989,\"start\":35985},{\"end\":36261,\"start\":36199},{\"end\":36518,\"start\":36514},{\"end\":36747,\"start\":36706},{\"end\":37016,\"start\":37012},{\"end\":37280,\"start\":37276},{\"end\":37543,\"start\":37539},{\"end\":37822,\"start\":37818},{\"end\":38128,\"start\":38092},{\"end\":38409,\"start\":38405},{\"end\":38609,\"start\":38591},{\"end\":38847,\"start\":38843},{\"end\":39152,\"start\":39138},{\"end\":39418,\"start\":39414},{\"end\":39601,\"start\":39597},{\"end\":39776,\"start\":39771},{\"end\":40004,\"start\":39997},{\"end\":40344,\"start\":40198},{\"end\":40727,\"start\":40723},{\"end\":41047,\"start\":41018},{\"end\":41409,\"start\":41404},{\"end\":41790,\"start\":41786},{\"end\":42258,\"start\":42254},{\"end\":42602,\"start\":42598},{\"end\":42942,\"start\":42905},{\"end\":43277,\"start\":43273},{\"end\":43573,\"start\":43569},{\"end\":43903,\"start\":43899},{\"end\":44186,\"start\":44182},{\"end\":44509,\"start\":44505},{\"end\":44817,\"start\":44813},{\"end\":45043,\"start\":45039},{\"end\":45259,\"start\":45180},{\"end\":41461,\"start\":41411}]"}}}, "year": 2023, "month": 12, "day": 17}