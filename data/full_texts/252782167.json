{"id": 252782167, "updated": "2022-10-15 13:23:25.7", "metadata": {"title": "Invariant Representation Learning for Multimedia Recommendation", "authors": "[{\"first\":\"Xiaoyu\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Zike\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Fuli\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Xiangnan\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Jinhui\",\"last\":\"Tang\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 30th ACM International Conference on Multimedia", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Multimedia recommendation forms a personalized ranking task with multimedia content representations which are mostly extracted via generic encoders. However, the generic representations introduce spurious correlations --- the meaningless correlation from the recommendation perspective. For example, suppose a user bought two dresses on the same model, this co-occurrence would produce a correlation between the model and purchases, but the correlation is spurious from the view of fashion recommendation. Existing work alleviates this issue by customizing preference-aware representations, requiring high-cost analysis and design. In this paper, we propose an Invariant Representation Learning Framework (InvRL) to alleviate the impact of the spurious correlations. We utilize environments to reflect the spurious correlations and determine each environment with a set of interactions. We then learn invariant representations --- the inherent factors attracting user attention --- to make a consistent prediction of user-item interaction across various environments. In this light, InvRL proposes two iteratively executed modules to cluster user-item interactions and learn invariant representations. With them, InvRL trains a final recommender model thus mitigating the spurious correlations. We demonstrate InvRL on a cutting-edge recommender model UltraGCN and conduct extensive experiments on three public multimedia recommendation datasets, Movielens, Tiktok, and Kwai. The experimental results validate the rationality and effectiveness of InvRL. Codes are released at https://github.com/nickwzk/InvRL.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/DuWF0022", "doi": "10.1145/3503161.3548405"}}, "content": {"source": {"pdf_hash": "6ed5ce6539d2e8c8aa61c8c37592730cc6cda356", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "05ec65a6b2494e16bc64f6c66ad26206c12fbdcd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6ed5ce6539d2e8c8aa61c8c37592730cc6cda356.txt", "contents": "\nIn-variant Representation Learning for Multimedia Recommendation\nACMCopyright ACMOctober 10-14, 2022. 2022. October 10-14, 2022\n\nXiaoyu Du \nZike Wu zikewu43@gmail.com \nFuli Feng fulifeng93@gmail.com \nXiangnan He xiangnanhe@gmail.com \nJinhui Tang jinhuitang@njust.edu.cn \nXiaoyu Du \nZike Wu \nFuli Feng \nXiangnan He \nJinhui Tang \n\nNanjing University of Science and Technology\nSouth China University of Technology\nUniversity of Science and Technology\nof China\n\n\nUniversity of Science and Technology\nof China\n\n\nNanjing University of Science and Technology\n\n\nIn-variant Representation Learning for Multimedia Recommendation\n\nPro-ceedings of the 30th ACM International Conference on Multimedia (MM '22)\nLisboa, Portugal; Lisboa, Portugal; New York, NY, USAACM10October 10-14, 2022. 2022. October 10-14, 202210.1145/3503161.3548405* This work is supported by the National Key Research and Development Program of China (2021ZD0111802), the National Natural Science Foundation of China (61925204 and 62172226), and the 2021 Jiangsu Shuangchuang (Mass Innovation and Entrepreneur-ship) Talent Program (JSSCBS20210200). Jinhui Tang is the corresponding author.\nMultimedia recommendation forms a personalized ranking task with multimedia content representations which are mostly extracted via generic encoders. However, the generic representations introduce spurious correlations -the meaningless correlation from the recommendation perspective. For example, suppose a user bought two dresses on the same model, this co-occurrence would produce a correlation between the model and purchases, but the correlation is spurious from the view of fashion recommendation. Existing work alleviates this issue by customizing preference-aware representations, requiring high-cost analysis and design.In this paper, we propose an Invariant Representation Learning Framework (InvRL) to alleviate the impact of the spurious correlations. We utilize environments to reflect the spurious correlations and determine each environment with a set of interactions. We then learn invariant representations -the inherent factors attracting user attention -to make a consistent prediction of user-item interaction across various environments. In this light, InvRL proposes two iteratively executed modules to cluster user-item interactions and learn invariant representations. With them, InvRL trains a final recommender model thus mitigating the spurious correlations. We demonstrate InvRL on a cutting-edge recommender model Ultra-GCN and conduct extensive experiments on three public multimedia recommendation datasets, Movielens, Tiktok, and Kwai. The experimental results validate the rationality and effectiveness of InvRL. Codes are released at https://github.com/nickwzk/InvRL.CCS CONCEPTS\u2022 Information systems \u2192 Multimedia information systems.\n\nINTRODUCTION\n\nMultimedia recommendation has become a core application in various online platforms for e-commerce [6], social media [41], video sharing [39], etc. Multimedia recommender models provide personalized services by learning user preferences from both historical interactions and multimedia item contents, including text, images, audio, and videos. The core is encoding the multimedia contents into proper semantic item representations matching user preferences, which is typically achieved by deep neural network-based encoders such as ResNet [10] and BERT [5]. These encoders are designed for generic content understanding tasks such as visual classification, objective recognition, and text classification, even pre-trained over datasets of these tasks.\n\nHowever, the generic content representations focus on general semantic information rather than the factors that attract user preferences, thus introducing spurious correlations [1], i.e., the irrelevant properties from the recommendation perspective. This will hinder the recommender model to capture genuine user preferences and provide accurate recommendations. Figure 1 exhibits two examples of the issue of spurious correlations. EX#1 is an e-commerce example, where a user recently viewed two dresses with clear style and pretty models as shown in the commodity photos. The generic visual encoder will extract both style and model representations, but the recommender can hardly differentiate the true preference over clothes style or model thus making accurate recommendations. EX#2 presents a movie sharing example, where a user watched two superhero movies both with tags of \"Hero\", \"Handsome\", and \"Fly\". Due to including more of these three tags, the recommender may prefer \"Harry Potter\" to \"Wonder Woman\". To alleviate the impact of spurious correlations, it is critical to remove the above preference-independent but strongly behavior-correlated factors from the multimedia representations. To approach this issue, existing researches mainly customize preference-aware representations [4,23,25,34,43]. On one hand, the existing methods avoid generic multimedia encoders and extract preference-aware representations with specifically designed multimedia models such as aesthetic-aware models [43] and styleaware models [25] for fashion recommendation, and modal-fusion models [31] for music recommendation. On the other hand, the existing methods adopt generic encoders and focus on blocking the effect of spurious multimedia representations [34]. Nevertheless, the existing methods face the limitation of domain-specific analysis and design, and thus can hardly be generalized. Therefore, we pursue a generic solution to automatically recognize preference-aware representation from the generic multimedia encoders.\n\nRecent advances in invariant risk minimization (IRM) [1] show that the spurious correlations in visual tasks are separable. They illustrate the example of classifying the cows and camels where the cow and the green pasture co-occurrence in many cases. As so, the cow and the green pasture form a spurious correlation: whether an animal is a camel or a cow would rely on whether the background is green. To address this issue, IRM aims to present invariant correlations that do not depend on the environment. Thus it learns invariant data representation by minimizing the max training error across multiple environments. EX#1 presents the spurious correlations between the dresses and the models. Intuitively, we can set two environments w/ and w/o models, and learn invariant item representations across them to obtain accurate preferenceaware representations. However, due to the too many unlabeled inherent factors impacting recommendation, to date, there is no study about invariant representation learning in the field of multimedia recommendation. Inspired by Heterogeneous Risk Minimization (HRM) [24], we aim to exploit a manner to partition the heterogeneous environments automatically.\n\nIn this paper, we propose an Invariant Representation Learning framework (InvRL) to alleviate the impact of spurious correlations. Specifically, we divide the raw multimedia representations into two parts: variant and invariant representations, where the variant representations lead to spurious correlations while the invariant representations reflect the real user preferences. We follow the concept of 'environment' in IRM [1] and form it as a subset of user behaviors that can be restored with the variant representations. In this light, we devise two iteratively executed modules to cluster the user-item interactions into heterogeneous environments and learn invariant representations across the environments. By repeating the two modules multiple times, the invariant representations are obtained to promote a final recommender model. To verify the effectiveness of InvRL, we instantiate InvRL over UltraGCN [28] and conduct extensive experiments on three public datasets, Movielens, Tiktok, and Kwai. The experimental results demonstrate that InvRL achieves state-of-the-art multimedia recommendation performance. This reveals the significance of the preference-aware invariant patterns. In addition, the ablation studies verify the existence and separability of the environments.\n\nTo summarize, the main contributions are as follows: (1) We reveal the issue of spurious correlations in multimedia recommendation and address the issue from the perspective of invariant learning, which is new for recommendation. (2) We propose a new multimedia recommendation framework named InvRL, which learns invariant item representations to alleviate the impact of spurious correlations. (3) We instantiate the framework over UltraGCN and conduct extensive experiments over three public datasets, verifying the existence and separability of spurious correlations, and validating the rationality and effectiveness of InvRL.\n\n\nMETHOD\n\nIn this section, we introduce the formulation of multimedia recommendation, the invariant learning perspective of blocking spurious correlations, and the proposed Invariant Representation Learning framework (InvRL). In this section, we use upper case bold letters to denote matrices, lower case bold letters to denote column vectors, non-bold letters to represent scalars, and calligraphic letters to represent sets. In addition, we adopt \u27e8\u00b7, \u00b7\u27e9, and \u2299 to indicate the inner product and element-wise product, respectively.\n\n\nProblem Formulation\n\n\u2022 Multimedia recommendation. This task aims to learn a recommender model \u0393( , , c |\u0398) to predict the preference of user on item with considerations of multimedia content representations c of the item, which is parameterized by \u0398. c is a representation vector that represents the multimedia contents of the item, including text, image, audio, and video. Without loss of generality, we assume c is extracted by generic deep neural network-based encoders 1 . A default choice to learn a multimedia recommender model is to optimize the parameters \u0398 over historical user-item interactions R = {( , )| \u2208 U \u2208 I = 1}, where U and I to denote the set of users and items, is a binary indicator of the interaction status. Formally, the optimization objective is:\narg min \u0398 L \u0393( , , c |\u0398)|R ,(1)\nwhere L (\u00b7) denotes a recommendation loss such as Binary Cross-Entropy. R = R R \u2212 denotes the training set where R \u2212 includes randomly selected negative samples with = 0. Note that we omit the regularization term (e.g., 2 -norm) for briefness which is commonly used for preventing overfitting.\n\n\u2022 Blocking spurious correlations. In an ideal case, all representations in c describe multimedia contents that match user preferences. However, c comes from generic encoders, which not only include preference-aware representations. Taking fashion recommendation as an example, c includes both representations of clothes (e.g., color and style) and representations of models (e.g., gender). Such redundant representations will result in spurious correlations (e.g., pretty model to interaction in Figure 1), when learning preferences from historical interactions. To pursue accurate recommendations, we set blocking the spurious correlation as an additional target for multimodal recommender learning.\n\n\nInvariant Learning for Recommendation\n\nInvariant learning [1,24] is an emerging technique for blocking spurious correlations. The key belief is that the spurious correlations are unstable across heterogeneous environments, e.g., the correlation between the background green grass and label cow is unstable across the images of natural photos and sketches. In this light, invariant learning pushes machine learning models to focus on invariant representation across environments, e.g., optimizing model parameters with an invariant risk minimization objective. Formally,\nL = E \u2208 E L + \u2225Var \u2208 E (\u2207 \u0398 L ) \u2225 2 ,(2)\nwhere the second term is the constraint over the variance across environments. L is an environment-specific loss:\nL = L \u0393( , , c })|\u0398)|R .(3)\nR is the set of training samples under the -th environment.\n\nThere are many natural environments in recommendation tasks, e.g., , regions for the localization-aware recommendation, genres for the movie recommendation, commodity categories for e-commerce recommendations, etc. However, these situations rely on specific scenarios thus requiring analysis and designs. Therefore, in this work, we propose the invariant representation learning framework for multimedia recommendation to partition the heterogeneous environments automatically and learn invariant representation across the environments.\n\n\nInvRL Framework\n\nHere we present the proposed framework InvRL. We first exhibit the workflow of our framework in Figure 2. There are six key modules (M1-M6) to generate the final model. M1 denotes the content extraction module which is done by pre-trained models. Therefore, InvRL actually starts from M2 which masks part of the content representations to capture the variant representations. According to that, M3 constructs independent interaction environments by dividing the original interactions into several subsets. Subsequently, M4 learns an invariant mask to make stable predictions across the subsets. The opposite of the invariant mask is the variant mask, which is fed into the upcoming M2 round. By executing the circle M2-M3-M4 for several times, we obtain stable environments and the invariant mask. Then, with M5 and M6, we train a final model on the invariant representations. We will illustrate M2 and M5 in Section 2.3.1, elaborate M3 in Section 2.3.2, present M4 in Section 2.3.3, and demonstrate M6 in Section 2.3.4.\n\n\nInvariant and Variant\n\nRepresentations. The modules M2 and M5 separate the raw multimedia representations into invariant and variant parts. Toward this target, we devise an invariant mask m \u2208 R to separate the raw multimedia content representation into invariant representation \u03a6 and \u03a8 , where is the dimension of the raw content representations. The invariant representation \u03a6 is defined as,\n\u03a6 = m \u2299 c .\n(4) After removing the invariant part, the rest is the variant representation \u03a8 , which is formulated as,\n\u03a8 = (1 \u2212 m) \u2299 c .(5)\nIn addition, we adopt \u03a6 = {\u03a6 | \u2208 I} and \u03a8 = {\u03a8 | \u2208 I} to demonstrate the sets of invariant and variant representations, respectively. Obviously, the key to the invariant item representations lies in the generation of the invariant mask m. It is optimized gradually during the recurrent modules M2-M3-M4. We elaborate on them within two procedures, Environment Partition and Mask Generation.\n\n\nEnvironment Partition. Environment\n\nPartition procedure relies on the module M3 shown in Figure 2, which takes in the observed user-item historical interactions and outputs an environment set E, where each environment \u2208 E reflects a type of spurious correlations between users and items. Taking the environments as references we separate the observed interactions into the environments. Correspondingly, we devise a two-phase partition approach.\n\n\u2022 Environment Learning Phase. Let R be the set of interactions in the environment , we now try to express the environment in this phase. Intuitively, the key differences among different environments are their viewpoints on spurious correlations. Thus we model the environment according to the spurious correlations. That is, for the interactions in the environment , we learn a predictive model \u0393 ( ) according to the variant representations \u03a8. In another word, to describe the environment , we learn the predictive model by,\narg min \u0398 L (\u0393 ( ) ( , , \u03a8 |\u0398 )|R ),(6)\nwhere \u0398 indicates the model parameters. Note that the environments are learned independently. They focus on different sets of interactions and gain different \u0393 ( ) . For more details about the backbone model \u0393, please refer to Section 2.3.5.\n\n\u2022 Interaction separation Phase. In this phase, we already have environments, which indicate different kinds of spurious correlations. Thus the interactions should be separated according to their spurious correlations. To differentiate the environments of the interactions ( , ), we adopt the following formula,\n( , ) = arg max \u2208 E \u0393 ( ) ( , , \u03a8 |\u0398 ).(7)\nThat means the interactions belong to the environment with the highest probability to recognize the interaction. Note that the prediction is based on the variant representations \u03a8, which expresses spurious information. Finally, the two phases are run alternatively till converged. Then the interaction partition results {R ( ) | \u2208 E} are left for the following mask generation procedure. ). We highlight that m is used to generate invariant item representation. Thus we aim to seek an m that can guarantees the predictive model performs consistently across the environments. We train a crossenvironment model and tune the value of m in two steps. Following the previous work [24], we define = ( 1 , ..., ) as,\n= max{0, min{1, + }},(8)\nwhere \u223c (0, 2 ). Then, the predictive function is,\n\u0393 ( , , \u2299 c |\u0398 ).(9)\n\u2022 Initialization. As m is for the separation of the raw multimedia representations, we separate representation equally before training. is initialized with the value of 0.5. Additionally, we initialize the predictive model \u0393 with the parameters from a pre-trained raw model of \u0393.\n\n\u2022 Optimization. In order to maintain the consistency of \u0393 across the environments, we utilize the learning target of HRM [24] and adapt it to our task. The object function is composed of two major terms,\nL = E \u2208E L + \u2225Var \u2208 E (\u2207 \u0398 L ) \u2299 \u2225 2 + \u2225m\u2225 2 ,(10)\nwhere the first term is the ordinary recommendation loss, the second term is the constraint across environments, and the third term is a regularization term. L is the average loss value inside the environment , that is,\nL = L (\u0393 ( , , \u2299 c )|\u0398 |R ).(11)\nAccordingly, the mask m is optimized to minimize L . To ensure 0 \u2264 \u2264 1, after each iteration, we clip the mask m with,\n\u2190 max{0, min{1, }}.(12)\nWhen the predictive model \u0393 converged, the invariant and invariant representations can be generated with Equation 4 and Equation 5, respectively. \n\nSummarily, the overall training process is presented in Algorithm 1. \n\u0393( , , c ) = \u0393(p ( ) , p ( ) , t , c ) = \u27e8p ( ) , t \u27e9 + \u27e8p ( ) , W \u00b7 c \u27e9,(14)\nwhere t and c denote the collaborative and raw multimedia representations of item , p ( ) and p ( ) denote the corresponding user representations, and W is a projection matrix to compress the dimension of the raw multimedia representation. UltraGCN pushes the representations to encode the user-item graph through the graph-based loss function,\nL = L + L + L ,(15)\nwhere and are hyper-parameters to balance the importance weights of these loss terms, and L , L and L indicate the objective loss, the user-item constraint loss, and item-item constraint loss, respectively. The objective loss is,\nL = \u2212 \u2211\ufe01 ( , ) \u2208R log( (\u0393( , ))) \u2212 \u2211\ufe01 ( , ) \u2208R \u2212 log( (\u2212\u0393( , ))).(16)\nThe constraint losses L and L are the keys to UltraGCN. They look toward the final stable status of graph-based approaches and try to append constraints to make the model learn the final representations directly. One is for the user-item interactive graph,\nL = \u2212 \u2211\ufe01 ( , ) \u2208R , log( (\u0393( , ))) \u2212 \u2211\ufe01 ( , ) \u2208R \u2212 , log( (\u2212\u0393( , ))),(17)\nwhere the fixed weight coefficients , and , are derived from the user-item interactive graph R by,\n, = 1 \u221a\ufe04 + 1 + 1 ,(18)\nwhere and denote the degrees of the corresponding nodes. Another constraint relies on an item-item correlation graph G = R R, where R indicates the user-item interactive graph. Thus,\nL = \u2212 \u2211\ufe01 ( , ) \u2208R \u2211\ufe01 \u2208 ( ) , log( (\u0393( , ))),(19)\nwhere ( ) indicates the adjacent item set of item . The weight coefficient , is computed by,\n, = , \u2212 , \u221a\ufe02 , = \u2211\ufe01 , ,(20)\nwhere and denote the degrees of item and item in G.\n\n\u2022 Improvement over UltraGCN. UltraGCN is a powerful generic collaborative filtering method. Empirically, it gains competitive performance with the multimedia recommendations by simply concatenating the multimedia representations. Although it obtains strong collaborative embeddings, the drawback of the raw multimedia representations limits its upper bound. In contrast, InvRL maintains all the structures UltraGCN but uses invariant item representations. This change will significantly enhance the model for multimedia recommendation.\n\n\nEXPERIMENTS\n\nTo evaluate the proposed InvRL, we conduct abundant experiments to answer the following research questions: RQ1: How does InvRL perform compared with the state-of-the-art multimedia recommendation methods? RQ2: How does the invariant mask m impact the results? RQ3: How does the environments E impact the results?  Table 1.\n\nMovielens 2 records the viewing history on movie information from the Movielens platform. In order to gain multimedia representations, the previous work [38] collected the corresponding trailers, titles, and descriptions and extracted the visual, acoustic, and textual semantic representations with pre-trained models (i.e. ResNet50 [10], VGGish [18], and Sentence2Vector [2]).\n\nTiktok 3 records the viewing history on micro-videos of the Tiktok platform. It provides visual, acoustic, and textual representations officially. As the original textual representations are given as sentences represented by one-hot words vectors, we take the sum of the word embeddings as the textual representations.\n\nKwai 4 records the viewing history on micro-videos of the Kwai platform. Each item corresponds to a 2048-d visual representation. Different from the above datasets, the items have no acoustic or textural representations. We follow the previous work [39,40] and take the same interaction records for our experiments.\n\nFor fair comparisons, we split the dataset following previous work [39,40] strictly. For each dataset, the interactions are put into the training/validation/testing sets with the ratio of 8:1:1. We tune the hyper-parameters according to the validation set and report the evaluation results on the testing set.\n\n\nEvaluation Protocols.\n\nFollowing the previous work [39,40], We score all the user-item interactions with trained models and rank them in descent order. For each user, we focus on the topitems and compute the Precision@K (P@K), Recall@K (R@K), and Normalized Discounted Cumulative Gain (N@K) according to the observed interactions in the testing set. We take the average scores of all users to evaluate the trained model.\n\n\nBaselines.\n\nTo verify the effectiveness of InvRL, we compare it with the state-of-the-art multimedia recommendation methods. Generally, we adopt the baselines from three categories. VBPR [11], DUIF [9] and CB2CF [3] incorporate multimedia contents to original collaborative filtering method (CF), thus belonging to the multimedia CF (M-CF) category. NGCF [37], DisenGCN [26] and MacridVAE [27] belong to the generic neural CF (G-NCF) category. MMGCN [38], HUIGN [39], and GRCN [40] are multimediaoriented NCF (M-NCF) Models. The performances of the above baselines are quoted from the previous work [39,40].\n\nUltraGCN [28] analyzes the message passing through the graph collaborative filtering and simplifies the message passing procedure as a regularization term. Due to its strong performances on collaborative filtering, we select it as our backbone. To adapt to the multimedia tasks, we concatenate the collaborative embeddings and content representations as the item representations.\n\nWe adopt InvRL to indicate our framework instantiated over UltraGCN. We implement UltraGCN and InvRL with Pytorch 5 . 3.1.4 Parameter Settings. More specifically, we empirically took the Adam [20] as the optimizer. We set the batch size as 512 and fix the embedding dimension to 64. We individually tune the learning rates and regularization factors for id-specific embeddings and other parameters. In many cases, we adopt regularization factors with 10 \u22124 weight for id-specific parameters and tune them in \n\n\nPerformance Comparison (RQ1)\n\nWe list the overall performance comparison among the methods in Table 2. We have the following observations,\n\n\u2022 The neural collaborative filtering (NCF) approaches mostly outperform the collaborative filtering (CF) approaches. This illustrates the effectiveness of modeling high-order correlations among users and items. Also, the worse performances of DUIF reveal the impact of collaborative support. In addition, the M-NCF approaches mostly outperform the G-NCF approaches, which verifies the necessity of multimedia-specific adaptation for the multimedia recommendation. GRCN yields the best performances in the NCF-based category since it incorporates both user intents and item contents significantly. Accordingly, an effective multimedia recommendation model should incorporate the power from both the user behaviors and item contents.\n\n\u2022 Our backbone UltraGCN is also a generic graph-based CF method. Although UltraGCN seems an MF-based model and just simply incorporates the multimedia contents, it outperforms other multimedia recommendation baselines by a large margin. The impressive performance reveals that UltraGCN can model the GCF embeddings through the constraint losses, thus exploiting the inherent collaborative information. However, there is no special design for the multimedia content, we believe that there is more space to enhance its performance.\n\n\u2022 InvRL achieves the best performances on all three datasets. It outperforms UltraGCN by 3.78%, 8.71%, and 7.07% in Movielens, Tiktok, and Kwai, respectively. Compared with UltraGCN, the final model of InvRL adopts the same predictive function and the same training target, except for the use of the content representation (as shown in Section 2.3.4). To alleviate the impact of spurious correlations, the InvRL takes in the invariant item representations which 5 https://pytorch.org/ are generated via a learned invariant mask. Therefore, we believe that the significant improvements are caused by the constraint of the mask. This verifies the effectiveness of our framework.\n\n\u2022 The improvement ratios rely on the properties of the datasets. The improvements of InvRL on Tiktok are greater than 7%, which is much larger than the other two datasets. Compared with Kwai, Tiktok consists of three modalities, while Kwai has only one. More modalities provide more perspectives, thus making it easier to engineer invariant representations. Compared with Movielens, Tiktok has much fewer interactions for each item. Due to the deficiency of collaborative information, they gain more improvements from the explorations of multimedia representations. This further reveals the significance of our work to the multimedia recommendation domain since the items usually have a very small number of interactions.\n\nTo emphasize the improvements of InvRL, we make another comparison between InvRL and the backbone UltraGCN, by listing their top-scores. Figure 3 exhibits the curves of NDCG scores w.r.t. the three datasets. A salient observation is that InvRL consistently outperforms UltraGCN for different . The scores of P@ and R@ perform similarly (unshown here due to the page limitation). This reflects the improvements of InvRL enhance the overall recommending results.\n\n\nStudy of the Mask (RQ2)\n\nInvRL utilizes an invariant representation indicator which is a mask vector to separate the raw multimedia representations. Different from the previous work HRM [24], we adopt a float vector as the substitute for the binary vector. Correspondingly, we revise the 0 norm to the 2 norm in Equation 10. Here, we conduct two experiments to verify the effectiveness of our settings and demonstrate the power of the mask for multimedia recommendation. Table 3 demonstrates the variants of the mask. B and F indicate the binary mask and float mask, respectively. 0 and 2 indicate two regularization terms for the masks, respectively. From the performances, we have two observations, (1) The results of float vectors always outperform those of binary vectors. This verifies that each dimension in the continuous representations is significant for the recommendation. The binary setting may remove some useful dimensions even though they are less important.\n\n(2) The 0 norm always impacts the performances negatively compared with 2 . The major reason is that the 0 norm is a sparse constraint. It may also suppress some valuable dimensions, thus reducing the testing scores.\n\nAccordingly, we suggest using the float mask constrained by the 2 norm in Equation 10.\n\nTo further comprehend how m works, we visualize the dimensional values of m in Figure 4. According to the value distribution, we observe that the mask differentiates representations automatically. The three modalities in Movielens and Tiktok perform different patterns. This verifies the drawback of the native use of the raw representations. Moreover, the weight distribution in a single modality also reveals the different contributions of different dimensions. This illustrates the necessity and the feasibility of capturing the invariant part with InvRL.   \n\n\nStudy of the Environments (RQ3)\n\nThe environments correspond to the spurious correlations in useritem interactions. We conduct two experiments to verify the existence and separability of the environments and explore the empirical experiences for further applications.\n\nIn the first experiment, we record the ratio of moved interactions during the process of module M3. Specifically, after the environments are updated, the interactions are partitioned again. According to Equation 7, if an interaction still fits the current environment, it would not be moved, otherwise, we move it to the new environment and count it as moved interaction. By repeating the environment partition several times, we gain the curves shown in Figure 5. All the three curves converge after being repeated 20 times. Only a small number of interactions would be moved then. Thus the environments come stable. This reveals the separability of the environments thus InvRL can model the spurious correlations correctly. In addition, the curve implies the appropriate loop count in module M3 of Algorithm 1.  To explore the partition of the environments, we conduct multiple experiments with different |E |, which indicates the number of environments. As shown in Figure 6, with the increment of |E |, the performances have an ascending trend. More environments support more detailed interaction partitions, thus leading to better performances. This also implies the existence of spurious correlations from another perspective. However, considering the expend of computational resources, we just test |E | with no larger than 30.\n\n\nRELATED WORK\n\nCollaborative Filtering (CF) is a crucial method for recommendation. It analyzes the observed interactions between users and items and captures the collaborative embeddings. The fundamental method is matrix factorization [16] that assumes users and items have many similar properties thus representing them with low-rank embeddings. FISM [19] assumes that users would like the things similar to the interacted items, thus representing users with interacted items. SVD++ [21] incorporates the angles and achieves impressive performance. Subsequently, many works explore the improvements over the fundamental CF methods. BPR [30] proposes a pair-wise loss to describe recommendation tasks in a ranking view. ALS [15] assigns weights to each interaction. APR [13] utilizes adversarial samples to train robust models. For better recommendation, many novel techniques are introduced, such as causality [8,17,35,44], knowledge graphs [36,42], neural networks [7,14], etc.\n\nNeural structures are widely used to exploit the inherent collaborative properties. NCF [14] feeds the user and item embeddings into a two-branch multi-layer perception network to predict the user preference. ConvNCF [7] models the cross-dimension information and predicts with a convolutional network. Liang et al. [22] proposed a VAE-based CF method to reconstruct the ideal interaction matrix. It is notable that graph neural networks perform very well for the recommending tasks since the user-item interaction matrices naturally describe a bipartite graph. Therefore, NGCF [37] introduces the graph convolutional network (GCN) to model the high-hop neighbor information. Similarly, PinSage [42] adopts random walk and GCN simultaneously to traverse the graph. KGAT [36] focuses on extra knowledge graph and also uses the random walk manner to seek the relations. Lightgcn [12] expands GCN in recommending perspective and proposes a fast and powerful model. The most impressive method is UltraGCN [28], which learns the graph-enhanced representation directly. Due to the simplicity and effectiveness, we select it as the backbone.\n\nMultimedia recommendation methods mostly correspond to some CF methods. VBPR [11], known as the benchmark of multimedia recommendation, follows the MF structure but represents items with collaborative embedding and content representations simultaneously. DUIF [9] is another MF-based method that represents the items with their content representations only. As GCN-based CF performs well in many cases, many works explore multimedia-oriented GCN. MMGCN [38] proposes a multi-graph neural network to process and integrate the multi-modal representations. MGAT [33] adopts an attentive mechanism to enhance the multi-modal representation integration. MKGAT [32] utilizes the attentive mechanism to generate representations across modals. DisenGCN [26] disentangles the latent factors of the interactions. MacridVAE [27] forces each dimension of the representations to independently reflect an isolated low-level factor. Compared with the ordinary CF models, multimedia recommendation explores the use of multimedia representations, a.k.a. , the generation of invariant item representations.\n\nThere are two directions to capture invariant representations, 1) to engineer preference-aware representations, and 2) to block the spurious correlations. For the first direction, Yu et al. [43] adopted an aesthetic model to provide an aesthetic evaluation as they aimed to address the fashion recommendation task. Deepstyle [25] separates the clothes representations into category and style parts since they argue that the category information may cause spurious correlation. AMAE [31] proposes an attention-based model to extract profitable representations from music-related information. For the second direction, Wang et al. [34] differentiated the real 'like' behavior from 'click' via causal reference, but more labeled data are required. Qiuet al. [29] debiases the visual representations from a causal perspective. However, These approaches rely on domain-specific analysis and design and lack a generic mechanism to eliminate the influence of the spurious correlation. Therefore, we propose InvRL to exploit an effective way.\n\n\nCONCLUSION\n\nIn this paper, we proposed an invariant representation learning framework (InvRL) for multimedia recommendation. We adopted heterogeneous environments to reflect the various spurious correlations and learn invariant item representations across the environments. According to the invariant representations, we finally trained a recommender model that achieves the best performances in the experiments. This work is a bold attempt on alleviating the spurious correlations from the multimedia contents. In future, we will make more efforts in exploring the user behaviors and the item multimedia contents, including but not limited to a new representation generative mechanism, a more practical and efficient representation separator, etc.\n\nFigure 1 :\n1Two examples to illustrate the spurious correlations in multimedia recommendation.\n\nFigure 2 :\n2The overall workflow of our framework. The numbered grey triangles indicate the key modules of our approach.2.3.3 Mask Generation. As shown inFigure 2, the module M4 takes in the environments and generates the invariant mask and variant mask, respectively. Actually, Equation4 and Equation 5 demonstrate that the two mask relies on a same vector m = ( 1 , ...,\n\n\nPredictive Model. By repeating the running flow M2-M3-M4 (as shown in Figure 2) for times till converged, the invariant mask becomes stable. Thus, we learn the final predictive model according to the invariant representation generated by M5. The learning target is a bit different from Equation 1\u0393 * ( , , \u03a6 |\u0398 * )|R ).\n\n\n{1, 0.1, 0.01, 0.001, 0} for other parameters, and we set the learning rate to 10 \u22123 for all parameters. The number of environments |E | is tuned in {1, 5, 10, 20, 30}, and in Equation 10 are respectively selected in {1, 0.5, 0.1}, and the learning rate for m in mask generation module is searched in {0.01, 0.001, 0.0001}. and are tuned in {2, 1, 0.1, 0.01, 0}. The iteration parameter is initially set as 5. The environment partition and mask generation modules are trained for 20 and 40 epochs, and the final predictive module is trained for 500 epochs. We choose the model according to the validation scores and report the corresponding testing scores.\n\nFigure 3 :Figure 4 :\n34The Comparison between UltraGCN and InvRL w.r.t. NDCG@ . The situation that InvRL outperforms UltraGCN consistently reveals the overall improvements over the recommending results. Visualization of the masks. The weight distributions from different modalities perform different patterns.\n\nFigure 5 :\n5The ratio of moved interactions during the progress of environment partitions.\n\nFigure 6 :\n6The impact of |E |, i.e., the number of environments. Sufficient environments mostly lead to better performances.\n\n\nAlgorithm 1: The overall training process. Data: R, R \u2212 , R Result: Final Predictive Model \u0393 * ( , |\u0398 * , \u03a6)1 for \u2190 1 to do \n/* M3 \n*/ \n\n2 \n\ndo \n\n3 \n\nfor \u2208 E do \n\n4 \n\nOptimize \u0393 ( ) via Eq. (6); \n\n5 \n\nend \n\n6 \n\nfor \u2208 E do \n\n7 \n\nCompute R via Eq. (7); \n\n8 \n\nend \n\n9 \n\nwhile Converged; \n/* M4 \n*/ \n\n10 \n\ndo \n\n11 \n\nLearn m via Eq. (10); \n\n12 \n\nwhile Converged; \n\n13 end \n/* M6 \n*/ \n\n14 Optimize \u0393  *  ( , |\u0398  *  , \u03a6) via Eq. (13); \n\n2.3.5 Backbone. Recently, UltraGCN [28] achieves impressive per-\nformances. Though it is a GCN-base method, it serves as an MF-like \nlinear model. Due to its effectiveness and simplicity, we select it as \nour backbone and make it adapt to multimedia recommendation. \nFormally, the predictive function of UltraGCN is defined as, \n\n\n\nTable 1 :\n1The statistics on the datasets, Movielens, Tiktok, and Kwai. V, A, and T indicate the dimensions of visual, acoustic, and textual modalities, respectively.Dataset \n#Interactions #Items #Users \nV \nA \nT \nMovielens \n1,239,508 \n5,986 \n55,485 2,048 128 100 \nTiktok \n726,065 \n76,085 36,656 \n128 128 128 \nKwai \n298,492 \n86,483 \n7,010 2,048 \n-\n-\n\n3.1 Experimental Settings \n\n3.1.1 Datasets. Following the previous SOTA work on multimedia \nrecommendation [39, 40], we conduct extensive experiments on \nthree public datasets, Movielens, Tiktok, and Kwai. The statistics \nof the datasets are listed in \n\nTable 2 :\n2Performances. The bold scores indicate the best performance. The underlined scores indicate the second-best performance. M-CF, G-NCF and M-NCF indicate the multimedia CF, generic NCF and multimedia NCF, respectively.Category \nMethods \nMovielens \nTiktok \nKwai \nP@10 \nR@10 N@10 P@10 \nR@10 N@10 P@10 \nR@10 N@10 \n\nM-CF \n\n\n\nTable 3 :\n3The impact of m-related settings. B and F indicate the binary mask and float mask, respectively.0 and 2 indicate \ntwo regularization terms for the masks, respectively \n\nDataset \nm-Type P@10 \nR@10 N@10 \n\nMovielens \n\nB+ 0 \n0.0640 0.2618 0.2762 \nB+ 2 \n0.0646 0.2639 0.2776 \nF+ 0 \n0.0649 0.2658 0.2798 \nF+ 2 \n0.0652 0.2672 0.2813 \n\nTiktok \n\nB+ 0 \n0.0191 0.1070 0.0940 \nB+ 2 \n0.0194 0.1068 0.0944 \nF+ 0 \n0.0194 0.1068 0.0933 \nF+ 2 \n0.0196 0.1079 0.0951 \n\nKwai \n\nB+ 0 \n0.0180 0.0530 0.0964 \nB+ 2 \n0.0184 0.0526 0.0972 \nF+ 0 \n0.0185 0.0523 0.0968 \nF+ 2 \n0.0187 0.0532 0.0984 \n\n\nc is typically a concatenation of outputs from multiple encoders.\nhttps://grouplens.org/datasets/movielens/. 3 http://ai-lab-challenge.bytedance.com/tce/vc/. 4 https://www.kuaishou.com/activity/uimc/.\n\n. Kartik Ahuja, Karthikeyan Shanmugam, R Kush, Amit Varshney, Dhurandhar, arXiv:2002.046922020. Invariant Risk Minimization Games. arXiv preprintKartik Ahuja, Karthikeyan Shanmugam, Kush R. Varshney, and Amit Dhurandhar. 2020. Invariant Risk Minimization Games. arXiv preprint arXiv:2002.04692 (2020).\n\nA simple but tough-to-beat baseline for sentence embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma, 5th International Conference on Learning Representations. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations.\n\nCB2CF: A neural multiview content-to-collaborative filtering model for completely cold item recommendations. Oren Barkan, Noam Koenigstein, Eylon Yogev, Ori Katz, 10.1145/3298689.334703813th ACM Conference on Recommender Systems. Oren Barkan, Noam Koenigstein, Eylon Yogev, and Ori Katz. 2019. CB2CF: A neural multiview content-to-collaborative filtering model for completely cold item recommendations. In 13th ACM Conference on Recommender Systems. 228- 236. https://doi.org/10.1145/3298689.3347038\n\nMMalfM: Explainable recommendation by leveraging reviews and images. Zhiyong Cheng, Xiaojun Chang, Lei Zhu, Rose C Kanjirathinkal, Mohan Kankanhalli, 10.1145/3291060ACM Transactions on Information Systems. 37Zhiyong Cheng, Xiaojun Chang, Lei Zhu, Rose C. Kanjirathinkal, and Mohan Kankanhalli. 2019. MMalfM: Explainable recommendation by leveraging reviews and images. ACM Transactions on Information Systems 37, 2 (2019), 1-28. https: //doi.org/10.1145/3291060\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming Wei Chang, Kenton Lee, Kristina Toutanova, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 1Jacob Devlin, Ming Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference 1 (2019), 4171-4186.\n\nModeling Instant User Intent and Content-Level Transition for Sequential Fashion Recommendation. Yujuan Ding, Yunshan Ma, Wai Keung Wong, Tat Seng Chua, 10.1109/TMM.2021.3088281IEEE Transactions on Multimedia. 24Yujuan Ding, Yunshan Ma, Wai Keung Wong, and Tat Seng Chua. 2022. Mod- eling Instant User Intent and Content-Level Transition for Sequential Fash- ion Recommendation. IEEE Transactions on Multimedia 24 (2022), 2687-2700. https://doi.org/10.1109/TMM.2021.3088281\n\nModeling embedding dimension correlations via convolutional neural collaborative filtering. Xiaoyu Du, F Xiangnan He, Jinhui Yuan, Tang, 10.1145/3357154ACM Transactions on Information Systems. 37Zhiguang Qin, and Tat Seng ChuaXiaoyu Du, Xiangnan He, F. Yuan, Jinhui Tang, Zhiguang Qin, and Tat Seng Chua. 2019. Modeling embedding dimension correlations via convolutional neural collaborative filtering. ACM Transactions on Information Systems 37, 4 (2019), 1-22. https://doi.org/10.1145/3357154\n\nJinhui Tang, and Tat Seng Chua. 2020. How to Learn Item Representation for Cold-Start Multimedia Recommendation?. Xiaoyu Du, Xiang Wang, Xiangnan He, Zechao Li, 10.1145/3394171.3413628Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaXiaoyu Du, Xiang Wang, Xiangnan He, Zechao Li, Jinhui Tang, and Tat Seng Chua. 2020. How to Learn Item Representation for Cold-Start Multimedia Recom- mendation? Proceedings of the 28th ACM International Conference on Multimedia (2020), 3469-3477. https://doi.org/10.1145/3394171.3413628\n\nLearning image and user features for recommendation in social networks. Xue Geng, Hanwang Zhang, Jingwen Bian, Tat Seng Chua, 10.1109/ICCV.2015.486Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXue Geng, Hanwang Zhang, Jingwen Bian, and Tat Seng Chua. 2015. Learning image and user features for recommendation in social networks. In Proceedings of the IEEE International Conference on Computer Vision, Vol. 2015 Inter. 4274-4282. https://doi.org/10.1109/ICCV.2015.486\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 10.1109/CVPR.2016.90Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Vol. 2016-December. 770-778. https://doi.org/10.1109/CVPR.2016.90\n\nVBPR: Visual Bayesian personalized ranking from implicit feedback. Ruining He, Julian Mcauley, 10.1609/aaai.v30i1.997330th AAAI Conference on Artificial Intelligence. 30Ruining He and Julian McAuley. 2016. VBPR: Visual Bayesian personalized ranking from implicit feedback. In 30th AAAI Conference on Artificial Intelligence, Vol. 30. 144-150. https://doi.org/10.1609/aaai.v30i1.9973\n\nLightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong Dong Zhang, Meng Wang, 10.1145/3397271.3401063Proceedings of the 43rd International ACM SI-GIR Conference on Research and Development in Information Retrieval. the 43rd International ACM SI-GIR Conference on Research and Development in Information RetrievalXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net- work for Recommendation. In Proceedings of the 43rd International ACM SI- GIR Conference on Research and Development in Information Retrieval. 639-648. https://doi.org/10.1145/3397271.3401063\n\nAdversarial personalized ranking for recommendation. Xiangnan He, Zhankui He, Xiaoyu Du, Tat Seng Chua, 10.1145/3209978.320998141st International ACM SI-GIR Conference on Research and Development in Information Retrieval. Xiangnan He, Zhankui He, Xiaoyu Du, and Tat Seng Chua. 2018. Adversar- ial personalized ranking for recommendation. In 41st International ACM SI- GIR Conference on Research and Development in Information Retrieval. 355-364. https://doi.org/10.1145/3209978.3209981\n\nNeural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat Seng Chua, 10.1145/3038912.305256926th International World Wide Web Conference. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat Seng Chua. 2017. Neural collaborative filtering. In 26th International World Wide Web Conference. 173-182. https://doi.org/10.1145/3038912.3052569\n\nFast matrix factorization with nonuniform weights on missing data. Xiangnan He, Jinhui Tang, Xiaoyu Du, Richang Hong, Tongwei Ren, Tat Seng Chua, 10.1109/TNNLS.2018.2890117IEEE Transactions on Neural Networks and Learning Systems. 31Xiangnan He, Jinhui Tang, Xiaoyu Du, Richang Hong, Tongwei Ren, and Tat Seng Chua. 2020. Fast matrix factorization with nonuniform weights on missing data. IEEE Transactions on Neural Networks and Learning Systems 31, 8 (2020), 2791- 2804. https://doi.org/10.1109/TNNLS.2018.2890117\n\nFast matrix factorization for online recommendation with implicit feedback. Xiangnan He, Hanwang Zhang, Min Yen Kan, Tat Seng Chua, 10.1145/2911451.2911489Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 39th International ACM SIGIR Conference on Research and Development in Information RetrievalXiangnan He, Hanwang Zhang, Min Yen Kan, and Tat Seng Chua. 2016. Fast ma- trix factorization for online recommendation with implicit feedback. In Proceed- ings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. 549-558. https://doi.org/10.1145/2911451.2911489\n\nXiangnan He, Yang Zhang, Fuli Feng, Chonggang Song, Lingling Yi, arXiv:2205.06532Guohui Ling, and Yongdong Zhang. 2022. Addressing Confounding Feature Issue for Causal Recommendation. arXiv preprintXiangnan He, Yang Zhang, Fuli Feng, Chonggang Song, Lingling Yi, Guohui Ling, and Yongdong Zhang. 2022. Addressing Confounding Feature Issue for Causal Recommendation. arXiv preprint arXiv:2205.06532 (2022).\n\nCNN architectures for large-scale audio classification. Shawn Hershey, Sourish Chaudhuri, P W Daniel, Jort F Ellis, Aren Gemmeke, R Jansen, Manoj Moore, Devin Plakal, Rif A Platt, Bryan Saurous, Malcolm Seybold, Ron J Slaney, Kevin Weiss, Wilson, 10.1109/ICASSP.2017.7952132IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEShawn Hershey, Sourish Chaudhuri, Daniel P.W. Ellis, Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Sey- bold, Malcolm Slaney, Ron J. Weiss, and Kevin Wilson. 2017. CNN architectures for large-scale audio classification. In IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 131-135. https://doi.org/10.1109/ICASSP. 2017.7952132\n\nFISM: Factored item similarity models for Top-N recommender systems. Santosh Kabbur, Xia Ning, George Karypis, 10.1145/2487575.2487589Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining128815Santosh Kabbur, Xia Ning, and George Karypis. 2013. FISM: Factored item similar- ity models for Top-N recommender systems. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Vol. Part F128815. 659-667. https://doi.org/10.1145/2487575.2487589\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Lei Kingma, Ba, 3rd International Conference on Learning Representations -Conference Track Proceedings. Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: A method for stochastic op- timization. 3rd International Conference on Learning Representations -Conference Track Proceedings (2015).\n\nFactorization meets the neighborhood: A multifaceted collaborative filtering model. Yehuda Koren, 10.1145/1401890.1401944Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYehuda Koren. 2008. Factorization meets the neighborhood: A multifaceted collaborative filtering model. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 426-434. https://doi.org/ 10.1145/1401890.1401944\n\nVariational autoencoders for collaborative filtering. Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, Tony Jebara, 10.1145/3178876.3186150Proceedings of the World Wide Web Conference. the World Wide Web ConferenceDawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In Proceedings of the World Wide Web Conference. 689-698. https://doi.org/10.1145/3178876.3186150\n\nUser diverse preference modeling by multimodal attentive metric learning. Fan Liu, Zhiyong Cheng, Changchang Sun, Yinglong Wang, Liqiang Nie, Mohan Kankanhalli, 10.1145/3343031.3350953Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaFan Liu, Zhiyong Cheng, Changchang Sun, Yinglong Wang, Liqiang Nie, and Mohan Kankanhalli. 2019. User diverse preference modeling by multimodal attentive metric learning. In Proceedings of the 27th ACM International Conference on Multimedia. 1526-1534. https://doi.org/10.1145/3343031.3350953\n\nKernelized Heterogeneous Risk Minimization. Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, Zheyan Shen, PMLRAdvances in Neural Information Processing Systems. 26Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. 2021. Kernelized Heterogeneous Risk Minimization. In Advances in Neural Information Processing Systems, Vol. 26. PMLR, 21720-21731.\n\nDeepstyle: Learning user preferences for visual recommendation. Qiang Liu, Shu Wu, Liang Wang, 10.1145/3077136.3080658Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR Conference on Research and Development in Information RetrievalQiang Liu, Shu Wu, and Liang Wang. 2017. Deepstyle: Learning user preferences for visual recommendation. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 841-844. https: //doi.org/10.1145/3077136.3080658\n\nDisentangled graph convolutional networks. Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, Wenwu Zhu, 36th International Conference on Machine Learning. Kamalika Chaudhuri and Ruslan SalakhutdinovJianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. 2019. Disentan- gled graph convolutional networks. In 36th International Conference on Machine Learning, Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.), Vol. 2019-June. PMLR, 7454-7463.\n\nLearning disentangled representations for recommendation. Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu, Advances in Neural Information Processing Systems. 32Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn- ing disentangled representations for recommendation. Advances in Neural Infor- mation Processing Systems 32 (2019).\n\nUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, Xiuqiang He, 10.1145/3459637.3482291International Conference on Information and Knowledge Management, Proceedings. 1253-1262. Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. In International Conference on Information and Knowledge Management, Proceedings. 1253-1262. https://doi.org/10.1145/3459637.3482291\n\nCausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. Ruihong Qiu, Sen Wang, Zhi Chen, Hongzhi Yin, Zi Huang, 10.1145/3474085.3475266Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on MultimediaRuihong Qiu, Sen Wang, Zhi Chen, Hongzhi Yin, and Zi Huang. 2021. CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. In Proceedings of the 29th ACM International Conference on Multimedia. 3844-3852. https://doi.org/10.1145/3474085.3475266\n\nBPR: Bayesian personalized ranking from implicit feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence. the 25th Conference on Uncertainty in Artificial IntelligenceSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (2009), 452-461.\n\nEnhancing Music Recommendation with Social Media Content: An Attentive Multimodal Autoencoder Approach. Tiancheng Shen, Jia Jia, Yan Li, Hanjie Wang, Bo Chen, 10.1109/IJCNN48605.2020.9206894Proceedings of the International Joint Conference on Neural Networks. IEEE. the International Joint Conference on Neural Networks. IEEETiancheng Shen, Jia Jia, Yan Li, Hanjie Wang, and Bo Chen. 2020. Enhancing Music Recommendation with Social Media Content: An Attentive Multimodal Autoencoder Approach. In Proceedings of the International Joint Conference on Neural Networks. IEEE, 1-8. https://doi.org/10.1109/IJCNN48605.2020.9206894\n\nMulti-modal Knowledge Graphs for Recommender Systems. Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, Kai Zheng, 10.1145/3340531.3411947International Conference on Information and Knowledge Management, Proceedings. Rui Sun, Xuezhi Cao, Yan Zhao, Junchen Wan, Kun Zhou, Fuzheng Zhang, Zhongyuan Wang, and Kai Zheng. 2020. Multi-modal Knowledge Graphs for Rec- ommender Systems. International Conference on Information and Knowledge Man- agement, Proceedings (2020), 1405-1414. https://doi.org/10.1145/3340531.3411947\n\nMGAT: Multimodal Graph Attention Network for Recommendation. Zhulin Tao, Yinwei Wei, Xiang Wang, 10.1016/j.ipm.2020.102277Information Processing and Management. 57102277Xiangnan He, Xianglin Huang, and Tat Seng ChuaZhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, and Tat Seng Chua. 2020. MGAT: Multimodal Graph Attention Network for Recommendation. Information Processing and Management 57, 5 (2020), 102277. https://doi.org/10. 1016/j.ipm.2020.102277\n\nClicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue. Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, Tat Seng Chua, 10.1145/3404835.3462962Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1288-1297. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1288-1297Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, and Tat Seng Chua. 2021. Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1288-1297. https://doi.org/ 10.1145/3404835.3462962\n\nXiangnan He, Min Lin, and Tat Seng Chua. 2022. Causal Representation Learning for Out-of-Distribution Recommendation. Wenjie Wang, Xinyu Lin, Fuli Feng, 10.1145/3485447.3512251Proceedings of the ACM Web Conference 2022. the ACM Web Conference 2022Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Min Lin, and Tat Seng Chua. 2022. Causal Representation Learning for Out-of-Distribution Recommendation. In Proceedings of the ACM Web Conference 2022. 3562-3571. https://doi.org/10. 1145/3485447.3512251\n\nKGAT: Knowledge graph attention network for recommendation. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat Seng Chua, 10.1145/3292500.3330989Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data MiningXiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat Seng Chua. 2019. KGAT: Knowledge graph attention network for recommendation. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 950-958. https://doi.org/10.1145/3292500.3330989\n\nNeural graph collaborative filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat Seng Chua, 10.1145/3331184.3331267Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat Seng Chua. 2019. Neural graph collaborative filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 165-174. https://doi.org/10.1145/3331184.3331267\n\nMMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. Yinwei Wei, Xiangnan He, Xiang Wang, Richang Hong, Liqiang Nie, Tat Seng Chua, 10.1145/3343031.3351034Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on MultimediaYinwei Wei, Xiangnan He, Xiang Wang, Richang Hong, Liqiang Nie, and Tat Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM International Conference on Multimedia. 1437-1445. https://doi.org/10.1145/3343031.3351034\n\nHierarchical User Intent Graph Network for Multimedia Recommendation. Yinwei Wei, Xiang Wang, Xiangnan He, Liqiang Nie, Yong Rui, Tat Seng Chua, 10.1109/TMM.2021.3088307IEEE Transactions on Multimedia. 24Yinwei Wei, Xiang Wang, Xiangnan He, Liqiang Nie, Yong Rui, and Tat Seng Chua. 2022. Hierarchical User Intent Graph Network for Multimedia Recommendation. IEEE Transactions on Multimedia 24 (2022), 2701-2712. https://doi.org/10.1109/ TMM.2021.3088307\n\nXiangnan He, and Tat Seng Chua. 2020. Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback. Yinwei Wei, Xiang Wang, Liqiang Nie, 10.1145/3394171.3413556Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaYinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat Seng Chua. 2020. Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback. Proceedings of the 28th ACM International Conference on Multimedia (2020), 3541-3549. https://doi.org/10.1145/3394171.3413556\n\nA Hierarchical Attention Model for Social Contextual Image Recommendation. Le Wu, Lei Chen, Richang Hong, Yanjie Fu, Xing Xie, Meng Wang, 10.1109/TKDE.2019.2913394IEEE Transactions on Knowledge and Data Engineering. 32Le Wu, Lei Chen, Richang Hong, Yanjie Fu, Xing Xie, and Meng Wang. 2020. A Hierarchical Attention Model for Social Contextual Image Recommendation. IEEE Transactions on Knowledge and Data Engineering 32, 10 (2020), 1854-1867. https://doi.org/10.1109/TKDE.2019.2913394\n\nGraph convolutional neural networks for web-scale recommender systems. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, Jure Leskovec, 10.1145/3219819.3219890Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 974-983. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 974-983Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining. 974-983. https://doi.org/10.1145/ 3219819.3219890\n\nAesthetic-based clothing recommendation. Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, Zheng Qin, 10.1145/3178876.3186146Proceedings of the World Wide Web Conference. the World Wide Web ConferenceWenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, and Zheng Qin. 2018. Aesthetic-based clothing recommendation. In Proceedings of the World Wide Web Conference. 649-658. https://doi.org/10.1145/3178876.3186146\n\nXinyuan Zhu, Yang Zhang, Fuli Feng, Xun Yang, arXiv:2205.07499Dingxian Wang, and Xiangnan He. 2022. Mitigating Hidden Confounding Effects for Causal Recommendation. arXiv preprintXinyuan Zhu, Yang Zhang, Fuli Feng, Xun Yang, Dingxian Wang, and Xiangnan He. 2022. Mitigating Hidden Confounding Effects for Causal Recommendation. arXiv preprint arXiv:2205.07499 (2022).\n", "annotations": {"author": "[{\"end\":140,\"start\":130},{\"end\":168,\"start\":141},{\"end\":200,\"start\":169},{\"end\":234,\"start\":201},{\"end\":271,\"start\":235},{\"end\":282,\"start\":272},{\"end\":291,\"start\":283},{\"end\":302,\"start\":292},{\"end\":315,\"start\":303},{\"end\":328,\"start\":316},{\"end\":458,\"start\":329},{\"end\":506,\"start\":459},{\"end\":554,\"start\":507}]", "publisher": "[{\"end\":69,\"start\":66},{\"end\":754,\"start\":751}]", "author_last_name": "[{\"end\":139,\"start\":137},{\"end\":148,\"start\":146},{\"end\":178,\"start\":174},{\"end\":212,\"start\":210},{\"end\":246,\"start\":242},{\"end\":281,\"start\":279},{\"end\":290,\"start\":288},{\"end\":301,\"start\":297},{\"end\":314,\"start\":312},{\"end\":327,\"start\":323}]", "author_first_name": "[{\"end\":136,\"start\":130},{\"end\":145,\"start\":141},{\"end\":173,\"start\":169},{\"end\":209,\"start\":201},{\"end\":241,\"start\":235},{\"end\":278,\"start\":272},{\"end\":287,\"start\":283},{\"end\":296,\"start\":292},{\"end\":311,\"start\":303},{\"end\":322,\"start\":316}]", "author_affiliation": "[{\"end\":457,\"start\":330},{\"end\":505,\"start\":460},{\"end\":553,\"start\":508}]", "title": "[{\"end\":65,\"start\":1},{\"end\":619,\"start\":555}]", "venue": "[{\"end\":697,\"start\":621}]", "abstract": "[{\"end\":2818,\"start\":1151}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2936,\"start\":2933},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2955,\"start\":2951},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2975,\"start\":2971},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3377,\"start\":3373},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3390,\"start\":3387},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3767,\"start\":3764},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4888,\"start\":4885},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4891,\"start\":4888},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4894,\"start\":4891},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4897,\"start\":4894},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4900,\"start\":4897},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5095,\"start\":5091},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5122,\"start\":5118},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5179,\"start\":5175},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5345,\"start\":5341},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5672,\"start\":5669},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6723,\"start\":6719},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7241,\"start\":7238},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7731,\"start\":7727},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11130,\"start\":11127},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11133,\"start\":11130},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16676,\"start\":16672},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17210,\"start\":17206},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17851,\"start\":17850},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20589,\"start\":20585},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20769,\"start\":20765},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20782,\"start\":20778},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20807,\"start\":20804},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21384,\"start\":21380},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21387,\"start\":21384},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21519,\"start\":21515},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21522,\"start\":21519},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21815,\"start\":21811},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21818,\"start\":21815},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22374,\"start\":22370},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22384,\"start\":22381},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22398,\"start\":22395},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22542,\"start\":22538},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22557,\"start\":22553},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22576,\"start\":22572},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22637,\"start\":22633},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22649,\"start\":22645},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22664,\"start\":22660},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22786,\"start\":22782},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22789,\"start\":22786},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22805,\"start\":22801},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23369,\"start\":23365},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27142,\"start\":27138},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27275,\"start\":27273},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28230,\"start\":28228},{\"end\":29279,\"start\":29269},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30641,\"start\":30637},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30758,\"start\":30754},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30890,\"start\":30886},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31043,\"start\":31039},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31130,\"start\":31126},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31176,\"start\":31172},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31316,\"start\":31313},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31319,\"start\":31316},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31322,\"start\":31319},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31325,\"start\":31322},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":31348,\"start\":31344},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31351,\"start\":31348},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31372,\"start\":31369},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31375,\"start\":31372},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31475,\"start\":31471},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31603,\"start\":31600},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31703,\"start\":31699},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31965,\"start\":31961},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32082,\"start\":32078},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32157,\"start\":32153},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32264,\"start\":32260},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32388,\"start\":32384},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32600,\"start\":32596},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32782,\"start\":32779},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32976,\"start\":32972},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33082,\"start\":33078},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33178,\"start\":33174},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33268,\"start\":33264},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33336,\"start\":33332},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33803,\"start\":33799},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33938,\"start\":33934},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34095,\"start\":34091},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34242,\"start\":34238},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34368,\"start\":34364},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35780,\"start\":35779}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35490,\"start\":35395},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35864,\"start\":35491},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36186,\"start\":35865},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36845,\"start\":36187},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37156,\"start\":36846},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37248,\"start\":37157},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37375,\"start\":37249},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38138,\"start\":37376},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38742,\"start\":38139},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39072,\"start\":38743},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39655,\"start\":39073}]", "paragraph": "[{\"end\":3585,\"start\":2834},{\"end\":5614,\"start\":3587},{\"end\":6810,\"start\":5616},{\"end\":8100,\"start\":6812},{\"end\":8730,\"start\":8102},{\"end\":9263,\"start\":8741},{\"end\":10038,\"start\":9287},{\"end\":10364,\"start\":10071},{\"end\":11066,\"start\":10366},{\"end\":11638,\"start\":11108},{\"end\":11793,\"start\":11680},{\"end\":11881,\"start\":11822},{\"end\":12419,\"start\":11883},{\"end\":13459,\"start\":12439},{\"end\":13854,\"start\":13485},{\"end\":13972,\"start\":13867},{\"end\":14384,\"start\":13994},{\"end\":14832,\"start\":14423},{\"end\":15359,\"start\":14834},{\"end\":15641,\"start\":15400},{\"end\":15953,\"start\":15643},{\"end\":16706,\"start\":15997},{\"end\":16782,\"start\":16732},{\"end\":17083,\"start\":16804},{\"end\":17288,\"start\":17085},{\"end\":17559,\"start\":17340},{\"end\":17711,\"start\":17593},{\"end\":17882,\"start\":17736},{\"end\":17953,\"start\":17884},{\"end\":18376,\"start\":18032},{\"end\":18626,\"start\":18397},{\"end\":18953,\"start\":18697},{\"end\":19126,\"start\":19028},{\"end\":19332,\"start\":19150},{\"end\":19474,\"start\":19382},{\"end\":19554,\"start\":19503},{\"end\":20091,\"start\":19556},{\"end\":20430,\"start\":20107},{\"end\":20809,\"start\":20432},{\"end\":21129,\"start\":20811},{\"end\":21446,\"start\":21131},{\"end\":21757,\"start\":21448},{\"end\":22180,\"start\":21783},{\"end\":22790,\"start\":22195},{\"end\":23171,\"start\":22792},{\"end\":23681,\"start\":23173},{\"end\":23822,\"start\":23714},{\"end\":24555,\"start\":23824},{\"end\":25086,\"start\":24557},{\"end\":25764,\"start\":25088},{\"end\":26487,\"start\":25766},{\"end\":26949,\"start\":26489},{\"end\":27925,\"start\":26977},{\"end\":28143,\"start\":27927},{\"end\":28231,\"start\":28145},{\"end\":28794,\"start\":28233},{\"end\":29064,\"start\":28830},{\"end\":30399,\"start\":29066},{\"end\":31381,\"start\":30416},{\"end\":32517,\"start\":31383},{\"end\":33607,\"start\":32519},{\"end\":34643,\"start\":33609},{\"end\":35394,\"start\":34658}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10070,\"start\":10039},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11679,\"start\":11639},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11821,\"start\":11794},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13866,\"start\":13855},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13993,\"start\":13973},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15399,\"start\":15360},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15996,\"start\":15954},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16731,\"start\":16707},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16803,\"start\":16783},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17339,\"start\":17289},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17592,\"start\":17560},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17735,\"start\":17712},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18031,\"start\":17954},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18396,\"start\":18377},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18696,\"start\":18627},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19027,\"start\":18954},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19149,\"start\":19127},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19381,\"start\":19333},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19502,\"start\":19475}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20429,\"start\":20422},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23785,\"start\":23778},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27430,\"start\":27423}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2832,\"start\":2820},{\"attributes\":{\"n\":\"2\"},\"end\":8739,\"start\":8733},{\"attributes\":{\"n\":\"2.1\"},\"end\":9285,\"start\":9266},{\"attributes\":{\"n\":\"2.2\"},\"end\":11106,\"start\":11069},{\"attributes\":{\"n\":\"2.3\"},\"end\":12437,\"start\":12422},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":13483,\"start\":13462},{\"attributes\":{\"n\":\"2.3.2\"},\"end\":14421,\"start\":14387},{\"attributes\":{\"n\":\"3\"},\"end\":20105,\"start\":20094},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":21781,\"start\":21760},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":22193,\"start\":22183},{\"attributes\":{\"n\":\"3.2\"},\"end\":23712,\"start\":23684},{\"attributes\":{\"n\":\"3.3\"},\"end\":26975,\"start\":26952},{\"attributes\":{\"n\":\"3.4\"},\"end\":28828,\"start\":28797},{\"attributes\":{\"n\":\"4\"},\"end\":30414,\"start\":30402},{\"attributes\":{\"n\":\"5\"},\"end\":34656,\"start\":34646},{\"end\":35406,\"start\":35396},{\"end\":35502,\"start\":35492},{\"end\":36867,\"start\":36847},{\"end\":37168,\"start\":37158},{\"end\":37260,\"start\":37250},{\"end\":38149,\"start\":38140},{\"end\":38753,\"start\":38744},{\"end\":39083,\"start\":39074}]", "table": "[{\"end\":38138,\"start\":37486},{\"end\":38742,\"start\":38306},{\"end\":39072,\"start\":38971},{\"end\":39655,\"start\":39181}]", "figure_caption": "[{\"end\":35490,\"start\":35408},{\"end\":35864,\"start\":35504},{\"end\":36186,\"start\":35867},{\"end\":36845,\"start\":36189},{\"end\":37156,\"start\":36870},{\"end\":37248,\"start\":37170},{\"end\":37375,\"start\":37262},{\"end\":37486,\"start\":37378},{\"end\":38306,\"start\":38151},{\"end\":38971,\"start\":38755},{\"end\":39181,\"start\":39085}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3959,\"start\":3951},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10870,\"start\":10862},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12543,\"start\":12535},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14484,\"start\":14476},{\"end\":26634,\"start\":26626},{\"end\":28320,\"start\":28312},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29528,\"start\":29520},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30042,\"start\":30034}]", "bib_author_first_name": "[{\"end\":39866,\"start\":39860},{\"end\":39885,\"start\":39874},{\"end\":39898,\"start\":39897},{\"end\":39909,\"start\":39905},{\"end\":40229,\"start\":40222},{\"end\":40243,\"start\":40237},{\"end\":40257,\"start\":40251},{\"end\":40606,\"start\":40602},{\"end\":40619,\"start\":40615},{\"end\":40638,\"start\":40633},{\"end\":40649,\"start\":40646},{\"end\":41070,\"start\":41063},{\"end\":41085,\"start\":41078},{\"end\":41096,\"start\":41093},{\"end\":41106,\"start\":41102},{\"end\":41108,\"start\":41107},{\"end\":41130,\"start\":41125},{\"end\":41544,\"start\":41539},{\"end\":41557,\"start\":41553},{\"end\":41561,\"start\":41558},{\"end\":41575,\"start\":41569},{\"end\":41589,\"start\":41581},{\"end\":42187,\"start\":42181},{\"end\":42201,\"start\":42194},{\"end\":42209,\"start\":42206},{\"end\":42215,\"start\":42210},{\"end\":42230,\"start\":42222},{\"end\":42657,\"start\":42651},{\"end\":42663,\"start\":42662},{\"end\":42683,\"start\":42677},{\"end\":43175,\"start\":43169},{\"end\":43185,\"start\":43180},{\"end\":43200,\"start\":43192},{\"end\":43211,\"start\":43205},{\"end\":43722,\"start\":43719},{\"end\":43736,\"start\":43729},{\"end\":43751,\"start\":43744},{\"end\":43766,\"start\":43758},{\"end\":44243,\"start\":44236},{\"end\":44255,\"start\":44248},{\"end\":44271,\"start\":44263},{\"end\":44281,\"start\":44277},{\"end\":44829,\"start\":44822},{\"end\":44840,\"start\":44834},{\"end\":45228,\"start\":45220},{\"end\":45237,\"start\":45233},{\"end\":45249,\"start\":45244},{\"end\":45259,\"start\":45256},{\"end\":45268,\"start\":45264},{\"end\":45273,\"start\":45269},{\"end\":45285,\"start\":45281},{\"end\":45920,\"start\":45912},{\"end\":45932,\"start\":45925},{\"end\":45943,\"start\":45937},{\"end\":45956,\"start\":45948},{\"end\":46386,\"start\":46378},{\"end\":46395,\"start\":46391},{\"end\":46409,\"start\":46402},{\"end\":46424,\"start\":46417},{\"end\":46433,\"start\":46430},{\"end\":46446,\"start\":46438},{\"end\":46813,\"start\":46805},{\"end\":46824,\"start\":46818},{\"end\":46837,\"start\":46831},{\"end\":46849,\"start\":46842},{\"end\":46863,\"start\":46856},{\"end\":46877,\"start\":46869},{\"end\":47339,\"start\":47331},{\"end\":47351,\"start\":47344},{\"end\":47362,\"start\":47359},{\"end\":47366,\"start\":47363},{\"end\":47380,\"start\":47372},{\"end\":47939,\"start\":47931},{\"end\":47948,\"start\":47944},{\"end\":47960,\"start\":47956},{\"end\":47976,\"start\":47967},{\"end\":47991,\"start\":47983},{\"end\":48399,\"start\":48394},{\"end\":48416,\"start\":48409},{\"end\":48429,\"start\":48428},{\"end\":48431,\"start\":48430},{\"end\":48444,\"start\":48440},{\"end\":48446,\"start\":48445},{\"end\":48458,\"start\":48454},{\"end\":48469,\"start\":48468},{\"end\":48483,\"start\":48478},{\"end\":48496,\"start\":48491},{\"end\":48508,\"start\":48505},{\"end\":48510,\"start\":48509},{\"end\":48523,\"start\":48518},{\"end\":48540,\"start\":48533},{\"end\":48553,\"start\":48550},{\"end\":48555,\"start\":48554},{\"end\":48569,\"start\":48564},{\"end\":49176,\"start\":49169},{\"end\":49188,\"start\":49185},{\"end\":49201,\"start\":49195},{\"end\":49748,\"start\":49747},{\"end\":49764,\"start\":49759},{\"end\":49768,\"start\":49765},{\"end\":50144,\"start\":50138},{\"end\":50660,\"start\":50655},{\"end\":50673,\"start\":50668},{\"end\":50675,\"start\":50674},{\"end\":50693,\"start\":50686},{\"end\":50695,\"start\":50694},{\"end\":50709,\"start\":50705},{\"end\":51121,\"start\":51118},{\"end\":51134,\"start\":51127},{\"end\":51152,\"start\":51142},{\"end\":51166,\"start\":51158},{\"end\":51180,\"start\":51173},{\"end\":51191,\"start\":51186},{\"end\":51692,\"start\":51685},{\"end\":51705,\"start\":51698},{\"end\":51714,\"start\":51710},{\"end\":51722,\"start\":51720},{\"end\":51733,\"start\":51727},{\"end\":52058,\"start\":52053},{\"end\":52067,\"start\":52064},{\"end\":52077,\"start\":52072},{\"end\":52638,\"start\":52631},{\"end\":52647,\"start\":52643},{\"end\":52656,\"start\":52653},{\"end\":52667,\"start\":52664},{\"end\":52679,\"start\":52674},{\"end\":53093,\"start\":53086},{\"end\":53103,\"start\":53098},{\"end\":53114,\"start\":53110},{\"end\":53127,\"start\":53120},{\"end\":53139,\"start\":53134},{\"end\":53480,\"start\":53474},{\"end\":53493,\"start\":53486},{\"end\":53501,\"start\":53499},{\"end\":53512,\"start\":53508},{\"end\":53524,\"start\":53517},{\"end\":53539,\"start\":53531},{\"end\":54044,\"start\":54037},{\"end\":54053,\"start\":54050},{\"end\":54063,\"start\":54060},{\"end\":54077,\"start\":54070},{\"end\":54085,\"start\":54083},{\"end\":54573,\"start\":54566},{\"end\":54591,\"start\":54582},{\"end\":54611,\"start\":54607},{\"end\":54625,\"start\":54621},{\"end\":55134,\"start\":55125},{\"end\":55144,\"start\":55141},{\"end\":55153,\"start\":55150},{\"end\":55164,\"start\":55158},{\"end\":55173,\"start\":55171},{\"end\":55705,\"start\":55702},{\"end\":55717,\"start\":55711},{\"end\":55726,\"start\":55723},{\"end\":55740,\"start\":55733},{\"end\":55749,\"start\":55746},{\"end\":55763,\"start\":55756},{\"end\":55780,\"start\":55771},{\"end\":55790,\"start\":55787},{\"end\":56269,\"start\":56263},{\"end\":56281,\"start\":56275},{\"end\":56292,\"start\":56287},{\"end\":56764,\"start\":56758},{\"end\":56775,\"start\":56771},{\"end\":56790,\"start\":56782},{\"end\":56802,\"start\":56795},{\"end\":56818,\"start\":56810},{\"end\":57535,\"start\":57529},{\"end\":57547,\"start\":57542},{\"end\":57557,\"start\":57553},{\"end\":57976,\"start\":57971},{\"end\":57991,\"start\":57983},{\"end\":58001,\"start\":57996},{\"end\":58011,\"start\":58007},{\"end\":58025,\"start\":58017},{\"end\":58550,\"start\":58545},{\"end\":58565,\"start\":58557},{\"end\":58574,\"start\":58570},{\"end\":58585,\"start\":58581},{\"end\":58600,\"start\":58592},{\"end\":59214,\"start\":59208},{\"end\":59228,\"start\":59220},{\"end\":59238,\"start\":59233},{\"end\":59252,\"start\":59245},{\"end\":59266,\"start\":59259},{\"end\":59280,\"start\":59272},{\"end\":59810,\"start\":59804},{\"end\":59821,\"start\":59816},{\"end\":59836,\"start\":59828},{\"end\":59848,\"start\":59841},{\"end\":59858,\"start\":59854},{\"end\":59872,\"start\":59864},{\"end\":60324,\"start\":60318},{\"end\":60335,\"start\":60330},{\"end\":60349,\"start\":60342},{\"end\":60866,\"start\":60864},{\"end\":60874,\"start\":60871},{\"end\":60888,\"start\":60881},{\"end\":60901,\"start\":60895},{\"end\":60910,\"start\":60906},{\"end\":60920,\"start\":60916},{\"end\":61350,\"start\":61347},{\"end\":61364,\"start\":61357},{\"end\":61376,\"start\":61369},{\"end\":61387,\"start\":61383},{\"end\":61409,\"start\":61402},{\"end\":61411,\"start\":61410},{\"end\":61426,\"start\":61422},{\"end\":62021,\"start\":62015},{\"end\":62031,\"start\":62026},{\"end\":62047,\"start\":62039},{\"end\":62054,\"start\":62052},{\"end\":62063,\"start\":62061},{\"end\":62076,\"start\":62071},{\"end\":62404,\"start\":62397},{\"end\":62414,\"start\":62410},{\"end\":62426,\"start\":62422},{\"end\":62436,\"start\":62433}]", "bib_author_last_name": "[{\"end\":39872,\"start\":39867},{\"end\":39895,\"start\":39886},{\"end\":39903,\"start\":39899},{\"end\":39918,\"start\":39910},{\"end\":39930,\"start\":39920},{\"end\":40235,\"start\":40230},{\"end\":40249,\"start\":40244},{\"end\":40260,\"start\":40258},{\"end\":40613,\"start\":40607},{\"end\":40631,\"start\":40620},{\"end\":40644,\"start\":40639},{\"end\":40654,\"start\":40650},{\"end\":41076,\"start\":41071},{\"end\":41091,\"start\":41086},{\"end\":41100,\"start\":41097},{\"end\":41123,\"start\":41109},{\"end\":41142,\"start\":41131},{\"end\":41551,\"start\":41545},{\"end\":41567,\"start\":41562},{\"end\":41579,\"start\":41576},{\"end\":41599,\"start\":41590},{\"end\":42192,\"start\":42188},{\"end\":42204,\"start\":42202},{\"end\":42220,\"start\":42216},{\"end\":42235,\"start\":42231},{\"end\":42660,\"start\":42658},{\"end\":42675,\"start\":42664},{\"end\":42688,\"start\":42684},{\"end\":42694,\"start\":42690},{\"end\":43178,\"start\":43176},{\"end\":43190,\"start\":43186},{\"end\":43203,\"start\":43201},{\"end\":43214,\"start\":43212},{\"end\":43727,\"start\":43723},{\"end\":43742,\"start\":43737},{\"end\":43756,\"start\":43752},{\"end\":43771,\"start\":43767},{\"end\":44246,\"start\":44244},{\"end\":44261,\"start\":44256},{\"end\":44275,\"start\":44272},{\"end\":44285,\"start\":44282},{\"end\":44832,\"start\":44830},{\"end\":44848,\"start\":44841},{\"end\":45231,\"start\":45229},{\"end\":45242,\"start\":45238},{\"end\":45254,\"start\":45250},{\"end\":45262,\"start\":45260},{\"end\":45279,\"start\":45274},{\"end\":45290,\"start\":45286},{\"end\":45923,\"start\":45921},{\"end\":45935,\"start\":45933},{\"end\":45946,\"start\":45944},{\"end\":45961,\"start\":45957},{\"end\":46389,\"start\":46387},{\"end\":46400,\"start\":46396},{\"end\":46415,\"start\":46410},{\"end\":46428,\"start\":46425},{\"end\":46436,\"start\":46434},{\"end\":46451,\"start\":46447},{\"end\":46816,\"start\":46814},{\"end\":46829,\"start\":46825},{\"end\":46840,\"start\":46838},{\"end\":46854,\"start\":46850},{\"end\":46867,\"start\":46864},{\"end\":46882,\"start\":46878},{\"end\":47342,\"start\":47340},{\"end\":47357,\"start\":47352},{\"end\":47370,\"start\":47367},{\"end\":47385,\"start\":47381},{\"end\":47942,\"start\":47940},{\"end\":47954,\"start\":47949},{\"end\":47965,\"start\":47961},{\"end\":47981,\"start\":47977},{\"end\":47994,\"start\":47992},{\"end\":48407,\"start\":48400},{\"end\":48426,\"start\":48417},{\"end\":48438,\"start\":48432},{\"end\":48452,\"start\":48447},{\"end\":48466,\"start\":48459},{\"end\":48476,\"start\":48470},{\"end\":48489,\"start\":48484},{\"end\":48503,\"start\":48497},{\"end\":48516,\"start\":48511},{\"end\":48531,\"start\":48524},{\"end\":48548,\"start\":48541},{\"end\":48562,\"start\":48556},{\"end\":48575,\"start\":48570},{\"end\":48583,\"start\":48577},{\"end\":49183,\"start\":49177},{\"end\":49193,\"start\":49189},{\"end\":49209,\"start\":49202},{\"end\":49757,\"start\":49749},{\"end\":49775,\"start\":49769},{\"end\":49779,\"start\":49777},{\"end\":50150,\"start\":50145},{\"end\":50666,\"start\":50661},{\"end\":50684,\"start\":50676},{\"end\":50703,\"start\":50696},{\"end\":50716,\"start\":50710},{\"end\":51125,\"start\":51122},{\"end\":51140,\"start\":51135},{\"end\":51156,\"start\":51153},{\"end\":51171,\"start\":51167},{\"end\":51184,\"start\":51181},{\"end\":51203,\"start\":51192},{\"end\":51696,\"start\":51693},{\"end\":51708,\"start\":51706},{\"end\":51718,\"start\":51715},{\"end\":51725,\"start\":51723},{\"end\":51738,\"start\":51734},{\"end\":52062,\"start\":52059},{\"end\":52070,\"start\":52068},{\"end\":52082,\"start\":52078},{\"end\":52641,\"start\":52639},{\"end\":52651,\"start\":52648},{\"end\":52662,\"start\":52657},{\"end\":52672,\"start\":52668},{\"end\":52683,\"start\":52680},{\"end\":53096,\"start\":53094},{\"end\":53108,\"start\":53104},{\"end\":53118,\"start\":53115},{\"end\":53132,\"start\":53128},{\"end\":53143,\"start\":53140},{\"end\":53484,\"start\":53481},{\"end\":53497,\"start\":53494},{\"end\":53506,\"start\":53502},{\"end\":53515,\"start\":53513},{\"end\":53529,\"start\":53525},{\"end\":53542,\"start\":53540},{\"end\":54048,\"start\":54045},{\"end\":54058,\"start\":54054},{\"end\":54068,\"start\":54064},{\"end\":54081,\"start\":54078},{\"end\":54091,\"start\":54086},{\"end\":54580,\"start\":54574},{\"end\":54605,\"start\":54592},{\"end\":54619,\"start\":54612},{\"end\":54640,\"start\":54626},{\"end\":55139,\"start\":55135},{\"end\":55148,\"start\":55145},{\"end\":55156,\"start\":55154},{\"end\":55169,\"start\":55165},{\"end\":55178,\"start\":55174},{\"end\":55709,\"start\":55706},{\"end\":55721,\"start\":55718},{\"end\":55731,\"start\":55727},{\"end\":55744,\"start\":55741},{\"end\":55754,\"start\":55750},{\"end\":55769,\"start\":55764},{\"end\":55785,\"start\":55781},{\"end\":55796,\"start\":55791},{\"end\":56273,\"start\":56270},{\"end\":56285,\"start\":56282},{\"end\":56297,\"start\":56293},{\"end\":56769,\"start\":56765},{\"end\":56780,\"start\":56776},{\"end\":56793,\"start\":56791},{\"end\":56808,\"start\":56803},{\"end\":56823,\"start\":56819},{\"end\":57540,\"start\":57536},{\"end\":57551,\"start\":57548},{\"end\":57562,\"start\":57558},{\"end\":57981,\"start\":57977},{\"end\":57994,\"start\":57992},{\"end\":58005,\"start\":58002},{\"end\":58015,\"start\":58012},{\"end\":58030,\"start\":58026},{\"end\":58555,\"start\":58551},{\"end\":58568,\"start\":58566},{\"end\":58579,\"start\":58575},{\"end\":58590,\"start\":58586},{\"end\":58605,\"start\":58601},{\"end\":59218,\"start\":59215},{\"end\":59231,\"start\":59229},{\"end\":59243,\"start\":59239},{\"end\":59257,\"start\":59253},{\"end\":59270,\"start\":59267},{\"end\":59285,\"start\":59281},{\"end\":59814,\"start\":59811},{\"end\":59826,\"start\":59822},{\"end\":59839,\"start\":59837},{\"end\":59852,\"start\":59849},{\"end\":59862,\"start\":59859},{\"end\":59877,\"start\":59873},{\"end\":60328,\"start\":60325},{\"end\":60340,\"start\":60336},{\"end\":60353,\"start\":60350},{\"end\":60869,\"start\":60867},{\"end\":60879,\"start\":60875},{\"end\":60893,\"start\":60889},{\"end\":60904,\"start\":60902},{\"end\":60914,\"start\":60911},{\"end\":60925,\"start\":60921},{\"end\":61355,\"start\":61351},{\"end\":61367,\"start\":61365},{\"end\":61381,\"start\":61377},{\"end\":61400,\"start\":61388},{\"end\":61420,\"start\":61412},{\"end\":61435,\"start\":61427},{\"end\":62024,\"start\":62022},{\"end\":62037,\"start\":62032},{\"end\":62050,\"start\":62048},{\"end\":62059,\"start\":62055},{\"end\":62069,\"start\":62064},{\"end\":62080,\"start\":62077},{\"end\":62408,\"start\":62405},{\"end\":62420,\"start\":62415},{\"end\":62431,\"start\":62427},{\"end\":62441,\"start\":62437}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2002.04692\",\"id\":\"b0\"},\"end\":40159,\"start\":39858},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":64908139},\"end\":40491,\"start\":40161},{\"attributes\":{\"doi\":\"10.1145/3298689.3347038\",\"id\":\"b2\",\"matched_paper_id\":202640625},\"end\":40992,\"start\":40493},{\"attributes\":{\"doi\":\"10.1145/3291060\",\"id\":\"b3\",\"matched_paper_id\":53290927},\"end\":41455,\"start\":40994},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52967399},\"end\":42082,\"start\":41457},{\"attributes\":{\"doi\":\"10.1109/TMM.2021.3088281\",\"id\":\"b5\",\"matched_paper_id\":236698463},\"end\":42557,\"start\":42084},{\"attributes\":{\"doi\":\"10.1145/3357154\",\"id\":\"b6\",\"matched_paper_id\":195657811},\"end\":43053,\"start\":42559},{\"attributes\":{\"doi\":\"10.1145/3394171.3413628\",\"id\":\"b7\",\"matched_paper_id\":222278158},\"end\":43645,\"start\":43055},{\"attributes\":{\"doi\":\"10.1109/ICCV.2015.486\",\"id\":\"b8\",\"matched_paper_id\":1975342},\"end\":44188,\"start\":43647},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":44753,\"start\":44190},{\"attributes\":{\"doi\":\"10.1609/aaai.v30i1.9973\",\"id\":\"b10\",\"matched_paper_id\":3099285},\"end\":45137,\"start\":44755},{\"attributes\":{\"doi\":\"10.1145/3397271.3401063\",\"id\":\"b11\",\"matched_paper_id\":211043589},\"end\":45857,\"start\":45139},{\"attributes\":{\"doi\":\"10.1145/3209978.3209981\",\"id\":\"b12\",\"matched_paper_id\":49641771},\"end\":46344,\"start\":45859},{\"attributes\":{\"doi\":\"10.1145/3038912.3052569\",\"id\":\"b13\",\"matched_paper_id\":13907106},\"end\":46736,\"start\":46346},{\"attributes\":{\"doi\":\"10.1109/TNNLS.2018.2890117\",\"id\":\"b14\",\"matched_paper_id\":53286389},\"end\":47253,\"start\":46738},{\"attributes\":{\"doi\":\"10.1145/2911451.2911489\",\"id\":\"b15\",\"matched_paper_id\":2896685},\"end\":47929,\"start\":47255},{\"attributes\":{\"doi\":\"arXiv:2205.06532\",\"id\":\"b16\"},\"end\":48336,\"start\":47931},{\"attributes\":{\"doi\":\"10.1109/ICASSP.2017.7952132\",\"id\":\"b17\",\"matched_paper_id\":8810481},\"end\":49098,\"start\":48338},{\"attributes\":{\"doi\":\"10.1145/2487575.2487589\",\"id\":\"b18\",\"matched_paper_id\":207204749},\"end\":49701,\"start\":49100},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6628106},\"end\":50052,\"start\":49703},{\"attributes\":{\"doi\":\"10.1145/1401890.1401944\",\"id\":\"b20\",\"matched_paper_id\":207168823},\"end\":50599,\"start\":50054},{\"attributes\":{\"doi\":\"10.1145/3178876.3186150\",\"id\":\"b21\",\"matched_paper_id\":3361310},\"end\":51042,\"start\":50601},{\"attributes\":{\"doi\":\"10.1145/3343031.3350953\",\"id\":\"b22\",\"matched_paper_id\":201125251},\"end\":51639,\"start\":51044},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":239769002},\"end\":51987,\"start\":51641},{\"attributes\":{\"doi\":\"10.1145/3077136.3080658\",\"id\":\"b24\",\"matched_paper_id\":35609905},\"end\":52586,\"start\":51989},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":174800708},\"end\":53026,\"start\":52588},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":202789109},\"end\":53389,\"start\":53028},{\"attributes\":{\"doi\":\"10.1145/3459637.3482291\",\"id\":\"b27\",\"matched_paper_id\":240070722},\"end\":53952,\"start\":53391},{\"attributes\":{\"doi\":\"10.1145/3474085.3475266\",\"id\":\"b28\",\"matched_paper_id\":235743179},\"end\":54505,\"start\":53954},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10795036},\"end\":55019,\"start\":54507},{\"attributes\":{\"doi\":\"10.1109/IJCNN48605.2020.9206894\",\"id\":\"b30\",\"matched_paper_id\":221611120},\"end\":55646,\"start\":55021},{\"attributes\":{\"doi\":\"10.1145/3340531.3411947\",\"id\":\"b31\",\"matched_paper_id\":224281034},\"end\":56200,\"start\":55648},{\"attributes\":{\"doi\":\"10.1016/j.ipm.2020.102277\",\"id\":\"b32\",\"matched_paper_id\":219446727},\"end\":56670,\"start\":56202},{\"attributes\":{\"doi\":\"10.1145/3404835.3462962\",\"id\":\"b33\",\"matched_paper_id\":235185402},\"end\":57409,\"start\":56672},{\"attributes\":{\"doi\":\"10.1145/3485447.3512251\",\"id\":\"b34\",\"matched_paper_id\":248367478},\"end\":57909,\"start\":57411},{\"attributes\":{\"doi\":\"10.1145/3292500.3330989\",\"id\":\"b35\",\"matched_paper_id\":159042183},\"end\":58505,\"start\":57911},{\"attributes\":{\"doi\":\"10.1145/3331184.3331267\",\"id\":\"b36\",\"matched_paper_id\":150380651},\"end\":59113,\"start\":58507},{\"attributes\":{\"doi\":\"10.1145/3343031.3351034\",\"id\":\"b37\",\"matched_paper_id\":201701022},\"end\":59732,\"start\":59115},{\"attributes\":{\"doi\":\"10.1109/TMM.2021.3088307\",\"id\":\"b38\",\"matched_paper_id\":236705901},\"end\":60188,\"start\":59734},{\"attributes\":{\"doi\":\"10.1145/3394171.3413556\",\"id\":\"b39\",\"matched_paper_id\":222278410},\"end\":60787,\"start\":60190},{\"attributes\":{\"doi\":\"10.1109/TKDE.2019.2913394\",\"id\":\"b40\",\"matched_paper_id\":118683953},\"end\":61274,\"start\":60789},{\"attributes\":{\"doi\":\"10.1145/3219819.3219890\",\"id\":\"b41\",\"matched_paper_id\":46949657},\"end\":61972,\"start\":61276},{\"attributes\":{\"doi\":\"10.1145/3178876.3186146\",\"id\":\"b42\",\"matched_paper_id\":4531813},\"end\":62395,\"start\":61974},{\"attributes\":{\"doi\":\"arXiv:2205.07499\",\"id\":\"b43\"},\"end\":62764,\"start\":62397}]", "bib_title": "[{\"end\":40220,\"start\":40161},{\"end\":40600,\"start\":40493},{\"end\":41061,\"start\":40994},{\"end\":41537,\"start\":41457},{\"end\":42179,\"start\":42084},{\"end\":42649,\"start\":42559},{\"end\":43167,\"start\":43055},{\"end\":43717,\"start\":43647},{\"end\":44234,\"start\":44190},{\"end\":44820,\"start\":44755},{\"end\":45218,\"start\":45139},{\"end\":45910,\"start\":45859},{\"end\":46376,\"start\":46346},{\"end\":46803,\"start\":46738},{\"end\":47329,\"start\":47255},{\"end\":48392,\"start\":48338},{\"end\":49167,\"start\":49100},{\"end\":49745,\"start\":49703},{\"end\":50136,\"start\":50054},{\"end\":50653,\"start\":50601},{\"end\":51116,\"start\":51044},{\"end\":51683,\"start\":51641},{\"end\":52051,\"start\":51989},{\"end\":52629,\"start\":52588},{\"end\":53084,\"start\":53028},{\"end\":53472,\"start\":53391},{\"end\":54035,\"start\":53954},{\"end\":54564,\"start\":54507},{\"end\":55123,\"start\":55021},{\"end\":55700,\"start\":55648},{\"end\":56261,\"start\":56202},{\"end\":56756,\"start\":56672},{\"end\":57527,\"start\":57411},{\"end\":57969,\"start\":57911},{\"end\":58543,\"start\":58507},{\"end\":59206,\"start\":59115},{\"end\":59802,\"start\":59734},{\"end\":60316,\"start\":60190},{\"end\":60862,\"start\":60789},{\"end\":61345,\"start\":61276},{\"end\":62013,\"start\":61974}]", "bib_author": "[{\"end\":39874,\"start\":39860},{\"end\":39897,\"start\":39874},{\"end\":39905,\"start\":39897},{\"end\":39920,\"start\":39905},{\"end\":39932,\"start\":39920},{\"end\":40237,\"start\":40222},{\"end\":40251,\"start\":40237},{\"end\":40262,\"start\":40251},{\"end\":40615,\"start\":40602},{\"end\":40633,\"start\":40615},{\"end\":40646,\"start\":40633},{\"end\":40656,\"start\":40646},{\"end\":41078,\"start\":41063},{\"end\":41093,\"start\":41078},{\"end\":41102,\"start\":41093},{\"end\":41125,\"start\":41102},{\"end\":41144,\"start\":41125},{\"end\":41553,\"start\":41539},{\"end\":41569,\"start\":41553},{\"end\":41581,\"start\":41569},{\"end\":41601,\"start\":41581},{\"end\":42194,\"start\":42181},{\"end\":42206,\"start\":42194},{\"end\":42222,\"start\":42206},{\"end\":42237,\"start\":42222},{\"end\":42662,\"start\":42651},{\"end\":42677,\"start\":42662},{\"end\":42690,\"start\":42677},{\"end\":42696,\"start\":42690},{\"end\":43180,\"start\":43169},{\"end\":43192,\"start\":43180},{\"end\":43205,\"start\":43192},{\"end\":43216,\"start\":43205},{\"end\":43729,\"start\":43719},{\"end\":43744,\"start\":43729},{\"end\":43758,\"start\":43744},{\"end\":43773,\"start\":43758},{\"end\":44248,\"start\":44236},{\"end\":44263,\"start\":44248},{\"end\":44277,\"start\":44263},{\"end\":44287,\"start\":44277},{\"end\":44834,\"start\":44822},{\"end\":44850,\"start\":44834},{\"end\":45233,\"start\":45220},{\"end\":45244,\"start\":45233},{\"end\":45256,\"start\":45244},{\"end\":45264,\"start\":45256},{\"end\":45281,\"start\":45264},{\"end\":45292,\"start\":45281},{\"end\":45925,\"start\":45912},{\"end\":45937,\"start\":45925},{\"end\":45948,\"start\":45937},{\"end\":45963,\"start\":45948},{\"end\":46391,\"start\":46378},{\"end\":46402,\"start\":46391},{\"end\":46417,\"start\":46402},{\"end\":46430,\"start\":46417},{\"end\":46438,\"start\":46430},{\"end\":46453,\"start\":46438},{\"end\":46818,\"start\":46805},{\"end\":46831,\"start\":46818},{\"end\":46842,\"start\":46831},{\"end\":46856,\"start\":46842},{\"end\":46869,\"start\":46856},{\"end\":46884,\"start\":46869},{\"end\":47344,\"start\":47331},{\"end\":47359,\"start\":47344},{\"end\":47372,\"start\":47359},{\"end\":47387,\"start\":47372},{\"end\":47944,\"start\":47931},{\"end\":47956,\"start\":47944},{\"end\":47967,\"start\":47956},{\"end\":47983,\"start\":47967},{\"end\":47996,\"start\":47983},{\"end\":48409,\"start\":48394},{\"end\":48428,\"start\":48409},{\"end\":48440,\"start\":48428},{\"end\":48454,\"start\":48440},{\"end\":48468,\"start\":48454},{\"end\":48478,\"start\":48468},{\"end\":48491,\"start\":48478},{\"end\":48505,\"start\":48491},{\"end\":48518,\"start\":48505},{\"end\":48533,\"start\":48518},{\"end\":48550,\"start\":48533},{\"end\":48564,\"start\":48550},{\"end\":48577,\"start\":48564},{\"end\":48585,\"start\":48577},{\"end\":49185,\"start\":49169},{\"end\":49195,\"start\":49185},{\"end\":49211,\"start\":49195},{\"end\":49759,\"start\":49747},{\"end\":49777,\"start\":49759},{\"end\":49781,\"start\":49777},{\"end\":50152,\"start\":50138},{\"end\":50668,\"start\":50655},{\"end\":50686,\"start\":50668},{\"end\":50705,\"start\":50686},{\"end\":50718,\"start\":50705},{\"end\":51127,\"start\":51118},{\"end\":51142,\"start\":51127},{\"end\":51158,\"start\":51142},{\"end\":51173,\"start\":51158},{\"end\":51186,\"start\":51173},{\"end\":51205,\"start\":51186},{\"end\":51698,\"start\":51685},{\"end\":51710,\"start\":51698},{\"end\":51720,\"start\":51710},{\"end\":51727,\"start\":51720},{\"end\":51740,\"start\":51727},{\"end\":52064,\"start\":52053},{\"end\":52072,\"start\":52064},{\"end\":52084,\"start\":52072},{\"end\":52643,\"start\":52631},{\"end\":52653,\"start\":52643},{\"end\":52664,\"start\":52653},{\"end\":52674,\"start\":52664},{\"end\":52685,\"start\":52674},{\"end\":53098,\"start\":53086},{\"end\":53110,\"start\":53098},{\"end\":53120,\"start\":53110},{\"end\":53134,\"start\":53120},{\"end\":53145,\"start\":53134},{\"end\":53486,\"start\":53474},{\"end\":53499,\"start\":53486},{\"end\":53508,\"start\":53499},{\"end\":53517,\"start\":53508},{\"end\":53531,\"start\":53517},{\"end\":53544,\"start\":53531},{\"end\":54050,\"start\":54037},{\"end\":54060,\"start\":54050},{\"end\":54070,\"start\":54060},{\"end\":54083,\"start\":54070},{\"end\":54093,\"start\":54083},{\"end\":54582,\"start\":54566},{\"end\":54607,\"start\":54582},{\"end\":54621,\"start\":54607},{\"end\":54642,\"start\":54621},{\"end\":55141,\"start\":55125},{\"end\":55150,\"start\":55141},{\"end\":55158,\"start\":55150},{\"end\":55171,\"start\":55158},{\"end\":55180,\"start\":55171},{\"end\":55711,\"start\":55702},{\"end\":55723,\"start\":55711},{\"end\":55733,\"start\":55723},{\"end\":55746,\"start\":55733},{\"end\":55756,\"start\":55746},{\"end\":55771,\"start\":55756},{\"end\":55787,\"start\":55771},{\"end\":55798,\"start\":55787},{\"end\":56275,\"start\":56263},{\"end\":56287,\"start\":56275},{\"end\":56299,\"start\":56287},{\"end\":56771,\"start\":56758},{\"end\":56782,\"start\":56771},{\"end\":56795,\"start\":56782},{\"end\":56810,\"start\":56795},{\"end\":56825,\"start\":56810},{\"end\":57542,\"start\":57529},{\"end\":57553,\"start\":57542},{\"end\":57564,\"start\":57553},{\"end\":57983,\"start\":57971},{\"end\":57996,\"start\":57983},{\"end\":58007,\"start\":57996},{\"end\":58017,\"start\":58007},{\"end\":58032,\"start\":58017},{\"end\":58557,\"start\":58545},{\"end\":58570,\"start\":58557},{\"end\":58581,\"start\":58570},{\"end\":58592,\"start\":58581},{\"end\":58607,\"start\":58592},{\"end\":59220,\"start\":59208},{\"end\":59233,\"start\":59220},{\"end\":59245,\"start\":59233},{\"end\":59259,\"start\":59245},{\"end\":59272,\"start\":59259},{\"end\":59287,\"start\":59272},{\"end\":59816,\"start\":59804},{\"end\":59828,\"start\":59816},{\"end\":59841,\"start\":59828},{\"end\":59854,\"start\":59841},{\"end\":59864,\"start\":59854},{\"end\":59879,\"start\":59864},{\"end\":60330,\"start\":60318},{\"end\":60342,\"start\":60330},{\"end\":60355,\"start\":60342},{\"end\":60871,\"start\":60864},{\"end\":60881,\"start\":60871},{\"end\":60895,\"start\":60881},{\"end\":60906,\"start\":60895},{\"end\":60916,\"start\":60906},{\"end\":60927,\"start\":60916},{\"end\":61357,\"start\":61347},{\"end\":61369,\"start\":61357},{\"end\":61383,\"start\":61369},{\"end\":61402,\"start\":61383},{\"end\":61422,\"start\":61402},{\"end\":61437,\"start\":61422},{\"end\":62026,\"start\":62015},{\"end\":62039,\"start\":62026},{\"end\":62052,\"start\":62039},{\"end\":62061,\"start\":62052},{\"end\":62071,\"start\":62061},{\"end\":62082,\"start\":62071},{\"end\":62410,\"start\":62397},{\"end\":62422,\"start\":62410},{\"end\":62433,\"start\":62422},{\"end\":62443,\"start\":62433}]", "bib_venue": "[{\"end\":40318,\"start\":40262},{\"end\":40721,\"start\":40679},{\"end\":41198,\"start\":41159},{\"end\":41750,\"start\":41601},{\"end\":42292,\"start\":42261},{\"end\":42750,\"start\":42711},{\"end\":43305,\"start\":43239},{\"end\":43861,\"start\":43794},{\"end\":44401,\"start\":44307},{\"end\":44920,\"start\":44873},{\"end\":45427,\"start\":45315},{\"end\":46079,\"start\":45986},{\"end\":46520,\"start\":46476},{\"end\":46967,\"start\":46910},{\"end\":47521,\"start\":47410},{\"end\":48113,\"start\":48012},{\"end\":48684,\"start\":48612},{\"end\":49327,\"start\":49234},{\"end\":49867,\"start\":49781},{\"end\":50268,\"start\":50175},{\"end\":50785,\"start\":50741},{\"end\":51294,\"start\":51228},{\"end\":51793,\"start\":51744},{\"end\":52218,\"start\":52107},{\"end\":52734,\"start\":52685},{\"end\":53194,\"start\":53145},{\"end\":53655,\"start\":53567},{\"end\":54182,\"start\":54116},{\"end\":54718,\"start\":54642},{\"end\":55285,\"start\":55211},{\"end\":55898,\"start\":55821},{\"end\":56361,\"start\":56324},{\"end\":56970,\"start\":56848},{\"end\":57629,\"start\":57587},{\"end\":58148,\"start\":58055},{\"end\":58741,\"start\":58630},{\"end\":59376,\"start\":59310},{\"end\":59934,\"start\":59903},{\"end\":60444,\"start\":60378},{\"end\":61003,\"start\":60952},{\"end\":61562,\"start\":61460},{\"end\":62149,\"start\":62105},{\"end\":62560,\"start\":62459},{\"end\":43358,\"start\":43307},{\"end\":43915,\"start\":43863},{\"end\":44482,\"start\":44403},{\"end\":45526,\"start\":45429},{\"end\":47619,\"start\":47523},{\"end\":49407,\"start\":49329},{\"end\":50348,\"start\":50270},{\"end\":50816,\"start\":50787},{\"end\":51347,\"start\":51296},{\"end\":52316,\"start\":52220},{\"end\":54235,\"start\":54184},{\"end\":54781,\"start\":54720},{\"end\":55346,\"start\":55287},{\"end\":57079,\"start\":56972},{\"end\":57658,\"start\":57631},{\"end\":58228,\"start\":58150},{\"end\":58839,\"start\":58743},{\"end\":59429,\"start\":59378},{\"end\":60497,\"start\":60446},{\"end\":61651,\"start\":61564},{\"end\":62180,\"start\":62151}]"}}}, "year": 2023, "month": 12, "day": 17}