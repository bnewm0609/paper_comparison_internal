{"id": 248970443, "updated": "2023-10-05 14:22:59.483", "metadata": {"title": "Knowledge Graph - Deep Learning: A Case Study in Question Answering in Aviation Safety Domain", "authors": "[{\"first\":\"Ankush\",\"last\":\"Agarwal\",\"middle\":[]},{\"first\":\"Raj\",\"last\":\"Gite\",\"middle\":[]},{\"first\":\"Shreya\",\"last\":\"Laddha\",\"middle\":[]},{\"first\":\"Pushpak\",\"last\":\"Bhattacharyya\",\"middle\":[]},{\"first\":\"Satyanarayan\",\"last\":\"Kar\",\"middle\":[]},{\"first\":\"Asif\",\"last\":\"Ekbal\",\"middle\":[]},{\"first\":\"Prabhjit\",\"last\":\"Thind\",\"middle\":[]},{\"first\":\"Rajesh\",\"last\":\"Zele\",\"middle\":[]},{\"first\":\"Ravi\",\"last\":\"Shankar\",\"middle\":[]}]", "venue": "LREC", "journal": "Proceedings of the Thirteenth Language Resources and Evaluation Conference", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In the commercial aviation domain, there are a large number of documents, like accident reports of NTSB and ASRS, and regulatory directives ADs. There is a need for a system to efficiently access these diverse repositories to serve the demands of the aviation industry, such as maintenance, compliance, and safety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning (DL) based Question Answering (QA) system to cater to these requirements. We construct a KG from aircraft accident reports and contribute this resource to the community of researchers. The efficacy of this resource is tested and proved by the proposed QA system. Questions in Natural Language are converted into SPARQL (the interface language of the RDF graph database) queries and are answered from the KG. On the DL side, we examine two different QA models, BERT-QA and GPT3-QA, covering the two paradigms of answer formulation in QA. We evaluate our system on a set of handcrafted queries curated from the accident reports. Our hybrid KG + DL QA system, KGQA + BERT-QA, achieves 7% and 40.3% increase in accuracy over KGQA and BERT-QA systems respectively. Similarly, the other combined system, KGQA + GPT3-QA, achieves 29.3% and 9.3% increase in accuracy over KGQA and GPT3-QA systems respectively. Thus, we infer that the combination of KG and DL is better than either KG or DL individually for QA, at least in our chosen domain.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.15952", "mag": null, "acl": "2022.lrec-1.673", "pubmed": null, "pubmedcentral": null, "dblp": "conf/lrec/AgarwalGLBKETZS22", "doi": "10.48550/arxiv.2205.15952"}}, "content": {"source": {"pdf_hash": "46f49f4d9086ebaa1d191355065ebb6b1ad21222", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.lrec-1.673.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "02071e8324a0b88c9a705209137fab8e0ca59714", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/46f49f4d9086ebaa1d191355065ebb6b1ad21222.txt", "contents": "\nKnowledge Graph -Deep Learning: A Case Study in Question Answering in Aviation Safety Domain\nJune 2022\n\nAnkush Agarwal \nIIT Bombay\n\n\nRaj Gite \nIIT Bombay\n\n\nShreya Laddha \nIIT Bombay\n\n\nPushpak Bhattacharyya \nIIT Bombay\n\n\nSatyanarayan Kar \n3 IITHoneywell, Patna\n\nAsif Ekbal \nPrabhjit Thind prabhjit.thind@honeywell.com \n3 IITHoneywell, Patna\n\nRajesh Zele rajeshzele@ee.iitb.ac.in \nIIT Bombay\n\n\nRavi Shankar ravishankar.r@honeywell.com \n3 IITHoneywell, Patna\n\nKnowledge Graph -Deep Learning: A Case Study in Question Answering in Aviation Safety Domain\n\nProceedings of the 13th Conference on Language Resources and Evaluation (LREC 2022)\nthe 13th Conference on Language Resources and Evaluation (LREC 2022)MarseilleJune 2022Language Resources Association (ELRA), licensed under CC-BY-NC-4.0 6260Question AnsweringKnowledge Discovery/RepresentationInformation ExtractionInformation Retrieval\nIn the commercial aviation domain, there are a large number of documents, like accident reports of NTSB and ASRS, and regulatory directives ADs. There is a need for a system to efficiently access these diverse repositories to serve the demands of the aviation industry, such as maintenance, compliance, and safety. In this paper, we propose a Knowledge Graph (KG) guided Deep Learning (DL) based Question Answering (QA) system to cater to these requirements. We construct a KG from aircraft accident reports and contribute this resource to the community of researchers. The efficacy of this resource is tested and proved by the proposed QA system. Questions in Natural Language are converted into SPARQL (the interface language of the RDF graph database) queries and are answered from the KG. On the DL side, we examine two different QA models, BERT-QA and GPT3-QA, covering the two paradigms of answer formulation in QA. We evaluate our system on a set of handcrafted queries curated from the accident reports. Our hybrid KG + DL QA system, KGQA + BERT-QA, achieves 7% and 40.3% increase in accuracy over KGQA and BERT-QA systems respectively. Similarly, the other combined system, KGQA + GPT3-QA, achieves 29.3% and 9.3% increase in accuracy over KGQA and GPT3-QA systems respectively. Thus, we infer that the combination of KG and DL is better than either KG or DL individually for QA, at least in our chosen domain.\n\nIntroduction\n\nAn extensive set of documents is used in the aerospace sector, viz., system descriptions, manuals, and procedures. In addition to the industrial technical manuals, which have limited accessibility, there is a wide range of publicly available datasets related to aircraft safety. The majority of these datasets are subject to specific regulations or procedures, and safety professionals utilize them to investigate accidents and trends in aviation safety. Naturally, a Question Answering system is desired due to the large number and extensive length of such documents. For example, when investigating an accident, the ability to query the documents is required to locate similar events or find rare occurrences.\n\nIn this paper, we present an Aviation Knowledge Graph, constructed from the accident reports, to help safety experts study the reported accidents. It contains detailed information about the accidents' cause, effect, aircraft, pilot, and meteorological conditions. KGs provides a way to integrate and relate a large number of scattered, isolated pieces of information, thus converting it into knowledge that supports making complex inferences. However, domain expertise is required to construct a KG. Besides, KGs are not as expressive as natural language sentences to explain complex events. This motivates the need for Deep Learning models such as BERT (Devlin et al., 2018) from Google, and GPT-3 (Brown et al., 2020) from Open AI, which can fetch full-length answers from unstructured text in the documents. Our goal in this study is to combine the high coverage of textual corpora with the entity relationships in KG to improve the retrieval coverage and accuracy of the overall system.\n\nIn this paper, we show that such an architecture relying on both Knowledge Graph and Deep Learning can benefit Question Answering. Our major contributions in this paper are:\n\n1. We introduce an Aviation Knowledge Graph, constructed from several accident reports using the domain knowledge and information extraction techniques.\n\n2. We propose a Question Answering system that answers the questions asked in natural language from two modules -(a) KG-based QA (KGQA) module, which uses a pipeline to convert Natural Language Queries to SPARQL queries and fetches responses from the Aviation Knowledge Graph, and (b) DL-based QA (DLQA) module that extracts answers from the plain text in the documents. The DLQA module has been tested using two different QA models, BERT-QA and GPT3-QA.\n\n3. We show that a combined Question Answering system, such as ours, outperforms the individual Knowledge Graph and Deep Learning meth-ods on our curated test set. The combined system KGQA + BERT-QA attains 7% and 40.3% increase in accuracy over KGQA and BERT-QA modules respectively. Similarly, the other hybrid system KGQA + GPT3-QA attains 29.3% and 9.3% increase over KGQA and GPT3-QA modules respectively. Hence, we provide strong evidence that the two modules, KGQA and DLQA, complement each other, and neither of them is dispensable for the task of QA.\n\nThis paper is organized as follows. In Section 2, we present a brief survey of the literature. Section 3 describes the public data repositories in the aviation domain relevant for building a QA system. Section 4 shows the details for constructing the Aviation Knowledge Graph and a brief explanation of its properties. Section 5 explains the overall system architecture used for Question Answering. Section 6 discusses the models, test datasets, and metrics used for evaluation. A summary of the main findings is presented in Section 7. We conclude with a direction to future work in Section 8.\n\n\nRelated Work\n\nKnowledge Graph construction in the aviation field has been an important research topic. Zhao et al. (2018) shows the development of a KG construction system in the aviation risk field, where the American Aviation Safety Reporting System (ASRS) reports are used as an example to verify the rationality and validity of the KG construction method. Cheng et al. (2019) built an ontology for civil aviation security by extracting knowledge from the data sources in the form of entities and relations. Wang et al. (2020) also discusses method of extracting entities and relations from structured and unstructured text in aviation domain. However, no prior work has been done on the widely available NTSB accident reports. We exploit this gap and construct a KG from the aircraft accident reports for aviation safety.\n\nQuerying Knowledge Graphs in natural language has been a long-standing research challenge. Early work focused on rule-based, and pattern-based systems (Affolter et al., 2019) for the Text-to-SQL task, which later moved towards using seq2seq architecture (Zhong et al., 2017) and pre-trained models (He et al., 2019) with the advent of Neural Networks (NN). We are interested in translating Natural Language Queries to SPARQL -the standardized query language for RDF graph databases. Several QA systems have been developed that use rule-based approaches (Diefenbach et al., 2017) to answer questions over DBpedia and Wikidata. Some of the prior research (Singh et al., 2018) divides the whole QA pipeline into smaller sub-tasks while others (Diefenbach et al., 2020) combine several components to make a pipeline. We use an approach similar to Liang et al. (2021) in which the translation task is divided into KG-dependent and KG-independent sub-tasks.\n\nQuestion Answering using Deep Learning has been a widely explored area in general. However, not much progress has been made in the aviation domain due to the frequently occurring in-domain technical jargon. A few existing works use large pre-trained transformer-based language models for Question Answering. Kierszbaum and Lapasset (2020) use Distilled BERT for Question Answering on ASRS reports for a small set of documents and limited test data. Arnold et al. (2020) employs a BM25-based retriever, followed by BERT fine-tuned for QA on a general domain data set. This model is used as a baseline for benchmarking our QA system.\n\nOur work is the first attempt to combine Knowledge Graph and Deep Learning in the aviation domain for Question Answering to the best of our knowledge.\n\n\nData Repositories\n\nMultiple organizations investigate aircraft accidents and warehouse all the details of casualties in a database.\n\nWe identify four such public repositories, and demonstrate the use of NTSB accident reports and ADREP taxonomy in this case study. The recognized databases are the following:\n\n\u2022 Accident reports capture all the aspects of an accident, namely aircraft specifications, pilot details, environmental state, and a comprehensive description of the suspected cause of the accident. We use two publicly available accident repositories -National Transportation Safety Board (NTSB) reports 1 (a sample report is shown in Figure 3 in Appendix) and Aviation Safety Reporting System (ASRS) reports 2 . The NTSB stores investigation reports of civil aviation accidents in the US, whereas ASRS gathers information from pilots and crew members about close call events during flight journeys. These documents contain information in structured as well as unstructured format.\n\n\u2022 Airworthiness Directives (ADs) 3 are notifications to owners of certified aircrafts about the unsafe conditions that exist in particular aircraft models and the corresponding corrective measures, which are absent in the accident reports. In our work, ADs issued by the Federal Aviation Administration (FAA) of the United States are considered. \u2022 Accident/Incident Data Reporting (ADREP) Taxonomy 4 (a part of the taxonomy is shown in Figure 4 in Appendix) contains a complex multilevel hierarchy of factual descriptors (time, place, aircraft models, engine, component manufacturers, etc.) and analytical descriptors of the occurrence of accident, such as event types and explanatory factors. It is especially helpful in the construction of ontology.\n\n\nAviation Knowledge Graph\n\nWe construct an Aviation Knowledge Graph using NTSB reports and ADREP taxonomy in Prot\u00e9g\u00e9 (Musen, 2015). A total of 4000 NTSB reports from 1962 to 2015, having an average of 3000 words per report, are used in the construction. The NTSB reports consist of paragraphs (unstructured) and tables (structured) containing the information about aircraft accidents. This section discusses the pre-processing of NTSB reports, ontology creation, Knowledge Graph construction, KG evaluation, and challenges in the creation of Aviation KG. Figure 1 shows the pipeline for building a KG from NTSB reports.\n\n\nPre-processing\n\nWe preprocess the NTSB reports before proceeding with entity and relation extraction. First, the NTSB reports are converted from PDF to TXT files, followed by the application of techniques like stopword-removal, PoS tagging, and lemmatization.\n\n\nOntology Creation\n\nWith the help of domain experts and ADREP taxonomy, we construct an ontology from the accident reports. For example, domain knowledge from ADREP 'Events' taxonomy (a snippet is shown in Figure 4 in Appendix) is used for creating the Event class. 'Events' taxonomy is relevant because every accident report has an event sequence that gives a gist of the cause. For ontology creation, we extract the ADREP taxonomy in a tree-like data structure such that a unique path exists from the root to each leaf node. Subsequently, we obtain classes by mapping NTSB occurrences to the adequate root-to-leaf paths. We use the following two techniques for mapping:\n\n\u2022 Mapping based on the distance between BERT embeddings: We obtain embeddings for each path in the ADREP taxonomy and NTSB events using the BERT model. The distance is calculated between ADREP and NTSB embeddings, and the ADREP node with least distance is mapped to the NTSB event. After that, the corresponding rootto-leaf path in the ADREP tree hierarchy is associated with the NTSB event.\n\n\u2022 Mapping based on Keyword Matching: We tokenize the NTSB report events and map each token to the nodes in the ADREP taxonomy. The node with the highest similarity score is associated with the NTSB event.\n\nFollowing is an example of an ADREP event mapped to NTSB occurrence (more detailed ADREP hierarchy is shown in Figure 4).\n\nSample root-to-leaf path in ADREP Taxonomy\n\n\nAircraft Events\n\nOperation of the aircraft related event Aircraft handling related event Dragged wing/rotor/pod/float ADREP 'Events' taxonomy contains a number of events and its root node is 'Aircraft Events'. An event involving a 'dragging of a wing' is under the category of 'Aircraft handling'. This root-to-leaf path of 'Dragged wing/rotor/pod/float' ADREP event is mapped to 'DRAGGED WING, ROTOR, POD, FLOAT OR TAIL/SKID', which occurs as an event in an NTSB report.\n\nHowever, ADREP taxonomy is not always compatible with NTSB documents for creating classes. Hence, we apply following extraction techniques to the NTSB text for finding entity classes and their instances:\n\n\u2022 Named Entity Recognition (NER) (Nadeau and Sekine, 2007) is used to identify names, organizations, etc., from the NTSB text. These named entities are inserted as entity classes in our Ontology. For example, Long Beach, Las Vegas, and Chicago are the instances of Location class present in NTSB reports identified by the NER method.\n\n\u2022 Term Frequency -Inverse Document Frequency (TF-IDF) (Juan, 2003) technique is used to determine the important terms across NTSB documents, which are later organized as entity classes in Ontology. For example, Aircraft ID, Aircraft Damage, and Pilot Certificate are some of the important terms identified by the TF-IDF technique.\n\n\u2022 C-value (Frantzi et al., 2000) overcomes the drawback of TF-IDF in obtaining multi-word terms. Multi-words are necessary to identify the entities such as Landing Gear, Vertical Stabilizers, etc., in aircraft reports.\n\n\u2022 Sentence Clustering (Lu and Fu, 1978) is used for grouping similar sentences and discovering the entities among them.\n\n\u2022 Syntactic Analysis technique is used where the corpus is annotated with part-of-speech tags to Figure 1: Aviation Knowledge Graph Construction Process from NTSB reports find hypernyms, hyponyms, and meronyms. We extract all the sentences containing three or more nouns in the analysis. We then manually try to identify lexically related entities. The intuition behind selecting nouns is that most of the time, the entities (classes and instances) are tagged as nouns in the ontology. The examples of such findings are -(a) 'Scratches are found on engine's nacelle.' From this, we can observe that nacelle is part of engine. (b) 'During the engine inspection, it was observed that the wing mounted engine was working, but the fin mounted engine was not working.' Here, we observe that wing mounted engine and fin mounted engine are the types of engine.\n\n\u2022 DL based techniques are also used for entity extraction such as NER DL 5 (trained on CoNLL dataset (Sang and De Meulder, 2003) and uses GloVe word embeddings (Pennington et al., 2014)), Onto 100 and NER DL BERT (trained on CoNLL dataset and uses BERT embeddings).\n\nDependency Analysis (Fundel et al., 2007) and Open-IE 6 techniques are used for relation extraction. The extracted relations are observed and inserted as properties in our Aviation Ontology. Furthermore, we manually add some properties by observing the entity classes in Ontology from NTSB reports.\n\n\nKnowledge Graph Construction\n\nWe have an Aviation Ontology with entity classes, instances, object properties, and data properties. The entities and relations must be linked in the form of triples. We look at the entities and relations in the constructed ontology to extract triples from the NTSB reports using the regular expressions. These 5 https://deeppavlov.ai/ 6 https://nlp.stanford.edu/software/ openie.html extracted triples are inserted into the ontology to form Knowledge Graph.\n\nAn example for formation of triplets is as follows: 'Directional control -Not attained' -It is an 'Aircraft Issue' present in the 'Findings' section of NTSB report in tabular format (see Figure 3 in Appendix). In our Aviation Ontology, 'Directional control' is an instance present in Aircraft cause class, and 'Not attained' is an instance in Aircraft cause reason class. The triples (subject, relation, object) formed from this snippet are -{Accident Number, isCausedByAircraftIssue, Directional control}, {Directional control, isCausedDueTo-AircraftIssue, Not attained} Accident Number is the entity class in our Aviation Ontology containing the unique accident numbers of all NTSB reports as instances. Accident Number class is an essential point source for our question answering system, and thus, it is linked with Aircraft cause class using relation isCausedByAircraftIssue. In the second triplet, a relation, isCausedDueTo-AircraftIssue, is formed for linking the Aircraft cause class with Aircraft cause reason class. \n\n\nKnowledge Graph Evaluation\n\nWe manually evaluate the terms obtained through entity and relation extraction techniques. A domain expert in Honeywell Corporation provided a small set of cases to validate the reach of the constructed Knowledge Graph. A total of 120 SPARQL queries with gold answers of different categories were tested, where our KG answered 83 questions, thereby achieving an accuracy of 69.2%.\n\n\nChallenges in Construction of Aviation Knowledge Graph\n\nThe main challenge we faced with Aviation Knowledge Graph is scalability. As explained previously, we use many different techniques for extracting entities and relations. Later, we require manual processing for creating Ontology from these extracted entities. Similarly, extraction of triplets from NTSB reports requires observing patterns in reports which may change with new reports. Another challenge we observed is that the domain specific terms or keywords present in NTSB reports are not identified effectively by the extraction models.  \n\n\nThe Question Answering System\n\nKnowledge Graph guided Deep Learning based QA system is composed of two modules -KG-based QA (KGQA) and DL-based QA (DLQA). The KGQA utilizes Aviation KG, while DLQA uses the text in the NTSB accident reports. These two modules work in parallel to answer questions as shown in Figure 2. The input to the QA system is a question in natural language, which is fed to both the modules, and the output of the QA system is a combination of responses from the two modules such that the output of DLQA module follows that of the KGQA module. The output of the KGQA module is a list of answers -entities, relations, or properties of the Aviation Knowledge Graph. DLQA module returns answers and passages whose specifications depend on the underlying model. If BERT-QA is used, we obtain a list of answer-passage pairs as response where each answer is a text span extracted from the corresponding passage. GPT3-QA returns a list of passages and an answer derived from the passage list. Figure 5 (in Appendix) shows a sample output from KGQA and DLQA modules. In our work, we give equal importance to both answers and passages because passages help the user when answers fetched by the QA system are incorrect.\n\nOur proposal of combining KG and DL is based on the following rationale:\n\n\u2022 KGQA has the advantage of domain knowledge present in the KG that captures essential details and helps in answering complex questions correctly and completely.\n\n\u2022 DLQA lacks domain knowledge, but it has the advantage of coverage. With DLQA, all the text in the domain is considered for QA. This allows DLQA to answer questions that KGQA is unable to answer since KG does not capture all the information from the text.\n\nSection 7 provides empirical evidence that such a system formed with the synergy of Knowledge Graph and Deep Learning surpasses the performance of each individual module. In the following subsections, we discuss the design of these modules.\n\n\nKGQA module\n\nFor querying Knowledge Graph stored in Prot\u00e9g\u00e9, we need to convert the Natural Language Query into a SPARQL query. We adapt an approach similar to Liang et al. (2021) for our NL2SPARQL pipeline (see Figure  7 in Appendix) which consists of the following:\n\n\u2022 Question Type Classification: We classify the questions into List/Boolean/Count categories to identify the keyword from DIS-TINCT/ASK/COUNT to use in the SELECT clause and then construct the WHERE clause in the SPARQL query.\n\n\u2022 Entity-Relation Extraction: We implement techniques such as PoS tagging, Tokenization & Compounding, N-gram tiling, and dependency parsing for extracting entities and relations from the question. Then, we compute the similarity scores between the BERT embeddings to map the retrieved entities and relations to the corresponding entries in the underlying Knowledge Graph. This sub-task is the only KG-dependent sub-task in the pipeline.\n\n\u2022 Triple Generation and Ranking: RDF triples are constructed using all the permutations of the mapped entities and relations. However, only the valid ones are retained by performing a quick check with the Knowledge Graph by executing an ASK query. We then conduct a ranking step based on the syntactic similarity between the triples and the input question.\n\n\u2022 Query Construction: SPARQL queries are generated using the keyword specifying question type (DISTINCT for List, ASK for Boolean, COUNT for Count) and the triples obtained from the previous step. Proper PREFIX is also defined, specifying the base Knowledge Graph used.\n\nThe constructed SPARQL queries are executed over the Aviation Knowledge Graph to retrieve answers from KG which are either subjects or objects in the triples. The KGQA does not produce any response when the pipeline cannot form a valid triple in the Triple Generation & Ranking sub-task.\n\n\nDLQA module\n\nWe examine two models, namely BERT-QA and GPT3-QA (Brown et al., 2020), which form two independent DLQA modules. The reason for reviewing BERT-QA and GPT3-QA is to cover the two paradigms of answer formulation in QA, namely extractive and abstractive. Both the models require a collection of passages from the documents. BERT-QA extracts a text span from the relevant passage as an answer (extractive QA), while GPT3-QA generates text from the relevant passages (abstractive QA). Both these models are based on the retriever-reader pipeline as follows:\n\n\u2022 Retriever: The retriever retrieves top-k relevant passages given a question. It projects the question and all the passages in the collection to a semantic vector space, such that passages relevant to a question are positioned closer to the question as compared to less relevant passages. This projection of query and passages into semantic vector space is accomplished by Sentence-BERT (Reimers and Gurevych, 2019a) in case of BERT-QA and by GPT3 'ada' model in case of GPT3-QA 7 . Passages relevant to the question obtain a higher relevance score, and top-k passages are passed to the reader.\n\n\u2022 Reader: The reader extracts/generates answers from the relevant passages given a question. It is a Machine Reading Comprehension (MRC) task where the underlying model understands the context passage and tries to answer the given question. In BERT-QA, we employ BERT (Devlin et al., 2018) fine-tuned on MRC task as the reader, which extracts text spans from the passages as the answer. In GPT3-QA, we utilize GPT3 'curie' model as the reader, which generates text from the passages as an answer. 7 https://beta.openai.com/docs/guides/ answers Figure 8 (in Appendix) shows the retriever-reader pipeline along with an example.\n\n\nExperimental Setup\n\nData Pre-processing: The NTSB reports, available as PDFs, are converted to TXT format for constructing the Knowledge Graph (as mentioned in Section 4). Passages from these reports are extracted and stored as JSON objects with three properties -passage heading, passage text, and report ID. The resulting JSON files are used by the BERT-QA model. The reports are also converted to JSONL format, where each row contains the extracted passages. These JSONL files are consumed by the GPT3-QA model.\n\n\nModels:\n\nThe details of the models used in our Question Answering system are as follows:\n\n\u2022 The bert-base-nli-mean-tokens 8 model from the Sentence Transformers (Reimers and Gurevych, 2019b) library is used for similarity calculation in the KGQA module.\n\n\u2022 In the BERT-QA model, we use a finetuned Sentence BERT model, namely multi-qa-MiniLM-L6-cos-v1 9 from the Sentence Transformers library, as the retriever. This Sentence BERT model is fine-tuned for the Semantic Search task using 215M question-answer pairs. Similarly, a BERT model, deepset/bert-base-cased-squad2 10 from the Transformers library (Wolf et al., 2020), fine-tuned on MRC task using SQuAD 2.0 dataset (Rajpurkar et al., 2018) is used as the reader.\n\n\u2022 A baseline QA system is built on a similar approach as BERT-QA, called BM25-BERT. In this model, BM25 (Robertson and Zaragoza, 2009) relevance score is used for retrieving most relevant passages using sparse lexical features like TF-IDF. The BERT model in BM25-BERT has the same configurations as the reader in the BERT-QA model.\n\n\u2022 From various flavors of GPT-3, we chose a combination of ada and curie as retriever and reader respectively, in the GPT3-QA model, based on offline evaluations.\n\nOur QA system, which comprises of KGQA and DLQA modules, is configured to output ten responses such that the outputs from the DLQA follow those from KGQA. When both the modules have sufficient outputs to display, each module returns its top five 8 https://huggingface.co/sentencetransformers/bert-base-nli-mean-tokens 9 https://huggingface.co/sentencetransformers/multi-qa-MiniLM-L6-cos-v1 10 https://huggingface.co/deepset/bertbase-cased-squad2 results, else DLQA adjusts its return count to make up for the shortfall in the KGQA responses.\n\nTest Dataset: For evaluating the performance of our QA system, a test set is curated from 50 NTSB reports. We restrict the number to 50 by considering the difficulty in incorporating all 50 reports while creating a single test instance. All these reports belong to the year 2002. A total of 150 test instances were created (a single instance of test set is shown in Figure  6 in Appendix), where each instance consists of a query, a list of actual answers, and a list of passages where the answers can potentially be found. The entire test set is manually curated by looking up every query in those 50 reports and recording the desired answers and the paragraphs. We tag each paragraph with the unique accident number in its report, which aids in evaluating the retriever sub-module of DLQA. We do not evaluate our QA system with standard QA datasets because the KG used in KGQA module is domain-specific, and thus, does not contribute when our QA system operates on other domains.\n\nEvaluation Metrics: Our overall system outputs both answers and passages. Sometimes, the predicted answer may not be correct, so reading the passage allows the user to reasonably satisfy the query intent. Thus, both the accuracy of answers and passages become important, and we evaluate both separately. Finally, we report the performance using four metrics:\n\n\u2022 Exact Match (binary value): An exact match occurs if the first answer predicted by the system is present in the list of actual answers exactly.\n\n\u2022 Exact Recall: We determine exact recall as the fraction of actual answers available exactly in the top 10 answers predicted by the system. However, for more than 10 actual answers, we consider only a subset such that they contain the maximum number of predicted answers. This is done to ensure a recall \u2264 1 since we predict 10 answers only.\n\n\u2022 Semantic Accuracy (binary value): We use this as a flexible alternative to Exact Match, which relies solely on lexical overlap. We define our output as semantically accurate if any value in the list of top 10 predicted targets is semantically similar to any value in the list of actual targets. This metric does not consider the rank of the predicted targets, unlike Exact Match.\n\n\u2022 Semantic Recall: We compute this metric as the fraction of actual targets which are semantically similar to the top 10 predictions of the system. Similar to Exact Recall, we consider only a subset of 10 targets such that they contain the maximum number of predicted targets.\n\nExact Match and Exact Recall are calculated only for the answers predicted by the system, whereas Semantic Accuracy and Semantic Recall are computed for both the answers and the passages. Exact Match and Semantic Accuracy are a measure of correctness of our Question Answering system, whereas Exact Recall and Semantic Recall estimate the completeness of the outputs of our system.\n\n\nResults and Analysis\n\nThis section analyzes the performance of 6 models -BM25-BERT (baseline), KGQA, BERT-QA, GPT3-QA, KGQA + BERT-QA, and KGQA + GPT3-QA. The last two models are the combined models that use both KG and DL, whereas the first four are individual models. We evaluate both the answers and passages predicted by these models. Table 2 presents the evaluation results of answers on Exact Match and Exact Recall metrics as well as the evaluation results of answers and passages on Semantic Accuracy and Semantic Recall metrics. As the KGQA system only outputs answers, we consider the answers from KGQA as passages when evaluating passages in KGQA + BERT-QA and KGQA + GPT3-QA models.\n\nIt is trivial to see that the Exact Match metric scores are relatively lower than the Semantic Accuracy metric scores because of the dependence of Exact Match on only the lexical aspect of answers. Thus, we are more interested in Semantic Accuracy and Semantic Recall values. Each of the KGQA, BERT-QA, and GPT3-QA models beat the baseline model on every metric. KGQA system performs better than the BERT-QA model, which validates the importance of domain knowledge. GPT3-QA model is the best performing among the individual models owing to its large size and extensive pre-training.\n\nOur combined models perform better than their counterparts in most cases. Since the top answers from each system are merged to generate top-10 answers for the combined KG + DL system, accuracy and recall scores increase. However, due to the fact that our system outputs KGQA answers at the top of DLQA, the Exact Match score of KGQA + GPT3-QA is lower than that of GPT3-QA. In other scenarios, the KGQA + GPT3-QA system achieves an increase of 29.3% and 9.3% increase in answer accuracy over KGQA and GPT3-QA respectively. The KGQA + BERT-QA system also attains an increase in answer accuracy of 7% over KGQA and 40.3% over BERT-QA. A similar trend is seen in passage retrieval accuracy for both the hybrid QA models. Overall, KGQA + GPT3-QA is the best model for the QA task.\n\nLimitations of KGQA module: The low accuracy of the KGQA model can be attributed to the following factors -(a) Natural language interface -KGQA can answer questions efficiently only when valid SPARQL queries can be formed. The forma-  Limitations of DLQA module: The BERT-QA system suffers from the lack of fine-tuning on the domain dataset and hence the scores for this model are very close to the baseline. We notice that GPT3-QA does not answer multi-hop questions adequately, i.e., questions that can only be answered based on the information in two different paragraphs of the same document or multiple documents. For example, the question 'Which accidents involved aircraft operated by Johnny Thornley and manufactured by Subaru?' is not answered correctly by GPT3-QA. One needs to look at two different paragraphs in a document for fetching answers, where GPT3-QA fails. However, the responses to such questions can be obtained by traversing multiple triples in the Knowledge Graph.\n\nStrengths of the combined system (KGQA + DLQA): Our combined system overcomes the shortcomings of the individual systems. It answers both the questions listed in the above paragraphs correctly. This combination solves the coverage issue of Knowledge Graph and the lack of domain knowledge in Deep Learning. The questions which remain unanswered by the KGQA module are answered by the DLQA module and vice versa. Both the modules complement each other, and thus the system formed by their synergy achieves better accuracy than the individual components.\n\nThe resources contributed by us are publicly available on GitHub 11 .\n\n\nConclusion and Future Work\n\nAircraft safety is indispensable in the aviation domain. Aircraft accidents are unfortunate, and thus the reported accidents must be studied carefully. We have successfully created an Aviation Knowledge Graph from NTSB aircraft accident reports to help experts in their study. We also provide a Knowledge Graph guided Deep Learning based Question Answering system, which outperforms the individual KGQA and DLQA systems. The dominance of the combined QA system is proved theoretically and empirically by evaluating it on our handcrafted test set.\n\nCurrently, the constructed Knowledge Graph is based on the NTSB reports. We will continue our work on Knowledge Graph construction for ASRS reports and ADs. We aim to merge all the KGs to expand our knowledge scope in aviation safety. KGs can be extensively utilized if there is a robust querying mechanism. Thus, we need to improve our NL2SPARQL mechanism so that complex Natural Language Queries can be converted correctly to SPARQL queries. We have shown that KGQA + GPT3-QA model performs best on most metrics. We plan to improve the system by devising a mechanism to re-rank the results of the combined system or by implementing a solid combination framework. \n\n\nAppendix\n\nFigure 2 :\n2Knowledge Graph guided Deep Learning based Question Answering system\n\nFigure 3 :Figure 4 :Figure 5 :Figure 6 :Figure 7 :Figure 8 :\n345678A snippet of NTSB report A snippet of ADREP Events Taxonomy (a) An example depicting the output of KGQA module (b) An example depicting the output of DLQA module when the underlying model is BERT-QA (c) An example depicting the output of DLQA module when the underlying model is GPT3-QA Examples defining QA task A single instance in the QA Test Set NL2SPARQL pipeline for KGQA system Retriever-Reader pipeline for DLQA system. The highlighted text in the passage is the answer extracted by the DLQA system.\n\nTable 1\n1describes the properties for Aviation Knowl-\nedge Graph constructed from the NTSB reports \nin Prot\u00e9g\u00e9. The total size of file containing the \nKnowledge Graph is around 12 MB. \n\n\n\nTable 1 :\n1Properties of Aviation Knowledge Graph:Aviation KG is constructed in Prot\u00e9g\u00e9 where the class count and instances are displayed.\n\nTable 2 :\n2Evaluation results of 'Answers' and 'Passages' predicted by Question Answering models on Exact Match, Exact Recall, Semantic Accuracy and Semantic Recall metrics. In most metrics, KGQA + GPT3-QA performs better compared to other models. tion of the SPARQL queries relies heavily on the NL2SPARQL pipeline, which being rule-based, is prone to errors. (b) Coverage -Not every sentence can be converted to a triple format; hence the amount of information present in the Knowledge Graph is limited. For example, the question 'What discrepancy was noted due to which flight landed at La Belle Municipal Airport?', which expects an answer 'problem with fuel gauge' cannot be answered using our KG as there exists no valid triple in the KG containing such information.\nhttps://www.icao.int/safety/ airnavigation/aig/pages/adreptaxonomies.aspx\n\nA comparative survey of recent natural language interfaces for databases. K Affolter, K Stockinger, A Bernstein, The VLDB Journal. 285Affolter, K., Stockinger, K., and Bernstein, A. (2019). A comparative survey of recent natural lan- guage interfaces for databases. The VLDB Journal, 28(5):793-819.\n\nA question-answering system for aircraft pilots' documentation. A Arnold, G Dupont, F Furger, C Kobus, Lancelot , F ; T B Mann, B Ryder, N Subbiah, M Kaplan, J Dhariwal, P Neelakantan, A Shyam, P Sastry, G Askell, A , arXiv:2011.13284arXiv:2005.14165Language models are few-shot learners. 11arXiv preprintArnold, A., Dupont, G., Furger, F., Kobus, C., and Lancelot, F. (2020). A question-answering system for aircraft pilots' documentation. arXiv preprint arXiv:2011.13284. 11 https://github.com/RajGite/KG- assisted-DL-based-QA-in-Aviation-Domain Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Lan- guage models are few-shot learners. arXiv preprint arXiv:2005.14165.\n\nResearch on construction method of knowledge graph in the civil aviation security field. Y Cheng, Y Jiao, W Wei, Z Wu, 2019 IEEE 1st International Conference on Civil Aviation Safety and Information Technology (ICCASIT). IEEECheng, Y., Jiao, Y., Wei, W., and Wu, Z. (2019). Re- search on construction method of knowledge graph in the civil aviation security field. In 2019 IEEE 1st International Conference on Civil Aviation Safety and Information Technology (ICCASIT), pages 556- 559. IEEE.\n\nJ Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n\nWdaqua-core0: A question answering component for the research community. D Diefenbach, K Singh, P Maret, Semantic Web Evaluation Challenge. SpringerDiefenbach, D., Singh, K., and Maret, P. (2017). Wdaqua-core0: A question answering component for the research community. In Semantic Web Eval- uation Challenge, pages 84-89. Springer.\n\nTowards a question answering system over the semantic web. D Diefenbach, A Both, K Singh, P Maret, Semantic Web. 113Diefenbach, D., Both, A., Singh, K., and Maret, P. (2020). Towards a question answering system over the semantic web. Semantic Web, 11(3):421-439.\n\nAutomatic recognition of multi-word terms:. the cvalue/nc-value method. K Frantzi, S Ananiadou, H Mima, International journal on digital libraries. 32Frantzi, K., Ananiadou, S., and Mima, H. (2000). Automatic recognition of multi-word terms:. the c- value/nc-value method. International journal on digital libraries, 3(2):115-130.\n\n. K Fundel, R K\u00fcffner, R Zimmer, Fundel, K., K\u00fcffner, R., and Zimmer, R. (2007).\n\nRelex-relation extraction using dependency parse trees. Bioinformatics. 233Relex-relation extraction using dependency parse trees. Bioinformatics, 23(3):365-371.\n\nBeyond the stars: Improving rating predictions using review text content. G Ganu, N Elhadad, Marian , A , WebDB. Ganu, G., Elhadad, N., and Marian, A. (2009). Beyond the stars: Improving rating predictions using review text content. In WebDB.\n\n. P He, Y Mao, K Chakrabarti, Chen , W , He, P., Mao, Y., Chakrabarti, K., and Chen, W. (2019).\n\narXiv:1908.08113reinforce schema representation with context. arXiv preprintX-sql: reinforce schema representation with context. arXiv preprint arXiv:1908.08113.\n\nUsing tf-idf to determine word relevance in document queries. R Juan, Proceedings of the first instructional conference on machine learning. the first instructional conference on machine learningCiteseer242Juan, R. (2003). Using tf-idf to determine word rel- evance in document queries. In Proceedings of the first instructional conference on machine learning, volume 242, pages 29-48. Citeseer.\n\nDense passage retrieval for open-domain question answering. V Karpukhin, B Oguz, S Min, P Lewis, L Wu, S Edunov, D Chen, W Yih, arXiv:2004.04906arXiv preprintKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. (2020). Dense passage retrieval for open-domain question answer- ing. arXiv preprint arXiv:2004.04906.\n\nApplying Distilled BERT for Question Answering on ASRS Reports. S Kierszbaum, L Lapasset, NTCA 2020 New Trends in Civil Aviation. Prague, Czech RepublicIEEENTCA 2020 New Trends in Civil AviationKierszbaum, S. and Lapasset, L. (2020). Applying Distilled BERT for Question Answering on ASRS Reports. In NTCA 2020 New Trends in Civil Avi- ation, NTCA 2020 New Trends in Civil Aviation, pages 33-38, Prague, Czech Republic, November. IEEE.\n\nQuerying knowledge graphs in natural language. S Liang, K Stockinger, T M De Farias, M Anisimova, Gil , M , Journal of big Data. 81Liang, S., Stockinger, K., de Farias, T. M., Anisi- mova, M., and Gil, M. (2021). Querying knowledge graphs in natural language. Journal of big Data, 8(1):1-23.\n\nA sentence-tosentence clustering procedure for pattern analysis. S.-Y Lu, K S Fu, IEEE Transactions on Systems, Man, and Cybernetics. 85Lu, S.-Y. and Fu, K. S. (1978). A sentence-to- sentence clustering procedure for pattern analysis. IEEE Transactions on Systems, Man, and Cybernet- ics, 8(5):381-389.\n\nThe prot\u00e9g\u00e9 project: a look back and a look forward. M A Musen, 1Musen, M. A. (2015). The prot\u00e9g\u00e9 project: a look back and a look forward. volume 1, pages 4-12.\n\nA survey of named entity recognition and classification. D Nadeau, S Sekine, Lingvisticae Investigationes. 30Nadeau, D. and Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3-26.\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C D Manning, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.\n\nKnow what you don't know: Unanswerable questions for squad. P Rajpurkar, R Jia, P Liang, Rajpurkar, P., Jia, R., and Liang, P. (2018). Know what you don't know: Unanswerable questions for squad.\n\nN Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprintReimers, N. and Gurevych, I. (2019a). Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\n\nSentence-bert: Sentence embeddings using siamese bert-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics11Reimers, N. and Gurevych, I. (2019b). Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Associa- tion for Computational Linguistics, 11.\n\nThe probabilistic relevance framework: BM25 and beyond. S Robertson, H Zaragoza, Now Publishers IncRobertson, S. and Zaragoza, H. (2009). The prob- abilistic relevance framework: BM25 and beyond. Now Publishers Inc.\n\nIntroduction to the conll-2003 shared task: Languageindependent named entity recognition. E F Sang, F De Meulder, cs/0306050arXiv preprintSang, E. F. and De Meulder, F. (2003). Intro- duction to the conll-2003 shared task: Language- independent named entity recognition. arXiv preprint cs/0306050.\n\nWhy reinvent the wheel: Let's build question answering systems together. K Singh, A S Radhakrishna, A Both, S Shekarpour, I Lytra, R Usbeck, A Vyas, A Khikmatullaev, D Punjani, C Lange, Proceedings of the 2018 World Wide Web Conference. the 2018 World Wide Web ConferenceSingh, K., Radhakrishna, A. S., Both, A., Shekarpour, S., Lytra, I., Usbeck, R., Vyas, A., Khikmatullaev, A., Punjani, D., Lange, C., et al. (2018). Why rein- vent the wheel: Let's build question answering sys- tems together. In Proceedings of the 2018 World Wide Web Conference, pages 1247-1256.\n\nResearch on the construction technology of knowledge graph in aviation. X Wang, X Yang, J Fu, X Qiu, IOP Conference Series: Materials Science and Engineering. IOP Publishing75112040Wang, X., Yang, X., Fu, J., and Qiu, X. (2020). Re- search on the construction technology of knowledge graph in aviation. In IOP Conference Series: Ma- terials Science and Engineering, volume 751, page 012040. IOP Publishing.\n\nTransformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnlineAssociation for Computational LinguisticsWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. (2020). Transformers: State-of-the-art natural language pro- cessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing: System Demonstrations, pages 38-45, Online, October. Association for Computational Linguistics.\n\nConstruction and application research of knowledge graph in aviation risk field. Q Zhao, Q Li, Wen , J , MATEC Web of Conferences. EDP Sciences1515003Zhao, Q., Li, Q., and Wen, J. (2018). Construction and application research of knowledge graph in aviation risk field. In MATEC Web of Conferences, volume 151, page 05003. EDP Sciences.\n\nV Zhong, C Xiong, R Socher, arXiv:1709.00103Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprintZhong, V., Xiong, C., and Socher, R. (2017). Seq2sql: Generating structured queries from natural lan- guage using reinforcement learning. arXiv preprint arXiv:1709.00103.\n", "annotations": {"author": "[{\"end\":133,\"start\":105},{\"end\":156,\"start\":134},{\"end\":184,\"start\":157},{\"end\":220,\"start\":185},{\"end\":261,\"start\":221},{\"end\":273,\"start\":262},{\"end\":341,\"start\":274},{\"end\":392,\"start\":342},{\"end\":457,\"start\":393}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":112},{\"end\":142,\"start\":138},{\"end\":170,\"start\":164},{\"end\":206,\"start\":193},{\"end\":237,\"start\":234},{\"end\":272,\"start\":267},{\"end\":288,\"start\":283},{\"end\":353,\"start\":349},{\"end\":405,\"start\":398}]", "author_first_name": "[{\"end\":111,\"start\":105},{\"end\":137,\"start\":134},{\"end\":163,\"start\":157},{\"end\":192,\"start\":185},{\"end\":233,\"start\":221},{\"end\":266,\"start\":262},{\"end\":282,\"start\":274},{\"end\":348,\"start\":342},{\"end\":397,\"start\":393}]", "author_affiliation": "[{\"end\":132,\"start\":121},{\"end\":155,\"start\":144},{\"end\":183,\"start\":172},{\"end\":219,\"start\":208},{\"end\":260,\"start\":239},{\"end\":340,\"start\":319},{\"end\":391,\"start\":380},{\"end\":456,\"start\":435}]", "title": "[{\"end\":93,\"start\":1},{\"end\":550,\"start\":458}]", "venue": "[{\"end\":635,\"start\":552}]", "abstract": "[{\"end\":2308,\"start\":889}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3712,\"start\":3691},{\"end\":3756,\"start\":3730},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6092,\"start\":6074},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6350,\"start\":6331},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6500,\"start\":6482},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6972,\"start\":6949},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7072,\"start\":7052},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7113,\"start\":7096},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7376,\"start\":7351},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7471,\"start\":7451},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7563,\"start\":7538},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7660,\"start\":7641},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8089,\"start\":8059},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8220,\"start\":8200},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10412,\"start\":10399},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13340,\"start\":13316},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13684,\"start\":13672},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13982,\"start\":13960},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14209,\"start\":14192},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15274,\"start\":15247},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15331,\"start\":15306},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15454,\"start\":15433},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20397,\"start\":20378},{\"end\":22156,\"start\":22136},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23057,\"start\":23028},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23526,\"start\":23505},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24572,\"start\":24543},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25004,\"start\":24985},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25077,\"start\":25053},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25236,\"start\":25206}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34031,\"start\":33950},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34607,\"start\":34032},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34795,\"start\":34608},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34935,\"start\":34796},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35709,\"start\":34936}]", "paragraph": "[{\"end\":3035,\"start\":2324},{\"end\":4027,\"start\":3037},{\"end\":4202,\"start\":4029},{\"end\":4356,\"start\":4204},{\"end\":4812,\"start\":4358},{\"end\":5372,\"start\":4814},{\"end\":5968,\"start\":5374},{\"end\":6796,\"start\":5985},{\"end\":7749,\"start\":6798},{\"end\":8382,\"start\":7751},{\"end\":8534,\"start\":8384},{\"end\":8668,\"start\":8556},{\"end\":8844,\"start\":8670},{\"end\":9527,\"start\":8846},{\"end\":10280,\"start\":9529},{\"end\":10901,\"start\":10309},{\"end\":11163,\"start\":10920},{\"end\":11836,\"start\":11185},{\"end\":12229,\"start\":11838},{\"end\":12435,\"start\":12231},{\"end\":12558,\"start\":12437},{\"end\":12602,\"start\":12560},{\"end\":13076,\"start\":12622},{\"end\":13281,\"start\":13078},{\"end\":13616,\"start\":13283},{\"end\":13948,\"start\":13618},{\"end\":14168,\"start\":13950},{\"end\":14289,\"start\":14170},{\"end\":15144,\"start\":14291},{\"end\":15411,\"start\":15146},{\"end\":15711,\"start\":15413},{\"end\":16202,\"start\":15744},{\"end\":17230,\"start\":16204},{\"end\":17641,\"start\":17261},{\"end\":18244,\"start\":17700},{\"end\":19478,\"start\":18278},{\"end\":19552,\"start\":19480},{\"end\":19715,\"start\":19554},{\"end\":19973,\"start\":19717},{\"end\":20215,\"start\":19975},{\"end\":20485,\"start\":20231},{\"end\":20713,\"start\":20487},{\"end\":21152,\"start\":20715},{\"end\":21510,\"start\":21154},{\"end\":21781,\"start\":21512},{\"end\":22070,\"start\":21783},{\"end\":22638,\"start\":22086},{\"end\":23235,\"start\":22640},{\"end\":23862,\"start\":23237},{\"end\":24379,\"start\":23885},{\"end\":24470,\"start\":24391},{\"end\":24635,\"start\":24472},{\"end\":25100,\"start\":24637},{\"end\":25433,\"start\":25102},{\"end\":25597,\"start\":25435},{\"end\":26140,\"start\":25599},{\"end\":27123,\"start\":26142},{\"end\":27483,\"start\":27125},{\"end\":27630,\"start\":27485},{\"end\":27974,\"start\":27632},{\"end\":28357,\"start\":27976},{\"end\":28635,\"start\":28359},{\"end\":29018,\"start\":28637},{\"end\":29715,\"start\":29043},{\"end\":30300,\"start\":29717},{\"end\":31078,\"start\":30302},{\"end\":32069,\"start\":31080},{\"end\":32623,\"start\":32071},{\"end\":32694,\"start\":32625},{\"end\":33271,\"start\":32725},{\"end\":33938,\"start\":33273}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29367,\"start\":29360}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2322,\"start\":2310},{\"attributes\":{\"n\":\"2.\"},\"end\":5983,\"start\":5971},{\"attributes\":{\"n\":\"3.\"},\"end\":8554,\"start\":8537},{\"attributes\":{\"n\":\"4.\"},\"end\":10307,\"start\":10283},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10918,\"start\":10904},{\"attributes\":{\"n\":\"4.2.\"},\"end\":11183,\"start\":11166},{\"end\":12620,\"start\":12605},{\"attributes\":{\"n\":\"4.3.\"},\"end\":15742,\"start\":15714},{\"attributes\":{\"n\":\"4.4.\"},\"end\":17259,\"start\":17233},{\"attributes\":{\"n\":\"4.5.\"},\"end\":17698,\"start\":17644},{\"attributes\":{\"n\":\"5.\"},\"end\":18276,\"start\":18247},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20229,\"start\":20218},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22084,\"start\":22073},{\"attributes\":{\"n\":\"6.\"},\"end\":23883,\"start\":23865},{\"end\":24389,\"start\":24382},{\"attributes\":{\"n\":\"7.\"},\"end\":29041,\"start\":29021},{\"attributes\":{\"n\":\"8.\"},\"end\":32723,\"start\":32697},{\"attributes\":{\"n\":\"10.\"},\"end\":33949,\"start\":33941},{\"end\":33961,\"start\":33951},{\"end\":34093,\"start\":34033},{\"end\":34616,\"start\":34609},{\"end\":34806,\"start\":34797},{\"end\":34946,\"start\":34937}]", "table": "[{\"end\":34795,\"start\":34618}]", "figure_caption": "[{\"end\":34031,\"start\":33963},{\"end\":34607,\"start\":34100},{\"end\":34935,\"start\":34808},{\"end\":35709,\"start\":34948}]", "figure_ref": "[{\"end\":9189,\"start\":9181},{\"end\":9973,\"start\":9965},{\"end\":10845,\"start\":10837},{\"end\":11379,\"start\":11371},{\"end\":12556,\"start\":12548},{\"end\":14396,\"start\":14388},{\"end\":16399,\"start\":16391},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18563,\"start\":18555},{\"end\":19263,\"start\":19255},{\"end\":20439,\"start\":20430},{\"end\":23789,\"start\":23781},{\"end\":26517,\"start\":26508}]", "bib_author_first_name": "[{\"end\":35860,\"start\":35859},{\"end\":35872,\"start\":35871},{\"end\":35886,\"start\":35885},{\"end\":36150,\"start\":36149},{\"end\":36160,\"start\":36159},{\"end\":36170,\"start\":36169},{\"end\":36180,\"start\":36179},{\"end\":36196,\"start\":36188},{\"end\":36200,\"start\":36199},{\"end\":36206,\"start\":36201},{\"end\":36214,\"start\":36213},{\"end\":36223,\"start\":36222},{\"end\":36234,\"start\":36233},{\"end\":36244,\"start\":36243},{\"end\":36256,\"start\":36255},{\"end\":36271,\"start\":36270},{\"end\":36280,\"start\":36279},{\"end\":36290,\"start\":36289},{\"end\":36300,\"start\":36299},{\"end\":36939,\"start\":36938},{\"end\":36948,\"start\":36947},{\"end\":36956,\"start\":36955},{\"end\":36963,\"start\":36962},{\"end\":37343,\"start\":37342},{\"end\":37356,\"start\":37352},{\"end\":37365,\"start\":37364},{\"end\":37372,\"start\":37371},{\"end\":37747,\"start\":37746},{\"end\":37761,\"start\":37760},{\"end\":37770,\"start\":37769},{\"end\":38067,\"start\":38066},{\"end\":38081,\"start\":38080},{\"end\":38089,\"start\":38088},{\"end\":38098,\"start\":38097},{\"end\":38344,\"start\":38343},{\"end\":38355,\"start\":38354},{\"end\":38368,\"start\":38367},{\"end\":38606,\"start\":38605},{\"end\":38616,\"start\":38615},{\"end\":38627,\"start\":38626},{\"end\":38923,\"start\":38922},{\"end\":38931,\"start\":38930},{\"end\":38947,\"start\":38941},{\"end\":38951,\"start\":38950},{\"end\":39095,\"start\":39094},{\"end\":39101,\"start\":39100},{\"end\":39108,\"start\":39107},{\"end\":39126,\"start\":39122},{\"end\":39130,\"start\":39129},{\"end\":39415,\"start\":39414},{\"end\":39810,\"start\":39809},{\"end\":39823,\"start\":39822},{\"end\":39831,\"start\":39830},{\"end\":39838,\"start\":39837},{\"end\":39847,\"start\":39846},{\"end\":39853,\"start\":39852},{\"end\":39863,\"start\":39862},{\"end\":39871,\"start\":39870},{\"end\":40166,\"start\":40165},{\"end\":40180,\"start\":40179},{\"end\":40586,\"start\":40585},{\"end\":40595,\"start\":40594},{\"end\":40609,\"start\":40608},{\"end\":40611,\"start\":40610},{\"end\":40624,\"start\":40623},{\"end\":40639,\"start\":40636},{\"end\":40643,\"start\":40642},{\"end\":40900,\"start\":40896},{\"end\":40906,\"start\":40905},{\"end\":40908,\"start\":40907},{\"end\":41189,\"start\":41188},{\"end\":41191,\"start\":41190},{\"end\":41355,\"start\":41354},{\"end\":41365,\"start\":41364},{\"end\":41588,\"start\":41587},{\"end\":41602,\"start\":41601},{\"end\":41612,\"start\":41611},{\"end\":41614,\"start\":41613},{\"end\":42079,\"start\":42078},{\"end\":42092,\"start\":42091},{\"end\":42099,\"start\":42098},{\"end\":42215,\"start\":42214},{\"end\":42226,\"start\":42225},{\"end\":42532,\"start\":42531},{\"end\":42543,\"start\":42542},{\"end\":43101,\"start\":43100},{\"end\":43114,\"start\":43113},{\"end\":43352,\"start\":43351},{\"end\":43354,\"start\":43353},{\"end\":43362,\"start\":43361},{\"end\":43634,\"start\":43633},{\"end\":43643,\"start\":43642},{\"end\":43645,\"start\":43644},{\"end\":43661,\"start\":43660},{\"end\":43669,\"start\":43668},{\"end\":43683,\"start\":43682},{\"end\":43692,\"start\":43691},{\"end\":43702,\"start\":43701},{\"end\":43710,\"start\":43709},{\"end\":43727,\"start\":43726},{\"end\":43738,\"start\":43737},{\"end\":44202,\"start\":44201},{\"end\":44210,\"start\":44209},{\"end\":44218,\"start\":44217},{\"end\":44224,\"start\":44223},{\"end\":44598,\"start\":44597},{\"end\":44606,\"start\":44605},{\"end\":44615,\"start\":44614},{\"end\":44623,\"start\":44622},{\"end\":44635,\"start\":44634},{\"end\":44647,\"start\":44646},{\"end\":44654,\"start\":44653},{\"end\":44664,\"start\":44663},{\"end\":44673,\"start\":44672},{\"end\":44681,\"start\":44680},{\"end\":44694,\"start\":44693},{\"end\":44705,\"start\":44704},{\"end\":44717,\"start\":44716},{\"end\":44731,\"start\":44730},{\"end\":44737,\"start\":44736},{\"end\":44748,\"start\":44747},{\"end\":44755,\"start\":44754},{\"end\":44761,\"start\":44760},{\"end\":44763,\"start\":44762},{\"end\":44771,\"start\":44770},{\"end\":44781,\"start\":44780},{\"end\":44790,\"start\":44789},{\"end\":44800,\"start\":44799},{\"end\":44802,\"start\":44801},{\"end\":45666,\"start\":45665},{\"end\":45674,\"start\":45673},{\"end\":45682,\"start\":45679},{\"end\":45686,\"start\":45685},{\"end\":45922,\"start\":45921},{\"end\":45931,\"start\":45930},{\"end\":45940,\"start\":45939}]", "bib_author_last_name": "[{\"end\":35869,\"start\":35861},{\"end\":35883,\"start\":35873},{\"end\":35896,\"start\":35887},{\"end\":36157,\"start\":36151},{\"end\":36167,\"start\":36161},{\"end\":36177,\"start\":36171},{\"end\":36186,\"start\":36181},{\"end\":36211,\"start\":36207},{\"end\":36220,\"start\":36215},{\"end\":36231,\"start\":36224},{\"end\":36241,\"start\":36235},{\"end\":36253,\"start\":36245},{\"end\":36268,\"start\":36257},{\"end\":36277,\"start\":36272},{\"end\":36287,\"start\":36281},{\"end\":36297,\"start\":36291},{\"end\":36945,\"start\":36940},{\"end\":36953,\"start\":36949},{\"end\":36960,\"start\":36957},{\"end\":36966,\"start\":36964},{\"end\":37350,\"start\":37344},{\"end\":37362,\"start\":37357},{\"end\":37369,\"start\":37366},{\"end\":37382,\"start\":37373},{\"end\":37758,\"start\":37748},{\"end\":37767,\"start\":37762},{\"end\":37776,\"start\":37771},{\"end\":38078,\"start\":38068},{\"end\":38086,\"start\":38082},{\"end\":38095,\"start\":38090},{\"end\":38104,\"start\":38099},{\"end\":38352,\"start\":38345},{\"end\":38365,\"start\":38356},{\"end\":38373,\"start\":38369},{\"end\":38613,\"start\":38607},{\"end\":38624,\"start\":38617},{\"end\":38634,\"start\":38628},{\"end\":38928,\"start\":38924},{\"end\":38939,\"start\":38932},{\"end\":39098,\"start\":39096},{\"end\":39105,\"start\":39102},{\"end\":39120,\"start\":39109},{\"end\":39420,\"start\":39416},{\"end\":39820,\"start\":39811},{\"end\":39828,\"start\":39824},{\"end\":39835,\"start\":39832},{\"end\":39844,\"start\":39839},{\"end\":39850,\"start\":39848},{\"end\":39860,\"start\":39854},{\"end\":39868,\"start\":39864},{\"end\":39875,\"start\":39872},{\"end\":40177,\"start\":40167},{\"end\":40189,\"start\":40181},{\"end\":40592,\"start\":40587},{\"end\":40606,\"start\":40596},{\"end\":40621,\"start\":40612},{\"end\":40634,\"start\":40625},{\"end\":40903,\"start\":40901},{\"end\":40911,\"start\":40909},{\"end\":41197,\"start\":41192},{\"end\":41362,\"start\":41356},{\"end\":41372,\"start\":41366},{\"end\":41599,\"start\":41589},{\"end\":41609,\"start\":41603},{\"end\":41622,\"start\":41615},{\"end\":42089,\"start\":42080},{\"end\":42096,\"start\":42093},{\"end\":42105,\"start\":42100},{\"end\":42223,\"start\":42216},{\"end\":42235,\"start\":42227},{\"end\":42540,\"start\":42533},{\"end\":42552,\"start\":42544},{\"end\":43111,\"start\":43102},{\"end\":43123,\"start\":43115},{\"end\":43359,\"start\":43355},{\"end\":43373,\"start\":43363},{\"end\":43640,\"start\":43635},{\"end\":43658,\"start\":43646},{\"end\":43666,\"start\":43662},{\"end\":43680,\"start\":43670},{\"end\":43689,\"start\":43684},{\"end\":43699,\"start\":43693},{\"end\":43707,\"start\":43703},{\"end\":43724,\"start\":43711},{\"end\":43735,\"start\":43728},{\"end\":43744,\"start\":43739},{\"end\":44207,\"start\":44203},{\"end\":44215,\"start\":44211},{\"end\":44221,\"start\":44219},{\"end\":44228,\"start\":44225},{\"end\":44603,\"start\":44599},{\"end\":44612,\"start\":44607},{\"end\":44620,\"start\":44616},{\"end\":44632,\"start\":44624},{\"end\":44644,\"start\":44636},{\"end\":44651,\"start\":44648},{\"end\":44661,\"start\":44655},{\"end\":44670,\"start\":44665},{\"end\":44678,\"start\":44674},{\"end\":44691,\"start\":44682},{\"end\":44702,\"start\":44695},{\"end\":44714,\"start\":44706},{\"end\":44728,\"start\":44718},{\"end\":44734,\"start\":44732},{\"end\":44745,\"start\":44738},{\"end\":44752,\"start\":44749},{\"end\":44758,\"start\":44756},{\"end\":44768,\"start\":44764},{\"end\":44778,\"start\":44772},{\"end\":44787,\"start\":44782},{\"end\":44797,\"start\":44791},{\"end\":44807,\"start\":44803},{\"end\":45671,\"start\":45667},{\"end\":45677,\"start\":45675},{\"end\":45928,\"start\":45923},{\"end\":45937,\"start\":45932},{\"end\":45947,\"start\":45941}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":195316636},\"end\":36083,\"start\":35785},{\"attributes\":{\"doi\":\"arXiv:2011.13284\",\"id\":\"b1\",\"matched_paper_id\":227209838},\"end\":36847,\"start\":36085},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":210995234},\"end\":37340,\"start\":36849},{\"attributes\":{\"id\":\"b3\"},\"end\":37671,\"start\":37342},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":33448777},\"end\":38005,\"start\":37673},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3670687},\"end\":38269,\"start\":38007},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":23069127},\"end\":38601,\"start\":38271},{\"attributes\":{\"id\":\"b7\"},\"end\":38683,\"start\":38603},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7626307},\"end\":38846,\"start\":38685},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18345070},\"end\":39090,\"start\":38848},{\"attributes\":{\"id\":\"b10\"},\"end\":39187,\"start\":39092},{\"attributes\":{\"id\":\"b11\"},\"end\":39350,\"start\":39189},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14638345},\"end\":39747,\"start\":39352},{\"attributes\":{\"id\":\"b13\"},\"end\":40099,\"start\":39749},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":229358151},\"end\":40536,\"start\":40101},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":230989083},\"end\":40829,\"start\":40538},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7676164},\"end\":41133,\"start\":40831},{\"attributes\":{\"id\":\"b17\"},\"end\":41295,\"start\":41135},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8310135},\"end\":41538,\"start\":41297},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1957433},\"end\":42016,\"start\":41540},{\"attributes\":{\"id\":\"b20\"},\"end\":42212,\"start\":42018},{\"attributes\":{\"id\":\"b21\"},\"end\":42465,\"start\":42214},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":201646309},\"end\":43042,\"start\":42467},{\"attributes\":{\"id\":\"b23\"},\"end\":43259,\"start\":43044},{\"attributes\":{\"id\":\"b24\"},\"end\":43558,\"start\":43261},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4886311},\"end\":44127,\"start\":43560},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":213392592},\"end\":44535,\"start\":44129},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":208117506},\"end\":45582,\"start\":44537},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":56407887},\"end\":45919,\"start\":45584},{\"attributes\":{\"id\":\"b29\"},\"end\":46240,\"start\":45921}]", "bib_title": "[{\"end\":35857,\"start\":35785},{\"end\":36147,\"start\":36085},{\"end\":36936,\"start\":36849},{\"end\":37744,\"start\":37673},{\"end\":38064,\"start\":38007},{\"end\":38341,\"start\":38271},{\"end\":38739,\"start\":38685},{\"end\":38920,\"start\":38848},{\"end\":39412,\"start\":39352},{\"end\":40163,\"start\":40101},{\"end\":40583,\"start\":40538},{\"end\":40894,\"start\":40831},{\"end\":41352,\"start\":41297},{\"end\":41585,\"start\":41540},{\"end\":42529,\"start\":42467},{\"end\":43631,\"start\":43560},{\"end\":44199,\"start\":44129},{\"end\":44595,\"start\":44537},{\"end\":45663,\"start\":45584}]", "bib_author": "[{\"end\":35871,\"start\":35859},{\"end\":35885,\"start\":35871},{\"end\":35898,\"start\":35885},{\"end\":36159,\"start\":36149},{\"end\":36169,\"start\":36159},{\"end\":36179,\"start\":36169},{\"end\":36188,\"start\":36179},{\"end\":36199,\"start\":36188},{\"end\":36213,\"start\":36199},{\"end\":36222,\"start\":36213},{\"end\":36233,\"start\":36222},{\"end\":36243,\"start\":36233},{\"end\":36255,\"start\":36243},{\"end\":36270,\"start\":36255},{\"end\":36279,\"start\":36270},{\"end\":36289,\"start\":36279},{\"end\":36299,\"start\":36289},{\"end\":36303,\"start\":36299},{\"end\":36947,\"start\":36938},{\"end\":36955,\"start\":36947},{\"end\":36962,\"start\":36955},{\"end\":36968,\"start\":36962},{\"end\":37352,\"start\":37342},{\"end\":37364,\"start\":37352},{\"end\":37371,\"start\":37364},{\"end\":37384,\"start\":37371},{\"end\":37760,\"start\":37746},{\"end\":37769,\"start\":37760},{\"end\":37778,\"start\":37769},{\"end\":38080,\"start\":38066},{\"end\":38088,\"start\":38080},{\"end\":38097,\"start\":38088},{\"end\":38106,\"start\":38097},{\"end\":38354,\"start\":38343},{\"end\":38367,\"start\":38354},{\"end\":38375,\"start\":38367},{\"end\":38615,\"start\":38605},{\"end\":38626,\"start\":38615},{\"end\":38636,\"start\":38626},{\"end\":38930,\"start\":38922},{\"end\":38941,\"start\":38930},{\"end\":38950,\"start\":38941},{\"end\":38954,\"start\":38950},{\"end\":39100,\"start\":39094},{\"end\":39107,\"start\":39100},{\"end\":39122,\"start\":39107},{\"end\":39129,\"start\":39122},{\"end\":39133,\"start\":39129},{\"end\":39422,\"start\":39414},{\"end\":39822,\"start\":39809},{\"end\":39830,\"start\":39822},{\"end\":39837,\"start\":39830},{\"end\":39846,\"start\":39837},{\"end\":39852,\"start\":39846},{\"end\":39862,\"start\":39852},{\"end\":39870,\"start\":39862},{\"end\":39877,\"start\":39870},{\"end\":40179,\"start\":40165},{\"end\":40191,\"start\":40179},{\"end\":40594,\"start\":40585},{\"end\":40608,\"start\":40594},{\"end\":40623,\"start\":40608},{\"end\":40636,\"start\":40623},{\"end\":40642,\"start\":40636},{\"end\":40646,\"start\":40642},{\"end\":40905,\"start\":40896},{\"end\":40913,\"start\":40905},{\"end\":41199,\"start\":41188},{\"end\":41364,\"start\":41354},{\"end\":41374,\"start\":41364},{\"end\":41601,\"start\":41587},{\"end\":41611,\"start\":41601},{\"end\":41624,\"start\":41611},{\"end\":42091,\"start\":42078},{\"end\":42098,\"start\":42091},{\"end\":42107,\"start\":42098},{\"end\":42225,\"start\":42214},{\"end\":42237,\"start\":42225},{\"end\":42542,\"start\":42531},{\"end\":42554,\"start\":42542},{\"end\":43113,\"start\":43100},{\"end\":43125,\"start\":43113},{\"end\":43361,\"start\":43351},{\"end\":43375,\"start\":43361},{\"end\":43642,\"start\":43633},{\"end\":43660,\"start\":43642},{\"end\":43668,\"start\":43660},{\"end\":43682,\"start\":43668},{\"end\":43691,\"start\":43682},{\"end\":43701,\"start\":43691},{\"end\":43709,\"start\":43701},{\"end\":43726,\"start\":43709},{\"end\":43737,\"start\":43726},{\"end\":43746,\"start\":43737},{\"end\":44209,\"start\":44201},{\"end\":44217,\"start\":44209},{\"end\":44223,\"start\":44217},{\"end\":44230,\"start\":44223},{\"end\":44605,\"start\":44597},{\"end\":44614,\"start\":44605},{\"end\":44622,\"start\":44614},{\"end\":44634,\"start\":44622},{\"end\":44646,\"start\":44634},{\"end\":44653,\"start\":44646},{\"end\":44663,\"start\":44653},{\"end\":44672,\"start\":44663},{\"end\":44680,\"start\":44672},{\"end\":44693,\"start\":44680},{\"end\":44704,\"start\":44693},{\"end\":44716,\"start\":44704},{\"end\":44730,\"start\":44716},{\"end\":44736,\"start\":44730},{\"end\":44747,\"start\":44736},{\"end\":44754,\"start\":44747},{\"end\":44760,\"start\":44754},{\"end\":44770,\"start\":44760},{\"end\":44780,\"start\":44770},{\"end\":44789,\"start\":44780},{\"end\":44799,\"start\":44789},{\"end\":44809,\"start\":44799},{\"end\":45673,\"start\":45665},{\"end\":45679,\"start\":45673},{\"end\":45685,\"start\":45679},{\"end\":45689,\"start\":45685},{\"end\":45930,\"start\":45921},{\"end\":45939,\"start\":45930},{\"end\":45949,\"start\":45939}]", "bib_venue": "[{\"end\":39547,\"start\":39493},{\"end\":40253,\"start\":40231},{\"end\":41799,\"start\":41720},{\"end\":42799,\"start\":42685},{\"end\":43831,\"start\":43797},{\"end\":45020,\"start\":44920},{\"end\":35914,\"start\":35898},{\"end\":36372,\"start\":36335},{\"end\":37068,\"start\":36968},{\"end\":37480,\"start\":37400},{\"end\":37811,\"start\":37778},{\"end\":38118,\"start\":38106},{\"end\":38417,\"start\":38375},{\"end\":38755,\"start\":38741},{\"end\":38959,\"start\":38954},{\"end\":39249,\"start\":39205},{\"end\":39491,\"start\":39422},{\"end\":39807,\"start\":39749},{\"end\":40229,\"start\":40191},{\"end\":40665,\"start\":40646},{\"end\":40963,\"start\":40913},{\"end\":41186,\"start\":41135},{\"end\":41402,\"start\":41374},{\"end\":41718,\"start\":41624},{\"end\":42076,\"start\":42018},{\"end\":42315,\"start\":42253},{\"end\":42683,\"start\":42554},{\"end\":43098,\"start\":43044},{\"end\":43349,\"start\":43261},{\"end\":43795,\"start\":43746},{\"end\":44286,\"start\":44230},{\"end\":44918,\"start\":44809},{\"end\":45713,\"start\":45689},{\"end\":46054,\"start\":45965}]"}}}, "year": 2023, "month": 12, "day": 17}