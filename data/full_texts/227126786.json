{"id": 227126786, "updated": "2023-10-06 08:39:01.044", "metadata": {"title": "HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features", "authors": "[{\"first\":\"Cheng\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Min\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Hwann-Tzong\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 11, "day": 23}, "abstract": "We present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution $512 \\times 1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.11498", "mag": "3109126057", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/0004SC21", "doi": "10.1109/cvpr46437.2021.00260"}}, "content": {"source": {"pdf_hash": "4e35f47d7658d0cf6bfd97cc66045691bd596246", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.11498v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2011.11498", "status": "GREEN"}}, "grobid": {"id": "ea13d780121b9b3816b9e59d514bbf68e98790a9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4e35f47d7658d0cf6bfd97cc66045691bd596246.txt", "contents": "\nHoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features\n\n\nCheng Sun chengsun@gapp.nthu.edu.tw \nMin Sun sunmin@ee.nthu.edu.tw \nHwann-Tzong Chen htchen@cs.nthu.edu.tw \nHoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features\n\nWe present HoHoNet, a versatile and efficient framework for holistic understanding of an indoor 360-degree panorama using a Latent Horizontal Feature (LHFeat). The compact LHFeat flattens the features along the vertical direction and has shown success in modeling per-column modality for room layout reconstruction. HoHoNet advances in two important aspects. First, the deep architecture is redesigned to run faster with improved accuracy. Second, we propose a novel horizon-to-dense module, which relaxes the per-column output shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones respectively, for modeling dense modalities from a high-resolution 512 \u00d7 1024 panorama. HoHoNet is also accurate. On the tasks of layout estimation and semantic segmentation, HoHoNet achieves results on par with current state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior arts by a large margin. Code is available at https:// github.com/ sunset1995/ HoHoNet.\n\nIntroduction\n\nPanoramic images can capture the complete 360\u00b0FOVs in one shot to provide a wide range of context that facilitates scene understanding [29]. As omnidirectional cameras become more easily accessible and several large-scale panorama datasets have been released, a growing number of techniques are developed for tasks of panoramic scene modeling such as semantic segmentation [9,16,28], depth estimation [13,24,27], layout reconstruction [21,26,33], and indoor real-time navigation [3].\n\nThis paper aims to address the problem of holistic scene modeling from a single high-resolution equirectangular projection (ERP) image that captures the 360\u00b0panorama. We present HoHoNet as an efficient, effective, and versatile framework to achieve this goal (Fig. 1). The input ERP 1 National Tsing Hua University 2 ASUS AICS Department 3 Joint Research Center for AI Technology and All Vista Healthcare 4 Aeolus Robotics Figure 1: One framework for all: HoHoNet is a novel deep learning framework for modeling layout structure, dense depth, and semantic segmentation through a Latent Horizontal Feature representation (LHFeat) whose height dimension is flattened. The proposed horizon-to-dense (h2d) module can produce dense predictions from the compact LHFeat.\n\nimage is first passed through a CNN backbone for feature pyramid extraction, and then a proposed efficient height compression module encodes the feature pyramid into a Latent Horizontal Feature representation (LHFeat) whose height dimension is flattened. Finally, from LHFeat, the HoHoNet framework can yield both per-column and per-pixel modalities with state-of-the-art quality.\n\nOur way of encoding ERP images into LHFeat is inspired by Sun et al. [21]. However, their model is only applicable to tasks of predicting per-column modalities (e.g., corners or boundaries of layout), which constrains its feasibility in other scenarios requiring per-pixel predictions. We show that LHFeat can flexibly encode latent features for recovering the target 2D per-pixel modalities, based on our observation of the strong regularity between human-made structures and gravity aligned y-axis of ERP images (Fig. 2).\n\nIn HoHoNet we introduce a new horizon-to-dense (h2d) module for recovering 2D per-pixel modalities while maintaining the efficiency of overall framework (Fig. 1). A naive method is to treat the channel dimension of horizontal prediction as height and apply a linear interpolation if required. However, this requires the shallow Conv1D layers to disentangle the row-dependent information from the row-independent LHFeat. The spatial (the row) blended essence of LHFeat motivates us to model dense information in the frequency domain, and we resort to the discrete cosine transform (DCT) for its long-standing applications in data compression. By replacing linear interpolation with IDCT, we are able to improve the dense prediction results. With our horizon-to-dense module, the efficiently encoded LHFeat can now model dense modalities.\n\nWe summarize the key merits and contributions of Ho-HoNet for holistic scene modeling from a 360\u00b0image.\n\n\u2022 Fast. HoHoNet can yield dense modalities for a highresolution 512 \u00d7 1024 panorama at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 respectively.\n\n\u2022 Versatile. Our method relaxes the final prediction space upon the compact LHFeat from O(W ) to the most common O(HW ), capable of modeling layout, dense depth, and semantic segmentation.\n\n\u2022 Accurate. The performances of HoHoNet on semantic segmentation and layout reconstruction are on par with the recent state-of-the-art. On dense depth estimation, HoHoNet outperforms prior arts by a margin.\n\n\nRelated work\n\nIndoor 360 datasets. Scene modeling on 360\u00b0images is a topic with a growing number of researches recently. Several 360 datasets are released to facilitate the learning-based methods. Stanford2D3D [1] and Matterport3D [2] datasets are currently the two largest real-world indoor 360 datasets with various modalities being provided. To model the higher level indoor structure, human-annotated layout datasets [25,26,29,32,33] are constructed with more data and topology. Structured3D [30] is a recently published photorealistic 360 dataset with abundant data and structure annotations from virtual environments. In this work, we focus on real-world datasets to model depth, semantic, and layout modalities.\n\nInput 360 format. Three standard 360 input formats are commonly used in the literature-i) equirectangular projection (ERP), ii) multiple perspective projections, and iii) icosahedron mesh. ERP preserves all captured information in one image, but it also introduces distortion that might degrade the performance of the conventional convolution layer designed for perspective imagery. A number of variants of convolution layers [6,7,19,20,22] have been proposed to address the issue of ERP distortion. Projecting the 360\u00b0s ignal to multiple planar images makes it applicable to use classical CNNs with plenty of pre-trained models available, but the FOV of each view is limited. Several padding [4,24] and view sampling [9] strategies are proposed to deliver context information between views. Recently, a few approaches  We show that the structure information of an image column can be better kept in compression when the y-axis of the image is gravity aligned. We sample 1000 depth maps from Structured3D [30] dataset for the statistic. A 512\u00d71024 depth map is compressed to 16 \u00d7 1024 via discrete cosine transform with high frequency truncated, which is applied to each column separately. We measure the absolute error between the original depth and the inverse transformed one.\n\npropose to represent the omnidirectional input via icosahedron mesh for scene modeling [16,28]. In this work, our model takes ERP as the 360\u00b0input format and apply classical convolution layers directly. Although we speculate that incorporating distortion-aware techniques into our model with extra computational overheads could potentially improve performance, for the sake of simplicity and efficiency, we do not digress to pursue in that direction as the proposed method already achieves state-of-the-art performance.\n\nDepth estimation on 360 imagery. To model depth on omnidirectional imagery, OmniDepth [31] designs encoder-decoder architectures considering the ERP distortion. PanoPopups [8] shows that learning 360 depth with plane-aware loss is beneficial in the synthetic environment. Recent works on panorama dense depth estimation propose to jointly learn from different projections [24] or different modalities [13,27]. In contrast to most recent methods [13,24,27] that employ multiple backbones with cascaded training stages, HoHoNet consists of only one backbone and is trained in only one stage. Besides, HoHoNet models dense depth through the compact LHFeat while the prior arts estimate depth from conventional dense features.\n\nSemantic segmentation on 360 imagery. Semantic segmentation is a fundamental task for scene modeling. Dist-Conv [22] proposes a distortion-aware deformable convolu- Figure 3: An overview of the HoHoNet framework for dense depth estimation. (a) A high-resolution panorama is first processed by the backbone (e.g., ResNet). (b) The feature pyramid is then squeezed and fused by the proposed Efficient Height Compression (EHC) module, with a Multi-Head Self-Attention (MHSA) for refinement (detailed in Sec. 3.2). Note that the resulting LHFeat is compact (e.g., it is R 256\u00d71024 if the input image is R 3\u00d7512\u00d71024 ), enabling the overall network to run much faster than conventional encoder-decoder networks for dense features. (c) Finally, 1D convolution layers are employed to yield the final prediction. We find that predicting in DCT frequency domain brings about superior results, so we apply IDCT to the prediction of each column (detailed in Sec. 3.4). Sec. 3 and supplementary material contain more architectural details.\n\ntion layer for dense depth and semantic prediction on ERP images. Most of the recent methods for 360 semantic segmentation design a trainable layer operating on representation related to icosahedral mesh [5,12,16,28]. However, all methods above run on a relatively low resolution for the panoramic signal. Tangent images [9] project omnidirectional signals to multiple planar images tangent to a subdivided icosahedron, which allows to process high-resolution panoramas and to deploy the pre-trained weights on perspective images. Similar to [9], HoHoNet can also operate on a high-resolution image, which is shown to be an essential factor in achieving better semantic segmentation accuracy. In contrast to the recent methods, HoHoNet runs on ERP images directly, and the highly optimized deep-learning library can easily implement all our operations.\n\nLatent horizontal features (LHFeat). HoHoNet is closely related to HorizonNet [21] on the motivation of using 1D features. However, HorizonNet only tackles a specific layout reconstruction task and can only predict horizontal modalities. We design a new architecture for encoding the LHFeat with much better speed and accuracy, and, importantly, we relax the constraint on output space via the proposed horizon-to-dense module, which enables densemodality holistic scene modeling. We show that the compact LHFeat can be effectively applied to more tasks including dense depth estimation and semantic segmentation.\n\n\nApproach\n\n\nFramework overview\n\nAn overview of the proposed framework is depicted in Fig. 3. We describe the details below.\n\nInput 360 image. We use the standard equirectangular projection (ERP) for 360\u00b0images. The resolution of input ERP images, H inp. \u00d7 W inp. , is a hyperparameter, and we set it according to the standard practice of each benchmark. We show in Fig. 2 that the structure signals of an image column are preserved better after compression if the gravity direction is aligned with the image's y-axis, which is also a desirable property for our framework to encode a column into a latent vector. In this work, the 360 data provided by the benchmarks are mostly well-aligned, so we do not apply any pre-processing. Future applications could consider using the IMU sensor or 360 VP detection algorithm [29,32] to pre-process and align the input for better robustness.\n\nBackbone. We adopt ResNet [10], and the intermediate features from the four ResNet stages form the fea-\nture pyramid-{R C \u00d7H \u00d7W } =1,2,3,4 where H = Hinp. 2 +1 , W = Winp.\n2 +1 and C is the latent dimension of ResNet. Extracting latent horizontal features (LHFeat). We propose an efficient height compression (EHC) module to extract the LHFeat R D\u00d7W1 from the backbone's feature pyramid. We detail the EHC module in Sec. 3.2.\n\nPredicting modalities. We use N in this work to denote the number of target channels for a task (e.g., N is set to 1 for depth estimation and is set to the number of classes for semantic segmentation). Given the LHFeat R D\u00d7W1 , we show how HoHoNet predicts 1D output R N \u00d7Winp. in Sec. 3.3. In Sec. 3.4, we propose the first method to yield 2D dense prediction R N \u00d7Hinp.\u00d7Winp. from the compact LHFeat, which widely extends the potential applications of the proposed efficient framework.\n\n\nEHC module for LHFeat\n\nThe proposed efficient height compression (EHC) module is illustrated in Fig. 4. We first employ EHC blocks to squeeze the height of each 2D feature from the backbone's pyramid. The resulting 1D features are then simply fused by summation. Within the EHC block, the input 2D features are first processed by a Conv2D block for channel reduction, and then the spatial width is upsampled to W 1 if needed, and finally, another Conv2D block refines the upsampled features. To efficiently reduce the feature height to 1, we design the ConvSqueezeH layer, a depthwise convolution layer with kernel size set to (h, 1) to cover full feature height without padding. Note that the parameter h of each EHC block is automatically pre-computed given H inp. . Finally, a Conv2D layer converts the number of channels to LHFeat's latent size D, and the height dimension is simply discarded as it is already reduced to 1 by the ConvSqueezeH layer.\n\nTo further refine the initial LHFeat, the similar prior work [21] adopts bidirectional LSTM [11] for horizontal prediction. We find the recurrent layer accounts for 22% of our deep net processing time, so we employ multi-head self-attention [23] (MHSA) instead. Our results show that MHSA runs faster and improves accuracy more.\n\n\nPredicting 1D per-column modalities\n\nThe target modality of some applications can be formulated into per-column prediction instead of the conventional per-pixel format. An example in this regard has been shown by Sun et al. [21] for layout estimation. To predict the 1D modalities, we first upsample the horizontal features from R D\u00d7W1 to R D\u00d7Winp. and apply three Conv1D layers of kernel size 3, 3, and 1 respectively with BN, ReLU in between. The last layer yields the final prediction in R N \u00d7Winp. .\n\n\nPredicting 2D per-pixel modalities\n\nThe strategy of shaping output space into per-column format does not apply to tasks that involve per-pixel modalities.\n\nHere we present the horizon-to-dense module of HoHoNet to derive dense prediction R N \u00d7Hinp.\u00d7Winp. from the compact LHFeat R D\u00d7W1 . This functionality opens the door to a more common scenario for various applications.\n\nThe trainable layers for 2D modality prediction are almost the same as the layers for 1D prediction introduced in Sec. 3.3 except that the number of channels in the output layer is augmented to E = N \u00b7 r where N is the number of target channels for a task and r is the number of components shared by a image column. The produced prediction is then reshaped from R E\u00d7Winp. to R N \u00d7r\u00d7Winp. . We present two different operations to recover R r back to R Hinp. for each column depending on the physical meaning we assign to the r predicted values.\n\nInterpolation. The simplest way is to view the latent dimension r as the output height and apply linear interpolation\nto resize r to H inp. if r < H inp. .\nInverse discrete cosine transform (IDCT). Inspired by the application of the DCT in image compression for its energy compaction property, we view the r predicted values as if they are in the DCT frequency domain with higher frequencies being truncated. In this case, we can apply IDCT to recover the low-pass signal back to the original signal. Let x = [x n ] r\u22121 n=0 \u2208 R r be the prediction; the final output X = [X m ] H\u22121 m=0 \u2208 R H can be recovered by\nX m = x 0 2 + r\u22121 n=1 x n cos \u03c0 H n m + 1 2 .(1)\nA unified view. We can put the two aforementioned operations into a unified view of matrix multiplication as X = M x where x \u2208 R r , X \u2208 R H , and M \u2208 R H\u00d7r consisting of r orthogonal column vectors. Depending on the choice of basis, this unified view can implement linear interpolation or IDCT, as shown in Fig. 5. We find that IDCT constantly outperforms linear interpolation. We elaborate the intuition as follows. The LHFeat blends the spatial-row information (as described in Sec. 3.2), so training the last layers to disentangle the row-dependent dense modality from the flattened row-less LHFeat would pose a challenge. Conversely, learning to predict in the frequency domain can benefit from the well defined basis functions with meaningful spatial frequencies that characterize each column's original row information as a whole, and therefore may alleviate the row-dependency problem.\n\n\nExperiments\n\nIn Sec. 4.1, we first conduct ablation studies for the proposed components in HoHoNet. We then compare the performance of HoHoNet with state-of-the-art methods on dense depth estimation (Sec. 4.2), semantic segmentation (Sec. 4.3), and layout estimation (Sec. 4.4). Note that we train HoHoNet for each task separately and focus on showcasing the effectiveness of HoHoNet in learning a modality. In Sec. 4.5, we analyze the effect of non-gravity-aligned view. More quantitative and qualitative results are included in the supplementary material. Ablation split for Matterport3D [2]. Matterport3D is a large-scale real-world dataset of indoor panoramas. We prepare the ablation split by splitting the official 61 training houses into 41 and 20 houses (containing 4,921 and 2,908 panoramas) for training and validation during ablation study. We do not use the official validation split for ablation study as it will be used for state-of-the-art comparison later. The input ERPs are resized to 512 \u00d7 1024.\n\n\nAblation study\n\nTraining and evaluation. We use Adam [14] to optimize the L1 loss for 40 epochs with batch-size of 4. The learning rate is set to 1e-4, and we apply polynomial learning rate decay with factor 0.9. Standard depth evaluation metric-MAE, RMSE, and \u03b4 1 -are used. We measure the average frame per second (FPS) for processing 50 individual 512 \u00d7 1024 panoramas on a GeForce RTX 2080 Ti.\n\nArchitecture of LHFeat extraction. Table 1a compares the proposed efficient height compression (EHC) module with the architecture used in the related work [21]. In [21], a sequence of convolution layers gradually reduces the feature heights to form the initial LHFeat, which is then followed by a bidirectional LSTM (Bi-LSTM) for feature refinement. (Detailed architectures are in the supplementary material.) Table 1a shows that employing the proposed EHC module for initial LHFeat extraction achieves better speed and accuracy under different refinement configurations. We also find that using multi-head self-attention for feature refinement provides a better speed-accuracy tradeoff. Finally, our overall architecture for extracting the LHFeat is considerably better than [21]'s-the depth MAE is improved from 0.3002 to 0.2835 with FPS from 38 to 52. All experiments in Table 1a deploy ResNet-50 as backbone and use the IDCT with r = 64 for dense prediction.\n\nHyperparameters of horizon-to-dense. We compare the two operations-linear interpolation (spatial domain) and IDCT (frequency domain)-applied to dense prediction un-  der different basis setups. As shown in Table 1b, learning to predict in frequency domain (with IDCT) is consistently better than predicting in spatial domain (with linear interpolation) for dense depth estimation upon the compact LHFeat. Interestingly, the number of components r is not monotonic to the resulting accuracy, and we find r = 64 is the best setting for our model. As the compared operations introduce negligible computational cost, the FPSs are almost identical even if we increase r. All experiments in Table 1b share the same deep net setting that consists of ResNet-50, the proposed EHC, and the MHSA.\n\nComparison of the backbones. We compare the results of different backbones in  Table 2: State-of-the-art comparison for depth estimation on real-world indoor 360 datasets-Matterport3D [2] and Stan-ford2D3D [1]. The evaluation protocol follows [24], where the depth is clipped to 10 meter without depth median alignment.\n\n\nDepth estimation 4.2.1 State-of-the-art comparison using the protocol of\n\nWang et al. [24] Datasets and evaluation protocol. We compare Ho-HoNet with state-of-the-art 360 depth estimation methods on real-world datasets following the testing protocol of [24]. Matterport3D [2] has 10,800 panoramas, and its training split contains 61 houses, and the testing results are reported on the merged official validation and test split. Stanford2D3D [1] contains 1,413 panoramas, and the fold-1 is used where the fifth area is for testing, and the other areas are for training. All the ERP images and depth maps are resized to 512\u00d71024. Standard depth estimation evaluation metrics-MRE, MAE, RMSE, RMSE (log), and \u03b4-are used. Depths are clipped to 10 meters without median alignment.\n\nImplementation details. We employ ResNet-50 as the backbone with the proposed EHC module for LHFeat extraction; the latent size D of LHFeat is set to 256; IDCT with r = 64 components is applied to the model predictions. We use Adam [14] to optimize the L1 loss for 60 epochs with a batch-size of 4. The learning rate is set to 1e-4, and we apply the polynomial learning rate decay with factor 0.9.\n\nResults. Table 2 shows the comparisons with prior arts. We demonstrate that the proposed HoHoNet outperforms the previous state-of-the-art, BiFuse [24], by a large margin. Note also that BiFuse takes both ERP and cubemap as their model inputs and thus requires two backbone networks. Ho-HoNet has only one backbone and the compact LHFeat can achieve superior results, which shows the effectiveness of the proposed framework.\n\nA qualitative comparison with BiFuse [24] is provided in Fig. 6, where we download their code 1 and the pre-trained 1 https://github.com/Yeh-yu-hsuan/BiFuse weights for the comparison. We find that HoHoNet is good at capturing the overall structure of the scene. However, some drawbacks of HoHoNet are also observable through the visualization in Fig. 6.\n\n\nState-of-the-art comparison using the protocol of\n\nJin et al. [13] Dataset and evaluation protocol. We also compare Ho-HoNet with another set of methods following the testing protocol of [13]. A subset of the real-world Stanford2D3D [ [31] 0.590 0.187 0.084 0.711 RectNet [31] 0.577 0.181 0.081 0.717 Sph. FCRN [22] 0.523 0.145 0.067 0.783 U-Net [18] 0 (a) Quantitative comparison for dense depth on Stanford2D3D [1] layout-available subset [32]. We strictly follow [13]  RGB-D Gauge Net [5] 39.4 55.9 RGB-D UGSCNN [12] 38.3 54.7 RGB-D HexRUNet [28] 43.3 58.6 RGB-D TangentImg [9] 37.  \n\n\nSemantic segmentation\n\nDataset and evaluation protocol. We evaluate Ho-HoNet's semantic segmentation performance on Stan-ford2D3D [1] dataset. As previous work, we report the averaged results from the official 3-fold cross-validation splits, using standard semantic segmentation evaluation metricsclass-wise mIoU and class-wise mAcc.\n\nImplementation detail. The architecture setting of Ho-HoNet for semantic segmentation is almost the same as for depth estimation in Sec. 4.2.1 except the last layer has E = N r = 13 \u00b7 64 = 832 channels. To compare with methods using a simple backbone under low resolution, we follow [9,12,28] to construct a shallow U-Net but purely with planar CNN. For results on high resolution, we use ResNet-101 as backbone. We use Adam [14] to optimize the cross-entropy loss for 60 epochs with a batch-size of 4. The learning rate is 1e-4 with polynomial decay of factor 0.9.\n\nResults. Table 3b shows the comparison with previous methods. On the lowest resolution, HexRUNet [28], with a specially designed kernel on icosahedron representation, achieves the best result. Ours with purely planar CNNs and compact LHFeat is still competitive with the distortion mitigated methods under the low-resolution settings. When scaling to a high resolution, we achieve similar mACC with the recent state-of-the-art [9], while our mIoU is significantly better. Note that the results of [9] are obtained from a stronger FCN-ResNet101 backbone and a higher input resolution. Limited by our device and ERP projection, we can only train on a lower 1024 \u00d7 2048 resolution but still obtain competitive performance with the current state-of-the-art on 360\u00b0semantic segmentation.\n\n\nRoom layout estimation\n\nDataset and evaluation protocol. We use MatterportLayout [33,25] dataset, which is a real-world 360 Manhattan layout dataset. The official evaluation function 2 for 2D IoU and 3D IoU is used directly, where the 2D IoU is measured by projecting floor corners to an aligned floor, while 3D IoU is for pop-up view considering both floor and ceiling corners.\n\nImplementation details. HoHoNet is compatible with the 1D layout representation proposed by HorizonNet [21]. Since our main focus is not to design a new method for layout reconstruction, we use [21]'s loss, training protocol, and post-processing algorithm directly. We find HoHoNet with ResNet-34 shows slightly better accuracy than ResNet-50 in validation, so we use the simpler ResNet-34 as backbone.\n\nResults. The comparison with previous methods on Mat-terportLayout is shown in Table 3c. The FPSs are obtained using the official codes 2345 and measured by the averaged feed-forward times of the models on a GeForce RTX 2080 Ti. The result of AtlantaNet [17] is obtained from their official new pre-trained weights 5 with aligned data split and re-evaluated by the official evaluation function 2 . Our result is on par with the state-of-the-art AtlantaNet but 22\u00d7 faster.\n\nHoHoNet also outperforms HorizonNet [21] by +0.77 3D\n\n\nInput\n\nBiFuse Ours Ground Truth Advantage Figure 6: Qualitative comparison of the estimated dense depth with the prior art-BiFuse [24]. The 'Advantage' column shows the MAE difference between ours and BiFuse's where the blue color indicates ours is better and the red color for vice versa. We find HoHoNet achieves good results in capturing the overall structure, but we also find some drawback in the visualization. First, HoHoNet's depth boundary is blurrier comparing to those of BiFuse. Second, some high-frequency signal in a column is discarded by HoHoNet. See the last row for an example. We find that i) the boundary of the chairs in the left of the image is blurrier, and ii) the lamp at the middle of the image is poorly reconstructed by HoHoNet while it seems to be easier to reconstruct from the conventional dense features. The intuitive reason for the qualitatively identified drawback is that the LHFeat focuses on learning the most prominent signals of a column, which makes it easier to optimize the training criterion.\n\nIoU and +0.61 2D IoU, and is 3.5\u00d7 faster, which shows the effectiveness of the designed architecture.\n\n\nResults on non-gravity-aligned views\n\nIn Fig. 2, we show that the structure signals of an image column suffer more losses in compression if the image's y-axis is not aligned with the gravity. Though the 360 data in all benchmarks we use are mostly well-aligned with gravity, the captured 360\u00b0views could be non-gravity-aligned in practice. In Table 4, we show the vulnerability of our model to heavy pitch or roll rotation (see Fig. 2c and Fig. 2b for visualization). The pre-trained model in our ablation study takes the rotated images directly as input, and the output depth maps are rotated back to the original view for a fair comparison. As expected, the pre-trained model performs poorly when input 360\u00b0views are not gravity-aligned. Introducing 10\u00b0of pitch or roll rotation increases MAE from 28.45cm to more than 44cm. A simple solution is to use the IMU sensor or 360 vanishing point detection algorithm [29,32] to ensure gravity alignment (the VP alignment is also a standard step in 360 layout benchmark [25,32,33]).\n\nWe also show the results by training with U(\u221230\u00b0, 30\u00b0) pitch/roll rotation as data augmentation, which makes the model much more robust against the non-canonical view but sacrifices the test-time performance when input 360\u00b0view \n\n\nConclusion\n\nThis work presents a novel framework, HoHoNet, which is the first step to learning compact latent horizontal features for dense modalities modeling of omnidirectional images. HoHoNet is fast, versatile, and accurate for solving layout reconstruction, depth estimation, and semantic segmentation with accuracy on par with or better than the state-of-the-art.\n\n\nSupplementary material A. Network architecture diagram\n\nWe show the detailed architecture diagram in Fig. 7. The shape of each feature tensor is denoted as \"# of channels, height, width\" within the box. The height and width of the input panorama are assumed to be 512 and 1024 respectively. D and E are hyperparameters. The ConvSqueezeH layer is a depthwise convolution layer with kernel size set to the prior known input feature height without padding, which produces output feature height 1. \n\n\nB. Comparing EHC block and HC block [21]\n\nThe height compression block aims to squeeze a 2D feature from the backbone to produce a 1D horizontal feature. Fig. 8 shows the architecture of our Efficient Height Compression block (EHC block) and the one of HC block [21] for comparison. The HC block [21] employs a sequence of convolution layers to gradually reduce the number of channels and heights, while we first use a convolution layer for channel reduction and then use bilinear upsampling and ConvSqueezeH layer to produce the features in horizontal shape. We show in our ablation experiments that replacing the HC block [21] with the proposed ECH block leads to better speed and accuracy.\n\n(a) The HC block in [21].\n\n(b) The proposed EHC block. Figure 8: Comparison of the proposed EHC block and the HC block in [21].\n\n\nC. Detailed semantic segmentation results\n\nWe show detailed per-class IoU and per-class Acc for semantic segmentation in Table 5. We achieve the best IoU on 10 out of 13 classes and superior overall mIoU; we achieve best Acc on 7 out of 13 classes and comparable overall mAcc.  \n\n\nD. Detailed layout estimation results\n\nWe show detailed quantitative results for room layout under different numbers of ground truth 2D corners in Table 6. Our training protocol and layout formalization are identical to HorizonNet [21], while we observe improvements (except rooms with six corners) by using our network architecture. In comparison with the most recent state-of-the-art-AtlantaNet [17], we show better results on scenes with fewer corners and similar accuracy on overall scenes; meanwhile, our model is 22\u00d7 faster than AtlantaNet [17].  Table 6: Detailed quantitative comparison for room layout estimation on MatterportLayout [33] under different numbers of ground-truth corners.\n\n\nMethod\n\n\nE. More qualitative comparisons for depth estimation\n\nWe show more qualitative comparisons with the prior art-BiFuse [24]-in Fig. 9. BiFuse's results are obtained from their official released model trained on the real-world Matterport3D [2] dataset.\n\n\nInput\n\nBiFuse Ours Ground Truth Advantage Figure 9: More qualitative comparisons of the estimated dense depth with the prior art-BiFuse [24]. The 'Advantage' column shows the MAE difference between ours and BiFuse's where the blue color indicates ours is better and the red color for vice versa.\n\n\nF. Qualitative results for semantic segmentation\n\nQualitative results for semantic segmentation on Stanford2D3D [1] dataset are shown in Fig. 10. We fail to build the prior art [9] from their public release for semantic segmentation on high-resolution panorama, so we only show our results. \n\n\nG. Qualitative comparisons for layout estimation\n\nWe show qualitative comparisons for room layout estimation with the prior art-AtlantaNet [17]-in Fig. 11. The results of AtlantaNet are obtained from their official code and pre-trained weights. We use [21] post-processing algorithm to produce Manhattan layouts; AtlantaNet [17]'s algorithm generates less restrictive Atlanta layouts. Our model achieves promising results comparable to the most recent AtlantaNet [17], while our model runs 22\u00d7 faster. Figure 11: Qualitative comparisons for room layout estimated with the competitive AtlantaNet [17]. The green, magenta, and blue are the ground truth layout, AtlantaNet's results, and our results respectively.\n\n\nGravity-aligned 360 image columns are easier to compress.\n\nFigure 2 :\n2Figure 2: We show that the structure information of an image column can be better kept in compression when the y-axis of the image is gravity aligned. We sample 1000 depth maps from Structured3D [30] dataset for the statistic. A 512\u00d71024 depth map is compressed to 16 \u00d7 1024 via discrete cosine transform with high frequency truncated, which is applied to each column separately. We measure the absolute error between the original depth and the inverse transformed one.\n\nFigure 4 :\n4The proposed efficient height compression (EHC) module. The sizes of 2D and 1D features are denoted as [C, H, W ] and [C, W ] respectively. The ConvSqueezeH layer is a depthwise convolution layer with kernel size set to the prior known input feature height without padding, which produces output feature height 1. See Sec. 3.2 for details.\n\nFigure 5 :\n5The predictions at each column act as the weights for the linear combination of components in basis M . Ho-HoNet learns to predict in the spatial domain if M implements linear interpolation, and learns in the frequency domain if M implements IDCT. See Sec. 3.4 for details.\n\nFigure 7 :\n7The detailed network architecture with ResNet50[10] backbone.\n\nFigure 10 :\n10Qualitative results for semantic segmentation on Stanford2D3D[1] dataset.\n\nTable 1\n1summarizes the results of ablation experiments, where we compare different settings of HoHoNet for dense depth estimation. Detailed descriptions are as follows.\n\n\nComparison on the different settings of the proposed horizon-todense module. The parameter r denotes the number of components in a basis. We compare the two bases that implement the linear interpolation (Interp.) and the inverse discrete cosine transform (IDCT).HC Refine \nMAE\u2193 RMSE\u2193 \n\u03b4 1 \u2191 \nFPS\u2191 \n[21] \n-\n0.3090 0.5238 0.8158 \n49 \nEHC \n0.3022 0.5102 0.8204 \n54 \n[21] Bi-LSTM \n0.3002 0.5147 0.8254 \n38 \nEHC \n0.2928 0.5036 0.8294 \n41 \n[21] MHSA \n0.2915 0.5035 0.8331 \n47 \nEHC \n0.2835 0.4916 0.8389 \n52 \n\n(a) Comparison of the components for LHFeat extraction. The 'HC' \ncolumn indicates the height compression block, which produces the \ninitial LHFeat from the backbone features. We compare the results \nof 'no feature refinement', 'refined by bidirectional LSTM' [11] (Bi-\nLSTM), and 'refined by multi-head self-attention' [23] (MHSA). \nRefinement with MHSA achieves the most favorable results. \n\nr \nBasis \nMAE\u2193 RMSE\u2193 \n\u03b4 1 \u2191 \nFPS\u2191 \n\n32 \nInterp. 0.2886 0.5013 0.8356 \n52 \nIDCT \n0.2847 0.4935 0.8369 \n52 \n\n64 \nInterp. 0.2880 0.4996 0.8351 \n52 \nIDCT \n0.2835 0.4916 0.8389 \n52 \n\n128 \nInterp. 0.2926 0.5043 0.8308 \n52 \nIDCT \n0.2850 0.4955 0.8405 \n52 \n\n256 \nInterp. 0.2937 0.5059 0.8260 \n52 \nIDCT \n0.2903 0.5028 0.8334 \n52 \n\n512 \nInterp. 0.3045 0.5189 0.8227 \n52 \nIDCT \n0.2913 0.5040 0.8341 \n52 \n\n(b) Backbone MAE\u2193 RMSE\u2193 \n\u03b4 1 \u2191 \nFPS\u2191 \nResNet34 \n0.2854 0.4976 0.8397 \n110 \nResNet50 \n0.2835 0.4916 0.8389 \n52 \n\n(c) Comparison of the results with different backbones. \n\n\n\nTable 1 :\n1Ablation study on depth modality using the ablation split of Matterport3D[2]. More details are in Sec. 4.1.\n\nTable 1c ,\n1cwhere we find that em-\nploying ResNet-34 can almost double the FPS with only a \nlittle drop in accuracy comparing to ResNet-50. \nDataset \n\nMethod \nMRE \nMAE RMSE RMSE (log) \n\u03b4 1 \n\u03b4 2 \n\u03b4 3 \n\nMatterport3D \n\nFCRN [15] \n0.2409 0.4008 0.6704 \n0.1244 \n0.7703 0.9174 0.9617 \nOmniDepth (bn) [31] 0.2901 0.4838 0.7643 \n0.1450 \n0.6830 0.8794 0.9429 \nEqui [24] \n0.2074 0.3701 0.6536 \n0.1176 \n0.8302 0.9245 0.9577 \nCube [24] \n0.2505 0.3929 0.6628 \n0.1281 \n0.7556 0.9135 0.9612 \nBiFuse [24] \n0.2048 0.3470 0.6259 \n0.1134 \n0.8452 0.9319 0.9632 \nOurs \n0.1488 0.2862 0.5138 \n0.0871 \n0.8786 0.9519 0.9771 \n\nStanford2D3D \n\nFCRN [15] \n0.1837 0.3428 0.5774 \n0.1100 \n0.7230 0.9207 0.9731 \nOmniDepth (bn) [31] 0.1996 0.3743 0.6152 \n0.1212 \n0.6877 0.8891 0.9578 \nEqui [24] \n0.1428 0.2711 0.4637 \n0.0911 \n0.8261 0.9458 0.9800 \nCube [24] \n0.1332 0.2588 0.4407 \n0.0844 \n0.8347 0.9523 0.9838 \nBiFuse [24] \n0.1209 0.2343 0.4142 \n0.0787 \n0.8660 0.9580 0.9860 \nOurs \n0.1014 0.2027 0.3834 \n0.0668 \n0.9054 0.9693 0.9886 \n\n\nTable 3 :\n3State-of-the-art comparison on various datasets and \ndifferent modalities. \n\n\n\n\nTraining Rot. Aug.Table 4: Vulnerability to non-gravity-aligned views.are gravity-aligned (MAE\u2191 from 28.35cm to 30.92cm).Testing \nCam. Rot. \n\nMAE (cm) \n0\u00b010\u00b020\u00b030\u00b0P \n\nitch \n28.35 44.88 62.77 75.79 \n30.92 31.30 31.80 32.97 \n\nRoll \n28.35 44.32 61.90 75.11 \n30.92 31.32 31.80 32.90 \n\n\n\nMethod overall beam board bookcase ceiling chair clutter column door floor sofa table wall window 64.5 96.8 10.6 69.1 79.3 28.4 High-resolution RGB-D TangentImg [9] 69.1 22.6 62.0 (b) Per-class Acc (%).Low-resolution RGB-D \nUGSCNN [12] 38.3 \n8.7 32.7 \n33.4 \n82.2 42.0 25.6 \n10.1 41.6 87.0 7.6 41.7 61.7 23.5 \nHexRUNet [28] 43.3 10.9 39.7 \n37.2 \n84.8 50.5 29.2 \n11.5 45.3 92.9 19.1 49.1 63.8 29.4 \nTangentImg [9] 37.5 10.9 26.6 \n31.9 \n82.0 38.5 29.3 \n5.9 \n36.2 89.4 12.6 40.4 56.5 26.7 \nOurs \n40.8 \n3.6 43.5 \n40.6 \n81.8 41.3 27.7 \n9.2 \n52.0 92.2 9.4 44.6 61.6 23.4 \nHigh-resolution RGB-D \nTangentImg [9] 51.9 \n4.5 49.9 \n50.3 \n85.5 71.5 42.4 \n11.7 50.0 94.3 32.1 61.4 70.5 50.0 \nOurs \n56.3 \n7.4 62.3 \n55.5 \n87.0 66.4 44.3 \n19.2 66.5 96.1 43.3 60.1 72.9 51.4 \n\n(a) Per-class IoU (%). \n\nMethod \noverall beam board bookcase ceiling chair clutter column door floor sofa table wall window \nLow-resolution RGB-D \nUGSCNN [12] 54.7 19.6 48.6 \n49.6 \n93.6 63.8 43.1 \n28.0 63.2 96.4 21.0 70.0 74.6 39.0 \nHexRUNet [28] 58.6 23.2 56.5 \n62.1 \n94.6 66.7 41.5 \n18.3 64.5 96.2 41.1 79.7 77.2 41.1 \nTangentImg [9] 50.2 25.6 33.6 \n44.3 \n87.6 51.5 44.6 \n12.1 64.6 93.6 26.2 47.2 78.7 42.7 \nOurs \n52.1 \n9.5 56.5 \n56.6 \n95.1 57.9 40.7 \n12.5 70.0 \n90.3 84.7 55.5 \n41.4 76.7 96.9 70.3 73.9 80.1 74.3 \nOurs \n68.9 16.7 79.0 \n71.8 \n96.4 79.2 59.7 \n26.9 77.7 98.2 58.0 79.6 85.9 66.3 \n\n\n\nTable 5 :\n5Detailed quantitative per-class results on Stanford2D3D[1] with RGB-D as input.\nhttps://github.com/zouchuhang/LayoutNetv2 3 https://github.com/SunDaDenny/DuLa-Net 4 https://github.com/sunset1995/HorizonNet 5 https://github.com/crs4/AtlantaNet\nAcknowledgements: This work was supported in part by the MOST, Taiwan under Grants 110-2634-F-001-009 and 110-2634-F-007-016, MOST Joint Research Center for AI Technology and All Vista Healthcare. We thank National Center for High-performance Computing (NCHC) for providing computational and storage resources.\nJoint 2d-3d-semantic data for indoor scene understanding. Iro Armeni, Sasha Sax, Silvio Amir Roshan Zamir, Savarese, abs/1702.01105CoRR1012Iro Armeni, Sasha Sax, Amir Roshan Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene un- derstanding. CoRR, abs/1702.01105, 2017. 2, 6, 7, 10, 12\n\nMatterport3d: Learning from RGB-D data in indoor environments. Angel X Chang, Angela Dai, Thomas A Funkhouser, Maciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, 2017 International Conference on 3D Vision. Qingdao, ChinaIEEE Computer Society611Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Ma- ciej Halber, Matthias Nie\u00dfner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGB-D data in indoor environments. In 2017 International Conference on 3D Vision, 3DV 2017, Qingdao, China, Oc- tober 10-12, 2017, pages 667-676. IEEE Computer Society, 2017. 2, 5, 6, 11\n\nNeural topological SLAM for visual navigation. Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh Gupta, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE2020Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta. Neural topological SLAM for visual navigation. In 2020 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 12872-12881. IEEE, 2020. 1\n\nCube padding for weakly-supervised saliency prediction in 360\u00b0videos. Hsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao-Kai Wen, Tyng-Luh Liu, Min Sun, 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018. Salt Lake City, UT, USAHsien-Tzu Cheng, Chun-Hung Chao, Jin-Dong Dong, Hao- Kai Wen, Tyng-Luh Liu, and Min Sun. Cube padding for weakly-supervised saliency prediction in 360\u00b0videos. In 2018 IEEE Conference on Computer Vision and Pattern Recogni- tion, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1420-1429. IEEE Computer Society, 2018. 2\n\nGauge equivariant convolutional networks and the icosahedral CNN. Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. Kamalika Chaudhuri and Ruslan Salakhutdinovthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA977Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral CNN. In Kamalika Chaudhuri and Rus- lan Salakhutdinov, editors, Proceedings of the 36th Inter- national Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Pro- ceedings of Machine Learning Research, pages 1321-1330. PMLR, 2019. 3, 7\n\nSpherical cnns. S Taco, Mario Cohen, Jonas Geiger, Max K\u00f6hler, Welling, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netTaco S. Cohen, Mario Geiger, Jonas K\u00f6hler, and Max Welling. Spherical cnns. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenRe- view.net, 2018. 2\n\nSpherenet: Learning spherical representations for detection and classification in omnidirectional images. Benjamin Coors, Alexandru Paul Condurache, Andreas Geiger, Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair WeissBenjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. Spherenet: Learning spherical representations for detection and classification in omnidirectional images. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision -ECCV 2018 -15th\n\nProceedings, Part IX. Part IXMunich, GermanySpringer11213European ConferenceEuropean Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part IX, volume 11213 of Lecture Notes in Computer Science, pages 525-541. Springer, 2018. 2\n\nPano popups: Indoor 3d reconstruction with a plane-aware network. Marc Eder, Pierre Moulon, Li Guan, 2019 International Conference on 3D Vision, 3DV 2019. Qu\u00e9bec City, QC, CanadaMarc Eder, Pierre Moulon, and Li Guan. Pano popups: Indoor 3d reconstruction with a plane-aware network. In 2019 Inter- national Conference on 3D Vision, 3DV 2019, Qu\u00e9bec City, QC, Canada, September 16-19, 2019, pages 76-84. IEEE, 2019. 2\n\nTangent images for mitigating spherical distortion. Marc Eder, Mykhailo Shvets, John Lim, Jan-Michael Frahm, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. Seattle, WA, USAIEEE1012Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael Frahm. Tangent images for mitigating spherical distortion. In 2020 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 12423-12431. IEEE, 2020. 1, 2, 3, 7, 10, 12\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer Society39Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770-778. IEEE Computer Society, 2016. 3, 9\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural Comput. 985Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-1780, 1997. 4, 5\n\nSpherical cnns on unstructured grids. Max Chiyu, Jingwei Jiang, Karthik Huang, Kashinath, Philip Prabhat, Matthias Marcus, Nie\u00dfner, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USA710Chiyu Max Jiang, Jingwei Huang, Karthik Kashinath, Prab- hat, Philip Marcus, and Matthias Nie\u00dfner. Spherical cnns on unstructured grids. In 7th International Conference on Learn- ing Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. 3, 7, 10\n\nGeometric structure based and regularized depth estimation from 360 indoor imagery. Lei Jin, Yanyu Xu, Jia Zheng, Junfei Zhang, Rui Tang, Shugong Xu, Jingyi Yu, Shenghua Gao, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USA20207IEEE, 2020. 1, 2, 6Lei Jin, Yanyu Xu, Jia Zheng, Junfei Zhang, Rui Tang, Shugong Xu, Jingyi Yu, and Shenghua Gao. Geometric struc- ture based and regularized depth estimation from 360 indoor imagery. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 886-895. IEEE, 2020. 1, 2, 6, 7\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USA67Conference Track ProceedingsDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Represen- tations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 5, 6, 7\n\nDeeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fourth International Conference on 3D Vision, 3DV 2016. Stanford, CA, USAIEEE Computer Society67Federico Tombari, and Nassir NavabIro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In Fourth Inter- national Conference on 3D Vision, 3DV 2016, Stanford, CA, USA, October 25-28, 2016, pages 239-248. IEEE Computer Society, 2016. 6, 7\n\nSpherephd: Applying cnns on a spherical polyhedron representation of 360deg images. Yeon Kun Lee, Jaeseok Jeong, Jong Seob Yun, Wonjune Cho, Kuk-Jin Yoon, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA23Computer Vision Foundation / IEEEYeon Kun Lee, Jaeseok Jeong, Jong Seob Yun, Wonjune Cho, and Kuk-Jin Yoon. Spherephd: Applying cnns on a spherical polyhedron representation of 360deg images. In IEEE Conference on Computer Vision and Pattern Recogni- tion, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 9181-9189. Computer Vision Foundation / IEEE, 2019. 1, 2, 3\n\nAtlantanet: Inferring the 3D indoor layout from a single 360 image beyond the manhattan world assumption. Giovanni Pintore, Marco Agus, Enrico Gobbetti, Proceedings of The European Conference on Computer Vision (ECCV). The European Conference on Computer Vision (ECCV)1013Giovanni Pintore, Marco Agus, and Enrico Gobbetti. At- lantanet: Inferring the 3D indoor layout from a single 360 im- age beyond the manhattan world assumption. In Proceedings of The European Conference on Computer Vision (ECCV), 2020. 7, 10, 13\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, ; William, M Wells, Iii , Alejandro F Frangi, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference. Munich, GermanySpringer9351Proceedings, Part IIIOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi, editors, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference Munich, Germany, October 5 -9, 2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, pages 234-241. Springer, 2015. 7\n\nLearning spherical convolution for fast features from 360\u00b0imagery. Yu-Chuan Su, Kristen Grauman, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USAYu-Chuan Su and Kristen Grauman. Learning spherical con- volution for fast features from 360\u00b0imagery. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wal- lach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 529-539, 2017. 2\n\nKernel transformer networks for compact spherical convolution. Yu-Chuan Su, Kristen Grauman, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAComputer Vision Foundation / IEEEYu-Chuan Su and Kristen Grauman. Kernel transformer net- works for compact spherical convolution. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 9442-9451. Computer Vision Foundation / IEEE, 2019. 2\n\nHorizonNet: learning room layout with 1d representation and pano stretch data augmentation. Cheng Sun, Chi-Wei Hsiao, Min Sun, Hwann-Tzong Chen, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA1013Cheng Sun, Chi-Wei Hsiao, Min Sun, and Hwann-Tzong Chen. HorizonNet: learning room layout with 1d representa- tion and pano stretch data augmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 1047-1056, 2019. 1, 3, 4, 5, 7, 9, 10, 13\n\nDistortion-aware convolutional filters for dense prediction in panoramic images. Keisuke Tateno, Nassir Navab, Federico Tombari, Computer Vision -ECCV 2018 -15th European Conference. Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair WeissMunich, GermanySpringer27Proceedings, Part XVIKeisuke Tateno, Nassir Navab, and Federico Tombari. Distortion-aware convolutional filters for dense prediction in panoramic images. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision -ECCV 2018 -15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI, vol- ume 11220 of Lecture Notes in Computer Science, pages 732-750. Springer, 2018. 2, 7\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman GarnettLong Beach, CA, USA45Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998-6008, 2017. 4, 5\n\nBifuse: Monocular 360 depth estimation via bi-projection fusion. Fu-En, Yu-Hsuan Wang, Min Yeh, Wei-Chen Sun, Yi-Hsuan Chiu, Tsai, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USAIEEE202011Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via bi-projection fusion. In 2020 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 459-468. IEEE, 2020. 1, 2, 6, 8, 11\n\nLayoutmp3d: Layout annotation of matter-port3d. CoRR, abs. Fu-En, Yu-Hsuan Wang, Min Yeh, Wei-Chen Sun, Yi-Hsuan Chiu, Tsai, Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai. Layoutmp3d: Layout annotation of matter- port3d. CoRR, abs/2003.13516, 2020. 2, 7, 8\n\nDula-net: A dual-projection network for estimating room layouts from a single RGB panorama. Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka, Min Sun, Hung-Kuo Chu, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USA10Computer Vision Foundation / IEEEShang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka, Min Sun, and Hung-Kuo Chu. Dula-net: A dual-projection network for estimating room layouts from a single RGB panorama. In IEEE Conference on Computer Vision and Pat- tern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3363-3372. Computer Vision Foundation / IEEE, 2019. 1, 2, 7, 10\n\nJoint 3d layout and depth prediction from a single indoor panorama image. Wei Zeng, Sezer Karaoglu, Theo Gevers, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmWei Zeng, Sezer Karaoglu, and Theo Gevers. Joint 3d layout and depth prediction from a single indoor panorama image. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan- Michael Frahm, editors, Computer Vision -ECCV 2020 -16th\n\nProceedings, Part XVI. Part XVIGlasgow, UKSpringer12361European ConferenceEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI, volume 12361 of Lecture Notes in Computer Science, pages 666-682. Springer, 2020. 1, 2\n\nOrientation-aware semantic segmentation on icosahedron spheres. Chao Zhang, Stephan Liwicki, William Smith, Roberto Cipolla, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019. Seoul, KoreaIEEE710October 27Chao Zhang, Stephan Liwicki, William Smith, and Roberto Cipolla. Orientation-aware semantic segmentation on icosa- hedron spheres. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), Octo- ber 27 -November 2, 2019, pages 3532-3540. IEEE, 2019. 1, 2, 3, 7, 10\n\nPanocontext: A whole-room 3d context model for panoramic scene understanding. Yinda Zhang, Shuran Song, Ping Tan, Jianxiong Xiao, Computer Vision -ECCV 2014 -13th European Conference. David J. Fleet, Tom\u00e1s Pajdla, Bernt Schiele, and Tinne TuytelaarsZurich, SwitzerlandSpringer8694Proceedings, Part VIYinda Zhang, Shuran Song, Ping Tan, and Jianxiong Xiao. Panocontext: A whole-room 3d context model for panoramic scene understanding. In David J. Fleet, Tom\u00e1s Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision - ECCV 2014 -13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI, volume 8694 of Lecture Notes in Computer Science, pages 668-686. Springer, 2014. 1, 2, 3, 8\n\nStructured3d: A large photo-realistic dataset for structured 3d modeling. Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, Zihan Zhou, Proceedings of The European Conference on Computer Vision (ECCV). The European Conference on Computer Vision (ECCV)2020Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In Proceedings of The European Conference on Computer Vision (ECCV), 2020. 2\n\nOmnidepth: Dense depth estimation for indoors spherical panoramas. Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, Petros Daras, Computer Vision -ECCV 2018 -15th European Conference. Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair WeissMunich, GermanySpringer112107Proceedings, Part VINikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras. Omnidepth: Dense depth estimation for indoors spherical panoramas. In Vittorio Ferrari, Martial Hebert, Cris- tian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 -15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VI, volume 11210 of Lecture Notes in Computer Science, pages 453-471. Springer, 2018. 2, 6, 7\n\nLayoutnet: Reconstructing the 3d room layout from a single RGB image. Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USA7Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem. Layoutnet: Reconstructing the 3d room layout from a single RGB image. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 2051-2059. IEEE Computer Society, 2018. 2, 3, 7, 8\n\n. Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu, Derek Hoiem, 103d manhattan room layout reconstruction from a single 360 image. CoRR, abs/1910.04099, 2019. 1, 2, 7, 8Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu, and Derek Hoiem. 3d manhattan room layout reconstruction from a single 360 image. CoRR, abs/1910.04099, 2019. 1, 2, 7, 8, 10\n", "annotations": {"author": "[{\"end\":114,\"start\":78},{\"end\":145,\"start\":115},{\"end\":185,\"start\":146}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":122,\"start\":119},{\"end\":162,\"start\":158}]", "author_first_name": "[{\"end\":83,\"start\":78},{\"end\":118,\"start\":115},{\"end\":157,\"start\":146}]", "author_affiliation": null, "title": "[{\"end\":75,\"start\":1},{\"end\":260,\"start\":186}]", "venue": null, "abstract": "[{\"end\":1328,\"start\":262}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1483,\"start\":1479},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1720,\"start\":1717},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1723,\"start\":1720},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1726,\"start\":1723},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1749,\"start\":1745},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1752,\"start\":1749},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1755,\"start\":1752},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1783,\"start\":1779},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1786,\"start\":1783},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1789,\"start\":1786},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1826,\"start\":1823},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2113,\"start\":2112},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3049,\"start\":3045},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5206,\"start\":5203},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5227,\"start\":5224},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5418,\"start\":5414},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5421,\"start\":5418},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5424,\"start\":5421},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5427,\"start\":5424},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5430,\"start\":5427},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5493,\"start\":5489},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6142,\"start\":6139},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6144,\"start\":6142},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6147,\"start\":6144},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6150,\"start\":6147},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6153,\"start\":6150},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6409,\"start\":6406},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6412,\"start\":6409},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6434,\"start\":6431},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6722,\"start\":6718},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7085,\"start\":7081},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7088,\"start\":7085},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7605,\"start\":7601},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7690,\"start\":7687},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7891,\"start\":7887},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7920,\"start\":7916},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7923,\"start\":7920},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7964,\"start\":7960},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7967,\"start\":7964},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7970,\"start\":7967},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8355,\"start\":8351},{\"end\":8748,\"start\":8744},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9475,\"start\":9472},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9478,\"start\":9475},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9481,\"start\":9478},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9484,\"start\":9481},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9592,\"start\":9589},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9813,\"start\":9810},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10204,\"start\":10200},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11557,\"start\":11553},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11560,\"start\":11557},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11650,\"start\":11646},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13557,\"start\":13553},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13588,\"start\":13584},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13737,\"start\":13733},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14051,\"start\":14047},{\"end\":17082,\"start\":17078},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17398,\"start\":17395},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17879,\"start\":17875},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18380,\"start\":18376},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18389,\"start\":18385},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19001,\"start\":18997},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20159,\"start\":20156},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20181,\"start\":20178},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20219,\"start\":20215},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20384,\"start\":20380},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20551,\"start\":20547},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20569,\"start\":20566},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20738,\"start\":20735},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21306,\"start\":21302},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21620,\"start\":21616},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21936,\"start\":21932},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22318,\"start\":22314},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22443,\"start\":22439},{\"end\":22486,\"start\":22485},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22491,\"start\":22487},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22528,\"start\":22524},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22567,\"start\":22563},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22602,\"start\":22598},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22668,\"start\":22665},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22697,\"start\":22693},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22722,\"start\":22718},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22743,\"start\":22740},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22771,\"start\":22767},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22801,\"start\":22797},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22832,\"start\":22829},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22974,\"start\":22971},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23462,\"start\":23459},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23465,\"start\":23462},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23468,\"start\":23465},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23605,\"start\":23601},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23844,\"start\":23840},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24173,\"start\":24170},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24243,\"start\":24240},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24613,\"start\":24609},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24616,\"start\":24613},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25015,\"start\":25011},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25106,\"start\":25102},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25570,\"start\":25566},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25825,\"start\":25821},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25974,\"start\":25970},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27899,\"start\":27895},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27902,\"start\":27899},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28001,\"start\":27997},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28004,\"start\":28001},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28007,\"start\":28004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29377,\"start\":29373},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29411,\"start\":29407},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29739,\"start\":29735},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29829,\"start\":29825},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29931,\"start\":29927},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30451,\"start\":30447},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30617,\"start\":30613},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30766,\"start\":30762},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30862,\"start\":30858},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31044,\"start\":31040},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31163,\"start\":31160},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31315,\"start\":31311},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31588,\"start\":31585},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31653,\"start\":31650},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32023,\"start\":32019},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32095,\"start\":32091},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32234,\"start\":32230},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32366,\"start\":32362},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33725,\"start\":33721},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33815,\"start\":33812},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35548,\"start\":35545},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38385,\"start\":38382}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32537,\"start\":32478},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33020,\"start\":32538},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33373,\"start\":33021},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33660,\"start\":33374},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33735,\"start\":33661},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33824,\"start\":33736},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33995,\"start\":33825},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35459,\"start\":33996},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35579,\"start\":35460},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36582,\"start\":35580},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":36672,\"start\":36583},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36955,\"start\":36673},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":38314,\"start\":36956},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":38406,\"start\":38315}]", "paragraph": "[{\"end\":1827,\"start\":1344},{\"end\":2592,\"start\":1829},{\"end\":2974,\"start\":2594},{\"end\":3499,\"start\":2976},{\"end\":4337,\"start\":3501},{\"end\":4442,\"start\":4339},{\"end\":4592,\"start\":4444},{\"end\":4782,\"start\":4594},{\"end\":4990,\"start\":4784},{\"end\":5711,\"start\":5007},{\"end\":6992,\"start\":5713},{\"end\":7513,\"start\":6994},{\"end\":8237,\"start\":7515},{\"end\":9266,\"start\":8239},{\"end\":10120,\"start\":9268},{\"end\":10735,\"start\":10122},{\"end\":10860,\"start\":10769},{\"end\":11618,\"start\":10862},{\"end\":11723,\"start\":11620},{\"end\":12045,\"start\":11792},{\"end\":12534,\"start\":12047},{\"end\":13490,\"start\":12560},{\"end\":13820,\"start\":13492},{\"end\":14326,\"start\":13860},{\"end\":14483,\"start\":14365},{\"end\":14702,\"start\":14485},{\"end\":15247,\"start\":14704},{\"end\":15366,\"start\":15249},{\"end\":15859,\"start\":15405},{\"end\":16802,\"start\":15909},{\"end\":17819,\"start\":16818},{\"end\":18219,\"start\":17838},{\"end\":19183,\"start\":18221},{\"end\":19970,\"start\":19185},{\"end\":20291,\"start\":19972},{\"end\":21068,\"start\":20368},{\"end\":21467,\"start\":21070},{\"end\":21893,\"start\":21469},{\"end\":22249,\"start\":21895},{\"end\":22838,\"start\":22303},{\"end\":23174,\"start\":22864},{\"end\":23741,\"start\":23176},{\"end\":24525,\"start\":23743},{\"end\":24906,\"start\":24552},{\"end\":25310,\"start\":24908},{\"end\":25783,\"start\":25312},{\"end\":25837,\"start\":25785},{\"end\":26876,\"start\":25847},{\"end\":26979,\"start\":26878},{\"end\":28009,\"start\":27020},{\"end\":28239,\"start\":28011},{\"end\":28611,\"start\":28254},{\"end\":29108,\"start\":28670},{\"end\":29803,\"start\":29153},{\"end\":29830,\"start\":29805},{\"end\":29932,\"start\":29832},{\"end\":30213,\"start\":29978},{\"end\":30911,\"start\":30255},{\"end\":31172,\"start\":30977},{\"end\":31470,\"start\":31182},{\"end\":31764,\"start\":31523},{\"end\":32477,\"start\":31817}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11791,\"start\":11724},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15404,\"start\":15367},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15908,\"start\":15860}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18264,\"start\":18256},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18639,\"start\":18631},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19103,\"start\":19095},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19399,\"start\":19391},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19878,\"start\":19870},{\"end\":20058,\"start\":20051},{\"end\":21485,\"start\":21478},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":23760,\"start\":23752},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25399,\"start\":25391},{\"end\":27332,\"start\":27325},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":30063,\"start\":30056},{\"end\":30370,\"start\":30363},{\"end\":30776,\"start\":30769}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1342,\"start\":1330},{\"attributes\":{\"n\":\"2.\"},\"end\":5005,\"start\":4993},{\"attributes\":{\"n\":\"3.\"},\"end\":10746,\"start\":10738},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10767,\"start\":10749},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12558,\"start\":12537},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13858,\"start\":13823},{\"attributes\":{\"n\":\"3.4.\"},\"end\":14363,\"start\":14329},{\"attributes\":{\"n\":\"4.\"},\"end\":16816,\"start\":16805},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17836,\"start\":17822},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20366,\"start\":20294},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":22301,\"start\":22252},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22862,\"start\":22841},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24550,\"start\":24528},{\"end\":25845,\"start\":25840},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27018,\"start\":26982},{\"attributes\":{\"n\":\"5.\"},\"end\":28252,\"start\":28242},{\"end\":28668,\"start\":28614},{\"end\":29151,\"start\":29111},{\"end\":29976,\"start\":29935},{\"end\":30253,\"start\":30216},{\"end\":30920,\"start\":30914},{\"end\":30975,\"start\":30923},{\"end\":31180,\"start\":31175},{\"end\":31521,\"start\":31473},{\"end\":31815,\"start\":31767},{\"end\":32549,\"start\":32539},{\"end\":33032,\"start\":33022},{\"end\":33385,\"start\":33375},{\"end\":33672,\"start\":33662},{\"end\":33748,\"start\":33737},{\"end\":33833,\"start\":33826},{\"end\":35470,\"start\":35461},{\"end\":35591,\"start\":35581},{\"end\":36593,\"start\":36584},{\"end\":38325,\"start\":38316}]", "table": "[{\"end\":35459,\"start\":34260},{\"end\":36582,\"start\":35594},{\"end\":36672,\"start\":36595},{\"end\":36955,\"start\":36796},{\"end\":38314,\"start\":37160}]", "figure_caption": "[{\"end\":32537,\"start\":32480},{\"end\":33020,\"start\":32551},{\"end\":33373,\"start\":33034},{\"end\":33660,\"start\":33387},{\"end\":33735,\"start\":33674},{\"end\":33824,\"start\":33751},{\"end\":33995,\"start\":33835},{\"end\":34260,\"start\":33998},{\"end\":35579,\"start\":35472},{\"end\":36796,\"start\":36675},{\"end\":37160,\"start\":36958},{\"end\":38406,\"start\":38327}]", "figure_ref": "[{\"end\":2096,\"start\":2088},{\"end\":2260,\"start\":2252},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3498,\"start\":3490},{\"end\":3662,\"start\":3654},{\"end\":8412,\"start\":8404},{\"end\":10828,\"start\":10822},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11108,\"start\":11102},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12639,\"start\":12633},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16223,\"start\":16217},{\"end\":21958,\"start\":21952},{\"end\":22248,\"start\":22242},{\"end\":25890,\"start\":25882},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27029,\"start\":27023},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27417,\"start\":27410},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27429,\"start\":27422},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28721,\"start\":28715},{\"end\":29271,\"start\":29265},{\"end\":29868,\"start\":29860},{\"end\":31054,\"start\":31048},{\"end\":31225,\"start\":31217},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31617,\"start\":31610},{\"end\":31921,\"start\":31914},{\"end\":32278,\"start\":32269}]", "bib_author_first_name": "[{\"end\":38942,\"start\":38939},{\"end\":38956,\"start\":38951},{\"end\":38968,\"start\":38962},{\"end\":39256,\"start\":39251},{\"end\":39258,\"start\":39257},{\"end\":39272,\"start\":39266},{\"end\":39284,\"start\":39278},{\"end\":39286,\"start\":39285},{\"end\":39305,\"start\":39299},{\"end\":39322,\"start\":39314},{\"end\":39339,\"start\":39332},{\"end\":39353,\"start\":39347},{\"end\":39364,\"start\":39360},{\"end\":39376,\"start\":39371},{\"end\":39878,\"start\":39870},{\"end\":39900,\"start\":39894},{\"end\":39923,\"start\":39916},{\"end\":39938,\"start\":39931},{\"end\":40400,\"start\":40391},{\"end\":40417,\"start\":40408},{\"end\":40432,\"start\":40424},{\"end\":40446,\"start\":40439},{\"end\":40460,\"start\":40452},{\"end\":40469,\"start\":40466},{\"end\":40976,\"start\":40972},{\"end\":40991,\"start\":40984},{\"end\":41006,\"start\":41000},{\"end\":41022,\"start\":41019},{\"end\":41680,\"start\":41679},{\"end\":41692,\"start\":41687},{\"end\":41705,\"start\":41700},{\"end\":41717,\"start\":41714},{\"end\":42223,\"start\":42215},{\"end\":42240,\"start\":42231},{\"end\":42265,\"start\":42258},{\"end\":42946,\"start\":42942},{\"end\":42959,\"start\":42953},{\"end\":42970,\"start\":42968},{\"end\":43350,\"start\":43346},{\"end\":43365,\"start\":43357},{\"end\":43378,\"start\":43374},{\"end\":43395,\"start\":43384},{\"end\":43845,\"start\":43838},{\"end\":43857,\"start\":43850},{\"end\":43873,\"start\":43865},{\"end\":43883,\"start\":43879},{\"end\":44302,\"start\":44298},{\"end\":44321,\"start\":44315},{\"end\":44502,\"start\":44499},{\"end\":44517,\"start\":44510},{\"end\":44532,\"start\":44525},{\"end\":44557,\"start\":44551},{\"end\":44575,\"start\":44567},{\"end\":45052,\"start\":45049},{\"end\":45063,\"start\":45058},{\"end\":45071,\"start\":45068},{\"end\":45085,\"start\":45079},{\"end\":45096,\"start\":45093},{\"end\":45110,\"start\":45103},{\"end\":45121,\"start\":45115},{\"end\":45134,\"start\":45126},{\"end\":45633,\"start\":45632},{\"end\":45649,\"start\":45644},{\"end\":46109,\"start\":46106},{\"end\":46126,\"start\":46117},{\"end\":46147,\"start\":46138},{\"end\":46691,\"start\":46687},{\"end\":46708,\"start\":46701},{\"end\":46720,\"start\":46716},{\"end\":46738,\"start\":46731},{\"end\":46751,\"start\":46744},{\"end\":47338,\"start\":47330},{\"end\":47353,\"start\":47348},{\"end\":47366,\"start\":47360},{\"end\":47812,\"start\":47808},{\"end\":47833,\"start\":47826},{\"end\":47849,\"start\":47843},{\"end\":47857,\"start\":47856},{\"end\":47868,\"start\":47867},{\"end\":47879,\"start\":47876},{\"end\":47891,\"start\":47882},{\"end\":47893,\"start\":47892},{\"end\":48585,\"start\":48577},{\"end\":48597,\"start\":48590},{\"end\":49350,\"start\":49342},{\"end\":49362,\"start\":49355},{\"end\":49864,\"start\":49859},{\"end\":49877,\"start\":49870},{\"end\":49888,\"start\":49885},{\"end\":49905,\"start\":49894},{\"end\":50407,\"start\":50400},{\"end\":50422,\"start\":50416},{\"end\":50438,\"start\":50430},{\"end\":51076,\"start\":51070},{\"end\":51090,\"start\":51086},{\"end\":51104,\"start\":51100},{\"end\":51118,\"start\":51113},{\"end\":51135,\"start\":51130},{\"end\":51148,\"start\":51143},{\"end\":51150,\"start\":51149},{\"end\":51164,\"start\":51158},{\"end\":51178,\"start\":51173},{\"end\":52003,\"start\":51995},{\"end\":52013,\"start\":52010},{\"end\":52027,\"start\":52019},{\"end\":52041,\"start\":52033},{\"end\":52521,\"start\":52513},{\"end\":52531,\"start\":52528},{\"end\":52545,\"start\":52537},{\"end\":52559,\"start\":52551},{\"end\":52827,\"start\":52819},{\"end\":52839,\"start\":52834},{\"end\":52853,\"start\":52846},{\"end\":52865,\"start\":52860},{\"end\":52876,\"start\":52873},{\"end\":52890,\"start\":52882},{\"end\":53457,\"start\":53454},{\"end\":53469,\"start\":53464},{\"end\":53484,\"start\":53480},{\"end\":54096,\"start\":54092},{\"end\":54111,\"start\":54104},{\"end\":54128,\"start\":54121},{\"end\":54143,\"start\":54136},{\"end\":54638,\"start\":54633},{\"end\":54652,\"start\":54646},{\"end\":54663,\"start\":54659},{\"end\":54678,\"start\":54669},{\"end\":55358,\"start\":55355},{\"end\":55372,\"start\":55366},{\"end\":55384,\"start\":55380},{\"end\":55392,\"start\":55389},{\"end\":55407,\"start\":55399},{\"end\":55418,\"start\":55413},{\"end\":55845,\"start\":55837},{\"end\":55862,\"start\":55855},{\"end\":55884,\"start\":55875},{\"end\":55901,\"start\":55895},{\"end\":56597,\"start\":56590},{\"end\":56607,\"start\":56603},{\"end\":56619,\"start\":56617},{\"end\":56631,\"start\":56626},{\"end\":57040,\"start\":57033},{\"end\":57055,\"start\":57046},{\"end\":57067,\"start\":57060},{\"end\":57078,\"start\":57074},{\"end\":57090,\"start\":57088},{\"end\":57102,\"start\":57097},{\"end\":57118,\"start\":57110},{\"end\":57129,\"start\":57124}]", "bib_author_last_name": "[{\"end\":38949,\"start\":38943},{\"end\":38960,\"start\":38957},{\"end\":38986,\"start\":38969},{\"end\":38996,\"start\":38988},{\"end\":39264,\"start\":39259},{\"end\":39276,\"start\":39273},{\"end\":39297,\"start\":39287},{\"end\":39312,\"start\":39306},{\"end\":39330,\"start\":39323},{\"end\":39345,\"start\":39340},{\"end\":39358,\"start\":39354},{\"end\":39369,\"start\":39365},{\"end\":39382,\"start\":39377},{\"end\":39892,\"start\":39879},{\"end\":39914,\"start\":39901},{\"end\":39929,\"start\":39924},{\"end\":39944,\"start\":39939},{\"end\":40406,\"start\":40401},{\"end\":40422,\"start\":40418},{\"end\":40437,\"start\":40433},{\"end\":40450,\"start\":40447},{\"end\":40464,\"start\":40461},{\"end\":40473,\"start\":40470},{\"end\":40982,\"start\":40977},{\"end\":40998,\"start\":40992},{\"end\":41017,\"start\":41007},{\"end\":41030,\"start\":41023},{\"end\":41685,\"start\":41681},{\"end\":41698,\"start\":41693},{\"end\":41712,\"start\":41706},{\"end\":41724,\"start\":41718},{\"end\":41733,\"start\":41726},{\"end\":42229,\"start\":42224},{\"end\":42256,\"start\":42241},{\"end\":42272,\"start\":42266},{\"end\":42951,\"start\":42947},{\"end\":42966,\"start\":42960},{\"end\":42975,\"start\":42971},{\"end\":43355,\"start\":43351},{\"end\":43372,\"start\":43366},{\"end\":43382,\"start\":43379},{\"end\":43401,\"start\":43396},{\"end\":43848,\"start\":43846},{\"end\":43863,\"start\":43858},{\"end\":43877,\"start\":43874},{\"end\":43887,\"start\":43884},{\"end\":44313,\"start\":44303},{\"end\":44333,\"start\":44322},{\"end\":44508,\"start\":44503},{\"end\":44523,\"start\":44518},{\"end\":44538,\"start\":44533},{\"end\":44549,\"start\":44540},{\"end\":44565,\"start\":44558},{\"end\":44582,\"start\":44576},{\"end\":44591,\"start\":44584},{\"end\":45056,\"start\":45053},{\"end\":45066,\"start\":45064},{\"end\":45077,\"start\":45072},{\"end\":45091,\"start\":45086},{\"end\":45101,\"start\":45097},{\"end\":45113,\"start\":45111},{\"end\":45124,\"start\":45122},{\"end\":45138,\"start\":45135},{\"end\":45642,\"start\":45634},{\"end\":45656,\"start\":45650},{\"end\":45660,\"start\":45658},{\"end\":46115,\"start\":46110},{\"end\":46136,\"start\":46127},{\"end\":46159,\"start\":46148},{\"end\":46699,\"start\":46692},{\"end\":46714,\"start\":46709},{\"end\":46729,\"start\":46721},{\"end\":46742,\"start\":46739},{\"end\":46756,\"start\":46752},{\"end\":47346,\"start\":47339},{\"end\":47358,\"start\":47354},{\"end\":47375,\"start\":47367},{\"end\":47824,\"start\":47813},{\"end\":47841,\"start\":47834},{\"end\":47854,\"start\":47850},{\"end\":47865,\"start\":47858},{\"end\":47874,\"start\":47869},{\"end\":47900,\"start\":47894},{\"end\":48588,\"start\":48586},{\"end\":48605,\"start\":48598},{\"end\":49353,\"start\":49351},{\"end\":49370,\"start\":49363},{\"end\":49868,\"start\":49865},{\"end\":49883,\"start\":49878},{\"end\":49892,\"start\":49889},{\"end\":49910,\"start\":49906},{\"end\":50414,\"start\":50408},{\"end\":50428,\"start\":50423},{\"end\":50446,\"start\":50439},{\"end\":51084,\"start\":51077},{\"end\":51098,\"start\":51091},{\"end\":51111,\"start\":51105},{\"end\":51128,\"start\":51119},{\"end\":51141,\"start\":51136},{\"end\":51156,\"start\":51151},{\"end\":51171,\"start\":51165},{\"end\":51189,\"start\":51179},{\"end\":51993,\"start\":51988},{\"end\":52008,\"start\":52004},{\"end\":52017,\"start\":52014},{\"end\":52031,\"start\":52028},{\"end\":52046,\"start\":52042},{\"end\":52052,\"start\":52048},{\"end\":52511,\"start\":52506},{\"end\":52526,\"start\":52522},{\"end\":52535,\"start\":52532},{\"end\":52549,\"start\":52546},{\"end\":52564,\"start\":52560},{\"end\":52570,\"start\":52566},{\"end\":52832,\"start\":52828},{\"end\":52844,\"start\":52840},{\"end\":52858,\"start\":52854},{\"end\":52871,\"start\":52866},{\"end\":52880,\"start\":52877},{\"end\":52894,\"start\":52891},{\"end\":53462,\"start\":53458},{\"end\":53478,\"start\":53470},{\"end\":53491,\"start\":53485},{\"end\":54102,\"start\":54097},{\"end\":54119,\"start\":54112},{\"end\":54134,\"start\":54129},{\"end\":54151,\"start\":54144},{\"end\":54644,\"start\":54639},{\"end\":54657,\"start\":54653},{\"end\":54667,\"start\":54664},{\"end\":54683,\"start\":54679},{\"end\":55364,\"start\":55359},{\"end\":55378,\"start\":55373},{\"end\":55387,\"start\":55385},{\"end\":55397,\"start\":55393},{\"end\":55411,\"start\":55408},{\"end\":55423,\"start\":55419},{\"end\":55853,\"start\":55846},{\"end\":55873,\"start\":55863},{\"end\":55893,\"start\":55885},{\"end\":55907,\"start\":55902},{\"end\":56601,\"start\":56598},{\"end\":56615,\"start\":56608},{\"end\":56624,\"start\":56620},{\"end\":56637,\"start\":56632},{\"end\":57044,\"start\":57041},{\"end\":57058,\"start\":57056},{\"end\":57072,\"start\":57068},{\"end\":57086,\"start\":57079},{\"end\":57095,\"start\":57091},{\"end\":57108,\"start\":57103},{\"end\":57122,\"start\":57119},{\"end\":57135,\"start\":57130}]", "bib_entry": "[{\"attributes\":{\"doi\":\"abs/1702.01105\",\"id\":\"b0\"},\"end\":39186,\"start\":38881},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":21435690},\"end\":39821,\"start\":39188},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":214754592},\"end\":40319,\"start\":39823},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":46937879},\"end\":40904,\"start\":40321},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b4\",\"matched_paper_id\":61153622},\"end\":41661,\"start\":40906},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3525710},\"end\":42107,\"start\":41663},{\"attributes\":{\"id\":\"b6\"},\"end\":42631,\"start\":42109},{\"attributes\":{\"id\":\"b7\"},\"end\":42874,\"start\":42633},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":195767107},\"end\":43292,\"start\":42876},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209414882},\"end\":43790,\"start\":43294},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":44272,\"start\":43792},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1915014},\"end\":44459,\"start\":44274},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57721134},\"end\":44963,\"start\":44461},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219964527},\"end\":45586,\"start\":44965},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6628106},\"end\":46036,\"start\":45588},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11091110},\"end\":46601,\"start\":46038},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":198165883},\"end\":47222,\"start\":46603},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":222072576},\"end\":47741,\"start\":47224},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3719281},\"end\":48508,\"start\":47743},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1505637},\"end\":49277,\"start\":48510},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":54457562},\"end\":49765,\"start\":49279},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":58004623},\"end\":50317,\"start\":49767},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52086927},\"end\":51041,\"start\":50319},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13756489},\"end\":51921,\"start\":51043},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":219963198},\"end\":52445,\"start\":51923},{\"attributes\":{\"id\":\"b25\"},\"end\":52725,\"start\":52447},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53988041},\"end\":53378,\"start\":52727},{\"attributes\":{\"id\":\"b27\"},\"end\":53787,\"start\":53380},{\"attributes\":{\"id\":\"b28\"},\"end\":54026,\"start\":53789},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":198985698},\"end\":54553,\"start\":54028},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15644143},\"end\":55279,\"start\":54555},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":199064623},\"end\":55768,\"start\":55281},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":50787222},\"end\":56518,\"start\":55770},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4387390},\"end\":57029,\"start\":56520},{\"attributes\":{\"id\":\"b34\"},\"end\":57457,\"start\":57031}]", "bib_title": "[{\"end\":39249,\"start\":39188},{\"end\":39868,\"start\":39823},{\"end\":40389,\"start\":40321},{\"end\":40970,\"start\":40906},{\"end\":41677,\"start\":41663},{\"end\":42940,\"start\":42876},{\"end\":43344,\"start\":43294},{\"end\":43836,\"start\":43792},{\"end\":44296,\"start\":44274},{\"end\":44497,\"start\":44461},{\"end\":45047,\"start\":44965},{\"end\":45630,\"start\":45588},{\"end\":46104,\"start\":46038},{\"end\":46685,\"start\":46603},{\"end\":47328,\"start\":47224},{\"end\":47806,\"start\":47743},{\"end\":48575,\"start\":48510},{\"end\":49340,\"start\":49279},{\"end\":49857,\"start\":49767},{\"end\":50398,\"start\":50319},{\"end\":51068,\"start\":51043},{\"end\":51986,\"start\":51923},{\"end\":52817,\"start\":52727},{\"end\":54090,\"start\":54028},{\"end\":54631,\"start\":54555},{\"end\":55353,\"start\":55281},{\"end\":55835,\"start\":55770},{\"end\":56588,\"start\":56520}]", "bib_author": "[{\"end\":38951,\"start\":38939},{\"end\":38962,\"start\":38951},{\"end\":38988,\"start\":38962},{\"end\":38998,\"start\":38988},{\"end\":39266,\"start\":39251},{\"end\":39278,\"start\":39266},{\"end\":39299,\"start\":39278},{\"end\":39314,\"start\":39299},{\"end\":39332,\"start\":39314},{\"end\":39347,\"start\":39332},{\"end\":39360,\"start\":39347},{\"end\":39371,\"start\":39360},{\"end\":39384,\"start\":39371},{\"end\":39894,\"start\":39870},{\"end\":39916,\"start\":39894},{\"end\":39931,\"start\":39916},{\"end\":39946,\"start\":39931},{\"end\":40408,\"start\":40391},{\"end\":40424,\"start\":40408},{\"end\":40439,\"start\":40424},{\"end\":40452,\"start\":40439},{\"end\":40466,\"start\":40452},{\"end\":40475,\"start\":40466},{\"end\":40984,\"start\":40972},{\"end\":41000,\"start\":40984},{\"end\":41019,\"start\":41000},{\"end\":41032,\"start\":41019},{\"end\":41687,\"start\":41679},{\"end\":41700,\"start\":41687},{\"end\":41714,\"start\":41700},{\"end\":41726,\"start\":41714},{\"end\":41735,\"start\":41726},{\"end\":42231,\"start\":42215},{\"end\":42258,\"start\":42231},{\"end\":42274,\"start\":42258},{\"end\":42953,\"start\":42942},{\"end\":42968,\"start\":42953},{\"end\":42977,\"start\":42968},{\"end\":43357,\"start\":43346},{\"end\":43374,\"start\":43357},{\"end\":43384,\"start\":43374},{\"end\":43403,\"start\":43384},{\"end\":43850,\"start\":43838},{\"end\":43865,\"start\":43850},{\"end\":43879,\"start\":43865},{\"end\":43889,\"start\":43879},{\"end\":44315,\"start\":44298},{\"end\":44335,\"start\":44315},{\"end\":44510,\"start\":44499},{\"end\":44525,\"start\":44510},{\"end\":44540,\"start\":44525},{\"end\":44551,\"start\":44540},{\"end\":44567,\"start\":44551},{\"end\":44584,\"start\":44567},{\"end\":44593,\"start\":44584},{\"end\":45058,\"start\":45049},{\"end\":45068,\"start\":45058},{\"end\":45079,\"start\":45068},{\"end\":45093,\"start\":45079},{\"end\":45103,\"start\":45093},{\"end\":45115,\"start\":45103},{\"end\":45126,\"start\":45115},{\"end\":45140,\"start\":45126},{\"end\":45644,\"start\":45632},{\"end\":45658,\"start\":45644},{\"end\":45662,\"start\":45658},{\"end\":46117,\"start\":46106},{\"end\":46138,\"start\":46117},{\"end\":46161,\"start\":46138},{\"end\":46701,\"start\":46687},{\"end\":46716,\"start\":46701},{\"end\":46731,\"start\":46716},{\"end\":46744,\"start\":46731},{\"end\":46758,\"start\":46744},{\"end\":47348,\"start\":47330},{\"end\":47360,\"start\":47348},{\"end\":47377,\"start\":47360},{\"end\":47826,\"start\":47808},{\"end\":47843,\"start\":47826},{\"end\":47856,\"start\":47843},{\"end\":47867,\"start\":47856},{\"end\":47876,\"start\":47867},{\"end\":47882,\"start\":47876},{\"end\":47902,\"start\":47882},{\"end\":48590,\"start\":48577},{\"end\":48607,\"start\":48590},{\"end\":49355,\"start\":49342},{\"end\":49372,\"start\":49355},{\"end\":49870,\"start\":49859},{\"end\":49885,\"start\":49870},{\"end\":49894,\"start\":49885},{\"end\":49912,\"start\":49894},{\"end\":50416,\"start\":50400},{\"end\":50430,\"start\":50416},{\"end\":50448,\"start\":50430},{\"end\":51086,\"start\":51070},{\"end\":51100,\"start\":51086},{\"end\":51113,\"start\":51100},{\"end\":51130,\"start\":51113},{\"end\":51143,\"start\":51130},{\"end\":51158,\"start\":51143},{\"end\":51173,\"start\":51158},{\"end\":51191,\"start\":51173},{\"end\":51995,\"start\":51988},{\"end\":52010,\"start\":51995},{\"end\":52019,\"start\":52010},{\"end\":52033,\"start\":52019},{\"end\":52048,\"start\":52033},{\"end\":52054,\"start\":52048},{\"end\":52513,\"start\":52506},{\"end\":52528,\"start\":52513},{\"end\":52537,\"start\":52528},{\"end\":52551,\"start\":52537},{\"end\":52566,\"start\":52551},{\"end\":52572,\"start\":52566},{\"end\":52834,\"start\":52819},{\"end\":52846,\"start\":52834},{\"end\":52860,\"start\":52846},{\"end\":52873,\"start\":52860},{\"end\":52882,\"start\":52873},{\"end\":52896,\"start\":52882},{\"end\":53464,\"start\":53454},{\"end\":53480,\"start\":53464},{\"end\":53493,\"start\":53480},{\"end\":54104,\"start\":54092},{\"end\":54121,\"start\":54104},{\"end\":54136,\"start\":54121},{\"end\":54153,\"start\":54136},{\"end\":54646,\"start\":54633},{\"end\":54659,\"start\":54646},{\"end\":54669,\"start\":54659},{\"end\":54685,\"start\":54669},{\"end\":55366,\"start\":55355},{\"end\":55380,\"start\":55366},{\"end\":55389,\"start\":55380},{\"end\":55399,\"start\":55389},{\"end\":55413,\"start\":55399},{\"end\":55425,\"start\":55413},{\"end\":55855,\"start\":55837},{\"end\":55875,\"start\":55855},{\"end\":55895,\"start\":55875},{\"end\":55909,\"start\":55895},{\"end\":56603,\"start\":56590},{\"end\":56617,\"start\":56603},{\"end\":56626,\"start\":56617},{\"end\":56639,\"start\":56626},{\"end\":57046,\"start\":57033},{\"end\":57060,\"start\":57046},{\"end\":57074,\"start\":57060},{\"end\":57088,\"start\":57074},{\"end\":57097,\"start\":57088},{\"end\":57110,\"start\":57097},{\"end\":57124,\"start\":57110},{\"end\":57137,\"start\":57124}]", "bib_venue": "[{\"end\":38937,\"start\":38881},{\"end\":39426,\"start\":39384},{\"end\":40013,\"start\":39946},{\"end\":40549,\"start\":40475},{\"end\":41115,\"start\":41036},{\"end\":41791,\"start\":41735},{\"end\":42213,\"start\":42109},{\"end\":42653,\"start\":42633},{\"end\":43029,\"start\":42977},{\"end\":43481,\"start\":43403},{\"end\":43963,\"start\":43889},{\"end\":44348,\"start\":44335},{\"end\":44660,\"start\":44593},{\"end\":45207,\"start\":45140},{\"end\":45718,\"start\":45662},{\"end\":46215,\"start\":46161},{\"end\":46827,\"start\":46758},{\"end\":47441,\"start\":47377},{\"end\":48004,\"start\":47902},{\"end\":48719,\"start\":48607},{\"end\":49441,\"start\":49372},{\"end\":49981,\"start\":49912},{\"end\":50500,\"start\":50448},{\"end\":51303,\"start\":51191},{\"end\":52121,\"start\":52054},{\"end\":52504,\"start\":52447},{\"end\":52965,\"start\":52896},{\"end\":53452,\"start\":53380},{\"end\":53810,\"start\":53789},{\"end\":54221,\"start\":54153},{\"end\":54737,\"start\":54685},{\"end\":55489,\"start\":55425},{\"end\":55961,\"start\":55909},{\"end\":56702,\"start\":56639},{\"end\":39442,\"start\":39428},{\"end\":40031,\"start\":40015},{\"end\":40574,\"start\":40551},{\"end\":41251,\"start\":41160},{\"end\":41814,\"start\":41793},{\"end\":42677,\"start\":42655},{\"end\":43054,\"start\":43031},{\"end\":43499,\"start\":43483},{\"end\":43983,\"start\":43965},{\"end\":44682,\"start\":44662},{\"end\":45225,\"start\":45209},{\"end\":45738,\"start\":45720},{\"end\":46234,\"start\":46217},{\"end\":46848,\"start\":46829},{\"end\":47492,\"start\":47443},{\"end\":48021,\"start\":48006},{\"end\":48859,\"start\":48840},{\"end\":49462,\"start\":49443},{\"end\":50002,\"start\":49983},{\"end\":50588,\"start\":50573},{\"end\":51443,\"start\":51424},{\"end\":52139,\"start\":52123},{\"end\":52986,\"start\":52967},{\"end\":53831,\"start\":53812},{\"end\":54235,\"start\":54223},{\"end\":54823,\"start\":54804},{\"end\":55540,\"start\":55491},{\"end\":56049,\"start\":56034},{\"end\":56727,\"start\":56704}]"}}}, "year": 2023, "month": 12, "day": 17}