{"id": 212415058, "updated": "2023-10-06 18:12:13.789", "metadata": {"title": "Towards Fair Cross-Domain Adaptation via Generative Learning", "authors": "[{\"first\":\"Tongxin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhengming\",\"last\":\"Ding\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Haixu\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Kun\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 3, "day": 4}, "abstract": "Domain Adaptation (DA) targets at adapting a model trained over the well-labeled source domain to the unlabeled target domain lying in different distributions. Existing DA normally assumes the well-labeled source domain is class-wise balanced, which means the size per source class is relatively similar. However, in real-world applications, labeled samples for some categories in the source domain could be extremely few due to the difficulty of data collection and annotation, which leads to decreasing performance over target domain on those few-shot categories. To perform fair cross-domain adaptation and boost the performance on these minority categories, we develop a novel Generative Few-shot Cross-domain Adaptation (GFCA) algorithm for fair cross-domain classification. Specifically, generative feature augmentation is explored to synthesize effective training data for few-shot source classes, while effective cross-domain alignment aims to adapt knowledge from source to facilitate the target learning. Experimental results on two large cross-domain visual datasets demonstrate the effectiveness of our proposed method on improving both few-shot and overall classification accuracy comparing with the state-of-the-art DA approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.02366", "mag": "3010314012", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/WangD0T021", "doi": "10.1109/wacv48630.2021.00050"}}, "content": {"source": {"pdf_hash": "40eae10cab5ddf380daa239296837818342c3a03", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.02366v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.02366", "status": "GREEN"}}, "grobid": {"id": "08a59cc5ada03e0970d3ea000318f1f39f1c3d46", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/40eae10cab5ddf380daa239296837818342c3a03.txt", "contents": "\nTowards Fair Cross-Domain Adaptation via Generative Learning\n\n\nTongxin Wang \nDepartment of Computer Science\nIndiana University Bloomington\nUSA\n\nZhengming Ding \nDepartment of CIT\nIndiana University-Purdue University Indianapolis\nUSA\n\nWei Shao shaowei@iu.edu \nDepartment of Medicine\nIndiana University School of Medicine\nUSA\n\nHaixu Tang hatang@indiana.edu \nDepartment of Computer Science\nIndiana University Bloomington\nUSA\n\nKun Huang kunhuang@iu.edu \nDepartment of Medicine\nIndiana University School of Medicine\nUSA\n\nRegenstrief Institute\nUSA\n\nTowards Fair Cross-Domain Adaptation via Generative Learning\nFew-shot learningDomain adaptationGenerative models\nDomain Adaptation (DA) targets at adapting a model trained over the well-labeled source domain to the unlabeled target domain lying in different distributions. Existing DA normally assumes the well-labeled source domain is class-wise balanced, which means the size per source class is relatively similar. However, in real-world applications, labeled samples for some categories in the source domain could be extremely few due to the difficulty of data collection and annotation, which leads to decreasing performance over target domain on those few-shot categories. To perform fair cross-domain adaptation and boost the performance on these minority categories, we develop a novel Generative Fewshot Cross-domain Adaptation (GFCA) algorithm for fair cross-domain classification. Specifically, generative feature augmentation is explored to synthesize effective training data for few-shot source classes, while effective cross-domain alignment aims to adapt knowledge from source to facilitate the target learning. Experimental results on two large crossdomain visual datasets demonstrate the effectiveness of our proposed method on improving both few-shot and overall classification accuracy comparing with the state-of-the-art DA approaches.\n\nIntroduction\n\nIn recent years, deep learning has achieved significant advances in various applications. In most real-world applications, the availability of large-scale labeled data is crucial. However, manually collecting and annotating data are extremely expensive and time consuming for every new domain. On the other hand, we can often access abundant data with limited or even no labels. Domain adaptation (DA) [24,36] considers the problem of transferring a machine learning model from a source domain towards a different target domain, where data from different domains lie in different distributions. By reducing both the marginal and conditional mismatch in feature space between domains, knowledge transfer from an external labeled, well-established source domain data to target domain can be achieved. Existing DA methods generally deal with domain adaptation by assuming source classes are balanced. However, in reality, labeled data from some categories may be limited due to the difficulty of data collection and annotation, making data in source domain extremely imbalanced. We refer to this as Few-shot Cross-domain Adaptation, where the classes with limited samples denote few-shot classes while others represent normal classes. The few-shot crossdomain adaptation problem raises concern when considering the fairness of the corresponding learning task as the available training data is already contaminated by bias towards classes with majority of samples [4].\n\nExisting DA methods can be roughly summarized into two categories: discrepancy-based methods and adversarial-based methods [36]. Discrepancy-based methods focus on diminishing the domain shift through minimizing discrepancy metrics, such as maximum mean discrepancy (MMD) [17,32] and correlation alignment (CORAL) loss [28,29,41]. More recently, inspired by the idea of generative adversarial networks [9], adversarial-based methods aiming to match feature distributions through adversarial training have attracted great attentions [7,16,31]. Methods in both categories are designed to reduce the cross-domain gap by aligning domain-wise distributions explicitly, so that the model derived from the source domain can be applied to the target domain directly.\n\nDealing with imbalanced data is commonly encountered in real-world applications where some classes may have very limited labeled training data. However, directly minimizing the average error to fit the entire training data could make learning techniques bias towards the majority population and lead to higher distribution of errors and unfair results in the minority population [4]. Moreover, the idea of learning novel concepts from very few examples have attracted much attention in few-shot learning approaches [6,25,30,38]. Among the existing approaches, data augmentation is a straightforward method to boost the performance on few-shot classes through synthesizing more data in input space [12,26,37,39] or in a learned feature space [5,34]. Generative models have been utilized to synthesize more training data for few-shot classes through capturing the data variation from base classes [9,22]. Moreover, extended from typical DA problem, methods have been proposed to directly tackle the problem of imbalanced data in DA [1,21,35]. Existing DA methods focus on sample re-weighting [1], instance re-weighting with subspace learning [21], and class re-weighting with distribution adaptation [35]. However, these methods did not fully utilize the powerful generative models to directly augment samples for underrepresented few-shot classes.\n\nTo this end, we propose a novel Generative Few-shot Cross-domain Adaptation model (GFCA) to enhance the adaptation ability from extremely imbalanced source domain for better and fairer target sample prediction. Specifically, we focus on generating effective auxiliary data for the few-shot classes in the source domain to facilitate the classification accuracy for the under-represented classes. To the best of our knowledge, this is the very pioneering work to explore the few-shot cross-domain adaptation problem for fair cross-domain classification. The main contributions of our paper are summarized as follows:\n\n-We incorporate the generative adversarial network in training a DA network for both normal and few-shot classes. Specifically, the generator attempts to synthesize effective fake data at the learned feature space to augment fewshot classes, while the discriminator guides the data generation in order to adapt data variation in normal classes to generate data in few-shot classes. -We combine the generative adversarial network with the DA network to train a general classifier for both source and target domain. Specifically, we build a conditional generative adversarial network to promote the performance of the DA network on few-shot classes. -We evaluate our proposed model on two cross-domain visual benchmarks.\n\nComparing with state-of-the-art methods, our model achieves significant improvement in both few-shot and overall classification accuracy, which indicates fairer classification capability.\n\n\nRelated Work\n\nIn this section, since few-shot cross-domain adaptation is still an open problem, we mainly review two research areas that are related to the proposed model: data augmentation and unsupervised DA.\n\n\nData Augmentation\n\nData augmentation is a straightforward approach to improve the performance on few-shot classification. Generating data with enough intra-class variation in few-shot classes is crucial to effective data augmentation. Hariharan et al. [12] proposed techniques to \"hallucinate\" additional training examples for few-shot classes by leveraging the mode of variation in the normal classes. Hauberg et al. [13] proposed to learn augmentation strategy on a per-class basis, where a statistical model is built for transformations within a given class and used for augmenting the dataset. DeVries and Taylor [5] proposed to augment data in the learned feature space using simple transformations. Recently, deep generative models [2,9,20,22,23] have been exploited to synthesize data for few-shot classes by capturing the intra-class variation in normal classes. Data Augmentation GAN (DAGAN) [2] synthesizes new data by adding noise to lower-dimensional representations of the data learned through a generator network with an encoder and a decoder. BAlancing GAN (BAGAN) [20] also utilizes an autoencoder for data generation and learns useful features from normal classes for few-shot data synthesis by applying class conditioning on the latent space in the autoencoder. However, the key difference between GFCA and existing works is that we utilize data generation in the feature domain rather than in the image domain using deep generative models while simultaneously seeking DA between source and target domains.\n\n\nUnsupervised Domain Adaptation\n\nUnsupervised DA aims to bridge the distribution difference between domains with unlabeled target domain data. Recently, deep DA methods have been attracting popularity by utilizing the power of deep neural networks. Discrepancybased methods aim to diminish the domain shift by minimizing certain discrepancy metrics [15,17,29,32,41]. Deep Domain Confusion (DDC) network [32] incorporates an additional loss based on the MMD between the source and target representations at the last fully-connected layer to jointly optimize for classification and domain invariance. Extended from DDC, Domain Adaptation Network (DAN) [17] utilizes multiple kernel MMD-based loss across domains on the last three task-specific layers and achieves better performance. More recently, Lee et al. [15] proposed Sliced Wasserstein Discrepancy (SWD), which can be minimized between task-specific classifiers to align the distributions across domains. Inspired by generative adversarial networks [9], adversarial-based methods utilize adversarial training to align feature distributions across domains. Domain-Adversarial Neural Network (DANN) [7] is one of the first methods to learn domain-invariant representations through adversarial learning between the domain classification and feature representation. Tzeng et al. [31] proposed a unified framework for unsupervised DA and further introduced Adversarial Discriminative Domain Adaptation (ADDA) by minimizing a GAN-based loss. More recently, Cao et al. [3] presented Partial Adversarial Domain Adaptation (PADA) for more effective adaptation when source domain and target domain do not share identical label space. Zhang et al. [40] introduced the well-designed theoryinduced Margin Disparity Discrepancy (MDD) for adversarial training in DA. In this paper, to demonstrate the necessity of leveraging generative models for data augmentation, we utilize the well-established DAN method for DA based on the real source data, synthetic source data, and target data.\n\n\nThe Proposed Algorithm\n\n\nPreliminary and Motivation\n\nIn the unsupervised DA problem, we are given a labeled source domain and an unlabeled target domain. The source domain contains n s data points from c classes: {X s , Y s } = {(x s,1 , y s,1 ), ..., (x s,ns , y s,ns )}, where x s,i \u2208 R dx is the feature vector with d x dimensions and y s,i \u2208 R c is the corresponding one-hot label vector. The target domain contains n t data points: {X t } = {x t,1 , ..., x t,nt }, in which x t,i \u2208 R dx . Source and target domains are different in terms of data distributions, but share consistent label information, and the goal is to recognize the unlabeled target samples.\n\nIn Few-shot Cross-domain Adaptation problem, we further assume that the labeled source domain data contains two sets of data without overlap in classes: Normal Set (i.e., normal classes) {X sn , Y sn } with c n classes and Few-Shot Set (i.e., few-shot classes)\n{X sl , Y sl } with c l classes, where {X s , Y s } = {X sn , Y sn } \u222a {X sl , Y sl } and c = c n + c l .\nIn this problem, few-shot set contains far less number Source domain contains labeled data of normal and few-shot classes without overlapping classes. Target domain contains unlabeled data sharing the same set of classes as source domain. Representation learning trains effective ResNet-50 feature extractor using the labeled source data to transfer the image into a feature space for both source and target domains. Generative data augmentation is used to synthesize effective training samples from random noise vector z and one-hot label vector y by training source generator G(\u00b7). Cross-domain adaptation seeks a general classifier C(\u00b7) for both source and target domain through effective cross-domain alignment by training the adaptation network E(\u00b7).\n\nof samples per class, and our goal is to achieve high classification performance in both normal classes and few-shot classes on the unlabeled target samples.\n\nIn reality, labeled data from some classes may be limited due to the difficulty of data collection and annotation. While existing DA methods generally assume source classes to be balanced, few-shot cross-domain adaptation is a challenging task because of extremely imbalanced source domain. Few-shot classes only contains limited labeled training data with limited variation within each class. Traditional data augmentation methods [12] uses human designed rules to generate more data for the few-shot classes, where the improvement to the classifier is limited. Moreover, it is important to balance between the normal set and the few-shot set, since trying to improve the performance on the few-shot classes could hurt normal set classification. Thus, it is important to generate effective data to expand the intra-class variation in few-shot classes. Generative models have shown great promise in synthesizing effective data. Since normal set usually contains larger intra-class variations, generative models can be leveraged to adapt the variation in normal set to few-shot set during data generation. Once we properly augment the source domain data to promote few-shot classes, we can adopt DA approaches to train a general classifier for both source and target domain and both normal and few-shot sets. In this paper, we utilize MMD-based regularization for cross-domain alignment, where a classifier can be subsequently trained to effectively classify data from both domains.\n\n\nGenerative Few-Shot Cross-Domain Learning\n\nAn illustration of our proposed Generative Few-shot Cross-domain Adaptation (GFCA) algorithm is shown in Figure 1. An representation model is built first with supervised learning framework to transfer the image into a discriminative feature domain by training on the source domain data. Taking the input of a random noise and a one-hot label vector, the generator is trained to synthesize effective source domain data with the guidance of the discriminator in the learned feature space. A MMD-regularized encoder and classifier is then trained using real source domain data, synthetic fake source domain data, and target domain data to perform effective classification on data from both domains.\n\nGenerative Data Augmentation Generative models trained via adversarial training can synthesize fake data from random noise [9]. Moreover, conditional generative models have been proved to be more effective in synthesizing meaningful data by conditioning the model on additional information such as class labels [22]. For generative data augmentation, our goal is to use generative models to generate fake data at the learned feature space with class labels for augmenting source domain training data. We denote the random noise by z \u2208 R dz , real feature vector by x \u2208 R dx , and the corresponding one-hot label vector by y \u2208 R c .\n\nThe generator G(\u00b7) attempts to synthesize fake feature vector x f with the inputs of z and y:\nG(z|y) = \u03c6 g (W g \uf8ee \uf8f0 z y \uf8f9 \uf8fb ) = \u03c6 g ( W z , W y \uf8ee \uf8f0 z y \uf8f9 \uf8fb ) = \u03c6 g (W z z + W y y),(1)\nwhere W z \u2208 R dx\u00d7dz and W y \u2208 R dx\u00d7c are the weight matrices. \u03c6 g (\u00b7) is the element-wise activation function.\n\nWe attempt to augment few-shot classes in the source domain by synthesizing fake data using generator G(\u00b7) in the learned feature space. To acquire meaningful synthetic features, we could span the feature space of each few-shot class around its center. Thus, we use W y y to capture the class center information and use W z z to capture the class variance information. We initialize W y with the class centroid of source domain features in the training set. The normal set contains abundant samples for each class, which makes the estimation of class centers more accurate than class centers of few-shot set during initialization. Therefore, we include an additional L2 regularization on the weight vectors for the normal set in W y to discourage it from changing dramatically comparing to initialization. For the random noise part, to faithfully capture the class variation, we initialize each column of W z by taking the top d z principle components from the source training data X s and scale each column of W z by multiplying the corresponding eigenvalues. Each element in z is sampled from a uniform distribution between -1 and 1. We also add a normalization process to make the synthetic feature vectors be in the same scale as the real source features. For a fake feature vector x f generated by G(\u00b7), we normalize\nx f as N (x f ) = x f / x f 2 * \u03b2, where \u03b2 is the average norm of real source domain features.\nThe discriminator D(\u00b7) attempts to distinguish the fake feature vectors from the real ones, which is designed as:\nD(x) = \u03c6 d (W d x + b d ),(2)\nwhere W d \u2208 R 1\u00d7dx and b d \u2208 R. \u03c6 d (\u00b7) projects the input to a value between 0 and 1. The generator G(\u00b7) aims to generate features similar to real source features, while the discriminator D(\u00b7) aims to differentiate fake features from real features. Therefore, the loss function for the generator L g and for the discriminator L d can be formulated as:\nL g = \u2212E[D(x f )](3)L d = \u2212E[D(x r )] \u2212 E[1 \u2212 D(x f )](4)\nwhere x r represents the real feature vector and x f = G(z|y) represents the fake feature vector given one-hot label y and random noise z. As shown above, the generator is trained by minimizing L g , while the discriminator is trained by minimizing L d . Thus, the training of generator G(\u00b7) and discriminator D(\u00b7) can be formulated as a two-player minimax problem.\n\n\nCross-Domain Alignment and Fair Classification\n\nMaximum Mean Discrepancy (MMD) [27] is a non-parametric distance estimate between two probability distributions based on their samples. MMD-based regularization has proven to be highly effective in many DA tasks [17,18,27]. Therefore, we utilize MMD to bridge the distribution gap between the learned representations of source and target domain features. Given samples X s = {x s,i } i=1,...,ns drawn from distribution D s and X t = {x t,i } i=1,...,nt drawn from distribution D t , the empirical estimate of MMD can be written as\nM M D(X s , X t ) = 1 n s ns i=1 \u03c6(x s,i ) \u2212 1 n t nt j=1 \u03c6(x t,j ) H(5)\nwhere \u03c6(\u00b7) is the feature space map from the original feature space to a universal Reproducing Kernel Hilbert Space (RKHS) H. Specifically, in this paper, we utilize a multiple kernel variant of MMD (MK-MMD) proposed by Gretton et al. [10], which is also the backbone of DAN [17]. We exploit an encoder E(\u00b7) for learning transferable features with dimension d h between source and target domains. For samples from the source domain X s and the target domain X t , the encoder attempts to minimize L e , which is defined as the MMD between their corresponding representations H s and H t produced by the encoder:\nL e = M M D(H s , H t ) = M M D(E(X s ), E(X t ))(6)\nFinally, we train a classifier C(\u00b7) with c classes and weight matrix W c \u2208 R dc\u00d7d h to classify samples from both domains. Since there are only limited labeled training samples for the c l few shot classes, in order to obtain a good classifier for few-shot classification, we use the generator to synthesize additional labeled samples from the source domain for training. Therefore, the classification loss can be formulated as\nL c = L sr + L sf = \u2212E[log P (Y = y r |h sr )] \u2212 E[log P (Y = y f |E(x sf )] = \u2212E[log P (Y = y r |E(x sr ))] \u2212 E[log P (Y = y f |E(G(z|y f )))](7)\nwhere L sr is the classification loss for labeled real data from source domain x sr with label y r and L sf is the classification loss for fake source domain feature vector x sf generated by G(\u00b7) with label y f .\n\nTo further balance between the few-shot set with limited training samples and the normal set, we incorporate a weight regularizer on the classifier using a fair classification (FC) term [11] for fairer classification across few-shot and normal classes. Denoting the classifier weight vector for the k-th class by w k , the FC term can be formulated as\nL f c = ( 1 C l k\u2208C l w k 2 2 \u2212 \u03b1) 2(8)\nwhere C l is the set of the class indices for the few-shot set. \u03b1 is the parameter learned from the average of the squared norms of weight vectors for the normal classes:\n\u03b1 = 1 |C n | k\u2208Cn w k 2 2 (9)\nwhere C n is the set of the class indices for the normal set. The FC term is based on the assumption that on average, samples in the few-shot classes should occupy a space of similar volume in the feature space as the normal classes. Therefore, in Eq. 8, the average of the squared norms of the weight vectors for the few-shot classes are regularized to the same scale as normal classes.\n\nIn summary, the overall objective for cross domain alignment and few-shot promotion can be written as\nL ec = L c + \u03bbL e + \u03b3L f c(10)\nwhere \u03bb and \u03b3 controls the relative importance of the MMD regularization term and the fair classification term. The encoder E(\u00b7) and classifier C(\u00b7) are trained jointly by minimizing L ec .\n\n\nImplementation Details\n\nWe choose the standard residual network with 50 layers (ResNet-50) [14] pre-trained on the ImageNet dataset as the backbone of our feature extractor. We fine-tune the ResNet-50 network using the labeled source domain data for each task and use the last pooling layer as the image representation. The hyper-parameters \u03bb and \u03b3 in the Eq. 10 are selected as 1 throughout all experiments. We use the sigmoid activation function for D(\u00b7), and the leaky-ReLU activation function for G(\u00b7), E(\u00b7), and C(\u00b7). We adopt the adaptive moment estimation (Adam) to train the network with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. The generator G(\u00b7) and discriminator D(\u00b7) is first pre-trained separately to acquire a good initialization of the generative feature augmentation network. During the end-to-end training of the entire network, for each iteration, we first sample real labeled source domain data and unlabeled target domain data. Fake labeled source domain features is then synthesized through the generator. The encoder and classifier are then updated to minimize L ec . Next, we fix the discriminator and optimize the generator by minimizing L g . Finally, we constrain the generator to update the discriminator by minimizing L d . Thus, we alternatively update different modules within GFCA until it converges.\n\n\nExperiments\n\n\nDatasets\n\nOffice31 [8] consists of images within 31 categories from 3 domains (Amazon, Webcan, and DSLR). Specifically, Amazon images are downloaded from online merchants with clear backgrounds. Webcam and DSLR images are taken from office environments, where Webcam images are taken by a low-resolution web camera and DSLR images are taken by a high-resolution digital SLR camera. Office-Home [33] consists of images within 65 categories from 4 domains (Art, Clipart, Product, and Real-world). Specifically, Art represents artistic depictions of objects; Clipart contains clipart images; Product consists of images of objects without a background, similar to the Amazon domain in the Office31 dataset; Real-world contains images of objects captured with a regular camera.\n\n\nComparison Experiments\n\nWe compare our proposed GFCA algorithm with the following state-of-the-art DA methods: DAN [17], DANN [7], ADDA [31], PADA [3], MDD [40], and SWD [15]. Specifically, DAN and SWD are discrepancy-based methods, while DANN, ADDA, PADA, and MDD are adversarial-based methods. For baseline comparison, we also directly train a network with the same structure as the encoder and classifier in GFCA using only ResNet-50 [14] features from the source domain (ResNet-50).\n\nFor the Office31 dataset, 10 classes are selected as few-shot classes. For the Office-Home dataset, 20 classes are selected as few-shot classes. Each few-shot class consists of 3 samples randomly sampled from the original dataset. In all our experiments, source domain data are first randomly over-sampled to create a balanced training set. For fair comparisons across different methods, ResNet-50 [14] pre-trained on the ImageNet dataset and fine-tuned using the source domain training samples is utilized as the backbone of the compared deep DA methods. We adopt the top-1 classification accuracy for the unlabeled target samples as the evaluation metric. The results on the Office31 dataset and the Office-Home dataset are shown in Table 1 and Table 2, respectively. From Tables 1 and 2, we observe that all compared methods achieves similar classification accuracy on the normal set, which is much higher than the baseline. This indicates that DA is necessary for effective knowledge transfer across domains with different distributions. Specifically, MDD obtains the highest normal set classification accuracy on the Office-Home dataset, which demonstrates the effectiveness of its theoretical analysis guided design. However, the proposed GFCA algorithm performs the best in normal set classification on the Office31 dataset, which represents that effective data augmentation through generative models could also benefit the learning of normal classes.\n\nFor few-shot classification, we observe that the proposed GFCA algorithm makes remarkable performance improvement in all DA tasks comparing with existing DA methods on both datasets. This demonstrates that our generative few-shot cross-domain alignment approach could indeed boost few-shot classification performance and fair classification among normal and few-shot classes. Existing methods generally perform better in few-shot classification when the size of the source dataset is small (e.g. D\u2192W and W\u2192D in Office31) since the imbalance between few-shot and normal set is less compelling. Surprisingly, the classical discrepancy-based DAN method achieves better performance in fewshot classification comparing with other existing methods. However, we also notice that DAN tends to obtain lower accuracy for normal set classification due to less cross-domain adaptation ability. Under more difficult DA tasks in the Office-Home dataset, the classification performance improvement produced by GFCA becomes much more significant. This demonstrates the necessity of generative data augmentation under complex DA problems with extremely imbalanced source data. Moreover, GFCA consistently achieves the highest overall classification accuracy comparing to existing DA approaches. Table 2: Accuracy (%) of few-shot classification for DA tasks in the Office-Home dataset, where Ar = Art, Cl = Clipart, Pr = Product, and Rw = Real-world. Avg l, Avg n, and Avg denote the average accuracy for few-shot classes, normal classes, and unlabeled target samples, respectively. \n\n\nEmpirical Evaluation\n\nTo demonstrate that our generative data augmentation model could indeed synthesize effective training data and promote fair classification across both normal and few-shot set, we utilize t-SNE [19] to visualize generated fake source data, along with source data in the training set and unused few-shot source data from the original full dataset. Figure 2a shows the few-shot t-SNE visualization for A\u2192D task in the Office31 dataset. We observe that synthetic few-shot data generally conform with the unused source samples in the full dataset for both few-shot and normal classes. Specifically, different classes form distinct clusters and the synthetic data generally spread around the center of each cluster. For few-shot classes, the generated data also expands the intra-class variation comparing with the few-shot training samples. We further evaluate the quality of the generated data using silhouette scores with clusters defined by class labels and distance calculated using cosine distance (Figure 2b). We observe that when combined with the synthetic fake data, silhouette scores of the original full dataset and the training source data both increases. Such observation is also consistent when all samples or only few-shot samples are considered. This indicates that the synthetic data could enhance the clustering structure of the data for normal and few-shot classes, which could benefit the following classification task. Moreover, from Figure 2a, we also notice that in cases where the sampled few-shot training data is far from the class centers estimated by the original full datasets, GFCA is able to generate data closer to the class centers. To further confirm this observation, we evaluate the cosine similarity between class centers of generated few-shot data and unused few-shot data, and compare it with the cosine similarity between class centers of sampled few-shot training data and un- Silhouette scores comparing data with or without synthetic fake data with clusters defined by class labels. \"All\" represents silhouette scores calculated using all samples and \"few-shot\" represents silhouette scores calculated with few-shot samples. (c) Cosine similarity of class centers between few-shot source training data (blue) or synthetic fake source data (red) and the unused few-shot source data from the original full dataset.\n\nused few-shot data (Figure 2c). We observe that for 7 out of 10 few-shot classes in A\u2192D task in the Office31 dataset, the class centers of the synthetic fake data obtain higher similarity with the few-shot data in the original full dataset, which demonstrates that the generator could acquire a better estimation of class centers through adversarial training. We further investigate the reason for few-shot classification improvement achieved by our method. The weight matrix W c in classifier C(\u00b7) consists of a weight vector w k for each class k. Guo et al. revealed that there exists a close connection between the norm of the weight vector and the volume of the corresponding class partition in the feature space [11]. Therefore, we calculate the norm of each class weight vector after training completed to evaluate whether the few-shot classes have been promoted with similar importance as the normal classes by the classifier. Figure 3 shows the average L2-norms of normal and few-shot class weight vectors in GFCA and DANN under different DA tasks in the Office31 dataset. DANN is trained with the same classifier structure as GFCA for fair comparison. We notice for tasks with more imbalanced training set (e.g. A\u2192W, A\u2192D), weight norms of few-shot classes produced by DANN are significantly lower than norms of the normal set, which further verifies the performance deterioration of DANN under tasks with more imbalanced training data. By introducing the fair classification term, GFCA yields similar weight norms for both normal and few-shot sets, which conforms with the superior performance of GFCA in few-shot set, normal set, and overall classification. We also evaluate the performance of GFCA under different few-shot settings. Specifically, two factors are investigated: number of few shot classes and number of samples per few-shot class. Firstly, we fix the number of samples per few-shot class to 3 and change the number of few-shot classes from 5 to 15 ( Figure 4a). Secondly, we fix the number few-shot class to 10 and change the number of samples per few-shot classes from 1 to 5 (Figure 4b). For meaningful and consistent comparison, we choose the same set of few-shot samples for the same few-shot class across different experiments when exploring the influence of a factor. For example, in the experiments with 5 and 9 few-shot classes, the shared 5 fewshot classes across these two experiments contain the same set of samples. The few-shot cross-domain learning problem becomes more difficult as the number of few-shot class increases or the number of samples per few-shot class decreases. We also train DANN using the same data for comparison. From Figure 4, we observe that GFCA consistently achieves remarkable improvement in both fewshot and overall classification under all few-shot settings. Specifically, when the number of few-shot classes increases, the overall classification accuracy in both methods gradually decrease. However, we observe fluctuation in few-shot classification accuracy under different number of few-shot classes, which could relate to the random sampling of few-shot training samples, as well as the variation in difficulty of classifying different categories. For DANN, the decrease in the number of normal classes potentially makes the model more difficult to learn the normal set while probably more generalizable to the few-shot set. When the number of samples per few-shot class increases, we observe that the performance of both methods increases, since larger samples per few-shot classes represents less imbalanced dataset and easier few-shot cross-domain learning problem.\n\n\nAblation Study\n\nWe compare GFCA with its two variants. The first variant is GFCA-2Stage, where data augmentation and domain adaptation in GFCA are performed in two stages. In GFCA-2Stage, the generator and discriminator are first trained using the labeled source domain data. Then, the generator is utilized to synthesize fake data for source domain data augmentation using balanced class labels. Finally, the encoder and classifier are trained using the augmented source data and target data. The second variant is GFCA-WoFC, where the FC regularization term on the classifier is removed. From Table 1 and 2, we observe that all variants of the GFCA algorithm achieve significant improvement in few-shot and overall classification under all DA tasks comparing to existing methods. Specifically, on average, GFCA performs better in few-shot and overall classification than GFCA-2Stage, which demonstrates the advantage of end-to-end training of our proposed model. GFCA-WoFC yields the highest accuracy for few-shot classification in the Office31 dataset. However, it also obtains lower normal set and overall classification accuracy comparing to GFCA. From Figure 3b, we observe that classifiers in GFCA-WoFC have larger norms of few-shot class weight vectors than norms of normal class weight vectors. This indicates that generative data augmentation can synthesize data with enough intra-class variation and expand the volume of few-shot classes in the feature space. Although generative data augmentation alone could significantly improves few-shot classification, regularization with FC term is still needed to maintain the performance in normal set classification and the classification fairness across both few-shot and normal classes. Moreover, in the Office-Home dataset, we observe that GFCA achieves higher accuracy in both few-shot and overall classification comparing to GFCA-WoFC, which demonstrates that FC term is still needed for few-shot promotion and fair classification under more difficult DA tasks.\n\n\nConclusion\n\nIn this paper, we propose a novel GFCA algorithm to solve the few-shot crossdomain adaptation problem for fair classification. We utilize generative data augmentation to synthesize effective source training data for few-shot classes and alleviate the bias in favor of the normal classes in the training set, where we train a conditional generative adversarial network to capture the intra-class variation of the normal classes. We then leverage effective domain alignment to adapt knowledge from the source domain to the target domain, where MMD-based regularization is utilized to learn transferable representations of training samples using real source data, fake synthetic source data, and target data. A general classifier is further trained with a FC regularization term to balance between the underrepresented few-shot classes and normal classes for fair classification. Experiments on two cross-domain benchmark datasets demonstrate that our method could significantly improve the performance for both few-shot and overall classification comparing to state-of-the-art DA methods.\n\nFig. 1 :\n1Illustration of the few-shot cross-domain adaptation problem with GFCA.\n\nFig. 2 :\n2Visualization of A\u2192D task in the Office31 dataset. (a) T-SNE visualization with synthetic fake data (red), sampled real few-shot source training data (green), and source data from the original full dataset (blue). (b)\n\nFig. 3 :\n3Average of L2-norms of the classifier weight vectors for normal classes (orange) and few shot classes (green) in different DA tasks of the Office31 dataset.\n\nFig. 4 :\n4Few-shot and overall classification accuracy of GFCA and DANN under different few-shot settings in the Office31 dataset. (a) Classification accuracy with respect to the number of few-shot classes. (b) Classification accuracy with respect to the number of samples per few-shot class.\n\nTable 1 :\n1Accuracy (%) of few-shot classification for DA tasks in the Office31 dataset, where A = Amazon, D = DSLR, and W = Webcam. Avg l, Avg n, and Avg denote the average classification accuracy for few-shot classes, normal classes, and all unlabeled target samples, respectively.Methods \nA\u2192W \nD\u2192W \nW\u2192D \nA\u2192D \nD\u2192A \nW\u2192A Avg l Avg n Avg \n\nResNet-50 \n19.3\u00b10.2 57.9\u00b10.5 51.2\u00b10.3 22.6\u00b10.3 23.8\u00b10.7 24.4\u00b10.3 33.2 73.5 60.1 \n\nDAN \n63.5\u00b10.1 71.7\u00b10.4 86.6\u00b10.1 67.4\u00b10.2 47.9\u00b10.1 51.3\u00b10.1 64.7 84.3 77.9 \n\nDANN \n37.8\u00b10.2 79.8\u00b10.1 85.4\u00b10.2 44.9\u00b10.4 38.6\u00b10.0 41.1\u00b10.1 54.6 86.0 75.6 \n\nADDA \n35.8\u00b10.1 65.5\u00b10.2 79.9\u00b10.1 37.7\u00b10.3 37.6\u00b10.1 40.0\u00b10.0 49.4 85.6 73.7 \n\nPADA \n19.7\u00b10.0 65.0\u00b10.3 74.4\u00b10.2 26.3\u00b10.4 33.4\u00b10.2 33.5\u00b10.2 42.0 83.8 70.0 \n\nMDD \n32.0\u00b10.3 75.5\u00b10.2 83.1\u00b10.3 39.0\u00b10.3 35.3\u00b10.3 39.1\u00b10.2 50.6 86.5 74.6 \n\nSWD \n25.5\u00b10.1 70.0\u00b10.2 77.6\u00b10.0 34.4\u00b10.2 34.1\u00b10.1 36.4\u00b10.1 46.3 83.6 71.3 \n\nGFCA-2Stage 71.3\u00b10.0 81.2\u00b10.1 90.5\u00b10.2 73.4\u00b10.6 46.3\u00b10.0 50.3\u00b10.0 68.8 86.6 80.8 \n\nGFCA-WoFC 71.6\u00b10.1 80.1\u00b10.2 90.7\u00b10.2 74.6\u00b10.9 53.0\u00b10.0 55.7\u00b10.0 71.0 86.7 81.6 \n\nGFCA \n71.3\u00b10.1 80.3\u00b10.2 90.5\u00b10.2 74.9\u00b10.8 52.8\u00b10.0 55.8\u00b10.0 70.9 86.9 81.7 \n\n\n\nDeep over-sampling framework for classifying imbalanced data. S Ando, C Y Huang, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerAndo, S., Huang, C.Y.: Deep over-sampling framework for classifying imbalanced data. In: Joint European Conference on Machine Learning and Knowledge Discov- ery in Databases. pp. 770-785. Springer (2017)\n\nA Antoniou, A Storkey, H Edwards, arXiv:1711.04340Data augmentation generative adversarial networks. arXiv preprintAntoniou, A., Storkey, A., Edwards, H.: Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340 (2017)\n\nPartial adversarial domain adaptation. Z Cao, L Ma, M Long, J Wang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Cao, Z., Ma, L., Long, M., Wang, J.: Partial adversarial domain adaptation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 135- 150 (2018)\n\nThe frontiers of fairness in machine learning. A Chouldechova, A Roth, arXiv:1810.08810arXiv preprintChouldechova, A., Roth, A.: The frontiers of fairness in machine learning. arXiv preprint arXiv:1810.08810 (2018)\n\nT Devries, G W Taylor, arXiv:1702.05538Dataset augmentation in feature space. arXiv preprintDeVries, T., Taylor, G.W.: Dataset augmentation in feature space. arXiv preprint arXiv:1702.05538 (2017)\n\nDiversity with cooperation: Ensemble methods for few-shot classification. N Dvornik, C Schmid, J Mairal, arXiv:1903.11341arXiv preprintDvornik, N., Schmid, C., Mairal, J.: Diversity with cooperation: Ensemble methods for few-shot classification. arXiv preprint arXiv:1903.11341 (2019)\n\nY Ganin, V Lempitsky, arXiv:1409.7495Unsupervised domain adaptation by backpropagation. arXiv preprintGanin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation. arXiv preprint arXiv:1409.7495 (2014)\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEEGong, B., Shi, Y., Sha, F., Grauman, K.: Geodesic flow kernel for unsupervised domain adaptation. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 2066-2073. IEEE (2012)\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural information processing systems. pp. 2672-2680 (2014)\n\nOptimal kernel choice for large-scale two-sample tests. A Gretton, D Sejdinovic, H Strathmann, S Balakrishnan, M Pontil, K Fukumizu, B K Sriperumbudur, Advances in neural information processing systems. Gretton, A., Sejdinovic, D., Strathmann, H., Balakrishnan, S., Pontil, M., Fuku- mizu, K., Sriperumbudur, B.K.: Optimal kernel choice for large-scale two-sample tests. In: Advances in neural information processing systems. pp. 1205-1213 (2012)\n\nOne-shot face recognition by promoting underrepresented classes. Y Guo, L Zhang, arXiv:1707.05574arXiv preprintGuo, Y., Zhang, L.: One-shot face recognition by promoting underrepresented classes. arXiv preprint arXiv:1707.05574 (2017)\n\nLow-shot visual recognition by shrinking and hallucinating features. B Hariharan, R Girshick, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionHariharan, B., Girshick, R.: Low-shot visual recognition by shrinking and halluci- nating features. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 3018-3027 (2017)\n\nDreaming more data: Class-dependent distributions over diffeomorphisms for learned data augmentation. S Hauberg, O Freifeld, A B L Larsen, J Fisher, L Hansen, Artificial Intelligence and Statistics. Hauberg, S., Freifeld, O., Larsen, A.B.L., Fisher, J., Hansen, L.: Dreaming more data: Class-dependent distributions over diffeomorphisms for learned data aug- mentation. In: Artificial Intelligence and Statistics. pp. 342-350 (2016)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n\nSliced wasserstein discrepancy for unsupervised domain adaptation. C Y Lee, T Batra, M H Baig, D Ulbricht, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLee, C.Y., Batra, T., Baig, M.H., Ulbricht, D.: Sliced wasserstein discrepancy for unsupervised domain adaptation. In: Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition. pp. 10285-10295 (2019)\n\nCoupled generative adversarial networks. M Y Liu, O Tuzel, Advances in neural information processing systems. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: Advances in neural information processing systems. pp. 469-477 (2016)\n\nM Long, Y Cao, J Wang, M I Jordan, arXiv:1502.02791Learning transferable features with deep adaptation networks. arXiv preprintLong, M., Cao, Y., Wang, J., Jordan, M.I.: Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791 (2015)\n\nDeep transfer learning with joint adaptation networks. M Long, H Zhu, J Wang, M I Jordan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Long, M., Zhu, H., Wang, J., Jordan, M.I.: Deep transfer learning with joint adap- tation networks. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 2208-2217. JMLR. org (2017)\n\nVisualizing data using t-sne. L V D Maaten, G Hinton, Journal of machine learning research. 9Maaten, L.v.d., Hinton, G.: Visualizing data using t-sne. Journal of machine learn- ing research 9(Nov), 2579-2605 (2008)\n\nG Mariani, F Scheidegger, R Istrate, C Bekas, C Malossi, arXiv:1803.09655Data augmentation with balancing gan. BaganarXiv preprintMariani, G., Scheidegger, F., Istrate, R., Bekas, C., Malossi, C.: Bagan: Data aug- mentation with balancing gan. arXiv preprint arXiv:1803.09655 (2018)\n\nUnsupervised domain adaptation with imbalanced crossdomain data. Ming Harry Hsu, T Yu Chen, W Hou, C A Hubert Tsai, Y H Yeh, Y R Frank Wang, Y C , Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionMing Harry Hsu, T., Yu Chen, W., Hou, C.A., Hubert Tsai, Y.H., Yeh, Y.R., Frank Wang, Y.C.: Unsupervised domain adaptation with imbalanced cross- domain data. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 4121-4129 (2015)\n\nM Mirza, S Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintMirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 (2014)\n\nConditional image synthesis with auxiliary classifier gans. A Odena, C Olah, J Shlens, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningJMLR. org70Odena, A., Olah, C., Shlens, J.: Conditional image synthesis with auxiliary classifier gans. In: Proceedings of the 34th International Conference on Machine Learning- Volume 70. pp. 2642-2651. JMLR. org (2017)\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 2210Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22(10), 1345-1359 (2009)\n\nFew-shot image recognition with knowledge transfer. Z Peng, Z Li, J Zhang, Y Li, G J Qi, J Tang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionPeng, Z., Li, Z., Zhang, J., Li, Y., Qi, G.J., Tang, J.: Few-shot image recognition with knowledge transfer. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 441-449 (2019)\n\nDelta-encoder: an effective sample synthesis method for few-shot object recognition. E Schwartz, L Karlinsky, J Shtok, S Harary, M Marder, A Kumar, R Feris, R Giryes, A Bronstein, Advances in Neural Information Processing Systems. Schwartz, E., Karlinsky, L., Shtok, J., Harary, S., Marder, M., Kumar, A., Feris, R., Giryes, R., Bronstein, A.: Delta-encoder: an effective sample synthesis method for few-shot object recognition. In: Advances in Neural Information Processing Systems. pp. 2845-2855 (2018)\n\nEquivalence of distance-based and rkhs-based statistics in hypothesis testing. D Sejdinovic, B Sriperumbudur, A Gretton, K Fukumizu, The Annals of Statistics. 415Sejdinovic, D., Sriperumbudur, B., Gretton, A., Fukumizu, K., et al.: Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The Annals of Statistics 41(5), 2263-2291 (2013)\n\nReturn of frustratingly easy domain adaptation. B Sun, J Feng, K Saenko, Thirtieth AAAI Conference on Artificial Intelligence. Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation. In: Thirtieth AAAI Conference on Artificial Intelligence (2016)\n\nDeep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, European Conference on Computer Vision. SpringerSun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain adaptation. In: European Conference on Computer Vision. pp. 443-450. Springer (2016)\n\nLearning compositional representations for few-shot recognition. P Tokmakov, Y X Wang, M Hebert, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionTokmakov, P., Wang, Y.X., Hebert, M.: Learning compositional representations for few-shot recognition. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 6372-6381 (2019)\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition. pp. 7167-7176 (2017)\n\nE Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv:1412.3474Deep domain confusion: Maximizing for domain invariance. arXiv preprintTzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474 (2014)\n\nDeep hashing network for unsupervised domain adaptation. H Venkateswara, J Eusebio, S Chakraborty, S Panchanathan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionVenkateswara, H., Eusebio, J., Chakraborty, S., Panchanathan, S.: Deep hashing network for unsupervised domain adaptation. In: Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition. pp. 5018-5027 (2017)\n\nAdversarial feature augmentation for unsupervised domain adaptation. R Volpi, P Morerio, S Savarese, V Murino, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionVolpi, R., Morerio, P., Savarese, S., Murino, V.: Adversarial feature augmentation for unsupervised domain adaptation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5495-5504 (2018)\n\nBalanced distribution adaptation for transfer learning. J Wang, Y Chen, S Hao, W Feng, Z Shen, 2017 IEEE International Conference on Data Mining (ICDM). IEEEWang, J., Chen, Y., Hao, S., Feng, W., Shen, Z.: Balanced distribution adaptation for transfer learning. In: 2017 IEEE International Conference on Data Mining (ICDM). pp. 1129-1134. IEEE (2017)\n\nDeep visual domain adaptation: A survey. M Wang, W Deng, Neurocomputing. 312Wang, M., Deng, W.: Deep visual domain adaptation: A survey. Neurocomputing 312, 135-153 (2018)\n\nLow-shot learning from imaginary data. Y X Wang, R Girshick, M Hebert, B Hariharan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang, Y.X., Girshick, R., Hebert, M., Hariharan, B.: Low-shot learning from imag- inary data. In: Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition. pp. 7278-7286 (2018)\n\nVariational few-shot learning. J Zhang, C Zhao, B Ni, M Xu, X Yang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhang, J., Zhao, C., Ni, B., Xu, M., Yang, X.: Variational few-shot learning. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1685- 1694 (2019)\n\nMetagan: An adversarial approach to few-shot learning. R Zhang, T Che, Z Ghahramani, Y Bengio, Y Song, Advances in Neural Information Processing Systems. Zhang, R., Che, T., Ghahramani, Z., Bengio, Y., Song, Y.: Metagan: An adversar- ial approach to few-shot learning. In: Advances in Neural Information Processing Systems. pp. 2365-2374 (2018)\n\nBridging theory and algorithm for domain adaptation. Y Zhang, T Liu, M Long, M Jordan, International Conference on Machine Learning. Zhang, Y., Liu, T., Long, M., Jordan, M.: Bridging theory and algorithm for do- main adaptation. In: International Conference on Machine Learning. pp. 7404-7413 (2019)\n\nDeep unsupervised convolutional domain adaptation. J Zhuo, S Wang, W Zhang, Q Huang, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaACMZhuo, J., Wang, S., Zhang, W., Huang, Q.: Deep unsupervised convolutional do- main adaptation. In: Proceedings of the 25th ACM international conference on Multimedia. pp. 261-269. ACM (2017)\n", "annotations": {"author": "[{\"end\":144,\"start\":64},{\"end\":233,\"start\":145},{\"end\":324,\"start\":234},{\"end\":422,\"start\":325},{\"end\":542,\"start\":423}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":72},{\"end\":159,\"start\":155},{\"end\":242,\"start\":238},{\"end\":335,\"start\":331},{\"end\":432,\"start\":427}]", "author_first_name": "[{\"end\":71,\"start\":64},{\"end\":154,\"start\":145},{\"end\":237,\"start\":234},{\"end\":330,\"start\":325},{\"end\":426,\"start\":423}]", "author_affiliation": "[{\"end\":143,\"start\":78},{\"end\":232,\"start\":161},{\"end\":323,\"start\":259},{\"end\":421,\"start\":356},{\"end\":514,\"start\":450},{\"end\":541,\"start\":516}]", "title": "[{\"end\":61,\"start\":1},{\"end\":603,\"start\":543}]", "venue": null, "abstract": "[{\"end\":1898,\"start\":656}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2320,\"start\":2316},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2323,\"start\":2320},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3377,\"start\":3374},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3507,\"start\":3503},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3656,\"start\":3652},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3659,\"start\":3656},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3703,\"start\":3699},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3706,\"start\":3703},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3709,\"start\":3706},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3785,\"start\":3782},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3915,\"start\":3912},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3918,\"start\":3915},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3921,\"start\":3918},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4522,\"start\":4519},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4658,\"start\":4655},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4661,\"start\":4658},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4664,\"start\":4661},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4667,\"start\":4664},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4841,\"start\":4837},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4844,\"start\":4841},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4847,\"start\":4844},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4850,\"start\":4847},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4884,\"start\":4881},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4887,\"start\":4884},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5038,\"start\":5035},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5041,\"start\":5038},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5173,\"start\":5170},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5176,\"start\":5173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5179,\"start\":5176},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5233,\"start\":5230},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5284,\"start\":5280},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5342,\"start\":5338},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7484,\"start\":7480},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7650,\"start\":7646},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7848,\"start\":7845},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7969,\"start\":7966},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7971,\"start\":7969},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7974,\"start\":7971},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7977,\"start\":7974},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7980,\"start\":7977},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8132,\"start\":8129},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8312,\"start\":8308},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9107,\"start\":9103},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9110,\"start\":9107},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9113,\"start\":9110},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9116,\"start\":9113},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9119,\"start\":9116},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9161,\"start\":9157},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9408,\"start\":9404},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9566,\"start\":9562},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9761,\"start\":9758},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9909,\"start\":9906},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10088,\"start\":10084},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10274,\"start\":10271},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10450,\"start\":10446},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13168,\"start\":13164},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15082,\"start\":15079},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15271,\"start\":15267},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18308,\"start\":18304},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18489,\"start\":18485},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18492,\"start\":18489},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18495,\"start\":18492},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19116,\"start\":19112},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19156,\"start\":19152},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20521,\"start\":20517},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21733,\"start\":21729},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22994,\"start\":22991},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23370,\"start\":23366},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23866,\"start\":23862},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23876,\"start\":23873},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23887,\"start\":23883},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23897,\"start\":23894},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23907,\"start\":23903},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23921,\"start\":23917},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24188,\"start\":24184},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24637,\"start\":24633},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27482,\"start\":27478},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30358,\"start\":30354}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36482,\"start\":36400},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36711,\"start\":36483},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36879,\"start\":36712},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37173,\"start\":36880},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38296,\"start\":37174}]", "paragraph": "[{\"end\":3378,\"start\":1914},{\"end\":4138,\"start\":3380},{\"end\":5486,\"start\":4140},{\"end\":6103,\"start\":5488},{\"end\":6823,\"start\":6105},{\"end\":7012,\"start\":6825},{\"end\":7225,\"start\":7029},{\"end\":8752,\"start\":7247},{\"end\":10780,\"start\":8787},{\"end\":11447,\"start\":10836},{\"end\":11709,\"start\":11449},{\"end\":12571,\"start\":11816},{\"end\":12730,\"start\":12573},{\"end\":14213,\"start\":12732},{\"end\":14954,\"start\":14259},{\"end\":15587,\"start\":14956},{\"end\":15682,\"start\":15589},{\"end\":15883,\"start\":15773},{\"end\":17206,\"start\":15885},{\"end\":17415,\"start\":17302},{\"end\":17798,\"start\":17446},{\"end\":18222,\"start\":17857},{\"end\":18803,\"start\":18273},{\"end\":19488,\"start\":18877},{\"end\":19969,\"start\":19542},{\"end\":20329,\"start\":20117},{\"end\":20682,\"start\":20331},{\"end\":20893,\"start\":20723},{\"end\":21311,\"start\":20924},{\"end\":21414,\"start\":21313},{\"end\":21635,\"start\":21446},{\"end\":22955,\"start\":21662},{\"end\":23744,\"start\":22982},{\"end\":24233,\"start\":23771},{\"end\":25693,\"start\":24235},{\"end\":27260,\"start\":25695},{\"end\":29635,\"start\":27285},{\"end\":33275,\"start\":29637},{\"end\":35298,\"start\":33294},{\"end\":36399,\"start\":35313}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11815,\"start\":11710},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15772,\"start\":15683},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17301,\"start\":17207},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17445,\"start\":17416},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17819,\"start\":17799},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17856,\"start\":17819},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18876,\"start\":18804},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19541,\"start\":19489},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20116,\"start\":19970},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20722,\"start\":20683},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20923,\"start\":20894},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21445,\"start\":21415}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24989,\"start\":24970},{\"end\":26980,\"start\":26973},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33880,\"start\":33873}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1912,\"start\":1900},{\"attributes\":{\"n\":\"2\"},\"end\":7027,\"start\":7015},{\"attributes\":{\"n\":\"2.1\"},\"end\":7245,\"start\":7228},{\"attributes\":{\"n\":\"2.2\"},\"end\":8785,\"start\":8755},{\"attributes\":{\"n\":\"3\"},\"end\":10805,\"start\":10783},{\"attributes\":{\"n\":\"3.1\"},\"end\":10834,\"start\":10808},{\"attributes\":{\"n\":\"3.2\"},\"end\":14257,\"start\":14216},{\"end\":18271,\"start\":18225},{\"end\":21660,\"start\":21638},{\"attributes\":{\"n\":\"4\"},\"end\":22969,\"start\":22958},{\"attributes\":{\"n\":\"4.1\"},\"end\":22980,\"start\":22972},{\"attributes\":{\"n\":\"4.2\"},\"end\":23769,\"start\":23747},{\"attributes\":{\"n\":\"4.3\"},\"end\":27283,\"start\":27263},{\"attributes\":{\"n\":\"4.4\"},\"end\":33292,\"start\":33278},{\"attributes\":{\"n\":\"5\"},\"end\":35311,\"start\":35301},{\"end\":36409,\"start\":36401},{\"end\":36492,\"start\":36484},{\"end\":36721,\"start\":36713},{\"end\":36889,\"start\":36881},{\"end\":37184,\"start\":37175}]", "table": "[{\"end\":38296,\"start\":37458}]", "figure_caption": "[{\"end\":36482,\"start\":36411},{\"end\":36711,\"start\":36494},{\"end\":36879,\"start\":36723},{\"end\":37173,\"start\":36891},{\"end\":37458,\"start\":37186}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14372,\"start\":14364},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27640,\"start\":27631},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28293,\"start\":28283},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28744,\"start\":28735},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29666,\"start\":29656},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30579,\"start\":30571},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31622,\"start\":31613},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31750,\"start\":31740},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32322,\"start\":32314},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34445,\"start\":34436}]", "bib_author_first_name": "[{\"end\":38361,\"start\":38360},{\"end\":38369,\"start\":38368},{\"end\":38371,\"start\":38370},{\"end\":38677,\"start\":38676},{\"end\":38689,\"start\":38688},{\"end\":38700,\"start\":38699},{\"end\":38962,\"start\":38961},{\"end\":38969,\"start\":38968},{\"end\":38975,\"start\":38974},{\"end\":38983,\"start\":38982},{\"end\":39320,\"start\":39319},{\"end\":39336,\"start\":39335},{\"end\":39489,\"start\":39488},{\"end\":39500,\"start\":39499},{\"end\":39502,\"start\":39501},{\"end\":39761,\"start\":39760},{\"end\":39772,\"start\":39771},{\"end\":39782,\"start\":39781},{\"end\":39973,\"start\":39972},{\"end\":39982,\"start\":39981},{\"end\":40248,\"start\":40247},{\"end\":40256,\"start\":40255},{\"end\":40263,\"start\":40262},{\"end\":40270,\"start\":40269},{\"end\":40574,\"start\":40573},{\"end\":40588,\"start\":40587},{\"end\":40605,\"start\":40604},{\"end\":40614,\"start\":40613},{\"end\":40620,\"start\":40619},{\"end\":40636,\"start\":40635},{\"end\":40645,\"start\":40644},{\"end\":40658,\"start\":40657},{\"end\":40991,\"start\":40990},{\"end\":41002,\"start\":41001},{\"end\":41016,\"start\":41015},{\"end\":41030,\"start\":41029},{\"end\":41046,\"start\":41045},{\"end\":41056,\"start\":41055},{\"end\":41068,\"start\":41067},{\"end\":41070,\"start\":41069},{\"end\":41448,\"start\":41447},{\"end\":41455,\"start\":41454},{\"end\":41688,\"start\":41687},{\"end\":41701,\"start\":41700},{\"end\":42131,\"start\":42130},{\"end\":42142,\"start\":42141},{\"end\":42154,\"start\":42153},{\"end\":42158,\"start\":42155},{\"end\":42168,\"start\":42167},{\"end\":42178,\"start\":42177},{\"end\":42509,\"start\":42508},{\"end\":42515,\"start\":42514},{\"end\":42524,\"start\":42523},{\"end\":42531,\"start\":42530},{\"end\":42932,\"start\":42931},{\"end\":42934,\"start\":42933},{\"end\":42941,\"start\":42940},{\"end\":42950,\"start\":42949},{\"end\":42952,\"start\":42951},{\"end\":42960,\"start\":42959},{\"end\":43378,\"start\":43377},{\"end\":43380,\"start\":43379},{\"end\":43387,\"start\":43386},{\"end\":43585,\"start\":43584},{\"end\":43593,\"start\":43592},{\"end\":43600,\"start\":43599},{\"end\":43608,\"start\":43607},{\"end\":43610,\"start\":43609},{\"end\":43912,\"start\":43911},{\"end\":43920,\"start\":43919},{\"end\":43927,\"start\":43926},{\"end\":43935,\"start\":43934},{\"end\":43937,\"start\":43936},{\"end\":44319,\"start\":44318},{\"end\":44323,\"start\":44320},{\"end\":44333,\"start\":44332},{\"end\":44505,\"start\":44504},{\"end\":44516,\"start\":44515},{\"end\":44531,\"start\":44530},{\"end\":44542,\"start\":44541},{\"end\":44551,\"start\":44550},{\"end\":44857,\"start\":44853},{\"end\":44863,\"start\":44858},{\"end\":44870,\"start\":44869},{\"end\":44881,\"start\":44880},{\"end\":44888,\"start\":44887},{\"end\":44890,\"start\":44889},{\"end\":44905,\"start\":44904},{\"end\":44907,\"start\":44906},{\"end\":44914,\"start\":44913},{\"end\":44916,\"start\":44915},{\"end\":44930,\"start\":44929},{\"end\":44932,\"start\":44931},{\"end\":45311,\"start\":45310},{\"end\":45320,\"start\":45319},{\"end\":45567,\"start\":45566},{\"end\":45576,\"start\":45575},{\"end\":45584,\"start\":45583},{\"end\":45970,\"start\":45969},{\"end\":45972,\"start\":45971},{\"end\":45979,\"start\":45978},{\"end\":46226,\"start\":46225},{\"end\":46234,\"start\":46233},{\"end\":46240,\"start\":46239},{\"end\":46249,\"start\":46248},{\"end\":46255,\"start\":46254},{\"end\":46257,\"start\":46256},{\"end\":46263,\"start\":46262},{\"end\":46679,\"start\":46678},{\"end\":46691,\"start\":46690},{\"end\":46704,\"start\":46703},{\"end\":46713,\"start\":46712},{\"end\":46723,\"start\":46722},{\"end\":46733,\"start\":46732},{\"end\":46742,\"start\":46741},{\"end\":46751,\"start\":46750},{\"end\":46761,\"start\":46760},{\"end\":47179,\"start\":47178},{\"end\":47193,\"start\":47192},{\"end\":47210,\"start\":47209},{\"end\":47221,\"start\":47220},{\"end\":47509,\"start\":47508},{\"end\":47516,\"start\":47515},{\"end\":47524,\"start\":47523},{\"end\":47794,\"start\":47793},{\"end\":47801,\"start\":47800},{\"end\":48081,\"start\":48080},{\"end\":48093,\"start\":48092},{\"end\":48095,\"start\":48094},{\"end\":48103,\"start\":48102},{\"end\":48478,\"start\":48477},{\"end\":48487,\"start\":48486},{\"end\":48498,\"start\":48497},{\"end\":48508,\"start\":48507},{\"end\":48862,\"start\":48861},{\"end\":48871,\"start\":48870},{\"end\":48882,\"start\":48881},{\"end\":48891,\"start\":48890},{\"end\":48901,\"start\":48900},{\"end\":49211,\"start\":49210},{\"end\":49227,\"start\":49226},{\"end\":49238,\"start\":49237},{\"end\":49253,\"start\":49252},{\"end\":49709,\"start\":49708},{\"end\":49718,\"start\":49717},{\"end\":49729,\"start\":49728},{\"end\":49741,\"start\":49740},{\"end\":50172,\"start\":50171},{\"end\":50180,\"start\":50179},{\"end\":50188,\"start\":50187},{\"end\":50195,\"start\":50194},{\"end\":50203,\"start\":50202},{\"end\":50509,\"start\":50508},{\"end\":50517,\"start\":50516},{\"end\":50680,\"start\":50679},{\"end\":50682,\"start\":50681},{\"end\":50690,\"start\":50689},{\"end\":50702,\"start\":50701},{\"end\":50712,\"start\":50711},{\"end\":51098,\"start\":51097},{\"end\":51107,\"start\":51106},{\"end\":51115,\"start\":51114},{\"end\":51121,\"start\":51120},{\"end\":51127,\"start\":51126},{\"end\":51485,\"start\":51484},{\"end\":51494,\"start\":51493},{\"end\":51501,\"start\":51500},{\"end\":51515,\"start\":51514},{\"end\":51525,\"start\":51524},{\"end\":51829,\"start\":51828},{\"end\":51838,\"start\":51837},{\"end\":51845,\"start\":51844},{\"end\":51853,\"start\":51852},{\"end\":52129,\"start\":52128},{\"end\":52137,\"start\":52136},{\"end\":52145,\"start\":52144},{\"end\":52154,\"start\":52153}]", "bib_author_last_name": "[{\"end\":38366,\"start\":38362},{\"end\":38377,\"start\":38372},{\"end\":38686,\"start\":38678},{\"end\":38697,\"start\":38690},{\"end\":38708,\"start\":38701},{\"end\":38966,\"start\":38963},{\"end\":38972,\"start\":38970},{\"end\":38980,\"start\":38976},{\"end\":38988,\"start\":38984},{\"end\":39333,\"start\":39321},{\"end\":39341,\"start\":39337},{\"end\":39497,\"start\":39490},{\"end\":39509,\"start\":39503},{\"end\":39769,\"start\":39762},{\"end\":39779,\"start\":39773},{\"end\":39789,\"start\":39783},{\"end\":39979,\"start\":39974},{\"end\":39992,\"start\":39983},{\"end\":40253,\"start\":40249},{\"end\":40260,\"start\":40257},{\"end\":40267,\"start\":40264},{\"end\":40278,\"start\":40271},{\"end\":40585,\"start\":40575},{\"end\":40602,\"start\":40589},{\"end\":40611,\"start\":40606},{\"end\":40617,\"start\":40615},{\"end\":40633,\"start\":40621},{\"end\":40642,\"start\":40637},{\"end\":40655,\"start\":40646},{\"end\":40665,\"start\":40659},{\"end\":40999,\"start\":40992},{\"end\":41013,\"start\":41003},{\"end\":41027,\"start\":41017},{\"end\":41043,\"start\":41031},{\"end\":41053,\"start\":41047},{\"end\":41065,\"start\":41057},{\"end\":41084,\"start\":41071},{\"end\":41452,\"start\":41449},{\"end\":41461,\"start\":41456},{\"end\":41698,\"start\":41689},{\"end\":41710,\"start\":41702},{\"end\":42139,\"start\":42132},{\"end\":42151,\"start\":42143},{\"end\":42165,\"start\":42159},{\"end\":42175,\"start\":42169},{\"end\":42185,\"start\":42179},{\"end\":42512,\"start\":42510},{\"end\":42521,\"start\":42516},{\"end\":42528,\"start\":42525},{\"end\":42535,\"start\":42532},{\"end\":42938,\"start\":42935},{\"end\":42947,\"start\":42942},{\"end\":42957,\"start\":42953},{\"end\":42969,\"start\":42961},{\"end\":43384,\"start\":43381},{\"end\":43393,\"start\":43388},{\"end\":43590,\"start\":43586},{\"end\":43597,\"start\":43594},{\"end\":43605,\"start\":43601},{\"end\":43617,\"start\":43611},{\"end\":43917,\"start\":43913},{\"end\":43924,\"start\":43921},{\"end\":43932,\"start\":43928},{\"end\":43944,\"start\":43938},{\"end\":44330,\"start\":44324},{\"end\":44340,\"start\":44334},{\"end\":44513,\"start\":44506},{\"end\":44528,\"start\":44517},{\"end\":44539,\"start\":44532},{\"end\":44548,\"start\":44543},{\"end\":44559,\"start\":44552},{\"end\":44867,\"start\":44864},{\"end\":44878,\"start\":44871},{\"end\":44885,\"start\":44882},{\"end\":44902,\"start\":44891},{\"end\":44911,\"start\":44908},{\"end\":44927,\"start\":44917},{\"end\":45317,\"start\":45312},{\"end\":45329,\"start\":45321},{\"end\":45573,\"start\":45568},{\"end\":45581,\"start\":45577},{\"end\":45591,\"start\":45585},{\"end\":45976,\"start\":45973},{\"end\":45984,\"start\":45980},{\"end\":46231,\"start\":46227},{\"end\":46237,\"start\":46235},{\"end\":46246,\"start\":46241},{\"end\":46252,\"start\":46250},{\"end\":46260,\"start\":46258},{\"end\":46268,\"start\":46264},{\"end\":46688,\"start\":46680},{\"end\":46701,\"start\":46692},{\"end\":46710,\"start\":46705},{\"end\":46720,\"start\":46714},{\"end\":46730,\"start\":46724},{\"end\":46739,\"start\":46734},{\"end\":46748,\"start\":46743},{\"end\":46758,\"start\":46752},{\"end\":46771,\"start\":46762},{\"end\":47190,\"start\":47180},{\"end\":47207,\"start\":47194},{\"end\":47218,\"start\":47211},{\"end\":47230,\"start\":47222},{\"end\":47513,\"start\":47510},{\"end\":47521,\"start\":47517},{\"end\":47531,\"start\":47525},{\"end\":47798,\"start\":47795},{\"end\":47808,\"start\":47802},{\"end\":48090,\"start\":48082},{\"end\":48100,\"start\":48096},{\"end\":48110,\"start\":48104},{\"end\":48484,\"start\":48479},{\"end\":48495,\"start\":48488},{\"end\":48505,\"start\":48499},{\"end\":48516,\"start\":48509},{\"end\":48868,\"start\":48863},{\"end\":48879,\"start\":48872},{\"end\":48888,\"start\":48883},{\"end\":48898,\"start\":48892},{\"end\":48909,\"start\":48902},{\"end\":49224,\"start\":49212},{\"end\":49235,\"start\":49228},{\"end\":49250,\"start\":49239},{\"end\":49266,\"start\":49254},{\"end\":49715,\"start\":49710},{\"end\":49726,\"start\":49719},{\"end\":49738,\"start\":49730},{\"end\":49748,\"start\":49742},{\"end\":50177,\"start\":50173},{\"end\":50185,\"start\":50181},{\"end\":50192,\"start\":50189},{\"end\":50200,\"start\":50196},{\"end\":50208,\"start\":50204},{\"end\":50514,\"start\":50510},{\"end\":50522,\"start\":50518},{\"end\":50687,\"start\":50683},{\"end\":50699,\"start\":50691},{\"end\":50709,\"start\":50703},{\"end\":50722,\"start\":50713},{\"end\":51104,\"start\":51099},{\"end\":51112,\"start\":51108},{\"end\":51118,\"start\":51116},{\"end\":51124,\"start\":51122},{\"end\":51132,\"start\":51128},{\"end\":51491,\"start\":51486},{\"end\":51498,\"start\":51495},{\"end\":51512,\"start\":51502},{\"end\":51522,\"start\":51516},{\"end\":51530,\"start\":51526},{\"end\":51835,\"start\":51830},{\"end\":51842,\"start\":51839},{\"end\":51850,\"start\":51846},{\"end\":51860,\"start\":51854},{\"end\":52134,\"start\":52130},{\"end\":52142,\"start\":52138},{\"end\":52151,\"start\":52146},{\"end\":52160,\"start\":52155}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10573410},\"end\":38674,\"start\":38298},{\"attributes\":{\"doi\":\"arXiv:1711.04340\",\"id\":\"b1\"},\"end\":38920,\"start\":38676},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":51980671},\"end\":39270,\"start\":38922},{\"attributes\":{\"doi\":\"arXiv:1810.08810\",\"id\":\"b3\"},\"end\":39486,\"start\":39272},{\"attributes\":{\"doi\":\"arXiv:1702.05538\",\"id\":\"b4\"},\"end\":39684,\"start\":39488},{\"attributes\":{\"doi\":\"arXiv:1903.11341\",\"id\":\"b5\"},\"end\":39970,\"start\":39686},{\"attributes\":{\"doi\":\"arXiv:1409.7495\",\"id\":\"b6\"},\"end\":40188,\"start\":39972},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6742009},\"end\":40542,\"start\":40190},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1033682},\"end\":40932,\"start\":40544},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9067102},\"end\":41380,\"start\":40934},{\"attributes\":{\"doi\":\"arXiv:1707.05574\",\"id\":\"b10\"},\"end\":41616,\"start\":41382},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9099040},\"end\":42026,\"start\":41618},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16183783},\"end\":42460,\"start\":42028},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":42862,\"start\":42462},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":73728950},\"end\":43334,\"start\":42864},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":10627900},\"end\":43582,\"start\":43336},{\"attributes\":{\"doi\":\"arXiv:1502.02791\",\"id\":\"b16\"},\"end\":43854,\"start\":43584},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":12757870},\"end\":44286,\"start\":43856},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5855042},\"end\":44502,\"start\":44288},{\"attributes\":{\"doi\":\"arXiv:1803.09655\",\"id\":\"b19\"},\"end\":44786,\"start\":44504},{\"attributes\":{\"id\":\"b20\"},\"end\":45308,\"start\":44788},{\"attributes\":{\"doi\":\"arXiv:1411.1784\",\"id\":\"b21\"},\"end\":45504,\"start\":45310},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1099052},\"end\":45936,\"start\":45506},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":740063},\"end\":46171,\"start\":45938},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207996214},\"end\":46591,\"start\":46173},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":49190644},\"end\":47097,\"start\":46593},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8308769},\"end\":47458,\"start\":47099},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":16439870},\"end\":47729,\"start\":47460},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":12453047},\"end\":48013,\"start\":47731},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":56657917},\"end\":48429,\"start\":48015},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4357800},\"end\":48859,\"start\":48431},{\"attributes\":{\"doi\":\"arXiv:1412.3474\",\"id\":\"b31\"},\"end\":49151,\"start\":48861},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2928248},\"end\":49637,\"start\":49153},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":19059279},\"end\":50113,\"start\":49639},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6480652},\"end\":50465,\"start\":50115},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3654323},\"end\":50638,\"start\":50467},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4608153},\"end\":51064,\"start\":50640},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":204961178},\"end\":51427,\"start\":51066},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":54028884},\"end\":51773,\"start\":51429},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":118638514},\"end\":52075,\"start\":51775},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":23664127},\"end\":52474,\"start\":52077}]", "bib_title": "[{\"end\":38358,\"start\":38298},{\"end\":38959,\"start\":38922},{\"end\":40245,\"start\":40190},{\"end\":40571,\"start\":40544},{\"end\":40988,\"start\":40934},{\"end\":41685,\"start\":41618},{\"end\":42128,\"start\":42028},{\"end\":42506,\"start\":42462},{\"end\":42929,\"start\":42864},{\"end\":43375,\"start\":43336},{\"end\":43909,\"start\":43856},{\"end\":44316,\"start\":44288},{\"end\":44851,\"start\":44788},{\"end\":45564,\"start\":45506},{\"end\":45967,\"start\":45938},{\"end\":46223,\"start\":46173},{\"end\":46676,\"start\":46593},{\"end\":47176,\"start\":47099},{\"end\":47506,\"start\":47460},{\"end\":47791,\"start\":47731},{\"end\":48078,\"start\":48015},{\"end\":48475,\"start\":48431},{\"end\":49208,\"start\":49153},{\"end\":49706,\"start\":49639},{\"end\":50169,\"start\":50115},{\"end\":50506,\"start\":50467},{\"end\":50677,\"start\":50640},{\"end\":51095,\"start\":51066},{\"end\":51482,\"start\":51429},{\"end\":51826,\"start\":51775},{\"end\":52126,\"start\":52077}]", "bib_author": "[{\"end\":38368,\"start\":38360},{\"end\":38379,\"start\":38368},{\"end\":38688,\"start\":38676},{\"end\":38699,\"start\":38688},{\"end\":38710,\"start\":38699},{\"end\":38968,\"start\":38961},{\"end\":38974,\"start\":38968},{\"end\":38982,\"start\":38974},{\"end\":38990,\"start\":38982},{\"end\":39335,\"start\":39319},{\"end\":39343,\"start\":39335},{\"end\":39499,\"start\":39488},{\"end\":39511,\"start\":39499},{\"end\":39771,\"start\":39760},{\"end\":39781,\"start\":39771},{\"end\":39791,\"start\":39781},{\"end\":39981,\"start\":39972},{\"end\":39994,\"start\":39981},{\"end\":40255,\"start\":40247},{\"end\":40262,\"start\":40255},{\"end\":40269,\"start\":40262},{\"end\":40280,\"start\":40269},{\"end\":40587,\"start\":40573},{\"end\":40604,\"start\":40587},{\"end\":40613,\"start\":40604},{\"end\":40619,\"start\":40613},{\"end\":40635,\"start\":40619},{\"end\":40644,\"start\":40635},{\"end\":40657,\"start\":40644},{\"end\":40667,\"start\":40657},{\"end\":41001,\"start\":40990},{\"end\":41015,\"start\":41001},{\"end\":41029,\"start\":41015},{\"end\":41045,\"start\":41029},{\"end\":41055,\"start\":41045},{\"end\":41067,\"start\":41055},{\"end\":41086,\"start\":41067},{\"end\":41454,\"start\":41447},{\"end\":41463,\"start\":41454},{\"end\":41700,\"start\":41687},{\"end\":41712,\"start\":41700},{\"end\":42141,\"start\":42130},{\"end\":42153,\"start\":42141},{\"end\":42167,\"start\":42153},{\"end\":42177,\"start\":42167},{\"end\":42187,\"start\":42177},{\"end\":42514,\"start\":42508},{\"end\":42523,\"start\":42514},{\"end\":42530,\"start\":42523},{\"end\":42537,\"start\":42530},{\"end\":42940,\"start\":42931},{\"end\":42949,\"start\":42940},{\"end\":42959,\"start\":42949},{\"end\":42971,\"start\":42959},{\"end\":43386,\"start\":43377},{\"end\":43395,\"start\":43386},{\"end\":43592,\"start\":43584},{\"end\":43599,\"start\":43592},{\"end\":43607,\"start\":43599},{\"end\":43619,\"start\":43607},{\"end\":43919,\"start\":43911},{\"end\":43926,\"start\":43919},{\"end\":43934,\"start\":43926},{\"end\":43946,\"start\":43934},{\"end\":44332,\"start\":44318},{\"end\":44342,\"start\":44332},{\"end\":44515,\"start\":44504},{\"end\":44530,\"start\":44515},{\"end\":44541,\"start\":44530},{\"end\":44550,\"start\":44541},{\"end\":44561,\"start\":44550},{\"end\":44869,\"start\":44853},{\"end\":44880,\"start\":44869},{\"end\":44887,\"start\":44880},{\"end\":44904,\"start\":44887},{\"end\":44913,\"start\":44904},{\"end\":44929,\"start\":44913},{\"end\":44935,\"start\":44929},{\"end\":45319,\"start\":45310},{\"end\":45331,\"start\":45319},{\"end\":45575,\"start\":45566},{\"end\":45583,\"start\":45575},{\"end\":45593,\"start\":45583},{\"end\":45978,\"start\":45969},{\"end\":45986,\"start\":45978},{\"end\":46233,\"start\":46225},{\"end\":46239,\"start\":46233},{\"end\":46248,\"start\":46239},{\"end\":46254,\"start\":46248},{\"end\":46262,\"start\":46254},{\"end\":46270,\"start\":46262},{\"end\":46690,\"start\":46678},{\"end\":46703,\"start\":46690},{\"end\":46712,\"start\":46703},{\"end\":46722,\"start\":46712},{\"end\":46732,\"start\":46722},{\"end\":46741,\"start\":46732},{\"end\":46750,\"start\":46741},{\"end\":46760,\"start\":46750},{\"end\":46773,\"start\":46760},{\"end\":47192,\"start\":47178},{\"end\":47209,\"start\":47192},{\"end\":47220,\"start\":47209},{\"end\":47232,\"start\":47220},{\"end\":47515,\"start\":47508},{\"end\":47523,\"start\":47515},{\"end\":47533,\"start\":47523},{\"end\":47800,\"start\":47793},{\"end\":47810,\"start\":47800},{\"end\":48092,\"start\":48080},{\"end\":48102,\"start\":48092},{\"end\":48112,\"start\":48102},{\"end\":48486,\"start\":48477},{\"end\":48497,\"start\":48486},{\"end\":48507,\"start\":48497},{\"end\":48518,\"start\":48507},{\"end\":48870,\"start\":48861},{\"end\":48881,\"start\":48870},{\"end\":48890,\"start\":48881},{\"end\":48900,\"start\":48890},{\"end\":48911,\"start\":48900},{\"end\":49226,\"start\":49210},{\"end\":49237,\"start\":49226},{\"end\":49252,\"start\":49237},{\"end\":49268,\"start\":49252},{\"end\":49717,\"start\":49708},{\"end\":49728,\"start\":49717},{\"end\":49740,\"start\":49728},{\"end\":49750,\"start\":49740},{\"end\":50179,\"start\":50171},{\"end\":50187,\"start\":50179},{\"end\":50194,\"start\":50187},{\"end\":50202,\"start\":50194},{\"end\":50210,\"start\":50202},{\"end\":50516,\"start\":50508},{\"end\":50524,\"start\":50516},{\"end\":50689,\"start\":50679},{\"end\":50701,\"start\":50689},{\"end\":50711,\"start\":50701},{\"end\":50724,\"start\":50711},{\"end\":51106,\"start\":51097},{\"end\":51114,\"start\":51106},{\"end\":51120,\"start\":51114},{\"end\":51126,\"start\":51120},{\"end\":51134,\"start\":51126},{\"end\":51493,\"start\":51484},{\"end\":51500,\"start\":51493},{\"end\":51514,\"start\":51500},{\"end\":51524,\"start\":51514},{\"end\":51532,\"start\":51524},{\"end\":51837,\"start\":51828},{\"end\":51844,\"start\":51837},{\"end\":51852,\"start\":51844},{\"end\":51862,\"start\":51852},{\"end\":52136,\"start\":52128},{\"end\":52144,\"start\":52136},{\"end\":52153,\"start\":52144},{\"end\":52162,\"start\":52153}]", "bib_venue": "[{\"end\":39105,\"start\":39056},{\"end\":41833,\"start\":41781},{\"end\":42678,\"start\":42616},{\"end\":43112,\"start\":43050},{\"end\":44069,\"start\":44016},{\"end\":44620,\"start\":44615},{\"end\":45056,\"start\":45004},{\"end\":45716,\"start\":45663},{\"end\":46391,\"start\":46339},{\"end\":48233,\"start\":48181},{\"end\":48659,\"start\":48597},{\"end\":49409,\"start\":49347},{\"end\":49891,\"start\":49829},{\"end\":50865,\"start\":50803},{\"end\":51255,\"start\":51203},{\"end\":52281,\"start\":52230},{\"end\":38461,\"start\":38379},{\"end\":38775,\"start\":38726},{\"end\":39054,\"start\":38990},{\"end\":39317,\"start\":39272},{\"end\":39564,\"start\":39527},{\"end\":39758,\"start\":39686},{\"end\":40058,\"start\":40009},{\"end\":40343,\"start\":40280},{\"end\":40716,\"start\":40667},{\"end\":41135,\"start\":41086},{\"end\":41445,\"start\":41382},{\"end\":41779,\"start\":41712},{\"end\":42225,\"start\":42187},{\"end\":42614,\"start\":42537},{\"end\":43048,\"start\":42971},{\"end\":43444,\"start\":43395},{\"end\":43695,\"start\":43635},{\"end\":44014,\"start\":43946},{\"end\":44378,\"start\":44342},{\"end\":44613,\"start\":44577},{\"end\":45002,\"start\":44935},{\"end\":45385,\"start\":45346},{\"end\":45661,\"start\":45593},{\"end\":46037,\"start\":45986},{\"end\":46337,\"start\":46270},{\"end\":46822,\"start\":46773},{\"end\":47256,\"start\":47232},{\"end\":47585,\"start\":47533},{\"end\":47848,\"start\":47810},{\"end\":48179,\"start\":48112},{\"end\":48595,\"start\":48518},{\"end\":48981,\"start\":48926},{\"end\":49345,\"start\":49268},{\"end\":49827,\"start\":49750},{\"end\":50266,\"start\":50210},{\"end\":50538,\"start\":50524},{\"end\":50801,\"start\":50724},{\"end\":51201,\"start\":51134},{\"end\":51581,\"start\":51532},{\"end\":51906,\"start\":51862},{\"end\":52228,\"start\":52162}]"}}}, "year": 2023, "month": 12, "day": 17}