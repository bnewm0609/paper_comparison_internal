{"id": 202790039, "updated": "2022-03-27 00:23:48.615", "metadata": {"title": "Neural Topic Model with Reinforcement Learning", "authors": "[{\"first\":\"Lin\",\"last\":\"Gui\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Leng\",\"middle\":[]},{\"first\":\"Gabriele\",\"last\":\"Pergola\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Ruifeng\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yulan\",\"last\":\"He\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2970397832", "acl": "D19-1350", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/GuiLPZXH19", "doi": "10.18653/v1/d19-1350"}}, "content": {"source": {"pdf_hash": "f8be525684d8643ac7bfcd0f6a202ea809dba294", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D19-1350.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D19-1350.pdf", "status": "HYBRID"}}, "grobid": {"id": "e74e283a2194bedb8177de322c63b45b6acdb104", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f8be525684d8643ac7bfcd0f6a202ea809dba294.txt", "contents": "\nNeural Topic Model with Reinforcement Learning\nNovember 3-7, 2019\n\nLin Gui lin.gui@warwick.ac.uk \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nJia Leng lengjia@stu.hit.edu.cngabriele.pergola@warwick.ac.uk \nHarbin Institute of Technology (Shenzhen)\nChina\n\nGabriele Pergola \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nYu Zhou yu.zhou.1@warwick.ac.uk \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nRuifeng Xu xuruifeng@hit.edu.cn \nHarbin Institute of Technology (Shenzhen)\nChina\n\nPeng Cheng Laboratory\nShenzhenChina\n\nJoint Lab of Harbin Institute of Technology and RICOH\n\n\nYulan He yulan.he@warwick.ac.uk \nDepartment of Computer Science\nUniversity of Warwick\nUK\n\nNeural Topic Model with Reinforcement Learning\n\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaNovember 3-7, 20193478\nIn recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models. *\n\nIntroduction\n\nProbabilistic topic models have been used widely in nature language processing (Li et al., 2016;Zeng et al., 2018). The fundamental principle is that words are assumed to be generated from latent topics which can be inferred from data based on word co-occurrence patterns (Neal, 1993;Andrieu et al., 2003). In recent years, Variational Autoencoder (VAE) has been proved more effective and efficient to approximating deep, complex and underestimated variance in integrals (Kingma and Welling, 2013;He et al., 2017). However, the VAE-based topic models focus on the construction of deep neural networks to approximate the \u00a7 The two authors contributed equally to this work. \u2020 Corresponding author.\n\nintractable distribution between observed words and latent topics based on log-likelihood and the learning objective is to minimise the error of reconstructing the original documents based on the learned latent topic vectors rather than improving the quality of learned topics, for example, measured by coherence scores (Kingma and Welling, 2013;S\u00f8nderby et al., 2016;Miao et al., 2016;Srivastava and Sutton, 2017;Bouchacourt et al., 2018). The lack of consideration of topic coherence measures during the learning process of VAE-based topic models makes it difficult to control the quality of the generated topics. Intuitively, one solution is to jointly consider coherence scores in the learning objective. However, this is not feasible since coherence score is an unsupervised measure of topics based on a largescale knowledge source, there is no ground truth \"best topics\". Another limitation of existing approaches is that they typically require a pre-processing step to filter infrequent and/or top frequent words in order to reduce the vocabulary size and achieve better topic extraction results. Word filtering is often done heuristically. Although there have been attempts to automatically distinguishing background words and topic words, existing approaches either require a switch variable defined at each word position to indicate whether the word is a background word, which makes the models cumbersome, or model each latent topic as the deviation in logfrequency from a constant background distribution (Eisenstein et al., 2011;.\n\nIn this paper, we propose a new framework to use reinforcement learning (Pan et al., 2018;Qin et al., 2018;Yin et al., 2018) to incorporate the topic coherence measures into the learning of a neural topic model and filter background words dynamically. More concretely, given an input document, its constituent words will first be sampled by a weight vector which assigns higher weights to words with higher coherence scores and have more concentrated topic distributions. The sampled words will then be fed into a VAE-based neural topic model to reconstruct the original document. A reward function is deployed to take into account both topic coherence scores and the degree of word overlapping between topics. The reward signal derived is subsequently used to update the sampling weight vector for each word. In this way, we do not need to directly add the coherence scores into the loss function. Our experimental results show that our proposed framework outperforms the traditional topic model and existing neural topic modelling approaches on the 20 Newsgroups (Lang, 1995) and the NIPS data  in topic coherence and perplexity.\n\nThe rest of the paper is organized as follows. Section 2 presents our proposed reinforcement learning framework for topic modelling. Section 3 reports the experimental setup and results. Section 4 concludes the paper and outlines future research directions.\n\n\nProposed Method\n\nIn this section, we introduce our proposed reinforcement learning (RL) framework for topic modelling. A standard RL framework contains three components: action, state, and reward. Here, the action aims to select words with high coherence scores and filter background words. The state is the distribution of latent topics among words, which is obtained from a VAE-based topic model. The reward is a function to measure the quality of topics based on an external corpus and guide the weight updating of the next word selecting action. The overall architecture is illustrated in Figure 1.\n\nWe detail our framework in the following.\n\n\nAction\n\nFor an input document d = {w 1 , w 2 , ..., w U }, where each word w i is represented by a one-hot representation, the action is determined by a probabilistic vector P = {p 1 , p 2 , ..., p U } which is used to filter the less topical-coherent and background words at each iteration of model learning. Here, each p i present the sampling probability for word w i , and U is the full vocabulary size. We aim to select V words from the full vocabulary based on P and mask out other words in d,\nthusd = {w 1 , w 2 , ..., w V } where V \u2264 U .\nThe goal of our method is to assign higher probabilities to words which contribute more to the topic coherence scores and lower probabilities to those less topical-coherent words and background words which occur equally likely across topics.\n\n\nState\n\nAfter word selection, the new document representationd is fed into a neural topic model to obtain the state, which is the topic distribution in the topic model. Here, we deploy the VAE (Miao et al., 2016; to learn the latent topics, which consists of two main components, the Inference Network and the Generation Network. For the Inference Network, we use VAE to approximate the posterior distribution over topics for all the training instances. In the Generation Network, the words are generated via Gaussian softmax construction from the topic distribution generated by the Inference Network. The architecture of the neural topic model is shown in Figure (1) and we describe the model in more details below. Inference Network. Following the idea of VAE which computes a variational approximation to an intractable posterior using MLPs, we define two MLPs, f \u00b5 \u03b8 and f \u03a3 \u03b8 , which takes as input the word counts in a document and outputs mean and variance of a Gaussian distribution, both being vec-\ntors in R K , \u00b5 \u03b8 = f \u00b5 \u03b8 (w d ), \u03a3 \u03b8 = diag(f \u03a3 \u03b8 (w d )).\nHere, 'diag' converts a column vector to a diagonal matrix. For a document d, its variational distribution is q(\u03b8) N (\u00b5 \u03b8 , \u03a3 \u03b8 ). With such a formulation, we can generate samples from q(\u03b8) by first sampling \u223c N (0, I 2 ) and then computin\u011d \u03b8 = \u03c3(\u00b5 \u03b8 + \u03a3 \u03b8 1/2 ). Generation Network. We feed the sampled\u03b8 to two MLPs to generate z d . Here, z d is a Kdimensional latent topic representation of document d. The probability of d-th word in n-th document w d,n can be parameterised by another network,\np(w d,n |w d , z d ) \u221d exp(m d + W \u00b7 z d ) (1) where m d is the V -dimensional background log- frequency word distribution, W \u2208 R V \u00d7K is a weight matrix.\nWith the sampled\u03b8 , for each document d \u2208 N d , we can estimate the Evidence Lower Bound (ELBO) with a Monte Carlo approximation using L independent samples:\nL t (w d ) \u2248 1 L L l=1 N d n=1 log p(w d,n |\u03b8 (l) ) \u2212 KL(q(z d |w d )||p(z d ))(2)\nBy minimising the ELBO in Eq.\n\n(2), the neural topic model reconstructs the input document w d . At the reconstruction layer, the matrix W \u2208 R V \u00d7K in a single-layer network, which is used to capture the sampling weights between each word and the latent topics, is the state which produces specific topic coherence scores.\n\n\nReward\n\nIntuitively, words with higher topic coherence and lower degree of overlapping among different topics should be assigned higher reward in the next iteration of learning. Hence, the reward function should be composed by two terms for each word: the average coherence score and topic overlapping value. The average coherence score is defined as:\nCO average = (W \u00b7 C v (W )) P w i \u2208w d(3)\nwhere matrix W is the distribution of latent topics among words, C v (W ) is a K-dimension vector which contains the coherence score for each topic based on the sampling weight matrix, P w i \u2208w d is the sampling probability for each word in document d (i.e., which action to take as described in Section 2.1), and is the element-wise product. Hence, CO average is a V -dimension weight vector to distribute coherence scores to the selected words based on sampling probabilities in action and topic distribution in topic modelling. The Topic Overlapping (T O) is defined as:\nT O = sum row (abs(I \u2212 W \u00b7 W T ))(4)\nwhere I is a V \u00d7 V identity matrix. T O \u2208 R |v| is to measure the separation based on mean distribution. In T O, the high value indicates that the associated word appears frequently across topics and hence could be considered as background words. Based on the average coherence score and the topic overlapping value, the reward function is:\nR t = CO average \u2212 \u03b1 \u00b7 T O (5) Q t = \u03b2 \u00b7 Q t\u22121 + (1 \u2212 \u03b2) \u00b7 R t\u22121(6)\nwhere \u03b1 and \u03b2 \u2208 (0, 1) are trade-off coefficient. Then, the reward at the current time step, R t , and the history rewards encoded by Q t will be used to update the sampling weight in the action:\nP t = max(P t\u22121 + \u03bb P \u00b7 (R t \u2212 Q t ), 0 + ) (7)\nwhere P t is the sampling vector in action, > 0 is a minimal value, and \u03bb P is the learning rate for P . We choose a ramp function with to ensure the sampling probability is positive.\n\n\nTraining\n\nFor pre-processing, we performed stop word removal * and use Adam to optimise the parameters in the neural networks. The learning rate for VAE and \u03b8 p are both 0.0001, the mini-batch size is 32, \u03b1 is 0.1, \u03b2 is 0.5, is 0.01, and the coherence scores are obtained from Wikipedia. The parameters in VAE are updated in each mini-batch, and the probabilistic vector P for action selection is updated every 2,000 mini-batches.\n\n\nExperiments\n\nWe evaluate our model on the 20 Newsgroups \u2020 consisting of 18K documents, and NIPS (Tan et al., 2017) consisting of 6.6k documents. We use 10% training data as the validation set to fine-tune the parameters. We compare our results with those obtained from the following baselines: LDA: Latent Dirichlet Allocation Model (Blei et al., 2003). NVDM: Neural Variational Document model (Miao et al., 2016). NGTM: Neural Generative Topic model . Scholar: Topic model with metadata . VTMRL: Our proposed Variational Topic Model with Reinforcement Learning. For all the baseline models, we follow the common pre-processing step in existing approaches by performing stop word removal, and selecting the most frequent 2,000 words as the vocabulary. Since our proposed framework dynamically select words at each iteration of learning, we do not need to pre-set the vocabulary prior to model learning. Instead, we only activate 2,000 words at each mini-batch of training based on the word sampling probabilities. As our model dynamically select words during the training process, in order to ensure fair comparison with other models, we also report the results of training baselines using the vocabulary dynamically generated by our model.\n\nIn our experiments, the models are evaluated based on the perplexity (PPL, lower is better) and topic coherence measure (C v ) based on external corpus (R\u00f6der et al., 2015) (higher is better). The results with 30 and 50 topics are shown in Table 1. LDA is a conventional topic model, while all the other models are neural topic models. It can be observed from Table 1 that NVDM and NGTM achieve better perplexities compared to LDA. However, in terms of topic coherence measure, NVDM and NGTM perform slightly worse than LDA. A similar observation has been reported in . Scholar achieves better coherence compared to other neural models. Nevertheless, after using reinforcement learning based on the topic coherence scores in our proposed model, VTMRL outperforms all the other models on the topic coherence measure by a large margin. RL could activate words which are semantically related to topics regardless of their occurrence frequency. The inclusion of some rare words would impact the models' predictive probabilities. As such, we observe worse perplexity results for models trained with RL-based vocabulary compared to frequency-based vocabulary in 20 Newsgroups, though the converse is true for NIPS. Nevertheless, the coherence scores improve for all the models with RL-based vocabulary.\n\nAs incorporating RL could increase the computational complexity of VTMRL, we report in Table 2 the total number of parameters and average training time per epoch when the vocabulary size is 2,000 and the number of topics is 50.\n\nStrictly speaking, the number of parameters in LDA is not directly comparable with neural models. Neural models have similar parameter size. With the incorporation of RL, VTMRL only increased the parameter size by 1.4%. Due to the efficiency of GPU, the running costs of neural models are better than that of LDA. Although our proposed VTMRL used full vocabulary, the active words in each epoch are limited. Hence, there is no significant increase in terms of the running cost.   We show in Table 3 example topics with/without RL by VTMRL in 20 Newsgroups. The RL method seems producing more interpretable topics. Also, due to the rewardbased words sampling in RL, words with low occurrence frequency would still have a chance to be promoted in specific topics, such as 'x11r5', which is a serial number of the Windows system. We next compare the topic coherence changes during model training. We observe that for VTMRL the coherence value increases at the beginning of the training and remains relatively stable in subsequent training iterations. As a contrast, the coherence value of our model without RL is not stable, and decreases rapidly after 10 training epochs. This is not surprising since the model without RL did not consider topic coherence in its learning process.\n\nWe also evaluate the effectiveness of using the learned topics as features to train text classifiers on the 20 Newsgroups data. The results are obtained by using logistic regression as the classifier trained from the topics generated by various aforementioned models. We also report the results by training logistic regression from the combination of word features (tf-idf) and topic features (#topic = 30). In addition, we include the results using neural models such as CNN and RNN in Table 4.\n\nUsing only topics extracted from topic models as features to train logistic regression, our pro-   posed model VTMRL beats other baselines. However, the topic features have only 30 dimensions so the performance is limited in comparison with CNN and RNN. When we combine the topic features with tf-idf based word features, the performance is boosted significantly compared to CNN and RNN and the best result is obtained by using the logistic regression model trained from the combined word features with topics generated by our proposed VTMRL.\n\n\nConclusion\n\nIn this paper, we have proposed a new reinforcement learning (RL) framework for neural topic modelling, where words are activated dynamically by RL according to topic coherence scores and topic overlapping values. The experiments on the 20 Newsgroups and NIPS datasets show encouraging results both on perplexity and topic coherence measures in comparison with existing neural topic models. In future work, we will explore extending our model for temporal topic modelling.\n\nFigure 1 :\n1Neural topic model with reinforcement learning.\n\nFigure 2 :\n2Topic coherence changes with/without RL (#Topics = 50).\n\nTable 2 :\n2Number of parameters for each model and the \naverage training time per epoch with vocabulary size \n2,000 and topic number 50. \n\nTopic \nTopic Words \nWithout RL \n1 \nuniversity <NUM> subject host idea \n2 \norganization article writes surrender lines \n3 \npeople organization posting article lines \nWith RL \n1 \nmouse x11r5 keyboard serial remote \n2 \nchip design products build system \n3 \ndrives friend sports espn michigan \n\n\n\nTable 3 :\n3Example topic words with/without RL by VTMRL (#Topics = 50) in 20 Newsgroup.\n\nTable 4 :\n4Text classification accuracy of different models on the 20 Newsgroups data.\nAcknowledgmentThis work was supported by the Innovate UK (grant no. 103652), the EU-H2020 (grant no. 794196), the National Natural Science Foundation of China U1636103, 61876053.\nAn introduction to MCMC for machine learning. Christophe Andrieu, Arnaud Nando De Freitas, Michael I Doucet, Jordan, 10.1023/A:1020281327116Machine Learning. 50Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan. 2003. An introduction to MCMC for machine learning. Machine Learning, 50(1-2):5-43.\n\nLatent dirichlet allocation. M David, Blei, Y Andrew, Michael I Jordan Ng, Journal of machine Learning research. 3David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of ma- chine Learning research, 3(Jan):993-1022.\n\nMulti-level variational autoencoder: Learning disentangled representations from grouped observations. Diane Bouchacourt, Ryota Tomioka, Sebastian Nowozin, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence. the Thirty-Second AAAI Conference on Artificial IntelligenceNew Orleans, Louisiana, USADiane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. 2018. Multi-level variational autoen- coder: Learning disentangled representations from grouped observations. In Proceedings of the Thirty- Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018.\n\nA neural framework for generalized topic models. Dallas Card, Chenhao Tan, Noah A Smith, abs/1705.09296CoRRDallas Card, Chenhao Tan, and Noah A. Smith. 2017. A neural framework for generalized topic models. CoRR, abs/1705.09296.\n\nNeural models for documents with metadata. Dallas Card, Chenhao Tan, Noah A Smith, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018Melbourne, AustraliaLong Papers1Dallas Card, Chenhao Tan, and Noah A. Smith. 2018. Neural models for documents with metadata. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2031-2040.\n\nSparse additive generative models of text. Jacob Eisenstein, Amr Ahmed, Eric P Xing, Proceedings of the 28th International conference on Machine Learning (ICML). the 28th International conference on Machine Learning (ICML)Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011. Sparse additive generative models of text. In Pro- ceedings of the 28th International conference on Machine Learning (ICML).\n\nAn unsupervised neural attention model for aspect extraction. Ruidan He, Hwee Tou Wee Sun Lee, Daniel Ng, Dahlmeier, 10.18653/v1/P17-1036Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsLong Papers1Association for Computational LinguisticsRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2017. An unsupervised neural attention model for aspect extraction. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 388-397. Association for Computational Linguis- tics.\n\nAutoencoding variational bayes. P Diederik, Max Kingma, Welling, abs/1312.6114CoRRDiederik P. Kingma and Max Welling. 2013. Auto- encoding variational bayes. CoRR, abs/1312.6114.\n\nNewsweeder: Learning to filter netnews. Ken Lang, Proceedings of the Twelfth International Conference on Machine Learning. the Twelfth International Conference on Machine LearningKen Lang. 1995. Newsweeder: Learning to filter net- news. In Proceedings of the Twelfth International Conference on Machine Learning, pages 331-339.\n\nTopic extraction from microblog posts using conversation structures. Jing Li, Ming Liao, Wei Gao, Yulan He, Kam-Fai Wong, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016Berlin, GermanyLong Papers1Jing Li, Ming Liao, Wei Gao, Yulan He, and Kam-Fai Wong. 2016. Topic extraction from microblog posts using conversation structures. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.\n\nNeural variational inference for text processing. Yishu Miao, Lei Yu, Phil Blunsom, Proceedings of the 33nd International Conference on Machine Learning. the 33nd International Conference on Machine LearningNew York City, NY, USAYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu- ral variational inference for text processing. In Pro- ceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1727-1736.\n\nProbabilistic inference using markov chain monte carlo methods. M Radford, Neal, Radford M Neal. 1993. Probabilistic inference using markov chain monte carlo methods.\n\nDiscourse marker augmented network with reinforcement learning for natural language inference. Yazheng Boyuan Pan, Zhou Yang, Yueting Zhao, Deng Zhuang, Xiaofei Cai, He, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Association for Computational LinguisticsBoyuan Pan, Yazheng Yang, Zhou Zhao, Yueting Zhuang, Deng Cai, and Xiaofei He. 2018. Dis- course marker augmented network with reinforce- ment learning for natural language inference. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 989-999. Association for Com- putational Linguistics.\n\nRobust distant supervision relation extraction via deep reinforcement learning. Pengda Qin, X U Weiran, William Yang Wang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics1Long Papers). Association for Computational LinguisticsPengda Qin, Weiran XU, and William Yang Wang. 2018. Robust distant supervision relation extrac- tion via deep reinforcement learning. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 2137-2147. Association for Computa- tional Linguistics.\n\nExploring the space of topic coherence measures. Michael R\u00f6der, Andreas Both, Alexander Hinneburg, 10.1145/2684822.2685324Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015. the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015Shanghai, ChinaMichael R\u00f6der, Andreas Both, and Alexander Hinneb- urg. 2015. Exploring the space of topic coherence measures. In Proceedings of the Eighth ACM Inter- national Conference on Web Search and Data Min- ing, WSDM 2015, Shanghai, China, February 2-6, 2015, pages 399-408.\n\nNeural models for documents with metadata. Noah A Smith, Dallas Card, Chenhao Tan, ACL. Noah A. Smith, Dallas Card, and Chenhao Tan. 2018. Neural models for documents with metadata. In ACL.\n\nLadder variational autoencoders. Tapani Casper Kaae S\u00f8nderby, Lars Raiko, Maal\u00f8e, Ole S\u00f8ren Kaae S\u00f8nderby, Winther, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016. Barcelona, SpainCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. 2016. Lad- der variational autoencoders. In Advances in Neu- ral Information Processing Systems 29: Annual Conference on Neural Information Processing Sys- tems 2016, December 5-10, 2016, Barcelona, Spain, pages 3738-3746.\n\nAutoencoding variational inference for topic models. Akash Srivastava, Charles Sutton, Proceedings of the 5th International Conference on Learning Representations. the 5th International Conference on Learning RepresentationsICLRAkash Srivastava and Charles Sutton. 2017. Autoen- coding variational inference for topic models. In Proceedings of the 5th International Conference on Learning Representations (ICLR).\n\nFriendships, rivalries, and trysts: Characterizing relations between ideas in texts. Chenhao Tan, Dallas Card, Noah A Smith, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Chenhao Tan, Dallas Card, and Noah A. Smith. 2017. Friendships, rivalries, and trysts: Characterizing re- lations between ideas in texts. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 773-783.\n\nDeep reinforcement learning for chinese zero pronoun resolution. Qingyu Yin, Yu Zhang, Wei-Nan Zhang, Ting Liu, William Yang Wang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Association for Computational LinguisticsQingyu Yin, Yu Zhang, Wei-Nan Zhang, Ting Liu, and William Yang Wang. 2018. Deep reinforce- ment learning for chinese zero pronoun resolution. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 569-578. Association for Com- putational Linguistics.\n\nMicroblog conversation recommendation via joint modeling of topics and discourse. Xingshan Zeng, Jing Li, Lu Wang, Nicholas Beauchamp, Sarah Shugars, Kam-Fai Wong, 10.18653/v1/N18-1035Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1Long PapersXingshan Zeng, Jing Li, Lu Wang, Nicholas Beauchamp, Sarah Shugars, and Kam-Fai Wong. 2018. Microblog conversation recommendation via joint modeling of topics and discourse. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 375-385. Association for Computational Linguistics.\n", "annotations": {"author": "[{\"end\":155,\"start\":68},{\"end\":267,\"start\":156},{\"end\":342,\"start\":268},{\"end\":432,\"start\":343},{\"end\":607,\"start\":433},{\"end\":697,\"start\":608}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":72},{\"end\":164,\"start\":160},{\"end\":284,\"start\":277},{\"end\":350,\"start\":346},{\"end\":443,\"start\":441},{\"end\":616,\"start\":614}]", "author_first_name": "[{\"end\":71,\"start\":68},{\"end\":159,\"start\":156},{\"end\":276,\"start\":268},{\"end\":345,\"start\":343},{\"end\":440,\"start\":433},{\"end\":613,\"start\":608}]", "author_affiliation": "[{\"end\":154,\"start\":99},{\"end\":266,\"start\":219},{\"end\":341,\"start\":286},{\"end\":431,\"start\":376},{\"end\":513,\"start\":466},{\"end\":550,\"start\":515},{\"end\":606,\"start\":552},{\"end\":696,\"start\":641}]", "title": "[{\"end\":47,\"start\":1},{\"end\":744,\"start\":698}]", "venue": "[{\"end\":906,\"start\":746}]", "abstract": "[{\"end\":2132,\"start\":1091}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2244,\"start\":2227},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2262,\"start\":2244},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2432,\"start\":2420},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2453,\"start\":2432},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2645,\"start\":2619},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2661,\"start\":2645},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3191,\"start\":3165},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3213,\"start\":3191},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3231,\"start\":3213},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3259,\"start\":3231},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3284,\"start\":3259},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4387,\"start\":4362},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4480,\"start\":4462},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4497,\"start\":4480},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4514,\"start\":4497},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5467,\"start\":5455},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7432,\"start\":7413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12138,\"start\":12119},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12199,\"start\":12180},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13200,\"start\":13180}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17421,\"start\":17361},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17490,\"start\":17422},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":17922,\"start\":17491},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":18011,\"start\":17923},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":18099,\"start\":18012}]", "paragraph": "[{\"end\":2843,\"start\":2148},{\"end\":4388,\"start\":2845},{\"end\":5521,\"start\":4390},{\"end\":5780,\"start\":5523},{\"end\":6385,\"start\":5800},{\"end\":6428,\"start\":6387},{\"end\":6930,\"start\":6439},{\"end\":7218,\"start\":6977},{\"end\":8228,\"start\":7228},{\"end\":8787,\"start\":8289},{\"end\":9100,\"start\":8943},{\"end\":9213,\"start\":9184},{\"end\":9506,\"start\":9215},{\"end\":9860,\"start\":9517},{\"end\":10476,\"start\":9903},{\"end\":10854,\"start\":10514},{\"end\":11118,\"start\":10923},{\"end\":11350,\"start\":11167},{\"end\":11783,\"start\":11363},{\"end\":13026,\"start\":11799},{\"end\":14324,\"start\":13028},{\"end\":14553,\"start\":14326},{\"end\":15832,\"start\":14555},{\"end\":16329,\"start\":15834},{\"end\":16873,\"start\":16331},{\"end\":17360,\"start\":16888}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6976,\"start\":6931},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8288,\"start\":8229},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8942,\"start\":8788},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9183,\"start\":9101},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9902,\"start\":9861},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10513,\"start\":10477},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10922,\"start\":10855},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11166,\"start\":11119}]", "table_ref": "[{\"end\":13276,\"start\":13268},{\"end\":13395,\"start\":13388},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15053,\"start\":15046},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16328,\"start\":16321}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2146,\"start\":2134},{\"attributes\":{\"n\":\"2\"},\"end\":5798,\"start\":5783},{\"attributes\":{\"n\":\"2.1\"},\"end\":6437,\"start\":6431},{\"attributes\":{\"n\":\"2.2\"},\"end\":7226,\"start\":7221},{\"attributes\":{\"n\":\"2.3\"},\"end\":9515,\"start\":9509},{\"attributes\":{\"n\":\"2.4\"},\"end\":11361,\"start\":11353},{\"attributes\":{\"n\":\"3\"},\"end\":11797,\"start\":11786},{\"attributes\":{\"n\":\"4\"},\"end\":16886,\"start\":16876},{\"end\":17372,\"start\":17362},{\"end\":17433,\"start\":17423},{\"end\":17501,\"start\":17492},{\"end\":17933,\"start\":17924},{\"end\":18022,\"start\":18013}]", "table": "[{\"end\":17922,\"start\":17503}]", "figure_caption": "[{\"end\":17421,\"start\":17374},{\"end\":17490,\"start\":17435},{\"end\":18011,\"start\":17935},{\"end\":18099,\"start\":18024}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6384,\"start\":6376},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7888,\"start\":7878}]", "bib_author_first_name": "[{\"end\":18335,\"start\":18325},{\"end\":18351,\"start\":18345},{\"end\":18377,\"start\":18370},{\"end\":18379,\"start\":18378},{\"end\":18630,\"start\":18629},{\"end\":18645,\"start\":18644},{\"end\":18670,\"start\":18654},{\"end\":18963,\"start\":18958},{\"end\":18982,\"start\":18977},{\"end\":19001,\"start\":18992},{\"end\":19528,\"start\":19522},{\"end\":19542,\"start\":19535},{\"end\":19552,\"start\":19548},{\"end\":19554,\"start\":19553},{\"end\":19752,\"start\":19746},{\"end\":19766,\"start\":19759},{\"end\":19776,\"start\":19772},{\"end\":19778,\"start\":19777},{\"end\":20326,\"start\":20321},{\"end\":20342,\"start\":20339},{\"end\":20354,\"start\":20350},{\"end\":20356,\"start\":20355},{\"end\":20746,\"start\":20740},{\"end\":20759,\"start\":20751},{\"end\":20779,\"start\":20773},{\"end\":21368,\"start\":21367},{\"end\":21382,\"start\":21379},{\"end\":21558,\"start\":21555},{\"end\":21917,\"start\":21913},{\"end\":21926,\"start\":21922},{\"end\":21936,\"start\":21933},{\"end\":21947,\"start\":21942},{\"end\":21959,\"start\":21952},{\"end\":22525,\"start\":22520},{\"end\":22535,\"start\":22532},{\"end\":22544,\"start\":22540},{\"end\":23006,\"start\":23005},{\"end\":23211,\"start\":23204},{\"end\":23228,\"start\":23224},{\"end\":23242,\"start\":23235},{\"end\":23253,\"start\":23249},{\"end\":23269,\"start\":23262},{\"end\":23942,\"start\":23936},{\"end\":23949,\"start\":23948},{\"end\":23951,\"start\":23950},{\"end\":23967,\"start\":23960},{\"end\":23972,\"start\":23968},{\"end\":24569,\"start\":24562},{\"end\":24584,\"start\":24577},{\"end\":24600,\"start\":24591},{\"end\":25142,\"start\":25138},{\"end\":25144,\"start\":25143},{\"end\":25158,\"start\":25152},{\"end\":25172,\"start\":25165},{\"end\":25325,\"start\":25319},{\"end\":25352,\"start\":25348},{\"end\":25371,\"start\":25368},{\"end\":25906,\"start\":25901},{\"end\":25926,\"start\":25919},{\"end\":26354,\"start\":26347},{\"end\":26366,\"start\":26360},{\"end\":26377,\"start\":26373},{\"end\":26379,\"start\":26378},{\"end\":26967,\"start\":26961},{\"end\":26975,\"start\":26973},{\"end\":26990,\"start\":26983},{\"end\":27002,\"start\":26998},{\"end\":27015,\"start\":27008},{\"end\":27020,\"start\":27016},{\"end\":27653,\"start\":27645},{\"end\":27664,\"start\":27660},{\"end\":27671,\"start\":27669},{\"end\":27686,\"start\":27678},{\"end\":27703,\"start\":27698},{\"end\":27720,\"start\":27713}]", "bib_author_last_name": "[{\"end\":18343,\"start\":18336},{\"end\":18368,\"start\":18352},{\"end\":18386,\"start\":18380},{\"end\":18394,\"start\":18388},{\"end\":18636,\"start\":18631},{\"end\":18642,\"start\":18638},{\"end\":18652,\"start\":18646},{\"end\":18673,\"start\":18671},{\"end\":18975,\"start\":18964},{\"end\":18990,\"start\":18983},{\"end\":19009,\"start\":19002},{\"end\":19533,\"start\":19529},{\"end\":19546,\"start\":19543},{\"end\":19560,\"start\":19555},{\"end\":19757,\"start\":19753},{\"end\":19770,\"start\":19767},{\"end\":19784,\"start\":19779},{\"end\":20337,\"start\":20327},{\"end\":20348,\"start\":20343},{\"end\":20361,\"start\":20357},{\"end\":20749,\"start\":20747},{\"end\":20771,\"start\":20760},{\"end\":20782,\"start\":20780},{\"end\":20793,\"start\":20784},{\"end\":21377,\"start\":21369},{\"end\":21389,\"start\":21383},{\"end\":21398,\"start\":21391},{\"end\":21563,\"start\":21559},{\"end\":21920,\"start\":21918},{\"end\":21931,\"start\":21927},{\"end\":21940,\"start\":21937},{\"end\":21950,\"start\":21948},{\"end\":21964,\"start\":21960},{\"end\":22530,\"start\":22526},{\"end\":22538,\"start\":22536},{\"end\":22552,\"start\":22545},{\"end\":23014,\"start\":23007},{\"end\":23020,\"start\":23016},{\"end\":23222,\"start\":23212},{\"end\":23233,\"start\":23229},{\"end\":23247,\"start\":23243},{\"end\":23260,\"start\":23254},{\"end\":23273,\"start\":23270},{\"end\":23277,\"start\":23275},{\"end\":23946,\"start\":23943},{\"end\":23958,\"start\":23952},{\"end\":23977,\"start\":23973},{\"end\":24575,\"start\":24570},{\"end\":24589,\"start\":24585},{\"end\":24610,\"start\":24601},{\"end\":25150,\"start\":25145},{\"end\":25163,\"start\":25159},{\"end\":25176,\"start\":25173},{\"end\":25346,\"start\":25326},{\"end\":25358,\"start\":25353},{\"end\":25366,\"start\":25360},{\"end\":25391,\"start\":25372},{\"end\":25400,\"start\":25393},{\"end\":25917,\"start\":25907},{\"end\":25933,\"start\":25927},{\"end\":26358,\"start\":26355},{\"end\":26371,\"start\":26367},{\"end\":26385,\"start\":26380},{\"end\":26971,\"start\":26968},{\"end\":26981,\"start\":26976},{\"end\":26996,\"start\":26991},{\"end\":27006,\"start\":27003},{\"end\":27025,\"start\":27021},{\"end\":27658,\"start\":27654},{\"end\":27667,\"start\":27665},{\"end\":27676,\"start\":27672},{\"end\":27696,\"start\":27687},{\"end\":27711,\"start\":27704},{\"end\":27725,\"start\":27721}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1023/A:1020281327116\",\"id\":\"b0\",\"matched_paper_id\":38363},\"end\":18598,\"start\":18279},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3177797},\"end\":18854,\"start\":18600},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1209557},\"end\":19471,\"start\":18856},{\"attributes\":{\"doi\":\"abs/1705.09296\",\"id\":\"b3\"},\"end\":19701,\"start\":19473},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":51862727},\"end\":20276,\"start\":19703},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8718058},\"end\":20676,\"start\":20278},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1036\",\"id\":\"b6\",\"matched_paper_id\":29907166},\"end\":21333,\"start\":20678},{\"attributes\":{\"doi\":\"abs/1312.6114\",\"id\":\"b7\"},\"end\":21513,\"start\":21335},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1921714},\"end\":21842,\"start\":21515},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14221574},\"end\":22468,\"start\":21844},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10796},\"end\":22939,\"start\":22470},{\"attributes\":{\"id\":\"b11\"},\"end\":23107,\"start\":22941},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":51812449},\"end\":23854,\"start\":23109},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":44144625},\"end\":24511,\"start\":23856},{\"attributes\":{\"doi\":\"10.1145/2684822.2685324\",\"id\":\"b14\",\"matched_paper_id\":7743332},\"end\":25093,\"start\":24513},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":51862727},\"end\":25284,\"start\":25095},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10447416},\"end\":25846,\"start\":25286},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":29842525},\"end\":26260,\"start\":25848},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":12038912},\"end\":26894,\"start\":26262},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":47019459},\"end\":27561,\"start\":26896},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1035\",\"id\":\"b20\",\"matched_paper_id\":44142931},\"end\":28477,\"start\":27563}]", "bib_title": "[{\"end\":18323,\"start\":18279},{\"end\":18627,\"start\":18600},{\"end\":18956,\"start\":18856},{\"end\":19744,\"start\":19703},{\"end\":20319,\"start\":20278},{\"end\":20738,\"start\":20678},{\"end\":21553,\"start\":21515},{\"end\":21911,\"start\":21844},{\"end\":22518,\"start\":22470},{\"end\":23202,\"start\":23109},{\"end\":23934,\"start\":23856},{\"end\":24560,\"start\":24513},{\"end\":25136,\"start\":25095},{\"end\":25317,\"start\":25286},{\"end\":25899,\"start\":25848},{\"end\":26345,\"start\":26262},{\"end\":26959,\"start\":26896},{\"end\":27643,\"start\":27563}]", "bib_author": "[{\"end\":18345,\"start\":18325},{\"end\":18370,\"start\":18345},{\"end\":18388,\"start\":18370},{\"end\":18396,\"start\":18388},{\"end\":18638,\"start\":18629},{\"end\":18644,\"start\":18638},{\"end\":18654,\"start\":18644},{\"end\":18675,\"start\":18654},{\"end\":18977,\"start\":18958},{\"end\":18992,\"start\":18977},{\"end\":19011,\"start\":18992},{\"end\":19535,\"start\":19522},{\"end\":19548,\"start\":19535},{\"end\":19562,\"start\":19548},{\"end\":19759,\"start\":19746},{\"end\":19772,\"start\":19759},{\"end\":19786,\"start\":19772},{\"end\":20339,\"start\":20321},{\"end\":20350,\"start\":20339},{\"end\":20363,\"start\":20350},{\"end\":20751,\"start\":20740},{\"end\":20773,\"start\":20751},{\"end\":20784,\"start\":20773},{\"end\":20795,\"start\":20784},{\"end\":21379,\"start\":21367},{\"end\":21391,\"start\":21379},{\"end\":21400,\"start\":21391},{\"end\":21565,\"start\":21555},{\"end\":21922,\"start\":21913},{\"end\":21933,\"start\":21922},{\"end\":21942,\"start\":21933},{\"end\":21952,\"start\":21942},{\"end\":21966,\"start\":21952},{\"end\":22532,\"start\":22520},{\"end\":22540,\"start\":22532},{\"end\":22554,\"start\":22540},{\"end\":23016,\"start\":23005},{\"end\":23022,\"start\":23016},{\"end\":23224,\"start\":23204},{\"end\":23235,\"start\":23224},{\"end\":23249,\"start\":23235},{\"end\":23262,\"start\":23249},{\"end\":23275,\"start\":23262},{\"end\":23279,\"start\":23275},{\"end\":23948,\"start\":23936},{\"end\":23960,\"start\":23948},{\"end\":23979,\"start\":23960},{\"end\":24577,\"start\":24562},{\"end\":24591,\"start\":24577},{\"end\":24612,\"start\":24591},{\"end\":25152,\"start\":25138},{\"end\":25165,\"start\":25152},{\"end\":25178,\"start\":25165},{\"end\":25348,\"start\":25319},{\"end\":25360,\"start\":25348},{\"end\":25368,\"start\":25360},{\"end\":25393,\"start\":25368},{\"end\":25402,\"start\":25393},{\"end\":25919,\"start\":25901},{\"end\":25935,\"start\":25919},{\"end\":26360,\"start\":26347},{\"end\":26373,\"start\":26360},{\"end\":26387,\"start\":26373},{\"end\":26973,\"start\":26961},{\"end\":26983,\"start\":26973},{\"end\":26998,\"start\":26983},{\"end\":27008,\"start\":26998},{\"end\":27027,\"start\":27008},{\"end\":27660,\"start\":27645},{\"end\":27669,\"start\":27660},{\"end\":27678,\"start\":27669},{\"end\":27698,\"start\":27678},{\"end\":27713,\"start\":27698},{\"end\":27727,\"start\":27713}]", "bib_venue": "[{\"end\":18435,\"start\":18419},{\"end\":18711,\"start\":18675},{\"end\":19086,\"start\":19011},{\"end\":19520,\"start\":19473},{\"end\":19883,\"start\":19786},{\"end\":20438,\"start\":20363},{\"end\":20902,\"start\":20815},{\"end\":21365,\"start\":21335},{\"end\":21636,\"start\":21565},{\"end\":22063,\"start\":21966},{\"end\":22622,\"start\":22554},{\"end\":23003,\"start\":22941},{\"end\":23366,\"start\":23279},{\"end\":24066,\"start\":23979},{\"end\":24730,\"start\":24635},{\"end\":25181,\"start\":25178},{\"end\":25519,\"start\":25402},{\"end\":26010,\"start\":25935},{\"end\":26474,\"start\":26387},{\"end\":27114,\"start\":27027},{\"end\":27889,\"start\":27747},{\"end\":19175,\"start\":19088},{\"end\":19987,\"start\":19885},{\"end\":20500,\"start\":20440},{\"end\":20976,\"start\":20904},{\"end\":21694,\"start\":21638},{\"end\":22162,\"start\":22065},{\"end\":22699,\"start\":22624},{\"end\":23440,\"start\":23368},{\"end\":24140,\"start\":24068},{\"end\":24827,\"start\":24732},{\"end\":25537,\"start\":25521},{\"end\":26072,\"start\":26012},{\"end\":26565,\"start\":26476},{\"end\":27188,\"start\":27116},{\"end\":28018,\"start\":27891}]"}}}, "year": 2023, "month": 12, "day": 17}