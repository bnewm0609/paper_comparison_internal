{"id": 212725250, "updated": "2023-11-11 03:57:13.126", "metadata": {"title": "GeoDA: A Geometric Framework for Black-Box Adversarial Attacks", "authors": "[{\"first\":\"Ali\",\"last\":\"Rahmati\",\"middle\":[]},{\"first\":\"Seyed-Mohsen\",\"last\":\"Moosavi-Dezfooli\",\"middle\":[]},{\"first\":\"Pascal\",\"last\":\"Frossard\",\"middle\":[]},{\"first\":\"Huaiyu\",\"last\":\"Dai\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 6, "day": 1}, "abstract": "Adversarial examples are known as carefully perturbed images fooling image classifiers. We propose a geometric framework to generate adversarial examples in one of the most challenging black-box settings where the adversary can only generate a small number of queries, each of them returning the top-1 label of the classifier. Our framework is based on the observation that the decision boundary of deep networks usually has a small mean curvature in the vicinity of data samples. We propose an effective iterative algorithm to generate query-efficient black-box perturbations with small p norms which is confirmed via experimental evaluations on state-of-the-art natural image classifiers. Moreover, for p=2, we theoretically show that our algorithm actually converges to the minimal perturbation when the curvature of the decision boundary is bounded. We also obtain the optimal distribution of the queries over the iterations of the algorithm. Finally, experimental results confirm that our principled black-box attack algorithm performs better than state-of-the-art algorithms as it generates smaller perturbations with a reduced number of queries.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3034619610", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/RahmatiMFD20", "doi": "10.1109/cvpr42600.2020.00847"}}, "content": {"source": {"pdf_hash": "efcfc3963577520f6d04e575018f07601ef058be", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.06468v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.06468", "status": "GREEN"}}, "grobid": {"id": "dfe6903229f77ca2d23a71493f626670582b8ff7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/efcfc3963577520f6d04e575018f07601ef058be.txt", "contents": "\nGeoDA: a geometric framework for black-box adversarial attacks\n\n\nAli Rahmati arahmat@ncsu.edu \nSeyed-Mohsen Moosavi-Dezfooli \nInstitue for Machine Learning\nETH Zurich \u2021 Ecole Polytechnique Federale de Lausanne\n\n\nPascal Frossard pascal.frossard@epfl.ch \nHuaiyu Dai hdai@ncsu.edu \n\nDepartment of ECE\nNorth Carolina State University\n\n\nGeoDA: a geometric framework for black-box adversarial attacks\n\nAdversarial examples are known as carefully perturbed images fooling image classifiers. We propose a geometric framework to generate adversarial examples in one of the most challenging black-box settings where the adversary can only generate a small number of queries, each of them returning the top-1 label of the classifier. Our framework is based on the observation that the decision boundary of deep networks usually has a small mean curvature in the vicinity of data samples. We propose an effective iterative algorithm to generate query-efficient black-box perturbations with small p norms for p \u2265 1, which is confirmed via experimental evaluations on state-of-the-art natural image classifiers. Moreover, for p = 2, we theoretically show that our algorithm actually converges to the minimal 2perturbation when the curvature of the decision boundary is bounded. We also obtain the optimal distribution of the queries over the iterations of the algorithm. Finally, experimental results confirm that our principled black-box attack algorithm performs better than state-of-the-art algorithms as it generates smaller perturbations with a reduced number of queries. 1\n\nIntroduction\n\nIt has become well known that deep neural networks are vulnerable to small adversarial perturbations, which are carefully designed to cause miss-classification in state-ofthe-art image classifiers [29]. Many methods have been proposed to evaluate adversarial robustness of classifiers in the white-box setting, where the adversary has full access to the target model [15,27,3]. However, the robustness of classifiers in black-box settings -where the adversary has only access to the output of the classifier -is of high relevance in many real-world applications of deep neural net- 1 The code of GeoDA is available at https://github.com/ thisisalirah/GeoDA. works such as autonomous systems and healthcare, where it poses serious security threats. Several black-box evaluation methods have been proposed in the literature. Depending on what the classifier gives as an output, black-box evaluation methods are either score-based [28,6,20] or decisionbased [4,2,22].\n\nIn this paper, we propose a novel geometric framework for decision-based black-box attacks in which the adversary only has access to the top-1 label of the target model. Intuitively small adversarial perturbations should be searched in directions where the classifier decision boundary comes close to data samples. We exploit the low mean curvature of the decision boundary in the vicinity of the data samples to effectively estimate the normal vector to the decision boundary. This key prior permits to considerably reduces the number of queries that are necessary to fool the blackbox classifier. Experimental results confirm that our Geometric Decision-based Attack (GeoDA) outperforms stateof-the-art black-box attacks, in terms of required number of queries to fool the classifier. Our main contributions are summarized as follows:\n\n\u2022 We propose a novel geometric framework based on linearizing the decision boundary of deep networks in the vicinity of samples. The error for the estimation of the normal vector to the decision boundary of classifiers with flat decision boundaries, including linear classifiers, is shown to be bounded in a non-asymptotic regime. The proposed framework is general enough to be deployed for any classifier with low curvature decision boundary.\n\n\u2022 We demonstrate how our proposed framework can be used to generate query-efficient p black-box perturbations. In particular, we provide algorithms to generate perturbations for p \u2265 1, and show their effectiveness via experimental evaluations on state-of-the-art natural image classifiers. In the case of p = 2, we also prove that our algorithm converges to the minimal 2perturbation. We further derive the optimal number of queries for each step of the iterative search strategy.\n\n\u2022 Finally, we show that our framework can incorporate different prior information, particularly transferability and subspace constraints on the adversarial perturbations. We show theoretically that having prior information can bias the normal vector estimation search space towards a more accurate estimation.\n\n\nRelated work\n\nAdversarial examples can be crafted in white-box setting [15,27,3], score-based black-box setting [28,6,20] or decision-based black-box scenario [4,2,22]. The latter settings are obviously the most challenging as little is known about the target classification settings. Yet, there are several recent works on the black-box attacks on image classifiers [20,21,32]. However, they assume that the loss function, the prediction probabilities, or several top sorted labels are available, which may be unrealistic in many real-world scenarios. In the most challenging settings, there are a few attacks that exploit only the top-1 label information returned by the classifier, including the Boundary Attack (BA) [2], the HopSkipJump Attack (HSJA) [5], the OPT attack [8], and qFool [22]. In [2], by starting from a large adversarial perturbation, BA can iteratively reduce the norm of the perturbation. In [5], the authors provided an attack based on [2] that improves the BA taking the advantage of an estimated gradient. This attack is quite query efficient and can be assumed as the state-of-the-art baseline in the black-box setting. In [8], an optimization-based hard-label black-box attack algorithm is introduced with guaranteed convergence rate in the hard-label black-box setting which outperforms the BA in terms of number of queries. Closer to our work, in [22], a heuristic algorithm based on the estimation of the normal vector to decision boundary is proposed for the case of 2 -norm perturbations.\n\nMost of the aforementioned attacks are however specifically designed for minimizing perturbation metrics such 2 and \u221e norms, and mostly use heuristics. In contrast, we introduce a powerful and generic framework grounded on the geometric properties of the decision boundary of deep networks, and propose a principled approach to design efficient algorithms to generate general p -norm perturbations, in which [22] can be seen as a special case. We also provide convergence guarantees for the 2 -norm perturbations. We obtained the optimal distribution of queries over iterations theoretically as well which permits to use the queries in a more efficient manner. Moreover, the parameters of our algorithm are further determined via empirical and theoretical analysis, not merely based on heuristics as done in [22].\n\n\nProblem statement\n\nLet us assume that we have a pre-trained L-class classifier with parameters \u03b8 represented as f :\nR d \u2192 R L , where x \u2208 R d is the input image andk(x) = argmax k f k (x) is the top-1 classification label where f k (x) is the k th compo- nent of f (x)\ncorresponds to the k th class. We consider the non-targeted black-box attack, where an adversary without any knowledge on \u03b8 computes an adversarial perturbation v to change the estimated label of an image x to any incorrect label, i.e.,k(x + v) =k(x). The distance metric D(x, x + v) can be any function including the p norms. We assume a general form optimization problem in which the goal is to fool the classifier while\nD(x, x + v) is mini- mized as: min v D(x, x + v) s.t.k(x + v) =k(x).(1)\nFinding a solution for (1) is a hard problem in general. To obtain an efficient approximate solution, one can try to estimate the point of the classifier decision boundary that is the closest to the data point x. Crafting an small adversarial perturbation then consists in pushing the data point beyond the decision boundary in the direction of its normal. The normal to the decision boundary is thus critical in a geometry-based attack. While it can be obtained using back-propagation in white box settings (e.g., [27]), its estimation in black-box settings becomes challenging.\n\nThe key idea here is to exploit the geometric properties of the decision boundary in deep networks for effective estimation in black-box settings. In particular, it has been shown that the decision boundaries of the state-of-theart deep networks have a quite low mean curvature in the neighborhood of data samples [12]. Specifically, the decision boundary at the vicinity of a data point x can be locally approximated by a hyperplane passing through a boundary point x B close to x, with a normal vector w [14,13]. Thus, by exploiting this property, the optimization problem in (1) can be locally linearized as:\nmin v D(x, x + v) s.t. w T (x + v) \u2212 w T x B = 0(2)\nTypically, x B is a point on the boundary, which can be found by binary search with a small number of queries.\n\nHowever, solving the problem (2) is quite challenging in black-box settings as one does not have any knowledge about the parameters \u03b8 and can only access the top-1 labelk(x) of the image classifier. A query is a request that results in the top-1 label of an image classifier for a given input, which prevents the use of zero-order black box optimization methods [34,33] that need more information to compute adversarial perturbations. The goal of our method is to estimate the normal vector to the decision boundary w resorting to geometric priors with a minimal number of queries to the classifier.\n\n\nThe estimator\n\nWe introduce an estimation method for the normal vector of classifiers with flat decision boundaries. It is worth noting that the proposed estimation is not limited to deep networks and applies to any classifier with low mean curvature boundary. We denote the estimate of the vector w normal to the flat decision boundary in (2) with\u0175 N when N queries are used. Without loss of generality, we assume that the boundary point x B is located at the origin. Thus, according to (2), the decision boundary hyperplane passes through the origin and we have w T x = 0 for any vector x on the decision boundary hyperplane. In order to estimate the normal vector to the decision boundary, the key idea is to generate N samples \u03b7 i , i \u2208 {1, . . . , N } from a multivariate normal distribution \u03b7 i \u223c N (0, \u03a3). Then, we query the image classifier N times to obtain the top-1 label output for each x B + \u03b7 i , \u2200i \u2208 N . For a given data point x, if w T x \u2264 0, the label is correct; if w T x \u2265 0, the classifier is fooled. Hence, if the generated perturbations are adversarial, they belong to the set\nS adv = {\u03b7 i |k(x B + \u03b7 i ) =k(x)} = {\u03b7 i | w T \u03b7 i \u2265 0}.(3)\nSimilarly, the perturbations on the other side of the hyperplane, which lead to correct classification, belong to the set\nS clean = {\u03b7 i |k(x B + \u03b7 i ) =k(x)} = {\u03b7 i | w T \u03b7 i \u2264 0}.(4)\nThe samples in each of the sets S adv and S clean can be assumed as samples drawn from a hyperplane (w T x = 0) truncated multivariate normal distribution with mean 0 and covariance matrix \u03a3. We define the PDF of the d dimensional zero mean multivariate normal distribution with covariance matrix \u03a3 as \u03c6 d (\u03b7|\u03a3). We define \u03a6 d (b|\u03a3) = \u221e b \u03c6 d (\u03b7|\u03a3)d\u03b7 as cumulative distribution function of the univariate normal distribution. Lemma 1. Given a multivariate Gaussian distribution N (0, \u03a3) truncated by the hyperplane w T x \u2265 0, the mean \u00b5 and covariance matrix R of the hyperplane truncated distribution are given by:\n\u00b5 = c 1 \u03a3w (5)\nwhere c 1 = (\u03a6 d (0)) \u22121 \u03c6 d (0) and the covariance matrix\nR = \u03a3 \u2212 \u03a3ww T \u03a3(\u03a6 d (0) 2 \u03b3 2 ) \u22121 \u03c6 d (0))d 2 (0) in which \u03b3 = (w T \u03a3w) 1 2 [30].\nAs it can be seen in (5), the mean is a function of both the covariance matrix \u03a3 and w. Our ultimate goal is to estimate the normal vector to the decision boundary. In order to recover w from \u00b5, a sufficient condition is to choose \u03a3 to be a full rank matrix.\n\nGeneral case We first consider the case where no prior information on the search space is available. The matrix \u03a3 = \u03c3I can be a simple choice to avoid unnecessary computations. The direction of the mean of the truncated distribution is an estimation for the direction of hyperplane normal vector as \u00b5 = c 1 \u03c3w. The covariance matrix of the truncated distribution is\nR = \u03c3I + c 2 ww T where c 2 = \u2212\u03c3 2 (\u03a6 d (0)) \u22122 \u03c6 2 d (0)\n. As the samples in both of the sets S adv and S clean are hyperplane truncated Gaussian distributions, the same estimation can be applied for the samples in the set S clean as well. Thus, by multiplying the samples in S clean by \u22121 and we can use them to approximate the desired gradient to have a more efficient estimation. Hence, the problem is reduced to the estimation of the mean of the N samples drawn from the hyperplane truncated distribution with mean \u00b5 and covariance matrix R. As a result, the estimator\u03bc N of \u00b5 with N samples is\u03bc N = 1\nN N i=1 \u03c1 i \u03b7 i , where \u03c1 i = 1k(x B + \u03b7 i ) =k(x) \u22121k(x B + \u03b7 i ) =k(x).(6)\nThe normalized direction of the normal vector of the boundary can be obtained as:\u0175\nN =\u03bc N \u03bc N 2(7)\nPerturbation priors We now consider the case where priors on the perturbations are available. In black-box settings, having prior information can significantly improve the performance of the attack. Although the attacker does not have access to the weights of the classifier, it may have some prior information about the data, classifier, etc. [21]. Here, using \u03a3, we can capture the prior knowledge for the estimation of the normal vector to the decision boundary. In the following, we unify the two common priors in our proposed estimator. In the first case, we have some prior information about the subspace in which we search for normal vectors, we can incorporate such information into \u03a3 to have a more efficient estimation. For instance, deploying low frequency sub-space R m in which m d, we can generate a rank m covariance matrix \u03a3. Let us assume that S = {s 1 , s 2 , ..., s m } is an orthonormal Discrete Cosine Transform (DCT) basis in the m-dimensional subspace of the input space [16]. In order to generate the samples from this low dimensional subspace, we use the following covariance matrix:\n\u03a3 = 1 m m i=1 s i s T i .(8)\nThe normal vector of the boundary can be obtained by plugging the modified \u03a3 in (5). Second, we consider transferability priors. It has been observed that adversarial perturbations well transfer across different trained models [31,26,9]. Now, if the adversary further has full access to another model T , yet different than the target black-box model T , it can take advantage of the transferability properties of adversarial perturbations. For a given datapoint, one can obtain the normal vector to the decision boundary in the vicinity of the datapoint for T , and bias the normal vector search space for the black-box classifier. Let us denote the transferred direction with unit-norm vector g. By incorporating this vector into \u03a3, we can bias the search space as:\n\u03a3 = \u03b2I + (1 \u2212 \u03b2)gg T(9)\nwhere \u03b2 \u2208 [0, 1] adjusts the trade-off between exploitation and exploration. Depending on how confident we are about the utility of the transferred direction, we can adjust its contribution by tuning the value of \u03b2. Substituting (9) into (5), after normalization to c 1 , one can get\n\u00b5 = \u03b2w + (1 \u2212 \u03b2)gg T w,(10)\nwhere the first term is the estimated normal vector to the boundary and the second term is the projection of the estimated normal vector on the transferred direction g. Having incorporated the prior information into \u03a3, one can generate perturbations \u03b7 i \u223c N (0, \u03a3) with the modified \u03a3 in an effective search space, which leads to a more accurate estimation of normal to the decision boundary.\n\nEstimator bound Finally, we are interested in quantifying the number of samples that are necessary for estimating the normal vectors in our geometry inspired framework. Given a real i.i.d. sequence, using the central limit theorem, if the samples have a finite variance, an asymptotic bound can be provided for the estimate. However, this bound is not of our interest as it is only asymptotically correct. We are interested in bounds of similar form with non-asymptotic inequalities as the number of queries is limited [23,17].\nLemma 2.\nThe mean estimation\u03bc N deployed in (9) obtained from N multivariate hyperplane truncated Gaussian Algorithm 1: p GeoDA (with optimal query distribution) for p > 1 1 Inputs: Original image x, query budget N , \u03bb, number of iterations T . 2 Output: Adversarial example x T . 3 Obtain the optimal query distribution N * t , \u2200t by (19). 4 Find a starting point on the boundary x 0 .\n5 for t = 1 : T do 6 Estimate normal\u0175 N * t at x t\u22121 by N * t queries. 7\nObtain v t according to (13).\n8r t \u2190 min{r > 0 :k(x + r v t ) =k(x)} 9 x t \u2190 x +r t\u0175N * t queries satisfies the probability P \u03bc N \u2212 \u00b5 \u2264 Tr(R) N + 2\u03bb max log(1/\u03b4) N \u2265 1\u2212\u03b4 (11)\nwhere Tr(R) and \u03bb max denote the trace and largest eigenvalue of the covariance matrix R, respectively.\n\nProof. The proof can be found in Appendix A.\n\nThis bound will be deployed in sub-section 5.1 to compute the optimal distribution of queries over iterations.\n\n\nGeometric decision-based attacks (GeoDA)\n\nBased on the estimator provided in Section 4, one can design efficient black-box evaluation methods. In this paper, we focus on the minimal p -norm perturbations, i.e., D(x, x + v) = v p . We first describe the general algorithm for p perturbations, and then provide algorithms to find black-box perturbations for p = 1, 2, \u221e. Furthermore, for p = 2, we prove the convergence of our method. The linearized optimization problem in (2) can be re-written as\nmin v v p s.t. w T (x + v) \u2212 w T x B = 0.(12)\nIn the black-box setting, one needs to estimate x B and w in order to solve this optimization problem. The boundary point x B can be found using a similar approach as [22]. Having x B , one then use the process described in Section 4 to compute the estimator of w -i.e.,\u0175 N1 -by making N 1 queries to the classifier. In the case of p = 2, the estimated direction\u0175 N is indeed the direction of the minimal perturbation. This process is depicted in Fig. 1.\n\nIf the curvature of the decision boundary is exactly zero, the solution of this problem gives the direction of the minimal p perturbation. However, for deep neural networks, even if N \u2192 \u221e, the obtained direction is not completely aligned with the minimal perturbation as these networks still have a small yet non-zero curvature (see Fig. 4c). Nevertheless, to overcome this issue, the solution v * of (12) can be used to obtain a boundary point x 1 = x +r 1 v * to the original image x than x 0 , for an appropriate value of r 1 > 0. For notation consistency, we define x 0 = x B . Now, we can again solve (12) for the new boundary point x 1 . Repeating this process results in an iterative algorithm to find the minimal p perturbation, where each iteration corresponds to solving (12) once. Formally, for a given image x, let x t be the boundary point estimated in the iteration t\u22121. Also, let N t be the number of queries used to estimate the normal to the decision boundary\u0175 Nt at the iteration t. Hence, the (normalized) solution to (12) in the t-th iteration, v t , can be written in closed-form as:\nv t = 1 \u0175 Nt p p\u22121 sign(\u0175 Nt ),(13)for p \u2208 [1, \u221e),\nwhere is the point-wise product. For the particular case of p = \u221e, the solution of (13) is simply reduced to: v t = sign(\u0175 Nt ).\n\nThe cases of the p = 1, 2 are presented later. In all cases, x t is then updated according to the following update rule:\nx t = x +r t v t(15)\nwherer t can be found using an efficient line search along v t . The general algorithm is summarized in Alg. 1.\n\n\n2 perturbation\n\nIn the 2 case, the update rule of (15) is reduced to x t = x +r t\u0175Nt wherer t is the 2 distance of x to the decision boundary at iteration t. We propose convergence guarantees and optimal distribution of queries over the successive iterations for this case. Convergence guarantees We prove that GeoDA converges to the minimal 2 perturbation given that the curvature of the decision boundary is bounded. We define the curvature of the decision boundary as \u03ba = 1 R , where R is the radius of the largest open ball included in the region that intersects with the boundary B [12]. In case N \u2192 \u221e, then r t \u2192 r t where r t is assumed as exact distance required to push the image x towards the boundary at iteration t with direction v t . The following Theorem holds: Theorem 1. Given a classifier with decision boundary of bounded curvature with \u03bar < 1, the sequence {r t } generated by Algorithm 1 converges linearly to the minimum 2 distance r since we have: lim\nt\u2192\u221er t+1 \u2212 r r t \u2212 r = \u03bb(16)\nwhere \u03bb < 1 is the convergence rate.\n\nProof. The proof can be found in Appendix B.\n\nOptimal query distribution In practice, however, the number of queries N is limited. One natural question is how should one choose the number of queries in each iteration of GeoDA. It can be seen in the experiments that allocating a smaller number of queries for the first iterations and then increasing it in each iteration can improve the convergence rate of the GeoDA. At early iterations, noisy normal vector estimates are fine because the noise is smaller relative to the potential improvement, whereas in later iterations noise has a bigger impact. This makes the earlier iterations cheaper in terms of queries, potentially speeding up convergence [11].\n\nWe assume a practical setting in which we have a limited budget N for the number of queries as the target system may block if the number of queries increases beyond a certain threshold [7]. The goal is to obtain the optimal distribution of the queries over the iterations. Theorem 2. Given a limited query budget N , the bounds for the GeoDA 2 perturbation error for total number of iterations T can be obtained as:\n\u03bb T (r 0 \u2212 r) \u2212 e(N ) \u2264r t \u2212 r \u2264 \u03bb T (r 0 \u2212 r) + e(N ) (17) where e(N ) = \u03b3 T i=1 \u03bb T \u2212i ri \u221a\nNi is the error due to limited number of queries, \u03b3 = Tr(R) + 2\u03bb max log(1/\u03b4) and N t is the number of queries to estimate the normal vector to the boundary at point x t\u22121 , and r 0 = x \u2212 x 0 .\n\nProof. The proof can be found in Appendix C.\n\nAs in (17), the error in the convergence is due to two factors: (i) curvature of the decision boundary (ii) limited number of queries. If the number of iterations increases, the effect of the curvature can vanish. However, the term \u03b3 ri \u221a Nt is not small enough as the number of queries is finite. Having unlimited number of the queries, the error term due to queries can vanish as well. However, given a limited number of queries, what should be the distribution of the queries to alleviate such an error? We define the following optimization problem: min\nN1,...,N T T i=1 \u03bb \u2212i r i \u221a N i s.t. T i=1 N i \u2264 N(18)\nwhere the objective is to minimize the error e(N ) while the query budget constraint is met over all iterations. Theorem 3. The optimal numbers of queries for (18) in each iteration form geometric sequence with the common ratio\nN * t+1 N * t \u2248 \u03bb \u2212 2 3 , where 0 \u2264 \u03bb \u2264 1. Moreover, we have N * t \u2248 \u03bb \u2212 2 3 t T i=1 \u03bb \u2212 2 3 i N.(19)\nProof. The proof can be found in Appendix D. \n\n\n1 perturbation (sparse case)\n\nThe framework proposed by GeoDA is general enough to find sparse adversarial perturbations in the black-box setting as well. The sparse adversarial perturbations can be computed using the following optimization problem with box constraints as:\nmin v v 1 s.t. w T (x + v) \u2212 w T x B = 0 l x + v u(20)\nIn the box constraint l x + v u, l and u denote the lower and upper bounds of the values of x + v. We can estimate the normal vector\u0175 N and the boundary point x B similarly to the 2 case with N queries. Now, the decision boundary B is approximated with the hyperplane {x :\u0175 T N (x \u2212 x B ) = 0}. The goal is to find the top-k coordinates of the normal vector\u0175 N with minimum k and pushing them to extreme values of the valid range depending on the sign of the coordinate until it hits the approximated hyperplane. In order to find the minimum k, we deploy binary search for a d-dimensional image. Here, we just consider one iteration for the sparse attack., while the initial point of the sparse case is obtained using the GeoDA for 2 case. The detailed Algorithm for the sparse version of GeoDA is given in Algorithm 2.\n\n\nExperiments\n\n\nSettings\n\nWe evaluate our algorithms on a pre-trained ResNet-50 [18] with a set X of 350 correctly classified and randomly selected images from the ILSVRC2012's validation set [10]. All the images are resized to 224 \u00d7 224 \u00d7 3.\n\nTo evaluate the performance of the attack we deploy the median of the p norm for p = 2, \u221e distance over all tested samples, defined by median turbations, we measure the performance by fooling rate defined as |x \u2208 X :k(x) =k(x adv )|/|X |. In evaluation of the sparse GeoDA, we define sparsity as the percentage of the perturbed coordinates of the given image\n\n\nPerformance analysis\n\nBlack-box attacks for p norms. We compare the performance of the GeoDA with state of the art attacks for p norms. There are several attacks in the literature including Boundary attack [2], HopSkipJump attack [5], qFool [22], and OPT attack [8]. In our experiments, we compare GeoDA with Boundary attack, qFool and HopSkipJump attack. We do not compare our algorithm with OPT attack as HopSkipJump already outperforms it considerably [5]. In our algorithm, the optimal distribution of the queries is obtained for any given number of queries for 2 case. The results for 2 and \u221e for different numbers of queries is    Table 1. GeoDA can outperform the-state-ofthe-art both in terms of smaller perturbations and number of iterations, which has the benefit of parallelization. In particular, the images can be fed into multiple GPUs with larger batch size. In Fig. 2a, the 2 norm of GeoDA, Boundary attack and HopSkipJump are compared. As shown, GeoDA can outperform the HopSkipJump attack especially when the number of queries is small. By increasing the number of queries, the performance of GeoDA and Hop-SkipJump are getting closer. In Fig. 2b, the number of iterations versus the number of queries for different algorithms are compared. As depicted, GeoDA needs fewer iterations compared to HopSkipJump and BA when the number of queries increases. Thus, on the one hand GeoDA generates smaller 2 perturbations compared to the HopSkipJump attack when the number of queries is small, on the other hand, it saves significant computation time due to parallelization of queries fed into the GPU. Now, we evaluate the performance of GeoDA for generating sparse perturbations. In Fig. 2c, the fooling rate versus sparsity is depicted. In experiments, we observed that instead of using the boundary point x B in the sparse GeoDA, the performance of the algorithm can be improved by further moving towards the other side of the hyperplane boundary. Thus, we use x B + \u03b6(x B \u2212 x), where \u03b6 \u2265 0. The parameter \u03b6 can adjust the trade-off between the fooling rate and the sparsity. It is observed that the higher the value for \u03b6, the higher the fooling rate and the sparsity and vice versa. In other words, choosing small values for \u03b6 produces sparser adversarial examples; however, it decreases the chance that it is an adversarial example for the actual boundary. In Fig. 2c, we depicted the trade-off between fooling rate and sparsity by increasing the value for \u03b6 for different query budgets. The larger the number of queries, the closer the initial point to the original image, and also the better our algorithm performs in generating sparse adversarial examples. In Table 2, the sparse GeoDA is compared with the white-box attack SparseFool. We show that with a limited number of queries, GeoDA can generate sparse perturbations with acceptable fooling rate with sparsity of about 3 percent with respect to the white-box attack Sparse-Fool. The adversarial perturbations generated by GeoDA for p norms are shown in Fig. 3 and the effect of different norms can be observed. Incorporating prior information. Here, we evaluate the methods proposed in Section 4 to incorporate prior information in order to improve the estimation of the normal vector to the decision boundary. As sub-space priors, we deploy the DCT basis functions in which m low frequency subspace directions are chosen [25]. As shown in Fig. 5, biasing the search space to the DCT sub-space can reduce the 2 norm of the perturbations by approximately 27% compared to the full-space case. For transferrability, we obtain the normal vector of the given image using the white box attack DeepFool [27] on a ResNet-34 classifier. We bias the search space for normal vector estimation as described in Section 4. As it can be seen in Fig. 5, prior information can improve the normal vector estimation significantly.\n\n\nEffect of hyper-parameters on the performance\n\nInstead of throwing out the gradient obtained from the previous iterations, we can take advantage of them in next iterations as well. To do this, we can bias the covariance matrix \u03a3 towards the gradient obtained from the previous iteration. The other way is to simply have a weighted average of the estimated gradient and previous gradients. As a general rule, \u03b2 given in (10) should be chosen in such a way that the estimated gradient in recent iterations get more weights compared to the first iterations.  In practice, we need to choose \u03c3 such that the locally flat assumption of the boundary is preserved. Upon generating the queries at boundary point x B to estimate the direction of the normal vector as in (7), the value for \u03c3 is chosen in such a way that the number of correctly classified images and adversarial images on the boundary are almost the same. In Fig. 4a, the effect of variance \u03c3 of added Gaussian perturbation on the number of correctly classified queries on the boundary point is illustrated. We obtained a random point x B on the decision boundary of the image classifier and query the image classifier 1000 times. As it can be seen, the variance \u03c3 is too small, none of the queries is correctly classified as the point x B is not exactly on the boundary. It is worth mentioning that in binary search we choose the point on the adversarial side as a boundary point. On the other hand if the variance is too high, all the images are classified as adversarial since they are highly perturbed.\n\nIn order to obtain the optimal query distribution for a given limited budget N , the values for \u03bb and T should be given. Having fixed \u03bb, if T is large, the number of queries allocated to the first iteration may be too small. To address this, we consider a fixed number of queries for the first iteration as N * 1 = 70. Thus, having fixed \u03bb, a reasonable choice for T can be obtained by solving (19) for T . Based on (19), if \u03bb \u2192 0, all the queries are allocated to the last iteration and when \u03bb = 1, the query distribution is uniform. A value between these two extremes is desirable for our algorithm. To obtain this value, we run our algorithm for different \u03bb for only 10 images different from X . As it can be seen in Fig. 4b, the algorithm has its worst performance when \u03bb is close to the two extreme cases: single iteration (\u03bb \u2192 0) and uniform distribution (\u03bb = 1). We thus choose the value \u03bb = 0.6 for our experiments. Finally, in Fig. 4c, the comparison between three different query distributions is shown. The optimal query distribution achieves the best performance while the single iteration preforms worst. Actually, the fact that the single iteration performs worst is reflected in our proposed bound in (17) as even with infinite number of queries it can not do better than \u03bb(r 0 \u2212r). Indeed the effect of curvature can be addressed only by increasing the number of iterations.\n\n\nConclusion\n\nIn this work, we propose a new geometric framework for designing query-efficient decision-based black-box attacks, in which the attacker only has access to the top-1 label of the classifier. Our method relies on the key observation that the curvature of the decision boundary of deep networks is small in the vicinity of data samples. This permits to estimate the normals to the decision boundary with a small number of queries to the classifier, hence to eventually design query-efficient p -norm attacks. In the particular case of 2 -norm attacks, we show theoretically that our algorithm converges to the minimal adversarial perturbations, and that the number of queries at each step of the iterative search can be optimized mathematically. We finally study GeoDA through extensive experiments that confirm its superior performance compared to state-of-the-art black-box attacks.\n\n\nAppendix\n\n\nA. Proof of Lemma 2\n\nProof. Let X i be a random vector taking values in R d with mean \u00b5 = E[X] and covariance matrix R = E(X \u2212 \u00b5)(X \u2212 \u00b5) T . Given the X 1 , . . . , X n , the goal is to estimate \u00b5. If X has a multivariate Gaussian or sub-Gaussian distribution, the sample mean\u03bc N = 1 N N i=1 X i is the result of MLE estimation, which satisfies, with probability at\nleast 1 \u2212 \u03b4 ||\u03bc N \u2212 \u00b5|| \u2264 Tr(R) N + 2\u03bb max log(1/\u03b4) N(21)\nwhere Tr(R) and \u03bb max denote the trace and largest eigenvalue of the covariance matrix R, respectively [17]. We already know the truncated normal distribution mean and variance. Although, the truncated distribution is similar to Gaussian, we need to prove that it satisfies the sub-Gaussian distribution property so that we can use the bound in (21). The truncated distribution with mean \u00b5 and covariance matrix R is a sub-Gaussian distribution. A given distribution is sub-Gaussian if for all unit vectors {v \u2208 R d : ||v|| = 1} [23], the following condition holds\nE [exp(\u03bb v, X \u2212 \u00b5 )] \u2264 exp(c\u03bb 2 v, \u03a3v ).(22)\nAssuming the hyperplane w T X \u2265 0 truncated Normal distribution with mean zero and covariance matrix \u03a3, the left hand side of the (13) can be computed as:\nE [exp(\u03bb v, X \u2212 \u00b5 )] = H + exp(\u03bbv T X)\u03c6 d (X|\u03a3)dX(23)\nwhere H + = {X \u2208 R d : w T X \u2265 0}. Since R is a symmetric, positive definite matrix, using Cholesky decomposition we can have R \u22121 = \u03a8 T \u03a8 where \u03a8 is a non-singular, upper triangular matrix [19]. By transforming the variables, we have Y = \u03a8X. Using Y , with some manipulation as in [30], one can get\nE [exp(\u03bb v, X \u2212 \u00b5 )] = exp 1 2 \u03bb 2 v T \u03a3v \u03a6 \u03bbw T \u03a3v \u03c3(24)\nand \u03c3 2 = w T \u03a3w, and \u03a6[.] is the cumulative distribution function of the univariate normal distribution. Plugging \u03a3 = I, one can get\nE [exp(\u03bb v, X )] = exp 1 2 \u03bb 2 \u03a6 \u03bbw T v \u2264 exp 1 2 \u03bb 2 ,(25)\nwhere the inequality is valid due to the fact that the CDF function is equal to 1 in the maximum. Comparing with the right hand side of the (13):\nexp 1 2 \u03bb 2 \u2264 exp 1 2 c\u03bb 2 ,(26)\none can see that it is valid for any c \u2265 1. Thus, the truncated Normal distribution is a sub-Gaussian distribution.\n\nThe above proof is consistent with our intuition as the truncated Gaussian has the tails approaching zero at least as fast as exponential distribution. The truncated part of the Gaussian is already equal to zero so there is no chance for being a heavy tailed distribution. Thus, the bound provided in (21) can be valid for our problem [23].\n\nSince the covariance matrix R is unknown, we need to find bounds for Tr(R) and \u03bb max as well. It can easily be obtained that\nTr(R) = d + c 2 w T w = d + c 2(27)\nIn order to obtain the maximum eigenvalue of the R, we use Weyl's inequality to have an upper bound for largest eigenvalue of the covariance matrix as [24]:\n\u03bb max (A + B) \u2264 \u03bb max (A) + \u03bb max (B)(28)\nThe largest eigenvalue for the identity matrix I is 1. For the rank-1 matrix c 2 ww T which is the outer product of the normal vector is given by:\n\u03bb max (c 2 ww T ) = c 2 Tr(ww T ) = c 2 w T w = c 2(29)\nwhich immediately results in \u03bb max (R) \u2264 1 + c 2 . Substituting the above values to the (12), the sample mean \u00b5 N = 1 N N i=1 X i is the result of MLE estimation, which satisfies, with probability at least 1 \u2212 \u03b4\n\u03bc N \u2212 \u00b5 \u2264 d + c 2 N + 2(1 + c 2 ) log(1/\u03b4) N(30)\nThis bound can provide an upper bound with probability at least 1 \u2212 \u03b4 for the error of the sample mean while getting N queries from the neural network.\nR sin(\u03b8 t ) = r t sin(\u03b8 t+1 )(31)\n\nB. Proof of Theorem 1\n\nIn the following subsections, we consider two cases for the curvature of the boundary.\n\n\nConvex Curved Bounded Boundary\n\nWe assume that the curvature of the boundary is convex as given in Fig. 6. As given in [12], if \u03b8 t satisfies the two Gradient Direction Figure 6: Convex decision boundary with bounded curvature.\n\nassumptions tan 2 (\u03b8 t ) \u2264 0.2R/r and r/R < 1, the value for x t \u2212 x 0 2 = r t is given as follows:\nr t = \u2212(R \u2212 r) cos(\u03b8 t ) + (R \u2212 r) 2 cos 2 (\u03b8 t ) + 2Rr \u2212 r 2 (32) where x t+1 \u2212 x 0 2 = r t+1\ncan be obtained in a similar way. It can be observed that the value of the r t is an increasing function of the \u03b8 t because:\n\u2202r t \u2202\u03b8 t = (R \u2212 r) sin(\u03b8 t ) \u2212 (R \u2212 r) 2 cos(\u03b8 t ) sin(\u03b8 t ) (R \u2212 r) 2 cos 2 (\u03b8 t ) + 2Rr \u2212 r 2 ,(33)\nSetting \u2202rt \u2202\u03b8t > 0, with some manipulations one can get 2R > r which shows that r t is an increasing function of the \u03b8 t . Thus, if we can show that \u03b8 t > \u03b8 t+1 , it means that r t > r t+1 which means that r t can converge to r. Here, we assume that the given image is in the vicinity of the boundary r/R < 1. The line connecting point o to x 0 intersects the two parallel lines. Based on the law of sines, one can get Since r t < R, one can conclude that \u03b8 t > \u03b8 t+1 using the sines law. Thus, as r t is an increasing function of \u03b8 t , we can get r t+1 < r t . Thus, after several iterations, the following update rule\nx t = x 0 + r t\u0175Nt(34)\nconverges to the minimum perturbation r.\n\nApplying the sine law for k iterations, one can get the following equation using (31):\nsin(\u03b8 t ) = t i=0 r i R t sin(\u03b8 0 )(35)\nWe know that r t < R and in each iteration, it gets smaller and smaller. Thus, for the convergence, we consider the worst case. We know that max i=0,1,...,t {r i } = r t . Thus, To Gradient Direction Figure 7: Concave decision boundary with bounded curvature.\n\nbound this, we can have:\nsin(\u03b8 t+K ) = K k=0 r t+k R K sin(\u03b8 t ) \u2264 ( r t R ) K sin(\u03b8 t ) (36)\nwhere can be reduced to\nsin(\u03b8 t+K ) \u2264 ( r t R ) K sin(\u03b8 t )(37)\nThis shows that sin(\u03b8 t+K ) converges to zero exponentially since r t < R. Thus, \u03b8 t+k goes to zero which results that the in coinciding the r t and r in the same magnitude. Thus, we have lim k\u2192\u221e r t+k = r\n\nWe already know that\nr t+1 = \u2212 (R \u2212 r) cos(\u03b8 t+1 ) + (R \u2212 r) 2 cos 2 (\u03b8 t+1 ) + 2Rr \u2212 r 2 ,(39)\nConsidering the cosine law, based on the figure, we can see that\nr 2 t = (R + r) 2 + R 2 \u2212 2R(R + r) cos(\u03b8 t+1 )(40)\nBy combining the above equations and eliminating the cos(\u03b8 t+1 ), one can get:\nr t+1 = \u2212(R \u2212 r) (R + r) 2 + R 2 \u2212 r 2 t 2R(R + r) + (R \u2212 r) 2 ((R + r) 2 + R 2 \u2212 r 2 t ) 2 4R 2 (R + r) 2 + 2Rr \u2212 r 2 ,(41)\nPlugging (41) into the following limit,\nlim t\u2192\u221e r t+1 \u2212 r r t \u2212 r(42)\nfor t \u2192 \u221e, we get 0 0 . Thus, using the L'Hospital's Rule, we take the derivative of the numerator and the denominator as:\n\u2202r t+1 \u2202r t = \u2212 r t (R \u2212 r) R(R + r) + ((R + r) 2 + R 2 \u2212 r 2 t )r t 2 (R\u2212r) 2 ((R+r) 2 +R 2 \u2212r 2 t ) 2 4R 2 (R+r) 2 + 2Rr \u2212 r 2 (R \u2212 r) 2 R 2 (R + r) 2(43)\nHaving t \u2192 \u221e, we can get r t \u2192 r, since we haver t \u2192 r t , thus:\nlim t\u2192\u221er t+1 \u2212 r r t \u2212 r = r 2 (R \u2212 r) R 2 (R + r) = \u03bb < 1(44)\nAs r < R, the rate of convergence \u03bb \u2208 (0, 1) which completes the proof.\n\n\nConcave Curved Bounded Boundary\n\nAs in [12], the value for x t \u2212x 0 2 = r t is given as follows:\nr t = (R + r) cos(\u03b8 t ) \u2212 (R + r) 2 cos 2 (\u03b8 t ) \u2212 2Rr \u2212 r 2 (45) where x t+1 \u2212 x 0 2 = r t+1\ncan be obtained in a similar way. It can easily be seen that the \u03b8 t > \u03b8 t+1 . Assuming r/R < 1, r t is a decreasing function with respect to \u03b8 t which results in r t < r t+1 . Similar proof of convergence can be obtained for this case as well.\n\n\nC. Proof of Theorem 2\n\nGiven the point r t\u22121 , the goal is to find the estimate of ther t with limited query. Assuming the normalized version of the true gradient w t = \u00b5 t / \u00b5 t 2 , we have\n\u0175 Nt \u2212 w t \u2264 \u03b3 \u221a N t(46)\nwhere \u03b3 = Tr(R) + 2\u03bb max log(1/\u03b4),\u0175 Nt is the estimated gradient at iteration t and N t is the number of queries to estimate the gradient at point x t\u22121 . Based on the reverse triangle inequality x \u2212 y \u2264 x \u2212 y , we can have\n\u0175 Nt \u2212 1 \u2264 \u0175 Nt \u2212 w t \u2264 \u03b3 \u221a N t .(47)\nMultiplying by r t , we have:\nr t \u2212 \u03b3r t \u221a N t \u2264r t \u2264 r t + \u03b3r t \u221a N t .(48)\nwherer t = r t \u0175 Nt . Here, we conduct the analysis in the limit sense and we observe in the simulations that it is valid in limited iterations as well. Given r t\u22121 , for large t, we have:\nr t \u2212 r \u2248 \u03bb(r t\u22121 \u2212 r)(49)\nConsidering the best and worst case for the estimated gradient, we can find the following bound. In particular, the best case is the case in which all the gradient errors are constructive and make ther t in each iteration smaller than r t . In contrast, the worst case happens when all the gradients directions are destructive and make ther t greater than r t . In practice, however, what is happening is something in between. Substituting r t from (48) in (49), one can obtain:\n\u03bb(r t\u22121 \u2212 r) \u2212 \u03b3r t \u221a N t \u2264r t \u2212 r \u2264 \u03bb(r t\u22121 \u2212 r) + \u03b3r t \u221a N t(50)\nBy using the iterative equation, one can get the following bound:\n\u03bb t (r 0 \u2212 r) \u2212 e(N ) \u2264r t \u2212 r \u2264 \u03bb t (r 0 \u2212 r) + e(N ) (51) where e(N ) = \u03b3 t i=1 \u03bb t\u2212i ri \u221a\nNi is the error due to limited number of queries.\n\n\nD. Proof of Theorem 3\n\nIt can easily be observed that the optimization problem is convex. Thus, the duality gap between this problem and its dual optimization problem is zero. Therefore, we can solve the given problem by solving its dual problem. The Lagrangian is given by:\nL(N , \u03b1) = T i=1 \u03bb \u2212i r i \u221a N i + \u03b1 T i=1 N i \u2212 N(52)\nwhere \u03b1 is the non-negative dual variable associated with the budget constraint. The KKT conditions are given as follows [1]:\n\u2202L(N , \u03b1) \u2202N t = 0, \u2200i (53) \u03b1 T i=0 N i \u2212 N = 0 (54) T i=1 N i \u2264 N(55)\nBased on (53), taking the derivative and setting equal to zero, we can have\nN t = \u03bb \u2212t r t 2\u03b1 2 3(56)\nWe see that the constraint holds with equality. Assume that t i=0 N i = N , then based on (54), \u03b1 = 0. If \u03b1 = 0 then based on (56), we have N i = \u221e, \u2200i which contradicts with (55). Substituting (56) in t i=0 N i = N , the Lagrangian multiplier can be obtained as\n\u03b1 2 3 = 1 2 2 3 T i=1 (\u03bb \u2212i r i ) 2 3 N(57)\nSubstituting \u03b1 in (56), one can get the optimal number of queries as:\nN * t = (\u03bb \u2212t r t ) 2 3 T i=1 (\u03bb \u2212i r i ) 2 3 N(58)\nFor t \u2192 \u221e, we have r t \u2192 r. Based on this, the ratio of the optimal number if queries for each iteration is given by:\nN * t \u2248 \u03bb \u2212 2 3 t T i=1 \u03bb \u2212 2 3 i N(59)\nThis equation shows that the distribution of the queries should be increased by a factor of \u03bb \u2212 2 3 where 0 < \u03bb < 1. By approximation, we have\nN * t+1 N * t \u2248 \u03bb \u2212 2 3(60)\nwhich completes the proof.\n\n\nAdditional experiment results\n\nHere we show more experiments on the performance of GeoDA on different p norms. In Figs. 8, Figs. 9, Figs. 10, and Figs. 11, we have generated adversarial examples using GeoDA. For each image, the first row consists of (from left to right) original image, 2 fullspace adversarial example, 2 subspace adversarial example, \u221e fullspace adversarial example, \u221e subspace adversarial example, and 1 adversarial example, respectively. However, as can be seen the perturbations are not quite visible in the actual adversarial examples in the first row. In the second row, we show the magnified version of perturbations for 2 and \u221e . To do so, the norm of all the perturbations is magnified to 100 given that the images coordinate normalized to the 0 to 1 scale. For the sparse case, we do not magnify the perturbations as they are visible and equal to their maximum (minimum) values. Finally, in the third row, we added a magnified version of the perturbation with norm of 30 to have a better visualization.  In Table 3, we have compared the performance of GeoDA with different deep network image classifiers. The proposed algorithm GeoDA follows almost the same trend on a wide variety of deep networks. The reason is that the core assumption of GeoDA, i.e. boundary has a low mean curvature in vicinity of the datapoints, is verified empirically for a wide variety of deep networks. We can provide the experimental results on different networks.     \n\nFigure 1 :\n1Linearization of the decision boundary.\n\nFigure 2 :\n2Performance evaluation of GeoDA for p when p = 1, 2 (a) Comparison for the performance of GeoDA, BA, and HSJA for 2 norm. (b) Comparison for the number of required iterations in GeoDA, BA, and HSJA. (c) Fooling rate vs. sparsity for different numbers of queries in sparse GeoDA.\n\n\nx\u2208X x \u2212 x adv p . For sparse per-Algorithm 2: Sparse GeoDA 1 Inputs: Original image x, query budget N , \u03bb, projection operator Q. 2 Output: Sparsely perturbed x adv , sparsity s. 3 Obtain x B ,\u0175 N with N queries by 2 GeoDA algorithm. 4 m l = 0, m u = d, J = round(log 2 (d)k absolute values of coordinates of\u0175 N as\u0175 sp .8 x j \u2190 Q(x + \u03bd sign(\u0175 sp )) 9 if\u0175 T N (x j \u2212 x B ) adv \u2190 x j , s \u2190 m u\n\nFigure 3 :Figure 4 :\n34Original images and adversarial perturbations generated by GeoDA for 2 fullspace, 2 subspace, \u221e fullspace, \u221e subspace, and 1 sparse with N = 10000 queries. (Perturbations are magnified \u223c 10\u00d7 for better visibility.) (a) The effect of the variance \u03c3 on the ratio of correctly classified queries C to the total number of queries N at boundary point x B . (b) Effect of \u03bb on the performance of the algorithm. (c) Comparison of two extreme cases of query distributions, i.e., single iteration (\u03bb \u2192 0) and uniform distribution (\u03bb = 1) with optimal distribution (\u03bb = 0.6).\n\nFigure 5 :\n5Effect of prior information, i.e., DCT sub-space and transferability on the performance of 2 perturbation.\n\nFigure 8 :\n8Original images and adversarial perturbations generated by GeoDA for 2 fullspace, 2 subspace, \u221e fullspace, \u221e subspace, and 1 sparse with N = 10000 queries.\n\nFigure 9 :\n9Original images and adversarial perturbations generated by GeoDA for 2 fullspace, 2 subspace, \u221e fullspace, \u221e subspace, and 1 sparse with N = 10000 queries.\n\nFigure 10 :\n10Original images and adversarial perturbations generated by GeoDA for 2 fullspace, 2 subspace, \u221e fullspace, \u221e subspace, and 1 sparse with N = 10000 queries.\n\nFigure 11 :\n11Original images and adversarial perturbations generated by GeoDA for 2 fullspace, 2 subspace, \u221e fullspace, \u221e subspace, and 1 sparse with N = 10000 queries.\n\nTable 1 :\n1The performance comparison of GeoDA with BA and HSJA for median 2 and \u221e on ImageNet dataset.Queries \nFooling rate Perturbation \n\n500 \n88.44 % \n4.29 % \n\nGeoDA \n\n2000 \n90.25 % \n3.04 % \n10000 \n91.17 % \n2.36 % \n\nSparseFool [2] \n-\n\n100 % \n0.23 % \n\n\n\nTable 2 :\n2The performance comparison of black-box sparse GeoDA for median sparsity compared to white box attack SparseFool[2] on ImageNet dataset.depicted in \n\nTable 3 :\n3The performance comparison of GeoDA on different ResNet image classifiers.\nAcknowledgementsThis work was supported in part by the US National Science Foundation under grants ECCS-1444009 and CNS-1824518. S. M. is supported by a Google Postdoctoral Fellowship.\nConvex optimization. Stephen Boyd, Lieven Vandenberghe, Cambridge university pressStephen Boyd and Lieven Vandenberghe. Convex optimiza- tion. Cambridge university press, 2004. 13\n\nDecision-based adversarial attacks: Reliable attacks against black-box machine learning models. Wieland Brendel, Jonas Rauber, Matthias Bethge, arXiv:1712.042487arXiv preprintWieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017. 1, 2, 6, 7\n\nTowards evaluating the robustness of neural networks. Nicholas Carlini, David Wagner, 2017 IEEE Symposium on Security and Privacy (SP). Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39-57, 2017. 1, 2, 7\n\nBoundary attack++: Query-efficient decision-based adversarial attack. Jianbo Chen, Michael I Jordan , arXiv:1904.021441arXiv preprintJianbo Chen and Michael I Jordan. Boundary attack++: Query-efficient decision-based adversarial attack. arXiv preprint arXiv:1904.02144, 2019. 1, 2\n\nHopskipjumpattack: A query-efficient decision-based attack. Jianbo Chen, Martin J Michael I Jordan, Wainwright, arXiv:1904.0214467arXiv preprintJianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient decision-based attack. arXiv preprint arXiv:1904.02144, 2019. 2, 6, 7\n\nZoo: Zeroth order optimization based blackbox attacks to deep neural networks without training substitute models. Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityACM1Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black- box attacks to deep neural networks without training sub- stitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 15-26. ACM, 2017. 1, 2\n\nStateful detection of black-box adversarial attacks. Steven Chen, Nicholas Carlini, David Wagner, arXiv:1907.05587arXiv preprintSteven Chen, Nicholas Carlini, and David Wagner. Stateful detection of black-box adversarial attacks. arXiv preprint arXiv:1907.05587, 2019. 5\n\nQuery-efficient hard-label blackbox attack: An optimization-based approach. Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, Cho-Jui Hsieh, arXiv:1807.0445726arXiv preprintMinhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-efficient hard-label black- box attack: An optimization-based approach. arXiv preprint arXiv:1807.04457, 2018. 2, 6\n\nShuyu Cheng, Yinpeng Dong, arXiv:1906.06919Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. arXiv preprintShuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. arXiv preprint arXiv:1906.06919, 2019. 4\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255, 2009. 6\n\nAditya Devarakonda, Maxim Naumov, Michael Garland, arXiv:1712.02029Adabatch: adaptive batch sizes for training deep neural networks. arXiv preprintAditya Devarakonda, Maxim Naumov, and Michael Garland. Adabatch: adaptive batch sizes for training deep neural net- works. arXiv preprint arXiv:1712.02029, 2017. 5\n\nRobustness of classifiers: from adversarial to random noise. Alhussein Fawzi, Pascal Seyed-Mohsen Moosavi-Dezfooli, Frossard, Advances in Neural Information Processing Systems. 1113Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Pro- cessing Systems, pages 1632-1640, 2016. 2, 5, 11, 13\n\nThe robustness of deep networks: A geometrical perspective. Alhussein Fawzi, Pascal Seyed-Mohsen Moosavi-Dezfooli, Frossard, IEEE Signal Processing Magazine. 346Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. The robustness of deep networks: A ge- ometrical perspective. IEEE Signal Processing Magazine, 34(6):50-62, 2017. 2\n\nEmpirical study of the topology and geometry of deep networks. Alhussein Fawzi, Pascal Seyed-Mohsen Moosavi-Dezfooli, Stefano Frossard, Soatto, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAlhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical study of the topol- ogy and geometry of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 3762-3770, 2018. 2\n\nJ Ian, Goodfellow, arXiv:1412.6572Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. 1arXiv preprintIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 1, 2\n\nChuan Guo, R Jacob, Yurong Gardner, Andrew Gordon You, Kilian Q Wilson, Weinberger, arXiv:1905.07121Simple black-box adversarial attacks. arXiv preprintChuan Guo, Jacob R Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Q Weinberger. Simple black-box adver- sarial attacks. arXiv preprint arXiv:1905.07121, 2019. 3\n\nA bound on tail probabilities for quadratic forms in independent random variables. David Lee Hanson, Farroll Tim Wright, The Annals of Mathematical Statistics. 42311David Lee Hanson and Farroll Tim Wright. A bound on tail probabilities for quadratic forms in independent ran- dom variables. The Annals of Mathematical Statistics, 42(3):1079-1083, 1971. 4, 11\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 6\n\nAnalysis of the Cholesky decomposition of a semi-definite matrix. J Nicholas, Higham, Oxford University Press11Nicholas J Higham. Analysis of the Cholesky decomposition of a semi-definite matrix. Oxford University Press, 1990. 11\n\nBlack-box adversarial attacks with limited queries and information. Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin, arXiv:1804.085981arXiv preprintAndrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. arXiv preprint arXiv:1804.08598, 2018. 1, 2\n\nAndrew Ilyas, Logan Engstrom, Aleksander Madry, arXiv:1807.07978Prior convictions: Black-box adversarial attacks with bandits and priors. 23arXiv preprintAndrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks with ban- dits and priors. arXiv preprint arXiv:1807.07978, 2018. 2, 3\n\nA geometry-inspired decision-based attack. Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, arXiv:1903.108266arXiv preprintYujia Liu, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. A geometry-inspired decision-based attack. arXiv preprint arXiv:1903.10826, 2019. 1, 2, 4, 6\n\nSub-gaussian estimators of the mean of a random vector. G\u00e1bor Lugosi, Shahar Mendelson, The Annals of Statistics. 4711G\u00e1bor Lugosi, Shahar Mendelson, et al. Sub-gaussian esti- mators of the mean of a random vector. The Annals of Statis- tics, 47(2):783-794, 2019. 4, 11\n\nBounds for the smallest and largest eigenvalues of hermitian matrices. Rachid Marsli, International Journal of Algebra. 9811Rachid Marsli. Bounds for the smallest and largest eigenval- ues of hermitian matrices. International Journal of Algebra, 9(8):379-394, 2015. 11\n\nGeometry of adversarial robustness of deep networks: methods and applications. Moosavi Seyed Mohsen, Dezfooli, EPFL. 8Technical reportSeyed Mohsen Moosavi Dezfooli. Geometry of adversar- ial robustness of deep networks: methods and applications. Technical report, EPFL, 2019. 8\n\nUniversal adversarial perturbations. Alhussein Seyed-Mohsen Moosavi-Dezfooli, Omar Fawzi, Pascal Fawzi, Frossard, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturba- tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1765-1773, 2017. 4\n\nDeepfool: a simple and accurate method to fool deep neural networks. Alhussein Seyed-Mohsen Moosavi-Dezfooli, Pascal Fawzi, Frossard, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2574-2582, 2016. 1, 2, 7, 8\n\nSimple black-box adversarial perturbations for deep networks. Nina Narodytska, Shiva Prasad Kasiviswanathan, arXiv:1612.062991arXiv preprintNina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial perturbations for deep networks. arXiv preprint arXiv:1612.06299, 2016. 1, 2\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 1\n\nPlane truncation in normal populations. G M Tallis, Journal of the Royal Statistical Society: Series B (Methodological). 27211GM Tallis. Plane truncation in normal populations. Journal of the Royal Statistical Society: Series B (Methodological), 27(2):301-307, 1965. 3, 11\n\nFlorian Tram\u00e8r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick Mcdaniel, arXiv:1704.03453The space of transferable adversarial examples. arXiv preprintFlorian Tram\u00e8r, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of transferable ad- versarial examples. arXiv preprint arXiv:1704.03453, 2017. 4\n\nAutozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng, arXiv:1805.11770arXiv preprintChun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. arXiv preprint arXiv:1805.11770, 2018. 2\n\nAutozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, Shin-Ming Cheng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In Pro- ceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 742-749, 2019. 3\n\nOn the design of black-box adversarial examples by leveraging gradient-free optimization and operator splitting method. Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, Xue Lin, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionPu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya Kailkhura, and Xue Lin. On the design of black-box adversarial examples by leveraging gradient-free optimiza- tion and operator splitting method. In Proceedings of the IEEE International Conference on Computer Vision, pages 121-130, 2019. 3\n", "annotations": {"author": "[{\"end\":95,\"start\":66},{\"end\":212,\"start\":96},{\"end\":253,\"start\":213},{\"end\":279,\"start\":254},{\"end\":332,\"start\":280}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":70},{\"end\":125,\"start\":109},{\"end\":228,\"start\":220},{\"end\":264,\"start\":261}]", "author_first_name": "[{\"end\":69,\"start\":66},{\"end\":108,\"start\":96},{\"end\":219,\"start\":213},{\"end\":260,\"start\":254}]", "author_affiliation": "[{\"end\":211,\"start\":127},{\"end\":331,\"start\":281}]", "title": "[{\"end\":63,\"start\":1},{\"end\":395,\"start\":333}]", "venue": null, "abstract": "[{\"end\":1565,\"start\":397}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1782,\"start\":1778},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1952,\"start\":1948},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1955,\"start\":1952},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1957,\"start\":1955},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2164,\"start\":2163},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2513,\"start\":2509},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2515,\"start\":2513},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2518,\"start\":2515},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2539,\"start\":2536},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2541,\"start\":2539},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2544,\"start\":2541},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4699,\"start\":4695},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4702,\"start\":4699},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4704,\"start\":4702},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4740,\"start\":4736},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4742,\"start\":4740},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4745,\"start\":4742},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4786,\"start\":4783},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4788,\"start\":4786},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4791,\"start\":4788},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4995,\"start\":4991},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4998,\"start\":4995},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5001,\"start\":4998},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5347,\"start\":5344},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5382,\"start\":5379},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5402,\"start\":5399},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5418,\"start\":5414},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5426,\"start\":5423},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5541,\"start\":5538},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5586,\"start\":5583},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5776,\"start\":5773},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6004,\"start\":6000},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6558,\"start\":6554},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6958,\"start\":6954},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8245,\"start\":8241},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8625,\"start\":8621},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8817,\"start\":8813},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8820,\"start\":8817},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9449,\"start\":9445},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9452,\"start\":9449},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10028,\"start\":10025},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10176,\"start\":10173},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11828,\"start\":11825},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13561,\"start\":13557},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14211,\"start\":14207},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14434,\"start\":14431},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14582,\"start\":14578},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14585,\"start\":14582},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14587,\"start\":14585},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16372,\"start\":16368},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16375,\"start\":16372},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16659,\"start\":16658},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16716,\"start\":16712},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16719,\"start\":16718},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16865,\"start\":16861},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17990,\"start\":17986},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18680,\"start\":18676},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18885,\"start\":18881},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19316,\"start\":19312},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20408,\"start\":20404},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21563,\"start\":21559},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21754,\"start\":21751},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22327,\"start\":22323},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23092,\"start\":23088},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24540,\"start\":24536},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24652,\"start\":24648},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25270,\"start\":25267},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25294,\"start\":25291},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25306,\"start\":25302},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25326,\"start\":25323},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25519,\"start\":25516},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28463,\"start\":28459},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28737,\"start\":28733},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30913,\"start\":30909},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30935,\"start\":30931},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31735,\"start\":31731},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33347,\"start\":33343},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33589,\"start\":33585},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33773,\"start\":33769},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34253,\"start\":34249},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34345,\"start\":34341},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35212,\"start\":35208},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35246,\"start\":35242},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35565,\"start\":35561},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35904,\"start\":35900},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36495,\"start\":36491},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37795,\"start\":37791},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39475,\"start\":39471},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":41851,\"start\":41848},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46800,\"start\":46797}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44340,\"start\":44288},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44632,\"start\":44341},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45026,\"start\":44633},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45616,\"start\":45027},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45736,\"start\":45617},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45905,\"start\":45737},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46074,\"start\":45906},{\"attributes\":{\"id\":\"fig_9\"},\"end\":46245,\"start\":46075},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46416,\"start\":46246},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46672,\"start\":46417},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46833,\"start\":46673},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46920,\"start\":46834}]", "paragraph": "[{\"end\":2545,\"start\":1581},{\"end\":3383,\"start\":2547},{\"end\":3828,\"start\":3385},{\"end\":4310,\"start\":3830},{\"end\":4621,\"start\":4312},{\"end\":6144,\"start\":4638},{\"end\":6959,\"start\":6146},{\"end\":7077,\"start\":6981},{\"end\":7653,\"start\":7231},{\"end\":8305,\"start\":7726},{\"end\":8918,\"start\":8307},{\"end\":9081,\"start\":8971},{\"end\":9682,\"start\":9083},{\"end\":10784,\"start\":9700},{\"end\":10967,\"start\":10846},{\"end\":11646,\"start\":11031},{\"end\":11720,\"start\":11662},{\"end\":12062,\"start\":11804},{\"end\":12429,\"start\":12064},{\"end\":13036,\"start\":12488},{\"end\":13196,\"start\":13114},{\"end\":14321,\"start\":13213},{\"end\":15118,\"start\":14351},{\"end\":15426,\"start\":15143},{\"end\":15847,\"start\":15455},{\"end\":16376,\"start\":15849},{\"end\":16763,\"start\":16386},{\"end\":16866,\"start\":16837},{\"end\":17115,\"start\":17012},{\"end\":17161,\"start\":17117},{\"end\":17273,\"start\":17163},{\"end\":17772,\"start\":17318},{\"end\":18273,\"start\":17819},{\"end\":19379,\"start\":18275},{\"end\":19559,\"start\":19431},{\"end\":19681,\"start\":19561},{\"end\":19814,\"start\":19703},{\"end\":20791,\"start\":19833},{\"end\":20857,\"start\":20821},{\"end\":20903,\"start\":20859},{\"end\":21564,\"start\":20905},{\"end\":21981,\"start\":21566},{\"end\":22269,\"start\":22076},{\"end\":22315,\"start\":22271},{\"end\":22873,\"start\":22317},{\"end\":23156,\"start\":22929},{\"end\":23304,\"start\":23259},{\"end\":23580,\"start\":23337},{\"end\":24455,\"start\":23636},{\"end\":24698,\"start\":24482},{\"end\":25058,\"start\":24700},{\"end\":28948,\"start\":25083},{\"end\":30513,\"start\":28998},{\"end\":31905,\"start\":30515},{\"end\":32802,\"start\":31920},{\"end\":33181,\"start\":32837},{\"end\":33804,\"start\":33240},{\"end\":34004,\"start\":33850},{\"end\":34358,\"start\":34059},{\"end\":34550,\"start\":34417},{\"end\":34756,\"start\":34611},{\"end\":34905,\"start\":34790},{\"end\":35247,\"start\":34907},{\"end\":35373,\"start\":35249},{\"end\":35566,\"start\":35410},{\"end\":35755,\"start\":35609},{\"end\":36023,\"start\":35812},{\"end\":36224,\"start\":36073},{\"end\":36369,\"start\":36283},{\"end\":36599,\"start\":36404},{\"end\":36700,\"start\":36601},{\"end\":36920,\"start\":36796},{\"end\":37644,\"start\":37024},{\"end\":37708,\"start\":37668},{\"end\":37796,\"start\":37710},{\"end\":38096,\"start\":37837},{\"end\":38122,\"start\":38098},{\"end\":38215,\"start\":38192},{\"end\":38461,\"start\":38256},{\"end\":38483,\"start\":38463},{\"end\":38623,\"start\":38559},{\"end\":38754,\"start\":38676},{\"end\":38919,\"start\":38880},{\"end\":39072,\"start\":38950},{\"end\":39294,\"start\":39230},{\"end\":39429,\"start\":39358},{\"end\":39528,\"start\":39465},{\"end\":39867,\"start\":39623},{\"end\":40060,\"start\":39893},{\"end\":40309,\"start\":40086},{\"end\":40377,\"start\":40348},{\"end\":40613,\"start\":40425},{\"end\":41119,\"start\":40641},{\"end\":41252,\"start\":41187},{\"end\":41395,\"start\":41346},{\"end\":41672,\"start\":41421},{\"end\":41852,\"start\":41727},{\"end\":41999,\"start\":41924},{\"end\":42288,\"start\":42026},{\"end\":42402,\"start\":42333},{\"end\":42572,\"start\":42455},{\"end\":42755,\"start\":42613},{\"end\":42810,\"start\":42784},{\"end\":44287,\"start\":42844}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7230,\"start\":7078},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7725,\"start\":7654},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8970,\"start\":8919},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10845,\"start\":10785},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11030,\"start\":10968},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11661,\"start\":11647},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11803,\"start\":11721},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12487,\"start\":12430},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13113,\"start\":13037},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13212,\"start\":13197},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14350,\"start\":14322},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15142,\"start\":15119},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15454,\"start\":15427},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16385,\"start\":16377},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16836,\"start\":16764},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17011,\"start\":16867},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17818,\"start\":17773},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19415,\"start\":19380},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19430,\"start\":19415},{\"attributes\":{\"id\":\"formula_20\"},\"end\":19702,\"start\":19682},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20820,\"start\":20792},{\"attributes\":{\"id\":\"formula_22\"},\"end\":22075,\"start\":21982},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22928,\"start\":22874},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23258,\"start\":23157},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23635,\"start\":23581},{\"attributes\":{\"id\":\"formula_26\"},\"end\":33239,\"start\":33182},{\"attributes\":{\"id\":\"formula_27\"},\"end\":33849,\"start\":33805},{\"attributes\":{\"id\":\"formula_28\"},\"end\":34058,\"start\":34005},{\"attributes\":{\"id\":\"formula_29\"},\"end\":34416,\"start\":34359},{\"attributes\":{\"id\":\"formula_30\"},\"end\":34610,\"start\":34551},{\"attributes\":{\"id\":\"formula_31\"},\"end\":34789,\"start\":34757},{\"attributes\":{\"id\":\"formula_32\"},\"end\":35409,\"start\":35374},{\"attributes\":{\"id\":\"formula_33\"},\"end\":35608,\"start\":35567},{\"attributes\":{\"id\":\"formula_34\"},\"end\":35811,\"start\":35756},{\"attributes\":{\"id\":\"formula_35\"},\"end\":36072,\"start\":36024},{\"attributes\":{\"id\":\"formula_36\"},\"end\":36258,\"start\":36225},{\"attributes\":{\"id\":\"formula_37\"},\"end\":36795,\"start\":36701},{\"attributes\":{\"id\":\"formula_38\"},\"end\":37023,\"start\":36921},{\"attributes\":{\"id\":\"formula_39\"},\"end\":37667,\"start\":37645},{\"attributes\":{\"id\":\"formula_40\"},\"end\":37836,\"start\":37797},{\"attributes\":{\"id\":\"formula_41\"},\"end\":38191,\"start\":38123},{\"attributes\":{\"id\":\"formula_42\"},\"end\":38255,\"start\":38216},{\"attributes\":{\"id\":\"formula_44\"},\"end\":38558,\"start\":38484},{\"attributes\":{\"id\":\"formula_45\"},\"end\":38675,\"start\":38624},{\"attributes\":{\"id\":\"formula_46\"},\"end\":38879,\"start\":38755},{\"attributes\":{\"id\":\"formula_47\"},\"end\":38949,\"start\":38920},{\"attributes\":{\"id\":\"formula_48\"},\"end\":39229,\"start\":39073},{\"attributes\":{\"id\":\"formula_49\"},\"end\":39357,\"start\":39295},{\"attributes\":{\"id\":\"formula_50\"},\"end\":39622,\"start\":39529},{\"attributes\":{\"id\":\"formula_51\"},\"end\":40085,\"start\":40061},{\"attributes\":{\"id\":\"formula_52\"},\"end\":40347,\"start\":40310},{\"attributes\":{\"id\":\"formula_53\"},\"end\":40424,\"start\":40378},{\"attributes\":{\"id\":\"formula_54\"},\"end\":40640,\"start\":40614},{\"attributes\":{\"id\":\"formula_55\"},\"end\":41186,\"start\":41120},{\"attributes\":{\"id\":\"formula_56\"},\"end\":41345,\"start\":41253},{\"attributes\":{\"id\":\"formula_57\"},\"end\":41726,\"start\":41673},{\"attributes\":{\"id\":\"formula_58\"},\"end\":41923,\"start\":41853},{\"attributes\":{\"id\":\"formula_59\"},\"end\":42025,\"start\":42000},{\"attributes\":{\"id\":\"formula_60\"},\"end\":42332,\"start\":42289},{\"attributes\":{\"id\":\"formula_61\"},\"end\":42454,\"start\":42403},{\"attributes\":{\"id\":\"formula_62\"},\"end\":42612,\"start\":42573},{\"attributes\":{\"id\":\"formula_63\"},\"end\":42783,\"start\":42756}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25705,\"start\":25698},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27748,\"start\":27741},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43854,\"start\":43847}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1579,\"start\":1567},{\"attributes\":{\"n\":\"2.\"},\"end\":4636,\"start\":4624},{\"attributes\":{\"n\":\"3.\"},\"end\":6979,\"start\":6962},{\"attributes\":{\"n\":\"4.\"},\"end\":9698,\"start\":9685},{\"attributes\":{\"n\":\"5.\"},\"end\":17316,\"start\":17276},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19831,\"start\":19817},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23335,\"start\":23307},{\"attributes\":{\"n\":\"6.\"},\"end\":24469,\"start\":24458},{\"attributes\":{\"n\":\"6.1.\"},\"end\":24480,\"start\":24472},{\"attributes\":{\"n\":\"6.2.\"},\"end\":25081,\"start\":25061},{\"attributes\":{\"n\":\"6.3.\"},\"end\":28996,\"start\":28951},{\"attributes\":{\"n\":\"7.\"},\"end\":31918,\"start\":31908},{\"attributes\":{\"n\":\"8.\"},\"end\":32813,\"start\":32805},{\"end\":32835,\"start\":32816},{\"end\":36281,\"start\":36260},{\"end\":36402,\"start\":36372},{\"end\":39463,\"start\":39432},{\"end\":39891,\"start\":39870},{\"end\":41419,\"start\":41398},{\"attributes\":{\"n\":\"9.\"},\"end\":42842,\"start\":42813},{\"end\":44299,\"start\":44289},{\"end\":44352,\"start\":44342},{\"end\":45048,\"start\":45028},{\"end\":45628,\"start\":45618},{\"end\":45748,\"start\":45738},{\"end\":45917,\"start\":45907},{\"end\":46087,\"start\":46076},{\"end\":46258,\"start\":46247},{\"end\":46427,\"start\":46418},{\"end\":46683,\"start\":46674},{\"end\":46844,\"start\":46835}]", "table": "[{\"end\":46672,\"start\":46521},{\"end\":46833,\"start\":46821}]", "figure_caption": "[{\"end\":44340,\"start\":44301},{\"end\":44632,\"start\":44354},{\"end\":45026,\"start\":44635},{\"end\":45616,\"start\":45051},{\"end\":45736,\"start\":45630},{\"end\":45905,\"start\":45750},{\"end\":46074,\"start\":45919},{\"end\":46245,\"start\":46090},{\"end\":46416,\"start\":46261},{\"end\":46521,\"start\":46429},{\"end\":46821,\"start\":46685},{\"end\":46920,\"start\":46846}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18272,\"start\":18266},{\"end\":18615,\"start\":18608},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25945,\"start\":25938},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26225,\"start\":26218},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26763,\"start\":26756},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27445,\"start\":27438},{\"end\":28096,\"start\":28090},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28483,\"start\":28477},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28873,\"start\":28867},{\"end\":29873,\"start\":29866},{\"end\":31242,\"start\":31235},{\"end\":31458,\"start\":31451},{\"end\":36477,\"start\":36471},{\"end\":36549,\"start\":36541},{\"end\":38045,\"start\":38037}]", "bib_author_first_name": "[{\"end\":47134,\"start\":47127},{\"end\":47147,\"start\":47141},{\"end\":47390,\"start\":47383},{\"end\":47405,\"start\":47400},{\"end\":47422,\"start\":47414},{\"end\":47723,\"start\":47715},{\"end\":47738,\"start\":47733},{\"end\":48043,\"start\":48037},{\"end\":48066,\"start\":48050},{\"end\":48315,\"start\":48309},{\"end\":48330,\"start\":48322},{\"end\":48677,\"start\":48671},{\"end\":48688,\"start\":48684},{\"end\":48700,\"start\":48696},{\"end\":48716,\"start\":48709},{\"end\":48728,\"start\":48721},{\"end\":49236,\"start\":49230},{\"end\":49251,\"start\":49243},{\"end\":49266,\"start\":49261},{\"end\":49531,\"start\":49525},{\"end\":49544,\"start\":49539},{\"end\":49555,\"start\":49549},{\"end\":49569,\"start\":49562},{\"end\":49578,\"start\":49574},{\"end\":49593,\"start\":49586},{\"end\":49841,\"start\":49836},{\"end\":49856,\"start\":49849},{\"end\":50226,\"start\":50223},{\"end\":50236,\"start\":50233},{\"end\":50250,\"start\":50243},{\"end\":50265,\"start\":50259},{\"end\":50273,\"start\":50270},{\"end\":50280,\"start\":50278},{\"end\":50577,\"start\":50571},{\"end\":50596,\"start\":50591},{\"end\":50612,\"start\":50605},{\"end\":50953,\"start\":50944},{\"end\":50967,\"start\":50961},{\"end\":51356,\"start\":51347},{\"end\":51370,\"start\":51364},{\"end\":51706,\"start\":51697},{\"end\":51720,\"start\":51714},{\"end\":51759,\"start\":51752},{\"end\":52180,\"start\":52179},{\"end\":52471,\"start\":52466},{\"end\":52478,\"start\":52477},{\"end\":52492,\"start\":52486},{\"end\":52508,\"start\":52502},{\"end\":52515,\"start\":52509},{\"end\":52529,\"start\":52521},{\"end\":52875,\"start\":52870},{\"end\":52879,\"start\":52876},{\"end\":52899,\"start\":52888},{\"end\":53200,\"start\":53193},{\"end\":53212,\"start\":53205},{\"end\":53228,\"start\":53220},{\"end\":53238,\"start\":53234},{\"end\":53661,\"start\":53660},{\"end\":53899,\"start\":53893},{\"end\":53912,\"start\":53907},{\"end\":53928,\"start\":53923},{\"end\":53943,\"start\":53938},{\"end\":54159,\"start\":54153},{\"end\":54172,\"start\":54167},{\"end\":54193,\"start\":54183},{\"end\":54528,\"start\":54523},{\"end\":54546,\"start\":54534},{\"end\":54571,\"start\":54565},{\"end\":54831,\"start\":54826},{\"end\":55118,\"start\":55112},{\"end\":55397,\"start\":55390},{\"end\":55636,\"start\":55627},{\"end\":55672,\"start\":55668},{\"end\":55686,\"start\":55680},{\"end\":56151,\"start\":56142},{\"end\":56189,\"start\":56183},{\"end\":56671,\"start\":56667},{\"end\":56689,\"start\":56684},{\"end\":56911,\"start\":56902},{\"end\":56929,\"start\":56921},{\"end\":56943,\"start\":56939},{\"end\":56959,\"start\":56955},{\"end\":57321,\"start\":57320},{\"end\":57323,\"start\":57322},{\"end\":57561,\"start\":57554},{\"end\":57577,\"start\":57570},{\"end\":57591,\"start\":57588},{\"end\":57607,\"start\":57604},{\"end\":57622,\"start\":57615},{\"end\":57997,\"start\":57988},{\"end\":58009,\"start\":58002},{\"end\":58022,\"start\":58016},{\"end\":58034,\"start\":58029},{\"end\":58044,\"start\":58040},{\"end\":58059,\"start\":58052},{\"end\":58071,\"start\":58064},{\"end\":58088,\"start\":58079},{\"end\":58493,\"start\":58484},{\"end\":58505,\"start\":58498},{\"end\":58518,\"start\":58512},{\"end\":58530,\"start\":58525},{\"end\":58540,\"start\":58536},{\"end\":58555,\"start\":58548},{\"end\":58567,\"start\":58560},{\"end\":58584,\"start\":58575},{\"end\":59142,\"start\":59140},{\"end\":59154,\"start\":59149},{\"end\":59166,\"start\":59160},{\"end\":59178,\"start\":59173},{\"end\":59191,\"start\":59186},{\"end\":59202,\"start\":59196},{\"end\":59217,\"start\":59214}]", "bib_author_last_name": "[{\"end\":47139,\"start\":47135},{\"end\":47160,\"start\":47148},{\"end\":47398,\"start\":47391},{\"end\":47412,\"start\":47406},{\"end\":47429,\"start\":47423},{\"end\":47731,\"start\":47724},{\"end\":47745,\"start\":47739},{\"end\":48048,\"start\":48044},{\"end\":48320,\"start\":48316},{\"end\":48347,\"start\":48331},{\"end\":48359,\"start\":48349},{\"end\":48682,\"start\":48678},{\"end\":48694,\"start\":48689},{\"end\":48707,\"start\":48701},{\"end\":48719,\"start\":48717},{\"end\":48734,\"start\":48729},{\"end\":49241,\"start\":49237},{\"end\":49259,\"start\":49252},{\"end\":49273,\"start\":49267},{\"end\":49537,\"start\":49532},{\"end\":49547,\"start\":49545},{\"end\":49560,\"start\":49556},{\"end\":49572,\"start\":49570},{\"end\":49584,\"start\":49579},{\"end\":49599,\"start\":49594},{\"end\":49847,\"start\":49842},{\"end\":49861,\"start\":49857},{\"end\":50231,\"start\":50227},{\"end\":50241,\"start\":50237},{\"end\":50257,\"start\":50251},{\"end\":50268,\"start\":50266},{\"end\":50276,\"start\":50274},{\"end\":50288,\"start\":50281},{\"end\":50589,\"start\":50578},{\"end\":50603,\"start\":50597},{\"end\":50620,\"start\":50613},{\"end\":50959,\"start\":50954},{\"end\":50997,\"start\":50968},{\"end\":51007,\"start\":50999},{\"end\":51362,\"start\":51357},{\"end\":51400,\"start\":51371},{\"end\":51410,\"start\":51402},{\"end\":51712,\"start\":51707},{\"end\":51750,\"start\":51721},{\"end\":51768,\"start\":51760},{\"end\":51776,\"start\":51770},{\"end\":52184,\"start\":52181},{\"end\":52196,\"start\":52186},{\"end\":52475,\"start\":52472},{\"end\":52484,\"start\":52479},{\"end\":52500,\"start\":52493},{\"end\":52519,\"start\":52516},{\"end\":52536,\"start\":52530},{\"end\":52548,\"start\":52538},{\"end\":52886,\"start\":52880},{\"end\":52906,\"start\":52900},{\"end\":53203,\"start\":53201},{\"end\":53218,\"start\":53213},{\"end\":53232,\"start\":53229},{\"end\":53242,\"start\":53239},{\"end\":53670,\"start\":53662},{\"end\":53678,\"start\":53672},{\"end\":53905,\"start\":53900},{\"end\":53921,\"start\":53913},{\"end\":53936,\"start\":53929},{\"end\":53947,\"start\":53944},{\"end\":54165,\"start\":54160},{\"end\":54181,\"start\":54173},{\"end\":54199,\"start\":54194},{\"end\":54532,\"start\":54529},{\"end\":54563,\"start\":54547},{\"end\":54580,\"start\":54572},{\"end\":54838,\"start\":54832},{\"end\":54856,\"start\":54840},{\"end\":55125,\"start\":55119},{\"end\":55410,\"start\":55398},{\"end\":55420,\"start\":55412},{\"end\":55666,\"start\":55637},{\"end\":55678,\"start\":55673},{\"end\":55692,\"start\":55687},{\"end\":55702,\"start\":55694},{\"end\":56181,\"start\":56152},{\"end\":56195,\"start\":56190},{\"end\":56205,\"start\":56197},{\"end\":56682,\"start\":56672},{\"end\":56712,\"start\":56690},{\"end\":56919,\"start\":56912},{\"end\":56937,\"start\":56930},{\"end\":56953,\"start\":56944},{\"end\":56965,\"start\":56960},{\"end\":57330,\"start\":57324},{\"end\":57568,\"start\":57562},{\"end\":57586,\"start\":57578},{\"end\":57602,\"start\":57592},{\"end\":57613,\"start\":57608},{\"end\":57631,\"start\":57623},{\"end\":58000,\"start\":57998},{\"end\":58014,\"start\":58010},{\"end\":58027,\"start\":58023},{\"end\":58038,\"start\":58035},{\"end\":58050,\"start\":58045},{\"end\":58062,\"start\":58060},{\"end\":58077,\"start\":58072},{\"end\":58094,\"start\":58089},{\"end\":58496,\"start\":58494},{\"end\":58510,\"start\":58506},{\"end\":58523,\"start\":58519},{\"end\":58534,\"start\":58531},{\"end\":58546,\"start\":58541},{\"end\":58558,\"start\":58556},{\"end\":58573,\"start\":58568},{\"end\":58590,\"start\":58585},{\"end\":59147,\"start\":59143},{\"end\":59158,\"start\":59155},{\"end\":59171,\"start\":59167},{\"end\":59184,\"start\":59179},{\"end\":59194,\"start\":59192},{\"end\":59212,\"start\":59203},{\"end\":59221,\"start\":59218}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":47285,\"start\":47106},{\"attributes\":{\"doi\":\"arXiv:1712.04248\",\"id\":\"b1\"},\"end\":47659,\"start\":47287},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2893830},\"end\":47965,\"start\":47661},{\"attributes\":{\"doi\":\"arXiv:1904.02144\",\"id\":\"b3\"},\"end\":48247,\"start\":47967},{\"attributes\":{\"doi\":\"arXiv:1904.02144\",\"id\":\"b4\"},\"end\":48555,\"start\":48249},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2179389},\"end\":49175,\"start\":48557},{\"attributes\":{\"doi\":\"arXiv:1907.05587\",\"id\":\"b6\"},\"end\":49447,\"start\":49177},{\"attributes\":{\"doi\":\"arXiv:1807.04457\",\"id\":\"b7\"},\"end\":49834,\"start\":49449},{\"attributes\":{\"doi\":\"arXiv:1906.06919\",\"id\":\"b8\"},\"end\":50168,\"start\":49836},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":50569,\"start\":50170},{\"attributes\":{\"doi\":\"arXiv:1712.02029\",\"id\":\"b10\"},\"end\":50881,\"start\":50571},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13451211},\"end\":51285,\"start\":50883},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5707980},\"end\":51632,\"start\":51287},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52044345},\"end\":52177,\"start\":51634},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b14\"},\"end\":52464,\"start\":52179},{\"attributes\":{\"doi\":\"arXiv:1905.07121\",\"id\":\"b15\"},\"end\":52785,\"start\":52466},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":123145182},\"end\":53145,\"start\":52787},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206594692},\"end\":53592,\"start\":53147},{\"attributes\":{\"id\":\"b18\"},\"end\":53823,\"start\":53594},{\"attributes\":{\"doi\":\"arXiv:1804.08598\",\"id\":\"b19\"},\"end\":54151,\"start\":53825},{\"attributes\":{\"doi\":\"arXiv:1807.07978\",\"id\":\"b20\"},\"end\":54478,\"start\":54153},{\"attributes\":{\"doi\":\"arXiv:1903.10826\",\"id\":\"b21\"},\"end\":54768,\"start\":54480},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13671192},\"end\":55039,\"start\":54770},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":18462702},\"end\":55309,\"start\":55041},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227928667},\"end\":55588,\"start\":55311},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":11558223},\"end\":56071,\"start\":55590},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12387176},\"end\":56603,\"start\":56073},{\"attributes\":{\"doi\":\"arXiv:1612.06299\",\"id\":\"b27\"},\"end\":56900,\"start\":56605},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b28\"},\"end\":57278,\"start\":56902},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":126137981},\"end\":57552,\"start\":57280},{\"attributes\":{\"doi\":\"arXiv:1704.03453\",\"id\":\"b30\"},\"end\":57884,\"start\":57554},{\"attributes\":{\"doi\":\"arXiv:1805.11770\",\"id\":\"b31\"},\"end\":58380,\"start\":57886},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":44079102},\"end\":59018,\"start\":58382},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":198953371},\"end\":59647,\"start\":59020}]", "bib_title": "[{\"end\":47713,\"start\":47661},{\"end\":48669,\"start\":48557},{\"end\":50221,\"start\":50170},{\"end\":50942,\"start\":50883},{\"end\":51345,\"start\":51287},{\"end\":51695,\"start\":51634},{\"end\":52868,\"start\":52787},{\"end\":53191,\"start\":53147},{\"end\":54824,\"start\":54770},{\"end\":55110,\"start\":55041},{\"end\":55388,\"start\":55311},{\"end\":55625,\"start\":55590},{\"end\":56140,\"start\":56073},{\"end\":57318,\"start\":57280},{\"end\":58482,\"start\":58382},{\"end\":59138,\"start\":59020}]", "bib_author": "[{\"end\":47141,\"start\":47127},{\"end\":47162,\"start\":47141},{\"end\":47400,\"start\":47383},{\"end\":47414,\"start\":47400},{\"end\":47431,\"start\":47414},{\"end\":47733,\"start\":47715},{\"end\":47747,\"start\":47733},{\"end\":48050,\"start\":48037},{\"end\":48069,\"start\":48050},{\"end\":48322,\"start\":48309},{\"end\":48349,\"start\":48322},{\"end\":48361,\"start\":48349},{\"end\":48684,\"start\":48671},{\"end\":48696,\"start\":48684},{\"end\":48709,\"start\":48696},{\"end\":48721,\"start\":48709},{\"end\":48736,\"start\":48721},{\"end\":49243,\"start\":49230},{\"end\":49261,\"start\":49243},{\"end\":49275,\"start\":49261},{\"end\":49539,\"start\":49525},{\"end\":49549,\"start\":49539},{\"end\":49562,\"start\":49549},{\"end\":49574,\"start\":49562},{\"end\":49586,\"start\":49574},{\"end\":49601,\"start\":49586},{\"end\":49849,\"start\":49836},{\"end\":49863,\"start\":49849},{\"end\":50233,\"start\":50223},{\"end\":50243,\"start\":50233},{\"end\":50259,\"start\":50243},{\"end\":50270,\"start\":50259},{\"end\":50278,\"start\":50270},{\"end\":50290,\"start\":50278},{\"end\":50591,\"start\":50571},{\"end\":50605,\"start\":50591},{\"end\":50622,\"start\":50605},{\"end\":50961,\"start\":50944},{\"end\":50999,\"start\":50961},{\"end\":51009,\"start\":50999},{\"end\":51364,\"start\":51347},{\"end\":51402,\"start\":51364},{\"end\":51412,\"start\":51402},{\"end\":51714,\"start\":51697},{\"end\":51752,\"start\":51714},{\"end\":51770,\"start\":51752},{\"end\":51778,\"start\":51770},{\"end\":52186,\"start\":52179},{\"end\":52198,\"start\":52186},{\"end\":52477,\"start\":52466},{\"end\":52486,\"start\":52477},{\"end\":52502,\"start\":52486},{\"end\":52521,\"start\":52502},{\"end\":52538,\"start\":52521},{\"end\":52550,\"start\":52538},{\"end\":52888,\"start\":52870},{\"end\":52908,\"start\":52888},{\"end\":53205,\"start\":53193},{\"end\":53220,\"start\":53205},{\"end\":53234,\"start\":53220},{\"end\":53244,\"start\":53234},{\"end\":53672,\"start\":53660},{\"end\":53680,\"start\":53672},{\"end\":53907,\"start\":53893},{\"end\":53923,\"start\":53907},{\"end\":53938,\"start\":53923},{\"end\":53949,\"start\":53938},{\"end\":54167,\"start\":54153},{\"end\":54183,\"start\":54167},{\"end\":54201,\"start\":54183},{\"end\":54534,\"start\":54523},{\"end\":54565,\"start\":54534},{\"end\":54582,\"start\":54565},{\"end\":54840,\"start\":54826},{\"end\":54858,\"start\":54840},{\"end\":55127,\"start\":55112},{\"end\":55412,\"start\":55390},{\"end\":55422,\"start\":55412},{\"end\":55668,\"start\":55627},{\"end\":55680,\"start\":55668},{\"end\":55694,\"start\":55680},{\"end\":55704,\"start\":55694},{\"end\":56183,\"start\":56142},{\"end\":56197,\"start\":56183},{\"end\":56207,\"start\":56197},{\"end\":56684,\"start\":56667},{\"end\":56714,\"start\":56684},{\"end\":56921,\"start\":56902},{\"end\":56939,\"start\":56921},{\"end\":56955,\"start\":56939},{\"end\":56967,\"start\":56955},{\"end\":57332,\"start\":57320},{\"end\":57570,\"start\":57554},{\"end\":57588,\"start\":57570},{\"end\":57604,\"start\":57588},{\"end\":57615,\"start\":57604},{\"end\":57633,\"start\":57615},{\"end\":58002,\"start\":57988},{\"end\":58016,\"start\":58002},{\"end\":58029,\"start\":58016},{\"end\":58040,\"start\":58029},{\"end\":58052,\"start\":58040},{\"end\":58064,\"start\":58052},{\"end\":58079,\"start\":58064},{\"end\":58096,\"start\":58079},{\"end\":58498,\"start\":58484},{\"end\":58512,\"start\":58498},{\"end\":58525,\"start\":58512},{\"end\":58536,\"start\":58525},{\"end\":58548,\"start\":58536},{\"end\":58560,\"start\":58548},{\"end\":58575,\"start\":58560},{\"end\":58592,\"start\":58575},{\"end\":59149,\"start\":59140},{\"end\":59160,\"start\":59149},{\"end\":59173,\"start\":59160},{\"end\":59186,\"start\":59173},{\"end\":59196,\"start\":59186},{\"end\":59214,\"start\":59196},{\"end\":59223,\"start\":59214}]", "bib_venue": "[{\"end\":47125,\"start\":47106},{\"end\":47381,\"start\":47287},{\"end\":47795,\"start\":47747},{\"end\":48035,\"start\":47967},{\"end\":48307,\"start\":48249},{\"end\":48812,\"start\":48736},{\"end\":49228,\"start\":49177},{\"end\":49523,\"start\":49449},{\"end\":49981,\"start\":49879},{\"end\":50353,\"start\":50290},{\"end\":50702,\"start\":50638},{\"end\":51058,\"start\":51009},{\"end\":51443,\"start\":51412},{\"end\":51855,\"start\":51778},{\"end\":52299,\"start\":52213},{\"end\":52602,\"start\":52566},{\"end\":52945,\"start\":52908},{\"end\":53321,\"start\":53244},{\"end\":53658,\"start\":53594},{\"end\":53891,\"start\":53825},{\"end\":54289,\"start\":54217},{\"end\":54521,\"start\":54480},{\"end\":54882,\"start\":54858},{\"end\":55159,\"start\":55127},{\"end\":55426,\"start\":55422},{\"end\":55781,\"start\":55704},{\"end\":56284,\"start\":56207},{\"end\":56665,\"start\":56605},{\"end\":57069,\"start\":56982},{\"end\":57399,\"start\":57332},{\"end\":57695,\"start\":57649},{\"end\":57986,\"start\":57886},{\"end\":58653,\"start\":58592},{\"end\":59290,\"start\":59223},{\"end\":48875,\"start\":48814},{\"end\":51919,\"start\":51857},{\"end\":53385,\"start\":53323},{\"end\":55845,\"start\":55783},{\"end\":56348,\"start\":56286},{\"end\":58701,\"start\":58655},{\"end\":59344,\"start\":59292}]"}}}, "year": 2023, "month": 12, "day": 17}