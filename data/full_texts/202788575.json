{"id": 202788575, "updated": "2023-12-13 10:46:55.988", "metadata": {"title": "Fine-Grained Analysis of Propaganda in News Article", "authors": "[{\"first\":\"Giovanni\",\"last\":\"Da San Martino\",\"middle\":[]},{\"first\":\"Seunghak\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Alberto\",\"last\":\"Barr\u00f3n-Cede\u00f1o\",\"middle\":[]},{\"first\":\"Rostislav\",\"last\":\"Petrov\",\"middle\":[]},{\"first\":\"Preslav\",\"last\":\"Nakov\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Propaganda aims at influencing people\u2019s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at fragment level with eighteen propaganda techniques and propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2978871345", "acl": "D19-1565", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/MartinoYBPN19", "doi": "10.18653/v1/d19-1565"}}, "content": {"source": {"pdf_hash": "a5f16acaf89a11453642d52008bc58ab34d39a8a", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D19-1565.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D19-1565.pdf", "status": "HYBRID"}}, "grobid": {"id": "e0eabd1de8da2c9cb3ecf3b31fabb40fcafb4102", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a5f16acaf89a11453642d52008bc58ab34d39a8a.txt", "contents": "\nFine-Grained Analysis of Propaganda in News Articles\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 3-7, 2019. 2019\n\nGiovanni Da \nSan Martino gmartino@hbku.edu.qa \nQatar Computing Research Institute\nHBKU\nQatar\n\nSeunghak Yu seunghak@csail.mit.edu \nMIT Computer Science and Artificial Intelligence Laboratory\nCambridgeMAUSA\n\nAlberto Barr\u00f3n-Cede\u00f1o \nUniversit\u00e0 di Bologna\n4 A Data ProForl\u00ec, SofiaItaly, Bulgaria\n\nRostislav Petrov \nPreslav Nakov pnakov@hbku.edu.qa \nQatar Computing Research Institute\nHBKU\nQatar\n\nFine-Grained Analysis of Propaganda in News Articles\n\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics5636November 3-7, 2019. 2019\nPropaganda aims at influencing people's mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at the document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at the fragment level with eighteen propaganda techniques and we propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.\n\nIntroduction\n\nResearch on detecting propaganda has focused primarily on articles (Barr\u00f3n-Cedeno et al., 2019;Rashkin et al., 2017). In many cases, there are no labeled data for individual articles, but there are such labels for entire news outlets. Thus, often all articles from the same news outlet get labeled the way that this outlet is labeled. Yet, it has been observed that propagandistic sources could post objective non-propagandistic articles periodically to increase their credibility (Horne et al., 2018). Similarly, media generally recognized as objective might occasionally post articles that promote a particular editorial agenda and are thus propagandistic. Thus, it is clear that transferring the label of the news outlet to each of its articles, could introduce noise. Such labels can still be useful for training robust systems, but they cannot be used to get a fair assessment of a system at testing time.\n\nOne option to deal with the lack of labels for articles is to crowdsource the annotation. However, in preliminary experiments we observed that the average annotator cannot detach her personal mindset from the judgment of propaganda and bias, i.e., if a clearly propagandistic text expresses ideas aligned with the annotator's beliefs, it is unlikely that she would judge it as such.\n\nWe argue that in order to study propaganda in a sound and reliable way, we need to rely on highquality trusted professional annotations and it is best to do so at the fragment level, targeting specific techniques rather than using a label for an entire document or an entire news outlet.\n\nOurs is the first work that goes at a fine-grained level: identifying specific instances of propaganda techniques used within an article. In particular, we create a corresponding corpus. For this purpose, we asked six experts to annotate articles from news outlets recognized as propagandistic and non-propagandistic, marking specific text spans with eighteen propaganda techniques. We also designed appropriate evaluation measures. Taken together, the annotated corpus and the evaluation measures represent the first manually-curated evaluation framework for the analysis of finegrained propaganda. We release the corpus (350K tokens) as well as our code in order to enable future research. 1 Our contributions are as follows:\n\n\u2022 We formulate a new problem: detect the use of specific propaganda techniques in text.\n\n\u2022 We build a new large corpus for this problem.\n\n\u2022 We propose a suitable evaluation measure.\n\n\u2022 We design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.\n\nOur corpus could enable research in propagandistic and non-objective news, including the development of explainable AI systems. A system that can detect instances of use of specific propagandistic techniques would be able to make it explicit to the users why a given article was predicted to be propagandistic. It could also help train the users to spot the use of such techniques in the news. The remainder of this paper is organized as follows: Section 2 presents the propagandistic techniques we focus on. Section 3 describes our corpus. Section 4 discusses an evaluation measures for comparing labeled fragments. Section 5 presents the formulation of the task and our proposed models. Section 6 describes our experiments and the evaluation results. Section 7 presents some relevant related work. Finally, Section 8 concludes and discusses future work.\n\n\nPropaganda and its Techniques\n\nPropaganda comes in many forms, but it can be recognized by its persuasive function, sizable target audience, the representation of a specific group's agenda, and the use of faulty reasoning and/or emotional appeals (Miller, 1939). Since propaganda is conveyed through the use of a number of techniques, their detection allows for a deeper analysis at the paragraph and the sentence level that goes beyond a single document-level judgment on whether a text is propagandistic.\n\nWhereas the definition of propaganda is widely accepted in the literature, the set of propaganda techniques differs between scholars (Torok, 2015). For instance, Miller (1939) considers seven techniques, whereas Weston (2018) lists at least 24, and Wikipedia discusses 69. 2 The differences are mainly due to some authors ignoring some techniques, or using definitions that subsume the definition used by other authors. Below, we describe the propaganda techniques we consider: a curated list of eighteen items derived from the aforementioned studies. The list only includes techniques that can be found in journalistic articles and can be judged intrinsically, without the need to retrieve supporting information from external resources. For example, we do not include techniques such as card stacking (Jowett and O'Donnell, 2012, page 237), since it would require comparing against external sources of information.\n\nThe eighteen techniques we consider are as follows (cf. Table 1 for examples): 1. Loaded language. Using words/phrases with strong emotional implications (positive or negative) to influence an audience (Weston, 2018, p. 6). Ex.: \"[. . . ] a lone lawmaker's childish shouting.\" 2. Name calling or labeling. Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable or otherwise loves or praises (Miller, 1939). Ex.: \"Republican congressweasels\", \"Bush the Lesser.\"\n\n3. Repetition. Repeating the same message over and over again, so that the audience will eventually accept it (Torok, 2015;Miller, 1939).\n\n\nExaggeration or minimization.\n\nEither representing something in an excessive manner: making things larger, better, worse (e.g., \"the best of the best\", \"quality guaranteed\") or making something seem less important or smaller than it actually is (Jowett and O'Donnell, 2012, p. 303), e.g., saying that an insult was just a joke. Ex.: \"Democrats bolted as soon as Trump's speech ended in an apparent effort to signal they can't even stomach being in the same room as the president\"; \"I was not fighting with her; we were just playing.\" 5. Doubt. Questioning the credibility of someone or something. Ex.: A candidate says about his opponent: \"Is he ready to be the Mayor?\" 6. Appeal to fear/prejudice. Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative, possibly based on preconceived judgments. Ex.: \"stop those refugees; they are terrorists.\" 7. Flag-waving. Playing on strong national feeling (or with respect to a group, e.g., race, gender, political preference) to justify or promote an action or idea (Hobbs and Mcgee, 2008). Ex.: \"entering this war will make us have a better future in our country.\" 8. Causal oversimplification. Assuming one cause when there are multiple causes behind an issue. We include scapegoating as well: the transfer of the blame to one person or group of people without investigating the complexities of an issue. Ex.: \"If France had not declared war on Germany, World War II would have never happened.\" Table 1: Instances of the different propaganda techniques from our corpus. We show the document ID, the technique, and the text snippet, in bold. When necessary, some context is provided to better understand the example. 9. Slogans. A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals (Dan, 2015). Ex.: \"Make America great again!\" 10. Appeal to authority. Stating that a claim is true simply because a valid authority/expert on the issue supports it, without any other supporting evidence (Goodwin, 2011). We include the special case where the reference is not an authority/expert, although it is referred to as testimonial in the literature (Jowett and O'Donnell, 2012, p. 237). 11. Black-and-white fallacy, dictatorship. Presenting two alternative options as the only possibilities, when in fact more possibilities exist (Torok, 2015). As an extreme case, telling the audience exactly what actions to take, eliminating any other possible choice (dictatorship). Ex.: \"You must be a Republican or Democrat; you are not a Democrat. Therefore, you must be a Republican\"; \"There is no alternative to war.\" 12. Thought-terminating clich\u00e9. Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract attention away from other lines of thought (Hunter, 2015, p. 78). Ex.: \"it is what it is\"; \"you cannot judge it without experiencing it\"; \"it's common sense\", \"nothing is permanent except change\", \"better late than never\"; \"mind your own business\"; \"nobody's perfect\"; \"it doesn't matter\"; \"you can't change human nature.\" 13. Whataboutism. Discredit an opponent's position by charging them with hypocrisy without directly disproving their argument (Richter, 2017). For example, mentioning an event that discredits the opponent: \"What about . . . ?\" (Richter, 2017). Ex.: Russia Today had a proclivity for whataboutism in its coverage of the 2015 Baltimore and Ferguson protests in the US, which re-vealed a consistent refrain: \"the oppression of blacks in the US has become so unbearable that the eruption of violence was inevitable\", and that the US therefore lacks \"the moral high ground to discuss human rights issues in countries like Russia and China.\"\n\n14. Reductio ad Hitlerum. Persuading an audience to disapprove an action or idea by suggesting that the idea is popular with groups hated in contempt by the target audience. It can refer to any person or concept with a negative connotation (Teninbaum, 2009). Ex.: \"Only one kind of person can think this way: a communist.\"\n\n15. Red herring. Introducing irrelevant material to the issue being discussed, so that everyone's attention is diverted away from the points made (Weston, 2018, p. 78). Those subjected to a red herring argument are led away from the issue that had been the focus of the discussion and urged to follow an observation or claim that may be associated with the original claim, but is not highly relevant to the issue in dispute (Teninbaum, 2009). Ex.: \"You may claim that the death penalty is an ineffective deterrent against crime -but what about the victims of crime? How do you think surviving family members feel when they see the man who murdered their son kept in prison at their expense? Is it right that they should pay for their son's murderer to be fed and housed?\" 16. Bandwagon. Attempting to persuade the target audience to join in and take the course of action because \"everyone else is taking the same action\" (Hobbs and Mcgee, 2008). Ex.: \"Would you vote for Clinton as president? 57% say yes.\"\n\n17. Obfuscation, intentional vagueness, confusion. Using deliberately unclear words, so that the audience may have its own interpretation (Suprabandari, 2007;Weston, 2018, p. 8). For instance, when an unclear phrase with multiple possible meanings is used within the argument, and, therefore, it does not really support the conclusion. Ex.: \"It is a good idea to listen to victims of theft. Therefore, if the victims say to have the thief shot, then you should do it.\"\n\n18. Straw man. When an opponent's proposition is substituted with a similar one which is then refuted in place of the original (Walton, 1996). Weston (2018, p. 78) specifies the characteristics of the substituted proposition: \"caricaturing an opposing view so that it is easy to refute.\"   We provided the above definitions, together with some examples and an annotation schema, to our professional annotators, so that they can manually annotate news articles. The details are provided in the next section.\n\n\nData Creation\n\nWe retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic, which we annotated as described below.\n\n\nArticle Retrieval\n\nFirst, we selected 13 propagandistic and 36 nonpropagandistic news media outlets, as labeled by Media Bias/Fact Check. 3 Then, we retrieved articles from these sources, as shown in Table 2. Note that 82.5% of the articles are from propagandistic sources, and these articles tend to be longer. Table 3 shows the number of articles retrieved from each propagandistic outlet. Overall, we have 350k word tokens, which is comparable to standard datasets for other fine-grained text analysis tasks, such as named entity recognition, e.g., CoNLL'02 and CoNLL'03 covered 381K, 333K, 310K, and 301K tokens for Spanish, Dutch, German, and English, respectively (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003).\n\n\n5640\n\n\nManual Annotation\n\nWe aim at obtaining text fragments annotated with any of the 18 techniques described in Section 2 (see Figure 1 for an example). Since the time required to understand and memorize all the propaganda techniques is significant, this annotation task is not well-suited for crowdsourcing. We partnered instead with a company that performs professional annotations, A Data Pro. 4 Appendix A shows details about the instructions and the tools provided to the annotators.\n\nWe computed the \u03b3 inter-annotator agreement (Mathet et al., 2015). We chose \u03b3 because (i) it is designed for tasks where both the span and its label are to be found and (ii) it can deal with overlaps in the annotations by the same annotator 5 (e.g., instances of doubt often use name calling or loaded language to reinforce their message). We computed \u03b3 s , where we only consider the identified spans, regardless of the technique, and \u03b3 sl , where we consider both the spans and their labels.\n\nLet a be an annotator. In a preliminary exercise, four annotators a [1,..,4] annotated six articles independently, and the agreement was \u03b3 s = 0.34 and \u03b3 sl = 0.31. Even taking into account that \u03b3 is a pessimistic measure (Mathet et al., 2015), these values are low. Thus, we designed an annotation schema composed of two stages and involving two annotator teams, each of which covered about 220 documents. In stage 1, both a 1 and a 2 annotated the same documents independently. In stage 2, they gathered with a consolidator c 1 to discuss all instances and to come up with a final annotation. Annotators a 3 and a 4 and consolidator c 2 followed the same procedure. Annotating the full corpus took 395 man hours. Table 4 shows the \u03b3 agreements on the full corpus. As in the preliminary annotation, the agreements for both teams are relatively low: 0.30 and 0.34 for span selection, and slightly lower when labeling is considered as well. After the annotators discussed with the consolidator on the disagreed cases, the \u03b3 values got much higher: up to 0.74 and 0.76 for each team. We further analyzed the annotations to determine the main cause for the disagreement by computing the percentage of instances spotted by one annotator only in the first stage that are retained as gold annotations.  Table 4: \u03b3 inter-annotator agreement between annotators spotting spans alone (spans) and spotting spans+labeling (+labels). The top-2 rows refer to the first stage: agreement between annotators. The bottom 4 rows refer to the consolidation stage: agreement between each annotator and the final gold annotation. Overall the percentage is 53% (5, 921 out of 11, 122), and for each annotator is a 1 = 70%, a 2 = 48%, a 3 = 57%, a 4 = 31%. Observing such percentages together with the relatively low differences in Table 4 between \u03b3 s and \u03b3 sl for the same pairs (a i , a j ) and (a i , c j ), we can conclude that disagreements are in general not due to the two annotators assigning different labels to the same or mostly overlapping spans, but rather because one has missed an instance in the first stage.\n\n\nStatistics about the Dataset\n\nThe total number of technique instances found in the articles, after the consolidation phase, is 7, 485, with respect to a total number of 21, 230 sentences (35.2%). Table 5 reports some statistics about the annotations. The average propagandistic fragment has a length of 47 characters and the average length of a sentence is 112.5 characters.\n\nOn average, the propagandistic techniques are half a sentence long. The most common ones are loaded language and name calling, labeling with 2, 547 and 1, 294 occurrences, respectively. They appear 6.7 and 4.7 times per article, while no other technique appears more than twice. Note that repetition are inflated as we asked the annotators to mark both the original and the repeated instances.  \n\n\nEvaluation Measures\n\nOur task is a sequence labeling one, with the following key characteristics: (i) a large number of techniques whose spans might overlap in the text, and (ii) large lengths of these spans. This requires an evaluation measure that gives credit for partial overlaps. 6 We derive an ad hoc measure following related work on named entity recognition (NER) (Nadeau and Sekine, 2007) and (intrinsic) plagiarism detection (PD) (Potthast et al., 2010). While in NER, the relevant fragments tend to be short multi-word strings, in PD -and in our propaganda technique identification task-the length varies widely (cf. Table 5), and instances span from single tokens to full sentences or even longer pieces of text. Thus, in our precision and recall versions, we give partial credit to imperfect matches at the character level, as in PD.\n\nLet document d be represented as a sequence of characters. A propagandistic text fragment is then represented as t = [t i , . . . , t j ] \u2286 d. A document includes a set of (possibly overlapping) fragments T . Similarly, a learning algorithm produces a set S with fragments s = [s m , . . . , s n ], predicted on d. A labeling function l(x) \u2208 {1, . . . , 18} associates s \u2208 S to one of the eighteen techniques. Figure 2 gives examples of gold and predicted fragments. 6 The evaluation measures for the CoNLL'02 and CoNLL'03 NER tasks, where an instance is considered properly identified if and only if both the boundaries and the label are correct (Tsai et al., 2006), are not suitable in our context. represented as a sequence of characters. The class of each fragment is shown in parentheses. s 1 goes beyond t 1 's proper boundaries; s 2 and s 3 partially spot t 2 , but fail to identify it entirely; s 4 spots the exact boundaries of t 3 , but fails to assign it the right label.\n\nWe define the following function to handle partial overlaps between fragments with same labels:\nC(s, t, h) = |(s \u2229 t)| h \u03b4 (l(s), l(t)) ,(1)\nwhere h is a normalizing factor and \u03b4(a, b) = 1 if a = b, and 0 otherwise. In the future, \u03b4 could be refined to account for custom distance functions between classes, e.g., we might consider mistaking loaded language for name calling or labeling less problematic than confusing it with Reduction ad Hitlerum. Given Eq.\n\n(1), we now define variants of precision and recall able to account for the imbalance in the corpus:\nP (S, T ) = 1 |S| s \u2208 S, t \u2208 T C(s, t, |s|),(2)R(S, T ) = 1 |T | s \u2208 S, t \u2208 T C(s, t, |t|),(3)\nWe define Eq.\n\n(2) to be zero if |S| = 0 and Eq. (3) to be zero if |T | = 0. Following Potthast et al. (2010), in Eqs. (2) and (3) we penalize systems predicting too many or too few instances by dividing by |S| and |T |, respectively, e.g., in Figure 2 P ({s 2 , s 3 }, T ) < P ({s 3 }, T ). Finally, we combine Eqs. (2) and (3) into an F 1 -measure, the harmonic mean of precision and recall.\n\nHaving a separate function C to be responsible for comparing two annotations gives us some additional flexibility that is missing in standard NER measures that operate at the token/character level. For example, in Eq. (1) we could easily change the factor that gives credit for partial overlaps by being more forgiving when only few characters are wrong.\n\n\nTasks and Proposed Models\n\nWe define two tasks based on the corpus described in Section 3: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularities, g 1 and g 2 , i.e., tokens for FLC and sentences for SLC. We split the corpus into training, development and test, each containing 293, 57, 101 articles and 14,857, 2,108, 4,265 sentences.\n\n\nBaselines\n\nWe depart from BERT (Devlin et al., 2019), as it has achieved state-of-the-art performance on multiple NLP benchmarks, and we design three baselines based on it.\n\nBERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in (Devlin et al., 2019). For the FLC task, we feed the final hidden representation for each token to a layer L g 2 that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure 3-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer L g 1 to make a binary classification.\n\nBERT-Joint. We use the layers for both tasks in the BERT baseline, L g 1 and L g 2 , and we train for both FLC and SLC jointly (cf. Figure 3-b).\n\nBERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC. Instead of using only the L g 2 layer for FLC, we concatenate L g 1 and L g 2 , and we add an extra 19-dimensional classification layer L g 1,2 on top of that concatenation to perform the prediction for FLC (cf. Figure 3-c).\n\n\nMulti-Granularity Network\n\nWe propose a model that can drive the highergranularity task (FLC) on the basis of the lowergranularity information (SLC), rather than simply using low-granularity information directly. Figure 3-d shows the architecture of this model. More generally, suppose there are k tasks of increasing granularity, e.g., document-level, paragraphlevel, sentence-level, word-level, subword-level, character-level.  T1  T2  TN  CLS  T1  T2  TN   CLS  T1  T2  TN  CLS  T1  T2  TN  Each task has a separated classification layer L g k that receives the feature representation of the specific level of granularity g k and outputs o g k . The dimension of the representation depends on the embedding layer, while the dimension of the output depends on the number of classes in the task. The output o g k generates a weight for the next granularity task g k+1 through a trainable gate f :\n\n\nCLS\nw g k = f (o g k )(4)\nThe gate f consists of a projection layer to one dimension and an activation function. The resulting weight is multiplied by each element of the output of layer L g k+1 to produce the output for task g k+1 :\no g k+1 = w g k * o g k+1(5)\nIf w g k = 0 for a given example, the output of the next granularity task o g k+1 would be 0 as well. In our setting this means that, if the sentence-level classifier is confident the sentence does not contain propaganda, i.e., w g k = 0, then o g k+1 = 0 and there would be no propagandistic technique predicted for any span within that sentence. Similarly, when back-propagating the error, if w g k = 0 for a given example, the final entropy loss would become zero; i.e. the model would not get any information from that example. As a result, only examples strongly classified as negative in a lowergranularity task would be ignored in the highgranularity task. Having the lower-granularity as the main task means that higher-granularity information can be selectively used as additional information to improve the performance, but only if the example is not considered as highly negative. We show this in Section 6.3.\n\nFor the loss function, we use a cross-entropy loss with sigmoid activation for every layer, except for the highest-granularity layer L g K , which uses a cross-entropy loss with softmax activation. Unlike softmax, which normalizes over all dimensions, the sigmoid allows each output component of layer L g k to be independent from the rest. Thus, the output of the sigmoid for the positive class increases the degree of freedom by not affecting the negative class, and vice versa. As we have two tasks, we use sigmoid activation for L g 1 and softmax activation for L g 2 . Moreover, we use a weighted sum of losses with a hyper-parameter \u03b1:\nL J = L g 1 * \u03b1 + L g 2 * (1 \u2212 \u03b1)(6)\nAgain, we use BERT (Devlin et al., 2019) for the contextualized embedding layer and we place the multi-granularity network on top of it.\n\n\nExperiments and Evaluation\n\n\nExperimental Setup\n\nWe used the PyTorch framework and the pretrained BERT model, which we fine-tuned for our tasks. We trained all models using the following hyper-parameters: batch size of 16, sequence length of 210, weight decay of 0.01, and early stopping on validation F 1 with patience of 7. For optimization, we used Adam with a learning rate of 3e-5 and a warmup proportion of 0.1. To deal with class imbalance, we give weight to the binary cross-entropy according to the proportion of positive samples. For the \u03b1 in the joint loss function, we use 0.9 for sentence classification, and 0.1 for word-level classification. In order to reduce the effect of random fluctuations for BERT, all the reported numbers are the average of three experimental runs with different random seeds. As it is standard, we tune our models on the dev partition and we report results on the test partition. Table 6 shows the performance for the three baselines and for our multi-granularity network on the FLC task. For the latter, we vary the degree to which the gate function is applied: using ReLU is more aggressive compared to using the Sigmoid, as the ReLU outputs zero for a negative input. Note that, even though we train the model to predict both the spans and the labels, we also evaluated it with respect to the spans only.  Table 6: Fragment-level experiments (FLC task). Shown are two evaluations: (i) Spans checks only whether the model has identified the fragment spans correctly, while (ii) Full task is evaluation wrt the actual task of identifying the spans and also assigning the correct propaganda technique for each span. Table 6 shows that joint learning (BERT-Joint) hurts the performance compared to single-task BERT. However, using additional information from the sentence-level for the token-level classification (BERT-Granularity) yields small improvements. The multi-granularity models outperform all baselines thanks to their higher precision. This shows the effect of the model excluding sentences that it determined to be non-propagandistic from being considered for token-level classification. Nevertheless, the performance of sentencelevel classification is far from perfect, achieving an F 1 of up to 60.98 (cf. Table 7). The information it contributes to the final classification is noisy and the more conservative removal of instances performed by the Sigmoid function yields better performance than the more aggressive ReLU. Table 7 shows the results for the SLC task. We apply our multi-granularity network model to the sentence-level classification task to see its effect on low granularity when we train the model with a high granularity task. Interestingly, it yields huge performance improvements on the sentence-level classification result. Compared to the BERT baseline, it increases the recall by 8.42%, resulting in a 3.24% increase of the F 1 score. In this case, the result of token-level classification is used as additional information for the sentence-level task, and it helps to find more positive samples. This shows the opposite effect of our model compared to the FLC task. Note also that using ReLU is more effective than using the Sigmoid, unlike in tokenlevel classification.\n\n\nFragment-Level Propaganda Detection\n\n\nSentence-Level Propaganda Detection\n\n\nModel\n\nPrecision Recall F1  Table 7: Sentence-level (SLC) results. All-propaganda is a baseline which always output the propaganda class.\n\nThus, since the performance range of the tokenlevel classification is low, we think it is more effective to get additional information after aggressively removing negative samples by using ReLU as a gate in the model.\n\n\nRelated Work\n\nPropaganda identification has been tackled mostly at the article level. Rashkin et al. (2017)  A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017Habernal et al. ( , 2018a introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017(Habernal et al., , 2018a, our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fragments related to a technique instead of flagging entire arguments.\n\n\nConclusion and Future Work\n\nWe have argued for a new way to study propaganda in news media: by focusing on identifying the instances of use of specific propaganda techniques. Going at this fine-grained level can yield more reliable systems and it also makes it possible to explain to the user why an article was judged as propagandistic by an automatic system.\n\nIn particular, we designed an annotation schema of 18 propaganda techniques, and we annotated a sizable dataset of documents with instances of these techniques in use. We further designed an evaluation measure specifically tailored for this task. We made the schema and the dataset publicly available, thus facilitating further research. We hope that the corpus would raise interest outside of the community of researchers studying propaganda: the techniques related to fallacies and the ones relying on emotions might provide a novel setting for the researchers interested in Argumentation and Sentiment Analysis.\n\nWe experimented with a number of BERT-based models and devised a novel architecture which outperforms standard BERT-based baselines. Our fine-grained task can complement document-level judgments, both to come out with an aggregated decision and to explain why a document -or an entire news outlet-has been flagged as potentially propagandistic by an automatic system.\n\nWe are collaborating with A Data Pro to expand the corpus. In the mid-term, we plan to build an online platform where professors in relevant fields (e.g., journalism, mass communication) can train their students to recognize and annotate propaganda techniques. The hope is to be able to accumulate annotations as a by-product of using the platform for training purposes.\n\nFigure 1 :\n1Example given to the annotators.\n\nFigure 2 :\n2Example of gold annotation (top) and the predictions of a supervised model (bottom) in a document\n\nFigure 3 :\n3The architecture of the baseline models (a-c), and of our proposed multi-granularity network (d).\n\nTable 2 :\n2Statistics about the articles retrieved with respect to the category of the media source: propagandistic, non-propagandistic, and all together.News Outlet \n# News Outlet \n# \n\nFreedom Outpost \n133 The Remnant Magazine 14 \nFrontpage Magazine \n56 Breaking911 \n11 \nshtfplan.com \n55 truthuncensored.net \n8 \nLew Rockwell \n26 The Washington Standard 6 \nvdare.com \n20 www.unz.com \n5 \nremnantnewspaper.com 19 www.clashdaily.com \n1 \nPersonal Liberty \n18 \n\n\n\nTable 3 :\n3Number of articles retrieved from news outlets deemed propagandistic by Media Bias/Fact Check.\n\nTable 5 :\n5Corpus statistics including instances per technique and their avg. length in terms of characters.\n\n\nSigmoid 44.12 35.01 38.98 24.42 21.05 22.58Model \nSpans \nFull Task \nP \nR \nF 1 \nP \nR \nF 1 \n\nBERT \n39.57 36.42 37.90 21.48 21.39 21.39 \nJoint \n39.26 35.48 37.25 20.11 19.74 19.92 \nGranu \n43.08 33.98 37.93 23.85 20.14 21.80 \nMulti-Granularity \nReLU \n43.29 34.74 38.28 23.98 20.33 21.82 \n\n\n\ncreated a corpus of news articles labelled as belonging to four categories: propaganda, trusted, hoax, or satire. They included articles from eight sources, two of which are propagandistic. Barr\u00f3n-Cede\u00f1o et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories.The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise(Horne et al., 2018).\nThe corpus, the evaluation measures, and the models are available at http://propaganda.qcri.org/\nhttp://en.wikipedia.org/wiki/ Propaganda_techniques; last visit May 2019.\nhttp://mediabiasfactcheck.com/\nhttp://www.aiidatapro.com 5 See(Meyer et al., 2014;Mathet et al., 2015) for other alternatives, which lack some properties; (ii) in particular.\nhttp://tanbih.qcri.org/\nAcknowledgmentsThis research is part of the Tanbih project, 7 which aims to limit the effect of \"fake news\", propaganda and media bias by making users aware of what they are reading. The project is developed in collaboration between the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Qatar Computing Research Institute (QCRI), HBKU.\ncall them by their real name, 'Satan.' 782086447 exaggeration, minimization \u2022 heal the situation of extremely grave immoral behavior 761969038 doubt \u2022 Can the same be said for the Obama Administration?. \u2022 dismissing the protesters as \"lefties\" and hugging Barros publicly 701225819 repetition \u2022 Farrakhan repeatedly refers to Jews as \"Satan. He states to his audience. 696694316 appeal to fear/prejudice \u2022 A dark, impenetrable and \"irreversible\" winter of persecution of the faithful by their own shepherds will fallcalling, labeling \u2022 dismissing the protesters as \"lefties\" and hugging Barros publicly 701225819 repetition \u2022 Farrakhan repeatedly refers to Jews as \"Satan.\" He states to his audience [. . . ] call them by their real name, 'Satan.' 782086447 exaggeration, minimization \u2022 heal the situation of extremely grave immoral behavior 761969038 doubt \u2022 Can the same be said for the Obama Administration? 696694316 appeal to fear/prejudice \u2022 A dark, impenetrable and \"irreversible\" winter of persecution of the faithful by their own shepherds will fall.\n\nTrump 776368676 flag-waving \u2022 attempt (Mueller) to stop the will of We the People!!! It's time to jail Mueller 735815173 causal oversimplification \u2022 he said The people who talk about the \"Jewish question\" are generally anti-Semites. Somehow I don't think 781768042 causal oversimplification \u2022 will not be reversed, which leaves no alternative as to why God judges and is judging America today 111111113 slogans \u2022 BUILD THE WALL!\" Trump tweeted. 783702663 appeal to authority \u2022 Monsignor Jean-Fran\u00e7ois Lantheaume, who served as first Counsellor of the Nunciature in Washington, confirmed that \"Vigan\u00f2 said the truth. That's all\" 783702663 black-and-white fallacy \u2022 Francis said these words. J Usa! -Donald, flag-waving \u2022 conflicted, and his 17 Angry Democrats that are doing his dirty work are a disgrace to. If we do not oppose evil, we tacitly feed it. 729410793 thought-terminating cliches \u2022 I do not really see any problems there. Marx is the President 770156173 whataboutism \u2022 President Trump -who himself avoided national military service in the 1960's-keeps beating the war drums over North Korea 778139122 reductio ad hitlerum \u2022 \"Vichy journalism,\" a term which now fits so much of the mainstream media. It collaborates in the same way that the Vichy government in France collaborated with the Nazis. 778139122 red herring \u2022 It describes the tsunami of vindictive personal abuse that has been heaped upon Julian from well-known journalists, many claiming liberal credentials. The Guardian, which used to consider itself the most enlightened newspaper in the country, has probably been the worstflag-waving \u2022 conflicted, and his 17 Angry Democrats that are doing his dirty work are a disgrace to USA! -Donald J. Trump 776368676 flag-waving \u2022 attempt (Mueller) to stop the will of We the People!!! It's time to jail Mueller 735815173 causal oversimplification \u2022 he said The people who talk about the \"Jewish question\" are gen- erally anti-Semites. Somehow I don't think 781768042 causal oversimplification \u2022 will not be reversed, which leaves no alternative as to why God judges and is judging America today 111111113 slogans \u2022 BUILD THE WALL!\" Trump tweeted. 783702663 appeal to authority \u2022 Monsignor Jean-Fran\u00e7ois Lantheaume, who served as first Counsellor of the Nunciature in Washington, confirmed that \"Vigan\u00f2 said the truth. That's all\" 783702663 black-and-white fallacy \u2022 Francis said these words: \"Everyone is guilty for the good he could have done and did not do . . . If we do not oppose evil, we tacitly feed it. 729410793 thought-terminating cliches \u2022 I do not really see any problems there. Marx is the President 770156173 whataboutism \u2022 President Trump -who himself avoided national military service in the 1960's-keeps beating the war drums over North Korea 778139122 reductio ad hitlerum \u2022 \"Vichy journalism,\" a term which now fits so much of the mainstream media. It collaborates in the same way that the Vichy government in France collaborated with the Nazis. 778139122 red herring \u2022 It describes the tsunami of vindictive personal abuse that has been heaped upon Julian from well-known journalists, many claiming liberal credentials. The Guardian, which used to consider itself the most enlightened newspaper in the country, has probably been the worst.\n\nthere is a possibility of liturgical \"blessing\" of gay unions, he answered the question in a more subtle way without giving an explicit \"yes. EU no longer considers #Hamas a terrorist group. Which is just Allen's more nuanced way of saying. Don't believe itbandwagon \u2022 He tweeted, \"EU no longer considers #Hamas a terrorist group. Time for US to do same.\" 729410793 obfusc., int. vagueness, confusion \u2022 The cardinal's office maintains that rather than saying \"yes,\" there is a possibility of liturgical \"blessing\" of gay unions, he answered the question in a more subtle way without giving an explicit \"yes.\" 783702663 straw man \u2022 \"Take it seriously, but with a large grain of salt.\" Which is just Allen's more nuanced way of saying: \"Don't believe it.\"\n\nProppy: A system to unmask propaganda in online news. Alberto Barr\u00f3n-Cede\u00f1o, Giovanni Da San, Israa Martino, Preslav Jaradat, Nakov, Proceedings of the 33rd AAAI Conference on Artificial Intelligence, AAAI '19. the 33rd AAAI Conference on Artificial Intelligence, AAAI '19Honolulu, HI, USAAlberto Barr\u00f3n-Cede\u00f1o, Giovanni Da San Martino, Is- raa Jaradat, and Preslav Nakov. 2019. Proppy: A system to unmask propaganda in online news. In Proceedings of the 33rd AAAI Conference on Artifi- cial Intelligence, AAAI '19, pages 9847-9848, Hon- olulu, HI, USA.\n\nProppy: Organizing the news based on their propagandistic content. Alberto Barr\u00f3n-Cedeno, Israa Jaradat, Giovanni Da San, Preslav Martino, Nakov, Information Processing & Management. 565Alberto Barr\u00f3n-Cedeno, Israa Jaradat, Giovanni Da San Martino, and Preslav Nakov. 2019. Proppy: Organizing the news based on their propagandistic content. Information Processing & Management, 56(5):1849-1864.\n\nTechniques for the Translation of Advertising Slogans. Lavinia Dan , Proceedings of the International Conference Literature, Discourse and Multicultural Dialogue, LDMD '15. the International Conference Literature, Discourse and Multicultural Dialogue, LDMD '15Mures, RomaniaLavinia Dan. 2015. Techniques for the Translation of Advertising Slogans. In Proceedings of the Interna- tional Conference Literature, Discourse and Multi- cultural Dialogue, LDMD '15, pages 13-23, Mures, Romania.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '19. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '19MN, USAMinneapolisJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, NAACL-HLT '19, pages 4171-4186, Min- neapolis, MN, USA.\n\nAccounting for the force of the appeal to authority. Jean Goodwin, Proceedings of the 9th International Conference of the Ontario Society for the Study of Argumentation, OSSA '11. the 9th International Conference of the Ontario Society for the Study of Argumentation, OSSA '11Ontario, CanadaJean Goodwin. 2011. Accounting for the force of the appeal to authority. In Proceedings of the 9th Inter- national Conference of the Ontario Society for the Study of Argumentation, OSSA '11, pages 1-9, On- tario, Canada.\n\nArgotario: Computational argumentation meets serious games. Ivan Habernal, Raffael Hannemann, Christian Pollak, Christopher Klamm, Patrick Pauli, Iryna Gurevych, Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP '17. the Conference on Empirical Methods in Natural Language Processing, EMNLP '17Copenhagen, DenmarkIvan Habernal, Raffael Hannemann, Christian Pol- lak, Christopher Klamm, Patrick Pauli, and Iryna Gurevych. 2017. Argotario: Computational argu- mentation meets serious games. In Proceedings of the Conference on Empirical Methods in Natu- ral Language Processing, EMNLP '17, pages 7-12, Copenhagen, Denmark.\n\nAdapting serious game for fallacious argumentation to German: pitfalls, insights, and best practices. Ivan Habernal, Patrick Pauli, Iryna Gurevych, Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC '18. the Eleventh International Conference on Language Resources and Evaluation, LREC '18Miyazaki, JapanIvan Habernal, Patrick Pauli, and Iryna Gurevych. 2018a. Adapting serious game for fallacious argu- mentation to German: pitfalls, insights, and best practices. In Proceedings of the Eleventh Interna- tional Conference on Language Resources and Eval- uation, LREC '18, Miyazaki, Japan.\n\nBefore name-calling: Dynamics and triggers of ad hominem fallacies in web argumentation. Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, Benno Stein, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '18New Orleans, LA, USAIvan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018b. Before name-calling: Dy- namics and triggers of ad hominem fallacies in web argumentation. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, NAACL-HLT '18, pages 386- 396, New Orleans, LA, USA.\n\nTeaching about propaganda: An examination of the historical roots of media literacy. Renee Hobbs, Sandra Mcgee, Journal of Media Literacy Education. 662Renee Hobbs and Sandra Mcgee. 2008. Teaching about propaganda: An examination of the historical roots of media literacy. Journal of Media Literacy Education, 6(62):56-67.\n\nSampling the news producers: A large news and feature data set for the study of the complex media landscape. Sara Benjamin D Horne, Sibel Khedr, Adali, International AAAI Conference on Web and Social Media, ICWSM '18. Stanford, CA, USABenjamin D Horne, Sara Khedr, and Sibel Adali. 2018. Sampling the news producers: A large news and fea- ture data set for the study of the complex media land- scape. In International AAAI Conference on Web and Social Media, ICWSM '18, Stanford, CA, USA.\n\nBrainwashing in a large group awareness training? The classical conditioning hypothesis of brainwashing. John Hunter, Pietermaritzburg, South AfricaUniversity of Kwazulu-NatalMaster's thesisJohn Hunter. 2015. Brainwashing in a large group awareness training? The classical conditioning hy- pothesis of brainwashing. Master's thesis, Uni- versity of Kwazulu-Natal, Pietermaritzburg, South Africa.\n\nWhat is propaganda, and how does it differ from persuasion?. Garth S Jowett, O&apos; Victoria, Donnell, Propaganda & Persuasion, chapter 1. Sage PublishingGarth S. Jowett and Victoria O'Donnell. 2012. What is propaganda, and how does it differ from persuasion? In Propaganda & Persuasion, chapter 1, pages 1-48. Sage Publishing.\n\nThe unified and holistic method gamma (\u03b3) for inter-annotator agreement measure and alignment. Yann Mathet, Antoine Widl\u00f6cher, Jean-Philippe M\u00e9tivier, Computational Linguistics. 413Yann Mathet, Antoine Widl\u00f6cher, and Jean-Philippe M\u00e9tivier. 2015. The unified and holistic method gamma (\u03b3) for inter-annotator agreement mea- sure and alignment. Computational Linguistics, 41(3):437-479.\n\nDKPro agreement: An open-source Java library for measuring inter-rater agreement. Christian M Meyer, Margot Mieskes, Christian Stab, Iryna Gurevych, Proceedings of the International Conference on Computational Linguistics, COL-ING '14. the International Conference on Computational Linguistics, COL-ING '14Dublin, IrelandChristian M. Meyer, Margot Mieskes, Christian Stab, and Iryna Gurevych. 2014. DKPro agreement: An open-source Java library for measuring inter-rater agreement. In Proceedings of the International Conference on Computational Linguistics, COL- ING '14, pages 105-109, Dublin, Ireland.\n\nThe Techniques of Propaganda. From \"How to Detect and Analyze Propaganda. R Clyde, Miller, an address given at Town Hall. The Center for learningClyde R. Miller. 1939. The Techniques of Propaganda. From \"How to Detect and Analyze Propaganda,\" an address given at Town Hall. The Center for learning.\n\nA survey of named entity recognition and classification. David Nadeau, Satoshi Sekine, Lingvisticae Investigationes. 30David Nadeau and Satoshi Sekine. 2007. A sur- vey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3-26.\n\nAn evaluation framework for plagiarism detection. Martin Potthast, Benno Stein, Alberto Barr\u00f3n-Cede\u00f1o, Paolo Rosso, Proceedings of the International Conference on Computational Linguistics, COLING '10. the International Conference on Computational Linguistics, COLING '10Beijing, ChinaMartin Potthast, Benno Stein, Alberto Barr\u00f3n-Cede\u00f1o, and Paolo Rosso. 2010. An evaluation framework for plagiarism detection. In Proceedings of the In- ternational Conference on Computational Linguis- tics, COLING '10, pages 997-1005, Beijing, China.\n\nTruth of varying shades: Analyzing language in fake news and political fact-checking. Eunsol Hannah Rashkin, Jin Yea Choi, Svitlana Jang, Yejin Volkova, Choi, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP '17. the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP '17Copenhagen, DenmarkHannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. Truth of varying shades: Analyzing language in fake news and polit- ical fact-checking. In Proceedings of the 2017 Con- ference on Empirical Methods in Natural Language Processing, EMNLP '17, pages 2931-2937, Copen- hagen, Denmark.\n\nThe Kremlin's platform for 'useful idiots' in the West: An overview of RT's editorial strategy and evidence of impact. L Monika, Richter, Kremlin WatchTechnical reportMonika L Richter. 2017. The Kremlin's platform for 'useful idiots' in the West: An overview of RT's ed- itorial strategy and evidence of impact. Technical report, Kremlin Watch.\n\nAmerican propaganda in John Steinbeck's The Moon is Down. Master's thesis. Francisca Niken Vitri Suprabandari, Yogyakarta, IndonesiaSanata Dharma UniversityFrancisca Niken Vitri Suprabandari. 2007. Ameri- can propaganda in John Steinbeck's The Moon is Down. Master's thesis, Sanata Dharma University, Yogyakarta, Indonesia.\n\nReductio ad Hitlerum: Trumping the judicial Nazi card. Michigan State Law Review. H Gabriel, Teninbaum, 541Gabriel H Teninbaum. 2009. Reductio ad Hitlerum: Trumping the judicial Nazi card. Michigan State Law Review, page 541.\n\nIntroduction to the CoNLL-2002 shared task: Language-independent named entity recognition. Erik F , Tjong Kim Sang, Proceedings of the 6th Conference on Natural Language Learning, CoNLL '02. the 6th Conference on Natural Language Learning, CoNLL '02Taipei, TaiwanErik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In Proceedings of the 6th Conference on Natural Language Learning, CoNLL '02, pages 155-158, Taipei, Taiwan.\n\nIntroduction to the CoNLL-2003 shared task: Language-independent named entity recognition. Erik F Tjong, Kim Sang, Fien De Meulder, Proceedings of the 7th Conference on Natural Language Learning, CoNLL '03. the 7th Conference on Natural Language Learning, CoNLL '03Edmonton, CanadaErik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the 7th Conference on Natural Lan- guage Learning, CoNLL '03, pages 142-147, Ed- monton, Canada.\n\nSymbiotic radicalisation strategies: Propaganda tools and neuro linguistic programming. Robyn Torok, Proceedings of the Australian Security and Intelligence Conference. the Australian Security and Intelligence ConferencePerth, AustraliaRobyn Torok. 2015. Symbiotic radicalisation strate- gies: Propaganda tools and neuro linguistic pro- gramming. In Proceedings of the Australian Se- curity and Intelligence Conference, pages 58-65, Perth, Australia.\n\nVarious criteria in the evaluation of biomedical named entity recognition. Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-Yi Sung, Wen-Lian Hsu, BMC bioinformatics. 792Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting- Yi Sung, and Wen-Lian Hsu. 2006. Various criteria in the evaluation of biomedical named entity recog- nition. BMC bioinformatics, 7:92.\n\nThe straw man fallacy. Douglas Walton, Royal Netherlands Academy of Arts and Sciences. Douglas Walton. 1996. The straw man fallacy. Royal Netherlands Academy of Arts and Sciences.\n\nA rulebook for arguments. Anthony Weston, Hackett PublishingAnthony Weston. 2018. A rulebook for arguments. Hackett Publishing.\n", "annotations": {"author": "[{\"end\":184,\"start\":172},{\"end\":265,\"start\":185},{\"end\":377,\"start\":266},{\"end\":463,\"start\":378},{\"end\":481,\"start\":464},{\"end\":562,\"start\":482}]", "publisher": "[{\"end\":95,\"start\":54},{\"end\":980,\"start\":939}]", "author_last_name": "[{\"end\":183,\"start\":181},{\"end\":196,\"start\":189},{\"end\":277,\"start\":275},{\"end\":399,\"start\":386},{\"end\":480,\"start\":474},{\"end\":495,\"start\":490}]", "author_first_name": "[{\"end\":180,\"start\":172},{\"end\":188,\"start\":185},{\"end\":274,\"start\":266},{\"end\":385,\"start\":378},{\"end\":473,\"start\":464},{\"end\":489,\"start\":482}]", "author_affiliation": "[{\"end\":264,\"start\":219},{\"end\":376,\"start\":302},{\"end\":462,\"start\":401},{\"end\":561,\"start\":516}]", "title": "[{\"end\":53,\"start\":1},{\"end\":615,\"start\":563}]", "venue": "[{\"end\":777,\"start\":617}]", "abstract": "[{\"end\":1912,\"start\":1009}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2023,\"start\":1995},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2044,\"start\":2023},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2429,\"start\":2409},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5668,\"start\":5654},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6061,\"start\":6048},{\"end\":6090,\"start\":6063},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6140,\"start\":6127},{\"end\":6756,\"start\":6718},{\"end\":7055,\"start\":7035},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7302,\"start\":7288},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7482,\"start\":7469},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7495,\"start\":7482},{\"end\":7779,\"start\":7744},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8588,\"start\":8565},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9353,\"start\":9342},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9561,\"start\":9546},{\"end\":9735,\"start\":9699},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9893,\"start\":9880},{\"end\":10471,\"start\":10450},{\"end\":11858,\"start\":11837},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12635,\"start\":12612},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12857,\"start\":12837},{\"end\":12876,\"start\":12857},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13310,\"start\":13296},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14523,\"start\":14501},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14559,\"start\":14523},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15120,\"start\":15099},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15793,\"start\":15772},{\"end\":18713,\"start\":18712},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18824,\"start\":18799},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18890,\"start\":18867},{\"end\":19743,\"start\":19742},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19941,\"start\":19922},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21025,\"start\":21003},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22308,\"start\":22287},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22531,\"start\":22510},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26232,\"start\":26211},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30281,\"start\":30258},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30392,\"start\":30371},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30417,\"start\":30392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30751,\"start\":30729},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30776,\"start\":30751},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34468,\"start\":34448},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34723,\"start\":34703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34743,\"start\":34723}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32744,\"start\":32699},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32855,\"start\":32745},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32966,\"start\":32856},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33425,\"start\":32967},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33532,\"start\":33426},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33642,\"start\":33533},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33929,\"start\":33643},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34469,\"start\":33930}]", "paragraph": "[{\"end\":2838,\"start\":1928},{\"end\":3222,\"start\":2840},{\"end\":3511,\"start\":3224},{\"end\":4240,\"start\":3513},{\"end\":4329,\"start\":4242},{\"end\":4378,\"start\":4331},{\"end\":4423,\"start\":4380},{\"end\":4547,\"start\":4425},{\"end\":5404,\"start\":4549},{\"end\":5913,\"start\":5438},{\"end\":6831,\"start\":5915},{\"end\":7357,\"start\":6833},{\"end\":7496,\"start\":7359},{\"end\":11365,\"start\":7530},{\"end\":11689,\"start\":11367},{\"end\":12697,\"start\":11691},{\"end\":13167,\"start\":12699},{\"end\":13675,\"start\":13169},{\"end\":13828,\"start\":13693},{\"end\":14560,\"start\":13850},{\"end\":15053,\"start\":14589},{\"end\":15548,\"start\":15055},{\"end\":17650,\"start\":15550},{\"end\":18027,\"start\":17683},{\"end\":18424,\"start\":18029},{\"end\":19273,\"start\":18448},{\"end\":20257,\"start\":19275},{\"end\":20354,\"start\":20259},{\"end\":20718,\"start\":20400},{\"end\":20820,\"start\":20720},{\"end\":20929,\"start\":20916},{\"end\":21309,\"start\":20931},{\"end\":21665,\"start\":21311},{\"end\":22253,\"start\":21695},{\"end\":22428,\"start\":22267},{\"end\":22964,\"start\":22430},{\"end\":23110,\"start\":22966},{\"end\":23425,\"start\":23112},{\"end\":24325,\"start\":23455},{\"end\":24561,\"start\":24354},{\"end\":25511,\"start\":24591},{\"end\":26154,\"start\":25513},{\"end\":26328,\"start\":26192},{\"end\":29578,\"start\":26380},{\"end\":29794,\"start\":29664},{\"end\":30013,\"start\":29796},{\"end\":30978,\"start\":30030},{\"end\":31341,\"start\":31009},{\"end\":31957,\"start\":31343},{\"end\":32326,\"start\":31959},{\"end\":32698,\"start\":32328}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20399,\"start\":20355},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20868,\"start\":20821},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20915,\"start\":20868},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24353,\"start\":24332},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24590,\"start\":24562},{\"attributes\":{\"id\":\"formula_5\"},\"end\":26191,\"start\":26155}]", "table_ref": "[{\"end\":6911,\"start\":6889},{\"end\":9003,\"start\":8996},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14038,\"start\":14031},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14150,\"start\":14143},{\"end\":16272,\"start\":16265},{\"end\":16854,\"start\":16847},{\"end\":17365,\"start\":17358},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17856,\"start\":17849},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19063,\"start\":19055},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23920,\"start\":23858},{\"end\":27259,\"start\":27252},{\"end\":27688,\"start\":27681},{\"end\":27995,\"start\":27988},{\"end\":28598,\"start\":28591},{\"end\":28814,\"start\":28807},{\"end\":29692,\"start\":29685}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1926,\"start\":1914},{\"attributes\":{\"n\":\"2\"},\"end\":5436,\"start\":5407},{\"attributes\":{\"n\":\"4.\"},\"end\":7528,\"start\":7499},{\"attributes\":{\"n\":\"3\"},\"end\":13691,\"start\":13678},{\"attributes\":{\"n\":\"3.1\"},\"end\":13848,\"start\":13831},{\"end\":14567,\"start\":14563},{\"attributes\":{\"n\":\"3.2\"},\"end\":14587,\"start\":14570},{\"attributes\":{\"n\":\"3.3\"},\"end\":17681,\"start\":17653},{\"attributes\":{\"n\":\"4\"},\"end\":18446,\"start\":18427},{\"attributes\":{\"n\":\"5\"},\"end\":21693,\"start\":21668},{\"attributes\":{\"n\":\"5.1\"},\"end\":22265,\"start\":22256},{\"attributes\":{\"n\":\"5.2\"},\"end\":23453,\"start\":23428},{\"end\":24331,\"start\":24328},{\"attributes\":{\"n\":\"6\"},\"end\":26357,\"start\":26331},{\"attributes\":{\"n\":\"6.1\"},\"end\":26378,\"start\":26360},{\"attributes\":{\"n\":\"6.2\"},\"end\":29616,\"start\":29581},{\"attributes\":{\"n\":\"6.3\"},\"end\":29654,\"start\":29619},{\"end\":29662,\"start\":29657},{\"attributes\":{\"n\":\"7\"},\"end\":30028,\"start\":30016},{\"attributes\":{\"n\":\"8\"},\"end\":31007,\"start\":30981},{\"end\":32710,\"start\":32700},{\"end\":32756,\"start\":32746},{\"end\":32867,\"start\":32857},{\"end\":32977,\"start\":32968},{\"end\":33436,\"start\":33427},{\"end\":33543,\"start\":33534}]", "table": "[{\"end\":33425,\"start\":33122},{\"end\":33929,\"start\":33688}]", "figure_caption": "[{\"end\":32744,\"start\":32712},{\"end\":32855,\"start\":32758},{\"end\":32966,\"start\":32869},{\"end\":33122,\"start\":32979},{\"end\":33532,\"start\":33438},{\"end\":33642,\"start\":33545},{\"end\":33688,\"start\":33645},{\"end\":34469,\"start\":33932}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14700,\"start\":14692},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19693,\"start\":19685},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22762,\"start\":22751},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23109,\"start\":23098},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23424,\"start\":23413},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23649,\"start\":23641}]", "bib_author_first_name": "[{\"end\":36954,\"start\":36953},{\"end\":40358,\"start\":40351},{\"end\":40382,\"start\":40374},{\"end\":40396,\"start\":40391},{\"end\":40413,\"start\":40406},{\"end\":40926,\"start\":40919},{\"end\":40947,\"start\":40942},{\"end\":40965,\"start\":40957},{\"end\":40981,\"start\":40974},{\"end\":41310,\"start\":41303},{\"end\":41314,\"start\":41311},{\"end\":41824,\"start\":41819},{\"end\":41841,\"start\":41833},{\"end\":41855,\"start\":41849},{\"end\":41869,\"start\":41861},{\"end\":42619,\"start\":42615},{\"end\":43139,\"start\":43135},{\"end\":43157,\"start\":43150},{\"end\":43178,\"start\":43169},{\"end\":43198,\"start\":43187},{\"end\":43213,\"start\":43206},{\"end\":43226,\"start\":43221},{\"end\":43842,\"start\":43838},{\"end\":43860,\"start\":43853},{\"end\":43873,\"start\":43868},{\"end\":44464,\"start\":44460},{\"end\":44482,\"start\":44475},{\"end\":44499,\"start\":44494},{\"end\":44515,\"start\":44510},{\"end\":45306,\"start\":45301},{\"end\":45320,\"start\":45314},{\"end\":45653,\"start\":45649},{\"end\":45677,\"start\":45672},{\"end\":46139,\"start\":46135},{\"end\":46493,\"start\":46488},{\"end\":46495,\"start\":46494},{\"end\":46511,\"start\":46504},{\"end\":46856,\"start\":46852},{\"end\":46872,\"start\":46865},{\"end\":46897,\"start\":46884},{\"end\":47235,\"start\":47226},{\"end\":47237,\"start\":47236},{\"end\":47251,\"start\":47245},{\"end\":47270,\"start\":47261},{\"end\":47282,\"start\":47277},{\"end\":47824,\"start\":47823},{\"end\":48111,\"start\":48106},{\"end\":48127,\"start\":48120},{\"end\":48365,\"start\":48359},{\"end\":48381,\"start\":48376},{\"end\":48396,\"start\":48389},{\"end\":48417,\"start\":48412},{\"end\":48938,\"start\":48932},{\"end\":48958,\"start\":48955},{\"end\":48962,\"start\":48959},{\"end\":48977,\"start\":48969},{\"end\":48989,\"start\":48984},{\"end\":49641,\"start\":49640},{\"end\":49963,\"start\":49942},{\"end\":50275,\"start\":50274},{\"end\":50514,\"start\":50510},{\"end\":50516,\"start\":50515},{\"end\":50528,\"start\":50519},{\"end\":51008,\"start\":51004},{\"end\":51010,\"start\":51009},{\"end\":51021,\"start\":51018},{\"end\":51032,\"start\":51028},{\"end\":51544,\"start\":51539},{\"end\":51995,\"start\":51978},{\"end\":52011,\"start\":52002},{\"end\":52023,\"start\":52016},{\"end\":52037,\"start\":52030},{\"end\":52047,\"start\":52043},{\"end\":52056,\"start\":52052},{\"end\":52072,\"start\":52065},{\"end\":52087,\"start\":52079},{\"end\":52376,\"start\":52369},{\"end\":52560,\"start\":52553}]", "bib_author_last_name": "[{\"end\":36967,\"start\":36955},{\"end\":40372,\"start\":40359},{\"end\":40389,\"start\":40383},{\"end\":40404,\"start\":40397},{\"end\":40421,\"start\":40414},{\"end\":40428,\"start\":40423},{\"end\":40940,\"start\":40927},{\"end\":40955,\"start\":40948},{\"end\":40972,\"start\":40966},{\"end\":40989,\"start\":40982},{\"end\":40996,\"start\":40991},{\"end\":41831,\"start\":41825},{\"end\":41847,\"start\":41842},{\"end\":41859,\"start\":41856},{\"end\":41879,\"start\":41870},{\"end\":42627,\"start\":42620},{\"end\":43148,\"start\":43140},{\"end\":43167,\"start\":43158},{\"end\":43185,\"start\":43179},{\"end\":43204,\"start\":43199},{\"end\":43219,\"start\":43214},{\"end\":43235,\"start\":43227},{\"end\":43851,\"start\":43843},{\"end\":43866,\"start\":43861},{\"end\":43882,\"start\":43874},{\"end\":44473,\"start\":44465},{\"end\":44492,\"start\":44483},{\"end\":44508,\"start\":44500},{\"end\":44521,\"start\":44516},{\"end\":45312,\"start\":45307},{\"end\":45326,\"start\":45321},{\"end\":45670,\"start\":45654},{\"end\":45683,\"start\":45678},{\"end\":45690,\"start\":45685},{\"end\":46146,\"start\":46140},{\"end\":46502,\"start\":46496},{\"end\":46520,\"start\":46512},{\"end\":46529,\"start\":46522},{\"end\":46863,\"start\":46857},{\"end\":46882,\"start\":46873},{\"end\":46906,\"start\":46898},{\"end\":47243,\"start\":47238},{\"end\":47259,\"start\":47252},{\"end\":47275,\"start\":47271},{\"end\":47291,\"start\":47283},{\"end\":47830,\"start\":47825},{\"end\":47838,\"start\":47832},{\"end\":48118,\"start\":48112},{\"end\":48134,\"start\":48128},{\"end\":48374,\"start\":48366},{\"end\":48387,\"start\":48382},{\"end\":48410,\"start\":48397},{\"end\":48423,\"start\":48418},{\"end\":48953,\"start\":48939},{\"end\":48967,\"start\":48963},{\"end\":48982,\"start\":48978},{\"end\":48997,\"start\":48990},{\"end\":49003,\"start\":48999},{\"end\":49648,\"start\":49642},{\"end\":49657,\"start\":49650},{\"end\":49976,\"start\":49964},{\"end\":50283,\"start\":50276},{\"end\":50294,\"start\":50285},{\"end\":50533,\"start\":50529},{\"end\":51016,\"start\":51011},{\"end\":51026,\"start\":51022},{\"end\":51043,\"start\":51033},{\"end\":51550,\"start\":51545},{\"end\":52000,\"start\":51996},{\"end\":52014,\"start\":52012},{\"end\":52028,\"start\":52024},{\"end\":52041,\"start\":52038},{\"end\":52050,\"start\":52048},{\"end\":52063,\"start\":52057},{\"end\":52077,\"start\":52073},{\"end\":52091,\"start\":52088},{\"end\":52383,\"start\":52377},{\"end\":52567,\"start\":52561}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36261,\"start\":35202},{\"attributes\":{\"id\":\"b1\"},\"end\":39540,\"start\":36263},{\"attributes\":{\"id\":\"b2\"},\"end\":40295,\"start\":39542},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":198189779},\"end\":40850,\"start\":40297},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":181473906},\"end\":41246,\"start\":40852},{\"attributes\":{\"id\":\"b5\"},\"end\":41735,\"start\":41248},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":42560,\"start\":41737},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":145609096},\"end\":43073,\"start\":42562},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":29489521},\"end\":43734,\"start\":43075},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":21731570},\"end\":44369,\"start\":43736},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3504755},\"end\":45214,\"start\":44371},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":140955628},\"end\":45538,\"start\":45216},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4399052},\"end\":46028,\"start\":45540},{\"attributes\":{\"id\":\"b13\"},\"end\":46425,\"start\":46030},{\"attributes\":{\"id\":\"b14\"},\"end\":46755,\"start\":46427},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14312501},\"end\":47142,\"start\":46757},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14505508},\"end\":47747,\"start\":47144},{\"attributes\":{\"id\":\"b17\"},\"end\":48047,\"start\":47749},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8310135},\"end\":48307,\"start\":48049},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10398193},\"end\":48844,\"start\":48309},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":29298828},\"end\":49519,\"start\":48846},{\"attributes\":{\"id\":\"b21\"},\"end\":49865,\"start\":49521},{\"attributes\":{\"id\":\"b22\"},\"end\":50190,\"start\":49867},{\"attributes\":{\"id\":\"b23\"},\"end\":50417,\"start\":50192},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3262157},\"end\":50911,\"start\":50419},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2470716},\"end\":51449,\"start\":50913},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53534573},\"end\":51901,\"start\":51451},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14566859},\"end\":52344,\"start\":51903},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204519913},\"end\":52525,\"start\":52346},{\"attributes\":{\"id\":\"b29\"},\"end\":52654,\"start\":52527}]", "bib_title": "[{\"end\":35403,\"start\":35202},{\"end\":36951,\"start\":36263},{\"end\":39682,\"start\":39542},{\"end\":40349,\"start\":40297},{\"end\":40917,\"start\":40852},{\"end\":41301,\"start\":41248},{\"end\":41817,\"start\":41737},{\"end\":42613,\"start\":42562},{\"end\":43133,\"start\":43075},{\"end\":43836,\"start\":43736},{\"end\":44458,\"start\":44371},{\"end\":45299,\"start\":45216},{\"end\":45647,\"start\":45540},{\"end\":46486,\"start\":46427},{\"end\":46850,\"start\":46757},{\"end\":47224,\"start\":47144},{\"end\":48104,\"start\":48049},{\"end\":48357,\"start\":48309},{\"end\":48930,\"start\":48846},{\"end\":50508,\"start\":50419},{\"end\":51002,\"start\":50913},{\"end\":51537,\"start\":51451},{\"end\":51976,\"start\":51903},{\"end\":52367,\"start\":52346}]", "bib_author": "[{\"end\":36969,\"start\":36953},{\"end\":40374,\"start\":40351},{\"end\":40391,\"start\":40374},{\"end\":40406,\"start\":40391},{\"end\":40423,\"start\":40406},{\"end\":40430,\"start\":40423},{\"end\":40942,\"start\":40919},{\"end\":40957,\"start\":40942},{\"end\":40974,\"start\":40957},{\"end\":40991,\"start\":40974},{\"end\":40998,\"start\":40991},{\"end\":41317,\"start\":41303},{\"end\":41833,\"start\":41819},{\"end\":41849,\"start\":41833},{\"end\":41861,\"start\":41849},{\"end\":41881,\"start\":41861},{\"end\":42629,\"start\":42615},{\"end\":43150,\"start\":43135},{\"end\":43169,\"start\":43150},{\"end\":43187,\"start\":43169},{\"end\":43206,\"start\":43187},{\"end\":43221,\"start\":43206},{\"end\":43237,\"start\":43221},{\"end\":43853,\"start\":43838},{\"end\":43868,\"start\":43853},{\"end\":43884,\"start\":43868},{\"end\":44475,\"start\":44460},{\"end\":44494,\"start\":44475},{\"end\":44510,\"start\":44494},{\"end\":44523,\"start\":44510},{\"end\":45314,\"start\":45301},{\"end\":45328,\"start\":45314},{\"end\":45672,\"start\":45649},{\"end\":45685,\"start\":45672},{\"end\":45692,\"start\":45685},{\"end\":46148,\"start\":46135},{\"end\":46504,\"start\":46488},{\"end\":46522,\"start\":46504},{\"end\":46531,\"start\":46522},{\"end\":46865,\"start\":46852},{\"end\":46884,\"start\":46865},{\"end\":46908,\"start\":46884},{\"end\":47245,\"start\":47226},{\"end\":47261,\"start\":47245},{\"end\":47277,\"start\":47261},{\"end\":47293,\"start\":47277},{\"end\":47832,\"start\":47823},{\"end\":47840,\"start\":47832},{\"end\":48120,\"start\":48106},{\"end\":48136,\"start\":48120},{\"end\":48376,\"start\":48359},{\"end\":48389,\"start\":48376},{\"end\":48412,\"start\":48389},{\"end\":48425,\"start\":48412},{\"end\":48955,\"start\":48932},{\"end\":48969,\"start\":48955},{\"end\":48984,\"start\":48969},{\"end\":48999,\"start\":48984},{\"end\":49005,\"start\":48999},{\"end\":49650,\"start\":49640},{\"end\":49659,\"start\":49650},{\"end\":49978,\"start\":49942},{\"end\":50285,\"start\":50274},{\"end\":50296,\"start\":50285},{\"end\":50519,\"start\":50510},{\"end\":50535,\"start\":50519},{\"end\":51018,\"start\":51004},{\"end\":51028,\"start\":51018},{\"end\":51045,\"start\":51028},{\"end\":51552,\"start\":51539},{\"end\":52002,\"start\":51978},{\"end\":52016,\"start\":52002},{\"end\":52030,\"start\":52016},{\"end\":52043,\"start\":52030},{\"end\":52052,\"start\":52043},{\"end\":52065,\"start\":52052},{\"end\":52079,\"start\":52065},{\"end\":52093,\"start\":52079},{\"end\":52385,\"start\":52369},{\"end\":52569,\"start\":52553}]", "bib_venue": "[{\"end\":40586,\"start\":40508},{\"end\":41522,\"start\":41421},{\"end\":42189,\"start\":42040},{\"end\":42853,\"start\":42742},{\"end\":43427,\"start\":43331},{\"end\":44084,\"start\":43985},{\"end\":44844,\"start\":44682},{\"end\":45775,\"start\":45758},{\"end\":47465,\"start\":47380},{\"end\":48594,\"start\":48511},{\"end\":49205,\"start\":49104},{\"end\":50682,\"start\":50610},{\"end\":51194,\"start\":51120},{\"end\":51687,\"start\":51620},{\"end\":35542,\"start\":35405},{\"end\":37069,\"start\":36969},{\"end\":39731,\"start\":39684},{\"end\":40506,\"start\":40430},{\"end\":41033,\"start\":40998},{\"end\":41419,\"start\":41317},{\"end\":42038,\"start\":41881},{\"end\":42740,\"start\":42629},{\"end\":43329,\"start\":43237},{\"end\":43983,\"start\":43884},{\"end\":44680,\"start\":44523},{\"end\":45363,\"start\":45328},{\"end\":45756,\"start\":45692},{\"end\":46133,\"start\":46030},{\"end\":46565,\"start\":46531},{\"end\":46933,\"start\":46908},{\"end\":47378,\"start\":47293},{\"end\":47821,\"start\":47749},{\"end\":48164,\"start\":48136},{\"end\":48509,\"start\":48425},{\"end\":49102,\"start\":49005},{\"end\":49638,\"start\":49521},{\"end\":49940,\"start\":49867},{\"end\":50272,\"start\":50192},{\"end\":50608,\"start\":50535},{\"end\":51118,\"start\":51045},{\"end\":51618,\"start\":51552},{\"end\":52111,\"start\":52093},{\"end\":52431,\"start\":52385},{\"end\":52551,\"start\":52527}]"}}}, "year": 2023, "month": 12, "day": 17}