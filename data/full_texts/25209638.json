{"id": 25209638, "updated": "2022-03-11 23:28:55.293", "metadata": {"title": "Hyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture co-design for energy-efficient, error-resilient language recognition", "authors": "[{\"first\":\"Haitong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Tony\",\"last\":\"Wu\",\"middle\":[\"F.\"]},{\"first\":\"Abbas\",\"last\":\"Rahimi\",\"middle\":[]},{\"first\":\"Kai-Shin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Miles\",\"last\":\"Rusch\",\"middle\":[]},{\"first\":\"Chang-Hsien\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Juo-Luen\",\"last\":\"Hsu\",\"middle\":[]},{\"first\":\"Mohamed\",\"last\":\"Sabry\",\"middle\":[\"M.\"]},{\"first\":\"S.\",\"last\":\"Eryilmaz\",\"middle\":[\"Burc\"]},{\"first\":\"Joon\",\"last\":\"Sohn\",\"middle\":[]},{\"first\":\"Wen-Cheng\",\"last\":\"Chiu\",\"middle\":[]},{\"first\":\"Min-Cheng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Tsung-Ta\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Jia-Min\",\"last\":\"Shieh\",\"middle\":[]},{\"first\":\"Wen-Kuan\",\"last\":\"Yeh\",\"middle\":[]},{\"first\":\"Jan\",\"last\":\"Rabaey\",\"middle\":[\"M.\"]},{\"first\":\"Subhasish\",\"last\":\"Mitra\",\"middle\":[]},{\"first\":\"H.-S.\",\"last\":\"Wong\",\"middle\":[\"Philip\"]}]", "venue": "2016 IEEE International Electron Devices Meeting (IEDM)", "journal": "2016 IEEE International Electron Devices Meeting (IEDM)", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "The ability to learn from few examples, known as one-shot learning, is a hallmark of human cognition. Hyperdimensional (HD) computing is a brain-inspired computational framework capable of one-shot learning, using random binary vectors with high dimensionality. Device-architecture co-design of HD cognitive computing systems using 3D VRRAM/CMOS is presented for language recognition. Multiplication-addition-permutation (MAP), the central operations of HD computing, are experimentally demonstrated on 4-layer 3D VRRAM/FinFET as non-volatile in-memory MAP kernels. Extensive cycle-to-cycle (up to 1012 cycles) and wafer-level device-to-device (256 RRAMs) experiments are performed to validate reproducibility and robustness. For 28-nm node, the 3D in-memory architecture reduces total energy consumption by 52.2% with 412 times less area compared with LP digital design (using registers as memory), owing to the energy-efficient VRRAM MAP kernels and dense connectivity. Meanwhile, the system trained with 21 samples texts achieves 90.4% accuracy recognizing 21 European languages on 21,000 test sentences. Hard-error analysis shows the HD architecture is amazingly resilient to RRAM endurance failures, making the use of various types of RRAMs/CBRAMs (1k \u223c 10M endurance) feasible.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2583857261", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1109/iedm.2016.7838428"}}, "content": {"source": {"pdf_hash": "116e1c68f5d47c1eab46589f3aed6f2579670133", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7559df061a2d9c34fb4537602878392a8ee85b4d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/116e1c68f5d47c1eab46589f3aed6f2579670133.txt", "contents": "\nHyperdimensional Computing with 3D VRRAM In-Memory Kernels: Device-Architecture Co-Design for Energy-Efficient, Error-Resilient Language Recognition\n\n\nHaitong Li *haitongl@stanford.edu \nStanford University\nUSA\n\nTony F Wu \nStanford University\nUSA\n\nAbbas Rahimi \nUniversity of California\nBerkeleyUSA\n\nKai-Shin Li \nNational Nano Device Laboratories\nTaiwan\n\nMiles Rusch \nUniversity of California\nBerkeleyUSA\n\nChang-Hsien Lin \nNational Nano Device Laboratories\nTaiwan\n\nJuo-Luen Hsu \nNational Nano Device Laboratories\nTaiwan\n\nMohamed M Sabry \nStanford University\nUSA\n\nS Burc Eryilmaz \nStanford University\nUSA\n\nJoon Sohn \nStanford University\nUSA\n\nWen-Cheng Chiu \nNational Nano Device Laboratories\nTaiwan\n\nMin-Cheng Chen \nTsung-Ta Wu \nNational Nano Device Laboratories\nTaiwan\n\nJia-Min Shieh \nNational Nano Device Laboratories\nTaiwan\n\nWen-Kuan Yeh \nNational Nano Device Laboratories\nTaiwan\n\nJan M Rabaey \nUniversity of California\nBerkeleyUSA\n\nNational Nano Device Laboratories\nTaiwan\n\nSubhasish Mitra \nStanford University\nUSA\n\nH.-S Philip Wong #hspwong@stanford.edu \nStanford University\nUSA\n\nHyperdimensional Computing with 3D VRRAM In-Memory Kernels: Device-Architecture Co-Design for Energy-Efficient, Error-Resilient Language Recognition\n\nThe ability to learn from few examples, known as one-shot learning, is a hallmark of human cognition. Hyperdimensional (HD) computing is a brain-inspired computational framework capable of one-shot learning, using random binary vectors with high dimensionality. Devicearchitecture co-design of HD cognitive computing systems using 3D VRRAM/CMOS is presented for language recognition. Multiplication-addition-permutation (MAP), the central operations of HD computing, are experimentally demonstrated on 4-layer 3D VRRAM/FinFET as non-volatile in-memory MAP kernels. Extensive cycle-to-cycle (up to 10 12 cycles) and wafer-level device-to-device (256 RRAMs) experiments are performed to validate reproducibility and robustness. For 28-nm node, the 3D in-memory architecture reduces total energy consumption by 52.2% with 412 times less area compared with LP digital design (using registers as memory), owing to the energy-efficient VRRAM MAP kernels and dense connectivity. Meanwhile, the system trained with 21 samples texts achieves 90.4% accuracy recognizing 21 European languages on 21,000 test sentences. Hard-error analysis shows the HD architecture is amazingly resilient to RRAM endurance failures, making the use of various types of RRAMs/CBRAMs (1k ~ 10M endurance) feasible.I. Introduction Brain-inspired computing aims at energy-efficient emulation of human cognition [1]. Such computing paradigm has advanced rapidly due to progress in emerging synaptic devices [2]-[7] and non-Von Neumann architectures [8]-[13], including hardware implementations of neural networks [1]-[8]. Human cognition features the ability to learn from few examples at a rapid pace, known as one-shot learning [14], whereas modern deep neural networks require large datasets and brute force training of billions of weights iteratively [15]. In contrast, hyperdimensional (HD) computing, a totally different braininspired computational framework, is capable of one-shot learning [16]. HD computing requires substantially less number of operations compared with modern deep learning algorithms. It is rooted in the observation that key aspects of human memory, perception and cognition can be explained by the subtle mathematical properties of high-dimensional spaces [16]. In this work, device-architecture co-design of HD language recognition systems using 3D VRRAM/CMOS is presented for the first time(Fig. 1). Extensive cycle-to-cycle (C2C) and device-to-device (D2D) measurements validate the robustness of 3D in-memory MAP kernels. The demonstration of energyefficient, error-resilient in-memory HD architecture paves the way towards efficient learning machines with hallmarks of human cognition.II. VRRAM In-Memory MAP KernelsIn the HD computing framework, information (letters, phonemes, DNA sequences, etc.) is represented and distributed in binary vectors with thousands of random '0s and '1's. These HD vectors are manipulated through multiplication-additionpermutation (MAP) kernels to not only classify, but also to bind, associate, and perform other types of cognitive operations in a one-shot manner(Fig. 1). Multiplication (MULT), addition (ADD), and permutation (PERM) are mapped onto 3D vertical RRAM (VRRAM) as in-memory MAP kernels ('1': low IEDM16-413 16.1.2\n\nresistance states (LRS); '0': high resistance states (HRS)) ( Fig.  2). XOR on {0, 1} binary code is equivalent to MULT on {1, -1} bipolar code. Based on the binary code, MULT is therefore performed by programming and evaluating XOR logic along vertical pillars in VRRAMs ( Fig. 2(a)). ADD and PERM are performed by current summing (Fig. 2(b)) and in-memory bit transfer (Fig. 2(c)), respectively. To experimentally demonstrate the MAP kernels, 4-layer TiN/Ti/HfOx/TiN 3D VRRAMs integrated with FinFET select transistors were fabricated. Detailed fabrication process was reported in [17]. Typical DC/endurance characteristics from bottom layer-1 (L1) to top layer-4 (L4) are shown in Fig. 3. Random '0's and '1's, the medium for HD computing, are naturally produced within VRRAM utilizing the intrinsic probabilistic switching behaviors ( Fig. 4) [18], [19]. D2D statistical distributions of SET probabilities (PSET) are also measured (Fig. 5), which are then incorporated into a variation-aware RRAM compact model on top of cycle-to-cycle variations [20]. PSET can be tuned by programming conditions. Shorter pulses result in tighter D2D spreads around certain PSET, owing to better reproducibility of filament morphology during C2C measurements (Fig. 5). 50% PSET (with +/-4% D2D variations) is used to produce random '0's and '1's for the following MAP operations.\n\nThe experimental implementations of MAP kernels are built upon 'in-memory computing' principle. Voltage division between RRAM cells and linear-region FinFET dynamically changes the pillar voltage (VP), which leads to SET/RESET/non-switching of upper-layer cells in 3D VRRAM. Thereby, XOR logic kernel can be programmed along the vertical pillar (Fig. 6). During programming, the truth tables of XOR and XNOR logic are memorized thanks to non-volatility. Hence, further logic evaluations are performed by selecting/reading the target pillar with inputs as decoding address. The read-dominant logic evaluations on XOR/XNOR are measured up to 10 12 cycles without output errors (Fig. 7). Logic evaluation voltage (VEVAL) is 0.1 V. The VRRAM inmemory logic implementation differs from conventional NVM lookup tables [13], in the sense that any other arbitrary functions can be also programmed online in VRRAM (Fig. 8). During each cycle marked with gray background, new functions are programmed online (same principle in Fig. 6). This feature supports variants of MAP kernels within the HD computing framework. Bit error rates (BER) of XOR logic kernel using elevated VEVAL are measured, under room temperature (RT) and 150\uf0b0C (Fig. 9). The BER data are obtained by performing intensive logic evaluations and monitoring the errors due to disturb on RRAM. Such behaviors are well captured by temperature-dependent compact model [20], under RT, 150\uf0b0C, and 260\uf0b0C (solder reflow temperature). Predicted BER under 0.1 V at 260\uf0b0C is below 10 -13 (0.0001 ppb), which shows the XOR kernel is extremely robust. Through current summing along vertical pillars, in-memory additions are measured on 4-L VRRAMs that store various 4bit vectors (Fig. 10). On each pillar, each in-memory addition is repeatedly measured for 10 11 cycles, and robust additions outputs are obtained without crosstalk or disturb errors among different layers of VRRAMs (Fig. 11). PERM operations are implemented within VRRAM by direct bit transfer among different RRAM cells without needing to first read the content IEDM16-412 16.1.1 978-1-5090-3902-9/16/$31.00 \u00a92016 IEEE followed by write-back. These cells form a pseudo-series connection, and thus resistance state of one cell can determine another's during pulse programming. Target cell is initialized (RESET) to '0'. After applying a pair of VDD/gnd pulses on the two RRAM cells, bit transfer is completed in situ (Fig. 12). The simple in-memory bit transfer does not require extra readout/write-back operations via memory controller/bus. Bit transfer between non-adjacent cells is also feasible for permutations in arbitrary orders (Fig. 13). Since read-dominant XOR and ADD operations are performed after PERM in algorithm, 10 11 read evaluations (0.1 V) are conducted after each cycle of permutations to emulate system-level behaviors (Fig. 14). Correct and robust permutations are maintained.\n\nFurthermore, wafer-level VRRAM in-memory MAP kernels are experimentally demonstrated and verified to support circuit design and implementation (Fig. 15). D2D measurements across 16 dies and 64 4-L VRRAM pillars (256 RRAM cells in total) are conducted. First, correct XOR outputs (stored in L4 cells) are obtained among the gray-code input combinations ( Fig. 15(a)). Second, measured current summing corresponds to vector-wise 4-bit additions, where the output current levels correctly match the desired results ( Fig. 15(b)). Last, permutations are performed by bit transfer from L1 (start) to L4 (destination) cells. In Fig. 15(c), the digits around arrows illustrate the pre-stored bits in L1/L4 cells. After permutations, the measured new data in L4 layer (color maps) correctly match the values stored in L1 layer, indicating the success of PERM.\n\nIII. In-Memory HD Computing Architecture Device-architecture-algorithm co-design is leveraged for inmemory HD computing systems recognizing 21 European languages. For training, 21 sample texts (100k~200k words/text) are taken from Wortschatz Corpora [21]. For inference, 21,000 unseen sentences (1,000 sentences/language) are taken from Europarl Parallel Corpus (independent sources) [22]. There are 3 levels of abstraction: algorithm, architecture design, and device operations (Fig. 16). In the algorithm level, 26 letters of the Latin alphabet plus ASCII space are represented by 27 1-kb HD vectors, and trigram (3 consecutive letters) encoding scheme is chosen. In the architecture level, 36-layer 3D VRRAM subarray is designed with vertical partition to carry out different stages of the algorithm pipeline. Individual HD vectors are loaded/stored in horizontal planes, and manipulated by MAP operations either vertically (vector-to-vector) or horizontally (vector-wise). At the device level, ~50% PSET is employed for random projection. In-memory MAP operations are essentially 'regular' R/W memory operations. Therefore, all the standard peripheral R/W analog/digital circuits are included. The language recognition system works as follows: an input text is sampled and projected into HD space by a sliding window of 3 consecutive letters. The letter HD vectors are first permuted (\uf072) in the Letter layers, and then multiplied (i.e., XORed in {0, 1} system) to compute trigram HD vectors that are temporarily stored in Trigram layers:\n\uf072 (\uf072 letter1) \uf0c5 \uf072 letter2 \uf0c5 letter3.\n(1) The generated trigram HD vectors are continually added (accumulated). A final text vector is generated/stored after comparing the sum with the threshold and writing '0's and '1's into the subarray. The 21 trained language vectors are stored in six LangMap layers (4 kb/layer), as visualized in Fig. 17. These language maps efficiently encode/capture the causal relations. During inference, 21,000 input sentences go through the same pipeline. Each generated test vector is then XORed with the 21 language vectors and summed in HamD layers, yielding Hamming distance (HamD) to identify and select the language (minimum HamD). It's observed that a test vector is very 'far' from the median of 20 unselected language vectors in HD space (Fig. 18), which is the underlying mechanism for robust recognition. Language recognition accuracy is 90.4% considering the D2D variability (50% PSET, +/-4%) of 3D VRRAM (Fig. 20). The choice of N-gram encoding scheme and HD vector dimension leads to different accuracy performance and circuit size requirement (Fig. 21).\n\nUsing 28-nm technology, the 3D in-memory architecture is compared with a low-power (LP) digital design. The LP digital design uses the RTL implementation reported in [23], which is also a non-Von Neumann architecture with distributed registers (no SRAMs) in encoding/search modules. Place and route and post simulations are conducted using the same 28-nm PDK. Running on the same sub-dataset with 1-kb HD vectors, 52.2% energy reduction is obtained by 3D in-memory architecture (Fig. 21). The benefits come from the energy-efficient MAP kernels and the 3D in-memory architecture eliminating long interconnect of the planar design. There was no intentional optimization of peripheral analog circuitry (SA/MUX/PG) for the HD design. Owing to the cost-effective 3D memory-centric structure, total area can be reduced by more than 400 times as compared with planar CMOS design (Fig. 22). When 10-kb HD vectors are chosen for HD computing, under 9-metal/chip area constraint (<10\uf0b4 the total area of all standard cells), the LP digital design fails in routing cleanly. As for the system robustness, for various levels of RRAM endurance considered, the in-memory HD architecture is amazingly error resilient in terms of RRAM endurance failures. This result was obtained by introducing hard stuck-at errors into entire architecture during simulations (Fig. 23). Using various RRAMs/CBRAMs having endurance from 1k ~ 10M (or more) cycles is feasible for HD computing. VRRAMs in this work have 1M cycles endurance (Fig. 3). HD computing also shows superior error resilience over conventional machine learning algorithm [23] (Fig. 24). Higher dimensionality is even more robust. These promising features are attributed to the high-dimensional and holographic representation: every piece of information in a HD vector is 'distributed' equally over all the components, making even the hard errors not \"contagious\".\n\nFinally, device-architecture co-optimization is performed. Sparsity is introduced into HD computing by initially tuning RRAM PSET while using the same MAP operations. Energy is reduced with more HRS cells involved during MAP operations. The penalty of accuracy drop can be mitigated by computing in higher dimensionality (Fig. 25). The 3D VRRAM design is further scaled up into a larger memory subsystem (Fig. 26). Two mats are activated at the same time for parallelism, and RRAM endurance can be relieved additionally by 16\uf0b4 since data/operations are distributed among multiple subarrays.\n\nIV. Conclusions Key achievements: (1) probabilistic switching of RRAM is utilized to efficiently generate random '0's and '1's for HD computing; (2) non-volatile VRRAM in-memory MAP kernels are experimentally demonstrated with verified reproducibility and robustness; (3) improved energy and area efficiencies are obtained from the novel 3D in-memory architecture over LP digital design with post-layout simulations;\n\n(4) RRAMs/CBRAMs having wide ranges of endurance (1k ~ 10M+) can be used in the error-resilient HD systems.            11 Measured consecutive inmemory addition outputs on 0000~1111 vectors up to 10 11 cycles without disturb/crosstalk errors. Summation across 8 bits is emulated by combining exp. data from two pillars. Applied read voltage is 0.1 V.\n\n\nFig.12\n\nMeasured resistance evolution (color-coded scale) of 4-L VRRAM during orderedpermutation of (a) bit '1' and (b) bit '0', in two sequences along the vertical pillars.                        \n1 1 \uf0ad 0 0 \uf0ad 1 0 \uf0ad 0 1 \uf0ad 1 1 \uf0ad 0 0 \uf0ad 1 0 \uf0ad 0 1 \uf0ad 0 1 \uf0ad 1 1 \uf0ad 0 0 \uf0ad 1 0 \uf0ad 0 1 \uf0ad 1 1 \uf0ad 0 0 \uf0ad\nFig. 1\n1Illustration of hyperdimensional (HD) computing framework and its association with this work.\n\nFig\nFig. 2 Schematic of mapping MAP kernels (a) multiplication, (b) addition, and (c) permutation onto 3D vertical RRAM. The inmemory MAP kernels perform central operations in HD cognitive computing. Hardware design is thus highly algorithm-driven.\n\nFig\nFig. 3 (a) I-V curves of 4-L 3D VRRAM. (b) Endurance data.\n\nFig. 4\n4Measured SET probabilities (PSET) as a function of SET pulse amplitude and pulse width. Each probability value is obtained from 200-cycle strong-RESET/weak-SET operations. Random '0'/'1' bits are produced via PSET ~50%.\n\nFig. 5\n5Measured and modeled device-to-device (D2D) statistical distributions of PSET around {25%, 50%, 75%}. Shorter pulses achieve tighter D2D distributions, as captured by both exp. and model. 25 devices are measured for each spread in the 50% PSET trials.\n\nFig. 6\n6(a) Schematic of executing Boolean logic on 4-layer 3D VRRAM/FinFET. Vo ltage division between RRAM and linear-region FinFET changes VP, triggering different desired switching events on upperlevel cells with logic outputs stored in situ. (b) Applied pulse train at L1-L4 BE to program XOR/XNOR (input AB = '10'). Digits show the states (LRS:1; HRS:0) of L1-L4 after each pulse cycle. (c) Measured initial/final states of L1-L4 for all combinations. Correct truth tables are measured.\n\nFig. 7\n7Measured logic evaluations on XOR/XNOR pillars up to 10 12 cycles. Four pillars store unique inputs/outputs. Each colored line shows the median of 10 evaluation cycles (gray) after reproducible XOR programming on each pillar.\n\nFig. 8\n8Measured 4-L VRRAM states during online programming. Each cycle either computes a new function (gray background) or evaluates the new logic. 8 different functions are measured correctly.\n\nFig. 9\n9Measured and modeled bit error rates (BER) of XOR logic kernel for different VEVAL and temperatures. Errors are due to disturb on RRAM during dataintensive logic read evaluations. Predicted BER for 0.1 V at 260\uf0b0C is below 10 -13 (0.0001 ppb).\n\nFig. 10\n10Measured in-memory addition on various 4-bit vectors (0000~1111) stored in 16 4-L VRRAMs, which are efficiently realized by current summing along vertical pillars. Bit summation along a certain vector calculates the Hamming distance relative to allzero vectors (measure 'difference').\n\nFig.\nFig.11 Measured consecutive inmemory addition outputs on 0000~1111 vectors up to 10 11 cycles without disturb/crosstalk errors. Summation across 8 bits is emulated by combining exp. data from two pillars. Applied read voltage is 0.1 V.\n\nFig. 13\n13Measured resistance evolution (color-coded scale) of 4-L VRRAM during arbitrarypermutation of (a) bit'1' and (b)  bit '0', for two arbitrary orders along the vertical pillars.\n\nFig. 14\n14Measured resistances (color-coded scale) of 4-L VRRAM during arbitrary permutation-andread cycles. 10 11 read cycles emulating logic evaluations are performed after each permutation.\n\n\n-and-Read Cycle (#) Resistance (\uf057)\n\nFig. 15\n15Wafer-level demonstration and verification of MAP kernels across 16 dies and 64 4-L VRRAM pillars (256 RRAM cells in total). (a) Measured L4 cells' resistances as the correct XOR outputs after programming and evaluating XOR logic. Digits are inputs stored in 1 st /2 nd layers. (b) Measured sum current of 4-bit vectors stored along VRRAM pillars. Digits are L1-L4 bit states. (c) Measured resistances of L4 cells after permutations (post-PERM R). The digits/arrows illustrate the bit transfer path from L1 to L4. Measured post-PERM data in L4 correctly represent the digits in L1 (arrow's left side).\n\nFig. 16\n16Algorithm pipeline and 3D in-memory devicearchitecture co-design for the language recognition system.\n\nFig. 18 (\n18a) Hamming distances (HamD) between 21,000 test sentences and 21 learned language maps. Min. distance leads to the identified language. (b) HamD distributions. Medians of HamDs are near 1/2 HD vector dimension.\n\nFig. 19\n19Confusion matrix of language recognition on 21,000 test sentences. Most samples can be predicted correctly, even though the 21 European languages are somewhat correlated. Recognition accuracy is 90.4% considering the D2D variability of 3D VRRAM.\n\nFig. 20\n20Recognition accuracy and required 3D VRRAM array size as a function of HD vector dimension and N-gram encoding scheme. Using 1-kb or 2-kb HD vectors with trigram scheme can achieve 90%+ accuracy with modest VRRAM circuit size requirement.\n\nFig. 21\n21Energy breakdown for lowpower digital design and 3D inmemory architecture for HD systems under 28 nm node. Energy is reduced by 52.2% with inmemory MAP kernels and unoptimized peripherals.\n\nFig. 22 (\n22a) Total area comparison between LP digital design and inmemory architecture. Digital design for 10-kb vectors is not routable under 9-metal/chip area constraints. (b) Area breakdown for VRRAM circuits. 36-L VRRAM cells have aspect ratio = 30 and trench slope = 89\uf0b0.\n\nFig. 23\n23Recognition accuracy as a function of RRAM endurance and vector size. The simulations assume the memory cells are stuck after endurance failures. Various types of RRAM/CBRAM with a wide range of endurance (1k~10M) can be used in error-resilient HD systems.\n\nFig. 24\n24Error resilience of HD computing and conventional nearest neighbor search (NNS) with hard stuck-at errors in text vectors/histograms during testing. Higher dimensionality in HD computing shows superior error resilience. Inset: linear-scale plot.\n\nFig. 25\n25Energy and accuracy as a function of sparsity (% of '0's) in 3D VRRAM array, initialized by tuning RRAM SET probabilities. Sparse representation is energy efficient, with the penalty of accuracy drop (less drop in higher dimensionality).\n\nFig. 26\n26Layout of one RRAM bank for the 28-nm in-memory HD computing system. The table summarizes the benefits/costs of scaling up VRRAM circuits into a larger memory-centric system for HD computing, compared with the single VRRAM subarray design.\nAcknowledgementThis work is supported in part by NSF ENIGMA Project, STARnet SONIC, Stanford NMTRI and Stanford SystemX Alliance. Device fabrication is performed by NDL facilities and supported by the Ministry of Science and Technology, Taiwan.\nC Mead, ; M Prezioso, Proc. IEEE. IEEE61C. Mead, Proc. IEEE, p.1629, 1990. [2] M. Prezioso et al., Nature, p.61, 2015. [3]\n\n. G W Burr, Nano Lett. 668ScienceG.W. Burr et al., IEDM, p.697, 2014. [4] D. Garbin et al., IEDM, p.661, 2014. [5] M. Suri et al., IEDM, p.235, 2012. [6] S. Park et al., IEDM, p.231, 2012. [7] S.H. Jo et al., Nano Lett., p.1297, 2010. [8] P. Merolla, et al., Science, p.668, 2014. [9] M.-F.\n\n. Chang , Nature Nanotech. 13Chang et al., ISSCC, p.318, 2015. [10] J.J. Yang et al., Nature Nanotech., p.13, 2013.\n\n. B Chen, Front. Neurosci. 64LREC. 23] A. Rahimi et al., ISLPEDB. Chen et al., IEDM, p.459, 2015. [12] T. Hasegawa et al., Adv. Mater., p.252, 2012. [13] H. Noguchi et al., IEDM, p.617, 2013. [14] S. Thorpe et al., Nature, p.520, 1996. [15] A. Krizhevsky et al., NIPS, 2012. [16] P. Kanerva, Cog. Comput., p.139, 2009. [17] H. Li et al., VLSI, p.194, 2016. [18] S. Yu et al., Front. Neurosci., p.186, 2013. [19] N. Raghavan et al., IEDM, p.554, 2013. [20] H. Li et al., DATE, p.1425, 2015. [21] U. Quasthoff, et al., LREC, 2006. [22] P. Koehn, MT Summit, 2005. [23] A. Rahimi et al., ISLPED, p.64, 2016.\n", "annotations": {"author": "[{\"end\":211,\"start\":152},{\"end\":247,\"start\":212},{\"end\":299,\"start\":248},{\"end\":354,\"start\":300},{\"end\":405,\"start\":355},{\"end\":464,\"start\":406},{\"end\":520,\"start\":465},{\"end\":562,\"start\":521},{\"end\":604,\"start\":563},{\"end\":640,\"start\":605},{\"end\":698,\"start\":641},{\"end\":714,\"start\":699},{\"end\":769,\"start\":715},{\"end\":826,\"start\":770},{\"end\":882,\"start\":827},{\"end\":976,\"start\":883},{\"end\":1018,\"start\":977},{\"end\":1083,\"start\":1019}]", "publisher": null, "author_last_name": "[{\"end\":162,\"start\":160},{\"end\":221,\"start\":219},{\"end\":260,\"start\":254},{\"end\":311,\"start\":309},{\"end\":366,\"start\":361},{\"end\":421,\"start\":418},{\"end\":477,\"start\":474},{\"end\":536,\"start\":531},{\"end\":578,\"start\":565},{\"end\":614,\"start\":610},{\"end\":655,\"start\":651},{\"end\":713,\"start\":709},{\"end\":726,\"start\":724},{\"end\":783,\"start\":778},{\"end\":839,\"start\":836},{\"end\":895,\"start\":889},{\"end\":992,\"start\":987},{\"end\":1035,\"start\":1031}]", "author_first_name": "[{\"end\":159,\"start\":152},{\"end\":216,\"start\":212},{\"end\":218,\"start\":217},{\"end\":253,\"start\":248},{\"end\":308,\"start\":300},{\"end\":360,\"start\":355},{\"end\":417,\"start\":406},{\"end\":473,\"start\":465},{\"end\":528,\"start\":521},{\"end\":530,\"start\":529},{\"end\":564,\"start\":563},{\"end\":609,\"start\":605},{\"end\":650,\"start\":641},{\"end\":708,\"start\":699},{\"end\":723,\"start\":715},{\"end\":777,\"start\":770},{\"end\":835,\"start\":827},{\"end\":886,\"start\":883},{\"end\":888,\"start\":887},{\"end\":986,\"start\":977},{\"end\":1023,\"start\":1019},{\"end\":1030,\"start\":1024}]", "author_affiliation": "[{\"end\":210,\"start\":187},{\"end\":246,\"start\":223},{\"end\":298,\"start\":262},{\"end\":353,\"start\":313},{\"end\":404,\"start\":368},{\"end\":463,\"start\":423},{\"end\":519,\"start\":479},{\"end\":561,\"start\":538},{\"end\":603,\"start\":580},{\"end\":639,\"start\":616},{\"end\":697,\"start\":657},{\"end\":768,\"start\":728},{\"end\":825,\"start\":785},{\"end\":881,\"start\":841},{\"end\":933,\"start\":897},{\"end\":975,\"start\":935},{\"end\":1017,\"start\":994},{\"end\":1082,\"start\":1059}]", "title": "[{\"end\":149,\"start\":1},{\"end\":1232,\"start\":1084}]", "venue": null, "abstract": "[{\"end\":4497,\"start\":1234}]", "bib_ref": "[{\"end\":5086,\"start\":5082},{\"end\":5356,\"start\":5352},{\"end\":5554,\"start\":5550},{\"end\":6684,\"start\":6680},{\"end\":7294,\"start\":7290},{\"end\":9888,\"start\":9884},{\"end\":10022,\"start\":10018},{\"end\":12444,\"start\":12440},{\"end\":15659,\"start\":15648}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15929,\"start\":15827},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16179,\"start\":15930},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16243,\"start\":16180},{\"attributes\":{\"id\":\"fig_3\"},\"end\":16472,\"start\":16244},{\"attributes\":{\"id\":\"fig_4\"},\"end\":16733,\"start\":16473},{\"attributes\":{\"id\":\"fig_5\"},\"end\":17226,\"start\":16734},{\"attributes\":{\"id\":\"fig_6\"},\"end\":17461,\"start\":17227},{\"attributes\":{\"id\":\"fig_7\"},\"end\":17657,\"start\":17462},{\"attributes\":{\"id\":\"fig_8\"},\"end\":17909,\"start\":17658},{\"attributes\":{\"id\":\"fig_9\"},\"end\":18205,\"start\":17910},{\"attributes\":{\"id\":\"fig_10\"},\"end\":18447,\"start\":18206},{\"attributes\":{\"id\":\"fig_11\"},\"end\":18634,\"start\":18448},{\"attributes\":{\"id\":\"fig_12\"},\"end\":18828,\"start\":18635},{\"attributes\":{\"id\":\"fig_15\"},\"end\":18865,\"start\":18829},{\"attributes\":{\"id\":\"fig_16\"},\"end\":19478,\"start\":18866},{\"attributes\":{\"id\":\"fig_17\"},\"end\":19591,\"start\":19479},{\"attributes\":{\"id\":\"fig_18\"},\"end\":19815,\"start\":19592},{\"attributes\":{\"id\":\"fig_19\"},\"end\":20072,\"start\":19816},{\"attributes\":{\"id\":\"fig_20\"},\"end\":20322,\"start\":20073},{\"attributes\":{\"id\":\"fig_21\"},\"end\":20522,\"start\":20323},{\"attributes\":{\"id\":\"fig_22\"},\"end\":20802,\"start\":20523},{\"attributes\":{\"id\":\"fig_23\"},\"end\":21070,\"start\":20803},{\"attributes\":{\"id\":\"fig_24\"},\"end\":21327,\"start\":21071},{\"attributes\":{\"id\":\"fig_25\"},\"end\":21576,\"start\":21328},{\"attributes\":{\"id\":\"fig_26\"},\"end\":21827,\"start\":21577}]", "paragraph": "[{\"end\":5866,\"start\":4499},{\"end\":8779,\"start\":5868},{\"end\":9632,\"start\":8781},{\"end\":11175,\"start\":9634},{\"end\":12272,\"start\":11213},{\"end\":14175,\"start\":12274},{\"end\":14767,\"start\":14177},{\"end\":15185,\"start\":14769},{\"end\":15537,\"start\":15187},{\"end\":15737,\"start\":15548}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11212,\"start\":11176},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15827,\"start\":15738}]", "table_ref": null, "section_header": "[{\"end\":15546,\"start\":15540},{\"end\":15834,\"start\":15828},{\"end\":15934,\"start\":15931},{\"end\":16184,\"start\":16181},{\"end\":16251,\"start\":16245},{\"end\":16480,\"start\":16474},{\"end\":16741,\"start\":16735},{\"end\":17234,\"start\":17228},{\"end\":17469,\"start\":17463},{\"end\":17665,\"start\":17659},{\"end\":17918,\"start\":17911},{\"end\":18211,\"start\":18207},{\"end\":18456,\"start\":18449},{\"end\":18643,\"start\":18636},{\"end\":18874,\"start\":18867},{\"end\":19487,\"start\":19480},{\"end\":19602,\"start\":19593},{\"end\":19824,\"start\":19817},{\"end\":20081,\"start\":20074},{\"end\":20331,\"start\":20324},{\"end\":20533,\"start\":20524},{\"end\":20811,\"start\":20804},{\"end\":21079,\"start\":21072},{\"end\":21336,\"start\":21329},{\"end\":21585,\"start\":21578}]", "table": null, "figure_caption": "[{\"end\":15929,\"start\":15836},{\"end\":16179,\"start\":15935},{\"end\":16243,\"start\":16185},{\"end\":16472,\"start\":16253},{\"end\":16733,\"start\":16482},{\"end\":17226,\"start\":16743},{\"end\":17461,\"start\":17236},{\"end\":17657,\"start\":17471},{\"end\":17909,\"start\":17667},{\"end\":18205,\"start\":17921},{\"end\":18447,\"start\":18212},{\"end\":18634,\"start\":18459},{\"end\":18828,\"start\":18646},{\"end\":18865,\"start\":18831},{\"end\":19478,\"start\":18877},{\"end\":19591,\"start\":19490},{\"end\":19815,\"start\":19605},{\"end\":20072,\"start\":19827},{\"end\":20322,\"start\":20084},{\"end\":20522,\"start\":20334},{\"end\":20802,\"start\":20536},{\"end\":21070,\"start\":20814},{\"end\":21327,\"start\":21082},{\"end\":21576,\"start\":21339},{\"end\":21827,\"start\":21588}]", "figure_ref": "[{\"end\":4569,\"start\":4561},{\"end\":4782,\"start\":4773},{\"end\":4841,\"start\":4831},{\"end\":4880,\"start\":4870},{\"end\":5189,\"start\":5183},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":5345,\"start\":5338},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":5442,\"start\":5434},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":5754,\"start\":5746},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":6220,\"start\":6213},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":6551,\"start\":6543},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":6781,\"start\":6773},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":6891,\"start\":6885},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":7098,\"start\":7090},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7601,\"start\":7592},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7803,\"start\":7795},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8306,\"start\":8297},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8524,\"start\":8516},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8730,\"start\":8721},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8932,\"start\":8924},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9145,\"start\":9135},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9305,\"start\":9295},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9410,\"start\":9403},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10122,\"start\":10113},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11518,\"start\":11511},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11960,\"start\":11951},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":12130,\"start\":12121},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12270,\"start\":12262},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12761,\"start\":12752},{\"attributes\":{\"ref_id\":\"fig_22\"},\"end\":13155,\"start\":13147},{\"attributes\":{\"ref_id\":\"fig_23\"},\"end\":13625,\"start\":13617},{\"end\":13785,\"start\":13778},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13896,\"start\":13888},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14506,\"start\":14498},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14589,\"start\":14581},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15308,\"start\":15306}]", "bib_author_first_name": "[{\"end\":22074,\"start\":22073},{\"end\":22084,\"start\":22081},{\"end\":22200,\"start\":22199},{\"end\":22202,\"start\":22201},{\"end\":22496,\"start\":22491},{\"end\":22609,\"start\":22608}]", "bib_author_last_name": "[{\"end\":22079,\"start\":22075},{\"end\":22093,\"start\":22085},{\"end\":22207,\"start\":22203},{\"end\":22614,\"start\":22610}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":22195,\"start\":22073},{\"attributes\":{\"id\":\"b1\"},\"end\":22487,\"start\":22197},{\"attributes\":{\"id\":\"b2\"},\"end\":22604,\"start\":22489},{\"attributes\":{\"id\":\"b3\"},\"end\":23209,\"start\":22606}]", "bib_title": null, "bib_author": "[{\"end\":22081,\"start\":22073},{\"end\":22095,\"start\":22081},{\"end\":22209,\"start\":22199},{\"end\":22499,\"start\":22491},{\"end\":22616,\"start\":22608}]", "bib_venue": "[{\"end\":22111,\"start\":22107},{\"end\":22105,\"start\":22095},{\"end\":22218,\"start\":22209},{\"end\":22514,\"start\":22499},{\"end\":22631,\"start\":22616}]"}}}, "year": 2023, "month": 12, "day": 17}