{"id": 49655339, "updated": "2023-11-11 02:45:48.81", "metadata": {"title": "Fully Convolutional Networks for Automated Segmentation of Abdominal Adipose Tissue Depots in Multicenter Water-Fat MRI", "authors": "[{\"first\":\"Taro\",\"last\":\"Langner\",\"middle\":[]},{\"first\":\"Anders\",\"last\":\"Hedstrom\",\"middle\":[]},{\"first\":\"Katharina\",\"last\":\"Morwald\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Weghuber\",\"middle\":[]},{\"first\":\"Anders\",\"last\":\"Forslund\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Bergsten\",\"middle\":[]},{\"first\":\"Haakan\",\"last\":\"Ahlstrom\",\"middle\":[]},{\"first\":\"Joel\",\"last\":\"Kullberg\",\"middle\":[]}]", "venue": "Magnetic resonance in medicine", "journal": "Magnetic resonance in medicine", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Purpose: An approach for the automated segmentation of visceral adipose tissue (VAT) and subcutaneous adipose tissue (SAT) in multicenter water-fat MRI scans of the abdomen was investigated, using two different neural network architectures. Methods: The two fully convolutional network architectures U-Net and V-Net were trained, evaluated and compared on the water-fat MRI data. Data of the study Tellus with 90 scans from a single center was used for a 10-fold cross-validation in which the most successful configuration for both networks was determined. These configurations were then tested on 20 scans of the multicenter study beta-cell function in JUvenile Diabetes and Obesity (BetaJudo), which involved a different study population and scanning device. Results: The U-Net outperformed the used implementation of the V-Net in both cross-validation and testing. In cross-validation, the U-Net reached average dice scores of 0.988 (VAT) and 0.992 (SAT). The average of the absolute quantification errors amount to 0.67% (VAT) and 0.39% (SAT). On the multi-center test data, the U-Net performs only slightly worse, with average dice scores of 0.970 (VAT) and 0.987 (SAT) and quantification errors of 2.80% (VAT) and 1.65% (SAT). Conclusion: The segmentations generated by the U-Net allow for reliable quantification and could therefore be viable for high-quality automated measurements of VAT and SAT in large-scale studies with minimal need for human intervention. The high performance on the multicenter test data furthermore shows the robustness of this approach for data of different patient demographics and imaging centers, as long as a consistent imaging protocol is used.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "3101019686", "acl": null, "pubmed": "30311704", "pubmedcentral": null, "dblp": "journals/corr/abs-1807-03122", "doi": "10.1002/mrm.27550"}}, "content": {"source": {"pdf_hash": "6fe149e588a5bf15bf89edfedb1a29cc31384ddc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1807.03122v5.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1807.03122", "status": "GREEN"}}, "grobid": {"id": "1aa86bea52dd7191197017e089e42ae9d0a87e5c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6fe149e588a5bf15bf89edfedb1a29cc31384ddc.txt", "contents": "\nFully Convolutional Networks for Automated Segmentation of Abdominal Adipose Tissue Depots in Multicenter Water-Fat MRI\n1 Nov 2018\n\nTaro Langner *taro.langner@surgsci.uu.se \nDept. of Radiology\nUppsala University\nUppsalaSweden\n\nAnders Hedstr\u00f6m \nAntaros Medical\nBioVenture HubM\u00f6lndalSweden\n\nKatharina Paulmichl \nDept. of Pediatrics\nParacelsus Medical University\n5020SalzburgAustria\n\nObesity Research Unit\nParacelsus Medical University\n5020SalzburgAustria\n\nDaniel Weghuber \nDept. of Pediatrics\nParacelsus Medical University\n5020SalzburgAustria\n\nObesity Research Unit\nParacelsus Medical University\n5020SalzburgAustria\n\nAnders Forslund \nDept. of Women's and Children's Health\nUppsala University\n751 05UppsalaSESweden\n\nPeter Bergsten \nDept. of Women's and Children's Health\nUppsala University\n751 05UppsalaSESweden\n\nDept. of Medical Cell Biology\nUppsala University\n751 23UppsalaSESweden\n\nH\u00e5kan Ahlstr\u00f6m \nDept. of Radiology\nUppsala University\nUppsalaSweden\n\nAntaros Medical\nBioVenture HubM\u00f6lndalSweden\n\nJoel Kullberg \nDept. of Radiology\nUppsala University\nUppsalaSweden\n\nAntaros Medical\nBioVenture HubM\u00f6lndalSweden\n\nFully Convolutional Networks for Automated Segmentation of Abdominal Adipose Tissue Depots in Multicenter Water-Fat MRI\n1 Nov 2018Published in Magnetic Resonance in Medicinedeep learningfully convolutional networkssegmentationwater-fat MRIadipose tissueabdominal 1\nPurpose: An approach for the automated segmentation of visceral adipose tissue (VAT) and subcutaneous adipose tissue (SAT) in multicenter water-fat MRI scans of the abdomen was investigated, using two different neural network architectures. Methods: The two fully convolutional network architectures U-Net and V-Net were trained, evaluated and compared on the water-fat MRI data. Data of the study Tellus with 90 scans from a single center was used for a 10-fold cross-validation in which the most successful configuration for both networks was determined. These configurations were then tested on 20 scans of the multicenter study beta-cell function in JUvenile Diabetes and Obesity (BetaJudo), which involved a different study population and scanning device. Results: The U-Net outperformed the used implementation of the V-Net in both cross-validation and testing. In cross-validation, the U-Net reached average dice scores of 0.988 (VAT) and 0.992 (SAT). The average of the absolute quantification errors amount to 0.67% (VAT) and 0.39% (SAT). On the multi-center test data, the U-Net performs only slightly worse, with average dice scores of 0.970 (VAT) and 0.987 (SAT) and quantification errors of 2.80% (VAT) and 1.65% (SAT). Conclusion: The segmentations generated by the U-Net allow for reliable quantification and could therefore be viable for high-quality automated measurements of VAT and SAT in large-scale studies with minimal need for human intervention. The high performance on the multicenter test data furthermore shows the robustness of this approach for data of different patient demographics and imaging centers, as long as a consistent imaging protocol is used.\n\nIntroduction\n\nThe quantification of human adipose tissue depots has the potential to provide new insights into the role of body composition as a factor for metabolic and cardiovascular disease. Abdominal obesity has been linked to conditions such as hypertension, inflammation and type 2 diabetes and is increasingly prevalent even among young adults and children [1]. Due to their different roles in the human metabolism, the total amount of abdominal adipose tissue is commonly separated into the two main components of visceral adipose tissue (VAT) and subcutaneous adipose tissue (SAT), with the former being more closely associated with health risks [2]. Other depots such as intramuscular adipose tissue and areas surrounding the spine are typically excluded. In practice the segmentation of VAT and SAT is usually performed with the help of automatic or semi-automatic methods which often require manual inspection and corrections by human experts. In larger studies with dozens or hundreds of scanned volumes this results in a high workload, so that an accurate, automated strategy that minimizes the need for human input and yields consistent results has the potential to reduce the cost and increase the feasibility of large scale studies.\n\nFor measurements of the quantity and distribution of adipose tissue in medical research, non-invasive imaging methods are commonly employed such as CT and MRI. When using chemical-shift encoded water-fat MRI, it is possible to obtain both co-registered water and fat signal images as well as voxel-wise fat fraction values [3] without exposing the patient to ionizing radiation. A variety of automated and semi-automated methods for VAT and SAT segmentation have been developed for images derived from both CT and MRI. In CT images, techniques have been proposed that use thresholding followed by ray tracing to isolate areas that are surrounded by lean tissue [4]. However, methods like this depend on the use of the Hounsfield scale, so that they can not be directly applied to MR images. In whole-body MRI of mice, a previously presented method used a combination of clustering and competitive region growing for the identification of narrow passages which delineate the depots of VAT and SAT [5]. To simplify the task of segmentation, a common strategy consists in imaging and segmenting only a single transverse slice located between the vertebrae L2-L5 as an indicator for overall VAT and SAT. However, it has been noted that the the accuracy of this approach is insufficient [6], [7], so that a volumetric assessment is expected to result in more reliable measurements. In water-fat MRI, further strategies have been proposed such as the transfer of segmentations between volumes [8], clustering techniques for masking and fitting of three-dimensional surfaces [6] or morphological operators that allow the identification of VAT and SAT.\n\nRather than relying on techniques such as clustering, thresholding and registration, the most successful methods for image-based semantic segmentation on current benchmark datasets in the computer vision community employ machine learning strategies such as convolutional neural networks [9], [10], [11], which have also seen success in medical applications [12]. For segmentation tasks, a network trained for classification can be applied in a sliding window technique to patches of an image to predict a label for each given central voxel. This approach has been previously used to generate adipose tissue segmentations in CT images [13]. However, the redundant feature extraction for adjacent patches by the sliding window technique is highly inefficient, so that specialized architectures for segmentation have emerged. Based on the concept of fully convolutional networks [14], derived architectures have been proposed such as the U-Net [15] for the segmentation of two-dimensional biomedical cell images and the V-Net [16] for segmentation of the human prostate in three-dimensional MR images. We therefore introduce a new approach using a fully convolutional network for the automated segmentation of VAT and SAT, with the goal of investigating how high of an accuracy and robustness can be achieved on this task. The network was applied to a representation of the water-fat MRI scans in which the water and fat signal as well as the calculated voxel-wise fat fractions are combined. Both the U-Net and the V-Net were adapted for this task and their performance was compared in both a ten-fold cross-validation as well as on a separate test dataset containing images from two different centers, each using a different MR system.\n\n\nMethods\n\nThe water-fat MRI data obtained from two separate studies was used to train and evaluate the performance of convolutional neural network architectures for semantic segmentation. The image data of the study Tellus was used for the training process, in which the learnable parameters of the network are adjusted, as well as the validation phase, in which a chosen network configuration together with its learned parameters is evaluated. The most successful network configurations were then tested on images of the study beta-cell function in JUvenile Diabetes and Obesity (Beta-Judo) [17].\n\n\nWater-fat MRI Data\n\nThe image data of the study Tellus was acquired from a cohort of adult male and female subjects, diagnosed with type 2 diabetes, aged 18-80 years, with a body mass index of up to 40kg/m 2 . The images were acquired with a 1.5T MR system Achieva dStream, Philips Healthcare, Best, The Netherlands at the University Hospital Uppsala, Sweden with anterior and posterior coils using the mDixon Quant sequence with TR 5.6ms, TE1 0.95ms, deltaTE 0.7ms, flip angle 5deg and fixed FOV. Among these patients, 45 were selected who participated in two visits, between 27 and 45 days apart, yielding 90 scan volumes with a typical image resolution of [256,256,20]  The image data of the study BetaJudo [17] was acquired in a collaboration between the Paracelsus Medical University Hospital in Salzburg, Austria and Uppsala University Hospital, Sweden. The study population consisted of 116 male and female individuals between the age of 10 and 18 years with complete records and includes both normal-weight and overweight subjects. A standardized imaging protocol was applied, with the Uppsala center using the same scanner and configuration as in the Tellus study, and the Salzburg center using a 1.5T Philips Ingenia system with TR 8.8ms, TE1 1.38ms, deltaTE 2.6ms, flip angle 5deg, again with fixed FOV. The typical image resolution for these scans is [256, 176, 21] voxels, again of size [2.07, 2.07, 8] mm. From this population 10 subjects were randomly selected for each center, resulting in a total of 20 scans that formed the test data set.\n\nThere were systematic differences between the scans of both studies. The images obtained from Tellus included the arms of the patients and were masked to contain a value of zero in the image background. In BetaJudo, the arms are not included and signal noise in the background leads to noisy fat fraction values. The volumes consist of 21 instead of 20 transverse slices and the scanned area is slightly shifted from (L4-L5) to (L3-L4), so that it includes less of the hip bone. Despite the lower age of the subjects, the labeled volumes for SAT in the chosen scans are on average about 75% larger, while the VAT volumes are about 50% smaller than in the images of Tellus. In order to obtain reference segmentations, the images of both studies were labeled with manual input. For the Tellus data, a first estimate at the correct labels was generated using the inside lean tissue filter (4) and exclusion of voxels with fat fraction values below 50%. The results were then manually corrected by an experienced operator by adjusting the delineation of VAT and SAT in the software SmartPaint (18) based on the water image and removing adipose tissue around the spinal column. On BetaJudo the reference segmentations were generated by another operator in a fully manual procedure using the software 3DSlicer on the fat-fraction image, likewise excluding voxels with fat fractions of less than 50% as well as adipose tissue around the spine.\n\n\nData formatting\n\nSeveral pre-processing steps were applied to the image data. In the Tellus dataset, the arms of the patients were removed from all scans. The contrast of the signal images of the water-fat MRI was then adjusted for each transverse slice by clipping the brightest one percent of their respective histograms and normalizing the remaining intensities. This strategy greatly decreases the variation in intensities seen both across different volumes as well as between the central and outer slices of a given scan where signal loss occurs. The fat fraction values were not normalized, so that the actual percentage is retained. The water and fat signal images as well as the fat fraction image were combined to form the three channels of a color image as seen in Figure Several pre-processing steps were applied to the image data. For BetaJudo, all image slices were zero-padded to a size of [256,256] and the noise in the background was masked out. In the Tellus dataset, the arms of the patients were removed from all scans to simplify the problem under the assumption that this step could be automatically performed in the future, possibly with a conventional algorithm. The contrast of the signal images of the water-fat MRI was then adjusted for each transverse slice by clipping the brightest one percent of their respective histograms and normalizing the remaining intensities to a float value range of [0, 1] for processing by the networks. This strategy greatly decreases the variation in intensities seen both across different volumes as well as between the central and outer slices of a given scan where signal loss occurs. The fat fraction values were not normalized, so that the actual percentage is retained. The water and fat signal images as well as the fat fraction image were combined to form the three channels of a color image as seen in Figure 1.\n\n\nAutomated Segmentation\n\nIn order to train and validate the different network architectures, the data of the Tellus study was split on the patient level for a 10-fold cross-validation. In this way, the available data was split into ten subsets, each of which was used to evaluate the performance of a network instance trained on the remaining nine sets. The presented metrics for the cross-validation are obtained by uniting the results on the individual sets. The highest-scoring network configurations were then trained once more on all available scans of Tellus in order to be tested on the data of the BetaJudo study. These splits of the image data were used for the training of convolutional neural networks for semantic segmentation. Based on the given reference segmentations, these architectures are able to successively apply convolutional filters for the extraction of hierarchical image features that allow for a segmentation to be automatically generated. The relevant features and their role in deciding on the shape of the segmentation are learned from the reference data by supervised learning. Both architectures follow an encoder-decoder structure in which the representations of the input are first downsampled and later upsampled in multiple steps, with the goal of extracting both fine as well as large-scale features. Long skip connections allow for both of these types of features to be combined in order to eventually assign a label to each voxel of the input image.\n\n\nU-Net\n\nAs a first network architecture, the U-Net [15] was trained on the two-dimensional transverse slices of the chosen volumes. Preliminary experiments showed that in the chosen configuration the dice scores of the segmentations did not improve when the number of pooling layers was increased or diminished. The architecture of the original paper with four downsampling steps was therefore retained and is shown in Figure 2. Due to their relatively small size, all slices can easily be processed as a whole without any need for tiling. Furthermore internal zero padding for the filters was used instead of mirrored padding of the input image, so that input and output of the network have the same dimensions. In this way the runtime and memory requirements for the network were roughly halved and no penalty on segmentation quality was observed. This strategy has been previously reported by as successful by multiple papers [18], [19]. \n\n\nV-Net\n\nThe implementation of the V-Net [16] that was used for the following experiments is based on a GitHub repository 1 with that includes modifications such as batch normalization and dropout. It is worth noting that the description of the V-Net architecture in the original paper also differs slightly from the actual implementation that was used by its authors to generate their reported results 2 . In order to process the highly anisotropic multi-channel data, the following additional architectural adjustments were necessary. The first adjustment affects the dimensionality reduction and is necessary due to the low number of slices. When applying a strided convolution for downsampling, the resolution of the volumetric feature maps in the V-Net is effectively halved in each step. When the third dimension of the input volumes has an extent of just 20, only the first two halving steps result in an even number of slices. Due to this restriction, all input volumes were padded to 24 slices by concatenating copies of the last slice. Additionally, the stride of the first and third strided convolution was set to a value Figure 3. V-Net architecture used for the experiments. A three-dimensional input volume with three channels is passed to the network, yielding voxel-wise scores for all three classes. In contrast to the original architecture, no short skip connection is used in the first convolutional block. The number of convolutional steps for the lower-resolution feature maps is increased less, especially in the decoding part of the network in order to save processing time and memory requirements. Rather than simply halving the resolution of the entire volume on each level, the strided convolution is adjusted to retain the number of slices after the first and third level despite compressing the other two dimensions. More detail is found in the uploaded PyTorch implementation.\n\nof one along the longitudinal axis, so they do not affect the number of slices. At the end of the downsampling path the volume is thereby represented by feature maps with 6 slices. A visualization of the resulting architecture is listed in the Appendix. When evaluating the dice score and other performance metrics on the segmentation results, the padding slices were excluded so that a direct comparison to the U-Net is possible. The second change is required due to the usage of multiple image channels. In the V-Net the result of the first convolution step is combined as an element-wise sum with the original input volume by a short skip connection. This skip connection was removed to avoid a conflict between the 3 input image channels and the 16 volumetric feature map channels. Furthermore, the number of convolution steps was reduced especially in the decoder part of the architecture in order to improve the network speed. The resulting architecture is shown in Figure  3.\n\n\nSettings and Evaluation\n\nAll reported results were achieved in the framework PyTorch using the Adam optimizer [20] and a learning rate of 0.0001 as well as a batch size of one. The learned weights were initialized with the PyTorch default settings, randomly sampled from a scaled, zero-centered Gaussian distribution1.\n\nNo benefit was observed when using class weights for the loss function, so that for the reported results all classes were weighted evenly. The U-Net was trained with a pixel-wise cross-entropy loss.  For the V-Net an improvement over the voxel-wise cross-entropy loss was found when using a dice loss with a term alpha = 0.1. to ensure numerical stability and avoid division by zero. For a set of voxels X labeled by the method to be evaluated and the set of voxels Y that are part of the reference segmentation, the loss function L to be minimized therefore takes the following form:\nL(X, Y ) = 1 \u2212 2 | X \u2229 Y | +alpha | X | + | Y | +alpha (1)\nThe U-Net was trained on a Nvidia GeForce GTX 1080Ti graphics card for 65000 iterations, requiring about one hour on each training split. The V-Net, in contrast, was trained for only 15000 iterations but required about ten hours per split. The resulting training curves are seen in Figure 4 and an implementation for the networks used in these experiments has been uploaded to GitHub 3 . The results were evaluated by dice score as well as the volume and percentage of the error of the segmentation. The effect of the pre-processing steps was analyzed and additionally, systematic errors and patterns in the network performance were visually evaluated. In order to judge in how far the retrieved dice scores might be affected by differences Visualizations of both network architectures and training curves are listed in the segmentation styles of the two operators, an additional inter-operator dice score was calculated. This score is based on the segmentation of 20 randomly chosen volumes as a representative sample from the Tellus dataset, created by the operator who segmented the images of BetaJudo. For both cross-validation and testing, the average performance metrics are seen in Table 1. The U-Net consistently outperforms the V-Net and generates segmentations that have an average error of less than one percent of the volume in cross-validation. Both networks achieve a slightly lower performance on the test data, but the U-Net still reaches an average error of less than 3% for the more difficult VAT volume. As an additional test, both networks were also evaluated on the test data without previous removal of the background noise. Although no such noise is present in the training data, the performance of the U-Net is virtually unaffected, with no change in the metrics at the significance given in Table 1. The V-Net is less robust and erratically oversegments random patterns in the background noise, so that no separate metrics are listed for this test. Relat ive Err r n SAT in % (d) Figure 5. Relative error in volume measurement (y-axis) as calculated in (predicted volume / reference volume -1) in relation to the reference volumes (x-axis) for the U-Net in cross-validation (a, b) and on the test set (c, d). The dotted lines denote two standard deviations away from the mean. Note how on the test set the data from Uppsala (dot markers) is on average closer to an error of zero than the data from Salzburg (x markers). Figure 5 shows the relationship between reference segmentation volumes and error percentage of the output of the U-Net in cross-validation and on the test data, respectively. A sample of resulting segmentations is shown in Figure 6. For the U-Net, the most commonly observed errors in cross-validation consist of oversegmentation of the hip bones as VAT, mistakes in ambiguous areas between VAT and SAT and leakage of VAT in rare cases where skin folds lead to multiple separate SAT compartments in more obese patients. On the test data, the majority of errors occur in ambiguous areas and occasional, insular oversegmentations. When assigning VAT labels there is also a tendency towards undersegmenting some of the outer areas of the depots. For the V-Net, the used implementation is outperformed by the U-Net in all cases with the exception of a marginally higher dice score for a single SAT volume.\n\n\nResults\n\nWhen examining those image volumes that were segmented by both operators, there is similar disagreement in manual segmentation around the spine and hip bones in VAT and intramuscular tissue in SAT. The average dice scores for the overlap between these segmentations are 0.969 (VAT) and 0.975 (SAT). It is important to note that the images used for this evaluation are not the same as the test set that was used to evaluate the network, so that the dice scores can not be directly compared.\n(a) (b) (c) (d) (e) (f) Input Legend Wat er Fat Fat Fract ion\nOut put Legend VAT SAT Figure 6. Segmentation results by the U-Net on the test dataset for the best (a, d), median (b, e) and worst (c, f) slice as measured by the sum of mislabeled voxels. The multi-channel input to the network is seen in (a-c), while (d-f) shows the respective output segmentations underneath, superimposed on the fat signal image.\n\n\nDiscussion\n\nOn the given water-fat MRI data, the fully convolutional network using the U-Net architecture was able to automatically generate highly accurate VAT and SAT segmentations and clearly outperforms the V-Net. When applied to the multicenter test data, the network was robust enough for the performance to only take a minor hit despite being faced with a different patient demographic and scanning device. In preliminary experiments, different augmentation strategies were evaluated such as varied translations, rotations, scales and volumetric deformations. Surprisingly, none of these strategies improved the results in the final configuration, so that all networks achieved their best performance with a comparably low number of training images. The networks furthermore benefited from the hand-crafted pre-processing strategy in which the contrast was adjusted, even though they could have learned to model this step internally. The fact that the networks did not learn to perform a comparably effective pre-processing step automatically shows that the optimization process during training is not guaranteed to use the full potential of the architecture. Human decisions on the formatting of the input are therefore still a relevant factor for the network performance.\n\nFor the U-Net, most resulting segmentations are of high quality and require no further corrections. The network is robust in regards to the background noise in the fat fraction values, even though no such noise was present in the training data. However, it was found that on average the performance on the data from Salzburg is slightly lower than on the data from the center in Uppsala, which is more similar to the training data. As a general rule the dice scores for more obese patients with larger volumes of VAT and SAT are higher than those for thin patients, a pattern that is also visible in Figure 5. This is probably due to the stronger effect of mislabeled single voxels in ambiguous regions in between and on the outline of the two depots. Preliminary experiments showed that modifications of the U-Net usually also achieved higher scores than the implementation of the V-Net, which indicates that the volumetric architecture may not be well suited for the given data. However, the underlying reasons for this effect can not easily be determined without further extensive experiments which are beyond the scope of this work. In our comparison the U-Net is more flexible, trains faster and generalizes better than the V-Net on the given scans.\n\nBoth networks perform worse on the test data than in cross-validation and have a tendency to undersegment both VAT and SAT. It is likely that such a decrease in performance is not only due to the different study population and the multicenter data in the test dataset. As observed in the representative sample of training images that were segmented by both operators, the average dice scores for the inter-operator variability are in the same range or slightly lower than the network performance on the test data. Similar values have been previously reported for the agreement between manual segmentations by different operators [8]. Even though the values presented here were obtained on different image sets, so that they do not allow for a direct comparison between the network and human performance, they indicate that the potential for further significant improvements is largely limited by the quality of the reference segmentations. Despite these observations, the lower performance on the test set as compared to the cross-validation indicates that the presented method should not be understood as a general solution to VAT and SAT segmentation in water-fat MRI. The approach can not be expected to retain this level of performance when applied to more strongly deviant images that, for example, strongly differ in position, image contrast or the usage of surface coils.\n\n\nConclusion\n\nIn conclusion, the experiments show that the proposed strategy is able to generate accurate and robust automated VAT and SAT segmentations in water-fat MRI scans with the help of the U-Net architecture. The segmentations allow for a volume quantification with an average of absolute errors below one percent in cross-validation and below three percent when applied to the multicenter test data. When examining the dice scores, the performance of the network on the test data is within range of the observed variability between different human operators both reported here and in the literature. In a direct comparison of network architectures, the implementation of the volumetric V-Net was clearly outperformed and less robust than the U-Net.\n\nThe results of the cross-validation suggest that, when given a comparable number of (N = 90) already segmented images as training data, this approach could be employed in the context of a large-scale study to automatically segment the remaining scans. The high degree of robustness of the approach in regards to differences in patient demographics as well as the usage of a different device (of the same vendor) is seen in the results on the multicenter test data. This result shows the potential of the strategy to successfully process data of studies without any existing segmentations. In practice, the network could accordingly be trained using existing reference data and be applied to future studies using the same imaging protocol in order to provide automated segmentations with a minimal need for manual intervention.\n\nFigure 1 .\n1By combining the fat and water signal as well as the voxel-wise fat fraction values obtained from water-fat MRI, a three-channel image can be formed as input for the networks.\n\nFigure 2 .\n2U-Net architecture used for the experiments. A two-dimensional input slice with three channels is passed to the network, yielding pixel-wise scores for all three classes. In contrast to the original architecture, zero-padding ensures that the size of the feature maps stays consistent on each level, so that no cropping is needed in the skip connections. More detail is found in the uploaded PyTorch implementation.\n\nFigure 4 .\n4Training curves for the U-Net and V-Net in both cross-validation and on the test data. The x-axis marks the given training iteration, with the y-position representing the average dice score on the validation or test data for VAT (dotted line) and SAT (continuous line). Based on the cross-validation, the iteration marked with a star symbol was chosen to calculate the listed results.\n\nTable 1 .\n1Average performance of the networks in cross-validation (CV) on the data of Tellus and on the test data of BetaJudo (Test). The listed error in % is the average of all absolute differences in measured and reference volumes divided by the reference volume.U-Net \nDepot Metric \nCV \nTest \nDice \n0.988 \u00b1 0.007 0.970 \u00b1 0.010 \nVAT \nError in % \n0.67 \u00b1 0.80 \n2.80 \u00b1 1.55 \nError in mL \n1 \u00b1 27 \n\u221241 \u00b1 32 \nDice \n0.992 \u00b1 0.003 0.987 \u00b1 0.004 \nSAT \nError in % \n0.39 \u00b1 0.35 \n1.65 \u00b1 0.80 \nError in mL \n2 \u00b1 26 \n\u2212112 \u00b1 45 \n\nV-Net \nDepot Metric \nCV \nTest \nDice \n0.982 \u00b1 0.009 0.916 \u00b1 0.059 \nVAT \nError in % \n1.15 \u00b1 1.06 \n8.86 \u00b1 10.15 \nError in mL \n\u221218 \u00b1 54 \n75 \u00b1 165 \nDice \n0.987 \u00b1 0.004 0.978 \u00b1 0.012 \nSAT \nError in % \n0.86 \u00b1 0.84 \n3.02 \u00b1 2.28 \nError in mL \n24 \u00b1 54 \n\u2212231 \u00b1 187 \n\n\nhttps://github.com/mattmacy/vnet.pytorch 2 https://github.com/faustomilletari/VNet/issues/9\nhttps://github.com/tarolangner/fcn_vatsat\nAcknowledgmentsThe research leading to these results has received funding from the European Union's Seventh Framework Program (FP7/2007-2013) under grant agreement number 279153.\nAbdominal obesity and metabolic syndrome. Jean- , Pierre Despr\u00e9s, Isabelle Lemieux, Nature. 4447121881Jean-Pierre Despr\u00e9s and Isabelle Lemieux. Abdominal obesity and metabolic syndrome. Nature, 444(7121):881, 2006.\n\nSegmentation and quantification of adipose tissue by magnetic resonance imaging. Magnetic Resonance Materials in Physics. Harry Houchun, Jun Hu, Wei Chen, Shen, Biology and Medicine. 292Houchun Harry Hu, Jun Chen, and Wei Shen. Segmentation and quantification of adipose tissue by magnetic resonance imaging. Magnetic Resonance Materials in Physics, Biology and Medicine, 29(2):259-276, 2016.\n\nQuantitative proton mr techniques for measuring fat. H H Hu, Hermien E Kan, NMR in biomedicine. 2612HH Hu and Hermien E Kan. Quantitative proton mr techniques for measuring fat. NMR in biomedicine, 26(12):1609-1629, 2013.\n\nAutomated analysis of liver fat, muscle and adipose tissue distribution from ct suitable for large-scale studies. Joel Kullberg, Anders Hedstr\u00f6m, John Brandberg, Robin Strand, Lars Johansson, G\u00f6ran Bergstr\u00f6m, H\u00e5kan Ahlstr\u00f6m, Scientific reports. 7110425Joel Kullberg, Anders Hedstr\u00f6m, John Brandberg, Robin Strand, Lars Johansson, G\u00f6ran Bergstr\u00f6m, and H\u00e5kan Ahlstr\u00f6m. Automated analysis of liver fat, muscle and adipose tissue distribution from ct suitable for large-scale studies. Scientific reports, 7(1):10425, 2017.\n\nAutomatic segmentation of intraabdominal and subcutaneous adipose tissue in 3d whole mouse mri. Petter Ranefall, Abdel Wahad Bidar, Paul D Hockings, Journal of Magnetic Resonance Imaging. 303Petter Ranefall, Abdel Wahad Bidar, and Paul D Hockings. Automatic segmentation of intra- abdominal and subcutaneous adipose tissue in 3d whole mouse mri. Journal of Magnetic Resonance Imaging, 30(3):554-560, 2009.\n\nValidation of volumetric and single-slice mri adipose analysis using a novel fully automated segmentation method. Shelby Bryan T Addeman, Kutty, G Thomas, Perkins, S Abraam, Curtis N Soliman, Colin M Wiens, Melanie D Mccurdy, Robert A Beaton, Charles A Hegele, Mckenzie, Journal of Magnetic Resonance Imaging. 411Bryan T Addeman, Shelby Kutty, Thomas G Perkins, Abraam S Soliman, Curtis N Wiens, Colin M McCurdy, Melanie D Beaton, Robert A Hegele, and Charles A McKenzie. Validation of volumetric and single-slice mri adipose analysis using a novel fully automated segmentation method. Journal of Magnetic Resonance Imaging, 41(1):233-241, 2015.\n\nAutomatic segmentation of abdominal organs and adipose tissue compartments in water-fat mri: application to weight-loss in obesity. Jun Shen, Thomas Baum, Christian Cordes, Beate Ott, Thomas Skurk, Hendrik Kooijman, J Ernst, Hans Rummeny, Hauner, H Bjoern, Dimitrios C Menze, Karampinos, European journal of radiology. 859Jun Shen, Thomas Baum, Christian Cordes, Beate Ott, Thomas Skurk, Hendrik Kooijman, Ernst J Rummeny, Hans Hauner, Bjoern H Menze, and Dimitrios C Karampinos. Auto- matic segmentation of abdominal organs and adipose tissue compartments in water-fat mri: application to weight-loss in obesity. European journal of radiology, 85(9):1613-1621, 2016.\n\nAutomatic intra-subject registration-based segmentation of abdominal fat from water-fat mri. A Anand, Joshi, H Houchun, Richard M Hu, Leahy, Krishna S Michael I Goran, Nayak, Journal of Magnetic Resonance Imaging. 372Anand A Joshi, Houchun H Hu, Richard M Leahy, Michael I Goran, and Krishna S Nayak. Automatic intra-subject registration-based segmentation of abdominal fat from water-fat mri. Journal of Magnetic Resonance Imaging, 37(2):423-430, 2013.\n\nThe pascal visual object classes challenge: A retrospective. Mark Everingham, Ali Eslami, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International journal of computer vision. 1111Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International journal of computer vision, 111(1):98-136, 2015.\n\nA review on deep learning techniques applied to semantic segmentation. Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, Jose Garcia-Rodriguez, arXiv:1704.06857arXiv preprintAlberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, and Jose Garcia-Rodriguez. A review on deep learning techniques applied to semantic segmenta- tion. arXiv preprint arXiv:1704.06857, 2017.\n\nDeep learning. nature. Yann Lecun, Yoshua Bengio, Geoffrey Hinton, 521436Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.\n\nDeep learning in medical image analysis. Dinggang Shen, Guorong Wu, Heung-Il Suk, Annual review of biomedical engineering. 19Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annual review of biomedical engineering, 19:221-248, 2017.\n\nA twostep convolutional neural network based computer-aided detection scheme for automatically segmenting adipose tissue volume depicting on ct images. Computer methods and programs in biomedicine. Yunzhi Wang, Yuchen Qiu, Theresa Thai, Kathleen Moore, Hong Liu, Bin Zheng, 144Yunzhi Wang, Yuchen Qiu, Theresa Thai, Kathleen Moore, Hong Liu, and Bin Zheng. A two- step convolutional neural network based computer-aided detection scheme for automatically segmenting adipose tissue volume depicting on ct images. Computer methods and programs in biomedicine, 144:97-104, 2017.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se- mantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431-3440, 2015.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer, 2015.\n\nV-net: Fully convolutional neural networks for volumetric medical image segmentation. Fausto Milletari, Nassir Navab, Seyed-Ahmad Ahmadi, 3D Vision (3DV), 2016 Fourth International Conference on. IEEEFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3D Vision (3DV), 2016 Fourth International Conference on, pages 565-571. IEEE, 2016.\n\nPancreatic fat is associated with metabolic syndrome and visceral fat but not beta-cell function or body mass index in pediatric obesity. Johan Staaf, Viktor Labmayr, Katharina Paulmichl, Hannes Manell, Jing Cen, Iris Ciba, Marie Dahlbom, Kirsten Roomp, Christian-Heinz Anderwald, Matthias Meissnitzer, Pancreas. 463358Johan Staaf, Viktor Labmayr, Katharina Paulmichl, Hannes Manell, Jing Cen, Iris Ciba, Marie Dahlbom, Kirsten Roomp, Christian-Heinz Anderwald, Matthias Meissnitzer, et al. Pancreatic fat is associated with metabolic syndrome and visceral fat but not beta-cell func- tion or body mass index in pediatric obesity. Pancreas, 46(3):358, 2017.\n\nAutomatic brain tumor detection and segmentation using u-net based fully convolutional networks. Hao Dong, Guang Yang, Fangde Liu, Yuanhan Mo, Yike Guo, Annual Conference on Medical Image Understanding and Analysis. SpringerHao Dong, Guang Yang, Fangde Liu, Yuanhan Mo, and Yike Guo. Automatic brain tu- mor detection and segmentation using u-net based fully convolutional networks. In Annual Conference on Medical Image Understanding and Analysis, pages 506-517. Springer, 2017.\n\nCnn-based segmentation of medical imaging data. Baris Kayalibay, Grady Jensen, Patrick Van Der, Smagt, arXiv:1701.03056arXiv preprintBaris Kayalibay, Grady Jensen, and Patrick van der Smagt. Cnn-based segmentation of medical imaging data. arXiv preprint arXiv:1701.03056, 2017.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n", "annotations": {"author": "[{\"end\":227,\"start\":133},{\"end\":289,\"start\":228},{\"end\":454,\"start\":290},{\"end\":615,\"start\":455},{\"end\":713,\"start\":616},{\"end\":882,\"start\":714},{\"end\":996,\"start\":883},{\"end\":1109,\"start\":997}]", "publisher": null, "author_last_name": "[{\"end\":145,\"start\":138},{\"end\":243,\"start\":235},{\"end\":309,\"start\":300},{\"end\":470,\"start\":462},{\"end\":631,\"start\":623},{\"end\":728,\"start\":720},{\"end\":897,\"start\":889},{\"end\":1010,\"start\":1002}]", "author_first_name": "[{\"end\":137,\"start\":133},{\"end\":234,\"start\":228},{\"end\":299,\"start\":290},{\"end\":461,\"start\":455},{\"end\":622,\"start\":616},{\"end\":719,\"start\":714},{\"end\":888,\"start\":883},{\"end\":1001,\"start\":997}]", "author_affiliation": "[{\"end\":226,\"start\":175},{\"end\":288,\"start\":245},{\"end\":380,\"start\":311},{\"end\":453,\"start\":382},{\"end\":541,\"start\":472},{\"end\":614,\"start\":543},{\"end\":712,\"start\":633},{\"end\":809,\"start\":730},{\"end\":881,\"start\":811},{\"end\":950,\"start\":899},{\"end\":995,\"start\":952},{\"end\":1063,\"start\":1012},{\"end\":1108,\"start\":1065}]", "title": "[{\"end\":120,\"start\":1},{\"end\":1229,\"start\":1110}]", "venue": null, "abstract": "[{\"end\":3058,\"start\":1375}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3427,\"start\":3424},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3718,\"start\":3715},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4637,\"start\":4634},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4975,\"start\":4972},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5310,\"start\":5307},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5596,\"start\":5593},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5601,\"start\":5598},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5801,\"start\":5798},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5882,\"start\":5879},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6247,\"start\":6244},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6253,\"start\":6249},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6259,\"start\":6255},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6318,\"start\":6314},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6595,\"start\":6591},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6837,\"start\":6833},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6902,\"start\":6898},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6984,\"start\":6980},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8289,\"start\":8285},{\"end\":8957,\"start\":8952},{\"end\":8961,\"start\":8957},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8964,\"start\":8961},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9007,\"start\":9003},{\"end\":12199,\"start\":12194},{\"end\":12203,\"start\":12199},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14717,\"start\":14713},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15595,\"start\":15591},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15601,\"start\":15597},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15649,\"start\":15645},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18610,\"start\":18606},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26893,\"start\":26890}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29414,\"start\":29226},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29843,\"start\":29415},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30241,\"start\":29844},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31016,\"start\":30242}]", "paragraph": "[{\"end\":4309,\"start\":3074},{\"end\":5955,\"start\":4311},{\"end\":7691,\"start\":5957},{\"end\":8290,\"start\":7703},{\"end\":9849,\"start\":8313},{\"end\":11287,\"start\":9851},{\"end\":13169,\"start\":11307},{\"end\":14660,\"start\":13196},{\"end\":15603,\"start\":14670},{\"end\":17509,\"start\":15613},{\"end\":18493,\"start\":17511},{\"end\":18814,\"start\":18521},{\"end\":19400,\"start\":18816},{\"end\":22806,\"start\":19460},{\"end\":23307,\"start\":22818},{\"end\":23720,\"start\":23370},{\"end\":25003,\"start\":23735},{\"end\":26259,\"start\":25005},{\"end\":27639,\"start\":26261},{\"end\":28397,\"start\":27654},{\"end\":29225,\"start\":28399}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19459,\"start\":19401},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23369,\"start\":23308}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20656,\"start\":20649},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21283,\"start\":21276}]", "section_header": "[{\"end\":3072,\"start\":3060},{\"end\":7701,\"start\":7694},{\"end\":8311,\"start\":8293},{\"end\":11305,\"start\":11290},{\"end\":13194,\"start\":13172},{\"end\":14668,\"start\":14663},{\"end\":15611,\"start\":15606},{\"end\":18519,\"start\":18496},{\"end\":22816,\"start\":22809},{\"end\":23733,\"start\":23723},{\"end\":27652,\"start\":27642},{\"end\":29237,\"start\":29227},{\"end\":29426,\"start\":29416},{\"end\":29855,\"start\":29845},{\"end\":30252,\"start\":30243}]", "table": "[{\"end\":31016,\"start\":30509}]", "figure_caption": "[{\"end\":29414,\"start\":29239},{\"end\":29843,\"start\":29428},{\"end\":30241,\"start\":29857},{\"end\":30509,\"start\":30254}]", "figure_ref": "[{\"end\":12079,\"start\":12065},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13168,\"start\":13160},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15089,\"start\":15081},{\"end\":16745,\"start\":16737},{\"end\":18492,\"start\":18483},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19750,\"start\":19742},{\"end\":21473,\"start\":21465},{\"end\":21913,\"start\":21905},{\"end\":22136,\"start\":22128},{\"end\":23401,\"start\":23393},{\"end\":25613,\"start\":25605}]", "bib_author_first_name": "[{\"end\":31377,\"start\":31372},{\"end\":31386,\"start\":31380},{\"end\":31404,\"start\":31396},{\"end\":31673,\"start\":31668},{\"end\":31686,\"start\":31683},{\"end\":31694,\"start\":31691},{\"end\":31994,\"start\":31993},{\"end\":31996,\"start\":31995},{\"end\":32281,\"start\":32277},{\"end\":32298,\"start\":32292},{\"end\":32313,\"start\":32309},{\"end\":32330,\"start\":32325},{\"end\":32343,\"start\":32339},{\"end\":32360,\"start\":32355},{\"end\":32377,\"start\":32372},{\"end\":32785,\"start\":32779},{\"end\":32801,\"start\":32796},{\"end\":32807,\"start\":32802},{\"end\":32819,\"start\":32815},{\"end\":32821,\"start\":32820},{\"end\":33210,\"start\":33204},{\"end\":33236,\"start\":33235},{\"end\":33255,\"start\":33254},{\"end\":33270,\"start\":33264},{\"end\":33272,\"start\":33271},{\"end\":33287,\"start\":33282},{\"end\":33289,\"start\":33288},{\"end\":33304,\"start\":33297},{\"end\":33306,\"start\":33305},{\"end\":33322,\"start\":33316},{\"end\":33324,\"start\":33323},{\"end\":33342,\"start\":33333},{\"end\":33872,\"start\":33869},{\"end\":33885,\"start\":33879},{\"end\":33901,\"start\":33892},{\"end\":33915,\"start\":33910},{\"end\":33927,\"start\":33921},{\"end\":33942,\"start\":33935},{\"end\":33954,\"start\":33953},{\"end\":33966,\"start\":33962},{\"end\":33985,\"start\":33984},{\"end\":34003,\"start\":33994},{\"end\":34005,\"start\":34004},{\"end\":34500,\"start\":34499},{\"end\":34516,\"start\":34515},{\"end\":34533,\"start\":34526},{\"end\":34535,\"start\":34534},{\"end\":34554,\"start\":34547},{\"end\":34556,\"start\":34555},{\"end\":34926,\"start\":34922},{\"end\":34942,\"start\":34939},{\"end\":34954,\"start\":34951},{\"end\":34966,\"start\":34965},{\"end\":34968,\"start\":34967},{\"end\":34986,\"start\":34982},{\"end\":35003,\"start\":34997},{\"end\":35374,\"start\":35367},{\"end\":35396,\"start\":35390},{\"end\":35418,\"start\":35412},{\"end\":35432,\"start\":35426},{\"end\":35455,\"start\":35451},{\"end\":35755,\"start\":35751},{\"end\":35769,\"start\":35763},{\"end\":35786,\"start\":35778},{\"end\":35943,\"start\":35935},{\"end\":35957,\"start\":35950},{\"end\":35970,\"start\":35962},{\"end\":36369,\"start\":36363},{\"end\":36382,\"start\":36376},{\"end\":36395,\"start\":36388},{\"end\":36410,\"start\":36402},{\"end\":36422,\"start\":36418},{\"end\":36431,\"start\":36428},{\"end\":36805,\"start\":36797},{\"end\":36816,\"start\":36812},{\"end\":36834,\"start\":36828},{\"end\":37269,\"start\":37265},{\"end\":37290,\"start\":37283},{\"end\":37306,\"start\":37300},{\"end\":37741,\"start\":37735},{\"end\":37759,\"start\":37753},{\"end\":37778,\"start\":37767},{\"end\":38223,\"start\":38218},{\"end\":38237,\"start\":38231},{\"end\":38256,\"start\":38247},{\"end\":38274,\"start\":38268},{\"end\":38287,\"start\":38283},{\"end\":38297,\"start\":38293},{\"end\":38309,\"start\":38304},{\"end\":38326,\"start\":38319},{\"end\":38349,\"start\":38334},{\"end\":38369,\"start\":38361},{\"end\":38839,\"start\":38836},{\"end\":38851,\"start\":38846},{\"end\":38864,\"start\":38858},{\"end\":38877,\"start\":38870},{\"end\":38886,\"start\":38882},{\"end\":39273,\"start\":39268},{\"end\":39290,\"start\":39285},{\"end\":39306,\"start\":39299},{\"end\":39544,\"start\":39543},{\"end\":39560,\"start\":39555}]", "bib_author_last_name": "[{\"end\":31394,\"start\":31387},{\"end\":31412,\"start\":31405},{\"end\":31681,\"start\":31674},{\"end\":31689,\"start\":31687},{\"end\":31699,\"start\":31695},{\"end\":31705,\"start\":31701},{\"end\":31999,\"start\":31997},{\"end\":32014,\"start\":32001},{\"end\":32290,\"start\":32282},{\"end\":32307,\"start\":32299},{\"end\":32323,\"start\":32314},{\"end\":32337,\"start\":32331},{\"end\":32353,\"start\":32344},{\"end\":32370,\"start\":32361},{\"end\":32386,\"start\":32378},{\"end\":32794,\"start\":32786},{\"end\":32813,\"start\":32808},{\"end\":32830,\"start\":32822},{\"end\":33226,\"start\":33211},{\"end\":33233,\"start\":33228},{\"end\":33243,\"start\":33237},{\"end\":33252,\"start\":33245},{\"end\":33262,\"start\":33256},{\"end\":33280,\"start\":33273},{\"end\":33295,\"start\":33290},{\"end\":33314,\"start\":33307},{\"end\":33331,\"start\":33325},{\"end\":33349,\"start\":33343},{\"end\":33359,\"start\":33351},{\"end\":33877,\"start\":33873},{\"end\":33890,\"start\":33886},{\"end\":33908,\"start\":33902},{\"end\":33919,\"start\":33916},{\"end\":33933,\"start\":33928},{\"end\":33951,\"start\":33943},{\"end\":33960,\"start\":33955},{\"end\":33974,\"start\":33967},{\"end\":33982,\"start\":33976},{\"end\":33992,\"start\":33986},{\"end\":34011,\"start\":34006},{\"end\":34023,\"start\":34013},{\"end\":34506,\"start\":34501},{\"end\":34513,\"start\":34508},{\"end\":34524,\"start\":34517},{\"end\":34538,\"start\":34536},{\"end\":34545,\"start\":34540},{\"end\":34572,\"start\":34557},{\"end\":34579,\"start\":34574},{\"end\":34937,\"start\":34927},{\"end\":34949,\"start\":34943},{\"end\":34963,\"start\":34955},{\"end\":34980,\"start\":34969},{\"end\":34995,\"start\":34987},{\"end\":35008,\"start\":35004},{\"end\":35019,\"start\":35010},{\"end\":35388,\"start\":35375},{\"end\":35410,\"start\":35397},{\"end\":35424,\"start\":35419},{\"end\":35449,\"start\":35433},{\"end\":35472,\"start\":35456},{\"end\":35761,\"start\":35756},{\"end\":35776,\"start\":35770},{\"end\":35793,\"start\":35787},{\"end\":35948,\"start\":35944},{\"end\":35960,\"start\":35958},{\"end\":35974,\"start\":35971},{\"end\":36374,\"start\":36370},{\"end\":36386,\"start\":36383},{\"end\":36400,\"start\":36396},{\"end\":36416,\"start\":36411},{\"end\":36426,\"start\":36423},{\"end\":36437,\"start\":36432},{\"end\":36810,\"start\":36806},{\"end\":36826,\"start\":36817},{\"end\":36842,\"start\":36835},{\"end\":37281,\"start\":37270},{\"end\":37298,\"start\":37291},{\"end\":37311,\"start\":37307},{\"end\":37751,\"start\":37742},{\"end\":37765,\"start\":37760},{\"end\":37785,\"start\":37779},{\"end\":38229,\"start\":38224},{\"end\":38245,\"start\":38238},{\"end\":38266,\"start\":38257},{\"end\":38281,\"start\":38275},{\"end\":38291,\"start\":38288},{\"end\":38302,\"start\":38298},{\"end\":38317,\"start\":38310},{\"end\":38332,\"start\":38327},{\"end\":38359,\"start\":38350},{\"end\":38381,\"start\":38370},{\"end\":38844,\"start\":38840},{\"end\":38856,\"start\":38852},{\"end\":38868,\"start\":38865},{\"end\":38880,\"start\":38878},{\"end\":38890,\"start\":38887},{\"end\":39283,\"start\":39274},{\"end\":39297,\"start\":39291},{\"end\":39314,\"start\":39307},{\"end\":39321,\"start\":39316},{\"end\":39553,\"start\":39545},{\"end\":39567,\"start\":39561},{\"end\":39571,\"start\":39569}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11944065},\"end\":31544,\"start\":31330},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":31500075},\"end\":31938,\"start\":31546},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7601477},\"end\":32161,\"start\":31940},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5251788},\"end\":32681,\"start\":32163},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":20949626},\"end\":33088,\"start\":32683},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":22879092},\"end\":33735,\"start\":33090},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":36744511},\"end\":34404,\"start\":33737},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":12346943},\"end\":34859,\"start\":34406},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":207252270},\"end\":35294,\"start\":34861},{\"attributes\":{\"doi\":\"arXiv:1704.06857\",\"id\":\"b9\"},\"end\":35726,\"start\":35296},{\"attributes\":{\"id\":\"b10\"},\"end\":35892,\"start\":35728},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":7961631},\"end\":36163,\"start\":35894},{\"attributes\":{\"id\":\"b12\"},\"end\":36739,\"start\":36165},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1629541},\"end\":37198,\"start\":36741},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3719281},\"end\":37647,\"start\":37200},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206429151},\"end\":38078,\"start\":37649},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4777984},\"end\":38737,\"start\":38080},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":30500715},\"end\":39218,\"start\":38739},{\"attributes\":{\"doi\":\"arXiv:1701.03056\",\"id\":\"b18\"},\"end\":39497,\"start\":39220},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b19\"},\"end\":39715,\"start\":39499}]", "bib_title": "[{\"end\":31370,\"start\":31330},{\"end\":31666,\"start\":31546},{\"end\":31991,\"start\":31940},{\"end\":32275,\"start\":32163},{\"end\":32777,\"start\":32683},{\"end\":33202,\"start\":33090},{\"end\":33867,\"start\":33737},{\"end\":34497,\"start\":34406},{\"end\":34920,\"start\":34861},{\"end\":35933,\"start\":35894},{\"end\":36795,\"start\":36741},{\"end\":37263,\"start\":37200},{\"end\":37733,\"start\":37649},{\"end\":38216,\"start\":38080},{\"end\":38834,\"start\":38739}]", "bib_author": "[{\"end\":31380,\"start\":31372},{\"end\":31396,\"start\":31380},{\"end\":31414,\"start\":31396},{\"end\":31683,\"start\":31668},{\"end\":31691,\"start\":31683},{\"end\":31701,\"start\":31691},{\"end\":31707,\"start\":31701},{\"end\":32001,\"start\":31993},{\"end\":32016,\"start\":32001},{\"end\":32292,\"start\":32277},{\"end\":32309,\"start\":32292},{\"end\":32325,\"start\":32309},{\"end\":32339,\"start\":32325},{\"end\":32355,\"start\":32339},{\"end\":32372,\"start\":32355},{\"end\":32388,\"start\":32372},{\"end\":32796,\"start\":32779},{\"end\":32815,\"start\":32796},{\"end\":32832,\"start\":32815},{\"end\":33228,\"start\":33204},{\"end\":33235,\"start\":33228},{\"end\":33245,\"start\":33235},{\"end\":33254,\"start\":33245},{\"end\":33264,\"start\":33254},{\"end\":33282,\"start\":33264},{\"end\":33297,\"start\":33282},{\"end\":33316,\"start\":33297},{\"end\":33333,\"start\":33316},{\"end\":33351,\"start\":33333},{\"end\":33361,\"start\":33351},{\"end\":33879,\"start\":33869},{\"end\":33892,\"start\":33879},{\"end\":33910,\"start\":33892},{\"end\":33921,\"start\":33910},{\"end\":33935,\"start\":33921},{\"end\":33953,\"start\":33935},{\"end\":33962,\"start\":33953},{\"end\":33976,\"start\":33962},{\"end\":33984,\"start\":33976},{\"end\":33994,\"start\":33984},{\"end\":34013,\"start\":33994},{\"end\":34025,\"start\":34013},{\"end\":34508,\"start\":34499},{\"end\":34515,\"start\":34508},{\"end\":34526,\"start\":34515},{\"end\":34540,\"start\":34526},{\"end\":34547,\"start\":34540},{\"end\":34574,\"start\":34547},{\"end\":34581,\"start\":34574},{\"end\":34939,\"start\":34922},{\"end\":34951,\"start\":34939},{\"end\":34965,\"start\":34951},{\"end\":34982,\"start\":34965},{\"end\":34997,\"start\":34982},{\"end\":35010,\"start\":34997},{\"end\":35021,\"start\":35010},{\"end\":35390,\"start\":35367},{\"end\":35412,\"start\":35390},{\"end\":35426,\"start\":35412},{\"end\":35451,\"start\":35426},{\"end\":35474,\"start\":35451},{\"end\":35763,\"start\":35751},{\"end\":35778,\"start\":35763},{\"end\":35795,\"start\":35778},{\"end\":35950,\"start\":35935},{\"end\":35962,\"start\":35950},{\"end\":35976,\"start\":35962},{\"end\":36376,\"start\":36363},{\"end\":36388,\"start\":36376},{\"end\":36402,\"start\":36388},{\"end\":36418,\"start\":36402},{\"end\":36428,\"start\":36418},{\"end\":36439,\"start\":36428},{\"end\":36812,\"start\":36797},{\"end\":36828,\"start\":36812},{\"end\":36844,\"start\":36828},{\"end\":37283,\"start\":37265},{\"end\":37300,\"start\":37283},{\"end\":37313,\"start\":37300},{\"end\":37753,\"start\":37735},{\"end\":37767,\"start\":37753},{\"end\":37787,\"start\":37767},{\"end\":38231,\"start\":38218},{\"end\":38247,\"start\":38231},{\"end\":38268,\"start\":38247},{\"end\":38283,\"start\":38268},{\"end\":38293,\"start\":38283},{\"end\":38304,\"start\":38293},{\"end\":38319,\"start\":38304},{\"end\":38334,\"start\":38319},{\"end\":38361,\"start\":38334},{\"end\":38383,\"start\":38361},{\"end\":38846,\"start\":38836},{\"end\":38858,\"start\":38846},{\"end\":38870,\"start\":38858},{\"end\":38882,\"start\":38870},{\"end\":38892,\"start\":38882},{\"end\":39285,\"start\":39268},{\"end\":39299,\"start\":39285},{\"end\":39316,\"start\":39299},{\"end\":39323,\"start\":39316},{\"end\":39555,\"start\":39543},{\"end\":39569,\"start\":39555},{\"end\":39573,\"start\":39569}]", "bib_venue": "[{\"end\":36985,\"start\":36923},{\"end\":31420,\"start\":31414},{\"end\":31727,\"start\":31707},{\"end\":32034,\"start\":32016},{\"end\":32406,\"start\":32388},{\"end\":32869,\"start\":32832},{\"end\":33398,\"start\":33361},{\"end\":34054,\"start\":34025},{\"end\":34618,\"start\":34581},{\"end\":35061,\"start\":35021},{\"end\":35365,\"start\":35296},{\"end\":35749,\"start\":35728},{\"end\":36015,\"start\":35976},{\"end\":36361,\"start\":36165},{\"end\":36921,\"start\":36844},{\"end\":37399,\"start\":37313},{\"end\":37843,\"start\":37787},{\"end\":38391,\"start\":38383},{\"end\":38953,\"start\":38892},{\"end\":39266,\"start\":39220},{\"end\":39541,\"start\":39499}]"}}}, "year": 2023, "month": 12, "day": 17}