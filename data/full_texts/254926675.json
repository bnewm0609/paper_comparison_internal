{"id": 254926675, "updated": "2023-10-05 06:22:10.453", "metadata": {"title": "Benchmarking Large Language Models for Automated Verilog RTL Code Generation", "authors": "[{\"first\":\"Shailja\",\"last\":\"Thakur\",\"middle\":[]},{\"first\":\"Baleegh\",\"last\":\"Ahmad\",\"middle\":[]},{\"first\":\"Zhenxing\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Hammond\",\"last\":\"Pearce\",\"middle\":[]},{\"first\":\"Benjamin\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Ramesh\",\"last\":\"Karri\",\"middle\":[]},{\"first\":\"Brendan\",\"last\":\"Dolan-Gavitt\",\"middle\":[]},{\"first\":\"Siddharth\",\"last\":\"Garg\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Automating hardware design could obviate a significant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pre-trained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM checkpoints are available: https://github.com/shailja-thakur/VGen.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.11140", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/date/ThakurAFPTKDG23", "doi": "10.23919/date56975.2023.10137086"}}, "content": {"source": {"pdf_hash": "e4f8cb7bb933d95bec8d6eaeeb9d1815ed095f21", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.11140v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "12cbdfdd3b6b7289f632091e277880eb9b3d6053", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e4f8cb7bb933d95bec8d6eaeeb9d1815ed095f21.txt", "contents": "\nBenchmarking Large Language Models for Automated Verilog RTL Code Generation\n\n\nShailja Thakur \nBaleegh Ahmad \nZhenxing Fan \nHammond Pearce \nBenjamin Tan \nUniversity of Calgary\n\n\nRamesh Karri \nBrendan Dolan-Gavitt \nSiddharth Garg \n\nNew York University\n\n\nBenchmarking Large Language Models for Automated Verilog RTL Code Generation\nIndex Terms-TransformersVerilogGPTsCode-based LLMs\nAutomating hardware design could obviate a significant amount of human error from the engineering process and lead to fewer errors. Verilog is a popular hardware description language to model and design digital systems, thus generating Verilog code is a critical first step. Emerging large language models (LLMs) are able to write high-quality code in other programming languages. In this paper, we characterize the ability of LLMs to generate useful Verilog. For this, we fine-tune pretrained LLMs on Verilog datasets collected from GitHub and Verilog textbooks. We construct an evaluation framework comprising test-benches for functional analysis and a flow to test the syntax of Verilog code generated in response to problems of varying difficulty. Our findings show that across our problem scenarios, the fine-tuning results in LLMs more capable of producing syntactically correct code (25.9% overall). Further, when analyzing functional correctness, a fine-tuned open-source CodeGen LLM can outperform the state-of-the-art commercial Codex LLM (6.5% overall). Training/evaluation scripts and LLM checkpoints are available: https://github.com/shailja-thakur/VGen.\n\nI. INTRODUCTION\n\nState-of-the-art hardware design flows use hardware description languages (HDLs) such as Verilog and VHDL to specify hardware architectures and behaviors. However, the process of writing HDL code is time-consuming and bugprone [1]. As design complexity grows, there is a need to reduce design costs and developer effort during hardware specification. High-level synthesis tools enable developers to specify functionality in languages like C but come at the expense of hardware efficiency. A promising new approach is the use of large language models (LLMs) [2] to automatically generate code from natural language specifications. LLMs are successful in generating code in languages like C and Python. Their use in generating HDL code requires study.\n\nLLMs are deep neural networks, typically based on transformer architectures, that aim to model the underlying distribution of a natural or structured language corpus. Given a sequence of words (or \"tokens\") LLMs predict a distribution over the next word/token. Used in a loop, LLMs can complete paragraphs in English starting with the first sentence, or code from comments or initial lines of code.\n\nWe undertake the first comprehensive evaluation of the syntactic and functional correctness of synthesizable Verilog code generated by both open-source and commercial LLMs. There are several challenges. First, baseline LLMs, including GitHub Copilot which ostensibly generates code in many programming languages including Verilog, frequently fail syntax, synthesis, and functional checks [3]. Fine-tuning LLMs on a Verilog corpus can help, but requires a large dataset of Verilog code which is lacking. Prior work trained models on template-generated hardware specifications and corresponding Verilog, but this is time-consuming and does not generalize to unseen problems [4]. Finally, test problems and methods to evaluate the syntactic and functional correctness of LLMgenerated code on a large scale are lacking. Our paper contributes the following.\n\n(1) By consolidating available open-source Verilog code as well as a broad search of textbooks about the Verilog HDL, we create (to the best of our knowledge) the largest training corpus of Verilog code yet used for training LLMs. (2) Using this corpus, we examine fine-tuning five different pre-trained LLMs models with parameter counts ranging from 345M to 16B, producing five new fine-tuned models specialized for Verilog. (3) To evaluate the efficacy of the models and determine the effect of the parameter sizes, we design a set of Verilog coding problems with varying difficulty levels, and corresponding test benches to test the functional correctness of generated code. Fig. 1 illustrates our experimental platform for studying the impact of parameters such as temperature, number of sequences generated per problem, and number of LLM parameters. Section III discusses creating the training data from GitHub and PDFs of Verilog textbooks 1 with pre-processing 2 and the five pre-trained LLMs 3 that we fine-tune 4 for completing Verilog code 5 . Section IV explains the evaluation setup, including our hand-designed prompts 6 . Section V presents our results from generating code suggestions 7 and evaluating with an analysis pipeline that compiles the Verilog and checks it against unit tests 8 . Section VI discusses how our evaluation shows that the largest code-based LLMs (i.e., CodeGen-16B) fine-tuned on our Verilog corpus outperforms all other evaluated LLMs. Qualitatively, our best-performing LLMs can generate functioning code for challenging problems.\n\n\nII. BACKGROUND AND RELATED WORK\n\n\nA. Background\n\nTransformer-based deep neural networks [5] have demonstrated impressive ability in myriad domains, including language-related tasks. Inputs to LLMs are in the form of tokens-a set of common character sequences where each has a unique numeric identifier using a byte pair encoding [6]. Given a sequence of tokens as an input prompt, the LLM outputs a probability distribution over the vocabulary for the next token given the prompt. A token is picked from this distribution, often the most likely token, appended to the prompt, and this sequence is fed back to the LLM, yielding a new token. This is repeated to generate a completion, a sequence of tokens that completes the input prompt.\n\nLLMs for code are trained on a corpus of code in a target programming language or sometimes even on a mix of source code files in various languages. Dataset sizes can often be on the order of hundreds of gigabytes. Prompts for these LLMs can be in the form of comments, code snippets, or both. An LLM trained on a mix of programming languages will often (implicitly) infer the language from the prompt.\n\nLLMs are expensive to train from scratch due to their large datasets and massive parameter counts. However, pretrained LLMs can be specialized for a user task by fine-tuning them on a specialized dataset. Fine-tuning is significantly faster than training from scratch because it only requires a small number of training epochs. Several LLMs pre-trained for both natural language and code either make the weights available, like NVIDIA's MegatronLM [7] or Salesforce's CodeGen models [8], or provide fine-tuning through an API, like AI21studio's Jurassic-1 (J1) models. 1\n\n\nB. Prior Work\n\nProgramming is a challenging task, given the need for human designers to interpret and transform natural language specifications into programming structures. This motivates the use of natural language processing (NLP) to transform language to code [9]. Hardware design using Verilog HDL is similar to programming. Prior work explored NLP techniques for generating assertions [10], albeit on a small scale. Pearce et al. trained DAVE, a small LLM to produce Verilog snippets from template-based natural language descriptions for a limited set of functions [4]. GitHub's Copilot was evaluated for security bugs produced during out-of-the-box Verilog completions [3] and was found to be lacking. This study is a large-scale exploration of the capabilities of LLMs across more design tasks using an automated evaluation framework. There is no open dataset to train and evaluate LLMs on writing Verilog.\n\n\nIII. LLM TRAINING\n\nIn this section, we describe our method for training (or finetuning) LLM models for Verilog code generation. We begin by describing our curated Verilog datasets, followed by the LLM architectures and the method for fine-tuning.\n\n\nA. Verilog Training Corpus\n\nOur primary Verilog training corpus comes from opensource Verilog code in public GitHub repositories. Additionally, we also created a dataset of text from Verilog textbooks to understand whether that further improved LLM performance. a) GitHub Corpus: We use Google BigQuery to gather Verilog repositories from GitHub, where it has a snapshot of over 2.8 million repositories. We use a query that looks for keywords such as \"Verilog\" and files with '.v' extension. We de-duplicated files (using MinHash and Jaccard similarity metrics [11]) and filtered files by keeping '.v' files that contain at least one pair of module and endmodule statements. Finally, we filtered large files (number of characters \u2265 20K). The training corpus from GitHub yielded \u223c50K files with a size of \u223c300 MB.\n\nb) Verilog Books Corpus: We downloaded 70 Verilogbased textbooks from an online e-library in PDF format, then extracted text using the Python-based tool pymuPDF which uses optical character recognition to extract text. Depending on the quality of the PDF, the text quality varies. We cleaned the text by filtering irrelevant passages (e.g., index, preface, and acknowledgments) and used regular expressions to check highlevel syntax of Verilog snippets from the surrounding prose, then use an overlapping sliding window on the filtered text corpus to produce training examples. The final Verilog corpus of textbook-extracted and GitHub code had a size of 400 MB. Table I shows the LLMs used in our study and summarizes design parameters including the number of layers, heads, embedding size (head dimension), context length, and the data source (natural language (NL) and/or code). Since codedavinci-002 is derived from GPT-3 [2], its architecture is the same as GPT-3. Its exact parameters are not known, so we leave these as NA in the table.\n\n\nB. Baseline LLM Architectures\n\n\nC. LLM fine-tuning\n\nFive LLMs from Table I are fine-tuned on our Verilog training datasets. Training the CodeGen LLMs was challenging due to the large number of parameters. At 16-bit precision, CodeGen-16B's parameters alone occupy 30 GB of GPU memory; fine-tuning additionally requires sufficient GPU memory to store optimizer states and intermediate calculations, requiring around 250GB across multiple GPUs. We use model and data parallelism and strategies for sharding the optimizer states across GPUs by basing our implementation on DeepSpeed. 2 We set the training hyperparameters to their defaults. The CodeGen LLMs (2B, 6B, 16B) are fine-tuned for 1 epoch on an HPC cluster with two RTX8000s, four RTX8000s, and three A100s, and training completes in two, four, and six days, respectively. Megatron-LM is fine-tuned for 9 epochs using one RTX8000 for 15 hours using the default configuration [7]. We use the commercial off-the-shelf AI21 studio for fine-tuning J1-Large.\n\n\nIV. LLM EVALUATION SETUP\n\nThe input to an LLM is a prompt from the problem set in Section IV-A. The LLM-produced code completions on the problem are then truncated at keywords end and endmodule, and checked for compilation and functional correctness. We use Icarus Verilog v11.0 for compiling the Verilog.\n\n\nA. The Problem Set\n\nThe problem set has 17 Verilog problems inspired by classroom exercises that our team is familiar with and examples from the HDLBits website. 3 We assign a difficulty level for each problem as shown in Table II. The problem set covers designs spanning combinational and sequential logic. It has three finite state machines with different requirements, operations such as permutation, shift left, and rotate, and basic blocks like a MUX, RAM, LFSR, adders, and counters.\n\nExamples for basic, intermediate and advanced problems are shown in Fig. 2-4 respectively. They are obtained using CodeGen-16B-FT and edited for visualization and brevity.\n\n\nB. Input parameters\n\nEach LLM query has a prompt, a sampling temperature (t), and a number of completions/prompt (n). Prompts: Three prompts with increasing detail i.e., low (L), medium (M), and high (H) are provided. Prompt L has an initial comment describing the function of the module and the module header with name and inputs/outputs with types. Internal signals are also declared. M includes L plus comments that describe the function using signal names. H replaces and/or appends comments in M with more detail and resembles pseudo-code as opposed to a predominantly natural language specification. Fig. 5 is an example for Problem 15. L has no 3 https://hdlbits.01xz.net/wiki/Problem sets Intermediate A 1-to-12 counter 7\n\nIntermediate LFSR with taps at 3 and 5 8\n\nIntermediate FSM with two states 9\n\nIntermediate Shift left and rotate 10\n\nIntermediate Random Access Memory 11\n\nIntermediate Permutation 12\n\nIntermediate Truth  \n\n\nC. Test benches\n\nFor each problem, we developed a test bench to check for functional correctness. The test benches exercise the designs 1 / / T h i s i s a c o u n t e r t h a t c o u n t s from 1 t o 12 2 module c o u n t e r ( i n p u t c l k , i n p u t r e s e t , o u t p u t r e g [ 3 : 0 ] q ) ; (a) Prompt 1 a l w a y s @( p o s e d g e c l k ) b e g i n 2 i f ( r e s e t ) q <= 4 ' d1 ; 3 e l s e b e g i n 4 i f ( q == 4 ' d12 ) q <= 4 ' d1 ; 5 e l s e q <= q + 4 ' d1 ;\n\n(b) Correct completion 1 a l w a y s @ ( p o s e d g e c l k ) b e g i n 2 i f ( r e s e t ) q <= 4 ' d1 ; 3 e l s e b e g i n 4 q <= q + 4 ' d1 ; 5 end end endmodule (c) Incorrect completion. Counter does not stop at 12. Fig. 3. Intermediate example -Problem 6: A 1 to 12 counter for corner cases and are exhaustive for basic and some intermediate cases. For the remaining cases, the test benches are analogous to unit tests. This keeps the time to evaluation reasonable. For example, for the RAM module, the data width is 8 and the address width is 6 in the prompt. An exhaustive test bench requires 2 14 test inputs and would take longer to simulate. In some cases, specifications in the prompts are ambiguous and thus can yield several correct responses. For example, when one does not specify whether a reset should be synchronous or asynchronous.\n\n\nV. LLM EVALUATION AND RESULTS\n\n\nA. Research Questions\n\nThis study examines four research questions (RQs) from the point of view of quality of Verilog generation given the scenarios and test-benches from Section IV: RQ1. How well do 'base' LLMs perform on the Verilog generation set? RQ2. Does fine-tuning LLMs improve that performance? RQ3. Are larger LLMs with more parameters better? RQ4. Does variability in problem description impact quality and the number of correct completions?\n\n\nB. Results\n\nWe measure the quality of the code generated by LLMs using problem sets described in Section IV. A scenario is a combination of problems across difficulties and description levels. We query the models with all prompt \u00d7 t \u00d7 n combinations. For fairness, we present each model's \"best results\" by focusing on the completions generated with the t for each model for which their completions were most successful at compiling and passing the functional tests (for each problem difficulty and description level). We present these best results for n = 10 in Table III and Table IV. Table III shows the proportion of completions that compile and Table IV shows the proportion of completions that pass functional tests, for the completions produced by a given temperature setting that resulted in the most successful completions for each scenario. As in prior work [8], we characterize the model performance with the Pass@k metric, where k is the number of problems in a scenario times n, the number of suggestions per problem. A higher Pass@k indicates a relatively 'better' result. For compilation (Table III), the Pass@k metric reflects the proportion of completions that compile. For functional tests, this metric is the fraction of the k code samples that pass. For interest, Table IV reports the inference time for each LLMs query, including communication time with a remote server if required. Note that the results are after fine-tuning the model using the training corpus from GitHub only. We discuss the case for fine-tuning on GitHub and PDFs combined as an ablation study in the discussion. Fine-tuned CodeGen-16B LLM outperforms all LLMs. All fine-tuned LLMs outperform their pre-trained counterparts. [Ans. RQ1 and RQ2].\n\n1) Completions vs. Temperature (t): Fig. 6 summarizes the Pass@(scenario*n) metric for our experiments sweeping temperature. Pass@(scenario*10) has the highest value for t = 0.1 and degrades exponentially with temperature. The LLM generates accurate solutions at low temperatures and accurate synthesizable codes are expected from a smaller number of candidates.\n\n2) Completions vs. # Completions/Prompt (n): We study synthesis quality as a function of completions/prompt. The right-hand panel in Fig. 6 shows the Pass@(scenario*n) for all LLMs. Pass@(scenario*1) is better than Pass@(scenario*10). This improves as the number of completions increases. This is the case because the number of candidate solutions at low temperatures increases, increasing the completions passing the test benches. n = 10 is good for all problem difficulty levels.\n\n3) Completions vs. LLM Size: Fig. 6 and 7 show that LLMs with more parameters (CodeGen-16B, code-davinci-002) outperform LLMs with less parameters such as Megatron-355M and CodeGen-2B. These LLMs yield more completions that pass test benches and more correct completions. [Ans. RQ3].\n\n4) Completions vs. Prompts: Prompt quality impacts the LLM generation quality. We study the effect of variations in the prompt description at two levels: How do difficulty of the prompt and the description of the prompt impact code completions. We use Pass@(scenario*10) as the metric. The righthand side panel in Fig. 7 shows that the Pass@(scenario*10) decreases with increasing prompt difficulty. Simple problems such as AND are easy to translate to Verilog, as opposed to advanced problems such as LFSR. The left-hand side panel in Fig. 7 shows that the number of correct solutions decreases with terse prompts. [Ans. RQ4]. Table IV, fine-tuned LLMs generate code that compiles better when compared to the pre-trained LLMs. Using the best Pass@(scenario*10) values, only 11.9% of the completions generated by pre-trained LLMs compiled while 64.6% of those by fine-tuned LLMs compiled. Thus, a designer may use these LLMs with text and/or pseudo-code to generate a syntactically-correct 'skeleton' of a design, before then tweaking it to meet functional requirements.\n\n\nVI. DISCUSSION AND LIMITATIONS As shown in\n\nWe assess LLMs' code completion using the associated Verilog test-benches. These test-benches are comprehensive for the Basic problems, but as the problems become more complex, the test-benches cover only those behaviors fully specified in the problem comments. As LLMs tend to provide similar responses when several completions per prompt are requested, the exact test-bench implementation can have a large impact on how many cases pass. We observe an example of this in the LLMs' responses to FSM problems 8, 15, and 17. Since the problem comments do not specify whether the reset is synchronous/asynchronous, the LLMs are free to produce any variation. As such, for all problems, we verify whether an active-high reset results in the appropriate value at the output, but we do not test the asynchronous/synchronous corner case nor other similar edge conditions. Even the best performing LLM (CodeGen-16B (FT)) per- formed poorly for some problem sets. For any given problem, CodeGen-16B (FT) produced 540 completions, but for Problems 7 (LFSR) and 12 (Truth table), none of the completions passed, and for Problem 9 (Shift and Rotate), only one passed. We manually investigated the completions and observed that for Prob. # 7, the LLMs had trouble concatenating the most significant bits with the feedback value. This was the problem in most cases and a better prompt might yield a correct result. This indicates the importance of creating the best prompt, pointing to prompt engineering as future work. For Prob. #9, completions either do not cover all values of the shift or assign incorrect bit positions. For Prob. #12, completions are close to the actual solution by using all input values in assign statements but fail to form correct expressions between input bits. This suggests insufficient diversity in the training corpus. Next, we study the impact of the training corpus on LLM fine-tuning. We conduct an ablation study using (a) CodeGen-16B fine-tuned with GitHub verilog repositories only and (b) CodeGen-16B fine-tuned with Verilog from Github and textbooks. The Pass@(scenario*10) for (a) and (b) show that option (b) is marginally better (1.4%) than (a). This is the case because the Verilog corpus from PDFs adds more examples and this helps the LLM to generalize to Verilog.\n\n\nVII. CONCLUSIONS\n\nThis paper describes a new paradigm for automatically generating and verifying Verilog from LLMs. Using the presented Pass@(scenario*n) values from Tables III-IV, pretuned LLMs produced completions that are functionally correct only 1.09% of the time. This number increases to 27.0% after tuning, showing a clear benefit to fine-tuning LLMs over a specific language. The fine-tuned CodeGen-16B LLM was the most successful in completions with respect to functional correctness. Overall it produced functionally correct code 41.9% of time, whereas the commercially available stateof-the-art (non-fine-tuned) code-davinci-002 LLM produced functionally correct code 35.4% of time.\n\nFig. 1 .\n1Experimental Evaluation of LLM Verilog Completions\n\n\ncompletion. Positions are offset by 1. Fig. 2. Basic example -Problem 3: A 3-bit priority encoder lines highlighted (the prompt is lines 1-4). M includes L and lines highlighted yellow (the prompt is lines 1-8). H includes M and lines in gray (the prompt is lines 1-15). Sampling temperature (t): A higher value means that the LLM takes more risks and yields more creative completions. We use t \u2208 {0.1, 0.3.0.5, 0.7, 1}. Completions per prompt (n): For each prompt, LLM generates n completions where n \u2208 {1, 10, 25}. For J1-Large, we skip n = 25 because they do not support this value. max tokens: The maximum number of tokens generated for each completion was set to 300 for all LLMs except J1-Large. For J1-Large the limit is 256. Nucleus sampling probability mass (top p) was set to the default value of 1.\n\n\ncompletion. Output is not assigned to state SAB.Fig. 4. Advanced example -Problem 17\n\nFig. 5 .\n5Varying the prompt details: Low, Medium and High. Problem 15.\n\nTABLE I BASELINE\nILLM ARCHITECTURES USED IN OUR STUDY.Model-Parameters / \nPre-Training Data \nLayers \nHeads \nEm-\nbed. \n\nContext \nLength \n\nMegatronLM-355M [7] / \nNL [12], [13] \n24 \n16 \n64 \n1024 \n\nJ1-Large-7B 1 / NL [14] \n32 \n32 \n128 \n4096 \n\nCodeGen-2B [8] / \nNL [15], Code \n32 \n32 \n80 \n2048 \n\nCodeGen-6B / NL [15], \nCode \n33 \n16 \n256 \n2048 \n\nCodeGen-16B / NL [15], \nCode \n34 \n24 \n256 \n2048 \n\ncode-davinci-002 [2] / \nNL [14], Code \nNA \nNA \nNA \n8000 \n\n\n\nTABLE II\nIIPROBLEM SET \n\nProb. # Difficulty \nDescription \n1 \nBasic \nA simple wire \n2 \nBasic \nA 2-input and gate \n3 \nBasic \nA 3-bit priority encoder \n4 \nBasic \nA 2-input multiplexer \n5 \nIntermediate A half adder \n6 \n\n\nTABLE III PASS@\nIII(SCENARIO*n) AT n=10 FOR COMPILED COMPLETIONS (PASS=COMPILING), PT = PRE-TRAINED, FT = FINE-TUNED. BOLD REFLECTS THE (BEST) HIGHEST PERFORMANCE FOR THAT DIFFICULTY. Fig.6. Pass@(scenario*n) for scenarios passing test-benches across temperature (t) and completions per prompt (n). Higher is better.Fig. 7. Pass@(scenario*n) for scenarios passing test-benches across problem difficulties and description levels. Higher is better.Model \nModel Type \nBasic \nIntermediate Advanced \n\nMegatronLM-345M \nPT \n0.000 \n0.000 \n0.000 \nFT \n0.730 \n0.391 \n0.165 \n\nCodeGen-2B \nPT \n0.080 \n0.065 \n0.176 \nFT \n0.902 \n0.612 \n0.592 \n\nCodeGen-6B \nPT \n0.052 \n0.152 \n0.187 \nFT \n0.987 \n0.689 \n0.599 \n\nJ1-Large-7B \nPT \n0.182 \n0.176 \n0.108 \nFT \n0.882 \n0.635 \n0.588 \n\nCodeGen-16B \nPT \n0.132 \n0.203 \n0.240 \nFT \n0.942 \n0.728 \n0.596 \n\ncode-davinci-002 \nPT \n0.847 \n0.452 \n0.569 \n\n\n\nTABLE IV\nIVPASS@(SCENARIO*n) AT n =10 FOR TEST BENCH PASSING COMPLETIONS (PASS=PASSED FUNCTIONAL TESTS), PT = PRE-TRAINED, FT = FINE-TUNED. BOLDED VALUE IN EACH TEST COLUMN REFLECTS THE (BEST) HIGHEST PERFORMANCE FOR THAT PROBLEM SET AND DIFFICULTY.Model \nModel \nType \n\nInference \nTime (s) \n\nBasic \nIntermediate \nAdvanced \n\nL \nM \nH \nL \nM \nH \nL \nM \nH \n\nMegatronLM-355M \nPT \n3.628 \n0.000 0.000 0.000 \n0.000 0.000 0.000 \n0.000 0.000 0.000 \nFT \n0.175 \n0.170 0.591 0.245 \n0.043 0.018 0.025 \n0.000 0.000 0.000 \n\nCodeGen-2B \nPT \n1.478 \n0.000 0.000 0.000 \n0.000 0.000 0.000 \n0.000 0.016 0.020 \nFT \n0.665 \n0.835 0.350 0.630 \n0.130 0.092 0.163 \n0.132 0.048 0.068 \n\nCodeGen-6B \nPT \n2.332 \n0.000 0.000 0.000 \n0.000 0.000 0.013 \n0.000 0.000 0.000 \nFT \n0.710 \n1.000 \n0.500 0.760 \n0.135 0.150 0.168 \n0.284 \n0.164 0.164 \n\nJ1-Large-7B \nPT \n7.146 \n0.044 0.058 0.067 \n0.000 0.000 0.021 \n0.000 0.000 0.000 \nFT \n2.029 \n0.388 0.283 0.342 \n0.125 0.075 0.200 \n0.000 0.000 0.000 \n\nCodeGen-16B \nPT \n2.835 \n0.000 0.085 0.055 \n0.035 0.003 0.045 \n0.012 0.000 0.016 \nFT \n1.994 \n0.745 \n0.720 \n0.745 \n0.213 0.270 0.255 \n0.246 \n0.290 \n0.294 \n\ncode-davinci-002 \nPT \n3.885 \n0.520 0.685 \n0.775 \n0.175 0.200 0.150 \n0.156 0.184 \n0.344 \n\n\nhttps://studio.ai21.com/docs/jurassic1-language-models/ #general-purpose-models\nhttps://huggingface.co/docs/transformers/main classes/deepspeed\nACKNOWLEDGEMENTSThis research work was supported in part by NSF Award 1553419, NSF Award 1646671, NSF Award 2039607, and ARO Award 77191NC. The opinions, findings, and conclusions, or recommendations expressed are those of the author(s) and do not necessarily reflect the views of any sponsors.\n. Idle =0, S1 =1, S10 =2, IDLE =0 , S1 =1 , S10 =2 , S101 = 3 ;\n\nHardfails: Insights into Software-Exploitable Hardware Bugs. G Dessouky, Proc. 28th USENIX Conf. Security Symp. USENIX Association. 28th USENIX Conf. Security Symp. USENIX AssociationG. Dessouky et al., \"Hardfails: Insights into Software-Exploitable Hard- ware Bugs,\" in Proc. 28th USENIX Conf. Security Symp. USENIX Association, 2019, pp. 213-230.\n\nEvaluating Large Language Models Trained on Code. M Chen, arXiv:2107.03374M. Chen et al., \"Evaluating Large Language Models Trained on Code,\" Jul. 2021, arXiv:2107.03374 [cs]. [Online]. Available: http://arxiv.org/abs/2107.03374\n\nAsleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions. H Pearce, 2022 IEEE Symp. on Security and Privacy (SP). H. Pearce et al., \"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions,\" in 2022 IEEE Symp. on Security and Privacy (SP), May 2022, pp. 754-768, iSSN: 2375-1207.\n\nDAVE: Deriving Automatically Verilog from English. H Pearce, B Tan, R Karri, Proc. of the 2020 ACM/IEEE Workshop on Machine Learning for CAD. of the 2020 ACM/IEEE Workshop on Machine Learning for CADACMH. Pearce, B. Tan, and R. Karri, \"DAVE: Deriving Automatically Verilog from English,\" in Proc. of the 2020 ACM/IEEE Workshop on Machine Learning for CAD. ACM, Nov. 2020, pp. 27-32. [Online].\n\n. https:/dl.acm.org/doi/10.1145/3380446.3430634Available: https://dl.acm.org/doi/10.1145/3380446.3430634\n\nAttention is All you Need. A Vaswani, Advances in Neural Information Processing Systems. I. Guyon et al.Curran Associates, Inc30A. Vaswani et al., \"Attention is All you Need,\" in Advances in Neural Information Processing Systems 30, I. Guyon et al., Eds. Curran Associates, Inc., 2017, pp. 5998-6008.\n\nA New Algorithm for Data Compression. P Gage, C Users Journal. 122P. Gage, \"A New Algorithm for Data Compression,\" C Users Journal, vol. 12, no. 2, pp. 23-38, Feb. 1994.\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. M Shoeybi, arXiv:1909.08053M. Shoeybi et al., \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,\" Mar. 2020, arXiv:1909.08053 [cs]. [Online]. Available: http://arxiv.org/abs/1909. 08053\n\nA Conversational Paradigm for Program Synthesis. E Nijkamp, arXiv:2203.13474E. Nijkamp et al., \"A Conversational Paradigm for Program Synthesis,\" Mar. 2022, arXiv:2203.13474 [cs]. [Online]. Available: http://arxiv.org/abs/2203.13474\n\nNLP (Natural Language Processing) for NLP (Natural Language Programming). R Mihalcea, H Liu, H Lieberman, Computational Linguistics and Intelligent Text Processing. A. GelbukhBerlin HeidelbergSpringerR. Mihalcea, H. Liu, and H. Lieberman, \"NLP (Natural Language Pro- cessing) for NLP (Natural Language Programming),\" in Computational Linguistics and Intelligent Text Processing, A. Gelbukh, Ed. Springer Berlin Heidelberg, 2006, pp. 319-330.\n\nGLAsT: Learning formal grammars to translate natural language specifications into hardware assertions. C B Harris, I G Harris, Design, Automation Test in Europe Conf. Exhibition (DATE). C. B. Harris and I. G. Harris, \"GLAsT: Learning formal grammars to translate natural language specifications into hardware assertions,\" in Design, Automation Test in Europe Conf. Exhibition (DATE), 2016, pp. 966-971.\n\nPrivMin: Differentially Private MinHash for Jaccard Similarity Computation. Z Yan, arXiv:1705.07258Z. Yan et al., \"PrivMin: Differentially Private MinHash for Jaccard Similarity Computation,\" May 2017, arXiv:1705.07258 [cs]. [Online].\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\" May 2019, arXiv:1810.04805 [cs]. [Online]. Available: http://arxiv.org/abs/ 1810.04805\n\nLanguage Models are Unsupervised Multitask Learners. A Radford, 24A. Radford et al., \"Language Models are Unsuper- vised Multitask Learners,\" p. 24, 2019. [Online]. Avail- able: https://cdn.openai.com/better-language-models/language models are unsupervised multitask learners.pdf\n\nLanguage Models are Few-Shot Learners. T Brown, Advances in Neural Information Processing Systems. H. Larochelle et al.Curran Associates, Inc33T. Brown et al., \"Language Models are Few-Shot Learners,\" in Advances in Neural Information Processing Systems, H. Larochelle et al., Eds., vol. 33. Curran Associates, Inc., 2020, pp. 1877- 1901. [Online]. Available: https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n\nThe Pile: An 800GB Dataset of Diverse Text for Language Modeling. L Gao, arXiv:2101.00027L. Gao et al., \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling,\" Dec. 2020, arXiv:2101.00027 [cs]. [Online].\n", "annotations": {"author": "[{\"end\":95,\"start\":80},{\"end\":110,\"start\":96},{\"end\":124,\"start\":111},{\"end\":140,\"start\":125},{\"end\":178,\"start\":141},{\"end\":192,\"start\":179},{\"end\":214,\"start\":193},{\"end\":230,\"start\":215},{\"end\":253,\"start\":231}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":88},{\"end\":109,\"start\":104},{\"end\":123,\"start\":120},{\"end\":139,\"start\":133},{\"end\":153,\"start\":150},{\"end\":191,\"start\":186},{\"end\":213,\"start\":201},{\"end\":229,\"start\":225}]", "author_first_name": "[{\"end\":87,\"start\":80},{\"end\":103,\"start\":96},{\"end\":119,\"start\":111},{\"end\":132,\"start\":125},{\"end\":149,\"start\":141},{\"end\":185,\"start\":179},{\"end\":200,\"start\":193},{\"end\":224,\"start\":215}]", "author_affiliation": "[{\"end\":177,\"start\":155},{\"end\":252,\"start\":232}]", "title": "[{\"end\":77,\"start\":1},{\"end\":330,\"start\":254}]", "venue": null, "abstract": "[{\"end\":1549,\"start\":382}]", "bib_ref": "[{\"end\":1798,\"start\":1795},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2128,\"start\":2125},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3110,\"start\":3107},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3394,\"start\":3391},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5238,\"start\":5235},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5479,\"start\":5476},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6740,\"start\":6737},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6775,\"start\":6772},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7128,\"start\":7125},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7256,\"start\":7252},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7435,\"start\":7432},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7540,\"start\":7537},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8593,\"start\":8589},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9771,\"start\":9768},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10470,\"start\":10469},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10823,\"start\":10820},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11372,\"start\":11371},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15508,\"start\":15505}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21680,\"start\":21619},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22492,\"start\":21681},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22579,\"start\":22493},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22652,\"start\":22580},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":23102,\"start\":22653},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23319,\"start\":23103},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":24183,\"start\":23320},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":25384,\"start\":24184}]", "paragraph": "[{\"end\":2317,\"start\":1568},{\"end\":2717,\"start\":2319},{\"end\":3571,\"start\":2719},{\"end\":5144,\"start\":3573},{\"end\":5883,\"start\":5196},{\"end\":6287,\"start\":5885},{\"end\":6859,\"start\":6289},{\"end\":7775,\"start\":6877},{\"end\":8024,\"start\":7797},{\"end\":8840,\"start\":8055},{\"end\":9885,\"start\":8842},{\"end\":10898,\"start\":9940},{\"end\":11206,\"start\":10927},{\"end\":11698,\"start\":11229},{\"end\":11871,\"start\":11700},{\"end\":12603,\"start\":11895},{\"end\":12645,\"start\":12605},{\"end\":12681,\"start\":12647},{\"end\":12720,\"start\":12683},{\"end\":12758,\"start\":12722},{\"end\":12787,\"start\":12760},{\"end\":12809,\"start\":12789},{\"end\":13293,\"start\":12829},{\"end\":14147,\"start\":13295},{\"end\":14634,\"start\":14205},{\"end\":16374,\"start\":14649},{\"end\":16738,\"start\":16376},{\"end\":17221,\"start\":16740},{\"end\":17506,\"start\":17223},{\"end\":18578,\"start\":17508},{\"end\":20921,\"start\":18625},{\"end\":21618,\"start\":20942}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9512,\"start\":9505},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9962,\"start\":9955},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11439,\"start\":11431},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15233,\"start\":15200},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15295,\"start\":15287},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15751,\"start\":15740},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15929,\"start\":15921},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18144,\"start\":18136},{\"end\":19692,\"start\":19679}]", "section_header": "[{\"end\":1566,\"start\":1551},{\"end\":5178,\"start\":5147},{\"end\":5194,\"start\":5181},{\"end\":6875,\"start\":6862},{\"end\":7795,\"start\":7778},{\"end\":8053,\"start\":8027},{\"end\":9917,\"start\":9888},{\"end\":9938,\"start\":9920},{\"end\":10925,\"start\":10901},{\"end\":11227,\"start\":11209},{\"end\":11893,\"start\":11874},{\"end\":12827,\"start\":12812},{\"end\":14179,\"start\":14150},{\"end\":14203,\"start\":14182},{\"end\":14647,\"start\":14637},{\"end\":18623,\"start\":18581},{\"end\":20940,\"start\":20924},{\"end\":21628,\"start\":21620},{\"end\":22589,\"start\":22581},{\"end\":22670,\"start\":22654},{\"end\":23112,\"start\":23104},{\"end\":23336,\"start\":23321},{\"end\":24193,\"start\":24185}]", "table": "[{\"end\":23102,\"start\":22708},{\"end\":23319,\"start\":23115},{\"end\":24183,\"start\":23767},{\"end\":25384,\"start\":24434}]", "figure_caption": "[{\"end\":21680,\"start\":21630},{\"end\":22492,\"start\":21683},{\"end\":22579,\"start\":22495},{\"end\":22652,\"start\":22591},{\"end\":22708,\"start\":22672},{\"end\":23767,\"start\":23340},{\"end\":24434,\"start\":24196}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4257,\"start\":4251},{\"end\":11776,\"start\":11768},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12486,\"start\":12480},{\"end\":13523,\"start\":13517},{\"end\":16418,\"start\":16412},{\"end\":16879,\"start\":16873},{\"end\":17264,\"start\":17252},{\"end\":17828,\"start\":17822},{\"end\":18050,\"start\":18044}]", "bib_author_first_name": "[{\"end\":25951,\"start\":25950},{\"end\":26290,\"start\":26289},{\"end\":26557,\"start\":26556},{\"end\":26861,\"start\":26860},{\"end\":26871,\"start\":26870},{\"end\":26878,\"start\":26877},{\"end\":27337,\"start\":27336},{\"end\":27650,\"start\":27649},{\"end\":27870,\"start\":27869},{\"end\":28143,\"start\":28142},{\"end\":28402,\"start\":28401},{\"end\":28414,\"start\":28413},{\"end\":28421,\"start\":28420},{\"end\":28874,\"start\":28873},{\"end\":28876,\"start\":28875},{\"end\":28886,\"start\":28885},{\"end\":28888,\"start\":28887},{\"end\":29251,\"start\":29250},{\"end\":29493,\"start\":29492},{\"end\":29506,\"start\":29502},{\"end\":29515,\"start\":29514},{\"end\":29522,\"start\":29521},{\"end\":29826,\"start\":29825},{\"end\":30093,\"start\":30092},{\"end\":30572,\"start\":30571}]", "bib_author_last_name": "[{\"end\":25833,\"start\":25826},{\"end\":25840,\"start\":25835},{\"end\":25848,\"start\":25842},{\"end\":25960,\"start\":25952},{\"end\":26295,\"start\":26291},{\"end\":26564,\"start\":26558},{\"end\":26868,\"start\":26862},{\"end\":26875,\"start\":26872},{\"end\":26884,\"start\":26879},{\"end\":27345,\"start\":27338},{\"end\":27655,\"start\":27651},{\"end\":27878,\"start\":27871},{\"end\":28151,\"start\":28144},{\"end\":28411,\"start\":28403},{\"end\":28418,\"start\":28415},{\"end\":28431,\"start\":28422},{\"end\":28883,\"start\":28877},{\"end\":28895,\"start\":28889},{\"end\":29255,\"start\":29252},{\"end\":29500,\"start\":29494},{\"end\":29512,\"start\":29507},{\"end\":29519,\"start\":29516},{\"end\":29532,\"start\":29523},{\"end\":29834,\"start\":29827},{\"end\":30099,\"start\":30094},{\"end\":30576,\"start\":30573}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":25887,\"start\":25824},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":199521054},\"end\":26237,\"start\":25889},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b2\"},\"end\":26467,\"start\":26239},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":245220588},\"end\":26807,\"start\":26469},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":221445942},\"end\":27201,\"start\":26809},{\"attributes\":{\"doi\":\"https:/dl.acm.org/doi/10.1145/3380446.3430634\",\"id\":\"b5\"},\"end\":27307,\"start\":27203},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13756489},\"end\":27609,\"start\":27309},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":59804030},\"end\":27780,\"start\":27611},{\"attributes\":{\"doi\":\"arXiv:1909.08053\",\"id\":\"b8\"},\"end\":28091,\"start\":27782},{\"attributes\":{\"doi\":\"arXiv:2203.13474\",\"id\":\"b9\"},\"end\":28325,\"start\":28093},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14790149},\"end\":28768,\"start\":28327},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8732293},\"end\":29172,\"start\":28770},{\"attributes\":{\"doi\":\"arXiv:1705.07258\",\"id\":\"b12\"},\"end\":29408,\"start\":29174},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b13\"},\"end\":29770,\"start\":29410},{\"attributes\":{\"id\":\"b14\"},\"end\":30051,\"start\":29772},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":218971783},\"end\":30503,\"start\":30053},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b16\"},\"end\":30720,\"start\":30505}]", "bib_title": "[{\"end\":25948,\"start\":25889},{\"end\":26554,\"start\":26469},{\"end\":26858,\"start\":26809},{\"end\":27334,\"start\":27309},{\"end\":27647,\"start\":27611},{\"end\":28399,\"start\":28327},{\"end\":28871,\"start\":28770},{\"end\":30090,\"start\":30053}]", "bib_author": "[{\"end\":25835,\"start\":25826},{\"end\":25842,\"start\":25835},{\"end\":25850,\"start\":25842},{\"end\":25962,\"start\":25950},{\"end\":26297,\"start\":26289},{\"end\":26566,\"start\":26556},{\"end\":26870,\"start\":26860},{\"end\":26877,\"start\":26870},{\"end\":26886,\"start\":26877},{\"end\":27347,\"start\":27336},{\"end\":27657,\"start\":27649},{\"end\":27880,\"start\":27869},{\"end\":28153,\"start\":28142},{\"end\":28413,\"start\":28401},{\"end\":28420,\"start\":28413},{\"end\":28433,\"start\":28420},{\"end\":28885,\"start\":28873},{\"end\":28897,\"start\":28885},{\"end\":29257,\"start\":29250},{\"end\":29502,\"start\":29492},{\"end\":29514,\"start\":29502},{\"end\":29521,\"start\":29514},{\"end\":29534,\"start\":29521},{\"end\":29836,\"start\":29825},{\"end\":30101,\"start\":30092},{\"end\":30578,\"start\":30571}]", "bib_venue": "[{\"end\":26019,\"start\":25962},{\"end\":26287,\"start\":26239},{\"end\":26610,\"start\":26566},{\"end\":26949,\"start\":26886},{\"end\":27396,\"start\":27347},{\"end\":27672,\"start\":27657},{\"end\":27867,\"start\":27782},{\"end\":28140,\"start\":28093},{\"end\":28490,\"start\":28433},{\"end\":28954,\"start\":28897},{\"end\":29248,\"start\":29174},{\"end\":29490,\"start\":29410},{\"end\":29823,\"start\":29772},{\"end\":30150,\"start\":30101},{\"end\":30569,\"start\":30505},{\"end\":26072,\"start\":26021},{\"end\":27008,\"start\":26951},{\"end\":28519,\"start\":28502}]"}}}, "year": 2023, "month": 12, "day": 17}