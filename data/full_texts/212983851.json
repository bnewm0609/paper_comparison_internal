{"id": 212983851, "updated": "2023-07-19 13:36:10.076", "metadata": {"title": "Balancing Quality and Human Involvement: An Effective Approach to Interactive Neural Machine Translation", "authors": "[{\"first\":\"Tianxiang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Lemao\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Guoping\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Huayang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yingling\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Liu\",\"last\":\"GuiQuan\",\"middle\":[]},{\"first\":\"Shuming\",\"last\":\"Shi\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Conventional interactive machine translation typically requires a human translator to validate every generated target word, even though most of them are correct in the advanced neural machine translation (NMT) scenario. Previous studies have exploited con\ufb01dence approaches to address the intensive human involvement issue, which request human guidance only for a few number of words with low con\ufb01dences. However, such approaches do not take the history of human involvement into account, and optimize the models only for the translation quality while ignoring the cost of human involvement. In response to these pitfalls, we propose a novel interactive NMT model, which explicitly accounts the history of human involvements and particularly is optimized towards two objectives corresponding to the translation quality and the cost of human involvement, respectively. Speci\ufb01cally, the model jointly predicts a target word and a decision on whether to request human guidance, which is based on both the partial translation and the history of human involvements. Since there is no explicit signals on the decisions of requesting human guidance in the bilingual corpus, we optimize the model with the reinforcement learning technique which enables our model to accurately predict when to request human guidance. Simulated and real experiments show that the proposed model can achieve higher translation quality with similar or less human involvement over the con\ufb01dence-based baseline.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2997088366", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/ZhaoLHLLLS20", "doi": "10.1609/aaai.v34i05.6514"}}, "content": {"source": {"pdf_hash": "58c24f2ff86d6be9204b138049a9a9924c3807dc", "pdf_src": "Anansi", "pdf_uri": "[\"https://ojs.aaai.org/index.php/AAAI/article/download/6514/6370\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://ojs.aaai.org/index.php/AAAI/article/download/6514/6370", "status": "GOLD"}}, "grobid": {"id": "c9f58dfb1812b062d19150d0a799af76fc3ec802", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/58c24f2ff86d6be9204b138049a9a9924c3807dc.txt", "contents": "\nBalancing Quality and Human Involvement: An Effective Approach to Interactive Neural Machine Translation\n\n\nTianxiang Zhao \nPenn State University\n\n\nLemao Liu \nTencent AI Lab\n\n\nGuoping Huang \nTencent AI Lab\n\n\nZhaopeng Tu \nTencent AI Lab\n\n\nHuayang Li \nYingling Liu \nTencent AI Lab\n\n\nUniversity of Science and Technology of China\n\n\nGuiquan Liu \nShuming Shi shumingshi@tencent.com \nTencent AI Lab\n\n\nUniversity of Science and Technology of China\n\n\nBalancing Quality and Human Involvement: An Effective Approach to Interactive Neural Machine Translation\n\nConventional interactive machine translation typically requires a human translator to validate every generated target word, even though most of them are correct in the advanced neural machine translation (NMT) scenario. Previous studies have exploited confidence approaches to address the intensive human involvement issue, which request human guidance only for a few number of words with low confidences. However, such approaches do not take the history of human involvement into account, and optimize the models only for the translation quality while ignoring the cost of human involvement. In response to these pitfalls, we propose a novel interactive NMT model, which explicitly accounts the history of human involvements and particularly is optimized towards two objectives corresponding to the translation quality and the cost of human involvement, respectively. Specifically, the model jointly predicts a target word and a decision on whether to request human guidance, which is based on both the partial translation and the history of human involvements. Since there is no explicit signals on the decisions of requesting human guidance in the bilingual corpus, we optimize the model with the reinforcement learning technique which enables our model to accurately predict when to request human guidance. Simulated and real experiments show that the proposed model can achieve higher translation quality with similar or less human involvement over the confidence-based baseline.\n\nIntroduction\n\nRecent years have witnessed a breakthrough in neural machine translation (NMT) (Bahdanau, Cho, and Bengio 2015;Vaswani et al. 2017), but its translation quality is still incapable of satisfying the requirements in many industrial applications. Interactive machine translation (IMT) (Foster, Isabelle, and Plamondon 1997;Langlais, Foster, and Lapalme 2000), where human and machines collaborate to translate in a joint strategy, has thereby drawn much research attention (Green et al. 2014;Wuebker et al. 2016;Knowles and Koehn 2016;Peris, Domingo, and Casacuberta 2017). In the conventional IMT, a user and machine collaboratively generate the translation from left-to-right: at each time the user validates all words in a prefix and corrects one of them, and then the machine generates new words based on the corrected prefix until the translation process is finished.\n\nDespite its appealing performance, the user has to validate all words generated by a machine although most of them are actually correct, leading to considerable human involvement (Ueffing and Ney 2005). Many efforts (Gonz\u00e1lez-Rubio, Ortiz-Mart\u00ednez, and Casacuberta 2010b; Cheng et al. 2016;Knowles and Koehn 2018) thereby have been made to reduce human involvement based on the notion of confidence estimation (Blatz et al. 2004;Gonz\u00e1lez-Rubio, Ortiz-Mart\u00ednez, and Casacuberta 2010a;Lam, Kreutzer, and Riezler 2018). They query for human guidance for few of those words with low confidences so that prevent manually validating other words with high confidences. However, in these confidence based approaches, the confidence model is either an external classifier (Ueffing and Ney 2005;Cheng et al. 2016) or inherited from the standard translation models (Knowles and Koehn 2018) and they do not take the human guidance into account at all. Moreover, their learning objectives for both the confidence and translation models completely ignore human guidance. As a result, their performance could be limited, failing to catch the long-term influence of human involvement.\n\nIn this paper, we propose a novel interactive machine translation to jointly predict a word and a decision on when to request human guidance, which not only reduces human involvement but also generates excellent translations. We cast the query action as a special token in the output dictionary. At each time, the proposed NMT model predicts either a target word or the special token which indicates a request for the correct word from human. Furthermore, our NMT model is trained towards two objectives, i.e., improving translation quality and reducing human involvement required. Since there is no explicit signals on the decisions of requesting human guidance in the bilingual corpus, we employ the reinforcement learning (RL) technique to optimize the joint model which is able to predict when to request human guidance. Unfortunately, the reward includes two objectives which are somehow contradictory with each other: a higher translation quality often requires more human guidance, the standard RL learning algorithm (Bahdanau et al. 2017) produced highly unstable results and was found not applicable in our preliminary experiments. To put RL into practice in our scenario, we additionally propose simple yet effective techniques into the standard RL training algorithm. Simulated and real experiments demonstrate that our proposed approach obtains higher BLEU while using less human involvement compared to the confidence based approach.\n\nThis paper makes the following contributions: \u2022 It proposes a joint interactive NMT model for the paradigm which takes historical human involvement into account in architecture and is optimized to human involvement besides translation quality. \u2022 It studies the behavior of reinforcement learning on the setting where its reward balances two contradictory objectives and highlights two important techniques in making RL successful. \u2022 The proposed approach obtains better balance between translation quality and human involvement on both simulated and real experiments than the confidence based approach.\n\n\nPreliminaries NMT Model\n\nWe consider the problem of learning to generate the output sentence Y = y 0 , \u00b7 \u00b7 \u00b7 , y |Y|\u22121 with length T being the target language based on an input X = x 0 , \u00b7 \u00b7 \u00b7 , x |X|\u22121 in the source language. Typical methods are based on the encoder-decoder framework with attention mechanism. The encoder first maps the source sentence X into a set of representations by mixing the information in different tokens. Then, at each decoding step t, the next output token is predicted according to P (y t | Y <t , X). Y <t = y 0 , \u00b7 \u00b7 \u00b7 , y t\u22121 is the hypothesis generated by the model, containing output tokens at each time-step, and V is the target-side vocabulary containing all candidate output tokens. Models are usually trained in a teacher-forcing manner, based on the maximum likelihood estimation (MLE) loss along the ground-truth reference. Currently, the model showing the highest performance is Transformer, proposed by Vaswani et al. (2017) and thus we employ it as the testbed in our experiments. Different from previous models such as RNN-based (Cho et al. 2014) and Convolution-based (Gehring et al. 2017), it relies purely on the self-attention structure in both encoding and decoding process, which can mix the features in different time-step positions more efficiently. Our methods utilized this model as the backbone structure.\n\n\nConfidence-based Baseline INMT\n\nIn this paper, we follow the conventional interactive machine translation to set the baseline which generates a translation from the the left-to-right (Foster, Isabelle, and Plamondon 1997). Conventional interactive machine translation requires human translators to validate all words in the prefix and then decide to correct a word accordingly. To further reduce human involvement, confidence based approaches have been proposed which focus on some of those words with low confidence and request human translator to type the correct words (Ueffing and Ney 2005;Gonz\u00e1lez-Rubio, Ortiz-Mart\u00ednez, and Casacuberta 2010b;Cheng et al. 2016).\n\nWe develop our confidence baseline on top of the advanced Transformer. Similar to (Knowles and Koehn 2018), the generative story of our confidence based IMT can be described as follows. Suppose Y <t has already been generated, and the next token is obtained through two steps:\n\n1. Generate a token y t by the NMT model; 2. Reset the token y t via an oracle O (for example, a human translator) if the P (y t | Y <t , X) \u2264 \u03b7, where \u03b7 is a predefined threshold to control the frequency of requesting human guidance.\n\nUnfortunately, there are at least two pitfalls in the baseline which might lead to limited performance. On one hand, the NMT model is insensitive to the human guidance in the prefix Y <t , i.e. it completely ignores which tokens in Y <t have been modified by the oracle O. On the other hand, the training objective of the model is the standard MLE, which does not take the human guidance into account.\n\n\nProposed INMT Methodology\n\nIn this section, we illustrate how we embed each query of human guidance as a special token in the output dictionary, and accordingly design our INMT model which takes historical queries of human guidance into account.\n\n\nJoint INMT Model\n\nTo account human guidance in the architecture, we propose an improved INMT model which models it explicitly. The improved model is on top of the oracle O besides X and jointly predicts both translation and interaction. Specifically, we append a special token \" orc \" to the target vocabulary: V = V \u222a { orc }. The joint model parameterized by \u03b8 is defined as follows:\nP (Y | X, O; \u03b8) = t P y t | Y <t , O(Y <t ), X; \u03b8 (1)\nwhere y t can be a real word in the vocabulary V or the special token \" orc \", and O(Y <t ) denotes the token sequence which is obtained by the oracle after specifying all \" orc \" in Y <t in the incremental manner to facilitate the left-to-right decoding strategy. For example, in Table 1, Y <3 =\"He is orc \" and O(Y ) <3 =\" He is my \" . By introducing the special token, our model can jointly translate and predict when to query for human guidance: if the predicted y t is in V, and our model does not to query the oracle, otherwise it needs the query from the oracle O.\n\nTo further define the network architecture of our model in Eq. (1), we mainly employ the same networks as in Transformer (Vaswani et al. 2017), to encode X into the representation vectors using self attention but with two significant differences. Firstly, since Y <t may include \" orc \" for multiple times, it is inferior to predict y t by directly feeding Y <t into the model compared to standard transformer, and Notation Value Y\n\nHe is orc colleague orc whom I traveled for orc three weeks . O(Y ) He is my colleague with whom I traveled for about three weeks . thus we feed O(Y <t ) into the decoder instead. In addition, to take the human guidance in the history into account, we represent Y <t with a binary sequence to indicate whether y i = orc or not for all i < t, and then we employ the technique to encode this binary sequence similar to position embedding in (Gehring et al. 2017). For example, in Table 1, Y <5 = 'He is orc colleague orc \", then O(Y <4 ) =\"He is my colleague with\"0 0 1 0 1\", both of which are actually encoded in our network to predict y 5 .\n\n\nProposed INMT Protocol\n\nThe generative story of our INMT for translation is achieved by the following two steps:\n\n\u2022 generate a token y t from the joint model defined in Eq.\n\n(1). \u2022 reset y t by requesting the oracle O if y t = orc .\n\nIn our protocol, we predict when to request human involvement by adding a special token in the target dictionary, and thus enlarge the output dimensions of the actor by 1 accordingly. our protocol advantages can be summarized: 1) it only introduces three extra embedding parameters (one for the special tokens and two for binary code) compared to the standard transformer; 2) takes the cost of human involvement from the history into account.\n\n\nTraining via Reinforcement Learning\n\nIn the confidence based INMT baseline, its translation model is trained only for translation quality and thus is insensitive to the cost of human involvement. In this section, we thereby propose to optimize the joint INMT model towards both objectives (i.e., translation quality and human involvement). Since there is no explicit signals on the decisions of requesting human guidance in the bilingual corpus, we optimize the model with the actor-critic algorithm such that our model is able to predict when to request human guidance. To the best of our knowledge, it is the first time to train translation models towards such both goals for interactive machine translation.\n\nHowever, it is far from trivial to train the INMT model via the actor-critic algorithm, because in our scenario the two goals are contradictory somehow: reducing the cost human involvement typically leads to worse translation quality. In the rest of this section, we will presents the components of our actor-critic algorithm and particularly we will analyze the issues suffered in our preliminary experiments in details and propose effective techniques to address them.\n\n\nPretraining\n\nSuppose we are given a bilingual training corpus { X n , Y n | n = 1, \u00b7 \u00b7 \u00b7 , N}, where Y n is the reference of source sentence X n and N is the size of the corpus. For each bilingual sentence X n , Y n , there is no special token \" orc \" in the reference Y n , and thus we can not train the joint INMT to predict \" orc \" by using the bilingual corpus alone.\n\nTo address this, we propose to train it in a simple knowledge-transfer way. To this end, we firstly train a standard Transformer model on the given bilingual corpus, and then use the trained model to modify the reference sentence Y n to Y n which includes \" orc \". Specifically, we use the trained model to rescore each token y n t in the reference Y n . If the model score of y n t is less than or equal to \u03b7, we set y n t = orc ; otherwise y n t = y n t . This procedure is similar to the baseline INMT protocol except that the former rescores the reference translation Y instead of a generated translation in the latter. Table 1 shows an example for Y , Y, A according to the reference. In this way, we can convert the bilingual corpus including a set of { X n , Y n | n = 1, \u00b7 \u00b7 \u00b7 , N} into the corpus { X n , Y n , Y n | n = 1, \u00b7 \u00b7 \u00b7 , N}. Note that in the converted corpus we memorize Y n such that it can recover \" orc \" in Y ,n by looking up Y n instead of query the oracle O.\n\nFinally we pretrain our critic model over the converted corpus in a teacher-forcing manner by maximize the following objective:\nt log P (y n t | Y n <t , Y n <t , X n ; \u03b8).(2)\nwhere the above model P is defined in Eq. (1) but it is conditioned on Y n <t rather than Y n <t and thus it does not need the oracle during the pretraining. We expect this pretrained model to capture the intrinsic structure in deciding when the human-input is needed, which may provide a reasonable initialization for our RL algorithm.\n\n\nSimulated Oracle and Reward\n\nDuring the training, we have to sample a translation Y for each source bilingual sentence X n , Y n following the generative story presented in our INMT protocol last section. Since it is too costly to employ a human translator as the oracle O, we instead provided a simulated oracle to mimic a human translator. Suppose at the timestep t, all tokens in Y <t are not equal to orc but y t = orc , we simulate O to reset y t as a new token in V such that the following holds:\ny t = arg max y\u2208V pBLEU(Y <t \u2022 y)\nwhere pBLEU denotes the partial BLEU score (Liu and Huang 2014), and Y <t \u2022 y denotes the new prefix which extends Y <t with a token y.\n\nTraditionally, reward R is defined using BLEU score. However, since our goal is to maximize translation quality while minimizing the number of human involvement, we introduce a penalty term to balance the accuracy and human involvement. In our scenario, we implement it in a most intuitive and simple way:\nR(Y , Y n , O) = BLEU(O(Y ), Y n )\u2212\u03bb\u00d7 t \u03b4 orc , y t\nwhere O(Y ) denotes the translation hypothesis where all \" orc \" have been reset by the simulated oracle O, \u03b4(y, y ) returns 1 if y = y or 0 otherwise, and BLEU denotes sentence-wise BLEU+1 as in (Bahdanau et al. 2017), and \u03bb is a hyperparameter to balance the both factors.\n\n\nCritic Model and Its Updating\n\nThe reward can only be computed after the hypothesis sentence is finished, but if these rewards are delayed until the end, the algorithm will degrade, as reported in (Bahdanau et al. 2017;Nguyen, Daum\u00e9, and Boyd-Graber 2017). Therefore, a proper way to estimate the reward for intermediate steps is important for this method to coverage. As directly applying Monte Carlo search often yields high variance and results in instability during training when the search space is large, we use a critic model to approximate the average future reward.\n\nFormally the critic model is defined by\nV Y <t , O(Y <t ), Y n ; \u03c6\nwhere Y is a translation of the source sentence X n and \u03c6 is the parameter of V . V is defined by the network which is almost the same as the joint model defined in Eq. 1 except two differences: its inputs include Y n rather than the source sentence X n ; in addition, its output is not a distribution over a vocabulary but a number which estimates the average future rewards at current time step t. The critic model is trained to approximate the reward of the proposal with respect to ground-truth using minimal square loss as follows:\nt G t (Y , Y n ; \u03c6) 2(3)\nwhere G t is defined by:\nG t (Y , Y n , O; \u03c6) = R(Y , Y n , O) \u2212 V Y <t , O(Y <t , ), Y n ; \u03c6 .\nNote that as critic model is only needed at the training phase, it will not cause a problem that the input contains groundtruth Y n during testing phase.\n\n\nUpdating Actor Model\n\nWith critic model \u03c6 fixed, the actor model \u03b8 (i.e. our joint NMT model in Eq.(1)) can be updated following the standard advantage policy-gradient criteria (Sutton, Barto, and others 1998). However, since training INMT in our scenario is much difficult than training automatic NMT due to the two contradictory goals, we found that the standard actor-critic algorithm in (Bahdanau et al. 2017;Wu et al. 2018) fails in our preliminary experiments and we observed two severe issues leading to its failure. The first issue is that our actor model suffers from socalled 'catastrophic forgetting': the change of the actor model after RL training begins is drastic, and consequently it soon loses the ability to give credential predictions in the following batches. This contributes to its ineffectiveness in exploring, and the actor will gradually degrade and get lower performance. To address this issue, we introduce two MLE auxiliary objectives and update actor model \u03b8 via the regularized policy gradient:\n\u2207 \u03b8 log P y t | Y <t , O(Y <t ), X n ; \u03b8 G t (Y , Y n ; \u03c6) + \u03bb 1 t log P (y n t | Y n <t , Y n <t , X n ; \u03b8) + \u03bb 2 t log P (y n t | Y n <t , Y n <t , X n ; \u03b8) . (4)\nwhere P parameterized by \u03b8 is defined in Eq. (1), and X n , Y n , Y n is the converted data from X, Y during pretraining. The loss corresponding to \u03bb 1 can guide the actor to produce correct tokens and the loss The loss corresponding to \u03bb 1 can guide the query for human's guidance at proper juncture respectively, making the updated \u03b8 more meaningful in exploration.\n\nThe second issue is that during training the actor model tends to change drastically after the RL training begins and thus the critic model V will soon be outdated, and incapable of giving accurate estimation of future rewards. To address this, we adopt a strategy of asynchronous updates between critic and actor models: we update critic model for three times while updating actor model once. This simple method shows a much more stable training trajectory. We attribute it to the reason that the value model can see more samples in the second way, and the variance introduced by sampling is alleviated.\n\nThe full training algorithm for our model is shown in Algorithm 1, in which the inputs are a threshold, two auxiliary weights and a bilingual dataset and the output is the actor model \u03b8 used for inference.\n\n\nExperiment\n\nWe conduct both simulated experiments and real experiments. For the simulated experiments, we employ the bilingual datasets from IWSLT14 German-English (De-En), IWSLT14&15 Chinese-English (Zh-En), and IWSLT17 French-English (Fr-En); while for real experiments, we only use the De-En dataset. Note that in the simulated experiments, we use the simulated oracle with partial BLEU as described in previous section to mimic human translators while in the real experiments, we use the human translators as the oracle.\n\n\nSetup\n\nData and Preprocessing For the IWSLT14 De-En dataset, we follow the procedures in (Ott et al. 2019) and use BPE (Sennrich, Haddow, and Birch 2016) to process the\n\n\nAlgorithm 1 RL for Proposed INMT Model\n\n\nRequire:\n\n{ X n , Y n | n = 1, \u00b7 \u00b7 \u00b7 , N}. 1: Initialize the actor model \u03b8 by pretraining; 2: Set asynchronous update variable \u03b1 = 0; 3: while Not Converged do 4: \u03b1 = \u03b1 + 1;\n\n\n5:\n\nReceive a random converted example X n ,Y n , Y n ; 6:\n\nSample Y for X n from the actor model \u03b8; Update the actor model \u03b8 by one-step gradient according to Eq. (4); 10: end if 11: end while source and target sentences. Data is tokenized and cleaned using Moses toolkit (Koehn et al. 2007). Similar steps are performed for IWSLT17 Fr-En. For the Zh-En dataset, we mainly adopt the same data-split method as Nguyen, Daum\u00e9, and Boyd-Graber (2017) and Li et al. (2019), except that we use both their 'Supervised training' and 'Bandit training' sets as the training set. We use the Stanford Chinese word segmenter (Chang, Galley, and Manning 2008) to segment Chinese sentences. Besides, BPE is also adopted for both source and target sentences.\n\nBaseline Since our aim is to demonstrate that our joint model with RL training is able to obtain better translation with less human involvement than the confidence based approach, we implement both approaches on top of the same NMT framework and employ the left-to-right interactive decoding strategy. The confidence baseline INMT protocol is the one we proposed in Section 2, which is directly based on the standard transformer. In addition, to further show the effectiveness our RL training, we use the pretrained joint model as our second baseline, which learns when to interact in a knowledge-transfer way, as defined in Section 4.\n\nModel Configuration Both the NMT model and the critic model are built upon 'transformer iwslt de en' setting as defined in (Ott et al. 2019) for all three datasets, which is recommended for IWSLT datasets. Concretely, we use six self-attention layers for both encoding and decoding, and the embedding dimension is set to 512. For training our model, the BLEU score in our reward is the sentence-wise BLEU+1 which is a common practice when sentence-level BLEU is considered (Bahdanau et al. 2017). We train our models by the Adam Optimizer (Kingma and Ba 2015) with \u03b2 1 = 0.9, \u03b2 2 = 0.999 and set the maximum tokens in a batch to 4000. For the pretraining process, we adopt a warm-up of 4000 steps and set the initial learning rate to 0.0003. Before the RL training begins, the actor model is initialized with parameters of the pretrained model, and the reward model is pretrained with learning rate 0.005 for one epoch. Then, we continue the training with learning rate 0.0001 for the ac-tor and 0.0005 for the reward model. The scale \u03bb of the query penalty in the reward function is set to 0.015(we found the improvement to be stable when it is between 0.015 and 0.02). And weight \u03bb 1 , \u03bb 2 of two auxiliary losses are initialized to 0.1 at the beginning and lowered by approximately 0.6 after each epoch.\n\n\nEvaluation\n\nThe comparison between our approach and baseline approaches is not straightforward, as it is in essence a multi-goal task and particularly our two goals are contradictory. A quick idea is to take several different interpolation coefficients and evaluate against the interpolated goal, but this idea is not applicable because our approach can directly optimize towards it by casting this interpolation coefficient as \u03bb in the reward, making the comparison unfair. Inspired by Duh et al. (2012), we employ the following criterion to conclude that one approach A is better than the approach B if and only if the approach A achieves higher BLEU while using less human involvement than the approach B. Since the baselines and our approach adopt the different ways to control the level of human involvement, it is not easy to maintain the similar human involvement for three different approaches. Therefore, in the simulated experiments, we independently running all three approaches with different hyperparameters and draw a piece-wise linear curve similar to ROC curves (Bradley 1997) for each approach in terms of both goals, then we validate whether the criterion is satisfied by picking points from the curve 1 . To reduce the cost in the real experiments, we pick one setting for the baseline and one setting for our approach from the simulated experiments, and then validate whether the criterion is satisfied during the real human-computer interaction.\n\n\nResults and Analysis\n\nSimulated Scenario The performance of each approach on three datasets are shown in Figure 1. It can be seen that in IWSLT14 De-En, our RL-based method shows an improvement of average 2 BLEU points over the baselines when using the same amount of human guidance. The improvement is more significant when query frequency is below 18 percent, with a gap of over 3 points. When the human effort is involved in over 22 percent of tokens, the gap drops a little, becoming 1.5. This result shows that despite the baseline approaches are already 11.5 BLEU points better than the traditional transformer when having an average guidance frequency of 0.16, their interaction policies are still sub-optimal. Our approach, which uses RL to directly trade off between human effort and translation quality, can outperform them with a large margin. Similar results can be found in Zh-En and Fr-En datasets, with an average improvement of 1.1 and 1.7 BLEU scores under the same query frequency respectively, as also shown in Figure 1. Figure 1 has shown the gains in directly learning when to query for human guidance. To better understand the learning process, we also provide an example of the curves over  BLEU scores and query times during training in Figure 2.\n\nWe can see clearly that the change of model's behavior in the first several epochs is drastic. The model tends to optimize towards relying more on human guidance in the beginning, and the translation quality soars rapidly. After arriving at certain point, it will stop this tendency, and seek to reduce the human efforts while keeping a high translation quality.  Real Scenario To validate whether our RL-based method can really reduce the amount of human involvement, we also perform an experiment to test its usage in the real human-computer interaction scenario. Since it is costly to conduct IMT experiments for all test sentences on the real scenario, we sample 400 samples from IWSLT14 De-En and then ask two human translators to conduct the human-computer translation. Each human translator has to interact with two INMT systems 2 (i.e. confidence-based and our RL-based) to translate one source sentence twice. To make the comparison fairer, when translating one source sentence we supply both systems to each translator in a random order such that the translator does not perceive which system it is. We record the BLEU scores of translation and query frequencies for each human translator, and find that the result from one translator correlate well with that from the other. Then we average both the BLEU scores and query frequencies as our final result, which is shown in Table 2. It could be seen that our RL approach can get higher BLEU score with similar human involvement.\n\n\nAnalysis on Queried Tokens\n\nWe try to look into what sorts of tokens are queried more frequently. We split all words into two sets, frequent tokens and infrequent tokens, and show the policy over several samples in Table 3 and 4. Here, we use recall to measure the ability of each model to successfully predict the ground-truth token, i.e. the possibility that this token is generated by a translation model during decoding. Here, TFM refers to standard transformer model without human intervention. First, it can be seen that the policy is closely related to the effectiveness of the translation model. For tokens that a traditional transformer can translate well, the query frequency is low. While for those which has a low recall by TFM, the query frequency becomes high. This further illustrates the efficiency of our interaction approach.\n\nSecond, it is shown in Table 3 that the model is more uncertain over its output when the ground-truth of that token does not have a specific meaning. It is not because they are under-represented in the dataset, but because they can be used in many conditions and have complex usages. On   \n\n\nRelated Work\n\nInteractive-predictive MT reaches back to early IBMtype (Foster, Isabelle, and Plamondon 1997) and phrasebased MT (Barrachina et al. 2009;Green et al. 2014). These methods seek to reduce human's effort in correcting model's translation by making human and machine collaborate on a joint iterative strategy. Advances have been introduced by utilizing a richer variety forms of feed-backs, like using judgements on the quality of partial translation results Lam, Kreutzer, and Riezler (2018), using guidance on the segments instead of on the prefix Peris, Domingo, and Casacuberta (2017), using human's correction of the leftmost wrong word Azadi and Khadivi (2015), etc. Another track of works dedicates to exploit more information from the feed-back. For example, Koehn (2009) propose to suggest more than one suffix for users to validate, Peris and Casacuberta (2018b) adopts online learning techniques to improve the system with the user feedback, and Peris and Casacuberta (2018a) uses active learning to choose the sentences that can gain more knowledge from users. However, few of these works address the problem of 'when to query'. In active learning domain, there are a few works seeking to evaluate the model's uncertainty towards its translation result. For example, Gonz\u00e1lez-Rubio, Ortiz-Mart\u00ednez, and Casacuberta (2012) uses the distribution of confidence score, and Peris and Casacuberta (2018a) propose to use the attention coverage and distraction. However, their methods are all based on heuristic criterion, instead of directly balancing the gain and penalty in obtaining human's guidance. Ibraheem, Altieri, and DeNero (2017) is close to our work in the sense that they also employ reinforcement learning to learn an interaction policy. However, they fix the translation model and only model the human actions as a binary variable over the attention vector.Therefore, their training process is more stable, but also resulting in the gains of only about 12 BLEU points over the standard TFM with 80% query frequency. On the other hand, we train both the translation and human action in a unified model and design a more complex guidance form, leading to a large action space and making our approach more difficult than theirs. But, the benefit of our work is that it can obtain more BLEU improvement with much less human involvement (with query frequency about 20%).\n\n\nConclusion and Future Work\n\nConfidence-based interactive machine translation is effective to reduce the human involvement because it only requires human translators to correct few of those words with low confidence while avoiding the validation for other words. In this paper, we propose a novel approach to IMT which does not require human translators to validate all output words but only focus on some words. Our approach relies on a novel neural model which considers human involvement in its architecture and is optimized towards both translation quality and the cost of human involvement via reinforcement learning. Simulated and real experiments show that the proposed approach outperforms the confidence baseline with a margin in translation quality by using similar or less human involvement. Since the training efficiency is relatively low compared to that of the standard NMT model, in the future, we plan to accelerate our approach and then apply it to large scale translation tasks.\n\nFigure 1 :\n1Comparison of different approaches. The x-axis refers to the frequency of making queries, with 1 meaning guidance is required at every time-step. The y-axis is the BLEU score.\n\nFigure 2 :\n2Progresses of the BLEU scores (upper) and the query times (lower) during training.\n\nTable 1 :\n1The examples. Y denotes a translation from our joint model, in which \" orc \" indicating that this token should be corrected by an oracle O. O(Y ) denotes the corrected translation by O.\n\nTable 2 :\n2Comparison of different approaches in real IMT scenario, where human translators serve as the oracle.\n\n\nrecall by TFM 0.986 0.912 0.907 0.697 0.817 0.237 0.190 0.254 0.155 0.220 recall by Ours 0.986 0.946 0.898 0.786 0.872 0.502 0.479 0.436 0.409 0.485Token \n\nthank but \nago \ntell \nsaid \nup \ndown out \n@@ \ning \nQuery freq \n0.006 0.046 0.054 0.057 0.060 0.437 0.444 0.468 0.468 0.469 \n\n\nTable 3 :\n3Examples of the interaction policy over frequent tokens.Token \ngulf mechanic empathy constant reward eland tives ayer escent tish \nQuery freq \n0 \n0 \n0 \n0 \n0 \n1 \n1 \n1 \n1 \n1 \nrecall by TFM 1 \n0.67 \n1 \n0 \n0.67 \n0 \n0 \n0 \n0 \n0 \nrecall by Ours 1 \n0.67 \n1 \n0 \n0.67 \n0.5 0.5 0.5 \n1 \n1 \n\n\n\nTable 4 :\n4Examples of the interaction policy over infrequent tokens. the contrary, for tokens have less ambiguity, the model will safely believe its prediction. For infrequent tokens, as shown in table 4, similar behavior can be observed. The model become confused and does not know what to predict when coming across meaningless tokens generated by BPE. For tokens with a clearer meaning, although they are also rare during training, the model can successfully predict them by itself.\nFor both baselines, the variant hyperparameter was \u03b7 taken from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7}. For our approach, we initialized it with different pretrained models.\nSince the pretrained model is comparable to the confidencebased baseline, and thus we only compare with the latter in the real scenario.\nAcknowledgmentsWe would like to thank all the anonymous reviewers for their\nImproved search strategy for interactive predictions in computer-assisted translation. F Azadi, S Khadivi, Proceedings of MT Summit. MT SummitAzadi, F., and Khadivi, S. 2015. Improved search strategy for inter- active predictions in computer-assisted translation. In Proceedings of MT Summit, 319-332.\n\nAn actor-critic algorithm for sequence prediction. D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe, J Pineau, A C Courville, Y Bengio, CoRR abs/1607.07086Bahdanau, D.; Brakel, P.; Xu, K.; Goyal, A.; Lowe, R.; Pineau, J.; Courville, A. C.; and Bengio, Y. 2017. An actor-critic algorithm for sequence prediction. CoRR abs/1607.07086.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, CoRR abs/1409.0473Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural ma- chine translation by jointly learning to align and translate. CoRR abs/1409.0473.\n\nStatistical approaches to computer-assisted translation. S Barrachina, O Bender, F Casacuberta, J Civera, E Cubel, S Khadivi, A L Lagarda, H Ney, J Tom\u00e1s, E Vidal, J M Vilar, Computational Linguistics. 35Barrachina, S.; Bender, O.; Casacuberta, F.; Civera, J.; Cubel, E.; Khadivi, S.; Lagarda, A. L.; Ney, H.; Tom\u00e1s, J.; Vidal, E.; and Vilar, J. M. 2009. Statistical approaches to computer-assisted translation. Computational Linguistics 35:3-28.\n\nConfidence estimation for machine translation. J Blatz, E Fitzgerald, G F Foster, S Gandrabur, C Goutte, A Kulesza, A Sanch\u00eds, N Ueffing, Proceedings of COLING. COLINGBlatz, J.; Fitzgerald, E.; Foster, G. F.; Gandrabur, S.; Goutte, C.; Kulesza, A.; Sanch\u00eds, A.; and Ueffing, N. 2004. Confidence esti- mation for machine translation. In Proceedings of COLING.\n\nThe use of the area under the roc curve in the evaluation of machine learning algorithms. A P Bradley, Pattern Recognition. 30Bradley, A. P. 1997. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern Recognition 30:1145-1159.\n\nOptimizing chinese word segmentation for machine translation performance. P.-C Chang, M Galley, C D Manning, WMT@ACL. Chang, P.-C.; Galley, M.; and Manning, C. D. 2008. Optimizing chinese word segmentation for machine translation performance. In WMT@ACL.\n\nPrimt: A pick-revise framework for interactive machine translation. S Cheng, S Huang, H Chen, X.-Y Dai, J Chen, Proceedings of NAACL-HLT. NAACL-HLTCheng, S.; Huang, S.; Chen, H.; Dai, X.-Y.; and Chen, J. 2016. Primt: A pick-revise framework for interactive machine translation. In Proceedings of NAACL-HLT, 1240-1249.\n\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, EMNLP. Cho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP.\n\nLearning to translate with multiple objectives. K Duh, K Sudoh, X Wu, H Tsukada, M Nagata, ACL. Duh, K.; Sudoh, K.; Wu, X.; Tsukada, H.; and Nagata, M. 2012. Learning to translate with multiple objectives. In ACL.\n\nTarget-text mediated interactive machine translation. G F Foster, P Isabelle, P Plamondon, Machine Translation. 12Foster, G. F.; Isabelle, P.; and Plamondon, P. 1997. Target-text mediated interactive machine translation. Machine Translation 12:175-194.\n\nConvolutional sequence to sequence learning. J Gehring, M Auli, D Grangier, D Yarats, Y Dauphin, ICML. Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin, Y. 2017. Convolutional sequence to sequence learning. In ICML.\n\nBalancing user effort and translation error in interactive machine translation via confidence measures. J Gonz\u00e1lez-Rubio, D Ortiz-Mart\u00ednez, F Casacuberta, Proceedings of the ACL. the ACLGonz\u00e1lez-Rubio, J.; Ortiz-Mart\u00ednez, D.; and Casacuberta, F. 2010a. Balancing user effort and translation error in interactive machine translation via confidence measures. In Proceedings of the ACL, 173-177.\n\nOn the use of confidence measures within an interactivepredictive machine translation system. J Gonz\u00e1lez-Rubio, D Ortiz-Mart\u00ednez, F Casacuberta, Proc. EAMT. EAMTGonz\u00e1lez-Rubio, J.; Ortiz-Mart\u00ednez, D.; and Casacuberta, F. 2010b. On the use of confidence measures within an interactive- predictive machine translation system. In Proc. EAMT.\n\nActive learning for interactive machine translation. J Gonz\u00e1lez-Rubio, D Ortiz-Mart\u00ednez, F Casacuberta, EACL. Gonz\u00e1lez-Rubio, J.; Ortiz-Mart\u00ednez, D.; and Casacuberta, F. 2012. Active learning for interactive machine translation. In EACL.\n\nHuman effort and machine learnability in computer aided translation. S Green, S I Wang, J Chuang, J Heer, S Schuster, C D Manning, EMNLP. Green, S.; Wang, S. I.; Chuang, J.; Heer, J.; Schuster, S.; and Man- ning, C. D. 2014. Human effort and machine learnability in com- puter aided translation. In EMNLP.\n\nLearning an interactive attention policy for neural machine translation. S Ibraheem, N D Altieri, J Denero, Proceedings of MT Summit. MT SummitIbraheem, S.; Altieri, N. D.; and DeNero, J. 2017. Learning an interactive attention policy for neural machine translation. In Pro- ceedings of MT Summit.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, CoRR abs/1412.6980Kingma, D. P., and Ba, J. 2015. Adam: A method for stochastic optimization. CoRR abs/1412.6980.\n\nNeural interactive translation prediction. R Knowles, P Koehn, Proceedings of AMTA. AMTAKnowles, R., and Koehn, P. 2016. Neural interactive translation prediction. In Proceedings of AMTA, 107-120.\n\nLightweight word-level confidence estimation for neural interactive translation prediction. R Knowles, P Koehn, Proceedings of AMTA 2018 Workshop. AMTA 2018 WorkshopKnowles, R., and Koehn, P. 2018. Lightweight word-level con- fidence estimation for neural interactive translation prediction. In Proceedings of AMTA 2018 Workshop, 35-40.\n\nMoses: Open source toolkit for statistical machine translation. P Koehn, H Hoang, A Birch, C Callison-Burch, M Federico, N Bertoldi, B Cowan, W Shen, C Moran, R Zens, C Dyer, O Bojar, A Constantin, E Herbst, ACL. Koehn, P.; Hoang, H.; Birch, A.; Callison-Burch, C.; Federico, M.; Bertoldi, N.; Cowan, B.; Shen, W.; Moran, C.; Zens, R.; Dyer, C.; Bojar, O.; Constantin, A.; and Herbst, E. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.\n\nA process study of computer-aided translation. P Koehn, Machine Translation. 23Koehn, P. 2009. A process study of computer-aided translation. Machine Translation 23:241-263.\n\nA reinforcement learning approach to interactive-predictive neural machine translation. T K Lam, J Kreutzer, S Riezler, CoRR abs/1805.01553Lam, T. K.; Kreutzer, J.; and Riezler, S. 2018. A reinforcement learning approach to interactive-predictive neural machine transla- tion. CoRR abs/1805.01553.\n\nTranstype: a computer-aided translation typing system. P Langlais, G Foster, G Lapalme, ANLP-NAACL 2000 Workshop: Embedded Machine Translation Systems. Langlais, P.; Foster, G.; and Lapalme, G. 2000. Transtype: a computer-aided translation typing system. In ANLP-NAACL 2000 Workshop: Embedded Machine Translation Systems.\n\nUnderstanding data augmentation in neural machine translation: Two perspectives towards generalization. G Li, L Liu, G Huang, C Zhu, T Zhao, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLPLi, G.; Liu, L.; Huang, G.; Zhu, C.; and Zhao, T. 2019. Understand- ing data augmentation in neural machine translation: Two perspec- tives towards generalization. In Proceedings of EMNLP-IJCNLP.\n\nSearch-aware tuning for machine translation. L Liu, L Huang, Proceedings of EMNLP. EMNLPLiu, L., and Huang, L. 2014. Search-aware tuning for machine translation. In Proceedings of EMNLP, 1942-1952.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. K Nguyen, H Daum\u00e9, J L Boyd-Graber, EMNLP. Nguyen, K.; Daum\u00e9, H.; and Boyd-Graber, J. L. 2017. Reinforce- ment learning for bandit neural machine translation with simulated human feedback. In EMNLP.\n\nfairseq: A fast, extensible toolkit for sequence modeling. M Ott, S Edunov, A Baevski, A Fan, S Gross, N Ng, D Grangier, M Auli, Proceedings of NAACL-HLT 2019: Demonstrations. NAACL-HLT 2019: DemonstrationsOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.; Grang- ier, D.; and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demon- strations.\n\nActive learning for interactive neural machine translation of data streams. \u00c1 Peris, F Casacuberta, In CoNLLPeris,\u00c1., and Casacuberta, F. 2018a. Active learning for interactive neural machine translation of data streams. In CoNLL.\n\nOnline learning for effort reduction in interactive neural machine translation. \u00c1 Peris, F Casacuberta, CoRR abs/1802.03594Peris,\u00c1., and Casacuberta, F. 2018b. Online learning for ef- fort reduction in interactive neural machine translation. CoRR abs/1802.03594.\n\nNeural machine translation of rare words with subword units. \u00c1 Peris, M Domingo, F Casacuberta, R Sennrich, B Haddow, A Birch, CoRR abs/1508.07909Computer Speech & Language. 45Interactive neural machine translationPeris,\u00c1.; Domingo, M.; and Casacuberta, F. 2017. Interactive neural machine translation. Computer Speech & Language 45:201- 220. Sennrich, R.; Haddow, B.; and Birch, A. 2016. Neural ma- chine translation of rare words with subword units. CoRR abs/1508.07909.\n\nIntroduction to reinforcement learning. R S Sutton, A G Barto, MIT press Cambridge135Sutton, R. S.; Barto, A. G.; et al. 1998. Introduction to reinforce- ment learning, volume 135. MIT press Cambridge.\n\nApplication of word-level confidence measures in interactive statistical machine translation. N Ueffing, H Ney, Proceedings of EAMT. EAMTUeffing, N., and Ney, H. 2005. Application of word-level con- fidence measures in interactive statistical machine translation. In Proceedings of EAMT, 262-270.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NIPS. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. In NIPS.\n\nA study of reinforcement learning for neural machine translation. L Wu, F Tian, T Qin, J Lai, T.-Y Liu, EMNLP. Wu, L.; Tian, F.; Qin, T.; Lai, J.; and Liu, T.-Y. 2018. A study of reinforcement learning for neural machine translation. In EMNLP.\n\nModels and inference for prefix-constrained machine translation. J Wuebker, S Green, J Denero, S Hasan, M.-T Luong, Proceedings of ACL. ACLWuebker, J.; Green, S.; DeNero, J.; Hasan, S.; and Luong, M.-T. 2016. Models and inference for prefix-constrained machine trans- lation. In Proceedings of ACL, 66-75.\n", "annotations": {"author": "[{\"end\":147,\"start\":108},{\"end\":175,\"start\":148},{\"end\":207,\"start\":176},{\"end\":237,\"start\":208},{\"end\":249,\"start\":238},{\"end\":328,\"start\":250},{\"end\":341,\"start\":329},{\"end\":442,\"start\":342}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":118},{\"end\":157,\"start\":154},{\"end\":189,\"start\":184},{\"end\":219,\"start\":217},{\"end\":248,\"start\":246},{\"end\":262,\"start\":259},{\"end\":340,\"start\":337},{\"end\":353,\"start\":350}]", "author_first_name": "[{\"end\":117,\"start\":108},{\"end\":153,\"start\":148},{\"end\":183,\"start\":176},{\"end\":216,\"start\":208},{\"end\":245,\"start\":238},{\"end\":258,\"start\":250},{\"end\":336,\"start\":329},{\"end\":349,\"start\":342}]", "author_affiliation": "[{\"end\":146,\"start\":124},{\"end\":174,\"start\":159},{\"end\":206,\"start\":191},{\"end\":236,\"start\":221},{\"end\":279,\"start\":264},{\"end\":327,\"start\":281},{\"end\":393,\"start\":378},{\"end\":441,\"start\":395}]", "title": "[{\"end\":105,\"start\":1},{\"end\":547,\"start\":443}]", "venue": null, "abstract": "[{\"end\":2033,\"start\":549}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2160,\"start\":2128},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2179,\"start\":2160},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2369,\"start\":2331},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2404,\"start\":2369},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2538,\"start\":2519},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2558,\"start\":2538},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2581,\"start\":2558},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2618,\"start\":2581},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3121,\"start\":3099},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3210,\"start\":3192},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3233,\"start\":3210},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3349,\"start\":3330},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3403,\"start\":3349},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3435,\"start\":3403},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3705,\"start\":3683},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3723,\"start\":3705},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3797,\"start\":3774},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5135,\"start\":5114},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7111,\"start\":7090},{\"end\":7234,\"start\":7208},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7279,\"start\":7258},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7729,\"start\":7691},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8102,\"start\":8080},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8156,\"start\":8102},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8174,\"start\":8156},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8283,\"start\":8259},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10498,\"start\":10477},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11249,\"start\":11228},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15770,\"start\":15750},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16420,\"start\":16398},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16698,\"start\":16676},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16734,\"start\":16698},{\"end\":18145,\"start\":18113},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18349,\"start\":18327},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18363,\"start\":18349},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20941,\"start\":20925},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21515,\"start\":21497},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21671,\"start\":21634},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21692,\"start\":21676},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21870,\"start\":21837},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22745,\"start\":22729},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23101,\"start\":23079},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23165,\"start\":23145},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24419,\"start\":24402},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25007,\"start\":24993},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29392,\"start\":29354},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29436,\"start\":29412},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29453,\"start\":29436},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29787,\"start\":29754},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29961,\"start\":29937},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30074,\"start\":30062},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30167,\"start\":30138},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30705,\"start\":30676},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30940,\"start\":30904}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":32867,\"start\":32679},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32963,\"start\":32868},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33161,\"start\":32964},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33275,\"start\":33162},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33558,\"start\":33276},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33850,\"start\":33559},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34338,\"start\":33851}]", "paragraph": "[{\"end\":2918,\"start\":2049},{\"end\":4088,\"start\":2920},{\"end\":5536,\"start\":4090},{\"end\":6140,\"start\":5538},{\"end\":7505,\"start\":6168},{\"end\":8175,\"start\":7540},{\"end\":8453,\"start\":8177},{\"end\":8689,\"start\":8455},{\"end\":9092,\"start\":8691},{\"end\":9340,\"start\":9122},{\"end\":9728,\"start\":9361},{\"end\":10354,\"start\":9783},{\"end\":10787,\"start\":10356},{\"end\":11429,\"start\":10789},{\"end\":11544,\"start\":11456},{\"end\":11604,\"start\":11546},{\"end\":11664,\"start\":11606},{\"end\":12108,\"start\":11666},{\"end\":12821,\"start\":12148},{\"end\":13293,\"start\":12823},{\"end\":13667,\"start\":13309},{\"end\":14653,\"start\":13669},{\"end\":14782,\"start\":14655},{\"end\":15167,\"start\":14831},{\"end\":15672,\"start\":15199},{\"end\":15842,\"start\":15707},{\"end\":16149,\"start\":15844},{\"end\":16476,\"start\":16202},{\"end\":17053,\"start\":16510},{\"end\":17094,\"start\":17055},{\"end\":17658,\"start\":17122},{\"end\":17708,\"start\":17684},{\"end\":17933,\"start\":17780},{\"end\":18960,\"start\":17958},{\"end\":19493,\"start\":19126},{\"end\":20099,\"start\":19495},{\"end\":20306,\"start\":20101},{\"end\":20833,\"start\":20321},{\"end\":21004,\"start\":20843},{\"end\":21221,\"start\":21058},{\"end\":21282,\"start\":21228},{\"end\":21967,\"start\":21284},{\"end\":22604,\"start\":21969},{\"end\":23912,\"start\":22606},{\"end\":25381,\"start\":23927},{\"end\":26654,\"start\":25406},{\"end\":28144,\"start\":26656},{\"end\":28990,\"start\":28175},{\"end\":29281,\"start\":28992},{\"end\":31680,\"start\":29298},{\"end\":32678,\"start\":31711}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9782,\"start\":9729},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14830,\"start\":14783},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15706,\"start\":15673},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16201,\"start\":16150},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17121,\"start\":17095},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17683,\"start\":17659},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17779,\"start\":17709},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19125,\"start\":18961}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10071,\"start\":10064},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11274,\"start\":11267},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14300,\"start\":14293},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28047,\"start\":28040},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28369,\"start\":28362},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29022,\"start\":29015}]", "section_header": "[{\"end\":2047,\"start\":2035},{\"end\":6166,\"start\":6143},{\"end\":7538,\"start\":7508},{\"end\":9120,\"start\":9095},{\"end\":9359,\"start\":9343},{\"end\":11454,\"start\":11432},{\"end\":12146,\"start\":12111},{\"end\":13307,\"start\":13296},{\"end\":15197,\"start\":15170},{\"end\":16508,\"start\":16479},{\"end\":17956,\"start\":17936},{\"end\":20319,\"start\":20309},{\"end\":20841,\"start\":20836},{\"end\":21045,\"start\":21007},{\"end\":21056,\"start\":21048},{\"end\":21226,\"start\":21224},{\"end\":23925,\"start\":23915},{\"end\":25404,\"start\":25384},{\"end\":28173,\"start\":28147},{\"end\":29296,\"start\":29284},{\"end\":31709,\"start\":31683},{\"end\":32690,\"start\":32680},{\"end\":32879,\"start\":32869},{\"end\":32974,\"start\":32965},{\"end\":33172,\"start\":33163},{\"end\":33569,\"start\":33560},{\"end\":33861,\"start\":33852}]", "table": "[{\"end\":33558,\"start\":33426},{\"end\":33850,\"start\":33627}]", "figure_caption": "[{\"end\":32867,\"start\":32692},{\"end\":32963,\"start\":32881},{\"end\":33161,\"start\":32976},{\"end\":33275,\"start\":33174},{\"end\":33426,\"start\":33278},{\"end\":33627,\"start\":33571},{\"end\":34338,\"start\":33863}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25497,\"start\":25489},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26422,\"start\":26414},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26432,\"start\":26424},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26653,\"start\":26645}]", "bib_author_first_name": "[{\"end\":34811,\"start\":34810},{\"end\":34820,\"start\":34819},{\"end\":35078,\"start\":35077},{\"end\":35090,\"start\":35089},{\"end\":35100,\"start\":35099},{\"end\":35106,\"start\":35105},{\"end\":35115,\"start\":35114},{\"end\":35123,\"start\":35122},{\"end\":35133,\"start\":35132},{\"end\":35135,\"start\":35134},{\"end\":35148,\"start\":35147},{\"end\":35427,\"start\":35426},{\"end\":35439,\"start\":35438},{\"end\":35446,\"start\":35445},{\"end\":35669,\"start\":35668},{\"end\":35683,\"start\":35682},{\"end\":35693,\"start\":35692},{\"end\":35708,\"start\":35707},{\"end\":35718,\"start\":35717},{\"end\":35727,\"start\":35726},{\"end\":35738,\"start\":35737},{\"end\":35740,\"start\":35739},{\"end\":35751,\"start\":35750},{\"end\":35758,\"start\":35757},{\"end\":35767,\"start\":35766},{\"end\":35776,\"start\":35775},{\"end\":35778,\"start\":35777},{\"end\":36107,\"start\":36106},{\"end\":36116,\"start\":36115},{\"end\":36130,\"start\":36129},{\"end\":36132,\"start\":36131},{\"end\":36142,\"start\":36141},{\"end\":36155,\"start\":36154},{\"end\":36165,\"start\":36164},{\"end\":36176,\"start\":36175},{\"end\":36187,\"start\":36186},{\"end\":36510,\"start\":36509},{\"end\":36512,\"start\":36511},{\"end\":36769,\"start\":36765},{\"end\":36778,\"start\":36777},{\"end\":36788,\"start\":36787},{\"end\":36790,\"start\":36789},{\"end\":37016,\"start\":37015},{\"end\":37025,\"start\":37024},{\"end\":37034,\"start\":37033},{\"end\":37045,\"start\":37041},{\"end\":37052,\"start\":37051},{\"end\":37362,\"start\":37361},{\"end\":37369,\"start\":37368},{\"end\":37388,\"start\":37387},{\"end\":37400,\"start\":37399},{\"end\":37412,\"start\":37411},{\"end\":37424,\"start\":37423},{\"end\":37435,\"start\":37434},{\"end\":37712,\"start\":37711},{\"end\":37719,\"start\":37718},{\"end\":37728,\"start\":37727},{\"end\":37734,\"start\":37733},{\"end\":37745,\"start\":37744},{\"end\":37933,\"start\":37932},{\"end\":37935,\"start\":37934},{\"end\":37945,\"start\":37944},{\"end\":37957,\"start\":37956},{\"end\":38178,\"start\":38177},{\"end\":38189,\"start\":38188},{\"end\":38197,\"start\":38196},{\"end\":38209,\"start\":38208},{\"end\":38219,\"start\":38218},{\"end\":38466,\"start\":38465},{\"end\":38484,\"start\":38483},{\"end\":38502,\"start\":38501},{\"end\":38850,\"start\":38849},{\"end\":38868,\"start\":38867},{\"end\":38886,\"start\":38885},{\"end\":39149,\"start\":39148},{\"end\":39167,\"start\":39166},{\"end\":39185,\"start\":39184},{\"end\":39404,\"start\":39403},{\"end\":39413,\"start\":39412},{\"end\":39415,\"start\":39414},{\"end\":39423,\"start\":39422},{\"end\":39433,\"start\":39432},{\"end\":39441,\"start\":39440},{\"end\":39453,\"start\":39452},{\"end\":39455,\"start\":39454},{\"end\":39715,\"start\":39714},{\"end\":39727,\"start\":39726},{\"end\":39729,\"start\":39728},{\"end\":39740,\"start\":39739},{\"end\":39985,\"start\":39984},{\"end\":39987,\"start\":39986},{\"end\":39997,\"start\":39996},{\"end\":40161,\"start\":40160},{\"end\":40172,\"start\":40171},{\"end\":40408,\"start\":40407},{\"end\":40419,\"start\":40418},{\"end\":40718,\"start\":40717},{\"end\":40727,\"start\":40726},{\"end\":40736,\"start\":40735},{\"end\":40745,\"start\":40744},{\"end\":40763,\"start\":40762},{\"end\":40775,\"start\":40774},{\"end\":40787,\"start\":40786},{\"end\":40796,\"start\":40795},{\"end\":40804,\"start\":40803},{\"end\":40813,\"start\":40812},{\"end\":40821,\"start\":40820},{\"end\":40829,\"start\":40828},{\"end\":40838,\"start\":40837},{\"end\":40852,\"start\":40851},{\"end\":41168,\"start\":41167},{\"end\":41384,\"start\":41383},{\"end\":41386,\"start\":41385},{\"end\":41393,\"start\":41392},{\"end\":41405,\"start\":41404},{\"end\":41650,\"start\":41649},{\"end\":41662,\"start\":41661},{\"end\":41672,\"start\":41671},{\"end\":42022,\"start\":42021},{\"end\":42028,\"start\":42027},{\"end\":42035,\"start\":42034},{\"end\":42044,\"start\":42043},{\"end\":42051,\"start\":42050},{\"end\":42342,\"start\":42341},{\"end\":42349,\"start\":42348},{\"end\":42588,\"start\":42587},{\"end\":42598,\"start\":42597},{\"end\":42607,\"start\":42606},{\"end\":42609,\"start\":42608},{\"end\":42847,\"start\":42846},{\"end\":42854,\"start\":42853},{\"end\":42864,\"start\":42863},{\"end\":42875,\"start\":42874},{\"end\":42882,\"start\":42881},{\"end\":42891,\"start\":42890},{\"end\":42897,\"start\":42896},{\"end\":42909,\"start\":42908},{\"end\":43279,\"start\":43278},{\"end\":43288,\"start\":43287},{\"end\":43515,\"start\":43514},{\"end\":43524,\"start\":43523},{\"end\":43760,\"start\":43759},{\"end\":43769,\"start\":43768},{\"end\":43780,\"start\":43779},{\"end\":43795,\"start\":43794},{\"end\":43807,\"start\":43806},{\"end\":43817,\"start\":43816},{\"end\":44213,\"start\":44212},{\"end\":44215,\"start\":44214},{\"end\":44225,\"start\":44224},{\"end\":44227,\"start\":44226},{\"end\":44470,\"start\":44469},{\"end\":44481,\"start\":44480},{\"end\":44701,\"start\":44700},{\"end\":44712,\"start\":44711},{\"end\":44723,\"start\":44722},{\"end\":44733,\"start\":44732},{\"end\":44746,\"start\":44745},{\"end\":44755,\"start\":44754},{\"end\":44757,\"start\":44756},{\"end\":44766,\"start\":44765},{\"end\":44776,\"start\":44775},{\"end\":45014,\"start\":45013},{\"end\":45020,\"start\":45019},{\"end\":45028,\"start\":45027},{\"end\":45035,\"start\":45034},{\"end\":45045,\"start\":45041},{\"end\":45258,\"start\":45257},{\"end\":45269,\"start\":45268},{\"end\":45278,\"start\":45277},{\"end\":45288,\"start\":45287},{\"end\":45300,\"start\":45296}]", "bib_author_last_name": "[{\"end\":34817,\"start\":34812},{\"end\":34828,\"start\":34821},{\"end\":35087,\"start\":35079},{\"end\":35097,\"start\":35091},{\"end\":35103,\"start\":35101},{\"end\":35112,\"start\":35107},{\"end\":35120,\"start\":35116},{\"end\":35130,\"start\":35124},{\"end\":35145,\"start\":35136},{\"end\":35155,\"start\":35149},{\"end\":35436,\"start\":35428},{\"end\":35443,\"start\":35440},{\"end\":35453,\"start\":35447},{\"end\":35680,\"start\":35670},{\"end\":35690,\"start\":35684},{\"end\":35705,\"start\":35694},{\"end\":35715,\"start\":35709},{\"end\":35724,\"start\":35719},{\"end\":35735,\"start\":35728},{\"end\":35748,\"start\":35741},{\"end\":35755,\"start\":35752},{\"end\":35764,\"start\":35759},{\"end\":35773,\"start\":35768},{\"end\":35784,\"start\":35779},{\"end\":36113,\"start\":36108},{\"end\":36127,\"start\":36117},{\"end\":36139,\"start\":36133},{\"end\":36152,\"start\":36143},{\"end\":36162,\"start\":36156},{\"end\":36173,\"start\":36166},{\"end\":36184,\"start\":36177},{\"end\":36195,\"start\":36188},{\"end\":36520,\"start\":36513},{\"end\":36775,\"start\":36770},{\"end\":36785,\"start\":36779},{\"end\":36798,\"start\":36791},{\"end\":37022,\"start\":37017},{\"end\":37031,\"start\":37026},{\"end\":37039,\"start\":37035},{\"end\":37049,\"start\":37046},{\"end\":37057,\"start\":37053},{\"end\":37366,\"start\":37363},{\"end\":37385,\"start\":37370},{\"end\":37397,\"start\":37389},{\"end\":37409,\"start\":37401},{\"end\":37421,\"start\":37413},{\"end\":37432,\"start\":37425},{\"end\":37442,\"start\":37436},{\"end\":37716,\"start\":37713},{\"end\":37725,\"start\":37720},{\"end\":37731,\"start\":37729},{\"end\":37742,\"start\":37735},{\"end\":37752,\"start\":37746},{\"end\":37942,\"start\":37936},{\"end\":37954,\"start\":37946},{\"end\":37967,\"start\":37958},{\"end\":38186,\"start\":38179},{\"end\":38194,\"start\":38190},{\"end\":38206,\"start\":38198},{\"end\":38216,\"start\":38210},{\"end\":38227,\"start\":38220},{\"end\":38481,\"start\":38467},{\"end\":38499,\"start\":38485},{\"end\":38514,\"start\":38503},{\"end\":38865,\"start\":38851},{\"end\":38883,\"start\":38869},{\"end\":38898,\"start\":38887},{\"end\":39164,\"start\":39150},{\"end\":39182,\"start\":39168},{\"end\":39197,\"start\":39186},{\"end\":39410,\"start\":39405},{\"end\":39420,\"start\":39416},{\"end\":39430,\"start\":39424},{\"end\":39438,\"start\":39434},{\"end\":39450,\"start\":39442},{\"end\":39463,\"start\":39456},{\"end\":39724,\"start\":39716},{\"end\":39737,\"start\":39730},{\"end\":39747,\"start\":39741},{\"end\":39994,\"start\":39988},{\"end\":40000,\"start\":39998},{\"end\":40169,\"start\":40162},{\"end\":40178,\"start\":40173},{\"end\":40416,\"start\":40409},{\"end\":40425,\"start\":40420},{\"end\":40724,\"start\":40719},{\"end\":40733,\"start\":40728},{\"end\":40742,\"start\":40737},{\"end\":40760,\"start\":40746},{\"end\":40772,\"start\":40764},{\"end\":40784,\"start\":40776},{\"end\":40793,\"start\":40788},{\"end\":40801,\"start\":40797},{\"end\":40810,\"start\":40805},{\"end\":40818,\"start\":40814},{\"end\":40826,\"start\":40822},{\"end\":40835,\"start\":40830},{\"end\":40849,\"start\":40839},{\"end\":40859,\"start\":40853},{\"end\":41174,\"start\":41169},{\"end\":41390,\"start\":41387},{\"end\":41402,\"start\":41394},{\"end\":41413,\"start\":41406},{\"end\":41659,\"start\":41651},{\"end\":41669,\"start\":41663},{\"end\":41680,\"start\":41673},{\"end\":42025,\"start\":42023},{\"end\":42032,\"start\":42029},{\"end\":42041,\"start\":42036},{\"end\":42048,\"start\":42045},{\"end\":42056,\"start\":42052},{\"end\":42346,\"start\":42343},{\"end\":42355,\"start\":42350},{\"end\":42595,\"start\":42589},{\"end\":42604,\"start\":42599},{\"end\":42621,\"start\":42610},{\"end\":42851,\"start\":42848},{\"end\":42861,\"start\":42855},{\"end\":42872,\"start\":42865},{\"end\":42879,\"start\":42876},{\"end\":42888,\"start\":42883},{\"end\":42894,\"start\":42892},{\"end\":42906,\"start\":42898},{\"end\":42914,\"start\":42910},{\"end\":43285,\"start\":43280},{\"end\":43300,\"start\":43289},{\"end\":43521,\"start\":43516},{\"end\":43536,\"start\":43525},{\"end\":43766,\"start\":43761},{\"end\":43777,\"start\":43770},{\"end\":43792,\"start\":43781},{\"end\":43804,\"start\":43796},{\"end\":43814,\"start\":43808},{\"end\":43823,\"start\":43818},{\"end\":44222,\"start\":44216},{\"end\":44233,\"start\":44228},{\"end\":44478,\"start\":44471},{\"end\":44485,\"start\":44482},{\"end\":44709,\"start\":44702},{\"end\":44720,\"start\":44713},{\"end\":44730,\"start\":44724},{\"end\":44743,\"start\":44734},{\"end\":44752,\"start\":44747},{\"end\":44763,\"start\":44758},{\"end\":44773,\"start\":44767},{\"end\":44787,\"start\":44777},{\"end\":45017,\"start\":45015},{\"end\":45025,\"start\":45021},{\"end\":45032,\"start\":45029},{\"end\":45039,\"start\":45036},{\"end\":45049,\"start\":45046},{\"end\":45266,\"start\":45259},{\"end\":45275,\"start\":45270},{\"end\":45285,\"start\":45279},{\"end\":45294,\"start\":45289},{\"end\":45306,\"start\":45301}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13478881},\"end\":35024,\"start\":34723},{\"attributes\":{\"doi\":\"CoRR abs/1607.07086\",\"id\":\"b1\"},\"end\":35353,\"start\":35026},{\"attributes\":{\"doi\":\"CoRR abs/1409.0473\",\"id\":\"b2\"},\"end\":35609,\"start\":35355},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":92327},\"end\":36057,\"start\":35611},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":773282},\"end\":36417,\"start\":36059},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13806304},\"end\":36689,\"start\":36419},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6566858},\"end\":36945,\"start\":36691},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7921947},\"end\":37264,\"start\":36947},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5590763},\"end\":37661,\"start\":37266},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2704506},\"end\":37876,\"start\":37663},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2044034},\"end\":38130,\"start\":37878},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3648736},\"end\":38359,\"start\":38132},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15916107},\"end\":38753,\"start\":38361},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":37259675},\"end\":39093,\"start\":38755},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":393084},\"end\":39332,\"start\":39095},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12657045},\"end\":39639,\"start\":39334},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":51898454},\"end\":39938,\"start\":39641},{\"attributes\":{\"doi\":\"CoRR abs/1412.6980\",\"id\":\"b17\"},\"end\":40115,\"start\":39940},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":921404},\"end\":40313,\"start\":40117},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4640543},\"end\":40651,\"start\":40315},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":794019},\"end\":41118,\"start\":40653},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2226896},\"end\":41293,\"start\":41120},{\"attributes\":{\"doi\":\"CoRR abs/1805.01553\",\"id\":\"b22\"},\"end\":41592,\"start\":41295},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1568059},\"end\":41915,\"start\":41594},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202775954},\"end\":42294,\"start\":41917},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7728179},\"end\":42493,\"start\":42296},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":215824512},\"end\":42785,\"start\":42495},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":91184134},\"end\":43200,\"start\":42787},{\"attributes\":{\"id\":\"b28\"},\"end\":43432,\"start\":43202},{\"attributes\":{\"doi\":\"CoRR abs/1802.03594\",\"id\":\"b29\"},\"end\":43696,\"start\":43434},{\"attributes\":{\"doi\":\"CoRR abs/1508.07909\",\"id\":\"b30\",\"matched_paper_id\":1114678},\"end\":44170,\"start\":43698},{\"attributes\":{\"id\":\"b31\"},\"end\":44373,\"start\":44172},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9979533},\"end\":44671,\"start\":44375},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13756489},\"end\":44945,\"start\":44673},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52100616},\"end\":45190,\"start\":44947},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8014052},\"end\":45497,\"start\":45192}]", "bib_title": "[{\"end\":34808,\"start\":34723},{\"end\":35666,\"start\":35611},{\"end\":36104,\"start\":36059},{\"end\":36507,\"start\":36419},{\"end\":36763,\"start\":36691},{\"end\":37013,\"start\":36947},{\"end\":37359,\"start\":37266},{\"end\":37709,\"start\":37663},{\"end\":37930,\"start\":37878},{\"end\":38175,\"start\":38132},{\"end\":38463,\"start\":38361},{\"end\":38847,\"start\":38755},{\"end\":39146,\"start\":39095},{\"end\":39401,\"start\":39334},{\"end\":39712,\"start\":39641},{\"end\":40158,\"start\":40117},{\"end\":40405,\"start\":40315},{\"end\":40715,\"start\":40653},{\"end\":41165,\"start\":41120},{\"end\":41647,\"start\":41594},{\"end\":42019,\"start\":41917},{\"end\":42339,\"start\":42296},{\"end\":42585,\"start\":42495},{\"end\":42844,\"start\":42787},{\"end\":43757,\"start\":43698},{\"end\":44467,\"start\":44375},{\"end\":44698,\"start\":44673},{\"end\":45011,\"start\":44947},{\"end\":45255,\"start\":45192}]", "bib_author": "[{\"end\":34819,\"start\":34810},{\"end\":34830,\"start\":34819},{\"end\":35089,\"start\":35077},{\"end\":35099,\"start\":35089},{\"end\":35105,\"start\":35099},{\"end\":35114,\"start\":35105},{\"end\":35122,\"start\":35114},{\"end\":35132,\"start\":35122},{\"end\":35147,\"start\":35132},{\"end\":35157,\"start\":35147},{\"end\":35438,\"start\":35426},{\"end\":35445,\"start\":35438},{\"end\":35455,\"start\":35445},{\"end\":35682,\"start\":35668},{\"end\":35692,\"start\":35682},{\"end\":35707,\"start\":35692},{\"end\":35717,\"start\":35707},{\"end\":35726,\"start\":35717},{\"end\":35737,\"start\":35726},{\"end\":35750,\"start\":35737},{\"end\":35757,\"start\":35750},{\"end\":35766,\"start\":35757},{\"end\":35775,\"start\":35766},{\"end\":35786,\"start\":35775},{\"end\":36115,\"start\":36106},{\"end\":36129,\"start\":36115},{\"end\":36141,\"start\":36129},{\"end\":36154,\"start\":36141},{\"end\":36164,\"start\":36154},{\"end\":36175,\"start\":36164},{\"end\":36186,\"start\":36175},{\"end\":36197,\"start\":36186},{\"end\":36522,\"start\":36509},{\"end\":36777,\"start\":36765},{\"end\":36787,\"start\":36777},{\"end\":36800,\"start\":36787},{\"end\":37024,\"start\":37015},{\"end\":37033,\"start\":37024},{\"end\":37041,\"start\":37033},{\"end\":37051,\"start\":37041},{\"end\":37059,\"start\":37051},{\"end\":37368,\"start\":37361},{\"end\":37387,\"start\":37368},{\"end\":37399,\"start\":37387},{\"end\":37411,\"start\":37399},{\"end\":37423,\"start\":37411},{\"end\":37434,\"start\":37423},{\"end\":37444,\"start\":37434},{\"end\":37718,\"start\":37711},{\"end\":37727,\"start\":37718},{\"end\":37733,\"start\":37727},{\"end\":37744,\"start\":37733},{\"end\":37754,\"start\":37744},{\"end\":37944,\"start\":37932},{\"end\":37956,\"start\":37944},{\"end\":37969,\"start\":37956},{\"end\":38188,\"start\":38177},{\"end\":38196,\"start\":38188},{\"end\":38208,\"start\":38196},{\"end\":38218,\"start\":38208},{\"end\":38229,\"start\":38218},{\"end\":38483,\"start\":38465},{\"end\":38501,\"start\":38483},{\"end\":38516,\"start\":38501},{\"end\":38867,\"start\":38849},{\"end\":38885,\"start\":38867},{\"end\":38900,\"start\":38885},{\"end\":39166,\"start\":39148},{\"end\":39184,\"start\":39166},{\"end\":39199,\"start\":39184},{\"end\":39412,\"start\":39403},{\"end\":39422,\"start\":39412},{\"end\":39432,\"start\":39422},{\"end\":39440,\"start\":39432},{\"end\":39452,\"start\":39440},{\"end\":39465,\"start\":39452},{\"end\":39726,\"start\":39714},{\"end\":39739,\"start\":39726},{\"end\":39749,\"start\":39739},{\"end\":39996,\"start\":39984},{\"end\":40002,\"start\":39996},{\"end\":40171,\"start\":40160},{\"end\":40180,\"start\":40171},{\"end\":40418,\"start\":40407},{\"end\":40427,\"start\":40418},{\"end\":40726,\"start\":40717},{\"end\":40735,\"start\":40726},{\"end\":40744,\"start\":40735},{\"end\":40762,\"start\":40744},{\"end\":40774,\"start\":40762},{\"end\":40786,\"start\":40774},{\"end\":40795,\"start\":40786},{\"end\":40803,\"start\":40795},{\"end\":40812,\"start\":40803},{\"end\":40820,\"start\":40812},{\"end\":40828,\"start\":40820},{\"end\":40837,\"start\":40828},{\"end\":40851,\"start\":40837},{\"end\":40861,\"start\":40851},{\"end\":41176,\"start\":41167},{\"end\":41392,\"start\":41383},{\"end\":41404,\"start\":41392},{\"end\":41415,\"start\":41404},{\"end\":41661,\"start\":41649},{\"end\":41671,\"start\":41661},{\"end\":41682,\"start\":41671},{\"end\":42027,\"start\":42021},{\"end\":42034,\"start\":42027},{\"end\":42043,\"start\":42034},{\"end\":42050,\"start\":42043},{\"end\":42058,\"start\":42050},{\"end\":42348,\"start\":42341},{\"end\":42357,\"start\":42348},{\"end\":42597,\"start\":42587},{\"end\":42606,\"start\":42597},{\"end\":42623,\"start\":42606},{\"end\":42853,\"start\":42846},{\"end\":42863,\"start\":42853},{\"end\":42874,\"start\":42863},{\"end\":42881,\"start\":42874},{\"end\":42890,\"start\":42881},{\"end\":42896,\"start\":42890},{\"end\":42908,\"start\":42896},{\"end\":42916,\"start\":42908},{\"end\":43287,\"start\":43278},{\"end\":43302,\"start\":43287},{\"end\":43523,\"start\":43514},{\"end\":43538,\"start\":43523},{\"end\":43768,\"start\":43759},{\"end\":43779,\"start\":43768},{\"end\":43794,\"start\":43779},{\"end\":43806,\"start\":43794},{\"end\":43816,\"start\":43806},{\"end\":43825,\"start\":43816},{\"end\":44224,\"start\":44212},{\"end\":44235,\"start\":44224},{\"end\":44480,\"start\":44469},{\"end\":44487,\"start\":44480},{\"end\":44711,\"start\":44700},{\"end\":44722,\"start\":44711},{\"end\":44732,\"start\":44722},{\"end\":44745,\"start\":44732},{\"end\":44754,\"start\":44745},{\"end\":44765,\"start\":44754},{\"end\":44775,\"start\":44765},{\"end\":44789,\"start\":44775},{\"end\":45019,\"start\":45013},{\"end\":45027,\"start\":45019},{\"end\":45034,\"start\":45027},{\"end\":45041,\"start\":45034},{\"end\":45051,\"start\":45041},{\"end\":45268,\"start\":45257},{\"end\":45277,\"start\":45268},{\"end\":45287,\"start\":45277},{\"end\":45296,\"start\":45287},{\"end\":45308,\"start\":45296}]", "bib_venue": "[{\"end\":34865,\"start\":34856},{\"end\":36226,\"start\":36220},{\"end\":37094,\"start\":37085},{\"end\":38547,\"start\":38540},{\"end\":38916,\"start\":38912},{\"end\":39784,\"start\":39775},{\"end\":40205,\"start\":40201},{\"end\":40480,\"start\":40462},{\"end\":42099,\"start\":42087},{\"end\":42384,\"start\":42379},{\"end\":42993,\"start\":42963},{\"end\":44512,\"start\":44508},{\"end\":45331,\"start\":45328},{\"end\":34854,\"start\":34830},{\"end\":35075,\"start\":35026},{\"end\":35424,\"start\":35355},{\"end\":35811,\"start\":35786},{\"end\":36218,\"start\":36197},{\"end\":36541,\"start\":36522},{\"end\":36807,\"start\":36800},{\"end\":37083,\"start\":37059},{\"end\":37449,\"start\":37444},{\"end\":37757,\"start\":37754},{\"end\":37988,\"start\":37969},{\"end\":38233,\"start\":38229},{\"end\":38538,\"start\":38516},{\"end\":38910,\"start\":38900},{\"end\":39203,\"start\":39199},{\"end\":39470,\"start\":39465},{\"end\":39773,\"start\":39749},{\"end\":39982,\"start\":39940},{\"end\":40199,\"start\":40180},{\"end\":40460,\"start\":40427},{\"end\":40864,\"start\":40861},{\"end\":41195,\"start\":41176},{\"end\":41381,\"start\":41295},{\"end\":41744,\"start\":41682},{\"end\":42085,\"start\":42058},{\"end\":42377,\"start\":42357},{\"end\":42628,\"start\":42623},{\"end\":42961,\"start\":42916},{\"end\":43276,\"start\":43202},{\"end\":43512,\"start\":43434},{\"end\":43870,\"start\":43844},{\"end\":44210,\"start\":44172},{\"end\":44506,\"start\":44487},{\"end\":44793,\"start\":44789},{\"end\":45056,\"start\":45051},{\"end\":45326,\"start\":45308}]"}}}, "year": 2023, "month": 12, "day": 17}