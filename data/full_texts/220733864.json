{"id": 220733864, "updated": "2022-01-23 08:18:54.75", "metadata": {"title": "Trajectory Planning of Upper Limb Rehabilitation Robot Based on Human Pose Estimation", "authors": "[{\"middle\":[],\"last\":\"Tao\",\"first\":\"Tangfei\"},{\"middle\":[],\"last\":\"Yang\",\"first\":\"Xingyu\"},{\"middle\":[],\"last\":\"Xu\",\"first\":\"Jiayu\"},{\"middle\":[],\"last\":\"Wang\",\"first\":\"Wei\"},{\"middle\":[],\"last\":\"Zhang\",\"first\":\"Sicong\"},{\"middle\":[],\"last\":\"Li\",\"first\":\"Ming\"},{\"middle\":[],\"last\":\"Xu\",\"first\":\"Guanghua\"}]", "venue": "2020 17th International Conference on Ubiquitous Robots (UR)", "journal": "2020 17th International Conference on Ubiquitous Robots (UR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Stroke has become the second leading cause of death in the world, and timely rehabilitation can effectively help patients recover. At present, with the shortage of rehabilitation doctors, using rehabilitation robots to help patients recover has become a more feasible solution. In order to plan a bionic motion trajectory of an upper limb rehabilitation robot more conveniently, a teaching trajectory planning method was proposed based on human pose estimation in this paper. The teaching trajectories were collected by Kinect's depth camera and human bone joints were tracked using deep neural networks OpenPose. The processed trajectories were verified with modeling simulation and robot motion. The planar trajectories were evaluated using the minimum Jerk principle on bio-imitability, the position determination coefficient is more 0.99, the speed determination coefficient is more than 0.94, and the acceleration determination coefficient is more than 0.88. In the case of block, the recognition success rate has increased by more than 73.4% compared with Kinect's bone binding OpenPose algorithm for human bone joint recognition. The bioimitability of the trajectories planned by this method can conveniently and quickly meet the needs of rehabilitation doctors in hospitals to plan the rehabilitation robot trajectory.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3072456739", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/urai/TaoYX0ZLX20", "doi": "10.1109/ur49135.2020.9144771"}}, "content": {"source": {"pdf_hash": "15d7809ef5b62311a896fea3a091c7e78dd2b0ff", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3d2047f0cb973c8d368a906a7bfb2bf6fbb3fc39", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/15d7809ef5b62311a896fea3a091c7e78dd2b0ff.txt", "contents": "\n\n\n\n\nXi'anChina\n\n\nSchool of Mechanical Engineering\nXi'an Jiaotong University\nXi'anChina\n\n\nState Key Laboratory for Manufacturing Systems Engineering\nXi'an Jiaotong University\nXi'anChina\nUpper limb rehabilitation robotTrajectory planningHuman pose estimationBio-imitability\nStroke has become the second leading cause of death in the world, and timely rehabilitation can effectively help patients recover. At present, with the shortage of rehabilitation doctors, using rehabilitation robots to help patients recover has become a more feasible solution. In order to plan a bionic motion trajectory of an upper limb rehabilitation robot more conveniently, a teaching trajectory planning method was proposed based on human pose estimation in this paper. The teaching trajectories were collected by Kinect's depth camera and human bone joints were tracked using deep neural networks OpenPose. The processed trajectories were verified with modeling simulation and robot motion. The planar trajectories were evaluated using the minimum Jerk principle on bioimitability, the position determination coefficient is more 0.99, the speed determination coefficient is more than 0.94, and the acceleration determination coefficient is more than 0.88. In the case of block, the recognition success rate has increased by more than 73.4% compared with Kinect's bone binding OpenPose algorithm for human bone joint recognition. The bio-imitability of the trajectories planned by this method can conveniently and quickly meet the needs of rehabilitation doctors in hospitals to plan the rehabilitation robot trajectory.\n\nI. INTRODUCTION\n\nAccording to a report released by the world's leading medical journal The Lancet in 2019, stroke was the second largest cause of death globally after ischemic heart disease\uff0c and East Asia has the highest age-standardized incidence of stroke, followed by Eastern Europe [1]. Scientific research shows that the human nervous system has plasticity and can generate compensation and repair after injury. Advanced and scientific rehabilitation treatment is expected to restore the normal life-care ability of stroke patients [2]. Human upper limb disability has a significant impact on a patient's normal self-care ability. Traditional upper limb rehabilitation treatment methods mainly rely on \"one-on-one\" treatment by rehabilitation therapists. This method is a great challenge to the physical strength and experience of the rehabilitation doctors, and it is difficult to ensure the treatment effect. With the development of science and technology, medical experiments have shown that robotics can be applied to the rehabilitation training process, instead of or assisting rehabilitation doctors to perform rehabilitation training on patients, and the rehabilitation effect is satisfactory [3].\n\nAt present, when using rehabilitation robots to assist rehabilitation treatment, researchers mainly plan the motion trajectory in advance to drive the patient's limb movement. Feng Yongfei [4] proposed a new trajectory planning method based on the dual quartic polynomial interpolation method. It could realize the adjustment of each joint maximum velocity during the training on account of patient recovery. C. Loconsole [5] proposed an online trajectory planning method for performing visually guided assisted reaching through a rehabilitation robotic exoskeleton, the L-Exos, in its concave workspace. Piazzi A [6] developed a new approach based on interval analysis to find the global minimum-jerk trajectory of a robot manipulator within a joint space scheme using cubic splines. Szyszkowski [7] used a piecewise cubic spline function to construct the joint trajectories with a given speed profile, and the problem was divided into two parts: geometric trajectory planning and trajectory speed control. Y. Pei [8] introduced a method for lower-limb physical rehabilitation by means of a robot that applies preliminary defined forces to a patient's foot while moving it on a preliminary defined trajectory. The ReoGo upper limb rehabilitation robot developed by Motorika Medical Company uses a three-axis robotic arm to guide the patient to move the upper limb. Based on the mechanical structure formula, the three-dimensional motion trajectory of the patient's upper limb is derived to complete the interaction with the rehabilitation training game specially designed based on clinical experience [9][10]. The MindMotion Pro device developed by the Swiss MindMaze company uses a depth camera to capture the three-dimensional spatial motion trajectory of the highlighted reflective ball worn by the patient, and uses an inertial tracker to detect the rotation and extension of the patient's forearm and wrist [11].\n\nMost of the above methods for trajectory planning of rehabilitation robots require researchers to plan their motion trajectories in advance. In actual rehabilitation treatment, due to the different conditions of patients, different rehabilitation trajectories need to be designed according to the actual situation. Most rehabilitation doctors do not have the ability, so it requires a trajectory planning method that can be easily\n\n\nTrajectory Planning of Upper Limb Rehabilitation Robot Based on Human Pose Estimation\n\nTangfei Tao 1,2 , IEEE member, Xingyu Yang 2 , IEEE student member, Jiayu Xu 2 , Wei Wang 2 , Sicong Zhang 1,2 , IEEE member, Ming Li 1,2 , IEEE member, Guanghua Xu 2,3 , IEEE member mastered by rehabilitation doctors. In this paper, a teaching method was proposed for rehabilitation robot teaching trajectory planning based on human pose recognition, which is innovatively applied to rehabilitation training. First, the rehabilitation doctor designs the corresponding rehabilitation trajectory according to the actual situation of the patient, and demonstrates the motion trajectory in front of the depth camera. The Kinect camera was used to record depth information and color information at the same time, and the human pose recognition algorithm is used to process the obtained video information.\n\nOpenPose, an open source library for real-time multi-person skeletal joint recognition proposed by CMU in 2017, was adpoted to solve the problem that Kinect's built-in bone binding algorithm may not recognize or misrecognize the human pose when the human body is partially occluded. The two-dimensional wrist joint point coordinates obtained by OpenPose was mapped to the depth information map obtained by Kinect to obtain the three-dimensional spatial coordinates of the wrist joint. Hult's two-parameter exponential smoothing method and median filtering method were used to process the obtained continuous trajectory, and the abnormal points were removed to obtain a smooth and stable motion trajectory. Because some faces might be blocked when collecting some trajectories, Kinect's own joint point recognition algorithm was compared with the proposed algorithms for joint point recognition when the face was blocked. It was found that the latter had a much higher success rate than the former. The simulation software was used to build a model of the Bert robot. The model can follow the collected trajectory and then transfer the trajectory into the Burt robot. The robot can lead the participants to follow the preset trajectory, which proves the correctness of the trajectory. The minimum Jerk was used to verify the bio-imitability of the position, velocity and acceleration of the plane trajectories in different directions, which proved that the trajectories collected by this method has good bio-imitability and can help the rehabilitation doctors to plan rehabilitation according to the specific patient's condition.\n\n\nII. METHODOLOGY\n\n\nA. The overall process of the proposed method\n\nThe method to plan the teaching trajectory proposed in this paper is shown in Figure 1. The teaching video is first recorded by Kinect camera, and the video contains both RGB and depth data. The teaching video is processed by the OpenPose framework introduced in the next section to extract the trajectory. A threshold filtering method for spatial trajectory points is proposed to improve the accuracy of the trajectory. Then, simulation experiments and robot experiments are used to verify the feasibility of the trajectory.\n\n\nB. Introduction to OpenPose\n\nIn recent years, researchers have designed many human pose estimation algorithms based on color or depth images. Among them, the human body pose estimation algorithm based on convolutional neural network is the most promising algorithm recently. The OpenPose algorithm [12] proposed by Carnegie Mellon University in 2017 implies a revolution in human pose estimation based on convolutional neural networks. OpenPose is a real-time, C ++ open source library for multi-person skeletal joint recognition. At present, developers can use OpenPose to obtain real-time and highly accurate 2D human skeletal joint point coordinates based on a monocular camera. OpenPose borrows the idea of using a large convolution kernel to obtain a large receptive field in a convolutional attitude machine, so that the OpenPose algorithm can better deal with the problem of human pose estimation in the case of occlusion. A two-branch multi-level convolutional neural network is used in OpenPose to process two-dimensional color images. Each level in the first branch predicts the position of the body's joints, and each level in the second branch is responsible for prediction PAF (part affinity fields). The overall flow of OpenPose is shown in Figure 2. The system takes a color image of size W \u00d7 H (shown in Figure 2. (a)) as input, and generates a two-dimensional image position (shown in Figure 2. (e)) of the key points of each person's bone in the image. First, the feedforward network simultaneously predicts a set of twodimensional confidence maps S (shown in Figure 2. (b)) of body parts and a set of two-dimensional vector fields L (see Figure 2. (c)) of partial affinity fields, where the partial affinity field indicates the degree of association between parts of the body. Finally, through the greedy reasoning (shown in Figure  2. (d)), the confidence map S and some affinity fields L are analyzed to output two-dimensional body joint points for everyone in the image.  \n\n\nC. 3D human pose estimation based on OpenPose and\n\nKinect Two-dimensional human skeleton key points can be obtained through OpenPose. In order to plan the end trajectory of the rehabilitation robot, three-dimensional trajectory points are needed. Therefore, it is necessary to map the twodimensional joint point coordinates obtained by OpenPose to the three-dimensional coordinates obtained by Kinect to obtain the three-dimensional coordinates of human joint points. First, the depth and color cameras of Kinect are automatically calibrated respectively to obtain the internal parameter matrix KRGB of the color camera and the internal parameter matrix KDEPTH of the depth camera as follow, f x , f y are scale factors of the camera in the u-axis and v-axis directions, (c x ,c y ) is the coordinate in the pixel coordinate system:\nK RGB = [ f x_RGB 0 c x_RGB 0 f y_RGB c y_RGB 0 0 1 ] (\uf031) K DEPTH = [ f x_DEPTH 0 c x_DEPTH 0 f y_DEPTH c y_DEPTH 0 0 1 ]\nThen, the transformation relationship between the color camera and the depth camera is calibrated, and the rotation matrix and translation vector of the depth camera coordinate system to the color camera coordinate system are R D-RGB and t D-RGB respectively. The two-dimensional coordinates of the depth image are mapped to threedimensional coordinates in the depth camera coordinate system. Combine (1) to convert the three-dimensional coordinate points (X D ,Y D ,Z D ) in the depth camera coordinate system to the three-dimensional coordinates (X RGB ,Y RGB ,Z RGB ) in the color camera coordinate system:\n[ X RGB Y RGB Z RGB ] =R D-RGB [ X D Y D Z D ] +t D-RGB (\uf033)\nAnd the coordinates (X RGB ,Y RGB ) of the points (X RGB ,Y RGB ,Z RGB ) in the color image coordinate system are:\n1 Z RGB \u00d7 [ x RGB y RGB 1 ] =K RGB \u00d7 [ X RGB Y RGB Z RGB ] (\uf034)\nAfter the mapping of the 2D joint points and the 3D joint points is established, the teaching trajectory video is processed by OpenPose to obtain the 2D information of the hand joint points, and the corresponding 3D trajectory information is obtained through the mapping.\n\n\nD. Track point processing\n\nAfter the mapping from the two-dimensional joint point coordinates to the three-dimensional joint point coordinates is completed, the trajectory points are filtered and smoothed due to the problem of abnormal points and unevenness in the trajectory points.  After removing the outliers, the Hult's two-parameter exponential smoothing method is used to smooth the trajectory of the three-dimensional joint points over time. Hult's twoparameter exponential smoothing method is a linear exponential smoothing method. This method does not use secondary exponential smoothing for time series with trend changes. Instead, it smoothes the trend data directly and predicts the original time series.\n\nHult exponential smoothing method has two basic smoothing formulas and a prediction formula. The two smoothing formulas are performed on two factors of the time series, respectively. They are:\n{ S t =\u03b1Y t +(1-\u03b1)(S t-1 +B t-1 ) B t =\u03b2(S t -S t-1 )+(1-\u03b2)B t-1 (\uf036)\nThe prediction formula is:\n\u0176 t+T =S t +TB t (\uf037)\nIn (6), \u03b1,\u03b2 are smoothing parameters, the value is between (0,1), Y t is the actual value, S t is the smoothing value of the tth time series, b t is the smoothing value of the t-th trend of the time series, is the predicted number of advanced periods, and \u0176 t+T is the predicted value for the t+T period of the time series. When initializing, take S 1 =Y 1 ,b 1 =Y 2 -Y 1 , and adjust the values of \u03b1 and \u03b2 to optimize the best model to filter the 3D trajectory coordinates.\n\n\nIII. RESULTS AND DISCUSSION\n\n\nA. Simulation and experimental verification\n\nThe performance of the proposed method was firstly compared with modeling simulation. A Burt's upper limb rehabilitation robotic arm is used in this experiment, as shown in Figure 5. The Burt manipulator has 3 active degrees of freedom. The robot in the picture is in the initial state, and the three joint rotation angles are all 0. Based on this model, a visual simulation of trajectory planning is performed. For a given end trajectory collected by the aforementioned method, according to the inverse kinematics model of the robotic arm, the change curve of the corresponding joint angle can be calculated to drive the movement of the robot, as shown in Figure 6. The simulation motion is in passive motion mode, and its direction is horizontal motion from right to left. The simulation motion proves that the robot can execute the trajectory correctly, and proves the correctness of the trajectory. Then, the experimenter performs experimental verification. During the experiment, the experimenter held the end of the Burt robot to touch the ball, and the robotic arm guided the experimenter to perform passive training according to the preset trajectory, as shown in Figure 7. During the movement, the robot control period is accurately maintained at 10 ms, and the kinematic signal sampling frequency is 100 Hz. The experimental results prove that the motion trajectory collected by the aforementioned method can be correctly performed on the rehabilitation robot. \n\n\nB. Test of bio-imitability\n\nThis paper aims to achieve the planar motion of the human arm, and proposes to establish the analytical formula of trajectory and time based on the \"Minimum Jerk Principle\". The goal is to minimize the square of the Jerk value of the hand and the time integral of the movement [13], the minimum Jerk target function is shown in (8).\nC j = 1 2 \u222b [( d 3 x dt 3 ) 2 + ( d 3 y dt 3 ) 2 ] dt t f 0 (\uf038)\nIn this formula, , are the coordinates of the upper extremity trajectory on the motion plane, and is the duration of the motion. For the Burt robot used in this paper, the analytical formula of the trajectory with respect to time can be obtained by minimizing (8). The plane trajectory in this paper is planned from the static state, so the initial velocity and initial acceleration are both 0. The analytical formula of the trajectory point and time on the plane is shown in (9).\n\n\n{\n\nx(t)=x 0 +(x 0 -x f )(15\u03c4 4 -6\u03c4 5 -10\u03c4 3 ) y(t)=y 0 +(y 0 -y f )(15\u03c4 4 -6\u03c4 5 -10\u03c4 3 ) (\uf039)\n\nIn (9), 0 , 0 are the starting points of the upper extremity end trajectories in the motion plane, , are the end points of the upper extremity end trajectories in the motion plane, and \u03c4 is the ratio of exercise time t to exercise duration t f . From (9), it can be seen that the \" Minimum Jerk Principle \" only needs to specify the start point, end point, and motion duration of the trajectory to uniquely determine the motion trajectory of the upper limb robot end in the horizontal plane, and the motion trajectory is a straight line. Participants moved along track 1 and track 2, as shown in Figure 8. In this experiment, there were 3 healthy male participants, with an average age of 24 years. Three experiments were performed for each trajectory, and the hand trajectory was collected and processed using the aforementioned method. The position, velocity, and acceleration were fitted to the trajectory using the \" Minimum Jerk Principle \" respectively.\n\nThe fitting result of trajectory one (from left to right) and trajectory one (from back to front) is shown in Figure 9 and 10. As can be seen from Figure 9, the \"Minimum Jerk Principle\" used in this paper can well fit the position, velocity and acceleration of the actual trajectory. The R 2 coefficient of determination is used here to evaluate the fitting results. The formula is as follow:\nR 2 =1- \u2211(y-\u0177) 2 \u2211(y-y \u0305) 2 (\uf031\uf030)\ny is the actual value, y \u0305 is the average of the actual values, and \u0177 is the predicted value. The value range of R 2 is [0,1]. The closer the value is to 1, the better the fitting effect. The results of fitting the position, velocity, and acceleration in the experiment are shown in the Table I:  In Table 1, group 1, 2 and 3 experiments are track 1 (from left to right), and group 4, 5, 6 experiments are track 2 (from back to front). It can be seen from the table that the R 2 coefficient of determination of the trajectory planned using the minimum jerk criterion and the true trajectory are large, the position fitting results are basically close to 1, the speed fitting results are basically greater than 0.90, and the acceleration fitting results are greater than 0.88. The trajectory of the hand is very close to the trajectory planned by the \" Minimum Jerk Principle \", which proves that the trajectory planned by the method proposed in this paper has high bio-imitability.\n\n\nC. Verification of using OpenPose to improve recognition rate\n\nWhen planning the spatial trajectory of upper limb rehabilitation, some poses will cover the face or other parts of the human body, and Kinect's own human bone tracking algorithm will fail to recognize human joint points under partial occlusion. To solve this problem, we use the open source library OpenPose for human joint point recognition. Take the action of \"taking and eating\" as an example, the action will cover the face, as shown in Figure 10. Comparative experiments were conducted using Kinect's own algorithm and OpenPose respectively to perform joint point recognition on this action, and the result is shown in Table II.  TABLE II.  As can be seen from Table II, in the case of occlusion, the recognition success rate of OpenPose is significantly improved compared to the success rate of Kinect's own joint point algorithm, which can be used to plan the motion trajectory of the upper limb rehabilitation robot.\n\n\nSUCCESS RATE OF THE ABOVE TWO METHODS\n\n\nMethod Performance\n\n\nD. Discussion\n\nBased on the experiment result, it can be proved that the planar trajectory planned by the above method has good bionics in terms of position, velocity and acceleration. Compared with other trajectory planning methods of rehabilitation robots, the teaching trajectory planning method based on human pose estimation is simpler and more convenient for rehabilitation doctors. The rehabilitation doctors can complete the specified actions in front of the depth camera according to the specific situation of the patient, and plan an applicable rehabilitation trajectory for him. Among them, the speed and acceleration of track 2 are not as good as those of track 1. The reason is that track 2 requires a Kinect 2.0 depth camera, and Kinect 2.0 uses a ToF (Time of flight) technology with a lower accuracy (about 1cm). According to Table II, experiments have shown that in the case of occlusion, using OpenPose to process the depth information collected by Kinect for bone tracking has a significant improvement over Kinect's own algorithm recognition rate.\n\n\nIV. CONCLUSION\n\nIn this paper, a method was proposed to plan the motion trajectory of an upper limb rehabilitation robot using human pose estimation. The teaching video was recorded by the Kinect camera, and the trajectory information was extracted by OpenPose and improved by the threshold filtering method. Firstly, The correctness of the trajectory was verified by simulation and robot, and its bio-imitability was also proved to be good. Secondly, the comparison results with other traditional methods shows that our method is efficient. On the one hand, video-based teaching greatly reduces rehabilitation difficulty in redesigning rehabilitation trajectories. On the other hand, the trajectory planned by this method has good bio-imitability and is conducive to the rehabilitation of stroke patients. At last, compared with Kinect's own joint point recognition algorithm, in the case of occlusion, the proposed method can significantly improve the success rate of joint point recognition in spatial trajectory planning. The proposed method provides a new simple and reliable idea for trajectory planning of upper limb rehabilitation robot.\n\nFigure 1 .\n1Flow chart of the proposed method\n\nFigure 2 .\n2Flow of OpenPoseThis paper applies the detection function of 18 body joint points based on Microsoft COCO data set provided by OpenPose. It takes a color image as input and outputs the twodimensional coordinates and confidence of 18 body joint points of each person in the image. The left hand coordinate is chosen as the trajectory tracking point. The skeletal structure of the 18 joint points provided by OpenPose is shown inFigure 3.\n\nFigure 3 .\n3Bone structure of OpenPose on COCO dataset.\n\n\nTake a point on the depth image as (x D ,y D ), and the depth value of this point is D(x D ,y D ) . The three-dimensional coordinates (X D ,Y D ,Z D ) of this point under the Kinect depth camera are:\n\nFirst\n, the trajectory points are filtered. A threshold filtering method for spatial trajectory points is proposed here. For a trajectory point, count the number of trajectory points in the range d of each point first. For a point (x d ,y d ,z d ), the definition of the trajectory point (x, y, z) located in the range d for:(x-x d ) 2 +(y-y d ) 2 +(z-z d ) 2 \u2264d 2 (\uf035)Find the average number of trajectories in the range d and round down. The average number of trajectory points in the corresponding range d is N d . For each point in the trajectory, find the number of trajectory points N i (i=1,2\u22efn), when N i <N d , consider this point as an abnormal point and delete it from the trajectory, as shown inFigure 4.\n\nFigure 4 .\n4Schematic diagram of Threshold filtering method\n\nFigure 5 .\n5D-H coordinate system of Burt robot and simulation model based on it\n\nFigure 6 .\n6The simulation model moves according to the acquisition trajectory.\n\nFigure 7 .\n7Experimenter uses Burt robot to verify trajectory\n\nFigure 8 .\n8Top view of experimental trajectory\n\nFigure 9 .\n9Fitting effect of trajectory 1 and 2. (a,b,c are the fitting effect of position, speed and acceleration in turn of trajectory 1, d,e,f are the the fitting effect of position, speed and acceleration in turn of trajectory 2, the same color represents the same set of experiments)\n\nFigure 10 .\n10\"taking and eating\" action.\n\nTABLE I .\nIR 2 COEFFICIENT OF DETERMINATION OF EACH EXPERIMENT \n\nExperiment Order \nR 2 coefficient of determination \n\nPosition \nSpeed \nAcceleration \n\n1 \n0.9979 \n0.9396 \n0.9108 \n2 \n0.9995 \n0.9830 \n0.9627 \n3 \n0.9903 \n0.9359 \n0.9137 \n4 \n0.9924 \n0.9194 \n0.8867 \n5 \n0.9970 \n0.9070 \n0.8848 \n6 \n0.9920 \n0.9462 \n0.8872 \n\n\n\nGlobal, regional, and national burden of stroke, 1990-2016 : A systematic analysis for the Global Burden of Disease Study 2016. C O Johnson, Lancet Neurology. 185C.O.Johnson, et al. \" Global, regional, and national burden of stroke, 1990-2016 : A systematic analysis for the Global Burden of Disease Study 2016.\" In: Lancet Neurology. 2019; Vol. 18, No. 5. pp. 439-458.\n\nPlasticity in the developing brain: Implications for rehabilitation. Michael V Johnston, Developmental Disabilities Research Reviews. 15Johnston, Michael V . \"Plasticity in the developing brain: Implications for rehabilitation.\" Developmental Disabilities Research Reviews 15.2(2009):94-101.\n\nEffects of robot assistive upper extremity rehabilitation on motor and cognitive recovery, the quality of life, and activities of daily living in stroke patients. Metli Zengin, Derya, Journal of Back and Musculoskeletal Rehabilitation. Zengin, Metli Derya, et al. \"Effects of robot assistive upper extremity rehabilitation on motor and cognitive recovery, the quality of life, and activities of daily living in stroke patients.\" Journal of Back and Musculoskeletal Rehabilitation (2018):1-6.\n\nTrajectory Planning of a Novel Lower Limb Rehabilitation Robot for Stroke Patient Passive Training. Yongfei Feng, Advances in Mechanical Engineering. Feng, Yongfei, et al. \"Trajectory Planning of a Novel Lower Limb Rehabilitation Robot for Stroke Patient Passive Training.\" Advances in Mechanical Engineering, Dec. 2017.\n\nAn online trajectory planning method for visually guided assisted reaching through a rehabilitation robot. C Loconsole, R Bartalucci, A Frisoli, M Bergamasco, 2011 IEEE International Conference on Robotics and Automation. ShanghaiC. Loconsole, R. Bartalucci, A. Frisoli and M. Bergamasco, \"An online trajectory planning method for visually guided assisted reaching through a rehabilitation robot,\" 2011 IEEE International Conference on Robotics and Automation, Shanghai, 2011, pp. 1445-1450.\n\nGlobal minimum-jerk trajectory planning of robot manipulators. A Piazzi, A Visioli, IEEE Transactions on Industrial Electronics. 47Piazzi, A. , and A. Visioli . \"Global minimum-jerk trajectory planning of robot manipulators.\" IEEE Transactions on Industrial Electronics 47.1(2000):140-149.\n\nTrajectory Planning and Speed Control for a Two-Link Rigid Manipulator. Walerian Szyszkowski, P N Nikiforuk, Journal of Mechanical Design. 124Szyszkowski, Walerian , and P. N. Nikiforuk . \"Trajectory Planning and Speed Control for a Two-Link Rigid Manipulator.\" Journal of Mechanical Design 124.3(2002):585-589.\n\nTrajectory planning of a robot for lower limb rehabilitation. Y Pei, Y Kim, G Obinata, K Hase, D Stefanov, 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society. Boston, MAY. Pei, Y. Kim, G. Obinata, K. Hase and D. Stefanov, \"Trajectory planning of a robot for lower limb rehabilitation,\" 2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Boston, MA, 2011, pp. 1259-1263.\n\nA review of rehabilitation robot. B Li, G Li, Y Sun, G Jiang, J Kong, D Jiang, 32nd Youth Academic Annual Conference of Chinese Association of Automation (YAC). HefeiB. Li, G. Li, Y. Sun, G. Jiang, J. Kong and D. Jiang, \"A review of rehabilitation robot,\" 2017 32nd Youth Academic Annual Conference of Chinese Association of Automation (YAC), Hefei, 2017, pp. 907-911.\n\nRehabilitation Technology with Motor Neuroprosthesis:Review and Outlook. Wang L Zhang, Nanotechnology and Precision Engineering. 1306Zhang. L, Wang. Z, \"Rehabilitation Technology with Motor Neuroprosthesis:Review and Outlook\", Nanotechnology and Precision Engineering, 2015,13(06):404-413.\n\nIncreasing upper limb training intensity in chronic stroke using embodied virtual reality: a pilot study. Perez-Marcos, Daniel, Journal of NeuroEngineering and Rehabilitation. 14119Perez-Marcos, Daniel , et al. \"Increasing upper limb training intensity in chronic stroke using embodied virtual reality: a pilot study.\" Journal of NeuroEngineering and Rehabilitation 14.1(2017):119.\n\nRealtime Multi-Person 2D Pose Estimation using Part Affinity Fields. Zhe Cao, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Cao, Zhe , et al. \"Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields.\" The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 7291-7299\n\nThe coordination of arm movements : An experimentally confirmed mathematical model. T Flush, The Journal of Neuroscience. 57Flush, T. \"The coordination of arm movements : An experimentally confirmed mathematical model.\" The Journal of Neuroscience 5(1985), 5 (7) 1688-1703.\n", "annotations": {"author": "[{\"start\":\"4\",\"end\":\"16\"},{\"start\":\"17\",\"end\":\"88\"},{\"start\":\"89\",\"end\":\"186\"}]", "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": "[{\"start\":\"5\",\"end\":\"15\"},{\"start\":\"18\",\"end\":\"87\"},{\"start\":\"90\",\"end\":\"185\"}]", "title": null, "venue": null, "abstract": "[{\"start\":\"273\",\"end\":\"1599\"}]", "bib_ref": "[{\"start\":\"1887\",\"end\":\"1890\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2138\",\"end\":\"2141\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2806\",\"end\":\"2809\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3001\",\"end\":\"3004\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3234\",\"end\":\"3237\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3426\",\"end\":\"3429\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"3609\",\"end\":\"3612\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"3827\",\"end\":\"3830\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"4414\",\"end\":\"4417\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"4417\",\"end\":\"4421\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"4725\",\"end\":\"4729\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"8576\",\"end\":\"8580\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"15712\",\"end\":\"15716\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"15763\",\"end\":\"15766\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"16092\",\"end\":\"16095\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"16308\",\"end\":\"16311\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"16660\",\"end\":\"16663\",\"attributes\":{\"ref_id\":\"b8\"}}]", "figure": "[{\"start\":\"22048\",\"end\":\"22094\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"22095\",\"end\":\"22544\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"22545\",\"end\":\"22601\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"22602\",\"end\":\"22803\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"22804\",\"end\":\"23520\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"23521\",\"end\":\"23581\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"23582\",\"end\":\"23663\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"23664\",\"end\":\"23744\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"23745\",\"end\":\"23807\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"23808\",\"end\":\"23856\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"23857\",\"end\":\"24147\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"24148\",\"end\":\"24190\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"24191\",\"end\":\"24505\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1618\",\"end\":\"2810\"},{\"start\":\"2812\",\"end\":\"4730\"},{\"start\":\"4732\",\"end\":\"5162\"},{\"start\":\"5252\",\"end\":\"6052\"},{\"start\":\"6054\",\"end\":\"7682\"},{\"start\":\"7750\",\"end\":\"8275\"},{\"start\":\"8307\",\"end\":\"10272\"},{\"start\":\"10326\",\"end\":\"11107\"},{\"start\":\"11230\",\"end\":\"11839\"},{\"start\":\"11900\",\"end\":\"12014\"},{\"start\":\"12078\",\"end\":\"12349\"},{\"start\":\"12379\",\"end\":\"13069\"},{\"start\":\"13071\",\"end\":\"13263\"},{\"start\":\"13333\",\"end\":\"13359\"},{\"start\":\"13381\",\"end\":\"13855\"},{\"start\":\"13933\",\"end\":\"15404\"},{\"start\":\"15435\",\"end\":\"15767\"},{\"start\":\"15832\",\"end\":\"16312\"},{\"start\":\"16318\",\"end\":\"16407\"},{\"start\":\"16409\",\"end\":\"17368\"},{\"start\":\"17370\",\"end\":\"17762\"},{\"start\":\"17796\",\"end\":\"18777\"},{\"start\":\"18843\",\"end\":\"19768\"},{\"start\":\"19847\",\"end\":\"20899\"},{\"start\":\"20918\",\"end\":\"22047\"}]", "formula": "[{\"start\":\"11108\",\"end\":\"11229\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"11840\",\"end\":\"11899\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"12015\",\"end\":\"12077\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"13264\",\"end\":\"13332\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"13360\",\"end\":\"13380\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"15768\",\"end\":\"15831\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"17763\",\"end\":\"17795\",\"attributes\":{\"id\":\"formula_6\"}}]", "table_ref": "[{\"start\":\"18083\",\"end\":\"18091\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"18096\",\"end\":\"18103\"},{\"start\":\"19468\",\"end\":\"19488\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"19510\",\"end\":\"19518\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"20674\",\"end\":\"20682\",\"attributes\":{\"ref_id\":\"tab_0\"}}]", "section_header": "[{\"start\":\"1601\",\"end\":\"1616\"},{\"start\":\"5165\",\"end\":\"5250\"},{\"start\":\"7685\",\"end\":\"7700\"},{\"start\":\"7703\",\"end\":\"7748\"},{\"start\":\"8278\",\"end\":\"8305\"},{\"start\":\"10275\",\"end\":\"10324\"},{\"start\":\"12352\",\"end\":\"12377\"},{\"start\":\"13858\",\"end\":\"13885\"},{\"start\":\"13888\",\"end\":\"13931\"},{\"start\":\"15407\",\"end\":\"15433\"},{\"start\":\"16315\",\"end\":\"16316\"},{\"start\":\"18780\",\"end\":\"18841\"},{\"start\":\"19771\",\"end\":\"19808\"},{\"start\":\"19811\",\"end\":\"19829\"},{\"start\":\"19832\",\"end\":\"19845\"},{\"start\":\"20902\",\"end\":\"20916\"},{\"start\":\"22049\",\"end\":\"22059\"},{\"start\":\"22096\",\"end\":\"22106\"},{\"start\":\"22546\",\"end\":\"22556\"},{\"start\":\"22805\",\"end\":\"22810\"},{\"start\":\"23522\",\"end\":\"23532\"},{\"start\":\"23583\",\"end\":\"23593\"},{\"start\":\"23665\",\"end\":\"23675\"},{\"start\":\"23746\",\"end\":\"23756\"},{\"start\":\"23809\",\"end\":\"23819\"},{\"start\":\"23858\",\"end\":\"23868\"},{\"start\":\"24149\",\"end\":\"24160\"},{\"start\":\"24192\",\"end\":\"24201\"}]", "table": "[{\"start\":\"24203\",\"end\":\"24505\"}]", "figure_caption": "[{\"start\":\"22061\",\"end\":\"22094\"},{\"start\":\"22108\",\"end\":\"22544\"},{\"start\":\"22558\",\"end\":\"22601\"},{\"start\":\"22604\",\"end\":\"22803\"},{\"start\":\"22811\",\"end\":\"23520\"},{\"start\":\"23534\",\"end\":\"23581\"},{\"start\":\"23595\",\"end\":\"23663\"},{\"start\":\"23677\",\"end\":\"23744\"},{\"start\":\"23758\",\"end\":\"23807\"},{\"start\":\"23821\",\"end\":\"23856\"},{\"start\":\"23870\",\"end\":\"24147\"},{\"start\":\"24163\",\"end\":\"24190\"}]", "figure_ref": "[{\"start\":\"7828\",\"end\":\"7836\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"9533\",\"end\":\"9541\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"9598\",\"end\":\"9606\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"9680\",\"end\":\"9688\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"9856\",\"end\":\"9865\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"9935\",\"end\":\"9943\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"10122\",\"end\":\"10131\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"14106\",\"end\":\"14114\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"14590\",\"end\":\"14598\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"15105\",\"end\":\"15113\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"17005\",\"end\":\"17013\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"17480\",\"end\":\"17488\",\"attributes\":{\"ref_id\":\"fig_10\"}},{\"start\":\"17517\",\"end\":\"17525\",\"attributes\":{\"ref_id\":\"fig_10\"}},{\"start\":\"19285\",\"end\":\"19294\",\"attributes\":{\"ref_id\":\"fig_0\"}}]", "bib_author_first_name": "[{\"start\":\"24635\",\"end\":\"24636\"},{\"start\":\"24637\",\"end\":\"24638\"},{\"start\":\"24947\",\"end\":\"24954\"},{\"start\":\"24955\",\"end\":\"24956\"},{\"start\":\"25334\",\"end\":\"25339\"},{\"start\":\"25764\",\"end\":\"25771\"},{\"start\":\"26093\",\"end\":\"26094\"},{\"start\":\"26106\",\"end\":\"26107\"},{\"start\":\"26120\",\"end\":\"26121\"},{\"start\":\"26131\",\"end\":\"26132\"},{\"start\":\"26542\",\"end\":\"26543\"},{\"start\":\"26552\",\"end\":\"26553\"},{\"start\":\"26842\",\"end\":\"26850\"},{\"start\":\"26864\",\"end\":\"26865\"},{\"start\":\"26866\",\"end\":\"26867\"},{\"start\":\"27145\",\"end\":\"27146\"},{\"start\":\"27152\",\"end\":\"27153\"},{\"start\":\"27159\",\"end\":\"27160\"},{\"start\":\"27170\",\"end\":\"27171\"},{\"start\":\"27178\",\"end\":\"27179\"},{\"start\":\"27573\",\"end\":\"27574\"},{\"start\":\"27579\",\"end\":\"27580\"},{\"start\":\"27585\",\"end\":\"27586\"},{\"start\":\"27592\",\"end\":\"27593\"},{\"start\":\"27601\",\"end\":\"27602\"},{\"start\":\"27609\",\"end\":\"27610\"},{\"start\":\"27982\",\"end\":\"27986\"},{\"start\":\"27987\",\"end\":\"27988\"},{\"start\":\"28652\",\"end\":\"28655\"},{\"start\":\"28996\",\"end\":\"28997\"}]", "bib_author_last_name": "[{\"start\":\"24639\",\"end\":\"24646\"},{\"start\":\"24957\",\"end\":\"24965\"},{\"start\":\"25340\",\"end\":\"25346\"},{\"start\":\"25348\",\"end\":\"25353\"},{\"start\":\"25772\",\"end\":\"25776\"},{\"start\":\"26095\",\"end\":\"26104\"},{\"start\":\"26108\",\"end\":\"26118\"},{\"start\":\"26122\",\"end\":\"26129\"},{\"start\":\"26133\",\"end\":\"26143\"},{\"start\":\"26544\",\"end\":\"26550\"},{\"start\":\"26554\",\"end\":\"26561\"},{\"start\":\"26851\",\"end\":\"26862\"},{\"start\":\"26868\",\"end\":\"26877\"},{\"start\":\"27147\",\"end\":\"27150\"},{\"start\":\"27154\",\"end\":\"27157\"},{\"start\":\"27161\",\"end\":\"27168\"},{\"start\":\"27172\",\"end\":\"27176\"},{\"start\":\"27180\",\"end\":\"27188\"},{\"start\":\"27575\",\"end\":\"27577\"},{\"start\":\"27581\",\"end\":\"27583\"},{\"start\":\"27587\",\"end\":\"27590\"},{\"start\":\"27594\",\"end\":\"27599\"},{\"start\":\"27603\",\"end\":\"27607\"},{\"start\":\"27611\",\"end\":\"27616\"},{\"start\":\"27989\",\"end\":\"27994\"},{\"start\":\"28306\",\"end\":\"28318\"},{\"start\":\"28320\",\"end\":\"28326\"},{\"start\":\"28656\",\"end\":\"28659\"},{\"start\":\"28998\",\"end\":\"29003\"}]", "bib_entry": "[{\"start\":\"24507\",\"end\":\"24876\",\"attributes\":{\"matched_paper_id\":\"73728592\",\"id\":\"b0\"}},{\"start\":\"24878\",\"end\":\"25169\",\"attributes\":{\"matched_paper_id\":\"26483953\",\"id\":\"b1\"}},{\"start\":\"25171\",\"end\":\"25662\",\"attributes\":{\"matched_paper_id\":\"49639357\",\"id\":\"b2\"}},{\"start\":\"25664\",\"end\":\"25984\",\"attributes\":{\"matched_paper_id\":\"115992953\",\"id\":\"b3\"}},{\"start\":\"25986\",\"end\":\"26477\",\"attributes\":{\"matched_paper_id\":\"17123961\",\"id\":\"b4\"}},{\"start\":\"26479\",\"end\":\"26768\",\"attributes\":{\"matched_paper_id\":\"2175191\",\"id\":\"b5\"}},{\"start\":\"26770\",\"end\":\"27081\",\"attributes\":{\"matched_paper_id\":\"111190859\",\"id\":\"b6\"}},{\"start\":\"27083\",\"end\":\"27537\",\"attributes\":{\"matched_paper_id\":\"4902417\",\"id\":\"b7\"}},{\"start\":\"27539\",\"end\":\"27907\",\"attributes\":{\"matched_paper_id\":\"46239123\",\"id\":\"b8\"}},{\"start\":\"27909\",\"end\":\"28198\",\"attributes\":{\"id\":\"b9\"}},{\"start\":\"28200\",\"end\":\"28581\",\"attributes\":{\"matched_paper_id\":\"5900458\",\"id\":\"b10\"}},{\"start\":\"28583\",\"end\":\"28910\",\"attributes\":{\"matched_paper_id\":\"16224674\",\"id\":\"b11\"}},{\"start\":\"28912\",\"end\":\"29185\",\"attributes\":{\"matched_paper_id\":\"18355250\",\"id\":\"b12\"}}]", "bib_title": "[{\"start\":\"24507\",\"end\":\"24633\"},{\"start\":\"24878\",\"end\":\"24945\"},{\"start\":\"25171\",\"end\":\"25332\"},{\"start\":\"25664\",\"end\":\"25762\"},{\"start\":\"25986\",\"end\":\"26091\"},{\"start\":\"26479\",\"end\":\"26540\"},{\"start\":\"26770\",\"end\":\"26840\"},{\"start\":\"27083\",\"end\":\"27143\"},{\"start\":\"27539\",\"end\":\"27571\"},{\"start\":\"27909\",\"end\":\"27980\"},{\"start\":\"28200\",\"end\":\"28304\"},{\"start\":\"28583\",\"end\":\"28650\"},{\"start\":\"28912\",\"end\":\"28994\"}]", "bib_author": "[{\"start\":\"24635\",\"end\":\"24648\"},{\"start\":\"24947\",\"end\":\"24967\"},{\"start\":\"25334\",\"end\":\"25348\"},{\"start\":\"25348\",\"end\":\"25355\"},{\"start\":\"25764\",\"end\":\"25778\"},{\"start\":\"26093\",\"end\":\"26106\"},{\"start\":\"26106\",\"end\":\"26120\"},{\"start\":\"26120\",\"end\":\"26131\"},{\"start\":\"26131\",\"end\":\"26145\"},{\"start\":\"26542\",\"end\":\"26552\"},{\"start\":\"26552\",\"end\":\"26563\"},{\"start\":\"26842\",\"end\":\"26864\"},{\"start\":\"26864\",\"end\":\"26879\"},{\"start\":\"27145\",\"end\":\"27152\"},{\"start\":\"27152\",\"end\":\"27159\"},{\"start\":\"27159\",\"end\":\"27170\"},{\"start\":\"27170\",\"end\":\"27178\"},{\"start\":\"27178\",\"end\":\"27190\"},{\"start\":\"27573\",\"end\":\"27579\"},{\"start\":\"27579\",\"end\":\"27585\"},{\"start\":\"27585\",\"end\":\"27592\"},{\"start\":\"27592\",\"end\":\"27601\"},{\"start\":\"27601\",\"end\":\"27609\"},{\"start\":\"27609\",\"end\":\"27618\"},{\"start\":\"27982\",\"end\":\"27996\"},{\"start\":\"28306\",\"end\":\"28320\"},{\"start\":\"28320\",\"end\":\"28328\"},{\"start\":\"28652\",\"end\":\"28661\"},{\"start\":\"28996\",\"end\":\"29005\"}]", "bib_venue": "[{\"start\":\"24648\",\"end\":\"24664\"},{\"start\":\"24967\",\"end\":\"25010\"},{\"start\":\"25355\",\"end\":\"25405\"},{\"start\":\"25778\",\"end\":\"25812\"},{\"start\":\"26145\",\"end\":\"26206\"},{\"start\":\"26563\",\"end\":\"26606\"},{\"start\":\"26879\",\"end\":\"26907\"},{\"start\":\"27190\",\"end\":\"27282\"},{\"start\":\"27618\",\"end\":\"27698\"},{\"start\":\"27996\",\"end\":\"28036\"},{\"start\":\"28328\",\"end\":\"28374\"},{\"start\":\"28661\",\"end\":\"28729\"},{\"start\":\"29005\",\"end\":\"29032\"},{\"start\":\"26208\",\"end\":\"26216\"},{\"start\":\"27284\",\"end\":\"27294\"},{\"start\":\"27700\",\"end\":\"27705\"}]"}}}, "year": 2023, "month": 12, "day": 17}