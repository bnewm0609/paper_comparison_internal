{"id": 13060737, "updated": "2023-09-28 21:22:17.152", "metadata": {"title": "Embedding Watermarks into Deep Neural Networks", "authors": "[{\"first\":\"Yusuke\",\"last\":\"Uchida\",\"middle\":[]},{\"first\":\"Yuki\",\"last\":\"Nagai\",\"middle\":[]},{\"first\":\"Shigeyuki\",\"last\":\"Sakazawa\",\"middle\":[]},{\"first\":\"Shin'ichi\",\"last\":\"Satoh\",\"middle\":[]}]", "venue": "ICMR '17 Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval Pages 269-277", "journal": null, "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Deep neural networks have recently achieved significant progress. Sharing trained models of these deep neural networks is very important in the rapid progress of researching or developing deep neural network systems. At the same time, it is necessary to protect the rights of shared trained models. To this end, we propose to use a digital watermarking technology to protect intellectual property or detect intellectual property infringement of trained models. Firstly, we formulate a new problem: embedding watermarks into deep neural networks. We also define requirements, embedding situations, and attack types for watermarking to deep neural networks. Secondly, we propose a general framework to embed a watermark into model parameters using a parameter regularizer. Our approach does not hurt the performance of networks into which a watermark is embedded. Finally, we perform comprehensive experiments to reveal the potential of watermarking to deep neural networks as a basis of this new problem. We show that our framework can embed a watermark in the situations of training a network from scratch, fine-tuning, and distilling without hurting the performance of a deep neural network. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark completely remains even after removing 65% of parameters were pruned. The implementation of this research is: https://github.com/yu4u/dnn-watermark", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1701.04082", "mag": "3103782966", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mir/UchidaNSS17", "doi": "10.1145/3078971.3078974"}}, "content": {"source": {"pdf_hash": "8ace99cef6d1f19ef55e43c0eb076748171d3198", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1701.04082v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1701.04082", "status": "GREEN"}}, "grobid": {"id": "99d162d57d9c46e34d8c383c43dde76257977893", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8ace99cef6d1f19ef55e43c0eb076748171d3198.txt", "contents": "\nEmbedding Watermarks into Deep Neural Networks\n\n\nYusuke Uchida \nKDDI Research, Inc. Tokyo\nJapan\n\nYuki Nagai \nKDDI Research, Inc. Tokyo\nJapan\n\nShigeyuki Sakazawa \nShin'ichi Satoh National Institute of Informatics Tokyo\nKDDI Research, Inc. Tokyo\nJapan, Japan\n\nEmbedding Watermarks into Deep Neural Networks\n\nSignificant progress has been made with deep neural networks recently. Sharing trained models of deep neural networks has been a very important in the rapid progress of research and development of these systems. At the same time, it is necessary to protect the rights to shared trained models. To this end, we propose to use digital watermarking technology to protect intellectual property and detect intellectual property infringement in the use of trained models. First, we formulate a new problem: embedding watermarks into deep neural networks. We also define requirements, embedding situations, and attack types on watermarking in deep neural networks. Second, we propose a general framework for embedding a watermark in model parameters, using a parameter regularizer. Our approach does not impair the performance of networks into which a watermark is placed because the watermark is embedded while training the host network. Finally, we perform comprehensive experiments to reveal the potential of watermarking deep neural networks as the basis of this new research effort. We show that our framework can embed a watermark during the training of a deep neural network from scratch, and during fine-tuning and distilling, without impairing its performance. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark remains complete even after 65% of parameters are pruned.\n\nIntroduction\n\nDeep neural networks have recently been significantly improved. In particular, deep convolutional neural networks (DCNN) such as LeNet [25], AlexNet [23], VGGNet [28], GoogLeNet [29], and ResNet [16] have become de facto standards for object recognition, image classification, and retrieval. In addition, many deep learning frameworks have been released that help engineers and researchers to develop systems based on deep learning or do research with less effort. Examples of these great deep learning frameworks are Caffe [19], Theano [3], Torch [7], Chainer [30], Tensor-Flow [26], and Keras [5].\n\nAlthough these frameworks have made it easy to utilize deep neural networks in real applications, the training of deep neural network models is still a difficult task because it requires a large amount of data and time; several weeks are needed to train a very deep ResNet with the latest GPUs on the ImageNet dataset for instance [16]. Therefore, trained models are sometimes provided on web sites to make it easy to try a certain model or reproduce the results in research articles without training. For example, Model Zoo 1 provides trained Caffe models for various tasks with useful utility tools. Fine-tuning or transfer learning [28] is a strategy to directly adapt such already trained models to another application with minimum re-training time. Thus, sharing trained models is very important in the rapid progress of both research and development of deep neural network systems. In the future, more systematic model-sharing platforms may appear, by analogy with video sharing sites. Furthermore, some digital distribution platforms for purchase and sale of the trained models or even artificial intelligence skills (e.g. Alexa Skills 2 ) may appear, similar to Google Play or App Store. In these situations, it is necessary to protect the rights to shared trained models.\n\nTo this end, we propose to utilize digital watermarking technology, which is used to identify ownership of the copyright of digital content such as images, audio, and videos. In particular, we propose a general framework to embed a watermark in deep neural networks models to protect intellectual property and detect intellectual property infringement of trained models. To the best of our knowledge, this is first attempt to embed a watermark in a deep neural network.\n\nThe contributions of this research are three-fold, as follows:\n\n1. We formulate a new problem: embedding watermarks in deep neural networks. We also define requirements, embedding situations, and attack types for watermarking deep neural networks.\n\n2. We propose a general framework to embed a watermark in model parameters, using a parameter regularizer. Our approach does not impair the performance of networks in which a watermark is embedded.\n\n3. We perform comprehensive experiments to reveal the potential of watermarking deep neural networks.\n\n\nProblem Formulation\n\nGiven a model network with or without trained parameters, we define the task of watermark embedding as to embed T -bit vector b \u2208 {0, 1} T into the parameters of one or more layers of the neural network. We refer to a neural network in which a watermark is embedded as a host network, and refer to the task that the host network is originally trying to perform as the original task.\n\nIn the following, we formulate (1) requirements for an embedded watermark or an embedding method, (2) embedding situations, and (3) expected attack types against which embedded watermarks should be robust. Table 1 summarizes the requirements for an effective watermarking algorithm in an image domain [15,8] and a neural network domain. While both domains share almost the same requirements, fidelity and robustness are different in image and neural network domains.\n\n\nRequirements\n\nFor fidelity in an image domain, it is essential to maintain the perceptual quality of the host image while embedding a watermark. However, in a neural network domain, the parameters themselves are not important. Instead, the performance of the original task is important. Therefore it is essential to maintain the performance of the trained host network, and not to hamper the training of a host network.\n\nRegarding robustness, as images are subject to various signal processing operations, an embedded watermark should stay in the host image even after these operations. And the greatest possible modification to a neural network is fine-tuning or transfer learning [28]. An embedded watermark in a neural network should be detectable after finetuning or other possible modifications.\n\n\nEmbedding Situations\n\nWe classify the embedding situations into three types: train-to-embed, fine-tune-to-embed, and distill-to-embed, as summarized in Table 2. Table 2. Three embedding situations. Fine-tune indicates whether parameters are initialized in embedding using already trained models, or not. Label availability indicates whether or labels for training data are available in embedding.\n\nFine-tune Label availability Train-to-embed Fine-tune-to-embed Distill-to-embed\n\nTrain-to-embed is the case in which the host network is trained from scratch while embedding a watermark where labels for training data are available.\n\nFine-tune-to-embed is the case in which a watermark is embedded while fine-tuning. In this case, model parameters are initialized with a pre-trained network. The network configuration near the output layer may be changed before fine-tuning.\n\nDistill-to-embed is the case in which a watermark is embedded into a trained network without labels using the distilling approach [17]. Embedding is performed in finetuning where the predictions of the trained model are used as labels. In the standard distill framework, a large network (or multiple networks) is first trained and then a smaller network is trained using the predicted labels of the large network in order to compress the large network. In this paper, we use the distill framework simply to train a network without labels.\n\nThe first two situations assume that the copyright holder of the host network is expected to embed a watermark to the host network in training or fine-tuning. Fine-tune-toembed is also useful when a model owner wants to embed individual watermarks to identify those to whom the model had been distributed. By doing so, individual instances can be tracked. The last situation assumes that a non-copyright holder (e.g., a platformer) is entrusted to embed a watermark on behalf of a copyright holder.\n\n\nExpected Attack Types\n\nRelated to the requirement for robustness in Section 2.1, we assume two types of attacks against which embedded watermarks should be robust: fine-tuning and model compression. These types of attack are very specific to deep neural networks, while one can easily imagine model compression by analogy with lossy image compression in the image domain.\n\nFine-tuning or transfer learning [28] seems to be the most feasible type of attack, because it reduces the burden of training deep neural networks. Many models have been constructed on top of existing state-of-the-art models. Finetuning alters the model parameters, and thus embedded watermarks should be robust against this alteration.\n\nModel compression is very important in deploying deep neural networks to embedded systems or mobile devices \n\n\nImage domain\n\nNeural networks domain Fidelity\n\nThe quality of the host image should not be degraded by embedding a watermark.\n\nThe effectiveness of the host network should not be degraded by embedding a watermark. Robustness The embedded watermark should be robust against common signal processing operations such as lossy compression, cropping, resizing, and so on.\n\nThe embedded watermark should be robust against model modifications such as finetuning and model compression.\n\n\nCapacity\n\nThe effective watermarking system must have the ability to embed a large amount of information. Security A watermark should in general be secret and should not be accessed, read, or modified by unauthorized parties. Efficiency\n\nThe watermark embedding and extraction processes should be fast.\n\nas it can significantly reduce memory requirements and/or computational cost. Lossy compression distorts model parameters, so we should explore how it affects the detection rate.\n\n\nProposed Framework\n\nIn this section, we propose a framework for embedding a watermark into a host network. Although we focus on a DCNN [25] as the host, our framework is essentially applicable to other networks such as standard multilayer perceptron (MLP), recurrent neural network (RNN), and long short-term memory (LSTM) [18].\n\n\nEmbedding Targets\n\nIn this paper, a watermark is assumed to be embedded into one of the convolutional layers in a host DCNN 3 . Let (S, S), D, and L respectively denote the size of the convolution filter, the depth of input to the convolutional layer, and the number of filters in the convolutional layer. The parameters of this convolutional layer are characterized by the tensor W \u2208 R S\u00d7S\u00d7D\u00d7L . The bias term is ignored here. Let us think of embedding a T -bit vector b \u2208 {0, 1} T into W . The tensor W is a set of L convolutional filters and the order of the filters does not affect the output of the network if the parameters of the subsequent layers are appropriately re-ordered. In order to remove this arbitrariness in the order of filters, we calculate the mean of W over L filters as\nW ijk = 1 L l W ijkl . Letting w \u2208 R M (M = S \u00d7 S \u00d7 D)\ndenote a flattened version of W , our objective is now to embed T -bit vector b into w.\n\n\nEmbedding Regularizer\n\nIt is possible to embed a watermark in a host network by directly modifying w of a trained network, as is usually done in the image domain. However, this approach de- 3 Fully-connected layers can also be used but we focus on convolutional layers here, because fully-connected layers are often discarded in fine-tuning. grades the performance of the host network in the original task as shown later in Section 4.2.6. Instead, we propose embedding a watermark while training a host network for the original task so that the existence of the watermark does not impair the performance of the host network in its original task. To this end, we utilize a parameter regularizer, which is an additional term in the original cost function for the original task. The cost function E(w) with a regularizer is defined as:\nE(w) = E 0 (w) + \u03bbE R (w),(1)\nwhere E 0 (w) is the original cost function, E R (w) is a regularization term that imposes a certain restriction on parameters w, and \u03bb is an adjustable parameter. A regularizer is usually used to prevent the parameters from growing too large. L 2 regularization (or weight decay [24]), L 1 regularization, and their combination are often used to reduce over-fitting of parameters for complex neural networks. For instance, E R (w) = ||w|| 2 2 in the L 2 regularization. In contrast to these standard regularizers, our regularizer imposes parameter w to have a certain statistical bias, as a watermark in a training process. We refer to this regularizer as an embedding regularizer. Before defining the embedding regularizer, we explain how to extract a watermark from w. Given a (mean) parameter vector w \u2208 R M and an embedding parameter X \u2208 R T \u00d7M , the watermark extraction is simply done by projecting w using X, followed by thresholding at 0. More precisely, the j-th bit is extracted\nas: b j = s(\u03a3 i X ji w i ),(2)\nwhere s(x) is a step function:\ns(x) = 1 x \u2265 0 0 else.(3)\nThis process can be considered to be a binary classification problem with a single-layer perceptron (without bias) 4 .\n\nTherefore, it is straightforward to define the loss function E R (w) for the embedding regularizer by using binary cross entropy:\nE R (w) = \u2212 T j=1 (b j log(y j ) + (1 \u2212 b j ) log(1 \u2212 y j )) ,(4)\nwhere y j = \u03c3(\u03a3 i X ji w i ) and \u03c3(\u00b7) is the sigmoid function:\n\u03c3(x) = 1 1 + exp(\u2212x)\n.\n\nWe call this loss function an embedding loss function. It may be confusing that an embedding loss function is used to update w, not X, in our framework. In a standard perceptron, w is an input and X is a parameter to be learned. In our case, w is an embedding target and X is a fixed parameter. The design of X is discussed in the following section. This approach does not impair the performance of the host network in the original task as confirmed in experiments, because deep neural networks are typically overparameterized. It is well-known that deep neural networks have many local minima, and that all local minima are likely to have an error very close to that of the global minimum [9,6]. Therefore, the embedding regularizer only needs to guide model parameters to one of a number of good local minima so that the final model parameters have an arbitrary watermark.\n\n\nRegularizer Parameters\n\nIn this section we discuss the design of the embedding parameter X, which can be considered as a secret key [15] in detecting and embedding watermarks. While X \u2208 R T \u00d7M can be an arbitrary matrix, it will affect the performance of an embedded watermark because it is used in both embedding and extraction of watermarks. In this paper, we consider three types of X: X direct , X diff , and X random .\n\nX direct is constructed so that one element in each row of X direct is '1' and the others are '0'. In this case, the j-th bit b j is directly embedded in a certain parameter w\u00ee s.t.\nX direct j\u00ee = 1.\nX diff is created so that each row has one '1' element and one '-1' element, and the others are '0'. Using X diff , the j-th bit b j is embedded into the difference between w i+ and w i\u2212 where X diff ji+ = 1 and X diff ji\u2212 = \u22121. Each element of X random is independently drawn from the standard normal distribution N (0, 1). Using X random , each bit is embedded into all instances of the parameter w with random weights. These three types of embedding parameters are compared in experiments. \n3 \u00d7 3, 16 \u00d7 k 3 \u00d7 3, 16 \u00d7 k \u00d7 N 144 \u00d7 k conv 3 16 \u00d7 16 3 \u00d7 3, 32 \u00d7 k 3 \u00d7 3, 32 \u00d7 k \u00d7 N 288 \u00d7 k conv 4 8 \u00d7 8 3 \u00d7 3, 64 \u00d7 k 3 \u00d7 3, 64 \u00d7 k \u00d7 N 576 \u00d7 k 1 \u00d7 1 avg-pool, fc, soft-max N/A\n\nExperiments\n\nIn this section, we demonstrate that our embedding regularizer can embed a watermark without impairing the performance of the host network, and the embedded watermark is robust against various types of attack.\n\n\nEvaluation Settings\n\nDatasets. For experiments, we used the well-known CIFAR-10 and Caltech-101 datasets. The CIFAR-10 dataset [22] consists of 60,000 32 \u00d7 32 color images in 10 classes, with 6,000 images per class. These images were separated into 50,000 training images and 10,000 test images. The Caltech-101 dataset [10] includes pictures of objects belonging to 101 categories; it contains about 40 to 800 images per category. The size of each image is roughly 300 \u00d7 200 pixels but we resized them in 32 \u00d7 32 for finetuning. For each category, we used 30 images for training and at most 40 of the remaining images for testing.\n\nHost network and training settings. We used the wide residual network [33] as the host network. The wide residual network is an efficient variant of the residual network [16]. Table 3 shows the structure of the wide residual network with a depth parameter N and a width parameter k. In all our experiments, we set N = 1 and k = 4, and used SGD with Nesterov momentum and cross-entropy loss in training. The initial learning rate was set at 0.1, weight decay to 5.0\u00d710 \u22124 , momentum to 0.9 and minibatch size to 64. The learning rate was dropped by a factor of 0.2 at 60, 120 and 160 epochs, and we trained for a total of 200 epochs, following the settings used in [33].\n\nWe embedded a watermark into one of the following convolution layers: the second convolution layer in the conv 2, conv 3, and conv 4 groups. In the following, we mention the location of the host layer by simply describing the conv 2, conv 3, or conv 4 group. In Table 3, the number M of parameter w is also shown for these layers. The parameter \u03bb in Eq. (1) is set to 0.01. As a watermark, we embedded b = 1 \u2208 {0, 1} T in the following experiments.  \n\n\nEmbedding Results\n\nFirst, we confirm that a watermark was successfully embedded in the host network by the proposed embedding regularizer. We trained the host network from scratch (train-toembed) on the CIFAR-10 dataset with and without embedding a watermark. In the embedding case, a 256-bit watermark (T = 256) was embedded into the conv 2 group. Figure 1 shows the training curves for the host network in CIFAR-10 as a function of epochs. Not embedded is the case that the host network is trained without the embedding regularizer. Embedded (direct), Embedded (diff), and Embedded (random) respectively represent training curves with embedding regularizers whose parameters are X direct , X diff , and X random . We can see that the training loss E(w) with a watermark becomes larger than the notembedded case if the parameters X direct and X diff are used. This large training loss is dominated by the embedding loss E R (w), which indicates that it is difficult to embed a watermark directly into a parameter or even into the difference of two parameters. On the other hand, the training loss of Embedded (random) is very close to that of Not embedded. Table 4 shows the best test errors and embedding losses E R (w) of the host networks with and without embedding.\n\n\nTest Error and Training Loss\n\nWe can see that the test errors of Not embedded and random are almost the same while those of direct and diff are slightly larger. The embedding loss E R (w) of random is extremely low compared with those of direct and diff. These results indicate that the random approach can effectively embed a watermark without impairing the performance in the original task. Figure 2 shows the histogram of the embedded watermark \u03c3(\u03a3 i X ji w i ) (before thresholding) with and without watermarks where (a) direct, (b) diff, and (c) random parameters are used in embedding and detection. If we binarize \u03c3(\u03a3 i X ji w i ) at a threshold of 0.5, all watermarks are correctly detected because \u2200j, \u03c3(\u03a3 i X ji w i ) \u2265 0.5 (\u21d4 \u03a3 i X ji w i \u2265 0) for all embedded cases. Please note that we embedded b = 1 \u2208 {0, 1} T as a watermark as mentioned before. Although random watermarks will be detected for the non-embedded cases, it can be easily judged that the watermark is not embedded because the distribution of \u03c3(\u03a3 i X ji w i ) is quite different from those for embedded cases.\n\n\nDetecting Watermarks\n\n\nDistribution of Model Parameters\n\nWe explore how trained model parameters are affected by the embedded watermarks. Figure 3 shows the distribution of model parameters W (not w) with and without watermarks. These parameters are taken only from the layer in which a watermark was embedded. Note that W is the parameter before taking the mean over filters, and thus the number of parameters is 3 \u00d7 3 \u00d7 64 \u00d7 64. We can see that direct and diff significantly alter the distribution of parameters while random does not. In direct, many parameters became large and a peak appears near 2 so that their mean over filters becomes a large positive value to reduce the embedding loss. In diff, most parameters were pushed in both positive and negative directions so that the differences between these parameters became large. In random, a watermark is diffused over all parameters with random weights and thus does not significantly alter the distribution. This is one of the desirable properties of watermarking related the security requirement; one may be aware of the existence of the embedded watermarks for the direct and diff cases.\n\nThe results so far indicated that the random approach seemed to be the best choice among the three, with low embedding loss, low test error in the original task, and not altering the parameter distribution. Therefore, in the following experiments, we used the random approach in embedding watermarks without explicitly indicating it.      \n\n\nFine-tune-to-embed and Distill-to-embed\n\nIn the above experiments, a watermark was embedded by training the host network from scratch (train-to-embed).\n\nHere, we evaluated the other two situations introduced in Section 2.2: fine-tune-to-embed and distill-to-embed. For fine-tune-to-embed, two experiments were performed. In the first experiment, the host network was trained on the CIFAR-10 dataset without embedding, and then fine-tuned on the same CIFAR-10 dataset with embedding and without embedding (for comparison). In the second experiment, the host network is trained on the Caltech-101 dataset, and then fine-tuned on the CIFAR-10 dataset with and without embedding.\n\nTable 5 (a) shows the result of the first experiment. Not embedded 1st corresponds to the first training without embedding. Not embedded 2nd corresponds to the second training without embedding and Embedded corresponds to the second training with embedding. Figure 4 shows the training curves of these fine-tunings 5 . We can see that Embedded achieved almost the same test error as Not embedded 2nd and a very low E R (w).\n\nTable 5 (b) shows the results of the second experiment. Not embedded 2nd corresponds to the second training without embedding and Embedded corresponds to the second training with embedding. The test error and training loss of the first training are not shown because they are not compatible between these two different training datasets. From these results, it was also confirmed that Embedded achieved almost the same test error as Not embedded 2nd and very low E R (w). Thus, we can say that the proposed method is effective even in the fine-tune-to-embed situation (in the same and different domains).\n\nFinally, embedding a watermark in the distill-to-embed situation was evaluated. The host network is first trained on the CIFAR-10 dataset without embedding. Then, the trained network was further fine-tuned on the same CIFAR-10 dataset with and without embedding. In this second training, the training labels of the CIFAR-10 dataset were not used. Instead, the predicted values of the trained network were used as soft targets [17]. In other words, no label was used in the second training. Table 5 (c) shows the results for the distill-to-embed situation. Not embedded 1st corresponds to the first training and Embedded (Not embedded 2nd) corresponds to the second distilling training with embedding (without embedding). It was found that the proposed method also achieved low test error and E R (w) in the distill-to-embed situation.\n\n\nCapacity of Watermark.\n\nIn this section, the capacity of the embedded watermark is explored by embedding different sizes of watermarks into different groups in the train-to-embed manner. Please note that the number of parameters w of conv 2, conv 3, and conv 4 groups were 576, 1152, and 2304, respectively. Table 6 shows test error (%) and embedding loss E R (w) for combinations of different embedded blocks and different number of embedded bits. We can see that embedded loss or test error becomes high if the number of embedded bits becomes larger than the number of parameters w (e.g. 2,048 bits in conv 3) because the embedding problem becomes overdetermined in such cases. Thus, the number of embedded bits should be smaller than the number of parameters w, which is a limitation of the embedding method using a single-layer perceptron. This limitation would be resolved by using a multi-layer perceptron in the embedding regularizer.  \n\n\nEmbedding without Training\n\nAs mentioned in Section 3.2, it is possible to embed a watermark to a host network by directly modifying the trained parameter w 0 as usually done in image domain. Here we try to do this by minimizing the following loss function instead of Eq. (1):\nE(w) = 1 2 ||w \u2212 w 0 || 2 2 + \u03bbE R (w),(6)\nwhere the embedding loss E R (w) is minimized while minimizing the difference between the modified parameter w and the original parameter w 0 . Table 7 summarizes the embedding results after Eq. (6) against the host network trained on the CIFAR-10 dataset. We can see that embedding fails for \u03bb \u2264 1 as bit error rate (BER) is larger than zero while the test error of the original task becomes too large for \u03bb > 1. Thus, it is not effective to directly embed a watermark without considering the original task.\n\n\nRobustness of Embedded Watermarks\n\nIn this section, the robustness of a proposed watermark is evaluated for the three attack types explained in Section 2.3: fine-tuning and model compression.\n\n\nRobustness against Fine-tuning\n\nFine-tuning or transfer learning [28] seems to be the most likely type of (unintentional) attack because it is frequently Table 8. Embedding loss before fine-tuning (ER(w)) and after fine-tuning (E R (w)), and the best test error (%) after fine-tuning.\n\nER(w) E R (w) Test error CIFAR-10 \u2192 CIFAR-10 4.76\u00d710 \u22124 8.66\u00d710 \u22124 7.69 Caltech-101 \u2192 CIFAR-10 5.96\u00d710 \u22123 1.56\u00d710 \u22122 7.88 performed on trained models to apply them to other but similar tasks with less effort than training a network from scratch or to avoid over-fitting when sufficient training data is not available.\n\nIn this experiment, two trainings we performed; in the first training, a 256-bit watermark was embedded in the conv 2 group in the train-to-embed manner, and then the host network was further fine-tuned in the second training without embedding, to determine whether the watermark embedded in the first training stayed in the host network or not, even after the second training (fine-tuning). Table 8 shows the embedding loss before fine-tuning (E R (w)) and after fine-tuning (E R (w)), and the best test error after fine-tuning. We evaluated fine-tuning in the same domain (CIFAR-10 \u2192 CIFAR-10) and in different domains (Caltech-101 \u2192 CIFAR-10). We can see that, in both cases, the embedding loss was slightly increased by fine-tuning but was still low. In addition, the bit error rate of the detected watermark was equal to zero in both cases. The reason why the embedding loss in fine-tuning in the different domains is higher than that in the same domain is that the Caltech-101 dataset is significantly more difficult than the CIFAR-10 dataset in our settings; all images in the Caltech-101 dataset were resized to 32 \u00d7 32 6 for compatibility with the CIFAR-10 dataset.\n\n\nRobustness against Model Compression\n\nIt is sometimes difficult to deploy deep neural networks to embedded systems or mobile devices because they are both computationally intensive and memory intensive. In order to solve this problem, the model parameters are often compressed [14,12,13]. The compression of model parameters can intentionally or unintentionally act as an attack against watermarks. In this section, we evaluate the robustness of our watermarks against model compression, in particular, against parameter pruning [14]. In parameter pruning, parameters whose absolute values are very small are cut-off to zero. In [13], quantization of weights and the Huffman coding of quantized values are further applied. Because quantization has less impact than parameter pruning and the Huffman coding is lossless compression, we focus on parameter pruning.\n\nIn order to evaluate the robustness against parameter pruning, we embedded a 256-bit watermark in the conv 2 group while training the host network on the CIFAR-10 dataset. We removed \u03b1% of the 3 \u00d7 3 \u00d7 64 \u00d7 64 parameters of the embedded layer and calculated embedding loss and bit error rate. Figure 5 (a) shows embedding loss E R (w) as a function of pruning rate \u03b1. Ascending (Descending) represents embedding loss when the top \u03b1% parameters are cut-off according to their absolute values in ascending (descending) order. Random represents embedding loss where \u03b1% of parameters are randomly removed. Ascending corresponds to parameter pruning and the others were evaluated for comparison. We can see that the embedding loss of Ascending increases more slowly than those of Descending and Random as \u03b1 increases. It is reasonable that model parameters with small absolute values have less impact on a detected watermark because the watermark is extracted from the dot product of the model parameter w and the constant embedding parameter (weight) X. Figure 5 (b) shows the bit error rate as a function of pruning rate \u03b1. Surprisingly, the bit error rate was still zero after removing 65% of the parameters and 2/256 even after 80% of the parameters were pruned (Ascending). We can say that the embedded watermark is sufficiently robust against parameter pruning because, in [13], the resulting pruning rate of convolutional layers ranged from to 16% to 65% for the AlexNet [23], and from 42% to 78% for VGGNet [28]. Furthermore, this degree of bit error can be easily corrected by an error correction code (e.g. the BCH code). Figure 6 shows the histogram of the detected watermark \u03c3(\u03a3 i X ji w i ) after pruning for \u03b1 = 0.8 and 0.95. For \u03b1 = 0.95, the histogram of the detected watermark is also shown for the host network into which no watermark is embedded. We can see that many of \u03c3(\u03a3 i X ji w i ) are still close to one for the embedded case, which might be used as a confidence score in judging the existence of a watermark (zero-bit watermarking).\n\n\nConclusions and Future Work\n\nIn this paper, we have proposed a general framework for embedding a watermark in deep neural network models to protect the rights to the trained models. First, we formulated a new problem: embedding watermarks into deep neural networks. We also defined requirements, embedding situations, and attack types for watermarking deep neural networks. Second, we proposed a general framework for embedding a watermark in model parameters using a parameter regularizer. Our approach does not impair the performance of networks into which a watermark is embedded. Finally, we performed comprehensive experiments to reveal the potential of watermarking deep neural networks as the basis of this new problem. We showed that our framework could embed a watermark without impairing the performance of a deep neural network. The embedded watermark did not disappear even after fine-tuning or parameter  pruning; the entire watermark remained even after 65% of the parameters were pruned.\n\n\nFuture Work\n\nAlthough we have obtained first insight into the new problem of embedding a watermark in deep neural networks, many things remain as future work.\n\nCompression as embedding. Compressing deep neural networks is a very important and active research topic. While we confirmed that our watermark is very robust against parameter pruning in this paper, a watermark might be embedded in conjunction with compressing models. For example, in [13], after parameter pruning, the network is re-trained to learn the final weights for the remaining sparse parameters. Our embedding regularizer can be used in this re-training to embed a watermark.\n\nNetwork morphism. In [4,32], a systematic study has been done on how to morph a well-trained neural network into a new one so that its network function can be completely preserved for further training. This network morphism can constitute a severe attack against our watermark because it may be impossible to detect the embedded watermark if the topology of the host network is severely modified. We left the investigation how the embedded watermark is affected by this network morphism for future work.\n\nSteganalysis. Steganalysis [27,21] is a method for detecting the presence of secretly hidden data (e.g. steganography or watermarks) in digital media files such as images, video, audio, and, in our case, deep neural networks. Watermarks ideally are robust against steganalysis. While, in this paper, we confirmed that embedding watermarks does not significantly change the distribution of model parameters, more exploration is needed to evaluate robustness against steganalysis. Conversely, developing effective steganalysis against watermarks for deep neural networks can be an interesting research topic.\n\nFingerprinting. Digital fingerprinting is an alternative to the watermarking approach for persistent identification of images [2], video [20,31], and audio clips [1,11]. In this paper, we focused on one of these two important approaches. Robust fingerprinting of deep neural networks is another and complementary direction to protect deep neural network models.\n\nFigure 1 .\n1Training curves for the host network on CIFAR-10 as a function of epochs. Solid lines denote test error (y-axis on the left) and dashed lines denote training loss E(w) (y-axis on the right).\n\nFigure 2 .\n2Histogram of the embedded watermark \u03c3(\u03a3iXjiwi) (before thresholding) with and without watermarks.\n\nFigure 3 .\n3Distribution of model parameters W with and without watermarks.\n\nFigure 4 .\n4Training curves for fine-tuning the host network. The first and second halves of epochs correspond to the first and second training. Solid lines denote test error (y-axis on the left) and dashed lines denote training loss (y-axis on the right).\n\nFigure 5 .\n5Embedding loss and bit error rate after pruning as a function of pruning rate.\n\nFigure 6 .\n6Histogram of the detected watermark \u03c3(\u03a3iXjiwi) after pruning.\n\nTable 1 .\n1Requirements for an effective watermarking algorithm in the image and neural network domains.\n\nTable 3 .\n3Structure of the host network.Group Output size \nBuilding block \n#w \nconv 1 \n32 \u00d7 32 \n[3 \u00d7 3, 16] \nN/A \n\nconv 2 \n32 \u00d7 32 \n\n\nTable 4 .\n4Test error (%) and embedding loss ER(w) with and without embedding.Test error \nE R (w) \nNot embedded \n8.04 \nN/A \ndirect \n8.21 \n1.24\u00d710 \u22121 \ndiff \n8.37 \n6.20\u00d710 \u22122 \nrandom \n7.97 \n4.76\u00d710 \u22124 \n\n\n\nTable 5 .\n5Test error (%) and embedding loss ER(w) with and without embedding in fine-tuning and distilling.(a) Fine-tune-to-embed (CIFAR-10 \u2192 CIFAR-10) \nTest error \nE R (w) \nNot embedded 1st \n8.04 \nN/A \nNot embedded 2nd \n7.66 \nN/A \nEmbedded \n7.70 \n4.93\u00d710 \u22124 \n(b) Fine-tune-to-embed (Caltech-101 \u2192 CIFAR-10) \nTest error \nE R (w) \nNot embedded 2nd \n7.93 \nN/A \nEmbedded \n7.94 \n4.83\u00d710 \u22124 \n(c) Distill-to-embed (CIFAR-10 \u2192 CIFAR-10) \nTest error \nE R (w) \nNot embedded 1st \n8.04 \nN/A \nNot embedded 2nd \n7.86 \nN/A \nEmbedded \n7.75 \n5.01\u00d710 \u22124 \n\n\n\nTable 6 .\n6Test error (%) and embedding loss ER(w) for the combi-\nnations of embedded groups and sizes of embedded bits. \n\n(a) Test error (%) \nEmbedded bits \nEmbedded group \nconv 2 conv 3 conv 4 \n256 \n7.97 \n7.98 \n7.92 \n512 \n8.47 \n8.22 \n7.84 \n1,024 \n8.43 \n8.12 \n7.84 \n2,048 \n8.17 \n8.93 \n7.75 \n(b) Embedding loss \nEmbedded bits \nEmbedded group \nconv 2 \nconv 3 \nconv 4 \n256 \n4.76\u00d710 \u22124 7.20\u00d710 \u22124 1.10\u00d710 \u22122 \n512 \n8.11\u00d710 \u22124 8.18\u00d710 \u22124 1.25\u00d710 \u22122 \n1,024 \n6.74\u00d710 \u22122 1.53\u00d710 \u22123 1.53\u00d710 \u22122 \n2,048 \n5.35\u00d710 \u22121 3.70\u00d710 \u22122 3.06\u00d710 \u22122 \n\n\n\nTable 7 .\n7Losses, test error, and bit error rate (BER) after embed-\nding a watermark with different \u03bb. \n\n\u03bb \n\n1 \n\n2 ||w \u2212 w 0 || 2 \n\n2 \n\nE R (w) Test error BER \n0 \n0.000 \n1.066 \n8.04 \n0.531 \n1 \n0.184 \n0.609 \n8.52 \n0.324 \n10 \n1.652 \n0.171 \n10.57 \n0.000 \n100 \n7.989 \n0.029 \n13.00 \n0.000 \n\n\nAlthough this single-layer perceptron can be deepened into multilayer perceptron, we focus on the simplest one in this paper.\nNote that the learning rate was also initialized to 0.1 at the beginning of the second training, while the learning rate was reduced to 8.0 \u00d7 10 \u22124 ) at the end of the first training.\nThis size is extremely small compared with their original sizes (roughly 300 \u00d7 200).\n\nMask: Robust local features for audio fingerprinting. X Anguera, A Garzon, T Adamek, Proc. of ICME. of ICMEX. Anguera, A. Garzon, and T. Adamek. Mask: Robust local features for audio fingerprinting. In Proc. of ICME, 2012.\n\nUsing digital watermarks with image signatures to mitigate the threat of the copy attack. J Barr, B Bradley, B T Hannigan, Proc. of ICASSP. of ICASSPJ. Barr, B. Bradley, and B. T. Hannigan. Using digital wa- termarks with image signatures to mitigate the threat of the copy attack. In Proc. of ICASSP, pages 69-72, 2003.\n\nTheano: a CPU and GPU math expression compiler. J Bergstra, O Breuleux, F Bastien, P Lamblin, R Pascanu, G Desjardins, J Turian, D Warde-Farley, Y Bengio, Proc. of the Python for Scientific Computing Conference (SciPy). of the Python for Scientific Computing Conference (SciPy)J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proc. of the Python for Scientific Computing Conference (SciPy), 2010.\n\nNet2net: Accelerating learning via knowledge transfer. T Chen, I Goodfellow, J Shlens, Proc. of ICLR. of ICLRT. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating learning via knowledge transfer. In Proc. of ICLR, 2016.\n\nF Chollet, Keras. GitHub repository. F. Chollet. Keras. GitHub repository, 2015.\n\nThe loss surfaces of multilayer networks. A Choromanska, M Henaff, M Mathieu, G Arous, Y Lecun, Proc. of AISTATS. of AISTATSA. Choromanska, M. Henaff, M. Mathieu, G. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Proc. of AISTATS, 2015.\n\nTorch7: A matlab-like environment for machine learning. R Collobert, K Kavukcuoglu, C Farabet, Proc. of NIPS Workshop on BigLearn. of NIPS Workshop on BigLearnR. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In Proc. of NIPS Workshop on BigLearn, 2011.\n\nDigital Watermarking and Steganography. I Cox, M Miller, J Bloom, J Fridrich, T Kalker, Morgan Kaufmann Publishers Inc2 editionI. Cox, M. Miller, J. Bloom, J. Fridrich, and T. Kalker. Dig- ital Watermarking and Steganography. Morgan Kaufmann Publishers Inc., 2 edition, 2008.\n\nIdentifying and attacking the saddle point problem in high-dimensional non-convex optimization. Y Dauphin, R Pascanu, C Gulcehre, K Cho, S Ganguli, Y Bengio, Proc. of NIPS. of NIPSY. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Proc. of NIPS, 2014.\n\nLearning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories. L Fei-Fei, R Fergus, P Perona, Proc. of CVPR Workshop on Generative-Model Based Vision. of CVPR Workshop on Generative-Model Based VisionL. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: an incremen- tal bayesian approach tested on 101 object categories. In Proc. of CVPR Workshop on Generative-Model Based Vi- sion, 2004.\n\nA highly robust audio fingerprinting system. J Haitsma, T Kalker, Proc. of ISMIR. of ISMIRJ. Haitsma and T. Kalker. A highly robust audio fingerprint- ing system. In Proc. of ISMIR, pages 107-115, 2002.\n\nEie: Efficient inference engine on compressed deep neural network. S Han, X Liu, H Mao, J Pu, A Pedram, M A Horowitz, W J Dally, Proc. of ISCA. of ISCAS. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally. Eie: Efficient inference engine on com- pressed deep neural network. In Proc. of ISCA, 2016.\n\nDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. S Han, H Mao, W J Dally, Proc. of ICLR. of ICLRS. Han, H. Mao, and W. J. Dally. Deep compression: Com- pressing deep neural networks with pruning, trained quanti- zation and huffman coding. In Proc. of ICLR, 2016.\n\nLearning both weights and connections for efficient neural networks. S Han, J Pool, J Tran, W J Dally, Proc. of NIPS. of NIPSS. Han, J. Pool, J. Tran, and W. J. Dally. Learning both weights and connections for efficient neural networks. In Proc. of NIPS, 2015.\n\nMultimedia watermarking techniques. F Hartung, M Kutter, Proceedings of the IEEE. the IEEE87F. Hartung and M. Kutter. Multimedia watermarking tech- niques. Proceedings of the IEEE, 87(7):1079-1107, 1999.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. of CVPR. of CVPRK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. of CVPR, 2016.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, Proc. of NIPS Workshop on Deep Learning and Representation Learning. of NIPS Workshop on Deep Learning and Representation LearningG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In Proc. of NIPS Workshop on Deep Learning and Representation Learning, 2014.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997.\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, Proc. of MM. of MMY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of MM, 2014.\n\nContent-based video copy detection in large databases: a local fingerprints statistical similarity search approach. A Joly, C Frelicot, O Buisson, Proc. of ICIP. of ICIPA. Joly, C. Frelicot, and O. Buisson. Content-based video copy detection in large databases: a local fingerprints sta- tistical similarity search approach. In Proc. of ICIP, pages 505-508, 2005.\n\nEnsemble classifiers for steganalysis of digital media. J Kodovsky, J Fridrich, V Holub, IEEE Trans. on Information Forensics and Security. 72J. Kodovsky, J. Fridrich, and V. Holub. Ensemble classifiers for steganalysis of digital media. IEEE Trans. on Information Forensics and Security, 7(2):432-444, 2012.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, Tech ReportA. Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. of NIPS. of NIPSA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Proc. of NIPS, 2012.\n\nA simple weight decay can improve generalization. A Krogh, J A Hertz, Proc. of NIPS. of NIPSA. Krogh and J. A. Hertz. A simple weight decay can im- prove generalization. In Proc. of NIPS, 1992.\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998.\n\nTensorflow: Large-scale machine learning on heterogeneous distributed systems. M Abadi, arXiv:1603.04467M. Abadi, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv:1603.04467, 2016.\n\nNeural network based steganalysis in still images. L Shaohui, Y Hongxun, G Wen, Proc. of ICME. of ICMEL. Shaohui, Y. Hongxun, and G. Wen. Neural network based steganalysis in still images. In Proc. of ICME, 2003.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, Proc. of ICLR. of ICLRK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In Proc. of ICLR, 2015.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proc. of CVPR. of CVPRC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proc. of CVPR, 2015.\n\nChainer: a next-generation open source framework for deep learning. S Tokui, K Oono, S Hido, J Clayton, Proc. of NIPS Workshop on Machine Learning Systems. of NIPS Workshop on Machine Learning SystemsS. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a next-generation open source framework for deep learning. In Proc. of NIPS Workshop on Machine Learning Systems, 2015.\n\nAccurate contentbased video copy detection with efficient feature indexing. Y Uchida, M Agrawal, S Sakazawa, Proc. of ICMR. of ICMRY. Uchida, M. Agrawal, and S. Sakazawa. Accurate content- based video copy detection with efficient feature indexing. In Proc. of ICMR, 2011.\n\nNetwork morphism. T Wei, C Wang, Y Rui, C W Chen, Proc. of ICML. of ICMLT. Wei, C. Wang, Y. Rui, and C. W. Chen. Network mor- phism. In Proc. of ICML, 2016.\n\nWide residual networks. S Zagoruyko, N Komodakis, Proc. of ECCV. of ECCVS. Zagoruyko and N. Komodakis. Wide residual networks. In Proc. of ECCV, 2016.\n", "annotations": {"author": "[{\"end\":97,\"start\":50},{\"end\":142,\"start\":98},{\"end\":258,\"start\":143}]", "publisher": null, "author_last_name": "[{\"end\":63,\"start\":57},{\"end\":108,\"start\":103},{\"end\":161,\"start\":153}]", "author_first_name": "[{\"end\":56,\"start\":50},{\"end\":102,\"start\":98},{\"end\":152,\"start\":143}]", "author_affiliation": "[{\"end\":96,\"start\":65},{\"end\":141,\"start\":110},{\"end\":257,\"start\":163}]", "title": "[{\"end\":47,\"start\":1},{\"end\":305,\"start\":259}]", "venue": null, "abstract": "[{\"end\":1728,\"start\":307}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1883,\"start\":1879},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1897,\"start\":1893},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1910,\"start\":1906},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1926,\"start\":1922},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1943,\"start\":1939},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2272,\"start\":2268},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2284,\"start\":2281},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2295,\"start\":2292},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2309,\"start\":2305},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2327,\"start\":2323},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2342,\"start\":2339},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2680,\"start\":2676},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2984,\"start\":2980},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5360,\"start\":5356},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5362,\"start\":5360},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6210,\"start\":6206},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7334,\"start\":7330},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8651,\"start\":8647},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10167,\"start\":10163},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10355,\"start\":10351},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11488,\"start\":11487},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12444,\"start\":12440},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13354,\"start\":13353},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14334,\"start\":14331},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14336,\"start\":14334},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14654,\"start\":14650},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16174,\"start\":16170},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16367,\"start\":16363},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16750,\"start\":16746},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16850,\"start\":16846},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17344,\"start\":17340},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22648,\"start\":22647},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23793,\"start\":23789},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26240,\"start\":26236},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28234,\"start\":28230},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28237,\"start\":28234},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28240,\"start\":28237},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28486,\"start\":28482},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28586,\"start\":28582},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30193,\"start\":30189},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30292,\"start\":30288},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30329,\"start\":30325},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32327,\"start\":32323},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32549,\"start\":32546},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32552,\"start\":32549},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33061,\"start\":33057},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33064,\"start\":33061},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33767,\"start\":33764},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33779,\"start\":33775},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":33782,\"start\":33779},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33803,\"start\":33800},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33806,\"start\":33803}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34203,\"start\":34000},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34314,\"start\":34204},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34391,\"start\":34315},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34649,\"start\":34392},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34741,\"start\":34650},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34816,\"start\":34742},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34922,\"start\":34817},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35057,\"start\":34923},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35260,\"start\":35058},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35802,\"start\":35261},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36332,\"start\":35803},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36621,\"start\":36333}]", "paragraph": "[{\"end\":2343,\"start\":1744},{\"end\":3625,\"start\":2345},{\"end\":4096,\"start\":3627},{\"end\":4160,\"start\":4098},{\"end\":4345,\"start\":4162},{\"end\":4544,\"start\":4347},{\"end\":4647,\"start\":4546},{\"end\":5053,\"start\":4671},{\"end\":5521,\"start\":5055},{\"end\":5943,\"start\":5538},{\"end\":6324,\"start\":5945},{\"end\":6723,\"start\":6349},{\"end\":6804,\"start\":6725},{\"end\":6956,\"start\":6806},{\"end\":7198,\"start\":6958},{\"end\":7738,\"start\":7200},{\"end\":8238,\"start\":7740},{\"end\":8612,\"start\":8264},{\"end\":8950,\"start\":8614},{\"end\":9060,\"start\":8952},{\"end\":9108,\"start\":9077},{\"end\":9188,\"start\":9110},{\"end\":9429,\"start\":9190},{\"end\":9540,\"start\":9431},{\"end\":9779,\"start\":9553},{\"end\":9845,\"start\":9781},{\"end\":10025,\"start\":9847},{\"end\":10356,\"start\":10048},{\"end\":11151,\"start\":10378},{\"end\":11294,\"start\":11207},{\"end\":12129,\"start\":11320},{\"end\":13149,\"start\":12160},{\"end\":13211,\"start\":13181},{\"end\":13356,\"start\":13238},{\"end\":13487,\"start\":13358},{\"end\":13616,\"start\":13554},{\"end\":13639,\"start\":13638},{\"end\":14515,\"start\":13641},{\"end\":14941,\"start\":14542},{\"end\":15124,\"start\":14943},{\"end\":15635,\"start\":15142},{\"end\":16040,\"start\":15831},{\"end\":16674,\"start\":16064},{\"end\":17345,\"start\":16676},{\"end\":17797,\"start\":17347},{\"end\":19070,\"start\":17819},{\"end\":20159,\"start\":19103},{\"end\":21311,\"start\":20219},{\"end\":21652,\"start\":21313},{\"end\":21806,\"start\":21696},{\"end\":22330,\"start\":21808},{\"end\":22755,\"start\":22332},{\"end\":23361,\"start\":22757},{\"end\":24197,\"start\":23363},{\"end\":25143,\"start\":24224},{\"end\":25422,\"start\":25174},{\"end\":25974,\"start\":25466},{\"end\":26168,\"start\":26012},{\"end\":26455,\"start\":26203},{\"end\":26774,\"start\":26457},{\"end\":27950,\"start\":26776},{\"end\":28814,\"start\":27991},{\"end\":30869,\"start\":28816},{\"end\":31874,\"start\":30901},{\"end\":32035,\"start\":31890},{\"end\":32523,\"start\":32037},{\"end\":33028,\"start\":32525},{\"end\":33636,\"start\":33030},{\"end\":33999,\"start\":33638}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11206,\"start\":11152},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12159,\"start\":12130},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13180,\"start\":13150},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13237,\"start\":13212},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13553,\"start\":13488},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13637,\"start\":13617},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15141,\"start\":15125},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15816,\"start\":15636},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25465,\"start\":25423}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5268,\"start\":5261},{\"end\":6486,\"start\":6479},{\"end\":6495,\"start\":6488},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16859,\"start\":16852},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17616,\"start\":17609},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18965,\"start\":18958},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23860,\"start\":23853},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24515,\"start\":24508},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25617,\"start\":25610},{\"end\":26332,\"start\":26325},{\"end\":27175,\"start\":27168}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1742,\"start\":1730},{\"attributes\":{\"n\":\"2.\"},\"end\":4669,\"start\":4650},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5536,\"start\":5524},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6347,\"start\":6327},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8262,\"start\":8241},{\"end\":9075,\"start\":9063},{\"end\":9551,\"start\":9543},{\"attributes\":{\"n\":\"3.\"},\"end\":10046,\"start\":10028},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10376,\"start\":10359},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11318,\"start\":11297},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14540,\"start\":14518},{\"attributes\":{\"n\":\"4.\"},\"end\":15829,\"start\":15818},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16062,\"start\":16043},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17817,\"start\":17800},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":19101,\"start\":19073},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":20182,\"start\":20162},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":20217,\"start\":20185},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":21694,\"start\":21655},{\"attributes\":{\"n\":\"4.2.5\"},\"end\":24222,\"start\":24200},{\"attributes\":{\"n\":\"4.2.6\"},\"end\":25172,\"start\":25146},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26010,\"start\":25977},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":26201,\"start\":26171},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":27989,\"start\":27953},{\"attributes\":{\"n\":\"5.\"},\"end\":30899,\"start\":30872},{\"attributes\":{\"n\":\"5.1.\"},\"end\":31888,\"start\":31877},{\"end\":34011,\"start\":34001},{\"end\":34215,\"start\":34205},{\"end\":34326,\"start\":34316},{\"end\":34403,\"start\":34393},{\"end\":34661,\"start\":34651},{\"end\":34753,\"start\":34743},{\"end\":34827,\"start\":34818},{\"end\":34933,\"start\":34924},{\"end\":35068,\"start\":35059},{\"end\":35271,\"start\":35262},{\"end\":35813,\"start\":35804},{\"end\":36343,\"start\":36334}]", "table": "[{\"end\":35057,\"start\":34965},{\"end\":35260,\"start\":35137},{\"end\":35802,\"start\":35370},{\"end\":36332,\"start\":35815},{\"end\":36621,\"start\":36345}]", "figure_caption": "[{\"end\":34203,\"start\":34013},{\"end\":34314,\"start\":34217},{\"end\":34391,\"start\":34328},{\"end\":34649,\"start\":34405},{\"end\":34741,\"start\":34663},{\"end\":34816,\"start\":34755},{\"end\":34922,\"start\":34829},{\"end\":34965,\"start\":34935},{\"end\":35137,\"start\":35070},{\"end\":35370,\"start\":35273}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18157,\"start\":18149},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19474,\"start\":19466},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20308,\"start\":20300},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22598,\"start\":22590},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":29116,\"start\":29108},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":29873,\"start\":29865},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":30450,\"start\":30442}]", "bib_author_first_name": "[{\"end\":37073,\"start\":37072},{\"end\":37084,\"start\":37083},{\"end\":37094,\"start\":37093},{\"end\":37333,\"start\":37332},{\"end\":37341,\"start\":37340},{\"end\":37352,\"start\":37351},{\"end\":37354,\"start\":37353},{\"end\":37613,\"start\":37612},{\"end\":37625,\"start\":37624},{\"end\":37637,\"start\":37636},{\"end\":37648,\"start\":37647},{\"end\":37659,\"start\":37658},{\"end\":37670,\"start\":37669},{\"end\":37684,\"start\":37683},{\"end\":37694,\"start\":37693},{\"end\":37710,\"start\":37709},{\"end\":38140,\"start\":38139},{\"end\":38148,\"start\":38147},{\"end\":38162,\"start\":38161},{\"end\":38313,\"start\":38312},{\"end\":38437,\"start\":38436},{\"end\":38452,\"start\":38451},{\"end\":38462,\"start\":38461},{\"end\":38473,\"start\":38472},{\"end\":38482,\"start\":38481},{\"end\":38708,\"start\":38707},{\"end\":38721,\"start\":38720},{\"end\":38736,\"start\":38735},{\"end\":38999,\"start\":38998},{\"end\":39006,\"start\":39005},{\"end\":39016,\"start\":39015},{\"end\":39025,\"start\":39024},{\"end\":39037,\"start\":39036},{\"end\":39332,\"start\":39331},{\"end\":39343,\"start\":39342},{\"end\":39354,\"start\":39353},{\"end\":39366,\"start\":39365},{\"end\":39373,\"start\":39372},{\"end\":39384,\"start\":39383},{\"end\":39737,\"start\":39736},{\"end\":39748,\"start\":39747},{\"end\":39758,\"start\":39757},{\"end\":40156,\"start\":40155},{\"end\":40167,\"start\":40166},{\"end\":40382,\"start\":40381},{\"end\":40389,\"start\":40388},{\"end\":40396,\"start\":40395},{\"end\":40403,\"start\":40402},{\"end\":40409,\"start\":40408},{\"end\":40419,\"start\":40418},{\"end\":40421,\"start\":40420},{\"end\":40433,\"start\":40432},{\"end\":40435,\"start\":40434},{\"end\":40741,\"start\":40740},{\"end\":40748,\"start\":40747},{\"end\":40755,\"start\":40754},{\"end\":40757,\"start\":40756},{\"end\":41025,\"start\":41024},{\"end\":41032,\"start\":41031},{\"end\":41040,\"start\":41039},{\"end\":41048,\"start\":41047},{\"end\":41050,\"start\":41049},{\"end\":41254,\"start\":41253},{\"end\":41265,\"start\":41264},{\"end\":41469,\"start\":41468},{\"end\":41475,\"start\":41474},{\"end\":41484,\"start\":41483},{\"end\":41491,\"start\":41490},{\"end\":41674,\"start\":41673},{\"end\":41684,\"start\":41683},{\"end\":41695,\"start\":41694},{\"end\":42018,\"start\":42017},{\"end\":42032,\"start\":42031},{\"end\":42232,\"start\":42231},{\"end\":42239,\"start\":42238},{\"end\":42252,\"start\":42251},{\"end\":42263,\"start\":42262},{\"end\":42274,\"start\":42273},{\"end\":42282,\"start\":42281},{\"end\":42294,\"start\":42293},{\"end\":42308,\"start\":42307},{\"end\":42639,\"start\":42638},{\"end\":42647,\"start\":42646},{\"end\":42659,\"start\":42658},{\"end\":42944,\"start\":42943},{\"end\":42956,\"start\":42955},{\"end\":42968,\"start\":42967},{\"end\":43253,\"start\":43252},{\"end\":43433,\"start\":43432},{\"end\":43447,\"start\":43446},{\"end\":43460,\"start\":43459},{\"end\":43462,\"start\":43461},{\"end\":43681,\"start\":43680},{\"end\":43690,\"start\":43689},{\"end\":43692,\"start\":43691},{\"end\":43907,\"start\":43906},{\"end\":43916,\"start\":43915},{\"end\":43926,\"start\":43925},{\"end\":43936,\"start\":43935},{\"end\":44186,\"start\":44185},{\"end\":44383,\"start\":44382},{\"end\":44394,\"start\":44393},{\"end\":44405,\"start\":44404},{\"end\":44614,\"start\":44613},{\"end\":44626,\"start\":44625},{\"end\":44816,\"start\":44815},{\"end\":44827,\"start\":44826},{\"end\":44834,\"start\":44833},{\"end\":44841,\"start\":44840},{\"end\":44853,\"start\":44852},{\"end\":44861,\"start\":44860},{\"end\":44873,\"start\":44872},{\"end\":44882,\"start\":44881},{\"end\":44895,\"start\":44894},{\"end\":45162,\"start\":45161},{\"end\":45171,\"start\":45170},{\"end\":45179,\"start\":45178},{\"end\":45187,\"start\":45186},{\"end\":45544,\"start\":45543},{\"end\":45554,\"start\":45553},{\"end\":45565,\"start\":45564},{\"end\":45760,\"start\":45759},{\"end\":45767,\"start\":45766},{\"end\":45775,\"start\":45774},{\"end\":45782,\"start\":45781},{\"end\":45784,\"start\":45783},{\"end\":45924,\"start\":45923},{\"end\":45937,\"start\":45936}]", "bib_author_last_name": "[{\"end\":37081,\"start\":37074},{\"end\":37091,\"start\":37085},{\"end\":37101,\"start\":37095},{\"end\":37338,\"start\":37334},{\"end\":37349,\"start\":37342},{\"end\":37363,\"start\":37355},{\"end\":37622,\"start\":37614},{\"end\":37634,\"start\":37626},{\"end\":37645,\"start\":37638},{\"end\":37656,\"start\":37649},{\"end\":37667,\"start\":37660},{\"end\":37681,\"start\":37671},{\"end\":37691,\"start\":37685},{\"end\":37707,\"start\":37695},{\"end\":37717,\"start\":37711},{\"end\":38145,\"start\":38141},{\"end\":38159,\"start\":38149},{\"end\":38169,\"start\":38163},{\"end\":38321,\"start\":38314},{\"end\":38449,\"start\":38438},{\"end\":38459,\"start\":38453},{\"end\":38470,\"start\":38463},{\"end\":38479,\"start\":38474},{\"end\":38488,\"start\":38483},{\"end\":38718,\"start\":38709},{\"end\":38733,\"start\":38722},{\"end\":38744,\"start\":38737},{\"end\":39003,\"start\":39000},{\"end\":39013,\"start\":39007},{\"end\":39022,\"start\":39017},{\"end\":39034,\"start\":39026},{\"end\":39044,\"start\":39038},{\"end\":39340,\"start\":39333},{\"end\":39351,\"start\":39344},{\"end\":39363,\"start\":39355},{\"end\":39370,\"start\":39367},{\"end\":39381,\"start\":39374},{\"end\":39391,\"start\":39385},{\"end\":39745,\"start\":39738},{\"end\":39755,\"start\":39749},{\"end\":39765,\"start\":39759},{\"end\":40164,\"start\":40157},{\"end\":40174,\"start\":40168},{\"end\":40386,\"start\":40383},{\"end\":40393,\"start\":40390},{\"end\":40400,\"start\":40397},{\"end\":40406,\"start\":40404},{\"end\":40416,\"start\":40410},{\"end\":40430,\"start\":40422},{\"end\":40441,\"start\":40436},{\"end\":40745,\"start\":40742},{\"end\":40752,\"start\":40749},{\"end\":40763,\"start\":40758},{\"end\":41029,\"start\":41026},{\"end\":41037,\"start\":41033},{\"end\":41045,\"start\":41041},{\"end\":41056,\"start\":41051},{\"end\":41262,\"start\":41255},{\"end\":41272,\"start\":41266},{\"end\":41472,\"start\":41470},{\"end\":41481,\"start\":41476},{\"end\":41488,\"start\":41485},{\"end\":41495,\"start\":41492},{\"end\":41681,\"start\":41675},{\"end\":41692,\"start\":41685},{\"end\":41700,\"start\":41696},{\"end\":42029,\"start\":42019},{\"end\":42044,\"start\":42033},{\"end\":42236,\"start\":42233},{\"end\":42249,\"start\":42240},{\"end\":42260,\"start\":42253},{\"end\":42271,\"start\":42264},{\"end\":42279,\"start\":42275},{\"end\":42291,\"start\":42283},{\"end\":42305,\"start\":42295},{\"end\":42316,\"start\":42309},{\"end\":42644,\"start\":42640},{\"end\":42656,\"start\":42648},{\"end\":42667,\"start\":42660},{\"end\":42953,\"start\":42945},{\"end\":42965,\"start\":42957},{\"end\":42974,\"start\":42969},{\"end\":43264,\"start\":43254},{\"end\":43444,\"start\":43434},{\"end\":43457,\"start\":43448},{\"end\":43469,\"start\":43463},{\"end\":43687,\"start\":43682},{\"end\":43698,\"start\":43693},{\"end\":43913,\"start\":43908},{\"end\":43923,\"start\":43917},{\"end\":43933,\"start\":43927},{\"end\":43944,\"start\":43937},{\"end\":44192,\"start\":44187},{\"end\":44391,\"start\":44384},{\"end\":44402,\"start\":44395},{\"end\":44409,\"start\":44406},{\"end\":44623,\"start\":44615},{\"end\":44636,\"start\":44627},{\"end\":44824,\"start\":44817},{\"end\":44831,\"start\":44828},{\"end\":44838,\"start\":44835},{\"end\":44850,\"start\":44842},{\"end\":44858,\"start\":44854},{\"end\":44870,\"start\":44862},{\"end\":44879,\"start\":44874},{\"end\":44892,\"start\":44883},{\"end\":44906,\"start\":44896},{\"end\":45168,\"start\":45163},{\"end\":45176,\"start\":45172},{\"end\":45184,\"start\":45180},{\"end\":45195,\"start\":45188},{\"end\":45551,\"start\":45545},{\"end\":45562,\"start\":45555},{\"end\":45574,\"start\":45566},{\"end\":45764,\"start\":45761},{\"end\":45772,\"start\":45768},{\"end\":45779,\"start\":45776},{\"end\":45789,\"start\":45785},{\"end\":45934,\"start\":45925},{\"end\":45947,\"start\":45938}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6631105},\"end\":37240,\"start\":37018},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16155254},\"end\":37562,\"start\":37242},{\"attributes\":{\"id\":\"b2\"},\"end\":38082,\"start\":37564},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6702706},\"end\":38310,\"start\":38084},{\"attributes\":{\"id\":\"b4\"},\"end\":38392,\"start\":38312},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2266226},\"end\":38649,\"start\":38394},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14365368},\"end\":38956,\"start\":38651},{\"attributes\":{\"id\":\"b7\"},\"end\":39233,\"start\":38958},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11657534},\"end\":39606,\"start\":39235},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2156851},\"end\":40108,\"start\":39608},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13284355},\"end\":40312,\"start\":40110},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1663491},\"end\":40632,\"start\":40314},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2134321},\"end\":40953,\"start\":40634},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2238772},\"end\":41215,\"start\":40955},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7752381},\"end\":41420,\"start\":41217},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":41625,\"start\":41422},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7200347},\"end\":41991,\"start\":41627},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1915014},\"end\":42167,\"start\":41993},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1799558},\"end\":42520,\"start\":42169},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12487737},\"end\":42885,\"start\":42522},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15788291},\"end\":43195,\"start\":42887},{\"attributes\":{\"id\":\"b21\"},\"end\":43365,\"start\":43197},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195908774},\"end\":43628,\"start\":43367},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10137788},\"end\":43823,\"start\":43630},{\"attributes\":{\"id\":\"b24\"},\"end\":44104,\"start\":43825},{\"attributes\":{\"doi\":\"arXiv:1603.04467\",\"id\":\"b25\"},\"end\":44329,\"start\":44106},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14532900},\"end\":44543,\"start\":44331},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14124313},\"end\":44781,\"start\":44545},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206592484},\"end\":45091,\"start\":44783},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":40821847},\"end\":45465,\"start\":45093},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4647472},\"end\":45739,\"start\":45467},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":395154},\"end\":45897,\"start\":45741},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15276198},\"end\":46049,\"start\":45899}]", "bib_title": "[{\"end\":37070,\"start\":37018},{\"end\":37330,\"start\":37242},{\"end\":37610,\"start\":37564},{\"end\":38137,\"start\":38084},{\"end\":38434,\"start\":38394},{\"end\":38705,\"start\":38651},{\"end\":39329,\"start\":39235},{\"end\":39734,\"start\":39608},{\"end\":40153,\"start\":40110},{\"end\":40379,\"start\":40314},{\"end\":40738,\"start\":40634},{\"end\":41022,\"start\":40955},{\"end\":41251,\"start\":41217},{\"end\":41466,\"start\":41422},{\"end\":41671,\"start\":41627},{\"end\":42015,\"start\":41993},{\"end\":42229,\"start\":42169},{\"end\":42636,\"start\":42522},{\"end\":42941,\"start\":42887},{\"end\":43430,\"start\":43367},{\"end\":43678,\"start\":43630},{\"end\":44380,\"start\":44331},{\"end\":44611,\"start\":44545},{\"end\":44813,\"start\":44783},{\"end\":45159,\"start\":45093},{\"end\":45541,\"start\":45467},{\"end\":45757,\"start\":45741},{\"end\":45921,\"start\":45899}]", "bib_author": "[{\"end\":37083,\"start\":37072},{\"end\":37093,\"start\":37083},{\"end\":37103,\"start\":37093},{\"end\":37340,\"start\":37332},{\"end\":37351,\"start\":37340},{\"end\":37365,\"start\":37351},{\"end\":37624,\"start\":37612},{\"end\":37636,\"start\":37624},{\"end\":37647,\"start\":37636},{\"end\":37658,\"start\":37647},{\"end\":37669,\"start\":37658},{\"end\":37683,\"start\":37669},{\"end\":37693,\"start\":37683},{\"end\":37709,\"start\":37693},{\"end\":37719,\"start\":37709},{\"end\":38147,\"start\":38139},{\"end\":38161,\"start\":38147},{\"end\":38171,\"start\":38161},{\"end\":38323,\"start\":38312},{\"end\":38451,\"start\":38436},{\"end\":38461,\"start\":38451},{\"end\":38472,\"start\":38461},{\"end\":38481,\"start\":38472},{\"end\":38490,\"start\":38481},{\"end\":38720,\"start\":38707},{\"end\":38735,\"start\":38720},{\"end\":38746,\"start\":38735},{\"end\":39005,\"start\":38998},{\"end\":39015,\"start\":39005},{\"end\":39024,\"start\":39015},{\"end\":39036,\"start\":39024},{\"end\":39046,\"start\":39036},{\"end\":39342,\"start\":39331},{\"end\":39353,\"start\":39342},{\"end\":39365,\"start\":39353},{\"end\":39372,\"start\":39365},{\"end\":39383,\"start\":39372},{\"end\":39393,\"start\":39383},{\"end\":39747,\"start\":39736},{\"end\":39757,\"start\":39747},{\"end\":39767,\"start\":39757},{\"end\":40166,\"start\":40155},{\"end\":40176,\"start\":40166},{\"end\":40388,\"start\":40381},{\"end\":40395,\"start\":40388},{\"end\":40402,\"start\":40395},{\"end\":40408,\"start\":40402},{\"end\":40418,\"start\":40408},{\"end\":40432,\"start\":40418},{\"end\":40443,\"start\":40432},{\"end\":40747,\"start\":40740},{\"end\":40754,\"start\":40747},{\"end\":40765,\"start\":40754},{\"end\":41031,\"start\":41024},{\"end\":41039,\"start\":41031},{\"end\":41047,\"start\":41039},{\"end\":41058,\"start\":41047},{\"end\":41264,\"start\":41253},{\"end\":41274,\"start\":41264},{\"end\":41474,\"start\":41468},{\"end\":41483,\"start\":41474},{\"end\":41490,\"start\":41483},{\"end\":41497,\"start\":41490},{\"end\":41683,\"start\":41673},{\"end\":41694,\"start\":41683},{\"end\":41702,\"start\":41694},{\"end\":42031,\"start\":42017},{\"end\":42046,\"start\":42031},{\"end\":42238,\"start\":42231},{\"end\":42251,\"start\":42238},{\"end\":42262,\"start\":42251},{\"end\":42273,\"start\":42262},{\"end\":42281,\"start\":42273},{\"end\":42293,\"start\":42281},{\"end\":42307,\"start\":42293},{\"end\":42318,\"start\":42307},{\"end\":42646,\"start\":42638},{\"end\":42658,\"start\":42646},{\"end\":42669,\"start\":42658},{\"end\":42955,\"start\":42943},{\"end\":42967,\"start\":42955},{\"end\":42976,\"start\":42967},{\"end\":43266,\"start\":43252},{\"end\":43446,\"start\":43432},{\"end\":43459,\"start\":43446},{\"end\":43471,\"start\":43459},{\"end\":43689,\"start\":43680},{\"end\":43700,\"start\":43689},{\"end\":43915,\"start\":43906},{\"end\":43925,\"start\":43915},{\"end\":43935,\"start\":43925},{\"end\":43946,\"start\":43935},{\"end\":44194,\"start\":44185},{\"end\":44393,\"start\":44382},{\"end\":44404,\"start\":44393},{\"end\":44411,\"start\":44404},{\"end\":44625,\"start\":44613},{\"end\":44638,\"start\":44625},{\"end\":44826,\"start\":44815},{\"end\":44833,\"start\":44826},{\"end\":44840,\"start\":44833},{\"end\":44852,\"start\":44840},{\"end\":44860,\"start\":44852},{\"end\":44872,\"start\":44860},{\"end\":44881,\"start\":44872},{\"end\":44894,\"start\":44881},{\"end\":44908,\"start\":44894},{\"end\":45170,\"start\":45161},{\"end\":45178,\"start\":45170},{\"end\":45186,\"start\":45178},{\"end\":45197,\"start\":45186},{\"end\":45553,\"start\":45543},{\"end\":45564,\"start\":45553},{\"end\":45576,\"start\":45564},{\"end\":45766,\"start\":45759},{\"end\":45774,\"start\":45766},{\"end\":45781,\"start\":45774},{\"end\":45791,\"start\":45781},{\"end\":45936,\"start\":45923},{\"end\":45949,\"start\":45936}]", "bib_venue": "[{\"end\":37125,\"start\":37118},{\"end\":37391,\"start\":37382},{\"end\":37841,\"start\":37784},{\"end\":38193,\"start\":38186},{\"end\":38518,\"start\":38508},{\"end\":38810,\"start\":38782},{\"end\":39415,\"start\":39408},{\"end\":39873,\"start\":39824},{\"end\":40200,\"start\":40192},{\"end\":40465,\"start\":40458},{\"end\":40787,\"start\":40780},{\"end\":41080,\"start\":41073},{\"end\":41307,\"start\":41299},{\"end\":41519,\"start\":41512},{\"end\":41832,\"start\":41771},{\"end\":42336,\"start\":42331},{\"end\":42691,\"start\":42684},{\"end\":43493,\"start\":43486},{\"end\":43722,\"start\":43715},{\"end\":44433,\"start\":44426},{\"end\":44660,\"start\":44653},{\"end\":44930,\"start\":44923},{\"end\":45293,\"start\":45249},{\"end\":45598,\"start\":45591},{\"end\":45813,\"start\":45806},{\"end\":45971,\"start\":45964},{\"end\":37116,\"start\":37103},{\"end\":37380,\"start\":37365},{\"end\":37782,\"start\":37719},{\"end\":38184,\"start\":38171},{\"end\":38347,\"start\":38323},{\"end\":38506,\"start\":38490},{\"end\":38780,\"start\":38746},{\"end\":38996,\"start\":38958},{\"end\":39406,\"start\":39393},{\"end\":39822,\"start\":39767},{\"end\":40190,\"start\":40176},{\"end\":40456,\"start\":40443},{\"end\":40778,\"start\":40765},{\"end\":41071,\"start\":41058},{\"end\":41297,\"start\":41274},{\"end\":41510,\"start\":41497},{\"end\":41769,\"start\":41702},{\"end\":42064,\"start\":42046},{\"end\":42329,\"start\":42318},{\"end\":42682,\"start\":42669},{\"end\":43025,\"start\":42976},{\"end\":43250,\"start\":43197},{\"end\":43484,\"start\":43471},{\"end\":43713,\"start\":43700},{\"end\":43904,\"start\":43825},{\"end\":44183,\"start\":44106},{\"end\":44424,\"start\":44411},{\"end\":44651,\"start\":44638},{\"end\":44921,\"start\":44908},{\"end\":45247,\"start\":45197},{\"end\":45589,\"start\":45576},{\"end\":45804,\"start\":45791},{\"end\":45962,\"start\":45949}]"}}}, "year": 2023, "month": 12, "day": 17}