{"id": 236940398, "updated": "2023-10-06 00:35:06.64", "metadata": {"title": "Actor-Critic-Based Learning for Zero-touch Joint Resource and Energy Control in Network Slicing", "authors": "[{\"first\":\"Farhad\",\"last\":\"Rezazadeh\",\"middle\":[]},{\"first\":\"Hatim\",\"last\":\"Chergui\",\"middle\":[]},{\"first\":\"Loizos\",\"last\":\"Christofi\",\"middle\":[]},{\"first\":\"Christos\",\"last\":\"Verikoukis\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ICC 2021 - IEEE International Conference on Communications", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "To harness the full potential of beyond 5G (B5G) communication systems, zero-touch network slicing (NS) is viewed as a promising fully-automated management and orchestration (MANO) system. This paper proposes a novel knowledge plane (KP)-based MANO framework that accommodates and exploits recent NS technologies and is termed KB5G. Specifically, we deliberate on algorithmic innovation and artificial intelligence (AI) in KB5G. We invoke a continuous model-free deep reinforcement learning (DRL) method to minimize energy consumption and virtual network function (VNF) instantiation cost. We present a novel Actor-Critic-based NS approach to stabilize learning called, twin-delayed double-Q soft Actor-Critic (TDSAC) method. The TDSAC enables central unit (CU) to learn continuously to accumulate the knowledge learned in the past to minimize future NS costs. Finally, we present numerical results to showcase the gain of the adopted approach and verify the performance in terms of energy consumption, CPU utilization, and time efficiency.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.08985", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2201-08985", "doi": "10.1109/icc42927.2021.9500265"}}, "content": {"source": {"pdf_hash": "e6245e15e1ff1c942ca358a9bfdc8defef5fd75f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.08985v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e47ba946cfad9f938b3d170c35d88a30c269554a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e6245e15e1ff1c942ca358a9bfdc8defef5fd75f.txt", "contents": "\nActor-Critic-Based Learning for Zero-touch Joint Resource and Energy Control in Network Slicing\n22 Jan 2022\n\nFarhad Rezazadeh \nTelecommunications Technological Center of Catalonia (CTTC)\nBarcelonaSpain\n\nHatim Chergui \nTelecommunications Technological Center of Catalonia (CTTC)\nBarcelonaSpain\n\nLoizos Christofi loizos.christofi@ebos.com.cy \neBOS Technologies Ltd\nLakatamiaCyprus\n\nChristos Verikoukis \nTelecommunications Technological Center of Catalonia (CTTC)\nBarcelonaSpain\n\nActor-Critic-Based Learning for Zero-touch Joint Resource and Energy Control in Network Slicing\n22 Jan 2022Contact Emails:Index Terms-Actor-CriticAIB5Genergy efficiencyknowl- edge planenetwork slicingresource allocationzero-touch\nTo harness the full potential of beyond 5G (B5G) communication systems, zero-touch network slicing (NS) is viewed as a promising fully-automated management and orchestration (MANO) system. This paper proposes a novel knowledge plane (KP)-based MANO framework that accommodates and exploits recent NS technologies and is termed KB5G. Specifically, we deliberate on algorithmic innovation and artificial intelligence (AI) in KB5G. We invoke a continuous model-free deep reinforcement learning (DRL) method to minimize energy consumption and virtual network function (VNF) instantiation cost. We present a novel Actor-Critic-based NS approach to stabilize learning called, twin-delayed double-Q soft Actor-Critic (TDSAC) method. The TDSAC enables central unit (CU) to learn continuously to accumulate the knowledge learned in the past to minimize future NS costs. Finally, we present numerical results to showcase the gain of the adopted approach and verify the performance in terms of energy consumption, CPU utilization, and time efficiency.\n\nI. INTRODUCTION\n\nN ETWORK slicing is a key enabler for B5G syetems, as it proposes a way of severing the network into different segments by leveraging network softwarization and virtualization technologies such as software-defined networking (SDN) and network function virtualization (NFV). To automate network slicing orchestration, zero-touch network and service management (ZSM) framework reference architecture [1] has been designed by ETSI, but the closed-loop operation of its building blocks is still an open research problem to fulfill an efficient and robust zero-touch management. Indeed, we still need a knowledge plane (KP) that plays the role of a pervasive system within the network by building and maintaining highlevel models of what the network is supposed to do, in order to provide services and advice to other elements of the network [2]- [3]. Specifically, the quest of automation and optimal control in dynamic telecommunication environments has aroused intensive research on the applications of DRL. The DRL can provide a promising technique to be incorporated in NS and solve the control and optimization issues.\n\nIn this context, [4]- [5] have presented softwarization approaches in NS. In [6], the authors have proposed vrAIn as a dynamic resource controller based on DRL for optimal allocation of computing and radio resources. Li et al. have proposed a deep deterministic policy gradient (DDPG)-based solution to enhance energy efficiency and obtain the optimal power control scheme [7]. Correspondingly, [8] has proposed a method to learn the optimum solution for demand-aware resource management in C-RAN NS. They have developed a DRL method as GAN-DDQN to handle resource management in NS. In [9], has leveraged advantage Actor-Critic (A2C) and incorporated the long short-term memory (LSTM) to track the user mobility and improve the system utility. More recently, Liu et al. have proposed a DRL-based method called, DeepSlicing where they decompose NS problem into a master problem and several slave problems wherein DDPG agents learn the optimal resource allocation policy [10]. In this paper, we present the following contributions:\n\n\u2022 We propose a KP for B5G NS dubbed KB5G and elaborate on how KP can join the architectural aspects of NS to make a harmonization in a continuous control setting through revisiting ZSM operational closed-loop building blocks. Specifically, we consider CPU and energy consumption control and optimization. \u2022 We propose TDSAC as an algorithmic innovation in NS.\n\nThis stochastic Actor-Critic approach supports continuous state and action spaces in telecommunication while stabilizing the learning procedure and improve time efficiency in B5G. Moreover, it benefits from a model-free approach to underpin dynamism and heterogeneous nature of NS while reducing the need for hyperparameter tuning. \u2022 We develop a 5G RAN NS environment called smartech-v2. It integrates both CPU and energy consumption simulators with an OpenAI Gym-based standardized interface to ensure reproducible comparison of different DRL algorithms.\n\nII. KNOWLEDGE-BASED BEYOND 5G (KB5G) NETWORKS As depicted in Figure 1, the KP encompasses machine learning (ML) and intelligent decision to handle knowledge discovery, data analysis, and optimization. In what follows we elaborate on the steps in KB5G: 1) Network Slicing \u2192 Analytics Platform: The analytics platform is gathering enough information to offer a complete view of the network and provide current and historical data for feeding learning algorithms. This data is categorized into two types, namely, users' data and operators' data, where both can be either local data or global data. This platform can rely on protocols and functions, such as network configuration protocol (NETCONF) [11] and network data analytics function (NWDAF) [12].\n\n2) Analytics Platform \u2192 ML \u2192 Intelligent Decision: The collected data is utilized by cloud computing platforms to feed learning algorithms for knowledge discovery, data analysis, optimization, and generally manage and control the network to facilitate inferencing. The data analysis represents hidden patterns in big data and can predict and model the future behavior of the network. The KB5G benefits from some indicative abilities of intelligent behavior like learning from experience.\n\n3) Intelligent Decision \u2192 SDN Controller(s): In SDN, the northbound application programming interfaces (APIs) present an abstraction of network functions with a programmable interface to dictate the behavior of the network at the top of the SDN stack. Using declarative languages for the SDN northbound interface and translating intelligent decisions to specific control directives is an open research question yet. The SDN controller receives the declarative primitives through its northbound interface and then renders the intent-driven language into specific imperative control actions [3]. 4) SDN Controller(s) \u2192 Network: Due to robustness issues, we consider the distributed control logic [13]. The distributed SDN consists of multiple interconnected network domains. In NS we have software-defined radio (SDR) for C-RAN, transport controllers, and virtual network function orchestration (NFVO) for cloud-native Core. The SDN concept can be applied in the KB5G through packet forwarding control protocol (PFCP) instead of OpenFlow-enabled protocol. Indeed, OpenFlow does not support all aspects of quality of service (QoS) issues and it is also packet-based, while PFCP is session-based [14]- [15]. Figure 1 shows the considered C-RAN CU-DU split-based network architecture. A total of N access points (APs) are covering M single-antenna users in a downlink setup, and are connected to CU hosting control agents and running as a set of VNFs of the same type. We define L \u2208 N as the number of slices in the network, and assume that the mobile network operator (MNO) collects the free and unused resources from the tenants and allocate them to the slices in need in a periodic fashion to avoid over-heading. A maximum of X \u2208 N VNFs can be deployed on top of the cloud, endowed with Z (z = 1, . . . , Z) active CPUs having a processing capability of P z million operations per time slot (MOPTS) [16]. Let us denote h m = [h 1,m , h 2,m , ..., h N,M ] H \u2208 C N \u00d71 as vector of channel gains from the N APs to the M users, where (\u00b7) H is the conjugate transpose and C represents the complex set. Moreover, we consider the optimal beamforming vector v m = [v 1,m , v 2,m , ..., v N,M ] H \u2208 C N \u00d71 associated with user m and whose expression is given by [17] as\n\n\nIII. SYSTEM MODEL\nv m = \u221a p m (IN + M j=1 1 \u03c3 2 hjh H j ) \u22121 hm (IN + M j=1 1 \u03c3 2 hjh H j ) \u22121 hm\n, where p m is beamforming power, I N denotes the N \u00d7 N identity matrix and\u03c3 2 is the noise variance. Therefore we model the received signal r m \u2208 C at user m as\nr m = h H m v m s m + M j =m h H m v j s j + n m ,\nwhere s j \u2208 C is data signal to user m and received noise n m is the white Gaussian noise with zero mean and variance \u03c3 2 . Let define the following channel model [18], h n,m = 10 \u2212L * (dn,m)/20 \u03d1 n,m \u0398 n,m g n,m , where L * (d n,m ) denotes the path loss with a distance of d n,m . Moreover, \u03d1 n,m is the antenna gain, \u0398 n,m is the shadowing coefficient and g n,m is the small-scale fading coefficient. Then the achievable rate for user m is given by\nR m = log \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed 1 + h H m v m 2 M j =m |h H m v j | 2 + \u03c3 2 SIN Rm \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 ,\nwhere SIN R stands for signal-to-interference-plus-noise ratio. Let define G n \u2208 C 1\u00d7N as G n = [0,...,0, n-1 1, 0, ..., 0], n > 0.\n\nThen the power consumption for AP n serving all potential m users can be written as [19],\nE n w = M m=1 v H m G H n G n v m .\nNote that the circuit power and fronthaul power consumption can be neglected because they are small compared with transmit power. Moreover, we consider energy consumption incurred by the running processors. The computing resource model follows that in [20]. We suppose \u2206 m is a fraction of a CPU core,\n\u2206 m =\u03b8R m + C 0 baseband + \u03b4 N n=1\n\u03a5|v n,m | transmission , where baseband processing refers to coding, modulation and fast Fourier transform (FFT). Furthermore,\u03b8 is experimental value, C 0 denotes constant complexity for FFT, \u03b4 > 0 is slope parameter and \u03a5(\u00b7) denotes the step function. The energy consumed by processor z in Watts is given by \u03b9P 3 z , where \u03b9 parameter denotes the processor structure [21]. We define constant value \u03c8 for VNF deployment and then compute energy consumption in CU with respect to total \u2206 m . Therefore, the whole energy consumption in network is given by\nE (t) N et = Z z=1 \u03b9P 3 z + X x=1 \u03c8 x baseband + N n=1 M m=1 v H m G H n G n v m transmission (1)\nThe objective is to minimize the overall network cost with respect to the incurred computing resources and energy consumption at each decision time step and thereby the continuous model-free DRL optimization is given by\nmin 1 M (t) (E (t) N et ) (2a) subject to p m \u2264 P max , m \u2208 M, (2b) SIN R m \u2265 SIN R th,l , m \u2208 M, l \u2208 L, (2c) \u2206 m \u2264 \u2206 th,l , m \u2208 M, l \u2208 L.\n(2d)\n\nNote that the higher traffic can induce higher costs. We consider the number of users (M (t) ) at each decision time step to normalize and balance network cost with respect to heavy and low traffic periods. Moreover, P max is an experimental value while SIN R th,l and also \u2206 th,l are predefined thresholds for slice l.\n\n\nIV. PROBLEM FORMULATION\n\nProblem (2) can be formulated from a Markov decision process (MDP) perspective, where the objective is to achieve lower total costs under user QoS, predefined thresholds, and computing resource constraints. This reflects the correlation between energy consumption and CPU usage, where beamforming power p m for each user affects SINR that in turn influences computing resource consumption. The MDP can be solved by finding an optimal policy for selecting the best actions with respect to beamforming power and computing resource allocation. Indeed, the MDP is mathematically characterized by a 5tuple (S, A, P, \u03b3, R) where S is the state space, A refers to the action space, P denotes the transition probability from current state s to the next state s \u2032 , \u03b3 is the reward discounting hyperparameter, and R stands for the reward function. The state value function for the policy \u03c0 is an explicit measure of how much reward to expect, V \u03c0 (s) = E \u03c0 [ \u221e n=0 (\u03b3 n R t+n+1 |S t = s)] and is defined as action-value function (referred as Q-Function)\nQ \u03c0 (s, a) = E \u03c0 [ \u221e n=0 (\u03b3 n R t+n+1 |S t = s, A t = a)\n]. The concerned MPD problem is defined as follows: -State space: The state space provides input data about possible network configurations for agent via interaction with NS environment parameters. In our scenario, the state transits to the next state at each time step t by S (t) = {S \nCP U \u2208 {o|o \u2208 R, \u2212C (t) N et \u2264 o \u2264 C (t) Z \u2212 C (t) N et }, where A (t)\nCP U is vertical scaling action for CPU resources, C (t) Z is CPU capacity and C (t) N et denotes the total CPU requirements. Moreover, we assign beamforming power according to SINR constraint,\nA (t) P \u2208 {o|o \u2208 R, 0 \u2264 o \u2264 P (t) max } the complete continuous multi- action space is given by A A (t) CP U \u222a A (t) P .\n-Reward: Due to to guide the agent for learning good results, we define \u03c7 (t)\n\nT as constraints function that is given by the following piecewise function,\n\u03c7 (t) T = 0, if SIN Rm \u2265 SIN R th,l and \u2206m \u2264 \u2206 th,l 1, otherwise(3)\nAccordingly, we define the penalty function as \u03b5 \nR (t) = 1 1 M (t) (E (t) N et ) + M m=1 \u03b5 (t) m \u03c9(4)\nwhere\u03c9 is a hyperparameter that guarantees R (t) \u2208 [\u22121, 1].\n\nDeep neural network (DNN) uses this return function for training while satisfying the main goal of overall objective function (2).\n\n\nV. TWIN-DELAYED DOUBLE-Q SOFT ACTOR-CRITIC\n\nActor-Critic methods are a combination of policy optimization and Q-Learning. We use \u03c1 \u03c0 (s t ) and \u03c1 \u03c0 (s t , a t ) to denote the state and state-action distribution respectively that induced by policy \u03c0 in NS environment \u03c0(a t |s t ). Unlike the DDPG [22] and TD3 [23], the TDSAC benefits from stochastic policy gradient. The basic idea behind policy-based algorithms is to adjust the parameters \u03c6 of the policy in the direction of the performance gradient \u2207 \u03c6 J(\u03c0 \u03c6 ) concerning the policy gradient theorem [24]. The goal in standard RL is to learn a policy \u03c0(a t , s t ) which maximizes the expected sum of rewards. We consider a more general entropy-augmented objective concerning stochastic policies approach where augments the objective with a policy entropy term H over \u03c1 \u03c0 (s t ). Maximum entropy RL can optimize the expected return and also the entropy of the policy and thereby improves the exploration efficiency of the policy. The objective for finite-horizon MDPs is given by, J \u03c0 = E T i=t \u03b3 i\u2212t [r i + \u03b1H(\u03c0(\u00b7|s i ))] . As we mentioned before, \u03b3 is the discount factor. The temperature parameter \u03b1 determines the relative importance of the H against the reward, thereby handle the stochasticity of the optimal policy. Maximum entropy RL gradually proceeds toward the conventional RL \u03b1 \u2192 0.\n\nLet us define entropy-augmented accumulated return or soft return as G t = T i=t \u03b3 i\u2212t [r i \u2212 \u03b1 log \u03c0(a i |s i )]. Then we can define soft Q-value with respect to policy \u03c0 as Q \u03c0 (s t , a t ) = E[r] + \u03b3E[G t+1 ]. We use soft policy iteration method for learning optimal maximum entropy policies that alternates between soft policy evaluation and soft policy improvement. In the soft policy iteration, we wish to compute the value of a policy \u03c0 according to the maximum entropy objective [25], thus the soft Q-value can be learned by applying a Bellman operator T \u03c0 under policy \u03c0 repeatedly as, T \u03c0 Q \u03c0 (s, a) = E[r] + \u03b3E[Q \u03c0 (s \u2032 , a \u2032 ) \u2212 \u03b1 log \u03c0(a \u2032 |s \u2032 )], The optimality and convergence of soft policy iteration have been verified in [26]. The main goal is to find a new policy \u03c0 new that is better than the current policy \u03c0 old and thereby J \u03c0new \u2265 J \u03c0 old . This particular choice of update can be accomplished by maximizing the entropy-augmented objective (J \u03c0 ) with respect to soft Qvalue, \u03c0 new = arg max \u03c0 E[Q \u03c0 old (s, a) \u2212 \u03b1 log \u03c0(a|s)].\n\nOur method (TDSAC) incorporates the following key approaches. The main aim is to stabilize the learning and improve time efficiency while mitigating very high sample complexity and meticulous hyperparameter tuning: 1) The (clipped) double Q-learning technique [23] parameterizes critic networks and critic targets by \u03b8 1 , \u03b8 2 and \u03b8 \u2032 1 ,\u03b8 \u2032 2 respectively. Unlike the TD3 in TDSAC, the next state-actions used in the target come from the current policy (\u03c6) instead of a target policy. 2) The target in Q-learning depends on the model's prediction so cannot be considered as a true target. To address this problem, we use another target network instead of using Q-network to calculate the target. 3) In TDSAC, the delayed strategy updates the policy, temperature, and target networks less frequently than the value network to estimate the value with a lower variance to have better policy [23]. 4) Experience replay enables RL to reuse and also memorize past experiences to solve the catastrophic interference problem. In our method, we store (s t , a t , r t , s t+1 ) to train deep Q-Network and sample random many batches from the experience replay \u03b2 (buffer/queue) as training data. We take a random batch B for all transitions (s tB , a tB , r tB , s tB +1 ).\n\nLet us define Q \u03b8 (s, a) and \u03c0 \u03c6 (a|s) as parameterized functions to approximate the soft Q-value and policy, respectively. We consider a pair of soft Q-value functions (Q \u03b81 , Q \u03b82 ) and separate target soft Q-value functions (Q \u03b8 \u2032 1 , Q \u03b8 \u2032 2 ). We calculate the update targets of Q \u03b81 , Q \u03b82 according to y = r + \u03b3( min i=1,2  \nQ \u03b8 \u2032 i (s \u2032 , a \u2032 )) \u2212 \u03b1 log \u03c0 \u03c6 (a \u2032 |s \u2032 ), a \u2032 \u223c \u03c0 \u03c6 we can, st B +1) \u03b8i \u2190\u2212 \u03b8i \u2212 \u2113Q\u2207 \u03b8 i JQ(\u03b8i), i=1,2 #Update soft Q-function if t mod f req then \u03c6 \u2190\u2212 \u03c6 + \u2113\u03c0\u2207 \u03c6 J\u03c0(\u03c6) #Update policy weights \u03b1 \u2190\u2212 \u03b1 \u2212 \u2113\u03b1\u2207\u03b1J(\u03b1) #Adjust temperature \u03b8 \u2032 i \u2190\u2212 \u03c4 \u03b8i + (1 \u2212 \u03c4 )\u03b8 \u2032 i i=1,J Q (\u03b8 i ) = E[(y \u2212 Q \u03b8i (s, a)) 2 ], i = 1, 2(5)\nTo obtain lower variance estimates, we use the reparameterization trick [25] and reparameterize the policy using a neural network transformation where a = f \u03c6 (\u03be; s). Therefore, the policy update gradients with respect to experience replay (\u03b2) is given by\n\u2207 \u03c6 J \u03c0 (\u03c6) = E[\u2212\u2207 \u03c6 \u03b1 log(\u03c0 \u03c6 (a|s)) + (\u2207 a Q \u03b8 (s, a)\n\u2212\u03b1\u2207 a log(\u03c0 \u03c6 (a|s))\u2207 \u03c6 f \u03c6 (\u03be; s))]\n\nWe can update temperature \u03b1 by minimizing the following objective\nJ(\u03b1) = E[\u2212\u03b1 log \u03c0 \u03c6 (a|s) \u2212 \u03b1H](7)\nTo enforce action bounds in algorithms with stochastic policy, we use an unbounded Gaussian as the action distribution [26]. The proposed approach is summarized in Algorithm 1.\n\n\nVI. NUMERICAL RESULTS\n\nWe use a PyTorch custom environment interfaced through OpenAI Gym as the most famous simulation environment in the DRL community and evaluate our method described in Section V against other SoA DRL approaches, namely, TD3 [23], DDPG [22], and SAC [26] with a minor change to keep all algorithms consistent. We consider three slices (A, B, and C) with different constraints where the number of new service requests for VNFs follows a distributed homogeneous Poisson process. There exist 20 APs and a maximum of 50 registered subscribers assigned to different slices randomly and the algorithm computes the computing requirements to allocate to the relevant VNF. The dedicated subscribers to Slice-A are less than Slice-B and Slice-C. Table I provides a comparison  of architectures and hyperparameters while Table II presents network parameters. The DNNs structure for the actor-critic networks and target networks are the same. We have set the hyperparameters following extensive experiments [27]. The evaluation computes every 20000 iterations concerning the average return over the best 3 of 5 episodes. The start_timesteps denotes the initial time steps for random policy to fill the buffer with enough samples. Moreover, the curves are smoothed for visual clarity in terms of the confidence interval. As shown in Figure 2, the learning curve of TDSAC outperforms all other algorithms in the final performance. Note that  the scenario has a big and complex state space. The learning procedures are based on interaction with the NS environment. The NS has different network configurations and parameters (states) and thereby the curves experience high fluctuation during learning. As we mentioned in Sec. IV, we use a rewardpenalty approach for constraints and thresholds in Problem (2). Indeed, this experimental approach (Eqn. 4) can lead the agent to good results because the problem formulation (2) is general. Figure 3 demonstrates the time efficiency of the different algorithms in terms of the wall-clock time consumption on the custom NS environment (smartech-v2). The results show that the TDSAC method yields performance improvement and it has comparable performance to TD3 and lower than SAC. Note that DDPG uses 4 DNNs (Table I) in its architecture and this results in lower wall-clock time consumption compared to other methods but it has the lowest average return between methods and thereby we should compare wall-clock time with average return. All evaluations were run on a single computer with a 3.40 GHz 5 core Intel CPU and evaluation is according to the average per 50 time steps and based on 100 evaluations. We consider the trade-off between CPU resource usage and energy consumption by defining a cross-layer and correlated cost function (Eqn. 1). Figures 4-(a), 4-(b), and 4-(c) show that the performance of TDSAC is better than other approaches. The agent learns to decrease VNFs instantiation and thereby reduce energy in the baseband part while tuning optimal wireless transmission power. In some scenarios, DDPG cannot learn perfectly because of some issues such as overestimation and lack of stable learning behavior, whereas the TDSAC, TD3, and SAC used the referred techniques (see Section (V)) to reduce overestimation, stabilize the training, surmount the curse of dimensionality, solve gradient explosion, and mitigate catastrophic forgetting problems. Note that a large part of energy consumption is constant and agent cannot minimize these values. In Figures 4-(d) and 4-(e), we consider MNO and slices (tenants) as a unified network where slices are isolated and trade-off computing resources with MNO. As shown in Figures 4-(d) and 4-(e), the TDSAC has a better performance compared with other methods. The TDSAC has better resource control between MNO and tenants. We consider CPU utilization efficiency as the ratio of exploited computing resources with respect to the total available CPU for the execution of a VNF.\n\n\nVII. CONCLUSION\n\nTo fulfill zero-touch NS, a knowledge-based scheme with an efficient resource provisioning ability should be adopted. We have proposed a KP for B5G NS called, KB5G and elaborated on how KP can solve control and optimization problems in NS. Specifically, we have deliberated on algorithmic innovation and AI-driven approach and also proposed a continuous model-free DRL method called, TDSAC to minimize energy consumption and VNF instantiation cost. Meanwhile, we have compared the network performance and costs between TDSAC and other DRL benchmarks. We have shown that the proposed solution outperforms other DRL methods.\n\nFigure 1 :\n1Proposed zero-touch KB5G network slicing.\n\n\nis the number of arrival requests for each slice corresponding to each VNF, refers to number of users being served in each slice.-Action space: We consider vertical scaling for computing resources consists of either scaling up or down procedure. The CU selects continuous value action with respect to traffic fluctuations to learn how to properly scaling up/down a VNF and thereby according to time step, we have A (t)\n\nT.\n= 1 , where \u033a m is the penalty coefficient for not fulfilling constraints and \u033a The objective is maximize the total return R (t) ,\n\nFigure 2 :\n2Learning curves of the smartech-v2 network slicing environment and continuous control benchmarks.\n\nFigure 3 :\n3Time efficiency comparison of different algorithms on the custom environment (smartech-v2).\n\nFigure 4 :\n4Network performance and costs comparison between TDSAC and other DRL benchmarks. The curves are smoothed for visual clarity. The solid lines demonstrate the mean and the shaded regions correspond to confidence interval over 3 trials.\n\nTable I :\nIComparison of hyperparameters tuning in simulation.Architecture \nDDPG \nSAC \nTD3 \nour Method (TDSAC) \n\nMethod \nActor-Critic \nActor-Critic \nActor-Critic \nActor-Critic \nModel Type \nMultilayer perceptron \nMultilayer perceptron \nMultilayer perceptron \nMultilayer perceptron \nPolicy Type \nDeterministic \nStochastic \nDeterministic \nStochastic \nPolicy Evaluation \nTD learning \nDouble Q-learning \nClipped double Q-learning \nClipped double Q-learning \nNo. of DNNs \n4 \n6 \n6 \n5 \nNo. of Policy DNNs \n1 \n1 \n1 \n1 \nNo. of Value DNNs \n1 \n2 \n2 \n2 \nNo. of Target DNNs \n2 \n3 \n3 \n2 \nNo. of hidden layers \n2 \n2 \n2 \n5 \nNo. of hidden units/layer \n200 \n256 \n400/300 \n128 \nNo. of Time Steps \n2e6 \n2e6 \n2e6 \n2e6 \nBatch Size \n64 \n256 \n100 \n128 \nOptimizer \nADAM \nADAM \nADAM \nADAM \nADAM Parameters (\u03b21, \u03b22) \n(0.9, 0.999) \n(0.9, 0.999) \n(0.9, 0.999) \n(0.9, 0.999) \nNonlinearity \nReLU \nReLU \nReLU \nGELU [28] \nTarget Smoothing (\u03c4 ) \n0.001 \n0.005 \n0.005 \n0.001 \nExploration Noise \n\u03b8, \u03c3 = 0.15, 0.2 \nNone \nN (0, 0.1) \nNone \nUpdate Interval (f req) \nNone \nNone \n2 \n2 \nPolicy Smoothing \nNone \nNone \n\u01eb \u223c clip(N (0, 0.2), \u22120.5, 0.5) \nNone \nExpected Entropy(H) \nNone \n-dim(Action) \nNone \n-dim(Action) \nActor Learning Rate \n0.0001 \n0.0001 \n0.001 \n0.001 \nCritic Learning Rate \n0.001 \n0.0001 \n0.001 \n0.001 \nDiscount Factor \n0.99 \n0.99 \n0.99 \n0.99 \nReplay Buffer Size \n1e6 \n1e6 \n1e6 \n1e6 \n\n\n\nTable II :\nIINetwork parameters in simulation.Network Parameter \nValue \n\nChannel bandwidth \n10 MHz \nBackground noise (\u03c3 2 ) \n-102 dBm \nAntenna gain (\u03d1n,m) \n9 dBi \nLog-normal shadowing (\u0398n,m) \n8 dB \nSmall-scale fading distribution (gn,m) \nCN (0, I) \nPath-loss at distance dn,m (km) \n148.1+37.6 log 2 (dm,n) dB \nDistance dm,n \ndistributed uniformly [0, 600] \n(\u03b9, Pz ) \n(10 \u221226 , 10 9 ) \n\n\nACKNOWLEDGEMENT This work has been supported in part by the research projects 5GSTEPFWD (722429), MonB5G (871780), 5G-SOLUTIONS (856691), AGAUR(2017-SGR-891) and SPOT5G (TEC2017-87456-P).\n. Etsi, Zero-touch Network and Service Management. ZSMETSI GS ZSM 002, \"Zero-touch Network and Service Management (ZSM);\n\nReference Architecture. Reference Architecture,\" 2019.\n\nA knowledge plane for the Int. D D Clark, SIGCOMM. D. D.Clark et al., \"A knowledge plane for the Int.,\" in SIGCOMM, 2003.\n\nKnowledge-defined networking. A Mestres, ACM SIGCOMM Computer Communication Review. 47A. Mestres et al., \"Knowledge-defined networking,\" in ACM SIGCOMM Computer Communication Review, Vol. 47, no. 3, 2017.\n\n5G White Paper. Ngmn Alliance, NGMN Alliance, \"5G White Paper\", Feb 2015.\n\nNetwork Functions Virtualization (NFV): Management and Orchestration, V1.1.1. Etsi Group Spec, ETSI Group Spec., \"Network Functions Virtualization (NFV): Manage- ment and Orchestration, V1.1.1\", Dec. 2014.\n\nvrAIn: A Deep Learning Approach Tailoring Computing and Radio Resources in Virtualized RANs. A Jose, Ayala-Romero, ACM Mobicom. Jose A. Ayala-Romero et al., \"vrAIn: A Deep Learning Approach Tailoring Computing and Radio Resources in Virtualized RANs,\" in ACM Mobicom, 2019.\n\nDeep Deterministic Policy Gradient Based Dynamic Power Control for Self-Powered Ultra-Dense Networks. H Li, GlobecomH. Liet al., \"Deep Deterministic Policy Gradient Based Dynamic Power Control for Self-Powered Ultra-Dense Networks,\" in Globecom, 2018.\n\nGan-powered deep distributional reinforcement learning for resource management in network slicing. Y Hua, IEEE JSAC. 38Y. Hua et al., \"Gan-powered deep distributional reinforcement learning for resource management in network slicing,\" in IEEE JSAC, vol. 38, no. 2, pp. 334-349, 2019.\n\nThe LSTM-based Advantage Actor-Critic Learning for Resource Management in Network Slicing with User Mobility. R Li, IEEE Communications Letters. 249R. Li et al., \"The LSTM-based Advantage Actor-Critic Learning for Resource Management in Network Slicing with User Mobility,\" in IEEE Communications Letters, vol. 24, no. 9, pp. 2005-2009, 2020.\n\nDeepSlicing: Deep Reinforcement Learning Assisted Resource Allocation for Network Slicing. Q Liu, arXivQ. Liu et al., \"DeepSlicing: Deep Reinforcement Learning Assisted Resource Allocation for Network Slicing,\" in arXiv:2008,07614, 2020.\n\n5G -Configuration with NETCONF. 5G -Configuration with NETCONF, 2019, [Online]. Available: https://wiki.onap.org/display/DW/5G+-+Configuration+with+NETCONF.\n\nData Analytics for 5G Networks: A Complete Framework for Network Access Selection and Traffic Steering. S Barmpounakis, Int. Journal on Advances in Telecommun. 113 & 4S. Barmpounakis et al., \"Data Analytics for 5G Networks: A Complete Framework for Network Access Selection and Traffic Steering,\" in Int. Journal on Advances in Telecommun., vol. 11, no. 3 & 4, 2018.\n\nStudy on Distributed SDN Controllers and Failover Mechanisms. T Thomas, N T Bhuvan, in IJIRCCE. 54T. Thomas and N. T.Bhuvan, \"Study on Distributed SDN Controllers and Failover Mechanisms,\" in IJIRCCE, Vol. 5, no. 4, pp. 7183-7185, 2017.\n\nLTE-Interface between the Control plane and the User Plane of EPC Nodes. 3GPP TS 29.244 version 14.0.0 Release 14LTE-Interface between the Control plane and the User Plane of EPC Nodes (3GPP TS 29.244 version 14.0.0 Release 14), 2017.\n\nOn evaluating different trends for virtualized and SDNready mobile network. I , IEEE CloudNet. I. Alawe et al., \"On evaluating different trends for virtualized and SDN- ready mobile network,\" in IEEE CloudNet, 2017.\n\nMachine Learning in Control Systems: An Overview of the State of the Art. S Moe, AI XXXV, SGAI 2018. S. Moe et al., \"Machine Learning in Control Systems: An Overview of the State of the Art,\" in AI XXXV, SGAI 2018, pp. 250-265, 2018.\n\nOptimal multiuser transmit beamforming: A difficult problem with a simple solution structure. E Bjornson, IEEE Signal Processing Magazine. 31E. Bjornson et al., \"Optimal multiuser transmit beamforming: A difficult problem with a simple solution structure,\" in IEEE Signal Processing Magazine, vol. 31, no. 4, pp. 142-148, 2014.\n\nGroup sparse beamforming for green Cloud-RAN. Y Shi, IEEE Transactions on Wireless Commun. 135Y. Shi et al., \"Group sparse beamforming for green Cloud-RAN,\" in IEEE Transactions on Wireless Commun., Vol. 13,No. 5, pp. 2809-2823, 2014.\n\nEnergy-efficient resource allocation optimization for multimedia heterogeneous cloud radio access networks. M Peng, IEEE Trans. Multimedia. 185M. Peng et al., \"Energy-efficient resource allocation optimization for multimedia heterogeneous cloud radio access networks,\" in IEEE Trans. Multimedia, vol. 18, no. 5, pp. 879-892, 2016.\n\nHow much computing capability is enough to run a cloud radio access network?. Y Liao, IEEE Comm. Letters. 211Y. Liao et al., \"How much computing capability is enough to run a cloud radio access network?,\" in IEEE Comm. Letters, vol. 21, no. 1, pp. 104- 107, 2017.\n\nSystem cost minimization in cloud RAN with limited fronthaul capacity. J Tang, IEEE TWC. 16J. Tang et al., \"System cost minimization in cloud RAN with limited fronthaul capacity,\" in IEEE TWC, vol. 16, no. 5, pp. 3371-3384, 2017.\n\nContinuous control with deep reinforcement learning. T P Lillicrap, ICLR. T.P. Lillicrap et al., \"Continuous control with deep reinforcement learn- ing,\" in ICLR, 2016.\n\nAddressing function approximation error in actorcritic methods. S Fujimoto, ICML. S. Fujimoto et al., \"Addressing function approximation error in actor- critic methods,\" in ICML, 2018\n\nDeterministic policy gradient algorithms. D Silver, ICML. D. Silver et al., \"Deterministic policy gradient algorithms,\" in ICML, 2014.\n\nSoft actor-critic algorithms and applications. T Haarnoja, arXiv:1812.05905T. Haarnoja et al., \"Soft actor-critic algorithms and applications,\" in arXiv:1812.05905, 2018.\n\nSoft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, ICML. T. Haarnoja et al., \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,\" in ICML, 2018.\n\nContinuous Multi-objective Zero-touch Network Slicing via Twin Delayed DDPG and OpenAI Gym. F Rezazadeh, IEEE GLOBE-COM. F. Rezazadeh et al., \"Continuous Multi-objective Zero-touch Network Slicing via Twin Delayed DDPG and OpenAI Gym,\" in IEEE GLOBE- COM, 2020.\n\nGaussian error linear units (gelus). D Hendrycks, K Gimpel, D. Hendrycks and K. Gimpel, \"Gaussian error linear units (gelus),\" [Online]. Available: https://arxiv.org/pdf/1606.08415.pdf\n", "annotations": {"author": "[{\"end\":203,\"start\":110},{\"end\":294,\"start\":204},{\"end\":380,\"start\":295},{\"end\":477,\"start\":381}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":117},{\"end\":217,\"start\":210},{\"end\":311,\"start\":302},{\"end\":400,\"start\":390}]", "author_first_name": "[{\"end\":116,\"start\":110},{\"end\":209,\"start\":204},{\"end\":301,\"start\":295},{\"end\":389,\"start\":381}]", "author_affiliation": "[{\"end\":202,\"start\":128},{\"end\":293,\"start\":219},{\"end\":379,\"start\":342},{\"end\":476,\"start\":402}]", "title": "[{\"end\":96,\"start\":1},{\"end\":573,\"start\":478}]", "venue": null, "abstract": "[{\"end\":1748,\"start\":708}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2168,\"start\":2165},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2607,\"start\":2604},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2612,\"start\":2609},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2908,\"start\":2905},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2913,\"start\":2910},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2968,\"start\":2965},{\"end\":3119,\"start\":3105},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3264,\"start\":3261},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3286,\"start\":3283},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3477,\"start\":3474},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3861,\"start\":3857},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5537,\"start\":5533},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5586,\"start\":5582},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6670,\"start\":6667},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6776,\"start\":6772},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7274,\"start\":7270},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7280,\"start\":7276},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7979,\"start\":7975},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8333,\"start\":8329},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8817,\"start\":8813},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9410,\"start\":9406},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9704,\"start\":9700},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10157,\"start\":10153},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13745,\"start\":13741},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13758,\"start\":13754},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14002,\"start\":13998},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15285,\"start\":15281},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15538,\"start\":15534},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16112,\"start\":16108},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16741,\"start\":16737},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17839,\"start\":17835},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18337,\"start\":18333},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18642,\"start\":18638},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18653,\"start\":18649},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18667,\"start\":18663},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19412,\"start\":19408},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20204,\"start\":20201}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23072,\"start\":23018},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23493,\"start\":23073},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23628,\"start\":23494},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23739,\"start\":23629},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23844,\"start\":23740},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24091,\"start\":23845},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":25450,\"start\":24092},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":25838,\"start\":25451}]", "paragraph": "[{\"end\":2886,\"start\":1767},{\"end\":3917,\"start\":2888},{\"end\":4278,\"start\":3919},{\"end\":4836,\"start\":4280},{\"end\":5587,\"start\":4838},{\"end\":6076,\"start\":5589},{\"end\":8336,\"start\":6078},{\"end\":8598,\"start\":8437},{\"end\":9101,\"start\":8650},{\"end\":9320,\"start\":9189},{\"end\":9411,\"start\":9322},{\"end\":9749,\"start\":9448},{\"end\":10337,\"start\":9785},{\"end\":10655,\"start\":10436},{\"end\":10799,\"start\":10795},{\"end\":11120,\"start\":10801},{\"end\":12192,\"start\":11148},{\"end\":12536,\"start\":12250},{\"end\":12801,\"start\":12608},{\"end\":13000,\"start\":12923},{\"end\":13078,\"start\":13002},{\"end\":13196,\"start\":13147},{\"end\":13309,\"start\":13250},{\"end\":13441,\"start\":13311},{\"end\":14792,\"start\":13488},{\"end\":15846,\"start\":14794},{\"end\":17112,\"start\":15848},{\"end\":17445,\"start\":17114},{\"end\":18018,\"start\":17763},{\"end\":18111,\"start\":18075},{\"end\":18178,\"start\":18113},{\"end\":18390,\"start\":18214},{\"end\":22375,\"start\":18416},{\"end\":23017,\"start\":22395}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8436,\"start\":8357},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8649,\"start\":8599},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9188,\"start\":9102},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9447,\"start\":9412},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9784,\"start\":9750},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10435,\"start\":10338},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10794,\"start\":10656},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12249,\"start\":12193},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12607,\"start\":12537},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12922,\"start\":12802},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13146,\"start\":13079},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13249,\"start\":13197},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17509,\"start\":17446},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17713,\"start\":17509},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17762,\"start\":17713},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18074,\"start\":18019},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18213,\"start\":18179}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19231,\"start\":19149},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20658,\"start\":20649}]", "section_header": "[{\"end\":1765,\"start\":1750},{\"end\":8356,\"start\":8339},{\"end\":11146,\"start\":11123},{\"end\":13486,\"start\":13444},{\"end\":18414,\"start\":18393},{\"end\":22393,\"start\":22378},{\"end\":23029,\"start\":23019},{\"end\":23497,\"start\":23495},{\"end\":23640,\"start\":23630},{\"end\":23751,\"start\":23741},{\"end\":23856,\"start\":23846},{\"end\":24102,\"start\":24093},{\"end\":25462,\"start\":25452}]", "table": "[{\"end\":25450,\"start\":24155},{\"end\":25838,\"start\":25498}]", "figure_caption": "[{\"end\":23072,\"start\":23031},{\"end\":23493,\"start\":23075},{\"end\":23628,\"start\":23498},{\"end\":23739,\"start\":23642},{\"end\":23844,\"start\":23753},{\"end\":24091,\"start\":23858},{\"end\":24155,\"start\":24104},{\"end\":25498,\"start\":25465}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4907,\"start\":4899},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7290,\"start\":7282},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19741,\"start\":19733},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20341,\"start\":20333},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21210,\"start\":21190},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21919,\"start\":21906},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22094,\"start\":22071}]", "bib_author_first_name": "[{\"end\":26237,\"start\":26236},{\"end\":26239,\"start\":26238},{\"end\":26359,\"start\":26358},{\"end\":26910,\"start\":26909},{\"end\":27194,\"start\":27193},{\"end\":27444,\"start\":27443},{\"end\":27740,\"start\":27739},{\"end\":28065,\"start\":28064},{\"end\":28475,\"start\":28474},{\"end\":28801,\"start\":28800},{\"end\":28811,\"start\":28810},{\"end\":28813,\"start\":28812},{\"end\":29289,\"start\":29288},{\"end\":29504,\"start\":29503},{\"end\":29759,\"start\":29758},{\"end\":30040,\"start\":30039},{\"end\":30338,\"start\":30337},{\"end\":30640,\"start\":30639},{\"end\":30898,\"start\":30897},{\"end\":31111,\"start\":31110},{\"end\":31113,\"start\":31112},{\"end\":31292,\"start\":31291},{\"end\":31455,\"start\":31454},{\"end\":31596,\"start\":31595},{\"end\":31820,\"start\":31819},{\"end\":32067,\"start\":32066},{\"end\":32275,\"start\":32274},{\"end\":32288,\"start\":32287}]", "bib_author_last_name": "[{\"end\":26033,\"start\":26029},{\"end\":26245,\"start\":26240},{\"end\":26367,\"start\":26360},{\"end\":26563,\"start\":26550},{\"end\":26702,\"start\":26687},{\"end\":26915,\"start\":26911},{\"end\":26929,\"start\":26917},{\"end\":27197,\"start\":27195},{\"end\":27448,\"start\":27445},{\"end\":27743,\"start\":27741},{\"end\":28069,\"start\":28066},{\"end\":28488,\"start\":28476},{\"end\":28808,\"start\":28802},{\"end\":28820,\"start\":28814},{\"end\":29508,\"start\":29505},{\"end\":29768,\"start\":29760},{\"end\":30044,\"start\":30041},{\"end\":30343,\"start\":30339},{\"end\":30645,\"start\":30641},{\"end\":30903,\"start\":30899},{\"end\":31123,\"start\":31114},{\"end\":31301,\"start\":31293},{\"end\":31462,\"start\":31456},{\"end\":31605,\"start\":31597},{\"end\":31829,\"start\":31821},{\"end\":32077,\"start\":32068},{\"end\":32285,\"start\":32276},{\"end\":32295,\"start\":32289}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":26147,\"start\":26027},{\"attributes\":{\"id\":\"b1\"},\"end\":26203,\"start\":26149},{\"attributes\":{\"id\":\"b2\"},\"end\":26326,\"start\":26205},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207611541},\"end\":26532,\"start\":26328},{\"attributes\":{\"id\":\"b4\"},\"end\":26607,\"start\":26534},{\"attributes\":{\"id\":\"b5\"},\"end\":26814,\"start\":26609},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":204714884},\"end\":27089,\"start\":26816},{\"attributes\":{\"id\":\"b7\"},\"end\":27342,\"start\":27091},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":150374097},\"end\":27627,\"start\":27344},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":221715368},\"end\":27971,\"start\":27629},{\"attributes\":{\"doi\":\"arXiv\",\"id\":\"b10\"},\"end\":28210,\"start\":27973},{\"attributes\":{\"id\":\"b11\"},\"end\":28368,\"start\":28212},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":220402564},\"end\":28736,\"start\":28370},{\"attributes\":{\"id\":\"b13\"},\"end\":28974,\"start\":28738},{\"attributes\":{\"id\":\"b14\"},\"end\":29210,\"start\":28976},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11082926},\"end\":29427,\"start\":29212},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53756616},\"end\":29662,\"start\":29429},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8501009},\"end\":29991,\"start\":29664},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15793521},\"end\":30227,\"start\":29993},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11783136},\"end\":30559,\"start\":30229},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":416524},\"end\":30824,\"start\":30561},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13613312},\"end\":31055,\"start\":30826},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16326763},\"end\":31225,\"start\":31057},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3544558},\"end\":31410,\"start\":31227},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13928442},\"end\":31546,\"start\":31412},{\"attributes\":{\"doi\":\"arXiv:1812.05905\",\"id\":\"b25\"},\"end\":31718,\"start\":31548},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":28202810},\"end\":31972,\"start\":31720},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":231632170},\"end\":32235,\"start\":31974},{\"attributes\":{\"id\":\"b28\"},\"end\":32421,\"start\":32237}]", "bib_title": "[{\"end\":26234,\"start\":26205},{\"end\":26356,\"start\":26328},{\"end\":26907,\"start\":26816},{\"end\":27441,\"start\":27344},{\"end\":27737,\"start\":27629},{\"end\":28472,\"start\":28370},{\"end\":28798,\"start\":28738},{\"end\":29286,\"start\":29212},{\"end\":29501,\"start\":29429},{\"end\":29756,\"start\":29664},{\"end\":30037,\"start\":29993},{\"end\":30335,\"start\":30229},{\"end\":30637,\"start\":30561},{\"end\":30895,\"start\":30826},{\"end\":31108,\"start\":31057},{\"end\":31289,\"start\":31227},{\"end\":31452,\"start\":31412},{\"end\":31817,\"start\":31720},{\"end\":32064,\"start\":31974}]", "bib_author": "[{\"end\":26035,\"start\":26029},{\"end\":26247,\"start\":26236},{\"end\":26369,\"start\":26358},{\"end\":26565,\"start\":26550},{\"end\":26704,\"start\":26687},{\"end\":26917,\"start\":26909},{\"end\":26931,\"start\":26917},{\"end\":27199,\"start\":27193},{\"end\":27450,\"start\":27443},{\"end\":27745,\"start\":27739},{\"end\":28071,\"start\":28064},{\"end\":28490,\"start\":28474},{\"end\":28810,\"start\":28800},{\"end\":28822,\"start\":28810},{\"end\":29292,\"start\":29288},{\"end\":29510,\"start\":29503},{\"end\":29770,\"start\":29758},{\"end\":30046,\"start\":30039},{\"end\":30345,\"start\":30337},{\"end\":30647,\"start\":30639},{\"end\":30905,\"start\":30897},{\"end\":31125,\"start\":31110},{\"end\":31303,\"start\":31291},{\"end\":31464,\"start\":31454},{\"end\":31607,\"start\":31595},{\"end\":31831,\"start\":31819},{\"end\":32079,\"start\":32066},{\"end\":32287,\"start\":32274},{\"end\":32297,\"start\":32287}]", "bib_venue": "[{\"end\":26076,\"start\":26035},{\"end\":26171,\"start\":26149},{\"end\":26254,\"start\":26247},{\"end\":26410,\"start\":26369},{\"end\":26548,\"start\":26534},{\"end\":26685,\"start\":26609},{\"end\":26942,\"start\":26931},{\"end\":27191,\"start\":27091},{\"end\":27459,\"start\":27450},{\"end\":27772,\"start\":27745},{\"end\":28062,\"start\":27973},{\"end\":28242,\"start\":28212},{\"end\":28528,\"start\":28490},{\"end\":28832,\"start\":28822},{\"end\":29047,\"start\":28976},{\"end\":29305,\"start\":29292},{\"end\":29528,\"start\":29510},{\"end\":29801,\"start\":29770},{\"end\":30082,\"start\":30046},{\"end\":30367,\"start\":30345},{\"end\":30665,\"start\":30647},{\"end\":30913,\"start\":30905},{\"end\":31129,\"start\":31125},{\"end\":31307,\"start\":31303},{\"end\":31468,\"start\":31464},{\"end\":31593,\"start\":31548},{\"end\":31835,\"start\":31831},{\"end\":32093,\"start\":32079},{\"end\":32272,\"start\":32237}]"}}}, "year": 2023, "month": 12, "day": 17}