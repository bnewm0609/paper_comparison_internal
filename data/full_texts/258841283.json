{"id": 258841283, "updated": "2023-10-24 13:03:21.167", "metadata": {"title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": "[{\"first\":\"Xinbei\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Yeyun\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Pengcheng\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Hai\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Nan\",\"last\":\"Duan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.14283", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-14283", "doi": "10.48550/arxiv.2305.14283"}}, "content": {"source": {"pdf_hash": "f743287be3ced6757de7ecb26d03815b22cd737b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.14283v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3ceff3a1f61117bed1dbeca53e005b50c84e5a98", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/f743287be3ced6757de7ecb26d03815b22cd737b.txt", "contents": "\nQuery Rewriting for Retrieval-Augmented Large Language Models\n23 Oct 2023\n\nXinbei Ma \nDepartment of Computer Science and Engineering\nShanghai Jiao Tong University\n\n\nKey Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering\nShanghai Jiao Tong University\n\n\nYeyun Gong yegong@microsoft.com \nMicrosoft Research Asia\n\n\nMicrosoft Research Asia\n\n\nPengcheng He \nMicrosoft Azure AI\n\n\nHai Zhao zhaohai@cs.sjtu.edu.cn \nDepartment of Computer Science and Engineering\nShanghai Jiao Tong University\n\n\nKey Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering\nShanghai Jiao Tong University\n\n\nNan Duan nanduan@microsoft.com \nMicrosoft Research Asia\n\n\nMicrosoft Research Asia\n\n\nQuery Rewriting for Retrieval-Augmented Large Language Models\n23 Oct 2023FF8CBAF4C17B4B1A62A8E0B2224874AEarXiv:2305.14283v3[cs.CL]\nLarge Language Models (LLMs) play powerful, black-box readers in the retrieve-thenread pipeline, making remarkable progress in knowledge-intensive tasks.This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting.Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval.We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts.Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline.A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader.The rewriter is trained using the feedback of the LLM reader by reinforcement learning.Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA.Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM 1 .\n\nIntroduction\n\nLarge Language Models (LLMs) have shown remarkable abilities for human language processing and extraordinary scalability and adaptability in few-or zero-shot settings.(Ouyang et al., 2022;Brown et al., 2020;Chowdhery et al., 2022).However, the training process depends on large-scale high-quality corpora but without the perception of the real world.Thus, LLMs still have to face the issue of hallucination (Yao et al., 2023;Bang et al., 2023) and temporal misalignment (R\u00f6ttger and Pierrehumbert, 2021;Luu et al., 2022;Jang et al., 2022).This affects the reliability of LLMs and hinders wider practical application, because the consistency between the LLM responses with the real world needs further validation.Existing work has proved that incorporating external knowledge (i.e., non-parametric knowledge) with internal knowledge (i.e., parametric knowledge) can effectively alleviate hallucination, especially for knowledge-intensive tasks.In fact, retrievalaugmented LLMs have been shown so effective that they have been regarded as a standard solution to alleviate the factuality drawbacks in naive LLM generations.Retrieval augmentation is applied to select relative passages as external contexts for the language model, which is retrieve-then-read framework (Lewis et al., 2020b;Karpukhin et al., 2020;Izacard et al., 2022).Take the open-domain Question-Answering task (open-domain QA) as an example, a retriever first searches for related documents for a question.Then the LLM receives the question and the documents, then predicts an answer.\n\nAs most LLMs are only accessible through inference APIs, they play the part of black-box frozen readers in the pipeline.This makes previous retrieval augmentation methods that require complete access (Lewis et al., 2020b;Guu et al., 2020;Izacard et al., 2022) no longer feasible.Recent studies on retrieval-augmented language models lean more on the LLM-oriented adaptation.An idea is to train a dense retrieval model to cater to the frozen language model (Shi et al., 2023).By using feedback from the LLM as a training objective, the retrieval model is tuned for better LLM input contexts.Another research line focuses on the design of interactions between the retriever and the reader (Yao et al., 2023;Khattab et al., 2022), where both the retriever and the reader are usually frozen.The idea is to trigger the emergent ability through carefully crafted prompts or a sophisticated prompt pipeline.Multiple interactions with external knowledge allow the LLM to approach the correct answer step by step.\n\nHowever, there are still problems remaining to be solved.Existing approaches overlook the adaptation of the query, i.e., the input of the retrievethen-read pipeline.The retrieval query is either original from datasets or directly determined by the black-box generation, thus is always fixed.However, there is inevitably a gap between the input text and the knowledge that is really needed to query.This limits performance and places a burden on retrieval capability enhancement and prompt engineering.\n\nIn consideration of this issue, this paper proposes Rewrite-Retrieve-Read, a new framework for retrieval augmentation, which can be further tuned for adapting to LLMs.In front of the retriever, a step of rewriting the input is added, filling the gap between the given input and retrieval need, as is shown in Figure 1.We adopt the off-the-shelf tool, an internet search engine, as the retriever, which avoids the maintenance of the search index and can access up-to-date knowledge (Lazaridou et al., 2022).Different from previous studies (Khattab et al., 2022;Yao et al., 2023) that require the memory of multiple interaction rounds between the retriever and the LLM for each sample, the motivation of our rewriting step is to clarify the retrieval need from the input text.\n\nWe also propose a trainable scheme for our rewrite-retrieve-read framework (Figure 1 (c)).The black-box retriever and the reader form a frozen system.To further smooth the steps of our pipeline, we apply a small, trainable language model to perform the rewriting step, denoted as the rewriter.The rewriter is trained by reinforcement learning using the LLM performance as a reward, learning to adapt the retrieval query to improve the reader on downstream tasks.\n\nOur proposed methods are evaluated on knowledge-intensive downstream tasks including open-domain QA (HotpoQA (Yang et al., 2018), AmbigNQ (Min et al., 2020), PopQA (Mallen et al., 2022)) and multiple choice QA (MMLU (Hendrycks et al., 2021)).The experiments are implemented on T5-large (Raffel et al., 2020) as the rewriter, ChatGPT (Ouyang et al., 2022) and Vicuna-13B (Chiang et al., 2023) as the LLM reader.The results show that query rewriting consistently improves the retrieve-augmented LLM performance.The results also indicate that the smaller language model can be competent for query rewriting.\n\nTo sum up, our proposed novel retrievalaugmentation method, rewrite-retrieve-read is the first framework where the input text is adapted for the frozen retriever and LLM reader.We introduce a tuneable scheme with a small, trainable model, achieving performance gains with less resource consumption.\n\n2 Related Work\n\n\nRetrieval Augmentation\n\nLanguage models require external knowledge to alleviate the factuality drawbacks.Retrieval augmentation has been regarded as the standard effective solution.With a retrieval module, related passages are provided to the language model as the context of the original input.Thus factual information like common sense or real-time news helps with output prediction through contextualized reading comprehension.\n\nEarlier studies use sparse retriever (Chen et al., 2017) or dense retriever (Karpukhin et al., 2020) in front of a pre-trained language model (PrLM).The neural retriever and reader are both PrLMs of trainable size like BERT (Devlin et al., 2019) or BART (Lewis et al., 2020a).Hence, the whole retrieve-then-reader framework is a tuneable endto-end system, where the retrieved contexts can be regarded as the intermediate results (Karpukhin et al., 2020;Lewis et al., 2020b).Approaches to smooth the two-step framework are proposed to optimize the retrieval and the reading comprehension (Sachan et al., 2021;Lee et al., 2022;Jiang et al., 2022).More recently, retrieval remains a powerful enhancement as the size of models and data scales rapidly (Mallen et al., 2022;Shi et al., 2023;Brown et al., 2020).On the other hand, retrieval enhancement can compensate for the shortfall in parameter size, compared to large-scale language models.For example, by jointly training the retriever and the reader, Atlas (Izacard et al., 2022) shows few-shot performance on par with 540B PalM (Chowdhery et al., 2022)  \u2705 \u2705\n\nFigure 1: Overview of our proposed pipeline.From left to right, we show (a) standard retrieve-then-read method, (b) LLM as a query rewriter for our rewrite-retrieve-read pipeline, and (c) our pipeline with a trainable rewriter.\n\nexternal knowledge.Komeili et al. (2022) use an internet search for relevant information based on the dialogue history to perform dialogue response generation.SeeKeR (Shuster et al., 2022) use a single Transformer to iteratively perform search query generation, then knowledge extraction for dialogue generation and sentence completion.For large-scale models, web search still shows effective for knowledge augmentation (Lazaridou et al., 2022), fact-checking (Menick et al., 2022), and LLM agent enhancement (Yao et al., 2023).\n\n\nCooperation with Black-box LLMs\n\nLarge Language Models, such as ChatGPT (Ouyang et al., 2022), Codex (Chen et al., 2021), PaLM (Chowdhery et al., 2022), emerge impressive natural language processing ability as well as remarkable scalability.This leads to a tendency to embrace LLMs on a wide range of NLP tasks.However, LLMs are only accessible as a black box in most cases, which is because (i) Some like Chat-GPT are not open-source and kept private; (ii) The large parameter scale requires computational resources that are not always affordable to users.This constraint means nothing is available except input and output texts.\n\nExisting studies have proved that LLMs' abilities can be better leveraged by carefully designed interaction methods.GenRead (Yu et al., 2023) prompts an LLM to generate context instead of deploying a retriever, showing that LLMs can retrieve internal knowledge by prompting.ReAct (Yao et al., 2023) and Self-Ask (Press et al., 2022) combines the Chain-of-Thought (CoT) (Wei et al., 2022;Wang et al., 2022) and inter-actions with web APIs.Only relying on prompt construction, Re-Act provides novel baselines for interactive tasks.Demonstrate-Search-Predict (DSP) (Khattab et al., 2022) defines a sophisticated pipeline between an LLM and a retriever.Unlike ReAct, DSP integrates prompts for demonstration bootstrap besides multihop breakdown and retrieval.\n\nDespite the promising performance in the zero or few-shot setting, the behavior of LLMs sometimes needs adjustments.A feasible approach is to append trainable small models in front of or after the LLM.The small models, as a part of the parameters of the system, can be fine-tuned for optimization.RePlug (Shi et al., 2023) is proposed to fine-tune a dense retriever for the frozen LLM in the retrievethen-read pipeline.The retriever is trained under the LLM's supervision to retrieve documents that are suitable for the LLM.With the same purpose, Directional Stimulus Prompting (Li et al., 2023) deploys a small model to provide the LLM with stimulus (e.g., keywords for summarization, or dialogue actions for response generation), which is updated according to the LLM reward.\n\nDifferent from the inspiring work mentioned above, our proposed pipeline contains a query rewriting step in front of the retrieve-then-read module.We further propose a trainable scheme with a small rewriting model, which is a novel enhancement for retrieval-augmented LLM by re-constructing the search query.\n\n\nMethodology\n\nWe present Rewrite-Retrieve-Read, a pipeline that improves the retrieval-augmented LLM from the perspective of query rewriting.Figure 1 shows an overview.This section first introduces the pipeline framework in section 3.1, then the trainable scheme in section 3.2.\n\n\nRewrite-Retrieve-Read\n\nA task with retrieval augmentation can be denoted as follows.Given a dataset of a knowledgeintensive task (e.g., open-domain QA), D = {(x, y) i }, i = 0, 1, 2, . . ., N , x (e.g., a question) is the input to the pipeline, y is the expected output (e.g., the correct answer).Our pipeline consists of three steps.(i) Query rewrite: generate a query x for required knowledge based on the original input x. (ii) Retrieve: search for related context, doc.(iii) Read: comprehend the input along with contexts [doc, x] and predict the output \u0177.\n\nA straightforward but effective method is to ask an LLM to rewrite queries to search for information that is potentially needed.We use a few-shot prompt to encourage the LLM to think, and the output can be none, one or more queries to search.\n\n\nTrainable Scheme\n\nBesides, total reliance on a frozen LLM has shown some drawbacks.Reasoning errors or invalid search hinders the performance (Yao et al., 2023;BehnamGhader et al., 2022).On the other hand, retrieved knowledge may sometimes mislead and compromise the language model (Mallen et al., 2022).To better align to the frozen modules, it is feasible to add a trainable model and adapt it by taking the LLM reader feedback as a reward.\n\nBased on our framework, we further propose to utilize a trainable small language model to take over the rewriting step, as is shown in the right part of Figure 1.The trainable model is initialized with the pre-trained T5-large (770M) (Raffel et al., 2020), denoted as trainable rewriter, G \u03b8 .The rewriter is first trained on pseudo data to warm up ( \u00a73.2.1), then continually trained by reinforcement learning ( \u00a73.2.2).\n\n\nRewriter Warm-up\n\nThe task, query rewriting, is quite different from the pre-training objective of sequence-to-sequence generative models like T5.First, we construct a pseudo dataset for the query rewriting task.Inspired by recent distillation methods (Hsieh et al., 2023;Ho et al., 2022), we prompt the LLM to rewrite the original questions x in the training set and collect the generated queries x as pseudo labels.The collected samples are then filtered: Those that get correct predictions from the LLM reader are selected into the warm-up dataset, denoted as D T rain = {(x, x)|\u0177 = y}.The rewriter G \u03b8 is finetuned on D T rain with the standard log-likelihood as the training objective, denoted as\nL warm = \u2212 t logp \u03b8 ( xt | x<t , x ).\n(1)\n\nThe rewriter model after warm-up shows modest performance, which depends on the pseudo data quality and rewriter capability.Highly relying on the human-written prompt line, x can be suboptimal.The relatively small scale of the rewriter size is also a limitation of the performance after the warm-up.Then we turn to reinforcement learning to align the rewriter to the following retriever and LLM reader.\n\n\nReinforcement Learning\n\nTo further fine-tune the rewriter to cater to the LLM reader, we adopt a policy gradient reinforcement learning framework.Task Formulation In the context of reinforcement learning, the rewriter optimization is formulated as a Markov Decision Process 5-tuple \u27e8S, A, P, R, \u03b3\u27e9.(i) The state space S is a finite set limited by the vocabulary and the sequence length.(ii) The action space A is equals to the vocabulary.(iii) The transition probability P is determined by the policy network, which is the rewriter model G \u03b8 .(iv) The reward function R gives a reward value that depends on the current state.The policy gradient is derived from rewards, used as the training objective.(v) \u03b3 denotes the discount factor.More specifically, the rewriter G \u03b8 after the warm-up is the initial policy model \u03c0 0 .At each step t, the action a t is to generate the next token xt based on the observation of the present state, s t = [x, x<t ].When the generation is stopped by the End-Of-Sentence token, one episode is ended.After finishing the retrieval and reading, a reward is computed by evaluating the final output, i.e., a score for the LLM reader prediction.Policy Optimization We adopt Proximal Policy Optimization (PPO) (Schulman et al., 2017), following (Ramamurthy et al., 2022).Maximization of the expectation of the reward R is formulated as\nmax \u03b8 E x\u223cp \u03b8 (\u2022|x) [R(x, x)], max \u03b8 E (st,at)\u223c\u03c0 \u03b8 \u2032 [min{k t,\u03b8 A \u03b8 \u2032 (s t , a t ) ; clip (k t,\u03b8 , 1 \u2212 \u03b5, 1 + \u03b5) A \u03b8 \u2032 (s t , a t )}], k t,\u03b8 = p \u03b8 (a t | s t ) p \u03b8 \u2032 (a t | s t ) ,(2)\nwhere \u03b8 \u2032 is the temporarily fixed policy for sampling and \u03b8 is updated.A denotes the advantage function, which is formulated based on the estimation of value network V \u03d5 .The value network V \u03d5 is initialized from the policy network \u03c0 0 .The formulation follows Generalized Advantage Estimation (GAE) (Schulman et al., 2015).\n\u03b4 t = R (s t , a t ) + V \u03d5 (s t+1 ) \u2212 V \u03d5 (s t ) , \u00c2\u03b8 t (s t , a t ) = \u221e t \u2032 =0 \u03bb t \u2032 \u03b4 t+t \u2032 , (3)\nwhere \u03bb is the bias-variance trade-off parameter.\n\nThe reward function R reflects the quality of the generated queries, which needs to be consistent with the final evaluation of the task.x is fed to the retriever and the reader for a final prediction \u0177.A part of the reward function is the measures of \u0177 compared to the golden label y (e.g., exact match and F 1 of the predicted answers), denoted as R lm .Besides, a KL-divergence regularization is added to prevent the model from deviating too far from the initialization (Ramamurthy et al., 2022;Ziegler et al., 2019).\nR (s t , a t ) = R lm ( x, y) \u2212 \u03b2KL (\u03c0 \u03b8 \u2225\u03c0 0 ) . (4)\nThe final loss function is composed of policy loss and value loss.\nL \u03b8 = \u2212 1 |S| T \u03c4 \u2208S T t=0 min(k t,\u03b8 A \u03b8 \u2032 , clip A \u03b8 \u2032 ), L \u03d5 = 1 |S| T \u03c4 \u2208S T t=0 (V \u03d5 (s t ) \u2212 R t ) 2 , L ppo = L \u03b8 + \u03bb v L \u03d5 .\n(5)\n\nHere, S denotes the sampled set, and T is for step numbers.\n\n\nImplementation\n\nRewriter For the frozen pipeline in \u00a73.1, we prompt an LLM to rewrite the query with few-shot in-context learning (Brown et al., 2020;Min et al., 2022).Our prompt follows the formulation of [instruction, demonstrations, input], where the input is x.The instruction is straightforward and demonstrations are 1-3 random examples from training sets and are kept constant across all runs, mainly for the task-specific output format illustration, i.e., a short phrase as an answer for HotpotQA, and an option as an answer for MMLU.For the training scheme in \u00a73.2, we fine-tuning a T5 as the rewriter.\n\nRetriever We use the Bing search engine as the retriever.It requires no candidate index construction like a dense retriever, nor candidates like a textbook.But it allows for a wide knowledge scope and up-to-time factuality.With Bing API, the retrieval is performed in two approaches.(i) For all retrieved web pages, we concatenate the snippets that are related sentences selected by Bing.This method is similar to using a search engine in a browser, input a query and press Enter, then collect the texts shown on the search result page.(ii) For retrieved web pages, we request the URLs and parser to get all the texts.This is similar to clicking on items on the search result page.Then we use BM25 to keep those with higher relevance scores with the query, reducing the document length.\n\nReader The reader is a frozen LLM, where we adopt ChatGPT (gpt-3.5-turbo)and Vicuna-13B.It performs reading comprehension and prediction with few-shot in-context learning.In our prompt, following the brief instruction and the demonstrations, the input is x or [doc, x] with retrieval augmentation.\n\nIt has been proved that both the phrasing of prompt lines (Zhang et al., 2023a) and the selection of demonstrations show effects on the in-context learning performance (Su et al., 2022;Zhang et al., 2023b).As it is not the focus of this work, we pay no more attention to prompt editing.\n\n\nExperiments\n\n\nTask Settings\n\n\nOpen-domain QA\n\nThree open-domain QA datasets are used for evaluation.(i) HotPotQA (Yang et al., 2018) consists of complex questions that require multi-hop reasoning.We evaluate the full test set.(ii) AmbigNQ (Min et al., 2020) provides a disambiguated version of Natural Questions (NQ) (Kwiatkowski et al., 2019).For ambiguous questions in NQ, minimal constraints are added to break it into several similar\n\n\nDirect prompt\n\nAnswer the question in the following format, end the answer with '**'.{demonstration} Question: {x} Answer:\n\n\nReader prompt in retrieval-augment pipelines\n\nAnswer the question in the following format, end the answer with '**'.{demonstration} Question: {doc} {x} Answer:\n\nPrompts for LLM as a frozen rewriter Open-domain QA: Think step by step to answer this question, and provide search engine queries for knowledge that you need.Split the queries with ';' and end the queries with '**'.{demonstration} Question: {x} Answer: Multiple choice QA: Provide a better search query for web search engine to answer the given question, end the queries with '**'.{demonstration} Question: {x} Answer: but specific questions.The first 1000 samples are evaluated in the test set.(iii) PopQA (Mallen et al., 2022) includes long-tail distributions as it contains more low-popularity knowledge than other popular QA tasks.We split the dataset into 13k for training and 714 for testing.\n\nOpen-domain QA benchmarks are sets of question-answer pairs denoted as {(q, a) i }.We use ChatGPT for both the reader and the frozen rewriter.The evaluation metrics are Exact Match (EM ) and F 1 scores.For the reward function in RL, we use an indicator to reward if the retrieved content hits the answer and penalize if misses the answer, denoted as Hit.The total reward is a weighted sum of EM, F 1 , and Hit.\nHit = 1 a in doc, \u22121 else R lm = EM + \u03bb f F 1 + \u03bb h Hit. (6)\n\nMultiple-choice QA\n\nFor multiple-choice QA, our evaluation is conducted on Massive Multi-task Language Understanding (MMLU) (Hendrycks et al., 2021), an exam question dataset including 4 categories: Humanities, STEM, Social Sciences, and Other.Each category is split into 80% for the training set and 20% for the test set.\n\nMultiple-choice QA can be formulated as {(q \u2032 , a) i }, where q \u2032 = [q, c 0 , c 1 , c 2 , c 3 ].c denotes the options, generally there are four for each question.The retrieved documents that are included in the officially provided contaminated lists are ignored.The questions with options are rewritten into search queries.The answer is one option.EM is reported as metrics and used for the reward.\nR lm = EM.(7)\nWe use ChatGPT as a frozen rewriter and the reader.\n\nWe also use Vicuna-13B as the reader for evaluation due to the rate limit issue of ChatGPT.More information on datasets and training setup are presented in the appendix.\n\n\nBaselines\n\nThe following settings are implemented to evaluate and support our methods.(i) Direct: The standard in-context learning without any augmentations.(ii) Retrieve-then-read: The standard retrieval-augmented method.Retrieved documents are concatenated with the question.(iii) LLM as a frozen rewriter: As is introduced in \u00a73.1, we prompt a frozen LLM to reason and generate queries by few-shot in-context learning.(iv) Trainable rewriter: Applying the fine-tuned rewriter, the output queries are used by the retriever and the reader.Table 1 presents prompt line forms.Please note that the prompts for prediction are kept the same for each task.\n\n\nResults\n\nExperimental results on open-domain QA are reported in Table 2.For the three datasets, query rewriting consistently brings performance gain with both a frozen rewriter and a trainable rewriter.On AmbigNQ and PopQA, the standard retrieval augments the reader, indicating useful external knowledge is retrieved.On HotpotQA, the standard retrieval hurts the reader.This shows that using complex questions as queries cannot compensate for the parametric knowledge, but bring noises instead (Mallen et al., 2022).This suggests that multi-hop questions are not suitable queries for the web search engine.The scores increase by adding the rewriting step.On PopQA, our trainable rewriter surpasses standard retrieval while being inferior to the LLM rewriter.This indicates that the distillation of query rewriting is sub-optimal.The scores on multiple-choice QA are presented in Table 3.With ChatGPT as a reader, it can be observed that query rewriting improves the scores in most of the settings, except for the social sciences category.With Vicuna as a reader, our method achieves more gains on the four categories compared to ChatGPT.This agrees with the intuition that a more powerful reader has more parametric memories, thus more difficult to compensate with external knowledge.6 Analysis\n\n\nModel\n\n\nTraining Process\n\nThe training process includes two stages, warm-up and reinforcement learning.This section shows the validation scores of the three open-domain QA datasets for further analysis.Figure 2 presents the metric scores through training iterations in the process of reinforcement learning.As the rewriting models have been warmed up on the pseudo data before RL, scores at \"0 iteration\" denote the ability acquired from the warm-up training.It can be observed that the curves show upward trends with some fluctuations on all the datasets.(i) For multi-hop questions in HotpotQA, the standard retrieval is relatively weaker.Complex questions can be not specific search queries and show a larger gap from rewritten queries, i.e., the green and red lines.(ii) On AmbigNQ and PopQA, our method surpasses the baselines after several iterations (3 or 4).This indicates that the RL training stage can compensate for the insufficiency of the distillation on the pseudo data during warm-up training.(iii) In particular, on PopQA, the trainable rewriter remains inferior to the LLM rewriter.This can be explained as the dataset is constructed for adaptive retrieval (Mallen et al., 2022), which only uses retrieval where it helps to avoid harmful redundant retrieval.Thus, \"None\" is a possible query that means no retrieval.This causes more complexity and uncertainty.LLM rewriter knows better when the retrieval is needed for itself as a reader, although the rewriting step is not concatenated as the input context of the reader.\n\nWe calculate the performance of query \"None\".The questions that can be correctly answered without retrieval (i.e., the \"Direct\" method) are those samples that need no more context.Comparing this retrieval-free set with those that are rewritten to be\"None\" query, the F 1 score of the LLM rewriter is 71.9% and the T5 rewriter score is 67.1%.If we consider the questions that can be correctly answered without retrieval but go wrong with retrieval as the retrieval-free set, the F 1 scores are 78.7% for LLM rewriter and 77.4% for T5.\n\n\nRetrieval Result\n\nOur proposed method is a pipeline framework, instead of an end-to-end system.The query rewriting first affects the retrieved context, then the context makes a difference to the output of the reader.Hence, QA metrics are indirect measurements.We take a closer look at the retrieved context and the reader capability through the retrieval metric, hit ratio.After text normalization, the hit rate is computed to measure whether the retrieved context contains the correct answers.\n\nTable 4 shows the scores on AmbigNQ.The scores in the second line are computed on a selection of the samples whose retrieved contexts hit correct answers (under the standard retrieve-thenread setting).The scores show the approximate upper bound ability of the reader with retrieval augmentation, abbreviated as the \"upper bound\" score.The effectiveness of retrieval is proved compared to the no retrieval setting (the first line).For each retrieval method, two settings are presented: (i) collecting Bing snippets, (ii) selecting from URLs by BM25.The metrics show that content selection with BM25 recalls better documents than snippets, 2 Our trainable rewriter is adapted to the retriever using BM25 during RL training.Using the output queries of the test set after training, the snippet hit rate is 73.4%.while query rewriting makes progress on both settings.We also observed that the improvement in the hit rate of the retriever is more significant than the improvement in the reader.This is consistent with the findings in related search (Mallen et al., 2022;Liu et al., 2023).\n\u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c \u274c \u274c \u274c \u274c \u274c\n\nCase Study\n\nTo intuitively show how the query rewriting makes a difference in the retrieved contexts and prediction performance, we present examples in Figure 3 to compare the original questions and the queries.In example 1, the original question asks for a film that the youngest daughter of Lady Mary-Gaye Curzon co-stars with two certain actors.Both query 1 and query 2 put the keyword film forward, closely following the youngest daughter of Lady Mary-Gaye Curzon.With both, the actress Charlotte Calthorpe and her movie information can be retrieved and the answer is included.The second is an example where the query from the LLM rewriter failed but the query from T5 gets the correct answer.The number 2000 is misunderstood in query 1, while query 2 keeps 200 movie together, avoiding meaningless retrieval.Example 3 is for multiple choice.\n\nThe query simplifies the background and enhances the keyword community planner.The retrieve contexts are mainly about Introduction to Community\n\nPlanning where the answer environment appears several times.\n\n\nConclusion\n\nThis paper introduces the Rewrite-Retrieve-Read pipeline, where a query rewriting step is added for the retrieval-augmented LLM.This approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever.Further, we propose to apply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader.\n\n\nLimitations\n\nWe acknowledge the limitations of this work.(i) There is still a trade-off between generalization and specialization among downstream tasks.Adding a training process, the scalability to direct transfer is compromised, compared to few-shot in-context learning.(ii) The research line of LLM agent has shown impressive performance but relies on multiple calls to the LLM for each sample (Khattab et al., 2022;Yao et al., 2023), where the LLM plays as an agent to flexibly call the retriever multiple times, reads the context in earlier hops, and generates follow-up questions.Different from these studies, our motivation is to enhance the oneturn retriever-then-read framework with a trainable query rewriter.(iii) Using a web search engine as the retriever also leads to some limitations.Neural dense retrievers that are based on professional, filtered knowledge bases may potentially achieve better and controllable retrieval.More discussion is included in the appendix.\n\n\nA Warm-up Dataset\n\nFor the warm-up training of the tuneable rewriter, we construct a pseudo dataset for the query rewriting task.For benchmarks that provide official training and test splits (HotpotQA and AmbigNQ), we use the whole training set.For those that have no official splits (PopQA and MMLU), we randomly split the full dataset.In detail, PopQA contains 16 types of questions, thus split into 13k for training and 714 for testing following stratified sampling.\n\nFor MMLU, each of the 4 categories is randomly split into 80% for the training set and 20% for the test set.Then the training sets of each benchmark are used to derive the pseudo dataset for the query rewriting, i.e., D T rain = {(x, x)|\u0177 = y}.\n\nWe present the statistics of the splits and warm-up dataset in Table 5.\n\n\nB Setup Details\n\nFor warm-up, we train the T5-large with 3e-5 learning rate, {16, 20} batch size, for {6,8,12} epochs.\n\nFor reinforcement learning, we set the sampling\n\u03b2 t+1 = \u03b2 t (1 + K \u03b2 e t ) ,\nwhere KL target is set to 0.2, K \u03b2 is set to 0.1.\u03b2 0 is initialized to be 0.001.The generation strategy follows the 4-beam search and returns the one sequence.In the implementation of the BM25based retriever, the textboxes from searched URLs are parsed from HTML code.We compute BM25 scores between the paragraph from each textbox and the query following the scikit-learn package, then keep those with higher scores until the reserved context reaches a max length.In reinforcement learning, the results of AmbigNQ are with the BM25 method, while others use snippets as context.\n\n\nC Web Search: Tool Use\n\nOur proposed pipeline integrates an externally built web search engine as the retriever module.We present more discussion on the advantages and disadvantages here.\n\nThe usage of external tools expands the ability boundary of language models, compensating for the parametric knowledge, and grounding the capabilities of language models to interact with environments (Qin et al., 2023;Schick et al., 2023).Recent studies show a trend to leverage plug-andplay tools like search engines to enhance language agents (Lazaridou et al., 2022;Menick et al., 2022;Shuster et al., 2022;Shen et al., 2023).Search engine APIs are well-developed retrievers, saving efforts to build and maintain another retriever, like a Contriever.Accessible to the whole Internet, the web search retrieves from a wide-range, up-to-date knowledge base.The temporal misalignment problem on a fixed candidate database can be alleviated.\n\nOn the other hand, web search APIs are commercial products requiring subscriptions.Also, the vast amount of knowledge on the web can be difficult to control.The retrieved context from the Internet can be occasionally inconsistent, redundant, and toxic, which hinders the LLM reader.\n\nBeyond retrieval augmentation, in a general scope, other tools called by LLMs, like code interpreters, online models, and expert applications, are all similar to search engines, without trainable parameters to optimize.There could be a gap between the LM and these tools.This paper proposes an idea to align them through a trainable small model.\n\nF1Figure 2 :\n2\nFigure 2: Reinforcement learning validation scores of (a)HotpotQA, (b)AmbigNQ, and (c)PopQA.The solid lines show EM (red) and F1 (blue) numbers through training iterations.The dashed lines are EM scores of the standard retrieve-then-read method (orange) and retrieval with an LLM as the rewriter (green).\n\n\nFigure 3 :\n3\nFigure 3: Examples for intuitive illustration.Q0 denotes original input, Q1 is from the LLM rewriter, and Q2 is from the trained T5 rewriter.Hit means retriever recall the answer, while Correct is for the reader output.\n\n\n\n\nbut be of 50\u00d7 smaller size.The Internet as a knowledge base More related to our work, the search engine can assume the role of the retriever and use the Internet as the source of\nInputInputInputExampleRetrieverRewriter Black-box LLMRewriter Small PrLMElia Kazan have in common? Input: What profession does Nicholas Ray andQueryQueryQuery: Nicholas Ray professionQuery: Elia Kazan professionDocumentsWeb SearchWeb SearchRetrieverRetrieverElia Kazan was an American film andtheatre director, producer,screenwriter and actor, described ......Black-box LLM ReaderDocumentsDocumentsNicholas Ray American author and director, original name RaymondNicholas Kienzle, born August 7,1911, Galesville, Wisconsin, U.S......OutputBlack-box LLM ReaderBlack-box LLM ReaderCorrect (reader)directorHit (retriever)OutputRewardOutput(a) Retrieve-then-read (b)Rewrite-retrieve-read(c) Trainable rewrite-retrieve-read\n\nTable 1 :\n1\nPrompt lines used for the LLMs.\n\n\nTable 2 :\n2\nMetrics of open-domain QA.\nEMF1HotpotQADirect32.3643.05Retrieve-then-read30.4741.34LLM rewriter32.8043.85Trainable rewriter34.3845.97AmbigNQDirect42.1053.05Retrieve-then-read45.8058.50LLM rewriter46.4058.74Trainable rewriter47.8060.71PopQADirect41.9444.61Retrieve-then-read43.2047.53LLM rewriter46.0049.74Trainable rewriter45.7249.51MMLUEMHuman. STEM Other SocialChatGPTDirect75.658.8 69.0 71.6Retrieve-then-read76.763.3 70.0 78.2LLM rewriter77.063.5 72.6 76.4Vicuna-13BDirect39.834.9 50.2 46.6Retrieve-then-read40.239.8 55.2 50.6LLM rewriter42.041.5 57.1 52.2Trainable rewriter43.240.9 59.3 51.2\n\nTable 3 :\n3\nMetrics of multiple choice QA.\n\n\nTable 4 :\n4\nRetrieval analysis on AmbigNQ.\nModelEMF1Hit ratioNo retrieval42.1053.05-Upper bound58.4069.45100Retrieve-then-readw/ snippet38.7050.5061.1w/ BM2545.8058.5076.4LLM rewriterw/ snippet39.8052.6463.5w/ BM2546.4058.7477.5Trainable rewriterw/ BM25 247.8060.7182.2\n\nTable 5 :\n5\nMetrics of multiple choice QA. steps to 5120, 10 threads, 512 steps for each.After sampling, the policy network is trained for {2,3,4} epochs, with learning rate as 2e-6 and batch size as {8,16}.\u03bbfand \u03bb h are 1.0.\u03b2 in Eq. 4 is dynamically adapted according toRamamurthy etal.(2022); Ziegler et al. (2019), e t = clip KL (\u03c0\u2225\u03c0 0 ) \u2212 KL target KL target , \u22120.2, 0.2 ,\n\n# Equal contribution.\u2020Corresponding author.This paper was partially supported by Joint Research Project of Yangtze River Delta Science and Technology Innovation Community (No. 2022CSJGG1400).\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint\n\nCan retriever-augmented language models reason? the blame game between the retriever and the language model. Parishad Behnamghader, Santiago Miret, Siva Reddy, arXiv:2212.091462022arXiv preprint\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nReading Wikipedia to answer opendomain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, Association for Computational Linguistics (ACL). 2017\n\n. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\u00e9 De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Joshua Achiam, Vedant Misra, Evan Morikawa, Alec RadfordJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. \n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191\n\nRetrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, International conference on machine learning. PMLR2020Panupong Pasupat, and Mingwei Chang\n\nMeasuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021\n\nNamgyu Ho, Laura Schmid, Se-Young Yun, arXiv:2212.10071Large language models are reasoning teachers. 2022arXiv preprint\n\nDistilling step-by-step! outperforming larger language models with less training data and smaller model sizes. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander J Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister, ArXiv, abs/2305.023012023\n\nFew-shot Learning with Retrieval Augmented Language Models. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave, 2022\n\nTemporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Minjoon Seo, 2022\n\nRetrieval as attention: End-to-end learning of retrieval and reading within a single transformer. Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, Graham Neubig, Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, UAE2022\n\nDense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020\n\nOmar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.14024Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. 2022arXiv preprint\n\nInternet-augmented dialogue generation. Mojtaba Komeili, Kurt Shuster, Jason Weston, 10.18653/v1/2022.acl-long.579Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221Long Papers)\n\nNatural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, 2019Transactions of the Association for Computational Linguistics\n\nInternetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022arXiv preprint\n\nYou only need one model for open-domain question answering. Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher Manning, Kyoung-Gu Woo, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics\n\nBART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020a. July 5-10, 2020\n\nTim Rockt\u00e4schel, et al. 2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Advances in Neural Information Processing Systems. 33\n\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan, arXiv:2302.11520Guiding large language models via directional stimulus prompting. 2023arXiv preprint\n\nKevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint\n\nTime waits for no one! analysis and challenges of temporal misalignment. Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, Noah A Smith, 10.18653/v1/2022.naacl-main.435Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterSeattle, United StatesHuman Language Technologies2022Association for Computational Linguistics\n\nWhen not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, Daniel Khashabi, 2022arXiv preprint\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, arXiv:2203.11147Teaching language models to support answers with verified quotes. 2022arXiv preprint\n\nRethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022\n\nAmbigQA: Answering ambiguous open-domain questions. Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, EMNLP. 2020\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, arXiv:2210.03350Measuring and narrowing the compositionality gap in language models. 2022arXiv preprint\n\nToolllm: Facilitating large language models to master 16000+ real-world apis. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, abs/2307.16789ArXiv preprint. 2023\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020\n\nIs reinforcement learning (not) for natural language processing. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi, Benchmarks, baselines, and building blocks for natural language policy optimization. 2022\n\nTemporal adaptation of BERT and performance on downstream document classification: Insights from social media. Paul R\u00f6ttger, Janet Pierrehumbert, 10.18653/v1/2021.findings-emnlp.206Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021\n\nEnd-toend training of multi-document reader and retriever for open-domain question answering. Devendra Singh Sachan, Siva Reddy, William L Hamilton, Chris Dyer, Dani Yogatama, Annual Conference on Neural Information Processing Systems. NeurIPS2021. 2021. 2021. December 6-14, 202134Advances in Neural Information Processing Systems\n\nToolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint\n\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438High-dimensional continuous control using generalized advantage estimation. 2015arXiv preprint\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint\n\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023arXiv preprint\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen-Tau Yih, arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint\n\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt completion. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab Emirates2022. December 7-11, 2022Association for Computational Linguistics\n\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, arXiv:2209.01975Selective annotation makes language models better fewshot learners. 2022arXiv preprint\n\nSelfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Denny Zhou, 10.48550/arXiv.2203.11171CoRR, abs/2203.111712022\n\nChain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018\n\nReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023\n\nGenerate rather than retrieve: Large language models are strong context generators. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, 2023In International Conference for Learning Representation (ICLR\n\nTempera: Test-time prompt editing via reinforcement learning. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E Gonzalez, The Eleventh International Conference on Learning Representations. 2023a\n\nAutomatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2023bICLR 2023\n\nNisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint\n", "annotations": {"author": "[{\"end\":299,\"start\":76},{\"end\":384,\"start\":300},{\"end\":419,\"start\":385},{\"end\":665,\"start\":420},{\"end\":749,\"start\":666}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":83},{\"end\":310,\"start\":306},{\"end\":397,\"start\":395},{\"end\":428,\"start\":424},{\"end\":674,\"start\":670}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":305,\"start\":300},{\"end\":394,\"start\":385},{\"end\":423,\"start\":420},{\"end\":669,\"start\":666}]", "author_affiliation": "[{\"end\":164,\"start\":87},{\"end\":298,\"start\":166},{\"end\":357,\"start\":333},{\"end\":383,\"start\":359},{\"end\":418,\"start\":399},{\"end\":530,\"start\":453},{\"end\":664,\"start\":532},{\"end\":722,\"start\":698},{\"end\":748,\"start\":724}]", "title": "[{\"end\":62,\"start\":1},{\"end\":811,\"start\":750}]", "venue": null, "abstract": "[{\"end\":2110,\"start\":881}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2314,\"start\":2293},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2333,\"start\":2314},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2356,\"start\":2333},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2551,\"start\":2533},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2569,\"start\":2551},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2629,\"start\":2596},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2646,\"start\":2629},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2664,\"start\":2646},{\"end\":3412,\"start\":3391},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3435,\"start\":3412},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3456,\"start\":3435},{\"end\":3899,\"start\":3878},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3916,\"start\":3899},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3937,\"start\":3916},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4152,\"start\":4134},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4383,\"start\":4365},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4404,\"start\":4383},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5692,\"start\":5668},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5747,\"start\":5725},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5764,\"start\":5747},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6555,\"start\":6536},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6583,\"start\":6565},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6612,\"start\":6591},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6667,\"start\":6643},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6734,\"start\":6713},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6781,\"start\":6760},{\"end\":6818,\"start\":6797},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7838,\"start\":7819},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7882,\"start\":7858},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8027,\"start\":8006},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8057,\"start\":8036},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8235,\"start\":8211},{\"end\":8255,\"start\":8235},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8390,\"start\":8369},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8407,\"start\":8390},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8426,\"start\":8407},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8550,\"start\":8529},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8567,\"start\":8550},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8586,\"start\":8567},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8811,\"start\":8789},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8885,\"start\":8861},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9161,\"start\":9140},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9308,\"start\":9287},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9565,\"start\":9541},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9602,\"start\":9581},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9648,\"start\":9630},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9745,\"start\":9724},{\"end\":9772,\"start\":9753},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9803,\"start\":9779},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10425,\"start\":10408},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10582,\"start\":10564},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10616,\"start\":10596},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10671,\"start\":10653},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10689,\"start\":10671},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10868,\"start\":10846},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11363,\"start\":11345},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11636,\"start\":11619},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13378,\"start\":13360},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13404,\"start\":13378},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13521,\"start\":13500},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13917,\"start\":13896},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14358,\"start\":14338},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14374,\"start\":14358},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16494,\"start\":16471},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16531,\"start\":16506},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17105,\"start\":17082},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17755,\"start\":17730},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17776,\"start\":17755},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18248,\"start\":18228},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18265,\"start\":18248},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19877,\"start\":19856},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19983,\"start\":19966},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20003,\"start\":19983},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20219,\"start\":20200},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20344,\"start\":20326},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20430,\"start\":20404},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21342,\"start\":21321},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22135,\"start\":22111},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24119,\"start\":24098},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26096,\"start\":26075},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28537,\"start\":28516},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28554,\"start\":28537},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30483,\"start\":30461},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30500,\"start\":30483},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33024,\"start\":33006},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33044,\"start\":33024},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33175,\"start\":33151},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33195,\"start\":33175},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33216,\"start\":33195},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33234,\"start\":33216}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34498,\"start\":34177},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34733,\"start\":34499},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35633,\"start\":34734},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35679,\"start\":35634},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36289,\"start\":35680},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36334,\"start\":36290},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36605,\"start\":36335},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":36984,\"start\":36606}]", "paragraph": "[{\"end\":3676,\"start\":2126},{\"end\":4682,\"start\":3678},{\"end\":5185,\"start\":4684},{\"end\":5961,\"start\":5187},{\"end\":6425,\"start\":5963},{\"end\":7031,\"start\":6427},{\"end\":7331,\"start\":7033},{\"end\":7347,\"start\":7333},{\"end\":7780,\"start\":7374},{\"end\":8890,\"start\":7782},{\"end\":9119,\"start\":8892},{\"end\":9649,\"start\":9121},{\"end\":10282,\"start\":9685},{\"end\":11039,\"start\":10284},{\"end\":11818,\"start\":11041},{\"end\":12128,\"start\":11820},{\"end\":12408,\"start\":12144},{\"end\":12971,\"start\":12434},{\"end\":13215,\"start\":12973},{\"end\":13660,\"start\":13236},{\"end\":14083,\"start\":13662},{\"end\":14787,\"start\":14104},{\"end\":14829,\"start\":14826},{\"end\":15233,\"start\":14831},{\"end\":16596,\"start\":15260},{\"end\":17106,\"start\":16781},{\"end\":17256,\"start\":17207},{\"end\":17777,\"start\":17258},{\"end\":17898,\"start\":17832},{\"end\":18034,\"start\":18031},{\"end\":18095,\"start\":18036},{\"end\":18709,\"start\":18114},{\"end\":19497,\"start\":18711},{\"end\":19796,\"start\":19499},{\"end\":20084,\"start\":19798},{\"end\":20524,\"start\":20133},{\"end\":20649,\"start\":20542},{\"end\":20811,\"start\":20698},{\"end\":21512,\"start\":20813},{\"end\":21924,\"start\":21514},{\"end\":22309,\"start\":22007},{\"end\":22709,\"start\":22311},{\"end\":22775,\"start\":22724},{\"end\":22946,\"start\":22777},{\"end\":23600,\"start\":22960},{\"end\":24898,\"start\":23612},{\"end\":26439,\"start\":24927},{\"end\":26974,\"start\":26441},{\"end\":27471,\"start\":26995},{\"end\":28555,\"start\":27473},{\"end\":29435,\"start\":28601},{\"end\":29580,\"start\":29437},{\"end\":29642,\"start\":29582},{\"end\":30061,\"start\":29657},{\"end\":31046,\"start\":30077},{\"end\":31518,\"start\":31068},{\"end\":31764,\"start\":31520},{\"end\":31837,\"start\":31766},{\"end\":31958,\"start\":31857},{\"end\":32007,\"start\":31960},{\"end\":32614,\"start\":32037},{\"end\":32804,\"start\":32641},{\"end\":33545,\"start\":32806},{\"end\":33829,\"start\":33547},{\"end\":34176,\"start\":33831},{\"end\":34497,\"start\":34193},{\"end\":34732,\"start\":34513},{\"end\":34915,\"start\":34737},{\"end\":35678,\"start\":35647},{\"end\":35719,\"start\":35693},{\"end\":36333,\"start\":36303},{\"end\":36378,\"start\":36348},{\"end\":36983,\"start\":36619}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14825,\"start\":14788},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16780,\"start\":16597},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17205,\"start\":17107},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17206,\"start\":17205},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17831,\"start\":17778},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18030,\"start\":17899},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21985,\"start\":21925},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22723,\"start\":22710},{\"attributes\":{\"id\":\"formula_8\"},\"end\":28587,\"start\":28556},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32036,\"start\":32008}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23496,\"start\":23495},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23674,\"start\":23673},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24490,\"start\":24489},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27480,\"start\":27479},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31836,\"start\":31835}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2124,\"start\":2112},{\"attributes\":{\"n\":\"2.1\"},\"end\":7372,\"start\":7350},{\"attributes\":{\"n\":\"2.2\"},\"end\":9683,\"start\":9652},{\"attributes\":{\"n\":\"3\"},\"end\":12142,\"start\":12131},{\"attributes\":{\"n\":\"3.1\"},\"end\":12432,\"start\":12411},{\"attributes\":{\"n\":\"3.2\"},\"end\":13234,\"start\":13218},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":14102,\"start\":14086},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":15258,\"start\":15236},{\"attributes\":{\"n\":\"4\"},\"end\":18112,\"start\":18098},{\"attributes\":{\"n\":\"5\"},\"end\":20098,\"start\":20087},{\"attributes\":{\"n\":\"5.1\"},\"end\":20114,\"start\":20101},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":20131,\"start\":20117},{\"end\":20540,\"start\":20527},{\"end\":20696,\"start\":20652},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":22005,\"start\":21987},{\"attributes\":{\"n\":\"5.2\"},\"end\":22958,\"start\":22949},{\"attributes\":{\"n\":\"5.3\"},\"end\":23610,\"start\":23603},{\"end\":24906,\"start\":24901},{\"attributes\":{\"n\":\"6.1\"},\"end\":24925,\"start\":24909},{\"attributes\":{\"n\":\"6.2\"},\"end\":26993,\"start\":26977},{\"attributes\":{\"n\":\"6.3\"},\"end\":28599,\"start\":28589},{\"attributes\":{\"n\":\"7\"},\"end\":29655,\"start\":29645},{\"end\":30075,\"start\":30064},{\"end\":31066,\"start\":31049},{\"end\":31855,\"start\":31840},{\"end\":32639,\"start\":32617},{\"end\":34190,\"start\":34178},{\"end\":34510,\"start\":34500},{\"end\":35644,\"start\":35635},{\"end\":35690,\"start\":35681},{\"end\":36300,\"start\":36291},{\"end\":36345,\"start\":36336},{\"end\":36616,\"start\":36607}]", "table": "[{\"end\":35633,\"start\":34916},{\"end\":36289,\"start\":35720},{\"end\":36605,\"start\":36379}]", "figure_caption": "[{\"end\":34498,\"start\":34192},{\"end\":34733,\"start\":34512},{\"end\":34916,\"start\":34736},{\"end\":35679,\"start\":35646},{\"end\":35720,\"start\":35692},{\"end\":36334,\"start\":36302},{\"end\":36379,\"start\":36347},{\"end\":36984,\"start\":36618}]", "figure_ref": "[{\"end\":5504,\"start\":5503},{\"end\":6047,\"start\":6046},{\"end\":8900,\"start\":8899},{\"end\":12279,\"start\":12278},{\"end\":13823,\"start\":13822},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25111,\"start\":25110},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28749,\"start\":28748}]", "bib_author_first_name": "[{\"end\":37182,\"start\":37177},{\"end\":37195,\"start\":37189},{\"end\":37215,\"start\":37209},{\"end\":37229,\"start\":37221},{\"end\":37238,\"start\":37235},{\"end\":37248,\"start\":37243},{\"end\":37260,\"start\":37256},{\"end\":37275,\"start\":37270},{\"end\":37288,\"start\":37280},{\"end\":37298,\"start\":37293},{\"end\":37311,\"start\":37306},{\"end\":37313,\"start\":37312},{\"end\":37321,\"start\":37318},{\"end\":37333,\"start\":37326},{\"end\":37601,\"start\":37593},{\"end\":37624,\"start\":37616},{\"end\":37636,\"start\":37632},{\"end\":37722,\"start\":37719},{\"end\":37738,\"start\":37730},{\"end\":37749,\"start\":37745},{\"end\":37764,\"start\":37757},{\"end\":37779,\"start\":37774},{\"end\":37781,\"start\":37780},{\"end\":37798,\"start\":37790},{\"end\":37815,\"start\":37809},{\"end\":37835,\"start\":37829},{\"end\":37849,\"start\":37843},{\"end\":37864,\"start\":37858},{\"end\":37987,\"start\":37982},{\"end\":37998,\"start\":37994},{\"end\":38011,\"start\":38006},{\"end\":38027,\"start\":38020},{\"end\":38097,\"start\":38093},{\"end\":38109,\"start\":38104},{\"end\":38124,\"start\":38118},{\"end\":38136,\"start\":38130},{\"end\":38151,\"start\":38143},{\"end\":38182,\"start\":38177},{\"end\":38199,\"start\":38191},{\"end\":38213,\"start\":38209},{\"end\":38229,\"start\":38221},{\"end\":38242,\"start\":38238},{\"end\":38257,\"start\":38253},{\"end\":38267,\"start\":38263},{\"end\":38282,\"start\":38274},{\"end\":38299,\"start\":38292},{\"end\":38313,\"start\":38308},{\"end\":38328,\"start\":38322},{\"end\":38343,\"start\":38337},{\"end\":38359,\"start\":38353},{\"end\":38371,\"start\":38366},{\"end\":38382,\"start\":38378},{\"end\":38397,\"start\":38390},{\"end\":38413,\"start\":38406},{\"end\":38427,\"start\":38421},{\"end\":38444,\"start\":38436},{\"end\":38462,\"start\":38455},{\"end\":38479,\"start\":38471},{\"end\":38512,\"start\":38506},{\"end\":38529,\"start\":38521},{\"end\":38543,\"start\":38536},{\"end\":38565,\"start\":38554},{\"end\":38783,\"start\":38776},{\"end\":38799,\"start\":38792},{\"end\":38806,\"start\":38804},{\"end\":38816,\"start\":38812},{\"end\":38832,\"start\":38824},{\"end\":38840,\"start\":38837},{\"end\":38855,\"start\":38848},{\"end\":38869,\"start\":38863},{\"end\":38885,\"start\":38878},{\"end\":38900,\"start\":38894},{\"end\":38902,\"start\":38901},{\"end\":39034,\"start\":39025},{\"end\":39052,\"start\":39046},{\"end\":39066,\"start\":39061},{\"end\":39082,\"start\":39075},{\"end\":39096,\"start\":39090},{\"end\":39109,\"start\":39105},{\"end\":39123,\"start\":39119},{\"end\":39137,\"start\":39132},{\"end\":39141,\"start\":39138},{\"end\":39156,\"start\":39149},{\"end\":39174,\"start\":39165},{\"end\":39355,\"start\":39350},{\"end\":39372,\"start\":39364},{\"end\":39386,\"start\":39380},{\"end\":39400,\"start\":39392},{\"end\":39897,\"start\":39891},{\"end\":39909,\"start\":39903},{\"end\":39919,\"start\":39915},{\"end\":40072,\"start\":40069},{\"end\":40090,\"start\":40084},{\"end\":40104,\"start\":40098},{\"end\":40117,\"start\":40113},{\"end\":40129,\"start\":40123},{\"end\":40143,\"start\":40139},{\"end\":40155,\"start\":40150},{\"end\":40323,\"start\":40317},{\"end\":40333,\"start\":40328},{\"end\":40350,\"start\":40342},{\"end\":40557,\"start\":40549},{\"end\":40575,\"start\":40565},{\"end\":40589,\"start\":40580},{\"end\":40601,\"start\":40595},{\"end\":40619,\"start\":40611},{\"end\":40636,\"start\":40627},{\"end\":40638,\"start\":40637},{\"end\":40653,\"start\":40647},{\"end\":40670,\"start\":40663},{\"end\":40681,\"start\":40676},{\"end\":40785,\"start\":40778},{\"end\":40802,\"start\":40795},{\"end\":40815,\"start\":40810},{\"end\":40829,\"start\":40824},{\"end\":40845,\"start\":40840},{\"end\":40859,\"start\":40855},{\"end\":40872,\"start\":40868},{\"end\":40891,\"start\":40885},{\"end\":40909,\"start\":40900},{\"end\":40925,\"start\":40918},{\"end\":41037,\"start\":41033},{\"end\":41054,\"start\":41044},{\"end\":41066,\"start\":41059},{\"end\":41077,\"start\":41072},{\"end\":41091,\"start\":41084},{\"end\":41106,\"start\":41098},{\"end\":41121,\"start\":41112},{\"end\":41134,\"start\":41127},{\"end\":41252,\"start\":41244},{\"end\":41264,\"start\":41260},{\"end\":41273,\"start\":41270},{\"end\":41286,\"start\":41281},{\"end\":41299,\"start\":41293},{\"end\":41311,\"start\":41306},{\"end\":41326,\"start\":41320},{\"end\":41486,\"start\":41478},{\"end\":41504,\"start\":41498},{\"end\":41516,\"start\":41511},{\"end\":41529,\"start\":41522},{\"end\":41543,\"start\":41537},{\"end\":41554,\"start\":41548},{\"end\":41568,\"start\":41563},{\"end\":41582,\"start\":41575},{\"end\":41853,\"start\":41849},{\"end\":41869,\"start\":41863},{\"end\":41885,\"start\":41881},{\"end\":41898,\"start\":41893},{\"end\":41908,\"start\":41903},{\"end\":41926,\"start\":41915},{\"end\":41939,\"start\":41934},{\"end\":42135,\"start\":42128},{\"end\":42149,\"start\":42145},{\"end\":42164,\"start\":42159},{\"end\":42505,\"start\":42502},{\"end\":42529,\"start\":42519},{\"end\":42546,\"start\":42540},{\"end\":42564,\"start\":42557},{\"end\":42579,\"start\":42574},{\"end\":42593,\"start\":42588},{\"end\":42611,\"start\":42603},{\"end\":42626,\"start\":42621},{\"end\":42644,\"start\":42639},{\"end\":42659,\"start\":42653},{\"end\":42837,\"start\":42829},{\"end\":42854,\"start\":42849},{\"end\":42876,\"start\":42868},{\"end\":42895,\"start\":42888},{\"end\":43008,\"start\":43002},{\"end\":43019,\"start\":43014},{\"end\":43034,\"start\":43027},{\"end\":43046,\"start\":43040},{\"end\":43069,\"start\":43058},{\"end\":43088,\"start\":43079},{\"end\":43450,\"start\":43446},{\"end\":43464,\"start\":43458},{\"end\":43475,\"start\":43470},{\"end\":43489,\"start\":43483},{\"end\":43516,\"start\":43505},{\"end\":43530,\"start\":43526},{\"end\":43544,\"start\":43537},{\"end\":43559,\"start\":43555},{\"end\":43958,\"start\":43951},{\"end\":43971,\"start\":43966},{\"end\":43989,\"start\":43979},{\"end\":44003,\"start\":43998},{\"end\":44021,\"start\":44013},{\"end\":44038,\"start\":44033},{\"end\":44054,\"start\":44046},{\"end\":44068,\"start\":44064},{\"end\":44083,\"start\":44076},{\"end\":44149,\"start\":44144},{\"end\":44160,\"start\":44154},{\"end\":44176,\"start\":44167},{\"end\":44187,\"start\":44181},{\"end\":44204,\"start\":44196},{\"end\":44216,\"start\":44210},{\"end\":44329,\"start\":44324},{\"end\":44348,\"start\":44344},{\"end\":44360,\"start\":44354},{\"end\":44376,\"start\":44369},{\"end\":44393,\"start\":44388},{\"end\":44411,\"start\":44406},{\"end\":44602,\"start\":44596},{\"end\":44614,\"start\":44608},{\"end\":44631,\"start\":44625},{\"end\":44652,\"start\":44644},{\"end\":44666,\"start\":44662},{\"end\":44668,\"start\":44667},{\"end\":45043,\"start\":45039},{\"end\":45057,\"start\":45052},{\"end\":45070,\"start\":45064},{\"end\":45086,\"start\":45078},{\"end\":45100,\"start\":45092},{\"end\":45119,\"start\":45113},{\"end\":45155,\"start\":45150},{\"end\":45168,\"start\":45164},{\"end\":45186,\"start\":45178},{\"end\":45200,\"start\":45196},{\"end\":45219,\"start\":45212},{\"end\":45232,\"start\":45226},{\"end\":45246,\"start\":45243},{\"end\":45263,\"start\":45255},{\"end\":45275,\"start\":45271},{\"end\":45305,\"start\":45297},{\"end\":45498,\"start\":45493},{\"end\":45509,\"start\":45504},{\"end\":45518,\"start\":45515},{\"end\":45534,\"start\":45529},{\"end\":45548,\"start\":45544},{\"end\":45564,\"start\":45556},{\"end\":45581,\"start\":45577},{\"end\":45889,\"start\":45884},{\"end\":45901,\"start\":45895},{\"end\":45919,\"start\":45911},{\"end\":45936,\"start\":45932},{\"end\":46036,\"start\":46032},{\"end\":46052,\"start\":46045},{\"end\":46059,\"start\":46057},{\"end\":46072,\"start\":46067},{\"end\":46089,\"start\":46082},{\"end\":46108,\"start\":46102},{\"end\":46123,\"start\":46118},{\"end\":46139,\"start\":46131},{\"end\":46157,\"start\":46149},{\"end\":46169,\"start\":46165},{\"end\":46238,\"start\":46234},{\"end\":46250,\"start\":46246},{\"end\":46263,\"start\":46258},{\"end\":46275,\"start\":46269},{\"end\":46289,\"start\":46285},{\"end\":46291,\"start\":46290},{\"end\":46303,\"start\":46299},{\"end\":46499,\"start\":46494},{\"end\":46511,\"start\":46505},{\"end\":46525,\"start\":46519},{\"end\":46536,\"start\":46530},{\"end\":46545,\"start\":46542},{\"end\":46555,\"start\":46551},{\"end\":46566,\"start\":46560},{\"end\":46575,\"start\":46572},{\"end\":46589,\"start\":46582},{\"end\":46600,\"start\":46596},{\"end\":46731,\"start\":46726},{\"end\":46744,\"start\":46740},{\"end\":46758,\"start\":46754},{\"end\":46777,\"start\":46768},{\"end\":46789,\"start\":46783},{\"end\":46805,\"start\":46798},{\"end\":46819,\"start\":46814},{\"end\":46829,\"start\":46826},{\"end\":46839,\"start\":46834},{\"end\":46841,\"start\":46840},{\"end\":46969,\"start\":46961},{\"end\":46992,\"start\":46982},{\"end\":47012,\"start\":47006},{\"end\":47027,\"start\":47023},{\"end\":47041,\"start\":47036},{\"end\":47057,\"start\":47048},{\"end\":47077,\"start\":47069},{\"end\":47095,\"start\":47090},{\"end\":47308,\"start\":47304},{\"end\":47323,\"start\":47318},{\"end\":47624,\"start\":47616},{\"end\":47643,\"start\":47639},{\"end\":47658,\"start\":47651},{\"end\":47660,\"start\":47659},{\"end\":47676,\"start\":47671},{\"end\":47687,\"start\":47683},{\"end\":47922,\"start\":47918},{\"end\":47935,\"start\":47931},{\"end\":47955,\"start\":47948},{\"end\":47970,\"start\":47963},{\"end\":47986,\"start\":47981},{\"end\":47999,\"start\":47995},{\"end\":48019,\"start\":48013},{\"end\":48036,\"start\":48030},{\"end\":48086,\"start\":48082},{\"end\":48104,\"start\":48097},{\"end\":48119,\"start\":48113},{\"end\":48135,\"start\":48128},{\"end\":48150,\"start\":48144},{\"end\":48275,\"start\":48271},{\"end\":48291,\"start\":48286},{\"end\":48308,\"start\":48300},{\"end\":48323,\"start\":48319},{\"end\":48337,\"start\":48333},{\"end\":48506,\"start\":48497},{\"end\":48519,\"start\":48513},{\"end\":48528,\"start\":48526},{\"end\":48543,\"start\":48534},{\"end\":48555,\"start\":48548},{\"end\":48567,\"start\":48560},{\"end\":48618,\"start\":48612},{\"end\":48629,\"start\":48624},{\"end\":48644,\"start\":48635},{\"end\":48662,\"start\":48655},{\"end\":48672,\"start\":48668},{\"end\":48684,\"start\":48680},{\"end\":48696,\"start\":48692},{\"end\":48717,\"start\":48710},{\"end\":48922,\"start\":48918},{\"end\":48939,\"start\":48932},{\"end\":48956,\"start\":48949},{\"end\":48973,\"start\":48966},{\"end\":48988,\"start\":48982},{\"end\":49001,\"start\":48996},{\"end\":49187,\"start\":49180},{\"end\":49197,\"start\":49192},{\"end\":49209,\"start\":49205},{\"end\":49215,\"start\":49210},{\"end\":49226,\"start\":49220},{\"end\":49238,\"start\":49232},{\"end\":49250,\"start\":49245},{\"end\":49259,\"start\":49256},{\"end\":49271,\"start\":49267},{\"end\":49287,\"start\":49283},{\"end\":49305,\"start\":49301},{\"end\":49307,\"start\":49306},{\"end\":49497,\"start\":49491},{\"end\":49509,\"start\":49504},{\"end\":49519,\"start\":49515},{\"end\":49536,\"start\":49532},{\"end\":49538,\"start\":49537},{\"end\":49545,\"start\":49543},{\"end\":49547,\"start\":49546},{\"end\":49558,\"start\":49553},{\"end\":49692,\"start\":49687},{\"end\":49704,\"start\":49698},{\"end\":49715,\"start\":49711},{\"end\":49735,\"start\":49728},{\"end\":49748,\"start\":49743},{\"end\":49760,\"start\":49757},{\"end\":49768,\"start\":49766},{\"end\":49770,\"start\":49769},{\"end\":49777,\"start\":49776},{\"end\":49789,\"start\":49784},{\"end\":49897,\"start\":49891},{\"end\":49908,\"start\":49904},{\"end\":49921,\"start\":49913},{\"end\":49935,\"start\":49929},{\"end\":49951,\"start\":49944},{\"end\":49965,\"start\":49959},{\"end\":49992,\"start\":49981},{\"end\":49994,\"start\":49993},{\"end\":50313,\"start\":50307},{\"end\":50326,\"start\":50319},{\"end\":50337,\"start\":50333},{\"end\":50345,\"start\":50342},{\"end\":50355,\"start\":50350},{\"end\":50372,\"start\":50365},{\"end\":50389,\"start\":50385},{\"end\":50552,\"start\":50546},{\"end\":50560,\"start\":50557},{\"end\":50575,\"start\":50567},{\"end\":50589,\"start\":50582},{\"end\":50602,\"start\":50594},{\"end\":50613,\"start\":50607},{\"end\":50631,\"start\":50622},{\"end\":50644,\"start\":50637},{\"end\":50655,\"start\":50651},{\"end\":50799,\"start\":50792},{\"end\":50813,\"start\":50807},{\"end\":50825,\"start\":50820},{\"end\":50836,\"start\":50832},{\"end\":50855,\"start\":50849},{\"end\":50857,\"start\":50856},{\"end\":51014,\"start\":51005},{\"end\":51027,\"start\":51022},{\"end\":51037,\"start\":51035},{\"end\":51046,\"start\":51042},{\"end\":51142,\"start\":51137},{\"end\":51168,\"start\":51161},{\"end\":51182,\"start\":51179},{\"end\":51184,\"start\":51183},{\"end\":51193,\"start\":51189},{\"end\":51206,\"start\":51201},{\"end\":51220,\"start\":51216},{\"end\":51237,\"start\":51229}]", "bib_author_last_name": "[{\"end\":37187,\"start\":37183},{\"end\":37207,\"start\":37196},{\"end\":37219,\"start\":37216},{\"end\":37233,\"start\":37230},{\"end\":37241,\"start\":37239},{\"end\":37254,\"start\":37249},{\"end\":37268,\"start\":37261},{\"end\":37278,\"start\":37276},{\"end\":37291,\"start\":37289},{\"end\":37304,\"start\":37299},{\"end\":37316,\"start\":37314},{\"end\":37324,\"start\":37322},{\"end\":37338,\"start\":37334},{\"end\":37614,\"start\":37602},{\"end\":37630,\"start\":37625},{\"end\":37642,\"start\":37637},{\"end\":37728,\"start\":37723},{\"end\":37743,\"start\":37739},{\"end\":37755,\"start\":37750},{\"end\":37772,\"start\":37765},{\"end\":37788,\"start\":37782},{\"end\":37807,\"start\":37799},{\"end\":37827,\"start\":37816},{\"end\":37841,\"start\":37836},{\"end\":37856,\"start\":37850},{\"end\":37871,\"start\":37865},{\"end\":37992,\"start\":37988},{\"end\":38004,\"start\":37999},{\"end\":38018,\"start\":38012},{\"end\":38034,\"start\":38028},{\"end\":38102,\"start\":38098},{\"end\":38116,\"start\":38110},{\"end\":38128,\"start\":38125},{\"end\":38141,\"start\":38137},{\"end\":38175,\"start\":38152},{\"end\":38189,\"start\":38183},{\"end\":38207,\"start\":38200},{\"end\":38219,\"start\":38214},{\"end\":38236,\"start\":38230},{\"end\":38251,\"start\":38243},{\"end\":38261,\"start\":38258},{\"end\":38272,\"start\":38268},{\"end\":38290,\"start\":38283},{\"end\":38306,\"start\":38300},{\"end\":38320,\"start\":38314},{\"end\":38335,\"start\":38329},{\"end\":38351,\"start\":38344},{\"end\":38364,\"start\":38360},{\"end\":38376,\"start\":38372},{\"end\":38388,\"start\":38383},{\"end\":38404,\"start\":38398},{\"end\":38419,\"start\":38414},{\"end\":38434,\"start\":38428},{\"end\":38453,\"start\":38445},{\"end\":38469,\"start\":38463},{\"end\":38504,\"start\":38480},{\"end\":38519,\"start\":38513},{\"end\":38534,\"start\":38530},{\"end\":38552,\"start\":38544},{\"end\":38571,\"start\":38566},{\"end\":38790,\"start\":38784},{\"end\":38802,\"start\":38800},{\"end\":38810,\"start\":38807},{\"end\":38822,\"start\":38817},{\"end\":38835,\"start\":38833},{\"end\":38846,\"start\":38841},{\"end\":38861,\"start\":38856},{\"end\":38876,\"start\":38870},{\"end\":38892,\"start\":38886},{\"end\":38911,\"start\":38903},{\"end\":39044,\"start\":39035},{\"end\":39059,\"start\":39053},{\"end\":39073,\"start\":39067},{\"end\":39088,\"start\":39083},{\"end\":39103,\"start\":39097},{\"end\":39117,\"start\":39110},{\"end\":39130,\"start\":39124},{\"end\":39147,\"start\":39142},{\"end\":39163,\"start\":39157},{\"end\":39183,\"start\":39175},{\"end\":39362,\"start\":39356},{\"end\":39378,\"start\":39373},{\"end\":39390,\"start\":39387},{\"end\":39410,\"start\":39401},{\"end\":39901,\"start\":39898},{\"end\":39913,\"start\":39910},{\"end\":39924,\"start\":39920},{\"end\":40082,\"start\":40073},{\"end\":40096,\"start\":40091},{\"end\":40111,\"start\":40105},{\"end\":40121,\"start\":40118},{\"end\":40137,\"start\":40130},{\"end\":40148,\"start\":40144},{\"end\":40166,\"start\":40156},{\"end\":40326,\"start\":40324},{\"end\":40340,\"start\":40334},{\"end\":40354,\"start\":40351},{\"end\":40563,\"start\":40558},{\"end\":40578,\"start\":40576},{\"end\":40593,\"start\":40590},{\"end\":40609,\"start\":40602},{\"end\":40625,\"start\":40620},{\"end\":40645,\"start\":40639},{\"end\":40661,\"start\":40654},{\"end\":40674,\"start\":40671},{\"end\":40689,\"start\":40682},{\"end\":40793,\"start\":40786},{\"end\":40808,\"start\":40803},{\"end\":40822,\"start\":40816},{\"end\":40838,\"start\":40830},{\"end\":40853,\"start\":40846},{\"end\":40866,\"start\":40860},{\"end\":40883,\"start\":40873},{\"end\":40898,\"start\":40892},{\"end\":40916,\"start\":40910},{\"end\":40931,\"start\":40926},{\"end\":41042,\"start\":41038},{\"end\":41057,\"start\":41055},{\"end\":41070,\"start\":41067},{\"end\":41082,\"start\":41078},{\"end\":41096,\"start\":41092},{\"end\":41110,\"start\":41107},{\"end\":41125,\"start\":41122},{\"end\":41138,\"start\":41135},{\"end\":41258,\"start\":41253},{\"end\":41268,\"start\":41265},{\"end\":41279,\"start\":41274},{\"end\":41291,\"start\":41287},{\"end\":41304,\"start\":41300},{\"end\":41318,\"start\":41312},{\"end\":41333,\"start\":41327},{\"end\":41496,\"start\":41487},{\"end\":41509,\"start\":41505},{\"end\":41520,\"start\":41517},{\"end\":41535,\"start\":41530},{\"end\":41546,\"start\":41544},{\"end\":41561,\"start\":41555},{\"end\":41573,\"start\":41569},{\"end\":41586,\"start\":41583},{\"end\":41861,\"start\":41854},{\"end\":41879,\"start\":41870},{\"end\":41891,\"start\":41886},{\"end\":41901,\"start\":41899},{\"end\":41913,\"start\":41909},{\"end\":41932,\"start\":41927},{\"end\":41945,\"start\":41940},{\"end\":41954,\"start\":41947},{\"end\":42143,\"start\":42136},{\"end\":42157,\"start\":42150},{\"end\":42171,\"start\":42165},{\"end\":42517,\"start\":42506},{\"end\":42538,\"start\":42530},{\"end\":42555,\"start\":42547},{\"end\":42572,\"start\":42565},{\"end\":42586,\"start\":42580},{\"end\":42601,\"start\":42594},{\"end\":42619,\"start\":42612},{\"end\":42637,\"start\":42627},{\"end\":42651,\"start\":42645},{\"end\":42663,\"start\":42660},{\"end\":42847,\"start\":42838},{\"end\":42866,\"start\":42855},{\"end\":42886,\"start\":42877},{\"end\":42904,\"start\":42896},{\"end\":43012,\"start\":43009},{\"end\":43025,\"start\":43020},{\"end\":43038,\"start\":43035},{\"end\":43056,\"start\":43047},{\"end\":43077,\"start\":43070},{\"end\":43092,\"start\":43089},{\"end\":43456,\"start\":43451},{\"end\":43468,\"start\":43465},{\"end\":43481,\"start\":43476},{\"end\":43503,\"start\":43490},{\"end\":43524,\"start\":43517},{\"end\":43535,\"start\":43531},{\"end\":43553,\"start\":43545},{\"end\":43571,\"start\":43560},{\"end\":43964,\"start\":43959},{\"end\":43977,\"start\":43972},{\"end\":43996,\"start\":43990},{\"end\":44011,\"start\":44004},{\"end\":44031,\"start\":44022},{\"end\":44044,\"start\":44039},{\"end\":44062,\"start\":44055},{\"end\":44074,\"start\":44069},{\"end\":44087,\"start\":44084},{\"end\":44152,\"start\":44150},{\"end\":44165,\"start\":44161},{\"end\":44179,\"start\":44177},{\"end\":44194,\"start\":44188},{\"end\":44208,\"start\":44205},{\"end\":44220,\"start\":44217},{\"end\":44342,\"start\":44330},{\"end\":44352,\"start\":44349},{\"end\":44367,\"start\":44361},{\"end\":44386,\"start\":44377},{\"end\":44404,\"start\":44394},{\"end\":44419,\"start\":44412},{\"end\":44426,\"start\":44421},{\"end\":44606,\"start\":44603},{\"end\":44623,\"start\":44615},{\"end\":44642,\"start\":44632},{\"end\":44660,\"start\":44653},{\"end\":44674,\"start\":44669},{\"end\":45050,\"start\":45044},{\"end\":45062,\"start\":45058},{\"end\":45076,\"start\":45071},{\"end\":45090,\"start\":45087},{\"end\":45111,\"start\":45101},{\"end\":45128,\"start\":45120},{\"end\":45162,\"start\":45156},{\"end\":45176,\"start\":45169},{\"end\":45194,\"start\":45187},{\"end\":45210,\"start\":45201},{\"end\":45224,\"start\":45220},{\"end\":45241,\"start\":45233},{\"end\":45253,\"start\":45247},{\"end\":45269,\"start\":45264},{\"end\":45295,\"start\":45276},{\"end\":45312,\"start\":45306},{\"end\":45502,\"start\":45499},{\"end\":45513,\"start\":45510},{\"end\":45527,\"start\":45519},{\"end\":45542,\"start\":45535},{\"end\":45554,\"start\":45549},{\"end\":45575,\"start\":45565},{\"end\":45593,\"start\":45582},{\"end\":45893,\"start\":45890},{\"end\":45909,\"start\":45902},{\"end\":45930,\"start\":45920},{\"end\":45948,\"start\":45937},{\"end\":46043,\"start\":46037},{\"end\":46055,\"start\":46053},{\"end\":46065,\"start\":46060},{\"end\":46080,\"start\":46073},{\"end\":46100,\"start\":46090},{\"end\":46116,\"start\":46109},{\"end\":46129,\"start\":46124},{\"end\":46147,\"start\":46140},{\"end\":46163,\"start\":46158},{\"end\":46173,\"start\":46170},{\"end\":46244,\"start\":46239},{\"end\":46256,\"start\":46251},{\"end\":46267,\"start\":46264},{\"end\":46283,\"start\":46276},{\"end\":46297,\"start\":46292},{\"end\":46309,\"start\":46304},{\"end\":46503,\"start\":46500},{\"end\":46517,\"start\":46512},{\"end\":46528,\"start\":46526},{\"end\":46540,\"start\":46537},{\"end\":46549,\"start\":46546},{\"end\":46558,\"start\":46556},{\"end\":46570,\"start\":46567},{\"end\":46580,\"start\":46576},{\"end\":46594,\"start\":46590},{\"end\":46605,\"start\":46601},{\"end\":46738,\"start\":46732},{\"end\":46752,\"start\":46745},{\"end\":46766,\"start\":46759},{\"end\":46781,\"start\":46778},{\"end\":46796,\"start\":46790},{\"end\":46812,\"start\":46806},{\"end\":46824,\"start\":46820},{\"end\":46832,\"start\":46830},{\"end\":46845,\"start\":46842},{\"end\":46980,\"start\":46970},{\"end\":47004,\"start\":46993},{\"end\":47021,\"start\":47013},{\"end\":47034,\"start\":47028},{\"end\":47046,\"start\":47042},{\"end\":47067,\"start\":47058},{\"end\":47088,\"start\":47078},{\"end\":47100,\"start\":47096},{\"end\":47316,\"start\":47309},{\"end\":47337,\"start\":47324},{\"end\":47637,\"start\":47625},{\"end\":47649,\"start\":47644},{\"end\":47669,\"start\":47661},{\"end\":47681,\"start\":47677},{\"end\":47696,\"start\":47688},{\"end\":47929,\"start\":47923},{\"end\":47946,\"start\":47936},{\"end\":47961,\"start\":47956},{\"end\":47979,\"start\":47971},{\"end\":47993,\"start\":47987},{\"end\":48011,\"start\":48000},{\"end\":48028,\"start\":48020},{\"end\":48044,\"start\":48037},{\"end\":48095,\"start\":48087},{\"end\":48111,\"start\":48105},{\"end\":48126,\"start\":48120},{\"end\":48142,\"start\":48136},{\"end\":48157,\"start\":48151},{\"end\":48284,\"start\":48276},{\"end\":48298,\"start\":48292},{\"end\":48317,\"start\":48309},{\"end\":48331,\"start\":48324},{\"end\":48344,\"start\":48338},{\"end\":48511,\"start\":48507},{\"end\":48524,\"start\":48520},{\"end\":48532,\"start\":48529},{\"end\":48546,\"start\":48544},{\"end\":48558,\"start\":48556},{\"end\":48574,\"start\":48568},{\"end\":48622,\"start\":48619},{\"end\":48633,\"start\":48630},{\"end\":48653,\"start\":48645},{\"end\":48666,\"start\":48663},{\"end\":48678,\"start\":48673},{\"end\":48690,\"start\":48685},{\"end\":48708,\"start\":48697},{\"end\":48721,\"start\":48718},{\"end\":48930,\"start\":48923},{\"end\":48947,\"start\":48940},{\"end\":48964,\"start\":48957},{\"end\":48980,\"start\":48974},{\"end\":48994,\"start\":48989},{\"end\":49008,\"start\":49002},{\"end\":49190,\"start\":49188},{\"end\":49203,\"start\":49198},{\"end\":49218,\"start\":49216},{\"end\":49230,\"start\":49227},{\"end\":49243,\"start\":49239},{\"end\":49254,\"start\":49251},{\"end\":49265,\"start\":49260},{\"end\":49281,\"start\":49272},{\"end\":49299,\"start\":49288},{\"end\":49313,\"start\":49308},{\"end\":49502,\"start\":49498},{\"end\":49513,\"start\":49510},{\"end\":49530,\"start\":49520},{\"end\":49541,\"start\":49539},{\"end\":49551,\"start\":49548},{\"end\":49563,\"start\":49559},{\"end\":49696,\"start\":49693},{\"end\":49709,\"start\":49705},{\"end\":49726,\"start\":49716},{\"end\":49741,\"start\":49736},{\"end\":49755,\"start\":49749},{\"end\":49764,\"start\":49761},{\"end\":49774,\"start\":49771},{\"end\":49782,\"start\":49778},{\"end\":49792,\"start\":49790},{\"end\":49798,\"start\":49794},{\"end\":49902,\"start\":49898},{\"end\":49911,\"start\":49909},{\"end\":49927,\"start\":49922},{\"end\":49942,\"start\":49936},{\"end\":49957,\"start\":49952},{\"end\":49979,\"start\":49966},{\"end\":50002,\"start\":49995},{\"end\":50317,\"start\":50314},{\"end\":50331,\"start\":50327},{\"end\":50340,\"start\":50338},{\"end\":50348,\"start\":50346},{\"end\":50363,\"start\":50356},{\"end\":50383,\"start\":50373},{\"end\":50393,\"start\":50390},{\"end\":50555,\"start\":50553},{\"end\":50565,\"start\":50561},{\"end\":50580,\"start\":50576},{\"end\":50592,\"start\":50590},{\"end\":50605,\"start\":50603},{\"end\":50620,\"start\":50614},{\"end\":50635,\"start\":50632},{\"end\":50649,\"start\":50645},{\"end\":50661,\"start\":50656},{\"end\":50805,\"start\":50800},{\"end\":50818,\"start\":50814},{\"end\":50830,\"start\":50826},{\"end\":50847,\"start\":50837},{\"end\":50866,\"start\":50858},{\"end\":51020,\"start\":51015},{\"end\":51033,\"start\":51028},{\"end\":51040,\"start\":51038},{\"end\":51052,\"start\":51047},{\"end\":51159,\"start\":51143},{\"end\":51177,\"start\":51169},{\"end\":51187,\"start\":51185},{\"end\":51199,\"start\":51194},{\"end\":51214,\"start\":51207},{\"end\":51227,\"start\":51221},{\"end\":51248,\"start\":51238},{\"end\":51256,\"start\":51250}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2302.04023\",\"id\":\"b0\"},\"end\":37482,\"start\":37177},{\"attributes\":{\"doi\":\"arXiv:2212.09146\",\"id\":\"b1\"},\"end\":37678,\"start\":37484},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":37930,\"start\":37680},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3618568},\"end\":38089,\"start\":37932},{\"attributes\":{\"id\":\"b4\"},\"end\":38774,\"start\":38091},{\"attributes\":{\"id\":\"b5\"},\"end\":39023,\"start\":38776},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b6\"},\"end\":39266,\"start\":39025},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1423\",\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":39840,\"start\":39268},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":211204736},\"end\":40015,\"start\":39842},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":221516475},\"end\":40315,\"start\":40017},{\"attributes\":{\"doi\":\"arXiv:2212.10071\",\"id\":\"b10\"},\"end\":40436,\"start\":40317},{\"attributes\":{\"doi\":\"ArXiv, abs/2305.02301\",\"id\":\"b11\"},\"end\":40716,\"start\":40438},{\"attributes\":{\"id\":\"b12\"},\"end\":40937,\"start\":40718},{\"attributes\":{\"id\":\"b13\"},\"end\":41144,\"start\":40939},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":254246471},\"end\":41417,\"start\":41146},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.550\",\"id\":\"b15\",\"matched_paper_id\":215737187},\"end\":41847,\"start\":41419},{\"attributes\":{\"doi\":\"arXiv:2212.14024\",\"id\":\"b16\"},\"end\":42086,\"start\":41849},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-long.579\",\"id\":\"b17\",\"matched_paper_id\":236034557},\"end\":42436,\"start\":42088},{\"attributes\":{\"id\":\"b18\"},\"end\":42730,\"start\":42438},{\"attributes\":{\"doi\":\"arXiv:2203.05115\",\"id\":\"b19\"},\"end\":42940,\"start\":42732},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":245131171},\"end\":43329,\"start\":42942},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.703\",\"id\":\"b21\",\"matched_paper_id\":204960716},\"end\":43852,\"start\":43331},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":218869575},\"end\":44142,\"start\":43854},{\"attributes\":{\"doi\":\"arXiv:2302.11520\",\"id\":\"b23\"},\"end\":44322,\"start\":44144},{\"attributes\":{\"doi\":\"arXiv:2307.03172\",\"id\":\"b24\"},\"end\":44521,\"start\":44324},{\"attributes\":{\"doi\":\"10.18653/v1/2022.naacl-main.435\",\"id\":\"b25\",\"matched_paper_id\":244117116},\"end\":44916,\"start\":44523},{\"attributes\":{\"id\":\"b26\"},\"end\":45148,\"start\":44918},{\"attributes\":{\"doi\":\"arXiv:2203.11147\",\"id\":\"b27\"},\"end\":45414,\"start\":45150},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":247155069},\"end\":45830,\"start\":45416},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":216056269},\"end\":45961,\"start\":45832},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":246426909},\"end\":46232,\"start\":45963},{\"attributes\":{\"doi\":\"arXiv:2210.03350\",\"id\":\"b31\"},\"end\":46414,\"start\":46234},{\"attributes\":{\"doi\":\"abs/2307.16789\",\"id\":\"b32\",\"matched_paper_id\":260334759},\"end\":46641,\"start\":46416},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":204838007},\"end\":46894,\"start\":46643},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":252693405},\"end\":47191,\"start\":46896},{\"attributes\":{\"doi\":\"10.18653/v1/2021.findings-emnlp.206\",\"id\":\"b35\",\"matched_paper_id\":233289460},\"end\":47520,\"start\":47193},{\"attributes\":{\"id\":\"b36\"},\"end\":47853,\"start\":47522},{\"attributes\":{\"doi\":\"arXiv:2302.04761\",\"id\":\"b37\"},\"end\":48080,\"start\":47855},{\"attributes\":{\"doi\":\"arXiv:1506.02438\",\"id\":\"b38\"},\"end\":48269,\"start\":48082},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b39\"},\"end\":48421,\"start\":48271},{\"attributes\":{\"doi\":\"arXiv:2303.17580\",\"id\":\"b40\"},\"end\":48610,\"start\":48423},{\"attributes\":{\"doi\":\"arXiv:2301.12652\",\"id\":\"b41\"},\"end\":48811,\"start\":48612},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":247627671},\"end\":49178,\"start\":48813},{\"attributes\":{\"doi\":\"arXiv:2209.01975\",\"id\":\"b43\"},\"end\":49417,\"start\":49180},{\"attributes\":{\"doi\":\"10.48550/arXiv.2203.11171\",\"id\":\"b44\"},\"end\":49614,\"start\":49419},{\"attributes\":{\"id\":\"b45\"},\"end\":49814,\"start\":49616},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":52822214},\"end\":50245,\"start\":49816},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":252762395},\"end\":50460,\"start\":50247},{\"attributes\":{\"id\":\"b48\"},\"end\":50728,\"start\":50462},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":259298576},\"end\":50940,\"start\":50730},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":252762275},\"end\":51135,\"start\":50942},{\"attributes\":{\"id\":\"b51\"},\"end\":51344,\"start\":51137}]", "bib_title": "[{\"end\":37717,\"start\":37680},{\"end\":37980,\"start\":37932},{\"end\":39348,\"start\":39268},{\"end\":39889,\"start\":39842},{\"end\":40067,\"start\":40017},{\"end\":41242,\"start\":41146},{\"end\":41476,\"start\":41419},{\"end\":42126,\"start\":42088},{\"end\":43000,\"start\":42942},{\"end\":43444,\"start\":43331},{\"end\":43949,\"start\":43854},{\"end\":44594,\"start\":44523},{\"end\":45491,\"start\":45416},{\"end\":45882,\"start\":45832},{\"end\":46030,\"start\":45963},{\"end\":46492,\"start\":46416},{\"end\":46724,\"start\":46643},{\"end\":46959,\"start\":46896},{\"end\":47302,\"start\":47193},{\"end\":47614,\"start\":47522},{\"end\":48916,\"start\":48813},{\"end\":49889,\"start\":49816},{\"end\":50305,\"start\":50247},{\"end\":50790,\"start\":50730},{\"end\":51003,\"start\":50942}]", "bib_author": "[{\"end\":37189,\"start\":37177},{\"end\":37209,\"start\":37189},{\"end\":37221,\"start\":37209},{\"end\":37235,\"start\":37221},{\"end\":37243,\"start\":37235},{\"end\":37256,\"start\":37243},{\"end\":37270,\"start\":37256},{\"end\":37280,\"start\":37270},{\"end\":37293,\"start\":37280},{\"end\":37306,\"start\":37293},{\"end\":37318,\"start\":37306},{\"end\":37326,\"start\":37318},{\"end\":37340,\"start\":37326},{\"end\":37616,\"start\":37593},{\"end\":37632,\"start\":37616},{\"end\":37644,\"start\":37632},{\"end\":37730,\"start\":37719},{\"end\":37745,\"start\":37730},{\"end\":37757,\"start\":37745},{\"end\":37774,\"start\":37757},{\"end\":37790,\"start\":37774},{\"end\":37809,\"start\":37790},{\"end\":37829,\"start\":37809},{\"end\":37843,\"start\":37829},{\"end\":37858,\"start\":37843},{\"end\":37873,\"start\":37858},{\"end\":37994,\"start\":37982},{\"end\":38006,\"start\":37994},{\"end\":38020,\"start\":38006},{\"end\":38036,\"start\":38020},{\"end\":38104,\"start\":38093},{\"end\":38118,\"start\":38104},{\"end\":38130,\"start\":38118},{\"end\":38143,\"start\":38130},{\"end\":38177,\"start\":38143},{\"end\":38191,\"start\":38177},{\"end\":38209,\"start\":38191},{\"end\":38221,\"start\":38209},{\"end\":38238,\"start\":38221},{\"end\":38253,\"start\":38238},{\"end\":38263,\"start\":38253},{\"end\":38274,\"start\":38263},{\"end\":38292,\"start\":38274},{\"end\":38308,\"start\":38292},{\"end\":38322,\"start\":38308},{\"end\":38337,\"start\":38322},{\"end\":38353,\"start\":38337},{\"end\":38366,\"start\":38353},{\"end\":38378,\"start\":38366},{\"end\":38390,\"start\":38378},{\"end\":38406,\"start\":38390},{\"end\":38421,\"start\":38406},{\"end\":38436,\"start\":38421},{\"end\":38455,\"start\":38436},{\"end\":38471,\"start\":38455},{\"end\":38506,\"start\":38471},{\"end\":38521,\"start\":38506},{\"end\":38536,\"start\":38521},{\"end\":38554,\"start\":38536},{\"end\":38573,\"start\":38554},{\"end\":38792,\"start\":38776},{\"end\":38804,\"start\":38792},{\"end\":38812,\"start\":38804},{\"end\":38824,\"start\":38812},{\"end\":38837,\"start\":38824},{\"end\":38848,\"start\":38837},{\"end\":38863,\"start\":38848},{\"end\":38878,\"start\":38863},{\"end\":38894,\"start\":38878},{\"end\":38913,\"start\":38894},{\"end\":39046,\"start\":39025},{\"end\":39061,\"start\":39046},{\"end\":39075,\"start\":39061},{\"end\":39090,\"start\":39075},{\"end\":39105,\"start\":39090},{\"end\":39119,\"start\":39105},{\"end\":39132,\"start\":39119},{\"end\":39149,\"start\":39132},{\"end\":39165,\"start\":39149},{\"end\":39185,\"start\":39165},{\"end\":39364,\"start\":39350},{\"end\":39380,\"start\":39364},{\"end\":39392,\"start\":39380},{\"end\":39412,\"start\":39392},{\"end\":39903,\"start\":39891},{\"end\":39915,\"start\":39903},{\"end\":39926,\"start\":39915},{\"end\":40084,\"start\":40069},{\"end\":40098,\"start\":40084},{\"end\":40113,\"start\":40098},{\"end\":40123,\"start\":40113},{\"end\":40139,\"start\":40123},{\"end\":40150,\"start\":40139},{\"end\":40168,\"start\":40150},{\"end\":40328,\"start\":40317},{\"end\":40342,\"start\":40328},{\"end\":40356,\"start\":40342},{\"end\":40565,\"start\":40549},{\"end\":40580,\"start\":40565},{\"end\":40595,\"start\":40580},{\"end\":40611,\"start\":40595},{\"end\":40627,\"start\":40611},{\"end\":40647,\"start\":40627},{\"end\":40663,\"start\":40647},{\"end\":40676,\"start\":40663},{\"end\":40691,\"start\":40676},{\"end\":40795,\"start\":40778},{\"end\":40810,\"start\":40795},{\"end\":40824,\"start\":40810},{\"end\":40840,\"start\":40824},{\"end\":40855,\"start\":40840},{\"end\":40868,\"start\":40855},{\"end\":40885,\"start\":40868},{\"end\":40900,\"start\":40885},{\"end\":40918,\"start\":40900},{\"end\":40933,\"start\":40918},{\"end\":41044,\"start\":41033},{\"end\":41059,\"start\":41044},{\"end\":41072,\"start\":41059},{\"end\":41084,\"start\":41072},{\"end\":41098,\"start\":41084},{\"end\":41112,\"start\":41098},{\"end\":41127,\"start\":41112},{\"end\":41140,\"start\":41127},{\"end\":41260,\"start\":41244},{\"end\":41270,\"start\":41260},{\"end\":41281,\"start\":41270},{\"end\":41293,\"start\":41281},{\"end\":41306,\"start\":41293},{\"end\":41320,\"start\":41306},{\"end\":41335,\"start\":41320},{\"end\":41498,\"start\":41478},{\"end\":41511,\"start\":41498},{\"end\":41522,\"start\":41511},{\"end\":41537,\"start\":41522},{\"end\":41548,\"start\":41537},{\"end\":41563,\"start\":41548},{\"end\":41575,\"start\":41563},{\"end\":41588,\"start\":41575},{\"end\":41863,\"start\":41849},{\"end\":41881,\"start\":41863},{\"end\":41893,\"start\":41881},{\"end\":41903,\"start\":41893},{\"end\":41915,\"start\":41903},{\"end\":41934,\"start\":41915},{\"end\":41947,\"start\":41934},{\"end\":41956,\"start\":41947},{\"end\":42145,\"start\":42128},{\"end\":42159,\"start\":42145},{\"end\":42173,\"start\":42159},{\"end\":42519,\"start\":42502},{\"end\":42540,\"start\":42519},{\"end\":42557,\"start\":42540},{\"end\":42574,\"start\":42557},{\"end\":42588,\"start\":42574},{\"end\":42603,\"start\":42588},{\"end\":42621,\"start\":42603},{\"end\":42639,\"start\":42621},{\"end\":42653,\"start\":42639},{\"end\":42665,\"start\":42653},{\"end\":42849,\"start\":42829},{\"end\":42868,\"start\":42849},{\"end\":42888,\"start\":42868},{\"end\":42906,\"start\":42888},{\"end\":43014,\"start\":43002},{\"end\":43027,\"start\":43014},{\"end\":43040,\"start\":43027},{\"end\":43058,\"start\":43040},{\"end\":43079,\"start\":43058},{\"end\":43094,\"start\":43079},{\"end\":43458,\"start\":43446},{\"end\":43470,\"start\":43458},{\"end\":43483,\"start\":43470},{\"end\":43505,\"start\":43483},{\"end\":43526,\"start\":43505},{\"end\":43537,\"start\":43526},{\"end\":43555,\"start\":43537},{\"end\":43573,\"start\":43555},{\"end\":43966,\"start\":43951},{\"end\":43979,\"start\":43966},{\"end\":43998,\"start\":43979},{\"end\":44013,\"start\":43998},{\"end\":44033,\"start\":44013},{\"end\":44046,\"start\":44033},{\"end\":44064,\"start\":44046},{\"end\":44076,\"start\":44064},{\"end\":44089,\"start\":44076},{\"end\":44154,\"start\":44144},{\"end\":44167,\"start\":44154},{\"end\":44181,\"start\":44167},{\"end\":44196,\"start\":44181},{\"end\":44210,\"start\":44196},{\"end\":44222,\"start\":44210},{\"end\":44344,\"start\":44324},{\"end\":44354,\"start\":44344},{\"end\":44369,\"start\":44354},{\"end\":44388,\"start\":44369},{\"end\":44406,\"start\":44388},{\"end\":44421,\"start\":44406},{\"end\":44428,\"start\":44421},{\"end\":44608,\"start\":44596},{\"end\":44625,\"start\":44608},{\"end\":44644,\"start\":44625},{\"end\":44662,\"start\":44644},{\"end\":44676,\"start\":44662},{\"end\":45052,\"start\":45039},{\"end\":45064,\"start\":45052},{\"end\":45078,\"start\":45064},{\"end\":45092,\"start\":45078},{\"end\":45113,\"start\":45092},{\"end\":45130,\"start\":45113},{\"end\":45164,\"start\":45150},{\"end\":45178,\"start\":45164},{\"end\":45196,\"start\":45178},{\"end\":45212,\"start\":45196},{\"end\":45226,\"start\":45212},{\"end\":45243,\"start\":45226},{\"end\":45255,\"start\":45243},{\"end\":45271,\"start\":45255},{\"end\":45297,\"start\":45271},{\"end\":45314,\"start\":45297},{\"end\":45504,\"start\":45493},{\"end\":45515,\"start\":45504},{\"end\":45529,\"start\":45515},{\"end\":45544,\"start\":45529},{\"end\":45556,\"start\":45544},{\"end\":45577,\"start\":45556},{\"end\":45595,\"start\":45577},{\"end\":45895,\"start\":45884},{\"end\":45911,\"start\":45895},{\"end\":45932,\"start\":45911},{\"end\":45950,\"start\":45932},{\"end\":46045,\"start\":46032},{\"end\":46057,\"start\":46045},{\"end\":46067,\"start\":46057},{\"end\":46082,\"start\":46067},{\"end\":46102,\"start\":46082},{\"end\":46118,\"start\":46102},{\"end\":46131,\"start\":46118},{\"end\":46149,\"start\":46131},{\"end\":46165,\"start\":46149},{\"end\":46175,\"start\":46165},{\"end\":46246,\"start\":46234},{\"end\":46258,\"start\":46246},{\"end\":46269,\"start\":46258},{\"end\":46285,\"start\":46269},{\"end\":46299,\"start\":46285},{\"end\":46311,\"start\":46299},{\"end\":46505,\"start\":46494},{\"end\":46519,\"start\":46505},{\"end\":46530,\"start\":46519},{\"end\":46542,\"start\":46530},{\"end\":46551,\"start\":46542},{\"end\":46560,\"start\":46551},{\"end\":46572,\"start\":46560},{\"end\":46582,\"start\":46572},{\"end\":46596,\"start\":46582},{\"end\":46607,\"start\":46596},{\"end\":46740,\"start\":46726},{\"end\":46754,\"start\":46740},{\"end\":46768,\"start\":46754},{\"end\":46783,\"start\":46768},{\"end\":46798,\"start\":46783},{\"end\":46814,\"start\":46798},{\"end\":46826,\"start\":46814},{\"end\":46834,\"start\":46826},{\"end\":46847,\"start\":46834},{\"end\":46982,\"start\":46961},{\"end\":47006,\"start\":46982},{\"end\":47023,\"start\":47006},{\"end\":47036,\"start\":47023},{\"end\":47048,\"start\":47036},{\"end\":47069,\"start\":47048},{\"end\":47090,\"start\":47069},{\"end\":47102,\"start\":47090},{\"end\":47318,\"start\":47304},{\"end\":47339,\"start\":47318},{\"end\":47639,\"start\":47616},{\"end\":47651,\"start\":47639},{\"end\":47671,\"start\":47651},{\"end\":47683,\"start\":47671},{\"end\":47698,\"start\":47683},{\"end\":47931,\"start\":47918},{\"end\":47948,\"start\":47931},{\"end\":47963,\"start\":47948},{\"end\":47981,\"start\":47963},{\"end\":47995,\"start\":47981},{\"end\":48013,\"start\":47995},{\"end\":48030,\"start\":48013},{\"end\":48046,\"start\":48030},{\"end\":48097,\"start\":48082},{\"end\":48113,\"start\":48097},{\"end\":48128,\"start\":48113},{\"end\":48144,\"start\":48128},{\"end\":48159,\"start\":48144},{\"end\":48286,\"start\":48271},{\"end\":48300,\"start\":48286},{\"end\":48319,\"start\":48300},{\"end\":48333,\"start\":48319},{\"end\":48346,\"start\":48333},{\"end\":48513,\"start\":48497},{\"end\":48526,\"start\":48513},{\"end\":48534,\"start\":48526},{\"end\":48548,\"start\":48534},{\"end\":48560,\"start\":48548},{\"end\":48576,\"start\":48560},{\"end\":48624,\"start\":48612},{\"end\":48635,\"start\":48624},{\"end\":48655,\"start\":48635},{\"end\":48668,\"start\":48655},{\"end\":48680,\"start\":48668},{\"end\":48692,\"start\":48680},{\"end\":48710,\"start\":48692},{\"end\":48723,\"start\":48710},{\"end\":48932,\"start\":48918},{\"end\":48949,\"start\":48932},{\"end\":48966,\"start\":48949},{\"end\":48982,\"start\":48966},{\"end\":48996,\"start\":48982},{\"end\":49010,\"start\":48996},{\"end\":49192,\"start\":49180},{\"end\":49205,\"start\":49192},{\"end\":49220,\"start\":49205},{\"end\":49232,\"start\":49220},{\"end\":49245,\"start\":49232},{\"end\":49256,\"start\":49245},{\"end\":49267,\"start\":49256},{\"end\":49283,\"start\":49267},{\"end\":49301,\"start\":49283},{\"end\":49315,\"start\":49301},{\"end\":49504,\"start\":49491},{\"end\":49515,\"start\":49504},{\"end\":49532,\"start\":49515},{\"end\":49543,\"start\":49532},{\"end\":49553,\"start\":49543},{\"end\":49565,\"start\":49553},{\"end\":49698,\"start\":49687},{\"end\":49711,\"start\":49698},{\"end\":49728,\"start\":49711},{\"end\":49743,\"start\":49728},{\"end\":49757,\"start\":49743},{\"end\":49766,\"start\":49757},{\"end\":49776,\"start\":49766},{\"end\":49784,\"start\":49776},{\"end\":49794,\"start\":49784},{\"end\":49800,\"start\":49794},{\"end\":49904,\"start\":49891},{\"end\":49913,\"start\":49904},{\"end\":49929,\"start\":49913},{\"end\":49944,\"start\":49929},{\"end\":49959,\"start\":49944},{\"end\":49981,\"start\":49959},{\"end\":50004,\"start\":49981},{\"end\":50319,\"start\":50307},{\"end\":50333,\"start\":50319},{\"end\":50342,\"start\":50333},{\"end\":50350,\"start\":50342},{\"end\":50365,\"start\":50350},{\"end\":50385,\"start\":50365},{\"end\":50395,\"start\":50385},{\"end\":50557,\"start\":50546},{\"end\":50567,\"start\":50557},{\"end\":50582,\"start\":50567},{\"end\":50594,\"start\":50582},{\"end\":50607,\"start\":50594},{\"end\":50622,\"start\":50607},{\"end\":50637,\"start\":50622},{\"end\":50651,\"start\":50637},{\"end\":50663,\"start\":50651},{\"end\":50807,\"start\":50792},{\"end\":50820,\"start\":50807},{\"end\":50832,\"start\":50820},{\"end\":50849,\"start\":50832},{\"end\":50868,\"start\":50849},{\"end\":51022,\"start\":51005},{\"end\":51035,\"start\":51022},{\"end\":51042,\"start\":51035},{\"end\":51054,\"start\":51042},{\"end\":51161,\"start\":51137},{\"end\":51179,\"start\":51161},{\"end\":51189,\"start\":51179},{\"end\":51201,\"start\":51189},{\"end\":51216,\"start\":51201},{\"end\":51229,\"start\":51216},{\"end\":51250,\"start\":51229},{\"end\":51258,\"start\":51250}]", "bib_venue": "[{\"end\":39778,\"start\":39615},{\"end\":40311,\"start\":40248},{\"end\":41413,\"start\":41399},{\"end\":41794,\"start\":41715},{\"end\":42378,\"start\":42291},{\"end\":43284,\"start\":43182},{\"end\":43789,\"start\":43701},{\"end\":44844,\"start\":44773},{\"end\":45785,\"start\":45683},{\"end\":47475,\"start\":47445},{\"end\":47765,\"start\":47758},{\"end\":49112,\"start\":49081},{\"end\":50200,\"start\":50112},{\"end\":37462,\"start\":37356},{\"end\":37591,\"start\":37484},{\"end\":37922,\"start\":37873},{\"end\":38083,\"start\":38036},{\"end\":38593,\"start\":38573},{\"end\":39021,\"start\":38913},{\"end\":39246,\"start\":39201},{\"end\":39590,\"start\":39432},{\"end\":39613,\"start\":39592},{\"end\":39970,\"start\":39926},{\"end\":40246,\"start\":40168},{\"end\":40416,\"start\":40372},{\"end\":40547,\"start\":40438},{\"end\":40776,\"start\":40718},{\"end\":41031,\"start\":40939},{\"end\":41397,\"start\":41335},{\"end\":41713,\"start\":41619},{\"end\":42066,\"start\":41972},{\"end\":42289,\"start\":42202},{\"end\":42500,\"start\":42438},{\"end\":42827,\"start\":42732},{\"end\":43180,\"start\":43094},{\"end\":43699,\"start\":43602},{\"end\":44138,\"start\":44089},{\"end\":44302,\"start\":44238},{\"end\":44501,\"start\":44444},{\"end\":44771,\"start\":44707},{\"end\":45037,\"start\":44918},{\"end\":45394,\"start\":45330},{\"end\":45681,\"start\":45595},{\"end\":45955,\"start\":45950},{\"end\":46224,\"start\":46175},{\"end\":46394,\"start\":46327},{\"end\":46635,\"start\":46621},{\"end\":46883,\"start\":46847},{\"end\":47185,\"start\":47102},{\"end\":47443,\"start\":47374},{\"end\":47756,\"start\":47698},{\"end\":47916,\"start\":47855},{\"end\":48249,\"start\":48175},{\"end\":48401,\"start\":48362},{\"end\":48495,\"start\":48423},{\"end\":48791,\"start\":48739},{\"end\":49079,\"start\":49010},{\"end\":49397,\"start\":49331},{\"end\":49489,\"start\":49419},{\"end\":49685,\"start\":49616},{\"end\":50110,\"start\":50024},{\"end\":50454,\"start\":50395},{\"end\":50544,\"start\":50462},{\"end\":50933,\"start\":50868},{\"end\":51119,\"start\":51054},{\"end\":51324,\"start\":51274}]"}}}, "year": 2023, "month": 12, "day": 17}