{"id": 46950555, "updated": "2023-09-30 22:47:44.161", "metadata": {"title": "Toward Diverse Text Generation with Inverse Reinforcement Learning", "authors": "[{\"first\":\"Zhan\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Xinchi\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xipeng\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Xuanjing\",\"last\":\"Huang\",\"middle\":[]}]", "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2018, "month": 4, "day": 30}, "abstract": "Text generation is a crucial task in NLP. Recently, several adversarial generative models have been proposed to improve the exposure bias problem in text generation. Though these models gain great success, they still suffer from the problems of reward sparsity and mode collapse. In order to address these two problems, in this paper, we employ inverse reinforcement learning (IRL) for text generation. Specifically, the IRL framework learns a reward function on training data, and then an optimal policy to maximum the expected total reward. Similar to the adversarial models, the reward and policy function in IRL are optimized alternately. Our method has two advantages: (1) the reward function can produce more dense reward signals. (2) the generation policy, trained by\"entropy regularized\"policy gradient, encourages to generate more diversified texts. Experiment results demonstrate that our proposed method can generate higher quality texts than the previous methods.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1804.11258", "mag": "2963730239", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/ShiCQH18", "doi": "10.24963/ijcai.2018/606"}}, "content": {"source": {"pdf_hash": "d350e3753756b1c6946d5d9150626b2de4f7a8e4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1804.11258v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2018/0606.pdf", "status": "BRONZE"}}, "grobid": {"id": "d44d941a16f6ed9d8bf553c993d6f1fabf58d8b8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d350e3753756b1c6946d5d9150626b2de4f7a8e4.txt", "contents": "\nToward Diverse Text Generation with Inverse Reinforcement Learning\n\n\nZhan Shi \nSchool of Computer Science\nShanghai Key Laboratory of Intelligent Information Processing\nFudan University\nFudan University\n\n\nXinchi Chen \nSchool of Computer Science\nShanghai Key Laboratory of Intelligent Information Processing\nFudan University\nFudan University\n\n\nXipeng Qiu \nSchool of Computer Science\nShanghai Key Laboratory of Intelligent Information Processing\nFudan University\nFudan University\n\n\nXuanjing Huang \nSchool of Computer Science\nShanghai Key Laboratory of Intelligent Information Processing\nFudan University\nFudan University\n\n\nToward Diverse Text Generation with Inverse Reinforcement Learning\n\nText generation is a crucial task in NLP. Recently, several adversarial generative models have been proposed to improve the exposure bias problem in text generation. Though these models gain great success, they still suffer from the problems of reward sparsity and mode collapse. In order to address these two problems, in this paper, we employ inverse reinforcement learning (IRL) for text generation. Specifically, the IRL framework learns a reward function on training data, and then an optimal policy to maximum the expected total reward. Similar to the adversarial models, the reward and policy function in IRL are optimized alternately. Our method has two advantages: (1) the reward function can produce more dense reward signals. (2) the generation policy, trained by \"entropy regularized\" policy gradient, encourages to generate more diversified texts. Experiment results demonstrate that our proposed method can generate higher quality texts than the previous methods.\n\nIntroduction\n\nText generation is one of the most attractive problems in NLP community. It has been widely used in machine translation, image captioning, text summarization and dialogue systems.\n\nCurrently, most of the existing methods [Graves, 2013] adopt auto-regressive models to predict the next words based on the historical predictions. Benefiting from the strong ability of deep neural models, such as long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997], these autoregressive models can achieve excellent performance. However, they suffer from the so-called exposure bias issue  due to the discrepancy distribution of histories between the training and inference stage. In training stage, the model predicts the next word according to groundtruth histories from the data distribution rather than its own historical predictions from the model distribution.\n\nRecently, some methods have been proposed to alleviate this problem, such as scheduled sampling [Bengio et al., * Corresponding Author, xpqiu@fudan.edu.cn 2015], Gibbs sampling [Su et al., 2018] and adversarial models, including SeqGAN [Yu et al., 2017], RankGAN [Lin et al., 2017], MaliGAN [Che et al., 2017] and LeakGAN [Guo et al., 2017]. Following the framework of generative adversarial networks (GAN) [Goodfellow et al., 2014], the adversarial text generation models use a discriminator to judge whether a given text is real or not. Then a generator is learned to maximize the reward signal provided by the discriminator via reinforcement learning (RL). Since the generator always generates a entire text sequence, these adversarial models can avoid the problem of exposure bias.\n\nInspired of their success, there are still two challenges in the adversarial model.\n\nThe first problem is reward sparsity. The adversarial model depends on the ability of the discriminator, therefore we wish the discriminator always correctly discriminates the real texts from the \"generated\" ones. Instead, a perfect discriminator increases the training difficulty due to the sparsity of the reward signals. There are two kinds of work to address this issue. The first one is to improve the signal from the discriminator. RankGAN [Lin et al., 2017] uses a ranker to take place of the discriminator, which can learn the relative ranking information between the generated and the real texts in the adversarial framework. MaliGAN [Che et al., 2017] develops normalized maximum likelihood optimization target to alleviate the reward instability problem. The second one is to decompose the discrete reward signal into various sub-signals. LeakGAN [Guo et al., 2017] takes a hierarchical generator, and in each step, generates a word using leaked information from the discriminator.\n\nThe second problem is the mode collapse. The adversarial model tends to learn limited patterns because of mode collapse. One kind of methods, such as TextGAN , uses feature matching [Salimans et al., 2016;Metz et al., 2016] to alleviate this problem, it is still hard to train due to the intrinsic nature of GAN. Another kind of methods [Bayer and Osendorfer, 2014;Chung et al., 2015;Serban et al., 2017; introduces latent random variables to model the variability of the generated sequences.\n\nTo tackle these two challenges, we propose a new method to generate diverse text via inverse reinforcement learning (IRL) [Ziebart et al., 2008]. Typically, the text generation can be regarded as an IRL problem. Each text in the training  data is generated by some experts with an unknown reward function. There are two alternately steps in IRL framework. Firstly, a reward function is learned to explain the expert behavior. Secondly, a generation policy is learned to maximize the expected total rewards. The reward function aims to increase the rewards of the real texts in training set and decrease the rewards of the generated texts. Intuitively, the reward function plays the similar role as the discriminator in SeqGAN. Unlike SeqGAN, the reward function is an instant reward of each step and action, thereby providing more dense reward signals. The generation policy generates text sequence by sampling one word at a time. The optimized policy be learned by \"entropy regularized\" policy gradient [Finn et al., 2016], which intrinsically leads to a more diversified text generator.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 We regard text generation as an IRL problem, which is a new perspective on this task. \u2022 Following the maximum entropy IRL [Ziebart et al., 2008], our method can improve the problems of reward sparsity and mode collapse. \u2022 To better evaluate the quality of the generated texts, we propose three new metrics based on BLEU score, which is very similar to precision, recall and F 1 in traditional machine learning task.\n\n\nText Generation via Inverse Reinforcement Learning\n\nText generation is to generate a text sequence x 1:T = x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x T with a parameterized auto-regressive probabilistic model q \u03b8 (x), where x t is a word in a given vocabulary V. The generation model q \u03b8 (x) is learned from a given dataset {x (n) } N n=1 with an underlying generating distribution p data .\n\nIn this paper, we formulate text generation as inverse reinforcement learning (IRL) problem. Firstly, the process of text generation can be regarded as Markov decision process (MDP). In each timestep t, the model generates x t according a policy \u03c0 \u03b8 (a t |s t ), where s t is the current state of the previous prediction x 1:t and a t is the action to select the next word x t+1 . A text sequence x 1:T = x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x T can be formulated by a trajectory of MDP \u03c4 = {s 1 , a 1 , s 2 , a 2 ..., s T , a T }. Therefore, the probability of x 1:T is\nq \u03b8 (x 1:T ) = q \u03b8 (\u03c4 ) = T \u22121 t=1 \u03c0 \u03b8 (a t = x t+1 |s t = x 1:t ),(1)\nwhere the state transition p(s t+1 = x 1:t+1 |s t = x 1:t , a t = x t+1 ) = 1 is deterministic and can be ignored.\n\nSecondly, the reward function is not explicitly given for text generation. Each text sequence x 1:T = x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x T in the training dataset is formulated by a trajectory \u03c4 by experts from the distribution p(\u03c4 ), and we have to learn a reward function that explains the expert behavior.\n\nConcretely, IRL consists of two phases: (1) estimate the underlying reward function of experts from the training dataset; (2) learn an optimal policy to generate texts, which aims to maximize the expected rewards. These two phases are executed alternately. The framework of our method is as shown in Figure 1.\n\n\nReward Approximator\n\nFollowing the framework of maximum entropy IRL [Ziebart et al., 2008], we assume that the texts in training set are sampled from the distribution p \u03c6 (\u03c4 ),\np \u03c6 (\u03c4 ) = 1 Z exp(R \u03c6 (\u03c4 )),(2)\nwhere R \u03c6 (\u03c4 ) an unknown reward function parameterized by \u03c6, Z = \u03c4 exp(R \u03c6 (\u03c4 ))d\u03c4 is the partition function. The reward of trajectory R \u03c6 (\u03c4 ) is a parameterized reward function and assumed to be summation of the rewards of each steps r \u03c6 (s t , a t ):\nR \u03c6 (\u03c4 ) = t r \u03c6 (s t , a t ),(3)\nwhere r \u03c6 (s t , a t ) is modeled a simple feed-forward neural network as shown in Figure 2.\n\n\nObjective of Reward Approximator\n\nThe objective of the reward approximator is to maximize the log-likehood of the samples in the training set:\nJ r (\u03c6) = 1 N N n=1 log p \u03c6 (\u03c4 n ) = 1 N N n=1 R \u03c6 (\u03c4 n ) \u2212 log Z,(4)\nwhere \u03c4 n denotes the n th sample in the training set D train .\n\nThus, the derivative of J r (\u03c6) is:\n\u2207 \u03c6 J r (\u03c6) = 1 N n \u2207 \u03c6 R \u03c6 (\u03c4 n )\u2212 1 Z \u03c4 exp(R \u03c6 (\u03c4 ))\u2207 \u03c6 R \u03c6 (\u03c4 )d\u03c4 = E \u03c4 \u223cp data \u2207 \u03c6 R \u03c6 (\u03c4 ) \u2212 E \u03c4 \u223cp \u03c6 (\u03c4 ) \u2207 \u03c6 R \u03c6 (\u03c4 ). (5)\nIntuitively, the reward approximator aims to increase the rewards of the real texts and decrease the trajectories drawn from the distribution p \u03c6 (\u03c4 ). As a result, p \u03c6 (\u03c4 ) will be an approximation of p data .\n\nImportance Sampling Though it is quite straightforward to sample \u03c4 \u223c p \u03c6 (\u03c4 ) in Eq. (5), it is actually inefficient in practice. Instead, we directly use trajectories sampled by text generator q \u03b8 (\u03c4 ) with importance sampling. Concretely, Eq.\n\n(5) is now formalized as:\n\u2207 \u03c6 Jr(\u03c6) \u2248 1 N N i=1 \u2207 \u03c6 R \u03c6 (\u03c4i)\u2212 1 j wj M j=1 wj\u2207 \u03c6 R \u03c6 (\u03c4 j ), (6) where w j \u221d exp(R \u03c6 (\u03c4j )) q \u03b8 (\u03c4j )\n. For each batch, we sample N texts from the train set and M texts drawn from q \u03b8 .\n\n\nText Generator\n\nThe text generator uses a policy \u03c0 \u03b8 (a|s) to predict the next word one by one. The current state s t can be modeled by LSTM neural network as shown in Figure 2. For \u03c4 = {s 1 , a 1 , s 2 , a 2 ..., s T , a T },\ns t = LSTM(s t\u22121 , e at\u22121 ), (7) \u03c0 \u03b8 (a t |s t ) = softmax(Ws t + b),(8)\nwhere s t is the vector representation of state s t ; a t is distribution over the vocabulary; e at\u22121 is the word embedding of a t\u22121 ; \u03b8 denotes learnable parameters including W, b and all the parameters of LSTM.\n\n\nObjective of Text Generator\n\nFollowing \"entropy regularized\" policy gradient [Williams, 1992;Nachum et al., 2017], the objective of text generator is to maximize the expected reward plus an entropy regularization.\nJ g (\u03b8) = E \u03c4 \u223cq \u03b8 (\u03c4 ) [R \u03c6 (\u03c4 )] + H(q \u03b8 (\u03c4 ))(9)\nwhere\nH(q \u03b8 (\u03c4 )) = \u2212E q \u03b8 (\u03c4 ) [log q \u03b8 (\u03c4 )]\nis an entropy term, which can prevent premature entropy collapse and encourage the policy to generate more diverse texts. Intuitively, the \"entropy regularized\" expected reward can be rewrite as\nJ g (\u03b8) = \u2212 KL(q \u03b8 (\u03c4 )||p \u03c6 (\u03c4 )) + log Z,(10)\nwhere Z = \u03c4 exp(R \u03c6 (\u03c4 ))d\u03c4 is the partition function and can be regarded as a constant unrelated to \u03b8. Therefore, the objective is also to minimize the KL divergence between the text generator q \u03b8 (\u03c4 ) and the underlying distribution p \u03c6 (\u03c4 ).\n\nThus, the derivative of J g (\u03b8) is \n\u2207 \u03b8 J g (\u03b8) = t E \u03c0 \u03b8 (at|st) \u2207 \u03b8 log \u03c0 \u03b8 (a t|s[R \u03c6 (\u03c4 t:T ) \u2212 log \u03c0 \u03b8 (a t |s t ) \u2212 1] .(11)\nwhere R \u03c6 (\u03c4 t:T ) denotes the reward of partial trajectory \u03c4 t , \u00b7 \u00b7 \u00b7 , \u03c4 T . For obtaining lower variance, R(\u03c4 t:T ) can be approximately computed by\nR \u03c6 (\u03c4 t:T ) \u2248 r \u03c6 (s t , a t ) + V (s t+1 ),(12)\nwhere V (s t+1 ) denotes the expected total reward at state s t+1 and can be approximately computed by MCMC. Figure 3 gives an illustration.\n\n\nWhy Can IRL Alleviate Mode Collapse?\n\nGANs often suffer from mode collapse, which is partially caused by the use of Jensen-Shannon (JS) divergence. There is a reverse KL divergence KL(q \u03b8 (\u03c4 ) p data ) in JS divergence. Since the p data is approximated by training data, the reverse KL divergence encourages q \u03b8 (\u03c4 ) to generate safe samples and avoid generating samples where the training data does not occur. In our method, the objective is KL(q \u03b8 (\u03c4 )||p \u03c6 (\u03c4 )). Different from GANs, we use p \u03c6 (\u03c4 ) in IRL framework instead of p data . Since p \u03c6 (\u03c4 ) never equals to zero due to its assumption, IRL can alleviate the model collapse problem in GANs.\n\n\nTraining\n\nThe training procedure consists of two steps: (I) reward approximator update step (r-step) and (II) text generator update step (g-step). These two steps are applied iteratively as described in Algorithm (1). Initially, we have r \u03c6 with random parameters and \u03c0 \u03b8 with pre-trained parameters by maximum log-likelihood estimation on D train . The r-step aims to update r \u03c6 with \u03c0 \u03b8 fixed. The g-step aims to update \u03c0 \u03b8 with r \u03c6 fixed.\n\n\nExperiment\n\nTo evaluate the proposed model, we experiment on three corpora: the synthetic oracle dataset , the COCO image caption dataset  and the IMDB movie review dataset [Diao et al., 2014]. Furthermore, we also evaluate the performance by human on the image caption dataset and the IMDB corpus. Experimental results show that Our method outperforms the previous methods. Table 1 gives the experimental settings on the three corpora.\n\n\nSynthetic Oracle\n\nThe synthetic oracle dataset is a set of sequential tokens which are regraded as simulated data comparing to the real-Algorithm 1 IRL for Text Generation 1: repeat 2:\n\nPretrain \u03c0 \u03b8 on D train with MLE 3:\n\nfor n r epochs in r-step do 4:\n\nDrawn \u03c4 (1) , \u03c4 (2) , \u00b7 \u00b7 \u00b7 , \u03c4 (i) , \u00b7 \u00b7 \u00b7 , \u03c4 (N ) \u223c p data 5:\n\nDrawn \u03c4 (1) , \u03c4 (2) , \u00b7 \u00b7 \u00b7 , \u03c4 (j) , \u00b7 \u00b7 \u00b7 , \u03c4 (M ) \u223c q \u03b8 6:\nUpdate \u03c6 \u2190 \u03c6 + \u03b1\u2207 \u03c6 J r (\u03c6) 7:\nend for 8:\n\nfor n g batches in g-step do 9:\n\nDrawn \u03c4 (1) , \u03c4 (2) , \u00b7 \u00b7 \u00b7 , \u03c4 (i) , \u00b7 \u00b7 \u00b7 , \u03c4 (N ) \u223c q \u03b8 10:\n\nCalculate expected reward R \u03c6 (\u03c4 t:T ) by MCMC 11:\n\nUpdate \u03b8 \u2190 \u03b8 + \u03b2\u2207 \u03b8 J g (\u03b8) world language data. It uses a randomly initialized LSTM 1 as the oracle model to generate 10000 samples of length 20 and 40 respectively as the training set for the following experiments. The oracle model, which has an intrinsic data distribution P oracle , can be used to evaluate the sentences generated by the generative models. The average negative loglikelihood(NLL) is usually conducted to score the quality of the generated sequences Guo et al., 2017;. The lower the NLL score is, the better token sequences we have generated.\n\nTraining Strategy In experiments, we find that the stability and performance of our framework depend on the training strategy. Figure 4 shows the effects of pretraining epochs. It works best in generating texts of length 20 with 50 epochs of MLE pretraining, and in generating texts of length 40 with 10 epochs of pretraining. Figure 5 shows that the proportion of n r : n g in Algorithm 1 affects the convergence and final performance. It implies that sufficient training on the approximator in each iteration will lead to better results and convergence. Therefore, we take n r : n g = 10 : 1 as our final training configuration.   Results Table 2 gives the results. We compare our method with other previous state-of-the-art methods: maximum likelihood estimation (MLE), SeqGAN, RankGAN and Leak-GAN. The listed ground truth values are the average NLL of the training set. Our method outperforms the previous stateof-the-art results (6.913 and 7.083 on length of 20 and 40 respectively). Figure 6 shows that Our method convergences faster and obtains better performance than other state-of-art methods.\n\nAnalysis Our method performs better due to the instant rewards approximated at each step of generation. It addresses the reward sparsity issue occurred in previous methods. Thus, the dense learning signals guide the generative policy to capture the underlying distribution of the training data more efficiently.\n\n\nCOCO Image Captions\n\nThe image caption dataset  consists of image-description pairs. The length of captions is between 8 and 20. Following LeakGAN [Guo et al., 2017], for preprocessing, we remove low frequency words (less than 10 times) as well as the sentences containing them. We randomly choose 80,000 texts as training set, and another 5,000 as test set. The vocabulary size of the dataset is 4,939. The average sentence length is 12.8.\n\n\nNew Evaluation Measures on BLEU\n\nTo evaluate different methods, we employ BLEU score to evaluate the qualities of the generated texts.\n\n\u2022 Forward BLEU (BLEU F ) uses the testset as reference, and evaluates each generated text with BLEU score. \u2022 Backward BLEU (BLEU B ) uses the generated texts as reference, and evaluates each text in testset with BLEU score. \u2022 BLEU HA is the harmonic average value of BLEU F and BLEU B . Intuitively, BLEU F aims to measure the precision (quality) of the generator, while BLEU B aims to measure the recall (diversity) of the generator.\n\nThe configurations of three proposed valuation measures are shown in Table 3  BLEU F For BLEU F , we sample 1000 texts for each method as evaluated texts. The reference texts are the whole test set. We list the BLEU F scores of different frameworks and ground truth as shown in first subtable of Table 4. Surprisingly, it shows that results of LeakGAN beat the rest, even the ground truth (LeakGAN has averagely 10 points higher than the ground truth). It may due to the mode collapse which frequently occurs in GAN. The text generator is prone to generate safe text patterns but misses many other patterns. Therefore, BLEU F is failing to measure the diversity of the generated sentences.   BLEU B For BLEU B , we sample 5000 texts for each method as reference texts. The evaluated texts consist 1000 texts sampled from the test set. The BLEU B of each method is listed in the second block of Table 4. Intuitively, the higher the BLEU B score is, the more diversity the generator gets. From Table 4, our method outperforms the other methods, which implies that our method generates more diversified texts than the other methods. As we have analyzed before, the diversity of our method may be derived from \"entropy regularization\" policy gradient.\n\nBLEU HA Finally, BLEU HA takes both generation quality and diversity into account and the results are shown in the last block of Table 4. The BLEU HA reveals that our work gains better performance than other methods. Models COCO IMDB\n\n\nMLE\n\n(1) A girl sitting at a table in front of medical chair.\n\n(2) The person looks at a bus stop while talking on a phone.\n\n(1) If somebody that goes into a films and all the film cuts throughout the movie.\n\n(2) Overall, it is what to expect to be she made the point where she came later.\n\n\nSeqGAN\n\n(1) A man holding a tennis racket on a tennis court.\n\n(2) A woman standing on a beach next to the ocean.\n\n(1) The story is modeled after the old classic \"B\" science fiction movies we hate to love, but do.\n\n(2) This does not star Kurt Russell, but rather allows him what amounts to an extended cameo.\n\n\nLeakGAN\n\n(1) A bathroom with a toilet , window , and white sink.\n\n(2) A man in a cowboy hat is milking a black cow.\n\n(1) I was surprised to hear that he put up his own money to make this movie for the first time.\n\n(2) It was nice to see a sci-fi movie with a story in which you didn't know what was going to happen next.\n\n\nIRL (This work)\n\n(1) A woman is standing underneath a kite on the sand.\n\n(2) A dog owner walks on the beach holding surfboards.\n\n(1) Need for Speed is a great movie with a very enjoyable storyline and a very talented cast.\n\n(2) The effects are nothing spectacular, but are still above what you would expect, all things considered.  \n\n\nIMDB Movie Reviews\n\nWe use a large IMDB text corpus [Diao et al., 2014] for training the generative models as long-length text generation. The dataset is a collection of 350K movie reviews. We select sentences with the length between 17 and 25, set word frequency at 180 as the threshold of frequently occurred words and remove sentences with low frequency words. Finally we randomly choose 80000 sentences for training and 3000 sentences for testing with the vocabulary size at 4979 and the average sentence length is 19.6. IMDB is a more challenging corpus. Unlike sentences in COCO Image captions dataset, which mainly contains simple sentences, e.g., sentences only with the subject-predicate structure, IMDB movie reviews are comprised of various kinds of compound sentences. Besides, the sentence length of IMDB is much longer than that of COCO.\n\nWe also use the same metrics (BLEU F , BLEU B , BLEU HA ) to evaluate our method. The results in Table 5 show our method outperforms other models.\n\n\nTuring Test and Case Study\n\nThe evaluation metrics mentioned above are still not sufficient for evaluating the quality of the sentences because they just focus on the local statistics, ignoring the long-term dependency characteristic of language. So we have to conduct a Turing Test based on scores by a group of people. Each sentence will get 1 point when it is viewed as a real one, otherwise 0 point. We perform the test on frameworks of MLE, SeqGAN, LeakGAN and our method on COCO Image captions dataset and IMDB movie review dataset.\n\nPractically, we sample 20 sentences by each generator from different methods, and for each sentence, we ask 20 different people to score it. Finally, we compute the average score for each sentence, and then calculate the average score for each method according to the sentences it generate. Table 6 shows some generated samples of our and the baseline methods. These samples are what we have collected for people to score.\n\nThe results in Table 7 indicate that the generated sentences of our method have better quality than those generated by MLE, SeqGAN and LeakGAN, especially for long texts.\n\n\nRelated Work\n\nText generation is a crucial task in NLP which is widely used in a bunch of NLP applications. Text generation is more difficult than image generation since texts consist of sequential discrete decisions. Therefore, GAN fails to back propagate the gradients to update the generator. Recently, several methods have been proposed to alleviate this problem, such as Gumbel-softmax GAN [Kusner and Hern\u00e1ndez-Lobato, 2016], RankGAN [Lin et al., 2017], TextGAN [Zhang et al., 2017], LeakGAN [Guo et al., 2017], etc.\n\nSeqGAN  addresses the differentiation problem by introducing RL methods, but still suffers from the problem of reward sparsity. LeakGAN [Guo et al., 2017] manages the reward sparsity problem via Hierarchical RL methods. Joji Toyama [2018] designs several reward functions for partial sequence to solve the issue. However, the generated texts of these methods still lack diversity due to the mode collapse issue. In this paper, we employ IRL framework [Finn et al., 2016] for text generation. Benefiting from its inherent instant reward learning and entropy regularization, our method can generate more diverse texts.\n\nIn this paper, we propose a new method for text generation by using inverse reinforcement learning (IRL). This method alleviates the problems of reward sparsity and mode collapse in the adversarial generation models. In addition, we propose three new evaluation measures based on BLEU score to better evaluate the generated texts.\n\nIn the future, we would like to generalize the IRL framework to the other NLP tasks, such as machine translation, summarization, question answering, etc.\n\nFigure 1 :\n1IRL framework for text generation.\n\nFigure 2 :\n2Illustration of text generator and reward approximator.\n\nFigure 4 :Figure 5 :Figure 6 :\n456Learning curves with different pretrain epochs (10, 25, 50, 100 respectively) on texts of length 20 and 40. Learning curves with different training equilibriums between text generator and reward approximator on texts of length 20 and 40. The proportion in the legend means nr : ng. Learning curves of different methods on the synthetic data of length 20 and 40 respectively.The vertical dashed line indicates the end of the pre-training of SeqGAN, LeakGAN and our method respectively. Since RankGAN didn't publish code, we cannot plot the result of RankGAN.\n\nTable 2 :\n2The overall NLL performance on synthetic data. \"Ground Truth\" consists of samples generated by the oracle LSTM model. Results with * are reported in their papers.10 Epochs \n25 Epochs \n50 Epochs \n100 Epochs \n\nText Length = 20 \n\n\n\n\n.Metrics Evaluated Texts Reference Texts \n\nBLEUF Generated Texts \nTest Set \nBLEUB \nTest Set \nGenerated Texts \n\nBLEUHA \n\n2\u00d7BLEU F \u00d7BLEU B \nBLEU F +BLEU B \n\n\n\nTable 3 :\n3Configurations of BLEUF, BLEUB and BLEUHA.\n\nTable 4 :\n4Results on COCO image caption dataset. Results of \nRankGAN with * are reported in [Guo et al., 2017]. Results of MLE, \nSeqGAN and LeakGAN are based on their published implementa-\ntions. \n\nMetrics \nMLE SeqGAN LeakGAN IRL \nGround \nTruth \n\nBLEUF-2 0.652 \n0.683 \n0.809 \n0.788 \n0.791 \nBLEUF-3 0.405 \n0.418 \n0.554 \n0.534 \n0.539 \nBLEUF-4 0.304 \n0.315 \n0.358 \n0.352 \n0.355 \nBLEUF-5 0.202 \n0.221 \n0.252 \n0.262 \n0.258 \n\nBLEUB-2 0.672 \n0.615 \n0.730 \n0.755 \n0.785 \nBLEUB-3 0.495 \n0.451 \n0.483 \n0.531 \n0.534 \nBLEUB-4 0.316 \n0.299 \n0.318 \n0.347 \n0.357 \nBLEUB-5 0.226 \n0.209 \n0.232 \n0.254 \n0.258 \n\nBLEUHA-2 0.662 \n0.647 \n0.767 \n0.771 \n0.788 \nBLEUHA-3 0.445 \n0.434 \n0.516 \n0.533 \n0.537 \nBLEUHA-4 0.310 \n0.307 \n0.337 \n0.350 \n0.356 \nBLEUHA-5 0.213 \n0.215 \n0.242 \n0.258 \n0.258 \n\n\n\nTable 5 :\n5Results on IMDB Movie Review dataset. Results of MLE, SeqGAN and LeakGAN are based on their published implementations. Since RankGAN didn't publish code, we cannot report the results of RankGAN on IMDB.\n\nTable 6 :\n6Case study. Generated texts from different models on COCO image caption and IMDB movie review datasets.Corpora MLE SeqGAN LeakGAN IRL \nGround \nTruth \n\nCOCO 0.205 \n0.450 \n0.543 \n0.550 \n0.725 \nIMDB 0.138 \n0.205 \n0.385 \n0.463 \n0.698 \n\n\n\nTable 7 :\n7Results of Turing test. Samples of MLE, SeqGAN and LeakGAN are generated based on their published implementations. Since RankGAN didn't publish code, we cannot generate samples of RankGAN.\nThe synthetic data and the oracle LSTM are publicly available at https://github.com/LantaoYu/SeqGAN and https://github.com/CR-Gjx/LeakGAN\nAcknowledgementsWe would like to thank the anonymous reviewers for their valuable comments. The\nScheduled sampling for sequence prediction with recurrent neural networks. Osendorfer ; Justin Bayer, Christian Osendorfer, ; Bengio, Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. arXivNIPSand Osendorfer, 2014] Justin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. arXiv, 2014. [Bengio et al., 2015] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for se- quence prediction with recurrent neural networks. In NIPS, pages 1171-1179, 2015.\n\nJointly modeling aspects, ratings and sentiments for movie recommendation (jmars). Maximum-likelihood augmented discrete generative adversarial networks. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua BengioAlex Graves9arXivSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memoryet al., 2017] Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio. Maximum-likelihood augmented discrete genera- tive adversarial networks. arXiv, 2017. [Chen et al., 2015] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data col- lection and evaluation server. arXiv, 2015. [Chung et al., 2015] Junyoung Chung, Kyle Kastner, Lau- rent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In NIPS, pages 2980-2988, 2015. [Diao et al., 2014] Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexander J Smola, Jing Jiang, and Chong Wang. Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars). In SIGKDD, pages 193-202. ACM, 2014. [Finn et al., 2016] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal con- trol via policy optimization. In ICML, pages 49-58, 2016. [Goodfellow et al., 2014] Ian Goodfellow, Jean Pouget- Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen- erative adversarial nets. In NIPS, pages 2672-2680, 2014. [Graves, 2013] Alex Graves. Generating sequences with re- current neural networks. arXiv, 2013. [Guo et al., 2017] Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. arXiv, 2017. [Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nToward learning better metrics for sequence generation training with policy gradient. ICLR submission. Kotaro Nakayama Yutaka Matsuo Joji Toyama, Yusuke Iwasawa, Kusner and Hern\u00e1ndez-Lobato, 2016] Matt J Kusner and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. GANs for sequences of discrete elements with the gumbel-softmax distribution. arXiv. [Joji Toyama, 2018] Kotaro Nakayama Yutaka Matsuo Joji Toyama, Yusuke Iwasawa. Toward learning better metrics for sequence generation training with policy gradient. ICLR submission, 2018. [Kusner and Hern\u00e1ndez-Lobato, 2016] Matt J Kusner and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. GANs for sequences of discrete elements with the gumbel-softmax distribution. arXiv, 2016.\n\nAdversarial ranking for language generation. Lin, Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv. Nachum et al., 2017] Ofir Nachum, Mohammad Norouzi, Kelvin XuLuke Metz, Ben Poole, DavidNIPS. and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. arXiv[Lin et al., 2017] Kevin Lin, Dianqi Li, Xiaodong He, Ming- ting Sun, and Zhengyou Zhang. Adversarial ranking for language generation. In NIPS, pages 3158-3168, 2017. [Metz et al., 2016] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv, 2016. [Nachum et al., 2017] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. arXiv, 2017.\n\nImproved techniques for training gans. Salimans, NIPS. [Salimans et al., 2016] Tim Salimans, Ian Goodfellow, Woj- ciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, pages 2234-2242, 2016.\n\nA hierarchical latent variable encoder-decoder model for generating dialogues. Serban, AAAI. Serban et al., 2017] Iulian Vlad Serban, Alessandro Sor- doni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. A hierarchical latent vari- able encoder-decoder model for generating dialogues. In AAAI, pages 3295-3301, 2017.\n\nSemantically conditioned LSTM-based natural language generation for spoken dialogue systems. [ Su, AAAI. 8arXivAAAI. Adversarial feature matching for text generation. arXiv[Su et al., 2018] Jinyue Su, Jiacheng Xu, Xipeng Qiu, and Xuanjing Huang. Incorporating discriminator in sentence generation: a gibbs sampling method. In AAAI, 2018. [Wang et al., 2017] Heng Wang, Zengchang Qin, and Tao Wan. Text generation based on generative adversarial nets with latent variable. arXiv, 2017. [Wen et al., 2015] Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. Semantically conditioned LSTM-based natural language generation for spoken dialogue systems. arXiv, 2015. [Williams, 1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce- ment learning. Machine learning, 8(3-4):229-256, 1992. [Yu et al., 2017] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In AAAI, pages 2852-2858, 2017. [Zhang et al., 2017] Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial feature matching for text generation. arXiv, 2017.\n\nMaximum entropy inverse reinforcement learning. [ Ziebart, AAAI. Chicago, IL, USA8[Ziebart et al., 2008] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.\n", "annotations": {"author": "[{\"end\":204,\"start\":70},{\"end\":342,\"start\":205},{\"end\":479,\"start\":343},{\"end\":620,\"start\":480}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":75},{\"end\":216,\"start\":212},{\"end\":353,\"start\":350},{\"end\":494,\"start\":489}]", "author_first_name": "[{\"end\":74,\"start\":70},{\"end\":211,\"start\":205},{\"end\":349,\"start\":343},{\"end\":488,\"start\":480}]", "author_affiliation": "[{\"end\":203,\"start\":80},{\"end\":341,\"start\":218},{\"end\":478,\"start\":355},{\"end\":619,\"start\":496}]", "title": "[{\"end\":67,\"start\":1},{\"end\":687,\"start\":621}]", "venue": null, "abstract": "[{\"end\":1666,\"start\":689}]", "bib_ref": "[{\"end\":2140,\"start\":2106},{\"end\":2698,\"start\":2640},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2738,\"start\":2721},{\"end\":2797,\"start\":2773},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2825,\"start\":2807},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2853,\"start\":2835},{\"end\":2884,\"start\":2858},{\"end\":2976,\"start\":2951},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3880,\"start\":3862},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4077,\"start\":4059},{\"end\":4292,\"start\":4266},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4615,\"start\":4592},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4633,\"start\":4615},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4775,\"start\":4747},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4794,\"start\":4775},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4814,\"start\":4794},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5048,\"start\":5026},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5927,\"start\":5908},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6200,\"start\":6178},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8285,\"start\":8263},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10520,\"start\":10504},{\"end\":10540,\"start\":10520},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12998,\"start\":12979},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14307,\"start\":14290},{\"end\":15969,\"start\":15943},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19635,\"start\":19616},{\"end\":22133,\"start\":22094},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22161,\"start\":22143},{\"end\":22191,\"start\":22163},{\"end\":22219,\"start\":22193},{\"end\":22381,\"start\":22355},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22465,\"start\":22452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22697,\"start\":22678}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23378,\"start\":23331},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23447,\"start\":23379},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24040,\"start\":23448},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":24280,\"start\":24041},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":24438,\"start\":24281},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":24493,\"start\":24439},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":25266,\"start\":24494},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":25481,\"start\":25267},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":25726,\"start\":25482},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":25927,\"start\":25727}]", "paragraph": "[{\"end\":1861,\"start\":1682},{\"end\":2542,\"start\":1863},{\"end\":3329,\"start\":2544},{\"end\":3414,\"start\":3331},{\"end\":4408,\"start\":3416},{\"end\":4902,\"start\":4410},{\"end\":5992,\"start\":4904},{\"end\":6052,\"start\":5994},{\"end\":6471,\"start\":6054},{\"end\":6843,\"start\":6526},{\"end\":7398,\"start\":6845},{\"end\":7584,\"start\":7470},{\"end\":7881,\"start\":7586},{\"end\":8192,\"start\":7883},{\"end\":8371,\"start\":8216},{\"end\":8659,\"start\":8405},{\"end\":8786,\"start\":8694},{\"end\":8931,\"start\":8823},{\"end\":9065,\"start\":9002},{\"end\":9102,\"start\":9067},{\"end\":9444,\"start\":9234},{\"end\":9690,\"start\":9446},{\"end\":9717,\"start\":9692},{\"end\":9909,\"start\":9826},{\"end\":10138,\"start\":9928},{\"end\":10424,\"start\":10212},{\"end\":10640,\"start\":10456},{\"end\":10698,\"start\":10693},{\"end\":10934,\"start\":10740},{\"end\":11227,\"start\":10983},{\"end\":11264,\"start\":11229},{\"end\":11512,\"start\":11360},{\"end\":11703,\"start\":11563},{\"end\":12359,\"start\":11744},{\"end\":12803,\"start\":12372},{\"end\":13242,\"start\":12818},{\"end\":13429,\"start\":13263},{\"end\":13466,\"start\":13431},{\"end\":13498,\"start\":13468},{\"end\":13564,\"start\":13500},{\"end\":13627,\"start\":13566},{\"end\":13669,\"start\":13659},{\"end\":13702,\"start\":13671},{\"end\":13766,\"start\":13704},{\"end\":13818,\"start\":13768},{\"end\":14382,\"start\":13820},{\"end\":15488,\"start\":14384},{\"end\":15801,\"start\":15490},{\"end\":16244,\"start\":15825},{\"end\":16381,\"start\":16280},{\"end\":16817,\"start\":16383},{\"end\":18066,\"start\":16819},{\"end\":18301,\"start\":18068},{\"end\":18365,\"start\":18309},{\"end\":18427,\"start\":18367},{\"end\":18511,\"start\":18429},{\"end\":18593,\"start\":18513},{\"end\":18656,\"start\":18604},{\"end\":18708,\"start\":18658},{\"end\":18808,\"start\":18710},{\"end\":18903,\"start\":18810},{\"end\":18970,\"start\":18915},{\"end\":19021,\"start\":18972},{\"end\":19118,\"start\":19023},{\"end\":19226,\"start\":19120},{\"end\":19300,\"start\":19246},{\"end\":19356,\"start\":19302},{\"end\":19451,\"start\":19358},{\"end\":19561,\"start\":19453},{\"end\":20415,\"start\":19584},{\"end\":20563,\"start\":20417},{\"end\":21104,\"start\":20594},{\"end\":21528,\"start\":21106},{\"end\":21700,\"start\":21530},{\"end\":22225,\"start\":21717},{\"end\":22843,\"start\":22227},{\"end\":23175,\"start\":22845},{\"end\":23330,\"start\":23177}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7469,\"start\":7399},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8404,\"start\":8372},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8693,\"start\":8660},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9001,\"start\":8932},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9233,\"start\":9103},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9825,\"start\":9718},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10211,\"start\":10139},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10692,\"start\":10641},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10739,\"start\":10699},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10982,\"start\":10935},{\"attributes\":{\"id\":\"formula_10\"},\"end\":11313,\"start\":11265},{\"attributes\":{\"id\":\"formula_11\"},\"end\":11359,\"start\":11313},{\"attributes\":{\"id\":\"formula_12\"},\"end\":11562,\"start\":11513},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13658,\"start\":13628}]", "table_ref": "[{\"end\":13188,\"start\":13181},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15032,\"start\":15025},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":16895,\"start\":16888},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17122,\"start\":17115},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17720,\"start\":17713},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17818,\"start\":17811},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":18204,\"start\":18197},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":20521,\"start\":20514},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":21404,\"start\":21397},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":21552,\"start\":21545}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1680,\"start\":1668},{\"attributes\":{\"n\":\"2\"},\"end\":6524,\"start\":6474},{\"attributes\":{\"n\":\"2.1\"},\"end\":8214,\"start\":8195},{\"end\":8821,\"start\":8789},{\"attributes\":{\"n\":\"2.2\"},\"end\":9926,\"start\":9912},{\"end\":10454,\"start\":10427},{\"attributes\":{\"n\":\"2.3\"},\"end\":11742,\"start\":11706},{\"attributes\":{\"n\":\"3\"},\"end\":12370,\"start\":12362},{\"attributes\":{\"n\":\"4\"},\"end\":12816,\"start\":12806},{\"attributes\":{\"n\":\"4.1\"},\"end\":13261,\"start\":13245},{\"attributes\":{\"n\":\"4.2\"},\"end\":15823,\"start\":15804},{\"end\":16278,\"start\":16247},{\"end\":18307,\"start\":18304},{\"end\":18602,\"start\":18596},{\"end\":18913,\"start\":18906},{\"end\":19244,\"start\":19229},{\"attributes\":{\"n\":\"4.3\"},\"end\":19582,\"start\":19564},{\"attributes\":{\"n\":\"4.4\"},\"end\":20592,\"start\":20566},{\"attributes\":{\"n\":\"5\"},\"end\":21715,\"start\":21703},{\"end\":23342,\"start\":23332},{\"end\":23390,\"start\":23380},{\"end\":23479,\"start\":23449},{\"end\":24051,\"start\":24042},{\"end\":24449,\"start\":24440},{\"end\":24504,\"start\":24495},{\"end\":25277,\"start\":25268},{\"end\":25492,\"start\":25483},{\"end\":25737,\"start\":25728}]", "table": "[{\"end\":24280,\"start\":24215},{\"end\":24438,\"start\":24284},{\"end\":25266,\"start\":24506},{\"end\":25726,\"start\":25597}]", "figure_caption": "[{\"end\":23378,\"start\":23344},{\"end\":23447,\"start\":23392},{\"end\":24040,\"start\":23483},{\"end\":24215,\"start\":24053},{\"end\":24284,\"start\":24283},{\"end\":24493,\"start\":24451},{\"end\":25481,\"start\":25279},{\"end\":25597,\"start\":25494},{\"end\":25927,\"start\":25739}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8191,\"start\":8183},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8785,\"start\":8777},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10088,\"start\":10080},{\"end\":11680,\"start\":11672},{\"end\":14519,\"start\":14511},{\"end\":14719,\"start\":14711},{\"end\":15382,\"start\":15374}]", "bib_author_first_name": "[{\"end\":26256,\"start\":26237},{\"end\":26273,\"start\":26264},{\"end\":26287,\"start\":26286},{\"end\":28815,\"start\":28781},{\"end\":28830,\"start\":28824},{\"end\":30857,\"start\":30856},{\"end\":32021,\"start\":32020}]", "bib_author_last_name": "[{\"end\":26262,\"start\":26257},{\"end\":26284,\"start\":26274},{\"end\":26294,\"start\":26288},{\"end\":28822,\"start\":28816},{\"end\":28838,\"start\":28831},{\"end\":29420,\"start\":29417},{\"end\":30217,\"start\":30209},{\"end\":30497,\"start\":30491},{\"end\":30860,\"start\":30858},{\"end\":32029,\"start\":32022}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1820089},\"end\":26673,\"start\":26162},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14193088},\"end\":28676,\"start\":26675},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3019438},\"end\":29370,\"start\":28678},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4857922},\"end\":30168,\"start\":29372},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1687220},\"end\":30410,\"start\":30170},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14857825},\"end\":30761,\"start\":30412},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":739696},\"end\":31970,\"start\":30763},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":336219},\"end\":32251,\"start\":31972}]", "bib_title": "[{\"end\":26235,\"start\":26162},{\"end\":26756,\"start\":26675},{\"end\":28779,\"start\":28678},{\"end\":29415,\"start\":29372},{\"end\":30207,\"start\":30170},{\"end\":30489,\"start\":30412},{\"end\":30854,\"start\":30763},{\"end\":32018,\"start\":31972}]", "bib_author": "[{\"end\":26264,\"start\":26237},{\"end\":26286,\"start\":26264},{\"end\":26296,\"start\":26286},{\"end\":28824,\"start\":28781},{\"end\":28840,\"start\":28824},{\"end\":29422,\"start\":29417},{\"end\":30219,\"start\":30209},{\"end\":30499,\"start\":30491},{\"end\":30862,\"start\":30856},{\"end\":32031,\"start\":32020}]", "bib_venue": "[{\"end\":29592,\"start\":29565},{\"end\":32053,\"start\":32037},{\"end\":26356,\"start\":26296},{\"end\":26827,\"start\":26758},{\"end\":29006,\"start\":28840},{\"end\":29502,\"start\":29422},{\"end\":30223,\"start\":30219},{\"end\":30503,\"start\":30499},{\"end\":30866,\"start\":30862},{\"end\":32035,\"start\":32031}]"}}}, "year": 2023, "month": 12, "day": 17}