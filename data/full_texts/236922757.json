{"id": 236922757, "updated": "2023-04-05 11:29:32.402", "metadata": {"title": "INFaaS: Automated Model-less Inference Serving", "authors": "[{\"first\":\"Francisco\",\"last\":\"Romero\",\"middle\":[]},{\"first\":\"Qian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Neeraja\",\"last\":\"Yadwadkar\",\"middle\":[\"J.\"]},{\"first\":\"Christoforos\",\"last\":\"Kozyrakis\",\"middle\":[\"E.\"]}]", "venue": "USENIX Annual Technical Conference", "journal": "397-411", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Despite existing work in machine learning inference serving, ease-of-use and cost ef\ufb01ciency remain challenges at large scales. Developers must manually search through thousands of model-variants \u2013 versions of already-trained models that differ in hardware, resource footprints, latencies, costs, and accuracies \u2013 to meet the diverse application requirements. Since requirements, query load, and applications themselves evolve over time, these decisions need to be made dynamically for each inference query to avoid excessive costs through naive autoscaling. To avoid navigating through the large and complex trade-off space of model-variants, developers often \ufb01x a variant across queries, and replicate it when load increases. However, given the diversity across variants and hardware platforms in the cloud, a lack of understanding of the trade-off space can incur signi\ufb01cant costs to developers. This paper introduces", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/usenix/Romero0YK21", "doi": null}}, "content": {"source": {"pdf_hash": "021f6b1e8c328a4ce0e90dbd0d74e11e90aa4224", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e63ab4aa5130c4d59e0117c33131bf6ad4bf5a4c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/021f6b1e8c328a4ce0e90dbd0d74e11e90aa4224.txt", "contents": "\nINFaaS: Automated Model-less Inference Serving INFaaS: Automated Model-less Inference Serving\nJuly 14-16, 2021\n\nFrancisco Romero faromero@stanford.edu \nStanford University\nStanford University\n\n\nQian Li qianli@cs.stanford.edu \nStanford University\nStanford University\n\n\nNeeraja J Yadwadkar neeraja@cs.stanford.edu \nStanford University\nStanford University\n\n\nChristos Kozyrakis kozyraki@stanford.edu \nStanford University\nStanford University\n\n\nFrancisco Romero \nStanford University\nStanford University\n\n\nQian Li \nStanford University\nStanford University\n\n\nNeeraja J Yadwadkar \nStanford University\nStanford University\n\n\nChristos Kozyrakis \nStanford University\nStanford University\n\n\nINFaaS: Automated Model-less Inference Serving INFaaS: Automated Model-less Inference Serving\n\nProceedings of the 2021 USENIX Annual Technical Conference\nthe 2021 USENIX Annual Technical ConferenceJuly 14-16, 2021This paper is included in the 978-1-939133-23-6 Open access to the Proceedings of the 2021 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc21/presentation/romero\nDespite existing work in machine learning inference serving, ease-of-use and cost efficiency remain challenges at large scales. Developers must manually search through thousands of model-variants -versions of already-trained models that differ in hardware, resource footprints, latencies, costs, and accuracies -to meet the diverse application requirements. Since requirements, query load, and applications themselves evolve over time, these decisions need to be made dynamically for each inference query to avoid excessive costs through naive autoscaling. To avoid navigating through the large and complex trade-off space of model-variants, developers often fix a variant across queries, and replicate it when load increases. However, given the diversity across variants and hardware platforms in the cloud, a lack of understanding of the trade-off space can incur significant costs to developers. This paper introduces INFaaS, an automated model-less system for distributed inference serving, where developers simply specify the performance and accuracy requirements for their applications without needing to specify a specific model-variant for each query. INFaaS generates model-variants from already trained models, and efficiently navigates the large trade-off space of model-variants on behalf of developers to meet application-specific objectives: (a) for each query, it selects a model, hardware architecture, and model optimizations, (b) it combines VM-level horizontal autoscaling with model-level autoscaling, where multiple, different modelvariants are used to serve queries within each machine. By leveraging diverse variants and sharing hardware resources across models, INFaaS achieves 1.3\u00d7 higher throughput, violates latency objectives 1.6\u00d7 less often, and saves up to 21.6\u00d7 in cost (8.5\u00d7 on average) compared to state-of-the-art inference serving systems on AWS EC2.\n\nIntroduction\n\nThe number of applications relying on inference from Machine Learning (ML) models is already large [14,47,48,60,67] and expected to keep growing. Facebook, for instance, serves tens-of-trillions of inference queries per day [40,43]. Distributed inference dominates ML production costs: on AWS, it accounts for over 90% of ML infrastructure cost [16].\n\nTypically, an ML lifecycle has two distinct phases -training and inference. Models are trained in the training phase; the training phase is usually characterized by long-running hyperparameter searches, dedicated hardware resource usage, * Equal contribution and no completion deadlines. In the inference phase, trained models are queried by various end-user applications. Being user-facing, inference serving requires cost-effective systems that render predictions with latency constraints while handling unpredictable and bursty request arrivals.\n\nInference serving systems face a number of challenges [61,73] due to the following factors. (a) Diverse application requirements: Applications issue queries that differ in latency, cost, accuracy, and even privacy [56] requirements [42,45,61]. Table 1 shows the same face recognition model queried by multiple applications with different requirements. Some applications, such as intruder detection, require inference in realtime but can tolerate lower accuracy. Other applications, such as tagging faces on social media, may prefer accuracy over latency. (b) Heterogeneous execution environments: Leveraging heterogeneous hardware resources (e.g., different generations of CPUs, GPUs, and accelerators like TPU [49] or AWS Inferentia [18]) helps meet the diverse needs of applications and the dynamic changes in the workload; however, it is nontrivial to manage and scale heterogeneous resources [40].\n\n(c) Diverse model-variants: Graph optimizers, such as TVM [22], TensorRT [3], and methods, such as layer fusion or quantization [15], produce versions of the same model, model-variants, that may differ in inference latency, memory footprint, and accuracy.\n\nTogether, these factors create a large search space. For instance, from 21 already-trained image classification models, we generated 166 model-variants, by (i) applying model graph optimizers, such as TensorRT [3], (ii) optimizing for different batch sizes, and (iii) changing underlying hardware resources (e.g., CPUs, GPUs, and Inferentia). These variants vary across many dimensions: the accuracies range from 56.6% to 82.5% (1.46\u00d7), the model loading latencies range from 590ms to 11s (18.7\u00d7), and the inference latencies for a single query range from 1.5ms to 5.7s (3,700\u00d7). Their computational requirements range from 0.48 to 24 GFLOPS (50\u00d7) [61], and the cost of hardware these variants incur [17] ranges from $0.096/hr for 2 vCPUs to $3.06/hr for a V100 GPU (32\u00d7). As new inference accelerators are introduced and new opti-mization techniques emerge, the number of model-variants will only grow.\n\nThis large search space makes it hard for developers to manually map the requirements of each inference query to decisions about selecting the right model and model optimizations, suitable hardware platforms, and auto-scaling configurations. The decision complexity is further exacerbated when the load varies, applications evolve, and the availability of hardware resources (GPUs, ASICs) changes. Unlike long-running batch data analytics or ML training jobs [8,26,36,55,68] that can be right-sized during or across subsequent executions, the dynamic nature of distributed inference serving makes it infeasible to select model-variants statically.\n\nOur key insight is that the large diversity of model-variants is not a nuisance but an opportunity: it allows us to meet the diverse and varying performance, cost, and accuracy requirements of applications, in the face of varying load and hardware resource availability, if only we can select and deploy the right model-variant effectively for each query. However, given the complexity of this search space, existing systems, including Clipper [25], TensorFlow Serving [5], AWS SageMaker [11], and others [1,6,35,38,75], ignore the opportunity. These systems require developers to select model-variants, batch sizes, instance/hardware types, and statically-defined autoscaling configurations, for meeting application requirements. If these decisions are made without understanding the trade-offs offered by the variants, the impact could be significant (note the wide cost, performance, and accuracy ranges spanned by the variants). We argue that in addition to traditional autoscaling, distributed inference serving systems should navigate this search space of model-variants on behalf of developers, and automatically manage model-variants and heterogeneous resources. Surprisingly, as also noted in prior work [73], no existing inference serving system does that.\n\nTo this end, we built INFaaS, an automated model-less system for distributed inference serving. INFaaS introduces a model-less interface where after registering trained models, developers specify only the high-level performance, cost, or accuracy requirements for each inference query. INFaaS generates model-variants of the registered models, and navigates the large space to select a model-variant and automatically switch between differently optimized variants to best meet the query requirements. INFaaS also automates resource provisioning for model-variants and schedules queries across a heterogeneous cluster.\n\nTo realize this, INFaaS generates model-variants and their performance-cost profiles on different hardware platforms. INFaaS tracks the dynamic status of variants (e.g., overloaded or interfered) using a state machine, to efficiently select the right variant for each query to meet the application requirements. Finally, INFaaS combines VM-level (horizontal scaling) and model-level autoscaling to dynamically react to the changing application requirements and request patterns. Given the large and complex search space of model-variants, we formulate an integer linear program (ILP) for our model-level autoscaling that finds the most cost-effective combination of model-variants, to meet the goals for queries in large scale inference serving.\n\nUsing query patterns derived from real-world applications and traces, we evaluate INFaaS against existing inference serving systems, including Clipper [25] and SageMaker [11], with 175 variants generated from 22 model architectures, on AWS. Compared to Clipper, INFaaS' ability to select suitable model-variants, leverage heterogeneous hardware (CPU, GPU, Inferentia), and share hardware resources across models and applications enables it to save 1.23\u00d7 in cost, violate latency objectives 1.6\u00d7 less often, and improve resource utilization by 2.8\u00d7. At low load, INFaaS saves cost by 21.6\u00d7 compared to Clipper, and 21.3\u00d7 compared to SageMaker.\n\n\nChallenges\n\n\nSelecting the right model-variant\n\nA model-variant is a version of a model defined by the following aspects: (a) model architecture (e.g., ResNet50, VGG16), (b) programming framework, (e.g., TensorFlow, Py-Torch, Caffe2, MXNet), (c) model graph optimizers (e.g., Ten-sorRT, Neuron, TVM, XLA [72]), (d) hyperparameters (e.g., optimizing for batch size of 1, 4, 8, or 16), and (e) hardware platforms (e.g., Haswell or Skylake CPUs, V100 or T4 GPUs, FPGA, and accelerators, such as Inferentia [18], TPU [49], Catapult [32], NPU [7]). Based on the 21 image classification models and the available hardware on AWS EC2 [9], we estimate the total number of possible model-variants would be 4,032. The performance, cost, and accuracy trade-off space offered by these variants is large [19,61]. As new inference accelerators are introduced and new optimization techniques emerge, the number of model-variants will only grow.\n\nExisting inference serving systems require developers to identify the model-variant that can meet diverse performance, accuracy, and cost requirements of applications. However, generating and leveraging these variants requires a substantial understanding of the frameworks, model graph optimizers, and characteristics of hardware architectures, thus limiting the variants an application developer can leverage. As shown in Table 1, one can use the same face recognition model for several applications, but selecting the appropriate modelvariant depends on the requirements of an application [61].\n\nWe argue that inference serving systems should automatically and efficiently select a model-variant for each query on behalf of developers to align with application requirements.\n\n\nReducing cost as load varies\n\nQuery patterns and service level objectives (SLOs) of applications, such as real-time translation and video analytics, can vary unpredictably [43,50,76]. Provisioning for peak demand leads to high cost, hence distributed inference serving systems need to dynamically respond to changes. Traditional autoscaling focuses on horizontal virtual machine replication   (VM-scaling), adding or removing worker machines [33,37]. However, relying only on worker replication may incur significant latency, as new machines must be spawned. Autoscalers used by existing inference serving systems [11,35,37] replicate a statically-fixed (developer-specified) modelvariant for all the queries of an application. This is insufficient because: (a) the right variant may change with load (e.g., a CPU variant may be more suitable at low QPS to meet the cost SLOs) and (b) the hardware resources needed to replicate the same variant may not always be available (e.g., shortage of GPU instances at some point).\n\nIn addition to using VM-scaling and replication-based model-scaling, we introduce model-level vertical scaling, where we switch to a differently optimized variant as load changes. The challenge is to identify which variant to scale to given available hardware resources and query requirements. Consider the example shown in Table 2 with three ResNet50 variants. Each variant runs on a different hardware resource and differs in latency, saturation throughput, and cost. In Table 3, we present three input loads and SLO requirements, with the goal of scaling to the most cost-effective combination of variants. In the first case (QPS = 10 and SLO = 300ms), though all variants can meet the QPS and SLO, using two instances of Variant A is the cheapest choice (8\u00d7 cheaper than using Variant C). In the second case, the load remains unchanged, but due to the stricter SLO, Variant B becomes the cheapest choice (5.3\u00d7 cheaper than using Variant C). In the third case, the combination of two instances of Variant B and one of Variant C is the cheapest. This configuration is 9\u00d7 cheaper than using 200 instances of Variant A (the most expensive configuration). Deciding the best configuration becomes more challenging as the number of variants increases and resource availability changes.\n\n\nImproving utilization at low load\n\nFor predictable performance, one may serve each model-variant on a dedicated machine to exclusively access hardware resources. But, this often results in underutilized resources and cost-inefficiency, especially at low load. Instead, the serving systems should support multi-tenancy by sharing resources across applications and models, thereby improving utilization and the overall cost. Recent work [36,71] has shown the benefits of sharing GPUs for deep-learning training jobs. ML inference jobs are typically less demanding for compute and memory resources than the training jobs, making inference jobs ideal for sharing GPUs and other accelerators [46,74]. However, how to share accelerators across multiple tenants while maintaining predictable performance is not obvious. Figure 1 shows the result of co-locating a large (Inception-ResNetV2) and a small (MobileNetV1) model on a GPU. At low load, sharing a GPU does not affect the performance of either model. At higher load, this co-location heavily impacts the performance of the small model, while the large model remains unaffected. The point when co-location starts affecting the performance varies across models, and depends on both the load and the hardware architecture.\n\n\nINFaaS\n\nDesign principles. We design INFaaS based on the following guidelines. First, INFaaS should support a declarative API: Developers should not need to specify the model, model optimizations, suitable hardware platforms, or autoscaling configurations; they should only focus on high-level performance, cost, or accuracy requirements. Second, INFaaS should automatically and efficiently select a model-variant, while considering the dynamic state of the model-variants and the hardware resources, for (a) serving each query, and (b) deciding how to scale in reaction to changing application load. Third, to improve resource utilization, the system should share hardware resources across model-variants and applications, without violating performance-cost constraints. Finally, the system design should be modular and extensible to allow new model-variant selection policies. By following these design principles, we naturally address the challenges raised in Section 2. Functionality. INFaaS generates new model-variants from the models registered by developers, and stores them in a repository (Section 3.2). These variants are optimized along different dimensions using model graph optimizers such as Neuron and TensorRT. For each inference query, INFaaS automatically selects a model-variant to satisfy its performance, cost, and accuracy objectives (Section 4.1). INFaaS' autoscaler combines VM-level autoscaling with model-level horizontal and vertical autoscaling to meet application per-API Parameters register_model modelName, modelBinary, valSet, appID inference_query input(s), appID, latency, accuracy inference_query input(s), modelName  Model registration. Developers register one or more models using the register_model API. This API accepts a developer-assigned model identifier (modelName), the model (modelBinary) in serialized format (e.g., a TensorFlow SavedModel or model in ONNX format), and a developerassigned application identifier (appID). Models for different prediction tasks within the same application (e.g., optical character recognition and language translation) can be registered with separate appIDs. Lines 1-2 in Figure 2 show how a developer registers two models, a ResNet50 and a MobileNet, for an application with appID=detectFaceApp. INFaaS generates multiple variants from these already trained models. For instance, using ResNet50 alone, INFaaS can generate about 50 variants by changing the batch size, the hardware, and the model graph optimizer (Section 3.2). Note that INFaaS is an inference serving system and does not train new models; INFaaS only generates variants from already-trained models.\n\n\nModel-less interface for inference\n\nThe register_model API takes a validation dataset (e.g., valSet) as input to calculate the accuracy of the newly generated variants. For each incoming query, INFaaS automatically selects the right model-variant to meet the specified goals.\n\nQuery submission. Being declarative, INFaaS' API allows developers to specify high-level goals without needing to specify the variants for their queries. Using the inference_query API, developers can submit inference queries in two ways:\n\n\u2022 Specifying application requirements. Developers may submit queries for their application and specify high-level application performance, cost, and accuracy requirements (e.g., Line 3 in Figure 2). INFaaS then navigates the search space of model-variants for the given application, and selects model-variants and scaling strategies. For instance, for a query with appID=detectFaceApp, INFaaS searches for a suitable variant of ResNet50 and MobileNet to meet the goal of latency (200ms) and accuracy (above 70%).\n\n1 r e g i s t e r _ m o d e l ( \" ResNet50 \" , ResNet50 . pt , v a l S e t , detectFaceApp ) 2 r e g i s t e r _ m o d e l ( \" MobileNet \" , MobileNet . pt , v a l S e t , detectFaceApp ) # Developer r e g i s t e r e d 2 models f o r t h e detectFaceApp ; # INFaaS generates v a r i a n t s from these two r e g i s t e r e d models 3 i n f e r e n c e _ q u e r y ( i n p u t . jpg , detectFaceApp , 200ms, 70%) # Developer s u b m i t t e d a query w i t h \" i n p u t . j p g \" as t h e i n p u t # and s p e c i f i e d r e q u i r e m e n t s ; INFaaS then s e l e c t s a v a r i a n t # a u t o m a t i c a l l y t o meet 200ms l a t e n c y and accuracy > 70% \u2022 Specifying a registered model. Developers may use this interface to specify the model, modelName, they registered for the corresponding application (e.g., \"ResNet50\" for detectFaceApp). This interface supports developers who want direct control over the model-variant used. This is the only option offered by existing inference systems. INFaaS then dynamically scales resources for the specified modelvariant based on the observed workload.\n\nINFaaS' serving workflow for an inference query. Applications interact with INFaaS by submitting inference queries through the Front-End, logically hosted at the Controller (see steps marked in Figure 3). The Controller selects a model-variant and dispatches the inference query to a Worker machine. Workers further send inference queries to the appropriate Hardware Executors according to the selected modelvariant, and reply with inference results to applications.\n\n\nArchitecture\n\nWe now describe INFaaS' architecture ( Figure 3) in detail.\n\nController. The logically-centralized controller receives model registration and inference requests. The controller hosts three modules: (a) The Dispatcher that uses the model-variant selection policy for selecting a variant to serve a query, (b) The VM-Autoscaler that is responsible for scaling the number of workers up and down based on the current load and resource utilization, and (c) The Model Registrar that handles model registration.\n\nWorkers. Worker machines execute inference queries assigned by the controller. Hardware-specific Executor daemons manage the deployment and execution of model-variants. The\n\nDispatcher forwards each query to a specific model-variant instance through the corresponding hardware executor. The Model-Autoscaler detects changes in the load and decides a scaling strategy that either replicates running variants or selects a different variant, within the worker. It uses the modelvariant selection policy to select a variant to scale to. The Monitoring Daemon tracks the utilization of machines and variants, and manages resources shared by multiple variants to avoid SLO violations.\n\nVariant-Generator and Variant-Profiler. From the registered models, the Variant-Generator generates model-variants optimized for different batch sizes, hardware, and hardwarespecific parameters (e.g., number of cores on Inferentia) using model graph optimizers, including TensorRT [3] and Neuron [15]. To allow for other model-variant selection algorithms, we designed INFaaS to decouple policies from mechanisms [53]. Metadata Store. The Metadata Store enables efficient access to the static and dynamic data about workers and model-variants; this is needed for making model-variant selection and scaling decisions. This data consists of (a) the information about available model architectures and their variants (e.g., accuracy and profiled inference latency), and (b) the resource usage and load statistics of variants and worker machines. The Metadata Store strategically uses data structures to ensure low access latencies (\u223c O(1)) for efficient decision-making. The Metadata Store runs on the same machine as the controller to reduce access latencies for selecting variants. Implementation and data structure details are described in Section 5. Model Repository. The Model Repository is a high-capacity persistent storage medium that stores serialized variants that are accessible to workers when needed to serve queries.\n\n\nSelecting and Scaling Model-Variants\n\nINFaaS uses the model-variant selection policy in two cases: (I) On arrival of a query: INFaaS' controller needs to select a variant for each query to meet an application's high-level requirements (Section 4.1). This invocation of the selection policy lies on the critical path of inference serving. (II) On changes in query load: As the query load changes, INFaaS' workers must decide whether to switch to a differently optimized variant (Section 4.2). The worker invokes the selection policy off the critical path. INFaaS provides an internal API, getVariant, for invoking model-variant selection policy. In both cases, INFaaS needs to consider both the static and dynamic states of variants and available resources. Only considering statically-profiled metadata is insufficient, since the following aspects can significantly impact the observed performance and cost: a selected variant (a) may not be loaded, hence we need to consider its loading latency, (b) may be already loaded but serving at its peak throughput, (c) may be already loaded but experiencing resource contention from co-located inference jobs, and (d) may not be loaded due to lack of resources required for that specific variant. We next describe how INFaaS tracks the dynamic state of model-variants, and then describe the policy used in the two cases. State machine for the lifecycle of model-variants. To track the dynamic state of each model-variant instance perapplication, INFaaS uses a state machine (shown in Figure 4). All the registered and generated model-variants start in the Inactive state: they are not loaded on any worker. Once a variant instance is loaded, it transitions to the Active state. These variant instances are serving less than their peak throughput, tracked by the worker's monitoring daemons. Variant instances enter the Overloaded state when they serve at their peak throughput. Finally, variant instances in the Interfered state are not overloaded but are still experiencing higher inference latencies than the profiled values. Interference occurs when co-located variants contend over shared resources (e.g., caches, memory bandwidth, or hardware threads).\n\nAlgorithm 1 Model-variant selection for case I (arrival of a query) 1: function GETVARIANT(appID,accuracy,latency) 2:\n\nif searchActiveVariants(appID,accuracy,latency) then 3:\n\nGet least-loaded worker, W ll , running activeVariant 4:\n\nreturn activeVariant, W ll 5:\n\nif searchInactiveVariants(appID,accuracy,latency) then 6:\n\nGet \n\n\nCase II: On changes in query load\n\nAs query load changes, INFaaS needs to revisit its variant selection decision to check whether a different variant is more cost-efficient. Existing inference serving systems [5,11,25,35,37]   The objective function that our ILP minimizes is the total cost of all the chosen scaling actions. For a variant v i j , this cost for an action \u03b4 i j is the sum of the hardware cost (in $/second), and the loading latency (in seconds) of the variant:\nCost(\u03b4 i j ) = C i j (\u03b4 i j + \u03bbT load i j max(\u03b4 i j , 0))\nwhere C i j is the hardware cost (in $/second) for running the variant, T load i j is the loading latency of the variant, and \u03bb (in 1 second ) is a tunable parameter for the query load unpredictability. Large values of \u03bb place more weight on minimizing loading latency to meet SLOs when the query load is unpredictable or spiky. Small values of \u03bb place more weight on minimizing the hardware cost when the query load is more stable.\n\nThus, our objective function representing the total cost for all the variants is: \u2211 i, j Cost(\u03b4 i j ). We impose the following constraints on our ILP: (1) With the chosen scaling actions, INFaaS supports the incoming query load.\n\n(2) The newly-loaded instances satisfy applications' SLOs.\n\n(3) The resources consumed by all variants do not exceed the total system resources.\n\n(4) The number of running instances is non-negative. We write these constraints formally as:\n\u2211 i, j Q i j (N i j + \u03b4 i j ) \u2265 L + slack for all i, j (1) T inf i j \u2264 S if \u03b4 i j > 0 (2) \u2211 i, j R type i j (N i j + \u03b4 i j ) \u2264 R type total for all types (3) N i j + \u03b4 i j \u2265 0 for all i, j(4)\nwhere ( The model-variant selection policy queries the Metadata Store to get the values of these variables. Practical limitation of the ILP. Unfortunately, this ILP is NP-complete and hence offers limited practical benefits [34,54,69]: it has to exhaustively search through all the model-variants, track their dynamically changing state, and accurately estimate the QPS each variant can support to find a scaling configuration that can sustain the changed query workload. Gurobi [41] took 23 seconds to find the optimal number of running variant instances across 50 model architectures, and 50 seconds for 100 model architectures. To meet realtime requirements of latency-sensitive applications, INFaaS must have sub-second response time to query workload changes.\n\n\nA Greedy Heuristic\n\nThe time taken to solve each instance of our ILP makes it impractical to use for INFaaS. Instead, we design a greedy heuristic algorithm that replaces our ILP's large search space by a subset of model-variants. This pruned search space allows INFaaS to meet the outlined constraints at sub-second latency. We evaluate the effectiveness of this algorithm in Section 6.2. Each worker machine runs a Model-Autoscaler that together with the model-variant selection policy approximates this ILP as follows: (a) Identify whether the constraints are in danger of being violated, (b) Consider two strategies, replicate or upgrade/downgrade, to satisfy the constraints, (c) Compute the objective for each of these scaling actions and pick the one that minimizes the objective cost function, and (d) Coordinate with the controller to invoke VM-level autoscaling if constraints cannot be satisfied with model-level autoscaling. Scaling up algorithm: To decide if there is a need to scale (Constraint #1), the Model-Autoscaler estimates the current headroom in capacities of running model-variants, given the profiled values of their saturation throughput, and the current load they are serving. We compute the current load served by a variant using the batch size and number of queries served per second. The load served by a worker is estimated by summing the load served by all running variants. The saturation throughput of all running variants is estimated in a similar manner using the profiled values of model-variants. The Model-Autoscaler then computes the current headroom of a worker as the ratio of the combined saturation throughput and the combined load currently served by the running variants on that worker. INFaaS maintains a minimum headroom, slack-threshold, on each worker to absorb sudden load spikes. We discuss the value of this tunable parameter in Section 5. When the current headroom is below the required minimum slack-threshold, the Model-Autoscaler concludes that we need to scale, and proceeds to answer the second question: how to scale (replicate or upgrade) to meet the incoming query load.\n\nTo decide how to scale, the Model-Autoscaler uses the model-variant selection policy's getVariant method to select the cheapest option between replication and upgrading. For this case, the input to getVariant is the incoming query load, and the output is the set of scaling actions. The policy first estimates the cost of model-horizontal scaling (replication) by estimating the number of instances of the running variant that would be added to meet the incoming query load (Constraints #1, #4). Secondly, the policy estimates the cost of model-vertical scaling (upgrade), by querying the Metadata Store to select variants of the same model architecture that can meet the SLO (Constraint #2), and support a higher throughput than the currently running variant. The required number of instances for these variants to meet the incoming query load is then estimated. Finally, the model-variant selection policy computes the cost function of our ILP, by using the hardware cost ($/s) and the variant loading latency to decide whether to replicate the running variant, or upgrade to a variant that supports higher throughput. The available resources on the worker limit the number of variant instances it can run (Constraint #3). Thus, if the strategy requires more resources than are available on the current worker (e.g., hardware accelerator), the worker coordinates with the controller to load the variant on a capable worker. Scaling down algorithm: To decide if and how to scale down (remove replicas or downgrade), the Model-Autoscaler on each worker uses the model-variant selection policy that follows a similar algorithm explained above for scaling up. At regular intervals, this policy checks if the incoming query load can be supported by removing an instance of the running variant, or downgrading to a cheaper variant (optimized for a lower batch size or running on different hardware). The Model-Autoscaler waits for T v time slots before executing the chosen strategy for a variant v, to avoid scaling down too quickly. T v is set equal to the loading latency of variant v. Comparison with ILP. As described in Section 4.2.1, the ILP does not have sub-second response time. Setting a larger headroom to allow the ILP to produce a solution can result in (a) scaling variants too quickly, which leads to underutilization and higher cost, and (b) violating SLOs during unpredictable load spikes. Besides traditional model-horizontal scaling, our model-vertical scaling further reduces cost by upgrading to a variant that supports higher throughput. Thus, INFaaS matches the throughput of the optimal solution, while the deviance from the ILP is bounded by the difference between the optimal cost and the cost of replicating running variants. Our greedy heuristic reacts to load changes (e.g., load spikes) within sub-second response time while reducing the cost over multiple scaling actions.\n\n\nVM-Autoscaler at controller\n\nIn addition to model-level scaling, INFaaS also scales the worker machines for deploying variants. Following the mechanisms used in existing systems [11,20,25,35,44], the VM-Autoscaler decides when to bring a worker up/down: 1. When the utilization of any hardware resource exceeds a configurable threshold across all workers, the VM-Autoscaler adds a new worker with the corresponding hardware resource. We empirically set the threshold to 80%, considering the time to instantiate VMs (20-30 seconds): a lower threshold triggers scaling too quickly and unnecessarily adds workers; a higher value may not scale in time. 2. When variants on a particular hardware platform (e.g., GPU) are in the Interfered state across all workers, the VM-Autoscaler adds a worker with that hardware resource. 3. When more than 80% of workers have Overloaded variants, the VM-Autoscaler starts a new worker. To improve utilization, INFaaS dispatches requests to workers using an online bin packing algorithm [64].\n\n\nImplementation\n\nWe implemented INFaaS in about 20K lines of C++ code 1 .\n\nINFaaS' API and communication logic between controller and workers are implemented using gRPC in C++ [2]. Developers can interact with INFaaS by issuing gRPC requests in languages supported by gRPC, such as Python, Go, and Java. INFaaS uses AWS S3 [10] for its Model Repository. The model-variant selection policy is implemented as an extensible C++ library that is linked into the controller's Dis-1 https://github.com/stanford-mast/INFaaS patcher and worker's Model-Autoscaler. getVariant is a virtual method, and can be overridden to add new algorithms.\n\nOn the controller machine, the Front-End, Dispatcher, and Model Registrar are threads of the same process for efficient query dispatch. The Dispatcher is multi-threaded to support higher query traffic. The VM-Autoscaler is a separate process, that polls system status periodically. We swept the polling interval between 0.5-5 seconds at 0.5 second increments (similar to prior work [51,63]), and arrived at a 2 seconds polling interval. Longer intervals did not scale up fast enough, especially during load spikes, and shorter intervals were too frequent given VM start-up latencies.\n\nOn worker machines, the Dispatcher and monitoring daemon run as separate processes. Every 2 seconds, the monitoring daemon updates compute and memory utilization of the worker, loading, and average inference latencies, along with the current state (as noted in Figure 4) for each variant running on that worker, to the Metadata Store. We deployed custom Docker containers for PyTorch and Inferentia variants, and leveraged Triton Inference Server-19.03 [6] to support TensorRT, Caffe2, and TensorFlow variants on GPU. We used the TensorFlow Serving container for TensorFlow variants on CPU [5]. The Model-Autoscaler's main thread makes scaling decisions periodically. We swept the same range (0.5-5 seconds at 0.5 second increments) as the VM-Autoscaler, and arrived at a 1 second polling interval. The interval is shorter than the VM-Autoscaler's polling interval as model loading latencies are shorter than VM start-up latencies. The main thread also manages a thread pool for asynchronously loading and unloading model-variants. To tune slack-threshold, we explored values between 1.01 and 1.1 [31], and set it to 1.05. In our setup, lower thresholds did not scale variants fast enough to meet load changes, while higher thresholds scaled variants too quickly.\n\nWe built the Variant-Generator using TensorRT [3] and Neuron [15]; it is extensible to other similar frameworks [22,59]. For each variant, the Variant-Profiler records the latency for batch sizes from 1 to 64 (power of two increments). For natural language processing models, we record the latencies of varying sentence lengths for each of these batch sizes.\n\nWe built the Metadata Store using Redis [62] and the Redox C++ library [4]. The Metadata Store uses hash maps and sorted sets for fast metadata lookups that constitute the majority of its queries. Per-application, each model-variant instance's state is encoded as a {variant, worker} pair that can be efficiently queried by the controller and worker.\n\n\nEvaluation\n\nWe first compare INFaaS with all of its optimizations and features to existing systems (Section 6.1). To further demonstrate the effectiveness of INFaaS' design decisions and optimizations, we evaluate its individual aspects: model-variant selection, scaling (Section 6.2), and SLO-aware resource sharing (Section 6.3). Finally, we quantify the overheads of INFaaS'   Table 6: Model architectures, tasks, and associated variants. throughput (7 QPS) when setting the SLO to 1 second. For the same SLO, Clipper + was able to achieve 10 QPS. As prior work has noted [66], Clipper's adaptive batching is insufficient for maintaining a high QPS, because it relies on an external scheduler to allocate resources for it. Since Clipper + benefits from INFaaS' resource allocation and management, variant performance degradation detection and mitigation, and variant optimizations, we use Clipper + in the place of Clipper for the remainder of our evaluation.\n\nSageMaker vs SM + . We also validated that the latency and throughput of CPU, GPU, and Inferentia variants with SM + closely match SageMaker, while offering the benefits outlined for Clipper + . Thus, we use SM + in the place of SageMaker as our baseline.\n\nModel-variants. Guided by the MLPerf Inference benchmark [61], we collected a variety of models. Table 6 shows the 8 model families (22 architectures) and the number of associated variants. Our models are pre-trained using Caffe2, TensorFlow, and PyTorch. Our classification models are pretrained on ImageNet [30]; translation models are pre-trained on the WMT16 [70] English-German dataset. We generated 175 variants in total, differing in the frameworks (Caffe2, Ten-sorFlow, PyTorch), compilers (TensorRT, Neuron), batch sizes (1 to 64), and hardware platforms (CPU, GPU, Inferentia). Workloads. We evaluated using both synthetic and realworld application query patterns. For synthetic workloads, we used common patterns [29] indicating flat and fluctuating loads, with a Poisson inter-arrival rate [39,61]. For real-world inference workloads, we used the timing information from a Twitter trace from 2018 collected over a month [13] since there is no publicly available inference serving production traces. Twitter queries are likely passed through hate speech detection models [28] before being posted. Furthermore, as noted in recent work on inference serving [75], this trace resembles real inference workloads with both diurnal patterns and unexpected spikes (consistent with production serving workloads [65]). For each experiment, we randomly selected one day out of the month from the Twitter trace. We also set the accuracy such that it can always be satisfied by the registered variants; INFaaS' handling of infeasible accuracy requirements is discussed in Section 4.1.\n\n\nINFaaS with production workload\n\nWe now show that through model selection, resource allocation, and autoscaling mechanisms, INFaaS improves the throughput, cost, utilization, and reduces SLO violations. Experimental setup. We mapped the Twitter trace to a range between 10 and 1K QPS for a total of 113,420 batch-1 queries.  [17].\n\nResults and discussion. Figure 5 shows   pre-loads and persists 2 instances of the TensorFlow CPU variant. Clipper + GPU persists one TensorRT variant optimized for batch size of 8, configured to serve the provided peak load by adaptive batching. SM + CPU horizontally scales the CPU variant. SM + GPU horizontally scales a batch-1 optimized TensorRT variant (the cheapest GPU variant). We measured throughput every 2 seconds, and calculated the total cost. The cost for a running variant instance is estimated based on AWS EC2 pricing [17], proportional to its memory footprint. We normalize cost to 0.031 per GB/s for CPU, 0.190 per GB/s for Inferentia, and 0.498 per GB/s for GPU. Workloads. We used three query patterns that are commonly observed in real-world setups [29]: (a) a flat, low load (4 QPS), (b) a steady, high load (slowly increase from 650 to 700 QPS), and (c) a fluctuating load (ranging between 4 and 80 QPS). Patterns (a) and (b) represent ideal cases for baselines, as they statically choose a variant; we used the most cost-effective CPU/GPU variant for each baseline. Results and discussion. Figures 6a and 6d show the throughput and total cost, respectively, for INFaaS and the baselines when serving a flat, low load. While all systems met this low throughput demand, Clipper + GPU and SM + GPU incurred high costs since they use GPUs. INFaaS automatically selected CPU variants as they met the demand, thus reducing cost by 21.6\u00d7 and 21.3\u00d7 compared to Clipper + GPU and SM + GPU , respectively. For a steady, high load (Figures 6b and 6e), the observed throughput of Clipper + CPU and SM + CPU (about 10 QPS) was significantly lower than the demand. INFaaS automatically selected the batch-8 GPU variant, and both INFaaS and Clipper + GPU met the throughput demand. While SM + GPU replicated to 2 batch-1 GPU variants to meet the load, it was 5% more expensive than INFaaS and served 15% fewer QPS. Finally, for a fluctuating load (Figures 6c and 6f) \n\n\nEffectiveness of sharing resources\n\nWe now show how INFaaS manages and shares accelerators across models without affecting performance of queries. We found that co-locating models on an Inferentia chip did not cause noticeable interference, since variants can run on separate cores on the chip. Thus, we focus on evaluating GPU sharing. INFaaS detects when model-variants enter the Overloaded/Interfered state, and either migrates the model to a different GPU, or scales to a new GPU worker if all existing variants on the GPUs are in the Overloaded/Interfered state. Experimental setup. We used the baseline of Clipper + with one model persisted on each GPU. Since Clipper + requires a pre-defined number of workers, we specified 2 GPU workers. For fairness, INFaaS started from one GPU and was allowed to scale up to 2 GPU workers. As noted in Section 2.3, the load at which sharing of GPUs starts affecting the performance negatively is different across models. We selected two model-variants that diverge in inference latency, throughput, and peak memory: Inception-ResNetV2 (large model) and MobileNetV1 (small model). Both variants are TensorRT-optimized for batch-1. We report throughput and P99 latency, measured every 30 seconds. Workloads. To show the impact of model popularity on re-source sharing, we evaluated a scenario with a popular model serving 80% QPS, and the other serving 20% QPS. We observed similar results with other popularity distributions or different models. We mapped the Twitter trace to a range between 50 and 500 QPS for a total of 75,000 batch-1 queries. Results and discussion. Figure 7 shows P99 latency and throughput for both models when Inception-ResNetV2 is popular. When Inception-ResNetV2 and MobileNetV1 exceeded their profiled latencies, INFaaS marked them as interfered around 30 and 50 seconds, respectively. INFaaS started a new GPU worker (\u223c30 seconds start-up latency), created an instance of each model on it, and spread the load for both models across the GPUs. The allocated resources for Inception-ResNetV2 with Clipper + were insufficient and led to a significant latency increase and throughput degradation. Unlike Clipper + , INFaaS could further mitigate the latency increase by adding more GPU workers (limited to two in this experiment). Moreover, INFaaS saved 10% cost compared to Clipper + by (a) bin-packing requests across models to one GPU at low load, and (b) only adding GPUs when contentions were detected.\n\n\nINFaaS' decision overhead\n\nOn the critical path of serving a query, INFaaS makes the following decisions: (a) selecting a model-variant and (b) selecting a worker. Table 7 shows the median latency of making these decisions for INFaaS and the speedup over a brute-force search. Each row corresponds to a query specifying (1) a registered model and (2) application requirements. For each query, we show the decision latency when the selected variant was in (a) Inactive, Overloaded, or Interfered state, and (b) Active state. These decisions are made using the model-variant selection policy (Section 4.1). When the registered model was explicitly specified, INFaaS incurred low overheads (\u223c1ms), as it needed to select only a worker. When the application requirements were provided, and the selected variant was not already loaded (State (a)), INFaaS selected a variant and the least-loaded worker for serving in 3.5ms. Otherwise, when the selected variant was already loaded (State (b)), INFaaS' decision latency for selecting the variant and a worker was 2.2ms.\n\nTo measure scalability, we varied the number of modelvariants from 10 to 166 (increments of 50); the latencies of  techniques [21,37]. If the main controller fails, the incoming queries are re-routed to a standby controller. Since the Metadata Store is on the same machine as the controller, it is a part of the replicated state. Query fairness. Using heterogeneous variants to serve queries means users may see different performance and accuracy results given the same query requirements, as INFaaS optimizes for cost-efficiency. However, INFaaS will always ensure the query requirements are met. INFaaS' API is extensible to support further requirements for improved query fairness (e.g., bounding performance/accuracy variation [77]).\n\nExplicitly controlling the runtime. INFaaS' model-less abstraction allows it to incorporate the ever-growing number of optimizers and runtimes. It also enables INFaaS' modelselection algorithms to be extended separately. Explicitly controlling the runtime may allow INFaaS to provide more of a clear-box approach to optimizing inference serving, but may limit its generality and extensibility (e.g., supporting limited hardware platforms).\n\n\nRelated Work\n\nInference serving systems. TensorFlow Serving [5] provided one of the first production environments for models trained using the TensorFlow framework. Clipper [25] generalized it to enable the use of different frameworks and application-level SLOs. Pretzel [52], Nexus [66], and Infer-Line [24] built upon Clipper for optimizing inference serving pipelines. SageMaker [11], AI Platform [35], and Azure ML [1] offer developers inference services that autoscale VMs based on load. Triton Inference Server [6] optimizes GPU inference serving, supports CPU models, but requires static model instance configuration. DeepRecSys [39] statically optimizes batching and hardware selection for recommender systems, but requires developers to specify a variant, manage and scale model resources as the load varies. Clockwork [38] reduces GPU inference latency variability by ordering queries based on their SLOs and only running one query at a time. Model-Switching [77] switches between models with different accuracies during load spikes, while preserving the fraction of correct predictions returned within an SLO. It assumes pre-loaded models and does not consider heterogeneous hardware resources. Tolerance Tiers [42] [3,12,22,72] perform optimizations, such as quantization and layer fusion, to improve latency and resource usage. However, developers still need to manually create and select variants, and manage the deployed variants. INFaaS uses these optimizers to create variants that can be used for meeting diverse application requirements, and automates model-variant selection for each query to minimize cost as load and resources vary. Scaling. Autoscale [33] reviewed scaling techniques and argued for a simple approach that maintains the right amount of slack resources while meeting SLOs. Similarly, INFaaS' autoscalers maintain headrooms and scale-down counters to cautiously scale resources. MArk [75] proposed SLO-aware model scheduling and scaling by using AWS Lambda to absorb unpredictable load bursts. Existing systems [1,11,35,37] only support VM-level and model-horizontal scaling, while INFaaS introduces model-vertical scaling that leverages multiple diverse variants. Sharing accelerators. NVIDIA MPS [57] enabled efficient sharing of GPUs that facilitated initial exploration into sharing GPUs for deep-learning. Existing systems [6,27,46,74] also explored how to share GPUs spatially, temporally, or both. NVIDIA's A100 GPUs support MIG [58]: hardware partitions and full isolation. AWS Inferentia supports spatial and temporal sharing via Neuron SDK [15]. INFaaS' current implementation builds on Triton Inference Server (GPUs) and Neuron SDK (Inferentia), and provides SLO-aware accelerator sharing. INFaaS can also be extended to leverage other mechanisms for sharing additional hardware resources.\n\n\nConclusion\n\nWe presented INFaaS: an automated model-less system for distributed inference serving. INFaaS' model-less interface allows application developers to specify high-level performance, cost or accuracy requirements for queries, leaving INFaaS to select and deploy the model-variant, hardware, and scaling configuration. INFaaS automatically provisions and manages resources for serving inference queries to meet their high-level goals. We demonstrated that INFaaS' modelvariant selection policy and resource sharing leads to reduced costs, better throughput, and fewer SLO violations compared to state-of-the-art inference serving systems.\n\nFigure 1 :\n1Impact of co-locating Inception-ResNetV2 and Mo-bileNetV1 on a V100 GPU. Both variants are TensorRT, batch-1, FP16. Graphs show average latency and throughput for each model running alone vs sharing, subjected to the same (QPS).\n\nFigure 2 :Figure 3 :\n23Registering models and submitting queries with INFaaS. Architecture of INFaaS. Modules in boxes with dashed border are on the critical path of serving queries. All other modules do not impact serving latency. Numbered circles correspond to the typical life-cycle of queries.\n\n\nINFaaS uses the validation set submitted by the developer to calculate the accuracy of the newly generated variants; INFaaS records this information in the Metadata Store. The Variant-Generator does not train or produce new model architectures: variants are generated only from registered models. To help model-variant selection, the Variant-Profiler conducts one-time profiling for each variant where it measures statistics, such as the loading and inference latencies, and peak memory utilization. These parameters, along with the corresponding appID, accuracy, and maximum supported batch size are recorded in the Metadata Store. After profiling, a variant is saved in the Model Repository. The total profiling time for all generated variants from a submitted model is a few minutes on a single VM with the variant's target hardware. This profiling cost will be amortized over long-term serving time in production settings. Model-Variant Selection Policy. INFaaS invokes the model-variant selection policy in two cases. (Case I) On arrival of a query: The controller's Dispatcher uses the policy to select a variant for each incoming query. This model-variant selection lies on the critical path of serving each query. To reduce the latency of decision-making, we designed an efficient variant search algorithm (Section 4.1). (Case II) On changes in query load: As the query load changes, the worker's Model-Autoscaler uses the policy to determine whether to replicate existing variants, or vertically scale to a different variant. The Model-Autoscaler monitors the incoming query load and the current throughput of INFaaS to detect the need for scaling. If a change is detected, the Model-Autoscaler invokes the policy in the background to select a suitable scaling strategy (Section 4.2).\n\nFigure 4 :\n4State machine capturing the dynamically changing status of model-variants.\n\n\n-Autoscaler at each worker To react to the changes in query load, INFaaS' Model-Autoscaler needs to decide the type and number of model-variants to use while minimizing the cost of running the variants. To figure out the type and number of model-variants needed, we formulated the following integer linear program (ILP) that decides a scaling action (replicate, upgrade, or downgrade) for each variant. This ILP minimizes the total cost of scaling actions for all the variants to meet the incoming query load. Formulation. For an application, the outcome (optimization variable) of our ILP is the optimal scaling action, \u03b4 i j , for each model-variant v i j , variant j of model architecture i. \u03b4 i j is an integer that captures the scaling action as follows: (a) A positive value denotes loading instances of the variant, (b) a negative value denotes unloading instances of this variant, and (c) a value of zero denotes no scaling needed. For a variant v i j that is already loaded, a positive value of \u03b4 i j indicates a replicate action. A positive value of \u03b4 i j for a variant v i j that is not already loaded indicates an upgrade or downgrade action depending on the hardware cost of v i j .\n\n\na) Q i j : the saturation QPS of variant v i j , (b) N i j : the number of running instances of variant v i j , (c) L: the incoming query load, (d) slack: configurable headroom to absorb sudden load spikes, (e) T inf i j : the inference latency of variant v i j , (f) T load i j : the loading latency of variant v i j , (g) S: SLO of the considered application, (h) R type i j : the resource requirements of variant v i j , for a resource type (CPU cores, CPU memory, GPU memory, number of Inferentia cores), and (i) R type total : the total available amount of resources of a type (CPUs, GPUs, Inferentia cores) on the underlying worker machine.\n\n6. 2\n2Selecting and scaling model-variants Next, we show the efficiency of INFaaS' model-variant selection policy to select and vertically scale the variants. Experimental setup. To compare INFaaS with common configurations developers would choose today, we considered two cases for a model: only GPU variants are used (Clipper + GPU , SM + GPU ) and only CPU variants are used (Clipper + CPU , SM + CPU ). We used variants derived from one model architecture, ResNet50, and one worker. Clipper +\n\nFigure 6 :\n6Throughput (top) and cost (bottom), with ResNet50 and batch-1 requests. INFaaS reduced cost and met the load.\n\nFigure 7 :\n7Performance of co-locating GPU model-variants when 80% of queries are served by Inception-ResNetV2.(60-90 and 150-180 seconds), INFaaS upgraded to an Inferentia batch-1 variant. Hence, on average, INFaaS was 3\u00d7 cheaper than SM + GPU and Clipper + GPU . If INFaaS were limited to CPU and GPU variants, it would still save 1.7\u00d7 cost over both the baselines. Similarly, even if we allowed baselines to use Inferentia, INFaaS would still save 1.9\u00d7 cost because baselines cannot dynamically switch between Inferentia and CPU variants. Figures 6a -6c indicate that a single variant is neither the most cost-effective, nor the most performant for all scenarios. INFaaS achieves ease-of-use while (a) matching the baselines' performance and cost in ideal cases (steady loads), and (b) outperforming the baselines during load changes. Thus, leveraging variants optimized for different hardware through model-vertical scaling, INFaaS is able to adapt to changes in load and query patterns, and improve cost by up to 21.6\u00d7 (10\u00d7 on average).\n\n\nVariant (hardware, framework) Lat. (ms) Req/s Cost ($/s)A (4 CPUs, TensorFlow) \n200 \n5 \n1 \nB (1 Inferentia core, Neuron) \n20 \n100 \n3 \nC (1 V100 GPU, TensorRT) \n15 \n800 \n16 \n\n\n\nTable 2 :\n2Latency, saturation throughput, and normalized cost (based on AWS pricing) for three ResNet50 variants.QPS SLO (ms) #Var. A #Var. B #Var. C Cost ($/s) \n\n10 \n300 \n2 \n0 \n0 \n2 \n10 \n50 \n0 \n1 \n0 \n3 \n1000 300 \n0 \n2 \n1 \n22 \n\n\n\nTable 3 :\n3Cheapest configuration (in #instances) of variants fromTable 2to meet the QPS and SLO; last column shows total cost.\n\nTable 4 :\n4INFaaS' declarative developer API.formance and cost requirements while improving utilization \nof resources (Section 4.2). INFaaS introduces model-vertical \nautoscaling that, through model selection, upgrades or down-\ngrades to a differently optimized model-variant by leverag-\ning the diversity of model-variants (Section 4.2.1). INFaaS \nefficiently maintains static and dynamic profiles of model-\nvariants and hardware resources to support low latencies for \nselecting and scaling model-variants (Sections 3.2 and 4). \nFinally, INFaaS features the model-variant selection policy \ndescribed in Section 4, but allows developers to extend and \ncustomize it. \n\n\n\nTable 4 lists\n4INFaaS' model-less API.\n\n\nlowest-util worker, W lu , with inactiveVariant's HW Maintaining the state machine. Each model-variant instance's state machine is maintained by the worker's monitoring daemons and is organized in the Metadata Store for fast access. This enables INFaaS' Dispatcher to use the modelvariant selection policy for serving queries on the order of hundreds of \u00b5s to ms (assessed further in Section 6.4). State machine implementation details are described in Section 5.4.1 Case I: On arrival of a queryWhen a query arrives, INFaaS' Dispatcher invokes the getVariant method of model-variant selection policy to choose a variant (Algorithm 1). For this case, the input to getVariant is the query's requirements, and the output is the variant and worker to serve the query. getVariant first checks whether any variants in the Active state match the query's requirements (Line 2). Variants in Active state do not incur a loading latency. If such a variant is found, INFaaS dispatches the query to the least-loaded worker running the7: \nreturn inactiveVariant, W lu \n8: \nreturn suggestVariant(appID,accuracy,latency) \n\nvariant instance (Lines 3-4). Otherwise, INFaaS considers \nvariants in the Inactive state: getVariant first enquires the \nMetadata Store and retrieves the variant with the lowest com-\nbined loading and inference latency that matches the query's \nrequirements (Line 5). If such a variant is found, INFaaS \nsends the query to the worker with the lowest utilization on \nthe variant's target hardware (Lines 6-7). Otherwise, since \nno registered or generated variant can meet the developer's \nrequirements, INFaaS suggests a variant that can achieve the \nclosest target accuracy and/or latency (Line 8). We assess the \nefficiency of this policy over brute-force search in Section 6.4. \n\nMitigating performance degradation. For better resource \nutilization, INFaaS co-locates variants on hardware resources; \nas a result, they may interfere and cause SLO violations. To \nprevent such violations, INFaaS avoids selecting variants that \nare in the Interfered or Overloaded state. For interfered vari-\nants, INFaaS triggers a mitigation process in the background \nto avoid affecting the performance of online serving. If there \nare idle resources available on the same worker, INFaaS mi-\ngrates the variant in the Interfered state to the available re-\nsources (e.g., a different set of cores). This avoids the need to \nfetch a variant from the model repository. If no resources are \navailable for loading the variant on the worker, the worker asks \nthe controller's Dispatcher to place the variant on the least-\nloaded worker. For variants in the Overloaded state, INFaaS' \nModel-Autoscaler assesses whether it is more cost-effective \nto scale to a different variant (see Section 4.2.1). \n\nExtensibility. The state machine and model-variant selec-\ntion policy are extensible. For instance, getVariant can be \nextended to prioritize particular variants in the Active state \n(e.g., prefer least power-hungry variants). \n\n\n\n\nlevel, horizontal autoscaling, we introduce model-vertical scaling: change (upgrade or downgrade) to a differently optimized model-variant, thus leveraging the diversity of modelvariants. INFaaS' autoscaling is a joint effort between the workers and the controller. Each worker hosts a Model-Autoscaler that consults with the model-variant selection policy to make model-level autoscaling decisions (Sections 4.2.1, and 4.2.2). The controller hosts a VM-Autoscaler that makes VM-level autoscaling decisions (Section 4.2.3).are agnostic to the diversity of model-\nvariants, and only replicate a statically fixed (developer-\nspecified) model-variant for all the queries of an application. \nHowever, as discussed in Section 2.2, autoscaling that repli-\ncates the same model-variant alone is not enough because: (a) \nthe right model-variant changes with load and (b) the required \nresources might not be available to replicate a specific variant. \nFor INFaaS' autoscaling, in addition to using traditional \nVM-\n\nTable 5 :\n5Comparison between INFaaS and the baselines. INFaaS' features and optimizations, including: support for model graph optimizations, and INFaaS' detection and mitigation of variant performance degradation. Clipper vs Clipper + . To validate our baseline configurations through INFaaS, we evaluated Clipper + against the open-source Clipper deployment (Clipper) [23] with its adaptive batching and prediction caching features enabled.We deployed two ResNet50 TensorFlow CPU instances for each. For Clipper, we swept its adaptive batching SLO from 500ms to 10 seconds, and found it achieved its maximumdecision-making (Section 6.4). We begin by describing the \nexperimental setup common across all experiments, the base-\nlines, the model-variants, and the workloads. \nExperimental setup. We deployed INFaaS on a heteroge-\nneous cluster of AWS EC2 [9] instances. We hosted the con-\ntroller on an m5.2xlarge instance (8 vCPUs, 32GiB DRAM), \nand workers on inf1.2xlarge (8 vCPUs, 16GiB DRAM, \none AWS Inferentia), p3.2xlarge (8 vCPUs, 61GiB DRAM, \none NVIDIA V100 GPU), and m5.2xlarge instances. All \ninstances feature Intel Xeon Platinum 8175M CPUs operat-\ning at 2.50GHz, Ubuntu 16.04 with 4.4.0 kernel, and up to \n10Gbps networking speed. \nBaselines. To the best of our knowledge, no existing system \nprovides a model-less interface like INFaaS; state-of-the-art \nserving systems require developers to specify the variant and \nhardware. For a fair comparison, we configured INFaaS to \nclosely resemble the resource management, autoscaling tech-\nniques, and APIs of existing systems, including TensorFlow \nServing [5] (TFS), Triton Inference Server (TIS) [6], Clip-\nper [25], AWS SageMaker [11] (SM), and Google AI Plat-\nform [35]. Specifically, we compared INFaaS to the following \nbaseline configurations for query execution: \n\u2022 Clipper + : Derived from TFS, TIS, and Clipper, this base-\nline pre-loads model-variants, and requires developers to \nset a pre-defined number of variant instances. Thus, we \nset the number of variant instances such that Clipper + \nachieves the highest performance given available resources. \n\u2022 SM + : Derived from SageMaker and AI Platform, this base-\nline scales each model-variant horizontally, but does not \nsupport model-vertical scaling that INFaaS introduces. \nTable 5 lists the differences between baselines and INFaaS. \nConfiguring the baselines with INFaaS (a) allowed for a fair \ncomparison by removing variabilities in execution environ-\nments (e.g., RPC and container technologies), and (b) enabled \nus to evaluate our design decision individually by giving the \nbaselines access to Model Family (Task) \n#Vars Model Family (Task) \n#Vars \nMobileNet (classification) 13 \nVGG (classification) \n30 \nAlexNet (classification) \n9 \nInception (classification) 25 \nDenseNet (classification) \n22 \nNasNet (classification) \n6 \nResNet (classification) \n61 \nGNMT (translation) \n9 \n\n\n\n\nINFaaS achieved 1.1\u00d7 and 1.3\u00d7 higher throughput, and 1.63\u00d7 and 2.54\u00d7 fewer SLO violations compared to Clipper + and SM + , respectively.INFaaS scaled models both horizontally and vertically: it up-\ngraded to Inferentia or GPU (higher batch) variants when \nneeded. In reaction to the increased load, INFaaS added a 3 rd \nInferentia worker at 40 seconds. Although SM + scales variants \nhorizontally, it achieved lower throughput and violated more \nSLOs due to frequently incurring variant loading penalties \nand being unable to upgrade variants. By leveraging variants \nthat span heterogeneous hardware (CPU, GPU, Inferentia), \nINFaaS achieved 1.23\u00d7 lower cost, while keeping SLO vi-\nolations under 4% on average. INFaaS also load-balanced \nrequests and mitigated overloaded or interfered variants. This \nresulted in an average worker utilization of 48.9%, with an \naverage GPU DRAM utilization of 58.6%. The latter is 5.6\u00d7 \nand 2.8\u00d7 higher than SM + and Clipper + , respectively. \nINFaaS achieved higher performance (1.3\u00d7 higher \nthroughput) and resource utilization (5.6\u00d7 higher GPU uti-\nlization), and lower SLO violations (2\u00d7 lower) and cost \n(1.23\u00d7 lower) compared to the baselines. \n\n\n\n\n, INFaaS, Clipper + GPU , and SM + GPU met the throughput demand, while both SM + CPU and Clipper + CPU served only 10 QPS. During low load periods, INFaaS selected a CPU variant. At load spikes\n\nTable 7\n7remain unchanged as the number of variants increases. This result was expected, since INFaaS' Metadata Store uses constant access time data structures.INFaaS keeps low overheads across its query submission modes: up to 44\u00d7 (35.5\u00d7 on average) faster than brute-force.Failure Recovery. INFaaS' VM-Autoscaler detects worker failures using RPC heartbeats, and starts a new worker with the state of the failed worker stored in the Metadata Store. For fault-tolerance, the controller is replicated using existing7 Discussion \n\n\n\nTable 7 :\n7Median latency and speedup of making variant and worker selection decisions across 3 runs. State (a): variants are in the Inactive state, Overloaded state, or Interfered state. State (b): variants are in the Active state.\n\n\nallows developers to programmatically trade accuracy off for latency. None of these existing systems offer a simple model-less interface, like INFaaS, to navigate the variant search space on developers' behalf, or dynamically leverage model-variants to meet applications' diverse requirements. However, prior work can be seen as complementary to INFaaS; e.g., INFaaS can adopt DeepRecSys' recommender system optimizations and Clockwork's predictable DNN worker. Model-variant generators. Model graph optimizers\nJuly 14-16, 2021 978-1-939133-23-6 Open access to the Proceedings of the 2021 USENIX Annual Technical Conference is sponsored by USENIX. INFaaS: Automated Model-less Inference Serving Francisco Romero, Qian Li, Neeraja J. Yadwadkar, and Christos Kozyrakis, Stanford University https://www.usenix.org/conference/atc21/presentation/romero\nAcknowledgmentsWe thank our shepherd, Sangeetha Abdu Jyothi, and the anonymous reviewers for their helpful feedback. We thank Honglin Yuan, Hilal Asi, Peter Kraft, Matei Zaharia, John Wilkes, and members of the MAST research group for their insightful discussions to improve this work. This work was supported by the Stanford Platform Lab and its industrial affiliates, the SRC Jump program (CRISP center), and Huawei.\nAzure Machine Learning. Azure Machine Learning, 2018. https://docs.microsoft.com/ en-us/azure/machine-learning/.\n\n. gRPC. gRPC, 2018. https://grpc.io/.\n\n. NVIDIA TensorRT: Programmable Inference Accelerator. NVIDIA TensorRT: Programmable Inference Accelerator, 2018. https: //developer.nvidia.com/tensorrt.\n\n. Redox, Redox, 2018. https://github.com/hmartiro/redox.\n\nTensorFlow Serving for model deployment in production. TensorFlow Serving for model deployment in production, 2018. https: //www.tensorflow.org/serving/.\n\n. NVIDIA Triton Inference Server. NVIDIA Triton Inference Server, 2020. https://github.com/ triton-inference-server/server.\n\nCherrypick: Adaptively unearthing the best cloud configurations for big data analytics. Omid Alipourfard, Harry Hongqiang, Jianshu Liu, Shivaram Chen, Minlan Venkataraman, Ming Yu, Zhang, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationOmid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang. Cherrypick: Adaptively unearthing the best cloud configurations for big data analytics. In 14th USENIX Symposium on Networked Systems Design and Implementa- tion (NSDI 17), pages 469-482, Boston, MA, March 2017. USENIX Association.\n\n. Ec2 Amazon, Amazon EC2. https://aws.amazon.com/ec2/, 2018.\n\n. S3 Amazon, Amazon S3. https://aws.amazon.com/s3/, 2018.\n\n. Amazon Sagemaker, Amazon SageMaker. https://aws.amazon.com/sagemaker/, 2018.\n\n. Amazon Sagemaker Neo, Amazon SageMaker Neo. https://aws.amazon.com/sagemaker/ neo/, 2018.\n\nTwitter Streaming Traces. Twitter Streaming Traces. https://archive.org/details/ archiveteam-twitter-stream-2018-04, 2018.\n\nMultilingual multi-class sentiment classification using convolutional neural networks. Mohammed Attia, Younes Samih, Ali Elkahky, Laura Kallmeyer, Miyazaki, JapanMohammed Attia, Younes Samih, Ali Elkahky, and Laura Kallmeyer. Multilingual multi-class sentiment classification using convolutional neural networks. pages 635-640, Miyazaki, Japan, 2018.\n\n. AWS Neuron. AWS Neuron. https://github.com/aws/aws-neuron-sdk.\n\nDeliver high performance ML inference with AWS Inferentia. Deliver high performance ML inference with AWS Inferentia. https://d1.awsstatic.com/events/reinvent/2019/REPEAT_ 1_Deliver_high_performance_ML_inference_with_AWS_ Inferentia_CMP324-R1.pdf.\n\n. Aws Ec2 Pricing, AWS EC2 Pricing. https://aws.amazon.com/ec2/pricing/ on-demand/, 2018.\n\n. Aws Inferentia, AWS Inferentia. https://aws.amazon.com/machine-learning/ inferentia/, 2018.\n\nBenchmark analysis of representative deep neural network architectures. Simone Bianco, Remi Cadene, Luigi Celona, Paolo Napoletano, IEEE Access. 6Simone Bianco, Remi Cadene, Luigi Celona, and Paolo Napoletano. Benchmark analysis of representative deep neural network architectures. IEEE Access, 6:64270-64277, 2018.\n\n. Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, John Wilkes, Kubernetes Borg, Queue, 1410Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, and John Wilkes. Borg, omega, and kubernetes. Queue, 14(1):10, 2016.\n\nConfluxdb: Multi-master replication for partitioned snapshot isolation databases. Prima Chairunnanda, Khuzaima Daudjee, M Tamer, \u00d6zsu, PVLDB. 7Prima Chairunnanda, Khuzaima Daudjee, and M. Tamer \u00d6zsu. Con- fluxdb: Multi-master replication for partitioned snapshot isolation databases. PVLDB, 7:947-958, 2014.\n\nTVM: An automated end-to-end optimizing compiler for deep learning. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: An automated end-to-end optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 578-594, Carlsbad, CA, 2018. USENIX Association.\n\nInferline: latency-aware provisioning and scaling for prediction serving pipelines. Daniel Crankshaw, Gur-Eyal, Xiangxi Sela, Corey Mo, Ion Zumar, Joseph Stoica, Alexey Gonzalez, Tumanov, Proceedings of the 11th ACM Symposium on Cloud Computing. the 11th ACM Symposium on Cloud ComputingDaniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion Sto- ica, Joseph Gonzalez, and Alexey Tumanov. Inferline: latency-aware provisioning and scaling for prediction serving pipelines. In Proceed- ings of the 11th ACM Symposium on Cloud Computing, pages 477-491, 2020.\n\nClipper: A low-latency online prediction serving system. Daniel Crankshaw, Xin Wang, Giulio Zhou, Michael J Franklin, Joseph E Gonzalez, Ion Stoica, 14th USENIX Symposium on Networked Systems Design and Implementation. Boston, MA, USADaniel Crankshaw, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper: A low-latency online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2017, Boston, MA, USA, March 27-29, 2017, pages 613-627, 2017.\n\nHydra: a federated resource manager for data-center scale analytics. Carlo Curino, Subru Krishnan, Konstantinos Karanasos, Sriram Rao, Giovanni M Fumarola, Botong Huang, Kishore Chaliparambil, Arun Suresh, Young Chen, Solom Heddaya, Roni Burd, Sarvesh Sakalanaga, Chris Douglas, Bill Ramsey, Raghu Ramakrishnan, 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19). Boston, MAUSENIX AssociationCarlo Curino, Subru Krishnan, Konstantinos Karanasos, Sriram Rao, Giovanni M. Fumarola, Botong Huang, Kishore Chaliparambil, Arun Suresh, Young Chen, Solom Heddaya, Roni Burd, Sarvesh Sakalanaga, Chris Douglas, Bill Ramsey, and Raghu Ramakrishnan. Hydra: a federated resource manager for data-center scale analytics. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19), pages 177-192, Boston, MA, February 2019. USENIX Association.\n\nTrims: Transparent and isolated model sharing for low latency deep learning inference in function as a service environments. Abdul Dakkak, Cheng Li, Simon Garcia De Gonzalo, Jinjun Xiong, Wen-Mei W Hwu, abs/1811.09732CoRRAbdul Dakkak, Cheng Li, Simon Garcia De Gonzalo, Jinjun Xiong, and Wen-Mei W. Hwu. Trims: Transparent and isolated model shar- ing for low latency deep learning inference in function as a service environments. CoRR, abs/1811.09732, 2018.\n\nAutomated Hate Speech Detection and the Problem of Offensive Language. Thomas Davidson, Dana Warmsley, Michael Macy, Ingmar Weber, Proceedings of the Eleventh International AAAI Conference on Web and Social Media. the Eleventh International AAAI Conference on Web and Social MediaICWSM 2017Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated Hate Speech Detection and the Problem of Offensive Lan- guage. In Proceedings of the Eleventh International AAAI Conference on Web and Social Media (ICWSM 2017), 2017.\n\nQuasar: Resourceefficient and qos-aware cluster management. Christina Delimitrou, Christos Kozyrakis, Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '14. the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '14New York, NY, USAACMChristina Delimitrou and Christos Kozyrakis. Quasar: Resource- efficient and qos-aware cluster management. In Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '14, pages 127-144, New York, NY, USA, 2014. ACM.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Kai Li Jia Li, Li Li, Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database. In In CVPR, 2009.\n\nJockey: Guaranteed job latency in data parallel clusters. Andrew D Ferguson, Peter Bodik, Srikanth Kandula, Eric Boutin, Rodrigo Fonseca, Proceedings of the 7th ACM European Conference on Computer Systems, EuroSys '12. the 7th ACM European Conference on Computer Systems, EuroSys '12New York, NY, USAACMAndrew D. Ferguson, Peter Bodik, Srikanth Kandula, Eric Boutin, and Rodrigo Fonseca. Jockey: Guaranteed job latency in data parallel clusters. In Proceedings of the 7th ACM European Conference on Computer Systems, EuroSys '12, pages 99-112, New York, NY, USA, 2012. ACM.\n\nA configurable cloud-scale dnn processor for real-time ai. Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Massengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Logan Adams, Mahdi Ghandi, Stephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz, Lisa Woods, Sitaram Lanka, Steven K Reinhardt, Adrian M Caulfield, Eric S Chung, Doug Burger, Proceedings of the 45th Annual International Symposium on Computer Architecture, ISCA '18. the 45th Annual International Symposium on Computer Architecture, ISCA '18Piscataway, NJ, USAIEEE PressJeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Mas- sengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman, Lo- gan Adams, Mahdi Ghandi, Stephen Heil, Prerak Patel, Adam Sapek, Gabriel Weisz, Lisa Woods, Sitaram Lanka, Steven K. Reinhardt, Adrian M. Caulfield, Eric S. Chung, and Doug Burger. A config- urable cloud-scale dnn processor for real-time ai. In Proceedings of the 45th Annual International Symposium on Computer Architecture, ISCA '18, pages 1-14, Piscataway, NJ, USA, 2018. IEEE Press.\n\nAutoscale: Dynamic, robust capacity management for multitier data centers. Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, Michael A Kozuch, ACM Transactions on Computer Systems (TOCS). 30414Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, and Michael A Kozuch. Autoscale: Dynamic, robust capacity management for multi- tier data centers. ACM Transactions on Computer Systems (TOCS), 30(4):14, 2012.\n\n. R Michael, David S Garey, Johnson, Computers and Intractability. Michael R. Garey and David S. Johnson. Computers and Intractability;\n\nA Guide to the Theory of NP-Completeness. W. H. Freeman & CoUSAA Guide to the Theory of NP-Completeness. W. H. Freeman & Co., USA, 1990.\n\n. Google Cloud, A I Platform, Google Cloud AI Platform. https://cloud.google.com/ ai-platform/, 2018.\n\nTiresias: A GPU cluster manager for distributed deep learning. Juncheng Gu, Mosharaf Chowdhury, G Kang, Yibo Shin, Myeongjae Zhu, Junjie Jeon, Hongqiang Qian, Chuanxiong Liu, Guo, 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19). Boston, MAUSENIX AssociationJuncheng Gu, Mosharaf Chowdhury, Kang G. Shin, Yibo Zhu, Myeong- jae Jeon, Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. Tiresias: A GPU cluster manager for distributed deep learning. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19), pages 485-500, Boston, MA, 2019. USENIX Association.\n\nSwayam: distributed autoscaling to meet slas of machine learning inference services with resource efficiency. Arpan Gujarati, Sameh Elnikety, Yuxiong He, S Kathryn, Mckinley, B Bj\u00f6rn, Brandenburg, Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference. the 18th ACM/IFIP/USENIX Middleware ConferenceACMArpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S McKinley, and Bj\u00f6rn B Brandenburg. Swayam: distributed autoscaling to meet slas of machine learning inference services with resource efficiency. In Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference, pages 109-120. ACM, 2017.\n\nServing DNNs like Clockwork: Performance Predictability from the Bottom Up. Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, Jonathan Mace, 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20). USENIX AssociationArpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf- mann, Ymir Vigfusson, and Jonathan Mace. Serving DNNs like Clock- work: Performance Predictability from the Bottom Up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 443-462. USENIX Association, November 2020.\n\nUdit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon, Wei, S Hsien-Hsin, David Lee, Carole-Jean Brooks, Wu, DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference. Udit Gupta, Samuel Hsia, Vikram Saraph, Xiaodong Wang, Brandon Reagen, Gu-Yeon Wei, Hsien-Hsin S. Lee, David Brooks, and Carole- Jean Wu. DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference, 2020.\n\nThe Architectural Implications of Facebook's DNN-Based Personalized Recommendation. Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Bran- don Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, et al. The Architectural Implications of Face- book's DNN-Based Personalized Recommendation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 488-501, Feb 2020.\n\n. LLC Gurobi Optimization. Gurobi Optimizer Reference Manual. LLC Gurobi Optimization. Gurobi Optimizer Reference Manual, 2020.\n\nOne size does not fit all: Quantifying and exposing the accuracy-latency trade-off in machine learning cloud service apis via tolerance tiers. M Halpern, B Boroujerdian, T Mummert, E Duesterwald, V Reddi, Proceedings of the 19th International Symposium on Performance Analysis of Systems and Software (ISPASS). the 19th International Symposium on Performance Analysis of Systems and Software (ISPASS)M. Halpern, B. Boroujerdian, T. Mummert, E. Duesterwald, and V. Reddi. One size does not fit all: Quantifying and exposing the accuracy-latency trade-off in machine learning cloud service apis via tolerance tiers. In Proceedings of the 19th International Symposium on Performance Analysis of Systems and Software (ISPASS), 2019.\n\nApplied machine learning at facebook: A datacenter infrastructure perspective. Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, Xiaodong Wang, Proceedings of the 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), HPCA '18. the 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), HPCA '18IEEEKim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, and Xiaodong Wang. Applied ma- chine learning at facebook: A datacenter infrastructure perspective. In Proceedings of the 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), HPCA '18. IEEE, 2018.\n\nMesos: A platform for fine-grained resource sharing in the data center. Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D Joseph, Randy Katz, Scott Shenker, Ion Stoica, Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI'11. the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI'11USAUSENIX AssociationBenjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, An- thony D. Joseph, Randy Katz, Scott Shenker, and Ion Stoica. Mesos: A platform for fine-grained resource sharing in the data center. In Pro- ceedings of the 8th USENIX Conference on Networked Systems Design and Implementation, NSDI'11, page 295-308, USA, 2011. USENIX Association.\n\nFocus: Querying large video datasets with low latency and low cost. Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B Gibbons, Onur Mutlu, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationKevin Hsieh, Ganesh Ananthanarayanan, Peter Bodik, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B. Gibbons, and Onur Mutlu. Focus: Querying large video datasets with low la- tency and low cost. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 269-286, Carlsbad, CA, October 2018. USENIX Association.\n\nDynamic spacetime scheduling for gpu inference. Paras Jain, Xiangxi Mo, Ajay Jain, Harikaran Subbaraj, Rehan Durrani, Alexey Tumanov, Joseph Gonzalez, Ion Stoica, LearningSys Workshop at Neural Information Processing Systems. Paras Jain, Xiangxi Mo, Ajay Jain, Harikaran Subbaraj, Rehan Durrani, Alexey Tumanov, Joseph Gonzalez, and Ion Stoica. Dynamic space- time scheduling for gpu inference. In LearningSys Workshop at Neural Information Processing Systems 2018, 2018.\n\nChameleon: Scalable adaptation of video analytics. Junchen Jiang, Ganesh Ananthanarayanan, Peter Bodik, Siddhartha Sen, Ion Stoica, Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18. the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18New York, NY, USAACMJunchen Jiang, Ganesh Ananthanarayanan, Peter Bodik, Siddhartha Sen, and Ion Stoica. Chameleon: Scalable adaptation of video analytics. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM '18, pages 253-266, New York, NY, USA, 2018. ACM.\n\nOccupy the cloud: Distributed computing for the 99%. Eric Jonas, Qifan Pu, Shivaram Venkataraman, Ion Stoica, Benjamin Recht, Proceedings of the 2017 Symposium on Cloud Computing, SoCC '17. the 2017 Symposium on Cloud Computing, SoCC '17New York, NY, USAACMEric Jonas, Qifan Pu, Shivaram Venkataraman, Ion Stoica, and Ben- jamin Recht. Occupy the cloud: Distributed computing for the 99%. In Proceedings of the 2017 Symposium on Cloud Computing, SoCC '17, pages 445-451, New York, NY, USA, 2017. ACM.\n\nIn-datacenter performance analysis of a tensor processing unit. Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-Luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon Mackean, Adriana Maggiore, Maire Mahony ; Vijay, Richard Vasudevan, Walter Walter, Eric Wang, Doe Hyun Wilcox, Yoon, Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA '17. Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorsonthe 44th Annual International Symposium on Computer Architecture, ISCA '17Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani; Bo Tian, Horia Toma, Erick Tuttle; New York, NY, USAACMNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gau- rav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Bo- den, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexan- der Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adri- ana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snel- ham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA '17, pages 1-12, New York, NY, USA, 2017. ACM.\n\nNoscope: Optimizing neural network queries over video at scale. Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, Matei Zaharia, Proc. VLDB Endow. 1011Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, and Matei Zaharia. Noscope: Optimizing neural network queries over video at scale. Proc. VLDB Endow., 10(11):1586-1597, August 2017.\n\nPocket: Elastic ephemeral storage for serverless analytics. Ana Klimovic, Yawen Wang, Patrick Stuedi, Animesh Trivedi, Jonas Pfefferle, Christos Kozyrakis, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationAna Klimovic, Yawen Wang, Patrick Stuedi, Animesh Trivedi, Jonas Pfefferle, and Christos Kozyrakis. Pocket: Elastic ephemeral storage for serverless analytics. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 427-444, Carlsbad, CA, 2018. USENIX Association.\n\nPRETZEL: Opening the black box of machine learning prediction serving systems. Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, Matteo Interlandi, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationYunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi. PRETZEL: Opening the black box of machine learning prediction serving systems. In 13th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 18), pages 611-626, Carlsbad, CA, 2018. USENIX Association.\n\nPolicy/mechanism separation in hydra. R Levin, E Cohen, W Corwin, F Pollack, W Wulf, Proceedings of the Fifth ACM Symposium on Operating Systems Principles, SOSP '75. the Fifth ACM Symposium on Operating Systems Principles, SOSP '75New York, NY, USAAssociation for Computing MachineryR. Levin, E. Cohen, W. Corwin, F. Pollack, and W. Wulf. Policy/mecha- nism separation in hydra. In Proceedings of the Fifth ACM Symposium on Operating Systems Principles, SOSP '75, page 132-140, New York, NY, USA, 1975. Association for Computing Machinery.\n\nEnergy efficient virtual machine placement algorithm with balanced and improved resource utilization in a data center. Xin Li, Zhuzhong Qian, Sanglu Lu, Jie Wu, Mathematical and Computer Modelling. 585-6Xin Li, Zhuzhong Qian, Sanglu Lu, and Jie Wu. Energy efficient virtual machine placement algorithm with balanced and improved resource utilization in a data center. Mathematical and Computer Modelling, 58(5-6):1222-1235, 2013.\n\nThemis: Fair and efficient GPU cluster scheduling. Kshiteej Mahajan, Arjun Balasubramanian, Arjun Singhvi, Shivaram Venkataraman, Aditya Akella, Amar Phanishayee, Shuchi Chawla, 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20). Santa Clara, CAKshiteej Mahajan, Arjun Balasubramanian, Arjun Singhvi, Shivaram Venkataraman, Aditya Akella, Amar Phanishayee, and Shuchi Chawla. Themis: Fair and efficient GPU cluster scheduling. In 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20), pages 289-304, Santa Clara, CA, February 2020. USENIX Asso- ciation.\n\nShredder: Learning noise distributions to protect inference privacy. Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Ali Jalali, Dean Tullsen, Hadi Esmaeilzadeh, Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '20. the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '20New York, NY, USAAssociation for Computing MachineryFatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ram- rakhyani, Ali Jalali, Dean Tullsen, and Hadi Esmaeilzadeh. Shredder: Learning noise distributions to protect inference privacy. In Proceed- ings of the Twenty-Fifth International Conference on Architectural Sup- port for Programming Languages and Operating Systems, ASPLOS '20, page 3-18, New York, NY, USA, 2020. Association for Computing Machinery.\n\n. Nvidia Mps, NVIDIA MPS. https://docs.nvidia.com/deploy/pdf/CUDA_ Multi_Process_Service_Overview.pdf, 2018.\n\n. Gpu Nvidia Multi-Instance, NVIDIA Multi-instance GPU. https://www.nvidia.com/en-us/ technologies/multi-instance-gpu/, 2020.\n\nA portable, automatic data quantizer for deep neural networks. H Young, Quan Oh, Daeyeon Quan, Seonghak Kim, Jun Kim, Sungjun Heo, Jaeyoung Jung, Jae W Jang, Lee, Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18. the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18New York, NY, USAACM17Young H. Oh, Quan Quan, Daeyeon Kim, Seonghak Kim, Jun Heo, Sungjun Jung, Jaeyoung Jang, and Jae W. Lee. A portable, automatic data quantizer for deep neural networks. In Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18, pages 17:1-17:14, New York, NY, USA, 2018. ACM.\n\nScanner: Efficient video analysis at scale. Alex Poms, Will Crichton, Pat Hanrahan, Kayvon Fatahalian, ACM Transactions on Graphics (TOG). 374Alex Poms, Will Crichton, Pat Hanrahan, and Kayvon Fatahalian. Scan- ner: Efficient video analysis at scale. ACM Transactions on Graphics (TOG), 37(4):1-13, 2018.\n\nMlperf inference benchmark. V J Reddi, C Cheng, D Kanter, P Mattson, G Schmuelling, C Wu, B Anderson, M Breughe, M Charlebois, W Chou, R Chukka, C Coleman, S Davis, P Deng, G Diamos, J Duke, D Fick, J S Gardner, I Hubara, S Idgunji, T B Jablin, J Jiao, T S John, P Kanwar, D Lee, J Liao, A Lokhmotov, F Massa, P Meng, P Micikevicius, C Osborne, G Pekhimenko, A T R Rajan, D Sequeira, A Sirasao, F Sun, H Tang, M Thomson, F Wei, E Wu, L Xu, K Yamada, B Yu, G Yuan, A Zhong, P Zhang, Y Zhou, 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Cole- man, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Os- borne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou. Mlperf inference benchmark. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA), pages 446-459, 2020.\n\nOnline and interference-aware scheduling for multi-scale heterogeneous systems. Francisco Romero, Christina Delimitrou, Mage, Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18. the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18New York, NY, USAAssociation for Computing MachineryFrancisco Romero and Christina Delimitrou. Mage: Online and interference-aware scheduling for multi-scale heterogeneous systems. In Proceedings of the 27th International Conference on Parallel Archi- tectures and Compilation Techniques, PACT '18, New York, NY, USA, 2018. Association for Computing Machinery.\n\nOn the online bin packing problem. Steven S Seiden, J. ACM. 495Steven S. Seiden. On the online bin packing problem. J. ACM, 49(5):640-671, September 2002.\n\nServerless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider. Mohammad Shahrad, Rodrigo Fonseca, Inigo Goiri, Gohar Irfan, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, Ricardo Bianchini, 2020 USENIX Annual Technical Conference (USENIX ATC 20). Boston, MA, USAUSENIX Association. To AppearMohammad Shahrad, Rodrigo Fonseca, Inigo Goiri, Gohar Irfan, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russi- novich, and Ricardo Bianchini. Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider. In 2020 USENIX Annual Technical Conference (USENIX ATC 20), Boston, MA, USA, July 2020. USENIX Association. To Appear.\n\nNexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis. Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, Ravi Sundaram, Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19. the 27th ACM Symposium on Operating Systems Principles, SOSP '19New York, NY, USAAssociation for Computing MachineryHaichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP '19, page 322-337, New York, NY, USA, 2019. As- sociation for Computing Machinery.\n\nSemantic lattice processing in contextual automatic speech recognition for google assistant. Leonid Velikovich, Ian Williams, Justin Scheiner, Petar S Aleksic, Pedro J Moreno, Michael Riley, Interspeech 2018, 19th Annual Conference of the International Speech Communication Association. Hyderabad, India, 2-6Leonid Velikovich, Ian Williams, Justin Scheiner, Petar S. Aleksic, Pedro J. Moreno, and Michael Riley. Semantic lattice processing in contextual automatic speech recognition for google assistant. In Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018., pages 2222-2226, 2018.\n\nErnest: Efficient performance prediction for large-scale advanced analytics. Shivaram Venkataraman, Zongheng Yang, Michael Franklin, Benjamin Recht, Ion Stoica, 13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16). Santa Clara, CAUSENIX AssociationShivaram Venkataraman, Zongheng Yang, Michael Franklin, Benjamin Recht, and Ion Stoica. Ernest: Efficient performance prediction for large-scale advanced analytics. In 13th USENIX Symposium on Net- worked Systems Design and Implementation (NSDI 16), pages 363-378, Santa Clara, CA, 2016. USENIX Association.\n\nA bound on solutions of linear integer equalities and inequalities. Joachim Von Zur Gathen, Malte Sieveking, Proceedings of the. theAmerican Mathematical Society72Joachim von zur Gathen and Malte Sieveking. A bound on solutions of linear integer equalities and inequalities. Proceedings of the American Mathematical Society, 72(1):155-158, 1978.\n\nACL 2016 First Conference on Machine Translation (WMT16). ACL 2016 First Conference on Machine Translation (WMT16). http: //www.statmt.org/wmt16/.\n\nGandiva: Introspective cluster scheduling for deep learning. Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, Lidong Zhou, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). Carlsbad, CAUSENIX AssociationWencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. Gandiva: Introspective cluster scheduling for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 595-610, Carlsbad, CA, 2018. USENIX Association.\n\nXLA: Optimizing Compiler for Machine Learning. XLA: Optimizing Compiler for Machine Learning. https://www. tensorflow.org/xla.\n\nA case for managed and model-less inference serving. J Neeraja, Francisco Yadwadkar, Qian Romero, Christos Li, Kozyrakis, Proceedings of the Workshop on Hot Topics in Operating Systems, HotOS '19. the Workshop on Hot Topics in Operating Systems, HotOS '19New York, NY, USAAssociation for Computing MachineryNeeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. A case for managed and model-less inference serving. In Proceedings of the Workshop on Hot Topics in Operating Systems, HotOS '19, page 184-191, New York, NY, USA, 2019. Association for Computing Machinery.\n\nSalus: Fine-grained GPU sharing primitives for deep learning applications. Peifeng Yu, Mosharaf Chowdhury, abs/1902.04610CoRRPeifeng Yu and Mosharaf Chowdhury. Salus: Fine-grained GPU shar- ing primitives for deep learning applications. CoRR, abs/1902.04610, 2019.\n\nMark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan, 2019 USENIX Annual Technical Conference (USENIX ATC 19). Renton, WAUSENIX AssociationChengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference (USENIX ATC 19), pages 1049-1062, Renton, WA, July 2019. USENIX Association.\n\nLive video analytics at scale with approximation and delay-tolerance. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, Michael J Freedman, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). Boston, MAUSENIX AssociationHaoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Phili- pose, Paramvir Bahl, and Michael J. Freedman. Live video analytics at scale with approximation and delay-tolerance. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17), pages 377-392, Boston, MA, March 2017. USENIX Association.\n\nModel-switching: Dealing with fluctuating workloads in machine-learning-as-a-service systems. Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, Siddharth Garg, 12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20). USENIX Association. Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, and Siddharth Garg. Model-switching: Dealing with fluctuating workloads in machine-learning-as-a-service systems. In 12th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 20). USENIX Association, July 2020.\n", "annotations": {"author": "[{\"end\":194,\"start\":113},{\"end\":268,\"start\":195},{\"end\":355,\"start\":269},{\"end\":439,\"start\":356},{\"end\":499,\"start\":440},{\"end\":550,\"start\":500},{\"end\":613,\"start\":551},{\"end\":675,\"start\":614}]", "publisher": null, "author_last_name": "[{\"end\":129,\"start\":123},{\"end\":202,\"start\":200},{\"end\":288,\"start\":279},{\"end\":374,\"start\":365},{\"end\":456,\"start\":450},{\"end\":507,\"start\":505},{\"end\":570,\"start\":561},{\"end\":632,\"start\":623}]", "author_first_name": "[{\"end\":122,\"start\":113},{\"end\":199,\"start\":195},{\"end\":276,\"start\":269},{\"end\":278,\"start\":277},{\"end\":364,\"start\":356},{\"end\":449,\"start\":440},{\"end\":504,\"start\":500},{\"end\":558,\"start\":551},{\"end\":560,\"start\":559},{\"end\":622,\"start\":614}]", "author_affiliation": "[{\"end\":193,\"start\":153},{\"end\":267,\"start\":227},{\"end\":354,\"start\":314},{\"end\":438,\"start\":398},{\"end\":498,\"start\":458},{\"end\":549,\"start\":509},{\"end\":612,\"start\":572},{\"end\":674,\"start\":634}]", "title": "[{\"end\":94,\"start\":1},{\"end\":769,\"start\":676}]", "venue": "[{\"end\":829,\"start\":771}]", "abstract": "[{\"end\":2984,\"start\":1099}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3103,\"start\":3099},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3106,\"start\":3103},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3109,\"start\":3106},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3112,\"start\":3109},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":3115,\"start\":3112},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3228,\"start\":3224},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3231,\"start\":3228},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3349,\"start\":3345},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3960,\"start\":3956},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":3963,\"start\":3960},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":4120,\"start\":4116},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4138,\"start\":4134},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4141,\"start\":4138},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4144,\"start\":4141},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4617,\"start\":4613},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4640,\"start\":4636},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4802,\"start\":4798},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4867,\"start\":4863},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4881,\"start\":4878},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4937,\"start\":4933},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5275,\"start\":5272},{\"end\":5640,\"start\":5632},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5714,\"start\":5710},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5766,\"start\":5762},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6429,\"start\":6426},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6432,\"start\":6429},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6435,\"start\":6432},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6438,\"start\":6435},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6441,\"start\":6438},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7064,\"start\":7060},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7088,\"start\":7085},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7108,\"start\":7104},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7124,\"start\":7121},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7126,\"start\":7124},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7129,\"start\":7126},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7132,\"start\":7129},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":7135,\"start\":7132},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":7833,\"start\":7829},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9405,\"start\":9401},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9424,\"start\":9420},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":10203,\"start\":10199},{\"end\":10277,\"start\":10262},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10402,\"start\":10398},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10412,\"start\":10408},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10427,\"start\":10423},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10524,\"start\":10521},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10689,\"start\":10685},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":10692,\"start\":10689},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11420,\"start\":11416},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11780,\"start\":11776},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11783,\"start\":11780},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":11786,\"start\":11783},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12050,\"start\":12046},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12053,\"start\":12050},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12222,\"start\":12218},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12225,\"start\":12222},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12228,\"start\":12225},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14351,\"start\":14347},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":14354,\"start\":14351},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14603,\"start\":14599},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":14606,\"start\":14603},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21928,\"start\":21925},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21944,\"start\":21940},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22061,\"start\":22057},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25246,\"start\":25245},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25720,\"start\":25717},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25723,\"start\":25720},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25726,\"start\":25723},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25729,\"start\":25726},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25732,\"start\":25729},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27367,\"start\":27363},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":27370,\"start\":27367},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":27373,\"start\":27370},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27622,\"start\":27618},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33125,\"start\":33121},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33128,\"start\":33125},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33131,\"start\":33128},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33134,\"start\":33131},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33137,\"start\":33134},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":33966,\"start\":33962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34148,\"start\":34145},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34296,\"start\":34292},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34988,\"start\":34984},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":34991,\"start\":34988},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35643,\"start\":35640},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35780,\"start\":35777},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36288,\"start\":36284},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36501,\"start\":36498},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36517,\"start\":36513},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36568,\"start\":36564},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":36571,\"start\":36568},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36886,\"start\":36883},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":37744,\"start\":37740},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":38447,\"start\":38443},{\"end\":38535,\"start\":38518},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38699,\"start\":38695},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":38753,\"start\":38749},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39114,\"start\":39110},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39192,\"start\":39188},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":39195,\"start\":39192},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39322,\"start\":39318},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39472,\"start\":39468},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":39556,\"start\":39552},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":39703,\"start\":39699},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40300,\"start\":40296},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40843,\"start\":40839},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":41079,\"start\":41075},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":45955,\"start\":45951},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":45958,\"start\":45955},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":46560,\"start\":46556},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":47069,\"start\":47066},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":47183,\"start\":47179},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":47281,\"start\":47277},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":47293,\"start\":47289},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":47314,\"start\":47310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47392,\"start\":47388},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":47410,\"start\":47406},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":47428,\"start\":47425},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":47526,\"start\":47523},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":47646,\"start\":47642},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":47838,\"start\":47834},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":47979,\"start\":47975},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48232,\"start\":48228},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":48236,\"start\":48233},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48239,\"start\":48236},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":48242,\"start\":48239},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":48245,\"start\":48242},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48684,\"start\":48680},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":48931,\"start\":48927},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":49057,\"start\":49054},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49060,\"start\":49057},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":49063,\"start\":49060},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":49066,\"start\":49063},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":49245,\"start\":49241},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49374,\"start\":49371},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":49377,\"start\":49374},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":49380,\"start\":49377},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":49383,\"start\":49380},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":49483,\"start\":49479},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":49597,\"start\":49593}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50735,\"start\":50494},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51034,\"start\":50736},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52830,\"start\":51035},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52918,\"start\":52831},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54116,\"start\":52919},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54765,\"start\":54117},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55263,\"start\":54766},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55386,\"start\":55264},{\"attributes\":{\"id\":\"fig_9\"},\"end\":56429,\"start\":55387},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":56606,\"start\":56430},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56837,\"start\":56607},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56966,\"start\":56838},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":57637,\"start\":56967},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57677,\"start\":57638},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":60698,\"start\":57678},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61707,\"start\":60699},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64626,\"start\":61708},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":65817,\"start\":64627},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":66014,\"start\":65818},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":66546,\"start\":66015},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":66780,\"start\":66547},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":67293,\"start\":66781}]", "paragraph": "[{\"end\":3350,\"start\":3000},{\"end\":3900,\"start\":3352},{\"end\":4803,\"start\":3902},{\"end\":5060,\"start\":4805},{\"end\":5965,\"start\":5062},{\"end\":6614,\"start\":5967},{\"end\":7882,\"start\":6616},{\"end\":8501,\"start\":7884},{\"end\":9248,\"start\":8503},{\"end\":9892,\"start\":9250},{\"end\":10823,\"start\":9943},{\"end\":11421,\"start\":10825},{\"end\":11601,\"start\":11423},{\"end\":12625,\"start\":11634},{\"end\":13909,\"start\":12627},{\"end\":15181,\"start\":13947},{\"end\":17829,\"start\":15192},{\"end\":18107,\"start\":17868},{\"end\":18346,\"start\":18109},{\"end\":18860,\"start\":18348},{\"end\":19973,\"start\":18862},{\"end\":20441,\"start\":19975},{\"end\":20517,\"start\":20458},{\"end\":20962,\"start\":20519},{\"end\":21136,\"start\":20964},{\"end\":21642,\"start\":21138},{\"end\":22971,\"start\":21644},{\"end\":25175,\"start\":23012},{\"end\":25294,\"start\":25177},{\"end\":25351,\"start\":25296},{\"end\":25409,\"start\":25353},{\"end\":25440,\"start\":25411},{\"end\":25499,\"start\":25442},{\"end\":25505,\"start\":25501},{\"end\":25985,\"start\":25543},{\"end\":26476,\"start\":26044},{\"end\":26706,\"start\":26478},{\"end\":26766,\"start\":26708},{\"end\":26852,\"start\":26768},{\"end\":26946,\"start\":26854},{\"end\":27903,\"start\":27139},{\"end\":30038,\"start\":27926},{\"end\":32940,\"start\":30040},{\"end\":33967,\"start\":32972},{\"end\":34042,\"start\":33986},{\"end\":34600,\"start\":34044},{\"end\":35185,\"start\":34602},{\"end\":36450,\"start\":35187},{\"end\":36810,\"start\":36452},{\"end\":37162,\"start\":36812},{\"end\":38127,\"start\":37177},{\"end\":38384,\"start\":38129},{\"end\":39968,\"start\":38386},{\"end\":40301,\"start\":40004},{\"end\":42281,\"start\":40303},{\"end\":44758,\"start\":42320},{\"end\":45823,\"start\":44788},{\"end\":46562,\"start\":45825},{\"end\":47003,\"start\":46564},{\"end\":49843,\"start\":47020},{\"end\":50493,\"start\":49858}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":26043,\"start\":25986},{\"attributes\":{\"id\":\"formula_1\"},\"end\":27138,\"start\":26947}]", "table_ref": "[{\"end\":4153,\"start\":4146},{\"end\":11255,\"start\":11248},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12958,\"start\":12951},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13107,\"start\":13100},{\"end\":37552,\"start\":37545},{\"end\":38490,\"start\":38483},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":44932,\"start\":44925}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2998,\"start\":2986},{\"attributes\":{\"n\":\"2\"},\"end\":9905,\"start\":9895},{\"attributes\":{\"n\":\"2.1\"},\"end\":9941,\"start\":9908},{\"attributes\":{\"n\":\"2.2\"},\"end\":11632,\"start\":11604},{\"attributes\":{\"n\":\"2.3\"},\"end\":13945,\"start\":13912},{\"attributes\":{\"n\":\"3\"},\"end\":15190,\"start\":15184},{\"attributes\":{\"n\":\"3.1\"},\"end\":17866,\"start\":17832},{\"attributes\":{\"n\":\"3.2\"},\"end\":20456,\"start\":20444},{\"attributes\":{\"n\":\"4\"},\"end\":23010,\"start\":22974},{\"attributes\":{\"n\":\"4.2\"},\"end\":25541,\"start\":25508},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":27924,\"start\":27906},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":32970,\"start\":32943},{\"attributes\":{\"n\":\"5\"},\"end\":33984,\"start\":33970},{\"attributes\":{\"n\":\"6\"},\"end\":37175,\"start\":37165},{\"attributes\":{\"n\":\"6.1\"},\"end\":40002,\"start\":39971},{\"attributes\":{\"n\":\"6.3\"},\"end\":42318,\"start\":42284},{\"attributes\":{\"n\":\"6.4\"},\"end\":44786,\"start\":44761},{\"attributes\":{\"n\":\"8\"},\"end\":47018,\"start\":47006},{\"attributes\":{\"n\":\"9\"},\"end\":49856,\"start\":49846},{\"end\":50505,\"start\":50495},{\"end\":50757,\"start\":50737},{\"end\":52842,\"start\":52832},{\"end\":54771,\"start\":54767},{\"end\":55275,\"start\":55265},{\"end\":55398,\"start\":55388},{\"end\":56617,\"start\":56608},{\"end\":56848,\"start\":56839},{\"end\":56977,\"start\":56968},{\"end\":57652,\"start\":57639},{\"end\":61718,\"start\":61709},{\"end\":66023,\"start\":66016},{\"end\":66557,\"start\":66548}]", "table": "[{\"end\":56606,\"start\":56488},{\"end\":56837,\"start\":56722},{\"end\":57637,\"start\":57013},{\"end\":60698,\"start\":58701},{\"end\":61707,\"start\":61224},{\"end\":64626,\"start\":62318},{\"end\":65817,\"start\":64765},{\"end\":66546,\"start\":66531}]", "figure_caption": "[{\"end\":50735,\"start\":50507},{\"end\":51034,\"start\":50760},{\"end\":52830,\"start\":51037},{\"end\":52918,\"start\":52844},{\"end\":54116,\"start\":52921},{\"end\":54765,\"start\":54119},{\"end\":55263,\"start\":54773},{\"end\":55386,\"start\":55277},{\"end\":56429,\"start\":55400},{\"end\":56488,\"start\":56432},{\"end\":56722,\"start\":56619},{\"end\":56966,\"start\":56850},{\"end\":57013,\"start\":56979},{\"end\":57677,\"start\":57654},{\"end\":58701,\"start\":57680},{\"end\":61224,\"start\":60701},{\"end\":62318,\"start\":61720},{\"end\":64765,\"start\":64629},{\"end\":66014,\"start\":65820},{\"end\":66531,\"start\":66025},{\"end\":66780,\"start\":66559},{\"end\":67293,\"start\":66783}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14733,\"start\":14725},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":17343,\"start\":17335},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":18544,\"start\":18536},{\"end\":20177,\"start\":20169},{\"end\":20505,\"start\":20497},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24510,\"start\":24502},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35456,\"start\":35448},{\"end\":40335,\"start\":40327},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41436,\"start\":41419},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":41868,\"start\":41849},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":42280,\"start\":42261},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":43906,\"start\":43898}]", "bib_author_first_name": "[{\"end\":68788,\"start\":68784},{\"end\":68807,\"start\":68802},{\"end\":68826,\"start\":68819},{\"end\":68840,\"start\":68832},{\"end\":68853,\"start\":68847},{\"end\":68872,\"start\":68868},{\"end\":69333,\"start\":69330},{\"end\":69394,\"start\":69392},{\"end\":69457,\"start\":69451},{\"end\":69537,\"start\":69531},{\"end\":69841,\"start\":69833},{\"end\":69855,\"start\":69849},{\"end\":69866,\"start\":69863},{\"end\":69881,\"start\":69876},{\"end\":70677,\"start\":70671},{\"end\":70690,\"start\":70686},{\"end\":70704,\"start\":70699},{\"end\":70718,\"start\":70713},{\"end\":70925,\"start\":70918},{\"end\":70938,\"start\":70933},{\"end\":70951,\"start\":70946},{\"end\":70969,\"start\":70965},{\"end\":70982,\"start\":70978},{\"end\":71001,\"start\":70991},{\"end\":71236,\"start\":71231},{\"end\":71259,\"start\":71251},{\"end\":71270,\"start\":71269},{\"end\":71532,\"start\":71526},{\"end\":71546,\"start\":71539},{\"end\":71561,\"start\":71555},{\"end\":71576,\"start\":71569},{\"end\":71589,\"start\":71584},{\"end\":71602,\"start\":71595},{\"end\":71615,\"start\":71609},{\"end\":71629,\"start\":71623},{\"end\":71641,\"start\":71636},{\"end\":71650,\"start\":71646},{\"end\":71663,\"start\":71657},{\"end\":71680,\"start\":71674},{\"end\":72277,\"start\":72271},{\"end\":72306,\"start\":72299},{\"end\":72318,\"start\":72313},{\"end\":72326,\"start\":72323},{\"end\":72340,\"start\":72334},{\"end\":72355,\"start\":72349},{\"end\":72815,\"start\":72809},{\"end\":72830,\"start\":72827},{\"end\":72843,\"start\":72837},{\"end\":72857,\"start\":72850},{\"end\":72859,\"start\":72858},{\"end\":72876,\"start\":72870},{\"end\":72878,\"start\":72877},{\"end\":72892,\"start\":72889},{\"end\":73357,\"start\":73352},{\"end\":73371,\"start\":73366},{\"end\":73394,\"start\":73382},{\"end\":73412,\"start\":73406},{\"end\":73426,\"start\":73418},{\"end\":73428,\"start\":73427},{\"end\":73445,\"start\":73439},{\"end\":73460,\"start\":73453},{\"end\":73480,\"start\":73476},{\"end\":73494,\"start\":73489},{\"end\":73506,\"start\":73501},{\"end\":73520,\"start\":73516},{\"end\":73534,\"start\":73527},{\"end\":73552,\"start\":73547},{\"end\":73566,\"start\":73562},{\"end\":73580,\"start\":73575},{\"end\":74296,\"start\":74291},{\"end\":74310,\"start\":74305},{\"end\":74320,\"start\":74315},{\"end\":74346,\"start\":74340},{\"end\":74361,\"start\":74354},{\"end\":74363,\"start\":74362},{\"end\":74703,\"start\":74697},{\"end\":74718,\"start\":74714},{\"end\":74736,\"start\":74729},{\"end\":74749,\"start\":74743},{\"end\":75228,\"start\":75219},{\"end\":75249,\"start\":75241},{\"end\":75881,\"start\":75878},{\"end\":75891,\"start\":75888},{\"end\":75905,\"start\":75898},{\"end\":75917,\"start\":75914},{\"end\":75931,\"start\":75929},{\"end\":76158,\"start\":76152},{\"end\":76160,\"start\":76159},{\"end\":76176,\"start\":76171},{\"end\":76192,\"start\":76184},{\"end\":76206,\"start\":76202},{\"end\":76222,\"start\":76215},{\"end\":76734,\"start\":76728},{\"end\":76748,\"start\":76743},{\"end\":76767,\"start\":76760},{\"end\":76785,\"start\":76781},{\"end\":76802,\"start\":76798},{\"end\":76814,\"start\":76808},{\"end\":76825,\"start\":76819},{\"end\":76842,\"start\":76835},{\"end\":76858,\"start\":76853},{\"end\":76871,\"start\":76866},{\"end\":76887,\"start\":76880},{\"end\":76900,\"start\":76894},{\"end\":76912,\"start\":76908},{\"end\":76927,\"start\":76920},{\"end\":76939,\"start\":76935},{\"end\":76954,\"start\":76947},{\"end\":76968,\"start\":76962},{\"end\":76970,\"start\":76969},{\"end\":76988,\"start\":76982},{\"end\":76990,\"start\":76989},{\"end\":77006,\"start\":77002},{\"end\":77008,\"start\":77007},{\"end\":77020,\"start\":77016},{\"end\":77823,\"start\":77817},{\"end\":77835,\"start\":77832},{\"end\":77855,\"start\":77852},{\"end\":77878,\"start\":77869},{\"end\":78153,\"start\":78152},{\"end\":78168,\"start\":78163},{\"end\":78170,\"start\":78169},{\"end\":78433,\"start\":78427},{\"end\":78442,\"start\":78441},{\"end\":78444,\"start\":78443},{\"end\":78599,\"start\":78591},{\"end\":78612,\"start\":78604},{\"end\":78625,\"start\":78624},{\"end\":78636,\"start\":78632},{\"end\":78652,\"start\":78643},{\"end\":78664,\"start\":78658},{\"end\":78680,\"start\":78671},{\"end\":78697,\"start\":78687},{\"end\":79254,\"start\":79249},{\"end\":79270,\"start\":79265},{\"end\":79288,\"start\":79281},{\"end\":79294,\"start\":79293},{\"end\":79315,\"start\":79314},{\"end\":79821,\"start\":79816},{\"end\":79836,\"start\":79832},{\"end\":79850,\"start\":79845},{\"end\":79863,\"start\":79860},{\"end\":79876,\"start\":79869},{\"end\":79891,\"start\":79887},{\"end\":79911,\"start\":79903},{\"end\":80340,\"start\":80336},{\"end\":80354,\"start\":80348},{\"end\":80367,\"start\":80361},{\"end\":80384,\"start\":80376},{\"end\":80398,\"start\":80391},{\"end\":80422,\"start\":80421},{\"end\":80440,\"start\":80435},{\"end\":80457,\"start\":80446},{\"end\":80881,\"start\":80877},{\"end\":80900,\"start\":80889},{\"end\":80913,\"start\":80905},{\"end\":80925,\"start\":80920},{\"end\":80941,\"start\":80934},{\"end\":80955,\"start\":80950},{\"end\":80972,\"start\":80964},{\"end\":80984,\"start\":80981},{\"end\":81000,\"start\":80996},{\"end\":81016,\"start\":81012},{\"end\":81732,\"start\":81731},{\"end\":81743,\"start\":81742},{\"end\":81759,\"start\":81758},{\"end\":81770,\"start\":81769},{\"end\":81785,\"start\":81784},{\"end\":82400,\"start\":82397},{\"end\":82417,\"start\":82412},{\"end\":82429,\"start\":82424},{\"end\":82445,\"start\":82438},{\"end\":82460,\"start\":82456},{\"end\":82474,\"start\":82468},{\"end\":82494,\"start\":82487},{\"end\":82506,\"start\":82502},{\"end\":82520,\"start\":82512},{\"end\":82532,\"start\":82526},{\"end\":82545,\"start\":82540},{\"end\":82556,\"start\":82551},{\"end\":82567,\"start\":82562},{\"end\":82578,\"start\":82572},{\"end\":82595,\"start\":82590},{\"end\":82614,\"start\":82609},{\"end\":82630,\"start\":82622},{\"end\":83384,\"start\":83376},{\"end\":83398,\"start\":83394},{\"end\":83415,\"start\":83410},{\"end\":83428,\"start\":83425},{\"end\":83444,\"start\":83437},{\"end\":83446,\"start\":83445},{\"end\":83460,\"start\":83455},{\"end\":83472,\"start\":83467},{\"end\":83485,\"start\":83482},{\"end\":84113,\"start\":84108},{\"end\":84127,\"start\":84121},{\"end\":84151,\"start\":84146},{\"end\":84167,\"start\":84159},{\"end\":84190,\"start\":84182},{\"end\":84204,\"start\":84197},{\"end\":84223,\"start\":84216},{\"end\":84225,\"start\":84224},{\"end\":84239,\"start\":84235},{\"end\":84771,\"start\":84766},{\"end\":84785,\"start\":84778},{\"end\":84794,\"start\":84790},{\"end\":84810,\"start\":84801},{\"end\":84826,\"start\":84821},{\"end\":84842,\"start\":84836},{\"end\":84858,\"start\":84852},{\"end\":84872,\"start\":84869},{\"end\":85249,\"start\":85242},{\"end\":85263,\"start\":85257},{\"end\":85287,\"start\":85282},{\"end\":85305,\"start\":85295},{\"end\":85314,\"start\":85311},{\"end\":85883,\"start\":85879},{\"end\":85896,\"start\":85891},{\"end\":85909,\"start\":85901},{\"end\":85927,\"start\":85924},{\"end\":85944,\"start\":85936},{\"end\":86398,\"start\":86392},{\"end\":86400,\"start\":86399},{\"end\":86414,\"start\":86409},{\"end\":86429,\"start\":86422},{\"end\":86442,\"start\":86437},{\"end\":86460,\"start\":86454},{\"end\":86478,\"start\":86470},{\"end\":86491,\"start\":86486},{\"end\":86505,\"start\":86499},{\"end\":86517,\"start\":86514},{\"end\":86527,\"start\":86525},{\"end\":86542,\"start\":86538},{\"end\":86560,\"start\":86550},{\"end\":86577,\"start\":86569},{\"end\":86589,\"start\":86584},{\"end\":86603,\"start\":86597},{\"end\":86617,\"start\":86613},{\"end\":86629,\"start\":86625},{\"end\":86642,\"start\":86635},{\"end\":86652,\"start\":86649},{\"end\":86663,\"start\":86659},{\"end\":86692,\"start\":86684},{\"end\":86711,\"start\":86704},{\"end\":86727,\"start\":86721},{\"end\":86738,\"start\":86737},{\"end\":86746,\"start\":86739},{\"end\":86755,\"start\":86751},{\"end\":86769,\"start\":86765},{\"end\":86780,\"start\":86774},{\"end\":86791,\"start\":86788},{\"end\":86804,\"start\":86798},{\"end\":86817,\"start\":86812},{\"end\":86830,\"start\":86826},{\"end\":86850,\"start\":86841},{\"end\":86866,\"start\":86859},{\"end\":86882,\"start\":86876},{\"end\":86898,\"start\":86894},{\"end\":86911,\"start\":86905},{\"end\":86924,\"start\":86919},{\"end\":86936,\"start\":86931},{\"end\":86950,\"start\":86945},{\"end\":86963,\"start\":86956},{\"end\":86973,\"start\":86968},{\"end\":86988,\"start\":86981},{\"end\":86998,\"start\":86994},{\"end\":87010,\"start\":87006},{\"end\":87025,\"start\":87019},{\"end\":87042,\"start\":87035},{\"end\":87058,\"start\":87053},{\"end\":87082,\"start\":87075},{\"end\":87100,\"start\":87094},{\"end\":87113,\"start\":87109},{\"end\":87123,\"start\":87120},{\"end\":87128,\"start\":87124},{\"end\":89044,\"start\":89038},{\"end\":89055,\"start\":89051},{\"end\":89069,\"start\":89064},{\"end\":89084,\"start\":89079},{\"end\":89098,\"start\":89093},{\"end\":89382,\"start\":89379},{\"end\":89398,\"start\":89393},{\"end\":89412,\"start\":89405},{\"end\":89428,\"start\":89421},{\"end\":89443,\"start\":89438},{\"end\":89463,\"start\":89455},{\"end\":89971,\"start\":89963},{\"end\":89984,\"start\":89977},{\"end\":90003,\"start\":89994},{\"end\":90015,\"start\":90010},{\"end\":90024,\"start\":90016},{\"end\":90045,\"start\":90039},{\"end\":90060,\"start\":90054},{\"end\":90556,\"start\":90555},{\"end\":90565,\"start\":90564},{\"end\":90574,\"start\":90573},{\"end\":90584,\"start\":90583},{\"end\":90595,\"start\":90594},{\"end\":91181,\"start\":91178},{\"end\":91194,\"start\":91186},{\"end\":91207,\"start\":91201},{\"end\":91215,\"start\":91212},{\"end\":91549,\"start\":91541},{\"end\":91564,\"start\":91559},{\"end\":91587,\"start\":91582},{\"end\":91605,\"start\":91597},{\"end\":91626,\"start\":91620},{\"end\":91639,\"start\":91635},{\"end\":91659,\"start\":91653},{\"end\":92179,\"start\":92167},{\"end\":92208,\"start\":92195},{\"end\":92223,\"start\":92216},{\"end\":92240,\"start\":92237},{\"end\":92253,\"start\":92249},{\"end\":92267,\"start\":92263},{\"end\":93132,\"start\":93129},{\"end\":93318,\"start\":93317},{\"end\":93330,\"start\":93326},{\"end\":93342,\"start\":93335},{\"end\":93357,\"start\":93349},{\"end\":93366,\"start\":93363},{\"end\":93379,\"start\":93372},{\"end\":93393,\"start\":93385},{\"end\":93403,\"start\":93400},{\"end\":93405,\"start\":93404},{\"end\":94029,\"start\":94025},{\"end\":94040,\"start\":94036},{\"end\":94054,\"start\":94051},{\"end\":94071,\"start\":94065},{\"end\":94316,\"start\":94315},{\"end\":94318,\"start\":94317},{\"end\":94327,\"start\":94326},{\"end\":94336,\"start\":94335},{\"end\":94346,\"start\":94345},{\"end\":94357,\"start\":94356},{\"end\":94372,\"start\":94371},{\"end\":94378,\"start\":94377},{\"end\":94390,\"start\":94389},{\"end\":94401,\"start\":94400},{\"end\":94415,\"start\":94414},{\"end\":94423,\"start\":94422},{\"end\":94433,\"start\":94432},{\"end\":94444,\"start\":94443},{\"end\":94453,\"start\":94452},{\"end\":94461,\"start\":94460},{\"end\":94471,\"start\":94470},{\"end\":94479,\"start\":94478},{\"end\":94487,\"start\":94486},{\"end\":94489,\"start\":94488},{\"end\":94500,\"start\":94499},{\"end\":94510,\"start\":94509},{\"end\":94521,\"start\":94520},{\"end\":94523,\"start\":94522},{\"end\":94533,\"start\":94532},{\"end\":94541,\"start\":94540},{\"end\":94543,\"start\":94542},{\"end\":94551,\"start\":94550},{\"end\":94561,\"start\":94560},{\"end\":94568,\"start\":94567},{\"end\":94576,\"start\":94575},{\"end\":94589,\"start\":94588},{\"end\":94598,\"start\":94597},{\"end\":94606,\"start\":94605},{\"end\":94622,\"start\":94621},{\"end\":94633,\"start\":94632},{\"end\":94647,\"start\":94646},{\"end\":94651,\"start\":94648},{\"end\":94660,\"start\":94659},{\"end\":94672,\"start\":94671},{\"end\":94683,\"start\":94682},{\"end\":94690,\"start\":94689},{\"end\":94698,\"start\":94697},{\"end\":94709,\"start\":94708},{\"end\":94716,\"start\":94715},{\"end\":94722,\"start\":94721},{\"end\":94728,\"start\":94727},{\"end\":94738,\"start\":94737},{\"end\":94744,\"start\":94743},{\"end\":94752,\"start\":94751},{\"end\":94761,\"start\":94760},{\"end\":94770,\"start\":94769},{\"end\":95608,\"start\":95599},{\"end\":95626,\"start\":95617},{\"end\":96257,\"start\":96251},{\"end\":96259,\"start\":96258},{\"end\":96485,\"start\":96477},{\"end\":96502,\"start\":96495},{\"end\":96517,\"start\":96512},{\"end\":96530,\"start\":96525},{\"end\":96542,\"start\":96538},{\"end\":96555,\"start\":96550},{\"end\":96570,\"start\":96563},{\"end\":96586,\"start\":96581},{\"end\":96601,\"start\":96597},{\"end\":96622,\"start\":96615},{\"end\":97201,\"start\":97194},{\"end\":97213,\"start\":97208},{\"end\":97226,\"start\":97220},{\"end\":97239,\"start\":97232},{\"end\":97252,\"start\":97246},{\"end\":97266,\"start\":97259},{\"end\":97284,\"start\":97278},{\"end\":97304,\"start\":97300},{\"end\":97970,\"start\":97964},{\"end\":97986,\"start\":97983},{\"end\":98003,\"start\":97997},{\"end\":98019,\"start\":98014},{\"end\":98021,\"start\":98020},{\"end\":98036,\"start\":98031},{\"end\":98038,\"start\":98037},{\"end\":98054,\"start\":98047},{\"end\":98623,\"start\":98615},{\"end\":98646,\"start\":98638},{\"end\":98660,\"start\":98653},{\"end\":98679,\"start\":98671},{\"end\":98690,\"start\":98687},{\"end\":99196,\"start\":99189},{\"end\":99218,\"start\":99213},{\"end\":99684,\"start\":99677},{\"end\":99696,\"start\":99691},{\"end\":99719,\"start\":99707},{\"end\":99735,\"start\":99728},{\"end\":99752,\"start\":99747},{\"end\":99768,\"start\":99761},{\"end\":99782,\"start\":99774},{\"end\":99794,\"start\":99790},{\"end\":99806,\"start\":99801},{\"end\":99819,\"start\":99813},{\"end\":99830,\"start\":99827},{\"end\":99843,\"start\":99837},{\"end\":100519,\"start\":100518},{\"end\":100538,\"start\":100529},{\"end\":100554,\"start\":100550},{\"end\":100571,\"start\":100563},{\"end\":101135,\"start\":101128},{\"end\":101148,\"start\":101140},{\"end\":101427,\"start\":101417},{\"end\":101442,\"start\":101435},{\"end\":101450,\"start\":101447},{\"end\":101461,\"start\":101457},{\"end\":101900,\"start\":101895},{\"end\":101914,\"start\":101908},{\"end\":101938,\"start\":101933},{\"end\":101953,\"start\":101946},{\"end\":101973,\"start\":101965},{\"end\":101987,\"start\":101980},{\"end\":101989,\"start\":101988},{\"end\":102531,\"start\":102527},{\"end\":102544,\"start\":102539},{\"end\":102561,\"start\":102555},{\"end\":102573,\"start\":102569},{\"end\":102590,\"start\":102581}]", "bib_author_last_name": "[{\"end\":68365,\"start\":68360},{\"end\":68800,\"start\":68789},{\"end\":68817,\"start\":68808},{\"end\":68830,\"start\":68827},{\"end\":68845,\"start\":68841},{\"end\":68866,\"start\":68854},{\"end\":68875,\"start\":68873},{\"end\":68882,\"start\":68877},{\"end\":69340,\"start\":69334},{\"end\":69401,\"start\":69395},{\"end\":69467,\"start\":69458},{\"end\":69551,\"start\":69538},{\"end\":69847,\"start\":69842},{\"end\":69861,\"start\":69856},{\"end\":69874,\"start\":69867},{\"end\":69891,\"start\":69882},{\"end\":70430,\"start\":70415},{\"end\":70520,\"start\":70506},{\"end\":70684,\"start\":70678},{\"end\":70697,\"start\":70691},{\"end\":70711,\"start\":70705},{\"end\":70729,\"start\":70719},{\"end\":70931,\"start\":70926},{\"end\":70944,\"start\":70939},{\"end\":70963,\"start\":70952},{\"end\":70976,\"start\":70970},{\"end\":70989,\"start\":70983},{\"end\":71006,\"start\":71002},{\"end\":71013,\"start\":71008},{\"end\":71249,\"start\":71237},{\"end\":71267,\"start\":71260},{\"end\":71276,\"start\":71271},{\"end\":71282,\"start\":71278},{\"end\":71537,\"start\":71533},{\"end\":71553,\"start\":71547},{\"end\":71567,\"start\":71562},{\"end\":71582,\"start\":71577},{\"end\":71593,\"start\":71590},{\"end\":71607,\"start\":71603},{\"end\":71621,\"start\":71616},{\"end\":71634,\"start\":71630},{\"end\":71644,\"start\":71642},{\"end\":71655,\"start\":71651},{\"end\":71672,\"start\":71664},{\"end\":71694,\"start\":71681},{\"end\":72287,\"start\":72278},{\"end\":72297,\"start\":72289},{\"end\":72311,\"start\":72307},{\"end\":72321,\"start\":72319},{\"end\":72332,\"start\":72327},{\"end\":72347,\"start\":72341},{\"end\":72364,\"start\":72356},{\"end\":72373,\"start\":72366},{\"end\":72825,\"start\":72816},{\"end\":72835,\"start\":72831},{\"end\":72848,\"start\":72844},{\"end\":72868,\"start\":72860},{\"end\":72887,\"start\":72879},{\"end\":72899,\"start\":72893},{\"end\":73364,\"start\":73358},{\"end\":73380,\"start\":73372},{\"end\":73404,\"start\":73395},{\"end\":73416,\"start\":73413},{\"end\":73437,\"start\":73429},{\"end\":73451,\"start\":73446},{\"end\":73474,\"start\":73461},{\"end\":73487,\"start\":73481},{\"end\":73499,\"start\":73495},{\"end\":73514,\"start\":73507},{\"end\":73525,\"start\":73521},{\"end\":73545,\"start\":73535},{\"end\":73560,\"start\":73553},{\"end\":73573,\"start\":73567},{\"end\":73593,\"start\":73581},{\"end\":74303,\"start\":74297},{\"end\":74313,\"start\":74311},{\"end\":74338,\"start\":74321},{\"end\":74352,\"start\":74347},{\"end\":74367,\"start\":74364},{\"end\":74712,\"start\":74704},{\"end\":74727,\"start\":74719},{\"end\":74741,\"start\":74737},{\"end\":74755,\"start\":74750},{\"end\":75239,\"start\":75229},{\"end\":75259,\"start\":75250},{\"end\":75886,\"start\":75882},{\"end\":75896,\"start\":75892},{\"end\":75912,\"start\":75906},{\"end\":75927,\"start\":75918},{\"end\":75934,\"start\":75932},{\"end\":75943,\"start\":75936},{\"end\":76169,\"start\":76161},{\"end\":76182,\"start\":76177},{\"end\":76200,\"start\":76193},{\"end\":76213,\"start\":76207},{\"end\":76230,\"start\":76223},{\"end\":76741,\"start\":76735},{\"end\":76758,\"start\":76749},{\"end\":76779,\"start\":76768},{\"end\":76796,\"start\":76786},{\"end\":76806,\"start\":76803},{\"end\":76817,\"start\":76815},{\"end\":76833,\"start\":76826},{\"end\":76851,\"start\":76843},{\"end\":76864,\"start\":76859},{\"end\":76878,\"start\":76872},{\"end\":76892,\"start\":76888},{\"end\":76906,\"start\":76901},{\"end\":76918,\"start\":76913},{\"end\":76933,\"start\":76928},{\"end\":76945,\"start\":76940},{\"end\":76960,\"start\":76955},{\"end\":76980,\"start\":76971},{\"end\":77000,\"start\":76991},{\"end\":77014,\"start\":77009},{\"end\":77027,\"start\":77021},{\"end\":77830,\"start\":77824},{\"end\":77850,\"start\":77836},{\"end\":77867,\"start\":77856},{\"end\":77885,\"start\":77879},{\"end\":78161,\"start\":78154},{\"end\":78176,\"start\":78171},{\"end\":78185,\"start\":78178},{\"end\":78439,\"start\":78434},{\"end\":78453,\"start\":78445},{\"end\":78602,\"start\":78600},{\"end\":78622,\"start\":78613},{\"end\":78630,\"start\":78626},{\"end\":78641,\"start\":78637},{\"end\":78656,\"start\":78653},{\"end\":78669,\"start\":78665},{\"end\":78685,\"start\":78681},{\"end\":78701,\"start\":78698},{\"end\":78706,\"start\":78703},{\"end\":79263,\"start\":79255},{\"end\":79279,\"start\":79271},{\"end\":79291,\"start\":79289},{\"end\":79302,\"start\":79295},{\"end\":79312,\"start\":79304},{\"end\":79321,\"start\":79316},{\"end\":79334,\"start\":79323},{\"end\":79830,\"start\":79822},{\"end\":79843,\"start\":79837},{\"end\":79858,\"start\":79851},{\"end\":79867,\"start\":79864},{\"end\":79885,\"start\":79877},{\"end\":79901,\"start\":79892},{\"end\":79916,\"start\":79912},{\"end\":80346,\"start\":80341},{\"end\":80359,\"start\":80355},{\"end\":80374,\"start\":80368},{\"end\":80389,\"start\":80385},{\"end\":80405,\"start\":80399},{\"end\":80414,\"start\":80407},{\"end\":80419,\"start\":80416},{\"end\":80433,\"start\":80423},{\"end\":80444,\"start\":80441},{\"end\":80464,\"start\":80458},{\"end\":80468,\"start\":80466},{\"end\":80887,\"start\":80882},{\"end\":80903,\"start\":80901},{\"end\":80918,\"start\":80914},{\"end\":80932,\"start\":80926},{\"end\":80948,\"start\":80942},{\"end\":80962,\"start\":80956},{\"end\":80979,\"start\":80973},{\"end\":80994,\"start\":80985},{\"end\":81010,\"start\":81001},{\"end\":81020,\"start\":81017},{\"end\":81740,\"start\":81733},{\"end\":81756,\"start\":81744},{\"end\":81767,\"start\":81760},{\"end\":81782,\"start\":81771},{\"end\":81791,\"start\":81786},{\"end\":82410,\"start\":82401},{\"end\":82422,\"start\":82418},{\"end\":82436,\"start\":82430},{\"end\":82454,\"start\":82446},{\"end\":82466,\"start\":82461},{\"end\":82485,\"start\":82475},{\"end\":82500,\"start\":82495},{\"end\":82510,\"start\":82507},{\"end\":82524,\"start\":82521},{\"end\":82538,\"start\":82533},{\"end\":82549,\"start\":82546},{\"end\":82560,\"start\":82557},{\"end\":82570,\"start\":82568},{\"end\":82588,\"start\":82579},{\"end\":82607,\"start\":82596},{\"end\":82620,\"start\":82615},{\"end\":82635,\"start\":82631},{\"end\":83392,\"start\":83385},{\"end\":83408,\"start\":83399},{\"end\":83423,\"start\":83416},{\"end\":83435,\"start\":83429},{\"end\":83453,\"start\":83447},{\"end\":83465,\"start\":83461},{\"end\":83480,\"start\":83473},{\"end\":83492,\"start\":83486},{\"end\":84119,\"start\":84114},{\"end\":84144,\"start\":84128},{\"end\":84157,\"start\":84152},{\"end\":84180,\"start\":84168},{\"end\":84195,\"start\":84191},{\"end\":84214,\"start\":84205},{\"end\":84233,\"start\":84226},{\"end\":84245,\"start\":84240},{\"end\":84776,\"start\":84772},{\"end\":84788,\"start\":84786},{\"end\":84799,\"start\":84795},{\"end\":84819,\"start\":84811},{\"end\":84834,\"start\":84827},{\"end\":84850,\"start\":84843},{\"end\":84867,\"start\":84859},{\"end\":84879,\"start\":84873},{\"end\":85255,\"start\":85250},{\"end\":85280,\"start\":85264},{\"end\":85293,\"start\":85288},{\"end\":85309,\"start\":85306},{\"end\":85321,\"start\":85315},{\"end\":85889,\"start\":85884},{\"end\":85899,\"start\":85897},{\"end\":85922,\"start\":85910},{\"end\":85934,\"start\":85928},{\"end\":85950,\"start\":85945},{\"end\":86407,\"start\":86401},{\"end\":86420,\"start\":86415},{\"end\":86435,\"start\":86430},{\"end\":86452,\"start\":86443},{\"end\":86468,\"start\":86461},{\"end\":86484,\"start\":86479},{\"end\":86497,\"start\":86492},{\"end\":86512,\"start\":86506},{\"end\":86523,\"start\":86518},{\"end\":86536,\"start\":86528},{\"end\":86548,\"start\":86543},{\"end\":86567,\"start\":86561},{\"end\":86582,\"start\":86578},{\"end\":86595,\"start\":86590},{\"end\":86611,\"start\":86604},{\"end\":86623,\"start\":86618},{\"end\":86633,\"start\":86630},{\"end\":86647,\"start\":86643},{\"end\":86657,\"start\":86653},{\"end\":86682,\"start\":86664},{\"end\":86702,\"start\":86693},{\"end\":86719,\"start\":86712},{\"end\":86735,\"start\":86728},{\"end\":86749,\"start\":86747},{\"end\":86763,\"start\":86756},{\"end\":86772,\"start\":86770},{\"end\":86786,\"start\":86781},{\"end\":86796,\"start\":86792},{\"end\":86810,\"start\":86805},{\"end\":86824,\"start\":86818},{\"end\":86839,\"start\":86831},{\"end\":86857,\"start\":86851},{\"end\":86874,\"start\":86867},{\"end\":86892,\"start\":86883},{\"end\":86903,\"start\":86899},{\"end\":86917,\"start\":86912},{\"end\":86929,\"start\":86925},{\"end\":86943,\"start\":86937},{\"end\":86954,\"start\":86951},{\"end\":86966,\"start\":86964},{\"end\":86979,\"start\":86974},{\"end\":86992,\"start\":86989},{\"end\":87004,\"start\":86999},{\"end\":87017,\"start\":87011},{\"end\":87033,\"start\":87026},{\"end\":87051,\"start\":87043},{\"end\":87073,\"start\":87059},{\"end\":87092,\"start\":87083},{\"end\":87107,\"start\":87101},{\"end\":87118,\"start\":87114},{\"end\":87135,\"start\":87129},{\"end\":87141,\"start\":87137},{\"end\":89049,\"start\":89045},{\"end\":89062,\"start\":89056},{\"end\":89077,\"start\":89070},{\"end\":89091,\"start\":89085},{\"end\":89106,\"start\":89099},{\"end\":89391,\"start\":89383},{\"end\":89403,\"start\":89399},{\"end\":89419,\"start\":89413},{\"end\":89436,\"start\":89429},{\"end\":89453,\"start\":89444},{\"end\":89473,\"start\":89464},{\"end\":89975,\"start\":89972},{\"end\":89992,\"start\":89985},{\"end\":90008,\"start\":90004},{\"end\":90037,\"start\":90025},{\"end\":90052,\"start\":90046},{\"end\":90071,\"start\":90061},{\"end\":90562,\"start\":90557},{\"end\":90571,\"start\":90566},{\"end\":90581,\"start\":90575},{\"end\":90592,\"start\":90585},{\"end\":90600,\"start\":90596},{\"end\":91184,\"start\":91182},{\"end\":91199,\"start\":91195},{\"end\":91210,\"start\":91208},{\"end\":91218,\"start\":91216},{\"end\":91557,\"start\":91550},{\"end\":91580,\"start\":91565},{\"end\":91595,\"start\":91588},{\"end\":91618,\"start\":91606},{\"end\":91633,\"start\":91627},{\"end\":91651,\"start\":91640},{\"end\":91666,\"start\":91660},{\"end\":92193,\"start\":92180},{\"end\":92214,\"start\":92209},{\"end\":92235,\"start\":92224},{\"end\":92247,\"start\":92241},{\"end\":92261,\"start\":92254},{\"end\":92280,\"start\":92268},{\"end\":93029,\"start\":93019},{\"end\":93154,\"start\":93133},{\"end\":93324,\"start\":93319},{\"end\":93333,\"start\":93331},{\"end\":93347,\"start\":93343},{\"end\":93361,\"start\":93358},{\"end\":93370,\"start\":93367},{\"end\":93383,\"start\":93380},{\"end\":93398,\"start\":93394},{\"end\":93410,\"start\":93406},{\"end\":93415,\"start\":93412},{\"end\":94034,\"start\":94030},{\"end\":94049,\"start\":94041},{\"end\":94063,\"start\":94055},{\"end\":94082,\"start\":94072},{\"end\":94324,\"start\":94319},{\"end\":94333,\"start\":94328},{\"end\":94343,\"start\":94337},{\"end\":94354,\"start\":94347},{\"end\":94369,\"start\":94358},{\"end\":94375,\"start\":94373},{\"end\":94387,\"start\":94379},{\"end\":94398,\"start\":94391},{\"end\":94412,\"start\":94402},{\"end\":94420,\"start\":94416},{\"end\":94430,\"start\":94424},{\"end\":94441,\"start\":94434},{\"end\":94450,\"start\":94445},{\"end\":94458,\"start\":94454},{\"end\":94468,\"start\":94462},{\"end\":94476,\"start\":94472},{\"end\":94484,\"start\":94480},{\"end\":94497,\"start\":94490},{\"end\":94507,\"start\":94501},{\"end\":94518,\"start\":94511},{\"end\":94530,\"start\":94524},{\"end\":94538,\"start\":94534},{\"end\":94548,\"start\":94544},{\"end\":94558,\"start\":94552},{\"end\":94565,\"start\":94562},{\"end\":94573,\"start\":94569},{\"end\":94586,\"start\":94577},{\"end\":94595,\"start\":94590},{\"end\":94603,\"start\":94599},{\"end\":94619,\"start\":94607},{\"end\":94630,\"start\":94623},{\"end\":94644,\"start\":94634},{\"end\":94657,\"start\":94652},{\"end\":94669,\"start\":94661},{\"end\":94680,\"start\":94673},{\"end\":94687,\"start\":94684},{\"end\":94695,\"start\":94691},{\"end\":94706,\"start\":94699},{\"end\":94713,\"start\":94710},{\"end\":94719,\"start\":94717},{\"end\":94725,\"start\":94723},{\"end\":94735,\"start\":94729},{\"end\":94741,\"start\":94739},{\"end\":94749,\"start\":94745},{\"end\":94758,\"start\":94753},{\"end\":94767,\"start\":94762},{\"end\":94775,\"start\":94771},{\"end\":95615,\"start\":95609},{\"end\":95637,\"start\":95627},{\"end\":95643,\"start\":95639},{\"end\":96266,\"start\":96260},{\"end\":96493,\"start\":96486},{\"end\":96510,\"start\":96503},{\"end\":96523,\"start\":96518},{\"end\":96536,\"start\":96531},{\"end\":96548,\"start\":96543},{\"end\":96561,\"start\":96556},{\"end\":96579,\"start\":96571},{\"end\":96595,\"start\":96587},{\"end\":96613,\"start\":96602},{\"end\":96632,\"start\":96623},{\"end\":97206,\"start\":97202},{\"end\":97218,\"start\":97214},{\"end\":97230,\"start\":97227},{\"end\":97244,\"start\":97240},{\"end\":97257,\"start\":97253},{\"end\":97276,\"start\":97267},{\"end\":97298,\"start\":97285},{\"end\":97313,\"start\":97305},{\"end\":97981,\"start\":97971},{\"end\":97995,\"start\":97987},{\"end\":98012,\"start\":98004},{\"end\":98029,\"start\":98022},{\"end\":98045,\"start\":98039},{\"end\":98060,\"start\":98055},{\"end\":98636,\"start\":98624},{\"end\":98651,\"start\":98647},{\"end\":98669,\"start\":98661},{\"end\":98685,\"start\":98680},{\"end\":98697,\"start\":98691},{\"end\":99211,\"start\":99197},{\"end\":99228,\"start\":99219},{\"end\":99689,\"start\":99685},{\"end\":99705,\"start\":99697},{\"end\":99726,\"start\":99720},{\"end\":99745,\"start\":99736},{\"end\":99759,\"start\":99753},{\"end\":99772,\"start\":99769},{\"end\":99788,\"start\":99783},{\"end\":99799,\"start\":99795},{\"end\":99811,\"start\":99807},{\"end\":99825,\"start\":99820},{\"end\":99835,\"start\":99831},{\"end\":99848,\"start\":99844},{\"end\":100527,\"start\":100520},{\"end\":100548,\"start\":100539},{\"end\":100561,\"start\":100555},{\"end\":100574,\"start\":100572},{\"end\":100585,\"start\":100576},{\"end\":101138,\"start\":101136},{\"end\":101158,\"start\":101149},{\"end\":101433,\"start\":101428},{\"end\":101445,\"start\":101443},{\"end\":101455,\"start\":101451},{\"end\":101465,\"start\":101462},{\"end\":101906,\"start\":101901},{\"end\":101931,\"start\":101915},{\"end\":101944,\"start\":101939},{\"end\":101963,\"start\":101954},{\"end\":101978,\"start\":101974},{\"end\":101998,\"start\":101990},{\"end\":102537,\"start\":102532},{\"end\":102553,\"start\":102545},{\"end\":102567,\"start\":102562},{\"end\":102579,\"start\":102574},{\"end\":102595,\"start\":102591}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":68162,\"start\":68050},{\"attributes\":{\"id\":\"b1\"},\"end\":68201,\"start\":68164},{\"attributes\":{\"id\":\"b2\"},\"end\":68356,\"start\":68203},{\"attributes\":{\"id\":\"b3\"},\"end\":68414,\"start\":68358},{\"attributes\":{\"id\":\"b4\"},\"end\":68569,\"start\":68416},{\"attributes\":{\"id\":\"b5\"},\"end\":68694,\"start\":68571},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":699198},\"end\":69326,\"start\":68696},{\"attributes\":{\"id\":\"b7\"},\"end\":69388,\"start\":69328},{\"attributes\":{\"id\":\"b8\"},\"end\":69447,\"start\":69390},{\"attributes\":{\"id\":\"b9\"},\"end\":69527,\"start\":69449},{\"attributes\":{\"id\":\"b10\"},\"end\":69620,\"start\":69529},{\"attributes\":{\"id\":\"b11\"},\"end\":69744,\"start\":69622},{\"attributes\":{\"id\":\"b12\"},\"end\":70096,\"start\":69746},{\"attributes\":{\"id\":\"b13\"},\"end\":70162,\"start\":70098},{\"attributes\":{\"id\":\"b14\"},\"end\":70411,\"start\":70164},{\"attributes\":{\"id\":\"b15\"},\"end\":70502,\"start\":70413},{\"attributes\":{\"id\":\"b16\"},\"end\":70597,\"start\":70504},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52903173},\"end\":70914,\"start\":70599},{\"attributes\":{\"id\":\"b18\"},\"end\":71147,\"start\":70916},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":614862},\"end\":71456,\"start\":71149},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52939079},\"end\":72185,\"start\":71458},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":222296313},\"end\":72750,\"start\":72187},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1701442},\"end\":73281,\"start\":72752},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":69753830},\"end\":74164,\"start\":73283},{\"attributes\":{\"doi\":\"abs/1811.09732\",\"id\":\"b24\"},\"end\":74624,\"start\":74166},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1733167},\"end\":75157,\"start\":74626},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207209661},\"end\":75823,\"start\":75159},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":57246310},\"end\":76092,\"start\":75825},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207193689},\"end\":76667,\"start\":76094},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":49568000},\"end\":77740,\"start\":76669},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3036526},\"end\":78148,\"start\":77742},{\"attributes\":{\"id\":\"b31\"},\"end\":78285,\"start\":78150},{\"attributes\":{\"id\":\"b32\"},\"end\":78423,\"start\":78287},{\"attributes\":{\"id\":\"b33\"},\"end\":78526,\"start\":78425},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":73725044},\"end\":79137,\"start\":78528},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8367854},\"end\":79738,\"start\":79139},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":219303421},\"end\":80334,\"start\":79740},{\"attributes\":{\"id\":\"b37\"},\"end\":80791,\"start\":80336},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":174801216},\"end\":81457,\"start\":80793},{\"attributes\":{\"id\":\"b39\"},\"end\":81586,\"start\":81459},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":133608086},\"end\":82316,\"start\":81588},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3541031},\"end\":83302,\"start\":82318},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":892222},\"end\":84038,\"start\":83304},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":31635004},\"end\":84716,\"start\":84040},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":57373811},\"end\":85189,\"start\":84718},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":51766446},\"end\":85824,\"start\":85191},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":854354},\"end\":86326,\"start\":85826},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":4202768},\"end\":88972,\"start\":86328},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":20732104},\"end\":89317,\"start\":88974},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52984442},\"end\":89882,\"start\":89319},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":52984791},\"end\":90515,\"start\":89884},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10524544},\"end\":91057,\"start\":90517},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":17377344},\"end\":91488,\"start\":91059},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":208104372},\"end\":92096,\"start\":91490},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":203593417},\"end\":93015,\"start\":92098},{\"attributes\":{\"id\":\"b55\"},\"end\":93125,\"start\":93017},{\"attributes\":{\"id\":\"b56\"},\"end\":93252,\"start\":93127},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":52965889},\"end\":93979,\"start\":93254},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":29155614},\"end\":94285,\"start\":93981},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":207880425},\"end\":95517,\"start\":94287},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":51898967},\"end\":96214,\"start\":95519},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":14164016},\"end\":96370,\"start\":96216},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":212633767},\"end\":97121,\"start\":96372},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":204812163},\"end\":97869,\"start\":97123},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":52191179},\"end\":98536,\"start\":97871},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":14366444},\"end\":99119,\"start\":98538},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":123362751},\"end\":99466,\"start\":99121},{\"attributes\":{\"id\":\"b67\"},\"end\":99614,\"start\":99468},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":52987896},\"end\":100335,\"start\":99616},{\"attributes\":{\"id\":\"b69\"},\"end\":100463,\"start\":100337},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":153314676},\"end\":101051,\"start\":100465},{\"attributes\":{\"doi\":\"abs/1902.04610\",\"id\":\"b71\"},\"end\":101317,\"start\":101053},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":196810567},\"end\":101823,\"start\":101319},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":11894138},\"end\":102431,\"start\":101825},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":220835829},\"end\":102956,\"start\":102433}]", "bib_title": "[{\"end\":68782,\"start\":68696},{\"end\":70669,\"start\":70599},{\"end\":71229,\"start\":71149},{\"end\":71524,\"start\":71458},{\"end\":72269,\"start\":72187},{\"end\":72807,\"start\":72752},{\"end\":73350,\"start\":73283},{\"end\":74695,\"start\":74626},{\"end\":75217,\"start\":75159},{\"end\":75876,\"start\":75825},{\"end\":76150,\"start\":76094},{\"end\":76726,\"start\":76669},{\"end\":77815,\"start\":77742},{\"end\":78589,\"start\":78528},{\"end\":79247,\"start\":79139},{\"end\":79814,\"start\":79740},{\"end\":80875,\"start\":80793},{\"end\":81729,\"start\":81588},{\"end\":82395,\"start\":82318},{\"end\":83374,\"start\":83304},{\"end\":84106,\"start\":84040},{\"end\":84764,\"start\":84718},{\"end\":85240,\"start\":85191},{\"end\":85877,\"start\":85826},{\"end\":86390,\"start\":86328},{\"end\":89036,\"start\":88974},{\"end\":89377,\"start\":89319},{\"end\":89961,\"start\":89884},{\"end\":90553,\"start\":90517},{\"end\":91176,\"start\":91059},{\"end\":91539,\"start\":91490},{\"end\":92165,\"start\":92098},{\"end\":93315,\"start\":93254},{\"end\":94023,\"start\":93981},{\"end\":94313,\"start\":94287},{\"end\":95597,\"start\":95519},{\"end\":96249,\"start\":96216},{\"end\":96475,\"start\":96372},{\"end\":97192,\"start\":97123},{\"end\":97962,\"start\":97871},{\"end\":98613,\"start\":98538},{\"end\":99187,\"start\":99121},{\"end\":99675,\"start\":99616},{\"end\":100516,\"start\":100465},{\"end\":101415,\"start\":101319},{\"end\":101893,\"start\":101825},{\"end\":102525,\"start\":102433}]", "bib_author": "[{\"end\":68367,\"start\":68360},{\"end\":68802,\"start\":68784},{\"end\":68819,\"start\":68802},{\"end\":68832,\"start\":68819},{\"end\":68847,\"start\":68832},{\"end\":68868,\"start\":68847},{\"end\":68877,\"start\":68868},{\"end\":68884,\"start\":68877},{\"end\":69342,\"start\":69330},{\"end\":69403,\"start\":69392},{\"end\":69469,\"start\":69451},{\"end\":69553,\"start\":69531},{\"end\":69849,\"start\":69833},{\"end\":69863,\"start\":69849},{\"end\":69876,\"start\":69863},{\"end\":69893,\"start\":69876},{\"end\":70432,\"start\":70415},{\"end\":70522,\"start\":70506},{\"end\":70686,\"start\":70671},{\"end\":70699,\"start\":70686},{\"end\":70713,\"start\":70699},{\"end\":70731,\"start\":70713},{\"end\":70933,\"start\":70918},{\"end\":70946,\"start\":70933},{\"end\":70965,\"start\":70946},{\"end\":70978,\"start\":70965},{\"end\":70991,\"start\":70978},{\"end\":71008,\"start\":70991},{\"end\":71015,\"start\":71008},{\"end\":71251,\"start\":71231},{\"end\":71269,\"start\":71251},{\"end\":71278,\"start\":71269},{\"end\":71284,\"start\":71278},{\"end\":71539,\"start\":71526},{\"end\":71555,\"start\":71539},{\"end\":71569,\"start\":71555},{\"end\":71584,\"start\":71569},{\"end\":71595,\"start\":71584},{\"end\":71609,\"start\":71595},{\"end\":71623,\"start\":71609},{\"end\":71636,\"start\":71623},{\"end\":71646,\"start\":71636},{\"end\":71657,\"start\":71646},{\"end\":71674,\"start\":71657},{\"end\":71696,\"start\":71674},{\"end\":72289,\"start\":72271},{\"end\":72299,\"start\":72289},{\"end\":72313,\"start\":72299},{\"end\":72323,\"start\":72313},{\"end\":72334,\"start\":72323},{\"end\":72349,\"start\":72334},{\"end\":72366,\"start\":72349},{\"end\":72375,\"start\":72366},{\"end\":72827,\"start\":72809},{\"end\":72837,\"start\":72827},{\"end\":72850,\"start\":72837},{\"end\":72870,\"start\":72850},{\"end\":72889,\"start\":72870},{\"end\":72901,\"start\":72889},{\"end\":73366,\"start\":73352},{\"end\":73382,\"start\":73366},{\"end\":73406,\"start\":73382},{\"end\":73418,\"start\":73406},{\"end\":73439,\"start\":73418},{\"end\":73453,\"start\":73439},{\"end\":73476,\"start\":73453},{\"end\":73489,\"start\":73476},{\"end\":73501,\"start\":73489},{\"end\":73516,\"start\":73501},{\"end\":73527,\"start\":73516},{\"end\":73547,\"start\":73527},{\"end\":73562,\"start\":73547},{\"end\":73575,\"start\":73562},{\"end\":73595,\"start\":73575},{\"end\":74305,\"start\":74291},{\"end\":74315,\"start\":74305},{\"end\":74340,\"start\":74315},{\"end\":74354,\"start\":74340},{\"end\":74369,\"start\":74354},{\"end\":74714,\"start\":74697},{\"end\":74729,\"start\":74714},{\"end\":74743,\"start\":74729},{\"end\":74757,\"start\":74743},{\"end\":75241,\"start\":75219},{\"end\":75261,\"start\":75241},{\"end\":75888,\"start\":75878},{\"end\":75898,\"start\":75888},{\"end\":75914,\"start\":75898},{\"end\":75929,\"start\":75914},{\"end\":75936,\"start\":75929},{\"end\":75945,\"start\":75936},{\"end\":76171,\"start\":76152},{\"end\":76184,\"start\":76171},{\"end\":76202,\"start\":76184},{\"end\":76215,\"start\":76202},{\"end\":76232,\"start\":76215},{\"end\":76743,\"start\":76728},{\"end\":76760,\"start\":76743},{\"end\":76781,\"start\":76760},{\"end\":76798,\"start\":76781},{\"end\":76808,\"start\":76798},{\"end\":76819,\"start\":76808},{\"end\":76835,\"start\":76819},{\"end\":76853,\"start\":76835},{\"end\":76866,\"start\":76853},{\"end\":76880,\"start\":76866},{\"end\":76894,\"start\":76880},{\"end\":76908,\"start\":76894},{\"end\":76920,\"start\":76908},{\"end\":76935,\"start\":76920},{\"end\":76947,\"start\":76935},{\"end\":76962,\"start\":76947},{\"end\":76982,\"start\":76962},{\"end\":77002,\"start\":76982},{\"end\":77016,\"start\":77002},{\"end\":77029,\"start\":77016},{\"end\":77832,\"start\":77817},{\"end\":77852,\"start\":77832},{\"end\":77869,\"start\":77852},{\"end\":77887,\"start\":77869},{\"end\":78163,\"start\":78152},{\"end\":78178,\"start\":78163},{\"end\":78187,\"start\":78178},{\"end\":78441,\"start\":78427},{\"end\":78455,\"start\":78441},{\"end\":78604,\"start\":78591},{\"end\":78624,\"start\":78604},{\"end\":78632,\"start\":78624},{\"end\":78643,\"start\":78632},{\"end\":78658,\"start\":78643},{\"end\":78671,\"start\":78658},{\"end\":78687,\"start\":78671},{\"end\":78703,\"start\":78687},{\"end\":78708,\"start\":78703},{\"end\":79265,\"start\":79249},{\"end\":79281,\"start\":79265},{\"end\":79293,\"start\":79281},{\"end\":79304,\"start\":79293},{\"end\":79314,\"start\":79304},{\"end\":79323,\"start\":79314},{\"end\":79336,\"start\":79323},{\"end\":79832,\"start\":79816},{\"end\":79845,\"start\":79832},{\"end\":79860,\"start\":79845},{\"end\":79869,\"start\":79860},{\"end\":79887,\"start\":79869},{\"end\":79903,\"start\":79887},{\"end\":79918,\"start\":79903},{\"end\":80348,\"start\":80336},{\"end\":80361,\"start\":80348},{\"end\":80376,\"start\":80361},{\"end\":80391,\"start\":80376},{\"end\":80407,\"start\":80391},{\"end\":80416,\"start\":80407},{\"end\":80421,\"start\":80416},{\"end\":80435,\"start\":80421},{\"end\":80446,\"start\":80435},{\"end\":80466,\"start\":80446},{\"end\":80470,\"start\":80466},{\"end\":80889,\"start\":80877},{\"end\":80905,\"start\":80889},{\"end\":80920,\"start\":80905},{\"end\":80934,\"start\":80920},{\"end\":80950,\"start\":80934},{\"end\":80964,\"start\":80950},{\"end\":80981,\"start\":80964},{\"end\":80996,\"start\":80981},{\"end\":81012,\"start\":80996},{\"end\":81022,\"start\":81012},{\"end\":81742,\"start\":81731},{\"end\":81758,\"start\":81742},{\"end\":81769,\"start\":81758},{\"end\":81784,\"start\":81769},{\"end\":81793,\"start\":81784},{\"end\":82412,\"start\":82397},{\"end\":82424,\"start\":82412},{\"end\":82438,\"start\":82424},{\"end\":82456,\"start\":82438},{\"end\":82468,\"start\":82456},{\"end\":82487,\"start\":82468},{\"end\":82502,\"start\":82487},{\"end\":82512,\"start\":82502},{\"end\":82526,\"start\":82512},{\"end\":82540,\"start\":82526},{\"end\":82551,\"start\":82540},{\"end\":82562,\"start\":82551},{\"end\":82572,\"start\":82562},{\"end\":82590,\"start\":82572},{\"end\":82609,\"start\":82590},{\"end\":82622,\"start\":82609},{\"end\":82637,\"start\":82622},{\"end\":83394,\"start\":83376},{\"end\":83410,\"start\":83394},{\"end\":83425,\"start\":83410},{\"end\":83437,\"start\":83425},{\"end\":83455,\"start\":83437},{\"end\":83467,\"start\":83455},{\"end\":83482,\"start\":83467},{\"end\":83494,\"start\":83482},{\"end\":84121,\"start\":84108},{\"end\":84146,\"start\":84121},{\"end\":84159,\"start\":84146},{\"end\":84182,\"start\":84159},{\"end\":84197,\"start\":84182},{\"end\":84216,\"start\":84197},{\"end\":84235,\"start\":84216},{\"end\":84247,\"start\":84235},{\"end\":84778,\"start\":84766},{\"end\":84790,\"start\":84778},{\"end\":84801,\"start\":84790},{\"end\":84821,\"start\":84801},{\"end\":84836,\"start\":84821},{\"end\":84852,\"start\":84836},{\"end\":84869,\"start\":84852},{\"end\":84881,\"start\":84869},{\"end\":85257,\"start\":85242},{\"end\":85282,\"start\":85257},{\"end\":85295,\"start\":85282},{\"end\":85311,\"start\":85295},{\"end\":85323,\"start\":85311},{\"end\":85891,\"start\":85879},{\"end\":85901,\"start\":85891},{\"end\":85924,\"start\":85901},{\"end\":85936,\"start\":85924},{\"end\":85952,\"start\":85936},{\"end\":86409,\"start\":86392},{\"end\":86422,\"start\":86409},{\"end\":86437,\"start\":86422},{\"end\":86454,\"start\":86437},{\"end\":86470,\"start\":86454},{\"end\":86486,\"start\":86470},{\"end\":86499,\"start\":86486},{\"end\":86514,\"start\":86499},{\"end\":86525,\"start\":86514},{\"end\":86538,\"start\":86525},{\"end\":86550,\"start\":86538},{\"end\":86569,\"start\":86550},{\"end\":86584,\"start\":86569},{\"end\":86597,\"start\":86584},{\"end\":86613,\"start\":86597},{\"end\":86625,\"start\":86613},{\"end\":86635,\"start\":86625},{\"end\":86649,\"start\":86635},{\"end\":86659,\"start\":86649},{\"end\":86684,\"start\":86659},{\"end\":86704,\"start\":86684},{\"end\":86721,\"start\":86704},{\"end\":86737,\"start\":86721},{\"end\":86751,\"start\":86737},{\"end\":86765,\"start\":86751},{\"end\":86774,\"start\":86765},{\"end\":86788,\"start\":86774},{\"end\":86798,\"start\":86788},{\"end\":86812,\"start\":86798},{\"end\":86826,\"start\":86812},{\"end\":86841,\"start\":86826},{\"end\":86859,\"start\":86841},{\"end\":86876,\"start\":86859},{\"end\":86894,\"start\":86876},{\"end\":86905,\"start\":86894},{\"end\":86919,\"start\":86905},{\"end\":86931,\"start\":86919},{\"end\":86945,\"start\":86931},{\"end\":86956,\"start\":86945},{\"end\":86968,\"start\":86956},{\"end\":86981,\"start\":86968},{\"end\":86994,\"start\":86981},{\"end\":87006,\"start\":86994},{\"end\":87019,\"start\":87006},{\"end\":87035,\"start\":87019},{\"end\":87053,\"start\":87035},{\"end\":87075,\"start\":87053},{\"end\":87094,\"start\":87075},{\"end\":87109,\"start\":87094},{\"end\":87120,\"start\":87109},{\"end\":87137,\"start\":87120},{\"end\":87143,\"start\":87137},{\"end\":89051,\"start\":89038},{\"end\":89064,\"start\":89051},{\"end\":89079,\"start\":89064},{\"end\":89093,\"start\":89079},{\"end\":89108,\"start\":89093},{\"end\":89393,\"start\":89379},{\"end\":89405,\"start\":89393},{\"end\":89421,\"start\":89405},{\"end\":89438,\"start\":89421},{\"end\":89455,\"start\":89438},{\"end\":89475,\"start\":89455},{\"end\":89977,\"start\":89963},{\"end\":89994,\"start\":89977},{\"end\":90010,\"start\":89994},{\"end\":90039,\"start\":90010},{\"end\":90054,\"start\":90039},{\"end\":90073,\"start\":90054},{\"end\":90564,\"start\":90555},{\"end\":90573,\"start\":90564},{\"end\":90583,\"start\":90573},{\"end\":90594,\"start\":90583},{\"end\":90602,\"start\":90594},{\"end\":91186,\"start\":91178},{\"end\":91201,\"start\":91186},{\"end\":91212,\"start\":91201},{\"end\":91220,\"start\":91212},{\"end\":91559,\"start\":91541},{\"end\":91582,\"start\":91559},{\"end\":91597,\"start\":91582},{\"end\":91620,\"start\":91597},{\"end\":91635,\"start\":91620},{\"end\":91653,\"start\":91635},{\"end\":91668,\"start\":91653},{\"end\":92195,\"start\":92167},{\"end\":92216,\"start\":92195},{\"end\":92237,\"start\":92216},{\"end\":92249,\"start\":92237},{\"end\":92263,\"start\":92249},{\"end\":92282,\"start\":92263},{\"end\":93031,\"start\":93019},{\"end\":93156,\"start\":93129},{\"end\":93326,\"start\":93317},{\"end\":93335,\"start\":93326},{\"end\":93349,\"start\":93335},{\"end\":93363,\"start\":93349},{\"end\":93372,\"start\":93363},{\"end\":93385,\"start\":93372},{\"end\":93400,\"start\":93385},{\"end\":93412,\"start\":93400},{\"end\":93417,\"start\":93412},{\"end\":94036,\"start\":94025},{\"end\":94051,\"start\":94036},{\"end\":94065,\"start\":94051},{\"end\":94084,\"start\":94065},{\"end\":94326,\"start\":94315},{\"end\":94335,\"start\":94326},{\"end\":94345,\"start\":94335},{\"end\":94356,\"start\":94345},{\"end\":94371,\"start\":94356},{\"end\":94377,\"start\":94371},{\"end\":94389,\"start\":94377},{\"end\":94400,\"start\":94389},{\"end\":94414,\"start\":94400},{\"end\":94422,\"start\":94414},{\"end\":94432,\"start\":94422},{\"end\":94443,\"start\":94432},{\"end\":94452,\"start\":94443},{\"end\":94460,\"start\":94452},{\"end\":94470,\"start\":94460},{\"end\":94478,\"start\":94470},{\"end\":94486,\"start\":94478},{\"end\":94499,\"start\":94486},{\"end\":94509,\"start\":94499},{\"end\":94520,\"start\":94509},{\"end\":94532,\"start\":94520},{\"end\":94540,\"start\":94532},{\"end\":94550,\"start\":94540},{\"end\":94560,\"start\":94550},{\"end\":94567,\"start\":94560},{\"end\":94575,\"start\":94567},{\"end\":94588,\"start\":94575},{\"end\":94597,\"start\":94588},{\"end\":94605,\"start\":94597},{\"end\":94621,\"start\":94605},{\"end\":94632,\"start\":94621},{\"end\":94646,\"start\":94632},{\"end\":94659,\"start\":94646},{\"end\":94671,\"start\":94659},{\"end\":94682,\"start\":94671},{\"end\":94689,\"start\":94682},{\"end\":94697,\"start\":94689},{\"end\":94708,\"start\":94697},{\"end\":94715,\"start\":94708},{\"end\":94721,\"start\":94715},{\"end\":94727,\"start\":94721},{\"end\":94737,\"start\":94727},{\"end\":94743,\"start\":94737},{\"end\":94751,\"start\":94743},{\"end\":94760,\"start\":94751},{\"end\":94769,\"start\":94760},{\"end\":94777,\"start\":94769},{\"end\":95617,\"start\":95599},{\"end\":95639,\"start\":95617},{\"end\":95645,\"start\":95639},{\"end\":96268,\"start\":96251},{\"end\":96495,\"start\":96477},{\"end\":96512,\"start\":96495},{\"end\":96525,\"start\":96512},{\"end\":96538,\"start\":96525},{\"end\":96550,\"start\":96538},{\"end\":96563,\"start\":96550},{\"end\":96581,\"start\":96563},{\"end\":96597,\"start\":96581},{\"end\":96615,\"start\":96597},{\"end\":96634,\"start\":96615},{\"end\":97208,\"start\":97194},{\"end\":97220,\"start\":97208},{\"end\":97232,\"start\":97220},{\"end\":97246,\"start\":97232},{\"end\":97259,\"start\":97246},{\"end\":97278,\"start\":97259},{\"end\":97300,\"start\":97278},{\"end\":97315,\"start\":97300},{\"end\":97983,\"start\":97964},{\"end\":97997,\"start\":97983},{\"end\":98014,\"start\":97997},{\"end\":98031,\"start\":98014},{\"end\":98047,\"start\":98031},{\"end\":98062,\"start\":98047},{\"end\":98638,\"start\":98615},{\"end\":98653,\"start\":98638},{\"end\":98671,\"start\":98653},{\"end\":98687,\"start\":98671},{\"end\":98699,\"start\":98687},{\"end\":99213,\"start\":99189},{\"end\":99230,\"start\":99213},{\"end\":99691,\"start\":99677},{\"end\":99707,\"start\":99691},{\"end\":99728,\"start\":99707},{\"end\":99747,\"start\":99728},{\"end\":99761,\"start\":99747},{\"end\":99774,\"start\":99761},{\"end\":99790,\"start\":99774},{\"end\":99801,\"start\":99790},{\"end\":99813,\"start\":99801},{\"end\":99827,\"start\":99813},{\"end\":99837,\"start\":99827},{\"end\":99850,\"start\":99837},{\"end\":100529,\"start\":100518},{\"end\":100550,\"start\":100529},{\"end\":100563,\"start\":100550},{\"end\":100576,\"start\":100563},{\"end\":100587,\"start\":100576},{\"end\":101140,\"start\":101128},{\"end\":101160,\"start\":101140},{\"end\":101435,\"start\":101417},{\"end\":101447,\"start\":101435},{\"end\":101457,\"start\":101447},{\"end\":101467,\"start\":101457},{\"end\":101908,\"start\":101895},{\"end\":101933,\"start\":101908},{\"end\":101946,\"start\":101933},{\"end\":101965,\"start\":101946},{\"end\":101980,\"start\":101965},{\"end\":102000,\"start\":101980},{\"end\":102539,\"start\":102527},{\"end\":102555,\"start\":102539},{\"end\":102569,\"start\":102555},{\"end\":102581,\"start\":102569},{\"end\":102597,\"start\":102581}]", "bib_venue": "[{\"end\":68072,\"start\":68050},{\"end\":68170,\"start\":68166},{\"end\":68256,\"start\":68205},{\"end\":68469,\"start\":68416},{\"end\":68603,\"start\":68573},{\"end\":68962,\"start\":68884},{\"end\":69646,\"start\":69622},{\"end\":69831,\"start\":69746},{\"end\":70110,\"start\":70100},{\"end\":70221,\"start\":70164},{\"end\":70742,\"start\":70731},{\"end\":71289,\"start\":71284},{\"end\":71774,\"start\":71696},{\"end\":72431,\"start\":72375},{\"end\":72969,\"start\":72901},{\"end\":73673,\"start\":73595},{\"end\":74289,\"start\":74166},{\"end\":74838,\"start\":74757},{\"end\":75394,\"start\":75261},{\"end\":75949,\"start\":75945},{\"end\":76311,\"start\":76232},{\"end\":77118,\"start\":77029},{\"end\":77930,\"start\":77887},{\"end\":78215,\"start\":78187},{\"end\":78327,\"start\":78287},{\"end\":78786,\"start\":78708},{\"end\":79397,\"start\":79336},{\"end\":79996,\"start\":79918},{\"end\":80557,\"start\":80470},{\"end\":81104,\"start\":81022},{\"end\":81519,\"start\":81461},{\"end\":81897,\"start\":81793},{\"end\":82748,\"start\":82637},{\"end\":83590,\"start\":83494},{\"end\":84325,\"start\":84247},{\"end\":84942,\"start\":84881},{\"end\":85426,\"start\":85323},{\"end\":86014,\"start\":85952},{\"end\":87232,\"start\":87143},{\"end\":89124,\"start\":89108},{\"end\":89553,\"start\":89475},{\"end\":90151,\"start\":90073},{\"end\":90682,\"start\":90602},{\"end\":91255,\"start\":91220},{\"end\":91746,\"start\":91668},{\"end\":92423,\"start\":92282},{\"end\":93528,\"start\":93417},{\"end\":94118,\"start\":94084},{\"end\":94858,\"start\":94777},{\"end\":95756,\"start\":95645},{\"end\":96274,\"start\":96268},{\"end\":96689,\"start\":96634},{\"end\":97394,\"start\":97315},{\"end\":98156,\"start\":98062},{\"end\":98777,\"start\":98699},{\"end\":99248,\"start\":99230},{\"end\":99524,\"start\":99468},{\"end\":99928,\"start\":99850},{\"end\":100382,\"start\":100337},{\"end\":100660,\"start\":100587},{\"end\":101126,\"start\":101053},{\"end\":101522,\"start\":101467},{\"end\":102078,\"start\":102000},{\"end\":102684,\"start\":102597},{\"end\":68974,\"start\":68964},{\"end\":71788,\"start\":71776},{\"end\":72474,\"start\":72433},{\"end\":72986,\"start\":72971},{\"end\":73685,\"start\":73675},{\"end\":74906,\"start\":74840},{\"end\":75531,\"start\":75396},{\"end\":76394,\"start\":76313},{\"end\":77213,\"start\":77120},{\"end\":78798,\"start\":78788},{\"end\":79445,\"start\":79399},{\"end\":81988,\"start\":81899},{\"end\":82846,\"start\":82750},{\"end\":83676,\"start\":83592},{\"end\":84339,\"start\":84327},{\"end\":85533,\"start\":85428},{\"end\":86080,\"start\":86016},{\"end\":87664,\"start\":87350},{\"end\":89567,\"start\":89555},{\"end\":90165,\"start\":90153},{\"end\":90766,\"start\":90684},{\"end\":91763,\"start\":91748},{\"end\":92568,\"start\":92425},{\"end\":93643,\"start\":93530},{\"end\":95871,\"start\":95758},{\"end\":96706,\"start\":96691},{\"end\":97477,\"start\":97396},{\"end\":98179,\"start\":98158},{\"end\":98794,\"start\":98779},{\"end\":99253,\"start\":99250},{\"end\":99942,\"start\":99930},{\"end\":100737,\"start\":100662},{\"end\":101534,\"start\":101524},{\"end\":102090,\"start\":102080}]"}}}, "year": 2023, "month": 12, "day": 17}