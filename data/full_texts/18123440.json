{"id": 18123440, "updated": "2023-09-30 17:08:56.82", "metadata": {"title": "From Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur", "authors": "[{\"first\":\"Dong\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Lingqiao\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yanning\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ian\",\"last\":\"Reid\",\"middle\":[]},{\"first\":\"Chunhua\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Anton\",\"last\":\"Hengel\",\"middle\":[\"van\",\"den\"]},{\"first\":\"Qinfeng\",\"last\":\"Shi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2016, "month": 12, "day": 8}, "abstract": "Removing pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but the extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach is thus that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1612.02583", "mag": "2951680943", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/Gong0LZ0SHS16", "doi": "10.1109/cvpr.2017.405"}}, "content": {"source": {"pdf_hash": "c8a9b265f2f2da3bf4050a929d338f4d6727215a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1612.02583v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1612.02583", "status": "GREEN"}}, "grobid": {"id": "256242cf7805c9832957d20c987c87d4bb3dca70", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c8a9b265f2f2da3bf4050a929d338f4d6727215a.txt", "contents": "\nFrom Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur\n\n\nDong Gong \nSchool of Computer Science and Engineering\nNorthwestern Polytechnical University\nChina\n\nSchool of Computer Science\nThe University of Adelaide\n\n\nJie Yang \nSchool of Computer Science\nThe University of Adelaide\n\n\nLingqiao Liu \nSchool of Computer Science\nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nYanning Zhang ynzhang@nwpu.edu.cn \nSchool of Computer Science and Engineering\nNorthwestern Polytechnical University\nChina\n\nIan Reid \nSchool of Computer Science\nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nChunhua Shen \nSchool of Computer Science\nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nAnton Van Den Hengel \nSchool of Computer Science\nThe University of Adelaide\n\n\nAustralian Centre for Robotic Vision\n\n\nQinfeng Shi \nSchool of Computer Science\nThe University of Adelaide\n\n\nFrom Motion Blur to Motion Flow: a Deep Learning Solution for Removing Heterogeneous Motion Blur\n\nRemoving pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but the extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach is thus that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.\n\nIntroduction\n\nMotion blur is ubiquitous in photography, especially when using light-weight mobile devices, such as cellphones and on-board cameras. While there has been a significant progress on image deblurring [9,6,41,27,28,10], most work focuses on spatially-uniform blur. Some recent methods [39,12,14,18,26] have been proposed to remove spatially-varying blur caused by camera panning, and/or (a) Blurry image (b) Xu and Jia [41] (c) Sun et.al. [32] (d) Ours Figure 1. A blurry image with heterogeneous motion blur from a widely used dataset Microsoft COCO [23]. Estimated motion flows are shown in the bottom right corner of each image.\n\nobject movement, with some restrictive assumptions on the types of blur, image prior, or both. In this work, we focus on recovering a blur-free latent image from a single observation degraded by heterogeneous motion blur, i.e. the blur kernels may independently vary from pixel to pixel. Motion blur in real images has a variety of causes, including camera [39,46] and object motion [15,26], leading to blur patterns with complex variations (See Figure 1 (a)). In practice, uniform deblurring methods [9,6,41] usually fail to remove the non-uniform blur (See Figure 1 (b)). Most existing non-uniform deblurring methods rely on a specific motion model, such as 3D camera motion modeling [11,39] and segment-wise motion [20,26]. Although a recent method [18] uses a flexible motion flow map to handle heterogeneous motion blur, it requires a time-consuming iterative estimator. In addition to the assumptions about the cause of blur, most existing deblurring methods also rely on predefined priors or manually designed image features. Most conventional methods [9,22,43] need to iteratively update the intermediate image and the blur kernel with using these predefined image priors to reduce the illposedness. However, solving these non-convex problems is non-trivial, and many real images do not conform to the assumptions behind a particular model. Recently, learningbased discriminative methods [4,7] have been proposed to learn blur image patterns and avoid the heavy computational cost of blur estimation. However, their representation and prediction abilities are limited by their manually designed features and simple mapping functions. Although a deep learning based method [32] aimed to overcome these problems, it restrictively conducts the learning process at the patch-level and thus cannot take full advantage of the context information from larger image regions.\n\nIn summary, there are three main problems with existing approaches: 1) the range of applicable motion types is limited, 2) manually defined priors and image features may not reflect the nature of the data and 3) complicated and timeconsuming optimization and/or post-processing is required. Generally, these problems limit the practical applicability of blur removal methods to real images, as they tend to cause worse artifacts than they cure.\n\nTo handle general heterogeneous motion blur, based on the motion flow model, we propose a deep neural network based method able to directly estimate a pixel-wise motion flow map from a single blurred image by learning from tens of thousands of examples. To summarize, the main contributions of this paper are:\n\n\u2022 We propose an approach to estimate and remove pixelwise heterogeneous motion blur by training on simulated examples. Our method uses a flexible blur model and makes almost no assumptions about the underlying images, resulting in effectiveness on diverse data. \u2022 We introduce a universal FCN for end-to-end estimation of dense heterogeneous motion flow from a single blurry image. Beyond the previous patch-level learning [32], we directly perform training and testing on the whole image, which utilizes the spatial context over a wider area and estimates a dense motion flow map accurately. Moreover, our method does not require any post-processing.\n\n\nRelated Work\n\nConventional blind image deblurring To constrain the solution space for blind deblurring, a common assumption is that image blur is spatially uniform [5,6,9,22,28,10]. Meanwhile, numerous image priors or regularizers have been studied to overcome the ill-posed nature of the prob-lem, such as the total variational regularizer [5,29], Gaussian scale mixture priors [9] and 1 / 2 -norms [19], 0norms [43,27], and dark channel [28] based regularizers. Moreover, various estimators have been proposed for more robust kernel estimation, such as edge-extractionbased maximum-a-posteriori (MAP) [6,33], gradient activation based MAP [10], variational Bayesian methods [21,22,45], etc . Although these powerful priors and estimators work well on many benchmark datasets, they are often characterised by restrictive assumptions that limit their practical applicability.\n\nSpatially-varying blur removal To handle spatiallyvarying blur, more flexible blur models are proposed. In [34], a projective motion path model formulates a blurry image as the weighted sum of a set of transformed sharp images, an approach which is which is simplified and extended in [39] and [44]. Gupta et.al. [11] model the camera motion as a motion density function for non-uniform deblurring. Several locally uniform overlapping-patch-based models [13,12] are proposed to reduce the computational burden. Zheng et.al. [46] specifically modelled the blur caused by forward camera motion. To handle blur caused by object motion, some methods [20,8,15,26] segment images into areas with different types of blur, and are thus heavily dependent on an accruate segmentation of a blurred image. Recently, a pixel-wise linear motion model [18] is proposed to handle heterogeneous motion blur. Although the motion is assumed to be locally linear, there is no assumption on the latent motion, making it flexible enough to handle an extensive range of possible motion.\n\nLearning based motion blur removing Recently, learning based methods have been used to achieve more flexible and efficient blur removal. Some discriminative methods are proposed for non-blind deconvolution based on Gaussian CRF [30], multi-layer perceptron (MLP) [31], and deep convolution neural network (CNN) [42], etc, which all require the known blur kernels. Some end-to-end methods [17,25] are proposed to reconstruct blur-free images, however, they can only handle mild Gaussian blur. Recently, Wieschollek et.al. [40] introduce an MLP based blind deblurring method by using information in multiple images with small variations. Chakrabarti [3] trains a patch-based neural network to estimate the frequency information for uniform motion blur removal. The most relevant work is a method based on CNN and patch-level blur type classification [32], which also focuses on estimating the motion flow from single blurry image. The authors train a CNN on small patch examples with uniform motion blur, where each patch is assigned a single motion label, violating the real data nature and ignoring the correspondence in larger areas. Many post-processing such as MRF are required for the final dense motion flow. \n\n\nEstimating Motion Flow for Blur Removal\n\n\nA Heterogeneous Motion Blur Model\n\nLetting * denote a general convolution operator, a P \u00d7 Q blurred image Y can be modeled as\nY = K * X + N,(1)\nwhere X denotes the latent sharp image, N refers to additive noise, and K denotes a heterogeneous motion blur kernel map with different blur kernels for each pixel in X. Let K (i,j) represent the kernel from K that operates on a region of the image centered at pixel (i, j). Thus, at each pixel of Y, we have\nY(i, j) = i ,j K (i,j) (i , j )X(i + i , j + j ).(2)\nIf we define an operator vec(\u00b7) which vectorises a matrix and let y = vec(Y), x = vec(X) and n = vec(N) then (1) can also be represented as\ny = H(K)x + n,(3)\nwhere H(K) \u2208 R P Q\u00d7P Q1 and each row corresponds to a blur kernel located at each pixel (i.e. K (i,j) ).\n\n\nBlur Removal via Motion Flow Estimation\n\nGiven a blurry image Y, our goal is to estimate the blur kernel K and recover a blur-free latent image X through non-blind deconvolution that can be performed by solving a convex problem (Figure 2 (b)). As mentioned above, kernel estimation is the most difficult and crucial part. Based on the model in (1) and (2), heterogeneous motion blur can be modeled by a set of blur kernels, one associated with each pixel and its motion. By using a linear motion model to indicate each pixel's motion during imaging process [18], and letting p = (i, j) denote a pixel location, the motion at pixel p, can be represented by a 2dimensional motion vector (u p , v p ), where u p and v p represent the movement in the horizontal and vertical directions, respectively (See Figure 3 (a)). By a slight abuse of 1 For simplicity, we assume X and Y have the same size. notation we express this as M p = (u p , v p ), which characterizes the movement at pixel p over the exposure time. If we have the feasible domain u p \u2208 D u and v p \u2208 D v , then M p \u2208 D u \u00d7 D v , but will be introduced in detail later. As shown in Figure 3, the blur kernel on each pixel appears as a line trace with nonzero components only along the motion trace. As a result, the motion blur K p in (2) can be expressed as [2]:\nK p (i , j ) = 0, if (i , j ) 2 \u2265 Mp 2 2 , 1 Mp 2 \u03b4(v p i \u2212u p j ), otherwise,(4)\nwhere \u03b4(\u00b7) denotes the Dirac delta function. We thus can achieve heterogeneous motion blur estimation by estimating the motion vectors on all pixels, the result of which is M, which is referred as motion flow. For convenience of expression, we let M = (U, V), where U and V denote the motion maps in the horizontal and vertical directions, respectively. For any pixel p = (i, j),\nwe define M p = (U(i, j), V(i, j)) with U(i, j) = u p and V(i, j) = v p .\nAs shown in Figure 2 (b), given a blurred image and the estimated motion flow, we can recover the sharp image by solving an non-blind deconvolution problem\nmin x y \u2212 H(K)x 2 2 + \u2126(x)\nwith regularizer \u2126(x) on the unknown sharp image. In practice, we use a Gaussian mixture model based regularizer as Figure 4. Our network structure. A blurred image goes through layers and produces a pixel-wise dense motion flow map. conv means a convolutional layer and uconv means a fractionally-strided convolutional (deconvolutional) layer, where n\u00d7n for each uconv layer denotes that the up-sampling size is n. Skip connections on top of pool2 and pool3 are used to combine features with different resolutions.\n!\"! #$\"! !\"! #$\"! $%#! $%#! $%#! $%#! $%#! &'&( )*+,%! #'#( -**.%! $'$( )*+,#! #'#( -**.#! /'/( )*+,/! #'#( -**./! /'/( )*+,0! #'#( -**.0! /'/( )*+,$! %'%()*+,&! %'%()*+,\"! #'#(( 1)*+,%! #'#( 1)*+,#! 0'0(( 1)*+,/! 2! 2! !! !! !!\n\u2126(x) [47,32].\n\n\nLearning for Motion Flow Estimation\n\nThe key contribution of our work is to show how to obtain the motion flow field that results in the pixel-wise motion blur. To do so we train a FCN to directly estimate the motion flow field from the blurry image.\n\nLet {(Y t , M t )} T t=1 be a set of blurred-image and motion-flow-map pairs, which we take as our training set. Our task is to learn an end-to-end mapping function M = f (Y) from any observed blurry image Y to the underlying motion flow M. In practice, the challenge is that obtaining the training ground-truth dense motion flow for sufficiently many and varied real blurry images is infeasible. Human labeling is impossible, and training from automated methods for image deblurring would defeat the purpose. To overcome this problem, we generate the training set by simulating motion flows maps. (See section 4.2). Specifically, we collect a set of sharp images {X n }, simulate T motion flows {M t } in total for all images in {X n }, and then generate T blurred images {Y t } based on the models in (1) and (4) (See Figure 2 (a)). Feasible domain of motion flow To simplify the training process, we train the FCN over a discrete output domain. Interestingly, classification on discrete output space has achieved some impressive results for some similar applications, e.g. optical flow estimation [35] and surface normal prediction [36]. In our work, we adopt an integer domain for both U and V, and treat the mapping M = f (Y) as a multi-class classification problem. Specifically, we uniformly discretize the motion values as integers with a 1 (pixel) interval, which provides a high-precision approximation to the latent continuous space. As a result, by assuming the maximum movements in the horizontal and vertical directions to be u max and v max , respectively, we have\nD u = {u|u \u2208 Z, |u| \u2264 u max } and D v = {v|v \u2208 Z, |v| \u2264 v max }, where Z denotes the integer domain.\nAs shown in Figure 3 (a), any linear blur kernel is symmetric. Any two motion vectors with same length and opposite directions, e.g. (u p , v p ) and (\u2212u p , \u2212v p ), generate the same blur pattern, which may confuse the learning pro-cess. We thus further restrict the motion in the horizontal direction to be nonnegative as shown in Figure 3 \n(b), i.e. u p \u2208 D + u = {u|u \u2208 Z + 0 , |u| \u2264 u max }, by letting (u p , v p ) = \u03c6(u p , v p ) where \u03c6(u p , v p ) = (\u2212u p , \u2212v p ), if u p < 0, (u p , v p ), otherwise.(5)\n\nDense Motion Flow Estimation\n\n\nNetwork Design\n\nThe goal of this FCN network is to achieve the end-toend mapping from a blurry image to its corresponding motion flow map. Given any RGB image with the arbitrary size P \u00d7 Q, the FCN is used to estimate a motion flow map M = (U, V) with the same size to the input image, where U(i, j) \u2208 D + u and V(i, j) \u2208 D v , \u2200i, j. For convenience, we let D = |D + u | + |D v | denote the total number of labels for both U and V. Our network structure is similar to the FCN in [24]. As shown in Figure 4, we use 7 convolutional (conv) layers and 4 max-pooling (pool) layers as well as 3 uconv layers to up-sample the prediction maps. Following [37], uconv denotes the fractionally-strided convolution, a.k.a. deconvolution. We use a small stride of 1 pixel for all convolutional layers. The uconv layers are initialized with bilinear interpolation and used to up-sample the activations. We also add skip connections which combine the information from different layers as shown in Figure 4.\n\nThe feature map of the last uconv layer (conv7 + uconv2) is a P \u00d7 Q \u00d7 D tensor with the top |D + u | slices of feature maps (P \u00d7 Q \u00d7 |D + u |) corresponding to the estimation of U, and the remaining |D v | slices of feature maps (P \u00d7 Q \u00d7 |D v |) corresponding to the estimation of V. Two separate soft-max layers are applied to those two parts respectively to obtain the posterior probability estimation of both channels. Let F u,i,j (Y) represent the probability that the pixel at (i, j) having a movement u along the horizontal direction, and F v,i,j (Y) represent the probability that the pixel at (i, j) having a movement v along the vertical direction, we then use the sum of the cross entropy loss from both channels as the final loss function:\nL(Y, M) =\u2212 P i=1 Q j=1 u\u2208D + u 1(U(i, j) = u) log(F u,i,j (Y)) \u2212 P i=1 Q j=1 v\u2208Dv 1(V(i, j) = v) log(F v,i,j (Y)),\nwhere 1 is an indicator function.\n\n\nSimulate Motion Flow for Data Generation\n\nThe gist of this section is generating a dataset that contains realistic blur patterns on diverse images for training. Although an i.i.d. random sampling may generate very diverse training samples, since the realistic motion flow preserves some properties such as piece-wise smoothness, we aim to design a simulation method to generate motion flows reflecting the natural properties of the movement in imaging process. Although the object motion [15] can lead to heterogeneous motion blur in real images, our method only simulates the motion flow caused by camera motion for learning. Even so, as shown in Section 5.5, data generated by our method can also give the model certain ability to handle object motion.\n\nFor simplicity, we generate a 3D coordinate system where the origin at the camera's optical center, the xy-plane is aligned with the camera sensors, and the z-axis is perpendicular to the xy-plane, as shown in Figure 5. Since our objective is the motion flow on an image grid, we directly simulate the motion flow projected on 2D image instead of the 3D motion trajectory [39]. Considering the ambiguities caused by rotations around x and y axis [11], we simulate a motion flow M by sampling four additive components:\nM = M Tx + M Ty + M Tz + M Rz ,(6)\nwhere M Tx , M Ty and M Tz denote the motion flows associated with the translations along x, y and z axis, receptively, and M Rz represents the motion from the rotation around z axis. We generate each element as the following.\n\nTranslation along x or y axis We describe the generation of M Tx as an example. We first sample a central pixel p Tx = (i Tx , j Tx ) on image plane, a basic motion value t Tx and a acceleration coefficient r Tx . Then M Tx = (U Tx , V Tx ) can be generated as the following\nU Tx (i, j) = (i \u2212 i Tx )r Tx + t Tx , V Tx (i, j) = 0.\nM Ty can be generated in a similar way.\n\nTranslation along z axis The translation along z axis usually causes radial motion blur pattern towards the vanishing point [46]. By ignoring the semantic context and assuming a simple radial pattern, M Tz can be generated by\nU Tz (i, j) = t Tz d(i, j) \u03b6 (i\u2212i Tz ), V Tz (i, j) = t Tz d(i, j) \u03b6 (j \u2212j Tz )\nwhere p Tz denotes a sampled vanishing point, d(i, j) = (i, j) \u2212 p Tz 2 is the distance from any pixel (i, j) to the vanishing point, \u03b6 and t Tz are used to control the shape of radial patterns, which reflects the moving speed.\n\nRotation around z axis We first sample a rotation center p Rz and an angular velocity \u03c9, where \u03c9 > 0 denotes the clockwise rotation. Let d(i, j) = (i, j) \u2212 p Rz 2 . The motion magnitude at each pixel is s(i, j) = 2d(i, j) tan(\u03c9/2). By letting \u03b8(i, j) = atan[(i \u2212 i Rz )/(j \u2212 j Rz )] \u2208 [\u2212\u03c0, \u03c0], motion vector at pixel (i, j) can be generated as U Rz (i, j) = s(i, j) cos(\u03b8(i, j) \u2212 \u03c0/2), V Rz (i, j) = s(i, j) sin(\u03b8(i, j) \u2212 \u03c0/2).\n\nWe place uniform priors over all the parameters corresponding to the motion flow simulation as Uniform(\u03b1, \u03b2). More details can be found in supplementary materials. Note that the four components in (6) are simulated in continuous domain and are then discretized as integers.\n\nTraining dataset generation We use 200 training images with sizes around 300 \u00d7 460 from the dataset BSD500 [1] as our sharp image set {X n }. We then independently simulate 10,000 motion flow maps {M t } with ranges u max = v max = 36 and assign each X n 50 motion flow maps without duplication. The non-blurred images {X n } with U(i, j) = 0 and V(i, j) = 0, \u2200i, j are used for training. As a result we have a dataset with 10,200 blurred-imagemotion-flow pairs {Y t , M t } for training. \n\n\nExperiments\n\nWe implement our model based on Caffe [16] and train it by stochastic gradient descent with momentum and batch size 1. In the training on the dataset simulated on BSD, we use a learning rate of 10 \u22129 and a step size of 2 \u00d7 10 5 . The training converges after 65 epochs.\n\n\nDatasets and Evaluation Metrics\n\nDatasets We conduct the experiments on both synthetic datasets and real-world images. Since ground truth motion flow and sharp image for real blurry image are difficult to obtain, to perform general quantitative evaluation, we first generate two synthetic datasets, which both contain 300 blurred images, with 100 sharp images randomly picked from BSD500 [1] 2 , and 3 different motion flow maps for each. Note that no two motion flow maps are the same. We simulate the motion flow with u max = v max = 36, which is same as in the training set. For fairness to the method [32] with a smaller output space, we also generate relative mild motion flows for the second dataset with u max = v max = 17. These two are referred as BSD-S and BSD-M, respectively. In addition, we evaluate the generalization ability of the proposed method using two synthetic datasets (MC-S and MC-M) with 60 blurry images generated from 20 sharp images from Microsoft COCO [23] and above motion flow generation setting. Evaluation Metrics For evaluating the accuracy of estimated motion flow, we measure the mean-squared-error (MSE) of the motion flow map. Specifically, given an estimated motion flow M and the ground truth M, the MSE is defined as 1\n2|M | i,j ((U(i, j) \u2212 U(i, j)) 2 + ((V(i, j) \u2212 V(i, j)) 2 ,\nwhere |M| denotes the number of motion vectors in M. For evaluation of the image quality, we adopt peak signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) [38].\n\n\nEvaluation of Motion Flow Estimation\n\nWe first compare with the method of Sun et.al.\n\n(\"patchCNN\") [32], which is the only method with available code for estimating motion flow from blurry images 3 . This method performs training and testing on small image patches, and uses MRF to improve the accuracy on 2 No overlapping with the training dataset. 3 The code of the other motion flow based method [18] is unavailable. the entire image. Its version without MRF post-processing (\"noMRF\") is also compared, where the soft-max output is directly used to get the motion flow as in our method. Table 2 shows the average MSE of the estimated motion flow maps on all images in BSD-S and BSD-M. It is noteworthy that, even without any post-processing such as MRF or CRF, the comparison manifests the high quality of our estimated motion flow maps. Furthermore, our method can still produce accurate motion flow even on the more challenging BSD-S dataset, on which the accuracies of the patch based method [32] decrease significantly. We also show an example of the the estimated motion flows in Figure 6, which shows that our result preserves a smooth motion flow very similar to the ground truth, and the method of Sun et.al. [32] is more sensitive to the image contents. From this example, we can see that the method of Sun et.al. [32] generally underestimates the motion values and produces errors near the strong edges, maybe because its patch-level processing is confused by the strong edges and ignores the blur pattern context in a larger area. To compare with other blind deblurring methods of Xu and Jia [41], Xu et.al. [43] and Whyte et.al. [39], which do not estimate the motion flow, we directly evaluate the quality of the image recovered using their estimated blur kernel. For fairness, we use the same non-blind deconvolution method with least square loss function and a Gaussian mixture model prior [47] to recover the sharp image. As the non-blind deconvolution method may limit the recovering quality, we evaluate the images recovered using the ground truth motion flow as reference. Table 1 shows the average values on all images in each dataset, which shows that our method produce significantly better results than the others.\n\n\nEvaluation of Generalization Ability\n\nTo evaluate the generalization ability of our approach on different images, we use the datasets based on the Microsoft COCO [23] (i.e. MC-S and MC-M) to evaluate our model trained on the dataset based on BSD500 [1]. Table 3 shows the evaluation and comparison with the \"patchCNN\" [32]. The results demonstrate that our method stably produces high accuracy results on both datasets. This experiment suggests that the generalization ability of our approach is strong. \n\n\nRunning-time Evaluation\n\nWe conduct a running-time comparison with the relevant motion flow estimation methods [32,18] by running motion flow estimation for 60 blurred images with sizes around 640 \u00d7 480 on a PC with an NVIDIA GeForce 980 graphics card and Intel Core i7 CPU. For the method in [18], we quote its running-time from the paper. Note that both the method of Sun et.al. and ours use the GPU to accelerate the computation. As shown in Table 4, the method in [18] takes very long time due to its iterative optimization scheme. Our method takes less than 10 seconds, which is more efficient than others. The patchCNN method [32] takes more time because many post-processing steps are required. \n\n\nEvaluation on Real-world Images\n\nAs the ground truth images of real-world blurry images are unavailable, we only present the visual evaluation and comparison against several state-of-the-art methods for spatially-varying blur removing. More results can be found in supplementary materials. Since the method of Sun et.al. performs on local patches, their motion flow components are often misestimated, especially when the blur pattern in a small local area is subtle or confusing, such as the areas with low illumination or textures. Thanks to the universal end-to-end mapping, our methods can generate more natural results with smooth flow and less clutters. Although we train our model on dataset with only smoothly varying motion flow, compared with [32], our method can obtain better results on images with moving object. Comparison with the method in [18] Kim et.al. [18] use the similar heterogeneous motion blur model as ours and also estimate motion flow for deblurring. As their code is unavailable, we directly perform a comparison on their realworld data. Figure 8 shows the results on an example. Compared with the results of Kim and Lee [18], our motion flow more accurately reflects the complex blur pattern, and our recovered image contains more details and less artifacts.\n\n(a) Blurry image (b) [18] (c) Ours (d) [32] (e) [18] (f) Ours Images with camera motion blur Figure 9 shows an example containing blur mainly caused by the camera motion. The deblurred image generated by the non-uniform camera shake deblurring method [39] suffers from heavy blur because its model ignores the blur caused by large forward motion. Compared with the result of Sun et.al. [32], our method produces a sharper result with more details and less artifacts. Images with object motion blur We evaluate our method on the images containing object motion blur. In Figure 10, the result of Whyte et.al. [39] contains heavy ringing artifacts due to the object motion. Our method can handle the strong blur in the background and generate a more natural image. We further compare with the segmentation-based deblurring method of Pan et.al. [26] on an image with large scale blur caused by moving object on static background. As shown in Figure 11, the result of Sun et.al. [32] is oversmooth due to the underestimate of motion flow. In the result of Pan et.al. [26], some details are lost due to the segmentation error. Our proposed method can recover the  \n\n\nConclusion\n\nIn this paper, we proposed a flexible and efficient deep learning based method for estimating and removing the heterogeneous motion blur. By representing the heterogeneous motion blur as pixel-wise linear motion blur, the proposed method uses a FCN to estimate the a dense motion flow map for blur removal. Moreover, we automatically generate training data with simulated motion flow maps for training the FCN. Experimental results on both synthetic and realworld data show the excellence of the proposed method.\n\nFigure 2 .\n2Overview of our scheme for heterogeneous motion blur removal. (a) We train an FCN using examples based on simulated motion flow maps. (b) Given a blurry image, we perform end-to-end motion flow estimation using the trained FCN, and then recover the sharp image via non-blind deconvolution.\n\nFigure 3 .\n3Motion blur and motion vector. (a) An example with blur cause by clock-wise rotation. Three examples of the blur pattern, linear blur kernel and motion vector are shown. The blur kernels on p1 and p3 caused by motions with opposite directions and have the same appearance. (b) Illustrations of the feasible domain of motion flow.\n\nFigure 5 .\n5Demonstration of the motion flow simulation. (a) A sharp example image and the coordinate system of camera. (b)-(c) The sampled motion flow and the corresponding blurred image by simulating the translation along x and y-axes (MT x + MT y ), translation along z-axis (MT z ) and rotation around z-axis (MR z ), respectively. (d) A sample based on the model considering all components in (6).\n\nFigure 6 .\n6A motion flow estimation example on a synthetic image in BSD-M. The method of Sun et.al. [32] is more sensitive to the image content (See the black box in (c)).\n\n\nResults of motion flow estimation We first compare the proposed method with the method of Sun et.al. [32] on motion flow estimation. Four examples are shown in Figure 7.\n\nFigure 8 .\n8Comparison with the method of Kim and Lee[18].\n\nFigure 7 .Figure 9 .Figure 10 .Figure 11 .\n791011Examples of motion flow estimation on real-world blurry images. From top to bottom: Blurry image Y, motion flow estimated by the patchCNN[32], and by our motion flow M. Our results are more smooth and more accurate on moving objects.(a) Blurry image (b) Whyte et.al. [39] (c) Sun et.al. Deblurring results on an image with camera motion blur. (a) Blurry image (b) Whyte et.al. [39] (c) Kim and Lee [18] (d) Sun et.al. Deblurring results on an non-uniform blur image with strong blur on background. (a) Blurry image (b) Pan et.al. [26] (c) Sun et.al. [32] (d) Ours Deblurring results on an image with large scale motion blur caused by moving object. details on blurred moving foreground and keep the sharp background as original.\n\nTable 1 .\n1Evaluation on motion blur estimation. Comparison on PSNR and SSIM of the recovered images with the estimated blur kernel. The best results are bold-faced.Dataset Metric \nGT K \nXu and Jia [41] Whyte et.al. [39] Xu et.al. [43] noMRF [32] patchCNN [32] \nOurs \nBSD-S \nPSNR 23.022 \n17.773 \n17.360 \n18.351 \n20.483 \n20.534 \n21.947 \nSSIM 0.6609 \n0.4431 \n0.3910 \n0.4766 \n0.5272 \n0.5296 \n0.6309 \nBSD-M PSNR 24.655 \n19.673 \n18.451 \n20.057 \n22.789 \n22.9683 \n23.978 \nSSIM 0.7481 \n0.5661 \n0.5010 \n0.5973 \n0.6666 \n0.6735 \n0.7249 \n\n\n\nTable 2 .\n2Evaluation on motion flow estimation (MSE). The best results are bold-faced.Dataset patchCNN [32] noMRF [32] \nOurs \nBSD-S \n50.1168 \n54.4863 \n6.6198 \nBSD-M \n15.6389 \n20.7761 \n5.2051 \n\n\n\nTable 3 .\n3Evaluation of the generalization ability on datasets MC-S and MC-M. The best results are bold-faced.Dataset Metric \nGT K patchCNN noMRF [32] Ours \nMSE \n-\n52.1234 \n60.9397 \n7.8038 \nMC-S \nPSNR 22.620 \n20.172 \n20.217 \n21.954 \nSSIM 0.6953 \n0.5764 \n0.5772 \n0.6641 \nMSE \n-\n22.4383 \n31.2754 \n7.3405 \nMC-M \nPSNR 23.827 \n22.186 \n22.028 \n23.227 \nSSIM \n0.7620 \n0.6924 \n0.6839 \n0.7402 \n\n\n\nTable 4 .\n4Running-time comparison.Method \n[18] patchCNN [32] noMRF [32] Ours \nTime (s) 1500 \n45.2 \n18.5 \n8.4 \n\n\n\nContour detection and hierarchical image segmentation. P Arbelaez, M Maire, C Fowlkes, J Malik, IEEE Trans. on PAMI. 3357P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE Trans. on PAMI, 33(5):898-916, 2011. 5, 6, 7\n\nBlind image deconvolution of linear motion blur. F Brusius, U Schwanecke, P Barth, International Conference on Computer Vision, Imaging and Computer Graphics. SpringerF. Brusius, U. Schwanecke, and P. Barth. Blind image de- convolution of linear motion blur. In International Confer- ence on Computer Vision, Imaging and Computer Graphics, pages 105-119. Springer, 2011. 3\n\nA neural approach to blind motion deblurring. A Chakrabarti, ECCV. 2A. Chakrabarti. A neural approach to blind motion deblur- ring. ECCV, 2016. 2\n\nAnalyzing spatially-varying blur. A Chakrabarti, T Zickler, W T Freeman, CVPR. A. Chakrabarti, T. Zickler, and W. T. Freeman. Analyzing spatially-varying blur. In CVPR, pages 2512-2519, 2010. 2\n\nTotal variation blind deconvolution. T F Chan, C.-K Wong, Trans. on Image Processing. 73T. F. Chan and C.-K. Wong. Total variation blind deconvolu- tion. Trans. on Image Processing, 7(3):370-375, 1998. 2\n\nFast motion deblurring. S Cho, S Lee, ASIA. 12S. Cho and S. Lee. Fast motion deblurring. SIGGRAPH ASIA, 2009. 1, 2\n\nLearning to estimate and remove non-uniform image blur. F Couzinie-Devy, J Sun, K Alahari, J Ponce, CVPR. F. Couzinie-Devy, J. Sun, K. Alahari, and J. Ponce. Learning to estimate and remove non-uniform image blur. In CVPR, pages 1075-1082, 2013. 2\n\nRemoving partial blur in a single image. S Dai, Y Wu, CVPR. S. Dai and Y. Wu. Removing partial blur in a single image. In CVPR, pages 2544-2551. IEEE, 2009. 2\n\nRemoving camera shake from a single photograph. R Fergus, B Singh, A Hertzmann, S T Roweis, W T Freeman, In ACM Transactions on Graphics. 12R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. T. Freeman. Removing camera shake from a single photograph. In ACM Transactions on Graphics, 2006. 1, 2\n\nBlind image deconvolution by automatic gradient activation. D Gong, M Tan, Y Zhang, A Van Den Hengel, Q Shi, CVPR. 1D. Gong, M. Tan, Y. Zhang, A. van den Hengel, and Q. Shi. Blind image deconvolution by automatic gradient activation. In CVPR, 2016. 1, 2\n\nSingle image deblurring using motion density functions. A Gupta, N Joshi, C L Zitnick, M Cohen, B Curless, ECCV. A. Gupta, N. Joshi, C. L. Zitnick, M. Cohen, and B. Curless. Single image deblurring using motion density functions. In ECCV, pages 171-184, 2010. 1, 2, 5\n\nFast removal of non-uniform camera shake. M Hirsch, C J Schuler, S Harmeling, B Sch\u00f6lkopf, ICCV. 1M. Hirsch, C. J. Schuler, S. Harmeling, and B. Sch\u00f6lkopf. Fast removal of non-uniform camera shake. In ICCV, 2011. 1, 2\n\nEfficient filter flow for space-variant multiframe blind deconvolution. M Hirsch, S Sra, B Sch\u00f6lkopf, S Harmeling, CVPR. 1M. Hirsch, S. Sra, B. Sch\u00f6lkopf, and S. Harmeling. Efficient filter flow for space-variant multiframe blind deconvolution. In CVPR, volume 1, page 2, 2010. 2\n\nJoint depth estimation and camera shake removal from single blurry image. Z Hu, L Xu, M.-H Yang, CVPR. Z. Hu, L. Xu, and M.-H. Yang. Joint depth estimation and camera shake removal from single blurry image. In CVPR, pages 2893-2900, 2014. 1\n\nDynamic scene deblurring. T Hyun Kim, B Ahn, K. Mu Lee, CVPR. 25T. Hyun Kim, B. Ahn, and K. Mu Lee. Dynamic scene de- blurring. In CVPR, pages 3160-3167, 2013. 1, 2, 5\n\nY Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, Caffe, Convolutional architecture for fast feature embedding. arXiv. Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir- shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv, 2014. 6\n\nAccurate image superresolution using very deep convolutional networks. J Kim, J K Lee, K M Lee, CVPR. J. Kim, J. K. Lee, and K. M. Lee. Accurate image super- resolution using very deep convolutional networks. In CVPR, 2016. 2\n\nSegmentation-free dynamic scene deblurring. T H Kim, K M Lee, CVPR. 7T. H. Kim and K. M. Lee. Segmentation-free dynamic scene deblurring. In CVPR, 2014. 1, 2, 3, 6, 7, 8\n\nBlind deconvolution using a normalized sparsity measure. D Krishnan, T Tay, R Fergus, CVPR. D. Krishnan, T. Tay, and R. Fergus. Blind deconvolution using a normalized sparsity measure. In CVPR, pages 233- 240, 2011. 2\n\nBlind motion deblurring using image statistics. A Levin, NIPS. 1A. Levin. Blind motion deblurring using image statistics. In NIPS, pages 841-848, 2006. 1, 2\n\nUnderstanding and evaluating blind deconvolution algorithms. A Levin, Y Weiss, F Durand, W T Freeman, CVPR. A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Under- standing and evaluating blind deconvolution algorithms. In CVPR, pages 1964-1971, 2009. 2\n\nEfficient marginal likelihood optimization in blind deconvolution. A Levin, Y Weiss, F Durand, W T Freeman, CVPR. A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. Efficient marginal likelihood optimization in blind deconvolution. In CVPR, pages 2657-2664, 2011. 2\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCV. 67T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In ECCV, pages 740-755, 2014. 1, 6, 7\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431- 3440, 2015. 4\n\nImage restoration using convolutional auto-encoders with symmetric skip connections. X.-J Mao, C Shen, Y.-B Yang, arXivX.-J. Mao, C. Shen, and Y.-B. Yang. Image restoration us- ing convolutional auto-encoders with symmetric skip con- nections. arXiv, 2016. 2\n\nSoftsegmentation guided object motion deblurring. J Pan, Z Hu, Z Su, H.-Y. Lee, M.-H Yang, J. Pan, Z. Hu, Z. Su, H.-Y. Lee, and M.-H. Yang. Soft- segmentation guided object motion deblurring. 2016. 1, 2, 7, 8\n\nDeblurring text images via l0-regularized intensity and gradient prior. J Pan, Z Hu, Z Su, M.-H Yang, CVPR. 1J. Pan, Z. Hu, Z. Su, and M.-H. Yang. Deblurring text im- ages via l0-regularized intensity and gradient prior. In CVPR, pages 2901-2908, 2014. 1, 2\n\nBlind image deblurring using dark channel prior. J Pan, D Sun, H Pfister, M.-H Yang, CVPR. 1J. Pan, D. Sun, H. Pfister, and M.-H. Yang. Blind image deblurring using dark channel prior. In CVPR, 2016. 1, 2\n\nTotal variation blind deconvolution: The devil is in the details. D Perrone, P Favaro, CVPR. D. Perrone and P. Favaro. Total variation blind deconvolu- tion: The devil is in the details. In CVPR, pages 2909-2916, 2014. 2\n\nDiscriminative non-blind deblurring. U Schmidt, C Rother, S Nowozin, J Jancsary, S Roth, CVPR. U. Schmidt, C. Rother, S. Nowozin, J. Jancsary, and S. Roth. Discriminative non-blind deblurring. In CVPR, pages 604- 611, 2013. 2\n\nA machine learning approach for non-blind image deconvolution. C J Schuler, H Christopher Burger, S Harmeling, B Scholkopf, CVPR. C. J. Schuler, H. Christopher Burger, S. Harmeling, and B. Scholkopf. A machine learning approach for non-blind image deconvolution. In CVPR, pages 1067-1074, 2013. 2\n\nLearning a convolutional neural network for non-uniform motion blur removal. J Sun, W Cao, Z Xu, J Ponce, CVPR. IEEE7J. Sun, W. Cao, Z. Xu, and J. Ponce. Learning a convolu- tional neural network for non-uniform motion blur removal. In CVPR, pages 769-777. IEEE, 2015. 1, 2, 4, 6, 7, 8\n\nEdge-based blur kernel estimation using patch priors. L Sun, S Cho, J Wang, J Hays, ICCP. L. Sun, S. Cho, J. Wang, and J. Hays. Edge-based blur kernel estimation using patch priors. In ICCP, pages 1-8, 2013. 2\n\nRichardson-lucy deblurring for scenes under a projective motion path. Y.-W Tai, P Tan, M S Brown, Trans. on PAMI. 338Y.-W. Tai, P. Tan, and M. S. Brown. Richardson-lucy de- blurring for scenes under a projective motion path. Trans. on PAMI, 33(8):1603-1618, 2011. 2\n\nDense optical flow prediction from a static image. J Walker, A Gupta, M Hebert, ICCV. J. Walker, A. Gupta, and M. Hebert. Dense optical flow prediction from a static image. In ICCV, pages 2443-2451, 2015. 4\n\nDesigning deep networks for surface normal estimation. X Wang, D Fouhey, A Gupta, CVPR. X. Wang, D. Fouhey, and A. Gupta. Designing deep net- works for surface normal estimation. In CVPR, pages 539- 547, 2015. 4\n\nGenerative image modeling using style and structure adversarial networks. X Wang, A Gupta, ECCV. X. Wang and A. Gupta. Generative image modeling using style and structure adversarial networks. In ECCV, 2016. 4\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Trans. on Image Processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. on Image Processing, 13(4):600- 612, 2004. 6\n\nNon-uniform deblurring for shaken images. O Whyte, J Sivic, A Zisserman, J Ponce, IJCV. 982O. Whyte, J. Sivic, A. Zisserman, and J. Ponce. Non-uniform deblurring for shaken images. IJCV, 98(2):168-186, 2012. 1, 2, 5, 6, 7, 8\n\nEnd-to-end learning for image burst deblurring. P Wieschollek, B Sch\u00f6lkopf, H P A Lensch, M Hirsch, ACCV. P. Wieschollek, B. Sch\u00f6lkopf, H. P. A. Lensch, and M. Hirsch. End-to-end learning for image burst deblurring. In ACCV, 2016. 2\n\nTwo-phase kernel estimation for robust motion deblurring. L Xu, J Jia, ECCV. 16L. Xu and J. Jia. Two-phase kernel estimation for robust motion deblurring. In ECCV, pages 157-170, 2010. 1, 6\n\nDeep convolutional neural network for image deconvolution. L Xu, J S Ren, C Liu, J Jia, NIPS. L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neu- ral network for image deconvolution. In NIPS, pages 1790- 1798, 2014. 2\n\nUnnatural l0 sparse representation for natural image deblurring. L Xu, S Zheng, J Jia, CVPR. 6L. Xu, S. Zheng, and J. Jia. Unnatural l0 sparse represen- tation for natural image deblurring. In CVPR, pages 1107- 1114, 2013. 2, 6\n\nNon-uniform camera shake removal using a spatially-adaptive sparse penalty. H Zhang, D Wipf, NIPS. H. Zhang and D. Wipf. Non-uniform camera shake removal using a spatially-adaptive sparse penalty. In NIPS, pages 1556-1564, 2013. 2\n\nMulti-image blind deblurring using a coupled adaptive sparse prior. H Zhang, D Wipf, Y Zhang, CVPR. H. Zhang, D. Wipf, and Y. Zhang. Multi-image blind deblur- ring using a coupled adaptive sparse prior. In CVPR, 2013. 2\n\nForward motion deblurring. S Zheng, L Xu, J Jia, CVPR. 25S. Zheng, L. Xu, and J. Jia. Forward motion deblurring. In CVPR, pages 1465-1472, 2013. 1, 2, 5\n\nFrom learning models of natural image patches to whole image restoration. D Zoran, Y Weiss, ICCV. 4D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In ICCV, pages 479-486, 2011. 4, 6\n", "annotations": {"author": "[{\"end\":254,\"start\":100},{\"end\":320,\"start\":255},{\"end\":429,\"start\":321},{\"end\":552,\"start\":430},{\"end\":657,\"start\":553},{\"end\":766,\"start\":658},{\"end\":883,\"start\":767},{\"end\":952,\"start\":884}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":105},{\"end\":263,\"start\":259},{\"end\":333,\"start\":330},{\"end\":443,\"start\":438},{\"end\":561,\"start\":557},{\"end\":670,\"start\":666},{\"end\":787,\"start\":773},{\"end\":895,\"start\":892}]", "author_first_name": "[{\"end\":104,\"start\":100},{\"end\":258,\"start\":255},{\"end\":329,\"start\":321},{\"end\":437,\"start\":430},{\"end\":556,\"start\":553},{\"end\":665,\"start\":658},{\"end\":772,\"start\":767},{\"end\":891,\"start\":884}]", "author_affiliation": "[{\"end\":197,\"start\":111},{\"end\":253,\"start\":199},{\"end\":319,\"start\":265},{\"end\":389,\"start\":335},{\"end\":428,\"start\":391},{\"end\":551,\"start\":465},{\"end\":617,\"start\":563},{\"end\":656,\"start\":619},{\"end\":726,\"start\":672},{\"end\":765,\"start\":728},{\"end\":843,\"start\":789},{\"end\":882,\"start\":845},{\"end\":951,\"start\":897}]", "title": "[{\"end\":97,\"start\":1},{\"end\":1049,\"start\":953}]", "venue": null, "abstract": "[{\"end\":2432,\"start\":1051}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2649,\"start\":2646},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2651,\"start\":2649},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2654,\"start\":2651},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2657,\"start\":2654},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2660,\"start\":2657},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2663,\"start\":2660},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2734,\"start\":2730},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2737,\"start\":2734},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2740,\"start\":2737},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2743,\"start\":2740},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2746,\"start\":2743},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2868,\"start\":2864},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2888,\"start\":2884},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3000,\"start\":2996},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3439,\"start\":3435},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3442,\"start\":3439},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3465,\"start\":3461},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3468,\"start\":3465},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3582,\"start\":3579},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3584,\"start\":3582},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3587,\"start\":3584},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3768,\"start\":3764},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3771,\"start\":3768},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3800,\"start\":3796},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3803,\"start\":3800},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3834,\"start\":3830},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4140,\"start\":4137},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4143,\"start\":4140},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4146,\"start\":4143},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4477,\"start\":4474},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4479,\"start\":4477},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4762,\"start\":4758},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6138,\"start\":6134},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6532,\"start\":6529},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6534,\"start\":6532},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6536,\"start\":6534},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6539,\"start\":6536},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6542,\"start\":6539},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6545,\"start\":6542},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6709,\"start\":6706},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6712,\"start\":6709},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6747,\"start\":6744},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6769,\"start\":6765},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6782,\"start\":6778},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6785,\"start\":6782},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6808,\"start\":6804},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6971,\"start\":6968},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6974,\"start\":6971},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7010,\"start\":7006},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7045,\"start\":7041},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7048,\"start\":7045},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7051,\"start\":7048},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7353,\"start\":7349},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7531,\"start\":7527},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7540,\"start\":7536},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7559,\"start\":7555},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7700,\"start\":7696},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7703,\"start\":7700},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7770,\"start\":7766},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7892,\"start\":7888},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7894,\"start\":7892},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7897,\"start\":7894},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7900,\"start\":7897},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8083,\"start\":8079},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8539,\"start\":8535},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8574,\"start\":8570},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8622,\"start\":8618},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8699,\"start\":8695},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8702,\"start\":8699},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8832,\"start\":8828},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8958,\"start\":8955},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9159,\"start\":9155},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10898,\"start\":10894},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11175,\"start\":11174},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11658,\"start\":11655},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13133,\"start\":13129},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13136,\"start\":13133},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14496,\"start\":14492},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14531,\"start\":14527},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16104,\"start\":16100},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16271,\"start\":16267},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18008,\"start\":18004},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18648,\"start\":18644},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18722,\"start\":18718},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19553,\"start\":19549},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20589,\"start\":20586},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20774,\"start\":20771},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21211,\"start\":21207},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22050,\"start\":22046},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22426,\"start\":22422},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22937,\"start\":22933},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23044,\"start\":23040},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23248,\"start\":23247},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23292,\"start\":23291},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23344,\"start\":23340},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23943,\"start\":23939},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24165,\"start\":24161},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24271,\"start\":24267},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24551,\"start\":24547},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24567,\"start\":24563},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24589,\"start\":24585},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24853,\"start\":24849},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25350,\"start\":25346},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25436,\"start\":25433},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25506,\"start\":25502},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25806,\"start\":25802},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25809,\"start\":25806},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25988,\"start\":25984},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26163,\"start\":26159},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26327,\"start\":26323},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27152,\"start\":27148},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27255,\"start\":27251},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27271,\"start\":27267},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27549,\"start\":27545},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27710,\"start\":27706},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27728,\"start\":27724},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27737,\"start\":27733},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27940,\"start\":27936},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28075,\"start\":28071},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28296,\"start\":28292},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28530,\"start\":28526},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28663,\"start\":28659},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28751,\"start\":28747},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30825,\"start\":30821},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31018,\"start\":31014}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29673,\"start\":29371},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30016,\"start\":29674},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30420,\"start\":30017},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30594,\"start\":30421},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30766,\"start\":30595},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30826,\"start\":30767},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31605,\"start\":30827},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32134,\"start\":31606},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32330,\"start\":32135},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32718,\"start\":32331},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32832,\"start\":32719}]", "paragraph": "[{\"end\":3076,\"start\":2448},{\"end\":4952,\"start\":3078},{\"end\":5398,\"start\":4954},{\"end\":5709,\"start\":5400},{\"end\":6362,\"start\":5711},{\"end\":7240,\"start\":6379},{\"end\":8305,\"start\":7242},{\"end\":9521,\"start\":8307},{\"end\":9691,\"start\":9601},{\"end\":10018,\"start\":9710},{\"end\":10211,\"start\":10072},{\"end\":10334,\"start\":10230},{\"end\":11659,\"start\":10378},{\"end\":12121,\"start\":11742},{\"end\":12351,\"start\":12196},{\"end\":12894,\"start\":12379},{\"end\":13137,\"start\":13124},{\"end\":13390,\"start\":13177},{\"end\":14971,\"start\":13392},{\"end\":15415,\"start\":15073},{\"end\":16612,\"start\":15636},{\"end\":17364,\"start\":16614},{\"end\":17513,\"start\":17480},{\"end\":18270,\"start\":17558},{\"end\":18789,\"start\":18272},{\"end\":19051,\"start\":18825},{\"end\":19327,\"start\":19053},{\"end\":19423,\"start\":19384},{\"end\":19650,\"start\":19425},{\"end\":19958,\"start\":19731},{\"end\":20387,\"start\":19960},{\"end\":20662,\"start\":20389},{\"end\":21153,\"start\":20664},{\"end\":21438,\"start\":21169},{\"end\":22700,\"start\":21474},{\"end\":22938,\"start\":22761},{\"end\":23025,\"start\":22979},{\"end\":25181,\"start\":23027},{\"end\":25688,\"start\":25222},{\"end\":26393,\"start\":25716},{\"end\":27683,\"start\":26429},{\"end\":28843,\"start\":27685},{\"end\":29370,\"start\":28858}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9709,\"start\":9692},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10071,\"start\":10019},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10229,\"start\":10212},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11741,\"start\":11660},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12195,\"start\":12122},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12378,\"start\":12352},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13123,\"start\":12895},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15072,\"start\":14972},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15587,\"start\":15416},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17479,\"start\":17365},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18824,\"start\":18790},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19383,\"start\":19328},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19730,\"start\":19651},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22760,\"start\":22701}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25043,\"start\":25036},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25445,\"start\":25438},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26143,\"start\":26136}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2446,\"start\":2434},{\"attributes\":{\"n\":\"2.\"},\"end\":6377,\"start\":6365},{\"attributes\":{\"n\":\"3.\"},\"end\":9563,\"start\":9524},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9599,\"start\":9566},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10376,\"start\":10337},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13175,\"start\":13140},{\"attributes\":{\"n\":\"4.\"},\"end\":15617,\"start\":15589},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15634,\"start\":15620},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17556,\"start\":17516},{\"attributes\":{\"n\":\"5.\"},\"end\":21167,\"start\":21156},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21472,\"start\":21441},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22977,\"start\":22941},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25220,\"start\":25184},{\"attributes\":{\"n\":\"5.4.\"},\"end\":25714,\"start\":25691},{\"attributes\":{\"n\":\"5.5.\"},\"end\":26427,\"start\":26396},{\"attributes\":{\"n\":\"6.\"},\"end\":28856,\"start\":28846},{\"end\":29382,\"start\":29372},{\"end\":29685,\"start\":29675},{\"end\":30028,\"start\":30018},{\"end\":30432,\"start\":30422},{\"end\":30778,\"start\":30768},{\"end\":30870,\"start\":30828},{\"end\":31616,\"start\":31607},{\"end\":32145,\"start\":32136},{\"end\":32341,\"start\":32332},{\"end\":32729,\"start\":32720}]", "table": "[{\"end\":32134,\"start\":31772},{\"end\":32330,\"start\":32223},{\"end\":32718,\"start\":32443},{\"end\":32832,\"start\":32755}]", "figure_caption": "[{\"end\":29673,\"start\":29384},{\"end\":30016,\"start\":29687},{\"end\":30420,\"start\":30030},{\"end\":30594,\"start\":30434},{\"end\":30766,\"start\":30597},{\"end\":30826,\"start\":30780},{\"end\":31605,\"start\":30877},{\"end\":31772,\"start\":31618},{\"end\":32223,\"start\":32147},{\"end\":32443,\"start\":32343},{\"end\":32755,\"start\":32731}]", "figure_ref": "[{\"end\":2906,\"start\":2898},{\"end\":3532,\"start\":3524},{\"end\":3649,\"start\":3637},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10574,\"start\":10565},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11146,\"start\":11138},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11486,\"start\":11478},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12216,\"start\":12208},{\"end\":12503,\"start\":12495},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14224,\"start\":14212},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15093,\"start\":15085},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15414,\"start\":15406},{\"end\":16126,\"start\":16118},{\"end\":16611,\"start\":16603},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18490,\"start\":18482},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24037,\"start\":24029},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27470,\"start\":27462},{\"end\":27786,\"start\":27778},{\"end\":28263,\"start\":28254},{\"end\":28632,\"start\":28623}]", "bib_author_first_name": "[{\"end\":32890,\"start\":32889},{\"end\":32902,\"start\":32901},{\"end\":32911,\"start\":32910},{\"end\":32922,\"start\":32921},{\"end\":33160,\"start\":33159},{\"end\":33171,\"start\":33170},{\"end\":33185,\"start\":33184},{\"end\":33531,\"start\":33530},{\"end\":33666,\"start\":33665},{\"end\":33681,\"start\":33680},{\"end\":33692,\"start\":33691},{\"end\":33694,\"start\":33693},{\"end\":33864,\"start\":33863},{\"end\":33866,\"start\":33865},{\"end\":33877,\"start\":33873},{\"end\":34056,\"start\":34055},{\"end\":34063,\"start\":34062},{\"end\":34204,\"start\":34203},{\"end\":34221,\"start\":34220},{\"end\":34228,\"start\":34227},{\"end\":34239,\"start\":34238},{\"end\":34438,\"start\":34437},{\"end\":34445,\"start\":34444},{\"end\":34605,\"start\":34604},{\"end\":34615,\"start\":34614},{\"end\":34624,\"start\":34623},{\"end\":34637,\"start\":34636},{\"end\":34639,\"start\":34638},{\"end\":34649,\"start\":34648},{\"end\":34651,\"start\":34650},{\"end\":34918,\"start\":34917},{\"end\":34926,\"start\":34925},{\"end\":34933,\"start\":34932},{\"end\":34942,\"start\":34941},{\"end\":34960,\"start\":34959},{\"end\":35169,\"start\":35168},{\"end\":35178,\"start\":35177},{\"end\":35187,\"start\":35186},{\"end\":35189,\"start\":35188},{\"end\":35200,\"start\":35199},{\"end\":35209,\"start\":35208},{\"end\":35424,\"start\":35423},{\"end\":35434,\"start\":35433},{\"end\":35436,\"start\":35435},{\"end\":35447,\"start\":35446},{\"end\":35460,\"start\":35459},{\"end\":35673,\"start\":35672},{\"end\":35683,\"start\":35682},{\"end\":35690,\"start\":35689},{\"end\":35703,\"start\":35702},{\"end\":35956,\"start\":35955},{\"end\":35962,\"start\":35961},{\"end\":35971,\"start\":35967},{\"end\":36150,\"start\":36149},{\"end\":36155,\"start\":36151},{\"end\":36162,\"start\":36161},{\"end\":36173,\"start\":36168},{\"end\":36293,\"start\":36292},{\"end\":36300,\"start\":36299},{\"end\":36313,\"start\":36312},{\"end\":36324,\"start\":36323},{\"end\":36335,\"start\":36334},{\"end\":36343,\"start\":36342},{\"end\":36355,\"start\":36354},{\"end\":36369,\"start\":36368},{\"end\":36699,\"start\":36698},{\"end\":36706,\"start\":36705},{\"end\":36708,\"start\":36707},{\"end\":36715,\"start\":36714},{\"end\":36717,\"start\":36716},{\"end\":36899,\"start\":36898},{\"end\":36901,\"start\":36900},{\"end\":36908,\"start\":36907},{\"end\":36910,\"start\":36909},{\"end\":37083,\"start\":37082},{\"end\":37095,\"start\":37094},{\"end\":37102,\"start\":37101},{\"end\":37293,\"start\":37292},{\"end\":37464,\"start\":37463},{\"end\":37473,\"start\":37472},{\"end\":37482,\"start\":37481},{\"end\":37492,\"start\":37491},{\"end\":37494,\"start\":37493},{\"end\":37726,\"start\":37725},{\"end\":37735,\"start\":37734},{\"end\":37744,\"start\":37743},{\"end\":37754,\"start\":37753},{\"end\":37756,\"start\":37755},{\"end\":37971,\"start\":37967},{\"end\":37978,\"start\":37977},{\"end\":37987,\"start\":37986},{\"end\":37999,\"start\":37998},{\"end\":38007,\"start\":38006},{\"end\":38017,\"start\":38016},{\"end\":38028,\"start\":38027},{\"end\":38038,\"start\":38037},{\"end\":38040,\"start\":38039},{\"end\":38297,\"start\":38296},{\"end\":38305,\"start\":38304},{\"end\":38318,\"start\":38317},{\"end\":38554,\"start\":38550},{\"end\":38561,\"start\":38560},{\"end\":38572,\"start\":38568},{\"end\":38776,\"start\":38775},{\"end\":38783,\"start\":38782},{\"end\":38789,\"start\":38788},{\"end\":38799,\"start\":38794},{\"end\":38809,\"start\":38805},{\"end\":39008,\"start\":39007},{\"end\":39015,\"start\":39014},{\"end\":39021,\"start\":39020},{\"end\":39030,\"start\":39026},{\"end\":39244,\"start\":39243},{\"end\":39251,\"start\":39250},{\"end\":39258,\"start\":39257},{\"end\":39272,\"start\":39268},{\"end\":39467,\"start\":39466},{\"end\":39478,\"start\":39477},{\"end\":39660,\"start\":39659},{\"end\":39671,\"start\":39670},{\"end\":39681,\"start\":39680},{\"end\":39692,\"start\":39691},{\"end\":39704,\"start\":39703},{\"end\":39913,\"start\":39912},{\"end\":39915,\"start\":39914},{\"end\":39926,\"start\":39925},{\"end\":39938,\"start\":39927},{\"end\":39948,\"start\":39947},{\"end\":39961,\"start\":39960},{\"end\":40225,\"start\":40224},{\"end\":40232,\"start\":40231},{\"end\":40239,\"start\":40238},{\"end\":40245,\"start\":40244},{\"end\":40489,\"start\":40488},{\"end\":40496,\"start\":40495},{\"end\":40503,\"start\":40502},{\"end\":40511,\"start\":40510},{\"end\":40719,\"start\":40715},{\"end\":40726,\"start\":40725},{\"end\":40733,\"start\":40732},{\"end\":40735,\"start\":40734},{\"end\":40964,\"start\":40963},{\"end\":40974,\"start\":40973},{\"end\":40983,\"start\":40982},{\"end\":41176,\"start\":41175},{\"end\":41184,\"start\":41183},{\"end\":41194,\"start\":41193},{\"end\":41408,\"start\":41407},{\"end\":41416,\"start\":41415},{\"end\":41619,\"start\":41618},{\"end\":41627,\"start\":41626},{\"end\":41629,\"start\":41628},{\"end\":41638,\"start\":41637},{\"end\":41640,\"start\":41639},{\"end\":41650,\"start\":41649},{\"end\":41652,\"start\":41651},{\"end\":41934,\"start\":41933},{\"end\":41943,\"start\":41942},{\"end\":41952,\"start\":41951},{\"end\":41965,\"start\":41964},{\"end\":42166,\"start\":42165},{\"end\":42181,\"start\":42180},{\"end\":42194,\"start\":42193},{\"end\":42198,\"start\":42195},{\"end\":42208,\"start\":42207},{\"end\":42410,\"start\":42409},{\"end\":42416,\"start\":42415},{\"end\":42602,\"start\":42601},{\"end\":42608,\"start\":42607},{\"end\":42610,\"start\":42609},{\"end\":42617,\"start\":42616},{\"end\":42624,\"start\":42623},{\"end\":42837,\"start\":42836},{\"end\":42843,\"start\":42842},{\"end\":42852,\"start\":42851},{\"end\":43077,\"start\":43076},{\"end\":43086,\"start\":43085},{\"end\":43301,\"start\":43300},{\"end\":43310,\"start\":43309},{\"end\":43318,\"start\":43317},{\"end\":43481,\"start\":43480},{\"end\":43490,\"start\":43489},{\"end\":43496,\"start\":43495},{\"end\":43682,\"start\":43681},{\"end\":43691,\"start\":43690}]", "bib_author_last_name": "[{\"end\":32899,\"start\":32891},{\"end\":32908,\"start\":32903},{\"end\":32919,\"start\":32912},{\"end\":32928,\"start\":32923},{\"end\":33168,\"start\":33161},{\"end\":33182,\"start\":33172},{\"end\":33191,\"start\":33186},{\"end\":33543,\"start\":33532},{\"end\":33678,\"start\":33667},{\"end\":33689,\"start\":33682},{\"end\":33702,\"start\":33695},{\"end\":33871,\"start\":33867},{\"end\":33882,\"start\":33878},{\"end\":34060,\"start\":34057},{\"end\":34067,\"start\":34064},{\"end\":34218,\"start\":34205},{\"end\":34225,\"start\":34222},{\"end\":34236,\"start\":34229},{\"end\":34245,\"start\":34240},{\"end\":34442,\"start\":34439},{\"end\":34448,\"start\":34446},{\"end\":34612,\"start\":34606},{\"end\":34621,\"start\":34616},{\"end\":34634,\"start\":34625},{\"end\":34646,\"start\":34640},{\"end\":34659,\"start\":34652},{\"end\":34923,\"start\":34919},{\"end\":34930,\"start\":34927},{\"end\":34939,\"start\":34934},{\"end\":34957,\"start\":34943},{\"end\":34964,\"start\":34961},{\"end\":35175,\"start\":35170},{\"end\":35184,\"start\":35179},{\"end\":35197,\"start\":35190},{\"end\":35206,\"start\":35201},{\"end\":35217,\"start\":35210},{\"end\":35431,\"start\":35425},{\"end\":35444,\"start\":35437},{\"end\":35457,\"start\":35448},{\"end\":35470,\"start\":35461},{\"end\":35680,\"start\":35674},{\"end\":35687,\"start\":35684},{\"end\":35700,\"start\":35691},{\"end\":35713,\"start\":35704},{\"end\":35959,\"start\":35957},{\"end\":35965,\"start\":35963},{\"end\":35976,\"start\":35972},{\"end\":36159,\"start\":36156},{\"end\":36166,\"start\":36163},{\"end\":36177,\"start\":36174},{\"end\":36297,\"start\":36294},{\"end\":36310,\"start\":36301},{\"end\":36321,\"start\":36314},{\"end\":36332,\"start\":36325},{\"end\":36340,\"start\":36336},{\"end\":36352,\"start\":36344},{\"end\":36366,\"start\":36356},{\"end\":36377,\"start\":36370},{\"end\":36384,\"start\":36379},{\"end\":36703,\"start\":36700},{\"end\":36712,\"start\":36709},{\"end\":36721,\"start\":36718},{\"end\":36905,\"start\":36902},{\"end\":36914,\"start\":36911},{\"end\":37092,\"start\":37084},{\"end\":37099,\"start\":37096},{\"end\":37109,\"start\":37103},{\"end\":37299,\"start\":37294},{\"end\":37470,\"start\":37465},{\"end\":37479,\"start\":37474},{\"end\":37489,\"start\":37483},{\"end\":37502,\"start\":37495},{\"end\":37732,\"start\":37727},{\"end\":37741,\"start\":37736},{\"end\":37751,\"start\":37745},{\"end\":37764,\"start\":37757},{\"end\":37975,\"start\":37972},{\"end\":37984,\"start\":37979},{\"end\":37996,\"start\":37988},{\"end\":38004,\"start\":38000},{\"end\":38014,\"start\":38008},{\"end\":38025,\"start\":38018},{\"end\":38035,\"start\":38029},{\"end\":38048,\"start\":38041},{\"end\":38302,\"start\":38298},{\"end\":38315,\"start\":38306},{\"end\":38326,\"start\":38319},{\"end\":38558,\"start\":38555},{\"end\":38566,\"start\":38562},{\"end\":38577,\"start\":38573},{\"end\":38780,\"start\":38777},{\"end\":38786,\"start\":38784},{\"end\":38792,\"start\":38790},{\"end\":38803,\"start\":38800},{\"end\":38814,\"start\":38810},{\"end\":39012,\"start\":39009},{\"end\":39018,\"start\":39016},{\"end\":39024,\"start\":39022},{\"end\":39035,\"start\":39031},{\"end\":39248,\"start\":39245},{\"end\":39255,\"start\":39252},{\"end\":39266,\"start\":39259},{\"end\":39277,\"start\":39273},{\"end\":39475,\"start\":39468},{\"end\":39485,\"start\":39479},{\"end\":39668,\"start\":39661},{\"end\":39678,\"start\":39672},{\"end\":39689,\"start\":39682},{\"end\":39701,\"start\":39693},{\"end\":39709,\"start\":39705},{\"end\":39923,\"start\":39916},{\"end\":39945,\"start\":39939},{\"end\":39958,\"start\":39949},{\"end\":39971,\"start\":39962},{\"end\":40229,\"start\":40226},{\"end\":40236,\"start\":40233},{\"end\":40242,\"start\":40240},{\"end\":40251,\"start\":40246},{\"end\":40493,\"start\":40490},{\"end\":40500,\"start\":40497},{\"end\":40508,\"start\":40504},{\"end\":40516,\"start\":40512},{\"end\":40723,\"start\":40720},{\"end\":40730,\"start\":40727},{\"end\":40741,\"start\":40736},{\"end\":40971,\"start\":40965},{\"end\":40980,\"start\":40975},{\"end\":40990,\"start\":40984},{\"end\":41181,\"start\":41177},{\"end\":41191,\"start\":41185},{\"end\":41200,\"start\":41195},{\"end\":41413,\"start\":41409},{\"end\":41422,\"start\":41417},{\"end\":41624,\"start\":41620},{\"end\":41635,\"start\":41630},{\"end\":41647,\"start\":41641},{\"end\":41663,\"start\":41653},{\"end\":41940,\"start\":41935},{\"end\":41949,\"start\":41944},{\"end\":41962,\"start\":41953},{\"end\":41971,\"start\":41966},{\"end\":42178,\"start\":42167},{\"end\":42191,\"start\":42182},{\"end\":42205,\"start\":42199},{\"end\":42215,\"start\":42209},{\"end\":42413,\"start\":42411},{\"end\":42420,\"start\":42417},{\"end\":42605,\"start\":42603},{\"end\":42614,\"start\":42611},{\"end\":42621,\"start\":42618},{\"end\":42628,\"start\":42625},{\"end\":42840,\"start\":42838},{\"end\":42849,\"start\":42844},{\"end\":42856,\"start\":42853},{\"end\":43083,\"start\":43078},{\"end\":43091,\"start\":43087},{\"end\":43307,\"start\":43302},{\"end\":43315,\"start\":43311},{\"end\":43324,\"start\":43319},{\"end\":43487,\"start\":43482},{\"end\":43493,\"start\":43491},{\"end\":43500,\"start\":43497},{\"end\":43688,\"start\":43683},{\"end\":43697,\"start\":43692}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206764694},\"end\":33108,\"start\":32834},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15922860},\"end\":33482,\"start\":33110},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14605354},\"end\":33629,\"start\":33484},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5821064},\"end\":33824,\"start\":33631},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18182246},\"end\":34029,\"start\":33826},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14206347},\"end\":34145,\"start\":34031},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1307614},\"end\":34394,\"start\":34147},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":420903},\"end\":34554,\"start\":34396},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":40060575},\"end\":34855,\"start\":34556},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14090151},\"end\":35110,\"start\":34857},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18417477},\"end\":35379,\"start\":35112},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3884789},\"end\":35598,\"start\":35381},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":838989},\"end\":35879,\"start\":35600},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2263441},\"end\":36121,\"start\":35881},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2667927},\"end\":36290,\"start\":36123},{\"attributes\":{\"id\":\"b15\"},\"end\":36625,\"start\":36292},{\"attributes\":{\"id\":\"b16\"},\"end\":36852,\"start\":36627},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15749523},\"end\":37023,\"start\":36854},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10849099},\"end\":37242,\"start\":37025},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11951016},\"end\":37400,\"start\":37244},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1610143},\"end\":37656,\"start\":37402},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1212427},\"end\":37922,\"start\":37658},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14113767},\"end\":38238,\"start\":37924},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1629541},\"end\":38463,\"start\":38240},{\"attributes\":{\"id\":\"b24\"},\"end\":38723,\"start\":38465},{\"attributes\":{\"id\":\"b25\"},\"end\":38933,\"start\":38725},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7666818},\"end\":39192,\"start\":38935},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":684325},\"end\":39398,\"start\":39194},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6266225},\"end\":39620,\"start\":39400},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1742103},\"end\":39847,\"start\":39622},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1894290},\"end\":40145,\"start\":39849},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1485453},\"end\":40432,\"start\":40147},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":382798},\"end\":40643,\"start\":40434},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":591674},\"end\":40910,\"start\":40645},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1768534},\"end\":41118,\"start\":40912},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9363077},\"end\":41331,\"start\":41120},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1541706},\"end\":41542,\"start\":41333},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":207761262},\"end\":41889,\"start\":41544},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10233982},\"end\":42115,\"start\":41891},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3879875},\"end\":42349,\"start\":42117},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8000561},\"end\":42540,\"start\":42351},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7036324},\"end\":42769,\"start\":42542},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1916689},\"end\":42998,\"start\":42771},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":601334},\"end\":43230,\"start\":43000},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2689559},\"end\":43451,\"start\":43232},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":7657380},\"end\":43605,\"start\":43453},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1726588},\"end\":43837,\"start\":43607}]", "bib_title": "[{\"end\":32887,\"start\":32834},{\"end\":33157,\"start\":33110},{\"end\":33528,\"start\":33484},{\"end\":33663,\"start\":33631},{\"end\":33861,\"start\":33826},{\"end\":34053,\"start\":34031},{\"end\":34201,\"start\":34147},{\"end\":34435,\"start\":34396},{\"end\":34602,\"start\":34556},{\"end\":34915,\"start\":34857},{\"end\":35166,\"start\":35112},{\"end\":35421,\"start\":35381},{\"end\":35670,\"start\":35600},{\"end\":35953,\"start\":35881},{\"end\":36147,\"start\":36123},{\"end\":36696,\"start\":36627},{\"end\":36896,\"start\":36854},{\"end\":37080,\"start\":37025},{\"end\":37290,\"start\":37244},{\"end\":37461,\"start\":37402},{\"end\":37723,\"start\":37658},{\"end\":37965,\"start\":37924},{\"end\":38294,\"start\":38240},{\"end\":39005,\"start\":38935},{\"end\":39241,\"start\":39194},{\"end\":39464,\"start\":39400},{\"end\":39657,\"start\":39622},{\"end\":39910,\"start\":39849},{\"end\":40222,\"start\":40147},{\"end\":40486,\"start\":40434},{\"end\":40713,\"start\":40645},{\"end\":40961,\"start\":40912},{\"end\":41173,\"start\":41120},{\"end\":41405,\"start\":41333},{\"end\":41616,\"start\":41544},{\"end\":41931,\"start\":41891},{\"end\":42163,\"start\":42117},{\"end\":42407,\"start\":42351},{\"end\":42599,\"start\":42542},{\"end\":42834,\"start\":42771},{\"end\":43074,\"start\":43000},{\"end\":43298,\"start\":43232},{\"end\":43478,\"start\":43453},{\"end\":43679,\"start\":43607}]", "bib_author": "[{\"end\":32901,\"start\":32889},{\"end\":32910,\"start\":32901},{\"end\":32921,\"start\":32910},{\"end\":32930,\"start\":32921},{\"end\":33170,\"start\":33159},{\"end\":33184,\"start\":33170},{\"end\":33193,\"start\":33184},{\"end\":33545,\"start\":33530},{\"end\":33680,\"start\":33665},{\"end\":33691,\"start\":33680},{\"end\":33704,\"start\":33691},{\"end\":33873,\"start\":33863},{\"end\":33884,\"start\":33873},{\"end\":34062,\"start\":34055},{\"end\":34069,\"start\":34062},{\"end\":34220,\"start\":34203},{\"end\":34227,\"start\":34220},{\"end\":34238,\"start\":34227},{\"end\":34247,\"start\":34238},{\"end\":34444,\"start\":34437},{\"end\":34450,\"start\":34444},{\"end\":34614,\"start\":34604},{\"end\":34623,\"start\":34614},{\"end\":34636,\"start\":34623},{\"end\":34648,\"start\":34636},{\"end\":34661,\"start\":34648},{\"end\":34925,\"start\":34917},{\"end\":34932,\"start\":34925},{\"end\":34941,\"start\":34932},{\"end\":34959,\"start\":34941},{\"end\":34966,\"start\":34959},{\"end\":35177,\"start\":35168},{\"end\":35186,\"start\":35177},{\"end\":35199,\"start\":35186},{\"end\":35208,\"start\":35199},{\"end\":35219,\"start\":35208},{\"end\":35433,\"start\":35423},{\"end\":35446,\"start\":35433},{\"end\":35459,\"start\":35446},{\"end\":35472,\"start\":35459},{\"end\":35682,\"start\":35672},{\"end\":35689,\"start\":35682},{\"end\":35702,\"start\":35689},{\"end\":35715,\"start\":35702},{\"end\":35961,\"start\":35955},{\"end\":35967,\"start\":35961},{\"end\":35978,\"start\":35967},{\"end\":36161,\"start\":36149},{\"end\":36168,\"start\":36161},{\"end\":36179,\"start\":36168},{\"end\":36299,\"start\":36292},{\"end\":36312,\"start\":36299},{\"end\":36323,\"start\":36312},{\"end\":36334,\"start\":36323},{\"end\":36342,\"start\":36334},{\"end\":36354,\"start\":36342},{\"end\":36368,\"start\":36354},{\"end\":36379,\"start\":36368},{\"end\":36386,\"start\":36379},{\"end\":36705,\"start\":36698},{\"end\":36714,\"start\":36705},{\"end\":36723,\"start\":36714},{\"end\":36907,\"start\":36898},{\"end\":36916,\"start\":36907},{\"end\":37094,\"start\":37082},{\"end\":37101,\"start\":37094},{\"end\":37111,\"start\":37101},{\"end\":37301,\"start\":37292},{\"end\":37472,\"start\":37463},{\"end\":37481,\"start\":37472},{\"end\":37491,\"start\":37481},{\"end\":37504,\"start\":37491},{\"end\":37734,\"start\":37725},{\"end\":37743,\"start\":37734},{\"end\":37753,\"start\":37743},{\"end\":37766,\"start\":37753},{\"end\":37977,\"start\":37967},{\"end\":37986,\"start\":37977},{\"end\":37998,\"start\":37986},{\"end\":38006,\"start\":37998},{\"end\":38016,\"start\":38006},{\"end\":38027,\"start\":38016},{\"end\":38037,\"start\":38027},{\"end\":38050,\"start\":38037},{\"end\":38304,\"start\":38296},{\"end\":38317,\"start\":38304},{\"end\":38328,\"start\":38317},{\"end\":38560,\"start\":38550},{\"end\":38568,\"start\":38560},{\"end\":38579,\"start\":38568},{\"end\":38782,\"start\":38775},{\"end\":38788,\"start\":38782},{\"end\":38794,\"start\":38788},{\"end\":38805,\"start\":38794},{\"end\":38816,\"start\":38805},{\"end\":39014,\"start\":39007},{\"end\":39020,\"start\":39014},{\"end\":39026,\"start\":39020},{\"end\":39037,\"start\":39026},{\"end\":39250,\"start\":39243},{\"end\":39257,\"start\":39250},{\"end\":39268,\"start\":39257},{\"end\":39279,\"start\":39268},{\"end\":39477,\"start\":39466},{\"end\":39487,\"start\":39477},{\"end\":39670,\"start\":39659},{\"end\":39680,\"start\":39670},{\"end\":39691,\"start\":39680},{\"end\":39703,\"start\":39691},{\"end\":39711,\"start\":39703},{\"end\":39925,\"start\":39912},{\"end\":39947,\"start\":39925},{\"end\":39960,\"start\":39947},{\"end\":39973,\"start\":39960},{\"end\":40231,\"start\":40224},{\"end\":40238,\"start\":40231},{\"end\":40244,\"start\":40238},{\"end\":40253,\"start\":40244},{\"end\":40495,\"start\":40488},{\"end\":40502,\"start\":40495},{\"end\":40510,\"start\":40502},{\"end\":40518,\"start\":40510},{\"end\":40725,\"start\":40715},{\"end\":40732,\"start\":40725},{\"end\":40743,\"start\":40732},{\"end\":40973,\"start\":40963},{\"end\":40982,\"start\":40973},{\"end\":40992,\"start\":40982},{\"end\":41183,\"start\":41175},{\"end\":41193,\"start\":41183},{\"end\":41202,\"start\":41193},{\"end\":41415,\"start\":41407},{\"end\":41424,\"start\":41415},{\"end\":41626,\"start\":41618},{\"end\":41637,\"start\":41626},{\"end\":41649,\"start\":41637},{\"end\":41665,\"start\":41649},{\"end\":41942,\"start\":41933},{\"end\":41951,\"start\":41942},{\"end\":41964,\"start\":41951},{\"end\":41973,\"start\":41964},{\"end\":42180,\"start\":42165},{\"end\":42193,\"start\":42180},{\"end\":42207,\"start\":42193},{\"end\":42217,\"start\":42207},{\"end\":42415,\"start\":42409},{\"end\":42422,\"start\":42415},{\"end\":42607,\"start\":42601},{\"end\":42616,\"start\":42607},{\"end\":42623,\"start\":42616},{\"end\":42630,\"start\":42623},{\"end\":42842,\"start\":42836},{\"end\":42851,\"start\":42842},{\"end\":42858,\"start\":42851},{\"end\":43085,\"start\":43076},{\"end\":43093,\"start\":43085},{\"end\":43309,\"start\":43300},{\"end\":43317,\"start\":43309},{\"end\":43326,\"start\":43317},{\"end\":43489,\"start\":43480},{\"end\":43495,\"start\":43489},{\"end\":43502,\"start\":43495},{\"end\":43690,\"start\":43681},{\"end\":43699,\"start\":43690}]", "bib_venue": "[{\"end\":32949,\"start\":32930},{\"end\":33267,\"start\":33193},{\"end\":33549,\"start\":33545},{\"end\":33708,\"start\":33704},{\"end\":33910,\"start\":33884},{\"end\":34073,\"start\":34069},{\"end\":34251,\"start\":34247},{\"end\":34454,\"start\":34450},{\"end\":34692,\"start\":34661},{\"end\":34970,\"start\":34966},{\"end\":35223,\"start\":35219},{\"end\":35476,\"start\":35472},{\"end\":35719,\"start\":35715},{\"end\":35982,\"start\":35978},{\"end\":36183,\"start\":36179},{\"end\":36446,\"start\":36386},{\"end\":36727,\"start\":36723},{\"end\":36920,\"start\":36916},{\"end\":37115,\"start\":37111},{\"end\":37305,\"start\":37301},{\"end\":37508,\"start\":37504},{\"end\":37770,\"start\":37766},{\"end\":38054,\"start\":38050},{\"end\":38332,\"start\":38328},{\"end\":38548,\"start\":38465},{\"end\":38773,\"start\":38725},{\"end\":39041,\"start\":39037},{\"end\":39283,\"start\":39279},{\"end\":39491,\"start\":39487},{\"end\":39715,\"start\":39711},{\"end\":39977,\"start\":39973},{\"end\":40257,\"start\":40253},{\"end\":40522,\"start\":40518},{\"end\":40757,\"start\":40743},{\"end\":40996,\"start\":40992},{\"end\":41206,\"start\":41202},{\"end\":41428,\"start\":41424},{\"end\":41696,\"start\":41665},{\"end\":41977,\"start\":41973},{\"end\":42221,\"start\":42217},{\"end\":42426,\"start\":42422},{\"end\":42634,\"start\":42630},{\"end\":42862,\"start\":42858},{\"end\":43097,\"start\":43093},{\"end\":43330,\"start\":43326},{\"end\":43506,\"start\":43502},{\"end\":43703,\"start\":43699}]"}}}, "year": 2023, "month": 12, "day": 17}