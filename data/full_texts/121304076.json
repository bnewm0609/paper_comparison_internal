{"id": 121304076, "updated": "2023-10-01 15:03:12.395", "metadata": {"title": "MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning", "authors": "[{\"first\":\"Sumanth\",\"last\":\"Chennupati\",\"middle\":[]},{\"first\":\"Ganesh\",\"last\":\"Sistu\",\"middle\":[]},{\"first\":\"Senthil\",\"last\":\"Yogamani\",\"middle\":[]},{\"first\":\"Samir\",\"last\":\"Rawashdeh\",\"middle\":[\"A\"]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "publication_date": {"year": 2019, "month": 4, "day": 15}, "abstract": "Multi-task learning is commonly used in autonomous driving for solving various visual perception tasks. It offers significant benefits in terms of both performance and computational complexity. Current work on multi-task learning networks focus on processing a single input image and there is no known implementation of multi-task learning handling a sequence of images. In this work, we propose a multi-stream multi-task network to take advantage of using feature representations from preceding frames in a video sequence for joint learning of segmentation, depth, and motion. The weights of the current and previous encoder are shared so that features computed in the previous frame can be leveraged without additional computation. In addition, we propose to use the geometric mean of task losses as a better alternative to the weighted average of task losses. The proposed loss function facilitates better handling of the difference in convergence rates of different tasks. Experimental results on KITTI, Cityscapes and SYNTHIA datasets demonstrate that the proposed strategies outperform various existing multi-task learning solutions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.08492", "mag": "2964089273", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ChennupatiSYR19", "doi": "10.1109/cvprw.2019.00159"}}, "content": {"source": {"pdf_hash": "09e4c0a4e1bd99cb44039d08c743ad53ca14476b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.08492v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.08492", "status": "GREEN"}}, "grobid": {"id": "4d792fbc6734f5304510e35ffb057e3dd58956cd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/09e4c0a4e1bd99cb44039d08c743ad53ca14476b.txt", "contents": "\nMultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning\n\n\nSumanth Chennupati \nValeo North America\n\n\nUniversity of Michigan-Dearborn\n\n\nGanesh Sistu ganesh.sistu@valeo.com \nValeo Vision Systems\n\n\nSenthil Yogamani senthil.yogamani@valeo.com \nValeo Vision Systems\n\n\nSamir A Rawashdeh \nUniversity of Michigan-Dearborn\n\n\nMultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning\n\nMulti-task learning is commonly used in autonomous driving for solving various visual perception tasks. It offers significant benefits in terms of both performance and computational complexity. Current work on multi-task learning networks focus on processing a single input image and there is no known implementation of multi-task learning handling a sequence of images. In this work, we propose a multistream multi-task network to take advantage of using feature representations from preceding frames in a video sequence for joint learning of segmentation, depth, and motion. The weights of the current and previous encoder are shared so that features computed in the previous frame can be leveraged without additional computation. In addition, we propose to use the geometric mean of task losses as a better alternative to the weighted average of task losses. The proposed loss function facilitates better handling of the difference in convergence rates of different tasks. Experimental results on KITTI, Cityscapes and SYNTHIA datasets demonstrate that the proposed strategies outperform various existing multi-task learning solutions.\n\nIntroduction\n\nMulti-task learning (MTL) [2] aims to jointly solve multiple tasks by leveraging the underlying similarities between independent or interdependent tasks. It is perceived as an attempt to improve generalization by learning a common feature representation for multiple tasks. Improvements in prediction accuracy and reduced computation complexities are significant benefits of MTL. This allowed deployment of MTL in various applications in computer vision (especially scene understanding) [55,22,4], natural language processing [43,11], speech recognition [57,50], reinforcement learning [9,8], drug discovery [34,25], etc.\n\nMTL networks were mainly built using Convolution Neural Networks (CNNs). These networks were usually limited to operate on a single stream of input data. However, Figure 1: Illustration of MultiNet++ where feature aggregation is performed to combine intermediate output data obtained from a shared encoder that operates on multiple input streams (Frames 't' and 't-1'). The aggregated features are later processed by task specific decoders. numerous works demonstrate using multiple streams of data as input to CNNs can improve performance drastically compared to using a single stream of input data. Recent attempts that use consecutive frames in a video sequence for semantic segmentation [46,51,48], activity recognition [19,49], optical flow estimation [35], moving object detection [47,56] are examples demonstrating the benefits of using multiple streams of input data. Similarly, a pair of images from stereo vision cameras [28] or multiple images from different cameras of a surround view system of a car can also be processed as multiple streams of input to CNNs. Some works considered processing input data from different domains [41] to solve certain tasks that require multimodal data representations. These significant benefits demand the construction of a multi-task learning network that can operate on multiple streams of input data. Thus, we propose MultiNet++, a novel multi-task network using simple feature aggrega-tion methods as shown in Figure 1 to combine multiple streams of input data, which can be further processed by task-specific decoders. Figure 1 illustrates a generic way to aggregate features temporally and we make use of a simple summation junction to combine temporal features in our experiments. MultiNet++ would be ideal to process video sequences for tasks like semantic segmentation, depth estimation, optical flow estimation, object detection and tracking, etc. with improved efficiency. We also propose a novel loss strategy for multi-task learning based on geometric mean representation to prioritize learning of all tasks equally. The motivation for MultiNet++ is derived from our position paper NeurAll [52] which proposes to move towards a unified visual perception model for autonomous driving. We propose to use three diverse tasks namely segmentation, depth estimation and motion segmentation which make use of appearance, geometry and motion cues respectively.\n\nThe rest of the contents in this paper are structured as follows. Section 2 reviews related work using feature aggregation for multiple streams of inputs to CNNs and different task loss weighing strategies used in MTL. Section 3 discusses in detail the proposed MultiNet++ network along with the geometric loss strategy used in this paper. Section 4 presents the experimental results on automotive datasets mainly KITTI [12], Cityscapes [6] and SYNTHIA [39]. Finally, Section 5 summarizes the paper with key observations and concluding remarks.\n\n\nRelated Work\n\n\nMulti-Task Learning\n\nMulti-task learning typically consists of two blocks, shared parameters, and task-specific parameters. Shared parameters are learned to represent commonalities between several tasks while task-specific parameters are learned to perform independent processing. In MTL networks built using CNNs, shared parameters are called encoders as they perform the key feature extraction and the task-specific parameters are called decoders as they decode the information from encoders. MTL networks are classified into hard parameter sharing or soft parameter sharing categories based on how they share their parameters. In hard parameter sharing, initial layers or parameters are shared between different tasks such that these parameters are common for all tasks. In soft parameter sharing, different tasks are allowed to have different initial layers with some extent of sharing between them. Cross stitch [31] and sluice networks [40] are examples of soft parameter sharing. Majority of the works in MTL use hard parameter sharing as it is easier to build and computationally less complex.\n\nThe performance of the MTL network is highly dependent on their shared parameters as they contain the knowledge learned from different tasks [2,1,38]. Inappropriate learning of these parameters can induce biased representations for a particular task which can hurt the performance of MTL networks. This phenomenon is referred to as negative transfer learning. In order to prevent it, meaningful feature representations and balanced learning methods are required.\n\n\nFeature Aggregation\n\nDifferent outputs from initial or mid-level convolution layers from CNNs (referred to as extracted features) are forwarded to the next stage of processing using feature aggregation. Feature aggregation is a meaningful way to combine these extracted features. These features can be extracted from different CNNs operating on different input data [62,37] or from a CNN operating on different resolutions of input [24]. Ranjan et al. [36] combines intermediate outputs from a CNN and passes to next stages of processing. Yu et al. [60] proposed several possibilities of feature aggregation.\n\nThere are plenty of choices to perform feature aggregation. These choices range from using simple concatenation techniques to complex Long Short Term Memory units (LSTMs) [17] or recurrent units. Simple concatenation or addition layers can capture short term temporal cues from a video sequence. Sun et al. [54] combine spatial and temporal features from video sequences for human activity recognition and Karpathy et al. [19] combine features from inputs separated by 15 frames in a video for classification. Hei Ng et al. [32] proposed several convolution and pooling operations to combine features for video classification while Sistu et al. [51] used simple 1\u00d71 bottleneck convolutions to combine features from consecutive frames for video segmentation.\n\nIn automotive or indoor robotic visual perception problems, simple concatenation techniques perform well but they fall short in some applications like video captioning [10,33] or summarization [42] where long term dependencies are required. LSTMs in such cases offer a better alternative [59,45]. Convolution-LSTMs (Conv-LSTMs) [58,53] and 3D convolutions [18] are other options. However, these options incur additional computational complexity and they are needed mainly for aggregation of features that are significant for long term dependencies.\n\n\nMulti-Task Loss\n\nWith the growing popularity of MTL, it is worth considering the possibility of imbalances in training an MTL network. It is often observed that some tasks dominate others during the training phase [14]. This dominance can be attributed to variations in task heuristics like complexities, uncertainties, and magnitudes of losses etc. Therefore an appropriate loss or prioritization strategy for all tasks in an MTL is a necessity.\n\nEarly works in MTL [55,22], use a weighted arith- Figure 2: Illustration of the MultiNet++ network operating on consecutive frames of input video sequence. Consecutive frames are processed by a shared siamese-style encoder and extracted features are concatenated and processed by task specific segmentation, depth estimation and moving object detection decoders. metic sum of individual task losses. Later, several works attempted to balance the task weights using certain task heuristics discussed earlier. Kendall et al. [20] proposed to use homoscedastic uncertainty of tasks to weigh them. This approach requires explicit modeling of uncertainty and more importantly, the task weights remain constant.\n\nGradNorm [3] is another notable work in which Chen et al. proposes to normalize gradients from all tasks to a common scale during backpropagation. Lui et al. [26] proposed Dynamic Weight Average (DWA) which uses an average of task losses over time to weigh the task losses. Guo et al. [14] on the other hand proposed dynamic task prioritization where the changes in the difficulty of tasks adjust the task weights. This allows distributing focus on harder problems first and then on less challenging tasks. On another hand, Liu et al. devised a different strategy to use a reinforcement learning based approach to learn optimal task weights. However, this method isn't simple and it brings additional complexity to the training phase.\n\nIn contrast to modeling multi-task problem as a single objective problem, Sener and Koltun [44] proposed to model it as a multi-optimization problem. Zhang and Yeung [61] proposed a convex formulation for multi-task learning and Desideri [7] proposed a multiple-gradient descent algorithm. In summary, these strategies either involve an explicit definition of loss function using task heuristics or require complex optimization techniques. Therefore, a loss strategy with minimal design complexities will be well suited for multi-task learning to accommodate a virtually unlimited number of joint tasks.\n\n\nProposed Solution\n\nWe introduce our novel multi-task network MultiNet++, that is capable of processing multiple streams of input data. The proposed architecture is scalable and can be readily applied in any multi-task problem. In the following subsection, we discuss how we built our MultiNet++ network shown in Figure 2.\n\n\nMulti-stream Multi-task Architecture\n\nMultiNet++ is a simple multi-task network with the ability to process multiple streams of input data. It is built using three main components, 1) Encoders that feed multiple streams of input into the network, 2) Feature aggregation layers that concatenate the encoded feature vectors from multiple streams and 3) Task-specific decoders that operate on aggregated feature space to perform task-specific operations. In this paper, we use MultiNet++ for joint semantic segmentation, depth estimation and moving object detection (or simply motion) on video sequences. We share the encoder between two consecutive frames from a given video sequence as shown in Figure 2. This can significantly reduce the computational load as the encoders require a daunting number of parameters. These input frames can be selected sparsely or densely from a video sequence by observing its motion histogram. One can also choose to pass keyframes as proposed by Kulhare et al. [23].\n\nOur encoders are selected by removing fully connected layers from ResNet-50 [16]. Outputs from ReLU [15] activation at layers 23, 39 and 46 from ResNet-50 [16] encoder are extracted and sent to feature aggregation layers. These feature maps extracted from different streams of inputs are concatenated and sent to task-specific decoders as shown in Figure 1. Segmentation decoder is built using FCN8 [27] architecture that comprises of 3 upsampling layers and skip connections from aggregated feature maps as shown in Figure 2. The final layer consists of softmax [13] units to predict pixel-wise classification labels. Similarly, we construct a motion decoder by changing the number of output classes in softmax units. Depth decoder is built by replacing softmax with regression units.\n\n\nGeometric Loss Strategy\n\nWe discussed the importance of a loss strategy that requires minimal effort during design phase in Section 2.3. The commonly used loss combination function is arithmetic mean and it suffers from differences in the scale of the individual losses. This is partially alleviated by weighted average of the losses but it is difficult to tune manually. We were motivated to explore geometric loss combination which is invariant to the scale of the individual losses. Thus we express the total loss of a multi-task learning problem as geometric mean of individual task losses. We refer to this as Geometric Loss Strategy (GLS). For an n-task problem with task losses 'L 1 ','L 2 ' . . . 'L n ', we express total loss as:\nL T otal = n i=1 n L i(1)\nFor example, in a 3-task problem with losses 'L 1 ','L 2 ' and 'L 3 ', we express total loss:\nL T otal = 3 L 1 L 2 L 3(2)\nEquations 1 and 2 are quite popular in geometric programming. This loss function is differentiable and can be optimized using an optimizer like Stochastic Gradient Descent (SGD). In fact, this definition makes sure that all tasks are making progress. We adapt our loss function to focus or give more attention to certain tasks by introducing Focused Loss Strategy (FLS) where we multiply geometric mean of losses of focused tasks to existing loss function. In this case, we define loss function with focus on m (m \u2264 n) important tasks as:\nL T otal = n i=1 n L i \u00d7 m j=1 m L j(3)\nEquation 3 provides an opportunity to focus on important tasks in a multi-task learning problem. Here we assume that the tasks are ordered in terms of priority so that first m tasks out of the total n tasks gets higher weightage.\n\nApplication of log function converts the product of losses to sum of log of individual losses and thus can be interpreted to be equivalent to normalizing individual losses and then adding them. However, it is computationally complex to make use of log function.\n\n\nExperiments and Results\n\nIn this section, we discuss the datasets used for evaluating the efficacy of the proposed models. Later, we discuss in detail how we constructed the proposed models and provide a complexity analysis of each. We also discuss the optimization strategies used during the training phase. Finally, we provide the results obtained along with a discussion.\n\n\nDatasets\n\nKITTI [12], Cityscapes [6] and SYNTHIA [39] are popular automotive datasets. KITTI has annotations for several tasks including semantic segmentation, depth estimation, object detection, etc. However, these annotations were done separately for each task and the input is not always common across the tasks. KITTI Stereo 2015 [30,29] dataset provides stereo images for depth estimation. A subset of these images is labeled for KITTI semantic segmentation [12]. This dataset consists of 200 train images and 200 test images. Cityscapes [6] dataset provides both segmentation and depth estimation annotations for \u2248 3500 images. Motion labels for these datasets are provided by Vertens et al. [56]. SYNTHIA [39] is a synthetic dataset that provides segmentation and depth annotations for raw video sequences simulated in different weather, light conditions and road types. KITTI [12] and Cityscapes [6] provide segmentation labels for 20 categories while SYNTHIA [39] dataset provides segmentation labels for 13 categories. In KITTI [12] and Cityscapes [6] datasets, images are sampled and annotated sparsely from raw videos. This poses a challenge to approaches that use temporal methods for segmentation or motion detection tasks in videos. In addition to KITTI [12] and Cityscapes [6] Table 2: Comparative study: Parameters needed to construct 1-task segmentation, depth and motion, 2-task segmentation and depth, 2-task segmentation and motion and 3-task segmentation, depth and motion models. We compare 2-task and 3-task models that operate on 1-frame and 2-frames.\n\nlike city) from SYNTHIA dataset for training and validation respectively in our experiments. These sequences provide segmentation and depth annotations for consecutive images in a video sequence. Thus they are more suitable for evaluating our multi-task model which operates on multiple streams of input data. Table 1 provides a summary of different properties of the 3 datasets discussed so far.\n\n\nModel Analysis\n\nWe constructed several models to evaluate the benefits of the proposed MultiNet++. We build 3 single task baseline models for segmentation, depth and motion tasks using ResNet-50 [16] as an encoder and different task-specific decoders as discussed in Section 3.1. Segmentation decoder predicts pixel-wise labels from 20 different categories for input in KITTI [12] & Cityscapes [6] datasets, while the decoder predicts from 13 categories in SYNTHIA [39] dataset. Depth decoder outputs a 16-bit integer at every pixel location to predict depth and motion decoder predicts a binary classification label for every pixel to classify as moving or static object. These models process one frame of input data. We also constructed 2-task and 3-task models that operate on a single frame and 2 consecutive frames of an input video sequence. MultiNet++ refers to models that operate on 2 consecutive frames which are built using feature aggregation as discussed in Section 3.1. Table 2 provides details about number parameters required to construct different models.\n\nMajority of computational load arises from ResNet-50 [16] encoder. Due to this property, 2-task and 3-task models required the almost same number of parameters as 1task model. This is one of the main reasons why multi-task networks are computationally efficient and favor embedded deployment. We build our 2-frame models with relatively very little increase in complexity (\u2248 100K parameters) by reusing the encoder between 2-frames. In 2-frames model, the aggregated features are larger in size when compared to the 1-frame model. It resulted in an increase of parameters.\n\n\nOptimization\n\nWe implemented our proposed models using Keras [5]. In all our experiments, we re-size the input images to 224\u00d7384. We used only 2-frames for feature aggregation because adding more frames would increase computational complexity with insignificant performance gains as demonstrated by Sistu et al. [51]. In our multi-task learning networks, we define the loss functions for each task separately and feed them to our geometric loss strategy (GLS) proposed in Section 2.3. For semantic segmentation and motion, we use pixel-wise cross-entropy loss for C classes averaged over a mini-batch with N samples as shown in Equation 4.\nL Seg or L M otion = \u2212 N j=1 C i=1 y i,j log(p i,j )(4)\nFor depth estimation, we use Huber loss as defined in Equation 5 with \u03b4 =250.\nL Depth = 1 2 [y \u2212\u0177] 2 : |y \u2212\u0177| \u2264 \u03b4 \u03b4 (|y \u2212\u0177| \u2212 \u03b4/2) : otherwise(5)\nThe total loss L T otal is defined as:  Table 3: Improvements in learning segmentation, depth estimation and motion detection as multiple tasks using equal weights, proposed geometric loss strategy (GLS) and 2 stream feature aggregation with GLS (MultiNet++) vs independent networks (1-Task) on KITTI, Cityscapes and SYNTHIA datasets.   [12] and Cityscapes [6] datasets.\nL T otal = 3 L Seg L Depth L M otion(6)\nWe optimize this loss function in our training phase using Adam optimizer [21]. Accuracy is used as an evaluation metric for segmentation and motion tasks while regression accuracy is used for depth estimation.\n\n\nResults\n\nIn Table 3, we compare the results of 2-task models and 3-task models using our geometric loss strategy (GLS) against naive equal task weight method. We also compare their performances with 1-task segmentation, depth and motion models. Our GLS method shows significant improvements in performance over equal weights method in both 2task and 3-task models. In Table 4, we compare the results of 3-task models using our geometric loss strategy (GLS) against naive equal task weights, uncertainty weight method proposed by Kendal et al. [20] and Dynamic Weight Average (DWA) proposed by Liu et al. [26]. In Figure 4 (4a, 4b, 4c, 4d, 4e and 4f), we show how validation loss for these models change over time during training phase. Our models using GLS demonstrated faster convergence on all tasks. In 3-task models solving for segmentation, depth, and motion, depth is usually the most complex task. Figures 4b and 4e show that depth estimation on KITTI [12] and Cityscapes [6] requires longer convergence time compared to segmentation (Figures 4a and 4d) and motion tasks (Figures 4c and  4f). In these cases, our GLS method has shown faster convergence compared to uncertainty [20] and DWA [26] methods. While solving for multiple tasks, uncertainty [20] and DWA [26] weigh the tasks that converge quickly higher than  the others. This led to faster convergence in segmentation and motion tasks but late convergence in depth task. In such circumstances, the encoder parameters might be biased towards segmentation and motion tasks. This can result in imbalanced learning of depth task. Our GLS method expresses the total loss as the geometric mean of individual losses, so it doesn't prioritize one task higher than others. In this way, we achieve balanced training and improved performances compared to other techniques. In Table 3, we also compare 2-task and 3-task mod- els with our novel MultiNet++ which uses both feature aggregation (for 2-frame input) and GLS. In KITTI [12] dataset, input images are sparsely sampled from raw video sequences which hinder the performance gains of Multi-Net++. In Cityscapes [6] dataset, MultiNet++ outperforms single task models by 4% and 3% for segmentation and depth tasks respectively as they provide images sampled closely compared to KITTI dataset. These improvements are much better in SYNTHIA [39] dataset (4% and 5% for segmentation and depth estimation tasks respectively) as they provide continuous frames of video sequences. We achieve similar performances for motion task compared to 1-task models.\n\nWe compare qualitative results of MultiNet++ with 1task segmentation model on Cityscapes [6] dataset in Figure  3. The main difference between 1-task models and 3-task models is that the latter have learned representations from other tasks using a common encoder. Knowledge acquired through these representations helps 3-task model to identify semantic boundaries better compared to 1-task model. It is clearly evident that MultiNet++ model has improved performance. Our models detect traffic signs, lights and other near range objects better compared to other models on KITTI dataset [12] as shown in Figure 5.\n\n\nConclusion\n\nWe introduced an efficient way of constructing Multi-Net++, a multi-task learning network that operates on multiple streams of input data. We demonstrated that our geometric loss strategy (GLS) is robust to different task heuristics like complexity, magnitude, etc. We achieved balanced training and improved performances for a multi-task learning network solving different tasks namely segmentation, depth estimation and motion on automotive datasets KITTI, Cityscapes, and SYNTHIA. Our GLS strategy is easy to implement and most importantly it allows for balanced learning of a large number of tasks in multi-task learning without requiring explicit loss modeling when compared to other multi-task learning loss strategies. In the future, we would like to explore the benefits of multi-task learning networks using our efficient feature aggregation and loss strategies for multi-modal data.\n\nFigure 3 :\n3Left to Right: Input Image, Single Task Network outputs, MultiNet++ Output, Ground Truth. More qualitative results of MultiNet++ model can be accessed via this link https://youtu.be/E378PzLq7lQ.\n\nFigure 4 :\n4Change of validation loss (X-axis) over several epochs (Y-axis) during training phase for 1-Task model vs 3-Task models for segmentation, depth and motion tasks on KITTI\n\nFigure 5 :\n5Comparison of Semantic Segmentation results: 1-Task Segmentation vs 3-Task models on KITTI dataset.\n\n\ndatasets, we use SEQS-02 (New York-like city) and SEQS-05 (New York-Method \n\nKITTI & Cityscapes \nSYNTHIA \nEncoder Segmentation Depth Motion \nTotal \nEncoder Segmentation Depth \nTotal \n1-Task Segmentation, Depth or Motion \n1-Task \n23.58M \n0.18M \n-\n-\n23.77M 23.58M \n0.14M \n-\n23.68M \n1-Task \n23.58M \n-\n3.88K \n-\n23.59M 23.58M \n-\n3.87K 23.59M \n1-Task \n23.58M \n-\n-\n8.33K 23.60M \n-\n-\n-\n-\n2-Task Segmentation and Depth \n1-Frame \n23.58M \n0.18M \n3.88K \n-\n23.77M 23.58M \n95.34K \n3.88K 23.69M \n2-Frames 23.58M \n0.26M \n7.46K \n-\n23.86M 23.58M \n0.14M \n7.46K 23.74M \n2-Task Segmentation and Motion \n1-Frame \n23.58M \n0.18M \n-\n8.33K 23.78M \n-\n-\n-\n-\n2-Frames 23.58M \n0.26M \n-\n15.50K 23.86M \n-\n-\n-\n-\n3-Task Segmentation, Depth and Motion \n1-Frame \n23.58M \n0.18M \n3.88K 8.33K 23.79M \n-\n-\n-\n-\n2-Frames 23.58M \n0.26M \n7.46K 15.50K 23.87M \n-\n-\n-\n-\n\n\n\nTable 4 :\n4Comparative Study: Performance of 1-Task, equal weights, 3-task uncertainty[20], Dynamic Weight Average (DWA)[26] and proposed geometric loss strategy (GLS) on KITTI and Cityscapes datasets.\nAcknowledgementsAuthors would like to thank their employer for supporting fundamental research. Authors would also like to thank Dr. Aditya Viswanathan and Dr. Thibault Julliand for helpful discussions.\nUniversal representations: The missing link between faces, text, planktons, and cat breeds. H Bilen, A Vedaldi, arXiv:1701.07275arXiv preprintH. Bilen and A. Vedaldi. Universal representations: The missing link between faces, text, planktons, and cat breeds. arXiv preprint arXiv:1701.07275, 2017. 2\n\nMultitask learning. R Caruana, Machine Learning. 28R. Caruana. Multitask learning. Machine Learning, 28(1):41-75, Jul 1997. 1, 2\n\nGradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Z Chen, V Badrinarayanan, C.-Y. Lee, A Rabinovich, In ICML. 3Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Gradnorm: Gradient normalization for adaptive loss balanc- ing in deep multitask networks. In ICML, 2018. 3\n\nAuxnet: Auxiliary tasks enhanced semantic segmentation for automated driving. S Chennupati, G Sistu, S Yogamani, S Rawashdeh, Proceedings of the 14th International Joint Conference on Computer Vision. the 14th International Joint Conference on Computer VisionSciTePress5INSTICCS. Chennupati, G. Sistu., S. Yogamani., and S. Rawashdeh. Auxnet: Auxiliary tasks enhanced semantic segmentation for automated driving. In Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Com- puter Graphics Theory and Applications -Volume 5: VIS- APP,, pages 645-652. INSTICC, SciTePress, 2019. 1\n\n. F Chollet, F. Chollet et al. Keras. https://keras.io, 2015. 5\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)7M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2, 4, 5, 7, 8\n\nMultiple-gradient descent algorithm (mgda) for multiobjective optimization. J.-A D\u00e9sid\u00e9ri, 350Comptes Rendus MathematiqueJ.-A. D\u00e9sid\u00e9ri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes Rendus Mathema- tique, 350(5-6):313-318, 2012. 3\n\nLearning modular neural network policies for multi-task and multi-robot transfer. C Devin, A Gupta, T Darrell, P Abbeel, S Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network policies for multi-task and multi-robot transfer. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2169-2176, May 2017. 1\n\nDigrad: Multi-task reinforcement learning with shared actions. P Dewangan, S P Teja, K M Krishna, A Sarkar, B Ravindran, abs/1802.10463CoRRP. Dewangan, S. P. Teja, K. M. Krishna, A. Sarkar, and B. Ravindran. Digrad: Multi-task reinforcement learning with shared actions. CoRR, abs/1802.10463, 2018. 1\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L A Hendricks, M Rohrbach, S Venugopalan, S Guadarrama, K Saenko, T Darrell, IEEE Transactions on Pattern Analysis and Machine Intelligence. 394677691J. Donahue, L. A. Hendricks, M. Rohrbach, S. Venugopalan, S. Guadarrama, K. Saenko, and T. Darrell. Long-term recur- rent convolutional networks for visual recognition and de- scription. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 39(4):677691, Apr 2017. 2\n\nMulti-task learning for multiple language translation. D Dong, H Wu, W He, D Yu, H Wang, ACL. D. Dong, H. Wu, W. He, D. Yu, and H. Wang. Multi-task learning for multiple language translation. In ACL, 2015. 1\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research. 78A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. 2, 4, 5, 7, 8\n\nDeep learning. I Goodfellow, Y Bengio, A Courville, MIT PressI. Goodfellow, Y. Bengio, and A. Courville. Deep learn- ing. MIT Press, pages 189-191, 2016. http://www. deeplearningbook.org. 4\n\nDynamic task prioritization for multitask learning. M Guo, A Haque, D.-A Huang, S Yeung, L Fei-Fei, European Conference on Computer Vision. Springer23M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei- Fei. Dynamic task prioritization for multitask learning. In European Conference on Computer Vision, pages 282-299. Springer, 2018. 2, 3\n\nPermitted and forbidden sets in symmetric threshold-linear networks. R H Hahnloser, H S Seung, Advances in Neural Information Processing Systems. R. H. Hahnloser and H. S. Seung. Permitted and forbidden sets in symmetric threshold-linear networks. In Advances in Neural Information Processing Systems, pages 217-223, 2001. 4\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 45K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 770- 778, June 2016. 4, 5\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 98[17] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-1780, Nov. 1997. 2\n\n3d convolutional neural networks for human action recognition. S Ji, W Xu, M Yang, K Yu, IEEE transactions on pattern analysis and machine intelligence. 35S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221- 231, 2013. 2\n\nLarge-scale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, CVPR. 1A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convo- lutional neural networks. In CVPR, 2014. 1, 2\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. A Kendall, Y Gal, R Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)37A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and seman- tics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3, 7\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2014. 7\n\nUbernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. I Kokkinos, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). I. Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In 2017 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 5454-5463, July 2017. 1, 2\n\nKey frame extraction for salient activity recognition. S Kulhare, S Sah, S Pillai, R Ptucha, 23rd International Conference on Pattern Recognition (ICPR). S. Kulhare, S. Sah, S. Pillai, and R. Ptucha. Key frame ex- traction for salient activity recognition. In 2016 23rd Inter- national Conference on Pattern Recognition (ICPR), pages 835-840, Dec 2016. 4\n\nMulti-level and multi-scale feature aggregation using pretrained convolutional neural networks for music auto-tagging. J Lee, J Nam, IEEE signal processing letters. 248J. Lee and J. Nam. Multi-level and multi-scale feature aggregation using pretrained convolutional neural networks for music auto-tagging. IEEE signal processing letters, 24(8):1208-1212, 2017. 2\n\nExploration on Deep Drug Discovery: Representation and Learning. S Liu, University of Wisconsin-MadisonPhD thesisS. Liu. Exploration on Deep Drug Discovery: Represen- tation and Learning. PhD thesis, University of Wisconsin- Madison, 2018. 1\n\nEnd-to-end multi-task learning with attention. S Liu, E Johns, A J Davison, arXiv:1803.1070437arXiv preprintS. Liu, E. Johns, and A. J. Davison. End-to-end multi-task learning with attention. arXiv preprint arXiv:1803.10704, 2018. 3, 7\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 3431-3440, 2015. 4\n\nMulti-view deep learning for consistent semantic mapping with rgb-d cameras. L Ma, J St\u00fcckler, C Kerl, D Cremers, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEL. Ma, J. St\u00fcckler, C. Kerl, and D. Cremers. Multi-view deep learning for consistent semantic mapping with rgb-d cameras. In 2017 IEEE/RSJ International Conference on In- telligent Robots and Systems (IROS), pages 598-605. IEEE, 2017. 1\n\nJoint 3d estimation of vehicles and scene flow. M Menze, C Heipke, A Geiger, ISPRS Workshop on Image Sequence Analysis (ISA). M. Menze, C. Heipke, and A. Geiger. Joint 3d estimation of vehicles and scene flow. In ISPRS Workshop on Image Sequence Analysis (ISA), 2015. 4\n\nObject scene flow. M Menze, C Heipke, A Geiger, ISPRS Journal of Photogrammetry and Remote Sensing. 4JPRS)M. Menze, C. Heipke, and A. Geiger. Object scene flow. ISPRS Journal of Photogrammetry and Remote Sensing (JPRS), 2018. 4\n\nCrossstitch networks for multi-task learning. I Misra, A Shrivastava, A Gupta, M Hebert, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross- stitch networks for multi-task learning. 2016 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), Jun 2016. 2\n\nBeyond short snippets: Deep networks for video classification. J Y Ng, M Hausknecht, S Vijayanarasimhan, O Vinyals, R Monga, G Toderici, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short snip- pets: Deep networks for video classification. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2015. 2\n\nImage description through fusion based recurrent multi-modal learning. R M Oruganti, S Sah, S Pillai, R Ptucha, 2016 IEEE International Conference on Image Processing (ICIP). R. M. Oruganti, S. Sah, S. Pillai, and R. Ptucha. Image de- scription through fusion based recurrent multi-modal learn- ing. In 2016 IEEE International Conference on Image Pro- cessing (ICIP), pages 3613-3617. IEEE, 2016. 2\n\nMassively multitask networks for drug discovery. B Ramsundar, S Kearnes, P Riley, D Webster, D Konerding, V Pande, arXiv:1502.02072arXiv preprintB. Ramsundar, S. Kearnes, P. Riley, D. Webster, D. Konerd- ing, and V. Pande. Massively multitask networks for drug discovery. 2015. arXiv preprint arXiv:1502.02072, 2015. 1\n\nOptical flow estimation using a spatial pyramid network. A Ranjan, M J Black, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Ranjan and M. J. Black. Optical flow estimation using a spatial pyramid network. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 4161-4170, 2017. 1\n\nHyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition. R Ranjan, V M Patel, R Chellappa, IEEE Transactions on Pattern Analysis and Machine Intelligence. 411121135R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(1):121135, Jan 2019. 2\n\nOptical flow augmented semantic segmentation networks for automated driving. H Rashed, S Yogamani, A El-Sallab, P K\u0159ek, M El-Helw, Proceedings of the 14th International Joint Conference on Computer Vision. the 14th International Joint Conference on Computer VisionSciTePress5INSTICCH. Rashed., S. Yogamani., A. El-Sallab., P. K\u0159ek, and M. El- Helw. Optical flow augmented semantic segmentation net- works for automated driving. In Proceedings of the 14th In- ternational Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications -Volume 5: VISAPP,, pages 165-172. INSTICC, SciTePress, 2019. 2\n\nEfficient parametrization of multi-domain deep neural networks. S.-A Rebuffi, H Bilen, A Vedaldi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS.-A. Rebuffi, H. Bilen, and A. Vedaldi. Efficient parametrization of multi-domain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8119-8127, 2018. 2\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, J Materzynska, D Vazquez, A M Lopez, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition5G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3234-3243, 2016. 2, 4, 5, 8\n\nLearning what to share between loosely related tasks. S Ruder, J Bingel, I Augenstein, A S\u00f8gaard, arXiv:1705.08142arXiv preprintS. Ruder, J. Bingel, I. Augenstein, and A. S\u00f8gaard. Learning what to share between loosely related tasks. arXiv preprint arXiv:1705.08142, 2017. 2\n\nMulti-Modal Deep Learning to Understand Vision and Language. S Sah, Rochester Institute of Technology.PhD thesisS. Sah. Multi-Modal Deep Learning to Understand Vision and Language. PhD thesis, Rochester Institute of Technol- ogy., 2018. 1\n\nSemantic text summarization of long videos. S Sah, S Kulhare, A Gray, S Venugopalan, E Prud&apos;hommeaux, R Ptucha, 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). S. Sah, S. Kulhare, A. Gray, S. Venugopalan, E. Prud'Hommeaux, and R. Ptucha. Semantic text summarization of long videos. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 989-997. IEEE, 2017. 2\n\nA hierarchical multi-task approach for learning embeddings from semantic tasks. V Sanh, T Wolf, S Ruder, V. Sanh, T. Wolf, and S. Ruder. A hierarchical multi-task ap- proach for learning embeddings from semantic tasks, 2018. 1\n\nMulti-task learning as multiobjective optimization. O Sener, V Koltun, Advances in Neural Information Processing Systems. O. Sener and V. Koltun. Multi-task learning as multi- objective optimization. In Advances in Neural Information Processing Systems, pages 525-536, 2018. 3\n\nAction recognition using visual attention. S Sharma, R Kiros, R Salakhutdinov, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionS. Sharma, R. Kiros, and R. Salakhutdinov. Action recogni- tion using visual attention. In Proceedings of the IEEE inter- national conference on computer vision, pages 4507-4515, 2015. 2\n\nDeep semantic segmentation for automated driving: Taxonomy, roadmap and challenges. M Siam, S Elkerdawy, M Jagersand, S Yogamani, 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEEM. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani. Deep semantic segmentation for automated driving: Taxon- omy, roadmap and challenges. In 2017 IEEE 20th Inter- national Conference on Intelligent Transportation Systems (ITSC), pages 1-8. IEEE, 2017. 1\n\nModnet: Motion and appearance based moving object detection network for autonomous driving. M Siam, H Mahgoub, M Zahran, S Yogamani, M Jagersand, A El-Sallab, 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEEM. Siam, H. Mahgoub, M. Zahran, S. Yogamani, M. Jager- sand, and A. El-Sallab. Modnet: Motion and appearance based moving object detection network for autonomous driv- ing. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC), pages 2859-2864. IEEE, 2018. 1\n\nConvolutional gated recurrent networks for video semantic segmentation in automated driving. M Siam, S Valipour, M J\u00e4gersand, N Ray, S Yogamani, IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). M. Siam, S. Valipour, M. J\u00e4gersand, N. Ray, and S. Yoga- mani. Convolutional gated recurrent networks for video se- mantic segmentation in automated driving. 2017 IEEE 20th International Conference on Intelligent Transportation Sys- tems (ITSC), pages 1-7, 2017. 1\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Advances in neural information processing systems. K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in neural information processing systems, pages 568-576, 2014. 1\n\nMultitask learning and system combination for automatic speech recognition. O Siohan, D Rybach, 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). O. Siohan and D. Rybach. Multitask learning and sys- tem combination for automatic speech recognition. In 2015 IEEE Workshop on Automatic Speech Recognition and Un- derstanding (ASRU), pages 589-595, Dec 2015. 1\n\nMulti-stream cnn based video semantic segmentation for automated driving. G Sistu, S Chennupati, S Yogamani, Proceedings of the 14th International Joint Conference on Computer Vision. the 14th International Joint Conference on Computer VisionSciTePress5IN-STICCG. Sistu., S. Chennupati, and S. Yogamani. Multi-stream cnn based video semantic segmentation for automated driving. In Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications -Volume 5: VISAPP,, pages 173-180. IN- STICC, SciTePress, 2019. 1, 2, 5\n\nNeurall: Towards a unified model for visual perception in automated driving. G Sistu, I Leang, S Chennupati, S Milz, S Yogamani, S Rawashdeh, arXiv:1902.03589arXiv preprintG. Sistu, I. Leang, S. Chennupati, S. Milz, S. Yogamani, and S. Rawashdeh. Neurall: Towards a unified model for visual perception in automated driving. arXiv preprint arXiv:1902.03589, 2019. 2\n\nPyramid dilated deeper convlstm for video salient object detection. H Song, W Wang, S Zhao, J Shen, K.-M Lam, The European Conference on Computer Vision (ECCV). H. Song, W. Wang, S. Zhao, J. Shen, and K.-M. Lam. Pyra- mid dilated deeper convlstm for video salient object de- tection. In The European Conference on Computer Vision (ECCV), September 2018. 2\n\nHuman action recognition using factorized spatio-temporal convolutional networks. L Sun, K Jia, D.-Y Yeung, B E Shi, IEEE International Conference on Computer Vision (ICCV). L. Sun, K. Jia, D.-Y. Yeung, and B. E. Shi. Human action recognition using factorized spatio-temporal convolutional networks. 2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015. 2\n\nMultinet: Real-time joint semantic reasoning for autonomous driving. M Teichmann, M Weber, M Zllner, R Cipolla, R Urtasun, 2018 IEEE Intelligent Vehicles Symposium (IV). 1M. Teichmann, M. Weber, M. Zllner, R. Cipolla, and R. Ur- tasun. Multinet: Real-time joint semantic reasoning for au- tonomous driving. In 2018 IEEE Intelligent Vehicles Sympo- sium (IV), pages 1013-1020, June 2018. 1, 2\n\nSmsnet: Semantic motion segmentation using deep convolutional neural networks. J Vertens, A Valada, W Burgard, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). J. Vertens, A. Valada, and W. Burgard. Smsnet: Semantic motion segmentation using deep convolutional neural net- works. In 2017 IEEE/RSJ International Conference on In- telligent Robots and Systems (IROS), pages 582-589, Sep. 2017. 1, 4\n\nDeep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. Z Wu, C Valentini-Botinhao, O Watts, S King, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Z. Wu, C. Valentini-Botinhao, O. Watts, and S. King. Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. In 2015 IEEE In- ternational Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), pages 4460-4464, April 2015. 1\n\nConvolutional lstm network: A machine learning approach for precipitation nowcasting. S Xingjian, Z Chen, H Wang, D.-Y Yeung, W.-K Wong, W.-C Woo, Advances in neural information processing systems. S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems, pages 802-810, 2015. 2\n\nDescribing videos by exploiting temporal structure. L Yao, A Torabi, K Cho, N Ballas, C Pal, H Larochelle, A Courville, Advances in Neural Information Processing Systems. L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Describing videos by exploiting tempo- ral structure. In Advances in Neural Information Processing Systems, 2015. 2\n\nDeep layer aggregation. F Yu, D Wang, E Shelhamer, T Darrell, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2\n\nA convex formulation for learning task relationships in multi-task learning. Y Zhang, D.-Y Yeung, UAI. 3Y. Zhang and D.-Y. Yeung. A convex formulation for learn- ing task relationships in multi-task learning. In UAI, 2010. 3\n\nFlow-guided feature aggregation for video object detection. X Zhu, Y Wang, J Dai, L Yuan, Y Wei, The IEEE International Conference on Computer Vision (ICCV). X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei. Flow-guided feature aggregation for video object detection. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017. 2\n", "annotations": {"author": "[{\"end\":175,\"start\":100},{\"end\":235,\"start\":176},{\"end\":303,\"start\":236},{\"end\":356,\"start\":304}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":108},{\"end\":188,\"start\":183},{\"end\":252,\"start\":244},{\"end\":321,\"start\":312}]", "author_first_name": "[{\"end\":107,\"start\":100},{\"end\":182,\"start\":176},{\"end\":243,\"start\":236},{\"end\":309,\"start\":304},{\"end\":311,\"start\":310}]", "author_affiliation": "[{\"end\":140,\"start\":120},{\"end\":174,\"start\":142},{\"end\":234,\"start\":213},{\"end\":302,\"start\":281},{\"end\":355,\"start\":323}]", "title": "[{\"end\":97,\"start\":1},{\"end\":453,\"start\":357}]", "venue": null, "abstract": "[{\"end\":1593,\"start\":455}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1638,\"start\":1635},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2100,\"start\":2096},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2103,\"start\":2100},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2105,\"start\":2103},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2139,\"start\":2135},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2142,\"start\":2139},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2167,\"start\":2163},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2170,\"start\":2167},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2198,\"start\":2195},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2200,\"start\":2198},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2221,\"start\":2217},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2224,\"start\":2221},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2927,\"start\":2923},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2930,\"start\":2927},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2933,\"start\":2930},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2960,\"start\":2956},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2963,\"start\":2960},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2993,\"start\":2989},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3023,\"start\":3019},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3026,\"start\":3023},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3167,\"start\":3163},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3376,\"start\":3372},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4385,\"start\":4381},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5069,\"start\":5065},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5085,\"start\":5082},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5102,\"start\":5098},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6128,\"start\":6124},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6153,\"start\":6149},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6454,\"start\":6451},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6456,\"start\":6454},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6459,\"start\":6456},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7145,\"start\":7141},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7148,\"start\":7145},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7211,\"start\":7207},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7231,\"start\":7227},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7328,\"start\":7324},{\"end\":7560,\"start\":7556},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7696,\"start\":7692},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7811,\"start\":7807},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7913,\"start\":7909},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8034,\"start\":8030},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8316,\"start\":8312},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8319,\"start\":8316},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8436,\"start\":8432},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8439,\"start\":8436},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8476,\"start\":8472},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8479,\"start\":8476},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8504,\"start\":8500},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8913,\"start\":8909},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9166,\"start\":9162},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9169,\"start\":9166},{\"end\":9670,\"start\":9651},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9862,\"start\":9859},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10012,\"start\":10008},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10139,\"start\":10135},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10681,\"start\":10677},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10756,\"start\":10752},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10827,\"start\":10824},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12514,\"start\":12510},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12597,\"start\":12593},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12621,\"start\":12617},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12676,\"start\":12672},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12920,\"start\":12916},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13084,\"start\":13080},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15663,\"start\":15659},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15679,\"start\":15676},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15696,\"start\":15692},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15981,\"start\":15977},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15984,\"start\":15981},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16110,\"start\":16106},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16189,\"start\":16186},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16345,\"start\":16341},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16359,\"start\":16355},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16531,\"start\":16527},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16550,\"start\":16547},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16615,\"start\":16611},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16685,\"start\":16681},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16704,\"start\":16701},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16916,\"start\":16912},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16935,\"start\":16932},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17819,\"start\":17815},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18000,\"start\":17996},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18017,\"start\":18014},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18089,\"start\":18085},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18751,\"start\":18747},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19333,\"start\":19330},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":19585,\"start\":19581},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20452,\"start\":20448},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20471,\"start\":20468},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20600,\"start\":20596},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21282,\"start\":21278},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21343,\"start\":21339},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21698,\"start\":21694},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21717,\"start\":21714},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21923,\"start\":21919},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21936,\"start\":21932},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21996,\"start\":21992},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22009,\"start\":22005},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22723,\"start\":22719},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22860,\"start\":22857},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23087,\"start\":23083},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23387,\"start\":23384},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23884,\"start\":23880},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26236,\"start\":26232},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26270,\"start\":26266}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25021,\"start\":24814},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25204,\"start\":25022},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25317,\"start\":25205},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26144,\"start\":25318},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":26347,\"start\":26145}]", "paragraph": "[{\"end\":2230,\"start\":1609},{\"end\":4643,\"start\":2232},{\"end\":5189,\"start\":4645},{\"end\":6308,\"start\":5228},{\"end\":6772,\"start\":6310},{\"end\":7383,\"start\":6796},{\"end\":8142,\"start\":7385},{\"end\":8692,\"start\":8144},{\"end\":9141,\"start\":8712},{\"end\":9848,\"start\":9143},{\"end\":10584,\"start\":9850},{\"end\":11189,\"start\":10586},{\"end\":11513,\"start\":11211},{\"end\":12515,\"start\":11554},{\"end\":13302,\"start\":12517},{\"end\":14043,\"start\":13330},{\"end\":14163,\"start\":14070},{\"end\":14730,\"start\":14192},{\"end\":15000,\"start\":14771},{\"end\":15263,\"start\":15002},{\"end\":15640,\"start\":15291},{\"end\":17219,\"start\":15653},{\"end\":17617,\"start\":17221},{\"end\":18692,\"start\":17636},{\"end\":19266,\"start\":18694},{\"end\":19908,\"start\":19283},{\"end\":20042,\"start\":19965},{\"end\":20481,\"start\":20111},{\"end\":20732,\"start\":20522},{\"end\":23293,\"start\":20744},{\"end\":23906,\"start\":23295},{\"end\":24813,\"start\":23921}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14069,\"start\":14044},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14191,\"start\":14164},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14770,\"start\":14731},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19964,\"start\":19909},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20110,\"start\":20043},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20521,\"start\":20482}]", "table_ref": "[{\"end\":16943,\"start\":16936},{\"end\":17538,\"start\":17531},{\"end\":18611,\"start\":18604},{\"end\":20158,\"start\":20151},{\"end\":20754,\"start\":20747},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21110,\"start\":21103},{\"end\":22574,\"start\":22567}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1607,\"start\":1595},{\"attributes\":{\"n\":\"2.\"},\"end\":5204,\"start\":5192},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5226,\"start\":5207},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6794,\"start\":6775},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8710,\"start\":8695},{\"attributes\":{\"n\":\"3.\"},\"end\":11209,\"start\":11192},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11552,\"start\":11516},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13328,\"start\":13305},{\"attributes\":{\"n\":\"4.\"},\"end\":15289,\"start\":15266},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15651,\"start\":15643},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17634,\"start\":17620},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19281,\"start\":19269},{\"attributes\":{\"n\":\"4.4.\"},\"end\":20742,\"start\":20735},{\"attributes\":{\"n\":\"5.\"},\"end\":23919,\"start\":23909},{\"end\":24825,\"start\":24815},{\"end\":25033,\"start\":25023},{\"end\":25216,\"start\":25206},{\"end\":26155,\"start\":26146}]", "table": "[{\"end\":26144,\"start\":25388}]", "figure_caption": "[{\"end\":25021,\"start\":24827},{\"end\":25204,\"start\":25035},{\"end\":25317,\"start\":25218},{\"end\":25388,\"start\":25320},{\"end\":26347,\"start\":26157}]", "figure_ref": "[{\"end\":2403,\"start\":2395},{\"end\":3700,\"start\":3692},{\"end\":3810,\"start\":3802},{\"end\":9201,\"start\":9193},{\"end\":11512,\"start\":11504},{\"end\":12218,\"start\":12210},{\"end\":12873,\"start\":12865},{\"end\":13042,\"start\":13034},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21356,\"start\":21348},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21657,\"start\":21640},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21794,\"start\":21776},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21832,\"start\":21813},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23408,\"start\":23399},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23905,\"start\":23897}]", "bib_author_first_name": "[{\"end\":26644,\"start\":26643},{\"end\":26653,\"start\":26652},{\"end\":26873,\"start\":26872},{\"end\":27072,\"start\":27071},{\"end\":27080,\"start\":27079},{\"end\":27102,\"start\":27097},{\"end\":27109,\"start\":27108},{\"end\":27378,\"start\":27377},{\"end\":27392,\"start\":27391},{\"end\":27401,\"start\":27400},{\"end\":27413,\"start\":27412},{\"end\":27916,\"start\":27915},{\"end\":28042,\"start\":28041},{\"end\":28052,\"start\":28051},{\"end\":28061,\"start\":28060},{\"end\":28070,\"start\":28069},{\"end\":28081,\"start\":28080},{\"end\":28094,\"start\":28093},{\"end\":28106,\"start\":28105},{\"end\":28116,\"start\":28115},{\"end\":28124,\"start\":28123},{\"end\":28640,\"start\":28636},{\"end\":28914,\"start\":28913},{\"end\":28923,\"start\":28922},{\"end\":28932,\"start\":28931},{\"end\":28943,\"start\":28942},{\"end\":28953,\"start\":28952},{\"end\":29339,\"start\":29338},{\"end\":29351,\"start\":29350},{\"end\":29353,\"start\":29352},{\"end\":29361,\"start\":29360},{\"end\":29363,\"start\":29362},{\"end\":29374,\"start\":29373},{\"end\":29384,\"start\":29383},{\"end\":29661,\"start\":29660},{\"end\":29672,\"start\":29671},{\"end\":29674,\"start\":29673},{\"end\":29687,\"start\":29686},{\"end\":29699,\"start\":29698},{\"end\":29714,\"start\":29713},{\"end\":29728,\"start\":29727},{\"end\":29738,\"start\":29737},{\"end\":30157,\"start\":30156},{\"end\":30165,\"start\":30164},{\"end\":30171,\"start\":30170},{\"end\":30177,\"start\":30176},{\"end\":30183,\"start\":30182},{\"end\":30353,\"start\":30352},{\"end\":30363,\"start\":30362},{\"end\":30371,\"start\":30370},{\"end\":30382,\"start\":30381},{\"end\":30616,\"start\":30615},{\"end\":30630,\"start\":30629},{\"end\":30640,\"start\":30639},{\"end\":30844,\"start\":30843},{\"end\":30851,\"start\":30850},{\"end\":30863,\"start\":30859},{\"end\":30872,\"start\":30871},{\"end\":30881,\"start\":30880},{\"end\":31201,\"start\":31200},{\"end\":31203,\"start\":31202},{\"end\":31216,\"start\":31215},{\"end\":31218,\"start\":31217},{\"end\":31504,\"start\":31503},{\"end\":31510,\"start\":31509},{\"end\":31519,\"start\":31518},{\"end\":31526,\"start\":31525},{\"end\":31824,\"start\":31823},{\"end\":31838,\"start\":31837},{\"end\":32042,\"start\":32041},{\"end\":32048,\"start\":32047},{\"end\":32054,\"start\":32053},{\"end\":32062,\"start\":32061},{\"end\":32389,\"start\":32388},{\"end\":32401,\"start\":32400},{\"end\":32413,\"start\":32412},{\"end\":32423,\"start\":32422},{\"end\":32432,\"start\":32431},{\"end\":32446,\"start\":32445},{\"end\":32722,\"start\":32721},{\"end\":32733,\"start\":32732},{\"end\":32740,\"start\":32739},{\"end\":33179,\"start\":33178},{\"end\":33181,\"start\":33180},{\"end\":33191,\"start\":33190},{\"end\":33414,\"start\":33413},{\"end\":33817,\"start\":33816},{\"end\":33828,\"start\":33827},{\"end\":33835,\"start\":33834},{\"end\":33845,\"start\":33844},{\"end\":34237,\"start\":34236},{\"end\":34244,\"start\":34243},{\"end\":34547,\"start\":34546},{\"end\":34772,\"start\":34771},{\"end\":34779,\"start\":34778},{\"end\":34788,\"start\":34787},{\"end\":34790,\"start\":34789},{\"end\":35018,\"start\":35017},{\"end\":35026,\"start\":35025},{\"end\":35039,\"start\":35038},{\"end\":35473,\"start\":35472},{\"end\":35479,\"start\":35478},{\"end\":35491,\"start\":35490},{\"end\":35499,\"start\":35498},{\"end\":35881,\"start\":35880},{\"end\":35890,\"start\":35889},{\"end\":35900,\"start\":35899},{\"end\":36123,\"start\":36122},{\"end\":36132,\"start\":36131},{\"end\":36142,\"start\":36141},{\"end\":36379,\"start\":36378},{\"end\":36388,\"start\":36387},{\"end\":36403,\"start\":36402},{\"end\":36412,\"start\":36411},{\"end\":36738,\"start\":36737},{\"end\":36740,\"start\":36739},{\"end\":36746,\"start\":36745},{\"end\":36760,\"start\":36759},{\"end\":36780,\"start\":36779},{\"end\":36791,\"start\":36790},{\"end\":36800,\"start\":36799},{\"end\":37188,\"start\":37187},{\"end\":37190,\"start\":37189},{\"end\":37202,\"start\":37201},{\"end\":37209,\"start\":37208},{\"end\":37219,\"start\":37218},{\"end\":37566,\"start\":37565},{\"end\":37579,\"start\":37578},{\"end\":37590,\"start\":37589},{\"end\":37599,\"start\":37598},{\"end\":37610,\"start\":37609},{\"end\":37623,\"start\":37622},{\"end\":37894,\"start\":37893},{\"end\":37904,\"start\":37903},{\"end\":37906,\"start\":37905},{\"end\":38382,\"start\":38381},{\"end\":38392,\"start\":38391},{\"end\":38394,\"start\":38393},{\"end\":38403,\"start\":38402},{\"end\":38831,\"start\":38830},{\"end\":38841,\"start\":38840},{\"end\":38853,\"start\":38852},{\"end\":38866,\"start\":38865},{\"end\":38874,\"start\":38873},{\"end\":39448,\"start\":39444},{\"end\":39459,\"start\":39458},{\"end\":39468,\"start\":39467},{\"end\":39936,\"start\":39935},{\"end\":39943,\"start\":39942},{\"end\":39954,\"start\":39953},{\"end\":39969,\"start\":39968},{\"end\":39980,\"start\":39979},{\"end\":39982,\"start\":39981},{\"end\":40474,\"start\":40473},{\"end\":40483,\"start\":40482},{\"end\":40493,\"start\":40492},{\"end\":40507,\"start\":40506},{\"end\":40757,\"start\":40756},{\"end\":40980,\"start\":40979},{\"end\":40987,\"start\":40986},{\"end\":40998,\"start\":40997},{\"end\":41006,\"start\":41005},{\"end\":41021,\"start\":41020},{\"end\":41043,\"start\":41042},{\"end\":41430,\"start\":41429},{\"end\":41438,\"start\":41437},{\"end\":41446,\"start\":41445},{\"end\":41630,\"start\":41629},{\"end\":41639,\"start\":41638},{\"end\":41899,\"start\":41898},{\"end\":41909,\"start\":41908},{\"end\":41918,\"start\":41917},{\"end\":42328,\"start\":42327},{\"end\":42336,\"start\":42335},{\"end\":42349,\"start\":42348},{\"end\":42362,\"start\":42361},{\"end\":42813,\"start\":42812},{\"end\":42821,\"start\":42820},{\"end\":42832,\"start\":42831},{\"end\":42842,\"start\":42841},{\"end\":42854,\"start\":42853},{\"end\":42867,\"start\":42866},{\"end\":43347,\"start\":43346},{\"end\":43355,\"start\":43354},{\"end\":43367,\"start\":43366},{\"end\":43380,\"start\":43379},{\"end\":43387,\"start\":43386},{\"end\":43814,\"start\":43813},{\"end\":43826,\"start\":43825},{\"end\":44142,\"start\":44141},{\"end\":44152,\"start\":44151},{\"end\":44526,\"start\":44525},{\"end\":44535,\"start\":44534},{\"end\":44549,\"start\":44548},{\"end\":45112,\"start\":45111},{\"end\":45121,\"start\":45120},{\"end\":45130,\"start\":45129},{\"end\":45144,\"start\":45143},{\"end\":45152,\"start\":45151},{\"end\":45164,\"start\":45163},{\"end\":45469,\"start\":45468},{\"end\":45477,\"start\":45476},{\"end\":45485,\"start\":45484},{\"end\":45493,\"start\":45492},{\"end\":45504,\"start\":45500},{\"end\":45840,\"start\":45839},{\"end\":45847,\"start\":45846},{\"end\":45857,\"start\":45853},{\"end\":45866,\"start\":45865},{\"end\":45868,\"start\":45867},{\"end\":46202,\"start\":46201},{\"end\":46215,\"start\":46214},{\"end\":46224,\"start\":46223},{\"end\":46234,\"start\":46233},{\"end\":46245,\"start\":46244},{\"end\":46605,\"start\":46604},{\"end\":46616,\"start\":46615},{\"end\":46626,\"start\":46625},{\"end\":47061,\"start\":47060},{\"end\":47067,\"start\":47066},{\"end\":47089,\"start\":47088},{\"end\":47098,\"start\":47097},{\"end\":47560,\"start\":47559},{\"end\":47572,\"start\":47571},{\"end\":47580,\"start\":47579},{\"end\":47591,\"start\":47587},{\"end\":47603,\"start\":47599},{\"end\":47614,\"start\":47610},{\"end\":47959,\"start\":47958},{\"end\":47966,\"start\":47965},{\"end\":47976,\"start\":47975},{\"end\":47983,\"start\":47982},{\"end\":47993,\"start\":47992},{\"end\":48000,\"start\":47999},{\"end\":48014,\"start\":48013},{\"end\":48298,\"start\":48297},{\"end\":48304,\"start\":48303},{\"end\":48312,\"start\":48311},{\"end\":48325,\"start\":48324},{\"end\":48642,\"start\":48641},{\"end\":48654,\"start\":48650},{\"end\":48851,\"start\":48850},{\"end\":48858,\"start\":48857},{\"end\":48866,\"start\":48865},{\"end\":48873,\"start\":48872},{\"end\":48881,\"start\":48880}]", "bib_author_last_name": "[{\"end\":26650,\"start\":26645},{\"end\":26661,\"start\":26654},{\"end\":26881,\"start\":26874},{\"end\":27077,\"start\":27073},{\"end\":27095,\"start\":27081},{\"end\":27106,\"start\":27103},{\"end\":27120,\"start\":27110},{\"end\":27389,\"start\":27379},{\"end\":27398,\"start\":27393},{\"end\":27410,\"start\":27402},{\"end\":27423,\"start\":27414},{\"end\":27924,\"start\":27917},{\"end\":28049,\"start\":28043},{\"end\":28058,\"start\":28053},{\"end\":28067,\"start\":28062},{\"end\":28078,\"start\":28071},{\"end\":28091,\"start\":28082},{\"end\":28103,\"start\":28095},{\"end\":28113,\"start\":28107},{\"end\":28121,\"start\":28117},{\"end\":28132,\"start\":28125},{\"end\":28649,\"start\":28641},{\"end\":28920,\"start\":28915},{\"end\":28929,\"start\":28924},{\"end\":28940,\"start\":28933},{\"end\":28950,\"start\":28944},{\"end\":28960,\"start\":28954},{\"end\":29348,\"start\":29340},{\"end\":29358,\"start\":29354},{\"end\":29371,\"start\":29364},{\"end\":29381,\"start\":29375},{\"end\":29394,\"start\":29385},{\"end\":29669,\"start\":29662},{\"end\":29684,\"start\":29675},{\"end\":29696,\"start\":29688},{\"end\":29711,\"start\":29700},{\"end\":29725,\"start\":29715},{\"end\":29735,\"start\":29729},{\"end\":29746,\"start\":29739},{\"end\":30162,\"start\":30158},{\"end\":30168,\"start\":30166},{\"end\":30174,\"start\":30172},{\"end\":30180,\"start\":30178},{\"end\":30188,\"start\":30184},{\"end\":30360,\"start\":30354},{\"end\":30368,\"start\":30364},{\"end\":30379,\"start\":30372},{\"end\":30390,\"start\":30383},{\"end\":30627,\"start\":30617},{\"end\":30637,\"start\":30631},{\"end\":30650,\"start\":30641},{\"end\":30848,\"start\":30845},{\"end\":30857,\"start\":30852},{\"end\":30869,\"start\":30864},{\"end\":30878,\"start\":30873},{\"end\":30889,\"start\":30882},{\"end\":31213,\"start\":31204},{\"end\":31224,\"start\":31219},{\"end\":31507,\"start\":31505},{\"end\":31516,\"start\":31511},{\"end\":31523,\"start\":31520},{\"end\":31530,\"start\":31527},{\"end\":31835,\"start\":31825},{\"end\":31850,\"start\":31839},{\"end\":32045,\"start\":32043},{\"end\":32051,\"start\":32049},{\"end\":32059,\"start\":32055},{\"end\":32065,\"start\":32063},{\"end\":32398,\"start\":32390},{\"end\":32410,\"start\":32402},{\"end\":32420,\"start\":32414},{\"end\":32429,\"start\":32424},{\"end\":32443,\"start\":32433},{\"end\":32454,\"start\":32447},{\"end\":32730,\"start\":32723},{\"end\":32737,\"start\":32734},{\"end\":32748,\"start\":32741},{\"end\":33188,\"start\":33182},{\"end\":33194,\"start\":33192},{\"end\":33423,\"start\":33415},{\"end\":33825,\"start\":33818},{\"end\":33832,\"start\":33829},{\"end\":33842,\"start\":33836},{\"end\":33852,\"start\":33846},{\"end\":34241,\"start\":34238},{\"end\":34248,\"start\":34245},{\"end\":34551,\"start\":34548},{\"end\":34776,\"start\":34773},{\"end\":34785,\"start\":34780},{\"end\":34798,\"start\":34791},{\"end\":35023,\"start\":35019},{\"end\":35036,\"start\":35027},{\"end\":35047,\"start\":35040},{\"end\":35476,\"start\":35474},{\"end\":35488,\"start\":35480},{\"end\":35496,\"start\":35492},{\"end\":35507,\"start\":35500},{\"end\":35887,\"start\":35882},{\"end\":35897,\"start\":35891},{\"end\":35907,\"start\":35901},{\"end\":36129,\"start\":36124},{\"end\":36139,\"start\":36133},{\"end\":36149,\"start\":36143},{\"end\":36385,\"start\":36380},{\"end\":36400,\"start\":36389},{\"end\":36409,\"start\":36404},{\"end\":36419,\"start\":36413},{\"end\":36743,\"start\":36741},{\"end\":36757,\"start\":36747},{\"end\":36777,\"start\":36761},{\"end\":36788,\"start\":36781},{\"end\":36797,\"start\":36792},{\"end\":36809,\"start\":36801},{\"end\":37199,\"start\":37191},{\"end\":37206,\"start\":37203},{\"end\":37216,\"start\":37210},{\"end\":37226,\"start\":37220},{\"end\":37576,\"start\":37567},{\"end\":37587,\"start\":37580},{\"end\":37596,\"start\":37591},{\"end\":37607,\"start\":37600},{\"end\":37620,\"start\":37611},{\"end\":37629,\"start\":37624},{\"end\":37901,\"start\":37895},{\"end\":37912,\"start\":37907},{\"end\":38389,\"start\":38383},{\"end\":38400,\"start\":38395},{\"end\":38413,\"start\":38404},{\"end\":38838,\"start\":38832},{\"end\":38850,\"start\":38842},{\"end\":38863,\"start\":38854},{\"end\":38871,\"start\":38867},{\"end\":38882,\"start\":38875},{\"end\":39456,\"start\":39449},{\"end\":39465,\"start\":39460},{\"end\":39476,\"start\":39469},{\"end\":39940,\"start\":39937},{\"end\":39951,\"start\":39944},{\"end\":39966,\"start\":39955},{\"end\":39977,\"start\":39970},{\"end\":39988,\"start\":39983},{\"end\":40480,\"start\":40475},{\"end\":40490,\"start\":40484},{\"end\":40504,\"start\":40494},{\"end\":40515,\"start\":40508},{\"end\":40761,\"start\":40758},{\"end\":40984,\"start\":40981},{\"end\":40995,\"start\":40988},{\"end\":41003,\"start\":40999},{\"end\":41018,\"start\":41007},{\"end\":41040,\"start\":41022},{\"end\":41050,\"start\":41044},{\"end\":41435,\"start\":41431},{\"end\":41443,\"start\":41439},{\"end\":41452,\"start\":41447},{\"end\":41636,\"start\":41631},{\"end\":41646,\"start\":41640},{\"end\":41906,\"start\":41900},{\"end\":41915,\"start\":41910},{\"end\":41932,\"start\":41919},{\"end\":42333,\"start\":42329},{\"end\":42346,\"start\":42337},{\"end\":42359,\"start\":42350},{\"end\":42371,\"start\":42363},{\"end\":42818,\"start\":42814},{\"end\":42829,\"start\":42822},{\"end\":42839,\"start\":42833},{\"end\":42851,\"start\":42843},{\"end\":42864,\"start\":42855},{\"end\":42877,\"start\":42868},{\"end\":43352,\"start\":43348},{\"end\":43364,\"start\":43356},{\"end\":43377,\"start\":43368},{\"end\":43384,\"start\":43381},{\"end\":43396,\"start\":43388},{\"end\":43823,\"start\":43815},{\"end\":43836,\"start\":43827},{\"end\":44149,\"start\":44143},{\"end\":44159,\"start\":44153},{\"end\":44532,\"start\":44527},{\"end\":44546,\"start\":44536},{\"end\":44558,\"start\":44550},{\"end\":45118,\"start\":45113},{\"end\":45127,\"start\":45122},{\"end\":45141,\"start\":45131},{\"end\":45149,\"start\":45145},{\"end\":45161,\"start\":45153},{\"end\":45174,\"start\":45165},{\"end\":45474,\"start\":45470},{\"end\":45482,\"start\":45478},{\"end\":45490,\"start\":45486},{\"end\":45498,\"start\":45494},{\"end\":45508,\"start\":45505},{\"end\":45844,\"start\":45841},{\"end\":45851,\"start\":45848},{\"end\":45863,\"start\":45858},{\"end\":45872,\"start\":45869},{\"end\":46212,\"start\":46203},{\"end\":46221,\"start\":46216},{\"end\":46231,\"start\":46225},{\"end\":46242,\"start\":46235},{\"end\":46253,\"start\":46246},{\"end\":46613,\"start\":46606},{\"end\":46623,\"start\":46617},{\"end\":46634,\"start\":46627},{\"end\":47064,\"start\":47062},{\"end\":47086,\"start\":47068},{\"end\":47095,\"start\":47090},{\"end\":47103,\"start\":47099},{\"end\":47569,\"start\":47561},{\"end\":47577,\"start\":47573},{\"end\":47585,\"start\":47581},{\"end\":47597,\"start\":47592},{\"end\":47608,\"start\":47604},{\"end\":47618,\"start\":47615},{\"end\":47963,\"start\":47960},{\"end\":47973,\"start\":47967},{\"end\":47980,\"start\":47977},{\"end\":47990,\"start\":47984},{\"end\":47997,\"start\":47994},{\"end\":48011,\"start\":48001},{\"end\":48024,\"start\":48015},{\"end\":48301,\"start\":48299},{\"end\":48309,\"start\":48305},{\"end\":48322,\"start\":48313},{\"end\":48333,\"start\":48326},{\"end\":48648,\"start\":48643},{\"end\":48660,\"start\":48655},{\"end\":48855,\"start\":48852},{\"end\":48863,\"start\":48859},{\"end\":48870,\"start\":48867},{\"end\":48878,\"start\":48874},{\"end\":48885,\"start\":48882}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1701.07275\",\"id\":\"b0\"},\"end\":26850,\"start\":26551},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":45998148},\"end\":26980,\"start\":26852},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4703661},\"end\":27297,\"start\":26982},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":58014148},\"end\":27911,\"start\":27299},{\"attributes\":{\"id\":\"b4\"},\"end\":27976,\"start\":27913},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":502946},\"end\":28558,\"start\":27978},{\"attributes\":{\"id\":\"b6\"},\"end\":28829,\"start\":28560},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":18015872},\"end\":29273,\"start\":28831},{\"attributes\":{\"doi\":\"abs/1802.10463\",\"id\":\"b8\"},\"end\":29575,\"start\":29275},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5736847},\"end\":30099,\"start\":29577},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3666937},\"end\":30308,\"start\":30101},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9455111},\"end\":30598,\"start\":30310},{\"attributes\":{\"id\":\"b12\"},\"end\":30789,\"start\":30600},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":52952193},\"end\":31129,\"start\":30791},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5989075},\"end\":31455,\"start\":31131},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":31797,\"start\":31457},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1915014},\"end\":31976,\"start\":31799},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1923924},\"end\":32317,\"start\":31978},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206592218},\"end\":32631,\"start\":32319},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4800342},\"end\":33132,\"start\":32633},{\"attributes\":{\"id\":\"b20\"},\"end\":33271,\"start\":33134},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8070108},\"end\":33759,\"start\":33273},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":24508893},\"end\":34115,\"start\":33761},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9128633},\"end\":34479,\"start\":34117},{\"attributes\":{\"id\":\"b24\"},\"end\":34722,\"start\":34481},{\"attributes\":{\"doi\":\"arXiv:1803.10704\",\"id\":\"b25\"},\"end\":34959,\"start\":34724},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1629541},\"end\":35393,\"start\":34961},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6813232},\"end\":35830,\"start\":35395},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17309230},\"end\":36101,\"start\":35832},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4465280},\"end\":36330,\"start\":36103},{\"attributes\":{\"id\":\"b30\"},\"end\":36672,\"start\":36332},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4245530},\"end\":37114,\"start\":36674},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":34479125},\"end\":37514,\"start\":37116},{\"attributes\":{\"doi\":\"arXiv:1502.02072\",\"id\":\"b33\"},\"end\":37834,\"start\":37516},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1379674},\"end\":38247,\"start\":37836},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14273023},\"end\":38751,\"start\":38249},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":58981583},\"end\":39378,\"start\":38753},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":215822736},\"end\":39830,\"start\":39380},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206594095},\"end\":40417,\"start\":39832},{\"attributes\":{\"doi\":\"arXiv:1705.08142\",\"id\":\"b39\"},\"end\":40693,\"start\":40419},{\"attributes\":{\"id\":\"b40\"},\"end\":40933,\"start\":40695},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":12613269},\"end\":41347,\"start\":40935},{\"attributes\":{\"id\":\"b42\"},\"end\":41575,\"start\":41349},{\"attributes\":{\"id\":\"b43\"},\"end\":41853,\"start\":41577},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":362506},\"end\":42241,\"start\":41855},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3935721},\"end\":42718,\"start\":42243},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":54462208},\"end\":43251,\"start\":42720},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3905277},\"end\":43743,\"start\":43253},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":11797475},\"end\":44063,\"start\":43745},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":25238775},\"end\":44449,\"start\":44065},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":57759288},\"end\":45032,\"start\":44451},{\"attributes\":{\"doi\":\"arXiv:1902.03589\",\"id\":\"b51\"},\"end\":45398,\"start\":45034},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":52954448},\"end\":45755,\"start\":45400},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":15080417},\"end\":46130,\"start\":45757},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5064446},\"end\":46523,\"start\":46132},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":7580580},\"end\":46953,\"start\":46525},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":12016916},\"end\":47471,\"start\":46955},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6352419},\"end\":47904,\"start\":47473},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":623318},\"end\":48271,\"start\":47906},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":30834643},\"end\":48562,\"start\":48273},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":18237764},\"end\":48788,\"start\":48564},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":206771431},\"end\":49129,\"start\":48790}]", "bib_title": "[{\"end\":26870,\"start\":26852},{\"end\":27069,\"start\":26982},{\"end\":27375,\"start\":27299},{\"end\":28039,\"start\":27978},{\"end\":28911,\"start\":28831},{\"end\":29658,\"start\":29577},{\"end\":30154,\"start\":30101},{\"end\":30350,\"start\":30310},{\"end\":30841,\"start\":30791},{\"end\":31198,\"start\":31131},{\"end\":31501,\"start\":31457},{\"end\":31821,\"start\":31799},{\"end\":32039,\"start\":31978},{\"end\":32386,\"start\":32319},{\"end\":32719,\"start\":32633},{\"end\":33411,\"start\":33273},{\"end\":33814,\"start\":33761},{\"end\":34234,\"start\":34117},{\"end\":35015,\"start\":34961},{\"end\":35470,\"start\":35395},{\"end\":35878,\"start\":35832},{\"end\":36120,\"start\":36103},{\"end\":36376,\"start\":36332},{\"end\":36735,\"start\":36674},{\"end\":37185,\"start\":37116},{\"end\":37891,\"start\":37836},{\"end\":38379,\"start\":38249},{\"end\":38828,\"start\":38753},{\"end\":39442,\"start\":39380},{\"end\":39933,\"start\":39832},{\"end\":40977,\"start\":40935},{\"end\":41627,\"start\":41577},{\"end\":41896,\"start\":41855},{\"end\":42325,\"start\":42243},{\"end\":42810,\"start\":42720},{\"end\":43344,\"start\":43253},{\"end\":43811,\"start\":43745},{\"end\":44139,\"start\":44065},{\"end\":44523,\"start\":44451},{\"end\":45466,\"start\":45400},{\"end\":45837,\"start\":45757},{\"end\":46199,\"start\":46132},{\"end\":46602,\"start\":46525},{\"end\":47058,\"start\":46955},{\"end\":47557,\"start\":47473},{\"end\":47956,\"start\":47906},{\"end\":48295,\"start\":48273},{\"end\":48639,\"start\":48564},{\"end\":48848,\"start\":48790}]", "bib_author": "[{\"end\":26652,\"start\":26643},{\"end\":26663,\"start\":26652},{\"end\":26883,\"start\":26872},{\"end\":27079,\"start\":27071},{\"end\":27097,\"start\":27079},{\"end\":27108,\"start\":27097},{\"end\":27122,\"start\":27108},{\"end\":27391,\"start\":27377},{\"end\":27400,\"start\":27391},{\"end\":27412,\"start\":27400},{\"end\":27425,\"start\":27412},{\"end\":27926,\"start\":27915},{\"end\":28051,\"start\":28041},{\"end\":28060,\"start\":28051},{\"end\":28069,\"start\":28060},{\"end\":28080,\"start\":28069},{\"end\":28093,\"start\":28080},{\"end\":28105,\"start\":28093},{\"end\":28115,\"start\":28105},{\"end\":28123,\"start\":28115},{\"end\":28134,\"start\":28123},{\"end\":28651,\"start\":28636},{\"end\":28922,\"start\":28913},{\"end\":28931,\"start\":28922},{\"end\":28942,\"start\":28931},{\"end\":28952,\"start\":28942},{\"end\":28962,\"start\":28952},{\"end\":29350,\"start\":29338},{\"end\":29360,\"start\":29350},{\"end\":29373,\"start\":29360},{\"end\":29383,\"start\":29373},{\"end\":29396,\"start\":29383},{\"end\":29671,\"start\":29660},{\"end\":29686,\"start\":29671},{\"end\":29698,\"start\":29686},{\"end\":29713,\"start\":29698},{\"end\":29727,\"start\":29713},{\"end\":29737,\"start\":29727},{\"end\":29748,\"start\":29737},{\"end\":30164,\"start\":30156},{\"end\":30170,\"start\":30164},{\"end\":30176,\"start\":30170},{\"end\":30182,\"start\":30176},{\"end\":30190,\"start\":30182},{\"end\":30362,\"start\":30352},{\"end\":30370,\"start\":30362},{\"end\":30381,\"start\":30370},{\"end\":30392,\"start\":30381},{\"end\":30629,\"start\":30615},{\"end\":30639,\"start\":30629},{\"end\":30652,\"start\":30639},{\"end\":30850,\"start\":30843},{\"end\":30859,\"start\":30850},{\"end\":30871,\"start\":30859},{\"end\":30880,\"start\":30871},{\"end\":30891,\"start\":30880},{\"end\":31215,\"start\":31200},{\"end\":31226,\"start\":31215},{\"end\":31509,\"start\":31503},{\"end\":31518,\"start\":31509},{\"end\":31525,\"start\":31518},{\"end\":31532,\"start\":31525},{\"end\":31837,\"start\":31823},{\"end\":31852,\"start\":31837},{\"end\":32047,\"start\":32041},{\"end\":32053,\"start\":32047},{\"end\":32061,\"start\":32053},{\"end\":32067,\"start\":32061},{\"end\":32400,\"start\":32388},{\"end\":32412,\"start\":32400},{\"end\":32422,\"start\":32412},{\"end\":32431,\"start\":32422},{\"end\":32445,\"start\":32431},{\"end\":32456,\"start\":32445},{\"end\":32732,\"start\":32721},{\"end\":32739,\"start\":32732},{\"end\":32750,\"start\":32739},{\"end\":33190,\"start\":33178},{\"end\":33196,\"start\":33190},{\"end\":33425,\"start\":33413},{\"end\":33827,\"start\":33816},{\"end\":33834,\"start\":33827},{\"end\":33844,\"start\":33834},{\"end\":33854,\"start\":33844},{\"end\":34243,\"start\":34236},{\"end\":34250,\"start\":34243},{\"end\":34553,\"start\":34546},{\"end\":34778,\"start\":34771},{\"end\":34787,\"start\":34778},{\"end\":34800,\"start\":34787},{\"end\":35025,\"start\":35017},{\"end\":35038,\"start\":35025},{\"end\":35049,\"start\":35038},{\"end\":35478,\"start\":35472},{\"end\":35490,\"start\":35478},{\"end\":35498,\"start\":35490},{\"end\":35509,\"start\":35498},{\"end\":35889,\"start\":35880},{\"end\":35899,\"start\":35889},{\"end\":35909,\"start\":35899},{\"end\":36131,\"start\":36122},{\"end\":36141,\"start\":36131},{\"end\":36151,\"start\":36141},{\"end\":36387,\"start\":36378},{\"end\":36402,\"start\":36387},{\"end\":36411,\"start\":36402},{\"end\":36421,\"start\":36411},{\"end\":36745,\"start\":36737},{\"end\":36759,\"start\":36745},{\"end\":36779,\"start\":36759},{\"end\":36790,\"start\":36779},{\"end\":36799,\"start\":36790},{\"end\":36811,\"start\":36799},{\"end\":37201,\"start\":37187},{\"end\":37208,\"start\":37201},{\"end\":37218,\"start\":37208},{\"end\":37228,\"start\":37218},{\"end\":37578,\"start\":37565},{\"end\":37589,\"start\":37578},{\"end\":37598,\"start\":37589},{\"end\":37609,\"start\":37598},{\"end\":37622,\"start\":37609},{\"end\":37631,\"start\":37622},{\"end\":37903,\"start\":37893},{\"end\":37914,\"start\":37903},{\"end\":38391,\"start\":38381},{\"end\":38402,\"start\":38391},{\"end\":38415,\"start\":38402},{\"end\":38840,\"start\":38830},{\"end\":38852,\"start\":38840},{\"end\":38865,\"start\":38852},{\"end\":38873,\"start\":38865},{\"end\":38884,\"start\":38873},{\"end\":39458,\"start\":39444},{\"end\":39467,\"start\":39458},{\"end\":39478,\"start\":39467},{\"end\":39942,\"start\":39935},{\"end\":39953,\"start\":39942},{\"end\":39968,\"start\":39953},{\"end\":39979,\"start\":39968},{\"end\":39990,\"start\":39979},{\"end\":40482,\"start\":40473},{\"end\":40492,\"start\":40482},{\"end\":40506,\"start\":40492},{\"end\":40517,\"start\":40506},{\"end\":40763,\"start\":40756},{\"end\":40986,\"start\":40979},{\"end\":40997,\"start\":40986},{\"end\":41005,\"start\":40997},{\"end\":41020,\"start\":41005},{\"end\":41042,\"start\":41020},{\"end\":41052,\"start\":41042},{\"end\":41437,\"start\":41429},{\"end\":41445,\"start\":41437},{\"end\":41454,\"start\":41445},{\"end\":41638,\"start\":41629},{\"end\":41648,\"start\":41638},{\"end\":41908,\"start\":41898},{\"end\":41917,\"start\":41908},{\"end\":41934,\"start\":41917},{\"end\":42335,\"start\":42327},{\"end\":42348,\"start\":42335},{\"end\":42361,\"start\":42348},{\"end\":42373,\"start\":42361},{\"end\":42820,\"start\":42812},{\"end\":42831,\"start\":42820},{\"end\":42841,\"start\":42831},{\"end\":42853,\"start\":42841},{\"end\":42866,\"start\":42853},{\"end\":42879,\"start\":42866},{\"end\":43354,\"start\":43346},{\"end\":43366,\"start\":43354},{\"end\":43379,\"start\":43366},{\"end\":43386,\"start\":43379},{\"end\":43398,\"start\":43386},{\"end\":43825,\"start\":43813},{\"end\":43838,\"start\":43825},{\"end\":44151,\"start\":44141},{\"end\":44161,\"start\":44151},{\"end\":44534,\"start\":44525},{\"end\":44548,\"start\":44534},{\"end\":44560,\"start\":44548},{\"end\":45120,\"start\":45111},{\"end\":45129,\"start\":45120},{\"end\":45143,\"start\":45129},{\"end\":45151,\"start\":45143},{\"end\":45163,\"start\":45151},{\"end\":45176,\"start\":45163},{\"end\":45476,\"start\":45468},{\"end\":45484,\"start\":45476},{\"end\":45492,\"start\":45484},{\"end\":45500,\"start\":45492},{\"end\":45510,\"start\":45500},{\"end\":45846,\"start\":45839},{\"end\":45853,\"start\":45846},{\"end\":45865,\"start\":45853},{\"end\":45874,\"start\":45865},{\"end\":46214,\"start\":46201},{\"end\":46223,\"start\":46214},{\"end\":46233,\"start\":46223},{\"end\":46244,\"start\":46233},{\"end\":46255,\"start\":46244},{\"end\":46615,\"start\":46604},{\"end\":46625,\"start\":46615},{\"end\":46636,\"start\":46625},{\"end\":47066,\"start\":47060},{\"end\":47088,\"start\":47066},{\"end\":47097,\"start\":47088},{\"end\":47105,\"start\":47097},{\"end\":47571,\"start\":47559},{\"end\":47579,\"start\":47571},{\"end\":47587,\"start\":47579},{\"end\":47599,\"start\":47587},{\"end\":47610,\"start\":47599},{\"end\":47620,\"start\":47610},{\"end\":47965,\"start\":47958},{\"end\":47975,\"start\":47965},{\"end\":47982,\"start\":47975},{\"end\":47992,\"start\":47982},{\"end\":47999,\"start\":47992},{\"end\":48013,\"start\":47999},{\"end\":48026,\"start\":48013},{\"end\":48303,\"start\":48297},{\"end\":48311,\"start\":48303},{\"end\":48324,\"start\":48311},{\"end\":48335,\"start\":48324},{\"end\":48650,\"start\":48641},{\"end\":48662,\"start\":48650},{\"end\":48857,\"start\":48850},{\"end\":48865,\"start\":48857},{\"end\":48872,\"start\":48865},{\"end\":48880,\"start\":48872},{\"end\":48887,\"start\":48880}]", "bib_venue": "[{\"end\":26641,\"start\":26551},{\"end\":26899,\"start\":26883},{\"end\":27129,\"start\":27122},{\"end\":27498,\"start\":27425},{\"end\":28212,\"start\":28134},{\"end\":28634,\"start\":28560},{\"end\":29030,\"start\":28962},{\"end\":29336,\"start\":29275},{\"end\":29810,\"start\":29748},{\"end\":30193,\"start\":30190},{\"end\":30434,\"start\":30392},{\"end\":30613,\"start\":30600},{\"end\":30929,\"start\":30891},{\"end\":31275,\"start\":31226},{\"end\":31602,\"start\":31532},{\"end\":31865,\"start\":31852},{\"end\":32129,\"start\":32067},{\"end\":32460,\"start\":32456},{\"end\":32834,\"start\":32750},{\"end\":33176,\"start\":33134},{\"end\":33495,\"start\":33425},{\"end\":33913,\"start\":33854},{\"end\":34280,\"start\":34250},{\"end\":34544,\"start\":34481},{\"end\":34769,\"start\":34724},{\"end\":35126,\"start\":35049},{\"end\":35588,\"start\":35509},{\"end\":35956,\"start\":35909},{\"end\":36201,\"start\":36151},{\"end\":36486,\"start\":36421},{\"end\":36876,\"start\":36811},{\"end\":37289,\"start\":37228},{\"end\":37563,\"start\":37516},{\"end\":37991,\"start\":37914},{\"end\":38477,\"start\":38415},{\"end\":38957,\"start\":38884},{\"end\":39555,\"start\":39478},{\"end\":40067,\"start\":39990},{\"end\":40471,\"start\":40419},{\"end\":40754,\"start\":40695},{\"end\":41121,\"start\":41052},{\"end\":41427,\"start\":41349},{\"end\":41697,\"start\":41648},{\"end\":42001,\"start\":41934},{\"end\":42457,\"start\":42373},{\"end\":42958,\"start\":42879},{\"end\":43477,\"start\":43398},{\"end\":43887,\"start\":43838},{\"end\":44236,\"start\":44161},{\"end\":44633,\"start\":44560},{\"end\":45109,\"start\":45034},{\"end\":45559,\"start\":45510},{\"end\":45929,\"start\":45874},{\"end\":46300,\"start\":46255},{\"end\":46715,\"start\":46636},{\"end\":47186,\"start\":47105},{\"end\":47669,\"start\":47620},{\"end\":48075,\"start\":48026},{\"end\":48404,\"start\":48335},{\"end\":48665,\"start\":48662},{\"end\":48946,\"start\":48887},{\"end\":27558,\"start\":27500},{\"end\":28286,\"start\":28214},{\"end\":32905,\"start\":32836},{\"end\":35190,\"start\":35128},{\"end\":38055,\"start\":37993},{\"end\":39017,\"start\":38959},{\"end\":39619,\"start\":39557},{\"end\":40131,\"start\":40069},{\"end\":42055,\"start\":42003},{\"end\":44693,\"start\":44635}]"}}}, "year": 2023, "month": 12, "day": 17}