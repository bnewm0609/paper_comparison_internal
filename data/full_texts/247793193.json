{"id": 247793193, "updated": "2023-11-08 03:41:48.165", "metadata": {"title": "Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection", "authors": "[{\"first\":\"Jinyuan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Zhanbo\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Guanyao\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Risheng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Zhong\",\"middle\":[]},{\"first\":\"Zhongxuan\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This study addresses the issue of fusing infrared and visible images that appear differently for object detection. Aiming at generating an image of high visual quality, previous approaches discover commons underlying the two modalities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary information are extremely important for both fusion and subsequent detection task. This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection, and then unrolls to a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one generator and dual discriminators seeks commons while learning from differences, which preserves structural information of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imaging system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark covering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also higher detection mAP than the state-of-the-art approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.16220", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiuFHWLZL22", "doi": "10.1109/cvpr52688.2022.00571"}}, "content": {"source": {"pdf_hash": "b728dbb572b6437e95c1e5062df9c57ccb6f74b7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.16220v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "da71097486ba0fafb7f2d8f58843c52616bf76c8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b728dbb572b6437e95c1e5062df9c57ccb6f74b7.txt", "contents": "\nTarget-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection\n\n\nJinyuan Liu \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nXin Fan \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nZhanbo Huang \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nGuanyao Wu \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nRisheng Liu rsliu@dlut.edu.cn \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nWei Zhong \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nZhongxuan Luo \nSchool of Software Technology\nDUT-RU International School of Information Science & Engineering\nM FD Pixel Distribution\nPeng Cheng Laboratory\nDalian University of Technology\nDalian University of Technology\n\n\nTarget-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection\n\n\n\nAbstract\n\nThis study addresses the issue of fusing infrared and visible images that appear differently for object detection. Aiming at generating an image of high visual quality, previous approaches discover commons underlying the two modalities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary information are extremely important for both fusion and subsequent detection task. This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection, and then unrolls to a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one generator and dual discriminators seeks commons while learning from differences, which preserves structural information of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imaging system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark covering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also higher detection mAP than the state-of-the-art approaches. The source code and benchmark are available at https://github.com/dlut-dimt/TarDAL.\n\n\nIntroduction\n\nMulti-modality imaging has attracted significant attention in a wide range of applications, e.g., surveillance [28] and autonomous driving [5], with the rapid development of sensing hardware. Especially, the combination of infrared and visible sensors has remarkable advantages for subsequent intelligent processing [11,38,39]. Visible imaging provides rich details with high spatial resolution under welldefined lighting conditions while infrared sensors, capturing ambient temperature variations emitted from objects, highlight structures of thermal targets insensitive to lighting changes. Unfortunately, infrared images are often accompanied by blurred details with lower spatial resolution. Owing to their evident appearance discrepancy, it is challenging to fuse visually appealing images and/or to support higher-level vision tasks such as segmentation [4,29], tracking [2,7], and detection [32], by making full use of the complementary information from the infrared and visible.\n\nNumerous infrared and visible image fusion (IVIF) approaches that aim at improving visual quality have been developed in the past decades. Traditional multi-scale transform [10,24], optimization model [16,20,41], spare representation [37,43], and subspace methods attempt to discover intrinsic common features of the two modalities and to design appropriate weighting rules for fusion. These approaches typically have to invoke a time consuming iterative optimization process. Recently, researchers introduce deep networks into IVIF by learning powerful feature representation and/or weighting strategies when redundant well-prepared image pairs are available for training [8,12,[21][22][23]35]. The fusion turns out to be an efficient inference process yielding fruitful quality improvements.\n\nNevertheless, either traditional or deep IVIF approaches strive for quality improving but leave alone the follow-up detection, which is the key to many practical computer vision applications. The fusion emphasizes more on 'seeking commons' but neglects the differences of these two modalities on presenting structural information of targets and textural details of ambient background. These differences play a critical role on differentiating distinct features of targets for object detection meanwhile generating clear appearance of high contrast favorable for human inspection.\n\nMoreover, learning from these differences (actually complementary information) demands a comprehensive collections of imaging data from the two modalities. The images capturing in scenarios varying with lighting and weather exhibit significantly different characteristics from both modalities. Unfortunately, existing data collections only cover limited conditions, placing an obstacle to learn the complementary information and validate the effectiveness.\n\nThis paper proposes a bilevel optimization formulation for the joint problem of fusion and detection. This formulation unrolls to a well-designed dual-adversarial fusion network, composed of one generator and two target-aware discriminators, and a commonly used detection network. One discriminator distinguishes foreground thermal targets from the image domain of infrared imaging while the other one differentiates the background textural details from gradient domain of the visible image. We also derive a cooperative training strategy to learn optimal parameters for the two networks. Figure 1 demonstrates that our method accurately detects objects from target-distinct and visualappealing fusion with less time and fewer parameters than the state-of-the-art (SOTA). Our contributions are four-fold:\n\n\u2022 We embrace both image fusion and object detection with a bilevel optimization formulation, producing high detection accuracy as well as the fused image with better visual effects. \u2022 We devise a target-aware dual adversarial learning network (TarDAL) with fewer parameters for detection-oriented fusion. This one-generator and dual-discriminator network 'seeks commons while learning from differences' that preserves information of targets from the infrared and textural details from the visible. \u2022 We derive a cooperative training scheme from the bilevel formulation yielding optimal network parameters for fast inference (fusion and detection).\n\n\u2022 We build a synchronized imaging system with wellcalibrated infrared and optical sensors and collect a multi-scenario multi-modality dataset (M 3 FD) with 4, 177 aligned infrared and visible image pairs and 23, 635 annotated objects. The dataset covers four major scenarios with various environments, illumination, season, and weather, having a wide range of pixel variations, as shown in Figure 1.\n\n\nRelated Works\n\nThe fusion module is critical to detect objects from multi-modality sensors. This section briefly reviews previous learning-based IVIF approaches closely related to ours and available benchmarks that are necessary for learningand empirical evaluation.\n\n\nLearning-based approaches\n\nDeep learning has achieved promising progress in lowlevel vision tasks [12,15,17,19,23,25,35,40] due to the powerful nonlinear fitting ability of multi-layer neural networks. Early works plugged deep networks into the IVIF process acting as a module of feature extraction or weights generation [8,12,13,16]. Liu et al. [16] cascaded two pretrained CNNs, one for feature and the other for weights learning. Researchers also resort to an end-to-end architecture so that one-step network inference can generate a plausible fused image by one set of network parameters. Li et al. [9] introduced a residual fusion network to learn enhanced features in a common space, yielding structure consistent results favorable to human inspection.\n\nMore recently, the IVIF approaches based on generative adversarial networks (GAN) [26,36,42] produce appealing results by transporting different distributions to the desired one [21][22][23]. For the first time, Ma et al. introduced an adversarial game between the fused and visible in order to enhance texture details [22]. However, this signal adversarial mechanism may lose the vital information from the infrared. Ma et al. apply an identical adversarial strategy to both the visible and infrared, which partially compensates the infrared information [21]. Unfortunately, all these approaches fail capturing the different characteristics of these two imaging types. It is worth investigating these differences complementary to each other from which both fusion and detection can benefit.\n\n\nBenchmarks\n\nIn recent years, we have witnessed the rapid evolution of IVIF benchmarks, including the TNO Image Fusion [33], INO Videos Analytics 1 , OSU Color-Thermal 2 , and RoadScene [35] and Multispectral datasets [32]. The   Table 1 lists the profiles of these datasets such as scale, resolution, lighting, and scenario categories. The low image resolution, limited number of object and scenario types, and few labels stumble wide applications of existing datasets to the higher-level detection task on multiple modalities.\n\n\nThe Proposed Method\n\nThis section details our method, staring from the bilevel optimization formulation of fusion and detection. Then, we elaborate the target-aware dual adversarial learning network for fusion. Finally, we give a cooperative training scheme to learn optimal parameters for both fusion and detection.\n\n\nProblem formulation\n\nUnlike previous approaches catering for high visual quality, we state that IVIF has to generate an image that benefits both visual inspection and computer perception, namely detection-oriented fusion. Suppose that the infrared, visible and fused are all gray-scale with the size of m \u00d7 n, denoted as column vectors x, y, and u \u2208 R mn\u00d71 , respectively. Following the truism Stackelberg's theory [14,18,27], we formulate the detection-oriented fusion as a bilevel optimization model:\nmin \u03c9d L d \u03a8(u * ; \u03c9 d ) ,(1)s.t. u * \u2208 arg min u f (u; x, y) + g T (u; x) + g D (u; y),(2)\nwhere L d is the detection-specific training loss and \u03a8 denotes a detection network with learnable parameters \u03c9 d . f (\u00b7) is an energy-based fidelity term containing the fused image u, and source images x and y while g T (\u00b7) and g D (\u00b7) are two feasibility constraints defined on the infrared and visible, respectively. Figure 2(a) illustrates that this bilevel formulation makes it possible to find the solution mutually favoring fusion and detection. Nevertheless, it is nontrivial to solve Eq. (2) by a traditional optimizing technique as the fusion task is not a simple equality/inequality constraint. Instead, we introduce a fusion network \u03a6 with learned parameters \u03c9 f and convert the bilevel optimization to single-level:\nmin \u03c9d,\u03c9f L d \u03a8(u * ; \u03c9 d ) , s.t. u * = \u03a6(x, y; \u03c9 f ). (3)\nHence, we unroll the optimization into two learning networks \u03a6 and \u03a8. We adopt YOLOv5 3 as our backbone for the detection network \u03a8, where L d also follows its setting, and carefully design the fusion network \u03a6 as below.\n\n\nTarget-aware dual adversarial network\n\nTypical deep fusion methods strive for learning common features underlying the two modalities that appear differently. Instead, our fusion network seeks commons while learning from differences that imply complementary characteristics of these two types of imaging. Typically, the infrared highlights distinct structures of targets while the visible provides textural details of background.\n\nWe introduce an adversarial game that consists one generator and two discriminators in order to combine common with distinct features from the two modalities, as shown in Figure 2(b). The generator G is encouraged to provide a realistic fused image to simultaneously fool both discriminators. The target discriminator D T evaluates the intensity consistence between the targets from the infrared and those masked out from the fused given by G (the top row of Figure 2(b)); the detail discriminator D D discriminates the gradient distribution of the visible from that of the fused (the bottom row of Figure 2(b)). These two discriminators work in different domains as targets exhibit consistent intensity distribution while gradients characterize textures.\n\nGenerator: The generator contributes to generate a fused image that preserves overall structures and maintains a similar intensity distribution as source images. The commonly used structural similarity index (SSIM) [34] acts as the loss function:\nL SSIM = (1 \u2212 SSIM u,x )/2 + (1 \u2212 SSIM u,y )/2,(4)\nwhere L SSIM denotes structure similarity loss. To balance the pixel intensity distribution of source images, we introduce a pixel loss based on the saliency degree weight (SDW). Supposing that the saliency value of x at the kth pixel can be obtained by S\nx(k) = 255 i=0 H x (i)|x(k) \u2212 i|,\nwhere x(k) is the value of the kth pixel and H x is the histogram of pixel value i, we define the pixel loss L pixel as:\nL pixel = u \u2212 \u03c9 1 x 1 + u \u2212 \u03c9 2 y 1 ,(5)\nwhere\n\u03c9 1 = S x (k)/[S x (k) \u2212 S y (k)], \u03c9 2 = 1 \u2212 \u03c9 1 .\n3 https://github.com/ultralytics/YOLOv5\n\nWe employ a 5-layer dense block [6] as G to extract common features, and then use a merge block with three convolutional layers for feature aggregation. Each convolutional layer consists of one convolutional operation, batch normalization and ReLU activation function. The generated fused images u has the same size with the sources.\n\nTarget and detail discriminators: The target discriminator D T is used to distinguish the foreground thermal targets of fused result to the infrared while the detail discriminator D D contributes to distinguish the background details of fused result to the visible. We employ a pretrained saliency detection network [3] to calculate the target mask m from infrared images so that the two discriminators can perform on their respective regions (target and background). Thus, we define the adversarial loss L adv f as:\nL f D T = E x\u223cp(R(x)) [D(x)] \u2212 Ex \u223cp(R(u)) [D(x)],(6)L f D D = E x\u223cp(R(\u2207y)) [D(x)] \u2212 Ex \u223cp(R(\u2207u)) [D(x)], (7) L adv f = L f D T + L f D D ,(8)\nwhere R = x m andR = 1 \u2212 R, differentiating targets from background, and denotes the point-wise multiplication. \u2207(\u00b7) denotes a gradient operation, e.g., Sobel.\n\nThe adversarial loss functions of these discriminators calculate the Wasserstein divergence to mutually identify whether the foreground thermal targets and background texture details are realistic, defined as:\nL D T = L f D T + kEx \u223cr(R(x)) [( \u2207D T (x) ) p ],(9)L D D = L f D D + kEx \u223cr(R(\u2207x)) [( \u2207D D (x) ) p ],(10)\nwherer(x) denotes sample space that is similar top(x). Commonly, k and p sets to 2 and 6, respectively. The two discriminators D T and D D share the same network structure, having four convolutional layers and one fully connection layer. Figure 3 demonstrates the detailed architectures of the generator and dual discriminators.\n\nTotally, L f is combination of the aforementioned three main parts:\nL f = L SSIM + \u03b1L pixel + \u03b2L adv f ,(11)\nwhere \u03b1 and \u03b2 are the trade-off parameters.  \n\n\nConv\n\n\nCooperative training strategy\n\nThe bilevel optimization naturally derive a cooperative training strategy to obtain optimal network parameters \u03c9 = (\u03c9 d , \u03c9 f ). We introduce a a fusion regularizer L f and convert Eq. (3) optimizing detection subject to the fusion constraint to to a mutual optimization:  \nL d \u03a8(u * ; \u03c9 d ) + \u03bbL f \u03a6(x, y; \u03c9 f ) ,(12)s.t. u * = \u03a6(x, y; \u03c9 f ),(13)\nwhere \u03bb is the trade-off parameter. Rather than designing a weighting rule, this regularizer can well balance fusion and detection. Figure 2(c) illustrates the flow of gradient propagation to cooperatively train the fusion and detection networks. The loss gradients with respect to \u03c9 d and \u03c9 f are calculated as:\n\u2202L d \u2202\u03c9d = \u2202L d \u2202\u03a8d \u2202\u03a8d \u2202\u03c9d , \u2202L d \u2202\u03c9f = \u2202L d \u2202\u03a8d \u2202\u03a8d \u2202\u03a8f \u2202\u03a8f \u2202\u03c9f + \u03bb \u2202L f \u2202\u03a8f \u2202\u03a8f \u2202\u03c9f .(14)\nThese equations reveal that the gradients of the detection loss w.r.t. the detection parameters along with those w.r.t. the fusion parameters are all back propagated and the latter also consists of the gradients of the fusion loss w.r.t. the fusion parameters. Finally, this strategy cannot only generate a visually appealing image but also output accurate detection given the trained network parameters, enabling us to find the optimal solution to detection-oriented fusion and to converge more efficient than independent training schemes.\n\n\nMulti-scenario Multi-modality Benchmark\n\nExisting datasets with infrared and visible images can hardly be applied to learn and/or evaluate detection from multi-modality data. Our benchmark M 3 FD contains infrared and visible images of high resolution covering diverse object types under various scenarios as given in the last row of Table 1. We constructed a synchronized system containing one binocular optical camera and one binocular infrared sensor (shown in Figure 5) in order to capture corresponding twomodality images of natural scenes. The baselines (distance between the focal centers of binocular lens) of the visible and infrared binocular cameras are 12cm and 20cm, respectively. The optical center distance between the visible and infrared senors is 4cm. Visible images have a high resolution of 1024\u00d7768 and a wide imaging range while infrared images have a standard resolution of 640\u00d7512 and the wavelength range is 8 \u2212 14\u00b5m.\n\nWe first calibrated all cameras to estimate their internal and external parameters, and then calculated a homography matrix that projects coordinates of infrared images to those of the visible. Eventually, we obtained well-aligned infrared/visible image pairs with the size of 1024 \u00d7 768 by warping all images to a common coordinate 4 .\n\nWe categorized all 4, 200 aligned pairs in M 3 FD into four typical types, i.e. Daytime, Overcast, Night, and Challenge, with ten sub-scenarios shown in Figure 4. Meanwhile, we annotated 33, 603 objects of six classes, i.e., People, Car, Bus, Motorcycle, Truck and Lamp, which commonly occur in surveillance and autonomous driving. The quantity and diversity of M 3 FD pave the possibility to learn and evaluate object detection by fusing images.\n\n\nExperiments\n\nWe performed experimental evaluations on four datasets (three for IVIF, i.e., TNO, Roadscene, and M 3 FD, and two for object detection, i.e., MS and M 3 FD). 180/3,500 multi-modality images are selected and cropped to 24k/151k patches with 320\u00d7320 pixels by random cropping and augmented for training the fusion and detection task, respectively. The tuning parameters \u03b1 and \u03b2 are set to 20 and 0.1, respectively. The Adam optimizer updates the network parameters with the learning rate of 1.0 \u00d7 10 \u22123 and exponential decay. The epoch is set to 300 with batch size of 64. Our approach is implemented on PyTorch with an NVIDIA Tesla V100 GPU.\n\n\nResults of infrared-visible image fusion\n\nWe evaluate the fusion performance of TarDAL by making a comparison with 7 state-of-the-art methods, including DenseFuse [8], FusionGAN [22], RFN [9], GANMcC [23], DDcGAN [21], MFEIF [12], and U2Fusion [35]. \n\n\nQualitative Comparisons\n\nThe intuitive qualitative results on three typical image pairs from three datasets are shown in Figure 6. Compared with other existing methods, our TarDAL has two significant advantages. First, the discriminative target from infrared images can be well preserved. As shown in Figure 6 (the green tangles of the second group), the people in our method exhibits high contrast and distinctive prominent contour, so that it is benefit to visual observation . Second, our results can preserve abundant textural details from visible images (the green tangles of the first and third group), which are more in line with human visual system. In contrast, visual inspection shows that DenseFuse, and FusionGAN cannot highlight the discriminative targets well, while GANMcC and DDcGAN fail to obtain rich textural details. Note that our TarDAL is able to generate more visual-friendly fused results with clear targets, sharper edge contours, and preserve abundant textural details.\n\nQuantitative Comparisons Subsequently, we compare our TarDAL with the above-mentioned competitors quantitatively on 400 image pairs ( 20 image pairs from TNO, 40 image pairs from the RoadScene, and 340 image pairs from M 3 FD). Besides, three evaluation metrics, i.e., mutual information (MI) [30], entropy (EN) [31] and standard deviation (SD) [1] are introduced for evaluation. The quantitative results are reported in Figure 7. As can be seen from the statistical results, our method continuously gen-erates the largest or the second-largest mean value on three datasets among all evaluation metrics. Meanwhile, achieving a lower variance indicates that our method is more stable in dealing with various visual scenes. In specific, the largest average value on MI proves that our method transfers more considerable information from both two source images. Values of EN and SD reveal that our results contain abundant information and the highest contrast between targets and the background. In conclusion, our method stably reserves useful information to a certain degree, especially the most discriminative target, the richest texture details, and considerable structure similarity with the source images.\n\n\nResults of infrared-visible object detection\n\nTo thoroughly discuss how does IVIF influences multimodality object detection performance, two datasets, i.e., Multispectral and M 3 FD, are employed. In which, we utilized YOLOv5 as the baseline model for object detection. For fair comparison, we retain the detection model on the fused result of seven state-of-the-art methods, respectively. Qualitative Comparisons As shown in Figure 8, note that merely using an infrared or visible sensor cannot detect well, e.g., a stopped car for infrared image and person for the visible one. On the contrary, almost all the fusion methods improve the detection performance by utilizing com-  Table 2. Quantitative results of object detection on the Multispectral and M 3 FD datasets among all the image fusion method + detector (YOLOv5). The best result is in red whereas the second best one is in blue.\n\nplementary information from both sides. With designing target-aware bilevel adversarial learning and a cooperative training scheme integration in our method, we can continuously generate a detection-friendly fused result, which has advantage in detecting person and vehicle, e.g., the sheltered car and pedestrians on distant rocks. Table 2 reported the quantitative results on two datasets. Almost all the fusion methods achieve promising detection results, in which the detection AP greatly exceed the case of using only the visible or infrared images. Note that our TarDAL is superior to other methods in terms of detection mAP on two datasets, which obtain 1.4% and 1.1% improvement compared to the second one, i.e., DenseFuse and GANMcC. It is worth pointing out that our TarDAL has advantage in dealing with the challenge scenes because TarDAL fully discovers the unique information from different modalities.\n\n\nQuantitative Comparisons\n\nComputational Complexity Analysis To comprehensively analyze the computational complexity of our method, we provide the time consumption and the computational efficiency of all the methods. As shown in the last column of Table 2, the strong computing ability of CNNs allows these learning-based methods to achieve high speed. Note that our method simultaneously achieves the highest running speed and lower computing complexity in terms of FLOPs and training parameters, ensembling the follow-up high-level vision application with high efficiency.\n\n\nAblation studies\n\nStudy on model architectures We investigate the model architecture of our method and further validate the effectiveness of different individual components. First, we remove the target discriminator D T from our whole network. In Figure 9, due to the lack of distinguishing significant infrared targets in this variant, the fused results tend to blur the target to a certain degree. Besides, in Table 3, note that D T also plays a vital role in boosting the detection performance after fusion. Second, the detail discriminator D D has a contribution in preserving textural details from the visible images. In the absence of D D , the background details of the fused image cannot be fully recovered, and the intuitive visual results can be found in Figure 9. However, D D has a tiny negative impact on object detection due to redundant background details. Furthermore, Without D T and D D integrating into our whole network, EN and SD can achieve the highest value on the TNO dataset. This is because that the heavy noise on the fusion results may cause a significant rise in terms of EN and SD.  Table 3. Quantitative comparisons of different model architectures. The best result is in red whereas the second best one is in blue.  Table 4. Quantitative comparisons of different training strategies. The best result is in red whereas the second best one is in blue. Analyzing the training loss functions We discuss the impact of different loss functions on our method. In Figure 10, it is easy to notice that our method can maintain much salient pixel distribution with high contrast than the method without SDW, which can illustrate the effectiveness of the newly designed SDW function. Meanwhile, the method without m may lose some vital details, e.g., leaves and chimney silhouette. This is because that m allows two discriminators to perform adversarial learning under their respective region, hence paying more attention to their unique features.\n\nEvaluating different versions of training strategy We further verify the advantages of our cooperative training (CT ) in comparing with direct training (DT ) and task-oriented training (T T ). As shown in Figure 11, T T only uses detection loss to train the network, resulting in a worse visual effect for observation. In contrast, CT has a significant advantage in boosting the detection performance and better visual effects. The same trend can be found in Table 4, CT reaches the largest or the second-largest scores among the two different datasets. \n\n\nConclusion\n\nWithin this paper, a bilevel optimization formulation for jointly realizing fusion and detection is proposed. By unrolling the model to a well-designed fusion network and a commonly used detection network, we can generate a visual-friendly result for fusion and object detection. To promote future researches in this field, we raise a synchronized imaging system with visible-infrared sensors and collect a multi-scenario multi-modality benchmark.\n\nFigure 1 .\n1From the left to right: Detection accuracy and computational comparisons with the state-of-the-art, the scenario and pixel distributions of our benchmark M 3 FD. Ours outperforms all counterparts with higher detection rates, lower average runtime, and fewer training parameters. M 3 FD covers comprehensive scenarios with a wide range of pixel variations especially on both modalities.\n\nFigure 2 .\n2Methodology framework: (a) bilevel optimization formulation for fusion and detection, (b) target-aware adversarial dual learning network for fusion, and (c) cooperative training scheme.\n\nFigure 3 .\n3The architectures of our generator and discriminator.\n\nFigure 4 .\n4Visualization of infrared-visible images in our M 3 FD dataset. The dataset covers extensive scenarios with various environments, illumination, season, and weather.min \u03c9d,\u03c9f\n\nFigure 5 .\n5Illustration of our synchronize imaging system.\n\nFigure 6 .Figure 7 .\n67Visual comparisons of our TarDAL with state-of-the-art methods on typical image pairs in TNO, RoadScene and M 3 FD datasets. Quantitative comparisons with seven IVIF methods on TNO, RoadScene and M 3 FD datasets, respectively. The x-axis represents metrics and the y-axis are the values. ( * ). In the boxes, the orange lines and the green tangles denote medium and mean values.\n\nFigure 8 .\n8Visual comparison of our TarDAL with state-of-the-art methods on the Multispectral and M 3 FD datasets. Bike Car Stop Cone All mAP@.5 Day Overcast Night Challenge All mAP@.5 SIZE(M) FLOPS(G) TIME(\n\nFigure 9 .\n9Progressive fusion results. From left to right: source images, base network, w/o-DT , w/o-DD and the full model.\n\nFigure 10 .Figure 11 .\n1011Qualitative results on discussing loss functions.Ir imageVis image TarDAL DT TarDAL T T TarDAL CT Visual comparisons of different training strategies.\n\n\nScene : x: Road y: Campus z: Street {: Hash Weather |: Disguise }: Smoogy~: Forest : OthersTable 1. Illustration of M 3 FD and existing aligned multi-modality datasets. Resolution refers to the average when it is different in a dataset.TNO dataset[33] is the most commonly used public available dateset for IVIF, which contains 261 pairs of multispectral imagery at day and night time. The INO dataset is provided by the National Optics Institute of Canada and contains aligned infrared and visible pairs. It contributes to developing multiple sensor types for video analysis applications in challenging environments.The OSU Color-Thermal Database is established for fusion-based object detection containing 285 pairs of registered infrared and color visible images. The whole dataset is collected at a busy pathway on the Ohio State University Campus during the daytime. Xu et al. released Roadscene, having 221 aligned infrared and visible pairs taken in the road scene containing rich objects, such as vehicles and pedestrians[35]. Takumi et al.[32] proposed a novel Multispectral dataset for autonomous driving that consists of RGB, NIR, MIR, and FIR images and annotated object categories.Dataset \nImg pairs Resolution Color Camera angle Nighttime Objects \nScene \nAnnotation \nTNO \n261 \n768\u00d7576 \n\nhorizontal \n65 \nfew \nx|}~ \n\nINO \n2100 \n328\u00d7254 \n\nsurveillance \n\nfew \nx{ \n\nOUS \n285 \n320\u00d7240 \n\nsurveillance \n\nfew \nx \n\nRoadscene \n221 \n768\u00d7576 \n\ndriving \n122 \nmedium \nxz \n\nMultispectral \n2999 \n768\u00d7576 \n\ndriving \n1139 \n14146 \nxz \n\nM 3 FD \n4200 \n1024\u00d7768 \n\nmultiplication \n1671 \n33603 \nx\u223c \n\n\n\n\n\nIn summary, our method depends on the intermediate results of each step, and each step plays a positive effect on the final fused result.Model \n\nDiscriminator \nTNO Dateset \nRoadscene Dataset \nM 3 FD Dataset \n\nD T \nD D \nMI \nEN \nSD \nMI \nEN \nSD \nMI \nEN \nSD \nmAP@.5 \n\nM1 \n\n\n\n\n2.506 \n7.223 \n53.107 \n3.307 \n7.295 \n48.561 \n2.942 \n7.282 \n44.857 \n0.722 \n\nM2 \n\n\n\n\n2.591 \n7.045 \n50.245 \n3.274 \n7.128 \n46.751 \n2.842 \n6.981 \n39.364 \n0.719 \n\nM3 \n\n\n\n\n2.596 \n7.024 \n46.727 \n3.127 \n7.037 \n42.656 \n2.814 \n7.086 \n41.255 \n0.781 \n\nM4 \n\n\n\n\n2.766 \n7.177 \n51.352 \n3.378 \n7.355 \n49.637 \n3.211 \n7.313 \n45.827 \n0.807 \n\n\nThis set includes pairs from one set of infrared and visible sensors, and the depth data from the binocular cameras will be published in the future.\n\nA new image quality metric for image fusion: the sum of the correlations of differences. V Aslantas, Bendes, Aeuinternational Journal of electronics and communications. 6912V Aslantas and E Bendes. A new image quality metric for image fusion: the sum of the correlations of differences. Aeu- international Journal of electronics and communications, 69(12):1890-1896, 2015.\n\nLearning a neural solver for multiple object tracking. Guillem Bras\u00f3, Laura Leal-Taix\u00e9, IEEE/CVF CVPR. Guillem Bras\u00f3 and Laura Leal-Taix\u00e9. Learning a neural solver for multiple object tracking. In IEEE/CVF CVPR, pages 6247-6257, 2020.\n\nR3net: Recurrent residual refinement network for saliency detection. Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guoqiang Han, Pheng-Ann Heng, IJCAI. Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin, Guoqiang Han, and Pheng-Ann Heng. R3net: Recurrent residual refinement network for saliency detection. In IJCAI, pages 684-690, 2018.\n\nDual attention network for scene segmentation. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu, IEEE/CVF CVPR. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In IEEE/CVF CVPR, pages 3146-3154, 2019.\n\nObject classification using cnn-based fusion of vision and lidar in autonomous vehicle environment. Hongbo Gao, Bo Cheng, Jianqiang Wang, Keqiang Li, Jianhui Zhao, Deyi Li, IEEE TII. 149Hongbo Gao, Bo Cheng, Jianqiang Wang, Keqiang Li, Jian- hui Zhao, and Deyi Li. Object classification using cnn-based fusion of vision and lidar in autonomous vehicle environ- ment. IEEE TII, 14(9):4224-4231, 2018.\n\nLaurens Van Der Maaten, and Kilian Weinberger. Convolutional networks with dense connectivity. Gao Huang, Zhuang Liu, Geoff Pleiss, IEEE TPAMI. Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger. Convolutional networks with dense connectivity. IEEE TPAMI, 2019.\n\nSiamrpn++: Evolution of siamese visual tracking with very deep networks. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan, IEEE/CVF CVPR. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In IEEE/CVF CVPR, pages 4282-4291, 2019.\n\nDensefuse: A fusion approach to infrared and visible images. Hui Li, Xiao-Jun Wu, IEEE TIP. 285Hui Li and Xiao-Jun Wu. Densefuse: A fusion approach to infrared and visible images. IEEE TIP, 28(5):2614-2623, 2018.\n\nRfn-nest: An end-toend residual fusion network for infrared and visible images. Hui Li, Xiao-Jun Wu, Josef Kittler, Information Fusion. 73Hui Li, Xiao-Jun Wu, and Josef Kittler. Rfn-nest: An end-to- end residual fusion network for infrared and visible images. Information Fusion, 73:72-86, 2021.\n\nImage fusion with guided filtering. Shutao Li, Xudong Kang, Jianwen Hu, IEEE TIP. 227Shutao Li, Xudong Kang, and Jianwen Hu. Image fusion with guided filtering. IEEE TIP, 22(7):2864-2875, 2013.\n\nMultiple task-oriented encoders for unified image fusion. Zhuoxiao Li, Jinyuan Liu, Risheng Liu, Xin Fan, Zhongxuan Luo, Wen Gao, IEEEICME. IEEEZhuoxiao Li, Jinyuan Liu, Risheng Liu, Xin Fan, Zhongx- uan Luo, and Wen Gao. Multiple task-oriented encoders for unified image fusion. In IEEEICME, pages 1-6. IEEE, 2021.\n\nLearning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion. Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, Zhongxuan Luo, IEEE TCSVT. 2021Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, and Zhongx- uan Luo. Learning a deep multi-scale feature ensemble and an edge-attention guidance for image fusion. IEEE TCSVT, 2021.\n\nSmoa: Searching a modality-oriented architecture for infrared and visible image fusion. Jinyuan Liu, Yuhui Wu, Zhanbo Huang, Risheng Liu, Xin Fan, IEEE SPL. 28Jinyuan Liu, Yuhui Wu, Zhanbo Huang, Risheng Liu, and Xin Fan. Smoa: Searching a modality-oriented architecture for infrared and visible image fusion. IEEE SPL, 28:1818- 1822, 2021.\n\nInvestigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, Zhouchen Lin, IEEE TPAMI. 2021Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learn- ing and vision from a unified perspective: A survey and be- yond. IEEE TPAMI, 2021.\n\nKnowledge-driven deep unrolling for robust image layer separation. Risheng Liu, Zhiying Jiang, Xin Fan, Zhongxuan Luo, IEEE TNNLS. Risheng Liu, Zhiying Jiang, Xin Fan, and Zhongxuan Luo. Knowledge-driven deep unrolling for robust image layer sep- aration. IEEE TNNLS, 2019.\n\nA bilevel integrated model with data-driven layer ensemble for multi-modality image fusion. Risheng Liu, Jinyuan Liu, Zhiying Jiang, Xin Fan, Zhongxuan Luo, IEEE TIP. 30Risheng Liu, Jinyuan Liu, Zhiying Jiang, Xin Fan, and Zhongxuan Luo. A bilevel integrated model with data-driven layer ensemble for multi-modality image fusion. IEEE TIP, 30:1261-1274, 2020.\n\nSearching a hierarchically aggregated fusion architecture for fast multi-modality image fusion. Risheng Liu, Zhu Liu, Jinyuan Liu, Xin Fan, ACM MM. Risheng Liu, Zhu Liu, Jinyuan Liu, and Xin Fan. Search- ing a hierarchically aggregated fusion architecture for fast multi-modality image fusion. In ACM MM, pages 1600- 1608, 2021.\n\nTask-oriented convex bilevel optimization with latent feasibility. Risheng Liu, Long Ma, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang, IEEE TIP. 2022Risheng Liu, Long Ma, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. Task-oriented convex bilevel optimization with latent feasibility. IEEE TIP, 2022.\n\nRetinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement. Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, Zhongxuan Luo, IEEE/CVF CVPR. Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongx- uan Luo. Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement. In IEEE/CVF CVPR, pages 10561-10570, 2021.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. Information Fusion. Jiayi Ma, Chen Chen, Chang Li, Jun Huang, 31Jiayi Ma, Chen Chen, Chang Li, and Jun Huang. Infrared and visible image fusion via gradient transfer and total variation minimization. Information Fusion, 31:100-109, 2016.\n\nDdcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion. Jiayi Ma, Han Xu, Junjun Jiang, Xiaoguang Mei, Xiao-Ping Zhang, IEEE TIP. 29Jiayi Ma, Han Xu, Junjun Jiang, Xiaoguang Mei, and Xiao- Ping Zhang. Ddcgan: A dual-discriminator conditional gen- erative adversarial network for multi-resolution image fu- sion. IEEE TIP, 29:4980-4995, 2020.\n\nFusiongan: A generative adversarial network for infrared and visible image fusion. Jiayi Ma, Wei Yu, Pengwei Liang, Chang Li, Junjun Jiang, Information Fusion. 48Jiayi Ma, Wei Yu, Pengwei Liang, Chang Li, and Junjun Jiang. Fusiongan: A generative adversarial network for in- frared and visible image fusion. Information Fusion, 48:11- 26, 2019.\n\nGanmcc: A generative adversarial network with multiclassification constraints for infrared and visible image fusion. Jiayi Ma, Hao Zhang, Zhenfeng Shao, Pengwei Liang, Han Xu, IEEE TIM. 70Jiayi Ma, Hao Zhang, Zhenfeng Shao, Pengwei Liang, and Han Xu. Ganmcc: A generative adversarial network with multiclassification constraints for infrared and visible image fusion. IEEE TIM, 70:1-14, 2020.\n\nInfrared and visible image fusion based on visual saliency map and weighted least square optimization. Jinlei Ma, Zhiqiang Zhou, Bo Wang, Hua Zong, Infrared Physics & Technology. 82Jinlei Ma, Zhiqiang Zhou, Bo Wang, and Hua Zong. In- frared and visible image fusion based on visual saliency map and weighted least square optimization. Infrared Physics & Technology, 82:8-17, 2017.\n\nLearning deep context-sensitive decomposition for low-light image enhancement. Long Ma, Risheng Liu, Jiaao Zhang, Xin Fan, Zhongxuan Luo, IEEE TNNLS. 2021Long Ma, Risheng Liu, Jiaao Zhang, Xin Fan, and Zhongx- uan Luo. Learning deep context-sensitive decomposition for low-light image enhancement. IEEE TNNLS, 2021.\n\nDual discriminator generative adversarial nets. Advances in neural information processing systems. Tu Nguyen, Trung Le, Hung Vu, Dinh Phung, 30Tu Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial nets. Advances in neural information processing systems, 30, 2017.\n\nBilevel optimization with nonsmooth lower level problems. Peter Ochs, Ren\u00e9 Ranftl, Thomas Brox, Thomas Pock, Lecture Notes in Computer Science. 9087Peter Ochs, Ren\u00e9 Ranftl, Thomas Brox, and Thomas Pock. Bilevel optimization with nonsmooth lower level problems. Lecture Notes in Computer Science, 9087:654-665, 2015.\n\nInfrared and visible image fusion using discrete cosine transform and swarm intelligence for surveillance applications. Nirmala Paramanandham, Kishore Rajendiran, Infrared Physics & Technology. 88Nirmala Paramanandham and Kishore Rajendiran. Infrared and visible image fusion using discrete cosine transform and swarm intelligence for surveillance applications. Infrared Physics & Technology, 88:13-22, 2018.\n\nGraphnet: learning image pseudo annotations for weaklysupervised semantic segmentation. Mengyang Pu, Yaping Huang, Qingji Guan, Qi Zou, ACM MM. ACMMengyang Pu, Yaping Huang, Qingji Guan, and Qi Zou. Graphnet: learning image pseudo annotations for weakly- supervised semantic segmentation. In ACM MM, pages 483- 491. ACM, 2018.\n\nInformation measure for performance of image fusion. Guihong Qu, Dali Zhang, Pingfan Yan, Electronics Letters. 387Guihong Qu, Dali Zhang, and Pingfan Yan. Information measure for performance of image fusion. Electronics Let- ters, 38(7):313-315, 2002.\n\nAssessment of image fusion procedures using entropy, image quality, and multispectral classification. Wesley J Roberts, Jan A Van, Fethi Ahmed, Journal of Applied Remote Sensing. 21Wesley J. Roberts, Jan A. Aardt Van, and Fethi Ahmed. As- sessment of image fusion procedures using entropy, image quality, and multispectral classification. Journal of Applied Remote Sensing, 2(1):1-28, 2008.\n\nMultispectral object detection for autonomous vehicles. Karasawa Takumi, Kohei Watanabe, Qishen Ha, Antonio Tejero-De-Pablos, Yoshitaka Ushiku, Tatsuya Harada, ACM MM. Karasawa Takumi, Kohei Watanabe, Qishen Ha, Antonio Tejero-De-Pablos, Yoshitaka Ushiku, and Tatsuya Harada. Multispectral object detection for autonomous vehicles. In ACM MM, pages 35-43, 2017.\n\nThe tno multiband image data collection. Data in brief. Alexander Toet, 15249Alexander Toet. The tno multiband image data collection. Data in brief, 15:249, 2017.\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Sheikh, P Eero, Simoncelli, IEEE TIP. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simon- celli, et al. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600-612, 2004.\n\nU2fusion: A unified unsupervised image fusion network. Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, Haibin Ling, IEEE TPAMI. Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin Ling. U2fusion: A unified unsupervised image fusion net- work. IEEE TPAMI, 2020.\n\nSpatial fusion gan for image synthesis. Fangneng Zhan, Hongyuan Zhu, Shijian Lu, IEEE/CVF CVPR. Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Spatial fu- sion gan for image synthesis. In IEEE/CVF CVPR, pages 3653-3662, 2019.\n\nSparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review. Qiang Zhang, Yi Liu, Rick S Blum, Jungong Han, Dacheng Tao, Information Fusion. 40Qiang Zhang, Yi Liu, Rick S Blum, Jungong Han, and Dacheng Tao. Sparse representation based multi-sensor im- age fusion for multi-focus and multi-modality images: A re- view. Information Fusion, 40:57-75, 2018.\n\nVifb: a visible and infrared image fusion benchmark. Xingchen Zhang, Ping Ye, Gang Xiao, IEEE/CVF CVPRW. Xingchen Zhang, Ping Ye, and Gang Xiao. Vifb: a vis- ible and infrared image fusion benchmark. In IEEE/CVF CVPRW, pages 104-105, 2020.\n\nMulti-focus image fusion with a natural enhancement via a joint multilevel deeply supervised convolutional neural network. Wenda Zhao, Dong Wang, Huchuan Lu, IEEE TCSVT. 294Wenda Zhao, Dong Wang, and Huchuan Lu. Multi-focus image fusion with a natural enhancement via a joint multi- level deeply supervised convolutional neural network. IEEE TCSVT, 29(4):1102-1115, 2018.\n\nDidfuse: Deep image decomposition for infrared and visible image fusion. Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Pengfei Li, Jiangshe Zhang, arXiv:2003.09210arXiv preprintZixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Pengfei Li, and Jiangshe Zhang. Didfuse: Deep image de- composition for infrared and visible image fusion. arXiv preprint arXiv:2003.09210, 2020.\n\nBayesian fusion for infrared and visible images. Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Jiangshe Zhang, Signal Processing. 177107734Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, and Jiangshe Zhang. Bayesian fusion for infrared and visible im- ages. Signal Processing, 177:107734, 2020.\n\nR2gan: Cross-modal recipe retrieval with generative adversarial network. Bin Zhu, Chong-Wah Ngo, Jingjing Chen, Yanbin Hao, IEEE/CVF CVPR. Bin Zhu, Chong-Wah Ngo, Jingjing Chen, and Yanbin Hao. R2gan: Cross-modal recipe retrieval with generative adver- sarial network. In IEEE/CVF CVPR, pages 11477-11486, 2019.\n\nA novel multi-modality image fusion method based on image decomposition and sparse representation. Zhiqin Zhu, Hongpeng Yin, Yi Chai, Yanxia Li, Guanqiu Qi, Information Sciences. 432Zhiqin Zhu, Hongpeng Yin, Yi Chai, Yanxia Li, and Guanqiu Qi. A novel multi-modality image fusion method based on image decomposition and sparse representation. Information Sciences, 432:516-529, 2018.\n", "annotations": {"author": "[{\"end\":357,\"start\":138},{\"end\":573,\"start\":358},{\"end\":794,\"start\":574},{\"end\":1013,\"start\":795},{\"end\":1251,\"start\":1014},{\"end\":1469,\"start\":1252},{\"end\":1691,\"start\":1470}]", "publisher": null, "author_last_name": "[{\"end\":149,\"start\":146},{\"end\":365,\"start\":362},{\"end\":586,\"start\":581},{\"end\":805,\"start\":803},{\"end\":1025,\"start\":1022},{\"end\":1261,\"start\":1256},{\"end\":1483,\"start\":1480}]", "author_first_name": "[{\"end\":145,\"start\":138},{\"end\":361,\"start\":358},{\"end\":580,\"start\":574},{\"end\":802,\"start\":795},{\"end\":1021,\"start\":1014},{\"end\":1255,\"start\":1252},{\"end\":1479,\"start\":1470}]", "author_affiliation": "[{\"end\":356,\"start\":151},{\"end\":572,\"start\":367},{\"end\":793,\"start\":588},{\"end\":1012,\"start\":807},{\"end\":1250,\"start\":1045},{\"end\":1468,\"start\":1263},{\"end\":1690,\"start\":1485}]", "title": "[{\"end\":135,\"start\":1},{\"end\":1826,\"start\":1692}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3375,\"start\":3371},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3402,\"start\":3399},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3580,\"start\":3576},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3583,\"start\":3580},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3586,\"start\":3583},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4123,\"start\":4120},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4126,\"start\":4123},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4140,\"start\":4137},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4142,\"start\":4140},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4162,\"start\":4158},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4425,\"start\":4421},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4428,\"start\":4425},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4453,\"start\":4449},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4456,\"start\":4453},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4459,\"start\":4456},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4486,\"start\":4482},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4489,\"start\":4486},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4924,\"start\":4921},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4927,\"start\":4924},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4931,\"start\":4927},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4935,\"start\":4931},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4939,\"start\":4935},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4942,\"start\":4939},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8310,\"start\":8306},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8313,\"start\":8310},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8316,\"start\":8313},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8319,\"start\":8316},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8322,\"start\":8319},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8325,\"start\":8322},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8328,\"start\":8325},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8331,\"start\":8328},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8532,\"start\":8529},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8535,\"start\":8532},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8538,\"start\":8535},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8541,\"start\":8538},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8814,\"start\":8811},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9054,\"start\":9050},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9057,\"start\":9054},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9060,\"start\":9057},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9150,\"start\":9146},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9154,\"start\":9150},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9158,\"start\":9154},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9291,\"start\":9287},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9527,\"start\":9523},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9884,\"start\":9880},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9951,\"start\":9947},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9983,\"start\":9979},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11030,\"start\":11026},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11033,\"start\":11030},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11036,\"start\":11033},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13624,\"start\":13620},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14288,\"start\":14285},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14907,\"start\":14904},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18826,\"start\":18825},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20101,\"start\":20098},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20117,\"start\":20113},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20126,\"start\":20123},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20139,\"start\":20135},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20152,\"start\":20148},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20164,\"start\":20160},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20183,\"start\":20179},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21482,\"start\":21478},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21501,\"start\":21497},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21533,\"start\":21530},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29853,\"start\":29849},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30635,\"start\":30631},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30654,\"start\":30650}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28167,\"start\":27769},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28366,\"start\":28168},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28433,\"start\":28367},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28620,\"start\":28434},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28681,\"start\":28621},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29084,\"start\":28682},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29294,\"start\":29085},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29420,\"start\":29295},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29599,\"start\":29421},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31192,\"start\":29600},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31787,\"start\":31193}]", "paragraph": "[{\"end\":3243,\"start\":1840},{\"end\":4246,\"start\":3260},{\"end\":5041,\"start\":4248},{\"end\":5622,\"start\":5043},{\"end\":6080,\"start\":5624},{\"end\":6886,\"start\":6082},{\"end\":7535,\"start\":6888},{\"end\":7936,\"start\":7537},{\"end\":8205,\"start\":7954},{\"end\":8966,\"start\":8235},{\"end\":9759,\"start\":8968},{\"end\":10289,\"start\":9774},{\"end\":10608,\"start\":10313},{\"end\":11113,\"start\":10632},{\"end\":11934,\"start\":11206},{\"end\":12215,\"start\":11995},{\"end\":12646,\"start\":12257},{\"end\":13403,\"start\":12648},{\"end\":13651,\"start\":13405},{\"end\":13958,\"start\":13703},{\"end\":14113,\"start\":13993},{\"end\":14160,\"start\":14155},{\"end\":14251,\"start\":14212},{\"end\":14586,\"start\":14253},{\"end\":15104,\"start\":14588},{\"end\":15407,\"start\":15248},{\"end\":15618,\"start\":15409},{\"end\":16054,\"start\":15726},{\"end\":16123,\"start\":16056},{\"end\":16210,\"start\":16165},{\"end\":16524,\"start\":16251},{\"end\":16911,\"start\":16599},{\"end\":17545,\"start\":17005},{\"end\":18490,\"start\":17589},{\"end\":18828,\"start\":18492},{\"end\":19276,\"start\":18830},{\"end\":19932,\"start\":19292},{\"end\":20185,\"start\":19977},{\"end\":21183,\"start\":20213},{\"end\":22393,\"start\":21185},{\"end\":23287,\"start\":22442},{\"end\":24204,\"start\":23289},{\"end\":24780,\"start\":24233},{\"end\":26750,\"start\":24801},{\"end\":27306,\"start\":26752},{\"end\":27768,\"start\":27321}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11143,\"start\":11114},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11205,\"start\":11143},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11994,\"start\":11935},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13702,\"start\":13652},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13992,\"start\":13959},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14154,\"start\":14114},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14211,\"start\":14161},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15158,\"start\":15105},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15247,\"start\":15158},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15671,\"start\":15619},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15725,\"start\":15671},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16164,\"start\":16124},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16569,\"start\":16525},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16598,\"start\":16569},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17004,\"start\":16912}]", "table_ref": "[{\"end\":9998,\"start\":9991},{\"end\":23083,\"start\":23076},{\"end\":23629,\"start\":23622},{\"end\":24461,\"start\":24454},{\"end\":25202,\"start\":25195},{\"end\":25903,\"start\":25896},{\"end\":26038,\"start\":26031},{\"end\":27218,\"start\":27211}]", "section_header": "[{\"end\":1838,\"start\":1830},{\"attributes\":{\"n\":\"1.\"},\"end\":3258,\"start\":3246},{\"attributes\":{\"n\":\"2.\"},\"end\":7952,\"start\":7939},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8233,\"start\":8208},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9772,\"start\":9762},{\"attributes\":{\"n\":\"3.\"},\"end\":10311,\"start\":10292},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10630,\"start\":10611},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12255,\"start\":12218},{\"end\":16217,\"start\":16213},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16249,\"start\":16220},{\"attributes\":{\"n\":\"4.\"},\"end\":17587,\"start\":17548},{\"attributes\":{\"n\":\"5.\"},\"end\":19290,\"start\":19279},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19975,\"start\":19935},{\"end\":20211,\"start\":20188},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22440,\"start\":22396},{\"end\":24231,\"start\":24207},{\"attributes\":{\"n\":\"5.3.\"},\"end\":24799,\"start\":24783},{\"attributes\":{\"n\":\"6.\"},\"end\":27319,\"start\":27309},{\"end\":27780,\"start\":27770},{\"end\":28179,\"start\":28169},{\"end\":28378,\"start\":28368},{\"end\":28445,\"start\":28435},{\"end\":28632,\"start\":28622},{\"end\":28703,\"start\":28683},{\"end\":29096,\"start\":29086},{\"end\":29306,\"start\":29296},{\"end\":29444,\"start\":29422}]", "table": "[{\"end\":31192,\"start\":30796},{\"end\":31787,\"start\":31332}]", "figure_caption": "[{\"end\":28167,\"start\":27782},{\"end\":28366,\"start\":28181},{\"end\":28433,\"start\":28380},{\"end\":28620,\"start\":28447},{\"end\":28681,\"start\":28634},{\"end\":29084,\"start\":28706},{\"end\":29294,\"start\":29098},{\"end\":29420,\"start\":29308},{\"end\":29599,\"start\":29449},{\"end\":30796,\"start\":29602},{\"end\":31332,\"start\":31195}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6679,\"start\":6671},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7935,\"start\":7927},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11534,\"start\":11526},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12830,\"start\":12819},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13118,\"start\":13107},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13258,\"start\":13247},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15972,\"start\":15964},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16739,\"start\":16731},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18020,\"start\":18012},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18991,\"start\":18983},{\"end\":20317,\"start\":20309},{\"end\":20497,\"start\":20489},{\"end\":21614,\"start\":21606},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22830,\"start\":22822},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25038,\"start\":25030},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25556,\"start\":25548},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26280,\"start\":26271},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26966,\"start\":26957}]", "bib_author_first_name": "[{\"end\":32028,\"start\":32027},{\"end\":32374,\"start\":32367},{\"end\":32387,\"start\":32382},{\"end\":32622,\"start\":32617},{\"end\":32636,\"start\":32629},{\"end\":32644,\"start\":32641},{\"end\":32657,\"start\":32650},{\"end\":32666,\"start\":32662},{\"end\":32680,\"start\":32672},{\"end\":32695,\"start\":32686},{\"end\":32949,\"start\":32946},{\"end\":32958,\"start\":32954},{\"end\":32970,\"start\":32964},{\"end\":32981,\"start\":32977},{\"end\":32993,\"start\":32986},{\"end\":33005,\"start\":32999},{\"end\":33019,\"start\":33012},{\"end\":33316,\"start\":33310},{\"end\":33324,\"start\":33322},{\"end\":33341,\"start\":33332},{\"end\":33355,\"start\":33348},{\"end\":33367,\"start\":33360},{\"end\":33378,\"start\":33374},{\"end\":33709,\"start\":33706},{\"end\":33723,\"start\":33717},{\"end\":33734,\"start\":33729},{\"end\":33981,\"start\":33979},{\"end\":33989,\"start\":33986},{\"end\":33999,\"start\":33994},{\"end\":34012,\"start\":34006},{\"end\":34028,\"start\":34020},{\"end\":34041,\"start\":34035},{\"end\":34313,\"start\":34310},{\"end\":34326,\"start\":34318},{\"end\":34546,\"start\":34543},{\"end\":34559,\"start\":34551},{\"end\":34569,\"start\":34564},{\"end\":34802,\"start\":34796},{\"end\":34813,\"start\":34807},{\"end\":34827,\"start\":34820},{\"end\":35021,\"start\":35013},{\"end\":35033,\"start\":35026},{\"end\":35046,\"start\":35039},{\"end\":35055,\"start\":35052},{\"end\":35070,\"start\":35061},{\"end\":35079,\"start\":35076},{\"end\":35373,\"start\":35366},{\"end\":35382,\"start\":35379},{\"end\":35390,\"start\":35388},{\"end\":35405,\"start\":35398},{\"end\":35420,\"start\":35411},{\"end\":35716,\"start\":35709},{\"end\":35727,\"start\":35722},{\"end\":35738,\"start\":35732},{\"end\":35753,\"start\":35746},{\"end\":35762,\"start\":35759},{\"end\":36079,\"start\":36072},{\"end\":36091,\"start\":36085},{\"end\":36100,\"start\":36097},{\"end\":36112,\"start\":36108},{\"end\":36127,\"start\":36119},{\"end\":36420,\"start\":36413},{\"end\":36433,\"start\":36426},{\"end\":36444,\"start\":36441},{\"end\":36459,\"start\":36450},{\"end\":36720,\"start\":36713},{\"end\":36733,\"start\":36726},{\"end\":36746,\"start\":36739},{\"end\":36757,\"start\":36754},{\"end\":36772,\"start\":36763},{\"end\":37085,\"start\":37078},{\"end\":37094,\"start\":37091},{\"end\":37107,\"start\":37100},{\"end\":37116,\"start\":37113},{\"end\":37386,\"start\":37379},{\"end\":37396,\"start\":37392},{\"end\":37409,\"start\":37401},{\"end\":37424,\"start\":37416},{\"end\":37434,\"start\":37431},{\"end\":37717,\"start\":37710},{\"end\":37727,\"start\":37723},{\"end\":37737,\"start\":37732},{\"end\":37748,\"start\":37745},{\"end\":37763,\"start\":37754},{\"end\":38111,\"start\":38106},{\"end\":38120,\"start\":38116},{\"end\":38132,\"start\":38127},{\"end\":38140,\"start\":38137},{\"end\":38437,\"start\":38432},{\"end\":38445,\"start\":38442},{\"end\":38456,\"start\":38450},{\"end\":38473,\"start\":38464},{\"end\":38488,\"start\":38479},{\"end\":38807,\"start\":38802},{\"end\":38815,\"start\":38812},{\"end\":38827,\"start\":38820},{\"end\":38840,\"start\":38835},{\"end\":38851,\"start\":38845},{\"end\":39187,\"start\":39182},{\"end\":39195,\"start\":39192},{\"end\":39211,\"start\":39203},{\"end\":39225,\"start\":39218},{\"end\":39236,\"start\":39233},{\"end\":39568,\"start\":39562},{\"end\":39581,\"start\":39573},{\"end\":39590,\"start\":39588},{\"end\":39600,\"start\":39597},{\"end\":39924,\"start\":39920},{\"end\":39936,\"start\":39929},{\"end\":39947,\"start\":39942},{\"end\":39958,\"start\":39955},{\"end\":39973,\"start\":39964},{\"end\":40259,\"start\":40257},{\"end\":40273,\"start\":40268},{\"end\":40282,\"start\":40278},{\"end\":40291,\"start\":40287},{\"end\":40520,\"start\":40515},{\"end\":40531,\"start\":40527},{\"end\":40546,\"start\":40540},{\"end\":40559,\"start\":40553},{\"end\":40901,\"start\":40894},{\"end\":40924,\"start\":40917},{\"end\":41280,\"start\":41272},{\"end\":41291,\"start\":41285},{\"end\":41305,\"start\":41299},{\"end\":41314,\"start\":41312},{\"end\":41572,\"start\":41565},{\"end\":41581,\"start\":41577},{\"end\":41596,\"start\":41589},{\"end\":41873,\"start\":41867},{\"end\":41875,\"start\":41874},{\"end\":41888,\"start\":41885},{\"end\":41890,\"start\":41889},{\"end\":41901,\"start\":41896},{\"end\":42221,\"start\":42213},{\"end\":42235,\"start\":42230},{\"end\":42252,\"start\":42246},{\"end\":42264,\"start\":42257},{\"end\":42292,\"start\":42283},{\"end\":42308,\"start\":42301},{\"end\":42585,\"start\":42576},{\"end\":42762,\"start\":42758},{\"end\":42773,\"start\":42769},{\"end\":42775,\"start\":42774},{\"end\":42784,\"start\":42783},{\"end\":42801,\"start\":42800},{\"end\":43066,\"start\":43063},{\"end\":43076,\"start\":43071},{\"end\":43087,\"start\":43081},{\"end\":43102,\"start\":43095},{\"end\":43114,\"start\":43108},{\"end\":43319,\"start\":43311},{\"end\":43334,\"start\":43326},{\"end\":43347,\"start\":43340},{\"end\":43608,\"start\":43603},{\"end\":43618,\"start\":43616},{\"end\":43628,\"start\":43624},{\"end\":43630,\"start\":43629},{\"end\":43644,\"start\":43637},{\"end\":43657,\"start\":43650},{\"end\":43958,\"start\":43950},{\"end\":43970,\"start\":43966},{\"end\":43979,\"start\":43975},{\"end\":44266,\"start\":44261},{\"end\":44277,\"start\":44273},{\"end\":44291,\"start\":44284},{\"end\":44591,\"start\":44584},{\"end\":44604,\"start\":44598},{\"end\":44616,\"start\":44609},{\"end\":44630,\"start\":44624},{\"end\":44643,\"start\":44636},{\"end\":44656,\"start\":44648},{\"end\":44949,\"start\":44942},{\"end\":44962,\"start\":44956},{\"end\":44974,\"start\":44967},{\"end\":44988,\"start\":44982},{\"end\":45002,\"start\":44994},{\"end\":45275,\"start\":45272},{\"end\":45290,\"start\":45281},{\"end\":45304,\"start\":45296},{\"end\":45317,\"start\":45311},{\"end\":45617,\"start\":45611},{\"end\":45631,\"start\":45623},{\"end\":45639,\"start\":45637},{\"end\":45652,\"start\":45646},{\"end\":45664,\"start\":45657}]", "bib_author_last_name": "[{\"end\":32037,\"start\":32029},{\"end\":32045,\"start\":32039},{\"end\":32380,\"start\":32375},{\"end\":32398,\"start\":32388},{\"end\":32627,\"start\":32623},{\"end\":32639,\"start\":32637},{\"end\":32648,\"start\":32645},{\"end\":32660,\"start\":32658},{\"end\":32670,\"start\":32667},{\"end\":32684,\"start\":32681},{\"end\":32700,\"start\":32696},{\"end\":32952,\"start\":32950},{\"end\":32962,\"start\":32959},{\"end\":32975,\"start\":32971},{\"end\":32984,\"start\":32982},{\"end\":32997,\"start\":32994},{\"end\":33010,\"start\":33006},{\"end\":33022,\"start\":33020},{\"end\":33320,\"start\":33317},{\"end\":33330,\"start\":33325},{\"end\":33346,\"start\":33342},{\"end\":33358,\"start\":33356},{\"end\":33372,\"start\":33368},{\"end\":33381,\"start\":33379},{\"end\":33715,\"start\":33710},{\"end\":33727,\"start\":33724},{\"end\":33741,\"start\":33735},{\"end\":33984,\"start\":33982},{\"end\":33992,\"start\":33990},{\"end\":34004,\"start\":34000},{\"end\":34018,\"start\":34013},{\"end\":34033,\"start\":34029},{\"end\":34045,\"start\":34042},{\"end\":34316,\"start\":34314},{\"end\":34329,\"start\":34327},{\"end\":34549,\"start\":34547},{\"end\":34562,\"start\":34560},{\"end\":34577,\"start\":34570},{\"end\":34805,\"start\":34803},{\"end\":34818,\"start\":34814},{\"end\":34830,\"start\":34828},{\"end\":35024,\"start\":35022},{\"end\":35037,\"start\":35034},{\"end\":35050,\"start\":35047},{\"end\":35059,\"start\":35056},{\"end\":35074,\"start\":35071},{\"end\":35083,\"start\":35080},{\"end\":35377,\"start\":35374},{\"end\":35386,\"start\":35383},{\"end\":35396,\"start\":35391},{\"end\":35409,\"start\":35406},{\"end\":35424,\"start\":35421},{\"end\":35720,\"start\":35717},{\"end\":35730,\"start\":35728},{\"end\":35744,\"start\":35739},{\"end\":35757,\"start\":35754},{\"end\":35766,\"start\":35763},{\"end\":36083,\"start\":36080},{\"end\":36095,\"start\":36092},{\"end\":36106,\"start\":36101},{\"end\":36117,\"start\":36113},{\"end\":36131,\"start\":36128},{\"end\":36424,\"start\":36421},{\"end\":36439,\"start\":36434},{\"end\":36448,\"start\":36445},{\"end\":36463,\"start\":36460},{\"end\":36724,\"start\":36721},{\"end\":36737,\"start\":36734},{\"end\":36752,\"start\":36747},{\"end\":36761,\"start\":36758},{\"end\":36776,\"start\":36773},{\"end\":37089,\"start\":37086},{\"end\":37098,\"start\":37095},{\"end\":37111,\"start\":37108},{\"end\":37120,\"start\":37117},{\"end\":37390,\"start\":37387},{\"end\":37399,\"start\":37397},{\"end\":37414,\"start\":37410},{\"end\":37429,\"start\":37425},{\"end\":37440,\"start\":37435},{\"end\":37721,\"start\":37718},{\"end\":37730,\"start\":37728},{\"end\":37743,\"start\":37738},{\"end\":37752,\"start\":37749},{\"end\":37767,\"start\":37764},{\"end\":38114,\"start\":38112},{\"end\":38125,\"start\":38121},{\"end\":38135,\"start\":38133},{\"end\":38146,\"start\":38141},{\"end\":38440,\"start\":38438},{\"end\":38448,\"start\":38446},{\"end\":38462,\"start\":38457},{\"end\":38477,\"start\":38474},{\"end\":38494,\"start\":38489},{\"end\":38810,\"start\":38808},{\"end\":38818,\"start\":38816},{\"end\":38833,\"start\":38828},{\"end\":38843,\"start\":38841},{\"end\":38857,\"start\":38852},{\"end\":39190,\"start\":39188},{\"end\":39201,\"start\":39196},{\"end\":39216,\"start\":39212},{\"end\":39231,\"start\":39226},{\"end\":39239,\"start\":39237},{\"end\":39571,\"start\":39569},{\"end\":39586,\"start\":39582},{\"end\":39595,\"start\":39591},{\"end\":39605,\"start\":39601},{\"end\":39927,\"start\":39925},{\"end\":39940,\"start\":39937},{\"end\":39953,\"start\":39948},{\"end\":39962,\"start\":39959},{\"end\":39977,\"start\":39974},{\"end\":40266,\"start\":40260},{\"end\":40276,\"start\":40274},{\"end\":40285,\"start\":40283},{\"end\":40297,\"start\":40292},{\"end\":40525,\"start\":40521},{\"end\":40538,\"start\":40532},{\"end\":40551,\"start\":40547},{\"end\":40564,\"start\":40560},{\"end\":40915,\"start\":40902},{\"end\":40935,\"start\":40925},{\"end\":41283,\"start\":41281},{\"end\":41297,\"start\":41292},{\"end\":41310,\"start\":41306},{\"end\":41318,\"start\":41315},{\"end\":41575,\"start\":41573},{\"end\":41587,\"start\":41582},{\"end\":41600,\"start\":41597},{\"end\":41883,\"start\":41876},{\"end\":41894,\"start\":41891},{\"end\":41907,\"start\":41902},{\"end\":42228,\"start\":42222},{\"end\":42244,\"start\":42236},{\"end\":42255,\"start\":42253},{\"end\":42281,\"start\":42265},{\"end\":42299,\"start\":42293},{\"end\":42315,\"start\":42309},{\"end\":42590,\"start\":42586},{\"end\":42767,\"start\":42763},{\"end\":42781,\"start\":42776},{\"end\":42790,\"start\":42785},{\"end\":42798,\"start\":42792},{\"end\":42806,\"start\":42802},{\"end\":42818,\"start\":42808},{\"end\":43069,\"start\":43067},{\"end\":43079,\"start\":43077},{\"end\":43093,\"start\":43088},{\"end\":43106,\"start\":43103},{\"end\":43119,\"start\":43115},{\"end\":43324,\"start\":43320},{\"end\":43338,\"start\":43335},{\"end\":43350,\"start\":43348},{\"end\":43614,\"start\":43609},{\"end\":43622,\"start\":43619},{\"end\":43635,\"start\":43631},{\"end\":43648,\"start\":43645},{\"end\":43661,\"start\":43658},{\"end\":43964,\"start\":43959},{\"end\":43973,\"start\":43971},{\"end\":43984,\"start\":43980},{\"end\":44271,\"start\":44267},{\"end\":44282,\"start\":44278},{\"end\":44294,\"start\":44292},{\"end\":44596,\"start\":44592},{\"end\":44607,\"start\":44605},{\"end\":44622,\"start\":44617},{\"end\":44634,\"start\":44631},{\"end\":44646,\"start\":44644},{\"end\":44662,\"start\":44657},{\"end\":44954,\"start\":44950},{\"end\":44965,\"start\":44963},{\"end\":44980,\"start\":44975},{\"end\":44992,\"start\":44989},{\"end\":45008,\"start\":45003},{\"end\":45279,\"start\":45276},{\"end\":45294,\"start\":45291},{\"end\":45309,\"start\":45305},{\"end\":45321,\"start\":45318},{\"end\":45621,\"start\":45618},{\"end\":45635,\"start\":45632},{\"end\":45644,\"start\":45640},{\"end\":45655,\"start\":45653},{\"end\":45667,\"start\":45665}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":120241709},\"end\":32310,\"start\":31938},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":209376180},\"end\":32546,\"start\":32312},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":51605528},\"end\":32897,\"start\":32548},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52180375},\"end\":33208,\"start\":32899},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52161398},\"end\":33609,\"start\":33210},{\"attributes\":{\"id\":\"b5\"},\"end\":33904,\"start\":33611},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57189581},\"end\":34247,\"start\":33906},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5060429},\"end\":34461,\"start\":34249},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":232146779},\"end\":34758,\"start\":34463},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":493320},\"end\":34953,\"start\":34760},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":236264098},\"end\":35270,\"start\":34955},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":234047512},\"end\":35619,\"start\":35272},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":238221295},\"end\":35961,\"start\":35621},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":231718655},\"end\":36344,\"start\":35963},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":198172253},\"end\":36619,\"start\":36346},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":229179182},\"end\":36980,\"start\":36621},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":239011619},\"end\":37310,\"start\":36982},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":245537589},\"end\":37605,\"start\":37312},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":228083778},\"end\":37994,\"start\":37607},{\"attributes\":{\"id\":\"b19\"},\"end\":38323,\"start\":37996},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":212708911},\"end\":38717,\"start\":38325},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":71142966},\"end\":39063,\"start\":38719},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":229647400},\"end\":39457,\"start\":39065},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":125122786},\"end\":39839,\"start\":39459},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":233471949},\"end\":40156,\"start\":39841},{\"attributes\":{\"id\":\"b25\"},\"end\":40455,\"start\":40158},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12300817},\"end\":40772,\"start\":40457},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":49364610},\"end\":41182,\"start\":40774},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53039492},\"end\":41510,\"start\":41184},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":119542121},\"end\":41763,\"start\":41512},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":119424378},\"end\":42155,\"start\":41765},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":24464826},\"end\":42518,\"start\":42157},{\"attributes\":{\"id\":\"b32\"},\"end\":42682,\"start\":42520},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":207761262},\"end\":43006,\"start\":42684},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220934367},\"end\":43269,\"start\":43008},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":55702038},\"end\":43494,\"start\":43271},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2880403},\"end\":43895,\"start\":43496},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":211068851},\"end\":44136,\"start\":43897},{\"attributes\":{\"id\":\"b38\"},\"end\":44509,\"start\":44138},{\"attributes\":{\"doi\":\"arXiv:2003.09210\",\"id\":\"b39\"},\"end\":44891,\"start\":44511},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":218596113},\"end\":45197,\"start\":44893},{\"attributes\":{\"id\":\"b41\"},\"end\":45510,\"start\":45199},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3416770},\"end\":45895,\"start\":45512}]", "bib_title": "[{\"end\":32025,\"start\":31938},{\"end\":32365,\"start\":32312},{\"end\":32615,\"start\":32548},{\"end\":32944,\"start\":32899},{\"end\":33308,\"start\":33210},{\"end\":33704,\"start\":33611},{\"end\":33977,\"start\":33906},{\"end\":34308,\"start\":34249},{\"end\":34541,\"start\":34463},{\"end\":34794,\"start\":34760},{\"end\":35011,\"start\":34955},{\"end\":35364,\"start\":35272},{\"end\":35707,\"start\":35621},{\"end\":36070,\"start\":35963},{\"end\":36411,\"start\":36346},{\"end\":36711,\"start\":36621},{\"end\":37076,\"start\":36982},{\"end\":37377,\"start\":37312},{\"end\":37708,\"start\":37607},{\"end\":38430,\"start\":38325},{\"end\":38800,\"start\":38719},{\"end\":39180,\"start\":39065},{\"end\":39560,\"start\":39459},{\"end\":39918,\"start\":39841},{\"end\":40513,\"start\":40457},{\"end\":40892,\"start\":40774},{\"end\":41270,\"start\":41184},{\"end\":41563,\"start\":41512},{\"end\":41865,\"start\":41765},{\"end\":42211,\"start\":42157},{\"end\":42756,\"start\":42684},{\"end\":43061,\"start\":43008},{\"end\":43309,\"start\":43271},{\"end\":43601,\"start\":43496},{\"end\":43948,\"start\":43897},{\"end\":44259,\"start\":44138},{\"end\":44940,\"start\":44893},{\"end\":45270,\"start\":45199},{\"end\":45609,\"start\":45512}]", "bib_author": "[{\"end\":32039,\"start\":32027},{\"end\":32047,\"start\":32039},{\"end\":32382,\"start\":32367},{\"end\":32400,\"start\":32382},{\"end\":32629,\"start\":32617},{\"end\":32641,\"start\":32629},{\"end\":32650,\"start\":32641},{\"end\":32662,\"start\":32650},{\"end\":32672,\"start\":32662},{\"end\":32686,\"start\":32672},{\"end\":32702,\"start\":32686},{\"end\":32954,\"start\":32946},{\"end\":32964,\"start\":32954},{\"end\":32977,\"start\":32964},{\"end\":32986,\"start\":32977},{\"end\":32999,\"start\":32986},{\"end\":33012,\"start\":32999},{\"end\":33024,\"start\":33012},{\"end\":33322,\"start\":33310},{\"end\":33332,\"start\":33322},{\"end\":33348,\"start\":33332},{\"end\":33360,\"start\":33348},{\"end\":33374,\"start\":33360},{\"end\":33383,\"start\":33374},{\"end\":33717,\"start\":33706},{\"end\":33729,\"start\":33717},{\"end\":33743,\"start\":33729},{\"end\":33986,\"start\":33979},{\"end\":33994,\"start\":33986},{\"end\":34006,\"start\":33994},{\"end\":34020,\"start\":34006},{\"end\":34035,\"start\":34020},{\"end\":34047,\"start\":34035},{\"end\":34318,\"start\":34310},{\"end\":34331,\"start\":34318},{\"end\":34551,\"start\":34543},{\"end\":34564,\"start\":34551},{\"end\":34579,\"start\":34564},{\"end\":34807,\"start\":34796},{\"end\":34820,\"start\":34807},{\"end\":34832,\"start\":34820},{\"end\":35026,\"start\":35013},{\"end\":35039,\"start\":35026},{\"end\":35052,\"start\":35039},{\"end\":35061,\"start\":35052},{\"end\":35076,\"start\":35061},{\"end\":35085,\"start\":35076},{\"end\":35379,\"start\":35366},{\"end\":35388,\"start\":35379},{\"end\":35398,\"start\":35388},{\"end\":35411,\"start\":35398},{\"end\":35426,\"start\":35411},{\"end\":35722,\"start\":35709},{\"end\":35732,\"start\":35722},{\"end\":35746,\"start\":35732},{\"end\":35759,\"start\":35746},{\"end\":35768,\"start\":35759},{\"end\":36085,\"start\":36072},{\"end\":36097,\"start\":36085},{\"end\":36108,\"start\":36097},{\"end\":36119,\"start\":36108},{\"end\":36133,\"start\":36119},{\"end\":36426,\"start\":36413},{\"end\":36441,\"start\":36426},{\"end\":36450,\"start\":36441},{\"end\":36465,\"start\":36450},{\"end\":36726,\"start\":36713},{\"end\":36739,\"start\":36726},{\"end\":36754,\"start\":36739},{\"end\":36763,\"start\":36754},{\"end\":36778,\"start\":36763},{\"end\":37091,\"start\":37078},{\"end\":37100,\"start\":37091},{\"end\":37113,\"start\":37100},{\"end\":37122,\"start\":37113},{\"end\":37392,\"start\":37379},{\"end\":37401,\"start\":37392},{\"end\":37416,\"start\":37401},{\"end\":37431,\"start\":37416},{\"end\":37442,\"start\":37431},{\"end\":37723,\"start\":37710},{\"end\":37732,\"start\":37723},{\"end\":37745,\"start\":37732},{\"end\":37754,\"start\":37745},{\"end\":37769,\"start\":37754},{\"end\":38116,\"start\":38106},{\"end\":38127,\"start\":38116},{\"end\":38137,\"start\":38127},{\"end\":38148,\"start\":38137},{\"end\":38442,\"start\":38432},{\"end\":38450,\"start\":38442},{\"end\":38464,\"start\":38450},{\"end\":38479,\"start\":38464},{\"end\":38496,\"start\":38479},{\"end\":38812,\"start\":38802},{\"end\":38820,\"start\":38812},{\"end\":38835,\"start\":38820},{\"end\":38845,\"start\":38835},{\"end\":38859,\"start\":38845},{\"end\":39192,\"start\":39182},{\"end\":39203,\"start\":39192},{\"end\":39218,\"start\":39203},{\"end\":39233,\"start\":39218},{\"end\":39241,\"start\":39233},{\"end\":39573,\"start\":39562},{\"end\":39588,\"start\":39573},{\"end\":39597,\"start\":39588},{\"end\":39607,\"start\":39597},{\"end\":39929,\"start\":39920},{\"end\":39942,\"start\":39929},{\"end\":39955,\"start\":39942},{\"end\":39964,\"start\":39955},{\"end\":39979,\"start\":39964},{\"end\":40268,\"start\":40257},{\"end\":40278,\"start\":40268},{\"end\":40287,\"start\":40278},{\"end\":40299,\"start\":40287},{\"end\":40527,\"start\":40515},{\"end\":40540,\"start\":40527},{\"end\":40553,\"start\":40540},{\"end\":40566,\"start\":40553},{\"end\":40917,\"start\":40894},{\"end\":40937,\"start\":40917},{\"end\":41285,\"start\":41272},{\"end\":41299,\"start\":41285},{\"end\":41312,\"start\":41299},{\"end\":41320,\"start\":41312},{\"end\":41577,\"start\":41565},{\"end\":41589,\"start\":41577},{\"end\":41602,\"start\":41589},{\"end\":41885,\"start\":41867},{\"end\":41896,\"start\":41885},{\"end\":41909,\"start\":41896},{\"end\":42230,\"start\":42213},{\"end\":42246,\"start\":42230},{\"end\":42257,\"start\":42246},{\"end\":42283,\"start\":42257},{\"end\":42301,\"start\":42283},{\"end\":42317,\"start\":42301},{\"end\":42592,\"start\":42576},{\"end\":42769,\"start\":42758},{\"end\":42783,\"start\":42769},{\"end\":42792,\"start\":42783},{\"end\":42800,\"start\":42792},{\"end\":42808,\"start\":42800},{\"end\":42820,\"start\":42808},{\"end\":43071,\"start\":43063},{\"end\":43081,\"start\":43071},{\"end\":43095,\"start\":43081},{\"end\":43108,\"start\":43095},{\"end\":43121,\"start\":43108},{\"end\":43326,\"start\":43311},{\"end\":43340,\"start\":43326},{\"end\":43352,\"start\":43340},{\"end\":43616,\"start\":43603},{\"end\":43624,\"start\":43616},{\"end\":43637,\"start\":43624},{\"end\":43650,\"start\":43637},{\"end\":43663,\"start\":43650},{\"end\":43966,\"start\":43950},{\"end\":43975,\"start\":43966},{\"end\":43986,\"start\":43975},{\"end\":44273,\"start\":44261},{\"end\":44284,\"start\":44273},{\"end\":44296,\"start\":44284},{\"end\":44598,\"start\":44584},{\"end\":44609,\"start\":44598},{\"end\":44624,\"start\":44609},{\"end\":44636,\"start\":44624},{\"end\":44648,\"start\":44636},{\"end\":44664,\"start\":44648},{\"end\":44956,\"start\":44942},{\"end\":44967,\"start\":44956},{\"end\":44982,\"start\":44967},{\"end\":44994,\"start\":44982},{\"end\":45010,\"start\":44994},{\"end\":45281,\"start\":45272},{\"end\":45296,\"start\":45281},{\"end\":45311,\"start\":45296},{\"end\":45323,\"start\":45311},{\"end\":45623,\"start\":45611},{\"end\":45637,\"start\":45623},{\"end\":45646,\"start\":45637},{\"end\":45657,\"start\":45646},{\"end\":45669,\"start\":45657}]", "bib_venue": "[{\"end\":32105,\"start\":32047},{\"end\":32413,\"start\":32400},{\"end\":32707,\"start\":32702},{\"end\":33037,\"start\":33024},{\"end\":33391,\"start\":33383},{\"end\":33753,\"start\":33743},{\"end\":34060,\"start\":34047},{\"end\":34339,\"start\":34331},{\"end\":34597,\"start\":34579},{\"end\":34840,\"start\":34832},{\"end\":35093,\"start\":35085},{\"end\":35436,\"start\":35426},{\"end\":35776,\"start\":35768},{\"end\":36143,\"start\":36133},{\"end\":36475,\"start\":36465},{\"end\":36786,\"start\":36778},{\"end\":37128,\"start\":37122},{\"end\":37450,\"start\":37442},{\"end\":37782,\"start\":37769},{\"end\":38104,\"start\":37996},{\"end\":38504,\"start\":38496},{\"end\":38877,\"start\":38859},{\"end\":39249,\"start\":39241},{\"end\":39636,\"start\":39607},{\"end\":39989,\"start\":39979},{\"end\":40255,\"start\":40158},{\"end\":40599,\"start\":40566},{\"end\":40966,\"start\":40937},{\"end\":41326,\"start\":41320},{\"end\":41621,\"start\":41602},{\"end\":41942,\"start\":41909},{\"end\":42323,\"start\":42317},{\"end\":42574,\"start\":42520},{\"end\":42828,\"start\":42820},{\"end\":43131,\"start\":43121},{\"end\":43365,\"start\":43352},{\"end\":43681,\"start\":43663},{\"end\":44000,\"start\":43986},{\"end\":44306,\"start\":44296},{\"end\":44582,\"start\":44511},{\"end\":45027,\"start\":45010},{\"end\":45336,\"start\":45323},{\"end\":45689,\"start\":45669}]"}}}, "year": 2023, "month": 12, "day": 17}