{"id": 254927543, "updated": "2023-12-14 08:44:58.393", "metadata": {"title": "ScaleHD: Robust Brain-Inspired Hyperdimensional Computing via Adapative Scaling", "authors": "[{\"first\":\"Sizhe\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Xun\",\"last\":\"Jiao\",\"middle\":[]}]", "venue": "2022 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "journal": "Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Brain-inspired hyperdimensional computing (HDC) has demonstrated promising capability in various cognition tasks such as robotics, bio-medical signal analysis, and natural language processing. Compared to deep neural networks, HDC models show advantages such as light-weight model and one/few-shot learning capabilities, making it a promising alternative paradigm to traditional resource-demanding deep learning models particularly in edge devices with limited resources. Despite the growing popularity of HDC, the robustness of HDC models and the approaches to enhance HDC robustness has not been systematically analyzed and sufficiently examined. HDC relies on high-dimensional numerical vectors referred to as hypervectors (HV) to perform cognition tasks and the values inside the HVs are critical to the robustness of an HDC model. We propose ScaleHD, an adaptive scaling method that scales the value of HVs in the associative memory of an HDC model to enhance the robustness of HDC models. We propose three different modes of ScaleHD including Global-ScaleHD, Class-ScaleHD, and (Class + Clip)-ScaleHD which are based on different adaptive scaling strategies. Results show that ScaleHD is able to enhance HDC robustness against memory errors up to 10,000X. Moreover, we leverage the enhanced HDC robustness in exchange for energy saving via voltage scaling method. Experimental results show that ScaleHD can reduce energy consumption on HDC memory system up to 72.2% with less than 1% accuracy loss.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccad/ZhangIJ22", "doi": "10.1145/3508352.3549376"}}, "content": {"source": {"pdf_hash": "8523bd6a8a086659b7b24599a9605d80c974a01e", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "9a427d3bcf7635d25d821a8dd749a6c5facd6ffa", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/8523bd6a8a086659b7b24599a9605d80c974a01e.txt", "contents": "\nScaleHD: Robust Brain-Inspired Hyperdimensional Computing via Adapative Scaling\n\n\nSizhe Zhang \nMohsen Imani \nXun Jiao \n\nVillanova University\nIrvine\n\n\nVillanova University\n\n\nScaleHD: Robust Brain-Inspired Hyperdimensional Computing via Adapative Scaling\n10.1145/3508352.3549376\nBrain-inspired hyperdimensional computing (HDC) has demonstrated promising capability in various cognition tasks such as robotics, bio-medical signal analysis, and natural language processing. Compared to deep neural networks, HDC models show advantages such as light-weight model and one/few-shot learning capabilities, making it a promising alternative paradigm to traditional resource-demanding deep learning models particularly in edge devices with limited resources. Despite the growing popularity of HDC, the robustness of HDC models and the approaches to enhance HDC robustness has not been systematically analyzed and sufficiently examined. HDC relies on high-dimensional numerical vectors referred to as hypervectors (HV) to perform cognition tasks and the values inside the HVs are critical to the robustness of an HDC model. We propose ScaleHD, an adaptive scaling method that scales the value of HVs in the associative memory of an HDC model to enhance the robustness of HDC models. We propose three different modes of ScaleHD including Global-ScaleHD, Class-ScaleHD, and (Class + Clip)-ScaleHD which are based on different adaptive scaling strategies. Results show that ScaleHD is able to enhance HDC robustness against memory errors up to 10, 000 . Moreover, we leverage the enhanced HDC robustness in exchange for energy saving via voltage scaling method. Experimental results show that ScaleHD can reduce energy consumption on HDC memory system up to 72.2% with less than 1% accuracy loss.\n\nINTRODUCTION\n\nThe developments in artificial intelligence (AI) and machine learning (ML), such as deep neural networks (DNNs), promise enormous societal and economic benefits. However, their deployment on hardware faces daunting difficulties due to their extremely-demanding and increasing computation requirements which may not always be satisfied by resource-constrained platforms. Recently, inspired by the way brain works with cognition tasks, especially the very large brain circuit size, hyperdimensional computing (HDC) has been introduced as an alternative computational model [7,8,17]. Compared to DNNs, HDC has shown several key advantages including more compact model, lower computation requirements, and one/few-shot * Xun Jiao is the corresponding author: xun.jiao@villanova.edu Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICCAD ' learning capability, making it a promising AI paradigm in resourceconstrained platforms such as mobile and wireless devices [2,24]. Such versatility has led to its success in various application domains including robotics [16], natural language process [17,20], drug discovery [14], and multimedia processing [5,15].\n\nAs the transistor size scales down to deep nanometer era, microelectronic circuits are becoming increasingly susceptible to microelectronic variability [9], such as variations in manufacturing, operating conditions (e.g., voltage droops and temperature fluctuations), and aging. Such variations can cause delay uncertainty which can prevent circuits from meeting their timing specification, thus resulting in timing errors. Timing errors typically appear as faults (bit flips) in the high-level operations, which can cause incorrect computing results. Furthermore, the radiation-induced soft error rate per bit is also estimated to increase 8% with each technology scaling by semiconductor vendors [3]. Such variations pose imminent threat to the correctness and quality of results delivered by computing systems, which typically requires a near-perfect execution with extremely low error rates to guarantee correctness.\n\nWhile AI/ML methods are generally more error-robust than conventional computing and the quality of model output results is relatively insensitive to the rising error rates, recent study shows that the inference accuracy of DNN models can still drop significantly when the error rates are too high, such as 0.001% [13,21].\n\nSimilarly, while HDC is claimed to be robust to hardware errors due to its holographic distributed representation with information equally distributed into high-dimensional vectors, recent studies show that HDC also experiences a significant accuracy drop with error rate reaches 0.001% [23]. Considering the increasing reliability threat posed by microelectronic variations, especially with the burgeoning use of microelectronic devices in mobile and wireless applications that can constantly experience variations in operating conditions (e.g., supply voltage droops and temperature fluctuations), enhancing the HDC robustness to hardware errors are of utmost importance.\n\nTo address this problem, we present ScaleHD in this paper, which aims to improve the robustness of HDC models to bit flip errors, which is one typical error caused by microelectronic variations and/or soft errors. ScaleHD leverages the distribution characteristics of the values in the HDC model to perform adaptive scaling, specifically, on the class hypervectors to obtain enhanced robustness. The enhanced robustness further open the doors for opportunistic reduction of design margin (e.g., voltage) to increase the operational efficiency of hardware in HDC systems.\n\nOur contributions are as follows:\n\n\u2022 We analyze the distribution of values of the class HVs in HDC models across multiple applications, based on which we propose ScaleHD. ScaleHD is an adaptive scaling method that can re-distribute the values in class HVs in HDC to improve the robustness against bit flip errors while incurring little to no impact on HDC inference accuracy. \u2022 We propose three different modes of ScaleHD, including Global-ScaleHD, Class-ScaleHD, and (Class + Clip)-ScaleHD, each focusing on different levels of scaling to achieve different level of performance. Experimental results across various applications and HDC configurations show that ScaleHD can significantly enhance HDC robustness to memory errors up to 10, 000 . \u2022 Using ScaleHD, we are able to enable higher energy efficiency. Specifically, we propose to use voltage scaling to reduce the memory energy. While voltage scaling can lead to memory errors, the enhanced robustness with ScaleHD can lead up to 72.2% energy saving on HDC system in 22nm SRAM architecture.\n\n\nRELATED WORKS\n\nAI/ML robustness to hardware errors have been intensively studied in the past, especially in the DNN accelerators context [11,18,19]. DNN robustness to timing errors caused by voltage and temperature variations has been first studied in [6], which shows that DNNs can experience significant accuracy drop for error rate as low as 0.001%, which is consistent with [21]. A set of DNN error injection tools are then proposed to assess the robustness to DNNs including Ares [18] and TensorFi [12]. A common observation of all these studies is that DNN is more robust to hardware errors to a certain degree depending on data types, values, data reuses, and types of layers in the design. Brain-inspired HDC has been proposed as a light-weight novel computing paradigm that has shown promising performance in robotics [16], natural language process [17,20], drug discovery [14], and multimedia processing [5,15]. For example, HDC outperforms state-of-the-art DNN models (e.g., graph/recurrent neural networks) in drug discovery in terms of accuracy with a significantly reduced computing costs [14]. While HDC is claimed to be robust to hardware errors [4,17], recent studies show that, like DNNs, HDC can also experience sharp accuracy drop with low error rates (e.g., 0.001%) [23]. Currently, most HDC studies rely on inherent error tolerance of existing HDC algorithms [4,17]. This unfortunately may not be the best way to protect HDC from hardware errors especially considering the increasing threat from microelectronic variations and environmental uncertainty and. Extra hardware circuitry like Razor [23] may mitigate the impact of bit flip errors but cost extra energy and chip area [22].\n\nConsidering the algorithmic difference between DNNs and HDC, existing algorithm-level DNNs error enhancement techniques cannot be directly applied to HDC algorithm. In this paper, we propose a general algorithmic-level method that does not require any hardware modification.\n\n\nHDC BACKGROUND\n\nIn this section, we describe the HDC model base element, operations and operating processes including the Encoding, Training, Retraining, and Inference. The flow shows in Fig. 1 \n\nEncoding HDC's framework begins with encoding. In HDC, samples are mapped from original data to HVs. Both training and inference are performed using the HVs. The encoding map raw features to HVs. For our HDC model, we employ the most common record-based encoding [2]. In the beginning, it creates position hypervectors( \u00ec ) and level hypervectors( \u00ec ). They are all bipolar{\u22121, 1} and have the same dimension. Each position HV is generated randomly and orthogonally to each other. Every level HV correlates with the nearby level HVs. The subsequent level HVs are generated by modifying certain number of bipolar elements from nearby level HV. Each feature in a sample will have its feature HV( \u00ec ) generated by multiplying the corresponding level HV and position HV. The sample HV( \u00ec ) will be generated by adding the feature HVs( \u00ec ) in total. The encoding is based on the equation\nEqn. 2. \u00ec = \u00ec * \u00ec \u00ec = \u00ec 1 + \u00ec 2 . . . \u00ec(2)\nTraining The encoding stage encodes each input sample into a sample HV. The training stage simply add all the sample HVs ( \u00ec ) of the same class together to create a class HV( \u00ec ) representing that specific class, as shown in Eq. 3. This is an element-by-element accumulation. HDC framework stores all class HVs in the associative \nmemory. \u00ec = \u00ec (3)\nRetraining Furthermore, HDC retraining, other than the singlepass method, is optional, but can significantly increase the model accuracy through several epochs of retraining. In this first stage, the training data will be tested. Upon incorrect prediction, the query HV( \u00ec ) will be added to the correct class HV( \u00ec ) and subtracted from the wrong predicted class HV( \u00ec ). In this manner, the class HV can be enhanced from samples, and other class HVs that may be distorted will be reduced in relation to them. The following Eq. 4 summarizes the process. Moreover, the learning rate can be an adaptive learning rate so as to enhance retraining.\n\u00ec = \u00ec \u2212 \u00ec \u00ec = \u00ec + \u00ec (4)\nInference The inference stage classifies unseen samples using the pre-trained class HVs in the associative memory. First, same as the training stage, testing samples are encoded into sample HVs (referred to as \"query HV\"). Second, the HDC model measures the similarity between the query HV( \u00ec ) and every class HV( \u00ec ) in the associative memory. The most typical similarity metric is cosine similarity as Eq. 6 [17]. Eventually, the class( ) with the highest similarity with the query HV will be used as the inference result of the sample as shown in Eq. 5.\n= ({cos( \u00ec , \u00ec 1 ), cos( \u00ec , \u00ec 2 ), . . . , cos( \u00ec , \u00ec )})(5)\n\nCLASS HV VALUE DISTRIBUTION ANALYSIS\n\nIn this section, we present a key observation by profiling and analyzing the value distribution of class HVs in HDC models, which inspires the development of ScaleHD. After training HDC models on different applications, we profile the values of class HVs in HDC models and examine their value distribution. We find that for different applications, class HVs would have different value ranges and distributions, as shown in Table 1. Because each application has a different amount of samples, and different HDC framework use different encoding method and training strategy, we need to use different bit-width to implement the each element in class HV for different HDC system to completely store these models. For example, in our implementation, the value range is (-5266, 6340) for CARDIO after training, which means 16 bits (i.e., int16) is enough to store the model. While for HAR with a value range of (-155595, 104273), we need 32 bits (i.e., int32) to store the value. Now we take a deeper look into HAR's class HVs values by profiling the value distribution, as shown in Fig. 2. We use two examples -\"class 6\" HV and \"class 7\" HV. As shown in Fig. 2, we can find the range and distribution of values for \"class 6\" HV and \"class 7\" HV are notably different. This is caused by HAR's unbalanced dataset. Class 6 HV value has a wider range because of more samples comparing to class 7. Class 7 HV values are largely clustered around 0 due to less training samples. From the Fig. 2 (a) and Table. 1, we realize the value range of this associative memory is larger than int16 (-32,768 to +32,767), but much less than int32 (-2,147,483,648 to +2,147,483,647), which means there are \"empty\" bits (i.e., \"0\" bits) in each value in the most significant positions.\n\nNote that different value will have different relative error for a bit flip. For example, for a 4-bit unsigned system, if the original data is 0010 (i.e., 2 10 ) and there is a bit flip on the least significant bit (LSB), the relative error is 1/2 = 50%. However, if the original data is 1110 (i.e., 14 10 ), the relative error of the same bit flip is only 1/14 = 7%. This inspires us to consider the following question: can we scale up values in HDC model without affecting the HDC classification result? If so, we can reduce the relative effects of bit flips and hence enhance the HDC robustness through fully using these extra \"space\" comes with fixed data-width to reduce the relative error. This inspired us to develop ScaleHD.\n\n\nAPPROACH OF SCALEHD\n\nIn this section, we first provide theoretical foundations for ScaleHD. Then, we present the detailed process of ScaleHD, along with three different modes of ScaleHD: Global-ScaleHD, Class-ScaleHD, and (Class + Clip)-ScaleHD. Finally, we use special metric briefly evaluate the robustness enhancements made by ScaleHD.\n\n\nMathematical Proof of ScaleHD\n\nBefore we introduce ScaleHD, we provide a theoretical foundation for ScaleHD by demonstrating the equivalence of HDC functionality before and after ScaleHD. As we described in the section on HDC background, HDC uses HV as its basic component. Moreover, the HDC uses the cosine similarity between two vectors as the main part of the inference process. By using cosine similarity, we can compare the direction between two vectors, and linearly scaling up/down each value in the class HV with the same ratio will not impact the direction of vectors. Therefore, the results of the cosine similarity check will not change, resulting in the same inference result. A mathematical proof of this can also be provided by Eq. 6. Due to this fact, scaling the same ratio to each value in a class HV will not affect the inferences of the HDC system. \ncos( \u00ec , \u00ec ) = \u00b7 \u00ec \u00b7 \u00ec || \u00ec \u00b7 || \u00d7 || \u00ec || = =1 \u00b7 \u210e \u00b7 \u210e =1 2 \u00b7 \u210e 2 \u00b7 =1 \u210e 2 = =1 \u210e \u00b7 \u210e =1 \u210e 2 \u00b7 =1 \u210e 2 = cos( \u00ec , \u00ec )(6)\n\nMechanisms and Implementation of ScaleHD\n\nScaleHD is an set of methods can enhance the HDC robustness though value scaling. Essentially, it is a post-processing step for the HDC model (associative memory) after training and before inference. The process is specific to its data type, so it can be applied to every integer representation HDC model with a wide range of settings and applications. Furthermore, the process does not require any additional hardware as it is a software-based modification.\n\nThere are limited overhead costs, and the process is almost free of charge. Following that, we will discuss the details of each ScaleHD. Global-ScaleHD : The first mode of ScaleHD is Global-ScaleHD. It basically scales up all values in class HVs with the same ratio. Specifically, we first let \u2212 \u2192 = [ 1 , 2 , . . . , ] be a dimensional class HV, and A = { 1 , 2 , . . . , } be an associative memory storing class HVs. represent the data type the system use. In this paper, we choice common used INT32, INT16 and INT8 as our data type. Second, we find the maximum absolute value V a in the A. We also define the maximum value that can be represented by the current bit width as V n . For example, for a 32-bit int32 value, the maximum value V n is 2,147,483,647. Then, we compute the ratio between V n and V a . Last, by scaling up every value in A by times, all values in A get scaled up to A s without overflow and precision loss. This process can be summarized as Algorithm 1. Scaled value will have less relatively error comparing to the original value when same bit flips occur. Fig. 3 shows a example how scale can reduce relatively error against bit flip. The original value is 5, after bit flip, it become to 17. The relative error is |17 \u2212 5|/5 = 240%. After 16 times scale up, the value become 80, With the same random bit flip, the relative error is |68 \u2212 80|/80 = 15% which is much smaller. Fig. 4 shows the value distribution of associative memory after Global-ScaleHD. We can find though the distribution pattern seems the same, the x-axis is 10000X larger after Global-ScaleHD. This will enhance the robustness of HDC.\n= A s = \u00d7(7)\nAlgorithm 1 Global-ScaleHD \n\u2190 ( ( ),( )) 8:\n\u2190 ( ) # Find the max value of this data type 9: # Compute scaling ratio 10: \u2190 / 11: # Scale and save each value in the associate memory to the scaled associate memory 12: \u2190 * 13: return Class-ScaleHD : Next, we present the second mode Class-ScaleHD. This is inspired by our observation that even for the same HDC model, different class HVs would have different value distributions. Thus, instead of scaling up class HVs using a global ratio, we individually scale up each class HV as shown in Fig. 5. Note that according to Section 5.1, the class-specific scaling up will also not change the classification results. We present the following steps to implement Class-ScaleHD, which is also illustrated in Algorithm 2. First, we find the maximum absolute value V c for each class HV H c . Then, we compute a ratio between V n (the maximum value that can be represented by ) and V c . Third, we scale up this class HV by . This process can be simplified as Eq. 8. We repeat these steps for each class HV so that we can get an updated associative memory. Fig. 6 shows the value distribution of associative memory after Class-ScaleHD. Compare to Fig. 4, we can find the Class-ScaleHD is effective especially for unbalanced data. Because of HDC's special training strategy, the class with more samples usually lead to a larger absolute value of class HV. Global-ScaleHD can not handle this situation. The different distribution of \"class 6\" HV and \"class 7\" HV validates this idea. Because the value range of each class HV is likely to be different from the whole associative memory value range, this method can fully scale up with balanced data trained associative memory which will further enhance the HDC robustness.\n= H s = \u00d7 A s = { 1 , 2 , . . . , }(8)\n(Class + Clip)-ScaleHD : At the end, we present the third mode of ScaleHD -(Class + Clip)-ScaleHD. The key idea is to cut/clip some extreme values in the class HV value distribution so that we are able to scale up even more, hence providing more robustness to HDC model. We propose the following steps. First, we use Class-ScaleHD to the model to fully scale up the value. Second, we set an absolute clip value C, and thus we have two clip points at \"+C\" and \"-C\". Third, (Class + Clip)-ScaleHD will clip every value if it is larger/smaller than the clipping values to reduce the range of values in class HVs. The principle of clip is shown in Eq. 9. Fig. 7 shows the value distribution of associative memory after clipping, and we can find some value has been clipped on both ends of the distribution. After the clipping, we perform the Global-ScaleHD like scaling based on Eq. 7. The value distribution further \"fill up\" the possible range that can be represented by current bit width after clip and scale. This detail of the procedure shows in Algorithm 3. \n\n\nFigure 7: Data distribution histogram in associative memory with (Class + Clip)-ScaleHD\n\nPlease note that because clipping has changed certain values in the class HVs, the class HV will be affected. As a result, the classification results and the inference results will be impacted. If we want more scaling space to provide more robustness, the absolute clip value should smaller. But with a smaller absolute clip value means more values will get clipped and the accuracy may drop more. To explore the trade-off between clipping values and HDC inference accuracy, we performed experiment for clipping value v.s. accuracy in 32-bit to find a balance between robustness and accuracy. The result is shown in Fig. 8. Through a careful examination, we choose a absolute clip value for each application so that the inference accuracy loss is less than 1%. Next, We calculate the ratio between the selected clip value and 32-bit data range as the clip ratio for each application. We use this ratio for other data-width (Class+Clip)-ScaleHD experiment. The clip ratio(R) and absolute clip value(C) (after Class-ScaleHD) for int32 we choose in our experiment are in Table. 1.\n[ ] = \u23a7 \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23a9 + , [ ] > + [ ], \u2212 \u2264 [ ] \u2264 + \u2212 , [ ] < \u2212(9)\n\nRobustness Enhancement Evaluation\n\nAs an initial step in assessing the ability of the ScaleHD to enhance HDC's robustness before exam with testing dataset, we explore an evaluation metric specially for HDC to evaluation the ScaleHD enhancement. Given that HDC uses cosine similarity for inference, we use the cosine similarity between error inject model and original model as the evaluation metric to check how error influenced HDC model get protected by ScaleHD. In general, the higher the cosine similarity, the more likely the error inject model will be  if H > then 20: \u2190 # Clip positive extreme values 21: end if 22: if H < -then close to the original model, which further demonstrates how robust the model is. We run a short test on four applications, each with a different error rate, through 32bit settings. The results of Fig. 9 indicate that, although HDC is robust by itself, ScaleHD is superior at overcoming random bit-flip errors. Furthermore, Class-ScaleHD has performed significantly better than Global-ScaleHD, while (Class+Clip)-ScaleHD has demonstrated the strongest performance in this evaluation metric. \n\u2190 ( ( ),( )) 10\n\nEXPERIMENTAL RESULTS\n\nTo fully explore the effectiveness of ScaleHD, we test HDC with ScaleHD on four different applications with three different datawidth. Datasets including speech recognition(ISOLET 1 ), human activities (HAR 2 ), medical diagnosis(CARDIO 3 ) and image Classification(MNIST [10]). In our model development, we implement a basic approach of HDC model without special encoding method and training strategy which means our model accuracy might not meet state of art HDC model accuracy. But experiment still effectively show the effectiveness robustness enhancement of ScaleHD to general HDC model. Hardware errors are injected to the HDC models as random bit flips with a pre-defined error rate, similar to existing studies [11,18,19,22]. This error model randomly flips a single bit upon the occurrence of an error in memory. We inject errors to HDC's associative memory with aimed error rates and error models during the inference stage. We apply ScaleHD to the HDC's associative memory after the training stage and before the inference stage.\n\n\nRobustness Enhancement by ScaleHD\n\nWe use three different HDC configurations in terms of three different data width: 32-bit, 16-bit, and 8-bit across four different applications. As shown in Table. 1, the value range of class HVs is different across different applicaiton. We perform Global-ScaleHD as the baseline quantization method for our 16/8-bit experiment for HAR and MNIST, and 8-bit experiment for ISOLET and CARDIO in order to reduce the precision loss caused by fixed ratio quantization. The different HDC configurations and their corresponding (error-free) accuracy are presented in Table 2. Note that quantizing to 8-bit data width incur negligible accuracy loss. Nevertheless, we focus on the relative accuracy drop after injecting errors and relative robustness enhancement.\n\nFor each configuration, we sweep 90 error rates equally space on a logarithmic scale from 10 \u221210 to 1, e.g., 8 * 10 \u221210 , 9 * 10 \u221210 , 1 * 10 \u22129 etc. At each error rate, we repeat our experiment 10 times and report the mean accuracy. The overall experimental results are shown in Fig. 10, based on which we present several key observations. First, by applying ScaleHD, HDC robustness is significantly enhanced across all application datasets. For example, in ISOLET 32-bit ( Fig. 10 (a)), the baseline classifier starts to see accuracy drop at error rates 10 \u22127 , while after applying (Class + Clip)-ScaleHD, it can push error rates to 10 \u22123 without accuracy loss. This is 10, 000 Second, among several modes of ScaleHD, we can consistently see that (Class + Clip)-ScaleHD outperforms Class-ScaleHD, which outperforms Global-ScaleHD. Using 32-bit MNIST (Fig. 10 (d)) as our example, we can see that (Class + Clip)-ScaleHD can push the error rates to around 10 \u22123 without incurring accuracy loss, while for Global-ScaleHD and Class-ScaleHD, the corresponding metrics are around 10 \u22125 and 10 \u22124 . This suggests that, for this application, (Class + Clip)-ScaleHD can improve robustness 10 and 100 more than Class-ScaleHD and Global-ScaleHD, respectively. Similar phenomenon can be observed for all the application datasets and configurations across Fig. 10 (a) -(l).\n\nFor certain scenarios, as we mentioned, we use Global-ScaleHD as model quantization method for the baseline. For example, in 8-bit ISOLET ( Fig. 10 (i)), the value range of class HVs are already beyond the range that can be represented by 8 bits. That is, all the bit positions are \"full\", and there are no \"empty\" bits for scaling. Actually, in this case, we need to down-scale the HV values to accommodate each value within 8 bits. Therefore, Global-ScaleHD can be seen as the baseline and quantization method in this case as we mention in the experiment setting. Regardless, by applying Class-ScaleHD and (Class + Clip) -ScaleHD, we are still able to improve HDC robustness around 100X.\n\nAnother interesting observation is in HAR 8-bit (Fig. 10 (j)) experiment. The accuracy of the model is higher with Class-ScaleHD (93.61%) and (Class+Clip)-ScaleHD (93.64%) compared to the baseline (91.71%). The situation is unusual as we proved that the error-free model with Global scale and Class scale will not affect the model inference accuracy, while the (Class+Clip)-ScaleHD may only reduce less Consequently, the Class-ScaleHD and (Class+Clip)-ScaleHD are also effective in recovering the precision loss from the quantization in some circumstances.\n\nLast but not least, if we compare Fig. 10 (a)(b)(c) with Fig. 9, we find that the two plots follow similar trends. This shows despite only comparing cosine similarity between the original model and the error inject model, it is still able to predict how errors affect the HDC model's accuracy without putting the dataset under test. Therefore, these results show our evaluation metric is a reliable tool for evaluating HDC model robustness.\n\n\nEnergy Saving by ScaleHD\n\nRecent research indicates that HDC associative memory can save up to 72.5% energy with an additional razor circuit to prevent errors [22]. To see how the robustness enhancement from ScaleHD can benefit HDC system, we performed simulations based on the relationship between voltage scaling and bit error rates, as depicted in Eq. 10 obtained from a real SRAM measurement fabricated in FDX22 (22nm) technology [1]. We map the tolerable error rates to the equation to obtain the corresponding voltage scaling budget. Then, using = 2 , we can obtain the corresponding energy saving with less than 1% accuracy loss. This refers to [22]'s setting. Additionally, we carried out our simulation on each of the four applications, using a variety of data types. Furthermore, we examined the average energy savings across the four applications using the same settings. The details of energy savings can be found in Table 3. As for 16-bit ISOLET, CARDIO, and all 8-bit experiments, we employ Global-ScaleHD as the quantization method. We put this part of results both in the baseline and Global-ScaleHD section in Table 3.\n\nNote that since HDC has inherent error tolerance, even the baseline HDC without quantization can save around 50% energy with negligible accuracy loss(<1%). Additionally, by using the Global-ScaleHD, the Class-ScaleHD, and the (Class + Clip)-ScaleHD, the average energy savings will rise to 62%, 65%, and 69%, respectively. Some settings within ISOLET and MNIST even able to save more than 70% of energy. When compared to the results found in [22] (Up to 72.5% energy saving), the memory has the capability of achieving the same level of energy savings with the ScaleHD applied. Nonetheless, existing work [22] requires extra protection circuit for the memory, which costs more space ,energy and require customized hardware, but ScaleHD is purely a software approach with no modification to the hardware and no overhead except a onetime post-processing procedure which takes place after training and prior to inference. Based upon our analysis, we believe our proposal is superior to the existing work.\n\n= 2e+08 * \u221261.69 * (10)\n\n\nCONCLUSION\n\nIn this paper, we propose ScaleHD, an adaptive scaling method that scales the value of HV in associative memory to enhance the robustness of HDC models. We propose three different modes of ScaleHD including Global-ScaleHD, Class-ScaleHD, and (Class + Clip)-ScaleHD. We evaluate ScaleHD by performing error injection experiments with a wide range of error rates, datasets, HDC configurations. Results show that ScaleHD is able to enhance HDC robustness against memory errors up to 10000 . Moreover, we leverage the enhanced HDC robustness in exchange for energysaving via the voltage scaling method. Experimental results show that ScaleHD can reduce energy consumption on HDC memory system up to 72.2% with less than 1% accuracy loss.\n\nFigure 2 :\n2Original data distribution histogram in associative memory\n\nFigure 3 :Figure 4 :\n34Mechanism of scaling to enhance robustness Data distribution histogram in associative memory with Global-ScaleHD\n\nFigure 5 :Figure 6 :\n56ClassParameters: Associate Memory ; Scaled Associate Memory ; Target data-width . 2: # Allocate memory for scaled associate memory according to original associate memory and targetData distribution histogram in associative memory with Class-ScaleHD\n\nFigure 8 :\n8Absolute clip values influence accuracy across different applications (32-bit with (Class + Clip) -ScaleHD).\n\nFigure 9 :Figure 10 :\n910Evaluation ScaleHD with cosine similarity check ScaleHD effects on HDC Robustness robustness improvement. Similarly, for all the other 32-bit HDC classifiers (HAR, CARDIO, MNIST), i.e.,Fig. 10 (b) -(d), the robustness are also significantly improved with 1000 -10000 .\n\n\n22, October 30-November 3, 2022, San Diego, CA, USA \u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9217-4/22/10. . . $15.00 https://doi.org/10.1145/3508352.3549376\n\n\ncan help understand the basic framework of HDC.ScaleHD \n\nTraining \n\nInference \n\nError \nInjection \n\nOriginal Data \n10110 \u00b7\u00b7\u00b7 01010 \n\n10100 \u00b7\u00b7\u00b7 01110 \n\nTraining \nHV \n\nTraining \nData \nClass HV \n\nEncoding \n\nTraining \n\nTesting \nHV \n\nTesting \nData \n\nEncoding \n\nSimilarity \nCheck \n\nClass HV \n\nClass HV \n\nAssociative \nMemory \n\n\u2026 \n\nRetraining \n\nFigure 1: HDC with ScaleHD framework diagram \n\nBasic Component and Operation HDC uses a high-dimensional \n(e.g., = 10000) pseudo-random and holographic vector( \u00ec ) called \na hypervector(HV) as the basic element shown in Eq. 1. HV values \ncan be integers, binary or bipolar. In HDC, HVs serve as a data \nblock to represent information depending on the application, such \nas language characters, audio signal amplitudes, and image pixel \nvalues. As part of HDC, different HV operations are used to bundle \nand bind different information. Arithmetic operations supported \nby HV are addition, multiplication, and permutation. Additions \nand multiplications use two input HVs to perform element-by-\nelement additions and multiplications. Permutation rotates one HV \ncyclically to generate a new HV. The dimensions of the input and the \noutput HVs stay the same across all the operations. By multiplying \nand permuting, generated HVs are orthogonal to their original, \nwhile adding retains 50% of the information of each original HV. \n\u2212 \n\u2192 = [\u210e 1 , \u210e 2 , . . . , \u210e ] \n\n\nTable 1 :\n1Pre-trained associative memory value range, selected clip ratio and absolute clip value for 32-bit.Dataset Minimum Maximum Clip Ratio(R) Clip Value (C) \n\nCARDIO \n-5266 \n6340 \n0.605 \n1.3e+9 \n\nHAR \n-155595 \n104273 \n0.698 \n1.5e+9 \n\nISOLET \n-22070 \n28294 \n0.652 \n1.4e+9 \n\nMNIST -958416 \n778792 \n0.466 \n1e+9 \n\n(a) Class 6 HV \n(b) Class 7 HV \n\n\n\nTable 2 :\n2Error-free accuracy for different datasets and configuration.Configuration ISOLET HAR CARDIO MNIST \n\n32-bit \n94.42% 93.77% 93.43% 91.04% \n\n16-bit \n94.42% 93.77% 93.43% 91.04% \n\n8-bit \n94.42% 91.71% 92.96% 91.01% \n\n\n\nTable 3 :\n3Energy saving through voltage scaling across different datasets and settings than 1% accuracy in our setting. In spite of the fact that this phenomenon does not manifest itself in other experiments, we believe it remains reasonable. First of all, the 8-bit model is quantized from the 32-bit model, and thereby the precision of the model may suffer in the quantization(integer linear scaling down). As shown in the table, HAR, CARDIO, and MNIST all experience a decline in baseline accuracy with a 8-bit model. Furthermore, the ratio of scaling down may also influence model precision. When the ratio of scaling down is high, greater precision is likely to be lost. (Class+Clip)-ScaleHD may result in a lower precision loss than the Global-since (Class+Clip)-ScaleHD need less quantization scaling down ratio.Baseline \nGlobal ScaleHD \nClass ScaleHD \n(Clip+Class) ScaleHD \n\n32-bit 16-bit \n8-bit \n32-bit 16-bit \n8-bit \n32-bit 16-bit \n8-bit \n32-bit 16-bit \n8-bit \n\nCARDIO 48.39% 48.73% 61.22% 61.49% 59.15% 61.22% 64.20% 63.21% 64.20% 66.82% 66.54% 68.65% \n\nHAR \n47.01% 59.15% 61.49% 59.73% 59.15% 61.49% 63.21% 61.49% 65.43% 65.86% 66.23% 67.06% \n\nISOLET 48.00% 48.00% 65.86% 66.23% 65.86% 65.86% 68.65% 68.65% 68.65% 72.20% 72.20% 71.97% \n\nMNIST 51.02% 61.49% 61.22% 59.73% 61.49% 61.22% 64.20% 64.90% 64.90% 69.57% 70.70% 70.21% \n\nAverage 48.60% 54.34% 62.45% 61.79% 61.41% 62.45% 65.07% 64.56% 65.80% 68.61% 68.92% 69.47% \n\n\nhttp://archive.ics.uci.edu/ml/datasets/ISOLET 2 https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones 3 https://archive.ics.uci.edu/ml/datasets/cardiotocography\nACKNOWLEDGMENTSThe authors would like to thank Ruixuan Wang and Dongning Ma from DETAIL lab at Villanova University for source code related to error injection. This work was partially supported by Villanova Faculty Summer Grant and NSF grant #2028889. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.\nPushing on-chip memories beyond reliability boundaries in micropower machine learning applications. Alfio Di Mauro, Francesco Conti, Pasquale Davide Schiavone, Davide Rossi, Luca Benini, 2019 IEEE International Electron Devices Meeting. IEEEAlfio Di Mauro, Francesco Conti, Pasquale Davide Schiavone, Davide Rossi, and Luca Benini. Pushing on-chip memories beyond reliability boundaries in micropower machine learning applications. In 2019 IEEE International Electron Devices Meeting, pages 30-4. IEEE, 2019.\n\nClassification using hyperdimensional computing: A review. IEEE Circuits and Systems Magazine. Lulu Ge, Lulu Ge et al. Classification using hyperdimensional computing: A review. IEEE Circuits and Systems Magazine, 2020.\n\nNeutron soft error rate measurements in a 90-nm cmos process and scaling trends in sram from 0.25-/spl mu/m to 90-nm generation. Peter Hazucha, Karnik, Maiz, Walstra, Bloechel, G Tschanz, Dermer, Hareland, S Armstrong, Borkar, IEEE International Electron Devices Meeting. IEEEPeter Hazucha, T Karnik, J Maiz, S Walstra, B Bloechel, J Tschanz, G Dermer, S Hareland, P Armstrong, and S Borkar. Neutron soft error rate measurements in a 90-nm cmos process and scaling trends in sram from 0.25-/spl mu/m to 90-nm generation. In IEEE International Electron Devices Meeting 2003, pages 21-5. IEEE, 2003.\n\nOnlinehd: Robust, efficient, and single-pass online learning using hyperdimensional system. Alejandro Hernandez-Cane, Namiko Matsumoto, Eric Ping, Mohsen Imani, 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEAlejandro Hernandez-Cane, Namiko Matsumoto, Eric Ping, and Mohsen Imani. Onlinehd: Robust, efficient, and single-pass online learning using hyperdimen- sional system. In 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE), pages 56-61. IEEE, 2021.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. Mohsen Imani, Deqian Kong, Abbas Rahimi, Tajana Rosing, 2017 IEEE International Conference on Rebooting Computing (ICRC). IEEEMohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. Voicehd: Hyper- dimensional computing for efficient speech recognition. In 2017 IEEE International Conference on Rebooting Computing (ICRC), pages 1-8. IEEE, 2017.\n\nAn assessment of vulnerability of hardware neural networks to dynamic voltage and temperature variations. Xun Jiao, Mulong Luo, Jeng-Hau Lin, Rajesh K Gupta, 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD). IEEEXun Jiao, Mulong Luo, Jeng-Hau Lin, and Rajesh K Gupta. An assessment of vulnerability of hardware neural networks to dynamic voltage and temperature variations. In 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 945-950. IEEE, 2017.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive computation. 12Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive computation, 1(2):139-159, 2009.\n\nIn-memory hyperdimensional computing. Geethan Karunaratne, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abbas Rahimi, Abu Sebastian, Nature Electronics. 36Geethan Karunaratne, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abbas Rahimi, and Abu Sebastian. In-memory hyperdimensional computing. Nature Electronics, 3(6):327-337, 2020.\n\nA 409 gops/w adaptive and resilient domino register file in 22 nm tri-gate cmos featuring in-situ timing margin and error detection for tolerance to within-die variation, voltage droop, temperature and aging. P Jaydeep, Carlos Kulkarni, Paolo A Tokunaga, Trang Aseron, Charles Nguyen, Augustine, W James, Vivek Tschanz, De, IEEE Journal of Solid-State Circuits. 511Jaydeep P Kulkarni, Carlos Tokunaga, Paolo A Aseron, Trang Nguyen, Charles Augustine, James W Tschanz, and Vivek De. A 409 gops/w adaptive and resilient domino register file in 22 nm tri-gate cmos featuring in-situ timing margin and error detection for tolerance to within-die variation, voltage droop, temperature and aging. IEEE Journal of Solid-State Circuits, 51(1):117-129, 2015.\n\nGradient-based learning applied to document recognition. Yann Lecun, Proceedings of the IEEE. the IEEEYann LeCun et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.\n\nUnderstanding error propagation in deep learning neural network (dnn) accelerators and applications. Guanpeng Li, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, Stephen W Keckler, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisGuanpeng Li, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, and Stephen W Keckler. Understanding error prop- agation in deep learning neural network (dnn) accelerators and applications. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-12, 2017.\n\nTensorfi: A configurable fault injector for tensorflow applications. Guanpeng Li, Karthik Pattabiraman, Nathan Debardeleben, 2018 IEEE International symposium on software reliability engineering workshops (ISSREW). IEEEGuanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. Tensorfi: A configurable fault injector for tensorflow applications. In 2018 IEEE International symposium on software reliability engineering workshops (ISSREW), pages 313-320. IEEE, 2018.\n\nVulnerability of hardware neural networks to dynamic operation point variations. Xun Jeng-Hau Lin, Mulong Jiao, Zhuowen Luo, Rajesh K Tu, Gupta, IEEE Design & Test. Jeng-Hau Lin, Xun Jiao, Mulong Luo, Zhuowen Tu, and Rajesh K Gupta. Vulner- ability of hardware neural networks to dynamic operation point variations. IEEE Design & Test, 2020.\n\nMolehd: Automated drug discovery using braininspired hyperdimensional computing. Dongning Ma, Xun Jiao, Dongning Ma and Xun Jiao. Molehd: Automated drug discovery using brain- inspired hyperdimensional computing, 2021.\n\nPerformance analysis of hyperdimensional computing for character recognition. Alec Xavier Manabat, ISMAC. Alec Xavier Manabat et al. Performance analysis of hyperdimensional computing for character recognition. In ISMAC, 2019.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. Anton Mitrokhin, Cornelia Sutor, Yiannis Ferm\u00fcller, Aloimonos, Science Robotics. 430Anton Mitrokhin, P Sutor, Cornelia Ferm\u00fcller, and Yiannis Aloimonos. Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. Science Robotics, 4(30), 2019.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. Abbas Rahimi, Pentti Kanerva, Jan M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignAbbas Rahimi, Pentti Kanerva, and Jan M Rabaey. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design, pages 64-69, 2016.\n\nAres: A framework for quantifying the resilience of deep neural networks. Udit Brandon Reagen, Lillian Gupta, Paul Pentecost, Whatmough, Niamh Sae Kyu Lee, David Mulholland, Gu-Yeon Brooks, Wei, 55th ACM/ESDA/IEEE Design Automation Conference (DAC). IEEEBrandon Reagen, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. Ares: A framework for quantifying the resilience of deep neural networks. In 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC), pages 1-6. IEEE, 2018.\n\nBit error robustness for energy-efficient dnn accelerators. David Stutz, Nandhini Chandramoorthy, Matthias Hein, Bernt Schiele, Proceedings of Machine Learning and Systems. Machine Learning and Systems32021David Stutz, Nandhini Chandramoorthy, Matthias Hein, and Bernt Schiele. Bit error robustness for energy-efficient dnn accelerators. Proceedings of Machine Learning and Systems, 3, 2021.\n\nSpamhd: Efficient text spam detection using brain-inspired hyperdimensional computing. Rahul Thapa, Bikal Lamichhane, Dongning Ma, Xun Jiao, IEEE Computer Society Annual Symposium on VLSI (ISVLSI). 2021Rahul Thapa, Bikal Lamichhane, Dongning Ma, and Xun Jiao. Spamhd: Efficient text spam detection using brain-inspired hyperdimensional computing. In IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2021.\n\nThundervolt: enabling aggressive voltage underscaling and timing error resilience for energy efficient deep learning accelerators. Jeff Zhang, Kartheek Rangineni, Zahra Ghodsi, Siddharth Garg, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceJeff Zhang, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. Thundervolt: enabling aggressive voltage underscaling and timing error resilience for energy efficient deep learning accelerators. In Proceedings of the 55th Annual Design Automation Conference, pages 1-6, 2018.\n\nEnergy-efficient brain-inspired hyperdimensional computing using voltage scaling. Sizhe Zhang, Ruixuan Wang, Dongning Ma, Jeff Zhang, Xunzhao Yin, Xun Jiao, 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEESizhe Zhang, Ruixuan Wang, Dongning Ma, Jeff Zhang, Xunzhao Yin, and Xun Jiao. Energy-efficient brain-inspired hyperdimensional computing using voltage scaling. In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2022.\n\nAssessing robustness of hyperdimensional computing against errors in associative memory. Sizhe Zhang, Ruixuan Wang, Jeff Jun Zhang, Abbas Rahimi, Xun Jiao, 2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP). IEEESizhe Zhang, Ruixuan Wang, Jeff Jun Zhang, Abbas Rahimi, and Xun Jiao. As- sessing robustness of hyperdimensional computing against errors in associative memory. In 2021 IEEE 32nd International Conference on Application-specific Sys- tems, Architectures and Processors (ASAP), pages 211-217. IEEE, 2021.\n\nScalable edge-based hyperdimensional learning system with brain-like neural adaptation. Zhuowen Zou, Yeseong Kim, Farhad Imani, Haleh Alimohamadi, Rosario Cammarota, Mohsen Imani, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisZhuowen Zou, Yeseong Kim, Farhad Imani, Haleh Alimohamadi, Rosario Cam- marota, and Mohsen Imani. Scalable edge-based hyperdimensional learning system with brain-like neural adaptation. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15, 2021.\n", "annotations": {"author": "[{\"end\":95,\"start\":83},{\"end\":109,\"start\":96},{\"end\":119,\"start\":110},{\"end\":149,\"start\":120},{\"end\":173,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":89},{\"end\":108,\"start\":103},{\"end\":118,\"start\":114}]", "author_first_name": "[{\"end\":88,\"start\":83},{\"end\":102,\"start\":96},{\"end\":113,\"start\":110}]", "author_affiliation": "[{\"end\":148,\"start\":121},{\"end\":172,\"start\":151}]", "title": "[{\"end\":80,\"start\":1},{\"end\":253,\"start\":174}]", "venue": null, "abstract": "[{\"end\":1783,\"start\":278}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2373,\"start\":2370},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2375,\"start\":2373},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2378,\"start\":2375},{\"end\":3160,\"start\":3159},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3288,\"start\":3285},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3291,\"start\":3288},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3387,\"start\":3383},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3418,\"start\":3414},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3421,\"start\":3418},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3442,\"start\":3438},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3476,\"start\":3473},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3634,\"start\":3631},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4180,\"start\":4177},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4718,\"start\":4714},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4721,\"start\":4718},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5015,\"start\":5011},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7162,\"start\":7158},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7165,\"start\":7162},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7168,\"start\":7165},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7276,\"start\":7273},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7403,\"start\":7399},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7510,\"start\":7506},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7528,\"start\":7524},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7852,\"start\":7848},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7883,\"start\":7879},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7886,\"start\":7883},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7907,\"start\":7903},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7938,\"start\":7935},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7941,\"start\":7938},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8128,\"start\":8124},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8186,\"start\":8183},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8189,\"start\":8186},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8312,\"start\":8308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8405,\"start\":8402},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8408,\"start\":8405},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8641,\"start\":8637},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8725,\"start\":8721},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9467,\"start\":9464},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11561,\"start\":11557},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13871,\"start\":13869},{\"end\":17901,\"start\":17898},{\"end\":17996,\"start\":17993},{\"end\":22451,\"start\":22448},{\"end\":22488,\"start\":22485},{\"end\":22499,\"start\":22496},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23319,\"start\":23315},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23766,\"start\":23762},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23769,\"start\":23766},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23772,\"start\":23769},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23774,\"start\":23772},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28097,\"start\":28093},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28371,\"start\":28368},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28590,\"start\":28586},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29517,\"start\":29513},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29680,\"start\":29676}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30917,\"start\":30846},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31054,\"start\":30918},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31327,\"start\":31055},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31449,\"start\":31328},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31744,\"start\":31450},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31927,\"start\":31745},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33328,\"start\":31928},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33679,\"start\":33329},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33906,\"start\":33680},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35344,\"start\":33907}]", "paragraph": "[{\"end\":3477,\"start\":1799},{\"end\":4399,\"start\":3479},{\"end\":4722,\"start\":4401},{\"end\":5397,\"start\":4724},{\"end\":5969,\"start\":5399},{\"end\":6004,\"start\":5971},{\"end\":7018,\"start\":6006},{\"end\":8726,\"start\":7036},{\"end\":9002,\"start\":8728},{\"end\":9199,\"start\":9021},{\"end\":10083,\"start\":9201},{\"end\":10458,\"start\":10127},{\"end\":11121,\"start\":10477},{\"end\":11703,\"start\":11146},{\"end\":13564,\"start\":11805},{\"end\":14298,\"start\":13566},{\"end\":14639,\"start\":14322},{\"end\":15510,\"start\":14673},{\"end\":16133,\"start\":15675},{\"end\":17768,\"start\":16135},{\"end\":17809,\"start\":17782},{\"end\":19539,\"start\":17826},{\"end\":20639,\"start\":19579},{\"end\":21808,\"start\":20731},{\"end\":23003,\"start\":21913},{\"end\":24083,\"start\":23043},{\"end\":24875,\"start\":24121},{\"end\":26240,\"start\":24877},{\"end\":26931,\"start\":26242},{\"end\":27489,\"start\":26933},{\"end\":27931,\"start\":27491},{\"end\":29069,\"start\":27960},{\"end\":30072,\"start\":29071},{\"end\":30097,\"start\":30074},{\"end\":30845,\"start\":30112}]", "formula": "[{\"attributes\":{\"id\":\"formula_1\"},\"end\":10126,\"start\":10084},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10476,\"start\":10459},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11145,\"start\":11122},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11765,\"start\":11704},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15631,\"start\":15511},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17781,\"start\":17769},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17825,\"start\":17810},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19578,\"start\":19540},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21876,\"start\":21809},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23019,\"start\":23004}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12235,\"start\":12228},{\"end\":13302,\"start\":13296},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21807,\"start\":21799},{\"end\":24283,\"start\":24277},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24688,\"start\":24681},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28870,\"start\":28863},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29068,\"start\":29061}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1797,\"start\":1785},{\"attributes\":{\"n\":\"2\"},\"end\":7034,\"start\":7021},{\"attributes\":{\"n\":\"3\"},\"end\":9019,\"start\":9005},{\"attributes\":{\"n\":\"4\"},\"end\":11803,\"start\":11767},{\"attributes\":{\"n\":\"5\"},\"end\":14320,\"start\":14301},{\"attributes\":{\"n\":\"5.1\"},\"end\":14671,\"start\":14642},{\"attributes\":{\"n\":\"5.2\"},\"end\":15673,\"start\":15633},{\"end\":20729,\"start\":20642},{\"attributes\":{\"n\":\"5.3\"},\"end\":21911,\"start\":21878},{\"attributes\":{\"n\":\"6\"},\"end\":23041,\"start\":23021},{\"attributes\":{\"n\":\"6.1\"},\"end\":24119,\"start\":24086},{\"attributes\":{\"n\":\"6.2\"},\"end\":27958,\"start\":27934},{\"attributes\":{\"n\":\"7\"},\"end\":30110,\"start\":30100},{\"end\":30857,\"start\":30847},{\"end\":30939,\"start\":30919},{\"end\":31076,\"start\":31056},{\"end\":31339,\"start\":31329},{\"end\":31472,\"start\":31451},{\"end\":33339,\"start\":33330},{\"end\":33690,\"start\":33681},{\"end\":33917,\"start\":33908}]", "table": "[{\"end\":33328,\"start\":31977},{\"end\":33679,\"start\":33440},{\"end\":33906,\"start\":33753},{\"end\":35344,\"start\":34728}]", "figure_caption": "[{\"end\":30917,\"start\":30859},{\"end\":31054,\"start\":30942},{\"end\":31327,\"start\":31079},{\"end\":31449,\"start\":31341},{\"end\":31744,\"start\":31476},{\"end\":31927,\"start\":31747},{\"end\":31977,\"start\":31930},{\"end\":33440,\"start\":33341},{\"end\":33753,\"start\":33692},{\"end\":34728,\"start\":33919}]", "figure_ref": "[{\"end\":9198,\"start\":9192},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12888,\"start\":12882},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12960,\"start\":12954},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13291,\"start\":13281},{\"end\":17225,\"start\":17219},{\"end\":17544,\"start\":17538},{\"end\":18325,\"start\":18319},{\"end\":18883,\"start\":18877},{\"end\":18973,\"start\":18967},{\"end\":20236,\"start\":20230},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21353,\"start\":21347},{\"end\":22715,\"start\":22709},{\"end\":25164,\"start\":25157},{\"end\":25363,\"start\":25352},{\"end\":25742,\"start\":25730},{\"end\":26239,\"start\":26223},{\"end\":26393,\"start\":26382},{\"end\":26993,\"start\":26981},{\"end\":27532,\"start\":27525},{\"end\":27554,\"start\":27548}]", "bib_author_first_name": "[{\"end\":36085,\"start\":36077},{\"end\":36102,\"start\":36093},{\"end\":36118,\"start\":36110},{\"end\":36125,\"start\":36119},{\"end\":36143,\"start\":36137},{\"end\":36155,\"start\":36151},{\"end\":36586,\"start\":36582},{\"end\":36842,\"start\":36837},{\"end\":36886,\"start\":36885},{\"end\":36915,\"start\":36914},{\"end\":37408,\"start\":37399},{\"end\":37431,\"start\":37425},{\"end\":37447,\"start\":37443},{\"end\":37460,\"start\":37454},{\"end\":37890,\"start\":37884},{\"end\":37904,\"start\":37898},{\"end\":37916,\"start\":37911},{\"end\":37931,\"start\":37925},{\"end\":38344,\"start\":38341},{\"end\":38357,\"start\":38351},{\"end\":38371,\"start\":38363},{\"end\":38385,\"start\":38377},{\"end\":38867,\"start\":38861},{\"end\":39132,\"start\":39125},{\"end\":39152,\"start\":39146},{\"end\":39155,\"start\":39153},{\"end\":39171,\"start\":39163},{\"end\":39187,\"start\":39183},{\"end\":39201,\"start\":39196},{\"end\":39213,\"start\":39210},{\"end\":39640,\"start\":39639},{\"end\":39656,\"start\":39650},{\"end\":39672,\"start\":39667},{\"end\":39674,\"start\":39673},{\"end\":39690,\"start\":39685},{\"end\":39706,\"start\":39699},{\"end\":39727,\"start\":39726},{\"end\":39740,\"start\":39735},{\"end\":40242,\"start\":40238},{\"end\":40499,\"start\":40491},{\"end\":40521,\"start\":40504},{\"end\":40535,\"start\":40528},{\"end\":40553,\"start\":40546},{\"end\":40567,\"start\":40560},{\"end\":40586,\"start\":40582},{\"end\":40600,\"start\":40593},{\"end\":40602,\"start\":40601},{\"end\":41252,\"start\":41244},{\"end\":41264,\"start\":41257},{\"end\":41285,\"start\":41279},{\"end\":41728,\"start\":41725},{\"end\":41749,\"start\":41743},{\"end\":41763,\"start\":41756},{\"end\":41777,\"start\":41769},{\"end\":42076,\"start\":42068},{\"end\":42084,\"start\":42081},{\"end\":42289,\"start\":42285},{\"end\":42540,\"start\":42535},{\"end\":42560,\"start\":42552},{\"end\":42575,\"start\":42568},{\"end\":42915,\"start\":42910},{\"end\":42930,\"start\":42924},{\"end\":42943,\"start\":42940},{\"end\":42945,\"start\":42944},{\"end\":43431,\"start\":43427},{\"end\":43455,\"start\":43448},{\"end\":43467,\"start\":43463},{\"end\":43495,\"start\":43490},{\"end\":43514,\"start\":43509},{\"end\":43534,\"start\":43527},{\"end\":43958,\"start\":43953},{\"end\":43974,\"start\":43966},{\"end\":43999,\"start\":43991},{\"end\":44011,\"start\":44006},{\"end\":44378,\"start\":44373},{\"end\":44391,\"start\":44386},{\"end\":44412,\"start\":44404},{\"end\":44420,\"start\":44417},{\"end\":44835,\"start\":44831},{\"end\":44851,\"start\":44843},{\"end\":44868,\"start\":44863},{\"end\":44886,\"start\":44877},{\"end\":45364,\"start\":45359},{\"end\":45379,\"start\":45372},{\"end\":45394,\"start\":45386},{\"end\":45403,\"start\":45399},{\"end\":45418,\"start\":45411},{\"end\":45427,\"start\":45424},{\"end\":45855,\"start\":45850},{\"end\":45870,\"start\":45863},{\"end\":45881,\"start\":45877},{\"end\":45885,\"start\":45882},{\"end\":45898,\"start\":45893},{\"end\":45910,\"start\":45907},{\"end\":46431,\"start\":46424},{\"end\":46444,\"start\":46437},{\"end\":46456,\"start\":46450},{\"end\":46469,\"start\":46464},{\"end\":46490,\"start\":46483},{\"end\":46508,\"start\":46502}]", "bib_author_last_name": "[{\"end\":36091,\"start\":36086},{\"end\":36108,\"start\":36103},{\"end\":36135,\"start\":36126},{\"end\":36149,\"start\":36144},{\"end\":36162,\"start\":36156},{\"end\":36589,\"start\":36587},{\"end\":36850,\"start\":36843},{\"end\":36858,\"start\":36852},{\"end\":36864,\"start\":36860},{\"end\":36873,\"start\":36866},{\"end\":36883,\"start\":36875},{\"end\":36894,\"start\":36887},{\"end\":36902,\"start\":36896},{\"end\":36912,\"start\":36904},{\"end\":36925,\"start\":36916},{\"end\":36933,\"start\":36927},{\"end\":37423,\"start\":37409},{\"end\":37441,\"start\":37432},{\"end\":37452,\"start\":37448},{\"end\":37466,\"start\":37461},{\"end\":37896,\"start\":37891},{\"end\":37909,\"start\":37905},{\"end\":37923,\"start\":37917},{\"end\":37938,\"start\":37932},{\"end\":38349,\"start\":38345},{\"end\":38361,\"start\":38358},{\"end\":38375,\"start\":38372},{\"end\":38391,\"start\":38386},{\"end\":38875,\"start\":38868},{\"end\":39144,\"start\":39133},{\"end\":39161,\"start\":39156},{\"end\":39181,\"start\":39172},{\"end\":39194,\"start\":39188},{\"end\":39208,\"start\":39202},{\"end\":39223,\"start\":39214},{\"end\":39648,\"start\":39641},{\"end\":39665,\"start\":39657},{\"end\":39683,\"start\":39675},{\"end\":39697,\"start\":39691},{\"end\":39713,\"start\":39707},{\"end\":39724,\"start\":39715},{\"end\":39733,\"start\":39728},{\"end\":39748,\"start\":39741},{\"end\":39752,\"start\":39750},{\"end\":40248,\"start\":40243},{\"end\":40502,\"start\":40500},{\"end\":40526,\"start\":40522},{\"end\":40544,\"start\":40536},{\"end\":40558,\"start\":40554},{\"end\":40580,\"start\":40568},{\"end\":40591,\"start\":40587},{\"end\":40610,\"start\":40603},{\"end\":41255,\"start\":41253},{\"end\":41277,\"start\":41265},{\"end\":41298,\"start\":41286},{\"end\":41741,\"start\":41729},{\"end\":41754,\"start\":41750},{\"end\":41767,\"start\":41764},{\"end\":41780,\"start\":41778},{\"end\":41787,\"start\":41782},{\"end\":42079,\"start\":42077},{\"end\":42089,\"start\":42085},{\"end\":42304,\"start\":42290},{\"end\":42550,\"start\":42541},{\"end\":42566,\"start\":42561},{\"end\":42585,\"start\":42576},{\"end\":42596,\"start\":42587},{\"end\":42922,\"start\":42916},{\"end\":42938,\"start\":42931},{\"end\":42952,\"start\":42946},{\"end\":43446,\"start\":43432},{\"end\":43461,\"start\":43456},{\"end\":43477,\"start\":43468},{\"end\":43488,\"start\":43479},{\"end\":43507,\"start\":43496},{\"end\":43525,\"start\":43515},{\"end\":43541,\"start\":43535},{\"end\":43546,\"start\":43543},{\"end\":43964,\"start\":43959},{\"end\":43989,\"start\":43975},{\"end\":44004,\"start\":44000},{\"end\":44019,\"start\":44012},{\"end\":44384,\"start\":44379},{\"end\":44402,\"start\":44392},{\"end\":44415,\"start\":44413},{\"end\":44425,\"start\":44421},{\"end\":44841,\"start\":44836},{\"end\":44861,\"start\":44852},{\"end\":44875,\"start\":44869},{\"end\":44891,\"start\":44887},{\"end\":45370,\"start\":45365},{\"end\":45384,\"start\":45380},{\"end\":45397,\"start\":45395},{\"end\":45409,\"start\":45404},{\"end\":45422,\"start\":45419},{\"end\":45432,\"start\":45428},{\"end\":45861,\"start\":45856},{\"end\":45875,\"start\":45871},{\"end\":45891,\"start\":45886},{\"end\":45905,\"start\":45899},{\"end\":45915,\"start\":45911},{\"end\":46435,\"start\":46432},{\"end\":46448,\"start\":46445},{\"end\":46462,\"start\":46457},{\"end\":46481,\"start\":46470},{\"end\":46500,\"start\":46491},{\"end\":46514,\"start\":46509}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":211205742},\"end\":36485,\"start\":35977},{\"attributes\":{\"id\":\"b1\"},\"end\":36706,\"start\":36487},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17255102},\"end\":37305,\"start\":36708},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":236151316},\"end\":37812,\"start\":37307},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":21351739},\"end\":38233,\"start\":37814},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4701490},\"end\":38734,\"start\":38235},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":733980},\"end\":39085,\"start\":38736},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":174797921},\"end\":39428,\"start\":39087},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":207031443},\"end\":40179,\"start\":39430},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14542261},\"end\":40388,\"start\":40181},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3347484},\"end\":41173,\"start\":40390},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":53357392},\"end\":41642,\"start\":41175},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":216532623},\"end\":41985,\"start\":41644},{\"attributes\":{\"id\":\"b13\"},\"end\":42205,\"start\":41987},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":209901021},\"end\":42433,\"start\":42207},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":182118830},\"end\":42818,\"start\":42435},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9812826},\"end\":43351,\"start\":42820},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49299508},\"end\":43891,\"start\":43353},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":224815683},\"end\":44284,\"start\":43893},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235806322},\"end\":44698,\"start\":44286},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49300436},\"end\":45275,\"start\":44700},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":248922506},\"end\":45759,\"start\":45277},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":237297487},\"end\":46334,\"start\":45761},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":239036966},\"end\":47035,\"start\":46336}]", "bib_title": "[{\"end\":36075,\"start\":35977},{\"end\":36835,\"start\":36708},{\"end\":37397,\"start\":37307},{\"end\":37882,\"start\":37814},{\"end\":38339,\"start\":38235},{\"end\":38859,\"start\":38736},{\"end\":39123,\"start\":39087},{\"end\":39637,\"start\":39430},{\"end\":40236,\"start\":40181},{\"end\":40489,\"start\":40390},{\"end\":41242,\"start\":41175},{\"end\":41723,\"start\":41644},{\"end\":42283,\"start\":42207},{\"end\":42533,\"start\":42435},{\"end\":42908,\"start\":42820},{\"end\":43425,\"start\":43353},{\"end\":43951,\"start\":43893},{\"end\":44371,\"start\":44286},{\"end\":44829,\"start\":44700},{\"end\":45357,\"start\":45277},{\"end\":45848,\"start\":45761},{\"end\":46422,\"start\":46336}]", "bib_author": "[{\"end\":36093,\"start\":36077},{\"end\":36110,\"start\":36093},{\"end\":36137,\"start\":36110},{\"end\":36151,\"start\":36137},{\"end\":36164,\"start\":36151},{\"end\":36591,\"start\":36582},{\"end\":36852,\"start\":36837},{\"end\":36860,\"start\":36852},{\"end\":36866,\"start\":36860},{\"end\":36875,\"start\":36866},{\"end\":36885,\"start\":36875},{\"end\":36896,\"start\":36885},{\"end\":36904,\"start\":36896},{\"end\":36914,\"start\":36904},{\"end\":36927,\"start\":36914},{\"end\":36935,\"start\":36927},{\"end\":37425,\"start\":37399},{\"end\":37443,\"start\":37425},{\"end\":37454,\"start\":37443},{\"end\":37468,\"start\":37454},{\"end\":37898,\"start\":37884},{\"end\":37911,\"start\":37898},{\"end\":37925,\"start\":37911},{\"end\":37940,\"start\":37925},{\"end\":38351,\"start\":38341},{\"end\":38363,\"start\":38351},{\"end\":38377,\"start\":38363},{\"end\":38393,\"start\":38377},{\"end\":38877,\"start\":38861},{\"end\":39146,\"start\":39125},{\"end\":39163,\"start\":39146},{\"end\":39183,\"start\":39163},{\"end\":39196,\"start\":39183},{\"end\":39210,\"start\":39196},{\"end\":39225,\"start\":39210},{\"end\":39650,\"start\":39639},{\"end\":39667,\"start\":39650},{\"end\":39685,\"start\":39667},{\"end\":39699,\"start\":39685},{\"end\":39715,\"start\":39699},{\"end\":39726,\"start\":39715},{\"end\":39735,\"start\":39726},{\"end\":39750,\"start\":39735},{\"end\":39754,\"start\":39750},{\"end\":40250,\"start\":40238},{\"end\":40504,\"start\":40491},{\"end\":40528,\"start\":40504},{\"end\":40546,\"start\":40528},{\"end\":40560,\"start\":40546},{\"end\":40582,\"start\":40560},{\"end\":40593,\"start\":40582},{\"end\":40612,\"start\":40593},{\"end\":41257,\"start\":41244},{\"end\":41279,\"start\":41257},{\"end\":41300,\"start\":41279},{\"end\":41743,\"start\":41725},{\"end\":41756,\"start\":41743},{\"end\":41769,\"start\":41756},{\"end\":41782,\"start\":41769},{\"end\":41789,\"start\":41782},{\"end\":42081,\"start\":42068},{\"end\":42091,\"start\":42081},{\"end\":42306,\"start\":42285},{\"end\":42552,\"start\":42535},{\"end\":42568,\"start\":42552},{\"end\":42587,\"start\":42568},{\"end\":42598,\"start\":42587},{\"end\":42924,\"start\":42910},{\"end\":42940,\"start\":42924},{\"end\":42954,\"start\":42940},{\"end\":43448,\"start\":43427},{\"end\":43463,\"start\":43448},{\"end\":43479,\"start\":43463},{\"end\":43490,\"start\":43479},{\"end\":43509,\"start\":43490},{\"end\":43527,\"start\":43509},{\"end\":43543,\"start\":43527},{\"end\":43548,\"start\":43543},{\"end\":43966,\"start\":43953},{\"end\":43991,\"start\":43966},{\"end\":44006,\"start\":43991},{\"end\":44021,\"start\":44006},{\"end\":44386,\"start\":44373},{\"end\":44404,\"start\":44386},{\"end\":44417,\"start\":44404},{\"end\":44427,\"start\":44417},{\"end\":44843,\"start\":44831},{\"end\":44863,\"start\":44843},{\"end\":44877,\"start\":44863},{\"end\":44893,\"start\":44877},{\"end\":45372,\"start\":45359},{\"end\":45386,\"start\":45372},{\"end\":45399,\"start\":45386},{\"end\":45411,\"start\":45399},{\"end\":45424,\"start\":45411},{\"end\":45434,\"start\":45424},{\"end\":45863,\"start\":45850},{\"end\":45877,\"start\":45863},{\"end\":45893,\"start\":45877},{\"end\":45907,\"start\":45893},{\"end\":45917,\"start\":45907},{\"end\":46437,\"start\":46424},{\"end\":46450,\"start\":46437},{\"end\":46464,\"start\":46450},{\"end\":46483,\"start\":46464},{\"end\":46502,\"start\":46483},{\"end\":46516,\"start\":46502}]", "bib_venue": "[{\"end\":36212,\"start\":36164},{\"end\":36580,\"start\":36487},{\"end\":36978,\"start\":36935},{\"end\":37539,\"start\":37468},{\"end\":38004,\"start\":37940},{\"end\":38464,\"start\":38393},{\"end\":38898,\"start\":38877},{\"end\":39243,\"start\":39225},{\"end\":39790,\"start\":39754},{\"end\":40273,\"start\":40250},{\"end\":40720,\"start\":40612},{\"end\":41388,\"start\":41300},{\"end\":41807,\"start\":41789},{\"end\":42066,\"start\":41987},{\"end\":42311,\"start\":42306},{\"end\":42614,\"start\":42598},{\"end\":43037,\"start\":42954},{\"end\":43601,\"start\":43548},{\"end\":44064,\"start\":44021},{\"end\":44482,\"start\":44427},{\"end\":44952,\"start\":44893},{\"end\":45505,\"start\":45434},{\"end\":46025,\"start\":45917},{\"end\":46624,\"start\":46516},{\"end\":40283,\"start\":40275},{\"end\":40815,\"start\":40722},{\"end\":43107,\"start\":43039},{\"end\":44094,\"start\":44066},{\"end\":44998,\"start\":44954},{\"end\":46719,\"start\":46626}]"}}}, "year": 2023, "month": 12, "day": 17}