{"id": 243848058, "updated": "2023-10-05 20:13:13.379", "metadata": {"title": "SMU: smooth activation function for deep networks using smoothing maximum technique", "authors": "[{\"first\":\"Koushik\",\"last\":\"Biswas\",\"middle\":[]},{\"first\":\"Sandeep\",\"last\":\"Kumar\",\"middle\":[]},{\"first\":\"Shilpak\",\"last\":\"Banerjee\",\"middle\":[]},{\"first\":\"Ashish\",\"last\":\"Pandey\",\"middle\":[\"Kumar\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Deep learning researchers have a keen interest in proposing two new novel activation functions which can boost network performance. A good choice of activation function can have significant consequences in improving network performance. A handcrafted activation is the most common choice in neural network models. ReLU is the most common choice in the deep learning community due to its simplicity though ReLU has some serious drawbacks. In this paper, we have proposed a new novel activation function based on approximation of known activation functions like Leaky ReLU, and we call this function Smooth Maximum Unit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the CIFAR100 dataset with the ShuffleNet V2 model.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.04682", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2111-04682", "doi": null}}, "content": {"source": {"pdf_hash": "d54205c50fa0407797b1303d6db455f69976fb46", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.04682v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e52ca010aba2725f362a155689dfe67e4045afe6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d54205c50fa0407797b1303d6db455f69976fb46.txt", "contents": "\nSMU: SMOOTH ACTIVATION FUNCTION FOR DEEP NETWORKS USING SMOOTHING MAXIMUM TECHNIQUE A PREPRINT\n\n\nKoushik Biswas \nSandeep Kumar \nShilpak Banerjee \nAshish Kumar Pandey \nSMU: SMOOTH ACTIVATION FUNCTION FOR DEEP NETWORKS USING SMOOTHING MAXIMUM TECHNIQUE A PREPRINT\nSmooth Activation Function \u00b7 Deep Learning \u00b7 Neural Network\nDeep learning researchers have a keen interest in proposing two new novel activation functions which can boost network performance. A good choice of activation function can have significant consequences in improving network performance. A handcrafted activation is the most common choice in neural network models. ReLU is the most common choice in the deep learning community due to its simplicity though ReLU has some serious drawbacks. In this paper, we have proposed a new novel activation function based on approximation of known activation functions like Leaky ReLU, and we call this function Smooth Maximum Unit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the CIFAR100 dataset with the ShuffleNet V2 model.\n\nRelated Work and Motivation\n\nIn a deep neural network, activations are either fixed before training or trainable. Researchers have proposed several activations in recent years by combining known functions. Some of these functions have hyperparameters or trainable parameters. In the case of trainable activation functions, parameters are optimized during training. Swish is a popular activation function that can be used as either a constant or trainable activation function, and it shows some good performance in a variety of deep learning tasks like image classification, object detection, machine translation etc. GELU shares similar properties like the Swish activation function, and it gains popularity in the deep learning community due to its efficacy in natural language processing tasks. GELU has been used in BERT [13], GPT-2 [14], and GPT-3 [15] architectures. Pad\u00e9 activation unit (PAU) has been proposed recently, and it is constructed from the approximation of the Leaky ReLU function by rational polynomials of a given order. Though PAU improves network performance in the image classification problem over ReLU, its variants, and Swish, it has a major drawback. PAU contains many trainable parameters, which significantly increases the network complexity and computational cost.  Motivated from these works, we propose activation functions using the smoothing maximum technique. The maximum function is non-smooth at the origin. We want to explore how the smooth approximation of the maximum function (which can be used as an activation function) affects a network's training dynamics and performance. Our experimental evaluation shows that our proposed activation functions are comparatively more effective than ReLU, Mish, Swish, GELU, PAU etc., across different deep learning tasks. We summarise the paper as follows:\n\n1. We have proposed activation functions by smoothing the maximum function. We show that it can approximate GELU, ReLU, Leaky ReLU or the general Maxout family. 2. We show that the proposed functions outperform widely used activation functions in a variety of deep learning tasks.\n\n\nSmooth Maximum Unit\n\nWe present Smooth Maximum Unit (SMU), smooth activation functions from the smooth approximation of the maximum function. Using the smooth approximation of the |x| function, one can find a general approximating formula for the maximum function, which can smoothly approximate the general Maxout [16] family, ReLU, Leaky ReLU or its variants, Swish etc. We also show that the well established GELU [8] function can be obtained as a special case of SMU.\n\n\nSmooth approximation of the maximum function\n\nNote that the maximum function can be expressed as following two different ways:\nmax(x 1 , x 2 ) = x 1 if x 1 \u2265 x 2 x 2 otherwise = (x 1 + x 2 ) + |x 1 \u2212 x 2 | 2(1)\nNote that the max function is not differentiable at the origin. Using approximations of the |x| function by a smooth function, we can create approximations to the maximum functions. There are many known approximations to |x|, but for the rest of this article, we will focus on two specific approximations of |x|, namely xerf(\u00b5x) and x 2 + \u00b5 2 . We noticed that the activations constructed using these two functions provide good performance on standard datasets on different deep learning problems (for more details, see the supplementary section). Note that x 2 + \u00b5 2 as \u00b5 \u2192 0 approximate |x| from above while xerf(\u00b5x). as \u00b5 \u2192 \u221e gives an approximation of |x| from below. The approximation is uniform on compact subsets of the real line. Here erf is the Gaussian error function defined as follows:\nerf(x) = 2 \u221a \u03c0 x 0 e \u2212t 2 dt.\nNow, replacing the |x| function by xerf(\u00b5x) in equation (1), we have the smooth approximation formula for maximum function as follows:\nf 1 (x 1 , x 2 ; \u00b5) = (x 1 + x 2 ) + (x 1 \u2212 x 2 ) erf(\u00b5(x 1 \u2212 x 2 )) 2 .(2)\nSimilarly, we can derive the the smooth approximation formula for the maximum function from equation (1) by replacing the |x| function by x 2 + \u00b5 2 as follows:\nf 2 (x 1 , x 2 ; \u00b5) = (x 1 + x 2 ) + (x 1 \u2212 x 2 ) 2 + \u00b5 2 2 (3) Note that as \u00b5 \u2192 \u221e, f 1 (x 1 , x 2 ; \u00b5) \u2192 max(x 1 , x 2 ) and as \u00b5 \u2192 0, f 2 (x 1 , x 2 ; \u00b5) \u2192 max(x 1 , x 2 ).\nFor particular values of x 1 and x 2 , we can approximate known activation functions. For example, consider x 1 = ax, x 2 = bx, with a = b in (2), we get:\nf 1 (ax, bx; \u00b5) = (a + b)x + (a \u2212 b)x erf(\u00b5(a \u2212 b)x) 2 .(4)\nThis is a simple case from the Maxout family [16] while more complicated cases can be found by considering nonlinear choices of x 1 and x 2 . We can similarly get smooth approximations to ReLU and Leaky ReLU. For example, consider x 1 = x and x 2 = 0, we have smooth approximation of ReLU as follows:\nf 1 (x, 0; \u00b5) = x + x erf(\u00b5x) 2 .(5)\nWe know that GELU [8] is a smooth approximation of ReLU. Notice that, if we choose \u00b5 = 1 \u221a 2 in equation (5), we can recover GELU activation function which also show that GELU is smooth approximation of ReLU. Also, considering x 1 = x and x 2 = \u03b1x, we have a smooth approximation of Leaky ReLU or Parametric ReLU depending on whether \u03b1 is a hyperparameter or a learnable parameter.\nf 1 (x, \u03b1x; \u00b5) = (1 + \u03b1)x + (1 \u2212 \u03b1)x erf(\u00b5(1 \u2212 \u03b1)x) 2 .(6)\nNote that, equation (5) and equation (6) approximate ReLU or Leaky ReLU from below. Similarly, we can derive approximating function from equation (3) which will approximate ReLU or Leaky ReLU from above.\n\nThe corresponding derivatives of equation (6) for input variable x is\nd dx f 1 (x, \u03b1x; \u00b5) = 1 2 [(1 + \u03b1) + (1 \u2212 \u03b1) erf(\u00b5(1 \u2212 \u03b1)x) + 2 \u221a \u03c0 \u00b5(1 \u2212 \u03b1) 2 xe \u2212(\u00b5(1\u2212\u03b1)x) 2 ](7)\nwhere  (3) and we call this function SMU-1. For all of our experiments, we will use SMU and SMU-1 as our proposed activation functions.\nd dx erf(x) = 2 \u221a \u03c0 e \u2212x 2 .\n\nLearning activation parameters via back-propagation\n\nTrainable activation function parameters are updated using backpropagation [17] technique. We implemented forward pass in both Pytorch [18] & Tensorflow-Keras [19] API, and automatic differentiation will update the parameters. Alternatively, CUDA [20] based implementation (see [2]) can be used and the gradients of equations 8 and 9) for the parameters \u03b1 and \u00b5 can be computed as follows:\n\u2202f 1 \u2202\u03b1 = 1 2 [x \u2212 x erf(\u00b5(1 \u2212 \u03b1)x) \u2212 (1 \u2212 \u03b1)\u00b5x 2 e \u2212(\u00b5(1\u2212\u03b1)x) 2 ](8)\u2202f 1 \u2202\u00b5 = 1 2 (1 \u2212 \u03b1) 2 x 2 e \u2212(\u00b5(1\u2212\u03b1)x) 2(9)\n\u03b1 and \u00b5 can be either hyperparameters or trainable parameters. For classification problem, we consider \u03b1 = 0.25, a hyperparameter. We consider \u00b5 as trainable parameter and initialise at 1.0 and 4.352665993287951e \u221209 for SMU and SMU-1 respectively. For object detection and semantic segmentation problem, we consider \u03b1 = 0.01, a hyperparameter. We consider \u00b5 as trainable parameter and initialise at 2.5 and 4.332461424154261e \u2212 09 for SMU and SMU-1 respectively. Now, note that the class of neural networks with SMU and SMU-1 activation function is dense in C(K), where K is a compact subset of R n and C(K) is the space of all continuous functions over K.\n\nThe proof follows from the following proposition (see [11]). \n\n\nExperiments\n\nIn the next subsections, we will provide a detailed experimental evaluation to compare performance for our proposed activation function and some widely used activation functions on 3 different deep learning problems like image classification, object detection, and semantic segmentation.\n\n\nCIFAR\n\nWe report the top-1 accuracy on Table 1 and Table 2 on CIFAR10 [22] and CIFAR100 [22] datasets respectively. We consider MobileNet V2 [23], Shufflenet V2 [24], PreActResNet [25], ResNet [26], VGG [27] (with batch-normalization [28]), and EfficientNet B0 [29].\n\n\nModel\n\nReLU SMU SMU-1 \n\n\nObject Detection\n\nIn this section, we report results on Pascal VOC dataset [30] on Single Shot MultiBox Detector(SSD) 300 [31] with VGG-16(with batch-normalization) [27] as the backbone network. We report the mean average precision (mAP) in Table 3 for the mean of 6 different runs.\n\n\nSemantic Segmentation\n\nWe report our experimental results in this section on the popular CityScapes dataset [32]. The U-net model [33] is considered as the segmentation framework, and we report the mean of 6 different runs for Pixel Accuracy and the mean Intersection-Over-Union (mIOU) on test data on   \n\n\nConclusion\n\nThis work uses the maximum smoothing technique to approximate Leaky ReLU, a well-established non-smooth (not differentiable at 0) function by two smooth functions. We call these two functions SMU and SMU-1, and we use them as a potential candidate for activation functions. Our experimental evaluation shows that the proposed functions beat the traditional activation functions in well-known deep learning problems and have the potential to replace them.\n\n\narXiv:2111.04682v2 [cs.LG] 11 Apr 2022\n\nFigure 1 :Figure 2 :Figure 3 :\n123Approximation of ReLU using SMU (\u03b1 = 0) for different values of \u00b5. As \u00b5 \u2192 \u221e, SMU smoothly approximate ReLU Approximation of Leaky ReLU (\u03b1 = 0.25) using SMU for different values of \u00b5. As \u00b5 \u2192 \u221e, SMU smoothly approximate Leaky ReLU First order derivatives of SMU for \u03b1 = 0.25 and different values of \u00b5.\n\nFigures 1, 2 ,\n2and 3 show the plots for f 1 (x, 0; \u00b5), f 1 (x, 0.25x; \u00b5), and derivative of f 1 (x, 0.25x; \u00b5) for different values of \u00b5. From the figures it is clear that as \u00b5 \u2192 \u221e, f 1 (x, \u03b1x; \u00b5) smoothly approximate ReLU or Leaky ReLU depending on value of \u03b1. We call the function in equation (6) as Smooth Maximum Unit (SMU). Similarly, We can derive a function by replacing x 1 = x and x 2 = \u03b1x in equation\n\nProposition 1 .\n1(Theorem 1.1 in Kidger and Lyons, 2020[21]) :-Let \u03c1 : R \u2192 R be any continuous function. Let N \u03c1 n represent the class of neural networks with activation function \u03c1, with n neurons in the input layer, one neuron in the output layer, and one hidden layer with an arbitrary number of neurons. Let K \u2286 R n be compact. Then N \u03c1 n is dense in C(K) if and only if \u03c1 is non-polynomial.\n\ntable 4 .\n4Model \nReLU \nSMU \nSMU-1 \n\nTop-1 accuracy Top-1 accuracy Top-1 accuracy \n\nShufflenet V2 1.0x \n64.41 \u00b1 0.25 \n70.60 \u00b1 0.21 \n69.96 \u00b1 0.22 \nShufflenet V2 2.0x \n67.52 \u00b1 0.25 \n73.74 \u00b1 0.20 \n73.45 \u00b1 0.23 \n\nPreActResNet 34 \n73.41 \u00b1 0.24 \n76.21 \u00b1 0.20 \n75.87 \u00b1 0.21 \n\nResNet 34 \n73.33 \u00b1 0.27 \n75.77 \u00b1 0.20 \n75.59 \u00b1 0.21 \n\nSeNet 34 \n75.12 \u00b1 0.22 \n76.79 \u00b1 0.18 \n75.79 \u00b1 0.21 \n\nMobileNet V2 \n74.17 \u00b1 0.24 \n76.31 \u00b1 0.19 \n76.03 \u00b1 0.19 \n\nXception \n71.22 \u00b1 0.26 \n74.62 \u00b1 0.23 \n74.11 \u00b1 0.23 \n\nEffitientNet B0 \n76.60 \u00b1 0.27 \n79.10 \u00b1 0.22 \n78.77 \u00b1 0.23 \n\nVGG16 \n71.87 \u00b1 0.30 \n73.26 \u00b1 0.22 \n72.79 \u00b1 0.23 \n\n\n\nTable 2 :\n2Comparison between our proposed activations and other baseline activations in the CIFAR100 dataset. We report the results for the mean of 15 different runs. mean\u00b1std is reported in the table.Activation Function \nmAP \nSMU \n78.1\u00b10.09 \nSMU-1 \n77.9\u00b10.10 \nReLU \n77.2\u00b10.14 \nLeaky ReLU \n77.2\u00b10.18 \nPReLU \n77.2\u00b10.17 \nReLU6 \n77.1\u00b10.15 \nELU \n75.1\u00b10.22 \nSoftplus \n74.2\u00b10.25 \nSwish \n77.3\u00b10.11 \nGELU \n77.3\u00b10.12 \n\n\n\nTable 3 :\n3A detailed comparison between SMU activation and other baseline activations in Pascal VOC dataset. mean\u00b1std is reported in the tableTable 4: A detailed comparison between SMU activation and other baseline activations in CityScapes dataset. mean\u00b1std is reported in the table.Activation Function \nPixel \nAccuracy \nmIOU \n\nSMU \n81.71\u00b10.38 71.45\u00b10.30 \nSMU-1 \n81.17\u00b10.41 71.10\u00b10.30 \nReLU \n79.49\u00b10.46 69.31\u00b10.28 \nPReLU \n78.95\u00b10.42 68.88\u00b10.41 \nReLU6 \n79.58\u00b10.41 69.70\u00b10.42 \nLeaky ReLU \n79.41\u00b10.41 69.64\u00b10.42 \nELU \n79.48\u00b10.50 68.19\u00b10.40 \nSoftplus \n78.45\u00b10.52 68.08\u00b10.49 \nSwish \n80.22\u00b10.46 69.81\u00b10.30 \nGELU \n80.14\u00b10.37 69.59\u00b10.40 \n\n\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th International Conference on Machine Learning (ICML-10). the 27th International Conference on Machine Learning (ICML-10)Haifa, IsraelOmnipressJohannes F\u00fcrnkranz and Thorsten JoachimsVinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Johannes F\u00fcrnkranz and Thorsten Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pages 807-814. Omnipress, 2010.\n\nRectifier nonlinearities improve neural network acoustic models. Andrew L Maas, Awni Y Hannun, Andrew Y Ng, ICML Workshop on Deep Learning for Audio, Speech and Language Processing. Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015.\n\nFast and accurate deep network learning by exponential linear units (elus). Djork-Arn\u00e9 Clevert, Thomas Unterthiner, Sepp Hochreiter, Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus), 2016.\n\nImproving deep neural networks using softplus units. Zhanlei Hao Zheng, Wenju Yang, Jizhong Liu, Yanpeng Liang, Li, 2015 International Joint Conference on Neural Networks (IJCNN). Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural networks using softplus units. In 2015 International Joint Conference on Neural Networks (IJCNN), pages 1-4, 2015.\n\nEmpirical evaluation of rectified activations in convolutional network. Bing Xu, Naiyan Wang, Tianqi Chen, Mu Li, Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network, 2015.\n\nSearching for activation functions. Prajit Ramachandran, Barret Zoph, Quoc V Le, Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017.\n\n. Dan Hendrycks, Kevin Gimpel, 2020Gaussian error linear units (gelusDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2020.\n\nMish: A self regularized non-monotonic activation function. Diganta Misra, Diganta Misra. Mish: A self regularized non-monotonic activation function, 2020.\n\nErfact and pserf: Non-monotonic smooth trainable activation functions. Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey, Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, and Ashish Kumar Pandey. Erfact and pserf: Non-monotonic smooth trainable activation functions, 2021.\n\nPad\u00e9 activation units: End-to-end learning of flexible activation functions in deep networks. Alejandro Molina, Patrick Schramowski, Kristian Kersting, Alejandro Molina, Patrick Schramowski, and Kristian Kersting. Pad\u00e9 activation units: End-to-end learning of flexible activation functions in deep networks, 2020.\n\nOrthogonal-pad\u00e9 activation functions: Trainable activation functions for smooth and faster convergence in deep networks. Koushik Biswas, Shilpak Banerjee, Ashish Kumar Pandey, Koushik Biswas, Shilpak Banerjee, and Ashish Kumar Pandey. Orthogonal-pad\u00e9 activation functions: Trainable activation functions for smooth and faster convergence in deep networks, 2021.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskeverand Dario Amodei. Language models are few-shot learnersTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n\n. Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio, Maxout networksIan J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks, 2013.\n\nBackpropagation applied to handwritten zip code recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D , Neural Computation. 14Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989.\n\n. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Pytorch: An imperative style, high-performance deep learning libraryAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.\n\n. Fran\u00e7ois Chollet, Fran\u00e7ois Chollet et al. Keras. https://keras.io, 2015.\n\nScalable parallel programming. J Nickolls, I Buck, M Garland, K Skadron, IEEE Hot Chips 20 Symposium (HCS). J. Nickolls, I. Buck, M. Garland, and K. Skadron. Scalable parallel programming. In 2008 IEEE Hot Chips 20 Symposium (HCS), pages 40-53, 2008.\n\nUniversal approximation with deep narrow networks. Patrick Kidger, Terry Lyons, Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks, 2020.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, University of TorontoTechnical reportAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Mobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks, 2019.\n\nShufflenet v2: Practical guidelines for efficient cnn architecture design. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun, Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design, 2018.\n\nIdentity mappings in deep residual networks. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks, 2016.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift, 2015.\n\nRethinking model scaling for convolutional neural networks. Mingxing Tan, V Quoc, Le, Efficientnet, Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks, 2020.\n\nThe pascal visual object classes (voc) challenge. Mark Everingham, Luc Gool, Christopher K Williams, John Winn, Andrew Zisserman, Int. J. Comput. Vision. 882Mark Everingham, Luc Gool, Christopher K. Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. Int. J. Comput. Vision, 88(2):303-338, June 2010.\n\nSsd: Single shot multibox detector. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C Berg, Lecture Notes in Computer Science. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. Ssd: Single shot multibox detector. Lecture Notes in Computer Science, page 21-37, 2016.\n\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding, 2016.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.\n", "annotations": {"author": "[{\"end\":113,\"start\":98},{\"end\":128,\"start\":114},{\"end\":146,\"start\":129},{\"end\":167,\"start\":147}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":106},{\"end\":127,\"start\":122},{\"end\":145,\"start\":137},{\"end\":166,\"start\":160}]", "author_first_name": "[{\"end\":105,\"start\":98},{\"end\":121,\"start\":114},{\"end\":136,\"start\":129},{\"end\":153,\"start\":147},{\"end\":159,\"start\":154}]", "author_affiliation": null, "title": "[{\"end\":95,\"start\":1},{\"end\":262,\"start\":168}]", "venue": null, "abstract": "[{\"end\":1054,\"start\":323}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1884,\"start\":1880},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1896,\"start\":1892},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1912,\"start\":1908},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3496,\"start\":3492},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3597,\"start\":3594},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5499,\"start\":5495},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5809,\"start\":5806},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6902,\"start\":6898},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6962,\"start\":6958},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6986,\"start\":6982},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7074,\"start\":7070},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7104,\"start\":7101},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8045,\"start\":8041},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8428,\"start\":8424},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8446,\"start\":8442},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8499,\"start\":8495},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8519,\"start\":8515},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8538,\"start\":8534},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8551,\"start\":8547},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8561,\"start\":8557},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8592,\"start\":8588},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8619,\"start\":8615},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8727,\"start\":8723},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8774,\"start\":8770},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8817,\"start\":8813},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9045,\"start\":9041},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9067,\"start\":9063},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10555,\"start\":10551}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":9747,\"start\":9707},{\"attributes\":{\"id\":\"fig_1\"},\"end\":10082,\"start\":9748},{\"attributes\":{\"id\":\"fig_2\"},\"end\":10494,\"start\":10083},{\"attributes\":{\"id\":\"fig_3\"},\"end\":10890,\"start\":10495},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":11488,\"start\":10891},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":11901,\"start\":11489},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":12536,\"start\":11902}]", "paragraph": "[{\"end\":2892,\"start\":1085},{\"end\":3174,\"start\":2894},{\"end\":3648,\"start\":3198},{\"end\":3777,\"start\":3697},{\"end\":4658,\"start\":3862},{\"end\":4823,\"start\":4689},{\"end\":5059,\"start\":4900},{\"end\":5389,\"start\":5235},{\"end\":5750,\"start\":5450},{\"end\":6169,\"start\":5788},{\"end\":6432,\"start\":6229},{\"end\":6503,\"start\":6434},{\"end\":6739,\"start\":6604},{\"end\":7212,\"start\":6823},{\"end\":7985,\"start\":7328},{\"end\":8048,\"start\":7987},{\"end\":8351,\"start\":8064},{\"end\":8620,\"start\":8361},{\"end\":8645,\"start\":8630},{\"end\":8930,\"start\":8666},{\"end\":9237,\"start\":8956},{\"end\":9706,\"start\":9252}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3861,\"start\":3778},{\"attributes\":{\"id\":\"formula_1\"},\"end\":4688,\"start\":4659},{\"attributes\":{\"id\":\"formula_2\"},\"end\":4899,\"start\":4824},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5234,\"start\":5060},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5449,\"start\":5390},{\"attributes\":{\"id\":\"formula_5\"},\"end\":5787,\"start\":5751},{\"attributes\":{\"id\":\"formula_6\"},\"end\":6228,\"start\":6170},{\"attributes\":{\"id\":\"formula_7\"},\"end\":6603,\"start\":6504},{\"attributes\":{\"id\":\"formula_8\"},\"end\":6768,\"start\":6740},{\"attributes\":{\"id\":\"formula_9\"},\"end\":7282,\"start\":7213},{\"attributes\":{\"id\":\"formula_10\"},\"end\":7327,\"start\":7282}]", "table_ref": "[{\"end\":8400,\"start\":8393},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8412,\"start\":8405},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":8896,\"start\":8889}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":1083,\"start\":1056},{\"attributes\":{\"n\":\"3\"},\"end\":3196,\"start\":3177},{\"attributes\":{\"n\":\"3.1\"},\"end\":3695,\"start\":3651},{\"attributes\":{\"n\":\"3.2\"},\"end\":6821,\"start\":6770},{\"attributes\":{\"n\":\"4\"},\"end\":8062,\"start\":8051},{\"attributes\":{\"n\":\"4.1\"},\"end\":8359,\"start\":8354},{\"end\":8628,\"start\":8623},{\"attributes\":{\"n\":\"4.2\"},\"end\":8664,\"start\":8648},{\"attributes\":{\"n\":\"4.3\"},\"end\":8954,\"start\":8933},{\"attributes\":{\"n\":\"5\"},\"end\":9250,\"start\":9240},{\"end\":9779,\"start\":9749},{\"end\":10098,\"start\":10084},{\"end\":10511,\"start\":10496},{\"end\":10901,\"start\":10892},{\"end\":11499,\"start\":11490},{\"end\":11912,\"start\":11903}]", "table": "[{\"end\":11488,\"start\":10903},{\"end\":11901,\"start\":11692},{\"end\":12536,\"start\":12188}]", "figure_caption": "[{\"end\":9747,\"start\":9709},{\"end\":10082,\"start\":9783},{\"end\":10494,\"start\":10100},{\"end\":10890,\"start\":10513},{\"end\":11692,\"start\":11501},{\"end\":12188,\"start\":11914}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":12605,\"start\":12600},{\"end\":12620,\"start\":12612},{\"end\":12622,\"start\":12621},{\"end\":13204,\"start\":13198},{\"end\":13206,\"start\":13205},{\"end\":13217,\"start\":13213},{\"end\":13219,\"start\":13218},{\"end\":13234,\"start\":13228},{\"end\":13236,\"start\":13235},{\"end\":13617,\"start\":13610},{\"end\":13629,\"start\":13622},{\"end\":13645,\"start\":13637},{\"end\":13655,\"start\":13651},{\"end\":13902,\"start\":13892},{\"end\":13918,\"start\":13912},{\"end\":13936,\"start\":13932},{\"end\":14153,\"start\":14146},{\"end\":14170,\"start\":14165},{\"end\":14184,\"start\":14177},{\"end\":14197,\"start\":14190},{\"end\":14554,\"start\":14550},{\"end\":14565,\"start\":14559},{\"end\":14578,\"start\":14572},{\"end\":14587,\"start\":14585},{\"end\":14759,\"start\":14753},{\"end\":14780,\"start\":14774},{\"end\":14791,\"start\":14787},{\"end\":14793,\"start\":14792},{\"end\":14896,\"start\":14893},{\"end\":14913,\"start\":14908},{\"end\":15103,\"start\":15096},{\"end\":15271,\"start\":15264},{\"end\":15287,\"start\":15280},{\"end\":15302,\"start\":15295},{\"end\":15319,\"start\":15313},{\"end\":15325,\"start\":15320},{\"end\":15589,\"start\":15580},{\"end\":15605,\"start\":15598},{\"end\":15627,\"start\":15619},{\"end\":15929,\"start\":15922},{\"end\":15945,\"start\":15938},{\"end\":15962,\"start\":15956},{\"end\":15968,\"start\":15963},{\"end\":16251,\"start\":16246},{\"end\":16268,\"start\":16260},{\"end\":16282,\"start\":16276},{\"end\":16296,\"start\":16288},{\"end\":16520,\"start\":16516},{\"end\":16534,\"start\":16530},{\"end\":16544,\"start\":16539},{\"end\":16557,\"start\":16552},{\"end\":16569,\"start\":16564},{\"end\":16582,\"start\":16578},{\"end\":16741,\"start\":16738},{\"end\":16743,\"start\":16742},{\"end\":16759,\"start\":16751},{\"end\":16770,\"start\":16766},{\"end\":16785,\"start\":16778},{\"end\":16800,\"start\":16795},{\"end\":16817,\"start\":16809},{\"end\":16834,\"start\":16828},{\"end\":16854,\"start\":16848},{\"end\":16868,\"start\":16862},{\"end\":16883,\"start\":16877},{\"end\":16900,\"start\":16892},{\"end\":16915,\"start\":16910},{\"end\":16938,\"start\":16930},{\"end\":16951,\"start\":16948},{\"end\":16967,\"start\":16962},{\"end\":16981,\"start\":16975},{\"end\":16996,\"start\":16990},{\"end\":16998,\"start\":16997},{\"end\":17015,\"start\":17008},{\"end\":17027,\"start\":17020},{\"end\":17047,\"start\":17036},{\"end\":17059,\"start\":17055},{\"end\":17070,\"start\":17066},{\"end\":17086,\"start\":17079},{\"end\":17790,\"start\":17787},{\"end\":17792,\"start\":17791},{\"end\":17810,\"start\":17805},{\"end\":17830,\"start\":17825},{\"end\":17843,\"start\":17838},{\"end\":17861,\"start\":17855},{\"end\":18059,\"start\":18058},{\"end\":18068,\"start\":18067},{\"end\":18077,\"start\":18076},{\"end\":18079,\"start\":18078},{\"end\":18089,\"start\":18088},{\"end\":18102,\"start\":18101},{\"end\":18104,\"start\":18103},{\"end\":18114,\"start\":18113},{\"end\":18125,\"start\":18124},{\"end\":18127,\"start\":18126},{\"end\":18352,\"start\":18348},{\"end\":18364,\"start\":18361},{\"end\":18381,\"start\":18372},{\"end\":18393,\"start\":18389},{\"end\":18406,\"start\":18401},{\"end\":18424,\"start\":18417},{\"end\":18439,\"start\":18433},{\"end\":18455,\"start\":18449},{\"end\":18468,\"start\":18461},{\"end\":18485,\"start\":18481},{\"end\":18499,\"start\":18494},{\"end\":18518,\"start\":18511},{\"end\":18531,\"start\":18525},{\"end\":18542,\"start\":18538},{\"end\":18557,\"start\":18551},{\"end\":18573,\"start\":18566},{\"end\":18588,\"start\":18582},{\"end\":18609,\"start\":18603},{\"end\":18621,\"start\":18619},{\"end\":18634,\"start\":18628},{\"end\":18647,\"start\":18640},{\"end\":19127,\"start\":19119},{\"end\":19225,\"start\":19224},{\"end\":19237,\"start\":19236},{\"end\":19245,\"start\":19244},{\"end\":19256,\"start\":19255},{\"end\":19503,\"start\":19496},{\"end\":19517,\"start\":19512},{\"end\":19674,\"start\":19670},{\"end\":19848,\"start\":19844},{\"end\":19864,\"start\":19858},{\"end\":19881,\"start\":19873},{\"end\":19893,\"start\":19887},{\"end\":19916,\"start\":19905},{\"end\":20208,\"start\":20200},{\"end\":20220,\"start\":20213},{\"end\":20235,\"start\":20228},{\"end\":20247,\"start\":20243},{\"end\":20444,\"start\":20437},{\"end\":20456,\"start\":20449},{\"end\":20472,\"start\":20464},{\"end\":20482,\"start\":20478},{\"end\":20648,\"start\":20641},{\"end\":20660,\"start\":20653},{\"end\":20676,\"start\":20668},{\"end\":20686,\"start\":20682},{\"end\":20873,\"start\":20868},{\"end\":20890,\"start\":20884},{\"end\":21114,\"start\":21108},{\"end\":21131,\"start\":21122},{\"end\":21346,\"start\":21338},{\"end\":21353,\"start\":21352},{\"end\":21542,\"start\":21538},{\"end\":21558,\"start\":21555},{\"end\":21576,\"start\":21565},{\"end\":21578,\"start\":21577},{\"end\":21593,\"start\":21589},{\"end\":21606,\"start\":21600},{\"end\":21870,\"start\":21867},{\"end\":21884,\"start\":21876},{\"end\":21902,\"start\":21895},{\"end\":21919,\"start\":21910},{\"end\":21934,\"start\":21929},{\"end\":21951,\"start\":21941},{\"end\":21965,\"start\":21956},{\"end\":21967,\"start\":21966},{\"end\":22324,\"start\":22318},{\"end\":22340,\"start\":22333},{\"end\":22357,\"start\":22348},{\"end\":22369,\"start\":22365},{\"end\":22385,\"start\":22379},{\"end\":22404,\"start\":22397},{\"end\":22695,\"start\":22691},{\"end\":22716,\"start\":22709},{\"end\":22732,\"start\":22726}]", "bib_author_last_name": "[{\"end\":12610,\"start\":12606},{\"end\":12629,\"start\":12623},{\"end\":13211,\"start\":13207},{\"end\":13226,\"start\":13220},{\"end\":13239,\"start\":13237},{\"end\":13620,\"start\":13618},{\"end\":13635,\"start\":13630},{\"end\":13649,\"start\":13646},{\"end\":13659,\"start\":13656},{\"end\":13910,\"start\":13903},{\"end\":13930,\"start\":13919},{\"end\":13947,\"start\":13937},{\"end\":14163,\"start\":14154},{\"end\":14175,\"start\":14171},{\"end\":14188,\"start\":14185},{\"end\":14203,\"start\":14198},{\"end\":14207,\"start\":14205},{\"end\":14557,\"start\":14555},{\"end\":14570,\"start\":14566},{\"end\":14583,\"start\":14579},{\"end\":14590,\"start\":14588},{\"end\":14772,\"start\":14760},{\"end\":14785,\"start\":14781},{\"end\":14796,\"start\":14794},{\"end\":14906,\"start\":14897},{\"end\":14920,\"start\":14914},{\"end\":15109,\"start\":15104},{\"end\":15278,\"start\":15272},{\"end\":15293,\"start\":15288},{\"end\":15311,\"start\":15303},{\"end\":15332,\"start\":15326},{\"end\":15596,\"start\":15590},{\"end\":15617,\"start\":15606},{\"end\":15636,\"start\":15628},{\"end\":15936,\"start\":15930},{\"end\":15954,\"start\":15946},{\"end\":15975,\"start\":15969},{\"end\":16258,\"start\":16252},{\"end\":16274,\"start\":16269},{\"end\":16286,\"start\":16283},{\"end\":16306,\"start\":16297},{\"end\":16528,\"start\":16521},{\"end\":16537,\"start\":16535},{\"end\":16550,\"start\":16545},{\"end\":16562,\"start\":16558},{\"end\":16576,\"start\":16570},{\"end\":16592,\"start\":16583},{\"end\":16749,\"start\":16744},{\"end\":16764,\"start\":16760},{\"end\":16776,\"start\":16771},{\"end\":16793,\"start\":16786},{\"end\":16807,\"start\":16801},{\"end\":16826,\"start\":16818},{\"end\":16846,\"start\":16835},{\"end\":16860,\"start\":16855},{\"end\":16875,\"start\":16869},{\"end\":16890,\"start\":16884},{\"end\":16908,\"start\":16901},{\"end\":16928,\"start\":16916},{\"end\":16946,\"start\":16939},{\"end\":16960,\"start\":16952},{\"end\":16973,\"start\":16968},{\"end\":16988,\"start\":16982},{\"end\":17006,\"start\":16999},{\"end\":17018,\"start\":17016},{\"end\":17034,\"start\":17028},{\"end\":17053,\"start\":17048},{\"end\":17064,\"start\":17060},{\"end\":17077,\"start\":17071},{\"end\":17093,\"start\":17087},{\"end\":17803,\"start\":17793},{\"end\":17823,\"start\":17811},{\"end\":17836,\"start\":17831},{\"end\":17853,\"start\":17844},{\"end\":17868,\"start\":17862},{\"end\":18065,\"start\":18060},{\"end\":18074,\"start\":18069},{\"end\":18086,\"start\":18080},{\"end\":18099,\"start\":18090},{\"end\":18111,\"start\":18105},{\"end\":18122,\"start\":18115},{\"end\":18359,\"start\":18353},{\"end\":18370,\"start\":18365},{\"end\":18387,\"start\":18382},{\"end\":18399,\"start\":18394},{\"end\":18415,\"start\":18407},{\"end\":18431,\"start\":18425},{\"end\":18447,\"start\":18440},{\"end\":18459,\"start\":18456},{\"end\":18479,\"start\":18469},{\"end\":18492,\"start\":18486},{\"end\":18509,\"start\":18500},{\"end\":18523,\"start\":18519},{\"end\":18536,\"start\":18532},{\"end\":18549,\"start\":18543},{\"end\":18564,\"start\":18558},{\"end\":18580,\"start\":18574},{\"end\":18601,\"start\":18589},{\"end\":18617,\"start\":18610},{\"end\":18626,\"start\":18622},{\"end\":18638,\"start\":18635},{\"end\":18656,\"start\":18648},{\"end\":19135,\"start\":19128},{\"end\":19234,\"start\":19226},{\"end\":19242,\"start\":19238},{\"end\":19253,\"start\":19246},{\"end\":19264,\"start\":19257},{\"end\":19510,\"start\":19504},{\"end\":19523,\"start\":19518},{\"end\":19685,\"start\":19675},{\"end\":19856,\"start\":19849},{\"end\":19871,\"start\":19865},{\"end\":19885,\"start\":19882},{\"end\":19903,\"start\":19894},{\"end\":19921,\"start\":19917},{\"end\":20211,\"start\":20209},{\"end\":20226,\"start\":20221},{\"end\":20241,\"start\":20236},{\"end\":20251,\"start\":20248},{\"end\":20447,\"start\":20445},{\"end\":20462,\"start\":20457},{\"end\":20476,\"start\":20473},{\"end\":20486,\"start\":20483},{\"end\":20651,\"start\":20649},{\"end\":20666,\"start\":20661},{\"end\":20680,\"start\":20677},{\"end\":20690,\"start\":20687},{\"end\":20882,\"start\":20874},{\"end\":20900,\"start\":20891},{\"end\":21120,\"start\":21115},{\"end\":21139,\"start\":21132},{\"end\":21350,\"start\":21347},{\"end\":21358,\"start\":21354},{\"end\":21362,\"start\":21360},{\"end\":21376,\"start\":21364},{\"end\":21553,\"start\":21543},{\"end\":21563,\"start\":21559},{\"end\":21587,\"start\":21579},{\"end\":21598,\"start\":21594},{\"end\":21616,\"start\":21607},{\"end\":21874,\"start\":21871},{\"end\":21893,\"start\":21885},{\"end\":21908,\"start\":21903},{\"end\":21927,\"start\":21920},{\"end\":21939,\"start\":21935},{\"end\":21954,\"start\":21952},{\"end\":21972,\"start\":21968},{\"end\":22331,\"start\":22325},{\"end\":22346,\"start\":22341},{\"end\":22363,\"start\":22358},{\"end\":22377,\"start\":22370},{\"end\":22395,\"start\":22386},{\"end\":22413,\"start\":22405},{\"end\":22707,\"start\":22696},{\"end\":22724,\"start\":22717},{\"end\":22737,\"start\":22733}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15539264},\"end\":13131,\"start\":12538},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16489696},\"end\":13515,\"start\":13133},{\"attributes\":{\"id\":\"b2\"},\"end\":13814,\"start\":13517},{\"attributes\":{\"id\":\"b3\"},\"end\":14091,\"start\":13816},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15556184},\"end\":14476,\"start\":14093},{\"attributes\":{\"id\":\"b5\"},\"end\":14715,\"start\":14478},{\"attributes\":{\"id\":\"b6\"},\"end\":14889,\"start\":14717},{\"attributes\":{\"id\":\"b7\"},\"end\":15034,\"start\":14891},{\"attributes\":{\"id\":\"b8\"},\"end\":15191,\"start\":15036},{\"attributes\":{\"id\":\"b9\"},\"end\":15484,\"start\":15193},{\"attributes\":{\"id\":\"b10\"},\"end\":15799,\"start\":15486},{\"attributes\":{\"id\":\"b11\"},\"end\":16162,\"start\":15801},{\"attributes\":{\"id\":\"b12\"},\"end\":16461,\"start\":16164},{\"attributes\":{\"id\":\"b13\"},\"end\":16734,\"start\":16463},{\"attributes\":{\"id\":\"b14\"},\"end\":17783,\"start\":16736},{\"attributes\":{\"id\":\"b15\"},\"end\":17995,\"start\":17785},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":41312633},\"end\":18344,\"start\":17997},{\"attributes\":{\"id\":\"b17\"},\"end\":19115,\"start\":18346},{\"attributes\":{\"id\":\"b18\"},\"end\":19191,\"start\":19117},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7344254},\"end\":19443,\"start\":19193},{\"attributes\":{\"id\":\"b20\"},\"end\":19613,\"start\":19445},{\"attributes\":{\"id\":\"b21\"},\"end\":19842,\"start\":19615},{\"attributes\":{\"id\":\"b22\"},\"end\":20123,\"start\":19844},{\"attributes\":{\"id\":\"b23\"},\"end\":20390,\"start\":20125},{\"attributes\":{\"id\":\"b24\"},\"end\":20593,\"start\":20392},{\"attributes\":{\"id\":\"b25\"},\"end\":20798,\"start\":20595},{\"attributes\":{\"id\":\"b26\"},\"end\":21012,\"start\":20800},{\"attributes\":{\"id\":\"b27\"},\"end\":21276,\"start\":21014},{\"attributes\":{\"id\":\"b28\"},\"end\":21486,\"start\":21278},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4246903},\"end\":21829,\"start\":21488},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2141740},\"end\":22209,\"start\":21831},{\"attributes\":{\"id\":\"b31\"},\"end\":22624,\"start\":22211},{\"attributes\":{\"id\":\"b32\"},\"end\":22861,\"start\":22626}]", "bib_title": "[{\"end\":12598,\"start\":12538},{\"end\":13196,\"start\":13133},{\"end\":14144,\"start\":14093},{\"end\":18056,\"start\":17997},{\"end\":19222,\"start\":19193},{\"end\":21536,\"start\":21488},{\"end\":21865,\"start\":21831}]", "bib_author": "[{\"end\":12612,\"start\":12600},{\"end\":12631,\"start\":12612},{\"end\":13213,\"start\":13198},{\"end\":13228,\"start\":13213},{\"end\":13241,\"start\":13228},{\"end\":13622,\"start\":13610},{\"end\":13637,\"start\":13622},{\"end\":13651,\"start\":13637},{\"end\":13661,\"start\":13651},{\"end\":13912,\"start\":13892},{\"end\":13932,\"start\":13912},{\"end\":13949,\"start\":13932},{\"end\":14165,\"start\":14146},{\"end\":14177,\"start\":14165},{\"end\":14190,\"start\":14177},{\"end\":14205,\"start\":14190},{\"end\":14209,\"start\":14205},{\"end\":14559,\"start\":14550},{\"end\":14572,\"start\":14559},{\"end\":14585,\"start\":14572},{\"end\":14592,\"start\":14585},{\"end\":14774,\"start\":14753},{\"end\":14787,\"start\":14774},{\"end\":14798,\"start\":14787},{\"end\":14908,\"start\":14893},{\"end\":14922,\"start\":14908},{\"end\":15111,\"start\":15096},{\"end\":15280,\"start\":15264},{\"end\":15295,\"start\":15280},{\"end\":15313,\"start\":15295},{\"end\":15334,\"start\":15313},{\"end\":15598,\"start\":15580},{\"end\":15619,\"start\":15598},{\"end\":15638,\"start\":15619},{\"end\":15938,\"start\":15922},{\"end\":15956,\"start\":15938},{\"end\":15977,\"start\":15956},{\"end\":16260,\"start\":16246},{\"end\":16276,\"start\":16260},{\"end\":16288,\"start\":16276},{\"end\":16308,\"start\":16288},{\"end\":16530,\"start\":16516},{\"end\":16539,\"start\":16530},{\"end\":16552,\"start\":16539},{\"end\":16564,\"start\":16552},{\"end\":16578,\"start\":16564},{\"end\":16594,\"start\":16578},{\"end\":16751,\"start\":16738},{\"end\":16766,\"start\":16751},{\"end\":16778,\"start\":16766},{\"end\":16795,\"start\":16778},{\"end\":16809,\"start\":16795},{\"end\":16828,\"start\":16809},{\"end\":16848,\"start\":16828},{\"end\":16862,\"start\":16848},{\"end\":16877,\"start\":16862},{\"end\":16892,\"start\":16877},{\"end\":16910,\"start\":16892},{\"end\":16930,\"start\":16910},{\"end\":16948,\"start\":16930},{\"end\":16962,\"start\":16948},{\"end\":16975,\"start\":16962},{\"end\":16990,\"start\":16975},{\"end\":17008,\"start\":16990},{\"end\":17020,\"start\":17008},{\"end\":17036,\"start\":17020},{\"end\":17055,\"start\":17036},{\"end\":17066,\"start\":17055},{\"end\":17079,\"start\":17066},{\"end\":17095,\"start\":17079},{\"end\":17805,\"start\":17787},{\"end\":17825,\"start\":17805},{\"end\":17838,\"start\":17825},{\"end\":17855,\"start\":17838},{\"end\":17870,\"start\":17855},{\"end\":18067,\"start\":18058},{\"end\":18076,\"start\":18067},{\"end\":18088,\"start\":18076},{\"end\":18101,\"start\":18088},{\"end\":18113,\"start\":18101},{\"end\":18124,\"start\":18113},{\"end\":18130,\"start\":18124},{\"end\":18361,\"start\":18348},{\"end\":18372,\"start\":18361},{\"end\":18389,\"start\":18372},{\"end\":18401,\"start\":18389},{\"end\":18417,\"start\":18401},{\"end\":18433,\"start\":18417},{\"end\":18449,\"start\":18433},{\"end\":18461,\"start\":18449},{\"end\":18481,\"start\":18461},{\"end\":18494,\"start\":18481},{\"end\":18511,\"start\":18494},{\"end\":18525,\"start\":18511},{\"end\":18538,\"start\":18525},{\"end\":18551,\"start\":18538},{\"end\":18566,\"start\":18551},{\"end\":18582,\"start\":18566},{\"end\":18603,\"start\":18582},{\"end\":18619,\"start\":18603},{\"end\":18628,\"start\":18619},{\"end\":18640,\"start\":18628},{\"end\":18658,\"start\":18640},{\"end\":19137,\"start\":19119},{\"end\":19236,\"start\":19224},{\"end\":19244,\"start\":19236},{\"end\":19255,\"start\":19244},{\"end\":19266,\"start\":19255},{\"end\":19512,\"start\":19496},{\"end\":19525,\"start\":19512},{\"end\":19687,\"start\":19670},{\"end\":19858,\"start\":19844},{\"end\":19873,\"start\":19858},{\"end\":19887,\"start\":19873},{\"end\":19905,\"start\":19887},{\"end\":19923,\"start\":19905},{\"end\":20213,\"start\":20200},{\"end\":20228,\"start\":20213},{\"end\":20243,\"start\":20228},{\"end\":20253,\"start\":20243},{\"end\":20449,\"start\":20437},{\"end\":20464,\"start\":20449},{\"end\":20478,\"start\":20464},{\"end\":20488,\"start\":20478},{\"end\":20653,\"start\":20641},{\"end\":20668,\"start\":20653},{\"end\":20682,\"start\":20668},{\"end\":20692,\"start\":20682},{\"end\":20884,\"start\":20868},{\"end\":20902,\"start\":20884},{\"end\":21122,\"start\":21108},{\"end\":21141,\"start\":21122},{\"end\":21352,\"start\":21338},{\"end\":21360,\"start\":21352},{\"end\":21364,\"start\":21360},{\"end\":21378,\"start\":21364},{\"end\":21555,\"start\":21538},{\"end\":21565,\"start\":21555},{\"end\":21589,\"start\":21565},{\"end\":21600,\"start\":21589},{\"end\":21618,\"start\":21600},{\"end\":21876,\"start\":21867},{\"end\":21895,\"start\":21876},{\"end\":21910,\"start\":21895},{\"end\":21929,\"start\":21910},{\"end\":21941,\"start\":21929},{\"end\":21956,\"start\":21941},{\"end\":21974,\"start\":21956},{\"end\":22333,\"start\":22318},{\"end\":22348,\"start\":22333},{\"end\":22365,\"start\":22348},{\"end\":22379,\"start\":22365},{\"end\":22397,\"start\":22379},{\"end\":22415,\"start\":22397},{\"end\":22709,\"start\":22691},{\"end\":22726,\"start\":22709},{\"end\":22739,\"start\":22726}]", "bib_venue": "[{\"end\":12709,\"start\":12631},{\"end\":13313,\"start\":13241},{\"end\":13608,\"start\":13517},{\"end\":13890,\"start\":13816},{\"end\":14271,\"start\":14209},{\"end\":14548,\"start\":14478},{\"end\":14751,\"start\":14717},{\"end\":15094,\"start\":15036},{\"end\":15262,\"start\":15193},{\"end\":15578,\"start\":15486},{\"end\":15920,\"start\":15801},{\"end\":16244,\"start\":16164},{\"end\":16514,\"start\":16463},{\"end\":18148,\"start\":18130},{\"end\":19299,\"start\":19266},{\"end\":19494,\"start\":19445},{\"end\":19668,\"start\":19615},{\"end\":19977,\"start\":19923},{\"end\":20198,\"start\":20125},{\"end\":20435,\"start\":20392},{\"end\":20639,\"start\":20595},{\"end\":20866,\"start\":20800},{\"end\":21106,\"start\":21014},{\"end\":21336,\"start\":21278},{\"end\":21640,\"start\":21618},{\"end\":22007,\"start\":21974},{\"end\":22316,\"start\":22211},{\"end\":22689,\"start\":22626},{\"end\":12787,\"start\":12711}]"}}}, "year": 2023, "month": 12, "day": 17}