{"id": 245986478, "updated": "2023-10-13 05:43:40.328", "metadata": {"title": "Boundary-aware Self-supervised Learning for Video Scene Segmentation", "authors": "[{\"first\":\"Jonghwan\",\"last\":\"Mun\",\"middle\":[]},{\"first\":\"Minchul\",\"last\":\"Shin\",\"middle\":[]},{\"first\":\"Gunsoo\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Sangho\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Seongsu\",\"last\":\"Ha\",\"middle\":[]},{\"first\":\"Joonseok\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Eun-Sol\",\"last\":\"Kim\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Self-supervised learning has drawn attention through its effectiveness in learning in-domain representations with no ground-truth annotations; in particular, it is shown that properly designed pretext tasks (e.g., contrastive prediction task) bring significant performance gains for downstream tasks (e.g., classification task). Inspired from this, we tackle video scene segmentation, which is a task of temporally localizing scene boundaries in a video, with a self-supervised learning framework where we mainly focus on designing effective pretext tasks. In our framework, we discover a pseudo-boundary from a sequence of shots by splitting it into two continuous, non-overlapping sub-sequences and leverage the pseudo-boundary to facilitate the pre-training. Based on this, we introduce three novel boundary-aware pretext tasks: 1) Shot-Scene Matching (SSM), 2) Contextual Group Matching (CGM) and 3) Pseudo-boundary Prediction (PP); SSM and CGM guide the model to maximize intra-scene similarity and inter-scene discrimination while PP encourages the model to identify transitional moments. Through comprehensive analysis, we empirically show that pre-training and transferring contextual representation are both critical to improving the video scene segmentation performance. Lastly, we achieve the new state-of-the-art on the MovieNet-SSeg benchmark. The code is available at https://github.com/kakaobrain/bassl.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.05277", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/accv/MunSHLHLK22", "doi": "10.1007/978-3-031-26316-3_29"}}, "content": {"source": {"pdf_hash": "bcd1dfaf6716476d19caec7fae58f821ed6daa4a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.05277v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ac7f03ec6d0a1a7cbbb2ca1541a09130af81e775", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bcd1dfaf6716476d19caec7fae58f821ed6daa4a.txt", "contents": "\nBOUNDARY-AWARE SELF-SUPERVISED LEARNING FOR VIDEO SCENE SEGMENTATION\n\n\nJonghwan Mun \nSeoul National University\nHanyang University\n\n\nMinchul Shin \nSeoul National University\nHanyang University\n\n\nGunsoo Han \nSeoul National University\nHanyang University\n\n\nKakao Brain \nSeoul National University\nHanyang University\n\n\nSangho Lee sangho.lee@snu.ac.kr \nSeoul National University\nHanyang University\n\n\nSeongsu Ha \nSeoul National University\nHanyang University\n\n\nJoonseok Lee joonseok@snu.ac.kr \nSeoul National University\nHanyang University\n\n\nEun-Sol Kim eunsolkim@hanyang.ac.kr \nSeoul National University\nHanyang University\n\n\nBOUNDARY-AWARE SELF-SUPERVISED LEARNING FOR VIDEO SCENE SEGMENTATION\n\nSelf-supervised learning has drawn attention through its effectiveness in learning in-domain representations with no ground-truth annotations; in particular, it is shown that properly designed pretext tasks (e.g., contrastive prediction task) bring significant performance gains for downstream tasks (e.g., classification task). Inspired from this, we tackle video scene segmentation, which is a task of temporally localizing scene boundaries in a video, with a self-supervised learning framework where we mainly focus on designing effective pretext tasks. In our framework, we discover a pseudo-boundary from a sequence of shots by splitting it into two continuous, non-overlapping sub-sequences and leverage the pseudo-boundary to facilitate the pre-training. Based on this, we introduce three novel boundary-aware pretext tasks: 1) Shot-Scene Matching (SSM), 2) Contextual Group Matching (CGM) and 3) Pseudo-boundary Prediction (PP); SSM and CGM guide the model to maximize intra-scene similarity and inter-scene discrimination while PP encourages the model to identify transitional moments. Through comprehensive analysis, we empirically show that pre-training and transferring contextual representation are both critical to improving the video scene segmentation performance. Lastly, we achieve the new state-of-the-art on the MovieNet-SSeg benchmark.\n\nINTRODUCTION\n\nUnderstanding long videos such as movies, for an AI system, has been viewed as an extremely challenging task. In contrast, as cognitive science (Tversky & Zacks, 2013) tells us, for humans, it is naturally achieved by breaking down a video into meaningful units (e.g., event) and reasoning about these units and their relation (Shou et al., 2021). From this point of view, dividing a long video into a series of shorter temporal segments can be considered as an essential step towards the highlevel video understanding. Motivated by this, in this paper, we tackle the video scene segmentation task, temporally localizing scene boundaries from a long video; the term scene is widely used in filmmaking, where the scene is considered as a basic unit for understanding the story of movies and is composed of a series of semantically cohesive shots.\n\nOne of the biggest challenges with video scene segmentation is that it is not achieved simply by detecting changes in visual cues. As shown in Figure 1(a), we present an example of nine shots, all of which belong to the same scene, where two characters are talking on the phone; the overall visual cues within the scene do not stay the same but rather change repeatedly when each character appears. On the other hand, Figure 1(b) shows two different scenes which contain visually similar shots (highlighted in blue) where the same character appears in the same place. Thus, it is expected that two adjacent scenes which share shots with similar visual cues need to be contextually discriminated. From this observation, it is important for the video scene segmentation task to model contextual  Figure 1: Examples of the video scene segmentation. In each row, we visualize the shots including similar visual cues (e.g., characters, places, etc.) with the same colored border. relation between shots by maximizing 1) intra-scene similarity (i.e., the shots in the same scene should be close to each other), and 2) inter-scene discrimination across two adjacent scenes (i.e., two neighbor shots across the scene boundary should be distinguishable).\n\nSupervised learning approaches (e.g., ) are clearly limited due to the lack of largescale datasets with reliable ground-truth annotations. Recently, self-supervision (Chen et al., 2020a;Caron et al., 2020;He et al., 2020;Roh et al., 2021) is spotlighted through its effectiveness in learning in-domain representation without relying on costly ground-truth annotations. The selfsupervised learning methods (Chen et al., 2021;Feichtenhofer et al., 2021;Dave et al., 2021;Qian et al., 2021) in the video domain are often designed to learn spatio-temporal patterns in short clips (e.g., shots in movies). This kind of learned representation is generic and can be applied to many video understanding tasks (e.g., action classification). However, shot-level representation is insufficient for video scene segmentation because this task requires not only a good representation for individual shots but also contextual representation considering neighboring shots at a higher level as observed in Figure 1. Motivated by this, we set our main goal to design self-supervised objectives (i.e., pretext tasks) that maximize intra-scene similarity as well as discriminate shots from different scenes by taking contextual information into account. This raises a penetrating question: how can we design boundary-relevant pretext tasks without access to ground truth boundary annotations?\n\nWe introduce a novel Boundary-aware Self-Supervised Learning (BaSSL) framework, which leverages pseudo-boundaries to learn contextual representation effective in capturing semantic transition during pre-training stage, thus leading to precise scene boundary detection. The pseudo-boundary is obtained by dividing the input sequence of shots into two semantically disjoint sub-sequences, and we use it to define boundary-relevant pretext tasks that are beneficial to the video scene segmentation task. On top of the discovered two sub-sequences and a pseudo-boundary, three boundary-aware pretext tasks are proposed: 1) Shot-Scene Matching (SSM); 2) Contextual Group Matching (CGM); and 3) Pseudo-boundary Prediction (PP). Note that SSM and CGM encourage the model to maximize intra-scene similarity and inter-scene discrimination, while PP enables the model to learn the capability of identifying transitional moments. In addition, we perform Masked Shot Modeling (MSM) task inspired by Sun et al. (2019a) to further learn temporal relationship between shots. The comprehensive analysis demonstrates the effectiveness of the proposed framework (i.e., pre-training of contextual relationship between shots) as well as the contribution of the proposed individual components (i.e., the algorithm for pseudo-boundary discovery and boundary-aware pretext tasks).\n\nOur main contributions are summarized as follows: (i) we introduce a novel boundary-aware pretraining framework which leverages pseudo-boundaries to learn contextual relationship between shots during the pre-training; (ii) we propose three boundary-aware pretext tasks, which are carefully designed to learn essential capabilities required for the video scene segmentation task; (iii) we perform extensive ablations to demonstrate the effectiveness of the proposed framework, including the observation that our framework is complementary to the existing framework; (iv) we achieve the new state-of-the-art on the MovieNet-SSeg benchmark while outperforming existing self-supervised learning-based methods by large margins.\n\n\nRELATED WORK\n\nVideo scene segmentation approaches formulate the task as a problem of temporal grouping of shots. In this formulation, the optimal grouping can be achieved by clustering-based (Rui et al., 1998;Rasheed & Shah, 2003;2005;Chasanis et al., 2008), dynamic programming-based (Han & Wu, 2011;Tapaswi et al., 2014;Rotman et al., 2017) or multi-modal input-based (Liang et al., 2009;Sidiropoulos et al., 2011) methods. However, the aforementioned methods have been trained and evaluated on small-scale datasets such as OVSD (Rotman et al., 2016) and BBC (Baraldi et al., 2015) which can produce a poorly generalized model. Recently,  introduce a largescale video scene segmentation dataset (i.e., MovieNet-SSeg) that contains hundreds of movies. Training with large-scale data,  proposes a strong supervised baseline model that performs a shot-level binary classification followed by grouping using the prediction scores. In addition, Chen et al. (2021) proposes a shot contrastive pre-training method that learns shot-level representation. We found ShotCoL (Chen et al., 2021) to be the most similar work to our method. However, our method is different from ShotCoL in that we specifically focus on learning contextual representations by considering the relationship between shots. We refer interested readers to the supplementary material for a more detailed comparison with ShotCoL.\n\nAction segmentation in videos is one of the related works for video scene segmentation, which identifies action labels of individual frames, thus can divide a video into a series of action segments. Supervised methods (Lea et al., 2016;Farha & Gall, 2019) proposed CNN-based architectures to effectively capture temporal relationship between frames in order to address an over-segmentation issue. As frame-level annotations are prohibitively costly to acquire, weakly supervised methods (Chang et al., 2019;Li & Todorovic, 2020;Souri et al., 2021;Shen et al., 2021;Zhukov et al., 2019;Fried et al., 2020) have been suggested to use an ordered list of actions occurring in a video as supervision. Most of the methods are trained to find (temporal) semantic alignment between frames and a given action list using an HMM-based architecture (Kuehne et al., 2018), a DP-based assignment algorithm (Fried et al., 2020) or a DTW-based temporal alignment method (Chang et al., 2019). Recently, unsupervised methods (Kumar et al., 2021;Wang et al., 2021;Kukleva et al., 2019;Li & Todorovic, 2021;VidalMata et al., 2021) have been further proposed; in a nutshell, clustering-based prototypes are discovered from unlabeled videos, then the methods segment the videos by assigning prototypes (corresponding to one of the actions) into frames. In contrast to the action segmentation task that is limited to localizing segments each of which represents a single action within an activity, video scene segmentation requires localizing more complex segments each of which may be composed of more than two actions (or activities).\n\nSelf-supervised learning in videos has been actively studied for the recent years with approaches proposing various pretext tasks such as future frame prediction (Srivastava et al., 2015;Vondrick et al., 2016;Ahsan et al., 2018), temporal ordering of frames (Misra et al., 2016;Lee et al., 2017;Xu et al., 2019), geometric transformations prediction (Jing & Tian, 2018), colorization of videos (Vondrick et al., 2018) and contrastive prediction (Feichtenhofer et al., 2021;Qian et al., 2021;Dave et al., 2021). In addition, CBT (Sun et al., 2019a;b) proposes a pretext task of masked frame modeling to learn temporal dependency between frames (or clips). Note that since most of those methods are proposed for the classification task, they would be sub-optimal to the video scene segmentation task. On the other hand, BSP  proposes boundary-sensitive pre-text tasks based on synthesized pseudo-boundaries that are obtained by concatenating two clips sampled from different videos. However, strictly speaking, BSP is not a self-supervised learning algorithm since it requires video-level class labels to synthesize pseudo-boundaries; the proposed pretext tasks are not applicable to videos such as movies that are hard to define semantic labels. Also, note that we empirically show that pseudo-boundaries identified by our method are more effective for pre-training than synthesized pseudo-boundaries.\n\n\nBOUNDARY-AWARE SELF-SUPERVISED LEARNING (BASSL)\n\nIn this section, we introduce our proposed approach, Boundary-aware Self-Supervised Learning (BaSSL). We start with the problem formulation followed by the overview. Then, we describe our novel boundary-aware pretext tasks for pre-training.\n\n\nPROBLEM FORMULATION\n\nTerminologies A video (e.g., documentaries, TV episodes and movies) is assumed to have a hierarchical structure at three-level semantics: scene, shot and frame. In detail, a video is a sequence of scenes, defined as a semantic unit for making a story. A scene is a series of shots, which is a set of frames physically captured by the same camera during an uninterrupted period of time.\n\nVideo Scene Semantic Segmentation Task Given a video, which contains a series of N shots {s 1 , ..., s N } with class labels {y 1 , ..., y N } where y i \u2208 {0, 1} indicating if it is at the scene boundary (more precisely, if it is the last shot of a scene), the video scene segmentation task is formulated as a simple binary classification problem at individual shot level. By definition, a scene boundary is where the semantic of a shot is considerably different from its (one-way) neighbors. Thus, it is in nature important to capture and leverage contextual transition across the scenes. Consequently, it is a common practice that the information of the neighbor shots are leveraged together when determining scene boundaries. With this formulation, existing approaches Chen et al., 2021) adopt a sliding window scheme with a window S n = {s n\u2212K , ..., s n , ..., s n+K } containing a sequence of 2K + 1 shots centered at the n th shot, s n , where K is the number of neighbor shots before and after s n . Then, the supervised learning methods typically train a parameterized (\u03b8) model by maximizing the expected log-likelihood:\n\u03b8 * = arg max \u03b8 E [log p \u03b8 (y n |S n )] .\nNote that each shot s is given by a set of N k key-frames, resulting in a tensor with size of (N k , C, H, W ) where C, H and W are the RGB channels, the height and the width, respectively.\n\n\nMODEL OVERVIEW\n\nOur method is based on two-stage training following common practice (Chen et al., 2021): pretraining on large-scale unlabeled data with self-supervision and fine-tuning on relatively small labeled data via transfer learning. Our main focus is in the pre-training stage, aiming at designing effective pretext tasks for video scene segmentation.\n\nAs illustrated in Figure 2, the model (\u03b8) consists of two main components: 1) shot encoder embedding a shot by capturing its spatio-temporal patterns, and 2) contextual relation network (CRN) capturing relationship between shots. Given a sequence S n = {s n\u2212K , ..., s n , ..., s n+K } of shots centered at s n , two-level representations are extracted as follows:\ne n = f ENC (s n ) and C n = f CRN (E n ),\nwhere f ENC : R N k \u00d7C\u00d7H\u00d7W \u2192 R De and f CRN : R (2K+1)\u00d7De \u2192 R (2K+1)\u00d7Dc represent the shot encoder and the contextual relation network while D e and D c mean dimensions of encoded and contextualized features, respectively. e n is an encoding of shot s n by f ENC while E n = {e n\u2212K , ..., e n , ..., e n+K } and C n = {c n\u2212K , ..., c n , ..., c n+K } correspond to the input and output feature sequence for f CRN , respectively. Figure 4: An example in each row shows a sequence of shots sampled from the same scene where there exists no ground-truth scene-level boundary. Our method finds a pseudo-boundary shot (highlighted in red) that divides a sequence into two pseudo-scenes (represented by green and orange bars, respectively) so that semantics (e.g., places, characters) maximally changes.\n\nOn top of encoded shot representations, BaSSL extracts a pseudo-boundary (left box of Figure 2) to self-supervise the model instead of relying on ground-truth annotations. To be specific, we leverage the dynamic time warping technique to divide the input sequence of shots into two semantically disjoint sub-sequences and output a pseudo-boundary. (See Section 3.3 for more details.)\n\nThen, as presented in Figure 3, using the discovered pseudo-boundary, we devise three novel boundary-aware pretext tasks in Section 3.4: 1) Shot-Scene Matching to match shots with their associated scenes, 2) Contextual Group Matching to align shots whether they belong to the same scene or not and 3) Pseudo-boundary Prediction to capture semantic changes. In addition, we adopt the masked shot modeling in CBT (Sun et al., 2019a) to further learn temporal relationship between shots. After pre-trained with the four pretext tasks, the model is fine-tuned with labeled video scene segmentation data. (See Section 3.5 for more details.)\n\n\nPSEUDO-BOUNDARY DISCOVERY\n\nThe goal of our pre-training is to make a model effective in capturing the semantic transition, thus leading to higher performance in downstream task (i.e., video scene segmentation). For this purpose, we leverage a pseudo-boundary-a probable moment where the actual semantic transition occurs-as a clue for self-supervision. Given an input sequence, we simply find a single moment (or boundary) with the maximum semantic transition. Note that even a sequence of shots without strong scene-level semantic transition, there always exists a shot within the sequence across which the semantic transition is maximum, and we use this shot as a pseudo-boundary. Figure 4 shows examples where we intentionally infer a pseudo-boundary based on a sequence of shots sampled from the same scene (that is, no scene boundary exists according to the ground truth); we observe that the resulting two sub-sequences are cognitively distinguishable.\n\nThe process, dividing the input sequence S n into two continuous, non-overlapping sub-sequences S left n and S right n with maximum semantic transition, can be seen as a temporal alignment problem between S n and S slow n ; specifically, observing the first shot should belong to S left n and the last one to S right n , we define S slow n = {s n\u2212K , s n+K }, which can be seen as a same video with S n with lower sampling frequency. Then, the problem becomes aligning intermediate shots either to the first shot s n\u2212K or the last shot s n+K while preserving continuity.\n\nUnder the problem setting, we adopt dynamic time warping (DTW) (Berndt & Clifford, 1994) to find the optimal alignment between S n and S slow n . In detail, DTW solves the following optimization problem using dynamic programming to maximize semantic coherence of the resulting two subsequences among all possible boundary candidates:\nb * = arg max b=\u2212K+1,...,K\u22121 1 b + K b i=\u2212K+1 sim(e n\u2212K , e n+i ) + 1 K \u2212 b \u2212 1 K\u22121 j=b+1 sim(e n+K , e n+j ),\nwhere b is the candidate boundary offset, b * is the optimal boundary offset, and sim(x, y) = x y x y computes cosine similarity between encodings of the given two shots. Two sub-sequences are inferred as S left n = {s n\u2212K , ..., s n+b * } and S right n = {s n+b * +1 , ..., s n+K }. s n+b * is the pseudoboundary shot, which is the last shot of S left n . More examples of pseudo-boundaries identified by our algorithm is presented in Figure 7 of the supplementary material. The results are used for learning boundary-aware pretext tasks, which will be described below.\n\n\nPRE-TRAINING OBJECTIVES\n\nAs illustrated in Figure 3, we propose three novel boundary-aware pretext tasks-1) shot-scene matching, 2) contextual group matching and 3) pseudo-boundary prediction-and adopt an additional standard pretext task (i.e., masked shot modeling).\n\nShot-Scene Matching (SSM) The objective of this task is to make the representations of a shot and its associated scene similar to each other, while the representations of the shot and other scenes dissimilar. In other words, SSM encourages the model to maximize intra-scene similarity, while minimizing inter-scene similarity. Considering the splitted two sub-sequences (S left n and S right n ) as pseudo-scenes, we train the model using the InfoNCE loss (Oord et al., 2018):\nL SSM = L NCE h SSM (e n\u2212K ), h SSM (r(S left n )) + L NCE h SSM (e n+K ), h SSM (r(S right n )) , L NCE (e, r) = \u2212 log e sim(e,r)/\u03c4 e sim(e,r)/\u03c4 + \u0113\u2208N e e sim(\u0113,r)/\u03c4 + r\u2208N r e sim(e,r)/\u03c4 ,(1)\nwhere h SSM is a SSM head of a linear layer, \u03c4 is a temperature hyperparameter and r(S) means a scene-level representation; we use the averaged encoding of shots in the sub-sequence S. N e and N r in Eq.\n\n(1) are constructed using other shots and pseudo-scenes in a mini-batch, respectively.\n\nContextual Group Matching (CGM) Since directly matching representations of shots and scenes would not be effective when the scenes are composed of visually dissimilar shots, CGM is introduced to bridge this gap. Similar to SSM, CGM is also designed to maximize intra-scene similarity and inter-scene discrimination. However, CGM measures semantic coherence of the shots rather than comparing visual cues. With CGM, the model learns to decide if the given two shots belong to the same group (i.e., scene) or not. In detail, we use the center shot s n in the input sequence as the anchor and construct a triplet of (s n , s pos , s neg ). We sample each shot from S left n and S right n ; the one sampled within the same sub-sequence with s n is used as the positive shot s pos , while the other as the negative s neg . The CGM loss is defined using a binary cross-entropy loss as follows:\nL CGM = \u2212 log (h CGM (c n , c pos )) \u2212 log (1 \u2212 h CGM (c n , c neg )) ,\nwhere h CGM is a CGM head taking two shots as input and predicting a matching score. c n , c pos and c neg are the contextualized features by f CRN for the center, positive and negative shots, respectively.\n\nPseudo-boundary Prediction (PP) Through the above two pretext tasks, our model learns the contextual relationship between shots. In addition to these, we design an extra pretext task, PP, which is more directly related to boundary detection; PP makes the model have a capability of identifying transitional moments that semantic changes. Based on the pseudo-boundary shot and one randomly sampled non-boundary shot, the PP loss is defined as a binary cross-entropy loss:\nL PP = \u2212 log (h PP (c n+b * )) \u2212 log (1 \u2212 h PP (cb)) ,\nwhere h PP is a PP head that projects the contextualized shot representation to a probability distribution over binary class. c n+b * and cb indicate the contextualized representation from f CRN for the pseudo-boundary shot s n+b * and randomly sampled non-boundary shot sb, respectively.\n\nMasked Shot Modeling (MSM) Inspired by masked frame modeling (Sun et al., 2019a;b), we adopt the MSM task whose goal is to reconstruct the representation of masked shots based on the their surrounding shots. In this task, given a set of encoded shot representations, we randomly apply masking each of them with a probability of 15%. For a set M of masked shot offsets, we learn to regress the output on each masked shot to its encoded shot representation, which is given by\nL MSM = m\u2208M e m \u2212 h MSM (c m ) 2 2 ,(2)\nwhere h MSM is a MSM head to match the dimension of contextualized shot representation with that of encoded one. e m and c m denote the encoded representation by f ENC and contextualized representation by f CRN for a masked shot s m , respectively.   and , respectively. indicates the methods exploiting additional modalities or semantics (e.g., audio, place, cast, transcript). The best numbers are in bold. Pre-training loss The final pre-training loss is defined by\nL pretrain = \u03b1 1 L SSM + \u03b1 2 L CGM + \u03b1 3 L PP + \u03b1 4 L MSM ,\nwhere \u03b1 1 , \u03b1 2 , \u03b1 3 , and \u03b1 4 are hyperparameters to balance the pretext tasks while all are set to 1.\n\n\nFINE-TUNING FOR SCENE BOUNDARY DETECTION\n\nRecall that we formulate the video scene segmentation as a binary classification task to identify contextual transition across the scene. Different from the pre-training stage, given an input sequence of shots S n , we employ a scene boundary detection head h SBD to infer a prediction from the contextualized representation (c n ) for the center shot s n . Following Chen et al. (2021), we freeze the parameters of the shot encoder and then train only the contextual relation network and the scene boundary detection head using a binary cross-entropy loss with the ground truth label y n as follows:\n\nL finetune = \u2212y n log(h SBD (c n )) + (1 \u2212 y n ) log(1 \u2212 h sbd (c n )).\n\nNote that, with a sidling window scheme, individual shots are decided to be a scene boundary when its prediction score is higher than a pre-defined threshold (set to 0.5).\n\n\nEXPERIMENT\n\n\nEXPERIMENTAL SETTINGS\n\nDataset We evaluate our proposed method on the MovieNet-SSeg dataset  that is a sub-dataset of MovieNet, containing 1,100 movies with 1.6M shots. Note that only 318 out of 1,100 movies have scene boundary annotations, which are divided into 190, 64, and 64 movies for training, validation, and test split, respectively. Following Chen et al. (2021), we use the entire 1,100 movies with no ground truth labels for the pre-training and fine-tune the model on the training split. The performance is measured on the test split.\n\nMetric Following Huang et al. (2020), we compare algorithms using Average Precision (AP) and mIoU that measures the averaged intersection over union (IoU) between predicted scene segments and their closest ground truth scene segments. Also, we adopt F1 score and AUC-ROC as additional evaluation metrics. Note that contrary to the previous works Chen et al., 2021) that report recall, we use F1 score to consider for balanced comparison between precision and recall. In addition, we report Meta-Sum metric inspired by the works (Chen et al., 2020b;Li et al., 2021) for easy and straightforward comparison of algorithms. Implementation details We employ ResNet-50 (He et al., 2016) and Transformer (Vaswani et al., 2017) as the shot encoder and the contextual relation network, respectively. For both pre-training and fine-tuning stages, we cross-validate the number of neighbor shots among K = {4, 8, 12, 16} and K = 8 is selected due to its good performance and computational efficiency. In all experiments, given a pre-trained model, we fine-tune the model 5 times with different random seeds and report their average score and standard deviation. More details are presented in supplementary material.\n\n\nCOMPARISON WITH STATE-OF-THE-ART METHODS\n\nWe compare our method, BaSSL, with 1) supervised methods including Siamese (Baraldi et al., 2015), MS-LSTM  and LGSS , 2) unsupervised methods including GraphCut (Rasheed & Shah, 2005), SCSA (Chasanis et al., 2008), DP (Han & Wu, 2011), StoryGraph (Tapaswi et al., 2014) and Grouping (Rotman et al., 2017), and 3) self-supervised methods including ShotCoL (Chen et al., 2021). Without fine-tuning on the downstream task, BaSSL can be seen as an unsupervised model in that it is trained to predict the pseudo-boundary by the PP task. Table 1 summarizes comparison against competing methods. BaSSL without fine-tuning shows competitive or outperforming performance based only on basic visual features compared to competing unsupervised methods; note that the method, Grouping, leverages additional modalities (e.g., audio and transcripts). Furthermore, fine-tuning BaSSL with ground-truth scene boundaries, AP is improved by 24.71%p and BaSSL outperforms all other algorithms. Finally, through longer pre-training (40 epochs), BaSSL surpasses the state-of-the-art method (i.e., ShotCoL) by a large margin (4.00%p in AP).\n\n\nCOMPARISON WITH PRE-TRAINING BASELINES\n\nWe perform extensive experiments to compare BaSSL with the other pre-training baselines that learn shot-level representation by f ENC . In the experiments, we compare the following three types of pretraining approaches; The first group (M1-2) trains f ENC using image-level supervision with object labels on ImageNet (Deng et al., 2009) or place labels on Places365 (Zhou et al., 2017). The second group (M3-5) trains f ENC through shot-level contrastive learning (i.e., SimCLR proposed by Chen et al. (2020a)) with different positive pair sampling strategies. Specifically, Instance (M3) takes an instance of the center shot with different augmentation, Temporal (M4) takes one randomly sampled neighbor shot as positive pair in local temporal window, and Nearest Neighbor (NN) (M5) takes the most visually similar shot among the neighbor shots as positive pair, which is also known as ShotCoL (Chen et al., 2021). The last group (M6-8) learns both f ENC and f CRN through boundaryaware pretext tasks proposed in this paper. Given pre-trained representations of f ENC , we train a video scene segmentation model with three different types of f CRN including MLP (Chen et al., 2021), MS-LSTM  1 and Transformer. For fair comparison, all pre-training methods employ ResNet-50 as the shot encoder f ENC and we pre-train the models for 10 epochs.  In Table 2, we found the following observations. First, when transferring pre-trained shot representation, employing MS-LSTM and Transformer as f CRN is more effective than using MLP, as they are favorably designed to capture contextual relation between shots (see M1-6). Second, BaSSL (M7) outperforms all competing baselines (M1-5) through learning contextual representation during pre-training. Also, it turns out that transferring the representation through f CRN is important for the boundary detection task where it leads to a performance gain of 5.44%p in AP (see M6-7). Finally, learning shot-level and contextual representations is complementary to each other; that is, incorporating ShotCoL (M5) and our framework (M7) provides further improved performance (M8).\n\n\nABLATION STUDIES\n\n\nImpact of individual pretext tasks\n\nWe investigate the contribution of individual pretext tasks. In this experiment, we train models by varying the combinations of the pretext tasks. From Table 3, we can obtain following two observations. First, when training a model with a single pretext task (P1-4), the MSM task leads to the worst performance compared to the others. This indicates that boundary-aware pretext tasks (i.e., SSM, CGM and PP) to learn contextual relation between shots is indeed important for video scene segmentation. Second, the more pretext tasks we include during pre-training, the better the performance is, and the best performance is obtained when using all tasks (P15). This means all tasks are complementary to each other, contributing to performance gain.\n\nPsuedo-boundary discovery method To check the effectiveness of DTW-based pseudo-boundary discovery, we train three models with different pseudo-boundary decision strategies-1) Random defining one randomly sampled shot in the input sequence as a pseudo-boundary, 2) Fixed always taking the center shot as a pseudo-boundary, and 3) Synthesized, inspired by , synthesizing the input sequence by concatenating two sub-sequences sampled from different movies and using the last shot of the first sub-sequence as a pseudo-boundary. Table 4(a) summarizes the results. Random and Fixed pseudo-boundaries hinder the learning and degenerate the boundary detection performance. It is notable that BaSSL with Synthesized pseudo-boundaries also outperforms the pre-training baselines in Table 2, which shows the effectiveness of our framework and importance of pre-training contextualized representation. Finally, adopting DTW to find pseudoboundaries achieves the best performance.\n\nHyperparameters We analyze the impact of two key hyperparameters: 1) the number of neighbor shots K and 2) pre-training epochs. Table 4(b) shows that we achieve higher performance with more neighbor shots, saturating around K = 12. Table 4(c) shows the impact of longer pre-training. We find that performance increases until certain numbers (i.e., 40 epochs) and decrease afterward. We conjecture that this is partly due to overfitting to noise from incorrect pseudo-boundaries.\n\n\nCONCLUSION\n\nWe present BaSSL, a novel self-supervised framework for video scene segmentation, especially designed to learn contextual relationship between shots. Through the pseudo-boundary discovery, we can define and conduct boundary-aware pretext tasks that encourage the model to learn the contextual relational representation and a capability of capturing transitional moments. Comprehensive experiments demonstrate the effectiveness of our framework and we achieve outstanding performance in the MovieNet-SSeg dataset.\n\n\nA ADDITIONAL IMPLEMENTATION DETAILS\n\nAdditional details of the shot encoder (i.e., ResNet-50) and the contextual relation network (i.e., Transformer) are as follows. For the shot encoder, each shot is given by three key-frames (i.e., N k = 3) and a shot encoding e is given by the averaged feature after inferring individual three key-frames using ResNet-50; note that, to speed up the training, we use randomly sample one key-frame out of three during the pre-training. For Transformer, the hyperparameters are set to (L = 2, H = 768, A = 8) where L, H and A mean the number of stacked transformer blocks, the dimension of hidden activation and the number of attention heads, respectively. We apply the Dropout technique (Srivastava et al., 2014) on hidden states and attention weights with a probability of 10% and use GELU (Hendrycks & Gimpel, 2016) as an activation function.\n\nFor data augmentation of key-frames in a shot, we adopt PyTorch's torchvision package. Given a sequence of shots, we apply random crop (with resize), random flip, random color jitter and random Gaussian blur. In detail, firstly, the cropping is performed with a random size (i.e., scales between [0.14, 1.0] of the original size) and a random aspect ratio (between 3/4 to 4/3), and then the cropped one is resized to (224,224). Secondly, we apply a random horizontal flip with a probability of 50%. Thirdly, as a color augmentation, we perform a random color jitter (with a probability of 80%) and a random color dropping to gray scale (with a probability of 20%). The color strength parameters for jittering are set to {brightness, contrast, saturation, hue} = (0.2, 0.2, 0.2, 0.05). Finally, Gaussian blur is applied with a probability of 50% where a standard-deviation of spatial kernel is set to [0.1, 2.0]. Note that the same augmentations are applied to all key-frames in the input sequence S n of shots while different color jittering is applied on individual shots. Also, for S slow n , we perform a different augmentation compared to that applied on S n .\n\nDuring the pre-training stage, the model parameters are randomly initialized and then trained using the proposed pretext tasks. We use LARS (You et al., 2017) to learn the model (except for parameters of bias and Batch-Normalization) with a mini-batch of 256 shot sequences, a base learning rate of 0.3, momentum of 0.9, weight decay of 10 \u22126 and trust coefficient of 0.001. We pre-train the model for 10 epochs with a linear warm-up strategy for 1 epoch (i.e., 10% of whole training epochs) followed by learning rate decaying with a cosine schedule. The temperature \u03c4 in Eq. (1) is set to 0.1. Using 16 V100 GPUs with mixed precision training, it takes less than 2 days for pre-training.\n\nIn the fine-tuning stage, we initialize the parameters of the shot encoder and the contextual relation network by that of the pre-trained ones. However, we freeze the parameters of the shot encoder following Chen et al. (2021). We fine-tune the contextual relation network and the scene boundary detection head for 20 epochs using Adam (Kingma & Ba, 2015) with a learning rate of 10 \u22125 and a mini-batch of 1024 training examples. The learning rate is decayed with a cosine schedule without a warm-up stage.\n\n\nB COMPARISON WITH SHOT-LEVEL SELF-SUPERVISED LEARNING\n\nAs mentioned in the main paper, our approach is distinguishable from the shot-level pre-training approach (Chen et al., 2021) in that the objectives used in our approach (BaSSL) is to learn contextual representations by taking neighbor shots into account. Figure 5 provides a clear summary of comparison between shot-level pre-training and our boundary-aware pre-training, BaSSL. Firstly, shot-level pre-training takes a pair of two shots as an input while BaSSL takes a sequence of shots. Secondly, shot-level pre-training aims to train shot encoder (f ENC ) only, while BaSSL trains both the shot encoder and the contextual relation network (f ENC and f CRN ). In contrast to the shot-level pre-training that requires to train f CRN from scratch during the fine-tuning stage, BaSSL benefits from weight transfer by pre-training the parameters of f CRN with large-scale in-domain data in advance. Note that the results (M6-7) in Table 2 show that the weight transfer of f CRN is important to improve the video scene segmentation performance. Finally, the contrastive learning objective in shot-level pre-training drives the representations of two shots (query and positive) to be close to each other, whereas Shot-Scene Matching objective in our approach performs the same task but with a shot (query) and its associated scene (positive; a sequence of shots). The Table 5 summarizes the aforementioned comparisons. Figure 5: Comparison between existing approaches and ours for video scene segmentation. The existing approach focuses only on learning shot-level representation given by shot encoder (f ENC ). In contrast, our boundary-aware pre-training method focuses on learning contextual representation by taking neighbor shots into account. Thus, our method can learn both the shot encoder (f ENC ) and the contextual relation network (CRN; f CRN ) and transfer their parameters during the fine-tuning stage.    Table 6 shows the data statistics of different video scene segmentation datasets. We found the limited number of datasets that provide the scene boundary annotations and, as far as we know, the MovieNet-SSeg  is the largest-scale video scene segmentation dataset. We further compare BaSSL with shot-level pre-training baselines on two additional datasets-BBC (Baraldi et al., 2015) and OVSD (Rotman et al., 2016). Note that the training and test splits are not available and the dataset size is extremely limited (11 and 21 videos in BBC and OVSD, respectively); in addition, 2 out of 21 videos in OVSD is not available. Thus, we infer predictions using models trained on MovieNet-SSeg without fine-tuning on BBC and OVSD. The results are summarized in Table 7. The result shows the superiority of our method compared to shot-level pre-training baselines.  The normalized mutual information (NMI) is a metric for clustering algorithms (e.g., K-Means), which measures the clustering quality. Since clustering with good representations forms clear boundaries between different classes, NMI can be considered as a proxy to measure the quality of our pre-trained models. Specifically, we randomly sample 100 scenes from the test split of MovieNet-SSeg while we vary the length of scenes N c \u2208 {8, 16, 32}. Then, we perform K-Means clustering on N c \u00d7 100 shot representations extracted by the pre-trained model with the number of classes K=100. This intends to form a single cluster for each scene, assuming that high-quality representation for movie scene segmentation would locate the shot embeddings within the same scene close to each other. Considering the randomness in the K-Means clustering and scene sampling, we report the averaged score from five trials.\n\nIn Table 8, we compare the NMI score between different pre-trained models; SimCLR (NN) is our SimCLR version implementation of ShotCoL. The result shows that BaSSL outperforms the shot-level pre-training baselines and the model pre-trained using ImageNet dataset. With respect to different scene lengths (N c ; the number of shots included in a single scene), we found our BaSSL is more robust than the other baselines. Since the visual diversity across the shots increases as the scenes become longer (N c =8 \u2192 32), it is natural that the NMI score for each baseline is degraded. However, it is remarkable that, by increasing the number of shots from 8 to 32, the performance of BaSSL drops only -0.68% while the other baselines suffer from severe degradation. This demonstrates the effectiveness of BaSSL in maximizing intra-scene similarity.\n\nIn addition, we perform ablation study of our algorithm by adding pretext tasks one by one, and measure the corresponding NMI scores. The result in Table 9 shows that better NMI score is achieved as more pretext tasks are combined together. This tendency is also observed in our ablation in Table 3, which indicates the NMI score of pre-trained models is highly correlated with the final performance after the fine-tuning.\n\n\nF QUALITATIVE ANALYSIS\n\nVisualization of similarities between consecutive shots To qualitatively check the effect of individual pretext tasks, we visualize the matrix of cosine similarity between shot representations from the randomly sampled 16 consecutive shots in Figure 6. The shot representations are computed by models without the fine-tuning in order to solely focus on the behavior of each objective at the pretraining stage. When the MSM is used only, approximately three clusterings are shown, but similarity around boundaries is smoothed. Next, when we add PP, dissimilarities around the boundaries are to be sharpened. Then, with additional CGM, the clusters are more clearly obtained. Finally, adding SSM makes the similarity of shots within the same cluster higher (i.e., more yellow ones). Figure 6: Visualization of similarity (below) between shot representations in randomly sampled consecutive shots (above). We observe that the shot representations are clearly clustered as adding pretext tasks one by one.\n\n\nPseudo-boundaries\n\nWe compare the quality of discovered pseudo-boundaries with the ground truth scene boundaries in Figure 7. In most cases, we observe the pseudo-boundaries identified by the DTW algorithm are successfully located in close distance with the ground truth ones. This result validates our idea considering the problem of discovering pseudo-boundary as a temporal alignment problem between two sequences with different frequencies (S n and S slow n ). At the same time, we illustrate the failure cases. Although discovered pseudo-boundary does not match the ground truth in this case, we figure the determined boundary is not always arbitrary. For example, the mismatch is often caused by the noise existing in the ground truth (see the first row in the failure cases). On the other hand, in case all shots are visually similar (see the third row in the failure cases), the DTW solely relying on the visual modality fails to find the correct boundary.\n\nPredicted scene boundaries The figure 8 illustrates the scene boundary predictions of different models. Comparing with the baselines, we observe that our approach, BaSSL, shows qualitatively better result for video scene segmentation. On the other hand, we observe the over-segmentation issue in many cases using any competing methods (including ours). Our finding implies that achieving the highest recall only does not guarantee the highest performance in practice. We reckon that further studies on this over-segmentation problem would be a highly important topic when it comes to real-world application. Figure 8: Comparison of boundary detection results from three pre-training approaches: ImageNet pre-trained ResNet, ShotCoL, and BaSSL. The first row shows the reference that is composed of two adjacent scenes divided by the ground truth boundary. We visualize the shots that are assigned to the same scene segments with the same colored border.\n\nFigure 2 :Figure 3 :\n23Overall pipeline of our proposed framework, BaSSL. Illustration of four pre-training pretext tasks.\n\nFigure 7 :\n7Comparison between the ground truth scene boundaries and the discovered pseudoboundaries based on the DTW algorithm. The examples are sampled from the MovieNet-SSeg dataset. All boundary shots are highlighted in red.\n\nTable 1 :\n1Comparison with other algorithms. \u2020 and \u2021 denote that the numbers are copied from\n\nTable 2 :\n2Average precision (AP) comparison with pre-training baselines. Note that SimCLR (NN) corresponds to our reproduced ShotCoL using SimCLR as the constrastive learning scheme.Method \nPre-training \nTransfer \nArchitecture of f CRN during fine-tuning \nf ENC f CRN f ENC f CRN \nMLP \nMS-LSTM \nTransformer \nSupervised pre-training using image dataset \nM1 ImageNet \n43.12 \u00b10.14 45.10 \u00b10.55 47.13 \u00b11.04 \nM2 Places365 \n43.82 \u00b10.10 45.87 \u00b10.40 48.71 \u00b10.50 \nShot-level pre-training \nM3 SimCLR (instance) \n45.60 \u00b10.07 49.09 \u00b10.24 51.51 \u00b10.31 \nM4 SimCLR (temporal) \n45.55 \u00b10.11 49.24 \u00b10.26 50.05 \u00b10.78 \nM5 SimCLR (NN) \n45.99 \u00b10.13 50.73 \u00b10.19 51.17 \u00b10.69 \nBoundary-aware pre-training \nM6 BaSSL \n46.53 \u00b10.11 50.58 \u00b10.14 50.82 \u00b10.69 \nM7 BaSSL \n-\n-\n56.26 \u00b10.04 \nM8 M5+M7 \n-\n-\n56.86 \u00b10.01 \n\n\n\nTable 3 :\n3Ablation study on varying combinations of pretext tasks for pre-training. The best scores are highlighted in bold.Pretext Tasks \nEvaluation Metric \nSSM CGM PP MSM \nAP \nmIoU \nAUC-ROC \nF1 \nSum \nP1 \n42.57 \u00b10.29 40.12 \u00b10.50 84.11 \u00b10.15 30.83 \u00b10.79 197.63 \nP2 \n36.76 \u00b10.02 40.59 \u00b10.18 82.06 \u00b10.04 30.94 \u00b10.32 190.35 \nP3 \n36.55 \u00b10.04 39.58 \u00b10.05 81.36 \u00b10.03 29.96 \u00b10.04 187.45 \nP4 \n13.33 \u00b10.23 29.80 \u00b10.39 64.65 \u00b10.98 18.68 \u00b10.39 126.45 \nP5 \n55.77 \u00b10.05 48.19 \u00b10.21 90.19 \u00b10.03 43.17 \u00b10.39 237.32 \nP6 \n56.04 \u00b10.08 49.00 \u00b10.16 90.13 \u00b10.02 44.74 \u00b10.29 239.91 \nP7 \n38.09 \u00b10.03 41.25 \u00b10.10 82.85 \u00b10.01 32.24 \u00b10.24 195.43 \nP8 \n54.39 \u00b10.07 47.54 \u00b10.18 89.72 \u00b10.03 42.48 \u00b10.22 234.13 \nP9 \n39.49 \u00b10.04 41.71 \u00b10.12 83.27 \u00b10.02 32.85 \u00b10.20 197.32 \nP10 \n38.53 \u00b10.07 40.85 \u00b10.15 82.78 \u00b10.04 31.47 \u00b10.16 193.63 \nP11 \n41.02 \u00b10.07 40.89 \u00b10.10 83.79 \u00b10.02 31.53 \u00b10.18 197.23 \nP12 \n56.10 \u00b10.08 49.10 \u00b10.17 90.09 \u00b10.03 45.42 \u00b10.30 240.71 \nP13 \n56.20 \u00b10.06 48.00 \u00b10.17 90.13 \u00b10.01 43.24 \u00b10.27 237.57 \nP14 \n56.26 \u00b10.02 48.42 \u00b10.33 90.25 \u00b10.01 43.98 \u00b10.58 238.91 \nP15 \n56.26 \u00b10.04 49.50 \u00b10.11 90.27 \u00b10.02 45.70 \u00b10.24 241.73 \n\n\n\nTable 4 :\n4Ablations to check the impact of pseudo-boundary discovery strategies, the number of neighboring shots (K) and longer pre-training. The best scores are in bold.Pseudo-boundary \nAP \nRandom \n46.64 \u00b10.37 \nFixed \n49.53 \u00b10.32 \nSynthesized \n54.61 \u00b10.03 \nDTW (ours) \n56.26 \u00b10.04 \n\n(a) Performance comparison depend-\ning on the pseudo-boundary discovery \nmethods. \n\n# Neighbors \nAP \n4 \n55.98 \u00b10.10 \n8 \n56.26 \u00b10.04 \n12 \n56.29 \u00b10.03 \n16 \n55.31 \u00b10.04 \n\n(b) Performance comparison \nwhen varying the number of \nneighbor shots. \n\nEpochs \nAP \n10 \n56.26 \u00b10.04 \n20 \n56.74 \u00b10.04 \n30 \n56.74 \u00b10.07 \n40 \n57.40 \u00b10.08 \n50 \n57.15 \u00b10.08 \n\n(c) Performance comparison \nwith respect to the number of \npre-training epochs. \n\n\n\nTable 5 :\n5Comparison between the shot-level pre-training and the proposed pre-training approaches.Check List \nShot-level Pre-training \nBoundary-aware Pre-training \nNetwork architecture \nf ENC \nf ENC + f CRN \nTraining input \na pair of shots (#shots: 2) a sequence of shots (#shots: 2K+1) \nWeights transferable for f ENC ? \nyes \nyes \nWeights transferable for f CRN ? \nno \nyes \nPositive pair in contrastive learning \nshot-shot \nshot-scene \n\n\n\nTable 6 :\n6Comparison of existing video scene segmentation datasets. Note that we brought the table from with an update on the MovieNet-SSeg dataset.Dataset \n#Video #Scene #Shot Time (h) \nSource \nBBC (Baraldi et al., 2015) \n11 \n670 \n4.9K \n9 \nDocumentary \nOVSD (Rotman et al., 2016) \n21 \n300 \n10K \n10 \nMiniFilm \nMovieNet-SSeg (Huang et al., 2020) \n318 \n42K \n500K \n-\nMovies \nMovieNet (Huang et al., 2020) \n1,100 \n-\n1.6M \n-\nMovies \n\n\n\nTable 7 :\n7Comparison between our method and shot-level pre-training baselines on BBC and OVSD datasets. The numbers mean AP and the best model is highlighted in bold.Model SimCLR (instance) SimCLR (temporal) SimCLR (NN) BaSSL \nBBC \n32.34 \n34.18 \n32.92 \n39.98 \nOVSD \n25.45 \n24.92 \n25.02 \n28.68 \n\nC RESULTS ON ADDITIONAL DATASETS \n\n\n\nTable 8 :\n8Scene clustering quality measured by normalized mutual information (NMI) metric.ModelSceneLength \u2206 \u2193 (Short \u2192 Long) Short (N c =8) Medium (N c =16) Long (N c =32)ImageNet \n67.50 \n61.60 \n56.25 \n-16.67% \nSimCLR (temporal) \n82.40 \n81.65 \n78.99 \n-4.14% \nSimCLR (NN) \n83.54 \n83.17 \n81.25 \n-2.75% \nBaSSL (ours) \n86.22 \n86.72 \n85.63 \n-0.68% \n\n\n\nTable 9 :\n9Ablation study on the combination of boundary-aware pretext tasks measured by NMI. MEASURING REPRESENTATION QUALITY AT PRE-TRAINING STAGEPretext Tasks \nNMI Gain (\u2206%) \nSSM \n85.48 \n0.00% \nSSM+MSM \n85.64 \n+0.19% \nSSM+MSM+CGM \n85.93 \n+0.33% \nSSM+MSM+CGM+PP 86.71 \n+0.91% \n\nE \nhttps://github.com/AnyiRao/SceneSeg/tree/master/lgss\nACKNOWLEDGMENTSWe would like to acknowledge and thank Brain Cloud Team at Kakao Brain for their support.Algorithm 1 DTW-based pseudo-boundary discovery 1: Input: Shot encoder f enc , contextual relation network f CRN , and an input shot sequence S n = {s n\u2212K , ..., s n , ..., s n+K } centered at n th shot s n with neighbor size K, two image augmentation functions \u03bb 1 aug , \u03bb 2 aug .aug (s i )) // extract shot-level representations for all shots 5:E n \u2190 {E n ; e i } // append 6: end for 7: for i in {n \u2212 K, n + K} do 8:aug (s i )) // extract shot-level representations for slow sequence 9:E slow n \u2190 {E slow n ; e i } // append 10: end for 11: S left n , S right n , b * \u2190 DTW(E n , E slow n ) // apply dynamic time warping 12: Output: Two continuous non-overlapping sub-sequences S left n and S right n and a pseudo boundary shot s n+b * . from tslearn import metrics import numpy as np def compute_dtw_path(self, seq_1, seq_2):\"\"\" Input: seq_1: sparse shots embedding, shape = torch. Listing 1: PyTorch code for alignment computation using DTW given two sequences. The tslearn package is used for DTW path calculation.D ALGORITHM FOR PSEUDO-BOUNDARY DISCOVERYIn this section, we describe the details of pseudo-boundary discovery method applying DTW on S n and S slow n . In practice, S n is given as a mini-batch resulting in a tensor with a shape of (B, S, N k , C, H, W ) where individuals mean the batch size, the number of shots in S n (i.e., 2K + 1), the number of key-frames in a shot, channels, frame height and frame width, respectively. Then, we obtain S slow n \u2208 R B\u00d72\u00d7N k \u00d7C\u00d7H\u00d7W that is composed of the first and last shots in S n . We apply two different augmentation functions into key-frames in S n and S slow n , respectively. Next, we compute encoded representation of shots from S n and S slow n using f ENC . Note that during the pre-training stage, we randomly sample one key-frame among N k candidates in a shot and then reshape the input tensor as (B*S, C, H, W ) or (B*2, C, H, W ) to be forwarded by the shot encoder f ENC ; thus the tensor shape of the encoded shot representation is given by (B, S, D e ) or (B, 2, D e ) after apply reshaping, where D e means the dimension of encoded feature. Finally, given two sequences of encoded representation for S n and S slow n , DTW provides two sub-sequences S left n and S right n and a pseudo boundary shot s n+b * . The algorithm 1 illustrates the details. In addition, to demonstrate the simplicity of the alignment computation using DTW, we include the PyTorch code in Listing 1. The implementation of DTW can be done in 5 lines of python code using tslearn package.\nDiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks. Unaiza Ahsan, Chen Sun, Irfan Essa, arXiv:1801.07230arXiv preprintUnaiza Ahsan, Chen Sun, and Irfan Essa. DiscrimNet: Semi-Supervised Action Recognition from Videos using Generative Adversarial Networks. arXiv preprint arXiv:1801.07230, 2018.\n\nA Deep Siamese Network for Scene Detection in Broadcast Videos. Lorenzo Baraldi, Costantino Grana, Rita Cucchiara, Proceedings of the 23rd ACM international conference on Multimedia. the 23rd ACM international conference on MultimediaLorenzo Baraldi, Costantino Grana, and Rita Cucchiara. A Deep Siamese Network for Scene De- tection in Broadcast Videos. In Proceedings of the 23rd ACM international conference on Multi- media, 2015.\n\nUsing Dynamic Time Warping to Find Patterns in Time Series. J Donald, James Berndt, Clifford, KDD workshop. Donald J Berndt and James Clifford. Using Dynamic Time Warping to Find Patterns in Time Series. In KDD workshop, 1994.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, arXiv:2006.09882Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. arXiv preprintMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. arXiv preprint arXiv:2006.09882, 2020.\n\nDiscriminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation. Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, Juan Carlos Niebles, CVPR. 3Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and Juan Carlos Niebles. D3TW: Dis- criminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation. In CVPR, 2019.\n\nScene Detection in Videos using Shot Clustering and Sequence Alignment. Vasileios T Chasanis, C Aristidis, Nikolaos P Likas, Galatsanos, IEEE transactions on multimedia. 111Vasileios T Chasanis, Aristidis C Likas, and Nikolaos P Galatsanos. Scene Detection in Videos using Shot Clustering and Sequence Alignment. IEEE transactions on multimedia, 11(1):89-100, 2008.\n\nShot Contrastive Self-Supervised Learning for Scene Boundary Detection. Shixing Chen, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat, Raffay Hamid, CVPR. 2021Shixing Chen, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat, and Raffay Hamid. Shot Contrastive Self-Supervised Learning for Scene Boundary Detection. In CVPR, 2021.\n\nA Simple Framework for Contrastive Learning of Visual Representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In ICML, 2020a.\n\nUNITER: UNiversal Image-TExt Representation Learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, ECCV. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020b.\n\nIshan Dave, Rohit Gupta, Mamshad Nayeem Rizve, Mubarak Shah, Tclr, arXiv:2101.07974Temporal Contrastive Learning for Video Representation. arXiv preprintIshan Dave, Rohit Gupta, Mamshad Nayeem Rizve, and Mubarak Shah. TCLR: Temporal Con- trastive Learning for Video Representation. arXiv preprint arXiv:2101.07974, 2021.\n\nImageNet: A Large-scale Hierarchical Image Database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-scale Hierarchical Image Database. In CVPR, 2009.\n\nMS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation. Abu Yazan, Jurgen Farha, Gall, CVPR. Yazan Abu Farha and Jurgen Gall. MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation. In CVPR, 2019.\n\nA Large-Scale Study on Unsupervised Spatiotemporal Representation Learning. Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming He, CVPR. 2021Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning. In CVPR, 2021.\n\nDaniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, Aida Nematzadeh, arXiv:2005.03684Learning to Segment Actions from Observation and Narration. arXiv preprintDaniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, and Aida Ne- matzadeh. Learning to Segment Actions from Observation and Narration. arXiv preprint arXiv:2005.03684, 2020.\n\nVideo Scene Segmentation using A Novel Boundary Evaluation Criterion and Dynamic Programming. Bo Han, Weiguo Wu, IEEE International conference on multimedia and expo. Bo Han and Weiguo Wu. Video Scene Segmentation using A Novel Boundary Evaluation Criterion and Dynamic Programming. In IEEE International conference on multimedia and expo, 2011.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In CVPR, 2016.\n\nMomentum Contrast for Unsupervised Visual Representation Learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, CVPR. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. In CVPR, 2020.\n\nDan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian Error Linear Units (GELUs). arXiv preprintDan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016.\n\nMovieNet: A Holistic Dataset for Movie Understanding. Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, Dahua Lin, ECCV. Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. MovieNet: A Holistic Dataset for Movie Understanding. In ECCV, 2020.\n\nSelf-Supervised Spatiotemporal Feature Learning by Video Geometric Transformations. Longlong Jing, Yingli Tian, arXiv:1811.11387arXiv preprintLonglong Jing and Yingli Tian. Self-Supervised Spatiotemporal Feature Learning by Video Geo- metric Transformations. arXiv preprint arXiv:1811.11387, 2018.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.\n\nA hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation. Hilde Kuehne, Alexander Richard, Juergen Gall, IEEE transactions on pattern analysis and machine intelligence. 42Hilde Kuehne, Alexander Richard, and Juergen Gall. A hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation. IEEE transactions on pattern analysis and machine intelligence, 42(4):765-779, 2018.\n\nUnsupervised Learning of Action Classes with Continuous Temporal Embedding. Anna Kukleva, Hilde Kuehne, Fadime Sener, Jurgen Gall, CVPR. Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen Gall. Unsupervised Learning of Action Classes with Continuous Temporal Embedding. In CVPR, 2019.\n\nSateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey Konin, Quoc-Huy Zeeshan Zia, Tran, arXiv:2105.13353Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering. arXiv preprintSateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey Konin, M Zeeshan Zia, and Quoc-Huy Tran. Unsupervised Activity Segmentation by Joint Representation Learning and Online Clustering. arXiv preprint arXiv:2105.13353, 2021.\n\nSegmental Spatiotemporal CNNs for Fine-Grained Action Segmentation. Colin Lea, Austin Reiter, Ren\u00e9 Vidal, Gregory D Hager, ECCV. Colin Lea, Austin Reiter, Ren\u00e9 Vidal, and Gregory D Hager. Segmental Spatiotemporal CNNs for Fine-Grained Action Segmentation. In ECCV, 2016.\n\nUnsupervised Representation Learning by Sorting Sequences. Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang, Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised Representa- tion Learning by Sorting Sequences. In ICCV, 2017.\n\nSet-Constrained Viterbi for Set-Supervised Action Segmentation. Jun Li, Sinisa Todorovic, CVPR. Jun Li and Sinisa Todorovic. Set-Constrained Viterbi for Set-Supervised Action Segmentation. In CVPR, 2020.\n\nAction Shuffle Alternating Learning for Unsupervised Action Segmentation. Jun Li, Sinisa Todorovic, CVPR. 2021Jun Li and Sinisa Todorovic. Action Shuffle Alternating Learning for Unsupervised Action Seg- mentation. In CVPR, 2021.\n\nWeakly Supervised Energy-based Learning for Action Segmentation. Jun Li, Peng Lei, Sinisa Todorovic, ICCV. Jun Li, Peng Lei, and Sinisa Todorovic. Weakly Supervised Energy-based Learning for Action Segmentation. In ICCV, 2019.\n\nVALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, NeurIPS. 2021Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. VALUE: A Multi-Task Benchmark for Video-and- Language Understanding Evaluation. In NeurIPS, 2021.\n\nA Novel Role-Based Movie Scene Segmentation Method. Chao Liang, Yifan Zhang, Jian Cheng, Changsheng Xu, Hanqing Lu, Pacific-Rim Conference on Multimedia. Chao Liang, Yifan Zhang, Jian Cheng, Changsheng Xu, and Hanqing Lu. A Novel Role-Based Movie Scene Segmentation Method. In Pacific-Rim Conference on Multimedia, pp. 917-922.\n\n. Springer, Springer, 2009.\n\nShuffle and Learn: Unsupervised Learning using Temporal Order Verification. Ishan Misra, Lawrence Zitnick, Martial Hebert, ECCV. Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and Learn: Unsupervised Learning using Temporal Order Verification. In ECCV, 2016.\n\nAaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation Learning with Contrastive Predictive Coding. arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Pre- dictive Coding. arXiv preprint arXiv:1807.03748, 2018.\n\nSpatiotemporal Contrastive Video Representation Learning. Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, Yin Cui, CVPR. 2021Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal Contrastive Video Representation Learning. In CVPR, 2021.\n\nA Local-to-Global Approach to Multi-modal Movie Scene Segmentation. Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin, CVPR. Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, and Dahua Lin. A Local-to-Global Approach to Multi-modal Movie Scene Segmentation. In CVPR, 2020.\n\nScene Detection in Hollywood Movies and TV Shows. Zeeshan Rasheed, Mubarak Shah, CVPR. Zeeshan Rasheed and Mubarak Shah. Scene Detection in Hollywood Movies and TV Shows. In CVPR, 2003.\n\nDetection and Representation of Scenes in Videos. Zeeshan Rasheed, Mubarak Shah, IEEE transactions on Multimedia. 76Zeeshan Rasheed and Mubarak Shah. Detection and Representation of Scenes in Videos. IEEE transactions on Multimedia, 7(6):1097-1105, 2005.\n\nSpatially Consistent Representation Learning. Byungseok Roh, Wuhyun Shin, Ildoo Kim, Sungwoong Kim, CVPR. 2021Byungseok Roh, Wuhyun Shin, Ildoo Kim, and Sungwoong Kim. Spatially Consistent Representa- tion Learning. In CVPR, 2021.\n\nRobust and efficient video scene detection using optimal sequential grouping. Daniel Rotman, Dror Porat, Gal Ashour, 2016 IEEE international symposium on multimedia (ISM). Daniel Rotman, Dror Porat, and Gal Ashour. Robust and efficient video scene detection using optimal sequential grouping. In 2016 IEEE international symposium on multimedia (ISM), 2016.\n\nOptimal Sequential Grouping for Robust Video Scene Detection using Multiple Modalities. Daniel Rotman, Dror Porat, Gal Ashour, International Journal of Semantic Computing. 1102Daniel Rotman, Dror Porat, and Gal Ashour. Optimal Sequential Grouping for Robust Video Scene Detection using Multiple Modalities. International Journal of Semantic Computing, 11(02):193- 208, 2017.\n\nExploring Video Structure beyond The Shots. Yong Rui, S Thomas, Sharad Huang, Mehrotra, Proceedings. IEEE International Conference on Multimedia Computing and Systems (Cat. No. 98TB100241). IEEE International Conference on Multimedia Computing and Systems (Cat. No. 98TB100241)Yong Rui, Thomas S Huang, and Sharad Mehrotra. Exploring Video Structure beyond The Shots. In Proceedings. IEEE International Conference on Multimedia Computing and Systems (Cat. No. 98TB100241), 1998.\n\nLearning To Segment Actions From Visual and Language Instructions via Differentiable Weak Sequence Alignment. Yuhan Shen, Lu Wang, Ehsan Elhamifar, CVPR. 2021Yuhan Shen, Lu Wang, and Ehsan Elhamifar. Learning To Segment Actions From Visual and Lan- guage Instructions via Differentiable Weak Sequence Alignment. In CVPR, 2021.\n\nGeneric event boundary detection: A benchmark for event segmentation. Mike Zheng Shou, Stan W Lei, Weiyao Wang, Deepti Ghadiyaram, Matt Feiszli, ICCV. 2021Mike Zheng Shou, Stan W Lei, Weiyao Wang, Deepti Ghadiyaram, and Matt Feiszli. Generic event boundary detection: A benchmark for event segmentation. In ICCV, 2021.\n\nTemporal Video Segmentation to Scenes using High-level Audiovisual Features. Panagiotis Sidiropoulos, Vasileios Mezaris, Ioannis Kompatsiaris, Hugo Meinedo, Miguel Bugalho, Isabel Trancoso, IEEE Transactions on Circuits and Systems for Video Technology. 21Panagiotis Sidiropoulos, Vasileios Mezaris, Ioannis Kompatsiaris, Hugo Meinedo, Miguel Bugalho, and Isabel Trancoso. Temporal Video Segmentation to Scenes using High-level Audiovisual Fea- tures. IEEE Transactions on Circuits and Systems for Video Technology, 21(8):1163-1177, 2011.\n\nFast Weakly Supervised Action Segmentation using Mutual Consistency. Yaser Souri, Mohsen Fayyaz, Luca Minciullo, Gianpiero Francesca, Juergen Gall, IEEE Transactions on Pattern Analysis and Machine Intelligence. Yaser Souri, Mohsen Fayyaz, Luca Minciullo, Gianpiero Francesca, and Juergen Gall. Fast Weakly Supervised Action Segmentation using Mutual Consistency. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, 2021.\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting. The journal of machine learning research. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.\n\nUnsupervised Learning of Video Representations using LSTMs. Nitish Srivastava, Elman Mansimov, Ruslan Salakhudinov, ICML. Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised Learning of Video Representations using LSTMs. In ICML, 2015.\n\nChen Sun, Fabien Baradel, Kevin Murphy, Cordelia Schmid, arXiv:1906.05743Learning Video Representations using Contrastive Bidirectional Transformer. arXiv preprintChen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning Video Representations using Contrastive Bidirectional Transformer. arXiv preprint arXiv:1906.05743, 2019a.\n\nVideoBERT: A Joint Model for Video and Language Representation Learning. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid, ICCV. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A Joint Model for Video and Language Representation Learning. In ICCV, 2019b.\n\nStoryGraphs: Visualizing Character Interactions as a Timeline. Makarand Tapaswi, Martin Bauml, Rainer Stiefelhagen, CVPR. Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen. StoryGraphs: Visualizing Character Interactions as a Timeline. In CVPR, 2014.\n\nOxford handbook of cognitive psychology. Barbara Tversky, M Jeffrey, Zacks, Event perceptionBarbara Tversky and Jeffrey M Zacks. Event perception. Oxford handbook of cognitive psychology, 2013.\n\nAttention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In NIPS, 2017.\n\nJoint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences. G Rosaura, Vidalmata, J Walter, Anna Scheirer, David Kukleva, Hilde Cox, Kuehne, WACV. 2021Rosaura G VidalMata, Walter J Scheirer, Anna Kukleva, David Cox, and Hilde Kuehne. Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences. In WACV, 2021.\n\nGenerating Videos with Scene Dynamics. Carl Vondrick, Hamed Pirsiavash, Antonio Torralba, NIPS. Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating Videos with Scene Dynamics. NIPS, 2016.\n\nTracking Emerges by Colorizing Videos. Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, Kevin Murphy, ECCV. Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Track- ing Emerges by Colorizing Videos. In ECCV, 2018.\n\nUnsupervised Action Segmentation with Self-supervised Feature Learning and Co-occurrence Parsing. Zhe Wang, Hao Chen, Xinyu Li, Chunhui Liu, Yuanjun Xiong, Joseph Tighe, Charless Fowlkes, arXiv:2105.14158arXiv preprintZhe Wang, Hao Chen, Xinyu Li, Chunhui Liu, Yuanjun Xiong, Joseph Tighe, and Charless Fowlkes. Unsupervised Action Segmentation with Self-supervised Feature Learning and Co-occurrence Parsing. arXiv preprint arXiv:2105.14158, 2021.\n\nSelf-Supervised Spatiotemporal Learning via Video Clip Order Prediction. Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, Yueting Zhuang, CVPR. Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-Supervised Spa- tiotemporal Learning via Video Clip Order Prediction. In CVPR, 2019.\n\nBoundary-sensitive Pre-training for Temporal Localization in Videos. Mengmeng Xu, Juan-Manuel P\u00e9rez-R\u00faa, Victor Escorcia, Brais Martinez, Xiatian Zhu, Li Zhang, Bernard Ghanem, Tao Xiang, arXiv:2011.10830arXiv preprintMengmeng Xu, Juan-Manuel P\u00e9rez-R\u00faa, Victor Escorcia, Brais Martinez, Xiatian Zhu, Li Zhang, Bernard Ghanem, and Tao Xiang. Boundary-sensitive Pre-training for Temporal Localization in Videos. arXiv preprint arXiv:2011.10830, 2020.\n\nLarge Batch Training of Convolutional Networks. Yang You, Igor Gitman, Boris Ginsburg, arXiv:1708.03888arXiv preprintYang You, Igor Gitman, and Boris Ginsburg. Large Batch Training of Convolutional Networks. arXiv preprint arXiv:1708.03888, 2017.\n\nPlaces: A 10 million Image Database for Scene Recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE Transactions on Pattern Analysis and Machine Intelligence. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 mil- lion Image Database for Scene Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nCross-Task Weakly Supervised Learning from Instructional Videos. Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, Josef Sivic, CVPR. Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. Cross-Task Weakly Supervised Learning from Instructional Videos. In CVPR, 2019.\n", "annotations": {"author": "[{\"end\":132,\"start\":72},{\"end\":193,\"start\":133},{\"end\":252,\"start\":194},{\"end\":312,\"start\":253},{\"end\":392,\"start\":313},{\"end\":451,\"start\":393},{\"end\":531,\"start\":452},{\"end\":615,\"start\":532}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":145,\"start\":141},{\"end\":204,\"start\":201},{\"end\":264,\"start\":259},{\"end\":323,\"start\":320},{\"end\":403,\"start\":401},{\"end\":464,\"start\":461},{\"end\":543,\"start\":540}]", "author_first_name": "[{\"end\":80,\"start\":72},{\"end\":140,\"start\":133},{\"end\":200,\"start\":194},{\"end\":258,\"start\":253},{\"end\":319,\"start\":313},{\"end\":400,\"start\":393},{\"end\":460,\"start\":452},{\"end\":539,\"start\":532}]", "author_affiliation": "[{\"end\":131,\"start\":86},{\"end\":192,\"start\":147},{\"end\":251,\"start\":206},{\"end\":311,\"start\":266},{\"end\":391,\"start\":346},{\"end\":450,\"start\":405},{\"end\":530,\"start\":485},{\"end\":614,\"start\":569}]", "title": "[{\"end\":69,\"start\":1},{\"end\":684,\"start\":616}]", "venue": null, "abstract": "[{\"end\":2042,\"start\":686}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2225,\"start\":2202},{\"end\":2404,\"start\":2385},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4338,\"start\":4318},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4357,\"start\":4338},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4373,\"start\":4357},{\"end\":4390,\"start\":4373},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4576,\"start\":4557},{\"end\":4603,\"start\":4576},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4621,\"start\":4603},{\"end\":4639,\"start\":4621},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6531,\"start\":6513},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7819,\"start\":7801},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7840,\"start\":7819},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7845,\"start\":7840},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7867,\"start\":7845},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7911,\"start\":7895},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7932,\"start\":7911},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7952,\"start\":7932},{\"end\":8000,\"start\":7980},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8026,\"start\":8000},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8162,\"start\":8141},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8193,\"start\":8171},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8570,\"start\":8552},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8694,\"start\":8675},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9240,\"start\":9222},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9259,\"start\":9240},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9511,\"start\":9491},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9532,\"start\":9511},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9551,\"start\":9532},{\"end\":9569,\"start\":9551},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9589,\"start\":9569},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9608,\"start\":9589},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9862,\"start\":9841},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9916,\"start\":9896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9978,\"start\":9958},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10031,\"start\":10011},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10049,\"start\":10031},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10070,\"start\":10049},{\"end\":10091,\"start\":10070},{\"end\":10114,\"start\":10091},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10806,\"start\":10781},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10828,\"start\":10806},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10847,\"start\":10828},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10897,\"start\":10877},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10914,\"start\":10897},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10930,\"start\":10914},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10988,\"start\":10969},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11036,\"start\":11013},{\"end\":11092,\"start\":11064},{\"end\":11110,\"start\":11092},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11128,\"start\":11110},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11166,\"start\":11147},{\"end\":11168,\"start\":11166},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13511,\"start\":13494},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14190,\"start\":14171},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16470,\"start\":16451},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18298,\"start\":18273},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19972,\"start\":19953},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22524,\"start\":22505},{\"end\":22526,\"start\":22524},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24022,\"start\":24004},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24869,\"start\":24851},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25410,\"start\":25392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25594,\"start\":25574},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25610,\"start\":25594},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25726,\"start\":25709},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25765,\"start\":25743},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26391,\"start\":26369},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26478,\"start\":26456},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26529,\"start\":26513},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26564,\"start\":26542},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26599,\"start\":26578},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":26669,\"start\":26650},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27791,\"start\":27772},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27840,\"start\":27821},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27964,\"start\":27945},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28369,\"start\":28350},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":28637,\"start\":28618},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33105,\"start\":33080},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33210,\"start\":33184},{\"end\":33661,\"start\":33656},{\"end\":33665,\"start\":33661},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34563,\"start\":34545},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":35321,\"start\":35303},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35450,\"start\":35431},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":35784,\"start\":35765},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37957,\"start\":37935},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":37988,\"start\":37967}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":43681,\"start\":43558},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43911,\"start\":43682},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44005,\"start\":43912},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":44789,\"start\":44006},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45901,\"start\":44790},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46610,\"start\":45902},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47051,\"start\":46611},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47483,\"start\":47052},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47816,\"start\":47484},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":48165,\"start\":47817},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":48449,\"start\":48166}]", "paragraph": "[{\"end\":2903,\"start\":2058},{\"end\":4150,\"start\":2905},{\"end\":5524,\"start\":4152},{\"end\":6883,\"start\":5526},{\"end\":7607,\"start\":6885},{\"end\":9002,\"start\":7624},{\"end\":10617,\"start\":9004},{\"end\":12019,\"start\":10619},{\"end\":12311,\"start\":12071},{\"end\":12720,\"start\":12335},{\"end\":13852,\"start\":12722},{\"end\":14084,\"start\":13895},{\"end\":14446,\"start\":14103},{\"end\":14812,\"start\":14448},{\"end\":15653,\"start\":14856},{\"end\":16038,\"start\":15655},{\"end\":16675,\"start\":16040},{\"end\":17636,\"start\":16705},{\"end\":18208,\"start\":17638},{\"end\":18543,\"start\":18210},{\"end\":19225,\"start\":18655},{\"end\":19495,\"start\":19253},{\"end\":19973,\"start\":19497},{\"end\":20370,\"start\":20167},{\"end\":20458,\"start\":20372},{\"end\":21347,\"start\":20460},{\"end\":21626,\"start\":21420},{\"end\":22098,\"start\":21628},{\"end\":22442,\"start\":22154},{\"end\":22917,\"start\":22444},{\"end\":23426,\"start\":22958},{\"end\":23591,\"start\":23487},{\"end\":24236,\"start\":23636},{\"end\":24309,\"start\":24238},{\"end\":24482,\"start\":24311},{\"end\":25044,\"start\":24521},{\"end\":26249,\"start\":25046},{\"end\":27412,\"start\":26294},{\"end\":29572,\"start\":27455},{\"end\":30377,\"start\":29630},{\"end\":31348,\"start\":30379},{\"end\":31828,\"start\":31350},{\"end\":32355,\"start\":31843},{\"end\":33237,\"start\":32395},{\"end\":34403,\"start\":33239},{\"end\":35093,\"start\":34405},{\"end\":35601,\"start\":35095},{\"end\":39337,\"start\":35659},{\"end\":40183,\"start\":39339},{\"end\":40607,\"start\":40185},{\"end\":41635,\"start\":40634},{\"end\":42602,\"start\":41657},{\"end\":43557,\"start\":42604}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13894,\"start\":13853},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14855,\"start\":14813},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18654,\"start\":18544},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20166,\"start\":19974},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21419,\"start\":21348},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22153,\"start\":22099},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22957,\"start\":22918},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23486,\"start\":23427}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26834,\"start\":26827},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28810,\"start\":28803},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29789,\"start\":29782},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30912,\"start\":30905},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31160,\"start\":31153},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31485,\"start\":31478},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31589,\"start\":31582},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":36596,\"start\":36589},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37031,\"start\":37024},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37583,\"start\":37576},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":38336,\"start\":38329},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39349,\"start\":39342},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40340,\"start\":40333},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40483,\"start\":40476}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2056,\"start\":2044},{\"attributes\":{\"n\":\"2\"},\"end\":7622,\"start\":7610},{\"attributes\":{\"n\":\"3\"},\"end\":12069,\"start\":12022},{\"attributes\":{\"n\":\"3.1\"},\"end\":12333,\"start\":12314},{\"attributes\":{\"n\":\"3.2\"},\"end\":14101,\"start\":14087},{\"attributes\":{\"n\":\"3.3\"},\"end\":16703,\"start\":16678},{\"attributes\":{\"n\":\"3.4\"},\"end\":19251,\"start\":19228},{\"attributes\":{\"n\":\"3.5\"},\"end\":23634,\"start\":23594},{\"attributes\":{\"n\":\"4\"},\"end\":24495,\"start\":24485},{\"attributes\":{\"n\":\"4.1\"},\"end\":24519,\"start\":24498},{\"attributes\":{\"n\":\"4.2\"},\"end\":26292,\"start\":26252},{\"attributes\":{\"n\":\"4.3\"},\"end\":27453,\"start\":27415},{\"attributes\":{\"n\":\"4.4\"},\"end\":29591,\"start\":29575},{\"end\":29628,\"start\":29594},{\"attributes\":{\"n\":\"5\"},\"end\":31841,\"start\":31831},{\"end\":32393,\"start\":32358},{\"end\":35657,\"start\":35604},{\"end\":40632,\"start\":40610},{\"end\":41655,\"start\":41638},{\"end\":43579,\"start\":43559},{\"end\":43693,\"start\":43683},{\"end\":43922,\"start\":43913},{\"end\":44016,\"start\":44007},{\"end\":44800,\"start\":44791},{\"end\":45912,\"start\":45903},{\"end\":46621,\"start\":46612},{\"end\":47062,\"start\":47053},{\"end\":47494,\"start\":47485},{\"end\":47827,\"start\":47818},{\"end\":48176,\"start\":48167}]", "table": "[{\"end\":44789,\"start\":44190},{\"end\":45901,\"start\":44916},{\"end\":46610,\"start\":46074},{\"end\":47051,\"start\":46711},{\"end\":47483,\"start\":47202},{\"end\":47816,\"start\":47652},{\"end\":48165,\"start\":47991},{\"end\":48449,\"start\":48315}]", "figure_caption": "[{\"end\":43681,\"start\":43582},{\"end\":43911,\"start\":43695},{\"end\":44005,\"start\":43924},{\"end\":44190,\"start\":44018},{\"end\":44916,\"start\":44802},{\"end\":46074,\"start\":45914},{\"end\":46711,\"start\":46623},{\"end\":47202,\"start\":47064},{\"end\":47652,\"start\":47496},{\"end\":47991,\"start\":47829},{\"end\":48315,\"start\":48178}]", "figure_ref": "[{\"end\":3056,\"start\":3048},{\"end\":3331,\"start\":3323},{\"end\":3707,\"start\":3699},{\"end\":5149,\"start\":5141},{\"end\":14474,\"start\":14466},{\"end\":15293,\"start\":15285},{\"end\":15749,\"start\":15741},{\"end\":16070,\"start\":16062},{\"end\":17369,\"start\":17361},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19099,\"start\":19091},{\"end\":19279,\"start\":19271},{\"end\":35923,\"start\":35915},{\"end\":37083,\"start\":37075},{\"end\":40885,\"start\":40877},{\"end\":41423,\"start\":41415},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41762,\"start\":41754},{\"end\":42643,\"start\":42635}]", "bib_author_first_name": "[{\"end\":51254,\"start\":51248},{\"end\":51266,\"start\":51262},{\"end\":51277,\"start\":51272},{\"end\":51563,\"start\":51556},{\"end\":51583,\"start\":51573},{\"end\":51595,\"start\":51591},{\"end\":51988,\"start\":51987},{\"end\":52002,\"start\":51997},{\"end\":52163,\"start\":52155},{\"end\":52176,\"start\":52171},{\"end\":52190,\"start\":52184},{\"end\":52204,\"start\":52199},{\"end\":52217,\"start\":52212},{\"end\":52236,\"start\":52230},{\"end\":52679,\"start\":52671},{\"end\":52692,\"start\":52687},{\"end\":52705,\"start\":52700},{\"end\":52713,\"start\":52711},{\"end\":52727,\"start\":52723},{\"end\":52734,\"start\":52728},{\"end\":53055,\"start\":53054},{\"end\":53077,\"start\":53067},{\"end\":53406,\"start\":53399},{\"end\":53420,\"start\":53413},{\"end\":53431,\"start\":53426},{\"end\":53445,\"start\":53437},{\"end\":53458,\"start\":53453},{\"end\":53471,\"start\":53465},{\"end\":53736,\"start\":53732},{\"end\":53748,\"start\":53743},{\"end\":53768,\"start\":53760},{\"end\":53786,\"start\":53778},{\"end\":54018,\"start\":54010},{\"end\":54031,\"start\":54025},{\"end\":54043,\"start\":54036},{\"end\":54053,\"start\":54048},{\"end\":54056,\"start\":54054},{\"end\":54070,\"start\":54064},{\"end\":54081,\"start\":54078},{\"end\":54089,\"start\":54087},{\"end\":54105,\"start\":54097},{\"end\":54298,\"start\":54293},{\"end\":54310,\"start\":54305},{\"end\":54325,\"start\":54318},{\"end\":54347,\"start\":54340},{\"end\":54671,\"start\":54668},{\"end\":54681,\"start\":54678},{\"end\":54695,\"start\":54688},{\"end\":54710,\"start\":54704},{\"end\":54718,\"start\":54715},{\"end\":54725,\"start\":54723},{\"end\":54960,\"start\":54957},{\"end\":54974,\"start\":54968},{\"end\":55204,\"start\":55195},{\"end\":55225,\"start\":55220},{\"end\":55233,\"start\":55231},{\"end\":55245,\"start\":55241},{\"end\":55263,\"start\":55256},{\"end\":55453,\"start\":55447},{\"end\":55474,\"start\":55461},{\"end\":55488,\"start\":55484},{\"end\":55503,\"start\":55498},{\"end\":55517,\"start\":55510},{\"end\":55529,\"start\":55525},{\"end\":55929,\"start\":55927},{\"end\":55941,\"start\":55935},{\"end\":56233,\"start\":56226},{\"end\":56245,\"start\":56238},{\"end\":56261,\"start\":56253},{\"end\":56271,\"start\":56267},{\"end\":56474,\"start\":56467},{\"end\":56484,\"start\":56479},{\"end\":56495,\"start\":56490},{\"end\":56507,\"start\":56500},{\"end\":56517,\"start\":56513},{\"end\":56685,\"start\":56682},{\"end\":56702,\"start\":56697},{\"end\":56948,\"start\":56941},{\"end\":56958,\"start\":56956},{\"end\":56970,\"start\":56966},{\"end\":56981,\"start\":56976},{\"end\":56993,\"start\":56988},{\"end\":57229,\"start\":57221},{\"end\":57242,\"start\":57236},{\"end\":57481,\"start\":57480},{\"end\":57497,\"start\":57492},{\"end\":57691,\"start\":57686},{\"end\":57709,\"start\":57700},{\"end\":57726,\"start\":57719},{\"end\":58094,\"start\":58090},{\"end\":58109,\"start\":58104},{\"end\":58124,\"start\":58118},{\"end\":58138,\"start\":58132},{\"end\":58309,\"start\":58302},{\"end\":58323,\"start\":58317},{\"end\":58337,\"start\":58332},{\"end\":58351,\"start\":58345},{\"end\":58367,\"start\":58359},{\"end\":58803,\"start\":58798},{\"end\":58815,\"start\":58809},{\"end\":58828,\"start\":58824},{\"end\":58845,\"start\":58836},{\"end\":59070,\"start\":59061},{\"end\":59083,\"start\":59076},{\"end\":59098,\"start\":59091},{\"end\":59116,\"start\":59106},{\"end\":59333,\"start\":59330},{\"end\":59344,\"start\":59338},{\"end\":59548,\"start\":59545},{\"end\":59559,\"start\":59553},{\"end\":59770,\"start\":59767},{\"end\":59779,\"start\":59775},{\"end\":59791,\"start\":59785},{\"end\":60015,\"start\":60009},{\"end\":60023,\"start\":60020},{\"end\":60032,\"start\":60029},{\"end\":60045,\"start\":60038},{\"end\":60058,\"start\":60050},{\"end\":60070,\"start\":60065},{\"end\":60081,\"start\":60079},{\"end\":60095,\"start\":60089},{\"end\":60105,\"start\":60102},{\"end\":60110,\"start\":60106},{\"end\":60124,\"start\":60117},{\"end\":60129,\"start\":60125},{\"end\":60438,\"start\":60434},{\"end\":60451,\"start\":60446},{\"end\":60463,\"start\":60459},{\"end\":60481,\"start\":60471},{\"end\":60493,\"start\":60486},{\"end\":60821,\"start\":60816},{\"end\":60837,\"start\":60829},{\"end\":60854,\"start\":60847},{\"end\":61019,\"start\":61014},{\"end\":61039,\"start\":61034},{\"end\":61049,\"start\":61044},{\"end\":61361,\"start\":61358},{\"end\":61376,\"start\":61368},{\"end\":61389,\"start\":61383},{\"end\":61406,\"start\":61396},{\"end\":61421,\"start\":61413},{\"end\":61433,\"start\":61428},{\"end\":61447,\"start\":61444},{\"end\":61708,\"start\":61704},{\"end\":61721,\"start\":61714},{\"end\":61728,\"start\":61726},{\"end\":61743,\"start\":61736},{\"end\":61755,\"start\":61748},{\"end\":61768,\"start\":61763},{\"end\":61780,\"start\":61775},{\"end\":62019,\"start\":62012},{\"end\":62036,\"start\":62029},{\"end\":62206,\"start\":62199},{\"end\":62223,\"start\":62216},{\"end\":62460,\"start\":62451},{\"end\":62472,\"start\":62466},{\"end\":62484,\"start\":62479},{\"end\":62499,\"start\":62490},{\"end\":62721,\"start\":62715},{\"end\":62734,\"start\":62730},{\"end\":62745,\"start\":62742},{\"end\":63089,\"start\":63083},{\"end\":63102,\"start\":63098},{\"end\":63113,\"start\":63110},{\"end\":63419,\"start\":63415},{\"end\":63426,\"start\":63425},{\"end\":63441,\"start\":63435},{\"end\":63966,\"start\":63961},{\"end\":63975,\"start\":63973},{\"end\":63987,\"start\":63982},{\"end\":64253,\"start\":64249},{\"end\":64259,\"start\":64254},{\"end\":64270,\"start\":64266},{\"end\":64272,\"start\":64271},{\"end\":64284,\"start\":64278},{\"end\":64297,\"start\":64291},{\"end\":64314,\"start\":64310},{\"end\":64586,\"start\":64576},{\"end\":64610,\"start\":64601},{\"end\":64627,\"start\":64620},{\"end\":64646,\"start\":64642},{\"end\":64662,\"start\":64656},{\"end\":64678,\"start\":64672},{\"end\":65113,\"start\":65108},{\"end\":65127,\"start\":65121},{\"end\":65140,\"start\":65136},{\"end\":65161,\"start\":65152},{\"end\":65180,\"start\":65173},{\"end\":65591,\"start\":65585},{\"end\":65612,\"start\":65604},{\"end\":65625,\"start\":65621},{\"end\":65642,\"start\":65638},{\"end\":65660,\"start\":65654},{\"end\":65972,\"start\":65966},{\"end\":65990,\"start\":65985},{\"end\":66007,\"start\":66001},{\"end\":66168,\"start\":66164},{\"end\":66180,\"start\":66174},{\"end\":66195,\"start\":66190},{\"end\":66212,\"start\":66204},{\"end\":66582,\"start\":66578},{\"end\":66594,\"start\":66588},{\"end\":66606,\"start\":66602},{\"end\":66622,\"start\":66617},{\"end\":66639,\"start\":66631},{\"end\":66889,\"start\":66881},{\"end\":66905,\"start\":66899},{\"end\":66919,\"start\":66913},{\"end\":67124,\"start\":67117},{\"end\":67135,\"start\":67134},{\"end\":67304,\"start\":67298},{\"end\":67318,\"start\":67314},{\"end\":67332,\"start\":67328},{\"end\":67346,\"start\":67341},{\"end\":67363,\"start\":67358},{\"end\":67376,\"start\":67371},{\"end\":67378,\"start\":67377},{\"end\":67392,\"start\":67386},{\"end\":67406,\"start\":67401},{\"end\":67687,\"start\":67686},{\"end\":67709,\"start\":67708},{\"end\":67722,\"start\":67718},{\"end\":67738,\"start\":67733},{\"end\":67753,\"start\":67748},{\"end\":68012,\"start\":68008},{\"end\":68028,\"start\":68023},{\"end\":68048,\"start\":68041},{\"end\":68215,\"start\":68211},{\"end\":68233,\"start\":68226},{\"end\":68254,\"start\":68247},{\"end\":68268,\"start\":68262},{\"end\":68286,\"start\":68281},{\"end\":68547,\"start\":68544},{\"end\":68557,\"start\":68554},{\"end\":68569,\"start\":68564},{\"end\":68581,\"start\":68574},{\"end\":68594,\"start\":68587},{\"end\":68608,\"start\":68602},{\"end\":68624,\"start\":68616},{\"end\":68975,\"start\":68969},{\"end\":68983,\"start\":68980},{\"end\":68994,\"start\":68990},{\"end\":69005,\"start\":69001},{\"end\":69014,\"start\":69012},{\"end\":69027,\"start\":69020},{\"end\":69281,\"start\":69273},{\"end\":69297,\"start\":69286},{\"end\":69315,\"start\":69309},{\"end\":69331,\"start\":69326},{\"end\":69349,\"start\":69342},{\"end\":69357,\"start\":69355},{\"end\":69372,\"start\":69365},{\"end\":69384,\"start\":69381},{\"end\":69706,\"start\":69702},{\"end\":69716,\"start\":69712},{\"end\":69730,\"start\":69725},{\"end\":69966,\"start\":69961},{\"end\":69978,\"start\":69973},{\"end\":69996,\"start\":69990},{\"end\":70009,\"start\":70005},{\"end\":70024,\"start\":70017},{\"end\":70381,\"start\":70374},{\"end\":70403,\"start\":70390},{\"end\":70420,\"start\":70413},{\"end\":70428,\"start\":70421},{\"end\":70442,\"start\":70437},{\"end\":70455,\"start\":70451},{\"end\":70469,\"start\":70464}]", "bib_author_last_name": "[{\"end\":51260,\"start\":51255},{\"end\":51270,\"start\":51267},{\"end\":51282,\"start\":51278},{\"end\":51571,\"start\":51564},{\"end\":51589,\"start\":51584},{\"end\":51605,\"start\":51596},{\"end\":51995,\"start\":51989},{\"end\":52009,\"start\":52003},{\"end\":52019,\"start\":52011},{\"end\":52169,\"start\":52164},{\"end\":52182,\"start\":52177},{\"end\":52197,\"start\":52191},{\"end\":52210,\"start\":52205},{\"end\":52228,\"start\":52218},{\"end\":52243,\"start\":52237},{\"end\":52685,\"start\":52680},{\"end\":52698,\"start\":52693},{\"end\":52709,\"start\":52706},{\"end\":52721,\"start\":52714},{\"end\":52742,\"start\":52735},{\"end\":53052,\"start\":53032},{\"end\":53065,\"start\":53056},{\"end\":53083,\"start\":53078},{\"end\":53095,\"start\":53085},{\"end\":53411,\"start\":53407},{\"end\":53424,\"start\":53421},{\"end\":53435,\"start\":53432},{\"end\":53451,\"start\":53446},{\"end\":53463,\"start\":53459},{\"end\":53477,\"start\":53472},{\"end\":53741,\"start\":53737},{\"end\":53758,\"start\":53749},{\"end\":53776,\"start\":53769},{\"end\":53793,\"start\":53787},{\"end\":54023,\"start\":54019},{\"end\":54034,\"start\":54032},{\"end\":54046,\"start\":54044},{\"end\":54062,\"start\":54057},{\"end\":54076,\"start\":54071},{\"end\":54085,\"start\":54082},{\"end\":54095,\"start\":54090},{\"end\":54109,\"start\":54106},{\"end\":54303,\"start\":54299},{\"end\":54316,\"start\":54311},{\"end\":54338,\"start\":54326},{\"end\":54352,\"start\":54348},{\"end\":54358,\"start\":54354},{\"end\":54676,\"start\":54672},{\"end\":54686,\"start\":54682},{\"end\":54702,\"start\":54696},{\"end\":54713,\"start\":54711},{\"end\":54721,\"start\":54719},{\"end\":54733,\"start\":54726},{\"end\":54966,\"start\":54961},{\"end\":54980,\"start\":54975},{\"end\":54986,\"start\":54982},{\"end\":55218,\"start\":55205},{\"end\":55229,\"start\":55226},{\"end\":55239,\"start\":55234},{\"end\":55254,\"start\":55246},{\"end\":55266,\"start\":55264},{\"end\":55459,\"start\":55454},{\"end\":55482,\"start\":55475},{\"end\":55496,\"start\":55489},{\"end\":55508,\"start\":55504},{\"end\":55523,\"start\":55518},{\"end\":55540,\"start\":55530},{\"end\":55933,\"start\":55930},{\"end\":55944,\"start\":55942},{\"end\":56236,\"start\":56234},{\"end\":56251,\"start\":56246},{\"end\":56265,\"start\":56262},{\"end\":56275,\"start\":56272},{\"end\":56477,\"start\":56475},{\"end\":56488,\"start\":56485},{\"end\":56498,\"start\":56496},{\"end\":56511,\"start\":56508},{\"end\":56526,\"start\":56518},{\"end\":56695,\"start\":56686},{\"end\":56709,\"start\":56703},{\"end\":56954,\"start\":56949},{\"end\":56964,\"start\":56959},{\"end\":56974,\"start\":56971},{\"end\":56986,\"start\":56982},{\"end\":56997,\"start\":56994},{\"end\":57234,\"start\":57230},{\"end\":57247,\"start\":57243},{\"end\":57490,\"start\":57482},{\"end\":57504,\"start\":57498},{\"end\":57508,\"start\":57506},{\"end\":57698,\"start\":57692},{\"end\":57717,\"start\":57710},{\"end\":57731,\"start\":57727},{\"end\":58102,\"start\":58095},{\"end\":58116,\"start\":58110},{\"end\":58130,\"start\":58125},{\"end\":58143,\"start\":58139},{\"end\":58315,\"start\":58310},{\"end\":58330,\"start\":58324},{\"end\":58343,\"start\":58338},{\"end\":58357,\"start\":58352},{\"end\":58379,\"start\":58368},{\"end\":58385,\"start\":58381},{\"end\":58807,\"start\":58804},{\"end\":58822,\"start\":58816},{\"end\":58834,\"start\":58829},{\"end\":58851,\"start\":58846},{\"end\":59074,\"start\":59071},{\"end\":59089,\"start\":59084},{\"end\":59104,\"start\":59099},{\"end\":59121,\"start\":59117},{\"end\":59336,\"start\":59334},{\"end\":59354,\"start\":59345},{\"end\":59551,\"start\":59549},{\"end\":59569,\"start\":59560},{\"end\":59773,\"start\":59771},{\"end\":59783,\"start\":59780},{\"end\":59801,\"start\":59792},{\"end\":60018,\"start\":60016},{\"end\":60027,\"start\":60024},{\"end\":60036,\"start\":60033},{\"end\":60048,\"start\":60046},{\"end\":60063,\"start\":60059},{\"end\":60077,\"start\":60071},{\"end\":60087,\"start\":60082},{\"end\":60100,\"start\":60096},{\"end\":60115,\"start\":60111},{\"end\":60134,\"start\":60130},{\"end\":60444,\"start\":60439},{\"end\":60457,\"start\":60452},{\"end\":60469,\"start\":60464},{\"end\":60484,\"start\":60482},{\"end\":60496,\"start\":60494},{\"end\":60721,\"start\":60713},{\"end\":60827,\"start\":60822},{\"end\":60845,\"start\":60838},{\"end\":60861,\"start\":60855},{\"end\":61032,\"start\":61020},{\"end\":61042,\"start\":61040},{\"end\":61057,\"start\":61050},{\"end\":61366,\"start\":61362},{\"end\":61381,\"start\":61377},{\"end\":61394,\"start\":61390},{\"end\":61411,\"start\":61407},{\"end\":61426,\"start\":61422},{\"end\":61442,\"start\":61434},{\"end\":61451,\"start\":61448},{\"end\":61712,\"start\":61709},{\"end\":61724,\"start\":61722},{\"end\":61734,\"start\":61729},{\"end\":61746,\"start\":61744},{\"end\":61761,\"start\":61756},{\"end\":61773,\"start\":61769},{\"end\":61784,\"start\":61781},{\"end\":62027,\"start\":62020},{\"end\":62041,\"start\":62037},{\"end\":62214,\"start\":62207},{\"end\":62228,\"start\":62224},{\"end\":62464,\"start\":62461},{\"end\":62477,\"start\":62473},{\"end\":62488,\"start\":62485},{\"end\":62503,\"start\":62500},{\"end\":62728,\"start\":62722},{\"end\":62740,\"start\":62735},{\"end\":62752,\"start\":62746},{\"end\":63096,\"start\":63090},{\"end\":63108,\"start\":63103},{\"end\":63120,\"start\":63114},{\"end\":63423,\"start\":63420},{\"end\":63433,\"start\":63427},{\"end\":63447,\"start\":63442},{\"end\":63457,\"start\":63449},{\"end\":63971,\"start\":63967},{\"end\":63980,\"start\":63976},{\"end\":63997,\"start\":63988},{\"end\":64264,\"start\":64260},{\"end\":64276,\"start\":64273},{\"end\":64289,\"start\":64285},{\"end\":64308,\"start\":64298},{\"end\":64322,\"start\":64315},{\"end\":64599,\"start\":64587},{\"end\":64618,\"start\":64611},{\"end\":64640,\"start\":64628},{\"end\":64654,\"start\":64647},{\"end\":64670,\"start\":64663},{\"end\":64687,\"start\":64679},{\"end\":65119,\"start\":65114},{\"end\":65134,\"start\":65128},{\"end\":65150,\"start\":65141},{\"end\":65171,\"start\":65162},{\"end\":65185,\"start\":65181},{\"end\":65602,\"start\":65592},{\"end\":65619,\"start\":65613},{\"end\":65636,\"start\":65626},{\"end\":65652,\"start\":65643},{\"end\":65674,\"start\":65661},{\"end\":65983,\"start\":65973},{\"end\":65999,\"start\":65991},{\"end\":66020,\"start\":66008},{\"end\":66172,\"start\":66169},{\"end\":66188,\"start\":66181},{\"end\":66202,\"start\":66196},{\"end\":66219,\"start\":66213},{\"end\":66586,\"start\":66583},{\"end\":66600,\"start\":66595},{\"end\":66615,\"start\":66607},{\"end\":66629,\"start\":66623},{\"end\":66646,\"start\":66640},{\"end\":66897,\"start\":66890},{\"end\":66911,\"start\":66906},{\"end\":66932,\"start\":66920},{\"end\":67132,\"start\":67125},{\"end\":67143,\"start\":67136},{\"end\":67150,\"start\":67145},{\"end\":67312,\"start\":67305},{\"end\":67326,\"start\":67319},{\"end\":67339,\"start\":67333},{\"end\":67356,\"start\":67347},{\"end\":67369,\"start\":67364},{\"end\":67384,\"start\":67379},{\"end\":67399,\"start\":67393},{\"end\":67417,\"start\":67407},{\"end\":67695,\"start\":67688},{\"end\":67706,\"start\":67697},{\"end\":67716,\"start\":67710},{\"end\":67731,\"start\":67723},{\"end\":67746,\"start\":67739},{\"end\":67757,\"start\":67754},{\"end\":67765,\"start\":67759},{\"end\":68021,\"start\":68013},{\"end\":68039,\"start\":68029},{\"end\":68057,\"start\":68049},{\"end\":68224,\"start\":68216},{\"end\":68245,\"start\":68234},{\"end\":68260,\"start\":68255},{\"end\":68279,\"start\":68269},{\"end\":68293,\"start\":68287},{\"end\":68552,\"start\":68548},{\"end\":68562,\"start\":68558},{\"end\":68572,\"start\":68570},{\"end\":68585,\"start\":68582},{\"end\":68600,\"start\":68595},{\"end\":68614,\"start\":68609},{\"end\":68632,\"start\":68625},{\"end\":68978,\"start\":68976},{\"end\":68988,\"start\":68984},{\"end\":68999,\"start\":68995},{\"end\":69010,\"start\":69006},{\"end\":69018,\"start\":69015},{\"end\":69034,\"start\":69028},{\"end\":69284,\"start\":69282},{\"end\":69307,\"start\":69298},{\"end\":69324,\"start\":69316},{\"end\":69340,\"start\":69332},{\"end\":69353,\"start\":69350},{\"end\":69363,\"start\":69358},{\"end\":69379,\"start\":69373},{\"end\":69390,\"start\":69385},{\"end\":69710,\"start\":69707},{\"end\":69723,\"start\":69717},{\"end\":69739,\"start\":69731},{\"end\":69971,\"start\":69967},{\"end\":69988,\"start\":69979},{\"end\":70003,\"start\":69997},{\"end\":70015,\"start\":70010},{\"end\":70033,\"start\":70025},{\"end\":70388,\"start\":70382},{\"end\":70411,\"start\":70404},{\"end\":70435,\"start\":70429},{\"end\":70449,\"start\":70443},{\"end\":70462,\"start\":70456},{\"end\":70475,\"start\":70470}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1801.07230\",\"id\":\"b0\"},\"end\":51490,\"start\":51150},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10400768},\"end\":51925,\"start\":51492},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":929893},\"end\":52153,\"start\":51927},{\"attributes\":{\"doi\":\"arXiv:2006.09882\",\"id\":\"b3\"},\"end\":52561,\"start\":52155},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57759274},\"end\":52958,\"start\":52563},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14139786},\"end\":53325,\"start\":52960},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":233423176},\"end\":53659,\"start\":53327},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211096730},\"end\":53954,\"start\":53661},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":216080982},\"end\":54291,\"start\":53956},{\"attributes\":{\"doi\":\"arXiv:2101.07974\",\"id\":\"b9\"},\"end\":54613,\"start\":54293},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57246310},\"end\":54879,\"start\":54615},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":67877065},\"end\":55117,\"start\":54881},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":233444206},\"end\":55445,\"start\":55119},{\"attributes\":{\"doi\":\"arXiv:2005.03684\",\"id\":\"b13\"},\"end\":55831,\"start\":55447},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":29145451},\"end\":56178,\"start\":55833},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":56398,\"start\":56180},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207930212},\"end\":56680,\"start\":56400},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b17\"},\"end\":56885,\"start\":56682},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220665753},\"end\":57135,\"start\":56887},{\"attributes\":{\"doi\":\"arXiv:1811.11387\",\"id\":\"b19\"},\"end\":57434,\"start\":57137},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":57606,\"start\":57436},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":56371872},\"end\":58012,\"start\":57608},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":102351314},\"end\":58300,\"start\":58014},{\"attributes\":{\"doi\":\"arXiv:2105.13353\",\"id\":\"b23\"},\"end\":58728,\"start\":58302},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13117003},\"end\":59000,\"start\":58730},{\"attributes\":{\"id\":\"b25\"},\"end\":59264,\"start\":59002},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":211532660},\"end\":59469,\"start\":59266},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":233033510},\"end\":59700,\"start\":59471},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":203593340},\"end\":59928,\"start\":59702},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":235377363},\"end\":60380,\"start\":59930},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8463335},\"end\":60709,\"start\":60382},{\"attributes\":{\"id\":\"b31\"},\"end\":60738,\"start\":60711},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9348728},\"end\":61012,\"start\":60740},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b33\"},\"end\":61298,\"start\":61014},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221090567},\"end\":61634,\"start\":61300},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":214802984},\"end\":61960,\"start\":61636},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9921540},\"end\":62147,\"start\":61962},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":8673768},\"end\":62403,\"start\":62149},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":232170443},\"end\":62635,\"start\":62405},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10895712},\"end\":62993,\"start\":62637},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":42918600},\"end\":63369,\"start\":62995},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14905107},\"end\":63849,\"start\":63371},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":235679473},\"end\":64177,\"start\":63851},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":231709491},\"end\":64497,\"start\":64179},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":17327960},\"end\":65037,\"start\":64499},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":235417515},\"end\":65474,\"start\":65039},{\"attributes\":{\"id\":\"b46\"},\"end\":65904,\"start\":65476},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11699847},\"end\":66162,\"start\":65906},{\"attributes\":{\"doi\":\"arXiv:1906.05743\",\"id\":\"b48\"},\"end\":66503,\"start\":66164},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":102483628},\"end\":66816,\"start\":66505},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":1055956},\"end\":67074,\"start\":66818},{\"attributes\":{\"id\":\"b51\"},\"end\":67269,\"start\":67076},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":13756489},\"end\":67591,\"start\":67271},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":210966443},\"end\":67967,\"start\":67593},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":9933254},\"end\":68170,\"start\":67969},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":49405781},\"end\":68444,\"start\":68172},{\"attributes\":{\"doi\":\"arXiv:2105.14158\",\"id\":\"b56\"},\"end\":68894,\"start\":68446},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":195504152},\"end\":69202,\"start\":68896},{\"attributes\":{\"doi\":\"arXiv:2011.10830\",\"id\":\"b58\"},\"end\":69652,\"start\":69204},{\"attributes\":{\"doi\":\"arXiv:1708.03888\",\"id\":\"b59\"},\"end\":69900,\"start\":69654},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":2608922},\"end\":70307,\"start\":69902},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":84187266},\"end\":70669,\"start\":70309}]", "bib_title": "[{\"end\":51554,\"start\":51492},{\"end\":51985,\"start\":51927},{\"end\":52669,\"start\":52563},{\"end\":53030,\"start\":52960},{\"end\":53397,\"start\":53327},{\"end\":53730,\"start\":53661},{\"end\":54008,\"start\":53956},{\"end\":54666,\"start\":54615},{\"end\":54955,\"start\":54881},{\"end\":55193,\"start\":55119},{\"end\":55925,\"start\":55833},{\"end\":56224,\"start\":56180},{\"end\":56465,\"start\":56400},{\"end\":56939,\"start\":56887},{\"end\":57478,\"start\":57436},{\"end\":57684,\"start\":57608},{\"end\":58088,\"start\":58014},{\"end\":58796,\"start\":58730},{\"end\":59328,\"start\":59266},{\"end\":59543,\"start\":59471},{\"end\":59765,\"start\":59702},{\"end\":60007,\"start\":59930},{\"end\":60432,\"start\":60382},{\"end\":60814,\"start\":60740},{\"end\":61356,\"start\":61300},{\"end\":61702,\"start\":61636},{\"end\":62010,\"start\":61962},{\"end\":62197,\"start\":62149},{\"end\":62449,\"start\":62405},{\"end\":62713,\"start\":62637},{\"end\":63081,\"start\":62995},{\"end\":63413,\"start\":63371},{\"end\":63959,\"start\":63851},{\"end\":64247,\"start\":64179},{\"end\":64574,\"start\":64499},{\"end\":65106,\"start\":65039},{\"end\":65964,\"start\":65906},{\"end\":66576,\"start\":66505},{\"end\":66879,\"start\":66818},{\"end\":67296,\"start\":67271},{\"end\":67684,\"start\":67593},{\"end\":68006,\"start\":67969},{\"end\":68209,\"start\":68172},{\"end\":68967,\"start\":68896},{\"end\":69959,\"start\":69902},{\"end\":70372,\"start\":70309}]", "bib_author": "[{\"end\":51262,\"start\":51248},{\"end\":51272,\"start\":51262},{\"end\":51284,\"start\":51272},{\"end\":51573,\"start\":51556},{\"end\":51591,\"start\":51573},{\"end\":51607,\"start\":51591},{\"end\":51997,\"start\":51987},{\"end\":52011,\"start\":51997},{\"end\":52021,\"start\":52011},{\"end\":52171,\"start\":52155},{\"end\":52184,\"start\":52171},{\"end\":52199,\"start\":52184},{\"end\":52212,\"start\":52199},{\"end\":52230,\"start\":52212},{\"end\":52245,\"start\":52230},{\"end\":52687,\"start\":52671},{\"end\":52700,\"start\":52687},{\"end\":52711,\"start\":52700},{\"end\":52723,\"start\":52711},{\"end\":52744,\"start\":52723},{\"end\":53054,\"start\":53032},{\"end\":53067,\"start\":53054},{\"end\":53085,\"start\":53067},{\"end\":53097,\"start\":53085},{\"end\":53413,\"start\":53399},{\"end\":53426,\"start\":53413},{\"end\":53437,\"start\":53426},{\"end\":53453,\"start\":53437},{\"end\":53465,\"start\":53453},{\"end\":53479,\"start\":53465},{\"end\":53743,\"start\":53732},{\"end\":53760,\"start\":53743},{\"end\":53778,\"start\":53760},{\"end\":53795,\"start\":53778},{\"end\":54025,\"start\":54010},{\"end\":54036,\"start\":54025},{\"end\":54048,\"start\":54036},{\"end\":54064,\"start\":54048},{\"end\":54078,\"start\":54064},{\"end\":54087,\"start\":54078},{\"end\":54097,\"start\":54087},{\"end\":54111,\"start\":54097},{\"end\":54305,\"start\":54293},{\"end\":54318,\"start\":54305},{\"end\":54340,\"start\":54318},{\"end\":54354,\"start\":54340},{\"end\":54360,\"start\":54354},{\"end\":54678,\"start\":54668},{\"end\":54688,\"start\":54678},{\"end\":54704,\"start\":54688},{\"end\":54715,\"start\":54704},{\"end\":54723,\"start\":54715},{\"end\":54735,\"start\":54723},{\"end\":54968,\"start\":54957},{\"end\":54982,\"start\":54968},{\"end\":54988,\"start\":54982},{\"end\":55220,\"start\":55195},{\"end\":55231,\"start\":55220},{\"end\":55241,\"start\":55231},{\"end\":55256,\"start\":55241},{\"end\":55268,\"start\":55256},{\"end\":55461,\"start\":55447},{\"end\":55484,\"start\":55461},{\"end\":55498,\"start\":55484},{\"end\":55510,\"start\":55498},{\"end\":55525,\"start\":55510},{\"end\":55542,\"start\":55525},{\"end\":55935,\"start\":55927},{\"end\":55946,\"start\":55935},{\"end\":56238,\"start\":56226},{\"end\":56253,\"start\":56238},{\"end\":56267,\"start\":56253},{\"end\":56277,\"start\":56267},{\"end\":56479,\"start\":56467},{\"end\":56490,\"start\":56479},{\"end\":56500,\"start\":56490},{\"end\":56513,\"start\":56500},{\"end\":56528,\"start\":56513},{\"end\":56697,\"start\":56682},{\"end\":56711,\"start\":56697},{\"end\":56956,\"start\":56941},{\"end\":56966,\"start\":56956},{\"end\":56976,\"start\":56966},{\"end\":56988,\"start\":56976},{\"end\":56999,\"start\":56988},{\"end\":57236,\"start\":57221},{\"end\":57249,\"start\":57236},{\"end\":57492,\"start\":57480},{\"end\":57506,\"start\":57492},{\"end\":57510,\"start\":57506},{\"end\":57700,\"start\":57686},{\"end\":57719,\"start\":57700},{\"end\":57733,\"start\":57719},{\"end\":58104,\"start\":58090},{\"end\":58118,\"start\":58104},{\"end\":58132,\"start\":58118},{\"end\":58145,\"start\":58132},{\"end\":58317,\"start\":58302},{\"end\":58332,\"start\":58317},{\"end\":58345,\"start\":58332},{\"end\":58359,\"start\":58345},{\"end\":58381,\"start\":58359},{\"end\":58387,\"start\":58381},{\"end\":58809,\"start\":58798},{\"end\":58824,\"start\":58809},{\"end\":58836,\"start\":58824},{\"end\":58853,\"start\":58836},{\"end\":59076,\"start\":59061},{\"end\":59091,\"start\":59076},{\"end\":59106,\"start\":59091},{\"end\":59123,\"start\":59106},{\"end\":59338,\"start\":59330},{\"end\":59356,\"start\":59338},{\"end\":59553,\"start\":59545},{\"end\":59571,\"start\":59553},{\"end\":59775,\"start\":59767},{\"end\":59785,\"start\":59775},{\"end\":59803,\"start\":59785},{\"end\":60020,\"start\":60009},{\"end\":60029,\"start\":60020},{\"end\":60038,\"start\":60029},{\"end\":60050,\"start\":60038},{\"end\":60065,\"start\":60050},{\"end\":60079,\"start\":60065},{\"end\":60089,\"start\":60079},{\"end\":60102,\"start\":60089},{\"end\":60117,\"start\":60102},{\"end\":60136,\"start\":60117},{\"end\":60446,\"start\":60434},{\"end\":60459,\"start\":60446},{\"end\":60471,\"start\":60459},{\"end\":60486,\"start\":60471},{\"end\":60498,\"start\":60486},{\"end\":60723,\"start\":60713},{\"end\":60829,\"start\":60816},{\"end\":60847,\"start\":60829},{\"end\":60863,\"start\":60847},{\"end\":61034,\"start\":61014},{\"end\":61044,\"start\":61034},{\"end\":61059,\"start\":61044},{\"end\":61368,\"start\":61358},{\"end\":61383,\"start\":61368},{\"end\":61396,\"start\":61383},{\"end\":61413,\"start\":61396},{\"end\":61428,\"start\":61413},{\"end\":61444,\"start\":61428},{\"end\":61453,\"start\":61444},{\"end\":61714,\"start\":61704},{\"end\":61726,\"start\":61714},{\"end\":61736,\"start\":61726},{\"end\":61748,\"start\":61736},{\"end\":61763,\"start\":61748},{\"end\":61775,\"start\":61763},{\"end\":61786,\"start\":61775},{\"end\":62029,\"start\":62012},{\"end\":62043,\"start\":62029},{\"end\":62216,\"start\":62199},{\"end\":62230,\"start\":62216},{\"end\":62466,\"start\":62451},{\"end\":62479,\"start\":62466},{\"end\":62490,\"start\":62479},{\"end\":62505,\"start\":62490},{\"end\":62730,\"start\":62715},{\"end\":62742,\"start\":62730},{\"end\":62754,\"start\":62742},{\"end\":63098,\"start\":63083},{\"end\":63110,\"start\":63098},{\"end\":63122,\"start\":63110},{\"end\":63425,\"start\":63415},{\"end\":63435,\"start\":63425},{\"end\":63449,\"start\":63435},{\"end\":63459,\"start\":63449},{\"end\":63973,\"start\":63961},{\"end\":63982,\"start\":63973},{\"end\":63999,\"start\":63982},{\"end\":64266,\"start\":64249},{\"end\":64278,\"start\":64266},{\"end\":64291,\"start\":64278},{\"end\":64310,\"start\":64291},{\"end\":64324,\"start\":64310},{\"end\":64601,\"start\":64576},{\"end\":64620,\"start\":64601},{\"end\":64642,\"start\":64620},{\"end\":64656,\"start\":64642},{\"end\":64672,\"start\":64656},{\"end\":64689,\"start\":64672},{\"end\":65121,\"start\":65108},{\"end\":65136,\"start\":65121},{\"end\":65152,\"start\":65136},{\"end\":65173,\"start\":65152},{\"end\":65187,\"start\":65173},{\"end\":65604,\"start\":65585},{\"end\":65621,\"start\":65604},{\"end\":65638,\"start\":65621},{\"end\":65654,\"start\":65638},{\"end\":65676,\"start\":65654},{\"end\":65985,\"start\":65966},{\"end\":66001,\"start\":65985},{\"end\":66022,\"start\":66001},{\"end\":66174,\"start\":66164},{\"end\":66190,\"start\":66174},{\"end\":66204,\"start\":66190},{\"end\":66221,\"start\":66204},{\"end\":66588,\"start\":66578},{\"end\":66602,\"start\":66588},{\"end\":66617,\"start\":66602},{\"end\":66631,\"start\":66617},{\"end\":66648,\"start\":66631},{\"end\":66899,\"start\":66881},{\"end\":66913,\"start\":66899},{\"end\":66934,\"start\":66913},{\"end\":67134,\"start\":67117},{\"end\":67145,\"start\":67134},{\"end\":67152,\"start\":67145},{\"end\":67314,\"start\":67298},{\"end\":67328,\"start\":67314},{\"end\":67341,\"start\":67328},{\"end\":67358,\"start\":67341},{\"end\":67371,\"start\":67358},{\"end\":67386,\"start\":67371},{\"end\":67401,\"start\":67386},{\"end\":67419,\"start\":67401},{\"end\":67697,\"start\":67686},{\"end\":67708,\"start\":67697},{\"end\":67718,\"start\":67708},{\"end\":67733,\"start\":67718},{\"end\":67748,\"start\":67733},{\"end\":67759,\"start\":67748},{\"end\":67767,\"start\":67759},{\"end\":68023,\"start\":68008},{\"end\":68041,\"start\":68023},{\"end\":68059,\"start\":68041},{\"end\":68226,\"start\":68211},{\"end\":68247,\"start\":68226},{\"end\":68262,\"start\":68247},{\"end\":68281,\"start\":68262},{\"end\":68295,\"start\":68281},{\"end\":68554,\"start\":68544},{\"end\":68564,\"start\":68554},{\"end\":68574,\"start\":68564},{\"end\":68587,\"start\":68574},{\"end\":68602,\"start\":68587},{\"end\":68616,\"start\":68602},{\"end\":68634,\"start\":68616},{\"end\":68980,\"start\":68969},{\"end\":68990,\"start\":68980},{\"end\":69001,\"start\":68990},{\"end\":69012,\"start\":69001},{\"end\":69020,\"start\":69012},{\"end\":69036,\"start\":69020},{\"end\":69286,\"start\":69273},{\"end\":69309,\"start\":69286},{\"end\":69326,\"start\":69309},{\"end\":69342,\"start\":69326},{\"end\":69355,\"start\":69342},{\"end\":69365,\"start\":69355},{\"end\":69381,\"start\":69365},{\"end\":69392,\"start\":69381},{\"end\":69712,\"start\":69702},{\"end\":69725,\"start\":69712},{\"end\":69741,\"start\":69725},{\"end\":69973,\"start\":69961},{\"end\":69990,\"start\":69973},{\"end\":70005,\"start\":69990},{\"end\":70017,\"start\":70005},{\"end\":70035,\"start\":70017},{\"end\":70390,\"start\":70374},{\"end\":70413,\"start\":70390},{\"end\":70437,\"start\":70413},{\"end\":70451,\"start\":70437},{\"end\":70464,\"start\":70451},{\"end\":70477,\"start\":70464}]", "bib_venue": "[{\"end\":51726,\"start\":51675},{\"end\":63648,\"start\":63561},{\"end\":51246,\"start\":51150},{\"end\":51673,\"start\":51607},{\"end\":52033,\"start\":52021},{\"end\":52336,\"start\":52261},{\"end\":52748,\"start\":52744},{\"end\":53128,\"start\":53097},{\"end\":53483,\"start\":53479},{\"end\":53799,\"start\":53795},{\"end\":54115,\"start\":54111},{\"end\":54430,\"start\":54376},{\"end\":54739,\"start\":54735},{\"end\":54992,\"start\":54988},{\"end\":55272,\"start\":55268},{\"end\":55616,\"start\":55558},{\"end\":55998,\"start\":55946},{\"end\":56281,\"start\":56277},{\"end\":56532,\"start\":56528},{\"end\":56762,\"start\":56727},{\"end\":57003,\"start\":56999},{\"end\":57219,\"start\":57137},{\"end\":57514,\"start\":57510},{\"end\":57795,\"start\":57733},{\"end\":58149,\"start\":58145},{\"end\":58492,\"start\":58403},{\"end\":58857,\"start\":58853},{\"end\":59059,\"start\":59002},{\"end\":59360,\"start\":59356},{\"end\":59575,\"start\":59571},{\"end\":59807,\"start\":59803},{\"end\":60143,\"start\":60136},{\"end\":60534,\"start\":60498},{\"end\":60867,\"start\":60863},{\"end\":61133,\"start\":61075},{\"end\":61457,\"start\":61453},{\"end\":61790,\"start\":61786},{\"end\":62047,\"start\":62043},{\"end\":62261,\"start\":62230},{\"end\":62509,\"start\":62505},{\"end\":62807,\"start\":62754},{\"end\":63165,\"start\":63122},{\"end\":63559,\"start\":63459},{\"end\":64003,\"start\":63999},{\"end\":64328,\"start\":64324},{\"end\":64751,\"start\":64689},{\"end\":65249,\"start\":65187},{\"end\":65583,\"start\":65476},{\"end\":66026,\"start\":66022},{\"end\":66311,\"start\":66237},{\"end\":66652,\"start\":66648},{\"end\":66938,\"start\":66934},{\"end\":67115,\"start\":67076},{\"end\":67423,\"start\":67419},{\"end\":67771,\"start\":67767},{\"end\":68063,\"start\":68059},{\"end\":68299,\"start\":68295},{\"end\":68542,\"start\":68446},{\"end\":69040,\"start\":69036},{\"end\":69271,\"start\":69204},{\"end\":69700,\"start\":69654},{\"end\":70097,\"start\":70035},{\"end\":70481,\"start\":70477}]"}}}, "year": 2023, "month": 12, "day": 17}