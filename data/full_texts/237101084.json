{"id": 237101084, "updated": "2022-01-10 12:48:17.586", "metadata": {"title": "Efficient Neural Network Verification via Layer-based Semidefinite Relaxations and Linear Cuts", "authors": "[{\"middle\":[],\"last\":\"batten\",\"first\":\"b.\"},{\"middle\":[],\"last\":\"kouvaros\",\"first\":\"p.\"},{\"middle\":[],\"last\":\"lomuscio\",\"first\":\"a.\"}]", "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We introduce an efficient and tight layer-based semidefinite relaxation for verifying local robustness of neural networks. The improved tightness is the result of the combination between semidefinite relaxations and linear cuts. We obtain a computationally efficient method by decomposing the semidefinite formulation into layerwise constraints. By leveraging on chordal graph decompositions, we show that the formulation here presented is provably tighter than current approaches. Experiments on a set of benchmark networks show that the approach here proposed enables the verification of more instances compared to other relaxation methods. The results also demonstrate that the SDP relaxation here proposed is one order of magnitude faster than previous SDP methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/BattenKLZ21", "doi": "10.24963/ijcai.2021/301"}}, "content": {"source": {"pdf_hash": "f3832b48f4032bcb31ede58e976540c7a583c47e", "pdf_src": "Anansi", "pdf_uri": "[\"https://www.ijcai.org/proceedings/2021/0301.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2021/0301.pdf", "status": "BRONZE"}}, "grobid": {"id": "3dfca5193cb9f26cd2559315e6b7f8b926ceb02c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f3832b48f4032bcb31ede58e976540c7a583c47e.txt", "contents": "\nEfficient Neural Network Verification via Layer-based Semidefinite Relaxations and Linear Cuts\n\n\nBen Batten b.batten20@imperial.ac.uk \nDepartment of Computing\nImperial College London\nUK\n\nPanagiotis Kouvaros p.kouvaros@imperial.ac.uk \nDepartment of Computing\nImperial College London\nUK\n\nAlessio Lomuscio a.lomuscio@imperial.ac.uk \nDepartment of Computing\nImperial College London\nUK\n\nYang Zheng y.zheng@imperial.ac.uk \nDepartment of Computing\nImperial College London\nUK\n\nEfficient Neural Network Verification via Layer-based Semidefinite Relaxations and Linear Cuts\n\nWe introduce an efficient and tight layer-based semidefinite relaxation for verifying local robustness of neural networks. The improved tightness is the result of the combination between semidefinite relaxations and linear cuts. We obtain a computationally efficient method by decomposing the semidefinite formulation into layerwise constraints. By leveraging on chordal graph decompositions, we show that the formulation here presented is provably tighter than current approaches. Experiments on a set of benchmark networks show that the approach here proposed enables the verification of more instances compared to other relaxation methods. The results also demonstrate that the SDP relaxation here proposed is one order of magnitude faster than previous SDP methods.\n\nIntroduction\n\nNeural networks (NNs) are known to be fragile and susceptible to adversarial attacks [Goodfellow et al., 2014]. In AIbased, safety-critical applications, it is important to formally verify that a network is correct with respect to noteworthy specifications, e.g., local adversarial robustness, before they are deployed. Current methods for NN verification can be categorized into complete and incomplete approaches. Aside from computational considerations, complete approaches are guaranteed to resolve any verification query. Incomplete approaches are normally based on various forms of convex approximations of the network, and only guarantee that whenever they output that the network is safe, then that is indeed the case. While this typically enables faster computation, the looser this approximation is, the more likely it is that the method may not be able to verify the problem instance. As a result, the present objective in incomplete methods is the development of tighter approximations, which can be efficiently computed, thereby strengthening the efficacy of the methods in answering the verification problem [Salman et al., 2019]. In this paper, we advance the state of the art towards this objective by developing a novel relaxation based on semidefinite programs (SDPs). Our SDP relaxation is provably tighter than related SDP approaches, whilst also being more efficient.\n\nRelated Work. Complete methods are either based on mixed-integer linear programming (MILP) [Bastani et al., 2016;Lomuscio and Maganti, 2017;Tjeng et al., 2019;Botoeva et al., 2020], satisfiability modulo theories [Katz et al., 2017;Ehlers, 2017], or bound propagation techniques coupled with input refinement [Wang et al., 2018;Henriksen and Lomuscio, 2020]. While these methods offer theoretical termination guarantees, at present they do not scale to the network sizes that incomplete approaches are able to address.\n\nIncomplete methods are typically based on bound propagation [Singh et al., 2018;2019b;Weng et al., 2018;, duality [Dvijotham et al., 2018;Dathathri et al., 2020;Wong and Kolter, 2018] and SDP relaxations Fazlyab et al., 2020]. A common theme in this research is the linear program (LP) relaxation for the univariate ReLU function. A foundational relaxation is the triangle relaxation from [Ehlers, 2017] which gives the tightest possible convex relaxation of the univariate ReLU function and forms the basis of many of the cited methods; see [Salman et al., 2019;Li et al., 2020] for detailed comparisons. It was recently shown that the efficacy of these methods is intrinsically limited by the same convex relaxation barrier which is characterised by the tightness of the triangular relaxation [Salman et al., 2019].\n\nTwo recent methods to bypass this barrier are the kPoly [Singh et al., 2019a] and the OptC2V [Tjandraatmadja et al., 2020]; both of these offer tighter LP relaxations by considering interactions of multiple neurons and multivariate inputs. Another way to bypass the barrier is to seek alternative stronger relaxations beyond LPs, such as SDPs Fazlyab et al., 2020]. It has been empirically observed that the SDP relaxation in   Contributions. We develop a novel, efficient, layer-based SDP relaxation for NN verification. Unlike [Dathathri et al., 2020] that centres around first-order algorithm developments, we here focus on tightening the SDP relaxations, and on exploiting the cascading structure of NNs for improved efficiency. Specifically, we first extend the SDP formulation in  with linear cuts from the triangle relaxation [Ehlers, 2017]. This leads to a new SDP relaxation that is provably tighter than both [Raghunathan et al., 2018;Dathathri et al., 2020] and the triangle relaxation from [Ehlers, 2017] (Proposition 1). We further show that said linear cuts enable the exploitation of the activation pattern of NNs to simplify our SDP formulation (Proposition 2). By exploiting the cascading structure of NNs, we develop an equivalent layer decomposition to the SDP approach which enjoys a significant computational speed-up (Proposition 3). This layer decomposition strategy is motivated by the advances in sparse SDPs [Vandenberghe and Andersen, 2015].\n\nWe evaluated our approach on benchmarks from Singh et al., 2019a;Dathathri et al., 2020;. The experiments reported that the layer-based SDP proposed here offers better accuracy, while being an order of magnitude faster than . Also, the method could verify more images than the state-of-the-art LP-based methods such as kPoly [Singh et al., 2019a] and OptC2V [Tjandraatmadja et al., 2020] for some networks, while the computational cost of the verification step remained in the same order of magnitude.\n\nThe rest of the paper is organised as follows: we introduce the neural network verification problem and two commonly used relaxations in Section 2. In Section 3, we present new SDP relaxations via linear cuts and layer-based decomposition. Section 4 reports a comprehensive evaluation of the method on various NNs that are commonly used in the literature. We derive some conclusions in Section 5.\n\n\nPreliminaries\n\nWe here outline the notation, present the verification problem, and introduce two widely used convex relaxations. Feed-forward ReLU neural networks (NNs). We consider an L-layer feed-forward NN f (x 0 ) : R d \u2192 R m . We usex i \u2208 R ni and x i \u2208 R ni to denote the pre-activation and activation vectors of the i-th layer, and define the NN output as f (x 0 ) :\n= W L x L + b L , with x i+1 = ReLU(x i+1 ) andx i+1 = W i x i + b i , i = 0, . . . , L \u2212 1, where W i \u2208 R ni+1\u00d7ni , b i \u2208 R ni+1\nare the weights and biases, respectively, n 0 = d, n L+1 = m are input and output dimensions, and the ReLU function is defined as ReLU(z) = max(z, 0) for z \u2208 R (the ReLU function is applied element-wise). We focus on classification networks whereby an input x 0 is assigned to the class associated with the network output of the highest value:\ni = arg max i=1,...,m f (x 0 ) i .\nVerification problem. Given a NN f : R d \u2192 R m , a nominal inputx \u2208 R d , a linear function \u03c6, also called the specification, on the network's outputs, and a perturbation radius \u2208 R, the verification problem we study is to determine whether\n\u03c6(y) > 0, subject to y = f (x 0 ), x 0 \u2212x \u221e \u2264 ,(1)\nwhere \u00b7 \u221e denotes the standard l \u221e norm of a vector. In particular, we hereafter focus on the local adversarial robustness problem whereby the specification is \u03c6(y) = y(i ) \u2212 y(i) for a target label i. A network is said to be certifiably robust on inputx and perturbation radius if the answer to the verification problem (1) is true for all i = i .\n\nVerification via mathematical optimisation. Problem (1) can be answered by solving the optimisation problem\n\u03b3 := min x0,...,x L c T x L + c 0 subject to x i+1 = ReLU(W i x i + b i ), i \u2208 [L], (2a) x 0 \u2212x \u221e \u2264 ,(2b)\nwhere (1) is true if and only if the optimal value, \u03b3 , of (2) is positive. The optimisation problem is however non-convex because of (2a) and is therefore generally difficult to solve. To obtain a tractable convex relaxation of the problem, we derive an outer-approximation of the feasible region (x 0 , x 1 , . . . , x L ) in (2) using a convex set, D. This relaxes (2) to a convex problem\nc T = W L (i , :) \u2212 W L (i, :), c 0 = b L (i ) \u2212 b L (i), and [L] denotes {0, 1, . . . , L \u2212 1}. The verification problem\u03b3 D := min (x0,x1,...,x L )\u2208D c T x L + c 0 (3)\nwhich provides a valid lower bound, \u03b3 \u2265 \u03b3 D . If \u03b3 D > 0, then the answer to the verification problem (1) is true. If however \u03b3 > 0 \u2265 \u03b3 D , then the verification problem cannot be decided. In this paper we are concerned with two convex relaxations: the triangle relaxation [Ehlers, 2017] and the semidefinite relaxation . Triangle relaxation. The triangle relaxation approximates a single univariate ReLU function z = max{x, 0} with its convex hull. Specifically, the ReLU constraints (2a) are approximated by a set of linear constraints\nx i+1 \u2265 0, x i+1 \u2265x i+1 , (4a) x i+1 \u2264 k i (x i+1 \u2212l i+1 ) + ReLU(l i+1 ),(4b)x i+1 = W i x i + b i ,l i+1 \u2264x i+1 \u2264\u00fb i+1 ,(4c)\nwhere i \u2208 [L], denotes the Hadamard product,\nk i := ReLU(\u00fbi+1)\u2212ReLU(li+1) ui+1\u2212li+1\n, and\u00fb i+1 ,l i+1 \u2208 R ni+1 are upper and lower bounds of the pre-activation variablex i+1 for any input satisfying (2b); these bounds can be computed using interval propagation methods, see, e.g., [Wong and Kolter, 2018;Wang et al., 2018]. The optimal value, \u03b3 LP , of the resulting LP relaxation is relatively easy to compute in practice. However, the quality of the LP relaxation (4) is intrinsically limited, i.e., there is always a positive gap \u03b3 \u2212 \u03b3 LP > 0 for many practical NNs, referred to as the convex relaxation barrier Semidefinite relaxation. This relaxation utilizes a single positive semidefinite (PSD) constraint that couples all ReLU constraints in (2a) to obtain a convex SDP [Raghunathan et al., 2018]. The key idea is to equivalently replace the ReLU constraints (2a) with the following quadratic constraints\nx i+1 \u2265 0, x i+1 \u2265 W i x i + b i , i \u2208 [L], (5a) x i+1 (x i+1 \u2212 W i x i \u2212 b i ) = 0, i \u2208 [L].(5b)\nAlso, the input constraint (2b) as well as the lower and upper bounds l i , u i \u2208 R ni on the activation vectors x i , i = 1, . . . , L \u2212 1 (which can be obtained using interval prorogation methods) can be reformulated as quadratic constraints\nx i x i \u2212 (l i + u i ) x i + l i u i \u2264 0, i \u2208 [L], (6) where i = 0 corresponds to the l \u221e input constraint (2b).\nPolynomial lifting and SDP-based hierarchies can be used to solve the resulting polynomial optimisation problem. Specifically a lifting matrix, P , of monomials\nP := vv T , where v := 1, x T 0 , x T 1 , . . . , x T L T \u2208 R 1+ L i=0 ni\ncan be defined . Then, all the constraints in (5) and (6) become linear in terms of the elements of P . By relaxing the monomial matrix P to be P 0, we obtain an SDP relaxation of (2) as follows min\nP c T P [x L ] + c 0 subject to P [x i+1 ] \u2265 0, P [x i+1 ] \u2265 W i P [x i ] + b i , (7a) diag P [x i+1 x T i+1 ] \u2212 W i P [x i x T i+1 ] \u2212 b i P [x i+1 ] = 0, i \u2208 [L] (7b) diag P [x i x T i ] \u2212 (l i + u i ) P [x i ] + l i u i \u2264 0, i \u2208 [L] (7c) P [1] = 1, P 0,(7d)\nwhere we adopt the same symbolic indexing P [\u00b7] as  to index the elements of P . It is clear that (7a) and (7b) correspond to the ReLU constraints (5), and that (7c) corresponds to the bounds on activation vectors in (6). We denote the optimal value of (7) as \u03b3 SDP,1 . We always have \u03b3 \u2265 \u03b3 SDP,1 , where the equality is achieved if the optimal solution, P , to (7) is of rank one. The exactness of a variant of the SDP relaxation (7) is discussed in  via geometric techniques.\n\n\nLinear Cuts and Layer-based Semidefinite Relaxations\n\nIn this section we develop new SDP relaxations for the verification problem (2). The resulting formulations will lead to both tighter and computationally more efficient relaxations. We begin our analysis by observing two potential drawbacks from the SDP relaxation (7). The first is that, despite the use of the PSD constraint P 0, the relaxation quality of (7) may be looser than the LP relaxation (4), i.e., \u03b3 LP > \u03b3 SDP,1 . The second is that the matrix variable, P , in the constraint P 0, is of size N \u00d7 N , where N = 1 + L i=0 n i corresponds to the network size; this may hinder the scalability of the approach for large networks. We resolve the first shortcoming by adding the linear cuts from (4) to the SDP relaxation (7), leading to a provably tighter SDP relaxation. We alleviate the second by introducing a layer-based SDP relaxation consisting of multiple PSD constraints of smaller sizes.\n\n\nTightened SDP Relaxations via Linear Cuts\n\nIt is known, see , that in certain cases the SDP relaxation (7) can be looser than the LP relaxation (4). Figure 1 illustrates this for the case of a single \n(x, z) \u2208 R 2 | z = ReLU(x), l \u2264 x \u2264 u}.\nLeft to right: 1) unstable neuron l = \u22124, u = 1; 2) inactive neuron l = \u22124, u = 0; 3) strictly active neuron l = 0, u = 1. The standard SDP relaxation (7) is inexact even for inactive/stable neurons, while the triangular relaxation becomes exact. This motivates us to add linear cuts (8) to the SDP (9).\n\nReLU neuron with a univariate input. As shown in Figure 1, the feasible region in the LP relaxation (4) becomes exact when u \u2264 0 (inactive neuron) or l \u2265 0 (strictly active neuron), whereas the feasible region in the SDP relaxation (7) is not exact unless l = u. As can be observed from the feasible regions, the tightness of the LP relaxation w.r.t the SDP relaxation is attributed to the linear cut (4b), which at times tightens the LP relaxation.\n\nTo resolve this, we extend the SDP relaxation (7) to include the linear cut (4b) thereby tightening the relaxation. We express the cut (4b) in terms of the matrix P as follows\nP [x i+1 ] \u2264 k i W i P [x i ] + b i \u2212l i + ReLU(l i ),(8)\nand add it to (7). This leads to the following SDP relaxation for the verification problem (2) \n\nDue to the linear cuts (8), the new SDP relaxation (9) is provably tighter than both the original SDP relaxation (7) and the standard triangle LP relaxation (4). Proposition 1. Given a non-convex NN verification instance (2), we have that \u03b3 \u2265 \u03b3 SDP,2 \u2265 max{\u03b3 SDP,1 , \u03b3 LP }.\n\nThe proof of \u03b3 \u2265 \u03b3 SDP,2 \u2265 \u03b3 SDP,1 is straightforward. To establish \u03b3 SDP,2 \u2265 \u03b3 LP , it suffices to show that any feasible solution in (9) can be used to construct a feasible solution in (4) with the same cost value.\n\nWe now compare SDP to LP relaxations. As already noted in , the single PSD constraint, P 0 in (7), captures the interaction of ReLU constraints (2a) implicitly, while the triangle relaxation (4) relaxes (2a) individually. However, this constraint P 0 alone does not guarantee an improved relaxation. The linear cuts (4) can still effectively remove some redundant outer-approximation in SDP relaxations (Proposition 1). Recent approaches such as kPoly [Singh et al., 2019a] and OptC2V [Tjandraatmadja et al., 2020] strengthen the standard triangular LP (4) by adding additional linear cuts via relaxing multiple ReLU neurons together, or reasoning multivariate inputs directly. Those linear cuts can be potentially combined with the SDP relaxation (9), leading to a tighter relaxation for (2).\n\nAnother flexibility in SDP relaxations lies in the fact that they allow the reformulation of different linear constraints into quadratic constraints and then their linearisation in terms of the matrix variable P . This is known as the reformulationlinearisation technique (RLT) [Anstreicher, 2009]. We have used this technique to derive (7c) from (6), which often tightens the SDP relaxation.\n\n\nSimplified SDP Relaxation via Activation Patterns\n\nIt is a common practice in LP-based relaxations and MILP formulations (e.g., [Singh et al., 2019b;Weng et al., 2018;Botoeva et al., 2020]) to simplify the verification problem (2) using the NN's activation pattern. After incorporating the linear cuts (8), the SDP relaxation (9) can also use the activation pattern to reduce the dimension of the PSD constraint P 0; this was not considered in Dathathri et al., 2020]. Particularly, given lower and upper bounds on the pre-activation vectorl i+1 \u2264x i+1 \u2264\u00fb i+1 , it is known that the constraints (4) for stable neurons of the (i+1)-th layer become exact and can be simplified: 1) if the kth neuron is strictly active, i.e.,\u00fb i+1 (k) \u2265l i+1 (k) \u2265 0, then\nx i+1 (k) = W i (k, :)x i + b i (k), or 2) if the neuron is inactive, i.e., 0 \u2265\u00fb i+1 (k) \u2265l i+1 (k), then x i+1 (k) = 0.\nThe information regarding inactive neurons can also be removed in (9) since P [x i+1 ](k) becomes zero due to the linear cuts (8). This effectively reduces the dimension of the PSD constraint P 0 without altering the optimal value. Proposition 2. Consider a non-convex NN verification instance (2), with n i neurons in each layer, i = 0, 1, ..., L. Given a set of lower and upper bounds l i , u i ,l i+1 ,\u00fb i+1 , i \u2208 [L], suppose the number of unstable and strictly active neurons isn i . Then, in (9), the PSD constraint P 0 of size (1 + L i=0 n i ) \u00d7 (1 + L i=0 n i ) can be equivalently replaced by a smaller PSD constraintP 0 of size (1 + L i=0n i ) \u00d7 (1 + L i=0n i ). In many practical cases, a significant portion of the neurons are stable under a given verification query, especially when small perturbation radiuses, , are considered. Thus, adding the linear cuts (8) not only makes the SDP relaxation (9) theoretically stronger (cf. Proposition 1), but also computationally easier (cf. Proposition 2) than (7) in [Raghunathan et al., 2018]. We note that it is possible to first prune the inactive neurons to form a new NN, and then apply the SDP (7) in  to this newly pruned NN. This process implicitly uses the power of linear cuts from (4), but was not discussed nor implemented in Dathathri et al., 2020].\n\n\nLayer-based SDP Relaxations\n\nWe here further reduce the dimensionality of the PSD constraint in (9) by exploiting the layer-wise cascading structure of NNs, whereby each activation vector of a layer depends only on the previous layer's activation vector. We begin with the equivalent quadratic formulation of (5). Instead of using a single big matrix, P , as in (7), we introduce multiple matrices of monomials P i for each i \u2208 [L]:\nP i := v i v T i , where v i := 1, x T i , x T i+1 T \u2208 R 1+ni+ni+1 . (10)\nThen, the constraints (5a)-(5b) become linear in P i :\nP i [x i+1 ] \u2265 0, P i [x i+1 ] \u2265 W i P i [x i ] + b i , (11a) diag P i [x i+1 x T i+1 ]\u2212W i P i [x i x T i+1 ] = b i P i [x i+1\n]. (11b) Also, (7c) and (8) can be written with respect to P i as\ndiag P i [x i x T i ] \u2212 (l i + u i ) P i [x i ] + l i u i \u2264 0, (12a) P i [x i+1 ] \u2264 k i W i P i [x i ] + b i \u2212l i + ReLU(l i ). (12b)\nUpon relaxing the monomial matrices P i to P i 0, we need to consider the input-output consistency among the P i 's, i.e.,\nP i [x i+1x T i+1 ] = P i+1 [x i+1x T i+1 ], i = 0, . . . , L \u2212 2,(13)\nwherex T i+1 := 1, x T i+1 . Now, we can introduce a new layer-based SDP relaxation for the verification problem (2):\n\u03b3 SDP,3 := min P0,...,P L\u22121 c T P L\u22121 [x L ] + c 0 subject to (11a), (11b), , i \u2208 [L], (12a), (12b), , i \u2208 [L], P i [1] = 1, P i 0, i \u2208 [L],(13).(14)\nInstead of a single, big PSD constraint, P 0 of network size in (9), the layer-based SDP relaxation (14) employs multiple smaller PSD constraints P i 0 for each layer. Smaller PSD constraints in an SDP are helpful to improve the efficiency of getting its solution using off-the-self solvers [Vandenberghe and Andersen, 2015;Zheng et al., 2020]. Moreover, the solution quality (14) is equivalent to that from (9). Formally, we have:\n\nProposition 3. Given a non-convex NN verification instance (2), we have that \u03b3 \u2265 \u03b3 SDP,3 = \u03b3 SDP,2 .\n\nProof. The proof relies on a celebrated chordal decomposition result for sparse PSD matrices [Vandenberghe and Andersen, 2015, Chapter 10]. In particular, the cascading structure in a network can be abstracted as a chain graph, which is chordal with L maximal cliques\nC i = {i, i + 1}, i \u2208 [L].\nThe block-chordal completion theorem [Zheng, 2019, Theorem 2.18] allows equivalently replacing the single PSD constraint P 0 in (9) with L smaller PSD constraints P i 0 in (14), each of which corresponds to maximal clique C i . Now, (14) becomes a reformulation of (9) via the consistency constraint (13).\n\nRemark 1 (Merging layers and conversion methods). Intuitively, (14) belongs to a class of chordal conversion methods in sparse SDPs [Fukuda et al., 2001;Zheng et al., 2020]. Besides two consecutive layers in (10), one can merge multiple layers (e.g., the first three layers) as one clique and form one corresponding PSD variable. The resulting SDP relaxation is equivalent to (9) and (14), meaning that they offer the same optimal solution. Similar to general conversion methods in SDPs [Fukuda et al., 2001;Zheng et al., 2020], there exists a tradeoff between the size of P i 0 and the number of additional equality constraints (13) for the efficiency of solving (14). Some heuristics exist to balance such a tradeoff [Fukuda et al., 2001]. Further relaxation via dropping equality constraints. As discussed above, the number of equality constraints (13) is quadratic in the number of neurons in each layer. Here, we consider an SDP relaxation that uses only a subset of the constraints in (13). In particular, we consider a linear number of consistency constraints as\nP i [x i+1 ] = P i+1 [x i+1 ], i = 0, . . . , L \u2212 2,(15)\nand form another layer-based SDP relaxation:\n\u03b3 SDP,4 := min P0,...,P L\u22121 c T P L\u22121 [x L ] + c 0 subject to (11a), (11b), , i \u2208 [L], (12a), (12b), , i \u2208 [L], P i [1] = 1, P i 0, i \u2208 [L],(15)\n.\n\nThe solution quality of (16) is worse than (14) but is faster to solve and it is still provably better than the LP relaxation (4), i.e., \u03b3 \u2265 \u03b3 SDP,3 \u2265 \u03b3 SDP,4 \u2265 \u03b3 LP . The proof is similar to Proposition 1.\n\n\nExperimental Evaluation\n\nWe now evaluate the performance of the proposed SDP formulations on several benchmarks from the literature. We \n\n\nImplementation and Experiment Setup\n\nVerification problem. We consider the standard robustness verification problem for image classifiers: given a correctly classified image, verify that the NN returns the same label for all input within an l \u221e perturbation of . Formally, given an imagex \u2208 [0, 1] d with a label i and a radius , a neural network is verified to be robust on (x, ) if \u03b3 in (2) is positive for all i = i . For LP-and SDP-based relaxation methods, we solve (2) multiple times for every potential adversarial target i = i , and check whether the lower bound is positive. Implementation of LP/SDP relaxations. We consider the formulation (7), originally proposed in  and our SDP formulations from (9), (14), and (16) for experiments. Note that (9) and (14) are equivalent. We used a subroutine from SparseCoLO [Fujisawa et al., 2009] for layer-decomposition that balances the size of PSD constraints and equality constraints (see Remark 1). We refer to (9) and (14) as LayerSDP, and its relaxed version (16) as FastSDP. We also consider the standard LP relaxation (4) for benchmark. We denote (7) as SDP-IP . The lower and upper bounds l i , u i ,l i+1 ,\u00fb i+1 were computed using a symbolic interval propagation algorithm in [Botoeva et al., 2020]. To get an upper bound of verified accuracy, we run a standard projected gradient descent (PGD) algorithm from [Dathathri et al., 2020]. For numerical computation, we need to convert the convex relaxations into a standard conic optimization before passing them to a numerical solver. The authors in  used the YALMIP toolbox [Lofberg, 2004] for the modelling process. However, we found that YALMIP introduced too much overhead time consumption, also because very similar instances of (2) need to be converted multiple times. Besides, we found that the YALMIP toolbox neglected the cascading structure in SDP relaxations. Thus, we implemented an automatic transformation from the convex relaxations into standard conic optimization. The resulting LP/SDPs were then solved by MOSEK [Mosek, 2015]. We use the time reported by MOSEK for comparison.\n\nNeural Networks. We considered eight fully connected ReLU networks trained on the MNIST dataset. To facilitate the comparison with existing tools, we divided our experiments into three groups: 1) One self-trained NN with two hidden layers, each having 64 neurons; no adversarial training was used. We varied the perturbation radius, , from 0.01 to 0.05; 2) Three NNs from : MLP-SDP, MLP-LP, and MLP-Adv. We followed closely the setup described in Dathathri et al., 2020], and tested a perturbation radius For each network, we verified the first 100 images from the MNIST test set and excluded those incorrectly classified. We performed our experiments on an Intel(R) i9-10850K CPU 3.60GHz machine with 32 GB of RAM, except for SDP-FO which was carried out on an Intel i7-1065G7 with 15GB RAM, due to a different implementation from [Dathathri et al., 2020]. , 2020], and standard LP. As expected (cf. Proposition 1), our formulation, LayerSDP, offered improved robust accuracies than SDP-IP and LP across different . Interestingly, we observe that SDP-IP verified fewer images than the standard LP relaxation when = 0.03, 0.035, 0.04, and required longer time. This indicates that the behavior in Figure 1 persists in practical NN verification, confirming the tightness of LayerSDP. Furthermore, a combination of inactive neuron pruning (Proposition 2) and layer decomposition (Section 3.3) made LayerSDP and FastSDP two orders of magnitude faster to solve than SDP-IP. We observe that SDP-FO   verified fewer images than SDP-IP using similar time.\n\n\nVerification Results\n\nThe results in Table 1 demonstrate that LayerSDP is also much faster than SDP-IP, while being more precise than the LP baseline across the networks in Dathathri et al., 2020;Singh et al., 2019a;. Furthermore, for the robustly trained NNs (MLP-Adv, MLP-SDP, MLP-LP), our LayerSDP achieved a very good verified accuracy compared to PGD, with MLP-SDP and MLP-LP matched; this is consistent with the experiments in [Dathathri et al., 2020]. However, we found that SDP- FO [Dathathri et al., 2020] is very sensitive to some hyper-parameters due to the nature of subgradient algorithms.\n\nCompared to the SoA LP-based methods, kPoly [Singh et al., 2019a] and OptC2V [Tjandraatmadja et al., 2020], our LayerSDP significantly improved the verified accuracy for 6\u00d7100 and 6\u00d7200 networks, while remaining competitive for the other two networks. We observe that the linear cuts in kPoly and OptC2V can be potentially combined in LayerSDP to obtain a stronger relaxation. Finally, while the timings in [Singh et al., 2019a; may not be comparable, the authors reported average times for kPoly within a range of 2 minutes to 8 minutes per image, and for OptC2V within a range of 2 minutes to 58 minutes per image.\n\n\nConclusions\n\nWe have presented a new layer-based semidefinite relaxation, LayerSDP, that is provably tighter than the SDP relaxations in Dathathri et al., 2020]. The additional linear cuts and layer decomposition also make LayerSDP an order of magnitude faster to solve than  using off-the-shelf solvers. Experiments on a set of fully connected NNs demonstrated the tightness and computational efficiency of LayerSDP. Note that convolutional neural networks have inherent sparsity and structure in convolutional layers, for which chordal graph decomposition is directly applicable and beneficial [Vandenberghe and Andersen, 2015;Zheng et al., 2020]. We leave for further work how to unroll efficiently the convolution layers and incorporate decompositions. Similarly, we would also like to explore customized algorithms similar to [Dathathri et al., 2020] for solving SDPs by exploiting the cascading network structures. Finally, we note that SDP-based approaches can easily deal with other robustness specifications involving linear and quadratic constraints on network inputs. This is a further direction that appears worth exploring.\n\n\n[Salman et al., 2019]. Two recent approaches that improve the tightness of the LP relaxation are the kPoly [Singh et al., 2019a], which explicitly considers the interaction of multiple ReLU constraints, and the OptC2V [Tjandraatmadja et al., 2020], which explicitly considers multivariate inputs of a single ReLU constraint.\n\nFigure 1 :\n1LP and SDP-based outer approximations of {\n\nc\nT P [x L ] + c 0 subject to (7a), (7b), (7c), (7d) (8), i \u2208 [L].\n\n\ncompare the resulting implementation against other state-ofthe-art (SoA) convex relaxation methods, including the original SDP formulation in [Raghunathan et al., 2018], the advanced SDP-FO algorithm [Dathathri et al., 2020], and two recent SoA LP relaxation methods, i.e., kPoly [Singh et al., 2019a] and OptC2V [Tjandraatmadja et al., 2020].\n\n\n= 0.1; 3) Four deep NNs from [Singh et al., 2019a]: 6\u00d7100 ( = 0.026), 9\u00d7100 ( = 0.026), 6\u00d7200 ( = 0.015), 9\u00d7200 ( = 0.015). These values were used in [Singh et al., 2019a; Tjandraatmadja et al., 2020] and cited there as challenging.\n\nFigure 2\n2reports the verified accuracy for the 64\u00d72 network with different perturbation radius, , using different verifiers: LayerSDP and FastSDP (from this paper), SDP-IP [Raghunathan et al., 2018], SDP-FO [Dathathri et al.\n\nFigure 2 :\n2Verified accuracy and runtime per image across various perturbation radius by different methods. These results were run for a 64\u00d72 neural network. As expected, our method LayerSDP consistently offered improved accuracy and was two orders of magnitude faster than SDP-IP.\n\n\nis much tighter than LP relaxations. However, SDPs are computationally harder solve. To overcome this, a recent work [Dathathri et al., 2020] develops a customized subgradient algorithm to solve a dual SDP relaxation to the one introduced in [Raghunathan et al., 2018].\n\n\n6% 52.8% \u2020 : These results are taken from previously reported values (SDP-IP from [Raghunathan et al., 2018], kPoly from [Singh et al., 2019a], and OptC2V from [Tjandraatmadja et al., 2020]); Dashes (-) indicate previously reported numbers are unavailable. \u2021 : We re-run the implementation of SDP-FO from [Dathathri et al., 2020], and the verified accuracies are slightly lower than reported numbers due to different hyper-parameters. indicates SDP-FO failed to verify any instance within maximum iterations. * : To facilitate time consumption comparison, we re-run SDP-IP [Raghunathan et al., 2018] over three images for these networks on our machine, and took an average per image.Accuracy \n\nLayerSDP \nFastSDP \nSDP-IP \nSDP-FO  \u2021 \nLP \nkPoly OptC2V \n\nModels \nCorrect PGD \nverified \ntime verified \ntime verified  \u2020 time  *  verified time \nverified verified  \u2020 verified  \u2020 \nMLP-Adv \n98% \n94% \n91% \n159.8 \n72% \n81.1 \n82% \n3 974 \n84% \n811.2 \n65% \n-\n-\nMLP-LP \n89% \n80% \n80% \n12.1 \n80% \n2 453 \n78% \n43.3 \n79% \n-\n-\nMLP-SDP \n98% \n84% \n84% \n3 392 \n80% \n17 726 \n64% \n153.3 \n35% \n-\n-\n6 \u00d7 100 \n99% \n91% \n75% \n545.3 \n24% \n31.3 \n-\n2 456 \n21% \n44.1% 42.9% \n9 \u00d7 100 \n97% \n86% \n35% \n470.3 \n18 % \n38.8 \n-\n2 386 \n18% \n36.9% 38.4% \n6 \u00d7 200 \n99% \n96% \n92% \n2 133 \n33 % \n89.2 \n-\n16 030 \n30% \n57.4% 60.1% \n9 \u00d7 200 \n97% \n91% \n42% \n1 874 \n27 % \n130.8 \n-\n19 698 \n27% \n50.\n\nTable 1 :\n1Verified accuracy and runtime per image (in seconds) for a set of NNs used inSingh et al., 2019a;Dathathri et al., 2020]. LayerSDP and FastSDP are identical for NNs with one hidden layer (denoted as ).\nProceedings of the Thirtieth International Joint Conference on Artificial Intelligence \nAcknowledgementsThis work is partly funded by DARPA under the Assured Autonomy programme (FA8750-18-C-0095), and the UKRI Centre for Doctoral Training in Safe and Trusted Artificial Intelligence. Alessio Lomuscio is supported by a Royal Academy of Engineering Chair in Emerging Technologies.\nSemidefinite programming versus the reformulation-linearization technique for nonconvex quadratically constrained quadratic programming. Anderson , Measur-Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. 43Math. Program.. IJCAI-21References [Anderson et al., 2020] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and J. Vielma. Strong mixed-integer programming formulations for trained neural networks. Math. Program., pages 1-37, 2020. [Anstreicher, 2009] K. Anstreicher. Semidefinite program- ming versus the reformulation-linearization technique for nonconvex quadratically constrained quadratic program- ming. J. Global Optim., 43:471-484, 2009. [Bastani et al., 2016] O. Bastani, Y. Ioannou, L. Lampropou- los, D. Vytiniotis, A. Nori, and A. Criminisi. Measur- Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)\n\nPushmeet. Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming. Botoeva, arXiv:1803.06567NeurIPS16. AAAI Press20arXiv preprintEfficient verification of neural networks via dependency analysis. A dual approach to scalable verification of deep networksing neural net robustness with constraints. In NeurIPS16, pages 2613-2621, 2016. [Botoeva et al., 2020] E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and R. Misener. Efficient verification of neu- ral networks via dependency analysis. In AAAI20, pages 3291-3299. AAAI Press, 2020. [Dathathri et al., 2020] S. Dathathri, K. Dvijotham, A. Ku- rakin, A. Raghunathan, J. Uesato, R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. Liang, and K. Push- meet. Enabling certification of verification-agnostic net- works via memory-efficient semidefinite programming. NeurIPS20, pages 5318-5331, 2020. [Dvijotham et al., 2018] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 2018.\n\nPappas. Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming. ; R Ehlers, ; M Ehlers, M Fazlyab, G J Morari, S Fujisawa, M Kim, Y Kojima, M Okamoto, M Yamashita ; M. Fukuda, K Kojima, K Murota, Nakata, Formal verification of piece-wise linear feed-forward neural networks. In ATVA17. Springer10482IEEE Trans. Automat. Contr.. Exploiting sparsity in semidefinite programming via matrix completion I: General frameworkEhlers, 2017] R. Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In ATVA17, volume 10482, pages 269-286. Springer, 2017. [Fazlyab et al., 2020] M. Fazlyab, M. Morari, and G. J. Pap- pas. Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite pro- gramming. IEEE Trans. Automat. Contr., pages 1-1, 2020. [Fujisawa et al., 2009] K. Fujisawa, S. Kim, M. Kojima, Y. Okamoto, and M. Yamashita. User's manual for Spar- seCoLO: Conversion methods for sparse conic-form lin- ear optimization problems. Dept. of Math. and Comp. Sci. Japan, Tech. Rep., pages 152-8552, 2009. [Fukuda et al., 2001] M. Fukuda, M. Kojima, K. Murota, and K. Nakata. Exploiting sparsity in semidefinite pro- gramming via matrix completion I: General framework.\n\nHenriksen and A. Lomuscio. Efficient neural network verification via adaptive refinement and adversarial search. Goodfellow, arXiv:1412.6572ECAI 2020. IOS Press11arXiv preprintExplaining and harnessing adversarial examplesSIAM J. Optim., 11(3):647-674, 2001. [Goodfellow et al., 2014] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial exam- ples. arXiv preprint arXiv:1412.6572, 2014. [Henriksen and Lomuscio, 2020] P. Henriksen and A. Lo- muscio. Efficient neural network verification via adaptive refinement and adversarial search. In ECAI 2020, pages 2513-2520. IOS Press, 2020.\n\nLomuscio and Maganti, 2017] A. Lomuscio and L. Maganti. An approach to reachability analysis for feed-forward relu neural networks. CoRR, abs/1706.07351. arXiv:2009.04131Reluplex: An efficient SMT solver for verifying deep neural networks. In CAV17. Vandenberghe and Andersen10426arXiv preprintEfficient formal safety analysis of neural networks. Towards fast computation of certified robustness for ReLU networks. Wong and Kolter, 2018] E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytopeet al., 2017] G. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. In CAV17, volume 10426, pages 97-117. Springer, 2017. [Katz et al., 2019] G. Katz, D. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim, P. Shah, S. Thakoor, H. Wu, A. Zeljic, D. Dill, M. Kochenderfer, and C. Barrett. The Marabou framework for verification and analysis of deep neural net- works. In CAV19, pages 443-452, 2019. [Li et al., 2020] L. Li, X. Qi, T. Xie, and B. Li. Sok: Cer- tified robustness for deep neural networks. arXiv preprint arXiv:2009.04131, 2020. [Lofberg, 2004] J. Lofberg. Yalmip: A toolbox for modeling and optimization in matlab. In ICRA04. IEEE, 2004. [Lomuscio and Maganti, 2017] A. Lomuscio and L. Maga- nti. An approach to reachability analysis for feed-forward relu neural networks. CoRR, abs/1706.07351, 2017. [Mosek, 2015] ApS Mosek. The mosek optimization tool- box for matlab manual, 2015. [Raghunathan et al., 2018] A. Raghunathan, J. Steinhardt, and P. Liang. Semidefinite relaxations for certifying ro- bustness to adversarial examples. In NeurIPS18, pages 10877-10887, 2018. [Salman et al., 2019] H. Salman, G. Yang, H. Zhang, C. Hsieh, and P. Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In NeurIPS19, pages 9835-9846, 2019. [Singh et al., 2018] G. Singh, T. Gehr, M. Mirman, M. P\u00fcschel, and M. Vechev. Fast and effective robustness certification. In NeurIPS18, pages 10802-10813, 2018. [Singh et al., 2019a] G. Singh, R. Ganvir, M. P\u00fcschel, and M. Vechev. Beyond the single neuron convex barrier for neural network certification. In NeurIPS19, pages 15098- 15109, 2019. [Singh et al., 2019b] G. Singh, T. Gehr, M. P\u00fcschel, and M. Vechev. An abstract domain for certifying neural net- works. Proceedings of the ACM on Programming Lan- guages, 3(POPL):41, 2019. [Tjandraatmadja et al., 2020] C. Tjandraatmadja, R. Ander- son, J. Huchette, W. Ma, K. PATEL, and J. Vielma. The convex relaxation barrier, revisited: Tightened single- neuron relaxations for neural network verification. In NeurIPS20, pages 21675-21686, 2020. [Tjeng et al., 2019] V. Tjeng, K. Xiao, and R. Tedrake. Eval- uating robustness of neural networks with mixed integer programming. In ICLR19, 2019. [Vandenberghe and Andersen, 2015] L. Vandenberghe and M. Andersen. Chordal graphs and semidefinite optimiza- tion. Foundations and Trends in Optimization, 1(4), 2015. [Wang et al., 2018] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Efficient formal safety analysis of neural net- works. In NeurIPS18, pages 6367-6377, 2018. [Weng et al., 2018] T. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, D. Boning, I. Dhillon, and L. Daniel. To- wards fast computation of certified robustness for ReLU networks. In ICML18, pages 5276-5285, 2018. [Wong and Kolter, 2018] E. Wong and Z. Kolter. Prov- able defenses against adversarial examples via the convex outer adversarial polytope. In ICML18, pages 5286-5295.\n\nOn the tightness of semidefinite relaxations for certifying robustness to adversarial examples. R Zhang ; Y. Zheng, G Fantuzzi, A Papachristodoulou, P Goulart, A Wynn, ; Y Zheng, NeurIPS20. 33University of OxfordPhD thesisChordal sparsity in control and optimization of large-scale systems, 2020] R. Zhang. On the tightness of semidefinite re- laxations for certifying robustness to adversarial examples. In NeurIPS20, volume 33, pages 3808-3820, 2020. [Zheng et al., 2020] Y. Zheng, G. Fantuzzi, A. Pa- pachristodoulou, P. Goulart, and A. Wynn. Chordal decomposition in operator-splitting methods for sparse semidefinite programs. Math. Program., 180(1):489-532, 2020. [Zheng, 2019] Y. Zheng. Chordal sparsity in control and op- timization of large-scale systems. PhD thesis, University of Oxford, 2019.\n", "annotations": {"author": "[{\"start\":\"98\",\"end\":\"187\"},{\"start\":\"188\",\"end\":\"286\"},{\"start\":\"287\",\"end\":\"382\"},{\"start\":\"383\",\"end\":\"469\"}]", "publisher": null, "author_last_name": "[{\"start\":\"102\",\"end\":\"108\"},{\"start\":\"199\",\"end\":\"207\"},{\"start\":\"295\",\"end\":\"303\"},{\"start\":\"388\",\"end\":\"393\"}]", "author_first_name": "[{\"start\":\"98\",\"end\":\"101\"},{\"start\":\"188\",\"end\":\"198\"},{\"start\":\"287\",\"end\":\"294\"},{\"start\":\"383\",\"end\":\"387\"}]", "author_affiliation": "[{\"start\":\"136\",\"end\":\"186\"},{\"start\":\"235\",\"end\":\"285\"},{\"start\":\"331\",\"end\":\"381\"},{\"start\":\"418\",\"end\":\"468\"}]", "title": "[{\"start\":\"1\",\"end\":\"95\"},{\"start\":\"470\",\"end\":\"564\"}]", "venue": null, "abstract": "[{\"start\":\"566\",\"end\":\"1335\"}]", "bib_ref": "[{\"start\":\"1436\",\"end\":\"1461\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2473\",\"end\":\"2494\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2832\",\"end\":\"2854\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2854\",\"end\":\"2881\"},{\"start\":\"2881\",\"end\":\"2900\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2900\",\"end\":\"2921\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2954\",\"end\":\"2973\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2973\",\"end\":\"2986\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3050\",\"end\":\"3069\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3069\",\"end\":\"3098\"},{\"start\":\"3321\",\"end\":\"3341\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3341\",\"end\":\"3347\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3347\",\"end\":\"3365\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3375\",\"end\":\"3399\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"3399\",\"end\":\"3422\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"3422\",\"end\":\"3444\"},{\"start\":\"3465\",\"end\":\"3486\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3650\",\"end\":\"3664\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3803\",\"end\":\"3824\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3824\",\"end\":\"3840\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4056\",\"end\":\"4077\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4136\",\"end\":\"4157\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"4166\",\"end\":\"4202\"},{\"start\":\"4423\",\"end\":\"4444\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4609\",\"end\":\"4633\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"4913\",\"end\":\"4927\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"4999\",\"end\":\"5025\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5025\",\"end\":\"5048\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5082\",\"end\":\"5096\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"5514\",\"end\":\"5547\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"5595\",\"end\":\"5615\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"5615\",\"end\":\"5638\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5875\",\"end\":\"5896\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"5901\",\"end\":\"5937\"},{\"start\":\"9025\",\"end\":\"9039\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"9698\",\"end\":\"9721\"},{\"start\":\"9721\",\"end\":\"9739\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"10195\",\"end\":\"10221\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"15194\",\"end\":\"15215\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"15815\",\"end\":\"15834\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"16060\",\"end\":\"16081\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"16081\",\"end\":\"16099\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"16099\",\"end\":\"16120\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"16376\",\"end\":\"16399\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"17828\",\"end\":\"17854\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"18099\",\"end\":\"18122\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"19769\",\"end\":\"19802\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"19802\",\"end\":\"19821\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"20747\",\"end\":\"20768\"},{\"start\":\"20768\",\"end\":\"20787\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"21102\",\"end\":\"21123\"},{\"start\":\"21123\",\"end\":\"21142\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"21334\",\"end\":\"21355\"},{\"start\":\"23094\",\"end\":\"23128\"},{\"start\":\"23520\",\"end\":\"23542\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"23654\",\"end\":\"23678\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"23867\",\"end\":\"23882\"},{\"start\":\"24322\",\"end\":\"24335\"},{\"start\":\"24835\",\"end\":\"24858\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"25220\",\"end\":\"25244\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"26112\",\"end\":\"26135\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"26135\",\"end\":\"26155\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"26372\",\"end\":\"26396\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"26426\",\"end\":\"26453\"},{\"start\":\"26950\",\"end\":\"26971\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"27299\",\"end\":\"27322\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"27758\",\"end\":\"27791\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"27791\",\"end\":\"27810\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"27993\",\"end\":\"28017\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"31551\",\"end\":\"31571\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"31571\",\"end\":\"31594\",\"attributes\":{\"ref_id\":\"b1\"}}]", "figure": "[{\"start\":\"28299\",\"end\":\"28625\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"28626\",\"end\":\"28681\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"28682\",\"end\":\"28749\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"28750\",\"end\":\"29095\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"29096\",\"end\":\"29330\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"29331\",\"end\":\"29557\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"29558\",\"end\":\"29841\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"29842\",\"end\":\"30113\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"30114\",\"end\":\"31461\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"31462\",\"end\":\"31675\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1351\",\"end\":\"2739\"},{\"start\":\"2741\",\"end\":\"3259\"},{\"start\":\"3261\",\"end\":\"4078\"},{\"start\":\"4080\",\"end\":\"5548\"},{\"start\":\"5550\",\"end\":\"6051\"},{\"start\":\"6053\",\"end\":\"6449\"},{\"start\":\"6467\",\"end\":\"6825\"},{\"start\":\"6956\",\"end\":\"7299\"},{\"start\":\"7335\",\"end\":\"7575\"},{\"start\":\"7627\",\"end\":\"7975\"},{\"start\":\"7977\",\"end\":\"8084\"},{\"start\":\"8191\",\"end\":\"8582\"},{\"start\":\"8752\",\"end\":\"9289\"},{\"start\":\"9417\",\"end\":\"9461\"},{\"start\":\"9501\",\"end\":\"10329\"},{\"start\":\"10428\",\"end\":\"10671\"},{\"start\":\"10785\",\"end\":\"10945\"},{\"start\":\"11020\",\"end\":\"11218\"},{\"start\":\"11480\",\"end\":\"11957\"},{\"start\":\"12014\",\"end\":\"12917\"},{\"start\":\"12963\",\"end\":\"13120\"},{\"start\":\"13161\",\"end\":\"13464\"},{\"start\":\"13466\",\"end\":\"13915\"},{\"start\":\"13917\",\"end\":\"14092\"},{\"start\":\"14151\",\"end\":\"14246\"},{\"start\":\"14248\",\"end\":\"14522\"},{\"start\":\"14524\",\"end\":\"14740\"},{\"start\":\"14742\",\"end\":\"15535\"},{\"start\":\"15537\",\"end\":\"15929\"},{\"start\":\"15983\",\"end\":\"16684\"},{\"start\":\"16806\",\"end\":\"18123\"},{\"start\":\"18155\",\"end\":\"18558\"},{\"start\":\"18633\",\"end\":\"18687\"},{\"start\":\"18816\",\"end\":\"18881\"},{\"start\":\"19016\",\"end\":\"19138\"},{\"start\":\"19210\",\"end\":\"19327\"},{\"start\":\"19478\",\"end\":\"19909\"},{\"start\":\"19911\",\"end\":\"20011\"},{\"start\":\"20013\",\"end\":\"20280\"},{\"start\":\"20308\",\"end\":\"20613\"},{\"start\":\"20615\",\"end\":\"21684\"},{\"start\":\"21742\",\"end\":\"21786\"},{\"start\":\"21932\",\"end\":\"21933\"},{\"start\":\"21935\",\"end\":\"22141\"},{\"start\":\"22169\",\"end\":\"22280\"},{\"start\":\"22320\",\"end\":\"24386\"},{\"start\":\"24388\",\"end\":\"25936\"},{\"start\":\"25961\",\"end\":\"26541\"},{\"start\":\"26543\",\"end\":\"27159\"},{\"start\":\"27175\",\"end\":\"28298\"}]", "formula": "[{\"start\":\"6826\",\"end\":\"6955\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"7300\",\"end\":\"7334\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"7576\",\"end\":\"7626\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"8085\",\"end\":\"8190\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"8583\",\"end\":\"8704\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"8704\",\"end\":\"8751\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"9290\",\"end\":\"9368\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"9368\",\"end\":\"9416\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"9462\",\"end\":\"9500\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"10330\",\"end\":\"10427\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"10672\",\"end\":\"10784\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"10946\",\"end\":\"11019\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"11219\",\"end\":\"11479\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"13121\",\"end\":\"13160\",\"attributes\":{\"id\":\"formula_13\"}},{\"start\":\"14093\",\"end\":\"14150\",\"attributes\":{\"id\":\"formula_14\"}},{\"start\":\"16685\",\"end\":\"16805\",\"attributes\":{\"id\":\"formula_16\"}},{\"start\":\"18559\",\"end\":\"18632\",\"attributes\":{\"id\":\"formula_17\"}},{\"start\":\"18688\",\"end\":\"18815\",\"attributes\":{\"id\":\"formula_18\"}},{\"start\":\"18882\",\"end\":\"19015\",\"attributes\":{\"id\":\"formula_19\"}},{\"start\":\"19139\",\"end\":\"19209\",\"attributes\":{\"id\":\"formula_20\"}},{\"start\":\"19328\",\"end\":\"19472\",\"attributes\":{\"id\":\"formula_21\"}},{\"start\":\"19472\",\"end\":\"19477\",\"attributes\":{\"id\":\"formula_22\"}},{\"start\":\"20281\",\"end\":\"20307\",\"attributes\":{\"id\":\"formula_23\"}},{\"start\":\"21685\",\"end\":\"21741\",\"attributes\":{\"id\":\"formula_24\"}},{\"start\":\"21787\",\"end\":\"21931\",\"attributes\":{\"id\":\"formula_25\"}}]", "table_ref": "[{\"start\":\"25976\",\"end\":\"25983\",\"attributes\":{\"ref_id\":\"tab_2\"}}]", "section_header": "[{\"start\":\"1337\",\"end\":\"1349\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"6452\",\"end\":\"6465\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"11960\",\"end\":\"12012\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"12920\",\"end\":\"12961\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"15932\",\"end\":\"15981\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"18126\",\"end\":\"18153\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"22144\",\"end\":\"22167\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"22283\",\"end\":\"22318\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"25939\",\"end\":\"25959\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"27162\",\"end\":\"27173\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"28627\",\"end\":\"28637\"},{\"start\":\"28683\",\"end\":\"28684\"},{\"start\":\"29332\",\"end\":\"29340\"},{\"start\":\"29559\",\"end\":\"29569\"},{\"start\":\"31463\",\"end\":\"31472\"}]", "table": "[{\"start\":\"30799\",\"end\":\"31461\"}]", "figure_caption": "[{\"start\":\"28301\",\"end\":\"28625\"},{\"start\":\"28639\",\"end\":\"28681\"},{\"start\":\"28685\",\"end\":\"28749\"},{\"start\":\"28752\",\"end\":\"29095\"},{\"start\":\"29098\",\"end\":\"29330\"},{\"start\":\"29342\",\"end\":\"29557\"},{\"start\":\"29571\",\"end\":\"29841\"},{\"start\":\"29844\",\"end\":\"30113\"},{\"start\":\"30116\",\"end\":\"30799\"},{\"start\":\"31474\",\"end\":\"31675\"}]", "figure_ref": "[{\"start\":\"13069\",\"end\":\"13077\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"13515\",\"end\":\"13523\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"25585\",\"end\":\"25593\",\"attributes\":{\"ref_id\":\"fig_1\"}}]", "bib_author_first_name": "[{\"start\":\"32193\",\"end\":\"32201\"},{\"start\":\"34178\",\"end\":\"34179\"},{\"start\":\"34180\",\"end\":\"34181\"},{\"start\":\"34190\",\"end\":\"34193\"},{\"start\":\"34202\",\"end\":\"34203\"},{\"start\":\"34213\",\"end\":\"34214\"},{\"start\":\"34215\",\"end\":\"34216\"},{\"start\":\"34225\",\"end\":\"34226\"},{\"start\":\"34237\",\"end\":\"34238\"},{\"start\":\"34244\",\"end\":\"34245\"},{\"start\":\"34254\",\"end\":\"34255\"},{\"start\":\"34265\",\"end\":\"34266\"},{\"start\":\"34290\",\"end\":\"34291\"},{\"start\":\"34300\",\"end\":\"34301\"},{\"start\":\"39612\",\"end\":\"39613\"},{\"start\":\"39632\",\"end\":\"39633\"},{\"start\":\"39644\",\"end\":\"39645\"},{\"start\":\"39665\",\"end\":\"39666\"},{\"start\":\"39676\",\"end\":\"39677\"},{\"start\":\"39684\",\"end\":\"39687\"}]", "bib_author_last_name": "[{\"start\":\"33078\",\"end\":\"33085\"},{\"start\":\"34182\",\"end\":\"34188\"},{\"start\":\"34194\",\"end\":\"34200\"},{\"start\":\"34204\",\"end\":\"34211\"},{\"start\":\"34217\",\"end\":\"34223\"},{\"start\":\"34227\",\"end\":\"34235\"},{\"start\":\"34239\",\"end\":\"34242\"},{\"start\":\"34246\",\"end\":\"34252\"},{\"start\":\"34256\",\"end\":\"34263\"},{\"start\":\"34267\",\"end\":\"34288\"},{\"start\":\"34292\",\"end\":\"34298\"},{\"start\":\"34302\",\"end\":\"34308\"},{\"start\":\"34310\",\"end\":\"34316\"},{\"start\":\"35457\",\"end\":\"35467\"},{\"start\":\"39614\",\"end\":\"39630\"},{\"start\":\"39634\",\"end\":\"39642\"},{\"start\":\"39646\",\"end\":\"39663\"},{\"start\":\"39667\",\"end\":\"39674\"},{\"start\":\"39678\",\"end\":\"39682\"},{\"start\":\"39688\",\"end\":\"39693\"}]", "bib_entry": "[{\"start\":\"32056\",\"end\":\"32962\",\"attributes\":{\"matched_paper_id\":\"1225955\",\"id\":\"b0\"}},{\"start\":\"32964\",\"end\":\"34049\",\"attributes\":{\"matched_paper_id\":\"225039902\",\"id\":\"b1\",\"doi\":\"arXiv:1803.06567\"}},{\"start\":\"34051\",\"end\":\"35342\",\"attributes\":{\"matched_paper_id\":\"67856043\",\"id\":\"b2\"}},{\"start\":\"35344\",\"end\":\"35955\",\"attributes\":{\"matched_paper_id\":\"208232329\",\"id\":\"b3\",\"doi\":\"arXiv:1412.6572\"}},{\"start\":\"35957\",\"end\":\"39514\",\"attributes\":{\"id\":\"b4\",\"doi\":\"arXiv:2009.04131\"}},{\"start\":\"39516\",\"end\":\"40320\",\"attributes\":{\"matched_paper_id\":\"219636171\",\"id\":\"b5\"}}]", "bib_title": "[{\"start\":\"32056\",\"end\":\"32191\"},{\"start\":\"32964\",\"end\":\"33076\"},{\"start\":\"34051\",\"end\":\"34176\"},{\"start\":\"35344\",\"end\":\"35455\"},{\"start\":\"35957\",\"end\":\"36109\"},{\"start\":\"39516\",\"end\":\"39610\"}]", "bib_author": "[{\"start\":\"32193\",\"end\":\"32204\"},{\"start\":\"33078\",\"end\":\"33087\"},{\"start\":\"34178\",\"end\":\"34190\"},{\"start\":\"34190\",\"end\":\"34202\"},{\"start\":\"34202\",\"end\":\"34213\"},{\"start\":\"34213\",\"end\":\"34225\"},{\"start\":\"34225\",\"end\":\"34237\"},{\"start\":\"34237\",\"end\":\"34244\"},{\"start\":\"34244\",\"end\":\"34254\"},{\"start\":\"34254\",\"end\":\"34265\"},{\"start\":\"34265\",\"end\":\"34290\"},{\"start\":\"34290\",\"end\":\"34300\"},{\"start\":\"34300\",\"end\":\"34310\"},{\"start\":\"34310\",\"end\":\"34318\"},{\"start\":\"35457\",\"end\":\"35469\"},{\"start\":\"39612\",\"end\":\"39632\"},{\"start\":\"39632\",\"end\":\"39644\"},{\"start\":\"39644\",\"end\":\"39665\"},{\"start\":\"39665\",\"end\":\"39676\"},{\"start\":\"39676\",\"end\":\"39684\"},{\"start\":\"39684\",\"end\":\"39695\"}]", "bib_venue": "[{\"start\":\"32204\",\"end\":\"32297\"},{\"start\":\"33103\",\"end\":\"33112\"},{\"start\":\"34318\",\"end\":\"34398\"},{\"start\":\"35484\",\"end\":\"35493\"},{\"start\":\"36127\",\"end\":\"36205\"},{\"start\":\"39695\",\"end\":\"39704\"}]"}}}, "year": 2023, "month": 12, "day": 17}