{"id": 225076003, "updated": "2023-10-06 10:10:39.609", "metadata": {"title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation", "authors": "[{\"first\":\"Andy\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Pete\",\"last\":\"Florence\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Tompson\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Welker\",\"middle\":[]},{\"first\":\"Jonathan\",\"last\":\"Chien\",\"middle\":[]},{\"first\":\"Maria\",\"last\":\"Attarian\",\"middle\":[]},{\"first\":\"Travis\",\"last\":\"Armstrong\",\"middle\":[]},{\"first\":\"Ivan\",\"last\":\"Krasin\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Duong\",\"middle\":[]},{\"first\":\"Ayzaan\",\"last\":\"Wahid\",\"middle\":[]},{\"first\":\"Vikas\",\"last\":\"Sindhwani\",\"middle\":[]},{\"first\":\"Johnny\",\"last\":\"Lee\",\"middle\":[]}]", "venue": "ArXiv", "journal": "726-747", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.14406", "mag": "3096099141", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/corl/ZengFTWCAAKDSL20", "doi": null}}, "content": {"source": {"pdf_hash": "eb6b9bc4ff3e4e2cf1724324d79ce7de43131478", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.14406v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4456173517a1372b9993b01064783fb0c5f1e6a4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/eb6b9bc4ff3e4e2cf1724324d79ce7de43131478.txt", "contents": "\nTransporter Networks: Rearranging the Visual World for Robotic Manipulation\n\n\nAndy Zeng \nRobotics at Google\n\n\nPete Florence \nRobotics at Google\n\n\nJonathan Tompson \nRobotics at Google\n\n\nStefan Welker \nRobotics at Google\n\n\nJonathan Chien \nRobotics at Google\n\n\nMaria Attarian \nRobotics at Google\n\n\nTravis Armstrong \nRobotics at Google\n\n\nIvan Krasin \nRobotics at Google\n\n\nDan Duong \nRobotics at Google\n\n\nAyzaan Wahid \nRobotics at Google\n\n\nVikas Sindhwani \nRobotics at Google\n\n\nJohnny Lee \nRobotics at Google\n\n\nTransporter Networks: Rearranging the Visual World for Robotic Manipulation\nDeep LearningVisionManipulation\nRobotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input -which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code will be made available at https://transporternets.github.io\n\nIntroduction\n\nEnd-to-end models that map directly from pixels to actions hold the capacity to learn complex manipulation skills, but are known to require copious amounts of data [1,2]. Integrating object-centric assumptions -e.g., object keypoints [3,4,5,6], embeddings [7,8], or dense descriptors [9,10,11] -has been shown to improve sample efficiency [10]. However, these representations often impose data collection burdens (i.e., configuring scenes with specific singulated objects) and still struggle to address challenging scenarios with unseen classes of objects, occluded objects, highly deformable objects, or piles of small objects [12]. This naturally leads us to ask: is there structure that we can incorporate into our end-to-end models to improve their learning efficiency, without imposing any of the limitations or burdens of explicit object representations?\n\nIn this work, we propose the Transporter Network, a simple end-to-end model architecture that preserves spatial structure for vision-based manipulation, without object-centric assumptions:\n\n\u2022 Manipulation involves rearranging things, which can be thought of as executing a sequence of spatial displacements: where the space being moved (i.e., transported) can encompass an object(s), part of an object, or end effector. We formulate vision for manipulation as estimating these displacements. Transporter Networks directly optimize for this by learning to 1) attend to a local region, and 2) predict its target spatial displacement via deep feature template matching -which then parameterizes robot actions for manipulation. This formulation enables high-level perceptual reasoning about which visual cues are important, and how they should be rearranged in a scene -the distributions of which can be learned from demonstrations. \u2022 Transporter Networks preserve the 3D spatial structure of the visual input. Prior end-to-end models [1,2] often use convolutional architectures with raw images, in which valuable spatial information can be lost to perspective distortions. Our method uses 3D reconstruction to project visual data onto a spatiallyconsistent representation as input, with which it is able to better exploit equivariance [13,14] for inductive biases that are present within the geometric symmetries [15] of the data for more efficient learning.\n\nIn our experiments, Transporter Networks exhibit superior sample efficiency on a number of tabletop manipulation tasks that involve changing the state of the robot's environment in a purposeful manner: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Transporter Networks excel in modeling 4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA. A Transporter Network is a simple model architecture that attends to a local region and predicts its spatial displacement (b) from visual input -which can parameterize robot actions. It is sample efficient in learning complex vision-based manipulation tasks: inserting blocks into fixtures (a), sequential pick-and-place in Towers of Hanoi (c), assembling kits with unseen objects (d), palletizing boxes (e), stacking a pyramid of blocks (f), manipulating rope (g), and pushing piles of small objects with closed-loop feedback (h) -and is practical to deploy on real production robots (k, m).\n\nmulti-modal spatial action distributions, and by construction generalize across rotations and translations of objects. They do not require any prior knowledge of the objects to be manipulated -they rely only on information contained within partial RGB-D data from demonstrations, and are capable of generalizing to new objects and configurations, and for some tasks, one-shot learning from a single demonstration.\n\nOur main contribution is a new perspective on the role of spatial structure and its capacity to improve end-toend learning for vision-based manipulation. We propose a simple model architecture that learns to attend to a local region and predict its spatial displacement, while retaining the spatial structure of the visual input. On 10 unique tabletop manipulation tasks, Transporter Networks trained from scratch are capable of achieving greater than 90% success on most tasks with objects in new configurations using 100 expert demonstrations, while other end-to-end alternatives struggle to generalize with the same amount of data. We also develop an extension to 6DoF tasks by combining 3DoF Transporter Networks with continuous regression to handle the remaining degrees of freedom. To facilitate further research in vision-based manipulation, we plan release code and open-source Ravens, our new simulated benchmark with all tasks. Ravens features a Gym-like API [16] with a built-in stochastic oracle to evaluate the sample efficiency of imitation learning methods.\n\n\nRelated Work\n\nObject-centric Representations. Considerable research in vision for manipulation has been devoted to object detectors and pose estimators [17,18,19,20,21]. These methods often require object-specific training data, making them difficult to scale to applications with unseen objects. Alternative representations including keypoints [3,4,5,6] and dense descriptors [9,10,11] have shown to be capable of category-level generalization and representing deformable objects, but still struggle to represent scenes with an unknown number of objects (e.g., piles of small objects) or occluded objects. End-to-end models for vision-based manipulation learn faster when integrated with such object-centric representations [10], providing sample-efficiency benefits but retaining the data-collection and representational constraints of object-centricness. In this work, we show that it is possible to achieve sample efficient end-to-end learning without object-centric representations -enabling the same model architecture to generalize to tasks with unseen objects, variable numbers of objects, deformable objects, and piles of small objects.\n\nPick-and-Place has had a long history in robotics, motivated by industrial needs. Classic systems use pose estimation of known objects [17,18], as well as scripted planning and motion control [22]. While robust in structured settings (e.g., manufacturing), these systems are difficult to deploy in unstructured settings (e.g., logistics, agriculture, household) -leading to renewed interest in leveraging learned models to achieve general pick-and-place policies [23,24,25,26,27,28,29,30] that can handle unseen objects. We benchmark against the more recent Form2Fit [24] on multiple tasks, and demonstrate that Transporter Networks are better able to handle tasks that require precise placing, multi-step sequencing, and closed-loop visual feedback.\n\n\nMethod\n\nConsider the problem of learning pick-and-place actions a t with a robot from visual observations o t : f(o t )\u2192a t =(T pick ,T place )\u2208A where T pick is the pose of the end effector used to pick an object, and T place to place the object. Both poses can be defined in SE (2) or SE(3), depending on the task and degrees of freedom available to the end effector. We can generalize beyond pick-and-place by considering manipulation with other motion primitives [22] that are also parameterized by two end effector poses. For example, we can parameterize pushing, where T pick is instead the pose of the end effector at the start of the push, and T place is where it moves to complete the push. In this work, we consider tasks that can be completed by a sequence of two-pose motion primitives. In the following section, we begin by describing our approach in the context of planar pick-and-place. We then discuss extensions to 6DoF pick-and-place, sequential multi-step tasks, and motion-primitive-based manipulation of deformable objects and piles of objects. Consider the planar pick-and-place problem on the right, where T pick ,T place \u2208 R 2 are 2D coordinates. In this task, the goal is to pick up the red block with an immobilizing grasp (e.g., with a suction gripper), then place it into the fixture. This simple setting reveals two fundamental aspects of the pick-and-place problem: (i) there may be a distribution of successful pick poses (from which T pick may be sampled e.g., on the surface of the block), and (ii) for each successful pick pose, there is a corresponding distribution of successful place poses (from which T place may be sampled).\n\n\nLearning to Transport\n\nThe goal of Transporter Networks is to recover these distributions from visual observations alone -without assumptions of objectness. While assigning canonical poses to the block and fixture might simplify the problem, there are many benefits to avoiding such object-centric representations -including the capacity to handle unseen objects, deformable objects, and uncountable piles of small objects. Hence we assume no prior information about the objects (e.g., no 3D models, poses, class categories, keypoints, etc.).\n\nTransporter Networks decompose the problem into (i) picking and (ii) pick-conditioned placing:\nf pick (o t )\u2192T pick f place (o t ,T pick )\u2192T place\nWe present a solution to both with Transporter Networks -but in this work, our primary contribution is on (ii), pick-conditioned placing via transporting. We first begin by briefly describing (i), picking.\n\nLearning Picking. Our visual observation o t is a projection of the scene (e.g., reconstructed from RGB-D images), defined on a regular grid of pixels {(u,v)} at timestep t of a sequential rearrangement task. Through camera-to-robot calibration, we can correspond each pixel in o t to a picking action at that location:\nT pick \u223c (u,v) \u2208 o t .\nThe distribution of successful picks over pixels in o t can be multi-modal in nature -particularly in cases when there are multiple instances of the same object in the scene, or when there are symmetries in the shapes of objects. As in prior works [31,32,33], Transporter Networks use fully convolutional networks (FCNs, commonly used in vision for image segmentation tasks [34]) to model an action-value function Q pick ((u,v)|o t ) that correlates with picking success (architecture in Sec. 3.2):\nT pick =argmax (u,v) Q pick ((u,v)|o t )\nFCNs are translationally equivariant by nature, which synergizes with spatial action representations: if an object to be picked in the scene is translated, then the picking pose also translates. Formally, equivariance here can be characterized as\nf pick (g\u2022o t )=g\u2022f pick (o t ),\nwhere g is a translation. Spatial equivariance has previously been shown to improve learning efficiency for vision-based picking [29,31,32,33,35]. To this end, FCNs excel in modeling complex multi-modal picking distributions anchored on visual features.\n\nSpatially Consistent Visual Representations. The benefits of spatial equivariance are most pronounced when observations o t are spatially consistent, meaning that the appearance of an object remains constant across different camera views, as well as across spatial transforms of rigid objects. This is in contrast to perspective images, in which the appearance of an object could scale or distort depending on the camera view and lens properties. In practice, spatial consistency also simplifies data augmentation during training, since applying rigid transforms to o t can yield data that appears as if objects in the scene were physically reconfigured. As with perspective images, however, spatially consistent representations still retain challenges from partial visibility and occlusion. As in [33], we convert RGB-D images into a spatially consistent form by unprojecting to a 3D point cloud, then rendering into an orthographic projection, in which each pixel (u,v) represents a fixed window of 3D space (Fig. 3a). Unlike in prior work, we use this spatially consistent representation for the transport operation, which we describe next. In this setting (a) where the task is to pick up the red block with an immobilizing grasp (e.g., suction) and place it into the fixture, the goal of Transporter Networks is to recover the distribution of successful picks (b), and distribution of successful placements (c) conditioned on a sampled pick.  [36,37,38] where o t [T pick ] is the exemplar and o t is the search area, or in retrieval systems literature where o t [T pick ] is the query and o t is the key. We formulate this as a template matching problem, using cross-correlation with dense feature embeddings \u03c8(\u00b7) and \u03c6(\u00b7) from two deep models:\nQ place (\u03c4|o t ,T pick )=\u03c8(o t [T pick ]) * \u03c6(o t )[\u03c4], T place =argmax {\u03c4i} Q place (\u03c4 i |o t ,T pick )(1)\nwhere Q place (\u03c4|o t ,T pick ) is an action-value function that correlates with placing success, and \u03c4 covers the space of all possible placing poses -which is equivalent to the space of 2D translations across o t in our first simple red block and fixture example. Since o t is spatially consistent, the structure of dense features \u03c8(o t ) and \u03c6(o t ) are also spatially consistent. A key property is that Eq. 1 is invariant to T pick as long as cross-correlation via transform T place produces the desired imagined overlay. This enables learning pick-conditioned placing from few examples (Fig. 5), without having to train over all possible picks [27]. Note that it is the deep features, not the RGB-D pixels, which are overlaid, and that the influence of the overlay extends beyond the crop, since for each dense pixel in feature space, the receptive field is large.\n\nLearning with Planar Rotations in SE(2). Our first example uses only pick-and-place actions in the space of 2D translations -however, T pick and T place may also represent rotations as in Fig. 3. In the planar SE(2) case, we discretize the space of SO(2) rotations into k bins, then rotate the input visual observation o t for each bin. o t is thereby defined on a grid of voxels {(u,v,w) i } where w lies on the k-discretized axis of rotations, and T pick and T place are represented discretely on this grid in SE (2). In practice, this is achieved by running the FCN k times in parallel, once for each rotated o t , sharing model weights across all rotations. Learning with Full Rigid Body Transformations in SE (3). While the previous sections discussed learning a Q function over a discretized space, extending to the full SE(3) case to handle all six degrees of freedom of rigid objects becomes infeasible to sufficiently discretize the 6-dimensional space -due to the curse of dimensionality. However, interestingly we find the transport operation to be an enabling building block into higher dimensions, using a multi-stage approach to extend into tasks which require SE(3) placing. First, the three SE(2) degrees-of-freedom are addressed as in the previous section, producing an estimatedT SE(2) place . Then, we extend the cross-correlation transport operation -rather than using one channel over SE (2) to classify the correct discretized action, we use three channels over SE (2) to regress the remaining rotational (r x ,r y ) and translational (z-height) degrees of freedom. We use a \u03c8 and \u03c6 which are identical to the \u03c8 and \u03c6 used for SE (2) classification, but with three separate cross-correlations ( * 3 ) from subsets of channels, and also add learnable nonlinearity after the cross-correlation via a three-headed MLP network, f(\u00b7): r x , r y ,\nz = f \u03c8 (o t [T pick ]) * 3 \u03c6 (o t )[T SE(2) place ]\n. This hybrid discrete/continuous approach affords a number of nice properties: the discrete SE(2) step is good at representing complex multi-modal distributions in those degrees of freedom, and once an approximate SE(2) alignment has been overlaid via the transport operation, this acts as an attention mechanism to aid the model in precise continuous regression. To handle multi-modality in the continuous degrees of freedom, we can regress mixture densities. In practice we have found that multiple approaches can work well once approximate SE(2) alignment has been applied, see the Appendix for more. Multi-stage SE(3) classification, using Eq. 1 iteratively, is also possible.\n\nLearning Sequential Pick-and-Place. Consider the challenging example with 3-disk Towers of Hanoi in Fig. 1c, where the task is to sequentially move 3 disks from the first tower to the third without placing a larger disk over a smaller disk. Completing this task requires correctly sequencing 7 pick-and-place actions. In this setup with a unique coloring of the base and disks, the task is Markovian. Hence actions can be directly conditioned on visual contextual cues, e.g., the next pick-and-place depends on the position (or absence due to occlusion) of observed objects in the scene. Our method is stateless, but experiments show that it can learn such sequencing behaviors through visual feedback. To enable this, it is important to increase the receptive field [39] of the FCNs to encompass most of the visual observation o t . Extending Transporter Networks with memory to handle non-Markovian tasks would be interesting future work.\n\nManipulation Beyond Pick-and-Place: Deformables and Piles of Objects. Beyond pick-and-place of rigid objects, Transporter Networks can generalize to tasks that can be completed with two-pose primitives. For example, they can learn to sequentially rearrange a deformable rope such that it connects the two end points of an incomplete 3-sided square (Fig. 1e), or sequentially push piles of small objects into a desired target set (Fig. 1h) using a spatula-shaped end effector. In both cases, our experiments (Sec. 4) demonstrate that they can learn closed-loop behaviors using visual feedback to complete these tasks. Both tasks are characterized by dynamics that cannot be modeled with rigid displacements -however, our results suggest that rigid displacements can serve as a useful prior for non-rigid ones [40].\n\n\nNetwork Architecture\n\nObservation Space: For tabletop manipulation, our visual observation o t is an orthographic top-down view of a 0.5\u00d71m tabletop workspace, generated in simulation by fusing 480\u00d7640 RGB-D images captured with calibrated cameras using known intrinsics and extrinsics. The top-down image o t has a pixel resolution of 160\u00d7320 -each pixel represents a 3.125\u00d73.125mm vertical column of 3D space in the workspace. The image o t contains both color (RGB) and scalar height-from-bottom (H) information, which enables the deep models to learn features that are rich in both visual texture and geometric shape.\n\nPicking: Our picking model is a single feed-forward FCN that takes as input the visual observation o t \u2208R H\u00d7W \u00d74 and outputs dense pixel-wise values that correlate with picking success: V pick \u2208R H\u00d7W = softmax(Q pick ((u,v)|o t )). Our picking model is an hourglass encoder decoder architecture: a 43-layer residual network (ResNet) [41] with 12 residual blocks and 8-stride (3 2-stride convolutions in the encoder and 3 bilinear-upsampling layers in the decoder), followed by image-wide softmax. Each convolutional layer after the first is equipped with dilation [42], and interleaved with ReLU activations [43] before the last layer. 8-stride was chosen to balance between maximizing receptive field coverage per pixel prediction, while minimizing loss of resolution in the latent mid-level features of the network.\n\nPlacing: Our placing model is a two-stream feed-forward FCN that takes as input the visual observation o t \u2208 R H\u00d7W \u00d74 and outputs two dense feature maps: query features \u03c8(o t ) \u2208 R H\u00d7W \u00d7d and key features \u03c6(o t ) \u2208 R H\u00d7W \u00d7d , where d is the feature dimensionality. Our placing model shares a similar hourglass encoder decoder architecture as the picking model: each stream is an 8-stride 43-layer ResNet, but without non-linear activations in the last layer. A partial crop \u03c8(o t [T pick ])\u2208R c\u00d7c\u00d7d of query features with size c centered around T pick =argmaxV pick is transformed by \u03c4 (which covers the space of possible placement configurations), then cross-correlated (Eq. 1) with the key feature map \u03c6(o t ) to output dense pixel-wise values that correlate with placing success: V place \u2208 R H\u00d7W \u00d7k = softmax(Q place (\u03c4|o t ,T pick )) where \u03c4 is discretized into k =36 angles (multiples of 10 \u2022 ). The translation and rotation \u03c4 which gives the highest correlation with the partial crop, via \u03c8(o t [T pick ]) * \u03c6(o t )[\u03c4], gives T place = argmaxV place . Note that this operation can be made fast if implemented with highly optimized matrix multiplication (where the crop is flattened) or as a convolution (where the crop is the convolving kernel). Total inference time (with both picking and placing models) amounts to 200ms on an Nvidia GTX 2080 GPU.\n\n\nTraining: Learning from Demonstrations\n\nWe assume access to a dataset D = {\u03b6 1 ,\u03b6 2 ,...,\u03b6 n } of n expert demonstrations, where each trajectory a 1 ), ...} is a sequence of one or more observation-action pairs (o t , a t ). In our experiments (Sec. 4), we evaluate Transporter Networks in their ability to learn from n = 1, 10, 100, or 1,000 demonstrations per task. Our method does not take any task-specific information as input, and is trained on one task per model (future work may investigate multi-task). During training, we uniformly sample observation-action pairs from the dataset D, from which each action a t can be unpacked into two training labels: T pick and T place , used to generate binary one-hot pixel maps Y pick \u2208 R H\u00d7W and Y place \u2208R H\u00d7W \u00d7k respectively. Our training loss is simply the cross-entropy between these one-hot pixel maps and the outputs of the picking and placing models:\n\u03b6 i = {(o 0 , a 0 ), (o 1 ,L=\u2212E Ypick [logV pick ]\u2212E Yplace [logV place ].\nWhile we only have labels for a single pixel per dense probability map, gradients are passed to all other pixels via image-wide softmax. We find this loss to be capable of learning multi-modal non-isotropic spatial action distributions. For SE(3) models involving regression, we use a Huber loss on each regression channel. Validation performance generally converges after a few hours of training on a single commodity GPU.\n\n\nResults\n\nWe execute experiments in both simulated and real settings to evaluate Transporter Networks across various tasks. Our goals are threefold: 1) investigate whether preserving visuo-spatial structure within Transporter Networks improves their sample efficiency and generalization, 2) compare them to common baselines for end-to-end vision-based manipulation, and 3) demonstrate that they are practical to deploy on real robots.\n\n\nSimulation Experiments\n\nprecise multimodal multi-step unseen Task placing placing sequencing objects block-insertion ( Fig. 3) place-red-in-green towers-of-hanoi ( Fig. 7) align-box-corner stack-block-pyramid * palletizing-boxes * \u00a7 assembling-kits * \u00a7 [24] packing-boxes * \u00a7 manipulating-rope * \u2020 sweeping-piles * \u2020 [12] * tasks that have more than one correct sequence of states. \u2020 tasks that are beyond pick-and-place of rigid objects. \u00a7 tasks that are commonly found in industry. We use behavior cloning simulation experiments to compare with baselines. In this work, we do not use simulation for sim-to-real transfer -rather only as a means to provide a consistent and controlled environment for fair comparisons and ablations. Ravens, our simulated benchmark learning environment built with PyBullet [44], consists of a Universal Robot UR5e with a suction gripper overlooking a 0.5\u00d71m tabletop workspace, with 3 simulated 640x480 RGB-D cameras pointing towards the workspace: one stationed in front (facing the robot), one on the left shoulder of the robot arm, and one on the right shoulder. Our method does not require 3 cameras -having more cameras only improves visual coverage. For example, our real experiments only use 1 camera per robot.\n\nTasks and Metrics. In simulation, we benchmark on 10 discrete-time tabletop manipulation tasks, some which require closed-loop visual feedback for multi-step sequencing. We position-control the robot end effector (suction cup, or spatula for pushing) with two-pose motion primitives [22]. For each task, all objects (including target zones) are randomly positioned and oriented in the workspace during training and testing. Each task comes with a scripted oracle that provides expert demonstrations by randomly sampling the distribution of successful T pick and T place -samples of these are made available for the learner. Some tasks include multi-modality and permutations -for example, in stacking, a successful 6-block pyramid is invariant to the permutation of blocks within each row, but not between rows. Performance is evaluated with a metric from 0 (failure) to 100 (success). Partial credit is assigned during tasks that require sequencing multiple actions. We report results with the models that have achieved highest validation performance during training, averaged over 100 unseen test runs for each task. More details in the Appendix.\n\nBaseline Methods. Form2Fit [24], as mentioned in Related Work, is different from our method in that it does not perform visuo-spatial transporting, but instead uses a matching network. ConvMLP is a common model architecture in the end-to-end literature [1] that uses a series of convolutional layers, followed by spatial-softmax and a multi-layer perceptron (MLP) -here, we use this to regress the sequence of two SE(2) poses. Form2Fit and ConvMLP use the same input images as Transporter Networks, but to get a sense of the difficulty of our tasks, we also supply two baselines which consume ground truth state (object poses and 3D bounding boxes) as input -i.e., assuming perfect object poses. GT-State MLP is an MLP which consumes these and regresses two SE(2) poses, while 2-step GT-State MLP first regresses the first SE(2) pose, then adds this to the observation vector and regresses the second SE(2) pose . To represent multi-modality, ConvMLP, GT-State MLP, and 2-step GT-State MLP regress mixture densities [45]. All methods, including ours, use identical data augmentation, with random SE(2) world-frame transforms.\n\n\nResults: Sample Efficiency and Generalization on Benchmark Tasks\n\nTab. 2 shows sample efficiency of baselines trained from stochastic demonstrations for each task, and evaluated on unseen test settings, with random rotations and translations of objects (including target zones). The benchmark is difficult -most baselines, while capable of over-fitting to the demonstration training set, generalize poorly with only 1,000 demonstrations. In general, Transporter Networks achieve orders of magnitude more sample efficiency than the image-based alternatives, and also provides better sample efficiency than multi-layer perceptrons trained with ground truth state. More analysis in the Appendix.\n\nblock-insertion place-red-in-green towers-of-hanoi align-box-corner stack-block-pyramid    Generalizing to Unseen Objects. Three of our benchmark tasks involve generalizing to unseen objects (see Tab. 1). In our variant of the kit assembly task with unseen objects (shown in Fig. 8), the gap in performance between Form2Fit and Transporter Networks is rather large. On a simpler set of unseen objects that have more primitive geometries (e.g., circular disks, squares), which reflect the distribution of objects used in the original paper [24], Form2Fit achieves 96.3% task success. This suggests that Form2Fit descriptors have the capacity to express larger differences in geometry, but are less capable of matching higher resolution information than our deep template-matching based model, with the same amount of data.\n\nLearning Sequential Manipulation with Closed-Loop Visual Feedback. In this work, Transporter Networks are stateless models that react only to information presented as visual input during the current timestep. However, our experiments show that the models have the capacity to learn visual feedback: they make use of contextual visual cues to determine which step of the task they are in, and use that to condition action-value predictions. For example, when a stack of blocks falls over, they can re-build the stack of blocks as if they had just started the task. If they accidentally push granular media out of the target zone during sweeping, they then push it back in (Fig. 5). We hypothesize that equivariance to rotations and translations enable them to learn these recovery behaviors even with little data on multi-step tasks (see Tab Table 3. Simplified 2DoF (no rotation) block-insertion is harder to learn with demonstrations from a stochastic oracle than a deterministic one.\n\nSample Complexity in Simplified Environments. Consider a simplified translation-only block-insertion task illustrated in Fig. 3, where no rotations are needed, and the block is initialized to the same location, with only the fixture location varying between environments. We investigate two variants of experts in this setting: (a) one that provides deterministic demonstrations where T pick , T place are fixed relative to the block and fixture respectively, and (b) one that provides stochastic demonstrations where T pick , T place are uniformly random sampled from the distribution Figure 5. Transporter Networks can predict the desired spatial displacements of piles of small objects (left), which informs how to push them towards a target set. Our experiments show that it can learn visual feedback -enabling it to recover from mistakes (e.g., top row: pushing objects out of the target set at t = 11), or sequentially rearrange a deformable rope (middle row). Since it learns pick-conditioned placing, it can also stack plates with varying initial predicted pick locations from only 10 demonstrations on a real robot (bottom row), without needing any object-specific representations. of successful actions. Intuitively, the stochastic demonstrator reflects a more realistic setting in which the expert is inconsistent. Tab. 3 displays sample complexity results on these tasks. Most baselines perform well with deterministic demonstrations, but begin to struggle with stochastic ones. Transporter Networks, by construction, work well in both cases. We show additional extrapolation experiments in the Appendix.  Learning 6DoF Pick-and-Place. We also test the SE(3) formulation discussed in Sec. 3.1 in the 6DoF environment shown in Fig. 4 with unseen target fixture configurations and stochastic demonstrations. On this more challenging task, our model demonstrates considerably better sample efficiency than the image-based baselines achieved on the strictly-easier 3DoF-only block-insertion task (Tab. 2 and 4). Additionally, while the hybrid discrete and continuous SE(3) model is not able to 1-shot generalize as the discrete-only SE(2) model could, we still observe substantially better performance than GT-State MLP models (Tab. 4) which struggle in the larger observation and action spaces. For further analysis of this task and models, see the Appendix. We validate Transporter Networks on both the kit assembly and the sweeping tasks (shown in Fig. 1k and 1m) using real UR5e robots. For kit assembly, the robot uses a suction gripper and an industrial Photoneo PhoXi camera (for high resolution and accurate IR-depth). For sweeping, the robot uses a brush end effector and a low-cost consumer-grade Azure Kinect (for RGB-depth). Despite COVID-19 lockdowns preventing physical access, we were still able to perform real experiments using our Unitybased [46] UI that enables people to remotely teleoperate our robots to calibrate, collect training data, and run tests. For these experiments, teleoperators are tasked with i) repeatedly assembling and disassembling a kit of 5 bottled mouthwashes, or ii) sweeping piles of small Go pieces into a target goal zone marked with green tape, and autonomous resets using a scripted scrambling motion. Table 5 shows test performance of the models, and the amount of training data collected from 13 human operators in terms of transitions (i.e., pick and place actions for kit assembly, pushes for sweeping). We also show qualitative results of our robots in action for both tasks in our supplementary video. Further quantitative experiments were infeasible at time of submission due to COVID-19. Additional experiment details in the Appendix.\n\n\nReal-World Experiments\n\n\nConclusion\n\nIn this work, we presented the Transporter Network, a simple model architecture that infers spatial displacements, which can parameterize robot actions from visual input. It makes no assumptions of objectness, exploits spatial symmetries, and is orders of magnitude more sample efficient in learning visionbased manipulation tasks than end-to-end alternatives: from stacking a pyramid of blocks, to assembling kits with unseen objects, to pushing piles of small objects with closed-loop feedback. In terms of its current limitations: it is sensitive to camera-robot calibration, and it remains unclear how to integrate torque/force actions with spatial action spaces. Overall, we are excited about this direction and plan to extend it to real-time high-rate control, and as well as tasks involving tool use.\n\n\nAppendix\n\nThe appendix consists of: descriptions of tasks and evaluation metrics in the Ravens simulation framework, additional experimental results, analysis, ablation studies, and real system details.\n\n\nSimulation Experimentation, Real World Experimentation, and COVID-19\n\nAs introduced in the main paper, we execute experiments in simulation with Ravens as a means to provide a consistent environment for comparisons to baselines and ablations. Models evaluated in simulation are trained only on simulated data. Likewise models evaluated in the real-world are trained only on real data. In simulation, we avoid assumptions that cannot transfer to a real setup: observation data contains only 640x480 RGB-D images and camera parameters; actions are end effector poses (transposed into joint positions with inverse kinematics). The only exception to this is our GT-State baseline, which learns MLPs that take ground truth state information (e.g., object poses, bounding boxes, and color) as input -shown to be less sample efficient than Transporter Networks for many tasks. A limitation of Ravens is that the rendering software stack may not fully reflect the noise characteristics often found in real data: inaccuracies in camera calibration (e.g., intrinsics and extrinsics), noise in commodity depth sensors. Hence, we also test our method on a real system (details in Sec. 6.14) using teleoperated demonstration data, and present qualitative results in our supplementary video. Extensive quantitative experiments on real robots was difficult due to limited physical access during COVID-19.\n\n\nRavens Simulation Framework\n\nIn our simulation framework, Ravens, each task comes with a scripted oracle that provides expert demonstrations by randomly sampling the distribution of successful picking and placing actions -samples of which are made available to the learner. Ravens also has several attributes that make it useful for studying different areas that are beyond the scope of this work, including: 1) active learning, since algorithms can query an oracle during specific moments of uncertainty to improve learning efficiency, 2) reinforcement learning, since we provide reward functions that provide partial credit for all tasks (used only for evaluation in this work), and 3) active perception, since camera parameters (e.g., extrinsics) are defined by an action (static in our experiments, but could be dynamic), which provides an opportunity to study algorithms that improve learning through active control of the camera position and orientation.\n\nDuring both training and testing, all objects (including target zones) are randomly positioned and oriented in the workspace. To succeed in the case where only a single demonstration is provided, the learner needs to be invariant to unseen configurations of objects. Information about the multi-modality of a task or its sequential permutations is only made available from the distribution of demonstrations. For example, moving disks in Towers of Hanoi may have only one correct sequence of states, but the distribution of how each disk can be picked or placed is learned from multiple demonstrations. Or, when palletizing homogeneous boxes, the arrangement of boxes across each layer on the pallet must be transposed, and boxes should be stacked stably on top other boxes already on the stack to avoid toppling. In our experiments, Transporter Networks are trained from scratch, without any pre-training from vision datasets. Exploring multi-task training for more efficient generalization to new tasks is interesting future work. Figure 6. Tasks: (row-major order) block-insertion, place-red-in-green, towers-of-hanoi, align-box-corner, stack-block-pyramid, palletizing-boxes, assembling-kits, packing-boxes, manipulating-rope, sweeping-piles. Goal states (not provided to learners) are shown in top left corner of each image.\n\n\nRavens-10 Benchmark Task Details\n\nA set of 10 tasks (illustrated in Fig. 6) comprise our Ravens-10 benchmark. In each of these tasks, the agent acts with motion primitives parameterized by a sequence of two SE(2) end effector poses. Note that although the two motion-primitive-defining poses are in SE (2), manipulation behaviors that are in 3D can still be achieved (such as stacking blocks on top of each other) due to motion primitives that involve out-of-plane movement.\n\nblock-insertion: picking up an L-shaped red block and placing it into an L-shaped fixture with a corresponding hole. The block is 8cm in diameter while the hole is 9cm in diameter. The block and fixture configurations are randomly sampled in the workspace -rejected if in collision with each other, or if partially out of bounds in the workspace.\n\nplace-red-in-green: picking up red blocks and placing them into green bowls amidst other visually distracting objects (e.g., randomly positioned blocks and bowls of different colors). There can be multiple red blocks and/or multiple green bowls, with a random chance for more bowls than blocks. This task requires handling multiple candidate picking and placing locations. Since bowls are twice as large as the blocks, this task does not require precision. Red blocks and green bowls are first added to the workspace, then 10 objects (blocks or bowls of other colors) are randomly spawned in collision-free configurations.\n\ntowers-of-hanoi: sequentially picking up disks and placing them into pegs such that all 3 disks initialized on one peg are moved to another, and that only smaller disks can be on top of larger ones. Completing this task requires sequencing 7 pick-and-places of the disks in a specific order. During evaluation, no information about which disk should be picked next is provided -the approach needs to infer this from contextual visual cues e.g., the positions or absence (due to occlusions) of other disks. This tests the capacity of the learner to infer the sequential semantics of Markovian vision-based manipulation tasks. Fig.  7 illustrates this task, its ground truth sequence, picking action distribution, and placing action distributions (unimodal for translation-only placing, or spiral shaped for the SE(2) placing). The base of the 3 pegs are randomly configured in the workspace, and disks are initialized on the peg on the lightest side of the base. Figure 7. In Towers of Hanoi (a), the task is to sequentially pick-and-place 3 disks from the first peg to the third peg without placing a larger disk over a smaller disk (c). In each step, the distribution of successful picks is ring-shaped (b), and the distribution of successful pick-conditioned placements (d) is unimodal with translation-only placing, or spiral-shaped with SE(2) placing (projected onto a ring for visualization, where the ring traverses different rotations). The nature of this task evaluates the capacity of Transporter Networks to recover non-trivial spatial action distributions, and to sequence actions by leveraging contextual visual cues.\n\nalign-box-corner: picking up a randomly sized box and repositioning it on the tabletop to align one of its corners to a green L-shaped marker labeled on the tabletop. This task requires precision, and the ability to generalize to new box sizes. Aligning any of the box's corners to the green marker yields task success. The marker and box are randomly configured in the workspace such that the box does not overlap the marker.\n\nstack-block-pyramid: sequentially picking up 6 blocks (each with a different color) and stacking them into a pyramid of 3-2-1. The bottom 3 blocks can only be purple, green, or blue blocks. The middle 2 blocks can only be orange or yellow blocks. The top block can only be red. A successful pyramid is invariant to the permutation of blocks within each row, but not between rows. The blocks and base of the pyramid are randomly spawned in collision-free configurations.\n\npalletizing-boxes: picking up fixed-sized boxes and stacking them on top of a pallet. To mimic real stability constraints, the arrangement of boxes should be transposed across each layer of the pallet. There are a fixed number of boxes -each one is spawned into the workspace prior to each pick-and-place action. assembling-kits: picking and placing different shaped objects onto corresponding locations on a board (visually marked with corresponding silhouettes of the objects). For each episode, 5 objects are randomly selected with replacement from a set of 20: 14 used during training (left), and 6 held out for testing (right). This task requires precise placing and generalizing to new objects by learning the concept of \"how things fit together\" [24]. The kit and 5 objects are randomly spawned in collision-free configurations.\n\npacking-boxes: picking and placing randomly sized boxes tightly into a randomly sized container. This task is the hardest one in the benchmark, which requires not only relying on visual cues to tightly fit objects together, but also implicitly learning to order the objects in ways that maximize container coverage. During initialization, the container size is first randomly sampled, then randomly partitioned using a binary search tree (with minimum size of 5cm along each dimension). Each partition then parameterizes the size of each box, individually spawned into the workspace with a collision-free configuration.\n\nmanipulating-rope: manipulating a deformable rope such that it connects the two endpoints of an incomplete 3-sided square (colored in green). The cable is initialized as a randomly formed pile in the workspace, which needs to be untangled then aligned with the square. This task requires sequencing actions with closed-loop visual feedback to incrementally adjust the rope. The rope is implemented as a set of 20 articulated spherical beads, randomly dropped onto the workspace. The 3-sided square is randomly configured in the workspace.\n\nsweeping-piles: pushing piles of small objects (randomly initialized) into a desired target goal zone on the tabletop marked with green boundaries. This sequential task (inspired by [12]) requires sequencing pushes with closed-loop visual feedback and handling piles of small objects. In this task, the suction end effector is swapped with a spatula. The small objects and the target goal zone are randomly initialized in the workspace.\n\n\nAdditional Tasks\n\nIn addition to the Ravens-10 tasks, we used two other tasks in the paper for analysis. The first was to provide a simplified test environment for testing the difference between stochastic and deterministic demonstrations (Sec. 4), and interpolation/extrapolation experiments with a small enough amount of degrees of freedom such that they could be plotted (Sec. 6.10). The second task was to test the SE(3) variant of Transporter Networks.\n\n2DoF simplified block-insertion: This task is identical to the block-insertion task, except: (i) the block is always in the same staring configuration, and (ii) the fixture varies only in translation. The environment initialization has 2 degrees of freedom total.\n\n6DoF block insertion: This task is identical to the block-insertion task, except that the fixture varies in 6 degrees of freedom, requiring SE (3) placing. In addition to the table-plane degrees of freedom, the fixture also varies in height (z) as well as the two rotational degrees of freedom out of the plane, which may be represented by roll and pitch. The block varies only in SE(2). The environment initialization has 9 degrees of freedom total.\n\n\nEvaluation Metrics\n\nPerformance on each task in simulation is evaluated in one of two ways:\n\nPose: object translation and rotation to target pose is less than a threshold \u03c4 =1cm and \u03c9 =15 \u2022 respectively. If completing a task requires sequencing multiple actions, then each successful action (where an object is moved to its correct pose) returns a partial reward r = 1 # of actions . Total rewards for completing a task always sums to 1. Object symmetries are accounted for. Tasks: block-insertion, towers-of-hanoi, place-red-in-green, align-box-corner, stack-block-pyramid, assembling-kits.\n\nZone: ratio of object(s) in the target zone. We discretize each object's 3D bounding box into 2cm 3 voxels. Total reward is the fraction of total voxels in the target zone r = # of voxels in target zone total # of voxels . Tasks: palletizing-boxes, packing-boxes, manipulating-cables, sweeping-piles. Achieving total reward r =1 on palletizing-boxes and packing-boxes requires tightly fitting all objects into the target zone (i.e., it is exponentially more difficult to achieve r =0.9 than it is to achieve the first r =0.5).\n\n\nData Augmentation\n\nAs is common with deep learning methods, we find that data augmentation can substantially benefit generalization performance for Transporter Nets. To control the experimental comparisons with baselines in this paper, all methods used an identical data augmentation distribution. Specifically, a random SE(2) transform, T aug W , relative to the world frame W was sampled, and all observations (whether the images, or the object poses for the GT-State methods) were adjusted as if the data was observed from the augmentation frame aug rather than the world frame. This augmentation makes the assumption that the absolute position of objects on the table plane does not matter for the tasks -this was indeed the case for all tasks shown. Note that this distribution provides augmentation along the 3 degrees of freedom of SE(2), but we do not apply other augmentations such as imagining different relative configurations of objects -this would not be a feasible assumption for certain tasks that involve multiple objects.\n\n\nAdditional Baseline Details\n\nForm2Fit [24] is a recent method for pick-and-place (also similar to Devin et al. [28]), using a matching network that computes dense visual descriptors [9,47] to associate picking actions to placing actions. We use an author-provided implementation of Form2Fit with 43-layer ResNet backbones of similar parameter count for a fair comparison.\n\nConvMLP is a model architecture first proposed in [1] for learning control policies from pixel inputcommonly used in end-to-end literature. While typically this architecture is used for real-time continuous control and was originally used with a joint torque action space, we tried it here for our motion-primitivebased setting with a spatial action space. The input images are the same as that of Transporter Networks and Form2Fit: top-down reconstructed views from RGB-D data. The architecture consists of a series of three convolutional layers, followed by spatial soft(arg)max [48] and a multi-layer perceptron to regress a 6-dimensional vector of the two SE(2) end-effector poses. The architecture used was almost exactly as in [1], but with the following changes: (i) we used channel depths of 64, 64, 64, rather than the 64, 32, 16 used in the original paper, (ii) we used MLP layer widths of 128, 128, (iii) we used the MLP to regress mixture densities [45] rather than direct regression in order to handle multi-modality, and (iv) we added depth image input to the network, by replicating depth along 3 channels, processing it with one separate 7x7 convolutional layer, and then fusing with RGB after the first layer via concatenation. The addition of depth image input into the model was to be more directly comparable with the other image-based options, which also used depth. As in [1], we use ImageNet-pretrained weights from GoogLeNet [49] for the first RGB convolutional layer, which we find to aid in stable training and generalization. While this network is 10x shallower (3 layers) than the network used for Form2Fit and Transporter Nets (43 layers), we tried to train the same 43-layer backbone network used in Transporter Networks with a spatial soft(arg)max and MLP, but found that the shallower 3-layer network achieved better generalization at least for our tested hyperparameters. Since many tasks involved multi-modal policies, it was necessary to train the network in a way that handles this multi-modality. Mixture Density Networks (MDN) [45] have previously been used for end-to-end manipulation control, for example in [50]. As in [45], we trained these with a negative log likelihood loss. We used a mixture of 26 Gaussians, with a training temperature of 2.5. To match translation and rotational units on a common scale, we equalized 1 m translation with 0.1 radians. We also found improved generalization by taking the features z extracted by the spatial soft(arg)max, and concatenating z\u2190[z, sin(z), cos(z)] as input to the MLP -this was inspired by [51,52]. We experimented with dropout in the MLP to add regularization, but found this to decrease generalization; the noise from the convolutional feature extraction was enough to regularize the MLP layers. The models were trained with the Adam optimizer for 20,000 steps, batch size 4, with a learning rate of 2e \u22124 .\n\nGT-State MLP is a model architecture for learning control policies directly from ground truth state (i.e., object poses and 3D bounding boxes) as input, similar to the baseline used in [10]. In the real world, this would be analogous to assuming perfect object pose estimation was provided by an external vision system. The model shares an identical MLP to the ConvMLP model described above, except the input z to the model was all degrees of freedom of the environment (object poses, bounding boxes, and in the case of the place-red-in-green task we also added the RGB color of each object as input). The bounding boxes are used for the 3 tasks (palletizing-boxes, align-box-corner, and packing-boxes) that involve boxes that vary over their length, width, and height, so the bounding boxes exactly parameterize these shapes. Some environments have a variable number of objects, and to handle this we padded the input vector with zeros to match the largest cardinality seen in the environment sampling. We used dropout in the MLP layers with a dropout rate of p = 0.1, which we found to improve generalization. The models were trained with the Adam optimizer for 40,000 steps, batch size 128, with a learning rate of 2e \u22124 .\n\nGT-State MLP 2-step is composed of two MLP networks, each of which are almost identical to GT-State MLP. The first regresses only the first SE(2) pose, and the result of this is added to the observation vector of the second network, which regresses the second SE(2) pose. We used this model to experiment with whether the benefit of Transporter Networks was due to their sequential model cascade, where the second model is conditioned on the first (i.e., pick-conditioned placing). We experimented with training the second MLP model on noisy regressions from the first model -but perhaps because our models were sufficiently regularized with p=0.1 dropout, we found this to not provide measurable advantages over simpler teacher forcing, in which the second model was only trained on the conditioning from the training data. (3) is almost identical to the 2-step model, except a third model is conditioned on the outputs of both the first and second model, and the additional out-of-plane degrees of freedom (height, roll, pitch) are regressed. We experimented with also using just one GT-State MLP for the SE(3) action space task, but found similar performance. Additionally, this model took in as input not only the SE(2) degrees of freedoms of objects, but their full SE(3) poses, represented with roll, pitch, yaw for the rotational degrees of freedom. We experimented with other forms of rotational observation, such as observing three 3D points on the object, which does not have 2\u03c0 wrapping issues, but for the 6DoF task tested we found similar performance to the roll, pitch, yaw observation.\n\n\nGT-State MLP 3-step SE\n\n\nAdditional Analysis of Sample Efficiency and Baseline Comparisons\n\nHere we add additional analysis of the results in Tab. 2 and Tab. 3, which show the sample efficiency of baselines trained from stochastic demonstrations for each task, and evaluated on unseen test settings.\n\nThe results suggest that Transporter Networks accelerate end-to-end learning on the wide variety of tasks tested. We believe this is due to the inductive bias of the model, which i) incorporates a high-level prior (that manipulation involves rearranging 3D space), and ii) uses the spatial structure of the visual input to exploit symmetries in the data to better learn useful correlations.\n\nAmong the two image-based baselines, Form2Fit performed better than ConvMLP on most tasks -all except the sweeping-piles task, which may be especially hard for the descriptor-based Form2Fit model to work well on the pile of small items, since no item in the pile is significantly different than the others, but rather the policy should be a function of their collective distribution. While for some tasks, like the place-red-in-green task, the Form2Fit model does comparably well to Transporter Networks, it struggles on tasks that require either more precision, or multi-step tasks. The Tab. 3 results for Form2Fit are also especially interesting -although the model can 1-shot generalize with 100% success on the simplified block insertion task, it starts to struggle fitting additional stochastic demonstrations. The ConvMLP model for the most part sees a monotonic increase in performance with additional demonstrations, but often does not generalize as precisely as other models in this small-data regime. In all scenarios tested, Transporter Networks outperformed both image-based baselines.\n\nFor the state-based baselines, these experiments also show that Transporter Networks often require less data than MLP policies trained from ground truth state (e.g., list of object poses, bounding boxes, and color). Of course, it is possible to have a policy that consumes the ground truth state of the scene and performs perfectly on all tasks -indeed, the scripted expert policies do exactly this. For learning generalizable policies from demonstration, however, our experiments suggest that directly reasoning over the spatial configuration of visual cues may instead provide grounds for discovering more generalizable patterns for manipulation. The simplest example of this may be that it's easier to learn, from few demonstrations, a policy that is \"pick a red pixel\" and \"place in a green pixel\", rather than to regress these values in a continuous space, from a vector of continuous values. Other additional cues that may be easier to learn directly from images include: aligning edges and corners when packing boxes and assembling kits, or matching gripper shapes to object surfaces for grasping.\n\nIn addition to the difference of using either images or ground-truth state, it is also important to keep in mind that the GT-State MLP model fundamentally differs in a couple other ways as well, including that its action space is continuous rather than discrete, and it is trained with a multi-modal regression loss rather than a classification loss. It would be interesting to consider other model architectures that consume ground-truth state information and better model intra-object relationships and variable-dimensionality input, such as graph neural networks (GNNs), and also consider models that naturally include some translation equivariance, which MLPs do not. Given the ubiquitity of MLPs in continuous regression tasks, however, and its proximity to the ConvMLP baseline, we consider our chosen MLP model to be a highly relevant baseline.\n\nOne additional note is that for the sweeping-piles task, we observe that a Transporter Network which is constrained to choose a sequence of two translation-only poses, where the rotation is determined as being orthogonal to the line between these two poses, works better than allowing the model to choose a rotation at each pose. This may be due to the specifics of the sweeping motion-primitive used for this task, but it also may indicate that the rigid-transformation bias in the model has its limits. The pile of objects does not get perfectly rigidly transformed, but rather has more complex dynamics. We plan to investigate this further in future work.\n\n\nSE(3) Transporter Networks: Architecture Discussion and 6DoF Task Analysis\n\nNetwork Architecture Considerations. We experimented with multiple versions of extending Transporter Networks into 6-degree-of-freedom, SE(3) action spaces. The hybrid discrete/continuous model we have presented in the main paper provides a number of nice features, and demonstrates the flexibility of the transport operation. It would also be interesting to try pure-discrete classification models, but fully sampling over the entire action space becomes burdensome with higher degrees of freedom. In the first SE(2) step, convolution can be used to efficiently imagine all overlays for the image-width and image-height dimensions, requiring sampling only over the in-plane rotations. However, the remaining 3 degrees of freedom in SE (3) are all orthogonal to the in-plane image dimensions. While a discrete version could be done with rotating 3D voxel grids, this might present memory and sparsity challenges. Instead, our presented choice of using continuous regression to directly regress the remaining degrees of freedom is an attractive one -essentially building on the same architecture as the SE(2) version with little modification.\n\nHere we provide additional details of our specific evaluated SE(3) approach with results shown in Tab. 4. As discussed in Section 3.1, all SE(2) degrees of freedom are resolved identically to the SE(2)-only models. An additional model then handles the remaining SE(3) degrees of freedom: roll, pitch, and z. The additional model is almost identical to the SE(2) pick-conditioned placing model, but is trained to do three independent regressions rather than one classification. In this way, it still benefits from the imagined overlay of the transport operation as an attention mechanism. To adjust the architecture for this purpose, we first increased the output dimensionality to give d=24 channels, rather than the d=3 channels used for cross-correlation in the the classification models. Additionally, rather than cross-correlating all 24 channels at once and reducing to an R H\u00d7W \u00d7k tensor, for image height H, width W , and in-plane rotation discretizations k, we cross-correlate three distinct subsets of 8 channels to give an R H\u00d7W \u00d7k\u00d73 tensor, which retains sufficient degrees of freedom to regress 3 independent values. These 3 values in this tensor can be directly trained to do regression, but we find precision improvements if we add a small amount of non-linearity. The small non-linearity mapping we use, f(\u00b7) in Sec. 3.1, is three independent MLP networks, which each take in one value and regress one value, and are each 3-layer, 32-width MLP networks with ReLu activations. This is similar to 1x1 convolutions on the spatial feature maps, but with non-linearity. We also find increased generalization if, rather than applying the regression loss only to the correct 1-hot label in R H\u00d7W \u00d7k , we also apply regression loss to a small window in that region. We used a window of 7x7 pixels, and +/-1 discretization in k.\n\nWe tried variations to the above described approach, many of which worked reasonably well. One option, for example, is to use only one pick-conditioned placing network, which produces an R H\u00d7W \u00d7k\u00d74 tensor, of which for the last tensor dimension, 1 channel is used for classification loss and the others are used for regression. We found, however, both the classification loss and the regression loss to benefit from using separate networks-this is potentially due to differences in the training dynamics of the classification and regression losses. We also tried various different methods of extracting the 3 regression channels from the spatial feature maps, including using spatial soft(arg)max from the overlaid crops followed by MLPs, and using concatenation rather than cross-correlation before the MLPs. Each of these worked reasonably well.\n\nAdditional 6DoF Task Performance Analysis. Here we expand on analysis of the results presented in Tab. 4. One important observation is that it was not clear, prior to these experiments, that the model would be able to successfully generalize due to its multi-step approach. In particular, the SE(2) placing step occurs before control of the remaining degrees of freedom. While this might be more challenging for different objects such as long and thin ones, we find that for our tested environments, we are able to get above-90% generalization on this challenging task given 1,000 training examples. Introspection into the accuracy of the different degrees of freedom of this model provides interesting comparisons as well. Specifically, for n=1,000 training examples, the mean absolute errors (MAE) for roll/pitch regressions were each in the range of 3 to 4 degrees, whereas the MAE for in-image-plane rotation (yaw) were in the range of 10 degrees, and most often contributed to unsuccessful deployments in test environments. This suggests that future work building on this model could investigate an iterative approach in which the initial degrees of freedom would once again be optimized, potentially iteratively N times, and potentially with increasingly fine resolution discretization. Additionally, it should be noted that in our tested model the discrete step occurs before the regression step, and so in a sense has a more challenging task, since there may not be good alignment of the remaining degrees of freedom. Additionally, we only used SE(2) augmentation as discussed in Sec. 6.6, since the presence of gravity does not make all roll/pitch-augmented worlds equal, and so the remaining SE(3) degrees of freedom do not benefit from the same level of augmentation. We do find however that even in the n=1 demonstrations case, with 38 % success, the SE(3) model behaves as if it uses one azimuth angle, consistent with a single roll/pitch being rotated around the z axis, but this is actually a manifold of different roll/pitch values. This shows the benefit of SE(2) augmentation even for the remaining SE(3) degrees of freedom.\n\nAlso as shown in Tab. 4, our tested GT-State MLP model significantly struggled with the 6DoF SE(3) task, much more so than it did with the 3DoF SE(2) task. Investigation into the cause of the challenge shows that this is primarily due to the additional degrees of freedom in the observation and action spaces. In particular we can see this in the tested 3-step model (described in Sec. 6.8), which often struggles to precisely regress the x and y translational degrees of freedom even with n=1,000 demonstrations, which was not a problem in the simpler task. Additionally, if we take this task's data, but reduce the observation and action spaces to only be SE(2), then with n=1,000 demonstrations the model is once again able to precisely regress the x and y translations. We hypothesize that the mix of the stochastic demonstrations and the full 6-DoF placing is challenging for the model to learn a generalizable mapping with this amount of data.\n\n\nInterpolation and Extrapolation\n\nTo gain a better qualitative understanding of the generalization abilities of Transporter Networks and the tested baseline models, we provide Fig. 9. Similar to the analysis in [10], this shows the spatial distribution of training configurations and successful/unsuccessful testing configurations-in this case, for translation-only block insertion with stochastic demonstrations. For each method we visualize its ability to interpolate and extrapolate from its training samples to handle new unseen poses of the target fixture.\n\nAs can be seen from these generalization plots, on this task Transporter Networks is capable of extrapolating to new testing episodes with just one demonstration. The dense deep feature transport operation enables the optimal placement of the block in the fixture to look similar under different picking and placing actions (exploiting translational equivariance of the task). This generalizes to new object configurations better than Conv MLP, which seems to only successfully interpolate around the majority of training episodes (often collapsing towards one mode of the training data).\n\nForm2Fit is also capable of extrapolating from a single example, since both its picking and placing networks (also modeled by FCNs) are translationally equivariant -such that the block insertion task amounts to detecting the block and fixture under different translations. Once the number of demonstrations increase, however, the matching network is forced to learn pixel-wise descriptors that index multiple possible picking actions to multiple possible placing actions. In the setting with stochastic demonstrations, the positive-pair samples available for training the matching network are sparse and limited only to the pick-and-place action pairs from demonstrations -making it difficult to geometrically regularize the descriptors uniformly over the pixels on the surface of the block and the fixture. Minor mis-matches during testing can also lead to a slight misalignment of the block, which leads to poor placement and task failure. This suggests that Form2Fit's matching network requires substantial training data to successful converge to a meaningful solution. The distribution of successful episodes with Form2Fit are also uniformly distributed over the workspace, which further confirms that its failures come largely from mismatching pick-and-place actions, rather than its inability to spatially interpolate or extrapolate.\n\nIn the low data regime (with 10 demonstrations), GT-State MLP seems to overtrain to correlations that do not necessarily enable task success, e.g., that picking locations are conditioned on the location of the target fixture (see distribution of failed pick locations and fixture locations). By dividing GT-State MLP into two steps -one for picking and one for placing, we see less issues with incorrect correlations. However, the inability to extrapolate too far beyond the distribution of training data still remains (e.g., see failure  testing configurations of fixture for GT-State MLP 2-Step with 100 demonstrations, which also caused an incorrect pick point prediction).\n\n\nAblation Studies\n\nTab. 6 also reports the performance of various ablations of Transporter Networks:\n\nNo-Transport uses two FCNs: one to predict a picking heatmap, another to predict placing heatmaps (one per discretized rotation). The transport model is not used here, hence there is no pick-conditioned placing.\n\nFor simpler tasks, such as place-red-in-green, this could suffice if the picking model predicts higher values on red pixels, and the placing model predicts higher values on green pixels. Or, for block-insertion, if the picking model detects the same location on the block, and the placing model detects the same corresponding location and orientation in the fixture. However, it is much more challenging to execute tasks that require additional visual reasoning, such as aligning unseen box corners. For palletizing and packing boxes, the learned policies are generally capable of placing a few of the boxes into the target zones, but fail to align them to each other tightly to achieve higher completion scores above 55%.\n\nPer-Pixel-CE-Loss is a variant of the Transporter Network, where the cross-entropy loss over the entire image (representing a single probability distribution) is replaced with per-pixel cross-entropy, where each pixel represents its own probability distribution. During training, for every demonstration pick-and-place: the picking and placing pixels unraveled from the data are chosen as positive samples for the models Figure 10. Visualizing Transporter Network Predictions. For the kit assembly task (a), Transporter Network picking predictions show 3 dominant modes on 3 different objects (b) which are not necessarily centered on the object (which may reflect biases learned from the distribution of training data). If a pick is off-center e.g., on the circle block, (c, top), the Transporter Network correctly makes placing predictions along the edges of the target placement location -which is visualized as a ring on Qplace maxed over rotations. If a pick is on the corner of the unseen E-shaped object (c, bottom), then two dominant modes lie on the correct corresponding locations of their target placements -but also with false positive modes from geometric symmetries in the letter E. We also show modes of predicted placing values over rotations for pick-and-place locations centered on the object and fixture, which appear uniform for circle, 4 modes for the cross shape, 5 modes for star shape, 2 modes for the unseen E-shaped block (dominant one being correct). We also visualize how the predicted placing values over rotations change from uniform, to unimodal, when shifting one pixel to the right from the center of the circle.\n\nrespectively, while a fixed number of 100 random negative pixels are sampled from everywhere else. In general, our experiments suggest that the model i) takes more training iterations to converge, and ii) requires more hyper-parameter tuning to balance the ratio of positives to negatives to regularize the loss over time.\n\nblock-insertion place-red-in-green towers-of-hanoi align-box-corner stack-block-pyramid  Table 6. Ablative comparisons. Task success rate (mean %) vs. # of demonstration episodes (1, 10, 100, or 1000) used in training.\n\n\nTraining Convergence\n\nWe train Transporter Networks with Adam [53], using fixed learning rates of 10 \u22125 . Our models are trained from scratch (i.e., random initialization) in Tensorflow. On the right, we show an example convergence plot over training from 100 demonstrations of manipulating rope. Compared with image-based baselines, Transporter Networks generally converge faster -within a few thousand training iterations, i.e., one or two hours of wall-clock time. Fig. 10 visualizes Transporter Network dense pixel-wise predictions of Q pick and Q place for the kit assembly task (trained with 1,000 demonstrations), and how the predicted modes change with respect to the locations of objects, their shapes, and conditioned picks. These visualizations show that due to rotation and translation equivariance, our method can quickly learn modes that reflect symmetries in the data.\n\n\nVisualizing Transporter Network Predictions\n\nThese visualizations also show an interesting failure mode, in which predicted placements of the unseen E-shaped object returns several 180 degree flipped false positives (see Fig. 10c). This is because the learned features from this task rely strongly on local geometry. Since the E shape is a new unseen object, a flipped E looks similar enough to return a false positive signal there (though not enough to be the top mode).\n\n\nReal-World Experiments Details\n\nTo demonstrate the real-world performance of Transporter Networks, we apply our approach to two challenging real-world tasks: assembling kits of mouthwash bottles with pick and place, and sweeping piles of small Go pieces with pushing. These tasks are shown in Fig. 11.\n\nFor kit assembly, we sequentially pick 5 small mouthwash bottles from a \"bin\" of identically shaped items and place each into a \"kit\" with 5 corresponding indentations for placement. This is a challenging task due to the multi-modality and ambiguity of the pick location (many identical items in the bin to pick from), as well as the tight millimeter tolerances of the destination kit. It is designed to mimic a production robotic task of placing items into a blister pack for consumer packaging. Kit assembly and disassembly are treated as separate tasks -both use the same Transporter Network architecture with autonomous switching between the two tasks at inference time. As shown in Table 5, our model achieves 98.9% on kit assembly, trained with 8,141 pick and place actions recorded from human demonstrations. Picking success and disassembly success are both above 99%. For sweeping, we sequentially push piles of small Go pieces into a target goal zone, marked with green tape. Upon completion of the task, the robot autonomously resets with a scripted scrambling motion. This task is challenging as it tests the capacity of the model to sequence actions using closedloop visual feedback, and to do it efficiently. As shown in Table 5, our model achieves 98.3% on this task, trained with 6,759 pushing actions recorded from human demonstrations. The average number of sweeps needed to complete the task is 20 \u00b1 2.5 sweeps, with the fastest run being 16 sweeps and the slowest 25. For reference, the average for human teleoperation is 18.1 sweeps.\n\nTo train our system for both tasks, we use human demonstration data captured by a remote teleoperation interface. The sub-optimal, biased and noisy distribution of human demonstrations presents a further challenge for robotic policy learning. In addition, 13 teleoperators of varying skill and performance level provide training data, which further increases the variance of ground-truth actions that we train from and subsequently increases the difficulty of the learning task.\n\nHardware Setup e c b d a Figure 12. Hardware setup. a) UR5e Robot, b) Piab suction cup, c) Photoneo camera, d) Linux PC, and e) air compressor and vacuum generator.\n\nThe hardware setup for kit assembly is shown in Fig 12. Our experiments make use of 2 Universal Robots UR5 workstations (one for kit assembly and one for sweeping), each with an industrial Linux PC, vacuum system, Piab suction cup or brush end-effector, and depth camera (statically mounted above the workstation). To test the robustness of our method to diverse data sources, we use high quality depth camera observations from a Photoneo PhoXi Model M for kit assembly, and those from low cost consumer-grade Azure Kinect for sweeping. The Photoneo camera supplies depth (with 0.1mm rated depth precision) and a greyscale IR image, both at 1032\u00d7772 resolution. The Kinect camera supplies depth as well as a color RGB image at 1280\u00d7720 resolution.\n\nTo calibrate the camera within the robot coordinate frame we use a two step procedure. Firstly, the intrinsics of the camera are calibrated by capturing multiple images of a large flat QR-Code panel in varying orientations, and we use OpenCV to calculate the camera intrinsics. Secondly, to calibrate extrinsics we attach QR code tags to the UR5 wrist joint and we capture multiple images of the robot in random end-effector poses. These images are then used to solve for the position of the robot base as well as the offset of the QR Tag to its respective joint with a stochastic optimization procedure.\n\nA Linux system embedded with the robot gathers robot and camera data and forwards it to the teleoperator over the internet. Any commands passed to the robots are queued up to be processed one-by one. All data is logged in real-time via network to a cloud storage database for model training and evaluation.  \n\n\nData Collection\n\nWe designed a 2D User-Interface (UI) scripted in Unity [46] in order to control the robot workstations remotely. An overview of this UI is shown in Fig 13. Features of this UI include: configuration of the robot workcell, movement and actuation of the suction end-effector, configuration of parameterized action trajectories, calibration of the robot and control of the robot using the same action parameterization used to train our model.\n\nA standard teleoperation session is as follows. The operator views a 3D rendering of the workstation depth data. The synthetic camera view can be rotated and translated using mouse movements as desired by the operator. The operator determines with mouse clicks which object to pick by selecting the intended pick-up point on the rendered depth geometry. A region of the point cloud around the pick location is then \"attached\" to the mouse cursor, where it is rendered transparent and overlaid on the existing depth (illustrated in Fig 13). The operator is able to translate this geometry with movements of the mouse and the single rotational degree of freedom is controlled by the mouse wheel. Once the operator moves the object into the desired location, an additional mouse click will select the 3DOF place location. This interface allows the operator to visualize the intended 3D configuration of the pick-and-place operation before the action is executed. The discrete teleoperator pick-and-place locations are then translated into dense robot trajectories using a Cartesian controller, checked for collisions and other safety constraints, and subsequently executed on-robot. The user then repeats this process until the task is complete.\n\nNote that we use the same data pipeline and motion primitives when executing our Transporter Networks model as is used during human teleoperation; the human UI interaction is replaced with our inference results, and additional logic is added to switch between assembly and disassembly tasks. Autonomous mode is executing within the standard UI by first inputting the task to execute (by unique task code identifier), and pressing start. The UI for this (and visualization of inference results) is shown in Fig 14. Using the above system we collected approximately 8,141 pick-and-place actions from 13 human operators in order to train the kit assembly model, an additional 2,300 actions for validation of model performance and hyper parameter tuning, and 6,759 pushing actions to train the sweeping model.\n\n\nSuccess Detection\n\nFor the on-robot evaluation in Section 4.2, we manually label robot actions for success or failure. However this process is time consuming and expensive. Additionally, our Transformer Network does not estimate when a multi-action kitting episode terminates (either successfully or unsuccessfully). So in order to perform fully-autonomous pick-and-place without manual success labelling, an automatic task-level success signal was implemented. We trained a ResNet50 classification model -pretrained on imagenet classification -\n\nFigure 1 .\n1Figure 1. A Transporter Network is a simple model architecture that attends to a local region and predicts its spatial displacement (b) from visual input -which can parameterize robot actions. It is sample efficient in learning complex vision-based manipulation tasks: inserting blocks into fixtures (a), sequential pick-and-place in Towers of Hanoi (c), assembling kits with unseen objects (d), palletizing boxes (e), stacking a pyramid of blocks (f), manipulating rope (g), and pushing piles of small objects with closed-loop feedback (h) -and is practical to deploy on real production robots (k, m).\n\nFigure 2 .\n2Simple planar pick-and-place task where (i) there is a distribution of successful pick poses, and (ii) for each successful pick pose, there is a corresponding distribution of successful place poses.\n\nFigure 3 .\n3Figure 3. In this setting (a) where the task is to pick up the red block with an immobilizing grasp (e.g., suction) and place it into the fixture, the goal of Transporter Networks is to recover the distribution of successful picks (b), and distribution of successful placements (c) conditioned on a sampled pick. For pick-conditioned placing (c), deep feature template matching occurs with a local crop around the sampled pick as the exemplar. Rotations of the crop around the pick are used to decode the best placing rotation. Our method preserves rotation and translation equivariance for efficient learning.\n\nFigure 4 .\n4In this 6-DoF variant of the task presented inFig. 3, the fixture location varies as well in the out-of-plane rotations (rx, ry), and height (z). Transporter Networks can address this task through multi-stage inference.\n\n\n. 1). We show examples of these emerging behaviors in our supplementary video. .0 81.0 69.0 11.0 31.0 52.0 68.0 GT-State MLP 100 100 100 100 21.0 77.0 100 100 GT-State MLP 2-Step 100 100 100 100 53.0 70.0 100 93.0\n\nFigure 8 .\n8Kitting objects used in training (left 14) and testing (right 6).\n\n\nof fixture (success, failure) testing pick points on block (success, failure) training configurations of fixture\n\nFigure 9 .\n9Depictions of the generalization ability of different models on the simplified translation-only block-insertion task. Each episode is visualized as a mark on the workspace (representing the pose of the fixture) -color-coded by task success (blue for success, red for failure). Environment configurations in the training set are shown in gray, and the convex hull of the training samples are drawn with a dotted line. Testing episodes inside the convex hell represent interpolation; those outside represent extrapolation. Plot inspired by[10].\n\nFigure 11 .\n11Real-world tasks. Left: assembling kits of mouthwash bottles, right: pushing piles of small Go pieces.\n\nFigure 13 .\n13Data Collection UI. Left: Unity-based UI. Middle: The operator selects the area where the object is to be picked up. Right: The operator can adjust the placement of the object and rotate it. The depth image around the pick-up point is overlaid and helps the operator align the placement.\n\nFigure 14 .\n14Autonomous Mode UI. User inputs the task to execute (by unique task code), and presses the play button to begin autonomous execution.\n\n\nFor pick-conditioned placing (c), deep feature template matching occurs with a local crop around the sampled pick as the exemplar. Rotations of the crop around the pick are used to decode the best placing rotation. Our method preserves rotation and translation equivariance for efficient learning. Pick-Conditioned Placing via Transporting. Spatially consistent visual representations enable us to perform visuo-spatial transporting, in which dense pixel-wise features from a partial crop o t [T pick ] centered on T pick are rigidly transformed then overlaid on top of another partial crop o t [\u03c4] centered on a candidate place pose \u03c4, where o t is the observation before the pick. Intuitively, if the crop encompasses an object, then this operation can be thought of as imagining the object at another location in the scene.Fig. 3cshows this in the context of overlaying the red block across the scene to find its placement in the fixture.In this work, the goal of Transporter Networks is to transport our partial crop o t [T pick ] densely across a set of poses {\u03c4 i } to search for its best placement, i.e. the o t [\u03c4 i ] with the highest feature correlation. This operation is analogous to the search function in visual tracking literatureLearning \n\nTable 1 .\n1Each task in Ravens is characterized by a unique set of attributes.\n\nTable 2 .\n2Baseline comparisons. Task success rate (mean %) vs. # of demonstration episodes (1, 10, 100, or 1000) used in training.\n\nTable 4 .\n46DoF block-insertion: our method generalizes with fewer demonstrations than GT-State MLPs.\n\n\nTable 5. Transporter Networks test performance on real robots with human demonstration data.Task \nTest % # Samples \n\nMouthwash Kit Assembly \n98.9 \n8141 \nSweeping Piles of Go Pieces 98.3 \n6759 \n\n\nAcknowledgmentsSpecial thanks to Ken Goldberg, Razvan Surdulescu, Daniel Seita, Ayzaan Wahid, Vincent Vanhoucke, Anelia Angelova, for helpful feedback on writing, Sean Snyder, Jonathan Vela, Larry Bisares, Michael Villanueva, Brandon Hurd for operations and hardware support, Robert Baruch for software infrastructure, Jared Braun for UI contributions, Erwin Coumans for PyBullet advice, Laura Graesser for video narration.\nEnd-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, P Abbeel, The Journal of Machine Learning Research. JMLRS. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research (JMLR), 2016.\n\nQt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, Conference on Robot Learning (CoRL). D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. Conference on Robot Learning (CoRL), 2018.\n\nkpam: Keypoint affordances for category-level robotic manipulation. L Manuelli, W Gao, P Florence, R Tedrake, International Symposium on Robotics Research (ISRR). L. Manuelli, W. Gao, P. Florence, and R. Tedrake. kpam: Keypoint affordances for category-level robotic manipulation. International Symposium on Robotics Research (ISRR), 2019.\n\nUnsupervised learning of object keypoints for perception and control. T D Kulkarni, A Gupta, C Ionescu, S Borgeaud, M Reynolds, A Zisserman, V Mnih, NeurIPST. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman, and V. Mnih. Unsupervised learning of object keypoints for perception and control. NeurIPS, 2019.\n\nDeep dynamics models for learning dexterous manipulation. A Nagabandi, K Konolige, S Levine, V Kumar, Conference on Robot Learning (CoRL). 2020A. Nagabandi, K. Konolige, S. Levine, and V. Kumar. Deep dynamics models for learning dexterous manipulation. Conference on Robot Learning (CoRL), 2020.\n\nKeypose: Multi-view 3d labeling and keypoint estimation for transparent objects. X Liu, R Jonschkowski, A Angelova, K Konolige, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020X. Liu, R. Jonschkowski, A. Angelova, and K. Konolige. Keypose: Multi-view 3d labeling and keypoint estimation for transparent objects. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nMetric learning for generalizing spatial relations to new objects. O Mees, N Abdo, M Mazuran, W Burgard, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). O. Mees, N. Abdo, M. Mazuran, and W. Burgard. Metric learning for generalizing spatial relations to new objects. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.\n\nOptimization beyond the convolution: Generalizing spatial relations with end-to-end metric learning. P Jund, A Eitel, N Abdo, W Burgard, IEEE International Conference on Robotics and Automation (ICRA). P. Jund, A. Eitel, N. Abdo, and W. Burgard. Optimization beyond the convolution: Generalizing spatial relations with end-to-end metric learning. IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nDense object nets: Learning dense visual object descriptors by and for robotic manipulation. P R Florence, L Manuelli, R Tedrake, Conference on Robot Learning (CoRL). P. R. Florence, L. Manuelli, and R. Tedrake. Dense object nets: Learning dense visual object descriptors by and for robotic manipulation. Conference on Robot Learning (CoRL), 2018.\n\nSelf-supervised correspondence in visuomotor policy learning. P Florence, L Manuelli, R Tedrake, IEEE Robotics and Automation Letters. RA-LP. Florence, L. Manuelli, and R. Tedrake. Self-supervised correspondence in visuomotor policy learning. IEEE Robotics and Automation Letters (RA-L), 2019.\n\nLearning rope manipulation policies using dense object descriptors trained on synthetic depth data. P Sundaresan, J Grannen, B Thananjeyan, A Balakrishna, M Laskey, K Stone, J E Gonzalez, K Goldberg, IEEE International Conference on Robotics and Automation (ICRA). 2020P. Sundaresan, J. Grannen, B. Thananjeyan, A. Balakrishna, M. Laskey, K. Stone, J. E. Gonzalez, and K. Goldberg. Learning rope manipulation policies using dense object descriptors trained on synthetic depth data. IEEE International Conference on Robotics and Automation (ICRA), 2020.\n\nThe surprising effectiveness of linear models for visual foresight in object pile manipulation. H Suh, R Tedrake, Workshop on Algorithmic Foundations of Robotics (WAFR). 2020H. Suh and R. Tedrake. The surprising effectiveness of linear models for visual foresight in object pile manipulation. Workshop on Algorithmic Foundations of Robotics (WAFR), 2020.\n\nOn the generalization of equivariance and convolution in neural networks to the action of compact groups. R Kondor, S Trivedi, International Conference on Machine Learning (ICML). R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. International Conference on Machine Learning (ICML), 2018.\n\nT Cohen, M Welling, Group equivariant convolutional networks. International Conference on Machine Learning (ICML). T. Cohen and M. Welling. Group equivariant convolutional networks. International Conference on Machine Learning (ICML), 2016.\n\nDeictic image mapping: An abstraction for learning pose invariant manipulation policies. R Platt, C Kohler, M Gualtieri, AAAI Conference on Artificial Intelligence. R. Platt, C. Kohler, and M. Gualtieri. Deictic image mapping: An abstraction for learning pose invariant manipulation policies. AAAI Conference on Artificial Intelligence, 2019.\n\n. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540Openai gym. arXiv preprintG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n\nReal-time tracking and pose estimation for industrial objects using geometric features. Y Yoon, G N Desouza, A C Kak, IEEE International Conference on Robotics and Automation (ICRA). Y. Yoon, G. N. DeSouza, and A. C. Kak. Real-time tracking and pose estimation for industrial objects using geometric features. IEEE International Conference on Robotics and Automation (ICRA), 2003.\n\nSingle image 3d object detection and pose estimation for grasping. ICRA. M Zhu, K G Derpanis, Y Yang, S Brahmbhatt, M Zhang, C Phillips, M Lecce, K Daniilidis, M. Zhu, K. G. Derpanis, Y. Yang, S. Brahmbhatt, M. Zhang, C. Phillips, M. Lecce, and K. Daniilidis. Single image 3d object detection and pose estimation for grasping. ICRA, 2014.\n\nMulti-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge. A Zeng, K.-T Yu, S Song, D Suo, E Walker, A Rodriguez, J Xiao, IEEE International Conference on Robotics and Automation. ICRAA. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker, A. Rodriguez, and J. Xiao. Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge. IEEE International Conference on Robotics and Automation (ICRA), 2017.\n\nSelf-supervised 6d object pose estimation for robot manipulation. X Deng, Y Xiang, A Mousavian, C Eppner, T Bretl, D Fox, IEEE International Conference on Robotics and Automation (ICRA). 2020X. Deng, Y. Xiang, A. Mousavian, C. Eppner, T. Bretl, and D. Fox. Self-supervised 6d object pose estimation for robot manipulation. IEEE International Conference on Robotics and Automation (ICRA), 2020.\n\nNormalized object coordinate space for category-level 6d object pose and size estimation. H Wang, S Sridhar, J Huang, J Valentin, S Song, L J Guibas, Computer Vision and Pattern Recognition (CVPR). H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. Computer Vision and Pattern Recognition (CVPR), 2019.\n\nManeuver-based motion planning for nonlinear systems with symmetries. E Frazzoli, M A Dahleh, E Feron, IEEE Transactions on Robotics. E. Frazzoli, M. A. Dahleh, and E. Feron. Maneuver-based motion planning for nonlinear systems with symmetries. IEEE Transactions on Robotics (T-RO), 2005.\n\nPick and place without geometric object models. M Gualtieri, A Pas, R Platt, IEEE International Conference on Robotics and Automation (ICRA). M. Gualtieri, A. ten Pas, and R. Platt. Pick and place without geometric object models. IEEE International Conference on Robotics and Automation (ICRA), 2018.\n\nForm2fit: Learning shape priors for generalizable assembly from disassembly. K Zakka, A Zeng, J Lee, S Song, IEEE International Conference on Robotics and Automation (ICRA). 2020K. Zakka, A. Zeng, J. Lee, and S. Song. Form2fit: Learning shape priors for generalizable assembly from disassembly. IEEE International Conference on Robotics and Automation (ICRA), 2020.\n\ngood robot!\": Efficient reinforcement learning for multi-step visual tasks with sim to real transfer. A Hundt, B Killeen, N Greene, H Wu, H Kwon, C Paxton, G D Hager, IEEE Robotics and Automation Letters. 2020RA-LA. Hundt, B. Killeen, N. Greene, H. Wu, H. Kwon, C. Paxton, and G. D. Hager. \"good robot!\": Efficient reinforcement learning for multi-step visual tasks with sim to real transfer. IEEE Robotics and Automation Letters (RA-L), 2020.\n\nSelf-supervised learning for precise pick-and-place without object model. L Berscheid, P Mei\u00dfner, T Kr\u00f6ger, IEEE Robotics and Automation Letters. 2020RA-LL. Berscheid, P. Mei\u00dfner, and T. Kr\u00f6ger. Self-supervised learning for precise pick-and-place without object model. IEEE Robotics and Automation Letters (RA-L), 2020.\n\nLearning to manipulate deformable objects without demonstrations. Y Wu, RSSW Yan, RSST Kurutach, RSSL Pinto, RSSP Abbeel, RSSRobotics: Science and Systems. 2020Y. Wu, W. Yan, T. Kurutach, L. Pinto, and P. Abbeel. Learning to manipulate deformable objects without demonstrations. Robotics: Science and Systems (RSS), 2020.\n\nSelf-supervised goal-conditioned pick and place. C Devin, P Rowghanian, C Vigorito, W Richards, K Rohanimanesh, arXiv:2008.11466arXiv preprintC. Devin, P. Rowghanian, C. Vigorito, W. Richards, and K. Rohanimanesh. Self-supervised goal-conditioned pick and place. arXiv preprint arXiv:2008.11466, 2020.\n\nAction image representation: Learning scalable deep grasping policies with zero real world data. M Khansari, D Kappler, J Luo, J Bingham, M Kalakrishnan, IEEE International Conference on Robotics and Automation (ICRA). 2020M. Khansari, D. Kappler, J. Luo, J. Bingham, and M. Kalakrishnan. Action image representation: Learning scalable deep grasping policies with zero real world data. IEEE International Conference on Robotics and Automation (ICRA), 2020.\n\nGrasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations. S Song, A Zeng, J Lee, T Funkhouser, IEEE Robotics and Automation Letters. 2020RA-LS. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters (RA-L), 2020.\n\nRobotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. A Zeng, S Song, K.-T Yu, E Donlon, F R Hogan, M Bauza, D Ma, O Taylor, M Liu, E Romo, International Conference on Robotics and Automation (ICRA). A. Zeng, S. Song, K.-T. Yu, E. Donlon, F. R. Hogan, M. Bauza, D. Ma, O. Taylor, M. Liu, E. Romo, et al. Robotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching. International Conference on Robotics and Automation (ICRA), 2018.\n\nClosing the loop for robotic grasping: A real-time, generative grasp synthesis approach. D Morrison, P Corke, J Leitner, Robotics: Science and Systems (RSS). D. Morrison, P. Corke, and J. Leitner. Closing the loop for robotic grasping: A real-time, generative grasp synthesis approach. Robotics: Science and Systems (RSS), 2018.\n\nLearning synergies between pushing and grasping with self-supervised deep reinforcement learning. A Zeng, S Song, S Welker, J Lee, A Rodriguez, T Funkhouser, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser. Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nLearning Visual Affordances for Robotic Manipulation. A Zeng, Princeton UniversityPhD thesisA. Zeng. Learning Visual Affordances for Robotic Manipulation. PhD thesis, Princeton University.\n\nSiamrpn++: Evolution of siamese visual tracking with very deep networks. B Li, W Wu, Q Wang, F Zhang, J Xing, J Yan, Conference on Computer Vision and Pattern Recognition. B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. Conference on Computer Vision and Pattern Recognition, 2019.\n\nL Bertinetto, J Valmadre, J F Henriques, A Vedaldi, P H Torr, Fully-convolutional siamese networks for object tracking. European Conference on Computer Vision (ECCV). L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr. Fully-convolutional siamese networks for object tracking. European Conference on Computer Vision (ECCV), 2016.\n\nSiamese instance search for tracking. R Tao, E Gavves, A W Smeulders, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). R. Tao, E. Gavves, and A. W. Smeulders. Siamese instance search for tracking. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nUnderstanding the effective receptive field in deep convolutional neural networks. W Luo, Y Li, R Urtasun, R Zemel, Advances in Neural Information Processing Systems (NeurIPS). W. Luo, Y. Li, R. Urtasun, and R. Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in Neural Information Processing Systems (NeurIPS), 2016.\n\nLearning to rearrange deformable cables, fabrics, and bags with goal-conditioned transporter networks. D Seita, P Florence, J Tompson, E Coumans, V Sindhwani, K Goldberg, A Zeng, arXiv:2012.03385arXiv preprintD. Seita, P. Florence, J. Tompson, E. Coumans, V. Sindhwani, K. Goldberg, and A. Zeng. Learning to rearrange deformable cables, fabrics, and bags with goal-conditioned transporter networks. arXiv preprint arXiv:2012.03385, 2020.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nF Yu, V Koltun, Multi-scale context aggregation by dilated convolutions. International Conference on Learning Representations. F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. International Conference on Learning Representations, 2016.\n\nV Nair, G E Hinton, Rectified linear units improve restricted boltzmann machines. International Conference on Machine Learning (ICML). V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. International Conference on Machine Learning (ICML), 2010.\n\nPybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, GitHub Repository. E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. GitHub Repository, 2016.\n\nMixture density networks. C M Bishop, Aston UniversityC. M. Bishop. Mixture density networks. Aston University, 1994.\n\nUnity game engine. Version: 2018.4.0f1. Unity TechnologiesUnity Technologies. Unity game engine. Version: 2018.4.0f1. URL https://unity.com.\n\nSelf-supervised visual descriptor learning for dense correspondence. T Schmidt, R Newcombe, D Fox, IEEE Robotics and Automation Letters. RA-LT. Schmidt, R. Newcombe, and D. Fox. Self-supervised visual descriptor learning for dense correspondence. IEEE Robotics and Automation Letters (RA-L), 2016.\n\nLearning to linearize under uncertainty. R Goroshin, M F Mathieu, Y Lecun, Advances in Neural Information Processing Systems (NeurIPS). R. Goroshin, M. F. Mathieu, and Y. LeCun. Learning to linearize under uncertainty. Advances in Neural Information Processing Systems (NeurIPS), 2015.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nVision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. ICRA. R Rahmatizadeh, P Abolghasemi, L B\u00f6l\u00f6ni, S Levine, R. Rahmatizadeh, P. Abolghasemi, L. B\u00f6l\u00f6ni, and S. Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. ICRA, 2018.\n\nB Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, Nerf: Representing scenes as neural radiance fields for view synthesis. European Conference on Computer Vision (ECCV). 2020B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. European Conference on Computer Vision (ECCV), 2020.\n\nImplicit neural representations with periodic activation functions. V Sitzmann, J N Martel, A W Bergman, D B Lindell, G Wetzstein, Advances in Neural Information Processing Systems (NeurIPS). 2020V. Sitzmann, J. N. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nto perform 3-way classification in order to detect full, empty or partial kits. We trained the classifier on 4848 training samples and evaluated on 128 validation set samples from a unique camera angle. Mean classification accuracy on the validation set was 96.9%. Upon completion of a pick-and-place attempt, classifier inference on the RGB image produces a success metric. D P Kingma, J Ba, International Conference on Learning Representations (ICLR). Adam: A method for stochastic optimization. which marks the successful completion of a Kitting (or emptying the kit) episode. This allows the fully autonomous mode of the system to alternate between tasks (kitting and emptying) upon completionD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015. to perform 3-way classification in order to detect full, empty or partial kits. We trained the classifier on 4848 training samples and evaluated on 128 validation set samples from a unique camera angle. Mean classification accuracy on the validation set was 96.9%. Upon completion of a pick-and-place attempt, classifier inference on the RGB image produces a success metric, which marks the successful completion of a Kitting (or emptying the kit) episode. This allows the fully autonomous mode of the system to alternate between tasks (kitting and emptying) upon completion.\n", "annotations": {"author": "[{\"end\":110,\"start\":79},{\"end\":146,\"start\":111},{\"end\":185,\"start\":147},{\"end\":221,\"start\":186},{\"end\":258,\"start\":222},{\"end\":295,\"start\":259},{\"end\":334,\"start\":296},{\"end\":368,\"start\":335},{\"end\":400,\"start\":369},{\"end\":435,\"start\":401},{\"end\":473,\"start\":436},{\"end\":506,\"start\":474}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":84},{\"end\":124,\"start\":116},{\"end\":163,\"start\":156},{\"end\":199,\"start\":193},{\"end\":236,\"start\":231},{\"end\":273,\"start\":265},{\"end\":312,\"start\":303},{\"end\":346,\"start\":340},{\"end\":378,\"start\":373},{\"end\":413,\"start\":408},{\"end\":451,\"start\":442},{\"end\":484,\"start\":481}]", "author_first_name": "[{\"end\":83,\"start\":79},{\"end\":115,\"start\":111},{\"end\":155,\"start\":147},{\"end\":192,\"start\":186},{\"end\":230,\"start\":222},{\"end\":264,\"start\":259},{\"end\":302,\"start\":296},{\"end\":339,\"start\":335},{\"end\":372,\"start\":369},{\"end\":407,\"start\":401},{\"end\":441,\"start\":436},{\"end\":480,\"start\":474}]", "author_affiliation": "[{\"end\":109,\"start\":90},{\"end\":145,\"start\":126},{\"end\":184,\"start\":165},{\"end\":220,\"start\":201},{\"end\":257,\"start\":238},{\"end\":294,\"start\":275},{\"end\":333,\"start\":314},{\"end\":367,\"start\":348},{\"end\":399,\"start\":380},{\"end\":434,\"start\":415},{\"end\":472,\"start\":453},{\"end\":505,\"start\":486}]", "title": "[{\"end\":76,\"start\":1},{\"end\":582,\"start\":507}]", "venue": null, "abstract": "[{\"end\":1872,\"start\":615}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2055,\"start\":2052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2057,\"start\":2055},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2125,\"start\":2122},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2127,\"start\":2125},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2129,\"start\":2127},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2131,\"start\":2129},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2147,\"start\":2144},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2149,\"start\":2147},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2175,\"start\":2172},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2178,\"start\":2175},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2181,\"start\":2178},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2231,\"start\":2227},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2520,\"start\":2516},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3784,\"start\":3781},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3786,\"start\":3784},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4086,\"start\":4082},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4089,\"start\":4086},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4164,\"start\":4160},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6667,\"start\":6663},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6925,\"start\":6921},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6928,\"start\":6925},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6931,\"start\":6928},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6934,\"start\":6931},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6937,\"start\":6934},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7117,\"start\":7114},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7119,\"start\":7117},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7121,\"start\":7119},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7123,\"start\":7121},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7149,\"start\":7146},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7152,\"start\":7149},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7155,\"start\":7152},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7498,\"start\":7494},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8055,\"start\":8051},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8058,\"start\":8055},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8112,\"start\":8108},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8383,\"start\":8379},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8386,\"start\":8383},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8389,\"start\":8386},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8392,\"start\":8389},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8395,\"start\":8392},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8398,\"start\":8395},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8401,\"start\":8398},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8404,\"start\":8401},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8487,\"start\":8483},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8952,\"start\":8949},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9140,\"start\":9136},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11828,\"start\":11824},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11831,\"start\":11828},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11834,\"start\":11831},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11954,\"start\":11950},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12529,\"start\":12525},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12532,\"start\":12529},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12535,\"start\":12532},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12538,\"start\":12535},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12541,\"start\":12538},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13453,\"start\":13449},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14103,\"start\":14099},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14106,\"start\":14103},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14109,\"start\":14106},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15162,\"start\":15158},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15898,\"start\":15895},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16097,\"start\":16094},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16792,\"start\":16789},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16870,\"start\":16867},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17035,\"start\":17032},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18750,\"start\":18746},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19733,\"start\":19729},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20697,\"start\":20693},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20928,\"start\":20924},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20972,\"start\":20968},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24639,\"start\":24635},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24703,\"start\":24699},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25192,\"start\":25188},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25922,\"start\":25918},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26816,\"start\":26812},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27041,\"start\":27038},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27805,\"start\":27801},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29150,\"start\":29146},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33289,\"start\":33285},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":43591,\"start\":43587},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45018,\"start\":45014},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":48653,\"start\":48649},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":48726,\"start\":48722},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":48796,\"start\":48793},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":48799,\"start\":48796},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":49037,\"start\":49034},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":49569,\"start\":49565},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":49720,\"start\":49717},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":49949,\"start\":49945},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":50381,\"start\":50378},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":50437,\"start\":50433},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":51053,\"start\":51049},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51136,\"start\":51132},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":51148,\"start\":51144},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":51571,\"start\":51567},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":51574,\"start\":51571},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":52077,\"start\":52073},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":66344,\"start\":66340},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":72598,\"start\":72594},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":78138,\"start\":78134},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":83767,\"start\":83763}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":81733,\"start\":81118},{\"attributes\":{\"id\":\"fig_1\"},\"end\":81945,\"start\":81734},{\"attributes\":{\"id\":\"fig_2\"},\"end\":82569,\"start\":81946},{\"attributes\":{\"id\":\"fig_3\"},\"end\":82802,\"start\":82570},{\"attributes\":{\"id\":\"fig_6\"},\"end\":83018,\"start\":82803},{\"attributes\":{\"id\":\"fig_8\"},\"end\":83097,\"start\":83019},{\"attributes\":{\"id\":\"fig_9\"},\"end\":83212,\"start\":83098},{\"attributes\":{\"id\":\"fig_10\"},\"end\":83768,\"start\":83213},{\"attributes\":{\"id\":\"fig_12\"},\"end\":83886,\"start\":83769},{\"attributes\":{\"id\":\"fig_13\"},\"end\":84189,\"start\":83887},{\"attributes\":{\"id\":\"fig_14\"},\"end\":84338,\"start\":84190},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":85594,\"start\":84339},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":85674,\"start\":85595},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":85807,\"start\":85675},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":85910,\"start\":85808},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":86107,\"start\":85911}]", "paragraph": "[{\"end\":2748,\"start\":1888},{\"end\":2938,\"start\":2750},{\"end\":4205,\"start\":2940},{\"end\":5277,\"start\":4207},{\"end\":5692,\"start\":5279},{\"end\":6766,\"start\":5694},{\"end\":7914,\"start\":6783},{\"end\":8666,\"start\":7916},{\"end\":10332,\"start\":8677},{\"end\":10877,\"start\":10358},{\"end\":10973,\"start\":10879},{\"end\":11231,\"start\":11026},{\"end\":11552,\"start\":11233},{\"end\":12074,\"start\":11576},{\"end\":12362,\"start\":12116},{\"end\":12649,\"start\":12396},{\"end\":14401,\"start\":12651},{\"end\":15378,\"start\":14510},{\"end\":17242,\"start\":15380},{\"end\":17977,\"start\":17296},{\"end\":18919,\"start\":17979},{\"end\":19734,\"start\":18921},{\"end\":20358,\"start\":19759},{\"end\":21177,\"start\":20360},{\"end\":22534,\"start\":21179},{\"end\":23444,\"start\":22577},{\"end\":23943,\"start\":23520},{\"end\":24379,\"start\":23955},{\"end\":25633,\"start\":24406},{\"end\":26783,\"start\":25635},{\"end\":27910,\"start\":26785},{\"end\":28605,\"start\":27979},{\"end\":29428,\"start\":28607},{\"end\":30415,\"start\":29430},{\"end\":34115,\"start\":30417},{\"end\":34962,\"start\":34155},{\"end\":35167,\"start\":34975},{\"end\":36559,\"start\":35240},{\"end\":37522,\"start\":36591},{\"end\":38853,\"start\":37524},{\"end\":39330,\"start\":38890},{\"end\":39678,\"start\":39332},{\"end\":40302,\"start\":39680},{\"end\":41933,\"start\":40304},{\"end\":42361,\"start\":41935},{\"end\":42832,\"start\":42363},{\"end\":43669,\"start\":42834},{\"end\":44290,\"start\":43671},{\"end\":44830,\"start\":44292},{\"end\":45268,\"start\":44832},{\"end\":45728,\"start\":45289},{\"end\":45993,\"start\":45730},{\"end\":46445,\"start\":45995},{\"end\":46539,\"start\":46468},{\"end\":47039,\"start\":46541},{\"end\":47567,\"start\":47041},{\"end\":48608,\"start\":47589},{\"end\":48982,\"start\":48640},{\"end\":51886,\"start\":48984},{\"end\":53113,\"start\":51888},{\"end\":54715,\"start\":53115},{\"end\":55017,\"start\":54810},{\"end\":55409,\"start\":55019},{\"end\":56508,\"start\":55411},{\"end\":57614,\"start\":56510},{\"end\":58467,\"start\":57616},{\"end\":59127,\"start\":58469},{\"end\":60347,\"start\":59206},{\"end\":62183,\"start\":60349},{\"end\":63032,\"start\":62185},{\"end\":65176,\"start\":63034},{\"end\":66127,\"start\":65178},{\"end\":66690,\"start\":66163},{\"end\":67280,\"start\":66692},{\"end\":68621,\"start\":67282},{\"end\":69299,\"start\":68623},{\"end\":69401,\"start\":69320},{\"end\":69614,\"start\":69403},{\"end\":70338,\"start\":69616},{\"end\":71985,\"start\":70340},{\"end\":72309,\"start\":71987},{\"end\":72529,\"start\":72311},{\"end\":73415,\"start\":72554},{\"end\":73889,\"start\":73463},{\"end\":74193,\"start\":73924},{\"end\":75748,\"start\":74195},{\"end\":76228,\"start\":75750},{\"end\":76394,\"start\":76230},{\"end\":77143,\"start\":76396},{\"end\":77749,\"start\":77145},{\"end\":78059,\"start\":77751},{\"end\":78518,\"start\":78079},{\"end\":79762,\"start\":78520},{\"end\":80569,\"start\":79764},{\"end\":81117,\"start\":80591}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11025,\"start\":10974},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11575,\"start\":11553},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12115,\"start\":12075},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12395,\"start\":12363},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14509,\"start\":14402},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17295,\"start\":17243},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23472,\"start\":23445},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23519,\"start\":23472}]", "table_ref": "[{\"end\":30270,\"start\":30267},{\"end\":30278,\"start\":30271},{\"end\":33682,\"start\":33675},{\"end\":72407,\"start\":72400},{\"end\":74889,\"start\":74882},{\"end\":75436,\"start\":75429}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1886,\"start\":1874},{\"attributes\":{\"n\":\"2\"},\"end\":6781,\"start\":6769},{\"attributes\":{\"n\":\"3\"},\"end\":8675,\"start\":8669},{\"attributes\":{\"n\":\"3.1\"},\"end\":10356,\"start\":10335},{\"attributes\":{\"n\":\"3.2\"},\"end\":19757,\"start\":19737},{\"attributes\":{\"n\":\"3.3\"},\"end\":22575,\"start\":22537},{\"attributes\":{\"n\":\"4\"},\"end\":23953,\"start\":23946},{\"attributes\":{\"n\":\"4.1\"},\"end\":24404,\"start\":24382},{\"end\":27977,\"start\":27913},{\"attributes\":{\"n\":\"4.2\"},\"end\":34140,\"start\":34118},{\"attributes\":{\"n\":\"5\"},\"end\":34153,\"start\":34143},{\"attributes\":{\"n\":\"6\"},\"end\":34973,\"start\":34965},{\"attributes\":{\"n\":\"6.1\"},\"end\":35238,\"start\":35170},{\"attributes\":{\"n\":\"6.2\"},\"end\":36589,\"start\":36562},{\"attributes\":{\"n\":\"6.3\"},\"end\":38888,\"start\":38856},{\"attributes\":{\"n\":\"6.4\"},\"end\":45287,\"start\":45271},{\"attributes\":{\"n\":\"6.5\"},\"end\":46466,\"start\":46448},{\"attributes\":{\"n\":\"6.6\"},\"end\":47587,\"start\":47570},{\"attributes\":{\"n\":\"6.7\"},\"end\":48638,\"start\":48611},{\"end\":54740,\"start\":54718},{\"attributes\":{\"n\":\"6.8\"},\"end\":54808,\"start\":54743},{\"attributes\":{\"n\":\"6.9\"},\"end\":59204,\"start\":59130},{\"attributes\":{\"n\":\"6.10\"},\"end\":66161,\"start\":66130},{\"attributes\":{\"n\":\"6.11\"},\"end\":69318,\"start\":69302},{\"attributes\":{\"n\":\"6.12\"},\"end\":72552,\"start\":72532},{\"attributes\":{\"n\":\"6.13\"},\"end\":73461,\"start\":73418},{\"attributes\":{\"n\":\"6.14\"},\"end\":73922,\"start\":73892},{\"end\":78077,\"start\":78062},{\"end\":80589,\"start\":80572},{\"end\":81129,\"start\":81119},{\"end\":81745,\"start\":81735},{\"end\":81957,\"start\":81947},{\"end\":82581,\"start\":82571},{\"end\":83030,\"start\":83020},{\"end\":83224,\"start\":83214},{\"end\":83781,\"start\":83770},{\"end\":83899,\"start\":83888},{\"end\":84202,\"start\":84191},{\"end\":85605,\"start\":85596},{\"end\":85685,\"start\":85676},{\"end\":85818,\"start\":85809}]", "table": "[{\"end\":85594,\"start\":85585},{\"end\":86107,\"start\":86005}]", "figure_caption": "[{\"end\":81733,\"start\":81131},{\"end\":81945,\"start\":81747},{\"end\":82569,\"start\":81959},{\"end\":82802,\"start\":82583},{\"end\":83018,\"start\":82805},{\"end\":83097,\"start\":83032},{\"end\":83212,\"start\":83100},{\"end\":83768,\"start\":83226},{\"end\":83886,\"start\":83784},{\"end\":84189,\"start\":83902},{\"end\":84338,\"start\":84205},{\"end\":85585,\"start\":84341},{\"end\":85674,\"start\":85607},{\"end\":85807,\"start\":85687},{\"end\":85910,\"start\":85820},{\"end\":86005,\"start\":85913}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13670,\"start\":13661},{\"end\":15108,\"start\":15100},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15574,\"start\":15568},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18086,\"start\":18079},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19278,\"start\":19269},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19359,\"start\":19350},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22687,\"start\":22682},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24507,\"start\":24501},{\"end\":24553,\"start\":24546},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":28888,\"start\":28882},{\"end\":30109,\"start\":30101},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30544,\"start\":30538},{\"end\":31011,\"start\":31003},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32161,\"start\":32155},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32883,\"start\":32876},{\"end\":38565,\"start\":38557},{\"end\":38930,\"start\":38924},{\"end\":40936,\"start\":40929},{\"end\":41274,\"start\":41266},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":66311,\"start\":66305},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70770,\"start\":70761},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":73007,\"start\":73000},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":73647,\"start\":73639},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":74192,\"start\":74185},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":76264,\"start\":76255},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":76451,\"start\":76444},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78234,\"start\":78227},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79058,\"start\":79051},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":80277,\"start\":80270}]", "bib_author_first_name": "[{\"end\":86582,\"start\":86581},{\"end\":86592,\"start\":86591},{\"end\":86600,\"start\":86599},{\"end\":86611,\"start\":86610},{\"end\":86903,\"start\":86902},{\"end\":86918,\"start\":86917},{\"end\":86927,\"start\":86926},{\"end\":86937,\"start\":86936},{\"end\":86946,\"start\":86945},{\"end\":86956,\"start\":86955},{\"end\":86964,\"start\":86963},{\"end\":86975,\"start\":86974},{\"end\":86984,\"start\":86983},{\"end\":87000,\"start\":86999},{\"end\":87373,\"start\":87372},{\"end\":87385,\"start\":87384},{\"end\":87392,\"start\":87391},{\"end\":87404,\"start\":87403},{\"end\":87716,\"start\":87715},{\"end\":87718,\"start\":87717},{\"end\":87730,\"start\":87729},{\"end\":87739,\"start\":87738},{\"end\":87750,\"start\":87749},{\"end\":87762,\"start\":87761},{\"end\":87774,\"start\":87773},{\"end\":87787,\"start\":87786},{\"end\":88037,\"start\":88036},{\"end\":88050,\"start\":88049},{\"end\":88062,\"start\":88061},{\"end\":88072,\"start\":88071},{\"end\":88357,\"start\":88356},{\"end\":88364,\"start\":88363},{\"end\":88380,\"start\":88379},{\"end\":88392,\"start\":88391},{\"end\":88752,\"start\":88751},{\"end\":88760,\"start\":88759},{\"end\":88768,\"start\":88767},{\"end\":88779,\"start\":88778},{\"end\":89163,\"start\":89162},{\"end\":89171,\"start\":89170},{\"end\":89180,\"start\":89179},{\"end\":89188,\"start\":89187},{\"end\":89574,\"start\":89573},{\"end\":89576,\"start\":89575},{\"end\":89588,\"start\":89587},{\"end\":89600,\"start\":89599},{\"end\":89892,\"start\":89891},{\"end\":89904,\"start\":89903},{\"end\":89916,\"start\":89915},{\"end\":90225,\"start\":90224},{\"end\":90239,\"start\":90238},{\"end\":90250,\"start\":90249},{\"end\":90265,\"start\":90264},{\"end\":90280,\"start\":90279},{\"end\":90290,\"start\":90289},{\"end\":90299,\"start\":90298},{\"end\":90301,\"start\":90300},{\"end\":90313,\"start\":90312},{\"end\":90775,\"start\":90774},{\"end\":90782,\"start\":90781},{\"end\":91141,\"start\":91140},{\"end\":91151,\"start\":91150},{\"end\":91407,\"start\":91406},{\"end\":91416,\"start\":91415},{\"end\":91738,\"start\":91737},{\"end\":91747,\"start\":91746},{\"end\":91757,\"start\":91756},{\"end\":91995,\"start\":91994},{\"end\":92007,\"start\":92006},{\"end\":92017,\"start\":92016},{\"end\":92031,\"start\":92030},{\"end\":92044,\"start\":92043},{\"end\":92056,\"start\":92055},{\"end\":92064,\"start\":92063},{\"end\":92348,\"start\":92347},{\"end\":92356,\"start\":92355},{\"end\":92358,\"start\":92357},{\"end\":92369,\"start\":92368},{\"end\":92371,\"start\":92370},{\"end\":92715,\"start\":92714},{\"end\":92722,\"start\":92721},{\"end\":92724,\"start\":92723},{\"end\":92736,\"start\":92735},{\"end\":92744,\"start\":92743},{\"end\":92758,\"start\":92757},{\"end\":92767,\"start\":92766},{\"end\":92779,\"start\":92778},{\"end\":92788,\"start\":92787},{\"end\":93079,\"start\":93078},{\"end\":93090,\"start\":93086},{\"end\":93096,\"start\":93095},{\"end\":93104,\"start\":93103},{\"end\":93111,\"start\":93110},{\"end\":93121,\"start\":93120},{\"end\":93134,\"start\":93133},{\"end\":93513,\"start\":93512},{\"end\":93521,\"start\":93520},{\"end\":93530,\"start\":93529},{\"end\":93543,\"start\":93542},{\"end\":93553,\"start\":93552},{\"end\":93562,\"start\":93561},{\"end\":93932,\"start\":93931},{\"end\":93940,\"start\":93939},{\"end\":93951,\"start\":93950},{\"end\":93960,\"start\":93959},{\"end\":93972,\"start\":93971},{\"end\":93980,\"start\":93979},{\"end\":93982,\"start\":93981},{\"end\":94326,\"start\":94325},{\"end\":94338,\"start\":94337},{\"end\":94340,\"start\":94339},{\"end\":94350,\"start\":94349},{\"end\":94594,\"start\":94593},{\"end\":94607,\"start\":94606},{\"end\":94614,\"start\":94613},{\"end\":94925,\"start\":94924},{\"end\":94934,\"start\":94933},{\"end\":94942,\"start\":94941},{\"end\":94949,\"start\":94948},{\"end\":95317,\"start\":95316},{\"end\":95326,\"start\":95325},{\"end\":95337,\"start\":95336},{\"end\":95347,\"start\":95346},{\"end\":95353,\"start\":95352},{\"end\":95361,\"start\":95360},{\"end\":95371,\"start\":95370},{\"end\":95373,\"start\":95372},{\"end\":95734,\"start\":95733},{\"end\":95747,\"start\":95746},{\"end\":95758,\"start\":95757},{\"end\":96047,\"start\":96046},{\"end\":96056,\"start\":96055},{\"end\":96066,\"start\":96065},{\"end\":96081,\"start\":96080},{\"end\":96093,\"start\":96092},{\"end\":96353,\"start\":96352},{\"end\":96362,\"start\":96361},{\"end\":96376,\"start\":96375},{\"end\":96388,\"start\":96387},{\"end\":96400,\"start\":96399},{\"end\":96704,\"start\":96703},{\"end\":96716,\"start\":96715},{\"end\":96727,\"start\":96726},{\"end\":96734,\"start\":96733},{\"end\":96745,\"start\":96744},{\"end\":97152,\"start\":97151},{\"end\":97160,\"start\":97159},{\"end\":97168,\"start\":97167},{\"end\":97175,\"start\":97174},{\"end\":97534,\"start\":97533},{\"end\":97542,\"start\":97541},{\"end\":97553,\"start\":97549},{\"end\":97559,\"start\":97558},{\"end\":97569,\"start\":97568},{\"end\":97571,\"start\":97570},{\"end\":97580,\"start\":97579},{\"end\":97589,\"start\":97588},{\"end\":97595,\"start\":97594},{\"end\":97605,\"start\":97604},{\"end\":97612,\"start\":97611},{\"end\":98055,\"start\":98054},{\"end\":98067,\"start\":98066},{\"end\":98076,\"start\":98075},{\"end\":98394,\"start\":98393},{\"end\":98402,\"start\":98401},{\"end\":98410,\"start\":98409},{\"end\":98420,\"start\":98419},{\"end\":98427,\"start\":98426},{\"end\":98440,\"start\":98439},{\"end\":98837,\"start\":98836},{\"end\":98845,\"start\":98844},{\"end\":98858,\"start\":98857},{\"end\":99159,\"start\":99158},{\"end\":99368,\"start\":99367},{\"end\":99374,\"start\":99373},{\"end\":99380,\"start\":99379},{\"end\":99388,\"start\":99387},{\"end\":99397,\"start\":99396},{\"end\":99405,\"start\":99404},{\"end\":99656,\"start\":99655},{\"end\":99670,\"start\":99669},{\"end\":99682,\"start\":99681},{\"end\":99684,\"start\":99683},{\"end\":99697,\"start\":99696},{\"end\":99708,\"start\":99707},{\"end\":99710,\"start\":99709},{\"end\":100046,\"start\":100045},{\"end\":100053,\"start\":100052},{\"end\":100063,\"start\":100062},{\"end\":100065,\"start\":100064},{\"end\":100380,\"start\":100379},{\"end\":100387,\"start\":100386},{\"end\":100393,\"start\":100392},{\"end\":100404,\"start\":100403},{\"end\":100769,\"start\":100768},{\"end\":100778,\"start\":100777},{\"end\":100790,\"start\":100789},{\"end\":100801,\"start\":100800},{\"end\":100812,\"start\":100811},{\"end\":100825,\"start\":100824},{\"end\":100837,\"start\":100836},{\"end\":101151,\"start\":101150},{\"end\":101157,\"start\":101156},{\"end\":101166,\"start\":101165},{\"end\":101173,\"start\":101172},{\"end\":101404,\"start\":101403},{\"end\":101410,\"start\":101409},{\"end\":101670,\"start\":101669},{\"end\":101678,\"start\":101677},{\"end\":101680,\"start\":101679},{\"end\":102044,\"start\":102043},{\"end\":102055,\"start\":102054},{\"end\":102247,\"start\":102246},{\"end\":102249,\"start\":102248},{\"end\":102551,\"start\":102550},{\"end\":102562,\"start\":102561},{\"end\":102574,\"start\":102573},{\"end\":102822,\"start\":102821},{\"end\":102834,\"start\":102833},{\"end\":102836,\"start\":102835},{\"end\":102847,\"start\":102846},{\"end\":103100,\"start\":103099},{\"end\":103111,\"start\":103110},{\"end\":103118,\"start\":103117},{\"end\":103125,\"start\":103124},{\"end\":103137,\"start\":103136},{\"end\":103145,\"start\":103144},{\"end\":103157,\"start\":103156},{\"end\":103166,\"start\":103165},{\"end\":103179,\"start\":103178},{\"end\":103584,\"start\":103583},{\"end\":103600,\"start\":103599},{\"end\":103615,\"start\":103614},{\"end\":103625,\"start\":103624},{\"end\":103813,\"start\":103812},{\"end\":103827,\"start\":103826},{\"end\":103829,\"start\":103828},{\"end\":103843,\"start\":103842},{\"end\":103853,\"start\":103852},{\"end\":103855,\"start\":103854},{\"end\":103865,\"start\":103864},{\"end\":103880,\"start\":103879},{\"end\":104288,\"start\":104287},{\"end\":104300,\"start\":104299},{\"end\":104302,\"start\":104301},{\"end\":104312,\"start\":104311},{\"end\":104314,\"start\":104313},{\"end\":104325,\"start\":104324},{\"end\":104327,\"start\":104326},{\"end\":104338,\"start\":104337},{\"end\":105002,\"start\":105001},{\"end\":105004,\"start\":105003},{\"end\":105014,\"start\":105013}]", "bib_author_last_name": "[{\"end\":86589,\"start\":86583},{\"end\":86597,\"start\":86593},{\"end\":86608,\"start\":86601},{\"end\":86618,\"start\":86612},{\"end\":86915,\"start\":86904},{\"end\":86924,\"start\":86919},{\"end\":86934,\"start\":86928},{\"end\":86943,\"start\":86938},{\"end\":86953,\"start\":86947},{\"end\":86961,\"start\":86957},{\"end\":86972,\"start\":86965},{\"end\":86981,\"start\":86976},{\"end\":86997,\"start\":86985},{\"end\":87010,\"start\":87001},{\"end\":87382,\"start\":87374},{\"end\":87389,\"start\":87386},{\"end\":87401,\"start\":87393},{\"end\":87412,\"start\":87405},{\"end\":87727,\"start\":87719},{\"end\":87736,\"start\":87731},{\"end\":87747,\"start\":87740},{\"end\":87759,\"start\":87751},{\"end\":87771,\"start\":87763},{\"end\":87784,\"start\":87775},{\"end\":87792,\"start\":87788},{\"end\":88047,\"start\":88038},{\"end\":88059,\"start\":88051},{\"end\":88069,\"start\":88063},{\"end\":88078,\"start\":88073},{\"end\":88361,\"start\":88358},{\"end\":88377,\"start\":88365},{\"end\":88389,\"start\":88381},{\"end\":88401,\"start\":88393},{\"end\":88757,\"start\":88753},{\"end\":88765,\"start\":88761},{\"end\":88776,\"start\":88769},{\"end\":88787,\"start\":88780},{\"end\":89168,\"start\":89164},{\"end\":89177,\"start\":89172},{\"end\":89185,\"start\":89181},{\"end\":89196,\"start\":89189},{\"end\":89585,\"start\":89577},{\"end\":89597,\"start\":89589},{\"end\":89608,\"start\":89601},{\"end\":89901,\"start\":89893},{\"end\":89913,\"start\":89905},{\"end\":89924,\"start\":89917},{\"end\":90236,\"start\":90226},{\"end\":90247,\"start\":90240},{\"end\":90262,\"start\":90251},{\"end\":90277,\"start\":90266},{\"end\":90287,\"start\":90281},{\"end\":90296,\"start\":90291},{\"end\":90310,\"start\":90302},{\"end\":90322,\"start\":90314},{\"end\":90779,\"start\":90776},{\"end\":90790,\"start\":90783},{\"end\":91148,\"start\":91142},{\"end\":91159,\"start\":91152},{\"end\":91413,\"start\":91408},{\"end\":91424,\"start\":91417},{\"end\":91744,\"start\":91739},{\"end\":91754,\"start\":91748},{\"end\":91767,\"start\":91758},{\"end\":92004,\"start\":91996},{\"end\":92014,\"start\":92008},{\"end\":92028,\"start\":92018},{\"end\":92041,\"start\":92032},{\"end\":92053,\"start\":92045},{\"end\":92061,\"start\":92057},{\"end\":92072,\"start\":92065},{\"end\":92353,\"start\":92349},{\"end\":92366,\"start\":92359},{\"end\":92375,\"start\":92372},{\"end\":92719,\"start\":92716},{\"end\":92733,\"start\":92725},{\"end\":92741,\"start\":92737},{\"end\":92755,\"start\":92745},{\"end\":92764,\"start\":92759},{\"end\":92776,\"start\":92768},{\"end\":92785,\"start\":92780},{\"end\":92799,\"start\":92789},{\"end\":93084,\"start\":93080},{\"end\":93093,\"start\":93091},{\"end\":93101,\"start\":93097},{\"end\":93108,\"start\":93105},{\"end\":93118,\"start\":93112},{\"end\":93131,\"start\":93122},{\"end\":93139,\"start\":93135},{\"end\":93518,\"start\":93514},{\"end\":93527,\"start\":93522},{\"end\":93540,\"start\":93531},{\"end\":93550,\"start\":93544},{\"end\":93559,\"start\":93554},{\"end\":93566,\"start\":93563},{\"end\":93937,\"start\":93933},{\"end\":93948,\"start\":93941},{\"end\":93957,\"start\":93952},{\"end\":93969,\"start\":93961},{\"end\":93977,\"start\":93973},{\"end\":93989,\"start\":93983},{\"end\":94335,\"start\":94327},{\"end\":94347,\"start\":94341},{\"end\":94356,\"start\":94351},{\"end\":94604,\"start\":94595},{\"end\":94611,\"start\":94608},{\"end\":94620,\"start\":94615},{\"end\":94931,\"start\":94926},{\"end\":94939,\"start\":94935},{\"end\":94946,\"start\":94943},{\"end\":94954,\"start\":94950},{\"end\":95323,\"start\":95318},{\"end\":95334,\"start\":95327},{\"end\":95344,\"start\":95338},{\"end\":95350,\"start\":95348},{\"end\":95358,\"start\":95354},{\"end\":95368,\"start\":95362},{\"end\":95379,\"start\":95374},{\"end\":95744,\"start\":95735},{\"end\":95755,\"start\":95748},{\"end\":95765,\"start\":95759},{\"end\":96050,\"start\":96048},{\"end\":96060,\"start\":96057},{\"end\":96075,\"start\":96067},{\"end\":96087,\"start\":96082},{\"end\":96100,\"start\":96094},{\"end\":96359,\"start\":96354},{\"end\":96373,\"start\":96363},{\"end\":96385,\"start\":96377},{\"end\":96397,\"start\":96389},{\"end\":96413,\"start\":96401},{\"end\":96713,\"start\":96705},{\"end\":96724,\"start\":96717},{\"end\":96731,\"start\":96728},{\"end\":96742,\"start\":96735},{\"end\":96758,\"start\":96746},{\"end\":97157,\"start\":97153},{\"end\":97165,\"start\":97161},{\"end\":97172,\"start\":97169},{\"end\":97186,\"start\":97176},{\"end\":97539,\"start\":97535},{\"end\":97547,\"start\":97543},{\"end\":97556,\"start\":97554},{\"end\":97566,\"start\":97560},{\"end\":97577,\"start\":97572},{\"end\":97586,\"start\":97581},{\"end\":97592,\"start\":97590},{\"end\":97602,\"start\":97596},{\"end\":97609,\"start\":97606},{\"end\":97617,\"start\":97613},{\"end\":98064,\"start\":98056},{\"end\":98073,\"start\":98068},{\"end\":98084,\"start\":98077},{\"end\":98399,\"start\":98395},{\"end\":98407,\"start\":98403},{\"end\":98417,\"start\":98411},{\"end\":98424,\"start\":98421},{\"end\":98437,\"start\":98428},{\"end\":98451,\"start\":98441},{\"end\":98842,\"start\":98838},{\"end\":98855,\"start\":98846},{\"end\":98866,\"start\":98859},{\"end\":99164,\"start\":99160},{\"end\":99371,\"start\":99369},{\"end\":99377,\"start\":99375},{\"end\":99385,\"start\":99381},{\"end\":99394,\"start\":99389},{\"end\":99402,\"start\":99398},{\"end\":99409,\"start\":99406},{\"end\":99667,\"start\":99657},{\"end\":99679,\"start\":99671},{\"end\":99694,\"start\":99685},{\"end\":99705,\"start\":99698},{\"end\":99715,\"start\":99711},{\"end\":100050,\"start\":100047},{\"end\":100060,\"start\":100054},{\"end\":100075,\"start\":100066},{\"end\":100384,\"start\":100381},{\"end\":100390,\"start\":100388},{\"end\":100401,\"start\":100394},{\"end\":100410,\"start\":100405},{\"end\":100775,\"start\":100770},{\"end\":100787,\"start\":100779},{\"end\":100798,\"start\":100791},{\"end\":100809,\"start\":100802},{\"end\":100822,\"start\":100813},{\"end\":100834,\"start\":100826},{\"end\":100842,\"start\":100838},{\"end\":101154,\"start\":101152},{\"end\":101163,\"start\":101158},{\"end\":101170,\"start\":101167},{\"end\":101177,\"start\":101174},{\"end\":101407,\"start\":101405},{\"end\":101417,\"start\":101411},{\"end\":101675,\"start\":101671},{\"end\":101687,\"start\":101681},{\"end\":102052,\"start\":102045},{\"end\":102059,\"start\":102056},{\"end\":102256,\"start\":102250},{\"end\":102559,\"start\":102552},{\"end\":102571,\"start\":102563},{\"end\":102578,\"start\":102575},{\"end\":102831,\"start\":102823},{\"end\":102844,\"start\":102837},{\"end\":102853,\"start\":102848},{\"end\":103108,\"start\":103101},{\"end\":103115,\"start\":103112},{\"end\":103122,\"start\":103119},{\"end\":103134,\"start\":103126},{\"end\":103142,\"start\":103138},{\"end\":103154,\"start\":103146},{\"end\":103163,\"start\":103158},{\"end\":103176,\"start\":103167},{\"end\":103190,\"start\":103180},{\"end\":103597,\"start\":103585},{\"end\":103612,\"start\":103601},{\"end\":103622,\"start\":103616},{\"end\":103632,\"start\":103626},{\"end\":103824,\"start\":103814},{\"end\":103840,\"start\":103830},{\"end\":103850,\"start\":103844},{\"end\":103862,\"start\":103856},{\"end\":103877,\"start\":103866},{\"end\":103883,\"start\":103881},{\"end\":104297,\"start\":104289},{\"end\":104309,\"start\":104303},{\"end\":104322,\"start\":104315},{\"end\":104335,\"start\":104328},{\"end\":104348,\"start\":104339},{\"end\":105011,\"start\":105005},{\"end\":105017,\"start\":105015}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7242892},\"end\":86816,\"start\":86532},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":49470584},\"end\":87302,\"start\":86818},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":80628296},\"end\":87643,\"start\":87304},{\"attributes\":{\"id\":\"b3\"},\"end\":87976,\"start\":87645},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":202750286},\"end\":88273,\"start\":87978},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":208637275},\"end\":88682,\"start\":88275},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5682214},\"end\":89059,\"start\":88684},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3849025},\"end\":89478,\"start\":89061},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":49394699},\"end\":89827,\"start\":89480},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202578098},\"end\":90122,\"start\":89829},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":211987382},\"end\":90676,\"start\":90124},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211252822},\"end\":91032,\"start\":90678},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":27366242},\"end\":91404,\"start\":91034},{\"attributes\":{\"id\":\"b13\"},\"end\":91646,\"start\":91406},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":49427027},\"end\":91990,\"start\":91648},{\"attributes\":{\"doi\":\"arXiv:1606.01540\",\"id\":\"b15\"},\"end\":92257,\"start\":91992},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5737018},\"end\":92639,\"start\":92259},{\"attributes\":{\"id\":\"b17\"},\"end\":92979,\"start\":92641},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1089045},\"end\":93444,\"start\":92981},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":202718847},\"end\":93839,\"start\":93446},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":57761160},\"end\":94253,\"start\":93841},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10767748},\"end\":94543,\"start\":94255},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3540936},\"end\":94845,\"start\":94545},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204961384},\"end\":95212,\"start\":94847},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":213270502},\"end\":95657,\"start\":95214},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":219687507},\"end\":95978,\"start\":95659},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204950125},\"end\":96301,\"start\":95980},{\"attributes\":{\"doi\":\"arXiv:2008.11466\",\"id\":\"b27\"},\"end\":96604,\"start\":96303},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":218629964},\"end\":97062,\"start\":96606},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":209140715},\"end\":97416,\"start\":97064},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4548237},\"end\":97963,\"start\":97418},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4891707},\"end\":98293,\"start\":97965},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4570620},\"end\":98778,\"start\":98295},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1629541},\"end\":99102,\"start\":98780},{\"attributes\":{\"id\":\"b34\"},\"end\":99292,\"start\":99104},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":57189581},\"end\":99653,\"start\":99294},{\"attributes\":{\"id\":\"b36\"},\"end\":100005,\"start\":99655},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1188600},\"end\":100294,\"start\":100007},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5665033},\"end\":100663,\"start\":100296},{\"attributes\":{\"doi\":\"arXiv:2012.03385\",\"id\":\"b39\"},\"end\":101102,\"start\":100665},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206594692},\"end\":101401,\"start\":101104},{\"attributes\":{\"id\":\"b41\"},\"end\":101667,\"start\":101403},{\"attributes\":{\"id\":\"b42\"},\"end\":101950,\"start\":101669},{\"attributes\":{\"id\":\"b43\"},\"end\":102218,\"start\":101952},{\"attributes\":{\"id\":\"b44\"},\"end\":102337,\"start\":102220},{\"attributes\":{\"id\":\"b45\"},\"end\":102479,\"start\":102339},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":2842187},\"end\":102778,\"start\":102481},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6600800},\"end\":103065,\"start\":102780},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206592484},\"end\":103469,\"start\":103067},{\"attributes\":{\"id\":\"b49\"},\"end\":103810,\"start\":103471},{\"attributes\":{\"id\":\"b50\"},\"end\":104217,\"start\":103812},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":219720931},\"end\":104624,\"start\":104219},{\"attributes\":{\"id\":\"b52\"},\"end\":106033,\"start\":104626}]", "bib_title": "[{\"end\":86579,\"start\":86532},{\"end\":86900,\"start\":86818},{\"end\":87370,\"start\":87304},{\"end\":88034,\"start\":87978},{\"end\":88354,\"start\":88275},{\"end\":88749,\"start\":88684},{\"end\":89160,\"start\":89061},{\"end\":89571,\"start\":89480},{\"end\":89889,\"start\":89829},{\"end\":90222,\"start\":90124},{\"end\":90772,\"start\":90678},{\"end\":91138,\"start\":91034},{\"end\":91735,\"start\":91648},{\"end\":92345,\"start\":92259},{\"end\":93076,\"start\":92981},{\"end\":93510,\"start\":93446},{\"end\":93929,\"start\":93841},{\"end\":94323,\"start\":94255},{\"end\":94591,\"start\":94545},{\"end\":94922,\"start\":94847},{\"end\":95314,\"start\":95214},{\"end\":95731,\"start\":95659},{\"end\":96044,\"start\":95980},{\"end\":96701,\"start\":96606},{\"end\":97149,\"start\":97064},{\"end\":97531,\"start\":97418},{\"end\":98052,\"start\":97965},{\"end\":98391,\"start\":98295},{\"end\":98834,\"start\":98780},{\"end\":99365,\"start\":99294},{\"end\":100043,\"start\":100007},{\"end\":100377,\"start\":100296},{\"end\":101148,\"start\":101104},{\"end\":102041,\"start\":101952},{\"end\":102548,\"start\":102481},{\"end\":102819,\"start\":102780},{\"end\":103097,\"start\":103067},{\"end\":104285,\"start\":104219},{\"end\":104999,\"start\":104626}]", "bib_author": "[{\"end\":86591,\"start\":86581},{\"end\":86599,\"start\":86591},{\"end\":86610,\"start\":86599},{\"end\":86620,\"start\":86610},{\"end\":86917,\"start\":86902},{\"end\":86926,\"start\":86917},{\"end\":86936,\"start\":86926},{\"end\":86945,\"start\":86936},{\"end\":86955,\"start\":86945},{\"end\":86963,\"start\":86955},{\"end\":86974,\"start\":86963},{\"end\":86983,\"start\":86974},{\"end\":86999,\"start\":86983},{\"end\":87012,\"start\":86999},{\"end\":87384,\"start\":87372},{\"end\":87391,\"start\":87384},{\"end\":87403,\"start\":87391},{\"end\":87414,\"start\":87403},{\"end\":87729,\"start\":87715},{\"end\":87738,\"start\":87729},{\"end\":87749,\"start\":87738},{\"end\":87761,\"start\":87749},{\"end\":87773,\"start\":87761},{\"end\":87786,\"start\":87773},{\"end\":87794,\"start\":87786},{\"end\":88049,\"start\":88036},{\"end\":88061,\"start\":88049},{\"end\":88071,\"start\":88061},{\"end\":88080,\"start\":88071},{\"end\":88363,\"start\":88356},{\"end\":88379,\"start\":88363},{\"end\":88391,\"start\":88379},{\"end\":88403,\"start\":88391},{\"end\":88759,\"start\":88751},{\"end\":88767,\"start\":88759},{\"end\":88778,\"start\":88767},{\"end\":88789,\"start\":88778},{\"end\":89170,\"start\":89162},{\"end\":89179,\"start\":89170},{\"end\":89187,\"start\":89179},{\"end\":89198,\"start\":89187},{\"end\":89587,\"start\":89573},{\"end\":89599,\"start\":89587},{\"end\":89610,\"start\":89599},{\"end\":89903,\"start\":89891},{\"end\":89915,\"start\":89903},{\"end\":89926,\"start\":89915},{\"end\":90238,\"start\":90224},{\"end\":90249,\"start\":90238},{\"end\":90264,\"start\":90249},{\"end\":90279,\"start\":90264},{\"end\":90289,\"start\":90279},{\"end\":90298,\"start\":90289},{\"end\":90312,\"start\":90298},{\"end\":90324,\"start\":90312},{\"end\":90781,\"start\":90774},{\"end\":90792,\"start\":90781},{\"end\":91150,\"start\":91140},{\"end\":91161,\"start\":91150},{\"end\":91415,\"start\":91406},{\"end\":91426,\"start\":91415},{\"end\":91746,\"start\":91737},{\"end\":91756,\"start\":91746},{\"end\":91769,\"start\":91756},{\"end\":92006,\"start\":91994},{\"end\":92016,\"start\":92006},{\"end\":92030,\"start\":92016},{\"end\":92043,\"start\":92030},{\"end\":92055,\"start\":92043},{\"end\":92063,\"start\":92055},{\"end\":92074,\"start\":92063},{\"end\":92355,\"start\":92347},{\"end\":92368,\"start\":92355},{\"end\":92377,\"start\":92368},{\"end\":92721,\"start\":92714},{\"end\":92735,\"start\":92721},{\"end\":92743,\"start\":92735},{\"end\":92757,\"start\":92743},{\"end\":92766,\"start\":92757},{\"end\":92778,\"start\":92766},{\"end\":92787,\"start\":92778},{\"end\":92801,\"start\":92787},{\"end\":93086,\"start\":93078},{\"end\":93095,\"start\":93086},{\"end\":93103,\"start\":93095},{\"end\":93110,\"start\":93103},{\"end\":93120,\"start\":93110},{\"end\":93133,\"start\":93120},{\"end\":93141,\"start\":93133},{\"end\":93520,\"start\":93512},{\"end\":93529,\"start\":93520},{\"end\":93542,\"start\":93529},{\"end\":93552,\"start\":93542},{\"end\":93561,\"start\":93552},{\"end\":93568,\"start\":93561},{\"end\":93939,\"start\":93931},{\"end\":93950,\"start\":93939},{\"end\":93959,\"start\":93950},{\"end\":93971,\"start\":93959},{\"end\":93979,\"start\":93971},{\"end\":93991,\"start\":93979},{\"end\":94337,\"start\":94325},{\"end\":94349,\"start\":94337},{\"end\":94358,\"start\":94349},{\"end\":94606,\"start\":94593},{\"end\":94613,\"start\":94606},{\"end\":94622,\"start\":94613},{\"end\":94933,\"start\":94924},{\"end\":94941,\"start\":94933},{\"end\":94948,\"start\":94941},{\"end\":94956,\"start\":94948},{\"end\":95325,\"start\":95316},{\"end\":95336,\"start\":95325},{\"end\":95346,\"start\":95336},{\"end\":95352,\"start\":95346},{\"end\":95360,\"start\":95352},{\"end\":95370,\"start\":95360},{\"end\":95381,\"start\":95370},{\"end\":95746,\"start\":95733},{\"end\":95757,\"start\":95746},{\"end\":95767,\"start\":95757},{\"end\":96055,\"start\":96046},{\"end\":96065,\"start\":96055},{\"end\":96080,\"start\":96065},{\"end\":96092,\"start\":96080},{\"end\":96105,\"start\":96092},{\"end\":96361,\"start\":96352},{\"end\":96375,\"start\":96361},{\"end\":96387,\"start\":96375},{\"end\":96399,\"start\":96387},{\"end\":96415,\"start\":96399},{\"end\":96715,\"start\":96703},{\"end\":96726,\"start\":96715},{\"end\":96733,\"start\":96726},{\"end\":96744,\"start\":96733},{\"end\":96760,\"start\":96744},{\"end\":97159,\"start\":97151},{\"end\":97167,\"start\":97159},{\"end\":97174,\"start\":97167},{\"end\":97188,\"start\":97174},{\"end\":97541,\"start\":97533},{\"end\":97549,\"start\":97541},{\"end\":97558,\"start\":97549},{\"end\":97568,\"start\":97558},{\"end\":97579,\"start\":97568},{\"end\":97588,\"start\":97579},{\"end\":97594,\"start\":97588},{\"end\":97604,\"start\":97594},{\"end\":97611,\"start\":97604},{\"end\":97619,\"start\":97611},{\"end\":98066,\"start\":98054},{\"end\":98075,\"start\":98066},{\"end\":98086,\"start\":98075},{\"end\":98401,\"start\":98393},{\"end\":98409,\"start\":98401},{\"end\":98419,\"start\":98409},{\"end\":98426,\"start\":98419},{\"end\":98439,\"start\":98426},{\"end\":98453,\"start\":98439},{\"end\":98844,\"start\":98836},{\"end\":98857,\"start\":98844},{\"end\":98868,\"start\":98857},{\"end\":99166,\"start\":99158},{\"end\":99373,\"start\":99367},{\"end\":99379,\"start\":99373},{\"end\":99387,\"start\":99379},{\"end\":99396,\"start\":99387},{\"end\":99404,\"start\":99396},{\"end\":99411,\"start\":99404},{\"end\":99669,\"start\":99655},{\"end\":99681,\"start\":99669},{\"end\":99696,\"start\":99681},{\"end\":99707,\"start\":99696},{\"end\":99717,\"start\":99707},{\"end\":100052,\"start\":100045},{\"end\":100062,\"start\":100052},{\"end\":100077,\"start\":100062},{\"end\":100386,\"start\":100379},{\"end\":100392,\"start\":100386},{\"end\":100403,\"start\":100392},{\"end\":100412,\"start\":100403},{\"end\":100777,\"start\":100768},{\"end\":100789,\"start\":100777},{\"end\":100800,\"start\":100789},{\"end\":100811,\"start\":100800},{\"end\":100824,\"start\":100811},{\"end\":100836,\"start\":100824},{\"end\":100844,\"start\":100836},{\"end\":101156,\"start\":101150},{\"end\":101165,\"start\":101156},{\"end\":101172,\"start\":101165},{\"end\":101179,\"start\":101172},{\"end\":101409,\"start\":101403},{\"end\":101419,\"start\":101409},{\"end\":101677,\"start\":101669},{\"end\":101689,\"start\":101677},{\"end\":102054,\"start\":102043},{\"end\":102061,\"start\":102054},{\"end\":102258,\"start\":102246},{\"end\":102561,\"start\":102550},{\"end\":102573,\"start\":102561},{\"end\":102580,\"start\":102573},{\"end\":102833,\"start\":102821},{\"end\":102846,\"start\":102833},{\"end\":102855,\"start\":102846},{\"end\":103110,\"start\":103099},{\"end\":103117,\"start\":103110},{\"end\":103124,\"start\":103117},{\"end\":103136,\"start\":103124},{\"end\":103144,\"start\":103136},{\"end\":103156,\"start\":103144},{\"end\":103165,\"start\":103156},{\"end\":103178,\"start\":103165},{\"end\":103192,\"start\":103178},{\"end\":103599,\"start\":103583},{\"end\":103614,\"start\":103599},{\"end\":103624,\"start\":103614},{\"end\":103634,\"start\":103624},{\"end\":103826,\"start\":103812},{\"end\":103842,\"start\":103826},{\"end\":103852,\"start\":103842},{\"end\":103864,\"start\":103852},{\"end\":103879,\"start\":103864},{\"end\":103885,\"start\":103879},{\"end\":104299,\"start\":104287},{\"end\":104311,\"start\":104299},{\"end\":104324,\"start\":104311},{\"end\":104337,\"start\":104324},{\"end\":104350,\"start\":104337},{\"end\":105013,\"start\":105001},{\"end\":105019,\"start\":105013}]", "bib_venue": "[{\"end\":86660,\"start\":86620},{\"end\":87047,\"start\":87012},{\"end\":87465,\"start\":87414},{\"end\":87713,\"start\":87645},{\"end\":88115,\"start\":88080},{\"end\":88468,\"start\":88403},{\"end\":88863,\"start\":88789},{\"end\":89261,\"start\":89198},{\"end\":89645,\"start\":89610},{\"end\":89962,\"start\":89926},{\"end\":90387,\"start\":90324},{\"end\":90846,\"start\":90792},{\"end\":91212,\"start\":91161},{\"end\":91519,\"start\":91426},{\"end\":91811,\"start\":91769},{\"end\":92440,\"start\":92377},{\"end\":92712,\"start\":92641},{\"end\":93197,\"start\":93141},{\"end\":93631,\"start\":93568},{\"end\":94037,\"start\":93991},{\"end\":94387,\"start\":94358},{\"end\":94685,\"start\":94622},{\"end\":95019,\"start\":94956},{\"end\":95417,\"start\":95381},{\"end\":95803,\"start\":95767},{\"end\":96134,\"start\":96105},{\"end\":96350,\"start\":96303},{\"end\":96823,\"start\":96760},{\"end\":97224,\"start\":97188},{\"end\":97677,\"start\":97619},{\"end\":98121,\"start\":98086},{\"end\":98527,\"start\":98453},{\"end\":98933,\"start\":98868},{\"end\":99156,\"start\":99104},{\"end\":99464,\"start\":99411},{\"end\":99820,\"start\":99717},{\"end\":100142,\"start\":100077},{\"end\":100471,\"start\":100412},{\"end\":100766,\"start\":100665},{\"end\":101244,\"start\":101179},{\"end\":101528,\"start\":101419},{\"end\":101802,\"start\":101689},{\"end\":102078,\"start\":102061},{\"end\":102244,\"start\":102220},{\"end\":102377,\"start\":102339},{\"end\":102616,\"start\":102580},{\"end\":102914,\"start\":102855},{\"end\":103257,\"start\":103192},{\"end\":103581,\"start\":103471},{\"end\":104002,\"start\":103885},{\"end\":104409,\"start\":104350},{\"end\":105078,\"start\":105019}]"}}}, "year": 2023, "month": 12, "day": 17}