{"id": 248366748, "updated": "2023-09-27 18:42:17.566", "metadata": {"title": "Towards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth", "authors": "[{\"first\":\"Guangkai\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Feng\",\"last\":\"Zhao\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Graphical abstract Our pipeline for dense 3D scene reconstruction is composed of a robust monocular depth estimation module, a metric depth recovery module", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.01470", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.52396/justc-2023-0061"}}, "content": {"source": {"pdf_hash": "16a0e4ddda243af129823a01ba33179274c726ab", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"https://export.arxiv.org/pdf/2202.01470v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://doi.org/10.52396/justc-2023-0061", "status": "HYBRID"}}, "grobid": {"id": "f18f1bb31e14f7ccc332a9c7e7c111d10c62c8b2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/16a0e4ddda243af129823a01ba33179274c726ab.txt", "contents": "\nTowards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth\n2023\n\nG K Xu \nF Zhao \n\nNational Engineering Laboratory for Brain-inspired Intelligence Technology and Application\nUniversity of Science and Technology of China\n230026HefeiChina\n\n\nNational Engineering Laboratory for Brain-inspired Intelligence Technology and Application\nUniversity of Science and Technology of China\n230026HefeiChina\n\nTowards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth\n\nJUSTC\n530202310.52396/JUSTC-2023-0061\u2709 Correspondence: Feng Zhao, J u s t A c c e p t e d \u2709 Correspondence: Feng Zhao, Cite This: JUSTC, 2023, 53(X): (9pp) Read Online Supporting Information3D scene reconstructionmonocular depth estimationlocally weighted linear regression CLC number: Document code: A\nGraphical abstractOur pipeline for dense 3D scene reconstruction is composed of a robust monocular depth estimation module, a metric depth recovery module, and an RGB-D fusion module. With our robust depth model trained on enormous data and the proposed locally weighted linear regression method, we can achieve robust and accurate 3D scene shapes.Public summary\u25a0 \u25a0 \u25a0 Article Abstract: Monocular depth estimation methods have achieved excellent robustness on diverse scenes, usually by predicting affine-invariant depth, up to an unknown scale and shift, rather than metric depth in that it is much easier to collect large-scale affine-invariant depth training data. However, in some video-based scenarios such as video depth estimation and 3D scene reconstruction, the unknown scale and shift residing in per-frame prediction may cause the predicted depth to be inconsistent. To tackle this problem, we propose a locally weighted linear regression method to recover the scale and shift map with very sparse anchor points, which ensures the consistency along consecutive frames. Extensive experiments show that our method can drop the Rel error of existing state-of-the-art approaches by 50% at most over several zero-shot benchmarks. Besides, we merge 6.3 million RGBD images to train robust depth models. By locally recovering scale and shift, our produced ResNet50-backbone model even outperforms the state-of-the-art DPT ViT-Large model. Combined with geometry-based reconstruction methods, we formulate a new dense 3D scene reconstruction pipeline, which benefits from both the scale consistency of sparse points and the robustness of monocular methods. By performing simple perframe prediction over a video, the accurate 3D scene geometry can be recovered.\n\nIntroduction\n\nDense monocular depth estimation [1][2][3][4][5][6][7][8][9] is a fundamental task in computer vision, which is important for a few downstream applications, such as autonomous driving [10,11] , virtual/augmented reality (VR/AR) [12,13] , 3D scene understanding [14][15][16] and reconstruction [17,18] . Existing supervised approaches [1,2,19] and unsupervised methods [9,[20][21][22] have made tremendous progress in accuracy and robustness. To solve the generalization issue over diverse scenes, current state-of-the-art methods, such as MiDaS [3] , LeReS [1] , ominidata [19] , and DPT [2] , propose to merge a large-scale multi-domains data and predict an affine-invariant depth/inverse depth [7] . Although strong generalizability has been achieved, the predicted depth/inverse depth is up to an unknown scale and shift. However, in many video-based scenarios, such as autonomous driving and VR/AR, losing metric information significantly limits its application. The variant scale and shift in per-frame prediction will cause the depth inconsistency and the failure of accurate 3D reconstruction over consecutive frames.\n\nHow to ensure the depth consistency over consecutive frames is receiving increasing attention. Bian et al. [9,20] employs an unsupervised paradigm and models the predicted depth prediction as scale-invariant. They propose a geometric consistency loss to implicitly learn the scale consistency over consecutive frames. Similarly, CVD [23] employs an unsuper-vised method but does the inference training to ensure consistency. By contrast, RCVD [24] takes the MiDaS model as the depth prior and estimates consistent dense depth maps and camera poses from a monocular video. They have achieved promising visual consistency of depth maps. However, we obverse that their reconstructed point clouds are still not so satisfactory. Please see the supplemental material for more detailed analyses. In this work, instead of the visual consistency, we focus on geometric consistency, i.e., achieving the 3D scene reconstruction from consecutive frames.\n\nFollowing previous methods, we enforce the model to predict an affine-invariant depth, thus recovering scale and shift for the prediction is the main barrier for 3D scene reconstruction over a monocular video. Existing methods [1][2][3] propose to directly compute a scale and shift value from least-squares fitting with the ground truth (GT), i.e. global recovery strategy. However, we observe that the optimal scale and shift are always heteroscedastic. During practical application, the global fitting does not consider the distribution difference between affine-invariant depth and ground-truth depth, and fails to effectively align the local regions. In Figure 2(a), we show an example error map visualization between the ground truth and such global scale-shift recovered depth, and there exists a lowfrequency error clearly. Such a coarse alignment method cannot recover a high-quality metric depth for reconstruction.\n\nMotivated by this observation, we propose a local recov-ery strategy to recover the locally aligned metric depth. Concretely, we employ the locally weighted linear regression as a novel metric depth alignment method, and demonstrate that heteroscedastic scale and shift maps can be recovered with as few as 25 points by enforcing spatial smoothness. Rather than fitting a unified scale map and shift map, i.e., sharing the same scale and shift values over all coordinates of the image, our local recovery strategy can retrieve a location-related scale map and shift map to adjust the distribution of prediction. Experiments show that our method can significantly improve metric accuracy. Although some sparse anchor points are required, compared with existing state-of-the-art depth completion methods [25][26][27][28] , which generally take more than hundreds to thousands of sparse points, our strategy requires much fewer points than them. Also experimentally we show that although completion methods have taken sparse depth measurements as inputs, our method can further boost their performance with our local recovery strategy. Besides boosting performance, the second benefit of our local recovery strategy is to better analyze the weakness of all existing depth estimation methods, and guide the design and choice of loss functions. The depth error can be decoupled into two parts: the coarse misalignment error and the detailmissing error. Compared with the error of global fitting, the alleviated error achieved by the local recovery is the coarse misalignment error, while the remaining one is the detailmissing error. Through our re-local-alignment analytical experiments in Table 3, we observe that current state-of-the-art depth estimation methods, including supervised learning metric depth, unsupervised learning scale-invariant depth, supervised learning affine-invariant depth, and depth completion, all suffer from noticeable misalignment issue w.r.t. the ground truth.\n\n\n6.3\n\nTo achieve 3D scene reconstruction from a monocular video, retrieving a strong and robust monocular depth prediction model is also essential. We collect over million images from the existing RGB-D datasets for training models using backbones such as ResNet50 [29] and Swin-L [30] , and investigate how much accuracy can we benefit from a largescale dataset. With the robust monocular depth and local recovery strategy, the metric depth can be recovered by locally aligning with some sparse points.\n\nThe last challenge is how to obtain accurate sparse anchor points as the metric guidance. For analytical evaluation, we leverage the ground-truth depth to decouple and analyze the composition of errors. For practical application, aligning with ground-truth depth can be the upper bound of our local recovery strategy. We perturb the ground truth manually and analyze the error to simulate the inaccuracy of sparse anchor points. Also, The SfM [31] can be employed to recover sparse depth points on distinguishable feature points in practice. Through per-frame alignment with such accurate guidance, we can achieve geometrically consistent depth and perform robust 3D scene reconstruction. Although existing geometrybased methods, e.g., multi-view stereo reconstruction [32][33][34] , also takes the similar paradigms of structure from motion (SfM), their performance may suffer from inaccurate corres-pondences in low-texture regions. By contrast, our per-frame prediction comes from our well-trained strong monocular depth estimation model, which is much more robust in the low-texture regions.\n\nFinally, through applying the local recovery strategy, our ResNet50 model drops the depth absolute relative error up to 50% on current affine-invariant depth evaluation benchmarks. For 3D scene reconstruction, our pipeline significantly improves both the accuracy and consistency, and achieves better performance than related works on five NYU [15] videos. To summarize, our main contributions are as follows:\n\n\u2022 We propose a novel and effective metric depth recovery strategy, i.e., locally weighted linear regression, which significantly improves the accuracy of the recovered metric depth with a very sparse set of anchor points. Extensive experiments show that the depth absolute relative error of state-ofthe-art methods can drop up to 50% with our proposed method.\n\n\u2022 Our local recovery strategy can be an analytical tool for subsequent depth prediction works, enabling decoupling the prediction errors and analyzing the weakness of their models.\n\n\u2022 We train a robust monocular depth estimation model on large diverse data that contains 6.3 million images in total. We provide detailed analyses of its performance w.r.t. the training dataset size using our analytical tool.\n\n\u2022 Aiming at the video-based scenarios, by combining our strong monocular depth estimation model with a geometrybased method for retrieving high-confidence anchor points, we design a new pipeline for robust and dense 3D scene reconstruction.\n\n\nMaterials and methods\n\nThe pipeline for our dense 3D scene reconstruction method is shown in Figure 1. Overall, our pipeline contains robust datadriven monocular depth estimation, a novel metric depth recovery, and RGB-D fusion [35] .\n\n\nOur Pipeline for Dense 3D Scene Reconstruction\n\nRobust Monocular Depth Estimation Module. Retrieving robust and accurate depth maps from 2D images is significant for 3D scene reconstruction. In the supplementary material, we analyzed that the unsupervised depth estimation methods suffer from the weak supervision of photometric loss, while inaccurate correspondences may degrade the accuracy and robustness of MVS-based methods. Thus, the supervised monocular depth estimation method is employed in our pipeline, whose promising robustness and accuracy have been demonstrated in recent works [1,3] .\n\nTo retrieve strong monocular depth estimation models, we collect over 6.3 million data from 14 diverse datasets, which cover a wide range of scenes, camera poses, and camera intrinsic parameters. Following previous works, we enforce the network to learn the affine-invariant depth. Several scale-shift invariant losses are employed during training for better learning the inherent geometric information of depth maps, including the pair-wise normal (PWN) loss [1] , image-level normalized regression (ILNR) loss [1] , and multi-scale gradient (MSG) loss [5] as follows.\nL PWN = 1 M M \u2211 i=1 |n Ai \u00b7 n Bi \u2212 n * Ai \u00b7 n * Bi | L INLR = 1 N N \u2211 i=1 |d i \u2212d * i | + | tanh(d i /100) \u2212 tanh(d * i /100)| L MSG = 1 N K \u2211 k=1 N \u2211 i=1 | \u25bd k x d i \u2212 \u25bd k xd * i | + | \u25bd k y d i \u2212 \u25bd k yd * i | L = L PWN + \u03b1L ILNR + \u03b2L MSG (1) n Ai n Bi Ai Bi n * Ai n * B\u00ee d * i = (d * i \u2212 \u00b5 trim )/\u03c3 trim \u00b5 trim \u03c3 trim \u25bd k x \u25bd k y k x y M d id * i K \u03b1 \u03b2\nHere, and represent the surface normal of sampled point pair ( , ). and are the corresponding ground truth.\n\nis the Z-score normalized groundtruth depth, and and represent the mean and standard deviation of the trimmed ground-truth depth map, which removes the nearest and farthest 10% values in advance. The and stand for the gradient at -th scale along and axis separately. The PWN loss samples paired points on edge, planar, and random regions to supervise the surface normal information. The ILNR loss is introduced to reduce the average element-wise difference of N pixels between the predicted depth and the normalized ground truth . The MSG loss ensures the accurate depth gradients at scales. The loss function is balanced by hyperparameters and , which are set to 1 and 0.2 individually in our experiments. Metric Depth Recovery Module. In the supplemental material, we analyzed the shape distortion and duplication caused by inaccurate shifts and inconsistent scales, which ensures the importance of recovering accurate and consistent scale-shift values along consecutive frames. Existing approaches recover a scale and a shift value for the depth map using least-squares fitting with some anchor points, which neglects the information of the distribution difference between affine-invariant prediction and anchor points. In contrast, we propose to perform locally weighted linear regression to recover the metric depth (Refer to Section 2.2 in detail). Compared to the global least-squares fitting, our local recovery strategy generates a scale and a shift map for each depth map, which can not only recover metric depth, but also correct the overall depth maps and ensure the accuracy and consistency of 3D reconstruction.\n\nOur proposed local recovery strategy leverages the sparse anchor points obtained from SLAM system [36] , SfM algorithms [31,37] , or some low-quality sensors, and can not only boost the performance of depth estimation, but also be an analytical tool to decouple the prediction depth errors into the coarse misalignment error and the detail missing error. Please see Section 3.2 in detail.\n\n\nRGB-D Fusion Module.\n\nThrough per-frame depth estimation and local recovery, locally scale-aligned monocular video depths are obtained. But the subtle details may still remain partially inconsistent between frames, which can cause outliers and duplication if we simply unproject it to 3D space without post-processing. Therefore, we propose to fuse multiframe information with an RGB-D fusion module, which takes the RGB frames, depth maps, camera poses, and intrinsic parameters as inputs. It balances the difference between frames, filters out outliers and inconsistent regions between frames, and outputs the fused dense 3D mesh or point cloud.\n\nIn this work, we employ the TSDF fusion [35] to fuse multiple depth maps into a projective truncated signed distance function (TSDF) voxel volume during reconstruction. The sparse guided points used for local alignment can be obtained from various SLAM [36] systems, SfM [31,37] algorithms, and some low-quality sensors such as ToF sensors of mobile phones. Two strong and robust monocular depth estimation models are trained based on ResNet50 [29] and Swin-L [30] backbones, respectively.\n\n\nMetric Depth RecoveryD\n\n= sD + \u03b8JD D s \u03b8 J Monocular depth estimation methods [1][2][3]7] have achieved promising results on diverse scenes. The problem is that their predicted depth/inverse depth is scale-shift-invariant, namely, affine-invariant depth/inverse depth [7] . Here we take the affineinvariant depth as an example. To recover the metric depth, it should be scaled and shifted, i.e., , where , , , and are the recovered metric depth, predicted affine-invariant depth, scale, shift and all-ones matrix respectively. The pipeline for dense 3D scene reconstruction. The robust monocular depth estimation model trained on 6.3 million images, locally weighted linear regression strategy, and TSDF fusion [35] are the main components of our method.\n\nXu et al. \n\n\nJ u s t A c c e p t e d\n\nSome methods propose to obtain them through a global leastsquares fitting method with ground-truth depth:\nmin \u03b2 (y \u2212 X\u03b2) T (y \u2212 X\u03b2), X = [d, 1] \u2208 R n\u00d72 \u03b2 = [s, \u03b8] T \u2208 R 2\u00d71 , d, y \u2208 R n\u00d71 \u03b2 = (X T X) \u22121 X T \u0177 D = sD + \u03b8J, withD, D, J \u2208 R H\u00d7W (2) y X d n = H \u00d7 W (H, W) \u03b2 s \u03b8\u03b2 \u03b2 s \u03b8 S \u0398\nwhere is the flattened ground-truth metric depth, is the homogeneous representation of the flattened predicted depth , represents the flattened length of depth map with a shape of . is composed of scale value and shift value , and is the optimized value of . Note that the scale value and shift value can be regarded as a scale map and a shift map shared on the whole map. However, such a globally scaling and shifting method often fails to reduce the spatially heteroscedastic errors, which follow a rather simple pattern. For example, we visualize the pixel-wise absolute relative error map between the ground truth and the globally recovered predicted depth in Figure 2(a), and observe the existence of the low-frequency spatial error. We see that the left part has a higher error than that of the right part. Motivated by this observation, we propose to leverage a locally recovering method, i.e., locally weighted linear regression (LWLR), to recover a scale and a shift map. Guided by very sparse ground-truth points, we can fix and quantify these low-rank spatial errors which are common in depth estimation tasks.\n\nLocally Weighted Linear Regression. We thus employ a locally weighted regression method, which is:\nmin \u03b2u,v (y \u2212 X\u03b2 u,v ) T W u,v (y \u2212 X\u03b2 u,v ), d, y \u2208 R m\u00d71 X = [d T , 1] \u2208 R m\u00d72 ,W u,v = diag(w 1 , w 2 , ..., w m ) \u03b2 u,v = [s u,v , \u03b8 u,v ] T \u2208 R 2\u00d71 \u03b2 u,v = (X T W u,v X) \u22121 X T W u,v \u0177 D = S \u2299 D + \u0398, withD, S, D, \u0398 \u2208 R H\u00d7W (3) y \u223c X d m\nwhere is the sampled sparse ground-truth metric depth (we use around 25 100 points in practice), is the homogeneous representation of the sampled sparse predicted depth , stands for the number of sampled points.\n(u, v) d y \u2113 2 W u,v S \u0398 s u,v \u03b8 u,v (u, v) D \u0398 \u2299 D S\nDifferent from recovering a scale-shift value or a globally shared scale-shift map by the global least-squares fitting method, we recover a location-aware scale-shift map. For each 2D coordinate , the predicted depth can be fitted to the ground-truth depth by minimizing the squared locally weighted distance, which is re-weighted by a diagonal weight matrix . It pays more attention to sparse points closer to the estimated location, based on the idea that points near each other in the explanatory variable space are more likely to be related in a simple way. By iterating over the whole image, the scale map and shift map can be generated composed of the scale values and shift values of each location . Finally, the locally recovered metric depth equals to the shift map plus the Hadamard product ( , known as element-wise product) of the affine-invariant depth and the scale map . In our implementation, we employ a Gaussian kernel function to compute the weight matrix: \n\n\nJ u s t A c c e p t e d\nw i = 1 \u221a 2\u03c0 exp(\u2212 dist 2 i 2b 2 ) (4) b dist i (u i , v i ) (u, v)\nwhere is the bandwidth of Gaussian kernel, and is the Euclidean distance between the guided point and target point .\n\u2113 2\nThe scale map and shift map obtained this way can yield much more accurate metric depth than the global methods. However, some scale maps can be fitted to negative due to the shift-invariant characteristic of monocular depth and flexibility of weighted linear regression, which inverses the distribution of depth prediction and lacks reasonableness. Since the bias is not centered and the solution space is not bounded, results are widely distributed with no physical meanings and far from the real scale and shift. Therefore, we conduct the global recovery strategy to monocular depth first, and restrict the solution to be simple by adding an regularization on the shift:\nmin \u03b2u,v (y \u2212 X\u03b2 u,v ) T W u,v (y \u2212 X\u03b2 u,v ) + \u03bb\u03b8 2 u,v X = [d T , 1] \u2208 R m\u00d72 ,W u,v = diag(w 1 , w 2 , ..., w m ) \u03b2 u,v = [s u,v , \u03b8 u,v ] T \u2208 R 2\u00d71 , d, y \u2208 R m\u00d71 \u03b2 u,v = (X T W u,v X + A) \u22121 X T W u,v y, A = [ \u03bb 0 0 0 ] D = S \u2299 D + \u0398, withD, S, D, \u0398 \u2208 R H\u00d7W (5) X d\nwhere is the homogeneous representation of the globally recovered depth . With the regularization of shift, the location-related scale map is encouraged to be positive. Please see supplemental material for the visualization of the scale value distribution.\n\nWith our proposed local recovery strategy, we only need very sparse ground-truth depth (around 25-100 points) to recover the metric depth map by fitting a location-related scale map and a shift map. Figure 2 compares the global leastsquares fitting and our proposed weighted linear regression results. Thanks to the optimized pixel-wise scale map (Figure 2(c)) and shift map (Figure 2(d)), the overall loss is reduced considerably (Figure 2(b)). Note that the predicted affine-invariant depths are the same for the two methods. Importantly, the recovered metric depth with our method is more linearly correlated to the ground truth (see Figure 2(e) and Figure 2(f)). Please see supplemental material for more ex-amples.\n\n\nResults and Discussion\n\nThe components of training datasets, the implementation details, and the evaluation details can be found in the supplemental material.\n\n\nDense 3D Scene Reconstruction\n\u03b4 1 l 1\nWith our model trained on 6.3 million data and the local scale and shift recovery method, we can achieve high-quality 3D scene reconstruction through per-frame prediction and TSDF fusion [35] . To evaluate the consistency and accuracy, we collect 5 NYU videos and compare with the single image 3D reconstruction method (LeReS [1] ), the state-of-the-art depth completion method (NLSPN [25] ), the robust consistent video depth estimation method (RCVD [24] ), the unsupervised video depth estimation method SC-DepthV2 [20] , and the learningbased MVS method DPSNet [38] . Note that NLSPN and SC-DepthV2 are trained on NYU, and only NLSPN can predict metric depth. Our method uses the same sparse ground truth (100 points) as NLSPN. For LeReS and RCVD, we align their predictions with metric depth globally. For SC-DepthV2 and DPSNet, only global scale values are recovered by ensuring the same medians as the ground truth. Besides leveraging sparse ground truth, we also sample points from SfM methods, e.g., COLMAP [37] , to reconstruct a 3D scene with an RGB video, ground-truth intrinsic and poses. The Rel, , the Chamfer distance and the F-score with the threshold of 5 cm are employed for evaluation.\n\nQuantitative comparisons are shown in Table 1. First, we compare with the depth completion method NLSPN [25] , which also uses the sparse guided points to obtain metric information. The main difference is that their model should be trained on the testing set and lacks generalization in the wild. By contrast, we can achieve better performance and generalize well to zero-shot datasets due to the robust depth prior. RCVD [24] and SC-DepthV2 [20] aim to solve the visual consistency problem for video depth prediction. LeReS [1] reconstructs 3D scene shape from a single image and performs well in the wild. DPSNet [38] leverages CNNs to extract features and match between frames automatically. Before evaluating, the NLSPN and SC-depthV2 have been trained on the NYU data- Table 1. Quantitative comparison of monocular depth estimation and 3D scene reconstruction with diverse related methods [1,20,24,25,38] on five NYU scenarios.\n\n\nMethod\n\nSparse Points  061 0.645 3.1 98.80.025 0.886 3.7 98.1 0.073 0.587 1.6 99.80.018 0.958 4.3 97.50.049 0.674 Xu et al. set. The \"global\" and \"local\" represent global and our proposed local metric depth recovery strategies separately. Ours-SfM (local) means performing the local recovery strategy with points sampled from SfM [31,37] depth. As a result, our pipeline of 3D scene reconstruction from video achieves stateof-the-art performance on all five scenes. Experiments of qualitative comparison are shown in the supplemental material. Depth completion method NLSPN performs well but misses some high-quality details due to the lack of geometry supervision during training. The RCVD focuses on visual depth consistency but fails to recover the shift of depth maps, leading to the distortion of the 3D structure. The SC-depthV2 achieves visually consistent video depth through an unsupervised paradigm, but the weak supervision brings some distortion during reconstruction. The LeReS achieves excellent detail prediction but lacks consistency between frames for misalignment caused by the global recovery strategy. The DPSNet improves the quality of extracted features with the help of CNNs, but without training on the NYU dataset, it lacks robustness to generalize to some unseen scenarios. With our local recovery strategy, our method can reconstruct better 3D point clouds than others. For Ours-SfM (local), we obtain SfM depth first, then fit the monocular depth with SfM depth and filter out the errors bigger than the 99th-percentile iteratively, before performing the local recovery strategy. Note that even with slightly inaccurate sparse SfM points, Ours-SfM (local) can still achieve comparable results with Ours(global), which requires ground-truth depth acquired from sensors.\nBasement_0001a Bedroom_0015 Dining_room_0004 Kitchen_0008 Classroom_0004 Rel\u2193 \u03b4 1 \u2191 l 1 C-\u2193 F-score\u2191 Rel\u2193 \u03b4 1 \u2191 l 1 C-\u2193 F-score\u2191 Rel\u2193 \u03b4 1 \u2191 l 1 C-\u2193 F-score\u2191 Rel\u2193 \u03b4 1 \u2191 l 1 C-\u2193 F-score\u2191 Rel\u2193 \u03b4 1 \u2191 l 1 C-\u2193 F-\n\nMonocular Depth Estimation\n\nComparison with State-of-the-Art Methods. In this experiment, we compare with state-of-the-art robust monocular depth estimation methods [1-7, 39, 40] on five zero-shot datasets, whose scale and shift are recovered with a globally leastsquares fitting method. During evaluation, the latest released model weights are adopted uniformly. As shown in Table 2, our ResNet50 [29] model outperforms other ResNet50 and Res-NeXt101 [41] models on four testing datasets, and our Swin-L [30] model achieves comparable results with the ViT-large [42] model of DPT [2] . Through recovering scale and shift with the proposed locally weighted linear regression method, our method with ResNet50 and Swin-L backbones (i.e., \"Ours-R50 (local)\", \"Ours-swin (local)\") can outperform all previous methods and our global predictions by a large margin over all zero-shot testing datasets. The qualitative comparison can be found in the supplemental material.\n\nEffectiveness of Locally Weighted Linear Regression. To demonstrate our proposed locally weighted linear regression can boost various monocular depth estimation methods, we enforce it on several different methods: 1) learning affineinvariant depth methods, e.g., LeReS [1] , MiDaS [3] , and DPT [2] ; 2) learning metric depth on a specific dataset (VNL [7] ); 3) learning scale-invariant depth with unsupervised methods (MonoDepth2 [21] ); 4) depth completion method (NLSPN [25] ). Results are shown in Table 3. We uniformly sample 100 guided points to perform the local recovery, and all their performances can be boosted significantly (see the \"w\" columns). Critically, even though the NLSPN method has input such 100 sampled points for completion, our method can still further boost its performance. Note that the latest released weights and code are used for this experiment, NLSPN (KITTI) and Monodepth2 are trained on the KITTI dataset, and NLSPN (NYU) and VNL are trained on the NYU dataset.\n\nDecoupling of Monocular Depth Error. Besides improving performance, the local recovery strategy is also performed to decouple monocular depth error between ground truth and globally aligned prediction into coarse misalignment error and detail-missing error. Compared with the error of global recovery, the alleviated error brought by local recovery represents the coarse misalignment error, and the remaining one stands for detail-missing error. As shown in Table 3, the percentage of coarse misalignment error (\"%\" columns) Table 2. Quantitative comparison of monocular depth estimation with state-of-the-art methods on five unseen datasets. The numbers in brackets represent the reduced absolute relative error brought by our local recovery method.\n\n\nMethod\n\nBackbone [9] ResNet50 [29]  Chen et al. [40] ResNet50 [29]  DPT-large [2] ViT-Large [42] 10.0 90. nearly remains consistent between affine-invariant depth estimation, which may reveal the drawback of coarse misalignment of monocular depth estimation. The bottleneck of NLSPN [25] and MonoDepth2 [21] is detail-missing for the alleviated error in percentage remains small.\nKITTI NYU ScanNet ETH3D DIODE Rank Rel\u2193 \u03b4 1 \u2191 Rel\u2193 \u03b4 1 \u2191 Rel\u2193 \u03b4 1 \u2191 Rel\u2193 \u03b4 1 \u2191 \u2193 Rel \u03b4 1 \u2191 OASIS\n\nAblation Study\n\nAblation Study for Training Data. In this experiment, we aim to study the relations between data volume and performance improvement. We gradually aggregate more data for training, and evaluate the performance on 5 zero-shot datasets. Note that 3 different quality data sources are increased in balance, and the results are illustrated in Table 4. We can observe that when the data size increases from 42K to 900K (around by 20 times), the performance is boosted significantly. However, when further increased by 7 times, the accuracy can only be improved slightly. We conjecture that such large-scale data has fully exploited the capacity of the model (ResNet50 backbone). Furthermore, we also conduct local recovery here to de-couple the error into coarse misalignment error and detailmissing error. As shown in Table 4, the percentage of coarse misalignment error remains nearly constant, which shows the model can study the detail information and the global structure simultaneously.\n\nb Ablation Study for Locally Weighted Linear Regression Method. The performance of our proposed local recovery strategy may be affected by the amount of sparse points, the sparsity distribution, random noises from sparse points, and the bandwidth . Their effects on the depth accuracy are explored and shown in Table 5. Here, \"Amount\", \"Distribution\", and \"Noise\" correspond to the number, distribution, and maximum perturbation percentage of the sampled ground truth. Parameter b represents the bandwidth of the Gaussian kernel function. \"Grid\" means sampling points from the vertexes of the evenly divided image plane, and \"Uniform\" means sampling randomly. The whole image and half image stand for only sampling ground truth from the original whole or half image. All experiments are conducted on the Table 3. Boosting various monocular depth estimations with our local recovery method. We compare the accuracy without (\"w/o\") and with (\"w\") our recovery methods, and show the reduced errors and percentages (\"w\" and \"%\").    J u s t A c c e p t e d  Table 6. Analysis for the amount of ground-truth points during recovering monocular metric depth. The Rel decreases faster with our proposed local recovery strategy with the increase of ground-truth points.\n\n\nGround-Truth Points\n\nLocal-UniformGlobal-UniformLocal-GridGlobal-Grid \n\nFig. 1 .\n1Fig. 1. The pipeline for dense 3D scene reconstruction. The robust monocular depth estimation model trained on 6.3 million images, locally weighted linear regression strategy, and TSDF fusion [35] are the main components of our method.\n\n\n10.52396/JUSTC-2023-0061 JUSTC, 2023, 53(X):\n\nFig. 2 .\n2The per-pixel error maps of ground-truth depth and predicted depth aligned through (a) global recovery and (b) local recovery, respectively. (c) The scale map and (d) the shift map of local recovery. Distribution of prediction-GT pairs obtained via (e) global recovery and (f) local recovery indi-DOI: 10.52396/JUSTC-2023-0061 JUSTC, 2023, 53(X):\n\n- 5\n5DOI: 10.52396/JUSTC-2023-0061 JUSTC, 2023, 53(X):\n\n\nDOI: 10.52396/JUSTC-2023-0061 JUSTC, 2023, 53(X):\n\n\nTowards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video DepthXu et al.1 \n9.8 \n90.3 \n7.8 \n93.8 \n7.8 \n94.6 \n18.2 \n75.8 \n4.2 \n\nOurs-R50 (global) \nResNet50 [29] \n10.9 \n88.5 \n8.2 \n92.6 \n8.9 \n92.0 \n8.4 \n92.1 \n22.0 \n80.1 \n4.6 \n\nOurs-swin (global) \nSwin-L [30] \n11.2 \n88.3 \n7.2 \n94.1 \n7.2 \n94.5 \n8.1 \n93.9 \n22.4 \n79.5 \n4.2 \n\nOurs-R50 (local) \nResNet50 [29] \n5.7 (\u22125.2) 95.4 4.7 (\u22123.5) 96.9 4.3 (\u22124.6) 97.4 5.0 (\u22123.4) 96.6 16.5 (\u22125.5) 85.2 \n1.6 \n\nOurs-swin (local) \nSwin-L [30] \n5.8 (\u22125.4) 95.0 4.5 (\u22122.7) 97.1 3.8 (\u22123.4) 97.9 4.7 (\u22123.4) 96.7 16.5 (\u22125.9) 85.1 \n1.3 \n\n-6 \nDOI: 10.52396/JUSTC-2023-0061 \nJUSTC, 2023, 53(X): \n\n\nTable 4 .\n4Ablation study for training data of our robust depth estimation module. With the increase of data, the performance of depth estimation improves gradually.Training Data \n\nKITTI \nNYU \nScanNet \nETH3D \nDIODE \n\nRel\u2193 \n\nGlobal Recovery \n\n42K \n14.1 \n11.0 \n11.8 \n9.5 \n23.2 \n\n352K \n15.0 \n9.2 \n10.0 \n9.4 \n22.1 \n\n900K \n11.5 \n8.6 \n9.7 \n9.0 \n21.6 \n\n3.8M \n11.1 \n8.2 \n8.9 \n8.4 \n21.9 \n\n6.3M \n10.9 \n8.2 \n8.9 \n8.4 \n22.0 \n\nLocal Recovery \n\n42K \n6.9 (\u221251%) \n5.8 (\u221247%) \n5.2 (\u221256%) \n5.9 (\u221238%) \n16.9 (\u221227%) \n\n\n\nTable 5 .\n5Ablation study for parameters of our proposed local recovery strategy.Amount \nDistribution \nNoise \nb \n\nNYU \n\nRel \u2193 \n\u03b4 \n1 \u2191 \n10\u00d710 \nGrid \n0% \n50 \n4.8 \n96.4 \n\n5\u00d75 \nGrid \n0% \n50 \n5.8 \n94.7 \n\n20\u00d720 \nGrid \n0% \n50 \n4.6 \n96.7 \n\n10\u00d710 \nGrid \n0% \n25 \n4.7 \n96.0 \n\n10\u00d710 \nGrid \n0% \n100 \n5.8 \n95.5 \n\n10\u00d710 \nGrid \n10% \n50 \n5.5 \n96.2 \n\n10\u00d710 \nGrid \n20% \n50 \n6.8 \n95.5 \n\n100 \nUniform (whole image) \n0% \n50 \n4.3 \n97.0 \n\n100 \nUniform (half image) \n0% \n50 \n11.3 \n87.6 \n\n\nJ u s t A c c e p t e d\nAccording to the experiments, simple ground-truth depth can be leveraged to recover metric depth and improve accuracy. More ground truth leads to more performance boost, please seeTable 6for more detailed analysis. The \"Global\" and \"Local\" represent the global recovery and local recovery strategies separately. \"Grid\" and \"Uniform\" stand for sampling from grid and sampling uniformly. Our strategy performs robust to the amount, distribution and noise of sparse points, but the overly concentrated sampling strategy should be avoided in practice. b l n l n 10 \u00d7 10 500 \u00d7 500As for bandwidth , it represents the effect of distance on the weight matrix. Experimentally, we suggest simply setting parameter bandwidth to the value , where is the width of the RGB image and is the number of sampled ground-truth points of one side. More precisely, if we sample ground-truth points for a image, the parameter bandwidth can be set to 50.ConclusionsIn this paper, we have leveraged the robust data-driven monocular depth estimation model, local recovery strategy and an RGB-D fusion module to implement a complete dense 3D scene reconstruction pipeline. Compared to existing 3D reconstruction methods, our pipeline achieves improved robustness, accuracy and consistency along consecutive RGB frames. Extensive experiments show that our method demonstrates a significantly better generalization ability to monocular depth estimation and 3D scene reconstruction.The proposed local recovery strategy can not only improve the accuracy and consistency of depth estimation significantly with robustness to both the amount and randomly-generated noises of the ground truth, but also can be an analytical tool to expose the shortcomings of existing depth estimation methods.Supplemental InformationThe supplemental information includes three sections, eight figures, and two tables. We first elaborate on some preliminary information of existing 3D scene reconstruction methods and affine-invariant depth estimation. Then, we introduce some related works. Finally, the experimental details and more visualization are supplied.Conflict of InterestThe authors declare that they have no conflict of interest, and promise the preprint version on arXiv has not been published anywhere else.\nLearning to recover 3d scene shape from a single image. Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, Chunhua Shen, IEEE Conf. Comput. Vis. Pattern Recogn. Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from a single image. In: IEEE Conf. Comput. Vis. Pattern Recogn., 204-213, 2021.\n\nVision transformers for dense prediction. Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun, IEEE Conf. Comput. Vis. Pattern Recogn. Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In: IEEE Conf. Comput. Vis. Pattern Recogn., 12179-12188, 2021.\n\nTowards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, 10.1109/TPAMI.2020.3019967IEEE Trans. Pattern Anal. Mach. Intell. 20223Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer. IEEE Trans. Pattern Anal. Mach. Intell., 2022, 44 (3): 1623-1637.\n\nStructure-Guided Ranking Loss for Single Image Depth Prediction. Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, Zhiguo Cao, IEEE Conf. Comput. Vis. Pattern Recogn. Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-Guided Ranking Loss for Single Image Depth Prediction. In: IEEE Conf. Comput. Vis. Pattern Recogn., 611-620, [4]\n\nMegadepth: Learning single-view depth prediction from internet photos. Zhengqi Li, Noah Snavely, IEEE Conf. Comput. Vis. Pattern Recogn. Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In: IEEE Conf. Comput. Vis. Pattern Recogn., 2041-2050, 2018.\n\nOASIS: A large-scale dataset for single image 3d in the wild. Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, Jia Deng, IEEE Conf. Comput. Vis. Pattern Recogn. Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. OASIS: A large-scale dataset for single image 3d in the wild. In: IEEE Conf. Comput. Vis. Pattern Recogn., 679-688, 2020.\n\nVirtual normal: Enforcing geometric constraints for accurate and robust depth prediction. Wei Yin, Yifan Liu, Chunhua Shen, TPAMI. 4410Wei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for accurate and robust depth prediction. TPAMI, 2021, 44 (10): 7282-7295.\n\nSingle-image depth perception in the wild. Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng, Adv. Neural Inform. Process. Syst. Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild. In: Adv. Neural Inform. Process. Syst., 730-738, 2016.\n\nUnsupervised scale-consistent depth learning from video. Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao Li, Le Zhang, Chunhua Shen, Ming-Ming Cheng, Ian Reid, 10.1007/s11263-021-01484-6Int. J. Comput. Vis. 1299Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao Li, Le Zhang, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsupervised scale-consistent depth learning from video. Int. J. Comput. Vis., 2021, 129 (9): 2548-2564.\n\nPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, Kilian Q Weinberger, IEEE Conf. Comput. Vis. Pattern Recogn. Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In: IEEE Conf. Comput. Vis. Pattern Recogn., 8445-8453, 2019.\n\nIda-3d: Instance-depthaware 3d object detection from stereo vision for autonomous driving. Wanli Peng, He Hao Pan, Yi Liu, Sun, IEEE Conf. Comput. Vis. Pattern Recogn. Wanli Peng, Hao Pan, He Liu, and Yi Sun. Ida-3d: Instance-depth- aware 3d object detection from stereo vision for autonomous driving. In: IEEE Conf. Comput. Vis. Pattern Recogn., 13015-13024, 2020.\n\nDuygu Ceylan, and Hailin Jin. 6-DOF VR videos with a single 360-camera. Jingwei Huang, Zhili Chen, IEEE Vir. Real. Jingwei Huang, Zhili Chen, Duygu Ceylan, and Hailin Jin. 6-DOF VR videos with a single 360-camera. In: IEEE Vir. Real., 37-44, 2017.\n\nDepth from motion for smartphone AR. Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, ACM Transh. Graph. 376Julien Valentin, Adarsh Kowdle, Jonathan T Barron, Neal Wadhwa, Max Dzitsiuk, Michael Schoenberg, Vivek Verma, Ambrus Csaszar, Eric Turner, Ivan Dryanovski, et al. Depth from motion for smartphone AR. ACM Transh. Graph., 2018, 37 (6): 1-19.\n\nSemantic scene completion from a single depth image. Shuran Song, Fisher Yu, Andy Zeng, X Angel, Manolis Chang, Thomas Savva, Funkhouser, IEEE Conf. Comput. Vis. Pattern Recogn. Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene completion from a single depth image. In: IEEE Conf. Comput. Vis. Pattern Recogn., 1746-1754, 2017.\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, Eur. Conf. Comput. Vis. SpringerNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In: Eur. Conf. Comput. Vis., 746-760. Springer, 2012.\n\nSun rgb-d: A rgb-d scene understanding benchmark suite. Shuran Song, P Samuel, Jianxiong Lichtenberg, Xiao, IEEE Conf. Comput. Vis. Pattern Recogn. Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In: IEEE Conf. Comput. Vis. Pattern Recogn., 567-576, 2015.\n\nMobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, Guofeng Zhang, 10.1109/TVCG.2020.3023634IEEE Transh. Vish. and Comph. Graphh. 202012Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transh. Vish. and Comph. Graphh., 2020, 26 (12): 3446-3456.\n\nNeural RGB-D surface reconstruction. Dejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, Justus Thies, IEEE Conf. Comput. Vis. Pattern Recogn. Dejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural RGB-D surface reconstruction. In: IEEE Conf. Comput. Vis. Pattern Recogn., 6290-6301, 2022.\n\nOmnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. Ainaz Eftekhar, Alexander Sax, Jitendra Malik, Amir Zamir, Int. Conf. Comput. Vis. Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In: Int. Conf. Comput. Vis., 10786-10796, 2021.\n\nAuto-Rectify Network for Unsupervised Indoor Depth Estimation. Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Tat-Jun Chin, Chunhua Shen, Ian Reid, TPAMI. 2021Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Tat-Jun Chin, Chunhua Shen, and Ian Reid. Auto-Rectify Network for Unsupervised Indoor Depth Estimation. TPAMI, 2021: 1-1.\n\nDigging into self-supervised monocular depth estimation. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J Brostow, Int. Conf. Comput. Vis. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In: Int. Conf. Comput. Vis., 3828-3838, 2019.\n\nThe temporal opportunist: Selfsupervised multi-frame monocular depth. Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel Brostow, Michael Firman, IEEE Conf. Comput. Vis. Pattern Recogn. Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel Brostow, and Michael Firman. The temporal opportunist: Self- supervised multi-frame monocular depth. In: IEEE Conf. Comput. Vis. Pattern Recogn., 1164-1174, 2021.\n\nConsistent video depth estimation. Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf, ACMTransXuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Trans.\n\nRobust consistent video depth estimation. Johannes Kopf, Xuejian Rong, Jia-Bin Huang, IEEE Conf. Comput. Vis. Pattern Recogn. Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In: IEEE Conf. Comput. Vis. Pattern Recogn., 1611-1621, 2021.\n\nChi-Kuei Liu, and In So Kweon. Non-local spatial propagation network for depth completion. Jinsun Park, Kyungdon Joo, Zhe Hu, Eur. Conf. Comput. Vis. SpringerJinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In So Kweon. Non-local spatial propagation network for depth completion. In: Eur. Conf. Comput. Vis., 120-136. Springer, 2020.\n\nUnsupervised depth completion with calibrated backprojection layers. Alex Wong, Stefano Soatto, Int. Conf. Comput. Vis. Alex Wong and Stefano Soatto. Unsupervised depth completion with calibrated backprojection layers. In: Int. Conf. Comput. Vis., 12747-12756, 2021.\n\nLearning topology from synthetic data for unsupervised depth completion. Alex Wong, Safa Cicek, Stefano Soatto, 10.1109/LRA.2021.3058072IEEE Rob. and Auto. Lett. 20212Alex Wong, Safa Cicek, and Stefano Soatto. Learning topology from synthetic data for unsupervised depth completion. IEEE Rob. and Auto. Lett., 2021, 6 (2): 1495-1502.\n\nDense depth posterior (ddp) from single image and sparse range. Yanchao Yang, Alex Wong, Stefano Soatto, IEEE Conf. Comput. Vis. Pattern Recogn. Yanchao Yang, Alex Wong, and Stefano Soatto. Dense depth posterior (ddp) from single image and sparse range. In: IEEE Conf. Comput. Vis. Pattern Recogn., 3353-3362, 2019.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE Conf. Comput. Vis. Pattern Recogn. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In: IEEE Conf. Comput. Vis. Pattern Recogn., 770-778, 2016.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Int. Conf. Comput. Vis. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In: Int. Conf. Comput. Vis., 10012-10022, 2021.\n\nStructure-frommotion revisited. L Johannes, Jan-Michael Schonberger, Frahm, IEEE Conf. Comput. Vis. Pattern Recogn. Johannes L Schonberger and Jan-Michael Frahm. Structure-from- motion revisited. In: IEEE Conf. Comput. Vis. Pattern Recogn., 4104-4113, 2016.\n\nDeepV2D: Video to Depth with Differentiable Structure from Motion. Zachary Teed, Jia Deng, Int. Conf. Learn. Represent. Zachary Teed and Jia Deng. DeepV2D: Video to Depth with Differentiable Structure from Motion. In: Int. Conf. Learn. Represent., 2020.\n\nNeural RGB \u2192 D sensing: Depth and uncertainty from a video camera. Chao Liu, Jinwei Gu, Kihwan Kim, G Srinivasa, Jan Narasimhan, Kautz, IEEE Conf. Comput. Vis. Pattern Recogn. Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G. Narasimhan, and Jan Kautz. Neural RGB \u2192 D sensing: Depth and uncertainty from a video camera. In: IEEE Conf. Comput. Vis. Pattern Recogn., 10986-10995, 2019.\n\nDeepVideoMVS: Multi-view stereo on video with recurrent spatio-temporal fusion. Arda Duzceker, Silvano Galliani, Christoph Vogel, Pablo Speciale, Mihai Dusmanu, Marc Pollefeys, IEEE Conf. Comput. Vis. Pattern Recogn. Arda Duzceker, Silvano Galliani, Christoph Vogel, Pablo Speciale, Mihai Dusmanu, and Marc Pollefeys. DeepVideoMVS: Multi-view stereo on video with recurrent spatio-temporal fusion. In: IEEE Conf. Comput. Vis. Pattern Recogn., 15324-15333, 2021.\n\nLearning local geometric descriptors from rgb-d reconstructions. Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser, IEEE Conf. Comput. Vis. Pattern Recogn. 3Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In: IEEE Conf. Comput. Vis. Pattern Recogn., 1802-1811, 2017.\n\nORB-SLAM: a Versatile and Accurate Monocular SLAM System. Ra\u00fal Mur-Artal, J M M Montiel, Juan D Tard\u00f3s, 10.1109/TRO.2015.2463671IEEE Transactions on Robotics. 315Ra\u00fal Mur-Artal, J. M. M. Montiel, and Juan D. Tard\u00f3s. ORB- SLAM: a Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, 2015, 31 (5): 1147-1163.\n\nPixelwise view selection for unstructured multiview stereo. L Johannes, Enliang Sch\u00f6nberger, Jan-Michael Zheng, Marc Frahm, Pollefeys, Eur. Conf. Comput. Vis. SpringerJohannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi- view stereo. In: Eur. Conf. Comput. Vis., 501-518. Springer, 2016.\n\n. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: Endto-end Deep Plane Sweep Stereo. In: Int. Conf. Learn. Represent. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: Endto-end Deep Plane Sweep Stereo. In: Int. Conf. Learn. Represent., 2019.\n\nWeb stereo video supervision for depth prediction from dynamic scenes. Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang, Int. Conf. 3D Vis. Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for depth prediction from dynamic scenes. In: Int. Conf. 3D Vis., 348-357, 2019.\n\nLearning single-image depth from videos using quality assessment networks. Weifeng Chen, Shengyi Qian, Jia Deng, IEEE Conf. Comput. Vis. Pattern Recogn. Weifeng Chen, Shengyi Qian, and Jia Deng. Learning single-image depth from videos using quality assessment networks. In: IEEE Conf. Comput. Vis. Pattern Recogn., 5604-5613, 2019.\n\nAg-gregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, IEEE Conf. Comput. Vis. Pattern Recogn. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Ag-gregated residual transformations for deep neural networks. In: IEEE Conf. Comput. Vis. Pattern Recogn., 1492-1500, 2017.\n\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, Int. Conf. Learn. Represent. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: Int. Conf. Learn. Represent., 2021.\n\n. Xu, 10.52396/JUSTC-2023-0061-9Xu et al. -9 DOI: 10.52396/JUSTC-2023-0061\n\n. JUSTC. 2023XJUSTC, 2023, 53(X):\n", "annotations": {"author": "[{\"end\":95,\"start\":88},{\"end\":103,\"start\":96},{\"end\":259,\"start\":104},{\"end\":415,\"start\":260}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":92},{\"end\":102,\"start\":98}]", "author_first_name": "[{\"end\":91,\"start\":88},{\"end\":97,\"start\":96}]", "author_affiliation": "[{\"end\":258,\"start\":105},{\"end\":414,\"start\":261}]", "title": "[{\"end\":81,\"start\":1},{\"end\":496,\"start\":416}]", "venue": "[{\"end\":503,\"start\":498}]", "abstract": "[{\"end\":2563,\"start\":801}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2615,\"start\":2612},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2618,\"start\":2615},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2621,\"start\":2618},{\"end\":2624,\"start\":2621},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2627,\"start\":2624},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2630,\"start\":2627},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2633,\"start\":2630},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2636,\"start\":2633},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2639,\"start\":2636},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2767,\"start\":2763},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2770,\"start\":2767},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2811,\"start\":2807},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2814,\"start\":2811},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2844,\"start\":2840},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2848,\"start\":2844},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2852,\"start\":2848},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2876,\"start\":2872},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2879,\"start\":2876},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2916,\"start\":2913},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2918,\"start\":2916},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2921,\"start\":2918},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2950,\"start\":2947},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2954,\"start\":2950},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2958,\"start\":2954},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2962,\"start\":2958},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3127,\"start\":3124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3139,\"start\":3136},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3156,\"start\":3152},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3170,\"start\":3167},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3278,\"start\":3275},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3815,\"start\":3812},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3818,\"start\":3815},{\"end\":4042,\"start\":4038},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4152,\"start\":4148},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4878,\"start\":4875},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4881,\"start\":4878},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4884,\"start\":4881},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6381,\"start\":6377},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6385,\"start\":6381},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6389,\"start\":6385},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6393,\"start\":6389},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7833,\"start\":7829},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7849,\"start\":7845},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8516,\"start\":8512},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8842,\"start\":8838},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8846,\"start\":8842},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8850,\"start\":8846},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9514,\"start\":9510},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10822,\"start\":10818},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11423,\"start\":11420},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11425,\"start\":11423},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11892,\"start\":11889},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11944,\"start\":11941},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11986,\"start\":11983},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14193,\"start\":14189},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14215,\"start\":14211},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14218,\"start\":14215},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15175,\"start\":15171},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15388,\"start\":15384},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15406,\"start\":15402},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15409,\"start\":15406},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15579,\"start\":15575},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15595,\"start\":15591},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15704,\"start\":15701},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15707,\"start\":15704},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15710,\"start\":15707},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15712,\"start\":15710},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15894,\"start\":15891},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16338,\"start\":16334},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21938,\"start\":21934},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22076,\"start\":22073},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22136,\"start\":22132},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22202,\"start\":22198},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22268,\"start\":22264},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22315,\"start\":22311},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22766,\"start\":22762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23061,\"start\":23057},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23379,\"start\":23375},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23399,\"start\":23395},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23481,\"start\":23478},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23572,\"start\":23568},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23850,\"start\":23847},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23853,\"start\":23850},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23856,\"start\":23853},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23859,\"start\":23856},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23862,\"start\":23859},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24222,\"start\":24218},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24225,\"start\":24222},{\"end\":26071,\"start\":26058},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26295,\"start\":26291},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26349,\"start\":26345},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26402,\"start\":26398},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26460,\"start\":26456},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26477,\"start\":26474},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27131,\"start\":27128},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27143,\"start\":27140},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27157,\"start\":27154},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27215,\"start\":27212},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27295,\"start\":27291},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27337,\"start\":27333},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28632,\"start\":28629},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28646,\"start\":28642},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28664,\"start\":28660},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28678,\"start\":28674},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28693,\"start\":28690},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28708,\"start\":28704},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28899,\"start\":28895},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28919,\"start\":28915}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31674,\"start\":31428},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31721,\"start\":31675},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32079,\"start\":31722},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32135,\"start\":32080},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32187,\"start\":32136},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32823,\"start\":32188},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33323,\"start\":32824},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33788,\"start\":33324}]", "paragraph": "[{\"end\":3703,\"start\":2579},{\"end\":4646,\"start\":3705},{\"end\":5573,\"start\":4648},{\"end\":7562,\"start\":5575},{\"end\":8067,\"start\":7570},{\"end\":9164,\"start\":8069},{\"end\":9575,\"start\":9166},{\"end\":9936,\"start\":9577},{\"end\":10118,\"start\":9938},{\"end\":10345,\"start\":10120},{\"end\":10587,\"start\":10347},{\"end\":10824,\"start\":10613},{\"end\":11427,\"start\":10875},{\"end\":11998,\"start\":11429},{\"end\":12462,\"start\":12355},{\"end\":14089,\"start\":12464},{\"end\":14479,\"start\":14091},{\"end\":15129,\"start\":14504},{\"end\":15620,\"start\":15131},{\"end\":16377,\"start\":15647},{\"end\":16389,\"start\":16379},{\"end\":16522,\"start\":16417},{\"end\":17824,\"start\":16703},{\"end\":17924,\"start\":17826},{\"end\":18378,\"start\":18167},{\"end\":19409,\"start\":18433},{\"end\":19620,\"start\":19504},{\"end\":20298,\"start\":19625},{\"end\":20824,\"start\":20568},{\"end\":21545,\"start\":20826},{\"end\":21706,\"start\":21572},{\"end\":22951,\"start\":21747},{\"end\":23885,\"start\":22953},{\"end\":25684,\"start\":23896},{\"end\":26857,\"start\":25921},{\"end\":27857,\"start\":26859},{\"end\":28609,\"start\":27859},{\"end\":28991,\"start\":28620},{\"end\":30092,\"start\":29106},{\"end\":31354,\"start\":30094},{\"end\":31427,\"start\":31378}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12354,\"start\":11999},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16702,\"start\":16523},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18166,\"start\":17925},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18432,\"start\":18379},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19503,\"start\":19436},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19624,\"start\":19621},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20567,\"start\":20299},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21746,\"start\":21739},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25891,\"start\":25685},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29088,\"start\":28992}]", "table_ref": "[{\"end\":7268,\"start\":7261},{\"end\":22998,\"start\":22991},{\"end\":23734,\"start\":23727},{\"end\":26277,\"start\":26269},{\"end\":27369,\"start\":27362},{\"end\":28324,\"start\":28317},{\"end\":28391,\"start\":28384},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29451,\"start\":29444},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29926,\"start\":29919},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30412,\"start\":30405},{\"end\":30905,\"start\":30898},{\"end\":31155,\"start\":31148}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2577,\"start\":2565},{\"end\":7568,\"start\":7565},{\"attributes\":{\"n\":\"2\"},\"end\":10611,\"start\":10590},{\"attributes\":{\"n\":\"2.1\"},\"end\":10873,\"start\":10827},{\"end\":14502,\"start\":14482},{\"attributes\":{\"n\":\"2.2\"},\"end\":15645,\"start\":15623},{\"end\":16415,\"start\":16392},{\"end\":19435,\"start\":19412},{\"attributes\":{\"n\":\"3\"},\"end\":21570,\"start\":21548},{\"attributes\":{\"n\":\"3.1\"},\"end\":21738,\"start\":21709},{\"end\":23894,\"start\":23888},{\"attributes\":{\"n\":\"3.2\"},\"end\":25919,\"start\":25893},{\"end\":28618,\"start\":28612},{\"attributes\":{\"n\":\"3.3\"},\"end\":29104,\"start\":29090},{\"end\":31376,\"start\":31357},{\"end\":31437,\"start\":31429},{\"end\":31731,\"start\":31723},{\"end\":32084,\"start\":32081},{\"end\":32834,\"start\":32825},{\"end\":33334,\"start\":33325}]", "table": "[{\"end\":32823,\"start\":32279},{\"end\":33323,\"start\":32990},{\"end\":33788,\"start\":33406}]", "figure_caption": "[{\"end\":31674,\"start\":31439},{\"end\":31721,\"start\":31677},{\"end\":32079,\"start\":31733},{\"end\":32135,\"start\":32086},{\"end\":32187,\"start\":32138},{\"end\":32279,\"start\":32190},{\"end\":32990,\"start\":32836},{\"end\":33406,\"start\":33336}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5315,\"start\":5307},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10691,\"start\":10683},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17375,\"start\":17367},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21033,\"start\":21025},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21182,\"start\":21173},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21213,\"start\":21201},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21269,\"start\":21257},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21471,\"start\":21463},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21487,\"start\":21479},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24001,\"start\":23911}]", "bib_author_first_name": "[{\"end\":36143,\"start\":36140},{\"end\":36157,\"start\":36149},{\"end\":36171,\"start\":36165},{\"end\":36183,\"start\":36178},{\"end\":36197,\"start\":36193},{\"end\":36208,\"start\":36203},{\"end\":36222,\"start\":36215},{\"end\":36525,\"start\":36521},{\"end\":36540,\"start\":36534},{\"end\":36561,\"start\":36554},{\"end\":36871,\"start\":36867},{\"end\":36886,\"start\":36880},{\"end\":36902,\"start\":36897},{\"end\":36917,\"start\":36911},{\"end\":36936,\"start\":36929},{\"end\":37329,\"start\":37327},{\"end\":37344,\"start\":37336},{\"end\":37358,\"start\":37352},{\"end\":37369,\"start\":37365},{\"end\":37378,\"start\":37375},{\"end\":37390,\"start\":37384},{\"end\":37711,\"start\":37704},{\"end\":37720,\"start\":37716},{\"end\":38002,\"start\":37995},{\"end\":38016,\"start\":38009},{\"end\":38028,\"start\":38023},{\"end\":38042,\"start\":38034},{\"end\":38054,\"start\":38051},{\"end\":38068,\"start\":38065},{\"end\":38415,\"start\":38412},{\"end\":38426,\"start\":38421},{\"end\":38439,\"start\":38432},{\"end\":38669,\"start\":38662},{\"end\":38680,\"start\":38676},{\"end\":38690,\"start\":38685},{\"end\":38700,\"start\":38697},{\"end\":38955,\"start\":38947},{\"end\":38971,\"start\":38962},{\"end\":38984,\"start\":38978},{\"end\":38998,\"start\":38991},{\"end\":39005,\"start\":39003},{\"end\":39020,\"start\":39013},{\"end\":39036,\"start\":39027},{\"end\":39047,\"start\":39044},{\"end\":39432,\"start\":39429},{\"end\":39446,\"start\":39439},{\"end\":39461,\"start\":39453},{\"end\":39475,\"start\":39468},{\"end\":39491,\"start\":39487},{\"end\":39510,\"start\":39502},{\"end\":39927,\"start\":39922},{\"end\":39936,\"start\":39934},{\"end\":39948,\"start\":39946},{\"end\":40277,\"start\":40270},{\"end\":40290,\"start\":40285},{\"end\":40490,\"start\":40484},{\"end\":40507,\"start\":40501},{\"end\":40524,\"start\":40516},{\"end\":40526,\"start\":40525},{\"end\":40539,\"start\":40535},{\"end\":40551,\"start\":40548},{\"end\":40569,\"start\":40562},{\"end\":40587,\"start\":40582},{\"end\":40601,\"start\":40595},{\"end\":40615,\"start\":40611},{\"end\":40628,\"start\":40624},{\"end\":40964,\"start\":40958},{\"end\":40977,\"start\":40971},{\"end\":40986,\"start\":40982},{\"end\":40994,\"start\":40993},{\"end\":41009,\"start\":41002},{\"end\":41023,\"start\":41017},{\"end\":41353,\"start\":41347},{\"end\":41370,\"start\":41365},{\"end\":41386,\"start\":41378},{\"end\":41397,\"start\":41394},{\"end\":41678,\"start\":41672},{\"end\":41686,\"start\":41685},{\"end\":41704,\"start\":41695},{\"end\":42015,\"start\":42008},{\"end\":42028,\"start\":42022},{\"end\":42042,\"start\":42035},{\"end\":42060,\"start\":42050},{\"end\":42073,\"start\":42067},{\"end\":42085,\"start\":42080},{\"end\":42098,\"start\":42091},{\"end\":42457,\"start\":42452},{\"end\":42475,\"start\":42468},{\"end\":42495,\"start\":42492},{\"end\":42497,\"start\":42496},{\"end\":42515,\"start\":42507},{\"end\":42531,\"start\":42525},{\"end\":42868,\"start\":42863},{\"end\":42888,\"start\":42879},{\"end\":42902,\"start\":42894},{\"end\":42914,\"start\":42910},{\"end\":43222,\"start\":43214},{\"end\":43238,\"start\":43229},{\"end\":43251,\"start\":43245},{\"end\":43265,\"start\":43258},{\"end\":43279,\"start\":43272},{\"end\":43289,\"start\":43286},{\"end\":43539,\"start\":43532},{\"end\":43553,\"start\":43548},{\"end\":43557,\"start\":43554},{\"end\":43572,\"start\":43565},{\"end\":43588,\"start\":43581},{\"end\":43590,\"start\":43589},{\"end\":43875,\"start\":43870},{\"end\":43889,\"start\":43884},{\"end\":43893,\"start\":43890},{\"end\":43907,\"start\":43901},{\"end\":43927,\"start\":43920},{\"end\":43944,\"start\":43937},{\"end\":44254,\"start\":44250},{\"end\":44267,\"start\":44260},{\"end\":44282,\"start\":44275},{\"end\":44298,\"start\":44293},{\"end\":44315,\"start\":44307},{\"end\":44503,\"start\":44495},{\"end\":44517,\"start\":44510},{\"end\":44531,\"start\":44524},{\"end\":44829,\"start\":44823},{\"end\":44844,\"start\":44836},{\"end\":44853,\"start\":44850},{\"end\":45144,\"start\":45140},{\"end\":45158,\"start\":45151},{\"end\":45416,\"start\":45412},{\"end\":45427,\"start\":45423},{\"end\":45442,\"start\":45435},{\"end\":45745,\"start\":45738},{\"end\":45756,\"start\":45752},{\"end\":45770,\"start\":45763},{\"end\":46044,\"start\":46037},{\"end\":46056,\"start\":46049},{\"end\":46072,\"start\":46064},{\"end\":46082,\"start\":46078},{\"end\":46365,\"start\":46363},{\"end\":46377,\"start\":46371},{\"end\":46386,\"start\":46383},{\"end\":46395,\"start\":46392},{\"end\":46406,\"start\":46400},{\"end\":46417,\"start\":46412},{\"end\":46432,\"start\":46425},{\"end\":46445,\"start\":46438},{\"end\":46722,\"start\":46721},{\"end\":46744,\"start\":46733},{\"end\":47022,\"start\":47015},{\"end\":47032,\"start\":47029},{\"end\":47274,\"start\":47270},{\"end\":47286,\"start\":47280},{\"end\":47297,\"start\":47291},{\"end\":47304,\"start\":47303},{\"end\":47319,\"start\":47316},{\"end\":47668,\"start\":47664},{\"end\":47686,\"start\":47679},{\"end\":47706,\"start\":47697},{\"end\":47719,\"start\":47714},{\"end\":47735,\"start\":47730},{\"end\":47749,\"start\":47745},{\"end\":48116,\"start\":48112},{\"end\":48129,\"start\":48123},{\"end\":48144,\"start\":48136},{\"end\":48161,\"start\":48154},{\"end\":48179,\"start\":48170},{\"end\":48192,\"start\":48186},{\"end\":48542,\"start\":48538},{\"end\":48555,\"start\":48554},{\"end\":48559,\"start\":48556},{\"end\":48573,\"start\":48569},{\"end\":48575,\"start\":48574},{\"end\":48873,\"start\":48872},{\"end\":48891,\"start\":48884},{\"end\":48916,\"start\":48905},{\"end\":48928,\"start\":48924},{\"end\":49184,\"start\":49176},{\"end\":49196,\"start\":49189},{\"end\":49529,\"start\":49521},{\"end\":49541,\"start\":49536},{\"end\":49557,\"start\":49549},{\"end\":49573,\"start\":49567},{\"end\":49855,\"start\":49848},{\"end\":49869,\"start\":49862},{\"end\":49879,\"start\":49876},{\"end\":50176,\"start\":50169},{\"end\":50186,\"start\":50182},{\"end\":50202,\"start\":50197},{\"end\":50218,\"start\":50211},{\"end\":50230,\"start\":50223},{\"end\":50553,\"start\":50547},{\"end\":50572,\"start\":50567},{\"end\":50589,\"start\":50580},{\"end\":50606,\"start\":50602},{\"end\":50627,\"start\":50620},{\"end\":50640,\"start\":50634},{\"end\":50661,\"start\":50654},{\"end\":50680,\"start\":50672},{\"end\":50696,\"start\":50691},{\"end\":50713,\"start\":50706},{\"end\":50726,\"start\":50721},{\"end\":50742,\"start\":50738}]", "bib_author_last_name": "[{\"end\":36147,\"start\":36144},{\"end\":36163,\"start\":36158},{\"end\":36176,\"start\":36172},{\"end\":36191,\"start\":36184},{\"end\":36201,\"start\":36198},{\"end\":36213,\"start\":36209},{\"end\":36227,\"start\":36223},{\"end\":36532,\"start\":36526},{\"end\":36552,\"start\":36541},{\"end\":36568,\"start\":36562},{\"end\":36878,\"start\":36872},{\"end\":36895,\"start\":36887},{\"end\":36909,\"start\":36903},{\"end\":36927,\"start\":36918},{\"end\":36943,\"start\":36937},{\"end\":37334,\"start\":37330},{\"end\":37350,\"start\":37345},{\"end\":37363,\"start\":37359},{\"end\":37373,\"start\":37370},{\"end\":37382,\"start\":37379},{\"end\":37394,\"start\":37391},{\"end\":37714,\"start\":37712},{\"end\":37728,\"start\":37721},{\"end\":38007,\"start\":38003},{\"end\":38021,\"start\":38017},{\"end\":38032,\"start\":38029},{\"end\":38049,\"start\":38043},{\"end\":38063,\"start\":38055},{\"end\":38073,\"start\":38069},{\"end\":38419,\"start\":38416},{\"end\":38430,\"start\":38427},{\"end\":38444,\"start\":38440},{\"end\":38674,\"start\":38670},{\"end\":38683,\"start\":38681},{\"end\":38695,\"start\":38691},{\"end\":38705,\"start\":38701},{\"end\":38960,\"start\":38956},{\"end\":38976,\"start\":38972},{\"end\":38989,\"start\":38985},{\"end\":39001,\"start\":38999},{\"end\":39011,\"start\":39006},{\"end\":39025,\"start\":39021},{\"end\":39042,\"start\":39037},{\"end\":39052,\"start\":39048},{\"end\":39437,\"start\":39433},{\"end\":39451,\"start\":39447},{\"end\":39466,\"start\":39462},{\"end\":39485,\"start\":39476},{\"end\":39500,\"start\":39492},{\"end\":39521,\"start\":39511},{\"end\":39932,\"start\":39928},{\"end\":39944,\"start\":39937},{\"end\":39952,\"start\":39949},{\"end\":39957,\"start\":39954},{\"end\":40283,\"start\":40278},{\"end\":40295,\"start\":40291},{\"end\":40499,\"start\":40491},{\"end\":40514,\"start\":40508},{\"end\":40533,\"start\":40527},{\"end\":40546,\"start\":40540},{\"end\":40560,\"start\":40552},{\"end\":40580,\"start\":40570},{\"end\":40593,\"start\":40588},{\"end\":40609,\"start\":40602},{\"end\":40622,\"start\":40616},{\"end\":40639,\"start\":40629},{\"end\":40969,\"start\":40965},{\"end\":40980,\"start\":40978},{\"end\":40991,\"start\":40987},{\"end\":41000,\"start\":40995},{\"end\":41015,\"start\":41010},{\"end\":41029,\"start\":41024},{\"end\":41041,\"start\":41031},{\"end\":41363,\"start\":41354},{\"end\":41376,\"start\":41371},{\"end\":41392,\"start\":41387},{\"end\":41404,\"start\":41398},{\"end\":41683,\"start\":41679},{\"end\":41693,\"start\":41687},{\"end\":41716,\"start\":41705},{\"end\":41722,\"start\":41718},{\"end\":42020,\"start\":42016},{\"end\":42033,\"start\":42029},{\"end\":42048,\"start\":42043},{\"end\":42065,\"start\":42061},{\"end\":42078,\"start\":42074},{\"end\":42089,\"start\":42086},{\"end\":42104,\"start\":42099},{\"end\":42466,\"start\":42458},{\"end\":42490,\"start\":42476},{\"end\":42505,\"start\":42498},{\"end\":42523,\"start\":42516},{\"end\":42537,\"start\":42532},{\"end\":42877,\"start\":42869},{\"end\":42892,\"start\":42889},{\"end\":42908,\"start\":42903},{\"end\":42920,\"start\":42915},{\"end\":43227,\"start\":43223},{\"end\":43243,\"start\":43239},{\"end\":43256,\"start\":43252},{\"end\":43270,\"start\":43266},{\"end\":43284,\"start\":43280},{\"end\":43294,\"start\":43290},{\"end\":43546,\"start\":43540},{\"end\":43563,\"start\":43558},{\"end\":43579,\"start\":43573},{\"end\":43598,\"start\":43591},{\"end\":43882,\"start\":43876},{\"end\":43899,\"start\":43894},{\"end\":43918,\"start\":43908},{\"end\":43935,\"start\":43928},{\"end\":43951,\"start\":43945},{\"end\":44258,\"start\":44255},{\"end\":44273,\"start\":44268},{\"end\":44291,\"start\":44283},{\"end\":44305,\"start\":44299},{\"end\":44320,\"start\":44316},{\"end\":44508,\"start\":44504},{\"end\":44522,\"start\":44518},{\"end\":44537,\"start\":44532},{\"end\":44834,\"start\":44830},{\"end\":44848,\"start\":44845},{\"end\":44856,\"start\":44854},{\"end\":45149,\"start\":45145},{\"end\":45165,\"start\":45159},{\"end\":45421,\"start\":45417},{\"end\":45433,\"start\":45428},{\"end\":45449,\"start\":45443},{\"end\":45750,\"start\":45746},{\"end\":45761,\"start\":45757},{\"end\":45777,\"start\":45771},{\"end\":46047,\"start\":46045},{\"end\":46062,\"start\":46057},{\"end\":46076,\"start\":46073},{\"end\":46086,\"start\":46083},{\"end\":46369,\"start\":46366},{\"end\":46381,\"start\":46378},{\"end\":46390,\"start\":46387},{\"end\":46398,\"start\":46396},{\"end\":46410,\"start\":46407},{\"end\":46423,\"start\":46418},{\"end\":46436,\"start\":46433},{\"end\":46449,\"start\":46446},{\"end\":46731,\"start\":46723},{\"end\":46756,\"start\":46745},{\"end\":46763,\"start\":46758},{\"end\":47027,\"start\":47023},{\"end\":47037,\"start\":47033},{\"end\":47278,\"start\":47275},{\"end\":47289,\"start\":47287},{\"end\":47301,\"start\":47298},{\"end\":47314,\"start\":47305},{\"end\":47330,\"start\":47320},{\"end\":47337,\"start\":47332},{\"end\":47677,\"start\":47669},{\"end\":47695,\"start\":47687},{\"end\":47712,\"start\":47707},{\"end\":47728,\"start\":47720},{\"end\":47743,\"start\":47736},{\"end\":47759,\"start\":47750},{\"end\":48121,\"start\":48117},{\"end\":48134,\"start\":48130},{\"end\":48152,\"start\":48145},{\"end\":48168,\"start\":48162},{\"end\":48184,\"start\":48180},{\"end\":48203,\"start\":48193},{\"end\":48552,\"start\":48543},{\"end\":48567,\"start\":48560},{\"end\":48582,\"start\":48576},{\"end\":48882,\"start\":48874},{\"end\":48903,\"start\":48892},{\"end\":48922,\"start\":48917},{\"end\":48934,\"start\":48929},{\"end\":48945,\"start\":48936},{\"end\":49187,\"start\":49185},{\"end\":49201,\"start\":49197},{\"end\":49534,\"start\":49530},{\"end\":49547,\"start\":49542},{\"end\":49565,\"start\":49558},{\"end\":49578,\"start\":49574},{\"end\":49860,\"start\":49856},{\"end\":49874,\"start\":49870},{\"end\":49884,\"start\":49880},{\"end\":50180,\"start\":50177},{\"end\":50195,\"start\":50187},{\"end\":50209,\"start\":50203},{\"end\":50221,\"start\":50219},{\"end\":50233,\"start\":50231},{\"end\":50565,\"start\":50554},{\"end\":50578,\"start\":50573},{\"end\":50600,\"start\":50590},{\"end\":50618,\"start\":50607},{\"end\":50632,\"start\":50628},{\"end\":50652,\"start\":50641},{\"end\":50670,\"start\":50662},{\"end\":50689,\"start\":50681},{\"end\":50704,\"start\":50697},{\"end\":50719,\"start\":50714},{\"end\":50736,\"start\":50727},{\"end\":50750,\"start\":50743},{\"end\":51111,\"start\":51109}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":229298063},\"end\":36477,\"start\":36084},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":232352612},\"end\":36768,\"start\":36479},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2020.3019967\",\"id\":\"b2\",\"matched_paper_id\":195776274},\"end\":37260,\"start\":36770},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219633501},\"end\":37631,\"start\":37262},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4572038},\"end\":37931,\"start\":37633},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219617574},\"end\":38320,\"start\":37933},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":232148117},\"end\":38617,\"start\":38322},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14395783},\"end\":38888,\"start\":38619},{\"attributes\":{\"doi\":\"10.1007/s11263-021-01484-6\",\"id\":\"b8\",\"matched_paper_id\":235186925},\"end\":39320,\"start\":38890},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":56177594},\"end\":39829,\"start\":39322},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":219635494},\"end\":40196,\"start\":39831},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206845618},\"end\":40445,\"start\":40198},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":54115081},\"end\":40903,\"start\":40447},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":20416090},\"end\":41285,\"start\":40905},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":545361},\"end\":41614,\"start\":41287},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6242669},\"end\":41934,\"start\":41616},{\"attributes\":{\"doi\":\"10.1109/TVCG.2020.3023634\",\"id\":\"b16\",\"matched_paper_id\":221841701},\"end\":42413,\"start\":41936},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233210013},\"end\":42768,\"start\":42415},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":238583434},\"end\":43149,\"start\":42770},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":245131636},\"end\":43473,\"start\":43151},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":46936649},\"end\":43798,\"start\":43475},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":233444160},\"end\":44213,\"start\":43800},{\"attributes\":{\"id\":\"b22\"},\"end\":44451,\"start\":44215},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":228083838},\"end\":44730,\"start\":44453},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":220647181},\"end\":45069,\"start\":44732},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":237278155},\"end\":45337,\"start\":45071},{\"attributes\":{\"doi\":\"10.1109/LRA.2021.3058072\",\"id\":\"b26\",\"matched_paper_id\":232071428},\"end\":45672,\"start\":45339},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":59336296},\"end\":45989,\"start\":45674},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206594692},\"end\":46288,\"start\":45991},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":232352874},\"end\":46687,\"start\":46290},{\"attributes\":{\"id\":\"b30\"},\"end\":46946,\"start\":46689},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":54482591},\"end\":47201,\"start\":46948},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":57759271},\"end\":47582,\"start\":47203},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":227254672},\"end\":48045,\"start\":47584},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11446141},\"end\":48478,\"start\":48047},{\"attributes\":{\"doi\":\"10.1109/TRO.2015.2463671\",\"id\":\"b35\",\"matched_paper_id\":206775100},\"end\":48810,\"start\":48480},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":977535},\"end\":49172,\"start\":48812},{\"attributes\":{\"id\":\"b37\"},\"end\":49448,\"start\":49174},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":131775376},\"end\":49771,\"start\":49450},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":49416362},\"end\":50104,\"start\":49773},{\"attributes\":{\"id\":\"b40\"},\"end\":50469,\"start\":50106},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":225039882},\"end\":51105,\"start\":50471},{\"attributes\":{\"doi\":\"10.52396/JUSTC-2023-0061\",\"id\":\"b42\"},\"end\":51181,\"start\":51107},{\"attributes\":{\"id\":\"b43\"},\"end\":51216,\"start\":51183}]", "bib_title": "[{\"end\":36138,\"start\":36084},{\"end\":36519,\"start\":36479},{\"end\":36865,\"start\":36770},{\"end\":37325,\"start\":37262},{\"end\":37702,\"start\":37633},{\"end\":37993,\"start\":37933},{\"end\":38410,\"start\":38322},{\"end\":38660,\"start\":38619},{\"end\":38945,\"start\":38890},{\"end\":39427,\"start\":39322},{\"end\":39920,\"start\":39831},{\"end\":40268,\"start\":40198},{\"end\":40482,\"start\":40447},{\"end\":40956,\"start\":40905},{\"end\":41345,\"start\":41287},{\"end\":41670,\"start\":41616},{\"end\":42006,\"start\":41936},{\"end\":42450,\"start\":42415},{\"end\":42861,\"start\":42770},{\"end\":43212,\"start\":43151},{\"end\":43530,\"start\":43475},{\"end\":43868,\"start\":43800},{\"end\":44493,\"start\":44453},{\"end\":44821,\"start\":44732},{\"end\":45138,\"start\":45071},{\"end\":45410,\"start\":45339},{\"end\":45736,\"start\":45674},{\"end\":46035,\"start\":45991},{\"end\":46361,\"start\":46290},{\"end\":46719,\"start\":46689},{\"end\":47013,\"start\":46948},{\"end\":47268,\"start\":47203},{\"end\":47662,\"start\":47584},{\"end\":48110,\"start\":48047},{\"end\":48536,\"start\":48480},{\"end\":48870,\"start\":48812},{\"end\":49519,\"start\":49450},{\"end\":49846,\"start\":49773},{\"end\":50167,\"start\":50106},{\"end\":50545,\"start\":50471}]", "bib_author": "[{\"end\":36149,\"start\":36140},{\"end\":36165,\"start\":36149},{\"end\":36178,\"start\":36165},{\"end\":36193,\"start\":36178},{\"end\":36203,\"start\":36193},{\"end\":36215,\"start\":36203},{\"end\":36229,\"start\":36215},{\"end\":36534,\"start\":36521},{\"end\":36554,\"start\":36534},{\"end\":36570,\"start\":36554},{\"end\":36880,\"start\":36867},{\"end\":36897,\"start\":36880},{\"end\":36911,\"start\":36897},{\"end\":36929,\"start\":36911},{\"end\":36945,\"start\":36929},{\"end\":37336,\"start\":37327},{\"end\":37352,\"start\":37336},{\"end\":37365,\"start\":37352},{\"end\":37375,\"start\":37365},{\"end\":37384,\"start\":37375},{\"end\":37396,\"start\":37384},{\"end\":37716,\"start\":37704},{\"end\":37730,\"start\":37716},{\"end\":38009,\"start\":37995},{\"end\":38023,\"start\":38009},{\"end\":38034,\"start\":38023},{\"end\":38051,\"start\":38034},{\"end\":38065,\"start\":38051},{\"end\":38075,\"start\":38065},{\"end\":38421,\"start\":38412},{\"end\":38432,\"start\":38421},{\"end\":38446,\"start\":38432},{\"end\":38676,\"start\":38662},{\"end\":38685,\"start\":38676},{\"end\":38697,\"start\":38685},{\"end\":38707,\"start\":38697},{\"end\":38962,\"start\":38947},{\"end\":38978,\"start\":38962},{\"end\":38991,\"start\":38978},{\"end\":39003,\"start\":38991},{\"end\":39013,\"start\":39003},{\"end\":39027,\"start\":39013},{\"end\":39044,\"start\":39027},{\"end\":39054,\"start\":39044},{\"end\":39439,\"start\":39429},{\"end\":39453,\"start\":39439},{\"end\":39468,\"start\":39453},{\"end\":39487,\"start\":39468},{\"end\":39502,\"start\":39487},{\"end\":39523,\"start\":39502},{\"end\":39934,\"start\":39922},{\"end\":39946,\"start\":39934},{\"end\":39954,\"start\":39946},{\"end\":39959,\"start\":39954},{\"end\":40285,\"start\":40270},{\"end\":40297,\"start\":40285},{\"end\":40501,\"start\":40484},{\"end\":40516,\"start\":40501},{\"end\":40535,\"start\":40516},{\"end\":40548,\"start\":40535},{\"end\":40562,\"start\":40548},{\"end\":40582,\"start\":40562},{\"end\":40595,\"start\":40582},{\"end\":40611,\"start\":40595},{\"end\":40624,\"start\":40611},{\"end\":40641,\"start\":40624},{\"end\":40971,\"start\":40958},{\"end\":40982,\"start\":40971},{\"end\":40993,\"start\":40982},{\"end\":41002,\"start\":40993},{\"end\":41017,\"start\":41002},{\"end\":41031,\"start\":41017},{\"end\":41043,\"start\":41031},{\"end\":41365,\"start\":41347},{\"end\":41378,\"start\":41365},{\"end\":41394,\"start\":41378},{\"end\":41406,\"start\":41394},{\"end\":41685,\"start\":41672},{\"end\":41695,\"start\":41685},{\"end\":41718,\"start\":41695},{\"end\":41724,\"start\":41718},{\"end\":42022,\"start\":42008},{\"end\":42035,\"start\":42022},{\"end\":42050,\"start\":42035},{\"end\":42067,\"start\":42050},{\"end\":42080,\"start\":42067},{\"end\":42091,\"start\":42080},{\"end\":42106,\"start\":42091},{\"end\":42468,\"start\":42452},{\"end\":42492,\"start\":42468},{\"end\":42507,\"start\":42492},{\"end\":42525,\"start\":42507},{\"end\":42539,\"start\":42525},{\"end\":42879,\"start\":42863},{\"end\":42894,\"start\":42879},{\"end\":42910,\"start\":42894},{\"end\":42922,\"start\":42910},{\"end\":43229,\"start\":43214},{\"end\":43245,\"start\":43229},{\"end\":43258,\"start\":43245},{\"end\":43272,\"start\":43258},{\"end\":43286,\"start\":43272},{\"end\":43296,\"start\":43286},{\"end\":43548,\"start\":43532},{\"end\":43565,\"start\":43548},{\"end\":43581,\"start\":43565},{\"end\":43600,\"start\":43581},{\"end\":43884,\"start\":43870},{\"end\":43901,\"start\":43884},{\"end\":43920,\"start\":43901},{\"end\":43937,\"start\":43920},{\"end\":43953,\"start\":43937},{\"end\":44260,\"start\":44250},{\"end\":44275,\"start\":44260},{\"end\":44293,\"start\":44275},{\"end\":44307,\"start\":44293},{\"end\":44322,\"start\":44307},{\"end\":44510,\"start\":44495},{\"end\":44524,\"start\":44510},{\"end\":44539,\"start\":44524},{\"end\":44836,\"start\":44823},{\"end\":44850,\"start\":44836},{\"end\":44858,\"start\":44850},{\"end\":45151,\"start\":45140},{\"end\":45167,\"start\":45151},{\"end\":45423,\"start\":45412},{\"end\":45435,\"start\":45423},{\"end\":45451,\"start\":45435},{\"end\":45752,\"start\":45738},{\"end\":45763,\"start\":45752},{\"end\":45779,\"start\":45763},{\"end\":46049,\"start\":46037},{\"end\":46064,\"start\":46049},{\"end\":46078,\"start\":46064},{\"end\":46088,\"start\":46078},{\"end\":46371,\"start\":46363},{\"end\":46383,\"start\":46371},{\"end\":46392,\"start\":46383},{\"end\":46400,\"start\":46392},{\"end\":46412,\"start\":46400},{\"end\":46425,\"start\":46412},{\"end\":46438,\"start\":46425},{\"end\":46451,\"start\":46438},{\"end\":46733,\"start\":46721},{\"end\":46758,\"start\":46733},{\"end\":46765,\"start\":46758},{\"end\":47029,\"start\":47015},{\"end\":47039,\"start\":47029},{\"end\":47280,\"start\":47270},{\"end\":47291,\"start\":47280},{\"end\":47303,\"start\":47291},{\"end\":47316,\"start\":47303},{\"end\":47332,\"start\":47316},{\"end\":47339,\"start\":47332},{\"end\":47679,\"start\":47664},{\"end\":47697,\"start\":47679},{\"end\":47714,\"start\":47697},{\"end\":47730,\"start\":47714},{\"end\":47745,\"start\":47730},{\"end\":47761,\"start\":47745},{\"end\":48123,\"start\":48112},{\"end\":48136,\"start\":48123},{\"end\":48154,\"start\":48136},{\"end\":48170,\"start\":48154},{\"end\":48186,\"start\":48170},{\"end\":48205,\"start\":48186},{\"end\":48554,\"start\":48538},{\"end\":48569,\"start\":48554},{\"end\":48584,\"start\":48569},{\"end\":48884,\"start\":48872},{\"end\":48905,\"start\":48884},{\"end\":48924,\"start\":48905},{\"end\":48936,\"start\":48924},{\"end\":48947,\"start\":48936},{\"end\":49189,\"start\":49176},{\"end\":49203,\"start\":49189},{\"end\":49536,\"start\":49521},{\"end\":49549,\"start\":49536},{\"end\":49567,\"start\":49549},{\"end\":49580,\"start\":49567},{\"end\":49862,\"start\":49848},{\"end\":49876,\"start\":49862},{\"end\":49886,\"start\":49876},{\"end\":50182,\"start\":50169},{\"end\":50197,\"start\":50182},{\"end\":50211,\"start\":50197},{\"end\":50223,\"start\":50211},{\"end\":50235,\"start\":50223},{\"end\":50567,\"start\":50547},{\"end\":50580,\"start\":50567},{\"end\":50602,\"start\":50580},{\"end\":50620,\"start\":50602},{\"end\":50634,\"start\":50620},{\"end\":50654,\"start\":50634},{\"end\":50672,\"start\":50654},{\"end\":50691,\"start\":50672},{\"end\":50706,\"start\":50691},{\"end\":50721,\"start\":50706},{\"end\":50738,\"start\":50721},{\"end\":50752,\"start\":50738},{\"end\":51113,\"start\":51109}]", "bib_venue": "[{\"end\":36267,\"start\":36229},{\"end\":36608,\"start\":36570},{\"end\":37009,\"start\":36971},{\"end\":37434,\"start\":37396},{\"end\":37768,\"start\":37730},{\"end\":38113,\"start\":38075},{\"end\":38451,\"start\":38446},{\"end\":38740,\"start\":38707},{\"end\":39099,\"start\":39080},{\"end\":39561,\"start\":39523},{\"end\":39997,\"start\":39959},{\"end\":40311,\"start\":40297},{\"end\":40658,\"start\":40641},{\"end\":41081,\"start\":41043},{\"end\":41428,\"start\":41406},{\"end\":41762,\"start\":41724},{\"end\":42167,\"start\":42131},{\"end\":42577,\"start\":42539},{\"end\":42944,\"start\":42922},{\"end\":43301,\"start\":43296},{\"end\":43622,\"start\":43600},{\"end\":43991,\"start\":43953},{\"end\":44248,\"start\":44215},{\"end\":44577,\"start\":44539},{\"end\":44880,\"start\":44858},{\"end\":45189,\"start\":45167},{\"end\":45499,\"start\":45475},{\"end\":45817,\"start\":45779},{\"end\":46126,\"start\":46088},{\"end\":46473,\"start\":46451},{\"end\":46803,\"start\":46765},{\"end\":47066,\"start\":47039},{\"end\":47377,\"start\":47339},{\"end\":47799,\"start\":47761},{\"end\":48243,\"start\":48205},{\"end\":48637,\"start\":48608},{\"end\":48969,\"start\":48947},{\"end\":49307,\"start\":49203},{\"end\":49597,\"start\":49580},{\"end\":49924,\"start\":49886},{\"end\":50273,\"start\":50235},{\"end\":50779,\"start\":50752},{\"end\":51190,\"start\":51185}]"}}}, "year": 2023, "month": 12, "day": 17}