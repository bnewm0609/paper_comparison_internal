{"id": 235265672, "updated": "2023-10-06 02:43:21.67", "metadata": {"title": "Discovering Diverse Nearly Optimal Policies with Successor Features", "authors": "[{\"first\":\"Tom\",\"last\":\"Zahavy\",\"middle\":[]},{\"first\":\"Brendan\",\"last\":\"O'Donoghue\",\"middle\":[]},{\"first\":\"Andre\",\"last\":\"Barreto\",\"middle\":[]},{\"first\":\"Volodymyr\",\"last\":\"Mnih\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Flennerhag\",\"middle\":[]},{\"first\":\"Satinder\",\"last\":\"Singh\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose Diverse Successive Policies, a method for discovering policies that are diverse in the space of Successor Features, while assuring that they are near optimal. We formalize the problem as a Constrained Markov Decision Process (CMDP) where the goal is to find policies that maximize diversity, characterized by an intrinsic diversity reward, while remaining near-optimal with respect to the extrinsic reward of the MDP. We also analyze how recently proposed robustness and discrimination rewards perform and find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose new explicit diversity rewards that aim to minimize the correlation between the Successor Features of the policies in the set. We compare the different diversity mechanisms in the DeepMind Control Suite and find that the type of explicit diversity we are proposing is important to discover distinct behavior, like for example different locomotion patterns.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2106.00669", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-00669", "doi": null}}, "content": {"source": {"pdf_hash": "032731295fb9434a82b31fdb5e50309a2cbfef3d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.00669v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d26bb993a240aeeb035b66522bef0e4466d06cb9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/032731295fb9434a82b31fdb5e50309a2cbfef3d.txt", "contents": "\nDiscovering Diverse Nearly Optimal Policies with Successor Features\n\n\nTom Zahavy tomzahavy@deepmind.com \nDeepMind\nLondon\n\nBrendan O&apos;donoghue bodonoghue@deepmind.com \nDeepMind\nLondon\n\nAndre Barreto andrebarreto@deepmind.com \nDeepMind\nLondon\n\nVolodymyr Mnih vmnih@deepmind.com \nDeepMind\nLondon\n\nSebastian Flennerhag flennerhag@deepmind.com \nDeepMind\nLondon\n\nSatinder Singh \nDeepMind\nLondon\n\nDiscovering Diverse Nearly Optimal Policies with Successor Features\n\nFinding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose Diverse Successive Policies, a method for discovering policies that are diverse in the space of Successor Features, while assuring that they are near optimal. We formalize the problem as a Constrained Markov Decision Process (CMDP) where the goal is to find policies that maximize diversity, characterized by an intrinsic diversity reward, while remaining near-optimal with respect to the extrinsic reward of the MDP. We also analyze how recently proposed robustness and discrimination rewards perform and find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose new explicit diversity rewards that aim to minimize the correlation between the Successor Features of the policies in the set. We compare the different diversity mechanisms in the DeepMind Control Suite and find that the type of explicit diversity we are proposing is important to discover distinct behavior, like for example different locomotion patterns.Preprint. Under review.\n\nIntroduction\n\nCreative problem solving is the mental process of searching for an original and previously unknown solution to a problem [29]. The relationship between creativity and intelligence is widely recognized across many fields; for example, in the field of Mathematics, finding different proofs to the same theorem is considered elegant and often leads to new insights.\n\nCloser to Artificial Intelligence (AI), consider the field of game playing and specifically the game of Chess in which a move is considered creative when it goes beyond known patterns [14]. In some cases, such moves can only be detected by human players while remaining invisible to currently state-of-the-art Chess engines. A famous example thereof is the winning move in game eight of the Classical World Chess Championship 2004 between Leko and Kramnik [6]. Humans and indeed many animals employ similarly creative behavior on a daily basis; faced with a challenging problem we often consider qualitatively different alternative solutions.\n\nYet, the majority of AI research is focused on finding a single best solution to a given problem. For example, in the field of Reinforcement Learning (RL), most algorithms are designed to find a single reward-maximizing policy. However, for many problems of interest there may be many qualitatively different optimal or near-optimal policies; finding such diverse set of policies may help an RL agent become more robust to changes in the task and/or environment and to generalize better to future tasks.\n\nIn the field of Quality-Diversity (QD), evolutionary algorithms are used to find useful diverse policies (e.g., [33,28,21,25,30,18,31,48]). In a related line of work, intrinsic rewards are used to find diverse skills for fast adaptation [19,16] to be robust to model miss-specification [24,45] and for exploration [2]. It was also suggested that policies that maximize diversity are more correlated with human behaviour than those that maximize only the extrinsic reward [26]. This work makes the following contributions. First, we propose an incremental method for discovering a diverse set of near-optimal policies. Each policy in the set is trained to solve a Constrained Markov Decision Process (CMDP). The main objective in the CMDP is to maximize the diversity of the growing set, measured in the space of Successor Features [SFs ; 4], and the constraint is that the policies are near-optimal. Second, we analyze how previously proposed robustness and discrimination mechanisms for the \"no-reward\" setting perform in terms of diversity in our setup. We find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose two explicit diversity rewards that aim to minimize the correlation between the SFs of the policies in the set. Third, we demonstrate our method in the DeepMind Control Suite [40]. Given an extrinsic reward (e.g. for standing or walking) our method discovers qualitatively diverse locomotion behaviours for approximately maximizing this reward.\n\n\nPreliminaries and Notation\n\nAn MDP [34] is a tuple M (S, A, P, r, \u03b3, \u03c1), where S is the set of states, A is the set of actions, P = {P a | a \u2208 A} is the set of transition kernels, \u03b3 \u2208 [0, 1) is the discount factor and \u03c1 is the initial state distribution. The function r : S \u00d7 A \u2192 R defines the rewards. A policy in M , denoted by \u03c0, is a mapping \u03c0 : S \u2192 P(A), where P(A) is the probability distributions over A.\n\nUsually in RL, the agent's objective is to maximize the expected cumulative extrinsic reward. In this work, we will also be interested in discovering and maximizing intrinsic reward functions [35]. These rewards can be a function of the policy (e.g., its entropy) or a function of observed features. Let \u03c6(s, a) \u2208 [0, 1] d be an observable vector of bounded features. Then there is a set of rewards induced by all possible linear combinations of the features \u03c6. Specifically, for any w \u2208 R d , we can define a reward function r w (s, a) = w \u00b7 \u03c6(s, a). Given w, the intrinsic reward r w is well defined and we will use w and r w interchangeably to refer to it. Any policy induces a state transition matrix P \u03c0 , where P \u03c0 (x, y) = P \u03c0(x) (x, y) is the probability of transitioning from state x to state y when the action is selected according to \u03c0(x). Thus, any policy yields a Markov chain (S, P \u03c0 ). By looking at the Markov chain induced by a policy we can study its long-term behavior, such as its stationary distribution. This in turn allows us to define a notion of diversity based on the limiting behavior of policies, in contrast with most previous work on diversity that focus on short-term behavior [19,16].\n\nConcretely, in defining diversity we use measures defined in the average-case setting. The stationary distribution d \u03c0 of a Markov chain with transition matrix P \u03c0 is defined to be d \u03c0 (s) = lim t\u2192\u221e Pr(s t = s|s 0 \u223c \u03c1, \u03c0), which we assume exists and is independent of s 0 for all policies. In ergodic MDPs this limit is unique and is known to be the probability distribution satisfying d \u03c0 = d \u03c0 P \u03c0 [34]. The asymptotic average reward value, hereafter simply value, of a policy \u03c0 under reward function r, denoted v \u03c0 r , can be defined as an expectation over d \u03c0 as: v \u03c0 r = E s\u223cd\u03c0 r(s, \u03c0(s)) = d \u03c0 \u00b7 r \u03c0 , where r \u03c0 is a vector with E a\u223c\u03c0(s) r(s, a) in its coordinates. A natural time scale in this long-term average-case context is the mixing time of the policy -the time until the Markov chain is \"close\" to its stationary state distribution. Formally, the -mixing time T mix of an ergodic Markov chain with a stationary distribution d \u03c0 is the smallest time t such that \u2200x 0 , TV[Pr t (\u00b7|x 0 ), d \u03c0 ] \u2264 , where Pr t (\u00b7|x 0 ) is the distribution over states after t steps, starting from x 0 and TV[\u00b7, \u00b7] is the total variation distance. In other words, if we follow a policy in an MDP for T mix steps, we will observe states that are approximately distributed according to d \u03c0 .\n\nSimilarly, we can define the expected features, also known as successor features, under d \u03c0 as \u03c8 \u03c0 = E x\u223cd\u03c0 \u03c6(x, \u03c0(x)). Note that the SFs are conditioned on \u03c1 and \u03c0 and that they are vectors in R d ; similar definitions were suggested in [27,44]. For linear rewards there is a simple way to express the average reward value of the policy (Section 2) using the SFs: v \u03c0 w = \u03c8 \u03c0 \u00b7 w. To keep the notation simple, we will refer to the SFs of policy \u03c0 i as \u03c8 i ; and, since we are dealing with different intrinsic rewards, we will use the notation v i d to refer to the value of policy \u03c0 i for reward r d .\n\n\nDiscovering diverse near-optimal policies\n\nWe are interested in discovering a set of n near-optimal policies \u03a0 n = {\u03c0 i } n i=1 that are maximally diverse according to some diversity metric. Let \u03a8 n be the set of SFs corresponding to the policies in \u03a0 n , then we are interested in solving the following constrained optimization problem: max\n\u03a0 n Diversity(\u03a8 n ) s.t v \u03c0 e \u2265 \u03b1v * e , \u2200\u03c0 \u2208 \u03a0 n ,(1)\nwhere Diversity : {R d } n \u2192 R measures the diversity of a set of SFs (\u03a8 n ) that we shall define shortly, and the constraint requires that all the policies in \u03a0 n achieve value better than a parameter \u03b1 \u2208 [0, 1] times the value of the optimal policy (here v \u03c0 e is the value of policy \u03c0 for extrinsic reward r e and v * e is the value of the optimal policy with respect to r e ). Note that \u03b1 controls how big a space of policies we search over for our diverse set of policies. In general, the smaller the \u03b1 parameter the larger the set of \u03b1-optimal policies and thus the greater the diversity of the policies found in \u03a0 n1 .\n\nAlgorithm 1 Diverse Successive Policies 1: Input: mechanism to compute rewards r e and r d .\n2: Initialize: \u03c0 0 \u2190 arg max \u03c0\u2208\u03a0 r e \u00b7 d \u03c0 , 3: v * e = v \u03c0 0 , \u03a0 0 = {\u03c0 0 } 4: for i = 1, . . . , T do 5: Compute diversity reward r i d = D(\u03a8 i\u22121 ) 6: \u03c0 i = arg max \u03c0 d \u03c0 \u00b7 r i d s.t. d \u03c0 \u00b7 r e \u2265 \u03b1v * e 7:\nEstimate the SFs \u03c8 i of the policy \u03c0 i 8:\n\u03a0 i = \u03a0 i\u22121 \u222a {\u03c0 i }, \u03a8 i = \u03a8 i\u22121 \u222a {\u03c8 i } 9: end for 10: return \u03a0 T\nCommon to many approaches is to define a diversity objective using intrinsic rewards [19,16,24,47], i.e., rewards not from the environment but defined by the agent itself. Our approach also uses intrinsic rewards to induce diversity, as we describe in Algorithm 1. The algorithm receives as input two reward functions r e and r d , which together define a CMDP. The reward r d corresponds to a diversity intrinsic reward. We will discuss five different candidate r d 's. The constraint reward r e will typically be the extrinsic reward, but we will also consider two alternative choices for r e . In the initialization step of Algorithm 1 (line 2) there are no policies in the set, and so the goal of the first policy \u03c0 0 is to solve the MDP with reward r e . Algorithm 1 then adds \u03c0 0 and its SFs to the set, and the variable v * e is set to be v 0 . v * e defines the near-optimality constraint \u03b1v * e for the other policies (say with \u03b1 = 0.9). After this first step, the algorithm proceeds in iterations. In iteration i, an intrinsic reward r i d is computed given the previous policies in the set \u03a0 i\u22121 . The next policy to be added to the set, \u03c0 i , is the solution to the following Constrained MDP (CMDP) (line 6 in Algorithm 1):\narg max \u03c0 d \u03c0 \u00b7 r i d s.t. d \u03c0 \u00b7 r e \u2265 \u03b1v * e .(2)\nIn words, the new policy optimizes the average intrinsic reward value subject to the constraint that it be near-optimal with respect to its average extrinsic reward value. In Section 5 we discuss the details of how to solve Eq. (2). Clearly, the behavior of Algorithm 1 strongly depends on the choice of r d , the intrinsic reward used to induce diversity. We now discuss five alternatives to define this reward.\n\n\nMeasuring Policy Diversity\n\nA key aspect of our method is the measure of diversity. Our focus is on diverse policies, as measured by their stationary distribution after they have mixed. This suggests we should measure diversity in the space of SFs, as they are defined under the policy's stationary distribution (see Section 2). In contrast, prior work have focused on learning diverse skills, which is often measured before the skill policy mixes. A common approach to measuring skill diversity is to measure skill discrimination in terms of trajectory-specific quantities such as terminal states [19], a mixture of the initial and terminal states [5], or trajectories [16]. An alternative approach that implicitly induces diversity is to learn policies that maximize the robustness of the set \u03a0 n to the worst-possible reward [24,47].\n\nIn Subsections 4.1 and 4.2, we analyze the diversity of these two approaches in the space of SFs and find that they both depend on the initialization of the algorithm and cannot guarantee diversity. Motivated by these findings, we develop two new explicit diversity rewards that aim to minimize the correlation between the SFs of the policies in the set. We discuss these new methods in Section 4.3.\n\n\nDiversity via Discrimination\n\nDiscriminative approaches rely on the intuition that skills should be distinguishable from one another simply by observing the states that they visit. Learning diverse skills is then a matter of learning skills that can be easily discriminated. For instance, DIAYN [16] maximizes the mutual information between skills and states as follows. Given a probability space (\u2126, F, P), we denote by I(S; Z) the mutual information between the random variable state S : \u2126 \u2192 S and latent random variable (skill) Z : \u2126 \u2192 Z [13]. We also use H[A|S] to refer to the conditional entropy of the action random variable A : \u2126 \u2192 A conditioned on state S. Finally, the conditional mutual information between A and Z given S is denoted by I(A; Z|S). Then, the DIAYN objective to be maximized, given a prior over the latents, p, is:\nI(S; Z) + H[A|S] \u2212 I(A; Z|S) = H[A|S, Z] + E z\u223cp(z) s\u223cd \u03c0 z [log p(z|s) \u2212 log p(z)].(3)\nThis is an entropy-regularized objective that seeks to maximize the information that states contain about the skill used to reach it. In particular, the term of interest is E z\u223cp(z),s\u223cd \u03c0 z [log p(z|s) \u2212 log p(z)], which corresponds to the value of a skill in an MDP with reward r(s|z) = log p(z|s) \u2212 log p(z). A skill policy \u03c0(a|s, z) controls the first component of this reward, p(z|s), which measures the probability of identifying the skill in state s. Hence, the policy is rewarded for visiting states that differentiates it from other skills, thereby implicitly encouraging diversity.\n\nThe exact form of p(z|s) depends on how skills are encoded [19]. The most common version is to encode z as a one-hot d-dimensional variable [e.g.; 19,1,16]. Similarly, we represent z as z \u2208 {1, . . . , n} to index n separate policies \u03c0 z . In addition, the concept of finding a small set of meaningful policies is appealing from the interpretability perspective.\n\np(z|s) is typically intractable to compute due to the large state space and is instead approximated via a learned discriminator q \u03c6 (z|s). In our case, we measure p(z|s) under the stationary distribution of the policy; that is, p(s|z) = d \u03c0 z (s). Therefore, for the purpose of analysis, we can find an analytic form for the objective of DIAYN before we apply the variational approximation. Given this, applying Bayes rule to p(z|s) yields\np(z|s) = d \u03c0 z (s)p(z) k d \u03c0 k (s)p(k) .(4)\nAnd in the kernel case, we define a Gibbs distribution\np(z|s) = p(z) exp (\u03c6(s) \u00b7 \u03c8 z ) p(k) exp(\u03c6(s) \u00b7 \u03c8 k ) .(5)\nPlugging p(z|s) from Eq. (4) in the objective of DIAYN, the relevant term in Eq. (3) becomes\nE z\u223cp(z),s\u223cd(\u03c0 z ) [log p(z|s)] = z p(z) s d \u03c0 z (s) log d \u03c0 z (s)p(z) k d \u03c0 k (s)p(k) .(6)\nFinding a policy with maximal value for this reward can be seen as solving an optimization program in d \u03c0 z under the constraint that the solution is a valid stationary state distribution (Section 2). The term s p(s|z) log p(s|z) corresponds to the negative entropy of d \u03c0 z , meaning that the objective to be maximized is convex in d \u03c0 z .\nLemma 1. The function s d \u03c0 z (s) log d \u03c0 z (s)p(z) k d \u03c0 k (s)p(k) is a convex function of d \u03c0 z .\nThe proof can be found in Appendix B; briefly, Lemma 1 holds because the function can be written as KL(d \u03c0 z || k p(k)d \u03c0 k ) + s d \u03c0 z (s) log p(z) and the KL-divergence is jointly convex on both arguments [9,Example 3.19]. The convexity of the objective results from the fact that the intrinsic reward log p(z|s) is a function of the policy. In the standard RL setup, the reward is not a function of the policy and the objective is linear in it, thus, maximizing and minimizing the reward are both convex minimization problems. However, when the reward is a function of the policy, maximization and minimization of the reward are not equivalent optimization problems. In DIAYN, the maximization of log p(z|s) leads to convex maximization while the minimization of the same reward leads to convex minimization. We note that the convexity of the objective has nothing to do with the variational approximation typically used to compute p(z|s); it is encountered with or without it.\n\nThe observation that discriminatory objectives lead to a set of n convex maximization problems in our setting is problematic, since the optimality of the solutions-in particular, their diversity-cannot be guaranteed. From the perspective of the policy set, the algorithm may converge to a set which is a local maxima rather than the global maxima, and therefore result in suboptimal diversity. In practice, different initializations and stochastic updates might mitigate the issue to some degree. In addition, it is possible that all the local maxima are close to optimal. For example, similar observations were made regarding the loss surface of deep neural networks, but the local optima points were shown to be very good in practice [15,11,36], mitigating the issues mentioned above. Thus, we recommend taking Lemma 1 as an observation regarding the optimization landscape of DIAYN which we hope to further explore in future work.\n\n\nDiversity via Robustness\n\nAn alternative approach that implicitly induces diversity is to seek robustness among a set of policies by maximizing the performance w.r.t the worst case reward [24,47]; for fixed n, the goal is:\nmax \u03a0 n \u2286\u03a0 min w\u2208B2 max \u03c0 i \u2208\u03a0 n \u03c8 i \u00b7 w.(7)\nHere B 2 is the 2 unit ball, \u03a0 is the set of all possible policies, \u03a0 n = {\u03c0 1 , . . . , \u03c0 n } is the set of n policies for which we are optimizing. Let us parse this objective term by term. First, the inner product \u03c8 i \u00b7 w yields the expected value under the steady-state distribution (see Section 2) of the policy \u03c0 i . The inner min-max is a two-player zero-sum game, where the minimizing player is finding the worst-case reward function (since weights and reward functions are in a one-to-one correspondence) that minimizes the expected value, and the maximizing player is finding the best policy from the set \u03a0 n (since policies and SFs are in a one-to-one correspondence) to maximize the value. The outer maximization is to find the best set of n policies that the maximizing player can use.\n\nIntuitively speaking, the solution \u03a0 n to this problem might be a diverse set of policies since a non-diverse set is likely to yield a low value of the game, that is, it would easily be exploited by the minimizing player. In this way diversity and robustness are dual to each other, in the same way as a diverse financial portfolio is more robust to risk than a heavily concentrated one. By forcing our policy set to be robust to an adversarially chosen reward it will be diverse.\n\nIn [24], the authors proposed a solution to Eq. (7) using a CMDP with r d as discrimination (via DIAYN) and r e is the extrinsic reward; we discuss it in more detail in Section 5. In [47], the authors proposed an iterative solution to Eq. (7) that incrementally adds policies to a solution set \u03a0 n (Algorithm 2 in the appendix). The authors define a Set Max Policy (SMP) as a policy that takes a set of policies and a reward as inputs and returns the best policy in the set for this reward. In each iteration, the algorithm computes the worst case reward w.r.t to the SMP, finds the policy that maximizes it, and adds it to the set. In iteration n The value of the SMP on the set \u03a0 n is defined as v n = min w\u2208B2 max \u03c0 i \u2208\u03a0 n \u03c8 i \u00b7 w, and it is guaranteed that this value strictly increases v n+1 > v n in each iteration until the optimal solution is found. The following Lemma suggests that this procedure is equivalent to a fully corrective FW [17] algorithm on the function f = || \u00b7 || 2 . As a consequence, it is guaranteed to convergence to the optimal solution in a linear rate [22]. Lemma 2. The iterative procedure in [47] is equivalent to a fully corrective FW algorithm to minimize the function f = ||\u03c8 \u03c0 || 2 . As a consequence, to achieve an \u2212optimal solution, the algorithm requires at most O(log(1/ )) iterations.\n\nThe proof in Appendix C suggests that the SMP policy is equivalent to the fully corrective search (maintaining a dictionary of solutions from previous iterations and choosing the best convex combination). The only difference between the two algorithms is that one of them solves a max-min problem where the other solves the equivalent min-max problem, and therefore they are guaranteed to have the same iterations from strong duality. Unfortunately this approach, like the discriminative approaches, has a weakness that can limit the ultimate diversity in the set. To see this note that\nmax \u03a0 n \u2286\u03a0 min w\u2208B2 max \u03c0 i \u2208\u03a0 n \u03c8 i \u00b7 w \u2264 min w\u2208B2 max \u03c0 i \u2208\u03a0 \u03c8 i \u00b7 w = max \u03c0 i \u2208\u03a0 min w\u2208B2 \u03c8 i \u00b7 w = \u2212 min \u03c0 i \u2208\u03a0 \u03c8 i def = v * ,\nwhere the inequality comes from the fact that \u03a0 n \u2286 \u03a0, and the first equality uses von Neumann's minimax theorem [42]. If we let \u03c0 * = arg min \u03c0 i \u2208\u03a0 \u03c8 i , then if \u03a0 n = {\u03c0 * } we have an optimal policy set for the game, since we have found a policy set that achieves the known upper bound on the value of the game, v * . In other words a single policy is a sufficient solution for Eq. (7), which is problematic since the goal was to build up a set of many diverse policies. Similar to the discriminative approaches, in practice we obtain more policies by initializing the set away from \u03c0 * , or alternatively restricting \u03a0 n to deterministic policies. However, this issue likely explains the empirical observations in [47] that there are only a few active policies in the optimal sets.\n\nNote that the results above hold only in the case that \u03a0 is the set of all the stochastic policies in the MDP; if only deterministic policies are used, we cannot apply the von Neumann's minimax theorem. This is not an issue since we are interested in stochastic policies for multiple reasons: optimal solutions to CMDPs are stochastic policies [3] and stochastic policies are the most common approach in continuous control tasks, which is the focus of our experiments.\n\n\nExplicit diversity methods\n\nThe two diversity mechanisms we have discussed so far were designed to maximize robustness or discrimination. Each one has its own merits in terms of diversity, but since they do not explicitly maximize a diversity measure they cannot guarantee that the resulting set of policies will be diverse. We now propose two reward signals designed to induce a diverse set of policies. The way they do so is to leverage the information about the policies' long-term behavior available in their SFs. Both rewards are based on the intuition that the correlation between SFs should be minimized.\n\nTo motivate this approach, we note that SFs can be seen as a compact representation of a policy's stationary distribution. This becomes clear when we consider the case of a finite MDP with |S|dimensional \"one-hot\" feature vectors \u03c6 whose elements encode the states: \u03c6 i (s) = I{s = i}, where I{ \u00b7} is the indicator function. In this special case the SFs of a policy \u03c0 coincide with its stationary distribution, that is, \u03c8 \u03c0 = d \u03c0 . Under this interpretation, minimizing the correlation between SFs intuitively corresponds to encouraging the associated policies to visit different regions of the state space-which in turn leads to diverse behavior. As long as we assume the tasks of interest are linear combinations of the features \u03c6 \u2208 R d , which we do, similar reasoning applies when d < |S|.\n\nBut how do we compute policies in order to minimize the correlation between their SFs? To answer this question, we first consider the extreme scenario where there is a single policy \u03c0 k in the set \u03a0. In this case the objective is: max \u03c8 z \u03c8 z \u00b7 w, where w = \u2212\u03c8 k . Solving this problem is an RL problem whose reward is linear in the features weighted by w. A similar objective was investigated in [20], but there w was sampled i.i.d from a fixed prior. The question we are trying to address is: how to define w taking into account multiple policies in the set \u03a0 n ?\n\nWe propose two answers to this question. The first one is to have w be the negative average of the SFs of the policies currently in the set, that is, w = \u2212 1 k \u03c8 k . This formulation is useful as it measures the sum of negative correlations within the set. However, when two policies in the set happen to have the same SFs with opposite signs, they cancel each other, and do not impact the diversity measure. This diversity objective shares some similarities with the novelty search algorithm in [12], where the mean pairwise distance between the current policy and an archive of other policies is used.\n\nThe second diversity-inducing reward we propose addresses this issue. It is defined as the minimum over the SFs in each state: r(s) = min k \u03c6(s) \u00b7 \u2212\u03c8 k . This objective encourages the policy to have the largest \"margin\" from the set, as it maximizes the negative correlation from the element that is \"closest\" to it. This objectives shares some similarities with a recent work [30] that uses the determinant of the kernel matrix and penalizes it to the closest agents in the population, building on ideas from Determinantal point processes [23]. Finally, we note that we also apply a non linear transformation to bound both of these rewards; the details are in the supplementary (Appendix D).\n\n\nSolving the constrained MDP\n\nAt the core of our approach is the solution of a CMDP. The literature on CMDPs is quite vast and we refer the reader to [3] and [39] for treatments of the subject at different levels of abstraction. In this work we will focus on a reduction of CMDPs to MDPs via gradient updates. The idea is to look at the Lagrangian of Eq. (2):\nL(\u03c0, \u03bb) = \u2212d \u03c0 \u00b7 (r d + \u03bbr e ) \u2212 \u03bb\u03b1v * e .(8)\nThen, solving the CMDP in Eq. (2) is equivalent to solving min \u03c0\u2208\u03a0 max \u03bb\u22650 L(\u03c0, \u03bb).\n\nSolving CMDPs via Lagrangian methods dates back to [8,7]; more recently the problem has been tackled using Deep RL techniques [41,10]. These algorithms perform primal-dual gradient updates on the min-max game. When the value function of the policy satisfies the constraint, the Lagrange multiplier will decrease, putting more emphasis on the extrinsic reward; when the constraint is not satisfied, the Lagrange multiplier will increase to satisfy the constraint.\n\nNon linear Lagrange multiplier. We would like our agent to optimize a bounded reward signal, and we discuss how to bound each reward r d in the supplementary (Appendix D). To guarantee that a combination of two bounded rewards remains bounded, it is sufficient to combine them via a convex combination. To achieve that, we use a Sigmoid activation on the Lagrange multiplier so the reward is a convex combination of the diversity and the extrinsic rewards:\n\nr(s) = \u03c3(\u03bb)r e (s) + (1 \u2212 \u03c3(\u03bb))r d (s).\n\nWe further introduce an entropy regularization on \u03bb to prevent \u03c3(\u03bb) from getting to extreme values (1 or 0), where the Sigmoid activation is saturated and has low gradients. This can happen, for example, at the beginning of learning where the agent's policy is sub-optimal and does not satisfy the constraint for many iterations. The objective for \u03bb is thus:\nf (\u03bb) = \u03c3(\u03bb)(v \u2212 \u03b1v * e ) \u2212 a h H(\u03c3(\u03bb)),(9)\nwhere H is the entropy function, a h is the weight of the entropy regularization andv is an estimate of the total cumulative extrinsic return that the agent obtained in recent trajectories. The Lagrangian \u03bb is updated by performing gradient descent on Eq. (9) every N \u03bb agent steps.\n\nEstimation of average rewards. Another important step of Algorithm 1 which is not directly related to solving the CMDP is the estimation of the average rewards. For that, we used a simple Monte Carlo estimates:\u1e7d j = 1 T T t=1 r t , i.e, the empirical average reward obtained by the agent in trajectory j (where T = 1000). We used the same estimator to estimate the average SFs (replace r t with \u03c6 t ).\n\nThe value\u1e7d j is a good estimate of the average reward, but it is not perfect. The issue is that the trajectory is of finite length, and therefore the samples in the beginning of the trajectory, before the policy is mixed, are biased. Our experiments are in the DM control suite [40] where the mixing time is small; the policies we discover roughly mix after \u223c 50 steps (as can be seen in the videos in the supplementary). Since the mixing time is much shorter than T , the effect of the biased samples is small (\u223c 5%). It is also possible to wait until the policy is mixed or to collect a perfect unbiased estimate of the average reward via Coupling From the Past procedure [32] as was done in [44]. Note that this is a known issue with any practical policy gradient method but was not found to make a big difference empirically.\n\nWe further average the estimate using a running average with decay factor of a d :v j = a dvj\u22121 + (1 \u2212 a d )\u1e7d j ; this is the estimate we use in Eq. (9). The running average variables are set to 0 between iterations of Algorithm 1. Finally, we note here that we also experimented with the discounted criteria (discounted SFs). In that case, we observed that there is too much emphasis on the features that are observed at the beginning of the trajectory, resulting in less diversity across the entire trajectory.\n\n\nDiscussion.\n\nA different feasible approach to combine r d and r e is to model the problem as a multiobjective MDP. That is, the diversity objective is added to the main one via a fixed, stationary weighting of the two rewards, e.g., r = a 1 r d + a 2 r e . We note that the solution of such a multiobjective MDP cannot be be a solution to a CMDP. I.e., it is not possible to find the optimal dual variables \u03bb * , plug them in Eq. (8) and simply solve the resulting (unconstrained) MDP. Such an approach ignores the fact the dual variables must be a 'best-response' to the policy and is referred to as the \"scalarization fallacy\" in [39,Section 4].\n\nWhile Multi objective MDPs have been used in prior QD-RL papers [21,25,30,18,31,48], we now outline a few potential advantages for using CMDPs. First, the CMDP formulation guarantees that the policies that we find are near optimal (satisfy the constraint). Secondly, the weighting coefficient in multi-objective MDPs has to be tuned, while in our case it is being adapted over time. This is particularly important in the context of maximizing diversity while satisficing reward. In many cases, as we observed in our experiments, the diversity reward might have no other option other than being the negative of the extrinsic reward. In these cases our algorithm will return good policies that are not diverse, while a solution to multi-objective MDP might fluctuate between the two objectives and not be useful at all.\n\nCMDPs in related QD papers. Kumar et al. [24] proposed that solving a CMDP with r d as discrimination reward and r e as the extrinsic reward will lead to a solution to the robustness objective (Eq. (7)). Sun et al. [38] also investigated CMDPs, but focused on the setup where the diversity reward has to satisfy a constraint, so the diversity reward is r e and the extrinsic reward is r d . But most importantly, we use a different method to solve CMDPs, which is based on Lagrange multipliers and SFs and is justified from CMDP theory [3,8,7], while these other two papers use techniques that are not guaranteed to solve CMDPs.\n\n\nExperiments\n\nWe conducted our experiments on domains from the DM Control Suite [40], standard continuous control locomotion tasks where diverse near-optimal policies should naturally correspond to different gaits. We focused on the setup where the agent is learning from feature observations corresponding to the positions and velocities of the body joints being controlled by the agent. Due to space considerations, we focus on domains where the diversity is interesting from a visual point of view, and in particular on Walker and Dog. In simpler domains like Cartpole and Reacher, we observed simple symmetric diversity -one policy moves a certain way clockwise and then the second policy moves in the same way anti-clockwise (see Fig. 4 in the supplementary). Later policies in the set are less distinguishable visually but can learn, for example, to balance the pole while moving. Note that without a diversity mechanism, the agent tends to only move in a single direction (e.g. clockwise).\n\nIn most of our experiments, the extrinsic reward r e , which defines the optimality constraint in Algorithm 1, is set to be the environment reward provided by the DM Control Suite . The first policy in the set is trained to only maximize the extrinsic reward, and the other policies has to satisfy the constraint of being \u03b1 = 0.9 optimal w.r.t it. In these experiments, we report the reward that each policy collects in white color on top of each figure. Additionally we report the reward of each policy in a small table in the main text.\n\nIn the QD community, there is no consensus regarding a single metric for measuring diversity, and some argue that there shouldn't be such (see, for example, the book \"Why Greatness Cannot Be Planned\" [37]). Inspired by this literature, we focus on measuring diversity only qualitatively by visualizing the learned policies. We strongly recommend the reader to check our visualization website where we show videos of the trajectories that each policy takes at https://anon98723.github.io/. In addition, we present \"motion figures\" by discretizing the videos (details in the Appendix) that give a fair impression of the policy behaviours. We would like to note that we did not tune our method to maximize diversity based on any metric other than constraint satisfaction (maintaining near-optimality). The main purpose of our experiments are the feasibility of the CMDP framework as proposed in Algorithm 1, i.e., to demonstrate that we discover diverse near-optimal policies.  Fig. 1a presents eight polices that were discovered by Algorithm 1 where r d is the minimum explicit diversity criteria for Walker.stand. As we can see, the policies exhibit different types of standing: standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy visualization). Similar figures for the other diversity mechanisms can be found in the supplementary material (Appendix E.2). We observed that in this domain the Average diversity criterion can also discover policies that behave differently, but they are not as diverse as the ones found using the Minimum criterion (see Appendix E.2 in the supplementary material)  The robustness mechanism can also provide diverse policies, but it tends to converge after a few iterations so no further diversity is achieved by the algorithm after 3 iterations. We also include a figure of different policies with no diversity mechanism in the supplementary (Fig. 9); in this case there is a small amount of diversity from training, but it is much less significant than the diversity we get with a diversity objective. Similarly, the discrimination method exhibits diversity but not as good as the explicit methods. We believe that this is due to the fact that the policies that maximize the extrinsic reward are already discriminative, and the algorithm fails to escape these local minima. In this domain we observed much better diversity with the explicit diversity mechanisms than with robustness or discrimination, see Appendix E.3. We also note that in both of the Walker environments, all (but one) of the discovered policies that we found are indeed near optimal, and satisfy the constraint (which was set to 90%).   Fig. 2a r d is the minimum explicit diversity criteria and in Fig. 2b there is no diversity mechanism. Inspecting Fig. 2b we can see that the dog learns how to stand (different policies are independent of each other so we leave the % blank), but in all cases, it stands with four legs on the ground. On the other hand, in Fig. 2a the dog learns different variations of \"three leg standing\" (lifting one of his legs), and still achieves high reward.\n\nNext, we present results in the no-reward setting, where the agent has no access to the reward from the environment. Our results with None diversity confirm that the implementation of these diversity mechanisms yields complex locomotion in the no-reward setting as was reported in the original papers. However, in more complex domains like Walker, without adding the explicit diversity we get static behaviours that resemble \"Yoga\" exercises, as was also reported, for example, in [47]. Fig. 3a presents results for Walker where r e is robustness and r d is average. Inspecting the results, we can see that the agent discovered complex locomotion skills such as kneeling backwards, crawling and flick-flack jumping. We also report the extrinsic reward for standing as another measure of zero-shot transfer (it was not used during training at all). In Appendix E. 4 we can see that other diversity mechanisms discovered other surprising skills such as \"head walking\".\n\nFinally, Fig. 3b presents results for Cheetah where r e is discrimination and r d is robustness. The cheetah learns to run forward, backwards, and then to do various jumps. While previous methods were able to discover similar behaviours, they are typically not that diverse with such a small set.\n\n(a) Walker, re as robustness and r d as average.\n\n(b) Cheetah, re as discrimination and r d as robustness. \n\n\nConclusion\n\nIn this work we proposed a framework for discovering near optimal diverse behaviours. We framed the problem as solving a CMDP where a diversity intrinsic reward and the extrinsic reward are adaptively combined. There are interesting connections to whitebox metagradients [43,46] -the updates of the Lagrangian can be viewed as the outer update in metagradients where satisfying the constraint is the outer loss. Using metagradients to learn other diversity hyperparameters or even to discover the diversity reward itself [49] are exciting directions for future work. Key to our approach was the idea of measuring diversity in the space of SFs. This design choice allowed us to provide insights on how existing diversity mechanisms behave from the perspective of convex optimization.\n\nThere are many exciting applications for our framework. For example, consider the process of using RL to train a robot to walk. The designer does not know a priori which reward will result in the desired walking pattern. Thus, robotic engineers often train a policy to maximize an initial reward, tweak the reward, and iterate until they reach the desired behaviour. Using our approach, the engineer would have multiple forms of walking to choose from in each attempt, which are also interpretable (linear in the weights).\n\n\nB Proof for Lemma 1\n\nProof. We will focus on the case that there are no zero elements in d \u03c0 which is a standard assumption in ergodic MDPs. Under this assumption f is a twice differentiable function so it is convex if its Hessian is positive semidefinite.\n\nRecall that the prior p(z) is constant, and that the policies k = 1, ..., k = z are also constant from the perspective of d \u03c0 z . We can therefore introduce a simplified notation and write the objective as\ns d \u03c0 z (s) log d \u03c0 z (s)p d \u03c0 z (s)p + c s\nThe variable d \u03c0 z is a vector in the |S|\u2212simplex. We can represent it using |S| \u2212 1 degrees of freedom\nx 1 , ..x s\u22121 \u2208 [0, 1] where the last element is x s = 1 \u2212 |S|\u22121 i=1 x i .\nNotice that x s is a function of x i so it has a derivative with respect to. x i which equals \u22121. So we have\nf (x) = i x i log x i p x i p + c i + x s log x s p x s p + c s\nThe first derivative of this function with respect to\nx i , i \u2208 [1, .., |S| \u2212 1 is \u2202f \u2202x i = log(x i ) + 1 + log(p) \u2212 x i p x i p + ci \u2212 log(x i p + c i ) \u2212 log(x s ) \u2212 1 \u2212 log(p) + x s p x s p + c s + log(x s p + c s ) = log(x i ) \u2212 log(x i p + c i ) \u2212 x i p x i p + ci (10) \u2212 log(x s ) \u2212 log(x s p + c i ) \u2212 x s p x s p + c s(11)\nWe can see that the terms in Eq. (10) depend only on x i and the terms in Eq. (11) depend only on x s . In addition, we will soon see that the derivatives of x s will be equal for any j \u2208 1, ..., s \u2212 1. These two observations imply that the Hessian will have the form of\nH = D + m1,\nwhere D is a diagonal matrix with derivatives of Eq. (10) with respect to x i as it elements, 1 is a matrix of all ones, and m is the derivative of Eq. (11) with respect to x j which we will show to be equal for all j. Notice that \u2200x, we have that\nx T (D + m1)x = D i x 2 i + m( x i ) 2 .\nThis implies that in order for the Hessian to be positive definite, we only need to show that the elements of d and the scalar m are positive. The derivative of Eq. (10) with respect to x i is\n1 x i \u2212 p px i + c i \u2212 p(px i + ci) \u2212 p 2 x i (px i + c i ) 2 = px i + c i \u2212 px i x i (px i + c i ) \u2212 pc i (px i + c i ) 2 = c i (px i + c i ) \u2212 px i c i x i (px i + c i ) 2 = c 2 i x i (px i + c i ) 2 ,(12)\nwhich is positive because\nx i \u2265 0.\nSimilarly, The derivative of Eq. (11) with respect to x j is\n1 x s \u2212 p px s + c s \u2212 p(px s + cs) \u2212 p 2 x s (px s + c s ) 2 = c 2 s x s (px s + c s ) 2 ,(13)\nwhich is also positive because x s \u2265 0 and concludes our proof.\n\nC Proof for Lemma 2 Algorithm 2 The iterative procedure in [47] Initialize:\nSample w \u223c N (0,1), \u03a0 0 \u2190 { }, \u03c0 1 \u2190 arg max \u03c0\u2208\u03a0 w \u00b7 \u03c8 \u03c0 , t \u2190 1 v SMP \u03a0 1 \u2190 \u2212||\u03c8 1 || repeat \u03a0 t \u2190 \u03a0 t\u22121 \u222a {\u03c0 t } \u03a8 t = \u03a8 t\u22121 \u222a {\u03c8 t } w SMP \u03a0 t \u2190 arg min w\u2208B2 max \u03c8\u2208\u03a8 i w \u00b7 \u03c8 \u03c0 t+1 \u2190 arg max \u03c0 \u03c8(\u03c0) \u00b7w SMP \u03a0 t t \u2190 t + 1 until v tw SMP \u03a0 n \u2264v SMP \u03a0 t\u22121 return \u03a0 t\u22121\nAlgorithm 3 Fully corrective FW for h(\u03c8) = 0.5||\u03c8|| 2 2 Initialize: Let \u03c0 1 be a random policy and let \u03c8 1 be its SFs. Also, let \u03a0 0 = {} and \u03a8 0 = {} and t \u2190 1.\nrepeat \u03a0 t = \u03a0 t\u22121 \u222a {\u03c0 t } \u03a8 t = \u03a8 t\u22121 \u222a {\u03c8 t } \u03c8 = arg min \u03c8\u2208Co(\u03a8 i ) 0.5||\u03c8|| 2 2 . \u03c0 t+1 = arg max \u03c0 \u03c8(\u03c0) \u00b7 \u2212\u2207h(\u03c8) = arg max \u03c0 \u03c8(\u03c0) \u00b7 \u2212\u03c8 t \u2190 t + 1 until h(\u03c8) \u2264 return \u03a0 t\u22121\nIn this section we show that the iterates of the fully corrective FW algorithm (Algorithm 3) correspond to the iterates of the Worst Case Policy Iteration algorithm (Algorithm 2). Examining the two algorithms, it is easy to see that all that is needed is to show that arg max \u03c0 \u03c8(\u03c0) \u00b7 \u2212\u03c8 = arg max \u03c0 \u03c8(\u03c0) \u00b7w SMP \u03a0 n .\n\nTo show this, first observe thatw SMP \u03a0 n can be also written as\nw SMP \u03a0 n = arg min w\u2208B2 max x\u2208\u03a8 i w \u00b7 \u03c8 = arg min w\u2208B2 max \u03c8\u2208Co(\u03a8 i ) w \u00b7 \u03c8,(14)\nthat is, maximizing \u03c8 over Co(\u03a8 i ) instead of \u03a8 i (SMP). This is correct because for any reward w there is always a maximizer in the convex hull that is one of the vertices (a property of the linear inner product). And therefore, the same maximum value is attained when maximizing over these two sets.\n\nNext, we have that arg min \n\nNow, if we denote the optimal solutions to Eq. (16) as\u0175,\u03c8 then, they are also an optimal solution to Eq. (14) via Von Neuman's min-max theorem. This means thatw SMP \u03a0 n =\u0175 = \u2212\u03c8/||\u03c8||. Thus arg max \u03c0 \u03c8(\u03c0) \u00b7w SMP \u03a0 n = arg max \u03c0 \u03c8(\u03c0) \u00b7 \u2212\u03c8/||\u03c8|| = arg max \u03c0 \u03c8(\u03c0) \u00b7 \u2212\u03c8,\n\nwhere the second inequality follows from the fact that dividing the reward by the same constant across all states does not change the optimal policy (the arg max).\n\nFinally, note that the function h = 0.5||x|| 2 Theorem 1 (Linear Convergence [22]). Suppose that h has L-Lipschitz gradient and is \u00b5-strongly convex. Let D = {d \u03c0 , \u2200\u03c0 \u2208 \u03a0} be the set of all the state occupancy's of deterministic policies in the MDP and let K = Co(D) be its Convex Hull. Such that K a polytope with vertices D, and let M = diam(K). Also, denote the Pyramidal Width of D, \u03b4 = P W idth(D) as in [22, Equation 9 1].\n\nThen the suboptimality h t of the iterates of all the fully corrective FW algorithm decreases geometrically at each step, that is\nh(x t+1 ) \u2264 (1 \u2212 \u03c1)h(x t ) , where \u03c1 = \u00b5\u03b4 2 4LM 2\n\nD Additional implementation details and hyper parameters\n\nWhen we add a new policy, \u03c0 t , to the set \u03a0 t\u22121 , we reset the maximum value v * e = max{v * e , v t }. This step is useful because the policies and their value functions are computed approximately in practice and in some of the domains the optimal performance is not achieved in the first iteration of Algorithm 1.\n\nTo bound the intrinsic rewards we first use the following transformationr w (s) = w\u00b7\u03c6(s) + w 2 w 2 and then apply the following non-linear transformation: r(s) = (1 \u2212 exp (\u2212\u03c4r w (s))) /(1 \u2212 exp(\u03c4 )),\n\nThis transformation is useful when we want the reward to be more sensitive to small variations of the inner product, i.e., when many policies are relatively similar to each other.\n\nFinally, Table 1 summarizes the hyperparameters that we use in Algorithm 1 \n\n\nE Additional results\n\nOur \"motion figures\" were created in the following manner. Given a trajectory of frames that composes a video f 1 , . . . , f T , we first trim and sub sample the trajectory into a point of interest in time: f n , . . . , f n+m . We always use the same trimming across the same set of policies (the sub figures in a figure). We then sub sample frames from the trimmed sequence at frequency 1/p: f n , f n+p , f n+2p . . . ,. After that, we take the maximum over the sequence and present this \"max\" image. In Python, this simply corresponds to, for example, to n=400, m=30, p=3 indices = range(n,n+m,p) im = np.max(f[indices])\n\nThis creates the effect of motion in single figure since the object has higher values then the background. \n\n\nE.1 Clockwise Diversity in Cartpole and Reacher\n\n\nChoice of r d : Given that our Diverse Successive Policies algorithm (1) can be used with different measures of diversity, we compared four different choices. The previously proposed robustness and discrimination measures and the new min and average explicit measures of diversity we proposed in Section 4.3, corresponding to: (1) Robustness: the worst case linear reward with respect to the previous policies in the set: r d (s) = w \u00b7 \u03c6(s), where w = min w\u2208B2 max z\u2208[1,..,n\u22121] \u03c8 z \u00b7 w is the internal minimization in Eq. (7). (2) Discrimination r d (s) = log( exp (\u03c6(s)\u00b7\u03c8 n ) n i=j exp(\u03c6(s)\u00b7\u03c8 j ) ), where \u03c8 n is the running average estimator of the SFs of the current policy. This reward corresponds to Eq. (5) with a uniform prior. (3) Min: r d (s) = min z\u2208[1,..,n\u22121] \u2212\u03c8 z \u00b7 \u03c6(s). (4) Average: r d (sj \u00b7 \u03c6(s). (5) None: r d (s) = 0 or no diversity.\n\nFigure 1 :\n1Diverse near optimal policies in Walker\n\nFig\n. 1b presents similar results in the Walker.walk environment where r d is the average explicit diversity criteria. In this case the walker discovered how to walk in different ways, such as lifting one of the legs while up walking, walking with high knees, or walking with the heels to the bottom.\n\nFigure 2 :\n2Diverse near optimal policies in Dog Fig. 2 presents results in the Dog.stand environment where in\n\nFigure 3 :\n3Diversity without reward in Walker and Cheetah.\n\nFigure 4 :Figure 22 :\n422Clockwise Diversity in Cartpole and Reacher. None\n\n\nWalker Stand, re as reward; r d as min.# \nre \n% \n1 920 100 \n2 809 \n88 \n3 820 \n89 \n4 878 \n95 \n5 818 \n89 \n6 818 \n89 \n7 490 \n53 \n8 926 101 \n\n(a) # \nre \n% \n1 951 100 \n2 866 \n91 \n3 813 \n85 \n4 872 \n92 \n5 971 102 \n6 837 \n88 \n7 876 \n92 \n8 870 \n91 \n\n(b) Walker Walk, re as reward; r d as average. \n\n\n\n\nDog Stand, re as reward; r d as min. Dog Stand, re as reward; r d as none.# \n\nre \n% \n1 921 100 \n2 870 \n94 \n3 879 \n95 \n4 909 \n98 \n5 944 102 \n6 975 106 \n7 938 102 \n8 930 101 \n\n(a) # \nr e \n% \n1 812 -\n2 936 -\n3 765 -\n4 892 -\n5 926 -\n6 891 -\n7 921 -\n8 948 -\n\n(b) \n\nTable 1 :\n1Hyperparameters table Lagrange entropy regularization weight a h (Eq. (9)) 0.01 Lagrange learning rate 0.1 Lagrange update frequency (N \u03bb ) 30 Estimation decay factor a d 0.9 Normalization temperature \u03c4 (Eq. (17)) 3Parameter \nValue \nOptimality level \u03b1 (Eq. (8)) \n0.9 \nEnvironment steps per policy \n10 6 \nNumber of policies \n8 \n\nWhen the extrinsic reward is positive (re(s, a) \u2265 0, \u2200s, a), the extrinsic value is positive v \u03c0 e \u2265 0, \u2200\u03c0, and setting \u03b1 = 0 in Eq. (1) is equivalent to the no-reward setting where the goal is to maximize diversity.\nhas 1-Lipschitz gradient and is strongly convex. Thus, since the algorithms are equivalent, Algorithm 2 achieves a linear convergence according to the following theorem.\nA Checklist1. For all authors (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work?[Yes]We discussed the limitations of diversity seeking methods, from being convex maximization problems (diayn) and from using auxiliary objectives (robustness). (c) Did you discuss any potential negative societal impacts of your work?[No]. Our paper studies how RL algorithms can find diverse solutions, we believe that promoting algorithmic diversity in AL should not have any negative societal impacts. (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?[No] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?[No] Instead of repeating the same experiment over multiple seeds, each of our experiments was performed to discover eight policies sequentially. Our only numerical claim in this paper is about constraint satisfaction, and as our results suggest, it is being satisfied, in any of these eight, consecutive but independent trials (the parameters were initialized after each iteration. Another axis in which we tested our algorithm was the domain. So instead of repeating Walker.Walk a few times, we performed the second experiment on Walker.Stand (so, a diff in the extrinsic reward) and the same in Dog. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\nJ Achiam, H Edwards, D Amodei, P Abbeel, arXiv:1807.10299Variational option discovery algorithms. arXiv preprintJ. Achiam, H. Edwards, D. Amodei, and P. Abbeel. Variational option discovery algorithms. arXiv preprint arXiv:1807.10299, 2018.\n\nPc-pg: Policy cover directed exploration for provable policy gradient learning. A Agarwal, M Henaff, S Kakade, W Sun, arXiv:2007.08459arXiv preprintA. Agarwal, M. Henaff, S. Kakade, and W. Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020.\n\nConstrained Markov decision processes. E Altman, CRC Press7E. Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.\n\nSuccessor features for transfer in reinforcement learning. A Barreto, W Dabney, R Munos, J J Hunt, T Schaul, H P Van Hasselt, D Silver, Advances in neural information processing systems. A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pages 4055-4065, 2017.\n\nK Baumli, D Warde-Farley, S Hansen, V Mnih, arXiv:2012.07827Relative variational intrinsic control. arXiv preprintK. Baumli, D. Warde-Farley, S. Hansen, and V. Mnih. Relative variational intrinsic control. arXiv preprint arXiv:2012.07827, 2020.\n\nGame 8: Leko wins to take the lead. R Behovits, R. Behovits. Game 8: Leko wins to take the lead, 2004. URL https://en.chessbase.com/post/ game-8-leko-wins-to-take-the-lead.\n\nAn online actor-critic algorithm with function approximation for constrained markov decision processes. S Bhatnagar, K Lakshmanan, Journal of Optimization Theory and Applications. 1533S. Bhatnagar and K. Lakshmanan. An online actor-critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688-708, 2012.\n\nAn actor-critic algorithm for constrained markov decision processes. V S Borkar, Systems & control letters. 543V. S. Borkar. An actor-critic algorithm for constrained markov decision processes. Systems & control letters, 54(3):207-213, 2005.\n\nS Boyd, L Vandenberghe, Convex optimization. Cambridge university pressS. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.\n\nBalancing constraints and rewards with meta-gradient d4pg. D A Calian, D J Mankowitz, T Zahavy, Z Xu, J Oh, N Levine, T Mann, International Conference on Learning Representations. D. A. Calian, D. J. Mankowitz, T. Zahavy, Z. Xu, J. Oh, N. Levine, and T. Mann. Balancing constraints and rewards with meta-gradient d4pg. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=TQt98Ya7UMP.\n\nThe loss surfaces of multilayer networks. A Choromanska, M Henaff, M Mathieu, G B Arous, Y Lecun, Artificial intelligence and statistics. PMLRA. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pages 192-204. PMLR, 2015.\n\nImproving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. E Conti, V Madhavan, F P Such, J Lehman, K O Stanley, J Clune, Proceedings of the 32nd International Conference on Neural Information Processing Systems. the 32nd International Conference on Neural Information Processing SystemsE. Conti, V. Madhavan, F. P. Such, J. Lehman, K. O. Stanley, and J. Clune. Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 5032-5043, 2018.\n\nElements of information theory. T M Cover, John Wiley & SonsT. M. Cover. Elements of information theory. John Wiley & Sons, 1999.\n\nSwapping songs with chess grandmaster garry kasparov. C Da Fonseca-Wollheim, C. da Fonseca-Wollheim. Swapping songs with chess grandmaster garry kasparov, 2020. URL https://www.nytimes.com/2020/12/18/arts/music/garry-kasparov-classical-music.html.\n\nIdentifying and attacking the saddle point problem in high-dimensional non-convex optimization. Y N Dauphin, R Pascanu, C Gulcehre, K Cho, S Ganguli, Y Bengio, Advances in Neural Information Processing Systems. Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. WeinbergerCurran Associates, Inc27Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https: //proceedings.neurips.cc/paper/2014/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf.\n\nDiversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.\n\nAn algorithm for quadratic programming. M Frank, P Wolfe, Naval research logistics quarterly. 31-2M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95-110, 1956.\n\nHarnessing distribution ratio estimators for learning agents with quality and diversity. T Gangwani, J Peng, Y Zhou, arXiv:2011.02614arXiv preprintT. Gangwani, J. Peng, and Y. Zhou. Harnessing distribution ratio estimators for learning agents with quality and diversity. arXiv preprint arXiv:2011.02614, 2020.\n\n. K Gregor, D J Rezende, D Wierstra, Variational intrinsic control. International Conference on Learning Representations. K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. International Confer- ence on Learning Representations, Workshop Track, 2017. URL https://openreview.net/forum? id=Skc-Fo4Yg.\n\nFast task inference with variational intrinsic successor features. S Hansen, W Dabney, A Barreto, D Warde-Farley, T V De Wiele, V Mnih, International Conference on Learning Representations. S. Hansen, W. Dabney, A. Barreto, D. Warde-Farley, T. V. de Wiele, and V. Mnih. Fast task inference with variational intrinsic successor features. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BJeAHkrYDS.\n\nDiversity-driven exploration strategy for deep reinforcement learning. Z.-W Hong, T.-Y Shann, S.-Y Su, Y.-H Chang, T.-J Fu, C.-Y. Lee, Proceedings of the 32nd International Conference on Neural Information Processing Systems. the 32nd International Conference on Neural Information Processing SystemsZ.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.-Y. Lee. Diversity-driven exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 10510-10521, 2018.\n\nOn the global linear convergence of frank-wolfe optimization variants. M Jaggi, S Lacoste-Julien, Advances in Neural Information Processing Systems. 28M. Jaggi and S. Lacoste-Julien. On the global linear convergence of frank-wolfe optimization variants. Advances in Neural Information Processing Systems, 28, 2015.\n\nDeterminantal point processes for machine learning. Foundations and Trends\u00ae in Machine Learning. A Kulesza, B Taskar, 5A. Kulesza, B. Taskar, et al. Determinantal point processes for machine learning. Foundations and Trends\u00ae in Machine Learning, 5(2-3):123-286, 2012.\n\nOne solution is not all you need: Few-shot extrapolation via structured maxent rl. S Kumar, A Kumar, S Levine, C Finn, Advances in Neural Information Processing Systems. 33S. Kumar, A. Kumar, S. Levine, and C. Finn. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33, 2020.\n\nDiversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse policies. M A Masood, F Doshi-Velez, arXiv:1906.00088arXiv preprintM. A. Masood and F. Doshi-Velez. Diversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse policies. arXiv preprint arXiv:1906.00088, 2019.\n\n. B Matusch, J Ba, D Hafner, arXiv:2012.11538Evaluating agents without rewards. arXiv preprintB. Matusch, J. Ba, and D. Hafner. Evaluating agents without rewards. arXiv preprint arXiv:2012.11538, 2020.\n\nTransfer in variable-reward hierarchical reinforcement learning. N Mehta, S Natarajan, P Tadepalli, A Fern, Machine Learning. 73289N. Mehta, S. Natarajan, P. Tadepalli, and A. Fern. Transfer in variable-reward hierarchical reinforcement learning. Machine Learning, 73(3):289, 2008.\n\nIlluminating search spaces by mapping elites. J.-B Mouret, J Clune, arXiv:1504.04909arXiv preprintJ.-B. Mouret and J. Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015.\n\nApplied imagination. A F Osborn, A. F. Osborn. Applied imagination. 1953.\n\nEffective diversity in population based reinforcement learning. J Parker-Holder, A Pacchiano, K M Choromanski, S J Roberts, Advances in Neural Information Processing Systems. 33J. Parker-Holder, A. Pacchiano, K. M. Choromanski, and S. J. Roberts. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.\n\nNon-local policy optimization via diversity-regularized collaborative exploration. Z Peng, H Sun, B Zhou, arXiv:2006.07781arXiv preprintZ. Peng, H. Sun, and B. Zhou. Non-local policy optimization via diversity-regularized collabo- rative exploration. arXiv preprint arXiv:2006.07781, 2020.\n\nExact sampling with coupled markov chains and applications to statistical mechanics. Random Structures and Algorithms. J G Propp, D B Wilson, J. G. Propp and D. B. Wilson. Exact sampling with coupled markov chains and applications to statistical mechanics. Random Structures and Algorithms, 1996.\n\nQuality diversity: A new frontier for evolutionary computation. J K Pugh, L B Soros, K O Stanley, Frontiers in Robotics and AI. 340J. K. Pugh, L. B. Soros, and K. O. Stanley. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016.\n\nMarkov decision processes: discrete stochastic dynamic programming. M L Puterman, John Wiley & SonsM. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1984.\n\nIntrinsically motivated reinforcement learning: An evolutionary perspective. S Singh, R L Lewis, A G Barto, J Sorg, IEEE Transactions on Autonomous Mental Development. 22S. Singh, R. L. Lewis, A. G. Barto, and J. Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2): 70-82, 2010.\n\nNo bad local minima: Data independent training error guarantees for multilayer neural networks. D Soudry, Y Carmon, arXiv:1605.08361arXiv preprintD. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.\n\nWhy greatness cannot be planned: The myth of the objective. K O Stanley, J Lehman, SpringerK. O. Stanley and J. Lehman. Why greatness cannot be planned: The myth of the objective. Springer, 2015.\n\nNovel policy seeking with constrained optimization. H Sun, Z Peng, B Dai, J Guo, D Lin, B Zhou, arXiv:2005.10696arXiv preprintH. Sun, Z. Peng, B. Dai, J. Guo, D. Lin, and B. Zhou. Novel policy seeking with constrained optimization. arXiv preprint arXiv:2005.10696, 2020.\n\nConstrained mdps and the reward hypothesis. C Szepesv\u00e1ri, C. Szepesv\u00e1ri. Constrained mdps and the reward hypothesis, 2020. URL https://readingsml. blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html.\n\n. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D , . L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. arXiv preprintY. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.\n\nReward constrained policy optimization. C Tessler, D J Mankowitz, S Mannor, International Conference on Learning Representations. C. Tessler, D. J. Mankowitz, and S. Mannor. Reward constrained policy optimization. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=SkfrvsA9FX.\n\nZur theorie der gesellschaftsspiele. J Neumann, Mathematische annalen. 1001J. von Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.\n\n. Z Xu, H Van Hasselt, D Silver, arXiv:1805.09801Meta-gradient reinforcement learning. arXiv preprintZ. Xu, H. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. arXiv preprint arXiv:1805.09801, 2018.\n\nAverage reward reinforcement learning with unknown mixing times. T Zahavy, A Cohen, H Kaplan, Y Mansour, The Conference on Uncertainty in Artificial Intelligence (UAI). 2020T. Zahavy, A. Cohen, H. Kaplan, and Y. Mansour. Average reward reinforcement learning with unknown mixing times. The Conference on Uncertainty in Artificial Intelligence (UAI), 2020.\n\nPlanning in hierarchical reinforcement learning: Guarantees for using local policies. T Zahavy, A Hasidim, H Kaplan, Y Mansour, Algorithmic Learning Theory. T. Zahavy, A. Hasidim, H. Kaplan, and Y. Mansour. Planning in hierarchical reinforcement learning: Guarantees for using local policies. In Algorithmic Learning Theory, pages 906-934, 2020.\n\nA self-tuning actor-critic algorithm. T Zahavy, Z Xu, V Veeriah, M Hessel, J Oh, H P Van Hasselt, D Silver, S Singh, Advances in Neural Information Processing Systems. 33T. Zahavy, Z. Xu, V. Veeriah, M. Hessel, J. Oh, H. P. van Hasselt, D. Silver, and S. Singh. A self-tuning actor-critic algorithm. Advances in Neural Information Processing Systems, 33, 2020.\n\nDiscovering a set of policies for the worst case reward. T Zahavy, A Barreto, D J Mankowitz, S Hou, B O&apos;donoghue, I Kemaev, S Singh, International Conference on Learning Representations. T. Zahavy, A. Barreto, D. J. Mankowitz, S. Hou, B. O'Donoghue, I. Kemaev, and S. Singh. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PUkhWz65dy5.\n\nLearning novel policies for tasks. Y Zhang, W Yu, G Turk, PMLRProceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research. K. Chaudhuri and R. Salakhutdinovthe 36th International Conference on Machine Learning, volume 97 of Machine Learning ResearchY. Zhang, W. Yu, and G. Turk. Learning novel policies for tasks. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pages 7483-7492. PMLR, 09-15 Jun 2019. URL http://proceedings.mlr.press/v97/zhang19q.html.\n\nOn learning intrinsic rewards for policy gradient methods. Z Zheng, J Oh, S Singh, arXiv:1804.06459arXiv preprintZ. Zheng, J. Oh, and S. Singh. On learning intrinsic rewards for policy gradient methods. arXiv preprint arXiv:1804.06459, 2018.\n", "annotations": {"author": "[{\"end\":122,\"start\":71},{\"end\":188,\"start\":123},{\"end\":246,\"start\":189},{\"end\":298,\"start\":247},{\"end\":361,\"start\":299},{\"end\":394,\"start\":362}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":75},{\"end\":146,\"start\":131},{\"end\":202,\"start\":195},{\"end\":261,\"start\":257},{\"end\":319,\"start\":309},{\"end\":376,\"start\":371}]", "author_first_name": "[{\"end\":74,\"start\":71},{\"end\":130,\"start\":123},{\"end\":194,\"start\":189},{\"end\":256,\"start\":247},{\"end\":308,\"start\":299},{\"end\":370,\"start\":362}]", "author_affiliation": "[{\"end\":121,\"start\":106},{\"end\":187,\"start\":172},{\"end\":245,\"start\":230},{\"end\":297,\"start\":282},{\"end\":360,\"start\":345},{\"end\":393,\"start\":378}]", "title": "[{\"end\":68,\"start\":1},{\"end\":462,\"start\":395}]", "venue": null, "abstract": "[{\"end\":1765,\"start\":464}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1906,\"start\":1902},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2333,\"start\":2329},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2604,\"start\":2601},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3410,\"start\":3406},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3413,\"start\":3410},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3416,\"start\":3413},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3419,\"start\":3416},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3422,\"start\":3419},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3425,\"start\":3422},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3428,\"start\":3425},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3431,\"start\":3428},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3535,\"start\":3531},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3538,\"start\":3535},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3584,\"start\":3580},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3587,\"start\":3584},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3611,\"start\":3608},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3769,\"start\":3765},{\"end\":4134,\"start\":4130},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4673,\"start\":4669},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4880,\"start\":4876},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5450,\"start\":5446},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6466,\"start\":6462},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6469,\"start\":6466},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6876,\"start\":6872},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7998,\"start\":7994},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8001,\"start\":7998},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9886,\"start\":9882},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9889,\"start\":9886},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9892,\"start\":9889},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9895,\"start\":9892},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11315,\"start\":11312},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12101,\"start\":12097},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12151,\"start\":12148},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12173,\"start\":12169},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12331,\"start\":12327},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12334,\"start\":12331},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13038,\"start\":13034},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13284,\"start\":13280},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14323,\"start\":14319},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14410,\"start\":14407},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14412,\"start\":14410},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14415,\"start\":14412},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16058,\"start\":16055},{\"end\":16071,\"start\":16058},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17570,\"start\":17566},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17573,\"start\":17570},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17576,\"start\":17573},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17958,\"start\":17954},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17961,\"start\":17958},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19322,\"start\":19318},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19502,\"start\":19498},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20265,\"start\":20261},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20403,\"start\":20399},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20445,\"start\":20441},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21480,\"start\":21476},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22086,\"start\":22082},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22498,\"start\":22495},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24431,\"start\":24427},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25097,\"start\":25093},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25583,\"start\":25579},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25746,\"start\":25742},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26049,\"start\":26046},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26058,\"start\":26054},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26441,\"start\":26438},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26443,\"start\":26441},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26517,\"start\":26513},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26520,\"start\":26517},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28722,\"start\":28718},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29118,\"start\":29114},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29138,\"start\":29134},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29423,\"start\":29420},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30219,\"start\":30216},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30422,\"start\":30418},{\"end\":30432,\"start\":30422},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30503,\"start\":30499},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30506,\"start\":30503},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30509,\"start\":30506},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30512,\"start\":30509},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30515,\"start\":30512},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30518,\"start\":30515},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31299,\"start\":31295},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31473,\"start\":31469},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31793,\"start\":31790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31795,\"start\":31793},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31797,\"start\":31795},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":31968,\"start\":31964},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33626,\"start\":33622},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37207,\"start\":37203},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37586,\"start\":37585},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":38385,\"start\":38381},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":38388,\"start\":38385},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38635,\"start\":38631},{\"end\":41872,\"start\":41861},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":41904,\"start\":41900},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43430,\"start\":43426},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43834,\"start\":43830},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44178,\"start\":44177}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46861,\"start\":46008},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46914,\"start\":46862},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47216,\"start\":46915},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47328,\"start\":47217},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47389,\"start\":47329},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47465,\"start\":47390},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47758,\"start\":47466},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48019,\"start\":47759},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48359,\"start\":48020}]", "paragraph": "[{\"end\":2143,\"start\":1781},{\"end\":2787,\"start\":2145},{\"end\":3292,\"start\":2789},{\"end\":4838,\"start\":3294},{\"end\":5252,\"start\":4869},{\"end\":6470,\"start\":5254},{\"end\":7754,\"start\":6472},{\"end\":8358,\"start\":7756},{\"end\":8702,\"start\":8404},{\"end\":9383,\"start\":8758},{\"end\":9477,\"start\":9385},{\"end\":9727,\"start\":9686},{\"end\":11032,\"start\":9797},{\"end\":11496,\"start\":11084},{\"end\":12335,\"start\":11527},{\"end\":12736,\"start\":12337},{\"end\":13579,\"start\":12769},{\"end\":14258,\"start\":13668},{\"end\":14622,\"start\":14260},{\"end\":15063,\"start\":14624},{\"end\":15162,\"start\":15108},{\"end\":15314,\"start\":15222},{\"end\":15747,\"start\":15407},{\"end\":16828,\"start\":15848},{\"end\":17763,\"start\":16830},{\"end\":17988,\"start\":17792},{\"end\":18831,\"start\":18034},{\"end\":19313,\"start\":18833},{\"end\":20642,\"start\":19315},{\"end\":21230,\"start\":20644},{\"end\":22149,\"start\":21363},{\"end\":22619,\"start\":22151},{\"end\":23233,\"start\":22650},{\"end\":24028,\"start\":23235},{\"end\":24595,\"start\":24030},{\"end\":25200,\"start\":24597},{\"end\":25894,\"start\":25202},{\"end\":26255,\"start\":25926},{\"end\":26385,\"start\":26302},{\"end\":26849,\"start\":26387},{\"end\":27307,\"start\":26851},{\"end\":27348,\"start\":27309},{\"end\":27708,\"start\":27350},{\"end\":28035,\"start\":27753},{\"end\":28438,\"start\":28037},{\"end\":29269,\"start\":28440},{\"end\":29783,\"start\":29271},{\"end\":30433,\"start\":29799},{\"end\":31252,\"start\":30435},{\"end\":31882,\"start\":31254},{\"end\":32880,\"start\":31898},{\"end\":33420,\"start\":32882},{\"end\":36720,\"start\":33422},{\"end\":37688,\"start\":36722},{\"end\":37986,\"start\":37690},{\"end\":38036,\"start\":37988},{\"end\":38095,\"start\":38038},{\"end\":38892,\"start\":38110},{\"end\":39416,\"start\":38894},{\"end\":39675,\"start\":39440},{\"end\":39882,\"start\":39677},{\"end\":40030,\"start\":39927},{\"end\":40214,\"start\":40106},{\"end\":40332,\"start\":40279},{\"end\":40881,\"start\":40611},{\"end\":41141,\"start\":40894},{\"end\":41375,\"start\":41183},{\"end\":41609,\"start\":41584},{\"end\":41679,\"start\":41619},{\"end\":41839,\"start\":41776},{\"end\":41916,\"start\":41841},{\"end\":42344,\"start\":42183},{\"end\":42839,\"start\":42522},{\"end\":42905,\"start\":42841},{\"end\":43290,\"start\":42988},{\"end\":43319,\"start\":43292},{\"end\":43586,\"start\":43321},{\"end\":43751,\"start\":43588},{\"end\":44182,\"start\":43753},{\"end\":44313,\"start\":44184},{\"end\":44739,\"start\":44423},{\"end\":44940,\"start\":44741},{\"end\":45121,\"start\":44942},{\"end\":45198,\"start\":45123},{\"end\":45848,\"start\":45223},{\"end\":45957,\"start\":45850}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8757,\"start\":8703},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9685,\"start\":9478},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9796,\"start\":9728},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11083,\"start\":11033},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13667,\"start\":13580},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15107,\"start\":15064},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15221,\"start\":15163},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15406,\"start\":15315},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15847,\"start\":15748},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18033,\"start\":17989},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21362,\"start\":21231},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26301,\"start\":26256},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27752,\"start\":27709},{\"attributes\":{\"id\":\"formula_13\"},\"end\":39926,\"start\":39883},{\"attributes\":{\"id\":\"formula_14\"},\"end\":40105,\"start\":40031},{\"attributes\":{\"id\":\"formula_15\"},\"end\":40278,\"start\":40215},{\"attributes\":{\"id\":\"formula_16\"},\"end\":40610,\"start\":40333},{\"attributes\":{\"id\":\"formula_17\"},\"end\":40893,\"start\":40882},{\"attributes\":{\"id\":\"formula_18\"},\"end\":41182,\"start\":41142},{\"attributes\":{\"id\":\"formula_19\"},\"end\":41583,\"start\":41376},{\"attributes\":{\"id\":\"formula_20\"},\"end\":41618,\"start\":41610},{\"attributes\":{\"id\":\"formula_21\"},\"end\":41775,\"start\":41680},{\"attributes\":{\"id\":\"formula_22\"},\"end\":42182,\"start\":41917},{\"attributes\":{\"id\":\"formula_23\"},\"end\":42521,\"start\":42345},{\"attributes\":{\"id\":\"formula_24\"},\"end\":42987,\"start\":42906},{\"attributes\":{\"id\":\"formula_26\"},\"end\":44363,\"start\":44314}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":45139,\"start\":45132}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1779,\"start\":1767},{\"attributes\":{\"n\":\"2\"},\"end\":4867,\"start\":4841},{\"attributes\":{\"n\":\"3\"},\"end\":8402,\"start\":8361},{\"attributes\":{\"n\":\"4\"},\"end\":11525,\"start\":11499},{\"attributes\":{\"n\":\"4.1\"},\"end\":12767,\"start\":12739},{\"attributes\":{\"n\":\"4.2\"},\"end\":17790,\"start\":17766},{\"attributes\":{\"n\":\"4.3\"},\"end\":22648,\"start\":22622},{\"attributes\":{\"n\":\"5\"},\"end\":25924,\"start\":25897},{\"end\":29797,\"start\":29786},{\"attributes\":{\"n\":\"6\"},\"end\":31896,\"start\":31885},{\"attributes\":{\"n\":\"7\"},\"end\":38108,\"start\":38098},{\"end\":39438,\"start\":39419},{\"end\":44421,\"start\":44365},{\"end\":45221,\"start\":45201},{\"end\":46007,\"start\":45960},{\"end\":46873,\"start\":46863},{\"end\":46919,\"start\":46916},{\"end\":47228,\"start\":47218},{\"end\":47340,\"start\":47330},{\"end\":47412,\"start\":47391},{\"end\":48030,\"start\":48021}]", "table": "[{\"end\":47758,\"start\":47507},{\"end\":48019,\"start\":47835},{\"end\":48359,\"start\":48247}]", "figure_caption": "[{\"end\":46861,\"start\":46010},{\"end\":46914,\"start\":46875},{\"end\":47216,\"start\":46920},{\"end\":47328,\"start\":47230},{\"end\":47389,\"start\":47342},{\"end\":47465,\"start\":47416},{\"end\":47507,\"start\":47468},{\"end\":47835,\"start\":47761},{\"end\":48247,\"start\":48032}]", "figure_ref": "[{\"end\":32625,\"start\":32619},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34404,\"start\":34397},{\"end\":35513,\"start\":35506},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36279,\"start\":36272},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36341,\"start\":36334},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36393,\"start\":36386},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36601,\"start\":36594},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37216,\"start\":37209},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37706,\"start\":37699},{\"end\":45546,\"start\":45539}]", "bib_author_first_name": "[{\"end\":50582,\"start\":50581},{\"end\":50592,\"start\":50591},{\"end\":50603,\"start\":50602},{\"end\":50613,\"start\":50612},{\"end\":50904,\"start\":50903},{\"end\":50915,\"start\":50914},{\"end\":50925,\"start\":50924},{\"end\":50935,\"start\":50934},{\"end\":51177,\"start\":51176},{\"end\":51334,\"start\":51333},{\"end\":51345,\"start\":51344},{\"end\":51355,\"start\":51354},{\"end\":51364,\"start\":51363},{\"end\":51366,\"start\":51365},{\"end\":51374,\"start\":51373},{\"end\":51384,\"start\":51383},{\"end\":51386,\"start\":51385},{\"end\":51401,\"start\":51400},{\"end\":51689,\"start\":51688},{\"end\":51699,\"start\":51698},{\"end\":51715,\"start\":51714},{\"end\":51725,\"start\":51724},{\"end\":51971,\"start\":51970},{\"end\":52213,\"start\":52212},{\"end\":52226,\"start\":52225},{\"end\":52570,\"start\":52569},{\"end\":52572,\"start\":52571},{\"end\":52744,\"start\":52743},{\"end\":52752,\"start\":52751},{\"end\":52959,\"start\":52958},{\"end\":52961,\"start\":52960},{\"end\":52971,\"start\":52970},{\"end\":52973,\"start\":52972},{\"end\":52986,\"start\":52985},{\"end\":52996,\"start\":52995},{\"end\":53002,\"start\":53001},{\"end\":53008,\"start\":53007},{\"end\":53018,\"start\":53017},{\"end\":53374,\"start\":53373},{\"end\":53389,\"start\":53388},{\"end\":53399,\"start\":53398},{\"end\":53410,\"start\":53409},{\"end\":53412,\"start\":53411},{\"end\":53421,\"start\":53420},{\"end\":53775,\"start\":53774},{\"end\":53784,\"start\":53783},{\"end\":53796,\"start\":53795},{\"end\":53798,\"start\":53797},{\"end\":53806,\"start\":53805},{\"end\":53816,\"start\":53815},{\"end\":53818,\"start\":53817},{\"end\":53829,\"start\":53828},{\"end\":54350,\"start\":54349},{\"end\":54352,\"start\":54351},{\"end\":54503,\"start\":54502},{\"end\":54794,\"start\":54793},{\"end\":54796,\"start\":54795},{\"end\":54807,\"start\":54806},{\"end\":54818,\"start\":54817},{\"end\":54830,\"start\":54829},{\"end\":54837,\"start\":54836},{\"end\":54848,\"start\":54847},{\"end\":55520,\"start\":55519},{\"end\":55533,\"start\":55532},{\"end\":55542,\"start\":55541},{\"end\":55551,\"start\":55550},{\"end\":55886,\"start\":55885},{\"end\":55895,\"start\":55894},{\"end\":56154,\"start\":56153},{\"end\":56166,\"start\":56165},{\"end\":56174,\"start\":56173},{\"end\":56378,\"start\":56377},{\"end\":56388,\"start\":56387},{\"end\":56390,\"start\":56389},{\"end\":56401,\"start\":56400},{\"end\":56766,\"start\":56765},{\"end\":56776,\"start\":56775},{\"end\":56786,\"start\":56785},{\"end\":56797,\"start\":56796},{\"end\":56813,\"start\":56812},{\"end\":56815,\"start\":56814},{\"end\":56827,\"start\":56826},{\"end\":57222,\"start\":57218},{\"end\":57233,\"start\":57229},{\"end\":57245,\"start\":57241},{\"end\":57254,\"start\":57250},{\"end\":57266,\"start\":57262},{\"end\":57276,\"start\":57271},{\"end\":57783,\"start\":57782},{\"end\":57792,\"start\":57791},{\"end\":58125,\"start\":58124},{\"end\":58136,\"start\":58135},{\"end\":58380,\"start\":58379},{\"end\":58389,\"start\":58388},{\"end\":58398,\"start\":58397},{\"end\":58408,\"start\":58407},{\"end\":58760,\"start\":58759},{\"end\":58762,\"start\":58761},{\"end\":58772,\"start\":58771},{\"end\":58994,\"start\":58993},{\"end\":59005,\"start\":59004},{\"end\":59011,\"start\":59010},{\"end\":59260,\"start\":59259},{\"end\":59269,\"start\":59268},{\"end\":59282,\"start\":59281},{\"end\":59295,\"start\":59294},{\"end\":59527,\"start\":59523},{\"end\":59537,\"start\":59536},{\"end\":59710,\"start\":59709},{\"end\":59712,\"start\":59711},{\"end\":59828,\"start\":59827},{\"end\":59845,\"start\":59844},{\"end\":59858,\"start\":59857},{\"end\":59860,\"start\":59859},{\"end\":59875,\"start\":59874},{\"end\":59877,\"start\":59876},{\"end\":60220,\"start\":60219},{\"end\":60228,\"start\":60227},{\"end\":60235,\"start\":60234},{\"end\":60547,\"start\":60546},{\"end\":60549,\"start\":60548},{\"end\":60558,\"start\":60557},{\"end\":60560,\"start\":60559},{\"end\":60790,\"start\":60789},{\"end\":60792,\"start\":60791},{\"end\":60800,\"start\":60799},{\"end\":60802,\"start\":60801},{\"end\":60811,\"start\":60810},{\"end\":60813,\"start\":60812},{\"end\":61076,\"start\":61075},{\"end\":61078,\"start\":61077},{\"end\":61294,\"start\":61293},{\"end\":61303,\"start\":61302},{\"end\":61305,\"start\":61304},{\"end\":61314,\"start\":61313},{\"end\":61316,\"start\":61315},{\"end\":61325,\"start\":61324},{\"end\":61681,\"start\":61680},{\"end\":61691,\"start\":61690},{\"end\":61952,\"start\":61951},{\"end\":61954,\"start\":61953},{\"end\":61965,\"start\":61964},{\"end\":62141,\"start\":62140},{\"end\":62148,\"start\":62147},{\"end\":62156,\"start\":62155},{\"end\":62163,\"start\":62162},{\"end\":62170,\"start\":62169},{\"end\":62177,\"start\":62176},{\"end\":62405,\"start\":62404},{\"end\":62577,\"start\":62576},{\"end\":62586,\"start\":62585},{\"end\":62595,\"start\":62594},{\"end\":62605,\"start\":62604},{\"end\":62613,\"start\":62612},{\"end\":62619,\"start\":62618},{\"end\":62621,\"start\":62620},{\"end\":62625,\"start\":62624},{\"end\":62627,\"start\":62626},{\"end\":62636,\"start\":62635},{\"end\":62646,\"start\":62645},{\"end\":62661,\"start\":62660},{\"end\":62670,\"start\":62669},{\"end\":62960,\"start\":62959},{\"end\":62971,\"start\":62970},{\"end\":62973,\"start\":62972},{\"end\":62986,\"start\":62985},{\"end\":63284,\"start\":63283},{\"end\":63423,\"start\":63422},{\"end\":63429,\"start\":63428},{\"end\":63444,\"start\":63443},{\"end\":63703,\"start\":63702},{\"end\":63713,\"start\":63712},{\"end\":63722,\"start\":63721},{\"end\":63732,\"start\":63731},{\"end\":64081,\"start\":64080},{\"end\":64091,\"start\":64090},{\"end\":64102,\"start\":64101},{\"end\":64112,\"start\":64111},{\"end\":64380,\"start\":64379},{\"end\":64390,\"start\":64389},{\"end\":64396,\"start\":64395},{\"end\":64407,\"start\":64406},{\"end\":64417,\"start\":64416},{\"end\":64423,\"start\":64422},{\"end\":64425,\"start\":64424},{\"end\":64440,\"start\":64439},{\"end\":64450,\"start\":64449},{\"end\":64761,\"start\":64760},{\"end\":64771,\"start\":64770},{\"end\":64782,\"start\":64781},{\"end\":64784,\"start\":64783},{\"end\":64797,\"start\":64796},{\"end\":64804,\"start\":64803},{\"end\":64823,\"start\":64822},{\"end\":64833,\"start\":64832},{\"end\":65189,\"start\":65188},{\"end\":65198,\"start\":65197},{\"end\":65204,\"start\":65203},{\"end\":65857,\"start\":65856},{\"end\":65866,\"start\":65865},{\"end\":65872,\"start\":65871}]", "bib_author_last_name": "[{\"end\":50589,\"start\":50583},{\"end\":50600,\"start\":50593},{\"end\":50610,\"start\":50604},{\"end\":50620,\"start\":50614},{\"end\":50912,\"start\":50905},{\"end\":50922,\"start\":50916},{\"end\":50932,\"start\":50926},{\"end\":50939,\"start\":50936},{\"end\":51184,\"start\":51178},{\"end\":51342,\"start\":51335},{\"end\":51352,\"start\":51346},{\"end\":51361,\"start\":51356},{\"end\":51371,\"start\":51367},{\"end\":51381,\"start\":51375},{\"end\":51398,\"start\":51387},{\"end\":51408,\"start\":51402},{\"end\":51696,\"start\":51690},{\"end\":51712,\"start\":51700},{\"end\":51722,\"start\":51716},{\"end\":51730,\"start\":51726},{\"end\":51980,\"start\":51972},{\"end\":52223,\"start\":52214},{\"end\":52237,\"start\":52227},{\"end\":52579,\"start\":52573},{\"end\":52749,\"start\":52745},{\"end\":52765,\"start\":52753},{\"end\":52968,\"start\":52962},{\"end\":52983,\"start\":52974},{\"end\":52993,\"start\":52987},{\"end\":52999,\"start\":52997},{\"end\":53005,\"start\":53003},{\"end\":53015,\"start\":53009},{\"end\":53023,\"start\":53019},{\"end\":53386,\"start\":53375},{\"end\":53396,\"start\":53390},{\"end\":53407,\"start\":53400},{\"end\":53418,\"start\":53413},{\"end\":53427,\"start\":53422},{\"end\":53781,\"start\":53776},{\"end\":53793,\"start\":53785},{\"end\":53803,\"start\":53799},{\"end\":53813,\"start\":53807},{\"end\":53826,\"start\":53819},{\"end\":53835,\"start\":53830},{\"end\":54358,\"start\":54353},{\"end\":54523,\"start\":54504},{\"end\":54804,\"start\":54797},{\"end\":54815,\"start\":54808},{\"end\":54827,\"start\":54819},{\"end\":54834,\"start\":54831},{\"end\":54845,\"start\":54838},{\"end\":54855,\"start\":54849},{\"end\":55530,\"start\":55521},{\"end\":55539,\"start\":55534},{\"end\":55548,\"start\":55543},{\"end\":55558,\"start\":55552},{\"end\":55892,\"start\":55887},{\"end\":55901,\"start\":55896},{\"end\":56163,\"start\":56155},{\"end\":56171,\"start\":56167},{\"end\":56179,\"start\":56175},{\"end\":56385,\"start\":56379},{\"end\":56398,\"start\":56391},{\"end\":56410,\"start\":56402},{\"end\":56773,\"start\":56767},{\"end\":56783,\"start\":56777},{\"end\":56794,\"start\":56787},{\"end\":56810,\"start\":56798},{\"end\":56824,\"start\":56816},{\"end\":56832,\"start\":56828},{\"end\":57227,\"start\":57223},{\"end\":57239,\"start\":57234},{\"end\":57248,\"start\":57246},{\"end\":57260,\"start\":57255},{\"end\":57269,\"start\":57267},{\"end\":57280,\"start\":57277},{\"end\":57789,\"start\":57784},{\"end\":57807,\"start\":57793},{\"end\":58133,\"start\":58126},{\"end\":58143,\"start\":58137},{\"end\":58386,\"start\":58381},{\"end\":58395,\"start\":58390},{\"end\":58405,\"start\":58399},{\"end\":58413,\"start\":58409},{\"end\":58769,\"start\":58763},{\"end\":58784,\"start\":58773},{\"end\":59002,\"start\":58995},{\"end\":59008,\"start\":59006},{\"end\":59018,\"start\":59012},{\"end\":59266,\"start\":59261},{\"end\":59279,\"start\":59270},{\"end\":59292,\"start\":59283},{\"end\":59300,\"start\":59296},{\"end\":59534,\"start\":59528},{\"end\":59543,\"start\":59538},{\"end\":59719,\"start\":59713},{\"end\":59842,\"start\":59829},{\"end\":59855,\"start\":59846},{\"end\":59872,\"start\":59861},{\"end\":59885,\"start\":59878},{\"end\":60225,\"start\":60221},{\"end\":60232,\"start\":60229},{\"end\":60240,\"start\":60236},{\"end\":60555,\"start\":60550},{\"end\":60567,\"start\":60561},{\"end\":60797,\"start\":60793},{\"end\":60808,\"start\":60803},{\"end\":60821,\"start\":60814},{\"end\":61087,\"start\":61079},{\"end\":61300,\"start\":61295},{\"end\":61311,\"start\":61306},{\"end\":61322,\"start\":61317},{\"end\":61330,\"start\":61326},{\"end\":61688,\"start\":61682},{\"end\":61698,\"start\":61692},{\"end\":61962,\"start\":61955},{\"end\":61972,\"start\":61966},{\"end\":62145,\"start\":62142},{\"end\":62153,\"start\":62149},{\"end\":62160,\"start\":62157},{\"end\":62167,\"start\":62164},{\"end\":62174,\"start\":62171},{\"end\":62182,\"start\":62178},{\"end\":62416,\"start\":62406},{\"end\":62583,\"start\":62578},{\"end\":62592,\"start\":62587},{\"end\":62602,\"start\":62596},{\"end\":62610,\"start\":62606},{\"end\":62616,\"start\":62614},{\"end\":62633,\"start\":62628},{\"end\":62643,\"start\":62637},{\"end\":62658,\"start\":62647},{\"end\":62667,\"start\":62662},{\"end\":62679,\"start\":62671},{\"end\":62968,\"start\":62961},{\"end\":62983,\"start\":62974},{\"end\":62993,\"start\":62987},{\"end\":63292,\"start\":63285},{\"end\":63426,\"start\":63424},{\"end\":63441,\"start\":63430},{\"end\":63451,\"start\":63445},{\"end\":63710,\"start\":63704},{\"end\":63719,\"start\":63714},{\"end\":63729,\"start\":63723},{\"end\":63740,\"start\":63733},{\"end\":64088,\"start\":64082},{\"end\":64099,\"start\":64092},{\"end\":64109,\"start\":64103},{\"end\":64120,\"start\":64113},{\"end\":64387,\"start\":64381},{\"end\":64393,\"start\":64391},{\"end\":64404,\"start\":64397},{\"end\":64414,\"start\":64408},{\"end\":64420,\"start\":64418},{\"end\":64437,\"start\":64426},{\"end\":64447,\"start\":64441},{\"end\":64456,\"start\":64451},{\"end\":64768,\"start\":64762},{\"end\":64779,\"start\":64772},{\"end\":64794,\"start\":64785},{\"end\":64801,\"start\":64798},{\"end\":64820,\"start\":64805},{\"end\":64830,\"start\":64824},{\"end\":64839,\"start\":64834},{\"end\":65195,\"start\":65190},{\"end\":65201,\"start\":65199},{\"end\":65209,\"start\":65205},{\"end\":65863,\"start\":65858},{\"end\":65869,\"start\":65867},{\"end\":65878,\"start\":65873}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1807.10299\",\"id\":\"b0\"},\"end\":50821,\"start\":50581},{\"attributes\":{\"doi\":\"arXiv:2007.08459\",\"id\":\"b1\"},\"end\":51135,\"start\":50823},{\"attributes\":{\"id\":\"b2\"},\"end\":51272,\"start\":51137},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4650427},\"end\":51686,\"start\":51274},{\"attributes\":{\"doi\":\"arXiv:2012.07827\",\"id\":\"b4\"},\"end\":51932,\"start\":51688},{\"attributes\":{\"id\":\"b5\"},\"end\":52106,\"start\":51934},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":41269031},\"end\":52498,\"start\":52108},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":20619091},\"end\":52741,\"start\":52500},{\"attributes\":{\"id\":\"b8\"},\"end\":52897,\"start\":52743},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":222310792},\"end\":53329,\"start\":52899},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2266226},\"end\":53650,\"start\":53331},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":24994970},\"end\":54315,\"start\":53652},{\"attributes\":{\"id\":\"b12\"},\"end\":54446,\"start\":54317},{\"attributes\":{\"id\":\"b13\"},\"end\":54695,\"start\":54448},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":11657534},\"end\":55447,\"start\":54697},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3521071},\"end\":55843,\"start\":55449},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":122654717},\"end\":56062,\"start\":55845},{\"attributes\":{\"doi\":\"arXiv:2011.02614\",\"id\":\"b17\"},\"end\":56373,\"start\":56064},{\"attributes\":{\"id\":\"b18\"},\"end\":56696,\"start\":56375},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":186206768},\"end\":57145,\"start\":56698},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3633374},\"end\":57709,\"start\":57147},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14606229},\"end\":58025,\"start\":57711},{\"attributes\":{\"id\":\"b22\"},\"end\":58294,\"start\":58027},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221095075},\"end\":58655,\"start\":58296},{\"attributes\":{\"doi\":\"arXiv:1906.00088\",\"id\":\"b24\"},\"end\":58989,\"start\":58657},{\"attributes\":{\"doi\":\"arXiv:2012.11538\",\"id\":\"b25\"},\"end\":59192,\"start\":58991},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7794529},\"end\":59475,\"start\":59194},{\"attributes\":{\"doi\":\"arXiv:1504.04909\",\"id\":\"b27\"},\"end\":59686,\"start\":59477},{\"attributes\":{\"id\":\"b28\"},\"end\":59761,\"start\":59688},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":211010962},\"end\":60134,\"start\":59763},{\"attributes\":{\"doi\":\"arXiv:2006.07781\",\"id\":\"b30\"},\"end\":60425,\"start\":60136},{\"attributes\":{\"id\":\"b31\"},\"end\":60723,\"start\":60427},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":21713708},\"end\":61005,\"start\":60725},{\"attributes\":{\"id\":\"b33\"},\"end\":61214,\"start\":61007},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7449640},\"end\":61582,\"start\":61216},{\"attributes\":{\"doi\":\"arXiv:1605.08361\",\"id\":\"b35\"},\"end\":61889,\"start\":61584},{\"attributes\":{\"id\":\"b36\"},\"end\":62086,\"start\":61891},{\"attributes\":{\"doi\":\"arXiv:2005.10696\",\"id\":\"b37\"},\"end\":62358,\"start\":62088},{\"attributes\":{\"id\":\"b38\"},\"end\":62572,\"start\":62360},{\"attributes\":{\"doi\":\"arXiv:1801.00690\",\"id\":\"b39\"},\"end\":62917,\"start\":62574},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":44095973},\"end\":63244,\"start\":62919},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":122961988},\"end\":63418,\"start\":63246},{\"attributes\":{\"doi\":\"arXiv:1805.09801\",\"id\":\"b42\"},\"end\":63635,\"start\":63420},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":162184387},\"end\":63992,\"start\":63637},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":67855650},\"end\":64339,\"start\":63994},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":226111432},\"end\":64701,\"start\":64341},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":231847016},\"end\":65151,\"start\":64703},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b47\",\"matched_paper_id\":153312225},\"end\":65795,\"start\":65153},{\"attributes\":{\"doi\":\"arXiv:1804.06459\",\"id\":\"b48\"},\"end\":66038,\"start\":65797}]", "bib_title": "[{\"end\":51331,\"start\":51274},{\"end\":52210,\"start\":52108},{\"end\":52567,\"start\":52500},{\"end\":52956,\"start\":52899},{\"end\":53371,\"start\":53331},{\"end\":53772,\"start\":53652},{\"end\":54791,\"start\":54697},{\"end\":55517,\"start\":55449},{\"end\":55883,\"start\":55845},{\"end\":56763,\"start\":56698},{\"end\":57216,\"start\":57147},{\"end\":57780,\"start\":57711},{\"end\":58377,\"start\":58296},{\"end\":59257,\"start\":59194},{\"end\":59825,\"start\":59763},{\"end\":60787,\"start\":60725},{\"end\":61291,\"start\":61216},{\"end\":62957,\"start\":62919},{\"end\":63281,\"start\":63246},{\"end\":63700,\"start\":63637},{\"end\":64078,\"start\":63994},{\"end\":64377,\"start\":64341},{\"end\":64758,\"start\":64703},{\"end\":65186,\"start\":65153}]", "bib_author": "[{\"end\":50591,\"start\":50581},{\"end\":50602,\"start\":50591},{\"end\":50612,\"start\":50602},{\"end\":50622,\"start\":50612},{\"end\":50914,\"start\":50903},{\"end\":50924,\"start\":50914},{\"end\":50934,\"start\":50924},{\"end\":50941,\"start\":50934},{\"end\":51186,\"start\":51176},{\"end\":51344,\"start\":51333},{\"end\":51354,\"start\":51344},{\"end\":51363,\"start\":51354},{\"end\":51373,\"start\":51363},{\"end\":51383,\"start\":51373},{\"end\":51400,\"start\":51383},{\"end\":51410,\"start\":51400},{\"end\":51698,\"start\":51688},{\"end\":51714,\"start\":51698},{\"end\":51724,\"start\":51714},{\"end\":51732,\"start\":51724},{\"end\":51982,\"start\":51970},{\"end\":52225,\"start\":52212},{\"end\":52239,\"start\":52225},{\"end\":52581,\"start\":52569},{\"end\":52751,\"start\":52743},{\"end\":52767,\"start\":52751},{\"end\":52970,\"start\":52958},{\"end\":52985,\"start\":52970},{\"end\":52995,\"start\":52985},{\"end\":53001,\"start\":52995},{\"end\":53007,\"start\":53001},{\"end\":53017,\"start\":53007},{\"end\":53025,\"start\":53017},{\"end\":53388,\"start\":53373},{\"end\":53398,\"start\":53388},{\"end\":53409,\"start\":53398},{\"end\":53420,\"start\":53409},{\"end\":53429,\"start\":53420},{\"end\":53783,\"start\":53774},{\"end\":53795,\"start\":53783},{\"end\":53805,\"start\":53795},{\"end\":53815,\"start\":53805},{\"end\":53828,\"start\":53815},{\"end\":53837,\"start\":53828},{\"end\":54360,\"start\":54349},{\"end\":54525,\"start\":54502},{\"end\":54806,\"start\":54793},{\"end\":54817,\"start\":54806},{\"end\":54829,\"start\":54817},{\"end\":54836,\"start\":54829},{\"end\":54847,\"start\":54836},{\"end\":54857,\"start\":54847},{\"end\":55532,\"start\":55519},{\"end\":55541,\"start\":55532},{\"end\":55550,\"start\":55541},{\"end\":55560,\"start\":55550},{\"end\":55894,\"start\":55885},{\"end\":55903,\"start\":55894},{\"end\":56165,\"start\":56153},{\"end\":56173,\"start\":56165},{\"end\":56181,\"start\":56173},{\"end\":56387,\"start\":56377},{\"end\":56400,\"start\":56387},{\"end\":56412,\"start\":56400},{\"end\":56775,\"start\":56765},{\"end\":56785,\"start\":56775},{\"end\":56796,\"start\":56785},{\"end\":56812,\"start\":56796},{\"end\":56826,\"start\":56812},{\"end\":56834,\"start\":56826},{\"end\":57229,\"start\":57218},{\"end\":57241,\"start\":57229},{\"end\":57250,\"start\":57241},{\"end\":57262,\"start\":57250},{\"end\":57271,\"start\":57262},{\"end\":57282,\"start\":57271},{\"end\":57791,\"start\":57782},{\"end\":57809,\"start\":57791},{\"end\":58135,\"start\":58124},{\"end\":58145,\"start\":58135},{\"end\":58388,\"start\":58379},{\"end\":58397,\"start\":58388},{\"end\":58407,\"start\":58397},{\"end\":58415,\"start\":58407},{\"end\":58771,\"start\":58759},{\"end\":58786,\"start\":58771},{\"end\":59004,\"start\":58993},{\"end\":59010,\"start\":59004},{\"end\":59020,\"start\":59010},{\"end\":59268,\"start\":59259},{\"end\":59281,\"start\":59268},{\"end\":59294,\"start\":59281},{\"end\":59302,\"start\":59294},{\"end\":59536,\"start\":59523},{\"end\":59545,\"start\":59536},{\"end\":59721,\"start\":59709},{\"end\":59844,\"start\":59827},{\"end\":59857,\"start\":59844},{\"end\":59874,\"start\":59857},{\"end\":59887,\"start\":59874},{\"end\":60227,\"start\":60219},{\"end\":60234,\"start\":60227},{\"end\":60242,\"start\":60234},{\"end\":60557,\"start\":60546},{\"end\":60569,\"start\":60557},{\"end\":60799,\"start\":60789},{\"end\":60810,\"start\":60799},{\"end\":60823,\"start\":60810},{\"end\":61089,\"start\":61075},{\"end\":61302,\"start\":61293},{\"end\":61313,\"start\":61302},{\"end\":61324,\"start\":61313},{\"end\":61332,\"start\":61324},{\"end\":61690,\"start\":61680},{\"end\":61700,\"start\":61690},{\"end\":61964,\"start\":61951},{\"end\":61974,\"start\":61964},{\"end\":62147,\"start\":62140},{\"end\":62155,\"start\":62147},{\"end\":62162,\"start\":62155},{\"end\":62169,\"start\":62162},{\"end\":62176,\"start\":62169},{\"end\":62184,\"start\":62176},{\"end\":62418,\"start\":62404},{\"end\":62585,\"start\":62576},{\"end\":62594,\"start\":62585},{\"end\":62604,\"start\":62594},{\"end\":62612,\"start\":62604},{\"end\":62618,\"start\":62612},{\"end\":62624,\"start\":62618},{\"end\":62635,\"start\":62624},{\"end\":62645,\"start\":62635},{\"end\":62660,\"start\":62645},{\"end\":62669,\"start\":62660},{\"end\":62681,\"start\":62669},{\"end\":62970,\"start\":62959},{\"end\":62985,\"start\":62970},{\"end\":62995,\"start\":62985},{\"end\":63294,\"start\":63283},{\"end\":63428,\"start\":63422},{\"end\":63443,\"start\":63428},{\"end\":63453,\"start\":63443},{\"end\":63712,\"start\":63702},{\"end\":63721,\"start\":63712},{\"end\":63731,\"start\":63721},{\"end\":63742,\"start\":63731},{\"end\":64090,\"start\":64080},{\"end\":64101,\"start\":64090},{\"end\":64111,\"start\":64101},{\"end\":64122,\"start\":64111},{\"end\":64389,\"start\":64379},{\"end\":64395,\"start\":64389},{\"end\":64406,\"start\":64395},{\"end\":64416,\"start\":64406},{\"end\":64422,\"start\":64416},{\"end\":64439,\"start\":64422},{\"end\":64449,\"start\":64439},{\"end\":64458,\"start\":64449},{\"end\":64770,\"start\":64760},{\"end\":64781,\"start\":64770},{\"end\":64796,\"start\":64781},{\"end\":64803,\"start\":64796},{\"end\":64822,\"start\":64803},{\"end\":64832,\"start\":64822},{\"end\":64841,\"start\":64832},{\"end\":65197,\"start\":65188},{\"end\":65203,\"start\":65197},{\"end\":65211,\"start\":65203},{\"end\":65865,\"start\":65856},{\"end\":65871,\"start\":65865},{\"end\":65880,\"start\":65871}]", "bib_venue": "[{\"end\":50677,\"start\":50638},{\"end\":50901,\"start\":50823},{\"end\":51174,\"start\":51137},{\"end\":51459,\"start\":51410},{\"end\":51786,\"start\":51748},{\"end\":51968,\"start\":51934},{\"end\":52286,\"start\":52239},{\"end\":52606,\"start\":52581},{\"end\":52786,\"start\":52767},{\"end\":53077,\"start\":53025},{\"end\":53467,\"start\":53429},{\"end\":53926,\"start\":53837},{\"end\":54347,\"start\":54317},{\"end\":54500,\"start\":54448},{\"end\":54906,\"start\":54857},{\"end\":55612,\"start\":55560},{\"end\":55937,\"start\":55903},{\"end\":56151,\"start\":56064},{\"end\":56495,\"start\":56412},{\"end\":56886,\"start\":56834},{\"end\":57371,\"start\":57282},{\"end\":57858,\"start\":57809},{\"end\":58122,\"start\":58027},{\"end\":58464,\"start\":58415},{\"end\":58757,\"start\":58657},{\"end\":59318,\"start\":59302},{\"end\":59521,\"start\":59477},{\"end\":59707,\"start\":59688},{\"end\":59936,\"start\":59887},{\"end\":60217,\"start\":60136},{\"end\":60544,\"start\":60427},{\"end\":60851,\"start\":60823},{\"end\":61073,\"start\":61007},{\"end\":61382,\"start\":61332},{\"end\":61678,\"start\":61584},{\"end\":61949,\"start\":61891},{\"end\":62138,\"start\":62088},{\"end\":62402,\"start\":62360},{\"end\":63047,\"start\":62995},{\"end\":63315,\"start\":63294},{\"end\":63804,\"start\":63742},{\"end\":64149,\"start\":64122},{\"end\":64507,\"start\":64458},{\"end\":64893,\"start\":64841},{\"end\":65338,\"start\":65215},{\"end\":65854,\"start\":65797},{\"end\":54002,\"start\":53928},{\"end\":57447,\"start\":57373},{\"end\":65466,\"start\":65373}]"}}}, "year": 2023, "month": 12, "day": 17}