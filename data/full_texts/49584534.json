{"id": 49584534, "updated": "2023-07-19 15:43:06.899", "metadata": {"title": "Deep code comment generation", "authors": "[{\"first\":\"Xing\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Ge\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Xia\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Lo\",\"middle\":[]},{\"first\":\"Zhi\",\"last\":\"Jin\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 26th Conference on Program Comprehension", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2884276923", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iwpc/HuLXLJ18", "doi": "10.1145/3196321.3196334"}}, "content": {"source": {"pdf_hash": "15f16252af84a630f1f9442d4e0367b6fba089cf", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "https://ink.library.smu.edu.sg/sis_research/4292", "status": "GREEN"}}, "grobid": {"id": "24d4408dfc1e95e0d396a3bbf48d3965afba43a1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/15f16252af84a630f1f9442d4e0367b6fba089cf.txt", "contents": "\nDeep Code Comment Generation\nACMCopyright ACM2018. May 27-28. 2018. May 27-28. 2018\n\nXing Hu \nMinistry of Education\nKey Laboratory of High Confidence Software Technologies (Peking University)\n\n\nInstitute of Software\nEECS\nPeking University\nBeijingChina\n\nGe Li \nMinistry of Education\nKey Laboratory of High Confidence Software Technologies (Peking University)\n\n\nInstitute of Software\nEECS\nPeking University\nBeijingChina\n\nXin Xia 3xin.xia@monash.edu \nFaculty of Information Technology\nMonash University\nAustralia\n\nDavid Lo 4davidlo@smu.edu.sg \nSchool of Information Systems\nSingapore Management University\nSingapore 1\n\nZhi Jin zhijin@pku.edu.cn \nMinistry of Education\nKey Laboratory of High Confidence Software Technologies (Peking University)\n\n\nInstitute of Software\nEECS\nPeking University\nBeijingChina\n\nXing Hu \nMinistry of Education\nKey Laboratory of High Confidence Software Technologies (Peking University)\n\n\nInstitute of Software\nEECS\nPeking University\nBeijingChina\n\nGe Li \nMinistry of Education\nKey Laboratory of High Confidence Software Technologies (Peking University)\n\n\nInstitute of Software\nEECS\nPeking University\nBeijingChina\n\nXin Xia \nFaculty of Information Technology\nMonash University\nAustralia\n\nDavid Lo \nSchool of Information Systems\nSingapore Management University\nSingapore 1\n\nZhi Jin \nMinistry of Education\nKey Laboratory of High Confidence Software Technologies (Peking University)\n\n\nInstitute of Software\nEECS\nPeking University\nBeijingChina\n\nDeep Code Comment Generation\n\nICPC '18: 26th IEEE/ACM International Confernece on Program Comprehension\nGothenburg, Sweden; New York, NY, USA; Gothenburg, SwedenACM112018. May 27-28. 2018. May 27-28. 201810.1145/3196321.3196334* This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201, and the National Natural Science Foun-dation of China under Grant Nos. 61232015 and 61620106007. Zhi Jin and Ge Li are corresponding authors./18/05. . . $15.00\nDuring software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.CCS CONCEPTS\u2022 Software and its engineering \u2192 Documentation; \u2022 Computing methodologies \u2192 Neural networks; KEYWORDS program comprehension, comment generation, deep learning ACM Reference Format:\n\nINTRODUCTION\n\nIn software development and maintenance, developers spend around 59% of their time on program comprehension activities [45]. Previous studies have shown that good comments are important to program comprehension, since developers can understand the meaning of a piece of code by using the natural language description of the comments [35]. Unfortunately, due to tight project schedule and other reasons, code comments are often mismatched, missing or outdated in many projects. Automatic generation of code comments can not only save developers' time in writing comments, but also help in source code understanding.\n\nMany approaches have been proposed to generate comments for methods [24,35] and classes [25] of Java, which is the most popular programming language in the past 10 years 1 . Their techniques vary from the use of manually-crafted [25] to Information Retrieval (IR) [14,15]. Moreno et al. [25] defined heuristics and stereotypes to synthesize comments for Java classes. These heuristics and stereotypes are used to select information that will be included in the comment. Haiduc et al. [14,15] applied IR approaches to generate summaries for classes and methods. IR approaches such as Vector Space Model (VSM) and Latent Semantic Indexing (LSI) usually search comments from similar code snippets. Although promising, these techniques have two main limitations: First, they fail to extract accurate keywords used for identifying similar code snippets when identifiers and methods are poorly named. Second, they rely on whether similar code snippets can be retrieved and how similar the snippets are.\n\nRecent years have seen an emerging interest in building probabilistic models for large-scale source code. Hindle et al. [17] have addressed the naturalness of software and demonstrated that code can be modeled by probabilistic models. Several subsequent studies have developed various probabilistic models for different software tasks [12,23,40,41]. When applied to code summarization, different from IR-based approaches, existing probabilistic-model-based approaches usually generate comments directly from code instead of synthesizing them from keywords. One of such probabilistic-modelbased approaches is by Iyer et al. [19] who propose an attentionbased Recurrent Neural Network (RNN) model called CODE-NN. It builds a language model for natural language comments and aligns the words in comments with individual code tokens directly by attention component. CODE-NN recommends code comments given source code snippets extracted from Stack Overflow. Experimental results demonstrate the effectiveness of probabilistic models on code summarization. These studies provide principled methods for probabilistically modeling and resolving ambiguities both in natural language descriptions and in the source code.\n\nIn this paper, to utilize the advantage of deep learning techniques, we propose a novel approach DeepCom to generate descriptive comments for Java methods which are functional units of Java language. DeepCom builds upon advances in Neural Machine Translation (NMT). NMT aims to automatically translate from one language (e.g., Chinese) to another language (e.g., English) and it has been shown to achieve great success for natural language corpora [6,37]. Intuitively, generating comments can be considered as a variant of the NMT problem, where source code written in a programming language needs to be translated to text in natural language. Compared to CODE-NN which only builds a language model for comments, the NMT model builds language models for both source code and comments. The words in comments align with the RNN hidden states which involve the semantics of code tokens. Deep-Com generates comments by automatically learning from features (e.g., identifier names, formatting, semantics, and syntax features) extracted from a large-scale Java corpus. Different from traditional machine translation, our task is challenging since:\n\n(1) Source code is structured: In contrast to natural language text which is weakly structured, programming languages are formal languages and source code written in them are unambiguous and structured [3]. Many probabilistic models used in NMT are sequence-based models that need to be adapted to structured code analysis. The main challenge and opportunity is how to take advantage of rich and unambiguous structure information of source code to boost effectiveness of existing NMT techniques. (2) Vocabulary: In natural language (NL) corpora normally used for NMT, the vocabulary is usually limited to the most common words, e.g., 30,000 words, and words outside the vocabulary are treated as unknown words -often marked as UNK . It is effective for such NL corpora because words outside the dominant vocabulary are so rare. In code corpora, the vocabulary consists of keywords, operators, and identifiers. It is common for developers to define various new identifiers, and thus they tend to proliferate. In our dataset, we get 794,711 unique tokens after replacing numerals and strings with generic tokens NUM and STR . In a codebase used to build probabilistic models, there are likely to be many out-of-vocabulary identifiers. As Table 1 illustrates, there are 794,621 unique identifiers in our dataset. If we use most common 30,000 tokens as the code vocabulary, about 95 % identifiers will be regarded as UNK . Hellendoorn and Devanbu [16] have demonstrated that it is unreasonable for source code to use such a vocabulary.\n\nTo address these issues, DeepCom customizes a sequence-based language model to analyze Abstract Syntax Trees (AST) which capture structures and semantics of Java methods. The ASTs are converted into sequences before they are fed into DeepCom. It is generally accepted that a tree cannot be restored from a sequence generated by a classical traversal method such as pre-order traversal and post-order traversal. To better present the structure of ASTs, and keep the sequences unambiguous, we propose a new structure-based traversal (SBT) method to traverse ASTs. Using  SBT, a subtree under a given node is included into a pair of brackets. The brackets represent the structure of the AST and we can restore a tree unambiguously from a sequence generated using SBT.\n\nMoreover, to address the vocabulary challenge, we propose a new method to represent unknown tokens. The tokens in AST sequences include terminal nodes, non-terminal nodes, and brackets in our work. The unknown tokens come from the terminal tokens of ASTs. We replace the unknown tokens with their types instead of a universal special UNK token.\n\nDeepCom generates comments word-by-word from AST sequences. We train and evaluate DeepCom on the Java dataset that consists of 9,714 Java projects from GitHub. The experimental results show that DeepCom can generate informative comments. Additionally, the results show that DeepCom achieves the best performance when compared with a number of baselines including the state-of-the-art approach by Iyer et al. [19].\n\nThe main contributions of this paper are as follows:\n\n\u2022 We formulate code comments generation task as a machine translation task. \u2022 We customize a sequence-based model to process structural information extracted from source code to generate comments for Java methods. In particular, we propose a new AST traversal method (namely structure-based traversal) and a domain-specific method to deal with out-of-vocabulary tokens better.\n\nPaper organization. The remainder of this paper is organized as follows. Section II presents background materials on language models and NMT. Section III elaborates on the details of DeepCom. Section IV and Section V present the experiment setup and results. Section VI discusses strengths of DeepCom, and threats to validity. Section VII surveys the related work. Finally, Section VIII concludes the paper and points out potential future directions.\n\n\nBACKGROUND 2.1 Language Models\n\nOur work is inspired by the machine translation problem in the NLP field. We exploit the language models learning from a large-scale source code corpus. The models generate code comments from the learned features. The language models learn the probabilistic distribution over sequences of words. They work tremendously well on a large variety of problem (e.g., machine translation [6], speech recognition [9], and question answering [46]). For a sequence x = (x 1 , x 2 , ..., x n ) (e.g., a statement), the language model aims to estimate the probability of it. The probability of a sequence is computed via each of its tokens. That is,\n&# $ % # % # # % $ ! \" ! )! %# % ' % % # % # !& # !& ! )! ! %# ! %# ! )! # # % ! )! ! % # + + + + + (%# %$ # \"& $% % # + (%# %$ # \"& $% % %&# & % % $P (x ) = P (x 1 )P (x2|x 1 )...P (x n |x 1 ...x n\u22121 )(1)\nIn this paper, we adopt a language model based on the deep neural network called Long Short-Term Memory (LSTM) [18]. LSTM is one of the state-of-the-art RNNs. LSTM outperforms general RNN because it is capable of learning long-term dependencies. It is a natural model to use for source code which has long dependencies (e.g., a class is used far away from its import statement). The details of RNN and LSTM are shown in Figure 1.\n\n\nRecurrent Neural Networks.\n\nRNNs are intimately related to sequences and lists because of their chain-like natures. It can in principle map from the entire history of previous inputs to each output. At each time step t, the unit in the RNN takes not only the input of the current step but also the hidden state outputted by its previous time step t \u2212 1. As Figure 1(a) illustrates, the hidden state of time step t is updated according to the input vector x t and its previous hidden\nstate h t \u22121 , namely, h t = tanh(W x t + U h t \u22121 + b)\nwhere W , U , and b are the trainable parameters which are updated while training, and tanh is the activation function: tanh(z) = (e z \u2212 e \u2212z )/(e z + e (\u2212z ) ).\n\nA prominent drawback of the standard RNN model is that gradients may explode or vanish during the back-propagation. These phenomena often appear when long dependencies exist in the sequences. To address these problems, some researchers have proposed several variants to preserve long-term dependencies. These variants include LSTM and Gated Recurrent Unit (GRU). In this paper, we adopt the LSTM which has achieved success on many NLP tasks [6,37].\n\n\nLong Short-Term\n\nMemory. LSTM introduces a structure called the memory cell to solve the problem that ordinary RNN is difficult to learn long-term dependencies in the data. The LSTM is trained to selectively \"forget\" information from the hidden states, thus allowing room to take in more important information [18]. LSTM introduces a gating mechanism to control when and how to read previous information from the memory cell and write new information. The memory cell vector in the recurrent unit preserves long-term dependencies. In this way, LSTM handles long-term dependencies more effectively than vanilla RNN. LSTM has been widely used to solve semantically related tasks and has achieved convincing performance. These advantages motivate us to exploit LSTM for building models for source code and comments. Figure   1(b) illustrates a typical LSTM unit and for more details of LSTM, please refer to [10,18].\n\n\nNeural Machine Translation\n\nNMT [44] is an end-to-end learning approach for automated translation. It is a deep learning based approach and has made rapid progress in recent years. NMT has shown impressive results surpassing those of phrase-based systems while addressing shortcomings such as the need for hand engineered features. Its architecture typically consists of two RNNs, one to consume the input text sequences and the other one to generate the translated output sequences. It is often accompanied by an attention mechanism that aligns target with source tokens [6].\n\nNMT bridges the gap between different natural languages. Generating comments from the source code is a variant of machine translation problem between the source code and the natural language. We explore whether the NMT approach can be applied to comments generation. In this paper, we follow the common Sequence-to-Sequence (Seq2Seq) [37] learning framework with attention [6] which helps cope effectively with the long source code.\n\n\nPROPOSED APPROACH\n\nThe transition process between source code and comments is similar to the translation process between different natural languages. Existing research has applied machine translation methods translating code from one source language (e.g., Java) to another (e.g., C#) [13]. A few studies adopt machine translation method for generating natural language descriptions from the source code. Oda et al. [30] present a machine translation approach to generate natural language Pseudo-code of the source code at the statement level. In this paper, DeepCom translates the source code to a high-level description at the method level.\n\nThe overall framework of DeepCom is illustrated in Figure 2. DeepCom mainly consists of three stages: data processing, model training, and online testing. The source code we obtained from GitHub is parsed and preprocessed into a parallel corpus of Java methods and their corresponding comments. In order to learn the structural information, the Java methods are converted into AST sequences by a special traversal approach before input into the model. With the parallel corpus of AST sequences and comments, we build and train generative neural models based on the idea of NMT. There are two challenges during training process: \u2022 How to represent ASTs to store the structural information and keep the representation unambiguous while traversing the ASTs? \u2022 How to deal with out-of-vocabulary tokens in source code?\n\nIn the following paragraphs, we will introduce the details of the model and the approaches we propose to resolve the abovementioned challenges.\n\n\nSequence-to-Sequence Model\n\nIn this paper, we apply a Sequence-to-Sequence (Seq2Seq) model to learn source code and generate comments. Seq2Seq model is widely used for machine translation [37], text summarization [34], dialogue system [39], etc. The model consists of three components, an Encoder, a Decoder, and an Attention component, in which the Encoder and Decoder are both LSTMs. Figure 3 illustrates the detailed Seq2Seq model.\n\n\nEncoder.\n\nThe encoder is an LSTM we describe in Section 2 and responsible for learning the source code. At each time step t, it reads one token x t of the sequence, then updates and records the current hidden state s t , namely,\ns t = f (x t , s t \u22121 )(2)\nwhere f is an LSTM unit that maps a word of source language x t into a hidden state s t . The encoder learns latent features from source code, and the features are encoded into the context vector c. These latent features include the identifiers naming conventions, control structures, and etc. In this paper, DeepCom adopts the attention mechanism to compute the context vector c.\n\n\nAttention.\n\nAttention mechanism is a recent model that selects the important parts from the input sequence for each target word. For example, the token \"whether\" in comments usually aligns with the \"if\" statements in the source code. The generation of each word is guided by a classic attention method proposed by Bahdanau et al. [6].\n\nIt defines individual c i for predicting each target word y i as a weighted sum of all hidden states s 1 , .., s m in encoder and computed as\nc i = m j=1 \u03b1 i j s j(3)\nThe weight \u03b1 i j of each hidden state s j is computed as\n\u03b1 i j = exp(e i j ) m k =1 exp(e ik )(4)\nand\ne i j = a(h i\u22121 , s j )(5)\nis an alignment model which scores how well the inputs around position j and the output at position i match.\n\n\nDecoder.\n\nThe Decoder aims to generate the target sequence y by sequentially predicting the probability of a word y i conditioned on the context vector c i and its previous generated words y 1 , ..., y i\u22121 , i.e.,\np(y i |y 1 , ..., y i\u22121 , x ) = \u0434(y i\u22121 , h i , c i )(6)\nwhere \u0434 is used to estimate the probability of the word y i . The goal of the model is to minimize the cross-entropy, i.e., minimize the following objective function:\nH (y) = \u2212 1 N N i=1 n j=1 lo\u0434p(y (i ) j )(7)\nwhere N is the total number of training instances, and n is the length of each target sequence. y (i ) j means the jth word in the ith instance. Through optimizing the objective function using optimization algorithms such as gradient descendant, the parameters can be estimated.\n\n\nAbstract Syntax Tree with SBT traversal\n\nTranslation between source code and NL is challenging due to the structure of source code. One simple way to model source code is to just view it as plain text. However, in such way, the structure information will be omitted, which will cause inaccuracies in the generated comments. To learn the semantic and syntactic information at the same time, we convert the ASTs into specially formatted sequences by traversing the ASTs. Sequences obtained by classical traversal methods (e.g., pre-order traversal) are lossy since the original ASTs cannot unambiguously be reconstructed back from them. This ambiguity may cause different Java methods (each with different comments) to be mapped to the same sequence representation. It is confusing for the neural network if there are multiple labels (in our setting, comments) given to a specific input. For addressing this problem, we propose a Structure-based Traversal (SBT) method to traverse the AST. The details are presented in Algorithm 1. Figure 4 illustrates a simple example of SBT to traverse a tree and the detailed procedure is as follows:\n\n\u2022 From the root node, we first use a pair of brackets to represent the tree structure and put the root node itself behind the right bracket, that is (1)1, shown in Figure 4. \u2022 Next, we traverse the subtrees of the root node and put all root nodes of subtrees into the brackets, i.e., (1 (2)2(3)3)1. \u2022 Recursively, we traverse each subtree until all nodes are traversed and the final sequence (1(2(4)4(5)5(6)6)2(3)3)1 is obtained.\nAlgorithm 1 Structure-based Traversal 1: procedure SBT(r )\nTraverse a tree from root r 2: seq \u2190 \u2205 seq is the sequence of a tree after traversal 3: if !r .hasChild then 4: seq \u2190 (r )r Add brackets for terminal nodes 5: else 6: seq \u2190 (r Add left bracket for non-terminal nodes 7: for c in childs do 8: seq \u2190 seq + SBT (c) 9: seq \u2190 seq+)r Add right bracket for non-terminal nodes after traversing all their children 10: return seq DeepCom processes each AST into a sequence following the SBT algorithm. For example, the AST sequence of the following Java method extracted from project Eclipse Che 2 is shown in Figure 5: The left part of Figure 5 is the AST of the method. The non-terminal nodes (those without boxes) illustrate the structural information of source code. They have the feature \"type\" which is a fixed set (e.g., IfStatement, Block, and ReturnStatement). The terminal nodes (those within boxes) not only have \"type\" but also have \"value\" (token within brackets). The \"value\" is the concrete token occurring in the source code and \"type\" indicates the type of the token. The right part of the figure is the sequence constructed by traversing the AST. The terminal nodes are represented by their \"type\" and \"value\" (connected by \"_\"), such as \"log\" is represented by \"Sim-pleName_Log\". The non-terminal nodes are represented by their \"type\". A subtree is included in a pair of brackets and we can restore the AST from the given sequence easily. In this way, we can keep the structural information and make the representation losslessthe original AST can be unambiguously reconstructed from the sequence.\n\n\nOut-of-vocabulary tokens\n\nVocabulary is another challenge to model source code [16]. In NL, studies usually limit vocabulary to the most common words (e.g., top 30,000) during data processing. The out-of-vocabulary tokens are replaced by a special unknown token, e.g., UNK . It is effective for NLP because words outside vocabulary are so rare. However, this method is arguably inappropriate when it comes to source code. In addition to fixed operators and keywords, there are user-defined identifiers which take up the majority of code tokens [7]. These identifiers have a substantial influence on the vocabulary of language models. If we keep a regular vocabulary size for source code, there will be many unknown tokens. If we want the occurrences of UNK tokens to be as few as possible, the vocabulary size will increase a lot. A large vocabulary size will make it difficult to train a deep learning model since it requires more training data, time, and memory. To achieve optimal and stable results, models need to run a larger number of iterations to tune the parameters for each word in the vocabulary. Hence, we propose a new method to represent the out-of-vocabulary tokens for source code. In AST, the non-terminal nodes have \"type\" feature, and terminal nodes not only have \"type\" feature, but also have \"value\" feature. DeepCom takes the AST sequences as inputs, the vocabulary consists of brackets, all \"type\" of nodes (including non-terminal nodes T non and terminal nodes T term ), and partial type-value pairs of terminal tokens. We keep the tokens which appear in the most frequent 30,000 tokens as the AST sequences vocabulary. For the type-value pairs outside the vocabulary, Deep-Com uses their \"type\" T term instead of the UNK token to replace them. For example, for the terminal nodes \"extractFor\" and \"id\" in the code presented above, their types are both \"SimpleName\" as shown in Figure 5. The tokens input into the model should be \"Sim-pleName_extractFor\" and \"SimpleName_id\" respectively. However, since the token \"SimpleName_extractFor\" is out of the vocabulary, we use its type \"SimpleName\" representing it instead. In this way, the out-of-vocabulary tokens are represented by their related type information instead of the meaningless word.\n2 https://github.com/eclipse/che & ! $ & ! ! $ \"' \" +\" \" &$ $ $ & ! \" +\" \" \" & $ ! *\"$ %% ! & & & & ! (! & ! \" ! \" ' \" *&$ & & ! ) & ,- \" &'$ & & & & ! (! & ! \" $ #' %& \" $ !( \" \" *&$ & !$ & ! $ & ! ! $ \"' ! $ \"' \" +\" \" &$ \" &$ \" +\" $ $ & ! \" +\" \" & $ \" & $ \" +\" \" \" $ $ & ! ! *\"$ %% ! & & & & ! (! & ! \" \" \" ' \" ' \" *&$ & & ! ) & ,- \" *&$ & & ! ) & ,- \" \" & ! (! & ! *\"$ %% ! & & & &'$ & & & & ! (! & ! \" $ #' %& \" $ #' %& \" $ !( \" $ !( \" \" & ! (! & ! &'$ & & & ! \" *&$ & !$ \" *&$ & !$ & ! $ & !\n\nEXPERIMENT SETUP\n\nThen we use the Eclipse's JDT compiler 3 to parse the Java methods into ASTs and extract corresponding Javadoc comments which are standard comments for Java methods. The methods without Javadoc are omitted in this paper. For each method with a comment, we use the first sentence appeared in its Javadoc description as  the comment since it typically describes the functionalities of Java methods according to Javadoc guidance 4 . Empty or just one-word descriptions are filtered out in this work because these comments have no ability to express the Java methods functionalities. We also exclude the setter, getter, constructor and test methods, since they are easy for a model to generate the comments. Finally, we get 588,108 Java method, comment pairs 5 . Similar to Jiang et al. [20]'s work, we randomly select 80% of the pairs for training, 10% of the pairs for validation, and rest 10% for testing. Table 1 and Table 2 illustrate statistics of the corpus. We also give the details of methods lengths and comments lengths. The average lengths of Java methods and comments are 99.94 and 8.86 tokens in this corpus. We find that more than 95% code comments have no more than 50 words and about 90% Java methods no longer than 200 tokens.\n\nDuring the training, the numerals and strings are replaced with generic tokens NUM and STR respectively. The maximum length of AST sequences is set to 400. We use a special symbol PAD to pad the shorter sequences and the longer sequences will be cut into sequences with 400 tokens. We add special tokens START and EOS to the decoder sequences during training. START is the start of the decoding sequence and the EOS means the end of it. The maximum comment length is set to 30. The vocabulary sizes for AST sequences and comments are both 30,000 in this paper. While there is no UNK in ASTs sequences, there are a few out-of-vocabulary tokens in comments that are replaced by UNK .\n\n\nTraining Details\n\nThe model is validated every 2,000 minibatches on the validation set by BLEU [31] which is a commonly used automatic metric for NMT. Training runs for about 50 epochs and we select the best model that has best results on the validation set as the final model. The model is then evaluated on the test set by computing average BLEU scores and the results will be discussed in Section 5. All models are implemented using the Tensorflow framework 6 and extended based on the Seq2Seq model in Tensorflow tutorials 7 . The parameters are shown as follows:\n\n\u2022 The SGD (with minibatch size 100 randomly chosen from training instances) is used to train the parameters. \u2022 DeepCom uses two-layered LSTMs with 512 dimensions of the hidden states and 512-dimensional word embeddings. \u2022 The learning rate is set to 0.5 and we clip the gradients norm by 5. The learning rate is decayed using the rate 0.99. \u2022 To prevent over-fitting, we use dropout with 0.5.\n\n\nEvaluation Measure: BLEU-4\n\nDeepCom uses machine translation evaluation metrics BLEU-4 score [31] to measure the quality of generated comments. BLEU score is a widely-used accuracy measure for NMT [22] and has been used in software tasks evaluation [12,20]. It calculates the similarity between the generated sequence and reference sequence (usually a human-written sequence). The BLEU score ranges from 1 to 100 as a percentage value. The higher the BLEU, the closer the candidate is to the reference. If the candidate is completely equal to the reference, the BLEU becomes 100%. Jiang et al. [20] exploit it to evaluate the generated summaries for commit messages. Gu et al. [12] use BLEU to evaluate the accuracy of generated API sequences from natural language queries. Their experiments show that BLEU score is reasonable to measure the accuracy of generated sequences.\n\nIt computes the n-gram precision of a candidate sequence to the reference. The score is computed as:\nBLEU = BP \u00b7 exp( N n=1 w n lo\u0434p n )(8)\nwhere p n is the ratio of length n subsequences in the candidate that are also in the reference. In this paper, we set N to 4, which is the maximum number of grams. BP is brevity penalty,\nBP = 1 i f c > r e (1\u2212r /c ) i f c \u2264 r (9)\nwhere c is the length of the candidate translation and r is the effective reference sequence length. In this paper, we regard a generated comment as a candidate and a programmer-written comment (extracted from Javadoc) as a reference.\n\n\nRESULTS\n\nIn this section, we evaluate different approaches by measuring their accuracy on generating Java methods' comments. Specifically, we mainly focus on the following research questions:\n\n\u2022 RQ1: How effective is DeepCom compared with the state-ofthe-art baseline? \u2022 RQ2: How effective is DeepCom to source code and comments of varying lengths?\n\n\nRQ1: DeepCom vs. Baseline\n\n\nBaseline.\n\nWe compare DeepCom with CODE-NN [19] which is a state-of-the-art code summarization approach and also a deep learning based method. CODE-NN is an end-to-end generation We also compare DeepCom with its variants, that are, the basic Seq2Seq model, the attention based Seq2Seq model, and DeepCom with a classical traversal method (i.e., pre-order traversal). The Seq2Seq model and the attention based Seq2Seq model take the source code as inputs. They aim to evaluate the effectiveness of NMT approaches for comments generation. To evaluate the effectiveness of SBT, we compare SBT with one of the most ordinary traversal methods -pre-order traversal. In addition, we also compare DeepCom with CODE-NN on the dataset that CODE-NN uses.\n\n\nResults.\n\nWe measure the gap between automatically generated comments and human-written comments. The difference is evaluated by a machine translation metric, i.e., BLEU-4 score. Table  3 illustrates the average BLEU-4 scores of different approaches to generating comments for Java methods. The accuracy of machine translation model Seq2Seq substantially outperforms CODE-NN. CODE-NN fails to learn the semantic of the source code when it generates comments from token embeddings of source code directly. Seq2Seq model exploits RNN to build a language model for the source code and effectively learns the semantics of Java methods. The BLEU-4 score increases further while integrating the structural information. Compared to DeepCom with the pre-order traversal, the SBT based model is much more capable of learning semantic and syntactic information within Java methods. In a word, the improvement of our proposed DeepCom (SBT) over CODE-NN is large. The average BLEU-4 score of DeepCom improves about 13% compared to CODE-NN. The results of DeepCom are comparable to the BLEU scores of state-of-the-art NMT models on natural language translation which are about 40% [21,44].\n\nWe further conduct experiments on the same datasets CODE-NN used, which includes C# and SQL snippets collected from Stack Overflow. The results are shown in Table 4. Since many of the code snippets in their provided dataset are incomplete and hard to parse them into ASTs, we compare the Seq2Seq model with CODE-NN. It highlights that the Seq2Seq outperforms the state-of-the-art method CODE-NN in different languages. The average BLEU scores of Seq2Seq improve more than 10% on various program languages compared to CODE-NN.\n\nThrough the evaluation, we have verified that comments generation task is very similar to machine translation except that the \n\n\nRQ2: BLEU-4 scores for source code and comments of different lengths\n\nWe further analyze the prediction accuracy for Java methods and comments of different lengths. Figure 6 presents the average BLEU-4 scores of DeepCom and CODE-NN for source code and ground truth comments of varying lengths. As Figure 6(a) illustrates, the average BLEU-4 scores tend to be lower when we increase source code length. For most code lengths, the average BLEU-4 scores of DeepCom improve those of CODE-NN by about 10%. For DeepCom, AST lengths grow rapidly as the source code lengths increase and as a result, some features are lost when cutting the long AST sequences into a fixed length sequence during training. For comments of different lengths, DeepCom maintains similar accuracy as shown in Figure 6(b). However, the accuracy of CODE-NN decreases sharply while code comment length increases. When the code comment lengths are greater than 25 tokens, the accuracy of CODE-NN decreases to less than 10%. DeepCom still performs better when we need to generate comments consisting of 25-28 words.\n\n\nDISCUSSION\n\n\nQualitative analysis\n\nHere, we perform qualitative analysis on the human-written comments and comments which are automatically generated by our approach. Table 5 shows some examples of Java methods, the comments generated by DeepCom and human-written comments. By analyzing cases of generated results, we find the cases can be divided into the following situations.\n\n\nExactly correct comments.\n\nDeepCom can generate exactly correct comments from the source code of different lengths (Case 1 and Case 2), which validate the capability of our approach to encode Java methods and decode comments. Generally, DeepCom  performs well when the business logic of these Java methods is clear and code conventions are universal.\n\n\nAlgorithm implementations.\n\nFor the Java methods which are more concerned about algorithm than business logic, Deep-Com can generate accurate comments. The algorithm concerned Java methods usually use similar structures to implement the same algorithm function. As Case 5 shows, the method \"sort\" aims to sort an array using Binary Sort, DeepCom captures the correct functionality and generates the correct comments.\n\n6.1.3 Cases when generated comments are better than humanwritten ones. By analyzing the generated comments and the source code, we find that DeepCom performs better than human written comments when the Java methods aim to determine something true or not. Developers write interrogative sentences as comments sometimes (shown in Case 6 and Case 7). These comments are nonstandard even though they can express the functionalities of Java methods. DeepCom can not only generate accurate comments but also more standard comments.\n\n\nAPI invocations intensive Java methods.\n\nDevelopers usually invoke APIs to implement a specific function. These APIs include platform standard APIs and customized APIs defined by third parties or developers themselves. We find that DeepCom can generate accurate comments when most API invocations are platform standard APIs (shown in Case 1). However, when the majority API invocations in a Java method are customized APIs, DeepCom does not perform as good as human-written comments (shown in Case 4 and Case 9). The influence of API invocations explains that Deep-Com can learn the platform standard APIs usage patterns from a large-scale dataset. However, it can not learn customized APIs well because the customized APIs with the same name have different usage patterns in different programs.\n\n6.1.5 Low BLEU score cases. The results with lower BLEU scores are mainly divided into two types, meaningless sentences, and sentences with clear semantics. The former mainly contains empty sentences and results with too many repetitive words. We conjecture the problems come from out-of-vocabulary words in original comments or mismatch between the Java methods and comments in the original dataset.\n\nIn the latter ones, most of them are irrelevant to original comments in their semantics. There are also some interesting results that hold relevant semantics but gain low BLEU scores (shown in Case 4). The automatically generated and manual comments may describe similar functionalities but with different words or order.\n\n\nUnknown words in generated comments.\n\nThere are unknown words in the generated comments sometimes. As Case 3 shows, DeepCom fails to predict the token \"FactoryConfigu-rationError\" which is the method name defined by developers. DeepCom is not good at learning the method or identifiers names occurred in comments. Developers define various names while programming and most of these tokens appearing at most once in the comments. During the training process, we replaced all unknown identifier tokens in AST sequences with their types, but we do not replace the unknown identifiers occur in comments. It is hard for DeepCom to learn these user-defined tokens in comments that have been replaced by the unknown token UNK .\n\n\nStrengths of DeepCom\n\nA major challenge for generating comments from code is the semantic gap between code and natural language descriptions. Existing approaches are based on manually crafted templates or information retrieval and lack a model to capture the semantic relationship between source code and natural language. DeepCom, a machine translation model, has the ability to bridge the gap between two languages, i.e., programming language and natural language.\n\n6.2.1 Probabilistic model connecting semantics of code and comments. One advantage of DeepCom is generating comments directly by learning source code instead of synthesizing comments from keywords or searching similar code snippets' comments.\n\nSynthesizing comments from keywords usually uses some manually crafted templates. The procedure of templates definition is time-consuming and the quality of keywords depends on the quality of a given Java method. They fail to extract accurate keywords when the identifiers and methods are poorly named. The IR based approaches usually search the similar code snippets and take their comments as the final results. These IR based approaches rely on whether similar code snippets can be retrieved and how similar the snippets are.\n\nDeepCom builds language models for code and natural language descriptions. The language models are able to handle the uncertainty in the correspondence between code and text. DeepCom Automatically generated: Sorts the array in ascending order,using the natural order. Human-written: Rearranges the array in ascending order,using the natural order. Automatically generated: It can be called when the product only or refresh has ended. Human-written: Removes a listener that was previously registered with listenFor-Subscriptions. learns common patterns from a large-scale source code and the encoder itself is a language model which remembers the likelihood of different Java methods. The decoder of DeepCom learns the context of source code which bridges the gap between natural language and code. Furthermore, the attention mechanism helps align code tokens and natural language words.\n\n\nGeneration assisted by structural information.\n\nProgramming languages are formal languages which are more structure dense than text and have formal syntax and semantics. It is difficult for models to learn semantic and syntax information at the same time just given code sequences. Existing approaches usually analyze source code directly and omit its syntax representation.\n\nIn contrast to traditional NMT models, DeepCom takes advantage of rich and unambiguous code structures. In this way, Deep-Com bridges the gap between code and natural language with the assistance of structure information within the source code. From the evaluation results, we find that the structural information improves the quality of comments. The improvements for methods implementing standard algorithms are much more obvious. Java methods realizing the same algorithm may define different variables while their ASTs are much more similar.\n\n\nThreats to Validity\n\nWe have identified the following threats to validity: Automatic evaluation metrics: We evaluate the gap between generated comments and human-written comments by machine translation metric BLEU which is gradually used in generativebased software issues [12,20]. The reason for this setting is that we want to reduce the impact of the subjectivity of manual evaluation.\n\n\nQuality of collected comments:\n\nWe collected the comments for Java methods from the first sentence of Javadoc as other work does [12]. Although we define heuristic rules to decrease the noise in comments, there are some mismatched comments in the dataset. In the future, we will investigate a better technique to build a better parallel corpus.\n\nComparisons on Java dataset: Another threat to validity is that our approach is experimented on Java dataset. Although we fail to evaluate DeepCom directly on CODE-NN' dataset which is difficult to parse into ASTs, the results on Java have proved the effectiveness of DeepCom. In the future, we will extend our approach to other programming languages (e.g., Python).\n\n\nRELATED WORK 7.1 Code Summarization\n\nAs a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. Automatic code summarization approaches vary from manuallycrafted template [24,35,36], IR [14,15,43] to learning-based approaches [4,19,28].\n\nCreating manually-crafted templates to generate code comments is one of the most common code summarization approaches. Sridhara et al. [35] use the Software Word Usage Model (SWUM) to create a rule-based model that generates natural language descriptions for Java methods. Moreno et al. [25] predefine heuristic rules to select information and generate comments for Java classes by combining the information. These rule-based approaches have been expanded to cover special types of code artifacts such as test cases [48] and code changes [8]. Human templates usually synthesize comments by extracting keywords from the given source code.\n\nIR approaches are widely used in summary generation and usually search comments from similar code snippets. Haiduc et al. [15] apply the Vector Space Model (VSM) and Latent Semantic Indexing (LSI) to generate term-based comments for classes and methods. Their works are replicated and expanded by Eddy et al. [11] which exploit a hierarchical topic model. Wong et al. [42] apply code clone detection techniques to find similar code snippets and use the comments from similar code snippets. The work is similar to their previous work AutoComment [43] which mines human-written descriptions for automatic comment generation from Stack Overflow.\n\nRecently, some studies try giving natural language summaries by deep learning approaches. Iyer et al. [19] present RNN networks with attention to produce summaries that describe C# code snippets and SQL queries. It takes source code as plain text and models the conditional distribution of the summary. Allamanis et al. [4] apply a neural convolutional attentional model to the problem that extremely summarizes the source code snippets into short, namelike summaries. These learning-based approaches mainly learn the latent features from source code, such as semantics, formatting, and etc. The comments are generated according to these learned features. The experimental results of them have proved the effectiveness of deep learning methods on code summarization. In this paper, DeepCom integrates the structure information which is verified important for comments generation.\n\n\nLanguage models for source code\n\nRecently, thanks to the insight of Hindle et al. [17], there is an emerging interest in building language models of source code. These language models vary from n-gram model [1,29], bimodal model [5], and RNNs [12,19]. Hindle et al. [17] first propose to explore N-gram to model the source code and demonstrate that most software is also natural and find regularities in natural code. Some studies build the models to bridge the gap between the programming language and natural language descriptions. Allamanis et al. [1] develop a framework to learn the code conventions of a codebase and the framework exploits N-gram model to name Java identifiers. Allamanis et al. [2] and Raychev et al. [33] suggest names for variables, methods, and classes. Mou et al. [26] present a tree-based convolutional neural networks to model the source code and classify programs. Gu et al. [12] present a classic encoder-decoder model to bridge the gap between the Java API sequences and natural language. Yin and Neubig [47] build a data-driven syntax-based neural network model for generating code from natural language.\n\nLearning from source code is applied to various software engineering tasks, e.g., fault detection [32], code completion [27,29], code clone [38] and code summarization [19]. In this paper, we explore the combination of deep learning methods and source code features to generate code comments. Compared to the previous works, DeepCom explains the code summarization procedure from a machine translation perspective. The experimental results also prove the ability of DeepCom.\n\n\nCONCLUSION\n\nThis paper formulates code summarization task as a machine translation problem which translates source code written in a programming language to comments in natural language. We propose Deep-Com, an attention-based Seq2Seq model, to generate comments for Java methods. DeepCom takes ASTs sequences as input. These ASTs are converted to specially formatted sequences using a new structure-based traversal (SBT) method. SBT can express the structural information and keep the representation lossless at the same time. DeepCom outperforms the state-of-the-art approaches and achieves better results on the machine translation metric. In future work, we plan to improve the effectiveness of our proposed approach by introducing more domain-specific customizations. We also plan to apply our proposed approach to other software engineering tasks that can be mapped to a machine translation problem (e.g., code migration, etc.).\n\nFigure 1 :\n1An illustration of basic RNN and LSTM\n\nFigure 2 :\n2Overall framework of DeepCom.\n\nFigure 3 :\n3Sequence-to-Sequence model.\n\nFigure 4 :\n4An example of sequencing an AST to a sequence by SBT. (For a number, the bold font number after bracket indicates node itself and the number in brackets denotes the tree structure by taking it as the root node.)\n\n\npublic String extractFor(Integer id){ LOG.debug(\"Extracting method with ID:{}\", id); return requests.remove(id); }\n\nFigure 5 :\n5AST of the Java method named extractFor.\n\nFigure 6 :\n6The average BLEU-4 scores of different lengths of code and comment in Java language. (We compare two methods DeepCom with SBT and CODE-NN)\n\n\nAutomatically generated: Returns true if the symbol is empty. Human-written: Is this symbol table empty?7 public boolean contains(int key){ return rank(key) != -1; } Automatically generated: Checks whether the given object is contained within the given set. Human-written: Is the key in this set of integers? 8 public void tag(String inputFileName,String outputFileName, OutputFormat outputFormat){ List<String> sentences=jsc.textFile(inputFileName).collect(); tag(sentences,outputFileName,outputFormat); } Automatically generated: Replaces the message with a given tag Human-written: Tags a text file , each sentence in a line and writes the result to an output file with a desired output format. 9 public void unlisten(String pattern){ UtilListener listener=listeners.get(pattern); if(listener!=null){ listener.destroy(); listeners.remove(pattern); }else{ client.onError(Topic.RECORD,Event.NOT_LISTENING,pattern); } }\n\nTable 1 :\n1Statistics for code snippets in our dataset#Methods \n#All \nTokens \n\n# All \nIdentifiers \n\n# Unique \nTokens \n\n#Unique \nIdentifiers \n\n588,108 \n44,378,497 13,779,297 794,711 \n794,621 \n\n\n\nTable 2 :\n2Statistics for code lengths and comments lengthsCode Lengths \n\nAvg Mode Median <100 \n<150 \n<200 \n99.94 \n16 \n65 \n68.63% 82.06% 89.00% \n\nComments Lengths \n\nAvg Mode Median \n<20 \n<30 \n<50 \n8.86 \n8 \n13 \n75.50% 86.79% 95.45% \n\n\n\nTable 3 :\n3Evaluation results on Java methodsApproaches \nBLEU-4 score (%) \n\nCODE-NN \n25.30 \nSeq2Seq \n34.87 \nAttention-based Seq2Seq \n35.50 \nDeepCom (Pre-order) \n36.01 \nDeepCom (SBT) \n38.17 \n\nsystem to generate summaries for code snippets. It exploits an RNN \nwith attention to generate summaries by integrating the token \nembeddings of source code instead of building language models for \nsource code. We do not use IR approaches as baselines, because the \nresults in CODE-NN has shown that CODE-NN outperforms the IR \nbased approaches. \n\n\nTable 4 :\n4Evaluation results on CODE-NN datasets including C# and SQL programming languages.Language Approaches BLEU-4 score(%) \n\nC# \nCODE-NN \n20.4 \nSeq2Seq \n30.00 \n\nSQL \nCODE-NN \n17.0 \nSeq2Seq \n30.94 \n\nstructural information in source code needs to be taken into ac-\ncount. DeepCom can generate more informative comments than the \nstate-of-the-art method. Compared to the model without AST, the \nBLEU score of DeepCom increases to 38.17% and the BLEU-4 scores \nof about 38% of the instances are greater than 50%. We evaluate two \ntraversal methods SBT and pre-order traversal. DeepCom with SBT \nperforms better than traditional pre-order traversal. This is the case \nbecause SBT better preserves the structure of ASTs. Experimental \nresults indicate that the structural information is important for \ntranslating text in structured languages to unstructured ones. \n\n\n\nTable 5 :\n5Examples of generated comments by DeepCom. These samples are necessarily limited to short methods because of space limitations. AST structure is not shown in the table, because AST is much longer than source code.Automatically generated: If no profile has been configured , set by default the \"dev\" profile. Human-written: If no profile has been configured , set by default the \"dev\" profile.Case ID \nJava method \nComments \n\n1 \n\npublic static byte[] bitmapToByte(Bitmap b){ \nByteArrayOutputStream o = new ByteArrayOutputStream(); \nb.compress(Bitmap.CompressFormat.PNG,100,o); \nreturn o.toByteArray(); \n} \n\nAutomatically generated: convert Bitmap to byte array \nHuman-written: convert Bitmap to byte array \n\n2 \n\nprivate static void addDefaultProfile(SpringApplication app, \nSimpleCommandLinePropertySource source){ \nif(!source.containsProperty(\"spring.profiles.active\") \n&&!System.getenv().containsKey(\"SPRING_PROFILES_ACTIVE\")){ \napp.setAdditionalProfiles(Constants.SPRING_PROFILE_DEVELOPMENT); \n} \n} \n\n3 \n\npublic FactoryConfigurationError(Exception e){ \nsuper(e.toString()); \nthis.exception=e; \n} \n\nAutomatically generated: Create a new UNK with a given Exception base cause \nof the error. \nHuman-written: Create a new FactoryConfigurationError with a given Exception \nbase cause of the error. \n\n4 \n\nprotected void createItemsLayout(){ \nif (mItemsLayout == null){ \nmItemsLayout=new LinearLayout(getContext()); \nmItemsLayout.setOrientation(LinearLayout.VERTICAL); \n} \n} \n\nAutomatically generated: Creates item layouts if any parameters \nHuman-written: Creates item layout if necessary \n\n5 \n\npublic static void sort(Comparable[] a){ \nint n=a.length; \nfor (int i=1; i < n; i++){ \nComparable v=a[i]; \nint lo=0, hi=i; \nwhile (lo < hi) { ... } \n... \n} \nassert isSorted(a); \n} \n\n\nhttps://www.tiobe.com/tiobe-index/ 200\nhttp://www.eclipse.org/jdt/\nhttp://www.oracle.com/technetwork/articles/java/index-137868.html5 Data is available at https://github.com/huxingfree/DeepCom 6 https://www.tensorflow.org/\nhttps://github.com/tensorflow/nmt\n\nLearning natural coding conventions. Miltiadis Allamanis, T Earl, Christian Barr, Charles Bird, Sutton, Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringACMMiltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2014. Learn- ing natural coding conventions. In Proceedings of the 22nd ACM SIGSOFT Interna- tional Symposium on Foundations of Software Engineering. ACM, 281-293.\n\nSuggesting accurate method and class names. Miltiadis Allamanis, T Earl, Christian Barr, Charles Bird, Sutton, Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. the 2015 10th Joint Meeting on Foundations of Software EngineeringACMMiltiadis Allamanis, Earl T Barr, Christian Bird, and Charles Sutton. 2015. Sug- gesting accurate method and class names. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. ACM, 38-49.\n\nMiltiadis Allamanis, T Earl, Premkumar Barr, Charles Devanbu, Sutton, arXiv:1709.06182A Survey of Machine Learning for Big Code and Naturalness. arXiv preprintMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2017. A Survey of Machine Learning for Big Code and Naturalness. arXiv preprint arXiv:1709.06182 (2017).\n\nA convolutional attention network for extreme summarization of source code. Miltiadis Allamanis, Hao Peng, Charles Sutton, International Conference on Machine Learning. Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional at- tention network for extreme summarization of source code. In International Conference on Machine Learning. 2091-2100.\n\nBimodal modelling of source code and natural language. Miltos Allamanis, Daniel Tarlow, Andrew Gordon, Yi Wei, International Conference on Machine Learning. Miltos Allamanis, Daniel Tarlow, Andrew Gordon, and Yi Wei. 2015. Bimodal modelling of source code and natural language. In International Conference on Machine Learning. 2123-2132.\n\nNeural Machine Translation by Jointly Learning to Align and Translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Computer Science. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. Computer Science (2014).\n\nA holistic approach to software quality at work. Manfred Broy, Florian Dei\u00dfenb\u00f6ck, Markus Pizka, Proc. 3rd World Congress for Software Quality (3WCSQ). 3rd World Congress for Software Quality (3WCSQ)Manfred Broy, Florian Dei\u00dfenb\u00f6ck, and Markus Pizka. 2005. A holistic approach to software quality at work. In Proc. 3rd World Congress for Software Quality (3WCSQ).\n\nAutomatically documenting program changes. P L Raymond, Westley R Buse, Weimer, Proceedings of the IEEE/ACM international conference on Automated software engineering. the IEEE/ACM international conference on Automated software engineeringACMRaymond PL Buse and Westley R Weimer. 2010. Automatically documenting program changes. In Proceedings of the IEEE/ACM international conference on Automated software engineering. ACM, 33-42.\n\nLarge scale language modeling in automatic speech recognition. Ciprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, Shankar Kumar, arXiv:1210.8440arXiv preprintCiprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, and Shankar Kumar. 2012. Large scale language modeling in automatic speech recognition. arXiv preprint arXiv:1210.8440 (2012).\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.3555arXiv preprintJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014).\n\nEvaluating source code summarization techniques: Replication and expansion. P Brian, Jeffrey A Eddy, Nicholas A Robinson, Jeffrey C Kraft, Carver, IEEE 21st International Conference on. IEEE. Program Comprehension (ICPC)Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. 2013. Evaluating source code summarization techniques: Replication and expansion. In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on. IEEE, 13-22.\n\nDeep API learning. Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim, Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software EngineeringACMXiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo- sium on Foundations of Software Engineering. ACM, 631-642.\n\nDeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning. Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim, arXiv:1704.07734arXiv preprintXiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2017. DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning. arXiv preprint arXiv:1704.07734 (2017).\n\nSupporting program comprehension with source code summarization. Sonia Haiduc, Jairo Aponte, Andrian Marcus, Proceedings of the 32nd. the 32ndSonia Haiduc, Jairo Aponte, and Andrian Marcus. 2010. Supporting program comprehension with source code summarization. In Proceedings of the 32nd\n\nACM/IEEE International Conference on Software Engineering. ACM2ACM/IEEE International Conference on Software Engineering-Volume 2. ACM, 223- 226.\n\nOn the use of automated text summarization techniques for summarizing source code. Sonia Haiduc, Jairo Aponte, Laura Moreno, Andrian Marcus, Reverse Engineering (WCRE), 2010 17th Working Conference on. IEEESonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. 2010. On the use of automated text summarization techniques for summarizing source code. In Reverse Engineering (WCRE), 2010 17th Working Conference on. IEEE, 35-44.\n\nAre deep neural networks the best choice for modeling source code. J Vincent, Premkumar Hellendoorn, Devanbu, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringACMVincent J Hellendoorn and Premkumar Devanbu. 2017. Are deep neural networks the best choice for modeling source code?. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. ACM, 763-773.\n\nOn the naturalness of software. Abram Hindle, T Earl, Zhendong Barr, Mark Su, Premkumar Gabel, Devanbu, Software Engineering (ICSE), 2012 34th International Conference on. IEEE. Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the naturalness of software. In Software Engineering (ICSE), 2012 34th International Conference on. IEEE, 837-847.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 9Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780.\n\nSummarizing Source Code using a Neural Attention Model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, ACL (1). Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model.. In ACL (1).\n\nAutomatically generating commit messages from diffs using neural machine translation. Siyuan Jiang, Ameer Armaly, Collin Mcmillan, Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. the 32nd IEEE/ACM International Conference on Automated Software EngineeringIEEE PressSiyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat- ing commit messages from diffs using neural machine translation. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. IEEE Press, 135-146.\n\nGoogle's multilingual neural machine translation system: enabling zeroshot translation. Melvin Johnson, Mike Schuster, V Quoc, Maxim Le, Yonghui Krikun, Zhifeng Wu, Nikhil Chen, Fernanda Thorat, Martin Vi\u00e9gas, Greg Wattenberg, Corrado, arXiv:1611.04558arXiv preprintMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. 2016. Google's multilingual neural machine translation system: enabling zero- shot translation. arXiv preprint arXiv:1611.04558 (2016).\n\nOpennmt: Open-source toolkit for neural machine translation. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, Alexander M Rush, arXiv:1701.02810arXiv preprintGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810 (2017).\n\nA Neural Architecture for Generating Natural Language Descriptions from Source Code Changes. Pablo Loyola, Edison Marrese-Taylor, Yutaka Matsuo, arXiv:1704.04856arXiv preprintPablo Loyola, Edison Marrese-Taylor, and Yutaka Matsuo. 2017. A Neural Ar- chitecture for Generating Natural Language Descriptions from Source Code Changes. arXiv preprint arXiv:1704.04856 (2017).\n\nAutomatic documentation generation via source code summarization of method context. W Paul, Collin Mcburney, Mcmillan, Proceedings of the 22nd International Conference on Program Comprehension. the 22nd International Conference on Program ComprehensionACMPaul W McBurney and Collin McMillan. 2014. Automatic documentation gen- eration via source code summarization of method context. In Proceedings of the 22nd International Conference on Program Comprehension. ACM, 279-290.\n\nAutomatic generation of natural language summaries for java classes. Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock, K Vijay-Shanker, IEEE 21st International Conference on. IEEE. Program Comprehension (ICPC)Laura Moreno, Jairo Aponte, Giriprasad Sridhara, Andrian Marcus, Lori Pollock, and K Vijay-Shanker. 2013. Automatic generation of natural language summaries for java classes. In Program Comprehension (ICPC), 2013 IEEE 21st International Conference on. IEEE, 23-32.\n\nConvolutional Neural Networks over Tree Structures for Programming Language Processing. Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin, AAAI. 2Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural Networks over Tree Structures for Programming Language Processing.. In AAAI, Vol. 2. 4.\n\nOn end-to-end program generation from user intention by deep neural networks. Lili Mou, Rui Men, Ge Li, Lu Zhang, Zhi Jin, arXiv:1510.07211arXiv preprintLili Mou, Rui Men, Ge Li, Lu Zhang, and Zhi Jin. 2015. On end-to-end pro- gram generation from user intention by deep neural networks. arXiv preprint arXiv:1510.07211 (2015).\n\nNatural language models for predicting programming comments. Dana Movshovitz, -Attias William W Cohen, Dana Movshovitz-Attias and William W Cohen. 2013. Natural language models for predicting programming comments. (2013).\n\nA statistical semantic language model for source code. Anh Tuan Tung Thanh Nguyen, Nguyen, Anh Hoan, Tien N Nguyen, Nguyen, Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering. the 2013 9th Joint Meeting on Foundations of Software EngineeringACMTung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2013. A statistical semantic language model for source code. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering. ACM, 532-542.\n\nLearning to generate pseudo-code from source code using statistical machine translation (t). Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, 30th IEEE/ACM International Conference on. IEEE. Automated Software Engineering (ASE)Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code from source code using statistical machine translation (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on. IEEE, 574- 584.\n\nBLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 311-318.\n\nOn the naturalness of buggy code. Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, Premkumar Devanbu, Proceedings of the 38th International Conference on Software Engineering. the 38th International Conference on Software EngineeringACMBaishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, and Premkumar Devanbu. 2016. On the naturalness of buggy code. In Proceedings of the 38th International Conference on Software Engineering. ACM, 428-439.\n\nPredicting program properties from big code. Veselin Raychev, Martin Vechev, Andreas Krause, In ACM SIGPLAN Notices. 50ACMVeselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting program properties from big code. In ACM SIGPLAN Notices, Vol. 50. ACM, 111-124.\n\nA neural attention model for abstractive sentence summarization. Sumit Alexander M Rush, Jason Chopra, Weston, arXiv:1509.00685arXiv preprintAlexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685 (2015).\n\nTowards automatically generating summary comments for java methods. Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, Vijay-Shanker, Proceedings of the IEEE/ACM international conference on Automated software engineering. the IEEE/ACM international conference on Automated software engineeringACMGiriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay- Shanker. 2010. Towards automatically generating summary comments for java methods. In Proceedings of the IEEE/ACM international conference on Automated software engineering. ACM, 43-52.\n\nAutomatically detecting and describing high level actions within methods. Giriprasad Sridhara, Lori Pollock, K Vijay-Shanker, Proceedings of the 33rd International Conference on Software Engineering. the 33rd International Conference on Software EngineeringACMGiriprasad Sridhara, Lori Pollock, and K Vijay-Shanker. 2011. Automatically detecting and describing high level actions within methods. In Proceedings of the 33rd International Conference on Software Engineering. ACM, 101-110.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems. 3104- 3112.\n\nA Machine Learning Based Approach for Evaluating Clone Detection Tools for a Generalized and Accurate Precision. Jeffrey Svajlenko, Roy, International Journal of Software Engineering and Knowledge Engineering. 26Jeffrey Svajlenko and Chanchal K Roy. 2016. A Machine Learning Based Approach for Evaluating Clone Detection Tools for a Generalized and Accurate Precision. International Journal of Software Engineering and Knowledge Engineering 26, 09n10 (2016), 1399-1429.\n\nOriol Vinyals, Quoc Le, arXiv:1506.05869A neural conversational model. arXiv preprintOriol Vinyals and Quoc Le. 2015. A neural conversational model. arXiv preprint arXiv:1506.05869 (2015).\n\nAutomatically learning semantic features for defect prediction. Song Wang, Taiyue Liu, Lin Tan, Proceedings of the 38th International Conference on Software Engineering. the 38th International Conference on Software EngineeringACMSong Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic features for defect prediction. In Proceedings of the 38th International Conference on Software Engineering. ACM, 297-308.\n\nDeep learning code fragments for code clone detection. Martin White, Michele Tufano, Christopher Vendome, Denys Poshyvanyk, Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. the 31st IEEE/ACM International Conference on Automated Software EngineeringACMMartin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. ACM, 87-98.\n\nClocom: Mining existing source code for automatic comment generation. Edmund Wong, Taiyue Liu, Lin Tan, Software Analysis, Evolution and Reengineering (SANER). IEEE 22nd International Conference on. IEEEEdmund Wong, Taiyue Liu, and Lin Tan. 2015. Clocom: Mining existing source code for automatic comment generation. In Software Analysis, Evolution and Reengineering (SANER), 2015 IEEE 22nd International Conference on. IEEE, 380- 389.\n\nAutocomment: Mining question and answer sites for automatic comment generation. Edmund Wong, Jinqiu Yang, Lin Tan, Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering. the 28th IEEE/ACM International Conference on Automated Software EngineeringIEEE PressEdmund Wong, Jinqiu Yang, and Lin Tan. 2013. Autocomment: Mining question and answer sites for automatic comment generation. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering. IEEE Press, 562-567.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016).\n\nMeasuring program comprehension: A large-scale field study with professionals. Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, Shanping Li, IEEE Transactions on Software Engineering. Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shan- ping Li. 2017. Measuring program comprehension: A large-scale field study with professionals. IEEE Transactions on Software Engineering (2017).\n\nJun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, Xiaoming Li, arXiv:1512.01337Neural generative question answering. arXiv preprintJun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, and Xiaoming Li. 2015. Neural generative question answering. arXiv preprint arXiv:1512.01337 (2015).\n\nA Syntactic Neural Model for General-Purpose Code Generation. Pengcheng Yin, Graham Neubig, arXiv:1704.01696arXiv preprintPengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General- Purpose Code Generation. arXiv preprint arXiv:1704.01696 (2017).\n\nAutomated documentation inference to explain failed tests. Sai Zhang, Cheng Zhang, Michael D Ernst, Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering. the 2011 26th IEEE/ACM International Conference on Automated Software EngineeringIEEE Computer SocietySai Zhang, Cheng Zhang, and Michael D Ernst. 2011. Automated documentation inference to explain failed tests. In Proceedings of the 2011 26th IEEE/ACM Inter- national Conference on Automated Software Engineering. IEEE Computer Society, 63-72.\n", "annotations": {"author": "[{\"end\":253,\"start\":86},{\"end\":419,\"start\":254},{\"end\":511,\"start\":420},{\"end\":616,\"start\":512},{\"end\":802,\"start\":617},{\"end\":970,\"start\":803},{\"end\":1136,\"start\":971},{\"end\":1208,\"start\":1137},{\"end\":1293,\"start\":1209},{\"end\":1461,\"start\":1294}]", "publisher": "[{\"end\":33,\"start\":30},{\"end\":1626,\"start\":1623}]", "author_last_name": "[{\"end\":93,\"start\":91},{\"end\":259,\"start\":257},{\"end\":427,\"start\":424},{\"end\":520,\"start\":518},{\"end\":624,\"start\":621},{\"end\":810,\"start\":808},{\"end\":976,\"start\":974},{\"end\":1144,\"start\":1141},{\"end\":1217,\"start\":1215},{\"end\":1301,\"start\":1298}]", "author_first_name": "[{\"end\":90,\"start\":86},{\"end\":256,\"start\":254},{\"end\":423,\"start\":420},{\"end\":517,\"start\":512},{\"end\":620,\"start\":617},{\"end\":807,\"start\":803},{\"end\":973,\"start\":971},{\"end\":1140,\"start\":1137},{\"end\":1214,\"start\":1209},{\"end\":1297,\"start\":1294}]", "author_affiliation": "[{\"end\":193,\"start\":95},{\"end\":252,\"start\":195},{\"end\":359,\"start\":261},{\"end\":418,\"start\":361},{\"end\":510,\"start\":449},{\"end\":615,\"start\":542},{\"end\":742,\"start\":644},{\"end\":801,\"start\":744},{\"end\":910,\"start\":812},{\"end\":969,\"start\":912},{\"end\":1076,\"start\":978},{\"end\":1135,\"start\":1078},{\"end\":1207,\"start\":1146},{\"end\":1292,\"start\":1219},{\"end\":1401,\"start\":1303},{\"end\":1460,\"start\":1403}]", "title": "[{\"end\":29,\"start\":1},{\"end\":1490,\"start\":1462}]", "venue": "[{\"end\":1565,\"start\":1492}]", "abstract": "[{\"end\":3221,\"start\":1974}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3360,\"start\":3356},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3574,\"start\":3570},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3925,\"start\":3921},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3928,\"start\":3925},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3945,\"start\":3941},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4086,\"start\":4082},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4121,\"start\":4117},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4124,\"start\":4121},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4144,\"start\":4140},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4341,\"start\":4337},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4344,\"start\":4341},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4975,\"start\":4971},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5190,\"start\":5186},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5193,\"start\":5190},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5196,\"start\":5193},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5199,\"start\":5196},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5478,\"start\":5474},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6514,\"start\":6511},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6517,\"start\":6514},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7411,\"start\":7408},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8653,\"start\":8649},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10263,\"start\":10259},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11567,\"start\":11564},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11591,\"start\":11588},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11620,\"start\":11616},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12142,\"start\":12138},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13605,\"start\":13602},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13608,\"start\":13605},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13926,\"start\":13922},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14521,\"start\":14517},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14524,\"start\":14521},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14564,\"start\":14560},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15103,\"start\":15100},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15444,\"start\":15440},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15482,\"start\":15479},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15830,\"start\":15826},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15961,\"start\":15957},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17339,\"start\":17335},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17364,\"start\":17360},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17386,\"start\":17382},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18556,\"start\":18553},{\"end\":21386,\"start\":21384},{\"end\":21443,\"start\":21441},{\"end\":21467,\"start\":21465},{\"end\":21514,\"start\":21512},{\"end\":21522,\"start\":21520},{\"end\":21574,\"start\":21572},{\"end\":21596,\"start\":21594},{\"end\":21619,\"start\":21617},{\"end\":21713,\"start\":21710},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22997,\"start\":22993},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23461,\"start\":23458},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26125,\"start\":26124},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26485,\"start\":26481},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27723,\"start\":27719},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28086,\"start\":28085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28152,\"start\":28151},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28685,\"start\":28681},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28789,\"start\":28785},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28841,\"start\":28837},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28844,\"start\":28841},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29186,\"start\":29182},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29269,\"start\":29265},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30498,\"start\":30494},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32369,\"start\":32365},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32372,\"start\":32369},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41372,\"start\":41368},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41375,\"start\":41372},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41619,\"start\":41615},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":42450,\"start\":42446},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":42453,\"start\":42450},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":42456,\"start\":42453},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42465,\"start\":42461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42468,\"start\":42465},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":42471,\"start\":42468},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42504,\"start\":42501},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":42507,\"start\":42504},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":42510,\"start\":42507},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":42652,\"start\":42648},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":42804,\"start\":42800},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":43033,\"start\":43029},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43054,\"start\":43051},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43278,\"start\":43274},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43465,\"start\":43461},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":43524,\"start\":43520},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":43701,\"start\":43697},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43902,\"start\":43898},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":44119,\"start\":44116},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44764,\"start\":44760},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":44888,\"start\":44885},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":44891,\"start\":44888},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":44910,\"start\":44907},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44925,\"start\":44921},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":44928,\"start\":44925},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":44948,\"start\":44944},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45232,\"start\":45229},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45383,\"start\":45380},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":45407,\"start\":45403},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":45474,\"start\":45470},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45588,\"start\":45584},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":45719,\"start\":45715},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45920,\"start\":45916},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":45942,\"start\":45938},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":45945,\"start\":45942},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":45962,\"start\":45958},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":45990,\"start\":45986},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":52592,\"start\":52591}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":47280,\"start\":47230},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47323,\"start\":47281},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47364,\"start\":47324},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47589,\"start\":47365},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47706,\"start\":47590},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47760,\"start\":47707},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47912,\"start\":47761},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48834,\"start\":47913},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":49028,\"start\":48835},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49263,\"start\":49029},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49803,\"start\":49264},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50672,\"start\":49804},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":52458,\"start\":50673}]", "paragraph": "[{\"end\":3851,\"start\":3237},{\"end\":4849,\"start\":3853},{\"end\":6061,\"start\":4851},{\"end\":7204,\"start\":6063},{\"end\":8737,\"start\":7206},{\"end\":9503,\"start\":8739},{\"end\":9849,\"start\":9505},{\"end\":10264,\"start\":9851},{\"end\":10318,\"start\":10266},{\"end\":10696,\"start\":10320},{\"end\":11148,\"start\":10698},{\"end\":11820,\"start\":11183},{\"end\":12456,\"start\":12027},{\"end\":12941,\"start\":12487},{\"end\":13159,\"start\":12998},{\"end\":13609,\"start\":13161},{\"end\":14525,\"start\":13629},{\"end\":15104,\"start\":14556},{\"end\":15538,\"start\":15106},{\"end\":16183,\"start\":15560},{\"end\":16999,\"start\":16185},{\"end\":17144,\"start\":17001},{\"end\":17581,\"start\":17175},{\"end\":17812,\"start\":17594},{\"end\":18220,\"start\":17840},{\"end\":18557,\"start\":18235},{\"end\":18700,\"start\":18559},{\"end\":18782,\"start\":18726},{\"end\":18827,\"start\":18824},{\"end\":18963,\"start\":18855},{\"end\":19179,\"start\":18976},{\"end\":19403,\"start\":19237},{\"end\":19727,\"start\":19449},{\"end\":20865,\"start\":19771},{\"end\":21296,\"start\":20867},{\"end\":22911,\"start\":21356},{\"end\":25181,\"start\":22940},{\"end\":26938,\"start\":25698},{\"end\":27621,\"start\":26940},{\"end\":28191,\"start\":27642},{\"end\":28585,\"start\":28193},{\"end\":29462,\"start\":28616},{\"end\":29564,\"start\":29464},{\"end\":29791,\"start\":29604},{\"end\":30069,\"start\":29835},{\"end\":30263,\"start\":30081},{\"end\":30420,\"start\":30265},{\"end\":31194,\"start\":30462},{\"end\":32373,\"start\":31207},{\"end\":32900,\"start\":32375},{\"end\":33028,\"start\":32902},{\"end\":34111,\"start\":33101},{\"end\":34492,\"start\":34149},{\"end\":34845,\"start\":34522},{\"end\":35264,\"start\":34876},{\"end\":35791,\"start\":35266},{\"end\":36589,\"start\":35835},{\"end\":36991,\"start\":36591},{\"end\":37314,\"start\":36993},{\"end\":38037,\"start\":37355},{\"end\":38506,\"start\":38062},{\"end\":38750,\"start\":38508},{\"end\":39280,\"start\":38752},{\"end\":40168,\"start\":39282},{\"end\":40545,\"start\":40219},{\"end\":41092,\"start\":40547},{\"end\":41483,\"start\":41116},{\"end\":41830,\"start\":41518},{\"end\":42198,\"start\":41832},{\"end\":42511,\"start\":42238},{\"end\":43150,\"start\":42513},{\"end\":43794,\"start\":43152},{\"end\":44675,\"start\":43796},{\"end\":45816,\"start\":44711},{\"end\":46292,\"start\":45818},{\"end\":47229,\"start\":46307}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11970,\"start\":11821},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12026,\"start\":11970},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12997,\"start\":12942},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17839,\"start\":17813},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18725,\"start\":18701},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18823,\"start\":18783},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18854,\"start\":18828},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19236,\"start\":19180},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19448,\"start\":19404},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21355,\"start\":21297},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25678,\"start\":25182},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29603,\"start\":29565},{\"attributes\":{\"id\":\"formula_12\"},\"end\":29834,\"start\":29792}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":8449,\"start\":8442},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26610,\"start\":26603},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26622,\"start\":26615},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31384,\"start\":31376},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32539,\"start\":32532},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34288,\"start\":34281}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3235,\"start\":3223},{\"attributes\":{\"n\":\"2\"},\"end\":11181,\"start\":11151},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":12485,\"start\":12459},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":13627,\"start\":13612},{\"attributes\":{\"n\":\"2.2\"},\"end\":14554,\"start\":14528},{\"attributes\":{\"n\":\"3\"},\"end\":15558,\"start\":15541},{\"attributes\":{\"n\":\"3.1\"},\"end\":17173,\"start\":17147},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":17592,\"start\":17584},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":18233,\"start\":18223},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":18974,\"start\":18966},{\"attributes\":{\"n\":\"3.2\"},\"end\":19769,\"start\":19730},{\"attributes\":{\"n\":\"3.3\"},\"end\":22938,\"start\":22914},{\"attributes\":{\"n\":\"4\"},\"end\":25696,\"start\":25680},{\"attributes\":{\"n\":\"4.1\"},\"end\":27640,\"start\":27624},{\"attributes\":{\"n\":\"4.2\"},\"end\":28614,\"start\":28588},{\"attributes\":{\"n\":\"5\"},\"end\":30079,\"start\":30072},{\"attributes\":{\"n\":\"5.1\"},\"end\":30448,\"start\":30423},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":30460,\"start\":30451},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":31205,\"start\":31197},{\"attributes\":{\"n\":\"5.2\"},\"end\":33099,\"start\":33031},{\"attributes\":{\"n\":\"6\"},\"end\":34124,\"start\":34114},{\"attributes\":{\"n\":\"6.1\"},\"end\":34147,\"start\":34127},{\"attributes\":{\"n\":\"6.1.1\"},\"end\":34520,\"start\":34495},{\"attributes\":{\"n\":\"6.1.2\"},\"end\":34874,\"start\":34848},{\"attributes\":{\"n\":\"6.1.4\"},\"end\":35833,\"start\":35794},{\"attributes\":{\"n\":\"6.1.6\"},\"end\":37353,\"start\":37317},{\"attributes\":{\"n\":\"6.2\"},\"end\":38060,\"start\":38040},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":40217,\"start\":40171},{\"attributes\":{\"n\":\"6.3\"},\"end\":41114,\"start\":41095},{\"end\":41516,\"start\":41486},{\"attributes\":{\"n\":\"7\"},\"end\":42236,\"start\":42201},{\"attributes\":{\"n\":\"7.2\"},\"end\":44709,\"start\":44678},{\"attributes\":{\"n\":\"8\"},\"end\":46305,\"start\":46295},{\"end\":47241,\"start\":47231},{\"end\":47292,\"start\":47282},{\"end\":47335,\"start\":47325},{\"end\":47376,\"start\":47366},{\"end\":47718,\"start\":47708},{\"end\":47772,\"start\":47762},{\"end\":48845,\"start\":48836},{\"end\":49039,\"start\":49030},{\"end\":49274,\"start\":49265},{\"end\":49814,\"start\":49805},{\"end\":50683,\"start\":50674}]", "table": "[{\"end\":49028,\"start\":48890},{\"end\":49263,\"start\":49089},{\"end\":49803,\"start\":49310},{\"end\":50672,\"start\":49898},{\"end\":52458,\"start\":51077}]", "figure_caption": "[{\"end\":47280,\"start\":47243},{\"end\":47323,\"start\":47294},{\"end\":47364,\"start\":47337},{\"end\":47589,\"start\":47378},{\"end\":47706,\"start\":47592},{\"end\":47760,\"start\":47720},{\"end\":47912,\"start\":47774},{\"end\":48834,\"start\":47915},{\"end\":48890,\"start\":48847},{\"end\":49089,\"start\":49041},{\"end\":49310,\"start\":49276},{\"end\":49898,\"start\":49816},{\"end\":51077,\"start\":50685}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12455,\"start\":12447},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12824,\"start\":12816},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14435,\"start\":14425},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16244,\"start\":16236},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17541,\"start\":17533},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20768,\"start\":20760},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21039,\"start\":21031},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21913,\"start\":21905},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21940,\"start\":21932},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24825,\"start\":24817},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33204,\"start\":33196},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33336,\"start\":33328},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33818,\"start\":33810}]", "bib_author_first_name": "[{\"end\":52763,\"start\":52754},{\"end\":52776,\"start\":52775},{\"end\":52792,\"start\":52783},{\"end\":52806,\"start\":52799},{\"end\":53295,\"start\":53286},{\"end\":53308,\"start\":53307},{\"end\":53324,\"start\":53315},{\"end\":53338,\"start\":53331},{\"end\":53735,\"start\":53726},{\"end\":53748,\"start\":53747},{\"end\":53764,\"start\":53755},{\"end\":53778,\"start\":53771},{\"end\":54149,\"start\":54140},{\"end\":54164,\"start\":54161},{\"end\":54178,\"start\":54171},{\"end\":54490,\"start\":54484},{\"end\":54508,\"start\":54502},{\"end\":54523,\"start\":54517},{\"end\":54534,\"start\":54532},{\"end\":54846,\"start\":54839},{\"end\":54866,\"start\":54857},{\"end\":54878,\"start\":54872},{\"end\":55116,\"start\":55109},{\"end\":55130,\"start\":55123},{\"end\":55149,\"start\":55143},{\"end\":55469,\"start\":55468},{\"end\":55471,\"start\":55470},{\"end\":55488,\"start\":55481},{\"end\":55490,\"start\":55489},{\"end\":55928,\"start\":55921},{\"end\":55940,\"start\":55937},{\"end\":55953,\"start\":55948},{\"end\":55971,\"start\":55964},{\"end\":55987,\"start\":55980},{\"end\":56297,\"start\":56289},{\"end\":56311,\"start\":56305},{\"end\":56331,\"start\":56322},{\"end\":56343,\"start\":56337},{\"end\":56649,\"start\":56648},{\"end\":56664,\"start\":56657},{\"end\":56666,\"start\":56665},{\"end\":56681,\"start\":56673},{\"end\":56683,\"start\":56682},{\"end\":56703,\"start\":56694},{\"end\":57066,\"start\":57058},{\"end\":57077,\"start\":57071},{\"end\":57092,\"start\":57085},{\"end\":57107,\"start\":57100},{\"end\":57595,\"start\":57587},{\"end\":57606,\"start\":57600},{\"end\":57621,\"start\":57614},{\"end\":57636,\"start\":57629},{\"end\":57917,\"start\":57912},{\"end\":57931,\"start\":57926},{\"end\":57947,\"start\":57940},{\"end\":58371,\"start\":58366},{\"end\":58385,\"start\":58380},{\"end\":58399,\"start\":58394},{\"end\":58415,\"start\":58408},{\"end\":58786,\"start\":58785},{\"end\":58805,\"start\":58796},{\"end\":59237,\"start\":59232},{\"end\":59247,\"start\":59246},{\"end\":59262,\"start\":59254},{\"end\":59273,\"start\":59269},{\"end\":59287,\"start\":59278},{\"end\":59606,\"start\":59602},{\"end\":59625,\"start\":59619},{\"end\":59840,\"start\":59830},{\"end\":59854,\"start\":59847},{\"end\":59869,\"start\":59864},{\"end\":59882,\"start\":59878},{\"end\":60143,\"start\":60137},{\"end\":60156,\"start\":60151},{\"end\":60171,\"start\":60165},{\"end\":60716,\"start\":60710},{\"end\":60730,\"start\":60726},{\"end\":60742,\"start\":60741},{\"end\":60754,\"start\":60749},{\"end\":60766,\"start\":60759},{\"end\":60782,\"start\":60775},{\"end\":60793,\"start\":60787},{\"end\":60808,\"start\":60800},{\"end\":60823,\"start\":60817},{\"end\":60836,\"start\":60832},{\"end\":61249,\"start\":61240},{\"end\":61261,\"start\":61257},{\"end\":61274,\"start\":61267},{\"end\":61285,\"start\":61281},{\"end\":61308,\"start\":61297},{\"end\":61630,\"start\":61625},{\"end\":61645,\"start\":61639},{\"end\":61668,\"start\":61662},{\"end\":61990,\"start\":61989},{\"end\":62003,\"start\":61997},{\"end\":62456,\"start\":62451},{\"end\":62470,\"start\":62465},{\"end\":62489,\"start\":62479},{\"end\":62507,\"start\":62500},{\"end\":62520,\"start\":62516},{\"end\":62531,\"start\":62530},{\"end\":62978,\"start\":62974},{\"end\":62986,\"start\":62984},{\"end\":62993,\"start\":62991},{\"end\":63004,\"start\":63001},{\"end\":63014,\"start\":63011},{\"end\":63275,\"start\":63271},{\"end\":63284,\"start\":63281},{\"end\":63292,\"start\":63290},{\"end\":63299,\"start\":63297},{\"end\":63310,\"start\":63307},{\"end\":63587,\"start\":63583},{\"end\":63607,\"start\":63600},{\"end\":63803,\"start\":63800},{\"end\":63808,\"start\":63804},{\"end\":63839,\"start\":63836},{\"end\":63852,\"start\":63846},{\"end\":64351,\"start\":64345},{\"end\":64365,\"start\":64357},{\"end\":64380,\"start\":64374},{\"end\":64396,\"start\":64389},{\"end\":64411,\"start\":64403},{\"end\":64425,\"start\":64419},{\"end\":64439,\"start\":64432},{\"end\":64920,\"start\":64913},{\"end\":64936,\"start\":64931},{\"end\":64949,\"start\":64945},{\"end\":64964,\"start\":64956},{\"end\":65479,\"start\":65470},{\"end\":65492,\"start\":65485},{\"end\":65512,\"start\":65506},{\"end\":65530,\"start\":65522},{\"end\":65542,\"start\":65535},{\"end\":65563,\"start\":65554},{\"end\":65998,\"start\":65991},{\"end\":66014,\"start\":66008},{\"end\":66030,\"start\":66023},{\"end\":66289,\"start\":66284},{\"end\":66313,\"start\":66308},{\"end\":66600,\"start\":66590},{\"end\":66616,\"start\":66611},{\"end\":66628,\"start\":66623},{\"end\":66644,\"start\":66640},{\"end\":67179,\"start\":67169},{\"end\":67194,\"start\":67190},{\"end\":67205,\"start\":67204},{\"end\":67639,\"start\":67635},{\"end\":67656,\"start\":67651},{\"end\":67672,\"start\":67666},{\"end\":68019,\"start\":68012},{\"end\":68375,\"start\":68370},{\"end\":68389,\"start\":68385},{\"end\":68628,\"start\":68624},{\"end\":68641,\"start\":68635},{\"end\":68650,\"start\":68647},{\"end\":69049,\"start\":69043},{\"end\":69064,\"start\":69057},{\"end\":69084,\"start\":69073},{\"end\":69099,\"start\":69094},{\"end\":69603,\"start\":69597},{\"end\":69616,\"start\":69610},{\"end\":69625,\"start\":69622},{\"end\":70050,\"start\":70044},{\"end\":70063,\"start\":70057},{\"end\":70073,\"start\":70070},{\"end\":70608,\"start\":70601},{\"end\":70617,\"start\":70613},{\"end\":70635,\"start\":70628},{\"end\":70643,\"start\":70642},{\"end\":70658,\"start\":70650},{\"end\":70671,\"start\":70663},{\"end\":70686,\"start\":70681},{\"end\":70701,\"start\":70697},{\"end\":70713,\"start\":70710},{\"end\":70724,\"start\":70719},{\"end\":71144,\"start\":71141},{\"end\":71158,\"start\":71150},{\"end\":71169,\"start\":71164},{\"end\":71183,\"start\":71174},{\"end\":71195,\"start\":71190},{\"end\":71197,\"start\":71196},{\"end\":71214,\"start\":71206},{\"end\":71485,\"start\":71482},{\"end\":71494,\"start\":71491},{\"end\":71511,\"start\":71502},{\"end\":71522,\"start\":71516},{\"end\":71534,\"start\":71530},{\"end\":71547,\"start\":71539},{\"end\":71850,\"start\":71841},{\"end\":71862,\"start\":71856},{\"end\":72106,\"start\":72103},{\"end\":72119,\"start\":72114},{\"end\":72136,\"start\":72127}]", "bib_author_last_name": "[{\"end\":52773,\"start\":52764},{\"end\":52781,\"start\":52777},{\"end\":52797,\"start\":52793},{\"end\":52811,\"start\":52807},{\"end\":52819,\"start\":52813},{\"end\":53305,\"start\":53296},{\"end\":53313,\"start\":53309},{\"end\":53329,\"start\":53325},{\"end\":53343,\"start\":53339},{\"end\":53351,\"start\":53345},{\"end\":53745,\"start\":53736},{\"end\":53753,\"start\":53749},{\"end\":53769,\"start\":53765},{\"end\":53786,\"start\":53779},{\"end\":53794,\"start\":53788},{\"end\":54159,\"start\":54150},{\"end\":54169,\"start\":54165},{\"end\":54185,\"start\":54179},{\"end\":54500,\"start\":54491},{\"end\":54515,\"start\":54509},{\"end\":54530,\"start\":54524},{\"end\":54538,\"start\":54535},{\"end\":54855,\"start\":54847},{\"end\":54870,\"start\":54867},{\"end\":54885,\"start\":54879},{\"end\":55121,\"start\":55117},{\"end\":55141,\"start\":55131},{\"end\":55155,\"start\":55150},{\"end\":55479,\"start\":55472},{\"end\":55495,\"start\":55491},{\"end\":55503,\"start\":55497},{\"end\":55935,\"start\":55929},{\"end\":55946,\"start\":55941},{\"end\":55962,\"start\":55954},{\"end\":55978,\"start\":55972},{\"end\":55993,\"start\":55988},{\"end\":56303,\"start\":56298},{\"end\":56320,\"start\":56312},{\"end\":56335,\"start\":56332},{\"end\":56350,\"start\":56344},{\"end\":56655,\"start\":56650},{\"end\":56671,\"start\":56667},{\"end\":56692,\"start\":56684},{\"end\":56709,\"start\":56704},{\"end\":56717,\"start\":56711},{\"end\":57069,\"start\":57067},{\"end\":57083,\"start\":57078},{\"end\":57098,\"start\":57093},{\"end\":57111,\"start\":57108},{\"end\":57598,\"start\":57596},{\"end\":57612,\"start\":57607},{\"end\":57627,\"start\":57622},{\"end\":57640,\"start\":57637},{\"end\":57924,\"start\":57918},{\"end\":57938,\"start\":57932},{\"end\":57954,\"start\":57948},{\"end\":58378,\"start\":58372},{\"end\":58392,\"start\":58386},{\"end\":58406,\"start\":58400},{\"end\":58422,\"start\":58416},{\"end\":58794,\"start\":58787},{\"end\":58817,\"start\":58806},{\"end\":58826,\"start\":58819},{\"end\":59244,\"start\":59238},{\"end\":59252,\"start\":59248},{\"end\":59267,\"start\":59263},{\"end\":59276,\"start\":59274},{\"end\":59293,\"start\":59288},{\"end\":59302,\"start\":59295},{\"end\":59617,\"start\":59607},{\"end\":59637,\"start\":59626},{\"end\":59845,\"start\":59841},{\"end\":59862,\"start\":59855},{\"end\":59876,\"start\":59870},{\"end\":59894,\"start\":59883},{\"end\":60149,\"start\":60144},{\"end\":60163,\"start\":60157},{\"end\":60180,\"start\":60172},{\"end\":60724,\"start\":60717},{\"end\":60739,\"start\":60731},{\"end\":60747,\"start\":60743},{\"end\":60757,\"start\":60755},{\"end\":60773,\"start\":60767},{\"end\":60785,\"start\":60783},{\"end\":60798,\"start\":60794},{\"end\":60815,\"start\":60809},{\"end\":60830,\"start\":60824},{\"end\":60847,\"start\":60837},{\"end\":60856,\"start\":60849},{\"end\":61255,\"start\":61250},{\"end\":61265,\"start\":61262},{\"end\":61279,\"start\":61275},{\"end\":61295,\"start\":61286},{\"end\":61313,\"start\":61309},{\"end\":61637,\"start\":61631},{\"end\":61660,\"start\":61646},{\"end\":61675,\"start\":61669},{\"end\":61995,\"start\":61991},{\"end\":62012,\"start\":62004},{\"end\":62022,\"start\":62014},{\"end\":62463,\"start\":62457},{\"end\":62477,\"start\":62471},{\"end\":62498,\"start\":62490},{\"end\":62514,\"start\":62508},{\"end\":62528,\"start\":62521},{\"end\":62545,\"start\":62532},{\"end\":62982,\"start\":62979},{\"end\":62989,\"start\":62987},{\"end\":62999,\"start\":62994},{\"end\":63009,\"start\":63005},{\"end\":63018,\"start\":63015},{\"end\":63279,\"start\":63276},{\"end\":63288,\"start\":63285},{\"end\":63295,\"start\":63293},{\"end\":63305,\"start\":63300},{\"end\":63314,\"start\":63311},{\"end\":63598,\"start\":63588},{\"end\":63623,\"start\":63608},{\"end\":63826,\"start\":63809},{\"end\":63834,\"start\":63828},{\"end\":63844,\"start\":63840},{\"end\":63859,\"start\":63853},{\"end\":63867,\"start\":63861},{\"end\":64355,\"start\":64352},{\"end\":64372,\"start\":64366},{\"end\":64387,\"start\":64381},{\"end\":64401,\"start\":64397},{\"end\":64417,\"start\":64412},{\"end\":64430,\"start\":64426},{\"end\":64448,\"start\":64440},{\"end\":64929,\"start\":64921},{\"end\":64943,\"start\":64937},{\"end\":64954,\"start\":64950},{\"end\":64968,\"start\":64965},{\"end\":65483,\"start\":65480},{\"end\":65504,\"start\":65493},{\"end\":65520,\"start\":65513},{\"end\":65533,\"start\":65531},{\"end\":65552,\"start\":65543},{\"end\":65571,\"start\":65564},{\"end\":66006,\"start\":65999},{\"end\":66021,\"start\":66015},{\"end\":66037,\"start\":66031},{\"end\":66306,\"start\":66290},{\"end\":66320,\"start\":66314},{\"end\":66328,\"start\":66322},{\"end\":66609,\"start\":66601},{\"end\":66621,\"start\":66617},{\"end\":66638,\"start\":66629},{\"end\":66652,\"start\":66645},{\"end\":66667,\"start\":66654},{\"end\":67188,\"start\":67180},{\"end\":67202,\"start\":67195},{\"end\":67219,\"start\":67206},{\"end\":67649,\"start\":67640},{\"end\":67664,\"start\":67657},{\"end\":67675,\"start\":67673},{\"end\":68029,\"start\":68020},{\"end\":68034,\"start\":68031},{\"end\":68383,\"start\":68376},{\"end\":68392,\"start\":68390},{\"end\":68633,\"start\":68629},{\"end\":68645,\"start\":68642},{\"end\":68654,\"start\":68651},{\"end\":69055,\"start\":69050},{\"end\":69071,\"start\":69065},{\"end\":69092,\"start\":69085},{\"end\":69110,\"start\":69100},{\"end\":69608,\"start\":69604},{\"end\":69620,\"start\":69617},{\"end\":69629,\"start\":69626},{\"end\":70055,\"start\":70051},{\"end\":70068,\"start\":70064},{\"end\":70077,\"start\":70074},{\"end\":70611,\"start\":70609},{\"end\":70626,\"start\":70618},{\"end\":70640,\"start\":70636},{\"end\":70648,\"start\":70644},{\"end\":70661,\"start\":70659},{\"end\":70679,\"start\":70672},{\"end\":70695,\"start\":70687},{\"end\":70708,\"start\":70702},{\"end\":70717,\"start\":70714},{\"end\":70728,\"start\":70725},{\"end\":70738,\"start\":70730},{\"end\":71148,\"start\":71145},{\"end\":71162,\"start\":71159},{\"end\":71172,\"start\":71170},{\"end\":71188,\"start\":71184},{\"end\":71204,\"start\":71198},{\"end\":71217,\"start\":71215},{\"end\":71489,\"start\":71486},{\"end\":71500,\"start\":71495},{\"end\":71514,\"start\":71512},{\"end\":71528,\"start\":71523},{\"end\":71537,\"start\":71535},{\"end\":71550,\"start\":71548},{\"end\":71854,\"start\":71851},{\"end\":71869,\"start\":71863},{\"end\":72112,\"start\":72107},{\"end\":72125,\"start\":72120},{\"end\":72142,\"start\":72137}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2437629},\"end\":53240,\"start\":52717},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9279336},\"end\":53724,\"start\":53242},{\"attributes\":{\"doi\":\"arXiv:1709.06182\",\"id\":\"b2\"},\"end\":54062,\"start\":53726},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2723946},\"end\":54427,\"start\":54064},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2706277},\"end\":54766,\"start\":54429},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11212020},\"end\":55058,\"start\":54768},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14066046},\"end\":55423,\"start\":55060},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9792514},\"end\":55856,\"start\":55425},{\"attributes\":{\"doi\":\"arXiv:1210.8440\",\"id\":\"b8\"},\"end\":56209,\"start\":55858},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b9\"},\"end\":56570,\"start\":56211},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12442030},\"end\":57037,\"start\":56572},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11540100},\"end\":57516,\"start\":57039},{\"attributes\":{\"doi\":\"arXiv:1704.07734\",\"id\":\"b12\"},\"end\":57845,\"start\":57518},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10747140},\"end\":58134,\"start\":57847},{\"attributes\":{\"id\":\"b14\"},\"end\":58281,\"start\":58136},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7843537},\"end\":58716,\"start\":58283},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":21164835},\"end\":59198,\"start\":58718},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2846066},\"end\":59576,\"start\":59200},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1915014},\"end\":59772,\"start\":59578},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8820379},\"end\":60049,\"start\":59774},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9237290},\"end\":60620,\"start\":60051},{\"attributes\":{\"doi\":\"arXiv:1611.04558\",\"id\":\"b21\"},\"end\":61177,\"start\":60622},{\"attributes\":{\"doi\":\"arXiv:1701.02810\",\"id\":\"b22\"},\"end\":61530,\"start\":61179},{\"attributes\":{\"doi\":\"arXiv:1704.04856\",\"id\":\"b23\"},\"end\":61903,\"start\":61532},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8649673},\"end\":62380,\"start\":61905},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1129667},\"end\":62884,\"start\":62382},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1914494},\"end\":63191,\"start\":62886},{\"attributes\":{\"doi\":\"arXiv:1510.07211\",\"id\":\"b27\"},\"end\":63520,\"start\":63193},{\"attributes\":{\"id\":\"b28\"},\"end\":63743,\"start\":63522},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":17169661},\"end\":64250,\"start\":63745},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15979705},\"end\":64847,\"start\":64252},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11080756},\"end\":65434,\"start\":64849},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":215751909},\"end\":65944,\"start\":65436},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14571254},\"end\":66217,\"start\":65946},{\"attributes\":{\"doi\":\"arXiv:1509.00685\",\"id\":\"b34\"},\"end\":66520,\"start\":66219},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9790585},\"end\":67093,\"start\":66522},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":11106352},\"end\":67581,\"start\":67095},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":7961699},\"end\":67897,\"start\":67583},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":40472428},\"end\":68368,\"start\":67899},{\"attributes\":{\"doi\":\"arXiv:1506.05869\",\"id\":\"b39\"},\"end\":68558,\"start\":68370},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10769502},\"end\":68986,\"start\":68560},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14867364},\"end\":69525,\"start\":68988},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":16690920},\"end\":69962,\"start\":69527},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":10506832},\"end\":70499,\"start\":69964},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b44\"},\"end\":71060,\"start\":70501},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":52988209},\"end\":71480,\"start\":71062},{\"attributes\":{\"doi\":\"arXiv:1512.01337\",\"id\":\"b46\"},\"end\":71777,\"start\":71482},{\"attributes\":{\"doi\":\"arXiv:1704.01696\",\"id\":\"b47\"},\"end\":72042,\"start\":71779},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14846118},\"end\":72586,\"start\":72044}]", "bib_title": "[{\"end\":52752,\"start\":52717},{\"end\":53284,\"start\":53242},{\"end\":54138,\"start\":54064},{\"end\":54482,\"start\":54429},{\"end\":54837,\"start\":54768},{\"end\":55107,\"start\":55060},{\"end\":55466,\"start\":55425},{\"end\":56646,\"start\":56572},{\"end\":57056,\"start\":57039},{\"end\":57910,\"start\":57847},{\"end\":58364,\"start\":58283},{\"end\":58783,\"start\":58718},{\"end\":59230,\"start\":59200},{\"end\":59600,\"start\":59578},{\"end\":59828,\"start\":59774},{\"end\":60135,\"start\":60051},{\"end\":61987,\"start\":61905},{\"end\":62449,\"start\":62382},{\"end\":62972,\"start\":62886},{\"end\":63798,\"start\":63745},{\"end\":64343,\"start\":64252},{\"end\":64911,\"start\":64849},{\"end\":65468,\"start\":65436},{\"end\":65989,\"start\":65946},{\"end\":66588,\"start\":66522},{\"end\":67167,\"start\":67095},{\"end\":67633,\"start\":67583},{\"end\":68010,\"start\":67899},{\"end\":68622,\"start\":68560},{\"end\":69041,\"start\":68988},{\"end\":69595,\"start\":69527},{\"end\":70042,\"start\":69964},{\"end\":71139,\"start\":71062},{\"end\":72101,\"start\":72044}]", "bib_author": "[{\"end\":52775,\"start\":52754},{\"end\":52783,\"start\":52775},{\"end\":52799,\"start\":52783},{\"end\":52813,\"start\":52799},{\"end\":52821,\"start\":52813},{\"end\":53307,\"start\":53286},{\"end\":53315,\"start\":53307},{\"end\":53331,\"start\":53315},{\"end\":53345,\"start\":53331},{\"end\":53353,\"start\":53345},{\"end\":53747,\"start\":53726},{\"end\":53755,\"start\":53747},{\"end\":53771,\"start\":53755},{\"end\":53788,\"start\":53771},{\"end\":53796,\"start\":53788},{\"end\":54161,\"start\":54140},{\"end\":54171,\"start\":54161},{\"end\":54187,\"start\":54171},{\"end\":54502,\"start\":54484},{\"end\":54517,\"start\":54502},{\"end\":54532,\"start\":54517},{\"end\":54540,\"start\":54532},{\"end\":54857,\"start\":54839},{\"end\":54872,\"start\":54857},{\"end\":54887,\"start\":54872},{\"end\":55123,\"start\":55109},{\"end\":55143,\"start\":55123},{\"end\":55157,\"start\":55143},{\"end\":55481,\"start\":55468},{\"end\":55497,\"start\":55481},{\"end\":55505,\"start\":55497},{\"end\":55937,\"start\":55921},{\"end\":55948,\"start\":55937},{\"end\":55964,\"start\":55948},{\"end\":55980,\"start\":55964},{\"end\":55995,\"start\":55980},{\"end\":56305,\"start\":56289},{\"end\":56322,\"start\":56305},{\"end\":56337,\"start\":56322},{\"end\":56352,\"start\":56337},{\"end\":56657,\"start\":56648},{\"end\":56673,\"start\":56657},{\"end\":56694,\"start\":56673},{\"end\":56711,\"start\":56694},{\"end\":56719,\"start\":56711},{\"end\":57071,\"start\":57058},{\"end\":57085,\"start\":57071},{\"end\":57100,\"start\":57085},{\"end\":57113,\"start\":57100},{\"end\":57600,\"start\":57587},{\"end\":57614,\"start\":57600},{\"end\":57629,\"start\":57614},{\"end\":57642,\"start\":57629},{\"end\":57926,\"start\":57912},{\"end\":57940,\"start\":57926},{\"end\":57956,\"start\":57940},{\"end\":58380,\"start\":58366},{\"end\":58394,\"start\":58380},{\"end\":58408,\"start\":58394},{\"end\":58424,\"start\":58408},{\"end\":58796,\"start\":58785},{\"end\":58819,\"start\":58796},{\"end\":58828,\"start\":58819},{\"end\":59246,\"start\":59232},{\"end\":59254,\"start\":59246},{\"end\":59269,\"start\":59254},{\"end\":59278,\"start\":59269},{\"end\":59295,\"start\":59278},{\"end\":59304,\"start\":59295},{\"end\":59619,\"start\":59602},{\"end\":59639,\"start\":59619},{\"end\":59847,\"start\":59830},{\"end\":59864,\"start\":59847},{\"end\":59878,\"start\":59864},{\"end\":59896,\"start\":59878},{\"end\":60151,\"start\":60137},{\"end\":60165,\"start\":60151},{\"end\":60182,\"start\":60165},{\"end\":60726,\"start\":60710},{\"end\":60741,\"start\":60726},{\"end\":60749,\"start\":60741},{\"end\":60759,\"start\":60749},{\"end\":60775,\"start\":60759},{\"end\":60787,\"start\":60775},{\"end\":60800,\"start\":60787},{\"end\":60817,\"start\":60800},{\"end\":60832,\"start\":60817},{\"end\":60849,\"start\":60832},{\"end\":60858,\"start\":60849},{\"end\":61257,\"start\":61240},{\"end\":61267,\"start\":61257},{\"end\":61281,\"start\":61267},{\"end\":61297,\"start\":61281},{\"end\":61315,\"start\":61297},{\"end\":61639,\"start\":61625},{\"end\":61662,\"start\":61639},{\"end\":61677,\"start\":61662},{\"end\":61997,\"start\":61989},{\"end\":62014,\"start\":61997},{\"end\":62024,\"start\":62014},{\"end\":62465,\"start\":62451},{\"end\":62479,\"start\":62465},{\"end\":62500,\"start\":62479},{\"end\":62516,\"start\":62500},{\"end\":62530,\"start\":62516},{\"end\":62547,\"start\":62530},{\"end\":62984,\"start\":62974},{\"end\":62991,\"start\":62984},{\"end\":63001,\"start\":62991},{\"end\":63011,\"start\":63001},{\"end\":63020,\"start\":63011},{\"end\":63281,\"start\":63271},{\"end\":63290,\"start\":63281},{\"end\":63297,\"start\":63290},{\"end\":63307,\"start\":63297},{\"end\":63316,\"start\":63307},{\"end\":63600,\"start\":63583},{\"end\":63625,\"start\":63600},{\"end\":63828,\"start\":63800},{\"end\":63836,\"start\":63828},{\"end\":63846,\"start\":63836},{\"end\":63861,\"start\":63846},{\"end\":63869,\"start\":63861},{\"end\":64357,\"start\":64345},{\"end\":64374,\"start\":64357},{\"end\":64389,\"start\":64374},{\"end\":64403,\"start\":64389},{\"end\":64419,\"start\":64403},{\"end\":64432,\"start\":64419},{\"end\":64450,\"start\":64432},{\"end\":64931,\"start\":64913},{\"end\":64945,\"start\":64931},{\"end\":64956,\"start\":64945},{\"end\":64970,\"start\":64956},{\"end\":65485,\"start\":65470},{\"end\":65506,\"start\":65485},{\"end\":65522,\"start\":65506},{\"end\":65535,\"start\":65522},{\"end\":65554,\"start\":65535},{\"end\":65573,\"start\":65554},{\"end\":66008,\"start\":65991},{\"end\":66023,\"start\":66008},{\"end\":66039,\"start\":66023},{\"end\":66308,\"start\":66284},{\"end\":66322,\"start\":66308},{\"end\":66330,\"start\":66322},{\"end\":66611,\"start\":66590},{\"end\":66623,\"start\":66611},{\"end\":66640,\"start\":66623},{\"end\":66654,\"start\":66640},{\"end\":66669,\"start\":66654},{\"end\":67190,\"start\":67169},{\"end\":67204,\"start\":67190},{\"end\":67221,\"start\":67204},{\"end\":67651,\"start\":67635},{\"end\":67666,\"start\":67651},{\"end\":67677,\"start\":67666},{\"end\":68031,\"start\":68012},{\"end\":68036,\"start\":68031},{\"end\":68385,\"start\":68370},{\"end\":68394,\"start\":68385},{\"end\":68635,\"start\":68624},{\"end\":68647,\"start\":68635},{\"end\":68656,\"start\":68647},{\"end\":69057,\"start\":69043},{\"end\":69073,\"start\":69057},{\"end\":69094,\"start\":69073},{\"end\":69112,\"start\":69094},{\"end\":69610,\"start\":69597},{\"end\":69622,\"start\":69610},{\"end\":69631,\"start\":69622},{\"end\":70057,\"start\":70044},{\"end\":70070,\"start\":70057},{\"end\":70079,\"start\":70070},{\"end\":70613,\"start\":70601},{\"end\":70628,\"start\":70613},{\"end\":70642,\"start\":70628},{\"end\":70650,\"start\":70642},{\"end\":70663,\"start\":70650},{\"end\":70681,\"start\":70663},{\"end\":70697,\"start\":70681},{\"end\":70710,\"start\":70697},{\"end\":70719,\"start\":70710},{\"end\":70730,\"start\":70719},{\"end\":70740,\"start\":70730},{\"end\":71150,\"start\":71141},{\"end\":71164,\"start\":71150},{\"end\":71174,\"start\":71164},{\"end\":71190,\"start\":71174},{\"end\":71206,\"start\":71190},{\"end\":71219,\"start\":71206},{\"end\":71491,\"start\":71482},{\"end\":71502,\"start\":71491},{\"end\":71516,\"start\":71502},{\"end\":71530,\"start\":71516},{\"end\":71539,\"start\":71530},{\"end\":71552,\"start\":71539},{\"end\":71856,\"start\":71841},{\"end\":71871,\"start\":71856},{\"end\":72114,\"start\":72103},{\"end\":72127,\"start\":72114},{\"end\":72144,\"start\":72127}]", "bib_venue": "[{\"end\":53004,\"start\":52921},{\"end\":53502,\"start\":53436},{\"end\":55259,\"start\":55212},{\"end\":55664,\"start\":55593},{\"end\":57306,\"start\":57218},{\"end\":57989,\"start\":57981},{\"end\":58977,\"start\":58911},{\"end\":60351,\"start\":60275},{\"end\":62157,\"start\":62099},{\"end\":64016,\"start\":63951},{\"end\":65123,\"start\":65055},{\"end\":65704,\"start\":65647},{\"end\":66828,\"start\":66757},{\"end\":67352,\"start\":67295},{\"end\":68787,\"start\":68730},{\"end\":69281,\"start\":69205},{\"end\":70248,\"start\":70172},{\"end\":72323,\"start\":72242},{\"end\":52919,\"start\":52821},{\"end\":53434,\"start\":53353},{\"end\":53869,\"start\":53812},{\"end\":54231,\"start\":54187},{\"end\":54584,\"start\":54540},{\"end\":54903,\"start\":54887},{\"end\":55210,\"start\":55157},{\"end\":55591,\"start\":55505},{\"end\":55919,\"start\":55858},{\"end\":56287,\"start\":56211},{\"end\":56762,\"start\":56719},{\"end\":57216,\"start\":57113},{\"end\":57585,\"start\":57518},{\"end\":57979,\"start\":57956},{\"end\":58193,\"start\":58136},{\"end\":58483,\"start\":58424},{\"end\":58909,\"start\":58828},{\"end\":59376,\"start\":59304},{\"end\":59657,\"start\":59639},{\"end\":59903,\"start\":59896},{\"end\":60273,\"start\":60182},{\"end\":60708,\"start\":60622},{\"end\":61238,\"start\":61179},{\"end\":61623,\"start\":61532},{\"end\":62097,\"start\":62024},{\"end\":62590,\"start\":62547},{\"end\":63024,\"start\":63020},{\"end\":63269,\"start\":63193},{\"end\":63581,\"start\":63522},{\"end\":63949,\"start\":63869},{\"end\":64497,\"start\":64450},{\"end\":65053,\"start\":64970},{\"end\":65645,\"start\":65573},{\"end\":66061,\"start\":66039},{\"end\":66282,\"start\":66219},{\"end\":66755,\"start\":66669},{\"end\":67293,\"start\":67221},{\"end\":67726,\"start\":67677},{\"end\":68107,\"start\":68036},{\"end\":68439,\"start\":68410},{\"end\":68728,\"start\":68656},{\"end\":69203,\"start\":69112},{\"end\":69685,\"start\":69631},{\"end\":70170,\"start\":70079},{\"end\":70599,\"start\":70501},{\"end\":71260,\"start\":71219},{\"end\":71604,\"start\":71568},{\"end\":71839,\"start\":71779},{\"end\":72240,\"start\":72144}]"}}}, "year": 2023, "month": 12, "day": 17}