{"id": 226222119, "updated": "2023-10-06 09:53:09.48", "metadata": {"title": "A Critical Assessment of State-of-the-Art in Entity Alignment", "authors": "[{\"first\":\"Max\",\"last\":\"Berrendorf\",\"middle\":[]},{\"first\":\"Ludwig\",\"last\":\"Wacker\",\"middle\":[]},{\"first\":\"Evgeniy\",\"last\":\"Faerman\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Lecture Notes in Computer Science", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In this work, we perform an extensive investigation of two state-of-the-art (SotA) methods for the task of Entity Alignment in Knowledge Graphs. Therefore, we first carefully examine the benchmarking process and identify several shortcomings, which make the results reported in the original works not always comparable. Furthermore, we suspect that it is a common practice in the community to make the hyperparameter optimization directly on a test set, reducing the informative value of reported performance. Thus, we select a representative sample of benchmarking datasets and describe their properties. We also examine different initializations for entity representations since they are a decisive factor for model performance. Furthermore, we use a shared train/validation/test split for a fair evaluation setting in which we evaluate all methods on all datasets. In our evaluation, we make several interesting findings. While we observe that most of the time SotA approaches perform better than baselines, they have difficulties when the dataset contains noise, which is the case in most real-life applications. Moreover, we find out in our ablation study that often different features of SotA methods are crucial for good performance than previously assumed. The code is available at https://github.com/mberr/ea-sota-comparison.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.16314", "mag": "3148407586", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ecir/BerrendorfWF21", "doi": "10.1007/978-3-030-72240-1_2"}}, "content": {"source": {"pdf_hash": "4a343b3c5a9b235b0f4b746610da72ff0e44e5ca", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.16314v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2010.16314", "status": "GREEN"}}, "grobid": {"id": "5a8ba23c5aa236cf9dd327576519c321c23079df", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4a343b3c5a9b235b0f4b746610da72ff0e44e5ca.txt", "contents": "\nA Critical Assessment of State-of-the-Art in Entity Alignment\n17 Mar 2021\n\nMax Berrendorf \nLudwig-Maximilians-Universit\u00e4t M\u00fcnchen\nMunichGermany\n\n] \nLudwig-Maximilians-Universit\u00e4t M\u00fcnchen\nMunichGermany\n\nLudwig Wacker l.wacker@campus.lmu.de \nLudwig-Maximilians-Universit\u00e4t M\u00fcnchen\nMunichGermany\n\nEvgeniy Faerman faerman@dbs.ifi.lmu.de \nLudwig-Maximilians-Universit\u00e4t M\u00fcnchen\nMunichGermany\n\nA Critical Assessment of State-of-the-Art in Entity Alignment\n17 Mar 2021Knowledge Graph \u00b7 Entity Alignment \u00b7 Word Embeddings\nIn this work, we perform an extensive investigation of two state-of-the-art (SotA) methods for the task of Entity Alignment in Knowledge Graphs. Therefore, we first carefully examine the benchmarking process and identify several shortcomings, making the results reported in the original works not always comparable. Furthermore, we suspect that it is a common practice in the community to make the hyperparameter optimization directly on a test set, reducing the informative value of reported performance. Thus, we select a representative sample of benchmarking datasets and describe their properties. We also examine different initializations for entity representations since they are a decisive factor for model performance. Furthermore, we use a shared train/validation/test split for an appropriate evaluation setting to evaluate all methods on all datasets. In our evaluation, we make several interesting findings. While we observe that most of the time SotA approaches perform better than baselines, they have difficulties when the dataset contains noise, which is the case in most real-life applications. Moreover, in our ablation study, we find out that often different features of SotA method are crucial for good performance than previously assumed. The code is available at https://github.com/mberr/ea-sota-comparison.\n\nIntroduction\n\nThe quality of information retrieval crucially depends on the accessible storage of information. Knowledge Graphs (KGs) often serve as such data structure [6]. Moreover, to satisfy diverse information needs, a combination of multiple data sources is often inevitable. Entity Alignment (EA) [2] is the discipline of aligning entities from different KGs. Once aligned, these entities facilitate information transfer between knowledge bases, or even fusing multiple KGs to a single knowledge base.\n\nIn this work, our goal is to analyze a SotA approach for the task of EA and identify which factors are essential for its performance. Although papers often use the same dataset in the evaluation and report the same evaluation metrics, the selection of SotA is not a trivial task: as we found out in our analysis, the usage of different types of external information for the initialization or train/test splits of different sizes 1 makes the results in different works incomparable. Therefore, while still guided by the reported evaluation metrics, we identified these common factors among strongly performing methods in multiple works:\n\n-They are based on Graph Neural Networks (GNNs). GNNs build the basis of the most recent works [16,9,12,21,22,4,23,25,10,14,7,17,20,18,19]. -They utilize entity names in the model. Supported by recent advances in word embeddings, these attributes provide distinctive features. -They consider different types of relations existing in KGs. Most GNNs ignore different relationship types and aggregate them in the preprocessing step.\n\nGiven these criteria, we selected Relation-aware Dual-Graph Convolutional Network (RDGCN) [17], as it also has demonstrated impressive performance in recent benchmarking studies [15,24]. Additionally, we include the recently published Deep Graph Matching Consensus (DGMC) [7] method in our analysis for two reasons: the studies mentioned above did not include it, and the authors reported surprisingly good performance, considering that this method does not make use of relation type information. We start our study by reviewing the used datasets and discussing the initializations based on entity names. Although both methods utilize entity names, the actual usage differs. For comparison, we thus evaluate both methods on all datasets with all available initializations. We also report the zero-shot performance, i.e., when only using initial representations alone, as well as a simple GNN model baseline. Furthermore, we address the problem of hyperparameter optimization. Related works often do not discuss how they chose hyperparameters and, e.g., rarely report validation splits. So far, this problem was not addressed in the community. In the recent comprehensive survey [15], the authors use cross-validation for the estimation of the test performance. The models are either evaluated with hyperparameters recommended for other datasets or selected by not reported procedure. Also, in the published code of the investigated approaches, we could not find any trace of train-validation splits, raising questions about reproducibility and fairness of their comparisons. We thus create a shared split with a test, train, and validation part and extensively tune the model's hyperparameters for each of the dataset/initialization combinations to ensure that they are sufficiently optimized. Finally, we provide an ablation study for many of the parameters of a SotA approach (RDGCN), giving insight into the individual components' contributions to the final performance. Table 1 provides a summary of a representative sample of datasets used for benchmarking of EA approaches. In the following, we first discuss each dataset's properties and, in the second part, the initialization of entity name attributes. \n\n\nDatasets & Initialization\n\n\nDatasets\n\nDBP15k The DBP15k dataset is the most popular dataset for the evaluation of EA approaches. It has three subsets, all of which base upon DBpedia. Each subset comprises a pair of graphs from different languages. As noted by [2], there exist multiple variations of the dataset, sharing the same entity alignment but differing in the number of exclusive entities in each graph. The alignments in the datasets are always 1:1 alignments, and due to the construction method for the datasets, exclusive entities do not have relations between them, but only to shared entities. Exclusive entities complicate the matching process, and in real-life applications, they are not easy to identify. Therefore, we believe that this dataset describes a realistic use-case only to a certain extent. We found another different variant of DBP15k as part of the PyTorch Geometric repository 2 , having a different set of aligned entities. This is likely due to extraction of alignments from data provided by [20] via Google Drive 3 as described in their GitHub repository. 4 As a result, the evaluation results published in [7] are not directly comparable to other published results. In our experiments, we use the (smaller) JAPE variant with approximately 19-20k entities in each graph since it is the predominantly used variant.\n\nOpenEA The OpenEA datasets published by [15] comprise graph pairs from DBPedia, YAGO, and Wikidata obtained by iterative degree-based sampling to match the degree distribution between the source KG and the extracted subset. The alignments are exclusively 1:1 matchings, and there are no exclusive entities, i.e., every entity occurs in both graphs. We believe that this is a relatively unrealistic scenario. In our experiments, we use all graph pairs with 15k entities (15K) in the dense variant (V2), i.e., en-de-15k-v2, en-fr-15k-v2, d-y-15k-v2, d-w-15k-v2.\n\nWK3l15k The Wk3l datasets are multi-lingual KG pairs extracted from Wikipedia. As in [2], we extract additional entity alignments from the triple alignments. The graphs contain additional exclusive entities, and there are m:n matchings. We only use the 15k variants, where each graph has approximately 15k entities. There are two graph pairs, en-de and en-fr. Moreover, the alignments in the dataset are relatively noisy: for example, en-de contains besides valid alignments such as (\"trieste\", \"triest\"), or (\"frederick i, holy roman emperor\", \"friedrich i. (hrr)\"), also ambiguous ones such as (\"1\", \"1. fc saarbr\u00fccken\"), (\"1\", \"1. fc schweinfurt 05\"), and errors such as (\"1\", \"157\"), and (\"101\", \"100\"). While the noise aggravates alignment, it also reflects a realistic setting.\n\n\nLabel-Based Initializations\n\nPrepared translations (DBP15k) For DBP15k, we investigate label-based initializations based on prepared translations to English from [17] and [7] (which, in turn, originate from [20]). Afterwards, they use Glove [11] embeddings to obtain an entity representation. While [17] only provides the final entity representation vectors without further describing the aggregation, [7] splits the label into words (by white-space) and uses the sum over the words' embeddings as entity representation. [17] additionally normalizes the norm of the representations to unit length.\n\nPrepared RDGCN Embeddings (OpenEA) OpenEA [15] benchmarks a large variety of contemporary entity alignment methods in a unified setting, also including RDGCN [17]. Since the graphs DBPedia and YAGO collect data from similar sources, the labels are usually equal. For those graph pairs, the authors propose to delete the labels. However, RDGCN requires a label based initialization.\n\nThus, the authors obtain labels via attribute triples of a pre-defined set of \"nameattributes\" 5 : skos:prefLabel, http://dbpedia.org/ontology/birthName for DBPedia-YAGO, and http://www.wikidata.org/entity/P373, http://www.wikidata.org/entity/P1476 for DBPedia-Wikidata. However, when investigating the published code, we noticed that if the label is not found via attribute, the last part of the entity URI is used instead. For DBPedia/YAGO, this effectively leaks ground truth since they share the same label. For DBPedia/Wikidata, this results in useless labels for the Wikidata side since their labels are the Wikidata IDs, e.g., Q3391163. Table 2 summarizes the frequency of both cases. For d-w, DPBedia entities always use the ground truth label. For 49% of the Wikidata entities, useless labels are used for initialization. For d-y, YAGO entity representations are always initialized via an attribute triple. For DBPedia, in 81% of all cases, the ground truth label is used. We store these initial entity representations produced by the OpenEA codebase into a file and refer in the following to them as Sun initialization (since they are taken from the implementation of [15]).\n\nMulti-lingual BERT (WK3l15k) Since we did not find related work with entity embedding initialization from labels on WK3l15k, we generated those using a pre-trained multi-lingual BERT model [5], BERT-Base, Multilingual Cased 6 . Following [5], we use the sum of the last four layers as token representation since it has comparable performance to the concatenation at a quarter of its size. To summarize the token representations of a single entity label, we explore sum, mean, and max aggregation as hyperparameters.\n\n\nMethods\n\nWe evaluate two SotA EA methods, RDGCN [17] which we reimplemented and DGMC [7] for which we used the original method implementation with adapted evaluation. In the following, we revisit their architectures and highlight differences between the architecture described in the paper and what we found in the published code.\n\nSimilarly to all GNN-based approaches, both models employ a Siamese architecture. Therefore, the same model with the same weights is applied to both graphs yielding representations of entities from both KGs. Given these entity representations, the EA approaches compute an affinity matrix that describes the similarity of entity representations from both graphs. Since the main difference between methods is the GNN model in the Siamese architecture, for brevity we only describe how it is applied on a single KG G = (E, R, T ).\n\n\nRelation-aware Dual-Graph Convolutional Network (RDGCN)\n\nArchitecture The RDGCN [17] model comprises two parts performing messagepassing processes applied sequentially. The message passing process performed by the first part can be seen as relation-aware. The model tries to learn the importance of relations and weights the messages from the entities connected by these relations correspondingly. The message passing performed by the second component utilizes a simple adjacency matrix indicating the existence of any relations between entities, which we call standard message passing. Both components employ a form of skip connections: (weighted) residual connections [8] in the first part and highway layers [13] in the second part.\n\nRelation-Aware Message Passing The entity embeddings from the first component are computed by several interaction rounds comprising four steps\nX c = RC(X e ), X c \u2208 R |R|\u00d72d\n(1)\nX r = DA(X r , X c ), X r \u2208 R |R|\u00d72d (2) X e = P A(X e , X r ) (3) X e = X 0 e + \u03b2 i \u00b7 X e(4)\nThe first step, in (1), obtains a relation context (RC) X c from the entity representations. For relation r \u2208 R, we extract its relation context as a concatenation of the mean entity representations for the head and the tail entities. By denoting the set of head and tail entities for relation r with H r and T r , we can thus express its computation as (X c ) i = 1 /|Hi| j\u2208Hi (X e ) j 1 /|Ti| j\u2208Ti (X e ) j where denotes the concatenation operation. An entity occurring multiple times as the head is weighted equally to an entity occurring only once.\n\nThe second step, in (2), is the dual graph attention (DA). The attention scores on the dual graph \u03b1 D ij are computed by dot product attention with leaky ReLU activation:\n\u03b1 D ij = J ij \u00b7 LeakyReLU (W L (X c ) i + W R (X c ) j ). Notice that W L (X c ) i + W R (X c ) j = (W L W R ) T ((X c ) i (X c ) j ),\nwhere denotes the concatenation operation. In the published code, we further found a weight sharing mechanism for W L and W R implemented, decomposing the projection weight matrices as\nW L = W \u2032 L W C and W R = W \u2032 R W C with W \u2032 L , W \u2032 R \u2208 R 1\u00d7h , W C \u2208 R h\u00d72d\nbeing trainable parameters, and W C shared between both projections. J ij denotes a fixed triple-based relation similarity score computed as the sum of the Jaccard similarities of the head and tail entity set for relation r i and r j : J ij := |Hi\u2229Hj | /|Hi\u222aHj| + |Ti\u2229Tj | /|Ti\u222aTj|. The softmax is then computed only over those relations, where J ij > 0, i.e., pairs sharing at least one head or tail entity. In the implementation, this is implemented as dense attention with masking, i.e. setting \u03b1 D ij = \u2212\u221e (or a very small value) for J ij = 0. While this increases the required memory consumption to O(|R| 2 ), the number of relations is usually small compared to the number of entities, cf. Table 1, and thus this poses no serious computational problem. With\u03b1 D ij denoting the softmax output, the new relation representation finally is (X r ) i = ReLU j\u03b1 D ij (X r ) j . In the third step, in (3), the entity representations are updated. To this end, a relation-specific scalar score is computed as \u03b1 r i = LeakyReLU (WX r + b) with trainable parameters W and b. Based upon the relation-specific scores, an attention score between two entities e i , e j with at least one relation between them is given as \u03b1 P ij = r\u2208Tij \u03b1 r i . These scores are normalized with a sparse softmax over all {j | \u2203r \u2208 R : (e i , r, e j ) \u2208 T }:\u03b1 P ij = softmax j \u2032 (\u03b1 P ij \u2032 ) j . The final output of the primal attention is (X e ) j = ReLU ( i\u03b1 ij (X e ) j ).\n\nThe fourth step, in (4), applies a skip connection from the initial representations to the current entity representation. The weight \u03b2 i is pre-defined (\u03b2 1 = 0.1, \u03b2 2 = 0.3) and not trained.\n\nStandard Message Passing The second part of the RDGCN consists of a sequence of GCN layers with highway layers. Each layer computes\nX \u2032 e = ReLU (AX e W) (5) \u03b2 = \u03c3(W g X e + b g ) (6) X e = \u03b2 \u00b7 X \u2032 e + (1 \u2212 \u03b2) \u00b7 X e(7)\nA \u2208 R |E L |\u00d7|E L | denotes the adjacency matrix of the primal graph. It is constructed by first creating an undirected, unweighted adjacency matrix where there is a connection between e i , e j \u2208 E L if there exists at least one triple (e i , r, e j ) \u2208 T L for some relation r \u2208 R L . Next, self-loops (e, e) are added for every entity e \u2208 E L . Finally, the matrix is normalized by setting A = D \u22121/2 AD \u22121/2 with D denoting the diagonal matrix of node degrees. When investigating the published code, we further found out that the weight matrix W is constrained to be a diagonal matrix and initialized as an identity matrix.\n\nTraining Let x L i denote the final entity representation for e L i \u2208 E L and anologously x R j for e R j \u2208 E R . RDGCN is trained with a margin-based loss formulation. It adopts a hard negative mining strategy, i.e., the set of negative examples for one pair is the top k most similar entities of one of the entities according to the similarity measure used for scoring. The negative l 1 distance is used as similarity, the margin is 1, k = 10, and the negative examples are updated every 10 epochs.\n\n\nDeep Graph Matching Consensus (DGMC)\n\nDGMC [7] also comprises two parts, which we name enrichment and correspondence refinement. The enrichment part is a sequence of GNN layers enriching the entity representations with information from their neighborhood. Each layer computes \u03c6(X) = ReLU (norm(A)XW 1 +norm(A T )XW 2 +XW 3 ), where A \u2208 R |E L |\u00d7|E L | denotes the symmetrically normalized adjacency matrix (as for second part of RDGCN), norm the row-wise normalization operation, X \u2208 R E L \u00d7din the layer's input, and W 1 , W 2 , W 3 \u2208 R din\u00d7dout trainable parameters of the layer. An optional batch normalization and dropout follow this layer. For the enrichment phase's final output, all individual layers' outputs are concatenated before a learned final linear projection layer reduces the dimension to d out .\n\nThe second phase, the correspondence refinement, first calculates the k = 10 most likely matches in the other graph for each entity as a sparse correspondence matrix S \u2208 R |E L |\u00d7|E R | , normalized using softmax. Next, it generates random vectors for each entity R \u2208 R |E L |\u00d7d rnd and sends these vectors to the probable matches via the softmax normalized sparse correspondence matrix, S T R \u2208 R |E R |\u00d7d rnd . A GNN layer \u03c8 as in phase one distributes these vectors in the neighborhood of the nodes: Y R = \u03c8(S T R). A two-layer MLP predicts an update for the correspondence matrix, given the difference between the representations Y L and Y R . This procedure is repeated for a fixed number of refinement steps L = 10.\n\n\nExperiments\n\nExperimental Setup For the general evaluation setting and description of metrics, we refer to [3]. Here, we primarily use Hits@1 (H@1), which measures the correct entity's relative frequency of being ranked in the first position. When investigating the published code of both, RDGCN [17] 7 and DGMC [7] 8 , we did not find any code for tuning the parameters, nor a train-validation split. Also, the papers themselves do not mention a train-validation split. Thus, it is unclear how they choose the hyperparameters without a test-leakage by directly optimizing the test set's performance. We thus decided to create a shared test-train-validation split used by all our experiments to enable a fair comparison. Since DGMC already uses PyTorch, we could use their published code and extend it with HPO code. RDGCN was re-implemented in PyTorch in our codebase. We use the official train-test split for all datasets, which reserves 70% of the alignments for testing. We split the remaining part into 80% train alignments and 20% validation alignments.\n\nWe continued by tuning numerous model parameters (cf. Table 3) of all models on each of the datasets in Table 1 and each of the available initializations described in Section 2.2 to obtain sufficiently well-tuned configurations. We used random search due to its higher sample efficiency than grid search [1]. We additionally evaluate a baseline, which uses the GNN variant from DGMC without the neighborhood consensus refinement, coined GCN-Align* due to its close correspondence to [16], and also evaluate the zero-shot performance of the initial node features.\n\nFor each tested configuration, we perform early stopping on validation H@1, i.e., select the epoch according to the best validation H@1. Across all tested configurations for a model-dataset-initialization combination, we then choose the best configuration according to validation H@1 and report the test performance in Table 4. We do not report performance for training on train+validation with the final configuration due to space restrictions. We decided to report performance when trained only on the train set to ensure that other works have performance numbers for comparison when tuning their own models. Table 3. Investigated hyperparameters for all methods. * denotes that these parameters share the same value range but were tuned independently.  Table 4 presents the overall results. We can observe several points.\n\n\nResults\n\nZero-Shot Performance Generally, there is an impressive Zero-Shot performance, ranging from 39.15% for OpenEA d-w to 83.85% WK3l15k en-de. Thus, even in the weakest setting, approximately 40% of the entities can be aligned solely from their label, without any sophisticated method. Consequently, this highlights that comparison against methods not using this information is unfair. For DBP15k, we can compare the initialization from Wu et al. [17], used, e.g., by RDGCN to the performance of the initialization by Xu et al. [7], used, e.g., by DGMC. We observe that Wu's initialization is 7-9% points stronger than Xu's initialization. For OpenEA d-w we obtain 39.15% zero-shot performance, despite the original labels of the w side being meaningless identifiers. This is only due to using attribute triples with a pre-defined set of \"name\" attributes, cf. Table 2.\n\nModel Performance When comparing the performance of both analyzed models, we can observe that they have a clear advantage over both baselines in two of three datasets. However, we cannot identify a single winner among them. Although the performance of DGMC dropped compared to the results reported originally 9 , it still leads by about 3-4 points on almost all DBP15k subsets. Therefore, it confirms our observation that a smaller test set automatically leads to better results. Furthermore, we can see that different initialization with entity name also affects model performance, which especially applies to the ja-en subset for DGMC or fr-en for GCN-Align*. RDGCN has a clear advantage on the OpenEA subsets extracted from DBPedia with a margin of between 10 and 13 points on both subsets. Note that we significantly improved results of RDGCN on the OpenEA dataset through our extensive hyperparameter search compared to the original evaluation [15]. Interestingly, as can be seen in the next section, the main reason is not the exploiting of information about different relations. The WK3L15k dataset constitutes an interesting exception. The performance of the DGMC method, which is supposed to be robust against noise due to its correspondence refinement, is not better than the zero-shot results. While RDGCN and GCN-Align* can improve the results, the improvement by 1-2 points does not look very convincing. From these results, we conclude that there exists no silver bullet for the task of EA, and the method itself is still a hyperparameter. At the same time, we see that the most realistic dataset poses a real challenge for SotA methods.\n\n\nAblation: RDGCN\n\nWe additionally present the results of an ablation study for some model parameters of RDGCN on the OpenEA datasets in Table 5. For each presented parameter and each possible value, we fix this one parameter and select the best configuration among all configurations with the chosen parameter setting according to validation H@1. The cell then shows the validation and test performance of this configuration. We highlight the best setting on the respective graph pair in bold font. Note that the test performance numbers also coincide with the performance reported in Table 4 for OpenEA. We make the following interesting observations: for all but one graph pair, always normalizing the entity representations before passing them into the layers is beneficial. For d-y, where this is not the case, the difference in performance is small. For the number of GCN layers, we observe an increase in performance from 0 to 2 layers, and on some datasets (d-w, d-y) even beyond. Thus, aggregating the entities' neighbor-hood seems beneficial, highlighting the importance of the graph structure. For the number of interaction layers, which perform relation-aware message passing, we observe that for two of the four subsets (d-y, en-de) the best configuration does not use any interaction layer. However, the difference is small. None of the best configurations uses trainable node embeddings. The negative l 1 similarity is superior on all datasets, with most of the others being close to it. Using the dot product seems to be sub-optimal, maybe due to its unbound value range. Regarding hard negative mining, there is no clear tendency, but considering the hard negatives' expensive calculation (all-to-all kNN), its use might not be worthwhile. Another observation is that sometimes there is a huge gap between the test performance for the best configuration according to validation performance and the best configuration according to test performance. For instance, if we had selected the hyperparameters according to test performance for en-de, we had obtained 93.53 H@1, while choosing them according to validation performance results in only 80.03 H@1 -a difference of 13.5% points. This difference emphasizes the need for a fair hyperparameter selection.\n\n\nConclusion\n\nIn this paper, we investigated state-of-the-art in Entity Alignment. Since we identified shortcomings in the commonly employed evaluation procedure, including the lack of validation sets for hyperparameter tuning and different initializations, we provided a fair and sound evaluation over a wide range of configurations. We additionally gave insight into the importance of individual components. Our results provide a strong, fair, and reproducible baseline for future works to compare against and offer deep insights into the inner workings of a GNN-based model. We plan to investigate the identified weakness against noisy labelings in future work and increase the robustness. Moreover, we aim to improve the usage of relation type information in the message passing phase of models like RDGCN, which only use them in an initial entity representation refinement stage. For some datasets such as OpenEA d-y and en-de, optimal configurations did not consider the relational information. However, intuitively, this information should help to improve the structural description of entities. Potential improvements include establishing a relation matching between the two graphs or modifying the mechanism used to integrate relational information.\n\nTable 1 .\n1Summary of the used EA datasets. We denote the entity set as E , the relation set as R, the triple set as T , the aligned entities as A and the exclusive entities as X .dataset \nsubset \ngraph \n|E| \n|R| \n|T | \n|A| \n|X | \n\nDBP15k \nzh-en \nzh \n19,388 \n1,701 \n70,414 \n15,000 \n4,388 \nen \n19,572 \n1,323 \n95,142 \n15,000 \n4,572 \nja-en \nja \n19,814 \n1,299 \n77,214 \n15,000 \n4,814 \nen \n19,780 \n1,153 \n93,484 \n15,000 \n4,780 \nfr-en \nfr \n19,661 \n903 \n105,998 \n15,000 \n4,661 \nen \n19,993 \n1,208 \n115,722 \n15,000 \n4,993 \n\nWK3l15k \nen-de \nen \n15,126 \n1,841 \n209,041 \n9,783 \n5,343 \nde \n14,603 \n596 \n144,244 \n10,021 \n4,582 \nen-fr \nen \n15,169 \n2,228 \n203,356 \n7,375 \n7,794 \nfr \n15,393 \n2,422 \n169,329 \n7,284 \n8,109 \n\nOpenEA \nen-de \nen \n15,000 \n169 \n84,867 \n15,000 \n0 \nde \n15,000 \n96 \n92,632 \n15,000 \n0 \nen-fr \nen \n15,000 \n193 \n96,318 \n15,000 \n0 \nfr \n15,000 \n166 \n80,112 \n15,000 \n0 \nd-y \nd \n15,000 \n72 \n68,063 \n15,000 \n0 \ny \n15,000 \n21 \n60,970 \n15,000 \n0 \nd-w \nd \n15,000 \n167 \n73,983 \n15,000 \n0 \nw \n15,000 \n121 \n83,365 \n15,000 \n0 \n\n\n\nTable 2 .\n2The statistics about label-based initialization in the OpenEA codebase:attribute denotes initialization via attribute values for a predefined set of \"name at-\ntributes\". id denotes initialization with the last part of the entity URI. For d-y this \nbasically leaks ground truth, whereas, for Wikidata, the URI contains only a numeric \nidentifier, thus rendering the initialization \"label\" useless. \n\nsubset \nside \nvia attribute \nvia id \nvia id (%) \n\nd-w \nd \n0 \n15,000 \n100.00% \nw \n8,391 \n7,301 \n48.67% \n\nd-y \nd \n2,883 \n12,122 \n80.81% \ny \n15,000 \n0 \n0.00% \n\n\n\nTable 4 .\n4Results in terms of H@1 for all investigated combinations of datasets, models, and initializations. Each cell represents the test performance of the best configuration of hyperparameters chosen according to validation performance.DBP15k (JAPE) \n\ninit \nWu [18] \nXu [20] \nsubset \nfr-en \nja-en \nzh-en \nfr-en \nja-en \nzh-en \n\nZero Shot \n79.47 \n63.48 \n56.07 \n83.70 \n65.64 \n59.40 \nGCN-Align* \n81.81 \n67.45 \n57.94 \n86.74 \n67.65 \n60.32 \nRDGCN \n86.91 \n72.90 \n66.44 \n86.82 \n74.35 \n69.54 \nDGMC \n89.35 \n72.17 \n69.98 \n90.12 \n76.60 \n68.76 \n\nOpenEA \n\ninit \nSun [15] \nsubset \nd-w \nd-y \nen-de \nen-fr \n\nZero Shot \n46.53 \n81.90 \n75.99 \n79.90 \nGCN-Align* \n45.76 \n84.65 \n85.34 \n89.41 \nRDGCN \n64.28 \n98.41 \n80.03 \n91.52 \nDGMC \n51.29 \n88.60 \n88.10 \n89.40 \n\nWK3l15k \ninit \nBERT \nsubset \nen-de \nen-fr \n\nZero Shot \n85.55 \n77.27 \nGCN-Align* \n85.92 \n78.22 \nRDGCN \n86.76 \n78.05 \nDGMC \n84.08 \n73.92 \n\n\n\nTable 5 .\n5Ablation results for RDGCN on OpenEA datasets. The setting used by[17] is underlined. The first number is validation H@1, the second number test H@1.Bold highlights the best configuration. Please notice that due to the specialties of EA \nevaluation, the test and validation performance are not directly comparable [3]. \n\nsubset \nparameter \nvalue \nd-w \nd-y \nen-de \nen-fr \n\nnormalization always \n84.06 / 64.28 \n99.44 / 97.48 \n97.72 / 93.56 \n96.89 / 91.52 \ninitial \n82.67 / 62.58 \n99.78 / 98.41 \n97.67 / 93.02 \n95.56 / 89.50 \nnever \n78.39 / 61.77 \n99.72 / 98.53 \n98.11 / 80.03 \n95.44 / 90.14 \n\nGCN \n0 \n57.33 / 50.79 \n92.33 / 83.83 \n98.11 / 80.03 \n92.22 / 86.94 \nlayers \n1 \n73.33 / 56.66 \n99.33 / 98.15 \n96.00 / 91.63 \n94.50 / 90.49 \n2 \n78.39 / 61.77 \n99.56 / 98.16 \n97.72 / 93.56 \n96.89 / 91.52 \n3 \n84.06 / 64.28 \n99.78 / 98.41 \n97.00 / 92.18 \n95.44 / 90.14 \n\ninteraction \n0 \n78.11 / 60.53 \n99.72 / 98.53 \n97.72 / 93.56 \n95.33 / 89.08 \nlayers \n1 \n78.39 / 61.77 \n99.78 / 98.41 \n97.67 / 92.59 \n95.44 / 90.14 \n2 \n82.67 / 62.58 \n99.56 / 98.16 \n98.11 / 80.03 \n96.89 / 91.52 \n3 \n84.06 / 64.28 \n99.50 / 97.85 \n97.67 / 93.02 \n95.56 / 89.50 \n\ntrainable \nno \n84.06 / 64.28 \n99.72 / 98.53 \n97.72 / 93.56 \n96.89 / 91.52 \nembeddings yes \n82.67 / 62.58 \n99.78 / 98.41 \n98.11 / 80.03 \n95.56 / 89.50 \n\nsimilarity \ncos \n82.67 / 62.58 \n99.56 / 98.16 \n98.11 / 80.03 \n95.56 / 89.50 \ndot \n63.28 / 40.80 \n91.50 / 79.81 \n85.17 / 78.54 \n89.94 / 78.17 \nl1 (inv.) \n77.89 / 60.78 \n99.50 / 97.85 \n93.78 / 88.96 \n94.06 / 88.69 \nl1 (neg.) \n84.06 / 64.28 \n99.72 / 98.53 \n97.72 / 93.56 \n96.89 / 91.52 \nl2 (inv.) \n75.28 / 60.20 \n96.72 / 92.06 \n95.06 / 90.13 \n94.44 / 89.60 \nl2 (neg.) \n72.50 / 51.04 \n99.78 / 98.41 \n94.61 / 89.40 \n94.28 / 87.79 \n\nhard \nno \n82.67 / 62.58 \n99.78 / 98.41 \n98.11 / 80.03 \n96.89 / 91.52 \nnegatives \nyes \n84.06 / 64.28 \n99.67 / 98.30 \n97.72 / 93.56 \n95.33 / 90.62 \n\n\nCommonly used evaluation metrics in EA automatically become better with a smaller size of test set[3].\nhttps://github.com/rusty1s/pytorch geometric/blob/d42a690fba68005f5738008a04f375ffd39bbb76/torch geometric/d 3 https://drive.google.com/open?id=1dYJtj1 J4nYJdrDY95ucGLCuZXDXI7PL 4 https://github.com/syxu828/Crosslingula-KG-Matching/blob/56710f8131ae072f00de97eb737315e4ac9510f2/READM\nhttps://github.com/nju-websoft/OpenEA/tree/2a6e0b03ec8cdcad4920704d1c38547a3ad72abe 6 https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/multilingual.md\nhttps://github.com/StephanieWyt/RDGCN 8 https://github.com/rusty1s/deep-graph-matching-consensus/\nAs a general rule, the results improve by 1-2 points when trained on train+validation, and it is not going to change the picture.\nAcknowledgementsThis work has been funded by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. The authors of this work take full responsibilities for its content.\nRandom search for hyper-parameter optimization. James Bergstra, Yoshua Bengio, J. Mach. Learn. Res. 13James Bergstra and Yoshua Bengio. Random search for hyper-parameter opti- mization. J. Mach. Learn. Res., 13:281-305, 2012.\n\nKnowledge graph entity alignment with graph convolutional networks: Lessons learned. Max Berrendorf, Evgeniy Faerman, Valentyn Melnychuk, Thomas Volker Tresp, M Seidl ; Joemon, Emine Jose, Jo\u00e3o Yilmaz, Pablo Magalh\u00e3es, Nicola Castells, M\u00e1rio J Ferro, Fl\u00e1vio Silva, Martins, Advances in Information Retrieval -42nd European Conference on IR Research. Lisbon, PortugalSpringer2020Proceedings, Part IIMax Berrendorf, Evgeniy Faerman, Valentyn Melnychuk, Volker Tresp, and Thomas Seidl. Knowledge graph entity alignment with graph convolutional net- works: Lessons learned. In Joemon M. Jose, Emine Yilmaz, Jo\u00e3o Magalh\u00e3es, Pablo Castells, Nicola Ferro, M\u00e1rio J. Silva, and Fl\u00e1vio Martins, editors, Advances in Information Retrieval -42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceedings, Part II, volume 12036 of Lecture Notes in Computer Science, pages 3-11. Springer, 2020.\n\nInterpretable and fair comparison of link prediction or entity alignment methods with adjusted mean rank. Max Berrendorf, Evgeniy Faerman, Laurent Vermue, Volker Tresp, abs/2002.06914CoRRMax Berrendorf, Evgeniy Faerman, Laurent Vermue, and Volker Tresp. Inter- pretable and fair comparison of link prediction or entity alignment methods with adjusted mean rank. CoRR, abs/2002.06914, 2020.\n\nMulti-channel graph neural network for entity alignment. Yixin Cao, Zhiyuan Liu, Chengjiang Li, Zhiyuan Liu, Juanzi Li, Tat-Seng Chua, ACL (1). Association for Computational LinguisticsYixin Cao, Zhiyuan Liu, Chengjiang Li, Zhiyuan Liu, Juanzi Li, and Tat-Seng Chua. Multi-channel graph neural network for entity alignment. In ACL (1), pages 1452-1461. Association for Computational Linguistics, 2019.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL-HLT (1). Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), pages 4171-4186. Association for Computational Linguistics, 2019.\n\nSpecial issue on knowledge graphs and semantics in text analysis and retrieval. Laura Dietz, Chenyan Xiong, Jeff Dalton, Edgar Meij, Inf. Retr. J. 223-4Laura Dietz, Chenyan Xiong, Jeff Dalton, and Edgar Meij. Special issue on knowl- edge graphs and semantics in text analysis and retrieval. Inf. Retr. J., 22(3-4):229- 231, 2019.\n\nDeep graph matching consensus. Matthias Fey, Jan Eric Lenssen, Christopher Morris, Jonathan Masci, Nils M Kriege, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Matthias Fey, Jan Eric Lenssen, Christopher Morris, Jonathan Masci, and Nils M. Kriege. Deep graph matching consensus. In 8th International Conference on Learn- ing Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Open- Review.net, 2020.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770-778. IEEE Computer Society, 2016.\n\nSemi-supervised entity alignment via joint knowledge embedding model and crossgraph model. Chengjiang Li, Yixin Cao, Lei Hou, Jiaxin Shi, Juanzi Li, Tat-Seng Chua, EMNLP/IJCNLP (1). Association for Computational LinguisticsChengjiang Li, Yixin Cao, Lei Hou, Jiaxin Shi, Juanzi Li, and Tat-Seng Chua. Semi-supervised entity alignment via joint knowledge embedding model and cross- graph model. In EMNLP/IJCNLP (1), pages 2723-2732. Association for Compu- tational Linguistics, 2019.\n\nMRAEA: an efficient and robust entity alignment approach for cross-lingual knowledge graph. Xin Mao, Wenting Wang, Huimin Xu, Man Lan, Yuanbin Wu, WSDM. ACMXin Mao, Wenting Wang, Huimin Xu, Man Lan, and Yuanbin Wu. MRAEA: an efficient and robust entity alignment approach for cross-lingual knowledge graph. In WSDM, pages 420-428. ACM, 2020.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Alessandro Moschitti, Bo Pang, and Walter Daelemansthe 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACLA meeting of SIGDAT, a Special Interest Group of the ACLJeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532-1543. ACL, 2014.\n\nModeling multi-mapping relations for precise crosslingual entity alignment. Xiaofei Shi, Yanghua Xiao, EMNLP/IJCNLP (1). Association for Computational LinguisticsXiaofei Shi and Yanghua Xiao. Modeling multi-mapping relations for precise cross- lingual entity alignment. In EMNLP/IJCNLP (1), pages 813-822. Association for Computational Linguistics, 2019.\n\nHighway networks. Klaus Rupesh Kumar Srivastava, J\u00fcrgen Greff, Schmidhuber, abs/1505.00387CoRRRupesh Kumar Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Highway net- works. CoRR, abs/1505.00387, 2015.\n\nKnowledge graph alignment network with gated multi-hop neighborhood aggregation. Zequn Sun, Chengming Wang, Wei Hu, Muhao Chen, Jian Dai, Wei Zhang, Yuzhong Qu, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceZequn Sun, Chengming Wang, Wei Hu, Muhao Chen, Jian Dai, Wei Zhang, and Yuzhong Qu. Knowledge graph alignment network with gated multi-hop neigh- borhood aggregation. In The Thirty-Fourth AAAI Conference on Artificial In- telligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 222-229. AAAI Press, 2020.\n\nA benchmarking study of embedding-based entity alignment for knowledge graphs. Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, Chengkai Li, Proc. VLDB Endow. VLDB Endow13Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farah- naz Akrami, and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge graphs. Proc. VLDB Endow., 13(11):2326-2340, 2020.\n\nCross-lingual knowledge graph alignment via graph convolutional networks. Zhichun Wang, Qingsong Lv, Xiaohan Lan, Yu Zhang, EMNLP. Association for Computational LinguisticsZhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In EMNLP, pages 349-357. Association for Computational Linguistics, 2018.\n\nRelation-aware entity alignment for heterogeneous knowledge graphs. Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, Dongyan Zhao, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019. Sarit Krausthe Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019Macao, Chinaijcai.orgYuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, and Dongyan Zhao. Relation-aware entity alignment for heterogeneous knowledge graphs. In Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 5278-5284. ijcai.org, 2019.\n\nJointly learning entity and relation representations for entity alignment. Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Dongyan Zhao, EMNLP/IJCNLP (1). Association for Computational LinguisticsYuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, and Dongyan Zhao. Jointly learning entity and relation representations for entity alignment. In EMNLP/IJCNLP (1), pages 240-249. Association for Computational Linguistics, 2019.\n\nHigh-order relation construction and mining for graph matching. Hui Xu, Liyao Xiang, Youmin Le, Xiaoying Gan, Yuting Jia, Luoyi Fu, Xinbing Wang, abs/2010.04348CoRRHui Xu, Liyao Xiang, Youmin Le, Xiaoying Gan, Yuting Jia, Luoyi Fu, and Xinbing Wang. High-order relation construction and mining for graph matching. CoRR, abs/2010.04348, 2020.\n\nCross-lingual knowledge graph alignment via graph matching neural network. Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song, Zhiguo Wang, Dong Yu, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Kun Xu, Liwei Wang, Mo Yu, Yansong Feng, Yan Song, Zhiguo Wang, and Dong Yu. Cross-lingual knowledge graph alignment via graph matching neural network. In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 3156-3161. Association for Computational Linguistics, 2019.\n\nAligning cross-lingual entities with multi-aspect information. Hsiu-Wei Yang, Yanyan Zou, Peng Shi, Wei Lu, Jimmy Lin, Xu Sun, EMNLP/IJCNLP (1). Association for Computational LinguisticsHsiu-Wei Yang, Yanyan Zou, Peng Shi, Wei Lu, Jimmy Lin, and Xu Sun. Align- ing cross-lingual entities with multi-aspect information. In EMNLP/IJCNLP (1), pages 4430-4440. Association for Computational Linguistics, 2019.\n\nA vectorized relational graph convolutional network for multi-relational network alignment. Rui Ye, Xin Li, Yujie Fang, Hongyu Zang, Mingzhong Wang, IJCAI. Rui Ye, Xin Li, Yujie Fang, Hongyu Zang, and Mingzhong Wang. A vectorized relational graph convolutional network for multi-relational network alignment. In IJCAI, pages 4135-4141. ijcai.org, 2019.\n\nMulti-view knowledge graph embedding for entity alignment. Qingheng Zhang, Zequn Sun, Wei Hu, Muhao Chen, Lingbing Guo, Yuzhong Qu, IJCAI. Qingheng Zhang, Zequn Sun, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong Qu. Multi-view knowledge graph embedding for entity alignment. In IJCAI, pages 5429-5435. ijcai.org, 2019.\n\nAn experimental study of state-of-the-art entity alignment approaches. X Zhao, W Zeng, J Tang, W Wang, F Suchanek, IEEE Transactions on Knowledge & Data Engineering. 01X. Zhao, W. Zeng, J. Tang, W. Wang, and F. Suchanek. An experimental study of state-of-the-art entity alignment approaches. IEEE Transactions on Knowledge & Data Engineering, (01):1-1, aug 5555.\n\nNeighborhoodaware attentional representation for multilingual knowledge graphs. Qiannan Zhu, Xiaofei Zhou, Jia Wu, Jianlong Tan, Li Guo, IJCAI. Qiannan Zhu, Xiaofei Zhou, Jia Wu, Jianlong Tan, and Li Guo. Neighborhood- aware attentional representation for multilingual knowledge graphs. In IJCAI, pages 1943-1949. ijcai.org, 2019.\n", "annotations": {"author": "[{\"end\":145,\"start\":76},{\"end\":202,\"start\":146},{\"end\":294,\"start\":203},{\"end\":388,\"start\":295}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":80},{\"end\":216,\"start\":210},{\"end\":310,\"start\":303}]", "author_first_name": "[{\"end\":79,\"start\":76},{\"end\":147,\"start\":146},{\"end\":209,\"start\":203},{\"end\":302,\"start\":295}]", "author_affiliation": "[{\"end\":144,\"start\":92},{\"end\":201,\"start\":149},{\"end\":293,\"start\":241},{\"end\":387,\"start\":335}]", "title": "[{\"end\":62,\"start\":1},{\"end\":450,\"start\":389}]", "venue": null, "abstract": "[{\"end\":1844,\"start\":515}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2018,\"start\":2015},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2153,\"start\":2150},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3092,\"start\":3088},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3094,\"start\":3092},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3097,\"start\":3094},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3100,\"start\":3097},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3103,\"start\":3100},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3105,\"start\":3103},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3108,\"start\":3105},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3111,\"start\":3108},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3114,\"start\":3111},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3117,\"start\":3114},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3119,\"start\":3117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3122,\"start\":3119},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3128,\"start\":3125},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3131,\"start\":3128},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3518,\"start\":3514},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3606,\"start\":3602},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3609,\"start\":3606},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3699,\"start\":3696},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4606,\"start\":4602},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5902,\"start\":5899},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6667,\"start\":6663},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6729,\"start\":6728},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6782,\"start\":6779},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7636,\"start\":7633},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8500,\"start\":8496},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8508,\"start\":8505},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8545,\"start\":8541},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8579,\"start\":8575},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8637,\"start\":8633},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8739,\"start\":8736},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8859,\"start\":8855},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8979,\"start\":8975},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9095,\"start\":9091},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10498,\"start\":10494},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10694,\"start\":10691},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10743,\"start\":10740},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11072,\"start\":11068},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11108,\"start\":11105},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11967,\"start\":11963},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12556,\"start\":12553},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12598,\"start\":12594},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17053,\"start\":17050},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18656,\"start\":18653},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18846,\"start\":18842},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19914,\"start\":19911},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20094,\"start\":20090},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21454,\"start\":21450},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21534,\"start\":21531},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22827,\"start\":22823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29612,\"start\":29608},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31501,\"start\":31498}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28077,\"start\":27057},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28646,\"start\":28078},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29529,\"start\":28647},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31399,\"start\":29530}]", "paragraph": "[{\"end\":2354,\"start\":1860},{\"end\":2991,\"start\":2356},{\"end\":3422,\"start\":2993},{\"end\":5636,\"start\":3424},{\"end\":6985,\"start\":5677},{\"end\":7546,\"start\":6987},{\"end\":8331,\"start\":7548},{\"end\":8931,\"start\":8363},{\"end\":9314,\"start\":8933},{\"end\":10500,\"start\":9316},{\"end\":11017,\"start\":10502},{\"end\":11350,\"start\":11029},{\"end\":11880,\"start\":11352},{\"end\":12618,\"start\":11940},{\"end\":12762,\"start\":12620},{\"end\":12797,\"start\":12794},{\"end\":13444,\"start\":12892},{\"end\":13616,\"start\":13446},{\"end\":13936,\"start\":13752},{\"end\":15461,\"start\":14015},{\"end\":15654,\"start\":15463},{\"end\":15787,\"start\":15656},{\"end\":16502,\"start\":15875},{\"end\":17004,\"start\":16504},{\"end\":17820,\"start\":17045},{\"end\":18543,\"start\":17822},{\"end\":19605,\"start\":18559},{\"end\":20169,\"start\":19607},{\"end\":20995,\"start\":20171},{\"end\":21872,\"start\":21007},{\"end\":23525,\"start\":21874},{\"end\":25797,\"start\":23545},{\"end\":27056,\"start\":25812}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12793,\"start\":12763},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12891,\"start\":12798},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13751,\"start\":13617},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14014,\"start\":13937},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15874,\"start\":15788}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5405,\"start\":5398},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9967,\"start\":9960},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14718,\"start\":14711},{\"end\":19668,\"start\":19661},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19718,\"start\":19711},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20497,\"start\":20490},{\"end\":20789,\"start\":20782},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20934,\"start\":20927},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21871,\"start\":21864},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23670,\"start\":23663},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24119,\"start\":24112}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1858,\"start\":1846},{\"attributes\":{\"n\":\"2\"},\"end\":5664,\"start\":5639},{\"attributes\":{\"n\":\"2.1\"},\"end\":5675,\"start\":5667},{\"attributes\":{\"n\":\"2.2\"},\"end\":8361,\"start\":8334},{\"attributes\":{\"n\":\"3\"},\"end\":11027,\"start\":11020},{\"attributes\":{\"n\":\"3.1\"},\"end\":11938,\"start\":11883},{\"attributes\":{\"n\":\"3.2\"},\"end\":17043,\"start\":17007},{\"attributes\":{\"n\":\"4\"},\"end\":18557,\"start\":18546},{\"attributes\":{\"n\":\"4.1\"},\"end\":21005,\"start\":20998},{\"attributes\":{\"n\":\"4.2\"},\"end\":23543,\"start\":23528},{\"attributes\":{\"n\":\"5\"},\"end\":25810,\"start\":25800},{\"end\":27067,\"start\":27058},{\"end\":28088,\"start\":28079},{\"end\":28657,\"start\":28648},{\"end\":29540,\"start\":29531}]", "table": "[{\"end\":28077,\"start\":27238},{\"end\":28646,\"start\":28161},{\"end\":29529,\"start\":28889},{\"end\":31399,\"start\":29691}]", "figure_caption": "[{\"end\":27238,\"start\":27069},{\"end\":28161,\"start\":28090},{\"end\":28889,\"start\":28659},{\"end\":29691,\"start\":29542}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":32459,\"start\":32454},{\"end\":32476,\"start\":32470},{\"end\":32721,\"start\":32718},{\"end\":32741,\"start\":32734},{\"end\":32759,\"start\":32751},{\"end\":32777,\"start\":32771},{\"end\":32793,\"start\":32792},{\"end\":32815,\"start\":32810},{\"end\":32826,\"start\":32822},{\"end\":32840,\"start\":32835},{\"end\":32858,\"start\":32852},{\"end\":32874,\"start\":32869},{\"end\":32876,\"start\":32875},{\"end\":32890,\"start\":32884},{\"end\":33662,\"start\":33659},{\"end\":33682,\"start\":33675},{\"end\":33699,\"start\":33692},{\"end\":33714,\"start\":33708},{\"end\":34006,\"start\":34001},{\"end\":34019,\"start\":34012},{\"end\":34035,\"start\":34025},{\"end\":34047,\"start\":34040},{\"end\":34059,\"start\":34053},{\"end\":34072,\"start\":34064},{\"end\":34434,\"start\":34429},{\"end\":34451,\"start\":34443},{\"end\":34465,\"start\":34459},{\"end\":34479,\"start\":34471},{\"end\":34865,\"start\":34860},{\"end\":34880,\"start\":34873},{\"end\":34892,\"start\":34888},{\"end\":34906,\"start\":34901},{\"end\":35150,\"start\":35142},{\"end\":35159,\"start\":35156},{\"end\":35164,\"start\":35160},{\"end\":35185,\"start\":35174},{\"end\":35202,\"start\":35194},{\"end\":35214,\"start\":35210},{\"end\":35216,\"start\":35215},{\"end\":35621,\"start\":35614},{\"end\":35633,\"start\":35626},{\"end\":35649,\"start\":35641},{\"end\":35659,\"start\":35655},{\"end\":36144,\"start\":36134},{\"end\":36154,\"start\":36149},{\"end\":36163,\"start\":36160},{\"end\":36175,\"start\":36169},{\"end\":36187,\"start\":36181},{\"end\":36200,\"start\":36192},{\"end\":36621,\"start\":36618},{\"end\":36634,\"start\":36627},{\"end\":36647,\"start\":36641},{\"end\":36655,\"start\":36652},{\"end\":36668,\"start\":36661},{\"end\":36923,\"start\":36916},{\"end\":36943,\"start\":36936},{\"end\":36963,\"start\":36952},{\"end\":36965,\"start\":36964},{\"end\":37735,\"start\":37728},{\"end\":37748,\"start\":37741},{\"end\":38031,\"start\":38026},{\"end\":38063,\"start\":38057},{\"end\":38299,\"start\":38294},{\"end\":38314,\"start\":38305},{\"end\":38324,\"start\":38321},{\"end\":38334,\"start\":38329},{\"end\":38345,\"start\":38341},{\"end\":38354,\"start\":38351},{\"end\":38369,\"start\":38362},{\"end\":39217,\"start\":39212},{\"end\":39231,\"start\":39223},{\"end\":39242,\"start\":39239},{\"end\":39256,\"start\":39247},{\"end\":39268,\"start\":39263},{\"end\":39283,\"start\":39275},{\"end\":39300,\"start\":39292},{\"end\":39638,\"start\":39631},{\"end\":39653,\"start\":39645},{\"end\":39665,\"start\":39658},{\"end\":39673,\"start\":39671},{\"end\":40006,\"start\":40000},{\"end\":40015,\"start\":40011},{\"end\":40028,\"start\":40021},{\"end\":40040,\"start\":40035},{\"end\":40050,\"start\":40047},{\"end\":40063,\"start\":40056},{\"end\":40713,\"start\":40707},{\"end\":40722,\"start\":40718},{\"end\":40735,\"start\":40728},{\"end\":40747,\"start\":40742},{\"end\":40761,\"start\":40754},{\"end\":41120,\"start\":41117},{\"end\":41130,\"start\":41125},{\"end\":41144,\"start\":41138},{\"end\":41157,\"start\":41149},{\"end\":41169,\"start\":41163},{\"end\":41180,\"start\":41175},{\"end\":41192,\"start\":41185},{\"end\":41474,\"start\":41471},{\"end\":41484,\"start\":41479},{\"end\":41493,\"start\":41491},{\"end\":41505,\"start\":41498},{\"end\":41515,\"start\":41512},{\"end\":41528,\"start\":41522},{\"end\":41539,\"start\":41535},{\"end\":42333,\"start\":42325},{\"end\":42346,\"start\":42340},{\"end\":42356,\"start\":42352},{\"end\":42365,\"start\":42362},{\"end\":42375,\"start\":42370},{\"end\":42383,\"start\":42381},{\"end\":42764,\"start\":42761},{\"end\":42772,\"start\":42769},{\"end\":42782,\"start\":42777},{\"end\":42795,\"start\":42789},{\"end\":42811,\"start\":42802},{\"end\":43090,\"start\":43082},{\"end\":43103,\"start\":43098},{\"end\":43112,\"start\":43109},{\"end\":43122,\"start\":43117},{\"end\":43137,\"start\":43129},{\"end\":43150,\"start\":43143},{\"end\":43415,\"start\":43414},{\"end\":43423,\"start\":43422},{\"end\":43431,\"start\":43430},{\"end\":43439,\"start\":43438},{\"end\":43447,\"start\":43446},{\"end\":43794,\"start\":43787},{\"end\":43807,\"start\":43800},{\"end\":43817,\"start\":43814},{\"end\":43830,\"start\":43822},{\"end\":43838,\"start\":43836}]", "bib_author_last_name": "[{\"end\":32468,\"start\":32460},{\"end\":32483,\"start\":32477},{\"end\":32732,\"start\":32722},{\"end\":32749,\"start\":32742},{\"end\":32769,\"start\":32760},{\"end\":32790,\"start\":32778},{\"end\":32808,\"start\":32794},{\"end\":32820,\"start\":32816},{\"end\":32833,\"start\":32827},{\"end\":32850,\"start\":32841},{\"end\":32867,\"start\":32859},{\"end\":32882,\"start\":32877},{\"end\":32896,\"start\":32891},{\"end\":32905,\"start\":32898},{\"end\":33673,\"start\":33663},{\"end\":33690,\"start\":33683},{\"end\":33706,\"start\":33700},{\"end\":33720,\"start\":33715},{\"end\":34010,\"start\":34007},{\"end\":34023,\"start\":34020},{\"end\":34038,\"start\":34036},{\"end\":34051,\"start\":34048},{\"end\":34062,\"start\":34060},{\"end\":34077,\"start\":34073},{\"end\":34441,\"start\":34435},{\"end\":34457,\"start\":34452},{\"end\":34469,\"start\":34466},{\"end\":34489,\"start\":34480},{\"end\":34871,\"start\":34866},{\"end\":34886,\"start\":34881},{\"end\":34899,\"start\":34893},{\"end\":34911,\"start\":34907},{\"end\":35154,\"start\":35151},{\"end\":35172,\"start\":35165},{\"end\":35192,\"start\":35186},{\"end\":35208,\"start\":35203},{\"end\":35223,\"start\":35217},{\"end\":35624,\"start\":35622},{\"end\":35639,\"start\":35634},{\"end\":35653,\"start\":35650},{\"end\":35663,\"start\":35660},{\"end\":36147,\"start\":36145},{\"end\":36158,\"start\":36155},{\"end\":36167,\"start\":36164},{\"end\":36179,\"start\":36176},{\"end\":36190,\"start\":36188},{\"end\":36205,\"start\":36201},{\"end\":36625,\"start\":36622},{\"end\":36639,\"start\":36635},{\"end\":36650,\"start\":36648},{\"end\":36659,\"start\":36656},{\"end\":36671,\"start\":36669},{\"end\":36934,\"start\":36924},{\"end\":36950,\"start\":36944},{\"end\":36973,\"start\":36966},{\"end\":37739,\"start\":37736},{\"end\":37753,\"start\":37749},{\"end\":38055,\"start\":38032},{\"end\":38069,\"start\":38064},{\"end\":38082,\"start\":38071},{\"end\":38303,\"start\":38300},{\"end\":38319,\"start\":38315},{\"end\":38327,\"start\":38325},{\"end\":38339,\"start\":38335},{\"end\":38349,\"start\":38346},{\"end\":38360,\"start\":38355},{\"end\":38372,\"start\":38370},{\"end\":39221,\"start\":39218},{\"end\":39237,\"start\":39232},{\"end\":39245,\"start\":39243},{\"end\":39261,\"start\":39257},{\"end\":39273,\"start\":39269},{\"end\":39290,\"start\":39284},{\"end\":39303,\"start\":39301},{\"end\":39643,\"start\":39639},{\"end\":39656,\"start\":39654},{\"end\":39669,\"start\":39666},{\"end\":39679,\"start\":39674},{\"end\":40009,\"start\":40007},{\"end\":40019,\"start\":40016},{\"end\":40033,\"start\":40029},{\"end\":40045,\"start\":40041},{\"end\":40054,\"start\":40051},{\"end\":40068,\"start\":40064},{\"end\":40716,\"start\":40714},{\"end\":40726,\"start\":40723},{\"end\":40740,\"start\":40736},{\"end\":40752,\"start\":40748},{\"end\":40766,\"start\":40762},{\"end\":41123,\"start\":41121},{\"end\":41136,\"start\":41131},{\"end\":41147,\"start\":41145},{\"end\":41161,\"start\":41158},{\"end\":41173,\"start\":41170},{\"end\":41183,\"start\":41181},{\"end\":41197,\"start\":41193},{\"end\":41477,\"start\":41475},{\"end\":41489,\"start\":41485},{\"end\":41496,\"start\":41494},{\"end\":41510,\"start\":41506},{\"end\":41520,\"start\":41516},{\"end\":41533,\"start\":41529},{\"end\":41542,\"start\":41540},{\"end\":42338,\"start\":42334},{\"end\":42350,\"start\":42347},{\"end\":42360,\"start\":42357},{\"end\":42368,\"start\":42366},{\"end\":42379,\"start\":42376},{\"end\":42387,\"start\":42384},{\"end\":42767,\"start\":42765},{\"end\":42775,\"start\":42773},{\"end\":42787,\"start\":42783},{\"end\":42800,\"start\":42796},{\"end\":42816,\"start\":42812},{\"end\":43096,\"start\":43091},{\"end\":43107,\"start\":43104},{\"end\":43115,\"start\":43113},{\"end\":43127,\"start\":43123},{\"end\":43141,\"start\":43138},{\"end\":43153,\"start\":43151},{\"end\":43420,\"start\":43416},{\"end\":43428,\"start\":43424},{\"end\":43436,\"start\":43432},{\"end\":43444,\"start\":43440},{\"end\":43456,\"start\":43448},{\"end\":43798,\"start\":43795},{\"end\":43812,\"start\":43808},{\"end\":43820,\"start\":43818},{\"end\":43834,\"start\":43831},{\"end\":43842,\"start\":43839}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15700257},\"end\":32631,\"start\":32406},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208158314},\"end\":33551,\"start\":32633},{\"attributes\":{\"doi\":\"abs/2002.06914\",\"id\":\"b2\"},\"end\":33942,\"start\":33553},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":196205749},\"end\":34345,\"start\":33944},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52967399},\"end\":34778,\"start\":34347},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":71146113},\"end\":35109,\"start\":34780},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":210846466},\"end\":35566,\"start\":35111},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":36041,\"start\":35568},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202770936},\"end\":36524,\"start\":36043},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":210882549},\"end\":36867,\"start\":36526},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1957433},\"end\":37650,\"start\":36869},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":202790121},\"end\":38006,\"start\":37652},{\"attributes\":{\"doi\":\"abs/1505.00387\",\"id\":\"b12\"},\"end\":38211,\"start\":38008},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208176414},\"end\":39131,\"start\":38213},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":212737039},\"end\":39555,\"start\":39133},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53082628},\"end\":39930,\"start\":39557},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":198354047},\"end\":40630,\"start\":39932},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":202712648},\"end\":41051,\"start\":40632},{\"attributes\":{\"doi\":\"abs/2010.04348\",\"id\":\"b18\"},\"end\":41394,\"start\":41053},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":167217453},\"end\":42260,\"start\":41396},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":202121966},\"end\":42667,\"start\":42262},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":199466148},\"end\":43021,\"start\":42669},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":174802832},\"end\":43341,\"start\":43023},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":226374030},\"end\":43705,\"start\":43343},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":199465777},\"end\":44037,\"start\":43707}]", "bib_title": "[{\"end\":32452,\"start\":32406},{\"end\":32716,\"start\":32633},{\"end\":33999,\"start\":33944},{\"end\":34427,\"start\":34347},{\"end\":34858,\"start\":34780},{\"end\":35140,\"start\":35111},{\"end\":35612,\"start\":35568},{\"end\":36132,\"start\":36043},{\"end\":36616,\"start\":36526},{\"end\":36914,\"start\":36869},{\"end\":37726,\"start\":37652},{\"end\":38292,\"start\":38213},{\"end\":39210,\"start\":39133},{\"end\":39629,\"start\":39557},{\"end\":39998,\"start\":39932},{\"end\":40705,\"start\":40632},{\"end\":41469,\"start\":41396},{\"end\":42323,\"start\":42262},{\"end\":42759,\"start\":42669},{\"end\":43080,\"start\":43023},{\"end\":43412,\"start\":43343},{\"end\":43785,\"start\":43707}]", "bib_author": "[{\"end\":32470,\"start\":32454},{\"end\":32485,\"start\":32470},{\"end\":32734,\"start\":32718},{\"end\":32751,\"start\":32734},{\"end\":32771,\"start\":32751},{\"end\":32792,\"start\":32771},{\"end\":32810,\"start\":32792},{\"end\":32822,\"start\":32810},{\"end\":32835,\"start\":32822},{\"end\":32852,\"start\":32835},{\"end\":32869,\"start\":32852},{\"end\":32884,\"start\":32869},{\"end\":32898,\"start\":32884},{\"end\":32907,\"start\":32898},{\"end\":33675,\"start\":33659},{\"end\":33692,\"start\":33675},{\"end\":33708,\"start\":33692},{\"end\":33722,\"start\":33708},{\"end\":34012,\"start\":34001},{\"end\":34025,\"start\":34012},{\"end\":34040,\"start\":34025},{\"end\":34053,\"start\":34040},{\"end\":34064,\"start\":34053},{\"end\":34079,\"start\":34064},{\"end\":34443,\"start\":34429},{\"end\":34459,\"start\":34443},{\"end\":34471,\"start\":34459},{\"end\":34491,\"start\":34471},{\"end\":34873,\"start\":34860},{\"end\":34888,\"start\":34873},{\"end\":34901,\"start\":34888},{\"end\":34913,\"start\":34901},{\"end\":35156,\"start\":35142},{\"end\":35174,\"start\":35156},{\"end\":35194,\"start\":35174},{\"end\":35210,\"start\":35194},{\"end\":35225,\"start\":35210},{\"end\":35626,\"start\":35614},{\"end\":35641,\"start\":35626},{\"end\":35655,\"start\":35641},{\"end\":35665,\"start\":35655},{\"end\":36149,\"start\":36134},{\"end\":36160,\"start\":36149},{\"end\":36169,\"start\":36160},{\"end\":36181,\"start\":36169},{\"end\":36192,\"start\":36181},{\"end\":36207,\"start\":36192},{\"end\":36627,\"start\":36618},{\"end\":36641,\"start\":36627},{\"end\":36652,\"start\":36641},{\"end\":36661,\"start\":36652},{\"end\":36673,\"start\":36661},{\"end\":36936,\"start\":36916},{\"end\":36952,\"start\":36936},{\"end\":36975,\"start\":36952},{\"end\":37741,\"start\":37728},{\"end\":37755,\"start\":37741},{\"end\":38057,\"start\":38026},{\"end\":38071,\"start\":38057},{\"end\":38084,\"start\":38071},{\"end\":38305,\"start\":38294},{\"end\":38321,\"start\":38305},{\"end\":38329,\"start\":38321},{\"end\":38341,\"start\":38329},{\"end\":38351,\"start\":38341},{\"end\":38362,\"start\":38351},{\"end\":38374,\"start\":38362},{\"end\":39223,\"start\":39212},{\"end\":39239,\"start\":39223},{\"end\":39247,\"start\":39239},{\"end\":39263,\"start\":39247},{\"end\":39275,\"start\":39263},{\"end\":39292,\"start\":39275},{\"end\":39305,\"start\":39292},{\"end\":39645,\"start\":39631},{\"end\":39658,\"start\":39645},{\"end\":39671,\"start\":39658},{\"end\":39681,\"start\":39671},{\"end\":40011,\"start\":40000},{\"end\":40021,\"start\":40011},{\"end\":40035,\"start\":40021},{\"end\":40047,\"start\":40035},{\"end\":40056,\"start\":40047},{\"end\":40070,\"start\":40056},{\"end\":40718,\"start\":40707},{\"end\":40728,\"start\":40718},{\"end\":40742,\"start\":40728},{\"end\":40754,\"start\":40742},{\"end\":40768,\"start\":40754},{\"end\":41125,\"start\":41117},{\"end\":41138,\"start\":41125},{\"end\":41149,\"start\":41138},{\"end\":41163,\"start\":41149},{\"end\":41175,\"start\":41163},{\"end\":41185,\"start\":41175},{\"end\":41199,\"start\":41185},{\"end\":41479,\"start\":41471},{\"end\":41491,\"start\":41479},{\"end\":41498,\"start\":41491},{\"end\":41512,\"start\":41498},{\"end\":41522,\"start\":41512},{\"end\":41535,\"start\":41522},{\"end\":41544,\"start\":41535},{\"end\":42340,\"start\":42325},{\"end\":42352,\"start\":42340},{\"end\":42362,\"start\":42352},{\"end\":42370,\"start\":42362},{\"end\":42381,\"start\":42370},{\"end\":42389,\"start\":42381},{\"end\":42769,\"start\":42761},{\"end\":42777,\"start\":42769},{\"end\":42789,\"start\":42777},{\"end\":42802,\"start\":42789},{\"end\":42818,\"start\":42802},{\"end\":43098,\"start\":43082},{\"end\":43109,\"start\":43098},{\"end\":43117,\"start\":43109},{\"end\":43129,\"start\":43117},{\"end\":43143,\"start\":43129},{\"end\":43155,\"start\":43143},{\"end\":43422,\"start\":43414},{\"end\":43430,\"start\":43422},{\"end\":43438,\"start\":43430},{\"end\":43446,\"start\":43438},{\"end\":43458,\"start\":43446},{\"end\":43800,\"start\":43787},{\"end\":43814,\"start\":43800},{\"end\":43822,\"start\":43814},{\"end\":43836,\"start\":43822},{\"end\":43844,\"start\":43836}]", "bib_venue": "[{\"end\":32999,\"start\":32983},{\"end\":35304,\"start\":35283},{\"end\":35759,\"start\":35741},{\"end\":37196,\"start\":37114},{\"end\":38545,\"start\":38528},{\"end\":39333,\"start\":39323},{\"end\":40284,\"start\":40185},{\"end\":41780,\"start\":41687},{\"end\":32504,\"start\":32485},{\"end\":32981,\"start\":32907},{\"end\":33657,\"start\":33553},{\"end\":34086,\"start\":34079},{\"end\":34504,\"start\":34491},{\"end\":34925,\"start\":34913},{\"end\":35281,\"start\":35225},{\"end\":35739,\"start\":35665},{\"end\":36223,\"start\":36207},{\"end\":36677,\"start\":36673},{\"end\":37061,\"start\":36975},{\"end\":37771,\"start\":37755},{\"end\":38024,\"start\":38008},{\"end\":38526,\"start\":38374},{\"end\":39321,\"start\":39305},{\"end\":39686,\"start\":39681},{\"end\":40172,\"start\":40070},{\"end\":40784,\"start\":40768},{\"end\":41115,\"start\":41053},{\"end\":41637,\"start\":41544},{\"end\":42405,\"start\":42389},{\"end\":42823,\"start\":42818},{\"end\":43160,\"start\":43155},{\"end\":43507,\"start\":43458},{\"end\":43849,\"start\":43844}]"}}}, "year": 2023, "month": 12, "day": 17}