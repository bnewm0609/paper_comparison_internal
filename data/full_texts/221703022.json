{"id": 221703022, "updated": "2023-10-06 11:05:50.489", "metadata": {"title": "Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product", "authors": "[{\"first\":\"Tiangang\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Haoran\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Youzheng\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xiaodong\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Bowen\",\"last\":\"Zhou\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "publication_date": {"year": 2020, "month": 9, "day": 15}, "abstract": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset will be released to the public.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.07162", "mag": "3104609290", "acl": "2020.emnlp-main.166", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/ZhuWLWHZ20", "doi": "10.18653/v1/2020.emnlp-main.166"}}, "content": {"source": {"pdf_hash": "97679b9616045bfa3723db15f360cf9f2c8b52ad", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.07162v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2009.07162", "status": "GREEN"}}, "grobid": {"id": "a3a6ad539b95fa819da6d7d6a9ba54004b750615", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/97679b9616045bfa3723db15f360cf9f2c8b52ad.txt", "contents": "\nMultimodal Joint Attribute Prediction and Value Extraction for E-commerce Product\n\n\nTiangang Zhu zhutiangang3@jd.com \nYue Wang wangyue274@jd.com \nHaoran Li lihaoran24@jd.com \nYouzheng Wu wuyouzheng1@jd.com \nXiaodong He xiaodong.he@jd.com \nJDBowen Zhou bowen.zhou@jd.com \nA I Research \nMultimodal Joint Attribute Prediction and Value Extraction for E-commerce Product\n\nProduct attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available 1 .\n\nIntroduction\n\nProduct attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015;Yu et al., 2017), product recommendations (Gong,  2009; Cao et al., 2018), and product retrieval (Liao et al., 2018;Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers' shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1.\n\nThough plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011;More, 2016;Shinzato and Sekine, 2013;Zheng et al., 2018;Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task.\n\n\nBERT\n\n\nThis wool trench coat features an elegant big lapel design\n\nText Self-Attention  Intuitively, product attributes and values are mutually indicative. Given a textual product description, we can extract attribute values more accurately with a known product attribute. We model the relationship between product attributes and values from the following three aspects. First, we apply a multitask learning (Caruana, 1997) method to predict the product attributes and the values jointly. Second, we extract values with the guidance of the predicted product attributes. Third, we adopt a Kullback-Leibler (KL) (Kullback and Leibler, 1951) measurement to penalize the inconsistency between the distribution of the product attribute prediction and that of the value extraction.\n\nFurthermore, beyond the textual product descriptions, product images can provide additional clues for the attribute prediction and value extraction tasks. Figure 1 illustrates this phenomenon. Given a description \"This golden band collar shirt can be dressed up with black shoes\", the term \"golden\" can be ambiguous for predicting the product attributes. While by viewing the product image, we can easily recognize the attribute corresponding to \"golden\" is \"Color\" instead of \"Material\". Moreover, the product image can indicate that the term \"black\" is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal informa-tion shows promising efficiency on many tasks (Lu et al., 2016;Li et al., 2017;Anderson et al., 2018;Yu et al., 2019;Li et al., 2019;Tan and Bansal, 2019;Su et al., 2020;. Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a globalgated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to improve the accuracy of values extraction.\n\nOur main contributions are threefold:\n\n\u2022 We propose an end-to-end model to predict product attributes and extract the corresponding values.\n\n\u2022 Our model can selectively adopt visual product information by global and regional visual gates to enhance the attribute prediction and value extraction model.\n\n\u2022 We build a multimodal product attribute value dataset that contains 87,194 instances, involving various product categories.\n\n\nOverview\n\nIn this work, we tackle the product attribute-value pair completion task, i.e., predicting attributes and extracting the corresponding values for ecommerce products. The input of the task is a \"textual product description, product image\" pair, and the outputs are the product attributes (there may be more than one attribute in the descriptions) and the corresponding values. We model the product attribute prediction task as a sequence-level multilabel classification task and the value extraction task as a sequence labeling task.\n\nThe framework of our proposed Multimodal Joint Attribute Prediction and Value Extraction model (M-JAVE) is shown in Figure 2. The input sentence is encoded by a pretrained BERT model (Devlin et al., 2019), and the image is encoded by a pretrained ResNet model (He et al., 2016). The global-gated cross-modality attention layer encodes text and image into the multimodal hidden representations. Then, the M-JAVE model predicts the product attributes based on the multimodal representations. Next, the model extracts the values based on the previously predicted product attributes and the multimodal representations obtained through the regional-gated cross-modality attention layer. We apply the multitask learning framework to jointly model the product attribute prediction and value extraction. Considering the constraints between the product attributes and values, we adopt a KL loss to penalize the inconsistency between the distribution of the product attribute prediction and that of the value extraction.\n\n\nText Encoder\n\nThe text embedding vectors are encoded by a BERT-base model, which uses a concatenation of WordPiece (Wu et al., 2016) embeddings, positional embeddings, and segment embeddings as the input representation. In addition, a special classification embedding ([CLS]) is inserted as the first token, and a special token ([SEP ]) is added as the final token. Given a textual product description sentence decorated with two special tokens x = ([CLS], x 1 , ..., x N , [SEP ]), BERT outputs an embedding sequence h = (h 0 , h 1 , ..., h N , h N +1 ).\n\n\nImage Encoder\n\nWe apply the ResNet (He et al., 2016) to encode the product images. We extract the activations from the last pooling layer of ResNet-101 that is pretrained on the ImageNet (Deng et al., 2009) as the global visual feature v G . We use the 7 \u00d7 7 \u00d7 2048 feature map of the conv 5 layer as the regional image feature v = (v 1 , ..., v K ), where K = 49.\n\n\nGlobal-Gated Cross-Modality Attention Layer\n\nIntuitively, for a specific product, as different modalities are semantically pertinent, we apply a cross-modality attention module to incorporate the textual and visual semantics into the multimodal hidden representations. Inspired by the self-attention mechanism (Vaswani et al., 2017), we build a cross-modality attention layer capable of directly associating source tokens at different positions of the sentence and different regions of the image, by computing the attention score between each token-token pair and token-region pair, respectively. We argue that what is crucial to the cross-modality attention layer is the ability to selectively enrich the semantic representation of a sentence through the aid of an image. In other words, we need to avoid introducing noises resulted from when the image fails to represent some semantic meaning of words, such as abstract concepts. To achieve this, we design a global visual gate to filter out visual noise for any words that are irrelevant based on the visual signals.\n\nSpecifically, we feed the text and image representations h i and v k into the global-gated crossmodality attention layer, and then we obtain the enhanced multimodal representation h i as follows:\ne t ij = (W t Q h i )(W t K h j ) T / \u221a d (1) \u03b1 t ij = exp(e t ij )/ m exp(e t im )(2)e v ik = (W v Q h i )(W v K v k ) T / \u221a d (3) \u03b1 v ik = exp(e v ik )/ n exp(e v in ) (4) h i = j \u03b1 t ij W t V h j + g G i k \u03b1 v ik W v V v k (5) where W t Q , W t K , W t V , W v Q , W v K , W v V are weight matrices, and d is the dimension of W t Q h i .\nThe global visual gate g G i is determined by the representation of the sentence and the image, which are obtained by the text encoder and the image encoder, respectively, as follows:\ng G i = \u03c3(W 1 h i + W 2 v G + b)(6)\nwhere W 1 and W 2 are weight matrices.\n\n\nProduct Attribute Prediction\n\nFor an instance in the dataset, given y a = (y a 1 , ..., y a L ), where y a l = 1 denotes the instance with l-th attribute label, to predict the product attributes, we feed the text representation h i , the multimodal representation h i , and h 0 perceptron (the special classification element [CLS] in BERT) into a feed-forward layer to output the predicted attribute labels\u0177 a = (\u0177 1 a , ...,\u0177 L a ):\ny a = \u03c3(W 3 i h i + W 4 i h i + W 5 h 0 ) (7)\nwhere W 3 , W 4 and W 5 are weight matrices. Then we calculate the loss of the attribute prediction task by binary cross entropy over all L labels:\nLoss a = CrossEntropy(y a ,\u0177 a )(8)\n\nProduct Value Extraction\n\nWe regard the value extraction as a sequence labeling task that tags the input x = (x 1 , ..., x N ) with the label sequence y v = (y v 1 , ..., y v N ) in the BIO format, e.g., attribute label \"Material\" corresponds to tags \"B-Material\" and \"I-Material\".\n\nWe argue that the product attributes can provide crucial indications for the attribute values. For example, given a sentence \"The red collar and golden buttons in the shirt form a colorful fashion topic\" and the predicted product attribute \"Color\", it is easy to recognize the value \"golden\" corresponding to attribute \"Color\" instead of \"Material\". Thus, we incorporate the result of the product attribute prediction\u0177 a to improve the value extraction.\n\nMoreover, for a given product attribute, some regions of the image corresponding are more important than others. Thus, we set a gate g R k for each image region to obtain a weighted visual semantic representation, which aims to use the regional image information more efficiently. Specifically, we feed text representation h i , multimodal representation h i , and image representation v k into a regional-gated cross-modality attention layer and output the value labels\u0177 v = (\u0177 1 v , ...,\u0177 N v ):\ny v i = softmax(W 6 h i + W 7 h i + W 8\u0177 a + k g R k \u03b1 v ik W v V v k ) (9)\nwhere W 6 , W 7 , W 8 , and W v V are weight matrices. The regional visual gate g R k is determined by the regional visual semantics and the product attributes as follows:\ng R k = \u03c3(W 9\u0177 a + W 10 v k )(10)\nwhere W 9 and W 10 are weight matrices. Then we calculate the loss of the value extraction task by cross entropy:\nLoss v = CrossEntropy(y v ,\u0177 v )(11)\n\nMultitask Learning\n\nTo jointly model product attribute prediction and value extraction, our method is trained end-to-end via minimizing Loss a and Loss v coordinatively. Moreover, the outputs of attribute prediction and value extraction are highly correlated, and thus we adopt a KL constraint between the outputs. Given the l-th attribute label, we assume that there are two corresponding value extraction tags e.g., attribute label \"Material\" corresponds to tags \"B-Material\" and \"I-Material\", and their probabilities can be expressed as y v (B l ) and y v (I l ). Then the attribute prediction distribution mapped from the output of the corresponding value extraction task can be as-\nsigned as\u0177 v\u2192a = (\u0177 v\u2192a 1 , ...,\u0177 v\u2192a L ), wher\u00ea y v\u2192a l = 1 2 (max i\u0177 v i (B l ) + max i\u0177 v i (I l )) (12)\nThe KL loss is:\nKL(\u0177 a ||\u0177 v\u2192a ) = l\u0177 a l log\u0177 a l y v\u2192a l(13)\nand the final joint loss function is\n\nLoss =Loss a + Loss v + \u03bbKL(\u0177 a ||\u0177 v\u2192a ) (14)\n\n\nDataset\n\nWe collect a Multimodal E-commerce Product Attribute Value Extraction (MEPAVE) dataset with textual product descriptions and product images. Specifically, we collect instances from a mainstream Chinese e-commerce platform 2 . Crowdsourcing annotators are well-experienced in the area of e-commerce. Given a sentence, they are required to annotate the position of values mentioned in the sentence and label the corresponding attributes. In addition, the annotators also need to check the validity of the product text-image from its main page in e-commerce websites, and the unqualified ones will be removed. We randomly select 1,000 instances to be annotated three times to ensure annotation consistency; the consistency rate is 92.83%. Finally, we obtained 87,194 textimage instances consisting of the following categories of products: Clothes, Pants, Dresses, Shoes, Boots, Luggage, and Bags, and involving 26 types of product attributes such as \"Material\", \"Collar Type\", \"Color\", etc. The distribution of different product categories and attribute values is shown in Table 1. We randomly split all the instances into a training set with 71,194 instances, a validation set with 8,000 instances, and a testing set with 8,000 instances. IV et al. (2017) release the English Multimodal Attribute Extraction (MAE) dataset. Each instance in the MAE dataset contains a textual product description, a collection of images, and attributevalue pairs, where the values are not constrained to present in the textual product description. To verify our model on the MAE dataset, we select the instances whose values are in the textual product description, and we label the values by exactly matching. We denote this subset of the MAE dataset as MAE-text and the rest as MAE-image (values can be only inferred by the images).\n\n\nExperiment\n\nWe compare our proposed methods with the following baselines: WSM is the method that uses attribute values in the training set to retrieve the attribute values in the testing set by word matching. Sep-BERT is the pretrained BERT model with feed-forward layers to perform these two subtasks separately.    we adopt these models to our task. RNN-LSTM and Attn-BiRNN use a bidirectional LSTM and an attention-based model for joint learning, respectively. Slot-Gated introduces a gate-based mechanism to learn the relationship between these two tasks. Joint-BERT finetunes the BERT model with joint learning. ScalingUp (Xu et al., 2019) adopts BiLSTM, CRF, and attention mechanism for introducing hidden semantic interaction between attribute and text. We report the results of our text-only and multimodal models, i.e., JAVE and M-JAVE. In addition, to eliminate the influences of different text encoders, we also conduct experiments with BiLSTM as the text encoder. Details about hyper-parameters are shown in Table 2.\n\n\nMain Results\n\nWe evaluate our model on two subtasks, including attribute prediction and value extraction. The main results in Table 3 show that our proposed M-JAVE model based on the BERT and the Bidirectional LSTM (BiLSTM) both outperform the baselines significantly (paired t-test, p-value < 0.01), which proves an excellent generalization ability of our methods. From the results of our proposed M-JAVE and JAVE models, we can observe that the BERT is advantageous over the LSTM and visual product information improves the performance. The M-JAVE model achieves the best performance of 90.69% and 87.17% F 1 scores on two subtasks. Moreover, experimental results demonstrate the superiority of our JAVE model (either based on LSTM or BERT) against the models of WSM, RNN-LSTM, Sep-BERT, and jointlearning based models including Attn-BiRNN, Slot-Gated and Joint-BERT, indicating that the strategies for integrating the relationship between attributes and values into our models are necessary for the tasks. We evaluate the ScalingUp model to predict the value for each given attribute on our dataset, and the result is unsatisfactory. With the in-depth study, we found that it can be ascribed to identifying values that do not correspond to the given attribute. Over 34.52% of the predicted values are not the actual values for the input attributes, whereas this number is only 16.51% for our JAVE model. As a result, the ScalingUp model obtains a higher recall score (93.78%) while a lower precision score of (65.48%) than our model (89.82% for recall score and 80.27% for the precision score). We argue that explicitly modeling the relationship between attributes and values facilitates our methods to establish the correspondence between them.\n\nMore details including the results for each product category and for each type of attribute are shown in Figure 3 and 4. We can find that our proposed method achieves satisfactory results for every category, and is not only suitable for simple attributes related to appearance, such as \"Color\" and \"Pant Length\", but also can deal with complex attributes, such as \"Elasticity\" and \"Material\".\n\nTo verify the adaptability of our proposed models, we conduct experiments on the English MAE dataset (IV et al., 2017). The model proposed along with the MAE dataset takes textual product descriptions, visual information, and product attributes as input and treats the attribute value extraction task   as predicting the value for a given product attribute. Thus, we compare our M-JAVE model with the MAE-model only on the value extraction task. As shown in Table 4, on the MAE-text subset, our M-JAVE (LSTM) and M-JAVE (BERT) models outperform the MAE-model with 1.45% and 2.05% accuracy gains, respectively. On the original MAE and MAE-image subset, the accuracy scores of the MAE-model are 59.48% and 52.11%, respectively, which are much lower than that on the MAE-text subset. We argue that it may be risky to predict the product values that do not appear in the textual product descriptions, and defining the value prediction as an extractive-based task is more reasonable for practical applications.\n\n\nAblation Study\n\nWe perform ablation studies to confirm the effectiveness of the main modules of our models.\n\n\nModeling the Relationship between Product Attributes and Values\n\nWe explore the relationship between attributes and values from three aspects, including 1) applying the multitask learning to jointly predict the product attributes and values, 2) extracting values based on the predicted product attributes, and 3) introducing a KL loss to penalize the inconsistency between the results of product attributes and values. Based on our text-only model, i.e., JAVE, we conduct experiments to evaluate the effectiveness of modeling the relationship by ablating the modules corresponding to the above three aspects.\n\n\u2022 w/o MTL is the model without multitask learn-   ing (i.e., the two subtasks are addressed separately).\n\n\u2022 w/o AttrPred is the model without using the predicted product attributes in value extraction (i.e., remove W 8\u0177 a in Eq. 9).\n\n\u2022 w/o KL loss is the model without the KL loss (i.e., set \u03bb = 0 in Eq. 14).\n\nFurthermore, we get the upper bound of attribute prediction training with the ground-truth values (Eq. 13); we get the upper bound of value extraction training with the ground-truth attributes (Eq. 9 and 13). Table 5 shows a comparison of the JAVE model concerning the ablations. We can see that the JAVE model achieves the best performance. The results of the method \"JAVE w/o MTL\", \"JAVE w/o Attr-Pred\", and \"JAVE w/o KL loss\" drop the F 1 scores by 0.62%, 1.24%, and 0.74% respectively for product attribute prediction, and drop the F 1 scores by 0.79%, 0.88% and 0.52% respectively for value extraction, showing the effectiveness of modeling the relationship between product attributes and values. The results for the upper bound study shows the strong correlation between product attribute prediction and value extraction.\n\n\nIntegrating Visual Product Information\n\nOur model mainly utilizes visual information of products from two aspects, including 1) predicting product attributes with a global-gated cross-modality attention module, and 2) extracting values with a regional-gated cross-modality attention module. We evaluate the effectiveness of visual product information as follows.\n\n\u2022 w/o Visual Info is the model without utilizing visual information (i.e., JAVE).\n\n\u2022 w/o Global-Gated CrossMAtt is the model without the global-gated cross-modality attention (i.e., remove the right part in Eq. 5).\n\n\u2022 w/o Regional-Gated CrossMAtt is the model without the regional-gated cross-modality attention (i.e., remove the right-most part in Eq. 9 inside the softmax function).\n\n\u2022 w/o Global Visual Gate is the model without the global visual gate (i.e., remove g G i in Eq. 5).\n\n\u2022 w/o Regional Visual Gate is the model without the regional visual gate, (i.e., remove g R k in Eq. 9).\n\nFrom Table 6, we can see that removing globalgated or regional-gated cross-modality attention modules degrades the performances on both subtasks, proving the effectiveness of visual information for our task.\n\nMoreover, for the models with cross-modality attention modules while without global or regional visual gates, i.e., M-JAVE w/o Global Visual Gate and M-JAVE w/o Regional Visual Gate, respectively, the performances are worse than that of M-JAVE significantly. Remarkably, the models of M-JAVE w/o Global Visual Gate and M-JAVE w/o Regional Visual Gate underperform the models thoroughly removing visual-related modules.\n\nTo sum up, using the visual product information indiscriminately poses detrimental effects on the model, and selectively utilizing visual product information with global and regional visual gates are essential for our tasks. Further experiment about the visual information is in the Appendices.\n\n\nAdversarial Evaluation of Attribute Prediction and Value Extraction\n\nTo further verify whether the visual product information can improve the performance of product attribute prediction and value extraction, we adopt an adversarial evaluation method (Elliott, 2018) that measures the performance variation when our model is presented with a random incongruent image.\n\nThe awareness score of a model M on an evaluation dataset D is defined as follows:\n\u2206 Awareness = 1 |D| |D| i a M (x i , y i , v i ,v i ) (15)\nWhere \u2206 Awareness denotes the image awareness. x, y denote the the text and the values of the product, respectively. v,v denote the congruent image and the incongruent image, respectively. We use the F 1 score to calculate awareness score for a single instance:\na M = F 1 (x i , y i , v i ) \u2212 F 1 (x i , y i ,v i )(16)\nUnder this definition, the output of the evaluation performance measure should be higher in the presence of the congruent data than incongruent data, i.e.,\nF 1 (x i , y i , v i ) > F 1 (x i , y i ,v i ).\nIf this is the case, on average, then the overall image awareness of a model \u2206 Awareness is positive. This can only happen when model outputs are evaluated more favourably in the presence of the congruent image data than the incongruent image data.\n\nTo determine if a model passes the proposed evaluation, we conduct the statistical test using the pairs of values that are calculated in the process of computing the image awareness scores (Eq. 16) Table 7 shows the evaluation results of product attribute prediction and value extraction. We find that, on both subtasks, the F 1 scores with incongruent images are much lower than that with the congruent images, and \u2206 Awareness is significant positive. Moreover, we use K = 8 separate p values from each test based on Fisher's method, and get X 2 =6790.80, p <0.0001 in product attribute prediction and X 2 =780.80, p <0.0001 in value extraction, which proves that the incongruent image significantly degrades the model's performance. We can conclude that the visual information make substantial contribution to the attribute prediction and value extraction tasks.   \n\n\nDomain Adaptation\n\nTo verify the robustness of our models, we conduct an evaluation on the out-of-domain data. The source domain is our formal product information (PI) mentioned in Section 3. The target domain is the oral Question-Answering (QA), where the textual description consists of QA pairs about the product in the real e-commerce customer service dialogue, and the visual information is from the image of product mentioned in the dialogue. We directly apply the JAVE and M-JAVE models trained on PI to test on the QA testing set containing 900 manually annotated instances. Table 8, on the QA testing set, M-JAVE outperforms JAVE with 4.31% and 5.70% F 1 scores on the attribute prediction and value extraction tasks, respectively. For the attribute prediction task, the gap between the results on the PI and QA testing reduces from 14.58% to 12.98% when using the visual information. Similarly, the gap reduces from 14.31% to 11.00% for the value extraction task. This demonstrates that visual product information makes our model more robust.  Table 9: Results (mean and standard deviation) with different sizes of of training data.\n\n\nAs shown in\n\u9ed1 \u8272 \u7a33 \u91cd \u5927 \u6c14 \u4e0e \u9ad8 \u5e2e \u8bbe \u8ba1 \u66f4 \u597d \u52a0 \u6301 \u5229 \u843d \u7684 \u5c0f \u7ffb \u9886 \u642d \u914d \u751c \u7f8e \u649e \u8272 \u7ed1 \u5e26\nneat little lapel with a sweet contrast band the stable black matches the high-top design very well Figure 5: Heat maps for global (blocks above the text) and regional (images on the right) visual gates.\n\n\nLow-Resource Evaluation\n\nTo further verify the robustness of our model, we evaluate our models trained with subsets of the whole training set in different proportions. For each proportion, we randomly sample the training data three times, and we report the mean and standard deviation in Table 9. It illustrates that visual product information brings considerable advantages on the robustness when few training instances are available.\n\n\nVisualization\n\nTo evaluate the global and regional visual gates qualitatively, we visualize these gates for different attribute values with the M-JAVE model. The results are shown in Figure 5. For the blocks above the text, the deeper color denotes the larger value for the global visual gate g G i , i.e., more visual information is used for enhancing the semantic meaning of the text. We can find that the global visual gates are positively related to the relevance between the text and the image. For the product image on the right, the lighter color denotes the larger value for the regional visual gate g R k , i.e., more visual information is drawn for extracting values. The results demonstrate that the regional visual gate successfully captures useful parts of the product image.\n\n\nRelated Work\n\nRecent approaches related to the attribute value pair completion task can be classified as the following two categories.\n\n1) Predicting integral attribute-value tags. Putthividhya and Hu (2011) and Zheng et al. (2018) introduce a set of entity tags for each attribute (e.g., \"B-Material\" and \"I-Material\" for the attribute \"Material\"). Putthividhya and Hu (2011) adopt a NER system with bootstrapping to predict values, and Zheng et al. (2018) apply a Bi-LSTM-CRF model with the attention mechanism. It may be challenging to handle the massive amounts of attributes in the real world.\n\n2) Predicting values for given attributes. Ghani et al. (2006) treat the task as a value classification task and create a specific text classifier for each given attribute. More (2016) and Xu et al. (2019) formulate the task as a special case of NER (Bikel et al., 1999;Collobert et al., 2011) task that predicts the values for each attribute. More (2016) combines CRF and structured perceptron with a curated normalization scheme to predict values, and Xu et al. (2019) regard attributes as queries and adopt BIO tags for any attributes, making it applicable for the large-scaled attribute system. However, our experimental results show that the model of Xu et al. (2019) may be insufficient to identify which attribute a value corresponds to.\n\nIn this paper, we propose a third category of method: jointly predicting attributes and extracting values. The attribute prediction module provides guidance and constraints for the value extraction module, which adapts our model to fit large-scaled attribute applications. Moreover, we explicitly model the relationship between attributes and values, which helps to establish the correspondence between them effectively.\n\n\nConclusion\n\nWe jointly tackle the tasks of e-commerce product attribute prediction and value extraction from multiple aspects towards the relationship between product attributes and values, and we prove that the models can benefit a lot from visual product information. The experimental results show that the correlations between product attributes and values are valuable for this task, and visual information should be selectively used.\n\nFigure 1 :\n1An example of predicting attributes and extracting values from the textual product description with the aid of the visual product information.\n\nFigure 2 :\n2Framework of our model.\n\n\nRNN-LSTM (Hakkani-T\u00fcr et al.,  2016), Attn-BiRNN(Liu and Lane, 2016),Slot- Gated (Goo et al., 2018), and Joint-BERT(Chen et al., 2019) are the models to address intent classification and slot filling tasks, which are similar to the attribute prediction and value extraction, and\n\nFigure 3 :\n3Experimental results of the M-JAVE model for each product category.\n\nFigure 4 :\n4Experimental results of the M-JAVE model for each type of attribute.\n\n\nTable 1: Statistics of the our dataset.Category #Product #Instance #Attr #Value \nClothes \n12,240 \n34,154 \n14 \n1,210 \nShoes \n9,022 \n20,525 \n10 \n1,036 \nBags \n3,376 \n8,307 \n8 \n631 \nLuggage \n1,291 \n2,227 \n7 \n275 \nDresses \n4,567 \n12,283 \n13 \n714 \nBoots \n713 \n2,090 \n11 \n322 \nPants \n2,832 \n7,608 \n13 \n595 \nTotal \n34,041 \n87,194 \n26 \n2,129 \n\n\n\nTable 2 :\n2Details about hyper-parameters.Model \nAttribute Value \nWSM \n77.20 \n72.52 \nSep-BERT \n86.34 \n83.12 \nRNN-LSTM (Hakkani-T\u00fcr et al., 2016) \n85.76 \n82.92 \nAttn-BiRNN (Liu and Lane, 2016) \n86.10 \n83.28 \nSlot-Gated (Goo et al., 2018) \n86.70 \n83.35 \nJoint-BERT (Chen et al., 2019) \n86.93 \n83.73 \nScalingUp (Xu et al., 2019) \n-\n77.12 \nJAVE (LSTM based) \n87.88 \n84.09 \nJAVE (BERT based) \n87.98 \n84.78 \nM-JAVE (LSTM based) \n90.19 \n86.41 \nM-JAVE (BERT based) \n90.69 \n87.17 \n\n\n\nTable 3 :\n3Main results (F 1 score %) of comparative methods and variants of our model.\n\nTable 4 :\n4Experimental results (accuracy %) of our proposed model and MAE baseline model (MAE-model).Model \nAttribute Value \nJAVE \n87.98 \n84.78 \nJAVE w/o MTL \n87.36 \n83.99 \nJAVE w/o AttrPred \n86.74 \n83.90 \nJAVE w/o KL-Loss \n87.24 \n84.26 \nJAVE (UpBound of Attribute Task) \n89.03 \n100.0 \nJAVE (UpBound of Value Task) \n100.0 \n88.72 \n\n\n\nTable 5 :\n5Experimental results (F 1 score %) for ablation \nstudy on the relationship between attributes and values. \n\"UpBound\" denotes \"Upper Bound\". \n\n\n\nTable 6 :\n6Experimental results (F 1 score %) for ablation \nstudy on the product images. \n\n\n\nTable 7 :\n7F 1 scores in the Congruent and Incongruent settings, along with the Meteor-awareness results. Incongruent and \u2206 Awareness scores are the mean and standard deviation of 8 permutations of product images in test dataset. JAVE 90.69 77.71 12.98 87.17 76.17 11.00Attribute \nValue \nModels \nPI \nQA \n\u2206 \u2193 \nPI \nQA \n\u2206 \u2193 \nJAVE \n87.98 73.40 14.58 84.78 70.47 14.31 \nM-\n\nTable 8 :\n8Experimental results (F 1 score %) for domain adaptation. \u2206 \u2193 denotes the F 1 score gap for the PI and QA domains.\nhttps://www.jd.com/\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In 2018 IEEE Conference on Computer Vision and Pat- tern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6077-6086.\n\nAn algorithm that learns what's in a name. M Daniel, Richard M Bikel, Ralph M Schwartz, Weischedel, 10.1023/A:1007558221122Mach. Learn. 341-3Daniel M. Bikel, Richard M. Schwartz, and Ralph M. Weischedel. 1999. An algorithm that learns what's in a name. Mach. Learn., 34(1-3):211-231.\n\nA novel hybrid collaborative filtering approach to recommendation using reviews: The product attribute perspective (S). Min Cao, Sijing Zhou, Honghao Gao, Youhuizi Li, 10.18293/SEKE2018-050The 30th International Conference on Software Engineering and Knowledge Engineering. Redwood City, California, USAMin Cao, Sijing Zhou, Honghao Gao, and Youhuizi Li. 2018. A novel hybrid collaborative filtering ap- proach to recommendation using reviews: The prod- uct attribute perspective (S). In The 30th Inter- national Conference on Software Engineering and Knowledge Engineering, Hotel Pullman, Redwood City, California, USA, July 1-3, 2018, pages 7-10.\n\nMultitask learning. Rich Caruana, https:/link.springer.com/article/10.1023/A:1007379606734Machine learning. 281Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41-75.\n\nBERT for joint intent classification and slot filling. Qian Chen, Zhu Zhuo, Wen Wang, abs/1902.10909CoRR. Qian Chen, Zhu Zhuo, and Wen Wang. 2019. BERT for joint intent classification and slot filling. CoRR, abs/1902.10909.\n\nNatural language processing (almost) from scratch. Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel P Kuksa, J. Mach. Learn. Res. 12Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493-2537.\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Fei-Fei Li, 10.1109/CVPR.2009.5206848IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Miami, Florida, USAJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A large-scale hi- erarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248-255.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. the 2019 Conference of the North American Chapter of the Association for Computational LinguisticsMinnesotaMinneapolisJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171-4186, Min- neapolis, Minnesota.\n\nAdversarial evaluation of multimodal machine translation. Desmond Elliott, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumDesmond Elliott. 2018. Adversarial evaluation of mul- timodal machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 -November 4, 2018, pages 2974-2978.\n\nText mining for product attribute extraction. Rayid Ghani, Katharina Probst, Yan Liu, Marko Krema, Andrew E Fano, 10.1145/1147234.1147241SIGKDD Explorations. 81Rayid Ghani, Katharina Probst, Yan Liu, Marko Krema, and Andrew E. Fano. 2006. Text mining for prod- uct attribute extraction. SIGKDD Explorations, 8(1):41-48.\n\nEmploying user attribute and item attribute to enhance the collaborative filtering recommendation. Songjie Gong, 10.4304/jsw.4.8.883-890JSW4SongJie Gong. 2009. Employing user attribute and item attribute to enhance the collaborative filtering recommendation. JSW, 4(8):883-890.\n\nSlot-gated modeling for joint slot filling and intent prediction. Guang Chih-Wen Goo, Yun-Kai Gao, Chih-Li Hsu, Tsung-Chieh Huo, Keng-Wei Chen, Yun-Nung Hsu, Chen, 10.18653/v1/N18-2118Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesShort Papers; New Orleans, Louisiana2Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun- Nung Chen. 2018. Slot-gated modeling for joint slot filling and intent prediction. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Pa- pers), pages 753-757, New Orleans, Louisiana.\n\nMulti-domain joint semantic frame parsing using bi-directional RNN-LSTM. Dilek Hakkani-T\u00fcr, G\u00f6khan T\u00fcr, \u00c7 Asli, Yun-Nung Elikyilmaz, Jianfeng Chen, Li Gao, Ye-Yi Deng, Wang, 10.21437/Interspeech.2016-402Interspeech 2016, 17th Annual Conference of the International Speech Communication Association. San Francisco, CA, USADilek Hakkani-T\u00fcr, G\u00f6khan T\u00fcr, Asli \u00c7 elikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM. In Inter- speech 2016, 17th Annual Conference of the Inter- national Speech Communication Association, San Francisco, CA, USA, September 8-12, 2016, pages 715-719.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In 2016 IEEE Conference on Computer Vi- sion and Pattern Recognition, CVPR 2016, Las Ve- gas, NV, USA, June 27-30, 2016, pages 770-778.\n\nMultimodal attribute extraction. L Robert, I V Logan, Samuel Humeau, Sameer Singh, 6th Workshop on Automated Knowledge Base Construction. Long Beach, California, USARobert L. Logan IV, Samuel Humeau, and Sameer Singh. 2017. Multimodal attribute extraction. In 6th Workshop on Automated Knowledge Base Con- struction, AKBC@NIPS 2017, Long Beach, Califor- nia, USA, December 8, 2017.\n\nOn information and sufficiency. The annals of mathematical statistics. Solomon Kullback, A Richard, Leibler, 22Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathe- matical statistics, 22(1):79-86.\n\nAspect-aware multimodal summarization for chinese e-commerce products. Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xiaodong He, Bowen Zhou, The Thirty-Fourth AAAI Conference on Artificial Intelligence. 2020Haoran Li, Peng Yuan, Song Xu, Youzheng Wu, Xi- aodong He, and Bowen Zhou. 2020. Aspect-aware multimodal summarization for chinese e-commerce products. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, pages 8188- 8195.\n\nMulti-modal sentence summarization with modality attention and image filtering. Haoran Li, Junnan Zhu, Tianshang Liu, Jiajun Zhang, Chengqing Zong, 10.24963/ijcai.2018/577Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018. the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018Stockholm, SwedenHaoran Li, Junnan Zhu, Tianshang Liu, Jiajun Zhang, and Chengqing Zong. 2018. Multi-modal sentence summarization with modality attention and image fil- tering. In Proceedings of the Twenty-Seventh Inter- national Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 4152-4158.\n\nMulti-modal summarization for asynchronous collection of text, image, audio and video. Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, Chengqing Zong, 10.18653/v1/D17-1114Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkHaoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing Zong. 2017. Multi-modal summariza- tion for asynchronous collection of text, image, au- dio and video. In Proceedings of the 2017 Confer- ence on Empirical Methods in Natural Language Processing, pages 1092-1102, Copenhagen, Den- mark.\n\nRead, watch, listen, and summarize: Multi-modal summarization for asynchronous text, image, audio and video. Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, Chengqing Zong, 10.1109/TKDE.2018.2848260IEEE Transactions on Knowledge and Data Engineering. 315Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing Zong. 2019. Read, watch, listen, and summarize: Multi-modal summarization for asyn- chronous text, image, audio and video. IEEE Transactions on Knowledge and Data Engineering, 31(5):996-1009.\n\nInterpretable multimodal retrieval for fashion products. Lizi Liao, Xiangnan He, Bo Zhao, Chong-Wah Ngo, Tat-Seng Chua, 10.1145/3240508.3240646ACM Multimedia Conference on Multimedia Conference, MM 2018. Seoul, Republic of KoreaLizi Liao, Xiangnan He, Bo Zhao, Chong-Wah Ngo, and Tat-Seng Chua. 2018. Interpretable multimodal retrieval for fashion products. In 2018 ACM Multi- media Conference on Multimedia Conference, MM 2018, Seoul, Republic of Korea, October 22-26, 2018, pages 1571-1579.\n\nAttention-based recurrent neural network models for joint intent detection and slot filling. Bing Liu, Ian Lane, 10.21437/Interspeech.2016-1352Interspeech 2016, 17th Annual Conference of the International Speech Communication Association. San Francisco, CA, USABing Liu and Ian Lane. 2016. Attention-based recur- rent neural network models for joint intent detection and slot filling. In Interspeech 2016, 17th Annual Conference of the International Speech Communica- tion Association, San Francisco, CA, USA, Septem- ber 8-12, 2016, pages 685-689.\n\nGraph convolution for multimodal information extraction from visually rich documents. Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao, 10.18653/v1/N19-2005Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesota2Industry Papers. MinneapolisXiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019. Graph convolution for multimodal in- formation extraction from visually rich documents. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 2 (Industry Papers), pages 32-39, Minneapo- lis, Minnesota.\n\nHierarchical question-image co-attention for visual question answering. Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016. Barcelona, SpainJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical question-image co-attention for visual question answering. In Advances in Neu- ral Information Processing Systems 29: Annual Conference on Neural Information Processing Sys- tems 2016, December 5-10, 2016, Barcelona, Spain, pages 289-297.\n\nNeural product retrieval at walmart.com. Alessandro Magnani, Feng Liu, Min Xie, Somnath Banerjee, 10.1145/3308560.3316603Companion of The 2019 World Wide Web Conference. San Francisco, CA, USAAlessandro Magnani, Feng Liu, Min Xie, and Som- nath Banerjee. 2019. Neural product retrieval at wal- mart.com. In Companion of The 2019 World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pages 367-372.\n\nAttribute extraction from product titles in ecommerce. Ajinkya More, abs/1608.04670CoRRAjinkya More. 2016. Attribute extraction from product titles in ecommerce. CoRR, abs/1608.04670.\n\nBootstrapped named entity recognition for product attribute extraction. Duangmanee Putthividhya, Junling Hu, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UKDuangmanee Putthividhya and Junling Hu. 2011. Boot- strapped named entity recognition for product at- tribute extraction. In Proceedings of the 2011 Con- ference on Empirical Methods in Natural Language Processing, pages 1557-1567, Edinburgh, Scotland, UK.\n\nUnsupervised extraction of attributes and their values from product description. Keiji Shinzato, Satoshi Sekine, Proceedings of the Sixth International Joint Conference on Natural Language Processing. the Sixth International Joint Conference on Natural Language ProcessingNagoya, JapanKeiji Shinzato and Satoshi Sekine. 2013. Unsupervised extraction of attributes and their values from prod- uct description. In Proceedings of the Sixth Interna- tional Joint Conference on Natural Language Pro- cessing, pages 1339-1347, Nagoya, Japan.\n\nVL-BERT: pretraining of generic visual-linguistic representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: pre- training of generic visual-linguistic representations. In 8th International Conference on Learning Repre- sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n\nLXMERT: Learning cross-modality encoder representations from transformers. Hao Tan, Mohit Bansal, 10.18653/v1/D19-1514Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaHao Tan and Mohit Bansal. 2019. LXMERT: Learning cross-modality encoder representations from trans- formers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5100-5111, Hong Kong, China.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 Decem- ber 2017, Long Beach, CA, USA, pages 5998-6008.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, abs/1609.08144Oriol Vinyals. CoRRYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John- son, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rud- nick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144.\n\nScaling up open tagging from tens to thousands: Comprehension empowered attribute value extraction from product title. Huimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, Man Lan, 10.18653/v1/P19-1514Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyHuimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, and Man Lan. 2019. Scaling up open tagging from tens to thousands: Comprehension empowered attribute value extraction from product title. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5214-5223, Flo- rence, Italy.\n\nSemantic parsing via staged query graph generation: Question answering with knowledge base. Ming-Wei Wen-Tau Yih, Xiaodong Chang, Jianfeng He, Gao, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaLong Papers1Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin- guistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321-1331, Beijing, China.\n\nImproved neural relation detection for knowledge base question answering. Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Bing Cicero Dos Santos, Bowen Xiang, Zhou, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Mo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2017. Im- proved neural relation detection for knowledge base question answering. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 571- 581, Vancouver, Canada.\n\nDeep modular co-attention networks for visual question answering. Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019. Long Beach, CA, USAZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019. Deep modular co-attention networks for visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6281-6290.\n\nOpentag: Open attribute value extraction from product profiles. Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, Feifei Li, 10.1145/3219819.3219839Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningLondon, UKGuineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, and Feifei Li. 2018. Opentag: Open attribute value extraction from product profiles. In Proceed- ings of the 24th ACM SIGKDD International Confer- ence on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018, pages 1049- 1058.\n", "annotations": {"author": "[{\"end\":118,\"start\":85},{\"end\":146,\"start\":119},{\"end\":175,\"start\":147},{\"end\":207,\"start\":176},{\"end\":239,\"start\":208},{\"end\":271,\"start\":240},{\"end\":285,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":94},{\"end\":127,\"start\":123},{\"end\":156,\"start\":154},{\"end\":187,\"start\":185},{\"end\":219,\"start\":217},{\"end\":252,\"start\":248},{\"end\":284,\"start\":276}]", "author_first_name": "[{\"end\":93,\"start\":85},{\"end\":122,\"start\":119},{\"end\":153,\"start\":147},{\"end\":184,\"start\":176},{\"end\":216,\"start\":208},{\"end\":247,\"start\":242},{\"end\":273,\"start\":272},{\"end\":275,\"start\":274}]", "author_affiliation": null, "title": "[{\"end\":82,\"start\":1},{\"end\":367,\"start\":286}]", "venue": null, "abstract": "[{\"end\":1767,\"start\":369}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2026,\"start\":2008},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2042,\"start\":2026},{\"end\":2074,\"start\":2068},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2099,\"start\":2082},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2142,\"start\":2123},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2163,\"start\":2142},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2920,\"start\":2893},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2931,\"start\":2920},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2957,\"start\":2931},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2976,\"start\":2957},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2992,\"start\":2976},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3716,\"start\":3701},{\"end\":3902,\"start\":3881},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3931,\"start\":3903},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4930,\"start\":4913},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4946,\"start\":4930},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4968,\"start\":4946},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4984,\"start\":4968},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5000,\"start\":4984},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5021,\"start\":5000},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5037,\"start\":5021},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6708,\"start\":6687},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6781,\"start\":6764},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7649,\"start\":7632},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8127,\"start\":8110},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8281,\"start\":8262},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8774,\"start\":8752},{\"end\":10641,\"start\":10636},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16060,\"start\":16043},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18709,\"start\":18692},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23608,\"start\":23593},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28353,\"start\":28327},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28377,\"start\":28358},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28603,\"start\":28584},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28808,\"start\":28789},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28930,\"start\":28919},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28951,\"start\":28935},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29016,\"start\":28996},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29039,\"start\":29016},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29216,\"start\":29200},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29418,\"start\":29402},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30617,\"start\":30597},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30683,\"start\":30664}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30509,\"start\":30354},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30546,\"start\":30510},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30827,\"start\":30547},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30908,\"start\":30828},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30990,\"start\":30909},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31328,\"start\":30991},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31803,\"start\":31329},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31892,\"start\":31804},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32226,\"start\":31893},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32381,\"start\":32227},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":32474,\"start\":32382},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":32843,\"start\":32475},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":32970,\"start\":32844}]", "paragraph": "[{\"end\":2808,\"start\":1783},{\"end\":3290,\"start\":2810},{\"end\":4068,\"start\":3360},{\"end\":5527,\"start\":4070},{\"end\":5566,\"start\":5529},{\"end\":5668,\"start\":5568},{\"end\":5830,\"start\":5670},{\"end\":5957,\"start\":5832},{\"end\":6502,\"start\":5970},{\"end\":7514,\"start\":6504},{\"end\":8072,\"start\":7531},{\"end\":8439,\"start\":8090},{\"end\":9511,\"start\":8487},{\"end\":9708,\"start\":9513},{\"end\":10233,\"start\":10050},{\"end\":10308,\"start\":10270},{\"end\":10744,\"start\":10341},{\"end\":10938,\"start\":10791},{\"end\":11257,\"start\":11002},{\"end\":11712,\"start\":11259},{\"end\":12211,\"start\":11714},{\"end\":12459,\"start\":12288},{\"end\":12607,\"start\":12494},{\"end\":13332,\"start\":12666},{\"end\":13456,\"start\":13441},{\"end\":13540,\"start\":13504},{\"end\":13588,\"start\":13542},{\"end\":15413,\"start\":13600},{\"end\":16444,\"start\":15428},{\"end\":18195,\"start\":16461},{\"end\":18589,\"start\":18197},{\"end\":19596,\"start\":18591},{\"end\":19706,\"start\":19615},{\"end\":20317,\"start\":19774},{\"end\":20423,\"start\":20319},{\"end\":20551,\"start\":20425},{\"end\":20628,\"start\":20553},{\"end\":21457,\"start\":20630},{\"end\":21822,\"start\":21500},{\"end\":21905,\"start\":21824},{\"end\":22038,\"start\":21907},{\"end\":22208,\"start\":22040},{\"end\":22309,\"start\":22210},{\"end\":22415,\"start\":22311},{\"end\":22624,\"start\":22417},{\"end\":23044,\"start\":22626},{\"end\":23340,\"start\":23046},{\"end\":23709,\"start\":23412},{\"end\":23793,\"start\":23711},{\"end\":24114,\"start\":23853},{\"end\":24327,\"start\":24172},{\"end\":24624,\"start\":24376},{\"end\":25493,\"start\":24626},{\"end\":26638,\"start\":25515},{\"end\":26914,\"start\":26711},{\"end\":27352,\"start\":26942},{\"end\":28143,\"start\":27370},{\"end\":28280,\"start\":28160},{\"end\":28744,\"start\":28282},{\"end\":29490,\"start\":28746},{\"end\":29912,\"start\":29492},{\"end\":30353,\"start\":29927}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9795,\"start\":9709},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10049,\"start\":9795},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10269,\"start\":10234},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10790,\"start\":10745},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10974,\"start\":10939},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12287,\"start\":12212},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12493,\"start\":12460},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12644,\"start\":12608},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13440,\"start\":13333},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13503,\"start\":13457},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23852,\"start\":23794},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24171,\"start\":24115},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24375,\"start\":24328},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26710,\"start\":26653}]", "table_ref": "[{\"end\":14677,\"start\":14670},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16443,\"start\":16436},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16580,\"start\":16573},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":19056,\"start\":19049},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":20846,\"start\":20839},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":22429,\"start\":22422},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":24831,\"start\":24824},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":26086,\"start\":26079},{\"end\":26557,\"start\":26550},{\"end\":27212,\"start\":27205}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1781,\"start\":1769},{\"end\":3297,\"start\":3293},{\"end\":3358,\"start\":3300},{\"attributes\":{\"n\":\"2.1\"},\"end\":5968,\"start\":5960},{\"attributes\":{\"n\":\"2.2\"},\"end\":7529,\"start\":7517},{\"attributes\":{\"n\":\"2.3\"},\"end\":8088,\"start\":8075},{\"attributes\":{\"n\":\"2.4\"},\"end\":8485,\"start\":8442},{\"attributes\":{\"n\":\"2.5\"},\"end\":10339,\"start\":10311},{\"attributes\":{\"n\":\"2.6\"},\"end\":11000,\"start\":10976},{\"attributes\":{\"n\":\"2.7\"},\"end\":12664,\"start\":12646},{\"attributes\":{\"n\":\"3\"},\"end\":13598,\"start\":13591},{\"attributes\":{\"n\":\"4\"},\"end\":15426,\"start\":15416},{\"attributes\":{\"n\":\"4.1\"},\"end\":16459,\"start\":16447},{\"attributes\":{\"n\":\"4.2\"},\"end\":19613,\"start\":19599},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":19772,\"start\":19709},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":21498,\"start\":21460},{\"attributes\":{\"n\":\"4.3\"},\"end\":23410,\"start\":23343},{\"attributes\":{\"n\":\"4.4\"},\"end\":25513,\"start\":25496},{\"end\":26652,\"start\":26641},{\"attributes\":{\"n\":\"4.5\"},\"end\":26940,\"start\":26917},{\"attributes\":{\"n\":\"4.6\"},\"end\":27368,\"start\":27355},{\"attributes\":{\"n\":\"5\"},\"end\":28158,\"start\":28146},{\"attributes\":{\"n\":\"6\"},\"end\":29925,\"start\":29915},{\"end\":30365,\"start\":30355},{\"end\":30521,\"start\":30511},{\"end\":30839,\"start\":30829},{\"end\":30920,\"start\":30910},{\"end\":31339,\"start\":31330},{\"end\":31814,\"start\":31805},{\"end\":31903,\"start\":31894},{\"end\":32237,\"start\":32228},{\"end\":32392,\"start\":32383},{\"end\":32485,\"start\":32476},{\"end\":32854,\"start\":32845}]", "table": "[{\"end\":31328,\"start\":31032},{\"end\":31803,\"start\":31372},{\"end\":32226,\"start\":31996},{\"end\":32381,\"start\":32239},{\"end\":32474,\"start\":32394},{\"end\":32843,\"start\":32746}]", "figure_caption": "[{\"end\":30509,\"start\":30367},{\"end\":30546,\"start\":30523},{\"end\":30827,\"start\":30549},{\"end\":30908,\"start\":30841},{\"end\":30990,\"start\":30922},{\"end\":31032,\"start\":30993},{\"end\":31372,\"start\":31341},{\"end\":31892,\"start\":31816},{\"end\":31996,\"start\":31905},{\"end\":32746,\"start\":32487},{\"end\":32970,\"start\":32856}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2807,\"start\":2799},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4233,\"start\":4225},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6628,\"start\":6620},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18310,\"start\":18302},{\"end\":26819,\"start\":26811},{\"end\":27546,\"start\":27538}]", "bib_author_first_name": "[{\"end\":33082,\"start\":33077},{\"end\":33101,\"start\":33093},{\"end\":33111,\"start\":33106},{\"end\":33127,\"start\":33121},{\"end\":33139,\"start\":33135},{\"end\":33156,\"start\":33149},{\"end\":33167,\"start\":33164},{\"end\":33642,\"start\":33641},{\"end\":33658,\"start\":33651},{\"end\":33660,\"start\":33659},{\"end\":33673,\"start\":33668},{\"end\":33675,\"start\":33674},{\"end\":34006,\"start\":34003},{\"end\":34018,\"start\":34012},{\"end\":34032,\"start\":34025},{\"end\":34046,\"start\":34038},{\"end\":34557,\"start\":34553},{\"end\":34775,\"start\":34771},{\"end\":34785,\"start\":34782},{\"end\":34795,\"start\":34792},{\"end\":34997,\"start\":34992},{\"end\":35014,\"start\":35009},{\"end\":35027,\"start\":35023},{\"end\":35043,\"start\":35036},{\"end\":35057,\"start\":35052},{\"end\":35076,\"start\":35071},{\"end\":35078,\"start\":35077},{\"end\":35358,\"start\":35355},{\"end\":35368,\"start\":35365},{\"end\":35382,\"start\":35375},{\"end\":35397,\"start\":35391},{\"end\":35405,\"start\":35402},{\"end\":35417,\"start\":35410},{\"end\":35913,\"start\":35908},{\"end\":35930,\"start\":35922},{\"end\":35944,\"start\":35938},{\"end\":35958,\"start\":35950},{\"end\":36606,\"start\":36599},{\"end\":37084,\"start\":37079},{\"end\":37101,\"start\":37092},{\"end\":37113,\"start\":37110},{\"end\":37124,\"start\":37119},{\"end\":37138,\"start\":37132},{\"end\":37140,\"start\":37139},{\"end\":37460,\"start\":37453},{\"end\":37704,\"start\":37699},{\"end\":37726,\"start\":37719},{\"end\":37739,\"start\":37732},{\"end\":37756,\"start\":37745},{\"end\":37770,\"start\":37762},{\"end\":37785,\"start\":37777},{\"end\":38594,\"start\":38589},{\"end\":38614,\"start\":38608},{\"end\":38621,\"start\":38620},{\"end\":38636,\"start\":38628},{\"end\":38657,\"start\":38649},{\"end\":38666,\"start\":38664},{\"end\":38677,\"start\":38672},{\"end\":39239,\"start\":39232},{\"end\":39251,\"start\":39244},{\"end\":39267,\"start\":39259},{\"end\":39277,\"start\":39273},{\"end\":39677,\"start\":39676},{\"end\":39687,\"start\":39686},{\"end\":39689,\"start\":39688},{\"end\":39703,\"start\":39697},{\"end\":39718,\"start\":39712},{\"end\":40104,\"start\":40097},{\"end\":40116,\"start\":40115},{\"end\":40347,\"start\":40341},{\"end\":40356,\"start\":40352},{\"end\":40367,\"start\":40363},{\"end\":40380,\"start\":40372},{\"end\":40393,\"start\":40385},{\"end\":40403,\"start\":40398},{\"end\":40809,\"start\":40803},{\"end\":40820,\"start\":40814},{\"end\":40835,\"start\":40826},{\"end\":40847,\"start\":40841},{\"end\":40864,\"start\":40855},{\"end\":41522,\"start\":41516},{\"end\":41533,\"start\":41527},{\"end\":41543,\"start\":41539},{\"end\":41554,\"start\":41548},{\"end\":41571,\"start\":41562},{\"end\":42188,\"start\":42182},{\"end\":42199,\"start\":42193},{\"end\":42209,\"start\":42205},{\"end\":42220,\"start\":42214},{\"end\":42237,\"start\":42228},{\"end\":42639,\"start\":42635},{\"end\":42654,\"start\":42646},{\"end\":42661,\"start\":42659},{\"end\":42677,\"start\":42668},{\"end\":42691,\"start\":42683},{\"end\":43169,\"start\":43165},{\"end\":43178,\"start\":43175},{\"end\":43716,\"start\":43708},{\"end\":43727,\"start\":43722},{\"end\":43738,\"start\":43733},{\"end\":43752,\"start\":43746},{\"end\":44532,\"start\":44526},{\"end\":44544,\"start\":44537},{\"end\":44556,\"start\":44551},{\"end\":44568,\"start\":44564},{\"end\":45077,\"start\":45067},{\"end\":45091,\"start\":45087},{\"end\":45100,\"start\":45097},{\"end\":45113,\"start\":45106},{\"end\":45511,\"start\":45504},{\"end\":45716,\"start\":45706},{\"end\":45738,\"start\":45731},{\"end\":46269,\"start\":46264},{\"end\":46287,\"start\":46280},{\"end\":46793,\"start\":46787},{\"end\":46804,\"start\":46798},{\"end\":46813,\"start\":46810},{\"end\":46822,\"start\":46819},{\"end\":46832,\"start\":46827},{\"end\":46841,\"start\":46837},{\"end\":46853,\"start\":46847},{\"end\":47288,\"start\":47285},{\"end\":47299,\"start\":47294},{\"end\":48041,\"start\":48035},{\"end\":48055,\"start\":48051},{\"end\":48069,\"start\":48065},{\"end\":48083,\"start\":48078},{\"end\":48100,\"start\":48095},{\"end\":48113,\"start\":48108},{\"end\":48115,\"start\":48114},{\"end\":48129,\"start\":48123},{\"end\":48143,\"start\":48138},{\"end\":48739,\"start\":48732},{\"end\":48748,\"start\":48744},{\"end\":48766,\"start\":48759},{\"end\":48777,\"start\":48773},{\"end\":48779,\"start\":48778},{\"end\":48792,\"start\":48784},{\"end\":48810,\"start\":48802},{\"end\":48826,\"start\":48821},{\"end\":48839,\"start\":48835},{\"end\":48848,\"start\":48845},{\"end\":48859,\"start\":48854},{\"end\":48874,\"start\":48870},{\"end\":48891,\"start\":48885},{\"end\":48904,\"start\":48898},{\"end\":48922,\"start\":48914},{\"end\":48934,\"start\":48928},{\"end\":48950,\"start\":48943},{\"end\":48967,\"start\":48958},{\"end\":48978,\"start\":48974},{\"end\":48991,\"start\":48985},{\"end\":49005,\"start\":49000},{\"end\":49021,\"start\":49015},{\"end\":49037,\"start\":49030},{\"end\":49048,\"start\":49045},{\"end\":49786,\"start\":49780},{\"end\":49798,\"start\":49791},{\"end\":49808,\"start\":49805},{\"end\":49819,\"start\":49814},{\"end\":49830,\"start\":49827},{\"end\":50446,\"start\":50438},{\"end\":50468,\"start\":50460},{\"end\":50484,\"start\":50476},{\"end\":51289,\"start\":51287},{\"end\":51301,\"start\":51294},{\"end\":51330,\"start\":51326},{\"end\":51355,\"start\":51350},{\"end\":51951,\"start\":51947},{\"end\":51959,\"start\":51956},{\"end\":51969,\"start\":51964},{\"end\":51982,\"start\":51975},{\"end\":51990,\"start\":51988},{\"end\":52415,\"start\":52408},{\"end\":52433,\"start\":52423},{\"end\":52448,\"start\":52445},{\"end\":52453,\"start\":52449},{\"end\":52466,\"start\":52460}]", "bib_author_last_name": "[{\"end\":33091,\"start\":33083},{\"end\":33104,\"start\":33102},{\"end\":33119,\"start\":33112},{\"end\":33133,\"start\":33128},{\"end\":33147,\"start\":33140},{\"end\":33162,\"start\":33157},{\"end\":33173,\"start\":33168},{\"end\":33649,\"start\":33643},{\"end\":33666,\"start\":33661},{\"end\":33684,\"start\":33676},{\"end\":33696,\"start\":33686},{\"end\":34010,\"start\":34007},{\"end\":34023,\"start\":34019},{\"end\":34036,\"start\":34033},{\"end\":34049,\"start\":34047},{\"end\":34565,\"start\":34558},{\"end\":34780,\"start\":34776},{\"end\":34790,\"start\":34786},{\"end\":34800,\"start\":34796},{\"end\":35007,\"start\":34998},{\"end\":35021,\"start\":35015},{\"end\":35034,\"start\":35028},{\"end\":35050,\"start\":35044},{\"end\":35069,\"start\":35058},{\"end\":35084,\"start\":35079},{\"end\":35363,\"start\":35359},{\"end\":35373,\"start\":35369},{\"end\":35389,\"start\":35383},{\"end\":35400,\"start\":35398},{\"end\":35408,\"start\":35406},{\"end\":35420,\"start\":35418},{\"end\":35920,\"start\":35914},{\"end\":35936,\"start\":35931},{\"end\":35948,\"start\":35945},{\"end\":35968,\"start\":35959},{\"end\":36614,\"start\":36607},{\"end\":37090,\"start\":37085},{\"end\":37108,\"start\":37102},{\"end\":37117,\"start\":37114},{\"end\":37130,\"start\":37125},{\"end\":37145,\"start\":37141},{\"end\":37465,\"start\":37461},{\"end\":37717,\"start\":37705},{\"end\":37730,\"start\":37727},{\"end\":37743,\"start\":37740},{\"end\":37760,\"start\":37757},{\"end\":37775,\"start\":37771},{\"end\":37789,\"start\":37786},{\"end\":37795,\"start\":37791},{\"end\":38606,\"start\":38595},{\"end\":38618,\"start\":38615},{\"end\":38626,\"start\":38622},{\"end\":38647,\"start\":38637},{\"end\":38662,\"start\":38658},{\"end\":38670,\"start\":38667},{\"end\":38682,\"start\":38678},{\"end\":38688,\"start\":38684},{\"end\":39242,\"start\":39240},{\"end\":39257,\"start\":39252},{\"end\":39271,\"start\":39268},{\"end\":39281,\"start\":39278},{\"end\":39684,\"start\":39678},{\"end\":39695,\"start\":39690},{\"end\":39710,\"start\":39704},{\"end\":39724,\"start\":39719},{\"end\":40113,\"start\":40105},{\"end\":40124,\"start\":40117},{\"end\":40133,\"start\":40126},{\"end\":40350,\"start\":40348},{\"end\":40361,\"start\":40357},{\"end\":40370,\"start\":40368},{\"end\":40383,\"start\":40381},{\"end\":40396,\"start\":40394},{\"end\":40408,\"start\":40404},{\"end\":40812,\"start\":40810},{\"end\":40824,\"start\":40821},{\"end\":40839,\"start\":40836},{\"end\":40853,\"start\":40848},{\"end\":40869,\"start\":40865},{\"end\":41525,\"start\":41523},{\"end\":41537,\"start\":41534},{\"end\":41546,\"start\":41544},{\"end\":41560,\"start\":41555},{\"end\":41576,\"start\":41572},{\"end\":42191,\"start\":42189},{\"end\":42203,\"start\":42200},{\"end\":42212,\"start\":42210},{\"end\":42226,\"start\":42221},{\"end\":42242,\"start\":42238},{\"end\":42644,\"start\":42640},{\"end\":42657,\"start\":42655},{\"end\":42666,\"start\":42662},{\"end\":42681,\"start\":42678},{\"end\":42696,\"start\":42692},{\"end\":43173,\"start\":43170},{\"end\":43183,\"start\":43179},{\"end\":43720,\"start\":43717},{\"end\":43731,\"start\":43728},{\"end\":43744,\"start\":43739},{\"end\":43757,\"start\":43753},{\"end\":44535,\"start\":44533},{\"end\":44549,\"start\":44545},{\"end\":44562,\"start\":44557},{\"end\":44575,\"start\":44569},{\"end\":45085,\"start\":45078},{\"end\":45095,\"start\":45092},{\"end\":45104,\"start\":45101},{\"end\":45122,\"start\":45114},{\"end\":45516,\"start\":45512},{\"end\":45729,\"start\":45717},{\"end\":45741,\"start\":45739},{\"end\":46278,\"start\":46270},{\"end\":46294,\"start\":46288},{\"end\":46796,\"start\":46794},{\"end\":46808,\"start\":46805},{\"end\":46817,\"start\":46814},{\"end\":46825,\"start\":46823},{\"end\":46835,\"start\":46833},{\"end\":46845,\"start\":46842},{\"end\":46857,\"start\":46854},{\"end\":47292,\"start\":47289},{\"end\":47306,\"start\":47300},{\"end\":48049,\"start\":48042},{\"end\":48063,\"start\":48056},{\"end\":48076,\"start\":48070},{\"end\":48093,\"start\":48084},{\"end\":48106,\"start\":48101},{\"end\":48121,\"start\":48116},{\"end\":48136,\"start\":48130},{\"end\":48154,\"start\":48144},{\"end\":48742,\"start\":48740},{\"end\":48757,\"start\":48749},{\"end\":48771,\"start\":48767},{\"end\":48782,\"start\":48780},{\"end\":48800,\"start\":48793},{\"end\":48819,\"start\":48811},{\"end\":48833,\"start\":48827},{\"end\":48843,\"start\":48840},{\"end\":48852,\"start\":48849},{\"end\":48868,\"start\":48860},{\"end\":48883,\"start\":48875},{\"end\":48896,\"start\":48892},{\"end\":48912,\"start\":48905},{\"end\":48926,\"start\":48923},{\"end\":48941,\"start\":48935},{\"end\":48956,\"start\":48951},{\"end\":48972,\"start\":48968},{\"end\":48983,\"start\":48979},{\"end\":48998,\"start\":48992},{\"end\":49013,\"start\":49006},{\"end\":49028,\"start\":49022},{\"end\":49043,\"start\":49038},{\"end\":49053,\"start\":49049},{\"end\":49789,\"start\":49787},{\"end\":49803,\"start\":49799},{\"end\":49812,\"start\":49809},{\"end\":49825,\"start\":49820},{\"end\":49834,\"start\":49831},{\"end\":50458,\"start\":50447},{\"end\":50474,\"start\":50469},{\"end\":50487,\"start\":50485},{\"end\":50492,\"start\":50489},{\"end\":51292,\"start\":51290},{\"end\":51305,\"start\":51302},{\"end\":51324,\"start\":51307},{\"end\":51348,\"start\":51331},{\"end\":51361,\"start\":51356},{\"end\":51367,\"start\":51363},{\"end\":51954,\"start\":51952},{\"end\":51962,\"start\":51960},{\"end\":51973,\"start\":51970},{\"end\":51986,\"start\":51983},{\"end\":51995,\"start\":51991},{\"end\":52421,\"start\":52416},{\"end\":52443,\"start\":52434},{\"end\":52458,\"start\":52454},{\"end\":52469,\"start\":52467}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3753452},\"end\":33596,\"start\":32992},{\"attributes\":{\"doi\":\"10.1023/A:1007558221122\",\"id\":\"b1\",\"matched_paper_id\":13512847},\"end\":33881,\"start\":33598},{\"attributes\":{\"doi\":\"10.18293/SEKE2018-050\",\"id\":\"b2\",\"matched_paper_id\":53242241},\"end\":34531,\"start\":33883},{\"attributes\":{\"doi\":\"https:/link.springer.com/article/10.1023/A:1007379606734\",\"id\":\"b3\",\"matched_paper_id\":45998148},\"end\":34714,\"start\":34533},{\"attributes\":{\"doi\":\"abs/1902.10909\",\"id\":\"b4\",\"matched_paper_id\":67855472},\"end\":34939,\"start\":34716},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":351666},\"end\":35300,\"start\":34941},{\"attributes\":{\"doi\":\"10.1109/CVPR.2009.5206848\",\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":35824,\"start\":35302},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":36539,\"start\":35826},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53082631},\"end\":37031,\"start\":36541},{\"attributes\":{\"doi\":\"10.1145/1147234.1147241\",\"id\":\"b9\",\"matched_paper_id\":4472774},\"end\":37352,\"start\":37033},{\"attributes\":{\"doi\":\"10.4304/jsw.4.8.883-890\",\"id\":\"b10\"},\"end\":37631,\"start\":37354},{\"attributes\":{\"doi\":\"10.18653/v1/N18-2118\",\"id\":\"b11\",\"matched_paper_id\":44169697},\"end\":38514,\"start\":37633},{\"attributes\":{\"doi\":\"10.21437/Interspeech.2016-402\",\"id\":\"b12\",\"matched_paper_id\":5775306},\"end\":39184,\"start\":38516},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":39641,\"start\":39186},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":28729345},\"end\":40024,\"start\":39643},{\"attributes\":{\"id\":\"b15\"},\"end\":40268,\"start\":40026},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":214442368},\"end\":40721,\"start\":40270},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/577\",\"id\":\"b17\",\"matched_paper_id\":51607177},\"end\":41427,\"start\":40723},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1114\",\"id\":\"b18\",\"matched_paper_id\":12522650},\"end\":42071,\"start\":41429},{\"attributes\":{\"doi\":\"10.1109/TKDE.2018.2848260\",\"id\":\"b19\",\"matched_paper_id\":96431765},\"end\":42576,\"start\":42073},{\"attributes\":{\"doi\":\"10.1145/3240508.3240646\",\"id\":\"b20\",\"matched_paper_id\":53036667},\"end\":43070,\"start\":42578},{\"attributes\":{\"doi\":\"10.21437/Interspeech.2016-1352\",\"id\":\"b21\",\"matched_paper_id\":7476732},\"end\":43620,\"start\":43072},{\"attributes\":{\"doi\":\"10.18653/v1/N19-2005\",\"id\":\"b22\",\"matched_paper_id\":85528598},\"end\":44452,\"start\":43622},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":868693},\"end\":45024,\"start\":44454},{\"attributes\":{\"doi\":\"10.1145/3308560.3316603\",\"id\":\"b24\",\"matched_paper_id\":153314529},\"end\":45447,\"start\":45026},{\"attributes\":{\"doi\":\"abs/1608.04670\",\"id\":\"b25\"},\"end\":45632,\"start\":45449},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2747277},\"end\":46181,\"start\":45634},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14131495},\"end\":46718,\"start\":46183},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":201317624},\"end\":47208,\"start\":46720},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1514\",\"id\":\"b29\",\"matched_paper_id\":201103729},\"end\":48006,\"start\":47210},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13756489},\"end\":48630,\"start\":48008},{\"attributes\":{\"doi\":\"abs/1609.08144\",\"id\":\"b31\",\"matched_paper_id\":3603249},\"end\":49659,\"start\":48632},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1514\",\"id\":\"b32\",\"matched_paper_id\":196211550},\"end\":50344,\"start\":49661},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":18309765},\"end\":51211,\"start\":50346},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7752968},\"end\":51879,\"start\":51213},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":195657908},\"end\":52342,\"start\":51881},{\"attributes\":{\"doi\":\"10.1145/3219819.3219839\",\"id\":\"b36\",\"matched_paper_id\":46939981},\"end\":52984,\"start\":52344}]", "bib_title": "[{\"end\":33075,\"start\":32992},{\"end\":33639,\"start\":33598},{\"end\":34001,\"start\":33883},{\"end\":34551,\"start\":34533},{\"end\":34769,\"start\":34716},{\"end\":34990,\"start\":34941},{\"end\":35353,\"start\":35302},{\"end\":35906,\"start\":35826},{\"end\":36597,\"start\":36541},{\"end\":37077,\"start\":37033},{\"end\":37697,\"start\":37633},{\"end\":38587,\"start\":38516},{\"end\":39230,\"start\":39186},{\"end\":39674,\"start\":39643},{\"end\":40339,\"start\":40270},{\"end\":40801,\"start\":40723},{\"end\":41514,\"start\":41429},{\"end\":42180,\"start\":42073},{\"end\":42633,\"start\":42578},{\"end\":43163,\"start\":43072},{\"end\":43706,\"start\":43622},{\"end\":44524,\"start\":44454},{\"end\":45065,\"start\":45026},{\"end\":45704,\"start\":45634},{\"end\":46262,\"start\":46183},{\"end\":46785,\"start\":46720},{\"end\":47283,\"start\":47210},{\"end\":48033,\"start\":48008},{\"end\":48730,\"start\":48632},{\"end\":49778,\"start\":49661},{\"end\":50436,\"start\":50346},{\"end\":51285,\"start\":51213},{\"end\":51945,\"start\":51881},{\"end\":52406,\"start\":52344}]", "bib_author": "[{\"end\":33093,\"start\":33077},{\"end\":33106,\"start\":33093},{\"end\":33121,\"start\":33106},{\"end\":33135,\"start\":33121},{\"end\":33149,\"start\":33135},{\"end\":33164,\"start\":33149},{\"end\":33175,\"start\":33164},{\"end\":33651,\"start\":33641},{\"end\":33668,\"start\":33651},{\"end\":33686,\"start\":33668},{\"end\":33698,\"start\":33686},{\"end\":34012,\"start\":34003},{\"end\":34025,\"start\":34012},{\"end\":34038,\"start\":34025},{\"end\":34051,\"start\":34038},{\"end\":34567,\"start\":34553},{\"end\":34782,\"start\":34771},{\"end\":34792,\"start\":34782},{\"end\":34802,\"start\":34792},{\"end\":35009,\"start\":34992},{\"end\":35023,\"start\":35009},{\"end\":35036,\"start\":35023},{\"end\":35052,\"start\":35036},{\"end\":35071,\"start\":35052},{\"end\":35086,\"start\":35071},{\"end\":35365,\"start\":35355},{\"end\":35375,\"start\":35365},{\"end\":35391,\"start\":35375},{\"end\":35402,\"start\":35391},{\"end\":35410,\"start\":35402},{\"end\":35422,\"start\":35410},{\"end\":35922,\"start\":35908},{\"end\":35938,\"start\":35922},{\"end\":35950,\"start\":35938},{\"end\":35970,\"start\":35950},{\"end\":36616,\"start\":36599},{\"end\":37092,\"start\":37079},{\"end\":37110,\"start\":37092},{\"end\":37119,\"start\":37110},{\"end\":37132,\"start\":37119},{\"end\":37147,\"start\":37132},{\"end\":37467,\"start\":37453},{\"end\":37719,\"start\":37699},{\"end\":37732,\"start\":37719},{\"end\":37745,\"start\":37732},{\"end\":37762,\"start\":37745},{\"end\":37777,\"start\":37762},{\"end\":37791,\"start\":37777},{\"end\":37797,\"start\":37791},{\"end\":38608,\"start\":38589},{\"end\":38620,\"start\":38608},{\"end\":38628,\"start\":38620},{\"end\":38649,\"start\":38628},{\"end\":38664,\"start\":38649},{\"end\":38672,\"start\":38664},{\"end\":38684,\"start\":38672},{\"end\":38690,\"start\":38684},{\"end\":39244,\"start\":39232},{\"end\":39259,\"start\":39244},{\"end\":39273,\"start\":39259},{\"end\":39283,\"start\":39273},{\"end\":39686,\"start\":39676},{\"end\":39697,\"start\":39686},{\"end\":39712,\"start\":39697},{\"end\":39726,\"start\":39712},{\"end\":40115,\"start\":40097},{\"end\":40126,\"start\":40115},{\"end\":40135,\"start\":40126},{\"end\":40352,\"start\":40341},{\"end\":40363,\"start\":40352},{\"end\":40372,\"start\":40363},{\"end\":40385,\"start\":40372},{\"end\":40398,\"start\":40385},{\"end\":40410,\"start\":40398},{\"end\":40814,\"start\":40803},{\"end\":40826,\"start\":40814},{\"end\":40841,\"start\":40826},{\"end\":40855,\"start\":40841},{\"end\":40871,\"start\":40855},{\"end\":41527,\"start\":41516},{\"end\":41539,\"start\":41527},{\"end\":41548,\"start\":41539},{\"end\":41562,\"start\":41548},{\"end\":41578,\"start\":41562},{\"end\":42193,\"start\":42182},{\"end\":42205,\"start\":42193},{\"end\":42214,\"start\":42205},{\"end\":42228,\"start\":42214},{\"end\":42244,\"start\":42228},{\"end\":42646,\"start\":42635},{\"end\":42659,\"start\":42646},{\"end\":42668,\"start\":42659},{\"end\":42683,\"start\":42668},{\"end\":42698,\"start\":42683},{\"end\":43175,\"start\":43165},{\"end\":43185,\"start\":43175},{\"end\":43722,\"start\":43708},{\"end\":43733,\"start\":43722},{\"end\":43746,\"start\":43733},{\"end\":43759,\"start\":43746},{\"end\":44537,\"start\":44526},{\"end\":44551,\"start\":44537},{\"end\":44564,\"start\":44551},{\"end\":44577,\"start\":44564},{\"end\":45087,\"start\":45067},{\"end\":45097,\"start\":45087},{\"end\":45106,\"start\":45097},{\"end\":45124,\"start\":45106},{\"end\":45518,\"start\":45504},{\"end\":45731,\"start\":45706},{\"end\":45743,\"start\":45731},{\"end\":46280,\"start\":46264},{\"end\":46296,\"start\":46280},{\"end\":46798,\"start\":46787},{\"end\":46810,\"start\":46798},{\"end\":46819,\"start\":46810},{\"end\":46827,\"start\":46819},{\"end\":46837,\"start\":46827},{\"end\":46847,\"start\":46837},{\"end\":46859,\"start\":46847},{\"end\":47294,\"start\":47285},{\"end\":47308,\"start\":47294},{\"end\":48051,\"start\":48035},{\"end\":48065,\"start\":48051},{\"end\":48078,\"start\":48065},{\"end\":48095,\"start\":48078},{\"end\":48108,\"start\":48095},{\"end\":48123,\"start\":48108},{\"end\":48138,\"start\":48123},{\"end\":48156,\"start\":48138},{\"end\":48744,\"start\":48732},{\"end\":48759,\"start\":48744},{\"end\":48773,\"start\":48759},{\"end\":48784,\"start\":48773},{\"end\":48802,\"start\":48784},{\"end\":48821,\"start\":48802},{\"end\":48835,\"start\":48821},{\"end\":48845,\"start\":48835},{\"end\":48854,\"start\":48845},{\"end\":48870,\"start\":48854},{\"end\":48885,\"start\":48870},{\"end\":48898,\"start\":48885},{\"end\":48914,\"start\":48898},{\"end\":48928,\"start\":48914},{\"end\":48943,\"start\":48928},{\"end\":48958,\"start\":48943},{\"end\":48974,\"start\":48958},{\"end\":48985,\"start\":48974},{\"end\":49000,\"start\":48985},{\"end\":49015,\"start\":49000},{\"end\":49030,\"start\":49015},{\"end\":49045,\"start\":49030},{\"end\":49055,\"start\":49045},{\"end\":49791,\"start\":49780},{\"end\":49805,\"start\":49791},{\"end\":49814,\"start\":49805},{\"end\":49827,\"start\":49814},{\"end\":49836,\"start\":49827},{\"end\":50460,\"start\":50438},{\"end\":50476,\"start\":50460},{\"end\":50489,\"start\":50476},{\"end\":50494,\"start\":50489},{\"end\":51294,\"start\":51287},{\"end\":51307,\"start\":51294},{\"end\":51326,\"start\":51307},{\"end\":51350,\"start\":51326},{\"end\":51363,\"start\":51350},{\"end\":51369,\"start\":51363},{\"end\":51956,\"start\":51947},{\"end\":51964,\"start\":51956},{\"end\":51975,\"start\":51964},{\"end\":51988,\"start\":51975},{\"end\":51997,\"start\":51988},{\"end\":52423,\"start\":52408},{\"end\":52445,\"start\":52423},{\"end\":52460,\"start\":52445},{\"end\":52471,\"start\":52460}]", "bib_venue": "[{\"end\":33263,\"start\":33240},{\"end\":34186,\"start\":34157},{\"end\":35543,\"start\":35524},{\"end\":36212,\"start\":36105},{\"end\":36792,\"start\":36704},{\"end\":38124,\"start\":37961},{\"end\":38837,\"start\":38815},{\"end\":39397,\"start\":39379},{\"end\":39808,\"start\":39781},{\"end\":41104,\"start\":40999},{\"end\":41776,\"start\":41686},{\"end\":42806,\"start\":42782},{\"end\":43333,\"start\":43311},{\"end\":44059,\"start\":43923},{\"end\":44712,\"start\":44696},{\"end\":45218,\"start\":45196},{\"end\":45925,\"start\":45831},{\"end\":46468,\"start\":46384},{\"end\":46938,\"start\":46917},{\"end\":47681,\"start\":47505},{\"end\":48289,\"start\":48270},{\"end\":50032,\"start\":49945},{\"end\":50817,\"start\":50657},{\"end\":51547,\"start\":51458},{\"end\":52087,\"start\":52068},{\"end\":52683,\"start\":52592},{\"end\":33238,\"start\":33175},{\"end\":33732,\"start\":33721},{\"end\":34155,\"start\":34072},{\"end\":34639,\"start\":34623},{\"end\":34820,\"start\":34816},{\"end\":35105,\"start\":35086},{\"end\":35522,\"start\":35447},{\"end\":36103,\"start\":35990},{\"end\":36702,\"start\":36616},{\"end\":37189,\"start\":37170},{\"end\":37451,\"start\":37354},{\"end\":37959,\"start\":37817},{\"end\":38813,\"start\":38719},{\"end\":39377,\"start\":39303},{\"end\":39779,\"start\":39726},{\"end\":40095,\"start\":40026},{\"end\":40470,\"start\":40410},{\"end\":40997,\"start\":40894},{\"end\":41684,\"start\":41598},{\"end\":42320,\"start\":42269},{\"end\":42780,\"start\":42721},{\"end\":43309,\"start\":43215},{\"end\":43921,\"start\":43779},{\"end\":44694,\"start\":44577},{\"end\":45194,\"start\":45147},{\"end\":45502,\"start\":45449},{\"end\":45829,\"start\":45743},{\"end\":46382,\"start\":46296},{\"end\":46915,\"start\":46859},{\"end\":47503,\"start\":47328},{\"end\":48268,\"start\":48156},{\"end\":49082,\"start\":49069},{\"end\":49943,\"start\":49856},{\"end\":50655,\"start\":50494},{\"end\":51456,\"start\":51369},{\"end\":52066,\"start\":51997},{\"end\":52590,\"start\":52494}]"}}}, "year": 2023, "month": 12, "day": 17}