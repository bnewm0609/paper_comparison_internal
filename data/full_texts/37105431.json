{"id": 37105431, "updated": "2023-11-07 20:12:46.926", "metadata": {"title": "ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition", "authors": "[{\"first\":\"Xin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Hua\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Diao\",\"middle\":[]},{\"first\":\"Dongyan\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 5, "day": 8}, "abstract": "In this paper, we introduce a new and challenging large-scale food image dataset called\"ChineseFoodNet\", which aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In our dataset, images of each food category of our dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. We present our efforts to build this large-scale image dataset, including food category selection, data collection, and data clean and label, in particular how to use machine learning methods to reduce manual labeling work that is an expensive process. We share a detailed benchmark of several state-of-the-art deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose a novel two-step data fusion approach referred as\"TastyNet\", which combines prediction results from different CNNs with voting method. Our proposed approach achieves top-1 accuracies of 81.43% on the validation set and 81.55% on the test set, respectively. The latest dataset is public available for research and can be achieved at https://sites.google.com/view/chinesefoodnet.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1705.02743", "mag": "2613010453", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ChenZD17", "doi": null}}, "content": {"source": {"pdf_hash": "de696cdb77a372e073076fd30b7dc6227761b6bf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1705.02743v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "58bd794e3309ef261484599ef764ca3a73b9d46a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/de696cdb77a372e073076fd30b7dc6227761b6bf.txt", "contents": "\nChineseFoodNet: A Large-scale Image Dataset for Chinese Food Recognition\n\n\nXin Chen \nYu Zhu \nHua Zhou \nLiang Diao \nDongyan Wang \nChineseFoodNet: A Large-scale Image Dataset for Chinese Food Recognition\n1Index Terms-dish recognitiondeep learningChineseFood- NetTastyNet\nIn this paper, we introduce a new and challenging large-scale food image dataset called \"ChineseFoodNet\", which aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In our dataset, images of each food category of our dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. We present our efforts to build this large-scale image dataset, including food category selection, data collection, and data clean and label, in particular how to use machine learning methods to reduce manual labeling work that is an expensive process. We share a detailed benchmark of several state-of-the-art deep convolutional neural networks (CNNs) on ChineseFoodNet. We further propose a novel two-step data fusion approach referred as \"TastyNet\", which combines prediction results from different CNNs with voting method. Our proposed approach achieves top-1 accuracies of 81.43% on the validation set and 81.55% on the test set, respectively. The latest dataset is public available for research and can be achieved at https://sites.google.com/view/chinesefoodnet/.\n\nI. INTRODUCTION\n\nF OOD plays an essential role in everyone's lives, and the behaviour of diet and eating impacts everyone's health [1]. Underestimating food intake directly relates to diverse psychological implications [2]. In recent years, photographing foods and sharing them on social networks have become a part of daily life. Consequently, several applications have been developed to record daily meal activities in personal food log system [3] [4] [5], which are employed to computeraided dietary assessment [6], further usage preference experiments [7] [8], calorie measurement [9] and nutrition balance estimation [10] [11]. As one of user-friendly ways to input of the food log, automatic recognition of dish pictures gives rise of a research field of interest.\n\nDeep convolutional neural networks (CNNs) have achieved state-of-the-art in a variety of computer vision tasks [12] [13]. The visual dish recognition task is the same situation [14]. The quality of training datasets always plays an important role for \u2020 These authors contributed equally to this work.\n\n* means corresponding author. Xin Chen, Hua Zhou, Yu Zhu   training a deep neural network, where the high performance of the deep model is still data-driven to some extent [15] [16].\n\nHowever, to the best of our knowledge, there still exist no effective Chinese food recognition system matured enough to be used in real-world. The major reason is absence of largescale and high quality image datasets. In [17], the Chinese food dataset includes 50 categories, each of which has only 100 images. Obviously, the size of this dataset is not sufficient to satisfy deep learning training requirements.\n\nThe visual dish recognition problem has widely been considered as one of challenging computer vision and pattern recognition tasks [14] [18]. Compared to other types of food such as Italian food and Japanese food, it is more difficult to recognize the images of Chinese dish as the following reasons:\n\n1) The images of same category appear differently. ornament, etc. In order to give impetus to the progress of visual food classification and related computer vision tasks, we build a large-scale image dataset of Chinese dish, named by Chine-seFoodNet. This dataset contains 185,628 images of 208 food categories covering most of popular Chinese food, and these images include web images and photos taken in real world under unconstrained conditions. To the best of our knowledge, ChineseFoodNet is the largest and most comprehensive dataset for visual Chinese food recognition. Some of images of ChineseFoodNet are shown in Figure. 2.\n\nWe benchmark nine CNNs models of four state-of-the-art deep CNNs, SqueezeNet [19], VGG [20], ResNet [21], and DenseNet [22], on our dataset. Experimental results reveal that ChineseFoodNet is capable of learning complex models.\n\nIn this paper, we also propose a novel two-step data fusion approach with voting. Although simple, voting is an effective way to fuse results [23] [24]. Guided by our benchmarks, we try some combination of different CNNs models Based on results on ChineseFoodNet, we take ResNet152, DenseNet121, DeneseNet169, DenseNet201 and VGG19-batch normalization (BN) [25] as our predictive models. 1 Then we fusing these results with voting as a final result. This method is designated as\" TastyNet\". Our proposed method has achieved top-1 accuracy 81.43% in validation set and 81.55% in test set, respectively. Compared to best results of the approaches with a single network structure, the improvements of 2.38% in validation set and 2.33% in these sets have been achieved, respectively. This paper takes three major contributions as following: 1) We present a large-scale image dataset, ChineseFoodNet, for Chinese food recognition tasks. ChineseFoodNet is made up with 185,628 images of 208 categories, and most of the food image are from users' daily life. It is public available for research in related topics. 2 2) We provide a benchmark on our dataset. Totally nine different models of four state-of-the-art CNNs architectures are evaluated. We presents the details of the methodology used in the evaluation and the pre-trained models will be public available for further research. 3) We propose a novel two-step data fusion approach for visual food recognition, which combines predictive results of different CNNs with voting. Experimental results on ChineseFoodNet have shown that approach improves the performance compared to one deep CNNs model. It has shown that data fusion should be an alternative way to improve accuracy instead of only increasing numbers of layers in CNNs. The paper is organized as follows. Section II briefly reviews some public food datasets and the state-of-the-art visual food recognition methods. Section III describes the procedure of building and tagging the ChineseFoodNet dataset. In section IV, several state-of-the-art CNNs methods are benchmarked on ChineseFoodNet. Section V details our proposed data fusion approach and present our results on Chinese-FoofNet. This paper closes with a conclusion of our work and some future directions in section VI.\n\n\nII. RELATED WORK\n\n\nA. Food Dataset\n\nThe scholars have developed some public food datasets 3 for food-related applications such as dietary assessment, computational cooking, food recipe retrieval and so on. Pittsburgh Food Image Dataset (PFID) collects 4,556 fast food images [26]. The UNICT-FD889 dataset of 3,583 images related to 889 distinct dishes are used for Near Duplicate Image retrieval (NDIR) [18]. UEC-Food100 [27] and UEC-Food256 [28] are both Japanese food datasets and contain 100 and 256 categories, respectively. The UPMC-FOOD-101 [29] and ETHZ-FOOD-101 [30] datasets are twin datasets and have same 101 food categories but different images. The images of UPMC-FOOD-101 are recipe images, in which each has the additional textual information, and the images of ETHZ-FOOD-101 are selfies. VIREO-172 [31] is a Chinese Food dataset containing a total of 353 ingredient labels and 110,241 images. However, it aims at cooking recipe retrieval with ingredient recognition.\n\n\nB. Visual Dish Recognition\n\nBefore introducing deep learning techniques to classification, traditional approaches with hand-crafted features have been applied to visual food recognition, including the pairwise feature distribution (PED) [32], Gabor filters [33], SIHT-based Bag of Visual Words (BoW) [34] [4], optimized bag-offeatures model [35], co-occurrence [36], textons [37], Random Forests (RF) [30], and Fisher Vector [38]. Like deep learning applied to other computer vision tasks, CNNs models have outperformed all of traditional methods and achieve higher and higher accuracy with deeper and deeper CNNs [4] [41].\n\nHowever, all of these approaches of both traditional methods and deep learning haven't been tested on a large-scale image dataset of Chinese food.\n\n\nIII. CHINESEFOODNET: A LARGE-SCALE CHINESE FOOD IMAGE DATASET\n\nTo the best of our knowledge, there is no such largescale image datasets for Chinese dish recognition which is mature enough to provided necessary resources for the datadriven techniques, e.g. deep learning, to train complex food recognition models. In this section, we present our procedures to build ChineseFoodNet. Labelling image is an expensive step in building large-scale dataset. In this paper, we design and develop a semi-supervised method to accelerate the whole process. \n\n\nA. Category Selection\n\nVarious cooking styles exist in Chinese food culture, such as Sichuan cuisine, Canton cuisine, etc. Our Chinese food dataset must cover the most popular of Chinese cuisines from different styles of cooking. In this subsection, we present our efforts to meet this goal.\n\nFirst, 250 food categories are gathered from the internet. 4 However, some dishes are missed in search engine yet because they are too popular to be searched such as Tomato omelette. In order to cover them, we conduct a survey of favorite Chinese dishes within our group. Combining with results of the survey, we select about 300 categories. Since Chinese cruise categories is complex and some dishes are very similar visually, such as Braised Chicken Wings and Cola Chicken Wings, we manually merge related categories. After this process, 208 categories of Chinese dish are taken. 5\n\n\nB. Data Collection\n\nThere are two resources of our images, web images and taken photos. The web images in our dataset are coming from social network of the Chinese food and drink/cooking, 6 where users uploaded their Chinese food pictures and also provided the tags (labels) of the image. Also some partial of the images in this dataset are collected by our group in daily life.\n\nAfter these steps, the number of images we brought together achieves more than 500,000. However, those images may contain missing labels, incorrect labels or unclear labels. 4 www.top.baidu.com 5 The names of Chinese dish in ChineseFoodNet are also listed at https: //sites.google.com/view/chinesefoodnet/ 6 www.douguo.com\n\n\nC. Data Clean and Label\n\nAfter collecting large number of food images, the next step is to clean these data and generate proper labels for each image. In this step, we first remove the images with irregular height or width (too large or too small) which usually are irrelevant images. Then we use entropy to clean the images without content. Entropy is a quantitative metric of image content [42]. We calculate the value of entropy of each channel. If the value of any channel is small, we remove it because the image doesn't have enough useful information. The following step is to remove duplicate and/or very similar images with two steps. First, we calculate 1,024 deep features with the last full connection layer of AlexNet [43]. Second, we calculate the Euclidean distance to measure the similarity. If the distance is below a threshold, we consider the images are very similar and remove one.\n\nSome of these images are clearly categorized with specific Chinese food name, such as most of recipe and menu images. The ground truth of this type of image can be directly extracted. However, the number of such images is very limited and the quality of those images are usually very high, e.g., the images are shot with sufficient light condition, good presentation of the food, and good angels, etc. Thus this type of images shows very different distributions comparing to the images captured in daily life, and brings a potential impact for the food recognition tasks in real life.\n\nThe other images are usually not well-labeled, and the food photos are taken in real world conditions. Those images are mainly from the users' daily uploads which show very preferred data distributions in food images in the wild. Besides, this type of images is usually associated with metadata. The metadata can be viewed as an description of each image in text format, which often describes the name, cooking recipe and other information about the food in that image. In our procedure, this metadata is utilized to filter the useful images with correct labels. Particularly, we manually generate a set of keywords for each food class in our database, and use each set of the keyword to match the image metadata. Images with metadata which contains the keywords of certain class are selected and labeled with that class.\n\nIt should be noted that, after the aforementioned step, there are still a number of incorrect labels, which are either caused by unclear descriptions in metadata or irrelevant images. Label validation by human labor on this large number of images is an expensive task in terms of both time and costs. Here we accelerate to label these image by some already labeled samples in advance. We first collect a small database of food images using the crowd-sourcing platform (no overlap between our current dataset) with same class labels. Then a shallow CNN model is trained for the food recognition task on this small database. Given this CNNs model, we classify our collected images into different classes representing candidate labels. Specifically, top n (e.g., 5) predictions from the shallow network are selected as the candidate labels for one image. Finally, we perform manually label validation to finalize the dataset by eliminating the wrong labelled images.\n\n\nD. Dataset Description\n\nAfter work of category selection, data collection, the data collection and cleaning mentioned in previous subsections, finally the ChineseFoodNet dataset contains 185,628 images, with total size of 19.4 Gigabyte (GB). Images in the dataset are kept their original size without any processing and color. The total number of categories is 208 for the current version of dataset, and each image is labelled with only one label from 0 to 207.\n\nWe split the whole dataset into training, testing and validation sets, approximately in the ratio 80%, 10% and 10%, respectively. Specifically, there are 145,066, 20,254 and 20,310 images for training, validation and testing set, respectively. Figure 1 and Figure 2 show some example images in our dataset.\n\n\nIV. BENCHMARK ON CHINESEFOODNET DATASET\n\nIn this section, we conducted benchmark experiments for the ChineseFoodNet dataset. First the experimental settings are described, then we introduce the experimental protocol and finally we provide the experimental results and analysis.\n\n\nA. Experimental settings\n\nOur experiments were all conducted using PyTorch [44] deep learning framework. In the training phase, the initial learning rate is set to 0.01, momentum is set 0.9, and weight decay is set to 1e-4. We set the learning rate to the initial learning rate decayed by 10 every 30 epoch. The number of epoch for the training is set to 90. Training optimization method is selected to stochastic gradient descent (SGD) with momentum. No augmentation process is applied except the resizing and mirror. Training images are firstly resized to 256x256, then a random crop of size 224x224 with hoizontal flip (probability 0.5) is applied. We have used the pretrained models from imagenet dataset [16] and fine-tuned the network with our food data. During the testing, images are resized to 256x256 and then we use center crop of size 224x224 to feed into the network. All the experiments were conducted on CentOS 7 operation system, with Intel Xeon E5 CPU (2.2G), 128GB RAM and Nvidia P100 Tesla GPUs hardware with 16G memory.\n\n\nB. Experimental Protocol\n\nThe dataset is split into training, validation, and test sets by random selection. There are 145,065 images in the training set. There are 20254 images in the validation set and the rest 20310 images are used for testing. Comprehensive experiments have been conducted using various popular deep learning network architectures with different structures and different number of layers. Specifically, we have benchmarked the performance of: Squeezenet (version 1.1) [19], VGG19 (with BN layer) [20], Resnet (18, 34, and 50) [21], DenseNet (121, 169, and 201) [22]. In order to have a fair comparison, all the experiments are using same input image size and same preprocessing/postprocessing procedures. Some implementation details of ResNet and Squeezenet are illuminated in Figure. \n\n\nC. Experimental Results\n\nThe recognition performance of different deep networks are shown in Table I, both top 1 accuracy and top 5 accuracy are presented. Table I has shown that the best top-1 performance on validation set is 79.05%, which is achieved by DenseNet201. The accuracies of VGG19 and DenseNet169 are also very close to the best results. On the test set, the best recognition rate is 79.22%, obtained by VGG19, the second best results is obtained by Resnet152, which is 0.22% lower than then VGG19.\n\nDeeper CNNs models generally achieve better performance [45] [46]. From the results, we can see that, CNN models obtains significant improvements in performance when number of layers in same network architecture are increased. E.g., ResNet with 18 layers has recognition rate 73.64%, while the deeper mode ResNet with 152 layers achieves about 5% improvement in both validation and test sets. Similar results can be observed in DenseNet architecture. On the other hand, deep models with wider structure also shows promising performance, e.g., VGG19-BN obtains the best results in test set, and the worst result (58.42 % and 58.24% on validation and test sets, respectively) is achieved by Squeezenet v1.1, which is a shallow and narrow network structure designed for fast and efficient inference.\n\n\nV. TASTYNET: A TWO-STEP DATA FUSION APPROACH A. Methodology\n\nAs shown in Table I, the accuracy has higher and higher with deeper and deeper model. If we would improve furthermore, a possible way to use much deeper CNNs models. However, it needs much computation and memory resources. What is more, deeper models easily lead to overfitting problem. The alternative way is the data confusion approach. Its idea is to fuse the inference results of different models. As shown in Figure. 5, predictions from different networks are gathered and a voting approach is utilized to obtain the final fused prediction.\n\nBased on some results of different combinations, as shown in Table. II, we select the combination of models that achieves the best top 1 result, ResNet152, DenseNet121, DenseNet169, DenseNet201 and VGG19-BN. The voting method is to average the results of all models. The algorithm is details in Algorithm.!1.\n\n\nB. Results and Analysis\n\nDifferent combinations of network architectures are applied and the experimental results are shown in Table II. From Fig. 5. Basic scheme of the two-step data fusion approach. The first one is to obtain some predictive results from different models. In TastyNet, we use Resnet152, DenseNet121, DenseNet169, DenseNet201 and VGG19-BN. The second one is to combine these result to one final result with voting policy. It his paper, we use weighted coefficient of the results of the first step. From our proposed approach, we get two conclusions as followings:\n\n1) By applying data fusing approach on different deep networks, the overall performance can be further boosted than using the single deep network; 2) Combination of different network architectures show more benefits in improving the performance than the combinations with same network architectures, and combination of deeper and wider networks obtains the best results in our evaluation;\n\n\nVI. CONCLUSION AND FUTURE WORK\n\nIn this paper, we have successfully created a very large-scale image dataset for Chinese dish recognition, ChineseFoodNet. It contains 185,628 images of 208 food categories, in which the images are from not only web images but also real world. As a consequence, the models trained on our dataset should have covered most of food recognition applications. Also, we present the benchmarks of nine state-of-the-art CNNs models of four well-known CNNs architectures on ChineseFoodNet. Finally, we propose a novel two-step data fusion approach, \"TastyNet\". Based on experimental results, we select Resnet 152, Densenent 121, Densenet 169, Densenet 201 and VGG19+BN models. After voting the results of these model, we obtain final inference result. It has shown the state-of-the-art results on ChineseFoodNet. What is more, our proposed approach has shown that data fusion is an effective way to obtain a better result instead of only working on one type CNNs model.\n\nFor our future work, we are extending the number of food category to over 500 that should be applied in much applications. Also, we will investigate new fusion methods to fuse the different results with different models to obtain the better performance.\n\nFig. 1 .\n1Example images from our dataset. Each row shows five images from one category of Chinese food. From top to bottom, the food names are Sichuan noodles with peppery sauce, Mapo tofu, potato silk, and scrambled egg with tomato, respectively. Variations in visual appearance of images of Chinese food caused by complex background, various illumination, different angle of view, different ingredients of the same category, etc. show challenges of visual food recognition. All of these image keep their original size.\n\nFig. 2 .\n2Fifty sample images of ChineseFoodNet dataset. The dataset contains 185,628 Chinese food images organized into 208 categories. All images in the dataset are color. Images are resized for better presentation\n\nFig. 3 .\n3We show basic architectures of four well-known CNNs in our evaluation. From left to right, the architectures are VGG, Resnet, Densenet, and Squeezenet, respectively.\n\n\n4.\n\nFig. 4 .\n4Illustration of residual block in ResNet and fire block in Squeezenet.\n\n\nand Dongyan Wang are with Midea Emerging Technology Center, San Jose, 95134, USA. Xin's email: chen1.xin@midea.com, Yu's email: zhu.yu@midea.com, Hua's email: hua.zhou@midea.com, and Dongyan's email: dongyan.wang@midea.com. Liang Diao is with Midea Artificial Intelligence Research Institute, Shenzhen, Guangdong, 528311, P. R. China. email: liang.diao@midea.com.\n\n\n[6] [14][39] [40] \n\nTABLE I RECOGNITION\nIRATES OF DIFFERENT DEEP NETWORKS ON OUR FOOD DATASET. BOTH TOP-1 AND TOP-5 ACCURACY ARE SHOWN ON VALIDATION SET AND TEST SET.Method \nValidation \nTest \nTop-1 Accuracy \nTop-5 Accuracy \nTop-1 Accuracy \nTop-5 Accuracy \n\nSqueezenet1 1 \n58.42% \n85.02% \n58.24% \n85.43% \nVGG19-BN \n78.96% \n95.73% \n79.22% \n95.99% \nResNet18 \n73.64% \n93.53% \n73.67% \n93.62% \nResNet34 \n75.51% \n94.29% \n75.82% \n94.56% \nResNet50 \n77.31% \n95.20% \n77.84% \n95.44% \nResNet152 \n78.34% \n95.51% \n79.00% \n95.79% \nDenseNet121 \n78.07% \n95.42% \n78.25% \n95.53% \nDenseNet169 \n78.87% \n95.80% \n78.72% \n95.83% \nDenseNet201 \n79.05% \n95.79% \n78.78% \n95.72% \n\n\n\nTABLE II EXPERIMENTAL\nIIRESULTS (RECOGNITION ACCURACIES) OF DIFFERENT FUSION SCHEMES ResNet (18 + 34 + 50 + 152) + Densenet (121 + 169 + 201) + VGG19-BN 81.23% 81.12% 96.79% 96.76% ResNet152 + DenseNet (121 + 169 + 201) + VGG19-BN Predictive result from Resnet152, p(i), i from 0 to 207 6: Predictive result from DenseNet121, p(i), i from 0 to 207 7: Predictive result from DenseNet169, p(i), i from 0 to 207 8: Predictive result from DenseNet201, p(i), i from 0 to 207 9: Predictive result from Resnet152, p(i), i from 0 to 207 10: Get average result p(i) of all p(i), i from 0 to 207 11: Find maximum p(i) and get i 12: The output is number i thistable, we can conclude that the overall performance is generally increasing for different combinations with ensemble more deep networks. The fusion results of ResNet with different number of layers, obtained higher performance (79.46% top 1 accuracy on test set) than single ResNet (ResNet 152, 77.84% top 1 accuracy on test set). Also the fusion results on DenseNet achieved a 1.12% improvement on test set than the best results achieved for single DenseNet architecture. Furthermore, combination of different types of CNNs networks (e.g., ResNets, DenseNets and VGG shown in Row 3 and 4 in Table II) further improves the overall recognition performance. The best result is obtained by fusing Resnet152 and Densenet 121, Densenet 169, Densenet 201, and VGG19-BN, the recognition accuracy is 81.43% on the validation set and 81.55% for the test set. This results is 2.38% and 2.33% higher than the single network on validation and test set, respectively. Based on the experimental results, we select five CNNs models ,Resnet152, DenseNet121, DenseNet169, DenseNet201 and VGG19-BN, as components of TastyNet.Fusion Method \nTop 1 Accuracy \nTop 5 Accuracy \nValidation \nTest \nValidation \nTest \n\nResNet (18 + 34 + 50 + 152) \n79.19% \n79.46% \n96.03% \n96.16% \nDenseNet (121 + 169 + 201) \n80.47% \n80.17% \n96.26% \n96.30% \nResNet (18 + 34 + 50 + 152) + Densenet (121 + 169 + 201) \n80.89% \n81.08% \n96.60% \n96.67% \n81.43% \n81.55% \n96.73% \n96.76% \n\nAlgorithm 1 Algorithm of TastyNet. \n1: Input: \n2: Image \n3: Output: \n4: Number \nRange from 0-207 \n5: \nThe name of CNNs networks consists of letters+numbers. Letters are type of CNNs, and following numbers are the number of layers.2 Our dataset can be accessed from https://sites.google.com/view/ chinesefoodnet/.\nIn order to review fairly, we only discuss the data that are available for download in this paper. The last access date is June 1, 2017\nACKNOWLEDGMENTThe authors would like to thank Zhi Zhang, Xukai Zhang, Zigang Wang, Xiangping Zeng, Xiaofei Xu, Dangdang Mi and Qingqing Chang for their discussions and efforts to collect data. All of them are with Midea Health and Nutrition Institute.\nSelected eating behaviours and excess body weight: a systematic review. A Mesas, M Mu\u00f1oz-Pareja, E L\u00f3pez-Garc\u00eda, F Rodr\u00edguez-Artalejo, Obesity Reviews. 132A. Mesas, M. Mu\u00f1oz-Pareja, E. L\u00f3pez-Garc\u00eda, and F. Rodr\u00edguez- Artalejo, \"Selected eating behaviours and excess body weight: a sys- tematic review,\" Obesity Reviews, vol. 13, no. 2, pp. 106-135, 2012.\n\nMarkers of the validity of reported energy intake. M B E Livingstone, A E Black, The Journal of nutrition. 1333M. B. E. Livingstone and A. E. Black, \"Markers of the validity of reported energy intake,\" The Journal of nutrition, vol. 133, no. 3, pp. 895S-920S, 2003.\n\nMultimedia foodlog: Diverse applications from selfmonitoring to social contributions. K Aizawa, ITE Transactions on Media Technology and Applications. 13K. Aizawa, \"Multimedia foodlog: Diverse applications from self- monitoring to social contributions,\" ITE Transactions on Media Tech- nology and Applications, vol. 1, no. 3, pp. 214-219, 2013.\n\nReal-time mobile food recognition system. Y Kawano, K Yanai, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsY. Kawano and K. Yanai, \"Real-time mobile food recognition system,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2013, pp. 1-7.\n\nMenumatch: restaurant-specific food logging from images. O Beijbom, N Joshi, D Morris, S Saponas, S Khullar, Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on. IEEEO. Beijbom, N. Joshi, D. Morris, S. Saponas, and S. Khullar, \"Menu- match: restaurant-specific food logging from images,\" in Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on. IEEE, 2015, pp. 844-851.\n\nDeepfood: Deep learning-based food image recognition for computer-aided dietary assessment. C Liu, Y Cao, Y Luo, G Chen, V Vokkarane, Y Ma, International Conference on Smart Homes and Health Telematics. SpringerC. Liu, Y. Cao, Y. Luo, G. Chen, V. Vokkarane, and Y. Ma, \"Deepfood: Deep learning-based food image recognition for computer-aided dietary assessment,\" in International Conference on Smart Homes and Health Telematics. Springer, 2016, pp. 37-48.\n\nComparative study of the routine daily usability of foodlog a smartphone-based food recording tool assisted by image retrieval. K Aizawa, K Maeda, M Ogawa, Y Sato, M Kasamatsu, K Waki, H Takimoto, Journal of diabetes science and technology. 82K. Aizawa, K. Maeda, M. Ogawa, Y. Sato, M. Kasamatsu, K. Waki, and H. Takimoto, \"Comparative study of the routine daily usability of foodlog a smartphone-based food recording tool assisted by image retrieval,\" Journal of diabetes science and technology, vol. 8, no. 2, pp. 203-208, 2014.\n\nEstimation of the attractiveness of food photography focusing on main ingredients. K Takahashi, K Doman, Y Kawanishi, T Hirayama, I Ide, D Deguchi, H Murase, Proceedings of the 9th Workshop on Multimedia for Cooking and Eating Activities in conjunction with The 2017 International Joint Conference on Artificial Intelligence. the 9th Workshop on Multimedia for Cooking and Eating Activities in conjunction with The 2017 International Joint Conference on Artificial IntelligenceACMK. Takahashi, K. Doman, Y. Kawanishi, T. Hirayama, I. Ide, D. Deguchi, and H. Murase, \"Estimation of the attractiveness of food photography focusing on main ingredients,\" in Proceedings of the 9th Workshop on Multimedia for Cooking and Eating Activities in conjunction with The 2017 International Joint Conference on Artificial Intelligence. ACM, 2017, pp. 1-6.\n\nMeasuring calorie and nutrition from food image. P Pouladzadeh, S Shirmohammadi, R Al-Maghrabi, IEEE Transactions on Instrumentation and Measurement. 638P. Pouladzadeh, S. Shirmohammadi, and R. Al-Maghrabi, \"Measuring calorie and nutrition from food image,\" IEEE Transactions on Instru- mentation and Measurement, vol. 63, no. 8, pp. 1947-1956, 2014.\n\nFood balance estimation by using personal dietary tendencies in a multimedia food log. K Aizawa, Y Maruyama, H Li, C Morikawa, IEEE Transactions on multimedia. 158K. Aizawa, Y. Maruyama, H. Li, and C. Morikawa, \"Food balance estimation by using personal dietary tendencies in a multimedia food log,\" IEEE Transactions on multimedia, vol. 15, no. 8, pp. 2176-2185, 2013.\n\nNutrinet: A deep learning food and drink image recognition system for dietary assessment. S Mezgec, B Korou\u0161i\u0107 Seljak, Nutrients. 97657S. Mezgec and B. Korou\u0161i\u0107 Seljak, \"Nutrinet: A deep learning food and drink image recognition system for dietary assessment,\" Nutrients, vol. 9, no. 7, p. 657, 2017.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, vol. 521, no. 7553, pp. 436-444, 2015.\n\nDeep learning in neural networks: An overview. J Schmidhuber, Neural networks. 61J. Schmidhuber, \"Deep learning in neural networks: An overview,\" Neural networks, vol. 61, pp. 85-117, 2015.\n\nFood image recognition using very deep convolutional networks. H Hassannejad, G Matrella, P Ciampolini, I De Munari, M Mordonini, S Cagnoni, Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary Management. the 2nd International Workshop on Multimedia Assisted Dietary ManagementACMH. Hassannejad, G. Matrella, P. Ciampolini, I. De Munari, M. Mor- donini, and S. Cagnoni, \"Food image recognition using very deep con- volutional networks,\" in Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary Management. ACM, 2016, pp. 41-49.\n\nLearning deep architectures for AI. Y Bengio, Foundations and trends R in Machine Learning. 2Y. Bengio et al., \"Learning deep architectures for AI,\" Foundations and trends R in Machine Learning, vol. 2, no. 1, pp. 1-127, 2009.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, Computer Vision and Pattern Recognition. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 248-255.\n\nAutomatic Chinese food identification and quantity estimation. M.-Y Chen, Y.-H Yang, C.-J Ho, S.-H Wang, S.-M Liu, E Chang, C.-H Yeh, M Ouhyoung, SIGGRAPH Asia 2012 Technical Briefs. ACMM.-Y. Chen, Y.-H. Yang, C.-J. Ho, S.-H. Wang, S.-M. Liu, E. Chang, C.- H. Yeh, and M. Ouhyoung, \"Automatic Chinese food identification and quantity estimation,\" in SIGGRAPH Asia 2012 Technical Briefs. ACM, 2012.\n\nA benchmark dataset to study the representation of food images. G M Farinella, D Allegra, F Stanco, European Conference on Computer Vision. SpringerG. M. Farinella, D. Allegra, and F. Stanco, \"A benchmark dataset to study the representation of food images,\" in European Conference on Computer Vision. Springer, 2014, pp. 584-599.\n\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and < 0.5 mb model size. F N Iandola, S Han, M W Moskewicz, K Ashraf, W J Dally, K Keutzer, arXiv:1602.07360arXiv preprintF. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, \"Squeezenet: Alexnet-level accuracy with 50x fewer parameters and < 0.5 mb model size,\" arXiv preprint arXiv:1602.07360, 2016.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nDensenet: Implementing efficient convnet descriptor pyramids. F Iandola, M Moskewicz, S Karayev, R Girshick, T Darrell, K Keutzer, arXiv:1404.1869arXiv preprintF. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Darrell, and K. Keutzer, \"Densenet: Implementing efficient convnet descriptor pyra- mids,\" arXiv preprint arXiv:1404.1869, 2014.\n\nParallel nonparametric binarization for degraded document images. X Chen, L Lin, Y Gao, Neurocomputing. 189X. Chen, L. Lin, and Y. Gao, \"Parallel nonparametric binarization for degraded document images,\" Neurocomputing, vol. 189, pp. 43-52, 2016.\n\nVoting for candidates: adapting data fusion techniques for an expert search task. C Macdonald, I Ounis, Proceedings of the 15th ACM international conference on Information and knowledge management. the 15th ACM international conference on Information and knowledge managementACMC. Macdonald and I. Ounis, \"Voting for candidates: adapting data fusion techniques for an expert search task,\" in Proceedings of the 15th ACM international conference on Information and knowledge management. ACM, 2006, pp. 387-396.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, International Conference on Machine Learning. S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in International Conference on Machine Learning, 2015, pp. 448-456.\n\nPFID: Pittsburgh fast-food image dataset. M Chen, K Dhingra, W Wu, L Yang, R Sukthankar, J Yang, 16th IEEE International Conference on. IEEEImage Processing (ICIP)M. Chen, K. Dhingra, W. Wu, L. Yang, R. Sukthankar, and J. Yang, \"PFID: Pittsburgh fast-food image dataset,\" in Image Processing (ICIP), 2009 16th IEEE International Conference on. IEEE, 2009, pp. 289- 292.\n\nRecognition of multiple-food images by detecting candidate regions. Y Matsuda, H Hoashi, K Yanai, Proc. of IEEE International Conference on Multimedia and Expo (ICME). of IEEE International Conference on Multimedia and Expo (ICME)Y. Matsuda, H. Hoashi, and K. Yanai, \"Recognition of multiple-food images by detecting candidate regions,\" in Proc. of IEEE International Conference on Multimedia and Expo (ICME), 2012.\n\nAutomatic expansion of a food image dataset leveraging existing categories with domain adaptation. Y Kawano, K Yanai, Proc. of ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV). of ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV)Y. Kawano and K. Yanai, \"Automatic expansion of a food image dataset leveraging existing categories with domain adaptation,\" in Proc. of ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2014.\n\nRecipe recognition with large multimodal food dataset. X Wang, D Kumar, N Thome, M Cord, F Precioso, Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on. IEEEX. Wang, D. Kumar, N. Thome, M. Cord, and F. Precioso, \"Recipe recognition with large multimodal food dataset,\" in Multimedia & Expo Workshops (ICMEW), 2015 IEEE International Conference on. IEEE, 2015, pp. 1-6.\n\nFood-101-mining discriminative components with random forests. L Bossard, M Guillaumin, L Van Gool, European Conference on Computer Vision. SpringerL. Bossard, M. Guillaumin, and L. Van Gool, \"Food-101-mining dis- criminative components with random forests,\" in European Conference on Computer Vision. Springer, 2014, pp. 446-461.\n\nDeep-based ingredient recognition for cooking recipe retrieval. J Chen, C.-W Ngo, Proceedings of the 2016 ACM on Multimedia Conference. the 2016 ACM on Multimedia ConferenceACMJ. Chen and C.-W. Ngo, \"Deep-based ingredient recognition for cooking recipe retrieval,\" in Proceedings of the 2016 ACM on Multimedia Conference. ACM, 2016, pp. 32-41.\n\nFood recognition using statistics of pairwise local features. S Yang, M Chen, D Pomerleau, R Sukthankar, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEES. Yang, M. Chen, D. Pomerleau, and R. Sukthankar, \"Food recognition using statistics of pairwise local features,\" in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 2249-2256.\n\nTechnology-assisted dietary assessment. F Zhu, A Mariappan, C J Boushey, D Kerr, K D Lutes, D S Ebert, E J Delp, Proceedings of SPIE. SPIENIH Public Access6814681411F. Zhu, A. Mariappan, C. J. Boushey, D. Kerr, K. D. Lutes, D. S. Ebert, and E. J. Delp, \"Technology-assisted dietary assessment,\" in Proceedings of SPIE, vol. 6814. NIH Public Access, 2008, p. 681411.\n\nDietcam: Automatic dietary assessment with mobile camera phones. F Kong, J Tan, Pervasive and Mobile Computing. 81F. Kong and J. Tan, \"Dietcam: Automatic dietary assessment with mobile camera phones,\" Pervasive and Mobile Computing, vol. 8, no. 1, pp. 147-163, 2012.\n\nA food recognition system for diabetic patients based on an optimized bag-of-features model. M M Anthimopoulos, L Gianola, L Scarnato, P Diem, S G Mougiakakou, IEEE journal of biomedical and health informatics. 184M. M. Anthimopoulos, L. Gianola, L. Scarnato, P. Diem, and S. G. Mougiakakou, \"A food recognition system for diabetic patients based on an optimized bag-of-features model,\" IEEE journal of biomedical and health informatics, vol. 18, no. 4, pp. 1261-1271, 2014.\n\nMultiple-food recognition considering co-occurrence employing manifold ranking. Y Matsuda, K Yanai, Pattern Recognition (ICPR), 2012 21st International Conference on. IEEEY. Matsuda and K. Yanai, \"Multiple-food recognition considering co-occurrence employing manifold ranking,\" in Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 2017- 2020.\n\nClassifying food images represented as bag of textons. G M Farinella, M Moltisanti, S Battiato, Image Processing (ICIP), 2014 IEEE International Conference on. IEEEG. M. Farinella, M. Moltisanti, and S. Battiato, \"Classifying food images represented as bag of textons,\" in Image Processing (ICIP), 2014 IEEE International Conference on. IEEE, 2014, pp. 5212-5216.\n\nFoodcam: A real-time mobile food recognition system employing fisher vector. Y Kawano, K Yanai, International Conference on Multimedia Modeling. SpringerY. Kawano and K. Yanai, \"Foodcam: A real-time mobile food recog- nition system employing fisher vector,\" in International Conference on Multimedia Modeling. Springer, 2014, pp. 369-373.\n\nFood recognition for dietary assessment using deep convolutional neural networks. S Christodoulidis, M Anthimopoulos, S Mougiakakou, International Conference on Image Analysis and Processing. SpringerS. Christodoulidis, M. Anthimopoulos, and S. Mougiakakou, \"Food recognition for dietary assessment using deep convolutional neural net- works,\" in International Conference on Image Analysis and Processing. Springer, 2015, pp. 458-465.\n\nWide-slice residual networks for food recognition. N Martinel, G L Foresti, C Micheloni, arXiv:1612.06543arXiv preprintN. Martinel, G. L. Foresti, and C. Micheloni, \"Wide-slice residual networks for food recognition,\" arXiv preprint arXiv:1612.06543, 2016.\n\nFood image recognition with deep convolutional features. Y Kawano, K Yanai, Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication. the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct PublicationACMY. Kawano and K. Yanai, \"Food image recognition with deep con- volutional features,\" in Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication. ACM, 2014, pp. 589-593.\n\nImage processing, analysis, and machine vision. M Sonka, V Hlavac, R Boyle, Cengage LearningM. Sonka, V. Hlavac, and R. Boyle, Image processing, analysis, and machine vision. Cengage Learning, 2014.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in neural infor- mation processing systems, 2012, pp. 1097-1105.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1-9.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for computer vision,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2818-2826.\n", "annotations": {"author": "[{\"end\":85,\"start\":76},{\"end\":93,\"start\":86},{\"end\":103,\"start\":94},{\"end\":115,\"start\":104},{\"end\":129,\"start\":116}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":80},{\"end\":92,\"start\":89},{\"end\":102,\"start\":98},{\"end\":114,\"start\":110},{\"end\":128,\"start\":124}]", "author_first_name": "[{\"end\":79,\"start\":76},{\"end\":88,\"start\":86},{\"end\":97,\"start\":94},{\"end\":109,\"start\":104},{\"end\":123,\"start\":116}]", "author_affiliation": null, "title": "[{\"end\":73,\"start\":1},{\"end\":202,\"start\":130}]", "venue": null, "abstract": "[{\"end\":1637,\"start\":270}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1773,\"start\":1770},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1861,\"start\":1858},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2088,\"start\":2085},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2096,\"start\":2093},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2156,\"start\":2153},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2198,\"start\":2195},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2202,\"start\":2199},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2227,\"start\":2224},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2265,\"start\":2261},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2270,\"start\":2266},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2526,\"start\":2522},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2531,\"start\":2527},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2592,\"start\":2588},{\"end\":2769,\"start\":2766},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2889,\"start\":2885},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2894,\"start\":2890},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3122,\"start\":3118},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3446,\"start\":3442},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3451,\"start\":3447},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4330,\"start\":4326},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4340,\"start\":4336},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4353,\"start\":4349},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4372,\"start\":4368},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4624,\"start\":4620},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4629,\"start\":4625},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4839,\"start\":4835},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4867,\"start\":4866},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7048,\"start\":7044},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7176,\"start\":7172},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7194,\"start\":7190},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7215,\"start\":7211},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7320,\"start\":7316},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7343,\"start\":7339},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7587,\"start\":7583},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7995,\"start\":7991},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8015,\"start\":8011},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8058,\"start\":8054},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8062,\"start\":8059},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8099,\"start\":8095},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8119,\"start\":8115},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8133,\"start\":8129},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8159,\"start\":8155},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8183,\"start\":8179},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8371,\"start\":8368},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8376,\"start\":8372},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10145,\"start\":10144},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10511,\"start\":10510},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10531,\"start\":10530},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11057,\"start\":11053},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11395,\"start\":11391},{\"end\":15070,\"start\":15066},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15704,\"start\":15700},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16526,\"start\":16522},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16554,\"start\":16550},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16584,\"start\":16580},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16619,\"start\":16615},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17414,\"start\":17410},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":17419,\"start\":17415},{\"end\":19220,\"start\":19214},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22670,\"start\":22667},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22679,\"start\":22675},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22684,\"start\":22680},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25635,\"start\":25634}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21816,\"start\":21294},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22034,\"start\":21817},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22211,\"start\":22035},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22216,\"start\":22212},{\"attributes\":{\"id\":\"fig_4\"},\"end\":22298,\"start\":22217},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22664,\"start\":22299},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22685,\"start\":22665},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":23318,\"start\":22686},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":25505,\"start\":23319}]", "paragraph": "[{\"end\":2409,\"start\":1656},{\"end\":2711,\"start\":2411},{\"end\":2895,\"start\":2713},{\"end\":3309,\"start\":2897},{\"end\":3611,\"start\":3311},{\"end\":4247,\"start\":3613},{\"end\":4476,\"start\":4249},{\"end\":6766,\"start\":4478},{\"end\":7751,\"start\":6805},{\"end\":8377,\"start\":7782},{\"end\":8525,\"start\":8379},{\"end\":9074,\"start\":8591},{\"end\":9368,\"start\":9100},{\"end\":9953,\"start\":9370},{\"end\":10334,\"start\":9976},{\"end\":10658,\"start\":10336},{\"end\":11561,\"start\":10686},{\"end\":12147,\"start\":11563},{\"end\":12970,\"start\":12149},{\"end\":13935,\"start\":12972},{\"end\":14400,\"start\":13962},{\"end\":14708,\"start\":14402},{\"end\":14988,\"start\":14752},{\"end\":16030,\"start\":15017},{\"end\":16839,\"start\":16059},{\"end\":17352,\"start\":16867},{\"end\":18150,\"start\":17354},{\"end\":18759,\"start\":18214},{\"end\":19069,\"start\":18761},{\"end\":19653,\"start\":19097},{\"end\":20043,\"start\":19655},{\"end\":21038,\"start\":20078},{\"end\":21293,\"start\":21040}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16942,\"start\":16935},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17005,\"start\":16998},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18233,\"start\":18226},{\"end\":18828,\"start\":18822},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19207,\"start\":19199}]", "section_header": "[{\"end\":1654,\"start\":1639},{\"end\":6785,\"start\":6769},{\"end\":6803,\"start\":6788},{\"end\":7780,\"start\":7754},{\"end\":8589,\"start\":8528},{\"end\":9098,\"start\":9077},{\"end\":9974,\"start\":9956},{\"end\":10684,\"start\":10661},{\"end\":13960,\"start\":13938},{\"end\":14750,\"start\":14711},{\"end\":15015,\"start\":14991},{\"end\":16057,\"start\":16033},{\"end\":16865,\"start\":16842},{\"end\":18212,\"start\":18153},{\"end\":19095,\"start\":19072},{\"end\":20076,\"start\":20046},{\"end\":21303,\"start\":21295},{\"end\":21826,\"start\":21818},{\"end\":22044,\"start\":22036},{\"end\":22226,\"start\":22218},{\"end\":22706,\"start\":22687},{\"end\":23341,\"start\":23320}]", "table": "[{\"end\":23318,\"start\":22833},{\"end\":25505,\"start\":25076}]", "figure_caption": "[{\"end\":21816,\"start\":21305},{\"end\":22034,\"start\":21828},{\"end\":22211,\"start\":22046},{\"end\":22216,\"start\":22214},{\"end\":22298,\"start\":22228},{\"end\":22664,\"start\":22301},{\"end\":22685,\"start\":22667},{\"end\":22833,\"start\":22708},{\"end\":25076,\"start\":23344}]", "figure_ref": "[{\"end\":4244,\"start\":4237},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14654,\"start\":14646},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14667,\"start\":14659},{\"end\":16838,\"start\":16831},{\"end\":18635,\"start\":18628}]", "bib_author_first_name": "[{\"end\":26178,\"start\":26177},{\"end\":26187,\"start\":26186},{\"end\":26203,\"start\":26202},{\"end\":26219,\"start\":26218},{\"end\":26513,\"start\":26512},{\"end\":26517,\"start\":26514},{\"end\":26532,\"start\":26531},{\"end\":26534,\"start\":26533},{\"end\":26815,\"start\":26814},{\"end\":27117,\"start\":27116},{\"end\":27127,\"start\":27126},{\"end\":27530,\"start\":27529},{\"end\":27541,\"start\":27540},{\"end\":27550,\"start\":27549},{\"end\":27560,\"start\":27559},{\"end\":27571,\"start\":27570},{\"end\":27973,\"start\":27972},{\"end\":27980,\"start\":27979},{\"end\":27987,\"start\":27986},{\"end\":27994,\"start\":27993},{\"end\":28002,\"start\":28001},{\"end\":28015,\"start\":28014},{\"end\":28466,\"start\":28465},{\"end\":28476,\"start\":28475},{\"end\":28485,\"start\":28484},{\"end\":28494,\"start\":28493},{\"end\":28502,\"start\":28501},{\"end\":28515,\"start\":28514},{\"end\":28523,\"start\":28522},{\"end\":28953,\"start\":28952},{\"end\":28966,\"start\":28965},{\"end\":28975,\"start\":28974},{\"end\":28988,\"start\":28987},{\"end\":29000,\"start\":28999},{\"end\":29007,\"start\":29006},{\"end\":29018,\"start\":29017},{\"end\":29762,\"start\":29761},{\"end\":29777,\"start\":29776},{\"end\":29794,\"start\":29793},{\"end\":30152,\"start\":30151},{\"end\":30162,\"start\":30161},{\"end\":30174,\"start\":30173},{\"end\":30180,\"start\":30179},{\"end\":30526,\"start\":30525},{\"end\":30536,\"start\":30535},{\"end\":30545,\"start\":30537},{\"end\":30753,\"start\":30752},{\"end\":30762,\"start\":30761},{\"end\":30772,\"start\":30771},{\"end\":30945,\"start\":30944},{\"end\":31152,\"start\":31151},{\"end\":31167,\"start\":31166},{\"end\":31179,\"start\":31178},{\"end\":31193,\"start\":31192},{\"end\":31206,\"start\":31205},{\"end\":31219,\"start\":31218},{\"end\":31702,\"start\":31701},{\"end\":31947,\"start\":31946},{\"end\":31955,\"start\":31954},{\"end\":31963,\"start\":31962},{\"end\":31976,\"start\":31972},{\"end\":31982,\"start\":31981},{\"end\":31988,\"start\":31987},{\"end\":32330,\"start\":32326},{\"end\":32341,\"start\":32337},{\"end\":32352,\"start\":32348},{\"end\":32361,\"start\":32357},{\"end\":32372,\"start\":32368},{\"end\":32379,\"start\":32378},{\"end\":32391,\"start\":32387},{\"end\":32398,\"start\":32397},{\"end\":32727,\"start\":32726},{\"end\":32729,\"start\":32728},{\"end\":32742,\"start\":32741},{\"end\":32753,\"start\":32752},{\"end\":33080,\"start\":33079},{\"end\":33082,\"start\":33081},{\"end\":33093,\"start\":33092},{\"end\":33100,\"start\":33099},{\"end\":33102,\"start\":33101},{\"end\":33115,\"start\":33114},{\"end\":33125,\"start\":33124},{\"end\":33127,\"start\":33126},{\"end\":33136,\"start\":33135},{\"end\":33453,\"start\":33452},{\"end\":33465,\"start\":33464},{\"end\":33692,\"start\":33691},{\"end\":33698,\"start\":33697},{\"end\":33707,\"start\":33706},{\"end\":33714,\"start\":33713},{\"end\":34111,\"start\":34110},{\"end\":34122,\"start\":34121},{\"end\":34135,\"start\":34134},{\"end\":34146,\"start\":34145},{\"end\":34158,\"start\":34157},{\"end\":34169,\"start\":34168},{\"end\":34459,\"start\":34458},{\"end\":34467,\"start\":34466},{\"end\":34474,\"start\":34473},{\"end\":34723,\"start\":34722},{\"end\":34736,\"start\":34735},{\"end\":35246,\"start\":35245},{\"end\":35255,\"start\":35254},{\"end\":35544,\"start\":35543},{\"end\":35552,\"start\":35551},{\"end\":35563,\"start\":35562},{\"end\":35569,\"start\":35568},{\"end\":35577,\"start\":35576},{\"end\":35591,\"start\":35590},{\"end\":35941,\"start\":35940},{\"end\":35952,\"start\":35951},{\"end\":35962,\"start\":35961},{\"end\":36389,\"start\":36388},{\"end\":36399,\"start\":36398},{\"end\":36887,\"start\":36886},{\"end\":36895,\"start\":36894},{\"end\":36904,\"start\":36903},{\"end\":36913,\"start\":36912},{\"end\":36921,\"start\":36920},{\"end\":37289,\"start\":37288},{\"end\":37300,\"start\":37299},{\"end\":37314,\"start\":37313},{\"end\":37622,\"start\":37621},{\"end\":37633,\"start\":37629},{\"end\":37965,\"start\":37964},{\"end\":37973,\"start\":37972},{\"end\":37981,\"start\":37980},{\"end\":37994,\"start\":37993},{\"end\":38344,\"start\":38343},{\"end\":38351,\"start\":38350},{\"end\":38364,\"start\":38363},{\"end\":38366,\"start\":38365},{\"end\":38377,\"start\":38376},{\"end\":38385,\"start\":38384},{\"end\":38387,\"start\":38386},{\"end\":38396,\"start\":38395},{\"end\":38398,\"start\":38397},{\"end\":38407,\"start\":38406},{\"end\":38409,\"start\":38408},{\"end\":38736,\"start\":38735},{\"end\":38744,\"start\":38743},{\"end\":39032,\"start\":39031},{\"end\":39034,\"start\":39033},{\"end\":39051,\"start\":39050},{\"end\":39062,\"start\":39061},{\"end\":39074,\"start\":39073},{\"end\":39082,\"start\":39081},{\"end\":39084,\"start\":39083},{\"end\":39495,\"start\":39494},{\"end\":39506,\"start\":39505},{\"end\":39847,\"start\":39846},{\"end\":39849,\"start\":39848},{\"end\":39862,\"start\":39861},{\"end\":39876,\"start\":39875},{\"end\":40234,\"start\":40233},{\"end\":40244,\"start\":40243},{\"end\":40579,\"start\":40578},{\"end\":40598,\"start\":40597},{\"end\":40615,\"start\":40614},{\"end\":40984,\"start\":40983},{\"end\":40996,\"start\":40995},{\"end\":40998,\"start\":40997},{\"end\":41009,\"start\":41008},{\"end\":41248,\"start\":41247},{\"end\":41258,\"start\":41257},{\"end\":41771,\"start\":41770},{\"end\":41780,\"start\":41779},{\"end\":41790,\"start\":41789},{\"end\":41988,\"start\":41987},{\"end\":42002,\"start\":42001},{\"end\":42015,\"start\":42014},{\"end\":42017,\"start\":42016},{\"end\":42302,\"start\":42301},{\"end\":42313,\"start\":42312},{\"end\":42320,\"start\":42319},{\"end\":42327,\"start\":42326},{\"end\":42339,\"start\":42338},{\"end\":42347,\"start\":42346},{\"end\":42359,\"start\":42358},{\"end\":42368,\"start\":42367},{\"end\":42381,\"start\":42380},{\"end\":42833,\"start\":42832},{\"end\":42844,\"start\":42843},{\"end\":42857,\"start\":42856},{\"end\":42866,\"start\":42865},{\"end\":42876,\"start\":42875}]", "bib_author_last_name": "[{\"end\":26184,\"start\":26179},{\"end\":26200,\"start\":26188},{\"end\":26216,\"start\":26204},{\"end\":26238,\"start\":26220},{\"end\":26529,\"start\":26518},{\"end\":26540,\"start\":26535},{\"end\":26822,\"start\":26816},{\"end\":27124,\"start\":27118},{\"end\":27133,\"start\":27128},{\"end\":27538,\"start\":27531},{\"end\":27547,\"start\":27542},{\"end\":27557,\"start\":27551},{\"end\":27568,\"start\":27561},{\"end\":27579,\"start\":27572},{\"end\":27977,\"start\":27974},{\"end\":27984,\"start\":27981},{\"end\":27991,\"start\":27988},{\"end\":27999,\"start\":27995},{\"end\":28012,\"start\":28003},{\"end\":28018,\"start\":28016},{\"end\":28473,\"start\":28467},{\"end\":28482,\"start\":28477},{\"end\":28491,\"start\":28486},{\"end\":28499,\"start\":28495},{\"end\":28512,\"start\":28503},{\"end\":28520,\"start\":28516},{\"end\":28532,\"start\":28524},{\"end\":28963,\"start\":28954},{\"end\":28972,\"start\":28967},{\"end\":28985,\"start\":28976},{\"end\":28997,\"start\":28989},{\"end\":29004,\"start\":29001},{\"end\":29015,\"start\":29008},{\"end\":29025,\"start\":29019},{\"end\":29774,\"start\":29763},{\"end\":29791,\"start\":29778},{\"end\":29806,\"start\":29795},{\"end\":30159,\"start\":30153},{\"end\":30171,\"start\":30163},{\"end\":30177,\"start\":30175},{\"end\":30189,\"start\":30181},{\"end\":30533,\"start\":30527},{\"end\":30552,\"start\":30546},{\"end\":30759,\"start\":30754},{\"end\":30769,\"start\":30763},{\"end\":30779,\"start\":30773},{\"end\":30957,\"start\":30946},{\"end\":31164,\"start\":31153},{\"end\":31176,\"start\":31168},{\"end\":31190,\"start\":31180},{\"end\":31203,\"start\":31194},{\"end\":31216,\"start\":31207},{\"end\":31227,\"start\":31220},{\"end\":31709,\"start\":31703},{\"end\":31952,\"start\":31948},{\"end\":31960,\"start\":31956},{\"end\":31970,\"start\":31964},{\"end\":31979,\"start\":31977},{\"end\":31985,\"start\":31983},{\"end\":31996,\"start\":31989},{\"end\":32335,\"start\":32331},{\"end\":32346,\"start\":32342},{\"end\":32355,\"start\":32353},{\"end\":32366,\"start\":32362},{\"end\":32376,\"start\":32373},{\"end\":32385,\"start\":32380},{\"end\":32395,\"start\":32392},{\"end\":32407,\"start\":32399},{\"end\":32739,\"start\":32730},{\"end\":32750,\"start\":32743},{\"end\":32760,\"start\":32754},{\"end\":33090,\"start\":33083},{\"end\":33097,\"start\":33094},{\"end\":33112,\"start\":33103},{\"end\":33122,\"start\":33116},{\"end\":33133,\"start\":33128},{\"end\":33144,\"start\":33137},{\"end\":33462,\"start\":33454},{\"end\":33475,\"start\":33466},{\"end\":33695,\"start\":33693},{\"end\":33704,\"start\":33699},{\"end\":33711,\"start\":33708},{\"end\":33718,\"start\":33715},{\"end\":34119,\"start\":34112},{\"end\":34132,\"start\":34123},{\"end\":34143,\"start\":34136},{\"end\":34155,\"start\":34147},{\"end\":34166,\"start\":34159},{\"end\":34177,\"start\":34170},{\"end\":34464,\"start\":34460},{\"end\":34471,\"start\":34468},{\"end\":34478,\"start\":34475},{\"end\":34733,\"start\":34724},{\"end\":34742,\"start\":34737},{\"end\":35252,\"start\":35247},{\"end\":35263,\"start\":35256},{\"end\":35549,\"start\":35545},{\"end\":35560,\"start\":35553},{\"end\":35566,\"start\":35564},{\"end\":35574,\"start\":35570},{\"end\":35588,\"start\":35578},{\"end\":35596,\"start\":35592},{\"end\":35949,\"start\":35942},{\"end\":35959,\"start\":35953},{\"end\":35968,\"start\":35963},{\"end\":36396,\"start\":36390},{\"end\":36405,\"start\":36400},{\"end\":36892,\"start\":36888},{\"end\":36901,\"start\":36896},{\"end\":36910,\"start\":36905},{\"end\":36918,\"start\":36914},{\"end\":36930,\"start\":36922},{\"end\":37297,\"start\":37290},{\"end\":37311,\"start\":37301},{\"end\":37323,\"start\":37315},{\"end\":37627,\"start\":37623},{\"end\":37637,\"start\":37634},{\"end\":37970,\"start\":37966},{\"end\":37978,\"start\":37974},{\"end\":37991,\"start\":37982},{\"end\":38005,\"start\":37995},{\"end\":38348,\"start\":38345},{\"end\":38361,\"start\":38352},{\"end\":38374,\"start\":38367},{\"end\":38382,\"start\":38378},{\"end\":38393,\"start\":38388},{\"end\":38404,\"start\":38399},{\"end\":38414,\"start\":38410},{\"end\":38741,\"start\":38737},{\"end\":38748,\"start\":38745},{\"end\":39048,\"start\":39035},{\"end\":39059,\"start\":39052},{\"end\":39071,\"start\":39063},{\"end\":39079,\"start\":39075},{\"end\":39096,\"start\":39085},{\"end\":39503,\"start\":39496},{\"end\":39512,\"start\":39507},{\"end\":39859,\"start\":39850},{\"end\":39873,\"start\":39863},{\"end\":39885,\"start\":39877},{\"end\":40241,\"start\":40235},{\"end\":40250,\"start\":40245},{\"end\":40595,\"start\":40580},{\"end\":40612,\"start\":40599},{\"end\":40627,\"start\":40616},{\"end\":40993,\"start\":40985},{\"end\":41006,\"start\":40999},{\"end\":41019,\"start\":41010},{\"end\":41255,\"start\":41249},{\"end\":41264,\"start\":41259},{\"end\":41777,\"start\":41772},{\"end\":41787,\"start\":41781},{\"end\":41796,\"start\":41791},{\"end\":41999,\"start\":41989},{\"end\":42012,\"start\":42003},{\"end\":42024,\"start\":42018},{\"end\":42310,\"start\":42303},{\"end\":42317,\"start\":42314},{\"end\":42324,\"start\":42321},{\"end\":42336,\"start\":42328},{\"end\":42344,\"start\":42340},{\"end\":42356,\"start\":42348},{\"end\":42365,\"start\":42360},{\"end\":42378,\"start\":42369},{\"end\":42392,\"start\":42382},{\"end\":42841,\"start\":42834},{\"end\":42854,\"start\":42845},{\"end\":42863,\"start\":42858},{\"end\":42873,\"start\":42867},{\"end\":42882,\"start\":42877}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":25470164},\"end\":26459,\"start\":26105},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":29827452},\"end\":26726,\"start\":26461},{\"attributes\":{\"id\":\"b2\"},\"end\":27072,\"start\":26728},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12472196},\"end\":27470,\"start\":27074},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206858072},\"end\":27878,\"start\":27472},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14068125},\"end\":28335,\"start\":27880},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18984747},\"end\":28867,\"start\":28337},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":19331986},\"end\":29710,\"start\":28869},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206720388},\"end\":30062,\"start\":29712},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14337063},\"end\":30433,\"start\":30064},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4960282},\"end\":30735,\"start\":30435},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1779661},\"end\":30895,\"start\":30737},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11715509},\"end\":31086,\"start\":30897},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207242933},\"end\":31663,\"start\":31088},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":207178999},\"end\":31891,\"start\":31665},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":57246310},\"end\":32261,\"start\":31893},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14987220},\"end\":32660,\"start\":32263},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8971953},\"end\":32991,\"start\":32662},{\"attributes\":{\"doi\":\"arXiv:1602.07360\",\"id\":\"b18\"},\"end\":33382,\"start\":32993},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b19\"},\"end\":33643,\"start\":33384},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206594692},\"end\":34046,\"start\":33645},{\"attributes\":{\"doi\":\"arXiv:1404.1869\",\"id\":\"b21\"},\"end\":34390,\"start\":34048},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":39028201},\"end\":34638,\"start\":34392},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":284959},\"end\":35149,\"start\":34640},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5808102},\"end\":35499,\"start\":35151},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1548631},\"end\":35870,\"start\":35501},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14072575},\"end\":36287,\"start\":35872},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14915460},\"end\":36829,\"start\":36289},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206822288},\"end\":37223,\"start\":36831},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12726540},\"end\":37555,\"start\":37225},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":207240186},\"end\":37900,\"start\":37557},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":304354},\"end\":38301,\"start\":37902},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":19009075},\"end\":38668,\"start\":38303},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":30304018},\"end\":38936,\"start\":38670},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12227279},\"end\":39412,\"start\":38938},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4067210},\"end\":39789,\"start\":39414},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5808384},\"end\":40154,\"start\":39791},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12478923},\"end\":40494,\"start\":40156},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":41420943},\"end\":40930,\"start\":40496},{\"attributes\":{\"doi\":\"arXiv:1612.06543\",\"id\":\"b39\"},\"end\":41188,\"start\":40932},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15628580},\"end\":41720,\"start\":41190},{\"attributes\":{\"id\":\"b41\"},\"end\":41920,\"start\":41722},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":195908774},\"end\":42267,\"start\":41922},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206592484},\"end\":42771,\"start\":42269},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206593880},\"end\":43249,\"start\":42773}]", "bib_title": "[{\"end\":26175,\"start\":26105},{\"end\":26510,\"start\":26461},{\"end\":26812,\"start\":26728},{\"end\":27114,\"start\":27074},{\"end\":27527,\"start\":27472},{\"end\":27970,\"start\":27880},{\"end\":28463,\"start\":28337},{\"end\":28950,\"start\":28869},{\"end\":29759,\"start\":29712},{\"end\":30149,\"start\":30064},{\"end\":30523,\"start\":30435},{\"end\":30750,\"start\":30737},{\"end\":30942,\"start\":30897},{\"end\":31149,\"start\":31088},{\"end\":31699,\"start\":31665},{\"end\":31944,\"start\":31893},{\"end\":32324,\"start\":32263},{\"end\":32724,\"start\":32662},{\"end\":33689,\"start\":33645},{\"end\":34456,\"start\":34392},{\"end\":34720,\"start\":34640},{\"end\":35243,\"start\":35151},{\"end\":35541,\"start\":35501},{\"end\":35938,\"start\":35872},{\"end\":36386,\"start\":36289},{\"end\":36884,\"start\":36831},{\"end\":37286,\"start\":37225},{\"end\":37619,\"start\":37557},{\"end\":37962,\"start\":37902},{\"end\":38341,\"start\":38303},{\"end\":38733,\"start\":38670},{\"end\":39029,\"start\":38938},{\"end\":39492,\"start\":39414},{\"end\":39844,\"start\":39791},{\"end\":40231,\"start\":40156},{\"end\":40576,\"start\":40496},{\"end\":41245,\"start\":41190},{\"end\":41985,\"start\":41922},{\"end\":42299,\"start\":42269},{\"end\":42830,\"start\":42773}]", "bib_author": "[{\"end\":26186,\"start\":26177},{\"end\":26202,\"start\":26186},{\"end\":26218,\"start\":26202},{\"end\":26240,\"start\":26218},{\"end\":26531,\"start\":26512},{\"end\":26542,\"start\":26531},{\"end\":26824,\"start\":26814},{\"end\":27126,\"start\":27116},{\"end\":27135,\"start\":27126},{\"end\":27540,\"start\":27529},{\"end\":27549,\"start\":27540},{\"end\":27559,\"start\":27549},{\"end\":27570,\"start\":27559},{\"end\":27581,\"start\":27570},{\"end\":27979,\"start\":27972},{\"end\":27986,\"start\":27979},{\"end\":27993,\"start\":27986},{\"end\":28001,\"start\":27993},{\"end\":28014,\"start\":28001},{\"end\":28020,\"start\":28014},{\"end\":28475,\"start\":28465},{\"end\":28484,\"start\":28475},{\"end\":28493,\"start\":28484},{\"end\":28501,\"start\":28493},{\"end\":28514,\"start\":28501},{\"end\":28522,\"start\":28514},{\"end\":28534,\"start\":28522},{\"end\":28965,\"start\":28952},{\"end\":28974,\"start\":28965},{\"end\":28987,\"start\":28974},{\"end\":28999,\"start\":28987},{\"end\":29006,\"start\":28999},{\"end\":29017,\"start\":29006},{\"end\":29027,\"start\":29017},{\"end\":29776,\"start\":29761},{\"end\":29793,\"start\":29776},{\"end\":29808,\"start\":29793},{\"end\":30161,\"start\":30151},{\"end\":30173,\"start\":30161},{\"end\":30179,\"start\":30173},{\"end\":30191,\"start\":30179},{\"end\":30535,\"start\":30525},{\"end\":30554,\"start\":30535},{\"end\":30761,\"start\":30752},{\"end\":30771,\"start\":30761},{\"end\":30781,\"start\":30771},{\"end\":30959,\"start\":30944},{\"end\":31166,\"start\":31151},{\"end\":31178,\"start\":31166},{\"end\":31192,\"start\":31178},{\"end\":31205,\"start\":31192},{\"end\":31218,\"start\":31205},{\"end\":31229,\"start\":31218},{\"end\":31711,\"start\":31701},{\"end\":31954,\"start\":31946},{\"end\":31962,\"start\":31954},{\"end\":31972,\"start\":31962},{\"end\":31981,\"start\":31972},{\"end\":31987,\"start\":31981},{\"end\":31998,\"start\":31987},{\"end\":32337,\"start\":32326},{\"end\":32348,\"start\":32337},{\"end\":32357,\"start\":32348},{\"end\":32368,\"start\":32357},{\"end\":32378,\"start\":32368},{\"end\":32387,\"start\":32378},{\"end\":32397,\"start\":32387},{\"end\":32409,\"start\":32397},{\"end\":32741,\"start\":32726},{\"end\":32752,\"start\":32741},{\"end\":32762,\"start\":32752},{\"end\":33092,\"start\":33079},{\"end\":33099,\"start\":33092},{\"end\":33114,\"start\":33099},{\"end\":33124,\"start\":33114},{\"end\":33135,\"start\":33124},{\"end\":33146,\"start\":33135},{\"end\":33464,\"start\":33452},{\"end\":33477,\"start\":33464},{\"end\":33697,\"start\":33691},{\"end\":33706,\"start\":33697},{\"end\":33713,\"start\":33706},{\"end\":33720,\"start\":33713},{\"end\":34121,\"start\":34110},{\"end\":34134,\"start\":34121},{\"end\":34145,\"start\":34134},{\"end\":34157,\"start\":34145},{\"end\":34168,\"start\":34157},{\"end\":34179,\"start\":34168},{\"end\":34466,\"start\":34458},{\"end\":34473,\"start\":34466},{\"end\":34480,\"start\":34473},{\"end\":34735,\"start\":34722},{\"end\":34744,\"start\":34735},{\"end\":35254,\"start\":35245},{\"end\":35265,\"start\":35254},{\"end\":35551,\"start\":35543},{\"end\":35562,\"start\":35551},{\"end\":35568,\"start\":35562},{\"end\":35576,\"start\":35568},{\"end\":35590,\"start\":35576},{\"end\":35598,\"start\":35590},{\"end\":35951,\"start\":35940},{\"end\":35961,\"start\":35951},{\"end\":35970,\"start\":35961},{\"end\":36398,\"start\":36388},{\"end\":36407,\"start\":36398},{\"end\":36894,\"start\":36886},{\"end\":36903,\"start\":36894},{\"end\":36912,\"start\":36903},{\"end\":36920,\"start\":36912},{\"end\":36932,\"start\":36920},{\"end\":37299,\"start\":37288},{\"end\":37313,\"start\":37299},{\"end\":37325,\"start\":37313},{\"end\":37629,\"start\":37621},{\"end\":37639,\"start\":37629},{\"end\":37972,\"start\":37964},{\"end\":37980,\"start\":37972},{\"end\":37993,\"start\":37980},{\"end\":38007,\"start\":37993},{\"end\":38350,\"start\":38343},{\"end\":38363,\"start\":38350},{\"end\":38376,\"start\":38363},{\"end\":38384,\"start\":38376},{\"end\":38395,\"start\":38384},{\"end\":38406,\"start\":38395},{\"end\":38416,\"start\":38406},{\"end\":38743,\"start\":38735},{\"end\":38750,\"start\":38743},{\"end\":39050,\"start\":39031},{\"end\":39061,\"start\":39050},{\"end\":39073,\"start\":39061},{\"end\":39081,\"start\":39073},{\"end\":39098,\"start\":39081},{\"end\":39505,\"start\":39494},{\"end\":39514,\"start\":39505},{\"end\":39861,\"start\":39846},{\"end\":39875,\"start\":39861},{\"end\":39887,\"start\":39875},{\"end\":40243,\"start\":40233},{\"end\":40252,\"start\":40243},{\"end\":40597,\"start\":40578},{\"end\":40614,\"start\":40597},{\"end\":40629,\"start\":40614},{\"end\":40995,\"start\":40983},{\"end\":41008,\"start\":40995},{\"end\":41021,\"start\":41008},{\"end\":41257,\"start\":41247},{\"end\":41266,\"start\":41257},{\"end\":41779,\"start\":41770},{\"end\":41789,\"start\":41779},{\"end\":41798,\"start\":41789},{\"end\":42001,\"start\":41987},{\"end\":42014,\"start\":42001},{\"end\":42026,\"start\":42014},{\"end\":42312,\"start\":42301},{\"end\":42319,\"start\":42312},{\"end\":42326,\"start\":42319},{\"end\":42338,\"start\":42326},{\"end\":42346,\"start\":42338},{\"end\":42358,\"start\":42346},{\"end\":42367,\"start\":42358},{\"end\":42380,\"start\":42367},{\"end\":42394,\"start\":42380},{\"end\":42843,\"start\":42832},{\"end\":42856,\"start\":42843},{\"end\":42865,\"start\":42856},{\"end\":42875,\"start\":42865},{\"end\":42884,\"start\":42875}]", "bib_venue": "[{\"end\":26255,\"start\":26240},{\"end\":26566,\"start\":26542},{\"end\":26877,\"start\":26824},{\"end\":27222,\"start\":27135},{\"end\":27651,\"start\":27581},{\"end\":28081,\"start\":28020},{\"end\":28576,\"start\":28534},{\"end\":29193,\"start\":29027},{\"end\":29860,\"start\":29808},{\"end\":30222,\"start\":30191},{\"end\":30563,\"start\":30554},{\"end\":30787,\"start\":30781},{\"end\":30974,\"start\":30959},{\"end\":31316,\"start\":31229},{\"end\":31755,\"start\":31711},{\"end\":32037,\"start\":31998},{\"end\":32444,\"start\":32409},{\"end\":32800,\"start\":32762},{\"end\":33077,\"start\":32993},{\"end\":33450,\"start\":33384},{\"end\":33797,\"start\":33720},{\"end\":34108,\"start\":34048},{\"end\":34494,\"start\":34480},{\"end\":34836,\"start\":34744},{\"end\":35309,\"start\":35265},{\"end\":35635,\"start\":35598},{\"end\":36038,\"start\":35970},{\"end\":36504,\"start\":36407},{\"end\":37006,\"start\":36932},{\"end\":37363,\"start\":37325},{\"end\":37691,\"start\":37639},{\"end\":38078,\"start\":38007},{\"end\":38435,\"start\":38416},{\"end\":38780,\"start\":38750},{\"end\":39147,\"start\":39098},{\"end\":39579,\"start\":39514},{\"end\":39949,\"start\":39887},{\"end\":40299,\"start\":40252},{\"end\":40686,\"start\":40629},{\"end\":40981,\"start\":40932},{\"end\":41383,\"start\":41266},{\"end\":41768,\"start\":41722},{\"end\":42075,\"start\":42026},{\"end\":42471,\"start\":42394},{\"end\":42961,\"start\":42884},{\"end\":27296,\"start\":27224},{\"end\":29346,\"start\":29195},{\"end\":31390,\"start\":31318},{\"end\":33861,\"start\":33799},{\"end\":34915,\"start\":34838},{\"end\":36102,\"start\":36040},{\"end\":36597,\"start\":36506},{\"end\":37730,\"start\":37693},{\"end\":38441,\"start\":38437},{\"end\":41487,\"start\":41385},{\"end\":42535,\"start\":42473},{\"end\":43025,\"start\":42963}]"}}}, "year": 2023, "month": 12, "day": 17}