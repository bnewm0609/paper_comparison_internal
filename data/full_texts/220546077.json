{"id": 220546077, "updated": "2023-10-06 12:55:59.199", "metadata": {"title": "Data Poisoning Attacks Against Federated Learning Systems", "authors": "[{\"first\":\"Vale\",\"last\":\"Tolpegin\",\"middle\":[]},{\"first\":\"Stacey\",\"last\":\"Truex\",\"middle\":[]},{\"first\":\"Mehmet\",\"last\":\"Gursoy\",\"middle\":[\"Emre\"]},{\"first\":\"Ling\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "Computer Security \u2013 ESORICS 2020", "journal": "Computer Security \u2013 ESORICS 2020", "publication_date": {"year": 2020, "month": 7, "day": 16}, "abstract": "Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants' data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2007.08432", "mag": "3043061209", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/esorics/TolpeginTGL20", "doi": "10.1007/978-3-030-58951-6_24"}}, "content": {"source": {"pdf_hash": "55791f97a6ad3cf4173fa2d0900aeed3022e2bbb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.08432v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.08432", "status": "GREEN"}}, "grobid": {"id": "31b9b4491f0dd3b0afc01510aebccc63fb11ded2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/55791f97a6ad3cf4173fa2d0900aeed3022e2bbb.txt", "contents": "\nData Poisoning Attacks Against Federated Learning Systems\n\n\nVale Tolpegin vtolpegin3@gatech.edu \nGeorgia Institute of Technology\n30332AtlantaGAUSA\n\nMehmetStacey Truex staceytruex@gatech.edu \nGeorgia Institute of Technology\n30332AtlantaGAUSA\n\nEmre Gursoy memregursoy@gatech.edu \nGeorgia Institute of Technology\n30332AtlantaGAUSA\n\nLing Liu ling.liu@cc.gatech.edu \nGeorgia Institute of Technology\n30332AtlantaGAUSA\n\nData Poisoning Attacks Against Federated Learning Systems\nFederated learning \u00b7 Adversarial machine learning \u00b7 Label flipping \u00b7 Data poisoning \u00b7 Deep learning\nFederated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants' data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.\n\nIntroduction\n\nMachine learning (ML) has become ubiquitous in today's society as a range of industries deploy predictive models into their daily workflows. This environment has not only put a premium on the ML model training and hosting technologies but also on the rich data that companies are collecting about their users to train and inform such models. Companies and users alike are consequently faced with 2 fundamental questions in this reality of ML: (1) How can privacy concerns around such pervasive data collection be moderated without sacrificing the efficacy of ML models? and (2) How can ML models be trusted as accurate predictors? Federated ML has seen increased adoption in recent years [17,40,9] in response to the growing legislative demand to address user privacy [1,26,38]. Federated learning (FL) allows data to remain at the edge with only model parameters being shared with a central server. Specifically, there is no centralized data curator who collects and verifies an aggregate dataset. Instead, each data holder (participant) is responsible for conducting training on their local data. In regular arXiv:2007.08432v2 [cs.LG] 11 Aug 2020 intervals participants are then send model parameter values to a central parameter server or aggregator where a global model is created through aggregation of the individual updates. A global model can thus be trained over all participants' data without any individual participant needing to share their private raw data.\n\nWhile FL systems allow participants to keep their raw data local, a significant vulnerability is introduced at the heart of question (2). Consider the scenario wherein a subset of participants are either malicious or have been compromised by some adversary. This can lead to these participants having mislabeled or poisonous samples in their local training data. With no central authority able to validate data, these malicious participants can consequently poison the trained global model. For example, consider Microsoft's AI chat bot Tay. Tay was released on Twitter with the underlying natural language processing model set to learn from the Twitter users it interacted with. Thanks to malicious users, Tay was quickly manipulated to learn offensive and racist language [41].\n\nIn this paper, we study the vulnerability of FL systems to malicious participants seeking to poison the globally trained model. We make minimal assumptions on the capability of a malicious FL participant -each can only manipulate the raw training data on their device. This allows for non-expert malicious participants to achieve poisoning with no knowledge of model type, parameters, and FL process. Under this set of assumptions, label flipping attacks become a feasible strategy to implement data poisoning, attacks which have been shown to be effective against traditional, centralized ML models [5,44,50,52]. We investigate their application to FL systems using complex deep neural network models.\n\nWe demonstrate our FL poisoning attacks using two popular image classification datasets: CIFAR-10 and Fashion-MNIST. Our results yield several interesting findings. First, we show that attack effectiveness (decrease in model utility) depends on the percentage of malicious users and the attack is effective even when this percentage is small. Second, we show that attacks can be targeted, i.e., they have large negative impact on the subset of classes that are under attack, but have little to no impact on remaining classes. This is desirable for adversaries who wish to poison a subset of classes while not completely corrupting the global model to avoid easy detection. Third, we evaluate the impact of attack timing (poisoning in early or late rounds of FL training) and the impact of malicious participant availability (whether malicious participants can increase their availability and selection rate to increase effectiveness). Motivated by our finding that the global model may still converge accurately after early-round poisoning stops, we conclude that largest poisoning impact can be achieved if malicious users participate in later rounds and with high availability.\n\nGiven the highly effective poisoning threat to FL systems, we then propose a defense strategy for the FL aggregator to identify malicious participants using their model updates. Our defense is based on the insight that updates sent from malicious participants have unique characteristics compared to honest participants' updates. Our defense extracts relevant parameters from the high-dimensional update vectors and applies PCA for dimensionality reduction. Results on CIFAR-10 and Fashion-MNIST across varying malicious participant rates (2-20%) show that the aggregator can obtain clear separation between malicious and honest participants' respective updates using our defense strategy. This enables the FL aggregator to identify and block malicious participants.\n\nThe rest of this paper is organized as follows. In Section 2, we introduce the FL setting, threat model, attack strategy, and attack evaluation metrics. In Section 3, we demonstrate the effectiveness of FL poisoning attacks and analyze their impact with respect to malicious participant percentage, choice of classes under attack, attack timing, and malicious participant availability. In Section 4, we describe and empirically demonstrate our defense strategy. We discuss related work in Section 5 and conclude in Section 6. Our source code is available 1 .\n\n\nPreliminaries and Attack Formulation\n\n\nFederated Machine Learning\n\nFL systems allow global model training without the sharing of raw private data. Instead, individual participants only share model parameter updates. Consider a deep neural network (DNN) model. DNNs consist of multiple layers of nodes where each node is a basic functional unit with a corresponding set of parameters. Nodes receive input from the immediately preceding layer and send output to the following layer; with the first layer nodes receiving input from the training data and the final layer nodes generating the predictive result.\n\nIn a traditional DNN learning scenario, there exists a training dataset D = (x 1 , ..., x n ) and a loss function L. Each x i \u2208 D is defined as a set of features f i and a class label c i \u2208 C where C is the set of all possible class values. The final layer of a DNN architecture for such a dataset will consequently contain |C| nodes, each corresponding to a different class in C. The loss of this DNN given parameters \u03b8 on D is denoted: L = 1 n n i L(\u03b8, x i ). When f i is fed through the DNN with model parameters \u03b8, the output is a set of predicted probabilities p i . Each value p c,i \u2208 p i is the predicted probability that x i has a class value c, and p i contains a probability p c,i for each class value c \u2208 C. Each predicted probability p c,i is computed by a node n c in the final layer of the DNN architecture using input received from the preceding layer and n c 's corresponding parameters in \u03b8. The predicted class for instance x i given a model M with parameters \u03b8 then becomes M \u03b8 (x i ) = argmax c\u2208C p c,i . Given a cross entropy loss function, the loss on x i can consequently can be calculated as L(\u03b8, x i ) = \u2212 c\u2208C y c,i log(p c,i ) where y c,i = 1 if c = c i and 0 otherwise. The goal of training a DNN model then becomes to find the parameter values for \u03b8 which minimize the chosen loss function L.\n\nThe process of minimizing this loss is typically done through an iterative process called stochastic gradient descent (SGD). At each step, the SGD algorithm (1) selects a batch of samples B \u2286 D, (2) computes the corresponding gradient g B = 1 |B| x\u2208B \u2207 \u03b8 L(\u03b8, x), and (3) then updates \u03b8 in the direction \u2212g B . In practice, D is shuffled and then evenly divided into |B| sized batches such that each sample occurs in exactly one batch. Applying SGD iteratively to each of the pre-determined batches is then referred to as one epoch.\n\nIn FL environments however, the training dataset D is not wholly available at the aggregator. Instead, N participants P each hold their own private training dataset D 1 , ..., D N . Rather than sharing their private raw data, participants instead execute the SGD training algorithm locally and then upload updated parameters to a centralized server (aggregator). Specifically, in the initialization phase (i.e., round 0), the aggregator generates a DNN architecture with parameters \u03b8 0 which is advertised to all participants. At each global training round r, a subset P r consisting of k \u2264 N participants is selected based on availability. Each participant P i \u2208 P r executes one epoch of SGD locally on D i to obtain updated parameters \u03b8 r,i , which are sent to the aggregator. The aggregator sets the global parameters \u03b8 r = 1 k i \u03b8 r,i \u2200i where P i \u2208 P r . The global parameters \u03b8 r are then advertised to all N participants. These global parameters at the end of round r are used in the next training round r + 1. After R total global training rounds, the model M is finalized with parameters \u03b8 R .\n\n\nThreat and Adversary Model\n\nThreat Model: We consider the scenario in which a subset of FL participants are malicious or are controlled by a malicious adversary. We denote the percentage of malicious participants among all participants P as m%. Malicious participants may be injected to the system by adding adversary-controlled devices, compromising m% of the benign participants' devices, or incentivizing (bribing) m% of benign participants to poison the global model for a certain number of FL rounds. We consider the aggregator to be honest and not compromised.\n\nAdversarial Goal: The goal of the adversary is to manipulate the learned parameters such that the final global model M has high errors for particular classes (a subset of C). The adversary is thereby conducting a targeted poisoning attack. This differs from untargeted attacks which instead seek indiscriminate high global model errors across all classes [6,14,51]. Targeted attacks have the desirable property that they decrease the possibility of the poisoning attack being detected by minimizing influence on non-targeted classes.\n\nAdversary Knowledge and Capability: We consider a realistic adversary model with the following constraints. Each malicious participant can manipulate the training data D i on their own device, but cannot access or manipulate other participants' data or the model learning process, e.g., SGD implementation, loss function, or server aggregation process. The attack is not specific to the DNN architecture, loss function or optimization function being used. It requires training data to be corrupted, but the learning algorithm remains unaltered.\n\n\nLabel Flipping Attacks in Federated Learning\n\nWe use a label flipping attack to implement targeted data poisoning in FL. Given a source class c src and a target class c target from C, each malicious participant P i modifies their dataset D i as follows: For all instances in D i whose class is c src , change their class to c target . We denote this attack by c src \u2192 c target . For example, in CIFAR-10 image classification, airplane \u2192 bird denotes that images whose original class labels are airplane will be poisoned by malicious participants by changing their class to bird. The goal of the attack is to make the final global model M more likely to misclassify airplane images as bird images at test time.\n\nLabel flipping is a well-known attack in centralized ML [43,44,50,52]. It is also suitable for the FL scenario given the adversarial goal and capabilities above. Unlike other types of poisoning attacks, label flipping does not require the adversary to know the global distribution of D, the DNN architecture, loss function L, etc. It is time and energy-efficient, an attractive feature considering FL is often executed on edge devices. It is also easy to carry out for non-experts and does not require modification or tampering with participant-side FL software. T Pi+F Ni \u00b7 100% where T P i is the number of instances x \u2208 D test where M \u03b8 R (x) = c i and c i is the true class label of x; and F N i is the number of instances x \u2208 D test where M \u03b8 R (x) = c i and the true class label of x is c i . Baseline Misclassification Count (m cnt i j ): Let M N P be a global model trained for R rounds using FL without any malicious attack. For classes c i = c j , the baseline misclassification count from c i to c j , denoted m cnt i j , is defined as the number of instances x \u2208 D test where M N P (x) = c j and the true class of x is c i . Table 1 provides a summary of the notation used in the rest of this paper.\n\n\nAnalysis of Label Flipping Attacks in FL\n\n\nExperimental Setup\n\nDatasets and DNN Architectures: We conduct our attacks using two popular image classification datasets: CIFAR-10 [22] and Fashion-MNIST [49]. CIFAR-10 consists of 60,000 color images in 10 object classes such as deer, airplane, and dog with 6,000 images included per class. The complete dataset is pre-divided into 50,000 training images and 10,000 test images. Fashion-MNIST consists of a training set of 60,000 images and a test set of 10,000 images. Each image in Fashion-MNIST is gray-scale and associated with one of 10 classes of clothing such as pullover, ankle boot, or bag. In experiments with CIFAR-10, we use a convolutional neural network with six convolutional layers, batch normalization, and two fully connected dense layers. This DNN architecture achieves a test accuracy of 79.90% in the centralized learning scenario, i.e. N = 1, without poisoning. In experiments with Fashion-MNIST, we use a two layer convolutional neural network with batch normalization, an architecture which achieves 91.75% test accuracy in the centralized scenario without poisoning. Further details of the datasets and DNN model architectures can be found in Appendix A. Federated Learning Setup: We implement FL in Python using the Py-Torch [35] library. By default, we have N = 50 participants, one central aggregator, and k = 5. We use an independent and identically distributed (iid ) data distribution, i.e., we assume the total training dataset is uniformly randomly distributed among all participants with each participant receiving a unique subset of the training data. The testing data is used for model evaluation only and is therefore not included in any participant P i 's train dataset D i . Observing that both DNN models converge after fewer than 200 training rounds, we set our FL experiments to run for R = 200 rounds total.\n\nLabel Flipping Process: In order to simulate the label flipping attack in a FL system with N participants of which m% are malicious, at the start of each experiment we randomly designate N \u00d7 m% of the participants from P as malicious. The rest are honest. To address the impact of random selection of malicious participants, by default we repeat each experiment 10 times and report the average results. Unless otherwise stated, we use m = 10%.\n\nFor both datasets we consider three label flipping attack settings representing a diverse set of conditions in which to base adversarial attacks. These conditions include (1) a source class \u2192 target class pairing whose source class was very frequently misclassified as the target class in federated, non-poisoned training, (2) a pairing where the source class was very infrequently misclassified as the target class, and (3) a pairing between these two extremes. Specifically, for CIFAR-10 we test (1) 5: dog \u2192 3: cat, (2) 0: airplane \u2192 2: bird, and (3) 1: automobile \u2192 9: truck. For Fashion-MNIST we experiment with (1) 6: shirt \u2192 0: t-shirt/top, (2) 1: trouser \u2192 3: dress, and (3) 4: coat \u2192 6: shirt.\n\n\nLabel Flipping Attack Feasibility\n\nWe start by investigating the feasibility of poisoning FL systems using label flipping attacks. Figure 1 outlines the global model accuracy and source class recall in scenarios with malicious participant percentage m ranging from 2% to 50%. Results demonstrate that as the malicious participant percentage, increases the global model utility (test accuracy) decreases. Even with small m, we observe a decrease in model accuracy compared to a non-poisoned model (denoted by M N P in the graphs), and there is an even larger decrease in source class recall. In experiments with CIFAR-10, once m reaches 40%, the recall of the source class decreases to 0% and the global model accuracy decreases from 78.3% in the  non-poisoned setting to 74.4% in the poisoned setting. Experiments conducted on Fashion-MNIST show a similar pattern of utility loss. With m = 4% source class recall drops by \u223c 10% and with m = 10% it drops by \u223c 20%. It is therefore clear that an adversary who controls even a minor proportion of the total participant population is capable of significantly impacting global model utility.\n\nWhile both datasets are vulnerable to label flipping attacks, the degree of vulnerability varies between datasets with CIFAR-10 demonstrating more vulnerability than Fashion-MNIST. For example, consider the 30% malicious scenario, Figure 1b shows the source class recall for the CIFAR-10 dataset drops to 19.7% while Figure 1d shows a much lower decrease for the Fashion-MNIST dataset with 58.2% source class recall under the same experimental settings.\n\nOn the other hand, vulnerability variation based on source and target class settings is less clear. In Table 2, we report the results of three different combinations of source \u2192 target attacks for each dataset. Consider the two extreme settings for the CIFAR-10 dataset: on the low end the 0 \u2192 2 setting has a baseline misclassification count of 16 while the high end count is 200 for the 5 \u2192 3 setting. Because of the DNN's relative challenge in differentiating class 5 from class 3 in the non-poisoned setting, it could be anticipated that conducting a  Table (3) Changes due to poisoning in source class recall, target class recall, and total recall for all remaining classes (non-source, non-target). Results are averaged from 10 runs in each setting. The maximum standard deviation observed was 1.45% in source class recall and 1.13% in target class recall.\n\nlabel flipping attack within the 5 \u2192 3 setting would result in the greatest impact on source class recall. However, this was not the case. Table 2 shows that in only two out of the six experimental scenarios did 5 \u2192 3 record the largest drop in source class recall. In fact, four scenarios' results show the 0 \u2192 2 setting, the setting with the lowest baseline misclassification count, as the most effective option for the adversary. Experiments with Fashion-MNIST show a similar trend, with label flipping attacks conducted in the 4 \u2192 6 setting being the most successful rather than the 6 \u2192 0 setting which has more than 2\u00d7 the number of baseline misclassifications. These results indicate that identifying the most vulnerable source and target class combination may be a non-trivial task for the adversary, and that there is not necessarily a correlation between non-poisoned misclassification performance and attack effectiveness. We additionally study a desirable feature of the label flipping attack: they appear to be targeted. Specifically, Table 3 reports the following quantities for each source \u2192 target flipping scenario: loss in source class recall, loss in target class recall, and loss in recall of all remaining classes. We observe that the attack causes substantial change in source class recall (> 6% drop in most cases) and target class recall. However, the attack impact on the recall of remaining classes is an order of magnitude smaller. CIFAR-10 experiments show a maximum of 0.34% change in class recalls attributable to non-source and non-target classes and Fashion-MNIST experiments similarly show a maximum change of 0.2% attributable to non-source and non-target classes, both of which are relatively minor compared to source and target classes. Thus, the attack is causing the global model to misclassify instances belonging to c src as c target at test time while other classes remain relatively unimpacted, demonstrating its targeted nature towards c src and c target . Considering the large impact of the attack on source class recall, changes in source class recall therefore make up the vast majority of the decreases in global model accuracy caused by label flipping attacks in FL systems. This observation can also be seen in Figure 2 where the change in global model accuracy closely follows the change in source class recall.\n\nThe targeted nature of the label flipping attack allows for adversaries to remain under the radar in many FL systems. Consider systems where the data contain 100 classes or more, as is the case in CIFAR-100 [22] and ImageNet [13]. In such cases, targeted attacks become much more stealthy due to their limited impact to classes other than source and target.\n\n\nAttack Timing in Label Flipping Attacks\n\nWhile label flipping attacks can occur at any point in the learning process and last for arbitrary lengths, it is important to understand the capabilities of adversaries who are available for only part of the training process. For instance, Google's Gboard application of FL requires all participant devices be plugged into power and connected to the internet via WiFi [9]. Such requirements create cyclic conditions where many participants are not available during the day, when phones are not plugged in and are actively in use. Adversaries can take advantage of this design choice, making themselves available at times when honest participants are unable to.\n\nWe consider two scenarios in which the adversary is restricted in the time in which they are able to make malicious participants available: one in which the adversary makes malicious participants available only before the 75th training round, and one in which malicious participants are available only after the 75th training round. As the rate of global model accuracy improvement decreases with both datasets by training round 75, we choose this point to highlight how pre-established model stability may effect an adversary's ability to launch an effective label flipping attack. Results for the first scenario are given in Figure 3 whereas the results for the second scenario are given in Figure 4.\n\nIn Figure 3, we compare source class recall in a non-poisoned setting versus with poisoning only before round 75. Results on both CIFAR-10 and Fashion-MNIST show that while there are observable drops in source class recall during the rounds with poisoning (1-75), the global model is able to recover quickly after poisoning finishes (after round 75). Furthermore, the final convergence of the models (towards the end of training) are not impacted, given the models with and without poisoning are converge with roughly the same recall values. We do note that some CIFAR-10 experiments exhibited delayed convergence by an additional 50-100 training rounds, but these circumstances were rare and  still eventually achieved the accuracy and recall levels of a non-poisoned model despite delayed convergence. c src \u2192 c target Source Class Recall (c recall src ) m% \u2208 P R > 0 m% \u2208 P R = 0 CIFAR-10 0 In Figure 4, we compare source class recall in a nonpoisoned setting versus with poisoning limited to the 75th and later training rounds. These results show the impact of such late poisoning demonstrating limited longevity; a phenomena which can be seen in the quick and dramatic changes in source class recall. Specifically, source class recall quickly returns to baseline levels once fewer malicious participants are selected in a training round even immediately following a round with a large number of malicious participants having caused a dramatic drop. However, the final poisoned model in the late-round poisoning scenario may show substantial difference in accuracy or recall compared to a non-poisoned model. This is evidenced by the CIFAR-10 experiment in Figure 4, in which the source recall of the poisoned model is \u223c 10% lower compared to non-poisoned. Furthermore, we observe that model convergence on both datasets is negatively impacted, as evidenced by the large variances in recall values between consecutive rounds. Consider Table 4 where results are compared when either (1) at least one malicious participant is selected for P R or (2) P R is made entirely of honest participants. When at least one malicious participant is selected, the final source class recall is, on average, 12.08% lower with the CIFAR-10 dataset and 24.46% lower with the Fashion-MNIST dataset. The utility impact from the label flipping attack is therefore predominantly tied to the number of malicious participants selected in the last few rounds of training.\n\n\nMalicious Participant Availability\n\nGiven the impact of malicious participation in late training rounds on attack effectiveness, we now introduce a malicious participant availability parameter \u03b1. By varying \u03b1 we can simulate the adversary's ability to control compromised participants' availability (i.e. ensuring connectivity or power access) at various points in training. Specifically, \u03b1 represents malicious participants' availability and therefore likeliness to be selected relative to honest participants. For example, if \u03b1 = 0.6, when selecting each participant P i \u2208 P r for round r, there is a 0.6 probability that P i will be one of the malicious participants. Larger \u03b1 implies higher likeliness of malicious participation. In cases where k > N \u00d7 m%, the number of malicious participants in P r is bounded by N \u00d7 m%. Figure 5 reports results for varying values of \u03b1 in late round poisoning, i.e., malicious participation is limited to rounds r \u2265 75. Specifically, we are interested in studying those scenarios where an adversary boosts the availability of the malicious participants enough that their selection becomes more likely than the non-malicious participants, hence in Figure 5 we use \u03b1 \u2265 0.6. The reported source class recalls in Figure 5 are averaged over the last 125 rounds (total 200 The results show that, when the adversary maintains sufficient representation in the participant pool (i.e. m \u2265 10%), manipulating the availability of malicious participants can yield significantly higher impact on the global model utility with source class recall losses in excess of 20%. On both datasets with m \u2265 10%, the negative impact on source class recall is highest with \u03b1 = 0.9, which is followed by \u03b1 = 0.8, \u03b1 = 0.7 and \u03b1 = 0.6, i.e., in decreasing order of malicious participant availability. Thus, in order to mount an impactful attack, it is in the best interests of the adversary to perform the attack with highest malicious participant availability in late rounds. We note that when k is significantly larger than N \u00d7 m%, increasing availability (\u03b1) will be insufficient for meaningfully increasing malicious participant selection in individual training rounds. Therefore, experiments where m < 10% show little variation despite changes in \u03b1.\n\nTo more acutely demonstrate the impact of \u03b1, Figure 6 reports source class recall by round when \u03b1 = 0.6 and \u03b1 = 0.9 for both the CIFAR-10 and Fashion-MNIST datasets. In both datasets, when malicious participants are available more frequently, the source class recall is effectively shifted lower in the graph, i.e., source class recall values with \u03b1 = 0.9 are often much smaller than those with \u03b1 = 0.6. We note that the high round-by-round variance in both graphs is due to the probabilistic variability in number of malicious participants in individual training rounds. When fewer malicious participants are selected in one training round relative to the previous round, source recall increases. When more malicious participants are selected in an individual round relative to the previous round, source recall falls.\n\nWe further explore and illustrate our last remark with respect to the impact of malicious parties' participation in consecutive rounds in Figure 7. In this figure, the x-axis represents the change in the number of malicious clients participating in consecutive rounds, i.e., (# of malicious \u2208 P r ) -(# of malicious \u2208 P r\u22121 ). The y-axis represents the change in source class recall between these consecutive rounds, i.e., (c recall src @ round r) -(c recall src @ round r \u22121). The reported results are then averaged across multiple runs of FL and all cases in which each participation difference was observed. The results confirm our intuition that, when P r contains Specifically, \u2200r \u2265 75 the y-axis represents (c recall src @ round r) -(c recall src @ round r\u22121) while the x-axis represents (# of malicious \u2208 P r ) -(# of malicious \u2208 P r\u22121 ). more malicious participants than P r\u22121 , there is a substantial drop in source class recall. For large differences (such as +3 or +4), the drop could be as high as 40% or 60%. In contrast, when P r contains fewer malicious participants than P r\u22121 , there is a substantial increase in source class recall, which can be as high as 60% or 40% when the difference is -4 or -3. Altogether, this demonstrates the possibility that the DNN could recover significantly even in few rounds of FL training, if a large enough decrease in malicious participation could be achieved.\n\n\nDefending Against Label Flipping Attacks\n\nGiven a highly effective adversary, how can a FL system defend against the label flipping attacks discussed thus far? To that end, we propose a defense which enables the aggregator to identify malicious participants.\n\n\nAlgorithm 1: Identifying Malicious Model Updates in FL\n\ndef evaluate updates(R : set of vulnerable train rounds, P : participant set): U = \u2205 for r \u2208 R do Pr \u2190 participants \u2208 P queried in training round r \u03b8r\u22121 \u2190 global model parameters after training round r \u2212 1 for Pi \u2208 Pr do \u03b8r,i \u2190 updated parameters after train DNN(\u03b8r\u22121, Di) \u03b8\u2206,i \u2190 \u03b8r,i \u2212 \u03b8r \u03b8 src \u2206,i \u2190 parameters \u2208 \u03b8\u2206,i connected to source class output node Add \u03b8 src\n\u2206,i to U U \u2190 standardize(U) U \u2190 PCA(U , components=2 ) plot(U )\nAfter identifying malicious participants, the aggregator may blacklist them or ignore their updates \u03b8 r,i in future rounds. We showed in Sections 3.3 and 3.4 that high-utility model convergence can be eventually achieved after eliminating malicious participation. The feasibility of such a recovery from early round attacks supports use of the proposed identification approach as a defense strategy.\n\nOur defense is based on the following insight: The parameter updates sent from malicious participants have unique characteristics compared to honest participants' updates for a subset of the parameter space. However, since DNNs have many parameters (i.e., \u03b8 r,i is extremely high dimensional) it is non-trivial to analyze parameter updates by hand. Thus, we propose an automated strategy for identifying the relevant parameter subset and for studying participant updates using dimensionality reduction (PCA).\n\n(a) CIFAR-10 m=2% (b) CIFAR-10 m=4% (c) CIFAR-10 m=10% (d) CIFAR-10 m=20% The description of our defense strategy is given in Algorithm 1. Let R denote the set of vulnerable FL training rounds and c src be the class that is suspected to be the source class of a poisoning attack. We note that if c src is unknown, the aggregator can defend against potential attacks such that c src = c \u2200c \u2208 C. We also note that for a given c src , Algorithm 1 considers label flipping for all possible c target . An aggregator therefore will conduct |C| independent iterations of Algorithm 1, which can be conducted in parallel. For each round r \u2208 R and participant P i \u2208 P r , the aggregator computes the delta in participant's model update compared to the global model, i.e., \u03b8 \u2206,i \u2190 \u03b8 r,i \u2212 \u03b8 r . Recall from Section 2.1 that a predicted probability for any given class c is computed by a specific node n c in the final layer DNN architecture. Given the aggregator's goal of defending against the label flipping attack from c src , only the subset of the parameters in \u03b8 \u2206,i corresponding to n csrc is extracted. The outcome of the extraction is denoted by \u03b8 src \u2206,i and added to a global list U built by the aggregator. After U is constructed across multiple rounds and participant deltas, it is standardized by removing the mean and scaling to unit variance. The standardized list U is fed into Principal Component Analysis (PCA), which is a popular ML technique used for dimensionality reduction and pattern visualization. For ease of visualization, we use and plot results with two dimensions (two components).\n(e) F-MNIST m=2% (f) F-MNIST m=4% (g) F-MNIST m =10% (h) F-MNIST m=20%\nIn Figure 8, we show the results of Algorithm 1 on CIFAR-10 and Fashion-MNIST across varying malicious participation rate m, with R = [10,200]. Even in scenarios with low m, as is shown in Figures 8a and 8e, our defense is capable of differentiating between malicious and honest participants. In all graphs, the PCA outcome shows that malicious participants' updates belong to a visibly different cluster compared to honest participants' updates which form their own cluster. Another interesting observation is that our defense does not suffer from the \"gradient drift\" problem. Gradient drift is a potential challenge in designing a robust defense, since changes in model updates may be caused both by actual DNN learning and convergence (which is desirable) or malicious poisoning attempt (which our defense is trying to identify and prevent). Our results show that, even though the defense is tested with a long period of rounds (190 training rounds since R = [10,200]), it remains capable of separating malicious and honest participants, demonstrating its robustness to gradient drift.\n\nA FL system aggregator can therefore effectively identify malicious participants, and consequently restrict their participation in mobile training, by conducting such gradient clustering prior to aggregating parameter updates at each round. Clustering model gradients for malicious participant identification presents a strong defense as it does not require access to any public validation dataset, as is required in [3], which is not necessarily possible to acquire.\n\n\nRelated Work\n\nPoisoning attacks are highly relevant in domains such as spam filtering [10,32], malware and network anomaly detection [11,24,39], disease diagnosis [29], computer vision [34], and recommender systems [15,54]. Several poisoning attacks were developed for popular ML models including SVM [6,12,44,45,50,52], regression [19], dimensionality reduction [51], linear classifiers [12,23,57], unsupervised learning [7], and more recently, neural networks [12,30,42,45,53,58]. However, most of the existing work is concerned with poisoning ML models in the traditional setting where training data is first collected by a centralized party. In contrast, our work studies poisoning attacks in the context of FL. As a result, many of the poisoning attacks and defenses that were designed for traditional ML are not suitable to FL. For example, attacks that rely on crafting optimal poison instances by observing the training data distribution are inapplicable since the malicious FL participant may only access and modify the training data s/he holds. Similarly, server-side defenses that rely on filtering and eliminating poison instances through anomaly detection or k-NN [36,37] are inapplicable to FL since the server only observes parameter updates from FL participants, not their individual instances.\n\nThe rising popularity of FL has led to the investigation of different attacks in the context of FL, such as backdoor attacks [2,46], gradient leakage attacks [18,27,59] and membership inference attacks [31,47,48]. Most closely related to our work are poisoning attacks in FL. There are two types of poisoning attacks in FL: data poisoning and model poisoning. Our work falls under the data poisoning category. In data poisoning, a malicious FL participant manipulates their training data, e.g., by adding poison instances or adversarially changing existing instances [16,43]. The local learning process is otherwise not modified. In model poisoning, the malicious FL participant modifies its learning process in order to create adversarial gradients and parameter updates. [4] and [14] demonstrated the possibility of causing high model error rates through targeted and untargeted model poisoning attacks. While model poisoning is also effective, data poisoning may be preferable or more convenient in certain scenarios, since it does not require adversarial tampering of model learning software on participant devices, it is efficient, and it allows for non-expert poisoning participants.\n\nFinally, FL poisoning attacks have connections to the concept of Byzantine threats, in which one or more participants in a distributed system fail or misbehave. In FL, Byzantine behavior was shown to lead to sub-optimal models or non-convergence [8,20]. This has spurred a line of work on Byzantine-resilient aggregation for distributed learning, such as Krum [8], Bulyan [28], trimmed mean, and coordinate-wise median [55]. While model poisoning may remain successful despite Byzantine-resilient aggregation [4,14,20], it is unclear whether optimal data poisoning attacks can be found to circumvent an individual Byzantineresilient scheme, or whether one data poisoning attack may circumvent multiple Byzantine-resilient schemes. We plan to investigate these issues in future work.\n\n\nConclusion\n\nIn this paper we studied data poisoning attacks against FL systems. We demonstrated that FL systems are vulnerable to label flipping poisoning attacks and that these attacks can significantly negatively impact the global model. We also showed that the negative impact on the global model increases as the proportion of malicious participants increases, and that it is possible to achieve targeted poisoning impact. Further, we demonstrated that adversaries can enhance attack effectiveness by increasing the availability of malicious participants in later rounds. Finally, we proposed a defense which helps an FL aggregator separate malicious from honest participants. We showed that our defense is capable of identifying malicious participants and it is robust to gradient drift.\n\nAs poisoning attacks against FL systems continue to emerge as important research topics in the security and ML communities [14,4,33,56,21], we plan to continue our work in several ways. First, we will study the impacts of the attack and defense on diverse FL scenarios differing in terms of data size, distribution among FL participants (iid vs non-iid), data type, total number of instances available per class, etc . Second, we will study more complex adversarial behaviors such as each malicious participant changing the labels of only a small portion of source samples or using more sophisticated poisoning strategies to avoid being detected. Third, while we designed and tested our defense against the label flipping attack, we hypothesize the defense will be useful against model poisoning attacks since malicious participants gradients are often dissimilar to those of honest participants. Since our defense identifies dissimilar or anomalous gradients, we expect the defense to be effective against other types of FL attacks that cause dissimilar or anomalous gradients. In future work, we will study the applicability of our defense against such other FL attacks including model poisoning, untargeted poisoning, and backdoor attacks. \n\n\nA DNN Architectures and Configuration\n\nAll NNs were trained using PyTorch version 1.2.0 with random weight initialization. Training and testing was completed using a NVIDIA 980 Ti GPUaccelerator. When necessary, all CUDA tensors were mapped to CPU tensors before exporting to Numpy arrays. Default drivers provided by Ubuntu 19.04 and built-in GPU support in PyTorch was used to accelerate training. Details can be found in our repository: https://github.com/git-disl/DataPoisoning_FL. Fashion-MNIST: We do not conduct data pre-processing. We use a Convolutional Neural Network with the architecture described in Values reflect mean and standard deviation of the ImageNet dataset [13] and are commonplace, even expected when using Torchvision [25] models. We additionally perform data augmentation with random horizontal flipping, random cropping with size 32, and default padding. Our CNN is detailed in Table 5.   \n\nM\n, MNP Model, model trained with no poisoning k Number of FL participants in each round R Total number of rounds of FL training Pr FL participants queried at round r, r \u2208 [1, R] \u03b8r, \u03b8r,i Global model parameters after round r and local model parameters at participant Pi after round r m% Percentage of malicious participants csrc, ctarget Source and target class in label flipping attack M acc Global model accuracy c recall i Class recall for class ci m cnt i j Baseline misclassification count from class ci to class cjTable (1) Notations used throughout the paper.Attack Evaluation Metrics: At the end of R rounds of FL, the model M is finalized with parameters \u03b8 R . Let D test denote the test dataset used in evaluating M , where D test \u2229 D i = \u2205 for all participant datasets D i . In the next sections, we provide a thorough analysis of label flipping attacks in FL. To do so, we use a number of evaluation metrics. Global Model Accuracy (M acc ): The global model accuracy is the percentage of instances x \u2208 D test where the global model M with final parameters \u03b8 R predicts M \u03b8 R (x) = c i and c i is indeed the true class label of x. Class Recall (c recall i ): For any class c i \u2208 C, its class recall is the percentage T Pi\n\n\na) CIFAR-10 M acc (b) CIFAR-10 c recall src (c) F-MNIST M acc (d) F-MNIST c recall src Fig. (1) Evaluation of attack feasibility and impact of malicious participant percentage on attack effectiveness. CIFAR-10 experiments are for the 5 \u2192 3 setting while Fashion-MNIST experiments are for the 4 \u2192 6 setting. Results are averaged from 10 runs for each setting of m%. The black bars are mean over the 10 runs and the green error bars denote standard deviation.\n\nFig. ( 2 )\n2Relationship between global model accuracy and source class recall across changing percentages of malicious participants for CIFAR-10 and Fashion-MNIST. As each dataset has 10 classes, the scale for M acc vs c recall src is 1:10.\n\nFig. ( 3 )\n3Source class recall by round for experiments with \"early round poisoning\", i.e., malicious participation only in the first 75 rounds (r < 75). The blue line indicates the round at which malicious participation is no longer allowed.\n\nFig. ( 4 )\n4Source class recall by round for experiments with \"late round poisoning\", i.e., malicious participation only after round 75 (r \u2265 75). The blue line indicates the round at which malicious participation starts.\n\nFig. ( 5 )\n5Evaluation of impact from malicious participants' availability \u03b1 on source class recall. Results are averaged from 3 runs for each setting.\n\nFig. ( 6 )\n6Source class recall by round when malicious participants' availability is close to that of honest participants (\u03b1 = 0.6) vs significantly increased (\u03b1 = 0.9). The blue line indicates the round in which attack starts. rounds minus first 75 rounds) to remove the impact of individual round variability; further, each experiment setting is repeated 3 times and results are averaged.\n\nFig. ( 7 )\n7Relationship between change in source class recall in consecutive rounds versus change in number of malicious participants in consecutive rounds.\n\nFig. ( 8 )\n8PCA plots with 2 components demonstrating the ability of Algorithm 1 to identify updates originating from a malicious versus honest participant. Plots represent relevant gradients collected from all training rounds r > 10. Blue Xs represent gradients from malicious participants while yellow Os represent gradients from honest participants.\n\n54 .\n54Yang, G., Gong, N.Z., Cai, Y.: Fake co-visitation injection attacks to recommender systems. In: NDSS (2017) 55. Yin, D., Chen, Y., Kannan, R., Bartlett, P.: Byzantine-robust distributed learning: Towards optimal statistical rates. In: International Conference on Machine Learning. pp. 5650-5659 (2018) 56. Zhao, L., Hu, S., Wang, Q., Jiang, J., Chao, S., Luo, X., Hu, P.: Shielding collaborative learning: Mitigating poisoning attacks through client-side detection. IEEE Transactions on Dependable and Secure Computing (2020) 57. Zhao, M., An, B., Gao, W., Zhang, T.: Efficient label contamination attacks against black-box learning models. In: IJCAI. pp. 3945-3951 (2017) 58. Zhu, C., Huang, W.R., Li, H., Taylor, G., Studer, C., Goldstein, T.: Transferable clean-label poisoning attacks on deep neural nets. In: International Conference on Machine Learning. pp. 7614-7623 (2019) 59. Zhu, L., Liu, Z., Han, S.: Deep leakage from gradients. In: Advances in Neural Information Processing Systems. pp. 14747-14756 (2019)\n\n\nc src \u2192 c target m cnt src 61% 7.16% 16% 29.2% 28.7% 37.1% 58.9% 6 \u2192 0 118 -1% 2.19% 7.34% 9.81% 19.9% 39% 43.4%Table (2) Loss in source class recall for three source \u2192 target class settings with differing baseline misclassification counts in CIFAR-10 and Fashion-MNIST. Loss averaged from 10 runs. Highlighted bold entries are highest loss in each.target \n\nPercentage of Malicious Participants (m%) \n2 \n4 \n10 \n20 \n30 \n40 \n50 \nCIFAR-10 \n0 \u2192 2 \n16 \n1.42% 2.93% 10.2% 14.1% 48.3% 73% 70.5% \n1 \u2192 9 \n56 \n0.69% 3.75% 6.04% 15% 36.3% 49.2% 54.7% \n5 \u2192 3 \n200 \n0% 3.21% 7.92% 25.4% 49.5% 69.2% 69.2% \nFashion-MNIST \n1 \u2192 3 \n18 \n0.12% 0.42% 2.27% 2.41% 40.3% 45.4% 42% \n4 \u2192 6 \n51 \n0.\n\nTable 6 .\n6In the table, Conv = Convolutional Layer, and Batch Norm = Batch Normalization. CIFAR-10: We conduct data pre-processing prior to training. Data is normalized with mean [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224, 0.225].\n\n\nLayer Type Size Conv + ReLu + Batch Norm 3x3x32 Conv + ReLu + Batch Norm 3x32x32 Max Pooling 2x2 Conv + ReLu + Batch Norm 3x32x64 Conv + ReLu + Batch Norm 3x64x64 Max Pooling 2x2 Conv + ReLu + Batch Norm 3x64x128 Conv + ReLu + Batch Norm 3x128x128 Max Pooling 2x2 Fully Connected 2048 Fully Connected + Softmax 128 / 10\n\nTable ( 5\n() CIFAR-10 CNN.Layer Type \nSize \nConv + ReLu + Batch Norm 5x1x16 \nMax Pooling \n2x2 \nConv + ReLu + Batch Norm 5x16x32 \nMax Pooling \n2x2 \nFully Connected \n1568 / 10 \n\n\n\nTable ( 6\n() Fashion-MNIST CNN.\nhttps://github.com/git-disl/DataPoisoning_FL\nAcknowledgements. This research is partially sponsored by NSF CISE SaTC 1564097. The second author acknowledges an IBM PhD Fellowship Award and the support from the Enterprise AI, Systems & Solutions division led by Sandeep Gopisetty at IBM Almaden Research Center. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or other funding agencies and companies mentioned above.\nHealth insurance portability and accountability act of 1996. A Act, Public law. 104191Act, A.: Health insurance portability and accountability act of 1996. Public law 104, 191 (1996)\n\nE Bagdasaryan, A Veit, Y Hua, D Estrin, V Shmatikov, arXiv:1807.00459How to backdoor federated learning. arXiv preprintBagdasaryan, E., Veit, A., Hua, Y., Estrin, D., Shmatikov, V.: How to backdoor federated learning. arXiv preprint arXiv:1807.00459 (2018)\n\nMitigating poisoning attacks on machine learning models: A data provenance based approach. N Baracaldo, B Chen, H Ludwig, J A Safavi, 10th ACM Workshop on Artificial Intelligence and Security. Baracaldo, N., Chen, B., Ludwig, H., Safavi, J.A.: Mitigating poisoning attacks on machine learning models: A data provenance based approach. In: 10th ACM Workshop on Artificial Intelligence and Security. pp. 103-110 (2017)\n\nAnalyzing federated learning through an adversarial lens. A N Bhagoji, S Chakraborty, P Mittal, S Calo, International Conference on Machine Learning. Bhagoji, A.N., Chakraborty, S., Mittal, P., Calo, S.: Analyzing federated learning through an adversarial lens. In: International Conference on Machine Learning. pp. 634-643 (2019)\n\nSupport vector machines under adversarial label noise. B Biggio, B Nelson, P Laskov, Asian conference on machine learning. Biggio, B., Nelson, B., Laskov, P.: Support vector machines under adversarial label noise. In: Asian conference on machine learning. pp. 97-112 (2011)\n\nPoisoning attacks against support vector machines. B Biggio, B Nelson, P Laskov, Proceedings of the 29th International Coference on International Conference on Machine Learning. the 29th International Coference on International Conference on Machine LearningBiggio, B., Nelson, B., Laskov, P.: Poisoning attacks against support vector ma- chines. In: Proceedings of the 29th International Coference on International Con- ference on Machine Learning. pp. 1467-1474 (2012)\n\nIs data clustering in adversarial settings secure. B Biggio, I Pillai, S Rota Bul\u00f2, D Ariu, M Pelillo, F Roli, Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security. the 2013 ACM Workshop on Artificial Intelligence and SecurityBiggio, B., Pillai, I., Rota Bul\u00f2, S., Ariu, D., Pelillo, M., Roli, F.: Is data clustering in adversarial settings secure? In: Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security. pp. 87-98 (2013)\n\nMachine learning with adversaries: Byzantine tolerant gradient descent. P Blanchard, R Guerraoui, J Stainer, NeurIPS. pp. Blanchard, P., Guerraoui, R., Stainer, J., et al.: Machine learning with adversaries: Byzantine tolerant gradient descent. In: NeurIPS. pp. 119-129 (2017)\n\nTowards federated learning at scale: System design. K Bonawitz, H Eichner, W Grieskamp, D Huba, A Ingerman, V Ivanov, C M Kiddon, J Konen, S Mazzocchi, B Mcmahan, T V Overveldt, D Petrou, D Ramage, J Roselander, SysML 2019. to appearBonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon, C.M., Konen, J., Mazzocchi, S., McMahan, B., Overveldt, T.V., Petrou, D., Ramage, D., Roselander, J.: Towards federated learning at scale: System design. In: SysML 2019 (2019), https://arxiv.org/abs/1902.01046, to appear\n\nAttacks against machine learning -an overview. E Bursztein, OnlineBursztein, E.: Attacks against machine learning -an overview. https://elie.net/ blog/ai/attacks-against-machine-learning-an-overview/ (2018), [Online]\n\nAutomated poisoning attacks and defenses in malware detection systems: An adversarial machine learning approach. S Chen, M Xue, L Fan, S Hao, L Xu, H Zhu, B Li, computers & Security. 73Chen, S., Xue, M., Fan, L., Hao, S., Xu, L., Zhu, H., Li, B.: Automated poison- ing attacks and defenses in malware detection systems: An adversarial machine learning approach. computers & Security 73, 326-344 (2018)\n\nWhy do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks. A Demontis, M Melis, M Pintor, M Jagielski, B Biggio, A Oprea, C Nita-Rotaru, F Roli, 28th USENIX Security Symposium. Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea, A., Nita- Rotaru, C., Roli, F.: Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks. In: 28th USENIX Security Symposium. pp. 321- 338 (2019)\n\nImagenet: A largescale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large- scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009)\n\nLocal model poisoning attacks to byzantinerobust federated learning. M Fang, X Cao, J Jia, N Z Gong, To appear in USENIX Security Symposium. Fang, M., Cao, X., Jia, J., Gong, N.Z.: Local model poisoning attacks to byzantine- robust federated learning. In: To appear in USENIX Security Symposium (2020)\n\nPoisoning attacks to graph-based recommender systems. M Fang, G Yang, N Z Gong, J Liu, Proceedings of the 34th Annual Computer Security Applications Conference. the 34th Annual Computer Security Applications ConferenceFang, M., Yang, G., Gong, N.Z., Liu, J.: Poisoning attacks to graph-based recom- mender systems. In: Proceedings of the 34th Annual Computer Security Applica- tions Conference. pp. 381-392 (2018)\n\nC Fung, C J Yoon, I Beschastnikh, arXiv:1808.04866Mitigating sybils in federated learning poisoning. arXiv preprintFung, C., Yoon, C.J., Beschastnikh, I.: Mitigating sybils in federated learning poi- soning. arXiv preprint arXiv:1808.04866 (2018)\n\nFederated learning for mobile keyboard prediction. A Hard, K Rao, R Mathews, S Ramaswamy, F Beaufays, S Augenstein, H Eichner, C Kiddon, D Ramage, arXiv:1811.03604arXiv preprintHard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S., Eichner, H., Kiddon, C., Ramage, D.: Federated learning for mobile keyboard pre- diction. arXiv preprint arXiv:1811.03604 (2018)\n\nDeep models under the gan: information leakage from collaborative deep learning. B Hitaj, G Ateniese, F Perez-Cruz, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityHitaj, B., Ateniese, G., Perez-Cruz, F.: Deep models under the gan: information leakage from collaborative deep learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. pp. 603-618 (2017)\n\nManipulating machine learning: Poisoning attacks and countermeasures for regression learning. M Jagielski, A Oprea, B Biggio, C Liu, C Nita-Rotaru, B Li, 2018 IEEE Symposium on Security and Privacy (SP). IEEEJagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., Li, B.: Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In: 2018 IEEE Symposium on Security and Privacy (SP). pp. 19-35. IEEE (2018)\n\nP Kairouz, H B Mcmahan, B Avent, A Bellet, M Bennis, A N Bhagoji, K Bonawitz, Z Charles, G Cormode, R Cummings, arXiv:1912.04977Advances and open problems in federated learning. arXiv preprintKairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977 (2019)\n\nY Khazbak, T Tan, G Cao, Mlguard: Mitigating poisoning attacks in privacy preserving distributed collaborative learning. Khazbak, Y., Tan, T., Cao, G.: Mlguard: Mitigating poisoning attacks in privacy preserving distributed collaborative learning (2020)\n\nA Krizhevsky, G Hinton, Learning multiple layers of features from tiny images. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)\n\nRobust linear regression against training data poisoning. C Liu, B Li, Y Vorobeychik, A Oprea, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityLiu, C., Li, B., Vorobeychik, Y., Oprea, A.: Robust linear regression against train- ing data poisoning. In: Proceedings of the 10th ACM Workshop on Artificial In- telligence and Security. pp. 91-102 (2017)\n\nTowards adversarial malware detection: Lessons learned from pdf-based attacks. D Maiorca, B Biggio, G Giacinto, ACM Computing Surveys (CSUR). 524Maiorca, D., Biggio, B., Giacinto, G.: Towards adversarial malware detection: Lessons learned from pdf-based attacks. ACM Computing Surveys (CSUR) 52(4), 1-36 (2019)\n\nTorchvision the machine-vision package of torch. S Marcel, Y Rodriguez, 18th ACM International Conference on Multimedia. Marcel, S., Rodriguez, Y.: Torchvision the machine-vision package of torch. In: 18th ACM International Conference on Multimedia. pp. 1485-1488 (2010)\n\nThe california consumer privacy act. K Mathews, C Bowman, Mathews, K., Bowman, C.: The california consumer privacy act of 2018 (2018)\n\nExploiting unintended feature leakage in collaborative learning. L Melis, C Song, E De Cristofaro, V Shmatikov, 2019 IEEE Symposium on Security and Privacy (SP). IEEEMelis, L., Song, C., De Cristofaro, E., Shmatikov, V.: Exploiting unintended fea- ture leakage in collaborative learning. In: 2019 IEEE Symposium on Security and Privacy (SP). pp. 691-706. IEEE (2019)\n\nE M E Mhamdi, R Guerraoui, S Rouault, arXiv:1802.07927The hidden vulnerability of distributed learning in byzantium. arXiv preprintMhamdi, E.M.E., Guerraoui, R., Rouault, S.: The hidden vulnerability of dis- tributed learning in byzantium. arXiv preprint arXiv:1802.07927 (2018)\n\nSystematic poisoning attacks on and defenses for machine learning in healthcare. M Mozaffari-Kermani, S Sur-Kolay, A Raghunathan, N K Jha, IEEE Journal of Biomedical and Health Informatics. 196Mozaffari-Kermani, M., Sur-Kolay, S., Raghunathan, A., Jha, N.K.: Systematic poisoning attacks on and defenses for machine learning in healthcare. IEEE Journal of Biomedical and Health Informatics 19(6), 1893-1905 (2014)\n\nTowards poisoning of deep learning algorithms with backgradient optimization. L Mu\u00f1oz-Gonz\u00e1lez, B Biggio, A Demontis, A Paudice, V Wongrassamee, E C Lupu, F Roli, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityMu\u00f1oz-Gonz\u00e1lez, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E.C., Roli, F.: Towards poisoning of deep learning algorithms with back- gradient optimization. In: Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. pp. 27-38 (2017)\n\nComprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. M Nasr, R Shokri, A Houmansadr, 2019 IEEE Symposium on Security and Privacy (SP). IEEENasr, M., Shokri, R., Houmansadr, A.: Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In: 2019 IEEE Symposium on Security and Privacy (SP). pp. 739-753. IEEE (2019)\n\nExploiting machine learning to subvert your spam filter. B Nelson, M Barreno, F J Chi, A D Joseph, B I Rubinstein, U Saini, C A Sutton, J D Tygar, K Xia, LEET. 8Nelson, B., Barreno, M., Chi, F.J., Joseph, A.D., Rubinstein, B.I., Saini, U., Sutton, C.A., Tygar, J.D., Xia, K.: Exploiting machine learning to subvert your spam filter. LEET 8, 1-9 (2008)\n\nPoisoning attacks on federated learning-based iot intrusion detection system. T D Nguyen, P Rieger, M Miettinen, A R Sadeghi, Nguyen, T.D., Rieger, P., Miettinen, M., Sadeghi, A.R.: Poisoning attacks on fed- erated learning-based iot intrusion detection system (2020)\n\nSok: Security and privacy in machine learning. N Papernot, P Mcdaniel, A Sinha, M P Wellman, 2018 IEEE European Symposium on Security and Privacy (EuroS&P). IEEEPapernot, N., McDaniel, P., Sinha, A., Wellman, M.P.: Sok: Security and privacy in machine learning. In: 2018 IEEE European Symposium on Security and Privacy (EuroS&P). pp. 399-414. IEEE (2018)\n\nPytorch: An imperative style, highperformance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, NeurIPS. pp. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high- performance deep learning library. In: NeurIPS. pp. 8024-8035 (2019)\n\nA Paudice, L Mu\u00f1oz-Gonz\u00e1lez, A Gyorgy, E C Lupu, arXiv:1802.03041Detection of adversarial training examples in poisoning attacks through anomaly detection. arXiv preprintPaudice, A., Mu\u00f1oz-Gonz\u00e1lez, L., Gyorgy, A., Lupu, E.C.: Detection of adversarial training examples in poisoning attacks through anomaly detection. arXiv preprint arXiv:1802.03041 (2018)\n\nLabel sanitization against label flipping poisoning attacks. A Paudice, L Mu\u00f1oz-Gonz\u00e1lez, E C Lupu, ECML-PKDD. SpringerPaudice, A., Mu\u00f1oz-Gonz\u00e1lez, L., Lupu, E.C.: Label sanitization against label flipping poisoning attacks. In: ECML-PKDD. pp. 5-15. Springer (2018)\n\nRegulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46. G D P Regulation, Official Journal of the European Union. 591-88294OJ)Regulation, G.D.P.: Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46. Official Journal of the European Union (OJ) 59(1-88), 294 (2016)\n\nAntidote: understanding and defending against poisoning of anomaly detectors. B I Rubinstein, B Nelson, L Huang, A D Joseph, S H Lau, S Rao, N Taft, J D Tygar, Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement. the 9th ACM SIGCOMM Conference on Internet MeasurementRubinstein, B.I., Nelson, B., Huang, L., Joseph, A.D., Lau, S.h., Rao, S., Taft, N., Tygar, J.D.: Antidote: understanding and defending against poisoning of anomaly detectors. In: Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement. pp. 1-14 (2009)\n\nA generic framework for privacy preserving deep learning. T Ryffel, A Trask, M Dahl, B Wagner, J Mancuso, D Rueckert, J Passerat-Palmbach, arXiv:1811.04017arXiv preprintRyffel, T., Trask, A., Dahl, M., Wagner, B., Mancuso, J., Rueckert, D., Passerat- Palmbach, J.: A generic framework for privacy preserving deep learning. arXiv preprint arXiv:1811.04017 (2018)\n\nLet's talk about race: Identity, chatbots, and ai. A Schlesinger, K P O&apos;hara, A S Taylor, Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. the 2018 CHI Conference on Human Factors in Computing SystemsSchlesinger, A., O'Hara, K.P., Taylor, A.S.: Let's talk about race: Identity, chat- bots, and ai. In: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. pp. 1-14 (2018)\n\nPoison frogs! targeted clean-label poisoning attacks on neural networks. A Shafahi, W R Huang, M Najibi, O Suciu, C Studer, T Dumitras, T Goldstein, Advances in Neural Information Processing Systems. Shafahi, A., Huang, W.R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., Gold- stein, T.: Poison frogs! targeted clean-label poisoning attacks on neural networks. In: Advances in Neural Information Processing Systems. pp. 6103-6113 (2018)\n\nAuror: Defending against poisoning attacks in collaborative deep learning systems. S Shen, S Tople, P Saxena, Proceedings of the 32nd Annual Conference on Computer Security Applications. the 32nd Annual Conference on Computer Security ApplicationsShen, S., Tople, S., Saxena, P.: Auror: Defending against poisoning attacks in collaborative deep learning systems. In: Proceedings of the 32nd Annual Conference on Computer Security Applications. pp. 508-519 (2016)\n\nCertified defenses for data poisoning attacks. J Steinhardt, P W W Koh, P S Liang, NeurIPS. Steinhardt, J., Koh, P.W.W., Liang, P.S.: Certified defenses for data poisoning attacks. In: NeurIPS. pp. 3517-3529 (2017)\n\nWhen does machine learning fail? generalized transferability for evasion and poisoning attacks. O Suciu, R Marginean, Y Kaya, Iii Daume, H Dumitras, T , 27th USENIX Security Symposium. Suciu, O., Marginean, R., Kaya, Y., Daume III, H., Dumitras, T.: When does machine learning fail? generalized transferability for evasion and poisoning attacks. In: 27th USENIX Security Symposium. pp. 1299-1316 (2018)\n\nZ Sun, P Kairouz, A T Suresh, H B Mcmahan, arXiv:1911.07963Can you really backdoor federated learning. arXiv preprintSun, Z., Kairouz, P., Suresh, A.T., McMahan, H.B.: Can you really backdoor fed- erated learning? arXiv preprint arXiv:1911.07963 (2019)\n\nS Truex, L Liu, M E Gursoy, L Yu, W Wei, arXiv:1807.09173Towards demystifying membership inference attacks. arXiv preprintTruex, S., Liu, L., Gursoy, M.E., Yu, L., Wei, W.: Towards demystifying member- ship inference attacks. arXiv preprint arXiv:1807.09173 (2018)\n\nDemystifying membership inference attacks in machine learning as a service. S Truex, L Liu, M E Gursoy, L Yu, W Wei, IEEE Transactions on Services Computing. Truex, S., Liu, L., Gursoy, M.E., Yu, L., Wei, W.: Demystifying membership in- ference attacks in machine learning as a service. IEEE Transactions on Services Computing (2019)\n\nFashion-mnist: a novel image dataset for benchmarking machine learning algorithms. H Xiao, K Rasul, R Vollgraf, arXiv:1708.07747arXiv preprintXiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: a novel image dataset for bench- marking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)\n\nAdversarial label flips attack on support vector machines. H Xiao, H Xiao, C Eckert, ECAI. Xiao, H., Xiao, H., Eckert, C.: Adversarial label flips attack on support vector machines. In: ECAI. pp. 870-875 (2012)\n\nIs feature selection secure against training data poisoning?. H Xiao, B Biggio, G Brown, G Fumera, C Eckert, F Roli, International Conference on Machine Learning. Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., Roli, F.: Is feature selection secure against training data poisoning? In: International Conference on Machine Learning. pp. 1689-1698 (2015)\n\nSupport vector machines under adversarial label contamination. H Xiao, B Biggio, B Nelson, H Xiao, C Eckert, F Roli, Neurocomputing. 160Xiao, H., Biggio, B., Nelson, B., Xiao, H., Eckert, C., Roli, F.: Support vector ma- chines under adversarial label contamination. Neurocomputing 160, 53-62 (2015)\n\nC Yang, Q Wu, H Li, Y Chen, arXiv:1703.01340Generative poisoning attack method against neural networks. arXiv preprintYang, C., Wu, Q., Li, H., Chen, Y.: Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340 (2017)\n", "annotations": {"author": "[{\"end\":148,\"start\":61},{\"end\":242,\"start\":149},{\"end\":329,\"start\":243},{\"end\":413,\"start\":330}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":66},{\"end\":167,\"start\":162},{\"end\":254,\"start\":248},{\"end\":338,\"start\":335}]", "author_first_name": "[{\"end\":65,\"start\":61},{\"end\":161,\"start\":155},{\"end\":247,\"start\":243},{\"end\":334,\"start\":330}]", "author_affiliation": "[{\"end\":147,\"start\":98},{\"end\":241,\"start\":192},{\"end\":328,\"start\":279},{\"end\":412,\"start\":363}]", "title": "[{\"end\":58,\"start\":1},{\"end\":471,\"start\":414}]", "venue": null, "abstract": "[{\"end\":1736,\"start\":572}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2444,\"start\":2440},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2447,\"start\":2444},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2449,\"start\":2447},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2523,\"start\":2520},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2526,\"start\":2523},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2529,\"start\":2526},{\"end\":2885,\"start\":2881},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3360,\"start\":3357},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4002,\"start\":3998},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4608,\"start\":4605},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4611,\"start\":4608},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4614,\"start\":4611},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4617,\"start\":4614},{\"end\":6436,\"start\":6429},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11715,\"start\":11712},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11718,\"start\":11715},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11721,\"start\":11718},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13210,\"start\":13206},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13213,\"start\":13210},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13216,\"start\":13213},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13219,\"start\":13216},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14544,\"start\":14540},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14567,\"start\":14563},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15665,\"start\":15661},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18857,\"start\":18855},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22443,\"start\":22439},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22461,\"start\":22457},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23005,\"start\":23002},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34428,\"start\":34424},{\"end\":34432,\"start\":34428},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35257,\"start\":35253},{\"end\":35261,\"start\":35257},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35801,\"start\":35798},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35941,\"start\":35937},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":35944,\"start\":35941},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":35988,\"start\":35984},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35991,\"start\":35988},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":35994,\"start\":35991},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36018,\"start\":36014},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36040,\"start\":36036},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36070,\"start\":36066},{\"end\":36073,\"start\":36070},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":36155,\"start\":36152},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36158,\"start\":36155},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36161,\"start\":36158},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36164,\"start\":36161},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36167,\"start\":36164},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36170,\"start\":36167},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36187,\"start\":36183},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36218,\"start\":36214},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36243,\"start\":36239},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36246,\"start\":36243},{\"end\":36249,\"start\":36246},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36276,\"start\":36273},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36317,\"start\":36313},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36320,\"start\":36317},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36323,\"start\":36320},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36326,\"start\":36323},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":36329,\"start\":36326},{\"end\":36332,\"start\":36329},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37032,\"start\":37028},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37035,\"start\":37032},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37291,\"start\":37288},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37294,\"start\":37291},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37325,\"start\":37321},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37328,\"start\":37325},{\"end\":37331,\"start\":37328},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37369,\"start\":37365},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37372,\"start\":37369},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37375,\"start\":37372},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37734,\"start\":37730},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37737,\"start\":37734},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37939,\"start\":37936},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37948,\"start\":37944},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38603,\"start\":38600},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38606,\"start\":38603},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38717,\"start\":38714},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38730,\"start\":38726},{\"end\":38777,\"start\":38773},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38866,\"start\":38863},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":38869,\"start\":38866},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38872,\"start\":38869},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":40060,\"start\":40056},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":40062,\"start\":40060},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":40065,\"start\":40062},{\"end\":40068,\"start\":40065},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40071,\"start\":40068},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41863,\"start\":41859},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41926,\"start\":41922}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43330,\"start\":42096},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43790,\"start\":43331},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44033,\"start\":43791},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44278,\"start\":44034},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44500,\"start\":44279},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44653,\"start\":44501},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45046,\"start\":44654},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45205,\"start\":45047},{\"attributes\":{\"id\":\"fig_8\"},\"end\":45559,\"start\":45206},{\"attributes\":{\"id\":\"fig_9\"},\"end\":46586,\"start\":45560},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47262,\"start\":46587},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47511,\"start\":47263},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47833,\"start\":47512},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48011,\"start\":47834},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48044,\"start\":48012}]", "paragraph": "[{\"end\":3222,\"start\":1752},{\"end\":4003,\"start\":3224},{\"end\":4707,\"start\":4005},{\"end\":5888,\"start\":4709},{\"end\":6656,\"start\":5890},{\"end\":7216,\"start\":6658},{\"end\":7825,\"start\":7286},{\"end\":9147,\"start\":7827},{\"end\":9681,\"start\":9149},{\"end\":10786,\"start\":9683},{\"end\":11355,\"start\":10817},{\"end\":11890,\"start\":11357},{\"end\":12436,\"start\":11892},{\"end\":13148,\"start\":12485},{\"end\":14361,\"start\":13150},{\"end\":16260,\"start\":14427},{\"end\":16705,\"start\":16262},{\"end\":17409,\"start\":16707},{\"end\":18548,\"start\":17447},{\"end\":19003,\"start\":18550},{\"end\":19867,\"start\":19005},{\"end\":22230,\"start\":19869},{\"end\":22589,\"start\":22232},{\"end\":23294,\"start\":22633},{\"end\":23998,\"start\":23296},{\"end\":26450,\"start\":24000},{\"end\":28718,\"start\":26489},{\"end\":29539,\"start\":28720},{\"end\":30954,\"start\":29541},{\"end\":31215,\"start\":30999},{\"end\":31641,\"start\":31274},{\"end\":32105,\"start\":31706},{\"end\":32615,\"start\":32107},{\"end\":34218,\"start\":32617},{\"end\":35379,\"start\":34290},{\"end\":35848,\"start\":35381},{\"end\":37161,\"start\":35865},{\"end\":38352,\"start\":37163},{\"end\":39136,\"start\":38354},{\"end\":39931,\"start\":39151},{\"end\":41176,\"start\":39933},{\"end\":42095,\"start\":41218}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":31705,\"start\":31642},{\"attributes\":{\"id\":\"formula_1\"},\"end\":34289,\"start\":34219}]", "table_ref": "[{\"end\":14294,\"start\":14287},{\"end\":19115,\"start\":19108},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19568,\"start\":19561},{\"end\":20015,\"start\":20008},{\"end\":20923,\"start\":20916},{\"end\":25946,\"start\":25939},{\"end\":42091,\"start\":42084}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1750,\"start\":1738},{\"attributes\":{\"n\":\"2\"},\"end\":7255,\"start\":7219},{\"attributes\":{\"n\":\"2.1\"},\"end\":7284,\"start\":7258},{\"attributes\":{\"n\":\"2.2\"},\"end\":10815,\"start\":10789},{\"attributes\":{\"n\":\"2.3\"},\"end\":12483,\"start\":12439},{\"attributes\":{\"n\":\"3\"},\"end\":14404,\"start\":14364},{\"attributes\":{\"n\":\"3.1\"},\"end\":14425,\"start\":14407},{\"attributes\":{\"n\":\"3.2\"},\"end\":17445,\"start\":17412},{\"attributes\":{\"n\":\"3.3\"},\"end\":22631,\"start\":22592},{\"attributes\":{\"n\":\"3.4\"},\"end\":26487,\"start\":26453},{\"attributes\":{\"n\":\"4\"},\"end\":30997,\"start\":30957},{\"end\":31272,\"start\":31218},{\"attributes\":{\"n\":\"5\"},\"end\":35863,\"start\":35851},{\"attributes\":{\"n\":\"6\"},\"end\":39149,\"start\":39139},{\"end\":41216,\"start\":41179},{\"end\":42098,\"start\":42097},{\"end\":43802,\"start\":43792},{\"end\":44045,\"start\":44035},{\"end\":44290,\"start\":44280},{\"end\":44512,\"start\":44502},{\"end\":44665,\"start\":44655},{\"end\":45058,\"start\":45048},{\"end\":45217,\"start\":45207},{\"end\":45565,\"start\":45561},{\"end\":47273,\"start\":47264},{\"end\":47844,\"start\":47835},{\"end\":48022,\"start\":48013}]", "table": "[{\"end\":47262,\"start\":46938},{\"end\":48011,\"start\":47861}]", "figure_caption": "[{\"end\":43330,\"start\":42099},{\"end\":43790,\"start\":43333},{\"end\":44033,\"start\":43804},{\"end\":44278,\"start\":44047},{\"end\":44500,\"start\":44292},{\"end\":44653,\"start\":44514},{\"end\":45046,\"start\":44667},{\"end\":45205,\"start\":45060},{\"end\":45559,\"start\":45219},{\"end\":46586,\"start\":45568},{\"end\":46938,\"start\":46589},{\"end\":47511,\"start\":47275},{\"end\":47833,\"start\":47514},{\"end\":47861,\"start\":47846},{\"end\":48044,\"start\":48024}]", "figure_ref": "[{\"end\":17551,\"start\":17543},{\"end\":18790,\"start\":18781},{\"end\":18876,\"start\":18867},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22137,\"start\":22129},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23931,\"start\":23923},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23997,\"start\":23989},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24011,\"start\":24003},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24905,\"start\":24897},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25669,\"start\":25661},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27288,\"start\":27280},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27648,\"start\":27640},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27710,\"start\":27702},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28773,\"start\":28765},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29687,\"start\":29679},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":34301,\"start\":34293},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":34496,\"start\":34479}]", "bib_author_first_name": "[{\"end\":48664,\"start\":48663},{\"end\":48787,\"start\":48786},{\"end\":48802,\"start\":48801},{\"end\":48810,\"start\":48809},{\"end\":48817,\"start\":48816},{\"end\":48827,\"start\":48826},{\"end\":49136,\"start\":49135},{\"end\":49149,\"start\":49148},{\"end\":49157,\"start\":49156},{\"end\":49167,\"start\":49166},{\"end\":49169,\"start\":49168},{\"end\":49521,\"start\":49520},{\"end\":49523,\"start\":49522},{\"end\":49534,\"start\":49533},{\"end\":49549,\"start\":49548},{\"end\":49559,\"start\":49558},{\"end\":49850,\"start\":49849},{\"end\":49860,\"start\":49859},{\"end\":49870,\"start\":49869},{\"end\":50121,\"start\":50120},{\"end\":50131,\"start\":50130},{\"end\":50141,\"start\":50140},{\"end\":50593,\"start\":50592},{\"end\":50603,\"start\":50602},{\"end\":50613,\"start\":50612},{\"end\":50626,\"start\":50625},{\"end\":50634,\"start\":50633},{\"end\":50645,\"start\":50644},{\"end\":51087,\"start\":51086},{\"end\":51100,\"start\":51099},{\"end\":51113,\"start\":51112},{\"end\":51345,\"start\":51344},{\"end\":51357,\"start\":51356},{\"end\":51368,\"start\":51367},{\"end\":51381,\"start\":51380},{\"end\":51389,\"start\":51388},{\"end\":51401,\"start\":51400},{\"end\":51411,\"start\":51410},{\"end\":51413,\"start\":51412},{\"end\":51423,\"start\":51422},{\"end\":51432,\"start\":51431},{\"end\":51445,\"start\":51444},{\"end\":51456,\"start\":51455},{\"end\":51458,\"start\":51457},{\"end\":51471,\"start\":51470},{\"end\":51481,\"start\":51480},{\"end\":51491,\"start\":51490},{\"end\":51881,\"start\":51880},{\"end\":52165,\"start\":52164},{\"end\":52173,\"start\":52172},{\"end\":52180,\"start\":52179},{\"end\":52187,\"start\":52186},{\"end\":52194,\"start\":52193},{\"end\":52200,\"start\":52199},{\"end\":52207,\"start\":52206},{\"end\":52553,\"start\":52552},{\"end\":52565,\"start\":52564},{\"end\":52574,\"start\":52573},{\"end\":52584,\"start\":52583},{\"end\":52597,\"start\":52596},{\"end\":52607,\"start\":52606},{\"end\":52616,\"start\":52615},{\"end\":52631,\"start\":52630},{\"end\":52981,\"start\":52980},{\"end\":52989,\"start\":52988},{\"end\":52997,\"start\":52996},{\"end\":53007,\"start\":53006},{\"end\":53009,\"start\":53008},{\"end\":53015,\"start\":53014},{\"end\":53021,\"start\":53020},{\"end\":53382,\"start\":53381},{\"end\":53390,\"start\":53389},{\"end\":53397,\"start\":53396},{\"end\":53404,\"start\":53403},{\"end\":53406,\"start\":53405},{\"end\":53670,\"start\":53669},{\"end\":53678,\"start\":53677},{\"end\":53686,\"start\":53685},{\"end\":53688,\"start\":53687},{\"end\":53696,\"start\":53695},{\"end\":54031,\"start\":54030},{\"end\":54039,\"start\":54038},{\"end\":54041,\"start\":54040},{\"end\":54049,\"start\":54048},{\"end\":54330,\"start\":54329},{\"end\":54338,\"start\":54337},{\"end\":54345,\"start\":54344},{\"end\":54356,\"start\":54355},{\"end\":54369,\"start\":54368},{\"end\":54381,\"start\":54380},{\"end\":54395,\"start\":54394},{\"end\":54406,\"start\":54405},{\"end\":54416,\"start\":54415},{\"end\":54744,\"start\":54743},{\"end\":54753,\"start\":54752},{\"end\":54765,\"start\":54764},{\"end\":55263,\"start\":55262},{\"end\":55276,\"start\":55275},{\"end\":55285,\"start\":55284},{\"end\":55295,\"start\":55294},{\"end\":55302,\"start\":55301},{\"end\":55317,\"start\":55316},{\"end\":55621,\"start\":55620},{\"end\":55632,\"start\":55631},{\"end\":55634,\"start\":55633},{\"end\":55645,\"start\":55644},{\"end\":55654,\"start\":55653},{\"end\":55664,\"start\":55663},{\"end\":55674,\"start\":55673},{\"end\":55676,\"start\":55675},{\"end\":55687,\"start\":55686},{\"end\":55699,\"start\":55698},{\"end\":55710,\"start\":55709},{\"end\":55721,\"start\":55720},{\"end\":56043,\"start\":56042},{\"end\":56054,\"start\":56053},{\"end\":56061,\"start\":56060},{\"end\":56298,\"start\":56297},{\"end\":56312,\"start\":56311},{\"end\":56533,\"start\":56532},{\"end\":56540,\"start\":56539},{\"end\":56546,\"start\":56545},{\"end\":56561,\"start\":56560},{\"end\":56996,\"start\":56995},{\"end\":57007,\"start\":57006},{\"end\":57017,\"start\":57016},{\"end\":57278,\"start\":57277},{\"end\":57288,\"start\":57287},{\"end\":57538,\"start\":57537},{\"end\":57549,\"start\":57548},{\"end\":57701,\"start\":57700},{\"end\":57710,\"start\":57709},{\"end\":57718,\"start\":57717},{\"end\":57735,\"start\":57734},{\"end\":58004,\"start\":58003},{\"end\":58008,\"start\":58005},{\"end\":58018,\"start\":58017},{\"end\":58031,\"start\":58030},{\"end\":58365,\"start\":58364},{\"end\":58386,\"start\":58385},{\"end\":58399,\"start\":58398},{\"end\":58414,\"start\":58413},{\"end\":58416,\"start\":58415},{\"end\":58777,\"start\":58776},{\"end\":58795,\"start\":58794},{\"end\":58805,\"start\":58804},{\"end\":58817,\"start\":58816},{\"end\":58828,\"start\":58827},{\"end\":58844,\"start\":58843},{\"end\":58846,\"start\":58845},{\"end\":58854,\"start\":58853},{\"end\":59420,\"start\":59419},{\"end\":59428,\"start\":59427},{\"end\":59438,\"start\":59437},{\"end\":59821,\"start\":59820},{\"end\":59831,\"start\":59830},{\"end\":59842,\"start\":59841},{\"end\":59844,\"start\":59843},{\"end\":59851,\"start\":59850},{\"end\":59853,\"start\":59852},{\"end\":59863,\"start\":59862},{\"end\":59865,\"start\":59864},{\"end\":59879,\"start\":59878},{\"end\":59888,\"start\":59887},{\"end\":59890,\"start\":59889},{\"end\":59900,\"start\":59899},{\"end\":59902,\"start\":59901},{\"end\":59911,\"start\":59910},{\"end\":60195,\"start\":60194},{\"end\":60197,\"start\":60196},{\"end\":60207,\"start\":60206},{\"end\":60217,\"start\":60216},{\"end\":60230,\"start\":60229},{\"end\":60232,\"start\":60231},{\"end\":60433,\"start\":60432},{\"end\":60445,\"start\":60444},{\"end\":60457,\"start\":60456},{\"end\":60466,\"start\":60465},{\"end\":60468,\"start\":60467},{\"end\":60811,\"start\":60810},{\"end\":60821,\"start\":60820},{\"end\":60830,\"start\":60829},{\"end\":60839,\"start\":60838},{\"end\":60848,\"start\":60847},{\"end\":60860,\"start\":60859},{\"end\":60870,\"start\":60869},{\"end\":60881,\"start\":60880},{\"end\":60888,\"start\":60887},{\"end\":60902,\"start\":60901},{\"end\":61160,\"start\":61159},{\"end\":61171,\"start\":61170},{\"end\":61189,\"start\":61188},{\"end\":61199,\"start\":61198},{\"end\":61201,\"start\":61200},{\"end\":61579,\"start\":61578},{\"end\":61590,\"start\":61589},{\"end\":61608,\"start\":61607},{\"end\":61610,\"start\":61609},{\"end\":62027,\"start\":62026},{\"end\":62031,\"start\":62028},{\"end\":62503,\"start\":62502},{\"end\":62505,\"start\":62504},{\"end\":62519,\"start\":62518},{\"end\":62529,\"start\":62528},{\"end\":62538,\"start\":62537},{\"end\":62540,\"start\":62539},{\"end\":62550,\"start\":62549},{\"end\":62552,\"start\":62551},{\"end\":62559,\"start\":62558},{\"end\":62566,\"start\":62565},{\"end\":62574,\"start\":62573},{\"end\":62576,\"start\":62575},{\"end\":63036,\"start\":63035},{\"end\":63046,\"start\":63045},{\"end\":63055,\"start\":63054},{\"end\":63063,\"start\":63062},{\"end\":63073,\"start\":63072},{\"end\":63084,\"start\":63083},{\"end\":63096,\"start\":63095},{\"end\":63392,\"start\":63391},{\"end\":63407,\"start\":63406},{\"end\":63409,\"start\":63408},{\"end\":63424,\"start\":63423},{\"end\":63426,\"start\":63425},{\"end\":63845,\"start\":63844},{\"end\":63856,\"start\":63855},{\"end\":63858,\"start\":63857},{\"end\":63867,\"start\":63866},{\"end\":63877,\"start\":63876},{\"end\":63886,\"start\":63885},{\"end\":63896,\"start\":63895},{\"end\":63908,\"start\":63907},{\"end\":64297,\"start\":64296},{\"end\":64305,\"start\":64304},{\"end\":64314,\"start\":64313},{\"end\":64725,\"start\":64724},{\"end\":64739,\"start\":64738},{\"end\":64743,\"start\":64740},{\"end\":64750,\"start\":64749},{\"end\":64752,\"start\":64751},{\"end\":64990,\"start\":64989},{\"end\":64999,\"start\":64998},{\"end\":65012,\"start\":65011},{\"end\":65022,\"start\":65019},{\"end\":65031,\"start\":65030},{\"end\":65043,\"start\":65042},{\"end\":65298,\"start\":65297},{\"end\":65305,\"start\":65304},{\"end\":65316,\"start\":65315},{\"end\":65318,\"start\":65317},{\"end\":65328,\"start\":65327},{\"end\":65330,\"start\":65329},{\"end\":65552,\"start\":65551},{\"end\":65561,\"start\":65560},{\"end\":65568,\"start\":65567},{\"end\":65570,\"start\":65569},{\"end\":65580,\"start\":65579},{\"end\":65586,\"start\":65585},{\"end\":65894,\"start\":65893},{\"end\":65903,\"start\":65902},{\"end\":65910,\"start\":65909},{\"end\":65912,\"start\":65911},{\"end\":65922,\"start\":65921},{\"end\":65928,\"start\":65927},{\"end\":66236,\"start\":66235},{\"end\":66244,\"start\":66243},{\"end\":66253,\"start\":66252},{\"end\":66514,\"start\":66513},{\"end\":66522,\"start\":66521},{\"end\":66530,\"start\":66529},{\"end\":66729,\"start\":66728},{\"end\":66737,\"start\":66736},{\"end\":66747,\"start\":66746},{\"end\":66756,\"start\":66755},{\"end\":66766,\"start\":66765},{\"end\":66776,\"start\":66775},{\"end\":67093,\"start\":67092},{\"end\":67101,\"start\":67100},{\"end\":67111,\"start\":67110},{\"end\":67121,\"start\":67120},{\"end\":67129,\"start\":67128},{\"end\":67139,\"start\":67138},{\"end\":67331,\"start\":67330},{\"end\":67339,\"start\":67338},{\"end\":67345,\"start\":67344},{\"end\":67351,\"start\":67350}]", "bib_author_last_name": "[{\"end\":48668,\"start\":48665},{\"end\":48799,\"start\":48788},{\"end\":48807,\"start\":48803},{\"end\":48814,\"start\":48811},{\"end\":48824,\"start\":48818},{\"end\":48837,\"start\":48828},{\"end\":49146,\"start\":49137},{\"end\":49154,\"start\":49150},{\"end\":49164,\"start\":49158},{\"end\":49176,\"start\":49170},{\"end\":49531,\"start\":49524},{\"end\":49546,\"start\":49535},{\"end\":49556,\"start\":49550},{\"end\":49564,\"start\":49560},{\"end\":49857,\"start\":49851},{\"end\":49867,\"start\":49861},{\"end\":49877,\"start\":49871},{\"end\":50128,\"start\":50122},{\"end\":50138,\"start\":50132},{\"end\":50148,\"start\":50142},{\"end\":50600,\"start\":50594},{\"end\":50610,\"start\":50604},{\"end\":50623,\"start\":50614},{\"end\":50631,\"start\":50627},{\"end\":50642,\"start\":50635},{\"end\":50650,\"start\":50646},{\"end\":51097,\"start\":51088},{\"end\":51110,\"start\":51101},{\"end\":51121,\"start\":51114},{\"end\":51354,\"start\":51346},{\"end\":51365,\"start\":51358},{\"end\":51378,\"start\":51369},{\"end\":51386,\"start\":51382},{\"end\":51398,\"start\":51390},{\"end\":51408,\"start\":51402},{\"end\":51420,\"start\":51414},{\"end\":51429,\"start\":51424},{\"end\":51442,\"start\":51433},{\"end\":51453,\"start\":51446},{\"end\":51468,\"start\":51459},{\"end\":51478,\"start\":51472},{\"end\":51488,\"start\":51482},{\"end\":51502,\"start\":51492},{\"end\":51891,\"start\":51882},{\"end\":52170,\"start\":52166},{\"end\":52177,\"start\":52174},{\"end\":52184,\"start\":52181},{\"end\":52191,\"start\":52188},{\"end\":52197,\"start\":52195},{\"end\":52204,\"start\":52201},{\"end\":52210,\"start\":52208},{\"end\":52562,\"start\":52554},{\"end\":52571,\"start\":52566},{\"end\":52581,\"start\":52575},{\"end\":52594,\"start\":52585},{\"end\":52604,\"start\":52598},{\"end\":52613,\"start\":52608},{\"end\":52628,\"start\":52617},{\"end\":52636,\"start\":52632},{\"end\":52986,\"start\":52982},{\"end\":52994,\"start\":52990},{\"end\":53004,\"start\":52998},{\"end\":53012,\"start\":53010},{\"end\":53018,\"start\":53016},{\"end\":53029,\"start\":53022},{\"end\":53387,\"start\":53383},{\"end\":53394,\"start\":53391},{\"end\":53401,\"start\":53398},{\"end\":53411,\"start\":53407},{\"end\":53675,\"start\":53671},{\"end\":53683,\"start\":53679},{\"end\":53693,\"start\":53689},{\"end\":53700,\"start\":53697},{\"end\":54036,\"start\":54032},{\"end\":54046,\"start\":54042},{\"end\":54062,\"start\":54050},{\"end\":54335,\"start\":54331},{\"end\":54342,\"start\":54339},{\"end\":54353,\"start\":54346},{\"end\":54366,\"start\":54357},{\"end\":54378,\"start\":54370},{\"end\":54392,\"start\":54382},{\"end\":54403,\"start\":54396},{\"end\":54413,\"start\":54407},{\"end\":54423,\"start\":54417},{\"end\":54750,\"start\":54745},{\"end\":54762,\"start\":54754},{\"end\":54776,\"start\":54766},{\"end\":55273,\"start\":55264},{\"end\":55282,\"start\":55277},{\"end\":55292,\"start\":55286},{\"end\":55299,\"start\":55296},{\"end\":55314,\"start\":55303},{\"end\":55320,\"start\":55318},{\"end\":55629,\"start\":55622},{\"end\":55642,\"start\":55635},{\"end\":55651,\"start\":55646},{\"end\":55661,\"start\":55655},{\"end\":55671,\"start\":55665},{\"end\":55684,\"start\":55677},{\"end\":55696,\"start\":55688},{\"end\":55707,\"start\":55700},{\"end\":55718,\"start\":55711},{\"end\":55730,\"start\":55722},{\"end\":56051,\"start\":56044},{\"end\":56058,\"start\":56055},{\"end\":56065,\"start\":56062},{\"end\":56309,\"start\":56299},{\"end\":56319,\"start\":56313},{\"end\":56537,\"start\":56534},{\"end\":56543,\"start\":56541},{\"end\":56558,\"start\":56547},{\"end\":56567,\"start\":56562},{\"end\":57004,\"start\":56997},{\"end\":57014,\"start\":57008},{\"end\":57026,\"start\":57018},{\"end\":57285,\"start\":57279},{\"end\":57298,\"start\":57289},{\"end\":57546,\"start\":57539},{\"end\":57556,\"start\":57550},{\"end\":57707,\"start\":57702},{\"end\":57715,\"start\":57711},{\"end\":57732,\"start\":57719},{\"end\":57745,\"start\":57736},{\"end\":58015,\"start\":58009},{\"end\":58028,\"start\":58019},{\"end\":58039,\"start\":58032},{\"end\":58383,\"start\":58366},{\"end\":58396,\"start\":58387},{\"end\":58411,\"start\":58400},{\"end\":58420,\"start\":58417},{\"end\":58792,\"start\":58778},{\"end\":58802,\"start\":58796},{\"end\":58814,\"start\":58806},{\"end\":58825,\"start\":58818},{\"end\":58841,\"start\":58829},{\"end\":58851,\"start\":58847},{\"end\":58859,\"start\":58855},{\"end\":59425,\"start\":59421},{\"end\":59435,\"start\":59429},{\"end\":59449,\"start\":59439},{\"end\":59828,\"start\":59822},{\"end\":59839,\"start\":59832},{\"end\":59848,\"start\":59845},{\"end\":59860,\"start\":59854},{\"end\":59876,\"start\":59866},{\"end\":59885,\"start\":59880},{\"end\":59897,\"start\":59891},{\"end\":59908,\"start\":59903},{\"end\":59915,\"start\":59912},{\"end\":60204,\"start\":60198},{\"end\":60214,\"start\":60208},{\"end\":60227,\"start\":60218},{\"end\":60240,\"start\":60233},{\"end\":60442,\"start\":60434},{\"end\":60454,\"start\":60446},{\"end\":60463,\"start\":60458},{\"end\":60476,\"start\":60469},{\"end\":60818,\"start\":60812},{\"end\":60827,\"start\":60822},{\"end\":60836,\"start\":60831},{\"end\":60845,\"start\":60840},{\"end\":60857,\"start\":60849},{\"end\":60867,\"start\":60861},{\"end\":60878,\"start\":60871},{\"end\":60885,\"start\":60882},{\"end\":60899,\"start\":60889},{\"end\":60909,\"start\":60903},{\"end\":61168,\"start\":61161},{\"end\":61186,\"start\":61172},{\"end\":61196,\"start\":61190},{\"end\":61206,\"start\":61202},{\"end\":61587,\"start\":61580},{\"end\":61605,\"start\":61591},{\"end\":61615,\"start\":61611},{\"end\":62042,\"start\":62032},{\"end\":62516,\"start\":62506},{\"end\":62526,\"start\":62520},{\"end\":62535,\"start\":62530},{\"end\":62547,\"start\":62541},{\"end\":62556,\"start\":62553},{\"end\":62563,\"start\":62560},{\"end\":62571,\"start\":62567},{\"end\":62582,\"start\":62577},{\"end\":63043,\"start\":63037},{\"end\":63052,\"start\":63047},{\"end\":63060,\"start\":63056},{\"end\":63070,\"start\":63064},{\"end\":63081,\"start\":63074},{\"end\":63093,\"start\":63085},{\"end\":63114,\"start\":63097},{\"end\":63404,\"start\":63393},{\"end\":63421,\"start\":63410},{\"end\":63433,\"start\":63427},{\"end\":63853,\"start\":63846},{\"end\":63864,\"start\":63859},{\"end\":63874,\"start\":63868},{\"end\":63883,\"start\":63878},{\"end\":63893,\"start\":63887},{\"end\":63905,\"start\":63897},{\"end\":63918,\"start\":63909},{\"end\":64302,\"start\":64298},{\"end\":64311,\"start\":64306},{\"end\":64321,\"start\":64315},{\"end\":64736,\"start\":64726},{\"end\":64747,\"start\":64744},{\"end\":64758,\"start\":64753},{\"end\":64996,\"start\":64991},{\"end\":65009,\"start\":65000},{\"end\":65017,\"start\":65013},{\"end\":65028,\"start\":65023},{\"end\":65040,\"start\":65032},{\"end\":65302,\"start\":65299},{\"end\":65313,\"start\":65306},{\"end\":65325,\"start\":65319},{\"end\":65338,\"start\":65331},{\"end\":65558,\"start\":65553},{\"end\":65565,\"start\":65562},{\"end\":65577,\"start\":65571},{\"end\":65583,\"start\":65581},{\"end\":65590,\"start\":65587},{\"end\":65900,\"start\":65895},{\"end\":65907,\"start\":65904},{\"end\":65919,\"start\":65913},{\"end\":65925,\"start\":65923},{\"end\":65932,\"start\":65929},{\"end\":66241,\"start\":66237},{\"end\":66250,\"start\":66245},{\"end\":66262,\"start\":66254},{\"end\":66519,\"start\":66515},{\"end\":66527,\"start\":66523},{\"end\":66537,\"start\":66531},{\"end\":66734,\"start\":66730},{\"end\":66744,\"start\":66738},{\"end\":66753,\"start\":66748},{\"end\":66763,\"start\":66757},{\"end\":66773,\"start\":66767},{\"end\":66781,\"start\":66777},{\"end\":67098,\"start\":67094},{\"end\":67108,\"start\":67102},{\"end\":67118,\"start\":67112},{\"end\":67126,\"start\":67122},{\"end\":67136,\"start\":67130},{\"end\":67144,\"start\":67140},{\"end\":67336,\"start\":67332},{\"end\":67342,\"start\":67340},{\"end\":67348,\"start\":67346},{\"end\":67356,\"start\":67352}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":40007426},\"end\":48784,\"start\":48602},{\"attributes\":{\"doi\":\"arXiv:1807.00459\",\"id\":\"b1\"},\"end\":49042,\"start\":48786},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":25042971},\"end\":49460,\"start\":49044},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":54203999},\"end\":49792,\"start\":49462},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2649314},\"end\":50067,\"start\":49794},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9089716},\"end\":50539,\"start\":50069},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6397074},\"end\":51012,\"start\":50541},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":28527385},\"end\":51290,\"start\":51014},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":59599820},\"end\":51831,\"start\":51292},{\"attributes\":{\"id\":\"b9\"},\"end\":52049,\"start\":51833},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3499678},\"end\":52452,\"start\":52051},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":128088823},\"end\":52926,\"start\":52454},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206597351},\"end\":53310,\"start\":52928},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208310035},\"end\":53613,\"start\":53312},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52198423},\"end\":54028,\"start\":53615},{\"attributes\":{\"doi\":\"arXiv:1808.04866\",\"id\":\"b15\"},\"end\":54276,\"start\":54030},{\"attributes\":{\"doi\":\"arXiv:1811.03604\",\"id\":\"b16\"},\"end\":54660,\"start\":54278},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5051282},\"end\":55166,\"start\":54662},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4551073},\"end\":55618,\"start\":55168},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b19\"},\"end\":56040,\"start\":55620},{\"attributes\":{\"id\":\"b20\"},\"end\":56295,\"start\":56042},{\"attributes\":{\"id\":\"b21\"},\"end\":56472,\"start\":56297},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":721235},\"end\":56914,\"start\":56474},{\"attributes\":{\"id\":\"b23\"},\"end\":57226,\"start\":56916},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":24961049},\"end\":57498,\"start\":57228},{\"attributes\":{\"id\":\"b25\"},\"end\":57633,\"start\":57500},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53099247},\"end\":58001,\"start\":57635},{\"attributes\":{\"doi\":\"arXiv:1802.07927\",\"id\":\"b27\"},\"end\":58281,\"start\":58003},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":18466660},\"end\":58696,\"start\":58283},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12424035},\"end\":59277,\"start\":58698},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":133091488},\"end\":59761,\"start\":59279},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15667091},\"end\":60114,\"start\":59763},{\"attributes\":{\"id\":\"b32\"},\"end\":60383,\"start\":60116},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":44237208},\"end\":60739,\"start\":60385},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":202786778},\"end\":61157,\"start\":60741},{\"attributes\":{\"doi\":\"arXiv:1802.03041\",\"id\":\"b35\"},\"end\":61515,\"start\":61159},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3678260},\"end\":61782,\"start\":61517},{\"attributes\":{\"id\":\"b37\"},\"end\":62422,\"start\":61784},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1553099},\"end\":62975,\"start\":62424},{\"attributes\":{\"doi\":\"arXiv:1811.04017\",\"id\":\"b39\"},\"end\":63338,\"start\":62977},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5041741},\"end\":63769,\"start\":63340},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4626477},\"end\":64211,\"start\":63771},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5976727},\"end\":64675,\"start\":64213},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":35426171},\"end\":64891,\"start\":64677},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3971778},\"end\":65295,\"start\":64893},{\"attributes\":{\"doi\":\"arXiv:1911.07963\",\"id\":\"b45\"},\"end\":65549,\"start\":65297},{\"attributes\":{\"doi\":\"arXiv:1807.09173\",\"id\":\"b46\"},\"end\":65815,\"start\":65551},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":86836429},\"end\":66150,\"start\":65817},{\"attributes\":{\"doi\":\"arXiv:1708.07747\",\"id\":\"b48\"},\"end\":66452,\"start\":66152},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2120593},\"end\":66664,\"start\":66454},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5077922},\"end\":67027,\"start\":66666},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":579561},\"end\":67328,\"start\":67029},{\"attributes\":{\"doi\":\"arXiv:1703.01340\",\"id\":\"b52\"},\"end\":67582,\"start\":67330}]", "bib_title": "[{\"end\":48661,\"start\":48602},{\"end\":49133,\"start\":49044},{\"end\":49518,\"start\":49462},{\"end\":49847,\"start\":49794},{\"end\":50118,\"start\":50069},{\"end\":50590,\"start\":50541},{\"end\":51084,\"start\":51014},{\"end\":51342,\"start\":51292},{\"end\":52162,\"start\":52051},{\"end\":52550,\"start\":52454},{\"end\":52978,\"start\":52928},{\"end\":53379,\"start\":53312},{\"end\":53667,\"start\":53615},{\"end\":54741,\"start\":54662},{\"end\":55260,\"start\":55168},{\"end\":56530,\"start\":56474},{\"end\":56993,\"start\":56916},{\"end\":57275,\"start\":57228},{\"end\":57698,\"start\":57635},{\"end\":58362,\"start\":58283},{\"end\":58774,\"start\":58698},{\"end\":59417,\"start\":59279},{\"end\":59818,\"start\":59763},{\"end\":60430,\"start\":60385},{\"end\":60808,\"start\":60741},{\"end\":61576,\"start\":61517},{\"end\":62024,\"start\":61784},{\"end\":62500,\"start\":62424},{\"end\":63389,\"start\":63340},{\"end\":63842,\"start\":63771},{\"end\":64294,\"start\":64213},{\"end\":64722,\"start\":64677},{\"end\":64987,\"start\":64893},{\"end\":65891,\"start\":65817},{\"end\":66511,\"start\":66454},{\"end\":66726,\"start\":66666},{\"end\":67090,\"start\":67029}]", "bib_author": "[{\"end\":48670,\"start\":48663},{\"end\":48801,\"start\":48786},{\"end\":48809,\"start\":48801},{\"end\":48816,\"start\":48809},{\"end\":48826,\"start\":48816},{\"end\":48839,\"start\":48826},{\"end\":49148,\"start\":49135},{\"end\":49156,\"start\":49148},{\"end\":49166,\"start\":49156},{\"end\":49178,\"start\":49166},{\"end\":49533,\"start\":49520},{\"end\":49548,\"start\":49533},{\"end\":49558,\"start\":49548},{\"end\":49566,\"start\":49558},{\"end\":49859,\"start\":49849},{\"end\":49869,\"start\":49859},{\"end\":49879,\"start\":49869},{\"end\":50130,\"start\":50120},{\"end\":50140,\"start\":50130},{\"end\":50150,\"start\":50140},{\"end\":50602,\"start\":50592},{\"end\":50612,\"start\":50602},{\"end\":50625,\"start\":50612},{\"end\":50633,\"start\":50625},{\"end\":50644,\"start\":50633},{\"end\":50652,\"start\":50644},{\"end\":51099,\"start\":51086},{\"end\":51112,\"start\":51099},{\"end\":51123,\"start\":51112},{\"end\":51356,\"start\":51344},{\"end\":51367,\"start\":51356},{\"end\":51380,\"start\":51367},{\"end\":51388,\"start\":51380},{\"end\":51400,\"start\":51388},{\"end\":51410,\"start\":51400},{\"end\":51422,\"start\":51410},{\"end\":51431,\"start\":51422},{\"end\":51444,\"start\":51431},{\"end\":51455,\"start\":51444},{\"end\":51470,\"start\":51455},{\"end\":51480,\"start\":51470},{\"end\":51490,\"start\":51480},{\"end\":51504,\"start\":51490},{\"end\":51893,\"start\":51880},{\"end\":52172,\"start\":52164},{\"end\":52179,\"start\":52172},{\"end\":52186,\"start\":52179},{\"end\":52193,\"start\":52186},{\"end\":52199,\"start\":52193},{\"end\":52206,\"start\":52199},{\"end\":52212,\"start\":52206},{\"end\":52564,\"start\":52552},{\"end\":52573,\"start\":52564},{\"end\":52583,\"start\":52573},{\"end\":52596,\"start\":52583},{\"end\":52606,\"start\":52596},{\"end\":52615,\"start\":52606},{\"end\":52630,\"start\":52615},{\"end\":52638,\"start\":52630},{\"end\":52988,\"start\":52980},{\"end\":52996,\"start\":52988},{\"end\":53006,\"start\":52996},{\"end\":53014,\"start\":53006},{\"end\":53020,\"start\":53014},{\"end\":53031,\"start\":53020},{\"end\":53389,\"start\":53381},{\"end\":53396,\"start\":53389},{\"end\":53403,\"start\":53396},{\"end\":53413,\"start\":53403},{\"end\":53677,\"start\":53669},{\"end\":53685,\"start\":53677},{\"end\":53695,\"start\":53685},{\"end\":53702,\"start\":53695},{\"end\":54038,\"start\":54030},{\"end\":54048,\"start\":54038},{\"end\":54064,\"start\":54048},{\"end\":54337,\"start\":54329},{\"end\":54344,\"start\":54337},{\"end\":54355,\"start\":54344},{\"end\":54368,\"start\":54355},{\"end\":54380,\"start\":54368},{\"end\":54394,\"start\":54380},{\"end\":54405,\"start\":54394},{\"end\":54415,\"start\":54405},{\"end\":54425,\"start\":54415},{\"end\":54752,\"start\":54743},{\"end\":54764,\"start\":54752},{\"end\":54778,\"start\":54764},{\"end\":55275,\"start\":55262},{\"end\":55284,\"start\":55275},{\"end\":55294,\"start\":55284},{\"end\":55301,\"start\":55294},{\"end\":55316,\"start\":55301},{\"end\":55322,\"start\":55316},{\"end\":55631,\"start\":55620},{\"end\":55644,\"start\":55631},{\"end\":55653,\"start\":55644},{\"end\":55663,\"start\":55653},{\"end\":55673,\"start\":55663},{\"end\":55686,\"start\":55673},{\"end\":55698,\"start\":55686},{\"end\":55709,\"start\":55698},{\"end\":55720,\"start\":55709},{\"end\":55732,\"start\":55720},{\"end\":56053,\"start\":56042},{\"end\":56060,\"start\":56053},{\"end\":56067,\"start\":56060},{\"end\":56311,\"start\":56297},{\"end\":56321,\"start\":56311},{\"end\":56539,\"start\":56532},{\"end\":56545,\"start\":56539},{\"end\":56560,\"start\":56545},{\"end\":56569,\"start\":56560},{\"end\":57006,\"start\":56995},{\"end\":57016,\"start\":57006},{\"end\":57028,\"start\":57016},{\"end\":57287,\"start\":57277},{\"end\":57300,\"start\":57287},{\"end\":57548,\"start\":57537},{\"end\":57558,\"start\":57548},{\"end\":57709,\"start\":57700},{\"end\":57717,\"start\":57709},{\"end\":57734,\"start\":57717},{\"end\":57747,\"start\":57734},{\"end\":58017,\"start\":58003},{\"end\":58030,\"start\":58017},{\"end\":58041,\"start\":58030},{\"end\":58385,\"start\":58364},{\"end\":58398,\"start\":58385},{\"end\":58413,\"start\":58398},{\"end\":58422,\"start\":58413},{\"end\":58794,\"start\":58776},{\"end\":58804,\"start\":58794},{\"end\":58816,\"start\":58804},{\"end\":58827,\"start\":58816},{\"end\":58843,\"start\":58827},{\"end\":58853,\"start\":58843},{\"end\":58861,\"start\":58853},{\"end\":59427,\"start\":59419},{\"end\":59437,\"start\":59427},{\"end\":59451,\"start\":59437},{\"end\":59830,\"start\":59820},{\"end\":59841,\"start\":59830},{\"end\":59850,\"start\":59841},{\"end\":59862,\"start\":59850},{\"end\":59878,\"start\":59862},{\"end\":59887,\"start\":59878},{\"end\":59899,\"start\":59887},{\"end\":59910,\"start\":59899},{\"end\":59917,\"start\":59910},{\"end\":60206,\"start\":60194},{\"end\":60216,\"start\":60206},{\"end\":60229,\"start\":60216},{\"end\":60242,\"start\":60229},{\"end\":60444,\"start\":60432},{\"end\":60456,\"start\":60444},{\"end\":60465,\"start\":60456},{\"end\":60478,\"start\":60465},{\"end\":60820,\"start\":60810},{\"end\":60829,\"start\":60820},{\"end\":60838,\"start\":60829},{\"end\":60847,\"start\":60838},{\"end\":60859,\"start\":60847},{\"end\":60869,\"start\":60859},{\"end\":60880,\"start\":60869},{\"end\":60887,\"start\":60880},{\"end\":60901,\"start\":60887},{\"end\":60911,\"start\":60901},{\"end\":61170,\"start\":61159},{\"end\":61188,\"start\":61170},{\"end\":61198,\"start\":61188},{\"end\":61208,\"start\":61198},{\"end\":61589,\"start\":61578},{\"end\":61607,\"start\":61589},{\"end\":61617,\"start\":61607},{\"end\":62044,\"start\":62026},{\"end\":62518,\"start\":62502},{\"end\":62528,\"start\":62518},{\"end\":62537,\"start\":62528},{\"end\":62549,\"start\":62537},{\"end\":62558,\"start\":62549},{\"end\":62565,\"start\":62558},{\"end\":62573,\"start\":62565},{\"end\":62584,\"start\":62573},{\"end\":63045,\"start\":63035},{\"end\":63054,\"start\":63045},{\"end\":63062,\"start\":63054},{\"end\":63072,\"start\":63062},{\"end\":63083,\"start\":63072},{\"end\":63095,\"start\":63083},{\"end\":63116,\"start\":63095},{\"end\":63406,\"start\":63391},{\"end\":63423,\"start\":63406},{\"end\":63435,\"start\":63423},{\"end\":63855,\"start\":63844},{\"end\":63866,\"start\":63855},{\"end\":63876,\"start\":63866},{\"end\":63885,\"start\":63876},{\"end\":63895,\"start\":63885},{\"end\":63907,\"start\":63895},{\"end\":63920,\"start\":63907},{\"end\":64304,\"start\":64296},{\"end\":64313,\"start\":64304},{\"end\":64323,\"start\":64313},{\"end\":64738,\"start\":64724},{\"end\":64749,\"start\":64738},{\"end\":64760,\"start\":64749},{\"end\":64998,\"start\":64989},{\"end\":65011,\"start\":64998},{\"end\":65019,\"start\":65011},{\"end\":65030,\"start\":65019},{\"end\":65042,\"start\":65030},{\"end\":65046,\"start\":65042},{\"end\":65304,\"start\":65297},{\"end\":65315,\"start\":65304},{\"end\":65327,\"start\":65315},{\"end\":65340,\"start\":65327},{\"end\":65560,\"start\":65551},{\"end\":65567,\"start\":65560},{\"end\":65579,\"start\":65567},{\"end\":65585,\"start\":65579},{\"end\":65592,\"start\":65585},{\"end\":65902,\"start\":65893},{\"end\":65909,\"start\":65902},{\"end\":65921,\"start\":65909},{\"end\":65927,\"start\":65921},{\"end\":65934,\"start\":65927},{\"end\":66243,\"start\":66235},{\"end\":66252,\"start\":66243},{\"end\":66264,\"start\":66252},{\"end\":66521,\"start\":66513},{\"end\":66529,\"start\":66521},{\"end\":66539,\"start\":66529},{\"end\":66736,\"start\":66728},{\"end\":66746,\"start\":66736},{\"end\":66755,\"start\":66746},{\"end\":66765,\"start\":66755},{\"end\":66775,\"start\":66765},{\"end\":66783,\"start\":66775},{\"end\":67100,\"start\":67092},{\"end\":67110,\"start\":67100},{\"end\":67120,\"start\":67110},{\"end\":67128,\"start\":67120},{\"end\":67138,\"start\":67128},{\"end\":67146,\"start\":67138},{\"end\":67338,\"start\":67330},{\"end\":67344,\"start\":67338},{\"end\":67350,\"start\":67344},{\"end\":67358,\"start\":67350}]", "bib_venue": "[{\"end\":50327,\"start\":50247},{\"end\":50791,\"start\":50730},{\"end\":53833,\"start\":53776},{\"end\":54935,\"start\":54865},{\"end\":56708,\"start\":56647},{\"end\":59000,\"start\":58939},{\"end\":62709,\"start\":62655},{\"end\":63574,\"start\":63513},{\"end\":64460,\"start\":64400},{\"end\":48680,\"start\":48670},{\"end\":48889,\"start\":48855},{\"end\":49235,\"start\":49178},{\"end\":49610,\"start\":49566},{\"end\":49915,\"start\":49879},{\"end\":50245,\"start\":50150},{\"end\":50728,\"start\":50652},{\"end\":51134,\"start\":51123},{\"end\":51514,\"start\":51504},{\"end\":51878,\"start\":51833},{\"end\":52232,\"start\":52212},{\"end\":52668,\"start\":52638},{\"end\":53094,\"start\":53031},{\"end\":53451,\"start\":53413},{\"end\":53774,\"start\":53702},{\"end\":54129,\"start\":54080},{\"end\":54327,\"start\":54278},{\"end\":54863,\"start\":54778},{\"end\":55370,\"start\":55322},{\"end\":55796,\"start\":55748},{\"end\":56161,\"start\":56067},{\"end\":56374,\"start\":56321},{\"end\":56645,\"start\":56569},{\"end\":57056,\"start\":57028},{\"end\":57347,\"start\":57300},{\"end\":57535,\"start\":57500},{\"end\":57795,\"start\":57747},{\"end\":58118,\"start\":58057},{\"end\":58471,\"start\":58422},{\"end\":58937,\"start\":58861},{\"end\":59499,\"start\":59451},{\"end\":59921,\"start\":59917},{\"end\":60192,\"start\":60116},{\"end\":60540,\"start\":60478},{\"end\":60922,\"start\":60911},{\"end\":61313,\"start\":61224},{\"end\":61626,\"start\":61617},{\"end\":62082,\"start\":62044},{\"end\":62653,\"start\":62584},{\"end\":63033,\"start\":62977},{\"end\":63511,\"start\":63435},{\"end\":63969,\"start\":63920},{\"end\":64398,\"start\":64323},{\"end\":64767,\"start\":64760},{\"end\":65076,\"start\":65046},{\"end\":65398,\"start\":65356},{\"end\":65657,\"start\":65608},{\"end\":65973,\"start\":65934},{\"end\":66233,\"start\":66152},{\"end\":66543,\"start\":66539},{\"end\":66827,\"start\":66783},{\"end\":67160,\"start\":67146},{\"end\":67432,\"start\":67374}]"}}}, "year": 2023, "month": 12, "day": 17}