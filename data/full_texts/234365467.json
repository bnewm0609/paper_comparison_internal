{"id": 234365467, "updated": "2023-10-06 05:44:01.024", "metadata": {"title": "DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks", "authors": "[{\"first\":\"Thomas\",\"last\":\"Neff\",\"middle\":[]},{\"first\":\"Pascal\",\"last\":\"Stadlbauer\",\"middle\":[]},{\"first\":\"Mathias\",\"last\":\"Parger\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Kurz\",\"middle\":[]},{\"first\":\"Joerg\",\"last\":\"Mueller\",\"middle\":[\"H.\"]},{\"first\":\"Chakravarty\",\"last\":\"Chaitanya\",\"middle\":[\"R.\",\"Alla\"]},{\"first\":\"Anton\",\"last\":\"Kaplanyan\",\"middle\":[]},{\"first\":\"Markus\",\"last\":\"Steinberger\",\"middle\":[]}]", "venue": "Computer Graphics Forum Volume 40, Issue 4, 2021", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.03231", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/cgf/NeffSPKMCKS21", "doi": "10.1111/cgf.14340"}}, "content": {"source": {"pdf_hash": "792f25e5a2f167df962e0d25b6bee5474a86605f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.03231v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.03231", "status": "GREEN"}}, "grobid": {"id": "0e488c72284954aa31441325fec4261596cf8b1f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/792f25e5a2f167df962e0d25b6bee5474a86605f.txt", "contents": "\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\n2021\n\nT Neff \nGraz University of Technology\nAustria\n\nP Stadlbauer \nGraz University of Technology\nAustria\n\nM Parger \nGraz University of Technology\nAustria\n\nA Kurz \nGraz University of Technology\nAustria\n\nJ H Mueller \nGraz University of Technology\nAustria\n\nC R A Chaitanya \nGraz University of Technology\nAustria\n\nA Kaplanyan \nGraz University of Technology\nAustria\n\nM Steinberger \nGraz University of Technology\nAustria\n\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\n\nEurographics Symposium on Rendering 2021 A. Bousseau and M\n4042021\n\n Figure 1\n: Example renderings with DONeRF at 400 \u00d7 400 pixels for our tested scenes (PSNR is in brackets). All shown results are rendered in real-time at 22 ms per frame on a single GPU and require approximately 4.35 MFLOP per pixel to compute. DONeRF requires only 4 samples per pixel thanks to a depth oracle network to guide sample placement, while NeRF uses 256 samples per pixel in total. We reduce the execution and training time by up to 48\u00d7 and achieve better quality (NeRF average PSNR at 30.52 dB vs. our 31.14 dB).\n\n\nAbstract\n\nThe recent research explosion around implicit neural representations, such as NeRF, shows that there is immense potential for implicitly storing high-quality scene and lighting information in compact neural networks. However, one major limitation preventing the use of NeRF in real-time rendering applications is the prohibitive computational cost of excessive network evaluations along each view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural representations closer to practical rendering of synthetic content in real-time applications, such as games and virtual reality. We show that the number of samples required for each view ray can be significantly reduced when samples are placed around surfaces in the scene without compromising image quality. To this end, we propose a depth oracle network that predicts ray sample locations for each view ray with a single network evaluation. We show that using a classification network around logarithmically discretized and spherically warped depth values is essential to encode surface locations rather than directly estimating depth. The combination of these techniques leads to DONeRF, our compact dual network design with a depth oracle network as its first step and a locally sampled shading network for ray accumulation. With DONeRF, we reduce the inference costs by up to 48x compared to NeRF when conditioning on available ground truth depth information. Compared to concurrent acceleration methods for raymarching-based neural representations, DONeRF does not require additional memory for explicit caching or acceleration structures, and can render interactively (20 frames per second) on a single GPU.\n\n\nCCS Concepts\n\n\u2022 Computing methodologies \u2192 Rendering;\n\n\nIntroduction\n\nReal-time rendering of photorealistic scenes with complex lighting is still an overly demanding problem. However, today, consumer machine learning accelerators are widespread from desktop GPUs to mobile phones and virtual reality (VR) headsets, making evaluation among two multilayer perceptron (MLP) networks to encode scene structure alongside lighting effects. For image generation, NeRF uses traditional volume rendering drawing 256 samples for each view ray, where each sample requires a full network evaluation.\n\nAlthough NeRF-like methods show significant potential for compact high-quality object and scene representations, they are too expensive to evaluate in real-time. Real-time rendering of such a representation onto a VR headset at 1440 \u00d7 1600 pixel per eye with 90 Hz would require 37 petaFLOPS (256 network evaluations each with 256 2 \u00b7 7 multiply add operations). Clearly, this is not possible on current GPU hardware and evaluation cost is a major limiting factor for neural representations to be used for real-time rendering. Additionally, NeRF only works well for small scale content, requiring splitting larger scenes into multiple NeRFs [ZRSK20; RJY*21], multiplying both the memory and evaluation cost.\n\nIn this work, we make neural representations practical for interactive and real-time rendering, while sticking to a tight memory budget. Particularly, our goal is to enable large scale synthetic content in movie quality in real-time rendering. We make the following contributions with depth oracle neural radiance fields (DONeRFs):\n\n\u2022 We propose a compact dual network design to reduce evaluation costs for neural rendering. An oracle network predicts sample locations along view rays and a shading network places a small number of samples guided by the oracle to deliver the final color. \u2022 We present a robust depth oracle network design and training scheme to efficiently provide sample locations for the shading network. The oracle uses filtered, discretized target depth values, which are readily available in synthetic content, and it learns to solve a classification task rather than to directly estimate depth. \u2022 We introduce a non-linear transformation to handle large, open scenes and show that sampling of the shading network should happen in a warped space, to better capture different frequencies in the fore-and background, capturing content in a single network beyond the capability of previous work. \u2022 Combining our efforts, we demonstrate high-quality real-time neural rendering of large synthetic scenes. At the same tight memory budget used by the original NeRF, we show equal quality for small scenes and significant improvements for large scenes, while reducing the computational cost by 24-98\u00d7.\n\nWith DONeRF, we are the first to render large-scale computer graphics scenes from a compact neural representation in real time. Additionally, DONeRF is significantly faster to train. We focus on static synthetic scenes and consider dynamic scenes and animations orthogonal to our work. Still, DONeRF can directly be used as a compact backdrop for distant parts of a game scene, or in VR and augmented reality (AR), where an environment map does not offer the required parallax for a stereo stimulus. Our source code and datasets are available at https://depthoraclenerf.github.io/.\n\n\nRelated work\n\nImage-based novel view synthesis Recently, image-based rendering techniques using multi-plane images (MPIs) [ZTF*18;FBD*19] managed to achieve impressive results by blending image layers. By blending between multiple MPIs [MSO*19] and using spherical image layers [BFO*20] the potential field of view can be increased at the cost of memory efficiency. Further extending MPIs, neural basis functions can be learned to enable real-time view synthesis [WPYS21]. Alternatively, an implicit mapping between view, time or illumination conditions can be learned [BMSR20]. Although explicit image-based representations can be efficiently rendered, they typically only allow for small viewing regions [STB*19] in addition to requiring densely sampled input images, which substantially increases memory requirements for larger viewing regions. Neural Radiance Fields Opening a whole new subdomain of research, Mildenhall et al. [MST*20] introduced NeRF, which replaced the learned raymarching from SRN with a fixed, differentiable ray marcher. In NeRF, all ray samples are transformed into a high dimensional sine-cosine or Fourier space [TSM*20], and fed into an MLP, followed by an accumulation step to generate the final RGB output. The simplicity and impressive results inspired many adaptations to the original NeRF, sometimes being referred to as the NeRF explosion [DY21] Although these NeRF variants show impressive quality, the large number of samples per ray typically makes NeRFs unsuitable for real-time applications. As a result, several recent publications incrementally improve run-time efficiency. To enable empty space skipping,neural sparse voxel fields (NSVFs) [LGZ*20] uses a selfpruning sparse voxel octree structure, where each ray sample includes information from a tri-linearly interpolated embedding of voxel vertices. Alternatively, to reduce the number of evaluations along a ray, partial integrals can be learned [LMW21]. Decomposed radiance fields (DeRFs) [RJY*20] decompose the scene with a Voronoi decomposition to train multiple NeRFs for each cell.\n\n\nImplicit neural scene representations\n\nBaking of Neural Radiance Fields Recent research has focused on baking components of NeRF to achieve performance gains at the cost of extensive memory consumption [YLT*21; HSM*21; RPLG21; GKJ*21]. While baking could also be applied to our work, it departs from the beauty of a compact neural representation, potentially requiring hundreds of MBs up to GBs for a scene that can be represented by 4 MB in NeRF or our approach.\n\n\nRay Unification\n\n\nNon-Linear Depth\n\nSampling Oracle Network Space Warping Local Shading Network Figure 2: To enable efficient rendering of large-scale neural representations, DONeRF uses a five stage pipeline: (1) ray descriptions are unified within a view cell, (2) depth is considered in a non-linear space, (3) an oracle network estimates the importance of samples, (4) sample positions are warped towards the view cell, and (5) radiance is generated from the shading MLP with only a few samples along each ray.\n\nIn our work, we increase the inference speed of NeRF-like representations while staying in the realm of compact MLPs without additional data structures or increased storage requirements. At the memory requirement of two MLPs, we show the most significant performance improvements compared to NeRF [MST*20]. Additionally, we increase image quality and support large-scale scenes, where the original NeRF, image-based methods, and methods that require additional data structures, like NSVF [LGZ*20], struggle.\n\n\nEfficient Neural Rendering using Depth Oracle Networks\n\nTo achieve real-time rendering of compact neural representations for generated content, we introduce DONeRF. DONeRF replaces the MLP-based raymarching scheme of NeRF [MST*20] with a compact local sampling strategy to only consider important samples around surfaces. DONeRF consists of two networks in a five-stage pipeline ( Figure 2): A sampling oracle network predicts optimal sample locations along the view ray using classification and a shading network uses NeRF-like raymarching accumulation to deliver RGBA output. To remove input ambiguity, we transform rays to a unified space and use non-linear sampling to focus on close regions. Between the two networks, we warp the local samples to direct high frequency predictions of the shading network to the foreground.\n\nView Cells For training, we use RGBD input images sampled from a view cell. A view cell is defined as a bounding box with a primary orientation and maximum viewing angle, i.e., it captures all view rays that originate in the bounding box and stay within a certain rotation, see Figure 3. In a streaming setup, trained network weights for partially overlapping view cells can be swapped or interpolated to enable seamless transitions between larger sets of potential views. We define the view cell specifics to provide a clear way of splitting large scenes and defining the potential input to our approach. As smaller view cells reduce the visible content of a scene, smaller view cells may lead to higher quality (with the extreme being a single view). However, large view cells can work similarly well while being even more memory efficient-depending on the network capacity used to represent them. Note that this is true for any scene representation, and comes at a cost of larger memory requirements.\n\n\nEfficient Neural Sampling\n\nInference performance of NeRF-like neural representations scales most significantly with the number of samples per ray. While there are methods that deploy multiple lower-capacity networks for a Figure 3: Top view of a view cell, defined by a bounding box, a forward direction (arrow) and a maximum viewing angle (blue with limiting arrows). Valid rays originate within the view cell and stay within the angle bounds (see example camera orientations).\n\nmoderate quality-speed tradeoff [RJY*21], the target output quality is fundamentally limited by the given network capacity. Thus, we consider network capacity optimizations orthogonal to our work. Instead, we consider different sample placement strategies to reduce the amount of samples for neural raymarching.\n\nUniform Sampling The default way of NeRF-style raymarching is to place samples uniformly between the near and far plane:\nx(d i ) = o + d i \u00b7 r\n(1)\nd i = d min + i \u00b7 (dmax \u2212 d min ) N , i = [0, 1, 2, \u00b7 \u00b7 \u00b7 , N],\nwhere o is the ray origin, r is the ray direction, N is the number of placed samples, and d min and dmax are the near and far plane distances from the camera pose. Sample locations are transformed to a view cell local coordinate system, divided by dmax, and positionally encoded to construct the feature vector f, where c is the view cell center:\nf(d i ) = encode x(d i ) \u2212 c dmax .(2)\nNon-linear Sampling While uniform sampling works well for individual objects and small scenes, large depth ranges are problematic.\n\nFocusing samples on objects closer to the camera is an intuitive first step to reduce network evaluations without losing quality. For non-linear sample placement, we use a logarithmic non-linearity:\nd i = d min + log(d i \u2212 d min + 1) log(dmax \u2212 d min + 1) \u00b7 (dmax \u2212 d min ).(3)\nNDC Sampling NeRF [MST*20] suggests to transform rays into an average camera frame, and to uniformly sample within the projected normalized device coordinates (NDC), directly feeding NDC samples into positional encoding. This approach is only applicable if all samples lie strictly in the front hemisphere of the average camera frame, which is a limitation not shared by the other sampling strategies. Large deviations from the average camera frame lead to increasingly high non-linear perspective distortions, which may impact the learning process. We refer the reader to the appendix in the original NeRF paper for the detailed derivations; we use the same transformation with d min = 1 and dmax = \u221e.\n\nSpace Warping Although non-linear sampling focuses the samples on the foreground, positional encoding is applied equally for foreand background. Early training for large scenes showed that the background often contains high frequencies that must be dampened by the network while the foreground requires those to achieve sufficient detail. This is not surprising, as real cameras and graphics techniques such as mip-mapping average background details.\n\nTo remedy this issue, we propose a warping of the 3D space towards the view cell center. We warp the entire space using a radial distortion, bringing the background closer to the view cell for positional encoding. Initial experiments showed that using an inverse square root transform works well:\nf = encode (x(d i ) \u2212 c) \u00b7W (x(d i ) \u2212 c) (4) W (p) = 1 |p| \u00b7 dmax .(5)\nWhile ray samples do not follow a straight line after warping, sample locations in space stay consistent, i.e., samples from different view points landing at the same 3D location are evaluated equally. See Figure 4 for a visualization of the uniform, logarithmic and log+warp placement strategies. The NDC sampling strategy is linear along each ray in NDC space, but follows a linear sampling in disparity from the near plane to infinity in the original space, with a more aggressive 1 x sampling curve compared to logarithmic sampling. From the perspective of the network input, it therefore combines the uniform and logarithmic sampling approaches.\n\nLocal Sampling Even when focusing samples on the foreground, NeRFs with large sample counts spend many samples in empty space. Given a ground truth surface representation, e.g., a depth texture, it is possible to take a fraction of the samples of a trained NeRF around the surface and still achieve mostly equal quality. This inspires the following question: Given a ground truth depth texture to place samples during training, what is the best quality-speed tradeoff that can possibly be reached?\n\nAblation Study To determine the effectiveness of the different sampling strategies, we run an ablation study based on the original NeRF. For all experiments, we assume static geometry and lighting and test on a single view cell. We vary the number of samples per ray N between [2,4,8,16,32,64,128] and only use a single MLP (without the refinement network proposed in the original NeRF work which dynamically influences sample placement). To investigate local sampling, we perform uniform, logarithmic, log+warp and NDC sampling around the ground truth depth, keeping the step size identical to N = 128 for all sample counts. We conduct our experiments on four diverse scenes, Bulldozer, Forest, Classroom and San Miguel. For more details on the evaluation setup and the evaluated test scenes, please refer to Section 6 and Appendix E.\n(a) uniform (b) logarithmic (c) log+warp\nAveraged results are shown in Figure 5, while per-scene details can be found in Appendix A. First, for non-local sampling, the results show that logarithmic sampling increases quality over uniform while log+warp increases quality further. With log+warp the number of samples can be halved to still achieve equal quality to uniform. Second, with local sampling, the number of samples can be reduced to two with nearly no decrease in quality. On average, log+warp adds about 1.3 dB in quality over uniform for local sampling. Sampling in NDC only reaches competitive results at more than 64 samples, falling behind uniform sampling in our evaluation. Since the network inputs in NDC are still uniform, the underlying nonlinear transformation between different rays must be reconstructed by the network, which seems difficult at lower sample counts.\n\nOur results indicate that-given an ideal sampling oraclesignificant gains in quality-speed tradeoffs can be achieved by placing samples locally. However, in practice, ground truth depth is often not available during inference for neural rendering due to memory or computational constraints: large-scale scenes require a significant amount of storage to represent via geometry or individual depth maps, and reprojection would be necessary to generalize to novel views. Thus local sampling from ground truth depth during inference can be considered a niche scenario, and we therefore target an efficient and compact representation via an MLP-based sampling oracle that only uses ground truth depth during training.\n\n\nSampling Oracle Network\n\nAs mentioned before, sampling around a known ground truth surface representation can reduce the number of required samples by up to 64\u00d7. However, relying on explicit surface representations would defeat the purpose of having a compact neural representation. Therefore, we introduce an oracle network to predict ideal sample locations for the raymarched shading network. This oracle network takes a ray as input and provides information about sample locations along that ray. We found that using an MLP of the same size as the shading network generates sufficiently consistent depth estimates for the majority of rays in simple scenes. However, accurate estimates around depth discontinuities remain difficult to predict, leading to significant visual artifacts ( Figure 6).\n\nTo mitigate this issue, we start with the following observation: In general, the exact geometric structure of the scene is faithfully reconstructed using neural raymarching, where samples are also placed in empty space. Thus, we can allow the oracle more freedom. While it must predict sample locations around the surface, it may provide additional estimates. Consider neighboring rays around a depth discontinuity, either hitting the foreground or the background: Representing this discontinuity accurately in ray space is difficult, as small changes in ray origin or direction may alternate between fore-and background. However, if the oracle is allowed to return the same result for all rays around discontinuities, i.e., sampling at the fore-and the background, the oracle's task is easier to learn.\n\n\nClassified Depth\n\nInterestingly, a simultaneous prediction of multiple real-valued depth outputs did not improve results compared to a single depth estimate per pixel in our experiments. Alternatively, the oracle can output sample likelihoods along the ray-i.e., a likelihood that a sample at a certain location will increase image quality. Unlike NeRF's refinement network, we only want to evaluate the oracle network once and reduce the sample count of the shading network.\n\nTo this end, we propose that the oracle is trained via classification, where each class corresponds to a discretized segment along the ray. For every discrete ray segment, a high value indicates that it should receive (multiple) samples; a low value indicates that the segment can be skipped. The surfaces are represented accurately, i.e.,\nCx,y(z) = 1, if dz \u2264 ds < d z+1 0, otherwise,(6)\nwhere C is the classification value, ds corresponds to the depth value of the first surface along the ray and dz and d z+1 describe the discretization boundaries for the ray segment z. This leads to a one-hot encoding as a target that can be trained using the common binary cross-entropy (BCE) loss.\n\nTo further aid the depth oracle in predicting consistent outputs at depth discontinuities, we provide a multi-class target that is filtered in image-space and along depth. We blur depth values from neighboring pixels in the ground truth target. To generate this filtered target, we use a radial (Euclidean distance) filter to include values\n(a) Bulldozer K = 1 (b) Bulldozer K = 5 (c) Bulldozer K = 9 (d) Classroom K = 1 (e) Classroom K = 5 (f) Classroom K = 9\nFigure 8: Visualization of the classified depth target for 16 classes along each ray and the largest classes mapped to G, R, B, i.e., a green value indicates a single class with the brightness encoding depth; a gray value means that all classes are at similar depth; and different colors in proximity indicate that classes vary. Small features are smoothed to neighboring pixels with increasing filter sizes (1-5-9).\n\nof neighboring rays with a lower contribution:\nCx,y(z) = max i, j \u2208\u00b1 K/2 C x+i,y+ j (z) \u2212 i 2 + j 2 \u221a 2 \u00b7 K/2(7)\nwhere K is the filter size. For example, using a 5 \u00d7 5 filter, rays at a distance of 3, 2, 1, 0 pixels contribute 0, \u2248 0.30, \u2248 0.65, 1 to the output, respectively. For multiple depth values with the same discretization result, we only take the maximum result (contribution from the closest neighboring ray) to ensure a valid classification target as well as that the largest value is placed at the actual surface.\n\nFollowing the idea of label smoothing, we also filter along the depth axis, using a simple 1D triangle filter with kernel size Z:\nC(z) = min Z/2 \u2211 i=\u2212 Z/2 \u0108 (z + i) Z/2 + 1 \u2212 |i| Z/2 + 1 , 1 .(8)\nFrom a classification point of view, filtering decreases false negatives (at the cost of false positives) and thus ensures that important regions are not missed. This becomes apparent when considering that rays exist in a continuous space, i.e., the oracle does not need to learn hard boundaries at depth discontinuities. Translating a higher false positive rate to raymarching, we increase the likelihood for sampling regions that do not need any samples. Thus, overall, we need to place more samples to hit the right sample locations, while at the same time reducing the chance to miss surfaces.\n\nGiven that the oracle network serves a similar purpose as the coarse network in NeRF with the same capacity, while being evaluated only once instead of 64 times, allowing the network to move towards false positive classifications is essential. Additionally, false positives are handled by the local raymarching network, as empty space will still result in no image contribution, even at low sample counts. A missed surface on the other hand would clearly reduce image quality. Figure 7 shows an example slice for a fully filtered target and Figure 8 shows visualizations for different filter sizes. Note that the filtering only applies to the training targets for the depth oracle-no filtering is done during inference.\n\nFinally, to translate the classification outputs to sample locations, we use the same approach as NeRF [MST*20] when going from the coarse to the fine network. Compared to NeRF, which builds a piecewise-constant probability density function (PDF) from the opacity outputs of the coarse shading network evaluated at multiple sample locations, we interpret our depth oracle output vector distance along the ray 1 1/8 2/8 3/8 4/8 5/8 6/8 7/8 pdf cdf uniform samples Figure 9: To place samples along the ray, we treat the classified depth output from the oracle as a piecewise-constant PDF (blue) and translate it into a CDF (green). Sampling the CDF at uniform steps (orange) concentrates samples (red \u00d7) around regions with a high classification value.\n\n(which comes from a single oracle network evaluation) directly as a piecewise-constant PDF and similarly sample along the inverse transform of the corresponding cumulative distribution function (CDF); see Figure 9.\n\n\nRay Unification and Oracle Input\n\nTo take the burden of disambiguating rays originating from different starting locations from the oracle, we map every ray starting location onto a sphere circumscribed around the view cell, see Figure 10. This unification works well for arbitrary views looking outside the view cell. For 360 \u2022 object captures, such as in the original NeRF work, our ray unification scheme be applied in an inverse manner, where the circumscribed sphere is placed around the object instead.\n\nTo ease the task of the oracle network further, we supply 3D positions along the ray as input. We place those at the centers of the discretized depth ranges and provide them as additional inputs to the first network layer. We do not use positional or Fourier encoding for the depth oracle inputs, as this did not improve results in our experiments. To work in unison with the shading network, for which we place samples logarithmically (Section 4), we also perform the same logarithmic transformation on the classification steps. \n\n\nOptimal Filter Sizes and Depth Oracle Inputs\n\nWe conduct an ablation study to evaluate the impact of the filter sizes for our neighborhood filter K (Equ. 7), depth smoothing filter Z (Equ. 8) and the number of 3D input positions (I = [1, 128]). We use the classified depth oracle network as the input for a locally raymarched shading network, similar to the experiment in Section 4.\n\nWe vary the number of samples per ray N = [2,4,8,16] . Both the depth oracle network and the shading network contain 8 hidden layers with 256 hidden units. We first train the depth oracle network for 300 000 iterations, before training the shading network for 300 000 iterations using the oracle's predictions. To illustrate the importance of our depth classification, we compare against a depth oracle that only predicts a single depth value with (SD unified) and without (SD) ray unification. Our metric is the resulting quality of the RGB output of the shading network, which should improve when given better sample positions by the depth oracle.\n\nThe results in Table 1 show that (1) ray unification adds about 0.6 dB in PSNR, (2) using a classification network adds another 0.6 dB-1.5 dB, (3) providing multiple samples along the ray as input adds 1 dB, (4) the neighborhood filter adds 1.3 dB-1.8 dB, and the depth smoothing filter adds 0.1 dB. These improvements come at no inference cost (filtering) or virtually no inference cost (ray unification, multi input). In total, our additions improve the PSNR by 3.3 dB-4.7 dB. Note that an even larger filter size may reduce overall quality, as many samples are placed in empty space rather than on the surface.\n\n\nEvaluation\n\nTo evaluate DONeRF, we focus on three competing requirements of (neural) scene representations: quality of the generated images, efficiency of the image generation, and compactness of the representation. Clearly tradeoffs between them are possible, but an ideal representation should generate high-quality outputs in real-time, while being compact and extensible, e.g., for streaming dynamic scenes.We compare against NeRF [MST*20], NSVF [LGZ*20], Local Light Field Fusion (LLFF) [MSO*19] and Neural Basis Expansion (NeX) [WPYS21] to evaluate methods that choose different tradeoffs among our three goals. Table 1: PSNR and FLIP results averaged over all scenes, evaluating the neighborhood filter size (K-X), the depth smoothing filter size (Z-X) and the number of inputs for the depth oracle (I-X) over the number of samples per ray N used for raymarching. We also compare against an oracle that only predicts a single depth value (SD), as well as a single depth value with unified input (SD unified). These methods capture a mix between being strictly MLP-based (NeRF), using explicit structures and MLPs (NSVF, NeX) and using a mostly image-based representation (LLFF). For NeX, we include an additional variant that does not bake radiance coefficients and neural basis functions into an MPI, but recomputes those via MLP inference during test time (NeX-MLP). For NSVF, we run a grid search and evaluate on three representative variants (NSVF-small, NSVF-medium and NSVF-large) that capture the lowest memory footprint, best quality-speed tradeoff, and best quality respectively. Furthermore, we include a variant of NeRF that uses our log+warp sampling to show the effect of the sampling strategy in isolation (NeRF (log+warp)). See Appendix E for details about the methods.\n\nWe analyze the ability to extract novel high-quality views for generated content where reference depth maps are available during training. As an additional proof-of-concept, we extract estimated depth maps from a densely sampled NeRF for each scene, and use these depth maps to train our depth oracles, showcasing a solution for scenes without available ground truth depth. We evaluate quality by computing PSNR and FLIP [ANA*20] against ground truth renderings, efficiency as FLOP per pixel and compactness by total storage cost for the representation. For all methods, images are downsampled to a resolution of 400 \u00d7 400 to speed up training.\n\n\nTraining and Real-time Implementation\n\nWe use K-5 Z-5 I-128 for our depth oracle network (see Section 5.3) and use various numbers of samples per ray, named DONeRF-X, where X denotes the number of samples per ray. We transform each sample logarithmically and warp it towards the view cell, as described in Section 4. We train each network for 300 000 iterations and use the checkpoint with the lowest validation loss for testing. We use Adam [KB15] with a learning rate of 0.0005 and a batch size of 4096 samples per iteration.\n\nFor the RGB output of the shading network we use standard MSE loss, while the depth classification uses BCE loss. Furthermore, during initial experiments, we observed that shading networks with low sample counts tend to \"cheat\" in their outputs, relying on the opacity outputs to mix the background color into the accumulation, Figure 11: Even on our challenging dataset, DONeRF achieves higher quality on average than all other methods at only 4 samples per ray. NeRF manages to faithfully reconstruct smaller scenes such as Bulldozer and Barbershop, but struggles with large-scale scenes such as Forest or highly complex geometry such as in Classroom. NSVF shows worse quality on average compared to DONeRF-4 at increased memory requirements and much worse performance. Although LLFF requires significantly more memory, it still struggles to represent fine details accurately, even at a cropped field of view. NeX achieves good quality for San Miguel, but larger rotations and offsets from its reference pose cause significant artifacts due to its explicit MPI representation. Table 2: Across all our test scenes, DONeRF shows a significant improvement in quality-speed tradeoff, beating NeRF in most scenes with only 2 \u2212 4 samples per ray, resulting in 48\u00d7 to 78\u00d7 fewer FLOP per pixel at equal or better quality. Where NSVF struggles to achieve a consistent quality or performance for our tested scenes, especially for larger scenes such as Forest, DONeRF performs consistently well across all scales. LLFF is the cheapest to compute, but fails to replicate the quality of the other methods, in addition to requiring storage that scales unfavorably with the amount of training images. Although NeX is also very efficient at rendering, it suffers from artifacts related to the MPI representation when poses differ too much from the reference pose, and requires roughly 24\u00d7 the amount of storage to achieve worse quality than DONeRF across the board. NeX-MLP is able to remedy some of the artifacts at a cost of 9\u00d7 worse performance compared to DONeRF-4. Finally, using depth maps extracted from a dense NeRF without depending on available ground truth depth, DONeRF-noGT still achieves the best tradeoff between performance, quality and storage overall. resulting in a darkening with black background color. At low sample counts, this tends to hurt generalization, especially when rays should accumulate to an opacity of at least 1 for opaque surfaces. As a result, dark pixel artifacts are visible in some test set views, which we remedy by adding an additional opacity loss term that forces the accumulated ray opacity \u03b4 to be at least 1:\nGround Truth (a) Ground Truth (b) DONeRF-4 (c) NeRF (d) NSVF-medium (e) LLFF (f) NeXloss O = \uf8f1 \uf8f2 \uf8f3 0, if \u2211 X i=1 \u03b4 i \u2265 1 \u2211 X i=1 \u03b4 i \u2212 1 2 , otherwise,(9)\nwhere X is again the number of samples per ray. Our final loss function is a weighted sum of both the MSE and the opacity loss\nloss = \u03b1 \u00b7 loss MSE + \u03b2 \u00b7 loss O .(10)\nFor all DONeRF experiments, we use \u03b1 = 1.0 and \u03b2 = 10.0 to conservatively remove all dark pixel artifacts. Lower values for \u03b2 could be selectively applied to further increase quality in certain scenes. The network architecture of DONeRF is visualized in Appendix D. For DONeRF without ground truth depth (DONeRF-X-noGT), we train a dense NeRF with 12 layers of 512 hidden units each using 128 samples per ray, and extract depth maps for all poses.\n\nUsing these depth maps, we train DONeRF in exactly the same way as when using the reference depth maps.\n\nFor our prototype real-time implementation of DONeRF, we use a combination of TensorRT and CUDA. Ray unification, space warping, feature generation and raymarching run in custom CUDA kernels with one thread per ray. Both networks are evaluated using TensorRT in half floating point precision. All parts still offer significant optimization potential, by, e.g., using multiple threads during input feature generation or reducing the bit length for the network evaluation further.\n\nNevertheless, we already achieve real-time rendering at medium resolutions on an NVIDIA RTX 3090. Ray unification and first feature generation (0.21 ms), the oracle network (12.64 ms), space warping and the second feature generation (3.46 ms), the shading network (34.9 ms), and final color generation (0.09 ms) take a total of 51.3 ms for a 800 \u00d7 800 image and 2 samples per ray, i.e., about 20 frames per second. Note that the shading network still is the major cost in our setup and that computation times increase nearly linearly with the number of samples: 34.9 ms, 65.4 ms, 136.4 ms and 270.4 ms for 2, 4, 8 and 16 samples, respectively.\n\n\nDataset\n\nWe collect a set of scenes that showcase both fine, high-frequency details as well as large depth ranges to validate that DONeRF is applicable to a wide variety of scenes. All datasets are rendered using Blender, using their Cycles path tracer to render 300 images for each scene (taking approximately 20 minutes per image on a 64 core CPU), which we split into train/validation/test sets at a 70%/10%/20% ratio. For each scene, we define a view cell that showcases a wide range of movement and rotation to reveal disocclusions and challenging geometry, while not intersecting geometry within the scene. Poses are randomly sampled within the view cell, limiting the rotation to 30 degrees in pitch and 20 degrees in yaw relative to the initial camera direction. We limit the rotation to be comparable to plane-based representations such as NeX and LLFF-DONeRF would be able to handle unrestricted rotation angles. Please refer to Appendix C for more details about the scenes.\n\n\nComparisons\n\nThe results of our main evaluation are summarized in Table 2 and qualitative example views are shown in Figure 11. In the following we individually compare the competing approaches to DONeRF in terms of quality, efficiency, and compactness.\n\nNeRF Compared to NeRF, the advantages of both our sampling strategy and oracle network are immediately visible. On average, DONeRF achieves equal or better quality with only 2 \u2212 4 samples per ray compared to NeRF's 256. At 16 samples, DONeRF achieves up to 7 dB higher PSNR, and outperforms NeRF on all scenes except for Barbershop. At the same time, DONeRF is 15-78\u00d7 faster to evaluate than NeRF. As both only use two MLPs, they are among the most compact methods-the small increase in memory of DONeRF is due to the increased number of inputs and outputs of our oracle network compared to the coarse NeRF network. Even when applying our improved sampling strategy for NeRF (log+warp), DONeRF achieves equal quality with just 8 samples. Overall, DONeRF is superior compared to NeRF -especially for large open scenesboth when considering quality, efficiency and compactness as a whole, and when considering them individually.\n\nNSVF The comparison to NSVF is interesting, as NSVF is able to adjust the tradeoff between quality, efficiency and compactness by changing the voxel size of its representation. Even though NSVFsmall is 2.5 dB lower in quality compared to DONeRF-2, it takes 27\u00d7 longer to evaluate. NSVF-medium and NSVF-large are similar in quality to DONeRF-2 and DONeRF-4 respectively, but take 43\u00d7 times longer to evaluate. As NSVF only uses a single MLP, using fewer voxels can be advantageous if a very low memory footprint is required at the cost of quality, and NSVF-small is the smallest scene representation in our comparison. However, to achieve competitive quality, NSVF requires more than 2\u00d7 the amount of memory of DONeRF. Additionally, NSVF takes about 6\u00d7 longer to train than DONeRF and also required ground truth depth for initialization for our challenging scenes. Thus, for every configuration, a DONeRF exists that provides identical or better quality at significantly better compactness that can be evaluated much more efficiently. LLFF Compared to purely image-based methods like LLFF, the advantages of neural scene representations become apparent. Even when using the entire training set as the basis for image-based rendering (and thus requiring more than 1000\u00d7 the memory of DONeRF) and cropping the image to remove border artifacts, LLFF achieves the lowest quality among all tested methods. Only for the highly challenging views of Forest and San Miguel, PSNR values are close to NeRF but still 1 dB-7 dB from DONeRF-2. However, due to its simplicity of only selecting few, small light fields for each pose at test time and blending between them, LLFF is the fastest method for novel view generation. Thus, if memory is no concern, LLFF may be the preferred option for low-power rendering.\n\nNeX NeX shows similar artifacts to LLFF, in that it suffers from its explicit MPI representation when generating views that differ too much from its reference view. Although it is very fast to evaluate (45\u00d7 faster than DONeRF) and achieves competitive quality for San Miguel, overall it requires 25\u00d7 more memory compared to DONeRF, and its quality is 3 dB lower than DONeRF-2 on average. Looking at the improved quality of NeX-MLP, we can confirm that transferring its neural scene representation into an MPI comes at a significant loss in quality. Compared to NeX-MLP, DONeRF achieves better quality using only 4\u22128 samples for most scenes, at a 10\u00d7 speedup and 25\u00d7 more compactness. For simple scenes, limited fields of view, and if compactness is no concern, precomputing an MPI from a neural scene representation enables high-resolution real-time performance. However, when targeting higher quality or streaming, a complete neural scene representation, like DONeRF, is the better choice. Finally, NeRF-like representations have also been shown to support dynamic scenes and relighting, which may tilt the scales further towards an approach like DONeRF, where the memory consumption of MPI videos becomes even more prohibitive. \n\n\nDONeRF without Ground Truth Depth\n\nOur proof-of-concept DONeRF that is trained without available ground truth depth (DONeRF-X-noGT) loses about 1 dB on average compared to DONeRF (see Table 2). The losses are largest for Bulldozer; for Pavillon and Barbershop DONeRF-X-noGT actually outperforms DONeRF for higher sample counts. For Bulldozer, NeRF outputs high frequency depth values across the entire background, which impedes the task of the depth oracle, only to create samples with zero contribution during shading-adding background detection to remove those samples during oracle training would likely resolve this issue. Looking at San Miguel (Figure 12), DONeRF is capable of reconstructing fine details for foliage. DONeRF-X-noGT interestingly produces sharper results than NeRF although we use a single NeRF depth output as ground truth depth. We attribute DONeRF's ability to recover such high quality to filtering the depth targets and being able to place samples all around the foliage.\n\nEven more surprising is the fact that DONeRF-X-noGT can outperform DONeRF, as in Pavillon and Barbershop (see Figure 13). Transparent surfaces and fully reflective mirrors pose an issue for DONeRF, as the low sample count combined with only a single depth estimate is not sufficient for the network to perfectly replicate these challenging view-dependent phenomena. However, for these scenes, NeRF essentially learns to place samples at multiple surfaces (for the refractive water) and in a virtual mirrored room (for the reflective mirror), and thus DONeRF-X-noGT places samples better than DONeRF for these parts and can reach higher quality for increased sample counts.\n\nOverall, these results show that even without ground truth depth to train its depth oracle network, DONeRF is able to achieve better quality compared to NeRF at much lower sample counts, and provides the best tradeoff in terms of quality, performance and storage requirements. Thus, a perfectly accurate depth estimate is not necessary to benefit from using DONeRF.\n\n\nConclusion, Limitations and Future Work\n\nStarting from an evaluation of the sampling strategies of NeRF, we showed that sampling non-linearly and warped towards a view cell is beneficial in a variety of scenes. Using ground truth depth for optimal sample placement, we showed that local sampling achieves equal quality with as few as 2 samples per ray compared to a fully sampled network with 128 samples. From this insight, we proposed our classified depth oracle network which discretizes the space along each ray, and spatially filters the target across x, y and depth to further improve the sample placement for challenging geometric scenarios. Using our oracle network to guide sample placement for a raymarched shading network, our compact DONeRF approach achieves equal or better quality compared to NeRF with 256 samples across most scenes, while using as few as 2 samples per ray at the memory requirement of only two MLPs. Compared to other scene representations and light field methods, DONeRF compares favorably in terms of storage requirements by a large margin and outperforms all other methods in quality. Only image-based methods can be rendered faster than DONeRF. Nevertheless, DONeRF makes a big step towards rendering directly from neural scene representations with all their advantages in real-time.\n\nPartially transparent objects and mirror-like surfaces can pose an issue when using standard depth maps for training, as the first surface depth value does not represent the necessary sample locations along the ray. Fortunately, due to being a classification network, it would be straightforward to extend the training target of the depth oracle network with multiple ground truth surface points. The proof-ofconcept DONeRF that is trained by using depth maps extracted from a NeRF already shows promising results for these surfaces. However, both, using multiple depth values for ground truth depth training, and eliminating the round trip through a full NeRF when there is no ground truth depth, are obvious next steps for DONeRF.\n\nFurthermore, we only focused on static content in our evaluation. For dynamic content, related research has already shown that depth-aware losses can be introduced to achieve more consistency [XHKK20;LNSW21]. Our classified depth sampling strategy could be adapted as a variation of these ideas, allowing for more consistency across dynamic content while staying within a compact neural representation. Another partially orthogonal approach to ours is caching and baking NeRFs to further increase evaluation speeds. Integrating an oracle, especially in combination with dynamic caching, may allow for further increases in rendering efficiency without compromising compactness for streaming.\n\nTo our knowledge, DONeRF is the first reliable method to render from a raymarched neural scene representation at interactive frame rates without exhaustive caching, and opens the door for compact high-quality dynamic rendering in real-time. We are confident that such a local sampling strategy will be essential for real-time neural rendering going forward.\n\n\nNeRF [MST*20]\n\nWe use an open source PyTorch port of NeRF (https://github.com/yenchenlin/nerf-pytorch) in our evaluation setup. For consistency, we use the same network architecture as used for evaluating DONeRF ( Figure 14) and use 64 coarse and 128 additional fine samples (resulting in 64 + 64 + 128 = 256 total network evaluations) as in the original NeRF [MST*20]. We train NeRF at 300 000 iterations total, taking 2048 samples from 2 images per iteration.\n\nNSVF [LGZ*20] For NSVF, we use the authors' open source code (https://github.com/facebookresearch/NSVF) and perform a grid search over the initial voxel size. As NSVF varies greatly in terms of quality and performance based on the initial voxel size, we choose 3 representative variants by choosing the best version within three scenarios: NSVF-large is selected by choosing the best quality, ignoring performance and memory constraints, NSVF-medium is aimed at the best quality-performance tradeoff, and NSVF-small aims at the lowest memory requirements. We train each NSVF for 150 000 iterations using 4096 samples per iteration.\n\n\nLLFF [MSO*19]\n\nFor LLFF, we use the authors' available code (https://github.com/Fyusion/LLFF), and evaluate the quality metrics by first removing a border of 10 % from each side of the image, as suggested by the authors.\n\nNeX [WPYS21] For NeX, we use the authors' open source code (https://github.com/nex-mpi/nex-code/) and their suggested parameters that fit into the memory budget of 11 GB GPU RAM, i.e., 256 hidden units for their main MLP, 6 layers and 12 sublayers for their MPI. This also provides a more competitive comparison in terms of storage requirements. We use the authors' real-time web viewer to render images for the evaluation, and train for 300 000 iterations with 4096 samples per iteration. Furthermore, we include an additional baseline for NeX, where we do not precompute the radiance and neural basis functions into an MPI, and instead query them from the MLPs directly during inference, to show the potential upper limit in quality. Note that this NeX-MLP variant has slightly higher storage requirements than NeX, as it contains the base color MPI in 32-bit floating point format. Table 3: Ablation results for various depth oracle configurations for the Bulldozer scene. Please refer to Section 5 of the main paper for a detailed explanation on these depth oracle configurations.   Table 9: Ablation results for various sampling methods for the Classroom scene. Please refer to Section 4 of the main paper for a detailed explanation on these sampling strategies.  \n\nFigure 4 :Figure 5 :\n45Different sampling approaches visualized: (a) uniform samples in equal steps between the near and far planes; (b) logarithmic sampling reduces the sample distance for close samples in favor of spacing out far samples; (c) log+warp pulls the space closer to the view cell center, making the scene appear smaller to the NeRF and bending rays (compare to the gray straight lines). Average PSNR results for various sample reduction schemes over the sample count N. Uniform sampling roughly increases quality by 1.8 dB for each doubling of N. logarithmic increases PSNR by 1.3 dB and inverse square root warping (log+warp) adds another 0.6 dB on top. Sampling in NDC only manages to surpass uniform sampling at 128 samples. When sampling around the ground truth depth (local), the number of samples hardly affects quality. Still, PSNR roughly increases by 0.75 dB with logarithmic and another 0.6 dB with log+warp. Local NDC sampling performs worse than all other local sampling approaches until N = 32.\n\nFigure 6 :\n6(a) A depth oracle network with a single depth output smooths depth around discontinuities. (b) As a result, local sampling mixes fore-and background and distorts features. (c) This becomes apparent when compared to the ground truth.\n\nFigure 7 :\n7A horizontal slice through the Bulldozer dataset. Filtering the depth classification target (black = 1, white = 0) in both image dimensions and along depth smooths the classification target. An oracle producing such an output still results in high quality sample locations (red \u00d7), with additional samples placed in free space. A smooth target is easier to learn as labels vary with lower frequency.\n\nFigure 10 :\n10Ray unification maps ray starting points to a sphere surrounding the view cell. Without ray unification (left) the same ray is encoded differently and requires different depth values; after ray unification (right) identical rays have identical depth values.\n\nFigure 12 :\n12(a) Example ground truth view of the test set of San Miguel with the corresponding inset (b). (c) DONeRF manages to preserve high-frequency detail around the leaves of the tree at only 4 samples per ray. (d) Although NeRF also preserves the tree faithfully, the edges are blurrier. (e) Even when trained with extracted NeRF depth maps (no GT depth), DONeRF produces slightly sharper results than NeRF.\n\nFigure 13 :\n13(a) Example image and ground truth depth from the Barbershop test set with the corresponding inset (b). (c) The reflective mirrors cannot be accurately represented by just sampling around the mirror's depth in DONeRF. (d) NeRF places samples behind the mirror, constructing a virtual reflected room. (e) As depth maps extracted from NeRF include the virtual room, DONeRF-4-noGT learns to represent the mirrors with much fewer samples than NeRF.\n\n\nAlthough explicit neural representations based on voxels [STH*19], MPIs [ZTF*18; FBD*19] or proxy geometry [HPP*18] enable fast novel view generation, they are fundamentally limited by the internal resolution of their representation. To circumvent this issue, implicit neural scene representations [PFS*19; SZW19] directly infer outputs from a continuous input space, such as ray origins and directions. Scene representation networks (SRNs) [SZW19] directly map 3D world coordinates to a feature representation and use a learned raymarcher to accumulate rays for the final RGB output. Similarly, neural volumes [LSS*19] use raymarching to accumulate rays in a learned, warped, volumetric representation. The quality of scene representations can be improved with periodic activation functions [SMB*20].\n\n\n: NeRFs can capture dynamic free-viewpoint video [PSB*20; LNSW21; XHKK20; PCPM21; DZY*20], generate photorealistic avatars [GTZN21; GSL*20; LSS*21], perform relighting on captured scenes [MRS*21; BXS*20; SDZ*21; BBJ*20], conditionally encode shape and appearance via latent codes [SLNG20; CMK*21; YYTK21; TY20] and compose scenes of multiple objects [OMT*21; YLSL21; NG21].\n\n\nTop results in each column are color coded as Top 1 , Top 2 and Top 3 .San Miguel \nPavillon \nClassroom \nBulldozer \nForest \nBarbershop \nAverage \n\nMethod \n\nStorage \n[MiB] \n\nMFLOP \nper pixel PSNR FLIP PSNR FLIP PSNR FLIP PSNR FLIP PSNR FLIP PSNR FLIP PSNR FLIP \n\nDONeRF-2 \n3.6 \n2.70 26.01 .094 30.50 .103 31.66 .061 30.15 .063 29.29 .082 29.41 .074 29.50 .079 \nDONeRF-2-noGT \n3.6 \n2.70 25.33 .098 29.84 .103 30.11 .067 26.92 .077 28.36 .089 29.01 .075 28.26 .085 \nDONeRF-4 \n3.6 \n4.36 27.41 .080 31.07 .098 33.43 .058 33.46 .048 30.63 .077 30.84 .065 31.14 .071 \nDONeRF-4-noGT \n3.6 \n4.36 26.19 .090 30.69 .096 31.44 .061 29.78 .060 29.31 .086 30.42 .067 29.64 .077 \nDONeRF-8 \n3.6 \n7.66 28.65 .071 31.46 .096 35.23 .048 35.88 .039 32.09 .070 31.72 .060 32.50 .064 \nDONeRF-8-noGT \n3.6 \n7.66 26.88 .086 31.56 .091 33.19 .055 32.96 .047 29.98 .084 31.73 .062 31.05 .071 \nDONeRF-16 \n3.6 \n14.29 29.67 .065 31.79 .094 36.27 .045 36.98 .036 31.32 .074 32.15 .059 33.03 .062 \nDONeRF-16-noGT \n3.6 \n14.29 27.70 .078 32.22 .088 34.63 .049 35.41 .040 30.74 .079 32.80 .057 32.25 .065 \n\nNeRF \n3.2 211.42 25.19 .117 29.54 .115 34.02 .056 36.83 .038 23.90 .151 33.63 .052 30.52 .088 \nNeRF (log + warp) \n3.2 211.42 28.98 .074 32.88 .089 35.19 .051 36.22 .040 28.97 .101 33.60 .055 32.64 .068 \n\nNSVF-small \n2.3 \n74.66 24.00 .132 29.42 .110 31.00 .070 25.75 .167 23.79 .159 27.72 .094 26.95 .122 \nNSVF-medium \n4.6 132.03 25.07 .110 29.81 .105 33.04 .055 26.51 .163 25.08 .135 29.62 .077 28.19 .108 \nNSVF-large \n8.3 187.52 25.73 .097 30.48 .099 34.06 .051 33.14 .042 26.05 .119 30.61 .061 30.01 .078 \n\nLLFF \n4130.6 \n.03 24.53 .106 27.50 .123 24.87 .114 24.76 .114 22.19 .148 24.13 .129 24.66 .122 \n\nNeX \n88.8 \n.06 28.07 .094 26.28 .174 30.34 .085 29.20 .072 20.95 .220 22.98 .152 26.30 .133 \nNeX-MLP \n89.0 \n42.71 30.68 .060 30.41 .102 34.10 .046 34.03 .046 24.65 .125 29.45 .075 30.55 .076 \n\n\n\nTable 7 :\n7Ablation results for various sampling methods for the Bulldozer scene. Please refer to Section 4 of the main paper for a detailed explanation on these sampling strategies. 34.362 0.047 34.261 0.047 34.651 0.045 29.265 0.075 30.169 0.052 32.551 0.047 32.640 0.047 29.172 0.071Bulldozer \nuniform \nlogarithmic \nlog+warp \nNDC \n\nuniform \nlocal \n\nlogarithmic \nlocal \n\nlog+warp \nlocal \n\nNDC \nlocal \n\nPSNR FLIP \nPSNR FLIP \nPSNR FLIP \nPSNR FLIP \nPSNR FLIP \nPSNR FLIP \nPSNR FLIP \nPSNR FLIP \n\nNumber of \nsamples N \n\n2 \n27.930 0.062 27.774 0.063 27.755 0.064 25.158 0.099 \n4 \n16.662 0.276 16.562 0.276 16.834 0.275 11.929 0.408 28.071 0.061 28.038 0.060 28.034 0.060 26.147 0.086 \n8 \n19.720 0.204 19.458 0.209 19.637 0.206 14.814 0.314 28.115 0.061 28.093 0.059 28.129 0.059 26.358 0.083 \n16 \n22.790 0.146 22.683 0.147 22.769 0.145 18.384 0.229 28.152 0.061 28.169 0.059 28.192 0.059 26.483 0.084 \n32 \n26.551 0.096 26.233 0.098 26.458 0.095 22.027 0.156 28.208 0.060 28.223 0.058 28.269 0.059 26.891 0.081 \n64 \n30.660 0.064 30.472 0.064 30.817 0.062 25.381 0.106 28.366 0.060 28.939 0.055 28.975 0.056 29.424 0.068 \n128 \n\nTable 8 :\n8Ablation results for various sampling methods for the Forest scene. Please refer to Section 4 of the main paper for a detailed explanation on these sampling strategies. 282 0.239 24.430 0.144 25.363 0.131 21.185 0.221 26.724 0.109 26.762 0.117 29.070 0.091 23.417 0.164 32 20.824 0.229 25.294 0.139 25.995 0.129 23.403 0.166 22.786 0.179 25.492 0.133 27.754 0.104 23.581 0.164 64 22.498 0.178 27.357 0.113 26.951 0.118 25.962 0.122 22.137 0.194 24.996 0.150 26.242 0.129 22.961 0.170 128 23.082 0.167 26.536 0.125 28.809 0.100 29.291 0.097 22.416 0.184 27.067 0.123 29.552 0.097 23.696 0.164Forest \nuniform \nlogarithmic \nlog+warp \nNDC \n\nuniform \nlocal \n\nlogarithmic \nlocal \n\nlog+warp \nlocal \n\nNDC \nlocal \n\nNumber of \nsamples N \n\n2 \n24.126 0.156 29.666 0.079 30.867 0.071 22.108 0.182 \n4 \n14.794 0.525 21.062 0.227 21.185 0.220 15.431 0.474 27.384 0.102 28.513 0.089 30.760 0.075 22.683 0.170 \n8 \n20.172 0.242 22.494 0.171 23.129 0.160 18.651 0.305 27.164 0.104 27.909 0.100 29.799 0.087 22.801 0.169 \n16 \n20.\n\n\n32.252 0.064 32.822 0.060 31.307 0.065 33.660 0.057 34.182 0.055 34.452 0.053 33.586 0.058 128 33.113 0.059 33.781 0.057 34.527 0.053 33.823 0.055 33.574 0.058 33.727 0.057 34.510 0.055 33.674 0.058Classroom \nuniform \nlogarithmic \nlog+warp \nNDC \n\nuniform \nlocal \n\nlogarithmic \nlocal \n\nlog+warp \nlocal \n\nNDC \nlocal \n\nNumber of \nsamples N \n\n2 \n33.389 0.054 33.668 0.054 33.458 0.055 32.157 0.060 \n4 \n21.135 0.194 21.804 0.204 21.497 0.211 18.920 0.275 33.994 0.052 33.964 0.053 33.815 0.054 33.033 0.057 \n8 \n24.357 0.141 25.263 0.130 25.689 0.120 22.690 0.180 33.889 0.053 33.949 0.054 33.948 0.054 33.259 0.056 \n16 \n26.519 0.111 27.313 0.105 27.838 0.094 25.689 0.123 33.346 0.055 33.688 0.054 33.952 0.053 33.325 0.056 \n32 \n28.882 0.085 29.678 0.082 30.435 0.072 28.342 0.088 33.495 0.056 33.786 0.055 33.883 0.054 33.587 0.057 \n64 \n31.322 0.068 \n\nTable 10 :\n10Ablation results for various sampling methods for the San Miguel scene. Please refer to Section 4 of the main paper for a detailed explanation on these sampling strategies. 27.227 0.092 27.391 0.089 28.825 0.075 29.453 0.069 28.498 0.079 28.132 0.079 28.869 0.075 29.582 0.069San Miguel \nuniform \nlogarithmic \nlog+warp \nNDC \n\nuniform \nlocal \n\nlogarithmic \nlocal \n\nlog+warp \nlocal \n\nNDC \nlocal \n\nNumber of \nsamples N \n\n2 \n28.689 0.072 28.658 0.071 28.919 0.068 28.286 0.073 \n4 \n20.763 0.213 21.442 0.197 21.662 0.190 17.843 0.291 28.824 0.071 28.705 0.071 28.861 0.070 28.627 0.072 \n8 \n22.607 0.171 23.610 0.146 24.031 0.137 21.549 0.190 28.810 0.072 28.632 0.072 28.879 0.072 28.913 0.071 \n16 \n23.634 0.149 24.588 0.128 25.250 0.114 23.711 0.138 28.675 0.075 28.595 0.074 28.935 0.072 29.096 0.071 \n32 \n24.795 0.126 25.369 0.114 26.429 0.098 25.702 0.103 28.654 0.076 28.533 0.077 29.027 0.072 29.330 0.071 \n64 \n26.137 0.105 26.283 0.102 27.607 0.086 27.783 0.080 28.476 0.079 28.460 0.079 28.976 0.074 29.566 0.070 \n128 \n\u00a9 2021 The Author(s) Computer Graphics Forum \u00a9 2021 The Eurographics Association and John Wiley & Sons Ltd.\nAppendix A: Additional Results: Efficient Neural Sampling Tables 7, 8, 9 and 10 show per-scene results for varying numbers of samples per ray, using the sampling methods described in Section 4.Figure 14: For evaluation, we use the same MLP architecture for both NeRF and DONeRF: 8 layers with 256 hidden units each, with a single skip connection to forward the encoded directions to the last layer. For the depth oracle network, we do not use a skip connection. The inputs and outputs are described in Section 5.1.shows a vast field of cel-shaded high-frequency foliage and trees, with a person in the foreground, which enforces a good foreground and background representation.Appendix E: Additional Evaluation Setup: BaselinesFor each baseline method, we converted our datasets to use their required format, i.e., we provide ground truth poses and undistorted ground truth images for each method. We count the storage by summing all required network weights, checkpoints or images that are necessary to render novel views. FLOP per pixel are counted either by profiling with NVIDIA Nsight Compute or directly from the neural network evaluation.\nFLIP: A Difference Evaluator for Alternating Images. ] Ana*20, Andersson, Pontus, Jim Nilsson, Tomas Akenine-M\u00f6ller, Proc. ACM Comput. Graph. Interact. Tech. 327ANA*20] ANDERSSON, PONTUS, NILSSON, JIM, AKENINE-M\u00d6LLER, TOMAS, et al. \"FLIP: A Difference Evaluator for Alternating Images\". Proc. ACM Comput. Graph. Interact. Tech. 3.2 (Aug. 2020) 7.\n\nNeRD: Neural Reflectance Decomposition from Image Collections. Mark Boss, Raphael Braun, Jampani Varun, arXiv:2012.03918[cs.CV]2BBJ*20[BBJ*20] BOSS, MARK, BRAUN, RAPHAEL, JAMPANI, VARUN, et al. \"NeRD: Neural Reflectance Decomposition from Image Collections\". (2020). arXiv: 2012.03918 [cs.CV] 2.\n\nImmersive Light Field Video with a Layered Mesh Representation. Michael Broxton, John Flynn, Ryan Overbeck, ACM Trans. Graph. 394BFO*20[BFO*20] BROXTON, MICHAEL, FLYNN, JOHN, OVERBECK, RYAN, et al. \"Immersive Light Field Video with a Layered Mesh Representation\". ACM Trans. Graph. 39.4 (July 2020) 2.\n\nX-Fields: Implicit Neural View-, Lightand Time-Image Interpolation. Mojtaba Bemana, Myszkowski, Karol, Hans-Peter Seidel, Ritschel , Tobias , ACM Trans. Graph. 396BEMANA, MOJTABA, MYSZKOWSKI, KAROL, SEIDEL, HANS- PETER, and RITSCHEL, TOBIAS. \"X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation\". ACM Trans. Graph. 39.6 (Nov. 2020) 2.\n\nNeural Reflectance Fields for Appearance Acquisition. Sai Bi, Xu, Zexiang, Pratul Srinivasan, arXiv:2008.03824[cs.CV]2BXS*20[BXS*20] BI, SAI, XU, ZEXIANG, SRINIVASAN, PRATUL, et al. \"Neural Reflectance Fields for Appearance Acquisition\". (2020). arXiv: 2008. 03824 [cs.CV] 2.\n\npi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis. Eric Chan, Monteiro, Marco, Petr Kellnhofer, *21] CHAN, ERIC, MONTEIRO, MARCO, KELLNHOFER, PETR, et al. \"pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D- Aware Image Synthesis\". CVPR. 2021 2.\n\nYen-Chen Dellaert, Lin , arXiv:2101.05204[cs.CV]2Neural Volume Rendering: NeRF And Beyond. 2021. DELLAERT, FRANK and YEN-CHEN, LIN. Neural Volume Render- ing: NeRF And Beyond. 2021. arXiv: 2101.05204 [cs.CV] 2.\n\nNeural Radiance Flow for 4D View Synthesis and Video Processing. Yilun Du, Yinan Zhang, Y U , Hong-Xing , arXiv:2012.097902arXiv preprintDZY*20[DZY*20] DU, YILUN, ZHANG, YINAN, YU, HONG-XING, et al. \"Neural Radiance Flow for 4D View Synthesis and Video Processing\". arXiv preprint arXiv:2012.09790 (2020) 2.\n\nDeepView: View Synthesis With Learned Gradient Descent. J Flynn, M Broxton, P Debevec, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). FBD*19[FBD*19] FLYNN, J., BROXTON, M., DEBEVEC, P., et al. \"DeepView: View Synthesis With Learned Gradient Descent\". 2019 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR). 2019, 2362- 2371 2.\n\nFastNeRF: High-Fidelity Neural Rendering at. Stephan J Garbin, Kowalski, Johnson Marek, Matthew , arXiv:2103.10380[cs.CV]2GKJ*21[GKJ*21] GARBIN, STEPHAN J., KOWALSKI, MAREK, JOHNSON, MATTHEW, et al. \"FastNeRF: High-Fidelity Neural Rendering at 200FPS\". (2021). arXiv: 2103.10380 [cs.CV] 2.\n\nPortrait Neural Radiance Fields from a Single Image. Chen Gao, Yichang Shih, Lai , Wei-Sheng , arXiv:2012.059032arXiv preprintGSL*20[GSL*20] GAO, CHEN, SHIH, YICHANG, LAI, WEI-SHENG, et al. \"Por- trait Neural Radiance Fields from a Single Image\". arXiv preprint arXiv:2012.05903 (2020) 2.\n\nDynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction. Guy Gafni, Thies, Justus, Michael Zollh\u00f6fer, Niessner , Matthias , GAFNI, GUY, THIES, JUSTUS, ZOLLH\u00d6FER, MICHAEL, and NIESSNER, MATTHIAS. \"Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction\". (June 2021), 8649-8658 2.\n\nDeep Blending for Free-Viewpoint Image-Based Rendering. Peter Hedman, Philip Julien, Price, True, ACM Trans. Graph. 376HPP*18[HPP*18] HEDMAN, PETER, PHILIP, JULIEN, PRICE, TRUE, et al. \"Deep Blending for Free-Viewpoint Image-Based Rendering\". ACM Trans. Graph. 37.6 (Dec. 2018) 2.\n\nBaking Neural Radiance Fields for Real-Time View Synthesis. Peter Hedman, Pratul P Srinivasan, Mildenhall, Ben, HSM*21. arXiv (2021) 2[HSM*21] HEDMAN, PETER, SRINIVASAN, PRATUL P., MILDENHALL, BEN, et al. \"Baking Neural Radiance Fields for Real-Time View Synthe- sis\". arXiv (2021) 2.\n\nAdam: A Method for Stochastic Optimization\". ICLR (Poster). Diederik P Kingma, Jimmy Ba, KINGMA, DIEDERIK P. and BA, JIMMY. \"Adam: A Method for Stochastic Optimization\". ICLR (Poster). 2015 7.\n\nNeural sparse voxel fields. Lingjie Liu, Jiatao Gu, Zaw Lin, Kyaw, Advances in Neural Information Processing Systems. 33LGZ*20[LGZ*20] LIU, LINGJIE, GU, JIATAO, ZAW LIN, KYAW, et al. \"Neural sparse voxel fields\". Advances in Neural Information Processing Systems 33 (2020) 2, 3, 7, 14.\n\nAutoInt: Automatic Integration for Fast Neural Volume Rendering. David B Lindell, Julien N P Martel, Wetzstein , Gordon , 2LINDELL, DAVID B., MARTEL, JULIEN N.P., and WETZSTEIN, GORDON. \"AutoInt: Automatic Integration for Fast Neural Volume Ren- dering\". (2021) 2.\n\nNeural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes. Zhengqi Li, Niklaus, Simon, Noah Snavely, Wang , Oliver Lombardi, Stephen, Tomas Simon, Jason Saragih, 11. [LSS*19ACM Trans. Graph. 24Neural Volumes: Learning Dynamic Renderable Volumes from ImagesLI, ZHENGQI, NIKLAUS, SIMON, SNAVELY, NOAH, and WANG, OLIVER. \"Neural Scene Flow Fields for Space-Time View Syn- thesis of Dynamic Scenes\". (June 2021), 6498-6508 2, 11. [LSS*19] LOMBARDI, STEPHEN, SIMON, TOMAS, SARAGIH, JASON, et al. \"Neural Volumes: Learning Dynamic Renderable Volumes from Images\". ACM Trans. Graph. 38.4 (July 2019), 65:1-65:14 2.\n\nMixture of Volumetric Primitives for Efficient Neural Rendering. Stephen Lombardi, Tomas Simon, Gabriel Schwartz, arXiv:2103.01954[cs.GR]2LSS*21[LSS*21] LOMBARDI, STEPHEN, SIMON, TOMAS, SCHWARTZ, GABRIEL, et al. Mixture of Volumetric Primitives for Efficient Neural Rendering. 2021. arXiv: 2103.01954 [cs.GR] 2.\n\nNeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. Ricardo Martin-Brualla, Radwan, Noha, Mehdi S M Sajjadi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)MRS*21[MRS*21] MARTIN-BRUALLA, RICARDO, RADWAN, NOHA, SAJJADI, MEHDI S. M., et al. \"NeRF in the Wild: Neural Radiance Fields for Un- constrained Photo Collections\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021, 7210- 7219 2.\n\nLocal Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, ACM Transactions on Graphics. 14MSO*19[MSO*19] MILDENHALL, BEN, SRINIVASAN, PRATUL P., ORTIZ- CAYON, RODRIGO, et al. \"Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines\". ACM Transactions on Graphics (TOG) (2019) 2, 7, 14.\n\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ] Mst*20, Mildenhall, Ben, Pratul P Srinivasan, Matthew Tancik, ECCV. 14MST*20] MILDENHALL, BEN, SRINIVASAN, PRATUL P., TANCIK, MATTHEW, et al. \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\". ECCV. 2020 1-4, 6, 7, 13, 14.\n\nGIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields. Geiger Niemeyer, Andreas , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)NIEMEYER, MICHAEL and GEIGER, ANDREAS. \"GIRAFFE: Rep- resenting Scenes As Compositional Generative Neural Feature Fields\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR). June 2021, 11453-11464 2.\n\nNeural Scene Graphs for Dynamic Scenes. Julian Ost, Mannan, Fahim, Nils Thuerey, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)OMT*21[OMT*21] OST, JULIAN, MANNAN, FAHIM, THUEREY, NILS, et al. \"Neu- ral Scene Graphs for Dynamic Scenes\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021, 2856-2865 2.\n\nD-NeRF: Neural Radiance Fields for Dynamic Scenes. Albert Pumarola, Enric Corona, Pons-Moll , Ger-Ard Moreno-Noguer, Francesc , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)PUMAROLA, ALBERT, CORONA, ENRIC, PONS-MOLL, GER- ARD, and MORENO-NOGUER, FRANCESC. \"D-NeRF: Neural Radiance Fields for Dynamic Scenes\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021, 10318- 10327 2.\n\nDeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. J J Park, P Florence, J Straub, PFS*19[PFS*19] PARK, J. J., FLORENCE, P., STRAUB, J., et al. \"DeepSDF: Learn- ing Continuous Signed Distance Functions for Shape Representation\".\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019, 165-174 2.\n\nDeformable Neural Radiance Fields. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, arXiv:2011.12948(2020)2arXiv preprintPSB*20[PSB*20] PARK, KEUNHONG, SINHA, UTKARSH, BARRON, JONATHAN T., et al. \"Deformable Neural Radiance Fields\". arXiv preprint arXiv:2011.12948 (2020) 2.\n\nDeRF: Decomposed Radiance Fields. Daniel Rebain, Wei Jiang, Yazdani , Soroosh , arXiv:2011.12490arXiv: 2011.12490 [cs.CV] 2arXiv e-printsRJY*20[RJY*20] REBAIN, DANIEL, JIANG, WEI, YAZDANI, SOROOSH, et al. \"DeRF: Decomposed Radiance Fields\". arXiv e-prints, arXiv:2011.12490 (Nov. 2020), arXiv:2011.12490. arXiv: 2011.12490 [cs.CV] 2.\n\nDeRF: Decomposed Radiance Fields. Daniel Rebain, Wei Jiang, Yazdani , Soroosh , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)3RJY*21[RJY*21] REBAIN, DANIEL, JIANG, WEI, YAZDANI, SOROOSH, et al. \"DeRF: Decomposed Radiance Fields\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021, 14153-14161 2, 3.\n\nKiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. Christian Reiser, Peng, Songyou, Yiyi Liao, Geiger , Andreas , arXiv:2103.13744[cs.CV]2REISER, CHRISTIAN, PENG, SONGYOU, LIAO, YIYI, and GEIGER, ANDREAS. \"KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs\". (2021). arXiv: 2103 . 13744 [cs.CV] 2.\n\nNeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis. Pratul P Srinivasan, Deng, Boyang, Xiuming Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*21] SRINIVASAN, PRATUL P., DENG, BOYANG, ZHANG, XIUMING, et al. \"NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021, 7495-7504 2.\n\nGRAF: Generative Radiance Fields for 3D-Aware Image Synthesis. Katja Schwarz, Yiyi Liao, Michael Niemeyer, Geiger , Andreas , Advances in Neural Information Processing Systems (NeurIPS). SCHWARZ, KATJA, LIAO, YIYI, NIEMEYER, MICHAEL, and GEIGER, ANDREAS. \"GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis\". Advances in Neural Information Processing Systems (NeurIPS). 2020 2.\n\nImplicit Neural Representations with Periodic Activation Functions. Vincent Sitzmann, Julien N P Martel, Alexander W Bergman, Proc. NeurIPS. SMB*20[SMB*20] SITZMANN, VINCENT, MARTEL, JULIEN N.P., BERGMAN, ALEXANDER W., et al. \"Implicit Neural Representations with Periodic Activation Functions\". Proc. NeurIPS. 2020 1, 2.\n\nPushing the Boundaries of View Extrapolation With Multiplane Images. P P Srinivasan, R Tucker, J T Barron, *19] SRINIVASAN, P. P., TUCKER, R., BARRON, J. T., et al. \"Pushing the Boundaries of View Extrapolation With Multiplane Images\". 2019\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019, 175-184 2.\n\nDeepvoxels: Learning persistent 3d feature embeddings. Vincent Sitzmann, Thies, Heide Justus, Felix , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSTH*19[STH*19] SITZMANN, VINCENT, THIES, JUSTUS, HEIDE, FELIX, et al. \"Deepvoxels: Learning persistent 3d feature embeddings\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019, 2437-2446 2.\n\nScene representation networks: Continuous 3d-structure-aware neural scene representations. Vincent Sitzmann, Michael Zollh\u00f6fer, Wet-Zstein , Gordon , Advances in Neural Information Processing Systems. SITZMANN, VINCENT, ZOLLH\u00d6FER, MICHAEL, and WET- ZSTEIN, GORDON. \"Scene representation networks: Continuous 3d- structure-aware neural scene representations\". Advances in Neural In- formation Processing Systems. 2019, 1121-1132 1, 2.\n\nFourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains. * , ] Tancik, Matthew Srinivasan, Pratul P Mildenhall, Ben, NeurIPS (2020) 2*20] TANCIK, MATTHEW, SRINIVASAN, PRATUL P., MILDENHALL, BEN, et al. \"Fourier Features Let Networks Learn High Frequency Func- tions in Low Dimensional Domains\". NeurIPS (2020) 2.\n\nGRF: Learning a General Radiance Field for 3D Scene Representation and Rendering. Yang Trevithick, B O , arXiv:2010.04595TREVITHICK, ALEX and YANG, BO. \"GRF: Learning a Gen- eral Radiance Field for 3D Scene Representation and Rendering\". arXiv:2010.04595. 2020 2.\n\nNeX: Real-time View Synthesis with Neural Basis Expansion. Suttisak Wizadwongsa, Phongthawee, Yenphraphai Pakkapon, Jiraphon , Suwajanakorn , Supasorn , IEEE Conference on Computer Vision and Pattern Recognition (CVPR). WIZADWONGSA, SUTTISAK, PHONGTHAWEE, PAKKAPON, YENPHRAPHAI, JIRAPHON, and SUWAJANAKORN, SUPASORN. \"NeX: Real-time View Synthesis with Neural Basis Expansion\". IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR). 2021 2, 7, 14.\n\nSpace-time Neural Irradiance Fields for Free-Viewpoint Video. Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Kim , Changil , arXiv:2011.12950211arXiv preprintXIAN, WENQI, HUANG, JIA-BIN, KOPF, JOHANNES, and KIM, CHANGIL. \"Space-time Neural Irradiance Fields for Free-Viewpoint Video\". arXiv preprint arXiv:2011.12950 (2020) 2, 11.\n\nSTaR: Self-Supervised Tracking and Reconstruction of Rigid Objects in Motion With Neural Rendering. Wentao Yuan, Lv, Zhaoyang, Tanner Schmidt, Lovegrove , Steven , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)YUAN, WENTAO, LV, ZHAOYANG, SCHMIDT, TANNER, and LOVEGROVE, STEVEN. \"STaR: Self-Supervised Tracking and Recon- struction of Rigid Objects in Motion With Neural Rendering\". Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR). June 2021, 13144-13152 2.\n\nPlenOctrees for Real-time Rendering of Neural Radiance Fields. Alex Yu, Li, Ruilong, Matthew Tancik, arXiv. 2021 2YLT*21[YLT*21] YU, ALEX, LI, RUILONG, TANCIK, MATTHEW, et al. \"PlenOc- trees for Real-time Rendering of Neural Radiance Fields\". arXiv. 2021 2.\n\npixelNeRF: Neural Radiance Fields From One or Few Images. Alex Yu, Y E Vickie, Matthew Tancik, Kanazawa , Angjoo , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)YU, ALEX, YE, VICKIE, TANCIK, MATTHEW, and KANAZAWA, ANGJOO. \"pixelNeRF: Neural Radiance Fields From One or Few Images\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021, 4578-4587 2.\n\nNeRF++: Analyzing and Improving Neural Radiance Fields. Kai Zhang, Riegler, Gernot, Noah Snavely, Koltun , Vladlen , arXiv:2010.07492[cs.CV]2ZHANG, KAI, RIEGLER, GERNOT, SNAVELY, NOAH, and KOLTUN, VLADLEN. \"NeRF++: Analyzing and Improving Neural Radi- ance Fields\". (2020). arXiv: 2010.07492 [cs.CV] 2.\n\nStereo Magnification: Learning View Synthesis Using Multiplane Images. Tinghui Zhou, Richard Tucker, Flynn , John , ACM Trans. Graph. 374ZTF*18[ZTF*18] ZHOU, TINGHUI, TUCKER, RICHARD, FLYNN, JOHN, et al. \"Stereo Magnification: Learning View Synthesis Using Multiplane Im- ages\". ACM Trans. Graph. 37.4 (July 2018) 2.\n\nZ-1 I-1 26. 28.776 30.476 31.926 0.074 0.061 0.052 0.047936K-1 Z-1 I-1 26.936 28.776 30.476 31.926 0.074 0.061 0.052 0.047\n\nZ-1 I-1 30. 33.241 34.893 36.019 0.059 0.046 0.041 0.038435K-5 Z-1 I-1 30.435 33.241 34.893 36.019 0.059 0.046 0.041 0.038\n\nPlease refer to Section 5 of the main paper for a detailed explanation on these depth oracle configurations. Forest PSNR \u2191 FLIP \u2193 Method N = 2 N = 4 N = 8 N = 16 N = 2 N = 4 N = 8 N = 16 SD. 29.884 30.640 31.832 33.212 0.067 0.062 0.057 0.053 SD unified 30.768 31.608 32.822 33.876 0.063 0.058 0.054 0.050Table 4: Ablation results for various depth oracle configurations for the Forest scene. Table 4: Ablation results for various depth oracle configurations for the Forest scene. Please refer to Section 5 of the main paper for a detailed explanation on these depth oracle configurations. Forest PSNR \u2191 FLIP \u2193 Method N = 2 N = 4 N = 8 N = 16 N = 2 N = 4 N = 8 N = 16 SD 29.884 30.640 31.832 33.212 0.067 0.062 0.057 0.053 SD unified 30.768 31.608 32.822 33.876 0.063 0.058 0.054 0.050\n\nZ-1 I-1 29.261. 30.933 32.425 33.501 0.069 0.061 0.056 0.052K-1 Z-1 I-1 29.261 30.933 32.425 33.501 0.069 0.061 0.056 0.052\n\nZ-1 I-1 30. 32.535 34.057 34.978 0.064 0.056 0.051 0.050262K-5 Z-1 I-1 30.262 32.535 34.057 34.978 0.064 0.056 0.051 0.050\n\nPlease refer to Section 5 of the main paper for a detailed explanation on these depth oracle configurations. 26.832 27.403 28.205 29.601 0.103 0.095 0.086 0.076 SD unified 27.732 28.315 29.233 30.351 0.091 0.086 0.078 0.071Table 5: Ablation results for various depth oracle configurations for the Classroom scene. Classroom PSNR \u2191 FLIP \u2193 Method N = 2 N = 4 N = 8 N = 16 N = 2 N = 4 N = 8 N = 16 SDTable 5: Ablation results for various depth oracle configurations for the Classroom scene. Please refer to Section 5 of the main paper for a detailed explanation on these depth oracle configurations. Classroom PSNR \u2191 FLIP \u2193 Method N = 2 N = 4 N = 8 N = 16 N = 2 N = 4 N = 8 N = 16 SD 26.832 27.403 28.205 29.601 0.103 0.095 0.086 0.076 SD unified 27.732 28.315 29.233 30.351 0.091 0.086 0.078 0.071\n\nZ-1 I-1 27. 28.882 29.587 31.733 0.092 0.081 0.079 0.068449K-1 Z-1 I-1 27.449 28.882 29.587 31.733 0.092 0.081 0.079 0.068\n\nZ-1 I-1 28.697 30.506 30. 32.998 0.083 0.072 0.074 0.063823K-5 Z-1 I-1 28.697 30.506 30.823 32.998 0.083 0.072 0.074 0.063\n\nTable 6: Ablation results for various depth oracle configurations for the San Miguel scene. Table 6: Ablation results for various depth oracle configurations for the San Miguel scene.\n\nZ-1 I-1 26. 27.580 28.547 29.364 0.086 0.078 0.071 0.066406K-5 Z-1 I-1 26.406 27.580 28.547 29.364 0.086 0.078 0.071 0.066\n", "annotations": {"author": "[{\"end\":151,\"start\":105},{\"end\":204,\"start\":152},{\"end\":253,\"start\":205},{\"end\":300,\"start\":254},{\"end\":352,\"start\":301},{\"end\":408,\"start\":353},{\"end\":460,\"start\":409},{\"end\":514,\"start\":461}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":107},{\"end\":164,\"start\":154},{\"end\":213,\"start\":207},{\"end\":260,\"start\":256},{\"end\":312,\"start\":305},{\"end\":368,\"start\":359},{\"end\":420,\"start\":411},{\"end\":474,\"start\":463}]", "author_first_name": "[{\"end\":106,\"start\":105},{\"end\":153,\"start\":152},{\"end\":206,\"start\":205},{\"end\":255,\"start\":254},{\"end\":302,\"start\":301},{\"end\":304,\"start\":303},{\"end\":354,\"start\":353},{\"end\":358,\"start\":355},{\"end\":410,\"start\":409},{\"end\":462,\"start\":461}]", "author_affiliation": "[{\"end\":150,\"start\":113},{\"end\":203,\"start\":166},{\"end\":252,\"start\":215},{\"end\":299,\"start\":262},{\"end\":351,\"start\":314},{\"end\":407,\"start\":370},{\"end\":459,\"start\":422},{\"end\":513,\"start\":476}]", "title": "[{\"end\":98,\"start\":1},{\"end\":612,\"start\":515}]", "venue": "[{\"end\":672,\"start\":614}]", "abstract": null, "bib_ref": "[{\"end\":6437,\"start\":6429},{\"end\":6444,\"start\":6437},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6778,\"start\":6770},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6884,\"start\":6876},{\"end\":7021,\"start\":7013},{\"end\":7247,\"start\":7221},{\"end\":7457,\"start\":7449},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7689,\"start\":7683},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8259,\"start\":8252},{\"end\":12242,\"start\":12234},{\"end\":16478,\"start\":16475},{\"end\":16480,\"start\":16478},{\"end\":16482,\"start\":16480},{\"end\":16485,\"start\":16482},{\"end\":16488,\"start\":16485},{\"end\":16491,\"start\":16488},{\"end\":16495,\"start\":16491},{\"end\":26774,\"start\":26771},{\"end\":26776,\"start\":26774},{\"end\":26778,\"start\":26776},{\"end\":26781,\"start\":26778},{\"end\":28496,\"start\":28488},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28538,\"start\":28530},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30884,\"start\":30878},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45109,\"start\":45101},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":45116,\"start\":45109},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47292,\"start\":47284}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49572,\"start\":48550},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49819,\"start\":49573},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50232,\"start\":49820},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50505,\"start\":50233},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50922,\"start\":50506},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51382,\"start\":50923},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":52186,\"start\":51383},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":52562,\"start\":52187},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":54433,\"start\":52563},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55554,\"start\":54434},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56575,\"start\":55555},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57424,\"start\":56576},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58460,\"start\":57425}]", "paragraph": "[{\"end\":1208,\"start\":692},{\"end\":2906,\"start\":1221},{\"end\":2961,\"start\":2923},{\"end\":3495,\"start\":2978},{\"end\":4204,\"start\":3497},{\"end\":4537,\"start\":4206},{\"end\":5721,\"start\":4539},{\"end\":6304,\"start\":5723},{\"end\":8392,\"start\":6321},{\"end\":8858,\"start\":8434},{\"end\":9375,\"start\":8897},{\"end\":9884,\"start\":9377},{\"end\":10714,\"start\":9943},{\"end\":11719,\"start\":10716},{\"end\":12200,\"start\":11749},{\"end\":12513,\"start\":12202},{\"end\":12635,\"start\":12515},{\"end\":12661,\"start\":12658},{\"end\":13072,\"start\":12726},{\"end\":13242,\"start\":13112},{\"end\":13442,\"start\":13244},{\"end\":14224,\"start\":13522},{\"end\":14676,\"start\":14226},{\"end\":14974,\"start\":14678},{\"end\":15697,\"start\":15047},{\"end\":16196,\"start\":15699},{\"end\":17033,\"start\":16198},{\"end\":17921,\"start\":17075},{\"end\":18635,\"start\":17923},{\"end\":19436,\"start\":18663},{\"end\":20241,\"start\":19438},{\"end\":20719,\"start\":20262},{\"end\":21060,\"start\":20721},{\"end\":21409,\"start\":21110},{\"end\":21751,\"start\":21411},{\"end\":22288,\"start\":21872},{\"end\":22336,\"start\":22290},{\"end\":22816,\"start\":22403},{\"end\":22947,\"start\":22818},{\"end\":23611,\"start\":23014},{\"end\":24332,\"start\":23613},{\"end\":25084,\"start\":24334},{\"end\":25300,\"start\":25086},{\"end\":25810,\"start\":25337},{\"end\":26342,\"start\":25812},{\"end\":26727,\"start\":26391},{\"end\":27378,\"start\":26729},{\"end\":27993,\"start\":27380},{\"end\":29787,\"start\":28008},{\"end\":30433,\"start\":29789},{\"end\":30963,\"start\":30475},{\"end\":33607,\"start\":30965},{\"end\":33889,\"start\":33763},{\"end\":34376,\"start\":33929},{\"end\":34481,\"start\":34378},{\"end\":34961,\"start\":34483},{\"end\":35606,\"start\":34963},{\"end\":36593,\"start\":35618},{\"end\":36849,\"start\":36609},{\"end\":37776,\"start\":36851},{\"end\":39576,\"start\":37778},{\"end\":40808,\"start\":39578},{\"end\":41809,\"start\":40846},{\"end\":42483,\"start\":41811},{\"end\":42850,\"start\":42485},{\"end\":44173,\"start\":42894},{\"end\":44907,\"start\":44175},{\"end\":45599,\"start\":44909},{\"end\":45958,\"start\":45601},{\"end\":46422,\"start\":45976},{\"end\":47055,\"start\":46424},{\"end\":47278,\"start\":47073},{\"end\":48549,\"start\":47280}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12657,\"start\":12636},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12725,\"start\":12662},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13111,\"start\":13073},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13521,\"start\":13443},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15046,\"start\":14975},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17074,\"start\":17034},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21109,\"start\":21061},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21871,\"start\":21752},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22402,\"start\":22337},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23013,\"start\":22948},{\"attributes\":{\"id\":\"formula_10\"},\"end\":33692,\"start\":33608},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33762,\"start\":33692},{\"attributes\":{\"id\":\"formula_12\"},\"end\":33928,\"start\":33890}]", "table_ref": "[{\"end\":27402,\"start\":27395},{\"end\":28621,\"start\":28614},{\"end\":32051,\"start\":32044},{\"end\":36669,\"start\":36662},{\"end\":41002,\"start\":40995},{\"end\":48172,\"start\":48165},{\"end\":48374,\"start\":48367}]", "section_header": "[{\"end\":1219,\"start\":1211},{\"end\":2921,\"start\":2909},{\"attributes\":{\"n\":\"1.\"},\"end\":2976,\"start\":2964},{\"attributes\":{\"n\":\"2.\"},\"end\":6319,\"start\":6307},{\"end\":8432,\"start\":8395},{\"end\":8876,\"start\":8861},{\"end\":8895,\"start\":8879},{\"attributes\":{\"n\":\"3.\"},\"end\":9941,\"start\":9887},{\"attributes\":{\"n\":\"4.\"},\"end\":11747,\"start\":11722},{\"attributes\":{\"n\":\"5.\"},\"end\":18661,\"start\":18638},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20260,\"start\":20244},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25335,\"start\":25303},{\"attributes\":{\"n\":\"5.3.\"},\"end\":26389,\"start\":26345},{\"attributes\":{\"n\":\"6.\"},\"end\":28006,\"start\":27996},{\"attributes\":{\"n\":\"6.1.\"},\"end\":30473,\"start\":30436},{\"attributes\":{\"n\":\"6.2.\"},\"end\":35616,\"start\":35609},{\"attributes\":{\"n\":\"6.3.\"},\"end\":36607,\"start\":36596},{\"attributes\":{\"n\":\"6.4.\"},\"end\":40844,\"start\":40811},{\"attributes\":{\"n\":\"7.\"},\"end\":42892,\"start\":42853},{\"end\":45974,\"start\":45961},{\"end\":47071,\"start\":47058},{\"end\":48571,\"start\":48551},{\"end\":49584,\"start\":49574},{\"end\":49831,\"start\":49821},{\"end\":50245,\"start\":50234},{\"end\":50518,\"start\":50507},{\"end\":50935,\"start\":50924},{\"end\":54444,\"start\":54435},{\"end\":55565,\"start\":55556},{\"end\":57436,\"start\":57426}]", "table": "[{\"end\":54433,\"start\":52636},{\"end\":55554,\"start\":54721},{\"end\":56575,\"start\":56158},{\"end\":57424,\"start\":56776},{\"end\":58460,\"start\":57715}]", "figure_caption": "[{\"end\":49572,\"start\":48574},{\"end\":49819,\"start\":49586},{\"end\":50232,\"start\":49833},{\"end\":50505,\"start\":50248},{\"end\":50922,\"start\":50521},{\"end\":51382,\"start\":50938},{\"end\":52186,\"start\":51385},{\"end\":52562,\"start\":52189},{\"end\":52636,\"start\":52565},{\"end\":54721,\"start\":54446},{\"end\":56158,\"start\":55567},{\"end\":56776,\"start\":56578},{\"end\":57715,\"start\":57439}]", "figure_ref": "[{\"end\":691,\"start\":683},{\"end\":8965,\"start\":8957},{\"end\":10276,\"start\":10268},{\"end\":11002,\"start\":10994},{\"end\":11952,\"start\":11944},{\"end\":15261,\"start\":15253},{\"end\":17113,\"start\":17105},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19434,\"start\":19426},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24098,\"start\":24090},{\"end\":24162,\"start\":24154},{\"end\":24805,\"start\":24797},{\"end\":25299,\"start\":25291},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25540,\"start\":25531},{\"end\":31302,\"start\":31293},{\"end\":36722,\"start\":36713},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":41470,\"start\":41460},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41930,\"start\":41921},{\"end\":46184,\"start\":46175}]", "bib_author_first_name": "[{\"end\":59769,\"start\":59768},{\"end\":59800,\"start\":59797},{\"end\":59815,\"start\":59810},{\"end\":60130,\"start\":60126},{\"end\":60144,\"start\":60137},{\"end\":60159,\"start\":60152},{\"end\":60431,\"start\":60424},{\"end\":60445,\"start\":60441},{\"end\":60457,\"start\":60453},{\"end\":60738,\"start\":60731},{\"end\":60776,\"start\":60766},{\"end\":60793,\"start\":60785},{\"end\":60802,\"start\":60796},{\"end\":61073,\"start\":61070},{\"end\":61097,\"start\":61091},{\"end\":61385,\"start\":61381},{\"end\":61413,\"start\":61409},{\"end\":61599,\"start\":61591},{\"end\":61613,\"start\":61610},{\"end\":61873,\"start\":61868},{\"end\":61883,\"start\":61878},{\"end\":61892,\"start\":61891},{\"end\":61894,\"start\":61893},{\"end\":61906,\"start\":61897},{\"end\":62169,\"start\":62168},{\"end\":62178,\"start\":62177},{\"end\":62189,\"start\":62188},{\"end\":62543,\"start\":62536},{\"end\":62545,\"start\":62544},{\"end\":62571,\"start\":62564},{\"end\":62586,\"start\":62579},{\"end\":62839,\"start\":62835},{\"end\":62852,\"start\":62845},{\"end\":62862,\"start\":62859},{\"end\":62874,\"start\":62865},{\"end\":63153,\"start\":63150},{\"end\":63183,\"start\":63176},{\"end\":63203,\"start\":63195},{\"end\":63214,\"start\":63206},{\"end\":63456,\"start\":63451},{\"end\":63471,\"start\":63465},{\"end\":63742,\"start\":63737},{\"end\":63757,\"start\":63751},{\"end\":63759,\"start\":63758},{\"end\":64031,\"start\":64023},{\"end\":64033,\"start\":64032},{\"end\":64047,\"start\":64042},{\"end\":64192,\"start\":64185},{\"end\":64204,\"start\":64198},{\"end\":64212,\"start\":64209},{\"end\":64514,\"start\":64509},{\"end\":64516,\"start\":64515},{\"end\":64532,\"start\":64526},{\"end\":64536,\"start\":64533},{\"end\":64554,\"start\":64545},{\"end\":64563,\"start\":64557},{\"end\":64791,\"start\":64784},{\"end\":64816,\"start\":64812},{\"end\":64830,\"start\":64826},{\"end\":64839,\"start\":64833},{\"end\":64864,\"start\":64859},{\"end\":64877,\"start\":64872},{\"end\":65406,\"start\":65399},{\"end\":65422,\"start\":65417},{\"end\":65437,\"start\":65430},{\"end\":65732,\"start\":65725},{\"end\":65768,\"start\":65763},{\"end\":65772,\"start\":65769},{\"end\":66319,\"start\":66316},{\"end\":66338,\"start\":66332},{\"end\":66340,\"start\":66339},{\"end\":66360,\"start\":66353},{\"end\":66709,\"start\":66708},{\"end\":66741,\"start\":66735},{\"end\":66743,\"start\":66742},{\"end\":66763,\"start\":66756},{\"end\":67043,\"start\":67037},{\"end\":67061,\"start\":67054},{\"end\":67515,\"start\":67509},{\"end\":67540,\"start\":67536},{\"end\":67994,\"start\":67988},{\"end\":68010,\"start\":68005},{\"end\":68028,\"start\":68019},{\"end\":68038,\"start\":68031},{\"end\":68062,\"start\":68054},{\"end\":68564,\"start\":68563},{\"end\":68566,\"start\":68565},{\"end\":68574,\"start\":68573},{\"end\":68586,\"start\":68585},{\"end\":68945,\"start\":68937},{\"end\":68959,\"start\":68952},{\"end\":68975,\"start\":68967},{\"end\":68977,\"start\":68976},{\"end\":69218,\"start\":69212},{\"end\":69230,\"start\":69227},{\"end\":69245,\"start\":69238},{\"end\":69255,\"start\":69248},{\"end\":69553,\"start\":69547},{\"end\":69565,\"start\":69562},{\"end\":69580,\"start\":69573},{\"end\":69590,\"start\":69583},{\"end\":70064,\"start\":70055},{\"end\":70092,\"start\":70088},{\"end\":70105,\"start\":70099},{\"end\":70115,\"start\":70108},{\"end\":70413,\"start\":70407},{\"end\":70415,\"start\":70414},{\"end\":70449,\"start\":70442},{\"end\":70952,\"start\":70947},{\"end\":70966,\"start\":70962},{\"end\":70980,\"start\":70973},{\"end\":70997,\"start\":70991},{\"end\":71007,\"start\":71000},{\"end\":71349,\"start\":71342},{\"end\":71366,\"start\":71360},{\"end\":71370,\"start\":71367},{\"end\":71388,\"start\":71379},{\"end\":71390,\"start\":71389},{\"end\":71667,\"start\":71666},{\"end\":71669,\"start\":71668},{\"end\":71683,\"start\":71682},{\"end\":71693,\"start\":71692},{\"end\":71695,\"start\":71694},{\"end\":72061,\"start\":72054},{\"end\":72084,\"start\":72079},{\"end\":72098,\"start\":72093},{\"end\":72566,\"start\":72559},{\"end\":72584,\"start\":72577},{\"end\":72606,\"start\":72596},{\"end\":72615,\"start\":72609},{\"end\":72993,\"start\":72992},{\"end\":72997,\"start\":72996},{\"end\":73013,\"start\":73006},{\"end\":73032,\"start\":73026},{\"end\":73034,\"start\":73033},{\"end\":73335,\"start\":73331},{\"end\":73349,\"start\":73348},{\"end\":73351,\"start\":73350},{\"end\":73581,\"start\":73573},{\"end\":73619,\"start\":73608},{\"end\":73638,\"start\":73630},{\"end\":73653,\"start\":73641},{\"end\":73664,\"start\":73656},{\"end\":74044,\"start\":74039},{\"end\":74058,\"start\":74051},{\"end\":74074,\"start\":74066},{\"end\":74084,\"start\":74081},{\"end\":74094,\"start\":74087},{\"end\":74410,\"start\":74404},{\"end\":74437,\"start\":74431},{\"end\":74456,\"start\":74447},{\"end\":74465,\"start\":74459},{\"end\":74991,\"start\":74987},{\"end\":75016,\"start\":75009},{\"end\":75245,\"start\":75241},{\"end\":75251,\"start\":75250},{\"end\":75253,\"start\":75252},{\"end\":75269,\"start\":75262},{\"end\":75286,\"start\":75278},{\"end\":75295,\"start\":75289},{\"end\":75756,\"start\":75753},{\"end\":75785,\"start\":75781},{\"end\":75801,\"start\":75795},{\"end\":75811,\"start\":75804},{\"end\":76079,\"start\":76072},{\"end\":76093,\"start\":76086},{\"end\":76107,\"start\":76102},{\"end\":76114,\"start\":76110}]", "bib_author_last_name": "[{\"end\":59776,\"start\":59770},{\"end\":59787,\"start\":59778},{\"end\":59795,\"start\":59789},{\"end\":59808,\"start\":59801},{\"end\":59830,\"start\":59816},{\"end\":60135,\"start\":60131},{\"end\":60150,\"start\":60145},{\"end\":60165,\"start\":60160},{\"end\":60439,\"start\":60432},{\"end\":60451,\"start\":60446},{\"end\":60466,\"start\":60458},{\"end\":60745,\"start\":60739},{\"end\":60757,\"start\":60747},{\"end\":60764,\"start\":60759},{\"end\":60783,\"start\":60777},{\"end\":61076,\"start\":61074},{\"end\":61080,\"start\":61078},{\"end\":61089,\"start\":61082},{\"end\":61108,\"start\":61098},{\"end\":61390,\"start\":61386},{\"end\":61400,\"start\":61392},{\"end\":61407,\"start\":61402},{\"end\":61424,\"start\":61414},{\"end\":61608,\"start\":61600},{\"end\":61876,\"start\":61874},{\"end\":61889,\"start\":61884},{\"end\":62175,\"start\":62170},{\"end\":62186,\"start\":62179},{\"end\":62197,\"start\":62190},{\"end\":62552,\"start\":62546},{\"end\":62562,\"start\":62554},{\"end\":62577,\"start\":62572},{\"end\":62843,\"start\":62840},{\"end\":62857,\"start\":62853},{\"end\":63159,\"start\":63154},{\"end\":63166,\"start\":63161},{\"end\":63174,\"start\":63168},{\"end\":63193,\"start\":63184},{\"end\":63463,\"start\":63457},{\"end\":63478,\"start\":63472},{\"end\":63485,\"start\":63480},{\"end\":63491,\"start\":63487},{\"end\":63749,\"start\":63743},{\"end\":63770,\"start\":63760},{\"end\":63782,\"start\":63772},{\"end\":63787,\"start\":63784},{\"end\":64040,\"start\":64034},{\"end\":64050,\"start\":64048},{\"end\":64196,\"start\":64193},{\"end\":64207,\"start\":64205},{\"end\":64216,\"start\":64213},{\"end\":64222,\"start\":64218},{\"end\":64524,\"start\":64517},{\"end\":64543,\"start\":64537},{\"end\":64794,\"start\":64792},{\"end\":64803,\"start\":64796},{\"end\":64810,\"start\":64805},{\"end\":64824,\"start\":64817},{\"end\":64848,\"start\":64840},{\"end\":64857,\"start\":64850},{\"end\":64870,\"start\":64865},{\"end\":64885,\"start\":64878},{\"end\":65415,\"start\":65407},{\"end\":65428,\"start\":65423},{\"end\":65446,\"start\":65438},{\"end\":65747,\"start\":65733},{\"end\":65755,\"start\":65749},{\"end\":65761,\"start\":65757},{\"end\":65780,\"start\":65773},{\"end\":66330,\"start\":66320},{\"end\":66351,\"start\":66341},{\"end\":66372,\"start\":66361},{\"end\":66716,\"start\":66710},{\"end\":66728,\"start\":66718},{\"end\":66733,\"start\":66730},{\"end\":66754,\"start\":66744},{\"end\":66770,\"start\":66764},{\"end\":67052,\"start\":67044},{\"end\":67519,\"start\":67516},{\"end\":67527,\"start\":67521},{\"end\":67534,\"start\":67529},{\"end\":67548,\"start\":67541},{\"end\":68003,\"start\":67995},{\"end\":68017,\"start\":68011},{\"end\":68052,\"start\":68039},{\"end\":68571,\"start\":68567},{\"end\":68583,\"start\":68575},{\"end\":68593,\"start\":68587},{\"end\":68950,\"start\":68946},{\"end\":68965,\"start\":68960},{\"end\":68984,\"start\":68978},{\"end\":69225,\"start\":69219},{\"end\":69236,\"start\":69231},{\"end\":69560,\"start\":69554},{\"end\":69571,\"start\":69566},{\"end\":70071,\"start\":70065},{\"end\":70077,\"start\":70073},{\"end\":70086,\"start\":70079},{\"end\":70097,\"start\":70093},{\"end\":70426,\"start\":70416},{\"end\":70432,\"start\":70428},{\"end\":70440,\"start\":70434},{\"end\":70455,\"start\":70450},{\"end\":70960,\"start\":70953},{\"end\":70971,\"start\":70967},{\"end\":70989,\"start\":70981},{\"end\":71358,\"start\":71350},{\"end\":71377,\"start\":71371},{\"end\":71398,\"start\":71391},{\"end\":71680,\"start\":71670},{\"end\":71690,\"start\":71684},{\"end\":71702,\"start\":71696},{\"end\":72070,\"start\":72062},{\"end\":72077,\"start\":72072},{\"end\":72091,\"start\":72085},{\"end\":72575,\"start\":72567},{\"end\":72594,\"start\":72585},{\"end\":73004,\"start\":72998},{\"end\":73024,\"start\":73014},{\"end\":73045,\"start\":73035},{\"end\":73050,\"start\":73047},{\"end\":73346,\"start\":73336},{\"end\":73593,\"start\":73582},{\"end\":73606,\"start\":73595},{\"end\":73628,\"start\":73620},{\"end\":74049,\"start\":74045},{\"end\":74064,\"start\":74059},{\"end\":74079,\"start\":74075},{\"end\":74415,\"start\":74411},{\"end\":74419,\"start\":74417},{\"end\":74429,\"start\":74421},{\"end\":74445,\"start\":74438},{\"end\":74994,\"start\":74992},{\"end\":74998,\"start\":74996},{\"end\":75007,\"start\":75000},{\"end\":75023,\"start\":75017},{\"end\":75248,\"start\":75246},{\"end\":75260,\"start\":75254},{\"end\":75276,\"start\":75270},{\"end\":75762,\"start\":75757},{\"end\":75771,\"start\":75764},{\"end\":75779,\"start\":75773},{\"end\":75793,\"start\":75786},{\"end\":76084,\"start\":76080},{\"end\":76100,\"start\":76094}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":220643528},\"end\":60061,\"start\":59715},{\"attributes\":{\"doi\":\"arXiv:2012.03918[cs.CV]2\",\"id\":\"b1\"},\"end\":60358,\"start\":60063},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":221105228},\"end\":60661,\"start\":60360},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":222091017},\"end\":61014,\"start\":60663},{\"attributes\":{\"doi\":\"arXiv:2008.03824[cs.CV]2\",\"id\":\"b4\"},\"end\":61291,\"start\":61016},{\"attributes\":{\"id\":\"b5\"},\"end\":61589,\"start\":61293},{\"attributes\":{\"doi\":\"arXiv:2101.05204[cs.CV]2\",\"id\":\"b6\"},\"end\":61801,\"start\":61591},{\"attributes\":{\"doi\":\"arXiv:2012.09790\",\"id\":\"b7\"},\"end\":62110,\"start\":61803},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":190000375},\"end\":62489,\"start\":62112},{\"attributes\":{\"doi\":\"arXiv:2103.10380[cs.CV]2\",\"id\":\"b9\"},\"end\":62780,\"start\":62491},{\"attributes\":{\"doi\":\"arXiv:2012.05903\",\"id\":\"b10\"},\"end\":63070,\"start\":62782},{\"attributes\":{\"id\":\"b11\"},\"end\":63393,\"start\":63072},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":54151018},\"end\":63675,\"start\":63395},{\"attributes\":{\"id\":\"b13\"},\"end\":63961,\"start\":63677},{\"attributes\":{\"id\":\"b14\"},\"end\":64155,\"start\":63963},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220686483},\"end\":64442,\"start\":64157},{\"attributes\":{\"id\":\"b16\"},\"end\":64708,\"start\":64444},{\"attributes\":{\"doi\":\"11. [LSS*19\",\"id\":\"b17\",\"matched_paper_id\":227208781},\"end\":65332,\"start\":64710},{\"attributes\":{\"doi\":\"arXiv:2103.01954[cs.GR]2\",\"id\":\"b18\"},\"end\":65645,\"start\":65334},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220968781},\"end\":66224,\"start\":65647},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219947110},\"end\":66634,\"start\":66226},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":213175590},\"end\":66955,\"start\":66636},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":227151657},\"end\":67467,\"start\":66957},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":227118710},\"end\":67935,\"start\":67469},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":227227965},\"end\":68480,\"start\":67937},{\"attributes\":{\"id\":\"b25\"},\"end\":68740,\"start\":68482},{\"attributes\":{\"id\":\"b26\"},\"end\":68900,\"start\":68742},{\"attributes\":{\"doi\":\"arXiv:2011.12948(2020)2\",\"id\":\"b27\"},\"end\":69176,\"start\":68902},{\"attributes\":{\"doi\":\"arXiv:2011.12490\",\"id\":\"b28\"},\"end\":69511,\"start\":69178},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":227162149},\"end\":69979,\"start\":69513},{\"attributes\":{\"id\":\"b30\"},\"end\":70323,\"start\":69981},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":227348246},\"end\":70882,\"start\":70325},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":220364071},\"end\":71272,\"start\":70884},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":219720931},\"end\":71595,\"start\":71274},{\"attributes\":{\"id\":\"b34\"},\"end\":71837,\"start\":71597},{\"attributes\":{\"id\":\"b35\"},\"end\":71997,\"start\":71839},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":54444417},\"end\":72466,\"start\":71999},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":174798113},\"end\":72901,\"start\":72468},{\"attributes\":{\"id\":\"b38\"},\"end\":73247,\"start\":72903},{\"attributes\":{\"id\":\"b39\"},\"end\":73512,\"start\":73249},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":232168851},\"end\":73975,\"start\":73514},{\"attributes\":{\"id\":\"b41\"},\"end\":74302,\"start\":73977},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":230523891},\"end\":74922,\"start\":74304},{\"attributes\":{\"id\":\"b43\"},\"end\":75181,\"start\":74924},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":227254854},\"end\":75695,\"start\":75183},{\"attributes\":{\"id\":\"b45\"},\"end\":75999,\"start\":75697},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":219893035},\"end\":76317,\"start\":76001},{\"attributes\":{\"id\":\"b47\"},\"end\":76441,\"start\":76319},{\"attributes\":{\"id\":\"b48\"},\"end\":76565,\"start\":76443},{\"attributes\":{\"id\":\"b49\"},\"end\":77352,\"start\":76567},{\"attributes\":{\"id\":\"b50\"},\"end\":77477,\"start\":77354},{\"attributes\":{\"id\":\"b51\"},\"end\":77601,\"start\":77479},{\"attributes\":{\"id\":\"b52\"},\"end\":78398,\"start\":77603},{\"attributes\":{\"id\":\"b53\"},\"end\":78522,\"start\":78400},{\"attributes\":{\"id\":\"b54\"},\"end\":78646,\"start\":78524},{\"attributes\":{\"id\":\"b55\"},\"end\":78831,\"start\":78648},{\"attributes\":{\"id\":\"b56\"},\"end\":78955,\"start\":78833}]", "bib_title": "[{\"end\":59766,\"start\":59715},{\"end\":60422,\"start\":60360},{\"end\":60729,\"start\":60663},{\"end\":62166,\"start\":62112},{\"end\":63449,\"start\":63395},{\"end\":64183,\"start\":64157},{\"end\":64782,\"start\":64710},{\"end\":65723,\"start\":65647},{\"end\":66314,\"start\":66226},{\"end\":66706,\"start\":66636},{\"end\":67035,\"start\":66957},{\"end\":67507,\"start\":67469},{\"end\":67986,\"start\":67937},{\"end\":69545,\"start\":69513},{\"end\":70405,\"start\":70325},{\"end\":70945,\"start\":70884},{\"end\":71340,\"start\":71274},{\"end\":72052,\"start\":71999},{\"end\":72557,\"start\":72468},{\"end\":73571,\"start\":73514},{\"end\":74402,\"start\":74304},{\"end\":75239,\"start\":75183},{\"end\":76070,\"start\":76001},{\"end\":76756,\"start\":76567},{\"end\":77710,\"start\":77603}]", "bib_author": "[{\"end\":59778,\"start\":59768},{\"end\":59789,\"start\":59778},{\"end\":59797,\"start\":59789},{\"end\":59810,\"start\":59797},{\"end\":59832,\"start\":59810},{\"end\":60137,\"start\":60126},{\"end\":60152,\"start\":60137},{\"end\":60167,\"start\":60152},{\"end\":60441,\"start\":60424},{\"end\":60453,\"start\":60441},{\"end\":60468,\"start\":60453},{\"end\":60747,\"start\":60731},{\"end\":60759,\"start\":60747},{\"end\":60766,\"start\":60759},{\"end\":60785,\"start\":60766},{\"end\":60796,\"start\":60785},{\"end\":60805,\"start\":60796},{\"end\":61078,\"start\":61070},{\"end\":61082,\"start\":61078},{\"end\":61091,\"start\":61082},{\"end\":61110,\"start\":61091},{\"end\":61392,\"start\":61381},{\"end\":61402,\"start\":61392},{\"end\":61409,\"start\":61402},{\"end\":61426,\"start\":61409},{\"end\":61610,\"start\":61591},{\"end\":61616,\"start\":61610},{\"end\":61878,\"start\":61868},{\"end\":61891,\"start\":61878},{\"end\":61897,\"start\":61891},{\"end\":61909,\"start\":61897},{\"end\":62177,\"start\":62168},{\"end\":62188,\"start\":62177},{\"end\":62199,\"start\":62188},{\"end\":62554,\"start\":62536},{\"end\":62564,\"start\":62554},{\"end\":62579,\"start\":62564},{\"end\":62589,\"start\":62579},{\"end\":62845,\"start\":62835},{\"end\":62859,\"start\":62845},{\"end\":62865,\"start\":62859},{\"end\":62877,\"start\":62865},{\"end\":63161,\"start\":63150},{\"end\":63168,\"start\":63161},{\"end\":63176,\"start\":63168},{\"end\":63195,\"start\":63176},{\"end\":63206,\"start\":63195},{\"end\":63217,\"start\":63206},{\"end\":63465,\"start\":63451},{\"end\":63480,\"start\":63465},{\"end\":63487,\"start\":63480},{\"end\":63493,\"start\":63487},{\"end\":63751,\"start\":63737},{\"end\":63772,\"start\":63751},{\"end\":63784,\"start\":63772},{\"end\":63789,\"start\":63784},{\"end\":64042,\"start\":64023},{\"end\":64052,\"start\":64042},{\"end\":64198,\"start\":64185},{\"end\":64209,\"start\":64198},{\"end\":64218,\"start\":64209},{\"end\":64224,\"start\":64218},{\"end\":64526,\"start\":64509},{\"end\":64545,\"start\":64526},{\"end\":64557,\"start\":64545},{\"end\":64566,\"start\":64557},{\"end\":64796,\"start\":64784},{\"end\":64805,\"start\":64796},{\"end\":64812,\"start\":64805},{\"end\":64826,\"start\":64812},{\"end\":64833,\"start\":64826},{\"end\":64850,\"start\":64833},{\"end\":64859,\"start\":64850},{\"end\":64872,\"start\":64859},{\"end\":64887,\"start\":64872},{\"end\":65417,\"start\":65399},{\"end\":65430,\"start\":65417},{\"end\":65448,\"start\":65430},{\"end\":65749,\"start\":65725},{\"end\":65757,\"start\":65749},{\"end\":65763,\"start\":65757},{\"end\":65782,\"start\":65763},{\"end\":66332,\"start\":66316},{\"end\":66353,\"start\":66332},{\"end\":66374,\"start\":66353},{\"end\":66718,\"start\":66708},{\"end\":66730,\"start\":66718},{\"end\":66735,\"start\":66730},{\"end\":66756,\"start\":66735},{\"end\":66772,\"start\":66756},{\"end\":67054,\"start\":67037},{\"end\":67064,\"start\":67054},{\"end\":67521,\"start\":67509},{\"end\":67529,\"start\":67521},{\"end\":67536,\"start\":67529},{\"end\":67550,\"start\":67536},{\"end\":68005,\"start\":67988},{\"end\":68019,\"start\":68005},{\"end\":68031,\"start\":68019},{\"end\":68054,\"start\":68031},{\"end\":68065,\"start\":68054},{\"end\":68573,\"start\":68563},{\"end\":68585,\"start\":68573},{\"end\":68595,\"start\":68585},{\"end\":68952,\"start\":68937},{\"end\":68967,\"start\":68952},{\"end\":68986,\"start\":68967},{\"end\":69227,\"start\":69212},{\"end\":69238,\"start\":69227},{\"end\":69248,\"start\":69238},{\"end\":69258,\"start\":69248},{\"end\":69562,\"start\":69547},{\"end\":69573,\"start\":69562},{\"end\":69583,\"start\":69573},{\"end\":69593,\"start\":69583},{\"end\":70073,\"start\":70055},{\"end\":70079,\"start\":70073},{\"end\":70088,\"start\":70079},{\"end\":70099,\"start\":70088},{\"end\":70108,\"start\":70099},{\"end\":70118,\"start\":70108},{\"end\":70428,\"start\":70407},{\"end\":70434,\"start\":70428},{\"end\":70442,\"start\":70434},{\"end\":70457,\"start\":70442},{\"end\":70962,\"start\":70947},{\"end\":70973,\"start\":70962},{\"end\":70991,\"start\":70973},{\"end\":71000,\"start\":70991},{\"end\":71010,\"start\":71000},{\"end\":71360,\"start\":71342},{\"end\":71379,\"start\":71360},{\"end\":71400,\"start\":71379},{\"end\":71682,\"start\":71666},{\"end\":71692,\"start\":71682},{\"end\":71704,\"start\":71692},{\"end\":72072,\"start\":72054},{\"end\":72079,\"start\":72072},{\"end\":72093,\"start\":72079},{\"end\":72101,\"start\":72093},{\"end\":72577,\"start\":72559},{\"end\":72596,\"start\":72577},{\"end\":72609,\"start\":72596},{\"end\":72618,\"start\":72609},{\"end\":72996,\"start\":72992},{\"end\":73006,\"start\":72996},{\"end\":73026,\"start\":73006},{\"end\":73047,\"start\":73026},{\"end\":73052,\"start\":73047},{\"end\":73348,\"start\":73331},{\"end\":73354,\"start\":73348},{\"end\":73595,\"start\":73573},{\"end\":73608,\"start\":73595},{\"end\":73630,\"start\":73608},{\"end\":73641,\"start\":73630},{\"end\":73656,\"start\":73641},{\"end\":73667,\"start\":73656},{\"end\":74051,\"start\":74039},{\"end\":74066,\"start\":74051},{\"end\":74081,\"start\":74066},{\"end\":74087,\"start\":74081},{\"end\":74097,\"start\":74087},{\"end\":74417,\"start\":74404},{\"end\":74421,\"start\":74417},{\"end\":74431,\"start\":74421},{\"end\":74447,\"start\":74431},{\"end\":74459,\"start\":74447},{\"end\":74468,\"start\":74459},{\"end\":74996,\"start\":74987},{\"end\":75000,\"start\":74996},{\"end\":75009,\"start\":75000},{\"end\":75025,\"start\":75009},{\"end\":75250,\"start\":75241},{\"end\":75262,\"start\":75250},{\"end\":75278,\"start\":75262},{\"end\":75289,\"start\":75278},{\"end\":75298,\"start\":75289},{\"end\":75764,\"start\":75753},{\"end\":75773,\"start\":75764},{\"end\":75781,\"start\":75773},{\"end\":75795,\"start\":75781},{\"end\":75804,\"start\":75795},{\"end\":75814,\"start\":75804},{\"end\":76086,\"start\":76072},{\"end\":76102,\"start\":76086},{\"end\":76110,\"start\":76102},{\"end\":76117,\"start\":76110}]", "bib_venue": "[{\"end\":59871,\"start\":59832},{\"end\":60124,\"start\":60063},{\"end\":60484,\"start\":60468},{\"end\":60821,\"start\":60805},{\"end\":61068,\"start\":61016},{\"end\":61379,\"start\":61293},{\"end\":61686,\"start\":61640},{\"end\":61866,\"start\":61803},{\"end\":62273,\"start\":62199},{\"end\":62534,\"start\":62491},{\"end\":62833,\"start\":62782},{\"end\":63148,\"start\":63072},{\"end\":63509,\"start\":63493},{\"end\":63735,\"start\":63677},{\"end\":64021,\"start\":63963},{\"end\":64273,\"start\":64224},{\"end\":64507,\"start\":64444},{\"end\":64914,\"start\":64898},{\"end\":65397,\"start\":65334},{\"end\":65870,\"start\":65782},{\"end\":66402,\"start\":66374},{\"end\":66776,\"start\":66772},{\"end\":67152,\"start\":67064},{\"end\":67638,\"start\":67550},{\"end\":68153,\"start\":68065},{\"end\":68561,\"start\":68482},{\"end\":68811,\"start\":68742},{\"end\":68935,\"start\":68902},{\"end\":69210,\"start\":69178},{\"end\":69681,\"start\":69593},{\"end\":70053,\"start\":69981},{\"end\":70545,\"start\":70457},{\"end\":71069,\"start\":71010},{\"end\":71413,\"start\":71400},{\"end\":71664,\"start\":71597},{\"end\":71908,\"start\":71839},{\"end\":72178,\"start\":72101},{\"end\":72667,\"start\":72618},{\"end\":72990,\"start\":72903},{\"end\":73329,\"start\":73249},{\"end\":73732,\"start\":73667},{\"end\":74037,\"start\":73977},{\"end\":74556,\"start\":74468},{\"end\":74985,\"start\":74924},{\"end\":75386,\"start\":75298},{\"end\":75751,\"start\":75697},{\"end\":76133,\"start\":76117},{\"end\":76329,\"start\":76319},{\"end\":76453,\"start\":76443},{\"end\":76958,\"start\":76872},{\"end\":77368,\"start\":77354},{\"end\":77489,\"start\":77479},{\"end\":77915,\"start\":77826},{\"end\":78410,\"start\":78400},{\"end\":78548,\"start\":78524},{\"end\":78738,\"start\":78648},{\"end\":78843,\"start\":78833},{\"end\":65945,\"start\":65872},{\"end\":67227,\"start\":67154},{\"end\":67713,\"start\":67640},{\"end\":68228,\"start\":68155},{\"end\":69756,\"start\":69683},{\"end\":70620,\"start\":70547},{\"end\":72242,\"start\":72180},{\"end\":74631,\"start\":74558},{\"end\":75461,\"start\":75388}]"}}}, "year": 2023, "month": 12, "day": 17}