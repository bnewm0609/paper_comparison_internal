{"id": 257313287, "updated": "2023-03-11 20:13:53.523", "metadata": {"title": "SeTransformer: A Transformer-Based Code Semantic Parser for Code Comment Generation", "authors": "[{\"first\":\"Zheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yonghao\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Zeyu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Doyle\",\"last\":\"Paul\",\"middle\":[]}]", "venue": "IEEE Transactions on Reliability", "journal": "IEEE Transactions on Reliability", "publication_date": {"year": 2023, "month": 3, "day": 1}, "abstract": "Automated code comment generation technologies can help developers understand code intent, which can significantly reduce the cost of software maintenance and revision. The latest studies in this field mainly depend on deep neural networks, such as convolutional neural networks and recurrent neural network. However, these methods may not generate high-quality and readable code comments due to the long-term dependence problem, which means that the code blocks used to summarize information are far from each other. Owing to the long-term dependence problem, these methods forget the previous input data\u2019s feature information during the training process. In this article, to solve the long-term dependence problem and extract both the text and structure information from the program code, we propose a novel improved-Transformer-based comment generation method, named SeTransformer. Specifically, the SeTransformer utilizes the code tokens and an abstract syntax tree (AST) of programs to extract information as the inputs, and then, it leverages the self-attention mechanism to analyze the text and structural features of code simultaneously. Experimental results based on public corpus gathered from large-scale open-source projects show that our method can significantly outperform five state-of-the-art baselines (such as Hybrid-DeepCom and AST-attendgru). Furthermore, we also conduct a questionnaire survey for developers, and the results show that the SeTransformer can generate higher quality comments than those of other baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tr/LiWPCSLD23", "doi": "10.1109/tr.2022.3154773"}}, "content": {"source": {"pdf_hash": "05d2bf4e174b5c767175e8721e01dd9b68604c0c", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a902630c550f4eab5dabd90518c9662aa72ba0eb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/05d2bf4e174b5c767175e8721e01dd9b68604c0c.txt", "contents": "\nSeTransformer: A Transformer-Based Code Semantic Parser for Code Comment Generation\nMARCH 2023\n\nZheng Li lizheng@mail.buct.edu.cn \nYonghao Wu \nBin Peng \nMember, IEEEXiang Chen \nZeyu Sun \nMember, IEEEYong Liu \nDoyle Paul paul.doyle@tudublin.ie. \nYong Liu \nZheng Li \nYonghao Wu \nBin Peng \nYong Liu \n\nCol-lege of Information Science and Technology\nSchool of Information Science and Technology\nBeijing University of Chem-ical Technology\n100013BeijingChina\n\n\nSchool of Electronic Engineering and Computer Science\nNantong University\n226007NantongChina\n\n\nSchool of Computer Science\nPeking University\n100871BeijingChina\n\n\nTechnological University Dublin\nD07 EWV4DublinIreland\n\nSeTransformer: A Transformer-Based Code Semantic Parser for Code Comment Generation\n\nIEEE TRANSACTIONS ON RELIABILITY\n721MARCH 202310.1109/TR.2022.3154773Manuscript received 9 February 2021; revised 21 July 2021 and 28 December 2021; accepted 22 February 2022. Date of publication 29 March 2022; date of current version 3 March 2023.258 (Corresponding author: Doyle Paul is with the Color versions of one or more figures in this article are available atIndex Terms-Code comment generationconvolutional neural network (CNN)deep learningprogram comprehensionTransformer\nAutomated code comment generation technologies can help developers understand code intent, which can significantly reduce the cost of software maintenance and revision. The latest studies in this field mainly depend on deep neural networks, such as convolutional neural networks and recurrent neural network. However, these methods may not generate high-quality and readable code comments due to the long-term dependence problem, which means that the code blocks used to summarize information are far from each other. Owing to the long-term dependence problem, these methods forget the previous input data's feature information during the training process. In this article, to solve the long-term dependence problem and extract both the text and structure information from the program code, we propose a novel improved-Transformer-based comment generation method, named SeTransformer. Specifically, the SeTransformer utilizes the code tokens and an abstract syntax tree (AST) of programs to extract information as the inputs, and then, it leverages the selfattention mechanism to analyze the text and structural features of code simultaneously. Experimental results based on public corpus gathered from large-scale open-source projects show that our method can significantly outperform five state-of-the-art baselines (such as Hybrid-DeepCom and AST-attendgru). Furthermore, we also conduct a questionnaire survey for developers, and the results show that the SeTransformer can generate higher quality comments than those of other baselines.\n\nI. INTRODUCTION\n\nD URING software development and maintenance, developers often spend much of their time on program comprehension. Previous studies have shown that high-quality code comments can effectively improve a programs' comprehensibility because the developers can understand the program by just reading natural-language-based comments [1]. Therefore, researchers have proposed different automated techniques to help developers generate code comments for the target programs and, thus, substantially reduce the effort required for software maintenance.\n\nThese methods were designed to generate code comments at the class [2] or function level [1], [3]. Initial methods were usually designed based on handcrafting or information retrieval techniques. However, with the advancement of deep learning models and the sharing of many corpora gathered from opensource projects, researchers gradually turned their attention to deep learning and proposed some neural-network-based code comment generation methods. Most of these methods chose sequence-to-sequence methods for neural network training. For example, Iyer et al. [4] trained an embedding matrix to represent each code token in their work and coupled them with a recurrent neural network (RNN) model through an attention mechanism to generate code comments. Hu et al. [5] proposed a new structure-based traversal (SBT) method to serialize abstract syntax trees (ASTs). Then, they proposed a method of automatically generating code comments based on the RNN.\n\nHowever, the RNN-based method has limitations in code comment generation. Some source codes may be very long, and the RNN-based model may not effectively capture the long-term dependence between the code tokens due to the long-term dependence problem. Thus, the model cannot recognize the relationship between the code contexts during the training process, which results in low-quality comments. In contrast to the RNN, the Transformer model leverages the self-attention mechanism to capture a wide range of dependence between texts [6]. Thus, the Transformer has shown the best performance in the research field of natural language processing. Moreover, to better utilize the advantages of neural network technology, we propose a new Transformer-based method named SeTransformer (syntaxbased Transformer). In particular, the SeTransformer improves the traditional Transformer model to simultaneously process the textual information and structural information of software code. Finally, to speed up network training, we use convolutional 0018-9529 \u00a9 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\n\nSee https://www.ieee.org/publications/rights/index.html for more information.\n\nneural networks (CNNs) to compress the dimensionality of the data after embedding tokens in the code and its AST before the model training.\n\nTo verify the effectiveness of our proposed method, we conduct the experiments on a corpus constructed from large-scale open-source Java projects, which contain 87 136 Java functions and the corresponding comments. The Javadoc's first sentence is extracted as a comment, and the comments are used to explain the design purpose and the functionality of functions. Later, we split this dataset into training sets, validation sets, and test sets by 8:1:1 in our study. To evaluate the performance of the SeTransformer, we leverage two performance metrics (i.e., BLEU [7] and MET EOR [8]) from the neural machine translation (NMT) domain. The results show that the SeTransformer can perform significantly better than five state-of-the-art baselines. Moreover, the CNN layer used in the SeTransformer can effectively reduce the training time without losing accuracy.\n\nTo the best of our knowledge, the main contributions of our study can be summarized as follows:\n\n1) We employ an advanced Transformer model and propose a novel automated code comment generation method SeTransformer, which can simultaneously process the content and structural information of program code and generate high-quality comments for programs. 2) We propose a dimensionality reduction method for text sequences, which can reduce the model's training cost without significantly decreasing the model performance.\n\n\n3) We conduct empirical studies on large-scale open-source\n\nprojects, and the results show that the SeTransformer can significantly outperform other five state-of-the-art baselines. 4) To facilitate the replication of our study and evaluation of future code comment generation techniques, we share our source code and corpus in the GitHub repository. 1 The rest of this article is organized as follows. Section II presents the background of our work. Section III introduces the framework and details of our proposed method SeTransformer. Section IV shows the experimental setup and result analysis. Section VI discusses the strength of the SeTransformer. Section VII discusses the threats to the validity of our study. Section VIII surveys the related studies and emphasizes the novelty of our study. Finally, Section IX concludes this article.\n\n\nII. BACKGROUND\n\n\nA. Languages Models\n\nOur work is based on NMT tasks in the field of natural language processing. The language models used in these tasks can determine word probability and, as a result, can generate whole sentences [5]. Similar to the language models, our proposed model SeTransformer aims to generate the code comments by estimating each word's probability. 1 [Online]. Available: https://github.com/appmlk/SeTransformer For a sentence x = (x 1 , x 2 , . . . , x n ), the probability of the sentence is calculated from the likelihood of each word generated from the past sentence, i.e., P (x) = P (x 1 )P (x 2 |x 1 ) \u00b7 \u00b7 \u00b7 P (x n |x n \u2212 1, . . ., x 2 , x 1 ). (1) Based on the above formula, various neural-network-based models were proposed to handle natural language processing problems. Recently, RNN [9], [10], Transformer [6], and CNN [11], [12] are three popularly used ones.\n\n1) Recurrent Neural Networks: The structure of the RNN is a chain, and its input data is a sequence. It can use the entire history of previous input to guide each output. Therefore, it is closely related to sequence and list, and it has unique advantages for sequence information with time relationships.\n\nHowever, the standard RNN model may suffer from the gradient explosion and disappearance problems during the training process [13], making the RNN unable to capture the long-term dependence between the code tokens effectively; thus, the network model is unable to train. Researchers proposed variants of RNNs (such as long short-term memory [14] and gated recurrent unit (GRU) [10]) and better neural network models (such as Transformer [6]) to solve this problem.\n\n2) Transformer: The Transformer is an encoder-decoder model that only relies on attention for computing the contextual representations for source and target sentences. It avoids recurring model architectures in the RNN, which can avoid the disadvantages of the RNN mentioned in Section II-A1.\n\nSince the Transformer [6] is a novel language model and superior to the RNN language model in terms of performance and time consumption on model training in the natural language processing field [9], [10], we use a Transformer-based deep learning language model to solve code comment generation problems in this article.\n\nThe Transformer model includes many components, the most critical component of which is multihead attention. Multihead attention is used to learn the relationship between each word in a sentence. The model SeTransformer proposed in this article includes all the standard Transformer components so that we will introduce them in detail in Section III-C.\n\n3) Convolutional Neural Networks: In previous practice on code comment generation, the target code may contain many program statements. Thus, the dimensionality of the input data is accordingly tremendous. Owing to a large number of neural network connections, there will be enormous parameters to be trained. However, a considerable amount of calculation will lead to huge time cost. Therefore, it is necessary to leverage the CNN to compress the input data's dimensionality, thus reducing the computational complexity.\n\nAs shown in Fig. 1, the CNN architecture consists of five components: input, convolution layers, pooling layers, fully connected layers, and output. These components can extract meaningful information from input data while ignoring the noise.\n\nIn practice, a convolution layer aims to utilize a convolution kernel to slide the kernel over an input tensor's each area with the same size as the kernel. Followed by a convolution layer is a pooling layer, which reduces the input size and, thus, boosts training speed. It keeps essential information for the next layer while scaling down input. The next layer is the fully connected layer that outputs the results. By utilizing the difference between the output and the actual target output to construct a cost function, we can then train the CNN by the backpropagation algorithm. Then, we update the model parameters by minimizing the loss between the network output and the target output. Finally, the model can learn meaningful information from input data while reducing the computational cost.\n\n\nB. Neural Machine Translation\n\nNMT [15] is an end-to-end learning method for automatic translation between different natural languages, such as English to Chinese. The most representative deep learning models of NMT are RNN and Transformer. NMT solves shortcomings such as manual design function requirements and achieves surprising promising results. It usually includes an Encoder module and a Decoder module. Here, the Encoder module is used to encode the input text sequence, and the Decoder module is used to decode the Encoder module's output and generate the corresponding text sequence.\n\nIn this article, we leverage the NMT method to train our neural network because NMT makes it possible to translate between different natural languages automatically. Meanwhile, the automatic generation of code comments is a variant of the translation problem between natural languages. For example, Hu et al. [5] proved that the NMT method could be applied to the automatic generation of code comments. Therefore, we also comply with the common sequence-to-sequence learning framework for neural network training.\n\n\nIII. OUR PROPOSED METHOD SETRANSFORMER\n\nIn this section, we present the overall framework of our proposed method SeTransformer, which is shown in Fig. 2. Our proposed method consists of two steps: data processing and Se-Transformer model training. Specifically, the data preprocessing step can extract the features of the source code. In this step, the source code will be extracted into two parts: lexical information (i.e., code tokens) and structure information (i.e., AST). Later, the SeTransformer model training step can simultaneously learn the program's lexical information and structure information. In the SeTransformer, we also use the CNN layer to reduce the length of the input features to improve the training speed. In the rest of this section, we will show the details of each component in our proposed method.\n\n\nA. Features Representation\n\nThe inputs of the SeTransformer include code tokens and its AST. Thus, the SeTransformer can learn lexical information from code tokens and learn structure information from the AST. Next, we will describe the details of code tokens and AST representation.\n\n1) Code Representation: The SeTransformer learns lexical information from source code tokens. The source code consists of keywords, operators, identifiers, and symbols. To extract each code tokens from the source code, we adopt a widely used tool javalang 2 to process the source code. Since the developers can freely define the identifiers, and these words in the identifiers usually indicate the features of a function or a variable, and so tokens' vocabulary size can be too large, we split each identifier in the source code according to the camel casing conversion and convert all code tokens to lowercase to decrease the vocabulary size. For example, the identifier \"calculateAverage\" would be split into \"calculate\" and \"average.\"\n\nWe also use positional embedding [16] to show tokens position within the sequence. Because the tokens, ASTs, and comments of code are sequential data, the order relationship between words usually affects the entire sentence's semantic. However, the self-attention layer of the Transformer model does not contain position information, which means that the Transformer model ignores the position information of each part of the input sequence. Therefore, in order to utilize the location information to the training, we need to construct the location information into a sequence with the same dimension as the input sequence (such as tokens, ASTs, and comments) and then add it to the input sequence to obtain new input sequence. Finally, we send the new sequence into the Transformer model so that the position information can participate in the training.\n\n2) AST Representation: The SeTransformer learns structure information from the AST. The AST is an abstract representation of the syntax structure of source code. Previous studies [17], [18] have proven that the AST is one of the essential features in source code analysis.\n\nThe AST has a tree structure, which cannot be directly used in neural network training. Hu et al. [5] put forward an SBT method to traverse ASTs. SBT uses brackets to represent the AST structure and can restore a tree unambiguously from an SBT sequence.\n\nHowever, the sequence generated by SBT is relatively long. To reduce the sequence's scale while retaining the input features, we slightly improve SBT, as shown in Fig. 3. In particular, we first apply the SBT method to traverse the ASTs to get the SBT sequence. Then, we replace the brackets with the serial number of the preorder traversal sequence. Finally, we divide the SBT sequence into two sequences: the node sequence and the preorder traversal serial number sequence. We utilize the node sequence as input embedding and employ the serial number as the positional embedding. Specifically, we first embed the node sequence and the number sequence as vectors and then add the two vectors together as input data for the neural network.   \n\n\nB. Convolutional Neural Networks\n\nThe multihead attention layer's memory and computational requirements grow quadratically with sequence length [19]. When multihead attention processes a sequence of length n, its time complexity is O(n 2 ). Therefore, the Transformer model has difficulty in handling long text sequences.\n\nInspired by the previous studies on image processing, the CNN can reduce the size of the image (e.g., length and width) and reduce the amount of calculation [20].\n\nIn this article, we consider the CNN, which is mainly used to shorten the input sequence's length, as shown in Fig. 4.\n\nWe first use a 3 \u00d7 1 convolution kernel to extract the features of adjacent characters. Then, to shorten the input sequence's length and maintain the input vector dimension, we use 2 \u00d7 1 filter to do Maxpooling for downsampling. After this series of operations, the input data dimensions can be effectively reduced while retaining the data features.\n\n\nC. SeTransformer Structure\n\nFig. 5 illustrates our SeTransformer model. Different from the traditional Transformer described in Section II-A2, we improve the original neural network structure so that the neural network model can simultaneously input plain code text data and AST data.\n\nThe detailed introduction of each component is illustrated as follows.\n\n1) Multihead Attention: We construct the multihead attention component according to the standard Transformer and reimplement its calculation process [6]. Since it can compute on the same sequence, we can capture the correlation within the sequence, and this function can be called self-attention.\n\nThe particular attention in Transformer is \"scaled dot-product attention.\" The input of scaled dot-product attention consists of where \u03b1 ij is a weight coefficient. W V , W Q , and W K are the parameters that are unique per layer and attention head. After the operation of multihead attention, we can extract the internal correlation features of input sequences.\nqueries (x i W Q ), keys (y j W K ), and values (y j W V ). For each attention head, source sequence x i (where x i \u2208 R d model ) and target sequence y j (where y j \u2208 R d model ) can be transformed into the output sequence o i , where o i \u2208 R d k . Thus, we compute the outputs o i as o i = m j=1 \u03b1 ij (y j W V )(2)\u03b1 ij = expe ij m k=1 expe ik (3) e ij = (x i W Q )(y j W K ) T \u221a d k(4)\n\n2) Residual Connections and Batch Normalization:\n\nResidual connections [21] are mainly used to solve the problem of deep neural network degradation. Batch normalization [22] keeps the input of each layer of the neural network in the same distribution during the deep neural network training process.\n\nEspecially, following the previous study [6], we add a layer of residual connections and batch normalization after each layer of multihead attention layer and fully connected layer. This kind of component is labeled as \"Add&Norm\" in Fig. 5.\n\n3) Encoder: Previous studies usually independently handle coded AST and code tokens in the Encoder component [23], [24]; they did not reflect the association within different features of a program. However, we found some relationship between the AST and code tokens. For example, the leaf nodes of the AST are identifiers or operators in code tokens. To learn the relationship between the AST and code tokens, we use multihead attention to encode the AST and code tokens jointly.\n\nThe input of the Encoder includes SBT with serial number information and code with position information. First, to learn the correlation within SBT nodes, we use the SBT sequence as the source sequence and the target sequence for multihead attention. Then, apply code tokens to perform the above operation again to understand the correlation within code tokens. Besides, to acquire the relationship between the AST and code tokens, we input the SBT sequence as the source sequence and the code sequence as the target sequence into multihead attention, plus using code as the source sequence and SBT as the target sequence. Next, we also leverage residual connections, batch normalization, and full connection to complete this neural network contract. As shown in Fig. 5, we utilize a three-layer Encoder network. Finally, the Encoder outputs the encoded SBT and code vectors and then input them into the Decoder.\n\n\n4) Decoder:\n\nThe Decoder's input includes partial comments with location information, encoded SBT sequences, and code token sequences. We describe the Decoder based on the Transformer [6].\n\nFirst, to learn the relationship between comments words, we use the comment sequence as the source sequence and the target sequence for masked multihead attention. In particular, to ensure that the predictions for position i can depend only on the known outputs at positions less than i, we masked multihead attention to hide the words behind each word.\n\nAfter that, we implement two more multihead attention to decode code information and AST information. One uses the comment sequence as the source sequence and the code sequence as the target sequence. Another uses the comment sequence as the source sequence and the SBT sequence as the target sequence. Next, we also leverage the residual connections, batch normalization, and full connection. Finally, Softmax outputs the probability of each word through the Softmax layer.\n\n\nIV. EMPIRICAL SETUP\n\n\nA. Research Questions\n\nTo evaluate the effectiveness of our proposed method Se-Transformer, we conduct both a large-scale empirical study and a human study to evaluate the performance of the SeTransformer. The designed research questions are introduced as follows.\n\nRQ1: Can SeTransformer outperform state-of-the-art code comment generation baselines?\n\nWe design this RQ to verify the effectiveness of the Se-Transformer for code comment generation. To answer this RQ, we conduct a set of empirical studies and compare the Se-Transformer with five state-of-the-art code comment generation baselines, including DeepCom [5], Hybrid-DeepCom [24], AST-attendgru [23], Dual Model [25], and Full Model [26].\n\nRQ2: How does code and comment length affect the performance of our proposed method SeTransformer?\n\nWe design this RQ to confirm that our method can achieve the best effectiveness under different corpus situations. In this RQ, we intend to analyze the source code and comment length on the SeTransformer method's effectiveness. To answer this RQ, we collected and analyzed the experimental results when using SeTransformer to generate comments for different source code lengths or code comment lengths.\n\nRQ3: What effect does CNN operation have on the performance of the SeTransformer?\n\nWe designed this RQ to analyze the influence of the CNN in the SeTransformer. To answer this RQ, we compared the performance of the SeTransformer and its model training time when using CNN and not using CNN. \n\n\nB. Corpus and Preprocessing\n\nWe conduct our experiments on a Java corpus [27]. Table I shows the statistics of this corpus. This corpus was collected from GitHub, which contains 87 136 pairs of <method, comment>. The dataset is split into training, test, and validation sets by 8:1:1 based on the suggestion by Hu et al. [27]. We preprocessing the dataset following the studies of Wei et al. [25], Ahmad et al. [26], and Hu et al. [27]. The first sentence of Javadoc is extracted as a code comment, and the code comment is used to explain the function of the Java method. Besides, identifiers in the Java method are split via camel casing 3 to alleviate the out-of-vocabulary problem.\n\n\nC. Metrics\n\nWe use two machine translation metrics (i.e., BLEU and METEOR) to evaluate the performance of the SeTransformer. These metrics have been widely used in the field of NMT [28] and have now been widely used in the current studies of code comment generation [25], [26].\n\n1) BLEU :BLEU [7] is a machine translation metric that we use to evaluate the performance of our proposed method SeTransformer. The score of BLEU represents the similarity between the generated sequence and the reference sequence. The percentage of BLEU scores ranges from 0 to 100%, and a higher BLEU score indicates a more similar generated sequence according to the reference sequence. If two sequences are the same, the BLEU score is 100%. If two sentences do not have the same word, the BLEU score is 0. Recent studies [5], [23], [26] already have widely used BLEU scores to evaluate the quality of code comments. 2) METEOR:METEOR [8] is also a machine translation metric. It evaluates the generated sequence by aligning the generated sequence with the reference sequence and calculating the sentence-level similarity score. METEOR employs WordNet 4 to calculate the matching relationship between specific sequences, synonyms, roots, affixes, and definitions, which can be regarded as the supplement of BLEU . 3 For example, the identifier \"hashCode\" can be split into \"hash\" and \"code\" and the identifier \"isAgentEmpty\" can be split into \"is,\" \"agent,\" and \"empty.\" 4 [Online]. Available: https://wordnet.princeton.edu/\n\n\nD. Baselines\n\nWe compared the SeTransformer with five baselines, which achieved state-of-the-art performance in the code comment generation field [24], [26]. The details of these baselines are summarized as follows:\n\n1) Baseline 1: DeepCom [5] formulates the code comment generation task as a machine translation task and uses an attention-based Seq2Seq model to generate comments of Java methods. DeepCom uses the SBT method to convert these ASTs into sequences and takes the SBT sequence as the model input. To compare the performance of SeTransformer and DeepCom, in our study, we reran the code of DeepCom. 2) Baseline 2: Hybrid-DeepCom [24] is an extended version of DeepCom, and its performance is better than DeepCom. Hybrid-DeepCom uses a variant of the attention-based Seq2Seq model to generate comments for Java methods. Hybrid-DeepCom combines the source code and the traversed AST sequences to generate the comments. Our study reran the code of Hybrid-DeepCom mode to perform the comparison. 3) Baseline 3: AST-attendgru [23] involves two unidirectional GRU layers: one is used to process the words from source code, and the other is designed to process the AST. AST-attendgru first uses an attention mechanism to associate the words in the output comments with the words in the code text and then use a different attention mechanism to associate the comments words with each part of the AST. In our study, we reran the code of AST-attendgru to perform the comparison. 4) Baseline 4: The Dual Model [25] is a dual learning framework to train code generation and code summarization models simultaneously to exploit the duality of them. To strengthen the duality, Dual Model adopts a new constraint on the attention mechanism. In our study, we directly use the experimental results of the corresponding study [25] to perform a comparison. 5) Baseline 5: The Full Model [26] explores the Transformer model that uses a self-attention mechanism and has shown to capture long-range dependence effectively. They additionally added extra copy attention to the decoder stack to learn the copy distribution, which can allow the Transformer to copy unusual tokens from the source code (e.g., function names and variable names) [29] and, therefore, improve the original Transformer. To conduct the comparison, we reran the code of Full Model in accordance with the corresponding study [26].\n\nWe employ the original RNN and Transformer as baselines to evaluate the performance of our proposed method. Ahmad et al. [26] also employed the original RNN and Transformer. Then, they conducted experiments on the same dataset as our study. Therefore, we directly use their experimental results to make the comparison.\n\nNote that the methods in which we directly use the original results (i.e., Dual Model, original RNN, and original Transformer) also use the same dataset and data split method as this article.\n\nThus, we can directly compare their results in this article's comparison.\n\nBesides, because the experiments in the papers of DeepCom, Hybrid-DeepCom, and AST-attendgru were conducted on a different dataset than the one used in our study, we must rerun their code on the dataset used in our article to complete the comparison. This also explains why the results of these methods in this article differ from their original paper.\n\n\nE. Hyperparameters\n\nWe followed Hu et al. [24] by replacing the constant numbers and strings in the source code with special tags <num > and < str>, respectively. We found that about 86% of code comments are less than 30 words in this corpus, approximately 89% of Java methods are less than 200 tokens, and 93% of SBT sequences are less than 300 tokens. Therefore, the maximum length of code sequence, ISBT sequence, and code comment is set to 200, 300, and 30, respectively. We add two special tokens <start > and < eos> for each comment. <start > means the start of the comment, and < eos> means the end of the comment. The vocabulary sizes of the code tokens, SBT tokens, and comments are set to 30 000, 30 000, and 23 428, respectively. Finally, out-of-vocabulary tags will be replaced by <unk>.\n\nBesides, during training, the model is validated every 5000 minibatches on the validation set by BLEU metric, and the maximum number of minibatches is 500 000, which means that the reading of samples from the training dataset will be repeated recursively 500 000 times before the training process is terminated.\n\nThe hyperparameters of the neural network model are essential factors that affect the performance of the model. Our model has undergone multiple hyperparameter adjustments and finally selected an optimal parameter combination. To ensure the fairness of our parameter settings, we only use the training set and the validation set to adjust the hyperparameters. The hyperparameters of our model are set as follows:\n\n1) We use the Adam algorithm [30] to train the parameters, and the minimum batch size (i.e., the number of samples selected from training examples for one training) is set to 32 due to the GPU memory limitation. 2) The hidden size is set to 768, and the embedded word has 768 dimensions. We future discuss this parameter setting and comparison in Section VI-A. 3) We use a three-layer Encoder block and a three-layer Decoder block. The multihead attention setting has eight heads. Feedforward has 2048 neurons. These parameters are optimized by the validation set. 4) We train the Transformer model using the Adam optimizer [30], and the initial learning rate is set to 1e-4. The learning rate is decayed using the rate of 0.99. 5) We use the dropout strategy [31] during the training process and set dropout probability to 0.8. This parameter's value is optimized based on the validation set. 6) The SeTransformer uses cross-entropy minimization as the cost function by referencing the work of Hu et al. [24]. \n\n\nF. Statistical Analysis 1) Hypothesis Testing Method:\n\nWe use the Wilcoxon signedrank test to further assess the trial findings since we conduct experiments between our proposed method and existing baselines on the same data with the same code and comments.\n\nThe Wilcoxon signed-rank test [32] is used as an alternative hypothesis test when the test data cannot be assumed to be normally distributed. As a result, it can provide a solid statistical foundation for comparing the effectiveness of various methods.\n\n2) Effect Size: An effect size is a quantitative measure of the strength of the association between two variables in a population or a sample-based estimation of that quantity in statistical analysis. To quantify the magnitude of difference between the two groups, we calculate the Cliff's Delta [33], which is a nonparametric effect size measure. Specifically, we use the Cliff's Delta to quantify the difference in terms of BLEU or MET EOR metrics between the SeTransformer and other baselines.\n\nFurthermore, the effect size classifies Cliff's Delta values of less than 0.147, between 0.147 and 0.33, between 0.33 and 0.474, and above 0.474 as negligible, small, medium, and large, respectively.\n\n\nV. RESULT ANALYSIS\n\n\nA. Result Analysis for RQ1\n\n\nRQ1: Can SeTransformer outperform state-of-the-art code comment generation baselines?\n\nWe first use two machine translation metrics (i.e., BLEU score and METEOR) to measure the difference between automatically generated comments and manually written comments and then employ five state-of-the-art baselines to verify the performance of the SeTransformer. Table II lists the overall results of the SeTransformer model and the five state-of-the-art baselines. In Table II, it is not hard to find that the SeTransformer outperforms all the other five baselines. More specifically, the SeTransformer improves by 4.53-8.90% on BLEU compared to state-of-the-art baselines and improves by 4.63-8.16% on METEOR.\n\nThe SeTransformer extracts lexical information, and grammatical information then encodes them jointly in the Encoder. In comparison, the shortcomings of these baselines are summarized as follows:\n\n1) The Dual Model and the Full Model only use lexical information but ignore grammatical information. 2) DeepCom only utilizes grammatical information but ignores lexical information. 3) Hybrid-DeepCom and AST-attendgru use lexical information and grammatical information. However, the grammatical information and lexical information are separately coded in the model's Encoder. Thus, the relationship between lexical and grammar is ignored. Note that the results in terms of BLEU and METEOR metrics in the Full-Model-related paper are 44.58 and 26.43%, respectively, and the slight differences are caused by random factors.\n\nThe experimental results show that the SeTransformer achieves the best performance and proves the importance of lexical information and grammatical information as feature information in this research field. Therefore, these two features should be considered simultaneously in the encoding process.\n\nMoreover, after further analysis of the performance between five state-of-the-art baselines in Table II, we can achieve the following findings.\n\n1) The performance of Hybrid-DeepCom is better than Deep-Com, which proves that grammatical information used in DeepCom cannot generate high-quality comments. 2) Hybrid-DeepCom is superior to ASTattendgru, which demonstrates that the generation of code comments is suitable for the encoder-decoder framework.\n\n3) The Dual Model is better than Hybrid-DeepCom, which shows a correlation between the code generation task and the comment generation task. 4) The Full Model can achieve the best performance among the five baselines, which shows that the Transformer model is better than the RNN model. Furthermore, we leverage the Wilcoxon signed-rank test to verify the competitiveness of our proposed method SeTransformer. The results of the hypothesis testing are shown in Table III. Owing to the reason that we only implemented four baseline methods, we can only conduct hypothesis testing on these four baselines. The following is the hypothesis used in our study, H 0 : In terms of BLEU and MET EOR, there is no significant difference between the SeTransformer and the other method. This test's significance threshold is set to 0.05. Because all of the p-values in Table III are less than 0.05, the null hypothesis was rejected by the statistical results. These results indicate that in terms of BLEU and MET EOR metrics, the performance of our proposed method differs significantly from that of these baselines. Since the results in Table II show that our method outperforms other baselines, we can conclude that the SeTransformer can achieve significantly better results than those of other baseline methods. Cliff's Delta is also used to assess the difference in terms of BLEU and MET EOR metrics between the SeTransformer and other baselines. The Cliff's Delta values are less than or equal to 0.25, which equates to a negligible or small effect size. As shown in Table IV. The results demonstrate that our method outperforms existing baselines to a lesser extent.\n\nSummary for RQ1: Experimental results show that the performance of the SeTransformer is better than that of the five state-of-the-art baselines and two basic baselines. It proves that lexical information and grammatical information are essential features in the code comment generation task, and researchers should consider both the features in the Encoder process of this field research.\n\n\nB. Result Analysis for RQ2\n\n\nRQ2: How does code and comment length affect the performance of our proposed method SeTransformer?\n\nCode and comment length is one of the main factors that affect the performance of the code comment generation model; therefore, we further analyze the impact of code and comment length on the performance of the SeTransformer. We reran three baselines, namely DeepCom, Hybrid-DeepCom, and ASTattendgru. Therefore, we use these three baselines as a comparison to evaluate the performance of the SeTransformer. Fig. 6 shows the performance of the SeTransformer and the other three baselines under different lengths of code and comments.\n\nTo make the results of Fig. 6 more concise and significant, we take the approximate length of the code to study the model's performance. We calculated the approximate length as follows:\nF (x) = 300, x \u2265 300 ( x/10 + 1) * 10, x < 300(5)\nwhere F (x) is the length of the code shown on the x-axis of Fig. 6, and x is the original code length. Fig. 6(a) shows the impact of code length on the performance of the four models on the BLEU metric. It can be seen that all models perform very poorly when the code length is short; this is because these short code may be incomplete. When the code length is larger than 200, all models are unstable; this is because developers will crop too large code; therefore, this kind of sample size is small, which cannot reflect this interval's true situation. From Fig. 6(a), we can find that our model's performance is the best. Compared with DeepCom, we notice that except for the first point, the BLEU of the SeTransformer is higher than DeepCom at other points. Among the four models, the SeTransformer can achieve the best performance at 22 points, with a ratio of 73.33%.   Fig. 6(a), when the code length is short, the performance of the model is low, and when the code length is greater than 200, the performance of the model becomes unstable. From Fig. 6(b), we can find that our model's performance is the best. Among the four models, the SeTransformer can achieve the best performance at 20 points, with a ratio of 66.67%. Fig. 6(b) and (d) shows the impact of code length on the BLEU and METEOR metrics of the four models. It can be seen from the figure that our model achieves the best results on both BLEU and METEOR. Among the four models, the SeTransformer can achieve the best performance at 30 points on both BLEU and METEOR, with a ratio of 100%. Besides, the three baselines models' performance decreases significantly when the comment length exceeds 20. In other words, when faced with the long-term dependence problem, the SeTransformer can still achieve the best performance compared with the other three baselines.\n\nIn addition, we use the Wilcoxon signed-rank test to verify the competitiveness of our proposed method. In other words, we utilize statistical analysis to determine whether our method's polyline in Fig. 6 is significantly higher than that of other methods. The hypothesis used in this section is specifically specified as follows, H 0 : In terms of BLEU and MET EOR, there is no significant difference between SeTransformer and the other methods in terms of various code lengths or comment lengths. Table V shows the p-value of H 0 for four different methods using the Wilcoxon signed-rank test. Because the p-value in most cases is less than 0.05, the preceding results that the SeTransformer can produce significantly superior performance than DeepCom,  Table VI show that, in the majority of cases, the SeTransformer outperforms alternative baselines in terms of BLEU and MET EOR metrics for different code and comment lengths.\n\nSummary for RQ2: When considering the impact of code and comment length on the model, the performance of the SeTransformer is better than that of the other three baselines. Besides, the experimental results also show that when the code is too short, the SeTransformer performs poorly because the source code with a shorter length is incomplete, which will affect the performance of the code comment generation method.\n\n\nC. Result Analysis for RQ3\n\n\nRQ3: What effect does CNN operation have on the performance of the SeTransformer?\n\nWe add a CNN layer to compress the dimensionality of the input data before the input data enters the Transformer model. In this section, we will analyze the impact of the CNN on the performance of the SeTransformer.\n\nNote that regardless of whether the neural network structure contains the CNN, the training process must be completed in its entirety and will not be aborted. The CNN's focus on reducing training time is reflected in each feedforward and feedback operation of the neural network, rather than limiting the number of training times, thereby reducing the total time required for training.\n\nTo study CNN's performance on the SeTransformer, we compare the SeTransformer with CNN (used in the previous experiment) and the SeTransformer without CNN. Table VII shows the final comparison results. We found that the SeTransformer with CNN and the SeTransformer without CNN are very similar in BLEU and METEOR metrics, which shows that adding a CNN layer to compress the dimensionality of the input data will not affect the performance of the model. However, in terms of training time, the SeTransformer without CNN takes 3745 min to train a model, but the SeTransformer with CNN only takes 2344 min, which shortened training time by 37.41%. Compared with the SeTransformer without CNN, although the SeTransformer with CNN has one more CNN layer network, the training time of the SeTransformer is shorter, which shows that using CNN to compress the data dimension can reduce the training time.\n\nTo verify whether there are significant differences in comment generation performance after using CNN, we use the Wilcoxon signed-rank test. In particular, we want to know if the evaluation metrics' values have decreased significantly after using CNN. First, the confidence level is set to \u03b1 = 0.05, and we define the null hypothesis (H 0 ) as that the performance of the SeTransformer without CNN is significantly better than the SeTransformer with CNN in terms of BLEU and METEOR metrics.\n\nFinally, the p-value of the Wilcoxon signed-rank test is 0.00001 in terms of BLEU metric, which is less than 0.05; therefore, we reject H 0 and accept H 1 . That is, the performance of comments generation is not significantly reduced after using CNN in terms of BLEU metric. For METEOR metric, the p-value is 0.10196, which is larger than 0.10196; therefore, H 0 is adopted in this case. That is, the two approaches produce similar results.\n\nSummary for RQ3: Using our proposed CNN layer in the SeTransformer can save 37.41% of the training time. The performance of the SeTransformer model would not be significantly reduced only in terms of BLEU metric.\n\n\nVI. DISCUSSIONS\n\n\nA. Impact of Hidden Size\n\nThe hidden size is an essential parameter of neural networks. It significantly impacts the performance of the trained neural network models. We performed a sensitivity analysis by adjusting the size of the neural network's hidden layer. To ensure a fair comparison of the experiments, we set the same value for the parameters except for the hidden size, and the results are presented in Table VIII.\n\nAs shown in Table VIII, the value of BLEU and METEOR generally increases with the increase in the hidden size, which means that expanding the scale of hidden size can effectively improve the performance of the SeTransformer.\n\nHowever, we also observed that the growth rate of BLEU and METEOR becomes smaller with the hidden size increase to a certain value. In particular, although BLEU can reach the highest value when hidden size is 768, the value of METEOR slightly reduces. Therefore, we set the scale of hidden size to 768 in our experiment.\n\n\nB. Human Evaluation\n\nIn this section, we conduct a manual evaluation on the quality of comments generated by the Full Model and the SeTransformer because, sometimes, the automated evaluation formula is not equal to the real evaluation of developers. Therefore, we conducted a human survey to evaluate the automated comment generation method by real developers. We hired four volunteers to participate in our manual evaluation (including undergraduates, masters, and Ph.D. students), each with two to five years of programming experience and large-scale project development experience.\n\nTo balance efficiency and experimental credibility, we used algorithms commonly used in sampling survey research [34] to calculate the number of comments that need to be extracted for evaluation. As described in Section IV-B, 87 136 Java programs are used in this experiment; it is almost impossible for the volunteers to mark all of them. The calculation formula of the sample size MIN is computed as follows:\nMIN = n 0 1 + n 0 \u22121 populationsize (6) n 0 = Z 2 \u00d7 0.25 e 2(7)\nwhere populationsize indicates the number of code comments, Z means the confidence level, and e is the error margin. In our experiment, we set the value of Z to 95% and the value of e to 0.05. Then, the calculated value of MIN is 384. We invite five volunteers with extensive development expertise to provide feedback for our comparison. We followed the experimental design principles 5 and conducted a within-subject experiment, as each volunteer will respond to the same questions under the same conditions.\n\nWe randomly chose 384 pairs of prediction outcomes and their references from the test set. The questionnaire contains 384 pages, and each page contains an input source code, comments generated by the SeTransformer and the Full Model, and a hand-written reference comment. We send each volunteer a copy of the 384-page questionnaire and invite them to evaluate two comments for each code. Additionally, to guarantee fairness, we randomly rank the comments created by the two methods on each page and remove their tags to ensure that the volunteers cannot know whether the comments are generated by the Full Model or the SeTransformer. During the manual evaluation, volunteers can resort to search engines (such as Google) for related material and unfamiliar concepts.\n\nTo enable volunteers to assess the quality of generated comments from a variety of perspectives, we adopt Gao et al.'s approach of considering two perspectives: naturalness and relevance [35]. Naturalness relates to the grammatical accuracy and fluency of generated comments, i.e., whether the content of a comment is easily readable and understandable by humans. Relevance relates to the relationship between the generated comments and the input code, i.e., whether humans can deduce the code's design intent from the corresponding comment. Fig. 7 depicts one page of our questionnaire on which volunteers should read the input code, reference comment, and two generated comments. Then, the volunteers should grade the naturalness and relevance of the two generated comments by using a scale of 1-5 (5 is the best).\n\nFinally, we calculate the mean results of the five volunteers' feedback, as shown in Table IX. For instance, the number 3.79 in the second row and the second column indicates that the Full Model's average relevance score is 3.79. It is discovered that SeTransformer's average naturalness and relevance scores outperform those of the Full Model by 0.19 and 0.16 points, respectively, which means that the volunteers prefer the comments generated by the SeTransformer. Besides, we employ Cliff's Delta [33] to evaluate the difference in naturalness and relevance of comments between the SeTransformer and the Full Model. Cliff's Delta values for naturalness and relevance are 0.11 and 0.09, respectively, indicating that the SeTransformer has a weak advantage when compared with the Full Model.\n\n\nC. Explanation of How the SeTransformer Generates Comments\n\nThe attention map in Fig. 8 shows an example of how the SeTransformer generates a comment. Specifically, the tokenized input is shown on the y-axis, while the comment generated output is displayed on the x-axis. The attention map represents the relationship between the input code tokens and the generated comments. The color range of the color block on the right side of the image is light (light blue) to dark (dark blue). The deeper the color, the higher the correlation degree of the token.\n\nThis attention map can help us understand how the SeTransformer generates specific words for input tokens. For example, the words \"renderer\" in comments are generated because of the tokens \"renderer\" in inputs (labeled 1, 3, and 4 in Fig. 8), which means that the SeTransformer can capture the keywords in the input tokens and keep them in the output comments. Besides, the plural word \"multiple\" in comments is generated because the \"renderers\" in the input tokens is plural (labeled 2 in Fig. 8), which means that the SeTransformer can capture the difference between singular nouns and plural nouns and present them in the output comments. Therefore, this example can explain how the SeTransformer can generate correct comments.\n\nTo further investigate the effect of code features on SeTransformer performance, we attempted to conduct qualitative study by analyzing the training data and output comments.\n\nThree different cases we chose to analyze are listed as follows: 1) Perfect case: The manual comment in this case is \"get a sorted array containing all column values for a given tuple iterator and field,\" and the comment generated by the SeTransformer is exactly the same as the manual comment. We counted the number of occurrences of the key phrases \"get a,\" \"a sorted,\" and \"sorted array\" in the training set as 317, 37, and 26, respectively. Therefore, the neural network trained by this training set will be familiar with the generation process of this series of phrases. 2) Medium case: The manual comment for this case is \"compute the union size of two bitsets,\" while the comment generated by the SeTransformer is \"compute the intersection size of two bitsets.\" It can be seen that the difference between the two comments appears in the middle part of the sentence. We counted the number of occurrences of the phrase \"compute the union\" in the training set only once, while the number of occurrences of \"compute the intersection\" was nine. It can be seen that the imbalance of the data causes the neural network to deviate when judging the two words \"union\" and \"intersection.\" 3) Poor case: According to our observations, cases with poor performance are generally wrong in the first word, which ultimately results in a huge difference between the entire sentence and the manual comment. For example, the beginning of a human comment is the word \"ignorable,\" and this word only appears twice as the beginning of a sentence in the entire training set. The SeTransformer predicts the beginning of this sentence as \"the,\" which occurs as the beginning of a sentence 666 times. This imbalance caused the neural network to forget \"ignorable,\" and in the end, the deviation of the first word led to errors in the entire comment generation process. From the above analysis, we think the balance of the dataset plays a crucial role in the effectiveness of the SeTransformer. Therefore, we will apply a more balanced dataset to our method in our future work.\n\n\nD. Impact of the Encoder and Decoder's Connection Mode\n\nAs illustrated in Fig. 5, the default connection method for the encoder and the decoder in the SeTransformer proposed in our study is to transfer the code token information first, followed by the SBT information. To determine the influence of this order on SeTransformer's effectiveness, we reverse the order of data transfer (i.e., transfer SBT information first, followed by code token information) and reconduct the experiment. Finally, Table X summarizes the experimental results.\n\nIn Table X, the term \"Readjusted SeTransformer\" refers to the neural network structure that alters the order in which data are transferred between the encoder and the decoder. As can be found in Table X, the effect of the readjusted SeTransformer is no worse than the original SeTransformer. Therefore, the connection mode of the encoder and the decoder in the SeTransformer does not lead to performance loss.\n\n\nE. Impact of CNN Convolution Kernel Size\n\nOur proposed SeTransformer incorporates a CNN component prior to the encoder, lowering the dimensionality of the input  XI  IMPACT OF THE CNN SIZE ON THE PERFORMANCE OF THE SETRANSFORMER   TABLE XII  IMPACT OF DATA TYPE ON THE PERFORMANCE OF THE SETRANSFORMER data and increasing the neural network's training speed. A larger convolution kernel results in a better compression ratio, which allows for a reduction in the dimensionality of the input data. A well-chosen convolution kernel can lower the dimensionality of the input data while retaining its features. However, an excessively large convolution kernel will cause the input data to lose its original information.\n\nIn order to study the influence of the size of the convolution kernel on the SeTransformer, we set the convolution kernel from the original 3\u00d71 to 5\u00d71, 10\u00d71, and 20\u00d71 and then conducted experiments. The experimental results are shown in Table XI.\n\nAs shown in Table XI, the SeTransformer performs almost the same with different values of CNN's convolution kernel size.\n\n\nF. Impact of Data Type\n\nTo investigate the influence of input data type on our proposed SeTransformer, we conducted an experiment in this subsection. Specifically, we train the SeTransformer using only code tokens or SBT sequences. When one data type is used, the other input is a sequence of zeros. Table XII summarizes the experimental  results. As shown in Table XII, the SeTransformer can achieve the best performance when two types of data are used together. Therefore, this experiment demonstrates that combining two types of data together can increase the SeTransformer method's performance.\n\n\nVII. THREATS TO VALIDITY\n\nIn this section, we discuss the potential threats to our study.\n\n\nA. Internal Validity\n\nOne threat to internal validity comes from possible errors in our experimental program code. To avoid this problem, we carefully checked the code and conducted a small-scale test before the formal experiment. Besides, we implemented our neural network model based on the well-known open-source machine learning platform TensorFlow 6 to ensure the neural network's correct operation.\n\nBesides, the replication error may cause the results of the baseline method in this article to be inaccurate. To mitigate this threat, we used the same parameter settings as the studies they conducted.\n\nAnother threat to internal validity is that the performance of our method may depend on the hyperparameter configuration. In this article, hyperparameter settings mainly come from validation set optimization and previous studies, which is discussed in Section IV-E.\n\nIn addition, the internal validity of the SeTransformer is threatened by the unpredictability of how the connection between the Encoder and the Decoder influences the experimental outcomes. In this article, we complete the network model by sequentially inputting two types of data into the Decoder. However, there are additional implementations of joint learning that are possible (e.g., using a simple merged feedforward network or a bilateral neural network). To address this threat, we intend to incorporate more joint learning approaches into our future work to go deeper into discovering improved connections between the Encoder and the Decoder and the causes for their effect differences.\n\n\nB. External Validity\n\nThe external validity relates to the corpus we collected for our experiment. This corpus was gathered from many opensource Java projects, and their comments from GitHub refer to previous comments generation studies [25]- [27]. Although the previous research has removed the corpus's noises, the corpus contains the same comment function pairs that could not match because of programs' rapid updations. In the future, we want to collect more programs with higher quality comments for experiments.\n\n\nC. Construct Validity\n\nThe construct validity relates to the suitability of the evaluation metrics used in our study. We utilize two evaluation metrics, namely BLEU and METEOR, because these metrics have been widely used in previous NMT and natural language process domains [25], [26], [28].\n\n\nVIII. RELATED WORK\n\n\nA. Code Summarization\n\nCode summarization improves the comprehensibility of the source code by generating alternative natural language descriptions for the source code. Code summarization methods can be divided into two categories: template-based code summarization [2], [36]- [40] and artificial intelligence (AI)-based code summarization [23], [24], [41], [42].\n\nTemplate-based automatic generation of code comments is the earliest automatic generation method of code comments, which can use various intermediate information extracted to generate comments for the source code. The software word usage model (SWUM) [?] is a technique for finding the part of speech of words in the code. Sridhara et al. [1] adapted the SWUM to use templates to create short comment phrases for the source code. Dawood et al. [43] defined some templates of program structure information and used the templates to generate comments for the source code. For example, a template defines the number of interfaces in the package or the parameter types used by methods. Wang et al. [44] use natural language processing to identify actions, topics, and auxiliary parameters to fill in templates and some basic information.\n\nWith AI technology development, AI-based code comment generation methods have become more and more popular. AIbased code comment generation methods are similar to machine translation, using neural network technology to generate code comments. Iyer et al. [4] were the first to try to use neural network technology to generate code comments and proved that neural networks could be used in the field of code comment generation. They developed a new AI-based code comment generation method called CODE-NN, which uses the RNN with an attention mechanism to generate natural language descriptions for C# code snippets and SQL queries. CODE-NN uses the source code as plain text as the input of the neural network. Hu et al. [5] propose an SBT method to traverse ASTs and generate SBT sequence. They regard the code comment generation task as a machine translation task and propose a novel code comment generation method called DeepCom. This method takes SBT as input. Hu et al. [24] proposed Hybrid-DeepCom to automatically generate natural language descriptions for the java method, which is an extension of DeepCom. Compared with DeepCom, Hybrid-DeepCom adds lexical information to the input. Moreover, it proves that lexical information is helpful to the generation of code comments. Wei et al. [25] argue that code generation and code summarization are related to each other and joint training of two models can learn this relationship. Therefore, they designed a dual learning framework to train both the code summarization and code generation models to take advantage of their duality. Wei et al. [41] argue that neural code comments generation methods tend to generate high-frequency words. Therefore, they concluded that the neural model was not sufficient to generate comments only based on the source code. And they combine information retrieval and neural network technology to propose a comment generation framework, namely Re2Com. Ahmad et al. [26] propose a Transformer-based method to generate natural language descriptions for java methods. This method uses the Transformer model to learn the order of tokens in a sequence or model the relationship between tokens. LeClair et al. [45] developed a method for generating comments that use a graph-based neural architecture that is more comparable to the default structure of the AST. Wang et al. [46] presented a novel strategy for generating code comments that incorporates numerous code features, such as type-augmented ASTs and program control flows, and the experimental results outperform existing approaches.\n\nWe introduced an AI-based code comment generation method called SeTransformer. Unlike the previous method, the SeTransformer uses the Transformer framework to learn the semantic information of the code. The empirical results also verify the effectiveness of our proposed method.\n\n\nB. Language Models for the Source Code\n\nThe language model is a formal system. The objective facts of the language can be automatically processed by the computer after being described by the language model. Therefore, the language model is of great significance to the information processing of natural language. In recent years, the language model for the source code has been successfully applied to many software engineering tasks, such as code comment generation [26], clone [47]- [49], code generation [17], [50], [51], and defect prediction [52]- [54].\n\nHindle et al. [55] first tried to model the language model of the source code and proved that the model did capture the advanced statistical laws of the software at the n-gram level. Allamanis et al. [56] propose a framework called NATURALIZE to solve the problem of local convention coding convention reasoning. The framework can provide some suggestions to improve the style consistency of the code base. NATURALIZE can be applied to rule-based formatter inference rules. Mou et al. [57] proposed a tree-based CNN framework called TBCNN, which is based on the AST of the program. They also put forward the concept of \"continuous binary tree.\" The TBCNN model is a general architecture and can be used in many software engineering tasks (code comment generation and clone detection). Yin and Neubig [58] propose a grammar-based neural network framework to generate code for natural language AST. This method generates the AST by applying actions in the grammar model.\n\nOur research proposes a novel code comment generation method called SeTransformer, which uses and improves the Transformer framework. This method can generate corresponding comments by learning the semantic information of the code.\n\n\nIX. CONCLUSION\n\nIn this article, we proposed a code comment generation method SeTransformer based on the Transformer neural network structure. In particular, we used the source program's lexical and grammatical information by inputting the linguistic data and the encoded AST information. Moreover, we leveraged CNN to compress the dimensionality of the data for speeding up the neural network's training process. Finally, the SeTransformer used an improved transformer model to perform encoding and decoding, thus generating useful and readable code comments.\n\nTo verify the performance of our method, we conducted a number of empirical studies on a public, large-scale, and open-source corpus. The experimental results showed that the performance SeTransformer is significantly better than that of the other five state-of-the-art baseline methods. Specifically, the SeTransformer's effectiveness led by 8.9% highest in terms of the BLEU metric and led by 8.16% highest in terms of the METEOR metric. Besides, we also conducted the questionnaire survey and the results showed that the SeTransformer can generate high-quality comments and lead the comparison method Hybrid-DeepCom by 0.35 points.\n\nIn the future, we plan to extract more features from the program's dynamic execution information and combine them with the more acceptable neural network structure to generate more accurate code comments.\n\nFig. 1 .\n1Overview of the CNN.\n\nFig. 2 .\n2Framework of the SeTransformer.\n\nFig. 3 .\n3SBT method.\n\nFig. 4 .\n4CNNs for the SeTransformer.\n\nFig. 5 .\n5Structure of the SeTransformer model.\n\nFig. 6 .\n6Performance of the SeTransformer and the three baselines under different lengths of code and comments. (a) BLEU scores for different code lengths. (b) BLEU scores for different comment lengths. (c) METEOR scores for different code lengths. (d) METEOR scores for different comment lengths.\n\nFig. 6\n6(c) shows the impact of code length on the performance of the four models on the METEOR metric. As in\n\nFig. 7 .\n7One page in the questionnaire of manual evaluation.\n\nFig. 8 .\n8Attention map for the correct comments generated by the SeTransformer.\n\nTABLE I STATISTICS\nIOF CORPUS\n\nTABLE II AVERAGE\nIIPERFORMANCE OF DIFFERENT METHODS\n\nTABLE III p\nIII-VALUE OF HYPOTHESIS IN SECTION V-A\n\nTABLE IV CLIFF\nIV'S DELTA BETWEEN THE SETRANSFORMER AND BASELINES\n\nTABLE V p\nV-VALUE OF HYPOTHESIS IN SECTION V-B Hybrid-DeepCom, and AST-attendgru are safe to accept. The Full Model does not differ significantly from our solution in this research question, indicating that the advantages of the Full Model can achieve better performance under certain code or comment lengths.In addition,Table VIcompares the Cliff's Delta between the SeTransformer and alternative techniques for various code or comment lengths using the BLEU and MET EOR metrics. The results in\n\nTABLE VI CLIFF\nVI'S DELTA UNDER DIFFERENT CODE LENGTHS AND COMMENT LENGTHSTABLE VII IMPACT OF THE CNN ON THE PERFORMANCE OF THE SETRANSFORMER\n\nTABLE VIII IMPACT\nVIIIOF HIDDEN SIZE ON THE PERFORMANCE OF THE SETRANSFORMER\n\nTABLE IX MANUAL\nIXANALYSIS\n\nTABLE X IMPACT\nXOF THE CONNECTION MODEL ON THE PERFORMANCE OF THE SETRANSFORMER\n\nTABLE\n\n[Online]. Available: https://pypi.org/project/javalang/\n[Online]. Available: https://opentextbc.ca/researchmethods/chapter/ experimental-design/\n[Online]. Available: https://www.tensorflow.org/\nHis current research interests include code generation, software testing, and deep learning testing. He is currently an Assistant Professor with the College of Information Science and Technology, Beijing University of Chemical Technology. He has authored or coauthored more than ten papers in referred journals or conferences, such as Journal of Systems and Software, Information Sciences, IEEE International Conference on Software Quality, Reliability and Security, International Conference on Software Analysis, Testing and Evolution (SATE), and International Computer Software and Applications Conference. His research interests include software engineering, in particular software debugging and software testing, such as source code analysis, mutation testing, and fault localization.Dr. Liu is a member of the China Computer Federation and the Association for Computing Machinery.Doyle Paul received the Ph.D. degree in astronomical distributed data processing from the Dublin Institute of Technology, Dublin, Ireland, in 2015.He is currently the Head of the School of Computer Science, Technological University Dublin, Dublin. He has spent more than 20 years in industry in Silicon Valley, CA, USA and Dublin. He was a Product and Quality Director of CR2, a banking software company, Dublin; a Senior Manager with Sun Microsystems, Menlo Park, CA; and a Senior Developer with a BlueStar Financial Investment. His research areas include big data processing of astronomical images, distributed systems, systems infrastructure, and educational pedagogy.\nTowards automatically generating summary comments for java methods. G Sridhara, E Hill, D Muppaneni, L L Pollock, K Vijay-Shanker, Proc. 25th. 25thG. Sridhara, E. Hill, D. Muppaneni, L. L. Pollock, and K. Vijay-Shanker, \"Towards automatically generating summary comments for java methods,\" in Proc. 25th IEEE/ACM Int. Conf. Autom. Softw. Eng., 2010, pp. 43-52.\n\nAutomatic generation of natural language summaries for java classes. L Moreno, J Aponte, G Sridhara, A Marcus, L L Pollock, K Vijay-Shanker, Proc. 21st Int. Conf. Prog. Comprehension. 21st Int. Conf. Prog. ComprehensionL. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. L. Pollock, and K. Vijay- Shanker, \"Automatic generation of natural language summaries for java classes,\" in Proc. 21st Int. Conf. Prog. Comprehension, 2013, pp. 23-32.\n\nAutomatic documentation generation via source code summarization of method context. P W Mcburney, C Mcmillan, Proc. 22nd Int. Conf. Prog. Comprehension. 22nd Int. Conf. Prog. ComprehensionP. W. McBurney and C. McMillan, \"Automatic documentation generation via source code summarization of method context,\" in Proc. 22nd Int. Conf. Prog. Comprehension, 2014, pp. 279-290.\n\nSummarizing source code using a neural attention model. S Iyer, I Konstas, A Cheung, L Zettlemoyer, Proc. 54th Annu. Meeting Assoc. Comput. Linguistics. 54th Annu. Meeting Assoc. Comput. LinguisticsS. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, \"Summarizing source code using a neural attention model,\" in Proc. 54th Annu. Meeting Assoc. Comput. Linguistics, 2016, pp. 2073-2083.\n\nDeep code comment generation. X Hu, G Li, X Xia, D Lo, Z Jin, Proc. 26th Conf. Prog. Comprehension. 26th Conf. Prog. ComprehensionX. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, \"Deep code comment generation,\" in Proc. 26th Conf. Prog. Comprehension, 2018, pp. 200-210.\n\nAttention is all you need. A Vaswani, Proc. Int. Conf. Neural Inf. Process. Syst. Int. Conf. Neural Inf. ess. SystA. Vaswani et al., \"Attention is all you need,\" in Proc. Int. Conf. Neural Inf. Process. Syst, 2017, pp. 5998-6008.\n\nBLEU: A method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W Zhu, Proc. 40th Annu. Meeting Assoc. Comput. Linguistics. 40th Annu. Meeting Assoc. Comput. LinguisticsK. Papineni, S. Roukos, T. Ward, and W. Zhu, \"BLEU: A method for automatic evaluation of machine translation,\" in Proc. 40th Annu. Meeting Assoc. Comput. Linguistics, 2002, pp. 311-318.\n\nMETEOR: An automatic metric for MT evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proc. Workshop Intrinsic Extrinsic Eval. Workshop Intrinsic Extrinsic EvalS. Banerjee and A. Lavie, \"METEOR: An automatic metric for MT evalua- tion with improved correlation with human judgments,\" in Proc. Workshop Intrinsic Extrinsic Eval. Measures Mach. Transl. Summarization, 2005, pp. 65-72.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural Comput., vol. 9, no. 8, pp. 1735-1780, 1997.\n\nLearning phrase representations using RNN encoderdecoder for statistical machine translation. K Cho, Proc. Conf. Empirical Methods Natural Lang. Process. Conf. Empirical Methods Natural Lang. essK. Cho et al., \"Learning phrase representations using RNN encoder- decoder for statistical machine translation,\" in Proc. Conf. Empirical Methods Natural Lang. Process., 2014, pp. 1724-1734.\n\nConvolutional neural networks for sentence classification. Y Kim, Proc. Conf. Empirical Methods Natural Lang. Process. Conf. Empirical Methods Natural Lang. essY. Kim, \"Convolutional neural networks for sentence classification,\" in Proc. Conf. Empirical Methods Natural Lang. Process., 2014, pp. 1746-1751.\n\nA convolutional encoder model for neural machine translation. J Gehring, M Auli, D Grangier, Y N Dauphin, Proc. 55th Annu. Meeting Assoc. Comput. Linguistics. 55th Annu. Meeting Assoc. Comput. LinguisticsJ. Gehring, M. Auli, D. Grangier, and Y. N. Dauphin, \"A convolutional en- coder model for neural machine translation,\" in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 123-135.\n\nOn the difficulty of training recurrent neural networks. R Pascanu, T Mikolov, Y Bengio, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnR. Pascanu, T. Mikolov, and Y. Bengio, \"On the difficulty of training recurrent neural networks,\" in Proc. Int. Conf. Mach. Learn., 2013, pp. 1310-1318.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, C Gulcehre, K Cho, Y Bengio, Proc. NIPS 2014 Workshop Deep Learn. NIPS 2014 Workshop Deep LearnJ. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \"Empirical evaluation of gated recurrent neural networks on sequence modeling,\" in Proc. NIPS 2014 Workshop Deep Learn., 2014.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Y Wu, arXiv:1609.08144Y. Wu et al., \"Google's neural machine translation system: Bridging the gap between human and machine translation,\" 2016, arXiv:1609.08144.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, Proc. Conf. North Amer. Conf. North AmerJ. Devlin, M. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of deep bidirectional transformers for language understanding,\" in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol., 2019, pp. 4171-4186.\n\nA grammar-based structural CNN decoder for code generation. Z Sun, Q Zhu, L Mou, Y Xiong, G Li, L Zhang, Proc. 33rd AAAI Conf. 33rd AAAI ConfZ. Sun, Q. Zhu, L. Mou, Y. Xiong, G. Li, and L. Zhang, \"A grammar-based structural CNN decoder for code generation,\" in Proc. 33rd AAAI Conf. Artif. Intell., 2019, pp. 7055-7062.\n\nTreeCaps: Treestructured capsule networks for program source code processing. V Jayasundara, N D Q Bui, L Jiang, D Lo, arXiv:1910.12306V. Jayasundara, N. D. Q. Bui, L. Jiang, and D. Lo, \"TreeCaps: Tree- structured capsule networks for program source code processing,\" 2019, arXiv:1910.12306.\n\nGenerating long sequences with sparse transformers. R Child, S Gray, A Radford, I Sutskever, arXiv:1904.10509R. Child, S. Gray, A. Radford, and I. Sutskever, \"Generating long se- quences with sparse transformers,\" 2019, arXiv:1904.10509.\n\nAn end-to-end compression framework based on convolutional neural networks. F Jiang, W Tao, S Liu, J Ren, X Guo, D Zhao, IEEE Trans. Circuits Syst. Video Technol. 2810F. Jiang, W. Tao, S. Liu, J. Ren, X. Guo, and D. Zhao, \"An end-to-end compression framework based on convolutional neural networks,\" IEEE Trans. Circuits Syst. Video Technol., vol. 28, no. 10, pp. 3007-3018, Oct. 2018.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770-778.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, Proc. 32nd Int. Conf. Mach. Learn. 32nd Int. Conf. Mach. LearnS. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in Proc. 32nd Int. Conf. Mach. Learn., 2015, pp. 448-456.\n\nA neural model for generating natural language summaries of program subroutines. A Leclair, S Jiang, C Mcmillan, Proc. 41st Int. Conf. Softw. Eng. 41st Int. Conf. Softw. EngA. LeClair, S. Jiang, and C. McMillan, \"A neural model for generating natural language summaries of program subroutines,\" in Proc. 41st Int. Conf. Softw. Eng., 2019, pp. 795-806.\n\nDeep code comment generation with hybrid lexical and syntactical information. X Hu, G Li, X Xia, D Lo, Z Jin, Empirical Softw. Eng. 253X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, \"Deep code comment generation with hybrid lexical and syntactical information,\" Empirical Softw. Eng., vol. 25, no. 3, pp. 2179-2217, 2020.\n\nCode generation as a dual task of code summarization. B Wei, G Li, X Xia, Z Fu, Z Jin, Proc. Annu. Conf. Neural Inf. Process. Syst. Annu. Conf. Neural Inf. ess. SystB. Wei, G. Li, X. Xia, Z. Fu, and Z. Jin, \"Code generation as a dual task of code summarization,\" in Proc. Annu. Conf. Neural Inf. Process. Syst., 2019, pp. 6559-6569.\n\nA transformerbased approach for source code summarization. W U Ahmad, S Chakraborty, B Ray, K Chang, Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang, \"A transformer- based approach for source code summarization,\" in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020, pp. 4998-5007.\n\nSummarizing source code with transferred API knowledge. X Hu, G Li, X Xia, D Lo, S Lu, Z Jin, Proc. 27th Int. Joint Conf. 27th Int. Joint ConfX. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, \"Summarizing source code with transferred API knowledge,\" in Proc. 27th Int. Joint Conf. Artif. Intell., 2018, pp. 2269-2275.\n\nTowards string-to-tree neural machine translation. R Aharoni, Y Goldberg, Proc. 55th Annu. Meeting Assoc. Comput. Linguistics. 55th Annu. Meeting Assoc. Comput. LinguisticsR. Aharoni and Y. Goldberg, \"Towards string-to-tree neural machine translation,\" in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 132-140.\n\nMulti-style generative reading comprehension. K Nishida, Proc. nullK. Nishida et al., \"Multi-style generative reading comprehension,\" in Proc.\n\n. Annu. Meeting Assoc. Comput. Linguistics. Annu. Meeting Assoc. Comput. Linguistics, Jul. 2019, pp. 2273-2284.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" 2014, arXiv:1412.6980.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdi- nov, \"Dropout: A simple way to prevent neural networks from overfitting,\" J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929-1958, 2014.\n\nAn introduction to statistical methods and data analysis. V R Prybutok, Technometrics. 313V. R. Prybutok, \"An introduction to statistical methods and data analysis,\" Technometrics, vol. 31, no. 3, pp. 389-390, 2012.\n\nDominance statistics: Ordinal analyses to answer ordinal questions. N Cliff, Psychol. Bull. 1143N. Cliff, \"Dominance statistics: Ordinal analyses to answer ordinal ques- tions,\" Psychol. Bull., vol. 114, no. 3, pp. 494-509, 1993.\n\n. R Singh, N S Mangat, Elements of Survey Sampling. 15SpringerR. Singh and N. S. Mangat, Elements of Survey Sampling, vol. 15. New York, NY, USA: Springer, 2013.\n\nGenerating question titles for stack overflow from mined code snippets. Z Gao, X Xia, J C Grundy, D Lo, Y Li, ACM Trans. Softw. Eng. Methodol. 294Z. Gao, X. Xia, J. C. Grundy, D. Lo, and Y. Li, \"Generating question titles for stack overflow from mined code snippets,\" ACM Trans. Softw. Eng. Methodol., vol. 29, no. 4, pp. 1-37, 2020.\n\nOn the use of automated text summarization techniques for summarizing source code. S Haiduc, J Aponte, L Moreno, A Marcus, Proc. 17th Work. Conf. Reverse Eng. 17th Work. Conf. Reverse EngS. Haiduc, J. Aponte, L. Moreno, and A. Marcus, \"On the use of automated text summarization techniques for summarizing source code,\" in Proc. 17th Work. Conf. Reverse Eng., 2010, pp. 35-44.\n\nSupporting program comprehension with source code summarization. S Haiduc, J Aponte, A Marcus, Proc. ACM/IEEE 32nd Int. Conf. Softw. Eng. ACM/IEEE 32nd Int. Conf. Softw. EngS. Haiduc, J. Aponte, and A. Marcus, \"Supporting program comprehension with source code summarization,\" in Proc. ACM/IEEE 32nd Int. Conf. Softw. Eng., 2010, pp. 223-226.\n\nAn eye-tracking study of java programmers and application to source code summarization. P Rodeghero, C Liu, P W Mcburney, C Mcmillan, IEEE Trans. Softw. Eng. 4111P. Rodeghero, C. Liu, P. W. McBurney, and C. McMillan, \"An eye-tracking study of java programmers and application to source code summarization,\" IEEE Trans. Softw. Eng., vol. 41, no. 11, pp. 1038-1054, Nov. 2015.\n\nAutomatic source code summarization of context for java methods. P W Mcburney, C Mcmillan, IEEE Trans. Softw. Eng. 422P. W. Mcburney and C. Mcmillan, \"Automatic source code summarization of context for java methods,\" IEEE Trans. Softw. Eng., vol. 42, no. 2, pp. 103-119, Feb. 2016.\n\nAutomatically capturing source code context of NL-queries for software maintenance and reuse. E Hill, L L Pollock, K Vijay-Shanker, Proc. 31st Int. Conf. Softw. Eng. 31st Int. Conf. Softw. EngE. Hill, L. L. Pollock, and K. Vijay-Shanker, \"Automatically capturing source code context of NL-queries for software maintenance and reuse,\" in Proc. 31st Int. Conf. Softw. Eng., 2009, pp. 232-242.\n\nRetrieve and refine: Exemplar-based neural comment generation. B Wei, Proc. 34th IEEE/ACM Int. Conf. Autom. Softw. Eng. 34th IEEE/ACM Int. Conf. Autom. Softw. EngB. Wei, \"Retrieve and refine: Exemplar-based neural comment gener- ation,\" in Proc. 34th IEEE/ACM Int. Conf. Autom. Softw. Eng., 2019, pp. 1250-1252.\n\nTAG: Type auxiliary guiding for code comment generation. R Cai, Z Liang, B Xu, Z Li, Y Hao, Y Chen, Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020R. Cai, Z. Liang, B. Xu, Z. Li, Y. Hao, and Y. Chen, \"TAG: Type auxiliary guiding for code comment generation,\" in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020, pp. 291-301.\n\nSource code analysis extractive approach to generate textual summary. K A Dawood, K Y Sharif, K T Wei, J. Theor. Appl. Inf. Technol. 9521K. A. Dawood, K. Y. Sharif, and K. T. Wei, \"Source code analysis extractive approach to generate textual summary,\" J. Theor. Appl. Inf. Technol., vol. 95, no. 21, pp. 5765-5777, 2017.\n\nAutomatically generating natural language descriptions for object-related statement sequences. X Wang, L L Pollock, K Vijay-Shanker, Proc. IEEE 24th Int. Conf. Softw. Anal., Evol. Reeng. IEEE 24th Int. Conf. Softw. Anal., Evol. ReengX. Wang, L. L. Pollock, and K. Vijay-Shanker, \"Automatically gen- erating natural language descriptions for object-related statement se- quences,\" in Proc. IEEE 24th Int. Conf. Softw. Anal., Evol. Reeng., 2017, pp. 205-216.\n\nImproved code summarization via a graph neural network. A Leclair, S Haque, L Wu, C Mcmillan, Proc. 28th Int. Conf. Prog. Comprehension, 2020. 28th Int. Conf. Prog. Comprehension, 2020A. LeClair, S. Haque, L. Wu, and C. McMillan, \"Improved code sum- marization via a graph neural network,\" in Proc. 28th Int. Conf. Prog. Comprehension, 2020, pp. 184-195.\n\nReinforcement-learning-guided source code summarization via hierarchical attention. W Wang, IEEE Trans. Softw. Eng. 481W. Wang et al., \"Reinforcement-learning-guided source code summariza- tion via hierarchical attention,\" IEEE Trans. Softw. Eng., vol. 48, no. 1, pp. 102-119, Jan. 2022.\n\nLearning-based recursive aggregation of abstract syntax trees for code clone detection. L B\u00fcch, A Andrzejak, Proc. 26th IEEE Int. 26th IEEE IntL. B\u00fcch and A. Andrzejak, \"Learning-based recursive aggregation of abstract syntax trees for code clone detection,\" in Proc. 26th IEEE Int. Conf. Softw. Anal. Evol. Reeng., 2019, pp. 95-104.\n\nFast, scalable and user-guided clone detection. J Svajlenko, C K Roy, Proc. 40th Int. Conf. Softw. Eng. Companion. 40th Int. Conf. Softw. Eng. CompanionJ. Svajlenko and C. K. Roy, \"Fast, scalable and user-guided clone detec- tion,\" in Proc. 40th Int. Conf. Softw. Eng. Companion, 2018, pp. 352-353.\n\nEnabling clone detection for Ethereum via smart contract birthmarks. H Liu, Z Yang, Y Jiang, W Zhao, J Sun, Proc. 27th Int. Conf. Prog. Comprehension. 27th Int. Conf. Prog. ComprehensionH. Liu, Z. Yang, Y. Jiang, W. Zhao, and J. Sun, \"Enabling clone detection for Ethereum via smart contract birthmarks,\" in Proc. 27th Int. Conf. Prog. Comprehension, 2019, pp. 105-115.\n\nTreeGen: A tree-based transformer architecture for code generation. Z Sun, Q Zhu, Y Xiong, Y Sun, L Mou, L Zhang, Proc. 34th AAAI Conf. Artif. Intell. 2020. 34th AAAI Conf. Artif. Intell. 2020Z. Sun, Q. Zhu, Y. Xiong, Y. Sun, L. Mou, and L. Zhang, \"TreeGen: A tree-based transformer architecture for code generation,\" in Proc. 34th AAAI Conf. Artif. Intell. 2020, pp. 8984-8991.\n\nA method of automatic code generation based on AADL model. C Zhang, X Niu, B Yu, Proc. 2nd Int. Conf. Comput. Sci. 2nd Int. Conf. Comput. SciC. Zhang, X. Niu, and B. Yu, \"A method of automatic code generation based on AADL model,\" in Proc. 2nd Int. Conf. Comput. Sci. Artif. Intell., 2018, pp. 180-184.\n\nAssessing practitioner beliefs about software defect prediction. N C Shrikanth, T Menzies, Proc. 42nd Int. Conf. Softw. Eng.: Softw. Eng. Pract. 42nd Int. Conf. Softw. Eng.: Softw. Eng. PractN. C. Shrikanth and T. Menzies, \"Assessing practitioner beliefs about software defect prediction,\" in Proc. 42nd Int. Conf. Softw. Eng.: Softw. Eng. Pract., 2020, pp. 182-190.\n\nEmpirical evaluation of the impact of class overlap on software defect prediction. L Gong, S Jiang, R Wang, L Jiang, Proc. 34th. 34thL. Gong, S. Jiang, R. Wang, and L. Jiang, \"Empirical evaluation of the impact of class overlap on software defect prediction,\" in Proc. 34th IEEE/ACM Int. Conf. Autom. Softw. Eng., 2019, pp. 698-709.\n\nA semantic convolutional auto-encoder model for software defect prediction. Z Wang, L Lu, Proc. 32nd Int. 32nd IntZ. Wang and L. Lu, \"A semantic convolutional auto-encoder model for software defect prediction,\" in Proc. 32nd Int. Conf. Softw. Eng. Knowl. Eng., 2020, pp. 323-328.\n\nOn the naturalness of software. A Hindle, E T Barr, Z Su, M Gabel, P T Devanbu, Proc. 34th Int. Conf. Softw. Eng. 34th Int. Conf. Softw. EngA. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. T. Devanbu, \"On the naturalness of software,\" in Proc. 34th Int. Conf. Softw. Eng., 2012, pp. 837-847.\n\nLearning natural coding conventions. M Allamanis, E T Barr, C Bird, C A Sutton, Proc. 22nd. 22ndM. Allamanis, E. T. Barr, C. Bird, and C. A. Sutton, \"Learning natural coding conventions,\" in Proc. 22nd ACM SIGSOFT Int. Symp. Found. Softw. Eng., 2014, pp. 281-293.\n\nConvolutional neural networks over tree structures for programming language processing. L Mou, G Li, L Zhang, T Wang, Z Jin, Proc. 30th AAAI Conf. 30th AAAI ConfL. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, \"Convolutional neural networks over tree structures for programming language processing,\" in Proc. 30th AAAI Conf. Artif. Intell., 2016, pp. 1287-1293.\n\nA syntactic neural model for general-purpose code generation. P Yin, G Neubig, Proc. 55th Annu. Meeting Assoc. Comput. Linguistics. 55th Annu. Meeting Assoc. Comput. LinguisticsP. Yin and G. Neubig, \"A syntactic neural model for general-purpose code generation,\" in Proc. 55th Annu. Meeting Assoc. Comput. Linguistics, 2017, pp. 440-450.\n", "annotations": {"author": "[{\"end\":131,\"start\":97},{\"end\":143,\"start\":132},{\"end\":153,\"start\":144},{\"end\":177,\"start\":154},{\"end\":187,\"start\":178},{\"end\":209,\"start\":188},{\"end\":245,\"start\":210},{\"end\":255,\"start\":246},{\"end\":265,\"start\":256},{\"end\":277,\"start\":266},{\"end\":287,\"start\":278},{\"end\":297,\"start\":288},{\"end\":453,\"start\":298},{\"end\":547,\"start\":454},{\"end\":613,\"start\":548},{\"end\":669,\"start\":614}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":103},{\"end\":142,\"start\":140},{\"end\":152,\"start\":148},{\"end\":176,\"start\":172},{\"end\":186,\"start\":183},{\"end\":208,\"start\":205},{\"end\":220,\"start\":216},{\"end\":254,\"start\":251},{\"end\":264,\"start\":262},{\"end\":276,\"start\":274},{\"end\":286,\"start\":282},{\"end\":296,\"start\":293}]", "author_first_name": "[{\"end\":102,\"start\":97},{\"end\":139,\"start\":132},{\"end\":147,\"start\":144},{\"end\":171,\"start\":166},{\"end\":182,\"start\":178},{\"end\":204,\"start\":200},{\"end\":215,\"start\":210},{\"end\":250,\"start\":246},{\"end\":261,\"start\":256},{\"end\":273,\"start\":266},{\"end\":281,\"start\":278},{\"end\":292,\"start\":288}]", "author_affiliation": "[{\"end\":452,\"start\":299},{\"end\":546,\"start\":455},{\"end\":612,\"start\":549},{\"end\":668,\"start\":615}]", "title": "[{\"end\":84,\"start\":1},{\"end\":753,\"start\":670}]", "venue": "[{\"end\":787,\"start\":755}]", "abstract": "[{\"end\":2779,\"start\":1238}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3127,\"start\":3124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3412,\"start\":3409},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3434,\"start\":3431},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3439,\"start\":3436},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3907,\"start\":3904},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4111,\"start\":4108},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4835,\"start\":4832},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6234,\"start\":6231},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6250,\"start\":6247},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7404,\"start\":7403},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8134,\"start\":8131},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8276,\"start\":8275},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8724,\"start\":8721},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8730,\"start\":8726},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8747,\"start\":8744},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8761,\"start\":8757},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8767,\"start\":8763},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9236,\"start\":9232},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9451,\"start\":9447},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9487,\"start\":9483},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9546,\"start\":9543},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9891,\"start\":9888},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10064,\"start\":10061},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10070,\"start\":10066},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12150,\"start\":12146},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13019,\"start\":13016},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15113,\"start\":15109},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16115,\"start\":16111},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16121,\"start\":16117},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16307,\"start\":16304},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17354,\"start\":17350},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17690,\"start\":17686},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18675,\"start\":18672},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19647,\"start\":19643},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19745,\"start\":19741},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19917,\"start\":19914},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20228,\"start\":20224},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20234,\"start\":20230},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21698,\"start\":21695},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23176,\"start\":23173},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23197,\"start\":23193},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23217,\"start\":23213},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23234,\"start\":23230},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23255,\"start\":23251},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24133,\"start\":24129},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24381,\"start\":24377},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24452,\"start\":24448},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24471,\"start\":24467},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24491,\"start\":24487},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24928,\"start\":24924},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25013,\"start\":25009},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25019,\"start\":25015},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25039,\"start\":25036},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25549,\"start\":25546},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25555,\"start\":25551},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25561,\"start\":25557},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25661,\"start\":25658},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26038,\"start\":26037},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26195,\"start\":26194},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26400,\"start\":26396},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26406,\"start\":26402},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26493,\"start\":26490},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26895,\"start\":26891},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27287,\"start\":27283},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27765,\"start\":27761},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28073,\"start\":28069},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28133,\"start\":28129},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28482,\"start\":28478},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28639,\"start\":28635},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28767,\"start\":28763},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29631,\"start\":29627},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31146,\"start\":31142},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31741,\"start\":31737},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31877,\"start\":31873},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32122,\"start\":32118},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32420,\"start\":32416},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32940,\"start\":32936},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":46271,\"start\":46267},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":48099,\"start\":48095},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":49230,\"start\":49226},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":57592,\"start\":57588},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":57598,\"start\":57594},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":58149,\"start\":58145},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":58155,\"start\":58151},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":58161,\"start\":58157},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":58455,\"start\":58452},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":58461,\"start\":58457},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":58467,\"start\":58463},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":58530,\"start\":58526},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":58536,\"start\":58532},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":58542,\"start\":58538},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":58548,\"start\":58544},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":58893,\"start\":58890},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":58999,\"start\":58995},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":59249,\"start\":59245},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":59644,\"start\":59641},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":60109,\"start\":60106},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":60364,\"start\":60360},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":60684,\"start\":60680},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":60989,\"start\":60985},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":61343,\"start\":61339},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":61582,\"start\":61578},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":61746,\"start\":61742},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":62714,\"start\":62710},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":62726,\"start\":62722},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":62732,\"start\":62728},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":62754,\"start\":62750},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":62760,\"start\":62756},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":62766,\"start\":62762},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":62794,\"start\":62790},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":62800,\"start\":62796},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":62821,\"start\":62817},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":63007,\"start\":63003},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":63292,\"start\":63288},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":63607,\"start\":63603}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":65441,\"start\":65410},{\"attributes\":{\"id\":\"fig_1\"},\"end\":65484,\"start\":65442},{\"attributes\":{\"id\":\"fig_2\"},\"end\":65507,\"start\":65485},{\"attributes\":{\"id\":\"fig_3\"},\"end\":65546,\"start\":65508},{\"attributes\":{\"id\":\"fig_4\"},\"end\":65595,\"start\":65547},{\"attributes\":{\"id\":\"fig_5\"},\"end\":65895,\"start\":65596},{\"attributes\":{\"id\":\"fig_6\"},\"end\":66006,\"start\":65896},{\"attributes\":{\"id\":\"fig_7\"},\"end\":66069,\"start\":66007},{\"attributes\":{\"id\":\"fig_8\"},\"end\":66151,\"start\":66070},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":66182,\"start\":66152},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":66235,\"start\":66183},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66287,\"start\":66236},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":66354,\"start\":66288},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":66851,\"start\":66355},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":66994,\"start\":66852},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":67072,\"start\":66995},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":67100,\"start\":67073},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":67181,\"start\":67101},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":67189,\"start\":67182}]", "paragraph": "[{\"end\":3340,\"start\":2798},{\"end\":4297,\"start\":3342},{\"end\":5445,\"start\":4299},{\"end\":5524,\"start\":5447},{\"end\":5665,\"start\":5526},{\"end\":6528,\"start\":5667},{\"end\":6625,\"start\":6530},{\"end\":7049,\"start\":6627},{\"end\":7896,\"start\":7112},{\"end\":8798,\"start\":7937},{\"end\":9104,\"start\":8800},{\"end\":9570,\"start\":9106},{\"end\":9864,\"start\":9572},{\"end\":10186,\"start\":9866},{\"end\":10540,\"start\":10188},{\"end\":11062,\"start\":10542},{\"end\":11306,\"start\":11064},{\"end\":12108,\"start\":11308},{\"end\":12705,\"start\":12142},{\"end\":13220,\"start\":12707},{\"end\":14049,\"start\":13263},{\"end\":14335,\"start\":14080},{\"end\":15074,\"start\":14337},{\"end\":15930,\"start\":15076},{\"end\":16204,\"start\":15932},{\"end\":16459,\"start\":16206},{\"end\":17203,\"start\":16461},{\"end\":17527,\"start\":17240},{\"end\":17691,\"start\":17529},{\"end\":17811,\"start\":17693},{\"end\":18162,\"start\":17813},{\"end\":18449,\"start\":18193},{\"end\":18521,\"start\":18451},{\"end\":18819,\"start\":18523},{\"end\":19183,\"start\":18821},{\"end\":19871,\"start\":19622},{\"end\":20113,\"start\":19873},{\"end\":20594,\"start\":20115},{\"end\":21508,\"start\":20596},{\"end\":21699,\"start\":21524},{\"end\":22054,\"start\":21701},{\"end\":22530,\"start\":22056},{\"end\":22819,\"start\":22578},{\"end\":22906,\"start\":22821},{\"end\":23256,\"start\":22908},{\"end\":23356,\"start\":23258},{\"end\":23760,\"start\":23358},{\"end\":23843,\"start\":23762},{\"end\":24053,\"start\":23845},{\"end\":24740,\"start\":24085},{\"end\":25020,\"start\":24755},{\"end\":26247,\"start\":25022},{\"end\":26465,\"start\":26264},{\"end\":28640,\"start\":26467},{\"end\":28960,\"start\":28642},{\"end\":29153,\"start\":28962},{\"end\":29228,\"start\":29155},{\"end\":29582,\"start\":29230},{\"end\":30384,\"start\":29605},{\"end\":30697,\"start\":30386},{\"end\":31111,\"start\":30699},{\"end\":32124,\"start\":31113},{\"end\":32384,\"start\":32182},{\"end\":32638,\"start\":32386},{\"end\":33136,\"start\":32640},{\"end\":33337,\"start\":33138},{\"end\":34093,\"start\":33477},{\"end\":34290,\"start\":34095},{\"end\":34916,\"start\":34292},{\"end\":35215,\"start\":34918},{\"end\":35360,\"start\":35217},{\"end\":35670,\"start\":35362},{\"end\":37331,\"start\":35672},{\"end\":37721,\"start\":37333},{\"end\":38386,\"start\":37853},{\"end\":38573,\"start\":38388},{\"end\":40458,\"start\":38624},{\"end\":41390,\"start\":40460},{\"end\":41809,\"start\":41392},{\"end\":42139,\"start\":41924},{\"end\":42526,\"start\":42141},{\"end\":43424,\"start\":42528},{\"end\":43916,\"start\":43426},{\"end\":44358,\"start\":43918},{\"end\":44572,\"start\":44360},{\"end\":45017,\"start\":44619},{\"end\":45243,\"start\":45019},{\"end\":45565,\"start\":45245},{\"end\":46152,\"start\":45589},{\"end\":46564,\"start\":46154},{\"end\":47138,\"start\":46629},{\"end\":47906,\"start\":47140},{\"end\":48724,\"start\":47908},{\"end\":49518,\"start\":48726},{\"end\":50075,\"start\":49581},{\"end\":50807,\"start\":50077},{\"end\":50983,\"start\":50809},{\"end\":53041,\"start\":50985},{\"end\":53584,\"start\":53100},{\"end\":53995,\"start\":53586},{\"end\":54712,\"start\":54040},{\"end\":54960,\"start\":54714},{\"end\":55082,\"start\":54962},{\"end\":55683,\"start\":55109},{\"end\":55775,\"start\":55712},{\"end\":56182,\"start\":55800},{\"end\":56385,\"start\":56184},{\"end\":56652,\"start\":56387},{\"end\":57348,\"start\":56654},{\"end\":57868,\"start\":57373},{\"end\":58162,\"start\":57894},{\"end\":58549,\"start\":58209},{\"end\":59384,\"start\":58551},{\"end\":61960,\"start\":59386},{\"end\":62240,\"start\":61962},{\"end\":62801,\"start\":62283},{\"end\":63771,\"start\":62803},{\"end\":64004,\"start\":63773},{\"end\":64567,\"start\":64023},{\"end\":65203,\"start\":64569},{\"end\":65409,\"start\":65205}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19499,\"start\":19184},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19570,\"start\":19499},{\"attributes\":{\"id\":\"formula_2\"},\"end\":38623,\"start\":38574},{\"attributes\":{\"id\":\"formula_3\"},\"end\":46628,\"start\":46565}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33753,\"start\":33745},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33859,\"start\":33851},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35320,\"start\":35312},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36142,\"start\":36133},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36537,\"start\":36528},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":36805,\"start\":36797},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":37239,\"start\":37231},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":40966,\"start\":40959},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":41224,\"start\":41216},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":42693,\"start\":42684},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":45016,\"start\":45006},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":45041,\"start\":45031},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":48819,\"start\":48811},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":53596,\"start\":53589},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":53788,\"start\":53781},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54299,\"start\":54160},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54959,\"start\":54951},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54982,\"start\":54974},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":55432,\"start\":55385},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":55454,\"start\":55445}]", "section_header": "[{\"end\":2796,\"start\":2781},{\"end\":7110,\"start\":7052},{\"end\":7913,\"start\":7899},{\"end\":7935,\"start\":7916},{\"end\":12140,\"start\":12111},{\"end\":13261,\"start\":13223},{\"end\":14078,\"start\":14052},{\"end\":17238,\"start\":17206},{\"end\":18191,\"start\":18165},{\"end\":19620,\"start\":19572},{\"end\":21522,\"start\":21511},{\"end\":22552,\"start\":22533},{\"end\":22576,\"start\":22555},{\"end\":24083,\"start\":24056},{\"end\":24753,\"start\":24743},{\"end\":26262,\"start\":26250},{\"end\":29603,\"start\":29585},{\"end\":32180,\"start\":32127},{\"end\":33358,\"start\":33340},{\"end\":33387,\"start\":33361},{\"end\":33475,\"start\":33390},{\"end\":37750,\"start\":37724},{\"end\":37851,\"start\":37753},{\"end\":41838,\"start\":41812},{\"end\":41922,\"start\":41841},{\"end\":44590,\"start\":44575},{\"end\":44617,\"start\":44593},{\"end\":45587,\"start\":45568},{\"end\":49579,\"start\":49521},{\"end\":53098,\"start\":53044},{\"end\":54038,\"start\":53998},{\"end\":55107,\"start\":55085},{\"end\":55710,\"start\":55686},{\"end\":55798,\"start\":55778},{\"end\":57371,\"start\":57351},{\"end\":57892,\"start\":57871},{\"end\":58183,\"start\":58165},{\"end\":58207,\"start\":58186},{\"end\":62281,\"start\":62243},{\"end\":64021,\"start\":64007},{\"end\":65419,\"start\":65411},{\"end\":65451,\"start\":65443},{\"end\":65494,\"start\":65486},{\"end\":65517,\"start\":65509},{\"end\":65556,\"start\":65548},{\"end\":65605,\"start\":65597},{\"end\":65903,\"start\":65897},{\"end\":66016,\"start\":66008},{\"end\":66079,\"start\":66071},{\"end\":66171,\"start\":66153},{\"end\":66200,\"start\":66184},{\"end\":66248,\"start\":66237},{\"end\":66303,\"start\":66289},{\"end\":66365,\"start\":66356},{\"end\":66867,\"start\":66853},{\"end\":67013,\"start\":66996},{\"end\":67089,\"start\":67074},{\"end\":67116,\"start\":67102},{\"end\":67188,\"start\":67183}]", "table": null, "figure_caption": "[{\"end\":65441,\"start\":65421},{\"end\":65484,\"start\":65453},{\"end\":65507,\"start\":65496},{\"end\":65546,\"start\":65519},{\"end\":65595,\"start\":65558},{\"end\":65895,\"start\":65607},{\"end\":66006,\"start\":65905},{\"end\":66069,\"start\":66018},{\"end\":66151,\"start\":66081},{\"end\":66182,\"start\":66173},{\"end\":66235,\"start\":66203},{\"end\":66287,\"start\":66252},{\"end\":66354,\"start\":66306},{\"end\":66851,\"start\":66367},{\"end\":66994,\"start\":66870},{\"end\":67072,\"start\":67018},{\"end\":67100,\"start\":67092},{\"end\":67181,\"start\":67118}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11082,\"start\":11076},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13375,\"start\":13369},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16630,\"start\":16624},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17810,\"start\":17804},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20112,\"start\":20106},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21365,\"start\":21359},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38267,\"start\":38261},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38417,\"start\":38411},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38691,\"start\":38685},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38737,\"start\":38728},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39194,\"start\":39185},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39509,\"start\":39500},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39686,\"start\":39677},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39860,\"start\":39854},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40664,\"start\":40658},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":48456,\"start\":48450},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":49608,\"start\":49602},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50317,\"start\":50311},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50573,\"start\":50567},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":53124,\"start\":53118}]", "bib_author_first_name": "[{\"end\":69010,\"start\":69009},{\"end\":69022,\"start\":69021},{\"end\":69030,\"start\":69029},{\"end\":69043,\"start\":69042},{\"end\":69045,\"start\":69044},{\"end\":69056,\"start\":69055},{\"end\":69373,\"start\":69372},{\"end\":69383,\"start\":69382},{\"end\":69393,\"start\":69392},{\"end\":69405,\"start\":69404},{\"end\":69415,\"start\":69414},{\"end\":69417,\"start\":69416},{\"end\":69428,\"start\":69427},{\"end\":69826,\"start\":69825},{\"end\":69828,\"start\":69827},{\"end\":69840,\"start\":69839},{\"end\":70170,\"start\":70169},{\"end\":70178,\"start\":70177},{\"end\":70189,\"start\":70188},{\"end\":70199,\"start\":70198},{\"end\":70530,\"start\":70529},{\"end\":70536,\"start\":70535},{\"end\":70542,\"start\":70541},{\"end\":70549,\"start\":70548},{\"end\":70555,\"start\":70554},{\"end\":70791,\"start\":70790},{\"end\":71059,\"start\":71058},{\"end\":71071,\"start\":71070},{\"end\":71081,\"start\":71080},{\"end\":71089,\"start\":71088},{\"end\":71475,\"start\":71474},{\"end\":71487,\"start\":71486},{\"end\":71818,\"start\":71817},{\"end\":71832,\"start\":71831},{\"end\":72071,\"start\":72070},{\"end\":72423,\"start\":72422},{\"end\":72734,\"start\":72733},{\"end\":72745,\"start\":72744},{\"end\":72753,\"start\":72752},{\"end\":72765,\"start\":72764},{\"end\":72767,\"start\":72766},{\"end\":73128,\"start\":73127},{\"end\":73139,\"start\":73138},{\"end\":73150,\"start\":73149},{\"end\":73444,\"start\":73443},{\"end\":73453,\"start\":73452},{\"end\":73465,\"start\":73464},{\"end\":73472,\"start\":73471},{\"end\":73822,\"start\":73821},{\"end\":74067,\"start\":74066},{\"end\":74077,\"start\":74076},{\"end\":74086,\"start\":74085},{\"end\":74093,\"start\":74092},{\"end\":74443,\"start\":74442},{\"end\":74450,\"start\":74449},{\"end\":74457,\"start\":74456},{\"end\":74464,\"start\":74463},{\"end\":74473,\"start\":74472},{\"end\":74479,\"start\":74478},{\"end\":74782,\"start\":74781},{\"end\":74797,\"start\":74796},{\"end\":74801,\"start\":74798},{\"end\":74808,\"start\":74807},{\"end\":74817,\"start\":74816},{\"end\":75049,\"start\":75048},{\"end\":75058,\"start\":75057},{\"end\":75066,\"start\":75065},{\"end\":75077,\"start\":75076},{\"end\":75312,\"start\":75311},{\"end\":75321,\"start\":75320},{\"end\":75328,\"start\":75327},{\"end\":75335,\"start\":75334},{\"end\":75342,\"start\":75341},{\"end\":75349,\"start\":75348},{\"end\":75669,\"start\":75668},{\"end\":75675,\"start\":75674},{\"end\":75684,\"start\":75683},{\"end\":75691,\"start\":75690},{\"end\":76037,\"start\":76036},{\"end\":76046,\"start\":76045},{\"end\":76380,\"start\":76379},{\"end\":76391,\"start\":76390},{\"end\":76400,\"start\":76399},{\"end\":76730,\"start\":76729},{\"end\":76736,\"start\":76735},{\"end\":76742,\"start\":76741},{\"end\":76749,\"start\":76748},{\"end\":76755,\"start\":76754},{\"end\":77023,\"start\":77022},{\"end\":77030,\"start\":77029},{\"end\":77036,\"start\":77035},{\"end\":77043,\"start\":77042},{\"end\":77049,\"start\":77048},{\"end\":77362,\"start\":77361},{\"end\":77364,\"start\":77363},{\"end\":77373,\"start\":77372},{\"end\":77388,\"start\":77387},{\"end\":77395,\"start\":77394},{\"end\":77762,\"start\":77761},{\"end\":77768,\"start\":77767},{\"end\":77774,\"start\":77773},{\"end\":77781,\"start\":77780},{\"end\":77787,\"start\":77786},{\"end\":77793,\"start\":77792},{\"end\":78074,\"start\":78073},{\"end\":78085,\"start\":78084},{\"end\":78398,\"start\":78397},{\"end\":78653,\"start\":78652},{\"end\":78655,\"start\":78654},{\"end\":78665,\"start\":78664},{\"end\":78847,\"start\":78846},{\"end\":78861,\"start\":78860},{\"end\":78871,\"start\":78870},{\"end\":78885,\"start\":78884},{\"end\":78898,\"start\":78897},{\"end\":79205,\"start\":79204},{\"end\":79207,\"start\":79206},{\"end\":79432,\"start\":79431},{\"end\":79597,\"start\":79596},{\"end\":79606,\"start\":79605},{\"end\":79608,\"start\":79607},{\"end\":79830,\"start\":79829},{\"end\":79837,\"start\":79836},{\"end\":79844,\"start\":79843},{\"end\":79846,\"start\":79845},{\"end\":79856,\"start\":79855},{\"end\":79862,\"start\":79861},{\"end\":80176,\"start\":80175},{\"end\":80186,\"start\":80185},{\"end\":80196,\"start\":80195},{\"end\":80206,\"start\":80205},{\"end\":80536,\"start\":80535},{\"end\":80546,\"start\":80545},{\"end\":80556,\"start\":80555},{\"end\":80903,\"start\":80902},{\"end\":80916,\"start\":80915},{\"end\":80923,\"start\":80922},{\"end\":80925,\"start\":80924},{\"end\":80937,\"start\":80936},{\"end\":81256,\"start\":81255},{\"end\":81258,\"start\":81257},{\"end\":81270,\"start\":81269},{\"end\":81568,\"start\":81567},{\"end\":81576,\"start\":81575},{\"end\":81578,\"start\":81577},{\"end\":81589,\"start\":81588},{\"end\":81929,\"start\":81928},{\"end\":82236,\"start\":82235},{\"end\":82243,\"start\":82242},{\"end\":82252,\"start\":82251},{\"end\":82258,\"start\":82257},{\"end\":82264,\"start\":82263},{\"end\":82271,\"start\":82270},{\"end\":82647,\"start\":82646},{\"end\":82649,\"start\":82648},{\"end\":82659,\"start\":82658},{\"end\":82661,\"start\":82660},{\"end\":82671,\"start\":82670},{\"end\":82673,\"start\":82672},{\"end\":82994,\"start\":82993},{\"end\":83002,\"start\":83001},{\"end\":83004,\"start\":83003},{\"end\":83015,\"start\":83014},{\"end\":83413,\"start\":83412},{\"end\":83424,\"start\":83423},{\"end\":83433,\"start\":83432},{\"end\":83439,\"start\":83438},{\"end\":83797,\"start\":83796},{\"end\":84090,\"start\":84089},{\"end\":84098,\"start\":84097},{\"end\":84385,\"start\":84384},{\"end\":84398,\"start\":84397},{\"end\":84400,\"start\":84399},{\"end\":84706,\"start\":84705},{\"end\":84713,\"start\":84712},{\"end\":84721,\"start\":84720},{\"end\":84730,\"start\":84729},{\"end\":84738,\"start\":84737},{\"end\":85076,\"start\":85075},{\"end\":85083,\"start\":85082},{\"end\":85090,\"start\":85089},{\"end\":85099,\"start\":85098},{\"end\":85106,\"start\":85105},{\"end\":85113,\"start\":85112},{\"end\":85447,\"start\":85446},{\"end\":85456,\"start\":85455},{\"end\":85463,\"start\":85462},{\"end\":85757,\"start\":85756},{\"end\":85759,\"start\":85758},{\"end\":85772,\"start\":85771},{\"end\":86143,\"start\":86142},{\"end\":86151,\"start\":86150},{\"end\":86160,\"start\":86159},{\"end\":86168,\"start\":86167},{\"end\":86470,\"start\":86469},{\"end\":86478,\"start\":86477},{\"end\":86707,\"start\":86706},{\"end\":86717,\"start\":86716},{\"end\":86719,\"start\":86718},{\"end\":86727,\"start\":86726},{\"end\":86733,\"start\":86732},{\"end\":86742,\"start\":86741},{\"end\":86744,\"start\":86743},{\"end\":87003,\"start\":87002},{\"end\":87016,\"start\":87015},{\"end\":87018,\"start\":87017},{\"end\":87026,\"start\":87025},{\"end\":87034,\"start\":87033},{\"end\":87036,\"start\":87035},{\"end\":87319,\"start\":87318},{\"end\":87326,\"start\":87325},{\"end\":87332,\"start\":87331},{\"end\":87341,\"start\":87340},{\"end\":87349,\"start\":87348},{\"end\":87653,\"start\":87652},{\"end\":87660,\"start\":87659}]", "bib_author_last_name": "[{\"end\":69019,\"start\":69011},{\"end\":69027,\"start\":69023},{\"end\":69040,\"start\":69031},{\"end\":69053,\"start\":69046},{\"end\":69070,\"start\":69057},{\"end\":69380,\"start\":69374},{\"end\":69390,\"start\":69384},{\"end\":69402,\"start\":69394},{\"end\":69412,\"start\":69406},{\"end\":69425,\"start\":69418},{\"end\":69442,\"start\":69429},{\"end\":69837,\"start\":69829},{\"end\":69849,\"start\":69841},{\"end\":70175,\"start\":70171},{\"end\":70186,\"start\":70179},{\"end\":70196,\"start\":70190},{\"end\":70211,\"start\":70200},{\"end\":70533,\"start\":70531},{\"end\":70539,\"start\":70537},{\"end\":70546,\"start\":70543},{\"end\":70552,\"start\":70550},{\"end\":70559,\"start\":70556},{\"end\":70799,\"start\":70792},{\"end\":71068,\"start\":71060},{\"end\":71078,\"start\":71072},{\"end\":71086,\"start\":71082},{\"end\":71093,\"start\":71090},{\"end\":71484,\"start\":71476},{\"end\":71493,\"start\":71488},{\"end\":71829,\"start\":71819},{\"end\":71844,\"start\":71833},{\"end\":72075,\"start\":72072},{\"end\":72427,\"start\":72424},{\"end\":72742,\"start\":72735},{\"end\":72750,\"start\":72746},{\"end\":72762,\"start\":72754},{\"end\":72775,\"start\":72768},{\"end\":73136,\"start\":73129},{\"end\":73147,\"start\":73140},{\"end\":73157,\"start\":73151},{\"end\":73450,\"start\":73445},{\"end\":73462,\"start\":73454},{\"end\":73469,\"start\":73466},{\"end\":73479,\"start\":73473},{\"end\":73825,\"start\":73823},{\"end\":74074,\"start\":74068},{\"end\":74083,\"start\":74078},{\"end\":74090,\"start\":74087},{\"end\":74103,\"start\":74094},{\"end\":74447,\"start\":74444},{\"end\":74454,\"start\":74451},{\"end\":74461,\"start\":74458},{\"end\":74470,\"start\":74465},{\"end\":74476,\"start\":74474},{\"end\":74485,\"start\":74480},{\"end\":74794,\"start\":74783},{\"end\":74805,\"start\":74802},{\"end\":74814,\"start\":74809},{\"end\":74820,\"start\":74818},{\"end\":75055,\"start\":75050},{\"end\":75063,\"start\":75059},{\"end\":75074,\"start\":75067},{\"end\":75087,\"start\":75078},{\"end\":75318,\"start\":75313},{\"end\":75325,\"start\":75322},{\"end\":75332,\"start\":75329},{\"end\":75339,\"start\":75336},{\"end\":75346,\"start\":75343},{\"end\":75354,\"start\":75350},{\"end\":75672,\"start\":75670},{\"end\":75681,\"start\":75676},{\"end\":75688,\"start\":75685},{\"end\":75695,\"start\":75692},{\"end\":76043,\"start\":76038},{\"end\":76054,\"start\":76047},{\"end\":76388,\"start\":76381},{\"end\":76397,\"start\":76392},{\"end\":76409,\"start\":76401},{\"end\":76733,\"start\":76731},{\"end\":76739,\"start\":76737},{\"end\":76746,\"start\":76743},{\"end\":76752,\"start\":76750},{\"end\":76759,\"start\":76756},{\"end\":77027,\"start\":77024},{\"end\":77033,\"start\":77031},{\"end\":77040,\"start\":77037},{\"end\":77046,\"start\":77044},{\"end\":77053,\"start\":77050},{\"end\":77370,\"start\":77365},{\"end\":77385,\"start\":77374},{\"end\":77392,\"start\":77389},{\"end\":77401,\"start\":77396},{\"end\":77765,\"start\":77763},{\"end\":77771,\"start\":77769},{\"end\":77778,\"start\":77775},{\"end\":77784,\"start\":77782},{\"end\":77790,\"start\":77788},{\"end\":77797,\"start\":77794},{\"end\":78082,\"start\":78075},{\"end\":78094,\"start\":78086},{\"end\":78406,\"start\":78399},{\"end\":78662,\"start\":78656},{\"end\":78668,\"start\":78666},{\"end\":78858,\"start\":78848},{\"end\":78868,\"start\":78862},{\"end\":78882,\"start\":78872},{\"end\":78895,\"start\":78886},{\"end\":78912,\"start\":78899},{\"end\":79216,\"start\":79208},{\"end\":79438,\"start\":79433},{\"end\":79603,\"start\":79598},{\"end\":79615,\"start\":79609},{\"end\":79834,\"start\":79831},{\"end\":79841,\"start\":79838},{\"end\":79853,\"start\":79847},{\"end\":79859,\"start\":79857},{\"end\":79865,\"start\":79863},{\"end\":80183,\"start\":80177},{\"end\":80193,\"start\":80187},{\"end\":80203,\"start\":80197},{\"end\":80213,\"start\":80207},{\"end\":80543,\"start\":80537},{\"end\":80553,\"start\":80547},{\"end\":80563,\"start\":80557},{\"end\":80913,\"start\":80904},{\"end\":80920,\"start\":80917},{\"end\":80934,\"start\":80926},{\"end\":80946,\"start\":80938},{\"end\":81267,\"start\":81259},{\"end\":81279,\"start\":81271},{\"end\":81573,\"start\":81569},{\"end\":81586,\"start\":81579},{\"end\":81603,\"start\":81590},{\"end\":81933,\"start\":81930},{\"end\":82240,\"start\":82237},{\"end\":82249,\"start\":82244},{\"end\":82255,\"start\":82253},{\"end\":82261,\"start\":82259},{\"end\":82268,\"start\":82265},{\"end\":82276,\"start\":82272},{\"end\":82656,\"start\":82650},{\"end\":82668,\"start\":82662},{\"end\":82677,\"start\":82674},{\"end\":82999,\"start\":82995},{\"end\":83012,\"start\":83005},{\"end\":83029,\"start\":83016},{\"end\":83421,\"start\":83414},{\"end\":83430,\"start\":83425},{\"end\":83436,\"start\":83434},{\"end\":83448,\"start\":83440},{\"end\":83802,\"start\":83798},{\"end\":84095,\"start\":84091},{\"end\":84108,\"start\":84099},{\"end\":84395,\"start\":84386},{\"end\":84404,\"start\":84401},{\"end\":84710,\"start\":84707},{\"end\":84718,\"start\":84714},{\"end\":84727,\"start\":84722},{\"end\":84735,\"start\":84731},{\"end\":84742,\"start\":84739},{\"end\":85080,\"start\":85077},{\"end\":85087,\"start\":85084},{\"end\":85096,\"start\":85091},{\"end\":85103,\"start\":85100},{\"end\":85110,\"start\":85107},{\"end\":85119,\"start\":85114},{\"end\":85453,\"start\":85448},{\"end\":85460,\"start\":85457},{\"end\":85466,\"start\":85464},{\"end\":85769,\"start\":85760},{\"end\":85780,\"start\":85773},{\"end\":86148,\"start\":86144},{\"end\":86157,\"start\":86152},{\"end\":86165,\"start\":86161},{\"end\":86174,\"start\":86169},{\"end\":86475,\"start\":86471},{\"end\":86481,\"start\":86479},{\"end\":86714,\"start\":86708},{\"end\":86724,\"start\":86720},{\"end\":86730,\"start\":86728},{\"end\":86739,\"start\":86734},{\"end\":86752,\"start\":86745},{\"end\":87013,\"start\":87004},{\"end\":87023,\"start\":87019},{\"end\":87031,\"start\":87027},{\"end\":87043,\"start\":87037},{\"end\":87323,\"start\":87320},{\"end\":87329,\"start\":87327},{\"end\":87338,\"start\":87333},{\"end\":87346,\"start\":87342},{\"end\":87353,\"start\":87350},{\"end\":87657,\"start\":87654},{\"end\":87667,\"start\":87661}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9790585},\"end\":69301,\"start\":68941},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1129667},\"end\":69739,\"start\":69303},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8649673},\"end\":70111,\"start\":69741},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8820379},\"end\":70497,\"start\":70113},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49584534},\"end\":70761,\"start\":70499},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13756489},\"end\":70992,\"start\":70763},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11080756},\"end\":71378,\"start\":70994},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7164502},\"end\":71791,\"start\":71380},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1915014},\"end\":71974,\"start\":71793},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5590763},\"end\":72361,\"start\":71976},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9672033},\"end\":72669,\"start\":72363},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6728280},\"end\":73068,\"start\":72671},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14650762},\"end\":73363,\"start\":73070},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5201925},\"end\":73719,\"start\":73365},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b14\"},\"end\":73982,\"start\":73721},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52967399},\"end\":74380,\"start\":73984},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53670269},\"end\":74701,\"start\":74382},{\"attributes\":{\"doi\":\"arXiv:1910.12306\",\"id\":\"b17\"},\"end\":74994,\"start\":74703},{\"attributes\":{\"doi\":\"arXiv:1904.10509\",\"id\":\"b18\"},\"end\":75233,\"start\":74996},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":24413524},\"end\":75620,\"start\":75235},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206594692},\"end\":75940,\"start\":75622},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5808102},\"end\":76296,\"start\":75942},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":59606259},\"end\":76649,\"start\":76298},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":189927337},\"end\":76966,\"start\":76651},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202769028},\"end\":77300,\"start\":76968},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218486987},\"end\":77703,\"start\":77302},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":49584957},\"end\":78020,\"start\":77705},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8078153},\"end\":78349,\"start\":78022},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":57721315},\"end\":78493,\"start\":78351},{\"attributes\":{\"id\":\"b29\"},\"end\":78606,\"start\":78495},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b30\"},\"end\":78777,\"start\":78608},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6844431},\"end\":79144,\"start\":78779},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":123347451},\"end\":79361,\"start\":79146},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":120113824},\"end\":79592,\"start\":79363},{\"attributes\":{\"id\":\"b34\"},\"end\":79755,\"start\":79594},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":218718703},\"end\":80090,\"start\":79757},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7843537},\"end\":80468,\"start\":80092},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":10747140},\"end\":80812,\"start\":80470},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13109013},\"end\":81188,\"start\":80814},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":17406565},\"end\":81471,\"start\":81190},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14387543},\"end\":81863,\"start\":81473},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":204838350},\"end\":82176,\"start\":81865},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":218516704},\"end\":82574,\"start\":82178},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":146096516},\"end\":82896,\"start\":82576},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":18199179},\"end\":83354,\"start\":82898},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":214802082},\"end\":83710,\"start\":83356},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":215880650},\"end\":83999,\"start\":83712},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":84182572},\"end\":84334,\"start\":84001},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":49300848},\"end\":84634,\"start\":84336},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":174804316},\"end\":85005,\"start\":84636},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":208248351},\"end\":85385,\"start\":85007},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":69147736},\"end\":85689,\"start\":85387},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":209444596},\"end\":86057,\"start\":85691},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":210697756},\"end\":86391,\"start\":86059},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":221194036},\"end\":86672,\"start\":86393},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":2846066},\"end\":86963,\"start\":86674},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":2437629},\"end\":87228,\"start\":86965},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":1914494},\"end\":87588,\"start\":87230},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":12718048},\"end\":87927,\"start\":87590}]", "bib_title": "[{\"end\":69007,\"start\":68941},{\"end\":69370,\"start\":69303},{\"end\":69823,\"start\":69741},{\"end\":70167,\"start\":70113},{\"end\":70527,\"start\":70499},{\"end\":70788,\"start\":70763},{\"end\":71056,\"start\":70994},{\"end\":71472,\"start\":71380},{\"end\":71815,\"start\":71793},{\"end\":72068,\"start\":71976},{\"end\":72420,\"start\":72363},{\"end\":72731,\"start\":72671},{\"end\":73125,\"start\":73070},{\"end\":73441,\"start\":73365},{\"end\":74064,\"start\":73984},{\"end\":74440,\"start\":74382},{\"end\":75309,\"start\":75235},{\"end\":75666,\"start\":75622},{\"end\":76034,\"start\":75942},{\"end\":76377,\"start\":76298},{\"end\":76727,\"start\":76651},{\"end\":77020,\"start\":76968},{\"end\":77359,\"start\":77302},{\"end\":77759,\"start\":77705},{\"end\":78071,\"start\":78022},{\"end\":78395,\"start\":78351},{\"end\":78844,\"start\":78779},{\"end\":79202,\"start\":79146},{\"end\":79429,\"start\":79363},{\"end\":79827,\"start\":79757},{\"end\":80173,\"start\":80092},{\"end\":80533,\"start\":80470},{\"end\":80900,\"start\":80814},{\"end\":81253,\"start\":81190},{\"end\":81565,\"start\":81473},{\"end\":81926,\"start\":81865},{\"end\":82233,\"start\":82178},{\"end\":82644,\"start\":82576},{\"end\":82991,\"start\":82898},{\"end\":83410,\"start\":83356},{\"end\":83794,\"start\":83712},{\"end\":84087,\"start\":84001},{\"end\":84382,\"start\":84336},{\"end\":84703,\"start\":84636},{\"end\":85073,\"start\":85007},{\"end\":85444,\"start\":85387},{\"end\":85754,\"start\":85691},{\"end\":86140,\"start\":86059},{\"end\":86467,\"start\":86393},{\"end\":86704,\"start\":86674},{\"end\":87000,\"start\":86965},{\"end\":87316,\"start\":87230},{\"end\":87650,\"start\":87590}]", "bib_author": "[{\"end\":69021,\"start\":69009},{\"end\":69029,\"start\":69021},{\"end\":69042,\"start\":69029},{\"end\":69055,\"start\":69042},{\"end\":69072,\"start\":69055},{\"end\":69382,\"start\":69372},{\"end\":69392,\"start\":69382},{\"end\":69404,\"start\":69392},{\"end\":69414,\"start\":69404},{\"end\":69427,\"start\":69414},{\"end\":69444,\"start\":69427},{\"end\":69839,\"start\":69825},{\"end\":69851,\"start\":69839},{\"end\":70177,\"start\":70169},{\"end\":70188,\"start\":70177},{\"end\":70198,\"start\":70188},{\"end\":70213,\"start\":70198},{\"end\":70535,\"start\":70529},{\"end\":70541,\"start\":70535},{\"end\":70548,\"start\":70541},{\"end\":70554,\"start\":70548},{\"end\":70561,\"start\":70554},{\"end\":70801,\"start\":70790},{\"end\":71070,\"start\":71058},{\"end\":71080,\"start\":71070},{\"end\":71088,\"start\":71080},{\"end\":71095,\"start\":71088},{\"end\":71486,\"start\":71474},{\"end\":71495,\"start\":71486},{\"end\":71831,\"start\":71817},{\"end\":71846,\"start\":71831},{\"end\":72077,\"start\":72070},{\"end\":72429,\"start\":72422},{\"end\":72744,\"start\":72733},{\"end\":72752,\"start\":72744},{\"end\":72764,\"start\":72752},{\"end\":72777,\"start\":72764},{\"end\":73138,\"start\":73127},{\"end\":73149,\"start\":73138},{\"end\":73159,\"start\":73149},{\"end\":73452,\"start\":73443},{\"end\":73464,\"start\":73452},{\"end\":73471,\"start\":73464},{\"end\":73481,\"start\":73471},{\"end\":73827,\"start\":73821},{\"end\":74076,\"start\":74066},{\"end\":74085,\"start\":74076},{\"end\":74092,\"start\":74085},{\"end\":74105,\"start\":74092},{\"end\":74449,\"start\":74442},{\"end\":74456,\"start\":74449},{\"end\":74463,\"start\":74456},{\"end\":74472,\"start\":74463},{\"end\":74478,\"start\":74472},{\"end\":74487,\"start\":74478},{\"end\":74796,\"start\":74781},{\"end\":74807,\"start\":74796},{\"end\":74816,\"start\":74807},{\"end\":74822,\"start\":74816},{\"end\":75057,\"start\":75048},{\"end\":75065,\"start\":75057},{\"end\":75076,\"start\":75065},{\"end\":75089,\"start\":75076},{\"end\":75320,\"start\":75311},{\"end\":75327,\"start\":75320},{\"end\":75334,\"start\":75327},{\"end\":75341,\"start\":75334},{\"end\":75348,\"start\":75341},{\"end\":75356,\"start\":75348},{\"end\":75674,\"start\":75668},{\"end\":75683,\"start\":75674},{\"end\":75690,\"start\":75683},{\"end\":75697,\"start\":75690},{\"end\":76045,\"start\":76036},{\"end\":76056,\"start\":76045},{\"end\":76390,\"start\":76379},{\"end\":76399,\"start\":76390},{\"end\":76411,\"start\":76399},{\"end\":76735,\"start\":76729},{\"end\":76741,\"start\":76735},{\"end\":76748,\"start\":76741},{\"end\":76754,\"start\":76748},{\"end\":76761,\"start\":76754},{\"end\":77029,\"start\":77022},{\"end\":77035,\"start\":77029},{\"end\":77042,\"start\":77035},{\"end\":77048,\"start\":77042},{\"end\":77055,\"start\":77048},{\"end\":77372,\"start\":77361},{\"end\":77387,\"start\":77372},{\"end\":77394,\"start\":77387},{\"end\":77403,\"start\":77394},{\"end\":77767,\"start\":77761},{\"end\":77773,\"start\":77767},{\"end\":77780,\"start\":77773},{\"end\":77786,\"start\":77780},{\"end\":77792,\"start\":77786},{\"end\":77799,\"start\":77792},{\"end\":78084,\"start\":78073},{\"end\":78096,\"start\":78084},{\"end\":78408,\"start\":78397},{\"end\":78664,\"start\":78652},{\"end\":78670,\"start\":78664},{\"end\":78860,\"start\":78846},{\"end\":78870,\"start\":78860},{\"end\":78884,\"start\":78870},{\"end\":78897,\"start\":78884},{\"end\":78914,\"start\":78897},{\"end\":79218,\"start\":79204},{\"end\":79440,\"start\":79431},{\"end\":79605,\"start\":79596},{\"end\":79617,\"start\":79605},{\"end\":79836,\"start\":79829},{\"end\":79843,\"start\":79836},{\"end\":79855,\"start\":79843},{\"end\":79861,\"start\":79855},{\"end\":79867,\"start\":79861},{\"end\":80185,\"start\":80175},{\"end\":80195,\"start\":80185},{\"end\":80205,\"start\":80195},{\"end\":80215,\"start\":80205},{\"end\":80545,\"start\":80535},{\"end\":80555,\"start\":80545},{\"end\":80565,\"start\":80555},{\"end\":80915,\"start\":80902},{\"end\":80922,\"start\":80915},{\"end\":80936,\"start\":80922},{\"end\":80948,\"start\":80936},{\"end\":81269,\"start\":81255},{\"end\":81281,\"start\":81269},{\"end\":81575,\"start\":81567},{\"end\":81588,\"start\":81575},{\"end\":81605,\"start\":81588},{\"end\":81935,\"start\":81928},{\"end\":82242,\"start\":82235},{\"end\":82251,\"start\":82242},{\"end\":82257,\"start\":82251},{\"end\":82263,\"start\":82257},{\"end\":82270,\"start\":82263},{\"end\":82278,\"start\":82270},{\"end\":82658,\"start\":82646},{\"end\":82670,\"start\":82658},{\"end\":82679,\"start\":82670},{\"end\":83001,\"start\":82993},{\"end\":83014,\"start\":83001},{\"end\":83031,\"start\":83014},{\"end\":83423,\"start\":83412},{\"end\":83432,\"start\":83423},{\"end\":83438,\"start\":83432},{\"end\":83450,\"start\":83438},{\"end\":83804,\"start\":83796},{\"end\":84097,\"start\":84089},{\"end\":84110,\"start\":84097},{\"end\":84397,\"start\":84384},{\"end\":84406,\"start\":84397},{\"end\":84712,\"start\":84705},{\"end\":84720,\"start\":84712},{\"end\":84729,\"start\":84720},{\"end\":84737,\"start\":84729},{\"end\":84744,\"start\":84737},{\"end\":85082,\"start\":85075},{\"end\":85089,\"start\":85082},{\"end\":85098,\"start\":85089},{\"end\":85105,\"start\":85098},{\"end\":85112,\"start\":85105},{\"end\":85121,\"start\":85112},{\"end\":85455,\"start\":85446},{\"end\":85462,\"start\":85455},{\"end\":85468,\"start\":85462},{\"end\":85771,\"start\":85756},{\"end\":85782,\"start\":85771},{\"end\":86150,\"start\":86142},{\"end\":86159,\"start\":86150},{\"end\":86167,\"start\":86159},{\"end\":86176,\"start\":86167},{\"end\":86477,\"start\":86469},{\"end\":86483,\"start\":86477},{\"end\":86716,\"start\":86706},{\"end\":86726,\"start\":86716},{\"end\":86732,\"start\":86726},{\"end\":86741,\"start\":86732},{\"end\":86754,\"start\":86741},{\"end\":87015,\"start\":87002},{\"end\":87025,\"start\":87015},{\"end\":87033,\"start\":87025},{\"end\":87045,\"start\":87033},{\"end\":87325,\"start\":87318},{\"end\":87331,\"start\":87325},{\"end\":87340,\"start\":87331},{\"end\":87348,\"start\":87340},{\"end\":87355,\"start\":87348},{\"end\":87659,\"start\":87652},{\"end\":87669,\"start\":87659}]", "bib_venue": "[{\"end\":69088,\"start\":69084},{\"end\":69522,\"start\":69487},{\"end\":69929,\"start\":69894},{\"end\":70311,\"start\":70266},{\"end\":70629,\"start\":70599},{\"end\":70877,\"start\":70845},{\"end\":71193,\"start\":71148},{\"end\":71569,\"start\":71536},{\"end\":72171,\"start\":72130},{\"end\":72523,\"start\":72482},{\"end\":72875,\"start\":72830},{\"end\":73211,\"start\":73189},{\"end\":73547,\"start\":73518},{\"end\":74145,\"start\":74129},{\"end\":74523,\"start\":74509},{\"end\":75785,\"start\":75745},{\"end\":76118,\"start\":76091},{\"end\":76471,\"start\":76445},{\"end\":77133,\"start\":77100},{\"end\":77513,\"start\":77462},{\"end\":77847,\"start\":77827},{\"end\":78194,\"start\":78149},{\"end\":78418,\"start\":78414},{\"end\":80279,\"start\":80251},{\"end\":80643,\"start\":80608},{\"end\":81665,\"start\":81639},{\"end\":82027,\"start\":81985},{\"end\":82388,\"start\":82337},{\"end\":83131,\"start\":83085},{\"end\":83540,\"start\":83499},{\"end\":84144,\"start\":84131},{\"end\":84488,\"start\":84451},{\"end\":84822,\"start\":84787},{\"end\":85199,\"start\":85164},{\"end\":85528,\"start\":85502},{\"end\":85882,\"start\":85836},{\"end\":86192,\"start\":86188},{\"end\":86507,\"start\":86499},{\"end\":86814,\"start\":86788},{\"end\":87061,\"start\":87057},{\"end\":87391,\"start\":87377},{\"end\":87767,\"start\":87722},{\"end\":69082,\"start\":69072},{\"end\":69485,\"start\":69444},{\"end\":69892,\"start\":69851},{\"end\":70264,\"start\":70213},{\"end\":70597,\"start\":70561},{\"end\":70843,\"start\":70801},{\"end\":71146,\"start\":71095},{\"end\":71534,\"start\":71495},{\"end\":71859,\"start\":71846},{\"end\":72128,\"start\":72077},{\"end\":72480,\"start\":72429},{\"end\":72828,\"start\":72777},{\"end\":73187,\"start\":73159},{\"end\":73516,\"start\":73481},{\"end\":73819,\"start\":73721},{\"end\":74127,\"start\":74105},{\"end\":74507,\"start\":74487},{\"end\":74779,\"start\":74703},{\"end\":75046,\"start\":74996},{\"end\":75396,\"start\":75356},{\"end\":75743,\"start\":75697},{\"end\":76089,\"start\":76056},{\"end\":76443,\"start\":76411},{\"end\":76781,\"start\":76761},{\"end\":77098,\"start\":77055},{\"end\":77460,\"start\":77403},{\"end\":77825,\"start\":77799},{\"end\":78147,\"start\":78096},{\"end\":78412,\"start\":78408},{\"end\":78537,\"start\":78497},{\"end\":78650,\"start\":78608},{\"end\":78933,\"start\":78914},{\"end\":79231,\"start\":79218},{\"end\":79453,\"start\":79440},{\"end\":79644,\"start\":79617},{\"end\":79898,\"start\":79867},{\"end\":80249,\"start\":80215},{\"end\":80606,\"start\":80565},{\"end\":80970,\"start\":80948},{\"end\":81303,\"start\":81281},{\"end\":81637,\"start\":81605},{\"end\":81983,\"start\":81935},{\"end\":82335,\"start\":82278},{\"end\":82707,\"start\":82679},{\"end\":83083,\"start\":83031},{\"end\":83497,\"start\":83450},{\"end\":83826,\"start\":83804},{\"end\":84129,\"start\":84110},{\"end\":84449,\"start\":84406},{\"end\":84785,\"start\":84744},{\"end\":85162,\"start\":85121},{\"end\":85500,\"start\":85468},{\"end\":85834,\"start\":85782},{\"end\":86186,\"start\":86176},{\"end\":86497,\"start\":86483},{\"end\":86786,\"start\":86754},{\"end\":87055,\"start\":87045},{\"end\":87375,\"start\":87355},{\"end\":87720,\"start\":87669}]"}}}, "year": 2023, "month": 12, "day": 17}