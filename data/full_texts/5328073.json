{"id": 5328073, "updated": "2023-08-03 15:20:48.173", "metadata": {"title": "SMPL: A Skinned Multi-Person Linear Model", "authors": "[{\"first\":\"Matthew\",\"last\":\"Loper\",\"middle\":[]},{\"first\":\"Naureen\",\"last\":\"Mahmood\",\"middle\":[]},{\"first\":\"Javier\",\"last\":\"Romero\",\"middle\":[]},{\"first\":\"Gerard\",\"last\":\"Pons-Moll\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Black\",\"middle\":[\"J.\"]}]", "venue": "SIGGRAPH 2015", "journal": "Seminal Graphics Papers: Pushing the Boundaries, Volume 2", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We present a learned model of human body shape and posedependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model (SMPL) is a skinned vertexbased model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of SMPL using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend- SCAPE model trained on the same data. We also extend SMPL to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, SMPL is compatible with existing rendering engines and we make it available for research purposes.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "1967554269", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/LoperM0PB15", "doi": "10.1145/3596711.3596800"}}, "content": {"source": {"pdf_hash": "058f958a104c151f441f4e31a2e099ceb933a8a3", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9e6e2e8264d05cf6de518dbda999c1a0f1f10be1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/058f958a104c151f441f4e31a2e099ceb933a8a3.txt", "contents": "\nSMPL: A Skinned Multi-Person Linear Model\n2015. November 2015\n\nAcm Reference Format \nM Loper \nN Mahmood \nJ Romero \nG Pons-Moll \nM J Black \n\nIndustrial Light and Magic\nSan FranciscoCA\n\nSMPL: A Skinned Multi-Person Linear Model\n\nACM Trans. Graph\n342015. November 2015Technical Paper, November 02 -05, 2015, Kobe, Japan. ACM 978-1-4503-3931-5/15/11.CR Categories: I33 [Computer Graphics]: Three-Dimensional Graphics and Realism-Animation Keywords: Body shapeskinningblendshapessoft-tissue \u21e4\nFigure 1: SMPL is a realistic learned model of human body shape and pose that is compatible with existing rendering engines, allows animator control, and is available for research purposes. (left) SMPL model (orange) fit to ground truth 3D meshes (gray). (right) Unity 5.0 game engine screenshot showing bodies from the CAESAR dataset animated in real time.AbstractWe present a learned model of human body shape and posedependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model (SMPL) is a skinned vertexbased model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of SMPL using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-SCAPE model trained on the same data. We also extend SMPL to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, SMPL is compatible with existing rendering engines and we make it available for research purposes.\n\nIntroduction\n\nOur goal is to create realistic animated human bodies that can represent different body shapes, deform naturally with pose, and exhibit soft-tissue motions like those of real humans. We want such models to be fast to render, easy to deploy, and compatible with existing rendering engines. The commercial approach commonly involves hand rigging a mesh and manually sculpting blend shapes to correct problems with traditional skinning methods. Many blend shapes are typically needed and the manual effort required to build them is large. As an alternative, the research community has focused on learning statistical body models from example scans of different bodies in a varied set of poses. While promising, these approaches are not compatible with existing graphics software and rendering engines that use standard skinning methods.\n\nOur goal is to automatically learn a model of the body that is both realistic and compatible with existing graphics software. To that end, we describe a \"Skinned Multi-Person Linear\" (SMPL) model of the human body that can realistically represent a wide range of human body shapes, can be posed with natural pose-dependent deformations, exhibits soft-tissue dynamics, is efficient to animate, and is compatible with existing rendering engines (Fig. 1).\n\nTraditional methods model how vertices are related to an underlying skeleton structure. Basic linear blend skinning (LBS) models are the most widely used, are supported by all game engines, and are efficient to render. Unfortunately they produce unrealistic deformations at joints including the well-known \"taffy\" and \"bowtie\" effects (see Fig. 2). Tremendous work has gone into skinning methods that ameliorate these effects [Lewis et al. 2000;Wang and Phillips 2002;Kavan and\u017d\u00e1ra 2005;Merry et al. 2006;Kavan et al. 2008]. There has also been a lot of work on learning highly realistic body models from data [Allen et al. 2006;Anguelov et al. 2005;Freifeld and Black 2012;Hasler et al. 2010;Chang and Zwicker 2009;Chen et al. 2013]. These methods can capture the body shape of many people as well as non-rigid deformations due to pose. The most realistic approaches are arguably based on triangle deformations [Anguelov et al. 2005;Chen et al. 2013;Hasler et al. 2010;Pons-Moll et al. 2015]. Despite the above research, existing mod- The far right (light gray) mesh is a 3D scan. Next to it (dark gray) is a registered mesh with the same topology as our model. We ask how well different models can approximate this registration. From left to right: (light green) Linear blend skinning (LBS), (dark green) Dualquaternion blend skinning (DQBS), (blue) BlendSCAPE, (red) SMPL-LBS, (orange) SMPL-DQBS. The zoomed regions highlight differences between the models at the subject's right elbow and hip. LBS and DQBS produce serious artifacts at the knees, elbows, shoulders and hips. BlendSCAPE and both SMPL models do similarly well at fitting the data. els either lack realism, do not work with existing packages, do not represent a wide variety of body shapes, are not compatible with standard graphics pipelines, or require significant manual labor.\n\nIn contrast to the previous approaches, a key goal of our work is to make the body model as simple and standard as possible so that it can be widely used, while, at the same time, keeping the realism of deformation-based models learned from data. Specifically we learn blend shapes to correct for the limitations of standard skinning. Different blend shapes for identity, pose, and soft-tissue dynamics are additively combined with a rest template before being transformed by blend skinning. A key component of our approach is that we formulate the pose blend shapes as a linear function of the elements of the part rotation matrices. This formulation is different from previous methods [Allen et al. 2006;Merry et al. 2006;Wang and Phillips 2002] and makes training and animating with the blend shapes simple. Because the elements of rotation matrices are bounded, so are the resulting deformations, helping our model generalize better.\n\nOur formulation admits an objective function that penalizes the pervertex disparities between registered meshes and our model, enabling training from data. To learn how people deform with pose, we use 1786 high-resolution 3D scans of different subjects in a wide variety of poses. We align our template mesh to each scan to create a training set. We optimize the blend weights, pose-dependent blend shapes, the mean template shape (rest pose), and a regressor from shape to joint locations to minimize the vertex error of the model on the training set. This joint regressor predicts the location of the joints as a function of the body shape and is critical to animating realistic pose-dependent deformations for any body shape. All parameters are automatically estimated from the aligned scans.\n\nWe learn linear models of male and female body shape from the CAESAR dataset [Robinette et al. 2002] (approximately 2000 scans per gender) using principal component analysis (PCA). We first register a template mesh to each scan and pose normalize the data, which is critical when learning a vertex-based shape model. The resulting principal components become body shape blend shapes.\n\nWe train the SMPL model in various forms and compare it quantitatively to a BlendSCAPE model [Hirshberg et al. 2012] trained with exactly the same data. We evaluate the models both qualitatively with animations and quantitatively using meshes that were not used for training. We fit SMPL and BlendSCAPE to these meshes and then compare the vertex errors. Two main variants of SMPL are explored, one using linear blend skinning (LBS) and the other with Dual-Quaternion blend skinning (DQBS); see Fig. 2. The surprise is that a vertex-based, skinned, model such as SMPL is actually more accurate than a deformation-based model like BlendSCAPE trained on the same data. The test meshes are available for research purposes so others can quantitatively compare to SMPL.\n\nWe extend the SMPL model to capture soft-tissue dynamics by adapting the Dyna model [Pons-Moll et al. 2015]. The resulting Dynamic-SMPL, or DMPL model, is trained from the same dataset of 4D meshes as Dyna. DMPL, however, is based on vertices instead of triangle deformations. We compute vertex errors between SMPL and Dyna training meshes, transformed into the rest pose, and use PCA to reduce the dimensionality, producing dynamic blend shapes. We then train a soft-tissue model based on angular velocities and accelerations of the parts and the history of dynamic deformations as in [Pons-Moll et al. 2015]. Since soft-tissue dynamics strongly depend on body shape, we train DMPL using bodies of varying body mass index and learn a model of dynamic deformations that depends of body shape. Animating soft-tissue dynamics in a standard rendering engine simply requires computing the dynamic linear blend shape coefficients from the sequence of poses. Side-by-side animations of Dyna and DMPL reveal that DMPL is more realistic. This extension of SMPL illustrates the generality of our additive blend shape approach, shows how deformations can depend on body shape, and demonstrates how the approach provides a extensible foundation for modeling body shape. SMPL models can be animated significantly faster than real time on a CPU using standard rendering engines. Consequently SMPL addresses an open problem in the field; it makes a realistic learned model accessible to animators. The SMPL base template is de-signed with animation in mind; it has a low-polygon count, a simple vertex topology, clean quad structure, a standard rig, and reasonable face and hand detail (though we do not rig the hands or face here). SMPL can be represented as an Autodesk Filmbox (FBX) file that can be imported into animation systems. We make the SMPL model available for research purposes and provide scripts to drive our model in Maya, Blender, Unreal Engine and Unity.\n\n\nRelated Work\n\nLinear blend skinning and blend shapes are widely used throughout the animation industry. While the research community has proposed many novel models of articulated body shape that produce high-quality results, they are not compatible with industry practice. Many authors have tried to bring these worlds together with varying degrees of success as we summarize below.\n\nBlend Skinning. Skeleton subspace deformation methods, also known as blend skinning, attach the surface of a mesh to an underlying skeletal structure. Each vertex in the mesh surface is transformed using a weighted influence of its neighboring bones. This influence can be defined linearly as in Linear Blend Skinning (LBS). The problems of LBS have been widely published and the literature is dense with generic methods that attempt to fix these, such as quaternion or dual-quaternion skinning, spherical skinning, etc. (e.g. [Wang and Phillips 2002;Kavan and\u017d\u00e1ra 2005;Kavan et al. 2008;Le and Deng 2012;Wang et al. 2007]). Generic methods, however, often produce unnatural results and here we focus on learning to correct the limitations of blend skinning, regardless of the particular formulation.\n\nAuto-rigging. There is a great deal of work on automatically rigging LBS models (e.g. [De Aguiar et al. 2008; Baran and Popovi\u0107 2007;Corazza and Gambaretto 2014;Schaefer and Yuksel 2007]) and commercial solutions exist. Most relevant here are methods that take a collection of meshes and infer the bones as well as the joints and blend weights (e.g. [Le and Deng 2014]). Such methods do not address the common problems of LBS models because they do not learn corrective blend shapes. Models created from sequences of meshes (e.g. [De Aguiar et al. 2008]) may not generalize well to new poses and motions. Here, we assume the kinematic structure is known, though the approach could be extended to also learn this using the methods above.\n\nThe key limitation of the above methods is that the models do not span a space of body shapes. Miller et al. [2010] partially address this by auto-rigging using a database of pre-rigged models. They formulate rigging and skinning as the process of transferring and adapting skinning weights from known models to a new model. Their method does not generate blend shapes, produces standard LBS artifacts, and does not minimize a clear objective function.\n\nBlend shapes. To address the shortcomings of basic blend skinning, the pose space deformation model (PSD) [Lewis et al. 2000] defines deformations (as vertex displacements) relative to a base shape, where these deformations are a function of articulated pose. This is the key formulation that is largely followed by later approaches and is also referred to as \"scattered data interpolation\" and \"corrective enveloping\" [Rouet and Lewis 1999]. We take an approach more similar to weighted pose-space deformation (WPSD) [Kurihara and Miyata 2004;Rhee et al. 2006], which defines the corrections in a rest pose and then applies a standard skinning equation (e.g. LBS). The idea is to define corrective shapes (sculpts) for specific key poses, so that when added to the base shape and transformed by blend skinning, produce the right shape. Typically one finds the distance (in pose space) to the exemplar poses and uses a function, e.g. a Radial Basis (RBF) kernel [Lewis et al. 2000], to weight the exemplars non-linearly based on distance. The sculpted blend shapes are then weighted and linearly combined.\n\nThese approaches are all based on computing weighted distances to exemplar shapes. Consequently, these methods require computation of the distances and weights at runtime to obtain the corrective blend shape. For a given animation (e.g. in a video game) these weights are often defined in advance based on the poses and \"baked\" into the model. Game engines apply the baked-in weights to the blend shapes. The sculpting process is typically done by an artist and then only for poses that will be used in the animation.\n\nLearning pose models. Allen et al. [2002] use this PSD approach but rather than hand-sculpt the corrections, learn them from registered 3D scans. Their work focuses primarily on modeling the torso and arms of individuals, rather than whole bodies of a population. They store deformations of key poses and interpolate between them. When at, or close to, a stored shape, these methods are effectively perfect. They do not tend to generalize well to new poses, requiring dense training data. It is not clear how many such shapes would be necessary to model the full range of articulated human pose. As the complexity of the model increases, so does the complexity of controlling all these shapes and how they interact.\n\nTo address this, Kry et al. [2002] learn a low-dimensional PCA basis for each joint's deformations. Pose-dependent deformations are described in terms of the coefficients of the basis vectors. Kavan et al. [2009] use example meshes generated using a non-linear skinning method to construct linear approximations. James and Twigg [2005] combine the idea of learning the bones (non-rigid, affine bones) and skinning weights directly from registered meshes. For blend shapes they use an approach similar to [Kry et al. 2002].\n\nAnother way to address the limitations of blend skinning is through \"multi-weight enveloping\" (MWE) [Wang and Phillips 2002]. Rather than weight each vertex by a weighted combination of the bone transformation matrices, MWE learns weights for the elements of these matrices. This increases the capacity of the model (more parameters). Like [James and Twigg 2005] they overparameterize the bone transformations to give more expressive power and then use PCA to remove unneeded degrees of freedom. Their experiments typically involve user interaction and the MWE approach is not supported by current game engines. Merry et al. [2006] find MWE to be overparameterized, because it allows vertices to deform differently depending on rotation in the global coordinate system. Their Animation Space model reduces the parameterization at minimal loss of representational power, while also showing computational efficiency on par with LBS.\n\nAnother alternative is proposed by Mohr and Gleicher [2003] who learn an efficient linear and realistic model from example meshes. To deal with the problems of LBS, however, they introduce extra \"bones\" to capture effects like muscle bulging. These extra bones increase complexity, are non-physical, and are non-intuitive for artists. Our blend shapes are simpler, more intuitive, more practical, and offer greater realism. Similarly, Wang et al. [2007] introduce joints related to surface deformation. Their rotational regression approach uses deformation gradients, which then must be converted to a vertex representation.\n\nLearning pose and shape models. The above methods focus on learning poseable single-shape models. Our goal, however, is to have realistic poseable models that cover the space of human shape variation. Early methods use PCA to characterize a space of human body shapes [Allen et al. 2003;Seo et al. 2003] but do not model how body shape changes with pose. The most successful class of models are based on SCAPE [Anguelov et al. 2005] and represent body shape and pose-dependent shape in terms of triangle deforma-tions rather than vertex displacements [Chen et al. 2013;Freifeld and Black 2012;Hasler et al. 2009;Hirshberg et al. 2012;Pons-Moll et al. 2015]. These methods learn statistical models of shape variation from training scans containing different body shapes and poses. Triangle deformations provide allow the composition of different transformations such as body shape variation, rigid part rotation, and pose-dependent deformation. Weber et al. [2007] present an approach that has properties of SCAPE but blends this with example shapes. These models are not consistent with existing animation software. Hasler et al. [2010] learn two linear blend rigs: one for pose and one for body shape. To represent shape change, they introduce abstract \"bones\" that control the shape change of the vertices. Animating a character of a particular shape involves manipulating the shape and pose bones. They learn a base mesh and blend weights but not blend shapes. Consequently the model lacks realism.\n\nWhat we would like is a vertex-based model that has the expressive power of the triangle deformation models so that it can capture a whole range of natural shapes and poses. Allen et al. [2006] formulate such a model. For a given base body shape, they define a standard LBS model with scattered/exemplar PSD to model pose deformations (using RBFs). They greedily define \"key angles\" at which to represent corrective blend shapes and they hold these fixed across all body shapes. A given body shape is parameterized by the vertices of the rest pose, corrective blend shapes (at the key angles), and bone lengths; these comprise a \"character vector.\" Given different character vectors for different bodies they learn a low-dimensional latent space that lets them generalize character vectors to new body shapes; they learn these parameters from data. Their model is more complex than ours, has fewer parameters, and is learned from much less data. A more detailed analysis of how this method compares to SMPL is presented in Sec. 7.\n\n\nModel Formulation\n\nOur Skinned Multi-Person Linear model (SMPL) is illustrated in Fig. 3. Like SCAPE, the SMPL model decomposes body shape into identity-dependent shape and non-rigid pose-dependent shape; unlike SCAPE, we take a vertex-based skinning approach that uses corrective blend shapes. A single blend shape is represented as a vector of concatenated vertex offsets. We begin with an artistcreated mesh with N = 6890 vertices and K = 23 joints. The mesh has the same topology for men and women, spatially varying resolution, a clean quad structure, a segmentation into parts, initial blend weights, and a skeletal rig. The part segmentation and initial blend weights are shown in Fig. 6.\n\nFollowing standard skinning practice, the model is defined by a mean template shape represented by a vector of N concatenated verticesT 2 R 3N in the zero pose,\u2713 \u21e4 ; a set of blend weights, W 2 R N \u21e5K , (Fig. 3(a)); a blend shape function, BS(~ ) : R |~ | 7 ! R 3N , that takes as input a vector of shape parameters,~ , ( Fig. 3(b)) and outputs a blend shape sculpting the subject identity; a function to predict K joint locations (white dots in Fig. 3\n(b)), J(~ ) : R |~ | 7 ! R 3K\nas a function of shape parameters,~ ; and a pose-dependent blend shape function, BP (\u2713) : R |\u2713| 7 ! R 3N , that takes as input a vector of pose parameters,\u2713, and accounts for the effects of posedependent deformations (Fig. 3(c)). The corrective blend shapes of these functions are added together in the rest pose as illustrated in (Fig. 3(c)). Finally, a standard blend skinning function W (\u00b7) (linear or dual-quaternion) is applied to rotate the vertices around the estimated joint centers with smoothing defined by the blend weights. The result is a model, M (~ ,\u2713; ) : R |\u2713|\u21e5|~ | 7 ! R 3N , that maps shape and pose parameters to vertices ( Fig. 3(d)). Here represents the learned model parameters described below.\n\nBelow we will use both LBS and DQBS skinning methods. In general the skinning method can be thought of as a generic black box. Given a particular skinning method our goal is to learn to correct for limitations of the method so as to model training meshes. Note that the learned pose blend shapes both correct errors caused by the blend skinning function and static soft-tissue deformations caused by changes in pose.\n\nBelow we describe each term in the model. For convenience, a notational summary is provided in Table 1 in the Appendix.\n\nBlend skinning. To fix ideas and define notation, we present the LBS version as it makes exposition clear (the DQBS version of SMPL only requires changing the skinning equation). Meshes and blend shapes are vectors of vertices represented by bold capital letters (e.g. X) and lowercase bold letters (e.g. x i 2 R 3 ) are vectors representing a particular vertex. The vertices are sometimes represented in homogeneous coordinates. We use the same notation for a vertex whether it is in standard or homogeneous coordinates as it should always be clear from the context which form is needed.\n\nThe pose of the body is defined by a standard skeletal rig, wher\u1ebd ! k 2 R 3 denotes the axis-angle representation of the relative rotation of part k with respect to its parent in the kinematic tree. Our rig has K = 23 joints, hence a pose\u2713 = [! T 0 , . . . ,! T K ] T is defined by |\u2713| = 3 \u21e5 23 + 3 = 72 parameters; i.e. 3 for each part plus 3 for the root orientation. Let! =! k! k denote the unit norm axis of rotation. Then the axis angle for every joint j is transformed to a rotation matrix using the Rodrigues formula\nexp(!j) = I + b !j sin(k!jk) + b ! 2 j cos(k!jk)(1)\nwhere b ! is the skew symmetric matrix of the 3-vector! and I is the 3 \u21e5 3 identity matrix. Using this, the standard linear blend skinning function W (T, J,\u2713, W) : R 3N \u21e53K\u21e5|\u2713|\u21e5|W| 7 ! R 3N takes vertices in the rest pose,T, joint locations, J, a pose,\u2713, and the blend weights, W, and returns the posed vertices. Each vertexti inT is transformed intot 0 i (both column vectors in homogeneous coordinates) ast\n0 i = K X k=1 w k,i G 0 k (\u2713, J)ti (2) G 0 k (\u2713, J) = G k (\u2713, J)G k (\u2713 \u21e4 , J) 1 (3) G k (\u2713, J) = Y j2A(k) \uf8ff exp(!j) jj 0 1(4)\nwhere w k,i is an element of the blend weight matrix W, representing how much the rotation of part k effects the vertex i, exp(\u2713j) is the local 3 \u21e5 3 rotation matrix corresponding to joint j, G k (\u2713, J) is the world transformation of joint k, and G 0 k (\u2713, J) is the same transformation after removing the transformation due to the rest pose, \u2713 \u21e4 . Each 3-element vector in J corresponding to a single joint center, j, is denoted jj. Finally, A(k) denotes the ordered set of joint ancestors of joint k. Note, for compatibility with existing rendering engines, we assume W is sparse and allow at most four parts to influence a vertex.\n\nMany methods have modified equation (2) to make skinning more expressive. For example MWE [Wang and Phillips 2002] replaces G k (\u2713, J) with a more general affine transformation matrix and replaces the scalar weight with a separate weight for every element of (a)T, W To maintain compatibility, we keep the basic skinning function and instead modify the template in an additive way and learn a function to predict joint locations. Our model,\n(b)T + B S (~ ), J(~ ) (c) T P (~ ,\u2713) =T+B S (~ )+B P (\u2713) (d) W (T P (~ ,\u2713), J(~ ),\u2713, W)M (~ ,\u2713; ) is then M (~ ,\u2713) = W (TP (~ ,\u2713), J(~ ),\u2713, W) (5) TP (~ ,\u2713) =T + BS(~ ) + BP (\u2713)(6)\nwhere BS(~ ) and BP (\u2713) are vectors of vertices representing offsets from the template. We refer to these as shape and pose blend shapes respectively.\n\nGiven this definition, a vertexti is transformed according t\u014d\nt 0 i = K X k=1 w k,i G 0 k (\u2713, J(~ ))(ti + bS,i(~ ) + bP,i(\u2713))(7)\nwhere bS,i(~ ), bP,i(\u2713) are vertices in BS(~ ) and BP (\u2713) respectively and represent the shape and pose blend shape offsets for the vertexti. Hence, the joint centers are now a function of body shape and the template mesh that is deformed by blend skinning is now a function of both pose and shape. Below we describe each term in detail.\n\nShape blend shapes. The body shapes of different people are represented by a linear function BS Notationally, the values to the right of a semicolon represent learned parameters, while those on the left are parameters set by an animator. For notational convenience, we often omit the learned parameters when they are not explicitly being optimized in training. Figure 3(b) illustrates the application of these shape blend shapes to the templateT to produce a new body shape.\nBS(~ ; S) = |~ | X n=1 nSn (8) where~ = [ 1, . . . , |~ | ] T ,\nPose blend shapes. We denote as R : R |\u2713| 7 ! R 9K a function that maps a pose vector\u2713 to a vector of concatenated part relative rotation matrices, exp(!). Given that our rig has 23 joints, R(\u2713) is a vector of length (23 \u21e5 9 = 207). Elements of R(\u2713) are functions of sines and cosines (Eq. (1)) of joint angles and therefore R(\u2713) is non-linear with\u2713.\n\nOur formulation differs from previous work in that we define the effect of the pose blend shapes to be linear in R \u21e4 (\u2713) = (R(\u2713) R(\u2713 \u21e4 )), where\u2713 \u21e4 denotes the rest pose. Let Rn(\u2713) denote the n th element of R(\u2713), then the vertex deviations from the rest template are\nBP (\u2713; P) = 9K X n=1 (Rn(\u2713) Rn(\u2713 \u21e4 ))Pn,(9)\nwhere the blend shapes, Pn 2 R 3N , are again vectors of vertex displacements. Here P = [P1, . . . , P9K ] 2 R 3N \u21e59K is a matrix of all 207 pose blend shapes. In this way, the pose blend shape function BP (\u2713; P) is fully defined by the matrix P, which we learn in Sec. 4.\n\nNote that subtracting the rest pose rotation vector, R(\u2713 \u21e4 ), guarantees that the contribution of the pose blend shapes is zero in the rest pose, which is important for animation. Joint locations. Different body shapes have different joint locations. Each joint is represented by its 3D location in the rest pose. It is critical that these are accurate, otherwise there will be artifacts when the model is posed using the skinning equation. For that reason, we define the joints as a function of the body shape,~ ,\nJ(~ ; J ,T, S) = J (T + BS(~ ; S))(10)\nwhere J is a matrix that transforms rest vertices into rest joints. We learn the regression matrix, J , from examples of different people in many poses, as part of our overall model learning in Sec. 4. This matrix models which mesh vertices are important and how to combine them to estimate the joint locations.\n\nSMPL model. We can now specify the full set of model parameters of the SMPL model as = T , W, S, J , P . We describe how to learn these in Sec. 4. Once learned they are held fixed and new \n\nand hence each vertex is transformed as\nt 0 i = K X k=1 w k,i G 0 k (\u2713, J(~ ; J ,T, S))tP,i(~ ,\u2713;T, S, P) (12)\nwhere tP,i(~ ,\u2713;T, S, P) =\nt i + |~ | X m=1 msm,i + 9K X n=1 (Rn(\u2713) Rn(\u2713 \u21e4 ))pn,i(13)\nrepresents the vertex i after applying the blend shapes and where sm,i, pn,i 2 R 3 are the elements of the shape and pose blend shapes corresponding to template vertexti.\n\nBelow we experiment with both LBS and DQBS and train the parameters for each. We refer to these models as SMPL-LBS and SMPL-DQBS; SMPL-DQBS is our default model, and we use SMPL as shorthand to mean SMPL-DQBS.\n\n\nTraining\n\nWe train the SMPL model parameters to minimize reconstruction error on two datasets. Each dataset contains meshes with the same topology as our template that have been aligned to high-resolution 3D scans using [Bogo et al. 2014]; we call these aligned meshes \"registrations.\" The multi-pose dataset consists of 1786 registrations of 40 individuals (891 registrations spanning 20 females, and 895 registrations spanning 20 males); a sampling is shown in Fig. 4. The multi-shape dataset consists of registrations to the CAESAR dataset [Robinette et al. 2002], totaling 1700 registrations for males and 2100 for females; a few examples are shown in Fig. 5. We denote the j th mesh in the multi-pose dataset as V P j and the j th mesh in the multi-shape dataset as V S j .\n\nOur goal is to train the parameters = T , W, S, J , P to minimize vertex reconstruction error on the datasets. Because our model decomposes shape and pose, we train these separately, simplifying optimization. We first train {J , W, P} using our multi-pose dataset and then train {T, S} using our multi-shape dataset. We train separate models for men and women (i.e. m and f ). \n\n\nPose Parameter Training\n\nWe first use the multi-pose dataset to train {J , W, P}. To this end, we need to compute the rest templates,T P i , and joint locations,\u0134 P i , for each subject, i, as well as the pose parameters,\u2713j, for each registration, j, in the dataset. We alternate between optimizing registration specific parameters\u2713j, subject-specific parameters {T P i ,\u0134 P i }, and global parameters {W, P}. We then learn the matrix, J , to regress from subject-specific vertex locations,T P i , to subject-specific joint locations,\u0134 P i . To achieve all this, we minimize an objective function consisting of a data term, ED, and several regularization terms {EJ , EY , EP , EW } defined below.\n\nThe data term penalizes the squared Euclidean distance between registration vertices and model vertices \n= {T P i } P subj i=1 ,\u0134 P = {\u0134 P i } P subj i=1\nare the sets of all rest poses and joints, and P subj is the number of subjects in the pose training set.\n\nWe estimate 207 \u21e5 3 \u21e5 6890 = 4, 278, 690 parameters for the pose blend shapes, P, 4 \u21e5 3 \u21e5 6890 = 82, 680 parameters for the skinning weights, W, and 3 \u21e5 6890 \u21e5 23 \u21e5 3 = 1, 426, 230 for the joint regressor matrix, J . To make the estimation well behaved, we regularize by making several assumptions. A symmetry regularization term, EY , penalizes left-right asymmetry for\u0134 P andT P\nEY (\u0134 P ,T P ) = P subj X i=1 U ||\u0134 P i U (\u0134 P i )|| 2 + ||T P i U (T P i )|| 2 ,\nwhere U = 100, and where U (T) finds a mirror image of vertices T, by flipping across the sagittal plane and swapping symmetric vertices. This term encourages symmetric template meshes and, more importantly, symmetric joint locations. Joints are unobserved variables and along the spine they are particularly difficult to localize. While models trained without the symmetry term produce reasonable results, enforcing symmetry produces models that are visually more intuitive for animation.\n\nOur model is hand segmented into 24 parts (Fig. 6). We use this segmentation to compute an initial estimate of the joint centers and a regressor JI from vertices to these centers. This regressor computes the initial joints by taking the average of the ring of vertices connecting two parts. When estimating the joints for each subject we regularize them to be close to this initial prediction:\nE J (T P ,\u0134 P ) = P subj X i=1 ||JIT P i \u0134 P i || 2 .\nTo help prevent overfitting of the pose-dependent blend shapes, we regularize them towards zero\nEP (P) = ||P|| 2 F ,\nwhere k \u00b7 kF denotes the Frobenius norm. Note that replacing the quadratic penalty with an L1 penalty would encourage greater sparsity of the blend shapes. We did not try this.\n\nWe also regularize the blend weights towards the initial weights, WI , shown in Fig. 6:\nEW (W) = ||W WI || 2 F .\nThe initial weights are computed by simply diffusing the segmentation.\n\nAltogether, the energy for training {W, P} is as follows:\nE\u21e4(T P ,\u0134 P , \u21e5, W, P) = E D + Y EY + J EJ + P EP + EW ,(14)\nwhere Y = 100, J = 100 and P = 25. These weights were set empirically. Our model has a large number of parameters and the Optimized. After optimization we find a sparse set of vertices and associated weights influencing each joint. regularization helps prevent overfitting. As the size of the training set grows, so does the strength of the data term, effectively reducing the influence of the regularization terms. Our experiments below with held-out test data suggest that the learned models are not overfit to the data and generalize well.\n\nThe overall optimization strategy is described in Sec. 4.3.\n\n\nJoint regressor.\n\nOptimizing the above gives a template mesh and joint locations for each subject, but we want to predict joint locations for new subjects with new body shapes. To that end, we learn the regressor matrix J to predict the training joints from the training bodies. We tried several regression strategies; what we found to work best, was to compute J using non-negative least squares [Lawson and Hanson 1995] with the inclusion of a term that encourages the weights to add to one. This approach encourages sparsity of the vertices used to predict the joints. Making weights positive and add to one discourages predicting joints outside the surface. These constraints enforce the predictions to be in the convex hull of surface points. Figure 7 shows the non-zero elements of the regression matrix, illustrating that a sparse set of surface vertices are linearly combined to estimate the joint centers.\n\n\nShape Parameter Training\n\nOur shape space is defined by a mean and principal shape directions {T, S}. It is computed by running PCA on shape registrations from our multi-shape database after pose normalization. Pose normalization transforms a raw registration V S j into a registration,T S j , in the rest pose,\u2713 \u21e4 . This normalization is critical to ensure that pose and shape are modeled separately.\n\nTo pose-normalize a registration, V S j , we first have to estimate its pose. We denoteT P \u00b5 and\u0134 P \u00b5 as the mean shape and mean joint locations from the multi-pose database respectively. Let We(T P \u00b5 ,\u0134 P \u00b5 ,\u2713, W), V S j,e 2 R 3 denote an edge of the model and of the registration respectively. An edge is obtained by subtracting a pair of neighboring vertices. To estimate the pose using an average generic shapeT P \u00b5 , we minimize the following sum of squared edge differences so that\u2713j = arg mi\u00f1 \u2713 X e ||We(T P \u00b5 + BP (\u2713; P),\u0134 P \u00b5 ,\u2713, W) V S j,e || 2 ,\n\nwhere the sum is taken over all edges in the mesh. This allows us to get a good estimate of the pose without knowing the subject specific shape.\n\n(a) PC 1 (b) PC 2 (c) PC 3 Figure 8: Shape blend shapes. The first three shape principal components of body shape are shown. PC1 and PC2 vary from -2 to +2 standard deviations from left to right, while PC3 varies from -5 to +5 standard deviations to make the shape variation more visible. Joint locations (red dots) vary as a function of body shape and are predicted using the learned regressor, J .\n\nOnce the pose\u2713j is known we solve forT S j by minimizin\u011d\nT S j = arg min T ||W (T + BP (\u2713j; P), JT,\u2713j, W) V S j || 2 .\nThis computes the shape that, when posed, matches the training registration. This shape is the pose-normalized shape.\n\nWe then run PCA on {T S j } S subj j=1 to obtain {T, S}. This procedure is designed to maximize the explained variance of vertex offsets in the rest pose, given a limited number of shape directions.\n\nNote that the optimization of pose is critically important when building a shape basis from vertices. Without this step, pose variations of the subjects in the shape training dataset would be captured in the shape blend shapes. The resulting model would not be decomposed properly into shape and pose. Note also that this approach constrasts with SCAPE or BlendSCAPE, where PCA is performed in the space of per-triangle deformations. Unlike vertices, triangle deformations do not live in a Euclidean space [Freifeld and Black 2012]. Hence PCA on vertices is more principled and is consistent with the registration data term, which consists of squared vertex disparities. Figure 8 visualizes the first three shape components. The figure also shows how the joint locations change with the changes in body shape. The joint positions are shown by the spheres and are computed from the surface meshes using the learned joint regression function. The lines connecting the joints across the standard deviations illustrate how the joint positions vary linearly with shape. Figure 9 shows the relative cumulative variance of SMPL and BlendSCAPE. While SMPL requires many fewer PCs to account for the same percentage of overall variance, the variance is different in the two cases: one is variance in vertex locations and the other is variance in triangle deformations. Explained variance in deformations does not directly translate into explained variance in vertex locations. While this makes the two models difficult to compare precisely, triangle deformations have many more degrees of freedom and it is likely that there are many deformations that produce visually similar shapes. A model requiring fewer components is generally preferable.\n\n\nOptimization summary\n\nPose parameters\u2713j in Eq. (14) are first initialized by minimizing the difference between the model and the registration edges, Figure 9: Cumulative relative variance of the CAESAR dataset explained as a function of the number of shape coefficients. For SMPL the variance is in vertex locations, while for BlendSCAPE it is in triangle deformations. similar to Eq. (15) using an average template shape. Then {T P ,\u0134 P , W, P, \u21e5} are estimated in an alternating manner to minimize Eq. 14. We proceed to estimate J from {\u0134 P ,T P }. We then run PCA on pose normalized subjects {T S j } S subj j=1 to obtain {T, S}. The final model is defined by {J , W, P,T, S}. Note that all training parameters except for {T, S} are found with gradientbased dogleg minimization [Nocedal and Wright 2006]. Gradients are computed with automatic differentiation using the the Chumpy framework .\n\n\nSMPL Evaluation\n\nAll training subjects gave prior informed written consent for their data to be used in creating statistical models for distribution. Registered meshes and identifiable subjects shown here are of professional models working under contract. Here we show SMPL-LBS, because TP shows more variation due to pose than SMPL-DQBS.\n\n\nQuantitative Evaluation\n\nWe evaluate two types of error. Model generalization is the ability of the model to fit to meshes of new people and poses; this tests both shape and pose blend shapes. Pose generalization is the ability to generalize a shape of an individual to new poses of the same person; this primarily tests how well pose blend shapes correct skinning artifacts and pose-dependent deformations. Both are measured by mean absolute vertex-to-vertex distance between the model and test registrations. For this evaluation we use 120 registered meshes of four women and two men from the public Dyna dataset [Dyn 2015]. These meshes contain a variety of body shapes and poses. All meshes are in alignment with our template and none were used to train our models. Figure 10 (gray) shows four examples of these registered meshes.\n\nWe evaluate SMPL-LBS and SMPL-DQBS. We also compare these with a BlendSCAPE model [Hirshberg et al. 2012] trained from exactly the same data as the SMPL models. The kinematic tree structure for SMPL and the BlendSCAPE model are the same: therefore we have the same number of pose parameters. We also compare the models using the same number of shape parameters.\n\nTo measure model generalization we first fit each model to each registered mesh, optimizing over shape~ and pose\u2713 to find the best fit in terms of squared vertex distances. Figure 10 shows fits of the SMPL-LBS (red) and BlendSCAPE (blue) models to the registered meshes. Both do a good job of fitting the data. The figure also shows how the model works. Illustrated are the estimated body shape,T+B S (~ ), and the effect of applying the pose blend shapes, TP (~ ,\u2713).\n\nFor pose generalization, we take each indvidual, select one of the estimated body shapes from the generalization task, and then optimize the pose,\u2713, for each of the other meshes of that subject, keeping the body shape fixed. The assumption behind pose generalization is that, if a model is properly decomposed into pose and shape, then the model should be able to fit the same subject in a different pose, without readjusting shape parameters. Note that the pose blend shapes are trained to fit observed registrations. As such, Figure 11: Model generalization indicates how well we can fit an independent registration. Mean absolute vertex error versus the number of shape coefficients used.\n\nthey correct for problems of blend skinning and try to capture posedependent deformations. Since the pose blend shapes are not dependent on body shape, they capture something about the average deformations in the training set. Figures 11 and 12 show the error of the SMPL models and Blend-SCAPE as a function of the number of body shape coefficients used. The differences between SMPL and BlendSCAPE are small (on the order of 0.5mm) but SMPL is more accurate on average. Remarkably, SMPL-LBS and SMPL-DQBS are essentially identical in model generalization and SMPL-LBS is actually slightly better at pose generalization. This is surprising because the pose blend shapes have much more to correct with LBS. Possibly the simplicity of LBS helps with generalization to new poses. This analysis is important because it says that users can choose the simpler and faster LBS model over the DQBS model.\n\nThe plots also show how well standard LBS fits the test data. This corresponds to the SMPL-LBS model with no pose blend shapes. Not surprisingly, LBS produces much higher error than either BlendSCAPE or SMPL. LBS is not as bad in Fig. 11 because here the model can vary body shape parameters, effectively using changes in identity to try to explain deformations due to pose. Figure 12 uses a fixed body shape and consequently illustrates how LBS does not model pose-dependent deformations realistically. Note that here we do not retrain a model specifically for LBS and expect such a model would be marginally more accurate.\n\n\nSparse SMPL\n\nThe pose blend shapes in SMPL are not sparse in the sense that a rotation of a part can influence any vertex of the mesh. With sufficient training data sparsity may emerge from data; e.g. the shoulder corrections will not be influenced by ankle motions. To make hand animation more intuitive, and to regularize the model to prevent long-range influences of joints, we can manually enforce sparsity. To this end, we trained a sparse version of SMPL by using the same sparsity pattern used for blend weights. In particular, we allow a vertex deviation to be influenced by at most 4 joints. Since every joint corresponds to a rotation matrix, the pose blend shape corresponding to any given vertex will be driven by 9 \u21e5 4 numbers as opposed to 9 \u21e5 23.\n\nThis model is referred to as SMPL-LBS-Sparse in Figs. 11 and 12. It is consistently less accurate than the regular SMPL-LBS model but may still be useful to animators. This suggests that SMPL-LBS is not overfit to the training data and that sparseness reduces the capacity of the model. The sparse model sometimes produces slight discontinuities at boundaries were vertices are influenced by different joint angles. Other strategies to enforce sparsity could be adopted, such as using an L1 prior or enforcing smoothness in the pose blend shapes. These approaches, however, would complicate the training process. Figure 13 illustrates the decomposition of shape parameters~ and pose parameters\u2713 in SMPL. Pose is held constant from left to right across each row while varying the shape. Similarly, the shape of each person is held constant while varying the pose from top to bottom in each column. The bodies are reposed using poses from the CMU mocap database [CMU 2000]. Note that the pose-dependent deformations look natural through a wide range of poses, despite very different body shapes. This illustrates that the joint regression works well and that the blend shapes are general.\n\n\nVisual Evaluation\n\nPlease see the Supplemental Video for many more examples and animations comparing SMPL and BlendSCAPE, illustrating the pose blend shapes, and illustrating body shape and pose variation.\n\n\nRun-time Performance\n\nThe run-time cost of SMPL is dominated by skinning and blendshape multiplication. Many skinning implementations exist, and we do not claim to have the fastest. Performance of our own CPUbased implementation, and a comparison against BlendSCAPE, is shown in Fig. 14. The plot shows the time needed to generate the vertices. Note that our BlendSCAPE rendering makes use of multiple cores, while the SMPL rendering does not; this is why the system time for BlendSCAPE is higher than the wall-clock time.\n\nNote that here we are showing the cost of changing body shape. For most applications, this is done once and the shape is then held fixed. The cost of animating the mesh then comes from only the pose blend shapes; this cost corresponds to 0 shape coefficients.\n\nFor meshes with the same number of vertices, SCAPE will always be slower. In SMPL each blend shape is of size 3N , requiring that many multiplications per shape. SCAPE uses triangle deformations with 9 elements per triangle and there are roughly twice as many triangles as vertices. This results in roughly a factor of 6 difference between SMPL and SCAPE in terms of basic multiplications.\n\n\nCompatibility with Rendering Engines\n\nBecause SMPL is built on standard skinning, it is compatible with existing 3D animation software. In particular, for a given body shape, we generate the subject-specific rest-pose template mesh and skeleton (estimated joint locations) and we export SMPL as a rigged model with pose blend shapes in Autodesk's Filmbox (FBX) file format, giving cross-platform compatibility. The model loads as a typical rigged mesh and can be animated as usual in standard 3D animation software.\n\nPose blend weights can be precomputed, baked into the model, and exported as an animated FBX file. This kind of file can be loaded into animation packages and played directly. We have tested the animated FBX files in Maya, Unity, and Blender.\n\nPose blend weights can also be computed on the fly given the pose, \u2713 t, at time t. To enable this, we provide scripts that take the joint angles and compute the pose blend weights. We have tested loading and animating SMPL in Maya 2013, 2014 and 2015. The animator can animate the model using any of the conventional animation methods typically used in Maya. We will provide a Python script that runs inside Maya to apply blend-shape corrections to an animated SMPL model. The pose blend shape values can be viewed and/or edited manually within Maya if desired. We have also tested SMPL in Unity. In SMPL, the blend weights range from -1 to +1 while in Unity they range form 0 to 1. Consequently, we scale and recenter our weights for compatibility. For runtime computation of pose blend shape coefficients, we provide a C# script that the user can attach to SMPL's mesh game object.\n\nThe SMPL model with shape and pose blend shapes, and the evaluation meshes, are available for research purposes at http://smpl.is.tue.mpg.de.\n\n\nDMPL: Dynamic SMPL\n\nWhile SMPL models static soft-tissue deformations with pose it does not model dynamic deformations that occur due to body movement and impact forces with the ground. Given 4D registrations that contain soft-tissue dynamics, we fit them by optimizing only the pose of a SMPL model with a personalized template shape. Displacements between SMPL and the observed meshes correspond to dynamic soft-tissue motions. To model these, we introduce a new set of additive blend shapes that we call dynamic blend shapes. These additional displacements are correlated with velocities and accelerations of the body and limbs rather than with pose. We follow the approach of Dyna [Pons-Moll et al. 2015], using the same training data, and apply the ideas to our additive vertex-based model.\n\nLet~ t = [\u2713t,\u2713t, vt, at,~ t 1,~ t 2] denote the dynamic control vector at time t. It is composed of pose velocities and acceleration\u1e61 \u2713t,\u2713t 2 R |\u2713| , root joint velocities and accelerations vt, at 2 R 3 and a history of two vectors of predicted dynamic coefficients t 1,~ t 2 2 R |~ | , described below. We extend our linear formulation from Sec. 3 and simply add the dynamic blend shape function, BD(~ t,~ ), to the other blend shapes in the rest pose before applying the skinning function. The shape in the zero pose becomes\nTD(~ ,\u2713t,~ t) =T + BS(~ ) + BP (\u2713t) + BD(~ t,~ ),(16)\nas illustrated in Fig. 15. Here, BD(~ t,~ ) takes as input the dynamic control vector at time t, and shape coefficients~ , and predicts vertex offsets in the rest pose.\n\nWhereas in [Pons-Moll et al. 2015] dynamic deformations are modeled using triangle deformations, DMPL models deformations in vertex space. We build male and female models using roughly 40, 000 registered male and female meshes from [ Dyn 2015]. We compute the pose in each frame and the displacements between SMPL and the registration. Using PCA, we obtain a mean and the dynamic blend shapes, \u00b5 D 2 R 3N and D 2 R 3N \u21e5|~ | respectively. We take |~ | = 300 principal components as in Dyna. Dynamic deformations vary significantly between subjects based on their body shape and fat distribution. To capture this, we train a model that depends on the body shape parameters~ as in Dyna.\n\nDynamic blendshapes are then predicted using\nBD(~ t,~ ; D) = \u00b5D + Df (~ t,~ )(17)\nanalogous to Eq. (22) in [Pons-Moll et al. 2015] where f (\u00b7) is a function that takes as input a dynamic control vector,~ t, and predicts the vector of dynamic shape coefficients,~ t. This formulation of soft-tissue displacements in terms of dynamic blend shapes means that, unlike Dyna, our model remains compatible with current graphics software. To animate the model, we only need a script to compute the coefficients,~ t = f (~ t,~ ), from the pose sequence and body shape. We observe that the DMPL model produces softtissue dynamics that appear more realistic than those of Dyna. See the Supplemental Video for visualizations of the training data, dynamic blend shapes, and resulting animations.\n\n\nDiscussion\n\nWhy does it work? First, good quality training data is important.\n\nHere we use thousands of high-quality registered template meshes. Importantly, the pose training data spans a range of body shapes enabling us to learn a good predictor of joint locations. Second, training all the parameters (template shape, blend weights, joint regressor, shape/pose/dynamic blend shapes) to minimize vertex reconstruction error is important to obtain a good model. Here the simplicity of the model is an advantage as it enables training everything with large amounts of data.\n\nIn contrast to the scattered-data interpolation methods, we learn the blend shapes from a large set of training meshes covering the space of possible poses and learn a simpler function relating pose to blend-shape weights. In particular, our function is linear in the elements of the part rotation matrices. The larger support of the learned linear functions as opposed to radial basis functions allows the model to generalize to arbitrary poses; in addition the simple linear form makes it fast to animate in a game engine without baking in the weights. Because elements of a rotation matrix are constrained, the model cannot \"blow up\" when generalizing outside the training set.\n\nSMPL is an additive model in vertex space. In contrast, while SCAPE also factors deformations into shape and pose deformations, SCAPE multiplies the triangle deformations. With SCAPE a bigger person will have bigger pose-dependent deformations even though these deformations are not learned for different body shapes. Despite this, in our experiments, the SCAPE approach is less accurate at generalizing to new shapes. Ideally one would have enough pose data from enough different people to learn a true body-shapedependent pose deformation space. Our work with DMPL, where deformations depend on body shape, suggests that this is possible.\n\nWhy is it more accurate than BlendSCAPE? Models based on the statistics of triangle deformations have dominated the recent literature [Anguelov et al. 2005;Chen et al. 2013;Freifeld and Black 2012;Hasler et al. 2009]. Such models are not trained to reproduce their training registrations directly. Instead, they are trained Figure 15: DMPL model of soft-tissue motion. Above, two frames of a \"running\" sequence of a male subject from the Dyna dataset, below two frames of a \"jumping jacks\" sequence of a female subject from the Dyna dataset. From left to right: SMPL, DMPL, and the dynamic blend shapes added to the base body shape. While SMPL models deformations due to pose well it does not model dynamic deformations. DMPL predicts dynamic deformations from motion and body shape, resulting in more life like animations.\n\nto reproduce the local deformations that produced those registrations. Part of the tractability of training these models comes from the ability to train deformations independently across triangles. As a result, long range distances and relationships are not preserved as well as local relationships between vertices. We speculate that an advantage of vertex based models (such as SMPL and [Allen et al. 2006]) is that they can be trained to minimize the mean squared error between the model and training vertices. Theoretically one could train a SCAPE model to minimize vertex error in global coordinates, but the inner loop of the optimization would involve solving a least-squares problem to reconstruct vertices from the deformations. This would significantly increase the cost of optimization and make it difficult to train the model with large amounts of data.\n\nWhy has it not been done before? While we think the SMPL model is a natural way to extend blend skinning, we are unaware of any previous published versions. Unfortunately, the obvious implementation makes the pose blend shapes a linear function of\u2713. This does not work. The key to SMPL's performance is to make the blendshapes a linear function of the elements of R \u21e4 (\u2713). This formulation, sufficient training data, and a good optimization strategy make it possible to learn the model.\n\nThe closest work to ours is the pioneering work of Allen et al. [2006]. Their model is more complex than ours, using radial basis functions for scattered data interpolation, shape-dependent pose deformations, and a fixed set of carrying angles. Consequently training it is also complex and requires a good initialization. They had limited data and difficulty with overfitting so they restricted their body shape PCA space. As a result, the model did not generalize well to new shapes and poses. Our simpler model lets us learn it from large datasets and having more data makes the simple model  Other features for driving pose blend shapes. We experimented with driving pose blendshapes linearly from other features, such as raw\u2713, simple polynomials of\u2713, and trigonometric functions (sin, cos) of\u2713. None of these performed as well as our proposed formulation. Using raw\u2713 has serious limitations because the values vary between -\u21e1 and \u21e1. Imagine a twist of the neck (Fig. 16), which produces negative and postive angles about the vertical axis. Standard LBS will cause the neck to shrink as it rotates in either direc-tion. To counteract this, we need a blend shape that increases the neck volume no matter which direction it rotates. Unfortunately, if the blendshapes are trained to expand during rightwards rotation (to counteract LBS shrinkage), they would contract during leftward rotation.\n\nIn general one can think of replacing the raw rotations with any functions of rotations and using these to weight the blend shapes. An exhaustive search is impossible and other features may work as well as our method; for example, we did not experiment with normalized quaternions.\n\nOur pose blend shapes function is also very different from scattered data interpolation methods like WPSD [Kurihara and Miyata 2004;Rhee et al. 2006], which use a discrete number of poses and associated corrections are interpolated between them using RBFs. In practice, a large number of poses might be needed to cover the pose space well. This makes animation slow since the closest key poses have to be found at run time.\n\nLimitations. The pose-dependent offsets of SMPL are not dependent on body shape. It is surprising how well SMPL works without this, but the general approach would likely not work if we were to model a space of nonrealistic animated characters in which body part scales vary widely, or a space of humans that includes infants and adults.\n\nThis limitation could be addressed by training a more general function that would take elements of R \u21e4 (\u2713) together with~ to predict the blend shape coefficients. Note that the dynamic blend shape coefficients do depend on body shape and therefore it should be possible to do the same for the pose blend shapes. This would not significantly complicate the model or run-time behavior but might require more training data.\n\nAs described, the SMPL model is a function of joint angles and shape parameters only: it does not model breathing, facial motion, muscle tension, or any changes independent of skeletal joint angles and overall shape. These could potentially be learned as additional additive blendshapes (as with DMPL) if the appropriate factored data is available (cf. [Tsoli et al. 2014]).\n\nWhile we learn most model parameters, we do not learn them all. We manually define the segmentation of the template into parts, the topology of the mesh, and the zero pose. Theoretically these could also be learned but we expect only marginal improvements for significant effort.\n\nFuture work. SMPL uses 207 pose blend shapes. This could likely be reduced by performing PCA on the blend shapes. This would reduce the number of multiplications and consequently increase rendering speed. Also, our dynamic model uses PCA to learn the dynamic blend shapes but we could learn the elements of these blend shapes directly as we do for the pose blend shapes. Finally, here we fit our model to registered meshes but could fit SMPL to mocap marker data (cf. MoSh ), depth data, or video. We anticipate that optimizing the pose and shape of a SMPL-LBS model will be significantly faster than optimizing a SCAPE model of similar quality.\n\n\nConclusions\n\nOur goal was to create a skeletally-driven human body model that could capture body shape and pose variation as well as, or better than, the best previous models while being compatible with existing graphics pipelines and software. To that end, SMPL uses standard skinning equations and defines body shape and pose blend shapes that modify the base mesh. We train the model on thousands of aligned scans of different people in different poses. The form of the model makes it possible to learn the parameters from large amounts of data while directly minimizing vertex reconstruction error. Specifically we learn the rest template, joint regressor, body shape model, pose blend shapes, and dynamic blend shapes. The surprising result is that, when BlendSCAPE and SMPL are trained on exactly the same data, the vertex-based model is more accurate and significantly more efficient to render than the deformationbased model. Also surprising is that a relatively small set of learned blend shapes do as good a job of correcting the errors of LBS as they do for DQBS. Using 4D registered meshes we extended SMPL to model dynamic soft-tissue deformations as a function of poses over time using an autoregressive model. SMPL can be exported as an FBX file and we make scripts available to animate the model in common rendering systems. This will allow anyone to realistically animate human bodies.\n\nFigure 2 :\n2Models compared with ground truth. This figure defines the color coding used throughout the paper and Supplemental Video.\n\nFigure 3 :\n3SMPL model. (a) Template mesh with blend weights indicated by color and joints shown in white. (b) With identity-driven blendshape contribution only; vertex and joint locations are linear in shape vector~ . (c) With the addition of of pose blend shapes in preparation for the split pose; note the expansion of the hips. (d) Deformed vertices reposed by dual quaternion skinning for the split pose. the transformation matrix. Such changes are expressive but are not compatible with existing animation systems.\n\nFigure 4 :\n4Sample registrations from the multipose dataset. body shapes and poses are created and animated by varying~ and \u2713 respectively. Then the SMPL model is finally defined as M (~ ,\u2713; ) = W \u21e3 TP (~ ,\u2713;T, S, P), J(~ ; J ,T, S),\u2713, W \u2318\n\nFigure 5 :\n5Sample registrations from the multishape dataset.\n\n\nED(T P ,\u0134 P , W, P, j) + BP (\u2713j; P),\u0134 P s(j) ,\u2713j, W)|| 2where \u21e5 = n\u2713 1, . . . ,\u2713P reg o , s(j)is the subject index corresponding to registration j, Preg are the number of meshes in the pose trainings set,T P\n\nFigure 6 :\n6Initialization of joints and blend weights. Discrete part segmentation in (a) is diffused to obtain initial blend weights, WI , in (b). Initial joint centers are shown as white dots.\n\nFigure 7 :\n7Joint regression. (left) Initialization. Joint locations can be influenced by locations on the surface, indicated by the colored lines. We assume that these influences are somewhat local. (right)\n\nFigure 10 :\n10Model fitting with intermediate stages. We fit both BlendSCAPE (blue) and SMPL-LBS, M (~ ,\u2713), (red) to registered meshes by optimizing pose and shape.T + BS(~ ) shows the estimated body shape and TP (~ ,\u2713) shows the effects of pose-dependent blend shapes.\n\nFigure 12 :\n12Pose generalization error indicates how well a fitted shape generalizes to new poses.\n\nFigure 13 :\n13Animating SMPL. Decomposition of SMPL parameters into pose and shape: Shape parameters,~ , vary across different subjects from left to right, while pose parameters,\u2713, vary from top to bottom for each subject.\n\nFigure 14 :\n14Performance of SMPL and BlendSCAPE vary with the number of body shape coefficients used. Performance shown here is from a 2014 Macbook Pro.\n\nFigure 16 :\n16Parameterizing pose blend shapes. (a) Pose blend shapes parameterized by Euler angles cause significant problems. (b) our proposed parameterization allows the head to rotate in either direction with natural deformations.\n\n\n|~ | is the number of linear shape coefficients, and the Sn 2 R 3N represent orthonormal principal components of shape displacements. Let S = [S 1, . . . , S |~ | ] 2 R 3N \u21e5|~ | be the matrix of all such shape displacements. Then the linear function BS(~ ; S) is fully defined by the matrix S, which is learned from registered training meshes, see Sec. 4.\nAcknowledgementsWe thank F. Bogo for help with registration and 3D body modeling; B. Allen and B. Curless for information about their model; B. Mohler, J. Tesch, and N. Troje for technical discussion; A. Keller, E. Holderness, S. Polikovsky, and S. Streuber for help with data acquisition; J. Anning for voice recording and technical support; and A. Quiros Ramirez for web development.Conflict-of-Interest Disclosure:MJB is a founder, shareholder, and member of the board of Body Labs Inc., which is commercializing body shape technology.A AppendixA.1 Mathematical NotationWe summarize our notation here and inTable 1. Matrices A 2 R n\u21e5m are denoted with math calligraphic typeface. Vectors A 2 R m are denoted with uppercase boldface, expect for the special case of 3-vectors, which are denoted with lower case a 2 R 3 to distinguish a particular vertex from a vector of concatenated vertices. The notation A() : R m 7 ! R n is used to denote a function that maps vectors in an m-dimensional space to vectors in ndimensional space. Typically, indices are used as follows: j iterates over mesh registrations, k iterates over joint angles and i iterates over subjects, and t denotes time.\nArticulated body deformation from range scan data. B Allen, B Curless, Z Popovi\u0107, ACM Trans. Graph. (Proc. SIGGRAPH). 21ALLEN, B., CURLESS, B., AND POPOVI\u0106, Z. 2002. Articulated body deformation from range scan data. ACM Trans. Graph. (Proc. SIGGRAPH) 21, 3 (July), 612-619.\n\nThe space of human body shapes: Reconstruction and parameterization from range scans. B Allen, B Curless, Z Popovi\u0107, ACM Trans. Graph. (Proc. SIGGRAPH). 22ALLEN, B., CURLESS, B., AND POPOVI\u0106, Z. 2003. The space of human body shapes: Reconstruction and parameterization from range scans. ACM Trans. Graph. (Proc. SIGGRAPH) 22, 3, 587- 594.\n\nLearning a correlated model of identity and posedependent body shape variation for real-time synthesis. B Allen, B Curless, Z Popovi\u0107, A Hertzmann, Proceedings of the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association. the 2006 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics AssociationAirela-Ville, Switzerland, Switzerland, SCA '06ALLEN, B., CURLESS, B., POPOVI\u0106, Z., AND HERTZMANN, A. 2006. Learning a correlated model of identity and pose- dependent body shape variation for real-time synthesis. In Pro- ceedings of the 2006 ACM SIGGRAPH/Eurographics Sympo- sium on Computer Animation, Eurographics Association, Aire- la-Ville, Switzerland, Switzerland, SCA '06, 147-156.\n\nSCAPE: Shape Completion and Animation of PEople. D Anguelov, P Srinivasan, D Koller, S Thrun, J Rodgers, J Davis, ACM Trans. Graph. (Proc. SIG-GRAPH. 24ANGUELOV, D., SRINIVASAN, P., KOLLER, D., THRUN, S., RODGERS, J., AND DAVIS, J. 2005. SCAPE: Shape Comple- tion and Animation of PEople. ACM Trans. Graph. (Proc. SIG- GRAPH 24, 3, 408-416.\n\nAutomatic rigging and animation of 3D characters. I Baran, J Popovi\u0107, ACM Trans. Graph. (Proc. SIGGRAPH). 263BARAN, I., AND POPOVI\u0106, J. 2007. Automatic rigging and anima- tion of 3D characters. ACM Trans. Graph. (Proc. SIGGRAPH) 26, 3 (July).\n\nFAUST: Dataset and evaluation for 3D mesh registration. F Bogo, J Romero, M Loper, M J Black, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)BOGO, F., ROMERO, J., LOPER, M., AND BLACK, M. J. 2014. FAUST: Dataset and evaluation for 3D mesh registration. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 3794 -3801.\n\nRange scan registration using reduced deformable models. W Chang, M Zwicker, Computer Graphics Forum. 28CHANG, W., AND ZWICKER, M. 2009. Range scan registration using reduced deformable models. Computer Graphics Forum 28, 2, 447-456.\n\nTensor-based human body modeling. Y Chen, Z Liu, Z Zhang, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). CHEN, Y., LIU, Z., AND ZHANG, Z. 2013. Tensor-based human body modeling. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 105-112.\n\nCMU graphics lab motion capture database. CMU graphics lab motion capture database. http:// mocap.cs.cmu.edu. Accessed: 2012-12-11.\n\nAutomatic generation of 3D character animation from 3D meshes. S Corazza, E Gambaretto, Aug. 5. US Patent. 8328CORAZZA, S., AND GAMBARETTO, E., 2014. Automatic gener- ation of 3D character animation from 3D meshes, Aug. 5. US Patent 8,797,328.\n\nAutomatic conversion of mesh animations into skeletonbased animations. E De Aguiar, C Theobalt, S Thrun, H.-P Seidel, Computer Graphics Forum. 27DE AGUIAR, E., THEOBALT, C., THRUN, S., AND SEIDEL, H.-P. 2008. Automatic conversion of mesh animations into skeleton- based animations. Computer Graphics Forum 27, 2, 389-397.\n\n. Dyna Dataset, Dyna dataset. http://dyna.is.tue.mpg.de/. Ac- cessed: 2015-05-15.\n\nLie bodies: A manifold representation of 3D human shape. O Freifeld, M J Black, European Conf. on Computer Vision (ECCV). A. Fitzgibbon et al.Springer-Verlag7572Part IFREIFELD, O., AND BLACK, M. J. 2012. Lie bodies: A mani- fold representation of 3D human shape. In European Conf. on Computer Vision (ECCV), Springer-Verlag, A. Fitzgibbon et al. (Eds.), Ed., Part I, LNCS 7572, 1-14.\n\nA statistical model of human pose and body shape. N Hasler, C Stoll, M Sunkel, B Rosenhahn, H Seidel, Computer Graphics Forum. 28HASLER, N., STOLL, C., SUNKEL, M., ROSENHAHN, B., AND SEIDEL, H. 2009. A statistical model of human pose and body shape. Computer Graphics Forum 28, 2, 337-346.\n\nLearning skeletons for shape and pose. N Hasler, T Thorm\u00e4hlen, B Rosenhahn, H.-P Seidel, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games. the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and GamesNew York, NY, USA, I3DACM10HASLER, N., THORM\u00c4HLEN, T., ROSENHAHN, B., AND SEIDEL, H.-P. 2010. Learning skeletons for shape and pose. In Proceed- ings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, ACM, New York, NY, USA, I3D '10, 23- 30.\n\nCoregistration: Simultaneous alignment and modeling of articulated 3D shape. D Hirshberg, M Loper, E Rachlin, M Black, European Conf. on Computer Vision (ECCV). A. F. et al.Part IVSpringer-Verlag7577HIRSHBERG, D., LOPER, M., RACHLIN, E., AND BLACK, M. 2012. Coregistration: Simultaneous alignment and modeling of articulated 3D shape. In European Conf. on Computer Vision (ECCV), Springer-Verlag, A. F. et al. (Eds.), Ed., LNCS 7577, Part IV, 242-255.\n\nSkinning mesh animations. D L James, C D Twigg, ACM Trans. Graph. 24JAMES, D. L., AND TWIGG, C. D. 2005. Skinning mesh anima- tions. ACM Trans. Graph. 24, 3 (July), 399-407.\n\nSpherical blend skinning: A real-time deformation of articulated models. L Kavan, J And\u017e\u00e1ra, Proceedings of the 2005 Symposium on Interactive 3D Graphics and Games. the 2005 Symposium on Interactive 3D Graphics and GamesNew York, NY, USA,ACMI3D '05KAVAN, L., AND\u017d\u00c1RA, J. 2005. Spherical blend skinning: A real-time deformation of articulated models. In Proceedings of the 2005 Symposium on Interactive 3D Graphics and Games, ACM, New York, NY, USA, I3D '05, 9-16.\n\nGeometric skinning with approximate dual quaternion blending. L Kavan, S Collins, J \u017d\u00e1ra, C Sullivan, ACM Transactions on Graphics (TOG). 2723KAVAN, L., COLLINS, S.,\u017d\u00c1RA, J., AND O'SULLIVAN, C. 2008. Geometric skinning with approximate dual quaternion blending. ACM Transactions on Graphics (TOG) 27, 4, 105:1-105:23.\n\nAutomatic linearization of nonlinear skinning. L Kavan, S Collins, C Sullivan, Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games. the 2009 Symposium on Interactive 3D Graphics and GamesNew York, NY, USAACMI3D '09KAVAN, L., COLLINS, S., AND O'SULLIVAN, C. 2009. Auto- matic linearization of nonlinear skinning. In Proceedings of the 2009 Symposium on Interactive 3D Graphics and Games, ACM, New York, NY, USA, I3D '09, 49-56.\n\nEigenSkin: Real time large deformation character skinning in hardware. P G Kry, D L James, D K Pai, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer AnimationNew York, NY, USA, SCA '02ACMKRY, P. G., JAMES, D. L., AND PAI, D. K. 2002. EigenSkin: Real time large deformation character skinning in hardware. In Proceedings of the 2002 ACM SIGGRAPH/Eurographics Sym- posium on Computer Animation, ACM, New York, NY, USA, SCA '02, 153-159.\n\nModeling deformable human hands from medical images. T Kurihara, N Miyata, Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics Association. the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Eurographics AssociationAire-la-Ville, Switzerland, Switzerland, SCA '04KURIHARA, T., AND MIYATA, N. 2004. Modeling deformable human hands from medical images. In Proceedings of the 2004 ACM SIGGRAPH/Eurographics Symposium on Computer An- imation, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, SCA '04, 355-363.\n\nSolving least squares problems. C L Lawson, R J Hanson, Society of industrial and applied mathematics. Philadelphia, PA. SIAMLAWSON, C. L., AND HANSON, R. J. 1995. Solving least squares problems. Classics in applied mathematics. SIAM, Philadelphia, PA. SIAM : Society of industrial and applied mathematics.\n\nSmooth skinning decomposition with rigid bones. B H Le, Z Deng, ACM Trans. Graph. 316LE, B. H., AND DENG, Z. 2012. Smooth skinning decomposi- tion with rigid bones. ACM Trans. Graph. 31, 6 (Nov.), 199:1- 199:10.\n\nRobust and accurate skeletal rigging from mesh sequences. B H Le, Z Deng, ACM Trans. Graph. 3310LE, B. H., AND DENG, Z. 2014. Robust and accurate skeletal rigging from mesh sequences. ACM Trans. Graph. 33, 4 (July), 84:1-84:10.\n\nPose space deformation: A unified approach to shape interpolation and skeleton-driven deformation. J P Lewis, M Cordner, N Fong, Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques. the 27th Annual Conference on Computer Graphics and Interactive TechniquesNew York, NY, USA, SIGGRAPH '00ACM Press/Addison-Wesley Publishing CoLEWIS, J. P., CORDNER, M., AND FONG, N. 2000. Pose space deformation: A unified approach to shape interpolation and skeleton-driven deformation. In Proceedings of the 27th An- nual Conference on Computer Graphics and Interactive Tech- niques, ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, SIGGRAPH '00, 165-172.\n\nOpenDR: An approximate differentiable renderer. M M Loper, M J Black, Computer Vision -ECCV 2014. Heidelberg, D. Fleet, T. Pajdla, B. Schiele, and T. TuytelaarsSpringer8695LOPER, M. M., AND BLACK, M. J. 2014. OpenDR: An ap- proximate differentiable renderer. In Computer Vision -ECCV 2014, Springer, Heidelberg, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds., vol. 8695 of Lecture Notes in Computer Sci- ence, 154-169.\n\nMoSh: Motion and shape capture from sparse markers. M M Loper, N Mahmood, M J Black, Proc. SIGGRAPH Asia). SIGGRAPH Asia)33LOPER, M. M., MAHMOOD, N., AND BLACK, M. J. 2014. MoSh: Motion and shape capture from sparse markers. ACM Trans. Graph., (Proc. SIGGRAPH Asia) 33, 6 (Nov.), 220:1-220:13.\n\nAnimation space: A truly linear framework for character animation. B Merry, P Marais, J Gain, ACM Trans. Graph. 254MERRY, B., MARAIS, P., AND GAIN, J. 2006. Animation space: A truly linear framework for character animation. ACM Trans. Graph. 25, 4 (Oct.), 1400-1423.\n\nFrankenrigs: Building character rigs from multiple sources. C Miller, O Arikan, D Fussell, Proceedings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games. the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and GamesNew York, NY, USA, I3DACM10MILLER, C., ARIKAN, O., AND FUSSELL, D. 2010. Franken- rigs: Building character rigs from multiple sources. In Proceed- ings of the 2010 ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, ACM, New York, NY, USA, I3D '10, 31- 38.\n\nBuilding efficient, accurate character skins from examples. A Mohr, M Gleicher, Proc. SIGGRAPH). SIGGRAPH)MOHR, A., AND GLEICHER, M. 2003. Building efficient, accu- rate character skins from examples. ACM Trans. Graph. (Proc. SIGGRAPH), 562-568.\n\nJ Nocedal, S J Wright, Numerical Optimization. New YorkSpringer2nd edNOCEDAL, J., AND WRIGHT, S. J. 2006. Numerical Optimization, 2nd ed. Springer, New York.\n\nDyna: A model of dynamic human shape in motion. G Pons-Moll, J Romero, N Mahmood, M J Black, Proc. SIGGRAPH). SIGGRAPH)3414PONS-MOLL, G., ROMERO, J., MAHMOOD, N., AND BLACK, M. J. 2015. Dyna: A model of dynamic human shape in mo- tion. ACM Transactions on Graphics, (Proc. SIGGRAPH) 34, 4 (July), 120:1-120:14.\n\nReal-time weighted pose-space deformation on the GPU. T Rhee, J Lewis, U Neumann, EUROGRAPH-ICS. 253RHEE, T., LEWIS, J., AND NEUMANN, U. 2006. Real-time weighted pose-space deformation on the GPU. EUROGRAPH- ICS 25, 3.\n\nCivilian American and European Surface Anthropometry Resource (CAESAR) final report. K Robinette, S Blackwell, H Daanen, M Boehmer, S Fleming, T Brill, D Hoeferlin, D Burnsides, AFRL-HE- WP-TR-2002-0169US Air Force Research LaboratoryTech. Rep.ROBINETTE, K., BLACKWELL, S., DAANEN, H., BOEHMER, M., FLEMING, S., BRILL, T., HOEFERLIN, D., AND BURNSIDES, D. 2002. Civilian American and European Surface Anthropom- etry Resource (CAESAR) final report. Tech. Rep. AFRL-HE- WP-TR-2002-0169, US Air Force Research Laboratory.\n\nMethod and apparatus for creating lifelike digital representations of computer animated objects by providing corrective enveloping. C Rouet, J Lewis, US Patent. 5638ROUET, C., AND LEWIS, J., 1999. Method and apparatus for cre- ating lifelike digital representations of computer animated ob- jects by providing corrective enveloping, Mar. 16. US Patent 5,883,638.\n\nExample-based skeleton extraction. S Schaefer, C Yuksel, Proceedings of the Fifth Eurographics Symposium on Geometry Processing. the Fifth Eurographics Symposium on Geometry ProcessingAirela-Ville, Switzerland, Switzerland, SGP '07SCHAEFER, S., AND YUKSEL, C. 2007. Example-based skele- ton extraction. In Proceedings of the Fifth Eurographics Sympo- sium on Geometry Processing, Eurographics Association, Aire- la-Ville, Switzerland, Switzerland, SGP '07, 153-162.\n\nSynthesizing animatable body models with parameterized shape modifications. H Seo, F Cordier, N Magnenat-Thalmann, Proceedings of the 2003 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, Eurographics Association. the 2003 ACM SIG-GRAPH/Eurographics Symposium on Computer Animation, Eurographics AssociationAire-la-Ville, Switzerland, Switzerland, SCA '03SEO, H., CORDIER, F., AND MAGNENAT-THALMANN, N. 2003. Synthesizing animatable body models with parameter- ized shape modifications. In Proceedings of the 2003 ACM SIG- GRAPH/Eurographics Symposium on Computer Animation, Eu- rographics Association, Aire-la-Ville, Switzerland, Switzerland, SCA '03, 120-125.\n\nBreathing life into shape: Capturing, modeling and animating 3D human breathing. A Tsoli, N Mahmood, M J Black, Proc. SIGGRAPH). SIGGRAPH)3311TSOLI, A., MAHMOOD, N., AND BLACK, M. J. 2014. Breathing life into shape: Capturing, modeling and animating 3D human breathing. ACM Trans. Graph., (Proc. SIGGRAPH) 33, 4 (July), 52:1-52:11.\n\nMulti-weight enveloping: Least-squares approximation techniques for skin animation. X C Wang, C Phillips, Proceedings of the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer Animation. the 2002 ACM SIGGRAPH/Eurographics Symposium on Computer AnimationNew York, NY, USA, SCA '02ACMWANG, X. C., AND PHILLIPS, C. 2002. Multi-weight enveloping: Least-squares approximation techniques for skin animation. In Proceedings of the 2002 ACM SIGGRAPH/Eurographics Sym- posium on Computer Animation, ACM, New York, NY, USA, SCA '02, 129-138.\n\nReal-time enveloping with rotational regression. R Y Wang, K Pulli, J Popovi\u0107, ACM Trans. Graph. (Proc. SIGGRAPH). 263WANG, R. Y., PULLI, K., AND POPOVI\u0106, J. 2007. Real-time enveloping with rotational regression. ACM Trans. Graph. (Proc. SIGGRAPH) 26, 3 (July).\n\nContext-aware skeletal shape deformation. O Weber, O Sorkine, Y Lipman, C Gotsman, Computer Graphics Forum. 263WEBER, O., SORKINE, O., LIPMAN, Y., AND GOTSMAN, C. 2007. Context-aware skeletal shape deformation. Computer Graphics Forum 26, 3 (Sept.), 265-274.\n", "annotations": {"author": "[{\"end\":85,\"start\":64},{\"end\":94,\"start\":86},{\"end\":105,\"start\":95},{\"end\":115,\"start\":106},{\"end\":128,\"start\":116},{\"end\":139,\"start\":129},{\"end\":184,\"start\":140}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":64},{\"end\":93,\"start\":88},{\"end\":104,\"start\":97},{\"end\":114,\"start\":108},{\"end\":127,\"start\":118},{\"end\":138,\"start\":133}]", "author_first_name": "[{\"end\":87,\"start\":86},{\"end\":96,\"start\":95},{\"end\":107,\"start\":106},{\"end\":117,\"start\":116},{\"end\":130,\"start\":129},{\"end\":132,\"start\":131}]", "author_affiliation": "[{\"end\":183,\"start\":141}]", "title": "[{\"end\":42,\"start\":1},{\"end\":226,\"start\":185}]", "venue": "[{\"end\":244,\"start\":228}]", "abstract": "[{\"end\":2066,\"start\":489}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3816,\"start\":3797},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3839,\"start\":3816},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3858,\"start\":3839},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3876,\"start\":3858},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3893,\"start\":3876},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4000,\"start\":3981},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4021,\"start\":4000},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4045,\"start\":4021},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4064,\"start\":4045},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4087,\"start\":4064},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4103,\"start\":4087},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4305,\"start\":4283},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4322,\"start\":4305},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4341,\"start\":4322},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4363,\"start\":4341},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5927,\"start\":5908},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5945,\"start\":5927},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5968,\"start\":5945},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7057,\"start\":7034},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7457,\"start\":7435},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8215,\"start\":8192},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8717,\"start\":8694},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11004,\"start\":10980},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11023,\"start\":11004},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11041,\"start\":11023},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11058,\"start\":11041},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11075,\"start\":11058},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11363,\"start\":11341},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11388,\"start\":11365},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11416,\"start\":11388},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11441,\"start\":11416},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11623,\"start\":11605},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11808,\"start\":11785},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12108,\"start\":12088},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12572,\"start\":12553},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12888,\"start\":12866},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12991,\"start\":12965},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13007,\"start\":12991},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13428,\"start\":13409},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14114,\"start\":14095},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14824,\"start\":14807},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15002,\"start\":14983},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15125,\"start\":15103},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15311,\"start\":15294},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15438,\"start\":15414},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15676,\"start\":15654},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15945,\"start\":15926},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16305,\"start\":16281},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16699,\"start\":16681},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17159,\"start\":17140},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17174,\"start\":17159},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17303,\"start\":17282},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17441,\"start\":17423},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17465,\"start\":17441},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17484,\"start\":17465},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17506,\"start\":17484},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17528,\"start\":17506},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17835,\"start\":17816},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18008,\"start\":17988},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18568,\"start\":18549},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24296,\"start\":24272},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28777,\"start\":28760},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29106,\"start\":29083},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33687,\"start\":33663},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36660,\"start\":36635},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38673,\"start\":38648},{\"end\":39730,\"start\":39720},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40045,\"start\":40023},{\"end\":44724,\"start\":44714},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":48826,\"start\":48803},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":49700,\"start\":49677},{\"end\":49909,\"start\":49900},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":50481,\"start\":50458},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":53191,\"start\":53169},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53208,\"start\":53191},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":53232,\"start\":53208},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53250,\"start\":53232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":54267,\"start\":54249},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":55285,\"start\":55266},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":57025,\"start\":56999},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":57041,\"start\":57025},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":58450,\"start\":58431}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":60920,\"start\":60786},{\"attributes\":{\"id\":\"fig_1\"},\"end\":61442,\"start\":60921},{\"attributes\":{\"id\":\"fig_2\"},\"end\":61683,\"start\":61443},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61746,\"start\":61684},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61956,\"start\":61747},{\"attributes\":{\"id\":\"fig_5\"},\"end\":62152,\"start\":61957},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62361,\"start\":62153},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62632,\"start\":62362},{\"attributes\":{\"id\":\"fig_8\"},\"end\":62733,\"start\":62633},{\"attributes\":{\"id\":\"fig_9\"},\"end\":62957,\"start\":62734},{\"attributes\":{\"id\":\"fig_10\"},\"end\":63112,\"start\":62958},{\"attributes\":{\"id\":\"fig_11\"},\"end\":63348,\"start\":63113},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":63706,\"start\":63349}]", "paragraph": "[{\"end\":2915,\"start\":2082},{\"end\":3369,\"start\":2917},{\"end\":5219,\"start\":3371},{\"end\":6158,\"start\":5221},{\"end\":6955,\"start\":6160},{\"end\":7340,\"start\":6957},{\"end\":8106,\"start\":7342},{\"end\":10066,\"start\":8108},{\"end\":10451,\"start\":10083},{\"end\":11253,\"start\":10453},{\"end\":11991,\"start\":11255},{\"end\":12445,\"start\":11993},{\"end\":13552,\"start\":12447},{\"end\":14071,\"start\":13554},{\"end\":14788,\"start\":14073},{\"end\":15312,\"start\":14790},{\"end\":16244,\"start\":15314},{\"end\":16870,\"start\":16246},{\"end\":18373,\"start\":16872},{\"end\":19405,\"start\":18375},{\"end\":20103,\"start\":19427},{\"end\":20557,\"start\":20105},{\"end\":21305,\"start\":20588},{\"end\":21723,\"start\":21307},{\"end\":21844,\"start\":21725},{\"end\":22434,\"start\":21846},{\"end\":22959,\"start\":22436},{\"end\":23420,\"start\":23012},{\"end\":24180,\"start\":23547},{\"end\":24622,\"start\":24182},{\"end\":24955,\"start\":24805},{\"end\":25018,\"start\":24957},{\"end\":25423,\"start\":25086},{\"end\":25899,\"start\":25425},{\"end\":26314,\"start\":25964},{\"end\":26583,\"start\":26316},{\"end\":26900,\"start\":26628},{\"end\":27416,\"start\":26902},{\"end\":27767,\"start\":27456},{\"end\":27957,\"start\":27769},{\"end\":27998,\"start\":27959},{\"end\":28096,\"start\":28070},{\"end\":28326,\"start\":28156},{\"end\":28537,\"start\":28328},{\"end\":29318,\"start\":28550},{\"end\":29697,\"start\":29320},{\"end\":30396,\"start\":29725},{\"end\":30502,\"start\":30398},{\"end\":30657,\"start\":30552},{\"end\":31039,\"start\":30659},{\"end\":31611,\"start\":31122},{\"end\":32006,\"start\":31613},{\"end\":32156,\"start\":32061},{\"end\":32354,\"start\":32178},{\"end\":32443,\"start\":32356},{\"end\":32539,\"start\":32469},{\"end\":32598,\"start\":32541},{\"end\":33202,\"start\":32660},{\"end\":33263,\"start\":33204},{\"end\":34180,\"start\":33284},{\"end\":34584,\"start\":34209},{\"end\":35142,\"start\":34586},{\"end\":35288,\"start\":35144},{\"end\":35689,\"start\":35290},{\"end\":35747,\"start\":35691},{\"end\":35927,\"start\":35810},{\"end\":36127,\"start\":35929},{\"end\":37864,\"start\":36129},{\"end\":38761,\"start\":37889},{\"end\":39102,\"start\":38781},{\"end\":39939,\"start\":39130},{\"end\":40302,\"start\":39941},{\"end\":40771,\"start\":40304},{\"end\":41464,\"start\":40773},{\"end\":42362,\"start\":41466},{\"end\":42988,\"start\":42364},{\"end\":43752,\"start\":43004},{\"end\":44940,\"start\":43754},{\"end\":45148,\"start\":44962},{\"end\":45673,\"start\":45173},{\"end\":45934,\"start\":45675},{\"end\":46325,\"start\":45936},{\"end\":46843,\"start\":46366},{\"end\":47087,\"start\":46845},{\"end\":47972,\"start\":47089},{\"end\":48115,\"start\":47974},{\"end\":48913,\"start\":48138},{\"end\":49441,\"start\":48915},{\"end\":49664,\"start\":49496},{\"end\":50349,\"start\":49666},{\"end\":50395,\"start\":50351},{\"end\":51133,\"start\":50433},{\"end\":51213,\"start\":51148},{\"end\":51709,\"start\":51215},{\"end\":52391,\"start\":51711},{\"end\":53033,\"start\":52393},{\"end\":53858,\"start\":53035},{\"end\":54725,\"start\":53860},{\"end\":55213,\"start\":54727},{\"end\":56608,\"start\":55215},{\"end\":56891,\"start\":56610},{\"end\":57316,\"start\":56893},{\"end\":57654,\"start\":57318},{\"end\":58076,\"start\":57656},{\"end\":58452,\"start\":58078},{\"end\":58733,\"start\":58454},{\"end\":59380,\"start\":58735},{\"end\":60785,\"start\":59396}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20587,\"start\":20558},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23011,\"start\":22960},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23546,\"start\":23421},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24711,\"start\":24623},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24804,\"start\":24711},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25085,\"start\":25019},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25963,\"start\":25900},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26627,\"start\":26584},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27455,\"start\":27417},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28069,\"start\":27999},{\"attributes\":{\"id\":\"formula_11\"},\"end\":28155,\"start\":28097},{\"attributes\":{\"id\":\"formula_12\"},\"end\":30551,\"start\":30503},{\"attributes\":{\"id\":\"formula_13\"},\"end\":31121,\"start\":31040},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32060,\"start\":32007},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32177,\"start\":32157},{\"attributes\":{\"id\":\"formula_16\"},\"end\":32468,\"start\":32444},{\"attributes\":{\"id\":\"formula_17\"},\"end\":32659,\"start\":32599},{\"attributes\":{\"id\":\"formula_19\"},\"end\":35809,\"start\":35748},{\"attributes\":{\"id\":\"formula_20\"},\"end\":49495,\"start\":49442},{\"attributes\":{\"id\":\"formula_21\"},\"end\":50432,\"start\":50396}]", "table_ref": "[{\"end\":21827,\"start\":21820}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2080,\"start\":2068},{\"attributes\":{\"n\":\"2\"},\"end\":10081,\"start\":10069},{\"attributes\":{\"n\":\"3\"},\"end\":19425,\"start\":19408},{\"attributes\":{\"n\":\"4\"},\"end\":28548,\"start\":28540},{\"attributes\":{\"n\":\"4.1\"},\"end\":29723,\"start\":29700},{\"end\":33282,\"start\":33266},{\"attributes\":{\"n\":\"4.2\"},\"end\":34207,\"start\":34183},{\"attributes\":{\"n\":\"4.3\"},\"end\":37887,\"start\":37867},{\"attributes\":{\"n\":\"5\"},\"end\":38779,\"start\":38764},{\"attributes\":{\"n\":\"5.1\"},\"end\":39128,\"start\":39105},{\"attributes\":{\"n\":\"5.2\"},\"end\":43002,\"start\":42991},{\"attributes\":{\"n\":\"5.3\"},\"end\":44960,\"start\":44943},{\"attributes\":{\"n\":\"5.4\"},\"end\":45171,\"start\":45151},{\"attributes\":{\"n\":\"5.5\"},\"end\":46364,\"start\":46328},{\"attributes\":{\"n\":\"6\"},\"end\":48136,\"start\":48118},{\"attributes\":{\"n\":\"7\"},\"end\":51146,\"start\":51136},{\"attributes\":{\"n\":\"8\"},\"end\":59394,\"start\":59383},{\"end\":60797,\"start\":60787},{\"end\":60932,\"start\":60922},{\"end\":61454,\"start\":61444},{\"end\":61695,\"start\":61685},{\"end\":61968,\"start\":61958},{\"end\":62164,\"start\":62154},{\"end\":62374,\"start\":62363},{\"end\":62645,\"start\":62634},{\"end\":62746,\"start\":62735},{\"end\":62970,\"start\":62959},{\"end\":63125,\"start\":63114}]", "table": null, "figure_caption": "[{\"end\":60920,\"start\":60799},{\"end\":61442,\"start\":60934},{\"end\":61683,\"start\":61456},{\"end\":61746,\"start\":61697},{\"end\":61956,\"start\":61749},{\"end\":62152,\"start\":61970},{\"end\":62361,\"start\":62166},{\"end\":62632,\"start\":62377},{\"end\":62733,\"start\":62648},{\"end\":62957,\"start\":62749},{\"end\":63112,\"start\":62973},{\"end\":63348,\"start\":63128},{\"end\":63706,\"start\":63351}]", "figure_ref": "[{\"end\":3367,\"start\":3360},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3717,\"start\":3711},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7843,\"start\":7837},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19496,\"start\":19490},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20102,\"start\":20096},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20318,\"start\":20308},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20436,\"start\":20427},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20557,\"start\":20551},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20815,\"start\":20805},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20929,\"start\":20919},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21241,\"start\":21232},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25794,\"start\":25786},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29009,\"start\":29003},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29202,\"start\":29196},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31663,\"start\":31655},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32442,\"start\":32436},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34022,\"start\":34014},{\"end\":35325,\"start\":35317},{\"end\":36808,\"start\":36800},{\"end\":37202,\"start\":37194},{\"end\":38024,\"start\":38016},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":39891,\"start\":39875},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40486,\"start\":40477},{\"end\":41310,\"start\":41301},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41710,\"start\":41693},{\"end\":42601,\"start\":42594},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42748,\"start\":42739},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44376,\"start\":44367},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":45438,\"start\":45430},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":49521,\"start\":49514},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":53368,\"start\":53359},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":56189,\"start\":56180}]", "bib_author_first_name": "[{\"end\":64947,\"start\":64946},{\"end\":64956,\"start\":64955},{\"end\":64967,\"start\":64966},{\"end\":65258,\"start\":65257},{\"end\":65267,\"start\":65266},{\"end\":65278,\"start\":65277},{\"end\":65616,\"start\":65615},{\"end\":65625,\"start\":65624},{\"end\":65636,\"start\":65635},{\"end\":65647,\"start\":65646},{\"end\":66301,\"start\":66300},{\"end\":66313,\"start\":66312},{\"end\":66327,\"start\":66326},{\"end\":66337,\"start\":66336},{\"end\":66346,\"start\":66345},{\"end\":66357,\"start\":66356},{\"end\":66644,\"start\":66643},{\"end\":66653,\"start\":66652},{\"end\":66894,\"start\":66893},{\"end\":66902,\"start\":66901},{\"end\":66912,\"start\":66911},{\"end\":66921,\"start\":66920},{\"end\":66923,\"start\":66922},{\"end\":67313,\"start\":67312},{\"end\":67322,\"start\":67321},{\"end\":67525,\"start\":67524},{\"end\":67533,\"start\":67532},{\"end\":67540,\"start\":67539},{\"end\":67955,\"start\":67954},{\"end\":67966,\"start\":67965},{\"end\":68208,\"start\":68207},{\"end\":68221,\"start\":68220},{\"end\":68233,\"start\":68232},{\"end\":68245,\"start\":68241},{\"end\":68465,\"start\":68461},{\"end\":68600,\"start\":68599},{\"end\":68612,\"start\":68611},{\"end\":68614,\"start\":68613},{\"end\":68978,\"start\":68977},{\"end\":68988,\"start\":68987},{\"end\":68997,\"start\":68996},{\"end\":69007,\"start\":69006},{\"end\":69020,\"start\":69019},{\"end\":69258,\"start\":69257},{\"end\":69268,\"start\":69267},{\"end\":69282,\"start\":69281},{\"end\":69298,\"start\":69294},{\"end\":69803,\"start\":69802},{\"end\":69816,\"start\":69815},{\"end\":69825,\"start\":69824},{\"end\":69836,\"start\":69835},{\"end\":70205,\"start\":70204},{\"end\":70207,\"start\":70206},{\"end\":70216,\"start\":70215},{\"end\":70218,\"start\":70217},{\"end\":70427,\"start\":70426},{\"end\":70436,\"start\":70435},{\"end\":70881,\"start\":70880},{\"end\":70890,\"start\":70889},{\"end\":70901,\"start\":70900},{\"end\":70909,\"start\":70908},{\"end\":71185,\"start\":71184},{\"end\":71194,\"start\":71193},{\"end\":71205,\"start\":71204},{\"end\":71656,\"start\":71655},{\"end\":71658,\"start\":71657},{\"end\":71665,\"start\":71664},{\"end\":71667,\"start\":71666},{\"end\":71676,\"start\":71675},{\"end\":71678,\"start\":71677},{\"end\":72165,\"start\":72164},{\"end\":72177,\"start\":72176},{\"end\":72730,\"start\":72729},{\"end\":72732,\"start\":72731},{\"end\":72742,\"start\":72741},{\"end\":72744,\"start\":72743},{\"end\":73054,\"start\":73053},{\"end\":73056,\"start\":73055},{\"end\":73062,\"start\":73061},{\"end\":73277,\"start\":73276},{\"end\":73279,\"start\":73278},{\"end\":73285,\"start\":73284},{\"end\":73547,\"start\":73546},{\"end\":73549,\"start\":73548},{\"end\":73558,\"start\":73557},{\"end\":73569,\"start\":73568},{\"end\":74186,\"start\":74185},{\"end\":74188,\"start\":74187},{\"end\":74197,\"start\":74196},{\"end\":74199,\"start\":74198},{\"end\":74620,\"start\":74619},{\"end\":74622,\"start\":74621},{\"end\":74631,\"start\":74630},{\"end\":74642,\"start\":74641},{\"end\":74644,\"start\":74643},{\"end\":74930,\"start\":74929},{\"end\":74939,\"start\":74938},{\"end\":74949,\"start\":74948},{\"end\":75191,\"start\":75190},{\"end\":75201,\"start\":75200},{\"end\":75211,\"start\":75210},{\"end\":75702,\"start\":75701},{\"end\":75710,\"start\":75709},{\"end\":75889,\"start\":75888},{\"end\":75900,\"start\":75899},{\"end\":75902,\"start\":75901},{\"end\":76096,\"start\":76095},{\"end\":76109,\"start\":76108},{\"end\":76119,\"start\":76118},{\"end\":76130,\"start\":76129},{\"end\":76132,\"start\":76131},{\"end\":76414,\"start\":76413},{\"end\":76422,\"start\":76421},{\"end\":76431,\"start\":76430},{\"end\":76665,\"start\":76664},{\"end\":76678,\"start\":76677},{\"end\":76691,\"start\":76690},{\"end\":76701,\"start\":76700},{\"end\":76712,\"start\":76711},{\"end\":76723,\"start\":76722},{\"end\":76732,\"start\":76731},{\"end\":76745,\"start\":76744},{\"end\":77233,\"start\":77232},{\"end\":77242,\"start\":77241},{\"end\":77500,\"start\":77499},{\"end\":77512,\"start\":77511},{\"end\":78008,\"start\":78007},{\"end\":78015,\"start\":78014},{\"end\":78026,\"start\":78025},{\"end\":78687,\"start\":78686},{\"end\":78696,\"start\":78695},{\"end\":78707,\"start\":78706},{\"end\":78709,\"start\":78708},{\"end\":79023,\"start\":79022},{\"end\":79025,\"start\":79024},{\"end\":79033,\"start\":79032},{\"end\":79523,\"start\":79522},{\"end\":79525,\"start\":79524},{\"end\":79533,\"start\":79532},{\"end\":79542,\"start\":79541},{\"end\":79779,\"start\":79778},{\"end\":79788,\"start\":79787},{\"end\":79799,\"start\":79798},{\"end\":79809,\"start\":79808}]", "bib_author_last_name": "[{\"end\":64953,\"start\":64948},{\"end\":64964,\"start\":64957},{\"end\":64975,\"start\":64968},{\"end\":65264,\"start\":65259},{\"end\":65275,\"start\":65268},{\"end\":65286,\"start\":65279},{\"end\":65622,\"start\":65617},{\"end\":65633,\"start\":65626},{\"end\":65644,\"start\":65637},{\"end\":65657,\"start\":65648},{\"end\":66310,\"start\":66302},{\"end\":66324,\"start\":66314},{\"end\":66334,\"start\":66328},{\"end\":66343,\"start\":66338},{\"end\":66354,\"start\":66347},{\"end\":66363,\"start\":66358},{\"end\":66650,\"start\":66645},{\"end\":66661,\"start\":66654},{\"end\":66899,\"start\":66895},{\"end\":66909,\"start\":66903},{\"end\":66918,\"start\":66913},{\"end\":66929,\"start\":66924},{\"end\":67319,\"start\":67314},{\"end\":67330,\"start\":67323},{\"end\":67530,\"start\":67526},{\"end\":67537,\"start\":67534},{\"end\":67546,\"start\":67541},{\"end\":67963,\"start\":67956},{\"end\":67977,\"start\":67967},{\"end\":68218,\"start\":68209},{\"end\":68230,\"start\":68222},{\"end\":68239,\"start\":68234},{\"end\":68252,\"start\":68246},{\"end\":68473,\"start\":68466},{\"end\":68609,\"start\":68601},{\"end\":68620,\"start\":68615},{\"end\":68985,\"start\":68979},{\"end\":68994,\"start\":68989},{\"end\":69004,\"start\":68998},{\"end\":69017,\"start\":69008},{\"end\":69027,\"start\":69021},{\"end\":69265,\"start\":69259},{\"end\":69279,\"start\":69269},{\"end\":69292,\"start\":69283},{\"end\":69305,\"start\":69299},{\"end\":69813,\"start\":69804},{\"end\":69822,\"start\":69817},{\"end\":69833,\"start\":69826},{\"end\":69842,\"start\":69837},{\"end\":70213,\"start\":70208},{\"end\":70224,\"start\":70219},{\"end\":70433,\"start\":70428},{\"end\":70444,\"start\":70437},{\"end\":70887,\"start\":70882},{\"end\":70898,\"start\":70891},{\"end\":70906,\"start\":70902},{\"end\":70918,\"start\":70910},{\"end\":71191,\"start\":71186},{\"end\":71202,\"start\":71195},{\"end\":71214,\"start\":71206},{\"end\":71662,\"start\":71659},{\"end\":71673,\"start\":71668},{\"end\":71682,\"start\":71679},{\"end\":72174,\"start\":72166},{\"end\":72184,\"start\":72178},{\"end\":72739,\"start\":72733},{\"end\":72751,\"start\":72745},{\"end\":73059,\"start\":73057},{\"end\":73067,\"start\":73063},{\"end\":73282,\"start\":73280},{\"end\":73290,\"start\":73286},{\"end\":73555,\"start\":73550},{\"end\":73566,\"start\":73559},{\"end\":73574,\"start\":73570},{\"end\":74194,\"start\":74189},{\"end\":74205,\"start\":74200},{\"end\":74628,\"start\":74623},{\"end\":74639,\"start\":74632},{\"end\":74650,\"start\":74645},{\"end\":74936,\"start\":74931},{\"end\":74946,\"start\":74940},{\"end\":74954,\"start\":74950},{\"end\":75198,\"start\":75192},{\"end\":75208,\"start\":75202},{\"end\":75219,\"start\":75212},{\"end\":75707,\"start\":75703},{\"end\":75719,\"start\":75711},{\"end\":75897,\"start\":75890},{\"end\":75909,\"start\":75903},{\"end\":76106,\"start\":76097},{\"end\":76116,\"start\":76110},{\"end\":76127,\"start\":76120},{\"end\":76138,\"start\":76133},{\"end\":76419,\"start\":76415},{\"end\":76428,\"start\":76423},{\"end\":76439,\"start\":76432},{\"end\":76675,\"start\":76666},{\"end\":76688,\"start\":76679},{\"end\":76698,\"start\":76692},{\"end\":76709,\"start\":76702},{\"end\":76720,\"start\":76713},{\"end\":76729,\"start\":76724},{\"end\":76742,\"start\":76733},{\"end\":76755,\"start\":76746},{\"end\":77239,\"start\":77234},{\"end\":77248,\"start\":77243},{\"end\":77509,\"start\":77501},{\"end\":77519,\"start\":77513},{\"end\":78012,\"start\":78009},{\"end\":78023,\"start\":78016},{\"end\":78044,\"start\":78027},{\"end\":78693,\"start\":78688},{\"end\":78704,\"start\":78697},{\"end\":78715,\"start\":78710},{\"end\":79030,\"start\":79026},{\"end\":79042,\"start\":79034},{\"end\":79530,\"start\":79526},{\"end\":79539,\"start\":79534},{\"end\":79550,\"start\":79543},{\"end\":79785,\"start\":79780},{\"end\":79796,\"start\":79789},{\"end\":79806,\"start\":79800},{\"end\":79817,\"start\":79810}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9825759},\"end\":65169,\"start\":64895},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":5095817},\"end\":65509,\"start\":65171},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":110009},\"end\":66249,\"start\":65511},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3423879},\"end\":66591,\"start\":66251},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13936317},\"end\":66835,\"start\":66593},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206592437},\"end\":67253,\"start\":66837},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5837861},\"end\":67488,\"start\":67255},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3052384},\"end\":67756,\"start\":67490},{\"attributes\":{\"id\":\"b8\"},\"end\":67889,\"start\":67758},{\"attributes\":{\"id\":\"b9\"},\"end\":68134,\"start\":67891},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1648505},\"end\":68457,\"start\":68136},{\"attributes\":{\"id\":\"b11\"},\"end\":68540,\"start\":68459},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4636468},\"end\":68925,\"start\":68542},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7245910},\"end\":69216,\"start\":68927},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7603110},\"end\":69723,\"start\":69218},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":19014218},\"end\":70176,\"start\":69725},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":493732},\"end\":70351,\"start\":70178},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2473703},\"end\":70816,\"start\":70353},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13380763},\"end\":71135,\"start\":70818},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7295606},\"end\":71582,\"start\":71137},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12944570},\"end\":72109,\"start\":71584},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":16425206},\"end\":72695,\"start\":72111},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":122862057},\"end\":73003,\"start\":72697},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3092633},\"end\":73216,\"start\":73005},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14390548},\"end\":73445,\"start\":73218},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12672235},\"end\":74135,\"start\":73447},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":17868098},\"end\":74565,\"start\":74137},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13940793},\"end\":74860,\"start\":74567},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":16247169},\"end\":75128,\"start\":74862},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3156486},\"end\":75639,\"start\":75130},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13962529},\"end\":75886,\"start\":75641},{\"attributes\":{\"id\":\"b31\"},\"end\":76045,\"start\":75888},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":5551454},\"end\":76357,\"start\":76047},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15856176},\"end\":76577,\"start\":76359},{\"attributes\":{\"doi\":\"AFRL-HE- WP-TR-2002-0169\",\"id\":\"b34\"},\"end\":77098,\"start\":76579},{\"attributes\":{\"id\":\"b35\"},\"end\":77462,\"start\":77100},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5746271},\"end\":77929,\"start\":77464},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14920447},\"end\":78603,\"start\":77931},{\"attributes\":{\"id\":\"b38\"},\"end\":78936,\"start\":78605},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6937392},\"end\":79471,\"start\":78938},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8109106},\"end\":79734,\"start\":79473},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9372718},\"end\":79994,\"start\":79736}]", "bib_title": "[{\"end\":64944,\"start\":64895},{\"end\":65255,\"start\":65171},{\"end\":65613,\"start\":65511},{\"end\":66298,\"start\":66251},{\"end\":66641,\"start\":66593},{\"end\":66891,\"start\":66837},{\"end\":67310,\"start\":67255},{\"end\":67522,\"start\":67490},{\"end\":67952,\"start\":67891},{\"end\":68205,\"start\":68136},{\"end\":68597,\"start\":68542},{\"end\":68975,\"start\":68927},{\"end\":69255,\"start\":69218},{\"end\":69800,\"start\":69725},{\"end\":70202,\"start\":70178},{\"end\":70424,\"start\":70353},{\"end\":70878,\"start\":70818},{\"end\":71182,\"start\":71137},{\"end\":71653,\"start\":71584},{\"end\":72162,\"start\":72111},{\"end\":72727,\"start\":72697},{\"end\":73051,\"start\":73005},{\"end\":73274,\"start\":73218},{\"end\":73544,\"start\":73447},{\"end\":74183,\"start\":74137},{\"end\":74617,\"start\":74567},{\"end\":74927,\"start\":74862},{\"end\":75188,\"start\":75130},{\"end\":75699,\"start\":75641},{\"end\":76093,\"start\":76047},{\"end\":76411,\"start\":76359},{\"end\":77230,\"start\":77100},{\"end\":77497,\"start\":77464},{\"end\":78005,\"start\":77931},{\"end\":78684,\"start\":78605},{\"end\":79020,\"start\":78938},{\"end\":79520,\"start\":79473},{\"end\":79776,\"start\":79736}]", "bib_author": "[{\"end\":64955,\"start\":64946},{\"end\":64966,\"start\":64955},{\"end\":64977,\"start\":64966},{\"end\":65266,\"start\":65257},{\"end\":65277,\"start\":65266},{\"end\":65288,\"start\":65277},{\"end\":65624,\"start\":65615},{\"end\":65635,\"start\":65624},{\"end\":65646,\"start\":65635},{\"end\":65659,\"start\":65646},{\"end\":66312,\"start\":66300},{\"end\":66326,\"start\":66312},{\"end\":66336,\"start\":66326},{\"end\":66345,\"start\":66336},{\"end\":66356,\"start\":66345},{\"end\":66365,\"start\":66356},{\"end\":66652,\"start\":66643},{\"end\":66663,\"start\":66652},{\"end\":66901,\"start\":66893},{\"end\":66911,\"start\":66901},{\"end\":66920,\"start\":66911},{\"end\":66931,\"start\":66920},{\"end\":67321,\"start\":67312},{\"end\":67332,\"start\":67321},{\"end\":67532,\"start\":67524},{\"end\":67539,\"start\":67532},{\"end\":67548,\"start\":67539},{\"end\":67965,\"start\":67954},{\"end\":67979,\"start\":67965},{\"end\":68220,\"start\":68207},{\"end\":68232,\"start\":68220},{\"end\":68241,\"start\":68232},{\"end\":68254,\"start\":68241},{\"end\":68475,\"start\":68461},{\"end\":68611,\"start\":68599},{\"end\":68622,\"start\":68611},{\"end\":68987,\"start\":68977},{\"end\":68996,\"start\":68987},{\"end\":69006,\"start\":68996},{\"end\":69019,\"start\":69006},{\"end\":69029,\"start\":69019},{\"end\":69267,\"start\":69257},{\"end\":69281,\"start\":69267},{\"end\":69294,\"start\":69281},{\"end\":69307,\"start\":69294},{\"end\":69815,\"start\":69802},{\"end\":69824,\"start\":69815},{\"end\":69835,\"start\":69824},{\"end\":69844,\"start\":69835},{\"end\":70215,\"start\":70204},{\"end\":70226,\"start\":70215},{\"end\":70435,\"start\":70426},{\"end\":70446,\"start\":70435},{\"end\":70889,\"start\":70880},{\"end\":70900,\"start\":70889},{\"end\":70908,\"start\":70900},{\"end\":70920,\"start\":70908},{\"end\":71193,\"start\":71184},{\"end\":71204,\"start\":71193},{\"end\":71216,\"start\":71204},{\"end\":71664,\"start\":71655},{\"end\":71675,\"start\":71664},{\"end\":71684,\"start\":71675},{\"end\":72176,\"start\":72164},{\"end\":72186,\"start\":72176},{\"end\":72741,\"start\":72729},{\"end\":72753,\"start\":72741},{\"end\":73061,\"start\":73053},{\"end\":73069,\"start\":73061},{\"end\":73284,\"start\":73276},{\"end\":73292,\"start\":73284},{\"end\":73557,\"start\":73546},{\"end\":73568,\"start\":73557},{\"end\":73576,\"start\":73568},{\"end\":74196,\"start\":74185},{\"end\":74207,\"start\":74196},{\"end\":74630,\"start\":74619},{\"end\":74641,\"start\":74630},{\"end\":74652,\"start\":74641},{\"end\":74938,\"start\":74929},{\"end\":74948,\"start\":74938},{\"end\":74956,\"start\":74948},{\"end\":75200,\"start\":75190},{\"end\":75210,\"start\":75200},{\"end\":75221,\"start\":75210},{\"end\":75709,\"start\":75701},{\"end\":75721,\"start\":75709},{\"end\":75899,\"start\":75888},{\"end\":75911,\"start\":75899},{\"end\":76108,\"start\":76095},{\"end\":76118,\"start\":76108},{\"end\":76129,\"start\":76118},{\"end\":76140,\"start\":76129},{\"end\":76421,\"start\":76413},{\"end\":76430,\"start\":76421},{\"end\":76441,\"start\":76430},{\"end\":76677,\"start\":76664},{\"end\":76690,\"start\":76677},{\"end\":76700,\"start\":76690},{\"end\":76711,\"start\":76700},{\"end\":76722,\"start\":76711},{\"end\":76731,\"start\":76722},{\"end\":76744,\"start\":76731},{\"end\":76757,\"start\":76744},{\"end\":77241,\"start\":77232},{\"end\":77250,\"start\":77241},{\"end\":77511,\"start\":77499},{\"end\":77521,\"start\":77511},{\"end\":78014,\"start\":78007},{\"end\":78025,\"start\":78014},{\"end\":78046,\"start\":78025},{\"end\":78695,\"start\":78686},{\"end\":78706,\"start\":78695},{\"end\":78717,\"start\":78706},{\"end\":79032,\"start\":79022},{\"end\":79044,\"start\":79032},{\"end\":79532,\"start\":79522},{\"end\":79541,\"start\":79532},{\"end\":79552,\"start\":79541},{\"end\":79787,\"start\":79778},{\"end\":79798,\"start\":79787},{\"end\":79808,\"start\":79798},{\"end\":79819,\"start\":79808}]", "bib_venue": "[{\"end\":65907,\"start\":65768},{\"end\":67059,\"start\":66999},{\"end\":69482,\"start\":69392},{\"end\":69905,\"start\":69898},{\"end\":70591,\"start\":70518},{\"end\":71360,\"start\":71288},{\"end\":71859,\"start\":71767},{\"end\":72435,\"start\":72295},{\"end\":72822,\"start\":72800},{\"end\":73772,\"start\":73667},{\"end\":74688,\"start\":74674},{\"end\":75396,\"start\":75306},{\"end\":75747,\"start\":75738},{\"end\":75943,\"start\":75935},{\"end\":76166,\"start\":76157},{\"end\":77695,\"start\":77593},{\"end\":78297,\"start\":78156},{\"end\":78743,\"start\":78734},{\"end\":79219,\"start\":79127},{\"end\":65011,\"start\":64977},{\"end\":65322,\"start\":65288},{\"end\":65766,\"start\":65659},{\"end\":66399,\"start\":66365},{\"end\":66697,\"start\":66663},{\"end\":66997,\"start\":66931},{\"end\":67355,\"start\":67332},{\"end\":67608,\"start\":67548},{\"end\":67798,\"start\":67758},{\"end\":67996,\"start\":67979},{\"end\":68277,\"start\":68254},{\"end\":68662,\"start\":68622},{\"end\":69052,\"start\":69029},{\"end\":69390,\"start\":69307},{\"end\":69884,\"start\":69844},{\"end\":70242,\"start\":70226},{\"end\":70516,\"start\":70446},{\"end\":70954,\"start\":70920},{\"end\":71286,\"start\":71216},{\"end\":71765,\"start\":71684},{\"end\":72293,\"start\":72186},{\"end\":72798,\"start\":72753},{\"end\":73085,\"start\":73069},{\"end\":73308,\"start\":73292},{\"end\":73665,\"start\":73576},{\"end\":74233,\"start\":74207},{\"end\":74672,\"start\":74652},{\"end\":74972,\"start\":74956},{\"end\":75304,\"start\":75221},{\"end\":75736,\"start\":75721},{\"end\":75933,\"start\":75911},{\"end\":76155,\"start\":76140},{\"end\":76454,\"start\":76441},{\"end\":76662,\"start\":76579},{\"end\":77259,\"start\":77250},{\"end\":77591,\"start\":77521},{\"end\":78154,\"start\":78046},{\"end\":78732,\"start\":78717},{\"end\":79125,\"start\":79044},{\"end\":79586,\"start\":79552},{\"end\":79842,\"start\":79819}]"}}}, "year": 2023, "month": 12, "day": 17}