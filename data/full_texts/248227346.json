{"id": 248227346, "updated": "2023-10-05 15:08:51.299", "metadata": {"title": "Efficient Reinforcement Learning for Unsupervised Controlled Text Generation", "authors": "[{\"first\":\"Bhargav\",\"last\":\"Upadhyay\",\"middle\":[]},{\"first\":\"Akhilesh\",\"last\":\"Sudhakar\",\"middle\":[]},{\"first\":\"Arjun\",\"last\":\"Maheswaran\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Controlled text generation tasks such as unsupervised text style transfer have increasingly adopted the use of Reinforcement Learning (RL). A major challenge in applying RL to such tasks is the sparse reward, which is available only after the full text is generated. Sparse rewards, combined with a large action space make RL training sample-inefficient and difficult to converge. Recently proposed reward-shaping strategies to address this issue have shown only negligible gains. In contrast, this work proposes a novel approach that provides dense rewards to each generated token. We evaluate our approach by its usage in unsupervised text style transfer. Averaged across datasets, our style transfer system improves upon current state-of-art systems by 21\\% on human evaluation and 12\\% on automatic evaluation. Upon ablated comparison with the current reward shaping approach (the `roll-out strategy'), using dense rewards improves the overall style transfer quality by 22\\% based on human evaluation. Further the RL training is 2.5 times as sample efficient, and 7 times faster.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.07696", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2204-07696", "doi": "10.48550/arxiv.2204.07696"}}, "content": {"source": {"pdf_hash": "9373f5d52237d0e54b3a48182b8b56ea0f7e00cf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.07696v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "59324cb81a51f2d1bdeecf45ed79eba03e5ecae0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9373f5d52237d0e54b3a48182b8b56ea0f7e00cf.txt", "contents": "\nEfficient Reinforcement Learning for Unsupervised Controlled Text Generation\n\n\nBhargav Upadhyay bhargav@agaralabs.com \nAkhilesh Sudhakar akhilesh@agaralabs.com \nArjun Maheswaran \nEfficient Reinforcement Learning for Unsupervised Controlled Text Generation\n\nControlled text generation tasks such as unsupervised text style transfer have increasingly adopted the use of Reinforcement Learning (RL). A major challenge in applying RL to such tasks is the sparse reward, which is available only after the full text is generated. Sparse rewards, combined with a large action space make RL training sample-inefficient and difficult to converge. Recently proposed rewardshaping strategies to address this issue have shown only negligible gains. In contrast, this work proposes a novel approach that provides dense rewards to each generated token. We evaluate our approach by its usage in unsupervised text style transfer. Averaged across datasets, our style transfer system improves upon current state-of-art systems by 21% on human evaluation and 12% on automatic evaluation. Upon ablated comparison with the current reward shaping approach (the 'roll-out strategy'), using dense rewards improves the overall style transfer quality by 22% based on human evaluation. Further the RL training is 2.5 times as sample efficient, and 7 times faster.definition of style that includes sentiment and formality arXiv:2204.07696v1 [cs.CL]\n\nIntroduction\n\nReinforcement Learning (RL) has been used in NLG tasks including neural machine translation (NMT) (Bahdanau et al., 2017), abstractive summarization (Paulus et al., 2018), and specifically in controlled text generation tasks such as paraphrase generation (Li et al., 2018c), sentence simplification (Zhang and Lapata, 2017) and unsupervised text style transfer (Luo et al., 2019;Gong et al., 2019).\n\nUnsupervised text style transfer is the task of re-writing text of a given style into a target style without using a parallel corpus of source style and target style sentences for training. For example, in the case of sentiment transfer 1 , 'the pizza was 1 Similar to previous works, we use a broad socio-linguistic rather bland' can be re-written with positive sentiment as 'the pizza was quite delicious'.\n\nRL has gained popularity in this task as it allows to directly optimize for commonly agreed upon objectives of style transfer, i.e, the generated sentences should 1) possess the target style, 2) preserve the non-stylistic parts (content) of the source sentence, and 3) be fluent and natural sounding. However these objectives are typically measured only at the level of the entire generated text and hence, any RL rewards based on them are only available as 'sparse rewards'. For example, Gong et al. (2019) use a style classifier model to provide a target style reward, which requires the complete generated text. On the other hand, generative RL models generate token-by-token, and require 'credit assignment' of reward to each individual token, based on their contribution to the overall sparse reward.\n\nTo address this problem, previous work uses different 'reward shaping' strategies over the sparse reward, of which, the 'roll-out strategy' based on Monte Carlo Tree Search (Kocsis and Szepesv\u00e1ri, 2006) is the commonly used approach. In the context of style transfer, Gong et al. (2019) use the roll-out strategy while Luo et al. (2019) use a naive reward shaping strategy of assigning the same discounted sparse reward to every token. However, as observed by Wu et al. (2018), the gains from these reward shaping strategies are negligible in practice. Since NLG tasks have a large action space (the vocabulary), sparse rewards leads to high variance in estimating 'returns' for each token used in policy gradient updates. This makes the training sample-inefficient, and can result in a sub-optimal policy.\n\nIn contrast to the roll-out strategy, we propose a novel approach to provide dense rewards directly to each generated token. The primary differentiating factor is that these dense rewards are obtained directly through reward models trained using the same training style transfer data.\n\nOur method of providing dense rewards is inspired by  and Sudhakar et al. (2019).  propose using Inverse Reinforcement Learning (IRL) (Ziebart et al., 2008) to compute dense rewards for unsupervised text generation. Their objective is to learn un-conditional generation of sentences that belong to a particular distribution for which they use a language model's token-level likelihood as a dense reward approximator. Sudhakar et al. (2019) disentangle style tokens and content tokens using self-attention scores of trained BERT style-classifier (Devlin et al., 2018). Style tokens are those that contribute to the style of the overall text. Content tokens are non-style tokens.\n\nWe propose separate dense rewards at the tokenlevel for each of target style, content preservation and fluency. a) For the style reward, we propose the use of self-attention scores of a trained style classifier (Sudhakar et al., 2019). b) For content preservation, we propose a simple n-gram based dense reward, motivated by BLEU (Papineni et al., 2002), and applicable to only content tokens. c) For the fluency reward, we use a pre-trained language model's token-level likelihood, motivated by .\n\nThe main contribution of our work is in that we propose a method that addresses the sparse reward problem for RL-based unsupervised text style transfer. We demonstrate how the resulting dense rewards can be used in designing a state-ofart RL-based style transfer system. This method, as compared to the roll-out strategy is a) sample efficient, b) converges to a better reward and c) computes token level reward in O(T ) as against O(T 2 ), where T is the number of tokens in the output. The method we propose is extensible to other controlled text generation problems.\n\n\nRelated Work\n\nThe previous section refers to RL-based related work in unsupervised text style transfer. We discuss a few more aspects about these works here. Xu et al. (2018) uses a cycled RL approach where a neutralization model disentangles the content and attributes, and then passes the content to an emotionalization model. Similar to our work, Gong et al. (2019) use three different rewards to control the style, content preservation and fluency. Their work also performs an MLE pre-training step prior to the RL training. Luo et al. (2019) use two RL models -one for each direction of style transfer, provide rewards for style and content, and do not disentangle content and style tokens. They alternate between MLE training and RL training steps.\n\nNon-RL based approaches too have been used for unsupervised text style transfer. Some of these works encode style and content in separate latent representations, and decode the style-dependent output from these representations (Fu et al., 2018;Hu et al., 2017;Yang et al., 2018). Other works that attempt to learn disentangled latent representations include John et al. (2019); ; Wu et al. (2019a); Liu et al. (2019). A few others explicitly identify attribute and content words from input texts and then train models to generate target style sentences from the content (Li et al., 2018a;Sudhakar et al., 2019;Wu et al., 2019b). A few recent works (Lample et al., 2018;Luo et al., 2019) avoid style-content disentanglement altogether.\n\nRL has also been used for unconditional text generation tasks.  propose the use of Inverse RL (IRL), Schmidt and Hofmann (2020) use a retrieval-based method, comparing the generated sentence with the retrieved reference using contextual BERT embeddings to get compute dense rewards, while Yu et al. (2017) use the roll-out strategy.\n\n\nProblem Statement\n\nWe assume a dataset D = {(x 1 , s 1 ), ..., (x m , s m )} where each sentence x i is associated with a specific style s i \u2208 S. For instance, for the task of sentiment transfer, S = {'Positive', 'Negative'}, and for formality transfer, S = {'Formal', 'Informal'}. We then aim to learn the conditional distribution P (y|x, s tgt ) such that the target (tgt) style of y is s tgt , and y's content is similar to that of x. 'Content' refers to the non-stylized part of the input.\n\n\nOur Approach\n\nWe introduce the efficient Dense (Reward) Reinforcement Learning based Style Transformer (hereby, DRL). DRL takes the source sentence x of style s src as input and generates the output sentence y of style s tgt . More formally, it learns:\nP (y|x, s tgt ; \u03b8)(1)\nThe RL-based setup consists of: a) a generative model, b) the training strategy and c) the rewards.\n\n\nModel\n\nThe architecture of DRL is a decoder-only Transformer, based on the implementation of the Generative Pre-trained Transformer (Radford et al.) (hereby, GPT), and is similar to that used by Sudhakar et al. (2019). The input to the model is the source sentence, as well as a marker indicating the target style, and the output is a sentence in the target style. We use the same input representation as Sudhakar et al. (2019). Following common practice (Luo et al., 2019;Wu et al., 2018) the training takes place in two phases: a) MLE pre-training, followed by b) RL training.\n\n\nMLE Pre-training\n\nAs shown by previous work (Luo et al., 2019;Wu et al., 2018;Gong et al., 2019) supervised or pseudo-supervised training helps with faster and sample efficient convergence. We pre-train DRL on a synthetically generated parallel corpus. We create this parallel corpus by using previous style transfer models, hereby 'baseline models' (described in section 5.2) on the original non-parallel corpus. This pre-training is performed by minimizing the standard MLE objective.\n\n\nRL Training\n\nDRL optimizes a policy using the policy gradient, to maximize a long term reward J(\u03b8). The parameters of the model \u03b8 define a policy \u03c0 \u03b8 (y t |s t ) which maps a state s t to an action y t . The state s t in our case in the input sentence x concatenated with the tokens generated by the policy till time t \u2212 1, i.e., s t = (x, y 1..t\u22121 ). The action y t is the token sampled from the vocabulary V . Sampling: For an input sentence x, we generate an output sentence by sampling from the vocabulary at each timestep t. Each output token (action) is sampled from the model's softmax distribution over the vocabulary (action space). We use a 'top-p sampling' (alternatively, 'nucleus sampling') (Holtzman et al., 2019) for the same. This sampling method samples a token from the set of top tokens that make up a cumulative softmax probability of p. For each input sentence x in the RLtraining set, we generate K different outputs by repeating the above process K times to get better estimation for the return. Policy Gradient:\n\nWe use the REINFORCE (Williams, 1992) policy gradient algorithm. RE-INFORCE relies on an estimated return by Monte-Carlo methods, using episode samples to update the policy parameter \u03b8.\n\u2207 \u03b8 J(\u03b8) = E \u03c0 [\u2207 \u03b8 log e \u03c0 \u03b8 (y t |s t ; \u03b8) * (Q \u03c0 (s t , y t )\u2212b)]\n(2) Here Q \u03c0 (s t , y t ) is the action-value function and is equal to the expected return of the state-action pair (s t , y t ). In the case of REINFORCE, it is estimated from the samples collected with the current policy \u03c0 \u03b8 . b is the baseline, which helps to reduce the variance in return estimation. Williams (1992) discuss different methods to calculate baseline. We use a constant baseline, which is the average of Q \u03c0 (s t , y t ) over all generated tokens during the batch gradient update.\nQ \u03c0 (s t , y t ) = E \u03c0 [r t + T i=t+1 \u03b3 i\u2212t * r i ] (3)\nHere, T is the episode length, \u03b3 \u2208 [0, 1] is the discount factor and r t is the reward associated with (s t , y t ).\n\n\nRewards\n\nThe model is provided with token-level dense rewards that indicate: 1) how well the generated text reflects target style, 2) how much of the content it preserves from the input, and 3) fluency. The final reward for each token is the weighted sum of these individual rewards. Separate rewards have also been used by Luo et al. (2019) and Gong et al. (2019).\n\n\nTarget Style Reward\n\nWe train a BERT style-classifier with weights \u03b8 CLS and use its self-attention scores to provide a reward for the target style. The classifier has multiple attention heads. Sudhakar et al. (2019) isolate a single attention-head such that the self-attention scores \u03b1 of this head correspond to how much a token contributes to the style of the output. To do this, they use a 'leave-one-out' approach on the dev set, computing \u03b1 for each head, and then removing the top \u03bb * |x| tokens based on \u03b1, from the input sentence x. Here \u03bb \u2208 [0, 1] is a parameter tuned for each dataset and |x| represents number of tokens in the sentence x. The head for which the classifier's score deviates the most is the one which Sudhakar et al. (2019) used to disentangle style and content tokens.\n\nUsing the same procedure, we arrive at a candidate head, and use its self-attention scores \u03b1 to assign a token level reward rs t . Formally, if the attention score associated with token y t is \u03b1 yt then the style reward for each token y t of the generated sentence y is,\nrs t = mask * (P (s tgt |y; \u03b8 CLS ) \u2212 0.5) (4) where, mask = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1, if y t in top \u03bb|y| tokens based on \u03b1 yt 0, else(5)\nThe tokens which influence the style of the sentence will be rewarded with P (s tgt |y; \u03b8 CLS ) \u2212 0.5, and purely content related tokens will receive a reward of 0.\n\n\nContent Preservation Reward\n\nWe use an n-gram based content reward to encourage the model to preserve content from the input. Let us define G t as the set of n-grams up to order 3 (set heuristically) associated with token y t of the generated sentence y and c(x t ) is the context window for the t th token in the input sentence x.\nG t = {{y t }, {y t\u22121 , y t }, {y t , y t+1 }, {y t\u22122 , y t\u22121 , y t }, {y t , y t+1 , y t+2 }} (6) c(x t ) = {x t\u22122 , x t\u22121 , x t , x t+1 , x t+2 } (7) r cont (z) = +1, if z is present in c(x) \u22121, else(8)\nThe context preservation reward rc t for token y t is,\nrc t = (1 \u2212 mask) * g\u2208Gt r cont (g) |G t |(9)\nWhere mask is as described in (5). The term (1 \u2212 mask) emphasises the fact that content preservation reward should apply to content tokens only and style tokens should not receive negative rewards for not preserving the content.\n\n\nFluency Reward\n\nThe fluency reward should apply to both attributes and content words. It should ensure that they are coherently used in context. Li et al. (2018b) observe that content preservation metrics (such as BLEU) do not correlate with human scores for fluency. Hence a separate fluency reward is necessary. We fine-tuned a pre-trained GPT (Radford et al.) model on the training data with language modeling (LM) task. We then use it to determine the fluency of each token y t using (10). We note here that d'Autume et al. (2019), use a similar dense reward to ours. We design the fluency reward as the LM likelihood:\nrf t = P (y t |y 1:t\u22121 ; \u03b8 LM )(10)\n\nOverall Reward:\n\nThe overall reward (r t ) for each token is a weighted sum of the style, content and fluency rewards from equations 4, 9 and 10:\nr t = \u03bb S * rs t + \u03bb C * rc t + \u03bb F * rf t(11)\n\u03bb S , \u03bb C and \u03bb F are reward weights. We heuristically set \u03bb S = 2.0, \u03bb C = 1.0 and \u03bb F = 0.5. Figure 1 shows a training example with input representation, rewards and decoding. The details of the RL training procedure are mentioned in Algorithm 1.\n\nAlgorithm 1: RL training Procedure Initialize: pre-trained decoder model f (\u03b8) with weights \u03b8; reward-change threshold ; initial baseline b ; train = T rue while train is True do Sample N sentences from training set D; for each sentence do Generate K output sentences; for each token y t of output sentence do calculate style reward rs t 4; calculate content reward rc t 9; calculate fluency reward rf t 10; calculate total reward rt 11; calculate return Q \u03c0 (s t , y t ) 3;\nend end previous_baseline b = b; baseline b = E[Q \u03c0 (s, y)]; calculate \u2207 \u03b8 J(\u03b8) from 2; Update \u03b8 based on \u2207 \u03b8 J(\u03b8); if |b \u2212 b |< then train = F alse; end\n\nReturn Estimation\n\nIt must be mentioned here that in previous work (Luo et al., 2019;Gong et al., 2019), the value of the discount factor \u03b3 is set to a non-zero value in eq. 3. This is to allow for estimation of token-level return from sparse rewards. However, the proposed method of designing dense rewards, solves the credit assignment problem directly. Consequently, there is no dependency on future rewards to estimate Q \u03c0 (s t , y t ). Hence, we set the value of \u03b3 to 0.\n\n\nExperiments\n\n\nDatasets\n\nFollowing are brief descriptions of the datasets we use, borrowed from works that use them previously. YELP: Each sentence is from a business review on Yelp, and is labeled as having either positive or negative sentiment (Li et al., 2018b). The task is to transfer positive sentences to negative sentences and vice-versa. Li et al. (2018b) publish a set of human reference outputs on the test set of YELP. GYAFC: Each sentence is labeled as either being formal or informal. We only use a subset of this dataset which corresponds to Yahoo answers in the Family and Relationships domain. The task is to transfer formal to informal sentences and viceversa. Rao and Tetreault (2018) publish a set of human reference outputs on the test set of GYAFC. Though GYAFC has source and target sentences aligned, we discard these alignments, since our problem setting is unsupervised. Table 1 shows train, dev and test statistics of the datasets.\n\n\nBaseline Models\n\nA baseline model refers to a pre-trained version of DRL, as described in section 4.2. We pre-train  three different baseline models -DO B , BGST-S B and BGST B . Baseline models of varying quality are deliberately chosen, so that we can observe the contribution of RL training over each of them independently. The parallel corpus for a) DO B is obtained using the DeleteOnly model (DO) (Li et al., 2018b), b) for BGST B , using the Blind Generative Style Transformer (BGST) (Sudhakar et al., 2019), and c) for BGST-S B , using BGST-Small, which is BGST trained over a subset of the training data (5%).\n\n\nReproducibility\n\nAll models described below are used from Huggingface 2 (Wolf et al., 2020). Style Classifier: We train the BERT-base uncased model with the default hyper-parameter setting. Generator: We use GPT (Radford et al.) as the generator model. The Generator training is done in two phases, 1) LM pre-training and 2) RL training. Pre-training: We train over GPT's pre-trained model for our pseudo-supervised pre-training RL-training: We set learning rate to lr = 6.25e \u2212 6, and gradient clipping to 1 to avoid overflow. Other hyper-parameters are set to the default values provided.\n\n\nEvaluation Methods\n\n\nHuman Evaluation\n\nWe obtain human evaluations of model outputs from crowd workers on MTurk. For each source sentence and target style, we present the outputs of all models being compared, to the same workers. They are all native English speakers from North America and were made familiar with the datasets. They were asked to rate each output on the following three criteria, each on a Likert scale from 1 to 5: a) target style match (Sty.), b) content preservation (Con.), c) fluency (Flu.). We also calculate the overall success rate (All) as the percentage of times a model received either a 4 or a 5 on all the above three criteria.\n\n\nAutomatic Evaluation\n\nTo measure target style strength (Sty.) of outputs, we use a pre-trained FastText 3 (Joulin et al., 2017) style classifiers for each dataset, which achieve accuracies of 96.5% and 89.5% on the test sets of YELP and GYAFC respectively. For content preservation (Con.), we calculate the average BLEU scores of the output with respect to the human reference sentences. Fluency (Flu.) is estimated by finetuning pre-trained OpenAI GPT-2 (Radford et al., 2019) models (different from any of the GPT models used in this work) on each dataset's training set, and using them to obtain the perplexity of the output sentences. They achieve perplexities of 21.42 and 52.5 on the test sets of YELP and GYAFC respectively. We also calculate the geometric mean, GM (All), of style and content scores. It must be noted, however, that most previous works such as Li et al. (2018b) and Sudhakar et al. (2019) note that human evaluations are more reliable than automatic evaluations. Hence, in all further analysis in this work, we draw conclusions from the human evaluations.\n\n\nResults\n\n\nImprovement over Baseline Models\n\nAs mentioned in section 5.2, we further train each baseline model using dense RL rewards. We observe that our novel RL training provides significant improvement over each of these baseline models. To further distill the effects of RL separately, we consider an additional simple baseline model (SOURCE B ), trained on a parallel corpus where the target sentence is the same as the source sentence. Human evaluations of these models are presented in Table 2. These evaluations show that DRL significantly improves over a variety of baseline (BL) models on style, content as well as fluency, irrespective of their original style transfer capabilities.\n\n\nComparison with Roll-out\n\nWe compare dense rewards with the roll-out strategy. To do so, we train a RL style transfer model which uses the roll-out strategy for reward shaping, as used by Gong et al. (2019). We keep all other factors such as pre-training and model architecture same. We compute the rolled out reward r t using the same process as Gong et al. (2019). We set mask = 1 in eq. 4 to calculate style reward rs t and mask = 0 in eq. 9 to calculate content reward rc t . We set \u03b3 = 1 while calculating estimated return Q \u03c0 (s t , y t ) using equation 3. This allows us to train a Roll-Out model (RO) which a) has the same architecture as DRL described in section 4 and b) is pre-trained with a synthetic corpus generated by the BGST-S B baseline.\n\n\nAnalysis:\n\nWe compare a) RO, b) its baseline model BGST-S B and c) DRL trained over the same baseline model. These results are presented in Table 2. The results indicate the superior performance of dense rewards in comparison to the roll-out strategy.\n\n\nSample Efficiency\n\nIn Figure 2, we compare the sample efficiencies of our DRL and RO. The overall reward (11) is normalized between 0 to 1 for easy visualization as the training steps progress. The peak reward of Figure 2: Training steps DRL is higher than RO in both cases. On YELP, DRL reaches its peak reward after 25K episodes whereas RO takes 74K episodes. On GYAFC, DRL takes 40K episodes, whereas RO takes 83K. Since all other factors are kept same in both setups, these gains can be attributed to accurate dense rewards for each token, resulting in better estimation of Q \u03c0 (s t , y t ), sample efficiency and convergence of the policy.\n\n\nTraining Time\n\nThe roll-out itself is an expensive operation, requiring O(T 2 ) computations for one complete output, where T is the output length. Time complexity for self-attention based reward shaping is O(T ). In practice, training DRL is 4x faster than RO on YELP, and 10x faster on GYAFC, owing to the differences in average sentence length.\n\n\nComparison with Fully Attention-based Reward\n\nAs an additional experiment, we explore directly using attention scores in the style and content rewards, without explicitly separating attributes and content. We modify the reward coefficient mask in equation 5 (which will reflect in the style and content rewards) to the attention weight as: mask = \u03b1 t and train the attention-reward model (DRL-At) over the BGST-S B baseline model, keeping all else the same as DRL.\n\nAnalysis: We compare a) DRL-At, b) its baseline model BGST-S B and c) DRL trained over the same baseline model. These results are presented in Table 2. While DRL-At does give encouraging results, we see that it still performs narrowly worse than DRL. We surmise that this might be because a) every word is assigned both a content and style reward, even though there exists a hard separation between content and attributes in most cases, and b) raw attention scores might not be the most appropriate reward coefficients.\n\n\nComparison with Previous Works\n\nWe compare our results with previous works that use different approaches for style transfer. We choose the state-of-art work from each approach. Multi Decoder (MD) (Fu et al., 2018) uses adversarial training to separate content and attribute. Back-Translation (BT) (Prabhumoye et al., 2018) uses back-translation that has reduced style, and then generate style-specific outputs. We also compare with RL based approaches described in previous sections. UnPaired (UnP) (Xu et al., 2018) uses a cyclic RL approach, RL-roll-out (RL-RO) (Gong et al., 2019) uses the roll-out strategy and DualRL (DuR) (Luo et al., 2019) uses a naive reward shaping. Finally, DO (Li et al., 2018b) and BGST (Sudhakar et al., 2019) both separate content and attributes, and train a generative model to reconstruct the source sentence, given only the content. Evaluations of these works and ours are presented in Table 3. In this table, DRL refers to DRL trained over the BGST B baseline model, since it is our best performing from Table 2.\n\n\nAnalysis:\n\nWe see that DRL improves over previous state-of-art models by a good margin on all metrics across all datasets, according to both human and automatic evaluations. On the overall scores (All), we improve over previous state-of-the art model (DuR) by 21% each on YELP and GYAFC, according to human evaluation in Table  3. From manual inspection we observe that DRL performs better than previous state-of-art models in the following ways: 1) maintains consistent context across longer sentences, 2) maintains consistency of style even for sentences having multiple attribute words, 3) produces output sentences having appropriate and meaningful attributes, many of which the model has not observed at training time, and 4) does away with redundancy and repetition of words in output sentences, which is commonly observed in previous works.\n\nSentiment versus formality: The last row in Table 3 shows the performance of humans (H) for each of the datasets. Humans perform better on YELP as compared to GYAFC, according to human evaluations. Further, through manual annotation from crowd workers on Mturk 4 , we observe that the average percentage of word changes to generate target style sentence is 23% for YELP, and 39% for GYAFC. The average sentence length is 8 words in YELP and 13 words in GYAFC. These statistics, along with manual investigations suggest that formality transfer on GYAFC is a more complex and ambiguous task than sentiment transfer on YELP. The efficacy of using dense rewards is further shown by the fact that DRL is only 6% worse than humans on GYAFC. Table 4 shows example outputs generated by our models from the test set.\n\n\nConclusion\n\nWe present a novel method to provide dense rewards for unsupervised text generation and compare it with the currently used roll-out strategy using ablation studies. We demonstrate the use of these dense rewards in building a state-of-art text style transfer system.   Table 3: Human and automatic evaluation results of comparison with previous works: Sty. = Target Style Match; Con. = Content Preservation; Flu. = Fluency; All (human evaluation) = Overall success rate ; All (automatic evaluation) = GM of Sty. and Con. of automatic evaluation. Fluency for automatic metrics is measured by perplexity, so the lower the better. DRL refers to DRL trained over the BGST B baseline model.\n\nYELP (Positive to Negative) steve was professional and found exactly the right unit to fit in our space steve was rude and didn't have the right unit to fit in our space . YELP (Negative to Positive) they tried to take advantage of me because i am young .\n\nthey take great care of me because i am young . GYAFC (Formal to Informal) do not approach her and let her know that you find her looks very attractive. don't approach her and let her know that you like her . GYAFC (Informal to Formal) well that is just the way it is I guess. that is just the way it is , i would advise . Table 4: Examples of generated sentences by DRL. Each cell has the source sentence first and the generated sentence second.\n\nFigure 1 :\n1An example from YELP for the task of sentiment transfer. <POS> represents positive target style, <SRC_START> indicates the start of the input sentence, <START> indicates the start of the output sentence and <END> is end-of-sentence marker.\n\nTable 1 :\n1Dataset statistics\n\nTable 2 :\n2Human evaluation results of improvements over baselines, roll-outs, and fully attention-based rewards: Sty. = Target Style Match; Con. = Content Preservation; Flu. = Fluency; All = Overall success rate. DRL in each cell denotes DRL trained over the corresponding baseline.HUMAN EVALUATION \nAUTOMATIC EVALUATION \nYELP \nGYAFC \nYELP \nGYAFC \nModel \nSty. \nCon. \nFlu. \nAll \nSty. \nCon. \nFlu. \nAll \nSty. \nCon. \nFlu.\u2193 \nAll \nSty. \nCon. \nFlu.\u2193 \nAll \n\nMD \n2.07 \n3.11 \n3.19 \n6% \n2.17 \n1.89 \n2.29 \n5% \n50.6 \n42.9 \n205.5 \n46.6 \n26.1 \n26.8 \n231.5 \n26.4 \nBT \n4.13 \n2.41 \n4.09 \n7% \n3.04 \n1.91 \n3.21 \n9% \n95.5 \n25.6 \n28.5 \n49.4 \n51.5 \n18.7 \n147.2 \n31.0 \nDO \n3.36 \n3.51 \n3.68 \n25% \n2.31 \n2.47 \n2.84 \n7% \n85.9 \n44.4 \n75.8 \n61.7 \n26.0 \n42.4 \n183.1 \n33.2 \nBGST \n3.78 \n3.89 \n3.82 \n36% \n3.01 \n3.16 \n3.11 \n14% \n86.7 \n57.1 \n38.6 \n70.4 \n69.0 \n47.4 \n89.0 \n57.2 \nUnP \n2.86 \n3.52 \n3.28 \n14% \n2.74 \n1.45 \n2.63 \n4% \n53.3 \n46.3 \n294.2 \n49.7 \n68.5 \n12.3 \n133.9 \n29.0 \nRL-RO \n2.93 \n3.41 \n3.72 \n15% \n2.83 \n1.75 \n2.61 \n6% \n55.7 \n47.2 \n177 \n50.2 \n69.2 \n14.8 \n141.9 \n31.2 \nDuR \n3.92 \n3.89 \n4.01 \n42% \n3.28 \n3.62 \n3.87 \n27% \n89.2 \n59.5 \n53.2 \n72.9 \n60.1 \n44.3 \n321.1 \n51.6 \nDRL \n4.45 \n4.41 \n4.24 \n63% \n4.32 \n3.58 \n4.10 \n48% \n96.8 \n59.6 \n40.5 \n75.7 \n98.9 \n47.7 \n27.9 \n68.5 \n\nH \n4.79 \n4.67 \n4.61 \n81% \n4.65 \n4.38 \n4.42 \n54% \n75.0 \n70.3 \n57.7 \n72.5 \n86.4 \n65.4 \n91.9 \n75.2 \n\n\nhttps://github.com/huggingface/transformers\nhttps://fasttext.cc/\nhttps://www.mturk.com/\n\nAn actorcritic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C Courville, Yoshua Bengio, abs/1607.07086ArXiv. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. 2017. An actor- critic algorithm for sequence prediction. ArXiv, abs/1607.07086.\n\nMihaela Cyprien De Masson D&apos;autume, Jack Rosca, Shakir Rae, Mohamed, arXiv:1905.09922Training language gans from scratch. arXiv preprintCyprien de Masson d'Autume, Mihaela Rosca, Jack Rae, and Shakir Mohamed. 2019. Training language gans from scratch. arXiv preprint arXiv:1905.09922.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.\n\nStyle transfer in text: Exploration and evaluation. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Thirty-Second AAAI Conference on Artificial Intelligence. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018. Style transfer in text: Explo- ration and evaluation. In Thirty-Second AAAI Con- ference on Artificial Intelligence.\n\nReinforcement learning based text style transfer without parallel training corpus. Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong, Wen Mei Hwu, Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong, and Wen mei Hwu. 2019. Reinforcement learning based text style transfer without parallel training cor- pus.\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi, Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degener- ation.\n\nToward controlled generation of text. Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P Xing, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, Australia70International Convention CentreZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017. Toward con- trolled generation of text. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Re- search, pages 1587-1596, International Convention Centre, Sydney, Australia. PMLR.\n\nDisentangled representation learning for non-parallel text style transfer. Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova, 10.18653/v1/P19-1041Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsVineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova. 2019. Disentangled representation learning for non-parallel text style transfer. In Pro- ceedings of the 57th Annual Meeting of the Associa- tion for Computational Linguistics, pages 424-434, Florence, Italy. Association for Computational Lin- guistics.\n\nBag of tricks for efficient text classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European ChapterShort PapersAssociation for Computational Linguistics2Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Con- ference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa- pers, pages 427-431. Association for Computational Linguistics.\n\nBandit based monte-carlo planning. Levente Kocsis, Csaba Szepesv\u00e1ri, Machine Learning: ECML 2006. Berlin, Heidelberg; Berlin HeidelbergSpringerLevente Kocsis and Csaba Szepesv\u00e1ri. 2006. Bandit based monte-carlo planning. In Machine Learn- ing: ECML 2006, pages 282-293, Berlin, Heidel- berg. Springer Berlin Heidelberg.\n\nPhrase-based & neural unsupervised machine translation. Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsGuillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc'Aurelio Ranzato. 2018. Phrase-based & neural unsupervised machine trans- lation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5039-5049, Brussels, Belgium. Association for Computational Linguistics.\n\nDelete, retrieve, generate: a simple approach to sentiment and style transfer. Juncen Li, Robin Jia, He He, Percy Liang, 10.18653/v1/N18-1169Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1Association for Computational LinguisticsJuncen Li, Robin Jia, He He, and Percy Liang. 2018a. Delete, retrieve, generate: a simple approach to sen- timent and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865-1874, New Orleans, Louisiana. Associ- ation for Computational Linguistics.\n\nHe He, and Percy Liang. 2018b. Delete, retrieve, generate: A simple approach to sentiment and style transfer. Juncen Li, Robin Jia, abs/1804.06437ArXiv. Juncen Li, Robin Jia, He He, and Percy Liang. 2018b. Delete, retrieve, generate: A simple approach to sen- timent and style transfer. ArXiv, abs/1804.06437.\n\nParaphrase generation with deep reinforcement learning. Zichao Li, Xin Jiang, Lifeng Shang, Hang Li, 10.18653/v1/D18-1421Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsZichao Li, Xin Jiang, Lifeng Shang, and Hang Li. 2018c. Paraphrase generation with deep reinforce- ment learning. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing, pages 3865-3878, Brussels, Belgium. Association for Computational Linguistics.\n\nDayiheng Liu, Jie Fu, Yidan Zhang, arXiv:1905.12304Chris Pal, and Jiancheng Lv. 2019. Revision in continuous space: Fine-grained control of text style transfer. arXiv preprintDayiheng Liu, Jie Fu, Yidan Zhang, Chris Pal, and Jiancheng Lv. 2019. Revision in continuous space: Fine-grained control of text style transfer. arXiv preprint arXiv:1905.12304.\n\nA dual reinforcement learning framework for unsupervised text style transfer. Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, Zhifang Sui, 10.24963/ijcai.2019/711Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, and Zhifang Sui. 2019. A dual rein- forcement learning framework for unsupervised text style transfer. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intel- ligence, IJCAI-19, pages 5116-5122. International Joint Conferences on Artificial Intelligence Organi- zation.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\nA deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, R Socher, abs/1705.04304ArXiv. Romain Paulus, Caiming Xiong, and R. Socher. 2018. A deep reinforced model for abstractive summariza- tion. ArXiv, abs/1705.04304.\n\nStyle transfer through back-translation. Yulia Shrimai Prabhumoye, Ruslan Tsvetkov, Alan W Salakhutdinov, Black, Proc. ACL. ACLShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut- dinov, and Alan W Black. 2018. Style transfer through back-translation. In Proc. ACL.\n\nTim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nDear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. Sudha Rao, Joel Tetreault, 10.18653/v1/N18-1012Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1Association for Computational LinguisticsSudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Cor- pus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long Papers), pages 129-140, New Orleans, Louisiana. Association for Computa- tional Linguistics.\n\nBert as a teacher: Contextual embeddings for sequence-level reward. Florian Schmidt, T Hofmann, abs/2003.02738ArXiv. Florian Schmidt and T. Hofmann. 2020. Bert as a teacher: Contextual embeddings for sequence-level reward. ArXiv, abs/2003.02738.\n\nToward diverse text generation with inverse reinforcement learning. Zhan Shi, Xinchi Chen, Xipeng Qiu, X Huang, IJCAI. Zhan Shi, Xinchi Chen, Xipeng Qiu, and X. Huang. 2018. Toward diverse text generation with inverse reinforcement learning. In IJCAI.\n\nTransforming delete, retrieve, generate approach for controlled text style transfer. A Sudhakar, Bhargav Upadhyay, A Maheswaran, abs/1908.09368ArXiv. A. Sudhakar, Bhargav Upadhyay, and A. Maheswaran. 2019. Transforming delete, retrieve, generate ap- proach for controlled text style transfer. ArXiv, abs/1908.09368.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning, 8(3-4):229-256.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language pro- cessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso- ciation for Computational Linguistics.\n\nA hierarchical reinforced sequence operation method for unsupervised text style transfer. Chen Wu, Xuancheng Ren, Fuli Luo, Xu Sun, 10.18653/v1/P19-1482Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsChen Wu, Xuancheng Ren, Fuli Luo, and Xu Sun. 2019a. A hierarchical reinforced sequence opera- tion method for unsupervised text style transfer. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4873- 4883, Florence, Italy. Association for Computational Linguistics.\n\nA study of reinforcement learning for neural machine translation. Lijun Wu, Fei Tian, J Tao Qin, T Lai, Liu, EMNLP. Lijun Wu, Fei Tian, Tao Qin, J. Lai, and T. Liu. 2018. A study of reinforcement learning for neural machine translation. In EMNLP.\n\nmask and infill\": Applying masked language model to sentiment transfer. Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, Songlin Hu, arXiv:1908.08039arXiv preprintXing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, and Songlin Hu. 2019b. \" mask and infill\": Applying masked language model to sentiment transfer. arXiv preprint arXiv:1908.08039.\n\nUnpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach. Jingjing Xu, Sun Xu, Qi Zeng, Xiaodong Zhang, Xuancheng Ren, Houfeng Wang, Wenjie Li, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia1Long Papers). Association for Computational LinguisticsJingjing Xu, Xu SUN, Qi Zeng, Xiaodong Zhang, Xu- ancheng Ren, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cy- cled reinforcement learning approach. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 979-988, Melbourne, Australia. Asso- ciation for Computational Linguistics.\n\nUnsupervised text style transfer using language models as discriminators. Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, Taylor Berg-Kirkpatrick, Advances in Neural Information Processing Systems. Zichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and Taylor Berg-Kirkpatrick. 2018. Unsupervised text style transfer using language models as discrimina- tors. In Advances in Neural Information Processing Systems, pages 7287-7298.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Weinan Zhang, Jun Wang, Yong Yu, Thirty-First AAAI Conference on Artificial Intelligence. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence generative adversarial nets with policy gradient. In Thirty-First AAAI Confer- ence on Artificial Intelligence.\n\nSentence simplification with deep reinforcement learning. Xingxing Zhang, Mirella Lapata, 10.18653/v1/D17-1062Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsXingxing Zhang and Mirella Lapata. 2017. Sentence simplification with deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584-594, Copenhagen, Denmark. Association for Computational Linguistics.\n\nLanguage style transfer from sentences with arbitrary unknown styles. Yanpeng Zhao, Wei Bi, Deng Cai, Xiaojiang Liu, Kewei Tu, Shuming Shi, Yanpeng Zhao, Wei Bi, Deng Cai, Xiaojiang Liu, Kewei Tu, and Shuming Shi. 2018. Language style transfer from sentences with arbitrary unknown styles.\n\nMaximum entropy inverse reinforcement learning. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI. the Twenty-Third AAAI Conference on Artificial Intelligence, AAAIChicago, Illinois, USAAAAI PressBrian D. Ziebart, Andrew L. Maas, J. Andrew Bag- nell, and Anind K. Dey. 2008. Maximum entropy inverse reinforcement learning. In Proceedings of the Twenty-Third AAAI Conference on Artificial In- telligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 1433-1438. AAAI Press.\n", "annotations": {"author": "[{\"end\":119,\"start\":80},{\"end\":161,\"start\":120},{\"end\":179,\"start\":162}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":88},{\"end\":137,\"start\":129},{\"end\":178,\"start\":168}]", "author_first_name": "[{\"end\":87,\"start\":80},{\"end\":128,\"start\":120},{\"end\":167,\"start\":162}]", "author_affiliation": null, "title": "[{\"end\":77,\"start\":1},{\"end\":256,\"start\":180}]", "venue": null, "abstract": "[{\"end\":1421,\"start\":258}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1558,\"start\":1535},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1607,\"start\":1586},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1710,\"start\":1692},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1760,\"start\":1736},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1816,\"start\":1798},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1834,\"start\":1816},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2754,\"start\":2736},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3256,\"start\":3227},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3340,\"start\":3322},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3390,\"start\":3373},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3530,\"start\":3514},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4228,\"start\":4206},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4304,\"start\":4282},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4587,\"start\":4565},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4714,\"start\":4693},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5061,\"start\":5038},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5180,\"start\":5157},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6072,\"start\":6056},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6266,\"start\":6248},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6444,\"start\":6427},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6898,\"start\":6881},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6914,\"start\":6898},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6932,\"start\":6914},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7030,\"start\":7012},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7051,\"start\":7034},{\"end\":7070,\"start\":7053},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7242,\"start\":7224},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7264,\"start\":7242},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7281,\"start\":7264},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7323,\"start\":7302},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7340,\"start\":7323},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7695,\"start\":7679},{\"end\":8746,\"start\":8730},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8815,\"start\":8793},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9025,\"start\":9003},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9071,\"start\":9053},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9087,\"start\":9071},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9241,\"start\":9223},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9257,\"start\":9241},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9275,\"start\":9257},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10395,\"start\":10372},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11975,\"start\":11958},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11998,\"start\":11980},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12218,\"start\":12196},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12752,\"start\":12730},{\"end\":14394,\"start\":14377},{\"end\":14594,\"start\":14578},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16050,\"start\":16032},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16068,\"start\":16050},{\"end\":16706,\"start\":16688},{\"end\":16806,\"start\":16789},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17145,\"start\":17121},{\"end\":17824,\"start\":17806},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17917,\"start\":17894},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18115,\"start\":18096},{\"end\":18252,\"start\":18236},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19404,\"start\":19383},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19753,\"start\":19732},{\"end\":20163,\"start\":20146},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20190,\"start\":20168},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21262,\"start\":21244},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21421,\"start\":21403},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24266,\"start\":24249},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24569,\"start\":24552},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24636,\"start\":24617},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24699,\"start\":24681},{\"end\":24759,\"start\":24741},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24792,\"start\":24769}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28416,\"start\":28164},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28447,\"start\":28417},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29791,\"start\":28448}]", "paragraph": "[{\"end\":1835,\"start\":1437},{\"end\":2245,\"start\":1837},{\"end\":3052,\"start\":2247},{\"end\":3860,\"start\":3054},{\"end\":4146,\"start\":3862},{\"end\":4825,\"start\":4148},{\"end\":5324,\"start\":4827},{\"end\":5895,\"start\":5326},{\"end\":6652,\"start\":5912},{\"end\":7388,\"start\":6654},{\"end\":7722,\"start\":7390},{\"end\":8218,\"start\":7744},{\"end\":8473,\"start\":8235},{\"end\":8595,\"start\":8496},{\"end\":9176,\"start\":8605},{\"end\":9665,\"start\":9197},{\"end\":10703,\"start\":9681},{\"end\":10890,\"start\":10705},{\"end\":11458,\"start\":10960},{\"end\":11631,\"start\":11515},{\"end\":11999,\"start\":11643},{\"end\":12798,\"start\":12023},{\"end\":13070,\"start\":12800},{\"end\":13360,\"start\":13196},{\"end\":13694,\"start\":13392},{\"end\":13954,\"start\":13900},{\"end\":14229,\"start\":14001},{\"end\":14854,\"start\":14248},{\"end\":15037,\"start\":14909},{\"end\":15333,\"start\":15085},{\"end\":15809,\"start\":15335},{\"end\":16440,\"start\":15984},{\"end\":17400,\"start\":16467},{\"end\":18021,\"start\":17420},{\"end\":18614,\"start\":18041},{\"end\":19274,\"start\":18656},{\"end\":20357,\"start\":19299},{\"end\":21053,\"start\":20404},{\"end\":21811,\"start\":21082},{\"end\":22065,\"start\":21825},{\"end\":22712,\"start\":22087},{\"end\":23062,\"start\":22730},{\"end\":23529,\"start\":23111},{\"end\":24050,\"start\":23531},{\"end\":25100,\"start\":24085},{\"end\":25950,\"start\":25114},{\"end\":26759,\"start\":25952},{\"end\":27458,\"start\":26774},{\"end\":27715,\"start\":27460},{\"end\":28163,\"start\":27717}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8495,\"start\":8474},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10959,\"start\":10891},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11514,\"start\":11459},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13195,\"start\":13071},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13899,\"start\":13695},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14000,\"start\":13955},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14890,\"start\":14855},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15084,\"start\":15038},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15963,\"start\":15810}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17346,\"start\":17339},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20860,\"start\":20853},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21961,\"start\":21954},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23681,\"start\":23674},{\"end\":24980,\"start\":24973},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25099,\"start\":25092},{\"end\":25432,\"start\":25424},{\"end\":26003,\"start\":25996},{\"end\":26694,\"start\":26687},{\"end\":27049,\"start\":27042},{\"end\":28047,\"start\":28040}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1435,\"start\":1423},{\"attributes\":{\"n\":\"2\"},\"end\":5910,\"start\":5898},{\"attributes\":{\"n\":\"3\"},\"end\":7742,\"start\":7725},{\"attributes\":{\"n\":\"4\"},\"end\":8233,\"start\":8221},{\"attributes\":{\"n\":\"4.1\"},\"end\":8603,\"start\":8598},{\"attributes\":{\"n\":\"4.2\"},\"end\":9195,\"start\":9179},{\"attributes\":{\"n\":\"4.3\"},\"end\":9679,\"start\":9668},{\"attributes\":{\"n\":\"4.4\"},\"end\":11641,\"start\":11634},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":12021,\"start\":12002},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":13390,\"start\":13363},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":14246,\"start\":14232},{\"attributes\":{\"n\":\"4.4.4\"},\"end\":14907,\"start\":14892},{\"attributes\":{\"n\":\"4.5\"},\"end\":15982,\"start\":15965},{\"attributes\":{\"n\":\"5\"},\"end\":16454,\"start\":16443},{\"attributes\":{\"n\":\"5.1\"},\"end\":16465,\"start\":16457},{\"attributes\":{\"n\":\"5.2\"},\"end\":17418,\"start\":17403},{\"attributes\":{\"n\":\"5.3\"},\"end\":18039,\"start\":18024},{\"attributes\":{\"n\":\"5.4\"},\"end\":18635,\"start\":18617},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":18654,\"start\":18638},{\"attributes\":{\"n\":\"5.4.2\"},\"end\":19297,\"start\":19277},{\"attributes\":{\"n\":\"6\"},\"end\":20367,\"start\":20360},{\"attributes\":{\"n\":\"6.1\"},\"end\":20402,\"start\":20370},{\"attributes\":{\"n\":\"6.2\"},\"end\":21080,\"start\":21056},{\"end\":21823,\"start\":21814},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":22085,\"start\":22068},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":22728,\"start\":22715},{\"attributes\":{\"n\":\"6.3\"},\"end\":23109,\"start\":23065},{\"attributes\":{\"n\":\"6.4\"},\"end\":24083,\"start\":24053},{\"end\":25112,\"start\":25103},{\"attributes\":{\"n\":\"7\"},\"end\":26772,\"start\":26762},{\"end\":28175,\"start\":28165},{\"end\":28427,\"start\":28418},{\"end\":28458,\"start\":28449}]", "table": "[{\"end\":29791,\"start\":28732}]", "figure_caption": "[{\"end\":28416,\"start\":28177},{\"end\":28447,\"start\":28429},{\"end\":28732,\"start\":28460}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15188,\"start\":15180},{\"end\":22098,\"start\":22090},{\"end\":22289,\"start\":22281}]", "bib_author_first_name": "[{\"end\":29938,\"start\":29931},{\"end\":29957,\"start\":29949},{\"end\":29972,\"start\":29966},{\"end\":29984,\"start\":29977},{\"end\":29996,\"start\":29992},{\"end\":30009,\"start\":30003},{\"end\":30023,\"start\":30018},{\"end\":30025,\"start\":30024},{\"end\":30043,\"start\":30037},{\"end\":30288,\"start\":30281},{\"end\":30326,\"start\":30322},{\"end\":30340,\"start\":30334},{\"end\":30577,\"start\":30572},{\"end\":30594,\"start\":30586},{\"end\":30608,\"start\":30602},{\"end\":30622,\"start\":30614},{\"end\":30995,\"start\":30988},{\"end\":31006,\"start\":31000},{\"end\":31018,\"start\":31012},{\"end\":31032,\"start\":31025},{\"end\":31042,\"start\":31039},{\"end\":31383,\"start\":31377},{\"end\":31394,\"start\":31390},{\"end\":31408,\"start\":31401},{\"end\":31419,\"start\":31413},{\"end\":31430,\"start\":31427},{\"end\":31648,\"start\":31645},{\"end\":31662,\"start\":31659},{\"end\":31676,\"start\":31669},{\"end\":31690,\"start\":31685},{\"end\":31853,\"start\":31846},{\"end\":31864,\"start\":31858},{\"end\":31878,\"start\":31871},{\"end\":31892,\"start\":31886},{\"end\":31912,\"start\":31908},{\"end\":31914,\"start\":31913},{\"end\":32511,\"start\":32505},{\"end\":32522,\"start\":32518},{\"end\":32535,\"start\":32528},{\"end\":32551,\"start\":32547},{\"end\":33174,\"start\":33168},{\"end\":33190,\"start\":33183},{\"end\":33203,\"start\":33198},{\"end\":33221,\"start\":33216},{\"end\":33751,\"start\":33744},{\"end\":33765,\"start\":33760},{\"end\":34095,\"start\":34086},{\"end\":34108,\"start\":34104},{\"end\":34120,\"start\":34114},{\"end\":34137,\"start\":34130},{\"end\":34164,\"start\":34147},{\"end\":34800,\"start\":34794},{\"end\":34810,\"start\":34805},{\"end\":34818,\"start\":34816},{\"end\":34828,\"start\":34823},{\"end\":35710,\"start\":35704},{\"end\":35720,\"start\":35715},{\"end\":35967,\"start\":35961},{\"end\":35975,\"start\":35972},{\"end\":35989,\"start\":35983},{\"end\":36001,\"start\":35997},{\"end\":36538,\"start\":36530},{\"end\":36547,\"start\":36544},{\"end\":36557,\"start\":36552},{\"end\":36966,\"start\":36962},{\"end\":36976,\"start\":36972},{\"end\":36984,\"start\":36981},{\"end\":37000,\"start\":36991},{\"end\":37013,\"start\":37007},{\"end\":37023,\"start\":37021},{\"end\":37036,\"start\":37029},{\"end\":37693,\"start\":37686},{\"end\":37709,\"start\":37704},{\"end\":37722,\"start\":37718},{\"end\":37737,\"start\":37729},{\"end\":38380,\"start\":38374},{\"end\":38396,\"start\":38389},{\"end\":38405,\"start\":38404},{\"end\":38613,\"start\":38608},{\"end\":38640,\"start\":38634},{\"end\":38655,\"start\":38651},{\"end\":38657,\"start\":38656},{\"end\":38933,\"start\":38929},{\"end\":38950,\"start\":38943},{\"end\":39150,\"start\":39146},{\"end\":39164,\"start\":39160},{\"end\":39174,\"start\":39169},{\"end\":39187,\"start\":39182},{\"end\":39199,\"start\":39194},{\"end\":39212,\"start\":39208},{\"end\":39486,\"start\":39481},{\"end\":39496,\"start\":39492},{\"end\":40360,\"start\":40353},{\"end\":40371,\"start\":40370},{\"end\":40604,\"start\":40600},{\"end\":40616,\"start\":40610},{\"end\":40629,\"start\":40623},{\"end\":40636,\"start\":40635},{\"end\":40871,\"start\":40870},{\"end\":40889,\"start\":40882},{\"end\":40901,\"start\":40900},{\"end\":41193,\"start\":41192},{\"end\":41454,\"start\":41448},{\"end\":41469,\"start\":41461},{\"end\":41483,\"start\":41477},{\"end\":41496,\"start\":41490},{\"end\":41514,\"start\":41507},{\"end\":41532,\"start\":41525},{\"end\":41545,\"start\":41538},{\"end\":41557,\"start\":41554},{\"end\":41569,\"start\":41565},{\"end\":41582,\"start\":41576},{\"end\":41597,\"start\":41594},{\"end\":41610,\"start\":41607},{\"end\":41626,\"start\":41621},{\"end\":41653,\"start\":41647},{\"end\":41664,\"start\":41658},{\"end\":41680,\"start\":41674},{\"end\":41691,\"start\":41686},{\"end\":41694,\"start\":41692},{\"end\":41706,\"start\":41699},{\"end\":41720,\"start\":41713},{\"end\":41736,\"start\":41729},{\"end\":41753,\"start\":41744},{\"end\":41755,\"start\":41754},{\"end\":42698,\"start\":42694},{\"end\":42712,\"start\":42703},{\"end\":42722,\"start\":42718},{\"end\":42730,\"start\":42728},{\"end\":43362,\"start\":43357},{\"end\":43370,\"start\":43367},{\"end\":43378,\"start\":43377},{\"end\":43389,\"start\":43388},{\"end\":43615,\"start\":43611},{\"end\":43623,\"start\":43620},{\"end\":43639,\"start\":43631},{\"end\":43653,\"start\":43646},{\"end\":43666,\"start\":43659},{\"end\":43975,\"start\":43967},{\"end\":43983,\"start\":43980},{\"end\":43990,\"start\":43988},{\"end\":44005,\"start\":43997},{\"end\":44022,\"start\":44013},{\"end\":44035,\"start\":44028},{\"end\":44048,\"start\":44042},{\"end\":44760,\"start\":44754},{\"end\":44774,\"start\":44767},{\"end\":44784,\"start\":44779},{\"end\":44795,\"start\":44791},{\"end\":44797,\"start\":44796},{\"end\":44810,\"start\":44804},{\"end\":45186,\"start\":45180},{\"end\":45197,\"start\":45191},{\"end\":45208,\"start\":45205},{\"end\":45219,\"start\":45215},{\"end\":45531,\"start\":45523},{\"end\":45546,\"start\":45539},{\"end\":46141,\"start\":46134},{\"end\":46151,\"start\":46148},{\"end\":46160,\"start\":46156},{\"end\":46175,\"start\":46166},{\"end\":46186,\"start\":46181},{\"end\":46198,\"start\":46191},{\"end\":46408,\"start\":46403},{\"end\":46410,\"start\":46409},{\"end\":46426,\"start\":46420},{\"end\":46428,\"start\":46427},{\"end\":46436,\"start\":46435},{\"end\":46443,\"start\":46437},{\"end\":46458,\"start\":46453},{\"end\":46460,\"start\":46459}]", "bib_author_last_name": "[{\"end\":29947,\"start\":29939},{\"end\":29964,\"start\":29958},{\"end\":29975,\"start\":29973},{\"end\":29990,\"start\":29985},{\"end\":30001,\"start\":29997},{\"end\":30016,\"start\":30010},{\"end\":30035,\"start\":30026},{\"end\":30050,\"start\":30044},{\"end\":30320,\"start\":30289},{\"end\":30332,\"start\":30327},{\"end\":30344,\"start\":30341},{\"end\":30353,\"start\":30346},{\"end\":30584,\"start\":30578},{\"end\":30600,\"start\":30595},{\"end\":30612,\"start\":30609},{\"end\":30632,\"start\":30623},{\"end\":30998,\"start\":30996},{\"end\":31010,\"start\":31007},{\"end\":31023,\"start\":31019},{\"end\":31037,\"start\":31033},{\"end\":31046,\"start\":31043},{\"end\":31388,\"start\":31384},{\"end\":31399,\"start\":31395},{\"end\":31411,\"start\":31409},{\"end\":31425,\"start\":31420},{\"end\":31438,\"start\":31431},{\"end\":31657,\"start\":31649},{\"end\":31667,\"start\":31663},{\"end\":31683,\"start\":31677},{\"end\":31695,\"start\":31691},{\"end\":31856,\"start\":31854},{\"end\":31869,\"start\":31865},{\"end\":31884,\"start\":31879},{\"end\":31906,\"start\":31893},{\"end\":31919,\"start\":31915},{\"end\":32516,\"start\":32512},{\"end\":32526,\"start\":32523},{\"end\":32545,\"start\":32536},{\"end\":32562,\"start\":32552},{\"end\":33181,\"start\":33175},{\"end\":33196,\"start\":33191},{\"end\":33214,\"start\":33204},{\"end\":33229,\"start\":33222},{\"end\":33758,\"start\":33752},{\"end\":33776,\"start\":33766},{\"end\":34102,\"start\":34096},{\"end\":34112,\"start\":34109},{\"end\":34128,\"start\":34121},{\"end\":34145,\"start\":34138},{\"end\":34172,\"start\":34165},{\"end\":34803,\"start\":34801},{\"end\":34814,\"start\":34811},{\"end\":34821,\"start\":34819},{\"end\":34834,\"start\":34829},{\"end\":35713,\"start\":35711},{\"end\":35724,\"start\":35721},{\"end\":35970,\"start\":35968},{\"end\":35981,\"start\":35976},{\"end\":35995,\"start\":35990},{\"end\":36004,\"start\":36002},{\"end\":36542,\"start\":36539},{\"end\":36550,\"start\":36548},{\"end\":36563,\"start\":36558},{\"end\":36970,\"start\":36967},{\"end\":36979,\"start\":36977},{\"end\":36989,\"start\":36985},{\"end\":37005,\"start\":37001},{\"end\":37019,\"start\":37014},{\"end\":37027,\"start\":37024},{\"end\":37040,\"start\":37037},{\"end\":37702,\"start\":37694},{\"end\":37716,\"start\":37710},{\"end\":37727,\"start\":37723},{\"end\":37741,\"start\":37738},{\"end\":38387,\"start\":38381},{\"end\":38402,\"start\":38397},{\"end\":38412,\"start\":38406},{\"end\":38632,\"start\":38614},{\"end\":38649,\"start\":38641},{\"end\":38671,\"start\":38658},{\"end\":38678,\"start\":38673},{\"end\":38941,\"start\":38934},{\"end\":38961,\"start\":38951},{\"end\":39158,\"start\":39151},{\"end\":39167,\"start\":39165},{\"end\":39180,\"start\":39175},{\"end\":39192,\"start\":39188},{\"end\":39206,\"start\":39200},{\"end\":39222,\"start\":39213},{\"end\":39490,\"start\":39487},{\"end\":39506,\"start\":39497},{\"end\":40368,\"start\":40361},{\"end\":40379,\"start\":40372},{\"end\":40608,\"start\":40605},{\"end\":40621,\"start\":40617},{\"end\":40633,\"start\":40630},{\"end\":40642,\"start\":40637},{\"end\":40880,\"start\":40872},{\"end\":40898,\"start\":40890},{\"end\":40912,\"start\":40902},{\"end\":41200,\"start\":41194},{\"end\":41210,\"start\":41202},{\"end\":41459,\"start\":41455},{\"end\":41475,\"start\":41470},{\"end\":41488,\"start\":41484},{\"end\":41505,\"start\":41497},{\"end\":41523,\"start\":41515},{\"end\":41536,\"start\":41533},{\"end\":41552,\"start\":41546},{\"end\":41563,\"start\":41558},{\"end\":41574,\"start\":41570},{\"end\":41592,\"start\":41583},{\"end\":41605,\"start\":41598},{\"end\":41619,\"start\":41611},{\"end\":41645,\"start\":41627},{\"end\":41656,\"start\":41654},{\"end\":41672,\"start\":41665},{\"end\":41684,\"start\":41681},{\"end\":41697,\"start\":41695},{\"end\":41711,\"start\":41707},{\"end\":41727,\"start\":41721},{\"end\":41742,\"start\":41737},{\"end\":41762,\"start\":41756},{\"end\":41768,\"start\":41764},{\"end\":42701,\"start\":42699},{\"end\":42716,\"start\":42713},{\"end\":42726,\"start\":42723},{\"end\":42734,\"start\":42731},{\"end\":43365,\"start\":43363},{\"end\":43375,\"start\":43371},{\"end\":43386,\"start\":43379},{\"end\":43393,\"start\":43390},{\"end\":43398,\"start\":43395},{\"end\":43618,\"start\":43616},{\"end\":43629,\"start\":43624},{\"end\":43644,\"start\":43640},{\"end\":43657,\"start\":43654},{\"end\":43669,\"start\":43667},{\"end\":43978,\"start\":43976},{\"end\":43986,\"start\":43984},{\"end\":43995,\"start\":43991},{\"end\":44011,\"start\":44006},{\"end\":44026,\"start\":44023},{\"end\":44040,\"start\":44036},{\"end\":44051,\"start\":44049},{\"end\":44765,\"start\":44761},{\"end\":44777,\"start\":44775},{\"end\":44789,\"start\":44785},{\"end\":44802,\"start\":44798},{\"end\":44827,\"start\":44811},{\"end\":45189,\"start\":45187},{\"end\":45203,\"start\":45198},{\"end\":45213,\"start\":45209},{\"end\":45222,\"start\":45220},{\"end\":45537,\"start\":45532},{\"end\":45553,\"start\":45547},{\"end\":46146,\"start\":46142},{\"end\":46154,\"start\":46152},{\"end\":46164,\"start\":46161},{\"end\":46179,\"start\":46176},{\"end\":46189,\"start\":46187},{\"end\":46202,\"start\":46199},{\"end\":46418,\"start\":46411},{\"end\":46433,\"start\":46429},{\"end\":46451,\"start\":46444},{\"end\":46464,\"start\":46461}]", "bib_entry": "[{\"attributes\":{\"doi\":\"abs/1607.07086\",\"id\":\"b0\",\"matched_paper_id\":14096841},\"end\":30279,\"start\":29881},{\"attributes\":{\"doi\":\"arXiv:1905.09922\",\"id\":\"b1\"},\"end\":30570,\"start\":30281},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b2\"},\"end\":30934,\"start\":30572},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6484065},\"end\":31292,\"start\":30936},{\"attributes\":{\"id\":\"b4\"},\"end\":31597,\"start\":31294},{\"attributes\":{\"id\":\"b5\"},\"end\":31806,\"start\":31599},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b6\",\"matched_paper_id\":20981275},\"end\":32428,\"start\":31808},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1041\",\"id\":\"b7\",\"matched_paper_id\":52190376},\"end\":33117,\"start\":32430},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1210515},\"end\":33707,\"start\":33119},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15184765},\"end\":34028,\"start\":33709},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5033497},\"end\":34713,\"start\":34030},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1169\",\"id\":\"b11\",\"matched_paper_id\":4937880},\"end\":35592,\"start\":34715},{\"attributes\":{\"doi\":\"abs/1804.06437\",\"id\":\"b12\",\"matched_paper_id\":4937880},\"end\":35903,\"start\":35594},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1421\",\"id\":\"b13\",\"matched_paper_id\":21646317},\"end\":36528,\"start\":35905},{\"attributes\":{\"doi\":\"arXiv:1905.12304\",\"id\":\"b14\"},\"end\":36882,\"start\":36530},{\"attributes\":{\"doi\":\"10.24963/ijcai.2019/711\",\"id\":\"b15\",\"matched_paper_id\":165163728},\"end\":37620,\"start\":36884},{\"attributes\":{\"doi\":\"10.3115/1073083.1073135\",\"id\":\"b16\",\"matched_paper_id\":11080756},\"end\":38317,\"start\":37622},{\"attributes\":{\"doi\":\"abs/1705.04304\",\"id\":\"b17\",\"matched_paper_id\":21850704},\"end\":38565,\"start\":38319},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13959787},\"end\":38832,\"start\":38567},{\"attributes\":{\"id\":\"b19\"},\"end\":39091,\"start\":38834},{\"attributes\":{\"id\":\"b20\"},\"end\":39364,\"start\":39093},{\"attributes\":{\"doi\":\"10.18653/v1/N18-1012\",\"id\":\"b21\",\"matched_paper_id\":4859003},\"end\":40283,\"start\":39366},{\"attributes\":{\"doi\":\"abs/2003.02738\",\"id\":\"b22\",\"matched_paper_id\":212415162},\"end\":40530,\"start\":40285},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46950555},\"end\":40783,\"start\":40532},{\"attributes\":{\"doi\":\"abs/1908.09368\",\"id\":\"b24\",\"matched_paper_id\":201666620},\"end\":41100,\"start\":40785},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2332513},\"end\":41386,\"start\":41102},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":208117506},\"end\":42602,\"start\":41388},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1482\",\"id\":\"b27\",\"matched_paper_id\":174799580},\"end\":43289,\"start\":42604},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52100616},\"end\":43537,\"start\":43291},{\"attributes\":{\"doi\":\"arXiv:1908.08039\",\"id\":\"b29\"},\"end\":43878,\"start\":43539},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":46889991},\"end\":44678,\"start\":43880},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":44061800},\"end\":45111,\"start\":44680},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3439214},\"end\":45463,\"start\":45113},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1062\",\"id\":\"b33\",\"matched_paper_id\":7473831},\"end\":46062,\"start\":45465},{\"attributes\":{\"id\":\"b34\"},\"end\":46353,\"start\":46064},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":336219},\"end\":46934,\"start\":46355}]", "bib_title": "[{\"end\":29929,\"start\":29881},{\"end\":30986,\"start\":30936},{\"end\":31844,\"start\":31808},{\"end\":32503,\"start\":32430},{\"end\":33166,\"start\":33119},{\"end\":33742,\"start\":33709},{\"end\":34084,\"start\":34030},{\"end\":34792,\"start\":34715},{\"end\":35702,\"start\":35594},{\"end\":35959,\"start\":35905},{\"end\":36960,\"start\":36884},{\"end\":37684,\"start\":37622},{\"end\":38372,\"start\":38319},{\"end\":38606,\"start\":38567},{\"end\":39479,\"start\":39366},{\"end\":40351,\"start\":40285},{\"end\":40598,\"start\":40532},{\"end\":40868,\"start\":40785},{\"end\":41190,\"start\":41102},{\"end\":41446,\"start\":41388},{\"end\":42692,\"start\":42604},{\"end\":43355,\"start\":43291},{\"end\":43965,\"start\":43880},{\"end\":44752,\"start\":44680},{\"end\":45178,\"start\":45113},{\"end\":45521,\"start\":45465},{\"end\":46401,\"start\":46355}]", "bib_author": "[{\"end\":29949,\"start\":29931},{\"end\":29966,\"start\":29949},{\"end\":29977,\"start\":29966},{\"end\":29992,\"start\":29977},{\"end\":30003,\"start\":29992},{\"end\":30018,\"start\":30003},{\"end\":30037,\"start\":30018},{\"end\":30052,\"start\":30037},{\"end\":30322,\"start\":30281},{\"end\":30334,\"start\":30322},{\"end\":30346,\"start\":30334},{\"end\":30355,\"start\":30346},{\"end\":30586,\"start\":30572},{\"end\":30602,\"start\":30586},{\"end\":30614,\"start\":30602},{\"end\":30634,\"start\":30614},{\"end\":31000,\"start\":30988},{\"end\":31012,\"start\":31000},{\"end\":31025,\"start\":31012},{\"end\":31039,\"start\":31025},{\"end\":31048,\"start\":31039},{\"end\":31390,\"start\":31377},{\"end\":31401,\"start\":31390},{\"end\":31413,\"start\":31401},{\"end\":31427,\"start\":31413},{\"end\":31440,\"start\":31427},{\"end\":31659,\"start\":31645},{\"end\":31669,\"start\":31659},{\"end\":31685,\"start\":31669},{\"end\":31697,\"start\":31685},{\"end\":31858,\"start\":31846},{\"end\":31871,\"start\":31858},{\"end\":31886,\"start\":31871},{\"end\":31908,\"start\":31886},{\"end\":31921,\"start\":31908},{\"end\":32518,\"start\":32505},{\"end\":32528,\"start\":32518},{\"end\":32547,\"start\":32528},{\"end\":32564,\"start\":32547},{\"end\":33183,\"start\":33168},{\"end\":33198,\"start\":33183},{\"end\":33216,\"start\":33198},{\"end\":33231,\"start\":33216},{\"end\":33760,\"start\":33744},{\"end\":33778,\"start\":33760},{\"end\":34104,\"start\":34086},{\"end\":34114,\"start\":34104},{\"end\":34130,\"start\":34114},{\"end\":34147,\"start\":34130},{\"end\":34174,\"start\":34147},{\"end\":34805,\"start\":34794},{\"end\":34816,\"start\":34805},{\"end\":34823,\"start\":34816},{\"end\":34836,\"start\":34823},{\"end\":35715,\"start\":35704},{\"end\":35726,\"start\":35715},{\"end\":35972,\"start\":35961},{\"end\":35983,\"start\":35972},{\"end\":35997,\"start\":35983},{\"end\":36006,\"start\":35997},{\"end\":36544,\"start\":36530},{\"end\":36552,\"start\":36544},{\"end\":36565,\"start\":36552},{\"end\":36972,\"start\":36962},{\"end\":36981,\"start\":36972},{\"end\":36991,\"start\":36981},{\"end\":37007,\"start\":36991},{\"end\":37021,\"start\":37007},{\"end\":37029,\"start\":37021},{\"end\":37042,\"start\":37029},{\"end\":37704,\"start\":37686},{\"end\":37718,\"start\":37704},{\"end\":37729,\"start\":37718},{\"end\":37743,\"start\":37729},{\"end\":38389,\"start\":38374},{\"end\":38404,\"start\":38389},{\"end\":38414,\"start\":38404},{\"end\":38634,\"start\":38608},{\"end\":38651,\"start\":38634},{\"end\":38673,\"start\":38651},{\"end\":38680,\"start\":38673},{\"end\":38943,\"start\":38929},{\"end\":38963,\"start\":38943},{\"end\":39160,\"start\":39146},{\"end\":39169,\"start\":39160},{\"end\":39182,\"start\":39169},{\"end\":39194,\"start\":39182},{\"end\":39208,\"start\":39194},{\"end\":39224,\"start\":39208},{\"end\":39492,\"start\":39481},{\"end\":39508,\"start\":39492},{\"end\":40370,\"start\":40353},{\"end\":40381,\"start\":40370},{\"end\":40610,\"start\":40600},{\"end\":40623,\"start\":40610},{\"end\":40635,\"start\":40623},{\"end\":40644,\"start\":40635},{\"end\":40882,\"start\":40870},{\"end\":40900,\"start\":40882},{\"end\":40914,\"start\":40900},{\"end\":41202,\"start\":41192},{\"end\":41212,\"start\":41202},{\"end\":41461,\"start\":41448},{\"end\":41477,\"start\":41461},{\"end\":41490,\"start\":41477},{\"end\":41507,\"start\":41490},{\"end\":41525,\"start\":41507},{\"end\":41538,\"start\":41525},{\"end\":41554,\"start\":41538},{\"end\":41565,\"start\":41554},{\"end\":41576,\"start\":41565},{\"end\":41594,\"start\":41576},{\"end\":41607,\"start\":41594},{\"end\":41621,\"start\":41607},{\"end\":41647,\"start\":41621},{\"end\":41658,\"start\":41647},{\"end\":41674,\"start\":41658},{\"end\":41686,\"start\":41674},{\"end\":41699,\"start\":41686},{\"end\":41713,\"start\":41699},{\"end\":41729,\"start\":41713},{\"end\":41744,\"start\":41729},{\"end\":41764,\"start\":41744},{\"end\":41770,\"start\":41764},{\"end\":42703,\"start\":42694},{\"end\":42718,\"start\":42703},{\"end\":42728,\"start\":42718},{\"end\":42736,\"start\":42728},{\"end\":43367,\"start\":43357},{\"end\":43377,\"start\":43367},{\"end\":43388,\"start\":43377},{\"end\":43395,\"start\":43388},{\"end\":43400,\"start\":43395},{\"end\":43620,\"start\":43611},{\"end\":43631,\"start\":43620},{\"end\":43646,\"start\":43631},{\"end\":43659,\"start\":43646},{\"end\":43671,\"start\":43659},{\"end\":43980,\"start\":43967},{\"end\":43988,\"start\":43980},{\"end\":43997,\"start\":43988},{\"end\":44013,\"start\":43997},{\"end\":44028,\"start\":44013},{\"end\":44042,\"start\":44028},{\"end\":44053,\"start\":44042},{\"end\":44767,\"start\":44754},{\"end\":44779,\"start\":44767},{\"end\":44791,\"start\":44779},{\"end\":44804,\"start\":44791},{\"end\":44829,\"start\":44804},{\"end\":45191,\"start\":45180},{\"end\":45205,\"start\":45191},{\"end\":45215,\"start\":45205},{\"end\":45224,\"start\":45215},{\"end\":45539,\"start\":45523},{\"end\":45555,\"start\":45539},{\"end\":46148,\"start\":46134},{\"end\":46156,\"start\":46148},{\"end\":46166,\"start\":46156},{\"end\":46181,\"start\":46166},{\"end\":46191,\"start\":46181},{\"end\":46204,\"start\":46191},{\"end\":46420,\"start\":46403},{\"end\":46435,\"start\":46420},{\"end\":46453,\"start\":46435},{\"end\":46466,\"start\":46453}]", "bib_venue": "[{\"end\":32065,\"start\":31995},{\"end\":32760,\"start\":32673},{\"end\":33346,\"start\":33291},{\"end\":33844,\"start\":33807},{\"end\":34350,\"start\":34262},{\"end\":35149,\"start\":35000},{\"end\":36202,\"start\":36114},{\"end\":37252,\"start\":37167},{\"end\":37958,\"start\":37855},{\"end\":38694,\"start\":38691},{\"end\":39821,\"start\":39672},{\"end\":41975,\"start\":41881},{\"end\":42932,\"start\":42845},{\"end\":44234,\"start\":44142},{\"end\":45753,\"start\":45663},{\"end\":46635,\"start\":46548},{\"end\":30071,\"start\":30066},{\"end\":30406,\"start\":30371},{\"end\":30730,\"start\":30650},{\"end\":31104,\"start\":31048},{\"end\":31375,\"start\":31294},{\"end\":31643,\"start\":31599},{\"end\":31993,\"start\":31925},{\"end\":32671,\"start\":32584},{\"end\":33289,\"start\":33231},{\"end\":33805,\"start\":33778},{\"end\":34260,\"start\":34174},{\"end\":34998,\"start\":34856},{\"end\":35745,\"start\":35740},{\"end\":36112,\"start\":36026},{\"end\":36689,\"start\":36581},{\"end\":37165,\"start\":37065},{\"end\":37853,\"start\":37766},{\"end\":38433,\"start\":38428},{\"end\":38689,\"start\":38680},{\"end\":38927,\"start\":38834},{\"end\":39144,\"start\":39093},{\"end\":39670,\"start\":39528},{\"end\":40400,\"start\":40395},{\"end\":40649,\"start\":40644},{\"end\":40933,\"start\":40928},{\"end\":41228,\"start\":41212},{\"end\":41879,\"start\":41770},{\"end\":42843,\"start\":42756},{\"end\":43405,\"start\":43400},{\"end\":43609,\"start\":43539},{\"end\":44140,\"start\":44053},{\"end\":44878,\"start\":44829},{\"end\":45279,\"start\":45224},{\"end\":45661,\"start\":45575},{\"end\":46132,\"start\":46064},{\"end\":46546,\"start\":46466}]"}}}, "year": 2023, "month": 12, "day": 17}