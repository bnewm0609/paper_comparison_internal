{"id": 250311130, "updated": "2023-10-05 12:41:10.273", "metadata": {"title": "Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods", "authors": "[{\"first\":\"Davoud\",\"last\":\"Tarzanagh\",\"middle\":[\"Ataee\"]},{\"first\":\"Laura\",\"last\":\"Balzano\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Online optimization is a well-established optimization paradigm that aims to make a sequence of correct decisions given knowledge of the correct answer to previous decision tasks. Bilevel programming involves a hierarchical optimization problem where the feasible region of the so-called outer problem is restricted by the graph of the solution set mapping of the inner problem. This paper brings these two ideas together and studies an online bilevel optimization setting in which a sequence of time-varying bilevel problems are revealed one after the other. We extend the known regret bounds for single-level online algorithms to the bilevel setting. Specifically, we introduce new notions of bilevel regret, develop an online alternating time-averaged gradient method that is capable of leveraging smoothness, and provide regret bounds in terms of the path-length of the inner and outer minimizer sequences.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "2207.02829", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-02829", "doi": "10.48550/arxiv.2207.02829"}}, "content": {"source": {"pdf_hash": "97d36b4fdebcda401818e5b269c097f1ca897223", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.02829v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7a945290fcbc4ef045a468985f34008c7270ad56", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/97d36b4fdebcda401818e5b269c097f1ca897223.txt", "contents": "\nOnline Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods\n\n\nDavoud Ataee Tarzanagh tarzanaq@umich.edu \nDepartment of Electrical Engineering and Computer Science\nUniversity of Michigan\nAnn Arbor\n\nLaura Balzano \nDepartment of Electrical Engineering and Computer Science\nUniversity of Michigan\nAnn Arbor\n\nOnline Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods\n\nOnline optimization is a well-established optimization paradigm that aims to make a sequence of correct decisions given knowledge of the correct answer to previous decision tasks. Bilevel programming involves a hierarchical optimization problem where the feasible region of the socalled outer problem is restricted by the graph of the solution set mapping of the inner problem. This paper brings these two ideas together and studies an online bilevel optimization setting in which a sequence of time-varying bilevel problems are revealed one after the other. We extend the known regret bounds for single-level online algorithms to the bilevel setting. Specifically, we introduce new notions of bilevel regret, develop an online alternating time-averaged gradient method that is capable of leveraging smoothness, and provide regret bounds in terms of the path-length of the inner and outer minimizer sequences.\n\nIntroduction\n\nBilevel optimization (BO) is rapidly evolving due to its wide array of applications in modern machine learning problems including meta-learning [25,10], hyperparameter optimization [28,24], neural network architecture search [57,8], data hypercleaning [70], and reinforcement learning [46,84]. A fundamental assumption in BO which has been adopted by almost all of the relevant literature [3,60,29,46,27,30,85,17,51,79], is that the inner and outer cost functions do not change throughout the horizon over which we seek to optimize. This offline setting may not be suitable to model temporal changes in today's machine learning problems such as online actor-critic [96,80], online meta-learning [26], strategic dynamic regression [35], and sequential decision makings for which the objective functions are time-varying and are not available to the decision-maker a priori. To address these challenges, this paper considers an online bilevel optimization (OBO) setting in which a sequence of bilevel problems are revealed one after the other, and studies computationally tractable notions of bilevel regret minimization.\n\nWe begin by introducing the setting of online (single-level) optimization, which is modeled as a game between a learner and an adversary [36,98]. During each round t \u2208 [T ] := {1, . . . , T } of a repeated decision process, the learner is tasked with predicting x t \u2208 X \u2282 R d 1 , a convex decision set. Concurrently, the adversary chooses a loss function f t : X \u2192 R; the learner then observes f t (x) and suffers a loss of f t (x t ). This process repeats across T rounds. In the non-static setting [11,48], the performance of the learner is measured through its (single-level) dynamic regret\nD-Reg T := T t=1 f t (x t ) \u2212 T t=1 f t (x * t ),(1)\nwhere x * t \u2208 arg min x\u2208X f t (x). In the case of static regret [36,98], x * t is replaced by x * \u2208 arg min x\u2208X T t=1 f t (x), i.e.,\n\n\nS-Reg\nT := T t=1 f t (x t ) \u2212 min x\u2208X T t=1 f t (x).(2)\nThe static regret (2) essentially assumes that the comparators do not change over time. This can be an unrealistic constraint in many practical streaming data problems, ranging from motion imagery formation to network analysis, for which the underlying environment is dynamic [32,11,48]. The parameters {x * t } T t=1 could correspond to frames in a video or the weights of edges in a social network and by nature are highly variable.\n\n\nOnline Bilevel Optimization\n\nBilevel optimization, also known as the Stackelberg leader-follower game [81], represents a situation involving two players. The choice of each player influences the other's outcome, in addition to her own. One player, the leader, knows the objective function of the other, the follower. The leader therefore can perfectly predict the follower's choice, and she will consider that when optimizing her own objective. The follower, on the other hand, only has knowledge of her own objective, and must consider that objective as restricted by the leader's choice. BO is in this sense equivalent to a non-cooperative two-person game [9].\n\nTo formulate an OBO problem, let x t \u2208 X \u2282 R d 1 and f t : X \u00d7 R d 2 denote the decision variable vector and the objective function for the leader, respectively; similarly define y t \u2208 R d 2 and g t \u2208 X \u00d7 R d 2 for the follower 1 . In each round t \u2208 [T ], knowing the decision x t\u22121 of the leader, the follower has to select y t in an attempt to minimize g t (x t , y) using the information from rounds t \u2212 1, t \u2212 2, . . . , 0. Being aware of the follower's selection, the leader then moves by selecting x t to minimize her own objective function f t (x, y t ). This procedure of play is repeated across T rounds. We characterize our approach in terms of the bilevel dynamic regret:\nBD-Reg T := T t=1 f t (x t , y * t (x t )) \u2212 T t=1 f t (x * t , y * t (x * t )),(3a)\nwhere y * t (x) \u2208 arg min y\u2208R d 2 g t (x, y), and x * t \u2208 arg min x\u2208X f t (x, y * t (x)) .\n\nWe also study the framework of regret minimization setting where x * t in (3) is replaced by x * \u2208 arg min x\u2208X T t=1 f t (x, y * t (x)). In this case, the goal of the leader is to generate a sequence of decisions {x t } T t=1 so that the following regret can be minimized:\nBS-Reg T := T t=1 f t (x t , y * t (x t )) \u2212 min x\u2208X T t=1 f t (x, y * t (x)).(4)\nNote that the above regret is not fully static and the inner optima {y * t } T t=1 are changing over T . Example 1 provides a machine learning application of OBO.\n\n\nExample 1. [Online Meta-Learning]\n\nMeta-learning aims to bootstrap from a set of given tasks to learn faster on future tasks [69,25]. A popular formulation is an online meta-learning (OML) setting where agents are faced with tasks one after another [26]. OML can be formulated as the follower-leader OBO. Specifically, for each task T t , the follower receives some training data D tr t for adapting the leader's model w t \u2208 W \u2282 R d 1 to the current task following the strategy:\nu * t (w t ) \u2208 arg min u u, \u2207f (w t ; D tr t ) + 1 2\u03b2 u \u2212 w t 2 ,\nfor some \u03b2 > 0. Then, the test data D ts t will be revealed to the leader for evaluating the performance of the follower's model u * In general, it is impossible to achieve a sublinear dynamic regret bound, due to the arbitrary fluctuation in the time-varying functions [98,11]. Existing single-level analysis show that it is indeed possible to bound the dynamic regret in terms of certain regularity of the comparator sequence [98,62,86,90], functional variation [11], or the gradient variation [19,48]. Hence, in order to achieve a sublinear regret, one has to impose some regularity constraints on the sequence of cost functions. In this work, we define the outer and inner path-length (of order p) quantities to capture the regularity of the sequences:\nP p,T := T t=2 x * t\u22121 \u2212 x * t p and Y p,T := T t=2 y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) p .(6)\nNote that P p,T is the path-length of the outer minimizers and is widely used for analyzing the dynamic regret of single-level non-stationary optimization [98,62,86,90]. Y p,T is a new regularity for OBO that measures how fast the minimizers of inner cost functions change. The following example shows that P p,T and Y p,T are not comparable in general and both measures play a key role in OBO.\nExample 2. Let X = [\u22121, 1]\nand consider a sequence of quadratic cost functions\nf t (x, y) = 1 2 (x + 2a (1) t ) 2 + 1 2 (y \u2212 a (2) t ) 2 + a (3) t , g t (x, y) = 1 2 y 2 \u2212 (x \u2212 a (2) t )y + a (4) t for t = 1, . . . , T , where {a (i) t } 4 i=1 are some time-varying constants. It follows from (3b) that y * t (x t ) = x t \u2212 a (2) t , x * t = \u2212a (1) t + a(2)\nt , and\ny * t (x * t ) = a (1) t . Let a(2)t = (\u22121) t / \u221a t for t = 1, . . . , T . \u2022 If a (1) t = a (2) t , then P 1,T = P 2,T = 0, Y 1,T = O( \u221a T ), and Y 2,T = O(log T ). \u2022 If a(1)t = 0, then P 1,T = O( \u221a T ), P 2,T = O(log T )\n, and Y 1,T = Y 2,T = 0. This shows that P p,T and Y p,T are not comparable.\n\n\nRegret Notion\n\n\nBound\n\n\nSingle-Level Regret Minimization\n\n[62]\n\nDynamic O(1 + P 1,T ) SC [90] O (1 + min(P 1,T , P 2,T )) Besides path-length regularity, other notions of regularities have also been considered in online learning such as function variation [19] V T := T t=2 sup x\u2208X |f t\u22121 (x)\u2212f t (x)| and the gradient variation\n[15] O (1 + P 2,T ) [38] Static O (log T ) [11] O(1 + T 2/3 V T 1/3 ) C [48] Dynamic O(1 + G T P 1,T ) [86] O (1 + P 1,T ) [98] Static O( \u221a T ) NC [40] Local O(T /w 2 ) Bilevel Regret Minimization SC Thm 5(I) Dynamic O (1 + P 2,T + Y 2,T ) Thm 5(II) Static O log T +\u0232 2,T C Thm 8(I) Dynamic O (1 + P 1,T + Y 1,T + Y 2,T ) Thm 8(II) Static O( \u221a T +\u0232 1,T +\u0232 2,T ) NC Thm 9 Local O((T + H T )/w)[11] G T := T t=2 sup x\u2208X \u2207f t\u22121 (x) \u2212 \u2207f t (x) 2 .\nFor the leader's regret analysis in the static and local settings, we also define\u0232 p,T := T t=2 y * t\u22121 (x * ) \u2212 y * t (x * ) p and H T := T t=2 sup x\u2208X y * t\u22121 (x)\u2212 y * t (x) 2 for capturing the dynamics of the inner problem.\n\n\nOur Results\n\nOur main contributions lie in developing several new results for OBO, including the first-known regret bound. More specifically, we \u2022 Define new notions of bilevel regret, given above in (3) and (4), applicable to a wide class of convex OBO problems. To minimize the proposed regret, we introduce an online alternating gradient descent (OAGD) method that is capable of leveraging the smoothness, and provide its regret bounds in terms of the path-length of the inner and/or outer minimizer sequences.\n\n\u2022 Present a problem-dependent regret bound on the proposed dynamic regret that depends solely on P 2,T and Y 2,T in Theorem 5. We then establish a lower bound for OAGD in Theorem 7 that matches the upper bound we obtain for smooth strongly convex functions.\n\n\u2022 Introduce a novel notion of bilevel local regret, which permits efficient OBO in the nonconvex setting. We give an alternating time-averaged gradient method, and prove in Theorem 9 that it achieves sublinear regret according to our proposed definition of bilevel local regret. Notation. Any notation is defined when it is used, but for reference the reader may also find it in Table 3.\n\n\nRelated Work\n\n\u2022 Static Regret Minimization: Single-level static regret (Eq. (2)) is well-studied in the literature of online learning [37, 71,38]. [98] shows that online gradient descent (OGD) provides a O( \u221a T ) regret bound for convex (possibly nonsmooth) functions. [38] improves this bound to O(d 1 log T ), and O(log T ) for exponentially concave and strongly-convex functions, respectively. \u2022 Dynamic Regret Minimization: Single-level dynamic regret forces the player to compete with time-varying comparators, and thus is particularly favored in non-stationary environments [78,11]. There are two kinds of dynamic regret in previous studies: The universal dynamic regret aims to compare with any feasible comparator sequence [98,89,94,92], while the worst-case dynamic regret (defined in (1)) specifies the comparator sequence to be the sequence of minimizers of online functions [11,48,62,86,90,6]. We present related works for the latter case, as it is the setting studied in this paper. For strongly convex and smooth functions, [62] shows that an O(1 + P 1,T ) bound is achievable. [86] shows that OGD enjoys an O(1 + P 1,T ) bound for convex and smooth functions with an additional vanishing gradient condition.\n\n[15] proves that OGD can achieve an O(1 + P 2,T ) regret bound without the bounded gradient assumption. [90] proposes the online multiple gradient descent algorithm and proves that it enjoys an O(1 + min(P 1,T , P 2,T )) bound, which has been recently enhanced to O(1 + min(P 1,T , P 2,T , V T )) [93]. [64,65] provide dynamic regret bounds in terms of\nT t=2 x * t\u22121 \u2212 x * t 1 .\n[11] shows that OGD attains an O(T 2/3 V T 1/3 ) regret for convex functions. \u2022 Local Regret Minimization: There are several approaches to treat the online (single-level) non-convex optimization including adversarial multi-armed bandit with a continuum of arms [14, 53, 55, 42, 41] and classical Follow-the-Perturbed-Leader algorithm with access to an offline non-convex optimization oracle [2,77,54]. Complementing this literature, [40] considers a local regret that averages a sliding window of gradients at the current model x t and quantifies the objective of predicting points with small gradients on average. \u2022 (Offline) Bilevel Optimization: Since its first formulation by Stackelberg [76] and the first mathematical model by Bracken and McGill [13] there has been a steady growth in investigations and applications of BO [74,58]. Some of the initial work [3,23,4,34,73,60, 63] reduce the problem into a single-level optimization problem by replacing the lower-level problem by its optimality conditions. Recently, (alternating) gradient-based approaches have gained popularity due to their simplicity and effectiveness [29,46,27,30,85,17,51,16,52,20]. This type of approach estimates the hypergradient \u2207f (x, y * (x)) for iterative updates, and can generally be divided into approximate implicit differentiation (AID) and iterative differentiation (ITD) categories. ITD-based approaches [61, 27,25,30] estimate the hypergradient \u2207f (x, y * (x)) in either a reverse (automatic differentiation) or forward manner. AID-based approaches [22,68,30, 51] estimate the hypergradient via implicit differentiation. [28] characterizes the asymptotic convergence of a backpropagation-based approach as one of ITD-based algorithms by assuming the inner-level problem is strongly convex. [70] provides a similar analysis for a truncated backpropagation scheme. [59, 56] analyze the asymptotic performance of ITD-based approaches when the inner-level problem is convex. Finite-time complexity analysis for BO has also been explored [29,51,46,17,79]. However, all these works assume that the cost function does not change throughout the horizon over which we seek to optimize. We address this limitation of BO by studying a new class of optimization algorithms in the online setting.\n\n\nAlgorithm and Regret Bounds\n\nIn this section, we provide regret bounds dependent on the inner and outer regularities P p,T and Y p,T . We present regret bounds in two settings: (i) the full information of the loss function is revealed at each step; (ii) only the true gradient of the inner function and an approximate gradient of the outer function are revealed.\n\n\nOBO with Full Information\n\nWe first list assumptions for OBO.\nAssumption A. For all t \u2208 [T ]: A1. f t is f,0 -Lipschitz continuous.\nA2. g t (x, y) is \u00b5 g -strongly convex in y for any x \u2208 X . A3. \u2207f t , \u2207g t , \u2207 2 g t are respectively f,1 , g,1 , g,2 -Lipschitz continuous.\nAssumption B. The convex decision set X \u2286 R d 1 is bounded, i.e., x \u2212 x \u2264 D for any x, x \u2208 X .\nAssumption A requires that the inner and outer functions {(f t , g t )} T t=1 are well-behaved. These assumptions are common in offline BO [29,46,51,85,17]. We first show that an O(1 + P 1,T + Y 1,T ) upper bound is achievable. Given (x 1 , y 1 ) \u2208 X \u00d7 R d 2 , we assume at each step the full information of the cost functions (f t , g t ) is revealed after the decisions (y t , x t ) is submitted. Then, we can update y t+1 and x t+1 , respectively, by\ny t+1 = arg min y\u2208R d 2 g t (x, y) and x t+1 = arg min x\u2208X f t (x, y t+1 ),(7)\nfor all t = 1, . . . , T .\n\nTheorem 1. Under Assumptions A1. and B, for the sequence {(x t+1 , y t+1 )} T t=1 generated by (7), we have BD-Reg T \u2264 O 1 + P 1,T + Y 1,T .\n\nTheorem 1 shows that by simply playing (at each round) the minimum of the previous round, i.e. (7), one can achieve the problem-dependent bound O (1 + P 1,T + Y 1,T ). It is notable that a similar upper bound of O(1 + P 1,T ) with the full information can be achieved in the single-level setting [86].\n\n\nOBO with Partial Information\n\nPerhaps the simplest algorithm that applies to the most general setting of online (single-level) optimization is OGD [98]: For each t \u2208 [T ], play x t \u2208 X , observe the cost function f t , and set\nx t+1 = \u03a0 X x t \u2212 \u03b1 t \u2207f t (x t ) , \u03b1 t > 0,\nwhere \u03a0 X is the projection onto X .\n\nWe consider a natural extension of OGD to the bilevel setting and show that it enjoys regret bounds in terms of the path-length of the inner and/or outer minimizer sequences. To do so, we need to compute the gradient \u2207f t (x, y * t (x)) where y * t (x) is defined in (3b). The computation of \u2207f t (x, y * t (x)) involves Jacobian \u2207 2 xy g t (x, y) and Hessian \u2207 2 y g t (x, y). More concretely, since \u2207 y g t (x, y * t (x)) = 0, it follows from Assumption A and the implicit function theorem that\n\u2207y * t (x)\u2207 2 yy g t (x, y * t (x)) + \u2207 2 xy g t (x, y * t (x)) = 0,(9a)\nwhich together with the chain rule gives \u2207f t (x, y * Algorithm 1 : OAGD OAGD OAGD for bilevel regret minimization\nRequire: (x 1 , y 1 ) \u2208 X \u00d7 R d 2 ; w, T, {K t } T t=1 \u2208 N; stepsizes {(\u03b1 t , \u03b2 t ) \u2208 R 2 ++ } T t=1 ; and {u i } w\u22121 i=0 with 1 = u 0 \u2265 u 1 . . . u w\u22121 > 0. 1: for t = 1 to T do 2: z 1 t \u2190 y t 3: for k = 1 to K t do 4: z k+1 t \u2190 z k t \u2212 \u03b2 t \u2207 z g t (x t , z k t ) 5: end for 6: y t+1 \u2190 z Kt+1 t 7: x t+1 \u2190 \u03a0 X x t \u2212 \u03b1 t\u2207 F t,u (x t , y t+1 ) 8: end for\nThe exact gradient \u2207f t (x, y * t (x)) requires information about y * t (x) that is not available in general. Hence, it is not possible to utilize gradient-type algorithms to minimize bilevel regret. In this work, inspired by offline bilevel [29] and online single-level [40, 6] optimization, we define a new time-averaged hypergradient as a surrogate of \u2207f t (x, y * t (x)) by replacing y * t (x t ) by y t \u2208 R d 2 and using the history of the gradients as follows. \n= 1. Let F t,u (x, y) := 1/W w\u22121 i=0 u i f t\u2212i (x, y) with W = w\u22121 i=0 u i and the convention f t \u2261 0 for t \u2264 0. Let M t (x, y)\nbe a solution to the following equation:\n\u2207 2 xy g t (x, y) + M t (x, y)\u2207 2 yy g t (x, y) = 0. (10a)\nThen, the time-averaged hypergradient with window size w is defined as\n\u2207F t,u (x, y) := 1 W w\u22121 i=0 u i\u2207 f t\u2212i (x, y), (10b) where\u2207 f t (x, y) := \u2207 x f t (x, y) + M t (x, y)\u2207 y f t (x, y). (10c)\nRemark 3. One can notice that the gradients of the loss functions from the w most recent rounds of play are used to define the hypergradient. Specifically, by setting u i = 1, it averages a sliding window of the online hypergradients at their corresponding parameters over a window at each update iteration. If we set u i = \u03b3 i for some \u03b3 \u2208 (0, 1), it assigns more weights to the most recent values, and it gives the exponential averaged of the hypergradients.\n\nThe pseudo-code of online alternating gradient descent (OAGD) method is presented in Algorithm 1. This algorithm is very simple to implement. At each timestep t \u2208 [T ], OAGD alternates between the gradient update on y t and the time-averaged projected hyper-gradient on x t . One can notice that the alternating update in Algorithm 1 serves as a template of running OGD on online bilevel problems. In OAGD, w and u i are tunable parameters. Remark 3 and Theorem 9 provide a suggested value for them. Intuitively, the value of w captures the level of averaging (smoothness) of the outer gradient at round t. We note that OAGD is similar to single-level time-smoothing OGD-type methods for the outer variable update [40, 6] and OGD with multiple local updates [90]. Also, without the inner variable and by setting the window size w = 1, our OAGD reduces to OGD-type methods such as [86]. It should be mentioned that w > 1 is not required for our bilevel dynamic and static regret minimization. However, evaluations in Section 4 reveal that smoothing hyper-gradient provides a performance boost over the case w = 1. Finally, we note that for w = 1, Algorithm 1 is similar to the alternating gradient methods [29,46,17,51] for solving offline bilevel problems.\nLemma 4. Under Assumption A, for all t \u2208 [T ], x,\u1e8b,\u1e8d \u2208 X , and y \u2208 R d 2 , we have \u2207 f t (x, y) \u2212 \u2207f t (x, y * (x)) \u2264 M f y \u2212 y * (x) , \u2207f t (\u1e8b, y * (\u1e8b)) \u2212 \u2207f t (\u1e8d, y * (\u1e8d)) \u2264 L f \u1e8b \u2212\u1e8d , y * t (\u1e8b) \u2212 y * t (\u1e8d) \u2264 L y \u1e8b \u2212\u1e8d .\nHere,\nL y = O(\u03ba g ), M f = O(\u03ba 2 g ), and L f = O(\u03ba 3 g ).\nWe are now ready to state the main results of this section. For ease of presentation, in Theorems 5 and 8, we set w = 1.\n\nTheorem 5 (Strongly-Convex). Suppose Assumptions A and B hold. Further, assume f t is strongly convex with parameter \u00b5 f and \u03b2 t = \u03b2 = 2/( g,1 + \u00b5 g ) for all t \u2208 [T ]. Then, Algorithm 1 guarantees the following.\n(I) If \u03b1 t = \u03b1 = 4c \u00b5 f and K t = K \u2265 0.5(\u03ba g +1) log 12M 2 f (1+ 1 c )+2 , for some c \u2264 \u00b5 2 f /(2L 2 f +2L 2 y ) and all t \u2208 [T ], then BD-Reg T \u2264 O 1 + 1 \u03b1 P 2,T + \u03b1 T t=1 \u2207f t (x * t , y * t (x * t )) + \u03b1Y 2,T . (12) (II) If \u03b1 t = 2 \u00b5 f t and K t = K \u2265 0.5(\u03ba g + 1) log 72L 2 y M 2 f \u00b5 f + \u00b5 f 2 , for all t \u2208 [T ], then BS-Reg T \u2264 O log T +\u0232 2,T ,(13)where\u0232 2,T = T t=2 y * t\u22121 (x * ) \u2212 y * t (x * ) 2 is the static variant of Y 2,T .\nTheorem 5 states that with fixed stepsizes (\u03b1, \u03b2) and K t = O(\u03ba g ), Algorithm 1 can achieve a problem-dependent bilevel regret bound. We note that the number of inner loops K t is set to be a constant depending on the condition number of the function. One may ask whether we can obtain a tighter bound by using a larger K t for all t \u2208 [T ]. Unfortunately, according to our analysis, even for sufficently large K t \u2192 \u221e, which means g t is minimized exactly, the regret bound can only be improved by a constant factor and the order remains the same. It should be mentioned that in (12), the first two terms are outer optimization errors that are common to OGD methods [90,15]. The third term is the inner optimization error.\n\nCorollary 6. Under the same setting as Theorem 5(I),\n(I) If f t is non-negative for each t \u2208 [T ], then BD-Reg T \u2264 O 1 + P 2,T /\u03b1 + \u03b1(F T + Y 2,T ) ,(14)\nwhere F T := T t=1 f t (x * Corollary 6 naturally interpolates between the single level and bilevel regret. In the case when Y 2,T = 0, Eq. (14) recovers the single-level regret for the strongly convex, smooth, and nonegative losses similar to [94,75]. We note that if the minimizers {x * t } T t=1 lie in the interior of the domain\nX , we have \u2207f t (x * t , y * t (x * t )) = 0 for all t \u2208 [T ]\n, which implies the O (1 + P 2,T /\u03b1 + \u03b1Y 2,T ) regret bound. Some interesting conclusions can be derived if we consider specific rates of variability in (15).\n\n\u2022 Off-line functions: If f t = f and g t = g, we have P 2,T = Y 2,T = 0 and it follows that the regret grows at a rate O(1). This gives convergence rates for offline bilevel gradient methods [51, 46, 17].\n\n\u2022 Logarithmic regret: If the difference between consecutive inner and outer arguments decreases as 1/t, we have that P 2,T = O(log T ) and Y 2,T = O(log T ) and the regret grows at a logarithmic rate.\n\nThe following theorem provides the lower bound \u2126(P 2,T + Y 2,T ) for OBO.\n\nTheorem 7 (Lower Bound). For any OBO algorithm, there always exists a sequence of smooth and strongly convex functions\n{(f t , g t )} T t=1 such that BD-Reg T = \u2126(1 + P 2,T + Y 2,T ).\nTheorems 5 and 7 state that with fixed stepsizes (\u03b1, \u03b2) and K t = O(\u03ba g ), Algorithm 1 can achieve an optimal bilevel dynamic regret bound.\n\nThe following theorem provides the regret bounds for online convex functions {f t } T t=1 . Theorem 8 (Convex). Suppose Assumptions A and B hold. Further, assume f t is convex, \u03b2 t = \u03b2 = 2/( g,1 + \u00b5 g ), and K t \u2265 0.5(\u03ba g + 1) log 4t 2 for all t \u2208 [T ]. Then, Algorithm 1 satisfies the following.\n(I) If \u2203 (x * t , y * t (x * t )) \u2208 X \u00d7 R d 2 such that \u2207f t (x * t , y * t (x * t )) = 0 and \u03b1 t = \u03b1 \u2264 1/(2L 2 f ), then BD-Reg T \u2264 O 1 + P 1,T + Y 1,T + Y 2,T . (16) (II) If \u03b1 t = D f,0 \u221a t , then BS-Reg T \u2264 O \u221a T +\u0232 1,T +\u0232 2,T ,(17)where\u0232 p,T = T t=2 y * t\u22121 (x * ) \u2212 y * t (x * ) p is the static variant of Y p,T .\nFrom Theorem 8, we see that Algorithm 1 achieves an O(1 + P 1,T + Y 1,T + Y 2,T ) dynamic regret for a sequence of loss functions that satisfy Assumption A with only gradient feedback. This is comparable to that achieved in the full information setting provided in Theorem 1, albeit with additional conditions (vanishing gradients and Assumptions A2.-A3.). Note that the condition \u2207f t (x * t , y * t (x * t )) = 0 is referred to as the vanishing gradient condition, which is widely used in the analysis of OGD methods in the single-level convex setting [86,Assumption 2]. We also emphasize that in general Y 1,T and Y 2,T are not comparable; see Example 2.\n\n\nLocal Regret Minimization\n\nIn this section, we consider the problem of online bilevel learning with non-convex outer losses. While minimizing the regret (3) makes sense for online convex functions {f t } T t=1 , it is not appropriate for general nonconvex online costs since the global minimization of a non-convex objective is intractable in general. We address this issue with a combined approach, leveraging optimality criteria and measures from (offline) non-convex bilevel analysis, together with smoothing of the online part of the outer objective function similar to [40]. Throughout this section, we set X = R d 1 .\n\nFor the sequence {u i } w\u22121 i=1 given in Definition 2 and for all w \u2208 [T ], we define the following bilevel local regret:\nBL-Reg T,u := T t=1 \u2207F t,u (x t , y * t (x t )) 2 .(18)\nHere,\ny * t (x) \u2208 arg min y\u2208R d 2 g t (x, y), and F t,u (x t , y * t (x t )) = w\u22121 i=0 u i f t\u2212i (x t , y * t (x t )) with the con- vention f t \u2261 0 for t \u2264 0.\nNote that in the single-level setting and for u i = 1, (18) reduces to the single-level local regret [40]:\nL-Reg T,u := T t=1 \u2207F t,u (x t ) 2 .(19)\nWe also introduce a new measure of variation of the function y * t (x) as follows:\nH T := T t=2 sup x\u2208X y * t\u22121 (x) \u2212 y * t (x) 2 .(20)\nAn immediate observation is that if the functions y * t (x) are bounded, we have H T = O(T ); as such, any regret guarantee stated in terms of H T automatically translates to O(T ). The main reason that we introduce H T instead of working with a more uniform constant is to account for cases where H T is naturally small. For example, in the online hyper-parameter problem mentioned in Section 4, H T corresponds to the variability of the labels over T which can be significantly small for a good range of hyperparameters x \u2208 X . Assumption C is commonly used in the literature [40]. The following theorem shows that Algorithm 1 achieves a sublinear local regret.\n\nTheorem 9 (Non-convex). Let {(f t , g t )} T t=1 be the sequence of functions presented to Algorithm 1,\nsatisfying Assumptions A-C. If \u03b1 t = \u03b1 \u2264 1 3L f , \u03b2 t = \u03b2 = 2 g,1 +\u00b5g , and K t = K \u2265 0.5(\u03ba g + 1) log max(6c, W ) , for c = 3(1 + L 2 y M 2 f \u03b1 2 )\nand all t \u2208 [T ]. Then, Algorithm 1 guarantees\nBL-Reg T,u \u2264 O T \u03b1W + \u03b1H T W .(21)\nThe regret can be made sublinear in T if H T = O(T ) and w is selected accordingly.\n\nTheorem 9 is tightly aligned with the state-of-the-art bounds in many nonconvex optimization settings. For example, in the single-level setting (i.e., H T = 0), the results in Theorem 9 recover the results in [40], albeit for a general weight sequence. Note that in the case when u i = 1 for all i = 1, . . . , w \u2212 1, we get a local regret bound O((T + H T )/w). In the offline case f t = f , we recover a convergence guarantee for gradient methods for nonconvex BO.\n\n\nData [5]\n\nOAGD OAGD OAGD (w = T ) OAGD OAGD OAGD (w = 10) HO Table 2: Comparison of OAGD, HO [28], and bayesopt in terms of squared test error (Err te ) and CPU times (sec).\n\n\nExperimental Results\n\nIn this section, we conduct preliminary experiments to evaluate the proposed algorithm.\n\nHyper-parameter optimization (HO) is the process of finding a best set of hyper-parameter values that cannot be learned using the training data alone [28]. An HO problem can be formulated as a bilevel optimization problem; the outer objective f (y * (x); D val ) aims to minimize the validation loss with respect to the hyper-parameters x, and the inner objective g(x, y; D tr ) gives a learning algorithm by minimizing the training loss with respect to the model parameters y.\n\n\nExample 3. [Online HO for Dynamic Regression]\n\nConsider online hyper-parameter learning for dynamic regression as follows: the players (follower and leader) sequentially receive data feature vectors and then predict each label and hyper-parameters. We focus on the problem of online (regularized) regression, where at each round t, new samples\n(a t , b t ) \u2208 D t := {D val t , D tr t } for t = 1, .\n. . T are received with a t \u2208 R d 2 being the feature vector and b t \u2208 R being the corresponding label, and the potential correct decision can change abruptly. Specifically, we consider an S-stage scenario where (x * s , y * s (x * s )) are potentially best decisions for s-th stage, i.e., for all s \u2208 [S]:\nx * s \u2208 argmin x\u2208X Ts t=1 f y * s (x); D val t s.t. y * s (x) \u2208 argmin y\u2208R d 2 Ts t=1 g x, y; D tr t .(22)\nFollowing the OBO setting introduced in Section 1.1, at each round t, given a sample (a t , b t ) \u2208 D tr t , the follower is required to make the prediction by a t y t based on the learned inner and outer models (x t\u22121 , y t\u22121 ) \u2208 X \u00d7 R d 2 ; then, as a consequence the follower suffers a loss g(x t\u22121 , y t ; D tr\nt ) = 1/2(a t y t \u2212 b t ) 2 + y t C(x t\u22121 )y t , where C(x) := diag(exp(x i )) d 1 i=1 .\nThe leader then receives the feedback of the inner model, i.e., y t , predicts the new hyper-parameter x t using a validation sample (a t , b t ) \u2208 D val t , and suffers the loss f (y t (x t );\nD val t ) = 1/2(a t y t (x t ) \u2212 b t ) 2 .\nThis process repeats across T = {T 1 , . . . , T S } rounds.\n\n\nSynthetic data\n\nThe synthetic data in Section 4 are generated as follows: To simulate the distribution changes, we generate the output according to b t = a t y *  We used a grid-search of parameters in our experiments. For the grid-search setting, we select the best performing parameters K t , \u03b1, and \u03b2 from a grid {5, 10} \u00d7 {0.001, 0.01, 0.1, 0.5} \u00d7 {0.001, 0.01, 0.1, 0.5}. In our implementation, we consider LBFGS [83] to solve (10). Following [6], the smoothing (averaging) parameter \u03b3 is set to 0.9 . In the 1 -regularized case and following [67], the parameter \u00b5 is initialized as \u00b5 1 = 1 and updated at each time step by \u00b5 t+1 = min 0.99\u00b5 t , 10\u00b5 1.3 t .\ns (x * s ) + t , where (x * s , y * s (x * s )) \u2208 R d 1 \u00d7 R d 2\nAll algorithms have been run on a Mac machine equipped with a 1.8 GHz Intel Core i5 processor and 8 GB RAM. Figure 1 demonstrates the variable variation P 2,T + Y 2,T and regret bound of OAGD with three different window size w \u2208 {1, 100, T } on the synthetic data sets. We observe that OAGD with w = T performs the best, and there is a gradual decrease in performance to w = 100 and w = 1. In addition, by increasing the number of optimal points (left figure), the algorithm restarts more often and yields larger regret. Finally, Figure 2 shows (on a 1-dimensional hyperparamter setting) that the performance of OAGD is comparable to the performance of the offline HO algorithm [28]. \n\n\nReal data\n\nThe real data sets in Section 4 are taken from UCI machine learning repository We consider an smoothing variant of the elastic net problem [67], i.e., the setting where the inner objective in (22) is replaced with g x, y; D tr\nt = 1 2 (a t y \u2212 b t ) 2 + y C(x)y + d 2 i=1 exp(x i )(y 2 i + \u00b5 2 ) 1 2 , wherex = (x d 2 +1\n, \u00b7 \u00b7 \u00b7 , x d 1 ) and \u00b5 > 0 is a smoothing parameter. For the sake of comparison, we also implement a hyper-parameter selection method that uses Bayesian optimization. We use bayesopt in MATLAB with \"MaxObjectiveEvaluations=30\" for Bayesian optimization. At each iteration of bayesopt, we make use of MATLAB built-in solver fmincon. Table 2 summarizes the obtained results of applying the two types of Algorithms 1, HO [28] and bayesopt to (22). The algorithms with \"500\" seconds stopped at the time-limit. Moreover, OAGD (w =T ) stands for Algorithm 1 using the time averaged gradient approach with the window size w =T and HO does the one using fmincon. The hyphens \"-\" in the line of Facebook for HO indicate that it terminates with an infeasible solution of the averaged problem (22). One can see that OAGD (w = T ) is stable against new data compared to OAGD (w = 10), and computationally efficient compared to HO [28] and bayesopt.\n\n\nConclusion\n\nThis paper studies online bilevel optimization and provides regret guarantees under different convexity assumptions on the time-varying objective functions. In particular, we propose a new class of online bilevel algorithms that are capable of leveraging smoothness and provide regret bound in terms of problem-dependent quantities such as the path-length of the comparator sequence. A limitation of our work is that we consider the local optimal follower, i.e. y * t (x) \u2208 arg min y\u2208R d 2 g t (x, y), which can be pessimistic in some applications. One direction for future work is to generalize our analysis to the setting where y * (x) \u2208 arg min y\u2208R d 2 T t=1 g t (x, y). [5] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.\n\n[6] Sergul Aydore, Tianhao Zhu, and Dean P Foster. Dynamic local regret for non-convex online forecasting. Advances in Neural Information Processing Systems, 32, 2019.\n\n[7] Dheeraj Baby and Yu-Xiang Wang. Online forecasting of total-variation-bounded sequences. arXiv preprint arXiv:1906.03364, 2019.\n\n[8] Juhan Bae and Roger B Grosse. Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians. Advances in Neural Information Processing Systems, 33:21725-21737, 2020.\n\n[9] Jonathan F Bard. \n\n\nA Related Work\n\nOnline learning and stochastic optimization are closely related. The key difference between them is that at each round t of the online optimization the loss function can be arbitrarily chosen by the adversary. Given the vastness of the online and stochastic optimization, we do not strive to provide an exhaustive review. Instead, we mainly focus on a few representative works on online static and worst-case dynamic regret minimization, as well as bilevel optimization; see [45,36] and [74,58] for survey of online and bilevel optimization, respectively.\n\n\u2022 Static Regret Minimization: In single-level online optimization, the goal of the player (learner) is to choose a sequence {x t } T t=1 such that her regret is minimized. There are different notions of regret in the literature including static, dynamic (defined in (1)), and adaptive [37, 71,72]. In the case of static regret, x * t is replaced by x * \u2208 arg min x\u2208X T t=1 f t (x). This type of regret is well-studied in the literature of online learning [37, 71,72]. [98] shows that online gradient descent (OGD) provides a O( \u221a T ) regret bound for convex (possibly nonsmooth) functions.\n\n[38] improves this bound to O(d 1 log T ), and O(log T ) for exponentially concave and strongly-convex functions, respectively. These results were also shown to be minimax optimal [1]. [97] provides regret bounds for online learning algorithms under relative Lipschitz and/or relative strongly-convexity assumptions.\n\nIn addition to exploiting convexity of the online functions, there are recent studies improving static regret by incorporating smoothness [75,19]. These problem-dependent bounds can safeguard the worst-case minimax rate yet be much better in easy cases of online learning problems (e.g., loss functions with a small deviation). For instance, [75] shows that for convex smooth non-negative functions, OGD can achieve an O(\n\u221a F T ) small-loss regret bound, where F T = T t=1 f t (x ) and x * \u2208 arg min x\u2208X T t=1 f t (x). For convex smooth functions, [19] establishes an O( \u221a G T ) bound, where G T = T t=2 sup x\u2208X \u2207f t\u22121 (x) \u2212 \u2207f t (x) 2\nis the gradient variation. These bounds are particularly favored in slowly changing environments in which the online functions evolve gradually [94].\n\n\u2022 Dynamic Regret Minimization: Single-level dynamic regret forces the player to compete with time-varying comparators, and thus is particularly favored in non-stationary environments [78]. The notion of dynamic regret is also referred to as tracking regret or shifting regret in the prediction with expert advice setting [43,44,12,82,95]. There are two kinds of dynamic regret in previous studies: The universal dynamic regret aims to compare with any feasible comparator sequence [98,89,94], while the worst-case dynamic regret (defined in (1)) specifies the comparator sequence to be the sequence of minimizers of online functions [11,48,62,86,90,6]. We present related works for the latter case as it is the setting studied in this paper.\n\nIt is known that in the worst case, sublinear dynamic regret is not attainable unless one imposes regularity of some form on the comparator sequence or the function sequence [11,32,48]. [86] shows that OGD enjoys an O( T P 1,T ) worst-case dynamic regret bound for convex functions when the path-length P 1,T is known. For strongly convex and smooth functions, [62] shows that an O(P 1,T ) dynamic regret bound is achievable.\n\n[15] proves that OGD can achieve an O(P 2,T ) regret bound without the bounded gradient assumption. [90] further proposes the online multiple gradient descent algorithm and proves that the algorithm enjoys an O(min{P 1,T , P 2,T }) regret bound; this bound has been recently enhanced to O(min{P 1,T , P 2,T , V T }) by an improved analysis [93], where\nV T = T t=2 sup x\u2208X |f t\u22121 (x) \u2212 f t (x)|\n. [86] further shows that O(P 2,T ) rate is attainable for convex and smooth functions, provided that all the minimizers x t lie in the interior of the domain X . The above results use the path-length (or squared path-length) as the regularity, which is in terms of the trajectory of the comparator sequence. [64, 65] extend the above results to the distributed settings and provide dynamic regret bounds in terms of the 1 path-length. [11] shows that OGD with a restarting strategy attains an O(T 2/3 V T 1/3 ) regret for convex functions when V T is available, which has been recently improved to O(T 1/3 V T 2/3 ) for the square loss [7]. \u2022 Adaptive Regret: Adaptive regret [39, 21,91,87,88] is also used to capture the dynamics in the environment. Specifically, it characterizes a local version of static regret, where\nRegret T ([r, s]) s t=r f t (x t ) \u2212 min x\u2208X s t=r f t (x), for each interval [r, s] \u2286 [T ].\n[91] provides a connection between strongly adaptive regret and dynamic regret and proposes an adaptive algorithm which can bound the dynamic regret without prior knowledge of the functional variation. [88] develops a new algorithm which can minimize the dynamic regret and the adaptive regret simultaneously.\n\n\u2022 Local Regret Minimization: Nonconvex online optimization is a more challenging setting than the convex case. Some notable works in the nonconvex literature include adversarial multi-armed bandit with a continuum of arms [14, 53, 55, 42, 41] and classical Follow-the-Perturbed-Leader algorithm with access to an offline non-convex optimization oracle [2,77,54].\n\n[40] introduces a local regret measure based on gradients of the loss to address intractable non-convex online models. Their regret is local in the sense that it averages a sliding window of gradients and quantifies the objective of predicting points with small gradients on average. They are motivated by a game-theoretic perspective, where an adversary reveals observations from an unknown static loss. The gradients of the loss functions from the w most recent rounds of play are evaluated at the current model parameters x t , and these gradients are then averaged. The motivation behind averaging is two-fold: (i) a randomly selected update has a small time-averaged gradient in expectation if an algorithm incurs local regret sublinear in T , and (ii) for any online algorithm, an adversarial sequence of loss functions can force the local regret incurred to scale with T as O(T /w 2 ). [33] extends the local regret minimization to online, non-smooth, non-convex problems. These arguments presented in [40, 6, 65, 33] inspire our use of local regret for bilevel online optimization.\n\n\u2022 (Offline) Bilevel Optimization: Since its first formulation by Stackelberg [76] and the first mathematical model by Bracken and McGill [13] there has been a significant growth in applications and developments of bilevel programming. Existing works either reduce the problem into a single-level optimization problem [3,23,4,34,73,60,63,74] or apply an (alternating) optimization method to solve the original problem. The single-level formulations (using the Karush-Kuhn-Tucker (KKT) conditions or penalty approaches) are generally difficult to solve [74].\n\nGradient-based approaches are more attractive for bilevel programming due to their simplicity and effectiveness. This type of approach estimates the hypergradient \u2207\u03c6(x) for iterative updates, and can generally be divided into two categories: approximate implicit differentiation (AID) and iterative differentiation (ITD) classes. ITD-based approaches [61, 27,25,30] estimate the hypergradient \u2207\u03c6(x) in either a reverse (automatic differentiation) or forward manner. AID-based approaches [22,68,30, 51] estimate the hypergradient via implicit differentiation. [28] characterized the asymptotic convergence of a backpropagation-based approach as one of ITD-based algorithms by assuming the inner-level problem is strongly convex. [70] provided a similar analysis for a truncated backpropagation scheme. [59, 56] analyzed the asymptotic performance of ITD-based approaches when the inner-level problem is convex.\n\nFinite-time complexity analysis for bilevel optimization has also been explored. [29] provided a finite-time convergence analysis for an AID-based algorithm under different loss geometries: \u03c6(\u00b7) is strongly convex, convex or nonconvex, and g(x, \u00b7) is strongly convex.\n\n[51] provided an improved finite-time analysis for both AID-and ITD-based algorithms under the nonconvex-strongly-convex geometry.\n\n[49] provided the lower bounds on complexity as well as upper bounds under these two geometries. When the objective functions can be expressed in an expected or finite-time form, [29,51,46] developed stochastic bilevel algorithms and provided the finite-time analysis. There have been subsequent studies on accelerating SGD-type bilevel optimization via momentum and variance reduction techniques [18, 31, 50, 47] as well. However, a fundamental assumption in all the aforementioned works is that the cost function does not change throughout the horizon over which we seek to optimize it. Leader's optimal decision at round t y * t (x) Follower's optimal decision at round t for a given x \u2207ht, \u2207 2 xy ht, \u2207 2 y ht Gradient, Jacobian, and Hessian of ht D\n\nThe (2-norm) diameter of X :\nD = max x,x \u2208X x \u2212 x M\nUpper bound on the outer function: |ft| \u2264 M M f Difference between each\u2207ft(x, yt) and \u2207ft(x,\ny * t (x)) w.r.t. y t (x) \u2212 yt Ly Lipschitz constant of y t (x) L f Lipschitz constant of \u2207ft(x) FT\nOuter function value at the optimum:\nT t=1 ft(x t , y t (x t )) Pp,T\nPath-length of the outer minimizers:\nT t=2 x t\u22121 \u2212 x t p\nYp,T Path-length of the inner minimizers:\nT t=2 y t\u22121 (x t\u22121 ) \u2212 y t (x t ) p HT\nInner minimizer funcation variation:\nT t=2 sup x\u2208X y t\u22121 (x) \u2212 y t (x) 2 VT\nOnline functions variation:\nT t=2 sup x\u2208X |ft\u22121(x) \u2212 ft(x)| GT\nOnline gradients variation: Proof. Denote by x * 0 = x 1 and y * 0 (x * 0 ) = y 1 . Then, it follows from Assumption A1. that\nT t=2 sup x\u2208X \u2207ft\u22121(x) \u2212 \u2207ft(x) 2 D-Reg T (single-level) dynamic regret: T t=1 ft(xt) \u2212 T t=1 ft(x t ) S-Reg T (single-level) static regret: T t=1 ft(xt) \u2212 minx\u2208X T t=1 ft(x) L-Reg T (single-T t=1 f t (x t , y t ) \u2212 T t=1 f t (x * t , y * t (x * t )) = T t=1 f t (x * t\u22121 , y * t\u22121 (x * t\u22121 )) \u2212 T t=1 f t (x * t , y * t (x * t )) \u2264 f,0 T t=1 x * t\u22121 \u2212 x * t + y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) = f,0 x 1 \u2212 x * 1 + f,0 T \u22121 t=1 x * t \u2212 x * t+1 + f,0 y * 1 (x 1 ) \u2212 y * 1 (x * 1 ) + f,0 T \u22121 t=1 y * t (x * t ) \u2212 y * t+1 (x * t+1 ) \u2264 f,0 D + f,0 L y D + f,0 (P 1,T + Y 1,T ),\nwhere the last inequality uses Lemma 4 and Assumption B. This completes the proof.\n\n\nC Lower Bound\n\n\nC.1 Proof of Theorem 7\n\nProof. We randomly generate a sequence of functions {(f t , g t )} T t=1 and show that there exists a distribution of online functions such that for any bilevel algorithm A, we have E [BD-Reg T ] \u2265 E[P p,T + Y p,T ]. Specifically, for any bilevel algorithm A that generates a sequence of {(x t , y t )} T t=1 , we consider the expected regret as follows:\nE [BD-Reg T ] = E T t=1 f t (x t , y t ) \u2212 T t=1 f t (x t , y t (x t )) . Let d 1 = d 2 = 1.\nFor each round t, we randomly sample vectors a \nf t (x, y) = 4 y \u2212 a (1) t + a (2) t 2 + 4 x \u2212 a (1) t 2 , and g t (x, y) = 1 2 y 2 \u2212 x + a (2) t y.\nIt follows from (3b) that\ny * t (x t ) = x t + a (2) t , x * t = a (1) t , and y * t (x * t ) = a (1) t + a(2)\nt . Hence,\nE [BD-Reg T ] = 4 T t=1 E y t \u2212 a (1) t + a (2) t 2 + E x t \u2212 a (1) t 2 \u2265 4 T t=1 2E a (1) t 2 + E a (2) t 2 \u2265 8( 1 T + log T ) + 4T. (A.1)\nHere, the fist inequality follows from the independence of a (1) t and a (2) t ; and the last inequality uses Lemma 16(I).\n\nIf\nT = 1, then E[P p,T + Y p,T ] = E[(a(1)1 ) 2 ] + E[(a (1) 1 + a(2)1 ) 2 ] = 3. For T \u2265 2, we obtain E [P p,T + Y p,T ] = T t=2 E a (1) t \u2212 a (1) t\u22121 2 + T t=2 E a (1) t + a (2) t \u2212 (a (1) t\u22121 + a (2) t\u22121 ) 2 = T t=2 2 E a (1) t 2 + E a (1) t\u22121 2 + E a (2) t 2 + E a (2) t\u22121 2 \u2264 2(1 + log(T \u2212 1) + log T ) + 2(T \u2212 1). (A.2)\nHere, the second equality follows from the independence of a \n\n\nD Proof for Convex OBO with Partial Information\n\nIn this section, we provide dynamic regret bound for OBO with partial information. Specifically, we derive a problem-dependent regret bound for Algorithm 1. We first provide some supporting lemmas.\n\n\nD.1 Auxiliary Lemma\n\nThe following lemma characterizes the inner estimation error y t+1 \u2212 y * t (x t ) , where y t+1 is the inner variable update via Algorithm 1. It shows that by applying inner gradient descent multiple times at each round t, we are able to extract more information from each inner function and therefore are more likely to obtain a tight bound for the inner error in terms of the path-length Y p,T . We should emphasize that Algorithm 1, though presented with an outer loop and an inner loop, reduces to a single-loop method when the parameter K t is set to 1. Allowing K t to be greater than 1 leads to better generality and better performance in both convex and nonconvex settings.\n\nLemma 10. Suppose Assumption A holds. In Algorithm 1, choose \u03b2 = 2 g,1 + \u00b5 g\nand K t = (\u03ba g + 1) log \u03c1 \u22122 t 2 (A.3)\nfor some positive deceasing sequence {\u03c1 t } T t=1 . Then, for the sequence {y t } T +1 t=1 generated by Algorithm 1:\nL1. If \u03c1 1 < 1/2, we have T t=1 y t+1 \u2212 y * t (x t ) 2 \u2264 \u03c1 2 1 1 \u2212 2\u03c1 2 1 y 1 \u2212 y * 1 (x 1 ) 2 + 6 1 \u2212 2\u03c1 2 1 L 2 y T t=1 \u03c1 2 t x t \u2212 x * t 2 + T t=2 \u03c1 2 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) 2 . L2. If \u03c1 1 < 1, we get T t=1 y t+1 \u2212 y * t (x t ) \u2264 \u03c1 1 1 \u2212 \u03c1 1 y 1 \u2212 y * 1 (x 1 ) + 1 1 \u2212 \u03c1 1 L y T t=1 \u03c1 t x t \u2212 x * t + T t=2 \u03c1 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) .\nwhich implies that\nT t=2 \u03c1 2 t y t \u2212 y * t (x t ) 2 \u2264 2 T t=2 \u03c1 2 t y t \u2212 y * t\u22121 (x t\u22121 ) 2 + y * t\u22121 (x t\u22121 ) \u2212 y * t (x t ) 2 \u2264 2 T t=1 \u03c1 2 t y t+1 \u2212 y * t (x t ) 2 + 2 T t=2 \u03c1 2 t y * t\u22121 (x t\u22121 ) \u2212 y * t (x t ) 2 .\n(A.8)\n\nIt follows from Lemma 15 that\n\u03c1 2 t y * t\u22121 (x t\u22121 ) \u2212 y * t (x t ) 2 \u2264 3\u03c1 2 t y * t (x t ) \u2212 y * t (x * t ) 2 + 3\u03c1 2 t y * t\u22121 (x t\u22121 ) \u2212 y * t\u22121 (x * t\u22121 ) 2 + 3\u03c1 2 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) 2 \u2264 3L 2 y \u03c1 2 t\u22121 x t\u22121 \u2212 x * t\u22121 2 + 3L 2 y \u03c1 2 t x t \u2212 x * t 2 + 3\u03c1 2 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) 2 , (A.9)\nwhere the second inequality uses the assumption that \u03c1 t \u2264 \u03c1 t\u22121 for all t \u2208 [T ]. Now, combining (A.8), (A.7), and (A.9), we obtain\nT t=1 1 \u2212 2\u03c1 2 t y t+1 \u2212 y * t (x t ) 2 \u2264 \u03c1 2 1 y 1 \u2212 y * 1 (x 1 ) 2 + 6L 2 y T t=1 \u03c1 2 t x t \u2212 x * t 2 + 6 T t=2 \u03c1 2 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) 2 , (A.10)\nwhich together with our assumption that \u03c1 t \u2264 \u03c1 t\u22121 completes the proof. \u00b5 g + f,0 \u00b5 g g,2 + g,1 g,2\n\u00b5 g = O(\u03ba 2 g ), L f := f,1 + g,1 ( f,1 + M f ) \u00b5 g + f,0 \u00b5 g g,2 + g,1 g,2 \u00b5 g = O(\u03ba 3 g ),\nwhere the other constants are defined in Assumption A. Proof. Recall the update rule of Algorithm 1 (with w = 1): x t+1 = \u03a0 X x t \u2212 \u03b1\u2207f t (x t , y t+1 ) . From the Pythagorean theorem, we get \nx t+1 \u2212 x * t 2 \u2264 \u03a0 X (x t \u2212 \u03b1\u2207f t (x t , y t+1 )) \u2212 x * t ) 2 = x t \u2212 x * t 2 \u2212 2\u03b1\u2207f t (x t , y t+1 ) (x t \u2212 x * t ) + \u03b1 2 \u2207 f t (x t ,y1 c \u2212 1 , c 2 (\u03b1) := \u03b1DL f , c 3 (\u03b1) := \u03b1 4 , c 4 (\u03b1) := 1 \u2212 c 2\u03b1 + 1 2\u03b1 1 c \u2212 1 D 2 + \u03b1 4 2L 2 y D 2 + y 1 \u2212 y * 1 (x 1 ) 2 + y * T (x * T ) \u2212 y * T +1 (x * T +1 ) 2 .\n(A.23)\n\nThen,\nBD-Reg T \u2264 c 1 (\u03b1)P 2,T + c 2 (\u03b1) T t=1 \u2207f t (x *\nFrom (A.28) and (A.29), we get\n1 \u03b1 t + c \u2212 \u00b5 f x t \u2212 x * 2 \u2212 1 \u03b1 t x t+1 \u2212 x * 2 + M 2 f \u03b1 t + 1 c y t+1 \u2212 y * t (x t ) 2 . (A.30) Note that \u03c1 \u2264 1 12L 2 y M 2 f (\u03b1 1 + 1 c ) + 2c 1 2 implies 2L 2 y M 2 f (\u03b1 1 + 1 c ) 6\u03c1 2 1 \u2212 2\u03c1 2 \u2264 c. (A.31)\nThen,\nM 2 f \u03b1 t + 1 c T t=1 y t+1 \u2212 y * t (x t ) 2 \u2264M 2 f \u03b1 1 + 1 c 6\u03c1 2 1 \u2212 2\u03c1 2 y 1 \u2212 y * 1 (x 1 ) 2 + 2L 2 y T t=1 x t \u2212 x * t 2 +2L 2 y x T +1 \u2212 x * T +1 2 + Y 2,T + y * T (x * T ) \u2212 y * T +1 (x * T +1 ) \u2264 c T t=1 x t \u2212 x * t 2 + c 2L 2 y Y 2,T + + c 2L 2 y y 1 \u2212 y * 1 (x 1 ) 2 + x T +1 \u2212 x * T +1 2 + c L 2 y y * T (x * ) \u2212 y * T +1 (x * ) . (A.32)\nSumming from t = 1 to T , setting c = \u00b5 f /4, and \u03b1 t = 2 \u00b5 f t (define 1 \u03b1 0 := 0), we have\n2 T t=1 f t (x t , y * t (x t )) \u2212 f t (x * , y * t (x * )) \u2264 T t=1 x t \u2212 x * 2 1 \u03b1 t \u2212 1 \u03b1 t\u22121 + c \u2212 \u00b5 f 2 + T t=1 \u03b1 t \u2207f t (x t , y * t (x t )) 2 + M 2 f T t=1 y t+1 \u2212 y * t (x t ) 2 \u03b1 t + 1 c \u2264 2 f,0 T t=1 \u03b1 t + T t=1 x t \u2212 x * 2 1 \u03b1 t \u2212 1 \u03b1 t\u22121 + 2c \u2212 \u00b5 f 2 + c 2L 2 y Y 2,T + c 2L 2 y y 1 \u2212 y * 1 (x 1 ) 2 + x T +1 \u2212 x * T +1 2 + c L 2 y y * T (x * ) \u2212 y * T +1 (x * ) . (A.33)\nLet This together with Lemma 10 gives\ne 1 := \u00b5 f 8L 2 y y 1 \u2212 y * 1 (x 1 ) 2 + y * T (x * ) \u2212 y * T +1 (x * ) + 1 2 D 2 + 2 f,0 \u00b5 f , e 2 := \u00b5 f8LT t=1 y t+1 \u2212 y * t (x t ) 2 \u2264 \u03c1 2 1 1 \u2212 2\u03c1 2 1 y 1 \u2212 y * 1 (x 1 ) 2 + 6 1 \u2212 2\u03c1 2 1 L 2 y T t=1 \u03c1 2 t x t \u2212 x * t 2 + T t=2 \u03c1 2 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) 2 \u2264 y 1 \u2212 y * 1 (x 1 ) 2 + \u03c0 4 60 L 2 y D 2 + \u03c0 4 60 L 2 y Y 2,T . (A.43a) Similarly, we obtain T t=1 y t+1 \u2212 y * t (x t ) \u2264 \u03c1 1 1 \u2212 \u03c1 1 y 1 \u2212 y * 1 (x 1 ) + 1 1 \u2212 \u03c1 1 L y T t=1 \u03c1 t x t \u2212 x * t + T t=2 \u03c1 t y * t\u22121 (x * t\u22121 ) \u2212 y * t (x * t ) \u2264 y 1 \u2212 y * 1 (x 1 ) + \u03c0 2 2 L y D + \u03c0 2 2 L y Y 1,T . (A.43b) Now, let\u0116 1 (\u03b1) := \u03b1M 2 f 2 y 1 \u2212 y * 1 (x 1 ) 2 + \u03c0 4 60 L 2 y D 2 + DM f y 1 \u2212 y * 1 (x 1 ) + \u03c0 2 2 L y D .\nSubstituting (A.43)-(A.43b) into (A.41), we have\n(1 \u2212 \u03b1L f ) T t=1 f t (x t , y * t (x t )) \u2212 f t (x * t , y * t (x * t )) \u2264\u0117 1 + D 2 2\u03b1 + D \u03b1 P 1,T + \u03b1M 2 f 2 \u03c0 4 60 L 2 y Y 2,T + DM f \u03c0 2 2 L y Y 1,T . (A.44) Let \u03b1 \u2264 1/(2L f ) an\u1e0b c 1 (\u03b1) := 1 1 \u2212 \u03b1L f\u0116 1 (\u03b1), c 2 (\u03b1) := 1 1 \u2212 \u03b1L f \u03b1M 2 f 2 \u03c0 4 60 L 2 y , c 3 (\u03b1) := 1 1 \u2212 \u03b1L f DM f \u03c0 2 2 L y , c 4 (\u03b1) := D 2 2(1 \u2212 \u03b1L f )\u03b1 + \u03b1L 2 f 4M f y 1 \u2212 y * 1 (x 1 ) 2 + DM f y 1 \u2212 y * 1 (x 1 ) . (A.45)\nThe above definitions together with (A.44) implies BD-Reg T \u2264\u010b 1 (\u03b1)P 1,T +\u010b 2 (\u03b1)Y 1,T +\u010b 3 (\u03b1)Y 2,T +\u010b 4 (\u03b1).\n\n\n(A.46)\n\nThis gives the desired result in (16).\n\n\nD.4.2 Proof of Theorem 8 (II)\n\nProof. From the convexity of f t and following Steps (A.36)-(A.37), we have\nf t (x t , y * t (x t )) \u2212 f t (x * , y * t (x * )) \u2264 \u03b1 t 2 \u2207f t (x t , y * t (x t )) 2 + x t \u2212 x * 2 \u2212 x t+1 \u2212 x * 2 2\u03b1 t + \u03b1 t M 2 f 2 y t+1 \u2212 y * t (x t ) 2 + DM f y t+1 \u2212 y * t (x t ) .\n(A.47)\n\nSumming from t = 1 to T , setting \u03b1 t = D f,0 \u221a t (define 1 \u03b1 0 := 0), we have D.5 Discussion on the number of inner iterations K t and the window size w:\n2f t (x t , y * t (x t )) \u2212 f t (x * , y * t (x * )) \u2264 T t=1 x t \u2212 x * 2 1 \u03b1 t \u2212 1 \u03b1 t\u22121 + T t=1 \u03b1 t \u2207f t (x t , y * t (x t )) 2 + T t=1 \u03b1 t M 2 f 2 y t+1 \u2212 y * t (x t ) 2 + DM f y t+1 \u2212 y * t (x t ) \u2264 D 2 \u03b1 T + T t=1 2 f,0 \u03b1 t + \u03b1 t M 2 f 2 y t+1 \u2212 y * t (x t ) 2 + DM f y t+1 \u2212 y * t (x t ) \u2264 3D f,0 \u221a T +\u0116 1 (\u03b1) + \u03b1 1 M 2 f 2 \u03c0 4 60 L 2 y Y 2,T + DM f \u03c0 2 2 L y Y 1,\nAs mentioned before, by using inner gradient descent multiple times, we are able to get more information from each inner function and obtain a tight bound for the dynamic regret in terms of Y p,T . However, according to our analysis in Theorem 5 and Theorem 8, even for sufficiently large K t and w > 1, the dynamic regret bound can only be improved by a constant factor. In fact, by comparing (A.21) and (A.42), we see that K t = 0.5(\u03ba g + 1) log \u03c1 \u22122 t with \u03c1 t = O(1/t 2 ) and O(1) give similar regret bound in terms of O(Y 2,T ); albeit with different constants. A related question is whether we can reduce the value of K t by using for example smoothness of \u2207y t (x) similar to the offline bilevel optimization [17] or adopting more advanced optimization techniques, such as the acceleration or momentum-type gradient methods for both inner and outer updates [66]. These are open problems to us, and will be investigated as a future work.\n\n\nE Proof for Nonconvex OBO with Partial Information\n\nThis section gives regret bounds for online bilevel learning in the non-convex setting.\n\n\nE.1 Auxiliary Lemma\n\nLemma 12. Under Assumption A, for all t \u2208 [T ], we have \u2207 F t,u (x, y t+1 ) \u2212 \u2207F t,u (x, y * t (x)) 2 \u2264 M 2 f y t+1 \u2212 y * t (x) 2 , (A.51)\n\nwhere\u2207F t,u is defined in (10b) and M f is given in (??).\n\nLet c = 3(1 + 2L 2 y M 2 f \u03b1 2 ). From the above inequality, we get Hence,\n1 \u2212 c\u03c1 2 T t=2 y t \u2212 y * t (x t ) 2 \u2264 c\u03c1 2 y 1 \u2212 y * 1 (x 1 ) 2 + 6L 2 y \u03b1 2 T t=2\n\u2207F t,u (x t , y * t (x t )) 2 + 3H T .\n\n(A.61) By our assumption 1 \u2212 c\u03c1 2 > 0. Now, dividing both side of the above inequality by 1 \u2212 c\u03c1 2 and combining with (A.55) completes the proof.\n\nThe following lemma shows that the difference between the time-averaged function F t,u computed at (x t , y * t (x t )) and (y * t+1 (x t+1 )) is bounded. This is an extension of the single-level setting to the generic weight sequence {u i } w\u22121 i=0 and the proof uses the ideas of [6, Lemmas 3.2, 3.3] and [40, Theorem 3].\n\n\nLemma 14.\n\nLet {(f t , g t )} T t=1 be the sequence of functions presented to Algorithm 1, satisfying Assumption C. Then, we have\nT t=1 F t,u (x t , y * t (x t )) \u2212 F t,u (x t+1 , y * t+1 (x t+1 )) \u2264 M (2T + 1) W + M.\nProof. Observe that\nF t+1,u (x t+1 , y * t+1 (x t+1 )) \u2212 F t,u (x t+1 , y * t+1 (x t+1 )) = 1 W w\u22121 i=0 u i f t+1\u2212i (x t+1 , y * t+1 (x t+1 )) \u2212 f t\u2212i (x t+1 , y * t+1 (x t+1 )) = u 0 W f t+1 (x t+1 , y * t+1 (x t+1 )) \u2212 u w\u22121 W f t\u2212w+1 (x t+1 , y * t+1 (x t+1 )) + 1 W w\u22121 i=1 [u i \u2212 u i\u22121 ] f t\u2212i+1 (x t+1 , y * t+1 (x t+1 )), (A.62)\nwhere {u i } w\u22121 i=0 is the weight sequence with 1 = u 0 \u2265 u 1 . . . u w\u22121 > 0, given in Definition 2. Combine Assumption C with (A.62) to get\nF t+1,u (x t+1 , y * t+1 (x t+1 )) \u2212 F t,u (x t+1 , y * t+1 (x t+1 )) \u2264 M (u 0 + u w\u22121 ) W + M w\u22121 i=1 [u i\u22121 \u2212 u i ] W \u2264 2M W . (A.63)\nSimilarly, using Assumption C, we obtain T t=1 F t,u (x t , y * t (x t )) \u2212 F t+1,u (x t+1 , y * t+1 (x t+1 )) \u2264 This completes the proof.\n\n\nF Useful Results and Facts\n\nIn this part we present several technical lemmas used in the proofs. We start by assembling some well-known facts about convex and smooth functions. \n\nDefinition 2 (\n2Time-Averaged Hypergradient). Given a window size w \u2208 [T ], let {u i } w\u22121 i=0 be a positive decreasing sequence with u 0\n\n\nAssumption C. For any t \u2208 [T ], |f t (x, y)| \u2264 M for some finite constant M > 0.\n\n\nis the underlying model for s-th stage, and t \u2208 [0, 0.1] is the random noise. We consider two setups for the underlying model: (i) there are three changes (S = 3) in the minimizers (x * s , y * s (x * s )), and (ii) the underlying model is fixed (S = 1), i.e., (x * , y * (x * )) = (x * s , y * s (x * s )) for all t \u2208 [T ]. The time horizon, the outer and inner dimensions are set to T = 5000, d 1 = 1 and d 2 = 5, respectively.\n\nFigure 1 :\n1Performance of OAGD on online hyper-parameter learning. OBO with (left) three comparators and (right) fixed comparator.\n\nFigure 2 :\n2Performance of OAGD and HO for learning hyper-parameter x1.\n\n\n[5]: Facebook Comment Volume (m = 40949, d 2 = 53), Insurance Company Benchmark (m = 9000, d 2 = 85), Student Performance for a math exam (m = 395, d 2 = 32), and BodyFat (m = 336, d 2 = 14). The m samples are divided into 3 groups (training, validation and test samples) with the same sample size m/3 . Hence, T {val,tr,te} = m/3 .\n\n[ 4 ]\n4Faiz A Al-Khayyal, Reiner Horst, and Panos M Pardalos. Global optimization of concave functions subject to quadratic constraints: an application in nonlinear bilevel programming. Annals of Operations Research, 34(1):125-147, 1992.\n\n\n(xt, yt) \u2212 T t=1 ft(x t , y t (x t )) BS-Reg T Bilevel (outer) static regret: T t=1 ft(xt, yt) \u2212 minx\u2208X T t=1 ft(x, y t (x)) BL-Reg T Bilevel local regret: T t=1 \u2207Ft,u(xt, y * t (xt)) 2 B Proof for OBO with Full Information B.1 Proof of Theorem 1\n\n\nand N (0, 1), respectively. For t = 1, . . . , T , let\n\n\nt \u2208 [T ]; and the inequality uses Lemma 16(I). Now, it follows from (A.1) and (A.2) that for any given algorithm A, there exists a sequence of functions {(f t , g t )} T t=1 such that E [BD-Reg T ] \u2265 E[P p,T + Y p,T ].\n\n\nReg T \u2264\u0117 1 \u221a T + +\u0117 2 Y 1,T +\u0117 3 Y 2,T +\u0117 4 . (A.50)This gives (17).\n\n\nA.64) over t \u2208 [T ] and combining with (A.63) completes the proof.\n\n\n(F1) (Smoothness): Suppose f (x) is L-smooth.Then, by definition, the following inequalities hold for any two points x, y \u2208 R d :\u2207f (x) \u2212 \u2207f (y) \u2264 L x \u2212 y , (A.72a) f (y) \u2212 f (x) \u2264 y \u2212 x, \u2207f (x) + L 2 y \u2212 x 2 . (A.72b) Further, if x * \u2208 arg min x\u2208R d f (x), then \u2207f (y) 2 \u2264 2L(f (y) \u2212 f (x * )). (A.72c) (F2) (Smoothness and Convexity): Suppose f (x)is L-smooth and convex. Then, the following holds for any two points x, y \u2208 R d :\u2207f (y) \u2212 \u2207f (x), y \u2212 x \u2265 1 L \u2207f (y) \u2212 \u2207f (x) 2 . (A.73) (F3) (Strong-Convexity): Suppose f (x) is \u00b5-strongly convex.Then, by definition, the following inequality holds for any two points x, y \u2208 R d :f (y) \u2212 f (x) \u2265 y \u2212 x, \u2207f (x) + \u00b5 2 y \u2212 x 2 . (A.74a)Using the above inequality, one can conclude that\u2207f (y) \u2212 \u2207f (x), y \u2212 x \u2265 \u00b5 y \u2212 x 2 . (A.74b)Lemma 15. For any set of vectors {x i } m i=1 with x i \u2208 R d 2 .(A.75) \n\nTable 1 :\n1Comparison with the prior works on regret minimization for strongly convex (SC), convex(C), and non-convex (NC) online learning. Here, w denotes the window size ( see, Definition 2). Note that[86] requires additional vanishing gradient assumption to get O(1 + P 1,T ).\n\n\nS\u00e9bastien Bubeck, Gilles Stoltz, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Online optimization in x-armed bandits. Advances in Neural Information Processing Systems, 21, 2008. [15] Ting-Jui Chang and Shahin Shahrampour. On online optimization: Dynamic regret analysis of strongly convex and smooth problems. In Proceedings of the AAAI Conference on Artificial Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. Advances in Neural Information Processing Systems, 34, 2021. [53] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural Information Processing Systems, 17, 2004. Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. arXiv preprint arXiv:2101.11517, 2021. [59] Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton. Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D.1 Auxiliary Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 D.2 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D.3 Proof of Corollary 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.4 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.5 Discussion on the number of inner iterations K t and the window size w: . . . . . . 35 E Proof for Nonconvex OBO with Partial Information 35 E.1 Auxiliary Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.2 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39Practical bilevel optimization: algorithms and applications, volume 30. \nSpringer Science & Business Media, 2013. \n\n[10] Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with \ndifferentiable closed-form solvers. In International Conference on Learning Representations, \n2018. \n\n[11] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations \nresearch, 63(5):1227-1244, 2015. \n\n[12] Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past \nposteriors. Journal of Machine Learning Research, 3(Nov):363-396, 2002. \n\n[13] Jerome Bracken and James T McGill. Mathematical programs with optimization problems in \nthe constraints. Operations Research, 21(1):37-44, 1973. \n\n[14] Intelligence, volume 35, pages 6966-6973, 2021. \n\n[16] Tianyi Chen, Yuejiao Sun, Quan Xiao, and Wotao Yin. A single-timescale method for stochastic \nbilevel optimization. In International Conference on Artificial Intelligence and Statistics, pages \n2466-2488. PMLR, 2022. \n\n[17] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating \nstochastic gradient methods for bilevel problems. Advances in Neural Information Processing \nSystems, 34, 2021. \n\n[18] Tianyi Chen, Yuejiao Sun, and Wotao Yin. A single-timescale stochastic bilevel optimization \nmethod. arXiv preprint arXiv:2102.04671, 2021. \n\n[19] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, \nand Shenghuo Zhu. Online optimization with gradual variations. In Conference on Learning \nTheory, pages 6-1. JMLR Workshop and Conference Proceedings, 2012. \n[36] Elad Hazan. Introduction to online convex optimization. Foundations and Trends\u00ae in \nOptimization, 2(3-4):157-325, 2016. \n\n[37] Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimiza-\ntion, 2(3-4):157-325, 2016. \n\n[38] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex \noptimization. Machine Learning, 69(2):169-192, 2007. \n\n[39] Elad Hazan and Comandur Seshadhri. Adaptive algorithms for online decision problems. In \nElectronic colloquium on computational complexity (ECCC), volume 14, 2007. \n\n[40] Elad Hazan, Karan Singh, and Cyril Zhang. Efficient regret minimization in non-convex games. \nIn International Conference on Machine Learning, pages 1433-1441. PMLR, 2017. \n\n[41] Am\u00e9lie H\u00e9liou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. Online non-\nconvex optimization with imperfect feedback. Advances in Neural Information Processing \nSystems, 33:17224-17235, 2020. \n\n[42] Am\u00e9lie H\u00e9liou, Matthieu Martin, Panayotis Mertikopoulos, and Thibaud Rahier. Zeroth-order \nnon-convex learning via hierarchical dual averaging. In International Conference on Machine \nLearning, pages 4192-4202. PMLR, 2021. \n\n[43] Mark Herbster and Manfred K Warmuth. Tracking the best expert. Machine learning, 32(2):151-\n178, 1998. \n\n[44] Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. Journal of Machine \nLearning Research, 1(281-309):10-1162, 2001. \n\n[45] Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. Online learning: A comprehensive \nsurvey. Neurocomputing, 459:249-289, 2021. \n\n[46] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework \nfor bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint \narXiv:2007.05170, 2020. \n\n[47] Feihu Huang and Heng Huang. Biadam: Fast adaptive bilevel optimization methods. arXiv \npreprint arXiv:2106.11396, 2021. \n\n[48] Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online \noptimization: Competing with dynamic comparators. In Artificial Intelligence and Statistics, \npages 398-406, 2015. \n\n[49] Kaiyi Ji and Yingbin Liang. Lower bounds and accelerated algorithms for bilevel optimization. \nArXiv, abs/2102.03926, 2021. \n\n[50] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Provably faster algorithms for bilevel optimization \nand applications to meta-learning. ArXiv, abs/2010.07962, 2020. \n\n[51] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and \nenhanced design. In International Conference on Machine Learning, pages 4882-4892. PMLR, \n2021. \n[52] [54] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In \nProceedings of the fortieth annual ACM symposium on Theory of computing, pages 681-690, \n2008. \n\n[55] Walid Krichene, Maximilian Balandat, Claire Tomlin, and Alexandre Bayen. The hedge \nalgorithm on a continuum. In International Conference on Machine Learning, pages 824-832. \nPMLR, 2015. \n\n[56] Junyi Li, Bin Gu, and Heng Huang. Improved bilevel model: Fast and optimal algorithm with \ntheoretical guarantee. arXiv preprint arXiv:2009.00690, 2020. \n\n[57] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. \narXiv preprint arXiv:1806.09055, 2018. \n\n[58] In International \nConference on Machine Learning, pages 6305-6315. PMLR, 2020. \n\n[60] Yibing Lv, Tiesong Hu, Guangmin Wang, and Zhongping Wan. A penalty function method \nbased on kuhn-tucker condition for solving linear bilevel programming. Applied Mathematics \nand Computation, 188(1):808-813, 2007. \n\n[61] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter opti-\nmization through reversible learning. In International conference on machine learning, pages \n2113-2122. PMLR, 2015. \n\n[62] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimiza-\ntion in dynamic environments: Improved regret rates for strongly convex problems. In 2016 \nIEEE 55th Conference on Decision and Control (CDC), pages 7195-7201. IEEE, 2016. \n\n[63] Gregory M Moore. Bilevel programming algorithms for machine learning model selection. \nRensselaer Polytechnic Institute, 2010. \n\n[64] Parvin Nazari, Esmaeil Khorram, and Davoud Ataee Tarzanagh. Adaptive online distributed \noptimization in dynamic environments. Optimization Methods and Software, pages 1-25, 2019. \n\n[65] Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis. Dadam: A consensus-based \ndistributed adaptive gradient method for online optimization. arXiv preprint arXiv:1901.09109, \n2019. \n\n[66] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer \nScience & Business Media, 2003. \nSupplementary Materials for \"Online Bilevel Optimization: Regret Analysis of \nOnline Alternating Gradient Methods \" \n\nTable of Contents \n\nA Related Work \n20 \n\nB Proof for OBO with Full Information \n23 \n\nB.1 C Lower Bound \n24 \n\nC.1 D Proof for Convex OBO with Partial Information \n24 \n\nF Useful Results and Facts \n40 \n\n\n\nTable 3 :\n3Summary of the NotationsNotation \nDescription \nt \nTime (round) index \nKt \nThe number of inner iterations at each round t \nT \nThe total number of rounds \nw \nWindow size for the averaged-hypergradient \n\u03b1 \nOuter stepsize \n\u03b2 \nInner stepsize \nxt \nLeader's decision at round t \nft \nLeader's objective at round t \nyt \nFollower's decision at round t \ngt \nFollower's objective at round t \nx  *  \n\nt \n\n\n\n\nt+1 ) By our assumptions, the conditions of Lemma 10 hold. Hence, substituting Lemma 10 in (A.19) and rearranging it, we get It follows from (A.21a) that c \u2264 y ) for \u03b1 = 4c/\u00b5 f . Now, let2 \n\n, \n\n(A.11) \n\u00b5 2 \n\nf \n\n2(L 2 \nf +L 2 \n\nc 0 (\u03b1) := 2 2 + \n2 \nc \n, \nc 1 (\u03b1) := \n1 \n2\u03b1 \n\n\nFor simplicity of analysis, we use R d 2 as the follower's decision set.\nt (w t ). The loss observed at this round f (u * t (w t ); D ts t ) can then be fed into leader's algorithm to update w t . Let w * t \u2208 arg min w\u2208W f (u * t (w); D ts t ) and w * \u2208 arg min w\u2208WT t=1 f (u * t (w); D ts t ).The bilevel dynamic and static regret are defined, respectively, as:BD-Reg T = T t=1 f (u * t (w t ); D ts t ) \u2212 T t=1 f (u * t (w * t ); D ts t ), BS-Reg T = T t=1 f (u * t (w t ); D ts t ) \u2212 T t=1 f (u * t (w * ); D ts t ).It should be mentioned that BS-Reg T is similar to the static regret notion introduced in[26] for OML.\nt (x)) = \u2207 x f t (x, y * t (x)) + \u2207y * t (x)\u2207 y f t (x, y * t (x)) .\nt , y * t (x * t )). (II) If T t=1 \u2207f t (x * t , y * t (x * t )) = O(1 + P 2,T /\u03b1 + \u03b1Y 2,T ), then BD-Reg T \u2264 O 1 + P 2,T /\u03b1 + \u03b1Y 2,T .(15)\nt (x t ) 2 = y t+1 \u2212 y * t (x t ) 2 \u2264 \u03c1 2 t y t \u2212 y * t (x t ) 2 . (A.6) Hence, T t=1 y t+1 \u2212 y * t (x t ) 2 \u2264 \u03c1 2 1 y 1 \u2212 y * 1 (x 1 ) 2 + T t=2 \u03c1 2 t y t \u2212 y * t (x t ) 2 , (A.7)\nt , y * t (x * t )) + c 3 (\u03b1)Y 2,T + c 4 (\u03b1). (A.24)\n(f t (x t , y * t (x t )) \u2212 f t (x * , y * t (x * ))) \u2264 2\u2207f t (x t , y * t (x t )) (x t \u2212 x * ) \u2212 \u00b5 f x t \u2212 x * 2 . (A.29)\nt (x t )) \u2212 f t (x * , y * t (x * ))) \u2264 2\u2207f t (x t , y * t (x t )) (x t \u2212 x * ) \u2212 \u00b5 f x t \u2212 x * 2 \u2264 \u03b1 t \u2207f t (x t , y * t (x t )) 2 +\nt (x t )) \u2212 f t (x * t , y * t (x * t ))). (A.39)\nt (x t ) 2 = z K+1 \u2212 y * t (x t ) 2 \u2264 1 \u2212 1 \u03ba g + 1 2K z 1 \u2212 y * t (x t ) 2 = \u03c1 2 y t \u2212 y * t (x t ) 2 , (A.54)\nAcknowledgmentThis work was supported by ARO YIP award W911NF1910027 and NSF CAREER award CCF-1845076.Proof. We show L1.. The proof of L2. follows similarly. Since \u03b2 = 2 g,1 +\u00b5g , from [66, Theorem 2.1.11], we haveBy our assumption K t = 0.5(\u03ba g + 1) log \u03c1 \u22122 t which implies thatThen, using (A.4) and (A.5), we have\u2207f t (x t , y * t (x t )) 2 (A.12)(A.14)Next, we upper bound each term of (A.12)-(A.15).\u2022 Bounding (A.12): Observe thatwhere the last inequlaity is obtained from Assumption B and Lemma 11.\u2022 Bounding (A.13): It follows from Lemma 15 thatfor any constant c > 0.\u2022 Bounding (A.14) and (A.15): From Lemma 4 and Lemma 15, we have(A.20)\u2022 Completing the proof of Theorem 5: Since {f t } T t=1 are strongly convex with parameter \u00b5 f , we haveObserve thatWe note that the above choice of c and \u03c1 satisfies the constraint c \u2208 (0, 1) and the condition of Lemma 10, respectively. Therefore, from (A.20) and (A.21), we obtainThis completes the proof.D.2.2 Proof of Theorem 5(II)Proof. From (A.11), we getFrom Lemma 4, for any c > 0, we haveCombining (A.26) and (A.27), we getApplying the definition of \u00b5 f -strong convexity to the pair of points {x t ,x * }, we have Note that by our definition 1 \u03b1 0 := 0 and x T +1 \u2212 x * 2 \u2265 0. Combining Lemma 16(II) with (A.33), we obtain BS-Reg T \u2264 e 3 log T + e 2 Y 2,T + e 1 .(A.35)D.3 Proof of Corollary 6The following lemma provides the self-bounding property of smooth functions [75, Lemma 3.1].Lemma 11. For a non-negative and L-smooth function f : X \u2192 R, we haveIt follows from [75, Lemma 2.1 and Lemma 3.1] that the non-negativity is required outside the domain X , and this is why we require the function f (\u00b7) to be non-negative outside the domain X .Proof.(. This together with(12)gives the desired result. Proof. From the update rule of Algorithm 1, we have x t+1 = \u03a0 X x t \u2212 \u03b1\u2207f t (x t , y t+1 ) . Now, from the Pythagorean theorem, we getRearranging the above inequality givesNext, we upper bound each term of (A.37).\u2022 Bounding (A.37a): Observe that(A.38)\u2022 Bounding (A.37b) and (A.37c): It follows from Lemma 4 and Assumption B thatHence,where the equality follows from the vanishing gradient condition. Hence,: Substituting (A.38) and (A.39) into (A.37), we get(A.40)\u2022 Completing the proof of Theorem 8: By the convexity of f t , we obtainObserve thatProof. From (10) we getHere, the first inequality uses the fact that a, b \u2264 1 2 ( a 2 + b 2 ); the second inequality uses (??); and the last equality follows since 1/W w\u22121 i=0 u i = 1. Similar to Lemma 10, the following lemma characterizes the inner estimation error y t+1 \u2212y * t (x t ) , where y t+1 is the inner variable update via Algorithm 1. In particular, it shows that by applying inner gradient descent K t times at each round t, we are able to obtain an error bound in terms of the local regret \u2207F t,u (x t , y * t (x t )) 2 and the inner solution variation H T = T t=2 sup x\u2208X y * t\u22121 (x) \u2212 y * t (x) 2 . Lemma 13. Suppose Assumption A holds. Let K t = K for all t, and K = (\u03ba g + 1) log \u03c1 \u22122 2 and \u03b2 = 2. Then, for the sequence {y t } T +1 t=1 generated by Algorithm 1, we haveFollowing the proof of Lemma 10 and using Assumption A, we haveFrom Lemme 15 we getBy a similar argument as in (A.6) and for t \u2208 {2, 3, . . . , T }, we haveHere, the third and last inequalities follows from Lemma 12 and (A.54), respectively. Further, from (A.54) and (A.56), we have(A.60)E.2 Proof of Theorem 9Proof. Due to the L f -smoothness of f t functions, F t is L f -smooth as well. Hence,(A.65)From Lemma 13, we getandSubstituting (A.67) and (A.66) into (A.65) gives(A.68)From Lemmas 13 and 14, we get T t=1 A(\u03b1) \u2207F t,u (x t , y * t (x t )) 2 \u2264 M (2T + 1)where c = 3(1 + L 2 y M 2 f \u03b1 2 ) andSince 0 < \u03b1 \u2264 1/(3L f ), we get \u03b1/2 + L f \u03b1 2 \u2264 \u03b1. Further, our choice of \u03c1 = min(1/ \u221a 6c, 1/ \u221a W ) together with Lemma 13 implies thatLemma 16. For all T \u2208 N, (I) log(T ) + 1 T \u2264 T t=1 1 t \u2264 log(T ) + 1;T ;(III) If 1 < s < \u221e, then \u03be(s) = T t=1 1/t s is called the Riemann \u03be-function and we have \u03be(2n) = (\u22121) n+1 (2\u03c0) 2n B 2n 2(2n)! , n = 1, 2, 3, . . . ,where the coefficients B 2n are the Bernoulli numbers.Lemma 17. For any x, y \u2208 R d , the following holds for any c > 0:x + y 2 \u2264 (1 + c) x 2 + 1 + 1 c y 2 , and (A.76)\nOptimal strategies and minimax lower bounds for online convex games. Jacob Abernethy, L Peter, Alexander Bartlett, Ambuj Rakhlin, Tewari, Jacob Abernethy, Peter L Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. 2008.\n\nLearning in non-convex games with an optimization oracle. Naman Agarwal, Alon Gonen, Elad Hazan, Conference on Learning Theory. PMLRNaman Agarwal, Alon Gonen, and Elad Hazan. Learning in non-convex games with an optimization oracle. In Conference on Learning Theory, pages 18-29. PMLR, 2019.\n\nA solution method for the static constrained stackelberg problem via penalty method. Eitaro Aiyoshi, Kiyotaka Shimizu, IEEE Transactions on Automatic Control. 2912Eitaro Aiyoshi and Kiyotaka Shimizu. A solution method for the static constrained stackelberg problem via penalty method. IEEE Transactions on Automatic Control, 29(12):1111-1114, 1984.\n\nA framework for bilevel optimization that enables stochastic and global variance reduction algorithms. Mathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, Thomas Moreau, arXiv:2201.13409arXiv preprintMathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. arXiv preprint arXiv:2201.13409, 2022.\n\nStrongly adaptive online learning. Amit Daniely, Alon Gonen, Shai Shalev-Shwartz, International Conference on Machine Learning. Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz. Strongly adaptive online learning. In International Conference on Machine Learning, pages 1405-1411, 2015.\n\nGeneric methods for optimization-based modeling. Justin Domke, Artificial Intelligence and Statistics. PMLRJustin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318-326. PMLR, 2012.\n\nAlgorithms for nonlinear bilevel mathematical programs. Arthur Thomas, Jonathan F Edmunds, Bard, IEEE transactions on Systems, Man, and Cybernetics. 21Thomas Arthur Edmunds and Jonathan F Bard. Algorithms for nonlinear bilevel mathematical programs. IEEE transactions on Systems, Man, and Cybernetics, 21(1):83-89, 1991.\n\nHyperparameter optimization. Matthias Feurer, Frank Hutter, Automated machine learning. ChamSpringerMatthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated machine learning, pages 3-33. Springer, Cham, 2019.\n\nModel-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. PMLRChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap- tation of deep networks. In International Conference on Machine Learning, pages 1126-1135. PMLR, 2017.\n\nOnline meta-learning. Chelsea Finn, Aravind Rajeswaran, Sham Kakade, Sergey Levine, International Conference on Machine Learning. PMLRChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International Conference on Machine Learning, pages 1920-1930. PMLR, 2019.\n\nForward and reverse gradient-based hyperparameter optimization. Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil, International Conference on Machine Learning. PMLRLuca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pages 1165-1173. PMLR, 2017.\n\nBilevel programming for hyperparameter optimization and meta-learning. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, Massimiliano Pontil, International Conference on Machine Learning. PMLRLuca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In International Conference on Machine Learning, pages 1568-1577. PMLR, 2018.\n\nSaeed Ghadimi, Mengdi Wang, arXiv:1802.02246Approximation methods for bilevel programming. arXiv preprintSaeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.\n\nOn the iteration complexity of hypergradient computation. Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, Saverio Salzo, International Conference on Machine Learning. PMLRRiccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity of hypergradient computation. In International Conference on Machine Learning, pages 3748-3758. PMLR, 2020.\n\nOn stochastic moving-average estimators for non-convex optimization. Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, Tianbao Yang, arXiv:2104.14840arXiv preprintZhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-average estimators for non-convex optimization. arXiv preprint arXiv:2104.14840, 2021.\n\nOnline convex optimization in dynamic environments. C Eric, Rebecca M Hall, Willett, IEEE Journal of Selected Topics in Signal Processing. 94Eric C Hall and Rebecca M Willett. Online convex optimization in dynamic environments. IEEE Journal of Selected Topics in Signal Processing, 9(4):647-662, 2015.\n\nRegret minimization in stochastic non-convex learning via a proximal-gradient approach. Nadav Hallak, Panayotis Mertikopoulos, Volkan Cevher, International Conference on Machine Learning. PMLRNadav Hallak, Panayotis Mertikopoulos, and Volkan Cevher. Regret minimization in stochastic non-convex learning via a proximal-gradient approach. In International Conference on Machine Learning, pages 4008-4017. PMLR, 2021.\n\nNew branch-and-bound rules for linear bilevel programming. Pierre Hansen, Brigitte Jaumard, Gilles Savard, SIAM Journal on scientific and Statistical Computing. 135Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel programming. SIAM Journal on scientific and Statistical Computing, 13(5):1194-1217, 1992.\n\nStateful strategic regression. Keegan Harris, Hoda Heidari, Steven Z Wu, Advances in Neural Information Processing Systems. 34Keegan Harris, Hoda Heidari, and Steven Z Wu. Stateful strategic regression. Advances in Neural Information Processing Systems, 34:28728-28741, 2021.\n\nOn lphyperparameter learning via bilevel nonsmooth optimization. Takayuki Okuno, Akiko Takeda, Akihiro Kawana, Motokazu Watanabe, Journal of Machine Learning Research. 22245Takayuki Okuno, Akiko Takeda, Akihiro Kawana, and Motokazu Watanabe. On lp- hyperparameter learning via bilevel nonsmooth optimization. Journal of Machine Learning Research, 22(245):1-47, 2021.\n\nHyperparameter optimization with approximate gradient. Fabian Pedregosa, International conference on machine learning. PMLRFabian Pedregosa. Hyperparameter optimization with approximate gradient. In International conference on machine learning, pages 737-746. PMLR, 2016.\n\nSimple principles of metalearning. Juergen Schmidhuber, Jieyu Zhao, M A Wiering, Technical report IDSIA. 69Juergen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple principles of metalearning. Technical report IDSIA, 69:1-23, 1996.\n\nTruncated backpropagation for bilevel optimization. Amirreza Shaban, Ching-An Cheng, Nathan Hatch, Byron Boots, The 22nd International Conference on Artificial Intelligence and Statistics. PMLRAmirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back- propagation for bilevel optimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1723-1732. PMLR, 2019.\n\nOnline learning and online convex optimization. Foundations and trends in Machine Learning. Shai Shalev-Shwartz, 4Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends in Machine Learning, 4(2):107-194, 2011.\n\nOnline learning: Theory, algorithms, and applications. Shai Shalev, - Shwartz, Yoram Singer, Shai Shalev-Shwartz and Yoram Singer. Online learning: Theory, algorithms, and applications. 2007.\n\nAn extended kuhn-tucker approach for linear bilevel programming. Chenggen Shi, Jie Lu, Guangquan Zhang, Applied Mathematics and Computation. 1621Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn-tucker approach for linear bilevel programming. Applied Mathematics and Computation, 162(1):51-63, 2005.\n\nA review on bilevel optimization: from classical to evolutionary approaches and applications. Ankur Sinha, Pekka Malo, Kalyanmoy Deb, IEEE Transactions on Evolutionary Computation. 222Ankur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: from classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Computation, 22(2):276-295, 2017.\n\nAdvances in neural information processing systems. Nathan Srebro, Karthik Sridharan, Ambuj Tewari, 23Smoothness, low noise and fast ratesNathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. Advances in neural information processing systems, 23, 2010.\n\nTheory of the market economy. Stackelberg Heinrich Von, Heinrich von Stackelberg et al. Theory of the market economy. 1952.\n\nOnline non-convex learning: Following the perturbed leader is optimal. Arun Sai Suggala, Praneeth Netrapalli, Algorithmic Learning Theory. PMLRArun Sai Suggala and Praneeth Netrapalli. Online non-convex learning: Following the perturbed leader is optimal. In Algorithmic Learning Theory, pages 845-861. PMLR, 2020.\n\nMachine learning in non-stationary environments: Introduction to covariate shift adaptation. Masashi Sugiyama, Motoaki Kawanabe, MIT pressMasashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments: Introduction to covariate shift adaptation. MIT press, 2012.\n\nMingchen Davoud Ataee Tarzanagh, Li, arXiv:2205.02215Christos Thrampoulidis, and Samet Oymak. Fednest: Federated bilevel, minimax, and compositional optimization. arXiv preprintDavoud Ataee Tarzanagh, Mingchen Li, Christos Thrampoulidis, and Samet Oymak. Fednest: Federated bilevel, minimax, and compositional optimization. arXiv preprint arXiv:2205.02215, 2022.\n\nOnline actor-critic algorithm to solve the continuous-time infinite horizon optimal control problem. G Kyriakos, Vamvoudakis, L Frank, Lewis, Automatica. 465Kyriakos G Vamvoudakis and Frank L Lewis. Online actor-critic algorithm to solve the continuous-time infinite horizon optimal control problem. Automatica, 46(5):878-888, 2010.\n\nThe theory of the market economy. Von Heinrich, Stackelberg Stackelberg, Heinrich Von, Oxford University PressHeinrich Von Stackelberg and Stackelberg Heinrich Von. The theory of the market economy. Oxford University Press, 1952.\n\nTracking the best expert in non-stationary stochastic environments. Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu, Advances in neural information processing systems. 29Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Tracking the best expert in non-stationary stochastic environments. Advances in neural information processing systems, 29:3972-3980, 2016.\n\nNumerical optimization. Stephen Wright, Jorge Nocedal, Springer Science357Stephen Wright, Jorge Nocedal, et al. Numerical optimization. Springer Science, 35(67-68):7, 1999.\n\nA finite-time analysis of two timescale actor-critic methods. Yue Frank Wu, Weitong Zhang, Pan Xu, Quanquan Gu, Advances in Neural Information Processing Systems. 33Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time- scale actor-critic methods. Advances in Neural Information Processing Systems, 33:17617-17628, 2020.\n\nProvably faster algorithms for bilevel optimization. Junjie Yang, Kaiyi Ji, Yingbin Liang, Advances in Neural Information Processing Systems. 342021Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. Advances in Neural Information Processing Systems, 34, 2021.\n\nTracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. Tianbao Yang, Lijun Zhang, Rong Jin, Jinfeng Yi, International Conference on Machine Learning. PMLRTianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. In International Conference on Machine Learning, pages 449-457. PMLR, 2016.\n\nAdaptive regret of convex and smooth functions. Lijun Zhang, Tie-Yan Liu, Zhi-Hua Zhou, International Conference on Machine Learning. Lijun Zhang, Tie-Yan Liu, and Zhi-Hua Zhou. Adaptive regret of convex and smooth functions. In International Conference on Machine Learning, pages 7414-7423, 2019.\n\nLijun Zhang, Shiyin Lu, Tianbao Yang, arXiv:2002.02085Minimizing dynamic regret and adaptive regret simultaneously. arXiv preprintLijun Zhang, Shiyin Lu, and Tianbao Yang. Minimizing dynamic regret and adaptive regret simultaneously. arXiv preprint arXiv:2002.02085, 2020.\n\nAdaptive online learning in dynamic environments. Lijun Zhang, Shiyin Lu, Zhi-Hua Zhou, Advances in neural information processing systems. Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments. In Advances in neural information processing systems, pages 1323-1333, 2018.\n\nImproved dynamic regret for non-degenerate functions. Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, Zhi-Hua Zhou, NIPS. Lijun Zhang, Tianbao Yang, Jinfeng Yi, Jing Rong, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. In NIPS, 2017.\n\nDynamic regret of strongly adaptive methods. Lijun Zhang, Tianbao Yang, Zhi-Hua Zhou, International Conference on Machine Learning. Lijun Zhang, Tianbao Yang, Zhi-Hua Zhou, et al. Dynamic regret of strongly adaptive methods. In International Conference on Machine Learning, pages 5882-5891, 2018.\n\nEfficient methods for non-stationary online learning. Peng Zhao, Yan-Feng Xie, Lijun Zhang, Zhi-Hua Zhou, Advances in Neural Information Processing Systems. Peng Zhao, Yan-Feng Xie, Lijun Zhang, and Zhi-Hua Zhou. Efficient methods for non-stationary online learning. In Advances in Neural Information Processing Systems.\n\nImproved analysis for dynamic regret of strongly convex and smooth functions. Peng Zhao, Lijun Zhang, In Learning for Dynamics and Control. PMLRPeng Zhao and Lijun Zhang. Improved analysis for dynamic regret of strongly convex and smooth functions. In Learning for Dynamics and Control, pages 48-59. PMLR, 2021.\n\nDynamic regret of convex and smooth functions. Peng Zhao, Yu-Jie Zhang, Lijun Zhang, Zhi-Hua Zhou, Advances in Neural Information Processing Systems. 33Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and smooth functions. Advances in Neural Information Processing Systems, 33:12510-12520, 2020.\n\nEquipping experts/bandits with long-term memory. Kai Zheng, Haipeng Luo, Ilias Diakonikolas, Liwei Wang, Advances in neural information processing systems. Kai Zheng, Haipeng Luo, Ilias Diakonikolas, and Liwei Wang. Equipping experts/bandits with long-term memory. Advances in neural information processing systems, 2019.\n\nOnline metacritic learning for off-policy actor-critic methods. Wei Zhou, Yiying Li, Yongxin Yang, Huaimin Wang, Timothy Hospedales, Advances in Neural Information Processing Systems. 33Wei Zhou, Yiying Li, Yongxin Yang, Huaimin Wang, and Timothy Hospedales. Online meta- critic learning for off-policy actor-critic methods. Advances in Neural Information Processing Systems, 33:17662-17673, 2020.\n\nRegret bounds without lipschitz continuity: online learning with relative-lipschitz losses. Yihan Zhou, Victor Sanches Portella, Mark Schmidt, Nicholas Harvey, Advances in Neural Information Processing Systems. 33Yihan Zhou, Victor Sanches Portella, Mark Schmidt, and Nicholas Harvey. Regret bounds without lipschitz continuity: online learning with relative-lipschitz losses. Advances in Neural Information Processing Systems, 33:15823-15833, 2020.\n\nOnline convex programming and generalized infinitesimal gradient ascent. Martin Zinkevich, Proceedings of the 20th international conference on machine learning (icml-03). the 20th international conference on machine learning (icml-03)Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th international conference on machine learning (icml-03), pages 928-936, 2003.\n", "annotations": {"author": "[{\"end\":221,\"start\":87},{\"end\":328,\"start\":222}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":100},{\"end\":235,\"start\":228}]", "author_first_name": "[{\"end\":93,\"start\":87},{\"end\":99,\"start\":94},{\"end\":227,\"start\":222}]", "author_affiliation": "[{\"end\":220,\"start\":130},{\"end\":327,\"start\":237}]", "title": "[{\"end\":84,\"start\":1},{\"end\":412,\"start\":329}]", "venue": null, "abstract": "[{\"end\":1323,\"start\":414}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1487,\"start\":1483},{\"end\":1490,\"start\":1487},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1524,\"start\":1520},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1527,\"start\":1524},{\"end\":1568,\"start\":1564},{\"end\":1570,\"start\":1568},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1595,\"start\":1591},{\"end\":1628,\"start\":1624},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1631,\"start\":1628},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1731,\"start\":1728},{\"end\":1734,\"start\":1731},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1737,\"start\":1734},{\"end\":1740,\"start\":1737},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1743,\"start\":1740},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1746,\"start\":1743},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1749,\"start\":1746},{\"end\":1752,\"start\":1749},{\"end\":1755,\"start\":1752},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1758,\"start\":1755},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2008,\"start\":2004},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2011,\"start\":2008},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2038,\"start\":2034},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2073,\"start\":2069},{\"end\":2601,\"start\":2597},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2604,\"start\":2601},{\"end\":2964,\"start\":2960},{\"end\":2967,\"start\":2964},{\"end\":3175,\"start\":3171},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3178,\"start\":3175},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3319,\"start\":3316},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3578,\"start\":3574},{\"end\":3581,\"start\":3578},{\"end\":3584,\"start\":3581},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3841,\"start\":3837},{\"end\":4396,\"start\":4393},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4628,\"start\":4627},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5908,\"start\":5904},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5911,\"start\":5908},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6032,\"start\":6028},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6598,\"start\":6594},{\"end\":6601,\"start\":6598},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6756,\"start\":6752},{\"end\":6759,\"start\":6756},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6762,\"start\":6759},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6765,\"start\":6762},{\"end\":6792,\"start\":6788},{\"end\":6824,\"start\":6820},{\"end\":6827,\"start\":6824},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7335,\"start\":7331},{\"end\":7338,\"start\":7335},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7341,\"start\":7338},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7344,\"start\":7341},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8331,\"start\":8327},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10546,\"start\":10543},{\"end\":10548,\"start\":10546},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10555,\"start\":10551},{\"end\":10677,\"start\":10673},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10988,\"start\":10984},{\"end\":10991,\"start\":10988},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11139,\"start\":11135},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11142,\"start\":11139},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11145,\"start\":11142},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11148,\"start\":11145},{\"end\":11294,\"start\":11290},{\"end\":11297,\"start\":11294},{\"end\":11300,\"start\":11297},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11303,\"start\":11300},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11306,\"start\":11303},{\"end\":11308,\"start\":11306},{\"end\":11446,\"start\":11442},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11500,\"start\":11496},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11736,\"start\":11732},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11929,\"start\":11925},{\"end\":11935,\"start\":11931},{\"end\":11938,\"start\":11935},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12401,\"start\":12398},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12404,\"start\":12401},{\"end\":12407,\"start\":12404},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12703,\"start\":12699},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12840,\"start\":12836},{\"end\":12843,\"start\":12840},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12873,\"start\":12870},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12876,\"start\":12873},{\"end\":12878,\"start\":12876},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12881,\"start\":12878},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12884,\"start\":12881},{\"end\":12886,\"start\":12884},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13138,\"start\":13134},{\"end\":13141,\"start\":13138},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13144,\"start\":13141},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13147,\"start\":13144},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13150,\"start\":13147},{\"end\":13153,\"start\":13150},{\"end\":13156,\"start\":13153},{\"end\":13159,\"start\":13156},{\"end\":13162,\"start\":13159},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13165,\"start\":13162},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13410,\"start\":13407},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13413,\"start\":13410},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13416,\"start\":13413},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13552,\"start\":13548},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13555,\"start\":13552},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13557,\"start\":13555},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13624,\"start\":13620},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13793,\"start\":13789},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14036,\"start\":14032},{\"end\":14039,\"start\":14036},{\"end\":14042,\"start\":14039},{\"end\":14045,\"start\":14042},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14048,\"start\":14045},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15162,\"start\":15158},{\"end\":15165,\"start\":15162},{\"end\":15168,\"start\":15165},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15171,\"start\":15168},{\"end\":15174,\"start\":15171},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16022,\"start\":16018},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":16177,\"start\":16173},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17621,\"start\":17617},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19490,\"start\":19486},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19612,\"start\":19608},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19937,\"start\":19933},{\"end\":19940,\"start\":19937},{\"end\":19943,\"start\":19940},{\"end\":19946,\"start\":19943},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21712,\"start\":21708},{\"end\":21715,\"start\":21712},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":22168,\"start\":22164},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22171,\"start\":22168},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24458,\"start\":24454},{\"end\":24471,\"start\":24458},{\"end\":25138,\"start\":25134},{\"end\":26388,\"start\":26384},{\"end\":27420,\"start\":27418},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27457,\"start\":27453},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27801,\"start\":27797},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30066,\"start\":30062},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30196,\"start\":30192},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31053,\"start\":31049},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31212,\"start\":31208},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31813,\"start\":31809},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31834,\"start\":31830},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32177,\"start\":32173},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32313,\"start\":32309},{\"end\":34122,\"start\":34118},{\"end\":34125,\"start\":34122},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34134,\"start\":34130},{\"end\":34137,\"start\":34134},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34493,\"start\":34490},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34496,\"start\":34493},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34663,\"start\":34660},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34666,\"start\":34663},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34672,\"start\":34668},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34974,\"start\":34971},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34980,\"start\":34976},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35251,\"start\":35247},{\"end\":35254,\"start\":35251},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35455,\"start\":35451},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35893,\"start\":35889},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36083,\"start\":36079},{\"end\":36221,\"start\":36217},{\"end\":36224,\"start\":36221},{\"end\":36227,\"start\":36224},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36230,\"start\":36227},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":36233,\"start\":36230},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36381,\"start\":36377},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36384,\"start\":36381},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":36387,\"start\":36384},{\"end\":36533,\"start\":36529},{\"end\":36536,\"start\":36533},{\"end\":36539,\"start\":36536},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36542,\"start\":36539},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36545,\"start\":36542},{\"end\":36547,\"start\":36545},{\"end\":36817,\"start\":36813},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36820,\"start\":36817},{\"end\":36822,\"start\":36820},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36829,\"start\":36825},{\"end\":37004,\"start\":37000},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37170,\"start\":37166},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37410,\"start\":37406},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":37466,\"start\":37462},{\"end\":37900,\"start\":37896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38145,\"start\":38142},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":38148,\"start\":38145},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38151,\"start\":38148},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38154,\"start\":38151},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38582,\"start\":38578},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39042,\"start\":39039},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39045,\"start\":39042},{\"end\":39047,\"start\":39045},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39948,\"start\":39944},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":40223,\"start\":40219},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40462,\"start\":40459},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40465,\"start\":40462},{\"end\":40467,\"start\":40465},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40470,\"start\":40467},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40473,\"start\":40470},{\"end\":40476,\"start\":40473},{\"end\":40479,\"start\":40476},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40482,\"start\":40479},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40697,\"start\":40693},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41059,\"start\":41056},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41062,\"start\":41059},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41065,\"start\":41062},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":41191,\"start\":41187},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41194,\"start\":41191},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41196,\"start\":41194},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41263,\"start\":41259},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":41432,\"start\":41428},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41696,\"start\":41692},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":42195,\"start\":42191},{\"end\":42198,\"start\":42195},{\"end\":42201,\"start\":42198},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45109,\"start\":45106},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45121,\"start\":45118},{\"end\":52683,\"start\":52679},{\"end\":52831,\"start\":52827},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":53173,\"start\":53172},{\"end\":53938,\"start\":53928},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":58043,\"start\":58040},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":58253,\"start\":58249},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":68387,\"start\":68383}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55232,\"start\":55094},{\"attributes\":{\"id\":\"fig_1\"},\"end\":55315,\"start\":55233},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55747,\"start\":55316},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55880,\"start\":55748},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55953,\"start\":55881},{\"attributes\":{\"id\":\"fig_5\"},\"end\":56288,\"start\":55954},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56527,\"start\":56289},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56776,\"start\":56528},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56833,\"start\":56777},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57054,\"start\":56834},{\"attributes\":{\"id\":\"fig_13\"},\"end\":57125,\"start\":57055},{\"attributes\":{\"id\":\"fig_14\"},\"end\":57194,\"start\":57126},{\"attributes\":{\"id\":\"fig_15\"},\"end\":58044,\"start\":57195},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":58325,\"start\":58045},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":67090,\"start\":58326},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":67495,\"start\":67091},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":67774,\"start\":67496}]", "paragraph": "[{\"end\":2458,\"start\":1339},{\"end\":3053,\"start\":2460},{\"end\":3239,\"start\":3107},{\"end\":3732,\"start\":3298},{\"end\":4397,\"start\":3764},{\"end\":5081,\"start\":4399},{\"end\":5257,\"start\":5167},{\"end\":5531,\"start\":5259},{\"end\":5776,\"start\":5614},{\"end\":6257,\"start\":5814},{\"end\":7080,\"start\":6324},{\"end\":7570,\"start\":7176},{\"end\":7649,\"start\":7598},{\"end\":7936,\"start\":7929},{\"end\":8235,\"start\":8159},{\"end\":8300,\"start\":8296},{\"end\":8566,\"start\":8302},{\"end\":9237,\"start\":9011},{\"end\":9753,\"start\":9253},{\"end\":10012,\"start\":9755},{\"end\":10401,\"start\":10014},{\"end\":11626,\"start\":10418},{\"end\":11980,\"start\":11628},{\"end\":14282,\"start\":12007},{\"end\":14647,\"start\":14314},{\"end\":14711,\"start\":14677},{\"end\":14923,\"start\":14782},{\"end\":15472,\"start\":15019},{\"end\":15578,\"start\":15552},{\"end\":15720,\"start\":15580},{\"end\":16023,\"start\":15722},{\"end\":16252,\"start\":16056},{\"end\":16334,\"start\":16298},{\"end\":16832,\"start\":16336},{\"end\":17020,\"start\":16906},{\"end\":17842,\"start\":17375},{\"end\":18011,\"start\":17971},{\"end\":18141,\"start\":18071},{\"end\":18726,\"start\":18266},{\"end\":19984,\"start\":18728},{\"end\":20212,\"start\":20207},{\"end\":20386,\"start\":20266},{\"end\":20600,\"start\":20388},{\"end\":21764,\"start\":21040},{\"end\":21818,\"start\":21766},{\"end\":22252,\"start\":21920},{\"end\":22474,\"start\":22316},{\"end\":22680,\"start\":22476},{\"end\":22882,\"start\":22682},{\"end\":22957,\"start\":22884},{\"end\":23077,\"start\":22959},{\"end\":23282,\"start\":23143},{\"end\":23580,\"start\":23284},{\"end\":24557,\"start\":23900},{\"end\":25183,\"start\":24587},{\"end\":25306,\"start\":25185},{\"end\":25368,\"start\":25363},{\"end\":25628,\"start\":25522},{\"end\":25752,\"start\":25670},{\"end\":26469,\"start\":25806},{\"end\":26574,\"start\":26471},{\"end\":26770,\"start\":26724},{\"end\":26889,\"start\":26806},{\"end\":27357,\"start\":26891},{\"end\":27533,\"start\":27370},{\"end\":27645,\"start\":27558},{\"end\":28124,\"start\":27647},{\"end\":28470,\"start\":28174},{\"end\":28832,\"start\":28526},{\"end\":29254,\"start\":28940},{\"end\":29537,\"start\":29344},{\"end\":29641,\"start\":29581},{\"end\":30306,\"start\":29660},{\"end\":31055,\"start\":30371},{\"end\":31295,\"start\":31069},{\"end\":32327,\"start\":31390},{\"end\":33092,\"start\":32342},{\"end\":33261,\"start\":33094},{\"end\":33394,\"start\":33263},{\"end\":33601,\"start\":33396},{\"end\":33624,\"start\":33603},{\"end\":34198,\"start\":33643},{\"end\":34789,\"start\":34200},{\"end\":35107,\"start\":34791},{\"end\":35530,\"start\":35109},{\"end\":35894,\"start\":35745},{\"end\":36637,\"start\":35896},{\"end\":37064,\"start\":36639},{\"end\":37417,\"start\":37066},{\"end\":38282,\"start\":37460},{\"end\":38685,\"start\":38376},{\"end\":39049,\"start\":38687},{\"end\":40140,\"start\":39051},{\"end\":40698,\"start\":40142},{\"end\":41609,\"start\":40700},{\"end\":41878,\"start\":41611},{\"end\":42010,\"start\":41880},{\"end\":42765,\"start\":42012},{\"end\":42795,\"start\":42767},{\"end\":42911,\"start\":42819},{\"end\":43048,\"start\":43012},{\"end\":43117,\"start\":43081},{\"end\":43179,\"start\":43138},{\"end\":43255,\"start\":43219},{\"end\":43322,\"start\":43295},{\"end\":43483,\"start\":43358},{\"end\":44143,\"start\":44061},{\"end\":44540,\"start\":44186},{\"end\":44681,\"start\":44634},{\"end\":44808,\"start\":44783},{\"end\":44904,\"start\":44894},{\"end\":45167,\"start\":45045},{\"end\":45171,\"start\":45169},{\"end\":45556,\"start\":45495},{\"end\":45805,\"start\":45608},{\"end\":46510,\"start\":45829},{\"end\":46588,\"start\":46512},{\"end\":46744,\"start\":46628},{\"end\":47130,\"start\":47112},{\"end\":47337,\"start\":47332},{\"end\":47368,\"start\":47339},{\"end\":47797,\"start\":47665},{\"end\":48065,\"start\":47965},{\"end\":48351,\"start\":48159},{\"end\":48664,\"start\":48658},{\"end\":48671,\"start\":48666},{\"end\":48752,\"start\":48722},{\"end\":48970,\"start\":48965},{\"end\":49412,\"start\":49320},{\"end\":49833,\"start\":49796},{\"end\":50571,\"start\":50523},{\"end\":51081,\"start\":50970},{\"end\":51130,\"start\":51092},{\"end\":51239,\"start\":51164},{\"end\":51436,\"start\":51430},{\"end\":51592,\"start\":51438},{\"end\":52906,\"start\":51963},{\"end\":53048,\"start\":52961},{\"end\":53210,\"start\":53072},{\"end\":53269,\"start\":53212},{\"end\":53345,\"start\":53271},{\"end\":53467,\"start\":53429},{\"end\":53614,\"start\":53469},{\"end\":53939,\"start\":53616},{\"end\":54071,\"start\":53953},{\"end\":54179,\"start\":54160},{\"end\":54638,\"start\":54496},{\"end\":54913,\"start\":54775},{\"end\":55093,\"start\":54944}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3106,\"start\":3054},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3297,\"start\":3248},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5166,\"start\":5082},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5613,\"start\":5532},{\"attributes\":{\"id\":\"formula_5\"},\"end\":6323,\"start\":6258},{\"attributes\":{\"id\":\"formula_6\"},\"end\":7175,\"start\":7081},{\"attributes\":{\"id\":\"formula_7\"},\"end\":7597,\"start\":7571},{\"attributes\":{\"id\":\"formula_8\"},\"end\":7928,\"start\":7650},{\"attributes\":{\"id\":\"formula_9\"},\"end\":7972,\"start\":7937},{\"attributes\":{\"id\":\"formula_10\"},\"end\":8111,\"start\":7972},{\"attributes\":{\"id\":\"formula_11\"},\"end\":8158,\"start\":8111},{\"attributes\":{\"id\":\"formula_12\"},\"end\":8959,\"start\":8567},{\"attributes\":{\"id\":\"formula_13\"},\"end\":9010,\"start\":8959},{\"attributes\":{\"id\":\"formula_14\"},\"end\":12006,\"start\":11981},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14781,\"start\":14712},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15018,\"start\":14924},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15551,\"start\":15473},{\"attributes\":{\"id\":\"formula_19\"},\"end\":16297,\"start\":16253},{\"attributes\":{\"id\":\"formula_20\"},\"end\":16905,\"start\":16833},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17374,\"start\":17021},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17970,\"start\":17843},{\"attributes\":{\"id\":\"formula_23\"},\"end\":18070,\"start\":18012},{\"attributes\":{\"id\":\"formula_24\"},\"end\":18265,\"start\":18142},{\"attributes\":{\"id\":\"formula_25\"},\"end\":20206,\"start\":19985},{\"attributes\":{\"id\":\"formula_26\"},\"end\":20265,\"start\":20213},{\"attributes\":{\"id\":\"formula_27\"},\"end\":20956,\"start\":20601},{\"attributes\":{\"id\":\"formula_28\"},\"end\":21039,\"start\":20956},{\"attributes\":{\"id\":\"formula_29\"},\"end\":21919,\"start\":21819},{\"attributes\":{\"id\":\"formula_30\"},\"end\":22315,\"start\":22253},{\"attributes\":{\"id\":\"formula_31\"},\"end\":23142,\"start\":23078},{\"attributes\":{\"id\":\"formula_32\"},\"end\":23816,\"start\":23581},{\"attributes\":{\"id\":\"formula_33\"},\"end\":23899,\"start\":23816},{\"attributes\":{\"id\":\"formula_34\"},\"end\":25362,\"start\":25307},{\"attributes\":{\"id\":\"formula_35\"},\"end\":25521,\"start\":25369},{\"attributes\":{\"id\":\"formula_36\"},\"end\":25669,\"start\":25629},{\"attributes\":{\"id\":\"formula_37\"},\"end\":25805,\"start\":25753},{\"attributes\":{\"id\":\"formula_38\"},\"end\":26723,\"start\":26575},{\"attributes\":{\"id\":\"formula_39\"},\"end\":26805,\"start\":26771},{\"attributes\":{\"id\":\"formula_40\"},\"end\":28525,\"start\":28471},{\"attributes\":{\"id\":\"formula_41\"},\"end\":28939,\"start\":28833},{\"attributes\":{\"id\":\"formula_42\"},\"end\":29343,\"start\":29255},{\"attributes\":{\"id\":\"formula_43\"},\"end\":29580,\"start\":29538},{\"attributes\":{\"id\":\"formula_44\"},\"end\":30370,\"start\":30307},{\"attributes\":{\"id\":\"formula_45\"},\"end\":31389,\"start\":31296},{\"attributes\":{\"id\":\"formula_46\"},\"end\":35744,\"start\":35531},{\"attributes\":{\"id\":\"formula_47\"},\"end\":37459,\"start\":37418},{\"attributes\":{\"id\":\"formula_48\"},\"end\":38375,\"start\":38283},{\"attributes\":{\"id\":\"formula_49\"},\"end\":42818,\"start\":42796},{\"attributes\":{\"id\":\"formula_50\"},\"end\":43011,\"start\":42912},{\"attributes\":{\"id\":\"formula_51\"},\"end\":43080,\"start\":43049},{\"attributes\":{\"id\":\"formula_52\"},\"end\":43137,\"start\":43118},{\"attributes\":{\"id\":\"formula_53\"},\"end\":43218,\"start\":43180},{\"attributes\":{\"id\":\"formula_54\"},\"end\":43294,\"start\":43256},{\"attributes\":{\"id\":\"formula_55\"},\"end\":43357,\"start\":43323},{\"attributes\":{\"id\":\"formula_56\"},\"end\":43675,\"start\":43484},{\"attributes\":{\"id\":\"formula_57\"},\"end\":44060,\"start\":43675},{\"attributes\":{\"id\":\"formula_58\"},\"end\":44633,\"start\":44541},{\"attributes\":{\"id\":\"formula_59\"},\"end\":44782,\"start\":44682},{\"attributes\":{\"id\":\"formula_60\"},\"end\":44893,\"start\":44809},{\"attributes\":{\"id\":\"formula_61\"},\"end\":45044,\"start\":44905},{\"attributes\":{\"id\":\"formula_62\"},\"end\":45211,\"start\":45172},{\"attributes\":{\"id\":\"formula_63\"},\"end\":45238,\"start\":45211},{\"attributes\":{\"id\":\"formula_64\"},\"end\":45494,\"start\":45238},{\"attributes\":{\"id\":\"formula_65\"},\"end\":46627,\"start\":46589},{\"attributes\":{\"id\":\"formula_66\"},\"end\":47111,\"start\":46745},{\"attributes\":{\"id\":\"formula_67\"},\"end\":47331,\"start\":47131},{\"attributes\":{\"id\":\"formula_68\"},\"end\":47664,\"start\":47369},{\"attributes\":{\"id\":\"formula_69\"},\"end\":47964,\"start\":47798},{\"attributes\":{\"id\":\"formula_70\"},\"end\":48158,\"start\":48066},{\"attributes\":{\"id\":\"formula_71\"},\"end\":48489,\"start\":48352},{\"attributes\":{\"id\":\"formula_72\"},\"end\":48657,\"start\":48489},{\"attributes\":{\"id\":\"formula_73\"},\"end\":48721,\"start\":48672},{\"attributes\":{\"id\":\"formula_74\"},\"end\":48964,\"start\":48753},{\"attributes\":{\"id\":\"formula_75\"},\"end\":49319,\"start\":48971},{\"attributes\":{\"id\":\"formula_76\"},\"end\":49795,\"start\":49413},{\"attributes\":{\"id\":\"formula_77\"},\"end\":49942,\"start\":49834},{\"attributes\":{\"id\":\"formula_78\"},\"end\":50522,\"start\":49942},{\"attributes\":{\"id\":\"formula_79\"},\"end\":50969,\"start\":50572},{\"attributes\":{\"id\":\"formula_80\"},\"end\":51429,\"start\":51240},{\"attributes\":{\"id\":\"formula_81\"},\"end\":51962,\"start\":51593},{\"attributes\":{\"id\":\"formula_82\"},\"end\":53428,\"start\":53346},{\"attributes\":{\"id\":\"formula_83\"},\"end\":54159,\"start\":54072},{\"attributes\":{\"id\":\"formula_84\"},\"end\":54495,\"start\":54180},{\"attributes\":{\"id\":\"formula_85\"},\"end\":54774,\"start\":54639}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":10400,\"start\":10393},{\"end\":27428,\"start\":27421},{\"end\":31730,\"start\":31723}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1337,\"start\":1325},{\"end\":3247,\"start\":3242},{\"attributes\":{\"n\":\"1.1\"},\"end\":3762,\"start\":3735},{\"end\":5812,\"start\":5779},{\"end\":8251,\"start\":8238},{\"end\":8259,\"start\":8254},{\"end\":8294,\"start\":8262},{\"attributes\":{\"n\":\"1.2\"},\"end\":9251,\"start\":9240},{\"attributes\":{\"n\":\"2\"},\"end\":10416,\"start\":10404},{\"attributes\":{\"n\":\"3\"},\"end\":14312,\"start\":14285},{\"attributes\":{\"n\":\"3.1\"},\"end\":14675,\"start\":14650},{\"attributes\":{\"n\":\"3.2\"},\"end\":16054,\"start\":16026},{\"attributes\":{\"n\":\"3.3\"},\"end\":24585,\"start\":24560},{\"end\":27368,\"start\":27360},{\"attributes\":{\"n\":\"4\"},\"end\":27556,\"start\":27536},{\"end\":28172,\"start\":28127},{\"attributes\":{\"n\":\"4.1\"},\"end\":29658,\"start\":29644},{\"attributes\":{\"n\":\"4.2\"},\"end\":31067,\"start\":31058},{\"attributes\":{\"n\":\"5\"},\"end\":32340,\"start\":32330},{\"end\":33641,\"start\":33627},{\"end\":44159,\"start\":44146},{\"end\":44184,\"start\":44162},{\"end\":45606,\"start\":45559},{\"end\":45827,\"start\":45808},{\"end\":51090,\"start\":51084},{\"end\":51162,\"start\":51133},{\"end\":52959,\"start\":52909},{\"end\":53070,\"start\":53051},{\"end\":53951,\"start\":53942},{\"end\":54942,\"start\":54916},{\"end\":55109,\"start\":55095},{\"end\":55759,\"start\":55749},{\"end\":55892,\"start\":55882},{\"end\":56295,\"start\":56290},{\"end\":58055,\"start\":58046},{\"end\":67101,\"start\":67092}]", "table": "[{\"end\":67090,\"start\":60358},{\"end\":67495,\"start\":67127},{\"end\":67774,\"start\":67685}]", "figure_caption": "[{\"end\":55232,\"start\":55111},{\"end\":55315,\"start\":55235},{\"end\":55747,\"start\":55318},{\"end\":55880,\"start\":55761},{\"end\":55953,\"start\":55894},{\"end\":56288,\"start\":55956},{\"end\":56527,\"start\":56297},{\"end\":56776,\"start\":56530},{\"end\":56833,\"start\":56779},{\"end\":57054,\"start\":56836},{\"end\":57125,\"start\":57057},{\"end\":57194,\"start\":57128},{\"end\":58044,\"start\":57197},{\"end\":58325,\"start\":58057},{\"end\":60358,\"start\":58328},{\"end\":67127,\"start\":67103},{\"end\":67685,\"start\":67498}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30487,\"start\":30479},{\"end\":30830,\"start\":30823},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30909,\"start\":30901}]", "bib_author_first_name": "[{\"end\":73550,\"start\":73545},{\"end\":73563,\"start\":73562},{\"end\":73580,\"start\":73571},{\"end\":73596,\"start\":73591},{\"end\":73825,\"start\":73820},{\"end\":73839,\"start\":73835},{\"end\":73851,\"start\":73847},{\"end\":74146,\"start\":74140},{\"end\":74164,\"start\":74156},{\"end\":74515,\"start\":74508},{\"end\":74531,\"start\":74525},{\"end\":74545,\"start\":74539},{\"end\":74560,\"start\":74554},{\"end\":74846,\"start\":74842},{\"end\":74860,\"start\":74856},{\"end\":74872,\"start\":74868},{\"end\":75149,\"start\":75143},{\"end\":75397,\"start\":75391},{\"end\":75414,\"start\":75406},{\"end\":75416,\"start\":75415},{\"end\":75694,\"start\":75686},{\"end\":75708,\"start\":75703},{\"end\":75960,\"start\":75953},{\"end\":75973,\"start\":75967},{\"end\":75988,\"start\":75982},{\"end\":76272,\"start\":76265},{\"end\":76286,\"start\":76279},{\"end\":76303,\"start\":76299},{\"end\":76318,\"start\":76312},{\"end\":76612,\"start\":76608},{\"end\":76632,\"start\":76625},{\"end\":76646,\"start\":76641},{\"end\":76669,\"start\":76657},{\"end\":77020,\"start\":77016},{\"end\":77038,\"start\":77033},{\"end\":77056,\"start\":77049},{\"end\":77072,\"start\":77064},{\"end\":77093,\"start\":77081},{\"end\":77397,\"start\":77392},{\"end\":77413,\"start\":77407},{\"end\":77681,\"start\":77673},{\"end\":77694,\"start\":77690},{\"end\":77719,\"start\":77707},{\"end\":77735,\"start\":77728},{\"end\":78081,\"start\":78073},{\"end\":78089,\"start\":78087},{\"end\":78099,\"start\":78094},{\"end\":78109,\"start\":78105},{\"end\":78122,\"start\":78115},{\"end\":78381,\"start\":78380},{\"end\":78395,\"start\":78388},{\"end\":78397,\"start\":78396},{\"end\":78724,\"start\":78719},{\"end\":78742,\"start\":78733},{\"end\":78764,\"start\":78758},{\"end\":79113,\"start\":79107},{\"end\":79130,\"start\":79122},{\"end\":79146,\"start\":79140},{\"end\":79438,\"start\":79432},{\"end\":79451,\"start\":79447},{\"end\":79469,\"start\":79461},{\"end\":79751,\"start\":79743},{\"end\":79764,\"start\":79759},{\"end\":79780,\"start\":79773},{\"end\":79797,\"start\":79789},{\"end\":80107,\"start\":80101},{\"end\":80361,\"start\":80354},{\"end\":80380,\"start\":80375},{\"end\":80388,\"start\":80387},{\"end\":80390,\"start\":80389},{\"end\":80610,\"start\":80602},{\"end\":80627,\"start\":80619},{\"end\":80641,\"start\":80635},{\"end\":80654,\"start\":80649},{\"end\":81067,\"start\":81063},{\"end\":81284,\"start\":81280},{\"end\":81294,\"start\":81293},{\"end\":81309,\"start\":81304},{\"end\":81491,\"start\":81483},{\"end\":81500,\"start\":81497},{\"end\":81514,\"start\":81505},{\"end\":81828,\"start\":81823},{\"end\":81841,\"start\":81836},{\"end\":81857,\"start\":81848},{\"end\":82177,\"start\":82171},{\"end\":82193,\"start\":82186},{\"end\":82210,\"start\":82205},{\"end\":82450,\"start\":82439},{\"end\":82609,\"start\":82605},{\"end\":82631,\"start\":82623},{\"end\":82950,\"start\":82943},{\"end\":82968,\"start\":82961},{\"end\":83146,\"start\":83138},{\"end\":83604,\"start\":83603},{\"end\":83629,\"start\":83628},{\"end\":83873,\"start\":83870},{\"end\":83895,\"start\":83884},{\"end\":84142,\"start\":84135},{\"end\":84153,\"start\":84148},{\"end\":84167,\"start\":84160},{\"end\":84437,\"start\":84430},{\"end\":84451,\"start\":84446},{\"end\":84651,\"start\":84642},{\"end\":84663,\"start\":84656},{\"end\":84674,\"start\":84671},{\"end\":84687,\"start\":84679},{\"end\":84996,\"start\":84990},{\"end\":85008,\"start\":85003},{\"end\":85020,\"start\":85013},{\"end\":85357,\"start\":85350},{\"end\":85369,\"start\":85364},{\"end\":85381,\"start\":85377},{\"end\":85394,\"start\":85387},{\"end\":85740,\"start\":85735},{\"end\":85755,\"start\":85748},{\"end\":85768,\"start\":85761},{\"end\":85991,\"start\":85986},{\"end\":86005,\"start\":85999},{\"end\":86017,\"start\":86010},{\"end\":86315,\"start\":86310},{\"end\":86329,\"start\":86323},{\"end\":86341,\"start\":86334},{\"end\":86628,\"start\":86623},{\"end\":86643,\"start\":86636},{\"end\":86657,\"start\":86650},{\"end\":86666,\"start\":86662},{\"end\":86680,\"start\":86673},{\"end\":86881,\"start\":86876},{\"end\":86896,\"start\":86889},{\"end\":86910,\"start\":86903},{\"end\":87187,\"start\":87183},{\"end\":87202,\"start\":87194},{\"end\":87213,\"start\":87208},{\"end\":87228,\"start\":87221},{\"end\":87533,\"start\":87529},{\"end\":87545,\"start\":87540},{\"end\":87815,\"start\":87811},{\"end\":87828,\"start\":87822},{\"end\":87841,\"start\":87836},{\"end\":87856,\"start\":87849},{\"end\":88145,\"start\":88142},{\"end\":88160,\"start\":88153},{\"end\":88171,\"start\":88166},{\"end\":88191,\"start\":88186},{\"end\":88483,\"start\":88480},{\"end\":88496,\"start\":88490},{\"end\":88508,\"start\":88501},{\"end\":88522,\"start\":88515},{\"end\":88536,\"start\":88529},{\"end\":88912,\"start\":88907},{\"end\":88925,\"start\":88919},{\"end\":88948,\"start\":88944},{\"end\":88966,\"start\":88958},{\"end\":89345,\"start\":89339}]", "bib_author_last_name": "[{\"end\":73560,\"start\":73551},{\"end\":73569,\"start\":73564},{\"end\":73589,\"start\":73581},{\"end\":73604,\"start\":73597},{\"end\":73612,\"start\":73606},{\"end\":73833,\"start\":73826},{\"end\":73845,\"start\":73840},{\"end\":73857,\"start\":73852},{\"end\":74154,\"start\":74147},{\"end\":74172,\"start\":74165},{\"end\":74523,\"start\":74516},{\"end\":74537,\"start\":74532},{\"end\":74552,\"start\":74546},{\"end\":74567,\"start\":74561},{\"end\":74854,\"start\":74847},{\"end\":74866,\"start\":74861},{\"end\":74887,\"start\":74873},{\"end\":75155,\"start\":75150},{\"end\":75404,\"start\":75398},{\"end\":75424,\"start\":75417},{\"end\":75430,\"start\":75426},{\"end\":75701,\"start\":75695},{\"end\":75715,\"start\":75709},{\"end\":75965,\"start\":75961},{\"end\":75980,\"start\":75974},{\"end\":75995,\"start\":75989},{\"end\":76277,\"start\":76273},{\"end\":76297,\"start\":76287},{\"end\":76310,\"start\":76304},{\"end\":76325,\"start\":76319},{\"end\":76623,\"start\":76613},{\"end\":76639,\"start\":76633},{\"end\":76655,\"start\":76647},{\"end\":76676,\"start\":76670},{\"end\":77031,\"start\":77021},{\"end\":77047,\"start\":77039},{\"end\":77062,\"start\":77057},{\"end\":77079,\"start\":77073},{\"end\":77100,\"start\":77094},{\"end\":77405,\"start\":77398},{\"end\":77418,\"start\":77414},{\"end\":77688,\"start\":77682},{\"end\":77705,\"start\":77695},{\"end\":77726,\"start\":77720},{\"end\":77741,\"start\":77736},{\"end\":78085,\"start\":78082},{\"end\":78092,\"start\":78090},{\"end\":78103,\"start\":78100},{\"end\":78113,\"start\":78110},{\"end\":78127,\"start\":78123},{\"end\":78386,\"start\":78382},{\"end\":78402,\"start\":78398},{\"end\":78411,\"start\":78404},{\"end\":78731,\"start\":78725},{\"end\":78756,\"start\":78743},{\"end\":78771,\"start\":78765},{\"end\":79120,\"start\":79114},{\"end\":79138,\"start\":79131},{\"end\":79153,\"start\":79147},{\"end\":79445,\"start\":79439},{\"end\":79459,\"start\":79452},{\"end\":79472,\"start\":79470},{\"end\":79757,\"start\":79752},{\"end\":79771,\"start\":79765},{\"end\":79787,\"start\":79781},{\"end\":79806,\"start\":79798},{\"end\":80117,\"start\":80108},{\"end\":80373,\"start\":80362},{\"end\":80385,\"start\":80381},{\"end\":80398,\"start\":80391},{\"end\":80617,\"start\":80611},{\"end\":80633,\"start\":80628},{\"end\":80647,\"start\":80642},{\"end\":80660,\"start\":80655},{\"end\":81082,\"start\":81068},{\"end\":81291,\"start\":81285},{\"end\":81302,\"start\":81295},{\"end\":81316,\"start\":81310},{\"end\":81495,\"start\":81492},{\"end\":81503,\"start\":81501},{\"end\":81520,\"start\":81515},{\"end\":81834,\"start\":81829},{\"end\":81846,\"start\":81842},{\"end\":81861,\"start\":81858},{\"end\":82184,\"start\":82178},{\"end\":82203,\"start\":82194},{\"end\":82217,\"start\":82211},{\"end\":82463,\"start\":82451},{\"end\":82621,\"start\":82610},{\"end\":82642,\"start\":82632},{\"end\":82959,\"start\":82951},{\"end\":82977,\"start\":82969},{\"end\":83169,\"start\":83147},{\"end\":83173,\"start\":83171},{\"end\":83613,\"start\":83605},{\"end\":83626,\"start\":83615},{\"end\":83635,\"start\":83630},{\"end\":83642,\"start\":83637},{\"end\":83882,\"start\":83874},{\"end\":83907,\"start\":83896},{\"end\":83921,\"start\":83909},{\"end\":84146,\"start\":84143},{\"end\":84158,\"start\":84154},{\"end\":84170,\"start\":84168},{\"end\":84444,\"start\":84438},{\"end\":84459,\"start\":84452},{\"end\":84654,\"start\":84652},{\"end\":84669,\"start\":84664},{\"end\":84677,\"start\":84675},{\"end\":84690,\"start\":84688},{\"end\":85001,\"start\":84997},{\"end\":85011,\"start\":85009},{\"end\":85026,\"start\":85021},{\"end\":85362,\"start\":85358},{\"end\":85375,\"start\":85370},{\"end\":85385,\"start\":85382},{\"end\":85397,\"start\":85395},{\"end\":85746,\"start\":85741},{\"end\":85759,\"start\":85756},{\"end\":85773,\"start\":85769},{\"end\":85997,\"start\":85992},{\"end\":86008,\"start\":86006},{\"end\":86022,\"start\":86018},{\"end\":86321,\"start\":86316},{\"end\":86332,\"start\":86330},{\"end\":86346,\"start\":86342},{\"end\":86634,\"start\":86629},{\"end\":86648,\"start\":86644},{\"end\":86660,\"start\":86658},{\"end\":86671,\"start\":86667},{\"end\":86685,\"start\":86681},{\"end\":86887,\"start\":86882},{\"end\":86901,\"start\":86897},{\"end\":86915,\"start\":86911},{\"end\":87192,\"start\":87188},{\"end\":87206,\"start\":87203},{\"end\":87219,\"start\":87214},{\"end\":87233,\"start\":87229},{\"end\":87538,\"start\":87534},{\"end\":87551,\"start\":87546},{\"end\":87820,\"start\":87816},{\"end\":87834,\"start\":87829},{\"end\":87847,\"start\":87842},{\"end\":87861,\"start\":87857},{\"end\":88151,\"start\":88146},{\"end\":88164,\"start\":88161},{\"end\":88184,\"start\":88172},{\"end\":88196,\"start\":88192},{\"end\":88488,\"start\":88484},{\"end\":88499,\"start\":88497},{\"end\":88513,\"start\":88509},{\"end\":88527,\"start\":88523},{\"end\":88547,\"start\":88537},{\"end\":88917,\"start\":88913},{\"end\":88942,\"start\":88926},{\"end\":88956,\"start\":88949},{\"end\":88973,\"start\":88967},{\"end\":89355,\"start\":89346}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":73760,\"start\":73476},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52987927},\"end\":74053,\"start\":73762},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":119863945},\"end\":74403,\"start\":74055},{\"attributes\":{\"doi\":\"arXiv:2201.13409\",\"id\":\"b3\"},\"end\":74805,\"start\":74405},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8342240},\"end\":75092,\"start\":74807},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2792062},\"end\":75333,\"start\":75094},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":32610219},\"end\":75655,\"start\":75335},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":11574468},\"end\":75884,\"start\":75657},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6719686},\"end\":76241,\"start\":75886},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":67856437},\"end\":76542,\"start\":76243},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8026824},\"end\":76943,\"start\":76544},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49194806},\"end\":77390,\"start\":76945},{\"attributes\":{\"doi\":\"arXiv:1802.02246\",\"id\":\"b12\"},\"end\":77613,\"start\":77392},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":220250381},\"end\":78002,\"start\":77615},{\"attributes\":{\"doi\":\"arXiv:2104.14840\",\"id\":\"b14\"},\"end\":78326,\"start\":78004},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":39226382},\"end\":78629,\"start\":78328},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":222310686},\"end\":79046,\"start\":78631},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":30363917},\"end\":79399,\"start\":79048},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":235364020},\"end\":79676,\"start\":79401},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":237572396},\"end\":80044,\"start\":79678},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":552516},\"end\":80317,\"start\":80046},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14099817},\"end\":80548,\"start\":80319},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":121190562},\"end\":80969,\"start\":80550},{\"attributes\":{\"id\":\"b23\"},\"end\":81223,\"start\":80971},{\"attributes\":{\"id\":\"b24\"},\"end\":81416,\"start\":81225},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1264213},\"end\":81727,\"start\":81418},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4626744},\"end\":82118,\"start\":81729},{\"attributes\":{\"id\":\"b27\"},\"end\":82407,\"start\":82120},{\"attributes\":{\"id\":\"b28\"},\"end\":82532,\"start\":82409},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":83458554},\"end\":82848,\"start\":82534},{\"attributes\":{\"id\":\"b30\"},\"end\":83136,\"start\":82850},{\"attributes\":{\"doi\":\"arXiv:2205.02215\",\"id\":\"b31\"},\"end\":83500,\"start\":83138},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11521287},\"end\":83834,\"start\":83502},{\"attributes\":{\"id\":\"b33\"},\"end\":84065,\"start\":83836},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":16488426},\"end\":84404,\"start\":84067},{\"attributes\":{\"id\":\"b35\"},\"end\":84578,\"start\":84406},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":218487029},\"end\":84935,\"start\":84580},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235377197},\"end\":85240,\"start\":84937},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":800505},\"end\":85685,\"start\":85242},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":135463423},\"end\":85984,\"start\":85687},{\"attributes\":{\"doi\":\"arXiv:2002.02085\",\"id\":\"b40\"},\"end\":86258,\"start\":85986},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":53023226},\"end\":86567,\"start\":86260},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":2240779},\"end\":86829,\"start\":86569},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":49314405},\"end\":87127,\"start\":86831},{\"attributes\":{\"id\":\"b44\"},\"end\":87449,\"start\":87129},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":219558949},\"end\":87762,\"start\":87451},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":220381233},\"end\":88091,\"start\":87764},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":170078936},\"end\":88414,\"start\":88093},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":212658036},\"end\":88813,\"start\":88416},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":221680083},\"end\":89264,\"start\":88815},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":553962},\"end\":89694,\"start\":89266}]", "bib_title": "[{\"end\":73818,\"start\":73762},{\"end\":74138,\"start\":74055},{\"end\":74840,\"start\":74807},{\"end\":75141,\"start\":75094},{\"end\":75389,\"start\":75335},{\"end\":75684,\"start\":75657},{\"end\":75951,\"start\":75886},{\"end\":76263,\"start\":76243},{\"end\":76606,\"start\":76544},{\"end\":77014,\"start\":76945},{\"end\":77671,\"start\":77615},{\"end\":78378,\"start\":78328},{\"end\":78717,\"start\":78631},{\"end\":79105,\"start\":79048},{\"end\":79430,\"start\":79401},{\"end\":79741,\"start\":79678},{\"end\":80099,\"start\":80046},{\"end\":80352,\"start\":80319},{\"end\":80600,\"start\":80550},{\"end\":81481,\"start\":81418},{\"end\":81821,\"start\":81729},{\"end\":82603,\"start\":82534},{\"end\":83601,\"start\":83502},{\"end\":84133,\"start\":84067},{\"end\":84640,\"start\":84580},{\"end\":84988,\"start\":84937},{\"end\":85348,\"start\":85242},{\"end\":85733,\"start\":85687},{\"end\":86308,\"start\":86260},{\"end\":86621,\"start\":86569},{\"end\":86874,\"start\":86831},{\"end\":87181,\"start\":87129},{\"end\":87527,\"start\":87451},{\"end\":87809,\"start\":87764},{\"end\":88140,\"start\":88093},{\"end\":88478,\"start\":88416},{\"end\":88905,\"start\":88815},{\"end\":89337,\"start\":89266}]", "bib_author": "[{\"end\":73562,\"start\":73545},{\"end\":73571,\"start\":73562},{\"end\":73591,\"start\":73571},{\"end\":73606,\"start\":73591},{\"end\":73614,\"start\":73606},{\"end\":73835,\"start\":73820},{\"end\":73847,\"start\":73835},{\"end\":73859,\"start\":73847},{\"end\":74156,\"start\":74140},{\"end\":74174,\"start\":74156},{\"end\":74525,\"start\":74508},{\"end\":74539,\"start\":74525},{\"end\":74554,\"start\":74539},{\"end\":74569,\"start\":74554},{\"end\":74856,\"start\":74842},{\"end\":74868,\"start\":74856},{\"end\":74889,\"start\":74868},{\"end\":75157,\"start\":75143},{\"end\":75406,\"start\":75391},{\"end\":75426,\"start\":75406},{\"end\":75432,\"start\":75426},{\"end\":75703,\"start\":75686},{\"end\":75717,\"start\":75703},{\"end\":75967,\"start\":75953},{\"end\":75982,\"start\":75967},{\"end\":75997,\"start\":75982},{\"end\":76279,\"start\":76265},{\"end\":76299,\"start\":76279},{\"end\":76312,\"start\":76299},{\"end\":76327,\"start\":76312},{\"end\":76625,\"start\":76608},{\"end\":76641,\"start\":76625},{\"end\":76657,\"start\":76641},{\"end\":76678,\"start\":76657},{\"end\":77033,\"start\":77016},{\"end\":77049,\"start\":77033},{\"end\":77064,\"start\":77049},{\"end\":77081,\"start\":77064},{\"end\":77102,\"start\":77081},{\"end\":77407,\"start\":77392},{\"end\":77420,\"start\":77407},{\"end\":77690,\"start\":77673},{\"end\":77707,\"start\":77690},{\"end\":77728,\"start\":77707},{\"end\":77743,\"start\":77728},{\"end\":78087,\"start\":78073},{\"end\":78094,\"start\":78087},{\"end\":78105,\"start\":78094},{\"end\":78115,\"start\":78105},{\"end\":78129,\"start\":78115},{\"end\":78388,\"start\":78380},{\"end\":78404,\"start\":78388},{\"end\":78413,\"start\":78404},{\"end\":78733,\"start\":78719},{\"end\":78758,\"start\":78733},{\"end\":78773,\"start\":78758},{\"end\":79122,\"start\":79107},{\"end\":79140,\"start\":79122},{\"end\":79155,\"start\":79140},{\"end\":79447,\"start\":79432},{\"end\":79461,\"start\":79447},{\"end\":79474,\"start\":79461},{\"end\":79759,\"start\":79743},{\"end\":79773,\"start\":79759},{\"end\":79789,\"start\":79773},{\"end\":79808,\"start\":79789},{\"end\":80119,\"start\":80101},{\"end\":80375,\"start\":80354},{\"end\":80387,\"start\":80375},{\"end\":80400,\"start\":80387},{\"end\":80619,\"start\":80602},{\"end\":80635,\"start\":80619},{\"end\":80649,\"start\":80635},{\"end\":80662,\"start\":80649},{\"end\":81084,\"start\":81063},{\"end\":81293,\"start\":81280},{\"end\":81304,\"start\":81293},{\"end\":81318,\"start\":81304},{\"end\":81497,\"start\":81483},{\"end\":81505,\"start\":81497},{\"end\":81522,\"start\":81505},{\"end\":81836,\"start\":81823},{\"end\":81848,\"start\":81836},{\"end\":81863,\"start\":81848},{\"end\":82186,\"start\":82171},{\"end\":82205,\"start\":82186},{\"end\":82219,\"start\":82205},{\"end\":82465,\"start\":82439},{\"end\":82623,\"start\":82605},{\"end\":82644,\"start\":82623},{\"end\":82961,\"start\":82943},{\"end\":82979,\"start\":82961},{\"end\":83171,\"start\":83138},{\"end\":83175,\"start\":83171},{\"end\":83615,\"start\":83603},{\"end\":83628,\"start\":83615},{\"end\":83637,\"start\":83628},{\"end\":83644,\"start\":83637},{\"end\":83884,\"start\":83870},{\"end\":83909,\"start\":83884},{\"end\":83923,\"start\":83909},{\"end\":84148,\"start\":84135},{\"end\":84160,\"start\":84148},{\"end\":84172,\"start\":84160},{\"end\":84446,\"start\":84430},{\"end\":84461,\"start\":84446},{\"end\":84656,\"start\":84642},{\"end\":84671,\"start\":84656},{\"end\":84679,\"start\":84671},{\"end\":84692,\"start\":84679},{\"end\":85003,\"start\":84990},{\"end\":85013,\"start\":85003},{\"end\":85028,\"start\":85013},{\"end\":85364,\"start\":85350},{\"end\":85377,\"start\":85364},{\"end\":85387,\"start\":85377},{\"end\":85399,\"start\":85387},{\"end\":85748,\"start\":85735},{\"end\":85761,\"start\":85748},{\"end\":85775,\"start\":85761},{\"end\":85999,\"start\":85986},{\"end\":86010,\"start\":85999},{\"end\":86024,\"start\":86010},{\"end\":86323,\"start\":86310},{\"end\":86334,\"start\":86323},{\"end\":86348,\"start\":86334},{\"end\":86636,\"start\":86623},{\"end\":86650,\"start\":86636},{\"end\":86662,\"start\":86650},{\"end\":86673,\"start\":86662},{\"end\":86687,\"start\":86673},{\"end\":86889,\"start\":86876},{\"end\":86903,\"start\":86889},{\"end\":86917,\"start\":86903},{\"end\":87194,\"start\":87183},{\"end\":87208,\"start\":87194},{\"end\":87221,\"start\":87208},{\"end\":87235,\"start\":87221},{\"end\":87540,\"start\":87529},{\"end\":87553,\"start\":87540},{\"end\":87822,\"start\":87811},{\"end\":87836,\"start\":87822},{\"end\":87849,\"start\":87836},{\"end\":87863,\"start\":87849},{\"end\":88153,\"start\":88142},{\"end\":88166,\"start\":88153},{\"end\":88186,\"start\":88166},{\"end\":88198,\"start\":88186},{\"end\":88490,\"start\":88480},{\"end\":88501,\"start\":88490},{\"end\":88515,\"start\":88501},{\"end\":88529,\"start\":88515},{\"end\":88549,\"start\":88529},{\"end\":88919,\"start\":88907},{\"end\":88944,\"start\":88919},{\"end\":88958,\"start\":88944},{\"end\":88975,\"start\":88958},{\"end\":89357,\"start\":89339}]", "bib_venue": "[{\"end\":75749,\"start\":75745},{\"end\":89500,\"start\":89437},{\"end\":73543,\"start\":73476},{\"end\":73888,\"start\":73859},{\"end\":74212,\"start\":74174},{\"end\":74506,\"start\":74405},{\"end\":74933,\"start\":74889},{\"end\":75195,\"start\":75157},{\"end\":75482,\"start\":75432},{\"end\":75743,\"start\":75717},{\"end\":76041,\"start\":75997},{\"end\":76371,\"start\":76327},{\"end\":76722,\"start\":76678},{\"end\":77146,\"start\":77102},{\"end\":77481,\"start\":77436},{\"end\":77787,\"start\":77743},{\"end\":78071,\"start\":78004},{\"end\":78465,\"start\":78413},{\"end\":78817,\"start\":78773},{\"end\":79207,\"start\":79155},{\"end\":79523,\"start\":79474},{\"end\":79844,\"start\":79808},{\"end\":80163,\"start\":80119},{\"end\":80422,\"start\":80400},{\"end\":80737,\"start\":80662},{\"end\":81061,\"start\":80971},{\"end\":81278,\"start\":81225},{\"end\":81557,\"start\":81522},{\"end\":81908,\"start\":81863},{\"end\":82169,\"start\":82120},{\"end\":82437,\"start\":82409},{\"end\":82671,\"start\":82644},{\"end\":82941,\"start\":82850},{\"end\":83299,\"start\":83191},{\"end\":83654,\"start\":83644},{\"end\":83868,\"start\":83836},{\"end\":84221,\"start\":84172},{\"end\":84428,\"start\":84406},{\"end\":84741,\"start\":84692},{\"end\":85077,\"start\":85028},{\"end\":85443,\"start\":85399},{\"end\":85819,\"start\":85775},{\"end\":86100,\"start\":86040},{\"end\":86397,\"start\":86348},{\"end\":86691,\"start\":86687},{\"end\":86961,\"start\":86917},{\"end\":87284,\"start\":87235},{\"end\":87589,\"start\":87553},{\"end\":87912,\"start\":87863},{\"end\":88247,\"start\":88198},{\"end\":88598,\"start\":88549},{\"end\":89024,\"start\":88975},{\"end\":89435,\"start\":89357}]"}}}, "year": 2023, "month": 12, "day": 17}