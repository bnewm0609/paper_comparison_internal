{"id": 244954483, "updated": "2023-10-05 19:17:37.014", "metadata": {"title": "BT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models", "authors": "[{\"first\":\"Narinder\",\"last\":\"Punn\",\"middle\":[\"Singh\"]},{\"first\":\"Sonali\",\"last\":\"Agarwal\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Deep learning has brought the most profound contribution towards biomedical image segmentation to automate the process of delineation in medical imaging. To accomplish such task, the models are required to be trained using huge amount of annotated or labelled data that highlights the region of interest with a binary mask. However, efficient generation of the annotations for such huge data requires expert biomedical analysts and extensive manual effort. It is a tedious and expensive task, while also being vulnerable to human error. To address this problem, a self-supervised learning framework, BT-Unet is proposed that uses the Barlow Twins approach to pre-train the encoder of a U-Net model via redundancy reduction in an unsupervised manner to learn data representation. Later, complete network is fine-tuned to perform actual segmentation. The BT-Unet framework can be trained with a limited number of annotated samples while having high number of unannotated samples, which is mostly the case in real-world problems. This framework is validated over multiple U-Net models over diverse datasets by generating scenarios of a limited number of labelled samples using standard evaluation metrics. With exhaustive experiment trials, it is observed that the BT-Unet framework enhances the performance of the U-Net models with significant margin under such circumstances.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2112.03916", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ml/PunnA22", "doi": "10.1007/s10994-022-06219-3"}}, "content": {"source": {"pdf_hash": "783eb950b067dbe9424e939fa9b838aff2d363d7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.03916v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bf91a5991acf462bdfff1b33bd48eff362aba964", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/783eb950b067dbe9424e939fa9b838aff2d363d7.txt", "contents": "\nBT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models\n\n\nNarinder Singh Punn \nDept. of information Technology\nIndian Institute of Information Technology Allahabad\nUttar PradeshPrayagrajIndia\n\nSonali Agarwal \nDept. of information Technology\nIndian Institute of Information Technology Allahabad\nUttar PradeshPrayagrajIndia\n\nBT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models\n*Corresponding author(s). E-mail(s): pse2017002@iiita.ac.in; Contributing authors: sonali@iiita.ac.in;Barlow TwinsBiomedical image segmentationSelf-supervised learningU-Net\nDeep learning has brought the most profound contribution towards biomedical image segmentation to automate the process of delineation in medical imaging. To accomplish such task, the models are required to be trained using huge amount of annotated or labelled data that highlights the region of interest with a binary mask. However, efficient generation of the annotations for such huge data requires expert biomedical analysts and extensive manual effort. It is a tedious and expensive task, while also being vulnerable to human error. To address this problem, a self-supervised learning framework, BT-Unet is proposed that uses the Barlow Twins approach to pre-train the encoder of a U-Net model via redundancy reduction in an unsupervised manner to learn data representation. Later, complete network is fine-tuned to perform actual segmentation. The BT-Unet framework can be trained with a limited number of annotated samples while having high number of unannotated samples, which is mostly the case in real-world problems. This framework is validated over multiple U-Net models over diverse datasets by generating scenarios of a limited number of labelled samples using standard evaluation metrics. With exhaustive experiment trials, it is observed that the BT-Unet framework enhances the performance of the U-Net models with significant margin under such circumstances.\n\nIntroduction\n\nWith the advent of advancements in deep learning technologies, there is a significant gain in the momentum of its applications in biomedical image analysis such as classification, localization, segmentation, etc. (Ker et al, 2017). Most medical applications require segregating the objects or regions (damaged tissues, cells, nuclei, organs, etc.) with fine boundaries using medical imaging such as CAT scans, X-Rays, Ultrasound, etc. for diagnosis, monitoring and treatment. This delineation is generally performed by expert clinicians or radiologists which is a complex and tedious task. With biomedical image segmentation being a precursor to computer-aided classification/localization, various deep learning based approaches are developed to automate the segmentation process for faster diagnosis and better treatment (Haque and Neubert, 2020). Among these approaches, U-Net (Ronneberger et al, 2015) based segmentation models gained significant popularity due to its mutable and modular structure that would result in the state-of-the-art diagnosis system (Punn and Agarwal, 2021a).\n\nHowever, such potential of deep learning segmentation models is only unlocked by training the models with a large amount of annotated data i.e., a fully supervised approach. Moreover, efficient generation of the annotations for such huge data requires expert biomedical analysts and extensive manual effort. It is a tedious and expensive task, while also being vulnerable to human error. To address this issue, various strategies are adopted to efficiently train the model with limited labelled data samples such as data augmentation, transfer learning, self-supervised learning, etc. In image data augmentation (Shorten and Khoshgoftaar, 2019) the aim is to increase the number of labelled data by geometric transformations, feature space augmentation, generative adversarial networks, etc. However, the diversity of the augmented samples is limited by the available annotated samples which could cause an overfitting problem in the model. Several attempts are also made towards transfer learning to alleviate the performance of model with limited annotated data samples. Though this strategy works very well with natural images, but is ineffectual in biomedical image analysis (Alzubaidi et al, 2020;Raghu et al, 2019) due to large variation in the associated complex patterns of biomedical imaging as compared to natural images.\n\nSelf-supervised learning (Jing and Tian, 2020) is an emerging technology that is effectively closing the gap with fully supervised methods on large computer vision benchmarks. It provides an effective solution to the limited availability of annotated data. Here, the aim is to perform pre-training with an unsupervised strategy for learning useful and better representations of the data samples. The pre-trained model is then fine-tuned with limited annotated samples to adopt the actual task such as segmentation, classification, etc. The recent development in self-supervised learning methods can be categorized as contrastive learning (MoCo (He et al, 2020), PIRL (Misra and Maaten, 2020), SimCLR (Chen et al, 2020)), clustering (DeepCluster (Caron et al, 2018), SeLA (Asano et al, 2019), SwAV (Caron et al, 2020)), distillation (BYOL (Grill et al, 2020), SimSiam (Chen and He, 2021)) and redundancy reduction (Barlow Twins (Zbontar et al, 2021)). The approaches categorized under contrastive learning, clustering and distillations work based on the similarity maximization that requires efficient generation of the positive (related images) and negative (unrelated images) samples for pre-training. However, in biomedical image analysis identifying the negative samples is a tedious and complex task (Zeng et al, 2021) due to similarities and dissimilarities at low and high level feature representations respectively, whereas in Barlow Twins there is no such requirement; therefore, more suitable for biomedical image segmentation. With this motivation, in the present work a self-supervised learning framework called BT-Unet is proposed for biomedical image segmentation, where Barlow Twins strategy is integrated with U-Net segmentation models. The main contributions of the present research work are highlighted below:\n\n\u2022 The challenge of limited biomedical annotated data availability is addressed by integrating redundancy reduction based self-supervised learning approach with U-Net segmentation models. \u2022 The pre-training of the U-Net encoder is performed with the Barlow Twins strategy to learn feature representations in an unsupervised manner (without data annotations). \u2022 The effect of pre-training on biomedical image segmentation performance is analyzed with multiple U-Net models over diverse datasets.\n\nThe rest of the paper is divided into several sections, where Section 2 presents the literature review of the recent developments in the self-supervised segmentation approaches, followed by the methods adopted in the proposed framework in Sections 3 and 4. Sections 5 and 6 present the experimental setup and the obtained results respectively. Finally, concluding remarks are presented in Section 7.\n\n\nRelated work\n\nIn recent years, due to developments in deep learning technologies, the researchers have developed a keen interest in computer-aided diagnosis systems to promote better healthcare services with a variety of applications (Lei et al, 2020) such as classification, detection, segmentation, etc. With segmentation being one of the critical aspects of diagnosis and follow up treatment plans, various deep learning based segmentation models are developed. However, the use of self-supervised learning strategies to improve the segmentation performance is relatively least explored.\n\nIn the context of biomedical image segmentation, most of these approaches can be grouped into pretext based and contrastive learning based strategies. In pretext based self-supervised learning, a proxy task is performed to learn the feature representations. There are variety of pretext or proxy tasks that can be used for pre-training such as inpainting (Pathak et al, 2016), jigsaw puzzles (Noroozi and Favaro, 2016), predicting the position of image patches (Doersch et al, 2015), predicting rotations (Gidaris et al, 2018), etc. However, there is a huge gap or variation between these tasks and the actual or downstream tasks due to which these strategies achieved limited success in deep learning applications. In contrastive learning based strategies, the feature representations are learned by effectively distinguishing the positive (similar) and negative (dissimilar) pairs. Recently, contrastive learning based unsupervised feature representations have gained significant interest. Following this context, Chaitanya et al (2020) proposed a contrastive learning framework that adapts global and local features using unannotated samples during pre-training in a stage-wise manner for biomedical image segmentation. Similarly, Zheng et al (2021) proposed a hierarchical self-supervised framework, where multiple heterogenous datasets across multiple modalities are utilized for multi-level contrastive pre-training to adapt the multiple segmentation tasks by fine-tuning. Dhere and Sivaswamy (2021) performed kidney segmentation with a self-supervision strategy where contrastive learning is used with pretext task defined as the classification of the pair of kidneys belonging to the same side, where the pre-training is performed using a siamese network and the pre-trained encoder is fine-tuned in the U-Net model for final segmentation.\n\nHowever, these contrastive approaches require generating effective positive and negative pairs which is not feasible in every task such as nuclei segmentation or skin lesion segmentation, where the input samples are almost related and it is relatively hard to generate negative pair of samples (Li et al, 2021). Following this context, a redundancy reduction based strategy is adopted that does not require generation of positive and negative pairs for pre-training. Here, the aim is to obtain invariant and independent feature representations for every neuron of a model by minimizing the true and observed crosscorrelation matrices which is the opposite of mutual information between the representations.\n\n\nMethods\n\nIn this section, the background knowledge of the redundancy reduction based Barlow Twins approach for self-supervised learning along with U-Net based models are presented that are integrated with Barlow Twins for biomedical image segmentation.\n\n\nBarlow Twins\n\nInspired by Horace Barlow's efficient coding hypothesis, where neurons communicate via spiking codes which aim to reduce the redundancy between neurons, Zbontar et al (2021) proposed a redundancy reduction based Barlow Twins (BT) framework for self-supervised learning. Here, the objective is to make each neuron satisfy two conditions that are producing feature representations: 1) Invariance -invariant under different augmentations, and 2) Reduce redundancy -independent of other neurons. The overall BT framework is presented in Fig. 1. In this, two identical encoders (f \u03b8 , siamese net), sharing the same parameters and weights, generates feature representations (Z A and Z B ) of the augmented or corrupted images (Y A and Y B ). Later, a cross-correlation matrix (C) is generated from batch normalized feature representations: Z A and Z B . Finally, to satisfy the above two properties, the model is fine-tuned to make the matrix C fairly similar to an identity matrix with a loss function, L BT defined as shown in Eq. 1. In the BT-Unet framework, the encoder of the U-Net models is pre-trained with the BT strategy and later fine-tuned to perform actual segmentation.\nL BT = i (1 \u2212 C ii ) 2 + \u03bb i j =i C ij 2 (1) C ij = b z A b,i z B b,j b (z A b,i ) 2 b (z B b,j ) 2(2)\nwhere i (1 \u2212 C ii ) 2 is an invariance term (diagonal or identity term) to direct neurons to produce same output under different augmentations, and i j =i C ij 2 is a redundancy reduction term (off-diagonal term) to make each neuron produce different output. The term \u03bb is used to balance the contribution of invariance and redundancy reduction terms, which however is kept equal to 0.2 (Zbontar et al, 2021).\n\n\nU-Net models\n\nU-Net (Ronneberger et al, 2015) is the most widely used model for biomedical image segmentation. As shown in Fig. 2, it follows symmetric encoder-decoder design to extract and reconstruct the feature maps respectively. The encoder phase uses the stack of ReLU activated convolution and pooling operations for feature extraction and later these feature maps are concatenated with the corresponding decoder block using the skip connections for feature up-sampling operation. Finally, 1 \u00d7 1 convolution is used in the output layer to generate a segmentation mask and categorize each pixel of an input image. The model was trained with the pixel-wise weighted cross-entropy function as defined in the Eq. 3. The U-Net model achieved state-of-the-art results in the ISBI cell tracking challenge. With this potential of the U-Net model, various U-Net based models are developed for different biomedical image segmentation applications (Punn and Agarwal, 2021a).\nE = x\u2208\u2126 w c (x) + w 0 \u00b7 exp \u2212 (d 1 (x) + d 2 (x)) 2 2\u03c3 2 log(p (x) (x))(3)\nwhere p k (x) is the output softmax function, d 1 and d 2 indicate the distances to the nearest and second nearest boundary points, w c represents weight map, w o and \u03c3 are constants.\n\nIn the present article, U-Net, attention U-Net (AU-Net) (Oktay et al, 2018), inception U-Net (I-Unet) (Punn and Agarwal, 2020) and residual cross spatial attention guided inception U-Net (RCA-IUnet) (Punn and Agarwal, 2021b) are considered to establish better comparative analysis of the segmentation performance. In contrast to the U-Net model, A-Unet adds attention filters in the skip connection to suppress irrelevant features of an input image, while following a similar encoder-decoder structure. Later, to efficiently capture the varied shape, size and location of the target structure, I-Unet introduces inception convolution layers where multi-scale features are extracted at the same layer, thereby forming a wider network. Moreover, a hybrid pooling layer is proposed that exploits the features of spatial max pooling and spectral pooling. Inspired from the potential of A-Unet and I-Unet, the RCA-IUnet model advances the attention filter to capture multi-scale feature maps and generate better attention descriptors for target regions, while also using the hybrid pooling and inception convolution layers by reducing the cost of computation and training parameters with the help of depthwise separable convolution (Chollet, 2017).\n\n\nProposed framework\n\nIn the present article, the state-of-the-art potential of U-Net models is expanded by integrating redundancy reduction based Barlow Twins selfsupervised learning for better performance in the segmentation with limited annotated data samples. The schematic representation of the proposed framework is shown in Fig. 3. The BT-Unet framework is divided into two phases: 1) Pre-training, and 2) Fine-tuning. In pre-training, the aim is to learn the complex feature representations using unannotated data samples. Here, the encoder network of the U-Net model is pre-trained using BT self-supervised learning strategy. Initially, the input image is augmented or corrupted with certain distortions such as random crop and rotations to generate two distorted images. This type of distortion follows from the results acquired by Zbontar et al (2021) while analyzing the effect of applying augmentations on pre-training performance. Each augmented image is analyzed with a U-Net encoder followed by a projection network to generate encoded feature representations in desired dimensions. The projection network follows from the feature maps produced by the encoder network with global average pooling and blocks of fully connected layers, ReLU activation and batch normalization (FC + ReLU + BN), and final encoded feature representations are generated by another FC layer. Following from the empirical observations, the number of neurons in each fully connected layer is kept half of the spatial dimension of an input image for efficient pre-training, e.g., if input, I \u2208 R s\u00d7s\u00d7c then the number of neurons are s/2, where s is a spatial dimension of an image. The number of neurons could be further increased but at the cost of heavy computation. However, no significant improvement was observed with increased dimensions. Since, in later layers, the network learns task specific features that are not aligned with the downstream segmentation task, hence the weights learned by the projection network can be neglected, whereas the weights of the entire encoder network can be transferred to the U-Net model. In the second phase, the weights of the encoder network in the U-Net model are initialized with pre-trained weights (from the first phase), whereas the rest of the network is initialized with default weights. Finally, the U-Net model is fine-tuned with limited annotated samples for the biomedical image segmentation.\n\n\nExperiment configuration\n\nThis section covers the details concerning the training and testing environment of the BT-Unet framework along with the datasets and modifications in U-Net models that are used for the comparative analysis. To establish robust results with the BT-Unet framework various state-of-the-art U-Net models are considered for experiments such as vanilla U-Net, attention U-Net (A-Unet), inception U-Net (I-Unet) and residual cross-spatial attention guided inception U-Net (RCA-IUnet). Inspired by the RCA-IUnet model, the following minor modifications for U-Net, A-Unet and I-Unet architectures are performed:\n\n\u2022 Standard 2D convolution operations are replaced with 2D depthwise separable convolution to reduce the number of training parameters and multiplication operations without affecting performance. \u2022 Batch normalization is performed after every convolution operation for stable training. \u2022 Each encoder layer is equipped with residual skip connection (mini-skip connection) to avoid the vanishing gradient problem. \u2022 Encoding and decoding phases are divided into four stages. With each stage in the encoding phase, the number of channels increases by a factor of 2 (starting with 16 in the first layer) and spatial resolution decreases by a factor 2 (starting with 256\u00d7256).\n\n\nDataset description and setup\n\nThe performance of the BT-Unet framework is validated using four datasets with different segmentation tasks as shown in Table 1. The dataset comprises images of organs, cells and lesions acquired under different imaging protocols. The Kaggle data science bowl 2018 (KDSB18) challenge is developed for automated nuclei segmentation. It contains annotated histopathological images with varying nuclei shapes, cell types, magnification and modalities (fluorescence/brightfield). Breast ultrasound image segmentation (BUSIS) benchmark dataset comprises breast ultrasound scans annotated with a binary mask of the tumor regions. The dataset covers a wide diversity of samples collected from various medical institutes and organizations. In another dataset ISIC18, skin lesion segmentation is performed with the help of annotated dermoscopy images. To add more diversity in the datasets, brain tumor segmentation 2018 (BraTS18) challenge is considered, which comprises of 3D volumes of MRI modalities with high-grade gliomas (HGG) and low-grade gliomas (LGG) to \n\n\nTraining and testing\n\nThe overall framework is developed using the TensorFlow v2.6 library on Nvidia RTX 2070 Max-Q GPU system. For all experiments, images from KDSB18, BUSI, ISIC18 and BraTS18 datasets are resized to 256\u00d7256. The datasets are split into 70% training data and 30% testing data. The pre-training is performed with complete training data without considering annotations. To simulate the scenario of limited annotated data availability, 20% of KDSB18 and BUSI training data and 10% of ISIC18 and BraTS18 training data are considered for fine-tuning the segmentation models with 5fold and 10-fold cross-validation respectively. Moreover, Adam optimizer with learning rate initialized at 1e \u2212 3 is used for all the experiments that decay by a factor of 0.1 once the learning stagnates for better segmentation results. The training phase is also assisted with the early-stopping strategy to avoid the overfitting problem by terminating the training process when the loss function stops decreasing. The pre-training is performed by minimizing the cross-correlation loss function defined in Eq. 1, whereas U-Net models are fine-tuned with segmentation loss function, L defined as the average of binary cross-entropy loss, L BC and dice coefficient loss, L DC as shown in Eq. 4.\nL = 1 2 L BC + 1 2 L DC (4) L BC (y, p (y)) = \u2212 N i (y i .log (p (y i )) + (1 \u2212 y i ) .log (1 \u2212 p (y i ))) (5) L DC (y, p (y)) = 1 \u2212 2 N i y i .p(y i ) N i | y i | 2 + N i | p(y i ) | 2(6)\nwhere y is the ground truth label of a pixel, p(y) is the predicted label of a pixel and N is the total number of pixels. The performance of trained U-Net models is validated on the test sets by using various evaluation metrics such as precision (Pr.) (Eq. 7), dice coefficient (DC) (Eq. 8) and mean intersection-over-union (mIoU) (Eq. 9).\nP r. = T P (T P + F P ) (7) DC = 2T P (2T P + F N + F P ) (8) mIoU = 1 10 t T P (T P + F N + F P )\n; t+ = 0.5 \u2264 0.95 (9) where, T P -true positive, T N -true negative, F P -false positive, F N -false negative and t -prediction threshold.\n\n\nResults and discussion\n\nThe proposed framework generates a segmentation mask for given medical imaging. The quantitative results of the U-Net models with and without the Barlow Twins based pre-training on four different biomedical imaging datasets is presented in Table 2 along with the percentage change in the segmentation performance of the models with Fig. 4. Moreover, Fig. 5 presents the qualitative comparison of the segmentation performance. Following are the observations made for each dataset:\n\n\u2022 KDSB18. In the cell nuclei segmentation task of KDSB18, the performance of the BT enabled U-Net models exceeds as compared to the models without BT (as shown in Table 2). It is also observed that as the architecture design becomes more complex then pre-training exhibits positive influence on the segmentation performance (as shown in Fig. 4, RCA-IUnet model achieves maximum gain in the performance as compared to other models). Moreover, the minimum change in the performance of the vanilla U-Net model indicates that a simpler encoder structure (close to vanilla U-Net, e.g. A-Unet)   face difficulty in extracting feature maps with limited annotated samples. A similar pattern can also be observed with qualitative results in Fig. 5. \u2022 BUSIS. The automated segmentation of breast tumor using ultrasound imaging achieves promising results with BT pre-training (as shown in Table 2). It is observed that U-Net and A-Unet models are not able to learn and extract feature maps concerning tumor regions (achieved 0 precision, DC and mIoU), however with pre-training, these models achieved noticeable improvement (as shown in Fig. 5). In case of I-Unet and RCA-IUnet models, considerable improvements are observed with pre-training, where dice coefficient increases by 5% and precision increases by 11% respectively (as shown in Fig. 4). \u2022 ISIC18. Skin lesion segmentation is another challenging task, where U-Net models with BT achieved satisfactory improvements in segmentation. The I-Unet and RCA-IUnet models are the most influenced networks that achieved 5.1% and 2.2% increase in precision respectively. However, a slight decline in performance is observed with vanilla U-Net and A-Unet (as shown in Fig. 4) while using BT pre-training. In contrast to I-Unet and RCA-IUnet, these models have simpler encoder structures due to which in pre-training the model fails to learn efficient feature representation about complex lesion regions. Furthermore, as observed from Fig. 5, the BT+RCA-IUnet model achieved best skin lesion segmentation results with smoother boundaries. \u2022 BraTS18. In this challenge of brain tumor segmentation, the models performed similarly as with the ISIC18 dataset. I-Unet and RCA-IUnet models achieved significant gain in the segmentation performance while using the BT-Unet framework, whereas the same behaviour is not observed with vanilla U-Net and A-Unet models because of their inability to effectively capture tumor feature representations during pre-training. As observed from Fig. 4, the RCA-IUnet model achieved gains of 5.3%, 6.1% and 6.7%, while I-Unet achieved gains of 4.4%, 4.7% and 4.6% in precision, dice coefficient and mIoU respectively. Furthermore, the performance of U-Net variants with and without the pretraining is analysed with multiple fractions of training datasets as shown in Table 3. For all datasets with training fractions less than 50%, similar change in performance is observed among the models as discussed in Table 2. Besides, in the case of without pre-training for U-Net and A-Unet, the increased fraction of BUSIS training samples improved the performance as compared to zero values (observed in Table 2) and the corresponding performance gain is also achieved with pre-training. However, for the fractions greater than 50%, the performance gap is narrowed i.e. results produced by the models with and without pre-training are not significantly different. This indicates that it is beneficial to utilize the pre-training strategy when there are limited annotations within the large pool of data samples.\n\n\nConclusion\n\nIn this research work, self-supervised learning assisted biomedical image segmentation framework BT-Unet, is proposed to address one of the major challenges of limited annotated data availability. The BT-Unet framework uses redundancy reduction based Barlow Twins strategy for pre-training the encoder network of the U-Net model with feature representations of the data in an unsupervised manner, followed by fine-tuning of the U-Net model for downstream biomedical image segmentation task with limited annotated data samples. With exhaustive experimental trials, it is evident that BT-Unet tends to improve the segmentation performance of U-Net models in such situations. Moreover, this improvement is also influenced by the underlying encoder structure and nature of biomedical image segmentation task. In future, more experiments can be conducted by modifying or exploring different pretraining strategies to generate better feature representations and ensure finer biomedical image segmentation.\n\n\nDeclarations\n\n\u2022 Acknowledgment: We thank our institute, Indian Institute of Information Technology Allahabad (IIITA), India and Big Data Analytics (BDA) lab for allocating necessary resources to perform this research. We extend our thanks to our colleagues for their valuable guidance and suggestions. \u2022 Conflict of interest: The authors have no relevant financial or non-financial interests to disclose. \u2022 Ethics approval: Not Applicable.\n\n\u2022 Consent to participate: Not Applicable.\n\n\u2022 Consent for publication: Not Applicable. \u2022 Authors' contributions: All authors contributed equally in conceptualizing the research problem and preparation of the manuscript. \u2022 Availability of data: All datasets are publicly accessible. \u2022 Code availability: Code for using BT-Unet framework is available at https: //github.com/nspunn1993/BT-Unet.\n\nFig. 1\n1Schematic representation of Barlow Twins(Zbontar et al, 2021).\n\nFig. 2\n2U-Net architecture(Ronneberger et al, 2015).\n\nFig. 3\n3BT-Unet framework. a) Pre-training U-Net encoder network, and b) Fine-tuning U-Net model that is initialized with pre-trained encoder weights.\n\n673 Fig. 4\n6734839 0.852 0.789 0.794 0.748 0.756 0.883 0.828 0.813 0.689 0.664 0.628 Y 0.931 0.921 0.913 0.833 0.818 0.771 0.918 0.833 0.821 0.734 0.701 0.Impact of BT pre-training on segmentation performance of the U-Net models.\n\nFig. 5\n5Qualitative comparative analysis of the segmentation performance.\n\nTable 1\n1Summary of biomedical datasets used in our experiment.Dataset \nDescription \nImages Size \n\nKDSB18 (Kaggle, 2018) \nNuclei segmentation using \nhistopathological cell images. \n\n670 \n256\u00d7256 \n\nBUSIS (Xian et al, 2018) \nBreast tumor segmentation \nusing ultrasound scans. \n\n562 \nVariable \n\nISIC18 (ISIC, 2018) \nSkin lesion segmentation \nusing dermoscopy images. \n\n2596 \nVariable \n\nBraTS18 (MICCAI, 2018) Brain tumor segmentation \nusing MRI modalities (T1, \nT1C, T2, FLAIR). \n\n285 \n240\u00d7240\u00d7155 \n\nhighlight different tumor regions: whole tumor (WT), tumor core (TC), and \nemerging tumor (ET). This task is simplified by extracting 4,200 2D slices \nfrom the 3D volumes of FLAIR modality and analyzing the segmentation mask \nassociated with the WT region. \n\n\n\nTable 2\n2Quantitative comparative analysis of segmentation models. The best results with and without BT are shown with bold black and blue color respectively.Model BT \nKDSB18 (20%) \nBUSIS (20%) \nISIC18 (10%) \nBraTS18 (10%) \n\nPr. \nDC \nmIoU \nPr. \nDC \nmIoU \nPr. \nDC \nmIoU \nPr. \nDC \nmIoU \n\nU-Net \n\n\nTable 3\n3Performance analysis of U-Net variants with and without pre-training using \nBarlow Twins (BT) over different fractions of training datasets (DS). \n\nDS Model \nBT \n30% \n50% \n80% \n100% \n\nPr. \nDC \nmIoU Pr. \nDC \nmIoU Pr. \nDC \nmIoU Pr. \nDC \nmIoU \n\nKDSB18 \n\nU-Net \nN \n0.903 0.944 0.921 \n0.952 0.965 0.951 \n0.955 0.968 0.954 \n0.957 0.969 0.954 \nY \n0.923 0.951 0.940 \n0.955 0.969 0.951 \n0.958 0.970 0.957 \n0.959 0.970 0.955 \n\nA-Unet \nN \n0.868 0.871 0.866 \n0.889 0.900 0.877 \n0.923 0.943 0.921 \n0.955 0.965 0.953 \nY \n0.901 0.921 0.893 \n0.923 0.944 0.913 \n0.944 0.968 0.943 \n0.954 0.965 0.952 \n\nI-Unet \nN \n0.896 0.908 0.894 \n0.946 0.958 0.943 \n0.955 0.969 0.955 \n0.958 0.971 0.955 \nY \n0.934 0.953 0.931 \n0.957 0.971 0.952 \n0.961 0.972 0.960 \n0.961 0.973 0.956 \nRCA-\nIUnet \n\nN \n0.873 0.888 0.865 \n0.950 0.964 0.948 \n0.964 0.970 0.963 \n0.966 0.971 0.964 \nY \n0.944 0.900 0.899 \n0.971 0.972 0.970 \n0.980 0.981 0.977 \n0.967 0.972 0.965 \n\nBUSIS \n\nU-Net \nN \n0.430 0.320 0.210 \n0.819 0.845 0.816 \n0.921 0.874 0.858 \n0.933 0.893 0.866 \nY \n0.856 0.723 0.689 \n0.919 0.888 0.843 \n0.925 0.879 0.867 \n0.934 0.894 0.867 \n\nA-Unet \nN \n0.394 0.312 0.191 \n0.851 0.824 0.821 \n0.933 0.887 0.869 \n0.942 0.911 0.897 \nY \n0.866 0.756 0.700 \n0.910 0.882 0.842 \n0.934 0.887 0.868 \n0.943 0.912 0.897 \n\nI-Unet \nN \n0.845 0.823 0.793 \n0.887 0.844 0.839 \n0.943 0.893 0.873 \n0.949 0.913 0.900 \nY \n0.886 0.855 0.843 \n0.923 0.901 0.889 \n0.946 0.911 0.899 \n0.951 0.928 0.911 \nRCA-\nIUnet \n\nN \n0.881 0.861 0.823 \n0.908 0.889 0.869 \n0.934 0.901 0.888 \n0.954 0.937 0.921 \nY \n0.911 0.873 0.855 \n0.923 0.901 0.898 \n0.952 0.926 0.911 \n0.956 0.938 0.922 \n\nISIC18 \n\nU-Net \nN \n0.929 0.874 0.856 \n0.949 0.899 0.891 \n0.954 0.922 0.911 \n0.972 0.958 0.946 \nY \n0.911 0.871 0.849 \n0.955 0.894 0.888 \n0.944 0.915 0.898 \n0.969 0.957 0.945 \n\nA-Unet \nN \n0.916 0.865 0.850 \n0.944 0.896 0.889 \n0.952 0.918 0.908 \n0.971 0.958 0.945 \nY \n0.913 0.851 0.842 \n0.942 0.872 0.866 \n0.959 0.938 0.930 \n0.969 0.953 0.944 \n\nI-Unet \nN \n0.894 0.855 0.843 \n0.941 0.882 0.879 \n0.952 0.932 0.931 \n0.980 0.969 0.954 \nY \n0.913 0.860 0.852 \n0.949 0.889 0.884 \n0.963 0.944 0.933 \n0.978 0.968 0.952 \nRCA-\nIUnet \n\nN \n0.893 0.855 0.843 \n0.948 0.883 0.880 \n0.964 0.943 0.934 \n0.982 0.969 0.959 \nY \n0.939 0.869 0.863 \n0.969 0.919 0.913 \n0.969 0.959 0.943 \n0.981 0.968 0.958 \n\nBraTS18 \n\nU-Net \nN \n0.555 0.551 0.438 \n0.601 0.589 0.532 \n0.653 0.623 0.615 \n0.652 0.623 0.614 \nY \n0.549 0.542 0.449 \n0.588 0.574 0.528 \n0.644 0.620 0.609 \n0.651 0.622 0.613 \n\nA-Unet \nN \n0.681 0.652 0.621 \n0.723 0.689 0.676 \n0.761 0.743 0.721 \n0.769 0.755 0.723 \nY \n0.653 0.622 0.599 \n0.713 0.677 0.666 \n0.758 0.741 0.719 \n0.765 0.752 0.721 \n\nI-Unet \nN \n0.690 0.673 0.620 \n0.752 0.711 0.699 \n0.798 0.763 0.754 \n0.844 0.813 0.808 \nY \n0.721 0.705 0.661 \n0.773 0.733 0.724 \n0.813 0.792 0.760 \n0.860 0.821 0.818 \nRCA-\nIUnet \n\nN \n0.773 0.734 0.713 \n0.822 0.788 0.762 \n0.864 0.834 0.811 \n0.923 0.899 0.875 \nY \n0.811 0.778 0.763 \n0.842 0.797 0.772 \n0.882 0.845 0.839 \n0.925 0.901 0.878 \n\n\nTowards a better understanding of transfer learning for medical imaging: a case study. L Alzubaidi, M A Fadhel, O Al-Shamma, Applied Sciences. 10134523Alzubaidi L, Fadhel MA, Al-Shamma O, et al (2020) Towards a better under- standing of transfer learning for medical imaging: a case study. Applied Sciences 10(13):4523\n\nSelf-labelling via simultaneous clustering and representation learning. Y M Asano, C Rupprecht, A Vedaldi, arXiv:191105371arXiv preprintAsano YM, Rupprecht C, Vedaldi A (2019) Self-labelling via simultaneous clustering and representation learning. arXiv preprint arXiv:191105371\n\nDeep clustering for unsupervised learning of visual features. M Caron, P Bojanowski, A Joulin, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Caron M, Bojanowski P, Joulin A, et al (2018) Deep clustering for unsupervised learning of visual features. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 132-149\n\nUnsupervised learning of visual features by contrasting cluster assignments. M Caron, I Misra, J Mairal, arXiv:200609882arXiv preprintCaron M, Misra I, Mairal J, et al (2020) Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:200609882\n\nContrastive learning of global and local features for medical image segmentation with limited annotations. K Chaitanya, E Erdil, N Karani, arXiv:200610511arXiv preprintChaitanya K, Erdil E, Karani N, et al (2020) Contrastive learning of global and local features for medical image segmentation with limited annotations. arXiv preprint arXiv:200610511\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, PMLRInternational conference on machine learning. Chen T, Kornblith S, Norouzi M, et al (2020) A simple framework for con- trastive learning of visual representations. In: International conference on machine learning, PMLR, pp 1597-1607\n\nExploring simple siamese representation learning. X Chen, K He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition15Chen X, He K (2021) Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 15,750-15,758\n\nXception: Deep learning with depthwise separable convolutions. F Chollet, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChollet F (2017) Xception: Deep learning with depthwise separable convo- lutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 1251-1258\n\nSelf-supervised learning for segmentation. A Dhere, J Sivaswamy, arXiv:210105456arXiv preprintDhere A, Sivaswamy J (2021) Self-supervised learning for segmentation. arXiv preprint arXiv:210105456\n\nUnsupervised visual representation learning by context prediction. C Doersch, A Gupta, A A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionDoersch C, Gupta A, Efros AA (2015) Unsupervised visual representation learning by context prediction. In: Proceedings of the IEEE international conference on computer vision, pp 1422-1430\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, arXiv:180307728arXiv preprintGidaris S, Singh P, Komodakis N (2018) Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:180307728\n\nBootstrap your own latent: A new approach to self-supervised learning. J B Grill, F Strub, F Altch\u00e9, arXiv:200607733arXiv preprintGrill JB, Strub F, Altch\u00e9 F, et al (2020) Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:200607733\n\nDeep learning approaches to biomedical image segmentation. Iri Haque, J Neubert, Informatics in Medicine. 18297Haque IRI, Neubert J (2020) Deep learning approaches to biomedical image segmentation. Informatics in Medicine Unlocked 18:100,297\n\nMomentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHe K, Fan H, Wu Y, et al (2020) Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 9729-9738\n\nSkin lesion analysis towards melanoma detection. ISIC. 15ISIC (2018) Isic 2018: Skin lesion analysis towards melanoma detection. https: //challenge2018.isic-archive.com/, [Online; accessed 15-August-2021]\n\nSelf-supervised visual feature learning with deep neural networks: A survey. L Jing, Y Tian, IEEE transactions. Jing L, Tian Y (2020) Self-supervised visual feature learning with deep neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence\n\nKaggle data science bowl challenge. Kaggle, Online; accessed 05Kaggle (2018) Kaggle data science bowl challenge 2018. https://www.kaggle .com/c/data-science-bowl-2018, [Online; accessed 05-May-2021]\n\nDeep learning applications in medical image analysis. J Ker, L Wang, J Rao, Ieee Access. 6Ker J, Wang L, Rao J, et al (2017) Deep learning applications in medical image analysis. Ieee Access 6:9375-9389\n\nMedical image segmentation using deep learning: a survey. T Lei, R Wang, Y Wan, arXiv:200913120arXiv preprintLei T, Wang R, Wan Y, et al (2020) Medical image segmentation using deep learning: a survey. arXiv preprint arXiv:200913120\n\nImbalance-aware self-supervised learning for 3d radiomic representations. H Li, F F Xue, K Chaitanya, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerLi H, Xue FF, Chaitanya K, et al (2021) Imbalance-aware self-supervised learn- ing for 3d radiomic representations. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, pp 36-46\n\nMultimodal brain tumor segmentation challenge. Miccai, Online; accessed 17-Sept-2021MICCAI (2018) Brats 2018: Multimodal brain tumor segmentation challenge. https://www.med.upenn.edu/sbia/brats2018.html, [Online; accessed 17- Sept-2021]\n\nSelf-supervised learning of pretext-invariant representations. I Misra, Maaten Lvd, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMisra I, Maaten Lvd (2020) Self-supervised learning of pretext-invariant rep- resentations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6707-6717\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, European conference on computer vision. SpringerNoroozi M, Favaro P (2016) Unsupervised learning of visual representations by solving jigsaw puzzles. In: European conference on computer vision, Springer, pp 69-84\n\nO Oktay, J Schlemper, L L Folgoc, arXiv:180403999Attention u-net: Learning where to look for the pancreas. arXiv preprintOktay O, Schlemper J, Folgoc LL, et al (2018) Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:180403999\n\nContext encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionPathak D, Krahenbuhl P, Donahue J, et al (2016) Context encoders: Feature learning by inpainting. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 2536-2544\n\nInception u-net architecture for semantic segmentation to identify nuclei in microscopy cell images. N S Punn, S Agarwal, ACM Transactions on Multimedia Computing. 161Communications, and Applications (TOMM)Punn NS, Agarwal S (2020) Inception u-net architecture for semantic seg- mentation to identify nuclei in microscopy cell images. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 16(1):1-15\n\nModality specific u-net variants for biomedical image segmentation: A survey. N S Punn, S Agarwal, arXiv:210704537arXiv preprintPunn NS, Agarwal S (2021a) Modality specific u-net variants for biomedical image segmentation: A survey. arXiv preprint arXiv:210704537\n\nRca-iunet: A residual cross-spatial attention guided inception u-net model for tumor segmentation in breast ultrasound imaging. N S Punn, S Agarwal, arXiv:210802508arXiv preprintPunn NS, Agarwal S (2021b) Rca-iunet: A residual cross-spatial attention guided inception u-net model for tumor segmentation in breast ultrasound imaging. arXiv preprint arXiv:210802508\n\nTransfusion: Understanding transfer learning for medical imaging. M Raghu, C Zhang, J Kleinberg, Advances in neural information processing systems. 32Raghu M, Zhang C, Kleinberg J, et al (2019) Transfusion: Understand- ing transfer learning for medical imaging. Advances in neural information processing systems 32\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerRonneberger O, Fischer P, Brox T (2015) U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention, Springer, pp 234-241\n\nA survey on image data augmentation for deep learning. C Shorten, T M Khoshgoftaar, Journal of Big Data. 61Shorten C, Khoshgoftaar TM (2019) A survey on image data augmentation for deep learning. Journal of Big Data 6(1):1-48\n\nA benchmark for breast ultrasound image segmentation (BUSIS). M Xian, Y Zhang, H D Cheng, Infinite StudyXian M, Zhang Y, Cheng HD, et al (2018) A benchmark for breast ultrasound image segmentation (BUSIS). Infinite Study\n\nBarlow twins: Self-supervised learning via redundancy reduction. J Zbontar, L Jing, I Misra, arXiv:210303230arXiv preprintZbontar J, Jing L, Misra I, et al (2021) Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:210303230\n\nPositional contrastive learning for volumetric medical image segmentation. D Zeng, Y Wu, X Hu, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerZeng D, Wu Y, Hu X, et al (2021) Positional contrastive learning for volumetric medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, pp 221-230\n\nHierarchical self-supervised learning for medical image segmentation based on multi-domain data aggregation. H Zheng, J Han, H Wang, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerZheng H, Han J, Wang H, et al (2021) Hierarchical self-supervised learning for medical image segmentation based on multi-domain data aggregation. In: International Conference on Medical Image Computing and Computer- Assisted Intervention, Springer, pp 622-632\n", "annotations": {"author": "[{\"end\":254,\"start\":120},{\"end\":384,\"start\":255}]", "publisher": null, "author_last_name": "[{\"end\":139,\"start\":129},{\"end\":269,\"start\":262}]", "author_first_name": "[{\"end\":128,\"start\":120},{\"end\":261,\"start\":255}]", "author_affiliation": "[{\"end\":253,\"start\":141},{\"end\":383,\"start\":271}]", "title": "[{\"end\":117,\"start\":1},{\"end\":501,\"start\":385}]", "venue": null, "abstract": "[{\"end\":2049,\"start\":675}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2295,\"start\":2278},{\"end\":2412,\"start\":2366},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2912,\"start\":2887},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2969,\"start\":2944},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3151,\"start\":3126},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3798,\"start\":3766},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4356,\"start\":4333},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4374,\"start\":4356},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4533,\"start\":4512},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5147,\"start\":5131},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5178,\"start\":5154},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5205,\"start\":5187},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5251,\"start\":5232},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5277,\"start\":5258},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5303,\"start\":5284},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5344,\"start\":5325},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5373,\"start\":5354},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5435,\"start\":5414},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5809,\"start\":5791},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7463,\"start\":7446},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8179,\"start\":8159},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8222,\"start\":8196},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8286,\"start\":8265},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8330,\"start\":8309},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8842,\"start\":8820},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9056,\"start\":9038},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9963,\"start\":9947},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12320,\"start\":12299},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12369,\"start\":12344},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13292,\"start\":13267},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13629,\"start\":13610},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13680,\"start\":13656},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13777,\"start\":13753},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14796,\"start\":14781},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15660,\"start\":15640},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27682,\"start\":27661},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27736,\"start\":27711}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27683,\"start\":27612},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27737,\"start\":27684},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27889,\"start\":27738},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28120,\"start\":27890},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28195,\"start\":28121},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28953,\"start\":28196},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29248,\"start\":28954},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32220,\"start\":29249}]", "paragraph": "[{\"end\":3152,\"start\":2065},{\"end\":4485,\"start\":3154},{\"end\":6313,\"start\":4487},{\"end\":6808,\"start\":6315},{\"end\":7209,\"start\":6810},{\"end\":7802,\"start\":7226},{\"end\":9651,\"start\":7804},{\"end\":10359,\"start\":9653},{\"end\":10614,\"start\":10371},{\"end\":11808,\"start\":10631},{\"end\":12321,\"start\":11912},{\"end\":13293,\"start\":12338},{\"end\":13552,\"start\":13369},{\"end\":14797,\"start\":13554},{\"end\":17235,\"start\":14820},{\"end\":17866,\"start\":17264},{\"end\":18539,\"start\":17868},{\"end\":19629,\"start\":18573},{\"end\":20918,\"start\":19654},{\"end\":21447,\"start\":21108},{\"end\":21685,\"start\":21547},{\"end\":22191,\"start\":21712},{\"end\":25763,\"start\":22193},{\"end\":26777,\"start\":25778},{\"end\":27219,\"start\":26794},{\"end\":27262,\"start\":27221},{\"end\":27611,\"start\":27264}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11911,\"start\":11809},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13368,\"start\":13294},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21107,\"start\":20919},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21546,\"start\":21448}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18700,\"start\":18693},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21959,\"start\":21952},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22363,\"start\":22356},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23078,\"start\":23071},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25033,\"start\":25026},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25173,\"start\":25166},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25363,\"start\":25356}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2063,\"start\":2051},{\"attributes\":{\"n\":\"2\"},\"end\":7224,\"start\":7212},{\"attributes\":{\"n\":\"3\"},\"end\":10369,\"start\":10362},{\"attributes\":{\"n\":\"3.1\"},\"end\":10629,\"start\":10617},{\"attributes\":{\"n\":\"3.2\"},\"end\":12336,\"start\":12324},{\"attributes\":{\"n\":\"4\"},\"end\":14818,\"start\":14800},{\"attributes\":{\"n\":\"5\"},\"end\":17262,\"start\":17238},{\"attributes\":{\"n\":\"5.1\"},\"end\":18571,\"start\":18542},{\"attributes\":{\"n\":\"5.2\"},\"end\":19652,\"start\":19632},{\"attributes\":{\"n\":\"6\"},\"end\":21710,\"start\":21688},{\"attributes\":{\"n\":\"7\"},\"end\":25776,\"start\":25766},{\"end\":26792,\"start\":26780},{\"end\":27619,\"start\":27613},{\"end\":27691,\"start\":27685},{\"end\":27745,\"start\":27739},{\"end\":27901,\"start\":27891},{\"end\":28128,\"start\":28122},{\"end\":28204,\"start\":28197},{\"end\":28962,\"start\":28955},{\"end\":29257,\"start\":29250}]", "table": "[{\"end\":28953,\"start\":28260},{\"end\":29248,\"start\":29113},{\"end\":32220,\"start\":29259}]", "figure_caption": "[{\"end\":27683,\"start\":27621},{\"end\":27737,\"start\":27693},{\"end\":27889,\"start\":27747},{\"end\":28120,\"start\":27906},{\"end\":28195,\"start\":28130},{\"end\":28260,\"start\":28206},{\"end\":29113,\"start\":28964}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11170,\"start\":11164},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12453,\"start\":12447},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15135,\"start\":15129},{\"end\":22050,\"start\":22044},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22068,\"start\":22062},{\"end\":22536,\"start\":22530},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22931,\"start\":22925},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23325,\"start\":23319},{\"end\":23528,\"start\":23522},{\"end\":23905,\"start\":23899},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24171,\"start\":24165},{\"end\":24711,\"start\":24705}]", "bib_author_first_name": "[{\"end\":32310,\"start\":32309},{\"end\":32323,\"start\":32322},{\"end\":32325,\"start\":32324},{\"end\":32335,\"start\":32334},{\"end\":32615,\"start\":32614},{\"end\":32617,\"start\":32616},{\"end\":32626,\"start\":32625},{\"end\":32639,\"start\":32638},{\"end\":32885,\"start\":32884},{\"end\":32894,\"start\":32893},{\"end\":32908,\"start\":32907},{\"end\":33300,\"start\":33299},{\"end\":33309,\"start\":33308},{\"end\":33318,\"start\":33317},{\"end\":33614,\"start\":33613},{\"end\":33627,\"start\":33626},{\"end\":33636,\"start\":33635},{\"end\":33930,\"start\":33929},{\"end\":33938,\"start\":33937},{\"end\":33951,\"start\":33950},{\"end\":34250,\"start\":34249},{\"end\":34258,\"start\":34257},{\"end\":34653,\"start\":34652},{\"end\":35027,\"start\":35026},{\"end\":35036,\"start\":35035},{\"end\":35248,\"start\":35247},{\"end\":35259,\"start\":35258},{\"end\":35268,\"start\":35267},{\"end\":35270,\"start\":35269},{\"end\":35658,\"start\":35657},{\"end\":35669,\"start\":35668},{\"end\":35678,\"start\":35677},{\"end\":35930,\"start\":35929},{\"end\":35932,\"start\":35931},{\"end\":35941,\"start\":35940},{\"end\":35950,\"start\":35949},{\"end\":36195,\"start\":36192},{\"end\":36204,\"start\":36203},{\"end\":36444,\"start\":36443},{\"end\":36450,\"start\":36449},{\"end\":36457,\"start\":36456},{\"end\":37095,\"start\":37094},{\"end\":37103,\"start\":37102},{\"end\":37547,\"start\":37546},{\"end\":37554,\"start\":37553},{\"end\":37562,\"start\":37561},{\"end\":37755,\"start\":37754},{\"end\":37762,\"start\":37761},{\"end\":37770,\"start\":37769},{\"end\":38005,\"start\":38004},{\"end\":38011,\"start\":38010},{\"end\":38013,\"start\":38012},{\"end\":38020,\"start\":38019},{\"end\":38658,\"start\":38657},{\"end\":38672,\"start\":38666},{\"end\":39096,\"start\":39095},{\"end\":39107,\"start\":39106},{\"end\":39331,\"start\":39330},{\"end\":39340,\"start\":39339},{\"end\":39353,\"start\":39352},{\"end\":39355,\"start\":39354},{\"end\":39638,\"start\":39637},{\"end\":39648,\"start\":39647},{\"end\":39662,\"start\":39661},{\"end\":40110,\"start\":40109},{\"end\":40112,\"start\":40111},{\"end\":40120,\"start\":40119},{\"end\":40516,\"start\":40515},{\"end\":40518,\"start\":40517},{\"end\":40526,\"start\":40525},{\"end\":40831,\"start\":40830},{\"end\":40833,\"start\":40832},{\"end\":40841,\"start\":40840},{\"end\":41134,\"start\":41133},{\"end\":41143,\"start\":41142},{\"end\":41152,\"start\":41151},{\"end\":41449,\"start\":41448},{\"end\":41464,\"start\":41463},{\"end\":41475,\"start\":41474},{\"end\":41853,\"start\":41852},{\"end\":41864,\"start\":41863},{\"end\":41866,\"start\":41865},{\"end\":42087,\"start\":42086},{\"end\":42095,\"start\":42094},{\"end\":42104,\"start\":42103},{\"end\":42106,\"start\":42105},{\"end\":42312,\"start\":42311},{\"end\":42323,\"start\":42322},{\"end\":42331,\"start\":42330},{\"end\":42582,\"start\":42581},{\"end\":42590,\"start\":42589},{\"end\":42596,\"start\":42595},{\"end\":43029,\"start\":43028},{\"end\":43038,\"start\":43037},{\"end\":43045,\"start\":43044}]", "bib_author_last_name": "[{\"end\":32320,\"start\":32311},{\"end\":32332,\"start\":32326},{\"end\":32345,\"start\":32336},{\"end\":32623,\"start\":32618},{\"end\":32636,\"start\":32627},{\"end\":32647,\"start\":32640},{\"end\":32891,\"start\":32886},{\"end\":32905,\"start\":32895},{\"end\":32915,\"start\":32909},{\"end\":33306,\"start\":33301},{\"end\":33315,\"start\":33310},{\"end\":33325,\"start\":33319},{\"end\":33624,\"start\":33615},{\"end\":33633,\"start\":33628},{\"end\":33643,\"start\":33637},{\"end\":33935,\"start\":33931},{\"end\":33948,\"start\":33939},{\"end\":33959,\"start\":33952},{\"end\":34255,\"start\":34251},{\"end\":34261,\"start\":34259},{\"end\":34661,\"start\":34654},{\"end\":35033,\"start\":35028},{\"end\":35046,\"start\":35037},{\"end\":35256,\"start\":35249},{\"end\":35265,\"start\":35260},{\"end\":35276,\"start\":35271},{\"end\":35666,\"start\":35659},{\"end\":35675,\"start\":35670},{\"end\":35688,\"start\":35679},{\"end\":35938,\"start\":35933},{\"end\":35947,\"start\":35942},{\"end\":35957,\"start\":35951},{\"end\":36201,\"start\":36196},{\"end\":36212,\"start\":36205},{\"end\":36447,\"start\":36445},{\"end\":36454,\"start\":36451},{\"end\":36460,\"start\":36458},{\"end\":37100,\"start\":37096},{\"end\":37108,\"start\":37104},{\"end\":37334,\"start\":37328},{\"end\":37551,\"start\":37548},{\"end\":37559,\"start\":37555},{\"end\":37566,\"start\":37563},{\"end\":37759,\"start\":37756},{\"end\":37767,\"start\":37763},{\"end\":37774,\"start\":37771},{\"end\":38008,\"start\":38006},{\"end\":38017,\"start\":38014},{\"end\":38030,\"start\":38021},{\"end\":38409,\"start\":38403},{\"end\":38664,\"start\":38659},{\"end\":38676,\"start\":38673},{\"end\":39104,\"start\":39097},{\"end\":39114,\"start\":39108},{\"end\":39337,\"start\":39332},{\"end\":39350,\"start\":39341},{\"end\":39362,\"start\":39356},{\"end\":39645,\"start\":39639},{\"end\":39659,\"start\":39649},{\"end\":39670,\"start\":39663},{\"end\":40117,\"start\":40113},{\"end\":40128,\"start\":40121},{\"end\":40523,\"start\":40519},{\"end\":40534,\"start\":40527},{\"end\":40838,\"start\":40834},{\"end\":40849,\"start\":40842},{\"end\":41140,\"start\":41135},{\"end\":41149,\"start\":41144},{\"end\":41162,\"start\":41153},{\"end\":41461,\"start\":41450},{\"end\":41472,\"start\":41465},{\"end\":41480,\"start\":41476},{\"end\":41861,\"start\":41854},{\"end\":41879,\"start\":41867},{\"end\":42092,\"start\":42088},{\"end\":42101,\"start\":42096},{\"end\":42112,\"start\":42107},{\"end\":42320,\"start\":42313},{\"end\":42328,\"start\":42324},{\"end\":42337,\"start\":42332},{\"end\":42587,\"start\":42583},{\"end\":42593,\"start\":42591},{\"end\":42599,\"start\":42597},{\"end\":43035,\"start\":43030},{\"end\":43042,\"start\":43039},{\"end\":43050,\"start\":43046}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":225769039},\"end\":32540,\"start\":32222},{\"attributes\":{\"doi\":\"arXiv:191105371\",\"id\":\"b1\"},\"end\":32820,\"start\":32542},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49865868},\"end\":33220,\"start\":32822},{\"attributes\":{\"doi\":\"arXiv:200609882\",\"id\":\"b3\"},\"end\":33504,\"start\":33222},{\"attributes\":{\"doi\":\"arXiv:200610511\",\"id\":\"b4\"},\"end\":33856,\"start\":33506},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b5\",\"matched_paper_id\":211096730},\"end\":34197,\"start\":33858},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":227118869},\"end\":34587,\"start\":34199},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2375110},\"end\":34981,\"start\":34589},{\"attributes\":{\"doi\":\"arXiv:210105456\",\"id\":\"b8\"},\"end\":35178,\"start\":34983},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9062671},\"end\":35587,\"start\":35180},{\"attributes\":{\"doi\":\"arXiv:180307728\",\"id\":\"b10\"},\"end\":35856,\"start\":35589},{\"attributes\":{\"doi\":\"arXiv:200607733\",\"id\":\"b11\"},\"end\":36131,\"start\":35858},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":213875226},\"end\":36374,\"start\":36133},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207930212},\"end\":36809,\"start\":36376},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":199457983},\"end\":37015,\"start\":36811},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":62841734},\"end\":37290,\"start\":37017},{\"attributes\":{\"id\":\"b16\"},\"end\":37490,\"start\":37292},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206491372},\"end\":37694,\"start\":37492},{\"attributes\":{\"doi\":\"arXiv:200913120\",\"id\":\"b18\"},\"end\":37928,\"start\":37696},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":232146794},\"end\":38354,\"start\":37930},{\"attributes\":{\"id\":\"b20\"},\"end\":38592,\"start\":38356},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208617491},\"end\":39018,\"start\":38594},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":187547},\"end\":39328,\"start\":39020},{\"attributes\":{\"doi\":\"arXiv:180403999\",\"id\":\"b23\"},\"end\":39585,\"start\":39330},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2202933},\"end\":40006,\"start\":39587},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":214794035},\"end\":40435,\"start\":40008},{\"attributes\":{\"doi\":\"arXiv:210704537\",\"id\":\"b26\"},\"end\":40700,\"start\":40437},{\"attributes\":{\"doi\":\"arXiv:210802508\",\"id\":\"b27\"},\"end\":41065,\"start\":40702},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":170078822},\"end\":41381,\"start\":41067},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3719281},\"end\":41795,\"start\":41383},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":195811894},\"end\":42022,\"start\":41797},{\"attributes\":{\"id\":\"b31\"},\"end\":42244,\"start\":42024},{\"attributes\":{\"doi\":\"arXiv:210303230\",\"id\":\"b32\"},\"end\":42504,\"start\":42246},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":235458627},\"end\":42917,\"start\":42506},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":235795346},\"end\":43407,\"start\":42919}]", "bib_title": "[{\"end\":32307,\"start\":32222},{\"end\":32882,\"start\":32822},{\"end\":33927,\"start\":33858},{\"end\":34247,\"start\":34199},{\"end\":34650,\"start\":34589},{\"end\":35245,\"start\":35180},{\"end\":36190,\"start\":36133},{\"end\":36441,\"start\":36376},{\"end\":36858,\"start\":36811},{\"end\":37092,\"start\":37017},{\"end\":37544,\"start\":37492},{\"end\":38002,\"start\":37930},{\"end\":38655,\"start\":38594},{\"end\":39093,\"start\":39020},{\"end\":39635,\"start\":39587},{\"end\":40107,\"start\":40008},{\"end\":41131,\"start\":41067},{\"end\":41446,\"start\":41383},{\"end\":41850,\"start\":41797},{\"end\":42579,\"start\":42506},{\"end\":43026,\"start\":42919}]", "bib_author": "[{\"end\":32322,\"start\":32309},{\"end\":32334,\"start\":32322},{\"end\":32347,\"start\":32334},{\"end\":32625,\"start\":32614},{\"end\":32638,\"start\":32625},{\"end\":32649,\"start\":32638},{\"end\":32893,\"start\":32884},{\"end\":32907,\"start\":32893},{\"end\":32917,\"start\":32907},{\"end\":33308,\"start\":33299},{\"end\":33317,\"start\":33308},{\"end\":33327,\"start\":33317},{\"end\":33626,\"start\":33613},{\"end\":33635,\"start\":33626},{\"end\":33645,\"start\":33635},{\"end\":33937,\"start\":33929},{\"end\":33950,\"start\":33937},{\"end\":33961,\"start\":33950},{\"end\":34257,\"start\":34249},{\"end\":34263,\"start\":34257},{\"end\":34663,\"start\":34652},{\"end\":35035,\"start\":35026},{\"end\":35048,\"start\":35035},{\"end\":35258,\"start\":35247},{\"end\":35267,\"start\":35258},{\"end\":35278,\"start\":35267},{\"end\":35668,\"start\":35657},{\"end\":35677,\"start\":35668},{\"end\":35690,\"start\":35677},{\"end\":35940,\"start\":35929},{\"end\":35949,\"start\":35940},{\"end\":35959,\"start\":35949},{\"end\":36203,\"start\":36192},{\"end\":36214,\"start\":36203},{\"end\":36449,\"start\":36443},{\"end\":36456,\"start\":36449},{\"end\":36462,\"start\":36456},{\"end\":37102,\"start\":37094},{\"end\":37110,\"start\":37102},{\"end\":37336,\"start\":37328},{\"end\":37553,\"start\":37546},{\"end\":37561,\"start\":37553},{\"end\":37568,\"start\":37561},{\"end\":37761,\"start\":37754},{\"end\":37769,\"start\":37761},{\"end\":37776,\"start\":37769},{\"end\":38010,\"start\":38004},{\"end\":38019,\"start\":38010},{\"end\":38032,\"start\":38019},{\"end\":38411,\"start\":38403},{\"end\":38666,\"start\":38657},{\"end\":38678,\"start\":38666},{\"end\":39106,\"start\":39095},{\"end\":39116,\"start\":39106},{\"end\":39339,\"start\":39330},{\"end\":39352,\"start\":39339},{\"end\":39364,\"start\":39352},{\"end\":39647,\"start\":39637},{\"end\":39661,\"start\":39647},{\"end\":39672,\"start\":39661},{\"end\":40119,\"start\":40109},{\"end\":40130,\"start\":40119},{\"end\":40525,\"start\":40515},{\"end\":40536,\"start\":40525},{\"end\":40840,\"start\":40830},{\"end\":40851,\"start\":40840},{\"end\":41142,\"start\":41133},{\"end\":41151,\"start\":41142},{\"end\":41164,\"start\":41151},{\"end\":41463,\"start\":41448},{\"end\":41474,\"start\":41463},{\"end\":41482,\"start\":41474},{\"end\":41863,\"start\":41852},{\"end\":41881,\"start\":41863},{\"end\":42094,\"start\":42086},{\"end\":42103,\"start\":42094},{\"end\":42114,\"start\":42103},{\"end\":42322,\"start\":42311},{\"end\":42330,\"start\":42322},{\"end\":42339,\"start\":42330},{\"end\":42589,\"start\":42581},{\"end\":42595,\"start\":42589},{\"end\":42601,\"start\":42595},{\"end\":43037,\"start\":43028},{\"end\":43044,\"start\":43037},{\"end\":43052,\"start\":43044}]", "bib_venue": "[{\"end\":32363,\"start\":32347},{\"end\":32612,\"start\":32542},{\"end\":32981,\"start\":32917},{\"end\":33297,\"start\":33222},{\"end\":33611,\"start\":33506},{\"end\":34009,\"start\":33965},{\"end\":34344,\"start\":34263},{\"end\":34740,\"start\":34663},{\"end\":35024,\"start\":34983},{\"end\":35345,\"start\":35278},{\"end\":35655,\"start\":35589},{\"end\":35927,\"start\":35858},{\"end\":36237,\"start\":36214},{\"end\":36543,\"start\":36462},{\"end\":36864,\"start\":36860},{\"end\":37127,\"start\":37110},{\"end\":37326,\"start\":37292},{\"end\":37579,\"start\":37568},{\"end\":37752,\"start\":37696},{\"end\":38118,\"start\":38032},{\"end\":38401,\"start\":38356},{\"end\":38759,\"start\":38678},{\"end\":39154,\"start\":39116},{\"end\":39435,\"start\":39379},{\"end\":39749,\"start\":39672},{\"end\":40170,\"start\":40130},{\"end\":40513,\"start\":40437},{\"end\":40828,\"start\":40702},{\"end\":41213,\"start\":41164},{\"end\":41568,\"start\":41482},{\"end\":41900,\"start\":41881},{\"end\":42084,\"start\":42024},{\"end\":42309,\"start\":42246},{\"end\":42687,\"start\":42601},{\"end\":43138,\"start\":43052},{\"end\":33032,\"start\":32983},{\"end\":34412,\"start\":34346},{\"end\":34804,\"start\":34742},{\"end\":35399,\"start\":35347},{\"end\":36611,\"start\":36545},{\"end\":38827,\"start\":38761},{\"end\":39813,\"start\":39751}]"}}}, "year": 2023, "month": 12, "day": 17}