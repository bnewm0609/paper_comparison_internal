{"id": 196171043, "updated": "2023-03-24 07:36:14.552", "metadata": {"title": "Buying or Browsing?: Predicting Real-time Purchasing Intent using Attention-based Deep Network with Multiple Behavior", "authors": "[{\"first\":\"Long\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Lifeng\",\"last\":\"Hua\",\"middle\":[]},{\"first\":\"Rongfei\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Binqiang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Xiaobo\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Cui\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "E-commerce platforms are becoming a primary place for people to find, compare and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to predict user purchasing intent, which is an important part of user understanding and allows for providing better services for both sellers and customers. However, previous work cannot predict real-time user purchasing intent with a high accuracy, limited by the representation capability of traditional browse-interactive behavior adopted. In this paper, we propose a novel end-to-end deep network, named Deep Intent Prediction Network (DIPN), to predict real-time user purchasing intent. In particular, besides the traditional browse-interactive behavior, we collect a new type of user interactive behavior, called touch-interactive behavior, which can capture more fine-grained real-time user features. To combine these behavior effectively, we propose a hierarchical attention mechanism, where the bottom attention layer focuses on the inner parts of each behavior sequence while the top attention layer learns the inter-view relations between different behavior sequences. In addition, we propose to train DIPN with multi-task learning to better distinguish user behavior patterns. In the experiments conducted on a large-scale industrial dataset, DIPN significantly outperforms the baseline solutions. Notably, DIPN gains about 18.96% improvement on AUC than the state-of-the-art solution only using traditional browse-interactive behavior sequences. Moreover, DIPN has been deployed in the operational system of Taobao. Online A/B testing results with more than 12.9 millions of users reveal the potential of knowing users' real-time purchasing intent.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2951045934", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kdd/GuoHJZW019", "doi": "10.1145/3292500.3330670"}}, "content": {"source": {"pdf_hash": "fb9ddc74bb9371bb0d6b31fe859ecd69e0fdb41d", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f42cea0c5a84c4d2fa9dfb028796af07ef9d3134", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fb9ddc74bb9371bb0d6b31fe859ecd69e0fdb41d.txt", "contents": "\nBuying or Browsing? : Predicting Real-time Purchasing Intent using Attention-based Deep Network with Multiple Behavior\nAugust 4-8, 2019. 2019. August 4-8, 2019\n\nLong Guo \nLifeng Hua \nRongfei Jia \nBinqiang Zhao \nXiaobo Wang \nBin Cui bin.cui@pku.edu.cn \nSchool of EECS Key Laboratory of High Confidence Software Technologies (MOE)\nPeking University\n\n\nLong Guo \nLifeng Hua \nRongfei Jia \nBinqiang Zhao \nBinXiaobo Wang \nSchool of EECS Key Laboratory of High Confidence Software Technologies (MOE)\nPeking University\n\n\nCui \u2020 \n\nAlibaba Group\nBeijing & HangzhouChina\n\nBuying or Browsing? : Predicting Real-time Purchasing Intent using Attention-based Deep Network with Multiple Behavior\n\nThe 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '19)\nAnchorage, AK, USA; Anchorage, AK, USAAugust 4-8, 2019. 2019. August 4-8, 201910.1145/3292500.3330670ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00. ACM, New York, NY, USA, 9 pages.CCS CONCEPTSInformation systems \u2192 Recommender systems;Applied computing \u2192 Online shopping KEYWORDS e-commerce, recommendation system, purchasing intent prediction, hierarchical attention mechanism, multiple behavior\nE-commerce platforms are becoming a primary place for people to find, compare and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to predict user purchasing intent, which is an important part of user understanding and allows for providing better services for both sellers and customers. However, previous work cannot predict real-time user purchasing intent with a high accuracy, limited by the representation capability of traditional browse-interactive behavior adopted. In this paper, we propose a novel end-to-end deep network, named Deep Intent Prediction Network (DIPN), to predict real-time user purchasing intent. In particular, besides the traditional browseinteractive behavior, we collect a new type of user interactive behavior, called touch-interactive behavior, which can capture more fine-grained real-time user features. To combine these behavior effectively, we propose a hierarchical attention mechanism, where the bottom attention layer focuses on the inner parts of each behavior sequence while the top attention layer learns the inter-view relations between different behavior sequences. In addition, we propose to train DIPN with multi-task learning to better distinguish user behavior patterns. In the experiments conducted on a large-scale industrial dataset, DIPN significantly outperforms the baseline solutions. Notably, DIPN gains about 18.96% improvement on AUC than the state-of-the-art solution only using traditional browse-interactive behavior sequences. Moreover, DIPN has been deployed in the operational system of Taobao. Online A/B testing results with more than 12.9 millions of users reveal the potential of knowing users' real-time purchasing intent.\n\nINTRODUCTION\n\nIn the internet era, large e-commerce platforms such as Taobao and Amazon are becoming a primary place for people to find, compare and ultimately purchase products. As an important part of user understanding, it is crucial to know whether a customer is buying or just browsing on an e-commerce application, as it allows for providing better services for both sellers and customers. From the perspective of the sellers, knowing users' current purchasing intent can increase their sales volume and profit margin. When the e-commerce platform has increased confidence that a subset of users are more likely to purchase, it can perform some proactive actions to maximize conversion based on this information. The platform may offer time-limited coupons or create bundles of complementary products to push the users to complete their purchases. From the perspective of the customers, recognizing users' current buying or browsing intent is vital for the e-commerce platform to set appropriate strategies for the recommendation system and search engine to improve user experience.\n\nPrevious studies focus on leveraging traditional user behavior, which we call browse-interactive behavior, to predict users' purchasing intent [15,18,19,23]. However, limited by the representation capability and frequency of occurrence of the browse-interactive behavior, e.g., browse, search or collect a product, it is hard to predict users' real-time purchasing intent depending solely on these actions. In other words, these actions contain insufficient information about user behavior patterns that would lead to a purchase in a short time. The purchase intent of a customer may slowly build over time and may not instantaneously lead to a purchase. As a result, it is challenging to identify the moment when the customer finally places the order. To this end, we need some more fine-grained behavior data to model user purchasing behavior patterns. In order to capture users' real-time purchasing intent, we collect a new type of interactive behavior data, which we call touchinteractive behavior. With the rapid development of hardware and software on mobile devices, we take advantage of the sensors and accelerometers of the mobile devices to automatically glean the real-time context of user interactions, such as the swipe and tap actions. Compared with the browse-interactive actions, the touchinteractive actions occur more frequently. As shown in Table 1, the number of swipe actions and tap actions generated per user per day are 37.7 times and 9.3 times more than that of the browseinteractive actions, respectively. As a result, the touch-interactive behavior contains more rich information about user behavior patterns. For example, we find that some customers would browse the product comments for a long time before they place the order. Such typical patterns can be easily captured by using the touchinteractive behavior. By combing the traditional browse-interactive behavior and the new touch-interactive behavior, we are able to model the user behavior patterns more comprehensively.\n\nHowever, there exist several challenges in predicting users' realtime purchasing intent. First, the touch-interactive behavior contains less semantic information than the browse-interactive behavior. Therefore, it is challenging to extract useful features from these data to improve the prediction performance. Second, it is necessary to figure out an effective fusion mechanism to combine the browseinteractive behavior and the touch-interactive behavior in order to bring their advantages into full play. Third, due to the complexity of the browsing behavior where the customers with different purchasing intent can appear to be very similar, it is essential to capture common features that can well depict the customers and unique features that would lead to different purchasing behavior.\n\nIn this paper, we propose a novel end-to-end deep network, named Deep Intent Prediction Network (DIPN), for the real-time purchasing intent prediction. In DIPN, the user behavior features are automatically learned from the raw data without the need of extensive feature engineering. In particular, we propose a hierarchical attention mechanism to fuse the views extracted from different interactive behavior sources. In the bottom attention layer, we design an intra-view attention mechanism which focuses on the inner parts of the behavior sequence. In the top attention layer, we propose an inter-view attention mechanism that learns the inter-view relations between different behavior sequences. In addition, we propose to train the real-time and long-term purchasing intent simultaneously with the same model. With the multi-task learning, DIPN can capture common features that well depict the customers and unique features that would lead to different purchasing behavior. The contribution of the paper can be summarized as follows:\n\n\u2022 We collect a new type of user behavior, the touch-interactive behavior, which contains rich information about user behavior patterns. Together with the traditional browse-interactive behavior, we are able to depict a user from different views for better performance of purchasing intent prediction. \u2022 We propose a deep network DIPN for real-time purchasing intent prediction. A novel hierarchical attention mechanism is proposed to fuse multiple views extracted from different interactive behavior sources. In addition, multi-task learning is introduced to better distinguish user behavior patterns. \u2022 We conduct extensive experiments to evaluate the performance of DIPN in both offline and online settings. Experimental results on a large-scale industrial dataset shows the superiority of DIPN in predicting purchasing intent. In particular, DIPN has been deployed in the operational system of Taobao and adopted in the coupon allocation task at a shopping festival. Online A/B testing shows the benefits of knowing users' real-time purchasing intent.\n\nThe rest of the paper is organized as follows. We discuss related work in Section 2, followed by the data description in Section 3. We describe the design of DIPN model in Section 4 and give an overview of the deployment of DIPN in Section 5. We present experiments in Section 6 and conclude the paper in Section 7.\n\n\nRELATED WORK 2.1 Purchasing Intent Prediction\n\nThe problem of purchasing intent prediction has been heavily studied, with a variety of classic machine learning and deep learning modelling techniques employed. The earliest work come from the RecSys 2015 challenge [2], which provides a public dataset consisting of 9.2 million user-item click sessions. Given a session, the goal of the challenge is to predict whether the user is going to buy something or not within this session. Romov et al. [15] won the competition using GBM with extensive feature engineering on session summarizing. The other feature-based work includes the ensemble model with neural net and GBM used by [23] and the deep belief networks and stacked denoising auto-encoders by [22]. To reduce the feature engineering work, several work [19,20,25] adopt the recurrent neural network (RNN) to model the sequence nature of sessions, where a bi-directional LSTM is used in [19,25] and a mixture of LSTM is used in [20].\n\nOur work is distinguished from previous work in the following aspects. First, given a history session, our goal is to predict a user's subsequent purchasing behavior within a given time, while the goal of previous work is to predict the purchasing behavior within the session. Our setting is more realistic because in reality we should predict the future behavior based on the current incomplete session. Second, the key difference of our work is that we collect touchinteractive actions to capture the real-time user behavior patterns. As a result, we need to handle several data sources in our model while previous work only deal with a single source.\n\n\nSequence Classification\n\nThe task of purchasing intent prediction is closely related to sequence classification. A brief survey by [26] categorizes the sequence classification methods into three groups: feature based methods [1,13,29], sequence distance based methods [10,17,24], and model based methods [4,27,31]. Our work is related to the model based approach, where we use an end-to-end deep network to model the sequences and save extensive feature engineering work. Our work is also related to sentence classification in natural language processing [9,11,30]. Text sentences and time series data are similar to each other in that they are both ordered sequences in nature. However, the semantic information contained in these two kinds of sequences are definitely different. Our work differs from the previous work in that we need to handle several different data sources with different formats while in traditional sequence classification, the data usually comes from a single source.  \n\n\nMulti-task Learning\n\nMulti-task learning has been used successfully across various applications of machine learning, from natural language processing [3,5] and speech recognition [6] to computer vision [8] and recommender systems [14]. By sharing representations between related tasks, multi-task learning can enable the model to capture more underlying factors and generalize better on its original task. Ruder [16] presents an overview of multi-task learning in deep learning, where multi-task learning is typically done with either hard or soft parameter sharing of hidden layers. The hard parameter sharing method is the most commonly used multi-task learning approach, which shares the hidden layers between all tasks and keeps several taskspecific output layers. Collobert et al. [5] simultaneously learn several NLP tasks using a language model with embedding lookup table sharing. In [8], multi-task learning is adopted to improve the performance of classifying object proposals using deep convolutional networks. Ni et al. [14] use deep multi-task representation learning to generate user representations for personalization in e-commerce portal. In soft parameter sharing, each task has its own model with its own parameters where the distance between the parameters is regularized. Duong et al. [7] uses l 2 distance for regularization while Yang et al. [28] use the trace norm. Our model is related to the hard parameter sharing method. We propose a novel way by partitioning a user's purchasing intent into three different phases and use multi-task learning to learn the unique behavior that would lead to different purchasing intent.\n\nTo the best of our knowledge, our work is the first study that uses the attention-based deep network with multi-learning on multiple user behavior sequences for real-time purchasing intent prediction.\n\n\nDATASET\n\nWe build two types of user interactive behavior dataset, i.e., the new touch-interactive behavior and the traditional browse-interactive behavior. In the following, we describe each dataset in details.\n\n\nTouch-interactive Behavior\n\nThe touch-interactive behavior dataset contains normal users' daily touch-interactive information when using the Taobao app, which is composed of the swipe-interactive and tap-interactive behavior.\n\nThe swipe-interactive behavior. This behavior includes four types of basic actions, i.e., Open Page, Leave Page, Swipe and Tap. Table 2a shows an example of raw data in the swipe-interactive behavior. A user's swipe-interactive track is a time sequence of actions, consisting of these four basic types of actions. Each action has a timestamp and a page index to identify when and where the action occurs. In addition, the positional coordinates of the action on the touch screen are also recorded. The duration presents how long the action lasts. As shown in Table 3a, for each action at a data point, we extract 14 raw features. The time duration of a swipe, time gap between two actions and positional coordinates of actions are continuous variables. Page indices, action indices and swipe directions (i.e., left/right and up/down) are categorical variables. We conduct discretization on all the raw features to ensure unified inputs for DIPN. The discretization of the continuous variables is described as follows:\n\n\u2022 Position. The positional coordinates of actions are continuous values, and are discretized according to the resolution of the touch screen. We divide the width of the screen into 17 uniform segments, and the length into 25 segments for one-hot vectors encoding. \u2022 Swipe Length. The length of a swipe is encoded into a onehot vector, the length of which is as twice as the length of the one-hot vectors of position encoding. The reason of applying twice length is that, for a swipe track, we consider the direction of the swipe. \u2022 Time Gap and Duration. We apply a step function to encode time gaps between actions and swipe duration as follow:\ny = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u230ax/f s \u230b, x < f b \u230ax/f b + 9\u230b, f b \u2264 x < 10 \u00d7 f b 19, x \u2265 10 \u00d7 f b where { f s = 100, f b = 1000} are used for time gap, and { f s = 25, f b = 250} are used for time duration.\nThe tap-interactive behavior. This behavior records the information associated with the tap actions, as shown in Table 2b. A user's tap-interactive track is a time sequence of tap actions. Each action has a timestamp and page index to identify when and where the action occurs. There is also an event id to identify whether a user taps on a page or a button. If a button is tapped, the button name is also recorded. As shown in  \n\n\nBrowse-interactive Behavior\n\nThe browse-interactive behavior represents the typical behavior users conduct on products when browsing an e-commerce application. It includes five types of actions, i.e., browse a product, search a product, collect a product, add a product to cart and purchase a product. A user's browse-interactive track is a time sequence of these actions. As shown in Table 3c, we extract 6 raw features for each action. The type index represents the type of an action. For the continuous variables, i.e., page stay time and timestamp, we perform similar discretization operation as for the time gap in the swipe-interactive behavior. A user's purchase behavior has a high correlation with her historical behavior, which can represent the activeness of the user. Active users who behave more frequently in the history may keep this trend in the future, while those who are inactive may stay quiet for a long time. Therefore, we also collect some statistics for the historical behavior based on the browse-interactive behavior. In more details, we count up the frequency of three types of actions in different time windows. Table 3d shows the features extracted for each action, where C.F., A.F. and P.F. represent the frequency of collecting a product, adding a product to cart and purchasing a product, respectively. For each action, we count the frequency of the action within one week, two weeks, one month, three months, six months and one year. To improve personalization, we also collect the user profile dataset containing various users' basic information, such as the age level and gender, as shown in Table 3e.\n\n\nMODEL ARCHITECTURE\n\nWe propose a novel deep network, named Deep Intent Prediction Network (DIPN), for the real-time purchasing intent prediction. Figure 1 shows the model architecture of DIPN. It includes an embedding lookup layer which embeds the one-hot vectors of the raw action features into dense vectors, followed by a fully-connected layer. After that, a bidirectional recurrent layer is applied to each user behavior sequence to model the long-term dependencies between complex user actions. Then a hierarchical attention layer is applied to fuse the outputs from the recurrent layer. Finally, DIPN is trained with multi-task learning. In the following, we will introduce each component of DIPN in details.\n\n\nEmbedding Layer\n\nAs introduced in Section 3, we use five groups of features in DIPN, i.e., User Swipe-interactive Behavior, User Touch-interactive Behavior, User Browse-interactive Behavior, User History and User Profile. In the discretization process, the raw values of every feature are encoded into one-hot vectors, the length of which is shown in the Dictionary Dim column of Table 3. Then the one-hot vectors are used as the inputs of DIPN. As the inputs are high dimensional binary vectors, we use the embedding layer to transform them into low dimensional dense representations. The embedding operation follows the table lookup mechanism. In more details, each feature is corresponding to one embedding matrix. For example, the embedding matrix of the Button Index feature in Table 3b is represented as E but t on = [e 1 ; e 2 ; ...; e n ] \u2208 R n e \u00d7n b , where e i \u2208 R n e represents an embedding vector with dimension n e , and n b represents the number of buttons that a user can tap. The embedding vector of the Button Index feature can then be obtained as E but t on \u00b7 B but t on \u2208 R n e , where B but ton \u2208 R n b is the one-hot vector of the Button Index feature. The length of every embedded feature is shown in the Embedding Dim column of Table 3. At last, for each feature group, all the embedded features are concatenated into a vector and fed into a fully-connected layer for reshape. During the training process, the embedding layer is trained at the same time with the model.\n\n\nRNN Layer\n\nThe user interactive behavior used in DIPN are all time sequence of actions. Therefore, we use RNN to model the long-term dependencies between actions. The adoption of RNN can eliminate the need for extensive feature engineering, which is very helpful because it is difficult to extract features from the touch-interactive behavior composed of swipe or tap actions with little semantic information.\n\nTo avoid the vanishing gradient problem suffered by the standard RNN, LSTM and GRU are proposed to control the update of the information via gates. We take GRU to model the dependency because GRU is faster than LSTM and more suitable for e-commerce system. The formulations of GRU are listed as follows:\nr t = \u03c3 (W er e t + W hr h t \u22121 + b r ) z t = \u03c3 (W ez e t + W hz h t \u22121 + b z ) h t = tanh(W eh e t + W hh (r t \u2299 h t \u22121 ) + b h ) h t = z t \u2299 h t \u22121 + (1 \u2212 z t ) \u2299h t .(1)\nwhere e t is embedding vector of the t-th action, h t is the t-th hidden states, \u03c3 is the sigmoid function and \u2299 is the element-wise product operator. To better capture the global information of the behavior sequences, we adopt a bidirectional recurrent layer composed of two GRU layers working in opposite directions. We obtain the representation of the t-th action by concatenating the forward hidden state \u2212 \u2192 h t and backward hidden states\n\u2190 \u2212 h t , i.e., h t = [ \u2212 \u2192 h t , \u2190 \u2212 h t ].\nIn this way, a behavior sequence is represented as h = {h 1 , h 2 , ..., h n } \u2208 R n\u00d72d , where d is the dimension of the hidden state .\n\nIn DIPN, we need to handle three types of behavior sequences, i.e., the swipe-interactive sequence, the tap-interactive sequence and the browse-interactive sequence. There are two ways to fuse these sequences: early fusion and late fusion. The early fusion refers to aligning the three sequences by timestamp before feeding them into a single GRU model, while the late fusion refers to first feeding each sequence to a separate GRU model and then concatenating the output hidden features. One disadvantage of the early fusion method is that the behavior sequences usually have different densities, as shown in Table 1. When aligning the sequences by timestamp, dense sequence could dominate the concatenated feature space and override the effects of sparse but important sequence. In addition, since the length of GRU model is limited, the early fusion method would result in information loss of the dense sequence when truncating the sessions. Therefore, we propose to use the late fusion method and feed the three behavior sequences to separate Bi-GRU models, as shown in Figure 1. After the RNN layer, we get three hidden outputs, i.e., h s = {h s1 , h s2 , ..., h sn } \u2208 R n\u00d72d , h t = {h t 1 , h t 2 , ..., h t n } \u2208 R n\u00d72d and h b = {h b1 , h b2 , ..., h bn } \u2208 R n\u00d72d , corresponding to the swipe-interactive, tag-interactive and browse-interactive sequence, respectively.\n\n\nHierarchical Attention Layer\n\nTo better fuse the views extracted from different behavior sequences, we propose a hierarchical attention mechanism, where the bottom attention layer focuses on the inner parts of each behavior sequence while the top attention layer learns the inter-view relations between different behavior sequences, as shown in Fig. 1. In the following, we introduce the hierarchical attention mechanism in details.\n\nIntra-view Attention. The intra-view attention mechanism at the bottom tries to identify the important actions within each sequence that contribute more to the purchasing intent prediction. Intuitively, the current behavior conducted by a user is most predictive of purchasing intent. To capitalize on this, we calculate the attention score between each action in a sequence and the current action, which is formulated as:\na t = exp(h t W a \u2212 \u2192 h n ) n i=1 exp(h i W a \u2212 \u2192 h n ) ,(2)\nwhere \u2212 \u2192 h n \u2208 R d is the final output state of the forward GRU model, h t \u2208 R 2d is t-th output state of the Bi-GRU model, W a \u2208 R 2d\u00d7d , and a t is the attention score calculated for h t . Attention score can reflect the relationship between h t and the current action \u2212 \u2192 h n . Therefore, the action that is more related to the current action can get a larger attention score.\n\nDifferent from traditional attention mechanism which conducts a weighted sum pooling operation to calculate the final output, the intra-view attention mechanism applies the element-wise product on the outputs of Bi-GRU and its corresponding attention score vector as follows:\nv s = h s \u2299 a s , v t = h t \u2299 a t , v b = h b \u2299 a b ,(3)\nwhere v s \u2208 R n\u00d72d , v t \u2208 R n\u00d72d and v b \u2208 R n\u00d72d are the outputs of the intra-view attention layer, corresponding to the swipeinteractive sequence, tag-interactive sequence and browse-interactive sequence, respectively.\n\nInter-view Attention. The swipe-interactive, tap-interactive and browse-interactive behavior depict a user from different views simultaneously. For example, a user interested in a product would browse some comments about the product and other similar products for comparison before she finally places the order. This process would generate some browse, swipe and tap actions. It is important to utilize these relationships to model the purchasing intent of a user. However, the actions from different views are usually asynchronous. The reasons are twofold. First, the synchronous actions become asynchronous due to the different densities of each behavior. Second, the related actions are originally asynchronous, e.g., some actions result in the occurrence of other actions. To this end, we propose a novel inter-view attention mechanism to better discover the asynchronous interactions between different views.\n\nThe inter-view attention mechanism takes two views as inputs, as shown in Figure 2. In particular, for each action in one view, we calculate its distance with all the actions in the other view. We borrow this idea from the self attention mechanism in Transformer [21]. In this way, the asynchronous interactions between actions can be captured effectively. The inter-view attention mechanism IA(v s , v b ) is formulated as follows, where we take the swipe-interactive view v s and the browse-interactive view v b for example: where A s \u2208 R n\u00d72d and A b \u2208 R n\u00d72d are attentive representations of v s and v b , respectively, and the element-wise product \u2299 is used to model the interactions between A s and A b . Note that IA(v s , v b ) is a symmetric operation and returns the interaction representation r sb between v s and v b . We can calculate r st and r tb following the same procedure. At last, we can get three representations r sb \u2208 R n\u00d72d , r st \u2208 R n\u00d72d and r tb \u2208 R n\u00d72d after the inter-view attention layer.\nIA(v s , v b ) = A s (v b , v s , v s ) \u2299 A b (v s , v b , v b ) A s (v b , v s , v s ) = softmax( v b v s T \u221a 2d )v s A b (v s , v b , v b ) = softmax( v s v b T \u221a 2d )v b ,(4)\n\nMulti-task Layer\n\nIn this work, we propose to train DIPN with multi-task learning. The reasons are twofold. First, with multi-task learning, DIPN can capture common features that well depict the customers and unique features that would lead to different purchasing behavior. Second, a model that learns multiple tasks simultaneously is able to learn a more robust representation and improve the generalization. As shown in Fig. 1, we use two tasks in this layer, i.e., the realtime purchasing intent prediction and the long-term purchasing intent prediction, which are defined as a predictive measure of subsequent purchasing behavior within one hour and one day, respectively. The reason that we define the period of the long-term purchasing intent as one day is because the purchasing behavior conducted one day later has a relative low correlation with the current behavior sequence. As a result, we partition user purchasing intent into three phases: real-time phase, long-term phase and irrelevant phase. Note that we use multi-task learning to handle the multi-class learning problem. By training the real-time intent and long-term intent with two separate tasks, we are able to distinguish between the subtle differences of user behavior.\n\nThe outputs of the hierarchical attention layer are first flattened and then concatenated with the user history feature, user profile feature and the concatenated last forward and backward state of the Bi-GRU models. The concatenated feature vectors are fed into two different branches. In each branch, fully connected layers are used to learn the combination of features automatically. The loss functions of the real-time purchasing intent prediction task and long-term purchasing intent prediction task are defined as follows:\nL shor t = \u2212 1 N N (x,y)\u2208 D (ylogp s (x) + (1 \u2212 y)log(1 \u2212 p s (x))) L lon\u0434 = \u2212 1 N N (x,y)\u2208 D (ylogp l (x) + (1 \u2212 y)log(1 \u2212 p l (x)))(5)\nwhere D is the training set with size N , x is the input of the network and y is the label, p s (x) and p l (x) represents the predicted Applied Data Science Track Paper KDD '19, August 4-8, 2019, Anchorage, AK, USA probability of sample x being purchased within one hour and one day, respectively. The global loss used in our DIPN model is:\nL \u0434lobal = L shor t + L lon\u0434 .(6)\n\nSYSTEM OVERVIEW\n\nBenefit from the rapid development of hardware and software on mobile devices, we are able to collect real-time behavior features to improve the performance of purchasing intent prediction. However, this benefit comes with a price that the high frequency of occurrence of the real-time features hinders DIPN from deploying in the industrial environment. If DIPN is deployed at the server side following traditional cloud-based computing architecture, the high frequency of features when used for prediction would result in a high communication cost unbearable to both the servers and the smartphones. To solve this problem, we propose to deploy our DIPN model on the mobile devices following the idea of edge computing 1 , which is defined as a distributed computing paradigm in which computation is largely or completely performed on distributed device nodes known as smart devices or edge devices. The overall structure of our prediction system deployed in a large-scale e-commerce platform, namely Taobao, is illustrated in Figure 3. The procedure is as follows. We first train our DIPN model in the cloud on powerful servers, and then use AliNN, which is Alibaba's solution for deploying machine learning models on mobile devices, to compress DIPN (with a size of 2MB) and deploy the compressed model on the devices. After that, the compressed DIPN can directly use the collected real-time features on a mobile device for prediction. Only the prediction results are sent to the cloud, which are stored in an online graph storage system and can be used to provide services for customers later. If we need to update DIPN , we only need to collect the training data from the devices and re-deploy the updated model to the devices.\n\nThere are several advantages to deploy DIPN on the mobile devices. First, it can reduce the communication cost between the cloud and the devices significantly. During the Alibaba 2018 Double 11 Shopping Festival, DIPN serves more than 10 million customers without suffering from the traditional peak traffic problem of the platform on that day. Second, it can greatly increase the response speed of DIPN by moving the model to the data instead of the data to the model. The response time of DIPN is between 20 \u223c 50 ms on different devices, which is immune to the influence of the network traffic and improves application performance. Third, it can well 1 https://en.wikipedia.org/wiki/Edge_computing protect the user privacy, because only the prediction scores, rather than the features capturing behavior patterns, are sent to the cloud.\n\n\nEXPERIMENTS\n\nIn this section, we present a comprehensive evaluation of the performance for DIPN. We first introduce the experimental setup and then present the experimental results under various settings. Finally, we share a case study for online serving.\n\n\nExperimental Setup\n\nDataset Statistics. We conduct the experiments on a large-scale industrial dataset collected from Taobao. The dataset contains normal users' daily interaction information when using our app, which consists of four subsets including the swipe-interactive behavior, tap-interactive behavior, browse-interactive behavior and also the user profiles. We collect 800,000 users' behavior for two weeks. For each user, we randomly truncate about 400 groups of samples on average. In total, we obtain 300 million groups of samples. Each group contains the user profile feature, the user history feature, and the three sequences with length 256 (padding 0 for short ones). Note that within each group, the timestamps of the last actions in the three sequences are the same. The real-time label and long-term label are then tagged for each group based on the timestamp of the last action. We use samples in the first 13 days for training and samples in the last day for evaluation.\n\nCompared Methods. We compare DIPN to the state-of-the-art approaches in purchasing intent prediction. Besides, we conduct experiments to verify the effect of each component in DIPN. In the following, we introduce the compared methods briefly.\n\n\u2022 GBDT [15]: A competitive gradient boosting model widely used in industrial environment. For a fair comparison, besides the session features used in [15], we also add the user history feature and user profile feature to the input of the model. Our goal is to see the benefit of using the new touchinteractive behavior in predicting user purchasing intent. \u2022 RNN+DNN [19]: A bidirectional RNN is used to model the dependency between the browse-interactive actions. Similar with GBDT, we modify RNN by adding the user history feature and user profile feature with DNN. \u2022 DIPN-early-fusion: Early fusion is applied in DIPN by first aligning the three sequences by timestamp and then feeding them into a single Bi-GRU layer. As a result, only intra-view attention mechanism is adopted. \u2022 DIPN-no-attention: A sub model of DIPN without using the hierarchical attention mechanism. The outputs from the RNN layer are simply concatenated. \u2022 DIPN-no-inter-view-attention: A sub model of DIPN by removing the inter-view attention mechanism. \u2022 DIPN-no-intra-view-attention A sub model of DIPN by removing the intra-view attention mechanism. \u2022 DIPN-no-multi-task A sub model of DIPN without using multi-task learning.\n\nExperimental Details. DIPN is trained with SGD, using the Adam optimizer [12] with initial hyper-parameters of \u03f5 = 10 \u22123 , \u03b2 1 = 0.9 and \u03b2 2 = 0.999. The dimension of the hidden state in the Bi-GRU model is set to d = 32. We train DIPN using a distributed   \n\n\nExperimental Results\n\nResults of different models. Table 4 shows the performance of the evaluated models. We have the following observations. (1) DIPN outperforms the baseline methods GBDT and RNN by a significant margin about 5.6% and 5.3% in terms of AUC, respectively. The improvement of DIPN over GBDT and RNN reveals the value of adopting the touch-interactive behavior to depict users from different views. (2) The early fusion manner is not appropriate for fusing views from different data sources. We can see that DIPNearly-fusion performs worst among the compared models. The reason is that the early fusion method could result in the imbalance of different views and information loss. (3) The hierarchical attention mechanism plays an important role in DIPN. As shown, DIPN-no-inter-view-attention and DIPN-no-intra-view-attention are superior to DIPN-no-attention but inferior to DIPN. This proves that the intra-view attention and inter-view attention mechanism are effective in identifying important actions within a view and discovering useful asynchronous interactions between views, respectively. (4) Prediction performance can be further improved by utilizing multi-task learning. Table 5 shows the results of DIPN with or without multi-task learning. As shown, the performance of realtime and long-term purchasing intent prediction can be improved by 0.6% and 0.7% when using multi-task learning, respectively.\n\nImpact of different sources. In this paper, DIPN predicts the real-time purchasing intent by utilizing multiple data sources simultaneously. To better understand the role that different data sources play, we conduct two types of tasks using DIPN. The first task is to predict the purchasing intent without each data source, while the second task only uses one single data source for prediction. The results are shown in Table 6. We can see that each data source gives a positive impact on the performance of DIPN. The user profile feature performs worst in the second task, which is as expected because it only provides basic information about a user. However, it increases AUC by about 0.5% when used together with other data sources, because it improves personalization in DIPN. The tap-interactive behavior plays a more significant role than the other behavior. The reason is that it captures more real-time behavior patterns compared with the browse-interactive behavior and contains more rich semantic information compared with the swipeinteractive behavior. It should be noted that the user history feature also contributes a lot to the performance of DIPN, demonstrating that the activeness of users has a great impact on their purchasing behavior. As shown, by utilizing all the data sources listed in our paper, DIPN gains about 18.96% improvement on AUC than the baseline only using traditional user behavior sequences.\n\n\nOnline A/B Testing\n\nCoupon allocation is an important strategy for improving the Gross Merchandise Volume (GMV) on e-commerce platforms. In this section, we introduce a new coupon allocation strategy based on the real-time purchasing intent predicted by DIPN in online traffic of Taobao. The online A/B testing was conducted at \"Double 11\" in 2018, which is a shopping festival in China, similar as the \"Black Friday\" in America.\n\nWe choose a coupon with 10 RMB nominal value for our testing and set three coupon allocation strategies to compare the performance, defined as follows:\n\n\u2022 All-allocation Strategy where everyone in this bucket is selected to get this coupon. \u2022 Non-allocation Strategy where everyone in this bucket is not selected to get this coupon. \u2022 Model-allocation Strategy which uses the score predicted by DIPN and the fixed thresholds to decide the allocation. The users selected to get this coupon will be pushed a popup in Taobao's mobile application.\n\nWe use the usage rate of coupons R c , and the GMV improvement per coupon I \u0434mv as evaluation metrics, defined as follows:\nR c = N wb N b ,(7)\nwhere N wb is the number of users who have used this coupon to buy something in a bucket b, and N b is the total number of users who have got this coupon in b.\nI \u0434mv = (G b \u2212 N b N non G non ) N wb = N non G b \u2212 N b G non N non N wb ,(8)\nwhere G b is the total GMV of users in a bucket b, N b is the number of users in b, G non and N non are the total GMV of users and the number of users in the non-allocation strategy bucket, respectively. We hypothesize that the users with a very low purchasing intent are hard to change their mind because of this coupon, while the users with a high purchasing intent are not needed to be given this promotion. Therefore, we set the lowest threshold t l = 0.2 and the uppermost threshold t u = 0.4. The users whose real-time purchasing intent score given by DIPN is between t l and t u are selected to get this coupon in the model-allocation strategy bucket.\n\nAs shown in Table 7 2 , there are more than 12.95 millions of users in this online A/B testing. It is notable that the model-allocation strategy contributes up to 41.1% R c and 39.8% I \u0434mv promotion compared with the all-allocation strategy in this large scale online traffic. The reason is that DIPN can help allocation system to understand user's real-time purchasing intent and allocate the coupon to the right person at the right time. Compared with the all-allocation strategy, a reasonable allocation strategy relied on DIPN would result in a significant GMV improvement.\n\n\nCONCLUSION\n\nIn this paper, we propose DIPN, a novel attention-based deep network with multi-task learning, for real-time purchasing intent prediction. Different from previous work, we collect a new type of user interactive behavior, i.e., the touch-interactive behavior, to capture comprehensive user behavior patterns. In order to fuse multiple user interactive behavior effectively, we propose a hierarchical attention mechanism including intra-view attention and inter-view attention. In addition, we use multi-task learning to train DIPN to better distinguish user behavior patterns. We conduct extensive experiments on a large-scale industrial dataset to evaluate the performance of DIPN. Experimental results show the superiority of DIPN under various settings. In particular, online A/B testing results reveal the potential of knowing users' real-time purchasing intent, which would result in a significant GMV improvement in the e-commerce platforms.\n\nFigure 1 :\n1The model architecture of DIPN.\n\nFigure 2 :\n2Inter-view attention mechanism.\n\nFigure 3 :\n3System overview under edge computing.\n\nTable 1 :\n1Average number of actions per user per day.Browse Tap Swipe \n\nNumber of actions 42 \n391 1583 \n\n\n\nTable 2 :\n2An example of raw data in the touch-interactive behavior dataset.\n\nTable 3b ,\n3bwe extract 3 raw features, all of which are categorical variables.Feature \n\nDictionary Dim Embedding Dim \n\nPage Index \n224 \n32 \nAction Index \n4 \n4 \nTime Gap \n20 \n8 \nTap Position X \n17 \n8 \nTap Position Y \n25 \n8 \nSwipe Start Position X \n17 \n8 \nSwipe Start Position Y \n25 \n8 \nSwipe End Position X \n17 \n8 \nSwipe End Position Y \n25 \n8 \nSwipe Length on X \n34 \n8 \nSwipe Length on Y \n50 \n16 \nSwipe Right/Left \n2 \n2 \nSwipe Up/Down \n2 \n2 \nSwipe Duration \n20 \n8 \n\n(a) Swipe-interactive behavior \n\nFeature \nDictionary Dim Embedding Dim \n\nEvent Index \n2 \n2 \nPage Index \n200 \n16 \nButton Index \n500 \n32 \n\n(b) Tap-interactive behavior \n\nFeature \nDictionary Dim Embedding Dim \n\nType Index \n6 \n4 \nLeaf Category Index \n19011 \n32 \nTop Category Index \n102 \n16 \nPage Index \n181 \n16 \nPage Stay Time \n179 \n16 \nTimestamp \n25 \n4 \n\n(c) Browse-interactive behavior \n\nFeature \nDictionary Dim Embedding Dim \n\nC.F. within one week \n20 \n8 \n... \n... \n... \nC.F. within one year \n100 \n16 \nA.F. within one week \n20 \n8 \n... \n... \n... \nA.F. within one year \n100 \n16 \nP.F. within one week \n20 \n8 \n... \n... \n... \nP.F. within one year \n100 \n16 \n\n(d) User history feature \n\nFeature \nDictionary Dim Embedding Dim \n\nAge Level \n9 \n4 \nGender \n3 \n2 \nBuyer Star \n17 \n8 \nTm Level \n6 \n4 \nVIP Level \n8 \n4 \nPhone Price \n11 \n4 \n... \n.... \n.... \n\n(e) User profile feature \n\n\n\nTable 3 :\n3Statistics of feature sets used in DIPN.\n\nTable 4 :\n4Comparison of different models.Model \nAUC \n\nGBDT \n0.7871 \nRNN+DNN \n0.7902 \nDIPN-early-fusion \n0.7708 \nDIPN-no-attention \n0.8345 \nDIPN-no-inter-view-attention \n0.8367 \nDIPN-no-intra-view-attention \n0.8401 \nDIPN-no-multi-task \n0.8371 \nDIPN \n0.8429 \n\n\n\nTable 5 :\n5Impact of multi-task learning.AUC(real-time) AUC(long-term) \n\nDIPN-no-multi-task \n0.8371 \n0.8204 \nDIPN \n0.8429 \n0.8276 \n\n\n\nTable 6 :\n6Impact of different sources. DIPN w/o history. 0.7862 DIPN w/ history. 0.7335 DIPN w/o browse. 0.8303 DIPN w/ browse. 0.6533 DIPN w/o swipe.TensorFlow with 1 parameter server and 100 workers. The metric used in our experiments is Area Under the Curve (AUC), which is insensitive to class imbalance and suitable to our experiments.Task1 \nAUC \nTask2 \nAUC \n\nDIPN w/o profile. \n0.8381 \nDIPN w/ profile. \n0.5419 \n0.8287 \nDIPN w/ swipe. \n0.6742 \nDIPN w/o tap. \n0.7978 DIPN w/ tap. \n0.7418 \n\n\n\nTable 7 :\n7The results of different coupon allocation strategies. Model-allocation 1.22M 57.0%(+41.1%) I m (+39.8%)Num. of Users R c \nI \u0434mv \n\nNone-allocation 1.38M \n/ \n0 \nAll-allocation \n10.35M \n40.4% \nI a \n\nAs the sensitive data policy, the I \u0434mv of the all-allocation strategy and the modelallocation strategy have been replace as I a and I m .\nACKNOWLEDGMENTS\nOn Effective Classification of Strings with Wavelets. C Charu, Aggarwal, Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningCharu C. Aggarwal. 2002. On Effective Classification of Strings with Wavelets. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 163-172.\n\nRecSys Challenge 2015 and the YOO-CHOOSE Dataset. David Ben-Shimon, Alexander Tsikinovsky, Michael Friedmann, Bracha Shapira, Lior Rokach, Johannes Hoerle, Proceedings of the 9th ACM Conference on Recommender Systems. the 9th ACM Conference on Recommender SystemsDavid Ben-Shimon, Alexander Tsikinovsky, Michael Friedmann, Bracha Shapira, Lior Rokach, and Johannes Hoerle. 2015. RecSys Challenge 2015 and the YOO- CHOOSE Dataset. In Proceedings of the 9th ACM Conference on Recommender Systems. 357-358.\n\nMulti-Task Learning for Abstractive and Extractive Summarization. Yangbin Chen, Yun Ma, Xudong Mao, Qing Li, Data Science and Engineering. 4Yangbin Chen, Yun Ma, Xudong Mao, and Qing Li. 2019. Multi-Task Learning for Abstractive and Extractive Summarization. Data Science and Engineering 4, 1 (01 Mar 2019), 14-23.\n\nProtein classification based on text document classification techniques. Betty Yee Man Cheng, Jaime G Carbonell, Judith Klein-Seetharaman, Proteins: Structure, Function, and Bioinformatics. 58Betty Yee Man Cheng, Jaime G. Carbonell, and Judith Klein-Seetharaman. 2005. Protein classification based on text document classification techniques. Proteins: Structure, Function, and Bioinformatics 58, 4 (2005), 955-970.\n\nA Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. Ronan Collobert, Jason Weston, Proceedings of the 25th International Conference on Machine Learning. the 25th International Conference on Machine LearningRonan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Lan- guage Processing: Deep Neural Networks with Multitask Learning. In Proceedings of the 25th International Conference on Machine Learning. 160-167.\n\nNew types of deep neural network learning for speech recognition and related applications: an overview. L Deng, G Hinton, B Kingsbury, 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. L. Deng, G. Hinton, and B. Kingsbury. 2013. New types of deep neural network learning for speech recognition and related applications: an overview. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing.\n\nLow Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser. Long Duong, Trevor Cohn, Steven Bird, Paul Cook, ACL-IJCNLP. 845-850Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser. In ACL-IJCNLP. 845-850.\n\nFast R-CNN. Ross Girshick, Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV. the 2015 IEEE International Conference on Computer Vision (ICCVRoss Girshick. 2015. Fast R-CNN. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). 1440-1448.\n\nCRAN: A Hybrid CNN-RNN Attention-Based Model for Text Classification. Long Guo, Dongxiang Zhang, Lei Wang, Han Wang, Bin Cui, 37th International Conferencel on Conceptual Modeling. Long Guo, Dongxiang Zhang, Lei Wang, Han Wang, and Bin Cui. 2018. CRAN: A Hybrid CNN-RNN Attention-Based Model for Text Classification. In 37th International Conferencel on Conceptual Modeling. 571-585.\n\nScaling Up Dynamic Time Warping for Datamining Applications. J Eamonn, Michael J Keogh, Pazzani, Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningEamonn J. Keogh and Michael J. Pazzani. 2000. Scaling Up Dynamic Time Warping for Datamining Applications. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 285-289.\n\nConvolutional Neural Networks for Sentence Classification. Yoon Kim, CoRR abs/1408.5882Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. CoRR abs/1408.5882 (2014).\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, CoRR abs/1412.6980Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti- mization. CoRR abs/1412.6980 (2014).\n\nMining Features for Sequence Classification. Neal Lesh, Mohammed J Zaki, Mitsunori Ogihara, Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningNeal Lesh, Mohammed J. Zaki, and Mitsunori Ogihara. 1999. Mining Features for Sequence Classification. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 342-346.\n\nPerceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks. Yabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, Luo Si, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, and Luo Si. 2018. Perceive Your Users in Depth: Learning Universal User Representa- tions from Multiple E-commerce Tasks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 596-605.\n\nRecSys Challenge 2015: Ensemble Learning with Categorical Features. Peter Romov, Evgeny Sokolov, In RecSys '15 Challenge. Article 1, 4 pagesPeter Romov and Evgeny Sokolov. 2015. RecSys Challenge 2015: Ensemble Learning with Categorical Features. In RecSys '15 Challenge. Article 1, 4 pages.\n\nAn Overview of Multi-Task Learning in Deep Neural Networks. Sebastian Ruder, CoRR abs/1706.05098Sebastian Ruder. 2017. An Overview of Multi-Task Learning in Deep Neural Networks. CoRR abs/1706.05098 (2017).\n\nFrequent-subsequence-based Prediction of Outer Membrane Proteins. Rong She, Fei Chen, Ke Wang, Martin Ester, Jennifer L Gardy, Fiona S L Brinkman, Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data MiningRong She, Fei Chen, Ke Wang, Martin Ester, Jennifer L. Gardy, and Fiona S. L. Brinkman. 2003. Frequent-subsequence-based Prediction of Outer Membrane Proteins. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 436-445.\n\nClassifying and Recommending Using Gradient Boosted Machines and Vector Space Models. Humphrey Sheil, Omer Rana, Advances in Computational Intelligence Systems. Humphrey Sheil and Omer Rana. 2018. Classifying and Recommending Using Gradient Boosted Machines and Vector Space Models. In Advances in Computa- tional Intelligence Systems. 214-221.\n\nPredicting purchasing intent: Automatic Feature Learning using Recurrent Neural Networks. Humphrey Sheil, Omer Rana, Ronan Reilly, CoRR abs/1807.08207Humphrey Sheil, Omer Rana, and Ronan Reilly. 2018. Predicting purchasing intent: Automatic Feature Learning using Recurrent Neural Networks. CoRR abs/1807.08207 (2018).\n\nPredicting Shopping Behavior with Mixture of RNNs. Arthur Toth, Louis Tan, Giuseppe Di Fabbrizio, Ankur Datta, ACM SIGIR Forum. Arthur Toth, Louis Tan, Giuseppe Di Fabbrizio, and Ankur Datta. 2017. Predicting Shopping Behavior with Mixture of RNNs. In ACM SIGIR Forum.\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30. 5998-6008.\n\nPredicting online user behaviour using deep learning algorithms. Armando Vieira, CoRR abs/1511.06247Armando Vieira. 2015. Predicting online user behaviour using deep learning algorithms. CoRR abs/1511.06247 (2015).\n\nTwo-Stage Approach to Item Recommendation from User Sessions. Maksims Volkovs, In RecSys '15 Challenge. Article 3, 4 pagesMaksims Volkovs. 2015. Two-Stage Approach to Item Recommendation from User Sessions. In RecSys '15 Challenge. Article 3, 4 pages.\n\nSemi-supervised Time Series Classification. Li Wei, Eamonn Keogh, Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningLi Wei and Eamonn Keogh. 2006. Semi-supervised Time Series Classification. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 748-753.\n\nNeural Modeling of Buying Behaviour for E-Commerce from Clicking Patterns. Zhenzhou Wu, Bao Hong Tan, Rubing Duan, Yong Liu, Rick Siow Mong Goh, In RecSys '15 Challenge. Article 12, 4 pagesZhenzhou Wu, Bao Hong Tan, Rubing Duan, Yong Liu, and Rick Siow Mong Goh. 2015. Neural Modeling of Buying Behaviour for E-Commerce from Clicking Patterns. In RecSys '15 Challenge. Article 12, 4 pages.\n\n. Zhengzheng Xing, Jian Pei, Eamonn Keogh, A Brief Survey on Sequence Classification. SIGKDD Explor. Newsl. 12Zhengzheng Xing, Jian Pei, and Eamonn Keogh. 2010. A Brief Survey on Sequence Classification. SIGKDD Explor. Newsl. 12, 1 (Nov. 2010), 40-48.\n\nDiscriminatively Trained Markov Model for Sequence Classification. Oksana Yakhnenko, Adrian Silvescu, Vasant Honavar, Proceedings of the Fifth IEEE International Conference on Data Mining. the Fifth IEEE International Conference on Data MiningOksana Yakhnenko, Adrian Silvescu, and Vasant Honavar. 2005. Discriminatively Trained Markov Model for Sequence Classification. In Proceedings of the Fifth IEEE International Conference on Data Mining. 498-505.\n\nTrace Norm Regularised Deep Multi-Task Learning. Yongxin Yang, Timothy M Hospedales, CoRR abs/1606.04038Yongxin Yang and Timothy M. Hospedales. 2016. Trace Norm Regularised Deep Multi-Task Learning. CoRR abs/1606.04038 (2016).\n\nTime Series Shapelets: A New Primitive for Data Mining. Lexiang Ye, Eamonn Keogh, KDD. Lexiang Ye and Eamonn Keogh. 2009. Time Series Shapelets: A New Primitive for Data Mining. In KDD. 947-956.\n\nCharacter-level Convolutional Networks for Text Classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing Systems1Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level Convolu- tional Networks for Text Classification. In Proceedings of the 28th International Conference on Neural Information Processing Systems -Volume 1. 649-657.\n\nDMDP: A Dynamic Multi-source Default Probability Prediction Framework. Yi Zhao, Yanyan Shen, Yong Huang, Data Science and Engineering. 4Yi Zhao, Yanyan Shen, and Yong Huang. 2019. DMDP: A Dynamic Multi-source Default Probability Prediction Framework. Data Science and Engineering 4, 1 (01 Mar 2019), 3-13.\n", "annotations": {"author": "[{\"end\":171,\"start\":162},{\"end\":183,\"start\":172},{\"end\":196,\"start\":184},{\"end\":211,\"start\":197},{\"end\":224,\"start\":212},{\"end\":349,\"start\":225},{\"end\":359,\"start\":350},{\"end\":371,\"start\":360},{\"end\":384,\"start\":372},{\"end\":399,\"start\":385},{\"end\":512,\"start\":400},{\"end\":519,\"start\":513},{\"end\":559,\"start\":520}]", "publisher": null, "author_last_name": "[{\"end\":170,\"start\":167},{\"end\":182,\"start\":179},{\"end\":195,\"start\":192},{\"end\":210,\"start\":206},{\"end\":223,\"start\":219},{\"end\":232,\"start\":229},{\"end\":358,\"start\":355},{\"end\":370,\"start\":367},{\"end\":383,\"start\":380},{\"end\":398,\"start\":394},{\"end\":414,\"start\":410}]", "author_first_name": "[{\"end\":166,\"start\":162},{\"end\":178,\"start\":172},{\"end\":191,\"start\":184},{\"end\":205,\"start\":197},{\"end\":218,\"start\":212},{\"end\":228,\"start\":225},{\"end\":354,\"start\":350},{\"end\":366,\"start\":360},{\"end\":379,\"start\":372},{\"end\":393,\"start\":385},{\"end\":409,\"start\":403},{\"end\":516,\"start\":513},{\"end\":518,\"start\":517}]", "author_affiliation": "[{\"end\":348,\"start\":253},{\"end\":511,\"start\":416},{\"end\":558,\"start\":521}]", "title": "[{\"end\":119,\"start\":1},{\"end\":678,\"start\":560}]", "venue": "[{\"end\":759,\"start\":680}]", "abstract": "[{\"end\":2889,\"start\":1155}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4128,\"start\":4124},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4131,\"start\":4128},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4134,\"start\":4131},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4137,\"start\":4134},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9463,\"start\":9460},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9694,\"start\":9690},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9877,\"start\":9873},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9950,\"start\":9946},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10009,\"start\":10005},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10012,\"start\":10009},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10015,\"start\":10012},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10142,\"start\":10138},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10145,\"start\":10142},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10183,\"start\":10179},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10977,\"start\":10973},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11070,\"start\":11067},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11073,\"start\":11070},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11076,\"start\":11073},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11114,\"start\":11110},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11117,\"start\":11114},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11120,\"start\":11117},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11149,\"start\":11146},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11152,\"start\":11149},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11155,\"start\":11152},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11400,\"start\":11397},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11403,\"start\":11400},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11406,\"start\":11403},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11991,\"start\":11988},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11993,\"start\":11991},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12020,\"start\":12017},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12043,\"start\":12040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12072,\"start\":12068},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12254,\"start\":12250},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12627,\"start\":12624},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12733,\"start\":12730},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12874,\"start\":12870},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13147,\"start\":13144},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13207,\"start\":13203},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26213,\"start\":26209},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33531,\"start\":33527},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33674,\"start\":33670},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33891,\"start\":33887},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34805,\"start\":34801}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41451,\"start\":41407},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41496,\"start\":41452},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41547,\"start\":41497},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41655,\"start\":41548},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41733,\"start\":41656},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43068,\"start\":41734},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43121,\"start\":43069},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43382,\"start\":43122},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43516,\"start\":43383},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44014,\"start\":43517},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44223,\"start\":44015}]", "paragraph": "[{\"end\":3979,\"start\":2905},{\"end\":5988,\"start\":3981},{\"end\":6782,\"start\":5990},{\"end\":7821,\"start\":6784},{\"end\":8877,\"start\":7823},{\"end\":9194,\"start\":8879},{\"end\":10184,\"start\":9244},{\"end\":10839,\"start\":10186},{\"end\":11835,\"start\":10867},{\"end\":13485,\"start\":11859},{\"end\":13687,\"start\":13487},{\"end\":13900,\"start\":13699},{\"end\":14128,\"start\":13931},{\"end\":15147,\"start\":14130},{\"end\":15794,\"start\":15149},{\"end\":16422,\"start\":15993},{\"end\":18061,\"start\":16454},{\"end\":18778,\"start\":18084},{\"end\":20275,\"start\":18798},{\"end\":20687,\"start\":20289},{\"end\":20992,\"start\":20689},{\"end\":21609,\"start\":21166},{\"end\":21791,\"start\":21655},{\"end\":23172,\"start\":21793},{\"end\":23607,\"start\":23205},{\"end\":24031,\"start\":23609},{\"end\":24473,\"start\":24093},{\"end\":24750,\"start\":24475},{\"end\":25029,\"start\":24808},{\"end\":25944,\"start\":25031},{\"end\":26965,\"start\":25946},{\"end\":28390,\"start\":27163},{\"end\":28920,\"start\":28392},{\"end\":29399,\"start\":29058},{\"end\":31183,\"start\":29452},{\"end\":32023,\"start\":31185},{\"end\":32281,\"start\":32039},{\"end\":33274,\"start\":32304},{\"end\":33518,\"start\":33276},{\"end\":34726,\"start\":33520},{\"end\":34986,\"start\":34728},{\"end\":36417,\"start\":35011},{\"end\":37848,\"start\":36419},{\"end\":38280,\"start\":37871},{\"end\":38433,\"start\":38282},{\"end\":38825,\"start\":38435},{\"end\":38949,\"start\":38827},{\"end\":39129,\"start\":38970},{\"end\":39866,\"start\":39208},{\"end\":40445,\"start\":39868},{\"end\":41406,\"start\":40460}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15992,\"start\":15795},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21165,\"start\":20993},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21654,\"start\":21610},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24092,\"start\":24032},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24807,\"start\":24751},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27143,\"start\":26966},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29057,\"start\":28921},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29433,\"start\":29400},{\"attributes\":{\"id\":\"formula_8\"},\"end\":38969,\"start\":38950},{\"attributes\":{\"id\":\"formula_9\"},\"end\":39207,\"start\":39130}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5349,\"start\":5342},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14266,\"start\":14258},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14697,\"start\":14689},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16114,\"start\":16106},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16818,\"start\":16810},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17573,\"start\":17565},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18060,\"start\":18052},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19168,\"start\":19161},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19572,\"start\":19564},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20041,\"start\":20034},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22410,\"start\":22403},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35047,\"start\":35040},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36194,\"start\":36187},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":36846,\"start\":36839},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39887,\"start\":39880}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2903,\"start\":2891},{\"attributes\":{\"n\":\"2\"},\"end\":9242,\"start\":9197},{\"attributes\":{\"n\":\"2.2\"},\"end\":10865,\"start\":10842},{\"attributes\":{\"n\":\"2.3\"},\"end\":11857,\"start\":11838},{\"attributes\":{\"n\":\"3\"},\"end\":13697,\"start\":13690},{\"attributes\":{\"n\":\"3.1\"},\"end\":13929,\"start\":13903},{\"attributes\":{\"n\":\"3.2\"},\"end\":16452,\"start\":16425},{\"attributes\":{\"n\":\"4\"},\"end\":18082,\"start\":18064},{\"attributes\":{\"n\":\"4.1\"},\"end\":18796,\"start\":18781},{\"attributes\":{\"n\":\"4.2\"},\"end\":20287,\"start\":20278},{\"attributes\":{\"n\":\"4.3\"},\"end\":23203,\"start\":23175},{\"attributes\":{\"n\":\"4.4\"},\"end\":27161,\"start\":27145},{\"attributes\":{\"n\":\"5\"},\"end\":29450,\"start\":29435},{\"attributes\":{\"n\":\"6\"},\"end\":32037,\"start\":32026},{\"attributes\":{\"n\":\"6.1\"},\"end\":32302,\"start\":32284},{\"attributes\":{\"n\":\"6.2\"},\"end\":35009,\"start\":34989},{\"attributes\":{\"n\":\"6.3\"},\"end\":37869,\"start\":37851},{\"attributes\":{\"n\":\"7\"},\"end\":40458,\"start\":40448},{\"end\":41418,\"start\":41408},{\"end\":41463,\"start\":41453},{\"end\":41508,\"start\":41498},{\"end\":41558,\"start\":41549},{\"end\":41666,\"start\":41657},{\"end\":41745,\"start\":41735},{\"end\":43079,\"start\":43070},{\"end\":43132,\"start\":43123},{\"end\":43393,\"start\":43384},{\"end\":43527,\"start\":43518},{\"end\":44025,\"start\":44016}]", "table": "[{\"end\":41655,\"start\":41603},{\"end\":43068,\"start\":41814},{\"end\":43382,\"start\":43165},{\"end\":43516,\"start\":43425},{\"end\":44014,\"start\":43859},{\"end\":44223,\"start\":44131}]", "figure_caption": "[{\"end\":41451,\"start\":41420},{\"end\":41496,\"start\":41465},{\"end\":41547,\"start\":41510},{\"end\":41603,\"start\":41560},{\"end\":41733,\"start\":41668},{\"end\":41814,\"start\":41748},{\"end\":43121,\"start\":43081},{\"end\":43165,\"start\":43134},{\"end\":43425,\"start\":43395},{\"end\":43859,\"start\":43529},{\"end\":44131,\"start\":44027}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18218,\"start\":18210},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22875,\"start\":22867},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23526,\"start\":23520},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26028,\"start\":26020},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27574,\"start\":27568},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30487,\"start\":30479}]", "bib_author_first_name": "[{\"end\":44434,\"start\":44433},{\"end\":44888,\"start\":44883},{\"end\":44910,\"start\":44901},{\"end\":44931,\"start\":44924},{\"end\":44949,\"start\":44943},{\"end\":44963,\"start\":44959},{\"end\":44980,\"start\":44972},{\"end\":45411,\"start\":45404},{\"end\":45421,\"start\":45418},{\"end\":45432,\"start\":45426},{\"end\":45442,\"start\":45438},{\"end\":45740,\"start\":45727},{\"end\":45753,\"start\":45748},{\"end\":45755,\"start\":45754},{\"end\":45773,\"start\":45767},{\"end\":46177,\"start\":46172},{\"end\":46194,\"start\":46189},{\"end\":46658,\"start\":46657},{\"end\":46666,\"start\":46665},{\"end\":46676,\"start\":46675},{\"end\":47095,\"start\":47091},{\"end\":47109,\"start\":47103},{\"end\":47122,\"start\":47116},{\"end\":47133,\"start\":47129},{\"end\":47352,\"start\":47348},{\"end\":47709,\"start\":47705},{\"end\":47724,\"start\":47715},{\"end\":47735,\"start\":47732},{\"end\":47745,\"start\":47742},{\"end\":47755,\"start\":47752},{\"end\":48082,\"start\":48081},{\"end\":48098,\"start\":48091},{\"end\":48100,\"start\":48099},{\"end\":48586,\"start\":48582},{\"end\":48758,\"start\":48757},{\"end\":48774,\"start\":48769},{\"end\":48967,\"start\":48963},{\"end\":48982,\"start\":48974},{\"end\":48984,\"start\":48983},{\"end\":49000,\"start\":48991},{\"end\":49518,\"start\":49514},{\"end\":49526,\"start\":49523},{\"end\":49538,\"start\":49531},{\"end\":49549,\"start\":49544},{\"end\":49559,\"start\":49554},{\"end\":49571,\"start\":49564},{\"end\":49581,\"start\":49578},{\"end\":50141,\"start\":50136},{\"end\":50155,\"start\":50149},{\"end\":50429,\"start\":50420},{\"end\":50638,\"start\":50634},{\"end\":50647,\"start\":50644},{\"end\":50656,\"start\":50654},{\"end\":50669,\"start\":50663},{\"end\":50685,\"start\":50677},{\"end\":50687,\"start\":50686},{\"end\":50700,\"start\":50695},{\"end\":50704,\"start\":50701},{\"end\":51268,\"start\":51260},{\"end\":51280,\"start\":51276},{\"end\":51618,\"start\":51610},{\"end\":51630,\"start\":51626},{\"end\":51642,\"start\":51637},{\"end\":51897,\"start\":51891},{\"end\":51909,\"start\":51904},{\"end\":51923,\"start\":51915},{\"end\":51926,\"start\":51924},{\"end\":51943,\"start\":51938},{\"end\":52143,\"start\":52137},{\"end\":52157,\"start\":52153},{\"end\":52171,\"start\":52167},{\"end\":52185,\"start\":52180},{\"end\":52202,\"start\":52197},{\"end\":52215,\"start\":52210},{\"end\":52217,\"start\":52216},{\"end\":52230,\"start\":52225},{\"end\":52604,\"start\":52597},{\"end\":52817,\"start\":52810},{\"end\":53047,\"start\":53045},{\"end\":53059,\"start\":53053},{\"end\":53521,\"start\":53513},{\"end\":53529,\"start\":53526},{\"end\":53534,\"start\":53530},{\"end\":53546,\"start\":53540},{\"end\":53557,\"start\":53553},{\"end\":53577,\"start\":53563},{\"end\":53841,\"start\":53831},{\"end\":53852,\"start\":53848},{\"end\":53864,\"start\":53858},{\"end\":54155,\"start\":54149},{\"end\":54173,\"start\":54167},{\"end\":54190,\"start\":54184},{\"end\":54593,\"start\":54586},{\"end\":54607,\"start\":54600},{\"end\":54609,\"start\":54608},{\"end\":54828,\"start\":54821},{\"end\":54839,\"start\":54833},{\"end\":55030,\"start\":55025},{\"end\":55043,\"start\":55038},{\"end\":55054,\"start\":55050},{\"end\":55528,\"start\":55526},{\"end\":55541,\"start\":55535},{\"end\":55552,\"start\":55548}]", "bib_author_last_name": "[{\"end\":44440,\"start\":44435},{\"end\":44450,\"start\":44442},{\"end\":44899,\"start\":44889},{\"end\":44922,\"start\":44911},{\"end\":44941,\"start\":44932},{\"end\":44957,\"start\":44950},{\"end\":44970,\"start\":44964},{\"end\":44987,\"start\":44981},{\"end\":45416,\"start\":45412},{\"end\":45424,\"start\":45422},{\"end\":45436,\"start\":45433},{\"end\":45445,\"start\":45443},{\"end\":45746,\"start\":45741},{\"end\":45765,\"start\":45756},{\"end\":45791,\"start\":45774},{\"end\":46187,\"start\":46178},{\"end\":46201,\"start\":46195},{\"end\":46663,\"start\":46659},{\"end\":46673,\"start\":46667},{\"end\":46686,\"start\":46677},{\"end\":47101,\"start\":47096},{\"end\":47114,\"start\":47110},{\"end\":47127,\"start\":47123},{\"end\":47138,\"start\":47134},{\"end\":47361,\"start\":47353},{\"end\":47713,\"start\":47710},{\"end\":47730,\"start\":47725},{\"end\":47740,\"start\":47736},{\"end\":47750,\"start\":47746},{\"end\":47759,\"start\":47756},{\"end\":48089,\"start\":48083},{\"end\":48106,\"start\":48101},{\"end\":48115,\"start\":48108},{\"end\":48590,\"start\":48587},{\"end\":48767,\"start\":48759},{\"end\":48781,\"start\":48775},{\"end\":48785,\"start\":48783},{\"end\":48972,\"start\":48968},{\"end\":48989,\"start\":48985},{\"end\":49008,\"start\":49001},{\"end\":49521,\"start\":49519},{\"end\":49529,\"start\":49527},{\"end\":49542,\"start\":49539},{\"end\":49552,\"start\":49550},{\"end\":49562,\"start\":49560},{\"end\":49576,\"start\":49572},{\"end\":49584,\"start\":49582},{\"end\":50147,\"start\":50142},{\"end\":50163,\"start\":50156},{\"end\":50435,\"start\":50430},{\"end\":50642,\"start\":50639},{\"end\":50652,\"start\":50648},{\"end\":50661,\"start\":50657},{\"end\":50675,\"start\":50670},{\"end\":50693,\"start\":50688},{\"end\":50713,\"start\":50705},{\"end\":51274,\"start\":51269},{\"end\":51285,\"start\":51281},{\"end\":51624,\"start\":51619},{\"end\":51635,\"start\":51631},{\"end\":51649,\"start\":51643},{\"end\":51902,\"start\":51898},{\"end\":51913,\"start\":51910},{\"end\":51936,\"start\":51927},{\"end\":51949,\"start\":51944},{\"end\":52151,\"start\":52144},{\"end\":52165,\"start\":52158},{\"end\":52178,\"start\":52172},{\"end\":52195,\"start\":52186},{\"end\":52208,\"start\":52203},{\"end\":52223,\"start\":52218},{\"end\":52237,\"start\":52231},{\"end\":52249,\"start\":52239},{\"end\":52611,\"start\":52605},{\"end\":52825,\"start\":52818},{\"end\":53051,\"start\":53048},{\"end\":53065,\"start\":53060},{\"end\":53524,\"start\":53522},{\"end\":53538,\"start\":53535},{\"end\":53551,\"start\":53547},{\"end\":53561,\"start\":53558},{\"end\":53581,\"start\":53578},{\"end\":53846,\"start\":53842},{\"end\":53856,\"start\":53853},{\"end\":53870,\"start\":53865},{\"end\":54165,\"start\":54156},{\"end\":54182,\"start\":54174},{\"end\":54198,\"start\":54191},{\"end\":54598,\"start\":54594},{\"end\":54620,\"start\":54610},{\"end\":54831,\"start\":54829},{\"end\":54845,\"start\":54840},{\"end\":55036,\"start\":55031},{\"end\":55048,\"start\":55044},{\"end\":55060,\"start\":55055},{\"end\":55533,\"start\":55529},{\"end\":55546,\"start\":55542},{\"end\":55558,\"start\":55553}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":5256404},\"end\":44831,\"start\":44379},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":33852998},\"end\":45336,\"start\":44833},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":117726873},\"end\":45652,\"start\":45338},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11187370},\"end\":46068,\"start\":45654},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2617020},\"end\":46551,\"start\":46070},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13953660},\"end\":46996,\"start\":46553},{\"attributes\":{\"doi\":\"ACL-IJCNLP. 845-850\",\"id\":\"b6\"},\"end\":47334,\"start\":46998},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206770307},\"end\":47633,\"start\":47336},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52845940},\"end\":48018,\"start\":47635},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3348832},\"end\":48521,\"start\":48020},{\"attributes\":{\"doi\":\"CoRR abs/1408.5882\",\"id\":\"b10\"},\"end\":48711,\"start\":48523},{\"attributes\":{\"doi\":\"CoRR abs/1412.6980\",\"id\":\"b11\"},\"end\":48916,\"start\":48713},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11362811},\"end\":49410,\"start\":48918},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":44145233},\"end\":50066,\"start\":49412},{\"attributes\":{\"id\":\"b14\"},\"end\":50358,\"start\":50068},{\"attributes\":{\"doi\":\"CoRR abs/1706.05098\",\"id\":\"b15\"},\"end\":50566,\"start\":50360},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17836332},\"end\":51172,\"start\":50568},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49411455},\"end\":51518,\"start\":51174},{\"attributes\":{\"doi\":\"CoRR abs/1807.08207\",\"id\":\"b18\"},\"end\":51838,\"start\":51520},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":59528391},\"end\":52108,\"start\":51840},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13756489},\"end\":52530,\"start\":52110},{\"attributes\":{\"doi\":\"CoRR abs/1511.06247\",\"id\":\"b21\"},\"end\":52746,\"start\":52532},{\"attributes\":{\"id\":\"b22\"},\"end\":52999,\"start\":52748},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16342187},\"end\":53436,\"start\":53001},{\"attributes\":{\"id\":\"b24\"},\"end\":53827,\"start\":53438},{\"attributes\":{\"id\":\"b25\"},\"end\":54080,\"start\":53829},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1699609},\"end\":54535,\"start\":54082},{\"attributes\":{\"doi\":\"CoRR abs/1606.04038\",\"id\":\"b27\"},\"end\":54763,\"start\":54537},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5144823},\"end\":54959,\"start\":54765},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":368182},\"end\":55453,\"start\":54961},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":85527543},\"end\":55760,\"start\":55455}]", "bib_title": "[{\"end\":44431,\"start\":44379},{\"end\":44881,\"start\":44833},{\"end\":45402,\"start\":45338},{\"end\":45725,\"start\":45654},{\"end\":46170,\"start\":46070},{\"end\":46655,\"start\":46553},{\"end\":47346,\"start\":47336},{\"end\":47703,\"start\":47635},{\"end\":48079,\"start\":48020},{\"end\":48961,\"start\":48918},{\"end\":49512,\"start\":49412},{\"end\":50632,\"start\":50568},{\"end\":51258,\"start\":51174},{\"end\":51889,\"start\":51840},{\"end\":52135,\"start\":52110},{\"end\":53043,\"start\":53001},{\"end\":54147,\"start\":54082},{\"end\":54819,\"start\":54765},{\"end\":55023,\"start\":54961},{\"end\":55524,\"start\":55455}]", "bib_author": "[{\"end\":44442,\"start\":44433},{\"end\":44452,\"start\":44442},{\"end\":44901,\"start\":44883},{\"end\":44924,\"start\":44901},{\"end\":44943,\"start\":44924},{\"end\":44959,\"start\":44943},{\"end\":44972,\"start\":44959},{\"end\":44989,\"start\":44972},{\"end\":45418,\"start\":45404},{\"end\":45426,\"start\":45418},{\"end\":45438,\"start\":45426},{\"end\":45447,\"start\":45438},{\"end\":45748,\"start\":45727},{\"end\":45767,\"start\":45748},{\"end\":45793,\"start\":45767},{\"end\":46189,\"start\":46172},{\"end\":46203,\"start\":46189},{\"end\":46665,\"start\":46657},{\"end\":46675,\"start\":46665},{\"end\":46688,\"start\":46675},{\"end\":47103,\"start\":47091},{\"end\":47116,\"start\":47103},{\"end\":47129,\"start\":47116},{\"end\":47140,\"start\":47129},{\"end\":47363,\"start\":47348},{\"end\":47715,\"start\":47705},{\"end\":47732,\"start\":47715},{\"end\":47742,\"start\":47732},{\"end\":47752,\"start\":47742},{\"end\":47761,\"start\":47752},{\"end\":48091,\"start\":48081},{\"end\":48108,\"start\":48091},{\"end\":48117,\"start\":48108},{\"end\":48592,\"start\":48582},{\"end\":48769,\"start\":48757},{\"end\":48783,\"start\":48769},{\"end\":48787,\"start\":48783},{\"end\":48974,\"start\":48963},{\"end\":48991,\"start\":48974},{\"end\":49010,\"start\":48991},{\"end\":49523,\"start\":49514},{\"end\":49531,\"start\":49523},{\"end\":49544,\"start\":49531},{\"end\":49554,\"start\":49544},{\"end\":49564,\"start\":49554},{\"end\":49578,\"start\":49564},{\"end\":49586,\"start\":49578},{\"end\":50149,\"start\":50136},{\"end\":50165,\"start\":50149},{\"end\":50437,\"start\":50420},{\"end\":50644,\"start\":50634},{\"end\":50654,\"start\":50644},{\"end\":50663,\"start\":50654},{\"end\":50677,\"start\":50663},{\"end\":50695,\"start\":50677},{\"end\":50715,\"start\":50695},{\"end\":51276,\"start\":51260},{\"end\":51287,\"start\":51276},{\"end\":51626,\"start\":51610},{\"end\":51637,\"start\":51626},{\"end\":51651,\"start\":51637},{\"end\":51904,\"start\":51891},{\"end\":51915,\"start\":51904},{\"end\":51938,\"start\":51915},{\"end\":51951,\"start\":51938},{\"end\":52153,\"start\":52137},{\"end\":52167,\"start\":52153},{\"end\":52180,\"start\":52167},{\"end\":52197,\"start\":52180},{\"end\":52210,\"start\":52197},{\"end\":52225,\"start\":52210},{\"end\":52239,\"start\":52225},{\"end\":52251,\"start\":52239},{\"end\":52613,\"start\":52597},{\"end\":52827,\"start\":52810},{\"end\":53053,\"start\":53045},{\"end\":53067,\"start\":53053},{\"end\":53526,\"start\":53513},{\"end\":53540,\"start\":53526},{\"end\":53553,\"start\":53540},{\"end\":53563,\"start\":53553},{\"end\":53583,\"start\":53563},{\"end\":53848,\"start\":53831},{\"end\":53858,\"start\":53848},{\"end\":53872,\"start\":53858},{\"end\":54167,\"start\":54149},{\"end\":54184,\"start\":54167},{\"end\":54200,\"start\":54184},{\"end\":54600,\"start\":54586},{\"end\":54622,\"start\":54600},{\"end\":54833,\"start\":54821},{\"end\":54847,\"start\":54833},{\"end\":55038,\"start\":55025},{\"end\":55050,\"start\":55038},{\"end\":55062,\"start\":55050},{\"end\":55535,\"start\":55526},{\"end\":55548,\"start\":55535},{\"end\":55560,\"start\":55548}]", "bib_venue": "[{\"end\":44639,\"start\":44554},{\"end\":45096,\"start\":45051},{\"end\":46326,\"start\":46273},{\"end\":47506,\"start\":47443},{\"end\":48302,\"start\":48218},{\"end\":49195,\"start\":49111},{\"end\":49769,\"start\":49686},{\"end\":50900,\"start\":50816},{\"end\":53250,\"start\":53167},{\"end\":54325,\"start\":54271},{\"end\":55227,\"start\":55153},{\"end\":44552,\"start\":44452},{\"end\":45049,\"start\":44989},{\"end\":45475,\"start\":45447},{\"end\":45842,\"start\":45793},{\"end\":46271,\"start\":46203},{\"end\":46765,\"start\":46688},{\"end\":47089,\"start\":46998},{\"end\":47441,\"start\":47363},{\"end\":47814,\"start\":47761},{\"end\":48216,\"start\":48117},{\"end\":48580,\"start\":48523},{\"end\":48755,\"start\":48713},{\"end\":49109,\"start\":49010},{\"end\":49684,\"start\":49586},{\"end\":50134,\"start\":50068},{\"end\":50418,\"start\":50360},{\"end\":50814,\"start\":50715},{\"end\":51333,\"start\":51287},{\"end\":51608,\"start\":51520},{\"end\":51966,\"start\":51951},{\"end\":52300,\"start\":52251},{\"end\":52595,\"start\":52532},{\"end\":52808,\"start\":52748},{\"end\":53165,\"start\":53067},{\"end\":53511,\"start\":53438},{\"end\":53935,\"start\":53872},{\"end\":54269,\"start\":54200},{\"end\":54584,\"start\":54537},{\"end\":54850,\"start\":54847},{\"end\":55151,\"start\":55062},{\"end\":55588,\"start\":55560}]"}}}, "year": 2023, "month": 12, "day": 17}