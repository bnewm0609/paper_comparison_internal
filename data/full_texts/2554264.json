{"id": 2554264, "updated": "2023-11-07 23:43:48.851", "metadata": {"title": "ShapeNet: An Information-Rich 3D Model Repository", "authors": "[{\"first\":\"Angel\",\"last\":\"Chang\",\"middle\":[\"X.\"]},{\"first\":\"Thomas\",\"last\":\"Funkhouser\",\"middle\":[]},{\"first\":\"Leonidas\",\"last\":\"Guibas\",\"middle\":[]},{\"first\":\"Pat\",\"last\":\"Hanrahan\",\"middle\":[]},{\"first\":\"Qixing\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Zimo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Silvio\",\"last\":\"Savarese\",\"middle\":[]},{\"first\":\"Manolis\",\"last\":\"Savva\",\"middle\":[]},{\"first\":\"Shuran\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Su\",\"middle\":[]},{\"first\":\"Jianxiong\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Yi\",\"middle\":[]},{\"first\":\"Fisher\",\"last\":\"Yu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2015, "month": 12, "day": 9}, "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1512.03012", "mag": "2190691619", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ChangFGHHLSSSSX15", "doi": null}}, "content": {"source": {"pdf_hash": "2327c389267019508bbb15788b5caa7bdda30998", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1512.03012v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6911d98fb9d03e7f254974de2efdfd56c65e842e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2327c389267019508bbb15788b5caa7bdda30998.txt", "contents": "\nShapeNet: An Information-Rich 3D Model Repository\n\n\nAngel X Chang \nStanford University\n\n\nThomas Funkhouser \nPrinceton University\n\n\nLeonidas Guibas \nStanford University\n\n\nPat Hanrahan \nStanford University\n\n\nQixing Huang \nToyota Technological Institute at Chicago\n\n\nZimo Li \nToyota Technological Institute at Chicago\n\n\nSilvio Savarese \nStanford University\n\n\nManolis Savva \nStanford University\n\n\nShuran Song \nPrinceton University\n\n\nHao Su \nStanford University\n\n\nJianxiong Xiao \nPrinceton University\n\n\nYi Li \nStanford University\n\n\nFisher Yu \nPrinceton University\n\n\nShapeNet: An Information-Rich 3D Model Repository\nAuthors listed alphabetically\nWe present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the Word-Net taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.\n\nIntroduction\n\nRecent technological developments have led to an explosion in the amount of 3D data that we can generate and store. Repositories of 3D CAD models are expanding continuously, predominantly through aggregation of 3D content on the web. RGB-D sensors and other technology for scanning and reconstruction are providing increasingly higher fidelity geometric representations of objects and real environments that can eventually become CAD-quality models.\n\nAt the same time, there are many open research problems due to fundamental challenges in using 3D content. Computing segmentations of 3D shapes, and establishing correspondences between them are two basic problems in geometric shape analysis. Recognition of shapes from par- * Contact authors: {msavva,haosu}@cs.stanford.edu tial scans is a research goal shared by computer graphics and vision. Scene understanding from 2D images is a grand challenge in vision that has recently benefited tremendously from 3D CAD models [28,34]. Navigation of autonomous robots and planning of grasping manipulations are two large areas in robotics that benefit from an understanding of 3D shapes. At the root of all these research problems lies the need for attaching semantics to representations of 3D shapes, and doing so at large scale.\n\nRecently, data-driven methods from the machine learning community have been exploited by researchers in vision and NLP (natural language processing). \"Big data\" in the visual and textual domains has led to tremendous progress towards associating semantics with content in both fields. Mirroring this pattern, recent work in computer graphics has also applied similar approaches to specific problems in the synthesis of new shape variations [10] and new arrangements of shapes [6]. However, a critical bottleneck facing the adoption of data-driven methods for 3D content is the lack of large-scale, curated datasets of 3D models that are available to the community.\n\nMotivated by the far-reaching impact of dataset efforts such as the Penn Treebank [20], WordNet [21] and Ima-geNet [4], which collectively have tens of thousands of citations, we propose establishing ShapeNet: a large-scale 3D model dataset. Making a comprehensive, semantically enriched shape dataset available to the community can have immense impact, enabling many avenues of future research.\n\nIn constructing ShapeNet we aim to fulfill several goals:\n\n\u2022 Collect and centralize 3D model datasets, helping to organize effort in the research community.\n\n\u2022 Support data-driven methods requiring 3D model data.\n\n\u2022 Enable evaluation and comparison of algorithms for fundamental tasks involving geometry (e.g., segmentation, alignment, correspondence).\n\n\u2022 Serve as a knowledge base for representing real-world objects and their semantics.\n\nThese goals imply several desiderata for ShapeNet:\n\n\u2022 Broad and deep coverage of objects observed in the real world, with thousands of object categories and millions of total instances. \u2022 Categorization scheme connected to other modalities of knowledge such as 2D images and language. \u2022 Annotation of salient physical attributes on models, such as canonical orientations, planes of symmetry, and part decompositions. \u2022 Web-based interfaces for searching, viewing and retrieving models in the dataset through several modalities: textual keywords, taxonomy traversal, image and shape similarity search.\n\nAchieving these goals and providing the resulting dataset to the community will enable many advances and applications in computer graphics and vision.\n\nIn this report, we first situate ShapeNet, explaining the overall goals of the effort and the types of data it is intended to contain, as well as motivating the long-term vision and infrastructural design decisions (Section 3). We then describe the acquisition and validation of annotations collected so far (Section 4), summarize the current state of all available ShapeNet datasets, and provide basic statistics on the collected annotations (Section 5). We end with a discussion of ShapeNet's future trajectory and connect it with several research directions (Section 7).\n\n\nBackground and Related Work\n\nThere has been substantial growth in the number of of 3D models available online over the last decade, with repositories like the Trimble 3D Warehouse providing millions of 3D polygonal models covering thousands of object and scene categories. Yet, there are few collections of 3D models that provide useful organization and annotations. Meaningful textual descriptions are rarely provided for individual models, and online repositories are usually either unorganized or grouped into gross categories (e.g., furniture, architecture, etc. [7]). As a result, they have been poorly utilized in research and applications.\n\nThere have been previous efforts to build organized collections of 3D models (e.g., [5,7]). However, they have provided quite small datasets, covered only a small number of semantic categories, and included few structural and semantic annotations. Most of these previous collections have been developed for evaluating shape retrieval and classification algorithms. For example, datasets are created annually for the Shape Retrieval Contest (SHREC) that commonly contains sets of models organized in object categories. However, those datasets are very small -the most recent SHREC iteration in 2014 [17] contains a \"large\" dataset with around 9,000 models consisting of models from a variety of sources organized into 171 categories ( Table 1).\n\nThe Princeton Shape Benchmark is probably the most well-known and frequently used 3D shape collection to date (with over 1000 citations) [27]. It contains around 1,800 3D models grouped into 90 categories, but has no annotations beyond category labels. Other commonly-used datasets contain segmentations [2], correspondences [13,12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations -but again only for small size datasets. For example, the Benchmark for 3D Mesh Segmentation contains just 380 models in 19 object classes [2].\n\nIn contrast, there has been a flurry of activity on collecting, organizing, and labeling large datasets in computer vision and related fields. For example, ImageNet [4] provides a set of 14M images organized into 20K categories associated with \"synsets\" of WordNet [21]. LabelMe provides segmentations and label annotations of hundreds of thousands of objects in tens of thousands of images [24]. The SUN dataset provides 3M annotations of objects in 4K categories appearing in 131K images of 900 types of scenes. Recent work demonstrated the benefit of a large dataset of 120K 3D CAD models in training a convolutional neural network for object recognition and next-best view prediction in RGB-D data [34]. Large datasets such as this and others (e.g., [14,18]) have revitalized data-driven algorithms for recognition, detection, and editing of images, which have revolutionized computer vision.\n\nSimilarly, large collections of annotated 3D data have had great influence on progress in other disciplines. For example, the Protein Data Bank [1] provides a database with 100K protein 3D structures, each labeled with its source and links to structural and functional annotations [15]. This database is a common repository of all 3D protein structures solved to date and provides a shared infrastructure for the collection and transfer of knowledge about each entry. It has accelerated the development of data-driven algorithms, facilitated the creation of benchmarks, and linked researchers and industry from around the world. We aim to provide a similar resource for 3D models of everyday objects.\n\n\nShapeNet: An Information-Rich 3D Model Repository\n\nShapeNet is a large, information-rich repository of 3D models. It contains models spanning a multitude of semantic categories. Unlike previous 3D model repositories, it provides extensive sets of annotations for every model and links between models in the repository and other multimedia data outside the repository.\n\nLike ImageNet, ShapeNet provides a view of the contained data in a hierarchical categorization according to WordNet synsets ( Figure 1). Unlike other model repositories, ShapeNet also provides a rich set of annotations for  [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9]. each shape and correspondences between shapes. The annotations include geometric attributes such as upright and front orientation vectors, parts and keypoints, shape symmetries (reflection plane, other rotational symmetries), and scale of object in real world units. These attributes provide valuable resources for processing, understanding and visualizing 3D shapes in a way that is aware of the semantics of the shape.\n\nWe have currently collected approximately 3 million shapes from online 3D model repositories, and categorized 300 thousand of them against the WordNet taxonomy. We have also annotated a subset of these models with shape properties such as upright and front orientations, symmetries, and hierarchical part decompositions. We are continuing the process of expanding the annotated set of models and also collecting new models from new data sources.\n\nIn the following sections, we discuss how 3D models are collected for ShapeNet, what annotations will be added, how those annotations will be generated, how annotations will be updated as the dataset evolves over time, and what tools will be provided for the community to search, browse, and utilize existing data, as well as contribute new data.\n\n\nData Collection\n\nThe raw 3D model data for ShapeNet comes from public online repositories or existing research datasets. ShapeNet is intended to be an evolving repository with regular updates as more and more 3D models become available, as more people contribute annotations, and as the data captured with new 3D sensors become prevalent.\n\nWe have collected 3D polygonal models from two popular public repositories: Trimble 3D Warehouse 1 and Yobi3D 2 . The Trimble 3D Warehouse contains 2.4M userdesigned 3D models and scenes. Yobi3D contains 350K additional models collected from a wide range of other online repositories. Together, they provide a diverse set of shapes from a broad set of object and scene categories -e.g., many organic shape categories (e.g., humans and mammals), which are rare in Warehouse3D, are plentiful in Yobi3D. For more detailed statistics on the currently available ShapeNet models refer to Section 5.\n\nThough the tools developed for this project will be general-purpose, we intend to include only 3D models of objects encountered by people in the everyday world. That is, it will not include CAD mechanical parts, molecular structures, or other domain-specific objects. However, we will include scenes (e.g., office), objects (e.g., laptop computer), and parts of objects (e.g., keyboard). Models are organized under WordNet [21] noun \"synsets\" (synonym sets). WordNet provides a broad and deep taxonomy with over 80K distinct synsets representing distinct noun concepts arranged as a DAG network of hyponym relationships (e.g., \"canary\" is a hyponym of \"bird\"). This taxonomy has been used by ImageNet to describe categories of objects at multiple scales [4]. Though we first use WordNet due to its popularity, the ShapeNet UI is designed to allow multiple views into the collection of shapes that it contains, including different taxonomy views and faceted navigation.\n\n\nAnnotation Types\n\nWe envision ShapeNet as far more than a collection of 3D models. ShapeNet will include a rich set of annotations that provide semantic information about those models, establish links between them, and links to other modalities of data (e.g., images). These annotations are exactly what make ShapeNet uniquely valuable. Figure 2 illustrates the value of this dense network of interlinked attributes on shapes, which we describe below.\n\nLanguage-related Annotations: Naming objects by their basic category is useful for indexing, grouping, and linking to related sources of data. As described in the previous section, we organize ShapeNet based on the Word-Net [21] taxonomy. Synsets are interlinked with various relations, such as hyper and hyponym, and part-whole relations. Due to the popularity of WordNet, we can leverage other resources linked to WordNet such as ImageNet, Con-ceptNet, Freebase, and Wikipedia. In particular, linking to ImageNet [4] will help transport information between images and shapes. We assign each 3D model in ShapeNet to one or more synsets in the WordNet taxonomy (i.e., we populate each synset with a collection of shapes). Please refer to Section 4.1 for details on the acquisition and validation of basic category annotations. Future planned annotations include natural language descriptions of objects and object part-part relation descriptions.\n\nGeometric Annotations: A critical property that distinguishes ShapeNet from image and video datasets is the fidelity with which 3D geometry represents real-world structures. We combine algorithmic predictions and manual annotations to organize shapes by category-level geometric properties and further derive rich geometric annotations from the raw 3D model geometry.\n\n\u2022 Rigid Alignments: Establishing a consistent canonical orientation (e.g., upright and front) for every model is important for various tasks such as visualizing shapes [13], shape classification [8] and shape recognition [34]. Fortunately, most raw 3D model data is by default placed in an upright orientation, and the front orientations are typically aligned with an axis. This allows us to use a hierarchical clustering and alignment approach to ensure consistent rigid alignments within each category (see Section 4.2).\n\n\u2022 Parts and Keypoints: Many shapes contain or have natural decompositions into important parts, as well as significant keypoints related to both their geometry and their semantics. For example, often different materials are associated with different parts. We intend to capture as much of that as possible into ShapeNet.\n\n\u2022 Symmetry: Bilateral symmetry planes and rotational symmetries are prevalent in artificial and natural objects, and deeply connected with the alignment and functionality of shapes. We refer to Section 4.4 for more details on how we compute symmetries for the shapes in ShapeNet.\n\n\u2022 Object Size: Object size is useful for many applications, such as reducing the hypothesis space in object recognition. Size annotations are discussed in Section 5.2.\n\nFunctional Annotations: Many objects, especially manmade artifacts such as furniture and appliances, can be used by humans. Functional annotations describe these usage patterns. Such annotations are often highly correlated with specific regions of an object. In addition, it is often related with the specific type of human action. ShapeNet aims to store functional annotations at the global shape level and at the object part level.\n\n\u2022 Functional Parts: Parts are critical for understanding object structure, human activities involving a 3D shape, and ergonomic product design. We plan to annotate parts according to their function -in fact the very definition of parts has to be based on both geometric and functional criteria.\n\n\u2022 Affordances: We are interested in affordance annotations that are function and activity specific. Examples of such annotations include supporting plane annotations, and graspable region annotations for various object manipulations.\n\nPhysical Annotations: Real objects exist in the physical world and typically have fixed physical properties such as dimensions and densities. Thus, it is important to store physical attribute annotations for 3D shapes.\n\n\u2022 Surface Material: We are especially interested in the optical properties and semantic names of surface materials. They are important for applications such as rendering and structural strength estimation.\n\n\u2022 Weight: A basic property of objects which is very useful for physical simulations, and reasoning about stability and static support.\n\nIn general, the issue of compact and informative representations for all the above attributes over shapes raises many interesting questions that we will need to address as part of the ShapeNet effort. Many annotations are currently ongoing projects and involve interesting open research problems.  \n\n\nAnnotation Methodology\n\nThough at first glance it might seem reasonable to collect the annotations we describe purely through manual human effort, we will in general take a hybrid approach. For annotation types where it is possible, we will first algorithmically predict the annotation for each model instance (e.g., global symmetry planes, consistent rigid alignments). We will then verify these predictions through crowd-sourcing pipelines and inspection by human experts. This hybrid strategy is sensible in the context of 3D shape data as there are already various algorithms we can leverage, and collecting corresponding annotations entirely through manual effort can be extremely labor intensive. In particular, since objects in a 3D representation are both more pure and more complete than objects in images, we can expect better and easier to establish correspondences between 3D shapes, enabling algorithmic transport of semantic annotations. In many cases, the design of the human annotation interfaces themselves is an open question -which stands in contrast to largely manual image labeling efforts such as ImageNet. As a concrete example, shape part annotation can be presented and performed in various ways with different trade-offs in the type of obtained part annotation, the accuracy and the efficiency of the annotation process.\n\nCoupled with this hybrid annotation strategy, we also take particular care to preserve the provenance and confidence of each algorithmic and human annotation. The annotation source (whether an algorithm, or human effort), and a measure of the trust we can place in each annotation are critical pieces of information especially when we have to combine, aggregate, and reconcile several annotations.\n\n\nAnnotation Schema and Web API\n\nTo provide convenient access to all of the model and annotation data contained within ShapeNet, we construct an index over all the 3D models and their associated annotations using the Apache Solr framework. 3 Each stored annotation for a given 3D model is contained within the index as a separate attribute that can be easily queried and filtered through a simple web-based UI. In addition, to make the dataset conveniently accessible to researchers, we provide a batched download capability.\n\n\nAnnotation Acquisition and Validation\n\nA key challenge in constructing ShapeNet is the methodology for acquiring and validating annotations. Our goal is to provide all annotations with high accuracy. In cases where full verification is not yet available, we aim to estimate a confidence metric for each annotation, as well as record its provenance. This will enable others to properly estimate the trustworthiness of the information we provide and use it for different applications.\n\n\nCategory Annotation\n\nAs described in Section 3.2, we assign each 3D model to one or more synsets in the WordNet taxonomy.\n\nAnnotation Models are retrieved by textual query into the online repositories that we collected, and the initial category annotation is set to the used textual query for each retrieved model. After we retrieve these models we use the popularity score of each model on the repository to sort models and ask human workers to verify the assigned category annotation. This is sensible since the more popular models tend to be high quality and correctly retrieved through the category keyword textual query. We stop verifying category annotations with people once the positive ratio is lower than a 2% threshold.\n\nClean-up In order for the dataset to be easily usable by researchers it should contain clean and high quality 3D models. Through inspection, we identify and group 3D models into the following categories: single 3D models, 3D scenes, billboards, and big ground plane.\n\n\u2022 Single 3D models: semantically distinct objects; focus of our ShapeNetCore annotation effort.\n\n\u2022 3D scenes: detected by counting the number of connected components in a voxelized representation. We manually verify these detections and mark scenes for future analysis.\n\n\u2022 Billboards: planes with a painted texture. Often used to represent people and trees. These models are generally not useful for geometric analysis. They can be detected by checking whether a single plane can fit all vertices.\n\n\u2022 Big ground plane: object of interest placed on a large horizontal plane or in front of large vertical plane. Although we do not currently use these models, the plane can easily be identified and removed through simple geometric analysis.\n\nWe currently include the single 3D models in the ShapeNetCore subset of ShapeNet.\n\n\nHierarchical Rigid Alignment\n\nThe goal of this step is to establish a consistent canonical orientation for models within each category. Such alignment is important for various tasks such as visualizing shapes, shape classification and shape recognition. Figure 3 shows several categories in ShapeNet that have been consistently aligned.\n\nThough the concept of consistent orientation seems natural, one issue has to be addressed. We explain by an example. \"armchair\", \"chair\" and \"seat\" are three categories in our taxonomy, each being a subcategory of its successor. Consistent orientation can be well defined for shapes in the \"armchair\" category, by checking arms, legs and backs. Yet, it becomes difficult to define for the \"chair\" category. For example, \"side chair\" and \"swivel chair\" are both subcategories of \"chair\", however, swivel chairs have a very different leg structure than most side chairs. It becomes even more ambiguous to define for \"seat\", which has subcategories such as \"stool\", \"couch\", and \"chair\". However, the concept of an upright orientation still applies throughout most levels of the taxonomy.\n\nFollowing the above discussion, it is natural for us to propose a hierarchical alignment method, with a small amount of human supervision. The basic idea is to hierarchically align models following the taxonomy of ShapeNet in a bottom-up manner, i.e., we start from aligning shapes in low-level categories and then gradually elevate to higher level categories. When we proceed to the higher level, the self-consistent orientation within a subcategory should be maintained. For the alignment at each level, we first use a geometric algorithm described in the Appendix A.1, and then ask human experts to check and correct possible misalignments. With this strategy, we efficiently obtain consistent orientations. In practice, most shapes in the same lowlevel categories can be well aligned algorithmically, requiring limited manual correction. Though the proportion of manual corrections increases for aligning higher-level categories, the number of categories at each level becomes logarithmically smaller.\n\n\nParts and Keypoints\n\nTo obtain part and keypoint annotations we start from some curated part annotations within each category. For parts, this acquisition can be speeded up by having algorithmically generated segmentations and then having users accept or modify parts from these. We intend to experiment with both 2D and 3D interfaces for this task. We then exploit a number of different algorithmic techniques to propagate this information to other nearby shapes. Such methods can rely on rigid alignments in 3D, feature descriptor alignments in an appropriately defined feature space, or general shape correspondences. We iterate this pipeline, using active learning to estimate the 3D models and regions of these models where further human annotation would be most informative, generate a new set of crowd-sourced annotation tasks, algorithmically propagate their results, and so on. In the end we have users verify all proposed parts and keypoints, as verification is much faster than direct annotation.\n\n\nSymmetry Estimation\n\nWe provide bilateral symmetry plane detections for all 3D models in ShapeNetCore. Our method is a modified version of [22]. The basic idea is to use hough transform to vote on the parameters of the symmetry plane. More specifically, we generate all combinations of pairs of vertices from the mesh. Each pair casts a vote of a possible symmetry plane in the discretized space of plane parameters partitioned evenly. We then pick the parameter with the most votes as the symmetry plane candidate. As a final step, this candidate is verified to ensure that every vertex has a symmetric counterpart.\n\n\nPhysical Property Estimation\n\nBefore computing physical attribute annotations, the dimensions of the models need to be correspond to the real world. We estimate the absolute dimensions of models using prior work in size estimation [25], followed by manual verification. With the given absolute dimensions, we now compute the total solid volume of each model through filled-in voxelization. We use the space carving approach implemented by Binvox [23]. Categories of objects that are known to be container-like (i.e., bottles, microwaves) are annotated as such and only the surface voxelization volume is used instead. We then estimate the proportional material composition of each object category and use a table of material densities along with each model instance volume to compute a rough total weight estimate for that instance. More details about the acquisition of these physical attribute annotations are available separately [26].\n\n\nCurrent Statistics\n\nAt the time of this technical report, ShapeNet has indexed roughly 3,000,000 models. 220,000 models of these models are classified into 3,135 categories (Word-Net synsets). Below we provide detailed statistics for the currently annotated models in ShapeNet as a whole, as well as details of the available publicly released subsets of ShapeNet. Figure 4 shows the distributions of the number of shapes per synset at various taxonomy levels for the current ShapeNetCore corpus. To the best of our knowledge, ShapeNet is the largest clean shape dataset available in terms of total number of shapes, average num-ber of shapes per category, as well as the number of categories.\n\n\nCategory Distribution\n\nWe observe that ShapeNet as a whole is strongly biased towards categories of rigid man-made artifacts, due to the bias of the source 3D model repositories. This is in contrast to common image database statistics that contain more natural objects such as plants and animals [30]. This distribution bias is probably due to a combination of factors: 1) meshes of natural objects are more difficult to design using common CAD software; 2) 3D model consumers are typically more interested in artificial objects such as those observed in modern urban lifestyles. The former factor can be mitigated in the near future by using the rapidly improving depth sensing and 3D scanning technology.\n\n\nShapeNetCore\n\nShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+ [35], a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore. The category distribution of ShapeNetCore is shown in Table 2.\n\n\nShapeNetSem\n\nShapeNetSem is a smaller, more densely annotated subset consisting of 12,000 models spread over a broader set of 270 categories. In addition to manually verified category labels and consistent alignments, these models are annotated with real-world dimensions, estimates of their material composition at the category level, and estimates of their total volume and weight. The total numbers of models for the top 100 categories in this subset are given in Table 3.\n\n\nDiscussion and Future Work\n\nThe construction of ShapeNet is a continuous, ongoing effort. Here we have just described the initial steps we have taken in defining ShapeNet and populating a core subset of model annotations that we hope will prove useful to the community. We plan to grow ShapeNet in four distinct directions:\n\nAdditional annotation types We will introduce several additional types of annotations that have strong connections to the semantics and functionality of objects. Firstly, hierarchical part decompositions of objects will provide a useful finer granularity description of object structure that can be leveraged for part segmentation and shape synthesis. Secondly, physical object property annotations such as materials and their attributes will allow higher fidelity physics and appearance simulation, adding another layer of understanding to methods in vision and graphics. 0 K 5 K 1 0 K 1 5 K 2 0 K 2 5 K 3 0 K 3 5 K 4 0 K 4 5 K 5 0 K 5 5 K 6 0 K 6 5 K 7 0 K 7 5 K 8 0 K 8 5 K 9 0 K 9 5 K1 0 0 K1 0 5 K1 1 0 K1 1 5 K1 2 0 K a r t i f a c t p l a n t p e r s o n a n i ma l a t h l e t i c s n a t u r a l o b j e c t g e o l o g i c a l R o o t 0 K 2 K 4 K 6 K 8 K 1 0 K 1 2 K 1 4 K 1 6 K 1 8 K 2 0 K 2 2 K 2 4 K 2 6 K 2 8 K 3 0 K 3 2 K 3 4 K d e v i c e f u r n i t u r e c o n t a i n e r t r a n s p o r t e q u i p me n t i mp l e me n t we a p o n r y  \nA r t i f a c t 0 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 8 K\n\nID\n\nName Num  ID  Name  Num  ID  Name  Num  04379243  table  8443 03593526  jar  597 04225987 skateboard  152  02958343  car  7497 02876657  bottle  498 04460130  tower  133  03001627  chair  6778 02871439  bookshelf  466 02942699  camera  113  02691156  airplane  4045 03642806  laptop  460 02801938  basket  113  04256520  sofa  3173 03624134  knife  424 02946921  can  108  04090263  rifle  2373 04468005  train  389 03938244  pillow  96  03636649  lamp  2318 02747177  trash bin  343 03710193  mailbox  94  04530566  watercraft  1939 03790512  motorbike  337 03207941 dishwasher  93  02828884  bench  1816 03948459  pistol  307 04099429  rocket  85  03691459 loudspeaker 1618 03337140 file cabinet  298 02773838  bag  83  02933112  cabinet  1572 02818832  bed  254 02843684  birdhouse  73  03211117  display  1095 03928116  piano  239 03261776  earphone  73  04401088  telephone  1052 04330267  stove  218 03759954 microphone  67  02924116  bus  939 03797390  mug  214 04074963  remote  67  02808440  bathtub  857 02880940  bowl  186 03085013  keyboard  65  03467517  guitar  797 04554684  washer  169 02834778  bicycle  59  03325088  faucet  744 04004475  printer  166 02954340  cap  56  03046257  clock  655 03513137  helmet  162  03991062  flowerpot  602 03761084 microwaves  152  Total  57386   Table 2. Statistics of ShapeNetCore synsets. ID corresponds to WordNet synset offset, which is aligned with ImageNet.  Chair  696  Monitor  127  WallLamp  78  Gun  54  FlagPole  38  Lamp  663  RoundTable  120  SideChair  77  Nightstand  53  TvStand  38  ChestOfDrawers  511  TrashBin  117  VideoGameConsole  75  Mug  51  Fireplace  37  Table  427 DrinkingUtensil  112  MediaStorage  73  AccentChair  50  Rack  37  Couch  413  DeskLamp  110  Painting  73  ChessBoard  49  LightSwitch  36  Computer  244  Clock  101  Desktop  71  Rug  49  Oven  36  Dresser  234  ToyFigure  101  AccentTable  70  WallUnit  46  Airplane  35  TV  233  Plant  98  Camera  70  Mirror  45  DresserWithMirror  35  WallArt  222  Armoire  95  Picture  69  Bowl  44  Calculator  34  Bed  221  QueenBed  94  Refrigerator  68  SodaCan  44  TableClock  34  Cabinet  221  Stool  92  Speaker  68  VideoGameController  44  Toilet  34  FloorLamp  201  EndTable  91  Sideboard  67  WallClock  43  Cup  33  Desk  189  Bottle  88  Barstool  66  Printer  42  Stapler Table 3. Total number of models for the top 100 ShapeNetSem categories (out of 270 categories). Each category is also linked to the corresponding WordNet synset, establishing the same linkage to WordNet and ImageNet as with ShapeNetCore.\n\nCorrespondences One of the most important goals of ShapeNet is to provide a dense network of correspondences between 3D models and their parts. This will be invaluable for enabling much shape analysis research and helping to improve and evaluate methods for many traditional tasks such as alignment and segmentation. Additionally, we plan to provide correspondences between 3D model parts and image patches in ImageNet -a link that will be critical for propagating information between image space and 3D models.\n\n\nRGB-D data\n\nThe rapid proliferation of commodity RGB-D sensors is already making the process of capturing real-world environments better and more efficient. Expanding ShapeNet to include shapes reconstructed from scanned RGB-D data is a critical goal. We foresee that over time, the amount of available reconstructed shape data will overshadow the existing designed 3D model data and as such this is a natural growth direction for ShapeNet. A related effort that we are currently undertaking is to align 3D models to objects observed in RGB-D frames. This will establish a powerful connection between real world observations and 3D models.\n\nAnnotation coverage We will continue to expand the set of annotated models to cover a bigger subset of the entirety of ShapeNet. We will explore combinations of algorithmic propagation methods and crowd-sourcing for verification of the algorithmic results.\n\n\nConclusion\n\nWe firmly believe that ShapeNet will prove to be an immensely useful resource to several research communities in several ways: Data-driven research By establishing ShapeNet as the first large-scale 3D shape dataset of its kind we can help to move computer graphics research toward a data-driven direction following recent developments in vision and NLP. Additionally, we can help to enable larger-scale quantitative analysis of proposed systems that can clarify the benefits of particular methodologies against a broader and more representative variety of 3D model data.\n\nTraining resource By providing a large-scale, richly annotated dataset we can also promote a broad class of recently resurgent machine learning and neural network methods for applications dealing with geometric data. Much like research in computer vision and natural language understanding, computational geometry and graphics stand to benefit immensely from these data-driven learning approaches.\n\nBenchmark dataset We hope that ShapeNet will grow to become a canonical benchmark dataset for several evaluation tasks and challenges. In this way, we would like to engage the broader research community in helping us define and grow ShapeNet to be a pivotal dataset with long-lasting impact.\n\nFigure 1 .\n1Screenshot of the online ShapeNet taxonomy view, organizing contained 3D models under WordNet synsets.\n\nFigure 2 .\n2ShapeNet annotations illustrated for an example chair model. Left: links to the WordNet taxonomy provide definitions of objects, is-a and has-a relations, and a connection to images from ImageNet. Middle-left: shape is aligned to a consistent upright and front orientation, and symmetries are computed Middle-right: hierarchical decomposition of shape into parts on which various attributes are defined: names, symmetries, dimensions, materials, and masses. Right: part-to-part and point-to-point connections are established at all levels within ShapeNet producing a dense and semantically rich network of correspondences. The gray background indicates annotations that are currently ongoing and not yet available for release.\n\nFigure 3 .\n3Examples of aligned models in the chair, laptop, bench, and airplane synsets.\n\nFigure 4 .\n4Plots of the distribution of ShapeNet models over WordNet synsets at multiple levels of the taxonomy (only the top few children synsets are shown at each level). The highest level (root) is at the top and the taxonomy levels become lower downwards and to the right. Note the bias towards rigid man-made artifacts at the top and the broad coverage of many low level categories towards the bottom.\n\n\nHypernyms: chair > seat > furniture > ... Part meronyms: backrest, seat, base Sister terms: armchair, barber chair, ...Swivel chair \n\nBackrest \n\nSeat \n\nBase \nLeg \n\nWheel \n\nImageNet \n\nWordNet synset \n\nPart Hierarchy Part Correspondences \nLink to WordNet Taxonomy \n\nSwivel chair: a chair that swivels \non its base \n\n\nhttp://lucene.apache.org/solr/\nA. Appendix A.1. Hierarchical Rigid AlignmentIn the following, we describe our hierarchical rigid alignment algorithm in more detail.As a pre-processing step, we first semi-automatically align the upright orientation of each shape. Fortunately, most shapes downloaded from the web are by default placed in the upright orientations. For those that are not, we filter them out by manual inspection. We then convert models to point clouds through furthest point sampling and perform PCA on the point sets. Finally, we ask a person to pick the vector of correct upright orientation from six candidates containing the PCA axes and their reverse directions.Starting from a leaf category in ShapeNet, we jointly align all shapes following prior work[8]. If a leaf category has more than 100 shapes, we further partition it into smaller, more coherent clusters by k-means clustering using pose-invariant global features, such as phase-invariant HoG features [see appendix]. Here we briefly review[8]. Each shape is associated with a random variable, denoting the transformation of the shape from its original pose to the consistent canonical pose. Over the set of shapes, a Markov Random Field (MRF) is constructed, whose energy function measures the consistency of all pairs of shapes after applying their transformations. In practice, the space of rigid transformations is discretized into N bins. We perform MAP inference over the MRF to find the optimal transformation for each shape. We then manual inspect the results and correct occasional errors.After this step, we represent each leaf node category by the shape in the centroid of the feature space. Then, we gather the representative shapes for all leaf categories of an intermediate category and apply[8]again for joint alignment. This higher-level algorithmic alignment is verified by a person again. The procedure is applied along the taxonomy hierarchy until the root node is reached.\nThe protein data bank. M Helen, John Berman, Zukang Westbrook, Gary Feng, Gilliland, Helge Bhat, Weissig, N Ilya, Philip E Shindyalov, Bourne, Nucleic Acids Res. 282Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, TN Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic Acids Res, 28:235-242, 2000. 2\n\nA benchmark for 3D mesh segmentation. Xiaobai Chen, Aleksey Golovinskiy, Thomas Funkhouser, 73:1-73:12ACM TOG. 283Xiaobai Chen, Aleksey Golovinskiy, and Thomas Funkhouser. A benchmark for 3D mesh segmentation. ACM TOG, 28(3):73:1-73:12, July 2009. 2\n\nBill Pang, and Thomas Funkhouser. Schelling points on 3D surface meshes. Xiaobai Chen, Abulhair Saparov, ACM TOGXiaobai Chen, Abulhair Saparov, Bill Pang, and Thomas Funkhouser. Schelling points on 3D surface meshes. ACM TOG, August 2012. 2\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. 1, 2, 4\n\n. Bianca Falcidieno, Aim@shape, Bianca Falcidieno. Aim@shape. http://www. aimatshape.net/ontologies/shapes/, 2005. 2\n\nExample-based synthesis of 3D object arrangements. Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, Pat Hanrahan, ACM TOG31135Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. Example-based synthesis of 3D object arrangements. ACM TOG, 31(6):135, 2012. 1\n\n. Paul-Louis George, Gamma, Paul-Louis George. Gamma. http://www.rocq. inria.fr/gamma/download/download.php, 2007. 2\n\nFine-grained semi-supervised labeling of large shape collections. Qixing Huang, Hao Su, Leonidas Guibas, ACM TOG3211Qixing Huang, Hao Su, and Leonidas Guibas. Fine-grained semi-supervised labeling of large shape collections. ACM TOG, 32:190:1-190:10, 2013. 4, 11\n\nDeveloping an engineering shape benchmark for CAD models. Subramaniam Jayanti, Yagnanarayanan Kalyanaraman, Natraj Iyer, Karthik Ramani, Computer-Aided Design. 3Subramaniam Jayanti, Yagnanarayanan Kalyanaraman, Na- traj Iyer, and Karthik Ramani. Developing an engineering shape benchmark for CAD models. Computer-Aided Design, 2006. 3\n\nA probabilistic model for component-based shape synthesis. Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne Koller, Vladlen Koltun, ACM TOG3155Evangelos Kalogerakis, Siddhartha Chaudhuri, Daphne Koller, and Vladlen Koltun. A probabilistic model for component-based shape synthesis. ACM TOG, 31:55, 2012. 1\n\nMobius transformations for global intrinsic symmetry analysis. Vladimir Kim, Yaron Lipman, Xiaobai Chen, Thomas Funkhouser, Symposium on Geometry Processing. Vladimir Kim, Yaron Lipman, Xiaobai Chen, and Thomas Funkhouser. Mobius transformations for global intrinsic symmetry analysis. Symposium on Geometry Processing, July 2010. 2\n\nLearning part-based templates from large collections of 3D shapes. Vladimir G Kim, Wilmot Li, Niloy J Mitra, Siddhartha Chaudhuri, Stephen Diverdi, Thomas Funkhouser, 70:1-70:12ACM TOG32Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Siddhartha Chaudhuri, Stephen DiVerdi, and Thomas Funkhouser. Learning part-based templates from large collections of 3D shapes. ACM TOG, 32(4):70:1-70:12, July 2013. 2\n\nExploring collections of 3D models using fuzzy correspondences. Vladimir G Kim, Wilmot Li, Niloy J Mitra, Stephen Diverdi, Thomas Funkhouser, ACM TOG. 3144Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen DiVerdi, and Thomas Funkhouser. Exploring collections of 3D models using fuzzy correspondences. ACM TOG, 31(4):54:1-54:11, July 2012. 2, 4\n\n3D object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, 4th International IEEE Workshop on 3D Representation and Recognition. Sydney, AustraliaJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013. 2\n\nPDBsum: A web-based database of summaries and analyses of all PDB structures. Gail Roman A Laskowski, Alex D Hutchinson, Michie, C Andrew, Wallace, L Martin, Janet M Jones, Thornton, Trends Biochem. Sci. 222Roman A Laskowski, E Gail Hutchinson, Alex D Michie, Andrew C Wallace, Martin L Jones, and Janet M Thornton. PDBsum: A web-based database of summaries and analyses of all PDB structures. Trends Biochem. Sci., 22:488-490, 1997. 2\n\nSHREC'12 track: generic 3D shape retrieval. Bo Li, Afzal Godil, Masaki Aono, Takahiko Bai, Furuya, Li, Henry L\u00f3pez-Sastre, Ryutarou Johan, Carolina Ohbuchi, Redondo-Cabrera, 5th Eurographics Conference on 3D Object Retrieval. Bo Li, Afzal Godil, Masaki Aono, X Bai, Takahiko Furuya, L Li, R L\u00f3pez-Sastre, Henry Johan, Ryutarou Ohbuchi, Car- olina Redondo-Cabrera, et al. SHREC'12 track: generic 3D shape retrieval. In 5th Eurographics Conference on 3D Ob- ject Retrieval, 2012. 3\n\nNihad Karim Chowdhury, Bin Fang, Takahiko Furuya, et al. SHREC'14 track: Large scale comprehensive 3D shape retrieval. Bo Li, Yijuan Lu, Chunyuan Li, Afzal Godil, Tobias Schreck, Masaki Aono, Qiang Chen, Eurographics Workshop on 3D Object Retrieval. Bo Li, Yijuan Lu, Chunyuan Li, Afzal Godil, Tobias Schreck, Masaki Aono, Qiang Chen, Nihad Karim Chowd- hury, Bin Fang, Takahiko Furuya, et al. SHREC'14 track: Large scale comprehensive 3D shape retrieval. In Euro- graphics Workshop on 3D Object Retrieval, 2014. 2\n\nMulti-view object class detection with a 3D geometric model. Joerg Liebelt, Cordelia Schmid, CVPR. Joerg Liebelt and Cordelia Schmid. Multi-view object class detection with a 3D geometric model. In CVPR, pages 1688- 1695. IEEE, 2010. 2\n\nCreating consistent scene graphs using a probabilistic grammar. Tianqiang Liu, Siddhartha Chaudhuri, Vladimir G Kim, Qi-Xing Huang, Niloy J Mitra, Thomas Funkhouser, ACM TOGTianqiang Liu, Siddhartha Chaudhuri, Vladimir G. Kim, Qi- Xing Huang, Niloy J. Mitra, and Thomas Funkhouser. Cre- ating consistent scene graphs using a probabilistic grammar. ACM TOG, December 2014. 2\n\nBuilding a large annotated corpus of english: The Penn Treebank. P Mitchell, Mary Ann Marcus, Beatrice Marcinkiewicz, Santorini, Computational linguistics. 192Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The Penn Treebank. Computational linguistics, 19(2):313-330, 1993. 1\n\nWordNet: a lexical database for English. CACM. George A Miller, George A. Miller. WordNet: a lexical database for English. CACM, 1995. 1, 2, 3, 4\n\nSymmetry in 3D geometry: Extraction and applications. J Niloy, Mark Mitra, Michael Pauly, Duygu Wand, Ceylan, Computer Graphics Forum. 32Niloy J Mitra, Mark Pauly, Michael Wand, and Duygu Cey- lan. Symmetry in 3D geometry: Extraction and applications. In Computer Graphics Forum, volume 32, pages 1-23, 2013. 7\n\nSimplification and repair of polygonal models using volumetric techniques. Visualization and Computer Graphics. S Fakir, Greg Nooruddin, Turk, IEEE Transactions on. 7Fakir S. Nooruddin and Greg Turk. Simplification and repair of polygonal models using volumetric techniques. Visualiza- tion and Computer Graphics, IEEE Transactions on, 2003. 7\n\nBuilding a database of 3D scenes from user annotations. C Bryan, Antonio Russell, Torralba, CVPR. Bryan C Russell and Antonio Torralba. Building a database of 3D scenes from user annotations. In CVPR, 2009. 2\n\nOn being the right scale: Sizing large collections of 3D models. Manolis Savva, Angel X Chang, Gilbert Bernstein, Christopher D Manning, Pat Hanrahan, SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision. Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christo- pher D. Manning, and Pat Hanrahan. On being the right scale: Sizing large collections of 3D models. In SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision, 2014. 7\n\nSemantically-Enriched 3D Models for Common-sense Knowledge. Manolis Savva, Angel X Chang, Pat Hanrahan, CVPR 2015 Workshop on Functionality, Physics, Intentionality and Causality. Manolis Savva, Angel X. Chang, and Pat Hanrahan. Semantically-Enriched 3D Models for Common-sense Knowledge. CVPR 2015 Workshop on Functionality, Physics, Intentionality and Causality, 2015. 7\n\nThe Princeton shape benchmark. Philip Shilane, Patrick Min, Michael Kazhdan, Thomas Funkhouser, Shape Modeling Applications. 23Philip Shilane, Patrick Min, Michael Kazhdan, and Thomas Funkhouser. The Princeton shape benchmark. In Shape Modeling Applications. IEEE, 2004. 2, 3\n\nSliding shapes for 3D object detection in depth images. Shuran Song, Jianxiong Xiao, ECCV. Shuran Song and Jianxiong Xiao. Sliding shapes for 3D ob- ject detection in depth images. In ECCV, 2014. 1\n\nA large-scale shape benchmark for 3D object retrieval: Toyohashi shape benchmark. Atsushi Tatsuma, Hitoshi Koyanagi, Masaki Aono, Asia Pacific Signal and Information Processing Association. Atsushi Tatsuma, Hitoshi Koyanagi, and Masaki Aono. A large-scale shape benchmark for 3D object retrieval: Toy- ohashi shape benchmark. In Asia Pacific Signal and Infor- mation Processing Association, 2012. 3\n\nAntonio Torralba, C Bryan, Jenny Russell, Yuen, La-Belme, Online image annotation and applications. Proceedings of the IEEE. 98Antonio Torralba, Bryan C Russell, and Jenny Yuen. La- belMe: Online image annotation and applications. Proceed- ings of the IEEE, 98(8):1467-1484, 2010. 7\n\nSHREC 2007 3D shape retrieval contest. C Remco, Veltkamp, Harr, UU-CS-2007-015Utrecht UniversityTechnical ReportRemco C. Veltkamp and FB ter Harr. SHREC 2007 3D shape retrieval contest. Technical report, Utrecht University Tech- nical Report UU-CS-2007-015, 2007. 3\n\n. V Dejan, Vrani\u0107, Germany3D model retrieval. University of LeipzigPhD thesisDejan V Vrani\u0107. 3D model retrieval. University of Leipzig, Germany, PhD thesis, 2004. 3\n\nA 3D shape benchmark for retrieval and automatic classification of architectural data. Raoul Wessel, Ina Bl\u00fcmel, Reinhard Klein, Eurographics 2009 Workshop on 3D Object Retrieval. Raoul Wessel, Ina Bl\u00fcmel, and Reinhard Klein. A 3D shape benchmark for retrieval and automatic classification of ar- chitectural data. In Eurographics 2009 Workshop on 3D Ob- ject Retrieval, pages 53-56. The Eurographics Association, 2009. 3\n\n3D ShapeNets: A Deep Representation for Volumetric Shapes. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao, Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin- guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3D ShapeNets: A Deep Representation for Volumetric Shapes. CVPR, 2015. 1, 2, 4\n\nBeyond PASCAL: A benchmark for 3D object detection in the wild. Yu Xiang, Roozbeh Mottaghi, Silvio Savarese, WACV. 27Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond PASCAL: A benchmark for 3D object detection in the wild. In WACV, 2014. 2, 7\n\nSUN3D: A database of big spaces reconstructed using SfM and object labels. Jianxiong Xiao, Andrew Owens, Antonio Torralba, ICCV. Jianxiong Xiao, Andrew Owens, and Antonio Torralba. SUN3D: A database of big spaces reconstructed using SfM and object labels. In ICCV, pages 1625-1632, 2013. 2\n\nRetrieving articulated 3-D models using medial surfaces and their graph spectra. In Energy minimization methods in computer vision and pattern recognition. Juan Zhang, Kaleem Siddiqi, Diego Macrini, Ali Shokoufandeh, Sven Dickinson, Juan Zhang, Kaleem Siddiqi, Diego Macrini, Ali Shokoufan- deh, and Sven Dickinson. Retrieving articulated 3-D mod- els using medial surfaces and their graph spectra. In Energy minimization methods in computer vision and pattern recog- nition, 2005. 3\n", "annotations": {"author": "[{\"end\":89,\"start\":53},{\"end\":131,\"start\":90},{\"end\":170,\"start\":132},{\"end\":206,\"start\":171},{\"end\":264,\"start\":207},{\"end\":317,\"start\":265},{\"end\":356,\"start\":318},{\"end\":393,\"start\":357},{\"end\":429,\"start\":394},{\"end\":459,\"start\":430},{\"end\":498,\"start\":460},{\"end\":527,\"start\":499},{\"end\":561,\"start\":528}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":61},{\"end\":107,\"start\":97},{\"end\":147,\"start\":141},{\"end\":183,\"start\":175},{\"end\":219,\"start\":214},{\"end\":272,\"start\":270},{\"end\":333,\"start\":325},{\"end\":370,\"start\":365},{\"end\":405,\"start\":401},{\"end\":436,\"start\":434},{\"end\":474,\"start\":470},{\"end\":504,\"start\":502},{\"end\":537,\"start\":535}]", "author_first_name": "[{\"end\":58,\"start\":53},{\"end\":60,\"start\":59},{\"end\":96,\"start\":90},{\"end\":140,\"start\":132},{\"end\":174,\"start\":171},{\"end\":213,\"start\":207},{\"end\":269,\"start\":265},{\"end\":324,\"start\":318},{\"end\":364,\"start\":357},{\"end\":400,\"start\":394},{\"end\":433,\"start\":430},{\"end\":469,\"start\":460},{\"end\":501,\"start\":499},{\"end\":534,\"start\":528}]", "author_affiliation": "[{\"end\":88,\"start\":68},{\"end\":130,\"start\":109},{\"end\":169,\"start\":149},{\"end\":205,\"start\":185},{\"end\":263,\"start\":221},{\"end\":316,\"start\":274},{\"end\":355,\"start\":335},{\"end\":392,\"start\":372},{\"end\":428,\"start\":407},{\"end\":458,\"start\":438},{\"end\":497,\"start\":476},{\"end\":526,\"start\":506},{\"end\":560,\"start\":539}]", "title": "[{\"end\":50,\"start\":1},{\"end\":611,\"start\":562}]", "venue": null, "abstract": "[{\"end\":1661,\"start\":642}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2653,\"start\":2649},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2656,\"start\":2653},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3398,\"start\":3394},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3433,\"start\":3430},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3706,\"start\":3702},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3720,\"start\":3716},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3738,\"start\":3735},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6357,\"start\":6354},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6522,\"start\":6519},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6524,\"start\":6522},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7037,\"start\":7033},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7321,\"start\":7317},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7487,\"start\":7484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7509,\"start\":7505},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7512,\"start\":7509},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7530,\"start\":7526},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7547,\"start\":7543},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7569,\"start\":7566},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7609,\"start\":7605},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7651,\"start\":7647},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7676,\"start\":7673},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7853,\"start\":7850},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8024,\"start\":8021},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8125,\"start\":8121},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8251,\"start\":8247},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8562,\"start\":8558},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8614,\"start\":8610},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8617,\"start\":8614},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8901,\"start\":8898},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9039,\"start\":9035},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10054,\"start\":10050},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10108,\"start\":10104},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10146,\"start\":10142},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10187,\"start\":10183},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10226,\"start\":10222},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10264,\"start\":10260},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10304,\"start\":10300},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10350,\"start\":10347},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12931,\"start\":12927},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13261,\"start\":13258},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14156,\"start\":14152},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14446,\"start\":14443},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15417,\"start\":15413},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15443,\"start\":15440},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15470,\"start\":15466},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26240,\"start\":26236},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26951,\"start\":26947},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27166,\"start\":27162},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27653,\"start\":27649},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28652,\"start\":28648},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29332,\"start\":29328}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36769,\"start\":36654},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37509,\"start\":36770},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37600,\"start\":37510},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38009,\"start\":37601},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38326,\"start\":38010}]", "paragraph": "[{\"end\":2126,\"start\":1677},{\"end\":2952,\"start\":2128},{\"end\":3618,\"start\":2954},{\"end\":4015,\"start\":3620},{\"end\":4074,\"start\":4017},{\"end\":4173,\"start\":4076},{\"end\":4229,\"start\":4175},{\"end\":4369,\"start\":4231},{\"end\":4455,\"start\":4371},{\"end\":4507,\"start\":4457},{\"end\":5057,\"start\":4509},{\"end\":5209,\"start\":5059},{\"end\":5784,\"start\":5211},{\"end\":6433,\"start\":5816},{\"end\":7178,\"start\":6435},{\"end\":7854,\"start\":7180},{\"end\":8752,\"start\":7856},{\"end\":9454,\"start\":8754},{\"end\":9824,\"start\":9508},{\"end\":10772,\"start\":9826},{\"end\":11219,\"start\":10774},{\"end\":11567,\"start\":11221},{\"end\":11908,\"start\":11587},{\"end\":12502,\"start\":11910},{\"end\":13472,\"start\":12504},{\"end\":13926,\"start\":13493},{\"end\":14874,\"start\":13928},{\"end\":15243,\"start\":14876},{\"end\":15767,\"start\":15245},{\"end\":16089,\"start\":15769},{\"end\":16370,\"start\":16091},{\"end\":16539,\"start\":16372},{\"end\":16974,\"start\":16541},{\"end\":17270,\"start\":16976},{\"end\":17505,\"start\":17272},{\"end\":17725,\"start\":17507},{\"end\":17932,\"start\":17727},{\"end\":18068,\"start\":17934},{\"end\":18368,\"start\":18070},{\"end\":19717,\"start\":18395},{\"end\":20116,\"start\":19719},{\"end\":20642,\"start\":20150},{\"end\":21127,\"start\":20684},{\"end\":21251,\"start\":21151},{\"end\":21860,\"start\":21253},{\"end\":22128,\"start\":21862},{\"end\":22225,\"start\":22130},{\"end\":22399,\"start\":22227},{\"end\":22627,\"start\":22401},{\"end\":22868,\"start\":22629},{\"end\":22951,\"start\":22870},{\"end\":23290,\"start\":22984},{\"end\":24077,\"start\":23292},{\"end\":25084,\"start\":24079},{\"end\":26094,\"start\":25108},{\"end\":26713,\"start\":26118},{\"end\":27654,\"start\":26746},{\"end\":28349,\"start\":27677},{\"end\":29058,\"start\":28375},{\"end\":29477,\"start\":29075},{\"end\":29955,\"start\":29493},{\"end\":30281,\"start\":29986},{\"end\":31341,\"start\":30283},{\"end\":33963,\"start\":31399},{\"end\":34476,\"start\":33965},{\"end\":35118,\"start\":34491},{\"end\":35376,\"start\":35120},{\"end\":35961,\"start\":35391},{\"end\":36360,\"start\":35963},{\"end\":36653,\"start\":36362}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":31393,\"start\":31342}]", "table_ref": "[{\"end\":7177,\"start\":7169},{\"end\":29954,\"start\":29947},{\"end\":32705,\"start\":31404},{\"end\":33725,\"start\":32817},{\"end\":33733,\"start\":33726}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1675,\"start\":1663},{\"attributes\":{\"n\":\"2.\"},\"end\":5814,\"start\":5787},{\"attributes\":{\"n\":\"3.\"},\"end\":9506,\"start\":9457},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11585,\"start\":11570},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13491,\"start\":13475},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18393,\"start\":18371},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20148,\"start\":20119},{\"attributes\":{\"n\":\"4.\"},\"end\":20682,\"start\":20645},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21149,\"start\":21130},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22982,\"start\":22954},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25106,\"start\":25087},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26116,\"start\":26097},{\"attributes\":{\"n\":\"4.5.\"},\"end\":26744,\"start\":26716},{\"attributes\":{\"n\":\"5.\"},\"end\":27675,\"start\":27657},{\"end\":28373,\"start\":28352},{\"attributes\":{\"n\":\"5.1.\"},\"end\":29073,\"start\":29061},{\"attributes\":{\"n\":\"5.2.\"},\"end\":29491,\"start\":29480},{\"attributes\":{\"n\":\"6.\"},\"end\":29984,\"start\":29958},{\"end\":31397,\"start\":31395},{\"end\":34489,\"start\":34479},{\"attributes\":{\"n\":\"7.\"},\"end\":35389,\"start\":35379},{\"end\":36665,\"start\":36655},{\"end\":36781,\"start\":36771},{\"end\":37521,\"start\":37511},{\"end\":37612,\"start\":37602}]", "table": "[{\"end\":38326,\"start\":38131}]", "figure_caption": "[{\"end\":36769,\"start\":36667},{\"end\":37509,\"start\":36783},{\"end\":37600,\"start\":37523},{\"end\":38009,\"start\":37614},{\"end\":38131,\"start\":38012}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9960,\"start\":9952},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13820,\"start\":13812},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23216,\"start\":23208},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28029,\"start\":28021}]", "bib_author_first_name": "[{\"end\":40323,\"start\":40322},{\"end\":40335,\"start\":40331},{\"end\":40350,\"start\":40344},{\"end\":40366,\"start\":40362},{\"end\":40389,\"start\":40384},{\"end\":40406,\"start\":40405},{\"end\":40419,\"start\":40413},{\"end\":40421,\"start\":40420},{\"end\":40697,\"start\":40690},{\"end\":40711,\"start\":40704},{\"end\":40731,\"start\":40725},{\"end\":40983,\"start\":40976},{\"end\":40998,\"start\":40990},{\"end\":41201,\"start\":41198},{\"end\":41211,\"start\":41208},{\"end\":41225,\"start\":41218},{\"end\":41240,\"start\":41234},{\"end\":41248,\"start\":41245},{\"end\":41255,\"start\":41253},{\"end\":41427,\"start\":41421},{\"end\":41595,\"start\":41588},{\"end\":41610,\"start\":41604},{\"end\":41627,\"start\":41620},{\"end\":41641,\"start\":41635},{\"end\":41657,\"start\":41654},{\"end\":42034,\"start\":42028},{\"end\":42045,\"start\":42042},{\"end\":42058,\"start\":42050},{\"end\":42295,\"start\":42284},{\"end\":42319,\"start\":42305},{\"end\":42340,\"start\":42334},{\"end\":42354,\"start\":42347},{\"end\":42630,\"start\":42621},{\"end\":42654,\"start\":42644},{\"end\":42672,\"start\":42666},{\"end\":42688,\"start\":42681},{\"end\":42943,\"start\":42935},{\"end\":42954,\"start\":42949},{\"end\":42970,\"start\":42963},{\"end\":42983,\"start\":42977},{\"end\":43281,\"start\":43273},{\"end\":43283,\"start\":43282},{\"end\":43295,\"start\":43289},{\"end\":43305,\"start\":43300},{\"end\":43307,\"start\":43306},{\"end\":43325,\"start\":43315},{\"end\":43344,\"start\":43337},{\"end\":43360,\"start\":43354},{\"end\":43678,\"start\":43670},{\"end\":43680,\"start\":43679},{\"end\":43692,\"start\":43686},{\"end\":43702,\"start\":43697},{\"end\":43704,\"start\":43703},{\"end\":43719,\"start\":43712},{\"end\":43735,\"start\":43729},{\"end\":44020,\"start\":44012},{\"end\":44036,\"start\":44029},{\"end\":44047,\"start\":44044},{\"end\":44056,\"start\":44054},{\"end\":44463,\"start\":44459},{\"end\":44487,\"start\":44483},{\"end\":44489,\"start\":44488},{\"end\":44511,\"start\":44510},{\"end\":44530,\"start\":44529},{\"end\":44544,\"start\":44539},{\"end\":44546,\"start\":44545},{\"end\":44864,\"start\":44862},{\"end\":44874,\"start\":44869},{\"end\":44888,\"start\":44882},{\"end\":44903,\"start\":44895},{\"end\":44926,\"start\":44921},{\"end\":44949,\"start\":44941},{\"end\":44965,\"start\":44957},{\"end\":45420,\"start\":45418},{\"end\":45431,\"start\":45425},{\"end\":45444,\"start\":45436},{\"end\":45454,\"start\":45449},{\"end\":45468,\"start\":45462},{\"end\":45484,\"start\":45478},{\"end\":45496,\"start\":45491},{\"end\":45881,\"start\":45876},{\"end\":45899,\"start\":45891},{\"end\":46125,\"start\":46116},{\"end\":46141,\"start\":46131},{\"end\":46161,\"start\":46153},{\"end\":46163,\"start\":46162},{\"end\":46176,\"start\":46169},{\"end\":46189,\"start\":46184},{\"end\":46191,\"start\":46190},{\"end\":46205,\"start\":46199},{\"end\":46493,\"start\":46492},{\"end\":46508,\"start\":46504},{\"end\":46512,\"start\":46509},{\"end\":46529,\"start\":46521},{\"end\":46822,\"start\":46816},{\"end\":46824,\"start\":46823},{\"end\":46971,\"start\":46970},{\"end\":46983,\"start\":46979},{\"end\":46998,\"start\":46991},{\"end\":47011,\"start\":47006},{\"end\":47341,\"start\":47340},{\"end\":47353,\"start\":47349},{\"end\":47630,\"start\":47629},{\"end\":47645,\"start\":47638},{\"end\":47855,\"start\":47848},{\"end\":47868,\"start\":47863},{\"end\":47870,\"start\":47869},{\"end\":47885,\"start\":47878},{\"end\":47908,\"start\":47897},{\"end\":47910,\"start\":47909},{\"end\":47923,\"start\":47920},{\"end\":48348,\"start\":48341},{\"end\":48361,\"start\":48356},{\"end\":48363,\"start\":48362},{\"end\":48374,\"start\":48371},{\"end\":48692,\"start\":48686},{\"end\":48709,\"start\":48702},{\"end\":48722,\"start\":48715},{\"end\":48738,\"start\":48732},{\"end\":48994,\"start\":48988},{\"end\":49010,\"start\":49001},{\"end\":49220,\"start\":49213},{\"end\":49237,\"start\":49230},{\"end\":49254,\"start\":49248},{\"end\":49538,\"start\":49531},{\"end\":49550,\"start\":49549},{\"end\":49563,\"start\":49558},{\"end\":49855,\"start\":49854},{\"end\":50085,\"start\":50084},{\"end\":50340,\"start\":50335},{\"end\":50352,\"start\":50349},{\"end\":50369,\"start\":50361},{\"end\":50737,\"start\":50730},{\"end\":50748,\"start\":50742},{\"end\":50761,\"start\":50755},{\"end\":50776,\"start\":50770},{\"end\":50789,\"start\":50781},{\"end\":50803,\"start\":50797},{\"end\":50819,\"start\":50810},{\"end\":51074,\"start\":51072},{\"end\":51089,\"start\":51082},{\"end\":51106,\"start\":51100},{\"end\":51343,\"start\":51334},{\"end\":51356,\"start\":51350},{\"end\":51371,\"start\":51364},{\"end\":51710,\"start\":51706},{\"end\":51724,\"start\":51718},{\"end\":51739,\"start\":51734},{\"end\":51752,\"start\":51749},{\"end\":51771,\"start\":51767}]", "bib_author_last_name": "[{\"end\":40329,\"start\":40324},{\"end\":40342,\"start\":40336},{\"end\":40360,\"start\":40351},{\"end\":40371,\"start\":40367},{\"end\":40382,\"start\":40373},{\"end\":40394,\"start\":40390},{\"end\":40403,\"start\":40396},{\"end\":40411,\"start\":40407},{\"end\":40432,\"start\":40422},{\"end\":40440,\"start\":40434},{\"end\":40702,\"start\":40698},{\"end\":40723,\"start\":40712},{\"end\":40742,\"start\":40732},{\"end\":40988,\"start\":40984},{\"end\":41006,\"start\":40999},{\"end\":41206,\"start\":41202},{\"end\":41216,\"start\":41212},{\"end\":41232,\"start\":41226},{\"end\":41243,\"start\":41241},{\"end\":41251,\"start\":41249},{\"end\":41263,\"start\":41256},{\"end\":41438,\"start\":41428},{\"end\":41449,\"start\":41440},{\"end\":41602,\"start\":41596},{\"end\":41618,\"start\":41611},{\"end\":41633,\"start\":41628},{\"end\":41652,\"start\":41642},{\"end\":41666,\"start\":41658},{\"end\":41863,\"start\":41846},{\"end\":41870,\"start\":41865},{\"end\":42040,\"start\":42035},{\"end\":42048,\"start\":42046},{\"end\":42065,\"start\":42059},{\"end\":42303,\"start\":42296},{\"end\":42332,\"start\":42320},{\"end\":42345,\"start\":42341},{\"end\":42361,\"start\":42355},{\"end\":42642,\"start\":42631},{\"end\":42664,\"start\":42655},{\"end\":42679,\"start\":42673},{\"end\":42695,\"start\":42689},{\"end\":42947,\"start\":42944},{\"end\":42961,\"start\":42955},{\"end\":42975,\"start\":42971},{\"end\":42994,\"start\":42984},{\"end\":43287,\"start\":43284},{\"end\":43298,\"start\":43296},{\"end\":43313,\"start\":43308},{\"end\":43335,\"start\":43326},{\"end\":43352,\"start\":43345},{\"end\":43371,\"start\":43361},{\"end\":43684,\"start\":43681},{\"end\":43695,\"start\":43693},{\"end\":43710,\"start\":43705},{\"end\":43727,\"start\":43720},{\"end\":43746,\"start\":43736},{\"end\":44027,\"start\":44021},{\"end\":44042,\"start\":44037},{\"end\":44052,\"start\":44048},{\"end\":44064,\"start\":44057},{\"end\":44481,\"start\":44464},{\"end\":44500,\"start\":44490},{\"end\":44508,\"start\":44502},{\"end\":44518,\"start\":44512},{\"end\":44527,\"start\":44520},{\"end\":44537,\"start\":44531},{\"end\":44552,\"start\":44547},{\"end\":44562,\"start\":44554},{\"end\":44867,\"start\":44865},{\"end\":44880,\"start\":44875},{\"end\":44893,\"start\":44889},{\"end\":44907,\"start\":44904},{\"end\":44915,\"start\":44909},{\"end\":44919,\"start\":44917},{\"end\":44939,\"start\":44927},{\"end\":44955,\"start\":44950},{\"end\":44973,\"start\":44966},{\"end\":44990,\"start\":44975},{\"end\":45423,\"start\":45421},{\"end\":45434,\"start\":45432},{\"end\":45447,\"start\":45445},{\"end\":45460,\"start\":45455},{\"end\":45476,\"start\":45469},{\"end\":45489,\"start\":45485},{\"end\":45501,\"start\":45497},{\"end\":45889,\"start\":45882},{\"end\":45906,\"start\":45900},{\"end\":46129,\"start\":46126},{\"end\":46151,\"start\":46142},{\"end\":46167,\"start\":46164},{\"end\":46182,\"start\":46177},{\"end\":46197,\"start\":46192},{\"end\":46216,\"start\":46206},{\"end\":46502,\"start\":46494},{\"end\":46519,\"start\":46513},{\"end\":46543,\"start\":46530},{\"end\":46554,\"start\":46545},{\"end\":46831,\"start\":46825},{\"end\":46977,\"start\":46972},{\"end\":46989,\"start\":46984},{\"end\":47004,\"start\":46999},{\"end\":47016,\"start\":47012},{\"end\":47024,\"start\":47018},{\"end\":47347,\"start\":47342},{\"end\":47363,\"start\":47354},{\"end\":47369,\"start\":47365},{\"end\":47636,\"start\":47631},{\"end\":47653,\"start\":47646},{\"end\":47663,\"start\":47655},{\"end\":47861,\"start\":47856},{\"end\":47876,\"start\":47871},{\"end\":47895,\"start\":47886},{\"end\":47918,\"start\":47911},{\"end\":47932,\"start\":47924},{\"end\":48354,\"start\":48349},{\"end\":48369,\"start\":48364},{\"end\":48383,\"start\":48375},{\"end\":48700,\"start\":48693},{\"end\":48713,\"start\":48710},{\"end\":48730,\"start\":48723},{\"end\":48749,\"start\":48739},{\"end\":48999,\"start\":48995},{\"end\":49015,\"start\":49011},{\"end\":49228,\"start\":49221},{\"end\":49246,\"start\":49238},{\"end\":49259,\"start\":49255},{\"end\":49547,\"start\":49539},{\"end\":49556,\"start\":49551},{\"end\":49571,\"start\":49564},{\"end\":49577,\"start\":49573},{\"end\":49587,\"start\":49579},{\"end\":49861,\"start\":49856},{\"end\":49871,\"start\":49863},{\"end\":49877,\"start\":49873},{\"end\":50091,\"start\":50086},{\"end\":50099,\"start\":50093},{\"end\":50347,\"start\":50341},{\"end\":50359,\"start\":50353},{\"end\":50375,\"start\":50370},{\"end\":50740,\"start\":50738},{\"end\":50753,\"start\":50749},{\"end\":50768,\"start\":50762},{\"end\":50779,\"start\":50777},{\"end\":50795,\"start\":50790},{\"end\":50808,\"start\":50804},{\"end\":50824,\"start\":50820},{\"end\":51080,\"start\":51075},{\"end\":51098,\"start\":51090},{\"end\":51115,\"start\":51107},{\"end\":51348,\"start\":51344},{\"end\":51362,\"start\":51357},{\"end\":51380,\"start\":51372},{\"end\":51716,\"start\":51711},{\"end\":51732,\"start\":51725},{\"end\":51747,\"start\":51740},{\"end\":51765,\"start\":51753},{\"end\":51781,\"start\":51772}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8721150},\"end\":40650,\"start\":40299},{\"attributes\":{\"doi\":\"73:1-73:12\",\"id\":\"b1\",\"matched_paper_id\":13327116},\"end\":40901,\"start\":40652},{\"attributes\":{\"id\":\"b2\"},\"end\":41143,\"start\":40903},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57246310},\"end\":41417,\"start\":41145},{\"attributes\":{\"id\":\"b4\"},\"end\":41535,\"start\":41419},{\"attributes\":{\"id\":\"b5\"},\"end\":41842,\"start\":41537},{\"attributes\":{\"id\":\"b6\"},\"end\":41960,\"start\":41844},{\"attributes\":{\"id\":\"b7\"},\"end\":42224,\"start\":41962},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12560597},\"end\":42560,\"start\":42226},{\"attributes\":{\"id\":\"b9\"},\"end\":42870,\"start\":42562},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6985960},\"end\":43204,\"start\":42872},{\"attributes\":{\"doi\":\"70:1-70:12\",\"id\":\"b11\"},\"end\":43604,\"start\":43206},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11128546},\"end\":43951,\"start\":43606},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14342571},\"end\":44379,\"start\":43953},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6633176},\"end\":44816,\"start\":44381},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8448546},\"end\":45297,\"start\":44818},{\"attributes\":{\"id\":\"b16\"},\"end\":45813,\"start\":45299},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1769372},\"end\":46050,\"start\":45815},{\"attributes\":{\"id\":\"b18\"},\"end\":46425,\"start\":46052},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":252796},\"end\":46767,\"start\":46427},{\"attributes\":{\"id\":\"b20\"},\"end\":46914,\"start\":46769},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8738055},\"end\":47226,\"start\":46916},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2153745},\"end\":47571,\"start\":47228},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8664684},\"end\":47781,\"start\":47573},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10967053},\"end\":48279,\"start\":47783},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1337733},\"end\":48653,\"start\":48281},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7156990},\"end\":48930,\"start\":48655},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206685792},\"end\":49129,\"start\":48932},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1856237},\"end\":49529,\"start\":49131},{\"attributes\":{\"id\":\"b29\"},\"end\":49813,\"start\":49531},{\"attributes\":{\"doi\":\"UU-CS-2007-015\",\"id\":\"b30\"},\"end\":50080,\"start\":49815},{\"attributes\":{\"id\":\"b31\"},\"end\":50246,\"start\":50082},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":12497675},\"end\":50669,\"start\":50248},{\"attributes\":{\"id\":\"b33\"},\"end\":51006,\"start\":50671},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11266650},\"end\":51257,\"start\":51008},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6033252},\"end\":51548,\"start\":51259},{\"attributes\":{\"id\":\"b36\"},\"end\":52033,\"start\":51550}]", "bib_title": "[{\"end\":40320,\"start\":40299},{\"end\":40688,\"start\":40652},{\"end\":41196,\"start\":41145},{\"end\":42282,\"start\":42226},{\"end\":42933,\"start\":42872},{\"end\":43668,\"start\":43606},{\"end\":44010,\"start\":43953},{\"end\":44457,\"start\":44381},{\"end\":44860,\"start\":44818},{\"end\":45416,\"start\":45299},{\"end\":45874,\"start\":45815},{\"end\":46490,\"start\":46427},{\"end\":46968,\"start\":46916},{\"end\":47338,\"start\":47228},{\"end\":47627,\"start\":47573},{\"end\":47846,\"start\":47783},{\"end\":48339,\"start\":48281},{\"end\":48684,\"start\":48655},{\"end\":48986,\"start\":48932},{\"end\":49211,\"start\":49131},{\"end\":50333,\"start\":50248},{\"end\":51070,\"start\":51008},{\"end\":51332,\"start\":51259}]", "bib_author": "[{\"end\":40331,\"start\":40322},{\"end\":40344,\"start\":40331},{\"end\":40362,\"start\":40344},{\"end\":40373,\"start\":40362},{\"end\":40384,\"start\":40373},{\"end\":40396,\"start\":40384},{\"end\":40405,\"start\":40396},{\"end\":40413,\"start\":40405},{\"end\":40434,\"start\":40413},{\"end\":40442,\"start\":40434},{\"end\":40704,\"start\":40690},{\"end\":40725,\"start\":40704},{\"end\":40744,\"start\":40725},{\"end\":40990,\"start\":40976},{\"end\":41008,\"start\":40990},{\"end\":41208,\"start\":41198},{\"end\":41218,\"start\":41208},{\"end\":41234,\"start\":41218},{\"end\":41245,\"start\":41234},{\"end\":41253,\"start\":41245},{\"end\":41265,\"start\":41253},{\"end\":41440,\"start\":41421},{\"end\":41451,\"start\":41440},{\"end\":41604,\"start\":41588},{\"end\":41620,\"start\":41604},{\"end\":41635,\"start\":41620},{\"end\":41654,\"start\":41635},{\"end\":41668,\"start\":41654},{\"end\":41865,\"start\":41846},{\"end\":41872,\"start\":41865},{\"end\":42042,\"start\":42028},{\"end\":42050,\"start\":42042},{\"end\":42067,\"start\":42050},{\"end\":42305,\"start\":42284},{\"end\":42334,\"start\":42305},{\"end\":42347,\"start\":42334},{\"end\":42363,\"start\":42347},{\"end\":42644,\"start\":42621},{\"end\":42666,\"start\":42644},{\"end\":42681,\"start\":42666},{\"end\":42697,\"start\":42681},{\"end\":42949,\"start\":42935},{\"end\":42963,\"start\":42949},{\"end\":42977,\"start\":42963},{\"end\":42996,\"start\":42977},{\"end\":43289,\"start\":43273},{\"end\":43300,\"start\":43289},{\"end\":43315,\"start\":43300},{\"end\":43337,\"start\":43315},{\"end\":43354,\"start\":43337},{\"end\":43373,\"start\":43354},{\"end\":43686,\"start\":43670},{\"end\":43697,\"start\":43686},{\"end\":43712,\"start\":43697},{\"end\":43729,\"start\":43712},{\"end\":43748,\"start\":43729},{\"end\":44029,\"start\":44012},{\"end\":44044,\"start\":44029},{\"end\":44054,\"start\":44044},{\"end\":44066,\"start\":44054},{\"end\":44483,\"start\":44459},{\"end\":44502,\"start\":44483},{\"end\":44510,\"start\":44502},{\"end\":44520,\"start\":44510},{\"end\":44529,\"start\":44520},{\"end\":44539,\"start\":44529},{\"end\":44554,\"start\":44539},{\"end\":44564,\"start\":44554},{\"end\":44869,\"start\":44862},{\"end\":44882,\"start\":44869},{\"end\":44895,\"start\":44882},{\"end\":44909,\"start\":44895},{\"end\":44917,\"start\":44909},{\"end\":44921,\"start\":44917},{\"end\":44941,\"start\":44921},{\"end\":44957,\"start\":44941},{\"end\":44975,\"start\":44957},{\"end\":44992,\"start\":44975},{\"end\":45425,\"start\":45418},{\"end\":45436,\"start\":45425},{\"end\":45449,\"start\":45436},{\"end\":45462,\"start\":45449},{\"end\":45478,\"start\":45462},{\"end\":45491,\"start\":45478},{\"end\":45503,\"start\":45491},{\"end\":45891,\"start\":45876},{\"end\":45908,\"start\":45891},{\"end\":46131,\"start\":46116},{\"end\":46153,\"start\":46131},{\"end\":46169,\"start\":46153},{\"end\":46184,\"start\":46169},{\"end\":46199,\"start\":46184},{\"end\":46218,\"start\":46199},{\"end\":46504,\"start\":46492},{\"end\":46521,\"start\":46504},{\"end\":46545,\"start\":46521},{\"end\":46556,\"start\":46545},{\"end\":46833,\"start\":46816},{\"end\":46979,\"start\":46970},{\"end\":46991,\"start\":46979},{\"end\":47006,\"start\":46991},{\"end\":47018,\"start\":47006},{\"end\":47026,\"start\":47018},{\"end\":47349,\"start\":47340},{\"end\":47365,\"start\":47349},{\"end\":47371,\"start\":47365},{\"end\":47638,\"start\":47629},{\"end\":47655,\"start\":47638},{\"end\":47665,\"start\":47655},{\"end\":47863,\"start\":47848},{\"end\":47878,\"start\":47863},{\"end\":47897,\"start\":47878},{\"end\":47920,\"start\":47897},{\"end\":47934,\"start\":47920},{\"end\":48356,\"start\":48341},{\"end\":48371,\"start\":48356},{\"end\":48385,\"start\":48371},{\"end\":48702,\"start\":48686},{\"end\":48715,\"start\":48702},{\"end\":48732,\"start\":48715},{\"end\":48751,\"start\":48732},{\"end\":49001,\"start\":48988},{\"end\":49017,\"start\":49001},{\"end\":49230,\"start\":49213},{\"end\":49248,\"start\":49230},{\"end\":49261,\"start\":49248},{\"end\":49549,\"start\":49531},{\"end\":49558,\"start\":49549},{\"end\":49573,\"start\":49558},{\"end\":49579,\"start\":49573},{\"end\":49589,\"start\":49579},{\"end\":49863,\"start\":49854},{\"end\":49873,\"start\":49863},{\"end\":49879,\"start\":49873},{\"end\":50093,\"start\":50084},{\"end\":50101,\"start\":50093},{\"end\":50349,\"start\":50335},{\"end\":50361,\"start\":50349},{\"end\":50377,\"start\":50361},{\"end\":50742,\"start\":50730},{\"end\":50755,\"start\":50742},{\"end\":50770,\"start\":50755},{\"end\":50781,\"start\":50770},{\"end\":50797,\"start\":50781},{\"end\":50810,\"start\":50797},{\"end\":50826,\"start\":50810},{\"end\":51082,\"start\":51072},{\"end\":51100,\"start\":51082},{\"end\":51117,\"start\":51100},{\"end\":51350,\"start\":51334},{\"end\":51364,\"start\":51350},{\"end\":51382,\"start\":51364},{\"end\":51718,\"start\":51706},{\"end\":51734,\"start\":51718},{\"end\":51749,\"start\":51734},{\"end\":51767,\"start\":51749},{\"end\":51783,\"start\":51767}]", "bib_venue": "[{\"end\":44153,\"start\":44136},{\"end\":40459,\"start\":40442},{\"end\":40761,\"start\":40754},{\"end\":40974,\"start\":40903},{\"end\":41269,\"start\":41265},{\"end\":41586,\"start\":41537},{\"end\":42026,\"start\":41962},{\"end\":42384,\"start\":42363},{\"end\":42619,\"start\":42562},{\"end\":43028,\"start\":42996},{\"end\":43271,\"start\":43206},{\"end\":43755,\"start\":43748},{\"end\":44134,\"start\":44066},{\"end\":44583,\"start\":44564},{\"end\":45042,\"start\":44992},{\"end\":45547,\"start\":45503},{\"end\":45912,\"start\":45908},{\"end\":46114,\"start\":46052},{\"end\":46581,\"start\":46556},{\"end\":46814,\"start\":46769},{\"end\":47049,\"start\":47026},{\"end\":47391,\"start\":47371},{\"end\":47669,\"start\":47665},{\"end\":48020,\"start\":47934},{\"end\":48459,\"start\":48385},{\"end\":48778,\"start\":48751},{\"end\":49021,\"start\":49017},{\"end\":49319,\"start\":49261},{\"end\":49654,\"start\":49589},{\"end\":49852,\"start\":49815},{\"end\":50426,\"start\":50377},{\"end\":50728,\"start\":50671},{\"end\":51121,\"start\":51117},{\"end\":51386,\"start\":51382},{\"end\":51704,\"start\":51550}]"}}}, "year": 2023, "month": 12, "day": 17}