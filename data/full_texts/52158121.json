{"id": 52158121, "updated": "2023-09-30 16:23:50.351", "metadata": {"title": "emrQA: A Large Corpus for Question Answering on Electronic Medical Records", "authors": "[{\"first\":\"Anusri\",\"last\":\"Pampari\",\"middle\":[]},{\"first\":\"Preethi\",\"last\":\"Raghavan\",\"middle\":[]},{\"first\":\"Jennifer\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Peng\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1809.00732", "mag": "2953235477", "acl": "D18-1258", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/PampariRLP18", "doi": "10.18653/v1/d18-1258"}}, "content": {"source": {"pdf_hash": "6cbdeb96380e028fb204f9802694e77bb772ed60", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D18-1258.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D18-1258.pdf", "status": "HYBRID"}}, "grobid": {"id": "643ffe1561459f0b4e3a58b1fded97b8e7c98867", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6cbdeb96380e028fb204f9802694e77bb772ed60.txt", "contents": "\nemrQA: A Large Corpus for Question Answering on Electronic Medical Records\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsOctober 31 -November 4. 2018. 2018\n\nAnusri Pampari \nDept. of Computer Science\nIL \u2021 Carle Illinois College of Medicine\nUniversity of Illinois Urbana Champaign\nUniversity of Illinois Urbana\nChampaignIL\n\nPreethi Raghavan \nMIT-IBM Watson AI Lab\nCambridgeMA\n\nIBM TJ Watson Research Center\nYorktown HeightsNY\n\nJennifer Liang jjliang@us.ibm.com \nIBM TJ Watson Research Center\nYorktown HeightsNY\n\nJian Peng jianpeng@illinois.edu\u2020praghav \nDept. of Computer Science\nIL \u2021 Carle Illinois College of Medicine\nUniversity of Illinois Urbana Champaign\nUniversity of Illinois Urbana\nChampaignIL\n\nemrQA: A Large Corpus for Question Answering on Electronic Medical Records\n\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\nthe 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsOctober 31 -November 4. 2018. 20182357\nWe propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets \u00a7 . The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping. *\n\nIntroduction\n\nAutomatic question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets (Voorhees et al., 1999;Ferrucci et al., 2010;Rajpurkar et al., 2016;Joshi et al., 2017). However, in the clinical domain this problem remains relatively unexplored. Physicians frequently seek answers to questions from unstructured electronic medical records (EMRs) to support clinical decision-making (Demner-Fushman et al., 2009). But in a significant majority of cases, they are unable to unearth the information they want from EMRs (Tang et al., 1994). Moreover to date, there is no general system for answering natural language questions asked by physicians on a patient's EMR (Figure 1) due to lack of largescale datasets (Raghavan and Patwardhan, 2016).\n\nEMRs are a longitudinal record of a patient's health information in the form of unstructured clinical notes (progress notes, discharge summaries etc.) and structured vocabularies. Physi- cians wish to answer questions about medical entities and relations from the EMR, requiring a deeper understanding of clinical notes. While this may be likened to machine comprehension, the longitudinal nature of clinical discourse, little to no redundancy in facts, abundant use of domain-specific terminology, temporal narratives with multiple related diseases, symptoms, medications that go back and forth in time, and misspellings, make it complex and difficult to apply existing NLP tools (Demner-Fushman et al., 2009;Raghavan and Patwardhan, 2016). Moreover, answers may be implicit or explicit and may require domain-knowledge and reasoning across clinical notes. Thus, building a credible QA system for patient-specific EMR QA requires largescale question and answer annotations that sufficiently capture the challenging nature of clinical narratives in the EMR. However, serious privacy concerns about sharing personal health information (Devereaux, 2013;Krumholz et al., 2016), and the tedious nature of assimilating answer annotations from across longitudinal clinical notes, makes this task impractical and possibly erroneous to do manually (Lee et al., 2017).\n\nIn this work, we address the lack of any publicly available EMR QA corpus by creating a large-scale dataset, emrQA, using a novel gener- ation framework that allows for minimal expert involvement and re-purposes existing annotations available for other clinical NLP tasks (i2b2 challenge datasets (Guo et al., 2006)). The annotations serve as a proxy-expert in generating questions, answers, and logical forms. Logical forms provide a human-comprehensible symbolic representation, linking questions to answers, and help build interpretable models, critical to the medical domain (Davis et al., 1977;Vellido et al., 2012). We analyze the emrQA dataset in terms of question complexity, relations, and the reasoning required to answer questions, and provide neural and heuristic baselines for learning to predict questionlogical forms and question-answers.\n\nThe main contributions of this work are as follows:\n\n\u2022 A novel framework for systematic generation of domain-specific large-scale QA datasets that can be used in any domain where manual annotations are challenging to obtain but limited annotations may be available for other NLP tasks.\n\n\u2022 The first accessible patient-specific EMR QA dataset, emrQA * , consisting of 400,000 question-answer pairs and 1 million questionlogical form pairs. The logical forms will allow users to train and benchmark interpretable models that justify answers with corresponding logical forms.\n\n\u2022 Two new reasoning challenges, namely arithmetic and temporal reasoning, that are absent in open-domain datasets like SQuAD (Rajpurkar et al., 2016). * https://github.com/panushri25/emrQA, scripts to generate emrQA from i2b2 data. i2b2 data is accessible by everyone subject to a license agreement.\n\n\nRelated Work\n\nQuestion Answering (QA) datasets are classified into two main categories: (1) machine comprehension (MC) using unstructured documents, and (2) QA using Knowledge Bases (KBs).\n\nMC systems aim to answer any question that could be posed against a reference text. Recent advances in crowd-sourcing and search engines have resulted in an explosion of large-scale (100K) MC datasets for factoid QA, having ample redundant evidence in text (Rajpurkar et al., 2016;Trischler et al., 2016;Joshi et al., 2017;Dhingra et al., 2017). On the other hand, complex domainspecific MC datasets such as MCTest (Richardson et al., 2013), biological process modeling (Berant et al., 2014), BioASQ (Tsatsaronis et al., 2015), InsuranceQA (Feng et al., 2015), etc have been limited in scale (500-10K) because of the complexity of the task or the need for expert annotations that cannot be crowd-sourced or gathered from the web. In contrast to the open-domain, EMR data cannot be released publicly due to privacy concerns (\u0160uster et al., 2017). Also, annotating unstructured EMRs requires a medical expert who can understand and interpret clinical text. Thus, very few datasets like i2b2, MIMIC (Johnson et al., 2016) (developed over several years in collaboration with large medical groups and hospitals), share small-scale annotated clinical notes. In this work, we take advantage of the limited expertly annotated resources to generate emrQA.\n\nKB-based QA datasets, used for semantic parsing, are traditionally limited by the requirement of annotated question and logical form (LF) pairs for supervision where the LF are used to retrieve answers from a schema (Cai and Yates, 2013;Lopez et al., 2013;Bordes et al., 2015). Roberts and Demner-Fushman (2016)   Recent advances in QA combine logic-based and neural MC approaches to build hybrid models (Usbeck et al., 2015;Palangi et al., 2018). These models are driven to combine the accuracy of neural approaches (Hermann et al., 2015) and the interpretability of the symbolic representations in logic-based methods (Gao et al.;Chabierski et al., 2017). Building interpretable yet accurate models is extremely important in the medical domain (Shickel et al., 2017). We generate large-scale ground truth annotations (questions, logical forms, and answers) that can provide supervision to learn such hybrid models. Our approach to generating emrQA is in the same spirit as Su et al. (2016), who generate graph queries (logical forms) from a structured KB and use them to collect answers. In contrast, our framework can be applied to generate QA dataset in any domain with minimal expert input using annotations from other NLP tasks.\n\n\nQA Dataset Generation Framework\n\nOur general framework for generating a largescale QA corpus given certain resources consists of three steps: (1) collecting questions to capture domain-specific user needs, followed by normalizing the collected questions to templates by replacing entities (that may be related via binary or composite relations) in the question with placeholders. The entity types replaced in the question are grounded in an ontology like WordNet (Miller, 1995), UMLS (Bodenreider, 2004), or a usergenerated schema that defines and relates different entity types. (2) We associate question templates with expert-annotated logical form templates; logical forms are symbolic representations using relations from the ontology/schema to express the relations in the question, and associate the ques-How was the |problem| managed ? How was the patient's |problem| treated ? What was done to correct the patient's |problem| ? Has the patient ever been treated for a |problem| ? What treatment has the patient had for his |problem| ? Has the patient ever received treatment for |problem| ? What treatments for |problem| has this patient tried ? tion entity type with an answer entity type. (3) We then proceed to the important step of re-purposing existing NLP annotations to populate questionlogical form templates and generate answers. QA is a complex task that requires addressing several fundamental NLP problems before accurately answering a question. Hence, obtaining expert manual annotations in complex domains is infeasible as it is tedious to expert-annotate answers that may be found across long document collections (e.g., longitudinal EMR) (Lee et al., 2017). Thus, we reverse engineer the process where we reuse expert annotations available in NLP tasks such as entity recognition, coreference, and relation learning, based on the information captured in the logical forms to populate entity placeholders in templates and generate answers. Reverse engineering serves as a proxy expert ensuring that the generated QA annotations are credible. The only manual effort is in annotating logical forms, thus significantly reducing expert labor. Moreover, in domain specific instances such as EMRs, manually annotated logical forms allow the experts to express information essential for natural language understanding such as domain knowledge, temporal relations, and negation (Gao et al.;Chabierski et al., 2017). This knowledge, once captured, can be used to generate QA pairs on new documents, making the framework scalable.\n\n\nGenerating the emrQA Dataset\n\nWe apply the proposed framework to generate the emrQA corpus consisting of questions posed by physicians against longitudinal EMRs of a patient, using annotations provided by i2b2 ( Figure 2).\n\n\nQuestion Collection and Normalization\n\nWe collect questions for EMR QA by, 1) polling physicians at the Veterans Administration for what they frequently want to know from the EMR (976 questions), 2) using an existing source of 5,696 questions generated by a team of medical experts from 71 patient records (Raghavan et al., 2017) and 3) using 15 prototypical questions from an ob-\nMedicationEvent \u2022 Medication/treatment \u2022 Dosage \u2022 Startdate \u2022 Enddate \u2022 Route \u2022 Adherence \u2022 Frequency \u2022 Treatment/test \u2022 Date \u2022 Result \u2022 Status \u2022 AbnormalResultFlag VitalEvent ConditionEvent SymptomEvent \u2022 IsTobaccoUser \u2022 PacksPerUse \u2022 YearsOfUSe \u2022 Treatment/test \u2022 Date \u2022 Result \u2022 Status \u2022 AbnormalResultFlag ProcedureEvent \u2022 Test \u2022 Date \u2022 Result \u2022 Status \u2022 AbnormalResultFlag LabEvent SmokingUseEvent SmokingQuitEvent \u2022 problem \u2022 Diagnosisdate \u2022 Status \u2022 QuitDate \u2022 problem\n\nFamilyHistoryEvent\n\nRelations Attributes I2b2 entity types as arguments servational study done by physicians (Tang et al., 1994). To obtain templates, the questions were automatically normalized by identifying medical entities (using MetaMap (Aronson, 2001)) in questions and replacing them with generic placeholders. The resulting \u223c2K noisy templates were expert reviewed and corrected (to account for any entity recognition errors by MetaMap). We align our entity types to those defined in the i2b2 concept extraction tasks (Uzuner et al., 2010a(Uzuner et al., , 2011 problem, test, treatment, mode and medication. E.g., The question What is the dosage of insulin? from the collection gets converted to the template What is the dosage of |medication|? as shown in Fig.2. This process resulted in 680 question templates. We do not correct for the usage/spelling errors in these templates, such as usage of \"pt\" for \"patient\", or make the templates gender neutral in order to provide a true representation of physicians' questions. Further, analyzing these templates shows that physicians most frequently ask about test results (11%), medications for problem (9%), and problem existence (8%). The long tail following this includes questions about medication dosage, response to treatment, medication duration, prescription date, etiology, etc. Temporal constraints were frequently imposed on questions related to tests, problem diagnosis and medication start/stop.\n\n\nAssociating Templates w/ Logical Forms\n\nThe 680 question templates were annotated by a physician with their corresponding logical form (LF) templates, which resulted in 94 unique LF templates. More than one question template that map to the same LF are considered paraphrases of each other and correspond to a particular question type (Table 2). Logical forms are defined based on an ontology schema designed by medical experts ( Figure 3). This schema captures entities in unstructured clinical notes through medical events and their attributes, interconnected through relations. We align the entity and relation types of i2b2 to this schema.\n\nA formal representation of the LF grammar using this schema (Figure 3) is as follows. Medical events are denoted as M E i (e.g LabEvent, ConditionEvent) and relations are denoted as RE i (e.g conducted/reveals). Now, M E[a 1 , .., a j , .., oper(a n )] is a medical event where a j represents the attribute of the event (such as result in LabEvent). An event may optionally include constraints on attributes captured by an operator (oper() \u2208 sort, range, check for null values, compare). These operators sometimes require values from external medical KB (indicated by ref, e.g. lab.ref low/lab.ref high to indicate range of reference standards considered healthy in lab results) indicating the need for medical knowledge to answer the question. Using these constructs, a LF can be defined using the following rules,\nLF \u2192 M E i | M 1 relation M 2 M 1 \u2192 M E i , M 2 \u2192 M E j M 1 \u2192 M 1 relation M 2 , M 2 \u2192 M 1 relation M 2 relation \u2192 OR | AN D | RE i\nAdvantages of our LF representation include the ability to represent composite relations, define attributes for medical events and constrain the attributes to precisely capture the information need in the question. While these can be achieved using different methods that combine lambda calculus and first order logic (Roberts and Demner-Fushman, 2016), our representation is more human comprehensible. This allows a physician to consider an ontology like Figure 3 and easily define a logical form. Some example question templates with their LF annotations are described in Table 3 using the above notation. The LF representation of the question in Figure  2 is MedicationEvent(|medication|) [dosage=x]. The entities seen in LF are the entities posed in the question and entity marked x indicates the answer entity type.\n\n\nTemplate Filling and Answer Extraction\n\nThe next step in the process is to populate the question and logical form (QL) templates with existing annotations in the i2b2 clinical datasets and extract answer evidence for the questions.  The i2b2 datasets are expert annotated with fine-grained annotations (Guo et al., 2006) that were developed for various shared NLP challenge tasks, including (1) smoking status classification (Uzuner et al., 2008), (2) diagnosis of obesity and its co-morbidities (Uzuner, 2009), extraction of (3) medication concepts (Uzuner et al., 2010a), (4) relations, concepts, assertions (Uzuner et al., 2010b(Uzuner et al., , 2011 (5) co-reference resolution (Uzuner et al., 2012) and (6) heart disease risk factor identification (Stubbs and Uzuner, 2015). In Figure 2, this would correspond to leveraging annotations from medications challenge between medications and their dosages, such as medica-tion=Nitroglycerin, dosage=40mg, to populate |medication| and generate several instances of the question \"What is the dosage of |medication|?\" and its corresponding logical form MedicationEvent(|medication|) [dosage=x]. The answer would be derived from the value of the dosage entity in the dataset.\n\nPreprocessing: The i2b2 entities are preprocessed before using them with our templates to ensure syntactic correctness of the generated questions. The pre-processing steps are designed based on the i2b2 annotations syntax guidelines (Guo et al., 2006). To estimate grammatical correctness, we randomly sampled 500 generated questions and found that <5% had errors. These errors include, among others, incorrect usage of article with the entity and incorrect entity phrasing.\n\nAnswer Extraction: The final step in the process is generating answer evidence corresponding to each question. The answers in emrQA are defined differently; instead of a single word or phrase we provide the entire i2b2 annotation line from the clinical note as the answer. This is because the context in which the answer entity or phrase is mentioned is extremely important in clinical decision making (Demner-Fushman et al., 2009).\n\nHence, we call them answer evidence instead of just answers. For example, consider the question Is the patient's hypertension controlled?.\n\nThe answer to this question is not a simple yes/no since the status of the patient's hypertension can change through the course of treatment. The answer evidence to this question in emrQA are multiple lines across the longitudinal notes that reflect this potentially changing status of the patients condition, e.g. Hypertension-borderline today. Additionally, for questions seeking specific answers we also provide the corresponding answer entities.\n\nThe overall process for answer evidence generation was vetted by a physician. Here is a brief overview of how the different i2b2 datasets were used in generating answers. The relations challenge datasets have various event-relation annotations across single/multiple lines in a clinical note. We used a combination of one or more of these, to generate answers for a question; in doing so we used the annotations provided by the i2b2 co-reference datasets. Similarly, the medications challenge dataset has various event-attribute annotations but since this dataset is not provided with co-reference annotations, it is currently not possible to combine all valid answers. The heart disease challenge dataset has longitudinal notes (\u223c5 per patient) with record dates. The events in this dataset are also provided with time annotations and are rich in quantitative entities. This dataset was primarily used to answer questions that require temporal and arithmetic reasoning on events. The patient records in the smoking and obesity challenge datasets are categorized into classes with no entity annotations. Thus, for questions generated on these datasets, the entire document acts as evidence and the annotated class information (7 classes) needs to be predicted as the answer.\n\nThe total questions, LFs and answers gener-ated using this framework are summarized in Table 1. Consider the question How much does the patient smoke? for which we do not have i2b2annotations to provide an answer. In cases where the answer entity is empty, we only generate the question and LF, resulting in more question types being used for QL than QA pairs: only 53% of question types have answers.\n\n\nemrQA Dataset Analysis\n\nWe analyze the complexity of emrQA by considering the LFs for question characteristics, variations in paraphrases, and the type of reasoning required for answering questions (Table 2, 3, 4).\n\n\nQuestion/Logical Form Characteristics\n\nA quantitative and qualitative analysis of emrQA question templates is shown in Table 3, where logical forms help formalize their characteristics (Su et al., 2016). Questions may request specific finegrained information (attribute values like dosage) or may express a more coarse-grained need (event entities like medications etc), or a combination of both. 25% of questions require complex operators (e.g compare(>)) and 12% of questions express the need for external medical knowledge (e.g. lab.refhigh). The questions in emrQA are highly compositional, where 47% of question templates have at least one event relation.\n\n\nParaphrase Complexity Analysis\n\nQuestions templates that map to the same LF are considered paraphrases (e.g, Table 2) and correspond to the same question type. In emrQA, an average of 7 paraphrase templates exist per question type. This is representative of FAQ types that are perhaps more important to the physician. Good paraphrases are lexically dissimilar to each other (Chen and Dolan, 2011). In order to understand the lexical variation within our paraphrases, we randomly select a question from the list of paraphrases as a reference and evaluate the others with respect to the reference, and report the average BLEU (0.74 \u00b1 0.06) and Jaccard Score (0.72 \u00b1 0.19). The low BLEU and Jaccard score with large standard deviation indicates the lexical diversity captured by emrQA's paraphrases (Papineni et al., 2002;Niwattanakul et al., 2013).\n\n\nAnswer Evidence Analysis\n\n33% of the questions in emrQA have more than one answer evidence, with the number ranging from 2 to 61. E.g., the question Medications Record? has all medications in the patient's longitudinal record as answer evidence. In order to analyze the reasoning required to answer emrQA questions, we sampled 35 clinical notes from the corpus and analyzed 3 random questions per note by manually labeling them with the categories described in Table 4. Categories are not mutually exclusive: a single example can fall into multiple categories. We compare and contrast this analysis with SQuAD (Rajpurkar et al., 2016), a popular MC dataset generated through crowdsourcing, to show that the framework is capable of generating a corpus as representative and even more complex. Compared to SQuAD, emrQA offers two new reasoning categories, temporal and arithmetic which make up 31% of the dataset. Additionally, over two times as many questions in emrQA require reasoning over multiple sentences. Long and noisy documents make the question answering task more difficult (Joshi et al., 2017). EMRs are inherently noisy and hence 29% have incomplete context and the document length is 27 times more than SQuAD which offers new challenges to existing QA models. Owing to the domain specific nature of the task, 39% of the examples required some form of medical/world knowledge.\n\nAs discussed in Section 4.3, 12% of the questions in emrQA corpus require a class category from i2b2 smoking and obesity datasets to be predicted. We also found 6% of the questions had other possible answers that were not included by emrQA, this is because of the lack of co-reference annotations for the medications challenge.\n\n\nBaseline Methods\n\nWe implement baseline models using neural and heuristic methods for question to logical form (Q-L) and question to answer (Q-A) mapping.\n\n\nQ-L Mapping\n\nHeuristic Models: We use a template-matching approach where we first split the data into train/test sets, and then normalize questions in the test set into templates by replacing entities with placeholders. The templates are then scored against the ground truth templates of the questions in the train set, to find the best match. The placeholders in the LF template corresponding to the best matched question template is then filled with the normalized entities to obtain the predicted LF. To normalize the test questions we use CLiNER   Table 5: Heuristic (HM) and neural (seq2seq) models performance on question to logical form learning in emrQA. (Boag et al., 2015) for emrQA and Jia and Liang (2016)'s work for ATIS and GeoQuery. Scoring and matching is done using two heuristics: (1) HM-1, which computes an identical match, and (2) HM-2, which generates a GloVe vector (Arora et al., 2016) representation of the templates using sentence2vec and then computes pairwise cosine similarity. Neural Model: We train a sequence-tosequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al., 2014;Luong et al., 2017) as our neural baseline (2 layers, each with 64 hidden units). The same setting when used with Geoquery and ATIS gives poor results because the parameters are not appropriate for the nature of that dataset. Hence, for comparison with GeoQuery and ATIS, we use the results of seq2seq model with a single 200 hidden units layer (Jia and Liang, 2016). At test time we automatically balance missing right parentheses. \u2020 results from Jia and Liang (2016) \n\n\nExperimental Setup\n\nWe randomly partition the QL pairs in the dataset in train(80%) and test(20%) sets in two ways. (1) In emrQL-1, we first split the paraphrase templates corresponding to a single LF template into train and test, and then generate the instances of QL pairs. (2) In emrQL-2, we first generate the instances of QL pairs from the templates and then distribute them into train and test sets. As a result, emrQL-1 has more lexical variation between train and test distribution compared to emrQL-2, resulting in increased paraphrase complexity. We use accuracy i.e, the total number of logical forms predicted correctly as a metric to evaluate our model.\n\n\nResults\n\nThe performance of the proposed models is summarized in Table 5. emrQL results are not directly comparable with GeoQuery and ATIS because of the differences in the lexicon and tools available for the domains. However, it helps us establish that QL learning in emrQA is non-trivial and supports significant future work.\n\nError analysis of heuristic models on emrQL-1 and emrQL-2 showed that 70% of the errors occurred because of incorrect question normalization. In fact, 30% of these questions had not been normalized at all. This shows that the entities added to the templates are complex and diverse and make the inverse process of template generation non trivial. This makes a challenging QL corpus that cannot trivially be solved by template matching based approaches.\n\nErrors made by the neural model on both emrQL-1 and emrQL-2 are due to long LFs (20%) and incorrectly identified entities (10%), which are harder for the attention-based model (Jia and Liang, 2016). The increased paraphrase complexity in emrQL-1 compared to emrQL-2 resulted in 20% more structural errors in emrQL-1, where the predicted event/grammar structure deviates significantly from the ground truth. This shows that the model is not adequately capturing the semantics in the questions to generalize to new paraphrases. Therefore, emrQL-1 can be used to benchmark QL models robust to paraphrasing.\n\n\nQ-A Mapping\n\nQuestion-answering on emrQA consists of two different tasks, (1) extraction of answer line from the clinical note (machine comprehension (MC)) and (2) prediction of answer class based on the entire clinical note. We provide baseline models to illustrate the complexity in doing both these tasks.\n\nMachine Comprehension: To do extractive QA on EMRs, we use DrQA's  document reader which is a multi-layer RNN based MC model. We use their best performing settings trained for SQuAD data using Glove vectors (300 dim-840B).\n\nClass Prediction: We build a multi-class logistic regression model for predicting a class as an answer based on the patient's clinical note. Features input to the classifier are TF-IDF vectors of the question and the clinical notes taken from i2b2 smoking and obesity datasets.\n\n\nExperimental setup\n\nWe consider a 80-20 split of the data for train-test. In order to evaluate worst-case performance, we train on question-evidence pairs in a clinical note obtained by using only one random paraphrase for a question instead of all the paraphrases. We use a slightly modified \u2021 version of the two popularly reported metrics in MC for evaluation since our evidence span is longer: Exact Match (EM) and F1. Wherever the answer entity in an evidence is explicitly known, EM checks if the answer entity is \u2021 using the original definitions, the evaluated values were far less than those obtained in Table 7 Model  present within the evidence, otherwise it checks if the predicted evidence span lies within \u00b120 characters of the ground truth evidence. For F1 we construct a bag of tokens for each evidence string and measure the F1 score of the overlap between the two bags of tokens. Since there may be multiple evidence for a given question, we consider only the top 10 predictions and report an average of EM and F1 over ground truth number of answers. In the class prediction setting, we report the subset accuracy.\n\n\nResults\n\nThe performance of the proposed models is summarized in Table 7. DrQA is one of the best performing models on SQuAD with an F1 of 78.8 and EM of 69.5. The relatively low performance of the models on emrQA (60.6 F1 and 59.2 EM) shows that QA on EMRs is a complex task and offers new challenges to existing QA models.\n\nTo understand model performance, we macroaverage the EM across all the questions corresponding to a LF template. We observe that LFs representing temporal and arithmetic \u00a7 needs had < 16% EM. LFs expressing the need for medical KB \u00a7 performed poorly since we used general Glove embeddings. An analysis of LFs which had approximately equal number of QA pair representation in the test set revealed an interesting relation between the model performance and LF complexity, as summarized in Table 6. The trend shows that performance is worse on multiple relation questions as compared to single relation and attribute questions, showing that the LFs sufficiently capture the complexity of the questions and give us an ability to do a qualitative model analysis.\n\nError analysis on a random sample of 50 questions containing at least one answer entity in an evidence showed that: (1) 38% of the examples required multiple sentence reasoning of which 16% were due to a missing evidence in a multiple evidence question, (2) 14% were due to syntactic variation, (3) 10% required medical reasoning and (4) in 14%, DrQA predicted an incomplete evidence span missing the answer entity in it. \u00a7 maximum representation of these templates comes from the i2b2 heart disease risk dataset  \n\n\nDiscussion\n\nIn this section, we describe how our generation framework may also be applied to generate opendomain QA datasets given the availability of other NLP resources. We also discuss possible extensions of the framework to increase the complexity of the generated datasets.\n\nOpen domain QA dataset generation: Consider the popularly used SQuAD (Rajpurkar et al., 2016) reading comprehension dataset generated by crowdworkers, where the answer to every question is a segment of text from the corresponding passage in the Wikipedia article. This dataset can easily be generated or extended using our proposed framework with existing NLP annotations on Wikipedia (Auer et al., 2007;Nothman et al., 2008;Ghaddar and Langlais, 2017).\n\nFor instance, consider DBPedia (Auer et al., 2007), an existing dataset of entities and their relations extracted from Wikipedia. It also has its own ontology which can serve as the semantic frames schema to define logical forms. Using these resources, our reverse engineering technique for QA dataset generation can be applied as follows. (1) Question templates can be defined for each entity type and relation in DBPedia. For example \u00b6 , consider the relation [place, country] field in DBpedia. For this we can define a question template In what country is |place| located?.\n\n(2) Every such question template can be annotated with a logical form template using existing DB-Pedia ontology. (3) By considering the entity values of DBPedia fields such as [place=Normandy, dbo:country=France], we can automatically generate the question In what country is Normandy located? and its corresponding logical form from the templates. The text span of country=France from the Wikipedia passage is then used as the answer (Daiber et al., 2013). Currently, this QA pair instance is a part of the SQuAD dev set. Using our framework we can generate many more instances like this example from different Wikipedia passages -without crowdsourcing efforts. \u00b6 example reference: http://dbpedia.org/page/Normandy Extensions to the framework: The complexity of the generated dataset can be further extended as follows.\n\n(1) We can use a coreferred or a lexical variant of the original entity in the question-logical form generation. This can allow for increased lexical variation between the question and answer line entities in the passage. (2) It is possible to combine two or more question templates to make compositional questions with the answers to these questions similarly combined. This can also result in more multiple sentence reasoning questions. (3) We can generate questions with entities not related to the context in the passage. This can increase empty answer questions in the dataset, resulting in increased negative training examples.\n\n\nConclusions and Future Work\n\nWe propose a novel framework that can generate a large-scale QA dataset using existing resources and minimal expert input. This has the potential to make a huge impact in domains like medicine, where obtaining manual QA annotations is tedious and infeasible. We apply this framework to generate a large scale EMR QA corpus (emrQA), consisting of 400,000 question-answers pairs and 1 million question-logical forms, and analyze the complexity of the dataset to show its non-trivial nature. We show that the logical forms provide a symbolic representation that is very useful for corpus generation and for model analysis. The logical forms also provide an opportunity to build interpretable systems by perhaps jointly (or latently) learning the logical form and answer for a question. In future, this framework may be applied to also re-purpose and integrate other NLP datasets such as MIMIC and generate a more diverse and representative EMR QA corpus (Johnson et al., 2016).\n\nFigure 1 :\n1Question-Answer pairs from emrQA clinical note.\n\nFigure 2 :\n2Our QA dataset generation framework using existing i2b2 annotations on a given patient's record to generate a question, its logical form and answer evidence. The highlights in the figure show the annotations being used for this example.\n\nFigure 3 :\n3Events, attributes & relations in emrQA's logical forms. Events & attributes accept i2b2 entities as arguments.\n\n\ngenerated a corpus byDatasets \n#QA \n#QL #notes Property \nStats. \nRelations \n141,243 1,061,710 425 Question len. 8.6 \nMedications 255,908 198,739 261 Evidence len. 18.7 \nHeart disease 30,731 36,746 \n119 LF len. \n33 \nObesity \n23,437 \n280 \n1,118 Note len. \n3825 \nSmoking \n4,518 \n6 \n502 # of evidence 1.5 \nemrQA \n455,837 1,295,814 2,425 # Ques. in note 187 \n\n\n\nTable 1 :\n1(left) i2b2 dataset distribution in emrQA, and \n\n(right) emrQA properties with length in tokens, averaged \n\nmanually annotating LFs on 468 EMR questions \n(not released publicly), thus limiting its ability to \ncreate large scale datasets. In contrast, we only \ncollect LFs for question templates from a domain-\nexpert -the rest of our corpus is automatically gen-\nerated. \n\n\nTable 2 :\n2Paraphrase templates of a question type in emrQA.\n\n\nQ: What is the dosage of |medication| ? LF: MedicationEvent (|medication|) [dosage=x] 62.7% Course grained answer type (event entity is answer) Q: What does the patient take |medication| for? LF: MedicationEvent(|medication|)given{ConditionEvent(x) OR SymptomEvent(x)} 52.1% Questions with operators on entities Q: What are the last set of labs with elevated numbers out of range? LF: LabEvent (x) [date=x, (result=x)>lab.refhigh] What are the last set of labs with elevated numbers out of range? LF: LabEvent (x) [date=x, (result=x)>lab.refhigh]Property \nExample Annotation \nStats. \nFine grained answer type \n(attribute entity is answer) \n\n25.5% \n\nQuestions which require \nmedical KB \n\nQ: 11.7% \n\nAt least one event relation \nWhat lab results does he have that are pertinent to |problem| diagnosis \nLF: LabEvent (x) [date=x, result=x] conducted/reveals ConditionEvent (|problem|) \n\n46.8% \n\n\n\nTable 3 :\n3Properties of question templates inferred from the corresponding logical form templates. The boldface words hint at the presence of the corresponding property in both question and the logical form template.\n\n\nQ: Has this patient ever been treated with insulin? E: Patient sugars were managed o/n with sliding scale insulin and diabeticMajor correspondence between the question and answer sentence requires world/medical knowledge to resolve Q: Has the patient complained of any CAD symptoms? E: 70-year-old female who comes in with substernal chest pressureReasoning \nDescription \nExample Annotation \nemrQA SQuAD \nLexical Variation \n(Synonym) \n\nMajor correspondence between the \nquestion and answer sentence are \nsynonyms. \n\n15.2% 33.3% \n\nLexical Variation \n(world/medical \nknowledge) \n\n39.0% 9.1% \n\nSyntactic Variation After the question is paraphrased \ninto declarative form, its syntac-\ntic dependency structure does not \nmatch that of the answer sentence \n\nQ: Has this patient ever been treated with ffp? \nE: attempt to reverse anticoagulation , one unit of \nFFP was begun \n\n60.0% 64.1% \n\nMultiple Sentence Co-reference and higher level fu-\nsion of multiple sentences \n\nQ: What happened when the patient was given as-\ncending aortic root replacement? \nE: The patient tolerated the procedure fairly well \nand was transferred to the ICU with his chest open \n\n23.8% 13.6% \n\nArithmetic \nKnowing comparison and subtrac-\ntion operators. \n\nQ: Show me any LDL > 100 mg/dl in the last 6 \nyears? \nE: gluc 192, LDL 115, TG 71, HDL 36 \n\n13.3% N.A. \n\nTemporal \nReasoning based on time frame \nQ: What were the results of the abnormal A1C on \n2115-12-14? \nE: HBA1C 12/14/2115 11.80 \n\n18.1% N.A. \n\nIncomplete \nContext \n\nUnstructured clinical text is noisy \nand may have missing context \n\nQ: What is her current dose of iron? \nE: Iron 325 mg p.o. t.i.d. \n\n28.6% N.A. \n\nClass Prediction \nQuestions for which a specific pre-\ndefined class needs to be predicted \n\nQ: Is the patient currently Obese? \nE: Yes \n\n12.4% N.A. \n\n\n\nTable 4 :\n4We manually labeled 105 examples into one or more of the above categories. Words relevant to the corresponding reasoning type are in bold and the answer entity (if any) in the evidence is in italics. We compare this analysis with SQuAD.Dataset \nTrain/Test \nHM-1 HM-2 Neural \nGeoQuery 600/280 \n32.8% 52.1% 74.6%  \u2020 \nATIS \n4,473/448 \n20.8% 52.2% 69.9%  \u2020 \nemrQL-1 \n1M/253K \n0.3% \n26.3% 22.4% \nemrQL-2 \n1.1M/296K 31.6% 32.0% 42.7% \n\n\n\nTable 7 :\n7Performance of baseline models on the two QA sub tasks, machine comprehension (MC) and class prediction.\n\n\n|test|) OR ProcedureEvent (|test|)} conducted {ConditionEvent(x) OR SymptomEvent (x)}Logical Form template \nProperty \nExact Match \nMedicationEvent (|medication|) [enddate=x] \nsingle attribute \n55.3% \n{LabEvent (single relation \n32.2% \n\n{MedicationEvent(|treatment|)ORProcedureEvent(|treatment|)} \nimproves/worsens/causes {ConditionEvent (x) OR SymptomEvent (x)} \n\nmultiple relation \n12.6% \n\n\n\nTable 6 :\n6Neural models (DrQA) performance on question-evidence corpus of emrQA stratified according to the logical form templates. Instance showing increasing complexity in the logical forms with decreasing model performance.\nAcknowledgmentsThis project is partially funded by Sloan Research Fellowship, PhRMA Foundation Award in Informatics, and NSF Career Award (1652815). The authors would like to thank Siddharth Patwardhan for his valuable feedback in formatting the paper.\nEffective mapping of biomedical text to the umls metathesaurus: the metamap program. Alan R Aronson, Proceedings of the AMIA Symposium. the AMIA Symposium17Alan R Aronson. 2001. Effective mapping of biomed- ical text to the umls metathesaurus: the metamap program. In Proceedings of the AMIA Symposium, page 17. American Medical Informatics Associa- tion.\n\nA simple but tough-to-beat baseline for sentence embeddings. Sanjeev Arora, Yingyu Liang, Tengyu Ma, Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016. A simple but tough-to-beat baseline for sentence em- beddings.\n\nDbpedia: A nucleus for a web of open data. S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The semantic web. SpringerS\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722-735. Springer.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\n\nModeling biological processes for reading comprehension. Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, Christopher D Manning, EMNLP. Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, and Christopher D Manning. 2014. Modeling biological processes for reading comprehension. In EMNLP.\n\nCliner: A lightweight tool for clinical named entity recognition. William Boag, Kevin Wacome, Tristan Naumann, Anna Rumshisky, AMIA Joint Summits on Clinical Research Informatics. posterWilliam Boag, Kevin Wacome, Tristan Naumann, and Anna Rumshisky. 2015. Cliner: A lightweight tool for clinical named entity recognition. AMIA Joint Summits on Clinical Research Informatics (poster).\n\nThe unified medical language system (umls): integrating biomedical terminology. Olivier Bodenreider, Nucleic acids research. 32suppl_1Olivier Bodenreider. 2004. The unified medical lan- guage system (umls): integrating biomedical termi- nology. Nucleic acids research, 32(suppl_1):D267- D270.\n\nLarge-scale simple question answering with memory networks. Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston, arXiv:1506.02075arXiv preprintAntoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075.\n\nLarge-scale semantic parsing via schema matching and lexicon extension. Qingqing Cai, Alexander Yates, ACL (1). Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In ACL (1), pages 423-433.\n\nLogic-based approach to machine comprehension of text. Piotr Chabierski, Alessandra Russo, Mark Law, Piotr Chabierski, Alessandra Russo, and Mark Law. 2017. Logic-based approach to machine compre- hension of text.\n\nReading wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, arXiv:1704.00051arXiv preprintDanqi Chen, Adam Fisch, Jason Weston, and An- toine Bordes. 2017. Reading wikipedia to an- swer open-domain questions. arXiv preprint arXiv:1704.00051.\n\nCollecting highly parallel data for paraphrase evaluation. L David, William B Chen, Dolan, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1David L Chen and William B Dolan. 2011. Collect- ing highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190-200. Association for Computational Linguistics.\n\nImproving efficiency and accuracy in multilingual entity extraction. Joachim Daiber, Max Jakob, Chris Hokamp, Pablo N Mendes, Proceedings of the 9th International Conference on Semantic Systems (I-Semantics). the 9th International Conference on Semantic Systems (I-Semantics)Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes. 2013. Improving efficiency and accuracy in multilingual entity extraction. In Proceedings of the 9th International Conference on Semantic Systems (I-Semantics).\n\nProduction rules as a representation for a knowledge-based consultation program. Randall Davis, Bruce Buchanan, Edward Shortliffe, Artificial intelligence. 81Randall Davis, Bruce Buchanan, and Edward Short- liffe. 1977. Production rules as a representation for a knowledge-based consultation program. Artificial intelligence, 8(1):15-45.\n\nWhat can natural language processing do for clinical decision support. Dina Demner-Fushman, Wendy Webber Chapman, Clement J Mcdonald, Journal of Biomedical Informatics. 425Dina Demner-Fushman, Wendy Webber Chapman, and Clement J. McDonald. 2009. What can natural lan- guage processing do for clinical decision support? Journal of Biomedical Informatics, 42(5):760-772.\n\nThe use of patient records (ehr) for research. Mary Devereaux, Mary Devereaux. 2013. The use of patient records (ehr) for research.\n\nQuasar: Datasets for question answering by search and reading. Bhuwan Dhingra, Kathryn Mazaitis, William W Cohen, arXiv:1707.03904arXiv preprintBhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. 2017. Quasar: Datasets for question an- swering by search and reading. arXiv preprint arXiv:1707.03904.\n\nApplying deep learning to answer selection: A study and an open task. Minwei Feng, Bing Xiang, R Michael, Lidan Glass, Bowen Wang, Zhou, Automatic Speech Recognition and Understanding (ASRU). IEEEMinwei Feng, Bing Xiang, Michael R Glass, Li- dan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on, pages 813-820. IEEE.\n\nHybrid question answering over knowledge base and free text. Yansong Feng, Songfang Huang, Dongyan Zhao, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. COLING 2016, the 26th International Conference on Computational Linguistics: Technical PapersYansong Feng, Songfang Huang, Dongyan Zhao, et al. 2016. Hybrid question answering over knowledge base and free text. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2397-2407.\n\nBuilding watson: An overview of the deepqa project. David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, William Murdock, Eric Nyberg, John Prager, AI magazine. 313David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building watson: An overview of the deepqa project. AI magazine, 31(3):59-79.\n\nMachine reading for question answering: from symbolic to neural computation. Jianfeng Gao, Rangan Majumder, Bill Dolan, Jianfeng Gao, Rangan Majumder, and Bill Dolan. Ma- chine reading for question answering: from sym- bolic to neural computation.\n\nWiner: A wikipedia annotated corpus for named entity recognition. Abbas Ghaddar, Phillippe Langlais, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language Processing1Abbas Ghaddar and Phillippe Langlais. 2017. Winer: A wikipedia annotated corpus for named en- tity recognition. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 413-422.\n\nIdentifying personal health information using support vector machines. Y Guo, R Gaizauskas, I Roberts, G Demetriou, M Hepple, i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data. Y. Guo, R. Gaizauskas, I. Roberts, G. Demetriou, and M. Hepple. 2006. Identifying personal health information using support vector machines. In i2b2 Workshop on Challenges in Natural Language Processing for Clinical Data, pages 10-11.\n\nTeaching machines to read and comprehend. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Advances in Neural Information Processing Systems. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1693-1701.\n\nData recombination for neural semantic parsing. Robin Jia, Percy Liang, arXiv:1606.03622arXiv preprintRobin Jia and Percy Liang. 2016. Data recombina- tion for neural semantic parsing. arXiv preprint arXiv:1606.03622.\n\nMimic-iii, a freely accessible critical care database. E W Alistair, Johnson, J Tom, Lu Pollard, H Lehman Shen, Mengling Li-Wei, Mohammad Feng, Benjamin Ghassemi, Peter Moody, Leo Anthony Szolovits, Roger G Celi, Mark, 3160035Scientific dataAlistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Moham- mad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. Mimic-iii, a freely accessible critical care database. Scientific data, 3:160035.\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, S Daniel, Luke Weld, Zettlemoyer, arXiv:1705.03551arXiv preprintMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. arXiv preprint arXiv:1705.03551.\n\nData acquisition, curation, and use for a continuously learning health system. Sharon F Harlan M Krumholz, Joanne Terry, Waldstreicher, Jama. 31616Harlan M Krumholz, Sharon F Terry, and Joanne Wald- streicher. 2016. Data acquisition, curation, and use for a continuously learning health system. Jama, 316(16):1669-1670.\n\nBig healthcare data analytics: Challenges and applications. Chonho Lee, Zhaojing Luo, Kee Yuan Ngiam, Meihui Zhang, Kaiping Zheng, Gang Chen, Beng Chin Ooi, Wei Luen James Yip, Handbook of Large-Scale Distributed Computing in Smart Healthcare. SpringerChonho Lee, Zhaojing Luo, Kee Yuan Ngiam, Mei- hui Zhang, Kaiping Zheng, Gang Chen, Beng Chin Ooi, and Wei Luen James Yip. 2017. Big health- care data analytics: Challenges and applications. In Handbook of Large-Scale Distributed Computing in Smart Healthcare, pages 11-41. Springer.\n\nEvaluating question answering over linked data. Vanessa Lopez, Christina Unger, Philipp Cimiano, Enrico Motta, Web Semantics: Science, Services and Agents on the World Wide Web. 21Vanessa Lopez, Christina Unger, Philipp Cimiano, and Enrico Motta. 2013. Evaluating question answering over linked data. Web Semantics: Science, Services and Agents on the World Wide Web, 21:3-13.\n\nNeural machine translation (seq2seq) tutorial. Minh-Thang Luong, Eugene Brevdo, Rui Zhao, Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. 2017. Neural machine translation (seq2seq) tutorial. https://github.com/tensorflow/nmt.\n\nWordnet: a lexical database for english. A George, Miller, Communications of the ACM. 3811George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39- 41.\n\nUsing of jaccard coefficient for keywords similarity. Suphakit Niwattanakul, Jatsada Singthongchai, Ekkachai Naenudorn, Supachanun Wanapu, Proceedings of the International MultiConference of Engineers and Computer Scientists. the International MultiConference of Engineers and Computer Scientists1Suphakit Niwattanakul, Jatsada Singthongchai, Ekkachai Naenudorn, and Supachanun Wanapu. 2013. Using of jaccard coefficient for keywords similarity. In Proceedings of the International MultiConference of Engineers and Computer Scientists, volume 1.\n\nTransforming wikipedia into named entity training data. Joel Nothman, Tara James R Curran, Murphy, Proceedings of the Australasian Language Technology Association Workshop. the Australasian Language Technology Association WorkshopJoel Nothman, James R Curran, and Tara Murphy. 2008. Transforming wikipedia into named entity training data. In Proceedings of the Australasian Language Technology Association Workshop 2008, pages 124-132.\n\nQuestion-answering with grammatically-interpretable representations. Hamid Palangi, Paul Smolensky, Xiaodong He, Li Deng, Proceedings of the 32nd AAAI Conference on Artificial Intelligence. the 32nd AAAI Conference on Artificial IntelligenceNew Orleans, LAHamid Palangi, Paul Smolensky, Xiaodong He, and Li Deng. 2018. Question-answering with grammatically-interpretable representations. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, New Orleans, LA.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Associa- tion for Computational Linguistics.\n\nQuestion answering on electronic medical records. Preethi Raghavan, Siddharth Patwardhan, Proceedings of the 2016 Summit on Clinical Research Informatics. the 2016 Summit on Clinical Research InformaticsSan Francisco, CAPreethi Raghavan and Siddharth Patwardhan. 2016. Question answering on electronic medical records. In Proceedings of the 2016 Summit on Clinical Research Informatics, San Francisco, CA, March 2016.\n\nAnnotating electronic medical records for question answering. Preethi Raghavan, Siddharth Patwardhan, Jennifer J Liang, Murthy V Devarakonda, arXiv:1805.06816Preethi Raghavan, Siddharth Patwardhan, Jennifer J. Liang, and Murthy V. Devarakonda. 2017. Annotat- ing electronic medical records for question answer- ing. arXiv:1805.06816.\n\nSquad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.05250arXiv preprintPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.\n\nMctest: A challenge dataset for the open-domain machine comprehension of text. Matthew Richardson, J C Christopher, Erin Burges, Renshaw, EMNLP. 34Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP, volume 3, page 4.\n\nAnnotating logical forms for ehr questions. Kirk Roberts, Dina Demner-Fushman, LREC... International Conference on Language Resources & Evaluation:[proceedings]. International Conference on Language Resources and Evaluation. NIH Public Access20163772Kirk Roberts and Dina Demner-Fushman. 2016. An- notating logical forms for ehr questions. In LREC... International Conference on Language Resources & Evaluation:[proceedings]. International Conference on Language Resources and Evaluation, volume 2016, page 3772. NIH Public Access.\n\nDeep ehr: A survey of recent advances in deep learning techniques for electronic health record (ehr) analysis. Benjamin Shickel, Patrick James Tighe, Azra Bihorac, Parisa Rashidi, IEEE Journal of Biomedical and Health Informatics. Benjamin Shickel, Patrick James Tighe, Azra Bihorac, and Parisa Rashidi. 2017. Deep ehr: A survey of re- cent advances in deep learning techniques for elec- tronic health record (ehr) analysis. IEEE Journal of Biomedical and Health Informatics.\n\nAnnotating longitudinal clinical narratives for de-identification: The 2014 i2b2/uthealth corpus. Amber Stubbs, \u00d6zlem Uzuner, Journal of biomedical informatics. 58Amber Stubbs and \u00d6zlem Uzuner. 2015. Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/uthealth corpus. Journal of biomedical informatics, 58:S20-S29.\n\nOn generating characteristic-rich question sets for qa evaluation. Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, Xifeng Yan, EMNLP. Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016. On generating characteristic-rich question sets for qa evaluation. In EMNLP, pages 562-572.\n\nA short review of ethical challenges in clinical natural language processing. Simon \u0160uster, St\u00e9phan Tulkens, Walter Daelemans, arXiv:1703.10090arXiv preprintSimon \u0160uster, St\u00e9phan Tulkens, and Walter Daelemans. 2017. A short review of ethical challenges in clin- ical natural language processing. arXiv preprint arXiv:1703.10090.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in neural information processing systems. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neu- ral networks. In Advances in neural information processing systems, pages 3104-3112.\n\nTraditional medical records as a source of clinical data in the outpatient setting. C Paul, Danielle Tang, Edward H Fafchamps, Shortliffe, Proceedings of the Annual Symposium on Computer Application in Medical Care. the Annual Symposium on Computer Application in Medical Care575Paul C Tang, Danielle Fafchamps, and Edward H Shortliffe. 1994. Traditional medical records as a source of clinical data in the outpatient setting. In Proceedings of the Annual Symposium on Computer Application in Medical Care, page 575. American Medical Informatics Association.\n\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman, arXiv:1611.09830Newsqa: A machine comprehension dataset. arXiv preprintAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har- ris, Alessandro Sordoni, Philip Bachman, and Ka- heer Suleman. 2016. Newsqa: A machine compre- hension dataset. arXiv preprint arXiv:1611.09830.\n\nGeorge Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke. Michael R Alvers, Dirk Weissenborn, AnastasiaGeorge Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia\n\nAn overview of the bioasq large-scale biomedical semantic indexing and question answering competition. Sergios Krithara, Dimitris Petridis, Polychronopoulos, BMC bioinformatics. 161138Krithara, Sergios Petridis, Dimitris Polychronopou- los, et al. 2015. An overview of the bioasq large-scale biomedical semantic indexing and ques- tion answering competition. BMC bioinformatics, 16(1):138.\n\nHawkhybrid question answering using linked data. Ricardo Usbeck, Axel-Cyrille Ngonga Ngomo, Lorenz B\u00fchmann, Christina Unger, European Semantic Web Conference. SpringerRicardo Usbeck, Axel-Cyrille Ngonga Ngomo, Lorenz B\u00fchmann, and Christina Unger. 2015. Hawk- hybrid question answering using linked data. In European Semantic Web Conference, pages 353- 368. Springer.\n\nRecognizing obesity and comorbidities in sparse data. \u00d6zlem Uzuner, Journal of the American Medical Informatics Association. 164\u00d6zlem Uzuner. 2009. Recognizing obesity and co- morbidities in sparse data. Journal of the American Medical Informatics Association, 16(4):561-570.\n\nEvaluating the state of the art in coreference resolution for electronic medical records. Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler Forbush, John Pestian, Brett R South, Journal of the American Medical Informatics Association. 195Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler Forbush, John Pestian, and Brett R South. 2012. Evaluating the state of the art in coreference res- olution for electronic medical records. Journal of the American Medical Informatics Association, 19(5):786-791.\n\nIdentifying patient smoking status from medical discharge records. \u00d6zlem Uzuner, Ira Goldstein, Yuan Luo, Isaac Kohane, Journal of the American Medical Informatics Association. 151\u00d6zlem Uzuner, Ira Goldstein, Yuan Luo, and Isaac Ko- hane. 2008. Identifying patient smoking status from medical discharge records. Journal of the American Medical Informatics Association, 15(1):14-24.\n\nExtracting medication information from clinical text. \u00d6zlem Uzuner, Imre Solti, Eithon Cadag, Journal of the American Medical Informatics Association. 175\u00d6zlem Uzuner, Imre Solti, and Eithon Cadag. 2010a. Extracting medication information from clinical text. Journal of the American Medical Informatics Association, 17(5):514-518.\n\nCommunity annotation experiment for ground truth generation for the i2b2 medication challenge. \u00d6zlem Uzuner, Imre Solti, Fei Xia, Eithon Cadag, Journal of the American Medical Informatics Association. 175\u00d6zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag. 2010b. Community annotation experiment for ground truth generation for the i2b2 medica- tion challenge. Journal of the American Medical Informatics Association, 17(5):519-523.\n\ni2b2/va challenge on concepts, assertions, and relations in clinical text. \u00d6zlem Uzuner, R Brett, Shuying South, Scott L Shen, Duvall, Journal of the American Medical Informatics Association. 185\u00d6zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2011. 2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552-556.\n\nMaking machine learning models interpretable. Alfredo Vellido, Jos\u00e9 David Mart\u00edn-Guerrero, Paulo Jg Lisboa, ESANN. Citeseer12Alfredo Vellido, Jos\u00e9 David Mart\u00edn-Guerrero, and Paulo JG Lisboa. 2012. Making machine learning models interpretable. In ESANN, volume 12, pages 163-172. Citeseer.\n\nThe trec-8 question answering track report. M Ellen, Voorhees, Trec. 99Ellen M Voorhees et al. 1999. The trec-8 question an- swering track report. In Trec, volume 99, pages 77- 82.\n", "annotations": {"author": "[{\"end\":368,\"start\":204},{\"end\":471,\"start\":369},{\"end\":556,\"start\":472},{\"end\":746,\"start\":557}]", "publisher": "[{\"end\":117,\"start\":76},{\"end\":1039,\"start\":998}]", "author_last_name": "[{\"end\":218,\"start\":211},{\"end\":385,\"start\":377},{\"end\":486,\"start\":481},{\"end\":566,\"start\":562}]", "author_first_name": "[{\"end\":210,\"start\":204},{\"end\":376,\"start\":369},{\"end\":480,\"start\":472},{\"end\":561,\"start\":557}]", "author_affiliation": "[{\"end\":367,\"start\":220},{\"end\":420,\"start\":387},{\"end\":470,\"start\":422},{\"end\":555,\"start\":507},{\"end\":745,\"start\":598}]", "title": "[{\"end\":75,\"start\":1},{\"end\":821,\"start\":747}]", "venue": "[{\"end\":909,\"start\":823}]", "abstract": "[{\"end\":1747,\"start\":1078}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b58\"},\"end\":1943,\"start\":1920},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1965,\"start\":1943},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1988,\"start\":1965},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2007,\"start\":1988},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2250,\"start\":2221},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2374,\"start\":2355},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2578,\"start\":2547},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3291,\"start\":3262},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3321,\"start\":3291},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3732,\"start\":3715},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3754,\"start\":3732},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3939,\"start\":3921},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4257,\"start\":4239},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4541,\"start\":4521},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":4562,\"start\":4541},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5520,\"start\":5496},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6144,\"start\":6120},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6167,\"start\":6144},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6186,\"start\":6167},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6207,\"start\":6186},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6303,\"start\":6278},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6354,\"start\":6333},{\"end\":6389,\"start\":6363},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6422,\"start\":6403},{\"end\":6707,\"start\":6686},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7348,\"start\":7327},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7367,\"start\":7348},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7387,\"start\":7367},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7422,\"start\":7389},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7536,\"start\":7515},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7557,\"start\":7536},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7650,\"start\":7628},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7743,\"start\":7731},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7767,\"start\":7743},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7879,\"start\":7857},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8102,\"start\":8086},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8825,\"start\":8811},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8851,\"start\":8832},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10028,\"start\":10010},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10753,\"start\":10741},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10777,\"start\":10753},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11448,\"start\":11425},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12105,\"start\":12086},{\"end\":12234,\"start\":12204},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12524,\"start\":12503},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12546,\"start\":12524},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15389,\"start\":15355},{\"end\":15739,\"start\":15729},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16179,\"start\":16162},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16306,\"start\":16285},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16369,\"start\":16356},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16432,\"start\":16410},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16491,\"start\":16470},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16513,\"start\":16491},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16563,\"start\":16542},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16638,\"start\":16613},{\"end\":17000,\"start\":16990},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17334,\"start\":17316},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17989,\"start\":17961},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20683,\"start\":20666},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21540,\"start\":21518},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21963,\"start\":21940},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21989,\"start\":21963},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22627,\"start\":22603},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23097,\"start\":23077},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24552,\"start\":24533},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24779,\"start\":24759},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24956,\"start\":24932},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25004,\"start\":24981},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25023,\"start\":25004},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25370,\"start\":25349},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25472,\"start\":25452},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27125,\"start\":27104},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31456,\"start\":31432},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31767,\"start\":31748},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31788,\"start\":31767},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31815,\"start\":31788},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31868,\"start\":31849},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32852,\"start\":32831},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34857,\"start\":34835}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34919,\"start\":34859},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35169,\"start\":34920},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35294,\"start\":35170},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35652,\"start\":35295},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36037,\"start\":35653},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36099,\"start\":36038},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36993,\"start\":36100},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37212,\"start\":36994},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39012,\"start\":37213},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":39455,\"start\":39013},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39572,\"start\":39456},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":39966,\"start\":39573},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40195,\"start\":39967}]", "paragraph": "[{\"end\":2579,\"start\":1763},{\"end\":3940,\"start\":2581},{\"end\":4795,\"start\":3942},{\"end\":4848,\"start\":4797},{\"end\":5082,\"start\":4850},{\"end\":5369,\"start\":5084},{\"end\":5670,\"start\":5371},{\"end\":5861,\"start\":5687},{\"end\":7109,\"start\":5863},{\"end\":8345,\"start\":7111},{\"end\":10891,\"start\":8381},{\"end\":11116,\"start\":10924},{\"end\":11499,\"start\":11158},{\"end\":13441,\"start\":11997},{\"end\":14087,\"start\":13484},{\"end\":14904,\"start\":14089},{\"end\":15857,\"start\":15037},{\"end\":17081,\"start\":15900},{\"end\":17557,\"start\":17083},{\"end\":17991,\"start\":17559},{\"end\":18131,\"start\":17993},{\"end\":18582,\"start\":18133},{\"end\":19858,\"start\":18584},{\"end\":20261,\"start\":19860},{\"end\":20478,\"start\":20288},{\"end\":21141,\"start\":20520},{\"end\":21990,\"start\":21176},{\"end\":23381,\"start\":22019},{\"end\":23710,\"start\":23383},{\"end\":23867,\"start\":23731},{\"end\":25473,\"start\":23883},{\"end\":26142,\"start\":25496},{\"end\":26472,\"start\":26154},{\"end\":26926,\"start\":26474},{\"end\":27531,\"start\":26928},{\"end\":27842,\"start\":27547},{\"end\":28066,\"start\":27844},{\"end\":28345,\"start\":28068},{\"end\":29478,\"start\":28368},{\"end\":29805,\"start\":29490},{\"end\":30564,\"start\":29807},{\"end\":31080,\"start\":30566},{\"end\":31361,\"start\":31095},{\"end\":31816,\"start\":31363},{\"end\":32394,\"start\":31818},{\"end\":33217,\"start\":32396},{\"end\":33852,\"start\":33219},{\"end\":34858,\"start\":33884}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11975,\"start\":11500},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15036,\"start\":14905}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13788,\"start\":13779},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15618,\"start\":15611},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20607,\"start\":20600},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21260,\"start\":21253},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22461,\"start\":22454},{\"end\":24429,\"start\":24422},{\"end\":26217,\"start\":26210},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28966,\"start\":28959},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29553,\"start\":29546},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":30301,\"start\":30294}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1761,\"start\":1749},{\"attributes\":{\"n\":\"2\"},\"end\":5685,\"start\":5673},{\"attributes\":{\"n\":\"3\"},\"end\":8379,\"start\":8348},{\"attributes\":{\"n\":\"4\"},\"end\":10922,\"start\":10894},{\"attributes\":{\"n\":\"4.1\"},\"end\":11156,\"start\":11119},{\"end\":11995,\"start\":11977},{\"attributes\":{\"n\":\"4.2\"},\"end\":13482,\"start\":13444},{\"attributes\":{\"n\":\"4.3\"},\"end\":15898,\"start\":15860},{\"attributes\":{\"n\":\"5\"},\"end\":20286,\"start\":20264},{\"attributes\":{\"n\":\"5.1\"},\"end\":20518,\"start\":20481},{\"attributes\":{\"n\":\"5.2\"},\"end\":21174,\"start\":21144},{\"attributes\":{\"n\":\"5.3\"},\"end\":22017,\"start\":21993},{\"attributes\":{\"n\":\"6\"},\"end\":23729,\"start\":23713},{\"attributes\":{\"n\":\"6.1\"},\"end\":23881,\"start\":23870},{\"attributes\":{\"n\":\"6.1.1\"},\"end\":25494,\"start\":25476},{\"attributes\":{\"n\":\"6.1.2\"},\"end\":26152,\"start\":26145},{\"attributes\":{\"n\":\"6.2\"},\"end\":27545,\"start\":27534},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":28366,\"start\":28348},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":29488,\"start\":29481},{\"attributes\":{\"n\":\"7\"},\"end\":31093,\"start\":31083},{\"attributes\":{\"n\":\"8\"},\"end\":33882,\"start\":33855},{\"end\":34870,\"start\":34860},{\"end\":34931,\"start\":34921},{\"end\":35181,\"start\":35171},{\"end\":35663,\"start\":35654},{\"end\":36048,\"start\":36039},{\"end\":37004,\"start\":36995},{\"end\":39023,\"start\":39014},{\"end\":39466,\"start\":39457},{\"end\":39977,\"start\":39968}]", "table": "[{\"end\":35652,\"start\":35318},{\"end\":36037,\"start\":35665},{\"end\":36993,\"start\":36648},{\"end\":39012,\"start\":37563},{\"end\":39455,\"start\":39261},{\"end\":39966,\"start\":39660}]", "figure_caption": "[{\"end\":34919,\"start\":34872},{\"end\":35169,\"start\":34933},{\"end\":35294,\"start\":35183},{\"end\":35318,\"start\":35297},{\"end\":36099,\"start\":36050},{\"end\":36648,\"start\":36102},{\"end\":37212,\"start\":37006},{\"end\":37563,\"start\":37215},{\"end\":39261,\"start\":39025},{\"end\":39572,\"start\":39468},{\"end\":39660,\"start\":39575},{\"end\":40195,\"start\":39979}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2510,\"start\":2501},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11114,\"start\":11106},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12748,\"start\":12743},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13882,\"start\":13874},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14158,\"start\":14149},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15501,\"start\":15493},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15695,\"start\":15686},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16651,\"start\":16643},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20477,\"start\":20462}]", "bib_author_first_name": "[{\"end\":40874,\"start\":40867},{\"end\":40888,\"start\":40882},{\"end\":40902,\"start\":40896},{\"end\":41069,\"start\":41064},{\"end\":41085,\"start\":41076},{\"end\":41099,\"start\":41093},{\"end\":41115,\"start\":41111},{\"end\":41132,\"start\":41125},{\"end\":41150,\"start\":41143},{\"end\":41454,\"start\":41447},{\"end\":41474,\"start\":41465},{\"end\":41486,\"start\":41480},{\"end\":41753,\"start\":41745},{\"end\":41767,\"start\":41762},{\"end\":41786,\"start\":41778},{\"end\":41797,\"start\":41793},{\"end\":41821,\"start\":41813},{\"end\":41835,\"start\":41831},{\"end\":41848,\"start\":41843},{\"end\":41869,\"start\":41856},{\"end\":42171,\"start\":42164},{\"end\":42183,\"start\":42178},{\"end\":42199,\"start\":42192},{\"end\":42213,\"start\":42209},{\"end\":42571,\"start\":42564},{\"end\":42845,\"start\":42838},{\"end\":42861,\"start\":42854},{\"end\":42876,\"start\":42871},{\"end\":42890,\"start\":42885},{\"end\":43174,\"start\":43166},{\"end\":43189,\"start\":43180},{\"end\":43406,\"start\":43401},{\"end\":43429,\"start\":43419},{\"end\":43441,\"start\":43437},{\"end\":43617,\"start\":43612},{\"end\":43628,\"start\":43624},{\"end\":43641,\"start\":43636},{\"end\":43657,\"start\":43650},{\"end\":43909,\"start\":43908},{\"end\":43926,\"start\":43917},{\"end\":44567,\"start\":44560},{\"end\":44579,\"start\":44576},{\"end\":44592,\"start\":44587},{\"end\":44606,\"start\":44601},{\"end\":44608,\"start\":44607},{\"end\":45078,\"start\":45071},{\"end\":45091,\"start\":45086},{\"end\":45108,\"start\":45102},{\"end\":45404,\"start\":45400},{\"end\":45426,\"start\":45421},{\"end\":45450,\"start\":45443},{\"end\":45452,\"start\":45451},{\"end\":45750,\"start\":45746},{\"end\":45901,\"start\":45895},{\"end\":45918,\"start\":45911},{\"end\":45936,\"start\":45929},{\"end\":45938,\"start\":45937},{\"end\":46212,\"start\":46206},{\"end\":46223,\"start\":46219},{\"end\":46232,\"start\":46231},{\"end\":46247,\"start\":46242},{\"end\":46260,\"start\":46255},{\"end\":46651,\"start\":46644},{\"end\":46666,\"start\":46658},{\"end\":46681,\"start\":46674},{\"end\":47197,\"start\":47192},{\"end\":47212,\"start\":47208},{\"end\":47228,\"start\":47220},{\"end\":47247,\"start\":47242},{\"end\":47258,\"start\":47253},{\"end\":47273,\"start\":47267},{\"end\":47275,\"start\":47274},{\"end\":47291,\"start\":47287},{\"end\":47306,\"start\":47299},{\"end\":47320,\"start\":47316},{\"end\":47333,\"start\":47329},{\"end\":47687,\"start\":47679},{\"end\":47699,\"start\":47693},{\"end\":47714,\"start\":47710},{\"end\":47922,\"start\":47917},{\"end\":47941,\"start\":47932},{\"end\":48440,\"start\":48439},{\"end\":48447,\"start\":48446},{\"end\":48461,\"start\":48460},{\"end\":48472,\"start\":48471},{\"end\":48485,\"start\":48484},{\"end\":48854,\"start\":48850},{\"end\":48876,\"start\":48871},{\"end\":48892,\"start\":48886},{\"end\":48912,\"start\":48907},{\"end\":48927,\"start\":48923},{\"end\":48940,\"start\":48933},{\"end\":48955,\"start\":48951},{\"end\":49308,\"start\":49303},{\"end\":49319,\"start\":49314},{\"end\":49530,\"start\":49529},{\"end\":49532,\"start\":49531},{\"end\":49553,\"start\":49552},{\"end\":49561,\"start\":49559},{\"end\":49579,\"start\":49571},{\"end\":49594,\"start\":49586},{\"end\":49611,\"start\":49603},{\"end\":49626,\"start\":49618},{\"end\":49642,\"start\":49637},{\"end\":49653,\"start\":49650},{\"end\":49661,\"start\":49654},{\"end\":49680,\"start\":49673},{\"end\":50067,\"start\":50061},{\"end\":50081,\"start\":50075},{\"end\":50089,\"start\":50088},{\"end\":50102,\"start\":50098},{\"end\":50433,\"start\":50427},{\"end\":50435,\"start\":50434},{\"end\":50461,\"start\":50455},{\"end\":50735,\"start\":50729},{\"end\":50749,\"start\":50741},{\"end\":50758,\"start\":50755},{\"end\":50777,\"start\":50771},{\"end\":50792,\"start\":50785},{\"end\":50804,\"start\":50800},{\"end\":50815,\"start\":50811},{\"end\":50820,\"start\":50816},{\"end\":50840,\"start\":50826},{\"end\":51261,\"start\":51254},{\"end\":51278,\"start\":51269},{\"end\":51293,\"start\":51286},{\"end\":51309,\"start\":51303},{\"end\":51641,\"start\":51631},{\"end\":51655,\"start\":51649},{\"end\":51667,\"start\":51664},{\"end\":51852,\"start\":51851},{\"end\":52069,\"start\":52061},{\"end\":52091,\"start\":52084},{\"end\":52115,\"start\":52107},{\"end\":52137,\"start\":52127},{\"end\":52614,\"start\":52610},{\"end\":52628,\"start\":52624},{\"end\":53065,\"start\":53060},{\"end\":53079,\"start\":53075},{\"end\":53099,\"start\":53091},{\"end\":53106,\"start\":53104},{\"end\":53539,\"start\":53532},{\"end\":53555,\"start\":53550},{\"end\":53568,\"start\":53564},{\"end\":53583,\"start\":53575},{\"end\":54121,\"start\":54114},{\"end\":54141,\"start\":54132},{\"end\":54552,\"start\":54545},{\"end\":54572,\"start\":54563},{\"end\":54593,\"start\":54585},{\"end\":54595,\"start\":54594},{\"end\":54609,\"start\":54603},{\"end\":54611,\"start\":54610},{\"end\":54885,\"start\":54879},{\"end\":54901,\"start\":54897},{\"end\":54919,\"start\":54909},{\"end\":54934,\"start\":54929},{\"end\":55226,\"start\":55219},{\"end\":55240,\"start\":55239},{\"end\":55242,\"start\":55241},{\"end\":55260,\"start\":55256},{\"end\":55510,\"start\":55506},{\"end\":55524,\"start\":55520},{\"end\":56114,\"start\":56106},{\"end\":56131,\"start\":56124},{\"end\":56137,\"start\":56132},{\"end\":56149,\"start\":56145},{\"end\":56165,\"start\":56159},{\"end\":56575,\"start\":56570},{\"end\":56589,\"start\":56584},{\"end\":56887,\"start\":56885},{\"end\":56896,\"start\":56892},{\"end\":56907,\"start\":56902},{\"end\":56924,\"start\":56916},{\"end\":56943,\"start\":56935},{\"end\":56956,\"start\":56949},{\"end\":56968,\"start\":56962},{\"end\":57256,\"start\":57251},{\"end\":57272,\"start\":57265},{\"end\":57288,\"start\":57282},{\"end\":57559,\"start\":57555},{\"end\":57576,\"start\":57571},{\"end\":57592,\"start\":57586},{\"end\":57911,\"start\":57910},{\"end\":57926,\"start\":57918},{\"end\":57939,\"start\":57933},{\"end\":57941,\"start\":57940},{\"end\":58390,\"start\":58386},{\"end\":58406,\"start\":58402},{\"end\":58419,\"start\":58413},{\"end\":58432,\"start\":58426},{\"end\":58451,\"start\":58441},{\"end\":58467,\"start\":58461},{\"end\":58483,\"start\":58477},{\"end\":58768,\"start\":58762},{\"end\":58790,\"start\":58782},{\"end\":59162,\"start\":59155},{\"end\":59181,\"start\":59173},{\"end\":59499,\"start\":59492},{\"end\":59527,\"start\":59508},{\"end\":59541,\"start\":59535},{\"end\":59560,\"start\":59551},{\"end\":59870,\"start\":59865},{\"end\":60183,\"start\":60178},{\"end\":60199,\"start\":60192},{\"end\":60216,\"start\":60209},{\"end\":60228,\"start\":60223},{\"end\":60242,\"start\":60238},{\"end\":60259,\"start\":60252},{\"end\":60663,\"start\":60658},{\"end\":60675,\"start\":60672},{\"end\":60691,\"start\":60687},{\"end\":60702,\"start\":60697},{\"end\":61033,\"start\":61028},{\"end\":61046,\"start\":61042},{\"end\":61060,\"start\":61054},{\"end\":61406,\"start\":61401},{\"end\":61419,\"start\":61415},{\"end\":61430,\"start\":61427},{\"end\":61442,\"start\":61436},{\"end\":61820,\"start\":61815},{\"end\":61830,\"start\":61829},{\"end\":61845,\"start\":61838},{\"end\":61860,\"start\":61853},{\"end\":62210,\"start\":62203},{\"end\":62224,\"start\":62220},{\"end\":62256,\"start\":62248},{\"end\":62492,\"start\":62491}]", "bib_author_last_name": "[{\"end\":40548,\"start\":40534},{\"end\":40880,\"start\":40875},{\"end\":40894,\"start\":40889},{\"end\":40905,\"start\":40903},{\"end\":41074,\"start\":41070},{\"end\":41091,\"start\":41086},{\"end\":41109,\"start\":41100},{\"end\":41123,\"start\":41116},{\"end\":41141,\"start\":41133},{\"end\":41155,\"start\":41151},{\"end\":41463,\"start\":41455},{\"end\":41478,\"start\":41475},{\"end\":41493,\"start\":41487},{\"end\":41760,\"start\":41754},{\"end\":41776,\"start\":41768},{\"end\":41791,\"start\":41787},{\"end\":41811,\"start\":41798},{\"end\":41829,\"start\":41822},{\"end\":41841,\"start\":41836},{\"end\":41854,\"start\":41849},{\"end\":41877,\"start\":41870},{\"end\":42176,\"start\":42172},{\"end\":42190,\"start\":42184},{\"end\":42207,\"start\":42200},{\"end\":42223,\"start\":42214},{\"end\":42583,\"start\":42572},{\"end\":42852,\"start\":42846},{\"end\":42869,\"start\":42862},{\"end\":42883,\"start\":42877},{\"end\":42897,\"start\":42891},{\"end\":43178,\"start\":43175},{\"end\":43195,\"start\":43190},{\"end\":43417,\"start\":43407},{\"end\":43435,\"start\":43430},{\"end\":43445,\"start\":43442},{\"end\":43622,\"start\":43618},{\"end\":43634,\"start\":43629},{\"end\":43648,\"start\":43642},{\"end\":43664,\"start\":43658},{\"end\":43915,\"start\":43910},{\"end\":43931,\"start\":43927},{\"end\":43938,\"start\":43933},{\"end\":44574,\"start\":44568},{\"end\":44585,\"start\":44580},{\"end\":44599,\"start\":44593},{\"end\":44615,\"start\":44609},{\"end\":45084,\"start\":45079},{\"end\":45100,\"start\":45092},{\"end\":45119,\"start\":45109},{\"end\":45419,\"start\":45405},{\"end\":45441,\"start\":45427},{\"end\":45461,\"start\":45453},{\"end\":45760,\"start\":45751},{\"end\":45909,\"start\":45902},{\"end\":45927,\"start\":45919},{\"end\":45944,\"start\":45939},{\"end\":46217,\"start\":46213},{\"end\":46229,\"start\":46224},{\"end\":46240,\"start\":46233},{\"end\":46253,\"start\":46248},{\"end\":46265,\"start\":46261},{\"end\":46271,\"start\":46267},{\"end\":46656,\"start\":46652},{\"end\":46672,\"start\":46667},{\"end\":46686,\"start\":46682},{\"end\":47206,\"start\":47198},{\"end\":47218,\"start\":47213},{\"end\":47240,\"start\":47229},{\"end\":47251,\"start\":47248},{\"end\":47265,\"start\":47259},{\"end\":47285,\"start\":47276},{\"end\":47297,\"start\":47292},{\"end\":47314,\"start\":47307},{\"end\":47327,\"start\":47321},{\"end\":47340,\"start\":47334},{\"end\":47691,\"start\":47688},{\"end\":47708,\"start\":47700},{\"end\":47720,\"start\":47715},{\"end\":47930,\"start\":47923},{\"end\":47950,\"start\":47942},{\"end\":48444,\"start\":48441},{\"end\":48458,\"start\":48448},{\"end\":48469,\"start\":48462},{\"end\":48482,\"start\":48473},{\"end\":48492,\"start\":48486},{\"end\":48869,\"start\":48855},{\"end\":48884,\"start\":48877},{\"end\":48905,\"start\":48893},{\"end\":48921,\"start\":48913},{\"end\":48931,\"start\":48928},{\"end\":48949,\"start\":48941},{\"end\":48963,\"start\":48956},{\"end\":49312,\"start\":49309},{\"end\":49325,\"start\":49320},{\"end\":49541,\"start\":49533},{\"end\":49550,\"start\":49543},{\"end\":49557,\"start\":49554},{\"end\":49569,\"start\":49562},{\"end\":49584,\"start\":49580},{\"end\":49601,\"start\":49595},{\"end\":49616,\"start\":49612},{\"end\":49635,\"start\":49627},{\"end\":49648,\"start\":49643},{\"end\":49671,\"start\":49662},{\"end\":49685,\"start\":49681},{\"end\":49691,\"start\":49687},{\"end\":50073,\"start\":50068},{\"end\":50086,\"start\":50082},{\"end\":50096,\"start\":50090},{\"end\":50107,\"start\":50103},{\"end\":50120,\"start\":50109},{\"end\":50453,\"start\":50436},{\"end\":50467,\"start\":50462},{\"end\":50482,\"start\":50469},{\"end\":50739,\"start\":50736},{\"end\":50753,\"start\":50750},{\"end\":50769,\"start\":50759},{\"end\":50783,\"start\":50778},{\"end\":50798,\"start\":50793},{\"end\":50809,\"start\":50805},{\"end\":50824,\"start\":50821},{\"end\":50844,\"start\":50841},{\"end\":51267,\"start\":51262},{\"end\":51284,\"start\":51279},{\"end\":51301,\"start\":51294},{\"end\":51315,\"start\":51310},{\"end\":51647,\"start\":51642},{\"end\":51662,\"start\":51656},{\"end\":51672,\"start\":51668},{\"end\":51859,\"start\":51853},{\"end\":51867,\"start\":51861},{\"end\":52082,\"start\":52070},{\"end\":52105,\"start\":52092},{\"end\":52125,\"start\":52116},{\"end\":52144,\"start\":52138},{\"end\":52622,\"start\":52615},{\"end\":52643,\"start\":52629},{\"end\":52651,\"start\":52645},{\"end\":53073,\"start\":53066},{\"end\":53089,\"start\":53080},{\"end\":53102,\"start\":53100},{\"end\":53111,\"start\":53107},{\"end\":53548,\"start\":53540},{\"end\":53562,\"start\":53556},{\"end\":53573,\"start\":53569},{\"end\":53587,\"start\":53584},{\"end\":54130,\"start\":54122},{\"end\":54152,\"start\":54142},{\"end\":54561,\"start\":54553},{\"end\":54583,\"start\":54573},{\"end\":54601,\"start\":54596},{\"end\":54623,\"start\":54612},{\"end\":54895,\"start\":54886},{\"end\":54907,\"start\":54902},{\"end\":54927,\"start\":54920},{\"end\":54940,\"start\":54935},{\"end\":55237,\"start\":55227},{\"end\":55254,\"start\":55243},{\"end\":55267,\"start\":55261},{\"end\":55276,\"start\":55269},{\"end\":55518,\"start\":55511},{\"end\":55539,\"start\":55525},{\"end\":56122,\"start\":56115},{\"end\":56143,\"start\":56138},{\"end\":56157,\"start\":56150},{\"end\":56173,\"start\":56166},{\"end\":56582,\"start\":56576},{\"end\":56596,\"start\":56590},{\"end\":56890,\"start\":56888},{\"end\":56900,\"start\":56897},{\"end\":56914,\"start\":56908},{\"end\":56933,\"start\":56925},{\"end\":56947,\"start\":56944},{\"end\":56960,\"start\":56957},{\"end\":56972,\"start\":56969},{\"end\":57263,\"start\":57257},{\"end\":57280,\"start\":57273},{\"end\":57298,\"start\":57289},{\"end\":57569,\"start\":57560},{\"end\":57584,\"start\":57577},{\"end\":57595,\"start\":57593},{\"end\":57916,\"start\":57912},{\"end\":57931,\"start\":57927},{\"end\":57951,\"start\":57942},{\"end\":57963,\"start\":57953},{\"end\":58400,\"start\":58391},{\"end\":58411,\"start\":58407},{\"end\":58424,\"start\":58420},{\"end\":58439,\"start\":58433},{\"end\":58459,\"start\":58452},{\"end\":58475,\"start\":58468},{\"end\":58491,\"start\":58484},{\"end\":58780,\"start\":58769},{\"end\":58798,\"start\":58791},{\"end\":59171,\"start\":59163},{\"end\":59190,\"start\":59182},{\"end\":59208,\"start\":59192},{\"end\":59506,\"start\":59500},{\"end\":59533,\"start\":59528},{\"end\":59549,\"start\":59542},{\"end\":59566,\"start\":59561},{\"end\":59877,\"start\":59871},{\"end\":60190,\"start\":60184},{\"end\":60207,\"start\":60200},{\"end\":60221,\"start\":60217},{\"end\":60236,\"start\":60229},{\"end\":60250,\"start\":60243},{\"end\":60265,\"start\":60260},{\"end\":60670,\"start\":60664},{\"end\":60685,\"start\":60676},{\"end\":60695,\"start\":60692},{\"end\":60709,\"start\":60703},{\"end\":61040,\"start\":61034},{\"end\":61052,\"start\":61047},{\"end\":61066,\"start\":61061},{\"end\":61413,\"start\":61407},{\"end\":61425,\"start\":61420},{\"end\":61434,\"start\":61431},{\"end\":61448,\"start\":61443},{\"end\":61827,\"start\":61821},{\"end\":61836,\"start\":61831},{\"end\":61851,\"start\":61846},{\"end\":61865,\"start\":61861},{\"end\":61873,\"start\":61867},{\"end\":62218,\"start\":62211},{\"end\":62246,\"start\":62225},{\"end\":62263,\"start\":62257},{\"end\":62498,\"start\":62493},{\"end\":62508,\"start\":62500}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14187105},\"end\":40804,\"start\":40449},{\"attributes\":{\"id\":\"b1\"},\"end\":41019,\"start\":40806},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7278297},\"end\":41374,\"start\":41021},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b3\"},\"end\":41686,\"start\":41376},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8471750},\"end\":42096,\"start\":41688},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16485439},\"end\":42482,\"start\":42098},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":205228801},\"end\":42776,\"start\":42484},{\"attributes\":{\"doi\":\"arXiv:1506.02075\",\"id\":\"b7\"},\"end\":43092,\"start\":42778},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2265838},\"end\":43344,\"start\":43094},{\"attributes\":{\"id\":\"b9\"},\"end\":43559,\"start\":43346},{\"attributes\":{\"doi\":\"arXiv:1704.00051\",\"id\":\"b10\"},\"end\":43847,\"start\":43561},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":215717103},\"end\":44489,\"start\":43849},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207206865},\"end\":44988,\"start\":44491},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":31567886},\"end\":45327,\"start\":44990},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13780041},\"end\":45697,\"start\":45329},{\"attributes\":{\"id\":\"b15\"},\"end\":45830,\"start\":45699},{\"attributes\":{\"doi\":\"arXiv:1707.03904\",\"id\":\"b16\"},\"end\":46134,\"start\":45832},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3477924},\"end\":46581,\"start\":46136},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14999613},\"end\":47138,\"start\":46583},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1831060},\"end\":47600,\"start\":47140},{\"attributes\":{\"id\":\"b20\"},\"end\":47849,\"start\":47602},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":19550350},\"end\":48366,\"start\":47851},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16833759},\"end\":48806,\"start\":48368},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6203757},\"end\":49253,\"start\":48808},{\"attributes\":{\"doi\":\"arXiv:1606.03622\",\"id\":\"b24\"},\"end\":49472,\"start\":49255},{\"attributes\":{\"id\":\"b25\"},\"end\":49969,\"start\":49474},{\"attributes\":{\"doi\":\"arXiv:1705.03551\",\"id\":\"b26\"},\"end\":50346,\"start\":49971},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":205073708},\"end\":50667,\"start\":50348},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9111991},\"end\":51204,\"start\":50669},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16645119},\"end\":51582,\"start\":51206},{\"attributes\":{\"id\":\"b30\"},\"end\":51808,\"start\":51584},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1671874},\"end\":52005,\"start\":51810},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14040055},\"end\":52552,\"start\":52007},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":18795006},\"end\":52989,\"start\":52554},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":19127468},\"end\":53466,\"start\":52991},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":11080756},\"end\":54062,\"start\":53468},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":29217228},\"end\":54481,\"start\":54064},{\"attributes\":{\"doi\":\"arXiv:1805.06816\",\"id\":\"b37\"},\"end\":54816,\"start\":54483},{\"attributes\":{\"doi\":\"arXiv:1606.05250\",\"id\":\"b38\"},\"end\":55138,\"start\":54818},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2100831},\"end\":55460,\"start\":55140},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":27080528},\"end\":55993,\"start\":55462},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3519414},\"end\":56470,\"start\":55995},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":26285231},\"end\":56816,\"start\":56472},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":348944},\"end\":57171,\"start\":56818},{\"attributes\":{\"doi\":\"arXiv:1703.10090\",\"id\":\"b44\"},\"end\":57501,\"start\":57173},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":7961699},\"end\":57824,\"start\":57503},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":42795659},\"end\":58384,\"start\":57826},{\"attributes\":{\"doi\":\"arXiv:1611.09830\",\"id\":\"b47\"},\"end\":58760,\"start\":58386},{\"attributes\":{\"id\":\"b48\"},\"end\":59050,\"start\":58762},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":7903613},\"end\":59441,\"start\":59052},{\"attributes\":{\"id\":\"b50\"},\"end\":59809,\"start\":59443},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":20081461},\"end\":60086,\"start\":59811},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14910898},\"end\":60589,\"start\":60088},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":9829828},\"end\":60972,\"start\":60591},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":20264071},\"end\":61304,\"start\":60974},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9313458},\"end\":61738,\"start\":61306},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":30029552},\"end\":62155,\"start\":61740},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":9237797},\"end\":62445,\"start\":62157},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":16944215},\"end\":62627,\"start\":62447}]", "bib_title": "[{\"end\":40532,\"start\":40449},{\"end\":41062,\"start\":41021},{\"end\":41743,\"start\":41688},{\"end\":42162,\"start\":42098},{\"end\":42562,\"start\":42484},{\"end\":43164,\"start\":43094},{\"end\":43906,\"start\":43849},{\"end\":44558,\"start\":44491},{\"end\":45069,\"start\":44990},{\"end\":45398,\"start\":45329},{\"end\":46204,\"start\":46136},{\"end\":46642,\"start\":46583},{\"end\":47190,\"start\":47140},{\"end\":47915,\"start\":47851},{\"end\":48437,\"start\":48368},{\"end\":48848,\"start\":48808},{\"end\":50425,\"start\":50348},{\"end\":50727,\"start\":50669},{\"end\":51252,\"start\":51206},{\"end\":51849,\"start\":51810},{\"end\":52059,\"start\":52007},{\"end\":52608,\"start\":52554},{\"end\":53058,\"start\":52991},{\"end\":53530,\"start\":53468},{\"end\":54112,\"start\":54064},{\"end\":55217,\"start\":55140},{\"end\":55504,\"start\":55462},{\"end\":56104,\"start\":55995},{\"end\":56568,\"start\":56472},{\"end\":56883,\"start\":56818},{\"end\":57553,\"start\":57503},{\"end\":57908,\"start\":57826},{\"end\":59153,\"start\":59052},{\"end\":59490,\"start\":59443},{\"end\":59863,\"start\":59811},{\"end\":60176,\"start\":60088},{\"end\":60656,\"start\":60591},{\"end\":61026,\"start\":60974},{\"end\":61399,\"start\":61306},{\"end\":61813,\"start\":61740},{\"end\":62201,\"start\":62157},{\"end\":62489,\"start\":62447}]", "bib_author": "[{\"end\":40550,\"start\":40534},{\"end\":40882,\"start\":40867},{\"end\":40896,\"start\":40882},{\"end\":40907,\"start\":40896},{\"end\":41076,\"start\":41064},{\"end\":41093,\"start\":41076},{\"end\":41111,\"start\":41093},{\"end\":41125,\"start\":41111},{\"end\":41143,\"start\":41125},{\"end\":41157,\"start\":41143},{\"end\":41465,\"start\":41447},{\"end\":41480,\"start\":41465},{\"end\":41495,\"start\":41480},{\"end\":41762,\"start\":41745},{\"end\":41778,\"start\":41762},{\"end\":41793,\"start\":41778},{\"end\":41813,\"start\":41793},{\"end\":41831,\"start\":41813},{\"end\":41843,\"start\":41831},{\"end\":41856,\"start\":41843},{\"end\":41879,\"start\":41856},{\"end\":42178,\"start\":42164},{\"end\":42192,\"start\":42178},{\"end\":42209,\"start\":42192},{\"end\":42225,\"start\":42209},{\"end\":42585,\"start\":42564},{\"end\":42854,\"start\":42838},{\"end\":42871,\"start\":42854},{\"end\":42885,\"start\":42871},{\"end\":42899,\"start\":42885},{\"end\":43180,\"start\":43166},{\"end\":43197,\"start\":43180},{\"end\":43419,\"start\":43401},{\"end\":43437,\"start\":43419},{\"end\":43447,\"start\":43437},{\"end\":43624,\"start\":43612},{\"end\":43636,\"start\":43624},{\"end\":43650,\"start\":43636},{\"end\":43666,\"start\":43650},{\"end\":43917,\"start\":43908},{\"end\":43933,\"start\":43917},{\"end\":43940,\"start\":43933},{\"end\":44576,\"start\":44560},{\"end\":44587,\"start\":44576},{\"end\":44601,\"start\":44587},{\"end\":44617,\"start\":44601},{\"end\":45086,\"start\":45071},{\"end\":45102,\"start\":45086},{\"end\":45121,\"start\":45102},{\"end\":45421,\"start\":45400},{\"end\":45443,\"start\":45421},{\"end\":45463,\"start\":45443},{\"end\":45762,\"start\":45746},{\"end\":45911,\"start\":45895},{\"end\":45929,\"start\":45911},{\"end\":45946,\"start\":45929},{\"end\":46219,\"start\":46206},{\"end\":46231,\"start\":46219},{\"end\":46242,\"start\":46231},{\"end\":46255,\"start\":46242},{\"end\":46267,\"start\":46255},{\"end\":46273,\"start\":46267},{\"end\":46658,\"start\":46644},{\"end\":46674,\"start\":46658},{\"end\":46688,\"start\":46674},{\"end\":47208,\"start\":47192},{\"end\":47220,\"start\":47208},{\"end\":47242,\"start\":47220},{\"end\":47253,\"start\":47242},{\"end\":47267,\"start\":47253},{\"end\":47287,\"start\":47267},{\"end\":47299,\"start\":47287},{\"end\":47316,\"start\":47299},{\"end\":47329,\"start\":47316},{\"end\":47342,\"start\":47329},{\"end\":47693,\"start\":47679},{\"end\":47710,\"start\":47693},{\"end\":47722,\"start\":47710},{\"end\":47932,\"start\":47917},{\"end\":47952,\"start\":47932},{\"end\":48446,\"start\":48439},{\"end\":48460,\"start\":48446},{\"end\":48471,\"start\":48460},{\"end\":48484,\"start\":48471},{\"end\":48494,\"start\":48484},{\"end\":48871,\"start\":48850},{\"end\":48886,\"start\":48871},{\"end\":48907,\"start\":48886},{\"end\":48923,\"start\":48907},{\"end\":48933,\"start\":48923},{\"end\":48951,\"start\":48933},{\"end\":48965,\"start\":48951},{\"end\":49314,\"start\":49303},{\"end\":49327,\"start\":49314},{\"end\":49543,\"start\":49529},{\"end\":49552,\"start\":49543},{\"end\":49559,\"start\":49552},{\"end\":49571,\"start\":49559},{\"end\":49586,\"start\":49571},{\"end\":49603,\"start\":49586},{\"end\":49618,\"start\":49603},{\"end\":49637,\"start\":49618},{\"end\":49650,\"start\":49637},{\"end\":49673,\"start\":49650},{\"end\":49687,\"start\":49673},{\"end\":49693,\"start\":49687},{\"end\":50075,\"start\":50061},{\"end\":50088,\"start\":50075},{\"end\":50098,\"start\":50088},{\"end\":50109,\"start\":50098},{\"end\":50122,\"start\":50109},{\"end\":50455,\"start\":50427},{\"end\":50469,\"start\":50455},{\"end\":50484,\"start\":50469},{\"end\":50741,\"start\":50729},{\"end\":50755,\"start\":50741},{\"end\":50771,\"start\":50755},{\"end\":50785,\"start\":50771},{\"end\":50800,\"start\":50785},{\"end\":50811,\"start\":50800},{\"end\":50826,\"start\":50811},{\"end\":50846,\"start\":50826},{\"end\":51269,\"start\":51254},{\"end\":51286,\"start\":51269},{\"end\":51303,\"start\":51286},{\"end\":51317,\"start\":51303},{\"end\":51649,\"start\":51631},{\"end\":51664,\"start\":51649},{\"end\":51674,\"start\":51664},{\"end\":51861,\"start\":51851},{\"end\":51869,\"start\":51861},{\"end\":52084,\"start\":52061},{\"end\":52107,\"start\":52084},{\"end\":52127,\"start\":52107},{\"end\":52146,\"start\":52127},{\"end\":52624,\"start\":52610},{\"end\":52645,\"start\":52624},{\"end\":52653,\"start\":52645},{\"end\":53075,\"start\":53060},{\"end\":53091,\"start\":53075},{\"end\":53104,\"start\":53091},{\"end\":53113,\"start\":53104},{\"end\":53550,\"start\":53532},{\"end\":53564,\"start\":53550},{\"end\":53575,\"start\":53564},{\"end\":53589,\"start\":53575},{\"end\":54132,\"start\":54114},{\"end\":54154,\"start\":54132},{\"end\":54563,\"start\":54545},{\"end\":54585,\"start\":54563},{\"end\":54603,\"start\":54585},{\"end\":54625,\"start\":54603},{\"end\":54897,\"start\":54879},{\"end\":54909,\"start\":54897},{\"end\":54929,\"start\":54909},{\"end\":54942,\"start\":54929},{\"end\":55239,\"start\":55219},{\"end\":55256,\"start\":55239},{\"end\":55269,\"start\":55256},{\"end\":55278,\"start\":55269},{\"end\":55520,\"start\":55506},{\"end\":55541,\"start\":55520},{\"end\":56124,\"start\":56106},{\"end\":56145,\"start\":56124},{\"end\":56159,\"start\":56145},{\"end\":56175,\"start\":56159},{\"end\":56584,\"start\":56570},{\"end\":56598,\"start\":56584},{\"end\":56892,\"start\":56885},{\"end\":56902,\"start\":56892},{\"end\":56916,\"start\":56902},{\"end\":56935,\"start\":56916},{\"end\":56949,\"start\":56935},{\"end\":56962,\"start\":56949},{\"end\":56974,\"start\":56962},{\"end\":57265,\"start\":57251},{\"end\":57282,\"start\":57265},{\"end\":57300,\"start\":57282},{\"end\":57571,\"start\":57555},{\"end\":57586,\"start\":57571},{\"end\":57597,\"start\":57586},{\"end\":57918,\"start\":57910},{\"end\":57933,\"start\":57918},{\"end\":57953,\"start\":57933},{\"end\":57965,\"start\":57953},{\"end\":58402,\"start\":58386},{\"end\":58413,\"start\":58402},{\"end\":58426,\"start\":58413},{\"end\":58441,\"start\":58426},{\"end\":58461,\"start\":58441},{\"end\":58477,\"start\":58461},{\"end\":58493,\"start\":58477},{\"end\":58782,\"start\":58762},{\"end\":58800,\"start\":58782},{\"end\":59173,\"start\":59155},{\"end\":59192,\"start\":59173},{\"end\":59210,\"start\":59192},{\"end\":59508,\"start\":59492},{\"end\":59535,\"start\":59508},{\"end\":59551,\"start\":59535},{\"end\":59568,\"start\":59551},{\"end\":59879,\"start\":59865},{\"end\":60192,\"start\":60178},{\"end\":60209,\"start\":60192},{\"end\":60223,\"start\":60209},{\"end\":60238,\"start\":60223},{\"end\":60252,\"start\":60238},{\"end\":60267,\"start\":60252},{\"end\":60672,\"start\":60658},{\"end\":60687,\"start\":60672},{\"end\":60697,\"start\":60687},{\"end\":60711,\"start\":60697},{\"end\":61042,\"start\":61028},{\"end\":61054,\"start\":61042},{\"end\":61068,\"start\":61054},{\"end\":61415,\"start\":61401},{\"end\":61427,\"start\":61415},{\"end\":61436,\"start\":61427},{\"end\":61450,\"start\":61436},{\"end\":61829,\"start\":61815},{\"end\":61838,\"start\":61829},{\"end\":61853,\"start\":61838},{\"end\":61867,\"start\":61853},{\"end\":61875,\"start\":61867},{\"end\":62220,\"start\":62203},{\"end\":62248,\"start\":62220},{\"end\":62265,\"start\":62248},{\"end\":62500,\"start\":62491},{\"end\":62510,\"start\":62500}]", "bib_venue": "[{\"end\":40583,\"start\":40550},{\"end\":40865,\"start\":40806},{\"end\":41173,\"start\":41157},{\"end\":41445,\"start\":41376},{\"end\":41884,\"start\":41879},{\"end\":42276,\"start\":42225},{\"end\":42607,\"start\":42585},{\"end\":42836,\"start\":42778},{\"end\":43204,\"start\":43197},{\"end\":43399,\"start\":43346},{\"end\":43610,\"start\":43561},{\"end\":44056,\"start\":43940},{\"end\":44698,\"start\":44617},{\"end\":45144,\"start\":45121},{\"end\":45496,\"start\":45463},{\"end\":45744,\"start\":45699},{\"end\":45893,\"start\":45832},{\"end\":46326,\"start\":46273},{\"end\":46796,\"start\":46688},{\"end\":47353,\"start\":47342},{\"end\":47677,\"start\":47602},{\"end\":48039,\"start\":47952},{\"end\":48570,\"start\":48494},{\"end\":49014,\"start\":48965},{\"end\":49301,\"start\":49255},{\"end\":49527,\"start\":49474},{\"end\":50059,\"start\":49971},{\"end\":50488,\"start\":50484},{\"end\":50911,\"start\":50846},{\"end\":51382,\"start\":51317},{\"end\":51629,\"start\":51584},{\"end\":51894,\"start\":51869},{\"end\":52231,\"start\":52146},{\"end\":52725,\"start\":52653},{\"end\":53179,\"start\":53113},{\"end\":53672,\"start\":53589},{\"end\":54217,\"start\":54154},{\"end\":54543,\"start\":54483},{\"end\":54877,\"start\":54818},{\"end\":55283,\"start\":55278},{\"end\":55685,\"start\":55541},{\"end\":56224,\"start\":56175},{\"end\":56631,\"start\":56598},{\"end\":56979,\"start\":56974},{\"end\":57249,\"start\":57173},{\"end\":57646,\"start\":57597},{\"end\":58040,\"start\":57965},{\"end\":58548,\"start\":58509},{\"end\":58859,\"start\":58800},{\"end\":59228,\"start\":59210},{\"end\":59600,\"start\":59568},{\"end\":59934,\"start\":59879},{\"end\":60322,\"start\":60267},{\"end\":60766,\"start\":60711},{\"end\":61123,\"start\":61068},{\"end\":61505,\"start\":61450},{\"end\":61930,\"start\":61875},{\"end\":62270,\"start\":62265},{\"end\":62514,\"start\":62510},{\"end\":40603,\"start\":40585},{\"end\":44159,\"start\":44058},{\"end\":44766,\"start\":44700},{\"end\":46891,\"start\":46798},{\"end\":48113,\"start\":48041},{\"end\":52303,\"start\":52233},{\"end\":52784,\"start\":52727},{\"end\":53247,\"start\":53181},{\"end\":53742,\"start\":53674},{\"end\":54284,\"start\":54219},{\"end\":58102,\"start\":58042},{\"end\":58906,\"start\":58861}]"}}}, "year": 2023, "month": 12, "day": 17}