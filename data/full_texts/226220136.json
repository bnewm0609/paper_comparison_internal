{"id": 226220136, "updated": "2023-04-05 08:04:02.616", "metadata": {"title": "URVOS: Uni\ufb01ed Referring Video Object Segmentation Network with a Large-Scale Benchmark", "authors": "[{\"first\":\"Seonguk\",\"last\":\"Seo\",\"middle\":[]},{\"first\":\"Joon-Young\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Bohyung\",\"last\":\"Han\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": ". We propose a uni\ufb01ed referring video object segmentation network (URVOS). URVOS takes a video and a referring expression as inputs, and estimates the object masks referred by the given language expression in the whole video frames. Our algorithm addresses the challenging problem by performing language-based object segmentation and mask propagation jointly using a single deep neural network with a proper combination of two attention models. In addition, we construct the \ufb01rst large-scale referring video object segmentation dataset called Refer-Youtube-VOS. We evaluate our model on two benchmark datasets including ours and demonstrate the e\ufb00ectiveness of the proposed approach. The dataset is released at https://github.com/ skynbe/Refer-Youtube-VOS .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3104844437", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/SeoLH20", "doi": "10.1007/978-3-030-58555-6_13"}}, "content": {"source": {"pdf_hash": "cd0255491af35c5bc5f7cfa473718c19f67fcac3", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4fa4ec965388e608ce02d23613290425449b493e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cd0255491af35c5bc5f7cfa473718c19f67fcac3.txt", "contents": "\nURVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark\n\n\nSeonguk Seo \nSeoul National University\n\n\nJoon-Young Lee \nAdobe Research\n\n\nBohyung Han \nSeoul National University\n\n\nURVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark\nvideo object segmentation, referring object segmentation\nWe propose a unified referring video object segmentation network (URVOS). URVOS takes a video and a referring expression as inputs, and estimates the object masks referred by the given language expression in the whole video frames. Our algorithm addresses the challenging problem by performing language-based object segmentation and mask propagation jointly using a single deep neural network with a proper combination of two attention models. In addition, we construct the first large-scale referring video object segmentation dataset called Refer-Youtube-VOS. We evaluate our model on two benchmark datasets including ours and demonstrate the effectiveness of the proposed approach. The dataset is released at https://github.com/ skynbe/Refer-Youtube-VOS.\n\nIntroduction\n\nVideo object segmentation, which separates foreground objects from background in a video sequence, has attracted wide attention due to its applicability to many practical problems including video analysis and video editing. Typically, this task has been addressed in unsupervised or semi-supervised ways. Unsupervised techniques [31,7] perform segmentation without the guidance for foreground objects, and aim to estimate the object masks using salient features, independent motions, or known class labels automatically. Due to the ambiguity and the lack of flexibility in defining foreground objects, such approaches may be suitable for video analysis but not for video editing that requires to segment arbitrary objects or their parts flexibly. In the semi-supervised scenario, where the ground-truth mask is available at least in a single frame, existing methods [2,24,35,32,29,22] propagate the ground-truth object mask to the rest of frames in a video. They fit well for interactive video editing but require tedious and time-consuming step to obtain ground-truth masks. To overcome such limitations, interactive approaches [1,3,4,21] have recently been investigated to allow user interventions during inference.\n\nDespite great progress in semi-supervised and interactive video object segmentation, pixel-level interactions are still challenging especially in mobile video editing and augmented reality use-cases. To address the challenge, we consider a different type of interaction, language expressions, and introduce a new task that segments an object referred by the given language expression in a video. We call this task as referring video object segmentation.\n\nA na\u00efve baseline for the task is applying referring image segmentation techniques [16,12,37,36] to each input frame independently. However, it does not leverage temporal coherency of video frames and, consequently, may result in inconsistent object mask predictions across frames. Another option is a sequential integration of referring image segmentation and semi-supervised video object segmentation. In this case, a referring image segmentation method initializes an object mask at a certain frame and then a video object segmentation method propagates the mask to the rest of the frames. This would work well if the initialization is successful. However, it often overfits to the particular characteristics in the anchor frame, which may not be robust in practice in the presence of occlusions or background clutter. Recently, Khoreva et al. [10] tackle this task by generating a set of mask proposals and choosing the most temporally-consistent set of candidates, but such a post-selection approach has inevitable limitation in maintaining temporal coherence.\n\nWe propose URVOS, a unified referring video object segmentation network. URVOS is an end-to-end framework for referring video object segmentation, which performs referring image segmentation and semi-supervised video object segmentation jointly in a single model. In this unified network, we incorporate two attention modules, cross-modal attention and memory attention modules, where memory attention encourages temporal consistency while cross-modal attention prevents drift. In addition, we introduce a new large-scale benchmark dataset for referring video object segmentation task, called Refer-Youtube-VOS. Our dataset is one order of magnitude larger than the previous benchmark [10], which enables researchers to develop new models and validate their performance. We evaluate the proposed method extensively and observe that our approach achieves outstanding performance gain on the new large-scale dataset.\n\nOur contributions are summarized below.\n\n\u2022 We construct a large-scale referring video object segmentation dataset, which contains 27, 000+ referring expressions for 3, 900+ videos.\n\n\u2022 We propose a unified end-to-end deep neural network that performs both language-based object segmentation and mask propagation in a single model.\n\n\u2022 Our method achieves significant performance gains over previous methods in the referring video object segmentation task.\n\n\nRelated Work\n\nReferring Image Segmentation This task aims to produce a segmentation mask of an object in an input image given a natural language expression. Hu et\n\nFull : \"A person on the right dressed in blue black walking while holding a white bottle.\" First : \"A woman in a blue shirt and a black bag.\"\n\nFull : \"A person showing his skateboard skills on the road.\" First : \"A man wearing a white cap.\" Fig. 1: Annotation examples of Refer-Youtube-VOS dataset. \"Full\" denotes that annotators watch the entire video for annotation while \"First\" means that they are given only the first frame of each video.\n\nal. [8] first propose the task with a baseline algorithm that relies on multimodal visual-and-linguistic features extracted from LSTM and CNN. RRN [12] utilizes the feature pyramid structures to take advantage of multi-scale semantics for referring image segmentation. MAttNet [37] introduces a modular attention network, which decomposes a multi-modal reasoning model into a subject, object and relationship modules, and exploits attention to focus on relevant modules. CMSA [36] employs cross-modal self-attentive features to bridge the attentions in language and vision domains and capture long-range correlations between visual and linguistic modalities effectively. Our model employs a variant of CMSA to obtain the cross-modal attentive features effectively.\n\nVideo Object Segmentation Video object segmentation is categorized into two types. Unsupervised approaches do not allow user interactions during test time, and aim to segment the most salient spatio-temporal object tubes. They typically employ two-stream networks to fuse motion and appearance cues [27,13,38] for learning spatio-temporal representations. Semi-supervised video object segmentation tracks an object mask in a whole video, assuming that the ground-truth object mask is provided for the first frame. With the introduction of DAVIS [25] and Youtube-VOS [34] datasets, there has been great progress in this task. There are two main categories, online learning and offline learning. Most approaches rely on online learning, which fine-tunes networks using the first-frame ground-truth at test-time [2,14,24]. While the online learning achieves outstanding results, its computational complexity at test-time limits its practical use. Offline methods alleviate this issue and reduce runtime [35,32,29,22]. STM [22] presents a space-time memory network by non-local matching between previous and current frames, which achieves stateof-the-art performance, even beating online learning methods. Our model also belongs to offline learning, which modifies the non-local module of STM for its integration into our memory attention network and exploits temporal coherence of segmentation results.\n\nMulti-modal Video Understanding The intersection of language and video understanding has been investigated in various areas including visual tracking [15], action segmentation [6,30], video captioning [19] and video question answering [5]. Gavrilyuk et al. [6] adopt a fully-convolutional model to segment an actor and its action in each frame of a video as specified by a language query. However, their method has been validated in the datasets with limited class diversities, A2D [33] and J-HMDB [9], which only have 8 and 21 predefined action classes, respectively. Khoreva et al. [10] have augmented the DAVIS dataset with language referring expressions and have proposed a way to transfer image-level grounding models to video domain. Although [10] is closely related to our work, it fails to exploit valuable temporal information in videos during training.\n\n\nRefer-Youtube-VOS Dataset\n\nThere exist previous works [6,10] that constructed referring segmentation datasets for videos. Gavrilyuk et al. [6] extended the A2D [33] and J-HMDB [9] datasets with natural sentences; the datasets focus on describing the 'actors' and 'actions' appearing in videos, therefore the instance annotations are limited to only a few object categories corresponding to the dominant 'actors' performing a salient 'action'. Khoreva et al. [10] built a dataset based on DAVIS [25], but the scales are barely sufficient to learn an end-to-end model from scratch. To facilitate referring video object segmentation, we have constructed a largescale video object segmentation dataset, Youtube-VOS [34], with referring expressions. Youtube-VOS has 4,519 high-resolution videos with 94 common object categories. Each video has pixel-level instance segmentation annotation at every 5 frames in 30-fps videos, and their durations are around 3 to 6 seconds.\n\nWe employed Amazon Mechanical Turk to annotate referring expressions. To ensure the quality of the annotations, we selected around 50 turkers after a validation test. Each turker was given a pair of videos, the original video and the mask-overlaid one with the target object highlighted, and was asked to provide a discriminative sentence within 20 words that describes the target object accurately. We collected two kinds of annotations, which describe the highlighted object (1) based on a whole video (Full-video expression) and (2) using only the first frame of the video (First-frame expression). After the initial annotation, we conducted verification and cleaning jobs for all annotations, and dropped objects if an object cannot be localized using language expressions only. The followings are the statistics and analysis of the two annotation types of the dataset after the verification. The number of annotated objects is lower than that of the full-video expressions because using only the first frame makes annotation more ambiguous and inconsistent and we dropped more annotations during the verification. On average, each video has 3.2 language expressions and each expression has 7.5 words. Fig. 1 illustrates examples of our dataset and shows the differences between two annotation types. The full-video expressions can use both static and dynamic information of a video while the first-frame expressions focus mostly on appearance information. We also provide the quantitative comparison of our dataset against the existing ones in Table 1, which presents that our dataset contains much more videos and language expressions.\n\n\nDataset analysis\n\n\nUnified Referring VOS Network\n\nGiven a video with N frames and a language query Q, the goal of referring video object segmentation is to predict binary segmentation masks for the object(s) corresponding to the query Q in the N frames. As mentioned earlier, a na\u00efve approach is to estimate the mask for each frame independently. However, a direct application of image-based solutions to referring object segmentation [12,18,36,37] would fail to exploit valuable information, temporal coherence across the frames. Therefore, we cast the referring video object segmentation task as a  joint problem of referring object segmentation in an image [12,18,36,37] and mask propagation in a video [32,22].\n\n\nOur Framework\n\nWe propose a unified framework that performs referring image segmentation and video object segmentation jointly. Given a video and a referring expression, our network estimates an object mask in an input frame using the linguistic referring expression and the mask predictions in the previous frames. We iteratively process video frames until the mask predictions in all frames converge. Fig. 2 illustrates the overall architecture of our network.\n\n\nVisual Encoder\n\nWe employ ResNet-50 as our backbone network to extract visual features from an input frame. To include spatial information of the visual feature, we augment 8-dimensional spatial coordinates following [8]. Formally, let F \u2208 R H\u00d7W \u00d7C f and f p \u2208 R C f denote a visual feature map 3 and a sliced visual feature at a certain spatial location p on F, where p \u2208 {1, 2, ..., H \u00d7 W }. We concatenate the spatial coordinates to the visual features f p to obtain locationaware visual featuresf p as follows.\nf p = [f p ; s p ] \u2208 R C f +8 ,(1)\nwhere s p is a 8-dimensional spatial coordinate features 4 .\n\nLanguage Encoder Given a referral expression, a set of words in the expression is encoded as a multi-hot vector and projected onto an embedding space in C e dimensions using a linear layer. To model the sequential nature of language expressions while maintaining the semantics in the expression, we add positional encoding [28] at each word position. Let w l \u2208 R Ce and p l \u2208 R Ce denote the embeddings for the l-th word and the position of the expression, respectively. Our lingual feature is obtained by the sum of the two embedding vectors, i.e., e l = w l + p l \u2208 R Ce .\n\nCross-modal Attention Module Using both visual and lingual features, we produce a joint cross-modal feature representation by concatenating the features in both the domains. Unlike [36], we first apply self-attention to each feature independently before producing a joint feature to capture complex alignments between both modalities effectively. Each self-attention module maps each feature to a C a -dimensional space for both modalities as follows:\nf p = SA vis (f p ) \u2208 R Ca , e l = SA lang (e l ) \u2208 R Ca(2)\nwhere SA * (\u00b7) ( * \u2208 {vis, lang}) denotes a self-attention module for each domain. Then a joint cross-modal feature at each spatial position p and each word position l is given by\nc pl = [ f p ; e l ] \u2208 R Ca+Ca .(3)\nWe collect all cross-model features c pl and form a cross-modal feature map as C = {c pl | \u2200p, \u2200l} \u2208 R H\u00d7W \u00d7L\u00d7(Ca+Ca) . The next step is to apply self-attention to this cross-modal feature map C. Fig. 3(a) iluustrates our cross-modal attention module. We generate a set of (key, query, value) triplets, denoted by (k, q, v), using 2D convolutions as follows:\nk = Conv key (C) \u2208 R L\u00d7H\u00d7W \u00d7Ca (4) q = Conv query (C) \u2208 R L\u00d7H\u00d7W \u00d7Ca (5) v = Conv value (C) \u2208 R L\u00d7H\u00d7W \u00d7Ca(6)\nand we compute cross-modal attentive features by estimating the correlation between all combinations of pixels and words as\nc pl = c pl + \u2200p ,\u2200l Softmax(q pl \u00b7 k p l )v p l ,(7)\nwhere \u00b7 denotes the dot-product operator. We average the self-attentive features over words and derive the final cross-modal feature as c p = 1 L l c pl and C = { c p | \u2200p} \u2208 R H\u00d7W \u00d7C b .\n\nMemory Attention Module To leverage information in the mask predictions at the frames processed earlier, we extend the idea introduced in [22] and design  a memory attention module. This module retrieves the relevant information from the previous frame by computing the correlation between the visual feature map of the current frame and the mask-encoded visual feature map of the previous frame. Note that the mask-encoded visual features is obtained from another feature extractor that takes 4-channel inputs given by stacking an RGB image and its segmentation mask in the channel direction. We will call the current and previous frames as target and memory frames, respectively, hereafter.\n\nDifferent from the previous method [22], we introduce a 12-dimensional spatiotemporal coordinate feature, s tp 5 , where the first 3 dimensions encode normalized temporal positions, the next 6 dimensions represent normalized vertical and horizontal positions, and the last 3 dimensions contain the information about duration and frame size of the whole video.\n\nLet T be the number of memory frames. For a target frame and T memory frames, we first compute key (k, k mem ) and value (v, v mem ) embeddings as follows: where f and f mem denotes target and memory visual features, and s and s denotes spatial and spatio-temporal coordinate features, respectively. Then, the memory-attentive feature m p at the spatial location p is given by\nF = {[f p ; s p ]|\u2200p} \u2208 R H\u00d7W \u00d7(C f +8) (8) k = Conv key (F) \u2208 R H\u00d7W \u00d7C b (9) v = Conv value (F) \u2208 R H\u00d7W \u00d7C b(10)F mem = {[f mem tp ; s tp ]|\u2200t, \u2200p} \u2208 R T \u00d7H\u00d7W \u00d7(C f +12)(11)k mem = Conv mem key (F mem ) \u2208 R T \u00d7H\u00d7W \u00d7C b (12) v mem = Conv mem value (F mem ) \u2208 R T \u00d7H\u00d7W \u00d7C b (13)m p = v p + \u2200t ,\u2200p f (k p , k mem t p )v mem t p(14)\nand M = { m p | \u2200p} \u2208 R H\u00d7W \u00d7C b . Fig. 3(b) presents the detailed illustration of the memory attention module, which shows how it computes the relevance between target frame and memory frames using key-value structure. Since it attends the regions in the target frame that are relevant to previous predictions, our algorithm produces temporally coherent segmentation results. Note that we employ the 4 th stage features (Res4) for both target and memory frames in this module because it requires more descriptive features to compute the correlation between local regions of the frames, while cross-modal attention module employs the 5 th stage features (Res5) to exploit more semantic information.\n\nDecoder with Feature Pyramid Network We employ a coarse-to-fine hierarchical structure in our decoder to combine three kinds of semantic features; the cross-modal attentive feature map C, the memory attentive feature map M, and the original visual feature map F l in different levels l \u2208 {2, 3, 4, 5}. Fig. 4 illustrates how our decoder combines those three features using a feature pyramid network in a progressive manner. Each layer in the feature pyramid network takes the output of the previous layer and the ResBlock-encoded visual feature in the same level F l . Additionally, its first and the second layers incorporate crossattentive features C and memory-attentive features M, respectively, to capture multi-modal and temporal information effectively. Note that each layer in the feature pyramid network is upsampled by the factor of 2 to match the feature map size to that of the subsequent level. Instead of using the outputs from individual layers in the feature pyramid for mask generation, we employ an additional self-attention module following BFPN [23] to strengthen feature semantics of all levels. To this end, we first average the output features in all levels after normalizing their sizes and apply a self-attention to the combined feature map. The resulting map is rescaled to the original sizes, and the rescaled maps are aggregated to the original output feature maps forwarded through identity connections. Finally, these multi-scale outputs are employed to estimate segmentation masks in 1/4 scale of the input image, following the same pipeline in [11].\n\nInference Our network takes three kinds of inputs: a target image, memory images and their mask predictions, and a language expression. Since there is no predicted mask at the first frame, we introduce a novel two-stage procedure for its inference to fully exploit our two attention modules.\n\nIn the first stage, we run our network with no memory frame, which results in independent mask prediction at each frame based only on the language expression. After the initial per-frame mask estimation, we select an anchor frame, which has the most confident mask prediction for the language expression. To this end, we calculate the confidence score of each frame by averaging the pixelwsie final segmentation scores and select the frame with the highest one.\n\nIn the second stage, we update our initial segmentation results starting from the anchor to both ends using our full network. We first set the anchor frame as a memory frame, and re-estimate the object mask by sequentially propagating the mask prediction from anchor frame. After updating mask prediction at each frame, we add the image and its mask to the memory. In practice, however, cumulating all previous frames to the memory may cause memory overflow issues and slow down inference speed. To alleviate this problem, we set the maximum number of memory frames to T . If the number of memory frames reaches T , then we replace the least confident frame in the memory with the new prediction. Note that we leverage the previous mask predictions in the memory frames and estimate the mask of the target frame. At the same time, we use a language expression for guidance during the second stage as well, which allows us to handle challenging scenarios like drift and occlusions. We iteratively refine segmentation by repeating the second stage based on the new anchor identified at each iteration.\n\n\nExperiments\n\nWe first evaluate the proposed method on our Refer-Youtube-VOS dataset, and perform comparison to the existing work on the Refer-DAVIS 17 dataset [10]. We also provide diverse ablation studies to validate the effectiveness of our dataset and framework.  \n\n\nImplementation Details\n\nWe employ a pretrained ResNet-50 on the ImageNet dataset as our backbone network. Every frame of an input video is resized to 320 \u00d7 320. The maximum length of an expression, L, is 20 and the dimensionality of the word embedding space, C e , is 1,000. We train our model using the Adam optimizer with a batch size 16. Our model is trained end-to-end for 120 epochs. The learning rate is initialized to 2 \u00d7 10 \u22125 and decayed by the factor of 10 at every 80 epochs. We set the maximum number of memory frames, T , to 4.\n\n\nEvaluation Metrics\n\nWe use two standard evaluation metrics, the region similarity (J ) and the contour accuracy (F) following [26]. Additionally, we also measure prec@X, the percentage of correctly segmented frames in the whole dataset, given a predefined threshold X sampled from the range [0.5, 0.9]. Note that segmentation in a frame is regarded as successful if its J score is higher than a threshold.\n\n\nQuantitative Results\n\nRefer-Youtube-VOS We present the experimental results of our framework on the Refer-Youtube-VOS dataset in Table 2. We follow the original Youtube-(a) A person wearing a white shirt with white helmet riding a bike.\n\n(b) A laying cat gets up and jumps towards the camera.\n\n(c) A tiger to the right of another tiger.\n\n(d) A man with an instrument standing in between two other people. VOS dataset [34] to split the data into training and validation sets. In Table 2, 'Baseline' denotes a variant of the frame-based model [36] with our feature pyramid decoder while 'Baseline + RNN' is an extension of the baseline model, which applies a GRU layer to the visual features extracted from multiple input frames for sequential estimation of masks. 'Ours w/o cross-modal attention' and 'Ours w/o memory attention' are the ablative models without the cross-modal attention module and the memory attention module, respectively, for both training and inference. As shown in Table 2, our full model achieves remarkable performance gain over all the compared models on the Refer-Youtube-VOS dataset. The huge performance boost in our full model with respect to the ablative ones implies crucial role of the integrated attention modules in this referring video object segmentation task.\n\nRefer-DAVIS 17 DAVIS 2017 [25] is the most popular benchmark dataset for the video object segmentation task, which consists of 197 objects in 89 videos. Each video is composed of high-resolution frames with segmentation annotations, and involves various challenges including occlusions, multi-object interactions, camera motion, etc. Refer-DAVIS 17 [10] is the extension of DAVIS 2017 with natural language expressions. We evaluated all the models tested on the Refer-Youtube-VOS dataset. Because the number of videos in the DAVIS dataset is not sufficient to train the models for our task from scratch, we pretrain the models on Refer-Youtube-VOS and then fine-tune them on Refer-DAVIS 17 . Table 3 shows the experimental results, where our model outperforms the existing \n\n\nQualitative Results\n\nFig . 5 illustrates the qualitative results of our method on the Refer-Youtube-VOS dataset. The proposed model segments the target objects successfully with sharp boundaries on many videos and queries. We observe that our framework handles occlusion, deformation, and target identification effectively. See our supplementary documents for more qualitative results.\n\n\nAnalysis\n\nDataset Scale To investigate how the accuracy of a model changes depending on dataset sizes, we conduct experiments on four different subsets of the Refer-Youtube-VOS dataset, which contains 10%, 20%, 30%, and 50% of training examples, respectively. Table 4 presents the impact of dataset scale on model performance. As expected, the accuracy gradually improves upon the increase in the dataset size, which demonstrates the importance of a large-scale dataset on the referring video object segmentation task.\n\nInference procedure To validate the effectiveness of our inference scheme, we compare it with two other options for mask prediction. The baseline method, denoted by 'Forward', computes the mask at the first frame and propagates it in the forward direction until the end of video. We have also tested a variant ('Anchor + Previous') of the proposed two-stage inference method. 'Anchor + Previous' first estimates the masks in each frame independently and propagate an anchor frame in a sequential manner, where the previous T frames are used as memory frames during the second stage. Table 5 presents that our full inference technique gives the best performance, which implies that both use of anchor frames and memory frame selection by confidence contribute to improving segmentation results.  Iterative inference We study the benefit given by the multiple iterations of the second stage inference step. Table 6 illustrates that the iterative inference procedure gradually improves accuracy and tends to be saturated after 5 iterations.\n\n\nConclusion\n\nWe have proposed a unified referring video object segmentation network to exploit both language-based object segmentation and mask propagation in a single model. Our two attention modules, cross-modal attention and memory attention, collaborate to obtain accurate target object masks specified by language expressions and achieve temporally coherent segmentation results across frames. We also constructed the first large-scale referring video object segmentation dataset. Our framework accomplishes remarkable performance gain on our new dataset as well as the existing one. We believe the new dataset and our proposed method will foster the new direction in this line of research.\n\n\nFull-video expressionYoutube-VOS has 6,459 and 1,063 unique objects in train and validation split, respectively. Among them, we cover 6,388 unique objects in 3,471 videos (6, 388/6, 459 = 98.9%) with 12,913 expressions in train split and 1,063 unique objects in 507 videos (1, 063/1, 063 = 100%) with 2,096 expressions in validation split. On average, each video has 3.8 language expressions and each expression has 10.0 words. First-frame expression There are 6,006 unique objects in 3,412 videos (6, 006 /6, 459 = 93.0%) with 10,897 expressions in train split and 1,030 unique objects in 507 videos (1, 030/1, 063 = 96.9%) with 1,993 expressions in validation split.\n\nFig. 2 :\n2The overall architecture of our framework. We employ ResNet-50 as our encoder and use the 4 th and the 5 th stage features (Res4 and Res5) to estimate memory and cross-modal attention, respectively. Both memory-attentive and cross-modal attentive features are progressively combined in the decoder.\n\nFig. 3 :\n3Detailed illustrations of cross-modal and memory attention modules. Each module retrieves relevant information from language and memory frames for the target image to obtain self-attentive features.\n\nFig. 4 :\n4Detailed illustration of our decoder. It first combines the cross-modal attentive feature map C, the memory attentive feature map M, and the original visual feature map F l in multiple levels l \u2208 {2, 3, 4, 5} in a progressive manner. 'R' denotes ResBlocks and '\u00d72' denotes upsampling layers in this figure. The multi-scale outputs are strengthened through a self-attention module, and then employed to estimate the final segmentation masks.\n\nFig. 5 :\n5Qualitative results of our models on Refer-Youtube-VOS dataset.\n\nTable 1 :\n1Datasets for referring video object segmentation. J-HMDB and A2D Sentences[6] focus on 'action' recognition along with 'actor' segmentation, which have different purposes than ours. Although Refer-DAVIS 16/17 are well-suited for our task, they are small datasets with limited diversity. Our dataset, Refer-Youtube-VOS, is the largest dataset containing objects in diverse categories.Dataset \nTarget \nVideos \nObjects \nExpressions \n\nJ-HMDB Sentences [6] \nActor \n928 \n928 \n928 \nA2D Sentences [6] \nActor \n3782 \n4825 \n6656 \nRefer-DAVIS16 [10] \nObject \n50 \n50 \n100 \nRefer-DAVIS17 [10] \nObject \n90 \n205 \n1544 \nRefer-Youtube-VOS (Ours) \nObject \n3975 \n7451 \n27899 \n\n\n\nTable 2 :\n2The quantitative evaluation of referring video object segmentation on the Refer-Youtube-VOS validation set.Method \nprec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 J \nF \nBaseline (Image-based) \n31.98 \n27.66 \n21.54 \n14.56 \n4.33 \n33.34 36.54 \nBaseline + RNN \n40.24 \n35.90 \n30.34 \n22.26 \n9.35 \n34.79 38.08 \nOurs w/o cross-modal attention 41.58 \n36.12 \n28.50 \n20.13 \n8.37 \n38.88 42.82 \nOurs w/o memory attention \n46.26 \n40.98 \n34.81 \n25.42 \n10.86 39.38 41.78 \nOurs \n52.19 \n46.77 \n40.16 \n27.68 \n14.11 45.27 49.19 \n\n\n\nTable 3 :\n3The quantitative evaluation of referring video object segmentation on Refer-DAVIS 17 validation set.Method \nPretrained \nJ \nF \nKhoreva et al. [10] \nRefCOCO [20] \n37.3 \n41.3 \nOurs \nRefCOCO [20] \n41.23 \n47.01 \nBaseline (frame-based) \nRefer-YV (ours) \n32.19 \n37.23 \nBasline + RNN \nRefer-YV (ours) \n36.94 \n43.45 \nOurs w/o cross-modal attention \nRefer-YV (ours) \n38.25 \n43.20 \nOurs w/o memory attention \nRefer-YV (ours) \n39.43 \n45.87 \nOurs (pretraining only) \nRefer-YV (ours) \n44.29 \n49.41 \nOurs \nRefer-YV (ours) \n47.29 \n55.96 \n\n\n\nTable 4 :\n4The effects of dataset scale on our algorithm. We evaluate on the same validation set for each scale. method[10] and the ablative models. For the fair comparison with[10], we pretrained our model on a large-scale referring image segmentation benchmark[20]; our method turns out to be better than[10] under the same pretraining environment. Also, note that our model pretrained on Refer-Youtube-VOS with no fine-tuning on Refer-DAVIS 17 outperforms all other baselines while our full model further boosts accuracy significantly. This demonstrates the effectiveness of the new large-scale dataset and the proposed network.Dataset Scale \n10% \n20% \n30% \n50% \n100% \nJ \n30.73 \n37.77 \n39.19 \n42.48 \n45.27 \nF \n32.92 \n41.02 \n43.15 \n46.05 \n49.19 \n\n\n\nTable 5 :\n5Ablation study on the effects of inference procedures.Inference scheme \nJ \nF \nForward \n43.13 \n49.07 \nAnchor + Previous \n44.58 \n49.14 \nOurs \n45.27 \n49.19 \n\n\n\nTable 6 :\n6Iteration of inference procedures in terms of region similarity (J ).Stage 1 \nStage 2 \n\nIter 1 \nIter 2 \nIter 3 \nIter 4 \nIter 5 \nIter 10 \nJ \n41.34 \n45.27 \n45.33 \n45.41 \n45.44 \n45.43 \n45.46 \n\n\n\u2020 This work was done during an internship at Adobe Research.\nWe use Res5 and Res4 feature maps in our model. 4 For each spatial grid (h, w), sp = [hmin, havg, hmax, wmin, wavg, wmax, 1 H , 1 W ], where h * , w * \u2208 [\u22121, 1] are relative coordinates of the grid. H and W denotes the height and width of the whole spatial feature map.\nstp = [tmin, tavg, tmax, hmin, havg, hmax, wmin, wavg, wmax, 1 T , 1 H , 1 W ].\n\nInteractive video object segmentation in the wild. A Benard, M Gygli, arXiv:1801.002691arXiv preprintBenard, A., Gygli, M.: Interactive video object segmentation in the wild. arXiv preprint arXiv:1801.00269 (2017) 1\n\nOne-shot video object segmentation. S Caelles, K K Maninis, J Pont-Tuset, L Leal-Taix\u00e9, D Cremers, L Van Gool, 13Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taix\u00e9, L., Cremers, D., Van Gool, L.: One-shot video object segmentation. In: CVPR (2017) 1, 3\n\nS Caelles, A Montes, K K Maninis, Y Chen, L Van Gool, F Perazzi, J Pont-Tuset, arXiv:1803.00557The 2018 davis challenge on video object segmentation. 1arXiv preprintCaelles, S., Montes, A., Maninis, K.K., Chen, Y., Van Gool, L., Perazzi, F., Pont- Tuset, J.: The 2018 davis challenge on video object segmentation. arXiv preprint arXiv:1803.00557 (2018) 1\n\nBlazingly fast video object segmentation with pixel-wise metric learning. Y Chen, J Pont-Tuset, A Montes, L Van Gool, CVPR1Chen, Y., Pont-Tuset, J., Montes, A., Van Gool, L.: Blazingly fast video object segmentation with pixel-wise metric learning. In: CVPR (2018) 1\n\nHeterogeneous memory enhanced multimodal attention model for video question answering. C Fan, X Zhang, S Zhang, W Wang, C Zhang, H Huang, CVPR4Fan, C., Zhang, X., Zhang, S., Wang, W., Zhang, C., Huang, H.: Heterogeneous memory enhanced multimodal attention model for video question answering. In: CVPR (2019) 4\n\nActor and action video segmentation from a sentence. K Gavrilyuk, A Ghodrati, Z Li, C G Snoek, CVPR4Gavrilyuk, K., Ghodrati, A., Li, Z., Snoek, C.G.: Actor and action video segmen- tation from a sentence. In: CVPR (2018) 4\n\nUnsupervised video object segmentation for deep reinforcement learning. V Goel, J Weng, P Poupart, NIPS. 1Goel, V., Weng, J., Poupart, P.: Unsupervised video object segmentation for deep reinforcement learning. In: NIPS (2018) 1\n\nSegmentation from natural language expressions. R Hu, M Rohrbach, T Darrell, ECCV36Hu, R., Rohrbach, M., Darrell, T.: Segmentation from natural language expres- sions. In: ECCV (2016) 3, 6\n\nTowards understanding action recognition. H Jhuang, J Gall, S Zuffi, C Schmid, M J Black, CVPR4Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.: Towards understanding action recognition. In: CVPR (2013) 4\n\nVideo object segmentation with language referring expressions. A Khoreva, A Rohrbach, B Schiele, ACCV1213Khoreva, A., Rohrbach, A., Schiele, B.: Video object segmentation with language referring expressions. In: ACCV (2018) 2, 4, 10, 11, 12, 13\n\nPanoptic feature pyramid networks. A Kirillov, R Girshick, K He, P Doll\u00e1r, CVPR10Kirillov, A., Girshick, R., He, K., Doll\u00e1r, P.: Panoptic feature pyramid networks. In: CVPR (2019) 10\n\nReferring image segmentation via recurrent refinement networks. R Li, K Li, Y C Kuo, M Shu, X Qi, X Shen, J Jia, CVPR. 26Li, R., Li, K., Kuo, Y.C., Shu, M., Qi, X., Shen, X., Jia, J.: Referring image segmentation via recurrent refinement networks. In: CVPR (2018) 2, 3, 5, 6\n\nUnsupervised video object segmentation with motion-based bilateral networks. S Li, B Seybold, A Vorobyov, X Lei, C C Jay Kuo, ECCV3Li, S., Seybold, B., Vorobyov, A., Lei, X., Jay Kuo, C.C.: Unsupervised video object segmentation with motion-based bilateral networks. In: ECCV (2018) 3\n\nVideo object segmentation with joint re-identification and attention-aware mask propagation. X Li, C Loy, In: ECCV. 3Li, X., Change Loy, C.: Video object segmentation with joint re-identification and attention-aware mask propagation. In: ECCV (2018) 3\n\nTracking by natural language specification. Z Li, R Tao, E Gavves, C G Snoek, A W Smeulders, 4Li, Z., Tao, R., Gavves, E., Snoek, C.G., Smeulders, A.W.: Tracking by natural language specification. In: CVPR (2017) 4\n\nRecurrent multimodal interaction for referring image segmentation. C Liu, Z Lin, X Shen, J Yang, X Lu, A Yuille, Liu, C., Lin, Z., Shen, X., Yang, J., Lu, X., Yuille, A.: Recurrent multimodal interaction for referring image segmentation. In: ICCV (2017) 2\n\nDeep extreme cut: From extreme points to object segmentation. K K Maninis, S Caelles, J Pont-Tuset, L Van Gool, CVPRManinis, K.K., Caelles, S., Pont-Tuset, J., Van Gool, L.: Deep extreme cut: From extreme points to object segmentation. In: CVPR (2018)\n\nDynamic multimodal instance segmentation guided by natural language queries. E Margffoy-Tuay, J C P\u00e9rez, E Botero, P Arbel\u00e1ez, ECCV56Margffoy-Tuay, E., P\u00e9rez, J.C., Botero, E., Arbel\u00e1ez, P.: Dynamic multimodal instance segmentation guided by natural language queries. In: ECCV (2018) 5, 6\n\nStreamlined dense video captioning. J Mun, L Yang, Z Ren, N Xu, B Han, CVPR4Mun, J., Yang, L., Ren, Z., Xu, N., Han, B.: Streamlined dense video captioning. In: CVPR (2019) 4\n\nModeling context between objects for referring expression understanding. V K Nagaraja, V I Morariu, L S Davis, ECCV1113Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for referring expression understanding. In: ECCV (2016) 11, 13\n\nFast user-guided video object segmentation by interaction-and-propagation networks. S W Oh, J Y Lee, N Xu, S J Kim, CVPR1Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Fast user-guided video object segmentation by interaction-and-propagation networks. In: CVPR (2019) 1\n\nVideo object segmentation using space-time memory networks. S W Oh, J Y Lee, N Xu, S J Kim, ICCV (2019). 1, 3, 6, 7, 8Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-time memory networks. In: ICCV (2019) 1, 3, 6, 7, 8\n\nLibra r-cnn: Towards balanced learning for object detection. J Pang, K Chen, J Shi, H Feng, W Ouyang, D Lin, CVPR10Pang, J., Chen, K., Shi, J., Feng, H., Ouyang, W., Lin, D.: Libra r-cnn: Towards balanced learning for object detection. In: CVPR (2019) 10\n\nLearning video object segmentation from static images. F Perazzi, A Khoreva, R Benenson, B Schiele, A Sorkine-Hornung, CVPR. 13Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learning video object segmentation from static images. In: CVPR (2017) 1, 3\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, CVPR312Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine- Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: CVPR (2016) 3, 4, 12\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, CVPR11Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine- Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: CVPR (2016) 11\n\nLearning video object segmentation with visual memory. P Tokmakov, K Alahari, C Schmid, 3Tokmakov, P., Alahari, K., Schmid, C.: Learning video object segmentation with visual memory. In: ICCV (2017) 3\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 7Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NIPS (2017) 7\n\nFeelvos: Fast end-to-end embedding learning for video object segmentation. P Voigtlaender, Y Chai, F Schroff, H Adam, B Leibe, L C Chen, CVPR13Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: Feelvos: Fast end-to-end embedding learning for video object segmentation. In: CVPR (2019) 1, 3\n\nAsymmetric cross-guided attention network for actor and action video segmentation from natural language query. H Wang, C Deng, J Yan, D Tao, ICCV4Wang, H., Deng, C., Yan, J., Tao, D.: Asymmetric cross-guided attention network for actor and action video segmentation from natural language query. In: ICCV (2019) 4\n\nLearning unsupervised video object segmentation through visual attention. W Wang, H Song, S Zhao, J Shen, S Zhao, S C Hoi, H Ling, CVPR1Wang, W., Song, H., Zhao, S., Shen, J., Zhao, S., Hoi, S.C., Ling, H.: Learning unsupervised video object segmentation through visual attention. In: CVPR (2019) 1\n\nFast video object segmentation by reference-guided mask propagation. S Wug Oh, J Y Lee, K Sunkavalli, S Kim, CVPR. 16Wug Oh, S., Lee, J.Y., Sunkavalli, K., Joo Kim, S.: Fast video object segmentation by reference-guided mask propagation. In: CVPR (2018) 1, 3, 6\n\nCan humans fly? action understanding with multiple classes of actors. C Xu, S H Hsieh, C Xiong, J J Corso, CVPR4Xu, C., Hsieh, S.H., Xiong, C., Corso, J.J.: Can humans fly? action understanding with multiple classes of actors. In: CVPR (2015) 4\n\nYoutube-vos: Sequence-to-sequence video object segmentation. N Xu, L Yang, Y Fan, J Yang, D Yue, Y Liang, B Price, S Cohen, T Huang, In: ECCV. 312Xu, N., Yang, L., Fan, Y., Yang, J., Yue, D., Liang, Y., Price, B., Cohen, S., Huang, T.: Youtube-vos: Sequence-to-sequence video object segmentation. In: ECCV (2018) 3, 4, 12\n\nEfficient video object segmentation via network modulation. L Yang, Y Wang, X Xiong, J Yang, A K Katsaggelos, CVPR. 13Yang, L., Wang, Y., Xiong, X., Yang, J., Katsaggelos, A.K.: Efficient video object segmentation via network modulation. In: CVPR (2018) 1, 3\n\nCross-modal self-attention network for referring image segmentation. L Ye, M Rochan, Z Liu, Y Wang, CVPR712Ye, L., Rochan, M., Liu, Z., Wang, Y.: Cross-modal self-attention network for referring image segmentation. In: CVPR (2019) 2, 3, 5, 6, 7, 12\n\nMattnet: Modular attention network for referring expression comprehension. L Yu, Z Lin, X Shen, J Yang, X Lu, M Bansal, T L Berg, CVPR. 26Yu, L., Lin, Z., Shen, X., Yang, J., Lu, X., Bansal, M., Berg, T.L.: Mattnet: Mod- ular attention network for referring expression comprehension. In: CVPR (2018) 2, 3, 5, 6\n\nMotion-attentive transition for zero-shot video object segmentation. T Zhou, S Wang, Y Zhou, Y Yao, J Li, L Shao, AAAI3Zhou, T., Wang, S., Zhou, Y., Yao, Y., Li, J., Shao, L.: Motion-attentive transition for zero-shot video object segmentation. In: AAAI (2020) 3\n", "annotations": {"author": "[{\"end\":131,\"start\":91},{\"end\":164,\"start\":132},{\"end\":205,\"start\":165}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":99},{\"end\":146,\"start\":143},{\"end\":176,\"start\":173}]", "author_first_name": "[{\"end\":98,\"start\":91},{\"end\":142,\"start\":132},{\"end\":172,\"start\":165}]", "author_affiliation": "[{\"end\":130,\"start\":104},{\"end\":163,\"start\":148},{\"end\":204,\"start\":178}]", "title": "[{\"end\":88,\"start\":1},{\"end\":293,\"start\":206}]", "venue": null, "abstract": "[{\"end\":1108,\"start\":351}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1457,\"start\":1453},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1459,\"start\":1457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1993,\"start\":1990},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1996,\"start\":1993},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1999,\"start\":1996},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2002,\"start\":1999},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2005,\"start\":2002},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2008,\"start\":2005},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2256,\"start\":2253},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2258,\"start\":2256},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2260,\"start\":2258},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2263,\"start\":2260},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2884,\"start\":2880},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2887,\"start\":2884},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2890,\"start\":2887},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2893,\"start\":2890},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3648,\"start\":3644},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4553,\"start\":4549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5852,\"start\":5849},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5996,\"start\":5992},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6126,\"start\":6122},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6325,\"start\":6321},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6914,\"start\":6910},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6917,\"start\":6914},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6920,\"start\":6917},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7160,\"start\":7156},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7181,\"start\":7177},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7423,\"start\":7420},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7426,\"start\":7423},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7429,\"start\":7426},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7615,\"start\":7611},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7618,\"start\":7615},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7621,\"start\":7618},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7624,\"start\":7621},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7634,\"start\":7630},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8166,\"start\":8162},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8191,\"start\":8188},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8194,\"start\":8191},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8217,\"start\":8213},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8250,\"start\":8247},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8272,\"start\":8269},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8498,\"start\":8494},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8513,\"start\":8510},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8600,\"start\":8596},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8765,\"start\":8761},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8934,\"start\":8931},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8937,\"start\":8934},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9019,\"start\":9016},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9041,\"start\":9037},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9056,\"start\":9053},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9339,\"start\":9335},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9375,\"start\":9371},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9592,\"start\":9588},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11928,\"start\":11924},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11931,\"start\":11928},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11934,\"start\":11931},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11937,\"start\":11934},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12153,\"start\":12149},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12156,\"start\":12153},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12159,\"start\":12156},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12162,\"start\":12159},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12199,\"start\":12195},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12202,\"start\":12199},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12891,\"start\":12888},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13610,\"start\":13606},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14044,\"start\":14040},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15563,\"start\":15559},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16154,\"start\":16150},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18952,\"start\":18948},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19463,\"start\":19459},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21487,\"start\":21483},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22267,\"start\":22263},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22966,\"start\":22962},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23090,\"start\":23086},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23871,\"start\":23867},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24194,\"start\":24190},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24530,\"start\":24528},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29067,\"start\":29064},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30827,\"start\":30823},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30885,\"start\":30881},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30970,\"start\":30966},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31014,\"start\":31010}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27930,\"start\":27260},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28240,\"start\":27931},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28450,\"start\":28241},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28902,\"start\":28451},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28977,\"start\":28903},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29647,\"start\":28978},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30166,\"start\":29648},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30702,\"start\":30167},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31453,\"start\":30703},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31621,\"start\":31454},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31824,\"start\":31622}]", "paragraph": "[{\"end\":2341,\"start\":1124},{\"end\":2796,\"start\":2343},{\"end\":3862,\"start\":2798},{\"end\":4778,\"start\":3864},{\"end\":4819,\"start\":4780},{\"end\":4960,\"start\":4821},{\"end\":5109,\"start\":4962},{\"end\":5233,\"start\":5111},{\"end\":5398,\"start\":5250},{\"end\":5541,\"start\":5400},{\"end\":5843,\"start\":5543},{\"end\":6609,\"start\":5845},{\"end\":8010,\"start\":6611},{\"end\":8874,\"start\":8012},{\"end\":9843,\"start\":8904},{\"end\":11486,\"start\":9845},{\"end\":12203,\"start\":11539},{\"end\":12668,\"start\":12221},{\"end\":13185,\"start\":12687},{\"end\":13281,\"start\":13221},{\"end\":13857,\"start\":13283},{\"end\":14310,\"start\":13859},{\"end\":14550,\"start\":14371},{\"end\":14945,\"start\":14587},{\"end\":15177,\"start\":15054},{\"end\":15419,\"start\":15232},{\"end\":16113,\"start\":15421},{\"end\":16474,\"start\":16115},{\"end\":16852,\"start\":16476},{\"end\":17881,\"start\":17183},{\"end\":19464,\"start\":17883},{\"end\":19757,\"start\":19466},{\"end\":20220,\"start\":19759},{\"end\":21321,\"start\":20222},{\"end\":21591,\"start\":21337},{\"end\":22134,\"start\":21618},{\"end\":22542,\"start\":22157},{\"end\":22781,\"start\":22567},{\"end\":22837,\"start\":22783},{\"end\":22881,\"start\":22839},{\"end\":23839,\"start\":22883},{\"end\":24614,\"start\":23841},{\"end\":25002,\"start\":24638},{\"end\":25523,\"start\":25015},{\"end\":26562,\"start\":25525},{\"end\":27259,\"start\":26577}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13220,\"start\":13186},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14370,\"start\":14311},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14586,\"start\":14551},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15053,\"start\":14946},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15231,\"start\":15178},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16966,\"start\":16853},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17027,\"start\":16966},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17130,\"start\":17027},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17182,\"start\":17130}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11401,\"start\":11394},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22681,\"start\":22674},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23030,\"start\":23023},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23537,\"start\":23530},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24540,\"start\":24533},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25272,\"start\":25265},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26115,\"start\":26108},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26437,\"start\":26430}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1122,\"start\":1110},{\"attributes\":{\"n\":\"2\"},\"end\":5248,\"start\":5236},{\"attributes\":{\"n\":\"3\"},\"end\":8902,\"start\":8877},{\"end\":11505,\"start\":11489},{\"attributes\":{\"n\":\"4\"},\"end\":11537,\"start\":11508},{\"attributes\":{\"n\":\"4.1\"},\"end\":12219,\"start\":12206},{\"end\":12685,\"start\":12671},{\"attributes\":{\"n\":\"5\"},\"end\":21335,\"start\":21324},{\"attributes\":{\"n\":\"5.1\"},\"end\":21616,\"start\":21594},{\"attributes\":{\"n\":\"5.2\"},\"end\":22155,\"start\":22137},{\"attributes\":{\"n\":\"5.3\"},\"end\":22565,\"start\":22545},{\"attributes\":{\"n\":\"5.4\"},\"end\":24636,\"start\":24617},{\"attributes\":{\"n\":\"5.5\"},\"end\":25013,\"start\":25005},{\"attributes\":{\"n\":\"6\"},\"end\":26575,\"start\":26565},{\"end\":27940,\"start\":27932},{\"end\":28250,\"start\":28242},{\"end\":28460,\"start\":28452},{\"end\":28912,\"start\":28904},{\"end\":28988,\"start\":28979},{\"end\":29658,\"start\":29649},{\"end\":30177,\"start\":30168},{\"end\":30713,\"start\":30704},{\"end\":31464,\"start\":31455},{\"end\":31632,\"start\":31623}]", "table": "[{\"end\":29647,\"start\":29373},{\"end\":30166,\"start\":29767},{\"end\":30702,\"start\":30279},{\"end\":31453,\"start\":31335},{\"end\":31621,\"start\":31520},{\"end\":31824,\"start\":31703}]", "figure_caption": "[{\"end\":27930,\"start\":27262},{\"end\":28240,\"start\":27942},{\"end\":28450,\"start\":28252},{\"end\":28902,\"start\":28462},{\"end\":28977,\"start\":28914},{\"end\":29373,\"start\":28990},{\"end\":29767,\"start\":29660},{\"end\":30279,\"start\":30179},{\"end\":31335,\"start\":30715},{\"end\":31520,\"start\":31466},{\"end\":31703,\"start\":31634}]", "figure_ref": "[{\"end\":5647,\"start\":5641},{\"end\":11057,\"start\":11051},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12615,\"start\":12609},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14792,\"start\":14783},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17227,\"start\":17218},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18191,\"start\":18185},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24645,\"start\":24642}]", "bib_author_first_name": "[{\"end\":32289,\"start\":32288},{\"end\":32299,\"start\":32298},{\"end\":32491,\"start\":32490},{\"end\":32502,\"start\":32501},{\"end\":32504,\"start\":32503},{\"end\":32515,\"start\":32514},{\"end\":32529,\"start\":32528},{\"end\":32543,\"start\":32542},{\"end\":32554,\"start\":32553},{\"end\":32713,\"start\":32712},{\"end\":32724,\"start\":32723},{\"end\":32734,\"start\":32733},{\"end\":32736,\"start\":32735},{\"end\":32747,\"start\":32746},{\"end\":32755,\"start\":32754},{\"end\":32767,\"start\":32766},{\"end\":32778,\"start\":32777},{\"end\":33143,\"start\":33142},{\"end\":33151,\"start\":33150},{\"end\":33165,\"start\":33164},{\"end\":33175,\"start\":33174},{\"end\":33424,\"start\":33423},{\"end\":33431,\"start\":33430},{\"end\":33440,\"start\":33439},{\"end\":33449,\"start\":33448},{\"end\":33457,\"start\":33456},{\"end\":33466,\"start\":33465},{\"end\":33702,\"start\":33701},{\"end\":33715,\"start\":33714},{\"end\":33727,\"start\":33726},{\"end\":33733,\"start\":33732},{\"end\":33735,\"start\":33734},{\"end\":33945,\"start\":33944},{\"end\":33953,\"start\":33952},{\"end\":33961,\"start\":33960},{\"end\":34151,\"start\":34150},{\"end\":34157,\"start\":34156},{\"end\":34169,\"start\":34168},{\"end\":34335,\"start\":34334},{\"end\":34345,\"start\":34344},{\"end\":34353,\"start\":34352},{\"end\":34362,\"start\":34361},{\"end\":34372,\"start\":34371},{\"end\":34374,\"start\":34373},{\"end\":34570,\"start\":34569},{\"end\":34581,\"start\":34580},{\"end\":34593,\"start\":34592},{\"end\":34788,\"start\":34787},{\"end\":34800,\"start\":34799},{\"end\":34812,\"start\":34811},{\"end\":34818,\"start\":34817},{\"end\":35001,\"start\":35000},{\"end\":35007,\"start\":35006},{\"end\":35013,\"start\":35012},{\"end\":35015,\"start\":35014},{\"end\":35022,\"start\":35021},{\"end\":35029,\"start\":35028},{\"end\":35035,\"start\":35034},{\"end\":35043,\"start\":35042},{\"end\":35290,\"start\":35289},{\"end\":35296,\"start\":35295},{\"end\":35307,\"start\":35306},{\"end\":35319,\"start\":35318},{\"end\":35326,\"start\":35325},{\"end\":35328,\"start\":35327},{\"end\":35592,\"start\":35591},{\"end\":35598,\"start\":35597},{\"end\":35796,\"start\":35795},{\"end\":35802,\"start\":35801},{\"end\":35809,\"start\":35808},{\"end\":35819,\"start\":35818},{\"end\":35821,\"start\":35820},{\"end\":35830,\"start\":35829},{\"end\":35832,\"start\":35831},{\"end\":36035,\"start\":36034},{\"end\":36042,\"start\":36041},{\"end\":36049,\"start\":36048},{\"end\":36057,\"start\":36056},{\"end\":36065,\"start\":36064},{\"end\":36071,\"start\":36070},{\"end\":36287,\"start\":36286},{\"end\":36289,\"start\":36288},{\"end\":36300,\"start\":36299},{\"end\":36311,\"start\":36310},{\"end\":36325,\"start\":36324},{\"end\":36555,\"start\":36554},{\"end\":36572,\"start\":36571},{\"end\":36574,\"start\":36573},{\"end\":36583,\"start\":36582},{\"end\":36593,\"start\":36592},{\"end\":36804,\"start\":36803},{\"end\":36811,\"start\":36810},{\"end\":36819,\"start\":36818},{\"end\":36826,\"start\":36825},{\"end\":36832,\"start\":36831},{\"end\":37017,\"start\":37016},{\"end\":37019,\"start\":37018},{\"end\":37031,\"start\":37030},{\"end\":37033,\"start\":37032},{\"end\":37044,\"start\":37043},{\"end\":37046,\"start\":37045},{\"end\":37288,\"start\":37287},{\"end\":37290,\"start\":37289},{\"end\":37296,\"start\":37295},{\"end\":37298,\"start\":37297},{\"end\":37305,\"start\":37304},{\"end\":37311,\"start\":37310},{\"end\":37313,\"start\":37312},{\"end\":37528,\"start\":37527},{\"end\":37530,\"start\":37529},{\"end\":37536,\"start\":37535},{\"end\":37538,\"start\":37537},{\"end\":37545,\"start\":37544},{\"end\":37551,\"start\":37550},{\"end\":37553,\"start\":37552},{\"end\":37778,\"start\":37777},{\"end\":37786,\"start\":37785},{\"end\":37794,\"start\":37793},{\"end\":37801,\"start\":37800},{\"end\":37809,\"start\":37808},{\"end\":37819,\"start\":37818},{\"end\":38028,\"start\":38027},{\"end\":38039,\"start\":38038},{\"end\":38050,\"start\":38049},{\"end\":38062,\"start\":38061},{\"end\":38073,\"start\":38072},{\"end\":38329,\"start\":38328},{\"end\":38340,\"start\":38339},{\"end\":38354,\"start\":38353},{\"end\":38368,\"start\":38367},{\"end\":38380,\"start\":38379},{\"end\":38389,\"start\":38388},{\"end\":38689,\"start\":38688},{\"end\":38700,\"start\":38699},{\"end\":38714,\"start\":38713},{\"end\":38728,\"start\":38727},{\"end\":38740,\"start\":38739},{\"end\":38749,\"start\":38748},{\"end\":39019,\"start\":39018},{\"end\":39031,\"start\":39030},{\"end\":39042,\"start\":39041},{\"end\":39193,\"start\":39192},{\"end\":39204,\"start\":39203},{\"end\":39215,\"start\":39214},{\"end\":39225,\"start\":39224},{\"end\":39238,\"start\":39237},{\"end\":39247,\"start\":39246},{\"end\":39249,\"start\":39248},{\"end\":39258,\"start\":39257},{\"end\":39268,\"start\":39267},{\"end\":39509,\"start\":39508},{\"end\":39525,\"start\":39524},{\"end\":39533,\"start\":39532},{\"end\":39544,\"start\":39543},{\"end\":39552,\"start\":39551},{\"end\":39561,\"start\":39560},{\"end\":39563,\"start\":39562},{\"end\":39859,\"start\":39858},{\"end\":39867,\"start\":39866},{\"end\":39875,\"start\":39874},{\"end\":39882,\"start\":39881},{\"end\":40136,\"start\":40135},{\"end\":40144,\"start\":40143},{\"end\":40152,\"start\":40151},{\"end\":40160,\"start\":40159},{\"end\":40168,\"start\":40167},{\"end\":40176,\"start\":40175},{\"end\":40178,\"start\":40177},{\"end\":40185,\"start\":40184},{\"end\":40431,\"start\":40430},{\"end\":40441,\"start\":40440},{\"end\":40443,\"start\":40442},{\"end\":40450,\"start\":40449},{\"end\":40464,\"start\":40463},{\"end\":40695,\"start\":40694},{\"end\":40701,\"start\":40700},{\"end\":40703,\"start\":40702},{\"end\":40712,\"start\":40711},{\"end\":40721,\"start\":40720},{\"end\":40723,\"start\":40722},{\"end\":40932,\"start\":40931},{\"end\":40938,\"start\":40937},{\"end\":40946,\"start\":40945},{\"end\":40953,\"start\":40952},{\"end\":40961,\"start\":40960},{\"end\":40968,\"start\":40967},{\"end\":40977,\"start\":40976},{\"end\":40986,\"start\":40985},{\"end\":40995,\"start\":40994},{\"end\":41254,\"start\":41253},{\"end\":41262,\"start\":41261},{\"end\":41270,\"start\":41269},{\"end\":41279,\"start\":41278},{\"end\":41287,\"start\":41286},{\"end\":41289,\"start\":41288},{\"end\":41523,\"start\":41522},{\"end\":41529,\"start\":41528},{\"end\":41539,\"start\":41538},{\"end\":41546,\"start\":41545},{\"end\":41779,\"start\":41778},{\"end\":41785,\"start\":41784},{\"end\":41792,\"start\":41791},{\"end\":41800,\"start\":41799},{\"end\":41808,\"start\":41807},{\"end\":41814,\"start\":41813},{\"end\":41824,\"start\":41823},{\"end\":41826,\"start\":41825},{\"end\":42085,\"start\":42084},{\"end\":42093,\"start\":42092},{\"end\":42101,\"start\":42100},{\"end\":42109,\"start\":42108},{\"end\":42116,\"start\":42115},{\"end\":42122,\"start\":42121}]", "bib_author_last_name": "[{\"end\":32296,\"start\":32290},{\"end\":32305,\"start\":32300},{\"end\":32499,\"start\":32492},{\"end\":32512,\"start\":32505},{\"end\":32526,\"start\":32516},{\"end\":32540,\"start\":32530},{\"end\":32551,\"start\":32544},{\"end\":32563,\"start\":32555},{\"end\":32721,\"start\":32714},{\"end\":32731,\"start\":32725},{\"end\":32744,\"start\":32737},{\"end\":32752,\"start\":32748},{\"end\":32764,\"start\":32756},{\"end\":32775,\"start\":32768},{\"end\":32789,\"start\":32779},{\"end\":33148,\"start\":33144},{\"end\":33162,\"start\":33152},{\"end\":33172,\"start\":33166},{\"end\":33184,\"start\":33176},{\"end\":33428,\"start\":33425},{\"end\":33437,\"start\":33432},{\"end\":33446,\"start\":33441},{\"end\":33454,\"start\":33450},{\"end\":33463,\"start\":33458},{\"end\":33472,\"start\":33467},{\"end\":33712,\"start\":33703},{\"end\":33724,\"start\":33716},{\"end\":33730,\"start\":33728},{\"end\":33741,\"start\":33736},{\"end\":33950,\"start\":33946},{\"end\":33958,\"start\":33954},{\"end\":33969,\"start\":33962},{\"end\":34154,\"start\":34152},{\"end\":34166,\"start\":34158},{\"end\":34177,\"start\":34170},{\"end\":34342,\"start\":34336},{\"end\":34350,\"start\":34346},{\"end\":34359,\"start\":34354},{\"end\":34369,\"start\":34363},{\"end\":34380,\"start\":34375},{\"end\":34578,\"start\":34571},{\"end\":34590,\"start\":34582},{\"end\":34601,\"start\":34594},{\"end\":34797,\"start\":34789},{\"end\":34809,\"start\":34801},{\"end\":34815,\"start\":34813},{\"end\":34825,\"start\":34819},{\"end\":35004,\"start\":35002},{\"end\":35010,\"start\":35008},{\"end\":35019,\"start\":35016},{\"end\":35026,\"start\":35023},{\"end\":35032,\"start\":35030},{\"end\":35040,\"start\":35036},{\"end\":35047,\"start\":35044},{\"end\":35293,\"start\":35291},{\"end\":35304,\"start\":35297},{\"end\":35316,\"start\":35308},{\"end\":35323,\"start\":35320},{\"end\":35336,\"start\":35329},{\"end\":35595,\"start\":35593},{\"end\":35602,\"start\":35599},{\"end\":35799,\"start\":35797},{\"end\":35806,\"start\":35803},{\"end\":35816,\"start\":35810},{\"end\":35827,\"start\":35822},{\"end\":35842,\"start\":35833},{\"end\":36039,\"start\":36036},{\"end\":36046,\"start\":36043},{\"end\":36054,\"start\":36050},{\"end\":36062,\"start\":36058},{\"end\":36068,\"start\":36066},{\"end\":36078,\"start\":36072},{\"end\":36297,\"start\":36290},{\"end\":36308,\"start\":36301},{\"end\":36322,\"start\":36312},{\"end\":36334,\"start\":36326},{\"end\":36569,\"start\":36556},{\"end\":36580,\"start\":36575},{\"end\":36590,\"start\":36584},{\"end\":36602,\"start\":36594},{\"end\":36808,\"start\":36805},{\"end\":36816,\"start\":36812},{\"end\":36823,\"start\":36820},{\"end\":36829,\"start\":36827},{\"end\":36836,\"start\":36833},{\"end\":37028,\"start\":37020},{\"end\":37041,\"start\":37034},{\"end\":37052,\"start\":37047},{\"end\":37293,\"start\":37291},{\"end\":37302,\"start\":37299},{\"end\":37308,\"start\":37306},{\"end\":37317,\"start\":37314},{\"end\":37533,\"start\":37531},{\"end\":37542,\"start\":37539},{\"end\":37548,\"start\":37546},{\"end\":37557,\"start\":37554},{\"end\":37783,\"start\":37779},{\"end\":37791,\"start\":37787},{\"end\":37798,\"start\":37795},{\"end\":37806,\"start\":37802},{\"end\":37816,\"start\":37810},{\"end\":37823,\"start\":37820},{\"end\":38036,\"start\":38029},{\"end\":38047,\"start\":38040},{\"end\":38059,\"start\":38051},{\"end\":38070,\"start\":38063},{\"end\":38089,\"start\":38074},{\"end\":38337,\"start\":38330},{\"end\":38351,\"start\":38341},{\"end\":38365,\"start\":38355},{\"end\":38377,\"start\":38369},{\"end\":38386,\"start\":38381},{\"end\":38405,\"start\":38390},{\"end\":38697,\"start\":38690},{\"end\":38711,\"start\":38701},{\"end\":38725,\"start\":38715},{\"end\":38737,\"start\":38729},{\"end\":38746,\"start\":38741},{\"end\":38765,\"start\":38750},{\"end\":39028,\"start\":39020},{\"end\":39039,\"start\":39032},{\"end\":39049,\"start\":39043},{\"end\":39201,\"start\":39194},{\"end\":39212,\"start\":39205},{\"end\":39222,\"start\":39216},{\"end\":39235,\"start\":39226},{\"end\":39244,\"start\":39239},{\"end\":39255,\"start\":39250},{\"end\":39265,\"start\":39259},{\"end\":39279,\"start\":39269},{\"end\":39522,\"start\":39510},{\"end\":39530,\"start\":39526},{\"end\":39541,\"start\":39534},{\"end\":39549,\"start\":39545},{\"end\":39558,\"start\":39553},{\"end\":39568,\"start\":39564},{\"end\":39864,\"start\":39860},{\"end\":39872,\"start\":39868},{\"end\":39879,\"start\":39876},{\"end\":39886,\"start\":39883},{\"end\":40141,\"start\":40137},{\"end\":40149,\"start\":40145},{\"end\":40157,\"start\":40153},{\"end\":40165,\"start\":40161},{\"end\":40173,\"start\":40169},{\"end\":40182,\"start\":40179},{\"end\":40190,\"start\":40186},{\"end\":40438,\"start\":40432},{\"end\":40447,\"start\":40444},{\"end\":40461,\"start\":40451},{\"end\":40468,\"start\":40465},{\"end\":40698,\"start\":40696},{\"end\":40709,\"start\":40704},{\"end\":40718,\"start\":40713},{\"end\":40729,\"start\":40724},{\"end\":40935,\"start\":40933},{\"end\":40943,\"start\":40939},{\"end\":40950,\"start\":40947},{\"end\":40958,\"start\":40954},{\"end\":40965,\"start\":40962},{\"end\":40974,\"start\":40969},{\"end\":40983,\"start\":40978},{\"end\":40992,\"start\":40987},{\"end\":41001,\"start\":40996},{\"end\":41259,\"start\":41255},{\"end\":41267,\"start\":41263},{\"end\":41276,\"start\":41271},{\"end\":41284,\"start\":41280},{\"end\":41301,\"start\":41290},{\"end\":41526,\"start\":41524},{\"end\":41536,\"start\":41530},{\"end\":41543,\"start\":41540},{\"end\":41551,\"start\":41547},{\"end\":41782,\"start\":41780},{\"end\":41789,\"start\":41786},{\"end\":41797,\"start\":41793},{\"end\":41805,\"start\":41801},{\"end\":41811,\"start\":41809},{\"end\":41821,\"start\":41815},{\"end\":41831,\"start\":41827},{\"end\":42090,\"start\":42086},{\"end\":42098,\"start\":42094},{\"end\":42106,\"start\":42102},{\"end\":42113,\"start\":42110},{\"end\":42119,\"start\":42117},{\"end\":42127,\"start\":42123}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1801.00269\",\"id\":\"b0\"},\"end\":32452,\"start\":32237},{\"attributes\":{\"id\":\"b1\"},\"end\":32710,\"start\":32454},{\"attributes\":{\"doi\":\"arXiv:1803.00557\",\"id\":\"b2\"},\"end\":33066,\"start\":32712},{\"attributes\":{\"id\":\"b3\"},\"end\":33334,\"start\":33068},{\"attributes\":{\"id\":\"b4\"},\"end\":33646,\"start\":33336},{\"attributes\":{\"id\":\"b5\"},\"end\":33870,\"start\":33648},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":29152511},\"end\":34100,\"start\":33872},{\"attributes\":{\"id\":\"b7\"},\"end\":34290,\"start\":34102},{\"attributes\":{\"id\":\"b8\"},\"end\":34504,\"start\":34292},{\"attributes\":{\"id\":\"b9\"},\"end\":34750,\"start\":34506},{\"attributes\":{\"id\":\"b10\"},\"end\":34934,\"start\":34752},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":52831465},\"end\":35210,\"start\":34936},{\"attributes\":{\"id\":\"b12\"},\"end\":35496,\"start\":35212},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3844018},\"end\":35749,\"start\":35498},{\"attributes\":{\"id\":\"b14\"},\"end\":35965,\"start\":35751},{\"attributes\":{\"id\":\"b15\"},\"end\":36222,\"start\":35967},{\"attributes\":{\"id\":\"b16\"},\"end\":36475,\"start\":36224},{\"attributes\":{\"id\":\"b17\"},\"end\":36765,\"start\":36477},{\"attributes\":{\"id\":\"b18\"},\"end\":36941,\"start\":36767},{\"attributes\":{\"id\":\"b19\"},\"end\":37201,\"start\":36943},{\"attributes\":{\"id\":\"b20\"},\"end\":37465,\"start\":37203},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":90262243},\"end\":37714,\"start\":37467},{\"attributes\":{\"id\":\"b22\"},\"end\":37970,\"start\":37716},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3155322},\"end\":38248,\"start\":37972},{\"attributes\":{\"id\":\"b24\"},\"end\":38608,\"start\":38250},{\"attributes\":{\"id\":\"b25\"},\"end\":38961,\"start\":38610},{\"attributes\":{\"id\":\"b26\"},\"end\":39163,\"start\":38963},{\"attributes\":{\"id\":\"b27\"},\"end\":39431,\"start\":39165},{\"attributes\":{\"id\":\"b28\"},\"end\":39745,\"start\":39433},{\"attributes\":{\"id\":\"b29\"},\"end\":40059,\"start\":39747},{\"attributes\":{\"id\":\"b30\"},\"end\":40359,\"start\":40061},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":52847087},\"end\":40622,\"start\":40361},{\"attributes\":{\"id\":\"b32\"},\"end\":40868,\"start\":40624},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52154988},\"end\":41191,\"start\":40870},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3655063},\"end\":41451,\"start\":41193},{\"attributes\":{\"id\":\"b35\"},\"end\":41701,\"start\":41453},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3441497},\"end\":42013,\"start\":41703},{\"attributes\":{\"id\":\"b37\"},\"end\":42277,\"start\":42015}]", "bib_title": "[{\"end\":33942,\"start\":33872},{\"end\":34998,\"start\":34936},{\"end\":35589,\"start\":35498},{\"end\":37525,\"start\":37467},{\"end\":38025,\"start\":37972},{\"end\":40428,\"start\":40361},{\"end\":40929,\"start\":40870},{\"end\":41251,\"start\":41193},{\"end\":41776,\"start\":41703}]", "bib_author": "[{\"end\":32298,\"start\":32288},{\"end\":32307,\"start\":32298},{\"end\":32501,\"start\":32490},{\"end\":32514,\"start\":32501},{\"end\":32528,\"start\":32514},{\"end\":32542,\"start\":32528},{\"end\":32553,\"start\":32542},{\"end\":32565,\"start\":32553},{\"end\":32723,\"start\":32712},{\"end\":32733,\"start\":32723},{\"end\":32746,\"start\":32733},{\"end\":32754,\"start\":32746},{\"end\":32766,\"start\":32754},{\"end\":32777,\"start\":32766},{\"end\":32791,\"start\":32777},{\"end\":33150,\"start\":33142},{\"end\":33164,\"start\":33150},{\"end\":33174,\"start\":33164},{\"end\":33186,\"start\":33174},{\"end\":33430,\"start\":33423},{\"end\":33439,\"start\":33430},{\"end\":33448,\"start\":33439},{\"end\":33456,\"start\":33448},{\"end\":33465,\"start\":33456},{\"end\":33474,\"start\":33465},{\"end\":33714,\"start\":33701},{\"end\":33726,\"start\":33714},{\"end\":33732,\"start\":33726},{\"end\":33743,\"start\":33732},{\"end\":33952,\"start\":33944},{\"end\":33960,\"start\":33952},{\"end\":33971,\"start\":33960},{\"end\":34156,\"start\":34150},{\"end\":34168,\"start\":34156},{\"end\":34179,\"start\":34168},{\"end\":34344,\"start\":34334},{\"end\":34352,\"start\":34344},{\"end\":34361,\"start\":34352},{\"end\":34371,\"start\":34361},{\"end\":34382,\"start\":34371},{\"end\":34580,\"start\":34569},{\"end\":34592,\"start\":34580},{\"end\":34603,\"start\":34592},{\"end\":34799,\"start\":34787},{\"end\":34811,\"start\":34799},{\"end\":34817,\"start\":34811},{\"end\":34827,\"start\":34817},{\"end\":35006,\"start\":35000},{\"end\":35012,\"start\":35006},{\"end\":35021,\"start\":35012},{\"end\":35028,\"start\":35021},{\"end\":35034,\"start\":35028},{\"end\":35042,\"start\":35034},{\"end\":35049,\"start\":35042},{\"end\":35295,\"start\":35289},{\"end\":35306,\"start\":35295},{\"end\":35318,\"start\":35306},{\"end\":35325,\"start\":35318},{\"end\":35338,\"start\":35325},{\"end\":35597,\"start\":35591},{\"end\":35604,\"start\":35597},{\"end\":35801,\"start\":35795},{\"end\":35808,\"start\":35801},{\"end\":35818,\"start\":35808},{\"end\":35829,\"start\":35818},{\"end\":35844,\"start\":35829},{\"end\":36041,\"start\":36034},{\"end\":36048,\"start\":36041},{\"end\":36056,\"start\":36048},{\"end\":36064,\"start\":36056},{\"end\":36070,\"start\":36064},{\"end\":36080,\"start\":36070},{\"end\":36299,\"start\":36286},{\"end\":36310,\"start\":36299},{\"end\":36324,\"start\":36310},{\"end\":36336,\"start\":36324},{\"end\":36571,\"start\":36554},{\"end\":36582,\"start\":36571},{\"end\":36592,\"start\":36582},{\"end\":36604,\"start\":36592},{\"end\":36810,\"start\":36803},{\"end\":36818,\"start\":36810},{\"end\":36825,\"start\":36818},{\"end\":36831,\"start\":36825},{\"end\":36838,\"start\":36831},{\"end\":37030,\"start\":37016},{\"end\":37043,\"start\":37030},{\"end\":37054,\"start\":37043},{\"end\":37295,\"start\":37287},{\"end\":37304,\"start\":37295},{\"end\":37310,\"start\":37304},{\"end\":37319,\"start\":37310},{\"end\":37535,\"start\":37527},{\"end\":37544,\"start\":37535},{\"end\":37550,\"start\":37544},{\"end\":37559,\"start\":37550},{\"end\":37785,\"start\":37777},{\"end\":37793,\"start\":37785},{\"end\":37800,\"start\":37793},{\"end\":37808,\"start\":37800},{\"end\":37818,\"start\":37808},{\"end\":37825,\"start\":37818},{\"end\":38038,\"start\":38027},{\"end\":38049,\"start\":38038},{\"end\":38061,\"start\":38049},{\"end\":38072,\"start\":38061},{\"end\":38091,\"start\":38072},{\"end\":38339,\"start\":38328},{\"end\":38353,\"start\":38339},{\"end\":38367,\"start\":38353},{\"end\":38379,\"start\":38367},{\"end\":38388,\"start\":38379},{\"end\":38407,\"start\":38388},{\"end\":38699,\"start\":38688},{\"end\":38713,\"start\":38699},{\"end\":38727,\"start\":38713},{\"end\":38739,\"start\":38727},{\"end\":38748,\"start\":38739},{\"end\":38767,\"start\":38748},{\"end\":39030,\"start\":39018},{\"end\":39041,\"start\":39030},{\"end\":39051,\"start\":39041},{\"end\":39203,\"start\":39192},{\"end\":39214,\"start\":39203},{\"end\":39224,\"start\":39214},{\"end\":39237,\"start\":39224},{\"end\":39246,\"start\":39237},{\"end\":39257,\"start\":39246},{\"end\":39267,\"start\":39257},{\"end\":39281,\"start\":39267},{\"end\":39524,\"start\":39508},{\"end\":39532,\"start\":39524},{\"end\":39543,\"start\":39532},{\"end\":39551,\"start\":39543},{\"end\":39560,\"start\":39551},{\"end\":39570,\"start\":39560},{\"end\":39866,\"start\":39858},{\"end\":39874,\"start\":39866},{\"end\":39881,\"start\":39874},{\"end\":39888,\"start\":39881},{\"end\":40143,\"start\":40135},{\"end\":40151,\"start\":40143},{\"end\":40159,\"start\":40151},{\"end\":40167,\"start\":40159},{\"end\":40175,\"start\":40167},{\"end\":40184,\"start\":40175},{\"end\":40192,\"start\":40184},{\"end\":40440,\"start\":40430},{\"end\":40449,\"start\":40440},{\"end\":40463,\"start\":40449},{\"end\":40470,\"start\":40463},{\"end\":40700,\"start\":40694},{\"end\":40711,\"start\":40700},{\"end\":40720,\"start\":40711},{\"end\":40731,\"start\":40720},{\"end\":40937,\"start\":40931},{\"end\":40945,\"start\":40937},{\"end\":40952,\"start\":40945},{\"end\":40960,\"start\":40952},{\"end\":40967,\"start\":40960},{\"end\":40976,\"start\":40967},{\"end\":40985,\"start\":40976},{\"end\":40994,\"start\":40985},{\"end\":41003,\"start\":40994},{\"end\":41261,\"start\":41253},{\"end\":41269,\"start\":41261},{\"end\":41278,\"start\":41269},{\"end\":41286,\"start\":41278},{\"end\":41303,\"start\":41286},{\"end\":41528,\"start\":41522},{\"end\":41538,\"start\":41528},{\"end\":41545,\"start\":41538},{\"end\":41553,\"start\":41545},{\"end\":41784,\"start\":41778},{\"end\":41791,\"start\":41784},{\"end\":41799,\"start\":41791},{\"end\":41807,\"start\":41799},{\"end\":41813,\"start\":41807},{\"end\":41823,\"start\":41813},{\"end\":41833,\"start\":41823},{\"end\":42092,\"start\":42084},{\"end\":42100,\"start\":42092},{\"end\":42108,\"start\":42100},{\"end\":42115,\"start\":42108},{\"end\":42121,\"start\":42115},{\"end\":42129,\"start\":42121}]", "bib_venue": "[{\"end\":37585,\"start\":37572},{\"end\":32286,\"start\":32237},{\"end\":32488,\"start\":32454},{\"end\":32860,\"start\":32807},{\"end\":33140,\"start\":33068},{\"end\":33421,\"start\":33336},{\"end\":33699,\"start\":33648},{\"end\":33975,\"start\":33971},{\"end\":34148,\"start\":34102},{\"end\":34332,\"start\":34292},{\"end\":34567,\"start\":34506},{\"end\":34785,\"start\":34752},{\"end\":35053,\"start\":35049},{\"end\":35287,\"start\":35212},{\"end\":35612,\"start\":35604},{\"end\":35793,\"start\":35751},{\"end\":36032,\"start\":35967},{\"end\":36284,\"start\":36224},{\"end\":36552,\"start\":36477},{\"end\":36801,\"start\":36767},{\"end\":37014,\"start\":36943},{\"end\":37285,\"start\":37203},{\"end\":37570,\"start\":37559},{\"end\":37775,\"start\":37716},{\"end\":38095,\"start\":38091},{\"end\":38326,\"start\":38250},{\"end\":38686,\"start\":38610},{\"end\":39016,\"start\":38963},{\"end\":39190,\"start\":39165},{\"end\":39506,\"start\":39433},{\"end\":39856,\"start\":39747},{\"end\":40133,\"start\":40061},{\"end\":40474,\"start\":40470},{\"end\":40692,\"start\":40624},{\"end\":41011,\"start\":41003},{\"end\":41307,\"start\":41303},{\"end\":41520,\"start\":41453},{\"end\":41837,\"start\":41833},{\"end\":42082,\"start\":42015}]"}}}, "year": 2023, "month": 12, "day": 17}