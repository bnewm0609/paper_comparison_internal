{"id": 211069632, "updated": "2023-10-06 19:29:17.016", "metadata": {"title": "Certified Robustness to Label-Flipping Attacks via Randomized Smoothing", "authors": "[{\"first\":\"Elan\",\"last\":\"Rosenfeld\",\"middle\":[]},{\"first\":\"Ezra\",\"last\":\"Winston\",\"middle\":[]},{\"first\":\"Pradeep\",\"last\":\"Ravikumar\",\"middle\":[]},{\"first\":\"J.\",\"last\":\"Kolter\",\"middle\":[\"Zico\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 7}, "abstract": "Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. While many heuristic defenses have been proposed, few defenses exist which are certified against worst-case corruption of the training data. In this work, we propose a strategy to build linear classifiers that are certifiably robust against a strong variant of label-flipping, where each test example is targeted independently. In other words, for each test point, our classifier makes a prediction and includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier. We derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with no additional runtime cost over standard classification. We generalize our results to the multi-class case, providing what we believe to be the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2002.03018", "mag": "3035419960", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/RosenfeldWRK20", "doi": null}}, "content": {"source": {"pdf_hash": "68c490a6f44fec270674683f3b21f4f725af8123", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.03018v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "15817310dfa0f3bf85ce940f99e6784a99b80519", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/68c490a6f44fec270674683f3b21f4f725af8123.txt", "contents": "\nCertified Robustness to Label-Flipping Attacks via Randomized Smoothing\n\n\nElan Rosenfeld \nEzra Winston \nPradeep Ravikumar \nJ Zico Kolter \nCertified Robustness to Label-Flipping Attacks via Randomized Smoothing\n\nMachine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. While many heuristic defenses have been proposed, few defenses exist which are certified against worst-case corruption of the training data. In this work, we propose a strategy to build linear classifiers that are certifiably robust against a strong variant of label-flipping, where each test example is targeted independently. In other words, for each test point, our classifier makes a prediction and includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee-with high probabilitytest-time robustness to adversarial manipulation of the input to a classifier. We derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with no additional runtime cost over standard classification. We generalize our results to the multi-class case, providing what we believe to be the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.\n\nIntroduction\n\nModern classifiers, despite their widespread empirical success, are known to be susceptible to adversarial attacks. In this paper, we are specifically concerned with so-called \"data-poisoning\" attacks (formally, causative attacks [ Barreno et al. 2006;Papernot et al. 2018]), where the attacker manipulates some aspects of the training data in order to cause the learning algorithm to output a faulty classifier. Automated machine-learning systems which rely on large, user-generated datasets-e.g. email spam filters, product recommendation engines, and fake review detectors-are particularly susceptible to such attacks. For example, by maliciously flagging legitimate emails as spam and mislabeling spam as innocuous, an adversary can trick a spam filter into mistakenly letting through a particular email.\n\nTypes of data poisoning attacks which have been studied include label-flipping attacks (Xiao et al., 2012), where the labels of a training set can be adversarially manipulated to decrease performance of the trained classifier; general data poisoning, where both the training inputs and labels can be manipulated (Steinhardt et al., 2017); and backdoor attacks Tran et al., 2018), where the training set is corrupted so as to cause the classifier to deviate from its expected behavior only when triggered by a specific pattern. However, unlike the alternative \"test-time\" adversarial setting, where reasonably effective provable defenses exist to build adversarially robust classifiers, comparatively little work has been done on building classifiers that are certifiably robust to targeted data poisoning attacks.\n\nIn this work, we propose a strategy for building classifiers that are certifiably robust against label-flipping attacks. In particular, we propose a pointwise certified defense-by this we mean that with each prediction, the classifier includes a certification guaranteeing that its prediction would not be different had it been trained on data with some number of labels flipped. Prior works on certified defenses make statistical guarantees over the entire test set, but they make no guarantees as to the robustness of a prediction on any particular test point. Thus, with these algorithms, a determined adversary could still cause a specific test point to be misclassified. We therefore consider the threat of a worst-case adversary that can make a training set perturbation to target each test point individually. This motivates a defense that can certify each of its individual predictions, as we present here. To the best of our knowledge, this work represents the first pointwise certified defense to data poisoning attacks.\n\nOur approach leverages randomized smoothing (Cohen et al., 2019), a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a deep network. However, where prior uses of randomized smoothing randomize over the input to the classifier for test-time guarantees, we instead randomize arXiv:2002.03018v1 [cs.LG] 7 Feb 2020 over the entire training procedure of the classifier. Specifically, by randomizing over the labels during this training process, we obtain an overall classification pipeline that is certified to be robust (i.e., to not change its prediction) when some number of labels are adversarially manipulated in the training set. Whereas previous applications of randomized smoothing perform Monte Carlo sampling to provide probabilistic bounds (due to the intractability of integrating the decision regions of a deep network), we derive an analytical bound that provides truly guaranteed robustness. Although a naive implementation of this approach would not be computationally feasible, we show that by using a linear least-squares classifier we can obtain these certified bounds with no additional runtime cost over standard classification.\n\nA further distinction of our approach is that the applicability of our robustness guarantees do not rely upon stringent model assumptions or the quality of the features. Existing work on robust linear classification or regression provides certificates that only hold under specific model assumptions, e.g., recovering the best-fit linear coefficients, which is most useful when the data exhibit a linear relationship in the feature space. In contrast, our classifier makes no assumptions about the separability of the data or quality of the features; this means our certificates remain valid when applying our classifier to arbitrary features, which in practice allows us to leverage advances in unsupervised feature learning (Le, 2013) and transfer learning (Donahue et al., 2014). As an example, we apply our classifier to pre-trained and unsupervised deep features to demonstrate its feasibility for classification of highly non-linear data such as ImageNet.\n\nWe evaluate our proposed classifier on several benchmark datasets common to the data poisoning literature. Specifically, we demonstrate that our randomized classifier is able to achieve 75.7% certified accuracy on MNIST 1/7 even when the number of allowed label flips would drive a standard, undefended classifier to an accuracy of less than 50%. Similar results in experiments on the Dogfish binary classification challenge from ImageNet validate our technique for more challenging datasets-our binary classifier maintains 81.3% certified accuracy in the face of an adversary who could reduce an undefended classifier to less than 1%. We further experiment on the full MNIST dataset to demonstrate our algorithm's effectiveness for multi-class classification. Moreover, our classifier maintains a reasonably competitive non-robust accuracy (e.g., 94.5% on MNIST 1/7 compared to 99.1% for the undefended classifier).\n\n\nRelated Work\n\nData-poisoning attacks. A data-poisoning attack (Mu\u00f1oz Gonz\u00e1lez et al., 2017;Yang et al., 2017) is an attack where an adversary corrupts some portion of the training set or adds new inputs, with the goal of degrading the performance of the learned model. The attack can be targeted to cause poor performance on a specific test example or can simply reduce the overall test performance. The adversary is assumed to have perfect knowledge of the learning algorithm, so security by design-as opposed to obscurity-is the only viable defense against such attacks. The adversary is also typically assumed to have access to the training set and, in some cases, the test set.\n\nPrevious work has investigated attacks and defenses for data-poisoning attacks applied to feature selection (Xiao et al., 2015), SVMs (Biggio et al., 2011;Xiao et al., 2012), linear regression , and PCA (Rubinstein et al., 2009), to name a few. Some attacks can even achieve success with \"clean-label\" attacks, inserting adversarially perturbed, seemingly correctly labeled training examples that cause the classifier to perform poorly (Shafahi et al., 2018;Zhu et al., 2019). Interestingly, our defense can also be viewed as (the first) certified defense to such attacks: perturbing an image such that the model's learned features no longer match the label is theoretically equivalent to changing the label such that it no longer matches the image. For an overview of data poisoning attacks and defenses in machine learning, see Biggio et al. (2014).\n\nLabel-flipping attacks. A label-flipping attack is a specific type of data-poisoning attack where the adversary is restricted to changing the training labels. The classifier is then trained on the corrupted training set, with no knowledge of which labels have been tampered with. For example, an adversary could mislabel spam emails as innocuous, or flag real product reviews as fake.\n\nUnlike random label noise, for which many robust learning algorithms have been successfully developed (Natarajan et al., 2013;Liu & Tao, 2016;Patrini et al., 2017), adversarial label-flipping attacks can be specifically targeted to exploit the structure of the learning algorithm, significantly degrading performance. Robustness to such attacks is therefore harder to achieve, both theoretically and empirically (Xiao et al., 2012;Biggio et al., 2011). A common defense technique is sanitization, whereby a defender attempts to identify and remove or relabel training points that may have had their labels corrupted (Paudice et al., 2019;Taheri et al., 2019). Unfortunately, recent work has demonstrated that this is often not enough against a sufficiently powerful adversary (Koh et al., 2018). Further, no existing defenses provide pointwise guarantees regarding their robustness.\n\nCertified defenses. Existing works on certified defenses to adversarial data poisoning attacks typically focus on the regression case and provide broad statistical guarantees over the entire test distribution. A common approach to such certifications is to show that a particular algorithm recovers some close approximation to the best linear fit coefficients (Diakonikolas et al., 2019;Prasad et al., 2018;Shen & Sanghavi, 2019), or that the expected loss on the test distribution is bounded (Klivans et al., 2018;Chen & Paschalidis, 2018). These results generally rely on assumptions on the data distribution: some assume sparsity in the coefficients (Karmalkar & Price, 2018;Chen et al., 2013) or corruption vector (Bhatia et al., 2015); others require limited effects of outliers (Steinhardt et al., 2017). As mentioned above, all of these methods fail to provide guarantees for individual test points. Additionally, most of these statistical guarantees are not as meaningful when their model assumptions do not hold.\n\nRandomized smoothing. Since the discovery of adversarial examples (Szegedy et al., 2013;Goodfellow et al., 2015), the research community has been investigating techniques for increasing the adversarial robustness of complex models such as deep networks. After a series of heuristic defenses (Metzen et al., 2017;Feinman et al., 2017), followed by attacks breaking them (Athalye et al., 2018;Carlini & Wagner, 2017), focus began to shift towards the development of provable robustness.\n\nOne approach which has gained popularity in recent work is randomized smoothing. Rather than certifying the original classifier f , randomized smoothing defines a new classifier g whose prediction at an input x is the class assigned the most probability when x is perturbed with noise from some distribution \u00b5 and passed through f . That is, g(x) = arg max c P \u223c\u00b5 (f (x + ) = c). This new classifier g is then certified as robust, ideally without sacrificing too much accuracy compared to f . The original formulation was presented by Lecuyer et al. (2018) and borrowed ideas from differential privacy. The above definition is due to Li et al. (2018) and was popularized by Cohen et al. (2019), who derived a tight robustness guarantee. Follow-up work has focused on optimizing the training procedure of f (Salman et al., 2019) and extending the analysis to other types of distributions (Lee et al., 2019). For more details, see Cohen et al. (2019).\n\n\nA General View of Randomized Smoothing\n\nWe begin by presenting a general viewpoint of randomized smoothing. Under our notation, randomized smoothing constructs an operator G(\u00b5, \u03c6) that maps a binary-valued 1 function \u03c6 : X \u2192 {0, 1} and a smoothing measure \u00b5 : X \u2192 R + , with X \u00b5(x)dx = 1, to the expected value of \u03c6 under \u00b5 (that is, G(\u00b5, \u03c6) represents the \"vote\" of \u03c6 weighted by \u00b5). For example, \u03c6 could be a binary image classifier and \u00b5 could be some small, random pixel noise applied to the to-be-classified image. We also define a \"hard threshold\" version g(\u00b5, \u03c6) that returns the most probable output (the majority vote winner). Formally,\nG(\u00b5, \u03c6) = E x\u223c\u00b5 [\u03c6(x)] = X \u00b5(x)\u03c6(x)dx, g(\u00b5, \u03c6) = 1{G(\u00b5, \u03c6) \u2265 1/2},\nwhere 1{\u00b7} is the indicator function. Where it is clear from context, we will omit the arguments, writing simply G or g. Intuitively, for two similar measures \u00b5, \u03c1, we would expect that for most \u03c6, even though G(\u00b5, \u03c6) and G(\u03c1, \u03c6) may not be equal, the threshold function g should satisfy g(\u00b5, \u03c6) = g(\u03c1, \u03c6). Further, the degree to which \u00b5 and \u03c1 can differ while still preserving this property should increase as G(\u00b5, \u03c6) approaches either 0 or 1, because this increases the \"margin\" with which the function \u03c6 is 0 or 1 respectively over the measure \u00b5. More formally, we define a general randomized smoothing guarantee as follows: Definition 1. Let \u00b5 : X \u2192 R + be a smoothing measure over X , with X \u00b5(x)dx = 1. Then a randomized smoothing robustness guarantee is a specification of a distance measure d(\u00b5, \u03c1) and a function f\n: [0, 1] \u2192 R + such that for all \u03c6 : X \u2192 {0, 1}, g(\u00b5, \u03c6) = g(\u00b5, \u03c1) whenever d(\u00b5, \u03c1) \u2264 f (G(\u00b5, \u03c6)). (1)\nFor brevity, we will sometimes use p in place of G(\u00b5, \u03c6), representing the fraction of the vote that the majority class receives (this is analogous to p A in Cohen et al. (2019)).\n\nInstantiations of randomized smoothing This definition is rather abstract, so we highlight concrete examples of how it can be applied to achieve certified guarantees against adversarial attacks. Example 1. The randomized smoothing guarantee of Cohen et al. (2019) uses the smoothing measures \u00b5 = N (x 0 , \u03c3 2 I), a Gaussian aroound the point x 0 to be classified, and \u03c1 = N (x 0 + \u03b4, \u03c3 2 I), the same measure perturbed by \u03b4. They prove that (1) holds for all classifiers \u03c6 if we define\nd(\u00b5, \u03c1) = 1 \u03c3 \u03b4 2 \u2261 2KL(\u00b5 \u03c1), f (p) = |\u03a6 \u22121 (p)|,\nwhere KL(\u00b7) denotes KL divergence and \u03a6 \u22121 denotes the inverse CDF of the Gaussian distribution.\n\nAlthough this work focused on the case of randomized smoothing of continuous data via Gaussian noise, this is by no means a requirement of the approach. For instance, Lee et al. (2019) considers an alternative approach for dealing with discrete variables.\n\nExample 2. The randomized smoothing guarantee of Lee et al. (2019) uses the factorized smoothing measure in d dimensions\n\u00b5 \u03b1,K (x) = \u03a0 d i=1 \u00b5 \u03b1,K,i (x i ), defined with re- spect to parameters \u03b1 \u2208 [0, 1], K \u2208 N, and a base input z \u2208 {0, . . . , K} d , where \u00b5 \u03b1,K,i (x i ) = \u03b1, if x i = z i 1\u2212\u03b1 K , if x i \u2208 {0, . . . , K}, x i = z i , with x i being the i th dimension of x. \u03c1 \u03b1,K is similarly defined for a perturbed input z . They guarantee that (1) holds if we define d(\u00b5, \u03c1) = r def = z \u2212z 0 , f (p) = F \u03b1,K,d (max(p, 1\u2212p)).(2)\nIn words, the smoothing distribution is such that each dimension is independently perturbed to one of the other K values uniformly at random with probability 1 \u2212 \u03b1. F \u03b1,K,d (p) is a combinatorial function defined as the maximum number of dimensions-out of d total-by which \u00b5 \u03b1,K and \u03c1 \u03b1,K can differ such that a set with measure p under \u00b5 \u03b1,K is guaranteed to have measure at least 1 2 under \u03c1 \u03b1,K . Lee et al. (2019) prove that this value is independent of z and z .\n\nFinally, Dvijotham et al. (2020) consider a more general form of randomized smoothing that doesn't require strict assumptions on the distributions but is still able to provide similar guarantees.\n\nExample 3 (Generic bound). Given any two smoothing distributions \u00b5, \u03c1, we have the generic randomized smoothing robustness certificate, ensuring that (1) holds with definitions\nd(\u00b5, \u03c1) = KL(\u03c1 \u00b5), f (p) = \u2212 1 2 log(4p(1 \u2212 p)). (3)\nRandomized smoothing in practice For deep classifiers, the expectation G cannot be computed exactly, and so we must resort to Monte Carlo approximation. In this \"standard\" form of randomized smoothing, we draw multiple random samples from \u00b5 and use these to construct a highprobability bound on G for certification. More precisely, this bound should be a lower bound on G when the hard prediction g = 1 and an upper bound otherwise; this ensures in both cases that we under-certify the true robustness of the classifier g. The procedure is shown in Algorithm 2 in Appendix A. These estimates can then be plugged into a randomized smoothing robustness guarantee to provide a high probability certified robustness bound for the classifier.\n\n\nLabel-Flipping Robustness\n\nWe now present the main contribution of this paper, a technique for using randomized smoothing to provide certified robustness against label-flipping attacks. Specifically, we first propose a generic strategy for applying randomized smoothing to certify a prediction function against pointwise label-flipping attacks. We show how this general approach can be made tractable using linear least-squares classification, and we use the Chernoff inequality to analytically bound the relevant probabilities for the randomized smoothing procedure. Notably, although we are employing a randomized approach, the final algorithm does not use any random sampling, but rather relies upon a convex optimization problem to compute the certified robustness.\n\nTo motivate the approach, we note that in prior work, randomized smoothing was applied at test time with the function \u03c6 : X \u2192 {0, 1} being a (potentially deep) classifier that we wish to smooth. However, there is no requirement that the function \u03c6 be a classifier at all; the theory holds for any binary-valued function. Instead of treating \u03c6 as a trained classifier, we consider \u03c6 to be an arbitrary learning algorithm which takes as input a training dataset\n{x i , y i } n i=1 \u2208 (X \u00d7 {0, 1}\n) n and additional test points without corresponding labels, which we aim to predict. 2 In other words, the combined goal of \u03c6 is to first train a classifier and then predict the label of the new example. Thus, we consider test time outputs to be a function of both the test time input and the training data that produced the classifier. This perspective allows us to reason about how changes to training data affect the classifier at test time, reminiscent of work on influence functions of deep neural networks Yeh et al., 2018). When applying randomized smoothing in this setting, we randomize over the labels in the training set, rather than over the test-time input to be classified. Analogous to previous applications of randomized smoothing, if the majority vote of the classifiers trained with these randomly sampled labels has a large margin, it will confer a degree of adversarial robustness to some number of adversarially corrupted labels.\n\nTo formalize this intuition, consider two different assignments of n training labels Y 1 , Y 2 \u2208 {0, 1} n which differ on precisely r labels. Let \u00b5 (resp. \u03c1) be the distribution resulting from independently flipping each of the labels in Y 1 (resp. Y 2 ) with probability q. It is clear that as r increases, d(\u00b5, \u03c1) should also increase. In fact, it is simple to show (see Appendix B.3 for derivation) that the exact KL divergence between these two distributions is\nKL(\u00b5 \u03c1) = KL(\u03c1 \u00b5) = r(1 \u2212 2q) log 1 \u2212 q q .(4)\nPlugging in the robustness guarantee (3), we have that\ng(\u00b5, \u03c6) = g(\u03c1, \u03c6) so long as r \u2264 log(4p(1 \u2212 p)) 2(1 \u2212 2q) log q 1\u2212q ,(5)\nwhere p = G(\u00b5, \u03c6). This implies that for any test point, as long as (5) is satisfied, g's prediction (the majority vote weighted by the smoothing distribution) will not change if an adversary corrupts the training set from Y 1 to Y 2 , or indeed to any other training set that differs on at most r labels. We can tune the noise hyperparameter q to achieve the largest possible upper bound in (5); more noise will likely decrease the margin of the majority vote p, but it will also decrease the divergence.\n\nComputing a tight bound This approach has a simple closed form, but the bound is not tight. We can derive a tight bound via a combinatorial approach as in Lee et al. (2019). By precomputing the quantities F \u22121 1\u2212q,1,n (r) from Equation (2) for each r, we can simply compare p to each of these and thereby certify robustness to the highest possible number of label flips. This computation can be expensive, but it provides a significantly tighter robustness guarantee, certifying approximately twice as many label flips for a given bound on G (See Figure 6 in Appendix D).\n\n\nEfficient implementation via least squares classifiers\n\nThere may appear to be one major impracticality of the algorithm proposed in the previous section, if considered naively: treating the function \u03c6 as an entire training-plussingle-prediction process would require that we train multiple classifiers, over multiple random draws of the labels y, all to make a prediction on a single example. In this section, we describe a sequence of tools we employ to restrict the architecture and training process in a manner that drastically reduces this cost, bringing it in line with the traditional cost of classifying a single example. The full procedure, with all the parts described below, can be found in Algorithm 1.\n\nLinear least-squares classification The fundamental simplifying assumption we make in this work is to restrict the \"training\" process done by the classifier \u03c6 to be done via the solution of a least-squares problem. Given the training set {x i , y i } n i=1 , we assume that there exists some feature mapping h : R d \u2192 R k (where k < n). If existing linear features are not available, this could instead consist of deep features learned from a similar task-the transferability of such features is well documented (Donahue et al., 2014;Bo et al., 2010;Yosinski et al., 2014). As another option, features could be learned in an unsupervised fashion on x 1:n (that is, independent of the training labels, which are potentially poisoned). Given this feature mapping, let Algorithm 1 Randomized smoothing for label-flipping robustness Input: feature mapping h : R d \u2192 R k ; noise parameter q; regularization parameter \u03bb;\ntraining set {(x i , y i ) \u2208 R d \u00d7 {0, 1}} n i=1 (with potentially adversarial labels); ad- ditional inputs to predict {x j \u2208 R d } m j=1 . 1. Pre-compute matrix M, M = X X T X + \u03bbI \u22121 where X \u2261 h(x 1:n ). for j = 1, . . . , m do 1. Compute vector \u03b1 j = Mh(x j ) T . 2. Compute optimal Chernoff parameter t via Newton's method t = arg min t t/2+ i:yi=1 log q + (1 \u2212 q)e \u2212t\u03b1 j i + i:yi=0 log (1 \u2212 q) + qe \u2212t\u03b1 j i and let p = max(1 \u2212 B |t | , 1/2) where B |t | is the Chernoff bound (6) evaluated at |t |.\nOutput: Prediction\u0177 j = 1 {t \u2265 0} and certification that prediction will remain constant for up to r training label flips, where\nr = \uf8ef \uf8ef \uf8ef \uf8f0 log(4p (1 \u2212 p )) 2(1 \u2212 2q) log q 1\u2212q \uf8fa \uf8fa \uf8fa \uf8fb .\nend for X = h(x 1:n ) \u2208 R n\u00d7k be the training point features and let y = y 1:n \u2208 {0, 1} n be the labels. Our training process consists of finding the least-squares fit to the training data, i.e., we find parameters\u03b2 \u2208 R k via the normal equation \u03b2 = X T X \u22121 X T y and then we make a prediction on the new example via the linear function h(x n+1 )\u03b2. Although it may seem odd to fit a classification task with least-squares loss, binary classification with linear regression is known to be equivalent to Fisher's linear discriminant (Mika, 2003) and often works quite well in practice.\n\nThe real advantage of the least-squares approach is that it reduces the prediction to a linear function of y, and thus randomizing over the labels is straightforward. Specifically, letting\n\u03b1 = X X T X \u22121 h(x n+1 ) T ,\nthe prediction h(x n+1 )\u03b2 can be equivalently given by \u03b1 T y (this is effectively the kernel representation of the linear classifier). Thus, we can simply compute \u03b1 one time and then randomly sample many different sets of labels in order to build a standard randomized smoothing bound. Further, we can pre-compute just the X X T X \u22121 term and reuse it for each test point.\n\n2 regularization for better conditioning Unfortunately, it is unlikely to be the case that the training points are wellbehaved for linear classification in the feature space. To address this, we instead solve an 2 regularized version of least-squares. This is a common tool for solving systems with ill-conditioned or random design matrices (Hsu et al., 2014;Suggala et al., 2018). Luckily, there still exists a precomputable closed-form solution to this problem, whereby we instead solve\n\u03b1 = X(X T X + \u03bbI) \u22121 h(x n+1 ) T .\nThe other parts of our algorithm remain unchanged. Following results in Suggala et al. (2018), we set the regularization parameter \u03bb = (1 + q)\u03c3 2 k 2n \u03ba(X T X) for all our experiments, where\u03c3 2 = y\u2212X\u03b2 OLS 2 2 n\u2212k is an estimate of the variance (Dicker, 2014) and \u03ba(\u00b7) is the condition number.\n\nEfficient tail bounds via the Chernoff inequality Even more compelling, due to the linear structure of this prediction, we can forego a sampling-based approach entirely and directly bound the tail probabilities using Chernoff bounds. Because the underlying binary prediction function \u03c6 will output the label 1 for the test point whenever \u03b1 T y \u2265 1/2 and 0 otherwise, we can derive an analytical upper bound on the probability that g predicts one label or the other via the Chernoff bound. By upper bounding the probability of the opposite prediction, we simultaneously derive a lower bound on p which can be plugged in to (5) to determine the classifier's robustness. Concretely, we can upper bound the probability that the classifier outputs the label 0 by (6) Conversely, the probability that the classifier outputs the label 1 is upper bounded by (6) but evaluated at \u2212t. Thus, we can solve the minimization problem unconstrained over t, and then let the sign of t dictate which label to predict and the value of t determine the bound. The objective (6) is logconvex in t and can be easily solved by Newton's method. Note that in some cases, neither Chernoff upper bound will be less than 1/2, meaning we cannot determine the true value of g. In these cases, we simply define the classifier's prediction to be determined by the sign of t. While we can't guarantee that this classification will match the true majority vote, our algorithm will certify a robustness to 0 flips, so the guarantee is still valid. We avoid abstaining so as to assess our classifier's non-robust accuracy.\nP (\u03b1 T y \u2264 1/2) \u2264 min t>0 e t/2 n i=1 E[e \u2212t\u03b1iyi ] = min t>0 e t/2 n i=1 qe \u2212t\u03b1i(1\u2212yi) + (1 \u2212 q)e \u2212t\u03b1iyi .\nThe key property we emphasize is that, unlike previous randomized smoothing applications, the final algorithm involves no randomness whatsoever. Instead, the prediction probabilities are bounded directly via Chernoff's inequality, without any need for Monte Carlo approximation. Thus, the method is able to generate truly certifiable robust predictions using approximately the same complexity as traditional predictions.\n\n\nExperiments\n\nFollowing  and Steinhardt et al. (2017), we perform experiments on MNIST 1/7, the IMDB review sentiment dataset (Maas et al., 2011), and the Dogfish binary classification challenge taken from ImageNet. We run additional experiments on the full MNIST dataset; to the best of our knowledge, this is the first multi-class classification algorithm with certified robustness to label-flipping attacks. For each dataset and each noise level q we report the certified test set accuracy at r training label flips. That is, for each possible number of flips r, we plot the fraction of the test set that was both correctly classified and certified to not change under at least r flips.\n\nBecause the above are binary classification tasks, one could technically achieve a certified accuracy of 50% at r = \u221e (or 10% for MNIST) by letting g be constant. A constant classifier would be infinitely robust, but it is not a very meaningful baseline. However, we include the accuracy of such a classifier in our plots (black dotted line) as a reference. We also ran our least-squares classifier on the same features but with q = 0 (black dash-dot line); this cannot certify robustness, but it gives a sense of the quality of the features for classification without any label noise.\n\nTo properly justify the need for such certified defenses, and to get a sense of the scale of our certifications, we also generated label-flipping attacks against the undefended binary MNIST and Dogfish models. Following previous work, the undefended models were implemented as convolutional neural networks, trained on the clean data, with all but the top layer frozen-this is equivalent to multinomial logistic regression on the learned features. For each test point we recorded how many flips were required to change the network's prediction. This number serves as an upper bound for the robustness of the network on that test point, but we note that our attacks were quite rudimentary and could almost certainly be improved upon to tighten this upper bound. Appendix C contains the details of our attack implementations. In all our plots, the solid lines represent certified accuracy (except for the undefended classifier, which is an Results on MNIST The MNIST 1/7 dataset (LeCun et al., 1998) consists of just the classes 1 and 7, totalling 13,007 training points and 2,163 test points. We trained a simple convolutional neural network on the other eight MNIST digits to learn a 50-dimensional feature embedding and then calculated Chernoff bounds for G as described in Section 4.1. Figure 1a displays the certified accuracy on the test set for varying probabilities q. As in prior work on randomized smoothing, the noise parameter q balances a trade-off; as q increases, the required margin |G \u2212 1 2 | to certify a given number of flips decreases. On the other hand, this results in more noisy training labels, which reduces the margin and therefore results in lower robustness and often lower accuracy. Figure 1b depicts the certified accuracy for the full MNIST test set-see Appendix B for derivations of the bounds and optimization algorithm in the multi-class case. In addition to this being a significantly more difficult classification task, our classifier could not rely on features learned from other handwritten digits; instead, we extracted the top 30 components with ICA (Hyvarinen, 1999) independently of the labels. Despite the lack of fine-tuned features, our algorithm still achieves significant certified accuracy under a large number of adversarial label flips. We observed that regularization did not make a large difference for the multiclass case, possibly due to the inaccuracy of the residual term in the noise estimate.\n\nSee Figure 4 in Appendix D for the effect of 2 regularization for the binary case. At a moderate cost to non-robust accuracy, the regularization results in substantially higher certified accuracy at almost all radii.\n\n\nResults on Dogfish\n\nThe Dogfish dataset contains images from the ImageNet dog and fish synsets, 900 training points and 300 test points from each. We trained a ResNet-50 (He et al., 2016) on the standard ImageNet training set but removed all images labeled dog or fish. Our pre-trained network therefore learned meaningful image features but had no features specific to either class. We used PCA to reduce the feature space to 5 dimensions before solving to avoid overfitting. Figure 2 displays the results of our poisoning attack along with our certified defense. Under the undefended model, more than 99% of the test points can be successfully attacked with no more than 23 label flips, whereas our model with q = 10 \u22124 can certifiably correctly Percentage of Training Set Figure 3. IMDB Review Sentiment (n = 25000) test set certified accuracy. The non-robust accuracy slightly decreases as q increases; for q = 0.01 the non-robust accuracy is 79.11%, while for q = 0.1 it is 78.96%.\n\nclassify 81.3% of the test points under the same attack. It would take more than four times as many flips-more than 5% of the training set-for each test point individually to reduce our model to less than 50% certified accuracy.\n\nBecause the \"votes\" are changed by flipping so few labels, high values of q reduce the models' predictions to almost pure chance-this means we are unable to achieve the margins necessary to certify a large number of flips. We therefore found that smaller levels of noise achieved higher certified test accuracy. This suggests that the more susceptible the original, non-robust classifier is to label flips, the lower q should be set for the corresponding randomized classifier.\n\nFor much smaller values of q, slight differences did not decrease the non-robust accuracy-they did however have a large effect on certified robustness. This indicates that the sign of t is relatively stable, but the margin of G is much less so. This same pattern was observed with the MNIST and IMDB datasets. We used a high-precision arithmetic library (Johansson et al., 2013) to achieve the necessary lower bounds, but the precision required for non-vacuous bounds grew extremely fast for q < 10 \u22124 ; optimizing (6) quickly became too computationally expensive.\n\nFinally, Figure 5 in Appendix D shows the performance of our classifier applied to unsupervised features. Because Dogfish is such a small dataset, deep unsupervised feature learning techniques were not feasible-we instead learned overcomplete features on 16x16 image patches using RICA (Le, 2013). It is worth noting that on datasets of a typical size for deep learning (i.e., n = 10 4 or higher), robustness and accuracy closer to those of the pre-trained features are likely achievable with modern unsupervised feature learning algorithms such as Deep Clustering (Caron et al., 2018). Figure 3 plots the result of our randomized smoothing procedure on the IMDB review sentiment dataset. This dataset contains 25,000 training examples and 25,000 test examples, evenly split between \"positive\" and \"negative\". To extract the features we applied the Google News pre-trained Word2Vec to all the words in each review and averaged them. This feature embedding is considerably noisier than that of an image dataset, as most of the words in a review are irrelevant to sentiment classification. Indeed, Steinhardt et al. (2017) also found that the IMDB dataset was much more susceptible to adversarial corruption than images when using bag-of-words features. Consistent with this, we found smaller levels of noise resulted in larger certified accuracy. We expect significant improvements could be made with a more refined choice of feature embedding.\n\n\nResults on IMDB\n\n\nConclusion\n\nIn this work we presented a certified defense against a strong class of adversarial label-flipping attacks where an adversary can flip labels to cause a misclassification on each test point individually. This contrasts with previous data poisoning settings which have typically only considered an adversary who wishes to degrade the classifier's accuracy on the test distribution as a whole, and it brings the adversary's objective more in line with that of backdoor attacks and test-time adversarial perturbations. Leveraging randomized smoothing, a method originally developed for certifying robustness to test-time perturbations, we presented a classifier that can be certified robust to these pointwise train-time attacks. We then offered a tractable algorithm for evaluating this classifier which, despite being rooted in randomization, can be computed with no Monte Carlo sampling whatsoever, resulting in a truly certifiably robust classifier. This results in the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.\n\nThere are several avenues for improvements to this line of work, perhaps the most immediate being the method for learning the input features. For example, rather than considering fixed features generated by a pre-trained deep network or learned without labels, extensions could leverage neural tangent kernels (Jacot et al., 2018) to allow for efficient learning with perturbed inputs or more flexible representations. The analysis could also be extended to other types of smoothing distributions applied to the training data, such as randomizing over the input features to provide robustness to more general data poisoning attacks. Finally, we hope that our defense to this threat model will inspire the development of more powerful train-time attacks, against which future defenses can be evaluated.\n\n\nA. Generic Randomized Smoothing Algorithm\n\nAlgorithm 2 Generic randomized smoothing procedure Input: function \u03c6 : X \u2192 {0, 1}, number of samples N , smoothing distribution \u00b5, test point to predict x 0 , failure probability \u03b4 > 0.\nfor i = 1, . . . , N do Sample x i \u223c \u00b5(x 0 ) and compute y i = \u03c6(x i ). end for Compute approximate smoothed output\u011d (\u00b5, \u03c6) = 1 1 N N i=1 y i \u2265 1 2 . Compute bound\u011c(\u00b5, \u03c6) such that with probability \u2265 1 \u2212 \u03b4 G(\u00b5, \u03c6) \u2264 G(\u00b5, \u03c6) if\u011d(\u00b5, \u03c6) = 1 \u2265 G(\u00b5, \u03c6) if\u011d(\u00b5, \u03c6) = 0.\nOutput: Prediction\u011d(\u00b5, \u03c6) and probability bound\u011c(\u00b5, \u03c6), or abstention if sign(\u011c(\u00b5, \u03c6) \u2212 1 2 ) = sign(\u011c(\u00b5, \u03c6) \u2212 1 2 ).\n\n\nB. The multi-class setting\n\nAlthough the notation and algorithms are slightly more complex, all the methods we have discussed in the main paper can be extended to the multi-class setting. In this case, we consider a class label y \u2208 {1, . . . , K}, and we again seek some smoothed prediction such that the classifier's prediction on a new point will not change with some number r flips of the labels in the training set.\n\n\nB.1. Randomized smoothing in the multi-class case\n\nWe here extend our notation to the case of more than two classes. Recall our original definition of G,\nG(\u00b5, \u03c6) = E x\u223c\u00b5 [\u03c6(x)] = X \u00b5(x)\u03c6(x)dx,\nwhere \u03c6 : X \u2192 {0, 1}. More generally, consider a classifier \u03c6 : X \u2192 [K], outputting the index of one of K classes. Under this formulation, for a given class c \u2208 [K], we have\nG(\u00b5, \u03c6, c) = E x\u223c\u00b5 [\u03c6 c (x)] = X \u00b5(x)\u03c6 c (x)dx,\nwhere \u03c6 c (x) = 1 {\u03c6(x) = c} is the indicator function for if \u03c6(x) outputs the class c. In this case, the hard threshold g is evaluated by returning the class with the highest probability. That is,\ng(\u00b5, \u03c6) = arg max c G(\u00b5, \u03c6, c).\n\nB.2. Linearization and Chernoff bound approach for the multiclass case\n\nUsing the same linearization approach as in the binary case, we can formulate an analogous approach which forgoes the need to actually perform random sampling at all and instead directly bounds the randomized classifier using the Chernoff bound.\n\nAdopting the same notation as in the main text, the equivalent least-squares classifier for the multi-class setting finds some set of weights\u03b2\n= X T X \u22121 X T Y\nwhere Y \u2208 {0, 1} n\u00d7K is a binary matrix with each row equal to a one-hot encoding of the class label (note that the resultin\u011d \u03b2 \u2208 R k\u00d7K is now a matrix, and we let\u03b2 i refer to the ith column). At prediction time, the predicted class of some new point x n+1 is simply given by the prediction with the highest value, i.e.,\ny n+1 = arg max i\u03b2 i T h(x n+1 ).\nAlternatively, following the same logic as in the binary case, this same prediction can be written in terms of the \u03b1 variable a\u015d\ny n+1 = arg max i \u03b1 T Y i where Y i denotes the ith column of Y i .\nIn our randomized smoothing setting, we again propose to flip the class of any label with probability q, selecting an alternative label uniformly at random from the remaining K \u2212 1 labels. Assuming that the predicted class label is i, we wish to bound the probability that\nP (\u03b1 T Y i < \u03b1 T Y i )\nfor all alternative classes i . By the Chernoff bound, we have that\nlog P (\u03b1 T Y i < \u03b1 T Y i ) = log P (\u03b1 T (Y i \u2212 Y i ) \u2264 0) \u2264 min t>0 \uf8f1 \uf8f2 \uf8f3 n j=1 log E e \u2212t\u03b1j (Yji\u2212Y ji ) \uf8fc \uf8fd \uf8fe .\nThe random variable Y ji \u2212 Y ji takes on three different distributions depending on if y j = i, if y j = i , or if y j = i and y j = i . Specifically, this variable can take on the terms +1, 0, \u22121 with the associated probabilities\nP (Y ji \u2212 Y ji = +1) = 1 \u2212 q if y j = i, q/(K \u2212 1) otherwise. P (Y ji \u2212 Y ji = \u22121) = 1 \u2212 q if y j = i , q/(K \u2212 1) otherwise. P (Y ji \u2212 Y ji = 0) = q(K \u2212 2)/(K \u2212 1) if y j = i or y j = i , 1 \u2212 2q/(K \u2212 1) otherwise.\nCombining these cases directly into the Chernoff bound gives\nlog P (\u03b1 T Y i < \u03b1 T Y i ) \u2264 min t>0 j:yj =i log (1 \u2212 q)e \u2212t\u03b1j + q K \u2212 2 K \u2212 1 + q K \u2212 1 e t\u03b1j + j:yj =i log q K \u2212 1 e \u2212t\u03b1j + q K \u2212 2 K \u2212 1 + (1 \u2212 q)e t\u03b1j + j:yj =i,yj =i log q K \u2212 1 e \u2212t\u03b1j + 1 \u2212 2 q K \u2212 1 + q K \u2212 1 e t\u03b1j .\nAgain, this problem is convex in t, and so can be solved efficiently using Newton's method. And again since the reverse case can be computed via the same expression we can similarly optimize this in an unconstrained fashion. Specifically, we can do this for every pair of classes i and i , and return the i which gives the smallest lower bound for the worst-case choice of i .\n\n\nB.3. KL Divergence Bound\n\nTo compute actual certification radii, we will derive the KL divergence bound for the the case of K classes. Let \u00b5, \u03c1 be defined as in Section 4, except that as in the previous section when a label is flipped with probability q it is changed to one of the other K \u2212 1 classes uniformly at random. Let \u00b5 i and \u03c1 i refer to the independent measures on each dimension which collectively make up the factorized distributions \u00b5 and \u03c1 (i.e., \u00b5(x) = d i=1 \u00b5 i (x)). Further, let Y i 1 be the i th element of Y 1 , meaning it is the \"original\" label which may or may not be flipped when sampling from \u00b5. First noting that each dimension of the distributions \u00b5 and \u03c1 are independent, we have\nKL(\u03c1 \u00b5) = n i=1 KL(\u03c1 i \u00b5 i ) = i:\u03c1i =\u00b5i KL(\u03c1 i \u00b5 i ) = r \uf8eb \uf8ed K j=1 \u03c1 i (j) log \u03c1 i (j) \u00b5 i (j) \uf8f6 \uf8f8 = r \u03c1 i (Y i 1 ) log \u03c1 i (Y i 1 ) \u00b5 i (Y i 1 ) + \u03c1 i (Y i 2 ) log \u03c1 i (Y i 2 ) \u00b5 i (Y i 2 ) = r (1 \u2212 q) log 1 \u2212 q q K\u22121 + q K \u2212 1 log q K\u22121 1 \u2212 q = r 1 \u2212 Kq K \u2212 1 log (1 \u2212 q)(K \u2212 1) q .\nPlugging in the robustness guarantee (3), we have that g(\u00b5, \u03c6) = g(\u03c1, \u03c6) so long as\nr \u2264 log(4p(1 \u2212 p)) 2(1 \u2212 Kq K\u22121 ) log q (1\u2212q)(K\u22121)\n. Setting K = 2 recovers the divergence term (4) and the bound (5).\n\n\nC. Description of Label-Flipping Attacks on MNIST 1/7 and Dogfish\n\nDue to the dearth of existing work on label-flipping attacks for deep networks, our attacks on MNIST and Dogfish were quite straightforward; we expect significant improvements could be made to tighten this upper bound.\n\nFor Dogfish, we used a pretrained Inception network (Szegedy et al., 2016) to evaluate the influence of each training point with respect to the loss of each test point . As in prior work, we froze all but the top layer of the network for retraining. Once we obtained the most influential points, we flipped the first one and recomputed approximate influence using only the top layer for efficiency. After each flip, we recorded which points were classified differently and maintained for each test point the successful attack which required the fewest flips. When this was finished, we also tried the reverse of each attack to see if any of them could be achieved with even fewer flips.\n\nFor MNIST we implemented two similar attacks and kept the best attack for each test point. The first attack simply ordered training labels by their 2 distance from the test point in feature space, as a proxy for influence. We then tried flipping these one at a time until the prediction changed, and we also tried the reverse. The second attack was essentially the same as the Dogfish attack, ordering the test points by influence. To calculate influence we again assumed a frozen feature map; specifically, using the same notation as , the influence of flipping the label of a training point z = (x, y) to z \u2212 = (x, 1 \u2212 y) on the loss at the test point z test is:\ndL(z test ,\u03b8 ,z \u2212 ,\u2212z ) d = \u2207 \u03b8 L(z test ,\u03b8) T d\u03b8 ,z \u2212 ,\u2212z d \u2248 \u2212\u2207 \u03b8 L(z test ,\u03b8) T H \u22121 \u03b8 \u2207 \u03b8 L(z \u2212 ,\u03b8) \u2212 \u2207 \u03b8 L(z,\u03b8) .\nFor logistic regression, these values can easily be computed in closed form. Percentage of Training Set Figure 4. MNIST 1/7 test set certified accuracy with and without 2 regularization in the computation of \u03b1. Note that the unregularized solution achieves almost 100% non-robust accuracy, but certifies significantly lower robustness. This implies that the \"training\" process is not robust enough to label noise, hence the lower margin by the ensemble. In comparison, the regularized solution achieves significantly higher margins, at a slight cost in overall accuracy. Percentage of Training Set Figure 5. Dogfish test set certified accuracy using features learned with RICA (Le, 2013). While not as performant as the pre-trained features, our classifier still achieves reasonable certified accuracy-note that the certified lines are lower bounds, while the undefended line is an upper bound. As the limitation is feature quality, deep unsupervised features could significantly boost performance, but would require a larger dataset. . Left: Required margin p to certify a given number of label flips using the generic KL bound (5) versus the tight discrete bound (2). Right: The same comparison, but inverted, showing the certifiable robustness for a given margin. The tight bound certifies robustness to approximately twice as many label flips.\n\n\nD. Additional Plots\n\nFigure 1 .\n1MNIST 1/7 (n = 13007, top) and full MNIST (n = 60000, bottom) test set certified accuracy to adversarial label flips as q is varied. The bottom axis represents the number of adversarial label flips to which each individual prediction is robust, while the top axis is the same value expressed as a percentage of the training set size. The solid lines represent certified accuracy; dashed lines of the same color are the overall non-robust accuracy of each classifier. The black dotted line is the (infinitely robust) performance of a constant classifier, while the black dash-dot line is the (uncertified) performance of our classifier with no label noise. upper bound), while the dashed lines of the same color are the overall non-robust accuracy of each classifier.\n\nFigure 2 .\n2Dogfish (n = 1800) test set certified accuracy to adversarial label flips as q is varied.\n\nFigure 6\n6Figure 6. Left: Required margin p to certify a given number of label flips using the generic KL bound (5) versus the tight discrete bound (2). Right: The same comparison, but inverted, showing the certifiable robustness for a given margin. The tight bound certifies robustness to approximately twice as many label flips.\nCarnegie Mellon University 2 Bosch Center for AI. Correspondence to: Elan Rosenfeld <elan@cmu.edu>.Copyright 2020 by the author(s).\nFor simplicity, we present the methodology here in terms of binary-valued functions, which will correspond eventually to binary classification problems. The extension to the multiclass setting requires additional notation, and thus is deferred to the appendix.\nNote that our algorithm does not actually require access to the test data to do the necessary precomputation. We present it here as such merely to give an intuitive idea of the procedure.\n\nObfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. A Athalye, N Carlini, D Wagner, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningAthalye, A., Carlini, N., and Wagner, D. Obfuscated gra- dients give a false sense of security: Circumventing de- fenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, July 2018.\n\nCan machine learning be secure?. M Barreno, B Nelson, R Sears, A D Joseph, J D Tygar, Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security. the 2006 ACM Symposium on Information, Computer and Communications SecurityNew York, NY, USABarreno, M., Nelson, B., Sears, R., Joseph, A. D., and Tygar, J. D. Can machine learning be secure? In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, pp. 16-25, New York, NY, USA, 2006.\n\nRobust regression via hard thresholding. K Bhatia, P Jain, P Kar, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USA1Bhatia, K., Jain, P., and Kar, P. Robust regression via hard thresholding. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, pp. 721-729, Cambridge, MA, USA, 2015.\n\nSupport vector machines under adversarial label noise. B Biggio, B Nelson, P Laskov, Proceedings of the Asian Conference on Machine Learning. the Asian Conference on Machine LearningTaoyuan, Taiwain20Biggio, B., Nelson, B., and Laskov, P. Support vector ma- chines under adversarial label noise. In Proceedings of the Asian Conference on Machine Learning, volume 20, pp. 97-112, South Garden Hotels and Resorts, Taoyuan, Taiwain, 14-15 Nov 2011.\n\nSecurity evaluation of pattern classifiers under attack. B Biggio, G Fumera, F Roli, IEEE Transactions on Knowledge and Data Engineering. 264Biggio, B., Fumera, G., and Roli, F. Security evaluation of pattern classifiers under attack. IEEE Transactions on Knowledge and Data Engineering, 26(4):984-996, April 2014.\n\nKernel descriptors for visual recognition. L Bo, X Ren, D Fox, Advances in Neural Information Processing Systems. 23Bo, L., Ren, X., and Fox, D. Kernel descriptors for visual recognition. In Advances in Neural Information Process- ing Systems 23, pp. 244-252. 2010.\n\nAdversarial examples are not easily detected: Bypassing ten detection methods. N Carlini, D Wagner, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityNew York, NY, USACarlini, N. and Wagner, D. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 3-14, New York, NY, USA, 2017.\n\nDeep clustering for unsupervised learning of visual features. M Caron, P Bojanowski, A Joulin, M Douze, The European Conference on Computer Vision (ECCV). Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In The European Conference on Computer Vision (ECCV), September 2018.\n\nA robust learning approach for regression models based on distributionally robust optimization. R Chen, I C Paschalidis, 1532-4435J. Mach. Learn. Res. 191Chen, R. and Paschalidis, I. C. A robust learning approach for regression models based on distributionally robust op- timization. J. Mach. Learn. Res., 19(1):517-564, January 2018. ISSN 1532-4435.\n\nX Chen, C Liu, B Li, K Lu, D Song, arXiv:1712.05526Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprintChen, X., Liu, C., Li, B., Lu, K., and Song, D. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nRobust sparse regression under adversarial corruption. Y Chen, C Caramanis, S Mannor, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningAtlanta, Georgia, USA28Chen, Y., Caramanis, C., and Mannor, S. Robust sparse regression under adversarial corruption. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pp. 774-782, Atlanta, Georgia, USA, 17-19 Jun 2013.\n\nCertified adversarial robustness via randomized smoothing. J Cohen, E Rosenfeld, J Z Kolter, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningLong Beach, California, USA97Cohen, J., Rosenfeld, E., and Kolter, J. Z. Certified adversar- ial robustness via randomized smoothing. In Proceedings of the 36th International Conference on Machine Learn- ing, volume 97, pp. 1310-1320, Long Beach, California, USA, 09-15 Jun 2019.\n\nEfficient algorithms and lower bounds for robust linear regression. I Diakonikolas, W Kong, A Stewart, Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms. the Thirtieth Annual ACM-SIAM Symposium on Discrete AlgorithmsPA, USADiakonikolas, I., Kong, W., and Stewart, A. Efficient algo- rithms and lower bounds for robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Sympo- sium on Discrete Algorithms, pp. 2745-2754, Philadel- phia, PA, USA, 2019.\n\nVariance estimation in high-dimensional linear models. L H Dicker, Biometrika. 1012Dicker, L. H. Variance estimation in high-dimensional linear models. Biometrika, 101(2):269-284, 03 2014.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, Darrell , T , Proceedings of the 31st International Conference on Machine Learning. the 31st International Conference on Machine LearningBejing, China32Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In Pro- ceedings of the 31st International Conference on Machine Learning, volume 32, pp. 647-655, Bejing, China, 22-24 Jun 2014.\n\nA framework for robustness certification of smoothed classifiers using f-divergences. K Dvijotham, J Hayes, B Balle, Z Kolter, C Qin, A Gyorgy, K Xiao, S Gowal, P Kohli, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Conference Track ProceedingsDvijotham, K., Hayes, J., Balle, B., Kolter, Z., Qin, C., Gy- orgy, A., Xiao, K., Gowal, S., and Kohli, P. A framework for robustness certification of smoothed classifiers using f-divergences. In 8th International Conference on Learn- ing Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, Conference Track Proceedings, 2020.\n\nDetecting adversarial samples from artifacts. R Feinman, R R Curtin, S Shintre, A B Gardner, arXiv:1703.00410arXiv preprintFeinman, R., Curtin, R. R., Shintre, S., and Gardner, A. B. Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410, 2017.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n\nRandom design analysis of ridge regression. D Hsu, S M Kakade, T Zhang, Found. Comput. Math. 143Hsu, D., Kakade, S. M., and Zhang, T. Random design analysis of ridge regression. Found. Comput. Math., 14 (3):569-600, June 2014.\n\nFast and robust fixed-point algorithms for independent component analysis. A Hyvarinen, IEEE Transactions on Neural Networks. 103Hyvarinen, A. Fast and robust fixed-point algorithms for independent component analysis. IEEE Transactions on Neural Networks, 10(3):626-634, May 1999.\n\nNeural tangent kernel: Convergence and generalization in neural networks. A Jacot, F Gabriel, C Hongler, Advances in Neural Information Processing Systems. 31Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31, pp. 8571-8580. 2018.\n\nmpmath: a Python library for arbitraryprecision floating-point arithmetic (version 0. F , Certified Robustness to Label-Flipping Attacks via Randomized Smoothing Johansson. 18December 2013Certified Robustness to Label-Flipping Attacks via Randomized Smoothing Johansson, F. et al. mpmath: a Python library for arbitrary- precision floating-point arithmetic (version 0.18), Decem- ber 2013. http://mpmath.org/.\n\nCompressed sensing with adversarial sparse noise via l1 regression. S Karmalkar, E Price, arXiv:1809.08055arXiv preprintKarmalkar, S. and Price, E. Compressed sensing with ad- versarial sparse noise via l1 regression. arXiv preprint arXiv:1809.08055, 2018.\n\nEfficient algorithms for outlier-robust regression. A Klivans, P K Kothari, R Meka, arXiv:1803.03241arXiv preprintKlivans, A., Kothari, P. K., and Meka, R. Efficient al- gorithms for outlier-robust regression. arXiv preprint arXiv:1803.03241, 2018.\n\nUnderstanding black-box predictions via influence functions. P W Koh, P Liang, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Koh, P. W. and Liang, P. Understanding black-box predic- tions via influence functions. In Proceedings of the 34th International Conference on Machine Learning -Volume 70, pp. 1885-1894, 2017.\n\nStronger data poisoning attacks break data sanitization defenses. P W Koh, J Steinhardt, P Liang, arXiv:1811.00741arXiv preprintKoh, P. W., Steinhardt, J., and Liang, P. Stronger data poisoning attacks break data sanitization defenses. arXiv preprint arXiv:1811.00741, 2018.\n\nBuilding high-level features using large scale unsupervised learning. Q V Le, 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. Le, Q. V. Building high-level features using large scale unsupervised learning. In 2013 IEEE International Con- ference on Acoustics, Speech and Signal Processing, pp. 8595-8598, May 2013.\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, Nov 1998.\n\nCertified robustness to adversarial examples with differential privacy. M Lecuyer, V Atlidakis, R Geambasu, D Hsu, Jana , S , arXiv:1802.03471arXiv preprintLecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S. Certified robustness to adversarial examples with differential privacy. arXiv preprint arXiv:1802.03471, 2018.\n\nTight certificates of adversarial robustness for randomly smoothed classifiers. G.-H Lee, Y Yuan, S Chang, T S Jaakkola, Advances in Neural Information Processing Systems 31. Lee, G.-H., Yuan, Y., Chang, S., and Jaakkola, T. S. Tight certificates of adversarial robustness for randomly smoothed classifiers. In Advances in Neural Information Processing Systems 31, 2019.\n\nCertified adversarial robustness with additive gaussian noise. B Li, C Chen, W Wang, Carin , L , arXiv:1809.03113arXiv preprintLi, B., Chen, C., Wang, W., and Carin, L. Certified ad- versarial robustness with additive gaussian noise. arXiv preprint arXiv:1809.03113, 2018.\n\nRobust linear regression against training data poisoning. C Liu, B Li, Y Vorobeychik, A Oprea, AISec 2017 -Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. 112017Liu, C., Li, B., Vorobeychik, Y., and Oprea, A. Robust linear regression against training data poisoning. In AISec 2017 -Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, co-located with CCS 2017, 11 2017.\n\nClassification with noisy labels by importance reweighting. T Liu, D Tao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 383Liu, T. and Tao, D. Classification with noisy labels by impor- tance reweighting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(3):447-461, March 2016.\n\nLearning word vectors for sentiment analysis. A L Maas, R E Daly, P T Pham, D Huang, A Y Ng, C Potts, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesStroudsburg, PA, USA1Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment anal- ysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan- guage Technologies -Volume 1, pp. 142-150, Strouds- burg, PA, USA, 2011.\n\nOn detecting adversarial perturbations. J H Metzen, T Genewein, V Fischer, B Bischoff, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsMetzen, J. H., Genewein, T., Fischer, V., and Bischoff, B. On detecting adversarial perturbations. In 5th Interna- tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n\nKernel fisher discriminants. S Mika, Mika, S. Kernel fisher discriminants, 2003.\n\nTowards poisoning of deep learning algorithms with back-gradient optimization. L Mu\u00f1oz Gonz\u00e1lez, B Biggio, A Demontis, A Paudice, V Wongrassamee, E C Lupu, F Roli, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityNew York, NY, USAMu\u00f1oz Gonz\u00e1lez, L., Biggio, B., Demontis, A., Paudice, A., Wongrassamee, V., Lupu, E. C., and Roli, F. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 27-38, New York, NY, USA, 2017.\n\nLearning with noisy labels. N Natarajan, I S Dhillon, P K Ravikumar, A Tewari, Advances in Neural Information Processing Systems. 26Natarajan, N., Dhillon, I. S., Ravikumar, P. K., and Tewari, A. Learning with noisy labels. In Advances in Neural In- formation Processing Systems 26, pp. 1196-1204. 2013.\n\nSecurity and privacy in machine learning. N Papernot, P Mcdaniel, A Sinha, M P Wellman, Sok, 2018 IEEE European Symposium on Security and Privacy (Eu-roS P). Papernot, N., McDaniel, P., Sinha, A., and Wellman, M. P. Sok: Security and privacy in machine learning. In 2018 IEEE European Symposium on Security and Privacy (Eu- roS P), pp. 399-414, April 2018.\n\nMaking deep neural networks robust to label noise: A loss correction approach. G Patrini, A Rozza, A Krishna Menon, R Nock, L Qu, Patrini, G., Rozza, A., Krishna Menon, A., Nock, R., and Qu, L. Making deep neural networks robust to label noise: A loss correction approach. pp. 2233-2241, 07 2017.\n\nLabel sanitization against label flipping poisoning attacks. A Paudice, L Mu\u00f1oz-Gonz\u00e1lez, E C Lupu, ECML PKDD 2018 Workshops. ChamPaudice, A., Mu\u00f1oz-Gonz\u00e1lez, L., and Lupu, E. C. Label sanitization against label flipping poisoning attacks. In ECML PKDD 2018 Workshops, pp. 5-15, Cham, 2019.\n\nRobust estimation via robust gradient estimation. A Prasad, A S Suggala, S Balakrishnan, P Ravikumar, arXiv:1802.06485arXiv preprintPrasad, A., Suggala, A. S., Balakrishnan, S., and Ravikumar, P. Robust estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.\n\nAntidote: Understanding and defending against poisoning of anomaly detectors. B I Rubinstein, B Nelson, L Huang, A D Joseph, S.-H Lau, S Rao, N Taft, J D Tygar, Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement. the 9th ACM SIGCOMM Conference on Internet MeasurementNew York, NY, USARubinstein, B. I., Nelson, B., Huang, L., Joseph, A. D., Lau, S.-h., Rao, S., Taft, N., and Tygar, J. D. Antidote: Un- derstanding and defending against poisoning of anomaly detectors. In Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement, pp. 1-14, New York, NY, USA, 2009.\n\nH Salman, G Yang, J Li, P Zhang, H Zhang, I Razenshteyn, S Bubeck, arXiv:1906.04584Provably robust deep learning via adversarially trained smoothed classifiers. arXiv preprintSalman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razen- shteyn, I., and Bubeck, S. Provably robust deep learn- ing via adversarially trained smoothed classifiers. arXiv preprint arXiv:1906.04584, 2019.\n\nPoison frogs! targeted clean-label poisoning attacks on neural networks. A Shafahi, W R Huang, M Najibi, O Suciu, C Studer, T Dumitras, T Goldstein, Advances in Neural Information Processing Systems. 31Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and Goldstein, T. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information Processing Systems 31, pp. 6103-6113. 2018.\n\nLearning with bad training data via iterative trimmed loss minimization. Y Shen, S Sanghavi, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningLong Beach, California, USA97Shen, Y. and Sanghavi, S. Learning with bad training data via iterative trimmed loss minimization. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 5739-5748, Long Beach, California, USA, 09-15 Jun 2019.\n\nCertified defenses for data poisoning attacks. J Koh, P W Liang, P , Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsUSACertified Robustness to Label-Flipping Attacks via Randomized Smoothing Steinhardt,Certified Robustness to Label-Flipping Attacks via Randomized Smoothing Steinhardt, J., Koh, P. W., and Liang, P. Certified defenses for data poisoning attacks. In Proceedings of the 31st International Conference on Neural Information Process- ing Systems, pp. 3520-3532, USA, 2017.\n\nConnecting optimization and regularization paths. A Suggala, A Prasad, P K Ravikumar, Advances in Neural Information Processing Systems. 31Suggala, A., Prasad, A., and Ravikumar, P. K. Connecting optimization and regularization paths. In Advances in Neural Information Processing Systems 31, pp. 10608- 10619. 2018.\n\nC Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, Fergus , R , arXiv:1312.6199Intriguing properties of neural networks. arXiv preprintSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n\nOn defending against label flipping attacks on malware detection systems. R Taheri, R Javidan, M Shojafar, Z Pooranian, A Miri, M Conti, arXiv:1908.04473arXiv preprintTaheri, R., Javidan, R., Shojafar, M., Pooranian, Z., Miri, A., and Conti, M. On defending against label flipping attacks on malware detection systems. arXiv preprint arXiv:1908.04473, 2019.\n\nSpectral signatures in backdoor attacks. B Tran, J Li, A Madry, Advances in Neural Information Processing Systems. 31Tran, B., Li, J., and Madry, A. Spectral signatures in back- door attacks. In Advances in Neural Information Process- ing Systems 31, pp. 8000-8010. 2018.\n\nAdversarial label flips attack on support vector machines. H Xiao, H Xiao, C Eckert, Proceedings of the 20th European Conference on Artificial Intelligence. the 20th European Conference on Artificial IntelligenceAmsterdam, The Netherlands, The NetherlandsXiao, H., Xiao, H., and Eckert, C. Adversarial label flips attack on support vector machines. In Proceedings of the 20th European Conference on Artificial Intelligence, pp. 870-875, Amsterdam, The Netherlands, The Netherlands, 2012.\n\nIs feature selection secure against training data poisoning?. H Xiao, B Biggio, G Brown, G Fumera, C Eckert, F Roli, Proceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningLille, France37Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., and Roli, F. Is feature selection secure against training data poisoning? In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pp. 1689- 1698, Lille, France, 07-09 Jul 2015.\n\nC Yang, Q Wu, H Li, Chen , Y , arXiv:1703.01340Generative poisoning attack method against neural networks. arXiv preprintYang, C., Wu, Q., Li, H., and Chen, Y. Generative poisoning attack method against neural networks. arXiv preprint arXiv:1703.01340, 2017.\n\nRepresenter point selection for explaining deep neural networks. C.-K Yeh, J Kim, I E Yen, .-H Ravikumar, P K , Advances in Neural Information Processing Systems. 31Yeh, C.-K., Kim, J., Yen, I. E.-H., and Ravikumar, P. K. Representer point selection for explaining deep neural networks. In Advances in Neural Information Processing Systems 31, pp. 9291-9301. 2018.\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in Neural Information Processing Systems. 27Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems 27, pp. 3320-3328. 2014.\n\nTransferable clean-label poisoning attacks on deep neural nets. C Zhu, W R Huang, H Li, G Taylor, C Studer, T Goldstein, Proceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine LearningLong Beach, California, USA97Zhu, C., Huang, W. R., Li, H., Taylor, G., Studer, C., and Goldstein, T. Transferable clean-label poisoning attacks on deep neural nets. In Proceedings of the 36th Inter- national Conference on Machine Learning, volume 97, pp. 7614-7623, Long Beach, California, USA, 09-15 Jun 2019.\n", "annotations": {"author": "[{\"end\":90,\"start\":75},{\"end\":104,\"start\":91},{\"end\":123,\"start\":105},{\"end\":138,\"start\":124}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":80},{\"end\":103,\"start\":96},{\"end\":122,\"start\":113},{\"end\":137,\"start\":126}]", "author_first_name": "[{\"end\":79,\"start\":75},{\"end\":95,\"start\":91},{\"end\":112,\"start\":105},{\"end\":125,\"start\":124}]", "author_affiliation": null, "title": "[{\"end\":72,\"start\":1},{\"end\":210,\"start\":139}]", "venue": null, "abstract": "[{\"end\":1584,\"start\":212}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1852,\"start\":1832},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1872,\"start\":1852},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2516,\"start\":2497},{\"end\":2747,\"start\":2722},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2788,\"start\":2770},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4321,\"start\":4301},{\"end\":4622,\"start\":4618},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6208,\"start\":6198},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6253,\"start\":6231},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7445,\"start\":7416},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7463,\"start\":7445},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8164,\"start\":8145},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8192,\"start\":8171},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8210,\"start\":8192},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8265,\"start\":8240},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8495,\"start\":8473},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8512,\"start\":8495},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8887,\"start\":8867},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9402,\"start\":9378},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9418,\"start\":9402},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9439,\"start\":9418},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9707,\"start\":9688},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9727,\"start\":9707},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9914,\"start\":9892},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9934,\"start\":9914},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10070,\"start\":10052},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10547,\"start\":10520},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10567,\"start\":10547},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10589,\"start\":10567},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10675,\"start\":10653},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10700,\"start\":10675},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10838,\"start\":10813},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10856,\"start\":10838},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10899,\"start\":10878},{\"end\":10969,\"start\":10944},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11271,\"start\":11249},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11295,\"start\":11271},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11495,\"start\":11474},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11516,\"start\":11495},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11574,\"start\":11552},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11597,\"start\":11574},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12225,\"start\":12204},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12319,\"start\":12303},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12362,\"start\":12343},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12496,\"start\":12475},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12574,\"start\":12556},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12617,\"start\":12598},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15260,\"start\":15243},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15399,\"start\":15382},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16284,\"start\":16267},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16368,\"start\":16345},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19297,\"start\":19280},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21040,\"start\":21023},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22692,\"start\":22670},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22708,\"start\":22692},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22730,\"start\":22708},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24309,\"start\":24297},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25302,\"start\":25284},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25323,\"start\":25302},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25560,\"start\":25539},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25725,\"start\":25711},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28021,\"start\":28002},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30151,\"start\":30131},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31259,\"start\":31242},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32010,\"start\":31993},{\"end\":33898,\"start\":33874},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34382,\"start\":34372},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34671,\"start\":34651},{\"end\":35206,\"start\":35174},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36961,\"start\":36941},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":43292,\"start\":43270},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":45377,\"start\":45367}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46839,\"start\":46060},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46942,\"start\":46840},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47274,\"start\":46943}]", "paragraph": "[{\"end\":2408,\"start\":1600},{\"end\":3223,\"start\":2410},{\"end\":4255,\"start\":3225},{\"end\":5470,\"start\":4257},{\"end\":6433,\"start\":5472},{\"end\":7351,\"start\":6435},{\"end\":8035,\"start\":7368},{\"end\":8888,\"start\":8037},{\"end\":9274,\"start\":8890},{\"end\":10158,\"start\":9276},{\"end\":11181,\"start\":10160},{\"end\":11667,\"start\":11183},{\"end\":12618,\"start\":11669},{\"end\":13266,\"start\":12661},{\"end\":14157,\"start\":13334},{\"end\":14440,\"start\":14261},{\"end\":14927,\"start\":14442},{\"end\":15074,\"start\":14978},{\"end\":15331,\"start\":15076},{\"end\":15453,\"start\":15333},{\"end\":16334,\"start\":15867},{\"end\":16531,\"start\":16336},{\"end\":16709,\"start\":16533},{\"end\":17500,\"start\":16763},{\"end\":18272,\"start\":17530},{\"end\":18733,\"start\":18274},{\"end\":19718,\"start\":18767},{\"end\":20185,\"start\":19720},{\"end\":20287,\"start\":20233},{\"end\":20866,\"start\":20361},{\"end\":21439,\"start\":20868},{\"end\":22156,\"start\":21498},{\"end\":23072,\"start\":22158},{\"end\":23705,\"start\":23577},{\"end\":24349,\"start\":23765},{\"end\":24539,\"start\":24351},{\"end\":24941,\"start\":24569},{\"end\":25431,\"start\":24943},{\"end\":25759,\"start\":25467},{\"end\":27346,\"start\":25761},{\"end\":27874,\"start\":27454},{\"end\":28565,\"start\":27890},{\"end\":29152,\"start\":28567},{\"end\":31602,\"start\":29154},{\"end\":31820,\"start\":31604},{\"end\":32809,\"start\":31843},{\"end\":33039,\"start\":32811},{\"end\":33518,\"start\":33041},{\"end\":34084,\"start\":33520},{\"end\":35529,\"start\":34086},{\"end\":36629,\"start\":35562},{\"end\":37432,\"start\":36631},{\"end\":37663,\"start\":37478},{\"end\":38044,\"start\":37927},{\"end\":38466,\"start\":38075},{\"end\":38622,\"start\":38520},{\"end\":38835,\"start\":38662},{\"end\":39081,\"start\":38884},{\"end\":39432,\"start\":39187},{\"end\":39576,\"start\":39434},{\"end\":39914,\"start\":39594},{\"end\":40077,\"start\":39949},{\"end\":40418,\"start\":40146},{\"end\":40509,\"start\":40442},{\"end\":40853,\"start\":40623},{\"end\":41128,\"start\":41068},{\"end\":41729,\"start\":41353},{\"end\":42440,\"start\":41758},{\"end\":42809,\"start\":42726},{\"end\":42928,\"start\":42861},{\"end\":43216,\"start\":42998},{\"end\":43904,\"start\":43218},{\"end\":44570,\"start\":43906},{\"end\":46037,\"start\":44690}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13333,\"start\":13267},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14260,\"start\":14158},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14977,\"start\":14928},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15866,\"start\":15454},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16762,\"start\":16710},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18766,\"start\":18734},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20232,\"start\":20186},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20360,\"start\":20288},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23576,\"start\":23073},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23764,\"start\":23706},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24568,\"start\":24540},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25466,\"start\":25432},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27453,\"start\":27347},{\"attributes\":{\"id\":\"formula_13\"},\"end\":37926,\"start\":37664},{\"attributes\":{\"id\":\"formula_14\"},\"end\":38661,\"start\":38623},{\"attributes\":{\"id\":\"formula_15\"},\"end\":38883,\"start\":38836},{\"attributes\":{\"id\":\"formula_16\"},\"end\":39113,\"start\":39082},{\"attributes\":{\"id\":\"formula_17\"},\"end\":39593,\"start\":39577},{\"attributes\":{\"id\":\"formula_18\"},\"end\":39948,\"start\":39915},{\"attributes\":{\"id\":\"formula_19\"},\"end\":40145,\"start\":40078},{\"attributes\":{\"id\":\"formula_20\"},\"end\":40441,\"start\":40419},{\"attributes\":{\"id\":\"formula_21\"},\"end\":40622,\"start\":40510},{\"attributes\":{\"id\":\"formula_22\"},\"end\":41067,\"start\":40854},{\"attributes\":{\"id\":\"formula_23\"},\"end\":41352,\"start\":41129},{\"attributes\":{\"id\":\"formula_24\"},\"end\":42725,\"start\":42441},{\"attributes\":{\"id\":\"formula_25\"},\"end\":42860,\"start\":42810},{\"attributes\":{\"id\":\"formula_26\"},\"end\":44689,\"start\":44571}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1598,\"start\":1586},{\"attributes\":{\"n\":\"2.\"},\"end\":7366,\"start\":7354},{\"attributes\":{\"n\":\"3.\"},\"end\":12659,\"start\":12621},{\"attributes\":{\"n\":\"4.\"},\"end\":17528,\"start\":17503},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21496,\"start\":21442},{\"attributes\":{\"n\":\"5.\"},\"end\":27888,\"start\":27877},{\"end\":31841,\"start\":31823},{\"end\":35547,\"start\":35532},{\"attributes\":{\"n\":\"6.\"},\"end\":35560,\"start\":35550},{\"end\":37476,\"start\":37435},{\"end\":38073,\"start\":38047},{\"end\":38518,\"start\":38469},{\"end\":39185,\"start\":39115},{\"end\":41756,\"start\":41732},{\"end\":42996,\"start\":42931},{\"end\":46059,\"start\":46040},{\"end\":46071,\"start\":46061},{\"end\":46851,\"start\":46841},{\"end\":46952,\"start\":46944}]", "table": null, "figure_caption": "[{\"end\":46839,\"start\":46073},{\"end\":46942,\"start\":46853},{\"end\":47274,\"start\":46954}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21423,\"start\":21415},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30451,\"start\":30442},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30873,\"start\":30864},{\"end\":31616,\"start\":31608},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32308,\"start\":32300},{\"end\":32606,\"start\":32598},{\"end\":34103,\"start\":34095},{\"end\":34681,\"start\":34673},{\"end\":44802,\"start\":44794},{\"end\":45296,\"start\":45288}]", "bib_author_first_name": "[{\"end\":47959,\"start\":47958},{\"end\":47970,\"start\":47969},{\"end\":47981,\"start\":47980},{\"end\":48389,\"start\":48388},{\"end\":48400,\"start\":48399},{\"end\":48410,\"start\":48409},{\"end\":48419,\"start\":48418},{\"end\":48421,\"start\":48420},{\"end\":48431,\"start\":48430},{\"end\":48433,\"start\":48432},{\"end\":48899,\"start\":48898},{\"end\":48909,\"start\":48908},{\"end\":48917,\"start\":48916},{\"end\":49383,\"start\":49382},{\"end\":49393,\"start\":49392},{\"end\":49403,\"start\":49402},{\"end\":49832,\"start\":49831},{\"end\":49842,\"start\":49841},{\"end\":49852,\"start\":49851},{\"end\":50134,\"start\":50133},{\"end\":50140,\"start\":50139},{\"end\":50147,\"start\":50146},{\"end\":50437,\"start\":50436},{\"end\":50448,\"start\":50447},{\"end\":50899,\"start\":50898},{\"end\":50908,\"start\":50907},{\"end\":50922,\"start\":50921},{\"end\":50932,\"start\":50931},{\"end\":51274,\"start\":51273},{\"end\":51282,\"start\":51281},{\"end\":51284,\"start\":51283},{\"end\":51530,\"start\":51529},{\"end\":51538,\"start\":51537},{\"end\":51545,\"start\":51544},{\"end\":51551,\"start\":51550},{\"end\":51557,\"start\":51556},{\"end\":51884,\"start\":51883},{\"end\":51892,\"start\":51891},{\"end\":51905,\"start\":51904},{\"end\":52352,\"start\":52351},{\"end\":52361,\"start\":52360},{\"end\":52374,\"start\":52373},{\"end\":52376,\"start\":52375},{\"end\":52858,\"start\":52857},{\"end\":52874,\"start\":52873},{\"end\":52882,\"start\":52881},{\"end\":53341,\"start\":53340},{\"end\":53343,\"start\":53342},{\"end\":53555,\"start\":53554},{\"end\":53566,\"start\":53565},{\"end\":53573,\"start\":53572},{\"end\":53584,\"start\":53583},{\"end\":53595,\"start\":53594},{\"end\":53604,\"start\":53603},{\"end\":53619,\"start\":53612},{\"end\":53623,\"start\":53622},{\"end\":54147,\"start\":54146},{\"end\":54160,\"start\":54159},{\"end\":54169,\"start\":54168},{\"end\":54178,\"start\":54177},{\"end\":54188,\"start\":54187},{\"end\":54195,\"start\":54194},{\"end\":54205,\"start\":54204},{\"end\":54213,\"start\":54212},{\"end\":54222,\"start\":54221},{\"end\":54734,\"start\":54733},{\"end\":54745,\"start\":54744},{\"end\":54747,\"start\":54746},{\"end\":54757,\"start\":54756},{\"end\":54768,\"start\":54767},{\"end\":54770,\"start\":54769},{\"end\":55005,\"start\":55004},{\"end\":55007,\"start\":55006},{\"end\":55021,\"start\":55020},{\"end\":55031,\"start\":55030},{\"end\":55431,\"start\":55430},{\"end\":55437,\"start\":55436},{\"end\":55446,\"start\":55445},{\"end\":55453,\"start\":55452},{\"end\":55747,\"start\":55746},{\"end\":55754,\"start\":55753},{\"end\":55756,\"start\":55755},{\"end\":55766,\"start\":55765},{\"end\":56006,\"start\":56005},{\"end\":56287,\"start\":56286},{\"end\":56296,\"start\":56295},{\"end\":56307,\"start\":56306},{\"end\":56650,\"start\":56649},{\"end\":57043,\"start\":57042},{\"end\":57056,\"start\":57055},{\"end\":57285,\"start\":57284},{\"end\":57296,\"start\":57295},{\"end\":57298,\"start\":57297},{\"end\":57309,\"start\":57308},{\"end\":57544,\"start\":57543},{\"end\":57546,\"start\":57545},{\"end\":57553,\"start\":57552},{\"end\":57947,\"start\":57946},{\"end\":57949,\"start\":57948},{\"end\":57956,\"start\":57955},{\"end\":57970,\"start\":57969},{\"end\":58227,\"start\":58226},{\"end\":58229,\"start\":58228},{\"end\":58585,\"start\":58584},{\"end\":58594,\"start\":58593},{\"end\":58604,\"start\":58603},{\"end\":58614,\"start\":58613},{\"end\":58864,\"start\":58863},{\"end\":58875,\"start\":58874},{\"end\":58888,\"start\":58887},{\"end\":58900,\"start\":58899},{\"end\":58910,\"start\":58906},{\"end\":58914,\"start\":58913},{\"end\":59207,\"start\":59203},{\"end\":59214,\"start\":59213},{\"end\":59222,\"start\":59221},{\"end\":59231,\"start\":59230},{\"end\":59233,\"start\":59232},{\"end\":59559,\"start\":59558},{\"end\":59565,\"start\":59564},{\"end\":59573,\"start\":59572},{\"end\":59585,\"start\":59580},{\"end\":59589,\"start\":59588},{\"end\":59828,\"start\":59827},{\"end\":59835,\"start\":59834},{\"end\":59841,\"start\":59840},{\"end\":59856,\"start\":59855},{\"end\":60256,\"start\":60255},{\"end\":60263,\"start\":60262},{\"end\":60557,\"start\":60556},{\"end\":60559,\"start\":60558},{\"end\":60567,\"start\":60566},{\"end\":60569,\"start\":60568},{\"end\":60577,\"start\":60576},{\"end\":60579,\"start\":60578},{\"end\":60587,\"start\":60586},{\"end\":60596,\"start\":60595},{\"end\":60598,\"start\":60597},{\"end\":60604,\"start\":60603},{\"end\":61193,\"start\":61192},{\"end\":61195,\"start\":61194},{\"end\":61205,\"start\":61204},{\"end\":61217,\"start\":61216},{\"end\":61228,\"start\":61227},{\"end\":61614,\"start\":61613},{\"end\":61746,\"start\":61745},{\"end\":61764,\"start\":61763},{\"end\":61774,\"start\":61773},{\"end\":61786,\"start\":61785},{\"end\":61797,\"start\":61796},{\"end\":61813,\"start\":61812},{\"end\":61815,\"start\":61814},{\"end\":61823,\"start\":61822},{\"end\":62315,\"start\":62314},{\"end\":62328,\"start\":62327},{\"end\":62330,\"start\":62329},{\"end\":62341,\"start\":62340},{\"end\":62343,\"start\":62342},{\"end\":62356,\"start\":62355},{\"end\":62634,\"start\":62633},{\"end\":62646,\"start\":62645},{\"end\":62658,\"start\":62657},{\"end\":62667,\"start\":62666},{\"end\":62669,\"start\":62668},{\"end\":63029,\"start\":63028},{\"end\":63040,\"start\":63039},{\"end\":63049,\"start\":63048},{\"end\":63066,\"start\":63065},{\"end\":63074,\"start\":63073},{\"end\":63309,\"start\":63308},{\"end\":63320,\"start\":63319},{\"end\":63338,\"start\":63337},{\"end\":63340,\"start\":63339},{\"end\":63590,\"start\":63589},{\"end\":63600,\"start\":63599},{\"end\":63602,\"start\":63601},{\"end\":63613,\"start\":63612},{\"end\":63629,\"start\":63628},{\"end\":63904,\"start\":63903},{\"end\":63906,\"start\":63905},{\"end\":63920,\"start\":63919},{\"end\":63930,\"start\":63929},{\"end\":63939,\"start\":63938},{\"end\":63941,\"start\":63940},{\"end\":63954,\"start\":63950},{\"end\":63961,\"start\":63960},{\"end\":63968,\"start\":63967},{\"end\":63976,\"start\":63975},{\"end\":63978,\"start\":63977},{\"end\":64424,\"start\":64423},{\"end\":64434,\"start\":64433},{\"end\":64442,\"start\":64441},{\"end\":64448,\"start\":64447},{\"end\":64457,\"start\":64456},{\"end\":64466,\"start\":64465},{\"end\":64481,\"start\":64480},{\"end\":64878,\"start\":64877},{\"end\":64889,\"start\":64888},{\"end\":64891,\"start\":64890},{\"end\":64900,\"start\":64899},{\"end\":64910,\"start\":64909},{\"end\":64919,\"start\":64918},{\"end\":64929,\"start\":64928},{\"end\":64941,\"start\":64940},{\"end\":65326,\"start\":65325},{\"end\":65334,\"start\":65333},{\"end\":65789,\"start\":65788},{\"end\":65796,\"start\":65795},{\"end\":65798,\"start\":65797},{\"end\":65807,\"start\":65806},{\"end\":66396,\"start\":66395},{\"end\":66407,\"start\":66406},{\"end\":66417,\"start\":66416},{\"end\":66419,\"start\":66418},{\"end\":66663,\"start\":66662},{\"end\":66674,\"start\":66673},{\"end\":66685,\"start\":66684},{\"end\":66698,\"start\":66697},{\"end\":66707,\"start\":66706},{\"end\":66716,\"start\":66715},{\"end\":66735,\"start\":66729},{\"end\":66739,\"start\":66738},{\"end\":67048,\"start\":67047},{\"end\":67059,\"start\":67058},{\"end\":67072,\"start\":67071},{\"end\":67081,\"start\":67080},{\"end\":67091,\"start\":67090},{\"end\":67455,\"start\":67454},{\"end\":67465,\"start\":67464},{\"end\":67476,\"start\":67475},{\"end\":67488,\"start\":67487},{\"end\":67501,\"start\":67500},{\"end\":67509,\"start\":67508},{\"end\":67781,\"start\":67780},{\"end\":67789,\"start\":67788},{\"end\":67795,\"start\":67794},{\"end\":68072,\"start\":68071},{\"end\":68080,\"start\":68079},{\"end\":68088,\"start\":68087},{\"end\":68564,\"start\":68563},{\"end\":68572,\"start\":68571},{\"end\":68582,\"start\":68581},{\"end\":68591,\"start\":68590},{\"end\":68601,\"start\":68600},{\"end\":68611,\"start\":68610},{\"end\":69020,\"start\":69019},{\"end\":69028,\"start\":69027},{\"end\":69034,\"start\":69033},{\"end\":69043,\"start\":69039},{\"end\":69047,\"start\":69046},{\"end\":69348,\"start\":69344},{\"end\":69355,\"start\":69354},{\"end\":69362,\"start\":69361},{\"end\":69364,\"start\":69363},{\"end\":69373,\"start\":69370},{\"end\":69386,\"start\":69385},{\"end\":69388,\"start\":69387},{\"end\":69702,\"start\":69701},{\"end\":69714,\"start\":69713},{\"end\":69723,\"start\":69722},{\"end\":69733,\"start\":69732},{\"end\":70046,\"start\":70045},{\"end\":70053,\"start\":70052},{\"end\":70055,\"start\":70054},{\"end\":70064,\"start\":70063},{\"end\":70070,\"start\":70069},{\"end\":70080,\"start\":70079},{\"end\":70090,\"start\":70089}]", "bib_author_last_name": "[{\"end\":47967,\"start\":47960},{\"end\":47978,\"start\":47971},{\"end\":47988,\"start\":47982},{\"end\":48397,\"start\":48390},{\"end\":48407,\"start\":48401},{\"end\":48416,\"start\":48411},{\"end\":48428,\"start\":48422},{\"end\":48439,\"start\":48434},{\"end\":48906,\"start\":48900},{\"end\":48914,\"start\":48910},{\"end\":48921,\"start\":48918},{\"end\":49390,\"start\":49384},{\"end\":49400,\"start\":49394},{\"end\":49410,\"start\":49404},{\"end\":49839,\"start\":49833},{\"end\":49849,\"start\":49843},{\"end\":49857,\"start\":49853},{\"end\":50137,\"start\":50135},{\"end\":50144,\"start\":50141},{\"end\":50151,\"start\":50148},{\"end\":50445,\"start\":50438},{\"end\":50455,\"start\":50449},{\"end\":50905,\"start\":50900},{\"end\":50919,\"start\":50909},{\"end\":50929,\"start\":50923},{\"end\":50938,\"start\":50933},{\"end\":51279,\"start\":51275},{\"end\":51296,\"start\":51285},{\"end\":51535,\"start\":51531},{\"end\":51542,\"start\":51539},{\"end\":51548,\"start\":51546},{\"end\":51554,\"start\":51552},{\"end\":51562,\"start\":51558},{\"end\":51889,\"start\":51885},{\"end\":51902,\"start\":51893},{\"end\":51912,\"start\":51906},{\"end\":52358,\"start\":52353},{\"end\":52371,\"start\":52362},{\"end\":52383,\"start\":52377},{\"end\":52871,\"start\":52859},{\"end\":52879,\"start\":52875},{\"end\":52890,\"start\":52883},{\"end\":53350,\"start\":53344},{\"end\":53563,\"start\":53556},{\"end\":53570,\"start\":53567},{\"end\":53581,\"start\":53574},{\"end\":53592,\"start\":53585},{\"end\":53601,\"start\":53596},{\"end\":53610,\"start\":53605},{\"end\":54157,\"start\":54148},{\"end\":54166,\"start\":54161},{\"end\":54175,\"start\":54170},{\"end\":54185,\"start\":54179},{\"end\":54192,\"start\":54189},{\"end\":54202,\"start\":54196},{\"end\":54210,\"start\":54206},{\"end\":54219,\"start\":54214},{\"end\":54228,\"start\":54223},{\"end\":54742,\"start\":54735},{\"end\":54754,\"start\":54748},{\"end\":54765,\"start\":54758},{\"end\":54778,\"start\":54771},{\"end\":55018,\"start\":55008},{\"end\":55028,\"start\":55022},{\"end\":55039,\"start\":55032},{\"end\":55434,\"start\":55432},{\"end\":55443,\"start\":55438},{\"end\":55450,\"start\":55447},{\"end\":55457,\"start\":55454},{\"end\":55751,\"start\":55748},{\"end\":55763,\"start\":55757},{\"end\":55772,\"start\":55767},{\"end\":56016,\"start\":56007},{\"end\":56293,\"start\":56288},{\"end\":56304,\"start\":56297},{\"end\":56315,\"start\":56308},{\"end\":57053,\"start\":57044},{\"end\":57062,\"start\":57057},{\"end\":57293,\"start\":57286},{\"end\":57306,\"start\":57299},{\"end\":57314,\"start\":57310},{\"end\":57550,\"start\":57547},{\"end\":57559,\"start\":57554},{\"end\":57953,\"start\":57950},{\"end\":57967,\"start\":57957},{\"end\":57976,\"start\":57971},{\"end\":58232,\"start\":58230},{\"end\":58591,\"start\":58586},{\"end\":58601,\"start\":58595},{\"end\":58611,\"start\":58605},{\"end\":58622,\"start\":58615},{\"end\":58872,\"start\":58865},{\"end\":58885,\"start\":58876},{\"end\":58897,\"start\":58889},{\"end\":58904,\"start\":58901},{\"end\":59211,\"start\":59208},{\"end\":59219,\"start\":59215},{\"end\":59228,\"start\":59223},{\"end\":59242,\"start\":59234},{\"end\":59562,\"start\":59560},{\"end\":59570,\"start\":59566},{\"end\":59578,\"start\":59574},{\"end\":59832,\"start\":59829},{\"end\":59838,\"start\":59836},{\"end\":59853,\"start\":59842},{\"end\":59862,\"start\":59857},{\"end\":60260,\"start\":60257},{\"end\":60267,\"start\":60264},{\"end\":60564,\"start\":60560},{\"end\":60574,\"start\":60570},{\"end\":60584,\"start\":60580},{\"end\":60593,\"start\":60588},{\"end\":60601,\"start\":60599},{\"end\":60610,\"start\":60605},{\"end\":61202,\"start\":61196},{\"end\":61214,\"start\":61206},{\"end\":61225,\"start\":61218},{\"end\":61237,\"start\":61229},{\"end\":61619,\"start\":61615},{\"end\":61761,\"start\":61747},{\"end\":61771,\"start\":61765},{\"end\":61783,\"start\":61775},{\"end\":61794,\"start\":61787},{\"end\":61810,\"start\":61798},{\"end\":61820,\"start\":61816},{\"end\":61828,\"start\":61824},{\"end\":62325,\"start\":62316},{\"end\":62338,\"start\":62331},{\"end\":62353,\"start\":62344},{\"end\":62363,\"start\":62357},{\"end\":62643,\"start\":62635},{\"end\":62655,\"start\":62647},{\"end\":62664,\"start\":62659},{\"end\":62677,\"start\":62670},{\"end\":62682,\"start\":62679},{\"end\":63037,\"start\":63030},{\"end\":63046,\"start\":63041},{\"end\":63063,\"start\":63050},{\"end\":63071,\"start\":63067},{\"end\":63077,\"start\":63075},{\"end\":63317,\"start\":63310},{\"end\":63335,\"start\":63321},{\"end\":63345,\"start\":63341},{\"end\":63597,\"start\":63591},{\"end\":63610,\"start\":63603},{\"end\":63626,\"start\":63614},{\"end\":63639,\"start\":63630},{\"end\":63917,\"start\":63907},{\"end\":63927,\"start\":63921},{\"end\":63936,\"start\":63931},{\"end\":63948,\"start\":63942},{\"end\":63958,\"start\":63955},{\"end\":63965,\"start\":63962},{\"end\":63973,\"start\":63969},{\"end\":63984,\"start\":63979},{\"end\":64431,\"start\":64425},{\"end\":64439,\"start\":64435},{\"end\":64445,\"start\":64443},{\"end\":64454,\"start\":64449},{\"end\":64463,\"start\":64458},{\"end\":64478,\"start\":64467},{\"end\":64488,\"start\":64482},{\"end\":64886,\"start\":64879},{\"end\":64897,\"start\":64892},{\"end\":64907,\"start\":64901},{\"end\":64916,\"start\":64911},{\"end\":64926,\"start\":64920},{\"end\":64938,\"start\":64930},{\"end\":64951,\"start\":64942},{\"end\":65331,\"start\":65327},{\"end\":65343,\"start\":65335},{\"end\":65793,\"start\":65790},{\"end\":65804,\"start\":65799},{\"end\":66404,\"start\":66397},{\"end\":66414,\"start\":66408},{\"end\":66429,\"start\":66420},{\"end\":66671,\"start\":66664},{\"end\":66682,\"start\":66675},{\"end\":66695,\"start\":66686},{\"end\":66704,\"start\":66699},{\"end\":66713,\"start\":66708},{\"end\":66727,\"start\":66717},{\"end\":67056,\"start\":67049},{\"end\":67069,\"start\":67060},{\"end\":67078,\"start\":67073},{\"end\":67088,\"start\":67082},{\"end\":67097,\"start\":67092},{\"end\":67462,\"start\":67456},{\"end\":67473,\"start\":67466},{\"end\":67485,\"start\":67477},{\"end\":67498,\"start\":67489},{\"end\":67506,\"start\":67502},{\"end\":67515,\"start\":67510},{\"end\":67786,\"start\":67782},{\"end\":67792,\"start\":67790},{\"end\":67801,\"start\":67796},{\"end\":68077,\"start\":68073},{\"end\":68085,\"start\":68081},{\"end\":68095,\"start\":68089},{\"end\":68569,\"start\":68565},{\"end\":68579,\"start\":68573},{\"end\":68588,\"start\":68583},{\"end\":68598,\"start\":68592},{\"end\":68608,\"start\":68602},{\"end\":68616,\"start\":68612},{\"end\":69025,\"start\":69021},{\"end\":69031,\"start\":69029},{\"end\":69037,\"start\":69035},{\"end\":69352,\"start\":69349},{\"end\":69359,\"start\":69356},{\"end\":69368,\"start\":69365},{\"end\":69383,\"start\":69374},{\"end\":69711,\"start\":69703},{\"end\":69720,\"start\":69715},{\"end\":69730,\"start\":69724},{\"end\":69740,\"start\":69734},{\"end\":70050,\"start\":70047},{\"end\":70061,\"start\":70056},{\"end\":70067,\"start\":70065},{\"end\":70077,\"start\":70071},{\"end\":70087,\"start\":70081},{\"end\":70100,\"start\":70091}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3310672},\"end\":48353,\"start\":47857},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15523031},\"end\":48855,\"start\":48355},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":995480},\"end\":49325,\"start\":48857},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2649314},\"end\":49772,\"start\":49327},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10022042},\"end\":50088,\"start\":49774},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14239489},\"end\":50355,\"start\":50090},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":207599948},\"end\":50834,\"start\":50357},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49865868},\"end\":51175,\"start\":50836},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b8\",\"matched_paper_id\":52163004},\"end\":51527,\"start\":51177},{\"attributes\":{\"doi\":\"arXiv:1712.05526\",\"id\":\"b9\"},\"end\":51826,\"start\":51529},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1831685},\"end\":52290,\"start\":51828},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":59842968},\"end\":52787,\"start\":52292},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":44166256},\"end\":53283,\"start\":52789},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":51987509},\"end\":53473,\"start\":53285},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6161478},\"end\":54058,\"start\":53475},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":213452491},\"end\":54685,\"start\":54060},{\"attributes\":{\"doi\":\"arXiv:1703.00410\",\"id\":\"b16\"},\"end\":54954,\"start\":54687},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6706414},\"end\":55382,\"start\":54956},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":55700,\"start\":55384},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":923145},\"end\":55928,\"start\":55702},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16135158},\"end\":56210,\"start\":55930},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219661119},\"end\":56561,\"start\":56212},{\"attributes\":{\"id\":\"b22\"},\"end\":56972,\"start\":56563},{\"attributes\":{\"doi\":\"arXiv:1809.08055\",\"id\":\"b23\"},\"end\":57230,\"start\":56974},{\"attributes\":{\"doi\":\"arXiv:1803.03241\",\"id\":\"b24\"},\"end\":57480,\"start\":57232},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":13193974},\"end\":57878,\"start\":57482},{\"attributes\":{\"doi\":\"arXiv:1811.00741\",\"id\":\"b26\"},\"end\":58154,\"start\":57880},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206741597},\"end\":58501,\"start\":58156},{\"attributes\":{\"id\":\"b28\"},\"end\":58789,\"start\":58503},{\"attributes\":{\"doi\":\"arXiv:1802.03471\",\"id\":\"b29\"},\"end\":59121,\"start\":58791},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":204904840},\"end\":59493,\"start\":59123},{\"attributes\":{\"doi\":\"arXiv:1809.03113\",\"id\":\"b31\"},\"end\":59767,\"start\":59495},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":721235},\"end\":60193,\"start\":59769},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":11777930},\"end\":60508,\"start\":60195},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1428702},\"end\":61150,\"start\":60510},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7071211},\"end\":61582,\"start\":61152},{\"attributes\":{\"id\":\"b36\"},\"end\":61664,\"start\":61584},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12424035},\"end\":62284,\"start\":61666},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":423350},\"end\":62589,\"start\":62286},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6726938},\"end\":62947,\"start\":62591},{\"attributes\":{\"id\":\"b40\"},\"end\":63245,\"start\":62949},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3678260},\"end\":63537,\"start\":63247},{\"attributes\":{\"doi\":\"arXiv:1802.06485\",\"id\":\"b42\"},\"end\":63823,\"start\":63539},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1553099},\"end\":64421,\"start\":63825},{\"attributes\":{\"doi\":\"arXiv:1906.04584\",\"id\":\"b44\"},\"end\":64802,\"start\":64423},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":4626477},\"end\":65250,\"start\":64804},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":67750403},\"end\":65739,\"start\":65252},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":35426171},\"end\":66343,\"start\":65741},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":54612551},\"end\":66660,\"start\":66345},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b49\"},\"end\":66986,\"start\":66662},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":206593880},\"end\":67378,\"start\":66988},{\"attributes\":{\"doi\":\"arXiv:1908.04473\",\"id\":\"b51\"},\"end\":67737,\"start\":67380},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":53298804},\"end\":68010,\"start\":67739},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2120593},\"end\":68499,\"start\":68012},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5077922},\"end\":69017,\"start\":68501},{\"attributes\":{\"doi\":\"arXiv:1703.01340\",\"id\":\"b55\"},\"end\":69277,\"start\":69019},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":53741665},\"end\":69643,\"start\":69279},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":362467},\"end\":69979,\"start\":69645},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":155092992},\"end\":70536,\"start\":69981}]", "bib_title": "[{\"end\":47956,\"start\":47857},{\"end\":48386,\"start\":48355},{\"end\":48896,\"start\":48857},{\"end\":49380,\"start\":49327},{\"end\":49829,\"start\":49774},{\"end\":50131,\"start\":50090},{\"end\":50434,\"start\":50357},{\"end\":50896,\"start\":50836},{\"end\":51271,\"start\":51177},{\"end\":51881,\"start\":51828},{\"end\":52349,\"start\":52292},{\"end\":52855,\"start\":52789},{\"end\":53338,\"start\":53285},{\"end\":53552,\"start\":53475},{\"end\":54144,\"start\":54060},{\"end\":55002,\"start\":54956},{\"end\":55428,\"start\":55384},{\"end\":55744,\"start\":55702},{\"end\":56003,\"start\":55930},{\"end\":56284,\"start\":56212},{\"end\":56647,\"start\":56563},{\"end\":57541,\"start\":57482},{\"end\":58224,\"start\":58156},{\"end\":59201,\"start\":59123},{\"end\":59825,\"start\":59769},{\"end\":60253,\"start\":60195},{\"end\":60554,\"start\":60510},{\"end\":61190,\"start\":61152},{\"end\":61743,\"start\":61666},{\"end\":62312,\"start\":62286},{\"end\":62631,\"start\":62591},{\"end\":63306,\"start\":63247},{\"end\":63901,\"start\":63825},{\"end\":64875,\"start\":64804},{\"end\":65323,\"start\":65252},{\"end\":65786,\"start\":65741},{\"end\":66393,\"start\":66345},{\"end\":67045,\"start\":66988},{\"end\":67778,\"start\":67739},{\"end\":68069,\"start\":68012},{\"end\":68561,\"start\":68501},{\"end\":69342,\"start\":69279},{\"end\":69699,\"start\":69645},{\"end\":70043,\"start\":69981}]", "bib_author": "[{\"end\":47969,\"start\":47958},{\"end\":47980,\"start\":47969},{\"end\":47990,\"start\":47980},{\"end\":48399,\"start\":48388},{\"end\":48409,\"start\":48399},{\"end\":48418,\"start\":48409},{\"end\":48430,\"start\":48418},{\"end\":48441,\"start\":48430},{\"end\":48908,\"start\":48898},{\"end\":48916,\"start\":48908},{\"end\":48923,\"start\":48916},{\"end\":49392,\"start\":49382},{\"end\":49402,\"start\":49392},{\"end\":49412,\"start\":49402},{\"end\":49841,\"start\":49831},{\"end\":49851,\"start\":49841},{\"end\":49859,\"start\":49851},{\"end\":50139,\"start\":50133},{\"end\":50146,\"start\":50139},{\"end\":50153,\"start\":50146},{\"end\":50447,\"start\":50436},{\"end\":50457,\"start\":50447},{\"end\":50907,\"start\":50898},{\"end\":50921,\"start\":50907},{\"end\":50931,\"start\":50921},{\"end\":50940,\"start\":50931},{\"end\":51281,\"start\":51273},{\"end\":51298,\"start\":51281},{\"end\":51537,\"start\":51529},{\"end\":51544,\"start\":51537},{\"end\":51550,\"start\":51544},{\"end\":51556,\"start\":51550},{\"end\":51564,\"start\":51556},{\"end\":51891,\"start\":51883},{\"end\":51904,\"start\":51891},{\"end\":51914,\"start\":51904},{\"end\":52360,\"start\":52351},{\"end\":52373,\"start\":52360},{\"end\":52385,\"start\":52373},{\"end\":52873,\"start\":52857},{\"end\":52881,\"start\":52873},{\"end\":52892,\"start\":52881},{\"end\":53352,\"start\":53340},{\"end\":53565,\"start\":53554},{\"end\":53572,\"start\":53565},{\"end\":53583,\"start\":53572},{\"end\":53594,\"start\":53583},{\"end\":53603,\"start\":53594},{\"end\":53612,\"start\":53603},{\"end\":53622,\"start\":53612},{\"end\":53626,\"start\":53622},{\"end\":54159,\"start\":54146},{\"end\":54168,\"start\":54159},{\"end\":54177,\"start\":54168},{\"end\":54187,\"start\":54177},{\"end\":54194,\"start\":54187},{\"end\":54204,\"start\":54194},{\"end\":54212,\"start\":54204},{\"end\":54221,\"start\":54212},{\"end\":54230,\"start\":54221},{\"end\":54744,\"start\":54733},{\"end\":54756,\"start\":54744},{\"end\":54767,\"start\":54756},{\"end\":54780,\"start\":54767},{\"end\":55020,\"start\":55004},{\"end\":55030,\"start\":55020},{\"end\":55041,\"start\":55030},{\"end\":55436,\"start\":55430},{\"end\":55445,\"start\":55436},{\"end\":55452,\"start\":55445},{\"end\":55459,\"start\":55452},{\"end\":55753,\"start\":55746},{\"end\":55765,\"start\":55753},{\"end\":55774,\"start\":55765},{\"end\":56018,\"start\":56005},{\"end\":56295,\"start\":56286},{\"end\":56306,\"start\":56295},{\"end\":56317,\"start\":56306},{\"end\":56653,\"start\":56649},{\"end\":57055,\"start\":57042},{\"end\":57064,\"start\":57055},{\"end\":57295,\"start\":57284},{\"end\":57308,\"start\":57295},{\"end\":57316,\"start\":57308},{\"end\":57552,\"start\":57543},{\"end\":57561,\"start\":57552},{\"end\":57955,\"start\":57946},{\"end\":57969,\"start\":57955},{\"end\":57978,\"start\":57969},{\"end\":58234,\"start\":58226},{\"end\":58593,\"start\":58584},{\"end\":58603,\"start\":58593},{\"end\":58613,\"start\":58603},{\"end\":58624,\"start\":58613},{\"end\":58874,\"start\":58863},{\"end\":58887,\"start\":58874},{\"end\":58899,\"start\":58887},{\"end\":58906,\"start\":58899},{\"end\":58913,\"start\":58906},{\"end\":58917,\"start\":58913},{\"end\":59213,\"start\":59203},{\"end\":59221,\"start\":59213},{\"end\":59230,\"start\":59221},{\"end\":59244,\"start\":59230},{\"end\":59564,\"start\":59558},{\"end\":59572,\"start\":59564},{\"end\":59580,\"start\":59572},{\"end\":59588,\"start\":59580},{\"end\":59592,\"start\":59588},{\"end\":59834,\"start\":59827},{\"end\":59840,\"start\":59834},{\"end\":59855,\"start\":59840},{\"end\":59864,\"start\":59855},{\"end\":60262,\"start\":60255},{\"end\":60269,\"start\":60262},{\"end\":60566,\"start\":60556},{\"end\":60576,\"start\":60566},{\"end\":60586,\"start\":60576},{\"end\":60595,\"start\":60586},{\"end\":60603,\"start\":60595},{\"end\":60612,\"start\":60603},{\"end\":61204,\"start\":61192},{\"end\":61216,\"start\":61204},{\"end\":61227,\"start\":61216},{\"end\":61239,\"start\":61227},{\"end\":61621,\"start\":61613},{\"end\":61763,\"start\":61745},{\"end\":61773,\"start\":61763},{\"end\":61785,\"start\":61773},{\"end\":61796,\"start\":61785},{\"end\":61812,\"start\":61796},{\"end\":61822,\"start\":61812},{\"end\":61830,\"start\":61822},{\"end\":62327,\"start\":62314},{\"end\":62340,\"start\":62327},{\"end\":62355,\"start\":62340},{\"end\":62365,\"start\":62355},{\"end\":62645,\"start\":62633},{\"end\":62657,\"start\":62645},{\"end\":62666,\"start\":62657},{\"end\":62679,\"start\":62666},{\"end\":62684,\"start\":62679},{\"end\":63039,\"start\":63028},{\"end\":63048,\"start\":63039},{\"end\":63065,\"start\":63048},{\"end\":63073,\"start\":63065},{\"end\":63079,\"start\":63073},{\"end\":63319,\"start\":63308},{\"end\":63337,\"start\":63319},{\"end\":63347,\"start\":63337},{\"end\":63599,\"start\":63589},{\"end\":63612,\"start\":63599},{\"end\":63628,\"start\":63612},{\"end\":63641,\"start\":63628},{\"end\":63919,\"start\":63903},{\"end\":63929,\"start\":63919},{\"end\":63938,\"start\":63929},{\"end\":63950,\"start\":63938},{\"end\":63960,\"start\":63950},{\"end\":63967,\"start\":63960},{\"end\":63975,\"start\":63967},{\"end\":63986,\"start\":63975},{\"end\":64433,\"start\":64423},{\"end\":64441,\"start\":64433},{\"end\":64447,\"start\":64441},{\"end\":64456,\"start\":64447},{\"end\":64465,\"start\":64456},{\"end\":64480,\"start\":64465},{\"end\":64490,\"start\":64480},{\"end\":64888,\"start\":64877},{\"end\":64899,\"start\":64888},{\"end\":64909,\"start\":64899},{\"end\":64918,\"start\":64909},{\"end\":64928,\"start\":64918},{\"end\":64940,\"start\":64928},{\"end\":64953,\"start\":64940},{\"end\":65333,\"start\":65325},{\"end\":65345,\"start\":65333},{\"end\":65795,\"start\":65788},{\"end\":65806,\"start\":65795},{\"end\":65810,\"start\":65806},{\"end\":66406,\"start\":66395},{\"end\":66416,\"start\":66406},{\"end\":66431,\"start\":66416},{\"end\":66673,\"start\":66662},{\"end\":66684,\"start\":66673},{\"end\":66697,\"start\":66684},{\"end\":66706,\"start\":66697},{\"end\":66715,\"start\":66706},{\"end\":66729,\"start\":66715},{\"end\":66738,\"start\":66729},{\"end\":66742,\"start\":66738},{\"end\":67058,\"start\":67047},{\"end\":67071,\"start\":67058},{\"end\":67080,\"start\":67071},{\"end\":67090,\"start\":67080},{\"end\":67099,\"start\":67090},{\"end\":67464,\"start\":67454},{\"end\":67475,\"start\":67464},{\"end\":67487,\"start\":67475},{\"end\":67500,\"start\":67487},{\"end\":67508,\"start\":67500},{\"end\":67517,\"start\":67508},{\"end\":67788,\"start\":67780},{\"end\":67794,\"start\":67788},{\"end\":67803,\"start\":67794},{\"end\":68079,\"start\":68071},{\"end\":68087,\"start\":68079},{\"end\":68097,\"start\":68087},{\"end\":68571,\"start\":68563},{\"end\":68581,\"start\":68571},{\"end\":68590,\"start\":68581},{\"end\":68600,\"start\":68590},{\"end\":68610,\"start\":68600},{\"end\":68618,\"start\":68610},{\"end\":69027,\"start\":69019},{\"end\":69033,\"start\":69027},{\"end\":69039,\"start\":69033},{\"end\":69046,\"start\":69039},{\"end\":69050,\"start\":69046},{\"end\":69354,\"start\":69344},{\"end\":69361,\"start\":69354},{\"end\":69370,\"start\":69361},{\"end\":69385,\"start\":69370},{\"end\":69391,\"start\":69385},{\"end\":69713,\"start\":69701},{\"end\":69722,\"start\":69713},{\"end\":69732,\"start\":69722},{\"end\":69742,\"start\":69732},{\"end\":70052,\"start\":70045},{\"end\":70063,\"start\":70052},{\"end\":70069,\"start\":70063},{\"end\":70079,\"start\":70069},{\"end\":70089,\"start\":70079},{\"end\":70102,\"start\":70089}]", "bib_venue": "[{\"end\":48058,\"start\":47990},{\"end\":48531,\"start\":48441},{\"end\":49012,\"start\":48923},{\"end\":49467,\"start\":49412},{\"end\":49910,\"start\":49859},{\"end\":50202,\"start\":50153},{\"end\":50533,\"start\":50457},{\"end\":50989,\"start\":50940},{\"end\":51326,\"start\":51307},{\"end\":51651,\"start\":51580},{\"end\":51982,\"start\":51914},{\"end\":52453,\"start\":52385},{\"end\":52969,\"start\":52892},{\"end\":53362,\"start\":53352},{\"end\":53694,\"start\":53626},{\"end\":54286,\"start\":54230},{\"end\":54731,\"start\":54687},{\"end\":55097,\"start\":55041},{\"end\":55528,\"start\":55459},{\"end\":55793,\"start\":55774},{\"end\":56054,\"start\":56018},{\"end\":56366,\"start\":56317},{\"end\":56734,\"start\":56653},{\"end\":57040,\"start\":56974},{\"end\":57282,\"start\":57232},{\"end\":57629,\"start\":57561},{\"end\":57944,\"start\":57880},{\"end\":58311,\"start\":58234},{\"end\":58582,\"start\":58503},{\"end\":58861,\"start\":58791},{\"end\":59296,\"start\":59244},{\"end\":59556,\"start\":59495},{\"end\":59952,\"start\":59864},{\"end\":60331,\"start\":60269},{\"end\":60728,\"start\":60612},{\"end\":61295,\"start\":61239},{\"end\":61611,\"start\":61584},{\"end\":61906,\"start\":61830},{\"end\":62414,\"start\":62365},{\"end\":62747,\"start\":62684},{\"end\":63026,\"start\":62949},{\"end\":63371,\"start\":63347},{\"end\":63587,\"start\":63539},{\"end\":64055,\"start\":63986},{\"end\":64582,\"start\":64506},{\"end\":65002,\"start\":64953},{\"end\":65413,\"start\":65345},{\"end\":65899,\"start\":65810},{\"end\":66480,\"start\":66431},{\"end\":66797,\"start\":66757},{\"end\":67168,\"start\":67099},{\"end\":67452,\"start\":67380},{\"end\":67852,\"start\":67803},{\"end\":68167,\"start\":68097},{\"end\":68686,\"start\":68618},{\"end\":69124,\"start\":69066},{\"end\":69440,\"start\":69391},{\"end\":69791,\"start\":69742},{\"end\":70170,\"start\":70102},{\"end\":48113,\"start\":48060},{\"end\":48625,\"start\":48533},{\"end\":49106,\"start\":49014},{\"end\":49525,\"start\":49469},{\"end\":50613,\"start\":50535},{\"end\":52058,\"start\":51984},{\"end\":52535,\"start\":52455},{\"end\":53040,\"start\":52971},{\"end\":53762,\"start\":53696},{\"end\":54309,\"start\":54288},{\"end\":55117,\"start\":55099},{\"end\":57684,\"start\":57631},{\"end\":60851,\"start\":60730},{\"end\":61311,\"start\":61297},{\"end\":61986,\"start\":61908},{\"end\":63377,\"start\":63373},{\"end\":64128,\"start\":64057},{\"end\":65495,\"start\":65415},{\"end\":65978,\"start\":65901},{\"end\":68267,\"start\":68169},{\"end\":68754,\"start\":68688},{\"end\":70252,\"start\":70172}]"}}}, "year": 2023, "month": 12, "day": 17}