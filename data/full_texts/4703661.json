{"id": 4703661, "updated": "2023-09-29 05:51:46.638", "metadata": {"title": "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks", "authors": "[{\"first\":\"Zhao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Vijay\",\"last\":\"Badrinarayanan\",\"middle\":[]},{\"first\":\"Chen-Yu\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Rabinovich\",\"middle\":[]}]", "venue": "Proceedings of the 35th International Conference on Machine Learning (2018), 793-802", "journal": null, "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1711.02257", "mag": "2962743139", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ChenBLR18", "doi": null}}, "content": {"source": {"pdf_hash": "78f5a6004c720a9ba63b64dfacd5efd731b580a1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.02257v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ffd6383a19dc1e1587ed08eaea95c835787b1320", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/78f5a6004c720a9ba63b64dfacd5efd731b580a1.txt", "contents": "\nGradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks GRADNORM: GRADIENT NORMALIZATION FOR ADAPTIVE LOSS BALANCING IN DEEP MULTITASK NETWORKS\n\n\nZhao Chen zchen@magicleap.com \nMagic Leap, Inc. Sunnyvale\n94089CAUSA\n\nVijay Badrinarayanan vbadrinarayanan@magicleap.com \nMagic Leap, Inc. Sunnyvale\n94089CAUSA\n\nChen-Yu Lee clee@magicleap.com \nMagic Leap, Inc. Sunnyvale\n94089CAUSA\n\nAndrew Rabinovich arabinovich@magicleap.com \nMagic Leap, Inc. Sunnyvale\n94089CAUSA\n\nGradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks GRADNORM: GRADIENT NORMALIZATION FOR ADAPTIVE LOSS BALANCING IN DEEP MULTITASK NETWORKS\n\nDeep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (Grad-Norm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter \u03b1. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\nINTRODUCTION\n\nSingle-task learning in computer vision has enjoyed much success in deep learning, with many models now performing at or beyond human accuracies for a wide array of tasks. However, a system that strives for full scene understanding cannot focus on one problem, but needs to perform many diverse perceptual tasks simultaneously. Such systems must also be efficient, especially within the restrictions of limited compute environments in embedded systems such as smartphones, wearable devices, and robots/drones. Multitask learning most naturally lends itself to this problem by sharing weights amongst different tasks within the same model and producing multiple predictions in one forward pass. Such networks are not only scalable, but the shared features within these networks tend to be better regularized and boost performance as a result. In the ideal limit, we can thus have the best of both worlds with multitask networks: both more efficiency and higher performance.\n\nThe key difficulty in multitask learning lies in the balancing of tasks, and perhaps the simplest way to control this balance is to choose the correct joint loss function. In practice, the multitask loss function is often assumed to be linear in the single task losses, L = i w i L i , where the sum runs over T tasks. The challenge is then to find the best value for each w i that balances the contribution of each task for optimal model training. Our proposed method is furthermore an adaptive method, allowing w i to vary with the training step t, and so w i = w i (t).\n\nOur key insight lies in the observation that these w i (t) influence training only because they control the magnitude of the gradients generated from task i. As such, manipulating the gradient norms themselves would be a more direct way to control the training dynamics. More specifically, we propose a simple heuristic that penalizes the network when backpropagated gradients from any task Figure 1: Gradient Normalization. Imbalanced gradient norms (left) result in suboptimal training within a multitask network, so we implement a novel gradient loss L grad (right) which detects such imbalances in gradient norms amongst tasks and tunes the weights in the loss function to compensate. We illustrate here a simplified case where such balancing results in equalized gradient norms, but in general some tasks may need higher or lower gradient norms relative to other tasks for optimal task balancing (discussed further in Section 3).\n\nare too large or too small. The correct balance is struck when tasks are training at similar rates; if task i is training relatively quickly, then its weight w i (t) should decrease relative to other task weights w j (t)| j =i to allow other tasks more influence on the network. Our method can be said to be a form of batch normalization (Ioffe & Szegedy (2015)) for backpropagation, ensuring that gradients from each task per batch lie on a common statistical scale. We will show that, when implemented, gradient normalization leads to across-the-board improvements in accuracy and suppresses overfitting.\n\nOur main contributions to the field of multitask learning are as follows:\n\n1. An attractively simple heuristic for multitask loss balancing involving training rate equalization, which is implemented through a novel gradient loss function.\n\n\n2.\n\nA simplification to exhaustive grid search (which has compute complexity O(N T ) for N grid points in one dimension) that only involves tuning one robust hyperparameter.\n\n3. Demonstration that direct interaction with gradients provides a powerful way of reasoning about multitask learning.\n\n\nRELATED WORK\n\nMultitask learning has existed well before the advent of deep learning (Caruana (1998); Bakker & Heskes (2003)), but the robust learned features within deep networks have spurned renewed interest. Although our primary application area is computer vision, multitask learning has applications in multiple other fields, from natural language processing (Hashimoto et al. (2016); Collobert & Weston (2008); S\u00f8gaard & Goldberg (2016)) to speech synthesis (Wu et al. (2015); Seltzer & Droppo (2013)), from very domain-specific applications like traffic prediction (Huang et al. (2014)) to very general cross-domain work (Bilen & Vedaldi (2017)).\n\nMultitask learning is very well-suited to the field of computer vision, where making multiple robust predictions is crucial for complete scene understanding. Deep networks have been used to solve various subsets of multiple vision tasks, from 3-task networks (Eigen & Fergus (2015); Teichmann et al. (2016)) to much larger subsets as in UberNet (Kokkinos (2016)). Often, single computer vision problems can even be framed as multitask problems, such as in Mask R-CNN for instance segmentation (He et al. (2017)) or YOLO-9000 for object detection (Redmon & Farhadi (2016)). Researchers often assume a fixed loss function or network architecture, but there has also been significant work on finding optimal ways to relate tasks to each other in a multitask model. Clustering methods have shown success beyond deep models (Kang et al. (2011);Jacob et al. (2009)), while constructs such as deep relationship networks (Long & Wang (2015)) and cross-stich networks (Misra et al. (2016)) search for meaningful relationships between tasks and learn which features to share between them. Work in Warde-Farley et al. (2014) and Lu et al. (2016) use groupings amongst labels to search through possible architectures for learning. Perhaps the most relevant to the current work, Kendall et al. (2017) uses a joint likelihood formulation to derive task weights based on the intrinsic uncertainty in each task.\n\n\nMETHODOLOGY\n\n\nA GRADIENT LOSS FUNCTION BASED ON RATE BALANCING\n\nWe begin with the standard multitask loss function with time dependency, L(t) = w i (t)L i (t), and our goal is to learn the functions w i (t). We argued in Section 1 that w i (t) is intimately related to the norm of gradients from each task backpropagated into the network. We thus must motivate a set of desirable gradient magnitudes, and use those desired magnitudes to set the task weights w i (t).\n\nConsider the norms of gradients from task i on some set of weights W within the network, norm(\u2207 W L i (t)) (specific choices for W to be discussed later). Our method of gradient normalization (hereafter referred to as GradNorm) works in two steps: (1) We first scale all gradient norms to an equal value as a neutral starting point. This value is most naturally chosen to be the average gradient norm amongst tasks, E task [norm(\u2207 W L i (t))], where we use E task [X] to denote the average value of a task-dependent quantity X across tasks. (2) We then modify gradient norms with a rate balancing term that ensures no task trains relatively too slowly. The gradient norms of task i should grow when task i trains relatively slowly, thereby boosting more sluggish tasks. Gradient norms thus should be an increasing function of the relative inverse training rate for each task.\n\nTo quantify training rates, we choose the loss ratio of task i at training step t, L i (t) := L i (t)/L i (0), as a measure of task i's inverse training rate; smaller values of L i (t) would mean that task i has trained more. If L i (t) denotes the inverse training rate of task i, then the relative inverse training rate is just L i (t)/E task [L i (t)]. Using this simple loss ratio metric is valid for both regression squared loss and classification cross-entropy loss, as we will see in Section 5.2 1 .\n\nOur desired gradient norms are therefore:\nnorm(\u2207 W L i (t)) \u2192 (average gradient norm) \u00d7 (relative inverse training rate of task i) \u03b1 = E task [norm(\u2207 W L(t))] L i (t) E task [L i (t)] \u03b1 (1)\nwhere \u03b1 is an additional hyperparameter. \u03b1 sets the strength of rate balancing in the multitask problem, and also is a measure of the asymmetry between tasks. In cases where tasks are very different in their complexity, leading to different learning dynamics, a higher value of \u03b1 should be used to pull tasks back towards a common training rate more forcefully. When tasks are more symmetric (e.g. the synthetic examples in Section 4), a lower value of \u03b1 is appropriate. Note that \u03b1 = 0 will always try to pin the norms of backpropped gradients from each task to be equal at W . Equation 1 sets a desired target for our gradient norms, and we want to update our loss weights w i (t) to move gradient norms towards this target. To accomplish this, GradNorm is implemented as a loss function L grad which is just the L1 distance between actual gradient norms and the targets in Equation 1:\nL (i) grad (t; W ) = |norm(\u2207 W L i (t)) \u2212 E task [norm(\u2207 W L(t))] L i (t) E task [L (t)] \u03b1 |.\n(2)\n\nThe above loss is for one task; the full loss is just the mean of the individual task losses,\nL grad (t; W ) = (1/T ) i L (i) grad (t; W )\n. L grad is then differentiated with respect to each w i (t), and its gradients are applied via standard update rules to update these weights (see Figure 1 for a schematic view). In principle, it is also possible to update all network weights (not just w i (t)) based on gradient of L grad , but in practice this adds undue complexity to the problem and often degrades performance.\n\nWe can choose W , the weights upon which we rate balance gradient norms, to be any subset of weights within layers of our network. In practice, in order to save on compute overhead, we choose W to be the weights in the last layer which is shared amongst all three tasks. This simplification greatly shortens the number of layers L grad must be backpropagated through, and with this choice of W in our experiments GradNorm only adds \u223c 5% of additional compute time. After every update step, we also renormalize the weights w i (t) so that i w i (t) = T in order to decouple gradient normalization from the global learning rate.\n\n\nA SIMPLE TOY EXAMPLE\n\nTo illustrate GradNorm on a simple system, we consider T regression tasks onto the functions\nf i (x) = \u03c3 i tanh((B + i )x),(3)\nwhere tanh acts element-wise. We use squared loss to train each task. The matrices B and i have elements generated IID from N (0, 10) and N (0, 3.5), respectively. Our task is thus to perform regression on multiple tasks with shared information B along with information specific to each task, i . The \u03c3 i are fixed scalars which set the variance of the outputs f i . Higher values of \u03c3 i induce higher values of squared loss for that task. These tasks are harder to learn due to the higher variances in their response values, but they also backpropagate larger gradients. Classically, such a scenario can lead to suboptimal training dynamics as the higher \u03c3 i tasks tend to dominate the training.\n\nAll toy problem runs use a 4-layer fully-connected ReLU-activated network with 100 neurons per layer as a common trunk. A final affine transformation per task gives T final predictions. Inputs are in R 250 , and outputs lie in R 100 . To ensure consistency, we only compare models initialized to the same random values and fed data generated from a fixed random seed. The asymmetry \u03b1 is set low to 0.12 for these experiments, as the output functions f i are all of the same form.\n\nIn these toy problems, we measure the task-normalized test-time loss, which is the sum of the test loss ratios for each task, i L i (t). A simple sum of losses is wholly inadequate to judge the overall performance of a multitask network, as it biases itself towards tasks with higher loss scales, and there exists no general metric by which to judge multitask performance in any setting. Luckily, our toy problem was designed with tasks which are statistically identical except for their loss scales \u03c3 i . For this simple example, there is therefore a clear measure of overall network performance, which is the sum of losses with each loss normalized to its \u03c3 i -precisely the sum of loss ratios.\n\nIn the case of T = 2, we choose the values (\u03c3 0 , \u03c3 1 ) = (1.0, 100.0). Classically, task 1 can suppress task 0's influence during training due to its higher loss scale. As shown in the top panels of Figure  2, gradient normalization remedies the issue by increasing w 0 (t) to counteract the larger gradients coming from T 1 , and the improved task balance results in better test-time performance.\n\nThe possible benefits of gradient normalization become even clearer when the number of tasks increases. For T = 10, we sample the \u03c3 i from a normal distribution and plot the results in the bottom row of Figure 2. GradNorm significantly improves test time performance over naively weighting each task the same. Like T = 2, for T = 10 the w i (t) grow larger for smaller \u03c3 i tasks; GradNorm is giving tasks with smaller loss scales more breathing room.\n\nFor both T = 2 and T = 10, GradNorm is more stable and outperforms the uncertainty weighting proposed by Kendall et al. (2017). Uncertainty weighting, which enforces that w i (t) \u223c 1/L i (t), Figure 2: Gradient Normalization on a toy 2-task (top) and 10-task (bottom) system. Diagrams of the network structure with loss scales are on the left, traces of w i (t) during training in the middle, and task-normalized test loss curves on the right. \u03b1 = 0.12 for all runs.\n\ntends to grow weights too large and too quickly as the loss for each task drops. Although such networks train quickly at the onset, the training soon crashes as the global learning rate grows too large. This issue is exacerbated as uncertainty weighting allows w i (t) to change unconstrained (compared to GradNorm which ensures w i (t) = T always), which pushes global learning rate up even further.\n\nOverall, the traces for each w i (t) during a single GradNorm run seem fairly stable and convergent. In fact, in Section 5.3 we will see how the time-averaged weights E t [w i (t)] lie close to the optimal static weights, suggesting GradNorm can greatly simplify the tedious grid search procedure.\n\n\nAPPLICATION TO A LARGE REAL-WORLD DATASET\n\nWe primarily use NYUv2 as our dataset of choice. The standard NYUv2 dataset carries depth, surface normals, and semantic segmentation labels (which we cluster into 13 distinct classes). NYUv2 is quite small as a dataset, with a training split of \u223c800 examples, but contains both regression and classification labels, making it a good choice to test the robustness of GradNorm.\n\nTo show GradNorm in action on a more large-scale multitask dataset, we also expand NYUv2 to 40,000 images complete with pixel-wise depth, surface normals, and room keypoint labels. Keypoint labels are obtained through professional human labeling services, while surface normals are generated from camera parameters and the depth maps through standard methods.\n\nFollowing Lee et al. (2017), the state-of-the-art in room layout prediction, all inputs are downsampled to 320 x 320 pixels and outputs to 80 x 80 pixels. These resolutions also speed up training without compromising complexity in the inputs or labels.\n\n\nMODEL AND INDIVIDUAL TASK LOSSES\n\nWe try two different models: (1) a SegNet (Badrinarayanan et al. (2015); Lee et al. (2017)) network with a symmetric VGG16 (Simonyan & Zisserman (2014)) encoder/decoder, and (2) an FCN ) network with a modified ResNet-50 (He et al. (2016) learns all upsampling filters. The ResNet architecture is further thinned (both in its filters and activations) to contrast with the heavier, more complex VGG SegNet: stride-2 layers are moved earlier and all 2048-filter layers are replaced by 1024-filter layers. Ultimately, the VGG SegNet has 29M parameters versus 15M for the thin ResNet. Although we will focus on the VGG SegNet in our more in-depth analysis, by designing and testing on two extremely different network topologies we will further demonstrate that GradNorm is very robust to the choice of base model.\n\nWe use standard pixel-wise loss functions for each task: cross entropy for segmentation, squared loss for depth, and cosine similarity for normals. As in Lee et al. (2017), for room layout we generate Gaussian heatmaps for each of 48 room keypoint types and predict these heatmaps with a pixel-wise squared loss. Note that all regression tasks are quadratic losses (our surface normal prediction uses a cosine loss which is quadratic to leading order), allowing us to use the loss ratio L i (t) of each task as a direct proxy for each task's inverse training rate.\n\n\nNETWORK PERFORMANCE\n\nIn Table 1 we display the performance of GradNorm on the NYUv2 dataset (with input/output resolutions as described in Section 5). Specific training schemes for all NYUv2 models are detailed in Appendix A. We see that GradNorm improves the performance of all three tasks with respect to the equal-weights baseline (where w i (t) = 1 for all t,i), and that GradNorm either surpasses or matches (within statistical noise) the best performance of single networks for each task. The GradNorm Converged Weights network is derived by calculating the GradNorm time-averaged weights E t [w i (t)] for each task (e.g. by averaging curves like those found in Appendix B), and retraining a network with weights fixed to those values. GradNorm thus can also be used to extract good values for static weights. We pursue this idea further in Section 5.3 and show that these weights lie very close to the optimal weights extracted from exhaustive grid search.  To show how GradNorm can perform in the presence of a much larger dataset, we also perform extensive experiments on the expanded NYUv2 dataset, which carries a factor of 50x more data. The results are shown in Table 2. As with the standard NYUv2 runs, GradNorm networks outperform GradNorm versus an equal weights baseline and uncertainty weighting (Kendall et al. (2017)).\n\nother multitask methods, and either matches (within noise) or surpasses the performance of singletask networks. Figure 3 shows test and training loss curves for GradNorm (\u03b1 = 1.5) and baselines on the expanded NYUv2 dataset for our VGG SegNet models. GradNorm improves test-time depth error by \u223c 5%, despite ending with much higher training loss. GradNorm achieves this by aggressively rate balancing the network (enforced by a high asymmetry \u03b1 = 1.5), and ultimately suppresses the depth weight w depth (t) to lower than 0.10 (see Appendix B for more details). The same trend exists for keypoint regression, and is a clear signal of network regularization. In contrast, the uncertainty weighting technique (Kendall et al. (2017)) causes both test and training error to move in lockstep, and thus is not a good regularizer. Only results for the VGG SegNet are shown here, but the Thin ResNet FCN produces consistent results.\n\n\nGRADIENT NORMALIZATION FINDS OPTIMAL GRID-SEARCH WEIGHTS IN ONE PASS\n\nFor our VGG SegNet, we train 100 networks from scratch with random task weights on expanded NYUv2. Weights are sampled from a uniform distribution and renormalized to sum to T = 3. For computational efficiency, we only train for 15000 iterations out of the normal 80000, and then compare the performance of that network to our GradNorm \u03b1 = 1.5 VGG SegNet network at the same 15000 steps. The results are shown in Figure 4.\n\nEven after 100 networks trained, grid search still falls short of our GradNorm network. But even more remarkably, there is a strong, negative correlation between network performance and task weight distance to our time-averaged GradNorm weights. At an L 2 distance of \u223c 3, grid search networks on average have almost double the errors per task compared to our GradNorm network. Grad-Norm has effectively allowed us to \"cheat\" and immediately find the optimal grid search weights without actually performing grid search, simplifying a process that is usually notoriously laborious. Figure 5 shows visualizations of the VGG SegNet outputs on test set images along with the ground truth, for both the expanded and downsampled NYUv2 datasets. Ground truth labels are juxtaposed with outputs from the equal weights network, 3 single networks, and our best GradNorm network.  Expanded NYUv2 with room layout labels is shown on the left, while downsampled NYUv2 with semantic segmentation labels is shown on the right. The qualitative improvements are incremental, but we find the GradNorm network tends to output smoother, more detailed pixel map predictions when compared to the other two baselines.\n\n\nQUALITATIVE RESULTS\n\n\nCONCLUSIONS\n\nGradient normalization acts as a good model regularizer and leads to superb performance in multitask networks by operating directly on the gradients in the network. GradNorm is driven by the attractively simple heuristic of rate balancing, and can accommodate problems of varying complexities within the same unified model using a single hyperparameter representing task asymmetry. A GradNorm network can also be used to quickly extract optimal fixed task weights, removing the need for exhaustive grid search methods that become exponentially more expensive with the number of tasks. We hope that our work has not only introduced a new methodology for quickly balancing multitask networks, but also has shown how direct gradient manipulation can be a powerful way to reason about task relationships within a multitask framework.\n\n\nAppendices\n\nA GENERAL TRAINING CHARACTERISTICS All runs are trained at a batch size of 24 across 4 Titan X GTX 12GB GPUs and run at 30fps on a single GPU at inference. NYUv2 runs begin with a learning rate of 2e-5. Expanded NYUv2 runs last 80000 steps with a learning rate decay of 0.2 every 25000 steps. Downsampled NYUv2 runs last 20000 steps with a learning rate decay of 0.2 every 6000 steps. Updating w i (t) is performed at a learning rate of 0.025 for both GradNorm and the uncertainty weighting (Kendall et al. (2017)) baseline. All optimizers are Adam, although we find that GradNorm is insensitive to the optimizer chosen. We implement GradNorm using TensorFlow v1.2.1.\n\n\nB EFFECTS OF TUNING THE ASYMMETRY \u03b1\n\nThe only hyperparameter in our technique is the asymmetry \u03b1. The optimal value of \u03b1 for NYUv2 lies near \u03b1 = 1.5, while in the highly symmetric toy example in Section 4 we used \u03b1 = 0.12. This observation reinforces why we call \u03b1 an asymmetry parameter. Figure 6: Weights w i (t) during training, expanded NYUv2. Traces of how the task weights w i (t) change during training for two different values of \u03b1. A larger value of \u03b1 pushes weights farther apart, leading to less symmetry between tasks.\n\nTuning \u03b1 leads to performance gains, but we found that for NYUv2, almost any value of 0 < \u03b1 < 3 will improve network performance over an equal weights baseline. Figure 6 shows that higher values of \u03b1 tend to push the weights w i (t) further apart, which more aggressively reduces the influence of tasks which overfit or learn too quickly (in our case, depth). Remarkably, at \u03b1 = 1.75 (not shown) w depth (t) is suppressed to below 0.02 at no detriment to network performance on the depth task.\n\nFigure 3 :\n3Test and training loss curves for all tasks in expanded NYUv2, VGG16 backbone.\n\nFigure 4 :\n4Gridsearch performance for random task weights, expanded NYUv2. Average change in performance across three tasks for a static network with weights w static i is plotted against the L 2 distance between w static i and our GradNorm network's time-averaged weights, E t [w i (t)]. All comparisons are made at 15000 steps of training.\n\nFigure 5 :\n5Visualizations at inference time.\n\n\n) encoder and shallow ResNet decoder. The VGG SegNet reuses maxpool indices to perform upsampling, while the ResNet FCNModel Type and \nDepth \nSegmentation \nNormals \nWeighting Method \nError (m) 100-mIoU (%) Error (1-|cos|) \nVGG SegNet, Depth Only \n1.038 \n-\n-\nVGG SegNet, Segmentation Only \n-\n70.0 \n-\nVGG SegNet, Normals Only \n-\n-\n0.169 \nVGG SegNet, Equal Weights \n0.944 \n70.1 \n0.192 \nVGG SegNet, GradNorm Converged Weights \n0.939 \n67.5 \n0.171 \nVGG SegNet, GradNorm \u03b1 = 1.5 \n0.925 \n67.8 \n0.174 \n\nTable 1: Test error, 320x320 NYUv2 for GradNorm and various baselines. \n\n\n\nTable 2 :\n2Test error, expanded 320x320 NYUv2 for GradNorm and various baselines.\nIn general, if L is a L2 or CE loss, one may instead prefer a loss \u03c6(L) for some invertible function \u03c6. In that case, the inverse training rate should be set to \u03c6 \u22121 (L i ) to retain consistency. An L1 loss, for example, would use (L i ) 2 as a measure of inverse training rate.\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, arXiv:1511.00561arXiv preprintVijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. arXiv preprint arXiv:1511.00561, 2015.\n\nTask clustering and gating for bayesian multitask learning. Bart Bakker, Tom Heskes, Journal of Machine Learning Research. 4Bart Bakker and Tom Heskes. Task clustering and gating for bayesian multitask learning. Journal of Machine Learning Research, 4(May):83-99, 2003.\n\nUniversal representations: The missing link between faces, text, planktons, and cat breeds. Hakan Bilen, Andrea Vedaldi, arXiv:1701.07275arXiv preprintHakan Bilen and Andrea Vedaldi. Universal representations: The missing link between faces, text, planktons, and cat breeds. arXiv preprint arXiv:1701.07275, 2017.\n\nMultitask learning. In Learning to learn. Rich Caruana, SpringerRich Caruana. Multitask learning. In Learning to learn, pp. 95-133. Springer, 1998.\n\nA unified architecture for natural language processing: Deep neural networks with multitask learning. Ronan Collobert, Jason Weston, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningACMRonan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural net- works with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160-167. ACM, 2008.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDavid Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2650-2658, 2015.\n\nA joint many-task model: Growing a neural network for multiple nlp tasks. Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher, arXiv:1611.01587arXiv preprintKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple nlp tasks. arXiv preprint arXiv:1611.01587, 2016.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\n. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, arXiv:1703.06870Mask r-cnn. arXiv preprintKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. arXiv preprint arXiv:1703.06870, 2017.\n\nDeep architecture for traffic flow prediction: deep belief networks with multitask learning. Wenhao Huang, Guojie Song, Haikun Hong, Kunqing Xie, IEEE Transactions on Intelligent Transportation Systems. 155Wenhao Huang, Guojie Song, Haikun Hong, and Kunqing Xie. Deep architecture for traffic flow prediction: deep belief networks with multitask learning. IEEE Transactions on Intelligent Transportation Systems, 15 (5):2191-2201, 2014.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International Conference on Machine Learning. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456, 2015.\n\nClustered multi-task learning: A convex formulation. Laurent Jacob, Jean-Philippe Vert, Francis R Bach, Advances in neural information processing systems. Laurent Jacob, Jean-philippe Vert, and Francis R Bach. Clustered multi-task learning: A convex formulation. In Advances in neural information processing systems, pp. 745-752, 2009.\n\nLearning with whom to share in multi-task feature learning. Zhuoliang Kang, Kristen Grauman, Fei Sha, Proceedings of the 28th International Conference on Machine Learning (ICML-11). the 28th International Conference on Machine Learning (ICML-11)Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 521-528, 2011.\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. Alex Kendall, Yarin Gal, Roberto Cipolla, arXiv:1705.07115arXiv preprintAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. arXiv preprint arXiv:1705.07115, 2017.\n\nUbernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. Iasonas Kokkinos, arXiv:1609.02132arXiv preprintIasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. arXiv preprint arXiv:1609.02132, 2016.\n\nRoomnet: End-to-end room layout estimation. Chen-Yu Lee, Vijay Badrinarayanan, Tomasz Malisiewicz, Andrew Rabinovich, arXiv:1703.06241arXiv preprintChen-Yu Lee, Vijay Badrinarayanan, Tomasz Malisiewicz, and Andrew Rabinovich. Roomnet: End-to-end room layout estimation. arXiv preprint arXiv:1703.06241, 2017.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, 2015.\n\nLearning multiple tasks with deep relationship networks. Mingsheng Long, Jianmin Wang, arXiv:1506.02117arXiv preprintMingsheng Long and Jianmin Wang. Learning multiple tasks with deep relationship networks. arXiv preprint arXiv:1506.02117, 2015.\n\nFully-adaptive feature sharing in multi-task networks with applications in person attribute classification. Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris, arXiv:1611.05377arXiv preprintYongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. arXiv preprint arXiv:1611.05377, 2016.\n\nCross-stitch networks for multi-task learning. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3994- 4003, 2016.\n\nJoseph Redmon, Ali Farhadi, arXiv:1612.08242Yolo9000: better, faster, stronger. arXiv preprintJoseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. arXiv preprint arXiv:1612.08242, 2016.\n\nMulti-task learning in deep neural networks for improved phoneme recognition. L Michael, Jasha Seltzer, Droppo, Acoustics, Speech and Signal Processing. IEEE2013 IEEE International Conference onMichael L Seltzer and Jasha Droppo. Multi-task learning in deep neural networks for improved phoneme recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 6965-6969. IEEE, 2013.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDeep multi-task learning with low level tasks supervised at lower layers. Anders S\u00f8gaard, Yoav Goldberg, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics2Anders S\u00f8gaard and Yoav Goldberg. Deep multi-task learning with low level tasks supervised at lower layers. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 2, pp. 231-235, 2016.\n\nMultinet: Realtime joint semantic reasoning for autonomous driving. Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, Raquel Urtasun, arXiv:1612.07695arXiv preprintMarvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla, and Raquel Urtasun. Multinet: Real- time joint semantic reasoning for autonomous driving. arXiv preprint arXiv:1612.07695, 2016.\n\nSelf-informed neural network structure learning. David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov, arXiv:1412.6563arXiv preprintDavid Warde-Farley, Andrew Rabinovich, and Dragomir Anguelov. Self-informed neural network structure learning. arXiv preprint arXiv:1412.6563, 2014.\n\nDeep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, Simon King, Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEEZhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King. Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp. 4460-4464. IEEE, 2015.\n", "annotations": {"author": "[{\"end\":248,\"start\":179},{\"end\":339,\"start\":249},{\"end\":410,\"start\":340},{\"end\":494,\"start\":411}]", "publisher": null, "author_last_name": "[{\"end\":188,\"start\":184},{\"end\":269,\"start\":255},{\"end\":351,\"start\":348},{\"end\":428,\"start\":418}]", "author_first_name": "[{\"end\":183,\"start\":179},{\"end\":254,\"start\":249},{\"end\":347,\"start\":340},{\"end\":417,\"start\":411}]", "author_affiliation": "[{\"end\":247,\"start\":210},{\"end\":338,\"start\":301},{\"end\":409,\"start\":372},{\"end\":493,\"start\":456}]", "title": "[{\"end\":176,\"start\":1},{\"end\":670,\"start\":495}]", "venue": null, "abstract": "[{\"end\":2047,\"start\":672}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4908,\"start\":4885},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5792,\"start\":5777},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5816,\"start\":5794},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6080,\"start\":6056},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6107,\"start\":6082},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6134,\"start\":6109},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6173,\"start\":6156},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6284,\"start\":6264},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6343,\"start\":6320},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6628,\"start\":6606},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6653,\"start\":6630},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6708,\"start\":6692},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6857,\"start\":6840},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6917,\"start\":6893},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7186,\"start\":7166},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7205,\"start\":7186},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7279,\"start\":7260},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7326,\"start\":7306},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7460,\"start\":7434},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7481,\"start\":7465},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7634,\"start\":7613},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14930,\"start\":14909},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16783,\"start\":16766},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17116,\"start\":17087},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17135,\"start\":17118},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17196,\"start\":17168},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17283,\"start\":17266},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18027,\"start\":18010},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19760,\"start\":19738},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20493,\"start\":20471},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23774,\"start\":23752}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25048,\"start\":24957},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25392,\"start\":25049},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25439,\"start\":25393},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26009,\"start\":25440},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":26092,\"start\":26010}]", "paragraph": "[{\"end\":3035,\"start\":2063},{\"end\":3609,\"start\":3037},{\"end\":4545,\"start\":3611},{\"end\":5153,\"start\":4547},{\"end\":5228,\"start\":5155},{\"end\":5393,\"start\":5230},{\"end\":5569,\"start\":5400},{\"end\":5689,\"start\":5571},{\"end\":6345,\"start\":5706},{\"end\":7742,\"start\":6347},{\"end\":8211,\"start\":7809},{\"end\":9088,\"start\":8213},{\"end\":9596,\"start\":9090},{\"end\":9639,\"start\":9598},{\"end\":10675,\"start\":9788},{\"end\":10773,\"start\":10770},{\"end\":10868,\"start\":10775},{\"end\":11295,\"start\":10914},{\"end\":11923,\"start\":11297},{\"end\":12040,\"start\":11948},{\"end\":12771,\"start\":12075},{\"end\":13252,\"start\":12773},{\"end\":13950,\"start\":13254},{\"end\":14350,\"start\":13952},{\"end\":14802,\"start\":14352},{\"end\":15270,\"start\":14804},{\"end\":15672,\"start\":15272},{\"end\":15971,\"start\":15674},{\"end\":16393,\"start\":16017},{\"end\":16754,\"start\":16395},{\"end\":17008,\"start\":16756},{\"end\":17854,\"start\":17045},{\"end\":18420,\"start\":17856},{\"end\":19762,\"start\":18444},{\"end\":20688,\"start\":19764},{\"end\":21183,\"start\":20761},{\"end\":22379,\"start\":21185},{\"end\":23246,\"start\":22417},{\"end\":23928,\"start\":23261},{\"end\":24461,\"start\":23968},{\"end\":24956,\"start\":24463}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9787,\"start\":9640},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10769,\"start\":10676},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10913,\"start\":10869},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12074,\"start\":12041}]", "table_ref": "[{\"end\":18454,\"start\":18447},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19606,\"start\":19599}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2061,\"start\":2049},{\"end\":5398,\"start\":5396},{\"attributes\":{\"n\":\"2\"},\"end\":5704,\"start\":5692},{\"attributes\":{\"n\":\"3\"},\"end\":7756,\"start\":7745},{\"attributes\":{\"n\":\"3.1\"},\"end\":7807,\"start\":7759},{\"attributes\":{\"n\":\"4\"},\"end\":11946,\"start\":11926},{\"attributes\":{\"n\":\"5\"},\"end\":16015,\"start\":15974},{\"attributes\":{\"n\":\"5.1\"},\"end\":17043,\"start\":17011},{\"attributes\":{\"n\":\"5.2\"},\"end\":18442,\"start\":18423},{\"attributes\":{\"n\":\"5.3\"},\"end\":20759,\"start\":20691},{\"attributes\":{\"n\":\"5.4\"},\"end\":22401,\"start\":22382},{\"attributes\":{\"n\":\"6\"},\"end\":22415,\"start\":22404},{\"end\":23259,\"start\":23249},{\"end\":23966,\"start\":23931},{\"end\":24968,\"start\":24958},{\"end\":25060,\"start\":25050},{\"end\":25404,\"start\":25394},{\"end\":26020,\"start\":26011}]", "table": "[{\"end\":26009,\"start\":25561}]", "figure_caption": "[{\"end\":25048,\"start\":24970},{\"end\":25392,\"start\":25062},{\"end\":25439,\"start\":25406},{\"end\":25561,\"start\":25442},{\"end\":26092,\"start\":26022}]", "figure_ref": "[{\"end\":4010,\"start\":4002},{\"end\":11069,\"start\":11061},{\"end\":14161,\"start\":14152},{\"end\":14563,\"start\":14555},{\"end\":15004,\"start\":14996},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19884,\"start\":19876},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21182,\"start\":21174},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21774,\"start\":21766},{\"end\":24228,\"start\":24220},{\"end\":24632,\"start\":24624}]", "bib_author_first_name": "[{\"end\":26460,\"start\":26455},{\"end\":26481,\"start\":26477},{\"end\":26498,\"start\":26491},{\"end\":26781,\"start\":26777},{\"end\":26793,\"start\":26790},{\"end\":27085,\"start\":27080},{\"end\":27099,\"start\":27093},{\"end\":27349,\"start\":27345},{\"end\":27559,\"start\":27554},{\"end\":27576,\"start\":27571},{\"end\":28060,\"start\":28055},{\"end\":28071,\"start\":28068},{\"end\":28511,\"start\":28505},{\"end\":28530,\"start\":28523},{\"end\":28547,\"start\":28538},{\"end\":28565,\"start\":28558},{\"end\":28844,\"start\":28837},{\"end\":28856,\"start\":28849},{\"end\":28872,\"start\":28864},{\"end\":28882,\"start\":28878},{\"end\":29241,\"start\":29234},{\"end\":29253,\"start\":29246},{\"end\":29269,\"start\":29264},{\"end\":29282,\"start\":29278},{\"end\":29549,\"start\":29543},{\"end\":29563,\"start\":29557},{\"end\":29576,\"start\":29570},{\"end\":29590,\"start\":29583},{\"end\":29988,\"start\":29982},{\"end\":30005,\"start\":29996},{\"end\":30320,\"start\":30313},{\"end\":30341,\"start\":30328},{\"end\":30357,\"start\":30348},{\"end\":30666,\"start\":30657},{\"end\":30680,\"start\":30673},{\"end\":30693,\"start\":30690},{\"end\":31143,\"start\":31139},{\"end\":31158,\"start\":31153},{\"end\":31171,\"start\":31164},{\"end\":31532,\"start\":31525},{\"end\":31822,\"start\":31815},{\"end\":31833,\"start\":31828},{\"end\":31856,\"start\":31850},{\"end\":31876,\"start\":31870},{\"end\":32145,\"start\":32137},{\"end\":32156,\"start\":32152},{\"end\":32174,\"start\":32168},{\"end\":32602,\"start\":32593},{\"end\":32616,\"start\":32609},{\"end\":32897,\"start\":32891},{\"end\":32910,\"start\":32902},{\"end\":32927,\"start\":32918},{\"end\":32936,\"start\":32934},{\"end\":32948,\"start\":32944},{\"end\":32964,\"start\":32957},{\"end\":33287,\"start\":33282},{\"end\":33302,\"start\":33295},{\"end\":33323,\"start\":33316},{\"end\":33338,\"start\":33331},{\"end\":33715,\"start\":33709},{\"end\":33727,\"start\":33724},{\"end\":33989,\"start\":33988},{\"end\":34004,\"start\":33999},{\"end\":34411,\"start\":34406},{\"end\":34428,\"start\":34422},{\"end\":34693,\"start\":34687},{\"end\":34707,\"start\":34703},{\"end\":35184,\"start\":35178},{\"end\":35203,\"start\":35196},{\"end\":35217,\"start\":35211},{\"end\":35235,\"start\":35228},{\"end\":35251,\"start\":35245},{\"end\":35542,\"start\":35537},{\"end\":35563,\"start\":35557},{\"end\":35584,\"start\":35576},{\"end\":35887,\"start\":35879},{\"end\":35898,\"start\":35892},{\"end\":35925,\"start\":35919},{\"end\":35938,\"start\":35933}]", "bib_author_last_name": "[{\"end\":26475,\"start\":26461},{\"end\":26489,\"start\":26482},{\"end\":26506,\"start\":26499},{\"end\":26788,\"start\":26782},{\"end\":26800,\"start\":26794},{\"end\":27091,\"start\":27086},{\"end\":27107,\"start\":27100},{\"end\":27357,\"start\":27350},{\"end\":27569,\"start\":27560},{\"end\":27583,\"start\":27577},{\"end\":28066,\"start\":28061},{\"end\":28078,\"start\":28072},{\"end\":28521,\"start\":28512},{\"end\":28536,\"start\":28531},{\"end\":28556,\"start\":28548},{\"end\":28572,\"start\":28566},{\"end\":28847,\"start\":28845},{\"end\":28862,\"start\":28857},{\"end\":28876,\"start\":28873},{\"end\":28886,\"start\":28883},{\"end\":29244,\"start\":29242},{\"end\":29262,\"start\":29254},{\"end\":29276,\"start\":29270},{\"end\":29291,\"start\":29283},{\"end\":29555,\"start\":29550},{\"end\":29568,\"start\":29564},{\"end\":29581,\"start\":29577},{\"end\":29594,\"start\":29591},{\"end\":29994,\"start\":29989},{\"end\":30013,\"start\":30006},{\"end\":30326,\"start\":30321},{\"end\":30346,\"start\":30342},{\"end\":30362,\"start\":30358},{\"end\":30671,\"start\":30667},{\"end\":30688,\"start\":30681},{\"end\":30697,\"start\":30694},{\"end\":31151,\"start\":31144},{\"end\":31162,\"start\":31159},{\"end\":31179,\"start\":31172},{\"end\":31541,\"start\":31533},{\"end\":31826,\"start\":31823},{\"end\":31848,\"start\":31834},{\"end\":31868,\"start\":31857},{\"end\":31887,\"start\":31877},{\"end\":32150,\"start\":32146},{\"end\":32166,\"start\":32157},{\"end\":32182,\"start\":32175},{\"end\":32607,\"start\":32603},{\"end\":32621,\"start\":32617},{\"end\":32900,\"start\":32898},{\"end\":32916,\"start\":32911},{\"end\":32932,\"start\":32928},{\"end\":32942,\"start\":32937},{\"end\":32955,\"start\":32949},{\"end\":32970,\"start\":32965},{\"end\":33293,\"start\":33288},{\"end\":33314,\"start\":33303},{\"end\":33329,\"start\":33324},{\"end\":33345,\"start\":33339},{\"end\":33722,\"start\":33716},{\"end\":33735,\"start\":33728},{\"end\":33997,\"start\":33990},{\"end\":34012,\"start\":34005},{\"end\":34020,\"start\":34014},{\"end\":34420,\"start\":34412},{\"end\":34438,\"start\":34429},{\"end\":34701,\"start\":34694},{\"end\":34716,\"start\":34708},{\"end\":35194,\"start\":35185},{\"end\":35209,\"start\":35204},{\"end\":35226,\"start\":35218},{\"end\":35243,\"start\":35236},{\"end\":35259,\"start\":35252},{\"end\":35555,\"start\":35543},{\"end\":35574,\"start\":35564},{\"end\":35593,\"start\":35585},{\"end\":35890,\"start\":35888},{\"end\":35917,\"start\":35899},{\"end\":35931,\"start\":35926},{\"end\":35943,\"start\":35939}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1511.00561\",\"id\":\"b0\"},\"end\":26715,\"start\":26373},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10436583},\"end\":26986,\"start\":26717},{\"attributes\":{\"doi\":\"arXiv:1701.07275\",\"id\":\"b2\"},\"end\":27301,\"start\":26988},{\"attributes\":{\"id\":\"b3\"},\"end\":27450,\"start\":27303},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2617020},\"end\":27945,\"start\":27452},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":102496818},\"end\":28429,\"start\":27947},{\"attributes\":{\"doi\":\"arXiv:1611.01587\",\"id\":\"b6\"},\"end\":28789,\"start\":28431},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206594692},\"end\":29230,\"start\":28791},{\"attributes\":{\"doi\":\"arXiv:1703.06870\",\"id\":\"b8\"},\"end\":29448,\"start\":29232},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16673459},\"end\":29886,\"start\":29450},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5808102},\"end\":30258,\"start\":29888},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":488434},\"end\":30595,\"start\":30260},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12817931},\"end\":31049,\"start\":30597},{\"attributes\":{\"doi\":\"arXiv:1705.07115\",\"id\":\"b13\"},\"end\":31383,\"start\":31051},{\"attributes\":{\"doi\":\"arXiv:1609.02132\",\"id\":\"b14\"},\"end\":31769,\"start\":31385},{\"attributes\":{\"doi\":\"arXiv:1703.06241\",\"id\":\"b15\"},\"end\":32079,\"start\":31771},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1629541},\"end\":32534,\"start\":32081},{\"attributes\":{\"doi\":\"arXiv:1506.02117\",\"id\":\"b17\"},\"end\":32781,\"start\":32536},{\"attributes\":{\"doi\":\"arXiv:1611.05377\",\"id\":\"b18\"},\"end\":33233,\"start\":32783},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1923223},\"end\":33707,\"start\":33235},{\"attributes\":{\"doi\":\"arXiv:1612.08242\",\"id\":\"b20\"},\"end\":33908,\"start\":33709},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14873729},\"end\":34336,\"start\":33910},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b22\"},\"end\":34611,\"start\":34338},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16661147},\"end\":35108,\"start\":34613},{\"attributes\":{\"doi\":\"arXiv:1612.07695\",\"id\":\"b24\"},\"end\":35486,\"start\":35110},{\"attributes\":{\"doi\":\"arXiv:1412.6563\",\"id\":\"b25\"},\"end\":35772,\"start\":35488},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12016916},\"end\":36331,\"start\":35774}]", "bib_title": "[{\"end\":26775,\"start\":26717},{\"end\":27552,\"start\":27452},{\"end\":28053,\"start\":27947},{\"end\":28835,\"start\":28791},{\"end\":29541,\"start\":29450},{\"end\":29980,\"start\":29888},{\"end\":30311,\"start\":30260},{\"end\":30655,\"start\":30597},{\"end\":32135,\"start\":32081},{\"end\":33280,\"start\":33235},{\"end\":33986,\"start\":33910},{\"end\":34685,\"start\":34613},{\"end\":35877,\"start\":35774}]", "bib_author": "[{\"end\":26477,\"start\":26455},{\"end\":26491,\"start\":26477},{\"end\":26508,\"start\":26491},{\"end\":26790,\"start\":26777},{\"end\":26802,\"start\":26790},{\"end\":27093,\"start\":27080},{\"end\":27109,\"start\":27093},{\"end\":27359,\"start\":27345},{\"end\":27571,\"start\":27554},{\"end\":27585,\"start\":27571},{\"end\":28068,\"start\":28055},{\"end\":28080,\"start\":28068},{\"end\":28523,\"start\":28505},{\"end\":28538,\"start\":28523},{\"end\":28558,\"start\":28538},{\"end\":28574,\"start\":28558},{\"end\":28849,\"start\":28837},{\"end\":28864,\"start\":28849},{\"end\":28878,\"start\":28864},{\"end\":28888,\"start\":28878},{\"end\":29246,\"start\":29234},{\"end\":29264,\"start\":29246},{\"end\":29278,\"start\":29264},{\"end\":29293,\"start\":29278},{\"end\":29557,\"start\":29543},{\"end\":29570,\"start\":29557},{\"end\":29583,\"start\":29570},{\"end\":29596,\"start\":29583},{\"end\":29996,\"start\":29982},{\"end\":30015,\"start\":29996},{\"end\":30328,\"start\":30313},{\"end\":30348,\"start\":30328},{\"end\":30364,\"start\":30348},{\"end\":30673,\"start\":30657},{\"end\":30690,\"start\":30673},{\"end\":30699,\"start\":30690},{\"end\":31153,\"start\":31139},{\"end\":31164,\"start\":31153},{\"end\":31181,\"start\":31164},{\"end\":31543,\"start\":31525},{\"end\":31828,\"start\":31815},{\"end\":31850,\"start\":31828},{\"end\":31870,\"start\":31850},{\"end\":31889,\"start\":31870},{\"end\":32152,\"start\":32137},{\"end\":32168,\"start\":32152},{\"end\":32184,\"start\":32168},{\"end\":32609,\"start\":32593},{\"end\":32623,\"start\":32609},{\"end\":32902,\"start\":32891},{\"end\":32918,\"start\":32902},{\"end\":32934,\"start\":32918},{\"end\":32944,\"start\":32934},{\"end\":32957,\"start\":32944},{\"end\":32972,\"start\":32957},{\"end\":33295,\"start\":33282},{\"end\":33316,\"start\":33295},{\"end\":33331,\"start\":33316},{\"end\":33347,\"start\":33331},{\"end\":33724,\"start\":33709},{\"end\":33737,\"start\":33724},{\"end\":33999,\"start\":33988},{\"end\":34014,\"start\":33999},{\"end\":34022,\"start\":34014},{\"end\":34422,\"start\":34406},{\"end\":34440,\"start\":34422},{\"end\":34703,\"start\":34687},{\"end\":34718,\"start\":34703},{\"end\":35196,\"start\":35178},{\"end\":35211,\"start\":35196},{\"end\":35228,\"start\":35211},{\"end\":35245,\"start\":35228},{\"end\":35261,\"start\":35245},{\"end\":35557,\"start\":35537},{\"end\":35576,\"start\":35557},{\"end\":35595,\"start\":35576},{\"end\":35892,\"start\":35879},{\"end\":35919,\"start\":35892},{\"end\":35933,\"start\":35919},{\"end\":35945,\"start\":35933}]", "bib_venue": "[{\"end\":26453,\"start\":26373},{\"end\":26838,\"start\":26802},{\"end\":27078,\"start\":26988},{\"end\":27343,\"start\":27303},{\"end\":27653,\"start\":27585},{\"end\":28147,\"start\":28080},{\"end\":28503,\"start\":28431},{\"end\":28965,\"start\":28888},{\"end\":29651,\"start\":29596},{\"end\":30059,\"start\":30015},{\"end\":30413,\"start\":30364},{\"end\":30777,\"start\":30699},{\"end\":31137,\"start\":31051},{\"end\":31523,\"start\":31385},{\"end\":31813,\"start\":31771},{\"end\":32261,\"start\":32184},{\"end\":32591,\"start\":32536},{\"end\":32889,\"start\":32783},{\"end\":33424,\"start\":33347},{\"end\":33787,\"start\":33753},{\"end\":34061,\"start\":34022},{\"end\":34404,\"start\":34338},{\"end\":34805,\"start\":34718},{\"end\":35176,\"start\":35110},{\"end\":35535,\"start\":35488},{\"end\":36032,\"start\":35945},{\"end\":27708,\"start\":27655},{\"end\":28201,\"start\":28149},{\"end\":29029,\"start\":28967},{\"end\":30842,\"start\":30779},{\"end\":32325,\"start\":32263},{\"end\":33488,\"start\":33426},{\"end\":34879,\"start\":34807}]"}}}, "year": 2023, "month": 12, "day": 17}