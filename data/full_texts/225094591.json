{"id": 225094591, "updated": "2023-10-06 09:45:29.09", "metadata": {"title": "Learning Time Reduction Using Warm Start Methods for a Reinforcement Learning Based Supervisory Control in Hybrid Electric Vehicle Applications", "authors": "[{\"first\":\"Bin\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Hou\",\"middle\":[]},{\"first\":\"Junzhe\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Huayi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Dhruvang\",\"last\":\"Rathod\",\"middle\":[]},{\"first\":\"Zhe\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zoran\",\"last\":\"Filipi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Reinforcement Learning (RL) is widely utilized in the field of robotics, and as such, it is gradually being implemented in the Hybrid Electric Vehicle (HEV) supervisory control. Even though RL exhibits excellent performance in terms of fuel consumption minimization in simulation, the large learning iteration number needs a long learning time, making it hardly applicable in real-world vehicles. In addition, the fuel consumption of initial learning phases is much worse than baseline controls. This study aims to reduce the learning iterations of Q-learning in HEV application and improve fuel consumption in initial learning phases utilizing warm start methods. Different from previous studies, which initiated Q-learning with zero or random Q values, this study initiates the Q-learning with different supervisory controls (i.e., Equivalent Consumption Minimization Strategy control and heuristic control), and detailed analysis is given. The results show that the proposed warm start Q-learning requires 68.8% fewer iterations than cold start Q-learning. The trained Q-learning is validated in two different driving cycles, and the results show 10-16% MPG improvement when compared to Equivalent Consumption Minimization Strategy control. Furthermore, real-time feasibility is analyzed, and the guidance of vehicle implementation is provided. The results of this study can be used to facilitate the deployment of RL in vehicle supervisory control applications.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2010.14575", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2010-14575", "doi": "10.1109/tte.2020.3019009."}}, "content": {"source": {"pdf_hash": "301691cfe7175b15ba90eaa45eb4cc2d5c696a91", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.14575v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d29c5c92e1921c648295562ef839234f6e92d181", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/301691cfe7175b15ba90eaa45eb4cc2d5c696a91.txt", "contents": "\n\n\n\n\nDepartment of Automotive Engineering\nClemson University\n4 Research Dr29607GreenvilleSCUSA\n\n\nDepartment of Electrical Engineering and Computer Science\nUniversity of Michigan\n1301 Beal Avenue48109Ann ArborMIUSA\n\n\nCivil and Environmental Engineering Department\nUniversity of California\n760 Davis Hall94720Berkeley, BerkeleyCAUSA\n\n\nDepartment of Mechanical Engineering\nUniversity of Michigan\n2350 Hayward St48109Ann ArborMIUSA\n\n\nDepartment of Automotive Engineering\nClemson University\n4 Research Dr29607GreenvilleSCUSA\n\n\nDepartment of Electrical Engineering and Computer Science\nUniversity of Michigan\n1301 Beal Avenue, Ann Arbor, MI48109USA\n1Index Terms-Learning Time ReductionReinforcement LearningQ-learningSupervisory ControlHybrid Electric VehicleReal-Time Implementation\nReinforcement Learning (RL) is widely utilized in the field of robotics, and as such, it is gradually being implemented in the Hybrid Electric Vehicle (HEV) supervisory control. Even though RL exhibits excellent performance in terms of fuel consumption minimization in simulation, the large learning iteration number needs a long learning time, making it hardly applicable in real-world vehicles. In addition, the fuel consumption of initial learning phases is much worse than baseline controls. This study aims to reduce the learning iterations of Q-learning in HEV application and improve fuel consumption in initial learning phases utilizing warm start methods. Different from previous studies, which initiated Q-learning with zero or random Q values, this study initiates the Q-learning with different supervisory controls (i.e., Equivalent Consumption Minimization Strategy control and heuristic control), and detailed analysis is given. The results show that the proposed warm start Q-learning requires 68.8% fewer iterations than cold start Q-learning. The trained Q-learning is validated in two different driving cycles, and the results show 10-16% MPG improvement when compared to Equivalent Consumption Minimization Strategy control. Furthermore, real-time feasibility is analyzed, and the guidance of vehicle implementation is provided. The results of this study can be used to facilitate the deployment of RL in vehicle supervisory control applications.\n\nimprovement techniques for passenger vehicles. Propulsion system electrification, as one of the efficiency improvement techniques, arose in the last two decades [1][2][3]. Hybrid Electric Vehicle (HEV) is an important step towards the vehicle propulsion system electrification as it directly improves the engine operating efficiency by using electric motor/ generator (EM) to help drive the vehicle [4].\n\nSupervisory control is the brain of the HEV as it decides the operating conditions of the engine, EM, and battery. Thus, it has a significant impact on the HEV performance. Numerous HEV supervisory controls were researched in the past two decades, and the popular ones include rule-based method [5], Dynamic Programming (DP) [6], Equivalent Consumption Minimization Strategy (ECMS) [7], and Model Predictive Control (MPC) [8]. Rule-based method can be directly implemented in real-time as it has the lowest computation cost among four supervisory controls. However, the heuristic rules from experts' expertise or experiments might not be optimal, which leads to the need for global optimization, such as DP, to optimize the rules. DP generates global optimal results offline. Then, the rules are extracted from DP results and implemented in real-time. The DP assisted rule-based method is regarded as an indirect optimization method because the optimization is conducted offline by DP. To move the optimization from offline to real-time, ECMS and MPC supervisory controls are investigated. ECMS minimizes the instantaneous equivalent fuel consumption of engine and EM at each time step based on the given vehicle speed and torque demand. Compared to ECMS, MPC optimizes the fuel consumption and other HEV metrics in a future time window, leading to a higher computational time. However, both ECMS and MPC cannot guarantee global optimization solutions. Compared to the supervisory controls discussed above, reinforcement learning (RL) as another supervisory control has several advantages: (1) Compared with the rulebased method, RL is an optimization-based method and can achieve better fuel economy; (2) Compared with DP, RL requires much less computation cost and is capable of running online; (3) Compared with ECMS and MPC, RL is model-free and thus is not constrained by the reduced order model accuracy. Furthermore, ECMS only optimizes the fuel consumption at the current time step, and MPC only optimizes the fuel consumption for a short horizon, while RL is not limited by the instant or short time window and aims to find the global optimum with the help of Bellman equation [9].\n\nIn the past five years, RL-based HEV supervisory control gained more and more researchers' attention. RL mimics human learning behavior in a new environment by interacting with the environment. It explores the environment by trying different actions and receiving state and reward information from the environment. Actions in supervisory control are generally torque or power split between engine and EM for parallel HEV [10], and EM torque, engine speed/ torque for series HEV [11]. Some common states are vehicle speed, vehicle acceleration, vehicle power, battery SOC [12]. RL methods have shown successful performance in terms of energy-saving or fuel economy when compared to existing controls like heuristic, rule-based, ECMS, and DP. Deep RL was implemented as the power split algorithm in a hybrid electric bus [13]. Neural networks were used to approximate the Q values, and deep Q-learning achieved 89% of DP fuel economy. A hierarchical table-based RL framework was proposed to save energy for a plug-in fuel cell HEV [14]. There was no internal combustion engine in the propulsion system, and only fuel cell and lithium battery were considered. The higher level RL determined the penalty coefficient of fuel cell to avoid frequent fuel cell start or stop. The lower level RL determined the power split between fuel cell and lithium battery, based on the real-time penalty coefficient determined by the higher level RL. The proposed framework consumed 8.65% more hydrogen compared with DP. A Q-learning strategy was implemented in an HEV to split power among fuel cell, lithium battery, and ultracapacitor [15]. Compared with ECMS strategy, the proposed Q-learning RL strategy achieved 7-10% fuel saving in city and highway driving cycles. A tablebased Q-learning was implemented in a plug-in HEV [16]. The proposed table-based Q-learning saved 16.8% energy compared with a rule-based energy management strategy. A deep RL algorithm was proposed to control a Plug-in HEV [17]. 16.3% energy saving was achieved by the deep RL using rule-based control as the baseline strategy. A model-based Q learning strategy was proposed for a parallel HEV [18]. The model was online trained via the interaction with the vehicle. More than 10% of fuel saving was achieved at city and highway driving cycles when compared with the rule-based method. An inner-outer loop deep RL framework was proposed to plan the path and save energy for a Toyota Prius plug-in HEV [19]. The outer loop generated the difference between the existing operation and the best operation recorded in history. The inner loop was an actor-critic RL framework.\n\nThough RL-based HEV supervisory control gains momentum in recent years and shows encouraging energy-saving performance compared with existing controls, it has one big obstacle in the path of real-world implementation, which is its long learning time.\n\nLarge number of iterations are required to achieve convergence of the RL-based supervisory control when neural networks are used to approximate Q values, such as 700-1,200 iterations [20], 15,000 iterations [13], 300,000 iterations [16], 150, 000 iterations [17], and 200,000 iterations [21]. Among the above RL literature in HEV applications, some used table-based RL methods [16,20,21], and the iterations varied from 700 to 300,000. The others used neural network based RL methods [13,17], and the iterations varied from 15,000 to 150,000. This long learning time obstacle exists not only in HEV application, but also in most applications using RL algorithms. In [22], the study investigated multiple robotics tasks, and the RL algorithm required 100,000 iterations to reach convergence. In [23], a door open task was executed by a robotic arm. The RL algorithm in simulation required over 5,000 iterations to reach convergence, whereas, in experiments, the RL algorithm took more than 50,000 iterations to reach convergence.\n\nThe above literature showed the RL-based supervisory control required more than thousands of iterations in the learning process to achieve satisfactory performance, and some even required more than 100,000 iterations. This large number of iterations require months of road or chassis dyno tests, which are costly both in time and money.\n\nTo reduce the learning time of the RL algorithms, several techniques are available: (i) learning rate adjustment [24], (ii) selective learning experience [25], and (iii) warm start [26]. Both the learning rate adjustment and selective learning experience have limited impacts on the learning time reduction due to the trade-off between learning time and vehicle performance. However, a warm start is not limited by that trade-off as it directly gives a one-time boost of vehicle performance in the initialization of the RL algorithm, where both the learning time and vehicle performance are improved at the same time. There is no warm start focused study in the field of HEV supervisory control, and only one study used a warm start method to facilitate learning convergence [20]. This reference used near-optimal Q values in the initialization, and the vehicle platform is plug-in HEV. It took 1200 driving cycles to converge for the cold start method and 700 driving cycles to converge for the warm start method. The difference between the reference and our work can be summarized as: (1) Our vehicle is SOC sustaining HEV rather than plug-in HEV (i.e., A plug-in HEV has more batteries than a SOC sustaining HEV does); (2) Our research provides surficent analysis as well as the detailed explanations of the warm start process. In the reference, only one sentence is given to describe the warm start and results (i.e., the initialization with simulated optimal or near-optimal solutions helps achieve a faster convergence); (3) It is not clear how the warm start process is achieved. Beyond the HEV application, warm start RL is researched in healthcare [26] and robotics [27]. Besides RL applications, warm start was also utilized in supervised learning [28,29]. Overall, the warm start focused works are still lacking in RL, and, to our best knowledge, no study is found in HEV supervisory control. Additionally, in the vehicle warm start application, the engine and EM efficiency maps and the rules from experts are different from robotics and healthcare applications.\n\nMore studies are required to investigate the RL-based supervisory control using warm start.\n\nThis study aims to investigate the warm start of RL based supervisory control in HEV application. To the best of our knowledge, this work has never been done before. The main contributions of this study are listed below:\n\n(1). This study shows the learning time reduction advantage of warm start Q-learning over cold start in HEV application.\n\n(2). For the first time, it achieves warm start Q-learning using two supervisory controls: ECMS and heuristic control.\n\n(3). For the first time, it presents the detailed analysis of warm start and cold start comparison using RL state-action value function map, engine/ EM operating efficiency map, torque, speed, battery SOC, engine fuel consumption. (4). It compares the warm start Q-learning supervisory control with ECMS control both in the driving cycle used in learning and the other two driving cycles Q-learning not used in learning.\n\nThe rest of the paper is organized as follows: Section II introduces the vehicle propulsion system model. Section III covers the Q-learning strategy and warm start strategies that utilized in this study. Reward, states, and actions are discussed in the Q-learning subsection. Heuristic strategy and ECMS are presented in the warm start strategies subsection. Section IV analyzes the results of cold start and warm start in detail and compares warm start Q-learning with ECMS in three different driving cycles (one cycle is the same one used in learning and two cycles that are not used in learning). Some key performance matrices are compared for the illustration, such as fuel consumption, engine/ EM torque, cycle speed tracking performance. In addition, real-time implementation feasibility of the warm start Q-learning is evaluated, and vehicle deployment guidance is given at the end of Section IV. Finally, the conclusion and future work are summarized in Section V.\n\n\nII. MODELING OF HEV PROPULSION SYSTEM\n\nThis study focuses on a middle-sized passenger car with a parallel hybrid propulsion system, and its architecture is shown in Fig. 1. Similar architecture can be found in BMW i8, which allocates an engine in the back and an electric motor in the front. The engine drives the front wheels, and the EM drives the rear wheels. The rated power of the engine is 64.6kW at 6000rpm, and the rated power of the EM is 143kW. There are four gear ratios in the transmission. The battery pack has 5.2kWh capacity. The primary parameters of the vehicle are summarized in Table 1. The vehicle model is built in Matlab/Simulink.  \n\n\nA. Torque Demand Modeling\n\nThe vehicle acceleration force is calculated as follows:\n= + + + (1)\nwhere , , , represent aerodynamic drag force, rolling resistance, road grade force, and traction force, respectively. The acceleration force is expressed as follows:\n= (2)\nwhere is vehicle weight and is vehicle acceleration. The vehicle acceleration is correlated to the vehicle speed:\n=\u0307\u210e (3)\nThe rest of the four forces are calculated as follows:\n{ = 1 2 \u210e 2 = ( ) = ( ) = + (4)\nwhere is the air density, is the air drag coefficient, is vehicle font area, \u210e is vehicle speed, is the road slop angle, is road rolling resistance coefficient, is the weight of the vehicle, is the gravity constant, and are the traction force provided by engine and EM, respectively.\n\nThe engine force is derived as follows:\n= \u210e , \u210e (5) \u210e , = ,(6)\n, =\n\nEngine fuel consumption is interpolated from the fuel map ( Fig. 2 (a)) as follows:\n\u0307= ( , ) (8) = ,(9), = \u210e ,(10)\u210e , = \u210e \u210e (11)\nwhere \u210e is wheel radius and it applies for front and rear wheels, is the gear ratio of the final drive, and is the gear ratio of transmission. The EM force is calculated as follows: \n\nBattery power output calculation is as follows:\n= { , , \u210e , , \u210e (14) , = ( , ) (15) = \u210e ,(16)\u210e , = \u210e \u210e (17)\nwhere the EM efficiency map is shown in Fig. 2(b). Total vehicle torque demand is determined by the driver command (acceleration pedal position) as follows:\n= (18)\nThe total torque demand is split by the energy management system, which is Q-learning in this study as follows:\n= , + ,(19)\nThe total EM torque demand includes the torque demand assigned by supervisory control and braking torque demand:\n= ,(20)\n, ,\n= , +(21)\nThe driver model is a PID feedback control, which takes actual vehicle speed and reference speed as the inputs, and outputs a  \n\n\nB. Battery Modeling\n\nThe time derivative of the battery state of charge (SOC) is calculated as follows:\n\u0307= \u2212(22)\nwhere , are the battery current and nominal capacity, respectively. Current and voltage are calculated as follows:\n= (23) = \u2212 (24)\nwhere is the battery power output, is the battery open circuit voltage, and is the battery internal resistance.\n\n\nIII. SUPERVISORY CONTROL STRATEGIES\n\n\nA. Q-learning strategy\n\nIn the RL framework, there are two objects: the environment on the left and agent on the right in Fig. 3. The agent is the RL Qlearning algorithm. Everything outside the agent belongs to the environment. In this application, the vehicle is the main object in the environment. Besides the vehicle, any factor influencing vehicle operating status is also integrated into the environment block, such as driver behavior and road conditions. The agent aims to maximize the reward by learning from the interaction with the environment through the data flow path. The interaction contains the action to the environment and feedback from the environment. The Q-learning algorithm learns the state-action value function -( , ). The ( , ) is a function of state and action . Its value represents the overall reward the agent can get if the respective action is taken at the given state. The learning process is basically improving the Q value estimation accuracy by correcting the old estimated Q value with the observed error (i.e.\n\nrepresenting Dopamine in a human brain). The Q value correction process is as follows:\n( , ) = ( , ) + ([ + ( \u2032, )] \u2212 ( , ))(25)\nwhere is the learning rate, is the state from current time step, \u2032 is the state from next time step, is the reward obtained in state transition from to \u2032, is the discount factor. The term [ + max ( \u2032, )] represents the observed overall reward respective to ( , ) state-action pair, and it is from Bellman Equation [9]. To be specific, the reward represents the reward from state transition from to \u2032 and max ( \u2032, ) represents the max overall reward from \u2032 to the infinite future horizon. Adding these two terms together represents the overall reward from to the infinite future. The discount factor aims to reduce the weight of future term max ( \u2032, ) as there could be uncertainty in the future or instantaneous reward could be what the agent cares. improved. More information about experience evaluation can be found in [30]. In this vehicle application, one iteration is one simulation over an entire driving cycle. -greedy method is used in the random action exploration, and is in the range of [0,1].\n\nMore specifically, in the vehicle operation, random action is selected with the possibility of and the optimal action (i.e., the action corresponding to the largest Q value among all the Q values in the given state) is selected with the possibility of (1-). Pseudocode of Q-learning algorithm implemented in this study. \n\n\nQ-learning Algorithm\n\n\n1) Reward\n\nIn RL, the reward is the metric of how good the action is, and it is the result of state transition from the current state to the next state. In Q-learning, the reward combined with an estimated future return is treated as the observed overall return, which is compared with the estimated overall return. The error from the comparison is considered to mimic the Dopamine during the new task learning in the human brain [31]. In this study, the fuel consumption is defined as the reward. The goal of the supervisory control is to maximize the reward. Battery usage is represented by the equivalent fuel consumption as follows:\n\u0307, = ( 1 + (1 \u2212 ) \u210e \u210e ) Q LHV (26) = { 1 , \u210e 0 , \u210e(27)\nwhere and \u210e represent the discharge and charge equivalent factors, and \u210e represent the discharge and charge efficiency, is the power output/ input of the battery, and is the low heating value of the fuel used by engine. The discharge and charge equivalent factors are assumed to be 3, and efficiencies are assumed to be 0.95. The reward expression is as follows:\n\n= \u2212(\u0307, +\u0307, ) + 1 (28) where is the time step in the simulation. A negative sign of fuel consumption turns the fuel minimization problem into a maximization problem, as the Q-learning is implemented in reward maximization problems by default. The addition of 1 in (28) ensures the reward to be a positive value in any conditions. 1 is enough in this vehicle as the first term of right-hand side in (28) \n\n\n2) States\n\nStates in RL represent the status of the environment and is one of the two feedback signals from the environment (the other signal is the reward). The RL action heavily depends on states. In this study, vehicle torque demand and vehicle speed are selected as the state vector. Vehicle torque demand is related to the vehicle traction force and acceleration. The vehicle torque demand selection over the vehicle acceleration is due to the direct connection between torque demand and action (i.e., EM torque demand) without unit change. Vehicle speed represents the accumulative vehicle acceleration. Different state vector examples in HEV RLbased supervisory controls can be found in literature [11,12,16,17]. State vector selection should be systematically conducted and is not the focus of this study. Detailed analysis can be found in [30].\n\n\n3) Action\n\nThe EM torque demand is chosen as the action in this study. The engine torque demand is then derived as the difference between the vehicle torque demand and EM torque demand. In order to maximize the vehicle efficiency, regenerative braking is active for all the braking events. The rest of the actions are done by local controllers, such as transmission shift and torque converter lock.greedy method is used in the random action exploration and is in the range of [0,1].\n\n\nB. Warm start strategies 1) Heuristic strategy\n\nThe engine generally has low efficiency when the vehicle has a large acceleration at vehicle start, which frequently occurs in urban driving scenarios. During this period, vehicle torque demand is great, and the torque demand magnitude varies with vehicle classes. In the vehicle of this study, the engine saturates at maximum torque during vehicle start, and the engine efficiency is not optimal. More specifically, the engine efficiency map utilized in this study is shown in Fig. 4. When engine speed is less than 2800rpm, the maximum engine efficiency at the torque saturation line is 28%. However, the maximum engine efficiency reaches 32% on the entire map. The idea of the heuristic strategy is to split part of the torque demand to EM during the vehicle fast acceleration at low speed so that the engine could operate at slightly lower torque and thus higher efficiency. To implement this heuristic strategy, the Q values are modified. First, the Q values are initialized to be zeros at all states and actions. Then, the Q values at low speed, high torque demand, and large EM torque are set to be 1.0, which guides the Q-learning to select large EM torque in this scenario. \n\n\n2) Equivalent Consumption Minimization Strategy\n\nEquivalent Consumption Minimization Strategy (ECMS), as one of the well-known supervisory controls, considers both engine fuel consumption and battery energy usage and minimizes the overall equivalent fuel consumption. It uses (26) to calculate the equivalent fuel consumption in a battery. In real-time vehicle operation, ECMS optimizes the torque split between the engine and EM at the given speed and driver torque demand based on the efficiency maps of engine/ EM.\n\n\nIV. SIMULATION AND RESULTS ANALYSIS\n\nUrban Dynamometer Driving Schedule (UDDS) driving cycle [32] is utilized in the learning as the vehicle operating conditions.\n\nThe simulation time step is set to be 1 second. The total number of iterations is set to be 5000. The Q value update only occurs when the sum of rewards from iteration is greater than that from simulation applying full greedy action. Full greedy action simulation is conducted after Q values are updated, the result of which is used as criteria to select the good experience for Q value update. The learning rate is fixed at 0.1, and the random action exploration rate is fixed at 0.1 (i.e. 10%). The discount factor is set to 1.0. For the discretization of states, vehicle torque demand is discretized in 5 values from 0 to 400Nm in 100Nm steps.\n\nVehicle speed is discretized in 5 values from 0 to 65mph in 16mph steps. For action discretization, EM torque is discretized in 20 values from -100 to 250Nm in 18.5Nm steps.\n\nThe results from cold start Q-learning are presented first as the reference to the warm start methods. Then the results from the warm start using ECMS are analyzed and compared with cold start results. After that, a heuristic rule of EMS is converted to Q values initialization as another warm start method, and the results are compared with the cold start and ECMS warm start.\n\n\nLearning with cold start\n\nIn the cold start Q-learning, the Q values are initialized with zeros. No knowledge is utilized to help initialize the Q values. The learning process is executed in 5000 iterations. Only the iterations generating good experience are utilized to update the Q values.\n\nThe theoretical maximum (calculated in experience evaluation criteria update in Table 2) of the sum of rewards and engine fuel economy along the 5000 iterations are shown in Fig. 5. From the figure, it is observed that the learning converges after 600\n\niterations. In Fig. 5(b), the fuel economy starts with 37mpg and converges to 59mpg, which presents a 59.5% improvement. \n\n\nLearning with warm start\n\nIn the ECMS warm start, the vehicle EM and engine torque split are controlled by ECMS in the first UDDS cycle. After this one cycle simulation. The result is used as an experience to update the Q values following the process in Table 2   Before the iterations, the initialized Q values from ECMS and heuristic warm start strategies are shown in Fig. 6 and Fig. 7, respectively. In the ECMS warm start Q values, main updates occur in the 50-150Nm torque demand range, as shown in Fig. 6(b).\n\nIn this subplot, there are multiple Q value updates (i.e., multiple EM torques action taken) at each vehicle speed. In Fig. 6(a), when total torque demand is below 50Nm, EM torque from ECMS is close to zero along with all the vehicle speed. At the top of Fig.   6(c), (d) and (e), there are Q value spikes, which are corresponding to large EM torque at low speed (<20mph) and high torque demand (150-400Nm). This scenario with low speed and high torque demand means the large acceleration at low speed.\n\nFor the heuristic warm start, the idea is similar to the observation from Fig. 6(c), (d), and (e), where EM helps vehicle acceleration at low speed and large acceleration scenario. There are two reasons to use EM in this scenario: (i) engine maximum torque is not large enough to satisfy the torque demand, (ii) engine efficiency at engine torque saturation line is not high. Thus the EM could help improve the overall efficiency of the propulsion system. When the vehicle speed is less than 20mph and torque demand is greater than 50Nm, Q value is assigned with 1.0, as shown in Fig. 7(b-e). By integrating all the subplots in Fig. 6 by selecting the maximum Q value at each state, the optimal action map is obtained in Fig. 8(a). Similarly, the optimal action map from heuristic initialization is obtained and shown in Fig. 9(a). The main trend of these two optimal action maps is the same that the high EM torque is used during low vehicle speed while low EM torque is used during high vehicle speed. The optimal EM torque in Fig.   8(a) is slightly different at different total torque demand, while this relationship is not observed in Fig. 9(a). In addition, some middle-range EM torques appear in the middle to high speed regions in Fig. 8(a), which reduces the burden of the engine. While the EM torque is zero in those speed range in Fig. 9(a). These differences result in the ECMS and heuristic initialization fuel economy difference (51mpg vs. 42mpg). The operational efficiencies of engine and EM using cold start, ECMS warm start, and heuristic warm start Q-learning are shown in Fig. 10, Fig. 11 and Fig. 12, respectively. When compared with Fig. 10(a), Fig. 11(a) has less operating points on the saturation torque line (i.e., low-efficiency region) and more operating points in the highlighted high-efficiency region. The high efficiency is beneficial to the fuel economy as the ECMS warm start Q-learning results in 51mpg while the cold start Q-learning results in 36mpg. This improvement significantly improves the overall fuel economy during the learning process. The high torque demand from the ICE in Fig. 10(a) is satisfied by the EM in the highlighted high torque region of Fig. 11(b). The engine operating points disparity between Fig. 10(a) and Fig. 12(a) is not obvious. The disparity is more obvious when comparing the EM operating torque between Fig. 10(b) and Fig. 12(b) as more large positive torque operating points locate in the highlighted region of Fig. 12(b). The high EM torque is the main principle of the heuristic warm start strategy. The ECMS and heuristic warm start Q-learning show similar EM operating efficiency. The main benefit of large EM torque is to reduce engine torque saturation and thus increase engine operating efficiency.   After the warm start initialization, 5000 iterations are conducted. The sum of rewards comparison between cold start and warm start from good experience along the iterations are shown in Fig. 13. Both warm start strategies give Q-learning a tremendous leading position at the initial iteration of 5000 iterations when compared with cold start. The fuel economy comparisons of cold start and warm start are shown in Table 3. The fuel economies at iteration 1 from ECMS and heuristic warm start are 51mpg and 42mpg, respectively, while the cold start only produces 36mpg. It takes 800 iterations for the cold start Q-learning to reach 59mpg, while the ECMS warm start Q-learning only requires 250 iterations to reach 59mpg, which is 68.8% iterations reduction. The distance of UDDS driving cycle is around 7.5miles, and the duration is 1369s. If the car OEMs design the EMS based on Q-learning, for the cold start, it requires 6000 miles/ 12.5 days of test driving to achieve 59mpg, while warm start only needs 1875 miles/ 4 days of test driving, which is a significant reduction in test time and cost. For a car owner to train the Q-learning based on the 60 miles daily commute distance, it takes 3.5 months to achieve 59mpg using cold start initialization strategy, while only takes only one month to reach a similar fuel economy using ECMS warm start. For the heuristic warm start, it takes a similar amount of iterations with ECMS warm start to achieve 59mpg.  After 5000 iterations, the Q value maps from ECMS warm start and heuristic warm start are shown in Fig. 14 and Fig. 15, respectively. Both Q value maps share the shapes in each of the total torque demand subplots. The similarity of Q value explains the similar fuel economy performance of the two Q value initialization strategies. When comparing the initialized Q values ( Fig.   6/ Fig. 7) and the Q values after 5000 iterations (Fig. 14/ Fig. 15), the shape from the initialization is still visible with significantly dampened magnitude. For instance, the spike at 0 EM torque in Fig. 6(a) still exists at the same location of the map in Fig. 14(a).\n\nHowever, the Q value decreases from 1 to 0.1. A similar observation can be found in Fig. 7(c) and Fig. 15(b). The reason for decreasing magnitude is the multiple updates during the 5000 iterations. Every time the maximum Q value in the entire Q value table is increased, the magnitude of initialized Q value is discounted relative to the maximum Q value. Combining the subplots of Q values by only selecting the maximum Q value along with actions, optimal action maps from ECMS and heuristic warm starts are obtained in Fig. 8(b) and Fig. 9(b), respectively. Comparing Fig. 8(a) and (b), one can easily tell the reason why ECMS warm start initialization has 51mpg, which is its similarity shape of optimal action map to the final map after 5000 iterations. In comparison, heuristic warm start optimal action maps show more differences between Fig. 9(a) and (b), which is corresponding to the 17 mpg difference (42mpg vs. 59mpg). Simulation results comparison among cold start initialization (iteration #1), ECMS warm start initialization (iteration #1), heuristic warm start initialization (iteration #1) and heuristic warm start after 5000 iterations (iteration #5000) are shown in Fig.   16 and Fig. 17. In Fig. 16(a), all four simulations show satisfactory vehicle speed tracking performance. Battery SOC results are shown in Fig. 16(b), where cold and heuristic initialization simulations have similar SOC trajectories. ECMS warm start initialization and heuristic warm start final show large SOC operating range. The starting SOC is 0.6, and the ending SOC for all three cases is between 0.58-0.6. Fig. 16(c-d) show the engine fuel rate and cumulative fuel consumption along the driving cycle.\n\nIn Fig. 16(c), multiple peak fuel rates come from the cold start initialization, which explains its high cumulative fuel consumption through the entire driving cycle. ECMS and heuristic warm start initialization burn less fuel than cold start. Torque information is considered for comparison in Fig. 17, where a zoom-in time window (400s-500s) result is plotted for better readability. In this 100s time window, the vehicle starts and stops twice, as shown in Fig. 17(a). There are two moments (410s and 460s) where the cumulative fuel consumption gaps increase among the four simulation cases, as shown in Fig. 17(d). The largest fuel consumption increase occurs in cold start case. At both moments, the vehicle starts to accelerate. In the first 1-2 seconds of the vehicle start at 401s, the total torque demand for all four cases is similar, as shown in Fig. 17(e). At this vehicle start scenario, all four cases split a similar amount of torque to the engine, while the EM torques are different, as shown in Fig. 17(f) and (g). For the EM torque, the cold start only assigns 65Nm, which is the default action when the Q values are the same for the entire 20 action discretizations.\n\nHowever, the ECMS warm start and heuristic warm start specifically initialize the Q values based on the low speed and high total torque demand scenario so that EM can boost the vehicle during vehicle start. Thus, ECMS warm start initialization and heuristic warm start initialization, assign 200-250Nm (high torque) during the vehicle start. The high EM torque reduces the burden of the engine torque output, as shown in Fig. 17(g) and thus reduce engine fuel rate. The low EM torque in the cold start results in a slightly large speed tracking error, as shown in Fig. 17(a), and thus leads to higher total torque demand in Fig. 17(e) as the \"PI\"\n\ncontroller driver tries to add more throttle angle to reduce the speed error. The great torque demand is accompanied by large engine fuel consumption in Fig. 17(c). In Fig. 17(b), (c) and (e), compared with heuristic initialization, ECMS initialization shows better alignment with heuristic warm start final result, which is reflected by the cumulative fuel consumption in Fig. 16(d). \n\n\nComparison with ECMS\n\nIn this subsection, heuristic warm start Q-learning is first compared with ECMS in the UDDS driving cycle, which aims to show the proposed warm start control with respect to existing supervisory control. After the comparison of supervisory controls, the trained Q-learning is then directly implemented in two different driving cycles, which aims to validate the generality of the UDDS trained Q-learning. In other words, the Q value updates are only conducted in the UDDS driving cycle, and there is no Q value update in the two validation driving cycle simulation. The is set as 0 in the -greedy action exploration method; thus the optimal policy is generated in the validation.\n\nIn the UDDS driving cycle simulation, the vehicle using ECMS supervisory control results in 54.6mpg. In comparison, the Qlearning with heuristic warm start results in 59mpg, which is around 10% fuel economy improvement. Simulation results comparison is shown in Fig. 18. The main difference between these two supervisory controls is the torque split in vehicle starting phases (i.e., 350s, 410s, and 450s in Fig. 18(a)). As shown in Fig. 18(f) and (g), ECMS uses more ICE than EM at vehicle starting phases, whereas Q-learning uses more EM than ICE. The ECMS supervisory control leads to a slightly greater vehicle speed tracking error, as shown in Fig. 18(a). The vehicle speed tracking error is then translated to great vehicle total torque demand as the virtual driver (i.e., PI controller) tries to reduce the speed gap. After three vehicle starting phases, the cumulative fuel consumption between ECMS and Q-learning reaches a gap. The difference in results reflects one intrinsic difference between ECMS and Qlearning, which is the model-based or model-free. In some existing publications [33,34], backward-looking vehicle model is used.\n\nIn the backward-looking vehicle model, the total vehicle torque demand or power demand is calculated using vehicle dynamics, assuming vehicle speed is perfectly matching driving cycle speed. In the simulations, the total vehicle torque demand is pre-known and fixed, which is not affected by different supervisory controls. However, in reality, the vehicle speed cannot perfectly match the reference speed as the vehicle operating conditions are impacted by the decision of supervisory controls. In this paper, a forward-looking vehicle model is used, which models the driver using a PI controller, and there is no perfect speed tracking assumption. This is why the total vehicle torque demand is different in Fig. 18  To validate the learned Q-learning agent in different driving conditions, trained Q-learning from the UDDS driving cycle is then implemented in WLTP [35] and HWFET driving cycles [32]. ECMS is implemented as the comparison strategy, and the fuel economy results are shown in Table 4. In both driving cycles, the fuel economy from the Q-learning supervisory control shows higher values than that from the ECMS supervisory control. The fuel economy improvement is in the range of 10-16%, which is not far from the improvement (i.e., 10%) in the training driving cycle -UDDS. Thus, the training and validation results are consistent. To implement the proposed warm start Q-learning supervisory control in an actual vehicle, the real-time computation feasibility is pivotal. In literature [14], a Q-learning control was implemented in an HEV. The experimental results showed that the Q-learning algorithm only took 0.447s CPU time to find the optimal action in the controller hardware at each time step. 0.5-1.0s time range is the controller sampling interval time in the actual vehicle controllers reported in [36,37]. The 0.447s is shorter than controller sampling time, and thus the real-time execution is possible for the Q-learning supervisory control. Besides the real-time optimal action search in the Q values, the Q-learning needs to update the Q values based on the collected data during the daily vehicle operation. In actual vehicle deployment, the Q value update process is only executed when the vehicle is at rest, such as in the evening. The Q value update takes 1370 time steps in this study, which is corresponding to 612s (i.e., 10mins) if one time step takes 0.447s CPU time of the controller hardware. Based on the analysis of real-time execution and Q-learning update time, the proposed ensemble control has the potential in real-time implementation. The schematic of vehicle implementation is shown in Fig. 19. The Q values are first initialized in the controller software design. The data used in the initialization is generated either by actual vehicle operation over one driving cycle with ECMS control or by expert knowledge. As shown in Table 2, this process is described from line 4 to line 9. The software is then loaded into the controller hardware, which is followed by vehicle operation to collect experience for Q value update. When the vehicle is at rest, the controller hardware undergoes Q value update, which includes experience evaluation, Q value update and experience selection criteria update (see line [16][17][18][19][20][21][22][23][24][25][26][27][28][29] in Table 2). If the Q values converge (i.e., there is no significant change between the Q values prior to and after the update), the learning is finished. Otherwise, more experience will be collected during vehicle operation until Q values converge. If vehicle operation condition changes (e.g., vehicle owner moves from big city to remote urban area), the algorithm will detect the significant change of fuel economy. It then collects data and updates Q values until convergence. Fig. 19. Schematic of warm start Q-learning supervisory control vehicle implementation.\n\n\nV. CONCLUSION\n\nThis study proposed the warm start of RL-based supervisory control for a parallel HEV. ECMS and heuristic controls are used as warm start controls for the Q-learning supervisory control. Compared with cold start, both ECMS and heuristic warm start controls reduce the Q-learning iterations. The proposed ECMS warm start reduces the learning time as much as 68.8%. Right after the warm start Q value initialization, the ECMS and heuristic warm start Q-learning lead to 51mpg and 42mpg, compared with 36mpg from the cold start Q-learning. The proposed warm start methods significantly improve the overall fuel economy during the learning process. The warm start controls reduce the fuel consumption by reducing the engine torque saturation and allocating more torque to the EM. This torque distribution increases the operating efficiency of the engine. Compared with ECMS warm start, the heuristic warm start control has more flexibility in Q state-action value function initialization. Such a method appropriately transfers the learned knowledge to the Q-learning function. In the UDDS driving cycle that Q-learning iterations are conducted, the heuristic warm start Q-learning control shows 10% MPG higher than the ECMS control. In addition, the UDDS trained Qlearning control and ECMS control are compared in two different driving cycles for validation purposes. The results show 10-16% fuel saving by Q-learning compared with ECMS. The real-time implementation feasibility is discussed, and the vehicle implementation schematic is given as the guidance.\n\nEven though substantial learning time reduction is achieved, there is room to improve. To reduce the RL warm start work, human behavior in the initial phase of learning can be studied. For example, rather than -greedy action exploration method, human action exploration is much more efficient and will be the focus of the next study. In addition, experimental work will help validate fuel economy and computation feasibility performance of the proposed supervisory control framework.\n\nFig. 1 .\n1Propulsion system architecture of the parallel HEV.\n\n\nbounded value within the range of [-1,1]. The brake pedal position is activated when the PID output is negative, and the acceleration pedal position is activated when PID output is positive. -1 and 1 are referred to the full positions of the brake pedal and acceleration pedal, respectively.\n\nFig. 2 .\n2(a) Engine fuel flow rate map and (b) EM efficiency map.\n\nFig. 3 .\n3Q-learning EMS and vehicle interaction diagram.\n\n\nis in the magnitude of 0.001. In other large vehicle applications, a great number can be used, such as 10 or 100. The positive reward attracts the greedy action selection at the beginning phase of the learning as the initial Q values are zeros. Otherwise, if the reward is a negative value, the greedy policy will choose non-visited Q values over visited negative Q values, which diverts the HEV from ideal performance in the learning process.\n\nFig. 4 .\n4Engine efficiency map.\n\nFig. 5 .\n5Sum of rewards and fuel economy from good experience along the 5000 iterations of cold start Q-learning.\n\nFig. 6 .\n6ECMS warm start Q values initialization. Q value is as a function of speed and torque at different torque demand.\n\nFig. 7 .\n7Heuristic warm start Q values initialization. Q value is a function of speed and torque at different torque demand.\n\nFig. 8 .Fig. 9 .\n89Optimal EM torque comparison between initial Q values and end Q values of ECMS warm start iterations: (a) initial Q values of warm start, and (b) end Q values of warm start after 5000 iterations. Optimal EM torque comparison between initial Q values and end Q values of heuristic warm start iterations: (a) initial Q values of warm start, and (b) end Q values of warm start after 5000 iterations.\n\nFig. 10 .\n10ICE and EM operating efficiency from the UDDS driving cycle using all-zero Q values (cold start) prior to 5000 iterations.\n\nFig. 11 .\n11ICE and EM operating efficiency from the UDDS driving cycle using ECMS initialized Q values (warm start) prior to 5000 iterations.\n\nFig. 12 .\n12ICE and EM operating efficiency from the UDDS driving cycle using heuristic initialized Q values (warm start) prior to 5000 iterations.\n\nFig. 13 .\n13Comparison of good experience obtained along the iterations from cold start and warm start: (a) sum of rewards, and (b) fuel economy. (mean of 5 runs)\n\nFig. 14 .Fig. 15 .\n1415ECMS warm start Q values after 5000 iterations. Q value is as a function of speed and torque at different torque demand. Heuristic warm start Q values after 5000 iterations. Q value is a function of speed and torque at different torque demand.(a) ECMS Initialization (b) ECMS after 5000 iterations\n\nFig. 16 .\n16Comparison of Q-learning simulation results among cold start initialization, ECMS warm start initialization, heuristic warm start initialization and heuristic warm start final the results after 5000 iterations: (a) vehicle speed (the reference is the driving cycle speed target), (b) battery SOC, (c) Engine fuel rate and (d) cumulative fuel consumption. (Initialization means the first iteration of the 5000 iterations with optimal policy and no random action, final results means the last iteration of the 5000 iterations with optimal policy and no random action.) Fig. 17. Comparison of Q-learning simulation results among cold start initialization, ECMS warm start initialization, heuristic warm start initialization and heuristic warm start final the results after 5000 iterations: (a) vehicle speed (the reference is the driving cycle speed target), (b) battery SOC, (c) Engine fuel rate, (d) cumulative fuel consumption, (e) total torque demand, (f) EM torque and (g) engine torque. (Initialization means the first iteration of the 5000 iterations with optimal policy and no random action, final results means the last iteration of the 5000 iterations with optimal policy and no random action)\n\nFig. 18 .\n18Comparison of ECMS and Q-learning with heuristic warm start in UDDS driving cycle: (a) vehicle speed (the reference is the driving cycle speed target), (b) battery SOC, (c) Engine fuel rate, (d) cumulative fuel consumption, (e) total torque demand for vehicle, (f) EM torque and (g) engine torque.\n\nTABLE 1 .\n1Parallel HEV specification.Curb weight \n1636 kg \n\nEngine max torque \n115 Nm \n\nEngine max power/ speed \n64.6kW @6000rpm \n\nEM max torque \n400 Nm \n\nEM max power \n143 kW \n\nTransmission gear number \n4 \n\nTransmission gear ratio \n\n1 st /2.847, 2 nd /1.552, \n\n3 rd /1.000, 4 th :0.700 \n\nfinal differential ratio \n4.13 \n\nBattery cell number \n250 \n\nnumber of series connection \n250 \n\nnumber of parallel connection \n1 \n\nBattery capacity (single cell) \n6.5 Ah \n\n\n\nTable 2\n2shows the pseudocode of the Q-learning algorithm used in this study. There are four main processes: (i) experience exploration, (ii) experience evaluation, (iii) Q value update, and (iv) experience evaluation criteria update. Experience exploration aims to try some random actions and generate new experiences so that the Q-learning performance gets improved. If the warm start method is used, the first iteration of experience exploration (i.e., i=1) is executed using warm start supervisory control. If ECMS is used, ECMS directly acts as supervisory control in the first iteration. If heuristic supervisory control is used, the Q values are modified with the heuristic rules, and then the modified Q values are used in supervisory control using -greedy method with = 0. Experience evaluation aims to evaluate the quality of the experience and filter the experience used for Q-learning stateaction value function update. Q value update aims to update the Q values using the selected experience. Experience evaluation criteria update aims to choose the best criteria based on the latest knowledge so that the experience selection process gradually gets\n\nTABLE 2 .\n2\n\n\n1 Initialize ( , ) with zeros, for all \u2208 , \u2208 ( ).Initialize \nwith zero. \n\n2 for \u2208 (1, \u2026 , ) do (for each iteration): \n\n3 Experience exploration: \n\n4 if (warm start==1 & i==1) do \n\n5 \nInitialize s \n\n6 \nfor \u2208 (1, \u2026 , ) do (for each time step of iteration): \n\n7 \nTake action using warm start supervisory controls, observe , +1 from environment (i.e., vehicle model) \n\n8 \nend for \n9 else \n\n10 \nInitialize s \n\n11 \nfor \u2208 (1, \u2026 , ) do (for each time step of iteration): \n\n12 \nChoose action at state using policy derived from \n\n( -greedy action selection method, = 0.1) \n\n13 \nTake action , observe , +1 from environment \n\n14 \nend for \n\n15 end if \n\n16 Experience evaluation: \n\n17 if \u2211 1 \n> \ndo \n\n18 \nQ value update: \n\n19 \nfor \u2208 (1, \u2026 , ) do (for each time step of iteration): \n\n20 \n( , ) = ( , ) + [ + max ( +1 , ) \u2212 ( , )] \n\n21 \nend for \n\n22 \nExperience evaluation criteria update: \n\n23 \nInitialize s \n\n24 \nfor \u2208 (1, \u2026 , ) do (for each time step of iteration): \n\n25 \nChoose action at state using policy derived from \n\n( -greedy action selection method, = 0) \n\n26 \nTake action , observe , +1 from environment \n\n27 \nend for \n\n28 \n= \u2211 1 \n\n29 end if \n\n30 end for \n\n\n\n\n. Prior to the update, the Q values are all zeros. After the update, the Q values are the ECMS warm start results, which then undergo 5000 iterations.Different from ECMS warm start, the heuristic warm start does not require the single UDDS simulation. First, all the Q values are initialized to be zeros. Then, the Q values at low speed and high torque demand are set to 1.0, which guides the Q-learning to select large EM torque in this scenario. After the warm start, the Q values then undergo 5000 iterations.\n\nTable 3 .\n3Fuel economy of ECMS and heuristic warm start initialization and final results of Q-learning in UDDS driving cycle.Learning stage \nCold start Q-learning \nHeuristic warm start Q-learning ECMS warm start Q-learning \n\nInitialization (1 st iteration) \n36.6 MPG \n42 MPG \n51 MPG \n\nFinal (5000 th iteration) \n59.4 MPG \n59.6 MPG \n59.5 MPG \n\n\n\n\n(e). ECMS does not adapt to the different vehicle speed tracking performance, whereas Q-learning can adapt to this situation. More specifically, a slightly large vehicle tracking error is detrimental to fuel economy. Q-learning adapts its Q values to improve the fuel economy during the iterations. Though Qlearning is model-free, the vehicle tracking error results are reflected in the state and reward feedback from the vehicle (i.e., large speed error results in high total torque demand and large fuel consumption).\n\nTABLE 4 .\n4Fuel Economy of ECMS and Q-learning at different driving cycles.(Note: Q-learning strategy is trained in UDDS driving cycle)Driving cycle \nECMS MPG \nQ-learning MPG \n\nWLTP \n36.6 \n42.5 \n\nHWFET \n53.1 \n58.3 \n\n\n\nUnified modeling of hybrid electric vehicle drivetrains. G Rizzoni, L Guzzella, B M Baumann, IEEE/ASME transactions on mechatronics. 4G. Rizzoni, L. Guzzella, and B. M. Baumann, \"Unified modeling of hybrid electric vehicle drivetrains,\" IEEE/ASME transactions on mechatronics, vol. 4, pp. 246-257, 1999.\n\nPlug-in hybrid electric vehicle charging infrastructure review. K Morrow, D Karner, J Francfort, 34US Department of Energy-Vehicle Technologies ProgramK. Morrow, D. Karner, and J. Francfort, \"Plug-in hybrid electric vehicle charging infrastructure review,\" US Department of Energy-Vehicle Technologies Program, vol. 34, 2008.\n\nA hierarchical energy management strategy for hybrid energy storage via vehicle-to-cloud connectivity. J Hou, Z Song, Applied Energy. 257113900J. Hou and Z. Song, \"A hierarchical energy management strategy for hybrid energy storage via vehicle-to-cloud connectivity,\" Applied Energy, vol. 257, p. 113900, 2020.\n\nEnergy Management Systems for Electrified Powertrains: State-of-The-Art Review and Future Trends. A Biswas, A Emadi, IEEE Transactions on Vehicular Technology. A. Biswas and A. Emadi, \"Energy Management Systems for Electrified Powertrains: State-of-The-Art Review and Future Trends,\" IEEE Transactions on Vehicular Technology, 2019.\n\nA heuristic supervisory controller for a 48V hybrid electric vehicle considering fuel economy and battery aging. F Malmir, B Xu, Z Filipi, SAE Technical Paper 0148-7191F. Malmir, B. Xu, and Z. Filipi, \"A heuristic supervisory controller for a 48V hybrid electric vehicle considering fuel economy and battery aging,\" SAE Technical Paper 0148-7191, 2019.\n\nOptimization of power management in an hybrid electric vehicle using dynamic programming. L V P\u00e9rez, G R Bossio, D Moitre, G O Garc\u00eda, Mathematics and Computers in Simulation. 73L. V. P\u00e9rez, G. R. Bossio, D. Moitre, and G. O. Garc\u00eda, \"Optimization of power management in an hybrid electric vehicle using dynamic programming,\" Mathematics and Computers in Simulation, vol. 73, pp. 244-254, 2006.\n\nA comparative study of supervisory control strategies for hybrid electric vehicles. P Pisu, G Rizzoni, IEEE Transactions on Control Systems Technology. 15P. Pisu and G. Rizzoni, \"A comparative study of supervisory control strategies for hybrid electric vehicles,\" IEEE Transactions on Control Systems Technology, vol. 15, pp. 506-518, 2007.\n\nSimultaneous Identification and Control for Hybrid Energy Storage System using Model Predictive Control and Active Signal Injection. Z Song, H Park, F P Delgado, H Wang, Z Li, H Hofmann, J Sun, J Hou, IEEE Transactions on Industrial Electronics. Z. Song, H. Park, F. P. Delgado, H. Wang, Z. Li, H. Hofmann, J. Sun, and J. Hou, \"Simultaneous Identification and Control for Hybrid Energy Storage System using Model Predictive Control and Active Signal Injection,\" IEEE Transactions on Industrial Electronics, 2019.\n\nDynamic programming. R Bellman, Science. 153R. Bellman, \"Dynamic programming,\" Science, vol. 153, pp. 34-37, 1966.\n\nEnergy management strategy for a hybrid electric vehicle based on deep reinforcement learning. Y Hu, W Li, K Xu, T Zahid, F Qin, C Li, Applied Sciences. 8187Y. Hu, W. Li, K. Xu, T. Zahid, F. Qin, and C. Li, \"Energy management strategy for a hybrid electric vehicle based on deep reinforcement learning,\" Applied Sciences, vol. 8, p. 187, 2018.\n\nA bi-level control for energy efficiency improvement of a hybrid tracked vehicle. T Liu, X Hu, IEEE Transactions on Industrial Informatics. 14T. Liu and X. Hu, \"A bi-level control for energy efficiency improvement of a hybrid tracked vehicle,\" IEEE Transactions on Industrial Informatics, vol. 14, pp. 1616-1625, 2018.\n\nReal-time Reinforcement Learning Optimized Energy Management for a 48V Mild Hybrid Electric Vehicle. B Xu, D Rathod, Z Filipi, SAE Technical Paper 0148-7191B. Xu, D. Rathod, and Z. Filipi, \"Real-time Reinforcement Learning Optimized Energy Management for a 48V Mild Hybrid Electric Vehicle,\" SAE Technical Paper 0148-7191, 2019.\n\nContinuous reinforcement learning of energy management with deep Q network for a power split hybrid electric bus. J Wu, H He, J Peng, Y Li, Z Li, Applied Energy. 222J. Wu, H. He, J. Peng, Y. Li, and Z. Li, \"Continuous reinforcement learning of energy management with deep Q network for a power split hybrid electric bus,\" Applied Energy, vol. 222, pp. 799-811, 2018.\n\nIntelligent energy management strategy based on hierarchical approximate global optimization for plug-in fuel cell hybrid electric vehicles. J Yuan, L Yang, Q Chen, International Journal of Hydrogen Energy. 43J. Yuan, L. Yang, and Q. Chen, \"Intelligent energy management strategy based on hierarchical approximate global optimization for plug-in fuel cell hybrid electric vehicles,\" International Journal of Hydrogen Energy, vol. 43, pp. 8063-8078, 2018.\n\nData-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles. H Sun, Z Fu, F Tao, L Zhu, P Si, Journal of Power Sources. 455227964H. Sun, Z. Fu, F. Tao, L. Zhu, and P. Si, \"Data-driven reinforcement-learning-based hierarchical energy management strategy for fuel cell/battery/ultracapacitor hybrid electric vehicles,\" Journal of Power Sources, vol. 455, p. 227964, 2020.\n\nReinforcement learning-based real-time power management for hybrid energy storage system in the plug-in hybrid electric vehicle. R Xiong, J Cao, Q Yu, Applied Energy. 211R. Xiong, J. Cao, and Q. Yu, \"Reinforcement learning-based real-time power management for hybrid energy storage system in the plug-in hybrid electric vehicle,\" Applied Energy, vol. 211, pp. 538-548, 2018.\n\nDeep reinforcement learning enabled self-learning control for energy efficient driving. X Qi, Y Luo, G Wu, K Boriboonsomsin, M Barth, Transportation Research Part C: Emerging Technologies. 99X. Qi, Y. Luo, G. Wu, K. Boriboonsomsin, and M. Barth, \"Deep reinforcement learning enabled self-learning control for energy efficient driving,\" Transportation Research Part C: Emerging Technologies, vol. 99, pp. 67-81, 2019.\n\nOnline Data-Driven Energy Management of a Hybrid Electric Vehicle Using Model-Based Q-Learning. H Lee, C Kang, Y.-I Park, N Kim, S W Cha, IEEE Access. 8H. Lee, C. Kang, Y.-I. Park, N. Kim, and S. W. Cha, \"Online Data-Driven Energy Management of a Hybrid Electric Vehicle Using Model-Based Q-Learning,\" IEEE Access, vol. 8, pp. 84444-84454, 2020.\n\nRoute Planning and Power Management for PHEVs With Reinforcement Learning. Q Zhang, K Wu, Y Shi, IEEE Transactions on Vehicular Technology. 69Q. Zhang, K. Wu, and Y. Shi, \"Route Planning and Power Management for PHEVs With Reinforcement Learning,\" IEEE Transactions on Vehicular Technology, vol. 69, pp. 4751-4762, 2020.\n\nData-driven reinforcement learning-based real-time energy management system for plug-in hybrid electric vehicles. X Qi, G Wu, K Boriboonsomsin, M J Barth, J Gonder, Transportation Research Record. 2572X. Qi, G. Wu, K. Boriboonsomsin, M. J. Barth, and J. Gonder, \"Data-driven reinforcement learning-based real-time energy management system for plug-in hybrid electric vehicles,\" Transportation Research Record, vol. 2572, pp. 1-8, 2016.\n\nReinforcement Learning-based Real-time Energy Management for Plug-in Hybrid Electric Vehicle with Hybrid Energy Storage System. J Cao, R Xiong, Energy Procedia. 142J. Cao and R. Xiong, \"Reinforcement Learning-based Real-time Energy Management for Plug-in Hybrid Electric Vehicle with Hybrid Energy Storage System,\" Energy Procedia, vol. 142, pp. 1896-1901, 2017.\n\nT P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, \"Continuous control with deep reinforcement learning,\" arXiv preprint arXiv:1509.02971, 2015.\n\nDeep reinforcement learning for robotic manipulation with asynchronous off-policy updates. S Gu, E Holly, T Lillicrap, S Levine, 2017 IEEE international conference on robotics and automation (ICRA. S. Gu, E. Holly, T. Lillicrap, and S. Levine, \"Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates,\" in 2017 IEEE international conference on robotics and automation (ICRA), 2017, pp. 3389-3396.\n\nMultiagent learning using a variable learning rate. M Bowling, M Veloso, Artificial Intelligence. 136M. Bowling and M. Veloso, \"Multiagent learning using a variable learning rate,\" Artificial Intelligence, vol. 136, pp. 215-250, 2002.\n\nSelf-imitation learning. J Oh, Y Guo, S Singh, H Lee, arXiv:1806.05635arXiv preprintJ. Oh, Y. Guo, S. Singh, and H. Lee, \"Self-imitation learning,\" arXiv preprint arXiv:1806.05635, 2018.\n\nEffective warm start for the online actor-critic reinforcement learning based mhealth intervention. F Zhu, P Liao, arXiv:1704.04866arXiv preprintF. Zhu and P. Liao, \"Effective warm start for the online actor-critic reinforcement learning based mhealth intervention,\" arXiv preprint arXiv:1704.04866, 2017.\n\nProLoNets: Neural-encoding Human Experts' Domain Knowledge to Warm Start Reinforcement Learning. A Silva, M Gombolay, arXiv:1902.06007arXiv preprintA. Silva and M. Gombolay, \"ProLoNets: Neural-encoding Human Experts' Domain Knowledge to Warm Start Reinforcement Learning,\" arXiv preprint arXiv:1902.06007, 2019.\n\nHarnessing deep neural networks with logic rules. Z Hu, X Ma, Z Liu, E Hovy, E Xing, arXiv:1603.06318arXiv preprintZ. Hu, X. Ma, Z. Liu, E. Hovy, and E. Xing, \"Harnessing deep neural networks with logic rules,\" arXiv preprint arXiv:1603.06318, 2016.\n\nCombining Knowledge with Deep Convolutional Neural Networks for Short Text Classification. J Wang, Z Wang, D Zhang, J Yan, IJCAI. J. Wang, Z. Wang, D. Zhang, and J. Yan, \"Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification,\" in IJCAI, 2017, pp. 2915-2921.\n\nParametric study on reinforcement learning optimized energy management strategy for a hybrid electric vehicle. B Xu, D Rathod, D Zhang, A Yebi, X Zhang, X Li, Z Filipi, Applied Energy. 259114200B. Xu, D. Rathod, D. Zhang, A. Yebi, X. Zhang, X. Li, and Z. Filipi, \"Parametric study on reinforcement learning optimized energy management strategy for a hybrid electric vehicle,\" Applied Energy, vol. 259, p. 114200, 2020.\n\nReinforcement learning: An introduction. R S Sutton, A G Barto, F Bach, MIT pressR. S. Sutton, A. G. Barto, and F. Bach, Reinforcement learning: An introduction: MIT press, 1998.\n\nVehicle and Fuel Emission Testing -Dynamometer Drive Schedules. EPAEPA. Vehicle and Fuel Emission Testing -Dynamometer Drive Schedules. Available: https://www.epa.gov/vehicle-and-fuel-emissions-testing/dynamometer- drive-schedules\n\nA deep reinforcement learning framework for optimizing fuel economy of hybrid electric vehicles. P Zhao, Y Wang, N Chang, Q Zhu, X Lin, 2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC). P. Zhao, Y. Wang, N. Chang, Q. Zhu, and X. Lin, \"A deep reinforcement learning framework for optimizing fuel economy of hybrid electric vehicles,\" in 2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC), 2018, pp. 196-202.\n\nDeep Q-Learning Based Energy Management Strategy for a Series Hybrid Electric Tracked Vehicle and Its Adaptability Validation. D He, Y Zou, J Wu, X Zhang, Z Zhang, R Wang, 2019 IEEE Transportation Electrification Conference and Expo (ITEC). D. He, Y. Zou, J. Wu, X. Zhang, Z. Zhang, and R. Wang, \"Deep Q-Learning Based Energy Management Strategy for a Series Hybrid Electric Tracked Vehicle and Its Adaptability Validation,\" in 2019 IEEE Transportation Electrification Conference and Expo (ITEC), 2019, pp. 1-6.\n\nDieselnet, Worldwide Harmonized Light Vehicles Test Cycle (WLTC). Dieselnet, \"Worldwide Harmonized Light Vehicles Test Cycle (WLTC),\" ed: Dieselnet.\n\nExperimental investigation on the dynamic performance of a hybrid PEM fuel cell/battery system for lightweight electric vehicle application. Y Tang, W Yuan, M Q Pan, Z P Wan, Applied Energy. 88Y. Tang, W. Yuan, M. Q. Pan, and Z. P. Wan, \"Experimental investigation on the dynamic performance of a hybrid PEM fuel cell/battery system for lightweight electric vehicle application,\" Applied Energy, vol. 88, pp. 68-76, 2011.\n\nIntelligent energy management for hybrid electric tracked vehicles using online reinforcement learning. G D Du, Y Zou, X D Zhang, Z H Kong, J L Wu, D B He, Applied Energy. 251G. D. Du, Y. Zou, X. D. Zhang, Z. H. Kong, J. L. Wu, and D. B. He, \"Intelligent energy management for hybrid electric tracked vehicles using online reinforcement learning,\" Applied Energy, vol. 251, 2019.\n", "annotations": {"author": "[{\"end\":95,\"start\":4},{\"end\":214,\"start\":96},{\"end\":331,\"start\":215},{\"end\":428,\"start\":332},{\"end\":520,\"start\":429},{\"end\":643,\"start\":521}]", "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": "[{\"end\":94,\"start\":5},{\"end\":213,\"start\":97},{\"end\":330,\"start\":216},{\"end\":427,\"start\":333},{\"end\":519,\"start\":430},{\"end\":642,\"start\":522}]", "title": null, "venue": null, "abstract": "[{\"end\":2243,\"start\":778}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2409,\"start\":2406},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2412,\"start\":2409},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2415,\"start\":2412},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2647,\"start\":2644},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2948,\"start\":2945},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2978,\"start\":2975},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3035,\"start\":3032},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3075,\"start\":3072},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4839,\"start\":4836},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5267,\"start\":5263},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5324,\"start\":5320},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5417,\"start\":5413},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5665,\"start\":5661},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5875,\"start\":5871},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6463,\"start\":6459},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6654,\"start\":6650},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6828,\"start\":6824},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6999,\"start\":6995},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7306,\"start\":7302},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7912,\"start\":7908},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7936,\"start\":7932},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7961,\"start\":7957},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7987,\"start\":7983},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8016,\"start\":8012},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8106,\"start\":8102},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8109,\"start\":8106},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8112,\"start\":8109},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8213,\"start\":8209},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8216,\"start\":8213},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8395,\"start\":8391},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8523,\"start\":8519},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9210,\"start\":9206},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9251,\"start\":9247},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9278,\"start\":9274},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9872,\"start\":9868},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10754,\"start\":10750},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10772,\"start\":10768},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10855,\"start\":10851},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10858,\"start\":10855},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11960,\"start\":11957},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17474,\"start\":17471},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17982,\"start\":17978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18943,\"start\":18939},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19586,\"start\":19582},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20679,\"start\":20675},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20682,\"start\":20679},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20685,\"start\":20682},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20688,\"start\":20685},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20822,\"start\":20818},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22825,\"start\":22821},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23162,\"start\":23158},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":36744,\"start\":36740},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36747,\"start\":36744},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37662,\"start\":37658},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37692,\"start\":37688},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":38298,\"start\":38294},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38620,\"start\":38616},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":38623,\"start\":38620},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40054,\"start\":40050},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40058,\"start\":40054},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40062,\"start\":40058},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40066,\"start\":40062},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":40070,\"start\":40066},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":40074,\"start\":40070},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":40078,\"start\":40074},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40082,\"start\":40078},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":40086,\"start\":40082},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40090,\"start\":40086},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40094,\"start\":40090},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40098,\"start\":40094},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":40102,\"start\":40098},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":40106,\"start\":40102}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42796,\"start\":42734},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43090,\"start\":42797},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43158,\"start\":43091},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43217,\"start\":43159},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43663,\"start\":43218},{\"attributes\":{\"id\":\"fig_6\"},\"end\":43697,\"start\":43664},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43813,\"start\":43698},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43938,\"start\":43814},{\"attributes\":{\"id\":\"fig_9\"},\"end\":44065,\"start\":43939},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44482,\"start\":44066},{\"attributes\":{\"id\":\"fig_11\"},\"end\":44618,\"start\":44483},{\"attributes\":{\"id\":\"fig_12\"},\"end\":44762,\"start\":44619},{\"attributes\":{\"id\":\"fig_13\"},\"end\":44911,\"start\":44763},{\"attributes\":{\"id\":\"fig_14\"},\"end\":45075,\"start\":44912},{\"attributes\":{\"id\":\"fig_15\"},\"end\":45397,\"start\":45076},{\"attributes\":{\"id\":\"fig_16\"},\"end\":46611,\"start\":45398},{\"attributes\":{\"id\":\"fig_17\"},\"end\":46922,\"start\":46612},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47385,\"start\":46923},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48549,\"start\":47386},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48562,\"start\":48550},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49718,\"start\":48563},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50233,\"start\":49719},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50579,\"start\":50234},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51101,\"start\":50580},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51319,\"start\":51102}]", "paragraph": "[{\"end\":2648,\"start\":2245},{\"end\":4840,\"start\":2650},{\"end\":7471,\"start\":4842},{\"end\":7723,\"start\":7473},{\"end\":8753,\"start\":7725},{\"end\":9091,\"start\":8755},{\"end\":11167,\"start\":9093},{\"end\":11260,\"start\":11169},{\"end\":11482,\"start\":11262},{\"end\":11604,\"start\":11484},{\"end\":11724,\"start\":11606},{\"end\":12146,\"start\":11726},{\"end\":13120,\"start\":12148},{\"end\":13777,\"start\":13162},{\"end\":13863,\"start\":13807},{\"end\":14041,\"start\":13876},{\"end\":14161,\"start\":14048},{\"end\":14224,\"start\":14170},{\"end\":14540,\"start\":14257},{\"end\":14581,\"start\":14542},{\"end\":14608,\"start\":14605},{\"end\":14693,\"start\":14610},{\"end\":14921,\"start\":14739},{\"end\":14970,\"start\":14923},{\"end\":15187,\"start\":15031},{\"end\":15306,\"start\":15195},{\"end\":15431,\"start\":15319},{\"end\":15443,\"start\":15440},{\"end\":15581,\"start\":15454},{\"end\":15687,\"start\":15605},{\"end\":15811,\"start\":15697},{\"end\":15939,\"start\":15828},{\"end\":17026,\"start\":16004},{\"end\":17114,\"start\":17028},{\"end\":18161,\"start\":17157},{\"end\":18483,\"start\":18163},{\"end\":19145,\"start\":18520},{\"end\":19563,\"start\":19201},{\"end\":19967,\"start\":19565},{\"end\":20823,\"start\":19981},{\"end\":21308,\"start\":20837},{\"end\":22542,\"start\":21359},{\"end\":23062,\"start\":22594},{\"end\":23227,\"start\":23102},{\"end\":23875,\"start\":23229},{\"end\":24050,\"start\":23877},{\"end\":24429,\"start\":24052},{\"end\":24723,\"start\":24458},{\"end\":24976,\"start\":24725},{\"end\":25099,\"start\":24978},{\"end\":25617,\"start\":25128},{\"end\":26121,\"start\":25619},{\"end\":31017,\"start\":26123},{\"end\":32717,\"start\":31019},{\"end\":33904,\"start\":32719},{\"end\":34552,\"start\":33906},{\"end\":34939,\"start\":34554},{\"end\":35643,\"start\":34964},{\"end\":36788,\"start\":35645},{\"end\":40675,\"start\":36790},{\"end\":42248,\"start\":40693},{\"end\":42733,\"start\":42250}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13875,\"start\":13864},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14047,\"start\":14042},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14169,\"start\":14162},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14256,\"start\":14225},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14604,\"start\":14582},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14713,\"start\":14694},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14724,\"start\":14713},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14738,\"start\":14724},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15016,\"start\":14971},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15030,\"start\":15016},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15194,\"start\":15188},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15318,\"start\":15307},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15439,\"start\":15432},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15453,\"start\":15444},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15696,\"start\":15688},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15827,\"start\":15812},{\"attributes\":{\"id\":\"formula_18\"},\"end\":17156,\"start\":17115},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19200,\"start\":19146}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13727,\"start\":13720},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24812,\"start\":24805},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25363,\"start\":25356},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29324,\"start\":29317},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37791,\"start\":37784},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39677,\"start\":39670},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40117,\"start\":40110}]", "section_header": "[{\"end\":13160,\"start\":13123},{\"end\":13805,\"start\":13780},{\"end\":15603,\"start\":15584},{\"end\":15977,\"start\":15942},{\"end\":16002,\"start\":15980},{\"end\":18506,\"start\":18486},{\"end\":18518,\"start\":18509},{\"end\":19979,\"start\":19970},{\"end\":20835,\"start\":20826},{\"end\":21357,\"start\":21311},{\"end\":22592,\"start\":22545},{\"end\":23100,\"start\":23065},{\"attributes\":{\"n\":\"4.1\"},\"end\":24456,\"start\":24432},{\"attributes\":{\"n\":\"4.2\"},\"end\":25126,\"start\":25102},{\"attributes\":{\"n\":\"4.3\"},\"end\":34962,\"start\":34942},{\"end\":40691,\"start\":40678},{\"end\":42743,\"start\":42735},{\"end\":43100,\"start\":43092},{\"end\":43168,\"start\":43160},{\"end\":43673,\"start\":43665},{\"end\":43707,\"start\":43699},{\"end\":43823,\"start\":43815},{\"end\":43948,\"start\":43940},{\"end\":44083,\"start\":44067},{\"end\":44493,\"start\":44484},{\"end\":44629,\"start\":44620},{\"end\":44773,\"start\":44764},{\"end\":44922,\"start\":44913},{\"end\":45095,\"start\":45077},{\"end\":45408,\"start\":45399},{\"end\":46622,\"start\":46613},{\"end\":46933,\"start\":46924},{\"end\":47394,\"start\":47387},{\"end\":48560,\"start\":48551},{\"end\":50244,\"start\":50235},{\"end\":51112,\"start\":51103}]", "table": "[{\"end\":47385,\"start\":46962},{\"end\":49718,\"start\":48614},{\"end\":50579,\"start\":50361},{\"end\":51319,\"start\":51238}]", "figure_caption": "[{\"end\":42796,\"start\":42745},{\"end\":43090,\"start\":42799},{\"end\":43158,\"start\":43102},{\"end\":43217,\"start\":43170},{\"end\":43663,\"start\":43220},{\"end\":43697,\"start\":43675},{\"end\":43813,\"start\":43709},{\"end\":43938,\"start\":43825},{\"end\":44065,\"start\":43950},{\"end\":44482,\"start\":44086},{\"end\":44618,\"start\":44496},{\"end\":44762,\"start\":44632},{\"end\":44911,\"start\":44776},{\"end\":45075,\"start\":44925},{\"end\":45397,\"start\":45100},{\"end\":46611,\"start\":45411},{\"end\":46922,\"start\":46625},{\"end\":46962,\"start\":46935},{\"end\":48549,\"start\":47396},{\"end\":48614,\"start\":48565},{\"end\":50233,\"start\":49721},{\"end\":50361,\"start\":50246},{\"end\":51101,\"start\":50582},{\"end\":51238,\"start\":51114}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13294,\"start\":13288},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14676,\"start\":14670},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15077,\"start\":15071},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16108,\"start\":16102},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21843,\"start\":21837},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24905,\"start\":24899},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25002,\"start\":24993},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25479,\"start\":25473},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":25491,\"start\":25484},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25613,\"start\":25607},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25744,\"start\":25738},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25882,\"start\":25874},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26206,\"start\":26197},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":26714,\"start\":26703},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26757,\"start\":26751},{\"end\":26853,\"start\":26844},{\"end\":26953,\"start\":26944},{\"end\":27160,\"start\":27152},{\"end\":27272,\"start\":27263},{\"end\":27371,\"start\":27362},{\"end\":27474,\"start\":27465},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27731,\"start\":27715},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27743,\"start\":27736},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27797,\"start\":27778},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28254,\"start\":28244},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28329,\"start\":28319},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28387,\"start\":28377},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28402,\"start\":28392},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28503,\"start\":28496},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28518,\"start\":28511},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28615,\"start\":28605},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29096,\"start\":29089},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30484,\"start\":30464},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30755,\"start\":30739},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30813,\"start\":30796},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30954,\"start\":30948},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31016,\"start\":31006},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":31109,\"start\":31103},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31127,\"start\":31117},{\"end\":31548,\"start\":31539},{\"end\":31562,\"start\":31553},{\"end\":31605,\"start\":31588},{\"end\":31879,\"start\":31862},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32211,\"start\":32202},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32223,\"start\":32216},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32238,\"start\":32228},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32358,\"start\":32348},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32629,\"start\":32622},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32732,\"start\":32722},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33021,\"start\":33014},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33189,\"start\":33179},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33336,\"start\":33326},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33583,\"start\":33576},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33738,\"start\":33731},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34334,\"start\":34327},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34480,\"start\":34470},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34537,\"start\":34530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34717,\"start\":34707},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34732,\"start\":34722},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34937,\"start\":34927},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35914,\"start\":35907},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36063,\"start\":36053},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36085,\"start\":36078},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36304,\"start\":36294},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37507,\"start\":37500},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39437,\"start\":39430},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40595,\"start\":40588}]", "bib_author_first_name": "[{\"end\":51379,\"start\":51378},{\"end\":51390,\"start\":51389},{\"end\":51402,\"start\":51401},{\"end\":51404,\"start\":51403},{\"end\":51691,\"start\":51690},{\"end\":51701,\"start\":51700},{\"end\":51711,\"start\":51710},{\"end\":52057,\"start\":52056},{\"end\":52064,\"start\":52063},{\"end\":52364,\"start\":52363},{\"end\":52374,\"start\":52373},{\"end\":52713,\"start\":52712},{\"end\":52723,\"start\":52722},{\"end\":52729,\"start\":52728},{\"end\":53044,\"start\":53043},{\"end\":53046,\"start\":53045},{\"end\":53055,\"start\":53054},{\"end\":53057,\"start\":53056},{\"end\":53067,\"start\":53066},{\"end\":53077,\"start\":53076},{\"end\":53079,\"start\":53078},{\"end\":53434,\"start\":53433},{\"end\":53442,\"start\":53441},{\"end\":53825,\"start\":53824},{\"end\":53833,\"start\":53832},{\"end\":53841,\"start\":53840},{\"end\":53843,\"start\":53842},{\"end\":53854,\"start\":53853},{\"end\":53862,\"start\":53861},{\"end\":53868,\"start\":53867},{\"end\":53879,\"start\":53878},{\"end\":53886,\"start\":53885},{\"end\":54227,\"start\":54226},{\"end\":54417,\"start\":54416},{\"end\":54423,\"start\":54422},{\"end\":54429,\"start\":54428},{\"end\":54435,\"start\":54434},{\"end\":54444,\"start\":54443},{\"end\":54451,\"start\":54450},{\"end\":54749,\"start\":54748},{\"end\":54756,\"start\":54755},{\"end\":55088,\"start\":55087},{\"end\":55094,\"start\":55093},{\"end\":55104,\"start\":55103},{\"end\":55431,\"start\":55430},{\"end\":55437,\"start\":55436},{\"end\":55443,\"start\":55442},{\"end\":55451,\"start\":55450},{\"end\":55457,\"start\":55456},{\"end\":55826,\"start\":55825},{\"end\":55834,\"start\":55833},{\"end\":55842,\"start\":55841},{\"end\":56285,\"start\":56284},{\"end\":56292,\"start\":56291},{\"end\":56298,\"start\":56297},{\"end\":56305,\"start\":56304},{\"end\":56312,\"start\":56311},{\"end\":56724,\"start\":56723},{\"end\":56733,\"start\":56732},{\"end\":56740,\"start\":56739},{\"end\":57059,\"start\":57058},{\"end\":57065,\"start\":57064},{\"end\":57072,\"start\":57071},{\"end\":57078,\"start\":57077},{\"end\":57096,\"start\":57095},{\"end\":57485,\"start\":57484},{\"end\":57492,\"start\":57491},{\"end\":57503,\"start\":57499},{\"end\":57511,\"start\":57510},{\"end\":57518,\"start\":57517},{\"end\":57520,\"start\":57519},{\"end\":57811,\"start\":57810},{\"end\":57820,\"start\":57819},{\"end\":57826,\"start\":57825},{\"end\":58172,\"start\":58171},{\"end\":58178,\"start\":58177},{\"end\":58184,\"start\":58183},{\"end\":58202,\"start\":58201},{\"end\":58204,\"start\":58203},{\"end\":58213,\"start\":58212},{\"end\":58623,\"start\":58622},{\"end\":58630,\"start\":58629},{\"end\":58859,\"start\":58858},{\"end\":58861,\"start\":58860},{\"end\":58874,\"start\":58873},{\"end\":58876,\"start\":58875},{\"end\":58884,\"start\":58883},{\"end\":58895,\"start\":58894},{\"end\":58904,\"start\":58903},{\"end\":58912,\"start\":58911},{\"end\":58921,\"start\":58920},{\"end\":58931,\"start\":58930},{\"end\":59310,\"start\":59309},{\"end\":59316,\"start\":59315},{\"end\":59325,\"start\":59324},{\"end\":59338,\"start\":59337},{\"end\":59703,\"start\":59702},{\"end\":59714,\"start\":59713},{\"end\":59912,\"start\":59911},{\"end\":59918,\"start\":59917},{\"end\":59925,\"start\":59924},{\"end\":59934,\"start\":59933},{\"end\":60175,\"start\":60174},{\"end\":60182,\"start\":60181},{\"end\":60479,\"start\":60478},{\"end\":60488,\"start\":60487},{\"end\":60745,\"start\":60744},{\"end\":60751,\"start\":60750},{\"end\":60757,\"start\":60756},{\"end\":60764,\"start\":60763},{\"end\":60772,\"start\":60771},{\"end\":61037,\"start\":61036},{\"end\":61045,\"start\":61044},{\"end\":61053,\"start\":61052},{\"end\":61062,\"start\":61061},{\"end\":61352,\"start\":61351},{\"end\":61358,\"start\":61357},{\"end\":61368,\"start\":61367},{\"end\":61377,\"start\":61376},{\"end\":61385,\"start\":61384},{\"end\":61394,\"start\":61393},{\"end\":61400,\"start\":61399},{\"end\":61702,\"start\":61701},{\"end\":61704,\"start\":61703},{\"end\":61714,\"start\":61713},{\"end\":61716,\"start\":61715},{\"end\":61725,\"start\":61724},{\"end\":62170,\"start\":62169},{\"end\":62178,\"start\":62177},{\"end\":62186,\"start\":62185},{\"end\":62195,\"start\":62194},{\"end\":62202,\"start\":62201},{\"end\":62652,\"start\":62651},{\"end\":62658,\"start\":62657},{\"end\":62665,\"start\":62664},{\"end\":62671,\"start\":62670},{\"end\":62680,\"start\":62679},{\"end\":62689,\"start\":62688},{\"end\":63329,\"start\":63328},{\"end\":63337,\"start\":63336},{\"end\":63345,\"start\":63344},{\"end\":63347,\"start\":63346},{\"end\":63354,\"start\":63353},{\"end\":63356,\"start\":63355},{\"end\":63715,\"start\":63714},{\"end\":63717,\"start\":63716},{\"end\":63723,\"start\":63722},{\"end\":63730,\"start\":63729},{\"end\":63732,\"start\":63731},{\"end\":63741,\"start\":63740},{\"end\":63743,\"start\":63742},{\"end\":63751,\"start\":63750},{\"end\":63753,\"start\":63752},{\"end\":63759,\"start\":63758},{\"end\":63761,\"start\":63760}]", "bib_author_last_name": "[{\"end\":51387,\"start\":51380},{\"end\":51399,\"start\":51391},{\"end\":51412,\"start\":51405},{\"end\":51698,\"start\":51692},{\"end\":51708,\"start\":51702},{\"end\":51721,\"start\":51712},{\"end\":52061,\"start\":52058},{\"end\":52069,\"start\":52065},{\"end\":52371,\"start\":52365},{\"end\":52380,\"start\":52375},{\"end\":52720,\"start\":52714},{\"end\":52726,\"start\":52724},{\"end\":52736,\"start\":52730},{\"end\":53052,\"start\":53047},{\"end\":53064,\"start\":53058},{\"end\":53074,\"start\":53068},{\"end\":53086,\"start\":53080},{\"end\":53439,\"start\":53435},{\"end\":53450,\"start\":53443},{\"end\":53830,\"start\":53826},{\"end\":53838,\"start\":53834},{\"end\":53851,\"start\":53844},{\"end\":53859,\"start\":53855},{\"end\":53865,\"start\":53863},{\"end\":53876,\"start\":53869},{\"end\":53883,\"start\":53880},{\"end\":53890,\"start\":53887},{\"end\":54235,\"start\":54228},{\"end\":54420,\"start\":54418},{\"end\":54426,\"start\":54424},{\"end\":54432,\"start\":54430},{\"end\":54441,\"start\":54436},{\"end\":54448,\"start\":54445},{\"end\":54454,\"start\":54452},{\"end\":54753,\"start\":54750},{\"end\":54759,\"start\":54757},{\"end\":55091,\"start\":55089},{\"end\":55101,\"start\":55095},{\"end\":55111,\"start\":55105},{\"end\":55434,\"start\":55432},{\"end\":55440,\"start\":55438},{\"end\":55448,\"start\":55444},{\"end\":55454,\"start\":55452},{\"end\":55460,\"start\":55458},{\"end\":55831,\"start\":55827},{\"end\":55839,\"start\":55835},{\"end\":55847,\"start\":55843},{\"end\":56289,\"start\":56286},{\"end\":56295,\"start\":56293},{\"end\":56302,\"start\":56299},{\"end\":56309,\"start\":56306},{\"end\":56315,\"start\":56313},{\"end\":56730,\"start\":56725},{\"end\":56737,\"start\":56734},{\"end\":56743,\"start\":56741},{\"end\":57062,\"start\":57060},{\"end\":57069,\"start\":57066},{\"end\":57075,\"start\":57073},{\"end\":57093,\"start\":57079},{\"end\":57102,\"start\":57097},{\"end\":57489,\"start\":57486},{\"end\":57497,\"start\":57493},{\"end\":57508,\"start\":57504},{\"end\":57515,\"start\":57512},{\"end\":57524,\"start\":57521},{\"end\":57817,\"start\":57812},{\"end\":57823,\"start\":57821},{\"end\":57830,\"start\":57827},{\"end\":58175,\"start\":58173},{\"end\":58181,\"start\":58179},{\"end\":58199,\"start\":58185},{\"end\":58210,\"start\":58205},{\"end\":58220,\"start\":58214},{\"end\":58627,\"start\":58624},{\"end\":58636,\"start\":58631},{\"end\":58871,\"start\":58862},{\"end\":58881,\"start\":58877},{\"end\":58892,\"start\":58885},{\"end\":58901,\"start\":58896},{\"end\":58909,\"start\":58905},{\"end\":58918,\"start\":58913},{\"end\":58928,\"start\":58922},{\"end\":58940,\"start\":58932},{\"end\":59313,\"start\":59311},{\"end\":59322,\"start\":59317},{\"end\":59335,\"start\":59326},{\"end\":59345,\"start\":59339},{\"end\":59711,\"start\":59704},{\"end\":59721,\"start\":59715},{\"end\":59915,\"start\":59913},{\"end\":59922,\"start\":59919},{\"end\":59931,\"start\":59926},{\"end\":59938,\"start\":59935},{\"end\":60179,\"start\":60176},{\"end\":60187,\"start\":60183},{\"end\":60485,\"start\":60480},{\"end\":60497,\"start\":60489},{\"end\":60748,\"start\":60746},{\"end\":60754,\"start\":60752},{\"end\":60761,\"start\":60758},{\"end\":60769,\"start\":60765},{\"end\":60777,\"start\":60773},{\"end\":61042,\"start\":61038},{\"end\":61050,\"start\":61046},{\"end\":61059,\"start\":61054},{\"end\":61066,\"start\":61063},{\"end\":61355,\"start\":61353},{\"end\":61365,\"start\":61359},{\"end\":61374,\"start\":61369},{\"end\":61382,\"start\":61378},{\"end\":61391,\"start\":61386},{\"end\":61397,\"start\":61395},{\"end\":61407,\"start\":61401},{\"end\":61711,\"start\":61705},{\"end\":61722,\"start\":61717},{\"end\":61730,\"start\":61726},{\"end\":62175,\"start\":62171},{\"end\":62183,\"start\":62179},{\"end\":62192,\"start\":62187},{\"end\":62199,\"start\":62196},{\"end\":62206,\"start\":62203},{\"end\":62655,\"start\":62653},{\"end\":62662,\"start\":62659},{\"end\":62668,\"start\":62666},{\"end\":62677,\"start\":62672},{\"end\":62686,\"start\":62681},{\"end\":62694,\"start\":62690},{\"end\":63046,\"start\":63037},{\"end\":63334,\"start\":63330},{\"end\":63342,\"start\":63338},{\"end\":63351,\"start\":63348},{\"end\":63360,\"start\":63357},{\"end\":63720,\"start\":63718},{\"end\":63727,\"start\":63724},{\"end\":63738,\"start\":63733},{\"end\":63748,\"start\":63744},{\"end\":63756,\"start\":63754},{\"end\":63764,\"start\":63762}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":110991017},\"end\":51624,\"start\":51321},{\"attributes\":{\"id\":\"b1\"},\"end\":51951,\"start\":51626},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208832260},\"end\":52263,\"start\":51953},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":155946299},\"end\":52597,\"start\":52265},{\"attributes\":{\"id\":\"b4\"},\"end\":52951,\"start\":52599},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1424425},\"end\":53347,\"start\":52953},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":24180321},\"end\":53689,\"start\":53349},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":209795094},\"end\":54203,\"start\":53691},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2242825},\"end\":54319,\"start\":54205},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3335319},\"end\":54664,\"start\":54321},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4618346},\"end\":54984,\"start\":54666},{\"attributes\":{\"id\":\"b11\"},\"end\":55314,\"start\":54986},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":115368293},\"end\":55682,\"start\":55316},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53534551},\"end\":56138,\"start\":55684},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":216433066},\"end\":56592,\"start\":56140},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13857490},\"end\":56968,\"start\":56594},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":115937496},\"end\":57386,\"start\":56970},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":218651392},\"end\":57733,\"start\":57388},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215844753},\"end\":58055,\"start\":57735},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53006226},\"end\":58492,\"start\":58057},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":116414677},\"end\":58856,\"start\":58494},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b21\"},\"end\":59216,\"start\":58858},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18389147},\"end\":59648,\"start\":59218},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11866341},\"end\":59884,\"start\":59650},{\"attributes\":{\"doi\":\"arXiv:1806.05635\",\"id\":\"b24\"},\"end\":60072,\"start\":59886},{\"attributes\":{\"doi\":\"arXiv:1704.04866\",\"id\":\"b25\"},\"end\":60379,\"start\":60074},{\"attributes\":{\"doi\":\"arXiv:1902.06007\",\"id\":\"b26\"},\"end\":60692,\"start\":60381},{\"attributes\":{\"doi\":\"arXiv:1603.06318\",\"id\":\"b27\"},\"end\":60943,\"start\":60694},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":21347790},\"end\":61238,\"start\":60945},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":213287908},\"end\":61658,\"start\":61240},{\"attributes\":{\"id\":\"b30\"},\"end\":61838,\"start\":61660},{\"attributes\":{\"id\":\"b31\"},\"end\":62070,\"start\":61840},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3500820},\"end\":62522,\"start\":62072},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":199510285},\"end\":63035,\"start\":62524},{\"attributes\":{\"id\":\"b34\"},\"end\":63185,\"start\":63037},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":108959736},\"end\":63608,\"start\":63187},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":191175936},\"end\":63989,\"start\":63610}]", "bib_title": "[{\"end\":51376,\"start\":51321},{\"end\":52054,\"start\":51953},{\"end\":52361,\"start\":52265},{\"end\":53041,\"start\":52953},{\"end\":53431,\"start\":53349},{\"end\":53822,\"start\":53691},{\"end\":54224,\"start\":54205},{\"end\":54414,\"start\":54321},{\"end\":54746,\"start\":54666},{\"end\":55428,\"start\":55316},{\"end\":55823,\"start\":55684},{\"end\":56282,\"start\":56140},{\"end\":56721,\"start\":56594},{\"end\":57056,\"start\":56970},{\"end\":57482,\"start\":57388},{\"end\":57808,\"start\":57735},{\"end\":58169,\"start\":58057},{\"end\":58620,\"start\":58494},{\"end\":59307,\"start\":59218},{\"end\":59700,\"start\":59650},{\"end\":61034,\"start\":60945},{\"end\":61349,\"start\":61240},{\"end\":62167,\"start\":62072},{\"end\":62649,\"start\":62524},{\"end\":63326,\"start\":63187},{\"end\":63712,\"start\":63610}]", "bib_author": "[{\"end\":51389,\"start\":51378},{\"end\":51401,\"start\":51389},{\"end\":51414,\"start\":51401},{\"end\":51700,\"start\":51690},{\"end\":51710,\"start\":51700},{\"end\":51723,\"start\":51710},{\"end\":52063,\"start\":52056},{\"end\":52071,\"start\":52063},{\"end\":52373,\"start\":52363},{\"end\":52382,\"start\":52373},{\"end\":52722,\"start\":52712},{\"end\":52728,\"start\":52722},{\"end\":52738,\"start\":52728},{\"end\":53054,\"start\":53043},{\"end\":53066,\"start\":53054},{\"end\":53076,\"start\":53066},{\"end\":53088,\"start\":53076},{\"end\":53441,\"start\":53433},{\"end\":53452,\"start\":53441},{\"end\":53832,\"start\":53824},{\"end\":53840,\"start\":53832},{\"end\":53853,\"start\":53840},{\"end\":53861,\"start\":53853},{\"end\":53867,\"start\":53861},{\"end\":53878,\"start\":53867},{\"end\":53885,\"start\":53878},{\"end\":53892,\"start\":53885},{\"end\":54237,\"start\":54226},{\"end\":54422,\"start\":54416},{\"end\":54428,\"start\":54422},{\"end\":54434,\"start\":54428},{\"end\":54443,\"start\":54434},{\"end\":54450,\"start\":54443},{\"end\":54456,\"start\":54450},{\"end\":54755,\"start\":54748},{\"end\":54761,\"start\":54755},{\"end\":55093,\"start\":55087},{\"end\":55103,\"start\":55093},{\"end\":55113,\"start\":55103},{\"end\":55436,\"start\":55430},{\"end\":55442,\"start\":55436},{\"end\":55450,\"start\":55442},{\"end\":55456,\"start\":55450},{\"end\":55462,\"start\":55456},{\"end\":55833,\"start\":55825},{\"end\":55841,\"start\":55833},{\"end\":55849,\"start\":55841},{\"end\":56291,\"start\":56284},{\"end\":56297,\"start\":56291},{\"end\":56304,\"start\":56297},{\"end\":56311,\"start\":56304},{\"end\":56317,\"start\":56311},{\"end\":56732,\"start\":56723},{\"end\":56739,\"start\":56732},{\"end\":56745,\"start\":56739},{\"end\":57064,\"start\":57058},{\"end\":57071,\"start\":57064},{\"end\":57077,\"start\":57071},{\"end\":57095,\"start\":57077},{\"end\":57104,\"start\":57095},{\"end\":57491,\"start\":57484},{\"end\":57499,\"start\":57491},{\"end\":57510,\"start\":57499},{\"end\":57517,\"start\":57510},{\"end\":57526,\"start\":57517},{\"end\":57819,\"start\":57810},{\"end\":57825,\"start\":57819},{\"end\":57832,\"start\":57825},{\"end\":58177,\"start\":58171},{\"end\":58183,\"start\":58177},{\"end\":58201,\"start\":58183},{\"end\":58212,\"start\":58201},{\"end\":58222,\"start\":58212},{\"end\":58629,\"start\":58622},{\"end\":58638,\"start\":58629},{\"end\":58873,\"start\":58858},{\"end\":58883,\"start\":58873},{\"end\":58894,\"start\":58883},{\"end\":58903,\"start\":58894},{\"end\":58911,\"start\":58903},{\"end\":58920,\"start\":58911},{\"end\":58930,\"start\":58920},{\"end\":58942,\"start\":58930},{\"end\":59315,\"start\":59309},{\"end\":59324,\"start\":59315},{\"end\":59337,\"start\":59324},{\"end\":59347,\"start\":59337},{\"end\":59713,\"start\":59702},{\"end\":59723,\"start\":59713},{\"end\":59917,\"start\":59911},{\"end\":59924,\"start\":59917},{\"end\":59933,\"start\":59924},{\"end\":59940,\"start\":59933},{\"end\":60181,\"start\":60174},{\"end\":60189,\"start\":60181},{\"end\":60487,\"start\":60478},{\"end\":60499,\"start\":60487},{\"end\":60750,\"start\":60744},{\"end\":60756,\"start\":60750},{\"end\":60763,\"start\":60756},{\"end\":60771,\"start\":60763},{\"end\":60779,\"start\":60771},{\"end\":61044,\"start\":61036},{\"end\":61052,\"start\":61044},{\"end\":61061,\"start\":61052},{\"end\":61068,\"start\":61061},{\"end\":61357,\"start\":61351},{\"end\":61367,\"start\":61357},{\"end\":61376,\"start\":61367},{\"end\":61384,\"start\":61376},{\"end\":61393,\"start\":61384},{\"end\":61399,\"start\":61393},{\"end\":61409,\"start\":61399},{\"end\":61713,\"start\":61701},{\"end\":61724,\"start\":61713},{\"end\":61732,\"start\":61724},{\"end\":62177,\"start\":62169},{\"end\":62185,\"start\":62177},{\"end\":62194,\"start\":62185},{\"end\":62201,\"start\":62194},{\"end\":62208,\"start\":62201},{\"end\":62657,\"start\":62651},{\"end\":62664,\"start\":62657},{\"end\":62670,\"start\":62664},{\"end\":62679,\"start\":62670},{\"end\":62688,\"start\":62679},{\"end\":62696,\"start\":62688},{\"end\":63048,\"start\":63037},{\"end\":63336,\"start\":63328},{\"end\":63344,\"start\":63336},{\"end\":63353,\"start\":63344},{\"end\":63362,\"start\":63353},{\"end\":63722,\"start\":63714},{\"end\":63729,\"start\":63722},{\"end\":63740,\"start\":63729},{\"end\":63750,\"start\":63740},{\"end\":63758,\"start\":63750},{\"end\":63766,\"start\":63758}]", "bib_venue": "[{\"end\":51452,\"start\":51414},{\"end\":51688,\"start\":51626},{\"end\":52085,\"start\":52071},{\"end\":52423,\"start\":52382},{\"end\":52710,\"start\":52599},{\"end\":53127,\"start\":53088},{\"end\":53499,\"start\":53452},{\"end\":53935,\"start\":53892},{\"end\":54244,\"start\":54237},{\"end\":54472,\"start\":54456},{\"end\":54804,\"start\":54761},{\"end\":55085,\"start\":54986},{\"end\":55476,\"start\":55462},{\"end\":55889,\"start\":55849},{\"end\":56341,\"start\":56317},{\"end\":56759,\"start\":56745},{\"end\":57157,\"start\":57104},{\"end\":57537,\"start\":57526},{\"end\":57873,\"start\":57832},{\"end\":58252,\"start\":58222},{\"end\":58653,\"start\":58638},{\"end\":59009,\"start\":58958},{\"end\":59414,\"start\":59347},{\"end\":59746,\"start\":59723},{\"end\":59909,\"start\":59886},{\"end\":60172,\"start\":60074},{\"end\":60476,\"start\":60381},{\"end\":60742,\"start\":60694},{\"end\":61073,\"start\":61068},{\"end\":61423,\"start\":61409},{\"end\":61699,\"start\":61660},{\"end\":61902,\"start\":61840},{\"end\":62279,\"start\":62208},{\"end\":62763,\"start\":62696},{\"end\":63101,\"start\":63048},{\"end\":63376,\"start\":63362},{\"end\":63780,\"start\":63766}]"}}}, "year": 2023, "month": 12, "day": 17}