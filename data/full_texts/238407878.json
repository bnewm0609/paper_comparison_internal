{"id": 238407878, "updated": "2023-10-05 22:07:43.486", "metadata": {"title": "MovingFashion: a Benchmark for the Video-to-Shop Challenge", "authors": "[{\"first\":\"Marco\",\"last\":\"Godi\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Joppi\",\"middle\":[]},{\"first\":\"Geri\",\"last\":\"Skenderi\",\"middle\":[]},{\"first\":\"Marco\",\"last\":\"Cristani\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 10, "day": 6}, "abstract": "Retrieving clothes which are worn in social media videos (Instagram, TikTok) is the latest frontier of e-fashion, referred to as\"video-to-shop\"in the computer vision literature. In this paper we present MovingFashion, the first publicly available dataset to cope with this challenge. MovingFashion is composed of 14855 social videos, each one of them associated to e-commerce\"shop\"images where the corresponding clothing items are clearly portrayed. In addition, we present a network for retrieving the shop images in this scenario, dubbed SEAM Match-RCNN. The model is trained by image-to-video domain adaptation, allowing to use video sequences where only their association with a shop image is given, eliminating the need of millions of annotated bounding boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted sum of few frames (10) of a social video is enough to individuate the correct product within the first 5 retrieved items in a 14K+ shop element gallery with an accuracy of 80%. This provides the best performance on MovingFashion, comparing exhaustively against the related state-of-the-art approaches and alternative baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2110.02627", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/GodiJSC22", "doi": "10.1109/wacv51458.2022.00059"}}, "content": {"source": {"pdf_hash": "01135e9285447269df56ab07135a1cae1476ce76", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2110.02627v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ea1df8d6e0d11a13e1b76b7e902d5eafdeb4c21f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/01135e9285447269df56ab07135a1cae1476ce76.txt", "contents": "\nMovingFashion: a Benchmark for the Video-to-Shop Challenge\n\n\nMarco Godi marco.godi@univr.it \nDepartment of Computer Science\nUniversity of Verona\n2 Humatics SrlVeronaItaly\n\nChristian Joppi christian.joppi@univr.it \nDepartment of Computer Science\nUniversity of Verona\n2 Humatics SrlVeronaItaly\n\nGeri Skenderi geri.skenderi@univr.it \nDepartment of Computer Science\nUniversity of Verona\n2 Humatics SrlVeronaItaly\n\nMarco Cristani marco.cristani@univr@humatics.it \nDepartment of Computer Science\nUniversity of Verona\n2 Humatics SrlVeronaItaly\n\nMovingFashion: a Benchmark for the Video-to-Shop Challenge\n\nRetrieving clothes that are worn in social media videos (Instagram, TikTok) is the latest frontier of e-fashion, referred to as \"video-to-shop\" in the computer vision literature. In this paper, we present MovingFashion, the first publicly available dataset to cope with this challenge. Mov-ingFashion is composed of 14855 social videos, each one of them associated with e-commerce \"shop\" images where the corresponding clothing items are clearly portrayed. In addition, we present a novel baseline for this scenario, dubbed SEAM Match-RCNN. The model is trained by image-tovideo domain adaptation, allowing the use of video sequences where only their association with a shop image is given, eliminating the need for millions of annotated bounding boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted sum of few frames (10) of a social video is enough to individuate the correct product within the first 5 retrieved items in a 14K+ shop element gallery with an accuracy of 80%. This provides the best performance on MovingFashion, comparing exhaustively against the related state-of-the-art approaches and alternative baselines 1 .\n\nIntroduction\n\nOne of the most recent challenges in e-fashion is the so-called video-to-shop [1,15], whose aim is to match a social video (Instagram, TikTok) containing one or more given clothing item(s), against an image gallery, potentially an e-commerce database (Fig. 2a,b). Individuating the outfit of a celebrity or social influencer can turn videos into priceless commercials, in a market where over a billion * indicates equal contribution 1 The code for SEAM Match-RCNN and the MovingFashion dataset are available here: https://github.com/HumaticsLAB/ SEAM-Match-RCNN hours of video are uploaded and viewed on a daily basis [2].Video-to-shop derives from the street-to-shop problem, where the probe data is a single image [6]. On one hand, video-to-shop allows an increase of the available information by adding additional frames as probes. On the other hand, as shown in Fig. 2b, this information could be noisy due to challenging illumination, drastic zooming, human poses, missing data and multiple people (dis)appearing in the video. Another issue is that a video-to-shop system needs training data with millions of bounding box annotations, linking each box with a shop item [1,15].\n\nOur first contribution is MovingFashion, the very first publicly available video-to-shop dataset, composed by \u223c15K different video sequences, each one related with at least one shop image. The videos of MovingFashion are obtained from the fashion e-shop Net-A-Porter (10132 videos) and the social media platforms Instagram and TikTok (4723 videos), and contain hundreds of frames per shop item, partitioned into a Regular and Hard setup.\n\nOur second contribution is the SElf-Attention Multiframe (SEAM) Match-RCNN, a video-to-shop baseline which individuates products and extracts features in a \"street\" video sequence by adopting a feature collection and aggregation mechanism, and then matching the products over a \"shop\" image gallery. SEAM Match-RCNN extends the popular Match-RCNN [4], state-of-the-art in the street-to-shop challenge, by applying image-to-video domain adaptation with the use of a novel Multi-frame Matching Head.\n\nTechnically, a pretraining on the image domain of the Match-RCNN enables it to provide initial pseudo-labels for a video sequence, individuating bounding boxes matching a particular product. The training on the target domain exploits our Multi-frame Matching Head, that aggregates features by means of a non-local block [12] between different frames, which in turn applies a temporal self-attention mechanism [3] and a scoring function. In this way an aggre- gation based on the attention score is used to create a single descriptor for a clothing item. In practice, SEAM Match-RCNN allows to train on video data where only the pairs <\"street\" video,\"shop\" image> are available, without annotated ground-truth bounding boxes. This policy permits to alleviate an intense annotation effort, which in the case of MovingFashion would have required drawing \u223c18M bounding boxes. In the experiments, SEAM Match-RCNN gives the best performances on MovingFashion, against multiple baselines and state-of-the-art techniques. Actually, few frames (10) of a social video are enough to individuate the correct product within the first 5 retrieved items with an accuracy of 80%, making SEAM Match-RCNN a proof of concept for a potential product in e-fashion. Finally, SEAM Match-RCNN gives explainable results: thanks to self-attention visualization, we understand that the initial quarter of a social video carries the most information for guessing the correct product.\n\n\nRelated Work\n\nVideo-to-Shop. The first street-to-shop approaches employed single street images [4,6,9]; \"street\" video queries followed afterwards, paving the way for video-to-shop methods [1,15]. AsymNet [1] aggregates frames by exploiting temporal continuity; it combines an LSTM and a binary tree, with each component requiring a separate training procedure. On the contrary, our SEAM Match-RCNN uses self-attention to learn a descriptor from a bunch of heterogeneous images, where temporal continuity is not required. DPRNet [15] manages the video-to-shop problem by treating it as street-to-shop, with a network that detects and tracks garments in the video, selecting automatically the frame with the highest quality (in terms of occlusions, blurring etc). That detection is finally fed into an imageto-image retrieval module. SEAM Match-RCNN does not perform this kind of tracking, which could be prohibitive on social videos that have strong heterogeneous variations on few frames.\n\nVideo-to-shop approaches shares similarities with video person Re-ID [8], where the goal is to match a video snippet of a person's silhouette against a gallery of image identities taken from a different camera. State-of-the-art approaches are VKD [10], NVAN [8] and MGH [14]. VKD proposes to learn using diverse views of the same target with a teacherstudent framework, where the teacher educates a student who observes fewer views. NVAN is based on a non-local block self-attention module, embedded into the backbone  CNN at multiple feature levels to incorporate both spatial and temporal characteristics of the pedestrian videos into the representation. Multi-Granular Hypergraph (MGH) is a novel graph-based framework which uses graph networks to cope with this problem. Video-to-shop datasets. Unfortunately, no video-toshop datasets are publicly available. The above quoted [1] and [15] use proprietary datasets, which have been not made open to the scientific community. We compare these datasets and their reported characteristics with our Moving-Fashion dataset in Table 1. It is visible that the datasets from AsymNet and DPRNet have a moderate number of sequences (526 and 818, respectively), while MovingFashion contains almost thirty times that amount (15K). In order to create more query data, DPRNet and AsymNet sample multiple sequences from the videos (generating 26K and 5K sub-trajectories, respectively). AsymNet contains 39K exact street-shop pairs and 85K diverse shop items, so one may infer shop distractors are present (shop items not present in the street set) but no details are provided on this. DPRNet has 21K Shop items, with no mentions about the exact pairs. MovingFashion has a single item associated with a unique shop image for each video, for a total of 15K unique (video) street-shop pairs. The DeepFashion2 dataset (DF2) [4] presents a particular scenario: DF2 is made for the street-to-shop challenge, but some shop items are related to more than one street image (coming from different sources), creating 11K pairings. This provides us with another experimental setting.\n\n\nMovingFashion\n\nMovingFashion has 5.854M annotated frames, organized into 15045 video-shop matching pairs, i.e., each video is associated with a distinct shop image. In particular, there are 14.8K unique videos, among which some se-\n#1 r a l u g e R n o i h s a F g n i v o M #1 #28 #47 #63 #103 #132 Shop Shop d r a H n o i h s a F g n i v o M #75 #194 #247 #1 #127 #190\na.\n\nb. Figure 2. MovingFashion dataset samples. The top row contains a \"Regular\" sequence, the bottom row a \"Hard\" sequence.\n\nquences (190 videos) have more than one associated shop item (e.g., a t-shirt and trousers). The length of the videos is detailed in Fig. 1b, while the frame rate amounts to about 30FPS. Shop items are divided in classes, following the DeepFashion2 [4] taxonomy. The list of classes and the number of occurrences for each class in the dataset is reported in Fig. 1a.\n\n\nData sources\n\nMovingFashion is formed by two subsets: Regular and Hard. Regular MovingFashion: Regular MovingFashion consists of 10132 videos downloaded from the e-commerce website Net-A-Porter 2 : in the street video a single person is wearing the shop item in an indoor scenario (which can vary), and the corresponding shop image consists in the shop item captured over a plain background. This is the canonical shop image we have used in our experiments. Additionally, we have collected: a front shop image captured in the same background of the street video and worn by the same model in a frontal pose; a rear view image and a detail of the fabric. These last three were not used in the experiments. An example of Regular MovingFashion is showed in Fig. 2a (more in the supplementary material). Hard MovingFashion: Hard MovingFashion consists of 4723 videos from the social platforms Instagram and Tik-Tok. In this case, shop images have been obtained either by downloading images associated to the video as multiple images of the Instagram post or as part of the video itself (the spatial layout of some raw videos was organized in two halves, one being a still picture of the \"shop\" item, the other with the \"street\" video). Hard MovingFashion represents the hardest challenge, since all of the critical conditions listed in the introduction are present here, as also visible in Fig. 2b. \n\n\nData Collection and cleaning\n\nAll of the videos in Net-A-Porter have been designed to promote a clothing item, which made the data collec-tion process simpler. Cleaning was necessary only to remove classes not compliant with the taxonomy of Deep-Fashion2 [4]. In contrast, Instagram and TikTok videos required a lot of work, starting with the search for the street videos and their shop counterparts using the available API, up to the careful scraping of hashtags and profiles. Other minor but time-consuming issues (fully/partially duplicate videos, wrongly associated shop items) are discussed in the supplementary material.\n\nAfter collection and cleaning, we split the data into a training and a testing partition, taking care of applying the same split for each single class. We perform a 90/10 train/test split. Bounding boxes are extracted using a clothing detector. We then utilize the training data to train our SEAM Match-RCNN following the unsupervised procedure shown in Sec. 4.4. Since video sequences contain more than one item, to evaluate SEAM Match-RCNN and all the comparative approaches we create a tracklet containing the correct item for each street video sequence. That is done by selecting the tracklet that matches most with the shop item, following the training tracking procedure detailed in Sec. 4.4. A tracklet is a set of consecutive detections which refer to the same object. We manually check each one of these to ensure that at least 50% of the detections in the tracklet actually contain the shop item. For the detections which are too noisy (i.e. they do not focus on any precise clothing item), we dropped the entire sequence, in order to speed up the collection procedure. Fortunately, this happened on a minority of sequences (\u223c150 videos), indicating a general success of the tracking procedure. The remaining tracklets are kept as noisy annotations in JSON format. All of the comparative approaches shown in Sec. 5 use these ground truth tracklets for training and testing.\n\n\nSEAM Match-RCNN\n\nSEAM Match-RCNN takes as input a sequence of street images i 1 ...i N , and compares it with the gallery of K shop images providing a list of matching scores as output. Once the model has learned, the retrieval happens by means of three procedures: 1) Tracklet creation; 2) Feature aggregation; 3) Video-to-shop matching. Going through these steps will allow us to present the structure of the network, detailed in Fig. 3.\n\n\nTracklet creation\n\nOn the input video sequence we need to locate a set of consecutive detections which refer to the same object, dubbed here tracklet. Since multiple objects might be on the video, multiple tracklets are expected. The module that deals with this is the Match-RCNN, which is composed of three functions:\n\u121a 1x256 1x256 N LB NLB g 1 2 1x1^2 FC \u121a 1 \u121a 2 \u121a \u121a 1x256\nMulti-frame Matching Head tures c i,t,k with i indicating the i-th tracklet, t indicating the frame, k the k-th detection in that frame;\n2. A 256-d feature extractor f i,t,k = f (c i,t,k ) \u2208 R 256\nwhich performs embedding of the convolutional features;\n3. A matching score function m(f i,t,k , f i,t \u2032 ,k \u2032 ) \u2208 [0, 1],\ncomparing different embeddings.\n\nf and m together form the Single-frame Matching Head.\n\nThe tracklet extraction procedure is performed in an iterative fashion, following a two-step process:\n\n1. Determining the pivot bounding box: This represents the most confident detection f i,t best ,k best in the sequence and acts as the central reference based on which the tracklet will be built. 2. Performing propagation based on the pivot: By comparing the embedding of the pivot f i,t best ,k best with all of the detections in every frame, the tracklet i can be built. In particular, a detection joins the tracklet if its matching score (matching function m of the Single-Frame Matching Head) is above a certain threshold, to avoid considering frames where the item is not visible.\n\nOnce the tracklet i is built, its detections are removed, and another tracklet focusing on a different item can be built.\n\n\nFeature aggregation\n\nThe next step is aggregating the information of a tracklet and condensing it into a single multi-frame descriptor. The module that deals with the feature aggregation procedure is the Multi-frame Matching Head and it is composed of the following functions and modules:\n1. A 256-d feature extractorf i,t =f (c i,t ) \u2208 R 256 oper-\nating on the bounding box at time t of the tracklet i, i.e., c i,t .\n\n\n2.\n\nA non-local block [12] module which applies selfattention, enriching {f i,t } t with information coming from all the other bounding boxes related to the object tracklet i. 3. An attention module g : R N \u00d7256 \u2192 R N that for each detection in a tracklet computes an importance score w t such that t w t = 1.\n\n\nAn aggregation module, which fuses\n{f i,t } t into a joint descriptorf i by a weighted average over the at- tention scores {w t }: h(x) = g(N LB(x)) \u00b7 x, x \u2208 R N \u00d7256 . 5. A matching score functionm(f j ,f i ) \u2208 [0, 1], which\ncompares the aggregated descriptor for item k and street sequence i (h({f i,t } t ) asf i ) with the the shop descriptor of image j.\n\nThe aggregation procedure starts with the feature extractorf , which creates the initial descriptors for each box in a sequence. Then, self-attention is computed by the non-local block module and afterwards the attention module calculates the attention weights for each descriptor. The aggregation module puts all of the above together, producing the single multi-frame descriptorf i . Note that we discard temporal continuity by design. Social network videos usually have dramatic zooms, very fast pose dynamics and occlusions; moreover, elaborated videos may have shot changes which can fragment temporal continuity.\n\n\nVideo-to-shop matching\n\nFollowing the feature aggregation procedure described above, we obtain a single multi-frame descriptorf i of the street tracklet i. In this final procedure, the matching score functionm of the Multi-frame Matching Head is used to match the aggregated multi-frame description with the single shop descriptor of image j,f j (which can be considered as a tracklet composed by a single frame), under the assumption that a single item is portrayed in the shop image.\n\nWe use the matching functionm on all the images in the shop gallery, producing in this way a list of matching scores between the street tracklet and all the images in the shop gallery, sorted in descending order, creating thus a ranking.\n\n\nModel Training\n\nTo avoid the need of bounding boxes annotations, a time-consuming procedure especially for videos, SEAM Match-RCNN is trained by domain adaptation, through two phases: pretraining on the source image domain and training on the target video domain.\n\nPretraining on Source domain. The Match-RCNN part of SEAM Match-RCNN (detector and Single-frame Matching Head) is pretrained on an image street-to-shop dataset (e.g. DeepFashion2). The purpose of this phase is to train a model that is able to estimate bounding boxes and matching scores in the target domain (even with noisy predictions due to the domain gap). Such predictions are used to generate tracklets and pseudo-labels to train the Multi-frame Matching Head.\n\nTraining on Target domain. The training procedure estimates the parameters for the Multi-frame Matching Head of the SEAM Match-RCNN, whose structure is detailed in Sec. 4.2, and fine-tunes the Single-frame Matching Head, while the detector's weights are frozen. The weights of the features extractorf and matching score functionm are initialized copying those of f and m from the pretrained Single-frame Matching Head. Conversely, the attention modules of h are randomly initialized. During training, image and street video sequence pairs (thanks to the Moving-Fashion pairing annotations) are sampled, which are leveraged in the tracking procedure (Sec. 4.1): the pivot selection is done by selecting the detection that matches the shop product the most in the whole video if the matching score inferred from the matching function m of the Single-frame Matching Head is over a certain threshold. The propagation step remains the same as in Sec. 4.1. With this training tracking procedure a tracklet is built such that, with a certain confidence, it contains the correct shop item due to the pivot selection starting from the ground truth shop image. This is considered as a positive match during training (i.e. we set 1 as a pseudo-label for the tracklet). For what concerns the Single-frame Matching Head fine-tuning, each detection that composes the tracklet is considered as positive match as well. The tracklet is then passed as input to the Multi-frame Matching head, which computes a singular multi-frame descriptorf i thanks to the aggregation procedure described in Sec. 4.2. In the end, this singular multi-frame descriptorf i is compared with the corresponding shop descriptorf j (obtained by using the feature extractorf j = f (c j )) utilizing the matching score functionm. This produces a matching score in the range [0, 1].\n\nTraining is done by Stochastic Gradient Descent using cross-entropy loss for the classification of street videos and shop images as positive/negative matches. Positive pairings are built using the aforementioned procedure. All of the other combinations between tracklets and shop image descriptors are considered negative pairings (i.e. pseudo-label of 0) for the Single-frame Matching Head and the Multiframe Matching Head.\n\n\nExperiments\n\nFor the retrieval performance evaluation, we follow the testing protocol of DeepFashion2 [4] for evaluating a street image probe against a shop image gallery, with some modifications in order to cope with videos. In DeepFashion2, a street image generates multiple detections: each street detection can generate a proper matching with some shop image, if it overlaps by a threshold with the corresponding ground truth street bounding box and if its item class is correct, otherwise the matching score is 0.\n\nOn MovingFashion, we compute detections on every street image and we build tracklets using the tracking procedure detailed in Sec. 4.1. Then, we compute the average IoU between each street tracklet and the ground truth tracklet. The one with the highest average IoU is chosen and used as a query. In order to guarantee fairness in experiments, all baselines and comparative methods have been pretrained on two different street-to-shop datasets: DeepFashion2 and Exact Street2Shop [6]; the former has 873K probe-gallery pairs, while the latter 39K pairs only. Detailed results are reported for the first case, since performances were higher, while in the second case we show the main retrieval results, where our SEAM Match-RCNN remains the best performing approach.\n\n\nExperiments on MovingFashion\n\nWe compare our technique with three types of approaches:\n\nMulti-frame baselines. They are extensions of singleframe techniques to multi-frame. The Max Confidence [4] keeps the most confident detection in a tracklet and uses it for Single-frame Matching, employing a Match-RCNN. The Max Matching is inspired from [1] and considers the max matching score between the tracklet's street frames and each shop image. These two baselines are actually working with a single image, which is selected by looking at the entire pool of frames in a tracklet. They are also useful to validate the performance boost that comes when using multiple frames instead of single ones.\n\nThe Average Distance is inspired by [1] and consists in averaging single-image matching scores of the tracklet street frames and each shop image. The SEAM Match-RCNN w/o N LB,g is obtained by averaging descriptors (and not matching scores) together by average pooling, removing in practice the NLB self-attention block and the at-\n\n\nMethod MovingFashion\n\nRegular-MovingFashion Hard- MovingFashion  T-1  T-5  T-10  T-20  T-1  T-5  T-10  T-20  T-1  T-5  T-10  T- Table 3. Top-K accuracy on MovingFashion, pretraining on S2S [6] tention scoring function g from the SEAM Match-RCNN (see the scheme in Fig. 3). Finally, SEAM Match-RCNN w/o N LB keeps the attention score, without the self-attention. These last three are proper multi-frame baselines, in the sense that they merge information coming from multiple frames. Video Re-Identification approaches. Video Re-Id approaches share similarities with Video-to-shop, in that they look for the best way to aggregate multi-frame information to match a person in a disjoint multi-camera setting. In practice, we consider each shop clothing item the equivalent of a Person Re-Identification Identity. The main differences between video-to-shop and Person Re-ID are that in our case less information is available in terms of pixels, since face and hair need to be discarded, focusing only on the clothing. Here we consider the SoA approaches of NVAN [8], VKD [10] and MGH [14] 3 .\n\nVideo-to-shop approaches. We consider the Asym-Net [1] approach 4 , and its modifications AsymNet [AVG] and AsymNet[MAX], in which the aggregations are made respectively by the average and the max of the similarity score nodes' outputs instead of using the fusion nodes binary tree. Asymnet exploits temporal continuity, yet it does not reach our results.\n\nWe set the sequence length to T = 10 for both training and testing, picking the frames using the Restricted Random Sampling strategy [7], thus ensuring coverage of the 3 At the moment of writing, the MGH approach is state-of-the-art in the MARS Video Person Re-Identification dataset, followed closely by VKD and NVAN. 4 The code is available at https://github.com/kyusbok/ Video2ShopExactMatching. entire sequence length. To analyze variability in the results, we analyze the testing samples by sub-sampling them into pool of 800, 20 times, averaging the rankings and computing standard deviations. Table 2 reports the results. Three facts become apparent: 1) As expected, single-frame approaches (Max Confidence, Max Matching) are definitely inferior (<15% on average) than multi-frame approaches; 2) The considered re-identification approaches, apart from top-1 scores, are inferior to genuine video-to-shop methods; 3) Our SEAM Match-RCNN surpasses all the competitors, including AsymNet, which gives a better aggregation than the AVG-distance in its [AVG] version and the MAX-distance in its [MAX] version. By looking at the ablative versions of SEAM Match-RCNN, one can note that the self-attention gives the strongest performance boost, followed by the attention layer. Their cooperation, i.e., the complete SEAM Match-RCNN, reaches the highest score.\n\nBy looking at the results within the Regular and Hard MovingFashion partitions, it is quite easy to note the general decrease in performance when it comes to the hard partition. To understand the performance qualitatively, Fig. 6 shows retrieval results from Regular (Fig. 6a) and Hard (Fig. 6b). Actually, even if Regular is apparently harder due to many shop alternatives which differ by fine grained results (see the flared jeans), the dramatic changes of poses and backgrounds of the Hard partition play a stronger role.\n\nFailure cases arise when the original video has discriminant parts of the clothing item covered for most of the sequence, for instance the logo of the light blue sweatshirt (Fig. 4a). In this case, self-attention overlooks such important details. Complex visual patterns like the hard-rock band logo (Fig. 4b), seem to be not well characterized, meaning that the best match is attributed considering the shape of the logo rather than its content (the \"Metallica\" logo has the same shape of the probe logo).\n\nThe results w.r.t the single clothing classes of Moving-Fashion are reported in Table 4, where it is possible to observe our advantage in all but three classes. Interestingly, we found that the simpler the clothing in terms of    texture, the lower the retrieval performance. This is reasonable, since texture adds discriminative details, and this is why classes with simpler texture like vest, sling, shorts and trousers performed worse. We computed textureness by gray-level co-occurrence matrix contrast; quantitatively speaking, textureness and top-1 accuracy in retrieval are found to be correlated (Spearman \u03d5 = 0.72, p \u2212 value \u2264 0.05).\n\nAnother experiment regards the length of the sequences. Fig. 5 reports, with the associated error bars, the performance of SEAM Match-RCNN when increasing the number of frames from 1 to 20. As expected, the curves for both partitions, at both the top-1 and top-20 are increasing, with the \"Hard\" partition showing a plateau after 10 frames, while the \"Regular\" partition seem to benefit systematically. The reason could be that \"Hard\" sequences are dramatically noisy, and adding more frames will augment the clutter we need to deal with, while the \"Regular\" ones benefit because of the fine grained details which characterize the partition. Comparative performances when varying the sequence's length against other approaches are in Tab. 5. Notably, Asymnet [1] does not reach our results even when doubling the number of input frames.\n\n\nExperiments on unrelated sets of images\n\nMovingFashion has street videos which depict clothing items in a variety of scenarios: indoor, outdoor, etc. We are interested in bringing this variety to the extreme, an-    [10] .21(.014) .27(.017) .33(.017) .38(.018) MGH (2020) [14] .  swering the following question: how does SEAM Match-RCNN behave when the street video sequence is formed by a few totally unrelated frames? In order to perform these experiments, we build Multi-DeepFashion2 from DeepFash-ion2 using the pairings between shop images and street sequences composed of multiple corresponding street images. The total pairings amount to 11K, each one composed of an image sequence (6 frames on average) sampled from different sources, along with the corresponding shop image. Results are in Tab. 6. Please note that, in order to be consistent with the 10-frames street sequence length we generate random repetitions for all the approaches given the smaller set of diverse images. The numbers indicate a decrease in general performance (less distinctive frames, more shop items); even in this case, we perform better than AsymNet.\n#1 #2 #3 #4 \u2026 #20 #1 #2 #3 #4 #20 \u2026 a.\nb. Figure 6. Qualitative retrieval results of SEAM Match-RCNN for the MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved with the corresponding rank. The correct matches are with a green border, otherwise red.\n\n\nExperiments on the attention mechanism\n\nThe ablation studies of Table 2 clearly show that the attention layers play a crucial role for the SEAM Match-RCNN performance. Here we explain their role qualitatively and quantitatively. In Fig. 7 we report the attention values obtained after the application of the attention layer g to the output of the self-attention layer N LB of Sec. 4.2, i.e., g(N LB(x)). On row a), one can note that the attention is high when the heart logo is visible (0.31, 0.23 in the first two frames) and it goes down when it vanishes, despite the light blue shirt (last frame) being very similar area-wise. This means that the mechanism considers the heart logo as important for retrieval. On the second row b), the effect of an occlusion in the attention score (last frame). On the third row c), a white top with a logo gives a stable attention score (around 0.28). We manually cover the logo in the third frame, causing a clear decrease in the attention, uniformly increasing the ones highlighting the logo.\n\nFinally, driven by best practices in social video editing [5], which state that a video message has to deliver its main content in the first 6 seconds to trigger the observers' attention, we calculate the attention every 5 percentiles on all the Moving fashion sequences, producing the curves in Fig. 8a) (on the whole MovingFashion dataset) and on the separate partitions Fig. 8b. Surprisingly, the data confirms this rule, showing a clear (Fig. 8a) peak around the first quartile (definitely within 6 seconds), then a decrease and a later increase with a local maximum on the fourth quartile. The same holds for the two separate partitions (Fig. 8b)), with less emphasis on the \"Hard partition\". The reason lies in the nature of the Net-A-Porter videos, which in many cases show the entire clothing item in the beginning of the sequence, with the model that moves subsequently, zooming up to critical detail (the belt for the shorts) towards the end (second peak). On the \"Hard\" partition, the attention for the clothing items is higher in the beginning, since the actors present their outfit and then exhibit their performance (dancing, gymnastics etc.), concluding in both the cases with uninteresting details clothing wise.   14.46s \u00b1 6.8s 0s Figure 8. Mean attention score every 5 percentiles of the video length. For each video we sampled 21 equally spaced frames. On the left we report the average attention (y-axis) and frame-timing information (x-axis labels) for the whole MovingFashion dataset. On the right for the Regular and Hard subsets. We show error bands for the standard deviation.\n\n\nConclusions\n\nOur SEAM Match-RCNN, trained on the new Mov-ingFashion dataset, provides a strong baseline that shows video-to-shop matching can be performed on videos in the wild like TikToks, possibly unveiling fashion trends directly from social platforms and consequently attracting big fashion players.\n\n\nA. MovingFashion Additional Details\n\n\nA.1. Image and video collection\n\nIn this section we give further details on the process of data collection and annotation of Moving Fashion.\n\nRegarding the data collected from the Net-A-Porter website, the data labeling was a long, yet linear process, the only issue being the removal of classes not in the DeepFashion2 taxonomy, in particular shoes (deserving of a specific fashion taxonomy) and jewelry (due to the lack of a shared and widely accepted aesthetical taxonomy). For the remaining classes, the association to the specific DF2 taxonomy was direct.\n\nPlenty more work was required for the data downloaded from Instagram. In order to to download the data, the Instaloader 5 tool was employed. We manually selected a list of hashtags and profiles with a lot of content, i.e. a lot of videos paired with fashion products for sale. Through the use of the tool, we downloaded posts containing videos only based on the previously mentioned hashtags and profiles. The layout of these videos was standard for the vast majority of them: the frame was divided vertically in two parts, one with just a still picture of the shop product and one with the video itself.\n\nWe manually annotated these videos by following these steps:\n\n\u2022 We checked that the product actually appears in the video, since in some cases the item never appears or appears very briefly in the frame; sometimes the item is in a different color than the one in the shop image.\n\n\u2022 We drew a bounding box around the area of the shop item(s), taking care to include as few other items as possible.\n\n\u2022 We drew another bounding box around the area of the video.\n\nUsing these annotations we crop the street videos and shop images. This results in pairings, where in some cases we have more than one shop item associated with a street video. Next, we dealt with duplicates of shop products. In some cases the same product is showcased in multiple videos by different users, but fortunately, the shop image used in such videos is the same. We leveraged this fact to perform a duplicate search for all the shop images. Products that were found to be duplicates were merged, creating pairings where for one shop product multiple videos are associated. To perform this search, for each product we searched for duplicates using a pre-existing tool 6 that employs Perceptual Hash. However we found out that in order to have a very high recall, this process also includes a lot of false positives. To perform a more thorough search, we tried an Image Registration technique using the RANSAC algorithm between each shop image and the duplicate candidates found using the tool. We tried to estimate a Similarity Transform, to account for translations and scaling (as is the case for these images). We then put a threshold on average pixel difference to separate between duplicates and non duplicates. Since no Python libraries that implement RANSAC are available, it was performed using a custom script.\n\nTo make sure that MovingFashion respects the privacy of social media users, we have rendered any face in the videos blurred using a publicly available, face blurring tool 7 .\n\n\nA.2. Tracklet generation\n\nAs described in the paper, for all data, noisy tracklet annotations are available. In order to create them:\n\n\u2022 Our SEAM Match-RCNN is trained on the data using only video-image pairing annotations. This results in a model where the Single-frame Matching Head can be effectively used for precisely tracking each item.\n\n\u2022 We use the trained model to build a set of tracklets for each video.\n\n\u2022 We manually go over each video and select the tracklets that contain the paired shop item, merging them if they are disjointed (this happens when an item is occluded completely or disappears from the frame and two separate tracklets are built).\n\nThe resulting tracklets are then saved. While for our approach, no tracklet annotations are used during training, they are used for all the comparative approaches. They are considered as equivalent to ours (the detector and the tracker are the same). It can be argued that they are actually better than ours as they are produced after the last epoch of training, while for our approach we start with a tracker that has not been trained yet. For the Person Re-ID approaches, the annotations are used to crop out part of the image according to the extracted bounding box. For detection based approaches, the bounding boxes are used as ground truth bounding boxes. The testing tracklets are used by all approaches for evaluation. During the SEAM Match-RCNN evaluation, they are used to select the tracklet among the ones produced automatically by the tracking procedure.\n\n\nB. SEAM Match-RCNN computational complexity\n\nIn this Section we discuss the computational complexity of our proposed SEAM Match-RCNN. In particular we  \n\n\nB.1. Single-frame Matching Head\n\nLet T F be the time taken for computing features by using the f function and T M the time taken for computing matching between two feature vectors using m.\n\nGiven a street image and a shop image, the cost of computing a matching between them, assuming that the detection from the street image has already been chosen in some way (for example by comparing it with a ground truth bounding box) is 2\u00d7T F +T M (features computed for both street and shop are compared).\n\n\nB.2. Multi-frame Matching Head\n\nWhen extending to Multi-frame matching, the cost of tracking has to be taken into consideration. Obviously the time taken for feature computation increases linearly with the number of frames sampled from the video.\n\nAsf andm are structured in the same way as f and m, we can assert that T F and T M also apply to them. Given a street video sequence from which we sample T frames, the  cost of building all the possible tracklets (using the tracking procedure, Sec 4-1 of the main paper) is related to the number of detections in each frame K (to simplify notation we assume that there are exactly K detections in each frame). First of all, Single-frame Matching Head features are computed, the time cost is T F \u00d7 K \u00d7 T . As a reminder, the tracking procedure consists of iteratively repeating the choice of pivot and propagation. The choice of the pivot is performed by choosing the most confident detection, so its cost is negligible as it is already included in the detection. The propagation consists of doing comparisons between the pivot features and all of the detection features in a frame. For a Single-frame the time necessary for the propagation step is T M \u00d7 K (a matching for each detection). This procedure is repeated for all frames resulting in T M \u00d7 K \u00d7 T . This results in a single tracklet, that is excluded from the set of detections for the following iterations. As the iteration is repeated until there are no more detections, we can assume that repeating the propagation K times results in a final cost of T M \u00d7 K 2 \u00d7 T . For the whole tracking procedure, the total time is\n(T F \u00d7 K \u00d7 T ) + (T M \u00d7 K 2 \u00d7 T ).\nAfter tracklets are built, we can assume that the correct tracklet is chosen, for example by using the Intersection over Union with the ground truth tracklets (analogous to selecting the correct bounding box in the Match-RCNN). Given a sequence of detection of length T (length of the video sequence), the cost of computing Multi-frame Matching features is again T F \u00d7 T . Then self-attetion with the Non-Local Block is performed, resulting in a time cost of T 2 \u00d7 T SA (T SA is the cost of computing self-attention between a pair of element in the sequence, usually a simple operation like a dot product). The attention score is then computed for each frame, with a cost of T \u00d7 T A (T A is the cost of computing the attention score, in our case a simple linear layer). Finally a weighted average pooling is performed and matching is computed between the aggregated descriptor and the shop feature vector (T F + T M ). The final cost for aggregation is (T F \u00d7 T ) + (T 2 \u00d7 T SA) + (T \u00d7 T A) + (T F + T M ).\n\n\nB.3. Discussion\n\nIt is expected that the extension from Single-frame to Multi-frame will come with an increased cost, in relation to the number of frames. The tracking procedure is a necessary step for any possible Multi-frame approach, as detections from each frame need to be grouped in some way. The matching component increases quadratically with the maximum number of detections in each frame and linearly with the number of frames sampled from the sequence.\n\nThe aggregation has a term that increases quadratically with the number of frames. For both of these, we have to take into consideration that we usually work with 10 samples and there are rarely many different people and clothing items in a video, so even with a quadratic complexity, the total effective time is relatively small. In our experiments, we never go over 2 seconds for the whole procedure, with the majority of the videos taking about 1 second to process.\n\n\nC. Additional experiments\n\nIn Table 7, we show the results of Single-frame baselines built on top of the Match-RCNN (the main building block of our SEAM Match-RCNN). In particular, SFM-1qrt uses the frame at the first quartile of all the available frames of that sequence, SFM-median uses the median frame and so on. SFM stands for Single-frame match and is a short term for Match-RCNN.\n\nThe correspondent baselines are shown, adopting the Deep Kronecker-Product Matching (KPM) [11] and the Easy Positive Triplet Mining approach (EPHN) [13]. The rationale of this choice was to focus on Single-frame Re-Identification approaches and compare them to the Match-RCNN. This was done to enlarge the spectrum of possible comparative approaches, which have open-source code available. The idea of considering Re-ID approaches against street-to-shop techniques was also presented in the DPRNet paper [15].\n\nThe inferiority of these baselines with respect of the Multi-frame of Table 2 in the main paper, and in particular with SEAM Match-RCNN, is evident and fully understandable.\n\nNotably, in almost all of the MovingFashion partitions (apart the regular one with EPHN), the \u00b7-1qrt baseline gives the higher results, which seems to be in accord with the best practices in social media video editing, that is, that videos have to deliver their main message within approximately 6 seconds [5].\n\nAs additional Multi-frame approaches, Table 8 shows Max Confidence, Max Matching and Average Matching scores when considering the KPM [11] and the EPHN [13] as Single-frame method ingredients, in the same way that Match-RCNN was used to calculate Max Confidence, Max Matching and Average Matching from Table 2 of the main paper.\n\nEven in this case, SEAM Match-RCNN gives the best performance, showing an overall superiority of Match-RCNN as a Single-frame tool to aggregate visual clothing information.\n\nThe same applies when it comes to MultiDeepFashion2 where we investigate only Multi-frame policies (Max Confidence, Matching, Avg Matching and Descriptor), since Single-frame policies do not have much sense, as the Single-frames are not part of a single sequence. Even in this case, SEAM Match-RCNN is the best alternative ( Table 9).\n\nAs additional qualitative results, on Fig. 9 results of SEAM Match-RCNN for the Hard-MovingFashion dataset are shown. Two types of considerations can be drawn: the first one is the variability of the videos, which here can be appreciated with more examples. Second, the retrieval results on the right display that SEAM Match-RCNN is capable of finding similar images, among a shop gallery that in some cases contains highly similar items (see for example the light gray trousers).\n\nOn Fig. 10 results of SEAM Match-RCNN for the Regular-MovingFashion dataset are shown. Here, on street frames which exhibit more regularities, the shop items are vice versa more insidious than the TikTok ones, since they exhibit a lower variability, see for example the black female dresses of row 6. The same rationale holds for the white shirts and the black paints.\n\nFinally, on Fig. 11 retrieval results on MultiDeepFash-ion2 are shown. Looking at the retrieval results, one can notice that shop items are way less regular/neutral than the ones on the MovingFashion (which anyway represent a more genuine excerpt of an e-commerce website): at the same time, street frames are often zoomed captures of the object of interest, in general offering a retrieval challenge different than the one on MovingFashion. The strong results obtained by SEAM Match-RCNN prove its versatility in working on a broader set of scenarios.\n\n\nD. Future perspectives\n\nWith SEAM Match-RCNN we showed how the contribution of multiple frames can boost the retrieval accuracy by 33% on MultiDeepFashion2 w.r.t Single-frame approaches and by 69% on the MovingFashion dataset. We also obtained new, state-of-the-art results on all of the benchmarks. Still, much progress has to be made in order to present a new product to the market: looking at the results, the probability of finding the correct shop match within the top 20 ranked shop images is 87% on TikTok/Instagram videos. In order to connect all the dots available within the data, one has to exploit all of the details of the clothing items shown in some of the frames, something which we are currently not able to perform (in fact, we are discarding them with low attention), because they cannot be mapped to the general layout of the clothing item. Therefore, we should probably consider 3D atlases and have a common reference there.\n\nThis setup can be attractive for many scenarios, for example: 1) a casual user can match a video snippet of a nice outfit he/she has captured with a gallery of products (e.g. Zalando, Amazon, etc.); 2) a fast fashion company can measure the similarity of clothing items contained in a viral video, or fashion show, with the items of its catalogue, deciding which item to promote the most; 3) Youtube videos can be automatically processed by video sharing platforms to build valuable statistics of popular outfits and discover emerging trends.   \n\nFigure 1 .\n1MovingFashion statistics; a) Cardinality of each clothing item class; b) Histogram of the number of frames for the street sequences.\n\nFigure 3 .\n3Architecture of our SEAM Match-RCNN system. Images are first processed by the Match-RCNN to extract bounding boxes and convolutional features. After tracking a clothing item across frames, its features are further processed by the Multi-frame Matching Head producing a final matching score between the street video sequence and the shop image.\n\nFigure 4 .\n4Failure cases results of SEAM Match-RCNN for the MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved with the corresponding rank. The correct matches are with a green border.\n\nFigure 5 .\n5Plot of the SEAM Match-RCNN retrieval accuracy (yaxis) using different numbers of frames (x-axis) for aggregation. Error bars represent standard deviation of the accuracy.\n\nFigure 7 .\n7Qualitative observations on the attention behaviour. On the left, for each video sequence we show the detection bounding boxes and the computed attention score. On the right the paired shop item.\n\nTable 7 .\n7SEAM Match-RCNN retrieval results on MovingFashion compared with Single-frame approaches. Note: T-K means Top-\n\nFigure 9 .\n9Qualitative retrieval results of SEAM Match-RCNN for the Hard-MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved starting from the closest match (left). The correct matches are represented with a green border.\n\nFigure 10 .\n10Qualitative retrieval results of SEAM Match-RCNN for the Regular-MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved starting from the closest match (left). The correct matches are represented with a green border.\n\nFigure 11 .\n11Qualitative retrieval results of SEAM Match-RCNN for the MultiDeepFashion2 dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved starting from the closest match (left). The correct matches are represented with a green border.\n\n\nTable 1. Comparison of Video-2-shop datasets. n.a. stands for not available.Dataset \n\n#Videos \n#Traject. \n#FramesXVideo [Avg.] \n#Shops \n[W, H] \n#Pairs Wild \nOcclusion Crowd Available \nAsymNet [1] \n526 \n26k \nn.a. \n85k \nn.a. \n39k \nn.a. \nn.a. \nn.a. \n\u2717 \nDPRNet [15] \n818 \n5k \nn.a. \n21k \nn.a. \nn.a. \nn.a. \nn.a. \nn.a. \n\u2717 \n\nMovingFashion \n15k \n15k \n390 \n14k \n[631\u00b1 12 , 770\u00b1 21] \n15k \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\n\n\nTable 4 .\n4Top-1 retrieval accuracy (and standard deviation) on MovingFashion for the 14 different item classes.\n\nTable 5 .\n5Top-1 accuracy on MovingFashion, with different number of frames.Method \nT-1 \nT-5 \nT-10 \nT-20 \n\nMax-Confidence \n.19(.014) .44(.020) .54(.020) .66(.017) \nMax Matching [1] \n.14(.015) .45(.020) .61(.019) .75(.017) \n\nNVAN (2019) [8] \n.22(.019) .37(.019) .43(.018) .49(.019) \nVKD (2020) \n\nTable 6 .\n6Video-to-Shop retrieval results on MultiDeepFashion2. Note: T-K means Top-K Accuracy.\n\nTable 8 .\n8SEAM Match-RCNN retrieval results on MovingFashion compared with Multi-frame approaches. Note: T-K means Top-K \nAccuracy. \n\nfocus on the difference between the Single-frame Matching \nHead and the Multi-frame Matching Head. \n\n\n\nTable 9 .\n9SEAM Match-RCNN retrieval results on MultiDeep-Fashion2 compared with Multi-frame approaches. Note: T-K means Top-K Accuracy.\nhttps://www.net-a-porter.com\nhttps://instaloader.github.io/ 6 https://github.com/umbertogriffo/ fast-near-duplicate-image-search\nhttps://github.com/ORB-HD/deface\n\nVideo2shop: Exact matching clothes in videos to online shopping images. Zhi-Qi Cheng, Xiao Wu, Yang Liu, Xian-Sheng Hua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZhi-Qi Cheng, Xiao Wu, Yang Liu, and Xian-Sheng Hua. Video2shop: Exact matching clothes in videos to online shopping images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4048- 4056, 2017.\n\nThe youtube marketing communication effect on cognitive, affective and behavioural attitudes among generation z consumers. Rodney Duffett, Sustainability. 12125075Rodney Duffett. The youtube marketing communication ef- fect on cognitive, affective and behavioural attitudes among generation z consumers. Sustainability, 12(12):5075, 2020.\n\nRevisiting temporal modeling for video-based person reid. Jiyang Gao, Ram Nevatia, arXiv:1805.02104arXiv preprintJiyang Gao and Ram Nevatia. Revisiting temporal modeling for video-based person reid. arXiv preprint arXiv:1805.02104, 2018.\n\nDeepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images. Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, Ping Luo, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. Deepfashion2: A versatile benchmark for de- tection, pose estimation, segmentation and re-identification of clothing images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5337- 5345, 2019.\n\nFacebook video ads: Best practices for. Maxwell Golling, Maxwell Golling. Facebook video ads: Best practices for 2019, 2018.\n\nWhere to buy it: Matching street clothing photos in online shops. Xufeng Hadi Kiapour, Svetlana Han, Lazebnik, C Alexander, Tamara L Berg, Berg, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionM Hadi Kiapour, Xufeng Han, Svetlana Lazebnik, Alexan- der C Berg, and Tamara L Berg. Where to buy it: Match- ing street clothing photos in online shops. In Proceedings of the IEEE international conference on computer vision, pages 3343-3351, 2015.\n\nDiversity regularized spatiotemporal attention for videobased person re-identification. Shuang Li, Slawomir Bak, Peter Carr, Xiaogang Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized spatiotemporal attention for video- based person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 369-378, 2018.\n\nSpatially and temporally efficient non-local attention network for video-based person re-identification. Chih-Ting Liu, Chih-Wei Wu, Yu-Chiang Frank Wang, Shao-Yi Chien, British Machine Vision Conference. Chih-Ting Liu, Chih-Wei Wu, Yu-Chiang Frank Wang, and Shao-Yi Chien. Spatially and temporally efficient non-local attention network for video-based person re-identification. In British Machine Vision Conference, 2019.\n\nDeepfashion: Powering robust clothes recognition and retrieval with rich annotations. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZiwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 1096-1104, 2016.\n\nRobust re-identification by multiple views knowledge distillation. Angelo Porrello, Luca Bergamini, Simone Calderara, The European Conference on Computer Vision (ECCV). 2020Angelo Porrello, Luca Bergamini, and Simone Calderara. Robust re-identification by multiple views knowledge dis- tillation. In The European Conference on Computer Vision (ECCV), 2020.\n\nPerson re-identification with deep kronecker-product matching and group-shuffling random walk. Yantao Shen, Tong Xiao, Shuai Yi, Dapeng Chen, Xiaogang Wang, Hongsheng Li, IEEE transactions on pattern analysis and machine intelligence. Yantao Shen, Tong Xiao, Shuai Yi, Dapeng Chen, Xiao- gang Wang, and Hongsheng Li. Person re-identification with deep kronecker-product matching and group-shuffling ran- dom walk. IEEE transactions on pattern analysis and ma- chine intelligence, 2019.\n\nNon-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 7794-7803, 2018.\n\nImproved embeddings with easy positive triplet mining. Hong Xuan, Abby Stylianou, Robert Pless, The IEEE Winter Conference on Applications of Computer Vision. Hong Xuan, Abby Stylianou, and Robert Pless. Improved embeddings with easy positive triplet mining. In The IEEE Winter Conference on Applications of Computer Vision, pages 2474-2482, 2020.\n\nLearning multi-granular hypergraphs for video-based person re-identification. Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, Ling Shao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning multi-granular hypergraphs for video-based person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nDress like an internet celebrity: Fashion retrieval in videos. Hongrui Zhao, Jin Yu, Yanan Li, Donghui Wang, Jie Liu, Hongxia Yang, Fei Wu, proceedings of the International Joint Conferences on Artificial Intelligence. the International Joint Conferences on Artificial Intelligence07Hongrui Zhao, Jin Yu, Yanan Li, Donghui Wang, Jie Liu, Hongxia Yang, and Fei Wu. Dress like an internet celebrity: Fashion retrieval in videos. In proceedings of the Inter- national Joint Conferences on Artificial Intelligence, pages 1054-1060, 07 2020.\n", "annotations": {"author": "[{\"end\":172,\"start\":62},{\"end\":293,\"start\":173},{\"end\":410,\"start\":294},{\"end\":538,\"start\":411}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":68},{\"end\":188,\"start\":183},{\"end\":307,\"start\":299},{\"end\":425,\"start\":417}]", "author_first_name": "[{\"end\":67,\"start\":62},{\"end\":182,\"start\":173},{\"end\":298,\"start\":294},{\"end\":416,\"start\":411}]", "author_affiliation": "[{\"end\":171,\"start\":94},{\"end\":292,\"start\":215},{\"end\":409,\"start\":332},{\"end\":537,\"start\":460}]", "title": "[{\"end\":59,\"start\":1},{\"end\":597,\"start\":539}]", "venue": null, "abstract": "[{\"end\":1754,\"start\":599}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1851,\"start\":1848},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1854,\"start\":1851},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2204,\"start\":2203},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2391,\"start\":2388},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2489,\"start\":2486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2947,\"start\":2944},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2950,\"start\":2947},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3742,\"start\":3739},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4215,\"start\":4211},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4303,\"start\":4300},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5448,\"start\":5445},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5450,\"start\":5448},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5452,\"start\":5450},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5542,\"start\":5539},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5545,\"start\":5542},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5558,\"start\":5555},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5883,\"start\":5879},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6413,\"start\":6410},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6592,\"start\":6588},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6602,\"start\":6599},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6615,\"start\":6611},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7224,\"start\":7221},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7233,\"start\":7229},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8203,\"start\":8200},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9203,\"start\":9200},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10976,\"start\":10973},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15216,\"start\":15212},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20315,\"start\":20312},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21213,\"start\":21210},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21693,\"start\":21690},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21843,\"start\":21840},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22231,\"start\":22228},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22717,\"start\":22714},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23587,\"start\":23584},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23597,\"start\":23593},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23610,\"start\":23606},{\"end\":23719,\"start\":23714},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24109,\"start\":24106},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24142,\"start\":24141},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24293,\"start\":24292},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27773,\"start\":27770},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28070,\"start\":28066},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28126,\"start\":28122},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28655,\"start\":28654},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30424,\"start\":30421},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41727,\"start\":41723},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41785,\"start\":41781},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":42141,\"start\":42137},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":42628,\"start\":42625},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":42769,\"start\":42765},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":42787,\"start\":42783}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":46516,\"start\":46371},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46873,\"start\":46517},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47150,\"start\":46874},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47335,\"start\":47151},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47544,\"start\":47336},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47667,\"start\":47545},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47980,\"start\":47668},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48298,\"start\":47981},{\"attributes\":{\"id\":\"fig_12\"},\"end\":48612,\"start\":48299},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":49006,\"start\":48613},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49120,\"start\":49007},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49415,\"start\":49121},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":49513,\"start\":49416},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49751,\"start\":49514},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":49889,\"start\":49752}]", "paragraph": "[{\"end\":2951,\"start\":1770},{\"end\":3390,\"start\":2953},{\"end\":3889,\"start\":3392},{\"end\":5347,\"start\":3891},{\"end\":6339,\"start\":5364},{\"end\":8451,\"start\":6341},{\"end\":8685,\"start\":8469},{\"end\":8827,\"start\":8825},{\"end\":8949,\"start\":8829},{\"end\":9317,\"start\":8951},{\"end\":10715,\"start\":9334},{\"end\":11344,\"start\":10748},{\"end\":12729,\"start\":11346},{\"end\":13171,\"start\":12749},{\"end\":13492,\"start\":13193},{\"end\":13685,\"start\":13549},{\"end\":13801,\"start\":13746},{\"end\":13899,\"start\":13868},{\"end\":13954,\"start\":13901},{\"end\":14057,\"start\":13956},{\"end\":14644,\"start\":14059},{\"end\":14767,\"start\":14646},{\"end\":15058,\"start\":14791},{\"end\":15187,\"start\":15119},{\"end\":15499,\"start\":15194},{\"end\":15860,\"start\":15728},{\"end\":16480,\"start\":15862},{\"end\":16968,\"start\":16507},{\"end\":17207,\"start\":16970},{\"end\":17473,\"start\":17226},{\"end\":17941,\"start\":17475},{\"end\":19781,\"start\":17943},{\"end\":20207,\"start\":19783},{\"end\":20728,\"start\":20223},{\"end\":21495,\"start\":20730},{\"end\":21584,\"start\":21528},{\"end\":22190,\"start\":21586},{\"end\":22522,\"start\":22192},{\"end\":23614,\"start\":22547},{\"end\":23971,\"start\":23616},{\"end\":25331,\"start\":23973},{\"end\":25857,\"start\":25333},{\"end\":26365,\"start\":25859},{\"end\":27009,\"start\":26367},{\"end\":27847,\"start\":27011},{\"end\":28987,\"start\":27891},{\"end\":29326,\"start\":29027},{\"end\":30361,\"start\":29369},{\"end\":31964,\"start\":30363},{\"end\":32271,\"start\":31980},{\"end\":32452,\"start\":32345},{\"end\":32872,\"start\":32454},{\"end\":33478,\"start\":32874},{\"end\":33540,\"start\":33480},{\"end\":33758,\"start\":33542},{\"end\":33876,\"start\":33760},{\"end\":33938,\"start\":33878},{\"end\":35269,\"start\":33940},{\"end\":35445,\"start\":35271},{\"end\":35581,\"start\":35474},{\"end\":35790,\"start\":35583},{\"end\":35862,\"start\":35792},{\"end\":36110,\"start\":35864},{\"end\":36979,\"start\":36112},{\"end\":37134,\"start\":37027},{\"end\":37325,\"start\":37170},{\"end\":37634,\"start\":37327},{\"end\":37883,\"start\":37669},{\"end\":39264,\"start\":37885},{\"end\":40306,\"start\":39300},{\"end\":40772,\"start\":40326},{\"end\":41242,\"start\":40774},{\"end\":41631,\"start\":41272},{\"end\":42142,\"start\":41633},{\"end\":42317,\"start\":42144},{\"end\":42629,\"start\":42319},{\"end\":42959,\"start\":42631},{\"end\":43133,\"start\":42961},{\"end\":43469,\"start\":43135},{\"end\":43951,\"start\":43471},{\"end\":44321,\"start\":43953},{\"end\":44875,\"start\":44323},{\"end\":45823,\"start\":44902},{\"end\":46370,\"start\":45825}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8824,\"start\":8686},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13548,\"start\":13493},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13745,\"start\":13686},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13867,\"start\":13802},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15118,\"start\":15059},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15727,\"start\":15537},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29026,\"start\":28988},{\"attributes\":{\"id\":\"formula_7\"},\"end\":39299,\"start\":39265}]", "table_ref": "[{\"end\":7422,\"start\":7415},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22652,\"start\":22575},{\"end\":22660,\"start\":22653},{\"end\":24580,\"start\":24573},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26454,\"start\":26447},{\"end\":29400,\"start\":29393},{\"end\":41282,\"start\":41275},{\"end\":42221,\"start\":42214},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":42676,\"start\":42669},{\"end\":42940,\"start\":42933},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":43468,\"start\":43460}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1768,\"start\":1756},{\"attributes\":{\"n\":\"2.\"},\"end\":5362,\"start\":5350},{\"attributes\":{\"n\":\"3.\"},\"end\":8467,\"start\":8454},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9332,\"start\":9320},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10746,\"start\":10718},{\"attributes\":{\"n\":\"4.\"},\"end\":12747,\"start\":12732},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13191,\"start\":13174},{\"attributes\":{\"n\":\"4.2.\"},\"end\":14789,\"start\":14770},{\"end\":15192,\"start\":15190},{\"attributes\":{\"n\":\"4.\"},\"end\":15536,\"start\":15502},{\"attributes\":{\"n\":\"4.3.\"},\"end\":16505,\"start\":16483},{\"attributes\":{\"n\":\"4.4.\"},\"end\":17224,\"start\":17210},{\"attributes\":{\"n\":\"5.\"},\"end\":20221,\"start\":20210},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21526,\"start\":21498},{\"end\":22545,\"start\":22525},{\"attributes\":{\"n\":\"5.2.\"},\"end\":27889,\"start\":27850},{\"attributes\":{\"n\":\"5.3.\"},\"end\":29367,\"start\":29329},{\"attributes\":{\"n\":\"6.\"},\"end\":31978,\"start\":31967},{\"end\":32309,\"start\":32274},{\"end\":32343,\"start\":32312},{\"end\":35472,\"start\":35448},{\"end\":37025,\"start\":36982},{\"end\":37168,\"start\":37137},{\"end\":37667,\"start\":37637},{\"end\":40324,\"start\":40309},{\"end\":41270,\"start\":41245},{\"end\":44900,\"start\":44878},{\"end\":46382,\"start\":46372},{\"end\":46528,\"start\":46518},{\"end\":46885,\"start\":46875},{\"end\":47162,\"start\":47152},{\"end\":47347,\"start\":47337},{\"end\":47555,\"start\":47546},{\"end\":47679,\"start\":47669},{\"end\":47993,\"start\":47982},{\"end\":48311,\"start\":48300},{\"end\":49017,\"start\":49008},{\"end\":49131,\"start\":49122},{\"end\":49426,\"start\":49417},{\"end\":49524,\"start\":49515},{\"end\":49762,\"start\":49753}]", "table": "[{\"end\":49006,\"start\":48691},{\"end\":49415,\"start\":49198},{\"end\":49751,\"start\":49526}]", "figure_caption": "[{\"end\":46516,\"start\":46384},{\"end\":46873,\"start\":46530},{\"end\":47150,\"start\":46887},{\"end\":47335,\"start\":47164},{\"end\":47544,\"start\":47349},{\"end\":47667,\"start\":47557},{\"end\":47980,\"start\":47681},{\"end\":48298,\"start\":47996},{\"end\":48612,\"start\":48314},{\"end\":48691,\"start\":48615},{\"end\":49120,\"start\":49019},{\"end\":49198,\"start\":49133},{\"end\":49513,\"start\":49428},{\"end\":49889,\"start\":49764}]", "figure_ref": "[{\"end\":2032,\"start\":2021},{\"end\":2643,\"start\":2636},{\"end\":8840,\"start\":8832},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9091,\"start\":9084},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9316,\"start\":9309},{\"end\":10081,\"start\":10074},{\"end\":10714,\"start\":10706},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13170,\"start\":13164},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22796,\"start\":22789},{\"end\":25562,\"start\":25556},{\"end\":25609,\"start\":25600},{\"end\":25628,\"start\":25619},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26040,\"start\":26032},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26168,\"start\":26159},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27073,\"start\":27067},{\"end\":29038,\"start\":29030},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29567,\"start\":29561},{\"end\":30666,\"start\":30659},{\"end\":30743,\"start\":30736},{\"end\":30813,\"start\":30804},{\"end\":31014,\"start\":31005},{\"end\":31619,\"start\":31611},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":43515,\"start\":43509},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43963,\"start\":43956},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44342,\"start\":44335}]", "bib_author_first_name": "[{\"end\":50131,\"start\":50125},{\"end\":50143,\"start\":50139},{\"end\":50152,\"start\":50148},{\"end\":50168,\"start\":50158},{\"end\":50676,\"start\":50670},{\"end\":50951,\"start\":50945},{\"end\":50960,\"start\":50957},{\"end\":51255,\"start\":51249},{\"end\":51266,\"start\":51260},{\"end\":51282,\"start\":51274},{\"end\":51295,\"start\":51289},{\"end\":51306,\"start\":51302},{\"end\":51799,\"start\":51792},{\"end\":51950,\"start\":51944},{\"end\":51973,\"start\":51965},{\"end\":51990,\"start\":51989},{\"end\":52008,\"start\":52002},{\"end\":52010,\"start\":52009},{\"end\":52488,\"start\":52482},{\"end\":52501,\"start\":52493},{\"end\":52512,\"start\":52507},{\"end\":52527,\"start\":52519},{\"end\":53039,\"start\":53030},{\"end\":53053,\"start\":53045},{\"end\":53073,\"start\":53058},{\"end\":53087,\"start\":53080},{\"end\":53440,\"start\":53435},{\"end\":53450,\"start\":53446},{\"end\":53459,\"start\":53456},{\"end\":53473,\"start\":53465},{\"end\":53486,\"start\":53480},{\"end\":53963,\"start\":53957},{\"end\":53978,\"start\":53974},{\"end\":53996,\"start\":53990},{\"end\":54349,\"start\":54343},{\"end\":54360,\"start\":54356},{\"end\":54372,\"start\":54367},{\"end\":54383,\"start\":54377},{\"end\":54398,\"start\":54390},{\"end\":54414,\"start\":54405},{\"end\":54770,\"start\":54762},{\"end\":54781,\"start\":54777},{\"end\":54799,\"start\":54792},{\"end\":54814,\"start\":54807},{\"end\":55217,\"start\":55213},{\"end\":55228,\"start\":55224},{\"end\":55246,\"start\":55240},{\"end\":55591,\"start\":55585},{\"end\":55600,\"start\":55597},{\"end\":55612,\"start\":55606},{\"end\":55621,\"start\":55619},{\"end\":55630,\"start\":55627},{\"end\":55640,\"start\":55636},{\"end\":55650,\"start\":55646},{\"end\":56149,\"start\":56142},{\"end\":56159,\"start\":56156},{\"end\":56169,\"start\":56164},{\"end\":56181,\"start\":56174},{\"end\":56191,\"start\":56188},{\"end\":56204,\"start\":56197},{\"end\":56214,\"start\":56211}]", "bib_author_last_name": "[{\"end\":50137,\"start\":50132},{\"end\":50146,\"start\":50144},{\"end\":50156,\"start\":50153},{\"end\":50172,\"start\":50169},{\"end\":50684,\"start\":50677},{\"end\":50955,\"start\":50952},{\"end\":50968,\"start\":50961},{\"end\":51258,\"start\":51256},{\"end\":51272,\"start\":51267},{\"end\":51287,\"start\":51283},{\"end\":51300,\"start\":51296},{\"end\":51310,\"start\":51307},{\"end\":51807,\"start\":51800},{\"end\":51963,\"start\":51951},{\"end\":51977,\"start\":51974},{\"end\":51987,\"start\":51979},{\"end\":52000,\"start\":51991},{\"end\":52015,\"start\":52011},{\"end\":52021,\"start\":52017},{\"end\":52491,\"start\":52489},{\"end\":52505,\"start\":52502},{\"end\":52517,\"start\":52513},{\"end\":52532,\"start\":52528},{\"end\":53043,\"start\":53040},{\"end\":53056,\"start\":53054},{\"end\":53078,\"start\":53074},{\"end\":53093,\"start\":53088},{\"end\":53444,\"start\":53441},{\"end\":53454,\"start\":53451},{\"end\":53463,\"start\":53460},{\"end\":53478,\"start\":53474},{\"end\":53491,\"start\":53487},{\"end\":53972,\"start\":53964},{\"end\":53988,\"start\":53979},{\"end\":54006,\"start\":53997},{\"end\":54354,\"start\":54350},{\"end\":54365,\"start\":54361},{\"end\":54375,\"start\":54373},{\"end\":54388,\"start\":54384},{\"end\":54403,\"start\":54399},{\"end\":54417,\"start\":54415},{\"end\":54775,\"start\":54771},{\"end\":54790,\"start\":54782},{\"end\":54805,\"start\":54800},{\"end\":54817,\"start\":54815},{\"end\":55222,\"start\":55218},{\"end\":55238,\"start\":55229},{\"end\":55252,\"start\":55247},{\"end\":55595,\"start\":55592},{\"end\":55604,\"start\":55601},{\"end\":55617,\"start\":55613},{\"end\":55625,\"start\":55622},{\"end\":55634,\"start\":55631},{\"end\":55644,\"start\":55641},{\"end\":55655,\"start\":55651},{\"end\":56154,\"start\":56150},{\"end\":56162,\"start\":56160},{\"end\":56172,\"start\":56170},{\"end\":56186,\"start\":56182},{\"end\":56195,\"start\":56192},{\"end\":56209,\"start\":56205},{\"end\":56217,\"start\":56215}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4892945},\"end\":50545,\"start\":50053},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":225715046},\"end\":50885,\"start\":50547},{\"attributes\":{\"doi\":\"arXiv:1805.02104\",\"id\":\"b2\"},\"end\":51124,\"start\":50887},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":59158744},\"end\":51750,\"start\":51126},{\"attributes\":{\"id\":\"b4\"},\"end\":51876,\"start\":51752},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1759984},\"end\":52392,\"start\":51878},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4321884},\"end\":52923,\"start\":52394},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":199442100},\"end\":53347,\"start\":52925},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206593370},\"end\":53888,\"start\":53349},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":220403571},\"end\":54246,\"start\":53890},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":208227848},\"end\":54733,\"start\":54248},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4852647},\"end\":55156,\"start\":54735},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":104292483},\"end\":55505,\"start\":55158},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219616556},\"end\":56077,\"start\":55507},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":220483424},\"end\":56615,\"start\":56079}]", "bib_title": "[{\"end\":50123,\"start\":50053},{\"end\":50668,\"start\":50547},{\"end\":51247,\"start\":51126},{\"end\":51942,\"start\":51878},{\"end\":52480,\"start\":52394},{\"end\":53028,\"start\":52925},{\"end\":53433,\"start\":53349},{\"end\":53955,\"start\":53890},{\"end\":54341,\"start\":54248},{\"end\":54760,\"start\":54735},{\"end\":55211,\"start\":55158},{\"end\":55583,\"start\":55507},{\"end\":56140,\"start\":56079}]", "bib_author": "[{\"end\":50139,\"start\":50125},{\"end\":50148,\"start\":50139},{\"end\":50158,\"start\":50148},{\"end\":50174,\"start\":50158},{\"end\":50686,\"start\":50670},{\"end\":50957,\"start\":50945},{\"end\":50970,\"start\":50957},{\"end\":51260,\"start\":51249},{\"end\":51274,\"start\":51260},{\"end\":51289,\"start\":51274},{\"end\":51302,\"start\":51289},{\"end\":51312,\"start\":51302},{\"end\":51809,\"start\":51792},{\"end\":51965,\"start\":51944},{\"end\":51979,\"start\":51965},{\"end\":51989,\"start\":51979},{\"end\":52002,\"start\":51989},{\"end\":52017,\"start\":52002},{\"end\":52023,\"start\":52017},{\"end\":52493,\"start\":52482},{\"end\":52507,\"start\":52493},{\"end\":52519,\"start\":52507},{\"end\":52534,\"start\":52519},{\"end\":53045,\"start\":53030},{\"end\":53058,\"start\":53045},{\"end\":53080,\"start\":53058},{\"end\":53095,\"start\":53080},{\"end\":53446,\"start\":53435},{\"end\":53456,\"start\":53446},{\"end\":53465,\"start\":53456},{\"end\":53480,\"start\":53465},{\"end\":53493,\"start\":53480},{\"end\":53974,\"start\":53957},{\"end\":53990,\"start\":53974},{\"end\":54008,\"start\":53990},{\"end\":54356,\"start\":54343},{\"end\":54367,\"start\":54356},{\"end\":54377,\"start\":54367},{\"end\":54390,\"start\":54377},{\"end\":54405,\"start\":54390},{\"end\":54419,\"start\":54405},{\"end\":54777,\"start\":54762},{\"end\":54792,\"start\":54777},{\"end\":54807,\"start\":54792},{\"end\":54819,\"start\":54807},{\"end\":55224,\"start\":55213},{\"end\":55240,\"start\":55224},{\"end\":55254,\"start\":55240},{\"end\":55597,\"start\":55585},{\"end\":55606,\"start\":55597},{\"end\":55619,\"start\":55606},{\"end\":55627,\"start\":55619},{\"end\":55636,\"start\":55627},{\"end\":55646,\"start\":55636},{\"end\":55657,\"start\":55646},{\"end\":56156,\"start\":56142},{\"end\":56164,\"start\":56156},{\"end\":56174,\"start\":56164},{\"end\":56188,\"start\":56174},{\"end\":56197,\"start\":56188},{\"end\":56211,\"start\":56197},{\"end\":56219,\"start\":56211}]", "bib_venue": "[{\"end\":50315,\"start\":50253},{\"end\":51453,\"start\":51391},{\"end\":52144,\"start\":52092},{\"end\":52675,\"start\":52613},{\"end\":53634,\"start\":53572},{\"end\":54960,\"start\":54898},{\"end\":55820,\"start\":55747},{\"end\":56360,\"start\":56298},{\"end\":50251,\"start\":50174},{\"end\":50700,\"start\":50686},{\"end\":50943,\"start\":50887},{\"end\":51389,\"start\":51312},{\"end\":51790,\"start\":51752},{\"end\":52090,\"start\":52023},{\"end\":52611,\"start\":52534},{\"end\":53128,\"start\":53095},{\"end\":53570,\"start\":53493},{\"end\":54057,\"start\":54008},{\"end\":54481,\"start\":54419},{\"end\":54896,\"start\":54819},{\"end\":55315,\"start\":55254},{\"end\":55745,\"start\":55657},{\"end\":56296,\"start\":56219}]"}}}, "year": 2023, "month": 12, "day": 17}