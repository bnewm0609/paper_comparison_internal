{"id": 244909191, "updated": "2023-12-16 14:59:34.159", "metadata": {"title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation", "authors": "[{\"first\":\"Zhao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jiaqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yansong\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hengshuang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Torr\",\"middle\":[\"H.S.\"]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": 6, "day": 1}, "abstract": "Referring image segmentation is a fundamental vision-language task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the referring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (\u201ccross-madal\u201d) decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer's overwhelming success in many other vision-language tasks. Adopting a different approach in this work, we show that significantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder network. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the well-proven correlation modeling power of a Transformer encoder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods on Ref CoCo, RefCOCO+, and G-Ref by large margins.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/0002WT0ZT22", "doi": "10.1109/cvpr52688.2022.01762"}}, "content": {"source": {"pdf_hash": "c5bfbc8c02c423ce938388dd31e75c07d8fea5da", "pdf_src": "IEEE", "pdf_uri": "[\"https://arxiv.org/pdf/2112.02244v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2112.02244", "status": "GREEN"}}, "grobid": {"id": "e315402c93ee476a51a12cad511dc730b8140c94", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/c5bfbc8c02c423ce938388dd31e75c07d8fea5da.txt", "contents": "\nLAVT: Language-Aware Vision Transformer for Referring Image Segmentation\n\n\nZhao Yang \nUniversity of Oxford\n\n\nJiaqi Wang \nLaboratory\nShanghaiAI\n\nYansong Tang \nUniversity of Oxford\n\n\nTsinghua-Berkeley Shenzhen Institute\nTsinghua University\n\n\nKai Chen \nLaboratory\nShanghaiAI\n\nSenseTime Research\n\n\nHengshuang Zhao \nUniversity of Oxford\n\n\nThe University of Hong Kong\n\n\nPhilip H S Torr \nUniversity of Oxford\n\n\nLAVT: Language-Aware Vision Transformer for Referring Image Segmentation\n10.1109/CVPR52688.2022.01762\nReferring image segmentation is a fundamental visionlanguage task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the referring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (\"cross-modal\") decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer's overwhelming success in many other vision-language tasks. Adopting a different approach in this work, we show that significantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder network. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the wellproven correlation modeling power of a Transformer encoder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.\n\nIntroduction\n\nGiven an image and a text description of the target object, referring image segmentation aims at predicting a pixel-wise mask that delineates that object [8,18]. It yields great value for various applications such as language-based human-robot interaction [55] and image editing [5]. In contrast to conventional single-modality visual segmentation tasks based on fixed category conditions [30,65], referring image segmentation has to deal with the much richer vocabularies and syntactic varieties of human natural languages. Language Encoder \"pink doughnut with frosting\" Figure 1. The task of referring image segmentation takes one image and one text description as inputs, and predicts a mask delineating the object specified in the description. (a) The previous state-of-the-art method (i.e., VLT [12]) leverages a visionlanguage Transformer decoder for cross-modal feature fusion. (b) Conversely, we propose to directly integrate linguistic information into visual features at intermediate levels of a vision Transformer network, where beneficial vision-language cues are jointly exploited. A light-weight mask predictor can thus readily replace the complicated cross-modal decoder in previous counterparts.\n\nIn this task, the target object is inferred from a free-form expression, which includes words and phrases presenting the concepts of entities, actions, attributes, positions, etc., organized by syntactic rules. Therefore, the key challenge of this task is to exploit visual features that are relevant to the given text conditions.\n\nThere have been growing efforts devoted to referring image segmentation over the past few years. A widely adopted paradigm is to first independently extract vision and language features from different encoder networks, and then fuse them together to make predictions with a cross-modal decoder. Concretely, the fusion strategies include recurrent interaction [28,31], cross-modal attention [4,20,48], multi-modal graph reasoning [21], linguistic structure-guided context modeling [22], etc. Recent advances (e.g., [12]) bring performance improvements via employing a crossmodal Transformer [54] decoder (illustrated in Fig. 1 (a)) to learn more effective cross-modal alignments, which is in concurrence with Transformer's overwhelming success in many other vision-language tasks [19,27,37,45].\n\nAlthough great progress has been achieved, the potentiality of the Transformer for enhancing referring image segmentation is still far from being sufficiently explored in the conventional paradigm. Specifically, cross-modal interactions occur only after feature encoding, and a crossmodal decoder is solely responsible for aligning the visual and linguistic features. As a result, previous methods fail to effectively leverage the rich Transformer layers in the encoder for excavating helpful multi-modal context. To address these issues, a potential solution is to exploit a visual encoder network for jointly embedding linguistic and visual features during visual encoding.\n\nAccordingly, we propose a Language-Aware Vision Transformer (LAVT) network, in which visual features are encoded together with linguistic features, being \"aware\" of their relevant linguistic context at each spatial location. As shown in Fig. 1 (b), LAVT makes full use of the multi-stage design in a modern vision Transformer backbone network, leading to a hierarchical language-aware visual encoding scheme. Specifically, we densely integrate linguistic features into visual features via a pixel-word attention mechanism, which occurs at each stage of the network. The beneficial vision-language cues are then exploited by the following Transformer blocks, e.g., [33], in the next encoder stage. This approach enables us to forgo a complicated cross-modal decoder, since the extracted language-aware visual features can be readily adopted to harvest accurate segmentation masks with a lightweight mask predictor.\n\nTo evaluate the effectiveness of the proposed method, we conduct extensive experiments on various mainstream referring image segmentation datasets. Our LAVT achieves 72.73%, 62.14%, 61.24%, and 60.50% overall IoU on the validation sets of RefCOCO [63], RefCOCO+ [63], G-Ref (UMD partition) [42], and G-Ref (Google partition) [40], improving the state of the art for these datasets by absolute margins of 7.08%, 6.64%, 6.84%, and 8.57%, respectively.\n\nTo summarize, our contributions are twofold:\n\n\u2022 We propose LAVT, a Transformer-based referring image segmentation framework that performs languageaware visual encoding in place of cross-modal fusion post feature extraction.\n\n\u2022 We achieve new state-of-the-art results on three datasets for referring image segmentation, demonstrating the effectiveness and generality of the proposed method. Source code is available at LAVT-RIS.\n\n\nRelated work\n\nReferring image segmentation has attracted growing attention in the research community and there are two main processes in conventional pipelines: (1) extracting features from the text and image inputs respectively, and (2) fusing the multi-modal features to predict the segmentation mask. In the first process, previous methods adopt recurrent neural networks [17,18,25,28,31] and language Transformers [2,11] to encode language inputs. To encode visual inputs, vanilla fully convolutional networks [18,31,35], DeeplabV3 [2,6,28], and DarkNet [25,39,47] have been successively employed in previous methods with the purpose of learning discriminative representations. The multi-modal feature fusion module is the key component that prior arts focus on. For example, Hu et al. [18] propose the first baseline based on the concatenation operation, which is improved by Liu  The methods most related to ours are VLT [12] and EFN [14], where the former designs a Transformer decoder for fusing linguistic and visual features, and the latter adopts a convolutional vision backbone network for encoding language information. Differently from [12], we propose an early fusion scheme which effectively exploits the Transformer encoder for modeling multi-modal context. Compared to [14], we do not rely on a complicated cross-modal decoder, leading to a clearer and more effective framework. Under fair comparisons, our method outperforms these two previous counterparts by large margins. Transformer is first introduced as a sequence-to-sequence deep attention-based language model [54], and has dominated the natural language processing (NLP) field [9,11,58] due to its strong capability on global context modeling. More recently, it has achieved great success on various computer vision tasks, e.g., image classification [13,33,52], action recognition [1,34], object detection [3,33,66], and semantic segmentation [33,51,64].\n\nThere has also been a rich line of work on Transformers in the intersection area of computer vision and NLP [26,46]. For example, Radford et al. devise a large-scale pretraining model, named CLIP [45], which applies contrastive learning [15,16,50] on features learned by a vision Transformer and a language Transformer. Hu et al. [19] propose a Unified Transformer (UniT) model that jointly learns multiple vision-language tasks across different domains. Besides, growing efforts have been devoted to other tasks such as visual question answering [37] and text-to-video retrieval [27]. However, to the best of our knowledge, there have been very few attempts on designing a unified Transformer model for the task of referring image segmentation.\n\n\nMethod\n\nFig . 2 illustrates the pipeline of our Language-Aware Vision Transformer (LAVT), which leverages a hierarchical vision Transformer to jointly embed language and vision information to facilitate cross-modal alignments. In this section, we start by introducing our language-aware visual encoding strategy in Sec. 3.1, which is achieved with a pixelword attention module detailed in Sec. 3.2 and a language pathway detailed in Sec. 3.3. Then in Sec. 3.4 we describe the light-weight mask predictor used to obtain final results.\n\n\nLanguage-aware visual encoding\n\nGiven an input pair of an image and a natural language expression that specifies an object from the image, our model outputs a pixel-wise mask that delineates the object. To extract language features, we employ a deep language representation model to embed the input expression into high-dimensional word vectors. We denote the language features as L \u2208 R Ct\u00d7T , where C t and T denote the number of channels and the number of words, respectively.\n\nAfter obtaining the language features, we perform joint visual feature encoding and vision-language (which is also called \"cross-modal\" or \"multi-modal\" in the following contents) feature fusion through a hierarchy of vision Transformer layers organized into four stages. We index each stage using i \u2208 {1, 2, 3, 4} in the bottom-up direction. Each stage employs a stack of Transformer encoding layers (with the same output size) \u03d5 i , a multi-modal feature fusion module \u03b8 i , and a learnable gating unit \u03c8 i . Within each stage, language-aware visual features are generated and refined via three steps. First, the Transformer layers \u03d5 i take the features from the previous stage as input, and output enriched visual features, denoted as V i \u2208 R Ci\u00d7Hi\u00d7Wi . Then, V i are combined with language features L via the multi-modal feature fusion module \u03b8 i to produce a set of multi-modal features, denoted as F i \u2208 R Ci\u00d7Hi\u00d7Wi . Finally, each element in F i is weighted by the learnable gating unit \u03c8 i and then added element-wise to V i to produce a set of enhanced visual features embedded with linguistic information, which we denote as E i \u2208 R Ci\u00d7Hi\u00d7Wi . We refer to the computations in this final step as the language pathway. Here, C i , H i , and W i denote the number of channels, the height, and the width of feature maps in the i-th stage, respectively.\n\nThe four stages of Transformer encoding layers correspond to the four stages in a Swin Transformer [33], which is an efficient hierarchical vision backbone suitable for addressing dense prediction tasks. The multi-modal feature fusion module within each stage is our proposed pixel-word attention module (PWAM), which is designed with the aim to densely align linguistic meanings with visual clues. And the gating unit is what we refer to as the language gate (LG), a special unit that we devise for regulating the flow of linguistic information along the language pathway (LP).\n\n\nPixel-word attention module\n\nIn order to separate a target object from its background, it is important to align the visual and linguistic representa-  First, a single-head scaled dot-product attention [54] is performed using the input visual feature maps Vi as queries and the input linguistic feature maps L as keys and values. The result, Gi, is a set of linguistic feature maps of the same spatial size as Vi. Gi is then multiplied element-wise with a projection of the input visual feature maps Vim, followed by another projection before final output. A detail which we found important empirically is the adoption of an instance normalization [53] layer in the projection functions \u03c9iq and \u03c9iw (see the text below and Table 3).\nInput\ntions of the object across modalities. One general approach is to combine the representation of each pixel with the representation of the referring expression, and learn multi-modal representations that are discriminative of a \"referent\" class and a \"background\" class. Previous approaches have developed various mechanisms for addressing this challenge, including dynamic convolutions [41], concatenations [18,28,41], cross-modal attentions [14,20,38,49,60], graph neural networks [32], etc. Compared to most of the previous crossmodal attention mechanisms [14,20,38,49,60], our pixelword attention module (PWAM) produces a much smaller memory footprint as it avoids computing attention weights between two image-sized spatial feature maps, and is also simpler due to fewer attention steps. Fig. 3 illustrates PWAM schematically. Given the input visual features V i \u2208 R Ci\u00d7Hi\u00d7Wi and linguistic features L \u2208 R Ct\u00d7T , PWAM performs multi-modal fusion in two steps, as introduced in the following. First, at each spatial location, PWAM aggregates the linguistic features L across the word dimension to generate a positionspecific, sentence-level feature vector, which collects linguistic information most relevant to the current local neigh-borhood. This step generates a set of spatial feature maps, G i \u2208 R Ci\u00d7Hi\u00d7Wi . Concretely, we obtain G i as follows\nV iq = flatten(\u03c9 iq (V i )),(1)L ik = \u03c9 ik (L),(2)L iv = \u03c9 iv (L),(3)G \u2032 i = softmax( V T iq L ik \u221a C i )L T iv ,(4)G i = \u03c9 iw (unflatten(G \u2032T i )),(5)\nwhere \u03c9 iq , \u03c9 ik , \u03c9 iv , and \u03c9 iw are projection functions. Each of the language projections \u03c9 ik and \u03c9 iv is implemented as a 1\u00d71 convolution with C i number of output channels. And the query projection \u03c9 iq and the final projection \u03c9 iw each is implemented as a 1\u00d71 convolution followed by instance normalization, with C i number of output channels. Here, 'flatten' refers to the operation of unrolling the two spatial dimensions into one dimension in row-major, C-style order, and 'unflatten' refers to the opposite operation. These two operations and transposing are used to transform feature maps into proper shapes for calculation. Eqs. 1 to 5 implement the scaled dot-product attention [54] using visual features V i as the query and linguistic features L as the key and the value, with instance normalization after linear transformation in the query projection function \u03c9 iq and the output projection function \u03c9 iw . Second, after obtaining the linguistic features G i which have the same shape as V i , we combine them to produce a set of multi-modal feature maps F i via element-wise multiplication. Specifically, our step is described as follows\nV im = \u03c9 im (V i ),(6)F i = \u03c9 io (V im \u2299 G i ),(7)\nwhere \u2299 denotes element-wise multiplication and \u03c9 im and \u03c9 io are a visual projection and a final multi-modal projection, respectively. Each of the two functions is implemented as a 1\u00d71 convolution followed by ReLU [43] nonlinearity.\n\n\nLanguage pathway\n\nAs described earlier, at each stage, we merge the output from PWAM, F i , with the output from the Transformer layers, V i . We refer to the computations in this merging operation as the language pathway. In order to prevent F i from overwhelming the visual signals in V i and to allow an adaptive amount of linguistic information flowing to the next stage of Transformer layers, we design a language gate which learns a set of element-wise weight maps based on F i to re-scale each element in F i . The language pathway is schematically illustrated in Fig. 4 and mathematically described as follows  LG is implemented as a two-layer perceptron.\nS i = \u03b3 i (F i ),(8)E i = S i \u2299 F i + V i ,(9)\nwhere \u2299 indicates element-wise multiplication and \u03b3 i is a two-layer perceptron, with the first layer being a 1\u00d71 convolution followed by ReLU [43] nonlinearity and the second layer being a 1\u00d71 convolution followed by a hyperbolic tangent function. As detailed in the ablation studies in Table 3, we have experimented with and without using a language gate along the language pathway, as well as different final nonlinear activation functions in the language gate, and found that using the gate with tanh final nonlinearity works the best for our model. The summation operation in Eq. 9 is an effective way of utilizing pre-trained vision Transformer layers for multi-modal embedding, as the treatment of multi-modal features as \"supplements\" (or \"residuals\") avoids disrupting the initialization weights pre-trained on pure vision data. We have observed much worse results in the case of adopting replacement or concatenation.\n\n\nSegmentation\n\nWe combine the multi-modal feature maps, F i , i \u2208 {1, 2, 3, 4}, in a top-down manner to exploit multi-scale semantics for final segmentation. The decoding process can be described by the following recursive function\nY 4 = F 4 , Y i = \u03c1 i ([\u03c5(Y i+1 ); F i ]), i = 3, 2, 1.(10)\nHere '[ ; ]' denotes feature concatenation along the channel dimension, \u03c5 represents upsampling via bilinear interpolation, and \u03c1 i is a projection function implemented as two 3\u00d73 convolutions connected by batch normalization [24] and ReLU [43] nonlinearity. The final feature maps, Y 1 , are projected into two class score maps via a 1\u00d71 convolution.\n\n\nImplementation\n\nWe implement our method in PyTorch [44] and use the BERT implementation from HuggingFace's Transformer library [56]. The Transformer layers in LAVT are initialized with classification weights pre-trained on ImageNet-22K [10] from the Swin Transformer [33]. Our language encoder is the base BERT model with 12 layers and hidden size 768 from [54] (hence C t in Sec. 3 is 768) and is initialized using the official pre-trained weights. The rest of weights in our model are randomly initialized. C i in Sec. 3 is set to 512 and the model is optimized with cross-entropy loss. Following [33], we adopt the AdamW [36] optimizer with weight decay 0.01 and initial learning rate 0.00005 with polynomial learning rate decay. We train our model for 40 epochs with batch size 32. We iterate through each object (while randomly sampling one referring expression for it) exactly once in an epoch. Images are resized to 480\u00d7480 and no data augmentation techniques are applied. During inference, argmax along the channel dimension of the score maps are used as predictions.\n\n\nExperiments\n\n\nDatasets and metrics\n\nWe evaluate our method on three standard benchmark datasets, RefCOCO [63], RefCOCO+ [63], and G-Ref [40,42]. Images in the three datasets are collected from the MS COCO dataset [30]  Conversely, RefCOCO and RefCOCO+ tend to have more objects of the same category per image (3.9 on average) compared to G-Ref (1.6 on average), therefore they better evaluate an algorithm's ability to comprehend instancelevel details. A characteristic of RefCOCO+ is that location words are banned in its expressions, which also makes it more challenging. Additionally, there are two different partitions of the G-Ref dataset, one by UMD [42] and the other by Google [40]. We report results on both. When evaluating on each dataset, we train our model on the training set of that dataset. Finally, we make note of the ambiguities and foul language found in many expressions of RefCOCO with the hope that future community efforts will address them.\n\nWe adopt the common metrics of overall intersectionover-union (oIoU), mean intersection-over-union (mIoU), and precision at the 0.5, 0.7, and 0.9 threshold values. The overall IoU is measured as the ratio between the total intersection area and the total union area of all test samples, each of which is a language expression and an image. This metric favors large objects. The mean IoU is the IoU between the prediction and ground truth averaged across all test samples. This metric treats large and small objects equally. The precision metric measures the percentage of test samples that pass an IoU threshold.  Table 1. Comparison with state-of-the-art methods in terms of overall IoU on three benchmark datasets. U: The UMD partition. G: The Google partition. We refer to the language model of each reference method as the main learnable function that transforms word embeddings before multi-modal feature fusion. Interested readers can refer to the respective papers for embedding initialization and other details.\n\n\nComparison with others\n\nIn Table 1, we evaluate LAVT against the state-ofthe-art referring image segmentation methods on the Ref-COCO [63], RefCOCO+ [63], and G-Ref [40,42] datasets using the oIoU metric. LAVT outperforms all previous methods on all evaluation subsets of all three datasets. Compared with the second-best method, VLT [12], LAVT achieves higher performance with absolute margins of 7.08%, 7.53%, and 6.06% on the validation, testA, and testB subsets of RefCOCO, respectively. Similarly, LAVT attains noticeable improvements over the previous state of the art on RefCOCO+ with wide margins of 6.64%, 9.18%, and 5.74% on the validation, testA, and testB subsets, respectively. On the most challenging G-Ref dataset (which contains significantly longer expressions), LAVT surpasses the respective second-best methods on the validation and test subsets from the UMD partition by absolute margins of 6.84% and 5.44%, respectively. Similarly on the validation set from the Google partition, LAVT outperforms the second-best method EFN [14] by an absolute margin of 8.57%. This performance is achieved without using Ref-COCO as additional training data in contrast to EFN.\n\n\nAblation study\n\nWe conduct several ablations to evaluate the effectiveness of the key components in our proposed network. Language pathway (LP). Eqs. 8 and 9, or schematically, the removal of the orange stream in Fig. 2) leads to a drop of 1.95 and 2.50 absolute points in overall IoU and mean IoU, respectively. In addition, precision drops by 3 to 4 points across all three thresholds. These results demonstrate the benefit of exploiting our vision Transformer encoder network for jointly embedding linguistic and visual features. Pixel-word attention module (PWAM). In this ablation study, we replace the spatial language feature maps G i in PWAM with a sentence feature vector globally pooled from all words [59]. As shown in Table 2, this ablation leads to a drop of 1.70 and 2.15 absolute points in overall IoU and mean IoU, respectively, and a drop of 1 to 2 absolute points in precision across the three thresholds. These results illustrate the effectiveness of densely aggregating linguistic context via our proposed attention mechanism for enhancing cross-modal alignments. Activation function in the language gate (LG). Our proposed LG learns a set of spatial weight maps, which give our network the flexibility to control the flow of language  information in the language pathway. In Table 3 (a), we compare the sigmoid function and the hyperbolic tangent function as the final activation function in LG. Using the sigmoid function leads to inferior results. Normalization layer in PWAM. As described in Sec. 3.2, we adopt a final instance normalization layer in the projection functions \u03c9 iq and \u03c9 iw in PWAM. As we illustrate in Table 3 (b), this particular choice of normalization function has a non-trivial effect. In addition to instance normalization (our default choice), we experiment with batch normalization, layer normalization, and without having a normalization layer in the functions \u03c9 iq and \u03c9 iw . All three other choices lead to 1 to 2 absolute points drop in the overall IoU and mean IoU metrics. Among these three choices, using batch normalization or layer normalization produces better results than not using a normalization layer.\n\nFeatures used for prediction. As shown in Fig. 4, the language-aware visual encoding process of LAVT produces three kinds of spatial feature maps which encapsulate visual and linguistic information, i.e., the outputs from PWAMs (F i , i \u2208 {1, 2, 3, 4}), the outputs from the Transformer layers (V i , i \u2208 {2, 3, 4}), and the inputs to the following Transformer layers (E i , i \u2208 {1, 2, 3}). While our default choice is to use F i for predicting the object mask, we also consider the other two types of feature maps natural candidates for this purpose. As shown in Fig. 2, E 4 is not generated in the standard architecture of LAVT. To have a convincing ablation study, we compute E 4 with an additional language pathway as defined in Eqs. 8 and 9. Therefore, we use E i , i \u2208 {1, 2, 3, 4} to predict the segmentation masks. In comparison, as multi-modal information has been progressively integrated into V 2 , V 3 , and V 4 along the bottom-up computation pathway while V 1 contains pure visual information, we do not use V 1 for prediction. In Table 3 (c), we report segmentation results when using each type of features with and without our proposed LG (indicated by \"G\" and \"NG\", respectively). Multi-modal attention module. In Table 3 (d), we com-\"white cell phone in middle\" \"blue phone\" \"phone on very bottom\" \"flip phone on right\"\n\nImage Ours Ground truth \"guy by the red wall with arms crossed \" Expressions:\n\nExpressions: \"gal touching hair\"\n\n\"guy in black sitting to left leaned over\" \"guy in strip shirt, on laptop\"\n\nOurs Ours Ours Ground truth Ground truth Ground truth pare PWAM with two state-of-the-art attention modules by directly replacing PWAM with them in our framework, using the same backbone, language model, and training recipes. Compared to both the grouped attention (GA or GARAN) [38,39] and the bi-directional cross-modal attention module (BCAM) [20], PWAM achieves higher scores across all metrics. Note that BCAM is representative of the computationally-heavy attention modules and GA is the most recent top-performing module.\n\nVisualized predictions. In Fig. 5, we visualize the predictions and feature maps of our full model and two ablated models (without the language pathway (\"w/o LP\") and without the pixel-word attention module (\"w/o PWAM\"), respectively). From the first row, we can observe that the higher-level feature maps (i.e., Y 4 , Y 3 , Y 2 ) in our full model can accurately locate the semantic concept given in text, while the low-level feature maps (i.e., Y 1 ) contain rich boundary information important to binary segmentation. Comparing the predicted masks between the three models, we can observe that the removal of LP and the removal of PWAM both lead to false negative predictions on the front window area of the target bus, while the removal of LP additionally results in the false positive identification of the middle bus. These qualitative results further validate the effectiveness of our proposed LP and PWAM mechanisms. More example visualizations are shown in Fig. 6.\n\nFair comparison with reference methods. To further validate the effectiveness of our proposed method of fusing cross-modal information via a vision Transformer encoder network, in Table 4, we provide fair comparisons between our method and three previous state-of-the-art methods, LTS [25], VLT [12] and EFN [14]. All models use BERT BASE as the language encoder and Swin-B as the vision backbone network, following the same training settings (described in Sec. 3.5). While LTS employs a \"locate-thensegment\" pipeline, VLT is representative of methods that employ a cross-modal Transformer decoder. Conversely, EFN is representative of methods which fuse cross-modal Method P@0.5 P@0.7 P@0.9 oIoU mIoU LTS (Swin-B+BERT) [25] 80  Table 4. Comparison between our method, LTS [25], VLT [12], and EFN [14] on the RefCOCO validation set, where all models use the same backbone, language model, and training recipes.\n\ninformation via an encoder network and additionally rely on a complicated decoder for obtaining the best results. As shown in Table 4, our method outperforms LTS, VLT, and EFN on the validation set of RefCOCO across all metrics.\n\nTo further verify that our proposed LAVT encoding scheme is more effective than its counterpart cross-modal decoder approach, we combine our approach with VLT by substituting our original light-weight mask predictor with the crossmodal Transformer decoder from VLT. As shown in this experiment (indicated by \"ours + VLT\" in Table 4), employing a Transformer decoder to perform additional crossmodal feature fusion after language-aware visual encoding by LAVT generally does not bring extra gains (except a marginal 0.11% improvement in P@0.5).\n\n\nConclusion\n\nIn this paper, we have proposed a Language-Aware Vision Transformer (LAVT) framework for referring image segmentation, which leverages the multi-stage design of a vision Transformer for jointly encoding multi-modal inputs. Experimental results on three benchmarks have demonstrated its advantage with respect to the state of the art.\n\n*\nEqual contribution. \u2020 Corresponding author. ) A paradigm of previous state-of-the-art methods (b) LAVT (ours)\n\nFigure 2 .\n2Overall pipeline of the proposed LAVT. We leverage a hierarchical vision Transformer[33] to perform language-aware visual encoding. At each stage, visual feature maps Vi, i \u2208 {1, 2, 3, 4} are encoded from the corresponding stage of Transformer layers (which are described in Sec. 3.1 and for diagrammatic clarity, are not illustrated in this figure). Then Vi are used as queries for generating a set of position-specific language feature maps Fi, i \u2208 {1, 2, 3, 4} in the pixel-word attention module (Sec. 3.2). Next, we adaptively fuse Fi with the original Vi via a language pathway (Sec. 3.3). The new visual feature maps Ei, i \u2208 {1, 2, 3} are then passed into the next stage of Transformer layers for further processing. A standard segmentation decoder head (Sec. 3.4) produces the final segmentation output.\n\nFigure 3 .\n3Pipeline of the pixel-word attention module (PWAM).\n\nFigure 4 .\n4The schema of the language pathway, which leverages a language gate (LG) for controlling multi-modal information flow.\n\n\nand annotated with natural language expressions. Each of RefCOCO, RefCOCO+, and G-Ref contains 19,994, 19,992, and 26,711 images, with 50,000, 49,856, and 54,822 annotated objects and 142,209, 141,564, and 104,560 annotated expressions, respectively. Expressions in RefCOCO and RefCOCO+ are very succinct (containing 3.5 words on average). In contrast, expressions in G-Ref are more complex (containing 8.4 words on average), which makes the dataset particularly challenging.\n\nFigure 5 .\n5Visualized predictions and feature maps on an example from the RefCOCO validation set. From top to bottom, the left-most column illustrates the input expression, the input image, and the ground-truth mask overlaid on the input image. In each row, we visualize the predicted mask and the feature maps used for final classification (i.e., Y4, Y3, Y2, and Y1) from left to right. LP represents the language pathway and PWAM represents the pixel-word attention module.P@0.5 P@0.7 P@0.9 oIoU mIoU (a) activation function in the language gate (LG)\n\nFigure 6 .\n6Visualizations of our predicted masks and the ground-truth masks on two examples from the RefCOCO validation set.\n\n\net al. [31] with a recurrent strategy. Shi et al. [48], Chen et al. [4], Ye et al. [60], and Hu et al. [20] model cross-modal relations between language and vision features via various attention mechanisms. Yu et al. [62] and Huang et al. [21] leverage knowledge about sentence structures to capture different concepts (e.g., categories, attributes, relations, etc.) in multi-modal features, while Hui et al. [22] exploit syntactic structures among words for guiding multi-modal context aggregation.\n\nTable 2 shows\n2that removing \nLP (which corresponds to, mathematically, the removal of \n\n\n\nTable 3 (\n3c) shows that using our default choice of F i with LG produces the best overall results among all choices. Also, we observe that while LG has a positive effect when using F i for segmentation, it slightly degrades the results when E i (72.06% vs. 72.27% in oIoU) or V i (71.38% vs. 72.29% in oIoU) are used for segmentation.\nAcknowledgements. This work is supported by the UKRI grant: Turing AI Fellowship EP/W002981/1, EP-SRC/MURI grant: EP/N019474/1, Shanghai Committee of Science and Technology, China (Grant No. 20DZ1100800), and HKU Startup Fund. We would also like to thank the Royal Academy of Engineering, Tencent, and FiveAI.\nVivit: A video vision transformer. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, Cordelia Schmid, ICCV. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vi- sion transformer. In ICCV, 2021. 2\n\nRefvos: A closer look at referring expressions for video object segmentation. Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, Xavier Giro-I Nieto, arXiv:2010.00263Miriam Bellver, Carles Ventura, Carina Silberer, Ioannis Kazakos, Jordi Torres, and Xavier Giro-i Nieto. Refvos: A closer look at referring expressions for video object segmen- tation. arXiv:2010.00263, 2020. 2\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, ECCV. 2020Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In ECCV, 2020. 2\n\nSee-through-text grouping for referring image segmentation. Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu, ICCV. 6Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh Liu. See-through-text grouping for re- ferring image segmentation. In ICCV, 2019. 1, 2, 6\n\nLanguage-based image editing with recurrent attentive models. Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, Xiaodong Liu, CVPR. Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, and Xiaodong Liu. Language-based image editing with recurrent attentive models. In CVPR, 2018. 1\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, arXiv:1706.05587Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587, 2017. 2\n\nReferring expression object segmentation with caption-aware consistency. Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian Wang, Yen-Yu Lin, Ming-Hsuan Yang, BMVC. Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian Wang, Yen-Yu Lin, and Ming-Hsuan Yang. Referring expression object segmen- tation with caption-aware consistency. In BMVC, 2019. 6\n\nImagespirit: Verbal guided image parsing. Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Vibhav Vineet, Paul Sturgess, Nigel Crook, J Niloy, Philip Mitra, Torr, TOG. Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Vibhav Vi- neet, Paul Sturgess, Nigel Crook, Niloy J. Mitra, and Philip Torr. Imagespirit: Verbal guided image parsing. In TOG, 2014. 1\n\nTransformer-XL: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov, ACL. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In ACL, 2019. 2\n\nImageNet: A Large-Scale Hierarchical Image Database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009. 5\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional trans- formers for language understanding. In NAACL, 2019. 2\n\nVision-language transformer and query generation for referring segmentation. Henghui Ding, Chang Liu, Suchen Wang, Xudong Jiang, ICCV. Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for refer- ring segmentation. In ICCV, 2021. 1, 2, 6, 8\n\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2\n\nEncoder fusion network with co-attention embedding for referring image segmentation. Guang Feng, Zhiwei Hu, Lihe Zhang, Huchuan Lu, CVPR. 6Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu. En- coder fusion network with co-attention embedding for refer- ring image segmentation. In CVPR, 2021. 2, 4, 6, 8\n\nDimensionality reduction by learning an invariant mapping. Raia Hadsell, Sumit Chopra, Yann Lecun, CVPR. Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension- ality reduction by learning an invariant mapping. In CVPR, 2006. 2\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, CVPR. 2020Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In CVPR, 2020. 2\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 2Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 1997. 2\n\nSegmentation from natural language expressions. Ronghang Hu, Marcus Rohrbach, Trevor Darrell, ECCV. Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg- mentation from natural language expressions. In ECCV, 2016. 1, 2, 4\n\nUnit: Multimodal multitask learning with a unified transformer. Ronghang Hu, Amanpreet Singh, ICCV. Ronghang Hu and Amanpreet Singh. Unit: Multimodal mul- titask learning with a unified transformer. In ICCV, 2021. 2\n\nBi-directional relationship inferring network for referring image segmentation. Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, Huchuan Lu, CVPR. 7Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, and Huchuan Lu. Bi-directional relationship inferring network for referring image segmentation. In CVPR, 2020. 1, 2, 4, 6, 7, 8\n\nReferring image segmentation via cross-modal progressive comprehension. Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li, CVPR, 2020. 26Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring im- age segmentation via cross-modal progressive comprehen- sion. In CVPR, 2020. 2, 6\n\nLinguistic structure guided context modeling for referring image segmentation. Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, Jizhong Han, ECCV. 2020Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. In ECCV, 2020. 2\n\nLinguistic structure guided context modeling for referring image segmentation. Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, Jizhong Han, ECCV. Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. In ECCV, 2020. 6\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, 2015. 5\n\nLocate then segment: A strong pipeline for referring image segmentation. Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, Tieniu Tan, CVPR, 2021. 6Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tie- niu Tan. Locate then segment: A strong pipeline for referring image segmentation. In CVPR, 2021. 2, 6, 8\n\nIshan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding. Aishwarya Kamath, Mannat Singh, Yann Lecun, Gabriel Synnaeve, ICCV. Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr- modulated detection for end-to-end multi-modal understand- ing. In ICCV, pages 1780-1790, 2021. 2\n\nLess is more: Clipbert for video-and-language learning via sparse sampling. Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, Jingjing Liu, CVPR. 23Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, 2021. 2, 3\n\nReferring image segmentation via recurrent refinement networks. Ruiyu Li, Kai-Can Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia, CVPR. Ruiyu Li, Kai-Can Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image segmen- tation via recurrent refinement networks. In CVPR, 2018. 1, 2, 4\n\nReferring image segmentation via recurrent refinement networks. Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia, CVPR. Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image segmen- tation via recurrent refinement networks. In CVPR, 2018. 6\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars. 15Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuyte- laars, editors, ECCV, 2014. 1, 5\n\nRecurrent multimodal interaction for referring image segmentation. Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Alan Yuille, ICCV. 1Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and Alan Yuille. Recurrent multimodal interaction for referring image segmentation. In ICCV, 2017. 1, 2\n\nCross-modal progressive comprehension for referring segmentation. Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, Guanbin Li, TPAMI, 2021. 46Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, and Guanbin Li. Cross-modal progressive comprehension for referring segmentation. In TPAMI, 2021. 4, 6\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, ICCV. 25Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 2, 3, 5\n\n. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu, arXiv:2106.13230Video swin transformer. 2Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv:2106.13230, 2021. 2\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, CVPR. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 2\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 5\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, NeurIPS. 23Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 2, 3\n\nCascade grouped attention network for referring expression segmentation. Gen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Chia-Wen Lin, Qi Tian, ACMMM, 2020. 4, 6. 7Gen Luo, Yiyi Zhou, Rongrong Ji, Xiaoshuai Sun, Jinsong Su, Chia-Wen Lin, and Qi Tian. Cascade grouped attention network for referring expression segmentation. In ACMMM, 2020. 4, 6, 7, 8\n\nMulti-task collaborative network for joint referring expression comprehension and segmentation. Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, Rongrong Ji, CVPR. 7Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collabora- tive network for joint referring expression comprehension and segmentation. In CVPR, 2020. 2, 6, 7, 8\n\nGeneration and comprehension of unambiguous object descriptions. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, Kevin Murphy, CVPR. 6Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 2, 5, 6\n\nDynamic multimodal instance segmentation guided by natural language queries. Edgar Margffoy-Tuay, C Juan, Emilio P\u00e9rez, Pablo Botero, Arbel\u00e1ez, ECCV. 46Edgar Margffoy-Tuay, Juan C P\u00e9rez, Emilio Botero, and Pablo Arbel\u00e1ez. Dynamic multimodal instance segmenta- tion guided by natural language queries. In ECCV, 2018. 4, 6\n\nModeling context between objects for referring expression understanding. K Varun, Vlad I Nagaraja, Larry S Morariu, Davis, ECCV. 6Varun K. Nagaraja, Vlad I. Morariu, and Larry S. Davis. Modeling context between objects for referring expression understanding. In ECCV, 2016. 2, 5, 6\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, ICML. 45Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010. 4, 5\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, NeurIPS. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 5\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, ICML. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 2\n\nDenseclip: Language-guided dense prediction with contextaware prompting. Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, CVPR. 2022Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context- aware prompting. In CVPR, 2022. 2\n\nJoseph Redmon, Ali Farhadi, arXiv:1804.02767Yolov3: An incremental improvement. Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv:1804.02767, 2018. 2\n\nKey-word-aware network for referring expression image segmentation. Hengcan Shi, Hongliang Li, Fanman Meng, Qingbo Wu, ECCV. 1Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu. Key-word-aware network for referring expression image seg- mentation. In ECCV, 2018. 1, 2\n\nKey-word-aware network for referring expression image segmentation. Hengcan Shi, Hongliang Li, Fanman Meng, Qingbo Wu, In ECCV. 4Hengcan Shi, Hongliang Li, Fanman Meng, and Qingbo Wu. Key-word-aware network for referring expression image seg- mentation. In ECCV, 2018. 4\n\nImproved deep metric learning with multiclass n-pair loss objective. Kihyuk Sohn, NeurIPS. Kihyuk Sohn. Improved deep metric learning with multi- class n-pair loss objective. In NeurIPS, 2016. 2\n\nSegmenter: Transformer for semantic segmentation. Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid, ICCV. Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmenta- tion. In ICCV, 2021. 2\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herve Jegou, ICML. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through at- tention. In ICML, 2021. 2\n\nInstance normalization: The missing ingredient for fast stylization. Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky, arXiv:1607.08022Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In- stance normalization: The missing ingredient for fast styliza- tion. arXiv:1607.08022, 2016. 4\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NeurIPS. 5Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2, 4, 5\n\nReinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang, CVPR. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self- supervised imitation learning for vision-language navigation. In CVPR, 2019. 1\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, EMNLP. Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Mor- gan Funtowicz, Joe Davison, Sam Shleifer, et al. Transform- ers: State-of-the-art natural language processing. In EMNLP, 2020. 5\n\nBottom-up shift and reasoning for referring image segmentation. Sibei Yang, Meng Xia, Guanbin Li, Hong-Yu Zhou, Yizhou Yu, CVPR. Sibei Yang, Meng Xia, Guanbin Li, Hong-Yu Zhou, and Yizhou Yu. Bottom-up shift and reasoning for referring im- age segmentation. In CVPR, 2021. 6\n\nXLNet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, NeurIPS. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: General- ized autoregressive pretraining for language understanding. In NeurIPS, 2019. 2\n\nHierarchical interaction network for video object segmentation from referring expressions. Zhao Yang, Yansong Tang, Luca Bertinetto, Hengshuang Zhao, Philip H S Torr, BMVC. Zhao Yang, Yansong Tang, Luca Bertinetto, Hengshuang Zhao, and Philip H.S. Torr. Hierarchical interaction network for video object segmentation from referring expressions. In BMVC, 2021. 6\n\nCross-modal self-attention network for referring image segmentation. Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, CVPR. 24Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image seg- mentation. In CVPR, 2019. 2, 4\n\nCross-modal self-attention network for referring image segmentation. Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, CVPR. Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for referring image seg- mentation. In CVPR, 2019. 6\n\nMattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, CVPR. 26Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular at- tention network for referring expression comprehension. In CVPR, 2018. 2, 6\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, ECCV. 6Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres- sions. In ECCV, 2016. 2, 5, 6\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, CVPR. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmen- tation from a sequence-to-sequence perspective with trans- formers. In CVPR, 2021. 2\n\nSemantic understanding of scenes through the ade20k dataset. IJCV. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi- dler, Adela Barriuso, and Antonio Torralba. Semantic under- standing of scenes through the ade20k dataset. IJCV, 2019. 1\n\nDeformable DETR: deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, ICLR. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable transformers for end-to-end object detection. In ICLR, 2021. 2\n", "annotations": {"author": "[{\"end\":109,\"start\":76},{\"end\":144,\"start\":110},{\"end\":240,\"start\":145},{\"end\":294,\"start\":241},{\"end\":364,\"start\":295},{\"end\":404,\"start\":365}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":81},{\"end\":120,\"start\":116},{\"end\":157,\"start\":153},{\"end\":249,\"start\":245},{\"end\":310,\"start\":306},{\"end\":380,\"start\":376}]", "author_first_name": "[{\"end\":80,\"start\":76},{\"end\":115,\"start\":110},{\"end\":152,\"start\":145},{\"end\":244,\"start\":241},{\"end\":305,\"start\":295},{\"end\":371,\"start\":365},{\"end\":375,\"start\":372}]", "author_affiliation": "[{\"end\":108,\"start\":87},{\"end\":143,\"start\":122},{\"end\":180,\"start\":159},{\"end\":239,\"start\":182},{\"end\":272,\"start\":251},{\"end\":293,\"start\":274},{\"end\":333,\"start\":312},{\"end\":363,\"start\":335},{\"end\":403,\"start\":382}]", "title": "[{\"end\":73,\"start\":1},{\"end\":477,\"start\":405}]", "venue": null, "abstract": "[{\"end\":1878,\"start\":507}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2051,\"start\":2048},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2054,\"start\":2051},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2154,\"start\":2150},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2176,\"start\":2173},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2287,\"start\":2283},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":2290,\"start\":2287},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2698,\"start\":2694},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3802,\"start\":3798},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3805,\"start\":3802},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3832,\"start\":3829},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3835,\"start\":3832},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3838,\"start\":3835},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3872,\"start\":3868},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3923,\"start\":3919},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3957,\"start\":3953},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4033,\"start\":4029},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4222,\"start\":4218},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4225,\"start\":4222},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4228,\"start\":4225},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4231,\"start\":4228},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5579,\"start\":5575},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6077,\"start\":6073},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":6092,\"start\":6088},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6120,\"start\":6116},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6155,\"start\":6151},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7086,\"start\":7082},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7089,\"start\":7086},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7092,\"start\":7089},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7095,\"start\":7092},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7098,\"start\":7095},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7128,\"start\":7125},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7131,\"start\":7128},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7225,\"start\":7221},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7228,\"start\":7225},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7231,\"start\":7228},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7246,\"start\":7243},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7248,\"start\":7246},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7251,\"start\":7248},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7269,\"start\":7265},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7272,\"start\":7269},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7275,\"start\":7272},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7501,\"start\":7497},{\"end\":7591,\"start\":7588},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7638,\"start\":7634},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7651,\"start\":7647},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7861,\"start\":7857},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7998,\"start\":7994},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8299,\"start\":8295},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8366,\"start\":8363},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8369,\"start\":8366},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8372,\"start\":8369},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8540,\"start\":8536},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8543,\"start\":8540},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8546,\"start\":8543},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8570,\"start\":8567},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8573,\"start\":8570},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8595,\"start\":8592},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8598,\"start\":8595},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8601,\"start\":8598},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8633,\"start\":8629},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8636,\"start\":8633},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8639,\"start\":8636},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8754,\"start\":8750},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8757,\"start\":8754},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8842,\"start\":8838},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8883,\"start\":8879},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8886,\"start\":8883},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8889,\"start\":8886},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8976,\"start\":8972},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9193,\"start\":9189},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9226,\"start\":9222},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11868,\"start\":11864},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12551,\"start\":12547},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12997,\"start\":12993},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13474,\"start\":13470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13495,\"start\":13491},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13498,\"start\":13495},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13501,\"start\":13498},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13530,\"start\":13526},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13533,\"start\":13530},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13536,\"start\":13533},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13539,\"start\":13536},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13542,\"start\":13539},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13570,\"start\":13566},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13646,\"start\":13642},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13649,\"start\":13646},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13652,\"start\":13649},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13655,\"start\":13652},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13658,\"start\":13655},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15290,\"start\":15286},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16020,\"start\":16016},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16895,\"start\":16891},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18199,\"start\":18195},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18213,\"start\":18209},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18378,\"start\":18374},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18454,\"start\":18450},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18563,\"start\":18559},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18594,\"start\":18590},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18684,\"start\":18680},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18926,\"start\":18922},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18951,\"start\":18947},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":19510,\"start\":19506},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":19525,\"start\":19521},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19541,\"start\":19537},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19544,\"start\":19541},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19618,\"start\":19614},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20061,\"start\":20057},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20090,\"start\":20086},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21528,\"start\":21524},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21543,\"start\":21539},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21559,\"start\":21555},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21562,\"start\":21559},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21728,\"start\":21724},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22439,\"start\":22435},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23290,\"start\":23286},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26551,\"start\":26547},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26554,\"start\":26551},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26618,\"start\":26614},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28062,\"start\":28058},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28072,\"start\":28068},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28085,\"start\":28081},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28497,\"start\":28493},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28550,\"start\":28546},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28560,\"start\":28556},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28574,\"start\":28570},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30021,\"start\":30017}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29919,\"start\":29807},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30743,\"start\":29920},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30808,\"start\":30744},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30940,\"start\":30809},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31418,\"start\":30941},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31973,\"start\":31419},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32100,\"start\":31974},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32602,\"start\":32101},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32693,\"start\":32603},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33030,\"start\":32694}]", "paragraph": "[{\"end\":3105,\"start\":1894},{\"end\":3437,\"start\":3107},{\"end\":4232,\"start\":3439},{\"end\":4909,\"start\":4234},{\"end\":5824,\"start\":4911},{\"end\":6275,\"start\":5826},{\"end\":6321,\"start\":6277},{\"end\":6500,\"start\":6323},{\"end\":6704,\"start\":6502},{\"end\":8640,\"start\":6721},{\"end\":9387,\"start\":8642},{\"end\":9923,\"start\":9398},{\"end\":10404,\"start\":9958},{\"end\":11763,\"start\":10406},{\"end\":12343,\"start\":11765},{\"end\":13077,\"start\":12375},{\"end\":14438,\"start\":13084},{\"end\":15749,\"start\":14591},{\"end\":16034,\"start\":15801},{\"end\":16700,\"start\":16055},{\"end\":17675,\"start\":16748},{\"end\":17908,\"start\":17692},{\"end\":18320,\"start\":17969},{\"end\":19398,\"start\":18339},{\"end\":20366,\"start\":19437},{\"end\":21387,\"start\":20368},{\"end\":22571,\"start\":21414},{\"end\":24738,\"start\":22590},{\"end\":26077,\"start\":24740},{\"end\":26156,\"start\":26079},{\"end\":26190,\"start\":26158},{\"end\":26266,\"start\":26192},{\"end\":26796,\"start\":26268},{\"end\":27771,\"start\":26798},{\"end\":28683,\"start\":27773},{\"end\":28913,\"start\":28685},{\"end\":29458,\"start\":28915},{\"end\":29806,\"start\":29473}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13083,\"start\":13078},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14470,\"start\":14439},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14489,\"start\":14470},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14508,\"start\":14489},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14555,\"start\":14508},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14590,\"start\":14555},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15772,\"start\":15750},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15800,\"start\":15772},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16721,\"start\":16701},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16747,\"start\":16721},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17968,\"start\":17909}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":13075,\"start\":13068},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17043,\"start\":17036},{\"end\":20989,\"start\":20982},{\"end\":21424,\"start\":21417},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23311,\"start\":23304},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23877,\"start\":23870},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24224,\"start\":24217},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25792,\"start\":25785},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25978,\"start\":25971},{\"end\":27960,\"start\":27953},{\"end\":28509,\"start\":28502},{\"end\":28818,\"start\":28811},{\"end\":29246,\"start\":29239}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1892,\"start\":1880},{\"attributes\":{\"n\":\"2.\"},\"end\":6719,\"start\":6707},{\"attributes\":{\"n\":\"3.\"},\"end\":9396,\"start\":9390},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9956,\"start\":9926},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12373,\"start\":12346},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16053,\"start\":16037},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17690,\"start\":17678},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18337,\"start\":18323},{\"attributes\":{\"n\":\"4.\"},\"end\":19412,\"start\":19401},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19435,\"start\":19415},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21412,\"start\":21390},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22588,\"start\":22574},{\"attributes\":{\"n\":\"5.\"},\"end\":29471,\"start\":29461},{\"end\":29809,\"start\":29808},{\"end\":29931,\"start\":29921},{\"end\":30755,\"start\":30745},{\"end\":30820,\"start\":30810},{\"end\":31430,\"start\":31420},{\"end\":31985,\"start\":31975},{\"end\":32617,\"start\":32604},{\"end\":32704,\"start\":32695}]", "table": "[{\"end\":32693,\"start\":32619}]", "figure_caption": "[{\"end\":29919,\"start\":29810},{\"end\":30743,\"start\":29933},{\"end\":30808,\"start\":30757},{\"end\":30940,\"start\":30822},{\"end\":31418,\"start\":30943},{\"end\":31973,\"start\":31432},{\"end\":32100,\"start\":31987},{\"end\":32602,\"start\":32103},{\"end\":33030,\"start\":32706}]", "figure_ref": "[{\"end\":2474,\"start\":2466},{\"end\":4068,\"start\":4058},{\"end\":5158,\"start\":5148},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9405,\"start\":9402},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13882,\"start\":13876},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16614,\"start\":16608},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22793,\"start\":22787},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24788,\"start\":24782},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25310,\"start\":25304},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26831,\"start\":26825},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27770,\"start\":27764}]", "bib_author_first_name": "[{\"end\":33382,\"start\":33376},{\"end\":33397,\"start\":33390},{\"end\":33413,\"start\":33408},{\"end\":33427,\"start\":33423},{\"end\":33438,\"start\":33433},{\"end\":33454,\"start\":33446},{\"end\":33699,\"start\":33693},{\"end\":33715,\"start\":33709},{\"end\":33731,\"start\":33725},{\"end\":33749,\"start\":33742},{\"end\":33764,\"start\":33759},{\"end\":33786,\"start\":33773},{\"end\":34075,\"start\":34068},{\"end\":34093,\"start\":34084},{\"end\":34108,\"start\":34101},{\"end\":34126,\"start\":34119},{\"end\":34145,\"start\":34136},{\"end\":34162,\"start\":34156},{\"end\":34428,\"start\":34420},{\"end\":34442,\"start\":34435},{\"end\":34455,\"start\":34448},{\"end\":34471,\"start\":34460},{\"end\":34486,\"start\":34478},{\"end\":34729,\"start\":34723},{\"end\":34742,\"start\":34736},{\"end\":34757,\"start\":34749},{\"end\":34771,\"start\":34763},{\"end\":34785,\"start\":34777},{\"end\":35023,\"start\":35012},{\"end\":35036,\"start\":35030},{\"end\":35056,\"start\":35049},{\"end\":35073,\"start\":35066},{\"end\":35337,\"start\":35331},{\"end\":35352,\"start\":35344},{\"end\":35367,\"start\":35359},{\"end\":35380,\"start\":35374},{\"end\":35396,\"start\":35386},{\"end\":35629,\"start\":35620},{\"end\":35642,\"start\":35637},{\"end\":35657,\"start\":35650},{\"end\":35669,\"start\":35663},{\"end\":35682,\"start\":35678},{\"end\":35698,\"start\":35693},{\"end\":35707,\"start\":35706},{\"end\":35721,\"start\":35715},{\"end\":35999,\"start\":35993},{\"end\":36011,\"start\":36005},{\"end\":36024,\"start\":36018},{\"end\":36036,\"start\":36031},{\"end\":36052,\"start\":36048},{\"end\":36054,\"start\":36053},{\"end\":36065,\"start\":36059},{\"end\":36325,\"start\":36322},{\"end\":36335,\"start\":36332},{\"end\":36349,\"start\":36342},{\"end\":36364,\"start\":36358},{\"end\":36372,\"start\":36369},{\"end\":36379,\"start\":36377},{\"end\":36624,\"start\":36619},{\"end\":36641,\"start\":36633},{\"end\":36655,\"start\":36649},{\"end\":36669,\"start\":36661},{\"end\":36941,\"start\":36934},{\"end\":36953,\"start\":36948},{\"end\":36965,\"start\":36959},{\"end\":36978,\"start\":36972},{\"end\":37286,\"start\":37280},{\"end\":37305,\"start\":37300},{\"end\":37322,\"start\":37313},{\"end\":37339,\"start\":37335},{\"end\":37360,\"start\":37353},{\"end\":37373,\"start\":37367},{\"end\":37394,\"start\":37387},{\"end\":37413,\"start\":37405},{\"end\":37429,\"start\":37424},{\"end\":37840,\"start\":37835},{\"end\":37853,\"start\":37847},{\"end\":37862,\"start\":37858},{\"end\":37877,\"start\":37870},{\"end\":38119,\"start\":38115},{\"end\":38134,\"start\":38129},{\"end\":38147,\"start\":38143},{\"end\":38358,\"start\":38351},{\"end\":38368,\"start\":38363},{\"end\":38379,\"start\":38374},{\"end\":38391,\"start\":38384},{\"end\":38401,\"start\":38397},{\"end\":38602,\"start\":38598},{\"end\":38621,\"start\":38615},{\"end\":38805,\"start\":38797},{\"end\":38816,\"start\":38810},{\"end\":38833,\"start\":38827},{\"end\":39045,\"start\":39037},{\"end\":39059,\"start\":39050},{\"end\":39276,\"start\":39270},{\"end\":39286,\"start\":39281},{\"end\":39298,\"start\":39293},{\"end\":39308,\"start\":39304},{\"end\":39323,\"start\":39316},{\"end\":39589,\"start\":39582},{\"end\":39604,\"start\":39597},{\"end\":39612,\"start\":39610},{\"end\":39625,\"start\":39618},{\"end\":39637,\"start\":39630},{\"end\":39650,\"start\":39643},{\"end\":39661,\"start\":39656},{\"end\":39669,\"start\":39667},{\"end\":39967,\"start\":39960},{\"end\":39975,\"start\":39973},{\"end\":39988,\"start\":39981},{\"end\":40003,\"start\":39996},{\"end\":40013,\"start\":40008},{\"end\":40022,\"start\":40018},{\"end\":40037,\"start\":40030},{\"end\":40323,\"start\":40316},{\"end\":40331,\"start\":40329},{\"end\":40344,\"start\":40337},{\"end\":40359,\"start\":40352},{\"end\":40369,\"start\":40364},{\"end\":40378,\"start\":40374},{\"end\":40393,\"start\":40386},{\"end\":40689,\"start\":40683},{\"end\":40706,\"start\":40697},{\"end\":40947,\"start\":40945},{\"end\":40957,\"start\":40954},{\"end\":40967,\"start\":40964},{\"end\":40979,\"start\":40974},{\"end\":40989,\"start\":40986},{\"end\":41000,\"start\":40994},{\"end\":41292,\"start\":41283},{\"end\":41307,\"start\":41301},{\"end\":41319,\"start\":41315},{\"end\":41334,\"start\":41327},{\"end\":41631,\"start\":41628},{\"end\":41643,\"start\":41637},{\"end\":41654,\"start\":41648},{\"end\":41664,\"start\":41661},{\"end\":41676,\"start\":41670},{\"end\":41678,\"start\":41677},{\"end\":41690,\"start\":41685},{\"end\":41707,\"start\":41699},{\"end\":41977,\"start\":41972},{\"end\":41989,\"start\":41982},{\"end\":42001,\"start\":41994},{\"end\":42015,\"start\":42007},{\"end\":42029,\"start\":42021},{\"end\":42042,\"start\":42034},{\"end\":42054,\"start\":42049},{\"end\":42317,\"start\":42312},{\"end\":42328,\"start\":42322},{\"end\":42340,\"start\":42333},{\"end\":42354,\"start\":42346},{\"end\":42368,\"start\":42360},{\"end\":42381,\"start\":42373},{\"end\":42393,\"start\":42388},{\"end\":42631,\"start\":42623},{\"end\":42644,\"start\":42637},{\"end\":42657,\"start\":42652},{\"end\":42673,\"start\":42668},{\"end\":42686,\"start\":42680},{\"end\":42699,\"start\":42695},{\"end\":42714,\"start\":42709},{\"end\":42724,\"start\":42723},{\"end\":42733,\"start\":42725},{\"end\":43146,\"start\":43140},{\"end\":43155,\"start\":43152},{\"end\":43168,\"start\":43161},{\"end\":43180,\"start\":43175},{\"end\":43190,\"start\":43187},{\"end\":43199,\"start\":43195},{\"end\":43443,\"start\":43441},{\"end\":43456,\"start\":43449},{\"end\":43469,\"start\":43462},{\"end\":43484,\"start\":43477},{\"end\":43492,\"start\":43490},{\"end\":43504,\"start\":43497},{\"end\":43759,\"start\":43757},{\"end\":43771,\"start\":43765},{\"end\":43780,\"start\":43777},{\"end\":43789,\"start\":43786},{\"end\":43800,\"start\":43794},{\"end\":43811,\"start\":43806},{\"end\":43826,\"start\":43819},{\"end\":43839,\"start\":43832},{\"end\":44046,\"start\":44044},{\"end\":44055,\"start\":44052},{\"end\":44065,\"start\":44062},{\"end\":44077,\"start\":44071},{\"end\":44088,\"start\":44083},{\"end\":44103,\"start\":44096},{\"end\":44112,\"start\":44109},{\"end\":44350,\"start\":44342},{\"end\":44361,\"start\":44357},{\"end\":44379,\"start\":44373},{\"end\":44563,\"start\":44559},{\"end\":44581,\"start\":44576},{\"end\":44791,\"start\":44785},{\"end\":44801,\"start\":44796},{\"end\":44813,\"start\":44809},{\"end\":44828,\"start\":44822},{\"end\":45096,\"start\":45093},{\"end\":45106,\"start\":45102},{\"end\":45121,\"start\":45113},{\"end\":45135,\"start\":45126},{\"end\":45148,\"start\":45141},{\"end\":45161,\"start\":45153},{\"end\":45169,\"start\":45167},{\"end\":45483,\"start\":45480},{\"end\":45493,\"start\":45489},{\"end\":45509,\"start\":45500},{\"end\":45522,\"start\":45515},{\"end\":45536,\"start\":45528},{\"end\":45546,\"start\":45541},{\"end\":45561,\"start\":45553},{\"end\":45859,\"start\":45853},{\"end\":45873,\"start\":45865},{\"end\":45890,\"start\":45881},{\"end\":45903,\"start\":45899},{\"end\":45917,\"start\":45913},{\"end\":45919,\"start\":45918},{\"end\":45933,\"start\":45928},{\"end\":46213,\"start\":46208},{\"end\":46230,\"start\":46229},{\"end\":46243,\"start\":46237},{\"end\":46256,\"start\":46251},{\"end\":46527,\"start\":46526},{\"end\":46539,\"start\":46535},{\"end\":46541,\"start\":46540},{\"end\":46557,\"start\":46552},{\"end\":46559,\"start\":46558},{\"end\":46803,\"start\":46798},{\"end\":46818,\"start\":46810},{\"end\":46820,\"start\":46819},{\"end\":47029,\"start\":47025},{\"end\":47041,\"start\":47038},{\"end\":47058,\"start\":47049},{\"end\":47070,\"start\":47066},{\"end\":47083,\"start\":47078},{\"end\":47101,\"start\":47094},{\"end\":47116,\"start\":47110},{\"end\":47132,\"start\":47126},{\"end\":47145,\"start\":47138},{\"end\":47162,\"start\":47158},{\"end\":47499,\"start\":47495},{\"end\":47513,\"start\":47509},{\"end\":47518,\"start\":47514},{\"end\":47529,\"start\":47524},{\"end\":47545,\"start\":47539},{\"end\":47561,\"start\":47554},{\"end\":47575,\"start\":47567},{\"end\":47591,\"start\":47585},{\"end\":47606,\"start\":47600},{\"end\":47621,\"start\":47615},{\"end\":47635,\"start\":47631},{\"end\":47651,\"start\":47643},{\"end\":47665,\"start\":47661},{\"end\":48039,\"start\":48031},{\"end\":48053,\"start\":48045},{\"end\":48067,\"start\":48060},{\"end\":48081,\"start\":48074},{\"end\":48093,\"start\":48088},{\"end\":48103,\"start\":48099},{\"end\":48114,\"start\":48111},{\"end\":48126,\"start\":48121},{\"end\":48344,\"start\":48338},{\"end\":48356,\"start\":48353},{\"end\":48587,\"start\":48580},{\"end\":48602,\"start\":48593},{\"end\":48613,\"start\":48607},{\"end\":48626,\"start\":48620},{\"end\":48859,\"start\":48852},{\"end\":48874,\"start\":48865},{\"end\":48885,\"start\":48879},{\"end\":48898,\"start\":48892},{\"end\":49131,\"start\":49125},{\"end\":49307,\"start\":49302},{\"end\":49324,\"start\":49317},{\"end\":49337,\"start\":49333},{\"end\":49354,\"start\":49346},{\"end\":49585,\"start\":49581},{\"end\":49603,\"start\":49595},{\"end\":49618,\"start\":49610},{\"end\":49635,\"start\":49626},{\"end\":49652,\"start\":49643},{\"end\":49672,\"start\":49667},{\"end\":49961,\"start\":49955},{\"end\":49977,\"start\":49971},{\"end\":49993,\"start\":49987},{\"end\":50208,\"start\":50202},{\"end\":50222,\"start\":50218},{\"end\":50236,\"start\":50232},{\"end\":50250,\"start\":50245},{\"end\":50267,\"start\":50262},{\"end\":50280,\"start\":50275},{\"end\":50282,\"start\":50281},{\"end\":50296,\"start\":50290},{\"end\":50310,\"start\":50305},{\"end\":50620,\"start\":50617},{\"end\":50634,\"start\":50627},{\"end\":50646,\"start\":50642},{\"end\":50668,\"start\":50660},{\"end\":50681,\"start\":50674},{\"end\":50697,\"start\":50688},{\"end\":50711,\"start\":50704},{\"end\":50716,\"start\":50712},{\"end\":50726,\"start\":50723},{\"end\":51049,\"start\":51043},{\"end\":51062,\"start\":51056},{\"end\":51081,\"start\":51073},{\"end\":51095,\"start\":51089},{\"end\":51109,\"start\":51102},{\"end\":51127,\"start\":51120},{\"end\":51140,\"start\":51133},{\"end\":51155,\"start\":51149},{\"end\":51170,\"start\":51167},{\"end\":51183,\"start\":51180},{\"end\":51511,\"start\":51506},{\"end\":51522,\"start\":51518},{\"end\":51535,\"start\":51528},{\"end\":51547,\"start\":51540},{\"end\":51560,\"start\":51554},{\"end\":51798,\"start\":51792},{\"end\":51811,\"start\":51805},{\"end\":51823,\"start\":51817},{\"end\":51835,\"start\":51830},{\"end\":51853,\"start\":51847},{\"end\":51873,\"start\":51869},{\"end\":51875,\"start\":51874},{\"end\":52174,\"start\":52170},{\"end\":52188,\"start\":52181},{\"end\":52199,\"start\":52195},{\"end\":52222,\"start\":52212},{\"end\":52235,\"start\":52229},{\"end\":52239,\"start\":52236},{\"end\":52517,\"start\":52511},{\"end\":52529,\"start\":52522},{\"end\":52541,\"start\":52538},{\"end\":52551,\"start\":52547},{\"end\":52784,\"start\":52778},{\"end\":52796,\"start\":52789},{\"end\":52808,\"start\":52805},{\"end\":52818,\"start\":52814},{\"end\":53053,\"start\":53046},{\"end\":53061,\"start\":53058},{\"end\":53074,\"start\":53067},{\"end\":53086,\"start\":53081},{\"end\":53096,\"start\":53093},{\"end\":53106,\"start\":53101},{\"end\":53121,\"start\":53115},{\"end\":53123,\"start\":53122},{\"end\":53374,\"start\":53367},{\"end\":53386,\"start\":53379},{\"end\":53400,\"start\":53396},{\"end\":53416,\"start\":53407},{\"end\":53418,\"start\":53417},{\"end\":53431,\"start\":53425},{\"end\":53433,\"start\":53432},{\"end\":53691,\"start\":53685},{\"end\":53706,\"start\":53699},{\"end\":53721,\"start\":53711},{\"end\":53735,\"start\":53728},{\"end\":53746,\"start\":53741},{\"end\":53758,\"start\":53752},{\"end\":53771,\"start\":53765},{\"end\":53784,\"start\":53776},{\"end\":53794,\"start\":53791},{\"end\":53803,\"start\":53802},{\"end\":53805,\"start\":53804},{\"end\":54152,\"start\":54147},{\"end\":54163,\"start\":54159},{\"end\":54176,\"start\":54170},{\"end\":54187,\"start\":54183},{\"end\":54199,\"start\":54194},{\"end\":54213,\"start\":54208},{\"end\":54231,\"start\":54224},{\"end\":54501,\"start\":54495},{\"end\":54513,\"start\":54507},{\"end\":54523,\"start\":54518},{\"end\":54531,\"start\":54528},{\"end\":54544,\"start\":54536},{\"end\":54557,\"start\":54551}]", "bib_author_last_name": "[{\"end\":33388,\"start\":33383},{\"end\":33406,\"start\":33398},{\"end\":33421,\"start\":33414},{\"end\":33431,\"start\":33428},{\"end\":33444,\"start\":33439},{\"end\":33461,\"start\":33455},{\"end\":33707,\"start\":33700},{\"end\":33723,\"start\":33716},{\"end\":33740,\"start\":33732},{\"end\":33757,\"start\":33750},{\"end\":33771,\"start\":33765},{\"end\":33792,\"start\":33787},{\"end\":34082,\"start\":34076},{\"end\":34099,\"start\":34094},{\"end\":34117,\"start\":34109},{\"end\":34134,\"start\":34127},{\"end\":34154,\"start\":34146},{\"end\":34172,\"start\":34163},{\"end\":34433,\"start\":34429},{\"end\":34446,\"start\":34443},{\"end\":34458,\"start\":34456},{\"end\":34476,\"start\":34472},{\"end\":34490,\"start\":34487},{\"end\":34734,\"start\":34730},{\"end\":34747,\"start\":34743},{\"end\":34761,\"start\":34758},{\"end\":34775,\"start\":34772},{\"end\":34789,\"start\":34786},{\"end\":35028,\"start\":35024},{\"end\":35047,\"start\":35037},{\"end\":35064,\"start\":35057},{\"end\":35078,\"start\":35074},{\"end\":35342,\"start\":35338},{\"end\":35357,\"start\":35353},{\"end\":35372,\"start\":35368},{\"end\":35384,\"start\":35381},{\"end\":35401,\"start\":35397},{\"end\":35635,\"start\":35630},{\"end\":35648,\"start\":35643},{\"end\":35661,\"start\":35658},{\"end\":35676,\"start\":35670},{\"end\":35691,\"start\":35683},{\"end\":35704,\"start\":35699},{\"end\":35713,\"start\":35708},{\"end\":35727,\"start\":35722},{\"end\":35733,\"start\":35729},{\"end\":36003,\"start\":36000},{\"end\":36016,\"start\":36012},{\"end\":36029,\"start\":36025},{\"end\":36046,\"start\":36037},{\"end\":36057,\"start\":36055},{\"end\":36079,\"start\":36066},{\"end\":36330,\"start\":36326},{\"end\":36340,\"start\":36336},{\"end\":36356,\"start\":36350},{\"end\":36367,\"start\":36365},{\"end\":36375,\"start\":36373},{\"end\":36387,\"start\":36380},{\"end\":36631,\"start\":36625},{\"end\":36647,\"start\":36642},{\"end\":36659,\"start\":36656},{\"end\":36679,\"start\":36670},{\"end\":36946,\"start\":36942},{\"end\":36957,\"start\":36954},{\"end\":36970,\"start\":36966},{\"end\":36984,\"start\":36979},{\"end\":37298,\"start\":37287},{\"end\":37311,\"start\":37306},{\"end\":37333,\"start\":37323},{\"end\":37351,\"start\":37340},{\"end\":37365,\"start\":37361},{\"end\":37385,\"start\":37374},{\"end\":37403,\"start\":37395},{\"end\":37422,\"start\":37414},{\"end\":37437,\"start\":37430},{\"end\":37845,\"start\":37841},{\"end\":37856,\"start\":37854},{\"end\":37868,\"start\":37863},{\"end\":37880,\"start\":37878},{\"end\":38127,\"start\":38120},{\"end\":38141,\"start\":38135},{\"end\":38153,\"start\":38148},{\"end\":38361,\"start\":38359},{\"end\":38372,\"start\":38369},{\"end\":38382,\"start\":38380},{\"end\":38395,\"start\":38392},{\"end\":38410,\"start\":38402},{\"end\":38613,\"start\":38603},{\"end\":38633,\"start\":38622},{\"end\":38808,\"start\":38806},{\"end\":38825,\"start\":38817},{\"end\":38841,\"start\":38834},{\"end\":39048,\"start\":39046},{\"end\":39065,\"start\":39060},{\"end\":39279,\"start\":39277},{\"end\":39291,\"start\":39287},{\"end\":39302,\"start\":39299},{\"end\":39314,\"start\":39309},{\"end\":39326,\"start\":39324},{\"end\":39595,\"start\":39590},{\"end\":39608,\"start\":39605},{\"end\":39616,\"start\":39613},{\"end\":39628,\"start\":39626},{\"end\":39641,\"start\":39638},{\"end\":39654,\"start\":39651},{\"end\":39665,\"start\":39662},{\"end\":39672,\"start\":39670},{\"end\":39971,\"start\":39968},{\"end\":39979,\"start\":39976},{\"end\":39994,\"start\":39989},{\"end\":40006,\"start\":40004},{\"end\":40016,\"start\":40014},{\"end\":40028,\"start\":40023},{\"end\":40041,\"start\":40038},{\"end\":40327,\"start\":40324},{\"end\":40335,\"start\":40332},{\"end\":40350,\"start\":40345},{\"end\":40362,\"start\":40360},{\"end\":40372,\"start\":40370},{\"end\":40384,\"start\":40379},{\"end\":40397,\"start\":40394},{\"end\":40695,\"start\":40690},{\"end\":40714,\"start\":40707},{\"end\":40952,\"start\":40948},{\"end\":40962,\"start\":40958},{\"end\":40972,\"start\":40968},{\"end\":40984,\"start\":40980},{\"end\":40992,\"start\":40990},{\"end\":41004,\"start\":41001},{\"end\":41299,\"start\":41293},{\"end\":41313,\"start\":41308},{\"end\":41325,\"start\":41320},{\"end\":41343,\"start\":41335},{\"end\":41635,\"start\":41632},{\"end\":41646,\"start\":41644},{\"end\":41659,\"start\":41655},{\"end\":41668,\"start\":41665},{\"end\":41683,\"start\":41679},{\"end\":41697,\"start\":41691},{\"end\":41711,\"start\":41708},{\"end\":41980,\"start\":41978},{\"end\":41992,\"start\":41990},{\"end\":42005,\"start\":42002},{\"end\":42019,\"start\":42016},{\"end\":42032,\"start\":42030},{\"end\":42047,\"start\":42043},{\"end\":42058,\"start\":42055},{\"end\":42320,\"start\":42318},{\"end\":42331,\"start\":42329},{\"end\":42344,\"start\":42341},{\"end\":42358,\"start\":42355},{\"end\":42371,\"start\":42369},{\"end\":42386,\"start\":42382},{\"end\":42397,\"start\":42394},{\"end\":42635,\"start\":42632},{\"end\":42650,\"start\":42645},{\"end\":42666,\"start\":42658},{\"end\":42678,\"start\":42674},{\"end\":42693,\"start\":42687},{\"end\":42707,\"start\":42700},{\"end\":42721,\"start\":42715},{\"end\":42741,\"start\":42734},{\"end\":43150,\"start\":43147},{\"end\":43159,\"start\":43156},{\"end\":43173,\"start\":43169},{\"end\":43185,\"start\":43181},{\"end\":43193,\"start\":43191},{\"end\":43206,\"start\":43200},{\"end\":43447,\"start\":43444},{\"end\":43460,\"start\":43457},{\"end\":43475,\"start\":43470},{\"end\":43488,\"start\":43485},{\"end\":43495,\"start\":43493},{\"end\":43507,\"start\":43505},{\"end\":43763,\"start\":43760},{\"end\":43775,\"start\":43772},{\"end\":43784,\"start\":43781},{\"end\":43792,\"start\":43790},{\"end\":43804,\"start\":43801},{\"end\":43817,\"start\":43812},{\"end\":43830,\"start\":43827},{\"end\":43843,\"start\":43840},{\"end\":44050,\"start\":44047},{\"end\":44060,\"start\":44056},{\"end\":44069,\"start\":44066},{\"end\":44081,\"start\":44078},{\"end\":44094,\"start\":44089},{\"end\":44107,\"start\":44104},{\"end\":44115,\"start\":44113},{\"end\":44355,\"start\":44351},{\"end\":44371,\"start\":44362},{\"end\":44387,\"start\":44380},{\"end\":44574,\"start\":44564},{\"end\":44588,\"start\":44582},{\"end\":44794,\"start\":44792},{\"end\":44807,\"start\":44802},{\"end\":44820,\"start\":44814},{\"end\":44832,\"start\":44829},{\"end\":45100,\"start\":45097},{\"end\":45111,\"start\":45107},{\"end\":45124,\"start\":45122},{\"end\":45139,\"start\":45136},{\"end\":45151,\"start\":45149},{\"end\":45165,\"start\":45162},{\"end\":45174,\"start\":45170},{\"end\":45487,\"start\":45484},{\"end\":45498,\"start\":45494},{\"end\":45513,\"start\":45510},{\"end\":45526,\"start\":45523},{\"end\":45539,\"start\":45537},{\"end\":45551,\"start\":45547},{\"end\":45564,\"start\":45562},{\"end\":45863,\"start\":45860},{\"end\":45879,\"start\":45874},{\"end\":45897,\"start\":45891},{\"end\":45911,\"start\":45904},{\"end\":45926,\"start\":45920},{\"end\":45940,\"start\":45934},{\"end\":46227,\"start\":46214},{\"end\":46235,\"start\":46231},{\"end\":46249,\"start\":46244},{\"end\":46263,\"start\":46257},{\"end\":46273,\"start\":46265},{\"end\":46533,\"start\":46528},{\"end\":46550,\"start\":46542},{\"end\":46567,\"start\":46560},{\"end\":46574,\"start\":46569},{\"end\":46808,\"start\":46804},{\"end\":46827,\"start\":46821},{\"end\":47036,\"start\":47030},{\"end\":47047,\"start\":47042},{\"end\":47064,\"start\":47059},{\"end\":47076,\"start\":47071},{\"end\":47092,\"start\":47084},{\"end\":47108,\"start\":47102},{\"end\":47124,\"start\":47117},{\"end\":47136,\"start\":47133},{\"end\":47156,\"start\":47146},{\"end\":47169,\"start\":47163},{\"end\":47507,\"start\":47500},{\"end\":47522,\"start\":47519},{\"end\":47537,\"start\":47530},{\"end\":47552,\"start\":47546},{\"end\":47565,\"start\":47562},{\"end\":47583,\"start\":47576},{\"end\":47598,\"start\":47592},{\"end\":47613,\"start\":47607},{\"end\":47629,\"start\":47622},{\"end\":47641,\"start\":47636},{\"end\":47659,\"start\":47652},{\"end\":47675,\"start\":47666},{\"end\":48043,\"start\":48040},{\"end\":48058,\"start\":48054},{\"end\":48072,\"start\":48068},{\"end\":48086,\"start\":48082},{\"end\":48097,\"start\":48094},{\"end\":48109,\"start\":48104},{\"end\":48119,\"start\":48115},{\"end\":48129,\"start\":48127},{\"end\":48351,\"start\":48345},{\"end\":48364,\"start\":48357},{\"end\":48591,\"start\":48588},{\"end\":48605,\"start\":48603},{\"end\":48618,\"start\":48614},{\"end\":48629,\"start\":48627},{\"end\":48863,\"start\":48860},{\"end\":48877,\"start\":48875},{\"end\":48890,\"start\":48886},{\"end\":48901,\"start\":48899},{\"end\":49136,\"start\":49132},{\"end\":49315,\"start\":49308},{\"end\":49331,\"start\":49325},{\"end\":49344,\"start\":49338},{\"end\":49361,\"start\":49355},{\"end\":49593,\"start\":49586},{\"end\":49608,\"start\":49604},{\"end\":49624,\"start\":49619},{\"end\":49641,\"start\":49636},{\"end\":49665,\"start\":49653},{\"end\":49678,\"start\":49673},{\"end\":49969,\"start\":49962},{\"end\":49985,\"start\":49978},{\"end\":50003,\"start\":49994},{\"end\":50216,\"start\":50209},{\"end\":50230,\"start\":50223},{\"end\":50243,\"start\":50237},{\"end\":50260,\"start\":50251},{\"end\":50273,\"start\":50268},{\"end\":50288,\"start\":50283},{\"end\":50303,\"start\":50297},{\"end\":50321,\"start\":50311},{\"end\":50625,\"start\":50621},{\"end\":50640,\"start\":50635},{\"end\":50658,\"start\":50647},{\"end\":50672,\"start\":50669},{\"end\":50686,\"start\":50682},{\"end\":50702,\"start\":50698},{\"end\":50721,\"start\":50717},{\"end\":50732,\"start\":50727},{\"end\":51054,\"start\":51050},{\"end\":51071,\"start\":51063},{\"end\":51087,\"start\":51082},{\"end\":51100,\"start\":51096},{\"end\":51118,\"start\":51110},{\"end\":51131,\"start\":51128},{\"end\":51147,\"start\":51141},{\"end\":51165,\"start\":51156},{\"end\":51178,\"start\":51171},{\"end\":51192,\"start\":51184},{\"end\":51516,\"start\":51512},{\"end\":51526,\"start\":51523},{\"end\":51538,\"start\":51536},{\"end\":51552,\"start\":51548},{\"end\":51563,\"start\":51561},{\"end\":51803,\"start\":51799},{\"end\":51815,\"start\":51812},{\"end\":51828,\"start\":51824},{\"end\":51845,\"start\":51836},{\"end\":51867,\"start\":51854},{\"end\":51878,\"start\":51876},{\"end\":52179,\"start\":52175},{\"end\":52193,\"start\":52189},{\"end\":52210,\"start\":52200},{\"end\":52227,\"start\":52223},{\"end\":52244,\"start\":52240},{\"end\":52520,\"start\":52518},{\"end\":52536,\"start\":52530},{\"end\":52545,\"start\":52542},{\"end\":52556,\"start\":52552},{\"end\":52787,\"start\":52785},{\"end\":52803,\"start\":52797},{\"end\":52812,\"start\":52809},{\"end\":52823,\"start\":52819},{\"end\":53056,\"start\":53054},{\"end\":53065,\"start\":53062},{\"end\":53079,\"start\":53075},{\"end\":53091,\"start\":53087},{\"end\":53099,\"start\":53097},{\"end\":53113,\"start\":53107},{\"end\":53128,\"start\":53124},{\"end\":53377,\"start\":53375},{\"end\":53394,\"start\":53387},{\"end\":53405,\"start\":53401},{\"end\":53423,\"start\":53419},{\"end\":53438,\"start\":53434},{\"end\":53697,\"start\":53692},{\"end\":53709,\"start\":53707},{\"end\":53726,\"start\":53722},{\"end\":53739,\"start\":53736},{\"end\":53750,\"start\":53747},{\"end\":53763,\"start\":53759},{\"end\":53774,\"start\":53772},{\"end\":53789,\"start\":53785},{\"end\":53800,\"start\":53795},{\"end\":53812,\"start\":53806},{\"end\":53818,\"start\":53814},{\"end\":54157,\"start\":54153},{\"end\":54168,\"start\":54164},{\"end\":54181,\"start\":54177},{\"end\":54192,\"start\":54188},{\"end\":54206,\"start\":54200},{\"end\":54222,\"start\":54214},{\"end\":54240,\"start\":54232},{\"end\":54505,\"start\":54502},{\"end\":54516,\"start\":54514},{\"end\":54526,\"start\":54524},{\"end\":54534,\"start\":54532},{\"end\":54549,\"start\":54545},{\"end\":54561,\"start\":54558}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232417054},\"end\":33613,\"start\":33341},{\"attributes\":{\"doi\":\"arXiv:2010.00263\",\"id\":\"b1\"},\"end\":34020,\"start\":33615},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218889832},\"end\":34358,\"start\":34022},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207979929},\"end\":34659,\"start\":34360},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":28316672},\"end\":34947,\"start\":34661},{\"attributes\":{\"doi\":\"arXiv:1706.05587\",\"id\":\"b5\"},\"end\":35256,\"start\":34949},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":204008128},\"end\":35576,\"start\":35258},{\"attributes\":{\"id\":\"b7\"},\"end\":35918,\"start\":35578},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57759363},\"end\":36267,\"start\":35920},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":36535,\"start\":36269},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":36855,\"start\":36537},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":236987044},\"end\":37152,\"start\":36857},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225039882},\"end\":37748,\"start\":37154},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":233740054},\"end\":38054,\"start\":37750},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8281592},\"end\":38282,\"start\":38056},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207930212},\"end\":38572,\"start\":38284},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1915014},\"end\":38747,\"start\":38574},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1931511},\"end\":38971,\"start\":38749},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":237204499},\"end\":39188,\"start\":38973},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":219616239},\"end\":39508,\"start\":39190},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219622248},\"end\":39879,\"start\":39510},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":222090388},\"end\":40235,\"start\":39881},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":222090388},\"end\":40587,\"start\":40237},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5808102},\"end\":40870,\"start\":40589},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":232417372},\"end\":41181,\"start\":40872},{\"attributes\":{\"id\":\"b25\"},\"end\":41550,\"start\":41183},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":231880022},\"end\":41906,\"start\":41552},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52831465},\"end\":42246,\"start\":41908},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52831465},\"end\":42578,\"start\":42248},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14113767},\"end\":43071,\"start\":42580},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3518818},\"end\":43373,\"start\":43073},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":234496945},\"end\":43682,\"start\":43375},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":232352874},\"end\":44040,\"start\":43684},{\"attributes\":{\"doi\":\"arXiv:2106.13230\",\"id\":\"b33\"},\"end\":44284,\"start\":44042},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1629541},\"end\":44518,\"start\":44286},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53592270},\"end\":44685,\"start\":44520},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":199453025},\"end\":45018,\"start\":44687},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":222278160},\"end\":45382,\"start\":45020},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":213176225},\"end\":45786,\"start\":45384},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8745888},\"end\":46129,\"start\":45788},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":49656155},\"end\":46451,\"start\":46131},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2893400},\"end\":46734,\"start\":46453},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15539264},\"end\":46953,\"start\":46736},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":202786778},\"end\":47422,\"start\":46955},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":231591445},\"end\":47956,\"start\":47424},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":244800733},\"end\":48336,\"start\":47958},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b46\"},\"end\":48510,\"start\":48338},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":52955377},\"end\":48782,\"start\":48512},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":52955377},\"end\":49054,\"start\":48784},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":911406},\"end\":49250,\"start\":49056},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":234470051},\"end\":49502,\"start\":49252},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":229363322},\"end\":49884,\"start\":49504},{\"attributes\":{\"doi\":\"arXiv:1607.08022\",\"id\":\"b52\"},\"end\":50173,\"start\":49886},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":13756489},\"end\":50513,\"start\":50175},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":53735892},\"end\":50981,\"start\":50515},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":265995886},\"end\":51440,\"start\":50983},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":235702944},\"end\":51716,\"start\":51442},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":195069387},\"end\":52077,\"start\":51718},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":249892372},\"end\":52440,\"start\":52079},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":104292134},\"end\":52707,\"start\":52442},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":104292134},\"end\":52969,\"start\":52709},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":3441497},\"end\":53322,\"start\":52971},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":1688357},\"end\":53591,\"start\":53324},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":229924195},\"end\":54078,\"start\":53593},{\"attributes\":{\"id\":\"b64\"},\"end\":54419,\"start\":54080},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":222208633},\"end\":54731,\"start\":54421}]", "bib_title": "[{\"end\":33374,\"start\":33341},{\"end\":34066,\"start\":34022},{\"end\":34418,\"start\":34360},{\"end\":34721,\"start\":34661},{\"end\":35329,\"start\":35258},{\"end\":35618,\"start\":35578},{\"end\":35991,\"start\":35920},{\"end\":36320,\"start\":36269},{\"end\":36617,\"start\":36537},{\"end\":36932,\"start\":36857},{\"end\":37278,\"start\":37154},{\"end\":37833,\"start\":37750},{\"end\":38113,\"start\":38056},{\"end\":38349,\"start\":38284},{\"end\":38596,\"start\":38574},{\"end\":38795,\"start\":38749},{\"end\":39035,\"start\":38973},{\"end\":39268,\"start\":39190},{\"end\":39580,\"start\":39510},{\"end\":39958,\"start\":39881},{\"end\":40314,\"start\":40237},{\"end\":40681,\"start\":40589},{\"end\":40943,\"start\":40872},{\"end\":41281,\"start\":41183},{\"end\":41626,\"start\":41552},{\"end\":41970,\"start\":41908},{\"end\":42310,\"start\":42248},{\"end\":42621,\"start\":42580},{\"end\":43138,\"start\":43073},{\"end\":43439,\"start\":43375},{\"end\":43755,\"start\":43684},{\"end\":44340,\"start\":44286},{\"end\":44557,\"start\":44520},{\"end\":44783,\"start\":44687},{\"end\":45091,\"start\":45020},{\"end\":45478,\"start\":45384},{\"end\":45851,\"start\":45788},{\"end\":46206,\"start\":46131},{\"end\":46524,\"start\":46453},{\"end\":46796,\"start\":46736},{\"end\":47023,\"start\":46955},{\"end\":47493,\"start\":47424},{\"end\":48029,\"start\":47958},{\"end\":48578,\"start\":48512},{\"end\":48850,\"start\":48784},{\"end\":49123,\"start\":49056},{\"end\":49300,\"start\":49252},{\"end\":49579,\"start\":49504},{\"end\":50200,\"start\":50175},{\"end\":50615,\"start\":50515},{\"end\":51041,\"start\":50983},{\"end\":51504,\"start\":51442},{\"end\":51790,\"start\":51718},{\"end\":52168,\"start\":52079},{\"end\":52509,\"start\":52442},{\"end\":52776,\"start\":52709},{\"end\":53044,\"start\":52971},{\"end\":53365,\"start\":53324},{\"end\":53683,\"start\":53593},{\"end\":54493,\"start\":54421}]", "bib_author": "[{\"end\":33390,\"start\":33376},{\"end\":33408,\"start\":33390},{\"end\":33423,\"start\":33408},{\"end\":33433,\"start\":33423},{\"end\":33446,\"start\":33433},{\"end\":33463,\"start\":33446},{\"end\":33709,\"start\":33693},{\"end\":33725,\"start\":33709},{\"end\":33742,\"start\":33725},{\"end\":33759,\"start\":33742},{\"end\":33773,\"start\":33759},{\"end\":33794,\"start\":33773},{\"end\":34084,\"start\":34068},{\"end\":34101,\"start\":34084},{\"end\":34119,\"start\":34101},{\"end\":34136,\"start\":34119},{\"end\":34156,\"start\":34136},{\"end\":34174,\"start\":34156},{\"end\":34435,\"start\":34420},{\"end\":34448,\"start\":34435},{\"end\":34460,\"start\":34448},{\"end\":34478,\"start\":34460},{\"end\":34492,\"start\":34478},{\"end\":34736,\"start\":34723},{\"end\":34749,\"start\":34736},{\"end\":34763,\"start\":34749},{\"end\":34777,\"start\":34763},{\"end\":34791,\"start\":34777},{\"end\":35030,\"start\":35012},{\"end\":35049,\"start\":35030},{\"end\":35066,\"start\":35049},{\"end\":35080,\"start\":35066},{\"end\":35344,\"start\":35331},{\"end\":35359,\"start\":35344},{\"end\":35374,\"start\":35359},{\"end\":35386,\"start\":35374},{\"end\":35403,\"start\":35386},{\"end\":35637,\"start\":35620},{\"end\":35650,\"start\":35637},{\"end\":35663,\"start\":35650},{\"end\":35678,\"start\":35663},{\"end\":35693,\"start\":35678},{\"end\":35706,\"start\":35693},{\"end\":35715,\"start\":35706},{\"end\":35729,\"start\":35715},{\"end\":35735,\"start\":35729},{\"end\":36005,\"start\":35993},{\"end\":36018,\"start\":36005},{\"end\":36031,\"start\":36018},{\"end\":36048,\"start\":36031},{\"end\":36059,\"start\":36048},{\"end\":36081,\"start\":36059},{\"end\":36332,\"start\":36322},{\"end\":36342,\"start\":36332},{\"end\":36358,\"start\":36342},{\"end\":36369,\"start\":36358},{\"end\":36377,\"start\":36369},{\"end\":36389,\"start\":36377},{\"end\":36633,\"start\":36619},{\"end\":36649,\"start\":36633},{\"end\":36661,\"start\":36649},{\"end\":36681,\"start\":36661},{\"end\":36948,\"start\":36934},{\"end\":36959,\"start\":36948},{\"end\":36972,\"start\":36959},{\"end\":36986,\"start\":36972},{\"end\":37300,\"start\":37280},{\"end\":37313,\"start\":37300},{\"end\":37335,\"start\":37313},{\"end\":37353,\"start\":37335},{\"end\":37367,\"start\":37353},{\"end\":37387,\"start\":37367},{\"end\":37405,\"start\":37387},{\"end\":37424,\"start\":37405},{\"end\":37439,\"start\":37424},{\"end\":37847,\"start\":37835},{\"end\":37858,\"start\":37847},{\"end\":37870,\"start\":37858},{\"end\":37882,\"start\":37870},{\"end\":38129,\"start\":38115},{\"end\":38143,\"start\":38129},{\"end\":38155,\"start\":38143},{\"end\":38363,\"start\":38351},{\"end\":38374,\"start\":38363},{\"end\":38384,\"start\":38374},{\"end\":38397,\"start\":38384},{\"end\":38412,\"start\":38397},{\"end\":38615,\"start\":38598},{\"end\":38635,\"start\":38615},{\"end\":38810,\"start\":38797},{\"end\":38827,\"start\":38810},{\"end\":38843,\"start\":38827},{\"end\":39050,\"start\":39037},{\"end\":39067,\"start\":39050},{\"end\":39281,\"start\":39270},{\"end\":39293,\"start\":39281},{\"end\":39304,\"start\":39293},{\"end\":39316,\"start\":39304},{\"end\":39328,\"start\":39316},{\"end\":39597,\"start\":39582},{\"end\":39610,\"start\":39597},{\"end\":39618,\"start\":39610},{\"end\":39630,\"start\":39618},{\"end\":39643,\"start\":39630},{\"end\":39656,\"start\":39643},{\"end\":39667,\"start\":39656},{\"end\":39674,\"start\":39667},{\"end\":39973,\"start\":39960},{\"end\":39981,\"start\":39973},{\"end\":39996,\"start\":39981},{\"end\":40008,\"start\":39996},{\"end\":40018,\"start\":40008},{\"end\":40030,\"start\":40018},{\"end\":40043,\"start\":40030},{\"end\":40329,\"start\":40316},{\"end\":40337,\"start\":40329},{\"end\":40352,\"start\":40337},{\"end\":40364,\"start\":40352},{\"end\":40374,\"start\":40364},{\"end\":40386,\"start\":40374},{\"end\":40399,\"start\":40386},{\"end\":40697,\"start\":40683},{\"end\":40716,\"start\":40697},{\"end\":40954,\"start\":40945},{\"end\":40964,\"start\":40954},{\"end\":40974,\"start\":40964},{\"end\":40986,\"start\":40974},{\"end\":40994,\"start\":40986},{\"end\":41006,\"start\":40994},{\"end\":41301,\"start\":41283},{\"end\":41315,\"start\":41301},{\"end\":41327,\"start\":41315},{\"end\":41345,\"start\":41327},{\"end\":41637,\"start\":41628},{\"end\":41648,\"start\":41637},{\"end\":41661,\"start\":41648},{\"end\":41670,\"start\":41661},{\"end\":41685,\"start\":41670},{\"end\":41699,\"start\":41685},{\"end\":41713,\"start\":41699},{\"end\":41982,\"start\":41972},{\"end\":41994,\"start\":41982},{\"end\":42007,\"start\":41994},{\"end\":42021,\"start\":42007},{\"end\":42034,\"start\":42021},{\"end\":42049,\"start\":42034},{\"end\":42060,\"start\":42049},{\"end\":42322,\"start\":42312},{\"end\":42333,\"start\":42322},{\"end\":42346,\"start\":42333},{\"end\":42360,\"start\":42346},{\"end\":42373,\"start\":42360},{\"end\":42388,\"start\":42373},{\"end\":42399,\"start\":42388},{\"end\":42637,\"start\":42623},{\"end\":42652,\"start\":42637},{\"end\":42668,\"start\":42652},{\"end\":42680,\"start\":42668},{\"end\":42695,\"start\":42680},{\"end\":42709,\"start\":42695},{\"end\":42723,\"start\":42709},{\"end\":42743,\"start\":42723},{\"end\":43152,\"start\":43140},{\"end\":43161,\"start\":43152},{\"end\":43175,\"start\":43161},{\"end\":43187,\"start\":43175},{\"end\":43195,\"start\":43187},{\"end\":43208,\"start\":43195},{\"end\":43449,\"start\":43441},{\"end\":43462,\"start\":43449},{\"end\":43477,\"start\":43462},{\"end\":43490,\"start\":43477},{\"end\":43497,\"start\":43490},{\"end\":43509,\"start\":43497},{\"end\":43765,\"start\":43757},{\"end\":43777,\"start\":43765},{\"end\":43786,\"start\":43777},{\"end\":43794,\"start\":43786},{\"end\":43806,\"start\":43794},{\"end\":43819,\"start\":43806},{\"end\":43832,\"start\":43819},{\"end\":43845,\"start\":43832},{\"end\":44052,\"start\":44044},{\"end\":44062,\"start\":44052},{\"end\":44071,\"start\":44062},{\"end\":44083,\"start\":44071},{\"end\":44096,\"start\":44083},{\"end\":44109,\"start\":44096},{\"end\":44117,\"start\":44109},{\"end\":44357,\"start\":44342},{\"end\":44373,\"start\":44357},{\"end\":44389,\"start\":44373},{\"end\":44576,\"start\":44559},{\"end\":44590,\"start\":44576},{\"end\":44796,\"start\":44785},{\"end\":44809,\"start\":44796},{\"end\":44822,\"start\":44809},{\"end\":44834,\"start\":44822},{\"end\":45102,\"start\":45093},{\"end\":45113,\"start\":45102},{\"end\":45126,\"start\":45113},{\"end\":45141,\"start\":45126},{\"end\":45153,\"start\":45141},{\"end\":45167,\"start\":45153},{\"end\":45176,\"start\":45167},{\"end\":45489,\"start\":45480},{\"end\":45500,\"start\":45489},{\"end\":45515,\"start\":45500},{\"end\":45528,\"start\":45515},{\"end\":45541,\"start\":45528},{\"end\":45553,\"start\":45541},{\"end\":45566,\"start\":45553},{\"end\":45865,\"start\":45853},{\"end\":45881,\"start\":45865},{\"end\":45899,\"start\":45881},{\"end\":45913,\"start\":45899},{\"end\":45928,\"start\":45913},{\"end\":45942,\"start\":45928},{\"end\":46229,\"start\":46208},{\"end\":46237,\"start\":46229},{\"end\":46251,\"start\":46237},{\"end\":46265,\"start\":46251},{\"end\":46275,\"start\":46265},{\"end\":46535,\"start\":46526},{\"end\":46552,\"start\":46535},{\"end\":46569,\"start\":46552},{\"end\":46576,\"start\":46569},{\"end\":46810,\"start\":46798},{\"end\":46829,\"start\":46810},{\"end\":47038,\"start\":47025},{\"end\":47049,\"start\":47038},{\"end\":47066,\"start\":47049},{\"end\":47078,\"start\":47066},{\"end\":47094,\"start\":47078},{\"end\":47110,\"start\":47094},{\"end\":47126,\"start\":47110},{\"end\":47138,\"start\":47126},{\"end\":47158,\"start\":47138},{\"end\":47171,\"start\":47158},{\"end\":47509,\"start\":47495},{\"end\":47524,\"start\":47509},{\"end\":47539,\"start\":47524},{\"end\":47554,\"start\":47539},{\"end\":47567,\"start\":47554},{\"end\":47585,\"start\":47567},{\"end\":47600,\"start\":47585},{\"end\":47615,\"start\":47600},{\"end\":47631,\"start\":47615},{\"end\":47643,\"start\":47631},{\"end\":47661,\"start\":47643},{\"end\":47677,\"start\":47661},{\"end\":48045,\"start\":48031},{\"end\":48060,\"start\":48045},{\"end\":48074,\"start\":48060},{\"end\":48088,\"start\":48074},{\"end\":48099,\"start\":48088},{\"end\":48111,\"start\":48099},{\"end\":48121,\"start\":48111},{\"end\":48131,\"start\":48121},{\"end\":48353,\"start\":48338},{\"end\":48366,\"start\":48353},{\"end\":48593,\"start\":48580},{\"end\":48607,\"start\":48593},{\"end\":48620,\"start\":48607},{\"end\":48631,\"start\":48620},{\"end\":48865,\"start\":48852},{\"end\":48879,\"start\":48865},{\"end\":48892,\"start\":48879},{\"end\":48903,\"start\":48892},{\"end\":49138,\"start\":49125},{\"end\":49317,\"start\":49302},{\"end\":49333,\"start\":49317},{\"end\":49346,\"start\":49333},{\"end\":49363,\"start\":49346},{\"end\":49595,\"start\":49581},{\"end\":49610,\"start\":49595},{\"end\":49626,\"start\":49610},{\"end\":49643,\"start\":49626},{\"end\":49667,\"start\":49643},{\"end\":49680,\"start\":49667},{\"end\":49971,\"start\":49955},{\"end\":49987,\"start\":49971},{\"end\":50005,\"start\":49987},{\"end\":50218,\"start\":50202},{\"end\":50232,\"start\":50218},{\"end\":50245,\"start\":50232},{\"end\":50262,\"start\":50245},{\"end\":50275,\"start\":50262},{\"end\":50290,\"start\":50275},{\"end\":50305,\"start\":50290},{\"end\":50323,\"start\":50305},{\"end\":50627,\"start\":50617},{\"end\":50642,\"start\":50627},{\"end\":50660,\"start\":50642},{\"end\":50674,\"start\":50660},{\"end\":50688,\"start\":50674},{\"end\":50704,\"start\":50688},{\"end\":50723,\"start\":50704},{\"end\":50734,\"start\":50723},{\"end\":51056,\"start\":51043},{\"end\":51073,\"start\":51056},{\"end\":51089,\"start\":51073},{\"end\":51102,\"start\":51089},{\"end\":51120,\"start\":51102},{\"end\":51133,\"start\":51120},{\"end\":51149,\"start\":51133},{\"end\":51167,\"start\":51149},{\"end\":51180,\"start\":51167},{\"end\":51194,\"start\":51180},{\"end\":51518,\"start\":51506},{\"end\":51528,\"start\":51518},{\"end\":51540,\"start\":51528},{\"end\":51554,\"start\":51540},{\"end\":51565,\"start\":51554},{\"end\":51805,\"start\":51792},{\"end\":51817,\"start\":51805},{\"end\":51830,\"start\":51817},{\"end\":51847,\"start\":51830},{\"end\":51869,\"start\":51847},{\"end\":51880,\"start\":51869},{\"end\":52181,\"start\":52170},{\"end\":52195,\"start\":52181},{\"end\":52212,\"start\":52195},{\"end\":52229,\"start\":52212},{\"end\":52246,\"start\":52229},{\"end\":52522,\"start\":52511},{\"end\":52538,\"start\":52522},{\"end\":52547,\"start\":52538},{\"end\":52558,\"start\":52547},{\"end\":52789,\"start\":52778},{\"end\":52805,\"start\":52789},{\"end\":52814,\"start\":52805},{\"end\":52825,\"start\":52814},{\"end\":53058,\"start\":53046},{\"end\":53067,\"start\":53058},{\"end\":53081,\"start\":53067},{\"end\":53093,\"start\":53081},{\"end\":53101,\"start\":53093},{\"end\":53115,\"start\":53101},{\"end\":53130,\"start\":53115},{\"end\":53379,\"start\":53367},{\"end\":53396,\"start\":53379},{\"end\":53407,\"start\":53396},{\"end\":53425,\"start\":53407},{\"end\":53440,\"start\":53425},{\"end\":53699,\"start\":53685},{\"end\":53711,\"start\":53699},{\"end\":53728,\"start\":53711},{\"end\":53741,\"start\":53728},{\"end\":53752,\"start\":53741},{\"end\":53765,\"start\":53752},{\"end\":53776,\"start\":53765},{\"end\":53791,\"start\":53776},{\"end\":53802,\"start\":53791},{\"end\":53814,\"start\":53802},{\"end\":53820,\"start\":53814},{\"end\":54159,\"start\":54147},{\"end\":54170,\"start\":54159},{\"end\":54183,\"start\":54170},{\"end\":54194,\"start\":54183},{\"end\":54208,\"start\":54194},{\"end\":54224,\"start\":54208},{\"end\":54242,\"start\":54224},{\"end\":54507,\"start\":54495},{\"end\":54518,\"start\":54507},{\"end\":54528,\"start\":54518},{\"end\":54536,\"start\":54528},{\"end\":54551,\"start\":54536},{\"end\":54563,\"start\":54551}]", "bib_venue": "[{\"end\":33467,\"start\":33463},{\"end\":33691,\"start\":33615},{\"end\":34178,\"start\":34174},{\"end\":34496,\"start\":34492},{\"end\":34795,\"start\":34791},{\"end\":35010,\"start\":34949},{\"end\":35407,\"start\":35403},{\"end\":35738,\"start\":35735},{\"end\":36084,\"start\":36081},{\"end\":36393,\"start\":36389},{\"end\":36686,\"start\":36681},{\"end\":36990,\"start\":36986},{\"end\":37443,\"start\":37439},{\"end\":37886,\"start\":37882},{\"end\":38159,\"start\":38155},{\"end\":38416,\"start\":38412},{\"end\":38653,\"start\":38635},{\"end\":38847,\"start\":38843},{\"end\":39071,\"start\":39067},{\"end\":39332,\"start\":39328},{\"end\":39684,\"start\":39674},{\"end\":40047,\"start\":40043},{\"end\":40403,\"start\":40399},{\"end\":40720,\"start\":40716},{\"end\":41016,\"start\":41006},{\"end\":41349,\"start\":41345},{\"end\":41717,\"start\":41713},{\"end\":42064,\"start\":42060},{\"end\":42403,\"start\":42399},{\"end\":42805,\"start\":42743},{\"end\":43212,\"start\":43208},{\"end\":43520,\"start\":43509},{\"end\":43849,\"start\":43845},{\"end\":44155,\"start\":44133},{\"end\":44393,\"start\":44389},{\"end\":44594,\"start\":44590},{\"end\":44841,\"start\":44834},{\"end\":45193,\"start\":45176},{\"end\":45570,\"start\":45566},{\"end\":45946,\"start\":45942},{\"end\":46279,\"start\":46275},{\"end\":46580,\"start\":46576},{\"end\":46833,\"start\":46829},{\"end\":47178,\"start\":47171},{\"end\":47681,\"start\":47677},{\"end\":48135,\"start\":48131},{\"end\":48416,\"start\":48382},{\"end\":48635,\"start\":48631},{\"end\":48910,\"start\":48903},{\"end\":49145,\"start\":49138},{\"end\":49367,\"start\":49363},{\"end\":49684,\"start\":49680},{\"end\":49953,\"start\":49886},{\"end\":50330,\"start\":50323},{\"end\":50738,\"start\":50734},{\"end\":51199,\"start\":51194},{\"end\":51569,\"start\":51565},{\"end\":51887,\"start\":51880},{\"end\":52250,\"start\":52246},{\"end\":52562,\"start\":52558},{\"end\":52829,\"start\":52825},{\"end\":53134,\"start\":53130},{\"end\":53444,\"start\":53440},{\"end\":53824,\"start\":53820},{\"end\":54145,\"start\":54080},{\"end\":54567,\"start\":54563}]"}}}, "year": 2023, "month": 12, "day": 17}