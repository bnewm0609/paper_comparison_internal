{"id": 237278112, "updated": "2023-10-06 00:10:01.147", "metadata": {"title": "DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras", "authors": "[{\"first\":\"Zachary\",\"last\":\"Teed\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Deng\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.10869", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/TeedD21", "doi": null}}, "content": {"source": {"pdf_hash": "67515d1f7df144683b059e684da7974e40aeaca1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.10869v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bff128a52e129b698907c6722bccb5a4a0db90c0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/67515d1f7df144683b059e684da7974e40aeaca1.txt", "contents": "\nDROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras\n\n\nZachary Teed zteed@princeton.edu \nPrinceton University\n\n\nJia Deng jiadeng@princeton.edu \nPrinceton University\n\n\nDROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras\n\nWe introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.\n\nIntroduction\n\nSimultaneous Localization and Mapping (SLAM) aims to (1) build a map of the environment and (2) localize the agent within the environment. It is a special form of Structure-from-Motion (SfM) focused on accurate tracking of long-term trajectories. It is a critical capability for robotics, especially autonomous vehicles. In this work, we address visual SLAM, where sensor recordings come in the form of images captured from a monocular, stereo, or RGB-D camera.\n\nThe SLAM problem has been approached from a number of different angles. Early work was built using probabilistic and filtering based approaches [12,30], and alternating optimization of the map and camera poses [34,16]. More recently, modern SLAM systems have leveraged least-squares optimization. A key element for accuracy has been full Bundle Adjustment (BA), which jointly optimizes the camera poses and the 3D map in a single optimization problem. One advantage of the optimization-based formulation is that a SLAM system can be easily modified to leverage different sensors. For example, ORB-SLAM3 [5] supports monocular, stereo, RGB-D, and IMU sensors, and modern systems can support a variety of camera models [5,27,43,6]. Despite significant progress, current SLAM systems lack the robustness demanded for many real-world applications. Failures come in many forms, such as lost feature tracks, divergence in the optimization algorithm, and accumulation of drift.\n\nDeep learning has been proposed as a solution to many of these failure cases. Previous work has investigated replacing hand-crafted with learned features [13,7,29,26,35], using neural 3D representations [47,1,9,46,45,25,22], and combining learned energy terms with classical optimization backends [59,58]. Other work has tried to learn SLAM or VO systems end-to-end [60,48,54,53,47]. While these systems are sometimes more robust, they fall far short of the accuracy of their classical counterparts on common benchmarks.\n\nIn this work we introduce DROID-SLAM, a new SLAM system based on deep learning. It has state-of-the-art performance, outperforming existing SLAM systems, classical or learning-based, on challenging benchmarks with very large margins. In particular, it has the following advantages:\n\n\u2022 High Accuracy: We achieve large improvements over prior work across multiple datasets and modalities. On the TartanAir SLAM competition [55], we reduce error by 62% over the best prior result on the monocular track and 60% on the stereo track. We rank 1st on the  ETH-3D RGB-D SLAM leaderboard [42], outperforming the second place by 35% under the AUC metric which considers both error and rate of catastrophic failure. On EuRoC [2], with monocular input, we reduce error by 82% among methods with zero failures, and by 43% over ORB-SLAM3 considering only the 10 out of 11 sequences it succeeds on. With stereo input, we reduce error by 71% over ORB-SLAM3. On TUM-RGBD [44], we reduce error by 83% among the methods with zero failures.\n\n\u2022 High Robustness: We have substantially fewer catastrophic failures than prior systems. On ETH-3D, we successfully track 30 of the 32 RGB-D datasets, while the next best successfully tracks only 19/32. On TartanAir, EuRoC, and TUM-RGBD, we have zero failures.\n\n\u2022 Strong Generalization: Our system, trained only with monocular input, can directly use stereo or RGB-D input to get improved accuracy without any retraining. All of our results across 4 datasets and 3 modalities are achieved by a single model, trained once with only monocular input entirely on the synthetic TartanAir dataset.\n\nThe strong performance and generalization of DROID-SLAM is made possible by its \"Differentiable Recurrent Optimization-Inspired Design\" (DROID), which is an end-to-end differentiable architecture that combines the strengths of both classical approaches and deep networks. Specifically, it consists of recurrent iterative updates, building upon RAFT [49] for optical flow but introducing two key innovations.\n\nFirst, unlike RAFT, which iteratively updates optical flow, we iteratively update camera poses and depth. Whereas RAFT operates on two frames, our updates are applied to an arbitrary number of frames, enabling joint global refinement of all camera poses and depth maps, essential for minimizing drift for long trajectories and loop closures.\n\nSecond, each update of camera poses and depth maps in DROID-SLAM is produced by a differentiable Dense Bundle Adjustment (DBA) layer, which computes a Gauss-Newton update to camera poses and dense per-pixel depth so as to maximize their compatibility with the current estimate of optical flow. This DBA layer leverages geometric constraints, improves accuracy and robustness, and enables a monocular system to handle stereo or RGB-D input without retraining.\n\nThe design of DROID-SLAM is novel. The closest prior deep architectures are DeepV2D [48] and BA-Net [47], both of which were focused on depth estimation and reported limited SLAM results. DeepV2D alternates between updating depth and updating camera poses, instead of bundle adjustment. BA-Net has a bundle adjustment layer, but their layer is substantially different: it is not \"dense\" in that it optimizes over a small number of coefficients used to linearly combine a depth basis (a set of pre-predicted depth maps), whereas we optimize over per-pixel depth directly, without being handicapped by a depth basis. In addition, BA-Net optimizes photometric reprojection error (in feature space), whereas we optimize geometric error, leveraging state-of-the-art flow estimation.\n\nWe perform extensive evaluation across four different datasets and three different sensor modalities, demonstrating state-of-the-art performance in all cases. We also include ablation studies that shed light on important design decisions and hyperparameters.\n\n\nRelated Work\n\nModern SLAM systems treat localization and mapping as a joint optimization problem [4].\n\nVisual SLAM focuses on observations in the form of monocular, stereo, or RGB-D images. These approaches are commonly categorized as either being direct or indirect [15]. Indirect approaches [31,32,5,38] first process the image into an intermediate representation by detecting points of interest and attaching feature descriptors. Features are then matched between images. Indirect approaches optimize camera pose and a 3D point cloud by minimizing reprojection error-the distance between a projected 3D point and its location in the image.\n\nDirect approaches model the image formation process and define an objective function over photometric error [16,15,61]. One advantage of direct approaches is that they can model more information about the image, such as lines and intensity variations [15] which are not used by indirect approaches. However, photometric errors typically lead to more difficult optimization problems, and direct approaches are less robust to geometric distortion such as rolling shutter artifacts. This approach requires more sophisticated optimization techniques, such as coarse-to-fine image pyramids to avoid local minimum.\n\nOur method does not clearly fit into either of the categories. Like the direct approach, we do not require preprocessing steps to detect and match features between the images. We instead use the full image, allowing us to leverage a wider range of information than indirect methods with typically only use corners and edges. However, we minimize reprojection error similar to indirect methods. This is an easier optimization problem and avoids the need for more complicated representations such as image pyramids. In this sense, our approach borrows the best of both approaches: the smoother objective function of indirect approaches with the greater modeling capacity of indirect approaches.\n\nDeep Learning has more recently been applied to the SLAM problem. Many works have focused on training systems for particular subproblems, such as feature detection [13,7,29,26,35], feature matching and outlier rejection [39,37], and localization [52,40]. SuperGlue [39] was designed to perform feature matching and verification and make 2-view pose estimate much more robust. Our network also draws inspiration from Dusmanu et al [14], which builds a neural network into the SfM pipeline to improve keypoint localization accuracy.\n\nOther works have focused on training SLAM systems end-to-end [60,47,8,51,24,48,54]. These methods are not full SLAM systems, but instead focus on small scale reconstruction on the order of two [8,51,54] up to a dozen frames [60,47,48]. They lack many of the core capabilities of modern SLAM systems such as loop closure and global bundle adjustment which inhibit their ability to perform large scale reconstruction as demonstrated in our experiments. \u2207SLAM [23] implements several existing SLAM algorithms as differentiable computation graphs, allowing for errors in the reconstruction to be backpropagated back to sensor measurements. While this approach is differentiable, it has no trainable parameters, meaning the performance of the system is limited by the accuracy of the classical algorithm they emulate.\n\nDeepFactors [9] is the most complete deep SLAM system, building on the earlier CodeSLAM [1]. It performs joint optimization of the pose and depth variables, and is capable of short and long-range loop closure. Similar to BA-Net [47], DeepFactors optimizes the parameters of a learned depth basis during inference. In contrast, we do not rely on a learned basis, but instead optimize pixelwise depth. This allows our network to better generalize to new datasets since our depth representation is not tied to the training dataset.\n\n\nApproach\n\nWe take a video as input with two objectives: estimate the trajectory of the camera and build a 3D map of the environment. We first describe the monocular setting; in Sec. 3.4 we describe how to generalize the system to stereo and RGB-D video.\n\nRepresentation: Our network operates on an ordered collection of images, {I t } N t=0 . For each image t, we maintain two state variables: camera pose G t \u2208 SE(3) and inverse depth d t \u2208 R H\u00d7W + . The set of poses, {G t } N t=0 , and set of inverse depths {d t } N t=0 are unknown state variables, which get iteratively updated during inference as new frames are processed. For the reminder of the paper, when we refer to depths, note that we are using the inverse depth parameterization.\n\nWe adopt a frame-graph (V, E) to represent co-visibility between frames. An edge (i, j) \u2208 E means image I i and I j have overlapping fields of view which shared points. The frame graph is built dynamically during training and inference. After each pose or depth update, we can recompute visibility to update the frame graph. If the camera returns to a previously mapped region, we add long range connections in the graph to perform loop closure.\n\n\nFeature Extraction and Correlation\n\nFeatures are extracted from each new image added to they system. Key components of this stage are borrowed from RAFT [49].\n\nFeature Extraction Each of the input images are processed by a feature extraction network. The network consists of 6 residual blocks and 3 downsampling layers, producing dense feature maps at 1/8 the input image resolution. Like RAFT [49], we use two separate networks: a feature network and a context network. The feature network is used to build the set of correlation volumes, while the context features are injected into the network during each application of the update operator.\n\nCorrelation Pyramid For each edge in the frame graph, (i, j) \u2208 E, we compute a 4D correlation volume by taking the dot product between all-pairs of feature vectors in g \u03b8 (I i ) and g \u03b8 (I j )\nC ij u1v1u2v2 = g \u03b8 (I i ) u1v1 , g \u03b8 (I j ) u2v2(1)\nWe then perform average pooling of the last two dimension of the correlation volume following RAFT [49] to form a 4-level correlation pyramid.\n\n\nCorrelation Lookup\n\nWe define a lookup operator which indexes the correlation volume using a grid with radius r, L r :\nR H\u00d7W \u00d7H\u00d7W \u00d7 R H\u00d7W \u00d72 \u2192 R H\u00d7W \u00d7(r+1) 2 .\nThe lookup operator takes an H \u00d7 W grid of coordinates as input and values are retrieved from the correlation volume using bilinear interpolation. The operator is applied to each correlation volume in the pyramid and the final feature vector is computed by concatenating the results at each level.\n\n\nUpdate Operator\n\nThe core component of our SLAM system is a learned update operator show in Fig. 2. The update operator is a 3 \u00d7 3 convolutional GRU with hidden state h. Each application of the operator updates the hidden state, and additionally produces a pose update, \u2206\u03be (k) , and depth update, \u2206d (k) . The pose and depth updates are applied to the current depth and pose estimates through retraction on the SE3 manifold and vector addition respectively\nG (k+1) = Exp(\u2206\u03be (k) ) \u2022 G (k) , d (k+1) = \u2206d (k) + d (k) .(2)\nIterative applications of the update operator produce a sequence of poses and depths, with the expectation of converging to a fixed point\n{G (k) } \u2192 G * , {d (k) } \u2192 d * , reflecting the true reconstruction.\nCorrespondence At the start of each iteration we use the current estimates of poses and depths to estimate correspondence. Given a grid of pixel coordinates, p i \u2208 R H\u00d7W \u00d72 in frame i, we compute the dense correspondence field p ij\np ij = \u03a0 c (G ij \u2022 \u03a0 \u22121 c (p i , d i )), p ij \u2208 R H\u00d7W \u00d72 G ij = G j \u2022 G \u22121 i .(3)\nfor each edge (i, j) \u2208 E in the frame graph. Here \u03a0 c is the camera model mapping a set of 3D points onto the image and \u03a0 \u22121 c is the inverse projection function mapping inverse depth map d and coordinate grid p i to a 3D point cloud (we provide formulas and Jacobians in the appendix). p ij represents the coordinates of pixels p i mapped into frame j using the estimated pose and depth.\n\nInputs We use the correspondence field to index the correlation volumes. For each edge (i, j) \u2208 E we use p ij to perform lookup from the correlation volume C ij to retrieve correlation features. Additionally, we use the correspondence field to derive optical flow induced by camera motion as the difference p ij \u2212 p j . Furthermore, the residual from the previous BA solution is concatenated with the flow field allowing the network to use feedback from the previous iteration.\n\nThe correlation features provide information about visual similarity in the neighbourhood of p ij allowing the network to learn to align visually similar image regions. However, correspondence is sometimes ambiguous. The flow provides an complementary source of information allowing the network to exploit smoothness in the motion fields to gain robustness.\n\nUpdate The correlation features and flow features are each mapped through two convolutional layers before being injected into the GRU. Additionally, we inject context features, as extracted by the context network, into the GRU through element-wise addition.\n\nThe ConvGRU is a local operation with a small receptive field. We extract global context by averaging the hidden state across the spatial dimensions of the image and use this feature vector as additional input to the GRU. Global context is important in SLAM because incorrect correspondences, caused by large moving objects for example, can degrade the accuracy of the system. It is important for the network to recognize and reject erroneous correspondence.\n\nThe GRU produces an updated hidden state h (k+1) . Instead of predicting updates to the depth or pose directly, we instead predict updates in the space of dense flow fields. We map the hidden state through two additional convoluation layers to produce two outputs: (1) a revision flow field r ij \u2208 R H\u00d7W \u00d72 and (2) associated confidence map w ij \u2208 R H\u00d7W \u00d72 + . The revision r ij is a correction term predicted by the network to correct errors in the dense correspondence field. We denote the corrected correspondence as p * ij = r ij + p ij We then pool the hidden state over all features which share the same source view i and predict a pixel-wise damping factor \u03bb. We use the softplus operator to ensure that the damping term is positive. Additionally, we use the pooled features to predict a 8x8 mask which can be used to upsample the inverse depth estimate.\n\nDense Bundle Adjustment Layer (DBA) The Dense Bundle Adjustment Layer (DBA) maps the set of flow revisions into a set of pose and pixelwise depth updates. We define the cost function over the entire frame graph\nE(G , d ) = (i,j)\u2208E p * ij \u2212 \u03a0 c (G ij \u2022 \u03a0 \u22121 c (p i , d i )) 2 \u03a3ij \u03a3 ij = diag w ij .(4)\nwhere \u00b7 \u03a3 is the Mahalanobis distance which weights the error terms based on the confidence weights w ij . Eqn. 4 states that we want an updated pose G and depth d such that reprojected points match the revised correspondence p * ij as predicted by the update operator. We use local parameterization to linearize Eqn. 4 and use the Gauss-Newton algorithm solve for updates (\u2206\u03be, \u2206d). Since each term in Eqn. 4 only includes a single depth variable, the Hessian matrix has block diagonal structure. Separating pose and depth variables, the system can be solved efficiently using the Schur complement with the pixelwise damping factor \u03bb added to the depth block\nB E E T C \u2206\u03be \u2206d = v w \u2206\u03be = [B \u2212 EC \u22121 E T ] \u22121 (v \u2212 EC \u22121 w) \u2206d = C \u22121 (w \u2212 E T \u2206\u03be)(5)\nwhere C is diagonal and can be cheaply inverted C \u22121 = 1/C. The DBA layer is implemented as part of the computation graph and backpropogation is performed through the layer during training.\n\n\nTraining\n\nOur SLAM system is implemented in PyTorch and we use the LieTorch extension [50] to perform backprogation in the tangent space of all group elements.\n\nRemoving gauge freedom In the monocular setting, the network is only able to recover the trajectory of the camera up to a similarity transform. One solution is to define a loss which is invariant to similarity transforms. However, the gauge-freedom still exists during training which poorly impacts the conditioning of the linear system and the stability of the gradients. We solve this problem by fixing the first two poses to the ground-truth poses of each training sequence. Fixing the first pose removes the 6-dof gauge freedom. Fixing the second pose resolves the scale freedom.\n\nConstructing training video Each training example consists of a 7-frame video sequence. In order to ensure stable training and good downstream performance, we want to sample videos which are not too easy nor too difficult.\n\nThe training set is composed of a collection of videos. For each video i of length N i , we precompute an N i \u00d7 N i distance matrix storing the average optical flow magnitude between each pair of frames. However, not all frames are covisible; and frames pairs with less than 50% overlap are assigned a distance of infinity. During training, we dynamically generate videos by sampling paths in the distance matrix, such that the average flow between adjacent video frames is between 8px and 96px.\n\nSupervision We supervise our network using a combination of pose loss and flow loss. The flow loss is applied to pairs of adjacent frames. We compute the optical flow induced by the predicted depth and poses and the flow induced by the ground truth depth and poses. The loss is taken to be the average l2 distance between the two flow fields.\n\nGiven a set of ground truth poses {T} N i and predicted poses {G} N i , the pose loss is taken to be the distance between the ground truth and predicted poses,\nL pose = i || Log SE3 (T \u22121 i \u00b7 G i )|| 2 .\nWe apply the losses to the output of every iteration with exponentially increasing weight using \u03b3 = 0.9.\n\n\nSLAM System\n\nDuring inference, we compose the network into a full SLAM system. The SLAM system takes a video stream as input, and performs reconstruction and localization in real-time. Our system contains two threads which run asynchronously. The frontend thread takes in new frames, extracts features, selects keyframes, and performs local bundle adjustment. The backend thread simultaneously performs global bundle adjustment over the entire history of keyframes. We provide an overview of the system here, and provide more information in the appendix.\n\nInitialization Initialization is simple with DROID-SLAM. We simply collect frames until we have a set of 12. As we accumulate frames, we only keep the previous frame if optical flow is greater than 16px (estimated by applying one update iteration). Once 12 frames have been accumulated, we initialize a frame graph by creating an edges between keyframes which are within 3 timesteps apart, then run 10 iterations of the update operator.\n\nFrontend The frontend operates directly on the incoming video stream. It maintains a collection of keyframes and a frame graph storing edges between covisible kefyrames. Keyframe poses and depths are actively being optimized. Features are first extracted from the incoming frames. The new frame is then added to the frame graph adding edges with its 3 closest neighbors as measured by mean optical flow. The pose is initialized using a linear motion model. We then apply several iterations of the update operator to update keyframe poses and depths. We fix the first two poses to remove gauge freedom but treat all depths as free variables.\n\nAfter the new frame is tracked, we select a keyframe for removal. We compute distance between pairs of frames by computing the average optical flow magnitude and remove redundant frames. If no frame is a good candidate for removal, we remove the oldest keyframe.\n\nBackend The backend performs global bundle adjustment over the entire history of keyframes. During each iteration, we rebuild the frame graph using the flow between all pairs of keyframes, represented as an N \u00d7 N distance matrix. We first add edges between temporally adjacent keyframes. We then sample new edges from the distance matrix in order of increasing flow. With each selected edge, we suppress neighboring edges within a distance of 2, where distance is defined as the Chebyshev distance between index pairs ||(i, j) \u2212 (k, l)|| \u221e = max(|i \u2212 k|, |j \u2212 l|).\n\nWe then apply the update operator to the entire frame graph, often consisting of thousands of frames and edges. Storing the full set of correlation volumes would quickly exceed video memory. Instead, we use the memory efficient implementation proposed in RAFT [49].  [21], ScanNet [10], Sintel [3], and ETH-3D [42]; all using monocular video.\n\nDuring training, we implement dense bundle adjustment in PyTorch to leverage the automatic differentiation engine. At inference time, we use a custom CUDA kernel which takes advantage of the block-sparse structure of the problem, then perform sparse Cholesky decomposition on the reduced camera block.\n\nWe only perform full bundle adjustment on keyframe images. In order to recover the poses of non-keyframes, we perform motion-only bundle adjustment by iteratively estimating flow between each keyframe and its neighboring non-keyframes. During testing, we evaluate on the full camera trajectory, not just keyframes.\n\nStereo and RGB-D Our system can be easily modified for stereo and RGB-D video. In the case of RGB-D, we still treat depth as a variable, since sensor depth can be noisy and have missing observations, and simply add a term to the optimization objective (Eqn. 4) which penalizes the squared distance between the measured and predicted depth. For stereo, we use the exact same system described above, with just double the frames, and fix the relative pose between the left and right frames in the DBA layer. Cross camera edges in the graph allow us to leverage stereo information.\n\n\nExperiments\n\nWe experiment on a diverse set of datasets and sensor modalities. We compare to both deep learning and established classical SLAM algorithms and put specific emphasis on cross-dataset generalization. Following prior work, we evaluate the accuracy of the camera trajectory [31,15,42], primarily using Absolute Trajectory Error (ATE) [44]. While some datasets have ground truth point clouds [21], there is no standard protocol to compare 3D reconstructions directly given by SLAM systems because a SLAM systems can choose which 3D points to reconstruct. Evaluating dense 3D reconstruction is typically considered in the domain of Multiview Stereo [19] and outside the scope of this work.\n\nOur network is trained entirely on monocular video from the synthetic TartanAir dataset [55]. We train our network for 250k steps with a batch size of 4, resolution 384 \u00d7 512, and 7 frame clips, and unroll 15 update iterations. Training takes 1 week on 4 RTX-3090 GPUs.  \n\n\nTartanAir [55] (Monocular & Stereo)\n\nThe TartanAir dataset is a challenging synthetic benchmark for evaluating SLAM algorithms and was used as part of the ECCV 2020 SLAM competition. We use the official test split [55], and provide ATE across all \"Hard\" sequences in Tab. 1.\n\nTab. 1 demonstrates both the robustness of our method (no catastrophic failures) and accuracy (very low drift). We retrain DeepV2D [48] on TartanAir as a baseline. On most sequences, we outperform existing methods by an order-of-magnitude and achieve 8x lower average error than TartanVO [54] and 20x lower than DeepV2D [48]. We also use the TartanAir dataset to compare with the top submissions to the ECCV 2020 SLAM competition in Tab   In the monocular setting, we achieve an average ATE of 2.2cm, reducing error by 82% among methods with zero failures, and by 43% over ORB-SLAM3 when only comparing sequences where ORB-SLAM3 is successful.\n\nWe compare to several deep learning approaches. We compare to DeepV2D trained on the TartanAir dataset and the publicly available version trained on NYUv2 [33] and ScanNet [10]. DeepFactors [9] was trained on ScanNet. We find that recent deep learning approaches [9,48,54] perform poorly on the EuRoC dataset compared to classical SLAM systems. This is due to poor generalization and dataset biases which lead to large amounts of drift; our method does not suffer from these issues. D3VO [58] is able to achieve both good robustness and accuracy by combining a neural network frontend with DSO as a backend, using 6 of the 11 sequences for evaluation and performing unsupervised training on the remaining ones, which contain the same scenes used for evaluation.\n\nTUM-RGBD [44] The RGBD dataset consists of indoor scenes captured with handheld camera. This is a notoriously difficult dataset for monocular methods due to rolling shutter artifacts, motion blur, and heavy rotation. We benchmark prior work on the entirety of the freiburg1 set in Tab. 4.\n\nClassical SLAM algorithms such as ORB-SLAM tend to fail on most of the sequences. While deep learning methods are more robust, they obtain low accuracy on most of the evaluated sequences. Our  Table 4: ATE on the TUM-RGBD benchmark. All methods are provided mono. video, 1 except DeepTAM which uses RGB-D and 2 TartanVO which uses ground truth to scale relative pose.\n\nmethod is both robust and accurate. It successfully tracks all 9 sequences while achieving 83% lower ATE than DeepFactors [9] and which succeeds on all videos and 90% lower ATE than DeepV2D [48].\n\n\nMethod AUC (train) AUC (test)\n\nBundleFusion [11] 84.10 33.84 ElasticFusion [57] 89.06 34.02 RFusion [56] 17.37 51.94 DVO-SLAM [20] 193.89 71.83 ORB-SLAM2 [32] 156.10 104.28 BAD-SLAM [42] 280  ETH3D-SLAM [42] (RGB-D) Finally, we evaluate the RGB-D performance on the ETH3D-SLAM benchmark. In this setup, the network is also provided measurements from an RGB-D camera. We take our network trained on TartanAir and add an addition term in the optimization objective penalizing the distance between the predicted inv. depth and inv. depth measured by the sensor. Without any finetuning, our method ranks 1st on both the train and test splits. Several of the datasets are \"dark\" meaning no image data is available; on these datasets we do not submit any predictions.\n\nOn the test set, we successfully track 30/32 RGB-D, improving over the next best of 19/32.\n\nTiming and Memory Our system can run in real-time with 2 3090 GPUs. Tracking and local BA is run on the first GPU, while global BA and loop closure is run on the second. On EuRoC, we average 20fps (camera hz) by downsampling to 320 \u00d7 512 resolution and skipping every other frame. Results in Tab. 3 were obtained in this setting. On TUM-RGBD, we average 30fps by downsampling to 240 \u00d7 320 and skipping every other frame, again the reported results where obtained in this setting. On TartanAir, due to much faster camera motion, we are unable to run in real-time, averaging 8fps. However, this is still a 16x speedup over the top 2 submissions to the TartanAir SLAM challenge, which rely on COLMAP [41].\n\nThe SLAM frontend can be run on GPUs with 8GB of memory. The backend, which requires storing feature maps from the full set of images, is more memory intensive. All results on TUM-RGBD can be produced on a single 1080Ti graphics card. Results on EuRoC, TartanAir and ETH-3D (where video can be up to 5000 frames) requires a GPU with 24GB memory. While memory and resource requirements are currently the biggest limitation of our system, we believe these can be drastically reduced by culling redundant computation and more efficient representations.\n\n\nConclusion\n\nWe introduce DROID-SLAM, an end-to-end neural architecture for visual SLAM. DROID-SLAM is accurate, robust, and versatile and can be used on monocular, stereo, and RGB-D video. It outperforms prior work by large margins on challenging benchmarks.  We provide stereo results on the EuRoC dataset [2] in Tab. 5 using our network trained on synthetic, monocular video. In the stereo setting, it is possible to recover the trajectory of the camera up to scale. Compared to ORB-SLAM3 [5] we reduce the average ATE by 71%.\n\n\nA Additional Results\n\n\nB Ablations\n\nKeyframe Image Keyframe Depth Optical Flow X-Confidence Y-Confidence \n\n\nC Camera Model and Jacobians\n\nWe represent 3D points using homogeneous coordinates X = (X, Y, Z, W ) T . An image point p with inverse depth d is re-projected from frame i into frame j according to the warping function\np = \u03a0 c (G ij \u00b7 \u03a0 \u22121 (p, d)) G ij = G j \u2022 G \u22121 i (6)\nwhere \u03a0 c is the pinhole projection function, and \u03a0 \u22121 c is the inverse projection\n\u03a0 c (X) = f x X Z + c y f y Y Z + c y \u03a0 \u22121 c (p, d) = \uf8eb \uf8ec \uf8ec \uf8ed px\u2212cx fx py\u2212cy fy 1 d \uf8f6 \uf8f7 \uf8f7 \uf8f8 .(7)\ngiven camera intrinsic parameters c = (f x , f y , c x , c y ).\n\nFor optimization, we need the Jacobians with respect to G i , G j , and d. We use the local parameterization e \u03bei G i and e \u03bej G j and treat d as a vector in R 1 . The Jacobians of the projection and inverse projection functions are given as\n\u2202\u03a0 c (X) \u2202X = f x 1 Z 0 \u2212f x X Z 2 0 0 f y 1 Z \u2212f y Y Z 2 0 \u2202\u03a0 \u22121 c (p, d) \u2202d = \uf8eb \uf8ec \uf8ed 0 0 0 1 \uf8f6 \uf8f7 \uf8f8 .(8)\nUsing the local parameterization, we compute the Jacobian of the 3D point transformation X = Exp(\u03be j ) \u00b7 G j \u00b7 (Exp(\u03be i ) \u00b7 G i ) \u22121 \u00b7 X = Exp(\u03be j ) \u00b7 G j \u00b7 G \u22121 i \u00b7 Exp(\u2212\u03be i ) \u00b7 X (9) using the adjoint operator to move the \u03be i term to the front of the expression X = Exp(\u03be j ) \u00b7 Exp(\u2212 Adj Gj G \u22121\ni \u03be i ) \u00b7 G j \u00b7 G \u22121 i \u00b7 X(10)\nallowing us to compute the Jacobians using the generators\n\u2202X \u2202\u03be j = \uf8eb \uf8ec \uf8ed W 0 0 0 Z \u2212Y 0 W 0 \u2212Z 0 X 0 0 W Y \u2212X 0 0 0 0 0 0 0 \uf8f6 \uf8f7 \uf8f8 (11) \u2202X \u2202\u03be i = \u2212 \uf8eb \uf8ec \uf8ed W 0 0 0 Z \u2212Y 0 W 0 \u2212Z 0 X 0 0 W Y \u2212X 0 0 0 0 0 0 0 \uf8f6 \uf8f7 \uf8f8 \u00b7 Adj Gj G \u22121 i(12)\nUsing the chain rule, we can compute the full Jacobians with respect to the variables \u2202p \u2202\u03be j = \u2202\u03a0 c (X ) \u2202X \n\nwhere (t x , t y , t z ) is the translation vector of G j \u2022 G \u22121 i .  Figure 8: Architecture of the feature and context encoders. Both extract features at 1/8 the input image resolution using a set of 6 basic residual blocks. Instance normalization is used in the feature encoder; no normalization is used in the context encoder. The feature encoder outputs features with dimension D=128 which the context encoder outputs features with dimension D=256. Corr Flow Context Figure 9: Architecture of the update operator. During each iteration, context, correlation, and flow features get injected into the GRU. The revision (r) and confidence weights (w) are predicted from the updated hidden state.\n\n\nD Network Architecture\n\nFigure 1 :\n1DROID-SLAM can operate on monocular, stereo, and RGB-D video. It builds a dense 3D map of the environment while simultaneously localizing the camera within the map.\n\nFigure 2 :\n2Illustration of the update operator. The operator acts on edges in the frame graph, predicting flow revisions which are mapped to depth and pose update through the (DBA) layer.\n\nFigure 3 :\n3DROID-SLAM can generalize to new datasets. In order, we show results from Tanks & Temples\n\nFigure 4 :\n4Generalization results on the RGB-D ETH3D-SLAM benchmark. (Left) Our method, which is trained only on the synthetic TartanAir dataset, ranks 1st on both the train and test splits. (Right) Plot of the number successful trajectories as a function of ATE. Our method successfully tracks 30/32 of the datasets where image data is available.\n\nFigure 5 :Figure 6 :Figure 7 :\n567Visualizations of keyframe image, depth, flow and confidence estimates. (Left) we show the performance of the system with different inputs (monocular vs. stereo) and whether global optimization is performed in addition to local BA (local vs. full). (Right) Tracking accuracy as a function of the number of keyframes. We use 5 keyframes (bold) in our experiments. (Left) Impact of global context in the update operator. (Right) Impact of using the bundle adjustment layer during training vs training directly on optical flow, then applying BA at test time.\n\n\n= \u2202\u03a0 c (X ) \u2202X \u2202X \u2202X \u2202\u03a0 \u22121 (p, d) \u2202d = \u2202\u03a0 c (X ) \u2202X = \u2202\u03a0 c (X )\n\nTable 1 :\n1Results on the TartanAir monocular benchmark.\n\nTable 2 :\n2Results on the TartanAir test set, compared with the top 3 submission to the ECCV 2020 \nSLAM competition. The score is computed using normalized relative pose error for all possible \nsequences of length {5, 10, 15, ..., 40} meters, see competition page for details. \n\nbuilt on top of COLMAP [41] and run 40x slower than real-time. Our method, on the other hand, \nruns 16x faster and achieves an error 62% lower on the monocular benchmark and 60% lower on the \nstereo benchmark. \n\nEuRoC [2] (Monocular & Stereo) In the remaining experiments, we are interested in the ability \nof our network to generalize to new cameras and environments. The EuRoC dataset consists of \nvideo captured from sensor on-board a micro aerial vehicle (MAV) and is a widely used benchmark \nto evaluate SLAM systems. We use the EuRoC dataset to evaluate both monocular and stereo \nperformance and report results on Tab. 3. \n\nMH01 MH02 MH03 MH04 MH05 V101 V102 V103 V201 V202 V203 \nAvg \nDeep/Hyb. \nDeepFactors [9] \n1.587 \n1.479 \n3.139 \n5.331 \n4.002 1.520 0.679 0.900 0.876 1.905 1.021 2.040 \nDeepV2D [48]  \u2020 \n0.739 \n1.144 \n0.752 \n1.492 \n1.567 0.981 0.801 1.570 0.290 2.202 2.743 1.298 \nDeepV2D (Tartan Air)  \u2020 1.614 \n1.492 \n1.635 \n1.775 \n1.013 0.717 0.695 1.483 0.839 1.052 0.591 1.173 \nTartanVO 1 [54]  \u2020 \n0.639 \n0.325 \n0.550 \n1.153 \n1.021 0.447 0.389 0.622 0.433 0.749 1.152 0.680 \nD3VO + DSO [58]  \u2020 \n-\n-\n0.08 \n-\n0.09 \n-\n-\n0.11 \n-\n0.05 \n0.19 \n-\n\nClassical \nORB-SLAM [31] \n0.071 \n0.067 \n0.071 \n0.082 \n0.060 0.015 0.020 \nX \n0.021 0.018 \nX \n-\nDSO [15]  \u2020 \n0.046 \n0.046 \n0.172 \n3.810 \n0.110 0.089 0.107 0.903 0.044 0.132 1.152 0.601 \nSVO [18]  \u2020 \n0.100 \n0.120 \n0.410 \n0.430 \n0.300 0.070 0.210 \nX \n0.110 0.110 1.080 \n-\nDSM [61] \n0.039 \n0.036 \n0.055 \n0.057 \n0.067 0.095 0.059 0.076 0.056 0.057 0.784 0.126 \nORB-SLAM3 [5] \n0.016 \n0.027 \n0.028 \n0.138 \n0.072 0.033 0.015 0.033 0.023 0.029 \nX \n-\n\nOurs (odometry only)  \u2020 \n0.163 \n0.121 \n0.242 \n0.399 \n0.270 0.103 0.165 0.158 0.102 0.115 0.204 0.186 \nOurs \n0.013 \n0.014 \n0.022 \n0.043 \n0.043 0.037 0.012 0.020 0.017 0.013 0.014 0.022 \n\n\n\nTable 3 :\n3Monocular SLAM on the EuRoC datasets, ATE[m]. \u2020 denotes visual odometry methods.\n\nTable 5 :\n5Stereo SLAM on the EuRoC datasets, ATE[m].\nAcknowledgements This work is partially supported by the National Science Foundation under Award IIS-1942981.Ablations We ablate various design choices regarding our SLAM system and network architecture. Ablations are performed on our validation split of the TartanAir dataset. InFig. 5we show visualizations on the validation set of keyframe depth estimates alongside optical flow and associated confidence weights.InFig.6(left) we show how the system benefits from both stereo video and global optimization. Although our network is only trained on monocular video, it can readily leverage stereo frames if available. InFig. 6 (right)we show how the number of keyframe affects odometery performance. InFig. 7we ablate components of the network architecture.Fig. 7 (left)shows the impact of using global context in the GRU through spatial pooling while 7 (right) demonstrates the importance of training with DBA as opposed to training on flow and applying BA at inference. We find that the SLAM system is unstable and prone to failure if the DBA is not used during training.\nCodeslam-learning a compact, optimisable representation for dense visual slam. M Bloesch, J Czarnowski, R Clark, S Leutenegger, A J Davison, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionM. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison. Codeslam-learning a compact, optimisable representation for dense visual slam. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2560-2568, 2018.\n\nThe euroc micro aerial vehicle datasets. M Burri, J Nikolic, P Gohl, T Schneider, J Rehder, S Omari, M W Achtelik, R Siegwart, The International Journal of Robotics Research. 3510M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart. The euroc micro aerial vehicle datasets. The International Journal of Robotics Research, 35(10):1157-1163, 2016.\n\nA naturalistic open source movie for optical flow evaluation. D J Butler, J Wulff, G B Stanley, M J Black, European Conf. on Computer Vision (ECCV), Part IV. A. Fitzgibbon et al.Springer-Verlag7577D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical flow evaluation. In A. Fitzgibbon et al. (Eds.), editor, European Conf. on Computer Vision (ECCV), Part IV, LNCS 7577, pages 611-625. Springer-Verlag, Oct. 2012.\n\nPast, present, and future of simultaneous localization and mapping: Toward the robust-perception age. C Cadena, L Carlone, H Carrillo, Y Latif, D Scaramuzza, J Neira, I Reid, J J Leonard, IEEE Transactions on robotics. 326C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Transactions on robotics, 32(6):1309-1332, 2016.\n\nOrb-slam3: An accurate open-source library for visual, visual-inertial and multi-map slam. C Campos, R Elvira, J J G Rodr\u00edguez, J M Montiel, J D Tard\u00f3s, arXiv:2007.11898arXiv preprintC. Campos, R. Elvira, J. J. G. Rodr\u00edguez, J. M. Montiel, and J. D. Tard\u00f3s. Orb-slam3: An accurate open-source library for visual, visual-inertial and multi-map slam. arXiv preprint arXiv:2007.11898, 2020.\n\nLarge-scale direct slam for omnidirectional cameras. D Caruso, J Engel, D Cremers, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEED. Caruso, J. Engel, and D. Cremers. Large-scale direct slam for omnidirectional cameras. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 141-148. IEEE, 2015.\n\nC B Choy, J Gwak, S Savarese, M Chandraker, arXiv:1606.03558Universal correspondence network. arXiv preprintC. B. Choy, J. Gwak, S. Savarese, and M. Chandraker. Universal correspondence network. arXiv preprint arXiv:1606.03558, 2016.\n\nLs-net: Learning to solve nonlinear least squares for monocular stereo. R Clark, M Bloesch, J Czarnowski, S Leutenegger, A J Davison, arXiv:1809.02966arXiv preprintR. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, and A. J. Davison. Ls-net: Learning to solve nonlinear least squares for monocular stereo. arXiv preprint arXiv:1809.02966, 2018.\n\nDeepfactors: Real-time probabilistic dense monocular slam. J Czarnowski, T Laidlow, R Clark, A J Davison, IEEE Robotics and Automation Letters. 52J. Czarnowski, T. Laidlow, R. Clark, and A. J. Davison. Deepfactors: Real-time probabilistic dense monocular slam. IEEE Robotics and Automation Letters, 5(2):721-728, 2020.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5828-5839, 2017.\n\nBundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. A Dai, M Nie\u00dfner, M Zollh\u00f6fer, S Izadi, C Theobalt, ACM Transactions on Graphics (ToG). 3641A. Dai, M. Nie\u00dfner, M. Zollh\u00f6fer, S. Izadi, and C. Theobalt. Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. ACM Transactions on Graphics (ToG), 36(4):1, 2017.\n\nMonoslam: Real-time single camera slam. A J Davison, I D Reid, N D Molton, O Stasse, IEEE transactions on pattern analysis and machine intelligence. 29A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse. Monoslam: Real-time single camera slam. IEEE transactions on pattern analysis and machine intelligence, 29(6):1052-1067, 2007.\n\nSuperpoint: Self-supervised interest point detection and description. D Detone, T Malisiewicz, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsD. DeTone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224-236, 2018.\n\nMulti-view optimization of local feature geometry. M Dusmanu, J L Schonberger, M Pollefeys, Proceedings of the 2020 European Conference on Computer Vision. the 2020 European Conference on Computer VisionM. Dusmanu, J. L. Schonberger, and M. Pollefeys. Multi-view optimization of local feature geometry. In Proceedings of the 2020 European Conference on Computer Vision, 2020.\n\nDirect sparse odometry. J Engel, V Koltun, D Cremers, IEEE transactions on pattern analysis and machine intelligence. 40J. Engel, V. Koltun, and D. Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611-625, 2017.\n\nLsd-slam: Large-scale direct monocular slam. J Engel, T Sch\u00f6ps, D Cremers, European conference on computer vision. SpringerJ. Engel, T. Sch\u00f6ps, and D. Cremers. Lsd-slam: Large-scale direct monocular slam. In European conference on computer vision, pages 834-849. Springer, 2014.\n\nOv2 slam: A fully online and versatile visual slam for real-time applications. M Ferrera, A Eudes, J Moras, M Sanfourche, G Le Besnerais, IEEE Robotics and Automation Letters. 62M. Ferrera, A. Eudes, J. Moras, M. Sanfourche, and G. Le Besnerais. Ov2 slam: A fully online and versatile visual slam for real-time applications. IEEE Robotics and Automation Letters, 6(2):1399-1406, 2021.\n\nSemidirect visual odometry for monocular and multicamera systems. C Forster, Z Zhang, M Gassner, M Werlberger, D Scaramuzza, Svo, IEEE Transactions on Robotics. 332C. Forster, Z. Zhang, M. Gassner, M. Werlberger, and D. Scaramuzza. Svo: Semidirect visual odometry for monocular and multicamera systems. IEEE Transactions on Robotics, 33(2):249-265, 2016.\n\nMulti-view stereo: A tutorial. Foundations and Trends\u00ae in Computer Graphics and Vision. Y Furukawa, C Hern\u00e1ndez, 9Y. Furukawa and C. Hern\u00e1ndez. Multi-view stereo: A tutorial. Foundations and Trends\u00ae in Computer Graphics and Vision, 9(1-2):1-148, 2015.\n\nDense visual slam for rgb-d cameras. C Kerl, J Sturm, D Cremers, IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEC. Kerl, J. Sturm, and D. Cremers. Dense visual slam for rgb-d cameras. In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 2100-2106. IEEE, 2013.\n\nTanks and temples: Benchmarking large-scale scene reconstruction. A Knapitsch, J Park, Q.-Y Zhou, V Koltun, ACM Transactions on Graphics (ToG). 364A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1-13, 2017.\n\nRobust consistent video depth estimation. J Kopf, X Rong, J.-B Huang, arXiv:2012.05901arXiv preprintJ. Kopf, X. Rong, and J.-B. Huang. Robust consistent video depth estimation. arXiv preprint arXiv:2012.05901, 2020.\n\nJ Krishna Murthy, S Saryazdi, G Iyer, L Paull, gradslam: Dense slam meets automatic differentiation. arXiv. J. Krishna Murthy, S. Saryazdi, G. Iyer, and L. Paull. gradslam: Dense slam meets automatic differentiation. arXiv, 2020.\n\nNeural rgb (r) d sensing: Depth and uncertainty from a video camera. C Liu, J Gu, K Kim, S G Narasimhan, J Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionC. Liu, J. Gu, K. Kim, S. G. Narasimhan, and J. Kautz. Neural rgb (r) d sensing: Depth and uncertainty from a video camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10986-10995, 2019.\n\nConsistent video depth estimation. X Luo, J.-B Huang, R Szeliski, K Matzen, J Kopf, ACM Transactions on Graphics (TOG). 394X. Luo, J.-B. Huang, R. Szeliski, K. Matzen, and J. Kopf. Consistent video depth estimation. ACM Transactions on Graphics (TOG), 39(4):71-1, 2020.\n\nGeodesc: Learning local descriptors by integrating geometry constraints. Z Luo, T Shen, L Zhou, S Zhu, R Zhang, Y Yao, T Fang, L Quan, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Z. Luo, T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang, and L. Quan. Geodesc: Learning local descriptors by integrating geometry constraints. In Proceedings of the European conference on computer vision (ECCV), pages 168-183, 2018.\n\nOmnidirectional dso: Direct sparse odometry with fisheye cameras. H Matsuki, L Stumberg, V Usenko, J St\u00fcckler, D Cremers, IEEE Robotics and Automation Letters. 34H. Matsuki, L. von Stumberg, V. Usenko, J. St\u00fcckler, and D. Cremers. Omnidirectional dso: Direct sparse odometry with fisheye cameras. IEEE Robotics and Automation Letters, 3(4):3693-3700, 2018.\n\nVoldor: Visual odometry from log-logistic dense optical flow residuals. Z Min, Y Yang, E Dunn, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZ. Min, Y. Yang, and E. Dunn. Voldor: Visual odometry from log-logistic dense optical flow residuals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4898-4909, 2020.\n\nWorking hard to know your neighbor's margins: Local descriptor learning loss. A Mishchuk, D Mishkin, F Radenovic, J Matas, arXiv:1705.10872arXiv preprintA. Mishchuk, D. Mishkin, F. Radenovic, and J. Matas. Working hard to know your neighbor's margins: Local descriptor learning loss. arXiv preprint arXiv:1705.10872, 2017.\n\nA multi-state constraint kalman filter for vision-aided inertial navigation. A I Mourikis, S I Roumeliotis, Proceedings 2007 IEEE International Conference on Robotics and Automation. 2007 IEEE International Conference on Robotics and AutomationIEEEA. I. Mourikis and S. I. Roumeliotis. A multi-state constraint kalman filter for vision-aided inertial navigation. In Proceedings 2007 IEEE International Conference on Robotics and Automation, pages 3565-3572. IEEE, 2007.\n\nOrb-slam: a versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE transactions on robotics. 31R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE transactions on robotics, 31(5):1147-1163, 2015.\n\nOrb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. R Mur-Artal, J D Tard\u00f3s, IEEE Transactions on Robotics. 335R. Mur-Artal and J. D. Tard\u00f3s. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE Transactions on Robotics, 33(5):1255-1262, 2017.\n\nIndoor segmentation and support inference from rgbd images. P K Nathan Silberman, Derek Hoiem, R Fergus, ECCV. P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.\n\nDtam: Dense tracking and mapping in real-time. R A Newcombe, S J Lovegrove, A J Davison, 2011 international conference on computer vision. IEEER. A. Newcombe, S. J. Lovegrove, and A. J. Davison. Dtam: Dense tracking and mapping in real-time. In 2011 international conference on computer vision, pages 2320-2327. IEEE, 2011.\n\nY Ono, E Trulls, P Fua, K M Yi, arXiv:1805.09662Lf-net: Learning local features from images. arXiv preprintY. Ono, E. Trulls, P. Fua, and K. M. Yi. Lf-net: Learning local features from images. arXiv preprint arXiv:1805.09662, 2018.\n\nOnline temporal calibration for monocular visual-inertial systems. T Qin, S Shen, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEET. Qin and S. Shen. Online temporal calibration for monocular visual-inertial systems. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3662-3669. IEEE, 2018.\n\nDeep fundamental matrix estimation. R Ranftl, V Koltun, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)R. Ranftl and V. Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision (ECCV), pages 284-299, 2018.\n\nKimera: an open-source library for real-time metricsemantic localization and mapping. A Rosinol, M Abate, Y Chang, L Carlone, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEEA. Rosinol, M. Abate, Y. Chang, and L. Carlone. Kimera: an open-source library for real-time metric- semantic localization and mapping. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 1689-1696. IEEE, 2020.\n\nSuperglue: Learning feature matching with graph neural networks. P.-E Sarlin, D Detone, T Malisiewicz, A Rabinovich, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionP.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4938-4947, 2020.\n\nBack to the feature: Learning robust camera localization from pixels to pose. P.-E Sarlin, A Unagar, M Larsson, H Germain, C Toft, V Larsson, M Pollefeys, V Lepetit, L Hammarstrand, F Kahl, arXiv:2103.09213arXiv preprintP.-E. Sarlin, A. Unagar, M. Larsson, H. Germain, C. Toft, V. Larsson, M. Pollefeys, V. Lepetit, L. Ham- marstrand, F. Kahl, et al. Back to the feature: Learning robust camera localization from pixels to pose. arXiv preprint arXiv:2103.09213, 2021.\n\nStructure-from-motion revisited. J L Schonberger, J.-M Frahm, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. L. Schonberger and J.-M. Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104-4113, 2016.\n\nBad slam: Bundle adjusted direct rgb-d slam. T Schops, T Sattler, M Pollefeys, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionT. Schops, T. Sattler, and M. Pollefeys. Bad slam: Bundle adjusted direct rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 134-144, 2019.\n\nDirect sparse odometry with rolling shutter. D Schubert, N Demmel, V Usenko, J Stuckler, D Cremers, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)D. Schubert, N. Demmel, V. Usenko, J. Stuckler, and D. Cremers. Direct sparse odometry with rolling shutter. In Proceedings of the European Conference on Computer Vision (ECCV), pages 682-697, 2018.\n\nA benchmark for the evaluation of rgb-d slam systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEJ. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 573-580. IEEE, 2012.\n\nE Sucar, S Liu, J Ortiz, A J Davison, arXiv:2103.12352imap: Implicit mapping and positioning in real-time. arXiv preprintE. Sucar, S. Liu, J. Ortiz, and A. J. Davison. imap: Implicit mapping and positioning in real-time. arXiv preprint arXiv:2103.12352, 2021.\n\nNodeslam: Neural object descriptors for multi-view shape reconstruction. E Sucar, K Wada, A Davison, 2020 International Conference on 3D Vision (3DV). IEEEE. Sucar, K. Wada, and A. Davison. Nodeslam: Neural object descriptors for multi-view shape reconstruc- tion. In 2020 International Conference on 3D Vision (3DV), pages 949-958. IEEE, 2020.\n\nC Tang, P Tan, arXiv:1806.04807Ba-net: Dense bundle adjustment network. arXiv preprintC. Tang and P. Tan. Ba-net: Dense bundle adjustment network. arXiv preprint arXiv:1806.04807, 2018.\n\nDeepv2d: Video to depth with differentiable structure from motion. Z Teed, J Deng, arXiv:1812.04605arXiv preprintZ. Teed and J. Deng. Deepv2d: Video to depth with differentiable structure from motion. arXiv preprint arXiv:1812.04605, 2018.\n\nRaft: Recurrent all-pairs field transforms for optical flow. Z Teed, J Deng, European Conference on Computer Vision. SpringerZ. Teed and J. Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European Conference on Computer Vision, pages 402-419. Springer, 2020.\n\nTangent space backpropagation for 3d transformation groups. Z Teed, J Deng, Conference on Computer Vision and Pattern Recognition. Z. Teed and J. Deng. Tangent space backpropagation for 3d transformation groups. In Conference on Computer Vision and Pattern Recognition, 2021.\n\nDemon: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionB. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5038-5047, 2017.\n\nL Stumberg, P Wenzel, N Yang, D Cremers, arXiv:2010.06323Lm-reloc: Levenberg-marquardt based direct visual relocalization. arXiv preprintL. von Stumberg, P. Wenzel, N. Yang, and D. Cremers. Lm-reloc: Levenberg-marquardt based direct visual relocalization. arXiv preprint arXiv:2010.06323, 2020.\n\nDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEES. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo: Towards end-to-end visual odometry with deep recur- rent convolutional neural networks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2043-2050. IEEE, 2017.\n\nW Wang, Y Hu, S Scherer, arXiv:2011.00359Tartanvo: A generalizable learning-based vo. arXiv preprintW. Wang, Y. Hu, and S. Scherer. Tartanvo: A generalizable learning-based vo. arXiv preprint arXiv:2011.00359, 2020.\n\nW Wang, D Zhu, X Wang, Y Hu, Y Qiu, C Wang, Y Hu, A Kapoor, S Scherer, arXiv:2003.14338Tartanair: A dataset to push the limits of visual slam. arXiv preprintW. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer. Tartanair: A dataset to push the limits of visual slam. arXiv preprint arXiv:2003.14338, 2020.\n\nReal-time large-scale dense rgb-d slam with volumetric fusion. T Whelan, M Kaess, H Johannsson, M Fallon, J J Leonard, J Mcdonald, The International Journal of Robotics Research. 344-5T. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and J. McDonald. Real-time large-scale dense rgb-d slam with volumetric fusion. The International Journal of Robotics Research, 34(4-5):598-626, 2015.\n\nElasticfusion: Dense slam without a pose graph. T Whelan, S Leutenegger, R Salas-Moreno, B Glocker, A Davison, Robotics: Science and Systems. T. Whelan, S. Leutenegger, R. Salas-Moreno, B. Glocker, and A. Davison. Elasticfusion: Dense slam without a pose graph. Robotics: Science and Systems, 2015.\n\nD3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. N Yang, L V Stumberg, R Wang, D Cremers, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionN. Yang, L. v. Stumberg, R. Wang, and D. Cremers. D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1281-1292, 2020.\n\nDeep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. N Yang, R Wang, J Stuckler, D Cremers, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)N. Yang, R. Wang, J. Stuckler, and D. Cremers. Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. In Proceedings of the European Conference on Computer Vision (ECCV), pages 817-833, 2018.\n\nDeeptam: Deep tracking and mapping. H Zhou, B Ummenhofer, T Brox, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)H. Zhou, B. Ummenhofer, and T. Brox. Deeptam: Deep tracking and mapping. In Proceedings of the European conference on computer vision (ECCV), pages 822-838, 2018.\n\nDirect sparse mapping. J Zubizarreta, I Aguinaga, J M M Montiel, IEEE Transactions on Robotics. 364J. Zubizarreta, I. Aguinaga, and J. M. M. Montiel. Direct sparse mapping. IEEE Transactions on Robotics, 36(4):1363-1370, 2020.\n", "annotations": {"author": "[{\"end\":129,\"start\":73},{\"end\":184,\"start\":130}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":81},{\"end\":138,\"start\":134}]", "author_first_name": "[{\"end\":80,\"start\":73},{\"end\":133,\"start\":130}]", "author_affiliation": "[{\"end\":128,\"start\":107},{\"end\":183,\"start\":162}]", "title": "[{\"end\":70,\"start\":1},{\"end\":254,\"start\":185}]", "venue": null, "abstract": "[{\"end\":786,\"start\":256}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1413,\"start\":1409},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1416,\"start\":1413},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1479,\"start\":1475},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1482,\"start\":1479},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1871,\"start\":1868},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1985,\"start\":1982},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1988,\"start\":1985},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1991,\"start\":1988},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1993,\"start\":1991},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2395,\"start\":2391},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2397,\"start\":2395},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2400,\"start\":2397},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2403,\"start\":2400},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2406,\"start\":2403},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2444,\"start\":2440},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2446,\"start\":2444},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2448,\"start\":2446},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2451,\"start\":2448},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2454,\"start\":2451},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2457,\"start\":2454},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2460,\"start\":2457},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":2538,\"start\":2534},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2541,\"start\":2538},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2607,\"start\":2603},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2610,\"start\":2607},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2613,\"start\":2610},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2616,\"start\":2613},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2619,\"start\":2616},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3184,\"start\":3180},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3342,\"start\":3338},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3476,\"start\":3473},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3717,\"start\":3713},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4727,\"start\":4723},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5674,\"start\":5670},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5690,\"start\":5686},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6726,\"start\":6723},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6897,\"start\":6893},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6923,\"start\":6919},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6926,\"start\":6923},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6928,\"start\":6926},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6931,\"start\":6928},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7382,\"start\":7378},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7385,\"start\":7382},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7388,\"start\":7385},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7525,\"start\":7521},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8742,\"start\":8738},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8744,\"start\":8742},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8747,\"start\":8744},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8750,\"start\":8747},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8753,\"start\":8750},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8798,\"start\":8794},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8801,\"start\":8798},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8824,\"start\":8820},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8827,\"start\":8824},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8843,\"start\":8839},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9008,\"start\":9004},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9171,\"start\":9167},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9174,\"start\":9171},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9176,\"start\":9174},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9179,\"start\":9176},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9182,\"start\":9179},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9185,\"start\":9182},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9188,\"start\":9185},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9302,\"start\":9299},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9305,\"start\":9302},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9308,\"start\":9305},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9334,\"start\":9330},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9337,\"start\":9334},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9340,\"start\":9337},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9567,\"start\":9563},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9935,\"start\":9932},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10011,\"start\":10008},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10152,\"start\":10148},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11801,\"start\":11797},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12042,\"start\":12038},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12639,\"start\":12635},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18322,\"start\":18318},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23084,\"start\":23080},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23091,\"start\":23087},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23105,\"start\":23101},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23117,\"start\":23114},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23134,\"start\":23130},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24652,\"start\":24648},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24655,\"start\":24652},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24658,\"start\":24655},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24712,\"start\":24708},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24769,\"start\":24765},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25025,\"start\":25021},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25155,\"start\":25151},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25555,\"start\":25551},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25748,\"start\":25744},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":25905,\"start\":25901},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25937,\"start\":25933},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26417,\"start\":26413},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26434,\"start\":26430},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26451,\"start\":26448},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26524,\"start\":26521},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26527,\"start\":26524},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":26530,\"start\":26527},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26750,\"start\":26746},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27034,\"start\":27030},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27805,\"start\":27802},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27874,\"start\":27870},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27926,\"start\":27922},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27957,\"start\":27953},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27982,\"start\":27978},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28008,\"start\":28004},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28036,\"start\":28032},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28064,\"start\":28060},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28085,\"start\":28081},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29434,\"start\":29430},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30299,\"start\":30296},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30483,\"start\":30480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33062,\"start\":32885},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33252,\"start\":33063},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33355,\"start\":33253},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33705,\"start\":33356},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34296,\"start\":33706},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34362,\"start\":34297},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34420,\"start\":34363},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36482,\"start\":34421},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36575,\"start\":36483},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36630,\"start\":36576}]", "paragraph": "[{\"end\":1263,\"start\":802},{\"end\":2235,\"start\":1265},{\"end\":2757,\"start\":2237},{\"end\":3040,\"start\":2759},{\"end\":3779,\"start\":3042},{\"end\":4041,\"start\":3781},{\"end\":4372,\"start\":4043},{\"end\":4781,\"start\":4374},{\"end\":5124,\"start\":4783},{\"end\":5584,\"start\":5126},{\"end\":6363,\"start\":5586},{\"end\":6623,\"start\":6365},{\"end\":6727,\"start\":6640},{\"end\":7268,\"start\":6729},{\"end\":7878,\"start\":7270},{\"end\":8572,\"start\":7880},{\"end\":9104,\"start\":8574},{\"end\":9918,\"start\":9106},{\"end\":10448,\"start\":9920},{\"end\":10704,\"start\":10461},{\"end\":11194,\"start\":10706},{\"end\":11641,\"start\":11196},{\"end\":11802,\"start\":11680},{\"end\":12288,\"start\":11804},{\"end\":12482,\"start\":12290},{\"end\":12678,\"start\":12536},{\"end\":12799,\"start\":12701},{\"end\":13138,\"start\":12841},{\"end\":13597,\"start\":13158},{\"end\":13798,\"start\":13661},{\"end\":14100,\"start\":13869},{\"end\":14571,\"start\":14183},{\"end\":15050,\"start\":14573},{\"end\":15409,\"start\":15052},{\"end\":15668,\"start\":15411},{\"end\":16128,\"start\":15670},{\"end\":16991,\"start\":16130},{\"end\":17203,\"start\":16993},{\"end\":17952,\"start\":17294},{\"end\":18229,\"start\":18040},{\"end\":18391,\"start\":18242},{\"end\":18976,\"start\":18393},{\"end\":19200,\"start\":18978},{\"end\":19697,\"start\":19202},{\"end\":20041,\"start\":19699},{\"end\":20202,\"start\":20043},{\"end\":20351,\"start\":20247},{\"end\":20908,\"start\":20367},{\"end\":21346,\"start\":20910},{\"end\":21988,\"start\":21348},{\"end\":22252,\"start\":21990},{\"end\":22818,\"start\":22254},{\"end\":23162,\"start\":22820},{\"end\":23465,\"start\":23164},{\"end\":23781,\"start\":23467},{\"end\":24360,\"start\":23783},{\"end\":25061,\"start\":24376},{\"end\":25334,\"start\":25063},{\"end\":25611,\"start\":25374},{\"end\":26256,\"start\":25613},{\"end\":27019,\"start\":26258},{\"end\":27309,\"start\":27021},{\"end\":27678,\"start\":27311},{\"end\":27875,\"start\":27680},{\"end\":28639,\"start\":27909},{\"end\":28731,\"start\":28641},{\"end\":29435,\"start\":28733},{\"end\":29986,\"start\":29437},{\"end\":30517,\"start\":30001},{\"end\":30625,\"start\":30556},{\"end\":30846,\"start\":30658},{\"end\":30982,\"start\":30900},{\"end\":31143,\"start\":31080},{\"end\":31386,\"start\":31145},{\"end\":31789,\"start\":31492},{\"end\":31878,\"start\":31821},{\"end\":32161,\"start\":32052},{\"end\":32859,\"start\":32163}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12535,\"start\":12483},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12840,\"start\":12800},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13660,\"start\":13598},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13868,\"start\":13799},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14182,\"start\":14101},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17293,\"start\":17204},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18039,\"start\":17953},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20246,\"start\":20203},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30899,\"start\":30847},{\"attributes\":{\"id\":\"formula_9\"},\"end\":31079,\"start\":30983},{\"attributes\":{\"id\":\"formula_10\"},\"end\":31491,\"start\":31387},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31820,\"start\":31790},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32051,\"start\":31879}]", "table_ref": "[{\"end\":26049,\"start\":26046},{\"end\":27511,\"start\":27504}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":800,\"start\":788},{\"attributes\":{\"n\":\"2\"},\"end\":6638,\"start\":6626},{\"attributes\":{\"n\":\"3\"},\"end\":10459,\"start\":10451},{\"attributes\":{\"n\":\"3.1\"},\"end\":11678,\"start\":11644},{\"end\":12699,\"start\":12681},{\"attributes\":{\"n\":\"3.2\"},\"end\":13156,\"start\":13141},{\"attributes\":{\"n\":\"3.3\"},\"end\":18240,\"start\":18232},{\"attributes\":{\"n\":\"3.4\"},\"end\":20365,\"start\":20354},{\"attributes\":{\"n\":\"4\"},\"end\":24374,\"start\":24363},{\"end\":25372,\"start\":25337},{\"end\":27907,\"start\":27878},{\"attributes\":{\"n\":\"5\"},\"end\":29999,\"start\":29989},{\"end\":30540,\"start\":30520},{\"end\":30554,\"start\":30543},{\"end\":30656,\"start\":30628},{\"end\":32884,\"start\":32862},{\"end\":32896,\"start\":32886},{\"end\":33074,\"start\":33064},{\"end\":33264,\"start\":33254},{\"end\":33367,\"start\":33357},{\"end\":33737,\"start\":33707},{\"end\":34373,\"start\":34364},{\"end\":34431,\"start\":34422},{\"end\":36493,\"start\":36484},{\"end\":36586,\"start\":36577}]", "table": "[{\"end\":36482,\"start\":34433}]", "figure_caption": "[{\"end\":33062,\"start\":32898},{\"end\":33252,\"start\":33076},{\"end\":33355,\"start\":33266},{\"end\":33705,\"start\":33369},{\"end\":34296,\"start\":33741},{\"end\":34362,\"start\":34299},{\"end\":34420,\"start\":34375},{\"end\":36575,\"start\":36495},{\"end\":36630,\"start\":36588}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13239,\"start\":13233},{\"end\":32241,\"start\":32233},{\"end\":32642,\"start\":32634}]", "bib_author_first_name": "[{\"end\":37786,\"start\":37785},{\"end\":37797,\"start\":37796},{\"end\":37811,\"start\":37810},{\"end\":37820,\"start\":37819},{\"end\":37835,\"start\":37834},{\"end\":37837,\"start\":37836},{\"end\":38287,\"start\":38286},{\"end\":38296,\"start\":38295},{\"end\":38307,\"start\":38306},{\"end\":38315,\"start\":38314},{\"end\":38328,\"start\":38327},{\"end\":38338,\"start\":38337},{\"end\":38347,\"start\":38346},{\"end\":38349,\"start\":38348},{\"end\":38361,\"start\":38360},{\"end\":38700,\"start\":38699},{\"end\":38702,\"start\":38701},{\"end\":38712,\"start\":38711},{\"end\":38721,\"start\":38720},{\"end\":38723,\"start\":38722},{\"end\":38734,\"start\":38733},{\"end\":38736,\"start\":38735},{\"end\":39201,\"start\":39200},{\"end\":39211,\"start\":39210},{\"end\":39222,\"start\":39221},{\"end\":39234,\"start\":39233},{\"end\":39243,\"start\":39242},{\"end\":39257,\"start\":39256},{\"end\":39266,\"start\":39265},{\"end\":39274,\"start\":39273},{\"end\":39276,\"start\":39275},{\"end\":39668,\"start\":39667},{\"end\":39678,\"start\":39677},{\"end\":39688,\"start\":39687},{\"end\":39692,\"start\":39689},{\"end\":39705,\"start\":39704},{\"end\":39707,\"start\":39706},{\"end\":39718,\"start\":39717},{\"end\":39720,\"start\":39719},{\"end\":40019,\"start\":40018},{\"end\":40029,\"start\":40028},{\"end\":40038,\"start\":40037},{\"end\":40331,\"start\":40330},{\"end\":40333,\"start\":40332},{\"end\":40341,\"start\":40340},{\"end\":40349,\"start\":40348},{\"end\":40361,\"start\":40360},{\"end\":40638,\"start\":40637},{\"end\":40647,\"start\":40646},{\"end\":40658,\"start\":40657},{\"end\":40672,\"start\":40671},{\"end\":40687,\"start\":40686},{\"end\":40689,\"start\":40688},{\"end\":40973,\"start\":40972},{\"end\":40987,\"start\":40986},{\"end\":40998,\"start\":40997},{\"end\":41007,\"start\":41006},{\"end\":41009,\"start\":41008},{\"end\":41297,\"start\":41296},{\"end\":41304,\"start\":41303},{\"end\":41306,\"start\":41305},{\"end\":41315,\"start\":41314},{\"end\":41324,\"start\":41323},{\"end\":41334,\"start\":41333},{\"end\":41348,\"start\":41347},{\"end\":41844,\"start\":41843},{\"end\":41851,\"start\":41850},{\"end\":41862,\"start\":41861},{\"end\":41875,\"start\":41874},{\"end\":41884,\"start\":41883},{\"end\":42191,\"start\":42190},{\"end\":42193,\"start\":42192},{\"end\":42204,\"start\":42203},{\"end\":42206,\"start\":42205},{\"end\":42214,\"start\":42213},{\"end\":42216,\"start\":42215},{\"end\":42226,\"start\":42225},{\"end\":42556,\"start\":42555},{\"end\":42566,\"start\":42565},{\"end\":42581,\"start\":42580},{\"end\":43037,\"start\":43036},{\"end\":43048,\"start\":43047},{\"end\":43050,\"start\":43049},{\"end\":43065,\"start\":43064},{\"end\":43387,\"start\":43386},{\"end\":43396,\"start\":43395},{\"end\":43406,\"start\":43405},{\"end\":43675,\"start\":43674},{\"end\":43684,\"start\":43683},{\"end\":43694,\"start\":43693},{\"end\":43989,\"start\":43988},{\"end\":44000,\"start\":43999},{\"end\":44009,\"start\":44008},{\"end\":44018,\"start\":44017},{\"end\":44032,\"start\":44031},{\"end\":44035,\"start\":44033},{\"end\":44362,\"start\":44361},{\"end\":44373,\"start\":44372},{\"end\":44382,\"start\":44381},{\"end\":44393,\"start\":44392},{\"end\":44407,\"start\":44406},{\"end\":44740,\"start\":44739},{\"end\":44752,\"start\":44751},{\"end\":44942,\"start\":44941},{\"end\":44950,\"start\":44949},{\"end\":44959,\"start\":44958},{\"end\":45288,\"start\":45287},{\"end\":45301,\"start\":45300},{\"end\":45312,\"start\":45308},{\"end\":45320,\"start\":45319},{\"end\":45582,\"start\":45581},{\"end\":45590,\"start\":45589},{\"end\":45601,\"start\":45597},{\"end\":45757,\"start\":45756},{\"end\":45765,\"start\":45758},{\"end\":45775,\"start\":45774},{\"end\":45787,\"start\":45786},{\"end\":45795,\"start\":45794},{\"end\":46057,\"start\":46056},{\"end\":46064,\"start\":46063},{\"end\":46070,\"start\":46069},{\"end\":46077,\"start\":46076},{\"end\":46079,\"start\":46078},{\"end\":46093,\"start\":46092},{\"end\":46522,\"start\":46521},{\"end\":46532,\"start\":46528},{\"end\":46541,\"start\":46540},{\"end\":46553,\"start\":46552},{\"end\":46563,\"start\":46562},{\"end\":46831,\"start\":46830},{\"end\":46838,\"start\":46837},{\"end\":46846,\"start\":46845},{\"end\":46854,\"start\":46853},{\"end\":46861,\"start\":46860},{\"end\":46870,\"start\":46869},{\"end\":46877,\"start\":46876},{\"end\":46885,\"start\":46884},{\"end\":47312,\"start\":47311},{\"end\":47323,\"start\":47322},{\"end\":47335,\"start\":47334},{\"end\":47345,\"start\":47344},{\"end\":47357,\"start\":47356},{\"end\":47676,\"start\":47675},{\"end\":47683,\"start\":47682},{\"end\":47691,\"start\":47690},{\"end\":48138,\"start\":48137},{\"end\":48150,\"start\":48149},{\"end\":48161,\"start\":48160},{\"end\":48174,\"start\":48173},{\"end\":48461,\"start\":48460},{\"end\":48463,\"start\":48462},{\"end\":48475,\"start\":48474},{\"end\":48477,\"start\":48476},{\"end\":48913,\"start\":48912},{\"end\":48926,\"start\":48925},{\"end\":48930,\"start\":48927},{\"end\":48941,\"start\":48940},{\"end\":48943,\"start\":48942},{\"end\":49229,\"start\":49228},{\"end\":49242,\"start\":49241},{\"end\":49244,\"start\":49243},{\"end\":49514,\"start\":49513},{\"end\":49516,\"start\":49515},{\"end\":49540,\"start\":49535},{\"end\":49549,\"start\":49548},{\"end\":49739,\"start\":49738},{\"end\":49741,\"start\":49740},{\"end\":49753,\"start\":49752},{\"end\":49755,\"start\":49754},{\"end\":49768,\"start\":49767},{\"end\":49770,\"start\":49769},{\"end\":50017,\"start\":50016},{\"end\":50024,\"start\":50023},{\"end\":50034,\"start\":50033},{\"end\":50041,\"start\":50040},{\"end\":50043,\"start\":50042},{\"end\":50317,\"start\":50316},{\"end\":50324,\"start\":50323},{\"end\":50649,\"start\":50648},{\"end\":50659,\"start\":50658},{\"end\":51022,\"start\":51021},{\"end\":51033,\"start\":51032},{\"end\":51042,\"start\":51041},{\"end\":51051,\"start\":51050},{\"end\":51443,\"start\":51439},{\"end\":51453,\"start\":51452},{\"end\":51463,\"start\":51462},{\"end\":51478,\"start\":51477},{\"end\":51957,\"start\":51953},{\"end\":51967,\"start\":51966},{\"end\":51977,\"start\":51976},{\"end\":51988,\"start\":51987},{\"end\":51999,\"start\":51998},{\"end\":52007,\"start\":52006},{\"end\":52018,\"start\":52017},{\"end\":52031,\"start\":52030},{\"end\":52042,\"start\":52041},{\"end\":52058,\"start\":52057},{\"end\":52378,\"start\":52377},{\"end\":52380,\"start\":52379},{\"end\":52398,\"start\":52394},{\"end\":52767,\"start\":52766},{\"end\":52777,\"start\":52776},{\"end\":52788,\"start\":52787},{\"end\":53189,\"start\":53188},{\"end\":53201,\"start\":53200},{\"end\":53211,\"start\":53210},{\"end\":53221,\"start\":53220},{\"end\":53233,\"start\":53232},{\"end\":53613,\"start\":53612},{\"end\":53622,\"start\":53621},{\"end\":53635,\"start\":53634},{\"end\":53645,\"start\":53644},{\"end\":53656,\"start\":53655},{\"end\":53967,\"start\":53966},{\"end\":53976,\"start\":53975},{\"end\":53983,\"start\":53982},{\"end\":53992,\"start\":53991},{\"end\":53994,\"start\":53993},{\"end\":54301,\"start\":54300},{\"end\":54310,\"start\":54309},{\"end\":54318,\"start\":54317},{\"end\":54574,\"start\":54573},{\"end\":54582,\"start\":54581},{\"end\":54828,\"start\":54827},{\"end\":54836,\"start\":54835},{\"end\":55063,\"start\":55062},{\"end\":55071,\"start\":55070},{\"end\":55344,\"start\":55343},{\"end\":55352,\"start\":55351},{\"end\":55624,\"start\":55623},{\"end\":55638,\"start\":55637},{\"end\":55646,\"start\":55645},{\"end\":55655,\"start\":55654},{\"end\":55664,\"start\":55663},{\"end\":55671,\"start\":55670},{\"end\":55686,\"start\":55685},{\"end\":56085,\"start\":56084},{\"end\":56097,\"start\":56096},{\"end\":56107,\"start\":56106},{\"end\":56115,\"start\":56114},{\"end\":56475,\"start\":56474},{\"end\":56483,\"start\":56482},{\"end\":56492,\"start\":56491},{\"end\":56499,\"start\":56498},{\"end\":56826,\"start\":56825},{\"end\":56834,\"start\":56833},{\"end\":56840,\"start\":56839},{\"end\":57043,\"start\":57042},{\"end\":57051,\"start\":57050},{\"end\":57058,\"start\":57057},{\"end\":57066,\"start\":57065},{\"end\":57072,\"start\":57071},{\"end\":57079,\"start\":57078},{\"end\":57087,\"start\":57086},{\"end\":57093,\"start\":57092},{\"end\":57103,\"start\":57102},{\"end\":57443,\"start\":57442},{\"end\":57453,\"start\":57452},{\"end\":57462,\"start\":57461},{\"end\":57476,\"start\":57475},{\"end\":57486,\"start\":57485},{\"end\":57488,\"start\":57487},{\"end\":57499,\"start\":57498},{\"end\":57826,\"start\":57825},{\"end\":57836,\"start\":57835},{\"end\":57851,\"start\":57850},{\"end\":57867,\"start\":57866},{\"end\":57878,\"start\":57877},{\"end\":58158,\"start\":58157},{\"end\":58166,\"start\":58165},{\"end\":58168,\"start\":58167},{\"end\":58180,\"start\":58179},{\"end\":58188,\"start\":58187},{\"end\":58689,\"start\":58688},{\"end\":58697,\"start\":58696},{\"end\":58705,\"start\":58704},{\"end\":58717,\"start\":58716},{\"end\":59118,\"start\":59117},{\"end\":59126,\"start\":59125},{\"end\":59140,\"start\":59139},{\"end\":59450,\"start\":59449},{\"end\":59465,\"start\":59464},{\"end\":59477,\"start\":59476},{\"end\":59481,\"start\":59478}]", "bib_author_last_name": "[{\"end\":37794,\"start\":37787},{\"end\":37808,\"start\":37798},{\"end\":37817,\"start\":37812},{\"end\":37832,\"start\":37821},{\"end\":37845,\"start\":37838},{\"end\":38293,\"start\":38288},{\"end\":38304,\"start\":38297},{\"end\":38312,\"start\":38308},{\"end\":38325,\"start\":38316},{\"end\":38335,\"start\":38329},{\"end\":38344,\"start\":38339},{\"end\":38358,\"start\":38350},{\"end\":38370,\"start\":38362},{\"end\":38709,\"start\":38703},{\"end\":38718,\"start\":38713},{\"end\":38731,\"start\":38724},{\"end\":38742,\"start\":38737},{\"end\":39208,\"start\":39202},{\"end\":39219,\"start\":39212},{\"end\":39231,\"start\":39223},{\"end\":39240,\"start\":39235},{\"end\":39254,\"start\":39244},{\"end\":39263,\"start\":39258},{\"end\":39271,\"start\":39267},{\"end\":39284,\"start\":39277},{\"end\":39675,\"start\":39669},{\"end\":39685,\"start\":39679},{\"end\":39702,\"start\":39693},{\"end\":39715,\"start\":39708},{\"end\":39727,\"start\":39721},{\"end\":40026,\"start\":40020},{\"end\":40035,\"start\":40030},{\"end\":40046,\"start\":40039},{\"end\":40338,\"start\":40334},{\"end\":40346,\"start\":40342},{\"end\":40358,\"start\":40350},{\"end\":40372,\"start\":40362},{\"end\":40644,\"start\":40639},{\"end\":40655,\"start\":40648},{\"end\":40669,\"start\":40659},{\"end\":40684,\"start\":40673},{\"end\":40697,\"start\":40690},{\"end\":40984,\"start\":40974},{\"end\":40995,\"start\":40988},{\"end\":41004,\"start\":40999},{\"end\":41017,\"start\":41010},{\"end\":41301,\"start\":41298},{\"end\":41312,\"start\":41307},{\"end\":41321,\"start\":41316},{\"end\":41331,\"start\":41325},{\"end\":41345,\"start\":41335},{\"end\":41356,\"start\":41349},{\"end\":41848,\"start\":41845},{\"end\":41859,\"start\":41852},{\"end\":41872,\"start\":41863},{\"end\":41881,\"start\":41876},{\"end\":41893,\"start\":41885},{\"end\":42201,\"start\":42194},{\"end\":42211,\"start\":42207},{\"end\":42223,\"start\":42217},{\"end\":42233,\"start\":42227},{\"end\":42563,\"start\":42557},{\"end\":42578,\"start\":42567},{\"end\":42592,\"start\":42582},{\"end\":43045,\"start\":43038},{\"end\":43062,\"start\":43051},{\"end\":43075,\"start\":43066},{\"end\":43393,\"start\":43388},{\"end\":43403,\"start\":43397},{\"end\":43414,\"start\":43407},{\"end\":43681,\"start\":43676},{\"end\":43691,\"start\":43685},{\"end\":43702,\"start\":43695},{\"end\":43997,\"start\":43990},{\"end\":44006,\"start\":44001},{\"end\":44015,\"start\":44010},{\"end\":44029,\"start\":44019},{\"end\":44045,\"start\":44036},{\"end\":44370,\"start\":44363},{\"end\":44379,\"start\":44374},{\"end\":44390,\"start\":44383},{\"end\":44404,\"start\":44394},{\"end\":44418,\"start\":44408},{\"end\":44423,\"start\":44420},{\"end\":44749,\"start\":44741},{\"end\":44762,\"start\":44753},{\"end\":44947,\"start\":44943},{\"end\":44956,\"start\":44951},{\"end\":44967,\"start\":44960},{\"end\":45298,\"start\":45289},{\"end\":45306,\"start\":45302},{\"end\":45317,\"start\":45313},{\"end\":45327,\"start\":45321},{\"end\":45587,\"start\":45583},{\"end\":45595,\"start\":45591},{\"end\":45607,\"start\":45602},{\"end\":45772,\"start\":45766},{\"end\":45784,\"start\":45776},{\"end\":45792,\"start\":45788},{\"end\":45801,\"start\":45796},{\"end\":46061,\"start\":46058},{\"end\":46067,\"start\":46065},{\"end\":46074,\"start\":46071},{\"end\":46090,\"start\":46080},{\"end\":46099,\"start\":46094},{\"end\":46526,\"start\":46523},{\"end\":46538,\"start\":46533},{\"end\":46550,\"start\":46542},{\"end\":46560,\"start\":46554},{\"end\":46568,\"start\":46564},{\"end\":46835,\"start\":46832},{\"end\":46843,\"start\":46839},{\"end\":46851,\"start\":46847},{\"end\":46858,\"start\":46855},{\"end\":46867,\"start\":46862},{\"end\":46874,\"start\":46871},{\"end\":46882,\"start\":46878},{\"end\":46890,\"start\":46886},{\"end\":47320,\"start\":47313},{\"end\":47332,\"start\":47324},{\"end\":47342,\"start\":47336},{\"end\":47354,\"start\":47346},{\"end\":47365,\"start\":47358},{\"end\":47680,\"start\":47677},{\"end\":47688,\"start\":47684},{\"end\":47696,\"start\":47692},{\"end\":48147,\"start\":48139},{\"end\":48158,\"start\":48151},{\"end\":48171,\"start\":48162},{\"end\":48180,\"start\":48175},{\"end\":48472,\"start\":48464},{\"end\":48489,\"start\":48478},{\"end\":48923,\"start\":48914},{\"end\":48938,\"start\":48931},{\"end\":48950,\"start\":48944},{\"end\":49239,\"start\":49230},{\"end\":49251,\"start\":49245},{\"end\":49533,\"start\":49517},{\"end\":49546,\"start\":49541},{\"end\":49556,\"start\":49550},{\"end\":49750,\"start\":49742},{\"end\":49765,\"start\":49756},{\"end\":49778,\"start\":49771},{\"end\":50021,\"start\":50018},{\"end\":50031,\"start\":50025},{\"end\":50038,\"start\":50035},{\"end\":50046,\"start\":50044},{\"end\":50321,\"start\":50318},{\"end\":50329,\"start\":50325},{\"end\":50656,\"start\":50650},{\"end\":50666,\"start\":50660},{\"end\":51030,\"start\":51023},{\"end\":51039,\"start\":51034},{\"end\":51048,\"start\":51043},{\"end\":51059,\"start\":51052},{\"end\":51450,\"start\":51444},{\"end\":51460,\"start\":51454},{\"end\":51475,\"start\":51464},{\"end\":51489,\"start\":51479},{\"end\":51964,\"start\":51958},{\"end\":51974,\"start\":51968},{\"end\":51985,\"start\":51978},{\"end\":51996,\"start\":51989},{\"end\":52004,\"start\":52000},{\"end\":52015,\"start\":52008},{\"end\":52028,\"start\":52019},{\"end\":52039,\"start\":52032},{\"end\":52055,\"start\":52043},{\"end\":52063,\"start\":52059},{\"end\":52392,\"start\":52381},{\"end\":52404,\"start\":52399},{\"end\":52774,\"start\":52768},{\"end\":52785,\"start\":52778},{\"end\":52798,\"start\":52789},{\"end\":53198,\"start\":53190},{\"end\":53208,\"start\":53202},{\"end\":53218,\"start\":53212},{\"end\":53230,\"start\":53222},{\"end\":53241,\"start\":53234},{\"end\":53619,\"start\":53614},{\"end\":53632,\"start\":53623},{\"end\":53642,\"start\":53636},{\"end\":53653,\"start\":53646},{\"end\":53664,\"start\":53657},{\"end\":53973,\"start\":53968},{\"end\":53980,\"start\":53977},{\"end\":53989,\"start\":53984},{\"end\":54002,\"start\":53995},{\"end\":54307,\"start\":54302},{\"end\":54315,\"start\":54311},{\"end\":54326,\"start\":54319},{\"end\":54579,\"start\":54575},{\"end\":54586,\"start\":54583},{\"end\":54833,\"start\":54829},{\"end\":54841,\"start\":54837},{\"end\":55068,\"start\":55064},{\"end\":55076,\"start\":55072},{\"end\":55349,\"start\":55345},{\"end\":55357,\"start\":55353},{\"end\":55635,\"start\":55625},{\"end\":55643,\"start\":55639},{\"end\":55652,\"start\":55647},{\"end\":55661,\"start\":55656},{\"end\":55668,\"start\":55665},{\"end\":55683,\"start\":55672},{\"end\":55691,\"start\":55687},{\"end\":56094,\"start\":56086},{\"end\":56104,\"start\":56098},{\"end\":56112,\"start\":56108},{\"end\":56123,\"start\":56116},{\"end\":56480,\"start\":56476},{\"end\":56489,\"start\":56484},{\"end\":56496,\"start\":56493},{\"end\":56507,\"start\":56500},{\"end\":56831,\"start\":56827},{\"end\":56837,\"start\":56835},{\"end\":56848,\"start\":56841},{\"end\":57048,\"start\":57044},{\"end\":57055,\"start\":57052},{\"end\":57063,\"start\":57059},{\"end\":57069,\"start\":57067},{\"end\":57076,\"start\":57073},{\"end\":57084,\"start\":57080},{\"end\":57090,\"start\":57088},{\"end\":57100,\"start\":57094},{\"end\":57111,\"start\":57104},{\"end\":57450,\"start\":57444},{\"end\":57459,\"start\":57454},{\"end\":57473,\"start\":57463},{\"end\":57483,\"start\":57477},{\"end\":57496,\"start\":57489},{\"end\":57508,\"start\":57500},{\"end\":57833,\"start\":57827},{\"end\":57848,\"start\":57837},{\"end\":57864,\"start\":57852},{\"end\":57875,\"start\":57868},{\"end\":57886,\"start\":57879},{\"end\":58163,\"start\":58159},{\"end\":58177,\"start\":58169},{\"end\":58185,\"start\":58181},{\"end\":58196,\"start\":58189},{\"end\":58694,\"start\":58690},{\"end\":58702,\"start\":58698},{\"end\":58714,\"start\":58706},{\"end\":58725,\"start\":58718},{\"end\":59123,\"start\":59119},{\"end\":59137,\"start\":59127},{\"end\":59145,\"start\":59141},{\"end\":59462,\"start\":59451},{\"end\":59474,\"start\":59466},{\"end\":59489,\"start\":59482}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4624670},\"end\":38243,\"start\":37706},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9999787},\"end\":38635,\"start\":38245},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4637111},\"end\":39096,\"start\":38637},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2596787},\"end\":39574,\"start\":39098},{\"attributes\":{\"doi\":\"arXiv:2007.11898\",\"id\":\"b4\"},\"end\":39963,\"start\":39576},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":749979},\"end\":40328,\"start\":39965},{\"attributes\":{\"doi\":\"arXiv:1606.03558\",\"id\":\"b6\"},\"end\":40563,\"start\":40330},{\"attributes\":{\"doi\":\"arXiv:1809.02966\",\"id\":\"b7\"},\"end\":40911,\"start\":40565},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":210701396},\"end\":41231,\"start\":40913},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7684883},\"end\":41739,\"start\":41233},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":32286806},\"end\":42148,\"start\":41741},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206764185},\"end\":42483,\"start\":42150},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4918026},\"end\":42983,\"start\":42485},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":212747761},\"end\":43360,\"start\":42985},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3299195},\"end\":43627,\"start\":43362},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14547347},\"end\":43907,\"start\":43629},{\"attributes\":{\"id\":\"b16\"},\"end\":44293,\"start\":43909},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206775501},\"end\":44649,\"start\":44295},{\"attributes\":{\"id\":\"b18\"},\"end\":44902,\"start\":44651},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10038098},\"end\":45219,\"start\":44904},{\"attributes\":{\"id\":\"b20\"},\"end\":45537,\"start\":45221},{\"attributes\":{\"doi\":\"arXiv:2012.05901\",\"id\":\"b21\"},\"end\":45754,\"start\":45539},{\"attributes\":{\"id\":\"b22\"},\"end\":45985,\"start\":45756},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":57759271},\"end\":46484,\"start\":45987},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":216915079},\"end\":46755,\"start\":46486},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":49863735},\"end\":47243,\"start\":46757},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":51941725},\"end\":47601,\"start\":47245},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219964046},\"end\":48057,\"start\":47603},{\"attributes\":{\"doi\":\"arXiv:1705.10872\",\"id\":\"b28\"},\"end\":48381,\"start\":48059},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12751695},\"end\":48852,\"start\":48383},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206775100},\"end\":49146,\"start\":48854},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206775640},\"end\":49451,\"start\":49148},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":545361},\"end\":49689,\"start\":49453},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1336659},\"end\":50014,\"start\":49691},{\"attributes\":{\"doi\":\"arXiv:1805.09662\",\"id\":\"b34\"},\"end\":50247,\"start\":50016},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":51905906},\"end\":50610,\"start\":50249},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52036399},\"end\":50933,\"start\":50612},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":203837063},\"end\":51372,\"start\":50935},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208291327},\"end\":51873,\"start\":51374},{\"attributes\":{\"doi\":\"arXiv:2103.09213\",\"id\":\"b39\"},\"end\":52342,\"start\":51875},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1728538},\"end\":52719,\"start\":52344},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":196201321},\"end\":53141,\"start\":52721},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":51904725},\"end\":53556,\"start\":53143},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206942855},\"end\":53964,\"start\":53558},{\"attributes\":{\"doi\":\"arXiv:2103.12352\",\"id\":\"b44\"},\"end\":54225,\"start\":53966},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":215547972},\"end\":54571,\"start\":54227},{\"attributes\":{\"doi\":\"arXiv:1806.04807\",\"id\":\"b46\"},\"end\":54758,\"start\":54573},{\"attributes\":{\"doi\":\"arXiv:1812.04605\",\"id\":\"b47\"},\"end\":54999,\"start\":54760},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":214667893},\"end\":55281,\"start\":55001},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":232307351},\"end\":55558,\"start\":55283},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":6159584},\"end\":56082,\"start\":55560},{\"attributes\":{\"doi\":\"arXiv:2010.06323\",\"id\":\"b51\"},\"end\":56378,\"start\":56084},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":9114952},\"end\":56823,\"start\":56380},{\"attributes\":{\"doi\":\"arXiv:2011.00359\",\"id\":\"b53\"},\"end\":57040,\"start\":56825},{\"attributes\":{\"doi\":\"arXiv:2003.14338\",\"id\":\"b54\"},\"end\":57377,\"start\":57042},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":7604429},\"end\":57775,\"start\":57379},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":16413702},\"end\":58075,\"start\":57777},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":211677466},\"end\":58585,\"start\":58077},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":49658377},\"end\":59079,\"start\":58587},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":51929808},\"end\":59424,\"start\":59081},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":118648883},\"end\":59652,\"start\":59426}]", "bib_title": "[{\"end\":37783,\"start\":37706},{\"end\":38284,\"start\":38245},{\"end\":38697,\"start\":38637},{\"end\":39198,\"start\":39098},{\"end\":40016,\"start\":39965},{\"end\":40970,\"start\":40913},{\"end\":41294,\"start\":41233},{\"end\":41841,\"start\":41741},{\"end\":42188,\"start\":42150},{\"end\":42553,\"start\":42485},{\"end\":43034,\"start\":42985},{\"end\":43384,\"start\":43362},{\"end\":43672,\"start\":43629},{\"end\":43986,\"start\":43909},{\"end\":44359,\"start\":44295},{\"end\":44939,\"start\":44904},{\"end\":45285,\"start\":45221},{\"end\":46054,\"start\":45987},{\"end\":46519,\"start\":46486},{\"end\":46828,\"start\":46757},{\"end\":47309,\"start\":47245},{\"end\":47673,\"start\":47603},{\"end\":48458,\"start\":48383},{\"end\":48910,\"start\":48854},{\"end\":49226,\"start\":49148},{\"end\":49511,\"start\":49453},{\"end\":49736,\"start\":49691},{\"end\":50314,\"start\":50249},{\"end\":50646,\"start\":50612},{\"end\":51019,\"start\":50935},{\"end\":51437,\"start\":51374},{\"end\":52375,\"start\":52344},{\"end\":52764,\"start\":52721},{\"end\":53186,\"start\":53143},{\"end\":53610,\"start\":53558},{\"end\":54298,\"start\":54227},{\"end\":55060,\"start\":55001},{\"end\":55341,\"start\":55283},{\"end\":55621,\"start\":55560},{\"end\":56472,\"start\":56380},{\"end\":57440,\"start\":57379},{\"end\":57823,\"start\":57777},{\"end\":58155,\"start\":58077},{\"end\":58686,\"start\":58587},{\"end\":59115,\"start\":59081},{\"end\":59447,\"start\":59426}]", "bib_author": "[{\"end\":37796,\"start\":37785},{\"end\":37810,\"start\":37796},{\"end\":37819,\"start\":37810},{\"end\":37834,\"start\":37819},{\"end\":37847,\"start\":37834},{\"end\":38295,\"start\":38286},{\"end\":38306,\"start\":38295},{\"end\":38314,\"start\":38306},{\"end\":38327,\"start\":38314},{\"end\":38337,\"start\":38327},{\"end\":38346,\"start\":38337},{\"end\":38360,\"start\":38346},{\"end\":38372,\"start\":38360},{\"end\":38711,\"start\":38699},{\"end\":38720,\"start\":38711},{\"end\":38733,\"start\":38720},{\"end\":38744,\"start\":38733},{\"end\":39210,\"start\":39200},{\"end\":39221,\"start\":39210},{\"end\":39233,\"start\":39221},{\"end\":39242,\"start\":39233},{\"end\":39256,\"start\":39242},{\"end\":39265,\"start\":39256},{\"end\":39273,\"start\":39265},{\"end\":39286,\"start\":39273},{\"end\":39677,\"start\":39667},{\"end\":39687,\"start\":39677},{\"end\":39704,\"start\":39687},{\"end\":39717,\"start\":39704},{\"end\":39729,\"start\":39717},{\"end\":40028,\"start\":40018},{\"end\":40037,\"start\":40028},{\"end\":40048,\"start\":40037},{\"end\":40340,\"start\":40330},{\"end\":40348,\"start\":40340},{\"end\":40360,\"start\":40348},{\"end\":40374,\"start\":40360},{\"end\":40646,\"start\":40637},{\"end\":40657,\"start\":40646},{\"end\":40671,\"start\":40657},{\"end\":40686,\"start\":40671},{\"end\":40699,\"start\":40686},{\"end\":40986,\"start\":40972},{\"end\":40997,\"start\":40986},{\"end\":41006,\"start\":40997},{\"end\":41019,\"start\":41006},{\"end\":41303,\"start\":41296},{\"end\":41314,\"start\":41303},{\"end\":41323,\"start\":41314},{\"end\":41333,\"start\":41323},{\"end\":41347,\"start\":41333},{\"end\":41358,\"start\":41347},{\"end\":41850,\"start\":41843},{\"end\":41861,\"start\":41850},{\"end\":41874,\"start\":41861},{\"end\":41883,\"start\":41874},{\"end\":41895,\"start\":41883},{\"end\":42203,\"start\":42190},{\"end\":42213,\"start\":42203},{\"end\":42225,\"start\":42213},{\"end\":42235,\"start\":42225},{\"end\":42565,\"start\":42555},{\"end\":42580,\"start\":42565},{\"end\":42594,\"start\":42580},{\"end\":43047,\"start\":43036},{\"end\":43064,\"start\":43047},{\"end\":43077,\"start\":43064},{\"end\":43395,\"start\":43386},{\"end\":43405,\"start\":43395},{\"end\":43416,\"start\":43405},{\"end\":43683,\"start\":43674},{\"end\":43693,\"start\":43683},{\"end\":43704,\"start\":43693},{\"end\":43999,\"start\":43988},{\"end\":44008,\"start\":43999},{\"end\":44017,\"start\":44008},{\"end\":44031,\"start\":44017},{\"end\":44047,\"start\":44031},{\"end\":44372,\"start\":44361},{\"end\":44381,\"start\":44372},{\"end\":44392,\"start\":44381},{\"end\":44406,\"start\":44392},{\"end\":44420,\"start\":44406},{\"end\":44425,\"start\":44420},{\"end\":44751,\"start\":44739},{\"end\":44764,\"start\":44751},{\"end\":44949,\"start\":44941},{\"end\":44958,\"start\":44949},{\"end\":44969,\"start\":44958},{\"end\":45300,\"start\":45287},{\"end\":45308,\"start\":45300},{\"end\":45319,\"start\":45308},{\"end\":45329,\"start\":45319},{\"end\":45589,\"start\":45581},{\"end\":45597,\"start\":45589},{\"end\":45609,\"start\":45597},{\"end\":45774,\"start\":45756},{\"end\":45786,\"start\":45774},{\"end\":45794,\"start\":45786},{\"end\":45803,\"start\":45794},{\"end\":46063,\"start\":46056},{\"end\":46069,\"start\":46063},{\"end\":46076,\"start\":46069},{\"end\":46092,\"start\":46076},{\"end\":46101,\"start\":46092},{\"end\":46528,\"start\":46521},{\"end\":46540,\"start\":46528},{\"end\":46552,\"start\":46540},{\"end\":46562,\"start\":46552},{\"end\":46570,\"start\":46562},{\"end\":46837,\"start\":46830},{\"end\":46845,\"start\":46837},{\"end\":46853,\"start\":46845},{\"end\":46860,\"start\":46853},{\"end\":46869,\"start\":46860},{\"end\":46876,\"start\":46869},{\"end\":46884,\"start\":46876},{\"end\":46892,\"start\":46884},{\"end\":47322,\"start\":47311},{\"end\":47334,\"start\":47322},{\"end\":47344,\"start\":47334},{\"end\":47356,\"start\":47344},{\"end\":47367,\"start\":47356},{\"end\":47682,\"start\":47675},{\"end\":47690,\"start\":47682},{\"end\":47698,\"start\":47690},{\"end\":48149,\"start\":48137},{\"end\":48160,\"start\":48149},{\"end\":48173,\"start\":48160},{\"end\":48182,\"start\":48173},{\"end\":48474,\"start\":48460},{\"end\":48491,\"start\":48474},{\"end\":48925,\"start\":48912},{\"end\":48940,\"start\":48925},{\"end\":48952,\"start\":48940},{\"end\":49241,\"start\":49228},{\"end\":49253,\"start\":49241},{\"end\":49535,\"start\":49513},{\"end\":49548,\"start\":49535},{\"end\":49558,\"start\":49548},{\"end\":49752,\"start\":49738},{\"end\":49767,\"start\":49752},{\"end\":49780,\"start\":49767},{\"end\":50023,\"start\":50016},{\"end\":50033,\"start\":50023},{\"end\":50040,\"start\":50033},{\"end\":50048,\"start\":50040},{\"end\":50323,\"start\":50316},{\"end\":50331,\"start\":50323},{\"end\":50658,\"start\":50648},{\"end\":50668,\"start\":50658},{\"end\":51032,\"start\":51021},{\"end\":51041,\"start\":51032},{\"end\":51050,\"start\":51041},{\"end\":51061,\"start\":51050},{\"end\":51452,\"start\":51439},{\"end\":51462,\"start\":51452},{\"end\":51477,\"start\":51462},{\"end\":51491,\"start\":51477},{\"end\":51966,\"start\":51953},{\"end\":51976,\"start\":51966},{\"end\":51987,\"start\":51976},{\"end\":51998,\"start\":51987},{\"end\":52006,\"start\":51998},{\"end\":52017,\"start\":52006},{\"end\":52030,\"start\":52017},{\"end\":52041,\"start\":52030},{\"end\":52057,\"start\":52041},{\"end\":52065,\"start\":52057},{\"end\":52394,\"start\":52377},{\"end\":52406,\"start\":52394},{\"end\":52776,\"start\":52766},{\"end\":52787,\"start\":52776},{\"end\":52800,\"start\":52787},{\"end\":53200,\"start\":53188},{\"end\":53210,\"start\":53200},{\"end\":53220,\"start\":53210},{\"end\":53232,\"start\":53220},{\"end\":53243,\"start\":53232},{\"end\":53621,\"start\":53612},{\"end\":53634,\"start\":53621},{\"end\":53644,\"start\":53634},{\"end\":53655,\"start\":53644},{\"end\":53666,\"start\":53655},{\"end\":53975,\"start\":53966},{\"end\":53982,\"start\":53975},{\"end\":53991,\"start\":53982},{\"end\":54004,\"start\":53991},{\"end\":54309,\"start\":54300},{\"end\":54317,\"start\":54309},{\"end\":54328,\"start\":54317},{\"end\":54581,\"start\":54573},{\"end\":54588,\"start\":54581},{\"end\":54835,\"start\":54827},{\"end\":54843,\"start\":54835},{\"end\":55070,\"start\":55062},{\"end\":55078,\"start\":55070},{\"end\":55351,\"start\":55343},{\"end\":55359,\"start\":55351},{\"end\":55637,\"start\":55623},{\"end\":55645,\"start\":55637},{\"end\":55654,\"start\":55645},{\"end\":55663,\"start\":55654},{\"end\":55670,\"start\":55663},{\"end\":55685,\"start\":55670},{\"end\":55693,\"start\":55685},{\"end\":56096,\"start\":56084},{\"end\":56106,\"start\":56096},{\"end\":56114,\"start\":56106},{\"end\":56125,\"start\":56114},{\"end\":56482,\"start\":56474},{\"end\":56491,\"start\":56482},{\"end\":56498,\"start\":56491},{\"end\":56509,\"start\":56498},{\"end\":56833,\"start\":56825},{\"end\":56839,\"start\":56833},{\"end\":56850,\"start\":56839},{\"end\":57050,\"start\":57042},{\"end\":57057,\"start\":57050},{\"end\":57065,\"start\":57057},{\"end\":57071,\"start\":57065},{\"end\":57078,\"start\":57071},{\"end\":57086,\"start\":57078},{\"end\":57092,\"start\":57086},{\"end\":57102,\"start\":57092},{\"end\":57113,\"start\":57102},{\"end\":57452,\"start\":57442},{\"end\":57461,\"start\":57452},{\"end\":57475,\"start\":57461},{\"end\":57485,\"start\":57475},{\"end\":57498,\"start\":57485},{\"end\":57510,\"start\":57498},{\"end\":57835,\"start\":57825},{\"end\":57850,\"start\":57835},{\"end\":57866,\"start\":57850},{\"end\":57877,\"start\":57866},{\"end\":57888,\"start\":57877},{\"end\":58165,\"start\":58157},{\"end\":58179,\"start\":58165},{\"end\":58187,\"start\":58179},{\"end\":58198,\"start\":58187},{\"end\":58696,\"start\":58688},{\"end\":58704,\"start\":58696},{\"end\":58716,\"start\":58704},{\"end\":58727,\"start\":58716},{\"end\":59125,\"start\":59117},{\"end\":59139,\"start\":59125},{\"end\":59147,\"start\":59139},{\"end\":59464,\"start\":59449},{\"end\":59476,\"start\":59464},{\"end\":59491,\"start\":59476}]", "bib_venue": "[{\"end\":37988,\"start\":37926},{\"end\":41499,\"start\":41437},{\"end\":42755,\"start\":42683},{\"end\":43188,\"start\":43141},{\"end\":46250,\"start\":46184},{\"end\":47007,\"start\":46958},{\"end\":47847,\"start\":47781},{\"end\":48627,\"start\":48566},{\"end\":50783,\"start\":50734},{\"end\":51640,\"start\":51574},{\"end\":52547,\"start\":52485},{\"end\":52949,\"start\":52883},{\"end\":53358,\"start\":53309},{\"end\":55834,\"start\":55772},{\"end\":58347,\"start\":58281},{\"end\":58842,\"start\":58793},{\"end\":59262,\"start\":59213},{\"end\":37924,\"start\":37847},{\"end\":38418,\"start\":38372},{\"end\":38793,\"start\":38744},{\"end\":39315,\"start\":39286},{\"end\":39665,\"start\":39576},{\"end\":40122,\"start\":40048},{\"end\":40422,\"start\":40390},{\"end\":40635,\"start\":40565},{\"end\":41055,\"start\":41019},{\"end\":41435,\"start\":41358},{\"end\":41929,\"start\":41895},{\"end\":42297,\"start\":42235},{\"end\":42681,\"start\":42594},{\"end\":43139,\"start\":43077},{\"end\":43478,\"start\":43416},{\"end\":43742,\"start\":43704},{\"end\":44083,\"start\":44047},{\"end\":44454,\"start\":44425},{\"end\":44737,\"start\":44651},{\"end\":45036,\"start\":44969},{\"end\":45363,\"start\":45329},{\"end\":45579,\"start\":45539},{\"end\":45862,\"start\":45803},{\"end\":46182,\"start\":46101},{\"end\":46604,\"start\":46570},{\"end\":46956,\"start\":46892},{\"end\":47403,\"start\":47367},{\"end\":47779,\"start\":47698},{\"end\":48135,\"start\":48059},{\"end\":48564,\"start\":48491},{\"end\":48981,\"start\":48952},{\"end\":49282,\"start\":49253},{\"end\":49562,\"start\":49558},{\"end\":49828,\"start\":49780},{\"end\":50107,\"start\":50064},{\"end\":50405,\"start\":50331},{\"end\":50732,\"start\":50668},{\"end\":51129,\"start\":51061},{\"end\":51572,\"start\":51491},{\"end\":51951,\"start\":51875},{\"end\":52483,\"start\":52406},{\"end\":52881,\"start\":52800},{\"end\":53307,\"start\":53243},{\"end\":53738,\"start\":53666},{\"end\":54071,\"start\":54020},{\"end\":54376,\"start\":54328},{\"end\":54643,\"start\":54604},{\"end\":54825,\"start\":54760},{\"end\":55116,\"start\":55078},{\"end\":55412,\"start\":55359},{\"end\":55770,\"start\":55693},{\"end\":56205,\"start\":56141},{\"end\":56577,\"start\":56509},{\"end\":56909,\"start\":56866},{\"end\":57183,\"start\":57129},{\"end\":57556,\"start\":57510},{\"end\":57917,\"start\":57888},{\"end\":58279,\"start\":58198},{\"end\":58791,\"start\":58727},{\"end\":59211,\"start\":59147},{\"end\":59520,\"start\":59491}]"}}}, "year": 2023, "month": 12, "day": 17}