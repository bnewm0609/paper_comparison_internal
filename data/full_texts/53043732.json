{"id": 53043732, "updated": "2023-09-30 13:32:25.457", "metadata": {"title": "DGC-Net: Dense Geometric Correspondence Network", "authors": "[{\"first\":\"Iaroslav\",\"last\":\"Melekhov\",\"middle\":[]},{\"first\":\"Aleksei\",\"last\":\"Tiulpin\",\"middle\":[]},{\"first\":\"Torsten\",\"last\":\"Sattler\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Pollefeys\",\"middle\":[]},{\"first\":\"Esa\",\"last\":\"Rahtu\",\"middle\":[]},{\"first\":\"Juho\",\"last\":\"Kannala\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 10, "day": 19}, "abstract": "This paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where ConvNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1810.08393", "mag": "2951836492", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/MelekhovTSPRK19", "doi": "10.1109/wacv.2019.00115"}}, "content": {"source": {"pdf_hash": "3db426a0ad0f021a77ac6f00e83d543dec794c1d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1810.08393v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1810.08393", "status": "GREEN"}}, "grobid": {"id": "ff69dae2b84b9eb6e8886a13699d52d8d20662d9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3db426a0ad0f021a77ac6f00e83d543dec794c1d.txt", "contents": "\nDGC-Net: Dense Geometric Correspondence Network\n\n\nIaroslav Melekhov \nAalto University\n\n\nAleksei Tiulpin \nUniversity of Oulu\n\n\nTorsten Sattler \nETH Z\u00fcrich\n\n\nMarc Pollefeys marc.pollefeys@inf.ethz.ch \nETH Z\u00fcrich\n\n\nEsa Rahtu \nTampere University of Technology\n5 Microsoft\n\nJuho Kannala jkannala1@aalto.fialeksei.tiulpin@oulu.fiesa.rahtu@tut.fisattlert \nAalto University\n\n\nDGC-Net: Dense Geometric Correspondence Network\n\nThis paper addresses the challenge of dense pixel correspondence estimation between two images. This problem is closely related to optical flow estimation task where Con-vNets (CNNs) have recently achieved significant progress. While optical flow methods produce very accurate results for the small pixel translation and limited appearance variation scenarios, they hardly deal with the strong geometric transformations that we consider in this work. In this paper, we propose a coarse-to-fine CNN-based framework that can leverage the advantages of optical flow approaches and extend them to the case of large transformations providing dense and subpixel accurate estimates. It is trained on synthetic transformations and demonstrates very good performance to unseen, realistic, data. Further, we apply our method to the problem of relative camera pose estimation and demonstrate that the model outperforms existing dense approaches.\n\nIntroduction\n\nFinding correspondences between images is a key task in many computer vision applications, including image alignment [28,29], visual localization [31,34,35], image retrieval [2,11], structure-from-motion [30], semantic correspondence [12,16], optical flow [14,15,26,33] and relative camera pose estimation [23,36]. In general, there are two ways to establish a pixel-wise correspondence field between images. The first group of methods is based on applying feature descriptors to an image pair and utilizing nearest neighbor criterion to match keypoints globally. However, these approaches do not produce dense correspondences explicitly and apply interpolation or local affine transformations [18] to turn a sparse set into a pixel-wise correspondences. Another possible direction of finding dense correspondences is to compare image patches in feature space. * The majority of the work was done during internship at ETH Z\u00fcrich.\n\nNeural networks have been widely used to learn discriminative and robust descriptors [3,21]. Those descriptors are then compared pair-wise by thresholding Euclidean distance between them [6,9,22] or by predicting a binary label [37,38]. In contrast, the proposed approach processes the image as a whole, and thus, it can handle a broader set of geometric changes in images and directly predict dense correspondences without any post-processing steps. Recent optical flow methods [14,33] have demonstrated great success at estimating dense sub-pixel correspondences. However, the main limitation of these methods is a spatially constrained correlation layer predicting the matches in a small vicinity around the center pixel of each image patch. Thus, captured transformations are very restricted. To some extent this restriction can be alleviated with pyramid structure [33] but not completely.\n\nIn this paper we propose a convolutional neural network (CNN) architecture, called DGC-Net, for learning dense pixel correspondences between a pair of images with strong geometric transformations. Following more recent optical flow methods [14,26,33] and the concept introduced by Lucas-Kanade [20], we exploit a coarse-to-fine image warping idea by creating a hierarchical network structure. Rather than considering only affine and thin-plate spline (TPS) transformations [28], we train our system on synthetic data in an end-to-end manner handling diverse geometric transformations present in real world. We demonstrate that the proposed approach substantially outperforms CNN-based optical flow and image matching methods on the challenging HPatches [4] and DTU [1] datasets.\n\nThe main contributions of this paper are: 1) We propose an end-to-end CNN-based method, DGC-Net, to establish dense pixel correspondences between images with strong geometric transformations; 2) We demonstrate that even if DGC-Net is trained only on synthetic transformations, it can generalize well to real data; 3) We apply the proposed approach to the problem of relative camera pose estimation and demonstrate that our method outperforms strong baseline approaches by a large margin. In addition, we modify the original structure of DGC-Net and seamlessly integrate a matchability decoder into DGC-Net that can significantly improve the computational efficiency of the relative camera pose estimation pipeline by removing tentative correspondences with low confidence scores.\n\n\nRelated Work\n\nThe traditional image matching pipeline begins with the detection of interest points and computation of descriptors. However, many of the most widely used descriptors [5,19] are based on hand-crafted features and have limited ability to cope with negative factors, such as strong illumination changes and large variation in viewpoint. In contrast, more recent [24,25] methods based on view-synthesizing have demonstrated state-of-the-art results in image matching by handling large viewing angle difference and appearance changes. However, they do not produce dense perpixel correspondences and do not perform any learning.\n\nApplying machine learning techniques has proven very effective in optical flow estimation problem [14,26,33] which is closely related to finding pixel correspondence task. Recently proposed methods, i.e. PWC-Net [33] and FlowNet2 [14], utilize a correlation layer to predict image similarities in some neighborhood around the center pixel in a coarse-to-fine manner. While such a spatially constrained correlation layer leads to state-of-the-art results in optical flow, it performs poorly for very strong geometric transformations that we consider in this work. Rocco et al. [28] proposed a CNN-based approach for determining correspondences between two images and applying it to instance-level and category-level tasks. In contrast to optical flow methods [14,33], it comprises a matching layer calculating the correlation between target and reference feature maps without any spatial constraint. The method casts finding pixel correspondences task as a regression problem and consisting of two independent Siamese CNNs trained separately and directly predicting affine and TPS geometric transformations parametrizing 6-element and 18-element vectors. On the contrary, we propose a more general approach handling more diverse transformations and operating in an endto-end fashion.\n\nSimilarly to [6], Fathy et al. [9] proposed a CNN-based dense descriptor for 2D and 3D matching. However, their goal is very different to ours requiring strong supervision in the form of per-pixel ground-truth labels to compare extracted feature vectors and establish correspondences.\n\n\nMethod\n\nOur goal is to determine correspondences between two input images I s , I t \u2208 R W \u00d7H\u00d73 . The most straightforward way to solve this task is to predict the parameters of the relative transformation matrix parametrized for different geometric transformations, such as an homogra-phy [8], an affine or a TPS [28] transformation. However, realistic scenes usually contain more complex geometric transformations which can be hardly described by such parametrization. Inspired by recent work in image compositing [17] and optical flow estimation, we propose to predict a dense pixel correspondence map \u03c9 \u2208 R W \u00d7H\u00d72 in an coarse-to-fine manner.\n\n\nNetwork Architecture\n\nIn this section, we first present the structure of the proposed network and the general principles behind it, then formulate the view correspondence objective function to predict geometric transformations between image pairs. Schematic representation of the proposed approach is shown in Fig. 1. A pair of input images is fed into a module consisting of two pre-trained CNN branches which construct a feature pyramid. The correlation layer takes feature maps of the source and target images from the coarse (top) level of the pyramid and estimates the pairwise similarity between them. Then, the correspondence map decoder takes the output of the correlation layer and directly predicts pixel correspondences for this particular level of the pyramid. The estimates are then refined in an iterative manner. Feature pyramid creator. In order to create a representation of an input image pair in feature space, we construct a Siamese neural network with two branches with shared weights.\n\nThe branches use the VGG-16 architecture [32] trained on ImageNet [7] and truncated at the last pooling layer followed by L2normalization [28]. We extract features f s , f t at different parts of each branch to create a 5-layer feature pyramid with the following spatial resolutions (from top to bottom): [15 \u00d7 15, 30 \u00d7 30, 60 \u00d7 60, 120 \u00d7 120, 240 \u00d7 240] and encoded with different colors in Fig. 1. The weights of CNN-branches are then fixed throughout the rest of the network training procedure. Correlation layer. In order to estimate a similarity score between two images, we follow an idea proposed in [28] and calculate the correlation volume between the normalized feature maps of the source and target images. In contrast to optical flow approaches [14,33], where the correlation volume is computed for the raw features in a restricted area around the center pixel, we compute global correlation and apply L2-normalization before and after the correlation layer to strongly down-weight ambiguous matches (c.f . Fig. 1). Specifically, the correlation layer computes the scalar product between each feature vector of the source f s and all vectors of the target f t feature maps f s , f t \u2208 R W \u00d7H\u00d7C and can be defined in the following way:  Figure 1: Overview of our proposed iterative architecture DGC-Net consisting of four major components: 1) the feature pyramid creator. 2) the correlation layer estimates the pairwise similarity score of the source and target feature descriptors.\nc st (i, j) = f s (i, j) , f t (i, j) ,(1)( ) F =\n3) the fully convolutional correspondence map decoders predict the dense correspondence map between input image pair at each level of the feature pyramid. 4) the warping layer warps features of the source image using the upsampled transforming grid from a correspondence map decoder. The matchability decoder is a tiny CNN that predicts a confidence map with higher scores for those pixels in the source image that have correspondences in the target. See Sec. 3.1 for more details.\n\nwhere . denotes the scalar product and c st is a L2normalized correlation volume c st \u2208 R W \u00d7H\u00d7(W \u00d7H) . Since the third dimension of the correlation volume is a product of its W and H, it is not feasible to calculate such volumes in the bottom layers of the pyramid where the spatial resolution of the feature maps is large. Thus, at the bottom feature pyramid layers, we concatenate descriptors channel-wise.\n\nCorrespondence map decoder. The output of the correlation layer is then fed into a correspondence map decoder consisting of 5 convolutional blocks (Conv-BN-ReLU) to estimate a 2D dense correspondence field \u03c9 (l) est at a particular level l of the feature pyramid. The estimates are parameterized such that each predicted pixel location in the map belongs to the interval [\u22121, 1] representing width and height normalized image coordinates. That is, we upsample the predicted correspondence field at the (l \u2212 1) th level to warp the feature maps of the source image at l th level toward the target features. Finally, the upsampled field, warped source\nf s (\u03c9 (l)\nest ) and target f (l) t features are concatenated along the channel dimension and provided accordingly as input to the correspondence map decoder at l th level.\n\nEach convolution layer in the decoder is padded to keep the spatial resolution of the feature maps intact. Moreover, in order to be able to capture more spatial context at the bottom layers of the pyramid, starting from l = 3 different dilation factors have been added to the convolution blocks to increase the receptive field. The feature pyramid creator, correlation layer and a hierarchical chain of the correspondence map decoders together form a CNN architecture that we will refer to as DGC-Net in the following.\n\nGiven an image pair and the ground truth pixel correspondence map \u03c9 gt , we can define a hierarchical objective loss function as follows:\nL c = L\u22121 l=0 \u03b1 (l) 1 N (l) val N (l) val x M (l) gt \u03c9 (l) est (x) \u2212 \u03c9 (l) gt (x) 1(2)\nwhere . 1 is the L1 distance between an estimated \u03c9 val according to the ground truth mask at each level l of the L-level feature pyramid. In order to adjust the weight of different pyramid layers, we introduce a vector of scalar weight coefficients \u03b1 (l) . Matchability decoder. According to recent advances in optical flow [15,33], it is still very challenging to estimate correct correspondences for ill-posed cases, such as occluded regions of an image pair. Thus, in addition to the pixel correspondence map produced by DGC-Net, we would like to directly predict a measure of confidence for each correspondence. Specifically, we modify the DGC-Net structure by adding a matchability branch. It contains four convolutional layers outputting a probability map (parametrized as a sigmoid) indicating a confidence score for each pixel location in the predicted correspondence map. We will refer to this architecture as called DGC+M-Net. Since, we consider this problem as a pixel classification task, we optimize a binary cross entropy (BCE) with logits loss that is defined as:\nL m = \u2212 1 N N \u22121 i=0 (y i log \u03c3 (\u0177 i ) + (1 \u2212 y i ) log (1 \u2212 \u03c3 (\u0177 i )))\n(3) where y i and\u0177 i are ground truth and estimated matchability masks, respectively; \u03c3 is the element-wise sigmoid function. The total loss for the DGC+M-Net model is the sum of the correspondence loss L c and the matchability loss L m with a weighted coefficient \u03b2 (\u03b2 = 1):\nL = L c + \u03b2L m .(4)\nWe provide the detailed information about the hyperparameters used in training as well as the exact network definitions of all network components in supplementary.\n\n\nExperiments\n\nWe discuss the experimental settings and evaluate the proposed method on two closely related tasks, i.e. finding correspondences between images and relative camera pose estimation.\n\n\nBaselines\n\nIn this work we compare our approach with several strong baselines. Image alignment. Rocco et al. [28] propose a CNN-based method to estimate geometric transformations between two images achieving state-of-the art results in a semantic correspondence task. The transformations are parameterized as a 18-element vector and directly regressed by the network. We apply the estimates to a regular grid of the size of the input images to produce a dense pixel correspondence map. Optical flow estimation requires finding correspondences between two input images. Therefore, we consider three CNN-based optical flow approaches, i.e. SPyNet [26], FlowNet2 [14] and the recently proposed PWC-Net [33] as baseline methods. In detail, PWC-Net is based on a coarseto-fine paradigm and predicts optical flow at different scales of feature maps produced by a Siamese CNN. The coarse estimates are then used to refine the flow. For optical flow methods we use pre-trained models from the original authors.\n\nDeepMatching [27] is matching algorithm aiming at finding semi-dense image correspondences. Specifically, it relies on a multi-scale image pyramid architecture with no any trainable parts and can cope with very challenging scenes, such as repetitive textures and non-rigid image transformations.\n\n\nDatasets\n\nWe compare the proposed approach with different baseline methods on two evaluation datasets. HPatches [4] consists of several sequences of real images with varying photometric and geometric changes. Each image sequence contains a reference (target) image and 5 source images taken under a different viewpoint. For all images the estimated ground truth homography H is provided, thus, dense correspondence maps can be obtained for each test image pair. There are 59 image sequences with challenging geometric transformations in total. DTU. The pixel correspondences produced by our method can be also used for relative camera pose estimation problem. Thus, in order to measure the performance of the proposed approach for this task, we utilize the DTU image dataset [1] consisting of 124 scenes with very accurate absolute camera poses collected by a precisely positioned robot. We create a list of camera pairs which have overlapping fields of view and then randomly choose about 3k image pairs covering all the scenes. Training datasets. We use training and validation splits proposed by [28] to compare both approaches fairly. Specifically, Rocco et al. [28] generate synthetic affine (aff) and thin-plate spline (TPS) transformations and apply them to images from Pascal VOC 2011 (P ) and Tokyo Time Machine (T ) datasets. Each synthetic dataset has 20k training and validation image pairs, respectively. However, those transformations are not very diverse. To be able to estimate the correspondences for HPatches scenes accurately, we therefore generate 20k labeled training examples [8] by applying random homography transformations to the (T ) dataset. All training datasets mentioned above represent only synthetic geometric transformations between images. However, it is hard to artificially generate such diverse transformations that are present in real 3D world. Therefore, in addition to synthetic data, we utilize the Citywall dataset used for 3D reconstruction and provided by [10]. Based on camera poses and depth maps estimated with the Multiview Reconstruction Environment [10], we create a list of 10k image pairs and ground truth correspondence maps. We use this data to fine-tune the proposed model. We emphasize that the objective of this experiment is to demonstrate that fine-tuning on realistic data leads to further improvement of the results. \n\n\nMetrics\n\nAs predicting a dense corresponding grid is closely related to optical flow estimation, we follow the standard evaluation metric used in this task, i.e. the average endpoint error (AEPE). AEPE is defined as the average Euclidean distance between the estimated and ground truth correspondence map. In addition to AEPE, we also use Percentage of Correct Keypoints (PCK) as the evaluation metric. PCK shows the percentage of the correctly matched estimated pointsx i that are within a certain threshold (in pixels) from the ground truth corresponding points x i .\n\nIn order to estimate the accuracy of matchability mask predictions, we report normalized Jaccard index (Intersection Over Union, IoU), i.e. 0 \u2264 J \u2264 1 for the ground truth and estimated masks. This metric is interpreted as a similarity measure between two finite sample sets and widely used in semantic segmentation [13].\n\n\nResults\n\nSynthetic Datasets. First, we experimentally compare the proposed DGC-Net and DGC-Net+M models with [28] by calculating AEPE. All the models have been trained on the data provided by [28]. More specifically, *-aff methods utilize only synthetic affine transformations during training but *-aff+tps methods additionally trained on TPS transformations. AEPE is measured only for valid pixel locations of (P ) and (T ) test data by applying the ground-truth mask. For DGC-Net+M-* models we also report normalized Jaccard index. Tab. 1 shows that DGC-Net significantly outperforms all baseline methods on both evaluation datasets. Despite the fact that DGC-Net+M model is marginally worse than DGC-Net in the case that the transformation between images can be described by an affine transformation, it is more universal approach as it additionally predicts a matchability map which is quite accurate according to the Jacard similarity score. It is worth noting that the proposed models generalize well to unseen data, since AEPE metric varies slightly for (P ) and (T ) evaluation datasets respectively. It shows that the model has learned the geometric transformations and not overfitting to the visual representation of images.  Table 1: AEPE metric on the data from [28]. For DGC-Net+M models, the Jaccard index is also reported.\n\nRealistic Datasets. To demonstrate the performance on more realistic data, we evaluate all baseline methods and our approach on the HPatches dataset. That is, we calculate AEPE over all image sequences belonging to the same viewpoint ID and report the numbers in Tab. 2. Compared to *-aff models, fine-tuning on TPS transformations lead to a significant improvement in the performance reducing the overall EPE by 20% for Viewpoint II and by 9% for Viewpoint V, respectively. The performance is improved further by finetuning the model on synthetic homography data. To prevent large errors caused by interpolation, we directly calculate AEPE metric for the semi-dense DeepMatching [27] estimates (i.e. hence [27] has unfair advantage in terms of AEPE). The Jaccard index for DGC+M-Net-* models is provided in Tab. 3. In addition, we report a number of correctly matched pixels between two images by calculating PCK metric with different thresholds. Especially the comparison with [28] is interesting as the coarse level of our pipeline is based on its matching strategy. As shown in Fig. 2, the proposed method correctly matches around 85% pixels for the case where geometric transformations are quite small (Viewpoint I). It significantly outperforms [28] trained on the same data without any external synthetic datasets and can be further improved by utilizing more diverse transformations during training. Compared to FlowNet2 and PWC-Net, DGC-Net, our method can handle scenarios exhibiting drastic changes between views (Viewpoint IV and V), achieving 59% of PCK with a 1-pixel threshold for the most challenging case.\n\nQualitative results on HPatches and DTU are illustrated   Table 3: Normalized Jaccard index (higher is better) produced by the DGC+M-Net model on HPatches evaluation dataset with different types of synthetic transformations of (T ) training dataset.\n\nin Fig. 4 and Fig. 5, respectively. Relative camera pose. In this section, we demonstrate the application of the proposed method for predicting relative camera pose. Given a list of correspondences and the intrinsic camera parameters matrix K, we estimate the essential matrix E by applying RANSAC. To decrease the randomness of RANSAC, for each image pair we run a 1000iteration loop for 5 times and choose the estimated essential matrix corresponding to the maximum inliers count. Once this process is predicted, relative pose can be recovered based on E and K respectively. Similarly to [23], we use the relative orientation error and the relative translation error as metrics for evaluating the performance. Both metrics compute the angle between the estimated orientation/translation and the ground truth. Fig. 3a and 3b show a set of normalized cumulative histograms of relative orientation and translation errors for each baseline models evaluated on all scenes of the DTU dataset (Sec. 4.2). As before, DGC-Net and DGC+M-Net have been trained on only synthetic transformations (aff+tps+homo). For a fair comparison, we resize images to 256 \u00d7 256 size for all baseline methods and change internal camera parameters accordingly. Interestingly, both PWC-Net [33] and FlowNet2 [14] estimate relative orientation quite well achieving 20 \u2022 and 24 \u2022 median error calculated at level 0.5, respectively. The proposed approach outperforms all CNN-based baselines by 18% and 40% at estimating relative orientation and translation median error compared to PWC-Net. We also evaluate DGC+M-Net model which additionally predicts a matchability mask. This mask can be considered as a filter to remove tentative correspondences with small confidence score from the relative pose estimation pipeline. According to Fig. 3, DGC+M-Net falls slightly behind of DGC-Net in estimating relative pose but it achieves significant advantages in terms of computational efficiency decreasing the elapsed time from 312 sec. to 162 sec. for estimating relative camera pose for all test image pairs. To experiment with more realistic transformations, we fine-tune DGC-Net model on the Citywall dataset (Sec. 4.2), illustrated in the supplementary material. We refer to this model as DGC-Net-Citywall. As can be clearly seen, ground-truth transformation maps are incomplete leading to multiple missing regions in the warped reference images (see the supplementary). However, using external data with more diverse transformations helps to improve the performance of the method remarkably, decreasing the median relative translation error by 17% according to Fig. 3b.\n\nIn addition, we calculate the epipolar error for the matches produced by our method, PWC-Net and FlowNet2. The error is defined in terms of the squared distances (d 2 ) between the points and corresponding epipolar lines as follows:\nD e = d 2 (x i , Fx i ) + d 2 (x i , F T x i ) 2 , \u2200i \u2208 N,(5)\nwhere x i = (x i , y i , 1) T and x i = (x i , y i , 1) T denote a pair of matching points in two images; F is the ground-truth fundamental matrix between two views; N is the number of image pixels (image resolution). The normalized cumulative histogram of the error is presented in Fig. 3c. Quantitatively, the proposed method provides quite accurate pixel correspondences between two views achieving a median error less than 4 pixels across the whole test dataset.\n\n\nAblation Study\n\nIn this section, we analyze some design decisions of the proposed approach. More specifically, our goal is to investigate the benefits of using global correlation layer compared to the one utilized in recent optical flow methods [14,33]. In addition, we experiment with another type of parametrization of ground truth data by representing a  (a) Performance of synthetic data. All the models trained on synthetic affine transformations provided by [28].  Global correlation layer: In contrast to the proposed approach, the PWC-Net architecture comprises a local correlation layer computing similarities between two feature maps in some restricted area around the center pixel at each level of the feature pyramid. However, it is very hard to compare DGC-Net and off-the-shelf PWC-Net approach fairly due to the significant difference in network structures (see Tab. 4c). Therefore, we construct a new coarse-to-fine N -level CNN model by keeping all the blocks of DGC-Net except the correlation layer. More specifically, each feature pyramid level is complemented by a local correlation layer as it is used in PWC-Net structure. We dubbed this model to PWCm-Net. As shown in Tab. 4a, the global correlation layer achieves a significant improvement over the case with a set of spatially constrained correlation layers. Particularly, the error is reduced from 6.73 to 0.95 pixels on the (P ) dataset. All results have been obtained for only affine transformations in training data. L2 normalization: As explained in Sec. 3.1, we L2 normalize the output of the correlation layer to down-weigh the putative matches. In Tab. 4a we compare original DGC-Net model and its modified version without correlation layer normalization step (DGC-Net no L2norm). According to the results, the normalization improves the error by about 15% for all test cases demonstrating the importance of this step. Different parametrization: Given two images, the proposed approach predicts a dense pixel correspondence map representing the absolute location of each image pixel. In contrast, all optical flow methods estimate pixel displacements between images. To dispel this doubt in parameterization, we train DGC-Net model on the same synthetic data as before but with ground-truth labels recalculated in an optical flow manner. We title this model DGC-Net-flow and provide the results in Tab. 4a and Tab. 4b. Interestingly, while DGC-Net-flow model marginally performs better on synthetic data, DGC-Net producing more accurate results in large geometric transformations case (Tab. 4b) demonstrating the benefit of the original parametrization.\n\n\nMethod\n\n\nConclusion\n\nOur paper addressed the challenging problem of finding dense pixel correspondences. We have proposed a coarse-to-fine network architecture that efficiently handles diverse transformations between two views. We have shown that our contributions were crucial to outperforming strong baselines on the challenging realistic datasets. Additionally, we have also applied the proposed method to the relative camera pose estimation problem, demonstrating very promising results. We hope this paper inspires more research into applying deep learning to accurate and reliable dense pixel correspondence estimation.   \n\n\nImplementation details\n\nWe train our network end-to-end using Adam [4] solver with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. As a preprocessing step, the training images are resized to 240 \u00d7 240 and further meancentered and normalized using mean and standard deviation of ImageNet dataset [1]. We use a batch size of 32, an initial learning rate of 10 \u22122 which is gradually decreased during training. For fine-tuning on the Citywall dataset ( Fig. 1), the learning rate is set to 10 \u22124 . The weight decay is initialized to 10 \u22125 in all experiments and no dropout was used in our experiments. Our method is implemented using PyTorch framework [5] and trained on two NVIDIA Titan X GPUs.\n\n\nAblation study\n\nDilated convolutions: The quantitative evaluation of the proposed method without any dilation factors used in the correspondence map decoders is presented in Tab \n\n\nQualitative results\n\nWe show more qualitative results of pixel-wise dense correspondence estimation on the HPatches and DTU datasets in Fig. 4 and Fig. 5 \n\n\nground truth binary mask (matchability mask) indicating whether each pixel in the source image has a correspondence in the target; x indexes over valid pixel locations N (l)\n\nFigure 2 :\n2PCK metric calculated for different Viepoint IDs of the HPatches dataset. The proposed DGC-Net model outperforms all the baseline methods with a large margin.\n\nFigure 3 :\n3Comparison of the proposed approach with different baseline methods on the DTU dataset. correspondence map as flow. Furthermore, we demonstrate the importance of L2 normalization of the correlation map. The results are presented in Tab. 4.\n\nFigure 5 :\n5Qualitative results produced by different baseline methods on DTU. All optical flow approaches produce artifacts caused by warping the reference image. In contrast, DGC-Net and DGC+M-Net give the best results on this dataset without any object duplicates in the warped image. More examples presented in the supplementary.\n\nFigure 1 :\n1Citywall dataset[2]  samples used for fine-tuning the model: the top row is a set of reference images; the middle row is corresponding target images; the bottom row is the warped reference images based on ground truth transformation data.\n\nFigure 3 :\n3The structure of the convolutional block 3a and the matchability decoder 3b used by DGC+M-Net model to predict an explainability mask. The proposed model clearly outperforms all strong baselines on realistic data 3c.\n\nTable 2 :\n2AEPE metric for different viewpoint IDs of the \nHPatches dataset (lower is better). The training datasets for \nour and [28]'s approaches are given in parentheses. *Note \nthat since [27] produces only semi-dense matches it has un-\nfair advantage in terms of AEPE. \n\nTransformation \nViewpoint ID \nI \nII \nIII \nIV \nV \naff \n0.682 0.617 0.562 0.523 0.445 \naff+tps \n0.700 0.650 0.603 0.573 0.496 \naff+tps+homo \n0.730 0.687 0.629 0.590 0.525 \n\n\n\nTable 4 :\n4Ablation study.We analyze the influence of dif-\nferent design choices of the proposed method. See Sec. 4.5 \nfor more details. \n\n\n\n\nFigure 4: Qualitative comparisons between different algorithms on the HPatches dataset. Our model produces more accurate correspondence map leading to better image alignment.Reference \n\nTarget \nGround Truth \nRocco et \nal. [28] \n\nPWC-\nNet [33] \nDGC+M-Net \nPredicted \nmask \n\nReference \nTarget \nFlowNet2 [14] \nPWC-\nNet [33] \nDGC+M-Net \nDGC-Net \n\n\n\n\n. 1. The dilated convolutions consistently improve the results on both, synthetic and realistic, datasets. Feature pyramid: Additionally, we report AEPE metric for the estimates obtained at different levels of the feature pyramid of DGC-Net model in Tab. 2. We apply bilinearMethod \nTrain: Tokyo Time Machine (T) \nTest: (T)-aff \n(P)-aff \nRocco[6] \n4.10 \n4.45 \nDGC-Net no dilation 1.03 \n1.17 \nDGC-Net \n0.90 \n1.03 \n\n(a) Synthetic data. \n\nMethod \nViewpoint ID \nI \nII \nIII \nIV \nV \nRocco [6] \n15.02 28.23 29.27 36.57 43.68 \nDGC-Net no dilation 5.36 \n12.60 16.27 20.67 27.61 \nDGC-Net \n5.12 \n13.01 15.08 20.14 26.47 \n\n(b) Realistic data (HPatches). \n\n\n\nTable 1 :\n1The effect of using dilated convolutions. We report AEPE on synthetic 1a and realistic 1b datasets for the models solely trained on synthetic affine transformations provided by[6].[120 \u00d7 120] 1.80 5.83 9.25 11.90 16.96 layer 4 [240 \u00d7 240] 1.55 5.53 8.98 11.66 16.70Pyramid layer \nViewpoint ID \nI \nII \nIII \nIV \nV \nlayer 0 [15 \u00d7 15] \n5.55 10.00 12.79 15.69 20.11 \nlayer 1 [30 \u00d7 30] \n3.34 7.65 \n10.80 13.37 18.30 \nlayer 2 [60 \u00d7 60] \n2.59 6.62 \n9.95 \n12.51 17.56 \nlayer 3 \n\nTable 2 :\n2AEPE metric for different viewpoint IDs of the HPatches dataset (lower is better) for the estimates obtained at different levels of the feature pyramid. interpolation with according scale factors, i.e. {16, 8, 4, 2} to compare the estimated and ground-truth correspondence maps pixel-wise. Clearly, Tab. 2 supports the idea of a hierarchical structure, as the error steadily decreasing from the top (layer 0) to the bottom (layer 4) layer of the pyramid.\n\n\nrespectively.Figure 2: Overview of our proposed hierarchical fully-convolutional architecture DGC-Net consisting of 5 correspondence map decoders. Each decoder incorporates a chain of convolutional blocks. The structure of the convolutional block is illustrated inFig. 3a. conv. block (in, out, ks, s, p, d) conv.block (128, 64, 3, 1, 1, 1) conv.block (32, 16, 3, 1, 1, 1) conv (16, 1, 3, 1, 1, 1)x2 \n\nx2 \nx2 \n\nf 2 (\u03c9 (1) ) \n\nest \n\nf 2 (\u03c9 (2) ) \n\nest \n\nf 2 (\u03c9 (3) ) \n\nest \n\nwarp \n\ncorr \n\nconv. block \n\n(225, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 96, 3, 1, 1, 1) \n\nconv. block \n\n(96, 64, 3, 1, 1, 1) \n\nconv. block \n\n(64, 32, 3, 1, 1, 1) \n\nconv \n\n(32, 2, 3, 1, 1, 1) \n\nconv. block \n\n(514, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 96, 3, 1, 1, 1) \n\nconv. block \n\n(96, 64, 3, 1, 1, 1) \n\nconv. block \n\n(64, 32, 3, 1, 1, 1) \n\nconv \n\n(32, 2, 3, 1, 1, 1) \n\nconv. block \n\n(130, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 96, 3, 1, 4, 4) \n\nconv. block \n\n(96, 64, 3, 1, 6, 6) \n\nconv \n\n(32, 2, 3, 1, 1, 1) \n\nconv. block \n\n(64, 32, 3, 1, 8, 8) \n\nconv. block \n\n(258, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 96, 3, 1, 2, 2) \n\nconv. block \n\n(96, 64, 3, 1, 3, 3) \n\nconv \n\n(32, 2, 3, 1, 1, 1) \n\nconv. block \n\n(64, 32, 3, 1, 4, 4) \n\nconv. block \n\n(130, 128, 3, 1, 1, 1) \n\nconv. block \n\n(128, 96, 3, 1, 4, 4) \n\nconv. block \n\n(96, 64, 3, 1, 12, 12) \n\nconv \n\n(32, 2, 3, 1, 1, 1) \n\nconv. block \n\n(64, 32, 3, 1, 16, 16) \n\nconv0 of \nvgg16_0 \n\nvgg16_0 \nvgg16_1 \nvgg16_2 \nvgg16_3 \n\nconv0 of \nvgg16_0 \n\nvgg16_0 \nvgg16_1 \nvgg16_2 \nvgg16_3 \n\n+ \n\nwarp \nwarp \nwarp \n\nf 1 [240x240x64] \n\nf 2 [240x240x64] \n\nf 1 [120x120x64] \n\nf 2 [120x120x64] \nf 2 [60x60x128] \n\nf 1 [60x60x128] \n\n\u03c9 (3) \n\nest \n[120x120x2] \n\nf 2 [30x30x256] \n\nf 1 [30x30x256] \n\n\u03c9 (4) \n\nest [240x240x2] \n\n\u03c9 (2) \n\nest \n[60x60x2] \n\n\u03c9 (1) \n\nest \n[30x30x2] \n\n\u03c9 (0) \n\nest \n[15x15x2] \n\nf corr [15x15x225] \n\nx2 \n\nf 2 (\u03c9 (0) ) \n\nest \n\n+ \n+ \n+ \n\nIm 1 \n\nIm 2 \n\nbn \n\nconv \n\nReLU \n\nks -kernel size; s -stride \np -padding; d -dilation \n\n(a) Convolutional block \n\nconv.block \n(64, 32, 3, 1, 1, 1) \n\n\nReference TargetGround Truth Rocco et al.\nLarge-Scale Data for Multiple-View Stereopsis. IJCV. H Aanaes, R R Jensen, G Vogiatzis, E Tola, A B Dahl, 14H. Aanaes, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl. Large-Scale Data for Multiple-View Stereopsis. IJCV, pages 1-16, 2016. 1, 4\n\nNeural Codes for Image Retrieval. A Babenko, A Slesarev, A Chigorin, V S Lempitsky, Proc. ECCV. ECCVA. Babenko, A. Slesarev, A. Chigorin, and V. S. Lempitsky. Neural Codes for Image Retrieval. In Proc. ECCV, 2014. 1\n\nPN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors. V Balntas, E Johns, L Tang, K Mikolajczyk, abs/1601.05030CoRRV. Balntas, E. Johns, L. Tang, and K. Mikolajczyk. PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors. CoRR, abs/1601.05030, 2016. 1\n\nHPatches: A benchmark and evaluation of handcrafted and learned local descriptors. V Balntas, K Lenc, A Vedaldi, K Mikolajczyk, Proc. CVPR. CVPR14V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk. HPatches: A benchmark and evaluation of handcrafted and learned local descriptors. In Proc. CVPR, 2017. 1, 4\n\nSpeeded-Up Robust Features (SURF). H Bay, A Ess, T Tuytelaars, L Van Gool, CVIU. 2H. Bay, A. Ess, T. Tuytelaars, and L. Van Gool. Speeded-Up Robust Features (SURF). CVIU, 2008. 2\n\nUniversal Correspondence Network. C Choy, J Gwak, S Savarese, M Chandraker, Proc. NIPS. NIPS1C. Choy, J. Gwak, S. Savarese, and M. Chandraker. Univer- sal Correspondence Network. In Proc. NIPS, 2016. 1, 2\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, F.-F Li, Proc. CVPR. CVPRJ. Deng, W.Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. Im- agenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. 2\n\nDeep Image Homography Estimation. D Detone, T Malisiewicz, A Rabinovich, Proc. in RSS Workshop on Limits and Potentials of Deep Learning in Robotics. in RSS Workshop on Limits and Potentials of Deep Learning in Robotics24D. DeTone, T. Malisiewicz, and A. Rabinovich. Deep Im- age Homography Estimation. In Proc. in RSS Workshop on Limits and Potentials of Deep Learning in Robotics, 2016. 2, 4\n\nHierarchical Metric Learning and Matching for 2D and 3D Geometric Correspondences. M E Fathy, Q.-H Tran, M Zeeshan, Z , P Vernaza, M Chandraker, Proc. ECCV. ECCV1M. E. Fathy, Q.-H. Tran, M. Zeeshan Z., P. Vernaza, and M. Chandraker. Hierarchical Metric Learning and Matching for 2D and 3D Geometric Correspondences. In Proc. ECCV, 2018. 1, 2\n\nMVE: A Multi-view Reconstruction Environment. S Fuhrmann, F Langguth, M Goesele, Proc. of the Eurographics Workshop on Graphics and Cultural Heritage. of the Eurographics Workshop on Graphics and Cultural HeritageS. Fuhrmann, F. Langguth, and M. Goesele. MVE: A Multi-view Reconstruction Environment. In Proc. of the Eu- rographics Workshop on Graphics and Cultural Heritage, pages 11-18, 2014. 4\n\nDeep Image Retrieval: Learning global representations for image search. A Gordo, J Almaz\u00e1n, J Revaud, D Larlus, Proc. ECCV. ECCVA. Gordo, J. Almaz\u00e1n, J. Revaud, and D. Larlus. Deep Image Retrieval: Learning global representations for image search. In Proc. ECCV, 2016. 1\n\nSCNet: Learning Semantic Correspondence. K Han, R S Rezende, B Ham, K K Wong, M Cho, C Schmid, J Ponce, Proc ICCV. ICCVK. Han, R. S. Rezende, B. Ham, K. K. Wong, M. Cho, C. Schmid, and J. Ponce. SCNet: Learning Semantic Cor- respondence. In Proc ICCV, 2017. 1\n\nTernausNet: U-Net with VGG11 encoder pre-trained on ImageNet for image segmentation. V Iglovikov, A Shvets, abs/1801.05746CoRRV. Iglovikov and A. Shvets. TernausNet: U-Net with VGG11 encoder pre-trained on ImageNet for image segmentation. CoRR, abs/1801.05746, 2018. 5\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, Proc. CVPR. CVPR69E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proc. CVPR, 2017. 1, 2, 4, 6, 9\n\nUnsupervised learning of multi-frame optical flow with occlusions. J Janai, G Fatma, R Anurag, M J Black, A Geiger, Proc. ECCV. ECCV14J. Janai, G. Fatma, R. Anurag, M. J. Black, and A. Geiger. Unsupervised learning of multi-frame optical flow with oc- clusions. In Proc. ECCV, 2018. 1, 4\n\nSemi-supervised semantic matching. Z Laskar, J Kannala, In Proc ECCW. 1Z. Laskar and J. Kannala. Semi-supervised semantic match- ing. In Proc ECCW, 2018. 1\n\nST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing. C Lin, E Yumer, O Wang, E Shechtman, S Lucey, Proc. CVPR. CVPRC. Lin, E. Yumer, O. Wang, E. Shechtman, and S. Lucey. ST- GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing. In Proc. CVPR, 2018. 2\n\nBilateral Functions for Global Motion Modeling. W.-Y Lin, M.-M Cheng, J Lu, H Yang, M N Do, P Torr, Proc. ECCV. ECCVW.-Y. Lin, M.-M. Cheng, J. Lu, H. Yang, M. N. Do, and P. Torr. Bilateral Functions for Global Motion Modeling. In Proc. ECCV, 2014. 1\n\nDistinctive Image Features from Scale-Invariant Keypoints. D G Lowe, IJCV. 2D. G. Lowe. Distinctive Image Features from Scale-Invariant Keypoints. IJCV, 2004. 2\n\nAn iterative image registration technique with an application to stereo vision. B D Lucas, T Kanade, Proc. IJ-CAI81. IJ-CAI81B. D. Lucas and T. Kanade. An iterative image registration technique with an application to stereo vision. In Proc. IJ- CAI81, 1981. 1\n\nEfficient Deep Learning for Stereo Matching. W Luo, A G Schwing, R Urtasun, Proc. CVPR. CVPRW. Luo, A. G. Schwing, and R. Urtasun. Efficient Deep Learning for Stereo Matching. In Proc. CVPR, 2016. 1\n\nImage Patch Matching using Convolutional Descriptors with Euclidean Distance. I Melekhov, J Kannala, E Rahtu, Proc. ACCVW. ACCVWI. Melekhov, J. Kannala, and E. Rahtu. Image Patch Match- ing using Convolutional Descriptors with Euclidean Dis- tance. In Proc. ACCVW, 2016. 1\n\nRelative Camera Pose Estimation Using Convolutional Neural Networks. I Melekhov, J Ylioinas, J Kannala, E Rahtu, Proc. ACIVS. ACIVS16I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu. Relative Camera Pose Estimation Using Convolutional Neural Net- works. In Proc. ACIVS, 2017. 1, 6\n\nMODS: Fast and robust method for two-view matching. D Mishkin, J Matas, M Perdoch, CVIU. 2D. Mishkin, J. Matas, and M. Perdoch. MODS: Fast and robust method for two-view matching. CVIU, 2015. 2\n\nASIFT: A New Framework for Fully Affine Invariant Image Comparison. J.-M Morel, G Yu, SIAM J. Img. Sci. 2J.-M. Morel and G. Yu. ASIFT: A New Framework for Fully Affine Invariant Image Comparison. SIAM J. Img. Sci., 2009. 2\n\nOptical Flow Estimation using a Spatial Pyramid Network. A Ranjan, M J Black, Proc. CVPR. CVPR6A. Ranjan and M. J. Black. Optical Flow Estimation using a Spatial Pyramid Network. In Proc. CVPR, 2017. 1, 2, 4, 6\n\nDeepMatching: Hierarchical Deformable Dense Matching. IJCV. J Revaud, P Weinzaepfel, Z Harchaoui, C Schmid, 1206J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid. DeepMatching: Hierarchical Deformable Dense Matching. IJCV, 120(3):300-323, 2016. 4, 5, 6\n\nConvolutional neural network architecture for geometric matching. I Rocco, R Arandjelovi\u0107, J Sivic, Proc. CVPR. CVPR79I. Rocco, R. Arandjelovi\u0107, and J. Sivic. Convolutional neu- ral network architecture for geometric matching. In Proc. CVPR, 2017. 1, 2, 4, 5, 6, 7, 9\n\nEnd-to-end weaklysupervised semantic alignment. I Rocco, R Arandjelovic, J Sivic, Proc. CVPR. CVPRI. Rocco, R. Arandjelovic, and J. Sivic. End-to-end weakly- supervised semantic alignment. In Proc. CVPR, 2018. 1\n\nStructure-from-Motion Revisited. J L Sch\u00f6nberger, J.-M Frahm, Proc. CVPR. CVPRJ. L. Sch\u00f6nberger and J.-M. Frahm. Structure-from-Motion Revisited. In Proc. CVPR, 2016. 1\n\nSemantic Visual Localization. J L Sch\u00f6nberger, M Pollefeys, A Geiger, T Sattler, Proc. CVPR. CVPRJ. L. Sch\u00f6nberger, M. Pollefeys, A. Geiger, and T. Sattler. Semantic Visual Localization. In Proc. CVPR, 2018. 1\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition. CoRR, abs/1409.1556. K Simonyan, A Zisserman, K. Simonyan and A. Zisserman. Very Deep Convolu- tional Networks for Large-Scale Image Recognition. CoRR, abs/1409.1556, 2014. 2\n\nPWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. D Sun, X Yang, M.-Y Liu, J Kautz, Proc. CVPR. CVPR79D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In Proc. CVPR, 2018. 1, 2, 4, 6, 7, 9\n\nInLoc: Indoor Visual Localization with Dense Matching and View Synthesis. H Taira, M Okutomi, T Sattler, M Cimpoi, M Pollefeys, J Sivic, T Pajdla, A Torii, Proc. CVPR. CVPRH. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic, T. Pajdla, and A. Torii. InLoc: Indoor Visual Local- ization with Dense Matching and View Synthesis. In Proc. CVPR, 2018. 1\n\nSemantic Match Consistency for Long-Term Visual Localization. C Toft, E Stenborg, L Hammarstrand, L Brynte, M Pollefeys, T Sattler, F Kahl, Proc. ECCV. ECCVC. Toft, E. Stenborg, L. Hammarstrand, L. Brynte, M. Polle- feys, T. Sattler, and F. Kahl. Semantic Match Consistency for Long-Term Visual Localization. In Proc. ECCV, 2018. 1\n\nDeMoN: Depth and Motion Network for Learning Monocular Stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, Proc. CVPR. CVPRB. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. DeMoN: Depth and Motion Network for Learning Monocular Stereo. In Proc. CVPR, 2017. 1\n\nLearning to Compare Image Patches via Convolutional Neural Networks. S Zagoruyko, N Komodakis, Proc. CVPR. CVPRS. Zagoruyko and N. Komodakis. Learning to Compare Im- age Patches via Convolutional Neural Networks. In Proc. CVPR, 2015. 1\n\nComputing the Stereo Matching Cost with a Convolutional Neural Network. J Zbontar, Y Lecun, Proc. CVPR. CVPRJ. Zbontar and Y. LeCun. Computing the Stereo Matching Cost with a Convolutional Neural Network. In Proc. CVPR, 2015. 1\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, F.-F Li, Proc. CVPR. CVPRJ. Deng, W.Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. Im- agenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. 1\n\nMVE: A Multiview Reconstruction Environment. S Fuhrmann, F Langguth, M Goesele, Proc. of the Eurographics Workshop on Graphics and Cultural Heritage. of the Eurographics Workshop on Graphics and Cultural HeritageS. Fuhrmann, F. Langguth, and M. Goesele. MVE: A Multi- view Reconstruction Environment. In Proc. of the Eurograph- ics Workshop on Graphics and Cultural Heritage, pages 11- 18, 2014. 1\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, Proc. CVPR. CVPRE. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proc. CVPR, 2017. 4\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, Proc. ICLR. ICLRD. Kingma and J. Ba. Adam: A method for stochastic opti- mization. In Proc. ICLR, 2014. 1\n\nAutomatic differentiation in PyTorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z De-Vito, Z Lin, A Desmaison, L Antiga, A Lerer, A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De- Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto- matic differentiation in PyTorch. 2017. 1\n\nConvolutional neural network architecture for geometric matching. I Rocco, R Arandjelovi\u0107, J Sivic, Proc. CVPR. CVPR13I. Rocco, R. Arandjelovi\u0107, and J. Sivic. Convolutional neural network architecture for geometric matching. In Proc. CVPR, 2017. 1, 3\n\nPWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. D Sun, X Yang, M.-Y Liu, J Kautz, Proc. CVPR. CVPR34D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In Proc. CVPR, 2018. 3, 4\n", "annotations": {"author": "[{\"end\":88,\"start\":51},{\"end\":126,\"start\":89},{\"end\":156,\"start\":127},{\"end\":212,\"start\":157},{\"end\":269,\"start\":213},{\"end\":368,\"start\":270}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":60},{\"end\":104,\"start\":97},{\"end\":142,\"start\":135},{\"end\":171,\"start\":162},{\"end\":222,\"start\":217},{\"end\":282,\"start\":275}]", "author_first_name": "[{\"end\":59,\"start\":51},{\"end\":96,\"start\":89},{\"end\":134,\"start\":127},{\"end\":161,\"start\":157},{\"end\":216,\"start\":213},{\"end\":274,\"start\":270}]", "author_affiliation": "[{\"end\":87,\"start\":70},{\"end\":125,\"start\":106},{\"end\":155,\"start\":144},{\"end\":211,\"start\":200},{\"end\":268,\"start\":224},{\"end\":367,\"start\":350}]", "title": "[{\"end\":48,\"start\":1},{\"end\":416,\"start\":369}]", "venue": null, "abstract": "[{\"end\":1352,\"start\":418}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1489,\"start\":1485},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1492,\"start\":1489},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1518,\"start\":1514},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1521,\"start\":1518},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1524,\"start\":1521},{\"end\":1545,\"start\":1542},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1548,\"start\":1545},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1576,\"start\":1572},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1606,\"start\":1602},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1609,\"start\":1606},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1628,\"start\":1624},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1631,\"start\":1628},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1634,\"start\":1631},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1637,\"start\":1634},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1678,\"start\":1674},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1681,\"start\":1678},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2066,\"start\":2062},{\"end\":2387,\"start\":2384},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2390,\"start\":2387},{\"end\":2489,\"start\":2486},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2491,\"start\":2489},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2494,\"start\":2491},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2531,\"start\":2527},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2534,\"start\":2531},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2782,\"start\":2778},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2785,\"start\":2782},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3173,\"start\":3169},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3439,\"start\":3435},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3442,\"start\":3439},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3445,\"start\":3442},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3493,\"start\":3489},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3672,\"start\":3668},{\"end\":3951,\"start\":3948},{\"end\":3963,\"start\":3960},{\"end\":4941,\"start\":4938},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4944,\"start\":4941},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5135,\"start\":5131},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5138,\"start\":5135},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5498,\"start\":5494},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5501,\"start\":5498},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5504,\"start\":5501},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5612,\"start\":5608},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5630,\"start\":5626},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5976,\"start\":5972},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6158,\"start\":6154},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6161,\"start\":6158},{\"end\":6696,\"start\":6693},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6714,\"start\":6711},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7259,\"start\":7256},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7284,\"start\":7280},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7486,\"start\":7482},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8668,\"start\":8664},{\"end\":8692,\"start\":8689},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8765,\"start\":8761},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9234,\"start\":9230},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9384,\"start\":9380},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9387,\"start\":9384},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12959,\"start\":12955},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12962,\"start\":12959},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14553,\"start\":14549},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15089,\"start\":15085},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15104,\"start\":15100},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15143,\"start\":15139},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15461,\"start\":15457},{\"end\":15857,\"start\":15854},{\"end\":16520,\"start\":16517},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16845,\"start\":16841},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16912,\"start\":16908},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17343,\"start\":17340},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17746,\"start\":17742},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17845,\"start\":17841},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19013,\"start\":19009},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19130,\"start\":19126},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19213,\"start\":19209},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20295,\"start\":20291},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21040,\"start\":21036},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21067,\"start\":21063},{\"end\":21170,\"start\":21169},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21339,\"start\":21335},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21611,\"start\":21607},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22825,\"start\":22821},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23498,\"start\":23494},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23516,\"start\":23512},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25885,\"start\":25881},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25888,\"start\":25885},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26104,\"start\":26100},{\"end\":28976,\"start\":28973},{\"end\":29183,\"start\":29180},{\"end\":29536,\"start\":29533}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30090,\"start\":29915},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30262,\"start\":30091},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30515,\"start\":30263},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30850,\"start\":30516},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31102,\"start\":30851},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31332,\"start\":31103},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31781,\"start\":31333},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31922,\"start\":31782},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32268,\"start\":31923},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32915,\"start\":32269},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33396,\"start\":32916},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":33863,\"start\":33397},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":35953,\"start\":33864}]", "paragraph": "[{\"end\":2297,\"start\":1368},{\"end\":3193,\"start\":2299},{\"end\":3973,\"start\":3195},{\"end\":4754,\"start\":3975},{\"end\":5394,\"start\":4771},{\"end\":6678,\"start\":5396},{\"end\":6964,\"start\":6680},{\"end\":7612,\"start\":6975},{\"end\":8621,\"start\":7637},{\"end\":10116,\"start\":8623},{\"end\":10648,\"start\":10167},{\"end\":11059,\"start\":10650},{\"end\":11710,\"start\":11061},{\"end\":11883,\"start\":11722},{\"end\":12403,\"start\":11885},{\"end\":12542,\"start\":12405},{\"end\":13709,\"start\":12630},{\"end\":14057,\"start\":13782},{\"end\":14241,\"start\":14078},{\"end\":14437,\"start\":14257},{\"end\":15442,\"start\":14451},{\"end\":15739,\"start\":15444},{\"end\":18120,\"start\":15752},{\"end\":18692,\"start\":18132},{\"end\":19014,\"start\":18694},{\"end\":20354,\"start\":19026},{\"end\":21978,\"start\":20356},{\"end\":22229,\"start\":21980},{\"end\":24870,\"start\":22231},{\"end\":25104,\"start\":24872},{\"end\":25633,\"start\":25167},{\"end\":28272,\"start\":25652},{\"end\":28903,\"start\":28296},{\"end\":29576,\"start\":28930},{\"end\":29757,\"start\":29595},{\"end\":29914,\"start\":29781}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10159,\"start\":10117},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10166,\"start\":10159},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11721,\"start\":11711},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12629,\"start\":12543},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13781,\"start\":13710},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14077,\"start\":14058},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25166,\"start\":25105}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":20260,\"start\":20253},{\"end\":22045,\"start\":22038},{\"end\":26517,\"start\":26513},{\"end\":29756,\"start\":29753}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1366,\"start\":1354},{\"attributes\":{\"n\":\"2.\"},\"end\":4769,\"start\":4757},{\"attributes\":{\"n\":\"3.\"},\"end\":6973,\"start\":6967},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7635,\"start\":7615},{\"attributes\":{\"n\":\"4.\"},\"end\":14255,\"start\":14244},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14449,\"start\":14440},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15750,\"start\":15742},{\"attributes\":{\"n\":\"4.3.\"},\"end\":18130,\"start\":18123},{\"attributes\":{\"n\":\"4.4.\"},\"end\":19024,\"start\":19017},{\"attributes\":{\"n\":\"4.5.\"},\"end\":25650,\"start\":25636},{\"end\":28281,\"start\":28275},{\"attributes\":{\"n\":\"5.\"},\"end\":28294,\"start\":28284},{\"attributes\":{\"n\":\"1.\"},\"end\":28928,\"start\":28906},{\"attributes\":{\"n\":\"2.\"},\"end\":29593,\"start\":29579},{\"attributes\":{\"n\":\"3.\"},\"end\":29779,\"start\":29760},{\"end\":30102,\"start\":30092},{\"end\":30274,\"start\":30264},{\"end\":30527,\"start\":30517},{\"end\":30862,\"start\":30852},{\"end\":31114,\"start\":31104},{\"end\":31343,\"start\":31334},{\"end\":31792,\"start\":31783},{\"end\":32926,\"start\":32917},{\"end\":33407,\"start\":33398}]", "table": "[{\"end\":31781,\"start\":31345},{\"end\":31922,\"start\":31809},{\"end\":32268,\"start\":32099},{\"end\":32915,\"start\":32546},{\"end\":33396,\"start\":33193},{\"end\":35953,\"start\":34263}]", "figure_caption": "[{\"end\":30090,\"start\":29917},{\"end\":30262,\"start\":30104},{\"end\":30515,\"start\":30276},{\"end\":30850,\"start\":30529},{\"end\":31102,\"start\":30864},{\"end\":31332,\"start\":31116},{\"end\":31809,\"start\":31794},{\"end\":32099,\"start\":31925},{\"end\":32546,\"start\":32271},{\"end\":33193,\"start\":32928},{\"end\":33863,\"start\":33409},{\"end\":34263,\"start\":33866}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":7931,\"start\":7925},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9021,\"start\":9015},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9649,\"start\":9642},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":9879,\"start\":9871},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21444,\"start\":21438},{\"end\":22240,\"start\":22234},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22251,\"start\":22245},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23056,\"start\":23042},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24041,\"start\":24035},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24869,\"start\":24862},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25457,\"start\":25450},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29341,\"start\":29334},{\"end\":29902,\"start\":29896},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29913,\"start\":29907}]", "bib_author_first_name": "[{\"end\":36050,\"start\":36049},{\"end\":36060,\"start\":36059},{\"end\":36062,\"start\":36061},{\"end\":36072,\"start\":36071},{\"end\":36085,\"start\":36084},{\"end\":36093,\"start\":36092},{\"end\":36095,\"start\":36094},{\"end\":36280,\"start\":36279},{\"end\":36291,\"start\":36290},{\"end\":36303,\"start\":36302},{\"end\":36315,\"start\":36314},{\"end\":36317,\"start\":36316},{\"end\":36539,\"start\":36538},{\"end\":36550,\"start\":36549},{\"end\":36559,\"start\":36558},{\"end\":36567,\"start\":36566},{\"end\":36841,\"start\":36840},{\"end\":36852,\"start\":36851},{\"end\":36860,\"start\":36859},{\"end\":36871,\"start\":36870},{\"end\":37102,\"start\":37101},{\"end\":37109,\"start\":37108},{\"end\":37116,\"start\":37115},{\"end\":37130,\"start\":37129},{\"end\":37281,\"start\":37280},{\"end\":37289,\"start\":37288},{\"end\":37297,\"start\":37296},{\"end\":37309,\"start\":37308},{\"end\":37506,\"start\":37505},{\"end\":37514,\"start\":37513},{\"end\":37522,\"start\":37521},{\"end\":37535,\"start\":37531},{\"end\":37541,\"start\":37540},{\"end\":37550,\"start\":37546},{\"end\":37744,\"start\":37743},{\"end\":37754,\"start\":37753},{\"end\":37769,\"start\":37768},{\"end\":38188,\"start\":38187},{\"end\":38190,\"start\":38189},{\"end\":38202,\"start\":38198},{\"end\":38210,\"start\":38209},{\"end\":38221,\"start\":38220},{\"end\":38225,\"start\":38224},{\"end\":38236,\"start\":38235},{\"end\":38494,\"start\":38493},{\"end\":38506,\"start\":38505},{\"end\":38518,\"start\":38517},{\"end\":38918,\"start\":38917},{\"end\":38927,\"start\":38926},{\"end\":38938,\"start\":38937},{\"end\":38948,\"start\":38947},{\"end\":39159,\"start\":39158},{\"end\":39166,\"start\":39165},{\"end\":39168,\"start\":39167},{\"end\":39179,\"start\":39178},{\"end\":39186,\"start\":39185},{\"end\":39188,\"start\":39187},{\"end\":39196,\"start\":39195},{\"end\":39203,\"start\":39202},{\"end\":39213,\"start\":39212},{\"end\":39464,\"start\":39463},{\"end\":39477,\"start\":39476},{\"end\":39719,\"start\":39718},{\"end\":39726,\"start\":39725},{\"end\":39735,\"start\":39734},{\"end\":39745,\"start\":39744},{\"end\":39755,\"start\":39754},{\"end\":39770,\"start\":39769},{\"end\":40038,\"start\":40037},{\"end\":40047,\"start\":40046},{\"end\":40056,\"start\":40055},{\"end\":40066,\"start\":40065},{\"end\":40068,\"start\":40067},{\"end\":40077,\"start\":40076},{\"end\":40295,\"start\":40294},{\"end\":40305,\"start\":40304},{\"end\":40500,\"start\":40499},{\"end\":40507,\"start\":40506},{\"end\":40516,\"start\":40515},{\"end\":40524,\"start\":40523},{\"end\":40537,\"start\":40536},{\"end\":40776,\"start\":40772},{\"end\":40786,\"start\":40782},{\"end\":40795,\"start\":40794},{\"end\":40801,\"start\":40800},{\"end\":40809,\"start\":40808},{\"end\":40811,\"start\":40810},{\"end\":40817,\"start\":40816},{\"end\":41035,\"start\":41034},{\"end\":41037,\"start\":41036},{\"end\":41218,\"start\":41217},{\"end\":41220,\"start\":41219},{\"end\":41229,\"start\":41228},{\"end\":41444,\"start\":41443},{\"end\":41451,\"start\":41450},{\"end\":41453,\"start\":41452},{\"end\":41464,\"start\":41463},{\"end\":41677,\"start\":41676},{\"end\":41689,\"start\":41688},{\"end\":41700,\"start\":41699},{\"end\":41942,\"start\":41941},{\"end\":41954,\"start\":41953},{\"end\":41966,\"start\":41965},{\"end\":41977,\"start\":41976},{\"end\":42209,\"start\":42208},{\"end\":42220,\"start\":42219},{\"end\":42229,\"start\":42228},{\"end\":42423,\"start\":42419},{\"end\":42432,\"start\":42431},{\"end\":42633,\"start\":42632},{\"end\":42643,\"start\":42642},{\"end\":42645,\"start\":42644},{\"end\":42848,\"start\":42847},{\"end\":42858,\"start\":42857},{\"end\":42873,\"start\":42872},{\"end\":42886,\"start\":42885},{\"end\":43113,\"start\":43112},{\"end\":43122,\"start\":43121},{\"end\":43138,\"start\":43137},{\"end\":43364,\"start\":43363},{\"end\":43373,\"start\":43372},{\"end\":43389,\"start\":43388},{\"end\":43562,\"start\":43561},{\"end\":43564,\"start\":43563},{\"end\":43582,\"start\":43578},{\"end\":43729,\"start\":43728},{\"end\":43731,\"start\":43730},{\"end\":43746,\"start\":43745},{\"end\":43759,\"start\":43758},{\"end\":43769,\"start\":43768},{\"end\":43999,\"start\":43998},{\"end\":44011,\"start\":44010},{\"end\":44226,\"start\":44225},{\"end\":44233,\"start\":44232},{\"end\":44244,\"start\":44240},{\"end\":44251,\"start\":44250},{\"end\":44505,\"start\":44504},{\"end\":44514,\"start\":44513},{\"end\":44525,\"start\":44524},{\"end\":44536,\"start\":44535},{\"end\":44546,\"start\":44545},{\"end\":44559,\"start\":44558},{\"end\":44568,\"start\":44567},{\"end\":44578,\"start\":44577},{\"end\":44859,\"start\":44858},{\"end\":44867,\"start\":44866},{\"end\":44879,\"start\":44878},{\"end\":44895,\"start\":44894},{\"end\":44905,\"start\":44904},{\"end\":44918,\"start\":44917},{\"end\":44929,\"start\":44928},{\"end\":45193,\"start\":45192},{\"end\":45207,\"start\":45206},{\"end\":45215,\"start\":45214},{\"end\":45224,\"start\":45223},{\"end\":45233,\"start\":45232},{\"end\":45240,\"start\":45239},{\"end\":45255,\"start\":45254},{\"end\":45516,\"start\":45515},{\"end\":45529,\"start\":45528},{\"end\":45756,\"start\":45755},{\"end\":45767,\"start\":45766},{\"end\":45966,\"start\":45965},{\"end\":45974,\"start\":45973},{\"end\":45982,\"start\":45981},{\"end\":45995,\"start\":45991},{\"end\":46001,\"start\":46000},{\"end\":46010,\"start\":46006},{\"end\":46215,\"start\":46214},{\"end\":46227,\"start\":46226},{\"end\":46239,\"start\":46238},{\"end\":46639,\"start\":46638},{\"end\":46646,\"start\":46645},{\"end\":46655,\"start\":46654},{\"end\":46665,\"start\":46664},{\"end\":46675,\"start\":46674},{\"end\":46690,\"start\":46689},{\"end\":46921,\"start\":46920},{\"end\":46931,\"start\":46930},{\"end\":47082,\"start\":47081},{\"end\":47092,\"start\":47091},{\"end\":47101,\"start\":47100},{\"end\":47113,\"start\":47112},{\"end\":47123,\"start\":47122},{\"end\":47131,\"start\":47130},{\"end\":47142,\"start\":47141},{\"end\":47149,\"start\":47148},{\"end\":47162,\"start\":47161},{\"end\":47172,\"start\":47171},{\"end\":47410,\"start\":47409},{\"end\":47419,\"start\":47418},{\"end\":47435,\"start\":47434},{\"end\":47668,\"start\":47667},{\"end\":47675,\"start\":47674},{\"end\":47686,\"start\":47682},{\"end\":47693,\"start\":47692}]", "bib_author_last_name": "[{\"end\":36057,\"start\":36051},{\"end\":36069,\"start\":36063},{\"end\":36082,\"start\":36073},{\"end\":36090,\"start\":36086},{\"end\":36100,\"start\":36096},{\"end\":36288,\"start\":36281},{\"end\":36300,\"start\":36292},{\"end\":36312,\"start\":36304},{\"end\":36327,\"start\":36318},{\"end\":36547,\"start\":36540},{\"end\":36556,\"start\":36551},{\"end\":36564,\"start\":36560},{\"end\":36579,\"start\":36568},{\"end\":36849,\"start\":36842},{\"end\":36857,\"start\":36853},{\"end\":36868,\"start\":36861},{\"end\":36883,\"start\":36872},{\"end\":37106,\"start\":37103},{\"end\":37113,\"start\":37110},{\"end\":37127,\"start\":37117},{\"end\":37139,\"start\":37131},{\"end\":37286,\"start\":37282},{\"end\":37294,\"start\":37290},{\"end\":37306,\"start\":37298},{\"end\":37320,\"start\":37310},{\"end\":37511,\"start\":37507},{\"end\":37519,\"start\":37515},{\"end\":37529,\"start\":37523},{\"end\":37538,\"start\":37536},{\"end\":37544,\"start\":37542},{\"end\":37553,\"start\":37551},{\"end\":37751,\"start\":37745},{\"end\":37766,\"start\":37755},{\"end\":37780,\"start\":37770},{\"end\":38196,\"start\":38191},{\"end\":38207,\"start\":38203},{\"end\":38218,\"start\":38211},{\"end\":38233,\"start\":38226},{\"end\":38247,\"start\":38237},{\"end\":38503,\"start\":38495},{\"end\":38515,\"start\":38507},{\"end\":38526,\"start\":38519},{\"end\":38924,\"start\":38919},{\"end\":38935,\"start\":38928},{\"end\":38945,\"start\":38939},{\"end\":38955,\"start\":38949},{\"end\":39163,\"start\":39160},{\"end\":39176,\"start\":39169},{\"end\":39183,\"start\":39180},{\"end\":39193,\"start\":39189},{\"end\":39200,\"start\":39197},{\"end\":39210,\"start\":39204},{\"end\":39219,\"start\":39214},{\"end\":39474,\"start\":39465},{\"end\":39484,\"start\":39478},{\"end\":39723,\"start\":39720},{\"end\":39732,\"start\":39727},{\"end\":39742,\"start\":39736},{\"end\":39752,\"start\":39746},{\"end\":39767,\"start\":39756},{\"end\":39775,\"start\":39771},{\"end\":40044,\"start\":40039},{\"end\":40053,\"start\":40048},{\"end\":40063,\"start\":40057},{\"end\":40074,\"start\":40069},{\"end\":40084,\"start\":40078},{\"end\":40302,\"start\":40296},{\"end\":40313,\"start\":40306},{\"end\":40504,\"start\":40501},{\"end\":40513,\"start\":40508},{\"end\":40521,\"start\":40517},{\"end\":40534,\"start\":40525},{\"end\":40543,\"start\":40538},{\"end\":40780,\"start\":40777},{\"end\":40792,\"start\":40787},{\"end\":40798,\"start\":40796},{\"end\":40806,\"start\":40802},{\"end\":40814,\"start\":40812},{\"end\":40822,\"start\":40818},{\"end\":41042,\"start\":41038},{\"end\":41226,\"start\":41221},{\"end\":41236,\"start\":41230},{\"end\":41448,\"start\":41445},{\"end\":41461,\"start\":41454},{\"end\":41472,\"start\":41465},{\"end\":41686,\"start\":41678},{\"end\":41697,\"start\":41690},{\"end\":41706,\"start\":41701},{\"end\":41951,\"start\":41943},{\"end\":41963,\"start\":41955},{\"end\":41974,\"start\":41967},{\"end\":41983,\"start\":41978},{\"end\":42217,\"start\":42210},{\"end\":42226,\"start\":42221},{\"end\":42237,\"start\":42230},{\"end\":42429,\"start\":42424},{\"end\":42435,\"start\":42433},{\"end\":42640,\"start\":42634},{\"end\":42651,\"start\":42646},{\"end\":42855,\"start\":42849},{\"end\":42870,\"start\":42859},{\"end\":42883,\"start\":42874},{\"end\":42893,\"start\":42887},{\"end\":43119,\"start\":43114},{\"end\":43135,\"start\":43123},{\"end\":43144,\"start\":43139},{\"end\":43370,\"start\":43365},{\"end\":43386,\"start\":43374},{\"end\":43395,\"start\":43390},{\"end\":43576,\"start\":43565},{\"end\":43588,\"start\":43583},{\"end\":43743,\"start\":43732},{\"end\":43756,\"start\":43747},{\"end\":43766,\"start\":43760},{\"end\":43777,\"start\":43770},{\"end\":44008,\"start\":44000},{\"end\":44021,\"start\":44012},{\"end\":44230,\"start\":44227},{\"end\":44238,\"start\":44234},{\"end\":44248,\"start\":44245},{\"end\":44257,\"start\":44252},{\"end\":44511,\"start\":44506},{\"end\":44522,\"start\":44515},{\"end\":44533,\"start\":44526},{\"end\":44543,\"start\":44537},{\"end\":44556,\"start\":44547},{\"end\":44565,\"start\":44560},{\"end\":44575,\"start\":44569},{\"end\":44584,\"start\":44579},{\"end\":44864,\"start\":44860},{\"end\":44876,\"start\":44868},{\"end\":44892,\"start\":44880},{\"end\":44902,\"start\":44896},{\"end\":44915,\"start\":44906},{\"end\":44926,\"start\":44919},{\"end\":44934,\"start\":44930},{\"end\":45204,\"start\":45194},{\"end\":45212,\"start\":45208},{\"end\":45221,\"start\":45216},{\"end\":45230,\"start\":45225},{\"end\":45237,\"start\":45234},{\"end\":45252,\"start\":45241},{\"end\":45260,\"start\":45256},{\"end\":45526,\"start\":45517},{\"end\":45539,\"start\":45530},{\"end\":45764,\"start\":45757},{\"end\":45773,\"start\":45768},{\"end\":45971,\"start\":45967},{\"end\":45979,\"start\":45975},{\"end\":45989,\"start\":45983},{\"end\":45998,\"start\":45996},{\"end\":46004,\"start\":46002},{\"end\":46013,\"start\":46011},{\"end\":46224,\"start\":46216},{\"end\":46236,\"start\":46228},{\"end\":46247,\"start\":46240},{\"end\":46643,\"start\":46640},{\"end\":46652,\"start\":46647},{\"end\":46662,\"start\":46656},{\"end\":46672,\"start\":46666},{\"end\":46687,\"start\":46676},{\"end\":46695,\"start\":46691},{\"end\":46928,\"start\":46922},{\"end\":46934,\"start\":46932},{\"end\":47089,\"start\":47083},{\"end\":47098,\"start\":47093},{\"end\":47110,\"start\":47102},{\"end\":47120,\"start\":47114},{\"end\":47128,\"start\":47124},{\"end\":47139,\"start\":47132},{\"end\":47146,\"start\":47143},{\"end\":47159,\"start\":47150},{\"end\":47169,\"start\":47163},{\"end\":47178,\"start\":47173},{\"end\":47416,\"start\":47411},{\"end\":47432,\"start\":47420},{\"end\":47441,\"start\":47436},{\"end\":47672,\"start\":47669},{\"end\":47680,\"start\":47676},{\"end\":47690,\"start\":47687},{\"end\":47699,\"start\":47694}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36243,\"start\":35996},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2836489},\"end\":36460,\"start\":36245},{\"attributes\":{\"doi\":\"abs/1601.05030\",\"id\":\"b2\"},\"end\":36755,\"start\":36462},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2474496},\"end\":37064,\"start\":36757},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14777911},\"end\":37244,\"start\":37066},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12578058},\"end\":37450,\"start\":37246},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":37707,\"start\":37452},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13429319},\"end\":38102,\"start\":37709},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4782216},\"end\":38445,\"start\":38104},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":17161363},\"end\":38843,\"start\":38447},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6532540},\"end\":39115,\"start\":38845},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":215762865},\"end\":39376,\"start\":39117},{\"attributes\":{\"doi\":\"abs/1801.05746\",\"id\":\"b12\"},\"end\":39646,\"start\":39378},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3759573},\"end\":39968,\"start\":39648},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51883701},\"end\":40257,\"start\":39970},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":59159050},\"end\":40414,\"start\":40259},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3692201},\"end\":40722,\"start\":40416},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14540545},\"end\":40973,\"start\":40724},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":174065},\"end\":41135,\"start\":40975},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2121536},\"end\":41396,\"start\":41137},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10845625},\"end\":41596,\"start\":41398},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":20092281},\"end\":41870,\"start\":41598},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7627602},\"end\":42154,\"start\":41872},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11679229},\"end\":42349,\"start\":42156},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3174256},\"end\":42573,\"start\":42351},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1379674},\"end\":42785,\"start\":42575},{\"attributes\":{\"id\":\"b26\"},\"end\":43044,\"start\":42787},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1824134},\"end\":43313,\"start\":43046},{\"attributes\":{\"id\":\"b28\"},\"end\":43526,\"start\":43315},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1728538},\"end\":43696,\"start\":43528},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4853777},\"end\":43907,\"start\":43698},{\"attributes\":{\"id\":\"b31\"},\"end\":44151,\"start\":43909},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":30824366},\"end\":44428,\"start\":44153},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4704473},\"end\":44794,\"start\":44430},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52952773},\"end\":45127,\"start\":44796},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6159584},\"end\":45444,\"start\":45129},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":215827033},\"end\":45681,\"start\":45446},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":13501868},\"end\":45910,\"start\":45683},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":57246310},\"end\":46167,\"start\":45912},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":63670383},\"end\":46566,\"start\":46169},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3759573},\"end\":46874,\"start\":46568},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6628106},\"end\":47041,\"start\":46876},{\"attributes\":{\"id\":\"b42\"},\"end\":47341,\"start\":47043},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1824134},\"end\":47593,\"start\":47343},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":30824366},\"end\":47858,\"start\":47595}]", "bib_title": "[{\"end\":36277,\"start\":36245},{\"end\":36838,\"start\":36757},{\"end\":37099,\"start\":37066},{\"end\":37278,\"start\":37246},{\"end\":37503,\"start\":37452},{\"end\":37741,\"start\":37709},{\"end\":38185,\"start\":38104},{\"end\":38491,\"start\":38447},{\"end\":38915,\"start\":38845},{\"end\":39156,\"start\":39117},{\"end\":39716,\"start\":39648},{\"end\":40035,\"start\":39970},{\"end\":40292,\"start\":40259},{\"end\":40497,\"start\":40416},{\"end\":40770,\"start\":40724},{\"end\":41032,\"start\":40975},{\"end\":41215,\"start\":41137},{\"end\":41441,\"start\":41398},{\"end\":41674,\"start\":41598},{\"end\":41939,\"start\":41872},{\"end\":42206,\"start\":42156},{\"end\":42417,\"start\":42351},{\"end\":42630,\"start\":42575},{\"end\":43110,\"start\":43046},{\"end\":43361,\"start\":43315},{\"end\":43559,\"start\":43528},{\"end\":43726,\"start\":43698},{\"end\":44223,\"start\":44153},{\"end\":44502,\"start\":44430},{\"end\":44856,\"start\":44796},{\"end\":45190,\"start\":45129},{\"end\":45513,\"start\":45446},{\"end\":45753,\"start\":45683},{\"end\":45963,\"start\":45912},{\"end\":46212,\"start\":46169},{\"end\":46636,\"start\":46568},{\"end\":46918,\"start\":46876},{\"end\":47407,\"start\":47343},{\"end\":47665,\"start\":47595}]", "bib_author": "[{\"end\":36059,\"start\":36049},{\"end\":36071,\"start\":36059},{\"end\":36084,\"start\":36071},{\"end\":36092,\"start\":36084},{\"end\":36102,\"start\":36092},{\"end\":36290,\"start\":36279},{\"end\":36302,\"start\":36290},{\"end\":36314,\"start\":36302},{\"end\":36329,\"start\":36314},{\"end\":36549,\"start\":36538},{\"end\":36558,\"start\":36549},{\"end\":36566,\"start\":36558},{\"end\":36581,\"start\":36566},{\"end\":36851,\"start\":36840},{\"end\":36859,\"start\":36851},{\"end\":36870,\"start\":36859},{\"end\":36885,\"start\":36870},{\"end\":37108,\"start\":37101},{\"end\":37115,\"start\":37108},{\"end\":37129,\"start\":37115},{\"end\":37141,\"start\":37129},{\"end\":37288,\"start\":37280},{\"end\":37296,\"start\":37288},{\"end\":37308,\"start\":37296},{\"end\":37322,\"start\":37308},{\"end\":37513,\"start\":37505},{\"end\":37521,\"start\":37513},{\"end\":37531,\"start\":37521},{\"end\":37540,\"start\":37531},{\"end\":37546,\"start\":37540},{\"end\":37555,\"start\":37546},{\"end\":37753,\"start\":37743},{\"end\":37768,\"start\":37753},{\"end\":37782,\"start\":37768},{\"end\":38198,\"start\":38187},{\"end\":38209,\"start\":38198},{\"end\":38220,\"start\":38209},{\"end\":38224,\"start\":38220},{\"end\":38235,\"start\":38224},{\"end\":38249,\"start\":38235},{\"end\":38505,\"start\":38493},{\"end\":38517,\"start\":38505},{\"end\":38528,\"start\":38517},{\"end\":38926,\"start\":38917},{\"end\":38937,\"start\":38926},{\"end\":38947,\"start\":38937},{\"end\":38957,\"start\":38947},{\"end\":39165,\"start\":39158},{\"end\":39178,\"start\":39165},{\"end\":39185,\"start\":39178},{\"end\":39195,\"start\":39185},{\"end\":39202,\"start\":39195},{\"end\":39212,\"start\":39202},{\"end\":39221,\"start\":39212},{\"end\":39476,\"start\":39463},{\"end\":39486,\"start\":39476},{\"end\":39725,\"start\":39718},{\"end\":39734,\"start\":39725},{\"end\":39744,\"start\":39734},{\"end\":39754,\"start\":39744},{\"end\":39769,\"start\":39754},{\"end\":39777,\"start\":39769},{\"end\":40046,\"start\":40037},{\"end\":40055,\"start\":40046},{\"end\":40065,\"start\":40055},{\"end\":40076,\"start\":40065},{\"end\":40086,\"start\":40076},{\"end\":40304,\"start\":40294},{\"end\":40315,\"start\":40304},{\"end\":40506,\"start\":40499},{\"end\":40515,\"start\":40506},{\"end\":40523,\"start\":40515},{\"end\":40536,\"start\":40523},{\"end\":40545,\"start\":40536},{\"end\":40782,\"start\":40772},{\"end\":40794,\"start\":40782},{\"end\":40800,\"start\":40794},{\"end\":40808,\"start\":40800},{\"end\":40816,\"start\":40808},{\"end\":40824,\"start\":40816},{\"end\":41044,\"start\":41034},{\"end\":41228,\"start\":41217},{\"end\":41238,\"start\":41228},{\"end\":41450,\"start\":41443},{\"end\":41463,\"start\":41450},{\"end\":41474,\"start\":41463},{\"end\":41688,\"start\":41676},{\"end\":41699,\"start\":41688},{\"end\":41708,\"start\":41699},{\"end\":41953,\"start\":41941},{\"end\":41965,\"start\":41953},{\"end\":41976,\"start\":41965},{\"end\":41985,\"start\":41976},{\"end\":42219,\"start\":42208},{\"end\":42228,\"start\":42219},{\"end\":42239,\"start\":42228},{\"end\":42431,\"start\":42419},{\"end\":42437,\"start\":42431},{\"end\":42642,\"start\":42632},{\"end\":42653,\"start\":42642},{\"end\":42857,\"start\":42847},{\"end\":42872,\"start\":42857},{\"end\":42885,\"start\":42872},{\"end\":42895,\"start\":42885},{\"end\":43121,\"start\":43112},{\"end\":43137,\"start\":43121},{\"end\":43146,\"start\":43137},{\"end\":43372,\"start\":43363},{\"end\":43388,\"start\":43372},{\"end\":43397,\"start\":43388},{\"end\":43578,\"start\":43561},{\"end\":43590,\"start\":43578},{\"end\":43745,\"start\":43728},{\"end\":43758,\"start\":43745},{\"end\":43768,\"start\":43758},{\"end\":43779,\"start\":43768},{\"end\":44010,\"start\":43998},{\"end\":44023,\"start\":44010},{\"end\":44232,\"start\":44225},{\"end\":44240,\"start\":44232},{\"end\":44250,\"start\":44240},{\"end\":44259,\"start\":44250},{\"end\":44513,\"start\":44504},{\"end\":44524,\"start\":44513},{\"end\":44535,\"start\":44524},{\"end\":44545,\"start\":44535},{\"end\":44558,\"start\":44545},{\"end\":44567,\"start\":44558},{\"end\":44577,\"start\":44567},{\"end\":44586,\"start\":44577},{\"end\":44866,\"start\":44858},{\"end\":44878,\"start\":44866},{\"end\":44894,\"start\":44878},{\"end\":44904,\"start\":44894},{\"end\":44917,\"start\":44904},{\"end\":44928,\"start\":44917},{\"end\":44936,\"start\":44928},{\"end\":45206,\"start\":45192},{\"end\":45214,\"start\":45206},{\"end\":45223,\"start\":45214},{\"end\":45232,\"start\":45223},{\"end\":45239,\"start\":45232},{\"end\":45254,\"start\":45239},{\"end\":45262,\"start\":45254},{\"end\":45528,\"start\":45515},{\"end\":45541,\"start\":45528},{\"end\":45766,\"start\":45755},{\"end\":45775,\"start\":45766},{\"end\":45973,\"start\":45965},{\"end\":45981,\"start\":45973},{\"end\":45991,\"start\":45981},{\"end\":46000,\"start\":45991},{\"end\":46006,\"start\":46000},{\"end\":46015,\"start\":46006},{\"end\":46226,\"start\":46214},{\"end\":46238,\"start\":46226},{\"end\":46249,\"start\":46238},{\"end\":46645,\"start\":46638},{\"end\":46654,\"start\":46645},{\"end\":46664,\"start\":46654},{\"end\":46674,\"start\":46664},{\"end\":46689,\"start\":46674},{\"end\":46697,\"start\":46689},{\"end\":46930,\"start\":46920},{\"end\":46936,\"start\":46930},{\"end\":47091,\"start\":47081},{\"end\":47100,\"start\":47091},{\"end\":47112,\"start\":47100},{\"end\":47122,\"start\":47112},{\"end\":47130,\"start\":47122},{\"end\":47141,\"start\":47130},{\"end\":47148,\"start\":47141},{\"end\":47161,\"start\":47148},{\"end\":47171,\"start\":47161},{\"end\":47180,\"start\":47171},{\"end\":47418,\"start\":47409},{\"end\":47434,\"start\":47418},{\"end\":47443,\"start\":47434},{\"end\":47674,\"start\":47667},{\"end\":47682,\"start\":47674},{\"end\":47692,\"start\":47682},{\"end\":47701,\"start\":47692}]", "bib_venue": "[{\"end\":36345,\"start\":36341},{\"end\":36901,\"start\":36897},{\"end\":37338,\"start\":37334},{\"end\":37571,\"start\":37567},{\"end\":37928,\"start\":37859},{\"end\":38265,\"start\":38261},{\"end\":38660,\"start\":38598},{\"end\":38973,\"start\":38969},{\"end\":39236,\"start\":39232},{\"end\":39793,\"start\":39789},{\"end\":40102,\"start\":40098},{\"end\":40561,\"start\":40557},{\"end\":40840,\"start\":40836},{\"end\":41262,\"start\":41254},{\"end\":41490,\"start\":41486},{\"end\":41726,\"start\":41721},{\"end\":42003,\"start\":41998},{\"end\":42669,\"start\":42665},{\"end\":43162,\"start\":43158},{\"end\":43413,\"start\":43409},{\"end\":43606,\"start\":43602},{\"end\":43795,\"start\":43791},{\"end\":44275,\"start\":44271},{\"end\":44602,\"start\":44598},{\"end\":44952,\"start\":44948},{\"end\":45278,\"start\":45274},{\"end\":45557,\"start\":45553},{\"end\":45791,\"start\":45787},{\"end\":46031,\"start\":46027},{\"end\":46381,\"start\":46319},{\"end\":46713,\"start\":46709},{\"end\":46952,\"start\":46948},{\"end\":47459,\"start\":47455},{\"end\":47717,\"start\":47713},{\"end\":36047,\"start\":35996},{\"end\":36339,\"start\":36329},{\"end\":36536,\"start\":36462},{\"end\":36895,\"start\":36885},{\"end\":37145,\"start\":37141},{\"end\":37332,\"start\":37322},{\"end\":37565,\"start\":37555},{\"end\":37857,\"start\":37782},{\"end\":38259,\"start\":38249},{\"end\":38596,\"start\":38528},{\"end\":38967,\"start\":38957},{\"end\":39230,\"start\":39221},{\"end\":39461,\"start\":39378},{\"end\":39787,\"start\":39777},{\"end\":40096,\"start\":40086},{\"end\":40327,\"start\":40315},{\"end\":40555,\"start\":40545},{\"end\":40834,\"start\":40824},{\"end\":41048,\"start\":41044},{\"end\":41252,\"start\":41238},{\"end\":41484,\"start\":41474},{\"end\":41719,\"start\":41708},{\"end\":41996,\"start\":41985},{\"end\":42243,\"start\":42239},{\"end\":42453,\"start\":42437},{\"end\":42663,\"start\":42653},{\"end\":42845,\"start\":42787},{\"end\":43156,\"start\":43146},{\"end\":43407,\"start\":43397},{\"end\":43600,\"start\":43590},{\"end\":43789,\"start\":43779},{\"end\":43996,\"start\":43909},{\"end\":44269,\"start\":44259},{\"end\":44596,\"start\":44586},{\"end\":44946,\"start\":44936},{\"end\":45272,\"start\":45262},{\"end\":45551,\"start\":45541},{\"end\":45785,\"start\":45775},{\"end\":46025,\"start\":46015},{\"end\":46317,\"start\":46249},{\"end\":46707,\"start\":46697},{\"end\":46946,\"start\":46936},{\"end\":47079,\"start\":47043},{\"end\":47453,\"start\":47443},{\"end\":47711,\"start\":47701}]"}}}, "year": 2023, "month": 12, "day": 17}