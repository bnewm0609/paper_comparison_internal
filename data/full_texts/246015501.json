{"id": 246015501, "updated": "2023-10-05 17:47:43.899", "metadata": {"title": "Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems", "authors": "[{\"first\":\"Wei\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Zhaojun\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Haichun\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhenglin\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Gang\",\"last\":\"Qu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Adversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have received a lot of attention recently. However, majority of the research on AEs is in the digital domain and the adversarial patches are static, which is very different from many real-world DNN applications such as Traffic Sign Recognition (TSR) systems in autonomous vehicles. In TSR systems, object detectors use DNNs to process streaming video in real time. From the view of object detectors, the traffic sign`s position and quality of the video are continuously changing, rendering the digital AEs ineffective in the physical world. In this paper, we propose a systematic pipeline to generate robust physical AEs against real-world object detectors. Robustness is achieved in three ways. First, we simulate the in-vehicle cameras by extending the distribution of image transformations with the blur transformation and the resolution transformation. Second, we design the single and multiple bounding boxes filters to improve the efficiency of the perturbation training. Third, we consider four representative attack vectors, namely Hiding Attack, Appearance Attack, Non-Target Attack and Target Attack. We perform a comprehensive set of experiments under a variety of environmental conditions, and considering illuminations in sunny and cloudy weather as well as at night. The experimental results show that the physical AEs generated from our pipeline are effective and robust when attacking the YOLO v5 based TSR system. The attacks have good transferability and can deceive other state-of-the-art object detectors. We launched HA and NTA on a brand-new 2021 model vehicle. Both attacks are successful in fooling the TSR system, which could be a life-threatening case for autonomous vehicles. Finally, we discuss three defense mechanisms based on image preprocessing, AEs detection, and model enhancing.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.06192", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ndss/JiaLZL0022", "doi": "10.14722/ndss.2022.24130"}}, "content": {"source": {"pdf_hash": "d12c672f511e471aa2d97278754b98fa311680a2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.06192v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5c32013700cf5e54ebab95422343f42de10f372b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d12c672f511e471aa2d97278754b98fa311680a2.txt", "contents": "\nFooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems\n\n\nWei Jia jiaw@hust.edu.cn \nZhaojun Lu \nHaichun Zhang \nZhenglin Liu liuzhenglin@hust.edu.cn \nJie Wang wangjie@seczone.cn \nGang Qu gangqu@umd.edu \n\nSchool of Cyber Science and Engineering\nSchool of Cyber Science and Engineering Huazhong Univ. of Sci. & Tech\nof Sci. & Tech\nHuazhong Uni. of Sci. & Tech\nHuazhong Univ. of Sci. & Tech\nHuazhong Univ\nShenzhen Kaiyuan Internet Security\nLtdy\n\n\nUniversity of Maryland\n\n\nFooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems\n\nAdversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have received a lot of attention recently. However, majority of the research on AEs is in the digital domain and the adversarial patches are static. Such research is very different from many real-world DNN applications such as Traffic Sign Recognition (TSR) systems in autonomous vehicles. In TSR systems, object detectors use DNNs to process streaming video in real time. From the view of object detectors, the traffic sign's position and quality of the video are continuously changing, rendering the digital AEs ineffective in the physical world.In this paper, we propose a systematic pipeline to generate robust physical AEs against real-world object detectors. Robustness is achieved in three ways. First, we simulate the in-vehicle cameras by extending the distribution of image transformations with the blur transformation and the resolution transformation. Second, we design the single and multiple bounding boxes filters to improve the efficiency of the perturbation training. Third, we consider four representative attack vectors, namely Hiding Attack (HA), Appearance Attack (AA), Non-Target Attack (NTA) and Target Attack (TA). For each of them, a loss function is defined to minimize the impact of the fabrication process on the physical AEs.We perform a comprehensive set of experiments under a variety of environmental conditions by varying the distance from 0m to 30m, changing the angle from \u221260 \u2022 to 60 \u2022 , and considering illuminations in sunny and cloudy weather as well as at night. The experimental results show that the physical AEs generated from our pipeline are effective and robust when attacking the YOLO v5 based TSR system. The attacks have good transferability and can deceive other state-of-the-art object detectors. We launched HA and NTA on a brand-new 2021 model vehicle. Both attacks are successful in fooling the TSR system, which could be a lifethreatening case for autonomous vehicles. Finally, we discuss three defense mechanisms based on image preprocessing, AEs detection, and model enhancing.\n\nI. INTRODUCTION\n\nArtificial Intelligence (AI) and Deep Neural Networks (DNNs) have boosted the performance of a large variety of applications, in particular computer vision tasks such as face recognition [1], image classification [2], and object detection [3]. Unfortunately, the ubiquity and diversity of AI applications have also created incentives and opportunities for attackers to attack DNNs for malicious purposes [4]. In 2014, Szegedy et al. [5] first introduced the concept of Adversarial Examples (AEs), which are well-crafted examples with imperceptive perturbations that deceive the image classifiers into misclassification. A lot of follow-up studies [6][7][8][9][10] demonstrated the vulnerabilities of DNNs under different types of AEs. One common feature of these attacks is that they are all in the digital domain instead of physical domain. Compared to the theoretical studies which have yielded prolific results on AEs and led to better understanding the principles of DNNs, the practicality of these AEs and their impact on real-world AI applications remain under-investigated.\n\nRecently there have been reported studies on the feasibility of AEs in the physical domain, where the digital adversarial images are printed to the physical domain first, and then pictures are taken from these physical images to create AEs [11][12][13][14][15]. However, the physical AEs generated by this digitalphysical-digital conversion becomes significantly less effective because of complicated physical conditions during the process such as the distance, angle, and illumination when the images are re-taken. Consequently, there are approaches to improve the robustness. Initial efforts have been devoted to improving the robustness of AEs. Athalye et al. [12] introduced the Expectation Over Transformation (EOT) method to simulate the effect of rotation, scaling, and perspective changes. Evtimov et al. [13] and Sitawarin et al. [14] enhanced this method by synthesizing the adversarial traffic signs to attack the image classifiers. Object detectors are adopted for the Traffic Sign Recognition (TSR) task to assist the safety-and securitycritical autonomous driving systems [15].\n\nUnlike image classifiers, fooling object detectors with physical AEs is much more challenging for the following reasons. Firstly, image classifiers only process static images, but the object detectors like those in autonomous vehicles are commonly working in environments where physical features of the object such as its relative position to the object detector keep changing. Secondly, the imperfections in the fabrication process have an uncontrollable impact on the effectiveness of AEs, creating a large gap between the digital domain and the physical domain for adversarial attacks. The digitalphysical-digital conversion would weaken the toxicity of the fabricated AEs. Thirdly, the AEs generation algorithms require some information of the targeted DNNs, which may not be realistic because attackers may not have control over the builtin systems of autonomous vehicles. One practical approach is to generate AEs under the white-box settings and use them to attack the black-box object detectors. To summarize, the key to fooling object detectors, the eyes of autonomous vehicles, is to improve the robustness of physical AEs.\n\nIn this paper, we propose a systematic pipeline to generate robust physical AEs and demonstrate its effectiveness against the object detectors used in TSR systems. To reflect the realworld scenarios, we generate physical AEs for traffic signs with a large range of physical parameters including distance, angle, and illumination. First, we extend the distribution of image transformations with the blur transformation and the resolution transformation to simulate the in-vehicle cameras. Then, Single Bounding Box (S-BBOX) and Multiple Bounding Box (M-BBOX) filters are designed to obtain the relative BBOXes before generating the physical AEs. We define four loss functions to train adversarial perturbations corresponding to the following four attack vectors: Hiding Attack (HA), Appearance Attack (AA), Non-Target Attack (NTA), and Target Attack (TA). HA hides AEs in the background so that the object detectors cannot detect them. AA makes the object detectors recognize a bizarre AE as a common category. Both NTA and TA deceive the object detector into misrecognition with imperceptible AEs. TA is more destructive since it makes the object detectors recognize an AE of one category as an object from another designated category. To improve the transferability of AEs, we use different background images in the AEs generation algorithm to avoid overfitting.\n\nFinally, we validate the robustness of our physical AEs by driving a brand-new 2021 model vehicle toward the physical AEs to see whether the vehicle's TSR system will be fooled. Our main contributions can be summarized as follows:\n\n1) A systematic approach to generate robust physical AEs: In our physical AE generation pipeline, we propose several approaches to improve the robustness of the physical AEs. We extend the distribution of image transformations to simulate the complicated environmental driving conditions. S-BBOX and M-BBOX filters are designed to obtain the BBOXes associated with the target object to train perturbation efficiently. Four loss functions are defined to generate AEs for attack vectors.\n\n2) A comprehensive set of experiments: To systematically evaluate the effectiveness and robustness of the generated physical AEs, we conduct extensive outdoor experiments. More than 1,000 video clips containing more than 100,000 image frames are taken by a high-resolution camera. Real driving scenarios are simulated with varying distances from 1m to 30m, angles from \u221260 \u2022 to 60 \u2022 , and illuminations for sunny, cloudy, and night.\n\n3) Successful attacks against YOLO v5 based object detector and TSR system in a 2021 model vehicle: To the best of our knowledge, this is the first set of adversarial attacks against YOLO v5 based object detectors in the physical domain. We successfully launch four attack vectors, especially NTA and TA, that are life-threatening in the real world. Our physical AEs also exhibit satisfactory transferability when attacking a production-grade TSR system of a brand-new 2021 model vehicle.\n\n\nII. BACKGROUND\n\nIn this section, we first introduce the advances in the field of image classification and object detection and explain why autonomous vehicles adopt object detectors for the TSR task. Then, we summarize the difficulties and limitations of the existing physical adversarial attacks against the object detectors.\n\n\nA. Object Detection\n\nAn image classifier f (\u00b7) : R h\u00d7w\u00d7\u03b5 \u2192 R N recognizes an input image x \u2208 R h\u00d7w\u00d7\u03b5 to a category label y x \u2208 (1, ..., N ) where h ,w and \u03b5 is the height, width and channel of x. The image classifier is commonly trained with supervised learning strategy, whose goal is to minimize a loss function between the output of the network f (x) and the expected label y x [17]. However, image classification cannot satisfy the demands of processing dynamic video streaming containing multiple objects for autonomous vehicles [18]. Take the TSR task for example, it is vital for the vehicle sensor to recognize a traffic sign quickly and accurately from the complicated road background, then give the correct instruction to the vehicle controller for rapid response [19]. Object detection based on DNNs is adopted by autonomous vehicles to process consecutive frames of images containing multi-category objects (e.g., traffic signs, vehicles, pedestrians, and cyclists) [20]. Modern DNNs based object detectors can be classified into two categories: one-stage architecture represented by YOLO series with higher detecting speed [21], and two-stage architecture represented by Faster R-CNN with higher detecting accuracy [22]. Since one-stage architecture processes BBOX regression and object classification concurrently without a region proposal stage, it is much faster than two-stage architecture so that meets the real-time requirement [23]. In recent years, many algorithms have been developed to balance the speed and the accuracy of the object detectors. Bochkovskiy et al. [24] proposed YOLO v4 that improved the accuracy compared with YOLO v3 [25] while maintaining the real-time object detection capability. Soon after the release of YOLO v4, a company named Ultralytics released YOLO v5's source code at Github [26]. YOLO v5 has higher mean Average Precision (mAP) and lower processing time than YOLO v4. It should be emphasized that YOLO v5 is trained with clear images, but it can be used to detect blurred images [16]. The prominent features allow YOLO v5 to detect objects under various conditions, which is well-performed in the TSR task.\n\nAs briefly introduced in Fig.1, YOLO v5 inherits some advanced techniques from YOLO v4, including the Darknet53 with Cross Stage Partial [27] (CSPDarknet53) as backbone, the Path Aggregation Network [28] (PANet) as neck, and the Spatial Pyramid Pooling [29] (SPP) block to increase the receptive field. Besides, YOLO v5 makes some fine-tuning on the basis of YOLO v4 such as slicing input images into four small ones and concatenating them together for convolution operation. As a classical anchor-base object detector, YOLO v5 first pre-processes the anchors, then corrects the Bounding Box (BBOX) according to the off-set values, finally obtains the probability of the detected object in this BBOX [24]. Each anchor outputs three parts, the object probability refers to the possibility that an object exists in this anchor, off-set value of BBOX refers to the off-set between the anchor BBOX and the ground truth BBOX, and the probability vector of all categories refers to the possibility that the object in this anchor belongs to each category. The number of anchors depends on the size of the feature map, in which each point has three anchors with different shapes predetermined by hyper-parameters. We have trained and tested the state-of-the-art object detectors on the TT100k dataset [30]. YOLO v5 achieves the best performance in terms of real time and accuracy. Therefore, we chose the YOLO v5 based TSR system as the attack target to evaluate our proposed physical adversarial attack.\n\n\nB. Physical Adversarial Examples\n\nMost of the prior research either focused on the digital adversarial attacks, or launched physical adversarial attacks against the image classifiers [31]. However, successfully attacking a few static images cannot threaten the object detectors dealing with video streaming. There is no practical and satisfactory attempt to physically attack the object detectors due to three major challenges, i.e., cross-domain conversion, image transformation, and limited capability.\n\n1) Cross-Domain Conversion: As illustrated in Fig.2, AEs are generated in the digital domain, then printed into the physical domain, and finally re-taken by the camera back to the digital domain. The experimental results show that the digital-physical-digital conversion significantly degrades the adversarial toxicity of AEs [32]. To address this challenge, Jan et al. [33] presented an image-to-image translation network to simulate the digital-physical conversation. A conditional Generative Adversarial Network (cGAN) [34] is used to learn the digital-physical conversion for generating a synthetic physical image, then it is served to produce adversarial noises in AEs generation. The cGANs model needs to be trained with a set of paired static images. Thus, the robustness of the physical AEs is improved when attacking several image classifiers.\n\n2) Image Transformation: Different distances, angles, and illuminations will result in image transformations that impact the robustness of the physical AEs. AEs generated through the L-BFGS attack [5], fast gradient sign method [7], and the C&W attack [6] often lose their adversarial nature once subjected to minor transformations [35,36]. To address this challenge, Athalye et al. [12] introduced the Expectation Over Transformation (EOT). EOT uses a chosen distribution of transformation functions and constrains the expected effective distance between AEs and the input images. EOT is able to generate 3D printed AEs which remain adversarial under a range of conditions. Within the framework of EOT, the physical AEs successfully deceive a standard image classifier.\n\n3) Limited Capability: Further difficulty comes from the fact that the DNNs model is usually only a component in the whole computer vision system. For most applications, the attackers can neither get access to data inside the system nor obtain the parameters of the DNNs model. Instead, they can only manipulate objects in the physical environment. Therefore, the limitation of the attacker's capability limits the threat of the physical AEs. NaturalAE [37] proposed in 2021 used the natural AEs generated under white-box settings to attack the black-box models. However, the attack success rate is low, and the attack range is only 5m. For a moving vehicle, the attack is only effective within less than half a second.\n\n\nIII. THE PROPOSED PHYSICAL AE PIPELINE\n\nIn this section, we will elaborate on our proposed physical AEs pipeline. We first give the threat model with the goal and capabilities of the attacker. Then we present the 3-step AE attack pipeline which addresses the challenges in the previous section. Finally, we describe each of the three steps and show in detail the AEs generation algorithms for four attacks: Hiding Attack (HA), Appearance Attack (AA), Non-Target Attack (NTA), and Target Attack (TA). \n\n\nA. Threat Model\n\nThe attacker's goal is to mislead the victim vehicle's TSR system to incorrect classification of a given traffic sign. To this end, the attack can be further modeled as Hiding Attack (HA) where the attacker wants to hide the traffic sign, Appearance Attack (AA) where the attacker wants to create an object which can be recognized as a specific sign, Non-Target Attack (NTA) where the attacker simply wants the classification result to be incorrect, and Target Attack (TA) where the attacker wants the misclassified result to be a specific sign that is not the original traffic sign. Formally speaking, let f (\u00b7) be the image classifier used in the TSR system, for a given input image x whose category label is y, the attacker wants the TSR system to output a label that is different from y.\n\nWe adopt (i) the black-box TSR model where the attacker does not know the implementation details of the TSR system such as its structure and parameters. Furthermore, we assume that (ii) the attacker does not have physical access to the invehicle networks to analyze or modify the data in the TSR system. However, the attacker (iii) has access to the traffic sign which he can physically modify. Under these assumptions, the attacker's goal can be formulated as, generate an AE x = x+\u03b4 for the input image x, where \u03b4 is a perturbation to x, such that:\nf (x ) = y(1)\nWe believe that this is the most natural and strongest model for the attack scenario where an attacker wants to fool the TSR system of a victim vehicle. Any prior knowledge of the victim vehicle's information such as the model of the TSR, the object detector it uses, the camera(s) equipped in the vehicle, and the driving status (e.g. speed, weather, road condition) might provide additional help to the attacker and make the attack easier.\n\nIt is important to clarify that the previous AEs in the digital domain cannot be applied to this threat model. First, adversarial attack assumes a white-box model of the target DNN, the TSR system is a black-box and the parameters and structure of the target DNN models used in the TSR (assumption i) are not available to the attacker. So the attacker will not be able to generate digital AEs directly (see the top of Fig.3). Second, even if the attacker manages to create some digital AEs, because he does not have access to the in-vehicle networks (assumption ii), the attacker will not be able to inject such digital AEs directly into the TSR system (see bottom of Fig.3). However, the attacker can modify the traffic sign and hope the victim vehicle's TSR system will misclassify it.\n\n\nB. Attack Pipeline\n\nAs illustrated in Fig.4, there are three major steps in the proposed adversarial attack pipeline.\n\nStep 1. As shown in Fig.4(a), in order to improve the robustness of the physical AEs, we extend the distribution of image transformations to simulate the changes of distance, angle, and illumination in the real world. The transformed images will be embedded into the background as the foreground to simulate the perspective of the object detectors.\n\nStep 2. As shown in Fig.4(b), The BBOXes are associated with x, and can be extracted from the prediction results of YOLO v5. Single BBOX (S-BBOX) filter and Multiple BBOX (M-BBOX) filter are designed to obtain BBOXes for efficient perturbation training.\n\nStep 3. As shown in Fig.4(c), each BBOX contains three types of information, i.e., the probability of containing x or x , the probability vector for each category, and the BBOXes position offset. With the first two pieces of information, four loss functions are presented to generate AEs for aforementioned four attack vectors respectively.\n\n\nC. Image Preprocessing\n\nThe conventional AEs generation algorithms adopted the image transformations in EOT [12], including rotation, perspective, brightness, contrast, saturation, hue, and Gaussian noise. Furthermore, we extend distribution with the blur transformation to simulate conditions such as camera shake, and the resolution transformation that improves the robustness of AEs for varying distance to the camera. In step 1, we address the image transformation by setting:\nx t = t(M (x ))(2)\nwhere M (\u00b7) is the mask function to constrain the area where the perturbation is added. t(\u00b7) is a transformation vector randomly selected from distribution of image transformations T .\n\nWe have attempted to fix the center point of the foreground at any position in the entire background. However, since part of the foreground is out of the background, it will make the perturbation training difficult to converge. Considering the fact that the camera will capture the complete traffic signs in most cases, we keep a certain distance between the center point and the edge of the background to guarantee that the entire foreground can be processed in the AEs generation algorithm. The image embedding algorithm is shown in Algorithm 1. During the optimization procedure, each transformed image should be embedded in different traffic backgrounds to imitate the perspective of the object detectors. random() can output \n7 w real = h real = random() \u00d7 m + a 8 s f = 3 i 9 p x = random()\u00d7(1\u2212w real \u22122\u00d7s f )+0.5\u00d7w real +s f // avoid clinging the edge 10 p y = random()\u00d7(1\u2212h real \u22122\u00d7s f )+0.5\u00d7h real +s f 11 bbox real = [p x , p y , h real , w real ] \u00d7 i a random value in [0, 1] R every time it is called. (p x , p y )\n, h real and w real are the center point, height and width of the foreground respectively. s f is a position factor to prevent the foreground from clinging to the edge of background or even out of background. To simulate the real scale change of foreground in the background, we choose two different scale ranges corresponding to the big object and small object to generate the height and width of each BBOX. r 1 , r 2 , and r 3 indicate the ratio of the transformed images embedded in the background. The higher the ratio, the larger the object. We found that when the object is small, the direction of gradient update is hard to search and the model may not converge. However, if the object is far from the camera, lowering the ratio will enhance the effectiveness of AEs. Our empirical study shows that the best results come from when we choose the ratio in [0.01, 0.1] with probability 20% and in [0.1, 0.5] with 80%. Therefore, (r 1 , r 2 , r 3 ) is set to (0.01, 0.1, 0.5) in our experiment.\n\n\nD. Bounding Box Filter\n\nAs mentioned before, YOLO v5 detects the object according to the feature maps of three different scales. For example, if the size of the input image is 640 \u00d7 640, the sizes of the three output feature maps are 20 \u00d7 20, 40 \u00d7 40, and 80 \u00d7 80 respectively. Each pixel in the feature maps is fixed with three anchors of different sizes to get the intermediate results, including the probability when there is an object in each anchor (Q, 1), the confidence vector of the object category (Q, N ), and the off-set value vector between the real object and the fixed anchor (Q, 4). Therefore, there are a total of Q = 3 \u00d7 (20 \u00d7 20 + 40 \u00d7 40 + 80 \u00d7 80) prediction BBOXes, and each has S = 1 + 4 + N prediction values. In total, the final output of YOLO v5 is a matrix with dimension Q \u00d7 S. However, most of the prediction results of YOLO v5 are useless for perturbation training because some BBOXes detect either only some parts of x or other objects. Therefore, we need to filter those useless BBOXes to improve the efficiency of the perturbation training.\n\nWe first extract the prediction BBOXes O bbox as defined below:\nO bbox = f (emb(b, x t , bbox real ))(3)\nwhere b refers to the background image, f (\u00b7) is the object detector (YOLO v5). emb(b, x t , bbox real ) is an embedding function, in which the foreground object x t is embedded into the background image b according to bbox real (Algorithm 1).\n\nTwo BF (\u00b7) methods, S-BBOX and M-BBOX, have been proposed to filter the prediction BBOXes of YOLO v5 and \nbbox of f set , bbox conf = extract(bbox pred ) 2 bbox = plus(bbox of f set , bbox anchor ) 3 IOU s = iou(bbox, bbox real ) 4 if attack is HA then 5 index = where(IOU s > 0.5) 6 middle = bbox pred [index] 7 middle conf = bbox conf [index] 8 index conf = top(middle conf , k) 9 bbox training = middle[index conf ] 10 else if attack is AA or NTA then 11 index = top(IOU s, k) 12 bbox training = bbox pred [index]\nextract k BBOXes for the perturbation training. S-BBOX is used for TA, while M-BBOX has two modes, one is used for HA and the other for AA and NTA.\n\nThen, we decompose the extracted k BBOXes and obtain the information for the perturbation training:\nV, P = split(BF (O bbox ))(4)\nwhere split(\u00b7) is the matrix split function. The first part is the probability P \u2208 R k of each target box containing the object x or x , the second part is the confidence vector V \u2208 R k\u00d7N of the object category. Finally, V and P are used in the four loss functions corresponding to four attack vectors to train the perturbation.\n\n\n1) S-BBOX:\n\nThe Intersection Over Union (IOU) value between two BBOXes is calculated as follows:\niou(bbox A , bbox B ) = area(bbox A ) \u2229 area(bbox B ) area(bbox A ) \u222a area(bbox B )(5)\nS-BBOX first filters out most of the prediction BBOXes whose detection confidence is lower than the threshold (equal to the NMS threshold of YOLO v5). Then, S-BBOX calculates the IOU value between each prediction BBOX and bbox real by Equ. (5). The prediction BBOX with the highest IOU value is for the perturbation training (k = 1 in S-BBOX).\n\n2) M-BBOX: The M-BBOX defined in Algorithm 2 has two modes, a hiding mode for HA, and a non-hiding mode for AA and NTA. extract(\u00b7) extracts the bbox of f set and bbox conf from the bbox pred . where(\u00b7) obtain the indexes that satisfy the confidence in the brackets. top(A, k) search the indexes of prior k in the ranking of vector A. plus(\u00b7) achieve that adding the coordinate value of two input BBOXes.\n\nHiding Mode: When launching HA, we first calculate the IOU values between all the prediction BBOXes and the bbox real . Then those BBOXes with IOU greater than the threshold are filtered out (Experimental results show that 0.5 is the best threshold). Finally, the filtered BBOXes are sorted according to their object confidence, and the prior k BBOXes in the ranking of confidence are used to train perturbation.\n\nNon-Hiding Mode: When launching AA and NTA, we first calculate the IOU values as in hiding mode. Then we directly obtain the BBOXes in the top k IOU values to train perturbation.\n\n\nE. Four Attack Vectors\n\nThe final goal of the four attack vectors is to deceive the target DNN models like YOLO v5 as imperceptible as possible. The optimization function is defined by:\n\narg min\n\nx,\u03b4\u2208R h\u00d7w\u00d7\u03b5 E b\u223cB,t\u223cT loss * (V, P, y, y )\n\nwhere B refers to the background collection. We use L 2 distance to constrain the size of perturbation to improve its imperceptibility. The L 2 distance loss function is:\nL dis = x \u2212 x 2 2(7)\n1) Hiding Attack: The goal of HA is to make the object detector fail to find the object. The perturbation needs to eliminate the features of x as a traffic sign by the loss function as below:\nloss H = c nk n i=1 k j=1 ( 1 1 \u2212 P j i ) + L dis(8)\n2) Appearance Attack: The main goal of AA is to make the object detector misrecognize AEs that cannot be recognized by human eyes as desired objects. This attack is to train perturbation on a blank image that can be recognized by the object detector as a specified category. Thus, the minimum Euclidean distance between AEs and the blank image is not required in AA. The loss function for AA is given as:\nloss A = c nk n i=1 k j=1 ( 1 P j i \u00d7 V j,y i )(9)\n3) Non-Target Attack: The goal of NTA is to make the object detector misrecognize AEs that belong to a certain category as some other categories. The perturbation needs to eliminate x's features of the correct category, but retain x's features of a traffic sign. The loss function for NTA can be defined as:\nloss N T = c nk n i=1 k j=1\n( 1 \nP j i + 1 1 \u2212 V j,y i ) + L dis(10)\n\n4) Target Attack:\n\nThe goal of TA is to make the object detector misrecognize AEs that belong to a certain category as the target category y . The perturbation needs to not only change x's features as NTA, but also add the features of y . Thus, it will be tricky to design the loss function in TA. The loss function can be defined as: 11) where N is the number of categories that YOLO v5 can detect. The adjustable parameter k is the number of target boxes that YOLO v5 extracts from each sample. In each loss function, c needs to be adjusted in each training phase according to the actual environmental conditions. n is the number of samples used for training. As shown in Fig.5, the larger the value of c is, the greater the perturbation will be added, and hence the more robust the generated AEs will be.\nloss T = c nk n i=1 k j=1 ( 1 P j i + 1 V j,y i + N z=1,z =y 1 1 \u2212 V j,z i ) + L dis(\n\nIV. EVALUATION\n\nWe launch the four attack vectors (i.e., HA, AA, NTA, and TA) against the state-of-the-art object detectors and a brandnew vehicle. Considering that the TSR system in the vehicle only recognizes the speed limit signs, we focus on physically attacking the speed limit of 40km/h (limit 40 for short), which is generally located on the exit ramp of the freeway thus is more life-threatening. Due to the space limit, we cannot present all the experimental results in this paper. More than 1,000 video clips and more evaluation results are uploaded on our demo website: https://seczone.cn/contents/422/1024.html.\n\n\nA. Experimental Setup\n\nAEs are generated on the server equipped with two Intel Xeon-E5-2680 V4 CPUs, four NVIDIA GTX3080 GPUs, and \n\n\nDetector mAP\n\nFaster R-CNN (ResNet-50) [22] 72.6 SSD (VGG-16) [39] 72.9 RetinaNet (ResNet-50) [40] 69.1 CenterNet (Resnet-50) [41] 72 YOLO v3 (Darknet-53) [25] 73.1 YOLO v5 (CSPDarknet-53) [26] 77.8 64GB RECC DDR4-2400MHz memory. Then we fabricate the physical AEs in accordance with the production specifications of the traffic signs, including the material and size, etc. We conduct multiple sets of outdoor experiments under a variety of environmental conditions. The distances range from 0m to 30m, the angles range from \u221260 \u2022 to 60 \u2022 , and the illuminations range from sunny days, cloudy days to nights. The videos are captured by the built-in cameras of the Huawei nova7 with 64 Mega pixels and aperture of f/1.8, and the Samsung S10 with 16 Mega pixels and aperture of f/2.4. The effectiveness and robustness of the physical AEs are evaluated by analyzing each image frame in the video streaming. In order to present the threat of the physical adversarial attack more comprehensively, we not only count the attack success rate but also measure the impact of AEs on the detection confidence.\n\n\n1) Dataset:\n\nThe YOLO v5 based TSR system is trained with the TT100k dataset [30], which is composed of 2048 \u00d7 2048 images. The TT100k dataset declares that there are 221 categories containing 100,000 images of 30,000 traffic sign samples. However, we only manage to obtain 16,823 images with 26,337 traffic sign samples, 6,107 images for training, 3,073 for testing, and 7,643 other images from its website. We use the original TT100K dataset to train the object detectors first. The detectors will not converge when the high-resolution images are directly resized into the 640 \u00d7 640 input images for training. Thus, we divide each 2048 \u00d7 2048 image into 16 640 \u00d7 640 images to train the TSR system. Besides, we find that only 150 categories have data in the TT100k dataset, and among the nonempty categories, half of them have less than 10 images. Such unbalanced distribution of training data will severely impact the performance of the DNNs models [38]. After many training experiments, we screened out the traffic sign categories with top 50 data volume to form the new dataset for our experiments. The new TT100k dataset has 50 categories of 21,881 images with 640 \u00d7 640, 14474 images for training, 7,407 images for testing, and contains 40,550 traffic sign samples. Table I shows the performance of the object detectors on the new dataset. Among the six detectors, Faster R-CNN [22] is a two-stage detection, others are all one-stage target detectors; CenterNet [41] is an anchor-free detector, others are all anchor-base detectors. The one-stage detectors, YOLO v3 [25], YOLO v5 [26], and SSD [39], have high detection accuracy, even outperforming the two-stage detector Faster R-CNN [22]. CenterNet [41] has lower detection accuracy than most anchor-base detectors on the new dataset. In general, YOLO v5 [26] performs the best on the new dataset.\n\n2) Hyper-Parameters Setup: Stochastic Gradient Descent (SGD) algorithm [42] is utilized for the optimization of the Hiding Attack. HA aims to hide limit 40 in the background. In HA, the c is set to 1e + 2, the learning rate is set to 0.1, and k is set to 10.\n\nAppearance Attack. AA aims to create a traffic sign which can be recognized as limit 5 by YOLO v5 from a blank image. In AA, x is a randomly sampled metric with 640 \u00d7 640. c is set to 1e + 5, the learning rate is set to 0.2, and k is set to 1.\n\nNon-target Attack. NTA aims to make YOLO v5 recognize the limit 40 as other categories. In NTA, c is set to 1e+5, the learning rate is set to 0.1, and k is set to 3.\n\nTarget Attack. TA aims to make YOLO v5 recognize the limit 40 as the limit 60. In TA, the c is set to 1e + 3, the learning rate is set to 0.1, and k is set to 1.\n\n\n3) Evaluation Metrics:\nN s = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 Z(x) > th \u2229 Z(x ) < th, HA g(V x ) = y , AA g(V x ) = y \u2229 g(V x ) = y, N T A g(V x ) = y \u2229 g(V x ) = y , T A N a = 1, HA&AA g(V x ) = y, T A&N T A (12)\nThe attack success rate is defined as R s = N s / N a \u00d7 100%. As shown in Equ. (12), g(\u00b7) denotes the argmax(\u00b7) function that outputs the index of the max value in the vector. In HA and AA, N a is every image frame captured by the camera, while in NTA and TA, N a is the image frame in which the object is correctly detected. N s depends on the attack goals of the four attack vectors. In HA, a successful attack means that the probability of detecting x (Z(x) = max(V x ) \u00d7 P x ) is greater than the threshold th = 0.25, and the probability of detecting x (Z(x ) = max(V x ) \u00d7 P x ) is less than the threshold. In AA, a successful attack means x is detected as the target category y . In NTA, a successful attack means x is detected as y, but x is not detected as the original category y. In TA, a successful attack means x is detected as y, and x is detected as the target category y .   Fig.6, more than 200 image frames are captured and processed ( Fig.13 and Fig.14 in appendix). Since the visibility is low at night, we only conduct experiments within 25m. The depth of red represents the value of R s , the darker the color indicates the higher the attack success rate.\n\nFor HA, we achieve the best attack effectiveness on a sunny day. For AA, we achieve the best attack effectiveness at night, especially in the region of [10m, 20m]. However, in the dark environment within 10m, R s is very low. On sunny and cloudy days, R s is high in the region of [10m, 15m], but AA almost failed at 25m away. Thus, R s is relatively higher in the darker light, while both larger angles and distances will significantly reduce R s . The experimental results indicated the huge impact of reflection at night on AA too. For TA, we achieve the best attack effectiveness in the region of [10m, 20m] at [\u221245 \u2022 , 45 \u2022 ]. At night, TA almost fails within 10m in the dark environment. On a sunny day and a cloudy day, R s degrades severely as the distance increases at 20m away. The results indicate that large angles reduce R s in TA when the distance is either very close or very far; while the impact of different illuminations on R s does not show a strong regularity.\n\nThen we increased the height of the traffic sign to about 5.5m and conducted the four attack vectors on a sunny day at 0 \u2022 as shown in Fig.11 in Appendix.\n\nThe experimental results in Fig.7 indicate that the height of AEs has little impact on HA within 25m, but R s drops by 13% in the region of [25m, 30m]. For AA, R s rises from 60% to 100% in the region of [0m, 10m] when the height of AEs rises from 1.5m to 5.5m, but R s drops by 25% in the region of [15m, 20m]. Thus, R s in AA increases with height in the closer region, while at 20m away, R s in AA decreases as the height of AEs increases. Similarly, NTA has higher R s in the We also test the four attack vectors in a vehicle with a speed of [20km/h, 30km/h] as shown in Fig.12 in Appendix. The test is conducted in the region of [0m, 30m] at [\u221230 \u2022 , 30 \u2022 ] on a sunny day. The attack effectiveness is listed in Table II. The average R s s in HA, NTA, and TA are all over 90%. The results of the real-road driving test for AA are consistent with the results in Fig.6. The experimental results in Table II show that our generated AEs are robust in the real-road driving test.\n\n2) Confidence: In order to further study the effectiveness and robustness of NTA and TA, we measure the detection confidence of each image frame. We fix the height of the traffic signs to about 1.5m and choose the video clips recorded on a sunny day and a cloudy day in the region of [0m, 30m] at 0 \u2022 , 30 \u2022 , and 45 \u2022 , respectively. As shown in Fig.8, the blue circle presents confidence of the correct category y. It needs to be emphasized that in NTA, the red cross represents the highest confidence of all the incorrect categories, while in TA, it represents the confidence of the target category y . Fig.8(a,e,i,m) in the leftmost column show confidence in each original input image. Limited by the resolution of the camera, the detection performance of the original images drops significantly at 25m away. In several image frames, the confidence of the correct category y drops severely, while the confidence of the incorrect categories increases.\n\nBy comparing the results of NTA on a sunny day in Fig.8(b,c,d) and the results of NTA on a cloudy day in Fig.8(f,g,h), we find that the illumination has little impact on the attack effectiveness of NTA. By comparing the results of TA on a sunny day in Fig.8(j,k,l) and the results of TA on a cloudy day in Fig.8(n,o,p), it indicates that the attack effectiveness of TA is better in a sunny day within 10m. By comparing the experimental results at 0 \u2022 in Fig.8(b,f,j,n), 30 \u2022 in Fig.8(c,g,k,o), and 45 \u2022 in Fig.8(d,h,l,p), it indicates three phenomena. First, the large angle has a greater impact on TA than NTA. Second, at very close and far distances, the large angle significantly degrades the attack effectiveness of TA. Third, attack effectiveness is better in a sunny day than in a cloudy day.\n\nC. Attack Transferability 1) State-of-the-art Object Detectors: We use AEs generated with YOLO v5 to attack the representative object detectors, including Faster R-CNN (two-state) [22], SSD [39] and RetinaNet [40] (one-stage and anchor-base), and CenterNet (one-stage and anchor-free) [41]. Those object detectors are trained on the same TT100k [30] dataset for fairness. The set of experiments is conducted in the region of [0m, 30m] at 0 \u2022 . According to the experimental results in Table III, HA and NTA achieve higher R s , especially on SSD and Faster R-CNN, while AA and TA on RetinaNet and CenterNet almost fail. We measure the performance of the four detectors when detecting large, medium and small objects, and find that RetinaNet and Centernet have low accuracy when detecting small objects. Therefore, low accuracy at long distances determines that they have low transferability in the region of [20m, 30m], but they still have high accuracy at close distances, and thus have good transferability in the region of [0m, 15m].\n\n2) Brand-new Vehicle: The major reason why we generated adversarial limit 40 is that the TSR system of the brand-new vehicle can only recognize the speed limit signs. As shown in Fig.9(a), only after the vehicle passes the speed limit sign, the recognition result can be displayed on the screen. In HA and NTA, Fig.9(b,c) show that after the vehicle passes the adversarial limit 40, it does not display the correct limit 40, which means HA and NTA successfully fool the eyes of the vehicle.\n\n\nD. Comparison and Discussion\n\nThere are few works focusing on the physical AEs against the object detectors. As listed in Table IV, We compare our proposed method with Zhao's method [18], ShapeShifter [31], and NaturalAE [37] from different dimensions. In general, our method has the following advantages. First of all, to the best of our knowledge, it is the first attempt to physically attack a production-grade object detector in a real-world vehicle and achieve significant effectiveness. Second, we take into account the impact of light reflection in the dark environment on robustness and conduct a complete set of experiments at night. Surprisingly, all the four attack vectors achieve almost 100% R s against the YOLO v5 based object detector at a certain distance and angle. Third, NTA and TA are implemented in the physical domain and exhibit satisfactory transferability against the state-of-the-art object detectors, which lays a foundation for studying the adversarial attacks against black-box models in both the digital and the physical domains.\n\nCombining all the experimental results, we can draw a conclusion that the physical AEs for HA and NTA have better robustness and transferability against multiple object detectors, while AA and TA have high R s only on specific attack targets. The lower R s and poorer transferability are due to two-fold reasons. First, HA only needs to eliminate the features of all the categories in the perturbation training process, and NTA only needs to eliminate the features of the correct category y. But for AA and TA, the features of the target category y should be imitated, which is much more difficult than the feature elimination. In addition, TA also needs to eliminate the features of y before the feature imitation. Therefore, the digital-physical-digital conversion has a greater impact on the AEs generated for AA and TA, which results in lower transferability. Second, the similarity of the two target models also determines the transferability of the physical AEs. The architectures of RetinaNet and CenterNet are quite different from YOLO v5. As for the TSR system of the brand-new vehicle, even the architecture of the DNNs model is unknown. The black-box setting is always the biggest obstacle for the adversarial attack in both digital and physical domains. In future work, we will launch the physical adversarial attack against the black-box DNNs model with the help of the sidechannel attack [43]. \n\n\nV. RELATED WORK\n\nWe survey the landmark works related to the adversarial attacks in Fig.10, and discuss the AEs defense mechanisms to protect the DNNs models from the adversarial attacks.\n\nA. Adversarial Attacks 1) Digital AEs: In 2013, Szegedy et al. [5] discovered the existence of AEs in the digital domain and generated AEs using the box-constrained L-BFGS. Since then, there have been a lot of initial works [6][7][8][9][10] focusing on generating digital AEs. Goodfellow et al. [7] proposed the fast gradient sign method to generate AEs on the MNIST dataset. Carlini and Wagner [6] introduced three C&W attacks for the L 0 , L 2 , and L \u221e distance metrics. The L 0 attack was the first published attack that caused the targeted misclassification on the ImageNet dataset. Although most of the digital AEs lose their adversarial nature in the physical environment [35,36], these three classic methods are still used for generating physical AEs.\n\n2) Physical AEs: Athalye et al. [12] proposed the Expectation Over Transformations (EOT) method that laid the cor-nerstone of the physical AEs generation algorithms. Evtimov et al. [44,45] used EOT method to generate robust physical AEs to attack YOLO v2 based TSR systems. Sitawarin et al. [14] presented two novel attack vectors against the traffic sign classifier. Based on [14], Morgulis et al. [46] claimed to attack the real production-grade image classifiers for the first time. Chen et al. [31] physically attacked the fast R-CNN based TSR systems using large perturbations. Lovisotto et al. [47] used the light of a projector to generate short-lived adversarial perturbations in the indoor experiments. However, the physical AEs generated by these methods cannot remain robust under a variety of environmental conditions. Until very recently, there were several attempts to improve the robustness of the physical AEs. Jan et al. [33] used an image-to-image translation network to simulate the digitalphysical conversion for generating the physical AEs. This improvement performed well on several image classifiers with static input images, but it was not verified on the object detectors with video streaming. Xue et al. [37] proposed the natural and robust physical AEs against the object detectors. Since the generated AEs are effective within 5 meters, they pose almost no threat to a moving vehicle. Zhao et al. [18] presented the hiding attack and appearance attack on the YOLO v3 based object detector and achieved better robustness. Limited by the capability of the AEs generation algorithm, the imperceptibility of AEs is not properly addressed which makes the two attack vectors lack real-world threat to the productiongrade TSR systems.\n\n\nB. Defense Mechanisms\n\nResearch has proposed several defense mechanisms to protect the DNNs models against AEs, which can be divided into three categories,i.e., input image preprocessing, AEs detection, and model enhancement.\n\n1) Input Image Preprocessing: Imperceptible requirements make AEs not robust enough to the external noises or data distortions [48]. Inspired by this opportunity, preprocessing the input image to eliminate the adversarial features could be a type of potential defense mechanism. In 2017, Das et al. [49] explored and demonstrated that the system's JPEG compression could be used as an effective preprocessing step in the classification pipeline to significantly reduce the impact of AEs. JPEG compression has ability to remove the highfrequency signal components inside the image, which helps eliminate malicious disturbances. Guo et al. [50] selected a small group of pixels and reconstructs the simplest image consistent with the selected pixels so that malicious disturbance in the image was removed. Similar to [50], Prakash et al. [51]  proposed a pixel deflection method that used semantic maps and randomization to select a small number of pixels, and then replace them with randomly selected neighboring pixels. This process would generate noise and needed a wavelet noise reduction filter. In order to solve this problem, Liao et al. [52] used U-net structure to modify the denoising autoencoder and proposed denoising U-net. However, the residual noise impact might still increase as the number of network layers increases. In 2019, Xie et al. [53] developed a new network architecture to improve robustness with a feature denoiser that combined confrontation training and graphics processing.\n\n2) AEs Detection: It is also possible to directly detect the adversarial features and prevent AEs from entering DNNs models. Li et al. [54] used a cascade classifier to detect AEs effectively. Lu et al. [55] proposed SafetyNet to detect AEs based on that benign images and AEs have different ReLU activation patterns in the model. Metzen et al. [56] chose a small detector sub-network to enhance the robustness of the DNNs model. The sub-network would be trained on a binary classification task to distinguish AEs from benign images. On this basis, Xu et al. [57] proposed a feature compression method to detect AEs by comparing the prediction consistency of the original image and the compressed image. If the classification result of the original input image and the compressed input image were different, the input might be an AE. In 2020, Chen et al. [58] presented a defense mechanism to detect the process of AEs generation. By keeping the historical record of past queries, it could determine when a series of queries appeared to generate AEs.\n\n3) Model Enhancement: Enhancing the DNNs models in the training phase is also a potential defense mechanism against adversarial attacks. In the next year after the AEs concept appeared, Goodfellow et al. [7] proposed to add AEs to the training set to make the trained DNNs model robust against specific types of AEs. Tramer et al. [59] believed that [7] was vulnerable to single-step attacks. They proposed an ensemble adversarial training method based on gradient masks, using AEs generated by other pre-trained classifiers to expand the training set. This ensemble adversarial training algorithm was more effective because it separated the training of the model and the process of generating AEs. In 2020, Wong et al. [60] paid more attention to the cost of adversarial training, and proposed a fast adversarial training method. Defensive distillation was proposed by Papernot et al. [61] in 2016. The basic idea was to transfer knowledge of complex networks to simple networks by modifying the network structure and optimization items to prevent the model from fitting too closely to normal samples. However, the network needed to be retrained and was usually only effective against AEs considered in the training process. In 2020, Goldblum et al. [62] studied how the adversarial robustness is transferred from the teacher DNNs model to the student DNNs model in the knowledge distillation process, and introduced adversarial robust distillation to refine the robustness to the student DNNs. Another robustness enhancement method was to use a generative model to project potential AEs onto a benign dataset, and then classify them. Among them, the PixelDefend method proposed by Song et al. [63] used the PixelCNN generative model, and the Defence-GAN method proposed by Samangouei et al. [64] used a generative adversarial network structure.\n\n\nVI. CONCLUSION\n\nIn this paper, we present a systematic pipeline to generate the physical AEs against the object detectors. In order to improve the robustness of AEs in the physical domain, we extend the distribution of image transformations, design the S-BBOX filter and the M-BBOX filter, and modify the four loss functions for the four attack vectors respectively. We conduct extensive experiments under a variety of environmental conditions, i.e., the distance varies from 0m to 30m, the angle varies from \u221260 \u2022 to 60 \u2022 , the illumination varies from sunny day to cloudy day to night. The experimental results show that HA, NTA, and TA achieve a success rate of more than 90% in real-world driving tests against the YOLO v5 based TSR system. The generated AEs exhibit high transferability against the other state-of-the-art object detectors. Furthermore, HA and NTA successfully fooled the TSR system of a brandnew vehicle, which is a life-threatening case for autonomous vehicles.     \n\n\nAPPENDIX\n\nFig. 1 .\n1Architecture of YOLO v5.\n\nFig. 2 .\n2The Digital-Physical-Digital cross-domain conversion of AEs.\n\nFig. 3 .\n3Threat model of physical adversarial attack: the TSR system is a black box and the attacker does not have access to it.\n\nFig. 4 .\n4Physical adversarial attack pipeline. (a) Image transformation and embedding. (b) BBOX filter. (c) Perturbation training for four attack vectors. Algorithm 1: Image Embedding Input: r 1 ,r 2 ,r 3 : three ratios of foreground; i: scale of background image. Output: bbox real : bounding box information of foreground. 1 if random()\n\nFig. 5 .\n5Impact of c on imperceptibility of AEs. (a) c is set to a small value. (b) c is set to a medium value. (c) c is set to a large value.\n\nFig. 6 .\n6Attack success rate of four attack vectors under a variety of environmental conditions. (a) HA on a sunny day. (b) AA on a sunny day. (c) NTA on a sunny day. (d) TA on a sunny day. (e) HA on a cloudy day. (f) AA on a cloudy day. (g) NTA on a cloudy day. (h) TA on a cloudy day. (i) HA at night. (j) AA at night. (k) NTA at night. (l) TA at night. loss functions, in which the transformation batch is set to 16, and the size of the input image and the background image is set to 640 \u00d7 640. For the four attack vectors, we set different experimental hyperparameters as follows:\n\nFig. 7 .\n7Impact of height on attack effectiveness.\n\nFig. 8 .\n8Confidences of NTA and TA under a variety of environmental conditions. (a) NTA with original image on a sunny day. (b) NTA at 0 \u2022 on a sunny day. (c) NTA at 30 \u2022 on a sunny day. (d) NTA at 45 \u2022 on a sunny day. (e) NTA with original image on a cloudy day. (f) NTA at 0 \u2022 on a cloudy day. (g) NTA at 30 \u2022 on a cloudy day. (h) NTA at 45 \u2022 on a cloudy day. (i) TA with original image on a sunny day. (j) TA at 0 \u2022 on a sunny day. (k) TA at 30 \u2022 on a sunny day. (l) TA at 45 \u2022 on a sunny day. (m) TA with original image on a cloudy day. (n) TA at 0 \u2022 on a cloudy day. (o) TA at 30 \u2022 on a cloudy day. (p) TA at 45 \u2022 on a cloudy day.\n\nFor\nNTA, since the strong reflection caused by direct lighting covers the perturbation features, R s degrades below 60% in the region of [15m, 20m] at [\u221230 \u2022 , 30 \u2022 ] at night. On a cloudy day, R s degrades below 20% in the region of [25m, 30m] at [\u221245 \u2022 , \u221230 \u2022 ] and [30 \u2022 , 45 \u2022 ]. The results indicate that both darker light and larger distances reduce R s , while the impact of different angles on R s does not show a strong regularity.\n\nFig. 9 .\n9HA and NTA against a brand-new vehicle. (a) The original speed limit 40. (b) HA. (c) NTA.\n\nFig. 11 .\n11Defense mechanisms against AEs. (a) Input image preprocessing. (b) AEs detection. (c) Model enhancement.\n\nFig. 12 .\n12Results of four attack vectors at a height of 5.5m. (a) HA. (b) AA. (c) NTA. (d) TA.\n\nFig. 13 .\n13Results of four attack vectors in real-road driving tests. (a) HA. (b) AA. (c) NTA. (d) TA.\n\nFig. 14 .\n14Results of four attack vectors at varying angles (\u221260 \u2022 ,\u221245 \u2022 ,\u221230 \u2022 ,0 \u2022 ,30 \u2022 ,45 \u2022 ,60 \u2022 ). (a) HA. (b) AA. (c) NTA. (d) TA.\n\nFig. 15 .\n15Results of HA under different illuminations. (a) Sunny day. (b) Cloud day. (c) Night.\n\n\nAlgorithm 2: M-BBOX Input: bbox pred : YOLO v5' s result metric; bbox real : BBOXes obtained at algorithm 1; bbox anchor : BBOXes of fixed anchors. Output: bbox training : BBOXes for the perturbation training 1\n\nTABLE I .\nIMAP / % OF STATE-OF-THE-ART OBJECT DETECTORS.\n\n\nB. Evaluation of Four Attack Vectors 1) Attack Success Rate: As shown inFig.6, the four attack vectors are evaluated in terms of the attack success rate R s under a variety of environmental conditions. In this set of experiments, we fixed the height of the traffic signs to about 1.5m.The distances are divided into five regions, i.e., [0m, 10m], [10m, 15m], [15m, 20m], [20m, 25m], [25m, 30m]. The video clips are recorded from far to close with a constant speed in each region at the angle of [\u221260 \u2022 , \u221245 \u2022 ], [\u221245 \u2022 , \u221230 \u2022 ], [\u221230 \u2022 , 30 \u2022 ], [30 \u2022 , 45 \u2022 ], and [45 \u2022 , 60 \u2022 ], respectively. For each R s in\n\n\nOn a cloudy day, in the region of [15m, 20m] at [45 \u2022 , 60 \u2022 ] and in the region of [25m, 30m] at [\u221230 \u2022 , 30 \u2022 ], R s degrades to 50%. At night, in the region of [10m, 15m] at [\u221245 \u2022 , \u221230 \u2022 ] and [\u221230 \u2022 , 30 \u2022 ], R s degrades below 70%. However, HA has a high R s at 10m away in the dark environment. The experimental results indicated that reflection at night has a greater impact on HA.\n\nTABLE II\nII. \nREAL-ROAD DRIVING TEST \n\nAttack Vector Number of image frames Rs/% \n\nHA \n824 \n96.48 \nAA \n630 \n60.48 \nNTA \n525 \n90.48 \nTA \n645 \n92.87 \n\nTABLE III. \nRs / % AND MAP / % ON STATE-OF-THE-ART OBJECT \n\nDETECTORS \n\nFaster \nR-CNN[22] \nSSD[39] RetinaNet[40] CenterNet[41] \n\nHA \n95.02 \n95.40 \n71.02 \n97.64 \nAA \n54.73 \n28.23 \n11.94 \n19.65 \nNTA \n99.78 \n100 \n52.18 \n47.8 \nTA \n58.62 \n46.38 \n2.03 \n0 \n\nmAP-s \n61.5 \n64.6 \n48.5 \n55.3 \nmAP-m \n77.7 \n79.3 \n77.2 \n77.9 \nmAP-l \n82.5 \n83.7 \n84 \n85.7 \n\nregion of [0m, 20m] and lower R s at 20m away when the \nheight of AEs rises from 1.5m to 5.5m. For TA, R s drops by \n45% in the region of [0m, 10m], but rises from 54% to 96% \nin the region of [20m, 25m] and rises from 3% to 57% in the \nregion of [25m, 30m] when the height of AEs rises from 1.5m \nto 5.5m. \n\n\n\nTABLE IV .\nIVCOMPARISON OF DIFFERENT ADVERSARIAL ATTACK METHODS IN PHYSICAL DOMAIN Fig. 10. Adversarial Attacks.Our Method \nZhao's Method [18] \nShapeShifter [31] \nNaturalAE [37] \n\nTarget Model \nYOLO v5 \nYOLO v3 \nFaster R-CNN \nYOLO v2 \nAttack Vector \nHA, AA, NTA, TA \nHA, AA \nTA \nTA \nDistance \n[0 m, 30 m] \n[0 m, 25 m] \n[0 m, 12 m] \n[0 m, 5 m] \nAngle \n[-60,60] \n[-60,60] \n[0,60] \n[-60,60] \nIllumination \nSunny, Cloudy, Night \nIndoor, Outdoor \nIndoor, Outdoor \nIndoor, Outdoor \nHeight \n1.5 m, 5.5 m \n1.5m \n1.5m \n1.5m \n\nTransferability \nFaster R-CNN, SSD, RetinaNet, CenterNet \nBrand-new vehicle (HA and NTA) \n\nFaster R-CNN, SSD \nRFCN, Mask R-CNN \nNone \nFaster R-CNN, SSD \n\n\n\nTABLE V .\nVDEFINITIONS OF THE NOTATIONS IN IMAGE FRAMESDefinitions5km/h speed limit 40km/h 60km/ht 80km/h 55ton weight limit other prohibition signsNotations in Images \npl5 \npl40 \npl60 \npl80 \npm50 \npo \n\n\n\nOn the robustness of face recognition algorithms against attacks and bias. R Singh, A Agarwal, M Singh, S Nagpal, M Vatsa, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34R. Singh, A. Agarwal, M. Singh, S. Nagpal, and M. Vatsa, \"On the robustness of face recognition algorithms against attacks and bias,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 09, 2020, pp. 13 583-13 589.\n\nOptimal feature selection-based medical image classification using deep learning model in internet of medical things. R J S Raj, S J Shobana, I V Pustokhina, D A Pustokhin, D Gupta, K Shankar, IEEE Access. 8R. J. S. Raj, S. J. Shobana, I. V. Pustokhina, D. A. Pustokhin, D. Gupta, and K. Shankar, \"Optimal feature selection-based medical image classification using deep learning model in internet of medical things,\" IEEE Access, vol. 8, pp. 58 006- 58 017, 2020.\n\nSaccadenet: A fast and accurate object detector. S Lan, Z Ren, Y Wu, L S Davis, G Hua, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS. Lan, Z. Ren, Y. Wu, L. S. Davis, and G. Hua, \"Saccadenet: A fast and accurate object detector,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, 2020, pp. 10 397-10 406.\n\nAdversarial examples: Opportunities and challenges. J Zhang, C Li, IEEE transactions on neural networks and learning systems. 31J. Zhang and C. Li, \"Adversarial examples: Opportunities and challenges,\" IEEE transactions on neural networks and learning systems, vol. 31, no. 7, pp. 2578-2593, 2019.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \"Intriguing properties of neural networks,\" arXiv preprint arXiv:1312.6199, 2013.\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, 2017 ieee symposium on security and privacy (sp). IEEEN. Carlini and D. Wagner, \"Towards evaluating the robustness of neural networks,\" in 2017 ieee symposium on security and privacy (sp). IEEE, 2017, pp. 39-57.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, arXiv:1412.6572arXiv preprintI. J. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and har- nessing adversarial examples,\" arXiv preprint arXiv:1412.6572, 2014.\n\nThe limitations of deep learning in adversarial settings. N Papernot, P Mcdaniel, S Jha, M Fredrikson, Z B Celik, A Swami, 2016 IEEE European symposium on security and privacy (EuroS&P). IEEEN. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, \"The limitations of deep learning in adversarial settings,\" in 2016 IEEE European symposium on security and privacy (EuroS&P). IEEE, 2016, pp. 372-387.\n\n2018 ieee security and privacy workshops (spw). J Kos, I Fischer, D Song, IEEEAdversarial examples for generative modelsJ. Kos, I. Fischer, and D. Song, \"Adversarial examples for generative models,\" in 2018 ieee security and privacy workshops (spw). IEEE, 2018, pp. 36-42.\n\nCommandersong: A systematic approach for practical adversarial voice recognition. X Yuan, Y Chen, Y Zhao, Y Long, X Liu, K Chen, S Zhang, H Huang, X Wang, C A Gunter, 27th {USENIX} Security Symposium ({USENIX} Security 18). X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang, H. Huang, X. Wang, and C. A. Gunter, \"Commandersong: A systematic approach for practical adversarial voice recognition,\" in 27th {USENIX} Security Symposium ({USENIX} Security 18), 2018, pp. 49-64.\n\nAdversarial examples in the physical world. A Kurakin, I Goodfellow, S Bengio, A. Kurakin, I. Goodfellow, S. Bengio et al., \"Adversarial exam- ples in the physical world,\" 2016.\n\nSynthesizing robust adversarial examples. A Athalye, L Engstrom, A Ilyas, K Kwok, International conference on machine learning. PMLRA. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, \"Synthesizing robust adversarial examples,\" in International conference on machine learning. PMLR, 2018, pp. 284-293.\n\nRobust physicalworld attacks on machine learning models. I Evtimov, K Eykholt, E Fernandes, T Kohno, B Li, A Prakash, A Rahmati, D Song, arXiv:1707.0894524arXiv preprintI. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song, \"Robust physical- world attacks on machine learning models,\" arXiv preprint arXiv:1707.08945, vol. 2, no. 3, p. 4, 2017.\n\nDarts: Deceiving autonomous cars with toxic signs. C Sitawarin, A N Bhagoji, A Mosenia, M Chiang, P Mittal, arXiv:1802.06430arXiv preprintC. Sitawarin, A. N. Bhagoji, A. Mosenia, M. Chiang, and P. Mittal, \"Darts: Deceiving autonomous cars with toxic signs,\" arXiv preprint arXiv:1802.06430, 2018.\n\nMultifeature fusion and enhancement single shot detector for traffic sign recognition. Y Jin, Y Fu, W Wang, J Guo, C Ren, X Xiang, IEEE Access. 8Y. Jin, Y. Fu, W. Wang, J. Guo, C. Ren, and X. Xiang, \"Multi- feature fusion and enhancement single shot detector for traffic sign recognition,\" IEEE Access, vol. 8, pp. 38 931-38 940, 2020.\n\nFast personal protective equipment detection for real construction sites using deep learning approaches. Z Wang, Y Wu, L Yang, A Thirunavukarasu, C Evison, Y Zhao, Sensors. 21103478Z. Wang, Y. Wu, L. Yang, A. Thirunavukarasu, C. Evison, and Y. Zhao, \"Fast personal protective equipment detection for real construction sites using deep learning approaches,\" Sensors, vol. 21, no. 10, p. 3478, 2021.\n\nA survey on semi-, self-and unsupervised learning for image classification. L Schmarje, M Santarossa, S.-M Schr\u00f6der, R Koch, IEEE Access. L. Schmarje, M. Santarossa, S.-M. Schr\u00f6der, and R. Koch, \"A survey on semi-, self-and unsupervised learning for image classification,\" IEEE Access, 2021.\n\nSeeing isn't believing: Towards more robust adversarial attack against real world object detectors. Y Zhao, H Zhu, R Liang, Q Shen, S Zhang, K Chen, Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security. the 2019 ACM SIGSAC Conference on Computer and Communications SecurityY. Zhao, H. Zhu, R. Liang, Q. Shen, S. Zhang, and K. Chen, \"Seeing isn't believing: Towards more robust adversarial attack against real world object detectors,\" in Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, 2019, pp. 1989-2004.\n\nA comparative study of state-of-the-art deep learning algorithms for vehicle detection. H Wang, Y Yu, Y Cai, X Chen, L Chen, Q Liu, IEEE Intelligent Transportation Systems Magazine. 112H. Wang, Y. Yu, Y. Cai, X. Chen, L. Chen, and Q. Liu, \"A comparative study of state-of-the-art deep learning algorithms for vehicle detection,\" IEEE Intelligent Transportation Systems Magazine, vol. 11, no. 2, pp. 82-95, 2019.\n\nOn the performance of one-stage and twostage object detectors in autonomous vehicles using camera data. M Carranza-Garc\u00eda, J Torres-Mateo, P Lara-Ben\u00edtez, J Garc\u00eda-Guti\u00e9rrez, Remote Sensing. 13189M. Carranza-Garc\u00eda, J. Torres-Mateo, P. Lara-Ben\u00edtez, and J. Garc\u00eda-Guti\u00e9rrez, \"On the performance of one-stage and two- stage object detectors in autonomous vehicles using camera data,\" Remote Sensing, vol. 13, no. 1, p. 89, 2021.\n\nYou only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" in Proceedings of the IEEE conference on computer vision and pattern recog- nition, 2016, pp. 779-788.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in neural information processing systems. 28S. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards real-time object detection with region proposal networks,\" Ad- vances in neural information processing systems, vol. 28, pp. 91-99, 2015.\n\nYolov4-5d: An effective and efficient object detector for autonomous driving. Y Cai, T Luan, H Gao, H Wang, L Chen, Y Li, M A Sotelo, Z Li, IEEE Transactions on Instrumentation and Measurement. 70Y. Cai, T. Luan, H. Gao, H. Wang, L. Chen, Y. Li, M. A. Sotelo, and Z. Li, \"Yolov4-5d: An effective and efficient object detector for autonomous driving,\" IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1-13, 2021.\n\nYolov4: Optimal speed and accuracy of object detection. A Bochkovskiy, C.-Y. Wang, H.-Y M Liao, arXiv:2004.10934arXiv preprintA. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, \"Yolov4: Optimal speed and accuracy of object detection,\" arXiv preprint arXiv:2004.10934, 2020.\n\nYolov3: An incremental improvement. J Redmon, A Farhadi, arXiv:1804.02767arXiv preprintJ. Redmon and A. Farhadi, \"Yolov3: An incremental improve- ment,\" arXiv preprint arXiv:1804.02767, 2018.\n\n. Ultralytics, EB/OLUltralytics, \"yolov5,\" [EB/OL], https://github.com/ultralytics/ yolov5 Accessed on May 15, 2020.\n\nCspnet: A new backbone that can enhance learning capability of cnn. C.-Y Wang, H.-Y M Liao, Y.-H Wu, P.-Y Chen, J.-W Hsieh, I.-H Yeh, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. the IEEE/CVF conference on computer vision and pattern recognition workshopsC.-Y. Wang, H.-Y. M. Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, and I.-H. Yeh, \"Cspnet: A new backbone that can enhance learning capability of cnn,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition work- shops, 2020, pp. 390-391.\n\nPath aggregation network for instance segmentation. S Liu, L Qi, H Qin, J Shi, J Jia, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionS. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, \"Path aggregation network for instance segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8759-8768.\n\nSpatial pyramid pooling in deep convolutional networks for visual recognition. K He, X Zhang, S Ren, J Sun, IEEE transactions on pattern analysis and machine intelligence. 37K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling in deep convolutional networks for visual recognition,\" IEEE trans- actions on pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904-1916, 2015.\n\nTraffic-sign detection and classification in the wild. Z Zhu, D Liang, S Zhang, X Huang, B Li, S Hu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZ. Zhu, D. Liang, S. Zhang, X. Huang, B. Li, and S. Hu, \"Traffic-sign detection and classification in the wild,\" in Pro- ceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2110-2118.\n\nShapeshifter: Robust physical adversarial attack on faster rcnn object detector. S.-T Chen, C Cornelius, J Martin, D H P Chau, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. SpringerS.-T. Chen, C. Cornelius, J. Martin, and D. H. P. Chau, \"Shapeshifter: Robust physical adversarial attack on faster r- cnn object detector,\" in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018, pp. 52-68.\n\nS Huang, N Papernot, I Goodfellow, Y Duan, P Abbeel, arXiv:1702.02284Adversarial attacks on neural network policies. arXiv preprintS. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel, \"Adversarial attacks on neural network policies,\" arXiv preprint arXiv:1702.02284, 2017.\n\nConnecting the digital and physical world: Improving the robustness of adversarial attacks. S T Jan, J Messou, Y.-C Lin, J.-B Huang, G Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33S. T. Jan, J. Messou, Y.-C. Lin, J.-B. Huang, and G. Wang, \"Connecting the digital and physical world: Improving the robustness of adversarial attacks,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 962-969.\n\nImage-toimage translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionP. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, \"Image-to- image translation with conditional adversarial networks,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1125-1134.\n\nFoveation-based mechanisms alleviate adversarial examples. Y Luo, X Boix, G Roig, T Poggio, Q Zhao, arXiv:1511.06292arXiv preprintY. Luo, X. Boix, G. Roig, T. Poggio, and Q. Zhao, \"Foveation-based mechanisms alleviate adversarial examples,\" arXiv preprint arXiv:1511.06292, 2015.\n\nNo need to worry about adversarial examples in object detection in autonomous vehicles. J Lu, H Sibai, E Fabry, D Forsyth, arXiv:1707.03501arXiv preprintJ. Lu, H. Sibai, E. Fabry, and D. Forsyth, \"No need to worry about adversarial examples in object detection in autonomous vehicles,\" arXiv preprint arXiv:1707.03501, 2017.\n\nNaturalae: Natural and robust physical adversarial examples for object detectors. M Xue, C Yuan, C He, J Wang, W Liu, Journal of Information Security and Applications. 57102694M. Xue, C. Yuan, C. He, J. Wang, and W. Liu, \"Naturalae: Natural and robust physical adversarial examples for object detectors,\" Journal of Information Security and Applications, vol. 57, p. 102694, 2021.\n\nCommunication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation. Y Chen, X Sun, Y Jin, IEEE transactions on neural networks and learning systems. 31Y. Chen, X. Sun, and Y. Jin, \"Communication-efficient federated deep learning with layerwise asynchronous model update and temporally weighted aggregation,\" IEEE transactions on neural networks and learning systems, vol. 31, no. 10, pp. 4229-4238, 2019.\n\n. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y , W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.\n\nSsd: Single shot multibox detector. A C Fu, Berg, SpringerFu, and A. C. Berg, \"Ssd: Single shot multibox detector,\" in European conference on computer vision. Springer, 2016, pp. 21-37.\n\nFocal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionT.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \"Focal loss for dense object detection,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980- 2988.\n\nObjects as points. X Zhou, D Wang, P Kr\u00e4henb\u00fchl, arXiv:1904.07850arXiv preprintX. Zhou, D. Wang, and P. Kr\u00e4henb\u00fchl, \"Objects as points,\" arXiv preprint arXiv:1904.07850, 2019.\n\nLarge-scale machine learning with stochastic gradient descent. L Bottou, Proceedings of COMPSTAT'2010. COMPSTAT'2010SpringerL. Bottou, \"Large-scale machine learning with stochastic gradi- ent descent,\" in Proceedings of COMPSTAT'2010. Springer, 2010, pp. 177-186.\n\nSide-channel attack in internet of things: a survey. M Devi, A Majumder, Applications of Internet of Things. SpringerM. Devi and A. Majumder, \"Side-channel attack in internet of things: a survey,\" in Applications of Internet of Things. Springer, 2021, pp. 213-222.\n\nRobust physicalworld attacks on deep learning visual classification. K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, C Xiao, A Prakash, T Kohno, D Song, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and D. Song, \"Robust physical- world attacks on deep learning visual classification,\" in Proceed- ings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 1625-1634.\n\nPhysical adversarial examples for object detectors. D Song, K Eykholt, I Evtimov, E Fernandes, B Li, A Rahmati, F Tramer, A Prakash, T Kohno, 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18). D. Song, K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rah- mati, F. Tramer, A. Prakash, and T. Kohno, \"Physical adversarial examples for object detectors,\" in 12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18), 2018.\n\nFooling a real car with adversarial traffic signs. N Morgulis, A Kreines, S Mendelowitz, Y Weisglass, arXiv:1907.00374arXiv preprintN. Morgulis, A. Kreines, S. Mendelowitz, and Y. Weisglass, \"Fooling a real car with adversarial traffic signs,\" arXiv preprint arXiv:1907.00374, 2019.\n\n{SLAP}: Improving physical adversarial examples with short-lived adversarial perturbations. G Lovisotto, H Turner, I Sluganovic, M Strohmeier, I Martinovic, 30th {USENIX} Security Symposium. 2021{USENIX} Security 21G. Lovisotto, H. Turner, I. Sluganovic, M. Strohmeier, and I. Martinovic, \"{SLAP}: Improving physical adversarial ex- amples with short-lived adversarial perturbations,\" in 30th {USENIX} Security Symposium ({USENIX} Security 21), 2021.\n\nAdversarial attacks and defenses on cyber-physical systems: A survey. J Li, Y Liu, T Chen, Z Xiao, Z Li, J Wang, IEEE Internet of Things Journal. 7J. Li, Y. Liu, T. Chen, Z. Xiao, Z. Li, and J. Wang, \"Adversarial attacks and defenses on cyber-physical systems: A survey,\" IEEE Internet of Things Journal, vol. 7, pp. 5103-5115, 2020.\n\nKeeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. N Das, M Shanbhogue, S.-T Chen, F Hohman, L Chen, M E Kounavis, D H Chau, arXiv:1705.02900arXiv preprintN. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, L. Chen, M. E. Kounavis, and D. H. Chau, \"Keeping the bad guys out: Protect- ing and vaccinating deep learning with jpeg compression,\" arXiv preprint arXiv:1705.02900, 2017.\n\nC Guo, M Rana, M Cisse, L Van Der Maaten, arXiv:1711.00117Countering adversarial images using input transformations. arXiv preprintC. Guo, M. Rana, M. Cisse, and L. Van Der Maaten, \"Coun- tering adversarial images using input transformations,\" arXiv preprint arXiv:1711.00117, 2017.\n\nDeflecting adversarial attacks with pixel deflection. A Prakash, N Moran, S Garber, A Dilillo, J Storer, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionA. Prakash, N. Moran, S. Garber, A. DiLillo, and J. Storer, \"Deflecting adversarial attacks with pixel deflection,\" in Pro- ceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8571-8580.\n\nDefense against adversarial attacks using high-level representation guided denoiser. F Liao, M Liang, Y Dong, T Pang, X Hu, J Zhu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionF. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu, \"De- fense against adversarial attacks using high-level representation guided denoiser,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1778-1787.\n\nFeature denoising for improving adversarial robustness. C Xie, Y Wu, L Maaten, A L Yuille, K He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionC. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He, \"Feature denoising for improving adversarial robustness,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 501-509.\n\nAdversarial examples detection in deep networks with convolutional filter statistics. X Li, F Li, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionX. Li and F. Li, \"Adversarial examples detection in deep networks with convolutional filter statistics,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 5764-5772.\n\nSafetynet: Detecting and rejecting adversarial examples robustly. J Lu, T Issaranon, D Forsyth, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJ. Lu, T. Issaranon, and D. Forsyth, \"Safetynet: Detecting and rejecting adversarial examples robustly,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 446-454.\n\nOn detecting adversarial perturbations. J H Metzen, T Genewein, V Fischer, B Bischoff, arXiv:1702.04267arXiv preprintJ. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, \"On detecting adversarial perturbations,\" arXiv preprint arXiv:1702.04267, 2017.\n\nFeature squeezing: Detecting adversarial examples in deep neural networks. W Xu, D Evans, Y Qi, arXiv:1704.01155arXiv preprintW. Xu, D. Evans, and Y. Qi, \"Feature squeezing: Detecting adversarial examples in deep neural networks,\" arXiv preprint arXiv:1704.01155, 2017.\n\nStateful detection of black-box adversarial attacks. S Chen, N Carlini, D Wagner, Proceedings of the 1st ACM Workshop on Security and Privacy on Artificial Intelligence. the 1st ACM Workshop on Security and Privacy on Artificial Intelligence2020S. Chen, N. Carlini, and D. Wagner, \"Stateful detection of black-box adversarial attacks,\" in Proceedings of the 1st ACM Workshop on Security and Privacy on Artificial Intelligence, 2020, pp. 30-39.\n\nEnsemble adversarial training: Attacks and defenses. F Tram\u00e8r, A Kurakin, N Papernot, I Goodfellow, D Boneh, P Mcdaniel, arXiv:1705.07204arXiv preprintF. Tram\u00e8r, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, \"Ensemble adversarial training: Attacks and defenses,\" arXiv preprint arXiv:1705.07204, 2017.\n\nFast is better than free: Revisiting adversarial training. E Wong, L Rice, J Z Kolter, arXiv:2001.03994arXiv preprintE. Wong, L. Rice, and J. Z. Kolter, \"Fast is better than free: Re- visiting adversarial training,\" arXiv preprint arXiv:2001.03994, 2020.\n\nDistillation as a defense to adversarial perturbations against deep neural networks. N Papernot, P Mcdaniel, X Wu, S Jha, A Swami, 2016 IEEE symposium on security and privacy (SP). IEEEN. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, \"Distillation as a defense to adversarial perturbations against deep neural networks,\" in 2016 IEEE symposium on security and privacy (SP). IEEE, 2016, pp. 582-597.\n\nAdversarially robust distillation. M Goldblum, L Fowl, S Feizi, T Goldstein, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34M. Goldblum, L. Fowl, S. Feizi, and T. Goldstein, \"Adversarially robust distillation,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, 2020, pp. 3996-4003.\n\nPixeldefend: Leveraging generative models to understand and defend against adversarial examples. Y Song, T Kim, S Nowozin, S Ermon, N Kushman, arXiv:1710.10766arXiv preprintY. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kush- man, \"Pixeldefend: Leveraging generative models to under- stand and defend against adversarial examples,\" arXiv preprint arXiv:1710.10766, 2017.\n\nDefense-gan: Protecting classifiers against adversarial attacks using generative models. P Samangouei, M Kabkab, R Chellappa, arXiv:1805.06605arXiv preprintP. Samangouei, M. Kabkab, and R. Chellappa, \"Defense-gan: Protecting classifiers against adversarial attacks using generative models,\" arXiv preprint arXiv:1805.06605, 2018.\n", "annotations": {"author": "[{\"end\":147,\"start\":122},{\"end\":159,\"start\":148},{\"end\":174,\"start\":160},{\"end\":212,\"start\":175},{\"end\":241,\"start\":213},{\"end\":265,\"start\":242},{\"end\":505,\"start\":266},{\"end\":531,\"start\":506}]", "publisher": null, "author_last_name": "[{\"end\":129,\"start\":126},{\"end\":158,\"start\":156},{\"end\":173,\"start\":168},{\"end\":187,\"start\":184},{\"end\":221,\"start\":217},{\"end\":249,\"start\":247}]", "author_first_name": "[{\"end\":125,\"start\":122},{\"end\":155,\"start\":148},{\"end\":167,\"start\":160},{\"end\":183,\"start\":175},{\"end\":216,\"start\":213},{\"end\":246,\"start\":242}]", "author_affiliation": "[{\"end\":504,\"start\":267},{\"end\":530,\"start\":507}]", "title": "[{\"end\":119,\"start\":1},{\"end\":650,\"start\":532}]", "venue": null, "abstract": "[{\"end\":2757,\"start\":652}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2966,\"start\":2963},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2992,\"start\":2989},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3018,\"start\":3015},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3183,\"start\":3180},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3212,\"start\":3209},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3426,\"start\":3423},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3429,\"start\":3426},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3432,\"start\":3429},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3435,\"start\":3432},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3439,\"start\":3435},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4102,\"start\":4098},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4106,\"start\":4102},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4110,\"start\":4106},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4114,\"start\":4110},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4118,\"start\":4114},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4525,\"start\":4521},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4675,\"start\":4671},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4701,\"start\":4697},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4948,\"start\":4944},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9809,\"start\":9805},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9962,\"start\":9958},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10202,\"start\":10198},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10406,\"start\":10402},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10564,\"start\":10560},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10656,\"start\":10652},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10875,\"start\":10871},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11016,\"start\":11012},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11087,\"start\":11083},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11257,\"start\":11253},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11462,\"start\":11458},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11728,\"start\":11724},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11790,\"start\":11786},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11844,\"start\":11840},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12291,\"start\":12287},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12884,\"start\":12880},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13273,\"start\":13269},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13922,\"start\":13918},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13966,\"start\":13962},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14118,\"start\":14114},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14646,\"start\":14643},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14677,\"start\":14674},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14701,\"start\":14698},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14782,\"start\":14778},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14785,\"start\":14782},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14833,\"start\":14829},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15675,\"start\":15671},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20230,\"start\":20226},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25809,\"start\":25806},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28760,\"start\":28757},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30120,\"start\":30116},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30143,\"start\":30139},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30175,\"start\":30171},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30207,\"start\":30203},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30236,\"start\":30232},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30270,\"start\":30266},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31258,\"start\":31254},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32133,\"start\":32129},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32566,\"start\":32562},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32650,\"start\":32646},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32754,\"start\":32750},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32768,\"start\":32764},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32782,\"start\":32778},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32873,\"start\":32869},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32889,\"start\":32885},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32995,\"start\":32991},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33110,\"start\":33106},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34149,\"start\":34145},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39304,\"start\":39300},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":39314,\"start\":39310},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39333,\"start\":39329},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":39409,\"start\":39405},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":39469,\"start\":39465},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40837,\"start\":40833},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40856,\"start\":40852},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40876,\"start\":40872},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":43119,\"start\":43115},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":43379,\"start\":43376},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":43540,\"start\":43537},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43543,\"start\":43540},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43546,\"start\":43543},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43549,\"start\":43546},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43553,\"start\":43549},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43611,\"start\":43608},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":43711,\"start\":43708},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":43996,\"start\":43992},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":43999,\"start\":43996},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44110,\"start\":44106},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":44259,\"start\":44255},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44262,\"start\":44259},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44369,\"start\":44365},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44455,\"start\":44451},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":44477,\"start\":44473},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":44576,\"start\":44572},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":44678,\"start\":44674},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45016,\"start\":45012},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":45308,\"start\":45304},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":45503,\"start\":45499},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":46190,\"start\":46186},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":46362,\"start\":46358},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":46701,\"start\":46697},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":46878,\"start\":46874},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":46899,\"start\":46895},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":47206,\"start\":47202},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":47417,\"start\":47413},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":47703,\"start\":47699},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":47771,\"start\":47767},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47913,\"start\":47909},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":48127,\"start\":48123},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":48423,\"start\":48419},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48823,\"start\":48820},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":48951,\"start\":48947},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48969,\"start\":48966},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":49340,\"start\":49336},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":49506,\"start\":49502},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":49871,\"start\":49867},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":50315,\"start\":50311},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":50413,\"start\":50409}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51501,\"start\":51466},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51573,\"start\":51502},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51704,\"start\":51574},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52045,\"start\":51705},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52190,\"start\":52046},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52777,\"start\":52191},{\"attributes\":{\"id\":\"fig_6\"},\"end\":52830,\"start\":52778},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53468,\"start\":52831},{\"attributes\":{\"id\":\"fig_8\"},\"end\":53911,\"start\":53469},{\"attributes\":{\"id\":\"fig_9\"},\"end\":54012,\"start\":53912},{\"attributes\":{\"id\":\"fig_10\"},\"end\":54130,\"start\":54013},{\"attributes\":{\"id\":\"fig_11\"},\"end\":54228,\"start\":54131},{\"attributes\":{\"id\":\"fig_12\"},\"end\":54333,\"start\":54229},{\"attributes\":{\"id\":\"fig_13\"},\"end\":54475,\"start\":54334},{\"attributes\":{\"id\":\"fig_14\"},\"end\":54574,\"start\":54476},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54787,\"start\":54575},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54845,\"start\":54788},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":55461,\"start\":54846},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55854,\"start\":55462},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56657,\"start\":55855},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57330,\"start\":56658},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57535,\"start\":57331}]", "paragraph": "[{\"end\":3856,\"start\":2776},{\"end\":4949,\"start\":3858},{\"end\":6084,\"start\":4951},{\"end\":7449,\"start\":6086},{\"end\":7681,\"start\":7451},{\"end\":8168,\"start\":7683},{\"end\":8602,\"start\":8170},{\"end\":9092,\"start\":8604},{\"end\":9421,\"start\":9111},{\"end\":11585,\"start\":9445},{\"end\":13083,\"start\":11587},{\"end\":13590,\"start\":13120},{\"end\":14444,\"start\":13592},{\"end\":15216,\"start\":14446},{\"end\":15937,\"start\":15218},{\"end\":16440,\"start\":15980},{\"end\":17251,\"start\":16460},{\"end\":17803,\"start\":17253},{\"end\":18259,\"start\":17818},{\"end\":19048,\"start\":18261},{\"end\":19168,\"start\":19071},{\"end\":19518,\"start\":19170},{\"end\":19773,\"start\":19520},{\"end\":20115,\"start\":19775},{\"end\":20598,\"start\":20142},{\"end\":20802,\"start\":20618},{\"end\":21534,\"start\":20804},{\"end\":22828,\"start\":21831},{\"end\":23903,\"start\":22855},{\"end\":23968,\"start\":23905},{\"end\":24253,\"start\":24010},{\"end\":24360,\"start\":24255},{\"end\":24919,\"start\":24772},{\"end\":25020,\"start\":24921},{\"end\":25379,\"start\":25051},{\"end\":25478,\"start\":25394},{\"end\":25909,\"start\":25566},{\"end\":26314,\"start\":25911},{\"end\":26728,\"start\":26316},{\"end\":26908,\"start\":26730},{\"end\":27096,\"start\":26935},{\"end\":27105,\"start\":27098},{\"end\":27149,\"start\":27107},{\"end\":27321,\"start\":27151},{\"end\":27534,\"start\":27343},{\"end\":27992,\"start\":27588},{\"end\":28351,\"start\":28044},{\"end\":28384,\"start\":28380},{\"end\":29229,\"start\":28441},{\"end\":29940,\"start\":29333},{\"end\":30074,\"start\":29966},{\"end\":31174,\"start\":30091},{\"end\":33033,\"start\":31190},{\"end\":33293,\"start\":33035},{\"end\":33538,\"start\":33295},{\"end\":33705,\"start\":33540},{\"end\":33868,\"start\":33707},{\"end\":35242,\"start\":34066},{\"end\":36225,\"start\":35244},{\"end\":36381,\"start\":36227},{\"end\":37362,\"start\":36383},{\"end\":38318,\"start\":37364},{\"end\":39118,\"start\":38320},{\"end\":40156,\"start\":39120},{\"end\":40648,\"start\":40158},{\"end\":41711,\"start\":40681},{\"end\":43121,\"start\":41713},{\"end\":43311,\"start\":43141},{\"end\":44072,\"start\":43313},{\"end\":45829,\"start\":44074},{\"end\":46057,\"start\":45855},{\"end\":47562,\"start\":46059},{\"end\":48614,\"start\":47564},{\"end\":50462,\"start\":48616},{\"end\":51454,\"start\":50481}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17817,\"start\":17804},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20617,\"start\":20599},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21830,\"start\":21535},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24009,\"start\":23969},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24771,\"start\":24361},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25050,\"start\":25021},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25565,\"start\":25479},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27342,\"start\":27322},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27587,\"start\":27535},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28043,\"start\":27993},{\"attributes\":{\"id\":\"formula_11\"},\"end\":28379,\"start\":28352},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28420,\"start\":28385},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29315,\"start\":29230},{\"attributes\":{\"id\":\"formula_14\"},\"end\":34065,\"start\":33894}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32457,\"start\":32450},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37108,\"start\":37100},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37292,\"start\":37284},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39614,\"start\":39605},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40781,\"start\":40773}]", "section_header": "[{\"end\":2774,\"start\":2759},{\"end\":9109,\"start\":9095},{\"end\":9443,\"start\":9424},{\"end\":13118,\"start\":13086},{\"end\":15978,\"start\":15940},{\"end\":16458,\"start\":16443},{\"end\":19069,\"start\":19051},{\"end\":20140,\"start\":20118},{\"end\":22853,\"start\":22831},{\"end\":25392,\"start\":25382},{\"end\":26933,\"start\":26911},{\"end\":28439,\"start\":28422},{\"end\":29331,\"start\":29317},{\"end\":29964,\"start\":29943},{\"end\":30089,\"start\":30077},{\"end\":31188,\"start\":31177},{\"end\":33893,\"start\":33871},{\"end\":40679,\"start\":40651},{\"end\":43139,\"start\":43124},{\"end\":45853,\"start\":45832},{\"end\":50479,\"start\":50465},{\"end\":51465,\"start\":51457},{\"end\":51475,\"start\":51467},{\"end\":51511,\"start\":51503},{\"end\":51583,\"start\":51575},{\"end\":51714,\"start\":51706},{\"end\":52055,\"start\":52047},{\"end\":52200,\"start\":52192},{\"end\":52787,\"start\":52779},{\"end\":52840,\"start\":52832},{\"end\":53473,\"start\":53470},{\"end\":53921,\"start\":53913},{\"end\":54023,\"start\":54014},{\"end\":54141,\"start\":54132},{\"end\":54239,\"start\":54230},{\"end\":54344,\"start\":54335},{\"end\":54486,\"start\":54477},{\"end\":54798,\"start\":54789},{\"end\":55864,\"start\":55856},{\"end\":56669,\"start\":56659},{\"end\":57341,\"start\":57332}]", "table": "[{\"end\":56657,\"start\":55867},{\"end\":57330,\"start\":56771},{\"end\":57535,\"start\":57480}]", "figure_caption": "[{\"end\":51501,\"start\":51477},{\"end\":51573,\"start\":51513},{\"end\":51704,\"start\":51585},{\"end\":52045,\"start\":51716},{\"end\":52190,\"start\":52057},{\"end\":52777,\"start\":52202},{\"end\":52830,\"start\":52789},{\"end\":53468,\"start\":52842},{\"end\":53911,\"start\":53474},{\"end\":54012,\"start\":53923},{\"end\":54130,\"start\":54026},{\"end\":54228,\"start\":54144},{\"end\":54333,\"start\":54242},{\"end\":54475,\"start\":54347},{\"end\":54574,\"start\":54489},{\"end\":54787,\"start\":54577},{\"end\":54845,\"start\":54800},{\"end\":55461,\"start\":54848},{\"end\":55854,\"start\":55464},{\"end\":56771,\"start\":56672},{\"end\":57480,\"start\":57343}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11617,\"start\":11612},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13643,\"start\":13638},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18684,\"start\":18679},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18934,\"start\":18929},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19094,\"start\":19089},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19195,\"start\":19190},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19545,\"start\":19540},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19800,\"start\":19795},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29101,\"start\":29096},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34961,\"start\":34956},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35049,\"start\":35019},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36368,\"start\":36362},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36416,\"start\":36411},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36964,\"start\":36958},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37254,\"start\":37249},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37716,\"start\":37711},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37984,\"start\":37970},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38382,\"start\":38370},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38437,\"start\":38425},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38584,\"start\":38572},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38638,\"start\":38626},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38779,\"start\":38774},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38812,\"start\":38798},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38840,\"start\":38826},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":40345,\"start\":40337},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":40479,\"start\":40469},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43214,\"start\":43208}]", "bib_author_first_name": "[{\"end\":57613,\"start\":57612},{\"end\":57622,\"start\":57621},{\"end\":57633,\"start\":57632},{\"end\":57642,\"start\":57641},{\"end\":57652,\"start\":57651},{\"end\":58133,\"start\":58132},{\"end\":58137,\"start\":58134},{\"end\":58144,\"start\":58143},{\"end\":58146,\"start\":58145},{\"end\":58157,\"start\":58156},{\"end\":58159,\"start\":58158},{\"end\":58173,\"start\":58172},{\"end\":58175,\"start\":58174},{\"end\":58188,\"start\":58187},{\"end\":58197,\"start\":58196},{\"end\":58529,\"start\":58528},{\"end\":58536,\"start\":58535},{\"end\":58543,\"start\":58542},{\"end\":58549,\"start\":58548},{\"end\":58551,\"start\":58550},{\"end\":58560,\"start\":58559},{\"end\":58981,\"start\":58980},{\"end\":58990,\"start\":58989},{\"end\":59270,\"start\":59269},{\"end\":59281,\"start\":59280},{\"end\":59292,\"start\":59291},{\"end\":59305,\"start\":59304},{\"end\":59314,\"start\":59313},{\"end\":59323,\"start\":59322},{\"end\":59337,\"start\":59336},{\"end\":59601,\"start\":59600},{\"end\":59612,\"start\":59611},{\"end\":59883,\"start\":59882},{\"end\":59885,\"start\":59884},{\"end\":59899,\"start\":59898},{\"end\":59909,\"start\":59908},{\"end\":60143,\"start\":60142},{\"end\":60155,\"start\":60154},{\"end\":60167,\"start\":60166},{\"end\":60174,\"start\":60173},{\"end\":60188,\"start\":60187},{\"end\":60190,\"start\":60189},{\"end\":60199,\"start\":60198},{\"end\":60553,\"start\":60552},{\"end\":60560,\"start\":60559},{\"end\":60571,\"start\":60570},{\"end\":60861,\"start\":60860},{\"end\":60869,\"start\":60868},{\"end\":60877,\"start\":60876},{\"end\":60885,\"start\":60884},{\"end\":60893,\"start\":60892},{\"end\":60900,\"start\":60899},{\"end\":60908,\"start\":60907},{\"end\":60917,\"start\":60916},{\"end\":60926,\"start\":60925},{\"end\":60934,\"start\":60933},{\"end\":60936,\"start\":60935},{\"end\":61309,\"start\":61308},{\"end\":61320,\"start\":61319},{\"end\":61334,\"start\":61333},{\"end\":61486,\"start\":61485},{\"end\":61497,\"start\":61496},{\"end\":61509,\"start\":61508},{\"end\":61518,\"start\":61517},{\"end\":61800,\"start\":61799},{\"end\":61811,\"start\":61810},{\"end\":61822,\"start\":61821},{\"end\":61835,\"start\":61834},{\"end\":61844,\"start\":61843},{\"end\":61850,\"start\":61849},{\"end\":61861,\"start\":61860},{\"end\":61872,\"start\":61871},{\"end\":62177,\"start\":62176},{\"end\":62190,\"start\":62189},{\"end\":62192,\"start\":62191},{\"end\":62203,\"start\":62202},{\"end\":62214,\"start\":62213},{\"end\":62224,\"start\":62223},{\"end\":62511,\"start\":62510},{\"end\":62518,\"start\":62517},{\"end\":62524,\"start\":62523},{\"end\":62532,\"start\":62531},{\"end\":62539,\"start\":62538},{\"end\":62546,\"start\":62545},{\"end\":62866,\"start\":62865},{\"end\":62874,\"start\":62873},{\"end\":62880,\"start\":62879},{\"end\":62888,\"start\":62887},{\"end\":62907,\"start\":62906},{\"end\":62917,\"start\":62916},{\"end\":63236,\"start\":63235},{\"end\":63248,\"start\":63247},{\"end\":63265,\"start\":63261},{\"end\":63277,\"start\":63276},{\"end\":63553,\"start\":63552},{\"end\":63561,\"start\":63560},{\"end\":63568,\"start\":63567},{\"end\":63577,\"start\":63576},{\"end\":63585,\"start\":63584},{\"end\":63594,\"start\":63593},{\"end\":64120,\"start\":64119},{\"end\":64128,\"start\":64127},{\"end\":64134,\"start\":64133},{\"end\":64141,\"start\":64140},{\"end\":64149,\"start\":64148},{\"end\":64157,\"start\":64156},{\"end\":64549,\"start\":64548},{\"end\":64568,\"start\":64567},{\"end\":64584,\"start\":64583},{\"end\":64600,\"start\":64599},{\"end\":64931,\"start\":64930},{\"end\":64941,\"start\":64940},{\"end\":64952,\"start\":64951},{\"end\":64964,\"start\":64963},{\"end\":65411,\"start\":65410},{\"end\":65418,\"start\":65417},{\"end\":65424,\"start\":65423},{\"end\":65436,\"start\":65435},{\"end\":65776,\"start\":65775},{\"end\":65783,\"start\":65782},{\"end\":65791,\"start\":65790},{\"end\":65798,\"start\":65797},{\"end\":65806,\"start\":65805},{\"end\":65814,\"start\":65813},{\"end\":65820,\"start\":65819},{\"end\":65822,\"start\":65821},{\"end\":65832,\"start\":65831},{\"end\":66185,\"start\":66184},{\"end\":66204,\"start\":66199},{\"end\":66215,\"start\":66211},{\"end\":66217,\"start\":66216},{\"end\":66436,\"start\":66435},{\"end\":66446,\"start\":66445},{\"end\":66782,\"start\":66778},{\"end\":66793,\"start\":66789},{\"end\":66795,\"start\":66794},{\"end\":66806,\"start\":66802},{\"end\":66815,\"start\":66811},{\"end\":66826,\"start\":66822},{\"end\":66838,\"start\":66834},{\"end\":67331,\"start\":67330},{\"end\":67338,\"start\":67337},{\"end\":67344,\"start\":67343},{\"end\":67351,\"start\":67350},{\"end\":67358,\"start\":67357},{\"end\":67786,\"start\":67785},{\"end\":67792,\"start\":67791},{\"end\":67801,\"start\":67800},{\"end\":67808,\"start\":67807},{\"end\":68158,\"start\":68157},{\"end\":68165,\"start\":68164},{\"end\":68174,\"start\":68173},{\"end\":68183,\"start\":68182},{\"end\":68192,\"start\":68191},{\"end\":68198,\"start\":68197},{\"end\":68648,\"start\":68644},{\"end\":68656,\"start\":68655},{\"end\":68669,\"start\":68668},{\"end\":68679,\"start\":68678},{\"end\":68683,\"start\":68680},{\"end\":69039,\"start\":69038},{\"end\":69048,\"start\":69047},{\"end\":69060,\"start\":69059},{\"end\":69074,\"start\":69073},{\"end\":69082,\"start\":69081},{\"end\":69414,\"start\":69413},{\"end\":69416,\"start\":69415},{\"end\":69423,\"start\":69422},{\"end\":69436,\"start\":69432},{\"end\":69446,\"start\":69442},{\"end\":69455,\"start\":69454},{\"end\":69895,\"start\":69894},{\"end\":69907,\"start\":69903},{\"end\":69914,\"start\":69913},{\"end\":69922,\"start\":69921},{\"end\":69924,\"start\":69923},{\"end\":70353,\"start\":70352},{\"end\":70360,\"start\":70359},{\"end\":70368,\"start\":70367},{\"end\":70376,\"start\":70375},{\"end\":70386,\"start\":70385},{\"end\":70663,\"start\":70662},{\"end\":70669,\"start\":70668},{\"end\":70678,\"start\":70677},{\"end\":70687,\"start\":70686},{\"end\":70983,\"start\":70982},{\"end\":70990,\"start\":70989},{\"end\":70998,\"start\":70997},{\"end\":71004,\"start\":71003},{\"end\":71012,\"start\":71011},{\"end\":71409,\"start\":71408},{\"end\":71417,\"start\":71416},{\"end\":71424,\"start\":71423},{\"end\":71749,\"start\":71748},{\"end\":71756,\"start\":71755},{\"end\":71768,\"start\":71767},{\"end\":71777,\"start\":71776},{\"end\":71788,\"start\":71787},{\"end\":71799,\"start\":71795},{\"end\":71898,\"start\":71897},{\"end\":71900,\"start\":71899},{\"end\":72091,\"start\":72087},{\"end\":72098,\"start\":72097},{\"end\":72107,\"start\":72106},{\"end\":72119,\"start\":72118},{\"end\":72125,\"start\":72124},{\"end\":72467,\"start\":72466},{\"end\":72475,\"start\":72474},{\"end\":72483,\"start\":72482},{\"end\":72688,\"start\":72687},{\"end\":72943,\"start\":72942},{\"end\":72951,\"start\":72950},{\"end\":73225,\"start\":73224},{\"end\":73236,\"start\":73235},{\"end\":73247,\"start\":73246},{\"end\":73260,\"start\":73259},{\"end\":73266,\"start\":73265},{\"end\":73277,\"start\":73276},{\"end\":73285,\"start\":73284},{\"end\":73296,\"start\":73295},{\"end\":73305,\"start\":73304},{\"end\":73786,\"start\":73785},{\"end\":73794,\"start\":73793},{\"end\":73805,\"start\":73804},{\"end\":73816,\"start\":73815},{\"end\":73829,\"start\":73828},{\"end\":73835,\"start\":73834},{\"end\":73846,\"start\":73845},{\"end\":73856,\"start\":73855},{\"end\":73867,\"start\":73866},{\"end\":74220,\"start\":74219},{\"end\":74232,\"start\":74231},{\"end\":74243,\"start\":74242},{\"end\":74258,\"start\":74257},{\"end\":74545,\"start\":74544},{\"end\":74558,\"start\":74557},{\"end\":74568,\"start\":74567},{\"end\":74582,\"start\":74581},{\"end\":74596,\"start\":74595},{\"end\":74975,\"start\":74974},{\"end\":74981,\"start\":74980},{\"end\":74988,\"start\":74987},{\"end\":74996,\"start\":74995},{\"end\":75004,\"start\":75003},{\"end\":75010,\"start\":75009},{\"end\":75330,\"start\":75329},{\"end\":75337,\"start\":75336},{\"end\":75354,\"start\":75350},{\"end\":75362,\"start\":75361},{\"end\":75372,\"start\":75371},{\"end\":75380,\"start\":75379},{\"end\":75382,\"start\":75381},{\"end\":75394,\"start\":75393},{\"end\":75396,\"start\":75395},{\"end\":75655,\"start\":75654},{\"end\":75662,\"start\":75661},{\"end\":75670,\"start\":75669},{\"end\":75679,\"start\":75678},{\"end\":75993,\"start\":75992},{\"end\":76004,\"start\":76003},{\"end\":76013,\"start\":76012},{\"end\":76023,\"start\":76022},{\"end\":76034,\"start\":76033},{\"end\":76492,\"start\":76491},{\"end\":76500,\"start\":76499},{\"end\":76509,\"start\":76508},{\"end\":76517,\"start\":76516},{\"end\":76525,\"start\":76524},{\"end\":76531,\"start\":76530},{\"end\":76984,\"start\":76983},{\"end\":76991,\"start\":76990},{\"end\":76997,\"start\":76996},{\"end\":77007,\"start\":77006},{\"end\":77009,\"start\":77008},{\"end\":77019,\"start\":77018},{\"end\":77481,\"start\":77480},{\"end\":77487,\"start\":77486},{\"end\":77879,\"start\":77878},{\"end\":77885,\"start\":77884},{\"end\":77898,\"start\":77897},{\"end\":78267,\"start\":78266},{\"end\":78269,\"start\":78268},{\"end\":78279,\"start\":78278},{\"end\":78291,\"start\":78290},{\"end\":78302,\"start\":78301},{\"end\":78557,\"start\":78556},{\"end\":78563,\"start\":78562},{\"end\":78572,\"start\":78571},{\"end\":78806,\"start\":78805},{\"end\":78814,\"start\":78813},{\"end\":78825,\"start\":78824},{\"end\":79251,\"start\":79250},{\"end\":79261,\"start\":79260},{\"end\":79272,\"start\":79271},{\"end\":79284,\"start\":79283},{\"end\":79298,\"start\":79297},{\"end\":79307,\"start\":79306},{\"end\":79581,\"start\":79580},{\"end\":79589,\"start\":79588},{\"end\":79597,\"start\":79596},{\"end\":79599,\"start\":79598},{\"end\":79863,\"start\":79862},{\"end\":79875,\"start\":79874},{\"end\":79887,\"start\":79886},{\"end\":79893,\"start\":79892},{\"end\":79900,\"start\":79899},{\"end\":80219,\"start\":80218},{\"end\":80231,\"start\":80230},{\"end\":80239,\"start\":80238},{\"end\":80248,\"start\":80247},{\"end\":80661,\"start\":80660},{\"end\":80669,\"start\":80668},{\"end\":80676,\"start\":80675},{\"end\":80687,\"start\":80686},{\"end\":80696,\"start\":80695},{\"end\":81024,\"start\":81023},{\"end\":81038,\"start\":81037},{\"end\":81048,\"start\":81047}]", "bib_author_last_name": "[{\"end\":57619,\"start\":57614},{\"end\":57630,\"start\":57623},{\"end\":57639,\"start\":57634},{\"end\":57649,\"start\":57643},{\"end\":57658,\"start\":57653},{\"end\":58141,\"start\":58138},{\"end\":58154,\"start\":58147},{\"end\":58170,\"start\":58160},{\"end\":58185,\"start\":58176},{\"end\":58194,\"start\":58189},{\"end\":58205,\"start\":58198},{\"end\":58533,\"start\":58530},{\"end\":58540,\"start\":58537},{\"end\":58546,\"start\":58544},{\"end\":58557,\"start\":58552},{\"end\":58564,\"start\":58561},{\"end\":58987,\"start\":58982},{\"end\":58993,\"start\":58991},{\"end\":59278,\"start\":59271},{\"end\":59289,\"start\":59282},{\"end\":59302,\"start\":59293},{\"end\":59311,\"start\":59306},{\"end\":59320,\"start\":59315},{\"end\":59334,\"start\":59324},{\"end\":59344,\"start\":59338},{\"end\":59609,\"start\":59602},{\"end\":59619,\"start\":59613},{\"end\":59896,\"start\":59886},{\"end\":59906,\"start\":59900},{\"end\":59917,\"start\":59910},{\"end\":60152,\"start\":60144},{\"end\":60164,\"start\":60156},{\"end\":60171,\"start\":60168},{\"end\":60185,\"start\":60175},{\"end\":60196,\"start\":60191},{\"end\":60205,\"start\":60200},{\"end\":60557,\"start\":60554},{\"end\":60568,\"start\":60561},{\"end\":60576,\"start\":60572},{\"end\":60866,\"start\":60862},{\"end\":60874,\"start\":60870},{\"end\":60882,\"start\":60878},{\"end\":60890,\"start\":60886},{\"end\":60897,\"start\":60894},{\"end\":60905,\"start\":60901},{\"end\":60914,\"start\":60909},{\"end\":60923,\"start\":60918},{\"end\":60931,\"start\":60927},{\"end\":60943,\"start\":60937},{\"end\":61317,\"start\":61310},{\"end\":61331,\"start\":61321},{\"end\":61341,\"start\":61335},{\"end\":61494,\"start\":61487},{\"end\":61506,\"start\":61498},{\"end\":61515,\"start\":61510},{\"end\":61523,\"start\":61519},{\"end\":61808,\"start\":61801},{\"end\":61819,\"start\":61812},{\"end\":61832,\"start\":61823},{\"end\":61841,\"start\":61836},{\"end\":61847,\"start\":61845},{\"end\":61858,\"start\":61851},{\"end\":61869,\"start\":61862},{\"end\":61877,\"start\":61873},{\"end\":62187,\"start\":62178},{\"end\":62200,\"start\":62193},{\"end\":62211,\"start\":62204},{\"end\":62221,\"start\":62215},{\"end\":62231,\"start\":62225},{\"end\":62515,\"start\":62512},{\"end\":62521,\"start\":62519},{\"end\":62529,\"start\":62525},{\"end\":62536,\"start\":62533},{\"end\":62543,\"start\":62540},{\"end\":62552,\"start\":62547},{\"end\":62871,\"start\":62867},{\"end\":62877,\"start\":62875},{\"end\":62885,\"start\":62881},{\"end\":62904,\"start\":62889},{\"end\":62914,\"start\":62908},{\"end\":62922,\"start\":62918},{\"end\":63245,\"start\":63237},{\"end\":63259,\"start\":63249},{\"end\":63274,\"start\":63266},{\"end\":63282,\"start\":63278},{\"end\":63558,\"start\":63554},{\"end\":63565,\"start\":63562},{\"end\":63574,\"start\":63569},{\"end\":63582,\"start\":63578},{\"end\":63591,\"start\":63586},{\"end\":63599,\"start\":63595},{\"end\":64125,\"start\":64121},{\"end\":64131,\"start\":64129},{\"end\":64138,\"start\":64135},{\"end\":64146,\"start\":64142},{\"end\":64154,\"start\":64150},{\"end\":64161,\"start\":64158},{\"end\":64565,\"start\":64550},{\"end\":64581,\"start\":64569},{\"end\":64597,\"start\":64585},{\"end\":64617,\"start\":64601},{\"end\":64938,\"start\":64932},{\"end\":64949,\"start\":64942},{\"end\":64961,\"start\":64953},{\"end\":64972,\"start\":64965},{\"end\":65415,\"start\":65412},{\"end\":65421,\"start\":65419},{\"end\":65433,\"start\":65425},{\"end\":65440,\"start\":65437},{\"end\":65780,\"start\":65777},{\"end\":65788,\"start\":65784},{\"end\":65795,\"start\":65792},{\"end\":65803,\"start\":65799},{\"end\":65811,\"start\":65807},{\"end\":65817,\"start\":65815},{\"end\":65829,\"start\":65823},{\"end\":65835,\"start\":65833},{\"end\":66197,\"start\":66186},{\"end\":66209,\"start\":66205},{\"end\":66222,\"start\":66218},{\"end\":66443,\"start\":66437},{\"end\":66454,\"start\":66447},{\"end\":66605,\"start\":66594},{\"end\":66787,\"start\":66783},{\"end\":66800,\"start\":66796},{\"end\":66809,\"start\":66807},{\"end\":66820,\"start\":66816},{\"end\":66832,\"start\":66827},{\"end\":66842,\"start\":66839},{\"end\":67335,\"start\":67332},{\"end\":67341,\"start\":67339},{\"end\":67348,\"start\":67345},{\"end\":67355,\"start\":67352},{\"end\":67362,\"start\":67359},{\"end\":67789,\"start\":67787},{\"end\":67798,\"start\":67793},{\"end\":67805,\"start\":67802},{\"end\":67812,\"start\":67809},{\"end\":68162,\"start\":68159},{\"end\":68171,\"start\":68166},{\"end\":68180,\"start\":68175},{\"end\":68189,\"start\":68184},{\"end\":68195,\"start\":68193},{\"end\":68201,\"start\":68199},{\"end\":68653,\"start\":68649},{\"end\":68666,\"start\":68657},{\"end\":68676,\"start\":68670},{\"end\":68688,\"start\":68684},{\"end\":69045,\"start\":69040},{\"end\":69057,\"start\":69049},{\"end\":69071,\"start\":69061},{\"end\":69079,\"start\":69075},{\"end\":69089,\"start\":69083},{\"end\":69420,\"start\":69417},{\"end\":69430,\"start\":69424},{\"end\":69440,\"start\":69437},{\"end\":69452,\"start\":69447},{\"end\":69460,\"start\":69456},{\"end\":69901,\"start\":69896},{\"end\":69911,\"start\":69908},{\"end\":69919,\"start\":69915},{\"end\":69930,\"start\":69925},{\"end\":70357,\"start\":70354},{\"end\":70365,\"start\":70361},{\"end\":70373,\"start\":70369},{\"end\":70383,\"start\":70377},{\"end\":70391,\"start\":70387},{\"end\":70666,\"start\":70664},{\"end\":70675,\"start\":70670},{\"end\":70684,\"start\":70679},{\"end\":70695,\"start\":70688},{\"end\":70987,\"start\":70984},{\"end\":70995,\"start\":70991},{\"end\":71001,\"start\":70999},{\"end\":71009,\"start\":71005},{\"end\":71016,\"start\":71013},{\"end\":71414,\"start\":71410},{\"end\":71421,\"start\":71418},{\"end\":71428,\"start\":71425},{\"end\":71753,\"start\":71750},{\"end\":71765,\"start\":71757},{\"end\":71774,\"start\":71769},{\"end\":71785,\"start\":71778},{\"end\":71793,\"start\":71789},{\"end\":71903,\"start\":71901},{\"end\":71909,\"start\":71905},{\"end\":72095,\"start\":72092},{\"end\":72104,\"start\":72099},{\"end\":72116,\"start\":72108},{\"end\":72122,\"start\":72120},{\"end\":72132,\"start\":72126},{\"end\":72472,\"start\":72468},{\"end\":72480,\"start\":72476},{\"end\":72494,\"start\":72484},{\"end\":72695,\"start\":72689},{\"end\":72948,\"start\":72944},{\"end\":72960,\"start\":72952},{\"end\":73233,\"start\":73226},{\"end\":73244,\"start\":73237},{\"end\":73257,\"start\":73248},{\"end\":73263,\"start\":73261},{\"end\":73274,\"start\":73267},{\"end\":73282,\"start\":73278},{\"end\":73293,\"start\":73286},{\"end\":73302,\"start\":73297},{\"end\":73310,\"start\":73306},{\"end\":73791,\"start\":73787},{\"end\":73802,\"start\":73795},{\"end\":73813,\"start\":73806},{\"end\":73826,\"start\":73817},{\"end\":73832,\"start\":73830},{\"end\":73843,\"start\":73836},{\"end\":73853,\"start\":73847},{\"end\":73864,\"start\":73857},{\"end\":73873,\"start\":73868},{\"end\":74229,\"start\":74221},{\"end\":74240,\"start\":74233},{\"end\":74255,\"start\":74244},{\"end\":74268,\"start\":74259},{\"end\":74555,\"start\":74546},{\"end\":74565,\"start\":74559},{\"end\":74579,\"start\":74569},{\"end\":74593,\"start\":74583},{\"end\":74607,\"start\":74597},{\"end\":74978,\"start\":74976},{\"end\":74985,\"start\":74982},{\"end\":74993,\"start\":74989},{\"end\":75001,\"start\":74997},{\"end\":75007,\"start\":75005},{\"end\":75015,\"start\":75011},{\"end\":75334,\"start\":75331},{\"end\":75348,\"start\":75338},{\"end\":75359,\"start\":75355},{\"end\":75369,\"start\":75363},{\"end\":75377,\"start\":75373},{\"end\":75391,\"start\":75383},{\"end\":75401,\"start\":75397},{\"end\":75659,\"start\":75656},{\"end\":75667,\"start\":75663},{\"end\":75676,\"start\":75671},{\"end\":75694,\"start\":75680},{\"end\":76001,\"start\":75994},{\"end\":76010,\"start\":76005},{\"end\":76020,\"start\":76014},{\"end\":76031,\"start\":76024},{\"end\":76041,\"start\":76035},{\"end\":76497,\"start\":76493},{\"end\":76506,\"start\":76501},{\"end\":76514,\"start\":76510},{\"end\":76522,\"start\":76518},{\"end\":76528,\"start\":76526},{\"end\":76535,\"start\":76532},{\"end\":76988,\"start\":76985},{\"end\":76994,\"start\":76992},{\"end\":77004,\"start\":76998},{\"end\":77016,\"start\":77010},{\"end\":77022,\"start\":77020},{\"end\":77484,\"start\":77482},{\"end\":77490,\"start\":77488},{\"end\":77882,\"start\":77880},{\"end\":77895,\"start\":77886},{\"end\":77906,\"start\":77899},{\"end\":78276,\"start\":78270},{\"end\":78288,\"start\":78280},{\"end\":78299,\"start\":78292},{\"end\":78311,\"start\":78303},{\"end\":78560,\"start\":78558},{\"end\":78569,\"start\":78564},{\"end\":78575,\"start\":78573},{\"end\":78811,\"start\":78807},{\"end\":78822,\"start\":78815},{\"end\":78832,\"start\":78826},{\"end\":79258,\"start\":79252},{\"end\":79269,\"start\":79262},{\"end\":79281,\"start\":79273},{\"end\":79295,\"start\":79285},{\"end\":79304,\"start\":79299},{\"end\":79316,\"start\":79308},{\"end\":79586,\"start\":79582},{\"end\":79594,\"start\":79590},{\"end\":79606,\"start\":79600},{\"end\":79872,\"start\":79864},{\"end\":79884,\"start\":79876},{\"end\":79890,\"start\":79888},{\"end\":79897,\"start\":79894},{\"end\":79906,\"start\":79901},{\"end\":80228,\"start\":80220},{\"end\":80236,\"start\":80232},{\"end\":80245,\"start\":80240},{\"end\":80258,\"start\":80249},{\"end\":80666,\"start\":80662},{\"end\":80673,\"start\":80670},{\"end\":80684,\"start\":80677},{\"end\":80693,\"start\":80688},{\"end\":80704,\"start\":80697},{\"end\":81035,\"start\":81025},{\"end\":81045,\"start\":81039},{\"end\":81058,\"start\":81049}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":211066225},\"end\":58012,\"start\":57537},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":215721598},\"end\":58477,\"start\":58014},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":214692972},\"end\":58926,\"start\":58479},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":52269752},\"end\":59225,\"start\":58928},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b4\"},\"end\":59544,\"start\":59227},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2893830},\"end\":59832,\"start\":59546},{\"attributes\":{\"doi\":\"arXiv:1412.6572\",\"id\":\"b6\"},\"end\":60082,\"start\":59834},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7004303},\"end\":60502,\"start\":60084},{\"attributes\":{\"id\":\"b8\"},\"end\":60776,\"start\":60504},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3405407},\"end\":61262,\"start\":60778},{\"attributes\":{\"id\":\"b10\"},\"end\":61441,\"start\":61264},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2645819},\"end\":61740,\"start\":61443},{\"attributes\":{\"doi\":\"arXiv:1707.08945\",\"id\":\"b12\"},\"end\":62123,\"start\":61742},{\"attributes\":{\"doi\":\"arXiv:1802.06430\",\"id\":\"b13\"},\"end\":62421,\"start\":62125},{\"attributes\":{\"id\":\"b14\"},\"end\":62758,\"start\":62423},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235228440},\"end\":63157,\"start\":62760},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235187536},\"end\":63450,\"start\":63159},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207947087},\"end\":64029,\"start\":63452},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":116751128},\"end\":64442,\"start\":64031},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":231644761},\"end\":64871,\"start\":64444},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206594738},\"end\":65328,\"start\":64873},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10328909},\"end\":65695,\"start\":65330},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232372031},\"end\":66126,\"start\":65697},{\"attributes\":{\"doi\":\"arXiv:2004.10934\",\"id\":\"b23\"},\"end\":66397,\"start\":66128},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b24\"},\"end\":66590,\"start\":66399},{\"attributes\":{\"id\":\"b25\"},\"end\":66708,\"start\":66592},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":208310312},\"end\":67276,\"start\":66710},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3698141},\"end\":67704,\"start\":67278},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":436933},\"end\":68100,\"start\":67706},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3355585},\"end\":68561,\"start\":68102},{\"attributes\":{\"id\":\"b30\"},\"end\":69036,\"start\":68563},{\"attributes\":{\"doi\":\"arXiv:1702.02284\",\"id\":\"b31\"},\"end\":69319,\"start\":69038},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53694866},\"end\":69827,\"start\":69321},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6200260},\"end\":70291,\"start\":69829},{\"attributes\":{\"doi\":\"arXiv:1511.06292\",\"id\":\"b34\"},\"end\":70572,\"start\":70293},{\"attributes\":{\"doi\":\"arXiv:1707.03501\",\"id\":\"b35\"},\"end\":70898,\"start\":70574},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":49904930},\"end\":71280,\"start\":70900},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":209677470},\"end\":71744,\"start\":71282},{\"attributes\":{\"id\":\"b38\"},\"end\":71859,\"start\":71746},{\"attributes\":{\"id\":\"b39\"},\"end\":72046,\"start\":71861},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":47252984},\"end\":72445,\"start\":72048},{\"attributes\":{\"doi\":\"arXiv:1904.07850\",\"id\":\"b41\"},\"end\":72622,\"start\":72447},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":115963355},\"end\":72887,\"start\":72624},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":224855431},\"end\":73153,\"start\":72889},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":29162614},\"end\":73731,\"start\":73155},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":49904930},\"end\":74166,\"start\":73733},{\"attributes\":{\"doi\":\"arXiv:1907.00374\",\"id\":\"b46\"},\"end\":74450,\"start\":74168},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":220403405},\"end\":74902,\"start\":74452},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":213674486},\"end\":75237,\"start\":74904},{\"attributes\":{\"doi\":\"arXiv:1705.02900\",\"id\":\"b49\"},\"end\":75652,\"start\":75239},{\"attributes\":{\"doi\":\"arXiv:1711.00117\",\"id\":\"b50\"},\"end\":75936,\"start\":75654},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4528012},\"end\":76404,\"start\":75938},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":604742},\"end\":76925,\"start\":76406},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":54462665},\"end\":77392,\"start\":76927},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":7733308},\"end\":77810,\"start\":77394},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":11438218},\"end\":78224,\"start\":77812},{\"attributes\":{\"doi\":\"arXiv:1702.04267\",\"id\":\"b56\"},\"end\":78479,\"start\":78226},{\"attributes\":{\"doi\":\"arXiv:1704.01155\",\"id\":\"b57\"},\"end\":78750,\"start\":78481},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":196470753},\"end\":79195,\"start\":78752},{\"attributes\":{\"doi\":\"arXiv:1705.07204\",\"id\":\"b59\"},\"end\":79519,\"start\":79197},{\"attributes\":{\"doi\":\"arXiv:2001.03994\",\"id\":\"b60\"},\"end\":79775,\"start\":79521},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":2672720},\"end\":80181,\"start\":79777},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":162184150},\"end\":80561,\"start\":80183},{\"attributes\":{\"doi\":\"arXiv:1710.10766\",\"id\":\"b63\"},\"end\":80932,\"start\":80563},{\"attributes\":{\"doi\":\"arXiv:1805.06605\",\"id\":\"b64\"},\"end\":81263,\"start\":80934}]", "bib_title": "[{\"end\":57610,\"start\":57537},{\"end\":58130,\"start\":58014},{\"end\":58526,\"start\":58479},{\"end\":58978,\"start\":58928},{\"end\":59598,\"start\":59546},{\"end\":60140,\"start\":60084},{\"end\":60858,\"start\":60778},{\"end\":61483,\"start\":61443},{\"end\":62508,\"start\":62423},{\"end\":62863,\"start\":62760},{\"end\":63233,\"start\":63159},{\"end\":63550,\"start\":63452},{\"end\":64117,\"start\":64031},{\"end\":64546,\"start\":64444},{\"end\":64928,\"start\":64873},{\"end\":65408,\"start\":65330},{\"end\":65773,\"start\":65697},{\"end\":66776,\"start\":66710},{\"end\":67328,\"start\":67278},{\"end\":67783,\"start\":67706},{\"end\":68155,\"start\":68102},{\"end\":68642,\"start\":68563},{\"end\":69411,\"start\":69321},{\"end\":69892,\"start\":69829},{\"end\":70980,\"start\":70900},{\"end\":71406,\"start\":71282},{\"end\":72085,\"start\":72048},{\"end\":72685,\"start\":72624},{\"end\":72940,\"start\":72889},{\"end\":73222,\"start\":73155},{\"end\":73783,\"start\":73733},{\"end\":74542,\"start\":74452},{\"end\":74972,\"start\":74904},{\"end\":75990,\"start\":75938},{\"end\":76489,\"start\":76406},{\"end\":76981,\"start\":76927},{\"end\":77478,\"start\":77394},{\"end\":77876,\"start\":77812},{\"end\":78803,\"start\":78752},{\"end\":79860,\"start\":79777},{\"end\":80216,\"start\":80183}]", "bib_author": "[{\"end\":57621,\"start\":57612},{\"end\":57632,\"start\":57621},{\"end\":57641,\"start\":57632},{\"end\":57651,\"start\":57641},{\"end\":57660,\"start\":57651},{\"end\":58143,\"start\":58132},{\"end\":58156,\"start\":58143},{\"end\":58172,\"start\":58156},{\"end\":58187,\"start\":58172},{\"end\":58196,\"start\":58187},{\"end\":58207,\"start\":58196},{\"end\":58535,\"start\":58528},{\"end\":58542,\"start\":58535},{\"end\":58548,\"start\":58542},{\"end\":58559,\"start\":58548},{\"end\":58566,\"start\":58559},{\"end\":58989,\"start\":58980},{\"end\":58995,\"start\":58989},{\"end\":59280,\"start\":59269},{\"end\":59291,\"start\":59280},{\"end\":59304,\"start\":59291},{\"end\":59313,\"start\":59304},{\"end\":59322,\"start\":59313},{\"end\":59336,\"start\":59322},{\"end\":59346,\"start\":59336},{\"end\":59611,\"start\":59600},{\"end\":59621,\"start\":59611},{\"end\":59898,\"start\":59882},{\"end\":59908,\"start\":59898},{\"end\":59919,\"start\":59908},{\"end\":60154,\"start\":60142},{\"end\":60166,\"start\":60154},{\"end\":60173,\"start\":60166},{\"end\":60187,\"start\":60173},{\"end\":60198,\"start\":60187},{\"end\":60207,\"start\":60198},{\"end\":60559,\"start\":60552},{\"end\":60570,\"start\":60559},{\"end\":60578,\"start\":60570},{\"end\":60868,\"start\":60860},{\"end\":60876,\"start\":60868},{\"end\":60884,\"start\":60876},{\"end\":60892,\"start\":60884},{\"end\":60899,\"start\":60892},{\"end\":60907,\"start\":60899},{\"end\":60916,\"start\":60907},{\"end\":60925,\"start\":60916},{\"end\":60933,\"start\":60925},{\"end\":60945,\"start\":60933},{\"end\":61319,\"start\":61308},{\"end\":61333,\"start\":61319},{\"end\":61343,\"start\":61333},{\"end\":61496,\"start\":61485},{\"end\":61508,\"start\":61496},{\"end\":61517,\"start\":61508},{\"end\":61525,\"start\":61517},{\"end\":61810,\"start\":61799},{\"end\":61821,\"start\":61810},{\"end\":61834,\"start\":61821},{\"end\":61843,\"start\":61834},{\"end\":61849,\"start\":61843},{\"end\":61860,\"start\":61849},{\"end\":61871,\"start\":61860},{\"end\":61879,\"start\":61871},{\"end\":62189,\"start\":62176},{\"end\":62202,\"start\":62189},{\"end\":62213,\"start\":62202},{\"end\":62223,\"start\":62213},{\"end\":62233,\"start\":62223},{\"end\":62517,\"start\":62510},{\"end\":62523,\"start\":62517},{\"end\":62531,\"start\":62523},{\"end\":62538,\"start\":62531},{\"end\":62545,\"start\":62538},{\"end\":62554,\"start\":62545},{\"end\":62873,\"start\":62865},{\"end\":62879,\"start\":62873},{\"end\":62887,\"start\":62879},{\"end\":62906,\"start\":62887},{\"end\":62916,\"start\":62906},{\"end\":62924,\"start\":62916},{\"end\":63247,\"start\":63235},{\"end\":63261,\"start\":63247},{\"end\":63276,\"start\":63261},{\"end\":63284,\"start\":63276},{\"end\":63560,\"start\":63552},{\"end\":63567,\"start\":63560},{\"end\":63576,\"start\":63567},{\"end\":63584,\"start\":63576},{\"end\":63593,\"start\":63584},{\"end\":63601,\"start\":63593},{\"end\":64127,\"start\":64119},{\"end\":64133,\"start\":64127},{\"end\":64140,\"start\":64133},{\"end\":64148,\"start\":64140},{\"end\":64156,\"start\":64148},{\"end\":64163,\"start\":64156},{\"end\":64567,\"start\":64548},{\"end\":64583,\"start\":64567},{\"end\":64599,\"start\":64583},{\"end\":64619,\"start\":64599},{\"end\":64940,\"start\":64930},{\"end\":64951,\"start\":64940},{\"end\":64963,\"start\":64951},{\"end\":64974,\"start\":64963},{\"end\":65417,\"start\":65410},{\"end\":65423,\"start\":65417},{\"end\":65435,\"start\":65423},{\"end\":65442,\"start\":65435},{\"end\":65782,\"start\":65775},{\"end\":65790,\"start\":65782},{\"end\":65797,\"start\":65790},{\"end\":65805,\"start\":65797},{\"end\":65813,\"start\":65805},{\"end\":65819,\"start\":65813},{\"end\":65831,\"start\":65819},{\"end\":65837,\"start\":65831},{\"end\":66199,\"start\":66184},{\"end\":66211,\"start\":66199},{\"end\":66224,\"start\":66211},{\"end\":66445,\"start\":66435},{\"end\":66456,\"start\":66445},{\"end\":66607,\"start\":66594},{\"end\":66789,\"start\":66778},{\"end\":66802,\"start\":66789},{\"end\":66811,\"start\":66802},{\"end\":66822,\"start\":66811},{\"end\":66834,\"start\":66822},{\"end\":66844,\"start\":66834},{\"end\":67337,\"start\":67330},{\"end\":67343,\"start\":67337},{\"end\":67350,\"start\":67343},{\"end\":67357,\"start\":67350},{\"end\":67364,\"start\":67357},{\"end\":67791,\"start\":67785},{\"end\":67800,\"start\":67791},{\"end\":67807,\"start\":67800},{\"end\":67814,\"start\":67807},{\"end\":68164,\"start\":68157},{\"end\":68173,\"start\":68164},{\"end\":68182,\"start\":68173},{\"end\":68191,\"start\":68182},{\"end\":68197,\"start\":68191},{\"end\":68203,\"start\":68197},{\"end\":68655,\"start\":68644},{\"end\":68668,\"start\":68655},{\"end\":68678,\"start\":68668},{\"end\":68690,\"start\":68678},{\"end\":69047,\"start\":69038},{\"end\":69059,\"start\":69047},{\"end\":69073,\"start\":69059},{\"end\":69081,\"start\":69073},{\"end\":69091,\"start\":69081},{\"end\":69422,\"start\":69413},{\"end\":69432,\"start\":69422},{\"end\":69442,\"start\":69432},{\"end\":69454,\"start\":69442},{\"end\":69462,\"start\":69454},{\"end\":69903,\"start\":69894},{\"end\":69913,\"start\":69903},{\"end\":69921,\"start\":69913},{\"end\":69932,\"start\":69921},{\"end\":70359,\"start\":70352},{\"end\":70367,\"start\":70359},{\"end\":70375,\"start\":70367},{\"end\":70385,\"start\":70375},{\"end\":70393,\"start\":70385},{\"end\":70668,\"start\":70662},{\"end\":70677,\"start\":70668},{\"end\":70686,\"start\":70677},{\"end\":70697,\"start\":70686},{\"end\":70989,\"start\":70982},{\"end\":70997,\"start\":70989},{\"end\":71003,\"start\":70997},{\"end\":71011,\"start\":71003},{\"end\":71018,\"start\":71011},{\"end\":71416,\"start\":71408},{\"end\":71423,\"start\":71416},{\"end\":71430,\"start\":71423},{\"end\":71755,\"start\":71748},{\"end\":71767,\"start\":71755},{\"end\":71776,\"start\":71767},{\"end\":71787,\"start\":71776},{\"end\":71795,\"start\":71787},{\"end\":71802,\"start\":71795},{\"end\":71905,\"start\":71897},{\"end\":71911,\"start\":71905},{\"end\":72097,\"start\":72087},{\"end\":72106,\"start\":72097},{\"end\":72118,\"start\":72106},{\"end\":72124,\"start\":72118},{\"end\":72134,\"start\":72124},{\"end\":72474,\"start\":72466},{\"end\":72482,\"start\":72474},{\"end\":72496,\"start\":72482},{\"end\":72697,\"start\":72687},{\"end\":72950,\"start\":72942},{\"end\":72962,\"start\":72950},{\"end\":73235,\"start\":73224},{\"end\":73246,\"start\":73235},{\"end\":73259,\"start\":73246},{\"end\":73265,\"start\":73259},{\"end\":73276,\"start\":73265},{\"end\":73284,\"start\":73276},{\"end\":73295,\"start\":73284},{\"end\":73304,\"start\":73295},{\"end\":73312,\"start\":73304},{\"end\":73793,\"start\":73785},{\"end\":73804,\"start\":73793},{\"end\":73815,\"start\":73804},{\"end\":73828,\"start\":73815},{\"end\":73834,\"start\":73828},{\"end\":73845,\"start\":73834},{\"end\":73855,\"start\":73845},{\"end\":73866,\"start\":73855},{\"end\":73875,\"start\":73866},{\"end\":74231,\"start\":74219},{\"end\":74242,\"start\":74231},{\"end\":74257,\"start\":74242},{\"end\":74270,\"start\":74257},{\"end\":74557,\"start\":74544},{\"end\":74567,\"start\":74557},{\"end\":74581,\"start\":74567},{\"end\":74595,\"start\":74581},{\"end\":74609,\"start\":74595},{\"end\":74980,\"start\":74974},{\"end\":74987,\"start\":74980},{\"end\":74995,\"start\":74987},{\"end\":75003,\"start\":74995},{\"end\":75009,\"start\":75003},{\"end\":75017,\"start\":75009},{\"end\":75336,\"start\":75329},{\"end\":75350,\"start\":75336},{\"end\":75361,\"start\":75350},{\"end\":75371,\"start\":75361},{\"end\":75379,\"start\":75371},{\"end\":75393,\"start\":75379},{\"end\":75403,\"start\":75393},{\"end\":75661,\"start\":75654},{\"end\":75669,\"start\":75661},{\"end\":75678,\"start\":75669},{\"end\":75696,\"start\":75678},{\"end\":76003,\"start\":75992},{\"end\":76012,\"start\":76003},{\"end\":76022,\"start\":76012},{\"end\":76033,\"start\":76022},{\"end\":76043,\"start\":76033},{\"end\":76499,\"start\":76491},{\"end\":76508,\"start\":76499},{\"end\":76516,\"start\":76508},{\"end\":76524,\"start\":76516},{\"end\":76530,\"start\":76524},{\"end\":76537,\"start\":76530},{\"end\":76990,\"start\":76983},{\"end\":76996,\"start\":76990},{\"end\":77006,\"start\":76996},{\"end\":77018,\"start\":77006},{\"end\":77024,\"start\":77018},{\"end\":77486,\"start\":77480},{\"end\":77492,\"start\":77486},{\"end\":77884,\"start\":77878},{\"end\":77897,\"start\":77884},{\"end\":77908,\"start\":77897},{\"end\":78278,\"start\":78266},{\"end\":78290,\"start\":78278},{\"end\":78301,\"start\":78290},{\"end\":78313,\"start\":78301},{\"end\":78562,\"start\":78556},{\"end\":78571,\"start\":78562},{\"end\":78577,\"start\":78571},{\"end\":78813,\"start\":78805},{\"end\":78824,\"start\":78813},{\"end\":78834,\"start\":78824},{\"end\":79260,\"start\":79250},{\"end\":79271,\"start\":79260},{\"end\":79283,\"start\":79271},{\"end\":79297,\"start\":79283},{\"end\":79306,\"start\":79297},{\"end\":79318,\"start\":79306},{\"end\":79588,\"start\":79580},{\"end\":79596,\"start\":79588},{\"end\":79608,\"start\":79596},{\"end\":79874,\"start\":79862},{\"end\":79886,\"start\":79874},{\"end\":79892,\"start\":79886},{\"end\":79899,\"start\":79892},{\"end\":79908,\"start\":79899},{\"end\":80230,\"start\":80218},{\"end\":80238,\"start\":80230},{\"end\":80247,\"start\":80238},{\"end\":80260,\"start\":80247},{\"end\":80668,\"start\":80660},{\"end\":80675,\"start\":80668},{\"end\":80686,\"start\":80675},{\"end\":80695,\"start\":80686},{\"end\":80706,\"start\":80695},{\"end\":81037,\"start\":81023},{\"end\":81047,\"start\":81037},{\"end\":81060,\"start\":81047}]", "bib_venue": "[{\"end\":57721,\"start\":57660},{\"end\":58218,\"start\":58207},{\"end\":58647,\"start\":58566},{\"end\":59052,\"start\":58995},{\"end\":59267,\"start\":59227},{\"end\":59669,\"start\":59621},{\"end\":59880,\"start\":59834},{\"end\":60269,\"start\":60207},{\"end\":60550,\"start\":60504},{\"end\":61000,\"start\":60945},{\"end\":61306,\"start\":61264},{\"end\":61569,\"start\":61525},{\"end\":61797,\"start\":61742},{\"end\":62174,\"start\":62125},{\"end\":62565,\"start\":62554},{\"end\":62931,\"start\":62924},{\"end\":63295,\"start\":63284},{\"end\":63686,\"start\":63601},{\"end\":64211,\"start\":64163},{\"end\":64633,\"start\":64619},{\"end\":65051,\"start\":64974},{\"end\":65491,\"start\":65442},{\"end\":65889,\"start\":65837},{\"end\":66182,\"start\":66128},{\"end\":66433,\"start\":66399},{\"end\":66935,\"start\":66844},{\"end\":67441,\"start\":67364},{\"end\":67876,\"start\":67814},{\"end\":68280,\"start\":68203},{\"end\":68772,\"start\":68690},{\"end\":69153,\"start\":69107},{\"end\":69523,\"start\":69462},{\"end\":70009,\"start\":69932},{\"end\":70350,\"start\":70293},{\"end\":70660,\"start\":70574},{\"end\":71066,\"start\":71018},{\"end\":71487,\"start\":71430},{\"end\":71895,\"start\":71861},{\"end\":72201,\"start\":72134},{\"end\":72464,\"start\":72447},{\"end\":72725,\"start\":72697},{\"end\":72996,\"start\":72962},{\"end\":73389,\"start\":73312},{\"end\":73935,\"start\":73875},{\"end\":74217,\"start\":74168},{\"end\":74641,\"start\":74609},{\"end\":75048,\"start\":75017},{\"end\":75327,\"start\":75239},{\"end\":75769,\"start\":75712},{\"end\":76120,\"start\":76043},{\"end\":76614,\"start\":76537},{\"end\":77105,\"start\":77024},{\"end\":77559,\"start\":77492},{\"end\":77975,\"start\":77908},{\"end\":78264,\"start\":78226},{\"end\":78554,\"start\":78481},{\"end\":78920,\"start\":78834},{\"end\":79248,\"start\":79197},{\"end\":79578,\"start\":79521},{\"end\":79956,\"start\":79908},{\"end\":80321,\"start\":80260},{\"end\":80658,\"start\":80563},{\"end\":81021,\"start\":80934},{\"end\":57769,\"start\":57723},{\"end\":58715,\"start\":58649},{\"end\":63758,\"start\":63688},{\"end\":65115,\"start\":65053},{\"end\":67013,\"start\":66937},{\"end\":67505,\"start\":67443},{\"end\":68344,\"start\":68282},{\"end\":69571,\"start\":69525},{\"end\":70073,\"start\":70011},{\"end\":72255,\"start\":72203},{\"end\":72740,\"start\":72727},{\"end\":73453,\"start\":73391},{\"end\":76184,\"start\":76122},{\"end\":76678,\"start\":76616},{\"end\":77173,\"start\":77107},{\"end\":77613,\"start\":77561},{\"end\":78029,\"start\":77977},{\"end\":78993,\"start\":78922},{\"end\":80369,\"start\":80323}]"}}}, "year": 2023, "month": 12, "day": 17}