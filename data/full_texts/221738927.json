{"id": 221738927, "updated": "2023-10-06 11:05:11.767", "metadata": {"title": "Text Generation by Learning from Off-Policy Demonstrations", "authors": "[{\"first\":\"Richard\",\"last\":\"Pang\",\"middle\":[\"Yuanzhe\"]},{\"first\":\"He\",\"last\":\"He\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 9, "day": 16}, "abstract": "Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as a reinforcement learning (RL) problem with expert demonstrations (i.e., the training data), where the goal is to maximize quality given model-generated histories. Prior RL approaches to generation often face optimization issues due to the large action space and sparse reward. We propose GOLD (generation by off-policy learning from demonstrations): an algorithm that learns from the off-policy demonstrations by importance weighting and does not suffer from degenerative solutions. We find that GOLD outperforms the baselines according to automatic and human evaluation on summarization, question generation, and machine translation, including attaining state-of-the-art results for CNN/DailyMail summarization. Further, we show that models trained by GOLD are less sensitive to decoding algorithms and the generation quality does not degrade much as the length increases.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2009.07839", "mag": "3085932930", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2009-07839", "doi": null}}, "content": {"source": {"pdf_hash": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.07839v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8e641fe47403404378e56ea12fb486dc0dcb445b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3ee38da21d8cf9cb7d4077b729e57f68e9c8d671.txt", "contents": "\nText Generation by Learning from Off-Policy Demonstrations\n\n\nRichard Yuanzhe Pang \nHe He \nText Generation by Learning from Off-Policy Demonstrations\n\nCurrent approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. modelgenerated). To alleviate these problems, we frame text generation as a reinforcement learning (RL) problem with expert demonstrations (i.e., the training data), where the goal is to maximize quality given model-generated histories. Prior RL approaches to generation often face optimization issues due to the large action space and sparse reward. We propose GOLD (generation by offpolicy learning from demonstrations): an algorithm that learns from the off-policy demonstrations by importance weighting and does not suffer from degenerative solutions. We find that GOLD outperforms the baselines according to automatic and human evaluation on summarization, question generation, and machine translation, including attaining state-of-the-art results for CNN/DailyMail summarization. Further, we show that models trained by GOLD are less sensitive to decoding algorithms and the generation quality does not degrade much as the length increases.\n\nIntroduction\n\nA common approach to text generation is to use autoregressive models (e.g., seq2seq) learned by maximum likelihood estimation (MLE). However, this approach introduces two well-known discrepancies between training and evaluation objectives that lead to undesired generations.\n\nFirst, the training loss is negative log-likelihood, whereas the 1 Courant Institute of Mathematical Sciences, New York University, New York, NY 10011, USA 2 Center for Data Science, New York University, New York, NY 10011, USA. Correspondence to: Richard Yuanzhe Pang <yzpang@nyu.edu>, He He <hehe@cs.nyu.edu>.\n\n\nPreprint.\n\nevaluation is based on human judgment of the output quality. Although MLE is consistent, under model misspecification it tends to over-generalize, assigning large probability mass to both high-quality and low-quality sequences (Husz\u00e1r, 2015;Simon et al., 2019). Empirically, direct sampling from the learned model often produces implausible sentences, and decoding algorithms such as beam search and top-k sampling must be used to select the high-quality outputs.\n\nSecond, during training, the model prediction is conditioned on the gold history/prefix; however, at evaluation time autoregressive generation conditions on model-generated token history. This is often known as the exposure bias problem in sequence generation (Ranzato et al., 2016;Bengio et al., 2015). In the worst case, one incorrect prediction can produce a low-probability prefix under the gold data distribution, and errors compound in each of the following steps (Ross et al., 2011). In practice, prior work has observed problems such as repetition and hallucination partly due to exposure bias (Holtzman et al., 2020;Wang & Sennrich, 2020).\n\nTo match training and evaluation objectives, we want to maximize output quality under the model-generated history distribution. This goal naturally fits the reinforcement learning (RL) objective, which is to maximize expected reward (quality) over trajectories (generations) induced by the policy (model). Prior RL approaches mainly focus on finetuning a learned model to optimize sequence-level metrics such as BLEU. However, optimizing this objective is notoriously difficult, and it remains unclear if RL is beneficial to text generation (Wu et al., 2018;Choshen et al., 2020).\n\nMany challenges in RL arise from exploring an exponentially large space of sequences with sparse rewards only on those close to the reference. We thus propose to learn from human demonstrations (the reference data) directly without interaction (i.e., the offline setting) using off-policy policy gradient. To match the training loss and the evaluation metric, we propose reward functions that approximate human judgment of the output quality. Concretely, we account for varying qualities of the references by estimating how likely a human would have generated it. To match history distributions during training and inference, we use importance weighting (Hastings, 1970)  higher. Intuitively, the learning algorithm focuses on examples where the history is more likely to be generated by the model. We call our algorithm GOLD (Generation by Off-policy Learning from Demonstrations).\n\nWe evaluate GOLD on a diverse set of generation tasks: news summarization on both CNN/DM (Hermann et al., 2015) and the highly abstractive XSum (Narayan et al., 2018), question generation (NQG;Zhou et al., 2017), and IWSLT14 De-En machine translation. We find that GOLD achieves better performance than MLE and RL fine-tuning by both task metrics and human-rated quality. Further, it maintains the advantage on pre-trained transformers: we achieve strong performance on CNN/DM and XSum using BART (Lewis et al., 2020). Our analysis shows that GOLD learns a high-precision distribution and is less sensitive to decoding algorithms. In addition, it alleviates exposure bias: the output quality does not degrade as generation length increases.\n\n\nFrom MLE to RL Framework\n\nSequence generation. We consider conditional sequence generation tasks. Given an input context x such as a document, we would like to generate a sequence of tokens 1 y = (y 0 , . . . , y T ), where y i comes from a vocabulary V. Typically, the generator is modeled as a conditional probability distribution parametrized by \u03b8: p \u03b8 (y | x) = N i=0 p \u03b8 (y t | y 0:t\u22121 , x), where y 0:t\u22121 denotes the prefix y 0 , . . . , y t\u22121 . Let p human (y | x) denote the data-generating distribution.\n\nL(\u03b8) = \u2212E y\u223cphuman T t=0 log p \u03b8 (y t | y 0:t\u22121 , x) . (1) The parameters are estimated by minimizing the average loss on the training set. At inference time, we generate tokens sequentially according to p \u03b8 .\n\nEvaluation. In practice, the quality of an output often relies on task-specific metrics such as fluency, correctness, and interestingness. Here for generality we consider perceptual quality (Husz\u00e1r, 2015;Hashimoto et al., 2019) which measures how likely a human would have generated the output given the context, i.e., p human (y | x). Thus the expected evaluation metric is E y\u223cp \u03b8 T t=0 log p human (y t | y 0:t\u22121 , x) .\n\n(2)\n\nComparing (1) and (2), we can see that the training objective encourages high recall: the model must put probability mass on all human-written outputs. In contrast, the evaluation 1 We start from index 0 so as to fit the RL formulation later.\n\nobjective encourages high precision: all outputs from the model must be of high quality. In addition to quality, diversity is another metric for tasks such as dialogue and story generation. In this work we focus on tasks where the main goal is to generate high quality outputs such as machine translation and summarization.\n\nUnfortunately, directly optimizing (2) is impossible because p human is unknown and the expectation is hard to estimate. We therefore develop a training objective that closely approximates the evaluation objective (2). To ease the later discussion, we first reformulate sequence generation in the RL setup.\n\nRL formulation. Consider generation as a sequential decision-making process. At each time step t, let the state s t be (y 0:t\u22121 , x). The stochastic policy \u03c0 maps the state to a probability distribution over the action space V:\n\u03c0 \u03b8 (a t | s t ) = p \u03b8 (a t | s t ) where a t \u2208 V.\nThe tth token is generated by sampling from \u03c0 \u03b8 (a t | s t ), and a reward r t is received. Thus we can represent the generation as a trajectory \u03c4 = (s 0 , a 0 , r 0 , . . . , s T , a T , r T ). With a little notation overloading, we use \u03c0 \u03b8 (\u03c4 ) to denote the distribution of \u03c4 induced by \u03c0 \u03b8 . The set of trajectories derived from the training data is often called demonstrations which show the desired behavior of a policy. The RL objective is to maximize the expected return of \u03c0 \u03b8 :\nJ(\u03b8) = E \u03c4 \u223c\u03c0 \u03b8 T t=0 \u03b3 t r t ,(3)\nwhere \u03b3 \u2208 (0, 1] is the discount factor. This objective is closely related to (2) as both expectations are taken over the model-induced distribution. Next, we describe how to estimate the expectation without directly sampling from \u03c0 \u03b8 and define reward functions that approximate p human .\n\n\nApproach\n\n\nOff-Policy Policy Gradient\n\nSuppose we have access to oracle rewards such that r t = p human (a t | s t ), then the RL objective (3) corresponds to our evaluation metric (2). Let's begin by considering how to learn the policy given oracle rewards.\n\nPolicy gradient. A straightforward way to optimize J(\u03b8) is policy gradient (PG) (Williams, 1992;Sutton et al., 2000), which directly estimates the gradient of the objective with respect to \u03b8. Formally, we have \u2207 \u03b8 J(\u03b8) =\nE \u03c4 \u223c\u03c0 \u03b8 t \u2207 \u03b8 log \u03c0 \u03b8 (a t | s t )Q(s t , a t ) ,(4)\nwhereQ\n(s t , a t ) = T t =t \u03b3 t \u2212t r t\nis the estimated return from state s t . In text generation, the gradient is often estimated by Monte Carlo samples from \u03c0 \u03b8 . The returnQ(s t , a t ) is a sequence-level reward such as BLEU score, which is zero for t < T . In practice, due to the high variance of the gradient, the policy is very likely to end in a region with zero reward, generating gibberish without receiving any learning signal (Li et al., 2018;Keneshloo et al., 2019). A common remedy is to initialize with the MLE solution and further interleave with MLE gradient update on training examples during PG. However, this remedy significantly biases the parameters towards the MLE solution; thus it only leads to marginal gains in practice.\n\nOff-policy learning. To avoid zero-reward regions, we would like to reduce interaction with the environment and stay close to the demonstrated trajectories. In the extreme case, the policy must be learned solely from the static demonstrations without additional interaction with the environment, which is referred to as the offline setting in RL. While it is in general a more challenging problem, we argue that the offline setting is appropriate for text generation (Serban et al., 2017;Jaques et al., 2019).\n\nFirst, the environment dynamics is known: once a token is generated, we deterministically transition to the next state with the additional token in the prefix. Thus no interaction is needed to learn about the environment. Second, while exploration may lead to high-quality sequences different from the given reference, we lack a good reward function to identify these sequences at the moment (Novikova et al., 2017;Aharoni & Goldberg, 2018;Clark et al., 2019). Therefore, the benefit of exploration in text generation is limited.\n\nIn the offline setting, we cannot directly estimate expected return of the current policy \u03c0 \u03b8 , and must learn from trajectories sampled from a different behavioral policy \u03c0 b . This is known as off-policy learning in RL. A common technique to estimate expectations under one distribution \u03c0 \u03b8 given samples from a different distribution \u03c0 b is importance sampling, where each sample is weighted by \u03c0 \u03b8 (\u03c4 ) \u03c0 b (\u03c4 ) . Using importance sampling, we have the following unbiased estimator of the gradient (Precup et al., 2000):\nE \u03c4 \u223c\u03c0 b t w t \u2207 \u03b8 log \u03c0 \u03b8 (a t | s t )Q(s t , a t ) ,\nwith importance weights\nw t = t t =0 \u03c0 \u03b8 (a t | s t ) \u03c0 b (a t | s t ) .\nApproximations. Computing the importance weights above requires multiplying per-action importance weight over multiple time steps, which leads to high variance. In practice, we have found that it is sensitive to optimization hyperparameters and takes longer to converge (Appendix A.4). Therefore, we use the per-action approximation: w t \u2248 \u03c0 \u03b8 (at|st) \u03c0 b (at|st) . This corresponds to optimizing the expected return under the off-policy state distribution induced by \u03c0 b and the on-policy action distribution of \u03c0 \u03b8 , thus the importance weight is only needed for each action instead of each trajectory. Although this estimator is biased, empirically it has been shown to reduce variance and work reasonably well if \u03c0 b and \u03c0 \u03b8 are close (Serban et al., 2017;Levine et al., 2020). 2 Another obstacle is that we do not know \u03c0 b which produced the demonstrations\nD = {(x (i) , y (i) )} N i=1 . 3\nOne option is to estimate \u03c0 b on D by supervised learning. Here we take a simpler approach that assumes a Dirac delta distribution: \u03c0 b (\u03c4 ) \u2248 1/N for \u03c4 \u2208 D and 0 otherwise. As a result, the denominator in w t is a constant and can be ignored in optimization.\n\nOur final approximated gradient has the form:\n\u2207 \u03b8 J(\u03b8) \u2248 N i=1 T t=0 \u03c0 \u03b8 (a i t | s i t )\u2207 \u03b8 log \u03c0 \u03b8 (a i t | s i t )Q(s i t , a i t ),(5)\nwhere the superscript i denotes states and actions from the ith trajectory. Compare (4) and (5) with the MLE gradient:\n\u2207 \u03b8 L(\u03b8) = N i=1 T t=0 \u2207 \u03b8 log \u03c0 \u03b8 (a i t | s i t ).(6)\nWe can see that PG upweights states with high return and offpolicy PG further upweights states where prediction from the current policy \u03c0 \u03b8 is close to the demonstrated/reference action. Intuitively, this observation makes sense because in these states \u03c0 \u03b8 is less likely to deviate away from the reference, thus alleviating exposure bias. It also allows the learning algorithm to focus on \"easy\" examples which improve precision.\n\n\nReward\n\nGiven the off-policy policy gradient algorithm described in the previous section, the remaining question is how to assign rewards to a generated token given some context and prefix. Let R be the reward function such that r t = R(s t , a t ).\n\nIn order to optimize the perceptual quality (see (2)), we want R(s, a) to approximate p human (a | s), i.e., how likely a human would have generated a given s. A general solution is to use a discriminator as in generative discriminative networks. However, recent results have shown that the discriminator tends to have low accuracy (Li et al., 2017) and the overall generation results are worse than MLE-trained models (Tevet et al., 2019;Caccia et al., 2020). One challenge here is that the discriminator must work well for a large space of generations including the references and adaptive model outputs. In the off-policy setting, however, we can restrict the domain of R to state-action pairs on the demonstrated trajectories. We propose three types of reward functions below.\n\n\u03b4-reward. An obvious choice for the reward is to use the Dirac delta function to define a sequence-level reward, where we consider all demonstrations to be equally good and assign zero reward to any other outputs. We thus have the following reward:\nR \u03b4 (s t , a t ) def = 1, if t = T and (s 0:T , a 0:T ) \u2208 D 0, otherwise(7)\nwhere a reward of one is received in the terminal state for any trajectory in the demonstrations. In this case, the returnQ t in (5) is 1 for all training examples, assuming the discount factor \u03b3 = 1.\n\nEstimated p human . It is not unreasonable to assume that all demonstrations are equally likely under p human ; in fact, MLE is equivalent to PG with the \u03b4-reward. However, for text generation tasks, there are many correct outputs and in practice the datasets often contain examples on the long tail, e.g., examples with rare words or complex syntax. Therefore, we learn a distribution q(a | s) to approximate p human (a | s) by minimizing KL (p human q), which is equivalent to finding the MLE solution (denoted by p MLE ). 4 Importantly, p MLE is a reasonable approximation to p human when restricted to the demonstrations: in the worst case it is a uniform distribution and all trajectories are equally likely. It is not a good reward function in general, however. As discussed in Section 2, it can assign probability mass to low-quality outputs.\n\nGiven the estimated per-action quality p MLE (a | s) \u2248 p human (a | s), we have two ways to define the reward function. To recover the evaluation objective in (2), we define\nR p (s, a) def = log p MLE (a | s).(8)\n4 Note that KL (phuman q) = Ep human log phuman \u2212 Ep human log q, thus minimizing the KL divergence with respect to q is equivalent to the MLE objective in (6).\n\nAssuming \u03b3 = 1, the return at time step t isQ t (s t , a t ) = T t =t log p MLE (a t | s t ). This corresponds to a product of probabilities; thus a policy has high return if and only if it receives high reward at all steps.\n\nTo allow for partial credits even if bad actions are taken at certain steps, we define\nR s (s, a) def = p MLE (a | s).(9)\nThis reward function corresponds to sum of probabilities whereQ(s t , a t ) = T t =t p MLE (a t | s t ), and a policy can recover from bad decisions if the subsequent actions receive high reward.\n\n\nThe GOLD Algorithm\n\nIn practice, the naive off-policy PG algorithm based on minibatch stochastic gradient descent often has high variance. Therefore, we take the following measures to stabilize training.\n\nThe first source of variance comes from the importance weights \u03c0 \u03b8 (a | s) which depend on the policy being learned. To avoid drastic changes during the initial stage of training, we initialize \u03c0 \u03b8 with the MLE solution. We also compute the importance weights by a weighting policy\u03c0 \u03b8 that synchronizes with \u03c0 \u03b8 periodically so that the weights do not change frequently between updates. In addition, to reduce variance, we lower-bound the importance weight by a small number a.\n\nThe second source of variance comes from noisy policy gradients. Since our return is computed from a sum or product of probabilities ((8) and (9)), we truncate the future trajectory after five steps:Q t (s t , a t ) = t+5 t =t r t , which effectively uses a discount factor of 0.8. 5 We follow the common practice to subtract a baseline b from the return. Moreover, to avoid negative reward on the demonstrations, we lower-bound p MLE in (8) and (9) by a small number c. We summarize the training algorithm in Algorithm 1. GOLD is easy to implement in practice with weighted negative log-likelihood loss for each token.\n\n\nExperiments\n\n\nSetup\n\nTasks and datasets. We chose four text generation tasks where quality is the major concern in evaluation: (1) question generation (NQG; Zhou et al., 2017): given a passage and a short span of the passage, the goal is to generate a question that can be answered by the span; (2) summarization (CNN/DM; Hermann et al., 2015): note that the CNN/DM reference summaries tend to be extractive; (3) extreme sum-\nAlgorithm 1: GOLD 1 Initialize with the MLE solution: \u03c0 \u03b8 \u2190 p MLE ,\u03c0 \u03b8 \u2190 p MLE 2 for step = 1, 2, . . . , M do 3 Sample a minibatch B = {(x i , y i )} |B| i=1 4 foreach (s i t , a i t ) do 5\nCompute importance weights using max(a,\u03c0 \u03b8 ), and compute returnQ(\ns i t , a i t ) \u2212 b 6\nUpdate \u03b8 by gradient descent using (5)   7 if step % k = 0 then\u03c0 \u03b8 \u2190 \u03c0 \u03b8  (Papineni et al., 2002) and ROUGE-1/2/L (Lin, 2004) respectively, as well as human ratings.\n\nModels and algorithms. We experiment with three variants of GOLD: GOLD-\u03b4, GOLD-p, GOLD-s, which uses the \u03b4-reward and the two estimated rewards (R p and R s ), respectively. Our baseline learning algorithm is standard MLE, and we compare with on-policy RL training using policy gradient in Section 4.3. We describe models for each task at the beginning of Section 4.2.\n\nImplementation details. To reduce variance, the baseline in PG can be set to any state-dependent value. We use b = \u221260 for GOLD-p and 0 for GOLD-s, which is roughly the reward of a uniform policy. To lower bound the return such that the it is non-negative on demonstrated trajectories, we tune the lower bound of p MLE in {0, 0.01, 0.05, 0.1} in (8) and (9) on the development set. Furthermore, to reduce variance for importance weights, we tune a \u2208 {0, 0.1, 0.15, 0.2}. See Appendix A.3 for more details.\n\n\nResults and Analysis\n\nGOLD improves both standard and transformer models. Recall that one of our main motivations is that MLE tends to over-generalize under model misspecification, i.e., high recall but low precision. One may wonder whether this problem can be fixed by better modeling. Therefore, we evaluated GOLD with both standard high-performing models and state-of-the-art pretrained model. respectively. 6 Table 1 shows that GOLD is better than MLE in terms of BLEU and ROUGE. In particular, we find that using estimated rewards is superior to the \u03b4-reward, showing the benefits of accounting for varying quality of the references. We thus consider only GOLD-p and GOLD-s in the rest of the experiments.\n\nFor transformer models (Vaswani et al., 2017), we used the pretrained BART (Lewis et al., 2020) for NQG, CNN/DM, and XSum; we used standard transformer for IWSLT14 De-En. Table 3 shows that GOLD achieves better scores than MLE across all tasks, including new SOTA on CNN/DM and XSum (Appendix B.2).\n\nWe further crowdsourced human evaluation 7 for NQG and the two summarization tasks by pairwise comparison (Appendix C). Table 4 shows that workers prefer outputs from models trained by GOLD more often than those trained by MLE. Specifically, for NQG human evaluation, for each unit of human evaluation, we present the source paragraph, the words to ask the question on, the question generated by MLE-trained model, as well as the question generated by GOLD-s-trained model. We ask the human evaluators 6 We didn't use standard seq2seq-based models for IWSLT14 and XSum as they are not competitive. 7 For human evaluations, we used Amazon Mechanical Turk. We designed dummy tasks with very obvious answers and integrated them into the regular annotation tasks. We blocked workers who failed the dummy tasks and discarded all their results.  Table 3. Results using transformer models on test sets. The advantage of GOLD is maintained on advanced models based on transformers and pretraining.  Table 4. Human comparisons on 200 randomly selected test examples for each task. \"Win\": the % of generations from MLE-trained BART that are better than from GOLD-trained BART, given the same source. For NQG, we show workers the entire input and the questions generated by two models, and we ask workers to select the better one. For summarization, we ask workers to select the generation closer in meaning to the reference. Workers prefer outputs from models trained from GOLD more than MLE. See Appendix C. the general question: which generated question is better? 8 Figure 5 in the appendix shows one example interface of pairwise comparisons. For summarization tasks, given that it is difficult to get high-quality low-variance annotations if we ask workers to read the entire news article, we did the following: given the reference summary, a summary generated from MLE model, and a summary generated from our model, we asked workers to compare which generated summary is closer in meaning to the reference summary. Figure 6 in the appendix shows one example interface of the mentioned pairwise comparison for summarization. In summary, from Table 4, we see that workers prefer GOLDs-trained model than MLE-trained model, more than the converse.\nObjective NQG CNN/DM XSum IWSLT14 De-En (BART) (BART) (BART) (Transformer) BLEU \u2191 ppl \u2193 R-1 \u2191 R-2 \u2191 R-L \u2191 ppl \u2193 R-1 \u2191 R-2 \u2191 R-L \u2191 ppl \u2193 BLEU \u2191 ppl \u2193 MLENQG CNN/DM XSum (BART) (BART)(\nExample generations are provided in Appendix B.4.\n\nGOLD encourages high-precision models. One interesting observation from Table 1 and Table 3 is that compared to MLE, GOLD leads to much higher held-out perplexities, while achieving better metric scores. Since both are evaluated against the reference, one would expect high perplexity to correlate with low metric scores.\n\nTo better understand the behavior of GOLD, we examine the distributions of token-level negative log-likelihood (NLL) loss (a monotonic transformation of perplexity) in Figure 1.\n\nWe see that the loss distribution of GOLD (compared to MLE) concentrates on near-zero losses (Figures 1a and 1c) with a long tail of large losses (Figures 1b and 1d), hence high perplexity. In contrast, MLE has much fewer near-zero losses and fewer large losses, suggesting it tries to learn all tokens. We conclude that GOLD achieves better metric scores by focusing on easy-to-learn tokens at the expense of lower recall with respect to the reference. These results empirically support our discussion on the behavior of MLE and RL in Section 2.\n\nAnother advantage of high-precision models is that they do not rely much on decoding algorithms to sample highquality outputs from the learned distribution. Further, from a RL perspective, the policy already considers future rewards when making local decisions, thus beam search is not necessary. As a result, we see in Table 2 that GOLD achieves similar performance with both argmax decoding and top-k sampling 9 . In contrast, MLE suffers significantly from sampling, which suggests that it learns a high-recall but low-precision model.\n\nGOLD alleviates exposure bias. GOLD does not suffer from exposure bias because it trains on the state/history distribution induced by the model instead of the reference data (see (3)). Here, we empirically quantify the exposure bias problem in learned models. If there is exposure bias, then the output quality is expected to degrade as output length increases, as the history is more likely to deviate from the reference distribution with accumulated generation steps.\n\nTo evaluate quality, we sampled 736 generations of different lengths from standard models trained by both MLE and GOLD on NQG. Given the paragraph, words to query on, and the generated questions, we then asked workers to rate the generations from 1 (worst) to 4 (best). See Appendix C for details. Figure 2a clearly shows that the output quality of the MLE-trained model degrades significantly when the sequence length is over 14 words, whereas the quality of the GOLD-s-trained model stays relatively stable across all lengths. 10 Qualitatively, we observe frequent degenerations 9 Results are similar for GOLD-p. 10 We also used BLEU as a quality metric and observed similar MLE learns high-recall models whose loss distribution is spread out; GOLD learns high-precision models whose loss distribution is concentrated on near-zero losses.  Table 5.\n\nIn contrast, Figure 2b shows the NLL loss conditioned on gold histories on NQG dev set. 11 We can see that without exposure bias, NLL loss (with respect to the next gold token) does not vary much as the length increases. Therefore, we conclude that the big performance drop for long generations using MLE is mainly due to exposure bias and GOLD does not suffer from the problem.\n\n\nComparison with On-Policy Training\n\nWhile offline RL is generally more challenging due to lack of interaction with the environment, we argue that the benefit from interaction is limited in text generation (Section 3.1) results, shown in Appendix B.3. 11 We also include a token prediction accuracy vs. time-step plot in the appendix, and the plot shows similar trend.\n\nInput that project was entitled the factory project to reference andy warhol and to create a factory to completely digitize the collection . MLE what was the name of the project that was not digitize to digitize ? GOLD what was the name of the project that was to reference andy warhol ?\n\nInput braddock (with george washington as one of his aides) led about 1,500 army troops and provincial militia on an expedition in june 1755 to take fort duquesne . MLE what was the name of the aid of george washington university ? GOLD who led about 1,500 army troops and provincial militia on an expedition ? and overweighed by the optimization challenges. In this section, we investigate the effect of on-policy training using task metrics as rewards.\n\nSpecifically, we pre-train the model using MLE and then fine-tune it using PG. To avoid degenerate solutions, we interleave MLE and PG updates evenly during fine-tuning. Similarly, we fine-tune GOLD-initialized models using PG. For on-policy fine-tuning, we use BLEU and ROUGE as rewards for NQG and CNN/DM respectively. 12 Table 6 shows that additional on-policy training improves both MLE and GOLD marginally. However, MLE with PG is still worse than GOLD. Overall, the benefit from on-policy training is unclear in our experiments.\n\n\nRelated Work\n\nExposure bias. In structured prediction, there is a flurry of works addressing exposure bias since Bengio et al. (2015). Most works focus on learning global sequence scores instead of locally normalized scores using either variants of beam search (Wiseman & Rush, 2016;Andor et al., 2016;Goyal et al., 2018) or energy networks (Belanger & Mc-Callum, 2016;Tu et al., 2019). These training algorithms are often more complex and costly than MLE. Additionally, exposure bias is well studied in imitation learning (Daum\u00e9 et al., 2009;Ross et al., 2011) and the learning-to-search approach has been applied to RNNs to incorporate losses of sequences deviating from the references (Leblond et al., 2018). However, they require annotations or cost functions on non-reference sequences, which may not be available for text generation.\n\nObjectives beyond MLE. Policy gradient-based algorithms have been used extensively in text generation to optimize sequence-level metrics (Ranzato et al., 2016;Shen et al., 2015;Pasunuru & Bansal, 2018). In addition, offpolicy RL is commonly used in dialogue where online interaction with users is expensive (Serban et al., 2017;Jaques et al., 2019). The main difference is that we take advantage of the demonstrations and design generic reward functions for generation tasks.\n\nThere is another line of work using policy gradient to optimize reward from a discriminator that differentiates good vs. bad generations (Yu et al., 2017;Li et al., 2017;Lu et al., 2019). However, these approaches often underperform MLE in practice (Tevet et al., 2019) due to optimization challenges. Recently, a concurrent work, Kang & Hashimoto (2020), proposed truncated log-loss which both optimizes distinguishability and enjoys efficient optimization.\n\nHigh-precision text generation. It is noticed early in neural text generation that MLE tends to produce highrecall models that over-generalize. Previously, high-quality outputs are selected mainly through decoding (e.g., beam search, low-temperature sampling, truncated sampling). Recently, there is an increasing amount of work on discourag-12 While the reward functions in Section 3.2 are useful on the demonstrations, they are not suitable for the on-policy setting as they cannot differentiate good vs. bad generations on the entire output space effectively. For example, Murray & Chiang (2018) (Tian et al., 2020). In contrast, we tackle the fundamental problem of mismatched objectives and propose a general learning framework.\n\n\nConclusion\n\nWe provide an efficient algorithm that addresses the two train/test discrepancies in MLE training for text generation: likelihood as learning objective vs. quality as evaluation metric; gold history in training vs. model-generated history in inference. We have demonstrated that off-policy RL is a promising framework for text generation, with matched train/test objectives and optimization advantages like MLE. We believe more advanced off-policy learning techniques (e.g., proximity constraints) can be easily integrated into text generation and further improve performance. While on-policy learning is attractive, current progress is hindered by the lack of robust reward functions for sequences distant from the references, which we leave for future work. \n\n\nA. Practical Setup and Implementation\n\n\nA.1. Tasks and Datasets\n\n(1) Natural question generation (NQG; Zhou et al., 2017) based on the SQuAD QA dataset (Rajpurkar et al., 2016): given a text passage and a short span of the passage, the goal is to generate a question that can be answered by the span.\n\n(2) CNN/DailyMail summarization (CNN/DM): Given a piece of news, generate a few sentences of summary. We use the entity-non-anonymized version of CNN/DM dataset, following See et al. (2017). The target summaries tend to be extractive, meaning there tends to be heavy text-span overlaps between the source article and the target summary.\n\n(3) Extreme summarization (XSum; Narayan et al., 2018) is based on BBC news. The target summaries are highly abstractive. Past extractive strategies that work well for CNN/DM may not work well for XSum. (4) IWSLT14 German to English machine translation (IWSLT14 De-En; Cettolo et al., 2014) is a popular machine translation benchmark. Machine translation is different from the above three tasks, given that intuitively, the space of high-quality generation is smaller.\n\nMore details on datasets. To download and preprocess the NQG data, we follow the following instructions: https: //github.com/clovaai/FocusSeq2Seq; to download and preprocess the summarization data, we follow the following instructions: https:// github.com/pytorch/fairseq/blob/master/ examples/bart/README.summarization.md; to download and preprocess the IWSLT14 De-En data, we follow the following instructions: https://github.com/pytorch/fairseq/ tree/master/examples/translation.\n\n\nA.2. Model Architectures\n\nWe use two sets of architectures for our experiments. Transformer architectures. For NQG, CNN/DM, and XSum, we also experiment with one of the top-performing models, BART (Lewis et al., 2020). Our experiments are based on the pretrained BART model provided by original authors 13 : it has 12 encoder layers and 12 decoder layers, and it is pretrained on around 3.3 billion words of Wikipedia articles and books. We use the model to investigate if our methods work with models with stronger capabilities. For IWSLT14 De-En, we use the standard transformer architecture, a top-performing architecture in machine translation.\n\n\nA.3. More on Reproducibility\n\nThe codebase will be released.\n\nHyperparameters and training details on standard architectures. This paragraph corresponds to results in Table 1. We use a learning rate of 5e-4. For NQG, we use a batch size of 32; for CNN/DM we use a batch size of 16. We train using a single Nvidia GTX 1080 Ti (memory: 12 GB) GPU.\n\nAs discussed in Section 3.3 and Section 4.1, we tune the lower bound of p MLE in {0, 0.01, 0.05, 0.1}. For NQG models, the lower bound of 0.1 produces best performance. For CNN/DM using GOLD-p, the lower bound is 0.01; for CNN/DM using GOLD-s, the lower bound is 0.\n\nRecall that as discussed in Section 3.3, the weighting polic\u1ef9 \u03c0 \u03b8 synchronizes with actual policy \u03c0 \u03b8 once every k steps so as to stabilize training. We tune k \u2208 {1500, 2691} (where 2691 steps corresponds to 1 epoch) for NQG and found that k = 1500 works better for all NQG models. We tune k \u2208 {1500, 3000, 5000} for CNN/DM; we found that k = 1500 works best for GOLD-\u03b4 and GOLD-p, and k = 5000 works best for GOLD-s. Note that in practice, we do not observe big gaps when using other k's in the set. For standard models, implementation is based on Cho et al. (2019). In all experiments, we evaluate once every epoch, and we do validation on the entire dev set, using task-specific metric (BLEU/ROUGE), following Cho et al. (2019) and standard practice in machine translation.\n\nHyperparameters and training details on transformer models. This paragraph corresponds to results in Table 3. For transformer models, we use Nvidia P40 GPUs (memory: 24 GB each). For NQG, CNN/DM, and XSum based on BART, we use 4 GPUs to train. For IWSLT14 De-En, we use 1 GPU. Note that fairseq defines batch size in terms 13 https://github.com/pytorch/fairseq/tree/master/examples; pretrained by corrupting the original document and optimized with respect to the reconstruction loss between the original document and the decoder output. of number of tokens instead of number of sequences. For NQG, we use 512 tokens as batch size (for each of the four GPUs); CNN/DM and XSum, we use 1024 tokens as batch size (for each of the four GPUs); for IWSLT14 De-En, we use 4096 tokens as batch size.\n\nWe use a learning rate of 2e-5 for NQG, CNN/DM, and XSum; 3e-4 for IWSLT14 De-En.\n\nRecall that as discussed in Section 3.3, the weighting polic\u1ef9 \u03c0 \u03b8 synchronizes with actual policy \u03c0 \u03b8 once every k steps so as to stabilize training. Here, k = 500 for NQG; k = 5000 for CNN/DM and XSum; k = 3000 for IWSLT14 De-En. As discussed in Section 3.3 and Section 4.1, the lower bound of p MLE is set to be 0.01 for GOLD-p and 0.1 for GOLD-s. For all other parameters that are not specific to GOLD, we use the default fairseq summarization parameters (which can be found through footnote 13).\n\nFor hyperparameter a as discussed in Section 4.1, for NQG and CNN/DM, a = 0.1; for XSum, a = 0.15; for IWSLT14 De-EN, a = 0.2.\n\nNumber of parameters in each model. For standard models, we use NQG++ for NQG, and it has 10372565 parameters. We use pointer generator for CNN/DM, and it has 19965705 parameters. For transformer models, the BART model for NQG, CNN/DM, and XSum all have 406290432 parameters; the transformer model used for IWSLT14 De-En has 39469056 parameters.\n\nAverage runtime. For standard models, based on the above models and the computing infrastructures, each epoch of NQG takes around 10 minutes to train and achieves best performance within 20 epochs. Each epoch of CNN/DM takes about 2 hours to train and achieves best performance within 15 epochs. For transformer models, each epoch of NQG takes around 5 minutes to train and achieves best dev performance within 5 epochs; each epoch of CNN/DM takes around 11 hours to train and achieves best dev performances within 5 epochs; each epoch of XSum takes around 8 hours to train; each epoch of IWSLT14 De-En takes around 3 minutes to train and achieves best performances within 100 epochs (as expected, given the large batch size 14 ). Note that our transformer models are trained on P40s; if the transformer models are trained on V100 GPUs, the training time per epoch will likely be shorter.\n\n\nA.4. Empirical Effect of Approximations\n\nOne-step approximation of importance weights. Recall that as discussed Section 3.1, we use the one-step approximation of importance weights (IW). We also experi-mented using full importance weights, using standard models on NQG. Empirically, the model performance is more sensitive to hyperparameters discussed in Appendix A.3, and training takes much longer (within 15 epochs using one-step approximation of IW, but within 25 epochs using full-prefix importance weights). However, the final test BLEU score on NQG (using GOLD-p) is 15.80, similar to 15.93 using one-step approximation.\n\n\nB. More on Results\n\n\nB.1. Lead-3 Baselines for Summarization\n\nThe lead-3 baseline (using first 3 sentences as summaries) is a popular strong baseline in summarization literature. The ROUGE-1/2/L scores of the lead-3 baselines are as follows: 40.42/17.62/36.67 for CNN/DM; 16.30/1.60/11.95 for XSum. Our performance using transformer models beat these baselines by a large margin.\n\n\nB.2. Performance with Transformer Architectures\n\nWe experiment using transformer architectures, as shown in Table 3; we also experiment on two more tasks (compared to using standard architectures): XSum and IWSLT14 De-En. We achieve state-of-the-art result (according to automatic metrics which have inherent limitations) on CNN/DM: at the time of writing, our results .81 using GOLD-s) are higher than 44.17/21.47/41.11 (PEGASUS;Zhang et al., 2020) and 44.20/21.17/41.30 (ProphetNet;Yan et al., 2020), both slightly higher than BART. Note the PEGASUS CNN/DM result is pretrained on 1.5B news articles (around 3.8 terabyte), whereas BART is pretrained on 3.3B words (around 0.16 tetrabyte). Our XSum results are also higher than PEGASUS (45.20/22.06/36.99) trained on Colossal Clean Crawled Corpus (C4; Raffel et al., 2019), but lower than the PEGASUS result using the publicly-unavailable 1.5Barticle 3.8 terabyte HugeNews (Zhang et al., 2020) as pretrained corpus.\n\nWe hypothesize that if our models are applied onto their architectures instead of pointer generator networks or BART, we would similarly get non-trivial improvements.\n\nWe also achieve 0.81 point of BLEU improvement on IWSLT14 De-En; GOLD-s performs better than the existing approaches that do not use knowledge distillation or data augmentation, as far as the authors are aware.\n\n\nB.3. More on Exposure Bias\n\nWith exposure bias. Recall that in Section 4.2, we used human evaluation (a score of 1,2,3,4) to approximate the output quality, and we found that the MLE-trained model degrades significantly when the generation length is long, whereas the quality of the GOLD-s-trained model stays relatively stable across lengths.\n\nHere, we use BLEU to approximate the quality of NQG generations, and we show that BLEU does not bias toward long sentences. Figure 3 shows the average sentence-level BLEU by sequence length. 15 Specifically, Figures 3a and 3c show the BLEU on randomly shuffled targets (from dev set), which show that longer sentences do not appear to punish BLEU scores. Figures 3b  and 3d show the BLEU by sentence length, on model generations. We see that MLE's BLEU decreases by length but GOLD-s's BLEU appears to stay relatively stable. We thus see some evidence that MLE is generating worse sentences as sentence gets longer.\n\nIf there is no exposure bias. In the main text, we used the NLL loss vs. length plot to demonstrate that without exposure bias, the loss does not vary much across length, so the MLE performance drop in Figure 2a is mainly due to exposure bias. Here, we provide another way to analyze the case without exposure bias. Figure 4 shows the token prediction accuracy conditioned on gold histories on NQG dev set. Note that for each example, we let t x = L x \u2212 5, where L x is the length of reference sentence x. We can see that without exposure bias, prediction accuracy does not vary much as the length increases. Therefore, we conclude that the big performance drop for long generations using MLE is mainly due to exposure bias and GOLD does not suffer from the problem.  The Isle of Wight father's decision not to pay a fine for taking his seven-year-old daughter on holiday during term time caused \"a huge amount of confusion\", a senior MP has said. GOLD-s The High Court ruling that a father could not be prosecuted for taking his seven-year-old daughter on a term-time holiday to Disney World caused \"a huge amount of confusion\", MPs have said. reference A High Court ruling backing a parent who refused to pay a fine for taking his child on holiday in term time will cause \"huge confusion\", an MP has said. XSum input [omitted due to length and copyright issues, but the original news article can be retrieved by searching the reference online] MLE Flood defences at a Denbighshire beach could be strengthened to reduce the risk of them being breached. GOLD-s A new dune system could be built to protect a Denbighshire beach from flooding. reference New sand dunes may be created to reduce the risk of flooding on a beach on the Denbighshire and Flintshire border. XSum input [omitted due to length and copyright issues, but the original news article can be retrieved by searching the reference online] MLE Fleetwood's League One play-off hopes suffered a blow as they were held to a goalless draw by League One strugglers Doncaster. GOLD-s Fleetwood and Blackburn played out a goalless draw in League One. reference Fleetwood Town dropped into the League One relegation places as they had to settle for a point after a stalemate with Doncaster.\n\n\nB.4. Examples\n\nIWSLT14 De-En input ich hab da so ne kognitive rckkopplung, du hast was projiziert, was du sehen mchtest. MLE i've been doing this with cognitive feedback, you've been prospecting what you want to see. GOLD-s i've got cognitive feedback, you've proved what you want to see. reference i have this cognitive feedback, you projected something you want to see. IWSLT14 De-En input es sind also alle werkzeuge vorhanden, und die einzige sache, die uns limitiert, ist unsere vorstellungskraft. MLE so there are all the tools available, and the only thing that's licensed to us is our imagination . GOLD-s so there are all the tools there, and the only thing that limited us is our imagination. reference so all the tools are out there, and the only thing that limits us is our imagination. IWSLT14 De-En input unser organismus hat eine groartige methode erfunden, um solche unangenehmen gefhle wie neid einfach zum verschwinden zu bringen. MLE our organism has invented a great way to get such uncomfortable emotions as neither of us to disappear. GOLD-s our organism invented a great way to make such uncomfortable emotions like envy easy to disappear. reference our organism has come up with an excellent method to make unpleasant feelings like envy simply disappear. Our goal is to enable high-quality generations that do not necessarily result in gold references. Given that corpus-level BLEU/ROUGE score is only a popular approximation of generation quality, we first conduct human ratings to confirm the hypothesis that our approaches are generating better sequences. For NQG, for each unit of human evaluation, we present the source paragraph, the words to ask the question on, the question generated by MLE-trained model, as well as the question generated by GOLD-s-trained model. We ask the human evaluators the general question: which generated question is better? Figure 5 shows one example interface of pairwise comparisons.\n\nUsing NQG dev set, on standard models, of the 183 pairs of comparison we conducted human evaluations on, 42 (23.0%) MLE-questions are better, 81 (44.3%) GOLD-squestions are better, and 60 (32.8%) are tied. We also evaluate on models based on BART, shown in Table 4 in the main text.\n\nFor summarization tasks, given that it is infeasible to get high-quality annotations if we let workers read the entire news article 16 , we only did the following: given the reference summary, a summary generated from MLE model, and a summary generated from our model, we asked workers to compare which generated summary is closer in meaning to the reference summary. Figure 6 shows one example interface of the mentioned pairwise comparison for summarization. See Table 4 for results.\n\n\nC.2. NQG Rating\n\nNQG rating was conducted to examine if longer sentences (generated by MLE-trained model) will result in worse human ratings, and if GOLD alleviates the problem. In Figure 2a, to reduce variance, we group length by buckets of two (e.g., [7,8], [9,10], [11,12], etc.). Furthermore, we sampled 736 annotations such that each bucket would contain at least 30 sentences (for human evaluation) for each of MLE and GOLD-s. We also shown the 95% confidence interval using standard bootstrapping, in Figure 2a.\n\nGiven the paragraph, words to query on, and the generations, we ask workers to rate the generations. Figure 7 shows an example interface of NQG human ratings. We ask workers to consider both the correctness of the generation (i.e., if the question is asking about the specified words using facts) and the quality of the generation (i.e., if the generation is fluent and coherent). We ask workers to rate from 1 to 4, 16 To obtain high-quality low-variance annotations, we may need to design QA tasks to make sure workers understood the news articles first, given the articles are usually very long.\n\nwhere 1 means very bad, 2 means slightly below average, 3 means slightly above average, and 4 means very good.   \n\nFigure 1 .\n1Histograms of token-level NLL loss using standard models on NQG and CNN/DM dev sets. Note that the undisplayed legends in (a), (c) are the same with (b), (d).\n\nFigure 2 .\n2(a) Average human ratings vs. generation length, on 736 NQG samples. (Colored regions: 95% confidence interval) Each data point includes at least 30 annotations. The quality of long generations from MLE-trained models drops heavily, but quality of GOLD-s-trained-model stays stable across lengths. (b) Average NLL loss of tth token given the gold prefix tokens vs. time-step t, on NQG dev set. Without exposure bias, NLL loss stays stable across lengths.(Holtzman et al., 2020; Welleck et al., 2020a)  including repetitions and hallucinations within a sentence generated by MLE-trained model, as shown in\n\n\nStandard architectures. For NQG, we use the model NQG++ (Zhou et al., 2017), a seq2seq-with-attention model based on GRU (Cho et al., 2014), and for summarization we use pointer generator network (See et al., 2017), a seq2seq-with-attention model based on LSTM (Hochreiter & Schmidhuber, 1997). Specifically, we use 2 layers for both the encoder and the decoder, for both tasks. Other hyperparameters are based on the following implementation: https://github.com/clovaai/ FocusSeq2Seq.\n\nFigure 5 .\n5Interface for NQG pairwise comparisons, using Amazon Mechanical Turk.\n\nFigure 6 .\n6Interface for summarization pairwise comparisons, using Amazon Mechanical Turk.\n\nFigure 7 .\n7Interface for NQG human ratings, using Amazon Mechanical Turk.\n\n\nsuch that training examples with higher probability under the current model are weighted arXiv:2009.07839v1 [cs.CL] 16 Sep 2020\n\n\nTable 1. BLEU/ROUGE (\u2191) and perplexity (\u2193) using nonpretrained standard models on test sets. Best results are bolded. GOLD achieves better metric scores despite high held-out perplexity. Refer toTable 3for results using transformer models.Table 2. Dev set results of standard models using different decoding algorithms. b: beam size. We report the average of 3 runs for top-k sampling. Models trained by GOLD are less sensitive to decoding algorithms.See Appendix A.3 \nfor model and training details. \n\nFor standard models, we chose two representative seq2seq-\nbased models, NQG++ (Zhou et al., 2017) and the pointer-\ngenerator model (See et al., 2017) for NQG and CNN/DM \n\nNQG \nCNN/DM \n(NQG++ net) \n(pointer generator network) \nBLEU \u2191 ppl \u2193 R-1 \u2191 R-2 \u2191 R-L \u2191 ppl \u2193 \nMLE \n14.23 29.25 39.00 17.10 36.07 20.11 \nGOLD-\u03b4 14.96 110.58 39.02 17.16 35.98 133.10 \nGOLD-p 15.93 148.84 39.20 17.31 36.23 143.58 \nGOLD-s 16.10 158.45 39.95 17.81 36.81 29.80 \n\nNQG \nCNN/DM \n(BLEU) \n(ROUGE-2) \nMLE GOLD-s MLE GOLD-s \n\ngreedy \n14.13 16.06 \n17.40 18.51 \nbeam search (b = 3) \n14.19 15.84 \n17.65 18.44 \nbeam search (b = 5) \n14.07 15.74 \n17.63 18.25 \ntop-k sampling (k = 5) 11.27 15.41 \n13.06 17.02 \ntop-k sampling (k = 20) 10.08 15.38 \n11.23 16.57 \n\n\n\nTable 5 .\n5NQG generations using standard models. Words to query on are bolded. Long generations from MLE-trained model often result in repetition or hallucination. More examples across different tasks using transformer models can be found in the appendix.NQG \nCNN/DM \nBLEU \nR-1 R-2 R-L \nMLE \n14.23 \n39.00 17.10 36.07 \nMLE +on-policy \n14.55 \n39.17 17.35 36.13 \nGOLD-s \n16.10 \n39.95 17.81 36.81 \nGOLD-s +on-policy 16.38 \n39.91 18.02 37.01 \n\nTable 6. BLEU/ROUGE (\u2191) on test sets, using standard mod-\nels finetuned with on-policy objectives. On-policy objectives \nmarginally improve upon both MLE and GOLD baselines. \n\n\n\n\n; Stahlberg & Byrne (2019) have shown that maximizing pMLE during decoding leads to empty generations. ing implausible samples during training, e.g., using negative sampling(Welleck et al., 2020b), self-training on high-quality samples(Kedzie & McKeown, 2019), and confidence-oriented decoding with calibration\n\n\nBengio, S.,Vinyals, O., Jaitly, N., and Shazeer, N. Scheduled  sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1171-1179, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D19-1308. URL https: //www.aclweb.org/anthology/D19-1308.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998-6008, 2017.Caccia, M., Caccia, L., Fedus, W., Larochelle, H., Pineau, \nJ., and Charlin, L. Language gans falling short. In \nInternational Conference on Learning Representations, \n2020. URL https://openreview.net/forum? \nid=BJgza6VtPB. \n\nCettolo, M., Niehues, J., St\u00fcker, S., Bentivogli, L., and Fed-\nerico, M. Report on the 11th IWSLT evaluation campaign. \nIn Proceedings of the International Workshop on Spoken \nLanguage Translation, volume 57, Hanoi, Vietnam, 2014. \n\nCho, J., Seo, M., and Hajishirzi, H. Mixture content se-\nlection for diverse sequence generation. In Proceed-\nings of the 2019 Conference on Empirical Methods \nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing \n(EMNLP-IJCNLP), pp. 3121-3131, Hong Kong, China, \nNovember 2019. Cho, K., van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, \nD., Bougares, F., Schwenk, H., and Bengio, Y. Learn-\ning phrase representations using RNN encoder-decoder \nfor statistical machine translation. In Proceedings of \nthe 2014 Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP), pp. 1724-1734, Doha, \nQatar, October 2014. Association for Computational Lin-\nguistics. doi: 10.3115/v1/D14-1179. URL https: \n//www.aclweb.org/anthology/D14-1179. \n\nChoshen, L., Fox, L., Aizenbud, Z., and Abend, O. On the \nweaknesses of reinforcement learning for neural machine \ntranslation. In International Conference on Learning \nRepresentations, 2020. URL https://openreview. \nnet/forum?id=H1eCw3EKvH. \n\nClark, E., Celikyilmaz, A., and Smith, N. A. Sentence \nmover's similarity: Automatic evaluation for multi-\nsentence texts. In Proceedings of the 57th Annual \nMeeting of the Association for Computational Linguis-\ntics, pp. 2748-2760, Florence, Italy, July 2019. Asso-\nciation for Computational Linguistics. doi: 10.18653/ \nv1/P19-1264. URL https://www.aclweb.org/ \nanthology/P19-1264. \n\nDaum\u00e9, H., Langford, J., and Marcu, D. Search-based \nstructured prediction. Machine learning, 75(3):297-325, \n2009. \n\nGoyal, K., Neubig, G., Dyer, C., and Berg-Kirkpatrick, T. \nA continuous relaxation of beam search for end-to-end \n\ntraining of neural sequence models. In Thirty-Second \nAAAI Conference on Artificial Intelligence, 2018. \n\nHashimoto, T., Zhang, H., and Liang, P. Unifying human \nand statistical evaluation for natural language genera-\ntion. In Proceedings of the 2019 Conference of the North \nAmerican Chapter of the Association for Computational \nLinguistics: Human Language Technologies, Volume 1 \n(Long and Short Papers), pp. 1689-1701, Minneapolis, \nMinnesota, June 2019. Association for Computational \nLinguistics. doi: 10.18653/v1/N19-1169. URL https: \n//www.aclweb.org/anthology/N19-1169. \n\nHastings, W. K. Monte carlo sampling methods using \nmarkov chains and their applications. Biometrika, 57 \n(1):97-109, 1970. ISSN 00063444. URL http://www. \njstor.org/stable/2334940. \n\nHermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, \nL., Kay, W., Suleyman, M., and Blunsom, P. Teaching \nmachines to read and comprehend. In Advances in Neural \nInformation Processing Systems, pp. 1693-1701, 2015. \n\nHochreiter, S. and Schmidhuber, J. Long short-term mem-\nory. Neural Comput., 9(8):17351780, November 1997. \nISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. \nURL https://doi.org/10.1162/neco.1997. \n9.8.1735. \n\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The \ncurious case of neural text degeneration. In International \nConference on Learning Representations, 2020. \n\nHusz\u00e1r, F. How (not) to train your generative model: Sched-\nuled sampling, likelihood, adversary? arXiv preprint \narXiv:1511.05101, 2015. \n\nJaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, \nC., Lapedriza, A., Jones, N., Gu, S., and Picard, R. \nWay off-policy batch deep reinforcement learning of \nimplicit human preferences in dialog. arXiv preprint \narXiv:1907.00456, 2019. \n\nKang, D. and Hashimoto, T. Improved natural lan-\nguage generation via loss truncation. arXiv preprint \narXiv:2004.14589, 2020. \n\nKedzie, C. and McKeown, K. A good sample is hard to \nfind: Noise injection sampling and self-training for neural \nlanguage generation models. In Proceedings of the 12th \nInternational Conference on Natural Language Genera-\ntion, pp. 584-593, 2019. \n\nKeneshloo, Y., Shi, T., Ramakrishnan, N., and Reddy, C. K. \nDeep reinforcement learning for sequence-to-sequence \nmodels. IEEE Transactions on Neural Networks and \nLearning Systems, 2019. \nProceedings, 2016. URL http://arxiv.org/abs/ \n1511.06732. \n\nRoss, S., Gordon, G., and Bagnell, D. A reduction of imita-\ntion learning and structured prediction to no-regret online \nlearning. In Proceedings of the Fourteenth International \nConference on Artificial Intelligence and Statistics, pp. \n627-635, 2011. \n\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and \nMoritz, P. Trust region policy optimization. In \nBach, F. and Blei, D. (eds.), Proceedings of the \n32nd International Conference on Machine Learn-\ning, volume 37 of Proceedings of Machine Learning \nResearch, pp. 1889-1897, Lille, France, 07-09 Jul \n2015. PMLR. URL http://proceedings.mlr. \npress/v37/schulman15.html. \n\nSee, A., Liu, P. J., and Manning, C. D. Get to the point: \nSummarization with pointer-generator networks. In Pro-\nceedings of the 55th Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Papers), \npp. 1073-1083, Vancouver, Canada, July 2017. Asso-\nciation for Computational Linguistics. doi: 10.18653/ \nv1/P17-1099. URL https://www.aclweb.org/ \nanthology/P17-1099. \n\nSerban, I. V., Sankar, C., Germain, M., Zhang, S., Lin, Z., \nSubramanian, S., Kim, T., Pieper, M., Chandar, S., Ke, \nN. R., et al. A deep reinforcement learning chatbot. arXiv \npreprint arXiv:1709.02349, 2017. \n\nShen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., \nand Liu, Y. Minimum risk training for neural machine \ntranslation. arXiv preprint arXiv:1512.02433, 2015. \n\nSimon, L., Webster, R., and Rabin, J. Revisiting preci-\nsion recall definition for generative modeling. In Chaud-\nhuri, K. and Salakhutdinov, R. (eds.), Proceedings of \nthe 36th International Conference on Machine Learn-\ning, volume 97 of Proceedings of Machine Learning Re-\nsearch, pp. 5799-5808, Long Beach, California, USA, 09-\n15 Jun 2019. PMLR. URL http://proceedings. \nmlr.press/v97/simon19a.html. \n\nStahlberg, F. and Byrne, B. On NMT search errors and \nmodel errors: Cat got your tongue? In Proceedings \nof the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint \nConference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 3356-3362, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics. \ndoi: 10.18653/v1/D19-1331. URL https://www. \naclweb.org/anthology/D19-1331. \n\nSutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, \nY. Policy gradient methods for reinforcement learning \n\nwith function approximation. In Advances in neural in-\nformation processing systems, pp. 1057-1063, 2000. \n\nTevet, G., Habib, G., Shwartz, V., and Berant, J. Eval-\nuating text GANs as language models. In Proceed-\nings of the 2019 Conference of the North American \nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long \nand Short Papers), pp. 2241-2247, Minneapolis, Min-\nnesota, June 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/N19-1233. URL https: \n//www.aclweb.org/anthology/N19-1233. \n\nTian, R., Narayan, S., Sellam, T., and Parikh, A. P. Stick-\ning to the facts: Confident decoding for faithful data-to-\ntext generation, 2020. URL https://openreview. \nnet/forum?id=HkxU2pNYPH. \n\nTu, L., Pang, R. Y., and Gimpel, K. Improving joint training \nof inference networks and structured prediction energy \nnetworks. arXiv preprint arXiv:1911.02891, 2019. \n\nWang, C. and Sennrich, R. On exposure bias, hallucination \nand domain shift in neural machine translation. In Pro-\nceedings of the 58th Annual Meeting of the Association \nfor Computational Linguistics. Association for Computa-\ntional Linguistics, 2020. \n\nWelleck, S., Kulikov, I., Kim, J., Pang, R. Y., and Cho, K. \nConsistency of a recurrent language model with respect to \nincomplete decoding. arXiv preprint arXiv:2002.02492, \n2020a. \n\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., \nand Weston, J. Neural text generation with unlikelihood \ntraining. In International Conference on Learning Rep-\nresentations, 2020b. URL https://openreview. \nnet/forum?id=SJeYe0NtvH. \n\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine \nlearning, 8(3-4):229-256, 1992. \n\nWiseman, S. and Rush, A. M. Sequence-to-sequence learn-\ning as beam-search optimization. In Proceedings of \nthe 2016 Conference on Empirical Methods in Natural \nLanguage Processing, pp. 1296-1306, Austin, Texas, \nNovember 2016. Association for Computational Lin-\nguistics. doi: 10.18653/v1/D16-1137. URL https: \n//www.aclweb.org/anthology/D16-1137. \nWu, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y. A study \nof reinforcement learning for neural machine transla-\ntion. In Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pp. 3612-\n3621, Brussels, Belgium, October-November 2018. As-\nsociation for Computational Linguistics. doi: 10.18653/ \nv1/D18-1397. URL https://www.aclweb.org/ \nanthology/D18-1397. \n\nYan, Y., Qi, W., Gong, Y., Liu, D., Duan, N., Chen, J., \nZhang, R., and Zhou, M. Prophetnet: Predicting fu-\nture n-gram for sequence-to-sequence pre-training. arXiv \npreprint arXiv:2001.04063, 2020. \n\nYu, L., Zhang, W., Wang, J., and Yu, Y. Seqgan: Sequence \ngenerative adversarial nets with policy gradient. In Thirty-\nFirst AAAI Conference on Artificial Intelligence, 2017. \n\nZhang, J., Zhao, Y., Saleh, M., and Liu, P. J. Pegasus: \nPre-training with extracted gap-sentences for abstractive \nsummarization. In Proceedings of the 37th International \nConference on Machine Learning, Proceedings of Ma-\nchine Learning Research. PMLR, 2020. \n\nZhou, Q., Yang, N., Wei, F., Tan, C., Bao, H., and Zhou, \nM. Neural question generation from text: A preliminary \nstudy. In National CCF Conference on Natural Lan-\nguage Processing and Chinese Computing, pp. 662-671. \nSpringer, 2017. \n\n\nTable 7 and\n7Table 8shows the example generations based on the transformer models.Figure 3. Exposure bias related figures on NQG dev set. Vertical axis: avg unsmoothed sentence-level BLEU. Horizontal axis: sentence length. The colored regions represent 95% confidence interval obtained using standard bootstrapping. Subfigures (a) and (c) show BLEU on randomly shuffled targets (from dev set); BLEU does not appear to punish long sentences. Subfigures (b) and (d) show BLEU vs. generation length; BLEU on generations from MLE-trained model decreases by length, but BLEU on generations from GOLD-trained model appears to stay relatively stable.Figure 4. Accuracy of correct predictions of tth token given all prefix reference tokens on NQG dev set. Colored regions represents 95% confidence interval obtained using standard bootstrapping. Without exposure bias, token prediction accuracy stays relatively stable across lengths.NQGinput Some members of this community emigrated to the United States in the 1890s . MLE when did some members of the portuguese-american community emigrate to the us ? GOLD-s when did some members of the community emigrate to the us ? reference in what era did some members of this community emigrate to the us ? NQG input Competition amongst workers tends to drive down wages due to the expendable nature of the worker in relation to his or her particular job . MLE what is one of the reasons that causes wages to be lower ? GOLD-s why do wages go down when there is competition amongst workers ? reference why does competition among workers drive down wages ? NQG input During the mid-eocene , it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch . MLE when was the purus arch formed ? GOLD-s when was the drainage basin of the amazon split ? reference in which point did the drainage basin of the amazon split ?CNN/DM input [omitted due to length and copyright issues, but the original news article can be retrieved by searching the reference online] MLE There are nearly 5,000 \"gems\" scattered across the country, ranging from museums to archaeological areas and monuments. Italy boasts the highest number of UNESCO World Heritage sites in the world. Several of which risk crumbling to the ground due to neglect and lack of public resources. GOLD-s Italy boasts the highest number of UNESCO World Heritage sites in the world . The Basilica of Assisi, where St.Frances' tomb lies, is badly in need of a restyle . Italy doesn't know how to exploit this treasure, says Francesco Totti . reference Italy boasts the highest number of UNESCO World Heritage sites in the world . Italy doesn't know how to exploit treasures, and appears not to care about them, writes Silvia Marchetti . CNN/DM input [omitted due to length and copyright issues, but the original news article can be retrieved by searching the reference online] MLE President Obama has argued with progressive potentate Elizabeth Warren, calling her \"wrong\" on trade policy. What everyone does next will be critical for the 2016 elections and the future of Democratic politics. Warren has publicly criticized \"fast track\" trade authority that would allow the White House to negotiate massive, multination trade deals. GOLD-s President Obama has argued with the progressive potentate Elizabeth Warren, calling her \"wrong\" on trade policy .Julian Zelizer: If Hillary Clinton wants to prove she's a real populist, now is her chance to be even more clear about her position on the TPP deal . reference Sen. Elizabeth Warren has publicly criticized so-called \"fast track\" trade authority . Sally Kohn: Why does President Obama call her wrong, and why is Hillary Clinton equivocating?10 \n15 \n20 \n\nsequence length \n\n0.01 \n\n0.02 \n\n0.03 \n\navg sent-level BLEU \n\nMLE \nGOLD-s \n\n(a) BLEU-2 on random targets \n\n5 \n10 \n15 \n20 \n\nsequence length \n\n0.20 \n\n0.25 \n\navg sent-level BLEU \n\nMLE \nGOLD-s \n\n(b) BLEU-2 on generations \n\n5 \n10 \n15 \n20 \n\nsequence length \n\n0.000 \n\n0.005 \n\n0.010 \n\navg sent-level BLEU \n\nMLE \nGOLD-s \n\n(c) BLEU-3 on random targets \n\n5 \n10 \n15 \n20 \n\nsequence length \n\n0.100 \n\n0.125 \n\n0.150 \n\navg sent-level BLEU \n\nMLE \nGOLD-s \n\n(d) BLEU-3 on generations \n\n\n\nTable 7 .\n7NQG and CNN/DM examples based on transformer models. For NQG, words to query on are bolded. XSum input [omitted due to length and copyright issues, but the original news article can be retrieved by searching the reference online] MLEText Generation by Learning from Off-Policy Demonstrations \n\ntask \nobjective example \n\n\n\nTable 8 .\n8XSum and IWSLT14 De-En examples based on transformer models. Text Generation by Learning from Off-Policy Demonstrations C. Human Evaluations C.1. Pairwise Comparison\nTheoretically, the approximation is a lower bound of J(\u03b8) provided that the total variation distance between \u03c0 b and \u03c0 \u03b8 is small(Schulman et al., 2015).3  We also use D to denote the set of trajectories derived from the training examples.\nNote that 1 + \u03b3 + . . . + \u03b3 T \u2248 1 1\u2212\u03b3 = 5 when \u03b3 = 0.8.\nThe pairwise comparison task has a \"tie\" choice, resulting in three possible options.\nWe use 4096 tokens (which corresponds to hundreds of sentences) as batch size for IWSLT14 De-En.\nWe use BLEU-2/3 given that without smoothing, sentencelevel BLEU-4 results in large variance.\nAcknowledgementsThe authors thank Alfredo Canziani, Graham Neubig, Ethan Perez, Clara Vania, Alex Warstadt (alphabetical order) for helpful discussions.\nSplit and rephrase: Better evaluation and stronger baselines. R Aharoni, Y Goldberg, doi: 10.18653/ v1/P18-2114Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia2Association for Computational LinguisticsAharoni, R. and Goldberg, Y. Split and rephrase: Better evaluation and stronger baselines. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), pp. 719-724, Melbourne, Australia, July 2018. Associa- tion for Computational Linguistics. doi: 10.18653/ v1/P18-2114. URL https://www.aclweb.org/ anthology/P18-2114.\n\nD Andor, C Alberti, D Weiss, A Severyn, A Presta, K Ganchev, S Petrov, Collins , M , arXiv:1603.06042Globally normalized transition-based neural networks. arXiv preprintAndor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S., and Collins, M. Globally nor- malized transition-based neural networks. arXiv preprint arXiv:1603.06042, 2016.\n\nStructured prediction energy networks. D Belanger, A Mccallum, Proceedings of The 33rd International Conference on Machine Learning. Balcan, M. F. and Weinberger, K. Q.The 33rd International Conference on Machine LearningNew York, New York, USA48Belanger, D. and McCallum, A. Structured predic- tion energy networks. In Balcan, M. F. and Wein- berger, K. Q. (eds.), Proceedings of The 33rd Interna- tional Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 983-992, New York, New York, USA, 20-22 Jun 2016. PMLR. URL http://proceedings.mlr. press/v48/belanger16.html.\n\nTraining RNNs with global-local losses. R Leblond, J.-B Alayrac, A Osokin, S Lacoste-Julien, Searnn, International Conference on Learning Representations. Leblond, R., Alayrac, J.-B., Osokin, A., and Lacoste-Julien, S. SEARNN: Training RNNs with global-local losses. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=HkUR_y-RZ.\n\nOffline reinforcement learning: Tutorial, review, and perspectives on open problems. S Levine, A Kumar, G Tucker, J Fu, arXiv:2005.01643arXiv preprintLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline rein- forcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nDenoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Bart, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsSeattle, USAAssociation for Computational LinguisticsLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Seattle, USA, July 2020. Association for Computational Linguistics.\n\nAdversarial learning for neural dialogue generation. J Li, W Monroe, T Shi, S Jean, A Ritter, Jurafsky , D , arXiv:1701.06547arXiv preprintLi, J., Monroe, W., Shi, T., Jean, S., Ritter, A., and Jurafsky, D. Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547, 2017.\n\nParaphrase generation with deep reinforcement learning. Z Li, X Jiang, L Shang, Li , H , 10.18653/v1/D18-1421Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsLi, Z., Jiang, X., Shang, L., and Li, H. Paraphrase generation with deep reinforcement learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3865-3878, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1421. URL https: //www.aclweb.org/anthology/D18-1421.\n\nROUGE: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsLin, C.-Y. ROUGE: A package for automatic evalua- tion of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Asso- ciation for Computational Linguistics. URL https: //www.aclweb.org/anthology/W04-1013.\n\nCoT: Cooperative training for generative modeling of discrete data. S Lu, L Yu, S Feng, Y Zhu, W Zhang, Proceedings of the 36th International Conference on Machine Learning. Chaudhuri, K. and Salakhutdinov, R.the 36th International Conference on Machine LearningLong Beach, California, USA97Lu, S., Yu, L., Feng, S., Zhu, Y., and Zhang, W. CoT: Cooperative training for generative modeling of discrete data. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Ma- chine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 4164-4172, Long Beach, Cali- fornia, USA, 09-15 Jun 2019. PMLR. URL http:// proceedings.mlr.press/v97/lu19d.html.\n\nCorrecting length bias in neural machine translation. K Murray, D Chiang, doi: 10.18653/ v1/W18-6322Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBelgium, BrusselsAssociation for Computational LinguisticsMurray, K. and Chiang, D. Correcting length bias in neu- ral machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 212-223, Belgium, Brussels, October 2018. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/W18-6322. URL https://www.aclweb.org/ anthology/W18-6322.\n\nDon't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. S Narayan, S B Cohen, M Lapata, 10.18653/v1/D18-1206Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsNarayan, S., Cohen, S. B., and Lapata, M. Don't give me the details, just the summary! topic-aware convolutional neu- ral networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1797-1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https: //www.aclweb.org/anthology/D18-1206.\n\nWhy we need new evaluation metrics for NLG. J Novikova, O Du\u0161ek, A Cercas Curry, V Rieser, doi: 10.18653/ v1/D17-1238Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsNovikova, J., Du\u0161ek, O., Cercas Curry, A., and Rieser, V. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2241- 2252, Copenhagen, Denmark, September 2017. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/D17-1238. URL https://www.aclweb.org/ anthology/D17-1238.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, doi: 10.3115/ 1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine transla- tion. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311- 318, Philadelphia, Pennsylvania, USA, July 2002. Asso- ciation for Computational Linguistics. doi: 10.3115/ 1073083.1073135. URL https://www.aclweb. org/anthology/P02-1040.\n\nMulti-reward reinforced summarization with saliency and entailment. R Pasunuru, M Bansal, 10.18653/v1/N18-2102Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersPasunuru, R. and Bansal, M. Multi-reward reinforced summarization with saliency and entailment. In Pro- ceedings of the 2018 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 2 (Short Papers), pp. 646-653, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2102. URL https://www. aclweb.org/anthology/N18-2102.\n\nEligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series. D Precup, R S Sutton, S Singh, UMass AmherstPrecup, D., Sutton, R. S., and Singh, S. Eligibility traces for off-policy policy evaluation. Computer Science De- partment Faculty Publication Series, UMass Amherst, 2000.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, arXiv:1910.10683arXiv preprintRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n\nSQuAD: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, doi: 10.18653/ v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehen- sion of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392, Austin, Texas, November 2016. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/D16-1264. URL https://www.aclweb.org/ anthology/D16-1264.\n\nSequence level training with recurrent neural networks. M Ranzato, S Chopra, M Auli, W Zaremba, International Conference on Learning Representations. San Juan, Puerto RicoRanzato, M., Chopra, S., Auli, M., and Zaremba, W. Se- quence level training with recurrent neural networks. In International Conference on Learning Representations, San Juan, Puerto Rico, May 2-4, 2016, Conference Track\n", "annotations": {"author": "[{\"end\":83,\"start\":62},{\"end\":90,\"start\":84}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":78},{\"end\":89,\"start\":87}]", "author_first_name": "[{\"end\":69,\"start\":62},{\"end\":77,\"start\":70},{\"end\":86,\"start\":84}]", "author_affiliation": null, "title": "[{\"end\":59,\"start\":1},{\"end\":149,\"start\":91}]", "venue": null, "abstract": "[{\"end\":1424,\"start\":151}]", "bib_ref": "[{\"end\":2282,\"start\":2268},{\"end\":2301,\"start\":2282},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2788,\"start\":2766},{\"end\":2808,\"start\":2788},{\"end\":2995,\"start\":2976},{\"end\":3131,\"start\":3108},{\"end\":3153,\"start\":3131},{\"end\":3714,\"start\":3697},{\"end\":3735,\"start\":3714},{\"end\":4408,\"start\":4392},{\"end\":4733,\"start\":4711},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4788,\"start\":4766},{\"end\":4815,\"start\":4810},{\"end\":4833,\"start\":4815},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5139,\"start\":5119},{\"end\":6294,\"start\":6280},{\"end\":6317,\"start\":6294},{\"end\":6700,\"start\":6699},{\"end\":8846,\"start\":8830},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8866,\"start\":8846},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9483,\"start\":9466},{\"end\":9506,\"start\":9483},{\"end\":10265,\"start\":10244},{\"end\":10285,\"start\":10265},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10703,\"start\":10680},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10728,\"start\":10703},{\"end\":10747,\"start\":10728},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11342,\"start\":11321},{\"end\":12232,\"start\":12211},{\"end\":12255,\"start\":12232},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13975,\"start\":13958},{\"end\":14065,\"start\":14045},{\"end\":14085,\"start\":14065},{\"end\":17674,\"start\":17673},{\"end\":18356,\"start\":18335},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18816,\"start\":18793},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18844,\"start\":18833},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20571,\"start\":20551},{\"end\":21279,\"start\":21278},{\"end\":21375,\"start\":21374},{\"end\":25929,\"start\":25927},{\"end\":26254,\"start\":26252},{\"end\":26798,\"start\":26796},{\"end\":28479,\"start\":28457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28498,\"start\":28479},{\"end\":28517,\"start\":28498},{\"end\":28565,\"start\":28537},{\"end\":28581,\"start\":28565},{\"end\":28739,\"start\":28719},{\"end\":28757,\"start\":28739},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28906,\"start\":28884},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29196,\"start\":29174},{\"end\":29214,\"start\":29196},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29238,\"start\":29214},{\"end\":29365,\"start\":29344},{\"end\":29385,\"start\":29365},{\"end\":29668,\"start\":29651},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29684,\"start\":29668},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29700,\"start\":29684},{\"end\":30592,\"start\":30573},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31661,\"start\":31637},{\"end\":31976,\"start\":31959},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32179,\"start\":32158},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33297,\"start\":33277},{\"end\":34911,\"start\":34894},{\"end\":39308,\"start\":39281},{\"end\":39362,\"start\":39308},{\"end\":39379,\"start\":39362},{\"end\":39633,\"start\":39607},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39701,\"start\":39681},{\"end\":39822,\"start\":39802},{\"end\":40765,\"start\":40763},{\"end\":46413,\"start\":46410},{\"end\":46415,\"start\":46413},{\"end\":46420,\"start\":46417},{\"end\":46423,\"start\":46420},{\"end\":46429,\"start\":46425},{\"end\":46432,\"start\":46429},{\"end\":47096,\"start\":47094}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":47562,\"start\":47391},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48180,\"start\":47563},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48668,\"start\":48181},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48751,\"start\":48669},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48844,\"start\":48752},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48920,\"start\":48845},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":49050,\"start\":48921},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":50284,\"start\":49051},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50902,\"start\":50285},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51215,\"start\":50903},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":61998,\"start\":51216},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":66207,\"start\":61999},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66540,\"start\":66208},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":66718,\"start\":66541}]", "paragraph": "[{\"end\":1714,\"start\":1440},{\"end\":2027,\"start\":1716},{\"end\":2504,\"start\":2041},{\"end\":3154,\"start\":2506},{\"end\":3736,\"start\":3156},{\"end\":4620,\"start\":3738},{\"end\":5362,\"start\":4622},{\"end\":5877,\"start\":5391},{\"end\":6088,\"start\":5879},{\"end\":6512,\"start\":6090},{\"end\":6517,\"start\":6514},{\"end\":6761,\"start\":6519},{\"end\":7086,\"start\":6763},{\"end\":7394,\"start\":7088},{\"end\":7623,\"start\":7396},{\"end\":8162,\"start\":7675},{\"end\":8487,\"start\":8198},{\"end\":8748,\"start\":8529},{\"end\":8970,\"start\":8750},{\"end\":9031,\"start\":9025},{\"end\":9775,\"start\":9065},{\"end\":10286,\"start\":9777},{\"end\":10817,\"start\":10288},{\"end\":11343,\"start\":10819},{\"end\":11422,\"start\":11399},{\"end\":12333,\"start\":11472},{\"end\":12626,\"start\":12367},{\"end\":12673,\"start\":12628},{\"end\":12885,\"start\":12767},{\"end\":13372,\"start\":12942},{\"end\":13624,\"start\":13383},{\"end\":14406,\"start\":13626},{\"end\":14656,\"start\":14408},{\"end\":14933,\"start\":14733},{\"end\":15784,\"start\":14935},{\"end\":15959,\"start\":15786},{\"end\":16159,\"start\":15999},{\"end\":16385,\"start\":16161},{\"end\":16473,\"start\":16387},{\"end\":16704,\"start\":16509},{\"end\":16910,\"start\":16727},{\"end\":17389,\"start\":16912},{\"end\":18010,\"start\":17391},{\"end\":18438,\"start\":18034},{\"end\":18696,\"start\":18630},{\"end\":18884,\"start\":18719},{\"end\":19254,\"start\":18886},{\"end\":19761,\"start\":19256},{\"end\":20474,\"start\":19786},{\"end\":20774,\"start\":20476},{\"end\":23016,\"start\":20776},{\"end\":23249,\"start\":23200},{\"end\":23572,\"start\":23251},{\"end\":23751,\"start\":23574},{\"end\":24299,\"start\":23753},{\"end\":24839,\"start\":24301},{\"end\":25310,\"start\":24841},{\"end\":26162,\"start\":25312},{\"end\":26542,\"start\":26164},{\"end\":26912,\"start\":26581},{\"end\":27201,\"start\":26914},{\"end\":27657,\"start\":27203},{\"end\":28193,\"start\":27659},{\"end\":29035,\"start\":28210},{\"end\":29512,\"start\":29037},{\"end\":29972,\"start\":29514},{\"end\":30707,\"start\":29974},{\"end\":31482,\"start\":30722},{\"end\":31785,\"start\":31550},{\"end\":32123,\"start\":31787},{\"end\":32593,\"start\":32125},{\"end\":33077,\"start\":32595},{\"end\":33728,\"start\":33106},{\"end\":33791,\"start\":33761},{\"end\":34076,\"start\":33793},{\"end\":34343,\"start\":34078},{\"end\":35121,\"start\":34345},{\"end\":35914,\"start\":35123},{\"end\":35997,\"start\":35916},{\"end\":36498,\"start\":35999},{\"end\":36626,\"start\":36500},{\"end\":36973,\"start\":36628},{\"end\":37863,\"start\":36975},{\"end\":38493,\"start\":37907},{\"end\":38875,\"start\":38558},{\"end\":39844,\"start\":38927},{\"end\":40012,\"start\":39846},{\"end\":40224,\"start\":40014},{\"end\":40570,\"start\":40255},{\"end\":41187,\"start\":40572},{\"end\":43435,\"start\":41189},{\"end\":45383,\"start\":43453},{\"end\":45667,\"start\":45385},{\"end\":46154,\"start\":45669},{\"end\":46675,\"start\":46174},{\"end\":47275,\"start\":46677},{\"end\":47390,\"start\":47277}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7674,\"start\":7624},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8197,\"start\":8163},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9024,\"start\":8971},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9064,\"start\":9032},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11398,\"start\":11344},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11471,\"start\":11423},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12366,\"start\":12334},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12766,\"start\":12674},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12941,\"start\":12886},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14732,\"start\":14657},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15998,\"start\":15960},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16508,\"start\":16474},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18629,\"start\":18439},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18718,\"start\":18697},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23169,\"start\":23017},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23199,\"start\":23169}]", "table_ref": "[{\"end\":20184,\"start\":20177},{\"end\":20654,\"start\":20647},{\"end\":20903,\"start\":20896},{\"end\":21623,\"start\":21616},{\"end\":21774,\"start\":21767},{\"end\":22920,\"start\":22913},{\"end\":23330,\"start\":23323},{\"end\":23342,\"start\":23335},{\"end\":24628,\"start\":24621},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26161,\"start\":26154},{\"end\":27990,\"start\":27983},{\"end\":35231,\"start\":35224},{\"end\":38993,\"start\":38986},{\"end\":45649,\"start\":45642},{\"end\":46141,\"start\":46134}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1438,\"start\":1426},{\"end\":2039,\"start\":2030},{\"attributes\":{\"n\":\"2.\"},\"end\":5389,\"start\":5365},{\"attributes\":{\"n\":\"3.\"},\"end\":8498,\"start\":8490},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8527,\"start\":8501},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13381,\"start\":13375},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16725,\"start\":16707},{\"attributes\":{\"n\":\"4.\"},\"end\":18024,\"start\":18013},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18032,\"start\":18027},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19784,\"start\":19764},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26579,\"start\":26545},{\"attributes\":{\"n\":\"5.\"},\"end\":28208,\"start\":28196},{\"attributes\":{\"n\":\"6.\"},\"end\":30720,\"start\":30710},{\"end\":31522,\"start\":31485},{\"end\":31548,\"start\":31525},{\"end\":33104,\"start\":33080},{\"end\":33759,\"start\":33731},{\"end\":37905,\"start\":37866},{\"end\":38514,\"start\":38496},{\"end\":38556,\"start\":38517},{\"end\":38925,\"start\":38878},{\"end\":40253,\"start\":40227},{\"end\":43451,\"start\":43438},{\"end\":46172,\"start\":46157},{\"end\":47402,\"start\":47392},{\"end\":47574,\"start\":47564},{\"end\":48680,\"start\":48670},{\"end\":48763,\"start\":48753},{\"end\":48856,\"start\":48846},{\"end\":50295,\"start\":50286},{\"end\":62011,\"start\":62000},{\"end\":66218,\"start\":66209},{\"end\":66551,\"start\":66542}]", "table": "[{\"end\":50284,\"start\":49504},{\"end\":50902,\"start\":50542},{\"end\":61998,\"start\":51748},{\"end\":66207,\"start\":65729},{\"end\":66540,\"start\":66453}]", "figure_caption": "[{\"end\":47562,\"start\":47404},{\"end\":48180,\"start\":47576},{\"end\":48668,\"start\":48183},{\"end\":48751,\"start\":48682},{\"end\":48844,\"start\":48765},{\"end\":48920,\"start\":48858},{\"end\":49050,\"start\":48923},{\"end\":49504,\"start\":49053},{\"end\":50542,\"start\":50297},{\"end\":51215,\"start\":50905},{\"end\":51748,\"start\":51218},{\"end\":65729,\"start\":62013},{\"end\":66453,\"start\":66220},{\"end\":66718,\"start\":66553}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22343,\"start\":22335},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22795,\"start\":22787},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23750,\"start\":23742},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23865,\"start\":23846},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23918,\"start\":23899},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25619,\"start\":25610},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26186,\"start\":26177},{\"end\":40704,\"start\":40696},{\"end\":40797,\"start\":40780},{\"end\":40945,\"start\":40927},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41400,\"start\":41391},{\"end\":41513,\"start\":41505},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":45330,\"start\":45322},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":46045,\"start\":46037},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46347,\"start\":46338},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46674,\"start\":46665},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":46786,\"start\":46778}]", "bib_author_first_name": "[{\"end\":67508,\"start\":67507},{\"end\":67519,\"start\":67518},{\"end\":68158,\"start\":68157},{\"end\":68167,\"start\":68166},{\"end\":68178,\"start\":68177},{\"end\":68187,\"start\":68186},{\"end\":68198,\"start\":68197},{\"end\":68208,\"start\":68207},{\"end\":68219,\"start\":68218},{\"end\":68235,\"start\":68228},{\"end\":68239,\"start\":68238},{\"end\":68563,\"start\":68562},{\"end\":68575,\"start\":68574},{\"end\":69173,\"start\":69172},{\"end\":69187,\"start\":69183},{\"end\":69198,\"start\":69197},{\"end\":69208,\"start\":69207},{\"end\":69597,\"start\":69596},{\"end\":69607,\"start\":69606},{\"end\":69616,\"start\":69615},{\"end\":69626,\"start\":69625},{\"end\":69944,\"start\":69943},{\"end\":69953,\"start\":69952},{\"end\":69960,\"start\":69959},{\"end\":69969,\"start\":69968},{\"end\":69986,\"start\":69985},{\"end\":69997,\"start\":69996},{\"end\":70005,\"start\":70004},{\"end\":70017,\"start\":70016},{\"end\":70692,\"start\":70691},{\"end\":70698,\"start\":70697},{\"end\":70708,\"start\":70707},{\"end\":70715,\"start\":70714},{\"end\":70723,\"start\":70722},{\"end\":70740,\"start\":70732},{\"end\":70744,\"start\":70743},{\"end\":70995,\"start\":70994},{\"end\":71001,\"start\":71000},{\"end\":71010,\"start\":71009},{\"end\":71020,\"start\":71018},{\"end\":71024,\"start\":71023},{\"end\":71688,\"start\":71684},{\"end\":72092,\"start\":72091},{\"end\":72098,\"start\":72097},{\"end\":72104,\"start\":72103},{\"end\":72112,\"start\":72111},{\"end\":72119,\"start\":72118},{\"end\":72779,\"start\":72778},{\"end\":72789,\"start\":72788},{\"end\":73464,\"start\":73463},{\"end\":73475,\"start\":73474},{\"end\":73477,\"start\":73476},{\"end\":73486,\"start\":73485},{\"end\":74202,\"start\":74201},{\"end\":74214,\"start\":74213},{\"end\":74223,\"start\":74222},{\"end\":74239,\"start\":74238},{\"end\":74926,\"start\":74925},{\"end\":74938,\"start\":74937},{\"end\":74948,\"start\":74947},{\"end\":74959,\"start\":74955},{\"end\":75688,\"start\":75687},{\"end\":75700,\"start\":75699},{\"end\":76627,\"start\":76626},{\"end\":76637,\"start\":76636},{\"end\":76639,\"start\":76638},{\"end\":76649,\"start\":76648},{\"end\":76928,\"start\":76927},{\"end\":76938,\"start\":76937},{\"end\":76949,\"start\":76948},{\"end\":76960,\"start\":76959},{\"end\":76967,\"start\":76966},{\"end\":76977,\"start\":76976},{\"end\":76987,\"start\":76986},{\"end\":76995,\"start\":76994},{\"end\":77001,\"start\":77000},{\"end\":77003,\"start\":77002},{\"end\":77328,\"start\":77327},{\"end\":77341,\"start\":77340},{\"end\":77350,\"start\":77349},{\"end\":77361,\"start\":77360},{\"end\":78039,\"start\":78038},{\"end\":78050,\"start\":78049},{\"end\":78060,\"start\":78059},{\"end\":78068,\"start\":78067}]", "bib_author_last_name": "[{\"end\":67516,\"start\":67509},{\"end\":67528,\"start\":67520},{\"end\":68164,\"start\":68159},{\"end\":68175,\"start\":68168},{\"end\":68184,\"start\":68179},{\"end\":68195,\"start\":68188},{\"end\":68205,\"start\":68199},{\"end\":68216,\"start\":68209},{\"end\":68226,\"start\":68220},{\"end\":68572,\"start\":68564},{\"end\":68584,\"start\":68576},{\"end\":69181,\"start\":69174},{\"end\":69195,\"start\":69188},{\"end\":69205,\"start\":69199},{\"end\":69223,\"start\":69209},{\"end\":69231,\"start\":69225},{\"end\":69604,\"start\":69598},{\"end\":69613,\"start\":69608},{\"end\":69623,\"start\":69617},{\"end\":69629,\"start\":69627},{\"end\":69950,\"start\":69945},{\"end\":69957,\"start\":69954},{\"end\":69966,\"start\":69961},{\"end\":69983,\"start\":69970},{\"end\":69994,\"start\":69987},{\"end\":70002,\"start\":69998},{\"end\":70014,\"start\":70006},{\"end\":70029,\"start\":70018},{\"end\":70035,\"start\":70031},{\"end\":70695,\"start\":70693},{\"end\":70705,\"start\":70699},{\"end\":70712,\"start\":70709},{\"end\":70720,\"start\":70716},{\"end\":70730,\"start\":70724},{\"end\":70998,\"start\":70996},{\"end\":71007,\"start\":71002},{\"end\":71016,\"start\":71011},{\"end\":71692,\"start\":71689},{\"end\":72095,\"start\":72093},{\"end\":72101,\"start\":72099},{\"end\":72109,\"start\":72105},{\"end\":72116,\"start\":72113},{\"end\":72125,\"start\":72120},{\"end\":72786,\"start\":72780},{\"end\":72796,\"start\":72790},{\"end\":73472,\"start\":73465},{\"end\":73483,\"start\":73478},{\"end\":73493,\"start\":73487},{\"end\":74211,\"start\":74203},{\"end\":74220,\"start\":74215},{\"end\":74236,\"start\":74224},{\"end\":74246,\"start\":74240},{\"end\":74935,\"start\":74927},{\"end\":74945,\"start\":74939},{\"end\":74953,\"start\":74949},{\"end\":74963,\"start\":74960},{\"end\":75697,\"start\":75689},{\"end\":75707,\"start\":75701},{\"end\":76634,\"start\":76628},{\"end\":76646,\"start\":76640},{\"end\":76655,\"start\":76650},{\"end\":76935,\"start\":76929},{\"end\":76946,\"start\":76939},{\"end\":76957,\"start\":76950},{\"end\":76964,\"start\":76961},{\"end\":76974,\"start\":76968},{\"end\":76984,\"start\":76978},{\"end\":76992,\"start\":76988},{\"end\":76998,\"start\":76996},{\"end\":77007,\"start\":77004},{\"end\":77338,\"start\":77329},{\"end\":77347,\"start\":77342},{\"end\":77358,\"start\":77351},{\"end\":77367,\"start\":77362},{\"end\":78047,\"start\":78040},{\"end\":78057,\"start\":78051},{\"end\":78065,\"start\":78061},{\"end\":78076,\"start\":78069}]", "bib_entry": "[{\"attributes\":{\"doi\":\"doi: 10.18653/ v1/P18-2114\",\"id\":\"b0\",\"matched_paper_id\":19176069},\"end\":68155,\"start\":67445},{\"attributes\":{\"doi\":\"arXiv:1603.06042\",\"id\":\"b1\"},\"end\":68521,\"start\":68157},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6366436},\"end\":69130,\"start\":68523},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":28099170},\"end\":69509,\"start\":69132},{\"attributes\":{\"doi\":\"arXiv:2005.01643\",\"id\":\"b4\"},\"end\":69832,\"start\":69511},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":204960716},\"end\":70636,\"start\":69834},{\"attributes\":{\"doi\":\"arXiv:1701.06547\",\"id\":\"b6\"},\"end\":70936,\"start\":70638},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1421\",\"id\":\"b7\",\"matched_paper_id\":21646317},\"end\":71626,\"start\":70938},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":964287},\"end\":72021,\"start\":71628},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52056574},\"end\":72722,\"start\":72023},{\"attributes\":{\"doi\":\"doi: 10.18653/ v1/W18-6322\",\"id\":\"b10\",\"matched_paper_id\":52132833},\"end\":73347,\"start\":72724},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1206\",\"id\":\"b11\",\"matched_paper_id\":215768182},\"end\":74155,\"start\":73349},{\"attributes\":{\"doi\":\"doi: 10.18653/ v1/D17-1238\",\"id\":\"b12\",\"matched_paper_id\":1929239},\"end\":74859,\"start\":74157},{\"attributes\":{\"doi\":\"doi: 10.3115/ 1073083.1073135\",\"id\":\"b13\",\"matched_paper_id\":11080756},\"end\":75617,\"start\":74861},{\"attributes\":{\"doi\":\"10.18653/v1/N18-2102\",\"id\":\"b14\",\"matched_paper_id\":4940548},\"end\":76515,\"start\":75619},{\"attributes\":{\"id\":\"b15\"},\"end\":76842,\"start\":76517},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b16\"},\"end\":77264,\"start\":76844},{\"attributes\":{\"doi\":\"doi: 10.18653/ v1/D16-1264\",\"id\":\"b17\",\"matched_paper_id\":11816014},\"end\":77980,\"start\":77266},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7147309},\"end\":78373,\"start\":77982}]", "bib_title": "[{\"end\":67505,\"start\":67445},{\"end\":68560,\"start\":68523},{\"end\":69170,\"start\":69132},{\"end\":69941,\"start\":69834},{\"end\":70992,\"start\":70938},{\"end\":71682,\"start\":71628},{\"end\":72089,\"start\":72023},{\"end\":72776,\"start\":72724},{\"end\":73461,\"start\":73349},{\"end\":74199,\"start\":74157},{\"end\":74923,\"start\":74861},{\"end\":75685,\"start\":75619},{\"end\":77325,\"start\":77266},{\"end\":78036,\"start\":77982}]", "bib_author": "[{\"end\":67518,\"start\":67507},{\"end\":67530,\"start\":67518},{\"end\":68166,\"start\":68157},{\"end\":68177,\"start\":68166},{\"end\":68186,\"start\":68177},{\"end\":68197,\"start\":68186},{\"end\":68207,\"start\":68197},{\"end\":68218,\"start\":68207},{\"end\":68228,\"start\":68218},{\"end\":68238,\"start\":68228},{\"end\":68242,\"start\":68238},{\"end\":68574,\"start\":68562},{\"end\":68586,\"start\":68574},{\"end\":69183,\"start\":69172},{\"end\":69197,\"start\":69183},{\"end\":69207,\"start\":69197},{\"end\":69225,\"start\":69207},{\"end\":69233,\"start\":69225},{\"end\":69606,\"start\":69596},{\"end\":69615,\"start\":69606},{\"end\":69625,\"start\":69615},{\"end\":69631,\"start\":69625},{\"end\":69952,\"start\":69943},{\"end\":69959,\"start\":69952},{\"end\":69968,\"start\":69959},{\"end\":69985,\"start\":69968},{\"end\":69996,\"start\":69985},{\"end\":70004,\"start\":69996},{\"end\":70016,\"start\":70004},{\"end\":70031,\"start\":70016},{\"end\":70037,\"start\":70031},{\"end\":70697,\"start\":70691},{\"end\":70707,\"start\":70697},{\"end\":70714,\"start\":70707},{\"end\":70722,\"start\":70714},{\"end\":70732,\"start\":70722},{\"end\":70743,\"start\":70732},{\"end\":70747,\"start\":70743},{\"end\":71000,\"start\":70994},{\"end\":71009,\"start\":71000},{\"end\":71018,\"start\":71009},{\"end\":71023,\"start\":71018},{\"end\":71027,\"start\":71023},{\"end\":71694,\"start\":71684},{\"end\":72097,\"start\":72091},{\"end\":72103,\"start\":72097},{\"end\":72111,\"start\":72103},{\"end\":72118,\"start\":72111},{\"end\":72127,\"start\":72118},{\"end\":72788,\"start\":72778},{\"end\":72798,\"start\":72788},{\"end\":73474,\"start\":73463},{\"end\":73485,\"start\":73474},{\"end\":73495,\"start\":73485},{\"end\":74213,\"start\":74201},{\"end\":74222,\"start\":74213},{\"end\":74238,\"start\":74222},{\"end\":74248,\"start\":74238},{\"end\":74937,\"start\":74925},{\"end\":74947,\"start\":74937},{\"end\":74955,\"start\":74947},{\"end\":74965,\"start\":74955},{\"end\":75699,\"start\":75687},{\"end\":75709,\"start\":75699},{\"end\":76636,\"start\":76626},{\"end\":76648,\"start\":76636},{\"end\":76657,\"start\":76648},{\"end\":76937,\"start\":76927},{\"end\":76948,\"start\":76937},{\"end\":76959,\"start\":76948},{\"end\":76966,\"start\":76959},{\"end\":76976,\"start\":76966},{\"end\":76986,\"start\":76976},{\"end\":76994,\"start\":76986},{\"end\":77000,\"start\":76994},{\"end\":77009,\"start\":77000},{\"end\":77340,\"start\":77327},{\"end\":77349,\"start\":77340},{\"end\":77360,\"start\":77349},{\"end\":77369,\"start\":77360},{\"end\":78049,\"start\":78038},{\"end\":78059,\"start\":78049},{\"end\":78067,\"start\":78059},{\"end\":78078,\"start\":78067}]", "bib_venue": "[{\"end\":67737,\"start\":67645},{\"end\":68767,\"start\":68691},{\"end\":70210,\"start\":70126},{\"end\":71223,\"start\":71135},{\"end\":71743,\"start\":71727},{\"end\":72312,\"start\":72232},{\"end\":72978,\"start\":72901},{\"end\":73691,\"start\":73603},{\"end\":74452,\"start\":74362},{\"end\":75186,\"start\":75083},{\"end\":76022,\"start\":75873},{\"end\":77567,\"start\":77483},{\"end\":78153,\"start\":78132},{\"end\":67643,\"start\":67556},{\"end\":68310,\"start\":68258},{\"end\":68654,\"start\":68586},{\"end\":69285,\"start\":69233},{\"end\":69594,\"start\":69511},{\"end\":70124,\"start\":70037},{\"end\":70689,\"start\":70638},{\"end\":71133,\"start\":71047},{\"end\":71725,\"start\":71694},{\"end\":72195,\"start\":72127},{\"end\":72899,\"start\":72824},{\"end\":73601,\"start\":73515},{\"end\":74360,\"start\":74274},{\"end\":75081,\"start\":74994},{\"end\":75871,\"start\":75729},{\"end\":76624,\"start\":76517},{\"end\":76925,\"start\":76844},{\"end\":77481,\"start\":77395},{\"end\":78130,\"start\":78078}]"}}}, "year": 2023, "month": 12, "day": 17}