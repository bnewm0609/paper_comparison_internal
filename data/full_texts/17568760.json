{"id": 17568760, "updated": "2023-09-30 13:01:22.672", "metadata": {"title": "Subjective and Objective Visual Quality Assessment of Textured 3D Meshes", "authors": "[{\"first\":\"Jinjiang\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Vincent\",\"last\":\"Vidal\",\"middle\":[]},{\"first\":\"Irene\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Anup\",\"last\":\"Basu\",\"middle\":[]},{\"first\":\"Atilla\",\"last\":\"Baskurt\",\"middle\":[]},{\"first\":\"Guillaume\",\"last\":\"Lavoue\",\"middle\":[]}]", "venue": null, "journal": "ACM Transactions on Applied Perception (TAP)", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "Objective visual quality assessment of 3D models is a fundamental issue in computer graphics. Quality assessment metrics may allow a wide range of processes to be guided and evaluated, such as level of detail creation, compression, filtering, and so on. Most computer graphics assets are composed of geometric surfaces on which several texture images can be mapped to 11 make the rendering more realistic. While some quality assessment metrics exist for geometric surfaces, almost no research has been conducted on the evaluation of texture-mapped 3D models. In this context, we present a new subjective study to evaluate the perceptual quality of textured meshes, based on a paired comparison protocol. We introduce both texture and geometry distortions on a set of 5 reference models to produce a database of 136 distorted models, evaluated using two rendering protocols. Based on analysis of the results, we propose two new metrics for visual quality assessment of textured mesh, as optimized linear combinations of accurate geometry and texture quality measurements. These proposed perceptual metrics outperform their counterparts in terms of correlation with human opinion. The database, along with the associated subjective scores, will be made publicly available online.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2102.03982", "mag": "2535540567", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2102-03982", "doi": "10.1145/2996296"}}, "content": {"source": {"pdf_hash": "97e594cecf7bdd5120498aea17705e8cee2e51eb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2102.03982v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2102.03982", "status": "GREEN"}}, "grobid": {"id": "50720081f52e9b3ec9aaa58a55b6ae214e71cf93", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/97e594cecf7bdd5120498aea17705e8cee2e51eb.txt", "contents": "\nSubjective and Objective Visual Quality Assessment of Textured 3D Meshes\nOctober 2016\n\nJinjiang Guo \nVincent Vidal \nJinjiang Guo \nVincent Vidal \nIrene Cheng \nAnup Basu \nAtilla Baskurt \nGuillaume \n\nUniv Lyon\nCNRS\nLIRIS\nFrance\n\n\nMultimedia Research Center\nATILLA BASKURT and GUILLAUME\nLAVOUE\nIRENE CHENG and ANUP BASU\nUniversity of Alberta\nCanada\n\n\nUniv Lyon\nCNRS\nLIRIS\nFrance\n\nSubjective and Objective Visual Quality Assessment of Textured 3D Meshes\n\nAppl. Percept\n1411October 201610.1145/299629611 ACM Reference Format:CCS Concepts: r Computing methodologies \u2192 Appearance and texture representationsPerceptionMesh modelsAdditional Key Words and Phrases: Textured mesh, visual quality assessment, subjective study\nObjective visual quality assessment of 3D models is a fundamental issue in computer graphics. Quality assessment metrics may allow a wide range of processes to be guided and evaluated, such as level of detail creation, compression, filtering, and so on. Most computer graphics assets are composed of geometric surfaces on which several texture images can be mapped to make the rendering more realistic. While some quality assessment metrics exist for geometric surfaces, almost no research has been conducted on the evaluation of texture-mapped 3D models. In this context, we present a new subjective study to evaluate the perceptual quality of textured meshes, based on a paired comparison protocol. We introduce both texture and geometry distortions on a set of 5 reference models to produce a database of 136 distorted models, evaluated using two rendering protocols. Based on analysis of the results, we propose two new metrics for visual quality assessment of textured mesh, as optimized linear combinations of accurate geometry and texture quality measurements. These proposed perceptual metrics outperform their counterparts in terms of correlation with human opinion. The database, along with the associated subjective scores, will be made publicly available online.\n\nINTRODUCTION\n\nTexture mapped 3D graphics are now commonplace in many applications, including digital entertainment, cultural heritage, and architecture. They consist of geometric surfaces on which several texture images can be mapped to make the rendering more realistic. Common texture maps include the diffuse map, the normal map, and the specular map. After their creation (by a designer or using a scanning/reconstruction process), these textured 3D assets may be subject to diverse processing operations, including simplification, compression, filtering, watermarking, and so on. For instance, with the goal of accelerating transmission for remote web-based 3D visualization (e.g., for a virtual museum Authors' addresses: J. Guo,V. Vidal,A. Baskurt,and G. Lavou\u00e9,LIRIS CNRS,INSA Lyon,B\u00e2timent Jules Verne,20 Avenue Albert Einstein, 69621 Villeurbanne, France; emails: jinjiang.guo@insa-lyon.fr, {vvidal, atilla.baskurt, glavoue}@liris.cnrs.fr; I. Cheng and A. Basu, Multimedia Research Centre, Department of Computing Science, University of Alberta, 114St. -89 Av. Edmonton Alberta, Canada T6G 2E8; emails: {locheng, basu}@ualberta.ca. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c 2016 ACM 1544-3558/2016/10-ART11 $15.00 DOI: http://dx.doi.org/10.1145/2996296 application), the geometry may be simplified and quantized, and the texture maps may be subject to JPEG compression. Similar geometry and texture degradations may also occur when these assets have to be adapted for lightweight mobile devices; in that case, textures may have to be sub-sampled or compressed using some GPU-friendly random-access methods (e.g., Str\u00f6m and Akenine-M\u00f6ller [2005]). These geometry and texture content corruptions may severely impact the visual quality of the 3D data. Therefore, there is a critical need for efficient perceptual metrics to evaluate the visual impact of these textured model artifacts on the visual quality of the rendered image.\n\nMany visual quality metrics have been introduced in the field of computer graphics. However, most of them can only be applied on images created during the rendering step. They mostly focus on detecting artifacts caused by global illumination approximation or tone mapping [Ayd\u0131n et al. 2010;Yeganeh and Wang 2013;\u010cad\u00edk et al. 2013]. On the contrary, another class of method focuses on evaluating the artifacts introduced on the 3D assets themselves. However, most consider only geometric distortions [Lavou\u00e9 2011;V\u00e1\u0161a and Rus 2012;. Little work has been conducted to evaluate the visual impact of both geometry and texture distortions on the appearance of the rendered image. Studying the complex perceptual interactions between these two types of information requires a ground-truth of subjective opinions on a variety of models with such degradations. To the best of our knowledge, only Pan et al. [2005] conducted such a subjective study. However, they considered only geometry and texture sub-sampling distortions. In this article, we present a large-scale subjective experiment for this purpose, based on a paired comparison protocol. As in Pan et al. [2005], we restrict the texture information to the diffuse maps. Our dataset contains 272 videos of animated 3D models created from five reference objects, five types of distortions, and two rendering settings. The experiments involved more than 100 people. After an analysis of the influence of lighting, as well as shape and texture content on the perception of artifacts, we then use this subjective ground-truth to evaluate the performance of a large set of state-of-the-art metrics (dedicated to image, video, and 3D models). Finally, we propose new metrics based on optimal combinations of geometric and image measurements.\n\nThe rest of this article is organized as follows. Section 2 provides a review of related work, while Section 3 describes our subjective experiments and their results. Section 4 presents a comprehensive evaluation of state-of-the-art image and mesh metrics with respect to our subjective ground-truth, along with details on our proposed perceptual metrics and their validation. Finally, concluding remarks and perspective on the work are outlined in Section 5.\n\n\nRELATED WORK\n\nOur goal in this work is to propose a visual quality metric for textured 3D models. Since this involves both geometry and 2D image information, we first review related work in image quality assessment and 3D mesh quality assessment. We then focus on the few works that have addressed the visual quality assessment of textured 3D models. For a global view of the topic of visual quality assessment in computer graphics, the reader may refer to Lavou\u00e9 and Mantiuk [2015].\n\n\nVisual Quality Assessment of 2D Images\n\nIn the field of 2D image processing, research into objective image quality assessment metrics is substantially developed [Wang and Bovik 2006]. Existing algorithms can be classified according to the availability of a reference image: full reference (FR), no-reference (NR), and reduced-reference (RR). The following discussion only focuses on FR methods, where the original distortion -free image is known as the reference image. Since the pioneering work of Mannos and Sakrison [1974], many metrics have been introduced with the aim of replacing the classical peak-signal-to-noise ratio (PSNR) which does not correlate well with human vision. Many techniques have tried to mimic the low-level mechanisms ACM Transactions on Applied Perception, Vol. 14, No. 2, Article 11, Publication date: October 2016. of the human visual system (HVS), such as the contrast sensitivity function (CSF), usually modeled by a band-pass filter, and the visual masking effect which defines the fact that one visual pattern can hide the visibility of another. These bottom-up approaches include the Sarnoff Visual Discrimination model (VDM) [Lubin 1993], the Visible Difference Predictor (VDP) [Daly 1993], and the more recent HDR-VDP-2 [Mantiuk et al. 2011], suited for any range of luminance. These computational metrics mostly focus on the visual detectability of near-threshold distortions and are usually less efficient for quantifying visual fidelity (i.e., supra-threshold distortions), except some works like VSNR [Chandler and Hemami 2007], which explicitly incorporates both models. In contrast to these computationally bottom-up approaches, some authors proposed top-down metrics which do not take into account any HVS models but instead operate based on some intuitive hypotheses of what the HVS attempts to achieve when shown a distorted image. The most well-known example is the Structural SIMilarity index (SSIM) [Wang et al. 2004] and its derivatives (Multi-Scale SSIM [Wang et al. 2003] and Information-weighted SSIM [Wang and Li 2011]). With a more theoretical definition, the Visual Information Fidelity (VIF) metric [Sheikh and Bovik 2006] was developed with the aim of quantifying the loss of image information resulting from the distortion process. Recent surveys and benchmarks [Zhang 2012] point out the superiority of these top-down approaches for visual fidelity (or quality) prediction. It is interesting, however, to note that these evaluations have been conducted on natural image databases. For the particular case of computer-generated images,\u010cad\u00edk et al. [2012] have shown that distortions introduced by common global illumination approximations are not adequately captured by these popular image metrics (e.g., SSIM).\n\n\nVisual Quality Assessment of 3D Meshes\n\nInspired by image quality metrics, several perceptually motivated metrics have been designed for 3D meshes. They are all full-reference and attempt to predict the visual fidelity of a given 3D mesh (subject to various geometric distortions) with respect to a reference one. The first authors who tried to incorporate some perceptual insights to improve the reliability of geometric distortion measurements were Karni and Gotsman [2000], who proposed combining the Root Mean Square (RMS) distance between corresponding vertices with the RMS distance of their Laplacian coordinates (which reflect a degree of smoothness of the surface). Lavou\u00e9 [2011] and  proposed metrics based on local differences of curvature statistics, while V\u00e1\u0161a and Rus [2012] considered the dihedral angle differences. These metrics consider local variations of attribute values at vertex or edge level, which are then pooled into a global score. In contrast, Corsini et al. [2007] and  compute global roughness values per model and then derive a simple global roughness difference. Similar to bottom-up image quality metrics, some of these latter algorithms V\u00e1\u0161a and Rus 2012] integrate perceptually motivated mechanisms, such as visual masking. A recent survey [Corsini et al. 2013] details these works and compares their performance with respect to their correlation with mean opinion scores derived from subjective rating experiments. This study shows that MSDM2 [Lavou\u00e9 2011], FMPD , and DAME [V\u00e1\u0161a and Rus 2012] are excellent predictors of visual quality. Besides these works on global visual fidelity assessment (suited for supra-threshold distortions), several relevant works were introduced very recently: Nader et al.\n\n[2016] introduced a bottom-up visibility threshold predictor for 3D meshes (assuming a flat-shaded rendering), and Guo et al. [2015] studied the local visibility of geometric artifacts and showed that curvature may be a good predictor of local distortions. Finally, a comprehensive study was introduced by Lavou\u00e9 et al. [2016] to investigate the use of image metrics computed on rendered images for assessing the visual quality of 3D models (without texture). It shows that some of them (Multi-Scale SSIM (MS-SSIM), in particular) may offer excellent performance. One of the contributions of the present work, is to validate whether this conclusion still holds for textured meshes.\n\n\nVisual Quality Assessment of Textured 3D Models\n\nOnly a few publications can be found in the literature dealing with quality assessment of textured 3D models. Existing works [Tian and AlRegib 2004;Yang et al. 2004;Pan et al. 2005] are mostly dedicated to choosing the appropriate mesh and texture levels of detail (LoD) for optimizing progressive transmission of textured meshes. Pan et al. [2005] introduced a metric directly based on mesh and texture resolutions, fitted on subjective data. Even though this metric is efficient for the purpose of optimizing transmission [Cheng and Basu 2007], it cannot be generalized to other types of distortions. AlRegib [2004, 2008] proposed the Fast Quality Measure (FQM) as a weighted combination of two simple error measurements: the mean squared surface error, approximated in the 2D domain for efficiency issues, and the mean squared error over texture pixels. The weighting coefficient between these two measurements is computed by studying the pixel error on rendered images of several LoDs (considered as a meaningful prediction of their subjective perceptual quality). The FQM metric was not subject to any perceptual validation by a subjective experiment; thus, in the experimental section, we compare our results with it. Yang et al. [2004] and Griffin and Olano [2015] also considered image metrics computed on rendered images as oracles of subjective quality. Yang et al. [2004] used the mean squared error over pixels and the Mannos model [Mannos and Sakrison 1974] for optimizing textured mesh transmission, while Griffin and Olano [2015] considered SSIM [Wang et al. 2004] to evaluate the masking effect between texture and normal maps (in the context of compression artifacts). As stated in the above section, our subjective data will provide a means of testing this hypothesis of performance of image metrics computed on rendered images or videos for textured mesh quality assessment.\n\nFinally, two related works are those by Ferwerda et al. [1997] and Qu and Meyer [2008]. The authors aim to evaluate how texture is able to mask geometric distortions (from simplification and remeshing), whereas our objective is to evaluate the effect of texture distortions (possibly combined with geometric distortions) on the final appearance.\n\n\nSUBJECTIVE EXPERIMENT\n\nWe conducted a large-scale subjective experiment to evaluate the visual impact of texture and geometry distortions on the appearance of textured 3D models. We chose a paired comparison technique, where observers are shown two stimuli side by side and are asked to choose the one that is most similar to the reference (forced-choice methodology). This protocol was shown to be more accurate than others (e.g., single stimulus rating) due to the simplicity of the subjects' task . This section provides details on the subjective study and its results.\n\n\nStimuli Generation\n\nWe selected five textured triangle meshes created using different methodologies and targeting different application domains (see Figure 1). The Hulk and Sport Car are artificial models created using a modeling software. Selected from a community model repository (ShareGC.com), they both have structured texture content and smooth texture seams (i.e., vertices associated with multiple texture coordinate pairs). The squirrel and the Easter Island statue come from a reconstruction process using multiple photographs, and are courtesy of the EPFL Computer Graphics and Geometry Laboratory. Finally, the Dwarf is a scanned model, courtesy of the ISTI-CNR Visual Computing Laboratory, Pisa (http://vcg.isti.cnr.it). These last three models, created respectively from reconstruction and scanning, exhibit a noisier texture image, and two of them have very complex texture seams.\n\nThe number of vertices of the five models ranges from 6,000 to 250,000, while the texture size ranges from 256 \u00d7 256 to 4,096 \u00d7 4,096. Note that the Hulk, Statue, and Sport Car are associated with several texture images. The full characteristics are detailed in Table I (wireframes with texture   illustrated in the supplementary material). These objects span a wide variety of geometry and texture data. These reference models have been corrupted by five types of distortions (three applied on the geometry and two applied on the texture), each applied with four different strengths:\n\nOn the geometry:\n\n-Compression: We consider uniform geometric quantization, the most common lossy process of compression algorithms. -Simplification: We consider the Quadric Error Metric algorithm by Garland and Heckbert [1997].\n\n-Smoothing: We consider Laplacian smoothing [Taubin 1995].\n\n\nOn the texture map:\n\n-JPEG: The most commonly used algorithm for lossy 2D image compression. -Sub-sampling: We reduce texture size by resampling through bilinear interpolation.\n\nThe strength of these distortions was adjusted manually in order to span the whole range of visual quality from imperceptible levels to high levels of impairment. For this task, a large set of distortions was generated and viewed by the authors, and a sub-set of them spanning the desired visual quality (i.e., \"Excellent,\" \"Good,\" \"Fair,\" and \"Poor\") was chosen to be included in the database. This perceptual ACM Transactions on Applied Perception, Vol. 14, No. 2, Article 11, Publication date: October 2016.\n\n\n11:6\n\n\u2022 J. Guo et al. \n\n\nRendering Parameters\n\nUser interaction: In existing subjective studies involving 3D content, different ways have been used to display the 3D models to the observers, from the most simple (such as static images, as in Watson et al. [2001]) to the most complex (by allowing free rotation, zoom, and translation, as in Corsini et al. [2007]). While it is important for the observer to have access to different viewpoints of the 3D object, the problem of allowing free interaction is the cognitive overload which may alter the results. A good compromise is to use animations, as in Pan et al. [2005]. For each object in our database, we generate a low-speed rotation animation around the vertical axis.\n\nLighting and shading: As noticed by Rogowitz and Rushmeier [2001], the position and type of light sources have a strong influence on the perception of the artifacts. Lighting from the front tends to have a masking effect; hence, we chose an indirect illumination. Sun and Perona [1998] showed that people tend to assume light is above and slightly to the left of the object when they interpret a shaded picture as a 3D scene. Their observations have been confirmed by O' Shea et al. [2008], who demonstrated that the viewer's perception of a 3D shape is more accurate when the angle between the light direction and viewing direction is 20 to 30 degrees above the viewpoint and to the left by 12 degrees from vertical. We follow this lighting condition by putting a spot light at this position. For the material, we kept the original parameters from the source reference objects, which are mainly diffuse. The video resolution is 1920 \u00d7 1080. The duration of the video is 15 seconds.\n\n\u2022 11:7 Fig. 2. Examples of distorted models from our dataset. The rendering is the same as in our videos. Note that for the Dwarf, we present here compound distortions (geometry and texture) from our validation set (see Section 4.5).\n\nIn order to study the influence of shading on the results, we also re-generated the same set of videos by keeping only the diffuse albedo (i.e., without shading). An example of such rendering is shown in Figure 2, for the Dwarf. We thus obtain 10 video sets (5 models \u00d7 2 rendering settings), for a total of 200 videos to evaluate.\n\n\nExperimental Procedure\n\nAs stated above, we opted for a paired comparison methodology, since this has been demonstrated to be more reliable than rating methods . Participants are shown two videos of distorted models at a time, side by side, and are asked to choose the one that is most similar to the reference. The observer can replay the videos as many times as (s)he wants. For the sake of readability, the reference video is not displayed on the same screen but is presented just before the beginning of the comparison, and can then be viewed at any time on a pop-up window by clicking on a button. The interface was developed in JavaScript, as a web platform, and is illustrated in Figure 3. The main issue with the paired comparison protocol is the large number of possible comparisons: 2 20 = 190 per video set. It is therefore unrealistic to ask a participant to perform a complete test even on a single video set. Fortunately, this large number of trials can be reduced by using sorting algorithms as recommended in Silverstein and Farrell [2001] and . The idea is to embed a sorting algorithm into the experimental platform; this algorithm then decides in an on-line fashion which pairs of videos to compare based on the previous comparisons.\n\nWe introduced a simple yet efficient sorting algorithm. The idea is to obtain a global ranking of the 20 stimuli by interleaving them progressively, one distortion type at a time (i.e., compression, simplification, smoothing, JPEG, and sub-sampling). For a given distortion D, we assume that the distortion strength ranges from D 1 (weak) to D 4 (strong). The assumption behind our algorithm is that for a given distortion D, the quality of D i is always better than D j for j > i (noted as Q D i > Q D j , \u2200 j > i). First, two distortion types Q and J are randomly chosen (in this example, quantization and JPEG), Q 4 and J 4 are then compared. The index of the not-selected video (e.g., Q 4 ) is pushed into a list (List 1), as the poorest quality version. In the next trial, the selected model from the previous round (J 4 ) and a distorted model with a decreased level from the other type (Q 3 ) are shuffled and displayed to the user. This process continues until all eight models are sorted from the worst to the best quality. This sorting process is repeated with two other distortion types (i.e., smoothing and simplification) to form a second list (List 2) which is then interleaved with the remaining distortion type (sub-sampling) and then with List 1 to obtain the final ranking of the 20 distortions. In our study, the average comparison number was 36 (instead of 190 for the full design). This sorting algorithm is illustrated in the supplementary material.\n\n\nParticipants\n\nA total of 101 subjects took part in the experiment, aged between 20 and 55, all with normal or corrected to normal vision. Participants were students and staff from the University of Lyon in France and the University of Alberta in Canada. On average, it took 12 minutes for one observer to finish the experiment for one set of videos. Eighty-nine subjects rated one set, eight rated two sets, and four rated three sets. None of these repetitions took place on the same day in order to prevent any learning effect. The experiments were all conducted on the same 15-inch MacBook Pro screen, in a dark room. In total, each of the 10 video sets (5 models \u00d7 2 rendering settings) was judged by between 11 and 15 observers (see Table III).  \n\n\nComputing Scores\n\nFor each video set and each subject, we obtain a global ranking of the n m distorted models (n m equals 20 in our experiment). From this ranking, it is easy to retrieve the full preference matrix (n m \u00d7 n m ), by applying the transitive relation: if object A is better than object B and B is better than C, then we can deduce that A is better than C. These per-subject preference matrices can then be summed into a single one (per video set). In this matrix P, each element P i, j represents the number of times the stimulus i was judged to be of higher quality than stimulus j. As in Ledda et al. [2005] and , we then consider the number of votes received by each stimuli as its quality score, which may then be divided by the number of human subjects n s for normalization among video sets:\ns i = n m j=1 P i, j n s(1)\nWe thus obtain one subjective score for each distorted model s i which belongs to [0,19]. Note that 19 occurs only for models which have been ranked first by every subject. Note that more sophisticated statistical methods exist for inferring scale values from a preference matrix. We computed scores using Thurstone's Law of Comparative Judgments, Case V [Thurstone 1927], which assumes that observers' choices can be thought of as sampled from a normal distribution of underlying quality scores. Since the values obtained were very close to the simple vote counts described above (more than 0.99 Pearson correlation between them), we decided to keep the latter.\n\n\nAnalysis and Discussion\n\n3.6.1 Observer Agreement. It is essential to analyze the agreements between the subjects before studying the results of the experiment. Since each observer outputs a global ranking of the stimuli, the best way to evaluate their agreement is to compute Kendall's coefficient of concordance W [Govindarajulu et al. 1992] which assesses the agreement among raters. Table III gives details on the results. W ranges from one, meaning complete agreement, to zero, meaning no agreement, while the p-value associated with W provides the likelihood of null hypothesis, which means no agreement between all the subjects. Table III shows that the overall Kendall's W coefficients are at least larger than 0.69, implying a strong agreement among the subjects, confirmed by the very low p-values.\n\n3.6.2 Influence of Shape and Texture on the Visual Impact of the Distortions. The impact of the distortions greatly depends on the textured 3D models and their characteristics as illustrated in Figure 4. We observe that geometric quantization is less visible on 3D shapes with a few thousands vertices (like the Squirrel) than on shapes with more than 100,000 (like the Dwarf, Sport Car, and Statue). The Hulk model is rather low-resolution but is also strongly damaged by the quantization; the reason is that its head has a high density of triangles and thus is severely damaged by this distortion. The sampling density of the surface actually influences the spatial frequency of the visual distortions created by quantization and thus their perceptual impact due to the Contrast Sensitivity Function (CSF) of the HVS. Note that, while there is a limit on the mesh density, beyond which the distortions become invisible again (as a result of overly high frequency), all of our models are below this threshold.\n\nWe also observe that meshes with complex texture seams, like the Statue, are particularly sensitive to simplification, which tends to damage these seams and thus leads to local texture shifting on the surface of the model.\n\nAnother observation is that highly curved models (e.g., the Hulk and Squirrel) are very sensitive to Laplacian smoothing which causes large modifications of their shape. Smoother and higher density shapes (e.g., the Dwarf and Statue) remain virtually unchanged by these distortions.\n\nFor textures, the effects depend on several factors: their amount of structure, their amount of noise (which relates to the Masking effect), their frequency (which relates to the CSF), and their resolution. For the Sport Car, another important factor is that many of its textures are interior and thus hidden from the observers.\n\n3.6.3 Influence of the Rendering. In this section, we investigate the influence of rendering on the visual impact of the distortions. Figure 5 presents the quality scores, averaged over the models for the two rendering conditions: with and without shading. As expected, when only the diffuse albedo of the surface is taken into account, the quality scores of the geometric distortions are consistently better since the impact of the geometry on the rendering is basically limited to the silhouette. By conducting one-tailed paired t-tests, we found a significant increase in the quality of geometric distortions (pvalue = 3.9\u00d710 \u22123 ) and a significant decrease in the quality of texture distortions (p-value = 1.9\u00d710 \u22125 ). These intuitive results have to be kept in mind when assessing the quality of textured 3D models. A calibration according to the rendering may be necessary. \n\n\nTOWARD AN OPTIMAL METRIC FOR ASSESSMENT OF TEXTURED MESH QUALITY\n\nIn this section, we propose an objective metric for textured mesh quality assessment as a simple linear combination of mesh quality and texture quality. We use the subjective dataset presented above to evaluate the performance of different mesh and texture metrics for this task. We also compare their performance to video metrics computed on the rendered videos. In the next section, we also propose a new metric for geometry quality assessment.\n\n\nA New Metric for Geometry Quality Assessment\n\nBy analyzing the subjective scores obtained for geometric distortions (smoothing, quantization, and simplification), we observed that the ranks of the distortions with the highest strengths from these three types show a pattern common to all models: the distorted model ranked as the worst visual quality always comes from either the strongest quantization method or from the strongest simplification. Distorted models from the strongest smoothing never appear at the end of the ranks. For instance, among the 20 distorted Hulk models, the distortion with the worst visual quality is the 7-bit quantization (subjective score: 0.91), while the distortion of the worst smoothing has a fairly high subjective score (7.45) (see Figure 6). Quality scores from L3,L4 (see Table II for meaning) are actually significantly higher than Q3,Q4 (p-values = 0.011 and 0.0063 for shaded/non-shaded data) and significantly higher than Si3,Si4 (p-values = 0.011 and 0.03). This subjective pattern is related to the perceptual mechanisms of the HVS, which is more sensitive to high-frequency variations on local areas (e.g., distortions caused by simplification or quantization) rather than to more global low-frequency variations (e.g., caused by smoothing).\n\nBased on these observations as well as previous studies which emphasize the reliability of curvature for predicting visual distortions [Lavou\u00e9 2011;Guo et al. 2015], we propose a novel local distortion measurement by computing the variance of curvature differences in local corresponding neighborhoods between two meshes (a distorted mesh M d and a reference mesh  \n\u03b4 h v = 1 k k j=1 \u0108 j \u2212 C j max(\u0108 j , C j ) + a \u2212 E \u0108 j \u2212 C j max(\u0108 j , C j ) + a 2 ,(2)\nwhere k is the number of vertices of the neighborhood, a is a constant to avoid instability when denominators are close to zero, and E(\u0108 j \u2212C j max(\u0108 j ,C j )+a ) is the mean value of curvature differences in the neighborhood. \u03b4 h v is theoretically upper bounded by 2 but can be clamped in [0,1]. Still, as in Lavou\u00e9 [2011], we carry out this computation for three different neighborhood sizes h i to capture the perceptually meaningful scales and improve the efficiency and robustness of the metric. We took three scales h i \u2208 {2 , 3 , 4 }, where = 2.5% of the max length of the bounding box of the model. Then, the multiscale local distortion measurement \u03b4 M (v) is computed as:\n\u03b4 M (v) = 1 n n i=1 \u03b4 h i v .\nn is the number of scales (three in our study). Finally, we consider a root mean square pooling of these local measurements to obtain our global Standard Deviation of Curvature Difference SDCD quality index:\nSDCD(M d , M r ) = 1 \u2212 \uf8eb \uf8ed 1 |M d | v\u2208M d \u03b4 M (v) 2 \uf8f6 \uf8f8 1 2 .(3)\nSDCD is within the range [0, 1]: a value of 0 means that the two objects are identical while values near 1 mean that they are visually very different. This metric captures local roughness variations, whereas more global changes (e.g., global shrinking of the model) are not considered as perceptually significant. The performance of this metric is evaluated in the following section.\n\n\nMesh and Image Metric Evaluation\n\nThe distorted models from our dataset are associated with a single attack (either on geometry or texture). Their subjective scores may thus be used as the ground truth to evaluate respectively mesh metrics and texture image metrics separately. The objective is to determine the most appropriate metrics for geometry and texture image, respectively, and then to combine them (see next section). For each of   [Wang et al. 2004] 0 our reference models, we split the dataset into two groups according to the distortion type: geometry or texture. We selected several commonly used perceptual geometric and image metrics. For geometry, we selected the Root Mean Square Error (RMSE) computed on geometry, MSDM2 [Lavou\u00e9 2011], which is one of the best performing perceptually-motivated metrics, and our newly proposed metric SDCD. For texture, we selected the RMSE on image pixels, SSIM [Wang et al. 2004], and MS-SSIM [Wang et al. 2003] (top performing metrics on natural images). Tables IV and V detail the Spearman and Pearson correlations between the objective metrics and the subjective scores for geometry and texture quality assessment, respectively.\n\nPrevious studies dedicated to quality assessment of 3D meshes [Corsini et al. 2013] and natural images [Zhang 2012] have shown that geometry and image RMSEs are not good predictors of visual quality. It is interesting to observe that these results are confirmed for our database for which they are outperformed by perceptual metrics. This result is interesting because, in our case, texture and geometry involve complex masking effects. Indeed, texture artifacts may be masked by geometric mapping and vice versa. Table IV shows that, on certain models, MSDM2 performs better than SDCD. However, for the Statue, SDCD demonstrates a significant improvement. Considering these performances, we will consider both MSDM2 and SDCD for our newly combined quality metric for textured mesh. For the texture metric (see Table V), MS-SSIM provides the best overall performance and will thus be chosen for our novel optimal combination.\n\n\nToward an Optimal Combination\n\nWe propose assessing the visual quality of a textured mesh as a simple linear combination of its geometry quality and its texture quality, measured, respectively, by a 3D mesh metric Q G (MSDM2 or SDCD) and an image metric Q T (MS-SSIM). The results show that this simple scheme can provide very good results. Our combined metric is thus defined as follows:\nCM = \u03b1 Q G + (1 \u2212 \u03b1)Q T ,(4)\nwhere \u03b1 is an optimal weight determined by greedy optimization through a five-fold cross-validation. For each model (respectively, Squirrel, Statue, Sport Car, Hulk, and Dwarf), we compute the optimal weight as that which maximizes the Spearman correlation over the four other models. We introduce two versions of our combined metric: CM 1 (the optimal combination of MSDM2 and MS-SSIM) and  CM 2 (the optimal combination of SDCD and MS-SSIM). Note that Q T and Q G , and thus CM 1 and CM 2 , are similarity indices, ranging from 0 (total dissimilarity) to 1 (perfect similarity). \n\n\nPerformance Evaluation and Comparison\n\nWe compare our metrics CM 1 and CM 2 to several state-of-the-art metrics:\n\n-FQM AlRegib 2004, 2008], a metric especially designed for textured mesh quality assessment. It is defined as a weighted combination of two simple error measurements: the mean squared surface distance and the mean squared error over texture pixels. Optimal weights are computed using cross-validation, as for our metric. -Several video quality metrics applied to the rendered videos: The Discrete Cosinus Transform-based video quality metric from Xiao [2000], the PSNR applied on all frames and averaged, and the MS-SSIM applied on all frames and averaged. These metrics were computed using the MSU Video Quality Measurement Tool 1 .\n\nThe performance of these metrics is evaluated using the Spearman and Pearson correlations between the objective metric values and the subjective scores, as well as the RMS error. The Pearson correlation and the RMS are computed after a logistic regression which provides a non-linear mapping between the objective and subjective scores. Results are shown in Tables VII and VIII for the renderings with and without shading, respectively. Scatter plots of subjective scores versus metric values are presented in Figure 7 (shaded rendering).\n\nAs illustrated in the tables, our metrics CM 1 and CM 2 outperform the others for most of the models. Given the fact that, for a given model, the weighting factor \u03b1 is learned using the other ones, this good performance demonstrates an excellent inter-model robustness. It is interesting to see that the performance of our metrics is generally better for the non-shaded rendering than for the shaded rendering. The reason is that shading involves complex masking interactions between texture and geometry that are not considered in our metrics since they evaluate geometry and texture separately. These  interactions are extremely limited in the non-shaded rendering, which accounts for the improved results of our metrics. The main problem with video-based metrics is that they overestimate the visual effect of the Laplacian smoothing, as can be seen in the first column of Figure 7. Indeed, smoothing tends to produce slight displacements of the object silhouette. While this effect is almost invisible to the human eye, it produces displacements of the salient edges in the rendered images, which are very harmful for image/video metrics.\n\nAll combined metrics (CM 1 , CM 2 , and FQM) have lower performances for the Statue. The reason behind this is visible in Figure 7. Indeed, in the corresponding plots, we observe that the visual impact of simplification distortions (green dots) is underestimated by these metrics (which provide good quality scores for simplified models). The reason is that the poor subjective quality of these distorted versions is due to the damage on the texture seams, which considerably alters the visual appearance but is totally unpredictable by combined metrics that do not take into account the texture mapping. The same underestimation of the simplification impact is observed for the Dwarf model, which also exhibits complex texture seams. For this latter model, the geometric metrics also underestimate the impact of quantization which is particularly harmful for such a high-resolution model.\n\nOne last observation is the superiority of video-based metrics for the Sport Car model (shaded rendering). The reason is as follows: this model has several interior parts (e.g., seats, radio) whose texture maps are severely damaged by JPEG and sub-sampling distortions. However, these interior parts are almost invisible due to the position of the camera in the videos. Hence, the subjective scores are rather good for these distorted models, whereas the image metrics predict very low-quality values. On the contrary, video-based metrics only take into account the visible parts and thus provide correct results (in particular Video-MS-SSIM). To verify this effect, we adopted a slightly different viewpoint for this particular model, in the non-shaded rendering, which improves the visibility of these interior parts. As expected, the results of the combined metrics are much better (see Table VIII). This observation argues for the integration of a visibility information in the combined metrics as an efficient way to overcome this drawback.\n\n\nValidation on Compound Distortions\n\nIn this section, we validate our objective metrics using a new set of compound geometry-texture distortions. We selected the Dwarf model, and we manually selected 36 distorted versions among the 12 \u00d7 8 = 96 possible combinations of the 12 geometry and 8 texture distortions detailed in Section 3.1, resulting in a new validation set of 36 models. Details about these mixed distortions are available in the supplementary material; some examples are shown in Figure 2, bottom right. As before, we created two sets of videos (each of 10 seconds duration) with and without shading, respectively. We performed a paired-comparison experiment to obtain the subjective scores. For this mixed-distortion setting, the  hypothesis Q D i > Q D j , \u2200 j > i from our sorting algorithm no longer holds. Hence, we implemented a more classical self-balancing binary tree, as in . The average number of comparisons for sorting the 36 distorted models was 140 (instead of 630 for the full comparison). Twenty observers took part in this new experiment, 19 rated 1 set and 1 rated both sets. We thus obtained 10 judgments for the shaded rendering and 11 for the non-shaded one. The average time to finish the experiment for one video set of 36 videos was 21 minutes. As for the previous experiment, the agreement is rather high (respectively, 0.75 and 0.71 for shaded and non-shaded settings). Detailed raw scores are included in the supplementary material.\n\nTo validate our metrics, we computed their optimal weights based on the models from our previous experiment (we excluded the Dwarf). Results are detailed in Table IX and Figure 8. Even for this difficult scenario (learning on single-type distortions and testing on compound distortions), our metrics offer excellent performance: 0.85 and 0.87 Spearman correlations for CM 2 , for both rendering settings, hence, demonstrating once again an excellent robustness.\n\n\nCONCLUSION AND PERSPECTIVES\n\nIn this work, we designed and constructed a new subjectively-rated database of textured 3D meshes. Our subjective study is based on a paired comparison protocol and involved more than 100 subjects. The database contains 136 distorted models (subject to geometry and texture distortions), which were evaluated within two rendering settings. The subjective results allowed us to draw interesting conclusions regarding the influence of the shape and texture content as well as the rendering on the perceptual impact of distortions. We then proposed new objective metrics for visual quality assessment of textured meshes as optimized linear combinations of mesh quality and texture quality. We used our subjective dataset to evaluate the performance of these metrics against state-of-the-art ones and explained the failure of metrics applied on the rendered images/videos. Such perceptually-validated metrics are of great interest for many applications such as 3D model simplification or compression, texture simplification, and so on. We also proposed a new measure for geometry quality assessment. Note that our dataset, subjective scores, and metric results will be made publicly available on-line. While our proposed metrics showed that they outperformed their counterparts for the task of textured mesh quality assessment, there is still room for improvement. For instance, when evaluating geometry and texture quality, we need to integrate some visibility information. Indeed, the interior parts of a 3D model do not contribute to its rendered visual appearance. As another example, regions in very convex areas will only be visible from a few viewpoints and thus will have little impact on subjective opinion. Another issue is the variation in texel size within a texture; indeed, different regions of a texture map are not necessarily mapped with the same size on the screen, and this effect should be taken into account. The angular resolution (in pixels per degree of visual angle) of the rendered scene is also important and may be integrated as a scale factor in the metric. Finally, a major issue is to incorporate texture coordinate distortions which are a common side effect of geometric changes. Based on our observation, even slight movements of texture seams may seriously harm visual appearance. Finally, we could conduct a comprehensive evaluation to determine, among the dozens of existing image metrics, the one best adapted to texture evaluation.\n\nFig. 1 .\n1Five models used in the subjective study (from left to right: Sport Car, Easter Island statue, squirrel, Hulk, and Dwarf).\n\nFig. 3 .\n3Illustration of our browser-based interface for the paired comparison task. Pairs of 3D models are presented as videos; the user selects the one (s)he prefers and then clicks on the Next button. The reference model can be displayed at any time by clicking on the See Original Model button below the Next button.\n\nFig. 4 .\n4Mean scores (averaged among the four strengths) for the five types of distortions and the five models for the rendering with shading. Higher scores mean better visual quality.\n\nFig. 5 .\n5Mean scores (averaged among the five types of distortions and the four models) for the two types of rendering: with shading (top-left light source, diffuse material) and without shading (diffuse albedo only).\n\nFig. 6 .\n6From left to right: Hulk model with 7-bit quantization (subjective score: 0.91), Original Hulk model, and Hulk model with Laplacian smoothing of four iterations (subjective score: 7.45).M r ). We first establish a correspondence between M d and M r (as in Lavou\u00e9 [2011]) before computing the mean curvature C on each vertex of M d and its corresponding curvature value\u0108 on the corresponding point of M r . For each vertex v from M d , we then compute the standard deviation \u03b4 h v of the local curvature differences in a connected Euclidean neighborhood of size h around v. In a similar way to the Mesh Structural Distortion Measure (MSDM2) [Lavou\u00e9 2011], local curvature differences are normalized by the maximum value. \u03b4 h v is computed as follows:\n\n\nMS-SSIM[Wang et al. 2003] 0.83 0.98 0.67 0.81 0.70 0.76 0.86 0.86 0.86 0.95 0.78 0.87\n\nFig. 7 .\n7Scatter plots of subjective scores versus objective metric values. Each point represents one distorted model. Fitted logistic curves are represented in black. ACM Transactions on Applied Perception, Vol. 14, No. 2, Article 11, Publication date: October 2016.\n\nTable I .\nIDetails on our 3D Models: Vertex Number, Texture Image Number, Sizes of Texture Images, Average Shape Curvature, Characteristics of the Texture Images, and Complexity of the Mapping (i.e., of Texture Seams)#Vertex \n#Texture \nTexture Size \nAverage Curv. \nText. Charac. \nMap. Complex. \n\nSquirrel \n6,185 \n1 \n2048\u00d72048 \nHigh \nHigh freq. & Noisy \nSimple \nHulk \n10,236 \n2 \n1,024\u00d71,024 & 512\u00d7512 \nHigh \nStructured \nSimple \nStatue \n104,019 \n4 \n355\u00d7226 to 4,096\u00d74,096 \nLow \nNoisy \nComplex \nSport Car 122,873 \n15 \n256\u00d7256 to 1,024\u00d7768 \nLow & sharp edges \nStructured \nSimple \nDwarf \n250,004 \n1 \n4,096\u00d74,096 \nIntermediate \nIntermediate \nComplex \n\n\n\nTable II .\nIIDetails on the Distortions Applied to Each Reference Model adjustment of distortion strength was also carried out for the LIVE Video Quality Database[Seshadrinathan et al. 2010]. Thus, we generated 100 distorted models (5 distortion types \u00d7 4 strengths \u00d7 5 reference models).Table IIprovides details on the distortion parameters, while Figure 2 illustrates some visual examples.ID \nDistortion type \nSquirrel \nHulk \nStatue \nSport Car \nDwarf \n\nL1 \nSmoothing \n1 iteration \n1 iteration \n10 iterations \n1 iteration \n15 iterations \n\nL2 \nSmoothing \n3 iterations \n2 iterations \n20 iterations \n2 iterations \n25 iterations \n\nL3 \nSmoothing \n5 iterations \n3 iterations \n30 iterations \n3 iterations \n40 iterations \n\nL4 \nSmoothing \n7 iterations \n4 iterations \n50 iterations \n4 iterations \n50 iterations \n\nSi1 \nSimplification \n50% removed \n30% removed 50% removed \n50% removed \n80% removed \n\nSi2 \nSimplification \n70% removed \n40% removed 70% removed \n60% removed \n92% removed \n\nSi3 \nSimplification \n75% removed \n50% removed 87.5% removed 75% removed \n97.5% removed \n\nSi4 \nSimplification \n87.5% removed 70% removed 95% removed \n87.5% removed 98.7% removed \n\nQ1 \nQuantization \n10 bits \n10 bits \n10 bits \n10 bits \n11 bits \n\nQ2 \nQuantization \n9 bits \n9 bits \n9 bits \n9 bits \n10 bits \n\nQ3 \nQuantization \n8 bits \n8 bits \n8 bits \n8 bits \n9 bits \n\nQ4 \nQuantization \n7 bits \n7 bits \n7 bits \n7 bits \n8 bits \n\nJ1 \nJPEG \n18% quality \n18% quality \n80% quality \n10% quality \n12% quality \n\nJ2 \nJPEG \n14% quality \n14% quality \n16% quality \n5% quality \n10% quality \n\nJ3 \nJPEG \n10% quality \n10% quality \n12% quality \n3% quality \n8% quality \n\nJ4 \nJPEG \n6% quality \n8% quality \n8% quality \n1% quality \n6% quality \n\nSu1 \nSub-sampling \n40% sampled \n40% sampled \n25% sampled \n50% sampled \n10% sampled \n\nSu2 \nSub-sampling \n30% sampled \n30% sampled \n20% sampled \n20% sampled \n8% sampled \n\nSu3 \nSub-sampling \n20% sampled \n20% sampled \n10% sampled \n10% sampled \n5% sampled \n\nSu4 \nSub-sampling \n10% sampled \n10% sampled \n5% sampled \n5% sampled \n3% sampled \n\n\n\nTable III .\nIIIAgreement between Observers (Kendall's W between Their Ranks) for Each Reference Model and Each Rendering ConditionRendering with shading \nRendering without shading \n\nObserver Number Kendall's W \np-value \nObserver Number \nKendall's W \np-value \n\nSquirrel \n11 \n0.69 \n<0.0001 \n11 \n0.71 \n<0.0001 \nHulk \n11 \n0.77 \n<0.0001 \n13 \n0.70 \n<0.0001 \nStatue \n11 \n0.83 \n<0.0001 \n15 \n0.76 \n<0.0001 \nSport Car \n11 \n0.74 \n<0.0001 \n11 \n0.76 \n<0.0001 \nDwarf \n11 \n0.72 \n<0.0001 \n12 \n0.77 \n<0.0001 \n\n\n\nTable IV .\nIVPerformance Comparison (Pearson r p and Spearman r p Correlations) of Several Geometric Metrics \non Our Subjective Database (Geometric Distortions Only). Rendering with Shading \n\nSquirrel \nHulk \nStatue \nSport Car \nDwarf \nAverage \n\nr p \nr s \nr p \nr s \nr p \nr s \nr p \nr s \nr p \nr s \nr p \nr s \nGeometry RMSE \n0.72 \n0.90 \n0.37 \n0.61 \n0.07 \n0.00 \n0.65 \n0.28 \n0.02 \n0.28 \n0.37 \n0.41 \nMSDM2 [Lavou\u00e9 2011] \n0.78 \n0.76 \n0.88 \n0.85 \n0.15 \n0.15 \n0.53 \n0.52 \n0.86 \n0.90 \n0.64 \n0.64 \nSDCD (Our Metric) \n0.60 \n0.55 \n0.84 \n0.80 \n0.64 \n0.64 \n0.40 \n0.41 \n0.89 \n0.86 \n0.67 \n0.65 \n\nTable V. Performance Comparison (Pearson r p and Spearman r p Correlations) of Several Image Metrics \non Our Subjective Database (Texture Distortions Only). Rendering with Shading \n\nSquirrel \nHulk \nStatue \nSport Car \nDwarf \nAverage \n\nr p \nr s \nr p \nr s \nr p \nr s \nr p \nr s \nr p \nr s \nr p \nr s \nImage RMSE \n0.73 0.86 0.21 0.28 0.39 0.50 0.93 1.00 0.65 0.73 0.58 0.67 \nSSIM \n\nTable VI .\nVI\u03b1 Values for Each Model and Each Rendering SettingSquirrel \nHulk \nStatue \nSport Car \nDwarf \n\n\u03b1CM 1 \n\u03b1CM 2 \n\u03b1CM 1 \n\u03b1CM 2 \n\u03b1CM 1 \n\u03b1CM 2 \n\u03b1CM 1 \n\u03b1CM 2 \n\u03b1CM 1 \n\u03b1CM 2 \nWith shading \n0.086 \n0.117 \n0.108 \n0.184 \n0.103 \n0.133 \n0.086 \n0.132 \n0.061 \n0.111 \nWithout shading \n0.061 \n0.109 \n0.026 \n0.108 \n0.061 \n0.118 \n0.061 \n0.109 \n0.061 \n0.109 \n\nTable VII. Performance Comparison (Pearson r p and Spearman r p Correlations, RMSE of the Residuals) of Several \nTextured Mesh Quality Metrics on Our Subjective Database. Rendering with Shading \n\nSquirrel \nHulk \nStatue \nSport Car \nDwarf \n\nr p \nr s \nRMS r p \nr s \nRMS r p \nr s \nRMS r p \nr s \nRMS r p \nr s \nRMS \nVideo-DCT \n0.12 0.09 4.76 \n0.18 0.36 4.99 \n0.26 0.30 5.04 \n0.62 0.67 3.97 \n0.24 0.25 4.75 \nVideo-PSNR \n0.22 0.26 4.68 \n0.33 0.36 4.79 \n0.21 0.26 5.12 \n0.67 0.70 3.68 \n0.31 0.32 4.63 \nVideo-MS-SSIM 0.24 0.39 4.64 \n0.17 0.41 5.00 \n0.25 0.42 5.05 \n0.67 0.72 3.64 \n0.38 0.40 4.50 \nFQM \n0.80 0.85 2.83 \n0.41 0.56 4.55 \n0.26 0.18 5.07 \n0.67 0.47 3.86 \n0.38 0.41 4.53 \nCM 1 (Our Metric) 0.82 0.82 2.24 0.81 0.81 2.92 0.36 0.30 4.93 \n0.70 0.60 3.58 0.56 0.70 3.95 \nCM 2 (Our Metric) 0.78 0.72 2.80 \n0.73 0.72 3.48 \n0.62 0.68 3.94 0.68 0.51 3.72 \n0.58 0.70 3.87 \n\n\n\n\nTable VI details the computed \u03b1 values.\n\nTable VIII .\nVIIIPerformance Comparison (Pearson r p and Spearman r p Correlations, RMSE of the Residuals) of Several Textured Mesh Quality Metrics on Our Subjective Database. Rendering without ShadingSquirrel \nHulk \nStatue \nSport Car \nDwarf \n\nr p \nr s \nRMS \nr p \nr s \nRMS \nr p \nr s \nRMS \nr p \nr s \nRMS \nr p \nr s \nRMS \nVideo-DCT \n\u22120.21 \u22120.26 4.76 0.22 0.25 4.72 0.34 0.24 4.74 0.58 0.42 4.23 0.24 0.21 4.89 \nVideo-PSNR \n\u22120.13 \u22120.04 4.82 0.35 0.39 4.50 0.13 0.22 4.97 0.51 0.56 4.32 0.31 0.29 4.78 \nVideo-MS-SSIM \u22120.05 \n0.14 4.86 0.28 0.61 4.64 0.34 0.37 4.72 0.67 0.69 3.77 0.65 0.63 3.80 \nFQM \n0.69 \n0.71 3.47 0.46 0.62 4.17 0.48 0.31 4.44 0.73 0.82 3.34 0.56 0.69 4.15 \nCM 1 (Our Metric) \n0.80 \n0.88 2.38 0.69 0.80 3.29 0.47 0.41 4.53 0.77 0.90 2.86 0.73 0.81 3.23 \nCM 2 (Our Metric) \n0.79 \n0.81 2.54 0.72 0.82 2.93 0.63 0.69 3.81 0.74 0.88 3.12 0.76 0.88 3.04 \n\n\n\nTable IX .\nIXPerformance Comparison on the Compound Distortion DatasetFig. 8. Scatter plots of subjective scores versus objective metric values for the compound distortion dataset. Each point represents one distorted model. Fitted logistic curves are represented in black.With shading \nWithout shading \n\nr p \nr s \nRMS \nr p \nr s \nRMS \nVideo-DCT \n0.32 \n0.50 \n7.81 \n0.35 \n0.32 \n8.23 \nVideo-PSNR \n0.33 \n0.58 \n7.32 \n0.40 \n0.33 \n8.05 \nVideo-MS-SSIM \n0.67 \n0.66 \n6.79 \n0.68 \n0.67 \n6.42 \nFQM \n0.64 \n0.66 \n6.90 \n0.76 \n0.77 \n5.73 \nCM 1 (Our Metric) \n0.74 \n0.77 \n6.01 \n0.85 \n0.86 \n4.47 \nCM 2 (Our Metric) \n0.80 \n0.85 \n5.33 \n0.86 \n0.87 \n4.44 \n\n\nACM Transactions on Applied Perception, Vol. 14, No. 2, Article 11, Publication date: October 2016.\nhttp://compression.ru/video/quality measure/video measurement tool en.html. ACM Transactions on Applied Perception, Vol. 14, No. 2, Article 11, Publication date: October 2016.\nACKNOWLEDGMENTSWe thank Massimiliano Corsini and the Visual Computing Laboratory of ISTI-CNR for the Dwarf model, as well as Mark Pauly and the EPFL Computer Graphics and Geometry Laboratory for the Squirrel and Statue models. This work is supported in part by the China Scholarship Council and Alberta Innovates Techology Futures.\nVideo quality assessment for computer graphics applications. Tun\u00e7 Ozan Ayd\u0131n, Karol Martin\u010dad\u00edk, Hans-Peter Myszkowski, Seidel, 10.1145/1882261.1866187ACM Transactions on Graphics. 296Tun\u00e7 Ozan Ayd\u0131n, Martin\u010cad\u00edk, Karol Myszkowski, and Hans-Peter Seidel. 2010. Video quality assessment for computer graph- ics applications. ACM Transactions on Graphics 29, 6 (Dec 2010), 1. DOI:http://dx.doi.org/10.1145/1882261.1866187\n\nLearning to predict localized distortions in rendered images. Robert Martin\u010dad\u00edk, Rafal Herzog, Radosaw Mantiuk, Karol Mantiuk, Hans-Peter Myszkowski, Seidel, Pacific Graphics. 32Martin\u010cad\u00edk, Robert Herzog, Rafal Mantiuk, Radosaw Mantiuk, Karol Myszkowski, and Hans-Peter Seidel. 2013. Learning to predict localized distortions in rendered images. In Pacific Graphics, Vol. 32.\n\nNew measurements reveal weaknesses of image quality metrics in evaluating graphics artifacts. Robert Martin\u010dad\u00edk, Rafal Herzog, Karol Mantiuk, Hans-Peter Myszkowski, Seidel, ACM Transactions on Graphics. 31147Martin\u010cad\u00edk, Robert Herzog, Rafal Mantiuk, Karol Myszkowski, and Hans-Peter Seidel. 2012. New measurements reveal weaknesses of image quality metrics in evaluating graphics artifacts. ACM Transactions on Graphics 31, 6 (2012), Article 147.\n\nVSNR: A wavelet-based visual signal-to-noise ratio for natural images. M Chandler, S S Hemami, IEEE Transactions on Image Processing. 16M. Chandler and S. S. Hemami. 2007. VSNR: A wavelet-based visual signal-to-noise ratio for natural images. IEEE Transac- tions on Image Processing 16, 9 (Sep 2007), 2284-2298.\n\nPerceptually optimized 3-D transmission over wireless networks. I Cheng, A Basu, 10.1109/TMM.2006.886291IEEE Transactions on Multimedia. 9I. Cheng and A. Basu. 2007. Perceptually optimized 3-D transmission over wireless networks. IEEE Transactions on Multimedia 9, 2 (Feb 2007), 386-396. DOI:http://dx.doi.org/10.1109/TMM.2006.886291\n\nWatermarked 3-D mesh quality assessment. Massimiliano Corsini, Elisa Drelie Gelasca, Touradj Ebrahimi, Mauro Barni, IEEE Transactions on Multimedia. 9Massimiliano Corsini, Elisa Drelie Gelasca, Touradj Ebrahimi, and Mauro Barni. 2007. Watermarked 3-D mesh quality assess- ment. IEEE Transactions on Multimedia 9, 2 (Feb 2007), 247-256.\n\nPerceptual metrics for static and dynamic triangle meshes. M Corsini, M C Larabi, G Lavou\u00e9, O Petrik, L V\u00e1\u0161a, K Wang, Computer Graphics Forum. 321M. Corsini, M. C. Larabi, G. Lavou\u00e9, O. Petrik, L. V\u00e1\u0161a, and K. Wang. 2013. Perceptual metrics for static and dynamic triangle meshes. Computer Graphics Forum 32, 1 (Feb 2013), 101-125.\n\nThe visible differences predictor: An algorithm for the assessment of image fidelity. Scott Daly, Digital Images and Human Vision. Andrew B. WatsonCambridgeMIT PressScott Daly. 1993. The visible differences predictor: An algorithm for the assessment of image fidelity. In Digital Images and Human Vision, Andrew B. Watson (Ed.). MIT Press, Cambridge, 179-206.\n\nA model of visual masking for computer graphics. J Ferwerda, S Pattanaik, P Shirley, D Greenberg, ACM SIG-GRAPH. J. Ferwerda, S. Pattanaik, P. Shirley, and D. Greenberg. 1997. A model of visual masking for computer graphics. In ACM SIG- GRAPH. 143-152.\n\nSurface simplification using quadric error metrics. M Garland, P.-S Heckbert, ACM SIGGRAPH. M. Garland and P.-S. Heckbert. 1997. Surface simplification using quadric error metrics. In ACM SIGGRAPH. 209-216.\n\nRank correlation methods. Z Govindarajulu, M Kendall, J D Gibbons, 10.2307/1269571Technometrics. 34108Z. Govindarajulu, M. Kendall, and J. D. Gibbons. 1992. Rank correlation methods. Technometrics 34, 1 (Feb 1992), 108. DOI:http://dx.doi.org/10.2307/1269571\n\nEvaluating texture compression masking effects using objective image quality assessment metrics. Wesley Griffin, Marc Olano, 10.1109/TVCG.2015.2429576IEEE Transactions on Visualization and Computer Graphics. 21Wesley Griffin and Marc Olano. 2015. Evaluating texture compression masking effects using objective image quality assess- ment metrics. IEEE Transactions on Visualization and Computer Graphics 21, 8 (2015), 970-079. DOI:http://dx.doi.org/ 10.1109/TVCG.2015.2429576\n\nEvaluating the local visibility of geometric artifacts. Jinjiang Guo, Vincent Vidal, Atilla Baskurt, Guillaume Lavou\u00e9, Proceedings of the ACM Symposium in Applied Perception. the ACM Symposium in Applied PerceptionJinjiang Guo, Vincent Vidal, Atilla Baskurt, and Guillaume Lavou\u00e9. 2015. Evaluating the local visibility of geometric artifacts. In Proceedings of the ACM Symposium in Applied Perception.\n\nNoRM: No-reference image quality metric for realistic image synthesis. Robert Herzog, Tun\u00e7 O Martin\u010dad\u00edk, Kwang In Ayd\u0131n, Karol Kim, Hans-P Myszkowski, Seidel, 10.1111/j.1467-8659.2012.03055.xACM Siggraph. 31Spectral compression of mesh geometryRobert Herzog, Martin\u010cad\u00edk, Tun\u00e7 O. Ayd\u0131n, Kwang In Kim, Karol Myszkowski, and Hans-P. Seidel. 2012. NoRM: No-reference image quality metric for realistic image synthesis. Computer Graphics Forum 31, 2 (Pt 3), 545-554. DOI:http://dx.doi.org/10.1111/j.1467-8659.2012.03055.x Z. Karni and C. Gotsman. 2000. Spectral compression of mesh geometry. In ACM Siggraph. 279-286.\n\nA multiscale metric for 3D mesh visual quality assessment. Guillaume Lavou\u00e9, Computer Graphics Forum. 30Guillaume Lavou\u00e9. 2011. A multiscale metric for 3D mesh visual quality assessment. Computer Graphics Forum 30, 5 (2011), 1427-1437.\n\nOn the efficiency of image metrics for evaluating the visual quality of 3D models. G Lavou\u00e9, M C Larabi, Libor V\u00e1\u0161a, 10.1109/TVCG.2015.2480079IEEE Transactions on Visualization and Computer Graphics. 22G. Lavou\u00e9, M. C. Larabi, and Libor V\u00e1\u0161a. 2016. On the efficiency of image metrics for evaluating the visual quality of 3D models. IEEE Transactions on Visualization and Computer Graphics 22, 8 (2016), 1987-1999. DOI:http://dx.doi.org/10.1109/ TVCG.2015.2480079\n\nQuality assessment in computer graphics. Guillaume Lavou\u00e9, Rafa Mantiuk, 10.1007/978-3-319-10368-69Visual Signal Quality Assessment: Quality of Experience (QoE. Guillaume Lavou\u00e9 and Rafa Mantiuk. 2015. Quality assessment in computer graphics. Visual Signal Quality Assessment: Qual- ity of Experience (QoE) (2015), 243-286. DOI:http://dx.doi.org/10.1007/978-3-319-10368-6 9\n\nEvaluation of tone mapping operators using a high dynamic range display. Patrick Ledda, Alan Chalmers, Tom Troscianko, Helge Seetzen, ACM Transactions on Graphics. 243640Patrick Ledda, Alan Chalmers, Tom Troscianko, and Helge Seetzen. 2005. Evaluation of tone mapping operators using a high dynamic range display. ACM Transactions on Graphics 24, 3 (Jul 2005), 640.\n\nThe use of psychophysical data and models in the analysis of display system performance. Jeffrey Lubin, Digital Images and Human Vision. A. B. WatsonJeffrey Lubin. 1993. The use of psychophysical data and models in the analysis of display system performance. In Digital Images and Human Vision, A. B. Watson (Ed.). 163-178.\n\nThe effects of a visual fidelity criterion of the encoding of images. J Mannos, D Sakrison, 10.1109/tit.1974.1055250IEEE Transactions on Information Theory. 204J. Mannos and D. Sakrison. 1974. The effects of a visual fidelity criterion of the encoding of images. IEEE Transactions on Information Theory 20, 4 (Jul 1974), 525-536. DOI:http://dx.doi.org/10.1109/tit.1974.1055250\n\nHDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. Rafal Mantiuk, Kil Joong Kim, Allan G Rempel, Wolfgang Heidrich, no. 40ACM Transactions on Graphics (Proc. of SIGGRAPH'11). 30Rafal Mantiuk, Kil Joong Kim, Allan G. Rempel, and Wolfgang Heidrich. 2011. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. In ACM Transactions on Graphics (Proc. of SIGGRAPH'11) 30, 4 (2011), Article no. 40.\n\nComparison of four subjective methods for image quality assessment. Rafa K Mantiuk, Anna Tomaszewska, Radosaw Mantiuk, 10.1111/j.1467-8659.2012.03188.xComputer Graphics Forum. 31Rafa K. Mantiuk, Anna Tomaszewska, and Radosaw Mantiuk. 2012. Comparison of four subjective methods for image quality assessment. Computer Graphics Forum 31, 8 (Dec 2012), 2478-2491. DOI:http://dx.doi.org/10.1111/j.1467-8659.2012.03188.x\n\nJust noticeable distortion profile for flat-shaded 3D mesh surfaces. Georges Nader, Kai Wang, H Franck, Florent Dupont, 10.1109/TVCG.2015.2507578IEEE Transactions on Visualization and Computer Graphics. Georges Nader, Kai Wang, H. Franck, and Florent Dupont. 2016. Just noticeable distortion profile for flat-shaded 3D mesh sur- faces. IEEE Transactions on Visualization and Computer Graphics (2016). DOI:http://dx.doi.org/10.1109/TVCG.2015.2507578\n\nThe assumed light direction for perceiving shape from shading. J P Shea, M S Banks, M Agrawala, Proceedings of the Symposium on Applied Perception in Graphics and Visualization. the Symposium on Applied Perception in Graphics and VisualizationJ. P. O'Shea, M. S. Banks, and M. Agrawala. 2008. The assumed light direction for perceiving shape from shading. In Proceedings of the Symposium on Applied Perception in Graphics and Visualization.\n\nQuality metric for approximating subjective evaluation of 3-D objects. Yixin Pan, Irene Cheng, Anup Basu, IEEE Transactions on Multimedia. 72Yixin Pan, Irene Cheng, and Anup Basu. 2005. Quality metric for approximating subjective evaluation of 3-D objects. IEEE Transactions on Multimedia 7, 2 (Apr 2005), 269-279.\n\nPerceptually guided polygon reduction. Lijun Qu, G W Meyer, 10.1109/TVCG.2008.51IEEE Transactions on Visualization and Computer Graphics. 14Lijun Qu and G. W. Meyer. 2008. Perceptually guided polygon reduction. IEEE Transactions on Visualization and Computer Graphics 14, 5 (2008), 1015-1029. DOI:http://dx.doi.org/10.1109/TVCG.2008.51\n\nAre image quality metrics adequate to evaluate the quality of geometric objects?. Bernice E Rogowitz, H Rushmeier, Proceedings of SPIE. SPIEBernice E. Rogowitz and H. Rushmeier. 2001. Are image quality metrics adequate to evaluate the quality of geometric objects? In Proceedings of SPIE. 340-348.\n\nStudy of subjective and objective quality assessment of video. Kalpana Seshadrinathan, Rajiv Soundararajan, Alan Conrad Bovik, Lawrence K Cormack, 10.1109/TIP.2010.2042111IEEE Transactions on Image Processing. 196Kalpana Seshadrinathan, Rajiv Soundararajan, Alan Conrad Bovik, and Lawrence K. Cormack. 2010. Study of subjec- tive and objective quality assessment of video. IEEE Transactions on Image Processing 19, 6 (Jun 2010), 1427-41. DOI:http://dx.doi.org/10.1109/TIP.2010.2042111\n\nImage information and visual quality. H R Sheikh, A C Bovik, IEEE Transactions on Image Processing. 152H. R. Sheikh and A. C. Bovik. 2006. Image information and visual quality. IEEE Transactions on Image Processing 15, 2 (Feb 2006), 430-444.\n\nEfficient method for paired comparison. D , Amnon Silverstein, Joyce E Farrell, 10.1117/1.1344187Journal of Electronic Imaging. 10394D. Amnon Silverstein and Joyce E. Farrell. 2001. Efficient method for paired comparison. Journal of Electronic Imaging 10, 2 (2001), 394. DOI:http://dx.doi.org/10.1117/1.1344187\n\nPACKMAN: High-quality, low-complexity texture compression for mobile phones. J Str\u00f6m, T Akenine-M\u00f6ller, 10.1145/1071866.1071877Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware. the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics HardwareJ. Str\u00f6m and T. Akenine-M\u00f6ller. 2005. i PACKMAN: High-quality, low-complexity texture compression for mobile phones. In Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware. 177-182. DOI:http://dx.doi.org/ 10.1145/1071866.1071877\n\nWhere is the sun?. Jennifer Sun, Pietro Perona, Nature Neuroscience. Jennifer Sun and Pietro Perona. 1998. Where is the sun? Nature Neuroscience (1998), 183-184. Retrieved from http://www. nature.com/neuro/journal/v1/n3/abs/nn0798.\n\nA signal processing approach to fair surface design. G Taubin, ACM Siggraph. G. Taubin. 1995. A signal processing approach to fair surface design. In ACM Siggraph. 351-358.\n\nA law of comparative judgments. L L Thurstone, Psychological Review. 34L. L. Thurstone. 1927. A law of comparative judgments. Psychological Review 34 (1927), 273-286.\n\nFQM. Dihong Tian, Ghassan Alregib, 10.1145/1027527.1027684Proceedings of the 12th Annual ACM International Conference on Multimedia -MULTIMEDIA 04. the 12th Annual ACM International Conference on Multimedia -MULTIMEDIA 04ACMDihong Tian and Ghassan AlRegib. 2004. FQM. In Proceedings of the 12th Annual ACM International Conference on Multimedia -MULTIMEDIA 04. Association for Computing Machinery (ACM). DOI:http://dx.doi.org/10.1145/1027527.1027684\n\nBatex3: Bit allocation for progressive transmission of textured 3-D models. Dihong Tian, G Alregib, IEEE Transactions on Circuits and Systems for Video Technology. 18Dihong Tian and G. AlRegib. 2008. Batex3: Bit allocation for progressive transmission of textured 3-D models. IEEE Transac- tions on Circuits and Systems for Video Technology 18, 1 (2008), 23-35.\n\nA curvature tensor distance for mesh visual quality assessment. Fakhri Torkhani, Kai Wang, Jean-Marc Chassery, ; \u2022 J Guo, Proceedings of the International Conference on Computer Vision and Graphics. the International Conference on Computer Vision and Graphics11Fakhri Torkhani, Kai Wang, and Jean-Marc Chassery. 2012. A curvature tensor distance for mesh visual quality assessment. In Proceedings of the International Conference on Computer Vision and Graphics. 11:20 \u2022 J. Guo et al.\n\nDihedral angle mesh error: A fast perception correlated distortion measure for fixed connectivity triangle meshes. Libor V\u00e1\u0161a, Computer Graphics Forum. 31Libor V\u00e1\u0161a and Jan Rus. 2012. Dihedral angle mesh error: A fast perception correlated distortion measure for fixed connectivity triangle meshes. Computer Graphics Forum 31, 5 (2012), 1715-1724.\n\nA fast roughness-based approach to the assessment of 3D mesh visual quality. Kai Wang, Fakhri Torkhani, Annick Montanvert, Computers & Graphics. 367Kai Wang, Fakhri Torkhani, and Annick Montanvert. 2012. A fast roughness-based approach to the assessment of 3D mesh visual quality. Computers & Graphics 36, 7 (2012), 808-818.\n\nImage quality assessment: From error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, 10.1109/tip.2003.819861IEEE Transactions on Image Processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. 2004. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (Apr 2004), 600-612. DOI:http://dx.doi.org/10.1109/tip.2003.819861\n\n. Zhou Wang, Alan C Bovik, 10.2200/S00010ED1V01Y200508IVM003Modern Image Quality Assessment. 2Morgan & Claypool PublishersZhou Wang and Alan C. Bovik. 2006. Modern Image Quality Assessment. Vol. 2. Morgan & Claypool Publishers. DOI:http://dx. doi.org/10.2200/S00010ED1V01Y200508IVM003\n\nInformation content weighting for perceptual image quality assessment. Zhou Wang, Qiang Li, 10.1109/tip.2010.2092435IEEE Transactions on Image Processing. 20Zhou Wang and Qiang Li. 2011. Information content weighting for perceptual image quality assessment. IEEE Transactions on Image Processing 20, 5 (May 2011), 1185-1198. DOI:http://dx.doi.org/10.1109/tip.2010.2092435\n\nMultiscale structural similarity for image quality assessment. Z Wang, E P Simoncelli, A C Bovik, 10.1109/ACSSC.2003.1292216IEEE Asilomar Conference on Signals, Systems and Computers. 2Z. Wang, E. P. Simoncelli, and A. C. Bovik. 2003. Multiscale structural similarity for image quality assessment. IEEE Asilomar Conference on Signals, Systems and Computers 2, 1 (2003), 1398-1402. DOI:http://dx.doi.org/10.1109/ACSSC.2003.1292216\n\nMeasuring and predicting visual fidelity. Benjamin Watson, Alinda Friedman, Aaron Mcgaffey, ACM SIGGRAPH. Benjamin Watson, Alinda Friedman, and Aaron McGaffey. 2001. Measuring and predicting visual fidelity. In ACM SIGGRAPH. 213-220.\n\nDCT-based Video Quality Evaluation. Feng Xiao, Stanford UniversityTechnical ReportFeng Xiao. 2000. DCT-based Video Quality Evaluation. Technical Report. Stanford University. Retrieved from http:// compression.ru/video/quality.\n\nOptimized mesh and texture multiplexing for progressive textured model transmission. Sheng Yang, Chao-Hua Lee, C C J Kuo, 10.1145/1027527.1027683Proceedings of the ACM Multimedia Conference. 676-683. the ACM Multimedia Conference. 676-683Sheng Yang, Chao-Hua Lee, and C. C. J. Kuo. 2004. Optimized mesh and texture multiplexing for progressive textured model transmission. In Proceedings of the ACM Multimedia Conference. 676-683. DOI:http://dx.doi.org/10.1145/1027527.1027683\n\nObjective quality assessment of tone-mapped images. Hojatollah Yeganeh, Zhou Wang, 10.1109/TIP.2012.2221725IEEE Transactions on Image Processing. 22Hojatollah Yeganeh and Zhou Wang. 2013. Objective quality assessment of tone-mapped images. IEEE Transactions on Image Processing 22, 2 (Feb 2013), 657-67. DOI:http://dx.doi.org/10.1109/TIP.2012.2221725\n\nA comprehensive evaluation of full reference image quality assessment algorithms. L Zhang, Proceedings of the International Conference on Image Processing (ICIP. the International Conference on Image Processing (ICIPL. Zhang. 2012. A comprehensive evaluation of full reference image quality assessment algorithms. In Proceedings of the Inter- national Conference on Image Processing (ICIP). 1477-1480.\n", "annotations": {"author": "[{\"end\":101,\"start\":88},{\"end\":116,\"start\":102},{\"end\":130,\"start\":117},{\"end\":145,\"start\":131},{\"end\":158,\"start\":146},{\"end\":169,\"start\":159},{\"end\":185,\"start\":170},{\"end\":196,\"start\":186},{\"end\":226,\"start\":197},{\"end\":346,\"start\":227},{\"end\":376,\"start\":347}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":97},{\"end\":115,\"start\":110},{\"end\":129,\"start\":126},{\"end\":144,\"start\":139},{\"end\":157,\"start\":152},{\"end\":168,\"start\":164},{\"end\":184,\"start\":177}]", "author_first_name": "[{\"end\":96,\"start\":88},{\"end\":109,\"start\":102},{\"end\":125,\"start\":117},{\"end\":138,\"start\":131},{\"end\":151,\"start\":146},{\"end\":163,\"start\":159},{\"end\":176,\"start\":170},{\"end\":195,\"start\":186}]", "author_affiliation": "[{\"end\":225,\"start\":198},{\"end\":345,\"start\":228},{\"end\":375,\"start\":348}]", "title": "[{\"end\":73,\"start\":1},{\"end\":449,\"start\":377}]", "venue": "[{\"end\":464,\"start\":451}]", "abstract": "[{\"end\":1988,\"start\":714}]", "bib_ref": "[{\"end\":2725,\"start\":2721},{\"end\":2734,\"start\":2725},{\"end\":2745,\"start\":2734},{\"end\":2759,\"start\":2745},{\"end\":2770,\"start\":2759},{\"end\":2780,\"start\":2770},{\"end\":2801,\"start\":2780},{\"end\":2803,\"start\":2801},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4388,\"start\":4357},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4963,\"start\":4944},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4985,\"start\":4963},{\"end\":5002,\"start\":4985},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5185,\"start\":5172},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5203,\"start\":5185},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5578,\"start\":5561},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5835,\"start\":5818},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7404,\"start\":7379},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7590,\"start\":7569},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7933,\"start\":7907},{\"end\":8252,\"start\":8153},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8580,\"start\":8569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8633,\"start\":8622},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8686,\"start\":8665},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8976,\"start\":8950},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9373,\"start\":9356},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9431,\"start\":9413},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9480,\"start\":9462},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9587,\"start\":9564},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9740,\"start\":9729},{\"end\":10021,\"start\":10015},{\"end\":10656,\"start\":10650},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10869,\"start\":10856},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10969,\"start\":10950},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11175,\"start\":11154},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11371,\"start\":11353},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11477,\"start\":11457},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11673,\"start\":11661},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11711,\"start\":11692},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12055,\"start\":12038},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12249,\"start\":12229},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12804,\"start\":12781},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12821,\"start\":12804},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12837,\"start\":12821},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13004,\"start\":12987},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13201,\"start\":13180},{\"end\":13279,\"start\":13259},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13898,\"start\":13880},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13927,\"start\":13903},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14038,\"start\":14020},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14126,\"start\":14100},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14200,\"start\":14176},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14234,\"start\":14217},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14613,\"start\":14591},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14637,\"start\":14618},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17184,\"start\":17157},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17243,\"start\":17231},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18201,\"start\":18181},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18301,\"start\":18280},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18559,\"start\":18542},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18729,\"start\":18700},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18949,\"start\":18928},{\"end\":19153,\"start\":19135},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21272,\"start\":21258},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24320,\"start\":24301},{\"end\":24622,\"start\":24619},{\"end\":24625,\"start\":24622},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24907,\"start\":24892},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25544,\"start\":25518},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30698,\"start\":30685},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30713,\"start\":30698},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31329,\"start\":31316},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32836,\"start\":32818},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33127,\"start\":33115},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33308,\"start\":33290},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":33339,\"start\":33322},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33644,\"start\":33624},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33676,\"start\":33665},{\"end\":35630,\"start\":35611},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36064,\"start\":36053},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":45950,\"start\":45932},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":47119,\"start\":47092}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44431,\"start\":44298},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44754,\"start\":44432},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44941,\"start\":44755},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45161,\"start\":44942},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45922,\"start\":45162},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46010,\"start\":45923},{\"attributes\":{\"id\":\"fig_10\"},\"end\":46280,\"start\":46011},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46928,\"start\":46281},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48958,\"start\":46929},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49453,\"start\":48959},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50403,\"start\":49454},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":51618,\"start\":50404},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51660,\"start\":51619},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":52527,\"start\":51661},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":53161,\"start\":52528}]", "paragraph": "[{\"end\":4670,\"start\":2004},{\"end\":6458,\"start\":4672},{\"end\":6919,\"start\":6460},{\"end\":7405,\"start\":6936},{\"end\":10178,\"start\":7448},{\"end\":11921,\"start\":10221},{\"end\":12604,\"start\":11923},{\"end\":14549,\"start\":12656},{\"end\":14896,\"start\":14551},{\"end\":15471,\"start\":14922},{\"end\":16369,\"start\":15494},{\"end\":16955,\"start\":16371},{\"end\":16973,\"start\":16957},{\"end\":17185,\"start\":16975},{\"end\":17245,\"start\":17187},{\"end\":17424,\"start\":17269},{\"end\":17936,\"start\":17426},{\"end\":17961,\"start\":17945},{\"end\":18662,\"start\":17986},{\"end\":19646,\"start\":18664},{\"end\":19881,\"start\":19648},{\"end\":20214,\"start\":19883},{\"end\":21469,\"start\":20241},{\"end\":22942,\"start\":21471},{\"end\":23695,\"start\":22959},{\"end\":24508,\"start\":23716},{\"end\":25199,\"start\":24537},{\"end\":26010,\"start\":25227},{\"end\":27022,\"start\":26012},{\"end\":27246,\"start\":27024},{\"end\":27530,\"start\":27248},{\"end\":27860,\"start\":27532},{\"end\":28742,\"start\":27862},{\"end\":29257,\"start\":28811},{\"end\":30548,\"start\":29306},{\"end\":30915,\"start\":30550},{\"end\":31686,\"start\":31005},{\"end\":31924,\"start\":31717},{\"end\":32373,\"start\":31990},{\"end\":33560,\"start\":32410},{\"end\":34487,\"start\":33562},{\"end\":34878,\"start\":34521},{\"end\":35489,\"start\":34908},{\"end\":35604,\"start\":35531},{\"end\":36239,\"start\":35606},{\"end\":36779,\"start\":36241},{\"end\":37923,\"start\":36781},{\"end\":38814,\"start\":37925},{\"end\":39861,\"start\":38816},{\"end\":41337,\"start\":39900},{\"end\":41800,\"start\":41339},{\"end\":44297,\"start\":41832}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24536,\"start\":24509},{\"attributes\":{\"id\":\"formula_1\"},\"end\":31004,\"start\":30916},{\"attributes\":{\"id\":\"formula_2\"},\"end\":31716,\"start\":31687},{\"attributes\":{\"id\":\"formula_3\"},\"end\":31989,\"start\":31925},{\"attributes\":{\"id\":\"formula_4\"},\"end\":34907,\"start\":34879}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16665,\"start\":16633},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23691,\"start\":23682},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25598,\"start\":25589},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25847,\"start\":25838},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30080,\"start\":30072},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34084,\"start\":34076},{\"end\":34381,\"start\":34373},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":39716,\"start\":39706},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":41504,\"start\":41496}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2002,\"start\":1990},{\"attributes\":{\"n\":\"2.\"},\"end\":6934,\"start\":6922},{\"attributes\":{\"n\":\"2.1\"},\"end\":7446,\"start\":7408},{\"attributes\":{\"n\":\"2.2\"},\"end\":10219,\"start\":10181},{\"attributes\":{\"n\":\"2.3\"},\"end\":12654,\"start\":12607},{\"attributes\":{\"n\":\"3.\"},\"end\":14920,\"start\":14899},{\"attributes\":{\"n\":\"3.1\"},\"end\":15492,\"start\":15474},{\"end\":17267,\"start\":17248},{\"end\":17943,\"start\":17939},{\"attributes\":{\"n\":\"3.2\"},\"end\":17984,\"start\":17964},{\"attributes\":{\"n\":\"3.3\"},\"end\":20239,\"start\":20217},{\"attributes\":{\"n\":\"3.4\"},\"end\":22957,\"start\":22945},{\"attributes\":{\"n\":\"3.5\"},\"end\":23714,\"start\":23698},{\"attributes\":{\"n\":\"3.6\"},\"end\":25225,\"start\":25202},{\"attributes\":{\"n\":\"4.\"},\"end\":28809,\"start\":28745},{\"attributes\":{\"n\":\"4.1\"},\"end\":29304,\"start\":29260},{\"attributes\":{\"n\":\"4.2\"},\"end\":32408,\"start\":32376},{\"attributes\":{\"n\":\"4.3\"},\"end\":34519,\"start\":34490},{\"attributes\":{\"n\":\"4.4\"},\"end\":35529,\"start\":35492},{\"attributes\":{\"n\":\"4.5\"},\"end\":39898,\"start\":39864},{\"attributes\":{\"n\":\"5.\"},\"end\":41830,\"start\":41803},{\"end\":44307,\"start\":44299},{\"end\":44441,\"start\":44433},{\"end\":44764,\"start\":44756},{\"end\":44951,\"start\":44943},{\"end\":45171,\"start\":45163},{\"end\":46020,\"start\":46012},{\"end\":46291,\"start\":46282},{\"end\":46940,\"start\":46930},{\"end\":48971,\"start\":48960},{\"end\":49465,\"start\":49455},{\"end\":50415,\"start\":50405},{\"end\":51674,\"start\":51662},{\"end\":52539,\"start\":52529}]", "table": "[{\"end\":46928,\"start\":46499},{\"end\":48958,\"start\":47321},{\"end\":49453,\"start\":49090},{\"end\":50403,\"start\":49468},{\"end\":51618,\"start\":50468},{\"end\":52527,\"start\":51863},{\"end\":53161,\"start\":52801}]", "figure_caption": "[{\"end\":44431,\"start\":44309},{\"end\":44754,\"start\":44443},{\"end\":44941,\"start\":44766},{\"end\":45161,\"start\":44953},{\"end\":45922,\"start\":45173},{\"end\":46010,\"start\":45925},{\"end\":46280,\"start\":46022},{\"end\":46499,\"start\":46293},{\"end\":47321,\"start\":46943},{\"end\":49090,\"start\":48975},{\"end\":50468,\"start\":50418},{\"end\":51660,\"start\":51621},{\"end\":51863,\"start\":51679},{\"end\":52801,\"start\":52542}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15631,\"start\":15623},{\"end\":19661,\"start\":19655},{\"end\":20095,\"start\":20087},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20912,\"start\":20904},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26214,\"start\":26206},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28004,\"start\":27996},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30038,\"start\":30030},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":36759,\"start\":36751},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":37665,\"start\":37657},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":38055,\"start\":38047},{\"end\":40365,\"start\":40357},{\"end\":41517,\"start\":41509}]", "bib_author_first_name": "[{\"end\":53853,\"start\":53848},{\"end\":53877,\"start\":53867},{\"end\":54259,\"start\":54253},{\"end\":54278,\"start\":54273},{\"end\":54294,\"start\":54287},{\"end\":54309,\"start\":54304},{\"end\":54329,\"start\":54319},{\"end\":54670,\"start\":54664},{\"end\":54689,\"start\":54684},{\"end\":54703,\"start\":54698},{\"end\":54723,\"start\":54713},{\"end\":55092,\"start\":55091},{\"end\":55104,\"start\":55103},{\"end\":55106,\"start\":55105},{\"end\":55398,\"start\":55397},{\"end\":55407,\"start\":55406},{\"end\":55721,\"start\":55709},{\"end\":55736,\"start\":55731},{\"end\":55743,\"start\":55737},{\"end\":55760,\"start\":55753},{\"end\":55776,\"start\":55771},{\"end\":56065,\"start\":56064},{\"end\":56076,\"start\":56075},{\"end\":56078,\"start\":56077},{\"end\":56088,\"start\":56087},{\"end\":56098,\"start\":56097},{\"end\":56108,\"start\":56107},{\"end\":56116,\"start\":56115},{\"end\":56429,\"start\":56424},{\"end\":56749,\"start\":56748},{\"end\":56761,\"start\":56760},{\"end\":56774,\"start\":56773},{\"end\":56785,\"start\":56784},{\"end\":57006,\"start\":57005},{\"end\":57020,\"start\":57016},{\"end\":57188,\"start\":57187},{\"end\":57205,\"start\":57204},{\"end\":57216,\"start\":57215},{\"end\":57218,\"start\":57217},{\"end\":57523,\"start\":57517},{\"end\":57537,\"start\":57533},{\"end\":57960,\"start\":57952},{\"end\":57973,\"start\":57966},{\"end\":57987,\"start\":57981},{\"end\":58006,\"start\":57997},{\"end\":58376,\"start\":58370},{\"end\":58389,\"start\":58385},{\"end\":58391,\"start\":58390},{\"end\":58410,\"start\":58405},{\"end\":58413,\"start\":58411},{\"end\":58426,\"start\":58421},{\"end\":58438,\"start\":58432},{\"end\":58983,\"start\":58974},{\"end\":59236,\"start\":59235},{\"end\":59246,\"start\":59245},{\"end\":59248,\"start\":59247},{\"end\":59262,\"start\":59257},{\"end\":59666,\"start\":59657},{\"end\":59679,\"start\":59675},{\"end\":60071,\"start\":60064},{\"end\":60083,\"start\":60079},{\"end\":60097,\"start\":60094},{\"end\":60115,\"start\":60110},{\"end\":60454,\"start\":60447},{\"end\":60754,\"start\":60753},{\"end\":60764,\"start\":60763},{\"end\":61172,\"start\":61167},{\"end\":61191,\"start\":61182},{\"end\":61202,\"start\":61197},{\"end\":61204,\"start\":61203},{\"end\":61221,\"start\":61213},{\"end\":61633,\"start\":61629},{\"end\":61635,\"start\":61634},{\"end\":61649,\"start\":61645},{\"end\":61670,\"start\":61663},{\"end\":62054,\"start\":62047},{\"end\":62065,\"start\":62062},{\"end\":62073,\"start\":62072},{\"end\":62089,\"start\":62082},{\"end\":62492,\"start\":62491},{\"end\":62494,\"start\":62493},{\"end\":62502,\"start\":62501},{\"end\":62504,\"start\":62503},{\"end\":62513,\"start\":62512},{\"end\":62946,\"start\":62941},{\"end\":62957,\"start\":62952},{\"end\":62969,\"start\":62965},{\"end\":63230,\"start\":63225},{\"end\":63236,\"start\":63235},{\"end\":63238,\"start\":63237},{\"end\":63612,\"start\":63605},{\"end\":63614,\"start\":63613},{\"end\":63626,\"start\":63625},{\"end\":63892,\"start\":63885},{\"end\":63914,\"start\":63909},{\"end\":63934,\"start\":63930},{\"end\":63941,\"start\":63935},{\"end\":63957,\"start\":63949},{\"end\":63959,\"start\":63958},{\"end\":64347,\"start\":64346},{\"end\":64349,\"start\":64348},{\"end\":64359,\"start\":64358},{\"end\":64361,\"start\":64360},{\"end\":64592,\"start\":64591},{\"end\":64600,\"start\":64595},{\"end\":64619,\"start\":64614},{\"end\":64621,\"start\":64620},{\"end\":64941,\"start\":64940},{\"end\":64950,\"start\":64949},{\"end\":65411,\"start\":65403},{\"end\":65423,\"start\":65417},{\"end\":65671,\"start\":65670},{\"end\":65824,\"start\":65823},{\"end\":65826,\"start\":65825},{\"end\":65970,\"start\":65964},{\"end\":65984,\"start\":65977},{\"end\":66492,\"start\":66486},{\"end\":66500,\"start\":66499},{\"end\":66843,\"start\":66837},{\"end\":66857,\"start\":66854},{\"end\":66873,\"start\":66864},{\"end\":66889,\"start\":66884},{\"end\":67378,\"start\":67373},{\"end\":67687,\"start\":67684},{\"end\":67700,\"start\":67694},{\"end\":67717,\"start\":67711},{\"end\":68008,\"start\":68007},{\"end\":68016,\"start\":68015},{\"end\":68018,\"start\":68017},{\"end\":68027,\"start\":68026},{\"end\":68029,\"start\":68028},{\"end\":68039,\"start\":68038},{\"end\":68041,\"start\":68040},{\"end\":68375,\"start\":68371},{\"end\":68386,\"start\":68382},{\"end\":68388,\"start\":68387},{\"end\":68730,\"start\":68726},{\"end\":68742,\"start\":68737},{\"end\":69092,\"start\":69091},{\"end\":69100,\"start\":69099},{\"end\":69102,\"start\":69101},{\"end\":69116,\"start\":69115},{\"end\":69118,\"start\":69117},{\"end\":69509,\"start\":69501},{\"end\":69524,\"start\":69518},{\"end\":69540,\"start\":69535},{\"end\":69734,\"start\":69730},{\"end\":70012,\"start\":70007},{\"end\":70027,\"start\":70019},{\"end\":70034,\"start\":70033},{\"end\":70038,\"start\":70035},{\"end\":70462,\"start\":70452},{\"end\":70476,\"start\":70472},{\"end\":70835,\"start\":70834}]", "bib_author_last_name": "[{\"end\":53846,\"start\":53831},{\"end\":53865,\"start\":53854},{\"end\":53888,\"start\":53878},{\"end\":53896,\"start\":53890},{\"end\":54271,\"start\":54260},{\"end\":54285,\"start\":54279},{\"end\":54302,\"start\":54295},{\"end\":54317,\"start\":54310},{\"end\":54340,\"start\":54330},{\"end\":54348,\"start\":54342},{\"end\":54682,\"start\":54671},{\"end\":54696,\"start\":54690},{\"end\":54711,\"start\":54704},{\"end\":54734,\"start\":54724},{\"end\":54742,\"start\":54736},{\"end\":55101,\"start\":55093},{\"end\":55113,\"start\":55107},{\"end\":55404,\"start\":55399},{\"end\":55412,\"start\":55408},{\"end\":55729,\"start\":55722},{\"end\":55751,\"start\":55744},{\"end\":55769,\"start\":55761},{\"end\":55782,\"start\":55777},{\"end\":56073,\"start\":56066},{\"end\":56085,\"start\":56079},{\"end\":56095,\"start\":56089},{\"end\":56105,\"start\":56099},{\"end\":56113,\"start\":56109},{\"end\":56121,\"start\":56117},{\"end\":56434,\"start\":56430},{\"end\":56758,\"start\":56750},{\"end\":56771,\"start\":56762},{\"end\":56782,\"start\":56775},{\"end\":56795,\"start\":56786},{\"end\":57014,\"start\":57007},{\"end\":57029,\"start\":57021},{\"end\":57202,\"start\":57189},{\"end\":57213,\"start\":57206},{\"end\":57226,\"start\":57219},{\"end\":57531,\"start\":57524},{\"end\":57543,\"start\":57538},{\"end\":57964,\"start\":57961},{\"end\":57979,\"start\":57974},{\"end\":57995,\"start\":57988},{\"end\":58013,\"start\":58007},{\"end\":58383,\"start\":58377},{\"end\":58403,\"start\":58392},{\"end\":58419,\"start\":58414},{\"end\":58430,\"start\":58427},{\"end\":58449,\"start\":58439},{\"end\":58457,\"start\":58451},{\"end\":58990,\"start\":58984},{\"end\":59243,\"start\":59237},{\"end\":59255,\"start\":59249},{\"end\":59267,\"start\":59263},{\"end\":59673,\"start\":59667},{\"end\":59687,\"start\":59680},{\"end\":60077,\"start\":60072},{\"end\":60092,\"start\":60084},{\"end\":60108,\"start\":60098},{\"end\":60123,\"start\":60116},{\"end\":60460,\"start\":60455},{\"end\":60761,\"start\":60755},{\"end\":60773,\"start\":60765},{\"end\":61180,\"start\":61173},{\"end\":61195,\"start\":61192},{\"end\":61211,\"start\":61205},{\"end\":61230,\"start\":61222},{\"end\":61643,\"start\":61636},{\"end\":61661,\"start\":61650},{\"end\":61678,\"start\":61671},{\"end\":62060,\"start\":62055},{\"end\":62070,\"start\":62066},{\"end\":62080,\"start\":62074},{\"end\":62096,\"start\":62090},{\"end\":62499,\"start\":62495},{\"end\":62510,\"start\":62505},{\"end\":62522,\"start\":62514},{\"end\":62950,\"start\":62947},{\"end\":62963,\"start\":62958},{\"end\":62974,\"start\":62970},{\"end\":63233,\"start\":63231},{\"end\":63244,\"start\":63239},{\"end\":63623,\"start\":63615},{\"end\":63636,\"start\":63627},{\"end\":63907,\"start\":63893},{\"end\":63928,\"start\":63915},{\"end\":63947,\"start\":63942},{\"end\":63967,\"start\":63960},{\"end\":64356,\"start\":64350},{\"end\":64367,\"start\":64362},{\"end\":64612,\"start\":64601},{\"end\":64629,\"start\":64622},{\"end\":64947,\"start\":64942},{\"end\":64965,\"start\":64951},{\"end\":65415,\"start\":65412},{\"end\":65430,\"start\":65424},{\"end\":65678,\"start\":65672},{\"end\":65836,\"start\":65827},{\"end\":65975,\"start\":65971},{\"end\":65992,\"start\":65985},{\"end\":66497,\"start\":66493},{\"end\":66508,\"start\":66501},{\"end\":66852,\"start\":66844},{\"end\":66862,\"start\":66858},{\"end\":66882,\"start\":66874},{\"end\":66893,\"start\":66890},{\"end\":67383,\"start\":67379},{\"end\":67692,\"start\":67688},{\"end\":67709,\"start\":67701},{\"end\":67728,\"start\":67718},{\"end\":68013,\"start\":68009},{\"end\":68024,\"start\":68019},{\"end\":68036,\"start\":68030},{\"end\":68052,\"start\":68042},{\"end\":68380,\"start\":68376},{\"end\":68394,\"start\":68389},{\"end\":68735,\"start\":68731},{\"end\":68745,\"start\":68743},{\"end\":69097,\"start\":69093},{\"end\":69113,\"start\":69103},{\"end\":69124,\"start\":69119},{\"end\":69516,\"start\":69510},{\"end\":69533,\"start\":69525},{\"end\":69549,\"start\":69541},{\"end\":69739,\"start\":69735},{\"end\":70017,\"start\":70013},{\"end\":70031,\"start\":70028},{\"end\":70042,\"start\":70039},{\"end\":70470,\"start\":70463},{\"end\":70481,\"start\":70477},{\"end\":70841,\"start\":70836}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/1882261.1866187\",\"id\":\"b0\",\"matched_paper_id\":10640860},\"end\":54189,\"start\":53770},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9384333},\"end\":54568,\"start\":54191},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13165089},\"end\":55018,\"start\":54570},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14067334},\"end\":55331,\"start\":55020},{\"attributes\":{\"doi\":\"10.1109/TMM.2006.886291\",\"id\":\"b4\",\"matched_paper_id\":28412699},\"end\":55666,\"start\":55333},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9630004},\"end\":56003,\"start\":55668},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":87332},\"end\":56336,\"start\":56005},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":62129590},\"end\":56697,\"start\":56338},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2823227},\"end\":56951,\"start\":56699},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":621181},\"end\":57159,\"start\":56953},{\"attributes\":{\"doi\":\"10.2307/1269571\",\"id\":\"b10\",\"matched_paper_id\":120895672},\"end\":57418,\"start\":57161},{\"attributes\":{\"doi\":\"10.1109/TVCG.2015.2429576\",\"id\":\"b11\",\"matched_paper_id\":16371010},\"end\":57894,\"start\":57420},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1366405},\"end\":58297,\"start\":57896},{\"attributes\":{\"doi\":\"10.1111/j.1467-8659.2012.03055.x\",\"id\":\"b13\",\"matched_paper_id\":2735382},\"end\":58913,\"start\":58299},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16657222},\"end\":59150,\"start\":58915},{\"attributes\":{\"doi\":\"10.1109/TVCG.2015.2480079\",\"id\":\"b15\",\"matched_paper_id\":9490200},\"end\":59614,\"start\":59152},{\"attributes\":{\"doi\":\"10.1007/978-3-319-10368-69\",\"id\":\"b16\",\"matched_paper_id\":6864285},\"end\":59989,\"start\":59616},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3352736},\"end\":60356,\"start\":59991},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":59774068},\"end\":60681,\"start\":60358},{\"attributes\":{\"doi\":\"10.1109/tit.1974.1055250\",\"id\":\"b19\",\"matched_paper_id\":24056678},\"end\":61059,\"start\":60683},{\"attributes\":{\"doi\":\"no. 40\",\"id\":\"b20\",\"matched_paper_id\":756729},\"end\":61559,\"start\":61061},{\"attributes\":{\"doi\":\"10.1111/j.1467-8659.2012.03188.x\",\"id\":\"b21\",\"matched_paper_id\":5667874},\"end\":61976,\"start\":61561},{\"attributes\":{\"doi\":\"10.1109/TVCG.2015.2507578\",\"id\":\"b22\",\"matched_paper_id\":216119384},\"end\":62426,\"start\":61978},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9609696},\"end\":62868,\"start\":62428},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6591149},\"end\":63184,\"start\":62870},{\"attributes\":{\"doi\":\"10.1109/TVCG.2008.51\",\"id\":\"b25\",\"matched_paper_id\":7434827},\"end\":63521,\"start\":63186},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16399426},\"end\":63820,\"start\":63523},{\"attributes\":{\"doi\":\"10.1109/TIP.2010.2042111\",\"id\":\"b27\",\"matched_paper_id\":206724285},\"end\":64306,\"start\":63822},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3716103},\"end\":64549,\"start\":64308},{\"attributes\":{\"doi\":\"10.1117/1.1344187\",\"id\":\"b29\",\"matched_paper_id\":16388957},\"end\":64861,\"start\":64551},{\"attributes\":{\"doi\":\"10.1145/1071866.1071877\",\"id\":\"b30\",\"matched_paper_id\":14568927},\"end\":65382,\"start\":64863},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2690113},\"end\":65615,\"start\":65384},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1019243},\"end\":65789,\"start\":65617},{\"attributes\":{\"id\":\"b33\"},\"end\":65957,\"start\":65791},{\"attributes\":{\"doi\":\"10.1145/1027527.1027684\",\"id\":\"b34\"},\"end\":66408,\"start\":65959},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15480121},\"end\":66771,\"start\":66410},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5948669},\"end\":67256,\"start\":66773},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3286512},\"end\":67605,\"start\":67258},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15637365},\"end\":67931,\"start\":67607},{\"attributes\":{\"doi\":\"10.1109/tip.2003.819861\",\"id\":\"b39\",\"matched_paper_id\":207761262},\"end\":68367,\"start\":67933},{\"attributes\":{\"doi\":\"10.2200/S00010ED1V01Y200508IVM003\",\"id\":\"b40\"},\"end\":68653,\"start\":68369},{\"attributes\":{\"doi\":\"10.1109/tip.2010.2092435\",\"id\":\"b41\",\"matched_paper_id\":106021},\"end\":69026,\"start\":68655},{\"attributes\":{\"doi\":\"10.1109/ACSSC.2003.1292216\",\"id\":\"b42\",\"matched_paper_id\":60600316},\"end\":69457,\"start\":69028},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":13271725},\"end\":69692,\"start\":69459},{\"attributes\":{\"id\":\"b44\"},\"end\":69920,\"start\":69694},{\"attributes\":{\"doi\":\"10.1145/1027527.1027683\",\"id\":\"b45\",\"matched_paper_id\":11138274},\"end\":70398,\"start\":69922},{\"attributes\":{\"doi\":\"10.1109/TIP.2012.2221725\",\"id\":\"b46\",\"matched_paper_id\":8926580},\"end\":70750,\"start\":70400},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10716320},\"end\":71153,\"start\":70752}]", "bib_title": "[{\"end\":53829,\"start\":53770},{\"end\":54251,\"start\":54191},{\"end\":54662,\"start\":54570},{\"end\":55089,\"start\":55020},{\"end\":55395,\"start\":55333},{\"end\":55707,\"start\":55668},{\"end\":56062,\"start\":56005},{\"end\":56422,\"start\":56338},{\"end\":56746,\"start\":56699},{\"end\":57003,\"start\":56953},{\"end\":57185,\"start\":57161},{\"end\":57515,\"start\":57420},{\"end\":57950,\"start\":57896},{\"end\":58368,\"start\":58299},{\"end\":58972,\"start\":58915},{\"end\":59233,\"start\":59152},{\"end\":59655,\"start\":59616},{\"end\":60062,\"start\":59991},{\"end\":60445,\"start\":60358},{\"end\":60751,\"start\":60683},{\"end\":61165,\"start\":61061},{\"end\":61627,\"start\":61561},{\"end\":62045,\"start\":61978},{\"end\":62489,\"start\":62428},{\"end\":62939,\"start\":62870},{\"end\":63223,\"start\":63186},{\"end\":63603,\"start\":63523},{\"end\":63883,\"start\":63822},{\"end\":64344,\"start\":64308},{\"end\":64589,\"start\":64551},{\"end\":64938,\"start\":64863},{\"end\":65401,\"start\":65384},{\"end\":65668,\"start\":65617},{\"end\":65821,\"start\":65791},{\"end\":65962,\"start\":65959},{\"end\":66484,\"start\":66410},{\"end\":66835,\"start\":66773},{\"end\":67371,\"start\":67258},{\"end\":67682,\"start\":67607},{\"end\":68005,\"start\":67933},{\"end\":68724,\"start\":68655},{\"end\":69089,\"start\":69028},{\"end\":69499,\"start\":69459},{\"end\":70005,\"start\":69922},{\"end\":70450,\"start\":70400},{\"end\":70832,\"start\":70752}]", "bib_author": "[{\"end\":53848,\"start\":53831},{\"end\":53867,\"start\":53848},{\"end\":53890,\"start\":53867},{\"end\":53898,\"start\":53890},{\"end\":54273,\"start\":54253},{\"end\":54287,\"start\":54273},{\"end\":54304,\"start\":54287},{\"end\":54319,\"start\":54304},{\"end\":54342,\"start\":54319},{\"end\":54350,\"start\":54342},{\"end\":54684,\"start\":54664},{\"end\":54698,\"start\":54684},{\"end\":54713,\"start\":54698},{\"end\":54736,\"start\":54713},{\"end\":54744,\"start\":54736},{\"end\":55103,\"start\":55091},{\"end\":55115,\"start\":55103},{\"end\":55406,\"start\":55397},{\"end\":55414,\"start\":55406},{\"end\":55731,\"start\":55709},{\"end\":55753,\"start\":55731},{\"end\":55771,\"start\":55753},{\"end\":55784,\"start\":55771},{\"end\":56075,\"start\":56064},{\"end\":56087,\"start\":56075},{\"end\":56097,\"start\":56087},{\"end\":56107,\"start\":56097},{\"end\":56115,\"start\":56107},{\"end\":56123,\"start\":56115},{\"end\":56436,\"start\":56424},{\"end\":56760,\"start\":56748},{\"end\":56773,\"start\":56760},{\"end\":56784,\"start\":56773},{\"end\":56797,\"start\":56784},{\"end\":57016,\"start\":57005},{\"end\":57031,\"start\":57016},{\"end\":57204,\"start\":57187},{\"end\":57215,\"start\":57204},{\"end\":57228,\"start\":57215},{\"end\":57533,\"start\":57517},{\"end\":57545,\"start\":57533},{\"end\":57966,\"start\":57952},{\"end\":57981,\"start\":57966},{\"end\":57997,\"start\":57981},{\"end\":58015,\"start\":57997},{\"end\":58385,\"start\":58370},{\"end\":58405,\"start\":58385},{\"end\":58421,\"start\":58405},{\"end\":58432,\"start\":58421},{\"end\":58451,\"start\":58432},{\"end\":58459,\"start\":58451},{\"end\":58992,\"start\":58974},{\"end\":59245,\"start\":59235},{\"end\":59257,\"start\":59245},{\"end\":59269,\"start\":59257},{\"end\":59675,\"start\":59657},{\"end\":59689,\"start\":59675},{\"end\":60079,\"start\":60064},{\"end\":60094,\"start\":60079},{\"end\":60110,\"start\":60094},{\"end\":60125,\"start\":60110},{\"end\":60462,\"start\":60447},{\"end\":60763,\"start\":60753},{\"end\":60775,\"start\":60763},{\"end\":61182,\"start\":61167},{\"end\":61197,\"start\":61182},{\"end\":61213,\"start\":61197},{\"end\":61232,\"start\":61213},{\"end\":61645,\"start\":61629},{\"end\":61663,\"start\":61645},{\"end\":61680,\"start\":61663},{\"end\":62062,\"start\":62047},{\"end\":62072,\"start\":62062},{\"end\":62082,\"start\":62072},{\"end\":62098,\"start\":62082},{\"end\":62501,\"start\":62491},{\"end\":62512,\"start\":62501},{\"end\":62524,\"start\":62512},{\"end\":62952,\"start\":62941},{\"end\":62965,\"start\":62952},{\"end\":62976,\"start\":62965},{\"end\":63235,\"start\":63225},{\"end\":63246,\"start\":63235},{\"end\":63625,\"start\":63605},{\"end\":63638,\"start\":63625},{\"end\":63909,\"start\":63885},{\"end\":63930,\"start\":63909},{\"end\":63949,\"start\":63930},{\"end\":63969,\"start\":63949},{\"end\":64358,\"start\":64346},{\"end\":64369,\"start\":64358},{\"end\":64595,\"start\":64591},{\"end\":64614,\"start\":64595},{\"end\":64631,\"start\":64614},{\"end\":64949,\"start\":64940},{\"end\":64967,\"start\":64949},{\"end\":65417,\"start\":65403},{\"end\":65432,\"start\":65417},{\"end\":65680,\"start\":65670},{\"end\":65838,\"start\":65823},{\"end\":65977,\"start\":65964},{\"end\":65994,\"start\":65977},{\"end\":66499,\"start\":66486},{\"end\":66510,\"start\":66499},{\"end\":66854,\"start\":66837},{\"end\":66864,\"start\":66854},{\"end\":66884,\"start\":66864},{\"end\":66895,\"start\":66884},{\"end\":67385,\"start\":67373},{\"end\":67694,\"start\":67684},{\"end\":67711,\"start\":67694},{\"end\":67730,\"start\":67711},{\"end\":68015,\"start\":68007},{\"end\":68026,\"start\":68015},{\"end\":68038,\"start\":68026},{\"end\":68054,\"start\":68038},{\"end\":68382,\"start\":68371},{\"end\":68396,\"start\":68382},{\"end\":68737,\"start\":68726},{\"end\":68747,\"start\":68737},{\"end\":69099,\"start\":69091},{\"end\":69115,\"start\":69099},{\"end\":69126,\"start\":69115},{\"end\":69518,\"start\":69501},{\"end\":69535,\"start\":69518},{\"end\":69551,\"start\":69535},{\"end\":69741,\"start\":69730},{\"end\":70019,\"start\":70007},{\"end\":70033,\"start\":70019},{\"end\":70044,\"start\":70033},{\"end\":70472,\"start\":70452},{\"end\":70483,\"start\":70472},{\"end\":70843,\"start\":70834}]", "bib_venue": "[{\"end\":56494,\"start\":56485},{\"end\":58110,\"start\":58071},{\"end\":62671,\"start\":62606},{\"end\":63663,\"start\":63659},{\"end\":65129,\"start\":65068},{\"end\":66180,\"start\":66107},{\"end\":67032,\"start\":66972},{\"end\":70160,\"start\":70122},{\"end\":70968,\"start\":70914},{\"end\":53949,\"start\":53921},{\"end\":54366,\"start\":54350},{\"end\":54772,\"start\":54744},{\"end\":55152,\"start\":55115},{\"end\":55468,\"start\":55437},{\"end\":55815,\"start\":55784},{\"end\":56146,\"start\":56123},{\"end\":56467,\"start\":56436},{\"end\":56810,\"start\":56797},{\"end\":57043,\"start\":57031},{\"end\":57256,\"start\":57243},{\"end\":57626,\"start\":57570},{\"end\":58069,\"start\":58015},{\"end\":58503,\"start\":58491},{\"end\":59015,\"start\":58992},{\"end\":59350,\"start\":59294},{\"end\":59775,\"start\":59715},{\"end\":60153,\"start\":60125},{\"end\":60493,\"start\":60462},{\"end\":60838,\"start\":60799},{\"end\":61289,\"start\":61238},{\"end\":61735,\"start\":61712},{\"end\":62179,\"start\":62123},{\"end\":62604,\"start\":62524},{\"end\":63007,\"start\":62976},{\"end\":63322,\"start\":63266},{\"end\":63657,\"start\":63638},{\"end\":64030,\"start\":63993},{\"end\":64406,\"start\":64369},{\"end\":64677,\"start\":64648},{\"end\":65066,\"start\":64990},{\"end\":65451,\"start\":65432},{\"end\":65692,\"start\":65680},{\"end\":65858,\"start\":65838},{\"end\":66105,\"start\":66017},{\"end\":66572,\"start\":66510},{\"end\":66970,\"start\":66895},{\"end\":67408,\"start\":67385},{\"end\":67750,\"start\":67730},{\"end\":68114,\"start\":68077},{\"end\":68460,\"start\":68429},{\"end\":68808,\"start\":68771},{\"end\":69210,\"start\":69152},{\"end\":69563,\"start\":69551},{\"end\":69728,\"start\":69694},{\"end\":70120,\"start\":70067},{\"end\":70544,\"start\":70507},{\"end\":70912,\"start\":70843}]"}}}, "year": 2023, "month": 12, "day": 17}