{"id": 254246440, "updated": "2023-10-05 07:43:38.277", "metadata": {"title": "Decoding natural image stimuli from fMRI data with a surface-based convolutional network", "authors": "[{\"first\":\"Zijin\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Keith\",\"last\":\"Jamison\",\"middle\":[]},{\"first\":\"Amy\",\"last\":\"Kuceyeski\",\"middle\":[]},{\"first\":\"Mert\",\"last\":\"Sabuncu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Due to the low signal-to-noise ratio and limited resolution of functional MRI data, and the high complexity of natural images, reconstructing a visual stimulus from human brain fMRI measurements is a challenging task. In this work, we propose a novel approach for this task, which we call Cortex2Image, to decode visual stimuli with high semantic fidelity and rich fine-grained detail. In particular, we train a surface-based convolutional network model that maps from brain response to semantic image features first (Cortex2Semantic). We then combine this model with a high-quality image generator (Instance-Conditioned GAN) to train another mapping from brain response to fine-grained image features using a variational approach (Cortex2Detail). Image reconstructions obtained by our proposed method achieve state-of-the-art semantic fidelity, while yielding good fine-grained similarity with the ground-truth stimulus. Our code is available at: https://github.com/zijin-gu/meshconv-decoding.git.", "fields_of_study": "[\"Computer Science\",\"Biology\"]", "external_ids": {"arxiv": "2212.02409", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2212-02409", "doi": "10.48550/arxiv.2212.02409"}}, "content": {"source": {"pdf_hash": "8e164661656007a5944c303e09bbc792d8700cb5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.02409v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f0035043996ba14531f460e42ae3e49d9ed14ccf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8e164661656007a5944c303e09bbc792d8700cb5.txt", "contents": "\nDecoding natural image stimuli from fMRI data with a surface-based convolutional network\n\n\nZijin Gu \nSchool of Electrical and Computer Engineering\nCornell University and Cornell Tech\nNew YorkNew YorkUSA\n\nKeith Jamison \nDepartment of Radiology\nWeill Cornell Medicine\nNew YorkNew YorkUSA\n\nAmy Kuceyeski \nDepartment of Radiology\nWeill Cornell Medicine\nNew YorkNew YorkUSA\n\nMert Sabuncu msabuncu@cornell.edu \nSchool of Electrical and Computer Engineering\nCornell University and Cornell Tech\nNew YorkNew YorkUSA\n\nDepartment of Radiology\nWeill Cornell Medicine\nNew YorkNew YorkUSA\n\nDecoding natural image stimuli from fMRI data with a surface-based convolutional network\n1-12functional MRIneural decodingimage reconstruction\nDue to the low signal-to-noise ratio and limited resolution of functional MRI data, and the high complexity of natural images, reconstructing a visual stimulus from human brain fMRI measurements is a challenging task. In this work, we propose a novel approach for this task, which we call Cortex2Image, to decode visual stimuli with high semantic fidelity and rich fine-grained detail. In particular, we train a surface-based convolutional network model that maps from brain response to semantic image features first (Cortex2Semantic). We then combine this model with a high-quality image generator (Instance-Conditioned GAN) to train another mapping from brain response to fine-grained image features using a variational approach (Cortex2Detail). Image reconstructions obtained by our proposed method achieve state-of-the-art semantic fidelity, while yielding good fine-grained similarity with the ground-truth stimulus.\n\nIntroduction\n\nNeural decoding, especially visual decoding that reconstructs the external visual stimulus from brain activity patterns, is a challenging task in neuroscience research. Development of neuroimaging techniques, e.g. functional magnetic resonance imaging (fMRI), and breakthroughs in artificial intelligence like deep learning, have been recently used to build models that can map brain responses to visual stimulus with high quality, thus paving the way for future \"mind-reading\" technologies that can have clinical and scientific applications.\n\nPrevious work Previous studies have demonstrated the feasibility of mapping from brain responses to visual stimulus. The predominant approach in early works used linear models to map fMRI-derived measurements to hand-crafted image features, such as Gabor wavelet coefficients (Miyawaki et al., 2008;Naselaris et al., 2009;Nishimoto et al., 2011). More recently, deep neural networks (DNNs) have become popular to compute nonlinear mappings from brain signals to visual stimuli. In these DNN methods, Generative Adversarial Networks (GANs) have become a popular building block to reconstruct realistic images (St-Yves and Naselaris, 2018;Seeliger et al., 2018;Shen et al., 2019a;Lin et al., 2019;Shen et al., 2019b;VanRullen and Reddy, 2019;Fang et al., 2020;Gu et al., 2022;Ozcelik et al., 2022), and self-supervision techniques, e.g. via an autoencoder (AE), have shown to improve generalization (Du et al., 2017;Beliy et al., 2019;Fang et al., 2020). While several stages of optimization may be required, these approaches have been able to reconstruct images with good similarity to the viewed ground-truth images.\n\nVirtually all previous work on visual decoding handles fMRI data by selecting voxels that belong to certain visual regions-of-interest (ROIs), e.g. early visual areas V1 -V4 (Ress and Heeger, 2003;Gardner et al., 2005), or higher visual areas FFA (Kanwisher et al., 1997), PPA (Epstein and Kanwisher, 1998) and LOC (Malach et al., 1995). These voxel measurements are then flattened into 1D vectors that are in turn provided as input to the visual decoder. This approach has several important weaknesses: 1) ROI definitions and the selection involved in fMRI preprocessing are subjective and can vary across individuals; and 2) the spatial topology of the 2D cortical areas is largely ignored when using the vectorized voxel responses.\n\nIn this work, we present a novel visual decoding framework to address these weaknesses. The proposed Cortex2Image framework contains one Cortex2Semantic model, one Cor-tex2Detail model and one pre-trained and frozen image generator called Instance-Conditioned GAN (IC-GAN). Comparing with the most state-of-the-art method which uses ridge regression to separately predict the feature and noise input vectors to IC-GAN (Ozcelik et al., 2022), our method has several key improvements: 1) the proposed Cortex2Image model has an architecture shared across individual subjects that consumes cortex-wide brain activity, since it uses a standardized mesh representation of the cortex, instead of relying on specific ROIs; 2) surface convolutions used in our model can exploit spatial information in brain activity patterns; 3) our approach to train the Cortex2Detail model in an end-to-end fashion by combining with IC-GAN improves computational efficiency comparing with previous methods which need extra steps on the noise vector optimization. Our experiments demonstrate that the proposed visual decoding framework is able to produce high-fidelity natural-looking images that achieve state-of-the-art accuracy in matching high-level semantics, while capturing much of the image details.\n\n\nMethod\n\n\nDataset\n\nNatural Scenes Dataset We used a recently collected densely-sampled fMRI dataset, called Natural Scenes Dataset (NSD) (Allen et al., 2021) to train and test the proposed framework 1 . In short, the NSD contains natural image stimuli and whole brain responses for 8 participants collected over approximately a year. Over the course of 30-40 7T fMRI scans with whole-brain gradient-echo EPI, 1.8-mm iso-voxel and 1.6s TR, each subject viewed 9,000-10,000 color natural scenes in 22,000 to 30,000 trials. Images were sourced from the Microsoft Common Objects in Context (COCO) database (Lin et al., 2014), and square cropped to present at a size of 8.4\u00b0\u00d7 8.4\u00b0. Images were presented for 3s on and 1s off while subjects were asked to fixate centrally. In order to encourage maintenance of attention, they also performed a long-term continuous recognition task on the images. Temporal interpolation and spatial interpolation were used to preprocess the fMRI data to correct for slice time differences and head motion. Then a general linear model (GLM) was applied to estimate the single-trial beta weights which represented the voxel-wise response to the image presented. Cortical surface-based response maps were generated using FreeSurfer 2 . Among the 10,000 images each participant viewed, 1,000 images are shared across individuals while the remaining 9,000 images are mutually exclusive across subjects. We used the data from four participants who completed all the designed trials (10,000 images in 30,000 trials). \n\n\nCortex2Image: mapping brain responses to images\n\nThe Cortex2Image model 3 consists of three components: a Cortex2Semantic model, a Cortex2Detail model and a image generator from IC-GAN, see Figure 1a.\n\nCortex2Semantic The Cortex2Semantic model maps the fMRI brain response to the image semantic features, e.g. object categories, corresponding to that response. Instead of flattening the brain voxel responses and simply applying a linear model like most previous works, e.g., (Ozcelik et al., 2022), we employ a spherical convolutional network using spherical kernels (Jiang et al., 2019) which takes into account the neighborhood information on the cortical surface (Ngo et al., 2022). In our implementation, brain voxel responses are represented as multi-channel data attached to the icosahedral mesh vertices, with each channel corresponding to a hemisphere. The proposed Cortex2Semantic model, shown in Figure 1b, contains a mesh convolution layer, followed by three downstream residual blocks and a dense layer. The loss function is the l 2 distance between the predicted semantic vector and ground truth semantic vector, which is an image representation extracted by a fixed pretrained network called \"Swapping The unsupervised Assignments between Views (SwAV)\" (Caron et al., 2020). SwAV is trained in self-supervised way and is also used as the feature extractor in IC-GAN training (Casanova et al., 2021). SwAV features are used in the IC-GAN image generator that we describe below.\n\nCortex2Detail In addition to the semantic image features, we trained another Cor-tex2Detail model which maps brain responses to fine-grained (detail) image features that capture factors like color, size, and orientation. This model's output is a stochastic layer that represents an approximate posterior distribution on the fine-grained image details and is trained with a variational approach. The architecture is the same as Cortex2Semantic, except for the final layer where the dimensions are different.\n\n\nIC-GAN Generator\n\nIn IC-GAN (Casanova et al., 2021), a complex distribution on, say images, p(x) is approximated by learning simpler conditional distributions around data\npoints: i.e. p(x) \u2248 1 M i p(x|h i ),\nwhere M is the number of data samples and h i is the features of data sample x i . The instance features are extracted using some embedding function. In IC-GAN, this embedding function is the pre-trained SwAV model (Caron et al., 2020) that computes semantic image features. The conditional distributions p(x|h i ) are modeled implicitly using a generative network G \u03b8 parameterized by \u03b8. The generator takes in a random vector sampled from a standard Gaussian prior z \u223c N (0, I) and a feature vector h i corresponding to an image x i sampled from a dataset; and generates a random sample x which is conditioned on h i . The generator is jointly trained with a discriminator in an adversarial approach. One advantage of IC-GAN is its capability of transferring to unseen datasets during training meanwhile keeping high fidelity. Note that the random vector z can be regarded as capturing fine-grained image details like color, size, and orientation, whereas h i reflects semantics. Therefore, the generated images will share the same semantic meaning as the image that is conditioned on, but will vary in details. The complete Cortex2Image model is shown in Figure 1a. We emphasize that the semantic image features can be pre-computed for all ground-truth images using the pretrained SwAV model. The Cortex2Semantic model, in turn, is trained directly on these features. On the other hand, obtaining the ground-truth fine-grained (detail) feature vector for a given image involves a costly optimization process, which we describe below. For training Cortex2Detail, we opted to avoid this computationally expensive step. Instead, we train Cortex2Detail in conjunction with of the Cortex2Image model in an end-to-end fashion, where both the Cortex2Semantic and Cortex2Detail model outputs are served as input to the IC-GAN generator to compute an image reconstruction. Here we take a variational, instead of a deterministic approach in predicting the detail vector based on the assumption that the exact details of an image cannot be uniquely decoded from noisy fMRI data. To ensure the reconstructed image has both high semantic and detail fidelity, the loss is composed of three parts:\nL = \u2212 \u03bb recon E z\u223cq(z|x i ) [log p \u03b8 (x i |z)] + \u03bb KL KL(q(z|y i )||p(z)) \u2212 \u03bb f eat E z\u223cq(z|x i ) [log f \u03c6 (x i |z)]\nwhere the first term is the pixel level reconstruction loss, the second term is the Kullback-Leibler divergence between the approximate posterior on the fine-grained detail vector and its prior, which is a standard Gaussian, and the third term is the SwAV latent space feature loss.\n\n\nBaselines and upper bound\n\nWe compare our model with a state-of-the-art visual decoding framework (Ozcelik et al., 2022), where the authors used a linear model with l 2 regularization. Similar to this model, we fitted two ridge regression models with the flattened fMRI data as the input: one for predicting the semantic image features and the other for the fine-grained image details. Same as Cortex2Semantic, the ground truth semantic features are derived using the pretrained SWAV latent space. To obtain the \"ground truth\" fine-grained details, we implemented the following iterative optimization procedure. We inputted the ground truth semantic image features and a randomly initialized fine-grained detail vector to the IC-GAN generator and optimized over the detail vector using the covariance matrix adaptation evolution strategy (CMAES) (Hansen and Ostermeier, 2001). This global optimization strategy works better than local optimization strategies, e.g. gradient-based methods. The loss function is the l 2 distance between the reconstructed image and the ground truth image in SwAV latent space. Note that this process is computationally demanding, e.g. on average 50 seconds for one optimization and around 140 hours for one subject if not done in parallel, and can be unstable (Ozcelik et al., 2022).\n\nWe compare our full Cortex2Image models with the following models: 1) RidgeSemantic: images reconstructed using a linear ridge regression approach that predicts the semantic vectors, combined with randomly sampled detail vectors from N \u223c (0, 1).\n\n2) RidgeImage: images reconstructed using a linear ridge regression approach that predicts both the semantic and optimized fine-grained detail vectors. This method is computationally demanding, due to expensive step of estimating the fine-grained detail vectors for all the training images.\n\n3) Cortex2Semantic: images reconstructed using the proposed Cortext2Semantic approach that predicts the semantic vectors, combined with randomly sampled detail vectors from N \u223c (0, 1).\n\nRidgeSemantic and Cortex2Semantic (which is an ablation of the proposed method) models were considered as baselines because it has been reported that reconstructions from predicted semantic features and randomly sampled fine-grained detail vectors can achieve better results than from predicted fine-grained detail vectors (Ozcelik et al., 2022).\n\nThe synthetic upper bound is the best reconstruction that the IC-GAN generator can achieve, given the semantic feature vector of the presented ground-truth stimulus. This is computed via the optimization of the fine-grained detail vector, as described above. Note that there is no fMRI data involved in this reconstruction.\n\n\nEvaluation metrics\n\nModel evaluation was done qualitatively and quantitatively. For qualitative evaluation, we displayed the reconstructed image along with the ground truth image to compare them visually. For quantitative evaluation, we adopted both high-level and low-level (fine-grained) metrics. High-level metrics include the latent space distance of SWAV and EfficientNet-B1 (Tan and Le, 2019). EfficientNet-B1 was chosen as it ranks top on BrainScore (Schrimpf et al., 2020), which measures the similarity between artificial neural networks and the brain's mechanisms for core object recognition. Low-level metrics include the classical Structural Similarity (SSIM) and pixel-wise correlation.\n\n\nImplementation Details\n\nWe implemented the Cortex2Image model using PyTorch 4 . The input surface meshes to the Cortex2Semantic and Cortex2Detail models are the left and right FreeSurfer cortical meshes (Van Essen et al., 2012) with 32,492 vertices per brain hemisphere. The downstream residual blocks reduce the number of vertices on the mesh from 32,492 to 92, with 7,842, 2,562 and 492 in between, and the number of channels increases from 2 to 2048, with 32, 128, 512 in between. The mesh convolution has stride 1 and all other Conv1d layers have kernel size 1 and stride 1. For the Cortex2Semantic model, the last linear layer maps the averaged pooled features to the final output of size 2048. For Cortex2Detail model, there are two penultimate fully connected layers, one for the mean and the other for the variance of the multivariate Gaussian vector and both map the averaged pooled features to the output of 119. The image generator from IC-GAN is built upon Big-GAN architecture (Brock et al., 2018) with the class embedding layer replaced by a feature embedding layer. The output images are of resolution 256 x 256. SwAV pre-trained model with ResNet-50 model (Caron et al., 2020) is used as the instance feature extractor, as it has been recently shown that unsupervised models are able to achieve equal or better performance than supervised models by learning hierarchical features to capture the structure of brain responses across the ventral visual stream (Konkle and Alvarez, 2022;Zhuang et al., 2021). The Cortex2Semantic model is trained first and then kept fixed during the training of Cortex2Detail. The IC-GAN generator is also fixed during training. The baseline linear models were implemented using the Ridge regression model implemented in the sklearn python package. Ridge models has about twice as many parameters as the Cortex models.\n\nWe trained subject-specific models using their own data given the unique response properties of individuals. The shared 1,000 images were used as the held-out test set, and the remaining 9,000 samples were divided into training set (8,500) and validation sets (500). AdamW optimizer was employed for training the neural networks. Learning rate was set to 1e \u2212 3 for Cortex2Semantic and 1e \u2212 4 for Cortex2Detail. Weight decay was set to 0.1. For the training of Cortex2Detail, \u03bb recon , \u03bb KL and \u03bb f eat were set to 1e \u2212 6, 1e \u2212 8 and 1, respectively, which were observed to give best performance on the validation set. Batch size was 8. The maximum training epoch was set to 1000, while monitoring the validation loss to save the model that gave the best performance. All the quantitative results reported are based on applying the final model to the 1000 test response-image samples.\n\n\nResults\n\nIn Figure 2 we visualize the viewed stimulus (top row, green frames) along with the synthetic upper bound reconstructions from extracted semantic vectors and optimized detail vectors (second row, black frames), and the reconstructions from four subjects' fMRI data (rows 3-6, green frames). We provide both well and poorly decoded examples in different image categories, including animals, buildings, food, etc. Looking at the well decoded examples, the fMRI reconstructions from the proposed method are able to largely match the semantic contents of the viewed images, for example, a zebra on the grass, a train on the railway track and a plane in the sky. Though fine-grained details (like color, pose and exact object shape/borders) are more difficult to recover, some details are preserved, such as the black and white stripes on the zebra body, the color of the pizza, the location of the tower, the orientation of the train and airplane, the framing of the athletes bodies, and the proportion of ground vs sky in the landscape.\n\nThere are still some failures of the proposed method, as shown in the poorly decoded examples. We noticed that these viewed images usually have multiple objects of different categories, and objects or their predictive features are occluded or only partially presented. For example, for a viewed image of a person and a giraffe, some subjects' models only reconstructed the person while others reconstructed the animal. The reason could be that different subjects focused on different objects during the scan. Future work can add eyetracking data to examine attention's effect on the reconstructed images. It should be noted that we have observed that subject-specific models do not generalize well across subjects (results not shown) and don't yield meaningful results. This is likely due to the variable response properties of subjects and the different signal-to-noise ratio levels in the fMRI data. Figure 2: Image reconstruction from fMRI data from four individuals using the NSD dataset.\n\n\nCortex2Image vs RidgeImage\n\nFigure 3: (a) Qualitative and (b) quantitative comparisons of image reconstruction results for the four different decoding models. For each metric, \u2193 means lower is better and \u2191 means higher is better. Figure 3 provides a comparison of our Cortex2Image reconstructions (green frame) with the reconstructions from RidgeImage baseline (grey frame). By leveraging the spatial information within the brain data, our reconstructions visually shares more similarities with the ground truth stimulus in both high-level (semantic) and low-level (fine-grained) features. The decoded object classes are correct in most cases, as well as the color and position of the objects and background. The RidgeImage model is able to reconstruct some of the semantic features, however, the overall visual quality is poor. In our quantitative comparisons, our reconstructions achieve better results than the RidgeImage baseline in terms of all evaluation metrics, except SSIM.\n\n\nThe effect of considering low level features in the decoding models\n\nWe characterized the significance of capturing fine-grained details in our model architecture. In this vein, we first compared the reconstructed images from our full Cortex2Image model with the images reconstructed with the Cortex2Semantic model, see Figure 3. We observe that the Cortex2Semantic model is able to capture object category, but the Cortex2Image model can achieve significantly better results in predicting the fine-grained features, such as the orientation of the bus and the arrangement of the toilet and washbasin. Similarly, when comparing the reconstructions of RidgeSemantic (light grey frame) and RidgeImage (grey frame), we observe a boost in quality that is due to the model's consideration of finegrained details in the decoding. This is also reflected in the quantitative results, presented in Figure 3b, where we see that the Cortex2Image model outperforms Cortex2Semantic model and the RidgeImage model is better than the RidgeSemantic model in all metrics.\n\n\nConclusion\n\nOur work highlights the importance of modeling the spatial information in fMRI brain responses for visual decoding. Our results show that the proposed surface-based convolutional architecture can achieve superior quality in image reconstructions over current state-of-theart baseline models. Additionally, combining the image generator in model training greatly reduces the time and computation cost when obtaining the \"ground truth\" detail vector, as noted in previous work. Overall, the present work is a novel approach for decoding visual stimuli from brain activity and has potential in brain machine interfaces and studying human brain response properties via external stimuli alterations. \n\nFigure 1 :\n1(a) The Cortex2Image model architecture , which consists of a Cortex2Semantic model (detailed in panel b), a Cortex2Detail model that shares the same archiecture as Cortex2Semantic model, and an image generator from IC-GAN.\n\nFigure 4 :\n4Qualitative comparison of image reconstructions from Cortex2Image and Cor-tex2Semantic, with random sampling.\n\u00a9 Z. Gu, K. Jamison, A. Kuceyeski & M. Sabuncu.\n. http://surfer.nmr.mgh.harvard.edu/ 3. Our code is available on github.\n. https://pytorch.org/\nAppendix A. More examples on the effect of Cortex2DetailFor the donuts example, the random sampling reconstructs many dessert/food-like images, which can be bread, cakes or oranges, while the Cortex2Image reconstructions all contain a clear hole in the middle of a donut which exactly matches the shape of the original image.\nA massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Ghislain Emily J Allen, Yihan St-Yves, Jesse L Wu, Jacob S Breedlove, Prince, Matthias Logan T Dowdle, Brad Nau, Franco Caron, Ian Pestilli, Charest, Nature neuroscience. Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, pages 1-11, 2021.\n\nFrom voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, Michal Irani, Advances in Neural Information Processing Systems. 32Roman Beliy, Guy Gaziv, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. From voxels to pixels and back: Self-supervision in natural-image reconstruction from fmri. Advances in Neural Information Processing Systems, 32, 2019.\n\nLarge scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, arXiv:1809.11096arXiv preprintAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nUnsupervised learning of visual features by contrasting cluster assignments. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin, Advances in Neural Information Processing Systems. 33Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Ad- vances in Neural Information Processing Systems, 33:9912-9924, 2020.\n\nInstance-conditioned gan. Arantxa Casanova, Marl\u00e8ne Careil, Jakob Verbeek, Michal Drozdzal, Adriana Romero Soriano, Advances in Neural Information Processing Systems. 342021Arantxa Casanova, Marl\u00e8ne Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano. Instance-conditioned gan. Advances in Neural Information Process- ing Systems, 34, 2021.\n\nSharing deep generative representation for perceived image reconstruction from human brain activity. Changde Du, Changying Du, Huiguang He, 2017 International Joint Conference on Neural Networks (IJCNN). IEEEChangde Du, Changying Du, and Huiguang He. Sharing deep generative representation for perceived image reconstruction from human brain activity. In 2017 International Joint Conference on Neural Networks (IJCNN), pages 1049-1056. IEEE, 2017.\n\nA cortical representation of the local visual environment. Russell Epstein, Nancy Kanwisher, Nature. 3926676Russell Epstein and Nancy Kanwisher. A cortical representation of the local visual envi- ronment. Nature, 392(6676):598-601, 1998.\n\nReconstructing perceptive images from brain activity by shape-semantic gan. Tao Fang, Yu Qi, Gang Pan, Advances in Neural Information Processing Systems. 33Tao Fang, Yu Qi, and Gang Pan. Reconstructing perceptive images from brain activity by shape-semantic gan. Advances in Neural Information Processing Systems, 33:13038- 13048, 2020.\n\nContrast adaptation and representation in human early visual cortex. L Justin, Pei Gardner, Allen Sun, Kenichi Waggoner, Keiji Ueno, Kang Tanaka, Cheng, Neuron. 474Justin L Gardner, Pei Sun, R Allen Waggoner, Kenichi Ueno, Keiji Tanaka, and Kang Cheng. Contrast adaptation and representation in human early visual cortex. Neuron, 47(4):607-620, 2005.\n\nNeurogen: activation optimized image synthesis for discovery neuroscience. Zijin Gu, Keith Wakefield Jamison, Meenakshi Khosla, Emily J Allen, Yihan Wu, Ghislain St-Yves, Thomas Naselaris, Kendrick Kay, R Mert, Amy Sabuncu, Kuceyeski, NeuroImage. 247118812Zijin Gu, Keith Wakefield Jamison, Meenakshi Khosla, Emily J Allen, Yihan Wu, Ghislain St-Yves, Thomas Naselaris, Kendrick Kay, Mert R Sabuncu, and Amy Kuceyeski. Neuro- gen: activation optimized image synthesis for discovery neuroscience. NeuroImage, 247: 118812, 2022.\n\nCompletely derandomized self-adaptation in evolution strategies. Nikolaus Hansen, Andreas Ostermeier, Evolutionary computation. 92Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159-195, 2001.\n\nChiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Marcus, Matthias Niessner, arXiv:1901.02039Spherical cnns on unstructured grids. arXiv preprintChiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Marcus, Matthias Niessner, et al. Spherical cnns on unstructured grids. arXiv preprint arXiv:1901.02039, 2019.\n\nThe fusiform face area: a module in human extrastriate cortex specialized for face perception. Nancy Kanwisher, Josh Mcdermott, Marvin M Chun, Journal of neuroscience. 1711Nancy Kanwisher, Josh McDermott, and Marvin M Chun. The fusiform face area: a module in human extrastriate cortex specialized for face perception. Journal of neuroscience, 17 (11):4302-4311, 1997.\n\nA self-supervised domain-general learning framework for human ventral stream representation. Talia Konkle, A George, Alvarez, Nature Communications. 131Talia Konkle and George A Alvarez. A self-supervised domain-general learning framework for human ventral stream representation. Nature Communications, 13(1):1-12, 2022.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nDcnn-gan: Reconstructing realistic image from fmri. Yunfeng Lin, Jiangbei Li, Hanjing Wang, 16th International Conference on Machine Vision Applications (MVA). IEEEYunfeng Lin, Jiangbei Li, and Hanjing Wang. Dcnn-gan: Reconstructing realistic image from fmri. In 2019 16th International Conference on Machine Vision Applications (MVA), pages 1-6. IEEE, 2019.\n\nObject-related activity revealed by functional magnetic resonance imaging in human occipital cortex. Rafael Malach, Reppas, Benson, H Kwong, Jiang, Wa Kennedy, Pj Ledden, Brady, Br Rosen, Proceedings of the National Academy of Sciences. 9218Rafael Malach, JB Reppas, RR Benson, KK Kwong, H Jiang, WA Kennedy, PJ Ledden, TJ Brady, BR Rosen, and RB Tootell. Object-related activity revealed by functional mag- netic resonance imaging in human occipital cortex. Proceedings of the National Academy of Sciences, 92(18):8135-8139, 1995.\n\nVisual image reconstruction from human brain activity using a combination of multiscale local image decoders. Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Masa-Aki Sato, Yusuke Morito, C Hiroki, Norihiro Tanabe, Yukiyasu Sadato, Kamitani, Neuron. 605Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Masa-aki Sato, Yusuke Morito, Hiroki C Tanabe, Norihiro Sadato, and Yukiyasu Kamitani. Visual image reconstruction from human brain activity using a combination of multiscale local image decoders. Neuron, 60 (5):915-929, 2008.\n\nBayesian reconstruction of natural images from human brain activity. Thomas Naselaris, J Ryan, Prenger, N Kendrick, Michael Kay, Jack L Oliver, Gallant, Neuron. 636Thomas Naselaris, Ryan J Prenger, Kendrick N Kay, Michael Oliver, and Jack L Gallant. Bayesian reconstruction of natural images from human brain activity. Neuron, 63(6): 902-915, 2009.\n\nPredicting individual task contrasts from resting-state functional connectivity using a surface-based convolutional network. H Gia, Meenakshi Ngo, Keith Khosla, Amy Jamison, Mert R Kuceyeski, Sabuncu, NeuroImage. 248118849Gia H Ngo, Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, and Mert R Sabuncu. Predicting individual task contrasts from resting-state functional connectivity using a surface-based convolutional network. NeuroImage, 248:118849, 2022.\n\nReconstructing visual experiences from brain activity evoked by natural movies. Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, Jack L Gallant, Current biology. 2119Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L Gallant. Reconstructing visual experiences from brain activity evoked by natural movies. Current biology, 21(19):1641-1646, 2011.\n\nReconstruction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans. Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, Rufin Vanrullen, arXiv:2202.12692arXiv preprintFurkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, and Rufin VanRullen. Re- construction of perceived images from fmri patterns and semantic brain exploration using instance-conditioned gans. arXiv preprint arXiv:2202.12692, 2022.\n\nNeuronal correlates of perception in early visual cortex. David Ress, J David, Heeger, Nature neuroscience. 64David Ress and David J Heeger. Neuronal correlates of perception in early visual cortex. Nature neuroscience, 6(4):414-420, 2003.\n\nIntegrative benchmarking to advance neurally mechanistic models of human intelligence. Martin Schrimpf, Jonas Kubilius, J Michael, Lee, Apurva Ratan, Robert Murty, James J Ajemian, Dicarlo, Neuron. 30605Martin Schrimpf, Jonas Kubilius, Michael J Lee, N Apurva Ratan Murty, Robert Ajemian, and James J DiCarlo. Integrative benchmarking to advance neurally mechanistic models of human intelligence. Neuron, 2020. URL https://www.cell.com/neuron/fulltext/ S0896-6273(20)30605-X.\n\nGenerative adversarial networks for reconstructing natural images from brain activity. Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, Van Gerven, NeuroImage. 181Katja Seeliger, Umut G\u00fc\u00e7l\u00fc, Luca Ambrogioni, Yagmur G\u00fc\u00e7l\u00fct\u00fcrk, and Marcel AJ van Gerven. Generative adversarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775-785, 2018.\n\nEnd-to-end deep image reconstruction from human brain activity. Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, Yukiyasu Kamitani, Frontiers in Computational Neuroscience. 21Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. End-to-end deep image reconstruction from human brain activity. Frontiers in Compu- tational Neuroscience, page 21, 2019a.\n\nDeep image reconstruction from human brain activity. Guohua Shen, Tomoyasu Horikawa, Kei Majima, Yukiyasu Kamitani, PLoS computational biology. 1511006633Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani. Deep image reconstruction from human brain activity. PLoS computational biology, 15(1):e1006633, 2019b.\n\nGenerative adversarial networks conditioned on brain activity reconstruct seen images. Ghislain St, -Yves , Thomas Naselaris, 2018 IEEE international conference on systems, man, and cybernetics (SMC). IEEEGhislain St-Yves and Thomas Naselaris. Generative adversarial networks conditioned on brain activity reconstruct seen images. In 2018 IEEE international conference on systems, man, and cybernetics (SMC), pages 1054-1061. IEEE, 2018.\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, International conference on machine learning. PMLRMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105-6114. PMLR, 2019.\n\nParcellations and hemispheric asymmetries of human cerebral cortex analyzed on surface-based atlases. David C Van Essen, F Matthew, Donna L Glasser, John Dierker, Timothy Harwell, Coalson, Cerebral cortex. 2210David C Van Essen, Matthew F Glasser, Donna L Dierker, John Harwell, and Timothy Coalson. Parcellations and hemispheric asymmetries of human cerebral cortex analyzed on surface-based atlases. Cerebral cortex, 22(10):2241-2262, 2012.\n\nReconstructing faces from fmri patterns using deep generative neural networks. Rufin Vanrullen, Leila Reddy, Communications biology. 21Rufin VanRullen and Leila Reddy. Reconstructing faces from fmri patterns using deep generative neural networks. Communications biology, 2(1):1-10, 2019.\n\nUnsupervised neural network models of the ventral visual stream. Chengxu Zhuang, Siming Yan, Aran Nayebi, Martin Schrimpf, C Michael, James J Frank, Dicarlo, Yamins, Proceedings of the National Academy of Sciences. 11832021Chengxu Zhuang, Siming Yan, Aran Nayebi, Martin Schrimpf, Michael C Frank, James J DiCarlo, and Daniel LK Yamins. Unsupervised neural network models of the ventral visual stream. Proceedings of the National Academy of Sciences, 118(3), 2021.\n", "annotations": {"author": "[{\"end\":204,\"start\":92},{\"end\":287,\"start\":205},{\"end\":370,\"start\":288},{\"end\":576,\"start\":371}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":98},{\"end\":218,\"start\":211},{\"end\":301,\"start\":292},{\"end\":383,\"start\":376}]", "author_first_name": "[{\"end\":97,\"start\":92},{\"end\":210,\"start\":205},{\"end\":291,\"start\":288},{\"end\":375,\"start\":371}]", "author_affiliation": "[{\"end\":203,\"start\":102},{\"end\":286,\"start\":220},{\"end\":369,\"start\":303},{\"end\":507,\"start\":406},{\"end\":575,\"start\":509}]", "title": "[{\"end\":89,\"start\":1},{\"end\":665,\"start\":577}]", "venue": null, "abstract": "[{\"end\":1641,\"start\":720}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2500,\"start\":2477},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2523,\"start\":2500},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2546,\"start\":2523},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2838,\"start\":2809},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2860,\"start\":2838},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2879,\"start\":2860},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2896,\"start\":2879},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2915,\"start\":2896},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2941,\"start\":2915},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2959,\"start\":2941},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2975,\"start\":2959},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2996,\"start\":2975},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3115,\"start\":3098},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3134,\"start\":3115},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3152,\"start\":3134},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3516,\"start\":3493},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3537,\"start\":3516},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3590,\"start\":3566},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3625,\"start\":3596},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3655,\"start\":3634},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4495,\"start\":4473},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5959,\"start\":5941},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7376,\"start\":7354},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7466,\"start\":7446},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7563,\"start\":7545},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8166,\"start\":8146},{\"end\":8291,\"start\":8268},{\"end\":8931,\"start\":8908},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9322,\"start\":9303},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11796,\"start\":11774},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12551,\"start\":12522},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12989,\"start\":12967},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14062,\"start\":14040},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14789,\"start\":14771},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14871,\"start\":14848},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15320,\"start\":15296},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16103,\"start\":16083},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16285,\"start\":16265},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16592,\"start\":16566},{\"end\":16612,\"start\":16592}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22869,\"start\":22633},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22992,\"start\":22870}]", "paragraph": "[{\"end\":2199,\"start\":1657},{\"end\":3317,\"start\":2201},{\"end\":4053,\"start\":3319},{\"end\":5337,\"start\":4055},{\"end\":6875,\"start\":5358},{\"end\":7078,\"start\":6927},{\"end\":8369,\"start\":7080},{\"end\":8877,\"start\":8371},{\"end\":9050,\"start\":8898},{\"end\":11273,\"start\":9088},{\"end\":11673,\"start\":11391},{\"end\":12990,\"start\":11703},{\"end\":13237,\"start\":12992},{\"end\":13529,\"start\":13239},{\"end\":13715,\"start\":13531},{\"end\":14063,\"start\":13717},{\"end\":14388,\"start\":14065},{\"end\":15090,\"start\":14411},{\"end\":16956,\"start\":15117},{\"end\":17842,\"start\":16958},{\"end\":18887,\"start\":17854},{\"end\":19881,\"start\":18889},{\"end\":20866,\"start\":19912},{\"end\":21922,\"start\":20938},{\"end\":22632,\"start\":21937}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9087,\"start\":9051},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11390,\"start\":11274}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1655,\"start\":1643},{\"attributes\":{\"n\":\"2.\"},\"end\":5346,\"start\":5340},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5356,\"start\":5349},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6925,\"start\":6878},{\"end\":8896,\"start\":8880},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11701,\"start\":11676},{\"attributes\":{\"n\":\"2.4.\"},\"end\":14409,\"start\":14391},{\"attributes\":{\"n\":\"2.5.\"},\"end\":15115,\"start\":15093},{\"attributes\":{\"n\":\"3.\"},\"end\":17852,\"start\":17845},{\"attributes\":{\"n\":\"3.1.\"},\"end\":19910,\"start\":19884},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20936,\"start\":20869},{\"attributes\":{\"n\":\"4.\"},\"end\":21935,\"start\":21925},{\"end\":22644,\"start\":22634},{\"end\":22881,\"start\":22871}]", "table": null, "figure_caption": "[{\"end\":22869,\"start\":22646},{\"end\":22992,\"start\":22883}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7077,\"start\":7068},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7794,\"start\":7785},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10255,\"start\":10246},{\"end\":17865,\"start\":17857},{\"end\":19799,\"start\":19791},{\"end\":20122,\"start\":20114},{\"end\":21197,\"start\":21189},{\"end\":21766,\"start\":21757}]", "bib_author_first_name": "[{\"end\":23559,\"start\":23551},{\"end\":23580,\"start\":23575},{\"end\":23595,\"start\":23590},{\"end\":23597,\"start\":23596},{\"end\":23607,\"start\":23602},{\"end\":23609,\"start\":23608},{\"end\":23637,\"start\":23629},{\"end\":23658,\"start\":23654},{\"end\":23670,\"start\":23664},{\"end\":23681,\"start\":23678},{\"end\":24104,\"start\":24099},{\"end\":24115,\"start\":24112},{\"end\":24128,\"start\":24123},{\"end\":24145,\"start\":24136},{\"end\":24160,\"start\":24157},{\"end\":24174,\"start\":24168},{\"end\":24550,\"start\":24544},{\"end\":24562,\"start\":24558},{\"end\":24577,\"start\":24572},{\"end\":24859,\"start\":24851},{\"end\":24872,\"start\":24867},{\"end\":24886,\"start\":24880},{\"end\":24900,\"start\":24895},{\"end\":24913,\"start\":24908},{\"end\":24932,\"start\":24926},{\"end\":25272,\"start\":25265},{\"end\":25290,\"start\":25283},{\"end\":25304,\"start\":25299},{\"end\":25320,\"start\":25314},{\"end\":25338,\"start\":25331},{\"end\":25345,\"start\":25339},{\"end\":25704,\"start\":25697},{\"end\":25718,\"start\":25709},{\"end\":25731,\"start\":25723},{\"end\":26111,\"start\":26104},{\"end\":26126,\"start\":26121},{\"end\":26364,\"start\":26361},{\"end\":26373,\"start\":26371},{\"end\":26382,\"start\":26378},{\"end\":26693,\"start\":26692},{\"end\":26705,\"start\":26702},{\"end\":26720,\"start\":26715},{\"end\":26733,\"start\":26726},{\"end\":26749,\"start\":26744},{\"end\":26760,\"start\":26756},{\"end\":27055,\"start\":27050},{\"end\":27065,\"start\":27060},{\"end\":27075,\"start\":27066},{\"end\":27094,\"start\":27085},{\"end\":27108,\"start\":27103},{\"end\":27110,\"start\":27109},{\"end\":27123,\"start\":27118},{\"end\":27136,\"start\":27128},{\"end\":27152,\"start\":27146},{\"end\":27172,\"start\":27164},{\"end\":27179,\"start\":27178},{\"end\":27189,\"start\":27186},{\"end\":27576,\"start\":27568},{\"end\":27592,\"start\":27585},{\"end\":27790,\"start\":27785},{\"end\":27805,\"start\":27798},{\"end\":27820,\"start\":27813},{\"end\":27838,\"start\":27832},{\"end\":27855,\"start\":27847},{\"end\":28200,\"start\":28195},{\"end\":28216,\"start\":28212},{\"end\":28236,\"start\":28228},{\"end\":28568,\"start\":28563},{\"end\":28578,\"start\":28577},{\"end\":28843,\"start\":28835},{\"end\":28856,\"start\":28849},{\"end\":28869,\"start\":28864},{\"end\":28885,\"start\":28880},{\"end\":28898,\"start\":28892},{\"end\":28911,\"start\":28907},{\"end\":28926,\"start\":28921},{\"end\":28945,\"start\":28935},{\"end\":29304,\"start\":29297},{\"end\":29318,\"start\":29310},{\"end\":29330,\"start\":29323},{\"end\":29712,\"start\":29706},{\"end\":29738,\"start\":29737},{\"end\":30254,\"start\":30248},{\"end\":30271,\"start\":30265},{\"end\":30285,\"start\":30280},{\"end\":30305,\"start\":30297},{\"end\":30318,\"start\":30312},{\"end\":30328,\"start\":30327},{\"end\":30345,\"start\":30337},{\"end\":30362,\"start\":30354},{\"end\":30744,\"start\":30738},{\"end\":30757,\"start\":30756},{\"end\":30774,\"start\":30773},{\"end\":30792,\"start\":30785},{\"end\":30802,\"start\":30798},{\"end\":30804,\"start\":30803},{\"end\":31145,\"start\":31144},{\"end\":31160,\"start\":31151},{\"end\":31171,\"start\":31166},{\"end\":31183,\"start\":31180},{\"end\":31199,\"start\":31193},{\"end\":31562,\"start\":31556},{\"end\":31576,\"start\":31574},{\"end\":31578,\"start\":31577},{\"end\":31589,\"start\":31583},{\"end\":31606,\"start\":31601},{\"end\":31621,\"start\":31618},{\"end\":31630,\"start\":31626},{\"end\":31632,\"start\":31631},{\"end\":31999,\"start\":31993},{\"end\":32015,\"start\":32009},{\"end\":32029,\"start\":32024},{\"end\":32045,\"start\":32040},{\"end\":32058,\"start\":32053},{\"end\":32404,\"start\":32399},{\"end\":32412,\"start\":32411},{\"end\":32675,\"start\":32669},{\"end\":32691,\"start\":32686},{\"end\":32703,\"start\":32702},{\"end\":32738,\"start\":32732},{\"end\":32751,\"start\":32746},{\"end\":32753,\"start\":32752},{\"end\":33151,\"start\":33146},{\"end\":33166,\"start\":33162},{\"end\":33178,\"start\":33174},{\"end\":33197,\"start\":33191},{\"end\":33514,\"start\":33508},{\"end\":33528,\"start\":33521},{\"end\":33541,\"start\":33538},{\"end\":33558,\"start\":33550},{\"end\":33577,\"start\":33569},{\"end\":33898,\"start\":33892},{\"end\":33913,\"start\":33905},{\"end\":33927,\"start\":33924},{\"end\":33944,\"start\":33936},{\"end\":34260,\"start\":34252},{\"end\":34270,\"start\":34265},{\"end\":34279,\"start\":34273},{\"end\":34686,\"start\":34678},{\"end\":34696,\"start\":34692},{\"end\":35052,\"start\":35051},{\"end\":35067,\"start\":35062},{\"end\":35069,\"start\":35068},{\"end\":35083,\"start\":35079},{\"end\":35100,\"start\":35093},{\"end\":35458,\"start\":35453},{\"end\":35475,\"start\":35470},{\"end\":35735,\"start\":35728},{\"end\":35750,\"start\":35744},{\"end\":35760,\"start\":35756},{\"end\":35775,\"start\":35769},{\"end\":35787,\"start\":35786},{\"end\":35802,\"start\":35797},{\"end\":35804,\"start\":35803}]", "bib_author_last_name": "[{\"end\":23573,\"start\":23560},{\"end\":23588,\"start\":23581},{\"end\":23600,\"start\":23598},{\"end\":23619,\"start\":23610},{\"end\":23627,\"start\":23621},{\"end\":23652,\"start\":23638},{\"end\":23662,\"start\":23659},{\"end\":23676,\"start\":23671},{\"end\":23690,\"start\":23682},{\"end\":23699,\"start\":23692},{\"end\":24110,\"start\":24105},{\"end\":24121,\"start\":24116},{\"end\":24134,\"start\":24129},{\"end\":24155,\"start\":24146},{\"end\":24166,\"start\":24161},{\"end\":24180,\"start\":24175},{\"end\":24556,\"start\":24551},{\"end\":24570,\"start\":24563},{\"end\":24586,\"start\":24578},{\"end\":24865,\"start\":24860},{\"end\":24878,\"start\":24873},{\"end\":24893,\"start\":24887},{\"end\":24906,\"start\":24901},{\"end\":24924,\"start\":24914},{\"end\":24939,\"start\":24933},{\"end\":25281,\"start\":25273},{\"end\":25297,\"start\":25291},{\"end\":25312,\"start\":25305},{\"end\":25329,\"start\":25321},{\"end\":25353,\"start\":25346},{\"end\":25707,\"start\":25705},{\"end\":25721,\"start\":25719},{\"end\":25734,\"start\":25732},{\"end\":26119,\"start\":26112},{\"end\":26136,\"start\":26127},{\"end\":26369,\"start\":26365},{\"end\":26376,\"start\":26374},{\"end\":26386,\"start\":26383},{\"end\":26700,\"start\":26694},{\"end\":26713,\"start\":26706},{\"end\":26724,\"start\":26721},{\"end\":26742,\"start\":26734},{\"end\":26754,\"start\":26750},{\"end\":26767,\"start\":26761},{\"end\":26774,\"start\":26769},{\"end\":27058,\"start\":27056},{\"end\":27083,\"start\":27076},{\"end\":27101,\"start\":27095},{\"end\":27116,\"start\":27111},{\"end\":27126,\"start\":27124},{\"end\":27144,\"start\":27137},{\"end\":27162,\"start\":27153},{\"end\":27176,\"start\":27173},{\"end\":27184,\"start\":27180},{\"end\":27197,\"start\":27190},{\"end\":27208,\"start\":27199},{\"end\":27583,\"start\":27577},{\"end\":27603,\"start\":27593},{\"end\":27796,\"start\":27791},{\"end\":27811,\"start\":27806},{\"end\":27830,\"start\":27821},{\"end\":27845,\"start\":27839},{\"end\":27864,\"start\":27856},{\"end\":28210,\"start\":28201},{\"end\":28226,\"start\":28217},{\"end\":28241,\"start\":28237},{\"end\":28575,\"start\":28569},{\"end\":28585,\"start\":28579},{\"end\":28594,\"start\":28587},{\"end\":28847,\"start\":28844},{\"end\":28862,\"start\":28857},{\"end\":28878,\"start\":28870},{\"end\":28890,\"start\":28886},{\"end\":28905,\"start\":28899},{\"end\":28919,\"start\":28912},{\"end\":28933,\"start\":28927},{\"end\":28953,\"start\":28946},{\"end\":29308,\"start\":29305},{\"end\":29321,\"start\":29319},{\"end\":29335,\"start\":29331},{\"end\":29719,\"start\":29713},{\"end\":29727,\"start\":29721},{\"end\":29735,\"start\":29729},{\"end\":29744,\"start\":29739},{\"end\":29751,\"start\":29746},{\"end\":29763,\"start\":29753},{\"end\":29774,\"start\":29765},{\"end\":29781,\"start\":29776},{\"end\":29791,\"start\":29783},{\"end\":30263,\"start\":30255},{\"end\":30278,\"start\":30272},{\"end\":30295,\"start\":30286},{\"end\":30310,\"start\":30306},{\"end\":30325,\"start\":30319},{\"end\":30335,\"start\":30329},{\"end\":30352,\"start\":30346},{\"end\":30369,\"start\":30363},{\"end\":30379,\"start\":30371},{\"end\":30754,\"start\":30745},{\"end\":30762,\"start\":30758},{\"end\":30771,\"start\":30764},{\"end\":30783,\"start\":30775},{\"end\":30796,\"start\":30793},{\"end\":30811,\"start\":30805},{\"end\":30820,\"start\":30813},{\"end\":31149,\"start\":31146},{\"end\":31164,\"start\":31161},{\"end\":31178,\"start\":31172},{\"end\":31191,\"start\":31184},{\"end\":31209,\"start\":31200},{\"end\":31218,\"start\":31211},{\"end\":31572,\"start\":31563},{\"end\":31581,\"start\":31579},{\"end\":31599,\"start\":31590},{\"end\":31616,\"start\":31607},{\"end\":31624,\"start\":31622},{\"end\":31640,\"start\":31633},{\"end\":32007,\"start\":32000},{\"end\":32022,\"start\":32016},{\"end\":32038,\"start\":32030},{\"end\":32051,\"start\":32046},{\"end\":32068,\"start\":32059},{\"end\":32409,\"start\":32405},{\"end\":32418,\"start\":32413},{\"end\":32426,\"start\":32420},{\"end\":32684,\"start\":32676},{\"end\":32700,\"start\":32692},{\"end\":32711,\"start\":32704},{\"end\":32716,\"start\":32713},{\"end\":32730,\"start\":32718},{\"end\":32744,\"start\":32739},{\"end\":32761,\"start\":32754},{\"end\":32770,\"start\":32763},{\"end\":33160,\"start\":33152},{\"end\":33172,\"start\":33167},{\"end\":33189,\"start\":33179},{\"end\":33207,\"start\":33198},{\"end\":33219,\"start\":33209},{\"end\":33519,\"start\":33515},{\"end\":33536,\"start\":33529},{\"end\":33548,\"start\":33542},{\"end\":33567,\"start\":33559},{\"end\":33586,\"start\":33578},{\"end\":33903,\"start\":33899},{\"end\":33922,\"start\":33914},{\"end\":33934,\"start\":33928},{\"end\":33953,\"start\":33945},{\"end\":34263,\"start\":34261},{\"end\":34289,\"start\":34280},{\"end\":34690,\"start\":34687},{\"end\":34699,\"start\":34697},{\"end\":35049,\"start\":35032},{\"end\":35060,\"start\":35053},{\"end\":35077,\"start\":35070},{\"end\":35091,\"start\":35084},{\"end\":35108,\"start\":35101},{\"end\":35117,\"start\":35110},{\"end\":35468,\"start\":35459},{\"end\":35481,\"start\":35476},{\"end\":35742,\"start\":35736},{\"end\":35754,\"start\":35751},{\"end\":35767,\"start\":35761},{\"end\":35784,\"start\":35776},{\"end\":35795,\"start\":35788},{\"end\":35810,\"start\":35805},{\"end\":35819,\"start\":35812},{\"end\":35827,\"start\":35821}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":245262002},\"end\":24005,\"start\":23463},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":195798779},\"end\":24474,\"start\":24007},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b2\"},\"end\":24772,\"start\":24476},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219721240},\"end\":25237,\"start\":24774},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":237491885},\"end\":25594,\"start\":25239},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13509748},\"end\":26043,\"start\":25596},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":920141},\"end\":26283,\"start\":26045},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":227275511},\"end\":26621,\"start\":26285},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18537582},\"end\":26973,\"start\":26623},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":234741984},\"end\":27501,\"start\":26975},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7524826},\"end\":27783,\"start\":27503},{\"attributes\":{\"doi\":\"arXiv:1901.02039\",\"id\":\"b11\"},\"end\":28098,\"start\":27785},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":86163},\"end\":28468,\"start\":28100},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":246247143},\"end\":28790,\"start\":28470},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14113767},\"end\":29243,\"start\":28792},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":58981900},\"end\":29603,\"start\":29245},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11672239},\"end\":30136,\"start\":29605},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17327816},\"end\":30667,\"start\":30138},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1609402},\"end\":31017,\"start\":30669},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233416161},\"end\":31474,\"start\":31019},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10788863},\"end\":31873,\"start\":31476},{\"attributes\":{\"doi\":\"arXiv:2202.12692\",\"id\":\"b21\"},\"end\":32339,\"start\":31875},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1417293},\"end\":32580,\"start\":32341},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221618949},\"end\":33057,\"start\":32582},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51709940},\"end\":33442,\"start\":33059},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":91110332},\"end\":33837,\"start\":33444},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":58667578},\"end\":34163,\"start\":33839},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":58670534},\"end\":34602,\"start\":34165},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":167217261},\"end\":34928,\"start\":34604},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12285818},\"end\":35372,\"start\":34930},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52945847},\"end\":35661,\"start\":35374},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":219947382},\"end\":36127,\"start\":35663}]", "bib_title": "[{\"end\":23549,\"start\":23463},{\"end\":24097,\"start\":24007},{\"end\":24849,\"start\":24774},{\"end\":25263,\"start\":25239},{\"end\":25695,\"start\":25596},{\"end\":26102,\"start\":26045},{\"end\":26359,\"start\":26285},{\"end\":26690,\"start\":26623},{\"end\":27048,\"start\":26975},{\"end\":27566,\"start\":27503},{\"end\":28193,\"start\":28100},{\"end\":28561,\"start\":28470},{\"end\":28833,\"start\":28792},{\"end\":29295,\"start\":29245},{\"end\":29704,\"start\":29605},{\"end\":30246,\"start\":30138},{\"end\":30736,\"start\":30669},{\"end\":31142,\"start\":31019},{\"end\":31554,\"start\":31476},{\"end\":32397,\"start\":32341},{\"end\":32667,\"start\":32582},{\"end\":33144,\"start\":33059},{\"end\":33506,\"start\":33444},{\"end\":33890,\"start\":33839},{\"end\":34250,\"start\":34165},{\"end\":34676,\"start\":34604},{\"end\":35030,\"start\":34930},{\"end\":35451,\"start\":35374},{\"end\":35726,\"start\":35663}]", "bib_author": "[{\"end\":23575,\"start\":23551},{\"end\":23590,\"start\":23575},{\"end\":23602,\"start\":23590},{\"end\":23621,\"start\":23602},{\"end\":23629,\"start\":23621},{\"end\":23654,\"start\":23629},{\"end\":23664,\"start\":23654},{\"end\":23678,\"start\":23664},{\"end\":23692,\"start\":23678},{\"end\":23701,\"start\":23692},{\"end\":24112,\"start\":24099},{\"end\":24123,\"start\":24112},{\"end\":24136,\"start\":24123},{\"end\":24157,\"start\":24136},{\"end\":24168,\"start\":24157},{\"end\":24182,\"start\":24168},{\"end\":24558,\"start\":24544},{\"end\":24572,\"start\":24558},{\"end\":24588,\"start\":24572},{\"end\":24867,\"start\":24851},{\"end\":24880,\"start\":24867},{\"end\":24895,\"start\":24880},{\"end\":24908,\"start\":24895},{\"end\":24926,\"start\":24908},{\"end\":24941,\"start\":24926},{\"end\":25283,\"start\":25265},{\"end\":25299,\"start\":25283},{\"end\":25314,\"start\":25299},{\"end\":25331,\"start\":25314},{\"end\":25355,\"start\":25331},{\"end\":25709,\"start\":25697},{\"end\":25723,\"start\":25709},{\"end\":25736,\"start\":25723},{\"end\":26121,\"start\":26104},{\"end\":26138,\"start\":26121},{\"end\":26371,\"start\":26361},{\"end\":26378,\"start\":26371},{\"end\":26388,\"start\":26378},{\"end\":26702,\"start\":26692},{\"end\":26715,\"start\":26702},{\"end\":26726,\"start\":26715},{\"end\":26744,\"start\":26726},{\"end\":26756,\"start\":26744},{\"end\":26769,\"start\":26756},{\"end\":26776,\"start\":26769},{\"end\":27060,\"start\":27050},{\"end\":27085,\"start\":27060},{\"end\":27103,\"start\":27085},{\"end\":27118,\"start\":27103},{\"end\":27128,\"start\":27118},{\"end\":27146,\"start\":27128},{\"end\":27164,\"start\":27146},{\"end\":27178,\"start\":27164},{\"end\":27186,\"start\":27178},{\"end\":27199,\"start\":27186},{\"end\":27210,\"start\":27199},{\"end\":27585,\"start\":27568},{\"end\":27605,\"start\":27585},{\"end\":27798,\"start\":27785},{\"end\":27813,\"start\":27798},{\"end\":27832,\"start\":27813},{\"end\":27847,\"start\":27832},{\"end\":27866,\"start\":27847},{\"end\":28212,\"start\":28195},{\"end\":28228,\"start\":28212},{\"end\":28243,\"start\":28228},{\"end\":28577,\"start\":28563},{\"end\":28587,\"start\":28577},{\"end\":28596,\"start\":28587},{\"end\":28849,\"start\":28835},{\"end\":28864,\"start\":28849},{\"end\":28880,\"start\":28864},{\"end\":28892,\"start\":28880},{\"end\":28907,\"start\":28892},{\"end\":28921,\"start\":28907},{\"end\":28935,\"start\":28921},{\"end\":28955,\"start\":28935},{\"end\":29310,\"start\":29297},{\"end\":29323,\"start\":29310},{\"end\":29337,\"start\":29323},{\"end\":29721,\"start\":29706},{\"end\":29729,\"start\":29721},{\"end\":29737,\"start\":29729},{\"end\":29746,\"start\":29737},{\"end\":29753,\"start\":29746},{\"end\":29765,\"start\":29753},{\"end\":29776,\"start\":29765},{\"end\":29783,\"start\":29776},{\"end\":29793,\"start\":29783},{\"end\":30265,\"start\":30248},{\"end\":30280,\"start\":30265},{\"end\":30297,\"start\":30280},{\"end\":30312,\"start\":30297},{\"end\":30327,\"start\":30312},{\"end\":30337,\"start\":30327},{\"end\":30354,\"start\":30337},{\"end\":30371,\"start\":30354},{\"end\":30381,\"start\":30371},{\"end\":30756,\"start\":30738},{\"end\":30764,\"start\":30756},{\"end\":30773,\"start\":30764},{\"end\":30785,\"start\":30773},{\"end\":30798,\"start\":30785},{\"end\":30813,\"start\":30798},{\"end\":30822,\"start\":30813},{\"end\":31151,\"start\":31144},{\"end\":31166,\"start\":31151},{\"end\":31180,\"start\":31166},{\"end\":31193,\"start\":31180},{\"end\":31211,\"start\":31193},{\"end\":31220,\"start\":31211},{\"end\":31574,\"start\":31556},{\"end\":31583,\"start\":31574},{\"end\":31601,\"start\":31583},{\"end\":31618,\"start\":31601},{\"end\":31626,\"start\":31618},{\"end\":31642,\"start\":31626},{\"end\":32009,\"start\":31993},{\"end\":32024,\"start\":32009},{\"end\":32040,\"start\":32024},{\"end\":32053,\"start\":32040},{\"end\":32070,\"start\":32053},{\"end\":32411,\"start\":32399},{\"end\":32420,\"start\":32411},{\"end\":32428,\"start\":32420},{\"end\":32686,\"start\":32669},{\"end\":32702,\"start\":32686},{\"end\":32713,\"start\":32702},{\"end\":32718,\"start\":32713},{\"end\":32732,\"start\":32718},{\"end\":32746,\"start\":32732},{\"end\":32763,\"start\":32746},{\"end\":32772,\"start\":32763},{\"end\":33162,\"start\":33146},{\"end\":33174,\"start\":33162},{\"end\":33191,\"start\":33174},{\"end\":33209,\"start\":33191},{\"end\":33221,\"start\":33209},{\"end\":33521,\"start\":33508},{\"end\":33538,\"start\":33521},{\"end\":33550,\"start\":33538},{\"end\":33569,\"start\":33550},{\"end\":33588,\"start\":33569},{\"end\":33905,\"start\":33892},{\"end\":33924,\"start\":33905},{\"end\":33936,\"start\":33924},{\"end\":33955,\"start\":33936},{\"end\":34265,\"start\":34252},{\"end\":34273,\"start\":34265},{\"end\":34291,\"start\":34273},{\"end\":34692,\"start\":34678},{\"end\":34701,\"start\":34692},{\"end\":35051,\"start\":35032},{\"end\":35062,\"start\":35051},{\"end\":35079,\"start\":35062},{\"end\":35093,\"start\":35079},{\"end\":35110,\"start\":35093},{\"end\":35119,\"start\":35110},{\"end\":35470,\"start\":35453},{\"end\":35483,\"start\":35470},{\"end\":35744,\"start\":35728},{\"end\":35756,\"start\":35744},{\"end\":35769,\"start\":35756},{\"end\":35786,\"start\":35769},{\"end\":35797,\"start\":35786},{\"end\":35812,\"start\":35797},{\"end\":35821,\"start\":35812},{\"end\":35829,\"start\":35821}]", "bib_venue": "[{\"end\":23720,\"start\":23701},{\"end\":24231,\"start\":24182},{\"end\":24542,\"start\":24476},{\"end\":24990,\"start\":24941},{\"end\":25404,\"start\":25355},{\"end\":25798,\"start\":25736},{\"end\":26144,\"start\":26138},{\"end\":26437,\"start\":26388},{\"end\":26782,\"start\":26776},{\"end\":27220,\"start\":27210},{\"end\":27629,\"start\":27605},{\"end\":27918,\"start\":27882},{\"end\":28266,\"start\":28243},{\"end\":28617,\"start\":28596},{\"end\":28993,\"start\":28955},{\"end\":29403,\"start\":29337},{\"end\":29840,\"start\":29793},{\"end\":30387,\"start\":30381},{\"end\":30828,\"start\":30822},{\"end\":31230,\"start\":31220},{\"end\":31657,\"start\":31642},{\"end\":31991,\"start\":31875},{\"end\":32447,\"start\":32428},{\"end\":32778,\"start\":32772},{\"end\":33231,\"start\":33221},{\"end\":33627,\"start\":33588},{\"end\":33981,\"start\":33955},{\"end\":34364,\"start\":34291},{\"end\":34745,\"start\":34701},{\"end\":35134,\"start\":35119},{\"end\":35505,\"start\":35483},{\"end\":35876,\"start\":35829}]"}}}, "year": 2023, "month": 12, "day": 17}