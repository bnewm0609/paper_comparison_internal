{"id": 246822774, "updated": "2023-11-30 14:18:39.069", "metadata": {"title": "GAMMA Challenge: Glaucoma grAding from Multi-Modality imAges", "authors": "[{\"first\":\"Junde\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Huihui\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Huazhu\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Fengbin\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Jiongcheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Qinji\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Sifan\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Xinxing\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yanyu\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Wensai\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lingxiao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shuai\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Huiqi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shihua\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Zhichao\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Chubin\",\"last\":\"Ou\",\"middle\":[]},{\"first\":\"Xifei\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Bingyuan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Riadh\",\"last\":\"Kobbi\",\"middle\":[]},{\"first\":\"Xiaoying\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Hrvoje\",\"last\":\"Bogunovi\u00b4c\",\"middle\":[]},{\"first\":\"Jos\u00b4e\",\"last\":\"Orlando\",\"middle\":[\"Ignacio\"]},{\"first\":\"Xiulan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yanwu\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yanwu\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "Medical image analysis", "journal": "Medical image analysis", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Glaucoma is a chronic neuro-degenerative condition that is one of the world\u2019s", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "37806020", "pubmedcentral": null, "dblp": "journals/mia/WuFLFLLHYSXXWWLLHLOWLKTLZHBOZX23", "doi": "10.1016/j.media.2023.102938"}}, "content": {"source": {"pdf_hash": "9b13e7e9e6bbc3d75c17199eb553c0a5d589454c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2202.06511v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b461be8280e38820716dcb845b3ee64c0eca5a37", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9b13e7e9e6bbc3d75c17199eb553c0a5d589454c.txt", "contents": "\nGAMMA Challenge: Glaucoma grAding from Multi-Modality imAges\n26 Dec 2022\n\nJunde Wu \nIntelligent Healthcare Unit\nBaidu Inc\nBeijingChina\n\nHuihui Fang \nIntelligent Healthcare Unit\nBaidu Inc\nBeijingChina\n\nFei Li \nState Key Laboratory of Ophthalmology\nGuangdong Provincial Key Laboratory of Ophthalmology and Visual Science\nZhongshan Ophthalmic Center\nSun Yat-sen University\nGuangzhouChina\n\nHuazhu Fu \nInstitute of High Performance Computing (IHPC)\nAgency for Science, Technology and Research (A*STAR)\nSingapore\n\nFengbin Lin \nState Key Laboratory of Ophthalmology\nGuangdong Provincial Key Laboratory of Ophthalmology and Visual Science\nZhongshan Ophthalmic Center\nSun Yat-sen University\nGuangzhouChina\n\nJiongcheng Li \nSchool of Informatics\nXiamen University\nXiamenChina\n\nYue Huang \nSchool of Informatics\nXiamen University\nXiamenChina\n\nQinji Yu \nShanghai Jiao Tong University\nShanghaiChina\n\nSifan Song \nXi'an Jiaotong-Liverpool University\nSuzhouChina\n\nXinxing Xu \nInstitute of High Performance Computing\nA*STAR\nSingapore\n\nYanyu Xu \nInstitute of High Performance Computing\nA*STAR\nSingapore\n\nWensai Wang \nInstitute of Biomedical Engineering\nChinese Academy of Medical Sciences and Peking Union Medical College\nTianjinChina\n\nLingxiao Wang \nShuai Lu \nInstitute of Biomedical Engineering\nChinese Academy of Medical Sciences and Peking Union Medical College\nTianjinChina\n\nSchool of Medical Technology\nBeijing Institute of Technology\nBeijingChina\n\nHuiqi Li \nSchool of Medical Technology\nBeijing Institute of Technology\nBeijingChina\n\nSchool of Information and Electronics\nBeijing Institute of Technology\nBeijingChina\n\nShihua Huang \nDepartment of Computing\nHong Kong Polytechnic University\nHong KongChina\n\nZhichao Lu \nDepartment of Computer Science and Engineering\nSouthern University of Science and Technology\nShenzhenChina\n\nChubin Ou \nm\u00c9 cole de technologie sup\u00e9rieure\nWeizhi Medical Technology Company\nSuzhou, MontrealMontrealChina, Canada\n\nXifei Wei \nm\u00c9 cole de technologie sup\u00e9rieure\nWeizhi Medical Technology Company\nSuzhou, MontrealMontrealChina, Canada\n\nBingyuan Liu \nRiadh Kobbi \nDIAGNOS Inc\nQuebecCanada\n\nXiaoying Tang \nDepartment of Electrical and Electronic Engineering\nSouthern University of Science and Technology\nShenzhenChina\n\nLi Lin \nDepartment of Electrical and Electronic Engineering\nSouthern University of Science and Technology\nShenzhenChina\n\nDepartment of Electrical and Electronic Engineering\nThe University of Hong Kong\nHong KongChina\n\nQiang Zhou \nSuixin (Shanghai) Technology Co., Ltd\nShanghaiChina\n\nQiang Hu \nSuixin (Shanghai) Technology Co., Ltd\nShanghaiChina\n\nHrvoje Bogunovi\u0107 \nDepartment of Ophthalmology\nChristian Doppler Lab for Artificial Intelligence in Retina\nMedical University of Vienna\nAustria\n\nJos\u00e9 Ignacio Orlando \nYatiris Group\nPLADEMA Institute\nCONICET\nUNICEN\nTandilArgentina\n\nXiulan Zhang zhangxl2@mail.sysu.edu.cn \nState Key Laboratory of Ophthalmology\nGuangdong Provincial Key Laboratory of Ophthalmology and Visual Science\nZhongshan Ophthalmic Center\nSun Yat-sen University\nGuangzhouChina\n\nYanwu Xu xuyanwu@baidu.com. \nIntelligent Healthcare Unit\nBaidu Inc\nBeijingChina\n\nYanwu Xu \nGAMMA Challenge: Glaucoma grAding from Multi-Modality imAges\n26 Dec 2022Preprint submitted to Journal of Medical Image Analysis December 27, 2022* * Corresponding authors: Xiulan Zhang ( 1 These authors co-organized the GAMMA challenge. All others contributed results of their algorithms presented in the paper.\nGlaucoma is a chronic neuro-degenerative condition that is one of the world's * These authors contributed equally to the work.\n\nleading causes of irreversible but preventable blindness. The blindness is generally caused by the lack of timely detection and treatment. Early screening is thus essential for early treatment to preserve vision and maintain life quality.\n\nColor fundus photography and Optical Coherence Tomography (OCT) are the two most cost-effective tools for glaucoma screening. Both imaging modalities have prominent biomarkers to indicate glaucoma suspects, such as the vertical cup-to-disc ratio (vCDR) on fundus images and retinal nerve fiber layer (RNFL) thickness on OCT volume. In clinical practice, it is often recommended to take both of the screenings for a more accurate and reliable diagnosis. However, although numerous algorithms are proposed based on fundus images or OCT volumes for the automated glaucoma detection, there are few methods that leverage both of the modalities to achieve the target. To fulfill the research gap, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA) Challenge to encourage the development of fundus & OCT-based glaucoma grading.\n\nThe primary task of the challenge is to grade glaucoma from both the 2D fundus images and 3D OCT scanning volumes. As part of GAMMA, we have publicly released a glaucoma annotated dataset with both 2D fundus color photography and 3D OCT volumes, which is the first multi-modality dataset for machine learning based glaucoma grading. In addition, an evaluation framework is also established to evaluate the performance of the submitted methods. During the challenge, 1272 results were submitted, and finally, ten best performing teams were selected for the final stage. We analyze their results and summarize their methods in the paper. Since all the teams submitted their source code in the challenge, we conducted a detailed ablation study to verify the effectiveness of the particular modules proposed. Finally, we identify the proposed techniques and strategies that could be of practical value for the clinical diagnosis of glaucoma. As the first in-depth study of fundus & OCT multi-modality glaucoma grading, we believe the GAMMA Challenge will serve as an essential guideline and benchmark for future research.\n\n\nIntroduction\n\n\nMultimodal Fused Classification\n\nNormal Early Glaucoma Progressive Glaucoma 3D OCT Volume 2D Fundus Photography Worldwide, glaucoma is the second-leading cause of blindness after cataracts (Resnikoff et al. (2004)). About 70 million people have glaucoma globally (Vos et al. (2016)). Glaucoma can occur without any cause, but is affected by many factors. The most important of which is the intra-ocular eye pressure (IOP).\n\nAqueous humor in the eyes flows through the pupil to the front of the eye.\n\nIn a healthy eye, the fluid leaves through a drainage canal located between the iris and cornea. With glaucoma, the drainage canals become clogged with microscopic deposits. The fluid builds up in the eye. This excess fluid puts pressure on the eye. Eventually, this elevated eye pressure can damage the optic nerve head (ONH) leading to glaucoma.\n\nMany forms of glaucoma have no warning signs. The effect is so gradual that one may not notice a change in vision until the condition is at an advanced stage.\n\nThat is why glaucoma is also called the 'silent thief of sight'. Because vision loss due to glaucoma can not be recovered, it is important for the early diagnosis.\n\nIf glaucoma is recognized early, vision loss can be slowed or prevented.\n\nThe function-based visual field test is the clinical gold standard of glaucoma screening, but it does not show signs of early glaucoma. Instead, an optic nerve head (ONH) assessment is a convenient way to detect early glaucoma and is currently performed widely for glaucoma screening (Jonas et al. (1999); Morgan et al. (2005); Fu et al. (2017)). As practical and noninvasive tools, 2D fundus photography and 3D optical coherence tomography (OCT) are the most commonly used imaging modalities to evaluate the optic nerve structure in clinical practice.\n\nThe main advantage of the fundus photographs is that they can clearly show the optic disc, optic cup, and blood vessels. Among them, the clinical parameters like the vertical cup to disc ratio (vCDR), disc diameter, and the ratio of blood vessels area in inferior-superior side to area of blood vessel in the nasal-temporal side have been validated to be of great significance for the glaucoma diagnosis (Jonas et al. (2000); Hancox OD (1999); Nayak et al. (2009); Li et al. (2022)). OCT measures retinal nerve fiber layer (RNFL) thickness based on its optical properties. RNFL thickness, computed from OCT volumes that are acquired in cylindrical sections surrounding the optic disc, is often used to identify glaucoma suspect. Though OCT volumes and fundus photographs are effective tools for diagnosing early glaucoma, neither of them alone can be used to exclude it. Clinically, ophthalmologists often recommend to take both of the screenings for a more accurate and reliable diagnosis. Recent report shows nearly 46.3% glaucoma cases would be ignored if using fundus images or OCT volume alone (Anton et al. (2021)).\n\nHowever, in terms of computer-aided glaucoma diagnosis, most algorithms are developed on only single modality. Although fundus photographs and OCT are both the mainstream glaucoma screening tools in clinical practice, few algorithms are established that make use of both modalities. This is primarily due to two reasons: a) there is no publicly available dataset to train and evaluate such models, and b) due to the discrepancy in the characteristics and the dimensionality between the two modalities, the task is technically challenging.\n\nIn order to overcome these issues, a challenge with an dataset , as a way to encourage the development of SOTA imaging technology on this clinically rele-vant task, may be an appropriate approach. Inspired by the success of Retinal Fundus Glaucoma Challenge (REFUGE) (Orlando et al. (2020)) we previously held, the Glaucoma grAding from Multi-Modality imAges (GAMMA) Challenge was organized in conjunction with the 8th Ophthalmic Medical Image Analysis (OMIA) workshop, during MICCAI 2021 (Strasbourg, France) to encourage the development of fundus & OCT-based multi-modal glaucoma grading algorithms.\n\nGiven a pair consisting of a fundus image and an OCT volume, the submitted algorithms need to predict the case as normal, early-glaucoma, or progressiveglaucoma (intermediate and advanced stage). An illustration is shown in Figure   1. We also describe an evaluation framework to rank the participated teams.\n\nTen top performing teams were invited to share their technical reports and source code. In brief, the primary contribution of the GAMMA Challenge is two-fold:\n\na) The first publicly available multi-modality glaucoma grading dataset for deep learning based methods is released, providing fundus photography and OCT volume pairs. b) State-of-the-art (SOTA) machine learning methods are evaluated to encourage the development of novel methodologies for fundus & OCT-based glaucoma grading.\n\nDue to the success of the challenge, GAMMA is expected to serve as the main benchmark for this clinically relevant task in the future.\n\nBesides glaucoma grading labels, the optic disc & cup (OD/OC) mask labels as well as fovea location labels are also provided in the GAMMA dataset. These auxiliary tasks were proposed to investigate the role of optic disc and fovea in glaucoma grading. Thus, the participants can also submit algorithms for the OD/OC segmentation task and fovea localization task, and the final team performance includes the achieved scores on these auxiliary tasks. An illustration of the auxiliary tasks is shown in Figure 2. In the GAMMA Challenge, the participants are encouraged to utilize the auxiliary tasks to improve the performance of glaucoma grading.\n\nThe inception of the GAMMA challenge encourages many participants to contribute SOTA machine learning techniques on this task. This manuscript summarizes the GAMMA Challenge, analyzes their results, and investigates their particular approaches. All top-10 teams submitted the source code of their algorithms. This allowed us to conduct a detailed ablation study to identify which techniques were the most effective ones for the screening task. We believe that our analysis of SOTA machine learning methods will greatly benefit the future algorithm design on this task.\n\n\nThe GAMMA challenge\n\nThe GAMMA challenge was officially launched from 20 Mar 2021 to 01 October 2021, which focuses on the field of glaucoma grading based on multi-modality images (Fundus photography and OCT volume). The challenge consisted of a preliminary stage and a final stage. During the preliminary stage, we released a training set for the participating teams to train the models. The registered teams were allowed to use the training set to learn their proposed algorithms for glaucoma grading, and, optionally, for OD/OC segmentation and fovea localization. Their results can be submitted on https://aistudio.baidu.com/ aistudio/competition/detail/90/0/submit-result and would be evaluated on the preliminary set. The registered teams then can see their performance on the preliminary set and adjust their algorithms. For a fair comparison of the proposed methods, the registered teams were not allowed to use any other private data set for developing their methods.\n\nThis preliminary stage lasted 30 days, and each team was allowed to make a maximum of five submissions per day. A total of 70 teams submitted 1272 valid results to the challenge platform during the preliminary stage, out of which ten teams, based on their method performance and the willingness to participate in the OMIA8 workshop, were selected to the final stage. The ten such selected teams were then ranked based on their performance on the final test set. For the final stage, teams were not allowed to modify their models anymore.\n\n\nGAMMA Dataset\n\nThe dataset released by GAMMA was provided by Sun Yat-sen Ophthalmic Center, Sun Yat-sen University, Guangzhou, China, and the glaucoma and nonglaucoma subjects were randomly selected from glaucoma and myopia cohort, respectively. The dataset contained 300 samples of fundus-OCT pairs. The image acquisitions were performed in a standardized darkroom, and the patients were requested to sit upright. The OCT volumes were all acquired with a Topcon DRI OCT Triton. The OCT was centered on the macula, had a 3 \u00d7 3 mm en-face field of view, and each volume contained 256 two-dimensional cross-sectional images with a size of 992 \u00d7 512 pixels. The fundus images were acquired using a KOWA camera with a resolution of 2000 \u00d7 2992 pixels and a Topcon TRC-NW400 camera with a resolution of 1934 \u00d7 1956 pixels. The fundus images in our dataset were centered on the macula or on the midpoint between optic disc and macula, with both optic disc and macula visible. The image quality was checked manually. In addition, the GAMMA dataset included the respective glaucoma grades, the fovea coordinates, and the mask of the cup and optic disc. The GAMMA dataset is publicly available through https://gamma.grand-challenge.org/, and is allowed to be used and distributed under CC BY-NC-ND (Attribution-NonCommercial-NoDerivs) licence. The following sections describe the implementation of the annotation processes of the three challenge tasks.\n\n\nGlaucoma Grading\n\nThe ground truth of glaucoma grading task for each sample was determined based on mean deviation (MD) values from visual field reports following the criteria below: early-stage with MD value higher than -6 dB, intermediate stage with MD value between -6 and -12 dB, advanced stage with MD value worse than -12 dB. These visual field reports were generated on the same day as the OCT examination and were reliable with fixation losses of under 2/13 and false-positive rate under 15% and false-negative rate under 25% (Li et al. (2020);Xiong et al. (2021)).\n\n\nFovea Localization\n\nThe initial fovea coordinate annotation of each fundus image was performed manually by four clinical ophthalmologists from Sun Yat-sen Ophthalmic Center, Sun Yat-sen University, China, who had an average of 8 years of experience in the field (range 5-10 years). All ophthalmologists independently located the fovea in the image using a cross marker without having access to any patient information or knowledge of disease prevalence in the data. The results from the four ophthalmologists were then fused by a senior ophthalmologist (who has more than ten years of experience in glaucoma), who checked the four markers and decided which of these markers should be retained to be averaged to produce the final reference coordinate.\n\n\nOptic Disc & Cup Segmentation\n\nSimilar to the previous task, the four ophthalmologists manually annotated the initial segmentation region of the optic cup and disc for each fundus image.\n\nThe senior ophthalmologist then examined the initial segmentations and selected the intersection of the annotated results of several ophthalmologists as the final reference mask.\n\n\nChallenge Evaluation\n\n\nGlaucoma Grading\n\nFor each instance, the participants will predict normal, early-glaucoma or progressive-glaucoma. We use Cohen's kappa as an evaluation metric for this ordinal ternary classification problem. Since our categories are ordered, kappa is quadratically weighted to manifest the different extents of the error. The final score of glaucoma grading is represented as:\nS g = 10 \u00d7 \u03ba = 10 \u00d7 p o \u2212 p e 1 \u2212 p e ,(1)\nwhere p o is the accuracy, and p e is the probability of predicting the correct categories by chance.\n\n\nFovea Localization\n\nFovea location is given by its X and Y coordinates. If the image does not contain a fovea, the estimated coordinate is set to be (0, 0). We use the average Euclidean distance between the estimated coordinates and the real coordinates as the evaluation criterion for this task. It is worth noting that the estimated and the ground-truth coordinate values are normalized according to the image size. The final score is based on the reciprocal of the average Euclidean distance (AED) value, and the denominator addition item is set to 0.1 to keep the score within 10 points:\nS f = 1 AED + 0.1\n(2)\n\n\nOptic Disc & Cup Segmentation\n\nThe Dice coefficient was calculated as the segmentation evaluation metric in the GAMMA challenge:\nDice = 2|A \u2229 B| |A| + |B| ,(3)\nwhere |A| and |B| represent the number of pixels of the prediction and ground truth, |A \u2229 B| represents the number of pixels in the overlap between the prediction and ground truth. In addition, we used Mean Absolute Error (MAE) to measure the differences of the vertical cup-to-disc ratio (vCDR) between the predicted results and the ground truth. vCDR has a direct clinical relevance as it is a common measure used in ophthalmology and optometry to assess glaucoma progression. The vCDR is calculated as the ratio of the maximum vertical diameters of the optic cup and optic disc. Each team was ranked based on the three metrics of optic cup Dice coefficient, optic disc Dice coefficient, and MAE. The final weighted score for the optic disc & cup segmentation task was as follows S m =0.25 \u00d7 Dice cup \u00d7 10 + 0.35 \u00d7 Dice cup \u00d7 10\n+ 0.4 \u00d7 1 M AE + 0.1 (4)\nwhere, the weights were chosen consistent with the REFUGE Challenge (Orlando et al. (2020)). Because vCDR was calculated based on OC and OD segmentation results, the weight for vCDR metric had the highest value, and because the OD region could limit the OC region, the metric weight for OD segmentation was set higher than that for OC segmentation.\n\n\nBaseline method\n\nBefore the challenge start, we provided a method to serve as a baseline implementation and performance for the challenge. As deep end-to-end learning  A simple dual-branch network was used to learn glaucoma grading from fundus images and 3D OCT volumes in an end-to-end manner. An illustration of the architecture is shown in Figure 3. Specifically, two CNN-based encoders are used to extract the features from fundus images and OCT volume, respectively. Two encoders are implemented following ResNet34 (He et al. (2016)) with the same architecture except for the first convolutional layer. In the fundus branch, the input channel of the first convolutional layer is set as 3, and in the OCT branch, it is set as 256. The encoded features of the fundus branch and OCT branch are concatenated and classified by a fully connected layer. The model is supervised by cross-entropy loss function in the training stage. We trained it on the training dataset, evaluated it on the preliminary stage data and reported its performance on the final test data. We input the fundus images with resolution 256\u00d7256, and the OCT images with resolution 512\u00d7512. We train the networks using Adam optimizer (Kingma and Ba (2014)) with batch size 4. More details of the baseline can be found in (Fang et al. (2021)). The code of the baseline is released at https://aistudio.baidu.com/aistudio/projectdetail/1948228.\n\nIn the clinic, ophthalmologists use a combination of fundus photographs and OCT volumes for a more accurate and reliable diagnosis. We find that this approach is still applicable in deep learning-based computer-aided glaucoma diagnosis. We compare the performance of the single fundus branch, single OCT branch, and dual-branch baseline in Table 1. Of note, only the basic dual-branch model was released as the baseline to the participants (the fifth row in Table 1). From the table, one can observe that the dual-branch model outperforms the single branch one by a large margin with less variances. This indicates that despite the simple multimodal fusion strategy we adopted, multimodal images can improve the glaucoma grading performance better than either of the modalities alone. This motivated us to hold the GAMMA Challenge to encourage the further exploration of advanced machine learning methods on this multimodal fusion task.\n\nDuring the implementation of the baseline, we identified some techniques that were found particularly useful to obtain good performance on the task (Fang et al. (2021)). The first is to utilize the local information of optic disc.\n\nClinically, glaucoma leads to lesions in the optic disc region, such as cup-disc ratio enlargement and optic disc hemorrhage (Orlando et al. (2020)). Thus, we cropped the optic disc region of fundus images as the network's input to make the network focus on the optic disc and cup. The optic disc region is obtained through pre-trained optic disc segmentation network. According to the results in Table 1, the local information extraction gains 7.2% improvement on mean kappa and 0.2% on standard deviation compared with the baseline.\n\nWe also note that glaucoma grading is actually an ordinal classification task.\n\nThe three classifications: normal, early-glaucoma, and progressive-glaucoma, are the deterioration of glaucoma. Thus, in the training process, the loss should be smaller if the prediction is closer to the ground-truth. For example, predicting the early-glaucoma as normal should be considered as a smaller error than predicting the progressive-glaucoma as normal. Therefore, we adopted ordinal regression strategy (Niu et al. (2016)) to perform two binary classifications, respectively. In this case, a severe error will be double-penalized by both of the classifiers. Specifically, the first classier divides the sample into 0 and 1, that is, to classify whether the input image is a glaucoma sample. The second classier divides the sample into 0 and 1 to identify the input image as progressive-glaucoma or early-glaucoma. The labels of the original triple classification task were converted according to the two binary classification tasks, that is, the labels of the normal samples were changed to (0,0), the labels of the early-glaucoma samples were changed to (1,0), and those of the progressiveglaucoma samples were changed to (1,1). The loss function used in the training processes was the sum of the two binary cross-entropy losses. According to the results in Table 1, ordinal regression independently resulted in an average 4.5% improvement of the models.\n\n\nMethods of participating teams\n\nThe methodology applied by the ten top performing teams in the GAMMA Challenge is summarized in Table 2. In this section, we introduce their methods in the aspects of data preprocessing, architecture and ensembling strategy.\n\n\nData Preprocessing\n\nIn the baseline implementation, we provided a default data augmentation implemented by some commonly used data augmentation techniques, including random crop, random flip and random rotation. Most of the teams used this default augmentation for data preprocessing.\n\nBesides the standard data augmentation, during training, DIAGNOS-ETS augments the input samples by rescaling with the shorter spatial side randomly sampled in a range of 224 to 480, and cropping with size of 224 \u00d7 224. In the test phase, they do test-time augmentation for multi-scale ensemble. Inputs are spatially resized such that the shorter sides are 224, 256, 384, 480 respectively for  (2016)). An illustration of their process is shown in Figure 4.\n\n\nArchitecture\n\nFor the fundus & OCT-based glaucoma grading, almost all the teams adopted dual-branch network structure. Analogously to the baseline method, two branches extract the features of fundus images and OCT volumes. The encoded features are then concatenated for the classification. Unlike this strategy, FATRI-AI used a single network inputted by concatenated fundus image and OCT volume. Besides FATRI-AI, HZL also used a single branch of network. They proposed a multi-task UNet network to jointly learn the glaucoma grading, optic disc & cup segmentation and fovea localization. The glaucoma grading head is attached to the UNet encoder, while the segmentation head and localization head are attached to the UNet decoder. Through the multi-task learning strategy, the correlated features of different tasks will be enhanced and thus improve the performance of all the tasks.\n\nAlthough most of the teams adopted a dual-branch network architecture, their implementations varied greatly. VoxelCloud and DIAGNOS-ETS adopted 3D Network (Tran et al. (2015)) in OCT branch to extract the features from 3D\n\nOCT volume. EyeStar adopted fundus Disc-aware Ensemble Network (DENet) (Fu et al. (2018b)) in fundus branch. Fundus disc-aware ensemble network uses three networks to respectively process the raw fundus image, optic disc region of the fundus image, and polar transformed optic disc region. The predictions of three networks are combined to obtain the final prediction. An illustration of 3D network and DENet is shown in Figure 5. WZMedTech used two independent networks to predict glaucoma grades based on fundus image and OCT volume, respectively. The final result is the average of the two predictions.\n\nRegarding the supervision signal, most teams applied cross-entropy loss.\n\nDIAGNOS-ETS has an extra loss to align the fundus feature and OCT feature.\n\nToward that end, they minimize the Kullback-Leibler (KL) divergence between these two encoded features. Instead of supervising the integrated features of two modalities, EyeStar and WZMedTech supervised the two branches independently. They took the average of the independent predictions as the final result. In the ablation experiments, we did not observe differences between these supervision strategies.\n\n\nEnsemble strategy\n\nEnsembling can substantially improve the quantitative result of glaucoma grading. A basic idea is to pick the best models on different validation folds and take the average of the results. Teams VoxelCloud, HZL, MedIPBIT, MedICAL adopted this strategy.\n\nA unique ensemble strategy adopted in GAMMA Challenge is to exploit the ordinal nature of class labels for ensembling. Separating the triple-classification problem into two binary-classification ones can help to improve the results.\n\nBoth SmartDSP and WZMedTech adopted a similar idea for their ensembling strategy. WZMedTech discriminated early/progressive cases based on the dualmodel agreed glaucoma cases. They double-checked the normal cases by two different models, i.e., first discriminated the normal/glaucoma cases, then classified progressive/early by the second model on predicted glaucoma cases. SmartDSP followed the same high-level idea, but adopted a more sophisticated strategy.\n\nThey first picked three models with the best accuracy on normal, early, and progressive cases, respectively. Then they discriminated the progressive cases by\n\nAlgorithm 1 Ensembling strategy of SmartDSP 1: Train k models, and pick three models with the best accuracy on normal, early, and progressive cases, which are denoted as M n , M e , and M p , respectively.\n\n2: for each sample x in dataset X do 3:\n\nx \u2190 early \u2212 glaucoma 4:\n\nif M p (x) > 0.6 then 5:\n\nx \u2190 progressive \u2212 glaucoma if M e (x) > 0.9 then 11:\n\nx \u2190 early \u2212 glaucoma Specifically, they combines the multi-scale results through:\np = N i=1 p t i N ,(5)\nwhere p i are the multi-scale predictions, p is the final prediction, N is the number of scales, t is a learned scalar parameter. FATRI-AI stacked two models, where the instances with high confidence in the first model (over 0.7) were used as pseudo labels to train the second model.\n\n\nResults\n\n\nChallenge Results\n\nThe top ten teams ranked by glaucoma grading score are SmartDSP, Voxelcloud, EyeStar, HZL, MedIPBIT, IBME, WZMedTech, DIAGNOS-ETS, Med-ICAL, and FATRI AI. The quantitative scores of the glaucoma grading task measured by kappa are shown in The confusion matrices calculated on the test set are shown in Figure 6.\n\nWe note that methods achieved similar performance in the prediction of normal/glaucoma. The error of predicting glaucoma as normal is generally in 4% to 8% range. This rate is lower than the reported misdiagnosed rate of junior ophthalmologists (Trobe et al. (1980)), indicating the clinical application potential of the models.\n\nDifferent approaches widened the gap in the performance of early/progressiveglaucoma classification. Teams ranked higher generally achieved better performance on both the early-glaucoma accuracy and progressive-glaucoma accuracy.\n\nIt is also worth noting that the accuracy of early glaucoma and progressive glaucoma has different significance in clinical scenarios. In clinical scenarios, predicting progressive-glaucoma as early-glaucoma is generally more undesirable than the other way around. Thus, among models with similar overall performance, those with higher progressive-glaucoma accuracy will be a better choice in clinical practice.\n\nTo encourage the teams participate in all three tasks of the GAMMA chal- \nScore =0.4 \u00d7 Score g + 0.3 \u00d7 Score f + 0.3 \u00d7 Score m .(6)\nThe published final ranking is shown in Table 4. The ranking of the auxiliary tasks is shown in the Appendix. The detailed leaderboards can also be accessed on the GAMMA challenge website at https://aistudio.baidu.com/ aistudio/competition/detail/90/0/leaderboard.\n\n\nMethodological Findings\n\nIn this section, we draw the key methodological findings by doing the ablation study on the techniques proposed in the GAMMA challenge. A brief conclusion is that a 3D Net & DENet dual branch architecture with ordinal ensemble strategy performs best on this task. The focus on the OD/OC region also helps to improve the glaucoma grading. The detailed analysis and discussion are as follow.\n\n\nAblation study on architectures in GAMMA\n\nIn order to fairly verify the effectiveness of proposed architectures, we did an ablation study utilizing our baseline implementation as reference. We kept everything the same as the baseline and only changed the architectures. The quantitative results are shown in Table 5. We measure the results by the overall kappa and also the accuracy value of each class. N-Acc, E-Acc and P-Acc denotes the accuracy values of normal, early-glaucoma and progressive-glaucoma, respectively. G-Acc denotes the glaucoma accuracy value of both early-glaucoma and progressive-glaucoma classes.\n\nFrom Table 5, we can see, first, the awareness of optic disc region is helpful for glaucoma grading. Res-DEN and SinMulti utilized the optic disc & cup segmentation mask, and they achieved higher and steadier performance. In particular, Res-DEN achieves a much better overall performance than standard DualRes (increases a 6.42% on mean kappa and decreases a 0.53% on standard deviation), indicating DENet is a better choice than the standard network for the fundus branch. In addition, Res-3D outperforms DualRes by a 3.41% on mean kappa and a 0.46% on kappa standard deviation, indicating 3D neural network works better than the standard network as an OCT branch. We also tried to combine the two advantages, by adopting 3D network on OCT branch and adopting DENet on fundus branch. The results are denoted as 3D-DEN in Table   5. We can see that the combined architecture outperforms both approaches.\n\nSpecifically, it outperforms Res-3D by a 5.74% on mean kappa, outperforms\n\nRes-DEN by a 2.73% on mean kappa, and outperforms basic DualRes by an outstanding 9.15% on mean kappa with the lowest standard deviation among all methods. In conclusion, in terms of the architecture, a 3D neural network OCT branch with a DENet fundus branch is suggested for multi-modal glaucoma grading.\n\n\nAblation study on ensemble strategies in GAMMA\n\nWe also performed the ablation study on the ensemble strategies proposed by top ten teams. The quantitative results are shown in Table 6. A valuable conclusion that can be drawn from the results is that multi-model ordinal ensembling method, which WZMedTech and SmartDSP adopted, are superior on glaucoma grading task. Specifically, 2-ordinal adopted by WZMedTech outperforms standard five fold average ensemble by a 4.52% mean kappa improvement and 0.09% standard deviation descent, 3-ordinal adopted by SmartDSP outperforms standard five fold average ensemble by a 7.53% mean kappa improvement and 0.23% standard deviation descent. This improvement comes from their divide and conquer strategy, i.e., separating this triple classification task to multiple binary classification tasks, where the models that perform the best on each sub-tasks will be picked for the final ensemble. SmartDSP also classified the sample as early-glaucoma by default when all the models do not have high confidence in their prediction, a strategy often applied by clinical experts in their decision making. In clinical practice, when multiple experts give diverse opinions and are not confident, this case will be considered suspected early glaucoma for further screening. Due to the similarity of strategies, the ordinal ensemble strategy may be of use in the real-world clinical scenario.\n\n\nEffects of auxiliary tasks\n\nParticipants are also encouraged to utilize the optic disc & cup mask and fovea location information to improve glaucoma grading. In the GAMMA Challenge, we saw that the prior knowledge of optic disc & cup mask helped to improve the glaucoma grading performance. EyeStar and MedIPBIT both cropped optic disc regions from the fundus images in data preprocessing. EyeStar also adopted DENet to individually process the optic disc region and polar transformed optic disc region. MedICAL utilized the optic disc & cup mask to enhance the fundus inputs. This is also in line with the previous studies (Wu  (2019)) and what we found in Section 2.3. In Table   1, we can also see that optic disc region improves the dual-branch model more than the single fundus-branch. This is because OCT volume corresponds to the optic disc region of the fundus image. Cropping the optic disc region helps to align the features of the two branches. However, we also noted that this improvement decreases in the high-performance models. As the multi-modality results shown in Table 1, on the models with no ordinal regression, disc region cropping improves a 6.79% from 70.2% to 77.0%. However, on the models with ordinal regression, disc region cropping only improves a 4.40% from 76.8% to 81.2%. We conjecture that the stronger models can extract the optic disc region on their own and do not need this prior knowledge anymore.\n\n\nDiscussion\n\n\nMultimodal Fusion strategies in GAMMA\n\nIn the GAMMA Challenge, we note that most multimodal fusion methods that gained high performance in GAMMA are very straightforward. Many ad-  2016)). However, they are also difficult to apply to our case, as the two modalities (OCT and fundus) have a significant scale difference and lack strong spatial correspondence. OCT images typically focus on a small region near the fovea, while fundus images cover a large area of the fundus. To our knowledge, few multimodal fusion techniques can be directly adopted for the fundus-OCT fusion task, and explains why the straightforward dual-branch concatenation model was the main choice in the GAMMA Challenge. This indicates that more specific multimodal fusion algorithms are required in this field. To prevent the submitted method to be overfitting, only the training dataset is released to the registered teams in the challenge. Moreover, each team was allowed to request a maximum five times evaluation on the preliminary data set per day to adjust their algorithms. In the final stage of the challenge, the submitted methods will only be evaluated once by the test dataset as the final results. Our conclusion and analysis can therefore remain unbiased by this issue. Since the online evaluation on the preliminary dataset is limited, most teams split the released training dataset to several parts offline for the private training and evaluation. The future challenges might perhaps consider to split the dataset to four parts, for the purpose of training, validation, online evaluation and final test, respectively. Among them, the training set and the validation set are the released labeled data sets, participants can use these sets to train the models directly, or they can mix them and design their own training and validation sets or cross-validation sets. These released labeled samples will not appear in the online evaluation and final test sets.\n\n\nChallenge strengths and limitations\n\nRegarding the technical methodology, we aim to find out the most effective solution for multi-modality glaucoma grading task in GAMMA. For that purpose, different from many other challenges, we did not allow the participants to use the extra data to train their models. In addition, the source code of the wining teams is required to be submitted with their final results. These ensure the methods proposed can be fairly compared, so that the effective techniques can be identified in the challenge. We also note that many factors are tangled together to effect the final results. This often bother the readers who want to quickly find out the most effective modules on this task, like which is the most effective architecture on this task or which is the most effective ensemble strategy on this task. Thus we also do the ablation study on the wining teams to identify the effectiveness of each proposed module. Since the source code of the wining teams is submitted, we are able to correctly reproduce their methods and do the comparison. Such an ablation study provides the future researchers/developers a cookbook to design their own models.\n\nOne limitation of GAMMA is the size of the dataset. Although GAMMA is the largest fundus-OCT paired data set to date, it is still not big enough for developing capable enough deep learning models. Fundus images based glaucoma classification often provided larger datasets, for instance, our previous REFUGE (Orlando et al. (2020)) and REFUGE2 (Fang et al. (2022)) challenges released 1600 annotated fundus images in total. Moreover, it is worth mentioning that the diverse ethnicities are lack in the GAMMA dataset, as the images correspond to a Chinese population. Although OCT may not vary too much, the fundus images of different ethnicities will be different due to changes in the pigment of the fundus. Therefore, the algorithms in the GAMMA challenge might need to be retrained before applying to a different population. These limitations should be addressed in future challenges by a large-scale multi-ethnicities collection of data, to ensure the generalization of the models. We think the main reason of the absence of explainability is we did not take explainability as a metric to rank the teams in the challenge.\n\nIn addition, we note that all the methods in the GAMMA challenge are based on black-box neural networks, and few of them are interpretable. Explainability is an important factor for the clinical adoption of CAD methods, but it is often less-explored in this field. One team, HZL, did make an effort to incorporate explainability into their model by using a multi-task learning approach that jointly learned glaucoma grading and optic disc and cup segmentation. In this way, the segmentation results could be used as evidence of the neural network's attention to the relevant parts of the image. Moreover, it is also ineffective to apply explainability methods from the deep learning community to our cases. In deep learning, explainability often refers to the ability to highlight the regions of interest in an image, such as a mustache on a face to recognize gender, or in our case, the optic disc and cup region on a fundus image to diagnose glaucoma.\n\nMany explainability techniques have been proposed for this purpose, such as the popular classification activation map (CAM) based methods. However, this type of explainability is not sufficient for medical image classification. In our case, it is not just the region but the ratio of the optic disc to the cup that is discriminative for diagnosing glaucoma. This is a problem that still needs to be studied by both machine learning and clinical research communities. In the future, we are considering adding explainability as one of the metrics in our challenges to encourage the development of these technologies for clinical applications. We have also acknowledged the limitations of the current methods and discussed the need for further research in this area in our paper.\n\n\nClinical implications of the results and future work\n\nThe GAMMA challenge is organized aiming to answer an open question:\n\nShould we develop automated glaucoma diagnosis based on a combination of fundus photography and OCT, like what we do clinically? Up to now, the GAMMA challenge seems to give us a preliminary but positive answer: fudus-OCT combined glaucoma grading obviously outperforms which using only fundus or OCT data. Comparing Table 1 and Table 4,  To move a step further, could these models be applied in the real clinical scenario to automatically screen the glaucoma suspect? It is still an open question.\n\nBut first, an automated, objective diagnosis system is able to to mitigate the human individual bias and to save human experts substantial time. In addition, as the non-invasive, cost-efficient and early-stage glaucoma sensitive glaucoma screening tools, fundus photographs and OCT are widely used by the clinical experts for the primary screening of glaucoma suspect (Chen et al. (2019)). In this case, fundus and OCT combined automated glaucoma detection seems to be an appropriate solution for the large-scale community screening. These models can achieve high sensitivities (above 0.9 for the top three teams) and better overall performance than single-modality models. Although these results are limited to a specific image population, we can still envision this technique to be widely used in clinic in the future.\n\nThe functional parameters like vision field test and IOP will be considered to be contained for the automated glaucoma detection in the future. Although the tools for ONH examination, like fundus images and OCT are cost-efficient and complementary to detect early-stage glaucoma, the clinical gold standard for glaucoma is vision field, which indicates the functional impairment scale.\n\nBesides, IOP is also a valid biomarker indicating the risk of damage to the optic nerve, causing glaucoma and permanent vision loss. In the future work, we will explore the possibility of further combining IOP measurement data and visual field test data to create an automated glaucoma detection model in full accordance with the clinical glaucoma diagnosis criteria. Such models may have the chance to be deployed in both large-scale community screening scenario and in-hospital diagnosis scenario.\n\n\nConclusion\n\nFollowing the clinical glaucoma screening standard, we held a challenge for automated glaucoma grading from both fundus images and OCT volumes, called\n\nGlaucoma grAding from Multi-Modality imAges (GAMMA) Challenge. In this paper, we introduced the released GAMMA dataset, the process of the challenge, the evaluation framework and the top-ranked algorithms in the challenge.\n\nDetailed comparisons and analyses are also conducted on the proposed methodologies. As the first in-depth study of fundus-OCT multi-modality glaucoma grading task, we believe GAMMA will be an essential starting point for further research on this important and clinically-relevant task.\n\nThe data and evaluation framework are publicly accessible through https:// gamma.grand-challenge.org/. The code and technical reports of top-10 teams are released at https://gamma.grand-challenge.org/technical-materials/.\n\nFuture participants are welcome to use our dataset and submit their results on the website to benchmark their methods.\n\nHe, H., Lin, L., Cai, Z., Tang, X., 2022. Joined: Prior guided multi-task learning for joint optic disc/cup segmentation and fovea detection. arXiv preprint arXiv:2203.00461 .\n\nHe    \n\n\nAppendix\n\nIn the following sections, we briefly introduce the methods proposed for the auxiliary tasks.\n\n\nFovea Localization\n\nThe ranking of fovea localization task is shown in Table 7. The results are evaluated by the fovea localization score (see Section 2.2) and Euclidean distance (ED). Teams are ranked by the fovea localization score. The methods of the teams are summarized in Table 9. Analogously to the glaucoma grading task, we also implemented a baseline for fovea localization task, which is shown in Figure 7. The input of the network is the whole fundus image, and the output is a 2D vector indicating the coordinate of the fovea center. The backbone of the network is ResNet50 and is supervised by the combination of Euclidean distance and MSE loss. On fovea localization task, the methods of the teams varies a lot. In the top-10 teams, SmartDSP, MedIPBIT, WZMedTech processed the task as a co-ordinate regression task, just like we did in the baseline method. VoxelCloud and DIAGNOS-ETS processed the task as a binary segmentation task. They generated a circle centered on the fovea location. The circle is then taken as the segmentation target for the binary segmentation task. The center of the segmented result is finally taken as the fovea location. Eyestar, IBME and MedICAL processed the task as a heatmap prediction task. They generated the ground-truth heatmap by Gaussian kernel. This strategy is similar to the binary segmentation, except it is supervised by a soft target, which is a normal distribution centered on fovea location. In contrast, FATRI-AI processed the task as a detection task. They generated a 160\u00d7160 square centered on the fovea location and used a YOLO (Redmon et al. (2016)) network to detect the region.\n\nAlmost half of the teams utilized a coarse-to-fine multi-stage strategy, including SmartDSP, EyeStar, MedIPBIT, WZMedTech and MedICAL. Most of them cropped Region Of Interest (ROI) based on the coarse stage predictions.\n\nThe cropped region is then refined by the later stage. EyeStar proposed a more sophisticated architecture based on this strategy and named it Two-Stage Self-Adaptive localization Architecture (TSSAA). They first cropped multi-scale ROI based on the coarse predictions. Then they fused both multi-scale ROI and coarse-level features using sequential ROI Align layer, concatenation, selfattention modules (Vaswani et al. (2017)) and Fuse layer. An illustration of TSSAA is shown in Figure 7.\n\n\nOD/OC Segmentation\n\nThe ranking of OD/OC segmentation task is shown in Teams are ranked by the OD/OC segmentation score. The methods of the teams are summarized in Table 8. A standard UNet is also adopted as the baseline of the task.\n\nLike the fovea localization task, all the teams except HZL adopted a coarse- to-fine multi-stage strategy. Generally speaking, OD ROI will be first obtained through the coarse OD segmentation stage. The cropped OD patches will be sent to a subsequent Fine-grained OD/OC segmentation network to obtain the final result. Different from the others, VoxelCloud utilized the blood vessel information to improve the OC/OD segmentation. They first used a pre-trained model to obtain the fundus images' blood vessel segmentation masks. The vessel masks are then concatenated with fundus images as the input. An illustration of their method is shown in Figure 8.  (ii) Fine-grained OD/OC segmentation.\n\nFinal results will be smoothed as ellipses Figure 7: An illustration of TSSAA proposed by EyeStar for fovea localization. TSSAA first predicts a coarse heatmap in the coarse stage. Then multi-scale ROI is cropped from the raw image as the input of the subsequent refine stage. In the refine stage, the coarse-level features will also be aligned and fused again for the final prediction. \n\nFigure 1 :\n1An illustration of the GAMMA Challenge. The primary goal of the challenge is to predict the cases as normal, early-glaucoma or progressive-glaucoma from fundus-OCT pairs.\n\n\nproved to be widely effective for the biology and medical image analysis (Zhang et al. (2020); Ge et al. (2022))\n\n\nthresholding the progressive model with 0.6, discriminated the cases as glaucoma by thresholding the normal model with 0.5, discriminated early-glaucoma by thresholding the early model with 0.9. The samples rejected by all three models will be classified as early-glaucoma by default. The pseudo code of this process is shown in Algorithm 1. Besides these ensembling strategies, DIAGNOS-ETS rescaled the input images to different sizes in the inference stage, and combined the multi-scale predictions by averaging them with temperature scaling.\n\n\nvanced multimodal fusion techniques proposed recently were not adopted for this task. The main reason is that the fusion of fundus image and OCT volume is very different from the other more common multimodal fusion tasks. Advanced multimodal fusion algorithms can be divided into two categories: pixel-level and feature-level. Pixel-level fusion operates directly on the raw pixels of the images, making it a simple and widely used technique in medical image classification. However, it can only be applied to data with the same dimensions, such as brain magnetic resonance imaging (MRI) and computed tomography(CT) scans (Sahu et al. (2014); Singh and Anand (2018)), Positron Emission Tomography (PET) and MRT scans (Bhavana and Krishnappa (2015); Shabanzade and Ghassemian (2017); Lai et al. (2017)), or the chest PET and CT scans (Liu et al. (2010)). This makes it inapplicable to our 2D-3D image fusion. Feature-level fusion, on the other hand, operates on features extracted from the images. Unlike the GAMMA methods, which typically perform fusion at the final embedding stage, these techniques often fuse multi-scale features with spatial attention, similarity matching (Meher et al. (2019)) or domain adaptation (Han et al. (2022); Bian et al. (2021); Dou et al. (2018)) in applications such as lung-based Fluoro-D-Glucose PET (FDG PET) and MRI fusion (Das and Kundu (2013)) or Ultrasound and Single-Photon emission CT (SPECT) fusion (Tang et al. (\n\n\nGAMMA was the first open initiative aiming to evaluate the possibility to develop automated methods for glaucoma grading from a combination of fundus images and OCT volume, mimicking the clinical operations of the ophthalmolo-gists to some extent. Toward that end, the challenge provided to the community with the largest public available dataset of fundus photographs and OCT volume pairs to date. The unique characteristic of GAMMA provides a platform to establish more reliable and clinical-alike automated glaucoma classification methods, inspired by the clinical observation that the complementary fundus image and OCT can significantly improve the diagnostic accuracy of ophthalmologists(Anton et al. (2021)). In addition, GAMMA provided the glaucoma diagnostic labels according to the clinical diagnostic standard (normal/earlystage/progressive) with a high quality reference OD/OC masks and fovea positions. These additional information is helpful to calibrate the glaucoma grading methods, as it was observed that training with fundus-derived labels have a negative impact on performance to detect truly diseased cases (Phene et al.(2019)).To our knowledge, GAMMA is also the only available dataset to establish valid deep neural networks for now. The only other dataset similar to GAMMA(Raja et al. (2020)) contains only 50 pairs of fundus-OCT scans, which is commonly not enough to train and evaluate the deep learning methods.In the GAMMA challenge, the evaluation framework we designed matched the principles for evaluating retinal image analysis algorithms proposed byTrucco et al. (2013). Specifically, the GAMMA dataset can be easily accessed through the website associated with the Grand Challenge. Moreover, an open and uniform evaluation interface is provided on the website to automatically evaluate any results submitted. The evaluation process is exactly the same as the GAMMA Challenge. Such an online evaluation provides the further participants a platform to test their algorithms and allows them to fairly compare with the algorithms on the GAMMA leaderboard. In this way, the effectiveness of the proposed techniques can be conveniently and fairly verified, which encourages the development of the further novel algorithms.\n\n\nwe can see a simple fusion of 2D fundus and 3D OCT gains about 10% improvement against the singlemodality. Another 10% improvement can gain from the advanced design of the model. To compare with the human experts, the sensitivities (considering the classification of normal and early/progressive glaucoma) of top-3 teams (0.959, 0.918 and 0.959) have been considerably higher than reported sensitivities of junior ophthalmologists (0.694 to 0.862) (Anton et al. (2021)).\n\n\n, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. Huang, S., Lu, Z., Cheng, R., He, C., 2021. Fapn: Feature-aligned pyramid network for dense image prediction, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 864-873. Jonas, J.B., Bergua, A., Schmitz-Valckenberg, P., Papastathopoulos, K.I., Budde, W.M., 2000. Ranking of optic disc variables for detection of glaucomatous optic nerve damage. Investigative Ophthalmology & Visual Science 41, 1764-1773. Jonas, J.B., Budde, W.M., Panda-Jonas, S., 1999. Ophthalmoscopic evaluation of the optic nerve head. Survey of ophthalmology 43, 293-320. Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 . Lai, S., Wang, J., He, C., Borjer, T.H., 2017. Medical image fusion combined with accelerated non-negative matrix factorization and expanded laplacian energy in shearlet domain. Journal of Engineering Science & Technology Review 10. Li, F., Song, D., Chen, H., Xiong, J., Li, X., Zhong, H., Tang, G., Fan, S., Lam, D.S., Pan, W., et al., 2020. Development and clinical deployment of a smartphone-based visual field deep learning system for glaucoma detection. NPJ digital medicine 3, 1-8. Li, L., Li, H., Kou, G., Yang, D., Hu, W., Peng, J., Li, S., 2022. Dynamic camouflage characteristics of a thermal infrared film inspired by honeycomb structure. Journal of Bionic Engineering 19, 458-470. Tang, L., Li, L., Qian, J., Zhang, J., Pan, J.S., 2016. Nsct-based multimodal medical image fusion with sparse representation and pulse coupled neural network. J. Inf. Hiding Multim. Signal Process. 7, 1306-1316. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M., 2015. Learning spatiotemporal features with 3d convolutional networks, in: Proceedings of the IEEE international conference on computer vision, pp. 4489-4497. Trobe, J.D., Glaser, J.S., Cassady, J.C., 1980. Optic atrophy: differential diagnosis by fundus observation alone. Archives of Ophthalmology 98, 1040-1045. Trucco, E., Ruggeri, A., Karnowski, T., Giancardo, L., Chaum, E., Hubschman, J.P., Al-Diri, B., Cheung, C.Y., Wong, D., Abramoff, M., et al., 2013. Validating retinal fundus image analysis algorithms: issues and a proposal. Investigative ophthalmology & visual science 54, 3546-3559. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you need, in: Advances in neural information processing systems, pp. 5998-6008. Vos, T., Allen, C., Arora, M., Barber, R.M., Bhutta, Z.A., Brown, A., Carter, A., Casey, D.C., Charlson, F.J., Chen, A.Z., et al., 2016. Global, regional, and national incidence, prevalence, and years lived with disability for 310 diseases and injuries, 1990-2015: a systematic analysis for the global burden of disease study 2015. The lancet 388, 1545-1602. Wu, J., Yu, S., Chen, W., Ma, K., Fu, R., Liu, H., Di, X., Zheng, Y., 2020. Leveraging undiagnosed data for glaucoma classification with teacher-student learning, in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer. pp. 731-740. Xie, R., Liu, J., Cao, R., Qiu, C.S., Duan, J., Garibaldi, J., Qiu, G., 2020. Endto-end fovea localisation in colour fundus images with a hierarchical deep regression network. IEEE Transactions on Medical Imaging 40, 116-128. Xiong, J., Li, F., Song, D., Tang, G., He, J., Gao, K., Zhang, H., Cheng, W., Song, Y., Lin, F., et al., 2021. Multimodal machine learning using visual fields and peripapillary circular oct scans in detection of glaucomatous optic neuropathy. Ophthalmology . Zhang, J., Zhao, J., Lin, H., Tan, Y., Cheng, J.X., 2020. High-speed chemical imaging by dense-net learning of femtosecond stimulated raman scattering. The journal of physical chemistry letters 11, 8573-8578. Zhao, R., Liao, W., Zou, B., Chen, Z., Li, S., 2019. Weakly-supervised simultaneous evidence identification and segmentation for automated glaucoma diagnosis, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 809-816.\n\nFigure 2 :Figure 3 :\n23An illustration of the GAMMA auxiliary tasks: optic disc/cup (OD/OC) segmentation and fovea localization on fundus images. Dual-branch network architecture for glaucoma grading. Blue blocks denote the OCT network branch. Red blocks denote the fundus network branch. The features of two branches are concatenated for the final classification.\n\nFigure 4 :FundusFigure 5 :\n45Data preprocessing of MedICAL. They enhanced fundus images by OD/OC mask and transfer the 3D OCT volume as 2D retinal thickness map. Branch implemented by Disc-aware Ensemble Network Illustration of OCT 3D Network branch and fundus DENet branch. 3D Network is adopted by team VoxelCloud and team DIAGNOS-ETS. DENet is adopted by team EyeStar.For the OCT 3D Network branch, the encoded feature is flattened and concatenated with that of the fundus branch. For the fundus Disc-aware ensemble branch, the features of three subbranches are concatenated with OCT features for the classification, respectively. The final prediction is the average of the three subbranches.\n\nFigure 6 :\n6Glaucoma grading confusion matrix of each team. N, E, P denote normal, earlyglaucoma and progressive-glaucoma respectively.\n\nFigure 8 :\n8The coarse stage of OD/OC segmentation model proposed by VoxelCloud. The blood vessel segmentation results predicted from a pre-trained network will be concatenated as the input for the coarse OD segmentation\n\n\nThe 300 samples in the GAMMA dataset correspond to 276 Chinese patients (42% female), which ranged in age from 19-77 and averaged at 40.64\u00b114.53 years old. Glaucoma accounted for 50% of the sample, including 52% in the early stage, 28.67% in the intermediate stage, and 19.33% in the advanced stage. Early glaucoma samples were obtained from 64 patients with average age of 43.47\u00b115.49, of whom 14 patients provided data from both eyes, another 30 patients provided data from the oculus sinister (OS), and 20 patients provided data from the oculus dexter (OD). Similarly, intermediate and advanced glaucoma samples were obtained from 35 and 27 patients with average ages of 47.98\u00b117.38 and 46.24\u00b114.47, respectively. In the intermediate glaucoma samples, 8 patients provided data from both eyes, 15 patients provided OS data, and 12 patients provided OD data. In the advanced glaucoma samples, 2 patients provided data from both eyes, 8 patients provided OS data, and 17 patients provided OD data. The non-glaucomatous samples in the dataset were collected from 150 patients with average age of 35.97\u00b111.29, 57 and 93 patients provided OS and OD data, respectively. We randomly divided the collected samples of each category (non-glaucoma, early-glaucoma, intermediate-glaucoma, and advanced-glaucoma) into three roughly equal parts and assigned them to each of the three challenge sets, corresponding to, we prepared 100 data pairs for each the training, preliminary process and final processes. Because the data sizes in intermediate and advanced glaucoma categories are relatively small compared to that of early glaucoma category, so we grouped the intermediate and advanced glaucoma into one category, i.e., progressive-glaucoma in the main challenge tasks.\n\nTable 1 :\n1Performance of the baselines for glaucoma grading. The results are shown in a format of mean(%) \u00b1 standard deviation(%). We run each method five times to calculate mean and standard deviation.\n\nTable 2 :\n2Summary of the ten top performing glaucoma grading methods in the GAMMA Challenge. Use the output of the OCT branch of the second best model when either of the two models predicts glaucoma. Predict glaucoma grading based on fundus images and OCT volume by two networks. The network predicts the probability of each class.each model, and all cropped to 224\u00d7224.Then they adopted ensemble over multiscale results for the prediction. MedIPBIT cropped the fundus images to the optic disc region. In the training stage, they used the optic disc mask provided in GAMMA dataset for this cropping. In the inference stage, they used instead the masks estimated by the pre-trained segmentation network. The segmentation networks were trained on the auxiliary tasks on GAMMA dataset. BesidesMedIPBIT, MedICAL also utilized OD/OC mask for data preprocessing. They enhanced their fundus image by OD/OC mask. Specifically, OD/OC region of the original image will be multiplied by a factor of 0.05 and added to the original image. MedICAL also transferred the 3D OCT volume to 2D retinal thickness heatmap by Iowa Reference Algorithm(Rosenthal et al.  Team \nArchitecture \nPreprocessing \nEnsemble \nMethod \n\nSmartDSP (Cai et al. (2022)) \nDual-branch ResNet (He et al. (2016)) \n\nFundus: Add Gaussian noise \n\nResize to 512\u00d7512 \n\nOCT: Crop height to 150-662 \n\nResize to 512\u00d7512 \n\nDefault Data Augmentation \n\nPick 3 models with \n\nbest accuracy on normal, \n\nearly and progressive cases, \n\nrespectively. Predict the \n\nresults by different thresholds. \n\nEnsemble the results by \n\nthe priorities of early, \n\nprogressive and normal. \n\nExtract the features of \n\nfundus images and OCT \n\nvolumes by two encoders. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nVoxelCloud \n\nDual-branch Network \n\nimplemented by \n\n3D EfficientNet and EfficientNet \n\n(Tan and Le (2019)) \n\nFundus: Crop Black Margin \n\nResize to 512\u00d7512 \n\nOCT: Resize to 256\u00d7256 \n\nDownsample channels to 128 \n\nDefault Data Augmentation \n\nPick 5 best models on 5 \n\ndifferent validation folds. \n\nEnsemble the results \n\nby taking the average. \n\nExtract the features of \n\nfundus images by EfficientNet. \n\nExtract the features of \n\nOCT volumes by 3D-EfficientNet. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nEyeStar \n\nDual-branch Network \n\nimplemented by \n\nSwin Transformer (Liu et al. (2021)) \n\nand DENet (Fu et al. (2018b)) \n\nFundus: Crop to optic disc region \n\nby pretrained segmentation network \n\nOCT: Randomly pick ten consecutive \n\nslices betwern 113-153 channels \n\nDefault Data Augmentation \n\nDuring the testing process, \n\nsuccessively feed 30 \n\ngroups of 10 \n\nconsecutive OCT slices \n\ninto the network. \n\nTaking the average of the 30 \n\npredictions as the final predictions \n\nExtract the features of \n\nfundus images by fundus disc-aware \n\nensemble network. \n\nExtract the features of \n\nOCT volumes by ResNet. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nHZL \nUNet (Ronneberger et al. (2015)) with \n\nEfficientNet Backbone \n\nFundus: Resize to 1024\u00d71024 \n\nOCT: Resize to 1024\u00d71024 \n\nDefault Data Augmentation \n\nPick 5 best models on 5 \n\ndifferent validation folds. \n\nEnsemble the results \n\nby taking the average. \n\nDesign a multi-task UNet \n\nto jointly learn glaucoma grading, \n\noptic disc & cup segmentation \n\nand fovea localization. \n\nThe embedding of the UNet encoder \n\nis discriminated by a full connected \n\nlayer for glaucoma grading. \n\nMedIPBIT \nDual-branch EfficientNet \n\nFundus: Crop to optic disc region \n\nby pretrained segmentation network. \n\nResize to 128\u00d7128 \n\nOCT: Crop the Black Background \n\nby gradient detector \n\nResize to 128\u00d7128 \n\nDefault Data Augmentation \n\nSplit the dataset for \n\ntraining and validation \n\nby three different strategies. \n\nPick 2 best performing models \n\nin each split \n\nto get a total of 6 models. \n\nEnsemble the results of 6 models \n\nby averaging. \n\nExtract the features of \n\nfundus images and OCT \n\nvolumes by two encoders. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nIBME \nDual-branch ResNet \n\nFundus: Resize to 256\u00d7256 \n\nOCT: Resize to 512\u00d7512 \n\nDefault Data Augmentation \n\nExtract the features of \n\nfundus images and OCT \n\nvolumes by two encoders. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nWZMedTech \nDual-branch ResNet \n\nFundus: Resize to 512\u00d7512 \n\nDefault Data Augmentation + Image Jitter \n\nOCT: Resize to 256\u00d7256 \n\nPick the first and the \n\nsecond best model. \n\nPredict as normal when \n\nboth models predicted \n\nthe case as normal. \n\nTake the average of \n\nthe two networks' results. \n\nDIAGNOS-ETS \n\nDual-branch Network \n\nimplemented by \n\n3D ResNet (Tran et al. (2015)) and ResNet \n\nFundus: Resize with the shorter spatial \n\nside randomly sampled in 224 to 480 \n\nand randomly crop to 224\u00d7224 \n\nOCT: Downsample channels to 16 \n\nRandomly pick one slice in training \n\nPick specific slices in the inference \n\nCrop width to 224-480 \n\nResize the original images with \n\nshorter spatial side randomly \n\nsampled in range 256-480 \n\nDefault Data Augmentation \n\nEnsemble multi-scale \n\nprediction by averaging them \n\nwith temperature scaling \n\nExtract the features of \n\nfundus image and OCT volume \n\nby ResNet and 3D ResNet, \n\nrespectively. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nDuring training, the encoded features of \n\ntwo networks are aligned by \n\nminimizing the KL divergence \n\nMedICAL \nDual-branch EfficientNet \n\nFundus: Resize 1024\u00d71024 \n\nEnhanced by optic disc and cup mask \n\nOCT: Transfer to Retina Tickness Heatmap \n\nResize 400\u00d7400 \n\nDefault Data Augmentation \n\nTake the average \n\nof multiple trained models \n\nExtract the features of \n\nfundus images and OCT \n\nvolumes by two encoders. \n\nConcatenate the encoded \n\nfeatures for the classification. \n\nFATRI-AI \nEfficientNet \n\nFundus: Crop Black Margin \n\nResize 224\u00d7224 \n\nOCT: Random pick 3 slices \n\nResize 224\u00d7224 \n\nDefault Data Augmentation \n\nStack two models, \n\noutput with confidence \n\n> 0.7 in the first model \n\nis used as pseudo labels \n\nto train the second model. \n\nConcatenate fundus image \n\nand OCT volume as the input \n\nto a single network. \n\n\n\nTable 3 .\n3We reported their performances in bust than the other methods. The teams ranked lower are generally caused by the worse generalization capability. In particular, for MedIPBIT, IBME, WZMedTech, DIAGNOS-ETS, and MedICAL, we can see a dramatic decrease of the performance on the final test set.the preliminary stage (evaluation on preliminary set) and the final stage (eval-\n\nuation on final test set). Comparing the ranking in the preliminary stage with \n\nthat of the final stage, we can see SmartDSP, Voxelcloud, EyeStar, HZL, IBME \n\nall keep or raise the rankings on the test dataset, indicating they are more ro-\n\n\n\nTable 3 :\n3Glaucoma grading results in the GAMMA Challenge. Kappa(%) is calculated to measure the performances. Teams are ranked by the overall score. Red and blue denote the rise and fall of the rankings, respectively, while Gray denotes no change in the ranking between the preliminary and the final test stage.Rank \nTeam \nPreliminary Final \n\n1 \nSmartDSP \n93.38 (1) \n85.49 \n\n2 \nVoxelCloud \n90.71 (6) \n85.00 \n\n3 \nEyeStar \n88.28 (7) \n84.77 \n\n4 \nHZL \n89.89 (8) \n84.01 \n\n5 \nIBME \n87.60 (9) \n82.56 \n\n6 \nMedIPBIT \n93.43 (2) \n80.48 \n\n7 \nWZMedTech \n90.44 (5) \n79.46 \n\n8 \nDIAGNOS-ETS \n91.70 (3) \n75.36 \n\n9 \nMedICAL \n90.65 (4) \n72.90 \n\n10 \nFATRI-AI \n87.34 (10) \n69.62 \n\nlenge, the official final ranking is calculated with the scores from all three com-\n\npetition tasks: \n\n\n\nTable 4 :\n4Final ranking of the GAMMA challenge.Rank \nTeam \nMember \nInstitute \nScore \n\n1 \nSmartDSP \nJiongcheng Li, Lexing Huang, Senlin Cai, \n\nYue Huang, Xinghao Ding \n\nXiamen University \n8.88892 \n\n2 \nVoxelcloud \nQinji Yu, Sifan Song, Kang Dang, Wenxiu \n\nShi, Jingqi Niu \n\nShanghai Jiao Tong University; Xi'an \n\nJiaotong-Liverpool University; VoxelCloud \n\nInc. \n\n8.83127 \n\n3 \nEyeStar \nXinxing Xu, Shaohua Li, Xiaofeng Lei, Yanyu \n\nXu, Yong Liu \n\nInstitute of High Performance Computing, ASTAR 8.72345 \n\n4 \nIBME \nWensai Wang, Lingxiao Wang \nChinese Academy of Medical Sciences and \n\nPeking Union Medical College \n\n8.70783 \n\n5 \nMedIPBIT \nShuai Lu, Zeheng Li,Hang Tian,Shengzhu \n\nYang,Jiapeng Wu \n\nBeijing Institute of Technology \n8.70561 \n\n6 \nHZL \nShihua Huang, Zhichao Lu \n\nHong Kong Polytechnic University; \n\nSouthern University of Science and \n\nTechnology \n\n8.68781 \n\n7 \nWZMedTech \nChubin Ou, Xifei Wei, Yong Peng, \n\nZhongrong Ye \n\nSouthern Medical University; Tianjin \n\nMedical University; Xinjiang University \n\n8.65384 \n\n8 \nDIAGNOS-ETS \n\nAdrian Galdran,Bingyuan Liu,Jos\u00e9 \n\nDolz,Waziha Kabir,Riadh Kobbi,Ismail Ben \n\nAyed \n\nETS Montreal; DIAGNOS Inc. \n8.59884 \n\n9 \nMedICAL \nLi Lin, Huaqing He, Zhiyuan Cai \nSouthern University of Science and \n\nTechnology \n\n8.43841 \n\n10 \nFATRI AI \nQiang Zhou, Hu Qiang, Cheng Zheng, \n\nTieshan Liu, Dongsheng Lu, Xinting Xiao \n\nSuixin (Shanghai) Technology Co., LTD. \n8.27601 \n\n\n\nTable 5 :\n5Comparison of the network architectures in the GAMMA Challenge. 'DualRes'denotes the dual-branch ResNet architecture adopted by SmartDSP, MedIPBIT, IBME, \n\nWZMedTech and MedICAL. 'Res-3D' denotes a dual-branch ResNet architecture with a 3D-\n\nResNet OCT branch and a standard fundus branch, which VoxelCloud and DIAGNOS-ETS \n\nadopt. 'Res-DEN' denotes a dual-branch ResNet architecture with DENet fundus branch and \n\nstandard OCT branch, which EyeStar adopts. 'SinCat' and 'SinMulti' denote the single net-\n\nwork inputted by fundus & OCT concatenation and multi-task learning strategy adopted by \n\nFATRI-AI and HZL, respectively. The results are shown in a format of mean(%) \u00b1 standard \n\ndeviation(%). We run each method five times to calculate mean and standard deviation. \n\nN-Acc \nE-Acc \nP-Acc \nG-Acc \nKappa \n\nDualRes 84.31\u00b11.77 27.20\u00b12.26 71.66\u00b12.33 42.04\u00b11.79 70.26\u00b10.94 \n\nRes-3D \n82.74\u00b11.27 24.26\u00b12.02 73.32\u00b11.21 52.18\u00b11.07 73.81\u00b10.48 \n\nRes-DEN 94.63\u00b11.02 24.21\u00b11.21 76.75\u00b11.62 48.13\u00b10.95 76.82\u00b10.41 \n\nSinCat \n74.31\u00b11.23 33.60\u00b13.60 52.50\u00b12.85 42.86\u00b12.22 61.31\u00b11.82 \n\nSinMulti \n89.02\u00b12.24 20.80\u00b13.18 67.21\u00b12.53 51.46\u00b11.91 75.31\u00b11.09 \n\n3D-DEN 97.88\u00b10.91 33.12\u00b11.76 54.56\u00b11.86 43.14\u00b10.92 79.55\u00b10.21 \n\n\n\nTable 6 :\n6Performance of proposed ensemble strategies in the GAMMA Challenge.'Stacked'denotes pseudo-label retraining strategy adopted by FATRI-AI, 'Rescale' denotes multi-scale \n\nmodels ensemble strategy adopted by DIAGNOS-ETS, '3-fold', '5-fold' denote averaging model \n\npredictions on 3-fold, 5-fold validation set respectively, '2-ordinal' denotes dual-model ordinal \n\nensemble strategy adopted by WZMedTech, '3-ordinal' denotes triple-model ordinal ensemble \n\nstrategy adopted by SmartDSP. The results are shown in a format of mean(%) \u00b1 standard \n\ndeviation(%). We run each method five times to calculate mean and standard deviation. \n\nEnsemble \nN-Acc \nE-Acc \nP-Acc \nG-Acc \nKappa \n\nStacked \n90.12\u00b10.51 11.82\u00b11.21 84.02\u00b11.54 47.45\u00b10.85 71.31\u00b10.58 \n\nRescale \n91.82\u00b10.64 24.21\u00b11.71 54.81\u00b11.02 40.15\u00b10.94 73.59\u00b10.42 \n\n3-fold-ave 92.57\u00b10.31 37.54\u00b11.25 57.92\u00b11.87 47.02\u00b10.61 74.55\u00b10.26 \n\n5-fold-ave 90.25\u00b10.22 27.72\u00b10.66 71.04\u00b10.74 49.56\u00b10.31 75.33\u00b10.14 \n\n2-ordinal 96.14\u00b10.14 44.02\u00b10.27 54.26\u00b10.19 49.06\u00b10.08 79.85\u00b10.05 \n\n3-ordinal 98.06\u00b10.05 52.06\u00b10.10 66.71\u00b10.12 59.23\u00b10.05 82.80\u00b10.03 \n\net al. (2020); Zhao et al. \n\nTable 7 :\n7Fovea localization ranking in the GAMMA Challenge.Rank \nTeam \nScore \nED \n\n1 \nDIAGNOS-ETS 9.60294 0.00413 \n\n2 \nIBME \n9.58847 0.00429 \n\n3 \nSmartDSP \n9.57458 0.00444 \n\n4 \nMedIPBIT \n9.53757 0.00485 \n\n5 \nVoxelcloud \n9.53443 0.00488 \n\n6 \nEyeStar \n9.51465 0.0051 \n\n7 \nWZMedTech \n9.45846 0.00573 \n\n8 \nMedICAL \n9.34639 0.00699 \n\n9 \nFATRI AI \n9.33749 0.0071 \n\n10 \nHZL \n9.22303 0.00842 \n\n\n\nTable 8 .\n8The results are evaluated by the two Dice values, vertical optic Cup-to-Disc Ratio (vCDR) and the OD/OC segmentation score (see Section 2.2) in the GAMMA Challenge.\n\nTable 8 :\n8OD/OC segmentation ranking in the GAMMA Challenge.Rank \nTeam \nScore \nDice-disc(%) Dice-cup(%) vCDR \n\n1 \nVoxelcloud \n8.36384 \n96.25 \n87.84 \n0.04292 \n\n2 \nDIAGNOS-ETS 8.3275 \n95.96 \n87.74 \n0.04411 \n\n3 \nWZMedTech \n8.31621 \n96.11 \n88.04 \n0.04538 \n\n4 \nHZL \n8.30093 \n95.83 \n88.00 \n0.04562 \n\n5 \nSmartDSP \n8.28488 \n95.79 \n88.01 \n0.04642 \n\n6 \nMedICAL \n8.27264 \n95.75 \n87.57 \n0.0464 \n\n7 \nIBME \n8.2309 \n95.79 \n87.66 \n0.04887 \n\n8 \nFATRI AI \n8.18773 \n95.40 \n86.69 \n0.04917 \n\n9 \nMedIPBIT \n8.15502 \n95.49 \n87.67 \n0.05258 \n\n10 \nEyeStar \n8.07253 \n94.77 \n85.83 \n0.05326 \n\n\n\nTable 9 :\n9Summary of the fovea localization methods in the GAMMA Challenge Fundus: Resize to 1024\u00d71024 OCT: Resize to 1024\u00d71024 Default Data Augmentation Pick 5 best models on 5 different validation folds. Ensemble the results by taking the average. A multi-task UNet to jointly learn glaucoma grading, OD/OC segmentation and fovea localization. ResNet50 for the third stage (i) Center crop to 1920\u00d71920 (ii) Resize to 224\u00d7224 (iii) Default Data Augmentation Three stages coordinate regression, predicted ROI of last stage is cropped as the input of the next stage DIAGNOS-ETS Double stacked W-Net (i) Resize to 512\u00d7512 (ii) Default data augmentation + If else, take the average of two results Two stages: (i) Coarse OD/Macular segmentation, crop ROI toTeam \nArchitecture \nPreprocessing \nEnsemble \nMethod \n\nSmartDSP He et al. (2022) \nEfficientnet-b4 \n\n(i) Center crop to 2000\u00d72000, \n\npadding when height \n\nor width less than 2000 \n\n(ii) Resize to 224\u00d7224 \n\n(iii) Default Data Augmentation \n\n2-fold esmble by averaging \n\nTwo stages coordinate regression: \n\n(i) Coarse localization, \n\ncrop to 512\u00d7512, \n\n(ii) Fine-grained localization. \n\nVoxelCloud \nTransUNet-like architecture \n\n(i) Remove black background \n\n(ii) Pad and resize to 512\u00d7512 \n\n(iii) Default Data Augmentation + \n\nBlur + JPEG compression + \n\nGaussNoise + Coarse Dropout \n\nEnsemble the predictions \n\nof 30 models train with \n\ndifferent hyper-parameters \n\nBinary segmentation the \n\nfovea centered circle. \n\nUsing the sum of \n\nbinary cross-entropy loss, \n\nSoftDice loss, SSIM loss, \n\nIOU loss and L1 loss \n\nto supervise \n\nEyeStar \n\nPoposed Two-Stage \n\nSelf-Adaptive \n\nlocalization Architecture \n\n(TSSAA) \n\n(i) Resize to 998\u00d7998 \n\n(ii) Crop to 896\u00d7896 \n\n(iii) Resize to 224\u00d7224 \n\n(iiii) Default data augmentation \n\nTwo stages heatmap prediction: \n\n(i) Coarse heatmap prediction, \n\ncrop to multi-scale ROI, \n\n(ii) Fine-grained localization fusing \n\nmulti-scale ROI and \n\ncoarse-level features \n\nHZL \nUNet with \n\nEfficientNet Backbone \n\nRecurrently run the \n\nmodel for coarse-to-fine \n\nlocalization \n\nMedIPBIT \n\nResNet50 for coarse \n\nlocalization \n\nResNet101 for Fine-grained \n\nlocalization \n\nResize to 512\u00d7512 \n\nThree stages coordinate regression: \n\n(i) Coarse localization, crop to ROI \n\n(ii) Sequential two-stage \n\nFine-grained localization. \n\nIBME \nUNet with EfficientNetB5 \n\nbackbone \n\n(i) Padding to 2000\u00d72992 \n\n(ii) Default Data Augmentation \n\nEnd-to-end heatmap prediction \n\nwith maximization likelihood \n\nfor the localization \n\nWZMedTech \n\nHDRNet (Xie et al. (2020)) \n\nfor the first \n\nand second stage \n\nColor Normalization \n\n4-fold temperature ensemble \nEnd-to-end binary segmentation \n\nthe fovea centered circle \n\nMedICAL \n\nResNet50 for coordinate \n\nregression branch \n\nEfficientNet-B0 for heatmap \n\npredication branch \n\n(i) Pick G channel of RGB image \n\n(ii) Histogram equalization \n\n(iii) Default data augmentation \n\nEnsemble the results of \n\nheatmap branch and \n\ncoordinate regression branch. \n\nIf Euclidean distance of \n\nthem larger than 30, \n\ntake the regression result. \n\n128\u00d7128 and 256\u00d7256 \n\n(ii) Feed 128\u00d7128 patches and \n\n256\u00d7256 patches \n\nto a heatmap predication \n\nnetwork and coordinate regression \n\nnetwork respectively, \n\nfuse the results of two branches \n\nfor the final predication \n\nFATRI-AI \nYOLOv5s (Redmon et al. (2016)) \n\n(i) Crop black background \n\n(ii) Default data augmentation \n\n+ Mosaic (Chen et al. (2020)) \n\n+ Cutout \n\nEnd-to-end macular \n\nregion detection, \n\nmacular region is generated by \n\na 160\u00d7160 square \n\ncentered on fovea location \n\n\nTable 10 :\n10Summary of the OD/OC segmentation methods in the GAMMA Challenge Fundus: Resize to 1024\u00d71024 OCT: Resize to 1024\u00d71024 Default Data Augmentation Pick 5 best models on 5 different validation folds. Ensemble the results by taking the average. A multi-task UNet to jointly learn glaucoma grading, OD/OC segmentation and fovea localization. FAM (Huang et al. (2021)) is Fine-grained segmentation (i) Center crop to 1920\u00d71920 (ii) Default data augmentation In the Fine-grained stage, ensemble the models supervised by cross-entropy loss + boundary loss + dice loss and that supervided by focal loss + dice loss by taking the average Two stages: (i) Coarse OD segmentation, crop ROI to 512\u00d7512 (ii) Fine-grained OD/OC segmentation DIAGNOS-ETS Double stacked W-Net (i) Resize to 512\u00d7512 (ii) Default data augmentation + Color normalization In coarse OD segmentation: 4-fold ensemble by taking average In Fine-grained OD/OC segmentation:Team \nArchitecture \nPreprocessing \nEnsemble \nMethod \n\nSmartDSP (He et al. (2022)) \n\nDeepLabv3 with ResNet34 \n\nencoder for coarse \n\nsegmentation DeepLabv3 with \n\nEfficientNet-b2 encoder for \n\nFine-grained segmentation \n\n(i) Crop to 512\u00d7512 centered \n\non the highest brightness point \n\n(ii) Default Data Augmentation \n\n2-fold ensemble by averaging \n\nTwo stages: (i) Coarse OD \n\nsegmentation, cropping \n\n(ii) Fine-grained OD/OC segmentation \n\nVoxelCloud \n\nTransUNet-like architecture \n\nfor coarse segmentation \n\nCENet, TransUNet and Segtran \n\nfor Fine-grained segmentation \n\n(i) Resize to 512\u00d7512 \n\n(ii) Default data augmentation \n\n5-fold ensemble by \n\naveraging for coarse segmentation \n\nEnsemble the predictions of \n\nfive folds, three networks and \n\ntwo kinds of input by \n\naveraging for Fine-grained \n\nsegmentation \n\nTwo stages: (i) Coarse OD \n\nsegmentation taking blood vessel \n\nmask concatenated fundus \n\nimage as input, cropping \n\n(ii) Fine-grained OD/OC segmentation \n\ntaking cropped patches \n\nand polar transformed \n\npatches as inputs. \n\nModel supervised by \n\nBCE loss + Dice loss \n\nEyeStar \nSegtran (Li et al. (2021)) with \n\nEfficientNet-B4 backbone \n\n(i) Crop to 576\u00d7576 disc \n\nregion by \n\nMNet DeepCDR (Fu et al. (2018a)) \n\n(ii) Resize to 288\u00d7288 \n\n(iii) Default data augmentation \n\nTwo stages: (i) Coarse OD \n\nsegmentation using \n\nCNN, cropping \n\n(ii) Fine-grained OD/OC segmentation \n\nusing Segtran \n\nHZL \nUNet with \n\nEfficientNet Backbone \n\nadopted for \n\nthe better segmentation \n\nMedIPBIT \n\nCNN-Transformer Mixed UNet \n\nCNN backbone implemented \n\nby ResNet34 \n\nResize to 512\u00d7512 \n\nTwo stages: (i) Coarse OD \n\nsegmentation, cropping \n\n(ii) Fine-grained OC segmentation \n\nIBME \n\nUNet with \n\nEfficientNetB3 backbone \n\nfor OC center localization \n\nUNet with \n\nEfficientNetB6 backbone \n\nfor Fine-grained segmentation \n\nDefault data augmentation \n\nTwo stages: (i) OC center \n\nlocalization, crop ROI \n\nto 512\u00d7512 \n\n(ii) Fine-grained OD/OC segmentation \n\nWZMedTech \n\nDeepLabV3 for \n\ncoarse segmentation \n\nTransUNet for \n\n4-fold temperature ensemble \n\nTwo stages: (i) Coarse OD \n\nsegmentation, crop ROI \n\nto 512\u00d7512 \n\n(ii) Fine-grained OD/OC segmentation \n\nMedICAL \nUNet with EfficientNet-B4 \n\nbackbone \n\n(i) Resize to 512\u00d7512 \n\n(ii) Default data augmentation \n\nThree stages: (i) Coarse OD/Macular \n\nsegmentation, crop OD ROI \n\nto 448\u00d7448 \n\n(ii) Fine-grained OD/OC segmentation, \n\ncrop OC ROI TO 256\u00d7256 \n\n(iii) Fine-grained OC segmentation \n\nFATRI-AI \nYOLOv5s for coarse segmentation \n\nHRNet for Fine-grained segmentation \n\n(i) Resize to 608\u00d7608 \n\n(ii) Default data augmentation \n\n+ Mosaic (Chen et al. (2020)) \n\n+ Cutout \n\nTwo stages: (i) Coarse OD \n\nsegmentation, crop ROI \n\nto 512\u00d7512 \n\n\nAcknowledgmentsThis research was supported by the High-level Hospital Construction Project, Zhongshan Ophthalmic Center, Sun Yat-sen University (303020104). H. Fu was supported by AME Programmatic Fund (A20H4b0141).\nDiagnostic accuracy and detection rate of glaucoma screening with optic disk photos, optical coherence tomography images, and telemedicine. A Anton, K Nolivos, M Pazos, G Fatti, M E Ayala, E Mart\u00ednez-Prats, O Peral, V Poposki, E Tsiroukis, A Morilla-Grasa, Journal of Clinical Medicine. 11216Anton, A., Nolivos, K., Pazos, M., Fatti, G., Ayala, M.E., Mart\u00ednez-Prats, E., Peral, O., Poposki, V., Tsiroukis, E., Morilla-Grasa, A., et al., 2021. Diagnostic accuracy and detection rate of glaucoma screening with optic disk photos, optical coherence tomography images, and telemedicine. Journal of Clinical Medicine 11, 216.\n\nMulti-modality medical image fusion using discrete wavelet transform. V Bhavana, H Krishnappa, Procedia Computer Science. 70Bhavana, V., Krishnappa, H., 2015. Multi-modality medical image fusion using discrete wavelet transform. Procedia Computer Science 70, 625-631.\n\nDomain adaptation meets zero-shot learning: An annotation-efficient approach to multi-modality medical image segmentation. C Bian, C Yuan, K Ma, S Yu, D Wei, Y Zheng, 10.1109/TMI.2021.3131245IEEE Transactions on Medical Imaging. Bian, C., Yuan, C., Ma, K., Yu, S., Wei, D., Zheng, Y., 2021. Domain adaptation meets zero-shot learning: An annotation-efficient approach to multi-modality medical image segmentation. IEEE Transactions on Medical Imaging , 1- 1doi:10.1109/TMI.2021.3131245.\n\nCorolla: An efficient multi-modality fusion framework with supervised contrastive learning for glaucoma grading. Z Cai, L Lin, H He, X Tang, 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). IEEECai, Z., Lin, L., He, H., Tang, X., 2022. Corolla: An efficient multi-modality fusion framework with supervised contrastive learning for glaucoma grading, in: 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1-4.\n\nDynamic scale training for object detection. Y Chen, P Zhang, Z Li, Y Li, X Zhang, L Qi, J Sun, J Jia, arXiv:2004.12432arXiv preprintChen, Y., Zhang, P., Li, Z., Li, Y., Zhang, X., Qi, L., Sun, J., Jia, J., 2020. Dynamic scale training for object detection. arXiv preprint arXiv:2004.12432 .\n\nCombination of enhanced depth imaging optical coherence tomography and fundus images for glaucoma screening. Z Chen, X Zheng, H Shen, Z Zeng, Q Liu, Z Li, Journal of Medical Systems. 43Chen, Z., Zheng, X., Shen, H., Zeng, Z., Liu, Q., Li, Z., 2019. Combination of enhanced depth imaging optical coherence tomography and fundus images for glaucoma screening. Journal of Medical Systems 43, 1-12.\n\nA neuro-fuzzy approach for medical image fusion. S Das, M K Kundu, IEEE transactions on biomedical engineering. 60Das, S., Kundu, M.K., 2013. A neuro-fuzzy approach for medical image fusion. IEEE transactions on biomedical engineering 60, 3347-3353.\n\nUnsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adversarial loss. Q Dou, C Ouyang, C Chen, H Chen, P A Heng, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceAAAI PressDou, Q., Ouyang, C., Chen, C., Chen, H., Heng, P.A., 2018. Unsupervised cross-modality domain adaptation of convnets for biomedical image segmen- tations with adversarial loss, in: Proceedings of the 27th International Joint Conference on Artificial Intelligence, AAAI Press. p. 691-697.\n\nRefuge2 challenge: Treasure for multi-domain learning in glaucoma assessment. H Fang, F Li, H Fu, X Sun, X Cao, J Son, S Yu, M Zhang, C Yuan, C Bian, arXiv:2202.08994arXiv preprintFang, H., Li, F., Fu, H., Sun, X., Cao, X., Son, J., Yu, S., Zhang, M., Yuan, C., Bian, C., et al., 2022. Refuge2 challenge: Treasure for multi-domain learning in glaucoma assessment. arXiv preprint arXiv:2202.08994 .\n\nMulti-modality images analysis: A baseline for glaucoma grading via deep learning, in: International Workshop on Ophthalmic Medical Image Analysis. H Fang, F Shang, H Fu, F Li, X Zhang, Y Xu, SpringerFang, H., Shang, F., Fu, H., Li, F., Zhang, X., Xu, Y., 2021. Multi-modality images analysis: A baseline for glaucoma grading via deep learning, in: In- ternational Workshop on Ophthalmic Medical Image Analysis, Springer. pp. 139-147.\n\nJoint optic disc and cup segmentation based on multi-label deep network and polar transformation. H Fu, J Cheng, Y Xu, D W K Wong, J Liu, X Cao, IEEE transactions on medical imaging. 37Fu, H., Cheng, J., Xu, Y., Wong, D.W.K., Liu, J., Cao, X., 2018a. Joint optic disc and cup segmentation based on multi-label deep network and polar transformation. IEEE transactions on medical imaging 37, 1597-1605.\n\nDisc-Aware Ensemble Network for Glaucoma Screening From Fundus Image. H Fu, J Cheng, IEEE Transactions on Medical Imaging. 37Fu, H., Cheng, J., et al., 2018b. Disc-Aware Ensemble Network for Glaucoma Screening From Fundus Image. IEEE Transactions on Medical Imaging 37, 2493-2501.\n\nSegmentation and quantification for angleclosure glaucoma assessment in anterior segment oct. H Fu, Y Xu, S Lin, X Zhang, D W K Wong, J Liu, A F Frangi, M Baskaran, T Aung, IEEE transactions on medical imaging. 36Fu, H., Xu, Y., Lin, S., Zhang, X., Wong, D.W.K., Liu, J., Frangi, A.F., Baskaran, M., Aung, T., 2017. Segmentation and quantification for angle- closure glaucoma assessment in anterior segment oct. IEEE transactions on medical imaging 36, 1930-1938.\n\nSrs-fish: A high-throughput platform linking microbiome metabolism to identity at the single-cell level. X Ge, F C Pereira, M Mitteregger, D Berry, M Zhang, B Hausmann, J Zhang, A Schintlmeister, M Wagner, J X Cheng, Proceedings of the National Academy of Sciences. 119Ge, X., Pereira, F.C., Mitteregger, M., Berry, D., Zhang, M., Hausmann, B., Zhang, J., Schintlmeister, A., Wagner, M., Cheng, J.X., 2022. Srs-fish: A high-throughput platform linking microbiome metabolism to identity at the single-cell level. Proceedings of the National Academy of Sciences 119, e2203519119.\n\nDeep symmetric adaptation network for cross-modality medical image segmentation. X Han, L Qi, Q Yu, Z Zhou, Y Zheng, Y Shi, Y Gao, 10.1109/TMI.2021.3105046IEEE Transactions on Medical Imaging. 41Han, X., Qi, L., Yu, Q., Zhou, Z., Zheng, Y., Shi, Y., Gao, Y., 2022. Deep symmetric adaptation network for cross-modality medical image segmenta- tion. IEEE Transactions on Medical Imaging 41, 121-132. doi:10.1109/TMI. 2021.3105046.\n\nOptic disc size, an important consideration in the glaucoma evaluation. O D Hancox, M D , Clinical Eye and Vision Care. 11Hancox OD, M.D., 1999. Optic disc size, an important consideration in the glaucoma evaluation. Clinical Eye and Vision Care 11, 59-62.\n\nMedical image segmentation using squeeze-and-expansion transformers. S Li, X Sui, X Luo, X Xu, Y Liu, R S M Goh, arXiv:2105.09511arXiv preprintLi, S., Sui, X., Luo, X., Xu, X., Liu, Y., Goh, R.S.M., 2021. Medical im- age segmentation using squeeze-and-expansion transformers. arXiv preprint arXiv:2105.09511 .\n\nPet/ct medical image fusion algorithm based on multiwavelet transform. Y Liu, J Yang, J Sun, 2010 2nd International Conference on Advanced Computer Control, IEEE. Liu, Y., Yang, J., Sun, J., 2010. Pet/ct medical image fusion algorithm based on multiwavelet transform, in: 2010 2nd International Conference on Advanced Computer Control, IEEE. pp. 264-268.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B., 2021. Swin transformer: Hierarchical vision transformer using shifted windows, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012-10022.\n\nA survey on region based image fusion methods. B Meher, S Agrawal, R Panda, A Abraham, Information Fusion. 48Meher, B., Agrawal, S., Panda, R., Abraham, A., 2019. A survey on region based image fusion methods. Information Fusion 48, 119-132.\n\nDigital imaging of the optic nerve head: monoscopic and stereoscopic analysis. J E Morgan, N J L Sheen, R V North, Y Choong, E Ansari, British journal of ophthalmology. 89Morgan, J.E., Sheen, N.J.L., North, R.V., Choong, Y., Ansari, E., 2005. Digital imaging of the optic nerve head: monoscopic and stereoscopic analysis. British journal of ophthalmology 89, 879-884.\n\nAutomated diagnosis of glaucoma using digital fundus images. J Nayak, R Acharya, P S Bhat, N Shetty, T C Lim, Journal of medical systems. 33Nayak, J., Acharya, R., Bhat, P.S., Shetty, N., Lim, T.C., 2009. Automated diagnosis of glaucoma using digital fundus images. Journal of medical systems 33, 337-346.\n\nOrdinal regression with multiple output cnn for age estimation. Z Niu, M Zhou, L Wang, X Gao, G Hua, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionNiu, Z., Zhou, M., Wang, L., Gao, X., Hua, G., 2016. Ordinal regression with multiple output cnn for age estimation, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4920-4928.\n\nRefuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. J I Orlando, H Fu, J B Breda, K Van Keer, D R Bathula, A Diaz-Pinto, R Fang, P A Heng, J Kim, J Lee, Medical image analysis. 59101570Orlando, J.I., Fu, H., Breda, J.B., van Keer, K., Bathula, D.R., Diaz-Pinto, A., Fang, R., Heng, P.A., Kim, J., Lee, J., et al., 2020. Refuge challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. Medical image analysis 59, 101570.\n\nDeep learning to assess glaucoma risk and associated features in fundus images. S Phene, R Carter Dunn, N Hammel, Y Liu, J Krause, N Kitade, arXiv:1812.08911arXiv preprintPhene, S., Carter Dunn, R., Hammel, N., Liu, Y., Krause, J., Kitade, N., et al., 2019. Deep learning to assess glaucoma risk and associated features in fundus images. arXiv preprint arXiv:1812.08911 .\n\nData on oct and fundus images for the detection of glaucoma. H Raja, M U Akram, S G Khawaja, M Arslan, A Ramzan, N Nazir, 105342Raja, H., Akram, M.U., Khawaja, S.G., Arslan, M., Ramzan, A., Nazir, N., 2020. Data on oct and fundus images for the detection of glaucoma. Data in brief 29, 105342.\n\nYou only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionRedmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: Unified, real-time object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779-788.\n\nGlobal data on visual impairment in the year 2002. S Resnikoff, D Pascolini, D Etya&apos;ale, I Kocur, R Pararajasegaram, G P Pokharel, S P Mariotti, Bulletin of the world health organization. 82Resnikoff, S., Pascolini, D., Etya'Ale, D., Kocur, I., Pararajasegaram, R., Pokharel, G.P., Mariotti, S.P., 2004. Global data on visual impairment in the year 2002. Bulletin of the world health organization 82, 844-851.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation, in: International Conference on Medical image computing and computer-assisted intervention, Springer. pp. 234-241.\n\nOphthalvis-making data analytics of optical coherence tomography reproducible. P Rosenthal, M Ritter, D Kowerko, C Heine, EuroRV 3 @ EuroVisRosenthal, P., Ritter, M., Kowerko, D., Heine, C., 2016. Ophthalvis-making data analytics of optical coherence tomography reproducible., in: EuroRV 3 @ EuroVis, pp. 9-13.\n\nMedical image fusion with laplacian pyramids. A Sahu, V Bhateja, A Krishn, 2014 International conference on medical imaging, mhealth and emerging communication systems. IEEESahu, A., Bhateja, V., Krishn, A., et al., 2014. Medical image fusion with laplacian pyramids, in: 2014 International conference on medical imaging, m- health and emerging communication systems (MedCom), IEEE. pp. 448-453.\n\nCombination of wavelet and contourlet transforms for pet and mri image fusion. F Shabanzade, H Ghassemian, 2017 artificial intelligence and signal processing conference (AISP), IEEE. Shabanzade, F., Ghassemian, H., 2017. Combination of wavelet and contourlet transforms for pet and mri image fusion, in: 2017 artificial intelligence and signal processing conference (AISP), IEEE. pp. 178-183.\n\nRipplet domain fusion approach for ct and mr medical image information. S Singh, R S Anand, Biomedical Signal Processing and Control. 46Singh, S., Anand, R.S., 2018. Ripplet domain fusion approach for ct and mr medical image information. Biomedical Signal Processing and Control 46, 281-292.\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. M Tan, Q Le, PMLRInternational conference on machine learning. Tan, M., Le, Q., 2019. Efficientnet: Rethinking model scaling for convolutional neural networks, in: International conference on machine learning, PMLR. pp. 6105-6114.\n", "annotations": {"author": "[{\"end\":136,\"start\":75},{\"end\":201,\"start\":137},{\"end\":386,\"start\":202},{\"end\":508,\"start\":387},{\"end\":698,\"start\":509},{\"end\":766,\"start\":699},{\"end\":830,\"start\":767},{\"end\":885,\"start\":831},{\"end\":946,\"start\":886},{\"end\":1016,\"start\":947},{\"end\":1084,\"start\":1017},{\"end\":1216,\"start\":1085},{\"end\":1231,\"start\":1217},{\"end\":1435,\"start\":1232},{\"end\":1604,\"start\":1436},{\"end\":1691,\"start\":1605},{\"end\":1811,\"start\":1692},{\"end\":1929,\"start\":1812},{\"end\":2047,\"start\":1930},{\"end\":2061,\"start\":2048},{\"end\":2100,\"start\":2062},{\"end\":2228,\"start\":2101},{\"end\":2445,\"start\":2229},{\"end\":2510,\"start\":2446},{\"end\":2573,\"start\":2511},{\"end\":2717,\"start\":2574},{\"end\":2803,\"start\":2718},{\"end\":3020,\"start\":2804},{\"end\":3101,\"start\":3021},{\"end\":3111,\"start\":3102}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":81},{\"end\":148,\"start\":144},{\"end\":208,\"start\":206},{\"end\":396,\"start\":394},{\"end\":520,\"start\":517},{\"end\":712,\"start\":710},{\"end\":776,\"start\":771},{\"end\":839,\"start\":837},{\"end\":896,\"start\":892},{\"end\":957,\"start\":955},{\"end\":1025,\"start\":1023},{\"end\":1096,\"start\":1092},{\"end\":1230,\"start\":1226},{\"end\":1240,\"start\":1238},{\"end\":1444,\"start\":1442},{\"end\":1617,\"start\":1612},{\"end\":1702,\"start\":1700},{\"end\":1821,\"start\":1819},{\"end\":1939,\"start\":1936},{\"end\":2060,\"start\":2057},{\"end\":2073,\"start\":2068},{\"end\":2114,\"start\":2110},{\"end\":2235,\"start\":2232},{\"end\":2456,\"start\":2452},{\"end\":2519,\"start\":2517},{\"end\":2590,\"start\":2581},{\"end\":2738,\"start\":2731},{\"end\":2816,\"start\":2811},{\"end\":3029,\"start\":3027},{\"end\":3110,\"start\":3108}]", "author_first_name": "[{\"end\":80,\"start\":75},{\"end\":143,\"start\":137},{\"end\":205,\"start\":202},{\"end\":393,\"start\":387},{\"end\":516,\"start\":509},{\"end\":709,\"start\":699},{\"end\":770,\"start\":767},{\"end\":836,\"start\":831},{\"end\":891,\"start\":886},{\"end\":954,\"start\":947},{\"end\":1022,\"start\":1017},{\"end\":1091,\"start\":1085},{\"end\":1225,\"start\":1217},{\"end\":1237,\"start\":1232},{\"end\":1441,\"start\":1436},{\"end\":1611,\"start\":1605},{\"end\":1699,\"start\":1692},{\"end\":1818,\"start\":1812},{\"end\":1935,\"start\":1930},{\"end\":2056,\"start\":2048},{\"end\":2067,\"start\":2062},{\"end\":2109,\"start\":2101},{\"end\":2231,\"start\":2229},{\"end\":2451,\"start\":2446},{\"end\":2516,\"start\":2511},{\"end\":2580,\"start\":2574},{\"end\":2722,\"start\":2718},{\"end\":2730,\"start\":2723},{\"end\":2810,\"start\":2804},{\"end\":3026,\"start\":3021},{\"end\":3107,\"start\":3102}]", "author_affiliation": "[{\"end\":135,\"start\":85},{\"end\":200,\"start\":150},{\"end\":385,\"start\":210},{\"end\":507,\"start\":398},{\"end\":697,\"start\":522},{\"end\":765,\"start\":714},{\"end\":829,\"start\":778},{\"end\":884,\"start\":841},{\"end\":945,\"start\":898},{\"end\":1015,\"start\":959},{\"end\":1083,\"start\":1027},{\"end\":1215,\"start\":1098},{\"end\":1359,\"start\":1242},{\"end\":1434,\"start\":1361},{\"end\":1519,\"start\":1446},{\"end\":1603,\"start\":1521},{\"end\":1690,\"start\":1619},{\"end\":1810,\"start\":1704},{\"end\":1928,\"start\":1823},{\"end\":2046,\"start\":1941},{\"end\":2099,\"start\":2075},{\"end\":2227,\"start\":2116},{\"end\":2348,\"start\":2237},{\"end\":2444,\"start\":2350},{\"end\":2509,\"start\":2458},{\"end\":2572,\"start\":2521},{\"end\":2716,\"start\":2592},{\"end\":2802,\"start\":2740},{\"end\":3019,\"start\":2844},{\"end\":3100,\"start\":3050}]", "title": "[{\"end\":61,\"start\":1},{\"end\":3172,\"start\":3112}]", "venue": null, "abstract": "[{\"end\":3550,\"start\":3424}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5977,\"start\":5953},{\"end\":6045,\"start\":6027},{\"end\":7316,\"start\":7296},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7338,\"start\":7318},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7356,\"start\":7340},{\"end\":7990,\"start\":7970},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8008,\"start\":7992},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8029,\"start\":8010},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8047,\"start\":8031},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8685,\"start\":8665},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9518,\"start\":9496},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15499,\"start\":15481},{\"end\":15518,\"start\":15499},{\"end\":18196,\"start\":18191},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18915,\"start\":18893},{\"end\":19713,\"start\":19687},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20486,\"start\":20467},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21694,\"start\":21675},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21906,\"start\":21884},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22807,\"start\":22789},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25947,\"start\":25929},{\"end\":29503,\"start\":29483},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39272,\"start\":39250},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39305,\"start\":39286},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":42813,\"start\":42794},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":45195,\"start\":45172},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47063,\"start\":47042},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":52146,\"start\":52126},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":52748,\"start\":52729}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49309,\"start\":49126},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49424,\"start\":49310},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49971,\"start\":49425},{\"attributes\":{\"id\":\"fig_4\"},\"end\":51430,\"start\":49972},{\"attributes\":{\"id\":\"fig_5\"},\"end\":53683,\"start\":51431},{\"attributes\":{\"id\":\"fig_6\"},\"end\":54156,\"start\":53684},{\"attributes\":{\"id\":\"fig_7\"},\"end\":58365,\"start\":54157},{\"attributes\":{\"id\":\"fig_8\"},\"end\":58731,\"start\":58366},{\"attributes\":{\"id\":\"fig_9\"},\"end\":59428,\"start\":58732},{\"attributes\":{\"id\":\"fig_10\"},\"end\":59565,\"start\":59429},{\"attributes\":{\"id\":\"fig_11\"},\"end\":59787,\"start\":59566},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":61552,\"start\":59788},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":61757,\"start\":61553},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":67857,\"start\":61758},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":68485,\"start\":67858},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":69252,\"start\":68486},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":70666,\"start\":69253},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":71880,\"start\":70667},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":72999,\"start\":71881},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":73389,\"start\":73000},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":73566,\"start\":73390},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":74132,\"start\":73567},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":77671,\"start\":74133},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":81308,\"start\":77672}]", "paragraph": "[{\"end\":3790,\"start\":3552},{\"end\":4627,\"start\":3792},{\"end\":5746,\"start\":4629},{\"end\":6186,\"start\":5797},{\"end\":6262,\"start\":6188},{\"end\":6611,\"start\":6264},{\"end\":6771,\"start\":6613},{\"end\":6936,\"start\":6773},{\"end\":7010,\"start\":6938},{\"end\":7564,\"start\":7012},{\"end\":8687,\"start\":7566},{\"end\":9227,\"start\":8689},{\"end\":9830,\"start\":9229},{\"end\":10140,\"start\":9832},{\"end\":10300,\"start\":10142},{\"end\":10628,\"start\":10302},{\"end\":10764,\"start\":10630},{\"end\":11410,\"start\":10766},{\"end\":11980,\"start\":11412},{\"end\":12959,\"start\":12004},{\"end\":13498,\"start\":12961},{\"end\":14944,\"start\":13516},{\"end\":15520,\"start\":14965},{\"end\":16273,\"start\":15543},{\"end\":16462,\"start\":16307},{\"end\":16642,\"start\":16464},{\"end\":17045,\"start\":16686},{\"end\":17190,\"start\":17089},{\"end\":17784,\"start\":17213},{\"end\":17806,\"start\":17803},{\"end\":17937,\"start\":17840},{\"end\":18799,\"start\":17969},{\"end\":19173,\"start\":18825},{\"end\":20587,\"start\":19193},{\"end\":21525,\"start\":20589},{\"end\":21757,\"start\":21527},{\"end\":22293,\"start\":21759},{\"end\":22373,\"start\":22295},{\"end\":23741,\"start\":22375},{\"end\":24000,\"start\":23776},{\"end\":24287,\"start\":24023},{\"end\":24745,\"start\":24289},{\"end\":25633,\"start\":24762},{\"end\":25856,\"start\":25635},{\"end\":26463,\"start\":25858},{\"end\":26537,\"start\":26465},{\"end\":26613,\"start\":26539},{\"end\":27021,\"start\":26615},{\"end\":27295,\"start\":27043},{\"end\":27529,\"start\":27297},{\"end\":27991,\"start\":27531},{\"end\":28150,\"start\":27993},{\"end\":28357,\"start\":28152},{\"end\":28398,\"start\":28359},{\"end\":28423,\"start\":28400},{\"end\":28449,\"start\":28425},{\"end\":28503,\"start\":28451},{\"end\":28586,\"start\":28505},{\"end\":28893,\"start\":28610},{\"end\":29236,\"start\":28925},{\"end\":29566,\"start\":29238},{\"end\":29797,\"start\":29568},{\"end\":30210,\"start\":29799},{\"end\":30285,\"start\":30212},{\"end\":30608,\"start\":30344},{\"end\":31025,\"start\":30636},{\"end\":31647,\"start\":31070},{\"end\":32552,\"start\":31649},{\"end\":32627,\"start\":32554},{\"end\":32934,\"start\":32629},{\"end\":34356,\"start\":32985},{\"end\":35794,\"start\":34387},{\"end\":37756,\"start\":35849},{\"end\":38941,\"start\":37796},{\"end\":40067,\"start\":38943},{\"end\":41022,\"start\":40069},{\"end\":41800,\"start\":41024},{\"end\":41924,\"start\":41857},{\"end\":42424,\"start\":41926},{\"end\":43246,\"start\":42426},{\"end\":43633,\"start\":43248},{\"end\":44134,\"start\":43635},{\"end\":44299,\"start\":44149},{\"end\":44523,\"start\":44301},{\"end\":44810,\"start\":44525},{\"end\":45033,\"start\":44812},{\"end\":45153,\"start\":45035},{\"end\":45330,\"start\":45155},{\"end\":45338,\"start\":45332},{\"end\":45444,\"start\":45351},{\"end\":47094,\"start\":45467},{\"end\":47315,\"start\":47096},{\"end\":47806,\"start\":47317},{\"end\":48042,\"start\":47829},{\"end\":48736,\"start\":48044},{\"end\":49125,\"start\":48738}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17088,\"start\":17046},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17802,\"start\":17785},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17968,\"start\":17938},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18824,\"start\":18800},{\"attributes\":{\"id\":\"formula_4\"},\"end\":28609,\"start\":28587},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30343,\"start\":30286}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20936,\"start\":20929},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21054,\"start\":21047},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22163,\"start\":22156},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23652,\"start\":23645},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23879,\"start\":23872},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30391,\"start\":30384},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31343,\"start\":31336},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":31661,\"start\":31654},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32480,\"start\":32471},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":33121,\"start\":33114},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35042,\"start\":35033},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35448,\"start\":35441},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42250,\"start\":42243},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":42263,\"start\":42255},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":45525,\"start\":45518},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":45732,\"start\":45725},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":47980,\"start\":47973}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":5761,\"start\":5749},{\"end\":5795,\"start\":5764},{\"attributes\":{\"n\":\"2.\"},\"end\":12002,\"start\":11983},{\"attributes\":{\"n\":\"2.1.\"},\"end\":13514,\"start\":13501},{\"attributes\":{\"n\":\"2.1.1.\"},\"end\":14963,\"start\":14947},{\"attributes\":{\"n\":\"2.1.2.\"},\"end\":15541,\"start\":15523},{\"attributes\":{\"n\":\"2.1.3.\"},\"end\":16305,\"start\":16276},{\"attributes\":{\"n\":\"2.2.\"},\"end\":16665,\"start\":16645},{\"attributes\":{\"n\":\"2.2.1.\"},\"end\":16684,\"start\":16668},{\"attributes\":{\"n\":\"2.2.2.\"},\"end\":17211,\"start\":17193},{\"attributes\":{\"n\":\"2.2.3.\"},\"end\":17838,\"start\":17809},{\"attributes\":{\"n\":\"2.3.\"},\"end\":19191,\"start\":19176},{\"attributes\":{\"n\":\"3.\"},\"end\":23774,\"start\":23744},{\"attributes\":{\"n\":\"3.1.\"},\"end\":24021,\"start\":24003},{\"attributes\":{\"n\":\"3.2.\"},\"end\":24760,\"start\":24748},{\"attributes\":{\"n\":\"3.3.\"},\"end\":27041,\"start\":27024},{\"attributes\":{\"n\":\"4.\"},\"end\":28903,\"start\":28896},{\"attributes\":{\"n\":\"4.1.\"},\"end\":28923,\"start\":28906},{\"attributes\":{\"n\":\"4.2.\"},\"end\":30634,\"start\":30611},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":31068,\"start\":31028},{\"attributes\":{\"n\":\"4.2.2.\"},\"end\":32983,\"start\":32937},{\"attributes\":{\"n\":\"4.2.3.\"},\"end\":34385,\"start\":34359},{\"attributes\":{\"n\":\"5.\"},\"end\":35807,\"start\":35797},{\"attributes\":{\"n\":\"5.1.\"},\"end\":35847,\"start\":35810},{\"attributes\":{\"n\":\"5.2.\"},\"end\":37794,\"start\":37759},{\"attributes\":{\"n\":\"5.3.\"},\"end\":41855,\"start\":41803},{\"attributes\":{\"n\":\"6.\"},\"end\":44147,\"start\":44137},{\"end\":45349,\"start\":45341},{\"end\":45465,\"start\":45447},{\"end\":47827,\"start\":47809},{\"end\":49137,\"start\":49127},{\"end\":58387,\"start\":58367},{\"end\":58759,\"start\":58733},{\"end\":59440,\"start\":59430},{\"end\":59577,\"start\":59567},{\"end\":61563,\"start\":61554},{\"end\":61768,\"start\":61759},{\"end\":67868,\"start\":67859},{\"end\":68496,\"start\":68487},{\"end\":69263,\"start\":69254},{\"end\":70677,\"start\":70668},{\"end\":71891,\"start\":71882},{\"end\":73010,\"start\":73001},{\"end\":73400,\"start\":73391},{\"end\":73577,\"start\":73568},{\"end\":74143,\"start\":74134},{\"end\":77683,\"start\":77673}]", "table": "[{\"end\":67857,\"start\":62907},{\"end\":68485,\"start\":68161},{\"end\":69252,\"start\":68800},{\"end\":70666,\"start\":69302},{\"end\":71880,\"start\":70752},{\"end\":72999,\"start\":71969},{\"end\":73389,\"start\":73062},{\"end\":74132,\"start\":73629},{\"end\":77671,\"start\":74888},{\"end\":81308,\"start\":78614}]", "figure_caption": "[{\"end\":49309,\"start\":49139},{\"end\":49424,\"start\":49312},{\"end\":49971,\"start\":49427},{\"end\":51430,\"start\":49974},{\"end\":53683,\"start\":51433},{\"end\":54156,\"start\":53686},{\"end\":58365,\"start\":54159},{\"end\":58731,\"start\":58390},{\"end\":59428,\"start\":58762},{\"end\":59565,\"start\":59442},{\"end\":59787,\"start\":59579},{\"end\":61552,\"start\":59790},{\"end\":61757,\"start\":61565},{\"end\":62907,\"start\":61770},{\"end\":68161,\"start\":67870},{\"end\":68800,\"start\":68498},{\"end\":69302,\"start\":69265},{\"end\":70752,\"start\":70679},{\"end\":71969,\"start\":71893},{\"end\":73062,\"start\":73012},{\"end\":73566,\"start\":73402},{\"end\":73629,\"start\":73579},{\"end\":74888,\"start\":74145},{\"end\":78614,\"start\":77686}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10066,\"start\":10056},{\"end\":11274,\"start\":11266},{\"end\":19527,\"start\":19519},{\"end\":24744,\"start\":24736},{\"end\":26287,\"start\":26279},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":29235,\"start\":29227},{\"end\":45862,\"start\":45854},{\"end\":47805,\"start\":47797},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":48696,\"start\":48688},{\"end\":48789,\"start\":48781}]", "bib_author_first_name": "[{\"end\":81666,\"start\":81665},{\"end\":81675,\"start\":81674},{\"end\":81686,\"start\":81685},{\"end\":81695,\"start\":81694},{\"end\":81704,\"start\":81703},{\"end\":81706,\"start\":81705},{\"end\":81715,\"start\":81714},{\"end\":81733,\"start\":81732},{\"end\":81742,\"start\":81741},{\"end\":81753,\"start\":81752},{\"end\":81766,\"start\":81765},{\"end\":82218,\"start\":82217},{\"end\":82229,\"start\":82228},{\"end\":82540,\"start\":82539},{\"end\":82548,\"start\":82547},{\"end\":82556,\"start\":82555},{\"end\":82562,\"start\":82561},{\"end\":82568,\"start\":82567},{\"end\":82575,\"start\":82574},{\"end\":83018,\"start\":83017},{\"end\":83025,\"start\":83024},{\"end\":83032,\"start\":83031},{\"end\":83038,\"start\":83037},{\"end\":83408,\"start\":83407},{\"end\":83416,\"start\":83415},{\"end\":83425,\"start\":83424},{\"end\":83431,\"start\":83430},{\"end\":83437,\"start\":83436},{\"end\":83446,\"start\":83445},{\"end\":83452,\"start\":83451},{\"end\":83459,\"start\":83458},{\"end\":83765,\"start\":83764},{\"end\":83773,\"start\":83772},{\"end\":83782,\"start\":83781},{\"end\":83790,\"start\":83789},{\"end\":83798,\"start\":83797},{\"end\":83805,\"start\":83804},{\"end\":84101,\"start\":84100},{\"end\":84108,\"start\":84107},{\"end\":84110,\"start\":84109},{\"end\":84419,\"start\":84418},{\"end\":84426,\"start\":84425},{\"end\":84436,\"start\":84435},{\"end\":84444,\"start\":84443},{\"end\":84452,\"start\":84451},{\"end\":84454,\"start\":84453},{\"end\":84988,\"start\":84987},{\"end\":84996,\"start\":84995},{\"end\":85002,\"start\":85001},{\"end\":85008,\"start\":85007},{\"end\":85015,\"start\":85014},{\"end\":85022,\"start\":85021},{\"end\":85029,\"start\":85028},{\"end\":85035,\"start\":85034},{\"end\":85044,\"start\":85043},{\"end\":85052,\"start\":85051},{\"end\":85457,\"start\":85456},{\"end\":85465,\"start\":85464},{\"end\":85474,\"start\":85473},{\"end\":85480,\"start\":85479},{\"end\":85486,\"start\":85485},{\"end\":85495,\"start\":85494},{\"end\":85843,\"start\":85842},{\"end\":85849,\"start\":85848},{\"end\":85858,\"start\":85857},{\"end\":85864,\"start\":85863},{\"end\":85868,\"start\":85865},{\"end\":85876,\"start\":85875},{\"end\":85883,\"start\":85882},{\"end\":86217,\"start\":86216},{\"end\":86223,\"start\":86222},{\"end\":86523,\"start\":86522},{\"end\":86529,\"start\":86528},{\"end\":86535,\"start\":86534},{\"end\":86542,\"start\":86541},{\"end\":86551,\"start\":86550},{\"end\":86555,\"start\":86552},{\"end\":86563,\"start\":86562},{\"end\":86570,\"start\":86569},{\"end\":86572,\"start\":86571},{\"end\":86582,\"start\":86581},{\"end\":86594,\"start\":86593},{\"end\":86999,\"start\":86998},{\"end\":87005,\"start\":87004},{\"end\":87007,\"start\":87006},{\"end\":87018,\"start\":87017},{\"end\":87033,\"start\":87032},{\"end\":87042,\"start\":87041},{\"end\":87051,\"start\":87050},{\"end\":87063,\"start\":87062},{\"end\":87072,\"start\":87071},{\"end\":87090,\"start\":87089},{\"end\":87100,\"start\":87099},{\"end\":87102,\"start\":87101},{\"end\":87554,\"start\":87553},{\"end\":87561,\"start\":87560},{\"end\":87567,\"start\":87566},{\"end\":87573,\"start\":87572},{\"end\":87581,\"start\":87580},{\"end\":87590,\"start\":87589},{\"end\":87597,\"start\":87596},{\"end\":87975,\"start\":87974},{\"end\":87977,\"start\":87976},{\"end\":87987,\"start\":87986},{\"end\":87989,\"start\":87988},{\"end\":88230,\"start\":88229},{\"end\":88236,\"start\":88235},{\"end\":88243,\"start\":88242},{\"end\":88250,\"start\":88249},{\"end\":88256,\"start\":88255},{\"end\":88263,\"start\":88262},{\"end\":88267,\"start\":88264},{\"end\":88543,\"start\":88542},{\"end\":88550,\"start\":88549},{\"end\":88558,\"start\":88557},{\"end\":88901,\"start\":88900},{\"end\":88908,\"start\":88907},{\"end\":88915,\"start\":88914},{\"end\":88922,\"start\":88921},{\"end\":88928,\"start\":88927},{\"end\":88935,\"start\":88934},{\"end\":88944,\"start\":88943},{\"end\":88951,\"start\":88950},{\"end\":89381,\"start\":89380},{\"end\":89390,\"start\":89389},{\"end\":89401,\"start\":89400},{\"end\":89410,\"start\":89409},{\"end\":89656,\"start\":89655},{\"end\":89658,\"start\":89657},{\"end\":89668,\"start\":89667},{\"end\":89672,\"start\":89669},{\"end\":89681,\"start\":89680},{\"end\":89683,\"start\":89682},{\"end\":89692,\"start\":89691},{\"end\":89702,\"start\":89701},{\"end\":90007,\"start\":90006},{\"end\":90016,\"start\":90015},{\"end\":90027,\"start\":90026},{\"end\":90029,\"start\":90028},{\"end\":90037,\"start\":90036},{\"end\":90047,\"start\":90046},{\"end\":90049,\"start\":90048},{\"end\":90317,\"start\":90316},{\"end\":90324,\"start\":90323},{\"end\":90332,\"start\":90331},{\"end\":90340,\"start\":90339},{\"end\":90347,\"start\":90346},{\"end\":90831,\"start\":90830},{\"end\":90833,\"start\":90832},{\"end\":90844,\"start\":90843},{\"end\":90850,\"start\":90849},{\"end\":90852,\"start\":90851},{\"end\":90861,\"start\":90860},{\"end\":90873,\"start\":90872},{\"end\":90875,\"start\":90874},{\"end\":90886,\"start\":90885},{\"end\":90900,\"start\":90899},{\"end\":90908,\"start\":90907},{\"end\":90910,\"start\":90909},{\"end\":90918,\"start\":90917},{\"end\":90925,\"start\":90924},{\"end\":91335,\"start\":91334},{\"end\":91344,\"start\":91343},{\"end\":91359,\"start\":91358},{\"end\":91369,\"start\":91368},{\"end\":91376,\"start\":91375},{\"end\":91386,\"start\":91385},{\"end\":91689,\"start\":91688},{\"end\":91697,\"start\":91696},{\"end\":91699,\"start\":91698},{\"end\":91708,\"start\":91707},{\"end\":91710,\"start\":91709},{\"end\":91721,\"start\":91720},{\"end\":91731,\"start\":91730},{\"end\":91741,\"start\":91740},{\"end\":91980,\"start\":91979},{\"end\":91990,\"start\":91989},{\"end\":92001,\"start\":92000},{\"end\":92013,\"start\":92012},{\"end\":92428,\"start\":92427},{\"end\":92441,\"start\":92440},{\"end\":92454,\"start\":92453},{\"end\":92471,\"start\":92470},{\"end\":92480,\"start\":92479},{\"end\":92499,\"start\":92498},{\"end\":92501,\"start\":92500},{\"end\":92513,\"start\":92512},{\"end\":92515,\"start\":92514},{\"end\":92858,\"start\":92857},{\"end\":92873,\"start\":92872},{\"end\":92884,\"start\":92883},{\"end\":93294,\"start\":93293},{\"end\":93307,\"start\":93306},{\"end\":93317,\"start\":93316},{\"end\":93328,\"start\":93327},{\"end\":93573,\"start\":93572},{\"end\":93581,\"start\":93580},{\"end\":93592,\"start\":93591},{\"end\":94003,\"start\":94002},{\"end\":94017,\"start\":94016},{\"end\":94390,\"start\":94389},{\"end\":94399,\"start\":94398},{\"end\":94401,\"start\":94400},{\"end\":94685,\"start\":94684},{\"end\":94692,\"start\":94691}]", "bib_author_last_name": "[{\"end\":81672,\"start\":81667},{\"end\":81683,\"start\":81676},{\"end\":81692,\"start\":81687},{\"end\":81701,\"start\":81696},{\"end\":81712,\"start\":81707},{\"end\":81730,\"start\":81716},{\"end\":81739,\"start\":81734},{\"end\":81750,\"start\":81743},{\"end\":81763,\"start\":81754},{\"end\":81780,\"start\":81767},{\"end\":82226,\"start\":82219},{\"end\":82240,\"start\":82230},{\"end\":82545,\"start\":82541},{\"end\":82553,\"start\":82549},{\"end\":82559,\"start\":82557},{\"end\":82565,\"start\":82563},{\"end\":82572,\"start\":82569},{\"end\":82581,\"start\":82576},{\"end\":83022,\"start\":83019},{\"end\":83029,\"start\":83026},{\"end\":83035,\"start\":83033},{\"end\":83043,\"start\":83039},{\"end\":83413,\"start\":83409},{\"end\":83422,\"start\":83417},{\"end\":83428,\"start\":83426},{\"end\":83434,\"start\":83432},{\"end\":83443,\"start\":83438},{\"end\":83449,\"start\":83447},{\"end\":83456,\"start\":83453},{\"end\":83463,\"start\":83460},{\"end\":83770,\"start\":83766},{\"end\":83779,\"start\":83774},{\"end\":83787,\"start\":83783},{\"end\":83795,\"start\":83791},{\"end\":83802,\"start\":83799},{\"end\":83808,\"start\":83806},{\"end\":84105,\"start\":84102},{\"end\":84116,\"start\":84111},{\"end\":84423,\"start\":84420},{\"end\":84433,\"start\":84427},{\"end\":84441,\"start\":84437},{\"end\":84449,\"start\":84445},{\"end\":84459,\"start\":84455},{\"end\":84993,\"start\":84989},{\"end\":84999,\"start\":84997},{\"end\":85005,\"start\":85003},{\"end\":85012,\"start\":85009},{\"end\":85019,\"start\":85016},{\"end\":85026,\"start\":85023},{\"end\":85032,\"start\":85030},{\"end\":85041,\"start\":85036},{\"end\":85049,\"start\":85045},{\"end\":85057,\"start\":85053},{\"end\":85462,\"start\":85458},{\"end\":85471,\"start\":85466},{\"end\":85477,\"start\":85475},{\"end\":85483,\"start\":85481},{\"end\":85492,\"start\":85487},{\"end\":85498,\"start\":85496},{\"end\":85846,\"start\":85844},{\"end\":85855,\"start\":85850},{\"end\":85861,\"start\":85859},{\"end\":85873,\"start\":85869},{\"end\":85880,\"start\":85877},{\"end\":85887,\"start\":85884},{\"end\":86220,\"start\":86218},{\"end\":86229,\"start\":86224},{\"end\":86526,\"start\":86524},{\"end\":86532,\"start\":86530},{\"end\":86539,\"start\":86536},{\"end\":86548,\"start\":86543},{\"end\":86560,\"start\":86556},{\"end\":86567,\"start\":86564},{\"end\":86579,\"start\":86573},{\"end\":86591,\"start\":86583},{\"end\":86599,\"start\":86595},{\"end\":87002,\"start\":87000},{\"end\":87015,\"start\":87008},{\"end\":87030,\"start\":87019},{\"end\":87039,\"start\":87034},{\"end\":87048,\"start\":87043},{\"end\":87060,\"start\":87052},{\"end\":87069,\"start\":87064},{\"end\":87087,\"start\":87073},{\"end\":87097,\"start\":87091},{\"end\":87108,\"start\":87103},{\"end\":87558,\"start\":87555},{\"end\":87564,\"start\":87562},{\"end\":87570,\"start\":87568},{\"end\":87578,\"start\":87574},{\"end\":87587,\"start\":87582},{\"end\":87594,\"start\":87591},{\"end\":87601,\"start\":87598},{\"end\":87984,\"start\":87978},{\"end\":88233,\"start\":88231},{\"end\":88240,\"start\":88237},{\"end\":88247,\"start\":88244},{\"end\":88253,\"start\":88251},{\"end\":88260,\"start\":88257},{\"end\":88271,\"start\":88268},{\"end\":88547,\"start\":88544},{\"end\":88555,\"start\":88551},{\"end\":88562,\"start\":88559},{\"end\":88905,\"start\":88902},{\"end\":88912,\"start\":88909},{\"end\":88919,\"start\":88916},{\"end\":88925,\"start\":88923},{\"end\":88932,\"start\":88929},{\"end\":88941,\"start\":88936},{\"end\":88948,\"start\":88945},{\"end\":88955,\"start\":88952},{\"end\":89387,\"start\":89382},{\"end\":89398,\"start\":89391},{\"end\":89407,\"start\":89402},{\"end\":89418,\"start\":89411},{\"end\":89665,\"start\":89659},{\"end\":89678,\"start\":89673},{\"end\":89689,\"start\":89684},{\"end\":89699,\"start\":89693},{\"end\":89709,\"start\":89703},{\"end\":90013,\"start\":90008},{\"end\":90024,\"start\":90017},{\"end\":90034,\"start\":90030},{\"end\":90044,\"start\":90038},{\"end\":90053,\"start\":90050},{\"end\":90321,\"start\":90318},{\"end\":90329,\"start\":90325},{\"end\":90337,\"start\":90333},{\"end\":90344,\"start\":90341},{\"end\":90351,\"start\":90348},{\"end\":90841,\"start\":90834},{\"end\":90847,\"start\":90845},{\"end\":90858,\"start\":90853},{\"end\":90870,\"start\":90862},{\"end\":90883,\"start\":90876},{\"end\":90897,\"start\":90887},{\"end\":90905,\"start\":90901},{\"end\":90915,\"start\":90911},{\"end\":90922,\"start\":90919},{\"end\":90929,\"start\":90926},{\"end\":91341,\"start\":91336},{\"end\":91356,\"start\":91345},{\"end\":91366,\"start\":91360},{\"end\":91373,\"start\":91370},{\"end\":91383,\"start\":91377},{\"end\":91393,\"start\":91387},{\"end\":91694,\"start\":91690},{\"end\":91705,\"start\":91700},{\"end\":91718,\"start\":91711},{\"end\":91728,\"start\":91722},{\"end\":91738,\"start\":91732},{\"end\":91747,\"start\":91742},{\"end\":91987,\"start\":91981},{\"end\":91998,\"start\":91991},{\"end\":92010,\"start\":92002},{\"end\":92021,\"start\":92014},{\"end\":92438,\"start\":92429},{\"end\":92451,\"start\":92442},{\"end\":92468,\"start\":92455},{\"end\":92477,\"start\":92472},{\"end\":92496,\"start\":92481},{\"end\":92510,\"start\":92502},{\"end\":92524,\"start\":92516},{\"end\":92870,\"start\":92859},{\"end\":92881,\"start\":92874},{\"end\":92889,\"start\":92885},{\"end\":93304,\"start\":93295},{\"end\":93314,\"start\":93308},{\"end\":93325,\"start\":93318},{\"end\":93334,\"start\":93329},{\"end\":93578,\"start\":93574},{\"end\":93589,\"start\":93582},{\"end\":93599,\"start\":93593},{\"end\":94014,\"start\":94004},{\"end\":94028,\"start\":94018},{\"end\":94396,\"start\":94391},{\"end\":94407,\"start\":94402},{\"end\":94689,\"start\":94686},{\"end\":94695,\"start\":94693}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":245720350},\"end\":82145,\"start\":81525},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":61382149},\"end\":82414,\"start\":82147},{\"attributes\":{\"doi\":\"10.1109/TMI.2021.3131245\",\"id\":\"b2\",\"matched_paper_id\":244729893},\"end\":82902,\"start\":82416},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":245853807},\"end\":83360,\"start\":82904},{\"attributes\":{\"doi\":\"arXiv:2004.12432\",\"id\":\"b4\"},\"end\":83653,\"start\":83362},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":141512034},\"end\":84049,\"start\":83655},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8788358},\"end\":84300,\"start\":84051},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13741866},\"end\":84907,\"start\":84302},{\"attributes\":{\"doi\":\"arXiv:2202.08994\",\"id\":\"b8\"},\"end\":85306,\"start\":84909},{\"attributes\":{\"id\":\"b9\"},\"end\":85742,\"start\":85308},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":22910518},\"end\":86144,\"start\":85744},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":29153456},\"end\":86426,\"start\":86146},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3536434},\"end\":86891,\"start\":86428},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":249580095},\"end\":87470,\"start\":86893},{\"attributes\":{\"doi\":\"10.1109/TMI.2021.3105046\",\"id\":\"b14\",\"matched_paper_id\":231632550},\"end\":87900,\"start\":87472},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":72058497},\"end\":88158,\"start\":87902},{\"attributes\":{\"doi\":\"arXiv:2105.09511\",\"id\":\"b16\"},\"end\":88469,\"start\":88160},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13904036},\"end\":88825,\"start\":88471},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":232352874},\"end\":89331,\"start\":88827},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":126786100},\"end\":89574,\"start\":89333},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13459453},\"end\":89943,\"start\":89576},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9766683},\"end\":90250,\"start\":89945},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1487039},\"end\":90708,\"start\":90252},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":203952960},\"end\":91252,\"start\":90710},{\"attributes\":{\"doi\":\"arXiv:1812.08911\",\"id\":\"b24\"},\"end\":91625,\"start\":91254},{\"attributes\":{\"id\":\"b25\"},\"end\":91920,\"start\":91627},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206594738},\"end\":92374,\"start\":91922},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":39872312},\"end\":92790,\"start\":92376},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3719281},\"end\":93212,\"start\":92792},{\"attributes\":{\"id\":\"b29\"},\"end\":93524,\"start\":93214},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":22114556},\"end\":93921,\"start\":93526},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4286084},\"end\":94315,\"start\":93923},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":125673949},\"end\":94608,\"start\":94317},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b33\",\"matched_paper_id\":167217261},\"end\":94914,\"start\":94610}]", "bib_title": "[{\"end\":81663,\"start\":81525},{\"end\":82215,\"start\":82147},{\"end\":82537,\"start\":82416},{\"end\":83015,\"start\":82904},{\"end\":83762,\"start\":83655},{\"end\":84098,\"start\":84051},{\"end\":84416,\"start\":84302},{\"end\":85840,\"start\":85744},{\"end\":86214,\"start\":86146},{\"end\":86520,\"start\":86428},{\"end\":86996,\"start\":86893},{\"end\":87551,\"start\":87472},{\"end\":87972,\"start\":87902},{\"end\":88540,\"start\":88471},{\"end\":88898,\"start\":88827},{\"end\":89378,\"start\":89333},{\"end\":89653,\"start\":89576},{\"end\":90004,\"start\":89945},{\"end\":90314,\"start\":90252},{\"end\":90828,\"start\":90710},{\"end\":91977,\"start\":91922},{\"end\":92425,\"start\":92376},{\"end\":92855,\"start\":92792},{\"end\":93570,\"start\":93526},{\"end\":94000,\"start\":93923},{\"end\":94387,\"start\":94317},{\"end\":94682,\"start\":94610}]", "bib_author": "[{\"end\":81674,\"start\":81665},{\"end\":81685,\"start\":81674},{\"end\":81694,\"start\":81685},{\"end\":81703,\"start\":81694},{\"end\":81714,\"start\":81703},{\"end\":81732,\"start\":81714},{\"end\":81741,\"start\":81732},{\"end\":81752,\"start\":81741},{\"end\":81765,\"start\":81752},{\"end\":81782,\"start\":81765},{\"end\":82228,\"start\":82217},{\"end\":82242,\"start\":82228},{\"end\":82547,\"start\":82539},{\"end\":82555,\"start\":82547},{\"end\":82561,\"start\":82555},{\"end\":82567,\"start\":82561},{\"end\":82574,\"start\":82567},{\"end\":82583,\"start\":82574},{\"end\":83024,\"start\":83017},{\"end\":83031,\"start\":83024},{\"end\":83037,\"start\":83031},{\"end\":83045,\"start\":83037},{\"end\":83415,\"start\":83407},{\"end\":83424,\"start\":83415},{\"end\":83430,\"start\":83424},{\"end\":83436,\"start\":83430},{\"end\":83445,\"start\":83436},{\"end\":83451,\"start\":83445},{\"end\":83458,\"start\":83451},{\"end\":83465,\"start\":83458},{\"end\":83772,\"start\":83764},{\"end\":83781,\"start\":83772},{\"end\":83789,\"start\":83781},{\"end\":83797,\"start\":83789},{\"end\":83804,\"start\":83797},{\"end\":83810,\"start\":83804},{\"end\":84107,\"start\":84100},{\"end\":84118,\"start\":84107},{\"end\":84425,\"start\":84418},{\"end\":84435,\"start\":84425},{\"end\":84443,\"start\":84435},{\"end\":84451,\"start\":84443},{\"end\":84461,\"start\":84451},{\"end\":84995,\"start\":84987},{\"end\":85001,\"start\":84995},{\"end\":85007,\"start\":85001},{\"end\":85014,\"start\":85007},{\"end\":85021,\"start\":85014},{\"end\":85028,\"start\":85021},{\"end\":85034,\"start\":85028},{\"end\":85043,\"start\":85034},{\"end\":85051,\"start\":85043},{\"end\":85059,\"start\":85051},{\"end\":85464,\"start\":85456},{\"end\":85473,\"start\":85464},{\"end\":85479,\"start\":85473},{\"end\":85485,\"start\":85479},{\"end\":85494,\"start\":85485},{\"end\":85500,\"start\":85494},{\"end\":85848,\"start\":85842},{\"end\":85857,\"start\":85848},{\"end\":85863,\"start\":85857},{\"end\":85875,\"start\":85863},{\"end\":85882,\"start\":85875},{\"end\":85889,\"start\":85882},{\"end\":86222,\"start\":86216},{\"end\":86231,\"start\":86222},{\"end\":86528,\"start\":86522},{\"end\":86534,\"start\":86528},{\"end\":86541,\"start\":86534},{\"end\":86550,\"start\":86541},{\"end\":86562,\"start\":86550},{\"end\":86569,\"start\":86562},{\"end\":86581,\"start\":86569},{\"end\":86593,\"start\":86581},{\"end\":86601,\"start\":86593},{\"end\":87004,\"start\":86998},{\"end\":87017,\"start\":87004},{\"end\":87032,\"start\":87017},{\"end\":87041,\"start\":87032},{\"end\":87050,\"start\":87041},{\"end\":87062,\"start\":87050},{\"end\":87071,\"start\":87062},{\"end\":87089,\"start\":87071},{\"end\":87099,\"start\":87089},{\"end\":87110,\"start\":87099},{\"end\":87560,\"start\":87553},{\"end\":87566,\"start\":87560},{\"end\":87572,\"start\":87566},{\"end\":87580,\"start\":87572},{\"end\":87589,\"start\":87580},{\"end\":87596,\"start\":87589},{\"end\":87603,\"start\":87596},{\"end\":87986,\"start\":87974},{\"end\":87992,\"start\":87986},{\"end\":88235,\"start\":88229},{\"end\":88242,\"start\":88235},{\"end\":88249,\"start\":88242},{\"end\":88255,\"start\":88249},{\"end\":88262,\"start\":88255},{\"end\":88273,\"start\":88262},{\"end\":88549,\"start\":88542},{\"end\":88557,\"start\":88549},{\"end\":88564,\"start\":88557},{\"end\":88907,\"start\":88900},{\"end\":88914,\"start\":88907},{\"end\":88921,\"start\":88914},{\"end\":88927,\"start\":88921},{\"end\":88934,\"start\":88927},{\"end\":88943,\"start\":88934},{\"end\":88950,\"start\":88943},{\"end\":88957,\"start\":88950},{\"end\":89389,\"start\":89380},{\"end\":89400,\"start\":89389},{\"end\":89409,\"start\":89400},{\"end\":89420,\"start\":89409},{\"end\":89667,\"start\":89655},{\"end\":89680,\"start\":89667},{\"end\":89691,\"start\":89680},{\"end\":89701,\"start\":89691},{\"end\":89711,\"start\":89701},{\"end\":90015,\"start\":90006},{\"end\":90026,\"start\":90015},{\"end\":90036,\"start\":90026},{\"end\":90046,\"start\":90036},{\"end\":90055,\"start\":90046},{\"end\":90323,\"start\":90316},{\"end\":90331,\"start\":90323},{\"end\":90339,\"start\":90331},{\"end\":90346,\"start\":90339},{\"end\":90353,\"start\":90346},{\"end\":90843,\"start\":90830},{\"end\":90849,\"start\":90843},{\"end\":90860,\"start\":90849},{\"end\":90872,\"start\":90860},{\"end\":90885,\"start\":90872},{\"end\":90899,\"start\":90885},{\"end\":90907,\"start\":90899},{\"end\":90917,\"start\":90907},{\"end\":90924,\"start\":90917},{\"end\":90931,\"start\":90924},{\"end\":91343,\"start\":91334},{\"end\":91358,\"start\":91343},{\"end\":91368,\"start\":91358},{\"end\":91375,\"start\":91368},{\"end\":91385,\"start\":91375},{\"end\":91395,\"start\":91385},{\"end\":91696,\"start\":91688},{\"end\":91707,\"start\":91696},{\"end\":91720,\"start\":91707},{\"end\":91730,\"start\":91720},{\"end\":91740,\"start\":91730},{\"end\":91749,\"start\":91740},{\"end\":91989,\"start\":91979},{\"end\":92000,\"start\":91989},{\"end\":92012,\"start\":92000},{\"end\":92023,\"start\":92012},{\"end\":92440,\"start\":92427},{\"end\":92453,\"start\":92440},{\"end\":92470,\"start\":92453},{\"end\":92479,\"start\":92470},{\"end\":92498,\"start\":92479},{\"end\":92512,\"start\":92498},{\"end\":92526,\"start\":92512},{\"end\":92872,\"start\":92857},{\"end\":92883,\"start\":92872},{\"end\":92891,\"start\":92883},{\"end\":93306,\"start\":93293},{\"end\":93316,\"start\":93306},{\"end\":93327,\"start\":93316},{\"end\":93336,\"start\":93327},{\"end\":93580,\"start\":93572},{\"end\":93591,\"start\":93580},{\"end\":93601,\"start\":93591},{\"end\":94016,\"start\":94002},{\"end\":94030,\"start\":94016},{\"end\":94398,\"start\":94389},{\"end\":94409,\"start\":94398},{\"end\":94691,\"start\":94684},{\"end\":94697,\"start\":94691}]", "bib_venue": "[{\"end\":84610,\"start\":84544},{\"end\":89086,\"start\":89030},{\"end\":90494,\"start\":90432},{\"end\":92164,\"start\":92102},{\"end\":81810,\"start\":81782},{\"end\":82267,\"start\":82242},{\"end\":82643,\"start\":82607},{\"end\":83112,\"start\":83045},{\"end\":83405,\"start\":83362},{\"end\":83836,\"start\":83810},{\"end\":84161,\"start\":84118},{\"end\":84542,\"start\":84461},{\"end\":84985,\"start\":84909},{\"end\":85454,\"start\":85308},{\"end\":85925,\"start\":85889},{\"end\":86267,\"start\":86231},{\"end\":86637,\"start\":86601},{\"end\":87157,\"start\":87110},{\"end\":87663,\"start\":87627},{\"end\":88020,\"start\":87992},{\"end\":88227,\"start\":88160},{\"end\":88632,\"start\":88564},{\"end\":89028,\"start\":88957},{\"end\":89438,\"start\":89420},{\"end\":89743,\"start\":89711},{\"end\":90081,\"start\":90055},{\"end\":90430,\"start\":90353},{\"end\":90953,\"start\":90931},{\"end\":91332,\"start\":91254},{\"end\":91686,\"start\":91627},{\"end\":92100,\"start\":92023},{\"end\":92567,\"start\":92526},{\"end\":92977,\"start\":92891},{\"end\":93291,\"start\":93214},{\"end\":93693,\"start\":93601},{\"end\":94104,\"start\":94030},{\"end\":94449,\"start\":94409},{\"end\":94745,\"start\":94701}]"}}}, "year": 2023, "month": 12, "day": 17}