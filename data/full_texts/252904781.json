{"id": 252904781, "updated": "2023-04-05 00:14:30.722", "metadata": {"title": "ITSM-GCN: Informative Training Sample Mining for Graph Convolutional Network-based Collaborative Filtering", "authors": "[{\"first\":\"Kaiqi\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Xiao\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Senzhang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Songsong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yong\",\"last\":\"Li\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recently, graph convolutional network (GCN) has become one of the most popular and state-of-the-art collaborative filtering (CF) methods. Existing GCN-based CF studies have made many meaningful and excellent efforts at loss function design and embedding propagation improvement. Despite their successes, we argue that existing methods have not yet properly explored more effective sampling strategy, including both positive sampling and negative sampling. To tackle this limitation, a novel framework named ITSM-GCN is proposed to carry out our designed Informative Training Sample Mining (ITSM) sampling strategy for the learning of GCN-based CF models. Specifically, we first adopt and improve the dynamic negative sampling (DNS) strategy, which achieves considerable improvements in both training efficiency and recommendation performance. More importantly, we design two potentially positive training sample mining strategies, namely a similarity-based sampler and score-based sampler, to further enhance GCN-based CF. Extensive experiments show that ITSM-GCN significantly outperforms state-of-the-art GCN-based CF models, including LightGCN, SGL-ED and SimpleX. For example, ITSM-GCN improves on SimpleX by 12.0%, 3.0%, and 1.2% on Recall@20 for Amazon-Books, Yelp2018 and Gowalla, respectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cikm/GongSWLL22", "doi": "10.1145/3511808.3557368"}}, "content": {"source": {"pdf_hash": "b9f59cfb6f92a2e8d4476672bb08bc47a29169aa", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e3a7a7b0a93001f588067f45a4f4c6f2d9998f72", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b9f59cfb6f92a2e8d4476672bb08bc47a29169aa.txt", "contents": "\nITSM-GCN: Informative Training Sample Mining for Graph Convolutional Network-based Collaborative Filtering ITSM-GCN: Informative Training Sample Mining for Graph Convolutional * Corresponding Author Network-based Collaborative\nACMCopyright ACMOctober 17-21, 2022. October 17-21, 2022\n\nG A Atlanta \nFiltering \nITSM-GCN: Informative Training Sample Mining for Graph Convolutional Network-based Collaborative Filtering ITSM-GCN: Informative Training Sample Mining for Graph Convolutional * Corresponding Author Network-based Collaborative\n\nProceedings of the 31st ACM Inter-national Conference on Information and Knowledge Management (CIKM '22)\nthe 31st ACM Inter-national Conference on Information and Knowledge Management (CIKM '22)Atlanta, GA, USA; New York, NY, USAACM10October 17-21, 2022. October 17-21, 202210.1145/3511808.3557368Recommender systemscollaborative filteringgraph convolutional networkspositive samplingnegative sampling\nRecently, graph convolutional network (GCN) has become one of the most popular and state-of-the-art collaborative filtering (CF) methods. Existing GCN-based CF studies have made many meaningful and excellent efforts at loss function design and embedding propagation improvement. Despite their successes, we argue that existing methods have not yet properly explored more effective sampling strategy, including both positive sampling and negative sampling. To tackle this limitation, a novel framework named ITSM-GCN is proposed to carry out our designed Informative Training Sample Mining (ITSM) sampling strategy for the learning of GCNbased CF models. Specifically, we first adopt and improve the dynamic negative sampling (DNS) strategy, which achieves considerable improvements in both training efficiency and recommendation performance. More importantly, we design two potentially positive training sample mining strategies, namely a similarity-based sampler and score-based sampler, to further enhance GCN-based CF. Extensive experiments show that ITSM-GCN significantly outperforms state-of-the-art GCN-based CF models, including LightGCN, SGL-ED and SimpleX. For example, ITSM-GCN improves on Sim-pleX by 12.0%, 3.0%, and 1.2% on Recall@20 for Amazon-Books, Yelp2018 and Gowalla, respectively.CCS CONCEPTS\u2022 Information systems \u2192 Recommender systems.\n\nINTRODUCTION\n\nThe influx of massive amounts of information into various Internet applications has seen the emergence of personalized recommendation systems [6,10,12,25,30]. Collaborative filtering (CF) is the most classical and popular method in recommendation systems [11,12,15,25]. The basic idea of CF is to leverage the collaborative signals between users and items based on the users' historical interaction data, and then to find and recommend items that these users may like. Recently, due to its outstanding advantages in capturing high-order collaborative signals, graph convolutional network (GCN) has become one of the most popular and state-of-the-art CF methods [2,11,28].\n\nThe training process of a GCN-based CF model contains three key components, i.e., loss function, sampling strategy, and embedding propagation, all of which play key roles in the model performance.\n\nThere have been many breakthroughs in the study of loss function and embedding propagation. For example, a large number of loss functions, such as Bayesian personalized ranking (BPR) loss [24], pointwise log loss [12], softmax cross-entropy loss [3], pairwise hinge loss [13], mean square error loss [1], cosine contrastive loss [19], and contrastive learning loss [17], are proposed and can be extended for GCN-based CF. As for embedding propagation, existing studies have investigated such perspectives as high-order connectivity modeling [28], graph convolution simplification [11], graph convolution over-smoothing [18], and graph convolution computational efficiency [20]. Despite great successes of these approaches, we argue that they may overlook a comprehensive and rational sampling strategy. Existing works show that a well-designed sampling strategy can also be equipped with a simple but effective loss function and embedding propagation scheme to form a promising GCN-based CF recommendation model [9,14,35].\n\nAlthough selecting informative training samples from a large number of items is extremely important to the performance of GCN-based CF recommendation models, many existing studies suffer from two issues on this front. First, although it has been proven that dynamic negative sampling (DNS) [14,[33][34][35] is an effective negative sampling method that can provide quite a few candidate informative negative samples for model training, the question of how to select better negative instances from among the candidates has largely not been explored. In addition, few studies utilize the false negative samples [5,23] during negative sampling to improve CF. Second, existing studies focus on mining high-quality negative training samples from items that have never been observed [4,14,33,35], but they only treat users' interaction items as positive samples. That is, they neglect the possible existence of a large number of positive samples among the unobserved items, which would be likely to enhance model training.\n\nBased on the above insights, we introduce an effective sampling strategy to mine informative training samples, including both negative and positive instances. Specifically, we first carefully tune up the execution process of DNS, including the tasks of selecting the number of negative samples and avoiding the false negative problem. Then, more importantly, we design two potentially positive sample mining strategies, namely similarity-based mining and score-based mining. Similarity-based mining strategy aims to select some of the closest matches from those items similar to users' interaction items as positive samples. Meanwhile, inspired by the false negative phenomenon in negative sampling, score-based mining strategy makes full use of the high-scored potentially positive samples obtained from negative sampling.\n\nIn this work, we propose a training framework (named ITSM-GCN) for a GCN-based CF recommendation model to realize our informative training sample mining strategy. In general, as shown in Figure 1, the ITSM-GCN framework mainly consists of a basic GCN model, our designed ITSM sampling strategy (including a conventional positive sampler, two novel positive samplers, and an improved dynamic negative sampler), and the BPR loss function. In ITSM, the conventional positive sampler randomly selects positive training samples from users' interaction items. Conversely, our two novel positive samplers augment more potentially informative positive instances for model training according to their respective rules, so as to improve the modeling ability of users' preferences.\n\nIn conclusion, our main contributions are as follows:\n\n\u2022 We emphasize the importance of both negative and positive sampling for CF model training, and accordingly design an effective sampling strategy (i.e., ITSM) to mine informative training samples. \u2022 We propose a model-agnostic training framework (i.e., ITSM-GCN) equipped with our sampling strategy to enhance GCNbased CF recommendations. The framework incorporates one basic GCN model, the designed ITSM sampling strategy, and the BPR loss function. \u2022 We perform extensive experiments on three public datasets to demonstrate that our ITSM-GCN is better than a number of existing CF models, including matrix factorization (MF) based, network embedding and GCN-based CF methods.\n\n\nBACKGROUND AND RELATED WORK\n\nIn this work, we focus on implicit collaborative filtering, since it is more common in real recommendation scenarios. The core of the implicit CF is to discover users' preferences about items based on implicit user-item interaction data (such as users' clicking, collecting and purchasing behaviors). The interaction data can be represented by a binary matrix , where = 1 indicates that user has interacted with item before, and vice versa. In general, the user-item interaction data includes a set of users U = { } and a set of items I = { }. Assume that the number of users and items are and , respectively. Based on the interaction data, we can construct a set of pairwise training data O consisting of a series of triples ( , , ), where is an interactive positive item of user and is a non-interactive negative item.\n\n\nClassical CF Methods\n\nIn this part,we list three classical CF methods that exhibit great differences in how they treat interaction data and mine collaborative signals from it.\n\n(1) MF-based methods. Early matrix factorization (MF) based methods (e.g., Singular Value Decomposition, SVD [26]) aimed to decompose the above interaction matrix into two low-dimensional latent matrices, namely a user matrix and item matrix, to represent the characteristics of users and items, respectively. However, due to the high computational complexity and general overfitting of such methods, gradient descent-based methods have become the mainstream of MF. Among these, MF-BPR [24] utilizes the Bayesian Personalized Ranking (BPR) criterion and stochastic gradient descent (SGD) method to optimize an MF model from constructed pairwise training data. Unlike MF-BPR, GMF and NeuMF [12] combine linear MF and non-linear components (i.e., a sigmoid activation function in GMF and multi-layer perceptron in NeuMF) to increase the expression ability of basic MF. Such methods must provide positive and negative samples as training data for the model. However, some studies, such as ENMF [1], claim that their models can be optimized efficiently without negative sampling.\n\n(2) Network embedding methods. Network embedding methods embed large-scale networks into low-dimensional vector spaces, with each network node represented by a feature vector. Such methods model the user-item interaction data as a graph structure, so as to inject the high-order collaborative signals in the interaction data into the embeddings of users and items. For example, inspired by Word2vec [21], DeepWalk [22] learns the node embeddings by sampling context nodes for each vertex through random walks and maximizing the log likelihood between the given vertex and the sampled nodes. Meanwhile, by adjusting the weights of random walks, Node2vec [8] makes the node embeddings better reflect the homophily and structural equivalence between nodes. Unlike Deep-Walk and Node2vec, which both sample positive samples through random walks, LINE [27] optimizes its objective function of firstand second-order proximities through an edge-sampling algorithm.\n\n(3) GCN-based methods. Like network embedding methods, GCN-based methods view the interaction data as a bipartite graph, and integrates the collaborative signals into the structural information of the nodes on the graph. Then, in order to capture CF signals between nodes along the graph structure, the messagepassing mechanism of the GCN involves multiple-layer embedding propagation. NGCF [28] proposes to inject the collaborative signals into the embedding propagation process by modeling of highorder connectivity in the user-item graph. However, LightGCN [11] demonstrates that NGCF is a heavy, burdensome GCN model for CF due to the feature transformation and nonlinear activation of the original GCN. DGCF [29] emphasizes there may be multiple reasons behind a specific user-item interaction and consequently generates disentangled embeddings for users and items. Given that existing studies focus on the design of more powerful interaction encoders, SimpleX [19] highlights the importance of loss functions and negative sampling in CF, and accordingly proposes a cosine contrastive loss. UltraGCN [20] analyzes the training efficiency of LightGCN and finds that message-passing largely slows down the convergence of model training, which allows researchers to skip infinite layers of explicit message-passing to improve the effectiveness and efficiency of GCN-based CF recommendations. Moreover, recent years have witnessed the great successes with other aspects of GCN-based CF methods, such as negative sampling technology in [14,[32][33][34]].\n\n\nFormulation of GCN-based CF\n\nIn general, the training process of a GCN-based CF model consists of three main elements, i.e., loss function, sampling strategy and embedding propagation.\n\n(1) Loss Function. Many effective loss functions have been applied to the learning processes of CF models, including BPR [24], PHL [12] and CLL [17]. Leaving the exploration of more powerful loss functions to future work, we choose the simple but effective BPR loss function, widely used in GCN-based CF models [11,28,29], to optimize our model. The objective function of BPR is as follows:\nL = ( , , ) \u2208 O \u2212ln (\u02c6 \u2212\u02c6 ) + \u0398 2 ,(1)\nwhere O = {( , , )|( , ) \u2208 R + , ( , ) \u2208 R \u2212 } represents the pairwise training data, R + indicates the observed interactions, and R \u2212 is the unobserved interactions; (\u00b7) is the sigmoid function; \u0398 denotes the trainable parameters of the GCN model, and controls the 2 regularization coefficient to prevent overfitting.\n\n(2) Sampling Strategy. BPR loss function assumes that users prefer observed items that better reflect their preferences to unobserved ones. Obviously, pairwise training data obtained from both negative and positive sampling greatly affects the final performance of a GCN-based CF model. Many competitive negative sampling strategies [4,14,34,35] seek to improve the training of GCN-based CF. However, they only view users' interaction items as positive samples, which may cause suboptimal performances. Therefore, the main work of this paper is to deeply investigate well-designed positive sampling strategies to promote GCN-based CF.\n\n(3) Embedding Propagation. Among the three CF methods mentioned in Section 2.1, the biggest difference between GCN-based methods and the other two approaches is the embedding propagation mechanism, the key reason why GCN works [16]. In fact, a GCN model without embedding propagation component can be reconstructed into MF and network embedding models. Moreover, from the perspective of sampling, embedding propagation actually implicitly regularizes the positive sampling distribution through graph Laplacian local smoothing [33]. These findings inspire us to further enhance the positive sampling strategy instead of only selecting positive samples from interaction items.\n\n\nITSM-GCN\n\nIn this section, we describe the proposed ITSM-GCN framework in detail. As illustrated in Figure 1, the simple but effective typical GCN-based CF model LightGCN [11] provides our default basic model, and the layer number of embedding propagation is set to 4 based on the results reported in the original paper [11].\n\n\nArchitecture of Basic Model\n\nThe architecture of the basic GCN-based CF model is illustrated in the lower half of Figure 1. It consists of three aspects, i.e., embedding initialization, embedding propagation and model prediction.\n\n(1) Embedding Initialization. Early CF methods used the row (column) vectors of interaction matrix , described in Section 2, as the user (item) feature vectors, creating a severe data sparsity problem. Drawing on the concept of the latent vectors of MF, the representation of a user (an item ) as a low-dimensional, dense embedding vector ( ) has gradually become the mainstream method in recommendation systems. Embedding vectors for all users and items can be built as an embedding matrix as follows:\n(0) = [ 1 , 2 , \u00b7 \u00b7 \u00b7 , , 1 , 2 , \u00b7 \u00b7 \u00b7 , ].(2)\nLike those based on MF, GCN-based methods also view the initial embedding matrix (0) as the training parameters of a recommendation model and optimize it with a corresponding loss function and sampling strategy. Nevertheless, MF-based methods directly calculate users' preference scores for items after obtaining the input of the embedding matrix. In contrast, GCN methods additionally perform multiple-layer embedding propagation based on the observed interaction item sets of all users to reshape the embedding matrix before feeding it into the model prediction component. This helps to transmit more collaborative signals between users and items, since the embedding propagation can successfully model the high-order connectivity between nodes.\n\n(2) Embedding Propagation. In general, two items that have been interacted by the same user may show some similarity; analogously, two users who have interacted with the same item often have similar behavioral tendency. Second-order connectivity in a user-item bipartite graph connected by interaction relationships can naturally and explicitly model this phenomenon. Furthermore, high-order connectivity in an interaction graph probably facilitates the capture of richer collaborative signals between users and items.\n\nAs mentioned above, the embedding vectors of users and items represent their features. Therefore, to transmit more feature information and collaborative signals, GCN explicitly performs a multiple-layer embedding propagating operation for all nodes. Taking LightGCN, which is adopted in this paper, as an example, given the -th layer's embedding vectors, we can calculate the ( +1)-th layer's embeddings as follows:\n( +1) = \u2208 N 1 |N | |N | ( ) ,(3)( +1) = \u2208 N 1 |N | |N | ( ) ,(4)\nwhere N and N represent the neighbors of user and item , respectively, and\n1 | N | | N |\nis the symmetric normalization. Note that LightGCN has empirically proved that symmetric normalization is superior to other normalization choices, such as 1 normalization, so we retain this default setting.  (3) Model Prediction. -layer embedding propagation will generate new embedding matrices with the same size as (0) . We then combine these embedding matrices obtained at each layer with the initial (0) to form the final representation vectors for all users and items. Experiments show that averaging all the embedding matrices is a simple but effective way to overcome the over-smooth problem, combine different semantics information at different layers, and retain the self-connection information.\n2 =1 3 4 ( \u2212 ) ( \u2212 ) ( \u2212 ) = 2 = 3 = 4 \u2026 ( ) \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 () ( ) ( ) ( ) (\nAfter being reshaped by embedding propagation and embedding combination, each node vector (e.g., node 1 as shown in Figure 1, which can be a user or an item ) captures rich collaborative signals and carries the feature information of both its adjacent neighbors and high-order neighbors. Finally, we use the inner product of the final embedding vectors of a user and an item to define their similarity, i.e., the user's predicted preference score for the item:\n= .(5)\n\nSampling Strategy\n\nIn this section, we elaborate the four components of our proposed informative training sample mining (ITSM) strategy, i.e., the improved dynamic negative sampler, the conventional positive sampler and the two novel positive samplers.\n\n(1) Improved Dynamic Negative Sampler. In CF implicit data, users only interact with a few items, thus leaving a large number of unobserved items. Therefore, the effectiveness and efficiency of a CF model largely depends on its negative sampling strategy.\n\nDynamic negative sampling (DNS) has proved that an effective negative sampling method can greatly improve the efficiency and performance of a CF model. The default setting of DNS is to select the item with the highest perference score (predicted by Equation (5)) as the hard negative sample from randomly sampled negative instances, i.e., the highest-scored item shown in Figure 2. However, this may be problematic because the highest-scored item is likely to be a potentially positive sample (i.e., it likely suffers from the false negative sample problem). Therefore, our improved dynamic negative sampler refuses to directly select the highest-scored item as the final hard negative sample. Instead, it finds the best negative sampling scheme / 1 by carefully tuning the number of negative samples 1 and the ranking position of the hard negative sample in the ranking diagram. Taking the Gowalla dataset as an example, the best sampling scheme, 2/200, means choosing the item with the 2nd highest score among 200 sampled negative items as the final negative training instance. In addition to this simple but effective negative sampling strategy, we pay more attention to the design of positive sampling strategies, as described below.\n\n(2) Conventional Positive Sampler. Users often interact with items according to their own interests. Therefore, the basic idea of CF is to find and leverage users' preferences to provide accurate personalized recommendation for them. Based on this, previous studies [11,14,28] directly extract positive training samples from users' positive feedback data (the observed items in Figure 1), which is a reasonable approach. The conventional positive sampler in this paper naturally inherits this setting, thus deducing the BPR loss function, as shown in Section 2.2.\n\nHowever, this default setting may often suffer from the suboptimal solution problem, given that there may in fact be items that users do not see but would be interested in. To tackle this issue, we propose a similarity-based positive sampler (as described below) to mine such potentially positive samples for model training.\n\nNotably, we can equip the basic model LightGCN with the traditional positive sampler and our improved dynamic negative sampler to obtain a simple GCN-based CF baseline, called ITSM-GCN N .\n\n(3) Similarity-based Positive Sampler. As we all know, observed interaction items directly reflect the interests of users. This inspires us to mine more potentially positive samples from items similar to interaction items; this is the core idea of our similaritybased positive sampler. Before performing similarity-based positive sampling, we need to construct the augmented positive sample set N \u2212si for each user and the augmented pairwise training data set O si . Formally, the augmented set N \u2212si of user is\nN \u2212si = \u2208 N { si ( , I)},(6)\nwhere N denotes the set of items that are interacted by user , and represents an item in N . si ( , I) indicates the si items with the highest similarity to item in I (the set of all items).\n\nThe essential question to determine N \u2212si is how to calculate the similarity between the positive interaction sample and all other items. A recent study of message-passing mechanism for GCNbased CF model simplification has formally derived the similarity , between two items (i.e., item and ) [20]:\n= ,(7), = , \u2212 , , = , ,(8)\nwhere is the adjacency matrix of the interaction bipartite graph. represents the item-item co-occurrence graph, and each element , represents the co-occurrence times of items and in the bipartite graph. and denote the degrees of item and item in , respectively.\n\nWith the item-item similarity calculation formula, we can determine the si items most similar to each positive item . By traversing all the positive interaction samples of user , we can construct this user's augmented positive sample set, N \u2212si . The augmented positive sample sets of all users can be obtained similarly. Finally, corresponding to the original pairwise training data O, the augmented pairwise training data O si can be generated based on the augmented positive items of all users.\n\nGiven the augmented positive samples of all users, when performing positive sampling for model optimization, we have two options: sampling from the positive interaction sample set or from the augmented positive sample set. However, while augmented positive samples are items that users may be interested in, positive interaction samples directly reflect users' preferences. Therefore, positive sampling should more often be carried out on positive interaction sample set. To this end, the sampling probability si is designed to control the probability of sampling from augmented positive sample set. For example, si = 0.2 means that the positive training instances are sampled from the augmented positive sample set with a probability of 0.2, and from the positive interaction sample set with a probability of 0.8.\n\nConsidering both the traditional positive sampler and the similaritybased positive sampler, one can rewrite the BPR loss function for GCN-based CF as follows:\nL \u2212si = ( , , ) \u2208O ( , si , ) \u2208O si \u2212ln ((1 \u2212 si )\u02c6 + si\u02c6 si \u2212\u02c6 ), (9) si = 1, rand(0, 1) \u2264 si , 0, rand(0, 1) > si ,(10)\nwhere si is a potentially positive item sampled from the augmented positive sample set of user . Note that represents the hard negative sample generated by the improved dynamic negative sampler.\n\n(4) Score-based Positive Sampler. Since there may exist unobserved items that users actually are interested in, sampled negative items from unobserved items,especially those with high scores, are likely to contain potentially positive samples. Therefore, we propose a score-based positive sampler that can mine potentially positive instances from the chosen negative samples. Optimizing the difference between these potentially positive samples and hard negative samples could improve the model performance.\n\nSpecifically, when constructing a score-based training item pair, the hard negative sample obtained by the improved dynamic negative sampler will continue to be used as the negative training instance. Meanwhile, the positive training instance will be the highest-scored item (predicted by Equation (5)) of 2 negatives generated by the second independent negative sampling. Practically, we only perform negative sampling once to get ( 1 + 2 ) negative items, of which the first 1 is for the improved dynamic negative sampler and the last 2 is for the score-based positive sampler. Finally, we can derive the following score-based loss function:\nL sc = \u2212ln (\u02c6 sc \u2212\u02c6 ),(11)\nwhere sc represents a potentially positive instance in the second negative sampling and is the same as the hard negative sample in Equation (9).\n\n\nModel Optimization\n\nFinally, considering the average impact of L \u2212si and L sc , we derive the following training objective for ITSM-GCN:\nL = mean(L \u2212si + L sc ) + \u0398 2 .(12)\nWith this loss function, we use the Adam optimizer to update the training parameters of the basic GCN model, i.e., the initial embedding matrix (0) . Notably, our ITSM-GCN does not introduce more training parameters, thus sharing the same model complexity as the matrix factorization and network embedding models.  users and pre-construct the augmented pairwise training data set before model training. Hence, the time cost mainly comes from the improved dynamic negative sampler and score-based positive sampler. For the former, the computational complexity of the preference score prediction of negative samples is O( 1 ), where 1 is the number of sampled negative items, and is the embedding dimension. Analogously, the complexity of the score-based positive sampler is O( 2 ). Therefore, the time complexity of ITSM is O(( 1 + 2 ) ), which lies in the same level with DNS's (O( )).\n\n\nDiscussion\n\n\nRelations to Other Models.\n\nIn this part, we discuss the relations between our ITSM-GCN and some other existing models. Relation to LightGCN. If the sampling strategy ITSM is replaced by BPR, ITSM-GCN is completely equivalent to LightGCN [11]. In contrast to LightGCN, however, ITSM-GCN can mine more informative training samples to help the GCN models learn. Generally speaking, our ITSM sampling strategy is model-agnostic and can be integrated into any GCN-based CF method.\n\nRelation to IPGAN. Some ideas in IPGAN [9] greatly align with ours. For example, both methods believe that high quality training samples contribute greatly to the CF performance. When designing a sampling strategy, we both consider the limitation of sampling positive instances from only positive interaction items, as well as the false negative instance problem in negative sampling. However, the complex design of the IPGAN's sampling strategy leads to its high time complexity, which is higher than O( ), where denotes the total number of all items and is much greater than ( 1 + 2 ) (refer to [9] for more details). In addition, instead of GAN, we choose a GCN as the basic model to leverage fully high-order collaborative signals hidden in the interaction data.\n\nRelation to data augmentation-based GCN models. For the purpose of data augmentation, SGL-ED [31] uses self-supervised learning to construct supervision signals from the original input data (i.e., a user-item interaction graph). NCL [17] explicitly incorporates potential neighbors, including structural and semantic ones, into contrastive pairs to capture potential neighborhood relationships between users and items. Actually, from the perspective of data augmentation, the similarity-based positive sampler uses information about structural neighbors, while the score-based positive sampler is similar to the semantic neighbor mining in NCL. However, unlike the above methods, ITSM-GCN naturally integrates data augmentation into the sampling strategy, which is a simple but effective way that introduces no additional training tasks.\n\n\nEXPERIMENTS\n\nIn this section, we first compare ITSM-GCN to various types of state-of-the-art CF baseline methods to demonstrate its effectiveness. Then, we perform ablation study to prove the rationality and effectiveness of the samplers we designed for ITSM-GCN. Finally, we further report detailed analysis results.\n\n\nExperimental Setup\n\nDatasets. To reduce the experiment workload and keep the comparisons fair, we closely follow the settings of many recent GCN-based CF studies [11,19,20,28]. As do these studies, we use three publicly available datasets (Amazon-Books, Yelp2018, Gowalla) and the same data split to conduct our experiments. Table 1 summarizes detailed statistics about the chosen datasets. Evaluation Metrics. For the evaluation protocol, we choose the commonly used Recall@20 and NDCG@20 as metrics. After recalling the top 20 highest-scored items for each user from all those items they have not interacted with, as well as calculating the corresponding evaluation metrics, we further report the average metrics results over all users. Compared Models. We compare our ITSM-GCN with the following state-of-the-art baseline models:\n\n\u2022 MF-based methods: MF-BPR [24], ENMF [1].\n\n\u2022 Network embedding methods: DeepWalk [22], LINE [27], Node2vec [8].\n\n\u2022 GCN-based methods: NGCF [28], DGCF [29], LightGCN [11], UltraGCN [20], SimpleX [19], IA-GCN [36]. Implementation Details. We implement our ITSM-GCN on top of the official code of LightGCN, based on PyTorch. Specifically, we combine LightGCN with our designed samplers in a pluggable way to study the proposed sampling strategy.\n\nAs with LightGCN, we fix the embedding dimension at 64, use the Xavier initializer [7] to initialize the embedding matrix and optimize ITSM-GCN with the Adam optimizer. We use the default mini-batch size of 2048, GCN layer number of 4 and a learning rate of 0.001. The best values of the 2 regularization coefficient on Amazon-Books, Yelp2018 and Gowalla are 10 \u22125 , 10 \u22123 and 10 \u22124 , respectively.\n\nIn addition, we tune the negative sampling scheme / 1 and the number of negative items 2 in the second negative sampling for each specific dataset. We also study different numbers of similaritybased positive items si in {1, 2, \u00b7 \u00b7 \u00b7 , 10} and its sampling probability si in {0.1, 0.2, \u00b7 \u00b7 \u00b7 , 0.8}. Finally, we report the impact of the above parameters in detail in Sections 4.4. Table 2 shows the performance comparison between the proposed ITSM-GCN and other baseline methods on three datasets under the same evaluation protocol. The results lead to the following observations:\n\n\nPerformance Comparison\n\n1) ITSM-GCN consistently achieves the best performance on almost all datasets. Specifically, it overwhelmingly beats all MFbased methods and network embedding methods on all datasets. For example, compared with MF, ENMF and DeepWalk, ITSM-GCN improves performance by 31.5%, 15.7%, and 51.7%, respectively, on Recall@20 for Yelp2018. Additionally, our ITSM-GCN is also superior to all other GCN-based baselines, except for UltraGCN on Amazon-Books, which will be analyzed more deeply below.\n\n2) Overall, ITSM-GCN N achieves performance competitive with most models. When equipped with a traditional positive sampler Table 2: Performance comparison with different CF baselines. To facilitate comparison, we highlight the top 5 best results in each column. RI stands for relative improvement of ITSM-GCN to other baselines. For convenience and consistency, we duplicate the best experimental results from existing papers.\n\n\nModel\n\nAmazon-Books Yelp2018 Gowalla Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 and our improved dynamic negative sampler, ITSM-GCN N attains excellent performance, being the 3rd, 2rd, and 3rd best model for Amazon-Books, Yelp2018 and Gowalla, respectively (leaving out ITSM-GCN). The ablation experiment in Section 4.3 further shows that ITSM-GCN N greatly improves not only performance but also efficiency, as compared to the original LightGCN. Such results strongly suggest that informative training sample mining can make a significant difference and should be carefully studied.\n\n3) Almost all of the GCN-based baselines perform better than the MF-based and network embedding methods. We attribute this to the unique embedding propagation of GCN, which is the core distinction between GCN-based CF models and MF-based and network embedding methods. We argue that the success of the embedding propagation mechanism lies in its ability to explicitly utilize the high-order connection similarity. This inspires us to mine more useful connection information by the similarity-based positive sampler to enhance model training. 4) Our ITSM-GCN performs better than the original Light-GCN by 58.9%, 11.2% and 3.6% on Recall@20 for Amazon-Books, Yelp2018 and Gowalla, respectively. In fact, other improved methods based on LightGCN, such as DGCF and IA-GCN, also achieve somewhat superior performance to LightGCN. However, our proposed LightGCN-based ITSM-GCN still consistently and significantly outperforms them across datasets, suggesting that our informative training sample mining strategy can substantially improve upon the basic GCN-based CF model. Therefore, we call for further and more comprehensive future research about sampling strategies. 5) When compared with the two strongest baselines, ITSM-GCN and SimpleX outperform UltraGCN on both Yelp2018 and Gowalla, but UltraGCN performs much better on Amazon-Books. A possible reason for this is that the loss functions and sampling strategies designed in ITSM-GCN and SimpleX better allow the model to capture the collaborative signals on Yelp2018 and Gowalla datasets. Conversely, the improved embedding propagation of UltraGCN may better allow the model to fit the interaction data of Amazon-Books. A similar phenomenon can be found when comparing MFbased methods with network embedding methods. Moreover, when applying ITSM to LightGCN on Amazon-Books, we actually remove the score-based positive sampler since we find it hurts the model performance. We leave these interesting findings about Amazon-Books for a more in-depth future study.\n\n\nAblation Study\n\nWe perform ablation studies of ITSM-GCN on the Yelp2018 and Gowalla datasets to demonstrate the rationality and effectiveness of our designs, including the improved negative sampler, similaritybased positive sampler and score-based positive sampler. Specifically, we consider the following variants: Impact of improved dynamic negative sampler. We plot the training curves for ITSM-GCN N and LightGCN in Figure 3, and investigate the best negative sampling schemes of ITSM-GCN N for Yelp2018 and Gowalla in detail in Section 4.4. Note that although ITSM-GCN N soon suffers from overfitting problem, we also train 400 epochs to facilitate comparison and analysis. As Figure 3 shows, ITSM-GCN N achieves better performance than LightGCN in a shorter time, which demonstrates that informative negative training samples can greatly boost a GCN-based CF model. In addition, we can observe the curves of ITSM-GCN N start to decline rapidly after quickly reaching their peak values. This indicates that ITSM-GCN N may only ever be able to achieve suboptimal performance, even if it mines more informative negative samples. Therefore, potentially positive sample mining will likely further improve the model performance of ITSM-GCN N .\n\nImpact of similarity-based and score-based positive samplers. Our proposed ITSM-GCN leverages the potentially positive samples in two ways. To verify the effectiveness of each kind of potentially positive item sampler, we conduct ablation studies to analyze their contributions. The results are reported in Figure 4, which show that ITSM-GCN outperforms ITSM-GCN sc and ITSM-GCN si over two datasets and thus that the proposed scorebased and similarity-based positive samplers can benefit model performance in different ways. When compared with the baseline ITSM-GCN N on Yelp2018, ITSM-GCN sc and ITSM-GCN si achieve the same performance improvement. However, for the Gowalla dataset, ITSM-GCN si is much better than ITSM-GCN N , while ITSM-GCN sc is only slightly superior. This indicates that when applied in different datasets, the similarity-based positive sampler has a more stable improvement effect than the score-based positive sampler. \n\n\nParameter Analysis\n\nIn this section, we first investigate the impact of different negative sampling schemes / 1 on the performance of ITSM-GCN N . Then, we study the impact on ITSM-GCN of the number of similaritybased augmented positive samples si , the sampling probability si , and the number of negatives 2 in the second negative sampling.\n\n4.4.1 Impact of / 1 . We test the performance of ITSM-GCN N with different negative sampling schemes / 1 on Yelp2018 and Gowalla. We report the cases where is 1, 2 and 3, which is enough to help find the best negative sampling schemes for different datasets. From the results shown in Figure 5, we can observe that the performances rise first and then fall no matter whether the ranking position is 1, 2 or 3. For Yelp2018, although we can achieve the same best performance by setting the negative sampling scheme / 1 as 1/50 or 2/125, we argue that the scheme 2/125 is suboptimal since more negative samples will increase the time complexity of model training. For Gowalla, it is obvious that the negative sampling schemes with = 2 are much better than = 1, perhaps because it is easier to sample false negative instances from the negative sampling on Gowalla than on Yelp2018. Finally, based on the results in Figure 5, we choose 2/200 as the negative sampling scheme for Gowalla. The above findings show that carefully selecting appropriate negative sampling schemes for different datasets will greatly improve model performance and training efficiency.\n\n4.4.2 Impact of si and si . We investigate the impact of different si and psi on the performance of ITSM-GCN on the Yelp2018 dataset. For the impact of si , we empirically find that if si is greater than 10, the model performance begins to decline gradually (detailed results are omitted due to space limitations); when si takes a value from 1 to 10, the model attains the best performance. Therefore, the best si is somewhere in the range of {1, 2, \u00b7 \u00b7 \u00b7 , 10}. The detailed results are shown in Figure 6(a), and indicate that ITSM-GCN achieves the best performance when si is 6. Furthermore, we test the performance of ITSM-GCN with different sampling probabilities si in the range {0.1, 0.2, \u00b7 \u00b7 \u00b7 , 0.8} and plot the experimental results in Figure 6(b). We can find that when si increases from 0.1 to 0.8, the performance follows a trend of first increasing and then decreasing. This is because, when si is too small, the model obtains less training information from the augmented potentially positive samples. When si is too large, on the other hand, positive interaction samples are not sufficiently exploited, which must hurt model performance.\n\n\n4.4.\n\n3 Impact of 2 . We suppose that in the score-based loss function L sc , the preference score of a potentially positive sample sc should be higher than that of a hard negative sample, so as to provide more useful information for model training. Therefore, we search for the best setting of 2 near 1 ( 1 /2 for Gowalla) for ITSM-GCN. The results appear in Figure 7. As the figure shows, a 2 that is too small or too large may weaken the performance of ITSM-GCN; a 2 slightly larger than 1 ( 1 /2 for Gowalla) often provides the best number of negative items for the second negative sampling. These findings empirically demonstrate the rationality and effectiveness of our hypothesis and guide us to construct an effective score-based loss function.  \n\n\nCompatibility Analysis\n\nSince the proposed ITSM sampling strategy is model agnostic, it should bring considerable performance improvements for more GCN-based CF models. In this part, we further set the basic model in Figure 1 as NGCF and SGL-ED, respectively, and test their performances to valid the compatibility of the ITSM strategy. The results are shown in Table 3.\n\nFrom the table, we can observe that the proposed ITSM strategy can consistently improve the performances of NGCF, Light-GCN, and SGL-ED, verifying its effectiveness and compatibility. Besides, the improvements on NGCF and LightGCN are much more  remarkable than that on SGL-ED. The reason may be that the selfsupervised learning tasks of SGL-ED limit the effectiveness of the ITSM strategy (SGL-ED can be seen as a combination of LightGCN and self-supervised learning tasks).\n\n\nCONCLUSION\n\nIn this work, we propose a GCN-based CF training framework (ITSM-GCN) equipped with a novel sampling strategy (ITSM) to mine informative training samples for the learning of GCN models. Specifically, we pay the same degree of attention to the critical impacts of both negative and positive sampling on the model performance. First, we improve the DNS strategy by proposing to carefully tune its parameters to obtain high-quality hard negative samples. Then, to mine more informative potentially positive samples, we make full use of items similar to the interaction items and of the false negative samples in the second negative sampling through our similarity-based and score-based positive samplers. Finally, extensive experiments on three public datasets demonstrate the effectiveness of the proposed ITSM-GCN.\n\n\nACKNOWLEDGMENTS\n\nThis work was supported by the National Key Research and Development Program of China (No. 2020YFB1712203).\n\nFigure 1 :\n1Overview of our proposed ITSM-GCN framework. The lower half is a typical GCN-based CF model, and the upper half is our informative training sample mining component, as applied to that model.\n\nFigure 2 :\n2Ranking diagram of 1 sampled negative instances. Original DNS defaults to selecting the highest-scored item as the final hard negative sample, which may induce the false negative sample problem; our improved dynamic negative sampler optimizes the negative sampling scheme by carefully tuning both the number of negative samples 1 and the ranking position of the hard negative sample.\n\n\n3.4.1 Complexity Analysis. In our ITSM sampling strategy, the time complexity of the similarity-based positive sampler is O(1), which is equivalent to the conventional positive sampler's, since we can pre-calculate the augmented positive sample sets of all\n\n\u2022\nITSM-GCN N : As described in Section 3.2, ITSM-GCN N is the basic model of LightGCN equipped with a traditional positive sampler and our improved dynamic negative sampler. \u2022 ITSM-GCN si : This variant removes the score-based positive sampler from ITSM-GCN, i.e., it combines ITSM-GCN N and the similarity-based positive sampler. \u2022 ITSM-GCN sc : In this variant, we remove the similaritybased positive sampler from ITSM-GCN to study the impact of the score-based positive sampler.\n\nFigure 3 :Figure 4 :\n34Training curves of ITSM-GCN N and LightGCN, which are evaluated by testing Recall@20 per 10 epochs on Yelp2018 and Gowalla. Performance of ITSM-GCN and its variants on Yelp2018 and Gowalla.\n\nFigure 5 :Figure 6 :\n56Performance comparison with different negative sampling schemes / 1 . Performance comparison with different si and si on Yelp2018.\n\nFigure 7 :\n7Performance comparison with different 2 .\n\n\n)Observed items \nUnobserved items \n\nSimilarity-based \nPositive Sampler \n\nConventional \nPositive Sampler \n\nImproved Dynamic \nNegative Sampler \n\nScore-based \nPositive Sampler \n\nBPR Loss \nFunction \n\nParameter \nUpdate \n\nBasic Model \n\nSampling \nStrategy \n\nModel Optimization \n\nNormalized Sum \n\nEmbedding Propagation \nEmbedding \nInitialization \n\nModel Prediction \n\n+ \n\nPreference Score \nCalculation \n\nEmbedding \nCombination \n\n\n\nTable 1 :\n1Statistics of the datasets.Dataset \n#Users #Items #Interactions Density \n\nAmazon-Books 52,643 91,599 \n2,984,108 \n0.062% \nYelp2018 \n31,668 38,048 \n1,561,406 \n0.130% \nGowalla \n29,858 40,981 \n1,027,370 \n0.084% \n\n\n\nTable 3 :\n3Performance comparison w.r.t. different GCN-based CF models. +ITSM-GCN 0.0617 (+79.4%) 0.0489 (+85.9%) 0.0650 (+12.3%) 0.0532 (+11.5%) +ITSM-GCN 0.0653 (+58.9%) 0.0519 (+64.8%) 0.0722 (+11.2%) 0.0593 (+11.9%) +ITSM-GCN 0.0620 (+29.7%) 0.0490 (+29.3%) 0.0698 (+3.4%) 0.0572 (+3.1%)Method \nAmazon-Books \nYelp2018 \nRecall@20 \nNDCG@20 \nRecall@20 \nNDCG@20 \n\nNGCF \n0.0344 \n0.0263 \n0.0579 \n0.0477 \nLightGCN \n0.0411 \n0.0315 \n0.0649 \n0.0530 \nSGL-ED \n0.0478 \n0.0379 \n0.0675 \n0.0555 \n\n\nEfficient Neural Matrix Factorization without Sampling for Recommendation. Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, Shaoping Ma, ACM Transactions on Information Systems. 38TOIS)Chong Chen, Min Zhang, Yongfeng Zhang, Yiqun Liu, and Shaoping Ma. 2020. Efficient Neural Matrix Factorization without Sampling for Recommendation. ACM Transactions on Information Systems (TOIS) 38, 2 (2020), 1-28.\n\nRevisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. Lei Chen, Le Wu, Richang Hong, Kun Zhang, Meng Wang, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 27-34.\n\nDeep Neural Networks for YouTube Recommendations. Paul Covington, Jay Adams, Emre Sargin, Proceedings of the 10th ACM Conference on Recommender Systems (RecSys). the 10th ACM Conference on Recommender Systems (RecSys)Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys). 191-198.\n\nReinforced Negative Sampling for Recommendation with Exposure Data. Jingtao Ding, Yuhan Quan, Xiangnan He, Yong Li, Depeng Jin, Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI. the 28th International Joint Conference on Artificial Intelligence (IJCAIJingtao Ding, Yuhan Quan, Xiangnan He, Yong Li, and Depeng Jin. 2019. Rein- forced Negative Sampling for Recommendation with Exposure Data. In Proceed- ings of the 28th International Joint Conference on Artificial Intelligence (IJCAI). 2230-2236.\n\nSimplify and Robustify Negative Sampling for Implicit Collaborative Filtering. Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, Depeng Jin, Advances in Neural Information Processing Systems (NeurIPS. Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, and Depeng Jin. 2020. Sim- plify and Robustify Negative Sampling for Implicit Collaborative Filtering. In Advances in Neural Information Processing Systems (NeurIPS). 1094-1105.\n\nDeeply Fusing Reviews and Contents for Cold Start Users in Cross-domain Recommendation Systems. Wenjing Fu, Zhaohui Peng, Senzhang Wang, Yang Xu, Jin Li, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)Wenjing Fu, Zhaohui Peng, Senzhang Wang, Yang Xu, and Jin Li. 2019. Deeply Fusing Reviews and Contents for Cold Start Users in Cross-domain Recommen- dation Systems. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 94-101.\n\nUnderstanding the Difficulty of Training Deep Feedforward Neural Networks. Xavier Glorot, Yoshua Bengio, Proceedings of the 13rd International Conference on Artificial Intelligence and Statistics (AISTATS). the 13rd International Conference on Artificial Intelligence and Statistics (AISTATS)Xavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In Proceedings of the 13rd International Conference on Artificial Intelligence and Statistics (AISTATS). 249-256.\n\nNode2vec: Scalable Feature Learning for Networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD. the 22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDDAditya Grover and Jure Leskovec. 2016. Node2vec: Scalable Feature Learning for Networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). 855-864.\n\nGuibing Guo, Huan Zhou, Bowei Chen, Zhirong Liu, Xiao Xu, Xu Chen, Zhenhua Dong, and Xiuqiang He. 2022. IPGAN: Generating Informative Item Pairs by Adversarial Sampling. IEEE Transactions on Neural Networks and Learning Systems (TNNLS). 33Guibing Guo, Huan Zhou, Bowei Chen, Zhirong Liu, Xiao Xu, Xu Chen, Zhenhua Dong, and Xiuqiang He. 2022. IPGAN: Generating Informative Item Pairs by Adversarial Sampling. IEEE Transactions on Neural Networks and Learning Systems (TNNLS) 33, 2 (2022), 694-706.\n\nPersonalized Recommendation of Social Software Items Based on Social Relations. Ido Guy, Naama Zwerdling, David Carmel, Inbal Ronen, Erel Uziel, Sivan Yogev, Shila Ofek-Koifman, Proceedings of the 3rd ACM Conference on Recommender Systems (RecSys). the 3rd ACM Conference on Recommender Systems (RecSys)Ido Guy, Naama Zwerdling, David Carmel, Inbal Ronen, Erel Uziel, Sivan Yogev, and Shila Ofek-Koifman. 2009. Personalized Recommendation of Social Software Items Based on Social Relations. In Proceedings of the 3rd ACM Conference on Recommender Systems (RecSys). 53-60.\n\nLightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIRXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 639-648.\n\nNeural Collaborative Filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebWWWXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web (WWW). 173-182.\n\nCollaborative Metric Learning. Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, Deborah Estrin, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebWWWCheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and Deborah Estrin. 2017. Collaborative Metric Learning. In Proceedings of the 26th International Conference on World Wide Web (WWW). 193-201.\n\nMixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems. Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, Jie Tang, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD. the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDDTinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021. MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD). 665-674.\n\nMatrix Factorization Techniques for Recommender Systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Tech- niques for Recommender Systems. Computer 42, 8 (2009), 30-37.\n\nDeeper Insights into Graph Convolutional Networks for Semi-supervised Learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI. the AAAI Conference on Artificial Intelligence (AAAIQimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper Insights into Graph Convolutional Networks for Semi-supervised Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 3538-3545.\n\nImproving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning. Zihan Lin, Changxin Tian, Yupeng Hou, Wayne Xin Zhao, Proceedings of the 31st International Conference on World Wide Web. the 31st International Conference on World Wide WebWWWZihan Lin, Changxin Tian, Yupeng Hou, and Wayne Xin Zhao. 2022. Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning. In Proceedings of the 31st International Conference on World Wide Web (WWW). 2320-2329.\n\nInterestaware Message-Passing GCN for Recommendation. Fan Liu, Zhiyong Cheng, Lei Zhu, Zan Gao, Liqiang Nie, Proceedings of the 30th International Conference on World Wide Web. the 30th International Conference on World Wide WebWWWFan Liu, Zhiyong Cheng, Lei Zhu, Zan Gao, and Liqiang Nie. 2021. Interest- aware Message-Passing GCN for Recommendation. In Proceedings of the 30th International Conference on World Wide Web (WWW). 1296-1305.\n\nSimpleX: A Simple and Strong Baseline for Collaborative Filtering. Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao, Xiuqiang He, Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM. the 30th ACM International Conference on Information & Knowledge Management (CIKMKelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao, and Xiuqiang He. 2021. SimpleX: A Simple and Strong Baseline for Collaborative Filtering. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM). 1243-1252.\n\nUltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, Xiuqiang He, Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM). the 30th ACM International Conference on Information & Knowledge Management (CIKM)Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (CIKM). 1253-1262.\n\nDistributed Representations of Words and Phrases and Their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in Neural Information Processing Systems (NeurIPS). Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In Advances in Neural Information Processing Systems (NeurIPS). 3111-3119.\n\nDeepwalk: Online Learning of Social Representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD. the 20th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDDBryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online Learn- ing of Social Representations. In Proceedings of the 20th ACM SIGKDD Interna- tional Conference on Knowledge Discovery & Data Mining (KDD). 701-710.\n\nRelationaware Graph Attention Model with Adaptive Self-adversarial Training. Xiao Qin, Nasrullah Sheikh, Berthold Reinwald, Lingfei Wu, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI. the AAAI Conference on Artificial Intelligence (AAAIXiao Qin, Nasrullah Sheikh, Berthold Reinwald, and Lingfei Wu. 2021. Relation- aware Graph Attention Model with Adaptive Self-adversarial Training. In Pro- ceedings of the AAAI Conference on Artificial Intelligence (AAAI). 9368-9376.\n\nBPR: Bayesian Personalized Ranking from Implicit Feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI. the 25th Conference on Uncertainty in Artificial Intelligence (UAISteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI). 452-461.\n\nItem-based Collaborative Filtering Recommendation Algorithms. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Proceedings of the 10th International Conference on World Wide Web. the 10th International Conference on World Wide WebWWWBadrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item-based Collaborative Filtering Recommendation Algorithms. In Proceedings of the 10th International Conference on World Wide Web (WWW). 285-295.\n\nIncremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, Proceedings of the 5th International Conference on Computer and Information Science. the 5th International Conference on Computer and Information ScienceBadrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2002. Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems. In Proceedings of the 5th International Conference on Computer and Information Science. 27-8.\n\nLINE: Large-scale Information Network Embedding. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei, Proceedings of the 24th International Conference on World Wide Web. the 24th International Conference on World Wide WebWWWJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. LINE: Large-scale Information Network Embedding. In Proceedings of the 24th International Conference on World Wide Web (WWW). 1067-1077.\n\nNeural Graph Collaborative Filtering. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR. the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIRXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 165-174.\n\nDisentangled Graph Collaborative Filtering. Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua, Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR. the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIRXiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua. 2020. Disentangled Graph Collaborative Filtering. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 1001-1010.\n\nCross-domain Recommendation for Cold-start Users via Neighborhood Based Feature Mapping. Xinghua Wang, Zhaohui Peng, Senzhang Wang, S Philip, Wenjing Yu, Xiaoguang Fu, Hong, International Conference on Database Systems for Advanced Applications (DASFAA). Xinghua Wang, Zhaohui Peng, Senzhang Wang, Philip S Yu, Wenjing Fu, and Xiaoguang Hong. 2018. Cross-domain Recommendation for Cold-start Users via Neighborhood Based Feature Mapping. In International Conference on Database Systems for Advanced Applications (DASFAA). 158-165.\n\nSelf-supervised Graph Learning for Recommendation. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIRJiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, and Xing Xie. 2021. Self-supervised Graph Learning for Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 726-735.\n\nChenxiao Yang, Qitian Wu, Jipeng Jin, Xiaofeng Gao, Junwei Pan, Guihai Chen, arXiv:2204.117522022. Trading Hard Negatives and True Negatives: A Debiased Contrastive Collaborative Filtering Approach. arXiv preprintChenxiao Yang, Qitian Wu, Jipeng Jin, Xiaofeng Gao, Junwei Pan, and Guihai Chen. 2022. Trading Hard Negatives and True Negatives: A Debiased Contrastive Collaborative Filtering Approach. arXiv preprint arXiv:2204.11752 (2022).\n\nUnderstanding Negative Sampling in Graph Representation Learning. Zhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, Jie Tang, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDDZhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. 2020. Understanding Negative Sampling in Graph Representation Learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD). 1666-1676.\n\nRegion or Global? A Principle for Negative Sampling in Graph-based Recommendation. Zhen Yang, Ming Ding, Xu Zou, Jie Tang, Bin Xu, Chang Zhou, Hongxia Yang, 10.1109/TKDE.2022.3155155IEEE Transactions on Knowledge and Data Engineering. TKDEZhen Yang, Ming Ding, Xu Zou, Jie Tang, Bin Xu, Chang Zhou, and Hongxia Yang. 2022. Region or Global? A Principle for Negative Sampling in Graph-based Recommendation. IEEE Transactions on Knowledge and Data Engineering (TKDE) (2022), 1-1. https://doi.org/10.1109/TKDE.2022.3155155\n\nOptimizing Top-n Collaborative Filtering via Dynamic Negative Item Sampling. Weinan Zhang, Tianqi Chen, Jun Wang, Yong Yu, Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR. the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIRWeinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. 2013. Optimizing Top-n Collaborative Filtering via Dynamic Negative Item Sampling. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 785-788.\n\nYinan Zhang, Pei Wang, Xiwei Zhao, Hao Qi, arXiv:2204.03827Jie He, Junsheng Jin, Changping Peng, Zhangang Lin, and Jingping Shao. 2022. IA-GCN: Interactive Graph Convolutional Network for Recommendation. arXiv preprintYinan Zhang, Pei Wang, Xiwei Zhao, Hao Qi, Jie He, Junsheng Jin, Changping Peng, Zhangang Lin, and Jingping Shao. 2022. IA-GCN: Interactive Graph Convo- lutional Network for Recommendation. arXiv preprint arXiv:2204.03827 (2022).\n", "annotations": {"author": "[{\"end\":298,\"start\":286},{\"end\":309,\"start\":299}]", "publisher": "[{\"end\":231,\"start\":228},{\"end\":770,\"start\":767}]", "author_last_name": "[{\"end\":297,\"start\":290},{\"end\":308,\"start\":299}]", "author_first_name": "[{\"end\":287,\"start\":286},{\"end\":289,\"start\":288}]", "author_affiliation": null, "title": "[{\"end\":227,\"start\":1},{\"end\":536,\"start\":310}]", "venue": "[{\"end\":642,\"start\":538}]", "abstract": "[{\"end\":2297,\"start\":940}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2458,\"start\":2455},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2461,\"start\":2458},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2464,\"start\":2461},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2467,\"start\":2464},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2470,\"start\":2467},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2572,\"start\":2568},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2575,\"start\":2572},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2578,\"start\":2575},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2581,\"start\":2578},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2977,\"start\":2974},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2980,\"start\":2977},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2983,\"start\":2980},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3376,\"start\":3372},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3401,\"start\":3397},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3433,\"start\":3430},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3459,\"start\":3455},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3487,\"start\":3484},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3517,\"start\":3513},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3553,\"start\":3549},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3729,\"start\":3725},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3768,\"start\":3764},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3807,\"start\":3803},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3860,\"start\":3856},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4199,\"start\":4196},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4202,\"start\":4199},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4205,\"start\":4202},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4502,\"start\":4498},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4506,\"start\":4502},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4510,\"start\":4506},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4514,\"start\":4510},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4820,\"start\":4817},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4823,\"start\":4820},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4988,\"start\":4985},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4991,\"start\":4988},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4994,\"start\":4991},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4997,\"start\":4994},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8700,\"start\":8696},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9077,\"start\":9073},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9280,\"start\":9276},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9581,\"start\":9578},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10067,\"start\":10063},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10082,\"start\":10078},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10320,\"start\":10317},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10515,\"start\":10511},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11018,\"start\":11014},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11187,\"start\":11183},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11340,\"start\":11336},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11593,\"start\":11589},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11732,\"start\":11728},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12163,\"start\":12159},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12167,\"start\":12163},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12171,\"start\":12167},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12175,\"start\":12171},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12491,\"start\":12487},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12501,\"start\":12497},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12514,\"start\":12510},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12681,\"start\":12677},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12684,\"start\":12681},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12687,\"start\":12684},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13452,\"start\":13449},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13455,\"start\":13452},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13458,\"start\":13455},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13461,\"start\":13458},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13983,\"start\":13979},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14282,\"start\":14278},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14604,\"start\":14600},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14753,\"start\":14749},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20657,\"start\":20653},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20660,\"start\":20657},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20663,\"start\":20660},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22498,\"start\":22494},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27225,\"start\":27221},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27503,\"start\":27500},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28061,\"start\":28058},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28326,\"start\":28322},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28466,\"start\":28462},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29555,\"start\":29551},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29558,\"start\":29555},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29561,\"start\":29558},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29564,\"start\":29561},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30254,\"start\":30250},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30264,\"start\":30261},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30309,\"start\":30305},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30320,\"start\":30316},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30334,\"start\":30331},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30367,\"start\":30363},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30378,\"start\":30374},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30393,\"start\":30389},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30408,\"start\":30404},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30422,\"start\":30418},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30435,\"start\":30431},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30754,\"start\":30751}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42822,\"start\":42619},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43219,\"start\":42823},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43478,\"start\":43220},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43961,\"start\":43479},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44175,\"start\":43962},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44330,\"start\":44176},{\"attributes\":{\"id\":\"fig_8\"},\"end\":44385,\"start\":44331},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44808,\"start\":44386},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45030,\"start\":44809},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45516,\"start\":45031}]", "paragraph": "[{\"end\":2984,\"start\":2313},{\"end\":3182,\"start\":2986},{\"end\":4206,\"start\":3184},{\"end\":5224,\"start\":4208},{\"end\":6049,\"start\":5226},{\"end\":6821,\"start\":6051},{\"end\":6876,\"start\":6823},{\"end\":7555,\"start\":6878},{\"end\":8407,\"start\":7587},{\"end\":8585,\"start\":8432},{\"end\":9662,\"start\":8587},{\"end\":10621,\"start\":9664},{\"end\":12177,\"start\":10623},{\"end\":12364,\"start\":12209},{\"end\":12756,\"start\":12366},{\"end\":13114,\"start\":12796},{\"end\":13750,\"start\":13116},{\"end\":14426,\"start\":13752},{\"end\":14754,\"start\":14439},{\"end\":14986,\"start\":14786},{\"end\":15490,\"start\":14988},{\"end\":16286,\"start\":15539},{\"end\":16806,\"start\":16288},{\"end\":17223,\"start\":16808},{\"end\":17363,\"start\":17289},{\"end\":18083,\"start\":17378},{\"end\":18628,\"start\":18168},{\"end\":18889,\"start\":18656},{\"end\":19146,\"start\":18891},{\"end\":20385,\"start\":19148},{\"end\":20950,\"start\":20387},{\"end\":21276,\"start\":20952},{\"end\":21466,\"start\":21278},{\"end\":21979,\"start\":21468},{\"end\":22199,\"start\":22009},{\"end\":22499,\"start\":22201},{\"end\":22788,\"start\":22527},{\"end\":23287,\"start\":22790},{\"end\":24103,\"start\":23289},{\"end\":24263,\"start\":24105},{\"end\":24580,\"start\":24386},{\"end\":25089,\"start\":24582},{\"end\":25734,\"start\":25091},{\"end\":25906,\"start\":25762},{\"end\":26045,\"start\":25929},{\"end\":26967,\"start\":26082},{\"end\":27459,\"start\":27011},{\"end\":28227,\"start\":27461},{\"end\":29066,\"start\":28229},{\"end\":29386,\"start\":29082},{\"end\":30221,\"start\":29409},{\"end\":30265,\"start\":30223},{\"end\":30335,\"start\":30267},{\"end\":30666,\"start\":30337},{\"end\":31066,\"start\":30668},{\"end\":31647,\"start\":31068},{\"end\":32163,\"start\":31674},{\"end\":32592,\"start\":32165},{\"end\":33189,\"start\":32602},{\"end\":35206,\"start\":33191},{\"end\":36452,\"start\":35225},{\"end\":37400,\"start\":36454},{\"end\":37745,\"start\":37423},{\"end\":38903,\"start\":37747},{\"end\":40056,\"start\":38905},{\"end\":40813,\"start\":40065},{\"end\":41186,\"start\":40840},{\"end\":41663,\"start\":41188},{\"end\":42491,\"start\":41678},{\"end\":42618,\"start\":42511}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12795,\"start\":12757},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15538,\"start\":15491},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17256,\"start\":17224},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17288,\"start\":17256},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17377,\"start\":17364},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18167,\"start\":18084},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18635,\"start\":18629},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22008,\"start\":21980},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22506,\"start\":22500},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22526,\"start\":22506},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24385,\"start\":24264},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25761,\"start\":25735},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26081,\"start\":26046}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29721,\"start\":29714},{\"end\":31455,\"start\":31448},{\"end\":32296,\"start\":32289},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":41185,\"start\":41178}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2311,\"start\":2299},{\"attributes\":{\"n\":\"2\"},\"end\":7585,\"start\":7558},{\"attributes\":{\"n\":\"2.1\"},\"end\":8430,\"start\":8410},{\"attributes\":{\"n\":\"2.2\"},\"end\":12207,\"start\":12180},{\"attributes\":{\"n\":\"3\"},\"end\":14437,\"start\":14429},{\"attributes\":{\"n\":\"3.1\"},\"end\":14784,\"start\":14757},{\"attributes\":{\"n\":\"3.2\"},\"end\":18654,\"start\":18637},{\"attributes\":{\"n\":\"3.3\"},\"end\":25927,\"start\":25909},{\"attributes\":{\"n\":\"3.4\"},\"end\":26980,\"start\":26970},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":27009,\"start\":26983},{\"attributes\":{\"n\":\"4\"},\"end\":29080,\"start\":29069},{\"attributes\":{\"n\":\"4.1\"},\"end\":29407,\"start\":29389},{\"attributes\":{\"n\":\"4.2\"},\"end\":31672,\"start\":31650},{\"end\":32600,\"start\":32595},{\"attributes\":{\"n\":\"4.3\"},\"end\":35223,\"start\":35209},{\"attributes\":{\"n\":\"4.4\"},\"end\":37421,\"start\":37403},{\"end\":40063,\"start\":40059},{\"attributes\":{\"n\":\"4.5\"},\"end\":40838,\"start\":40816},{\"attributes\":{\"n\":\"5\"},\"end\":41676,\"start\":41666},{\"attributes\":{\"n\":\"6\"},\"end\":42509,\"start\":42494},{\"end\":42630,\"start\":42620},{\"end\":42834,\"start\":42824},{\"end\":43481,\"start\":43480},{\"end\":43983,\"start\":43963},{\"end\":44197,\"start\":44177},{\"end\":44342,\"start\":44332},{\"end\":44819,\"start\":44810},{\"end\":45041,\"start\":45032}]", "table": "[{\"end\":44808,\"start\":44389},{\"end\":45030,\"start\":44848},{\"end\":45516,\"start\":45323}]", "figure_caption": "[{\"end\":42822,\"start\":42632},{\"end\":43219,\"start\":42836},{\"end\":43478,\"start\":43222},{\"end\":43961,\"start\":43482},{\"end\":44175,\"start\":43986},{\"end\":44330,\"start\":44200},{\"end\":44385,\"start\":44344},{\"end\":44389,\"start\":44388},{\"end\":44848,\"start\":44821},{\"end\":45323,\"start\":45043}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6246,\"start\":6238},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14537,\"start\":14529},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14879,\"start\":14871},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18292,\"start\":18284},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19528,\"start\":19520},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20773,\"start\":20765},{\"end\":35637,\"start\":35629},{\"end\":35899,\"start\":35891},{\"end\":36769,\"start\":36761},{\"end\":38040,\"start\":38032},{\"end\":38667,\"start\":38659},{\"end\":39410,\"start\":39402},{\"end\":39658,\"start\":39650},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":40427,\"start\":40419},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41041,\"start\":41033}]", "bib_author_first_name": "[{\"end\":45598,\"start\":45593},{\"end\":45608,\"start\":45605},{\"end\":45624,\"start\":45616},{\"end\":45637,\"start\":45632},{\"end\":45651,\"start\":45643},{\"end\":46027,\"start\":46024},{\"end\":46036,\"start\":46034},{\"end\":46048,\"start\":46041},{\"end\":46058,\"start\":46055},{\"end\":46070,\"start\":46066},{\"end\":46502,\"start\":46498},{\"end\":46517,\"start\":46514},{\"end\":46529,\"start\":46525},{\"end\":46925,\"start\":46918},{\"end\":46937,\"start\":46932},{\"end\":46952,\"start\":46944},{\"end\":46961,\"start\":46957},{\"end\":46972,\"start\":46966},{\"end\":47475,\"start\":47468},{\"end\":47487,\"start\":47482},{\"end\":47502,\"start\":47494},{\"end\":47512,\"start\":47508},{\"end\":47523,\"start\":47517},{\"end\":47920,\"start\":47913},{\"end\":47932,\"start\":47925},{\"end\":47947,\"start\":47939},{\"end\":47958,\"start\":47954},{\"end\":47966,\"start\":47963},{\"end\":48423,\"start\":48417},{\"end\":48438,\"start\":48432},{\"end\":48919,\"start\":48913},{\"end\":48932,\"start\":48928},{\"end\":49345,\"start\":49338},{\"end\":49355,\"start\":49351},{\"end\":49367,\"start\":49362},{\"end\":49381,\"start\":49374},{\"end\":49391,\"start\":49387},{\"end\":49398,\"start\":49396},{\"end\":49920,\"start\":49917},{\"end\":49931,\"start\":49926},{\"end\":49948,\"start\":49943},{\"end\":49962,\"start\":49957},{\"end\":49974,\"start\":49970},{\"end\":49987,\"start\":49982},{\"end\":50000,\"start\":49995},{\"end\":50499,\"start\":50491},{\"end\":50508,\"start\":50504},{\"end\":50520,\"start\":50515},{\"end\":50530,\"start\":50527},{\"end\":50543,\"start\":50535},{\"end\":50555,\"start\":50551},{\"end\":51121,\"start\":51113},{\"end\":51130,\"start\":51126},{\"end\":51144,\"start\":51137},{\"end\":51159,\"start\":51152},{\"end\":51168,\"start\":51165},{\"end\":51181,\"start\":51173},{\"end\":51555,\"start\":51545},{\"end\":51569,\"start\":51563},{\"end\":51579,\"start\":51576},{\"end\":51593,\"start\":51585},{\"end\":51604,\"start\":51599},{\"end\":51622,\"start\":51615},{\"end\":52062,\"start\":52055},{\"end\":52076,\"start\":52070},{\"end\":52087,\"start\":52083},{\"end\":52098,\"start\":52094},{\"end\":52113,\"start\":52105},{\"end\":52125,\"start\":52120},{\"end\":52135,\"start\":52132},{\"end\":52654,\"start\":52648},{\"end\":52668,\"start\":52662},{\"end\":52680,\"start\":52675},{\"end\":52931,\"start\":52926},{\"end\":52943,\"start\":52936},{\"end\":52958,\"start\":52949},{\"end\":53390,\"start\":53385},{\"end\":53404,\"start\":53396},{\"end\":53417,\"start\":53411},{\"end\":53432,\"start\":53423},{\"end\":53860,\"start\":53857},{\"end\":53873,\"start\":53866},{\"end\":53884,\"start\":53881},{\"end\":53893,\"start\":53890},{\"end\":53906,\"start\":53899},{\"end\":54317,\"start\":54311},{\"end\":54330,\"start\":54323},{\"end\":54343,\"start\":54336},{\"end\":54356,\"start\":54350},{\"end\":54369,\"start\":54362},{\"end\":54378,\"start\":54376},{\"end\":54393,\"start\":54385},{\"end\":54944,\"start\":54938},{\"end\":54957,\"start\":54950},{\"end\":54965,\"start\":54963},{\"end\":54976,\"start\":54972},{\"end\":54988,\"start\":54981},{\"end\":55003,\"start\":54995},{\"end\":55548,\"start\":55543},{\"end\":55562,\"start\":55558},{\"end\":55577,\"start\":55574},{\"end\":55588,\"start\":55584},{\"end\":55590,\"start\":55589},{\"end\":55604,\"start\":55600},{\"end\":55961,\"start\":55956},{\"end\":55975,\"start\":55971},{\"end\":55991,\"start\":55985},{\"end\":56498,\"start\":56494},{\"end\":56513,\"start\":56504},{\"end\":56530,\"start\":56522},{\"end\":56548,\"start\":56541},{\"end\":56975,\"start\":56968},{\"end\":56993,\"start\":56984},{\"end\":57013,\"start\":57009},{\"end\":57027,\"start\":57023},{\"end\":57503,\"start\":57497},{\"end\":57518,\"start\":57512},{\"end\":57534,\"start\":57528},{\"end\":57548,\"start\":57544},{\"end\":57995,\"start\":57989},{\"end\":58010,\"start\":58004},{\"end\":58026,\"start\":58020},{\"end\":58040,\"start\":58036},{\"end\":58511,\"start\":58507},{\"end\":58522,\"start\":58518},{\"end\":58534,\"start\":58527},{\"end\":58545,\"start\":58541},{\"end\":58556,\"start\":58553},{\"end\":58569,\"start\":58562},{\"end\":58956,\"start\":58951},{\"end\":58971,\"start\":58963},{\"end\":58980,\"start\":58976},{\"end\":58991,\"start\":58987},{\"end\":59006,\"start\":58998},{\"end\":59529,\"start\":59524},{\"end\":59542,\"start\":59536},{\"end\":59550,\"start\":59548},{\"end\":59566,\"start\":59558},{\"end\":59575,\"start\":59571},{\"end\":59588,\"start\":59580},{\"end\":60175,\"start\":60168},{\"end\":60189,\"start\":60182},{\"end\":60204,\"start\":60196},{\"end\":60212,\"start\":60211},{\"end\":60228,\"start\":60221},{\"end\":60242,\"start\":60233},{\"end\":60669,\"start\":60662},{\"end\":60679,\"start\":60674},{\"end\":60690,\"start\":60686},{\"end\":60705,\"start\":60697},{\"end\":60715,\"start\":60710},{\"end\":60729,\"start\":60722},{\"end\":60740,\"start\":60736},{\"end\":61256,\"start\":61248},{\"end\":61269,\"start\":61263},{\"end\":61280,\"start\":61274},{\"end\":61294,\"start\":61286},{\"end\":61306,\"start\":61300},{\"end\":61318,\"start\":61312},{\"end\":61759,\"start\":61755},{\"end\":61770,\"start\":61766},{\"end\":61782,\"start\":61777},{\"end\":61796,\"start\":61789},{\"end\":61810,\"start\":61803},{\"end\":61820,\"start\":61817},{\"end\":62370,\"start\":62366},{\"end\":62381,\"start\":62377},{\"end\":62390,\"start\":62388},{\"end\":62399,\"start\":62396},{\"end\":62409,\"start\":62406},{\"end\":62419,\"start\":62414},{\"end\":62433,\"start\":62426},{\"end\":62887,\"start\":62881},{\"end\":62901,\"start\":62895},{\"end\":62911,\"start\":62908},{\"end\":62922,\"start\":62918},{\"end\":63422,\"start\":63417},{\"end\":63433,\"start\":63430},{\"end\":63445,\"start\":63440},{\"end\":63455,\"start\":63452}]", "bib_author_last_name": "[{\"end\":45603,\"start\":45599},{\"end\":45614,\"start\":45609},{\"end\":45630,\"start\":45625},{\"end\":45641,\"start\":45638},{\"end\":45654,\"start\":45652},{\"end\":46032,\"start\":46028},{\"end\":46039,\"start\":46037},{\"end\":46053,\"start\":46049},{\"end\":46064,\"start\":46059},{\"end\":46075,\"start\":46071},{\"end\":46512,\"start\":46503},{\"end\":46523,\"start\":46518},{\"end\":46536,\"start\":46530},{\"end\":46930,\"start\":46926},{\"end\":46942,\"start\":46938},{\"end\":46955,\"start\":46953},{\"end\":46964,\"start\":46962},{\"end\":46976,\"start\":46973},{\"end\":47480,\"start\":47476},{\"end\":47492,\"start\":47488},{\"end\":47506,\"start\":47503},{\"end\":47515,\"start\":47513},{\"end\":47527,\"start\":47524},{\"end\":47923,\"start\":47921},{\"end\":47937,\"start\":47933},{\"end\":47952,\"start\":47948},{\"end\":47961,\"start\":47959},{\"end\":47969,\"start\":47967},{\"end\":48430,\"start\":48424},{\"end\":48445,\"start\":48439},{\"end\":48926,\"start\":48920},{\"end\":48941,\"start\":48933},{\"end\":49349,\"start\":49346},{\"end\":49360,\"start\":49356},{\"end\":49372,\"start\":49368},{\"end\":49385,\"start\":49382},{\"end\":49394,\"start\":49392},{\"end\":49403,\"start\":49399},{\"end\":49924,\"start\":49921},{\"end\":49941,\"start\":49932},{\"end\":49955,\"start\":49949},{\"end\":49968,\"start\":49963},{\"end\":49980,\"start\":49975},{\"end\":49993,\"start\":49988},{\"end\":50013,\"start\":50001},{\"end\":50502,\"start\":50500},{\"end\":50513,\"start\":50509},{\"end\":50525,\"start\":50521},{\"end\":50533,\"start\":50531},{\"end\":50549,\"start\":50544},{\"end\":50560,\"start\":50556},{\"end\":51124,\"start\":51122},{\"end\":51135,\"start\":51131},{\"end\":51150,\"start\":51145},{\"end\":51163,\"start\":51160},{\"end\":51171,\"start\":51169},{\"end\":51186,\"start\":51182},{\"end\":51561,\"start\":51556},{\"end\":51574,\"start\":51570},{\"end\":51583,\"start\":51580},{\"end\":51597,\"start\":51594},{\"end\":51613,\"start\":51605},{\"end\":51629,\"start\":51623},{\"end\":52068,\"start\":52063},{\"end\":52081,\"start\":52077},{\"end\":52092,\"start\":52088},{\"end\":52103,\"start\":52099},{\"end\":52118,\"start\":52114},{\"end\":52130,\"start\":52126},{\"end\":52140,\"start\":52136},{\"end\":52660,\"start\":52655},{\"end\":52673,\"start\":52669},{\"end\":52689,\"start\":52681},{\"end\":52934,\"start\":52932},{\"end\":52947,\"start\":52944},{\"end\":52961,\"start\":52959},{\"end\":53394,\"start\":53391},{\"end\":53409,\"start\":53405},{\"end\":53421,\"start\":53418},{\"end\":53437,\"start\":53433},{\"end\":53864,\"start\":53861},{\"end\":53879,\"start\":53874},{\"end\":53888,\"start\":53885},{\"end\":53897,\"start\":53894},{\"end\":53910,\"start\":53907},{\"end\":54321,\"start\":54318},{\"end\":54334,\"start\":54331},{\"end\":54348,\"start\":54344},{\"end\":54360,\"start\":54357},{\"end\":54374,\"start\":54370},{\"end\":54383,\"start\":54379},{\"end\":54396,\"start\":54394},{\"end\":54948,\"start\":54945},{\"end\":54961,\"start\":54958},{\"end\":54970,\"start\":54966},{\"end\":54979,\"start\":54977},{\"end\":54993,\"start\":54989},{\"end\":55006,\"start\":55004},{\"end\":55556,\"start\":55549},{\"end\":55572,\"start\":55563},{\"end\":55582,\"start\":55578},{\"end\":55598,\"start\":55591},{\"end\":55609,\"start\":55605},{\"end\":55969,\"start\":55962},{\"end\":55983,\"start\":55976},{\"end\":55998,\"start\":55992},{\"end\":56502,\"start\":56499},{\"end\":56520,\"start\":56514},{\"end\":56539,\"start\":56531},{\"end\":56551,\"start\":56549},{\"end\":56982,\"start\":56976},{\"end\":57007,\"start\":56994},{\"end\":57021,\"start\":57014},{\"end\":57042,\"start\":57028},{\"end\":57510,\"start\":57504},{\"end\":57526,\"start\":57519},{\"end\":57542,\"start\":57535},{\"end\":57554,\"start\":57549},{\"end\":58002,\"start\":57996},{\"end\":58018,\"start\":58011},{\"end\":58034,\"start\":58027},{\"end\":58046,\"start\":58041},{\"end\":58516,\"start\":58512},{\"end\":58525,\"start\":58523},{\"end\":58539,\"start\":58535},{\"end\":58551,\"start\":58546},{\"end\":58560,\"start\":58557},{\"end\":58573,\"start\":58570},{\"end\":58961,\"start\":58957},{\"end\":58974,\"start\":58972},{\"end\":58985,\"start\":58981},{\"end\":58996,\"start\":58992},{\"end\":59011,\"start\":59007},{\"end\":59534,\"start\":59530},{\"end\":59546,\"start\":59543},{\"end\":59556,\"start\":59551},{\"end\":59569,\"start\":59567},{\"end\":59578,\"start\":59576},{\"end\":59593,\"start\":59589},{\"end\":60180,\"start\":60176},{\"end\":60194,\"start\":60190},{\"end\":60209,\"start\":60205},{\"end\":60219,\"start\":60213},{\"end\":60231,\"start\":60229},{\"end\":60245,\"start\":60243},{\"end\":60251,\"start\":60247},{\"end\":60672,\"start\":60670},{\"end\":60684,\"start\":60680},{\"end\":60695,\"start\":60691},{\"end\":60708,\"start\":60706},{\"end\":60720,\"start\":60716},{\"end\":60734,\"start\":60730},{\"end\":60744,\"start\":60741},{\"end\":61261,\"start\":61257},{\"end\":61272,\"start\":61270},{\"end\":61284,\"start\":61281},{\"end\":61298,\"start\":61295},{\"end\":61310,\"start\":61307},{\"end\":61323,\"start\":61319},{\"end\":61764,\"start\":61760},{\"end\":61775,\"start\":61771},{\"end\":61787,\"start\":61783},{\"end\":61801,\"start\":61797},{\"end\":61815,\"start\":61811},{\"end\":61825,\"start\":61821},{\"end\":62375,\"start\":62371},{\"end\":62386,\"start\":62382},{\"end\":62394,\"start\":62391},{\"end\":62404,\"start\":62400},{\"end\":62412,\"start\":62410},{\"end\":62424,\"start\":62420},{\"end\":62438,\"start\":62434},{\"end\":62893,\"start\":62888},{\"end\":62906,\"start\":62902},{\"end\":62916,\"start\":62912},{\"end\":62925,\"start\":62923},{\"end\":63428,\"start\":63423},{\"end\":63438,\"start\":63434},{\"end\":63450,\"start\":63446},{\"end\":63458,\"start\":63456}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":211041929},\"end\":45918,\"start\":45518},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":210932292},\"end\":46446,\"start\":45920},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":207240067},\"end\":46848,\"start\":46448},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":199466324},\"end\":47387,\"start\":46850},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":221535154},\"end\":47815,\"start\":47389},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":69882694},\"end\":48340,\"start\":47817},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5575601},\"end\":48861,\"start\":48342},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207238980},\"end\":49336,\"start\":48863},{\"attributes\":{\"id\":\"b8\"},\"end\":49835,\"start\":49338},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7108068},\"end\":50408,\"start\":49837},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":211043589},\"end\":51079,\"start\":50410},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13907106},\"end\":51512,\"start\":51081},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11129957},\"end\":51965,\"start\":51514},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236980247},\"end\":52589,\"start\":51967},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":58370896},\"end\":52844,\"start\":52591},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11118105},\"end\":53294,\"start\":52846},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":246823233},\"end\":53801,\"start\":53296},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":231979546},\"end\":54242,\"start\":53803},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":237940736},\"end\":54853,\"start\":54244},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":240070722},\"end\":55464,\"start\":54855},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16447573},\"end\":55901,\"start\":55466},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3051291},\"end\":56415,\"start\":55903},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":231925144},\"end\":56907,\"start\":56417},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10795036},\"end\":57433,\"start\":56909},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8047550},\"end\":57894,\"start\":57435},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1058329},\"end\":58456,\"start\":57896},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8399404},\"end\":58911,\"start\":58458},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":150380651},\"end\":59478,\"start\":58913},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220347145},\"end\":60077,\"start\":59480},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3664138},\"end\":60609,\"start\":60079},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":224814335},\"end\":61246,\"start\":60611},{\"attributes\":{\"doi\":\"arXiv:2204.11752\",\"id\":\"b31\"},\"end\":61687,\"start\":61248},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":218720040},\"end\":62281,\"start\":61689},{\"attributes\":{\"doi\":\"10.1109/TKDE.2022.3155155\",\"id\":\"b33\",\"matched_paper_id\":247214120},\"end\":62802,\"start\":62283},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2610055},\"end\":63415,\"start\":62804},{\"attributes\":{\"doi\":\"arXiv:2204.03827\",\"id\":\"b35\"},\"end\":63864,\"start\":63417}]", "bib_title": "[{\"end\":45591,\"start\":45518},{\"end\":46022,\"start\":45920},{\"end\":46496,\"start\":46448},{\"end\":46916,\"start\":46850},{\"end\":47466,\"start\":47389},{\"end\":47911,\"start\":47817},{\"end\":48415,\"start\":48342},{\"end\":48911,\"start\":48863},{\"end\":49915,\"start\":49837},{\"end\":50489,\"start\":50410},{\"end\":51111,\"start\":51081},{\"end\":51543,\"start\":51514},{\"end\":52053,\"start\":51967},{\"end\":52646,\"start\":52591},{\"end\":52924,\"start\":52846},{\"end\":53383,\"start\":53296},{\"end\":53855,\"start\":53803},{\"end\":54309,\"start\":54244},{\"end\":54936,\"start\":54855},{\"end\":55541,\"start\":55466},{\"end\":55954,\"start\":55903},{\"end\":56492,\"start\":56417},{\"end\":56966,\"start\":56909},{\"end\":57495,\"start\":57435},{\"end\":57987,\"start\":57896},{\"end\":58505,\"start\":58458},{\"end\":58949,\"start\":58913},{\"end\":59522,\"start\":59480},{\"end\":60166,\"start\":60079},{\"end\":60660,\"start\":60611},{\"end\":61753,\"start\":61689},{\"end\":62364,\"start\":62283},{\"end\":62879,\"start\":62804}]", "bib_author": "[{\"end\":45605,\"start\":45593},{\"end\":45616,\"start\":45605},{\"end\":45632,\"start\":45616},{\"end\":45643,\"start\":45632},{\"end\":45656,\"start\":45643},{\"end\":46034,\"start\":46024},{\"end\":46041,\"start\":46034},{\"end\":46055,\"start\":46041},{\"end\":46066,\"start\":46055},{\"end\":46077,\"start\":46066},{\"end\":46514,\"start\":46498},{\"end\":46525,\"start\":46514},{\"end\":46538,\"start\":46525},{\"end\":46932,\"start\":46918},{\"end\":46944,\"start\":46932},{\"end\":46957,\"start\":46944},{\"end\":46966,\"start\":46957},{\"end\":46978,\"start\":46966},{\"end\":47482,\"start\":47468},{\"end\":47494,\"start\":47482},{\"end\":47508,\"start\":47494},{\"end\":47517,\"start\":47508},{\"end\":47529,\"start\":47517},{\"end\":47925,\"start\":47913},{\"end\":47939,\"start\":47925},{\"end\":47954,\"start\":47939},{\"end\":47963,\"start\":47954},{\"end\":47971,\"start\":47963},{\"end\":48432,\"start\":48417},{\"end\":48447,\"start\":48432},{\"end\":48928,\"start\":48913},{\"end\":48943,\"start\":48928},{\"end\":49351,\"start\":49338},{\"end\":49362,\"start\":49351},{\"end\":49374,\"start\":49362},{\"end\":49387,\"start\":49374},{\"end\":49396,\"start\":49387},{\"end\":49405,\"start\":49396},{\"end\":49926,\"start\":49917},{\"end\":49943,\"start\":49926},{\"end\":49957,\"start\":49943},{\"end\":49970,\"start\":49957},{\"end\":49982,\"start\":49970},{\"end\":49995,\"start\":49982},{\"end\":50015,\"start\":49995},{\"end\":50504,\"start\":50491},{\"end\":50515,\"start\":50504},{\"end\":50527,\"start\":50515},{\"end\":50535,\"start\":50527},{\"end\":50551,\"start\":50535},{\"end\":50562,\"start\":50551},{\"end\":51126,\"start\":51113},{\"end\":51137,\"start\":51126},{\"end\":51152,\"start\":51137},{\"end\":51165,\"start\":51152},{\"end\":51173,\"start\":51165},{\"end\":51188,\"start\":51173},{\"end\":51563,\"start\":51545},{\"end\":51576,\"start\":51563},{\"end\":51585,\"start\":51576},{\"end\":51599,\"start\":51585},{\"end\":51615,\"start\":51599},{\"end\":51631,\"start\":51615},{\"end\":52070,\"start\":52055},{\"end\":52083,\"start\":52070},{\"end\":52094,\"start\":52083},{\"end\":52105,\"start\":52094},{\"end\":52120,\"start\":52105},{\"end\":52132,\"start\":52120},{\"end\":52142,\"start\":52132},{\"end\":52662,\"start\":52648},{\"end\":52675,\"start\":52662},{\"end\":52691,\"start\":52675},{\"end\":52936,\"start\":52926},{\"end\":52949,\"start\":52936},{\"end\":52963,\"start\":52949},{\"end\":53396,\"start\":53385},{\"end\":53411,\"start\":53396},{\"end\":53423,\"start\":53411},{\"end\":53439,\"start\":53423},{\"end\":53866,\"start\":53857},{\"end\":53881,\"start\":53866},{\"end\":53890,\"start\":53881},{\"end\":53899,\"start\":53890},{\"end\":53912,\"start\":53899},{\"end\":54323,\"start\":54311},{\"end\":54336,\"start\":54323},{\"end\":54350,\"start\":54336},{\"end\":54362,\"start\":54350},{\"end\":54376,\"start\":54362},{\"end\":54385,\"start\":54376},{\"end\":54398,\"start\":54385},{\"end\":54950,\"start\":54938},{\"end\":54963,\"start\":54950},{\"end\":54972,\"start\":54963},{\"end\":54981,\"start\":54972},{\"end\":54995,\"start\":54981},{\"end\":55008,\"start\":54995},{\"end\":55558,\"start\":55543},{\"end\":55574,\"start\":55558},{\"end\":55584,\"start\":55574},{\"end\":55600,\"start\":55584},{\"end\":55611,\"start\":55600},{\"end\":55971,\"start\":55956},{\"end\":55985,\"start\":55971},{\"end\":56000,\"start\":55985},{\"end\":56504,\"start\":56494},{\"end\":56522,\"start\":56504},{\"end\":56541,\"start\":56522},{\"end\":56553,\"start\":56541},{\"end\":56984,\"start\":56968},{\"end\":57009,\"start\":56984},{\"end\":57023,\"start\":57009},{\"end\":57044,\"start\":57023},{\"end\":57512,\"start\":57497},{\"end\":57528,\"start\":57512},{\"end\":57544,\"start\":57528},{\"end\":57556,\"start\":57544},{\"end\":58004,\"start\":57989},{\"end\":58020,\"start\":58004},{\"end\":58036,\"start\":58020},{\"end\":58048,\"start\":58036},{\"end\":58518,\"start\":58507},{\"end\":58527,\"start\":58518},{\"end\":58541,\"start\":58527},{\"end\":58553,\"start\":58541},{\"end\":58562,\"start\":58553},{\"end\":58575,\"start\":58562},{\"end\":58963,\"start\":58951},{\"end\":58976,\"start\":58963},{\"end\":58987,\"start\":58976},{\"end\":58998,\"start\":58987},{\"end\":59013,\"start\":58998},{\"end\":59536,\"start\":59524},{\"end\":59548,\"start\":59536},{\"end\":59558,\"start\":59548},{\"end\":59571,\"start\":59558},{\"end\":59580,\"start\":59571},{\"end\":59595,\"start\":59580},{\"end\":60182,\"start\":60168},{\"end\":60196,\"start\":60182},{\"end\":60211,\"start\":60196},{\"end\":60221,\"start\":60211},{\"end\":60233,\"start\":60221},{\"end\":60247,\"start\":60233},{\"end\":60253,\"start\":60247},{\"end\":60674,\"start\":60662},{\"end\":60686,\"start\":60674},{\"end\":60697,\"start\":60686},{\"end\":60710,\"start\":60697},{\"end\":60722,\"start\":60710},{\"end\":60736,\"start\":60722},{\"end\":60746,\"start\":60736},{\"end\":61263,\"start\":61248},{\"end\":61274,\"start\":61263},{\"end\":61286,\"start\":61274},{\"end\":61300,\"start\":61286},{\"end\":61312,\"start\":61300},{\"end\":61325,\"start\":61312},{\"end\":61766,\"start\":61755},{\"end\":61777,\"start\":61766},{\"end\":61789,\"start\":61777},{\"end\":61803,\"start\":61789},{\"end\":61817,\"start\":61803},{\"end\":61827,\"start\":61817},{\"end\":62377,\"start\":62366},{\"end\":62388,\"start\":62377},{\"end\":62396,\"start\":62388},{\"end\":62406,\"start\":62396},{\"end\":62414,\"start\":62406},{\"end\":62426,\"start\":62414},{\"end\":62440,\"start\":62426},{\"end\":62895,\"start\":62881},{\"end\":62908,\"start\":62895},{\"end\":62918,\"start\":62908},{\"end\":62927,\"start\":62918},{\"end\":63430,\"start\":63417},{\"end\":63440,\"start\":63430},{\"end\":63452,\"start\":63440},{\"end\":63460,\"start\":63452}]", "bib_venue": "[{\"end\":45695,\"start\":45656},{\"end\":46145,\"start\":46077},{\"end\":46608,\"start\":46538},{\"end\":47066,\"start\":46978},{\"end\":47587,\"start\":47529},{\"end\":48039,\"start\":47971},{\"end\":48547,\"start\":48447},{\"end\":49044,\"start\":48943},{\"end\":49573,\"start\":49405},{\"end\":50084,\"start\":50015},{\"end\":50680,\"start\":50562},{\"end\":51254,\"start\":51188},{\"end\":51697,\"start\":51631},{\"end\":52229,\"start\":52142},{\"end\":52699,\"start\":52691},{\"end\":53030,\"start\":52963},{\"end\":53505,\"start\":53439},{\"end\":53978,\"start\":53912},{\"end\":54494,\"start\":54398},{\"end\":55105,\"start\":55008},{\"end\":55670,\"start\":55611},{\"end\":56101,\"start\":56000},{\"end\":56620,\"start\":56553},{\"end\":57125,\"start\":57044},{\"end\":57622,\"start\":57556},{\"end\":58131,\"start\":58048},{\"end\":58641,\"start\":58575},{\"end\":59131,\"start\":59013},{\"end\":59713,\"start\":59595},{\"end\":60332,\"start\":60253},{\"end\":60864,\"start\":60746},{\"end\":61445,\"start\":61341},{\"end\":61928,\"start\":61827},{\"end\":62516,\"start\":62465},{\"end\":63045,\"start\":62927},{\"end\":63619,\"start\":63476},{\"end\":46200,\"start\":46147},{\"end\":46665,\"start\":46610},{\"end\":47141,\"start\":47068},{\"end\":48094,\"start\":48041},{\"end\":48634,\"start\":48549},{\"end\":49132,\"start\":49046},{\"end\":50140,\"start\":50086},{\"end\":50785,\"start\":50682},{\"end\":51307,\"start\":51256},{\"end\":51750,\"start\":51699},{\"end\":52303,\"start\":52231},{\"end\":53084,\"start\":53032},{\"end\":53558,\"start\":53507},{\"end\":54031,\"start\":53980},{\"end\":54577,\"start\":54496},{\"end\":55189,\"start\":55107},{\"end\":56189,\"start\":56103},{\"end\":56674,\"start\":56622},{\"end\":57193,\"start\":57127},{\"end\":57675,\"start\":57624},{\"end\":58201,\"start\":58133},{\"end\":58694,\"start\":58643},{\"end\":59236,\"start\":59133},{\"end\":59818,\"start\":59715},{\"end\":60969,\"start\":60866},{\"end\":62016,\"start\":61930},{\"end\":63150,\"start\":63047}]"}}}, "year": 2023, "month": 12, "day": 17}