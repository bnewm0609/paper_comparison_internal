{"id": 237386023, "updated": "2023-11-07 17:37:06.86", "metadata": {"title": "Learning to Prompt for Vision-Language Models", "authors": "[{\"first\":\"Kaiyang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Jingkang\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Loy\",\"middle\":[\"Change\"]},{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming -- one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt's context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.01134", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijcv/ZhouYLL22", "doi": "10.1007/s11263-022-01653-1"}}, "content": {"source": {"pdf_hash": "96ea07447d2f9adefe03852a878517a2a6d45b96", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2109.01134v6.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2109.01134", "status": "GREEN"}}, "grobid": {"id": "fc76467c5940616441e0d624a7cd70b5d6ae37c4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/96ea07447d2f9adefe03852a878517a2a6d45b96.txt", "contents": "\nLearning to Prompt for Vision-Language Models\n\n\nKaiyang Zhou kaiyang.zhou@ntu.edu.sg \n\u00b7 Jingkang jingkang001@ntu.edu.sg \nYang \u00b7 Chen \nChange Loy ccloy@ntu.edu.sg \nZiwei Liu ziwei.liu@ntu.edu.sg \nKaiyang Zhou \nJingkang Yang \nChen Change Loy \nZiwei Liu \n\nS-Lab\nS-Lab\nNanyang Technological University\nSingapore\n\n\nS-Lab\nNanyang Technological University\nSingapore\n\n\nS-Lab\nNanyang Technological University\nSingapore\n\n\nNanyang Technological University\nSingapore\n\nLearning to Prompt for Vision-Language Models\nReceived: date / Accepted: dateNoname manuscript No. (will be inserted by the editor)\nLarge pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zeroshot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consumingone needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt's context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and classspecific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15% (with the highest reaching over 45%). Despite being a learningbased approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.\n\nIntroduction\n\nA common approach for building state-of-the-art visual recognition systems is to train vision models to predict for a fixed set of object categories using discrete labels (He et al., 2016;Dosovitskiy et al., 2021). From a technical point of view, this is achieved by matching image features-produced by a vision model like ResNet (He et al., 2016) or ViT (Dosovitskiy et al., 2021)-with a fixed set of weights that are seen as visual concepts and initialized randomly. Although training categories often have a textual form, such as \"goldfish\" or \"toilet paper,\" they will be converted into discrete labels just for easing the computation of the cross-entropy loss, leaving the semantics encapsulated in texts largely unexploited. Such a learning paradigm limits visual recognition systems to closed-set visual concepts, making them unable to deal with new categories since additional data are required for learning a new classifier.\n\nRecently, vision-language pre-training such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) has emerged as a promising alternative for visual representation learning. The main idea is to align  (CoOp). The former needs to use a held-out validation set for words tuning, which is inefficient; the latter automates the process and requires only a few labeled images for learning.\n\nimages and raw texts using two separate encoders-one for each modality. For instance, both CLIP and ALIGN formulate the learning objective as a contrastive loss, which pulls together images and their textual descriptions while pushes away unmatched pairs in the feature space. By pre-training at a large scale, models can learn diverse visual concepts and can readily be transferred to any downstream task through prompting (Radford et al., 2021;Jia et al., 2021;F\u00fcrst et al., 2021;Li et al., 2021;Singh et al., 2021;Yuan et al., 2021). In particular, for any new classification task one can first synthesize the classification weights by giving sentences describing task-relevant categories to the text encoder, and then compare with image features produced by the image encoder.\n\nWe observe that for pre-trained vision-language models, the text input, known as prompt, plays a key role in downstream datasets. However, identifying the right prompt is a non-trivial task, which often takes a significant amount of time for words tuning-a slight change in wording could make a huge difference in performance. For instance, for Caltech101 (Figure 1(a), 2nd vs 3rd prompt), adding \"a\" before the class token brings more than 5% increase in accuracy. Moreover, prompt engineering also requires prior knowledge about the task and ideally the language model's underlying mechanism. This is exemplified in Figure 1(b-d) where adding task-relevant context can lead to significant improvements, i.e., \"flower\" for Flowers102, \"texture\" for DTD and \"satellite\" for EuroSAT. Tuning the sentence structure could bring further improvements, e.g., putting \"a type of flower\" after the class token for Flowers102, keeping only \"texture\" in the context for DTD, and adding \"centered\" before \"satellite photo\" for EuroSAT. However, even with extensive tuning, the resulting prompts are by no means guaranteed to be optimal for these downstream tasks.\n\nInspired by recent prompt learning research in natural language processing (NLP) (Shin et al., 2020;Jiang et al., 2020;Zhong et al., 2021), we propose a simple approach called Context Optimization (CoOp) 1 to automate prompt engineering, specifically for pre-trained vision-language models. Concretely, CoOp models a prompt's context words with learnable vectors, which could be initialized with either random values or pretrained word embeddings (see Figure 2). Two implementations are provided to handle tasks of different natures: one is based on unified context, which shares the same context with all classes and works well on most categories; while the other is based on class-specific context, which learns a specific set of context tokens for each class and is found to be more suitable for some fine-grained categories. During training, we simply minimize prediction errors using the cross-entropy loss with respect to the learnable context vectors while keeping the entire pre-trained parameters fixed. The gradients can be back-propagated all the way through the text encoder, distilling the rich knowledge encoded in the parameters for learning task-relevant context.\n\nTo demonstrate the effectiveness of CoOp, we benchmark on 11 datasets, which cover a diverse set of visual recognition tasks including classification on generic objects, scenes, actions and fine-grained categories, as well as specialized tasks like recognizing textures and satellite imagery. The results show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin. The performance can be further boosted by using more shots, e.g., with 16 shots the margin over hand-crafted prompts averages at around 15% and reaches over 45% for the highest. CoOp also outperforms the linear probe model, which is known as a strong few-shot learning baseline (Tian et al., 2020). Furthermore, CoOp demonstrates much stronger robustness than the zero-shot model (which uses manual prompts) to domain shifts, despite being a learningbased approach.\n\nIn summary, we make the following contributions:\n\n1. We present a timely study on the adaptation of recently proposed vision-language models in downstream applications and identify a critical problem associated with the deployment efficiency, i.e., prompt engineering. 2. To automate prompt engineering specifically for pre-trained vision-language models, we propose a simple approach based on continuous prompt learning and provide two implementations that can handle different recognition tasks. 3. We for the first time show that the proposed prompt learning-based approach outperforms both handcrafted prompts and the linear probe model in terms of downstream transfer learning performance and robustness under domain shifts for large visionlanguage models. 4. We open-source our project at https://github. com/KaiyangZhou/CoOp.\n\nWe hope the findings together with the open-source code can inspire and facilitate future research on efficient adaptation methods for large vision-language models-an emerging topic related to democratization of foundation models (Bommasani et al., 2021) i.e., making them easier and cheaper to adapt for the wider community.\n\n\nRelated Work\n\n\nVision-Language Models\n\nVision-language models have recently demonstrated great potential in learning generic visual representations and allowing zero-shot transfer to a variety of downstream classification tasks via prompting (Radford et al., 2021;Jia et al., 2021;Zhang et al., 2020;Singh et al., 2021;Yuan et al., 2021).\n\nTo our knowledge, the recent developments in vision-language learning, particularly CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021), are largely driven by advances in the following three areas: i) text representation learning with Transformers (Vaswani et al., 2017), ii) large-minibatch contrastive representation learning He et al., 2020;H\u00e9naff et al., 2020), and iii) web-scale training datasets-CLIP benefits from 400 million curated image-text pairs while ALIGN exploits 1.8 billion noisy image-text pairs.\n\nThe idea of mapping images and text onto a common embedding space has been studied since nearly a decade ago Frome et al., 2013;Elhoseiny et al., 2013), but with drastically different technologies. For text features extraction, early work has mainly utilized pre-trained word vectors Frome et al., 2013) or the hand-crafted TF-IDF features (Elhoseiny et al., 2013;Lei Ba et al., 2015). Matching images and text features has been formulated as metric learning (Frome et al., 2013), multilabel classification (Joulin et al., 2016;Gomez et al., 2017), n-gram language learning (Li et al., 2017), and the recently proposed captioning (Desai and Johnson, 2021).\n\nOur work is orthogonal to recent research in visionlanguage models, aiming to facilitate the adaptation and deployment of such models in downstream datasets.\n\n\nPrompt Learning in NLP\n\nKnowledge probing for large pre-trained language models, formally defined by Petroni et al. (2019) as \"fill-inthe-blank\" cloze tests, has recently sparked interest in prompt learning research in NLP (Shin et al., 2020;Jiang et al., 2020;Li and Liang, 2021;Zhong et al., 2021;Lester et al., 2021;Gao et al., 2020;Liu et al., 2021b).\n\nThe basic idea of knowledge probing is to induce pre-trained language models to generate answers given cloze-style prompts, which can benefit a number of downstream tasks, such as sentiment analysis. Jiang et al. (2020) propose to generate candidate prompts through text mining and paraphrasing, and identify the optimal ones that give the highest training accuracy. Shin et al. (2020) introduce a gradient-based approach, which searches for tokens with the largest gradient changes in the label likelihood.\n\nMost related to our work are continuous prompt learning methods (Zhong et al., 2021;Li and Liang, 2021;Lester et al., 2021) which optimize continuous vectors in the word embedding space. A drawback of such methods compared to searching discrete tokens is the lack of a clear way to visualize what \"words\" are learned for the vectors. We refer readers to Liu et al. (2021a) for a comprehensive survey in the topic of prompt learning in NLP.\n\nIt is worth noting that we are the first to apply prompt learning to the adaptation of large vision-   \nk = \" > A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Yl X w k T 2 O Q 8 J s k = \" > A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Yl X w k T 2 O Q 8 J s k = \" > A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y s 2 K n 8 1 m H X S r t f 8 q 5 p 3 X y 8 3 q n l o B X S O L l A V + e g a N d A d a q I W I o i j Z / S K 3 p w X 5 9 3 5 c D 4 X r R t O P n O G l u B 8 / Q I m m Z + d < / l a t e x i t > .\n. .\n[V]1 [V]2\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" i 4 z p m u R m 7 l 6 o l i 4 l X w k T  The main idea is to model a prompt's context using a set of learnable vectors, which can be optimized through minimizing the classification loss. Two designs are proposed: one is unified context, which shares the same context vectors with all classes; and the other is class-specific context, which learns for each class a specific set of context vectors.\n2 O Q 8 J s k = \" > A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Yl X w k T 2 O Q 8 J s k = \" > A A A C F X i c b V C 7 T g J B F J 3 1 i f h C L W 0 2 g g k V 2 a V Q S x I b S 0 z k k c C G z M 7 O w o R 5 b G b u a s i G n 7 C x 0 F + x M 7 b W / o m l A 2 w h 4 E l u c n L O v b n 3 n j D h z I D n f T s b m 1 v b O 7 u F v e L + w e H R c e n k t G 1 U q g l t E c W V 7 o b Y U M 4 k b Q E D T r u J p l i E n H b C 8 e 3 M 7 z x S b Z i S D z B J a C D w U L K Y E Q x W 6 l b 6 o 0 i B q Q x K Z a / m z e G u E z 8 n Z Z S j O S j 9 9 C N F U k E l E I 6 N 6 f l e A k G G N T D C 6 b T Y T w 1 N M B n j I e 1 Z K r G g J s j m 9 0 7 d S 6 t E b q y 0 L Q n u X P 0 7 k W F h z E S E t l N g G J l V b y b + 5 / V S i G + C j M k k B S r J Y l G c c h e U O 3 v e j Z i m B P j E E k w 0 s 7 e 6 Z I Q 1 J m A j W t o S i q U f M p F y Y F o 9 T Y\nlanguage models in computer vision-which we view as an important topic for democratizing foundation models (Bommasani et al., 2021)-and justify that prompt learning not only brings significant improvements to computer vision tasks in terms of transfer learning performance but also produces robust models that can handle domain shifts.\n\n\nMethodology\n\n\nVision-Language Pre-training\n\nWe briefly introduce vision-language pre-training with a particular focus on CLIP (Radford et al., 2021). Our approach is applicable to broader CLIP-like visionlanguage models.\n\nModels CLIP consists of two encoders, one for images and the other for text. The image encoder aims to map high-dimensional images into a low-dimensional embedding space. Specifically, given a sequence of words (tokens), such as \"a photo of a dog,\" CLIP first converts each one of the token (including punctuation) into a lowercased byte pair encoding (BPE) representation (Sennrich et al., 2016), which is essentially a unique numeric ID. The vocabulary size in CLIP is 49,152. To facilitate minibatch processing, each text sequence is encompassed with the [SOS] and [EOS] tokens and capped at a fixed length of 77. After that, the IDs are mapped to 512-D word embedding vectors, which are then passed on to the Transformer. Finally, the features at the [EOS] token position are layer normalized and further processed by a linear projection layer.\n\nTraining CLIP is trained to align the two embedding spaces learned for images and text respectively. Specifically, the learning objective is formulated as a contrastive loss. Given a batch of image-text pairs, CLIP maximizes the cosine similarity for matched pairs while minimizes the cosine similarity for all other unmatched pairs. To learn diverse visual concepts that are more transferable to downstream tasks, CLIP's team collects a large training dataset consisting of 400 million imagetext pairs.\n\n\nZero-Shot Inference\n\nSince CLIP is pre-trained to predict whether an image matches a textual description, it naturally fits zero-shot recognition. This is achieved by comparing image features with the classification weights synthesized by the text encoder, which takes as input textual descriptions specifying classes of interest. Formally, let f be image features extracted by the image encoder for an image x and {w i } K i=1 a set of weight vectors generated by the text encoder. K denotes the number of classes and each w i is derived from a prompt that could have the form of \"a photo of a [CLASS].\" where the class token is replaced by the specific class name, such as \"cat,\" \"dog\" or \"car.\" The prediction probability is then computed as\np(y = i|x) = exp(cos(w i , f )/\u03c4 ) K j=1 exp(cos(w j , f )/\u03c4 ) ,(1)\nwhere \u03c4 is a temperature parameter learned by CLIP and cos(\u00b7, \u00b7) denotes cosine similarity. Compared with the traditional classifier learning approach where closed-set visual concepts are learned from random vectors, vision-language pre-training allows open-set visual concepts to be explored through a high-capacity text encoder, leading to a broader semantic space and in turn making the learned representations more transferable to downstream tasks.\n\n\nContext Optimization\n\nWe propose Context Optimization (CoOp), which avoids manual prompt tuning by modeling context words with continuous vectors that are end-to-end learned from data while the massive pre-trained parameters are frozen. An overview is shown in Figure 2. Below we provide several different implementations.\n\nUnified Context We first introduce the unified context version, which shares the same context with all classes. Specifically, the prompt given to the text encoder g(\u00b7) is designed with the following form,\nt = [V] 1 [V] 2 . . . [V] M [CLASS],(2)\nwhere each [V] m (m \u2208 {1, . . . , M }) is a vector with the same dimension as word embeddings (i.e., 512 for CLIP), and M is a hyperparameter specifying the number of context tokens. By forwarding a prompt t to the text encoder g(\u00b7), we can obtain a classification weight vector representing a visual concept (still from the [EOS] token position). The prediction probability is computed as\np(y = i|x) = exp(cos(g(t i ), f )/\u03c4 ) K j=1 exp(cos(g(t j ), f )/\u03c4 ) ,(3)\nwhere the class token within each prompt t i is replaced by the corresponding word embedding vector(s) of the i-th class name. Other than placing the class token at the end of a sequence as in Equation (2), we can also put it in the middle like\nt = [V] 1 . . . [V] M 2 [CLASS][V] M 2 +1 . . . [V] M ,(4)\nwhich increases flexibility for learning-the prompt is allowed to either fill the latter cells with supplementary descriptions or cut off the sentence earlier by using a termination signal such as full stop.\n\n\nClass-Specific Context\n\nAnother option is to design class-specific context (CSC) where context vectors are independent to each class, i.e.,\n[V] i 1 [V] i 2 . . . [V] i M = [V] j 1 [V] j 2 . . . [V]\nj M for i = j and i, j \u2208 {1, . . . , K}. As an alternative to unified context, we find that CSC is particularly useful for some fine-grained classification tasks.\n\nTraining is performed to minimize the standard classification loss based on the cross-entropy, and the gradients can be back-propagated all the way through the text encoder g(\u00b7), making use of the rich knowledge encoded in the parameters to optimize the context. The design of continuous representations also allows full exploration in the word embedding space, which facilitates the learning of task-relevant context.\n\n\nDiscussion\n\nOur approach specifically addresses the emerging problem of the adaptation of recently proposed large visionlanguage models such as CLIP (Radford et al., 2021). There are some differences that distinguish our approach from the prompt learning methods developed in NLP for language models (e.g., GPT-3 (Brown et al., 2020)). First, the backbone architectures are clearly different for CLIP-like models and language models-the former take both visual and textual data as input and produce alignment scores used for image recognition, while the latter are tailored to handle textual data only. Second, the pre-training objectives are different: contrastive learning vs autoregressive learning. This would lead to different model behaviors and thus require different module designs. . These datasets constitute a comprehensive benchmark, which covers a diverse set of vision tasks including classification on generic objects, scenes, actions and fine-grained categories, as well as specialized tasks like recognizing textures and satellite imagery. Fig. 3 Main results of few-shot learning on the 11 datasets. Overall, CoOp effectively turns CLIP into a strong few-shot learner (solid lines), achieving significant improvements over zero-shot CLIP (stars) and performing favorably against the linear probe alternative (dashed lines). M denotes the context length. \"end\" or \"mid\" means putting the class token in the end or middle. CSC means class-specific context.\n\n\nExperiments\n\n\nFew-Shot Learning\n\nWe follow the few-shot evaluation protocol adopted in CLIP (Radford et al., 2021), using 1, 2, 4, 8 and 16 shots for training respectively and deploying models in the full test sets. The average results over three runs are reported for comparison.\n\nTraining Details CoOp has four versions: positioning the class token in the end or middle; unified context vs CSC. Unless otherwise stated, ResNet-50 (He et al., 2016) is used as the image encoder's backbone and the number of context tokens M is set to 16. Investigations on other design choices are discussed in Section 4.3. All models are built on top of CLIP's opensource code. 2 CoOp's context vectors are randomly initialized by drawing from a zero-mean Gaussian distribution with standard deviation equal to 0.02. Training is done with SGD and an initial learning rate of 0.002, which is decayed by the cosine annealing rule. The maximum epoch is set to 200 for 16/8 shots, 100 for 4/2 shots, and 50 for 1 shot (except for ImageNet where the maximum epoch is fixed to 50). To mitigate explosive gradients observed in the early training iterations, we use the warmup trick by fixing the learning rate to 1e\u22125, only during the first epoch.\n\n\nBaseline Methods\n\nWe compare CoOp with two baseline methods. The first is zero-shot CLIP, which is based on hand-crafted prompts. We follow the guideline of prompt engineering introduced by Radford et al. (2021). For generic objects and scenes, \"a photo of a [CLASS].\" is adopted. For fine-grained categories, taskrelevant context is added like \"a type of pet\" for Ox-fordPets and \"a type of food\" for Food101. When it comes to specialized tasks such as recognizing textures in DTD, the prompt is customized as \"[CLASS] texture.\" where the class names are adjectives like \"bubbly\" and \"dotted.\" See Appendix A for the details. The second baseline is the linear probe model. As suggested by Radford et al. (2021) and a recent study on few-shot learning (Tian et al., 2020), training a linear classifier on top of high-quality pre-trained models' features (like CLIP) can easily achieve performance that is on a par with that of state-of-the-art few-shot learning methods, which are often much more sophisticated. We follow the same training method used by Radford et al. (2021) to train the linear probe model. Figure 3 summarizes the results. Our default model is CLIP+CoOp with the class token positioned in the end. The two different ways of positioning the class token achieve similar performance as their curves highly overlap. From the average performance displayed in the top-left corner, we observe that CLIP+CoOp is a strong few-shot learner, requiring only two shots on average to obtain a decent margin over zero-shot CLIP. Given 16 shots for training, the average gap brought by CoOp can be further increased to around 15%. Figure 4 ranks the absolute improvements obtained by CoOp at 16 shots over hand-crafted prompts. Huge improvements are observed on specialized tasks namely EuroSAT and DTD where the increase in performance reaches over 45% and 20% respectively. The jumps in performance are also significant (those more than 2 https://github.com/openai/CLIP. 10%) on most fine-grained datasets including Flow-ers102, StanfordCars and FGVCAircraft, as well as on scene and action recognition datasets (i.e., SUN397 & UCF101). Since ImageNet is a challenging dataset that contains 1,000 classes, the 4.77% improvement is also noteworthy. In contrast, the increases on the two fine-grained datasets, OxfordPets and Food101, are less appealing. 3 By digging into CLIP+CoOp's curves on these two datasets in Figure 3, we find there is a loss of momentum in performance improvements even with more shots used, seemingly an overfitting problem. A potential solution is to impose higher regularization like increasing the weight decay. Nonetheless, the overall results are strong enough to serve as evidence of CoOp's capability of learning task-relevant prompts in a dataefficient manner.\n\n\nComparison with Hand-Crafted Prompts\n\nComparison with Linear Probe CLIP In terms of the overall performance (Figure 3, top-left), CLIP+CoOp demonstrates clear advantages over the linear probe model. The latter requires more than 4 shots on average to match the zero-shot's performance while CoOp's average gain at 4 shots is already impressive. It is also clear that the gaps in the extreme low-data regime such as one or two shots are much larger, suggesting that CoOp is much more effective than learning a linear classifier from scratch for few-shot learning. We also observe that the linear probe model is comparable to CLIP+CoOp on the two specialized tasks (DTD & EuroSAT) as well as on a couple of finegrained datasets (Flowers102 & FGVCAircraft)-this is not too surprising as the pre-trained CLIP space has been proved powerful, making the linear probe model a strong competitor. Nevertheless, CoOp's CSC version can beat the linear probe CLIP on the aforementioned datasets, and moreover, shows much better potential when more shots become available. We later show that CoOp obtains much stronger performance than the linear probe model in domain generalization.\n\nUnified vs Class-Specific Context On average, using unified context leads to better performance. In terms of when to apply CSC and when not to, we have the following suggestions. For generic objects (Im-ageNet & Caltech101), scenes (SUN397) and actions (UCF101), using unified context is clearly better. Unified context also works better on some fine-grained datasets including OxfordPets and Food101, but on others like StanfordCars, Flowers102 and FGVCAircraft the CSC version is preferred. CSC also yields better performance on the two specialized tasks, DTD and EuroSAT, at 16 shots in particular. However, CSC mostly underperforms unified context in challenging low-data scenarios (fewer than 8 shots), which makes sense because CSC has more parameters than unified context and needs more data for training.\n\n\nDomain Generalization\n\nSince CoOp requires training on a specific data distribution, it risks learning spurious correlations that are detrimental to generalization in unseen distributions (domains), as suggested in recent studies (Taori et al., 2020;Zhou et al., 2021). On the contrary, zero-shot CLIP is not tied to a specific data distribution and has exhibited strong robustness to distribution shifts (Radford et al., 2021). In this section, we aim to unveil how robust CoOp is to distribution shifts, in comparison to zero-shot CLIP and the linear probe model.\n\n\nDatasets\n\nThe source dataset is ImageNet. The target datasets are ImageNetV2 (Recht et al., 2019), ImageNet-Sketch , ImageNet-A (Hendrycks et al., 2021b) and ImageNet-R (Hendrycks et al., 2021a), all of which have compatible class names with ImageNet allowing seamless transfer for the prompts learned by CoOp. ImageNetV2 is a reproduced test set using different sources while following ImageNet's data collection process. ImageNet-Sketch contains sketch images belonging to the same 1,000 ImageNet classes. Both ImageNet-A and -R contain 200 classes derived from a subset of ImageNet's 1,000 classes. The former consists of real-world adversarially filtered images that cause current ImageNet classifiers to produce low results, whereas the latter features a rendition of the ImageNet classes in diverse image styles such as paintings, cartoons and sculptures.   Results Table 1 summarizes the results (with a variety of vision backbones). It is surprising that CoOp enhances CLIP's robustness to distribution shifts, despite the exposure to the source dataset. This suggests that the learned prompts are also generalizable. Moreover, it is interesting to see that using fewer context tokens leads to better robustness. In contrast, the linear probe model obtains much worse results on these target datasets, exposing its weakness in domain generalization. In Appendix B, we provide the domain generalization results on DOSCO-2k (Zhou et al., 2022b), a recently proposed benchmark focusing on contextual domain shift.\n\n\nFurther Analysis\n\nContext Length How many context tokens should be used? And is it better to have more context tokens?\n\nThe results in Section 4.2 suggest having a shorter context length benefits domain generalization (probably due to less overfitting as fewer parameters are learned).\n\nHere we study this hyperparameter for source datasets. Specifically, we repeat experiments on the 11 datasets by varying the context length from 4 to 8 to 16. The average results are shown in Figure 5(a), which indicate that having more context tokens leads to better performance and that positioning the class token in the middle gains more momentum with longer context length. To sum up, there is no golden rule for selecting perfect context length since one needs to balance between performance and robustness to distribution shift. Figure 5(b) summarizes the results on the 11 datasets using a variety of vision backbones covering both CNNs and ViTs. The results are expected: the more advanced the backbone, the better the performance. The gap between CoOp and handcrafted prompts is significant across all architectures.\n\n\nVision Backbones\n\nComparison with Prompt Ensembling The authors of CLIP (Radford et al., 2021) have suggested that additional improvements can be obtained by ensembling over multiple zero-shot classifiers generated using different hand-crafted prompts, such as \"a photo of the large [CLASS].\", \"a bad photo of the [CLASS].\" and \"a origami [CLASS].\", which reflect a different scale, view and abstraction respectively for an image. We are interested to know whether the prompts learned by CoOp can still maintain advantages when compared with prompt ensembling. For fair comparison, we use the select prompts from Radford et al. (2021), which have been extensively tuned on Ima-geNet, to construct the ensemble classifier. Table 2 shows the comparison and justifies the superiority of CoOp. Given the potential of prompt ensembling, future work could investigate how to improve CoOp from the ensembling perspective.\n\n\nComparison with Other Fine-tuning Methods\n\nWe further compare CoOp with other fine-tuning methods: i) fine-tuning CLIP's image encoder; ii) optimizing a transformation layer added to the text encoder's output; iii) optimizing a bias term added to the text encoder's output. The results are shown in Table 5. Obviously, fine-tuning the image encoder does not work well. Adding a transformation layer slightly improves upon the zero-shot model. Adding a bias term shows promising results, but still largely underperforms CoOp, which suggests that the gradients that went through the text encoder provide more useful information.\n\nInitialization We compare random initialization with manual initialization. The latter uses the embeddings of \"a photo of a\" to initialize the context vectors for the 11 datasets. For fair comparison, we also set the context length to 4 when using random initialization. Table 3 suggests a \"good\" initialization does not make much difference. Though further tuning of the initialization words might help, in practice we suggest using the simple random initialization method.\n\nInterpreting the Learned Prompts is difficult because the context vectors are optimized in a continuous space. We resort to an indirect way by searching within the vocabulary for words that are closest to the learned vectors based on the Euclidean distance. Note that CLIP (Radford et al., 2021) uses the BPE representation (Sennrich et al., 2016) for tokenization, so the vocabulary includes subwords that frequently appear in text, such as \"hu\" (subsumed by many words like \"hug\" and \"human\"). Table 4 shows the searched results on some datasets. We observe that a few words are somewhat relevant to the tasks, such as \"enjoyed\" for Food101, \"fluffy\" and \"paw\" for OxfordPets, and \"pretty\" for DTD. But when connecting all the nearest words together, the prompts do not make much sense. We also observe that when using manual initialization (like \"a photo of a\"), the nearest words for the converged vectors are mostly the ones used for initialization. We conjecture that the learned vectors might encode meanings that are beyond the existing vocabulary. Overall, we are unable to draw any firm conclusion based on the observations because using nearest words to interpret the learned prompts could be inaccuratethe semantics of the vectors is not necessarily correlated with the nearest words.\n\n\nConclusion, Limitations and Future Work\n\nLarge pre-trained vision-language models have shown surprisingly powerful capabilities in diverse downstream applications. However, these models, also called vision foundation models given their \"critically central yet incomplete\" nature (Bommasani et al., 2021), need to be adapted using automated techniques for better downstream performance and efficiency.\n\nOur research provides timely insights on how CLIPlike models can be turned into a data-efficient learner by using prompt learning, and reveals that despite being a learning-based approach, CoOp performs much better in domain generalization than manual prompts. The results serve as strong evidence that prompt learning has potential for large vision models. It is worth noting that our paper presents the first comprehensive study about adapting large vision models with prompt learning.\n\nThough the performance is excellent, the results of CoOp are relatively difficult to interpret, like other continuous prompt learning methods in NLP. The experiments also reveal that CoOp is sensitive to noisy labels given the weak performance on Food101.\n\nNevertheless, the simplicity of CoOp allows easy extension for future work and there remain many interesting questions to explore, such as cross-dataset transfer (Zhou et al., 2022a) and test-time adaptation . It would also be interesting to investigate more generic adaptation methods for mega-size vision models (Jia et al., 2022;Bahng et al., 2022;Gao et al., 2021). In summary, we hope the empirical findings and insights presented in this work could pave the way for future research on efficient adaptation methods for emerging foundation models, which is still a nascent research topic.  benchmark (Zhou et al., 2022b) contains 7 image recognition datasets, which cover a wide range of classification problems, such as generic object recognition, fine-grained recognition on aircraft models, and action recognition. Unlike existing domain generalization datasets where the domain labels are manually defined and often limited to image style variations, DOSCO-2k focuses on broader contextual domain shift, which is automatically detected by a neural network pre-trained on the Places dataset (Zhou et al., 2017). Following Zhou et al. (2022b), we use the 2k version where the training and validation splits in each dataset have 2,000 images in total (1,600 for training and 400 for validation).\n\n\nResults\n\nWe study three methods' domain generalization performance on DOSCO-2k: CLIP, CoOp and CoCoOp (Zhou et al., 2022a). All models are trained on the training set and the checkpoints with the best validation performance are used for final test in unseen domains. Table 7 shows the results of four different architectures. It is clear that the two learnable methods outperform the zero-shot method with a large margin, despite having only a small number of parameters to tune. CoCoOp beats CoOp on 4 out of 7 datasets but CoOp's average performance is higher. In summary, the results suggest that efficient adaptation methods like CoOp and\n\nCoCoOp have great potential in tackling transfer learning problems. \n\nFig. 2\n2Overview of Context Optimization (CoOp).\n\nDatasets\nWe select 11 publicly available image classification datasets used in CLIP: ImageNet (Deng et al., 2009), Caltech101 (Fei-Fei et al., 2004), Oxford-Pets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback and Zisserman, 2008), Food101 (Bossard et al., 2014), FGVCAircraft (Maji et al., 2013), SUN397 (Xiao et al., 2010), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019) and UCF101 (Soomro et al., 2012) (see Appendix A for their statistics)\n\nFig. 5\n5Investigations on CoOp's context length and various vision backbones.\n\na [\naCLASS]. a photo of [CLASS]. a photo of a [CLASS].[V]1 [V]2 \u2026 [V]M [CLASS]. \n\n82.68 \n\n80.81 \n\n86.29 \n\n91.83 \n\nCaltech101 \nPrompt \nAccuracy \n\nDescribable Textures (DTD) \n\na photo of a [CLASS]. \n\na photo of a [CLASS] texture. \n\n[CLASS] texture. \n\n[V]1 [V]2 \u2026 [V]M [CLASS]. \n\n39.83 \n\n40.25 \n\n42.32 \n\n63.58 \n\nPrompt \nAccuracy \n\nFlowers102 \na photo of a [CLASS]. \n\na flower photo of a [CLASS]. \n\na photo of a [CLASS], a type of flower. \n\n[V]1 [V]2 \u2026 [V]M [CLASS]. \n\n60.86 \n\n65.81 \n\n66.14 \n\n94.51 \n\nPrompt \nAccuracy \n\nEuroSAT \na photo of a [CLASS]. \n\na satellite photo of [CLASS]. \n\na centered satellite photo of [CLASS]. \n\n[V]1 [V]2 \u2026 [V]M [CLASS]. \n\n24.17 \n\n37.46 \n\n37.56 \n\n83.53 \n\nPrompt \nAccuracy \n\n(a) \n(b) \n\n(c) \n(d) \n\nFig. 1 Prompt engineering vs Context Optimization \n\n\nThe architecture of the image encoder can take the form of a CNN like ResNet-50 (He et al., 2016) or a ViT (Dosovitskiy et al., 2021). On the other hand, the text encoder is built on top of a Transformer (Vaswani et al., 2017) and aims to generate text representations from natural language.\n\n\nCLIP + CoOp (M=16, end) vs. Zero-Shot CLIPFig. 4Comparison with hand-crafted prompts.0 \n10 \n20 \n30 \n40 \nAbsolute improvement (%) \n\nFood101 \n\nOxfordPets \n\nImageNet \n\nCaltech101 \n\nSUN397 \n\nFGVCAircraft \n\nUCF101 \n\nStanfordCars \n\nDTD \n\nFlowers102 \n\nEuroSAT \n\n-2.64 \n\n+1.24 \n\n+4.77 \n\n+5.54 \n\n+10.74 \n\n+13.98 \n\n+14.25 \n\n+17.75 \n\n+21.26 \n\n+28.37 \n\n+45.97 \n\n\n\nTable 1\n1Comparison with zero-shot CLIP on robustness to distribution shift using different vision backbones. M : CoOp's context length.Source \nTarget \n\nMethod \nImageNet \n-V2 \n-Sketch \n-A \n-R \n\nResNet-50 \nZero-Shot CLIP \n58.18 \n51.34 \n33.32 \n21.65 \n56.00 \nLinear Probe CLIP \n55.87 \n45.97 \n19.07 \n12.74 \n34.86 \nCLIP + CoOp (M = 16) \n62.95 \n55.11 \n32.74 \n22.12 \n54.96 \nCLIP + CoOp (M = 4) \n63.33 \n55.40 \n34.67 \n23.06 \n56.60 \n\nResNet-101 \nZero-Shot CLIP \n61.62 \n54.81 \n38.71 \n28.05 \n64.38 \nLinear Probe CLIP \n59.75 \n50.05 \n26.80 \n19.44 \n47.19 \nCLIP + CoOp (M = 16) \n66.60 \n58.66 \n39.08 \n28.89 \n63.00 \nCLIP + CoOp (M = 4) \n65.98 \n58.60 \n40.40 \n29.60 \n64.98 \n\nViT-B/32 \nZero-Shot CLIP \n62.05 \n54.79 \n40.82 \n29.57 \n65.99 \nLinear Probe CLIP \n59.58 \n49.73 \n28.06 \n19.67 \n47.20 \nCLIP + CoOp (M = 16) \n66.85 \n58.08 \n40.44 \n30.62 \n64.45 \nCLIP + CoOp (M = 4) \n66.34 \n58.24 \n41.48 \n31.34 \n65.78 \n\nViT-B/16 \nZero-Shot CLIP \n66.73 \n60.83 \n46.15 \n47.77 \n73.96 \nLinear Probe CLIP \n65.85 \n56.26 \n34.77 \n35.68 \n58.43 \nCLIP + CoOp (M = 16) \n71.92 \n64.18 \n46.71 \n48.41 \n74.32 \nCLIP + CoOp (M = 4) \n71.73 \n64.56 \n47.89 \n49.93 \n75.14 \n\n\n\nTable 2\n2Comparison with prompt engineering and prompt ensembling on ImageNet using different vision backbones.Method \nResNet-50 \nResNet-101 \nViT-B/32 \nViT-B/16 \n\nPrompt engineering \n58.18 \n61.26 \n62.05 \n66.73 \nPrompt ensembling \n60.41 \n62.54 \n63.71 \n68.74 \nCoOp \n62.95 \n66.60 \n66.85 \n71.92 \n\nTable 3 Random vs manual initialization. \n\nAvg % \n\n[V] 1 [V] 2 [V] 3 [V] 4 \n72.65 \n\"a photo of a\" \n72.65 \n\n\n\nTable 4\n4The nearest words for each of the 16 context vectors learned by CoOp, with their distances shown in parentheses. N/A means non-Latin characters.# \nImageNet \nFood101 \nOxfordPets \nDTD \nUCF101 \n\n1 \npotd (1.7136) \nlc (0.6752) \ntosc (2.5952) \nboxed (0.9433) meteorologist (1.5377) \n2 \nthat (1.4015) enjoyed (0.5305) \njudge (1.2635) \nseed (1.0498) \nexe (0.9807) \n3 \nfilmed (1.2275) \nbeh (0.5390) \nfluffy (1.6099) \nanna (0.8127) \nparents (1.0654) \n4 \nfruit (1.4864) matches (0.5646) \ncart (1.3958) mountain (0.9509) \nmasterful (0.9528) \n5 \n,... (1.5863) nytimes (0.6993) \nharlan (2.2948) \neldest (0.7111) \nfe (1.3574) \n6\u00b0(1.7502) \nprou (0.5905) \npaw (1.3055) \npretty (0.8762) \nthof (1.2841) \n7 \nexcluded (1.2355) \nlower (0.5390) \nincase (1.2215) \nfaces (0.7872) \nwhere (0.9705) \n8 \ncold (1.4654) \nN/A \nbie (1.5454) \nhoney (1.8414) \nkristen (1.1921) \n9 \nstery (1.6085) minute (0.5672) \nsnuggle (1.1578) \nseries (1.6680) \nimam (1.1297) \n10 \nwarri (1.3055) \n\u223c (0.5529) \nalong (1.8298) \ncoca (1.5571) \nnear (0.8942) \n11 marvelcomics (1.5638) \nwell (0.5659) enjoyment (2.3495) \nmoon (1.2775) \ntummy (1.4303) \n12 \n.: (1.7387) \nends (0.6113) \njt (1.3726) \nlh (1.0382) \nhel (0.7644) \n13 \nN/A \nmis (0.5826) improving (1.3198) \nwon (0.9314) \nboop (1.0491) \n14 \nlation (1.5015) somethin (0.6041) \nsrsly (1.6759) \nreplied (1.1429) \nN/A \n15 \nmuh (1.4985) seminar (0.5274) asteroid (1.3395) \nsent (1.3173) \nfacial (1.4452) \n16 \n.# (1.9340) \nN/A \nN/A piedmont (1.5198) \nduring (1.1755) \n\n\n\nTable 5\n5CoOp vs other fine-tuning methods on ImageNet (w/ 16 shots). \u2206: difference with the zero-shot model. B Results on DOSCO-2k DOSCO-2k The DOSCO (DOmain Shift in COntext)ImageNet \n\u2206 \n\nZero-shot CLIP \n58.18 \n-\nLinear probe \n55.87 \n-2.31 \nFine-tuning CLIP's image encoder \n18.28 \n-39.90 \nOptimizing transformation layer (text) \n58.86 \n0.68 \nOptimizing bias (text) \n60.93 \n+2.75 \nCoOp \n62.95 \n+4.77 \n\n\n\nTable 6\n6Datasets statistics.Dataset \nClasses \nTrain \nVal \nTest \nHand-crafted prompt \n\nImageNet \n1,000 \n1.28M \nN/A 50,000 \n\"a photo of a [CLASS].\" \nCaltech101 \n100 \n4,128 \n1,649 \n2,465 \n\"a photo of a [CLASS].\" \nOxfordPets \n37 \n2,944 \n736 \n3,669 \n\"a photo of a [CLASS], a type of pet.\" \nStanfordCars \n196 \n6,509 \n1,635 \n8,041 \n\"a photo of a [CLASS].\" \nFlowers102 \n102 \n4,093 \n1,633 \n2,463 \n\"a photo of a [CLASS], a type of flower.\" \nFood101 \n101 50,500 20,200 30,300 \n\"a photo of [CLASS], a type of food.\" \nFGVCAircraft \n100 \n3,334 \n3,333 \n3,333 \"a photo of a [CLASS], a type of aircraft.\" \nSUN397 \n397 15,880 \n3,970 19,850 \n\"a photo of a [CLASS].\" \nDTD \n47 \n2,820 \n1,128 \n1,692 \n\"[CLASS] texture.\" \nEuroSAT \n10 13,500 \n5,400 \n8,100 \n\"a centered satellite photo of [CLASS].\" \nUCF101 \n101 \n7,639 \n1,898 \n3,783 \n\"a photo of a person doing [CLASS].\" \n\nCoOp is pronounced as /ku:p/.\nWe find that the negative results on Food101, for learningbased models including CoOp and linear probe, are caused by the noisy training data with \"intense colors and sometimes wrong labels\"(Bossard et al., 2014).\nAcknowledgements This work is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and inkind contribution from the industry partner(s). Corresponding author: Ziwei Liu (ziwei.liu@ntu.edu.sg).Appendix A Datasets DetailsThe detailed statistics of the 11 datasets, as well as the four variants of ImageNet, are shown inTable 6. The handcrafted prompts used for zero-shot CLIP are also detailed in the table. For Caltech101, the \"BACKGROUND Google\" and \"Faces easy\" classes are discarded. For the video dataset, UCF101, the middle frame of each video is used as input to the image encoder.\nH Bahng, A Jahanian, S Sankaranarayanan, P Isola, arXiv:220317274Visual prompting: Modifying pixel space to adapt pretrained models. arXiv preprintBahng H, Jahanian A, Sankaranarayanan S, Isola P (2022) Visual prompting: Modifying pixel space to adapt pre- trained models. arXiv preprint arXiv:220317274\n\nR Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:210807258On the opportunities and risks of foundation models. arXiv preprintBommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E, et al. (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:210807258\n\nFood-101-mining discriminative components with random forests. L Bossard, M Guillaumin, L ; Eccv Van Gool, T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, arXiv:200514165arXiv preprintLanguage models are few-shot learnersBossard L, Guillaumin M, Van Gool L (2014) Food-101- mining discriminative components with random forests. In: ECCV Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhari- wal P, Neelakantan A, Shyam P, Sastry G, Askell A, et al. (2020) Language models are few-shot learners. arXiv preprint arXiv:200514165\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. T Chen, S Kornblith, M Norouzi, G Hinton, M Icml Cimpoi, S Maji, I Kokkinos, Mohamed S Vedaldi, A ; Cvpr Deng, J Dong, W Socher, R Li, L J Li, K Fei-Fei L ; Cvpr Dosovitskiy, A Beyer, L Kolesnikov, A Weissenborn, D Zhai, X Unterthiner, T Dehghani, M Minderer, M Heigold, G Gelly, S , CVPR-W ImageNetV2. ICLR Elhoseiny M, Saleh B, Elgammal A10ICCV Fei-Fei LChen T, Kornblith S, Norouzi M, Hinton G (2020) A sim- ple framework for contrastive learning of visual represen- tations. In: ICML Cimpoi M, Maji S, Kokkinos I, Mohamed S, Vedaldi A (2014) Describing textures in the wild. In: CVPR Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009) Imagenet: A large-scale hierarchical image database. In: CVPR Desai K, Johnson J (2021) Virtex: Learning visual represen- tations from textual annotations. In: CVPR Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, et al. (2021) An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR Elhoseiny M, Saleh B, Elgammal A (2013) Write a classifier: Zero-shot learning using purely textual descriptions. In: ICCV Fei-Fei L, Fergus R, Perona P (2004) Learning generative visual models from few training examples: An incremen- tal bayesian approach tested on 101 object categories. In: CVPR-W ImageNetV2 1,000\n\n. N/A N/A 10, ImageNet-Sketch. 10N/A N/A 10,000 \"a photo of a [CLASS].\" ImageNet-Sketch 1,000\n\n. N/A N/A 50, 7ImageNet-R 200 N/A N/A 30,000 \"a photo of a [CLASSN/A N/A 50,889 \"a photo of a [CLASS].\" ImageNet-A 200 N/A N/A 7,500 \"a photo of a [CLASS].\" ImageNet-R 200 N/A N/A 30,000 \"a photo of a [CLASS].\"\n\nAmong the three approaches, CoOp and its follow-up, CoCoOp, contain learnable components while CLIP here denotes the zero-shot model. Both CoOp and CoCoOp use four learnable context tokens initialized with the word embeddings of. Bold denotes the best performance on each dataset for a specific architecture. P-Air P-Cars P-Ctech P-Ins P-Mam P-Pets P-UCF Avg. Table 7 Domain generalization results on DOSCO-2k, a recently proposed benchmark focusing on broader contextual domain shiftTable 7 Domain generalization results on DOSCO-2k, a recently proposed benchmark focusing on broader contextual domain shift. Among the three approaches, CoOp and its follow-up, CoCoOp, contain learnable components while CLIP here denotes the zero-shot model. Both CoOp and CoCoOp use four learnable context tokens initialized with the word embeddings of \"a photo of a\". Bold denotes the best performance on each dataset for a specific architecture. P-Air P-Cars P-Ctech P-Ins P-Mam P-Pets P-UCF Avg\n\nDevise: A deep visual-semantic embedding model. A Frome, G Corrado, J Shlens, S Bengio, J Dean, M Ranzato, T Mikolov, E Rumetshofer, V Tran, H Ramsauer, F Tang, J Lehner, D Kreil, M Kopp, G Klambauer, A Bitto-Nemling, arXiv:211011316Cloob: Modern hopfield networks with infoloob outperform clip. arXiv preprintFrome A, Corrado G, Shlens J, Bengio S, Dean J, Ranzato M, Mikolov T (2013) Devise: A deep visual-semantic em- bedding model. In: NeurIPS F\u00fcrst A, Rumetshofer E, Tran V, Ramsauer H, Tang F, Lehner J, Kreil D, Kopp M, Klambauer G, Bitto-Nemling A, et al. (2021) Cloob: Modern hopfield networks with in- foloob outperform clip. arXiv preprint arXiv:211011316\n\nP Gao, S Geng, R Zhang, T Ma, R Fang, Y Zhang, H Li, Y Qiao, arXiv:211004544Clip-adapter: Better vision-language models with feature adapters. arXiv preprintGao P, Geng S, Zhang R, Ma T, Fang R, Zhang Y, Li H, Qiao Y (2021) Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:211004544\n\nMaking pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, arXiv:201215723arXiv preprintGao T, Fisch A, Chen D (2020) Making pre-trained lan- guage models better few-shot learners. arXiv preprint arXiv:201215723\n\nEslami SMA, van den Oord A (2020) Data-efficient image recognition with contrastive predictive coding. L Gomez, Y Patel, M Rusi\u00f1ol, D Karatzas, C Jawahar, K Cvpr He, X Zhang, S Ren, J Sun, K Cvpr He, H Fan, Y Wu, S Xie, R ; Cvpr Girshick, P Helber, B Bischke, A Dengel, D ; Borth, A Srinivas, J D Fauw, A Razavi, C ; Icml Doersch, D Hendrycks, S Basart, N Mu, S Kadavath, F Wang, E Dorundo, R Desai, T Zhu, S Parajuli, M Guo, D Song, J Steinhardt, J Gilmer, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing H\u00e9naff OJ. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. The many faces of robustness: A critical analysis of out-of-distribution generalizationGomez L, Patel Y, Rusi\u00f1ol M, Karatzas D, Jawahar C (2017) Self-supervised learning of visual features through embed- ding images into text topic spaces. In: CVPR He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: CVPR He K, Fan H, Wu Y, Xie S, Girshick R (2020) Momentum contrast for unsupervised visual representation learning. In: CVPR Helber P, Bischke B, Dengel A, Borth D (2019) Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing H\u00e9naff OJ, Srinivas A, Fauw JD, Razavi A, Doersch C, Es- lami SMA, van den Oord A (2020) Data-efficient image recognition with contrastive predictive coding. In: ICML Hendrycks D, Basart S, Mu N, Kadavath S, Wang F, Dorundo E, Desai R, Zhu T, Parajuli S, Guo M, Song D, Steinhardt J, Gilmer J (2021a) The many faces of robustness: A crit- ical analysis of out-of-distribution generalization. ICCV\n\nScaling up visual and vision-language representation learning with noisy text supervision. D Hendrycks, K Zhao, S Basart, J Steinhardt, D ; Cvpr Song, C Jia, Y Yang, Y Xia, Y T Chen, Z Parekh, H Pham, Q V Le, Y Sung, Z Li, T Duerig, M Icml Jia, L Tang, B C Chen, C Cardie, S Belongie, B Hariharan, S N Lim, arXiv:220312119Visual prompt tuning. arXiv preprintNatural adversarial examplesHendrycks D, Zhao K, Basart S, Steinhardt J, Song D (2021b) Natural adversarial examples. In: CVPR Jia C, Yang Y, Xia Y, Chen YT, Parekh Z, Pham H, Le QV, Sung Y, Li Z, Duerig T (2021) Scaling up visual and vision-language representation learning with noisy text su- pervision. In: ICML Jia M, Tang L, Chen BC, Cardie C, Belongie S, Hariharan B, Lim SN (2022) Visual prompt tuning. arXiv preprint arXiv:220312119\n\nLearning visual features from large weakly supervised data. Z Jiang, F F Xu, J Araki, G Neubig, A Acl Joulin, L Van Der Maaten, A Jabri, N ; Eccv Vasilache, J Krause, M Stark, J Deng, L Fei-Fei, ICCV-W. 3object representations for fine-grained categorizationJiang Z, Xu FF, Araki J, Neubig G (2020) How can we know what language models know? ACL Joulin A, Van Der Maaten L, Jabri A, Vasilache N (2016) Learning visual features from large weakly supervised data. In: ECCV Krause J, Stark M, Deng J, Fei-Fei L (2013) 3d object repre- sentations for fine-grained categorization. In: ICCV-W\n\nPredicting deep zero-shot convolutional neural networks using textual descriptions. Lei Ba, J Swersky, K Fidler, S , arXiv:210408691Constant N (2021) The power of scale for parameter-efficient prompt tuning. ICCV Lester B, Al-Rfou RarXiv preprintLei Ba J, Swersky K, Fidler S, et al. (2015) Predicting deep zero-shot convolutional neural networks using textual de- scriptions. In: ICCV Lester B, Al-Rfou R, Constant N (2021) The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:210408691\n\nLearning visual n-grams from web data. A Li, A Jabri, A Joulin, L Van Der Maaten, arXiv:210100190ICCV Li XL, Liang P (2021) Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprintLi A, Jabri A, Joulin A, van der Maaten L (2017) Learning visual n-grams from web data. In: ICCV Li XL, Liang P (2021) Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:210100190\n\nSupervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. Y Li, F Liang, L Zhao, Y Cui, W Ouyang, J Shao, F Yu, J Yan, arXiv:211005208arXiv preprintLi Y, Liang F, Zhao L, Cui Y, Ouyang W, Shao J, Yu F, Yan J (2021) Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:211005208\n\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, arXiv:210713586arXiv preprintLiu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G (2021a) Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:210713586\n\n. X Liu, Y Zheng, Z Du, M Ding, Y Qian, Z Yang, J Tang, arXiv:210310385Gpt understands, too. arXiv preprintLiu X, Zheng Y, Du Z, Ding M, Qian Y, Yang Z, Tang J (2021b) Gpt understands, too. arXiv preprint arXiv:210310385\n\nFine-grained visual classification of aircraft. S Maji, E Rahtu, J Kannala, M Blaschko, A Vedaldi, arXiv:13065151arXiv preprintMaji S, Rahtu E, Kannala J, Blaschko M, Vedaldi A (2013) Fine-grained visual classification of aircraft. arXiv preprint arXiv:13065151\n\nAutoprompt: Eliciting knowledge from language models with automatically generated prompts. M E Nilsback, A Zisserman, O M Icvgip Parkhi, A Vedaldi, A Zisserman, C Jawahar, F Cvpr Petroni, T Rockt\u00e4schel, P Lewis, A Bakhtin, Y Wu, A H Miller, S ; Emnlp Riedel, A Radford, J W Kim, C Hallacy, Ramesh A Goh, G Agarwal, S Sastry, G Askell, A Mishkin, P Clark, J , arXiv:211204482Flava: A foundational language and vision alignment model. arXiv preprintLearning transferable visual models from natural language supervisionNilsback ME, Zisserman A (2008) Automated flower classifi- cation over a large number of classes. In: ICVGIP Parkhi OM, Vedaldi A, Zisserman A, Jawahar C (2012) Cats and dogs. In: CVPR Petroni F, Rockt\u00e4schel T, Lewis P, Bakhtin A, Wu Y, Miller AH, Riedel S (2019) Language models as knowledge bases? In: EMNLP Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, Sastry G, Askell A, Mishkin P, Clark J, et al. (2021) Learning transferable visual models from natural language supervision. In: ICML Recht B, Roelofs R, Schmidt L, Shankar V (2019) Do ima- genet classifiers generalize to imagenet? In: ICML Sennrich R, Haddow B, Birch A (2016) Neural machine trans- lation of rare words with subword units. In: ACL Shin T, Razeghi Y, Logan IV RL, Wallace E, Singh S (2020) Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In: EMNLP Singh A, Hu R, Goswami V, Couairon G, Galuba W, Rohrbach M, Kiela D (2021) Flava: A foundational language and vision alignment model. arXiv preprint arXiv:211204482\n\nZero-shot learning through cross-modal transfer. R Socher, M Ganjoo, H Sridhar, O Bastani, C D Manning, A Y Ng, arXiv:12120402NeurIPS Soomro K, Zamir AR, Shah M (2012) Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprintSocher R, Ganjoo M, Sridhar H, Bastani O, Manning CD, Ng AY (2013) Zero-shot learning through cross-modal trans- fer. In: NeurIPS Soomro K, Zamir AR, Shah M (2012) Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:12120402\n\nMeasuring robustness to natural distribution shifts in image classification. R Taori, A Dave, V Shankar, N Carlini, B Recht, L Schmidt, Y ; Neurips Tian, Y Wang, D Krishnan, J B Tenenbaum, P ; Eccv Isola, A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I ; Neurips Polosukhin, D Wang, E Shelhamer, S Liu, B Olshausen, T Darrell, arXiv:200610726Fully test-time adaptation by entropy minimization. TentarXiv preprintAttention is all you needTaori R, Dave A, Shankar V, Carlini N, Recht B, Schmidt L (2020) Measuring robustness to natural distribution shifts in image classification. In: NeurIPS Tian Y, Wang Y, Krishnan D, Tenenbaum JB, Isola P (2020) Rethinking few-shot image classification: a good embed- ding is all you need? In: ECCV Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is all you need. In: NeurIPS Wang D, Shelhamer E, Liu S, Olshausen B, Darrell T (2020) Tent: Fully test-time adaptation by entropy minimization. arXiv preprint arXiv:200610726\n\nLearning robust global representations by penalizing local predictive power. H Wang, S Ge, Z Lipton, Ep ; Neurips Xing, J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, L Cvpr Yuan, Chen D Chen, Y L Codella, N Dai, X Gao, J Hu, H Huang, X Li, B Li, C , arXiv:211111432A new foundation model for computer vision. FlorencearXiv preprintSun database: Large-scale scene recognition from abbey to zooWang H, Ge S, Lipton Z, Xing EP (2019) Learning robust global representations by penalizing local predictive power. In: NeurIPS Xiao J, Hays J, Ehinger KA, Oliva A, Torralba A (2010) Sun database: Large-scale scene recognition from abbey to zoo. In: CVPR Yuan L, Chen D, Chen YL, Codella N, Dai X, Gao J, Hu H, Huang X, Li B, Li C, et al. (2021) Florence: A new foundation model for computer vision. arXiv preprint arXiv:211111432\n\nY Zhang, H Jiang, Y Miura, C D Manning, C P Langlotz, arXiv:201000747Contrastive learning of medical visual representations from paired images and text. arXiv preprintZhang Y, Jiang H, Miura Y, Manning CD, Langlotz CP (2020) Contrastive learning of medical visual represen- tations from paired images and text. arXiv preprint arXiv:201000747\n\nFactual probing is [mask]: Learning vs. learning to recall. Z Zhong, D Friedman, D ; Naacl Chen, B Zhou, A Lapedriza, A Khosla, A Oliva, A Torralba, IEEE transactions. 406Places: A 10 million image database for scene recognitionZhong Z, Friedman D, Chen D (2021) Factual probing is [mask]: Learning vs. learning to recall. In: NAACL Zhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017) Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intel- ligence 40(6):1452-1464\n\nK Zhou, Z Liu, Y Qiao, T Xiang, C C Loy, arXiv:210302503Domain generalization: A survey. arXiv preprintZhou K, Liu Z, Qiao Y, Xiang T, Loy CC (2021) Domain generalization: A survey. arXiv preprint arXiv:210302503\n\nConditional prompt learning for vision-language models. K Zhou, J Yang, C C Loy, Z Liu, arXiv:220305557arXiv preprintZhou K, Yang J, Loy CC, Liu Z (2022a) Conditional prompt learning for vision-language models. arXiv preprint arXiv:220305557\n\n. K Zhou, Y Zhang, Y Zang, J Yang, C C Loy, Z Liu, arXiv:220907521On-device domain generalization. arXiv preprintZhou K, Zhang Y, Zang Y, Yang J, Loy CC, Liu Z (2022b) On-device domain generalization. arXiv preprint arXiv:220907521\n", "annotations": {"author": "[{\"end\":86,\"start\":49},{\"end\":121,\"start\":87},{\"end\":134,\"start\":122},{\"end\":163,\"start\":135},{\"end\":195,\"start\":164},{\"end\":209,\"start\":196},{\"end\":224,\"start\":210},{\"end\":241,\"start\":225},{\"end\":252,\"start\":242},{\"end\":309,\"start\":253},{\"end\":360,\"start\":310},{\"end\":411,\"start\":361},{\"end\":456,\"start\":412}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":57},{\"end\":97,\"start\":89},{\"end\":133,\"start\":129},{\"end\":145,\"start\":142},{\"end\":173,\"start\":170},{\"end\":208,\"start\":204},{\"end\":223,\"start\":219},{\"end\":240,\"start\":237},{\"end\":251,\"start\":248}]", "author_first_name": "[{\"end\":56,\"start\":49},{\"end\":88,\"start\":87},{\"end\":126,\"start\":122},{\"end\":128,\"start\":127},{\"end\":141,\"start\":135},{\"end\":169,\"start\":164},{\"end\":203,\"start\":196},{\"end\":218,\"start\":210},{\"end\":229,\"start\":225},{\"end\":236,\"start\":230},{\"end\":247,\"start\":242}]", "author_affiliation": "[{\"end\":308,\"start\":254},{\"end\":359,\"start\":311},{\"end\":410,\"start\":362},{\"end\":455,\"start\":413}]", "title": "[{\"end\":46,\"start\":1},{\"end\":502,\"start\":457}]", "venue": null, "abstract": "[{\"end\":2399,\"start\":589}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2603,\"start\":2586},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2628,\"start\":2603},{\"end\":2762,\"start\":2738},{\"end\":2796,\"start\":2766},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3424,\"start\":3402},{\"end\":3453,\"start\":3429},{\"end\":3562,\"start\":3556},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4187,\"start\":4165},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4204,\"start\":4187},{\"end\":4223,\"start\":4204},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4239,\"start\":4223},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4258,\"start\":4239},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4276,\"start\":4258},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5777,\"start\":5758},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5796,\"start\":5777},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5815,\"start\":5796},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7653,\"start\":7634},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8911,\"start\":8887},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9249,\"start\":9227},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9266,\"start\":9249},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9285,\"start\":9266},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9304,\"start\":9285},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9322,\"start\":9304},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9436,\"start\":9414},{\"end\":9465,\"start\":9441},{\"end\":9600,\"start\":9565},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9674,\"start\":9658},{\"end\":9694,\"start\":9674},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9975,\"start\":9956},{\"end\":9998,\"start\":9975},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10150,\"start\":10131},{\"end\":10211,\"start\":10187},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10231,\"start\":10211},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10326,\"start\":10306},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10375,\"start\":10354},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10394,\"start\":10375},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10438,\"start\":10421},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10907,\"start\":10888},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10926,\"start\":10907},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10945,\"start\":10926},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10964,\"start\":10945},{\"end\":10984,\"start\":10964},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11001,\"start\":10984},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11019,\"start\":11001},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11241,\"start\":11222},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11615,\"start\":11595},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11634,\"start\":11615},{\"end\":11654,\"start\":11634},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11903,\"start\":11885},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16789,\"start\":16765},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17614,\"start\":17591},{\"end\":19176,\"start\":19169},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22343,\"start\":22321},{\"end\":22505,\"start\":22479},{\"end\":25086,\"start\":25080},{\"end\":25141,\"start\":25134},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25586,\"start\":25565},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25646,\"start\":25627},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25951,\"start\":25930},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29915,\"start\":29895},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29933,\"start\":29915},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30092,\"start\":30070},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30386,\"start\":30361},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30427,\"start\":30402},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31683,\"start\":31663},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32964,\"start\":32942},{\"end\":33160,\"start\":33153},{\"end\":33191,\"start\":33184},{\"end\":33216,\"start\":33209},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33504,\"start\":33483},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35238,\"start\":35215},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36493,\"start\":36469},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37519,\"start\":37500},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37670,\"start\":37652},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37689,\"start\":37670},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37706,\"start\":37689},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37962,\"start\":37942},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38455,\"start\":38436},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38486,\"start\":38467},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38763,\"start\":38743},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45883,\"start\":45861}]", "figure": "[{\"attributes\":{\"id\":\"fig_4\"},\"end\":39403,\"start\":39354},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39890,\"start\":39404},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39969,\"start\":39891},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40744,\"start\":39970},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41038,\"start\":40745},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41391,\"start\":41039},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42506,\"start\":41392},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42908,\"start\":42507},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44385,\"start\":42909},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44791,\"start\":44386},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45640,\"start\":44792}]", "paragraph": "[{\"end\":3348,\"start\":2415},{\"end\":3739,\"start\":3350},{\"end\":4521,\"start\":3741},{\"end\":5675,\"start\":4523},{\"end\":6856,\"start\":5677},{\"end\":7821,\"start\":6858},{\"end\":7871,\"start\":7823},{\"end\":8655,\"start\":7873},{\"end\":8982,\"start\":8657},{\"end\":9323,\"start\":9024},{\"end\":9845,\"start\":9325},{\"end\":10503,\"start\":9847},{\"end\":10662,\"start\":10505},{\"end\":11020,\"start\":10689},{\"end\":11529,\"start\":11022},{\"end\":11970,\"start\":11531},{\"end\":12075,\"start\":11972},{\"end\":14631,\"start\":14628},{\"end\":15080,\"start\":14642},{\"end\":16993,\"start\":16658},{\"end\":17216,\"start\":17040},{\"end\":18066,\"start\":17218},{\"end\":18571,\"start\":18068},{\"end\":19318,\"start\":18595},{\"end\":19839,\"start\":19387},{\"end\":20164,\"start\":19864},{\"end\":20370,\"start\":20166},{\"end\":20800,\"start\":20411},{\"end\":21119,\"start\":20875},{\"end\":21386,\"start\":21179},{\"end\":21528,\"start\":21413},{\"end\":21749,\"start\":21587},{\"end\":22169,\"start\":21751},{\"end\":23644,\"start\":22184},{\"end\":23927,\"start\":23680},{\"end\":24872,\"start\":23929},{\"end\":27674,\"start\":24893},{\"end\":28848,\"start\":27715},{\"end\":29662,\"start\":28850},{\"end\":30230,\"start\":29688},{\"end\":31751,\"start\":30243},{\"end\":31872,\"start\":31772},{\"end\":32039,\"start\":31874},{\"end\":32867,\"start\":32041},{\"end\":33784,\"start\":32888},{\"end\":34413,\"start\":33830},{\"end\":34889,\"start\":34415},{\"end\":36187,\"start\":34891},{\"end\":36590,\"start\":36231},{\"end\":37079,\"start\":36592},{\"end\":37336,\"start\":37081},{\"end\":38638,\"start\":37338},{\"end\":39283,\"start\":38650},{\"end\":39353,\"start\":39285}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12847,\"start\":12076},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13640,\"start\":12847},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14627,\"start\":13640},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14641,\"start\":14632},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15864,\"start\":15081},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16657,\"start\":15864},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19386,\"start\":19319},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20410,\"start\":20371},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20874,\"start\":20801},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21178,\"start\":21120},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21586,\"start\":21529}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31112,\"start\":31105},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33599,\"start\":33592},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34093,\"start\":34086},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34693,\"start\":34686},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35394,\"start\":35387},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38915,\"start\":38908}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2413,\"start\":2401},{\"attributes\":{\"n\":\"2\"},\"end\":8997,\"start\":8985},{\"attributes\":{\"n\":\"2.1\"},\"end\":9022,\"start\":9000},{\"attributes\":{\"n\":\"2.2\"},\"end\":10687,\"start\":10665},{\"attributes\":{\"n\":\"3\"},\"end\":17007,\"start\":16996},{\"attributes\":{\"n\":\"3.1\"},\"end\":17038,\"start\":17010},{\"end\":18593,\"start\":18574},{\"attributes\":{\"n\":\"3.2\"},\"end\":19862,\"start\":19842},{\"end\":21411,\"start\":21389},{\"attributes\":{\"n\":\"3.3\"},\"end\":22182,\"start\":22172},{\"attributes\":{\"n\":\"4\"},\"end\":23658,\"start\":23647},{\"attributes\":{\"n\":\"4.1\"},\"end\":23678,\"start\":23661},{\"end\":24891,\"start\":24875},{\"end\":27713,\"start\":27677},{\"attributes\":{\"n\":\"4.2\"},\"end\":29686,\"start\":29665},{\"end\":30241,\"start\":30233},{\"attributes\":{\"n\":\"4.3\"},\"end\":31770,\"start\":31754},{\"end\":32886,\"start\":32870},{\"end\":33828,\"start\":33787},{\"attributes\":{\"n\":\"5\"},\"end\":36229,\"start\":36190},{\"end\":38648,\"start\":38641},{\"end\":39361,\"start\":39355},{\"end\":39413,\"start\":39405},{\"end\":39898,\"start\":39892},{\"end\":39974,\"start\":39971},{\"end\":41400,\"start\":41393},{\"end\":42515,\"start\":42508},{\"end\":42917,\"start\":42910},{\"end\":44394,\"start\":44387},{\"end\":44800,\"start\":44793}]", "table": "[{\"end\":40744,\"start\":40025},{\"end\":41391,\"start\":41126},{\"end\":42506,\"start\":41529},{\"end\":42908,\"start\":42619},{\"end\":44385,\"start\":43063},{\"end\":44791,\"start\":44563},{\"end\":45640,\"start\":44822}]", "figure_caption": "[{\"end\":39403,\"start\":39363},{\"end\":39890,\"start\":39414},{\"end\":39969,\"start\":39900},{\"end\":40025,\"start\":39976},{\"end\":41038,\"start\":40747},{\"end\":41126,\"start\":41041},{\"end\":41529,\"start\":41402},{\"end\":42619,\"start\":42517},{\"end\":43063,\"start\":42919},{\"end\":44563,\"start\":44396},{\"end\":44822,\"start\":44802}]", "figure_ref": "[{\"end\":4891,\"start\":4879},{\"end\":5149,\"start\":5141},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":6137,\"start\":6129},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20111,\"start\":20103},{\"end\":23235,\"start\":23229},{\"end\":25993,\"start\":25985},{\"end\":26518,\"start\":26510},{\"end\":27304,\"start\":27296},{\"end\":27794,\"start\":27785},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32241,\"start\":32233},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32585,\"start\":32577}]", "bib_author_first_name": "[{\"end\":46601,\"start\":46600},{\"end\":46610,\"start\":46609},{\"end\":46622,\"start\":46621},{\"end\":46642,\"start\":46641},{\"end\":46906,\"start\":46905},{\"end\":46919,\"start\":46918},{\"end\":46921,\"start\":46920},{\"end\":46931,\"start\":46930},{\"end\":46940,\"start\":46939},{\"end\":46950,\"start\":46949},{\"end\":46959,\"start\":46958},{\"end\":46970,\"start\":46969},{\"end\":46972,\"start\":46971},{\"end\":46985,\"start\":46984},{\"end\":46993,\"start\":46992},{\"end\":47005,\"start\":47004},{\"end\":47372,\"start\":47371},{\"end\":47383,\"start\":47382},{\"end\":47404,\"start\":47396},{\"end\":47416,\"start\":47415},{\"end\":47418,\"start\":47417},{\"end\":47427,\"start\":47426},{\"end\":47435,\"start\":47434},{\"end\":47444,\"start\":47443},{\"end\":47455,\"start\":47454},{\"end\":47465,\"start\":47464},{\"end\":47477,\"start\":47476},{\"end\":47492,\"start\":47491},{\"end\":47501,\"start\":47500},{\"end\":47511,\"start\":47510},{\"end\":47970,\"start\":47969},{\"end\":47978,\"start\":47977},{\"end\":47991,\"start\":47990},{\"end\":48002,\"start\":48001},{\"end\":48012,\"start\":48011},{\"end\":48027,\"start\":48026},{\"end\":48035,\"start\":48034},{\"end\":48053,\"start\":48046},{\"end\":48055,\"start\":48054},{\"end\":48073,\"start\":48065},{\"end\":48081,\"start\":48080},{\"end\":48089,\"start\":48088},{\"end\":48099,\"start\":48098},{\"end\":48105,\"start\":48104},{\"end\":48107,\"start\":48106},{\"end\":48113,\"start\":48112},{\"end\":48145,\"start\":48144},{\"end\":48154,\"start\":48153},{\"end\":48168,\"start\":48167},{\"end\":48183,\"start\":48182},{\"end\":48191,\"start\":48190},{\"end\":48206,\"start\":48205},{\"end\":48218,\"start\":48217},{\"end\":48230,\"start\":48229},{\"end\":48241,\"start\":48240},{\"end\":48250,\"start\":48249},{\"end\":50657,\"start\":50656},{\"end\":50666,\"start\":50665},{\"end\":50677,\"start\":50676},{\"end\":50687,\"start\":50686},{\"end\":50697,\"start\":50696},{\"end\":50705,\"start\":50704},{\"end\":50716,\"start\":50715},{\"end\":50727,\"start\":50726},{\"end\":50742,\"start\":50741},{\"end\":50750,\"start\":50749},{\"end\":50762,\"start\":50761},{\"end\":50770,\"start\":50769},{\"end\":50780,\"start\":50779},{\"end\":50789,\"start\":50788},{\"end\":50797,\"start\":50796},{\"end\":50810,\"start\":50809},{\"end\":51277,\"start\":51276},{\"end\":51284,\"start\":51283},{\"end\":51292,\"start\":51291},{\"end\":51301,\"start\":51300},{\"end\":51307,\"start\":51306},{\"end\":51315,\"start\":51314},{\"end\":51324,\"start\":51323},{\"end\":51330,\"start\":51329},{\"end\":51661,\"start\":51660},{\"end\":51668,\"start\":51667},{\"end\":51677,\"start\":51676},{\"end\":51942,\"start\":51941},{\"end\":51951,\"start\":51950},{\"end\":51960,\"start\":51959},{\"end\":51971,\"start\":51970},{\"end\":51983,\"start\":51982},{\"end\":51994,\"start\":51993},{\"end\":52005,\"start\":52004},{\"end\":52014,\"start\":52013},{\"end\":52021,\"start\":52020},{\"end\":52028,\"start\":52027},{\"end\":52039,\"start\":52038},{\"end\":52046,\"start\":52045},{\"end\":52052,\"start\":52051},{\"end\":52066,\"start\":52058},{\"end\":52078,\"start\":52077},{\"end\":52088,\"start\":52087},{\"end\":52099,\"start\":52098},{\"end\":52111,\"start\":52108},{\"end\":52120,\"start\":52119},{\"end\":52132,\"start\":52131},{\"end\":52134,\"start\":52133},{\"end\":52142,\"start\":52141},{\"end\":52159,\"start\":52151},{\"end\":52170,\"start\":52169},{\"end\":52183,\"start\":52182},{\"end\":52193,\"start\":52192},{\"end\":52199,\"start\":52198},{\"end\":52211,\"start\":52210},{\"end\":52219,\"start\":52218},{\"end\":52230,\"start\":52229},{\"end\":52239,\"start\":52238},{\"end\":52246,\"start\":52245},{\"end\":52258,\"start\":52257},{\"end\":52265,\"start\":52264},{\"end\":52273,\"start\":52272},{\"end\":52287,\"start\":52286},{\"end\":53658,\"start\":53657},{\"end\":53671,\"start\":53670},{\"end\":53679,\"start\":53678},{\"end\":53689,\"start\":53688},{\"end\":53710,\"start\":53702},{\"end\":53718,\"start\":53717},{\"end\":53725,\"start\":53724},{\"end\":53733,\"start\":53732},{\"end\":53740,\"start\":53739},{\"end\":53742,\"start\":53741},{\"end\":53750,\"start\":53749},{\"end\":53760,\"start\":53759},{\"end\":53768,\"start\":53767},{\"end\":53770,\"start\":53769},{\"end\":53776,\"start\":53775},{\"end\":53784,\"start\":53783},{\"end\":53790,\"start\":53789},{\"end\":53800,\"start\":53799},{\"end\":53812,\"start\":53811},{\"end\":53820,\"start\":53819},{\"end\":53822,\"start\":53821},{\"end\":53830,\"start\":53829},{\"end\":53840,\"start\":53839},{\"end\":53852,\"start\":53851},{\"end\":53865,\"start\":53864},{\"end\":53867,\"start\":53866},{\"end\":54427,\"start\":54426},{\"end\":54436,\"start\":54435},{\"end\":54438,\"start\":54437},{\"end\":54444,\"start\":54443},{\"end\":54453,\"start\":54452},{\"end\":54463,\"start\":54462},{\"end\":54477,\"start\":54476},{\"end\":54495,\"start\":54494},{\"end\":54511,\"start\":54503},{\"end\":54524,\"start\":54523},{\"end\":54534,\"start\":54533},{\"end\":54543,\"start\":54542},{\"end\":54551,\"start\":54550},{\"end\":55041,\"start\":55038},{\"end\":55047,\"start\":55046},{\"end\":55058,\"start\":55057},{\"end\":55068,\"start\":55067},{\"end\":55509,\"start\":55508},{\"end\":55515,\"start\":55514},{\"end\":55524,\"start\":55523},{\"end\":55534,\"start\":55533},{\"end\":55979,\"start\":55978},{\"end\":55985,\"start\":55984},{\"end\":55994,\"start\":55993},{\"end\":56002,\"start\":56001},{\"end\":56009,\"start\":56008},{\"end\":56019,\"start\":56018},{\"end\":56027,\"start\":56026},{\"end\":56033,\"start\":56032},{\"end\":56371,\"start\":56370},{\"end\":56378,\"start\":56377},{\"end\":56386,\"start\":56385},{\"end\":56392,\"start\":56391},{\"end\":56401,\"start\":56400},{\"end\":56412,\"start\":56411},{\"end\":56648,\"start\":56647},{\"end\":56655,\"start\":56654},{\"end\":56664,\"start\":56663},{\"end\":56670,\"start\":56669},{\"end\":56678,\"start\":56677},{\"end\":56686,\"start\":56685},{\"end\":56694,\"start\":56693},{\"end\":56916,\"start\":56915},{\"end\":56924,\"start\":56923},{\"end\":56933,\"start\":56932},{\"end\":56944,\"start\":56943},{\"end\":56956,\"start\":56955},{\"end\":57222,\"start\":57221},{\"end\":57224,\"start\":57223},{\"end\":57236,\"start\":57235},{\"end\":57249,\"start\":57248},{\"end\":57251,\"start\":57250},{\"end\":57268,\"start\":57267},{\"end\":57279,\"start\":57278},{\"end\":57292,\"start\":57291},{\"end\":57303,\"start\":57302},{\"end\":57319,\"start\":57318},{\"end\":57334,\"start\":57333},{\"end\":57343,\"start\":57342},{\"end\":57354,\"start\":57353},{\"end\":57360,\"start\":57359},{\"end\":57362,\"start\":57361},{\"end\":57380,\"start\":57371},{\"end\":57390,\"start\":57389},{\"end\":57401,\"start\":57400},{\"end\":57403,\"start\":57402},{\"end\":57410,\"start\":57409},{\"end\":57426,\"start\":57420},{\"end\":57428,\"start\":57427},{\"end\":57435,\"start\":57434},{\"end\":57446,\"start\":57445},{\"end\":57456,\"start\":57455},{\"end\":57466,\"start\":57465},{\"end\":57477,\"start\":57476},{\"end\":57486,\"start\":57485},{\"end\":58738,\"start\":58737},{\"end\":58748,\"start\":58747},{\"end\":58758,\"start\":58757},{\"end\":58769,\"start\":58768},{\"end\":58780,\"start\":58779},{\"end\":58782,\"start\":58781},{\"end\":58793,\"start\":58792},{\"end\":58795,\"start\":58794},{\"end\":59287,\"start\":59286},{\"end\":59296,\"start\":59295},{\"end\":59304,\"start\":59303},{\"end\":59315,\"start\":59314},{\"end\":59326,\"start\":59325},{\"end\":59335,\"start\":59334},{\"end\":59346,\"start\":59345},{\"end\":59364,\"start\":59363},{\"end\":59372,\"start\":59371},{\"end\":59384,\"start\":59383},{\"end\":59386,\"start\":59385},{\"end\":59406,\"start\":59398},{\"end\":59415,\"start\":59414},{\"end\":59426,\"start\":59425},{\"end\":59437,\"start\":59436},{\"end\":59447,\"start\":59446},{\"end\":59460,\"start\":59459},{\"end\":59469,\"start\":59468},{\"end\":59471,\"start\":59470},{\"end\":59480,\"start\":59479},{\"end\":59500,\"start\":59489},{\"end\":59514,\"start\":59513},{\"end\":59522,\"start\":59521},{\"end\":59535,\"start\":59534},{\"end\":59542,\"start\":59541},{\"end\":59555,\"start\":59554},{\"end\":60332,\"start\":60331},{\"end\":60340,\"start\":60339},{\"end\":60346,\"start\":60345},{\"end\":60367,\"start\":60355},{\"end\":60375,\"start\":60374},{\"end\":60383,\"start\":60382},{\"end\":60391,\"start\":60390},{\"end\":60393,\"start\":60392},{\"end\":60404,\"start\":60403},{\"end\":60413,\"start\":60412},{\"end\":60425,\"start\":60424},{\"end\":60441,\"start\":60437},{\"end\":60443,\"start\":60442},{\"end\":60451,\"start\":60450},{\"end\":60453,\"start\":60452},{\"end\":60464,\"start\":60463},{\"end\":60471,\"start\":60470},{\"end\":60478,\"start\":60477},{\"end\":60484,\"start\":60483},{\"end\":60493,\"start\":60492},{\"end\":60499,\"start\":60498},{\"end\":60505,\"start\":60504},{\"end\":61083,\"start\":61082},{\"end\":61092,\"start\":61091},{\"end\":61101,\"start\":61100},{\"end\":61110,\"start\":61109},{\"end\":61112,\"start\":61111},{\"end\":61123,\"start\":61122},{\"end\":61125,\"start\":61124},{\"end\":61486,\"start\":61485},{\"end\":61495,\"start\":61494},{\"end\":61515,\"start\":61506},{\"end\":61523,\"start\":61522},{\"end\":61531,\"start\":61530},{\"end\":61544,\"start\":61543},{\"end\":61554,\"start\":61553},{\"end\":61563,\"start\":61562},{\"end\":61958,\"start\":61957},{\"end\":61966,\"start\":61965},{\"end\":61973,\"start\":61972},{\"end\":61981,\"start\":61980},{\"end\":61990,\"start\":61989},{\"end\":61992,\"start\":61991},{\"end\":62228,\"start\":62227},{\"end\":62236,\"start\":62235},{\"end\":62244,\"start\":62243},{\"end\":62246,\"start\":62245},{\"end\":62253,\"start\":62252},{\"end\":62417,\"start\":62416},{\"end\":62425,\"start\":62424},{\"end\":62434,\"start\":62433},{\"end\":62442,\"start\":62441},{\"end\":62450,\"start\":62449},{\"end\":62452,\"start\":62451},{\"end\":62459,\"start\":62458}]", "bib_author_last_name": "[{\"end\":46607,\"start\":46602},{\"end\":46619,\"start\":46611},{\"end\":46639,\"start\":46623},{\"end\":46648,\"start\":46643},{\"end\":46916,\"start\":46907},{\"end\":46928,\"start\":46922},{\"end\":46937,\"start\":46932},{\"end\":46947,\"start\":46941},{\"end\":46956,\"start\":46951},{\"end\":46967,\"start\":46960},{\"end\":46982,\"start\":46973},{\"end\":46990,\"start\":46986},{\"end\":47002,\"start\":46994},{\"end\":47015,\"start\":47006},{\"end\":47380,\"start\":47373},{\"end\":47394,\"start\":47384},{\"end\":47413,\"start\":47405},{\"end\":47424,\"start\":47419},{\"end\":47432,\"start\":47428},{\"end\":47441,\"start\":47436},{\"end\":47452,\"start\":47445},{\"end\":47462,\"start\":47456},{\"end\":47474,\"start\":47466},{\"end\":47489,\"start\":47478},{\"end\":47498,\"start\":47493},{\"end\":47508,\"start\":47502},{\"end\":47518,\"start\":47512},{\"end\":47975,\"start\":47971},{\"end\":47988,\"start\":47979},{\"end\":47999,\"start\":47992},{\"end\":48009,\"start\":48003},{\"end\":48024,\"start\":48013},{\"end\":48032,\"start\":48028},{\"end\":48044,\"start\":48036},{\"end\":48063,\"start\":48056},{\"end\":48078,\"start\":48074},{\"end\":48086,\"start\":48082},{\"end\":48096,\"start\":48090},{\"end\":48102,\"start\":48100},{\"end\":48110,\"start\":48108},{\"end\":48142,\"start\":48114},{\"end\":48151,\"start\":48146},{\"end\":48165,\"start\":48155},{\"end\":48180,\"start\":48169},{\"end\":48188,\"start\":48184},{\"end\":48203,\"start\":48192},{\"end\":48215,\"start\":48207},{\"end\":48227,\"start\":48219},{\"end\":48238,\"start\":48231},{\"end\":48247,\"start\":48242},{\"end\":49328,\"start\":49318},{\"end\":49423,\"start\":49413},{\"end\":50663,\"start\":50658},{\"end\":50674,\"start\":50667},{\"end\":50684,\"start\":50678},{\"end\":50694,\"start\":50688},{\"end\":50702,\"start\":50698},{\"end\":50713,\"start\":50706},{\"end\":50724,\"start\":50717},{\"end\":50739,\"start\":50728},{\"end\":50747,\"start\":50743},{\"end\":50759,\"start\":50751},{\"end\":50767,\"start\":50763},{\"end\":50777,\"start\":50771},{\"end\":50786,\"start\":50781},{\"end\":50794,\"start\":50790},{\"end\":50807,\"start\":50798},{\"end\":50824,\"start\":50811},{\"end\":51281,\"start\":51278},{\"end\":51289,\"start\":51285},{\"end\":51298,\"start\":51293},{\"end\":51304,\"start\":51302},{\"end\":51312,\"start\":51308},{\"end\":51321,\"start\":51316},{\"end\":51327,\"start\":51325},{\"end\":51335,\"start\":51331},{\"end\":51665,\"start\":51662},{\"end\":51674,\"start\":51669},{\"end\":51682,\"start\":51678},{\"end\":51948,\"start\":51943},{\"end\":51957,\"start\":51952},{\"end\":51968,\"start\":51961},{\"end\":51980,\"start\":51972},{\"end\":51991,\"start\":51984},{\"end\":52002,\"start\":51995},{\"end\":52011,\"start\":52006},{\"end\":52018,\"start\":52015},{\"end\":52025,\"start\":52022},{\"end\":52036,\"start\":52029},{\"end\":52043,\"start\":52040},{\"end\":52049,\"start\":52047},{\"end\":52056,\"start\":52053},{\"end\":52075,\"start\":52067},{\"end\":52085,\"start\":52079},{\"end\":52096,\"start\":52089},{\"end\":52106,\"start\":52100},{\"end\":52117,\"start\":52112},{\"end\":52129,\"start\":52121},{\"end\":52139,\"start\":52135},{\"end\":52149,\"start\":52143},{\"end\":52167,\"start\":52160},{\"end\":52180,\"start\":52171},{\"end\":52190,\"start\":52184},{\"end\":52196,\"start\":52194},{\"end\":52208,\"start\":52200},{\"end\":52216,\"start\":52212},{\"end\":52227,\"start\":52220},{\"end\":52236,\"start\":52231},{\"end\":52243,\"start\":52240},{\"end\":52255,\"start\":52247},{\"end\":52262,\"start\":52259},{\"end\":52270,\"start\":52266},{\"end\":52284,\"start\":52274},{\"end\":52294,\"start\":52288},{\"end\":53668,\"start\":53659},{\"end\":53676,\"start\":53672},{\"end\":53686,\"start\":53680},{\"end\":53700,\"start\":53690},{\"end\":53715,\"start\":53711},{\"end\":53722,\"start\":53719},{\"end\":53730,\"start\":53726},{\"end\":53737,\"start\":53734},{\"end\":53747,\"start\":53743},{\"end\":53757,\"start\":53751},{\"end\":53765,\"start\":53761},{\"end\":53773,\"start\":53771},{\"end\":53781,\"start\":53777},{\"end\":53787,\"start\":53785},{\"end\":53797,\"start\":53791},{\"end\":53809,\"start\":53801},{\"end\":53817,\"start\":53813},{\"end\":53827,\"start\":53823},{\"end\":53837,\"start\":53831},{\"end\":53849,\"start\":53841},{\"end\":53862,\"start\":53853},{\"end\":53871,\"start\":53868},{\"end\":54433,\"start\":54428},{\"end\":54441,\"start\":54439},{\"end\":54450,\"start\":54445},{\"end\":54460,\"start\":54454},{\"end\":54474,\"start\":54464},{\"end\":54492,\"start\":54478},{\"end\":54501,\"start\":54496},{\"end\":54521,\"start\":54512},{\"end\":54531,\"start\":54525},{\"end\":54540,\"start\":54535},{\"end\":54548,\"start\":54544},{\"end\":54559,\"start\":54552},{\"end\":55044,\"start\":55042},{\"end\":55055,\"start\":55048},{\"end\":55065,\"start\":55059},{\"end\":55512,\"start\":55510},{\"end\":55521,\"start\":55516},{\"end\":55531,\"start\":55525},{\"end\":55549,\"start\":55535},{\"end\":55982,\"start\":55980},{\"end\":55991,\"start\":55986},{\"end\":55999,\"start\":55995},{\"end\":56006,\"start\":56003},{\"end\":56016,\"start\":56010},{\"end\":56024,\"start\":56020},{\"end\":56030,\"start\":56028},{\"end\":56037,\"start\":56034},{\"end\":56375,\"start\":56372},{\"end\":56383,\"start\":56379},{\"end\":56389,\"start\":56387},{\"end\":56398,\"start\":56393},{\"end\":56409,\"start\":56402},{\"end\":56419,\"start\":56413},{\"end\":56652,\"start\":56649},{\"end\":56661,\"start\":56656},{\"end\":56667,\"start\":56665},{\"end\":56675,\"start\":56671},{\"end\":56683,\"start\":56679},{\"end\":56691,\"start\":56687},{\"end\":56699,\"start\":56695},{\"end\":56921,\"start\":56917},{\"end\":56930,\"start\":56925},{\"end\":56941,\"start\":56934},{\"end\":56953,\"start\":56945},{\"end\":56964,\"start\":56957},{\"end\":57233,\"start\":57225},{\"end\":57246,\"start\":57237},{\"end\":57265,\"start\":57252},{\"end\":57276,\"start\":57269},{\"end\":57289,\"start\":57280},{\"end\":57300,\"start\":57293},{\"end\":57316,\"start\":57304},{\"end\":57331,\"start\":57320},{\"end\":57340,\"start\":57335},{\"end\":57351,\"start\":57344},{\"end\":57357,\"start\":57355},{\"end\":57369,\"start\":57363},{\"end\":57387,\"start\":57381},{\"end\":57398,\"start\":57391},{\"end\":57407,\"start\":57404},{\"end\":57418,\"start\":57411},{\"end\":57432,\"start\":57429},{\"end\":57443,\"start\":57436},{\"end\":57453,\"start\":57447},{\"end\":57463,\"start\":57457},{\"end\":57474,\"start\":57467},{\"end\":57483,\"start\":57478},{\"end\":58745,\"start\":58739},{\"end\":58755,\"start\":58749},{\"end\":58766,\"start\":58759},{\"end\":58777,\"start\":58770},{\"end\":58790,\"start\":58783},{\"end\":58798,\"start\":58796},{\"end\":59293,\"start\":59288},{\"end\":59301,\"start\":59297},{\"end\":59312,\"start\":59305},{\"end\":59323,\"start\":59316},{\"end\":59332,\"start\":59327},{\"end\":59343,\"start\":59336},{\"end\":59361,\"start\":59347},{\"end\":59369,\"start\":59365},{\"end\":59381,\"start\":59373},{\"end\":59396,\"start\":59387},{\"end\":59412,\"start\":59407},{\"end\":59423,\"start\":59416},{\"end\":59434,\"start\":59427},{\"end\":59444,\"start\":59438},{\"end\":59457,\"start\":59448},{\"end\":59466,\"start\":59461},{\"end\":59477,\"start\":59472},{\"end\":59487,\"start\":59481},{\"end\":59511,\"start\":59501},{\"end\":59519,\"start\":59515},{\"end\":59532,\"start\":59523},{\"end\":59539,\"start\":59536},{\"end\":59552,\"start\":59543},{\"end\":59563,\"start\":59556},{\"end\":60337,\"start\":60333},{\"end\":60343,\"start\":60341},{\"end\":60353,\"start\":60347},{\"end\":60372,\"start\":60368},{\"end\":60380,\"start\":60376},{\"end\":60388,\"start\":60384},{\"end\":60401,\"start\":60394},{\"end\":60410,\"start\":60405},{\"end\":60422,\"start\":60414},{\"end\":60435,\"start\":60426},{\"end\":60448,\"start\":60444},{\"end\":60461,\"start\":60454},{\"end\":60468,\"start\":60465},{\"end\":60475,\"start\":60472},{\"end\":60481,\"start\":60479},{\"end\":60490,\"start\":60485},{\"end\":60496,\"start\":60494},{\"end\":60502,\"start\":60500},{\"end\":61089,\"start\":61084},{\"end\":61098,\"start\":61093},{\"end\":61107,\"start\":61102},{\"end\":61120,\"start\":61113},{\"end\":61134,\"start\":61126},{\"end\":61492,\"start\":61487},{\"end\":61504,\"start\":61496},{\"end\":61520,\"start\":61516},{\"end\":61528,\"start\":61524},{\"end\":61541,\"start\":61532},{\"end\":61551,\"start\":61545},{\"end\":61560,\"start\":61555},{\"end\":61572,\"start\":61564},{\"end\":61963,\"start\":61959},{\"end\":61970,\"start\":61967},{\"end\":61978,\"start\":61974},{\"end\":61987,\"start\":61982},{\"end\":61996,\"start\":61993},{\"end\":62233,\"start\":62229},{\"end\":62241,\"start\":62237},{\"end\":62250,\"start\":62247},{\"end\":62257,\"start\":62254},{\"end\":62422,\"start\":62418},{\"end\":62431,\"start\":62426},{\"end\":62439,\"start\":62435},{\"end\":62447,\"start\":62443},{\"end\":62456,\"start\":62453},{\"end\":62463,\"start\":62460}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:220317274\",\"id\":\"b0\"},\"end\":46903,\"start\":46600},{\"attributes\":{\"doi\":\"arXiv:210807258\",\"id\":\"b1\"},\"end\":47306,\"start\":46905},{\"attributes\":{\"doi\":\"arXiv:200514165\",\"id\":\"b2\"},\"end\":47891,\"start\":47308},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":225039882},\"end\":49314,\"start\":47893},{\"attributes\":{\"id\":\"b4\"},\"end\":49409,\"start\":49316},{\"attributes\":{\"id\":\"b5\"},\"end\":49621,\"start\":49411},{\"attributes\":{\"id\":\"b6\"},\"end\":50606,\"start\":49623},{\"attributes\":{\"doi\":\"arXiv:211011316\",\"id\":\"b7\",\"matched_paper_id\":261138},\"end\":51274,\"start\":50608},{\"attributes\":{\"doi\":\"arXiv:211004544\",\"id\":\"b8\"},\"end\":51597,\"start\":51276},{\"attributes\":{\"doi\":\"arXiv:201215723\",\"id\":\"b9\"},\"end\":51836,\"start\":51599},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":162168848},\"end\":53564,\"start\":51838},{\"attributes\":{\"doi\":\"arXiv:220312119\",\"id\":\"b11\"},\"end\":54364,\"start\":53566},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5776303},\"end\":54952,\"start\":54366},{\"attributes\":{\"doi\":\"arXiv:210408691\",\"id\":\"b13\",\"matched_paper_id\":14149995},\"end\":55467,\"start\":54954},{\"attributes\":{\"doi\":\"arXiv:210100190\",\"id\":\"b14\",\"matched_paper_id\":18975368},\"end\":55878,\"start\":55469},{\"attributes\":{\"doi\":\"arXiv:211005208\",\"id\":\"b15\"},\"end\":56263,\"start\":55880},{\"attributes\":{\"doi\":\"arXiv:210713586\",\"id\":\"b16\"},\"end\":56643,\"start\":56265},{\"attributes\":{\"doi\":\"arXiv:210310385\",\"id\":\"b17\"},\"end\":56865,\"start\":56645},{\"attributes\":{\"doi\":\"arXiv:13065151\",\"id\":\"b18\"},\"end\":57128,\"start\":56867},{\"attributes\":{\"doi\":\"arXiv:211204482\",\"id\":\"b19\"},\"end\":58686,\"start\":57130},{\"attributes\":{\"doi\":\"arXiv:12120402\",\"id\":\"b20\",\"matched_paper_id\":2808203},\"end\":59207,\"start\":58688},{\"attributes\":{\"doi\":\"arXiv:200610726\",\"id\":\"b21\",\"matched_paper_id\":220280805},\"end\":60252,\"start\":59209},{\"attributes\":{\"doi\":\"arXiv:211111432\",\"id\":\"b22\",\"matched_paper_id\":173188134},\"end\":61080,\"start\":60254},{\"attributes\":{\"doi\":\"arXiv:201000747\",\"id\":\"b23\"},\"end\":61423,\"start\":61082},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":233210199},\"end\":61955,\"start\":61425},{\"attributes\":{\"doi\":\"arXiv:210302503\",\"id\":\"b25\"},\"end\":62169,\"start\":61957},{\"attributes\":{\"doi\":\"arXiv:220305557\",\"id\":\"b26\"},\"end\":62412,\"start\":62171},{\"attributes\":{\"doi\":\"arXiv:220907521\",\"id\":\"b27\"},\"end\":62645,\"start\":62414}]", "bib_title": "[{\"end\":47967,\"start\":47893},{\"end\":49851,\"start\":49623},{\"end\":50654,\"start\":50608},{\"end\":51939,\"start\":51838},{\"end\":54424,\"start\":54366},{\"end\":55036,\"start\":54954},{\"end\":55506,\"start\":55469},{\"end\":57219,\"start\":57130},{\"end\":58735,\"start\":58688},{\"end\":59284,\"start\":59209},{\"end\":60329,\"start\":60254},{\"end\":61483,\"start\":61425}]", "bib_author": "[{\"end\":46609,\"start\":46600},{\"end\":46621,\"start\":46609},{\"end\":46641,\"start\":46621},{\"end\":46650,\"start\":46641},{\"end\":46918,\"start\":46905},{\"end\":46930,\"start\":46918},{\"end\":46939,\"start\":46930},{\"end\":46949,\"start\":46939},{\"end\":46958,\"start\":46949},{\"end\":46969,\"start\":46958},{\"end\":46984,\"start\":46969},{\"end\":46992,\"start\":46984},{\"end\":47004,\"start\":46992},{\"end\":47017,\"start\":47004},{\"end\":47382,\"start\":47371},{\"end\":47396,\"start\":47382},{\"end\":47415,\"start\":47396},{\"end\":47426,\"start\":47415},{\"end\":47434,\"start\":47426},{\"end\":47443,\"start\":47434},{\"end\":47454,\"start\":47443},{\"end\":47464,\"start\":47454},{\"end\":47476,\"start\":47464},{\"end\":47491,\"start\":47476},{\"end\":47500,\"start\":47491},{\"end\":47510,\"start\":47500},{\"end\":47520,\"start\":47510},{\"end\":47977,\"start\":47969},{\"end\":47990,\"start\":47977},{\"end\":48001,\"start\":47990},{\"end\":48011,\"start\":48001},{\"end\":48026,\"start\":48011},{\"end\":48034,\"start\":48026},{\"end\":48046,\"start\":48034},{\"end\":48065,\"start\":48046},{\"end\":48080,\"start\":48065},{\"end\":48088,\"start\":48080},{\"end\":48098,\"start\":48088},{\"end\":48104,\"start\":48098},{\"end\":48112,\"start\":48104},{\"end\":48144,\"start\":48112},{\"end\":48153,\"start\":48144},{\"end\":48167,\"start\":48153},{\"end\":48182,\"start\":48167},{\"end\":48190,\"start\":48182},{\"end\":48205,\"start\":48190},{\"end\":48217,\"start\":48205},{\"end\":48229,\"start\":48217},{\"end\":48240,\"start\":48229},{\"end\":48249,\"start\":48240},{\"end\":48253,\"start\":48249},{\"end\":49330,\"start\":49318},{\"end\":49425,\"start\":49413},{\"end\":50665,\"start\":50656},{\"end\":50676,\"start\":50665},{\"end\":50686,\"start\":50676},{\"end\":50696,\"start\":50686},{\"end\":50704,\"start\":50696},{\"end\":50715,\"start\":50704},{\"end\":50726,\"start\":50715},{\"end\":50741,\"start\":50726},{\"end\":50749,\"start\":50741},{\"end\":50761,\"start\":50749},{\"end\":50769,\"start\":50761},{\"end\":50779,\"start\":50769},{\"end\":50788,\"start\":50779},{\"end\":50796,\"start\":50788},{\"end\":50809,\"start\":50796},{\"end\":50826,\"start\":50809},{\"end\":51283,\"start\":51276},{\"end\":51291,\"start\":51283},{\"end\":51300,\"start\":51291},{\"end\":51306,\"start\":51300},{\"end\":51314,\"start\":51306},{\"end\":51323,\"start\":51314},{\"end\":51329,\"start\":51323},{\"end\":51337,\"start\":51329},{\"end\":51667,\"start\":51660},{\"end\":51676,\"start\":51667},{\"end\":51684,\"start\":51676},{\"end\":51950,\"start\":51941},{\"end\":51959,\"start\":51950},{\"end\":51970,\"start\":51959},{\"end\":51982,\"start\":51970},{\"end\":51993,\"start\":51982},{\"end\":52004,\"start\":51993},{\"end\":52013,\"start\":52004},{\"end\":52020,\"start\":52013},{\"end\":52027,\"start\":52020},{\"end\":52038,\"start\":52027},{\"end\":52045,\"start\":52038},{\"end\":52051,\"start\":52045},{\"end\":52058,\"start\":52051},{\"end\":52077,\"start\":52058},{\"end\":52087,\"start\":52077},{\"end\":52098,\"start\":52087},{\"end\":52108,\"start\":52098},{\"end\":52119,\"start\":52108},{\"end\":52131,\"start\":52119},{\"end\":52141,\"start\":52131},{\"end\":52151,\"start\":52141},{\"end\":52169,\"start\":52151},{\"end\":52182,\"start\":52169},{\"end\":52192,\"start\":52182},{\"end\":52198,\"start\":52192},{\"end\":52210,\"start\":52198},{\"end\":52218,\"start\":52210},{\"end\":52229,\"start\":52218},{\"end\":52238,\"start\":52229},{\"end\":52245,\"start\":52238},{\"end\":52257,\"start\":52245},{\"end\":52264,\"start\":52257},{\"end\":52272,\"start\":52264},{\"end\":52286,\"start\":52272},{\"end\":52296,\"start\":52286},{\"end\":53670,\"start\":53657},{\"end\":53678,\"start\":53670},{\"end\":53688,\"start\":53678},{\"end\":53702,\"start\":53688},{\"end\":53717,\"start\":53702},{\"end\":53724,\"start\":53717},{\"end\":53732,\"start\":53724},{\"end\":53739,\"start\":53732},{\"end\":53749,\"start\":53739},{\"end\":53759,\"start\":53749},{\"end\":53767,\"start\":53759},{\"end\":53775,\"start\":53767},{\"end\":53783,\"start\":53775},{\"end\":53789,\"start\":53783},{\"end\":53799,\"start\":53789},{\"end\":53811,\"start\":53799},{\"end\":53819,\"start\":53811},{\"end\":53829,\"start\":53819},{\"end\":53839,\"start\":53829},{\"end\":53851,\"start\":53839},{\"end\":53864,\"start\":53851},{\"end\":53873,\"start\":53864},{\"end\":54435,\"start\":54426},{\"end\":54443,\"start\":54435},{\"end\":54452,\"start\":54443},{\"end\":54462,\"start\":54452},{\"end\":54476,\"start\":54462},{\"end\":54494,\"start\":54476},{\"end\":54503,\"start\":54494},{\"end\":54523,\"start\":54503},{\"end\":54533,\"start\":54523},{\"end\":54542,\"start\":54533},{\"end\":54550,\"start\":54542},{\"end\":54561,\"start\":54550},{\"end\":55046,\"start\":55038},{\"end\":55057,\"start\":55046},{\"end\":55067,\"start\":55057},{\"end\":55071,\"start\":55067},{\"end\":55514,\"start\":55508},{\"end\":55523,\"start\":55514},{\"end\":55533,\"start\":55523},{\"end\":55551,\"start\":55533},{\"end\":55984,\"start\":55978},{\"end\":55993,\"start\":55984},{\"end\":56001,\"start\":55993},{\"end\":56008,\"start\":56001},{\"end\":56018,\"start\":56008},{\"end\":56026,\"start\":56018},{\"end\":56032,\"start\":56026},{\"end\":56039,\"start\":56032},{\"end\":56377,\"start\":56370},{\"end\":56385,\"start\":56377},{\"end\":56391,\"start\":56385},{\"end\":56400,\"start\":56391},{\"end\":56411,\"start\":56400},{\"end\":56421,\"start\":56411},{\"end\":56654,\"start\":56647},{\"end\":56663,\"start\":56654},{\"end\":56669,\"start\":56663},{\"end\":56677,\"start\":56669},{\"end\":56685,\"start\":56677},{\"end\":56693,\"start\":56685},{\"end\":56701,\"start\":56693},{\"end\":56923,\"start\":56915},{\"end\":56932,\"start\":56923},{\"end\":56943,\"start\":56932},{\"end\":56955,\"start\":56943},{\"end\":56966,\"start\":56955},{\"end\":57235,\"start\":57221},{\"end\":57248,\"start\":57235},{\"end\":57267,\"start\":57248},{\"end\":57278,\"start\":57267},{\"end\":57291,\"start\":57278},{\"end\":57302,\"start\":57291},{\"end\":57318,\"start\":57302},{\"end\":57333,\"start\":57318},{\"end\":57342,\"start\":57333},{\"end\":57353,\"start\":57342},{\"end\":57359,\"start\":57353},{\"end\":57371,\"start\":57359},{\"end\":57389,\"start\":57371},{\"end\":57400,\"start\":57389},{\"end\":57409,\"start\":57400},{\"end\":57420,\"start\":57409},{\"end\":57434,\"start\":57420},{\"end\":57445,\"start\":57434},{\"end\":57455,\"start\":57445},{\"end\":57465,\"start\":57455},{\"end\":57476,\"start\":57465},{\"end\":57485,\"start\":57476},{\"end\":57489,\"start\":57485},{\"end\":58747,\"start\":58737},{\"end\":58757,\"start\":58747},{\"end\":58768,\"start\":58757},{\"end\":58779,\"start\":58768},{\"end\":58792,\"start\":58779},{\"end\":58800,\"start\":58792},{\"end\":59295,\"start\":59286},{\"end\":59303,\"start\":59295},{\"end\":59314,\"start\":59303},{\"end\":59325,\"start\":59314},{\"end\":59334,\"start\":59325},{\"end\":59345,\"start\":59334},{\"end\":59363,\"start\":59345},{\"end\":59371,\"start\":59363},{\"end\":59383,\"start\":59371},{\"end\":59398,\"start\":59383},{\"end\":59414,\"start\":59398},{\"end\":59425,\"start\":59414},{\"end\":59436,\"start\":59425},{\"end\":59446,\"start\":59436},{\"end\":59459,\"start\":59446},{\"end\":59468,\"start\":59459},{\"end\":59479,\"start\":59468},{\"end\":59489,\"start\":59479},{\"end\":59513,\"start\":59489},{\"end\":59521,\"start\":59513},{\"end\":59534,\"start\":59521},{\"end\":59541,\"start\":59534},{\"end\":59554,\"start\":59541},{\"end\":59565,\"start\":59554},{\"end\":60339,\"start\":60331},{\"end\":60345,\"start\":60339},{\"end\":60355,\"start\":60345},{\"end\":60374,\"start\":60355},{\"end\":60382,\"start\":60374},{\"end\":60390,\"start\":60382},{\"end\":60403,\"start\":60390},{\"end\":60412,\"start\":60403},{\"end\":60424,\"start\":60412},{\"end\":60437,\"start\":60424},{\"end\":60450,\"start\":60437},{\"end\":60463,\"start\":60450},{\"end\":60470,\"start\":60463},{\"end\":60477,\"start\":60470},{\"end\":60483,\"start\":60477},{\"end\":60492,\"start\":60483},{\"end\":60498,\"start\":60492},{\"end\":60504,\"start\":60498},{\"end\":60508,\"start\":60504},{\"end\":61091,\"start\":61082},{\"end\":61100,\"start\":61091},{\"end\":61109,\"start\":61100},{\"end\":61122,\"start\":61109},{\"end\":61136,\"start\":61122},{\"end\":61494,\"start\":61485},{\"end\":61506,\"start\":61494},{\"end\":61522,\"start\":61506},{\"end\":61530,\"start\":61522},{\"end\":61543,\"start\":61530},{\"end\":61553,\"start\":61543},{\"end\":61562,\"start\":61553},{\"end\":61574,\"start\":61562},{\"end\":61965,\"start\":61957},{\"end\":61972,\"start\":61965},{\"end\":61980,\"start\":61972},{\"end\":61989,\"start\":61980},{\"end\":61998,\"start\":61989},{\"end\":62235,\"start\":62227},{\"end\":62243,\"start\":62235},{\"end\":62252,\"start\":62243},{\"end\":62259,\"start\":62252},{\"end\":62424,\"start\":62416},{\"end\":62433,\"start\":62424},{\"end\":62441,\"start\":62433},{\"end\":62449,\"start\":62441},{\"end\":62458,\"start\":62449},{\"end\":62465,\"start\":62458}]", "bib_venue": "[{\"end\":59636,\"start\":59632},{\"end\":60575,\"start\":60567},{\"end\":46731,\"start\":46665},{\"end\":47083,\"start\":47032},{\"end\":47369,\"start\":47308},{\"end\":48270,\"start\":48253},{\"end\":49345,\"start\":49330},{\"end\":49981,\"start\":49853},{\"end\":50902,\"start\":50841},{\"end\":51417,\"start\":51352},{\"end\":51658,\"start\":51599},{\"end\":52386,\"start\":52296},{\"end\":53655,\"start\":53566},{\"end\":54567,\"start\":54561},{\"end\":55160,\"start\":55086},{\"end\":55652,\"start\":55566},{\"end\":55976,\"start\":55880},{\"end\":56368,\"start\":56265},{\"end\":56913,\"start\":56867},{\"end\":57561,\"start\":57504},{\"end\":58926,\"start\":58814},{\"end\":59630,\"start\":59580},{\"end\":60565,\"start\":60523},{\"end\":61233,\"start\":61151},{\"end\":61591,\"start\":61574},{\"end\":62044,\"start\":62013},{\"end\":62225,\"start\":62171}]"}}}, "year": 2023, "month": 12, "day": 17}