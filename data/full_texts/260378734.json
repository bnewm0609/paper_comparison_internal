{"id": 260378734, "updated": "2023-10-01 11:49:30.235", "metadata": {"title": "LEMMA: Learning Language-Conditioned Multi-Robot Manipulation", "authors": "[{\"first\":\"Ran\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Xiaofeng\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Qiaozi\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Suhaila\",\"last\":\"Shakiah\",\"middle\":[]},{\"first\":\"Govind\",\"last\":\"Thattai\",\"middle\":[]},{\"first\":\"Gaurav\",\"last\":\"Sukhatme\",\"middle\":[\"S.\"]}]", "venue": "IEEE Robotics and Automation Letters, vol. 8, no. 10, pp. 6835-6842, Oct. 2023", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.00937", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ral/GongGGSTS23", "doi": "10.1109/lra.2023.3313058"}}, "content": {"source": {"pdf_hash": "92f6346346a46f5813b6996b9c1f10d9602eb39a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.00937v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "41ad6df7556eba25a6c524a5e2914dc3834a5f27", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/92f6346346a46f5813b6996b9c1f10d9602eb39a.txt", "contents": "\nLEMMA: Learning Language-Conditioned Multi-Robot Manipulation\n\n\nIeee \nLetters \nVersion \nAugust \nLEMMA: Learning Language-Conditioned Multi-Robot Manipulation\n1Index Terms-Data Sets for Robot LearningNatural Dialog for HRIMulti-Robot Systems\nComplex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and longhorizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot systems.\n\nI. INTRODUCTION\n\nT HERE is growing interest in connecting human language to robot actions, particularly in single-agent systems [1], [2], [3], [4], [5]. However, there remains a research gap in enabling multi-robot systems to work together in response to language input.\n\nRecent vision and language tasks have primarily focused on navigation and object interactions [4], [6], [7]. However, the lack of physical manipulation in these works makes the settings oversimplified. Although some recent studies, such as [1], [5], address vision and language object manipulation in single-robot settings, the language instructions provided specify only short-term goals, neglecting long-term objectives. [8] attempt to address these limitations by exploring longhorizon planning with manipulation for individual robots. Nevertheless, there remains a need to investigate multi-robot systems capable of accomplishing a broader range of longhorizon tasks while following language instructions.\n\nLearning policies for multi-robot systems introduces distinct challenges, including diverse capabilities arising from physical Manuscript received: April, 18, 2023; Revised August, 1, 2023; Accepted August, 21, 2023. This paper was recommended for publication by Editor Aleksandra Faust upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by Amazon Alexa AI. Corresponding author: Xiaofeng Gao Project website: https://lemma-benchmark.github.io constraints such as the location and reach of different robots. Moreover, task planning heavily depends on the spatial and physical relations between the objects and robots, in addition to the geometries of the objects. To ensure suitable task assignments, an awareness of each robot's specific physical capabilities is needed.\n\nTo tackle the language-conditioned vision-based multi-robot object manipulation problem, we have developed LEMMA, a benchmark that contains 8 types of collaborative object manipulation tasks with varying degrees of complexity. Some tasks require the robot to use tools for object-object interactions. For each task, the object poses, appearances, and robot types are randomized, requiring object affordance estimation and robot capability understanding. To enable multi-task learning, each task is paired with an expert demonstration and several language instructions specifying the task at different granularities. As a result, LEMMA introduces a diverse range of challenges in multi-robot collaboration, including physics-based object manipulation, long-horizon task planning, scheduling and allocation, robot capability and object affordance estimation, tool use, and language grounding. Each aspect poses distinct challenges and is crucial for a multi-robot system that follows human instructions to complete tasks. To evaluate existing techniques on LEMMA, we further provide several baseline methods and compare their performance to each other. We assess task performance by utilizing the latest languageconditioned policy learning models. Our results indicate that current models for language-conditioned manipulation and task planning face significant challenges in LEMMA, especially when dealing with complex human instructions.\n\nWe make the following contributions: \u2022 We design eight novel collaborative object manipulation tasks involving robots with different physical configurations implemented in Nvidia Omniverse -Isaac Sim. \u2022 We provide an open-source dataset comprising 6,400 expert demonstrations and natural language instructions, including human and high-level instructions. \u2022 We implement a modular hierarchical planning approach as a baseline, which integrates language understanding, task planning, task allocation, and object manipulation.\n\n\nII. RELATED WORK\n\nLanguage Conditioned Manipulation. Recent research has shown a growing interest in connecting human language to robot actions [2], [9], [6], [3], [5], [10], [11], [12]. However, these studies typically use a single-robot setting. In contrast, our work emphasizes multi-robot collaboration. In particular, our task settings are designed to require the collaboration of  We use minimal instructions that specify the goal, thus, it is possible that two different tasks may have the same instruction. E.g., tasks in (c) and (d) have the same instruction, but (d) requires robots to make use of the tool to poke the blocks so that the red robot can reach them. Note that the pair of robots involved in each demonstration can be different. The pair of robots in homogeneous settings (e.g., a, b, and d) have the same reach, while in heterogenous cases the reach can be different for each robot (e.g., c).\n\ntwo robots to successfully complete the task. In contrast, in [13], [14], a single agent can still finish the task although multiple agents are available. In our setting, due to the limited workspace reachability of each robot, each target object can only be manipulated by one robot initially, making collaboration necessary to achieve the task goals. Visual Multi-Agent Collaboration. Visual multi-agent collaboration has attracted attention in recent embodied AI research [13], [15], [16], [17], [18], [19], [14]. However, among the works that involve object manipulation, simplified non-physics based atomic actions are often employed as an abstraction for manipulation. These works often use a magic glove to attach an object to the gripper as long as handcrafted conditions are met. For example, an object within 15cm to the gripper can be automatically snapped to the gripper [20]. While this simplification does not affect learning robot task planning, it is unsuitable for learning low-level manipulation policies. We do not make such a simplification here instead using a gripper to interact with objects physically. Additionally, most previous works study the collaboration problem in a single-task setting. In contrast, our work uses a multi-task setting requiring the comprehension of a textual description to understand the goal. Bimanual Robot Manipulation. There is a rich set of literature on bimanual robot manipulation [24], [25], [26], [27], [28], [29], [30]. These works address important problems in dual-arm coordination with a focus on coordinated control and collision avoidance. However, there is less exploration of multi-robot task planning and allocation for long-horizon tasks with strong temporal dependencies, along with workspace management. In addition, these works typically do not involve vision and language inputs, especially for the recognition of the physical limitations of different robots from vision input. More importantly, previous research usually employs robot arms of the same type. In contrast, our work considers the settings of both heterogeneous and homogeneous robot arms. Visual Robot Task and Motion Planning. Traditionally, most works in this area use search over pre-defined domains for planning, which require extensive domain knowledge and accurate perception. Moreover, they often scale poorly with an increasing number of objects. Another line of work involves generating task and motion plans given scene images [31], [32]. In contrast, in our settings, the model uses both RGBD images and textual descriptions as input for multi-task  LEMMA evaluates the performance of language-conditioned multi-agent object manipulation in long-horizon tasks. Multi-task: using a multi-task setting. Language: language instructions to specify goal. Manipulation: physical object manipulation. Multi-agent: requiring multiple agents for task completion. Tool use: requiring the robot to use a tool to interact with other objects. Temporal Dep: temporal dependency between sub-tasks.\nLanguage \u2713 \u2713 \u2713 \u2713 \u2717 \u2713 \u2713 \u2717 \u2713 \u2713 Multi-task \u2713 \u2713 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 Manipulation \u2717 \u2713 \u2713 \u2717 \u2713 \u2713 \u2717 \u2717 \u2717 \u2713 Multi-agent \u2717 \u2717 \u2717 \u2713 \u2717 \u2717 \u2713 \u2713 \u2713 \u2713 Tool Use \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713 Temporal Dep. \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 \u2713\nlearning. Most recently, [33] generated robot policies in the form of code tokens using large language models (LLMs). Nonetheless, they only focus on single-agent task planning. We compare and contrast our benchmark with existing works in Table I. III. PROBLEM FORMULATION Assume a robot system comprised of N robots that is tasked to complete a complex manipulation task, the goal of which is specified by a language instruction x L = {x l } L l=1 , which is a sequence of L word tokens. The full task can be decomposed into M sub-tasks d M = {d m } M m=1 . d M represents the full task, and d m represent each sub-task in d M . Given the language instruction x L , our goal is to find a valid and optimal subtask allocation. We define q im and c im as the quality and cost, respectively, for allocating robot i to work on sub-task m. Then the combined utility for the sub-task is:\nu im = q im \u2212 c im , if robot i can execute sub-task m \u2212\u221e. otherwise\nWe define the assignment of sub-task m to robot i as v im = 1, robot i is assigned to sub-task m 0. otherwise with \u03b3 m = i being an assignment variable for each sub-task m, indicating that sub-task m is assigned to robot i. The goal is to maximize the utility of the full manipulation task under a time constraint. Defining the execution time for task m by robot i as \u03c4 im , and the maximum time allowed to execute the task as T max , we can express the task decomposition and assignment problem as follows:\narg max v N \u2211 i=1 M \u2211 m=1 u im v im(1)\nSubject to:\n\u2211 i \u2211 m \u03c4 im v im \u2264 T max \u2211 i v im \u2264 1 \u2200m \u2208 M v im \u2208 {0, 1} \u2200i \u2208 N, \u2200m \u2208 M\nAs pointed out by [34], this problem cannot be solved in polynomial time. In this work, we tackle this problem by learning from expert demonstrations, so that each sub-task can be assigned to a capable robot to ensure successful task execution. With the sub-task d m and its assignment \u03b3 m , an object manipulation policy \u03c0(s t+1 |s t , o t N , x L , d m , \u03b3 m ) can be used to move a specific object from its current pose s t to its target pose s t+1 , given the observations o t N = {o t i } N i=1 and language instruction x L . In this work, the observation O N consists of robot joint configurations, RGBD images associated with each robot, and camera parameters.\n\nIV. LEMMA BENCHMARK We introduce LEMMA to address the language-conditioned multi-robot manipulation problem. This benchmark is designed to evaluate a system's ability to dynamically perform task allocation and object manipulation in a tabletop environment. LEMMA sets itself apart from existing languageconditioned robotic manipulation benchmarks, such as [22], [3], [2], in several key aspects: \u2022 All tasks in LEMMA feature strong temporal dependencies, making the execution order of sub-tasks critically important. Out-of-order execution will result in task failure. \u2022 LEMMA tasks are exclusively multi-agent based. Due to the robots' reachable space limitations, it is impossible to complete tasks in LEMMA using a single agent. \u2022 As illustrated in Figure 1, robots are provided with only minimal high-level instructions. This requires the model to have a deep understanding of the environment and plan a sequence of actions accordingly to reach the goal specified by the instruction. Using a language classifier to determine the task type and a template to form task plans, as in [35], is inadequate, as multiple tasks may share the same language instructions. \u2022 LEMMA allows robots with different physical configurations to collaborate on manipulation tasks. Two types of robots are provided in LEMMA, namely UR10 and UR5. A detailed comparison between LEMMA and other related benchmarks is shown in Table I.\n\n\nA. Task settings\n\nIn LEMMA, we focus on tasks that require a multi-robot system to complete, given a single instruction and visual observations from top-down cameras placed above each robot. Specifically, we consider a two-robot setting under centralized control. The high-level goal is specified by a single natural language instruction such as \"Place the red block on top of the white pad\". However, this instruction may not provide all the necessary details on how to complete the task. To thoroughly assess system performance in language-conditioned multirobot collaboration, we have designed 8 tasks of varying difficulty. The tasks are implemented in a simulated tabletop environment in NVIDIA Omniverse Isaac-Sim. Task statistics can be found in Table II.  Tasks require 2-7 sub-tasks, with each sub-task requiring the robot to pick up an object, move to a location and put it down. Some tasks require the robots to use tools. In addition, the most difficult task, Hook&Stack, requires passing the tool from one robot to the other.\n\nPass. The first robot is required to pick up a cube of a designated color in its own reachable workspace and place it within a shared workspace, i.e. a space that is reachable by both robots. Following this, the second robot must pick up the designated cube and position it on top of a specified pad in its reachable workspace.\n\nStack. The first robot picks up a designated cube in its reachable workspace and places it in the shared workspace. Subsequently, the second robot picks up a different designated cube and stacks it on top of the first cube.\n\nPoke. Initially, the cube is unreachable by either robot. One robot has access to a tool and must use it to poke the designated cube into the other robot's reachable workspace.\n\nThe second robot then picks up the cube and places it in the target pad in its reachable workspace.\n\nHook. Initially, the cube is unreachable by either robot. However, one of the robots can use a tool to hook the cube into its own reachable space. After that, the robot need to move it to the shared workspace so that the other robot can pick it up and position it on the target pad.\n\nPass2. As an extension of Pass, the task requires the robots to pass two different objects to each other and place them in their respective target locations.\n\nStack2. As an extension of Stack, the task requires the robots to construct two block towers, each requires two cubes of different designated colors.\n\nPoke&Stack. The task requires one robot to use a tool to poke two designated cubes into the other robot's workspace. The other robot then places one cube on top of the other.\n\nHook&Stack. The task requires one robot to use a tool to hook a designated cube into its own reachable space and pass the tool to the second robot, allowing it to perform its hook action on the second cube. The first robot then moves the first hooked cube to the shared workspace so that the second robot can pick it up and stack it on the second block.\n\n\nB. Action Space and Observation Space\n\nThe default action space of each robot is the end-effector position. Nevertheless, alternative control mechanisms such as joint positions, joint velocities, and joint torques can also be accommodated for controlling 6 degrees-of-freedom robots. Due to physical constraints, each robot has a unique reachable workspace, allowing it to only interact with a limited set of objects in the task.\n\nTo obtain visual observations, we place a fixed RGBD camera above each robot. Each camera has a limited field of view and cannot capture all the objects. We follow [1] to process the vision input: we first generate the scene point cloud based on all RGBD images and camera parameters, and use the point cloud to obtain the top-down orthographic RGBD reconstruction of the scene.\n\n\nC. Task Generation\n\nFor diversity among the tasks, we use a rejection sampling mechanism to generate task instances. Each task instance specifies the initial environment configurations and goal conditions. To ensure that the task completion requires both robots, we make sure that the initial location of the target object falls into the reachable workspace of only one robot. In addition to the target object, we also add some distractor objects to make the task more challenging: 1) Specify each robot's type, location, and color. There are two robot types, UR5 and UR10, respectively. Each robot has two different colors, red and white. Colors and types are randomly assigned to each robot. 2) Sample the goal condition of the task, including the colors of the target objects and their goal locations, according to the task type. E.g., the goal condition for the Stack pink on red task is satisfied when a pink block is on top of a red block, as shown in Figure 1c. There are five available colors for blocks and pads, including pink, red, white, blue, and green. The color of the tool is always yellow. 3) Sample the initial locations of target objects, so that each object is only within the reachable space of a single robot. 4) Sample the colors and locations of distractor objects while making sure the colors of the target objects are unique. The above sampling mechanism is repeated until one valid task instance is generated.\n\n\nD. Expert Demonstration\n\nGiven a task specification, we use an oracle task and motion planner to create expert demonstrations. Based on the initial configurations and goal conditions of the task, the oracle creates a task plan consisting of a sequence of sub-tasks d M , and the allocated robot \u03b3 m for each sub-task d m . The subtask follows a pick-and-place procedure, specifying the target object to pick up and the target pose at which to place the object. Once the task plan is generated, an RMPflow motion planner is utilized to generate a motion plan based on ground truth object locations at each time step. All the motion plans are executed by the designated robot, and the corresponding RGBD images and camera poses are recorded to form the expert demonstration data.  Example high-level instruction and crowd-sourced human language instruction templates to specify manipulation tasks in LEMMA. pick-color and place-color represent the color of objects being picked up and placed on.\n\n\nE. Language Instructions\n\nWe assign high-level instruction and a human instruction to each task instance. For high-level instruction, we manually define a template for each task and lexicalize the template using the goal of the task. For human instruction, we first crowd-sourced 500 templates on Amazon Mechanical Turk and selected 80 valid templates that are not too verbose and uniquely specify the goal given the visual observation, 10 for each task type. For each task instance, we then sample a template from the template pool and lexicalize it to generate human instruction. Some examples of the instruction templates can be found in Table III. In summary, each datapoint in LEMMA contains the following information: 1) a manipulation task, specified by the initial configurations and goal conditions, 2) an expert demonstration, including camera poses and RGBD images at each timestep, 3) a high-level instruction and a human instruction specifying the task. As a result, we generate 800 data sessions for each task type, with a total of 6400 sessions. For each task type, we keep 700 sessions in the training set, 40 in the validation set, and 60 in the test set.\n\n\nF. Evaluation Metrics\n\nWe have adopted success rate as the evaluation metric in LEMMA. Task success is defined as 1 if the task goal-conditions are met at the end of the episode, and 0 otherwise. The time constraint of each episode T max is set to 100 seconds. The task specified by the instruction Place the red block on the green circle, for example, is considered successful if, at the end of the episode, the red block is on top of the green circular pad.\n\n\nV. BASELINE MODELS\n\nConsider the problem of learning multi-agent task and motion planning with two robots given a single language instruction and visual observations. The problem can be decomposed into two sub-problems: (a) multi-robot task planning and task allocation, and (b) single-robot planning given the assigned sub-task. To this end, we design a modular baseline model which includes a high-level planner for deciding which subtask a robot should work on and a low-level planner for generating pick and place locations for the gripper given the assigned sub-task. Figure 2a shows the overall architecture of our baseline model. We follow [1] to use the fused point clouds generated from the RGB and depth images of two cameras. Then we use the top orthographic projection of the point cloud as the visual input.\n\n\nA. Action Primitives\n\nWe define six action primitives: move, prehook, hook, prepoke, poke and stop. The non-stop action primitives follow generalized pick-and-place settings. Move is the standard pickand-place primitive, requiring the robot to pick up the object and move it to a specified location. Prehook and prepoke require the robot to pick up the tool and align it with the target object to prepare for hook and poke respectively. The required height of the gripper is different for each primitive. For hook and poke, the height of the gripper after picking is set at 5cm, enabling the tool to come into contact with the target object. For other primitives, the height of the gripper is set to 30cm to avoid contact with other objects between the pick and place actions. The sequence of action primitives required to complete the task depends on the relative poses of target objects, such as cubes, tools, and pads. As a result, the same language instruction can correspond to completely different sequences of primitive actions, depending on the specific arrangement of these objects.\n\n\nB. Multi-Robot Task Planning and Task Allocations\n\nThe multi-robot collaboration task can be decomposed into a sequence of sub-tasks, each can be completed by a single robot. The sub-task allocation can be represented by a tuple (d t m , g t m , \u03b3 t m ). g t m = (e, p, q) represents the entities to specify the sub-task d t m , including the action primitive e, the pick entity p and place entity q. E.g. (Move, Red Cube, Shared Space) indicates moving a red cube on top of the shared workspace between two robots. \u03b3 t m is the sub-task assignment indicating which robot is to perform the task. (d t m , g t m , \u03b3 t m ) are further lexicalized using templates to form the sub-instruction specifying the sub-task and its allocation (Figure 2a). In practice, we use Episodic Transformer [36], a vision and language task planner to generate the sub-instructions. The approach is a language-conditioned visual task planning method that employs a transformer architecture for long-horizon planning. As shown in Figure 2b, ET uses the historical visual and language information in the entire episode to capture long-term dependencies between actions. It leverages the transformer architecture to first separately encodes image histories, language instructions, and past action histories, and then perform crossattention across modalities to decode robot assignment, action primitives, and the target object separately.\n\nThe choice between neural-based open-loop planning and closed-loop planning is often debatable. Open-loop planning cannot adapt to errors made in the planning process. However, it is more stable to train since the training distribution is often more aligned with the testing distribution. Here we consider both open-loop planning and close-loop planning for our benchmark and compare their performance. Note the original Episodic Transformer is closed-loop only and requires new observation at each time step to plan the next action (i.e. Single-step). We modify the algorithm to use only the initial visual observation by providing it as input repeatedly during the loop to plan the whole sub-instruction sequences (i.e. Multi-step).\n\n\nC. Single Agent Object Manipulation and Grounding\n\nIn this work, we use CLIPort [1] as the low-level planner. Formally, at each time step, the algorithm focuses on learning a goal-conditioned policy \u03c0 that produces actions a t based on the current visual observation o t and a language instruction\nx t L = {x t l } L l=1 .\nThe visual observation o t is an orthographic topdown RGB-D reconstruction of the scene, where each pixel corresponds to a point in 3D space. As shown in Figure 2a, in our use case, each input language instruction x t L = (g t m , \u03b3 t m ) specifies a sub-task being allocated to robot \u03b3 t m at time step t. As a result, the goal-conditioned policy is defined as\n\u03c0 o t N , x t L = \u03c0 o t N , g t m , \u03b3 t m \u2192 a t = T pick , T place \u2208 A,\nwhere the actions a = (T pick, T place) denote the endeffector poses for picking and placing respectively. CLIPort is designed for tabletop tasks, with T pick, T place \u2208 SE(2).\n\n\nD. Multi-agent Cliport\n\nSince the sub-instruction already contains the sub-task allocation \u03b3 t m and action primitive e, the CLIPort module used as our low-level planner only needs to predict the pick and place location. To compare with this modular approach, we present a modified version of the original CLIPort module (i.e. M-CLIPort) to perform task planning and task allocation explicitly in an end-to-end fashion. We extend the output to a higher dimensional vector to predict the robot assignment and action primitive to use in addition to pick and place locations, as shown in Figure 3.\n\n\nVI. EXPERIMENTS\n\n\nA. Evaluation Workflow\n\nWe train the CLIPort module for 300K steps on the training set and save a checkpoint every 20K steps. Then we perform checkpoint selection on the validation split for both the highlevel planner and the low-level planner. For the low-level planner, we use the ground-truth sub-instructions as input for checkpoint selection. For the high-level planner, we train the     based on the accuracy of predicted task plans on the validation split. We report the performance of a combination of bestperforming high-level and low-level checkpoints on the test set. Since the physics and rendering in Isaac-sim are not deterministic, we evaluate all tasks for 10 runs and report the means and standard deviations.\n\n\nB. Experiment Results\n\n\n1) High-level Instructions:\n\nThe results shown in Table IV compare language-conditioned policies with different taskplanning modules. The M-CLIPort fails for all tasks with longer horizons. In comparison, our modular hierarchical planning approach works reasonably well for most tasks but fails for very long-horizon tasks (i.e. poke&stack and hook&stack). As an ablation, we further supply ground truth task allocation to the planning module (i.e. GTA). The results show that system performance can be greatly improved with ground truth task allocation. This indicates that task allocation is quite challenging since it requires understanding the reachable workspace of robots to determine which robot should be assigned to a certain sub-task.\n\n2) Human Instructions: The results shown in Table V demonstrate the system performance under human instructions. Human instructions are more complex than high-level instructions since they feature different input lengths, levels of detail, and word choices. Our results show there is a significant gap between the performance of high-level instructions and human instructions, showing that current models are not capable of handling the increased complexity in language. Among all the results, models perform significantly worse on Stack2 and Pass2 with human instructions. This discrepancy is likely due to the fact that the orders of objects vary in human instructions for these tasks (e.g. Place the red cube under the white cube\" vs \"Place the white cube on top of the red cube\"), as demonstrated in Table III, which poses greater challenges in language understanding. In addition, for tasks requiring tool use, human instructions do not always involve the tools, e.g. one instruction for poke is \"push the red block toward the other robot and put it on the blue circle\". In these cases, the model often fails to predict the correct object to manipulate.\n\n3) Further Analysis: To provide more insight into the factors affecting task performance, we further show a breakdown of performance under different settings, including robot types and the number of distractors in the scene. We observe that in general, the task performance decreases with an increasing number of distractor objects (Table VI). As for the robot types, the collaborations among two UR10s exhibit significantly higher performance using the hierarchical planning model given high-level instructions (Table VII). This is probably due to the fact that UR5s have a smaller reachable space compared to UR10s, making it more critical to accurately predict the reachable workspace of each robot.\n\nVII. CONCLUSION, LIMITATIONS AND FUTURE WORK In this paper, we presented LEMMA, the first public benchmark for language-conditioned multi-robot tabletop manipulation. LEMMA combines the problems of language grounding, task planning, task allocation, tool use, capability estimation, long-horizon manipulation, and multi-modal scene understanding. All these subproblems of LEMMA pose significant challenges for existing algorithms. We plan to open-source the simulation environment, the generated dataset, the baseline models, and other tools used during the project's development stage, and we hope our benchmark can inspire new methods in these areas. In this work, we assume tasks can be easily decomposed into independent sub-goals and we restrict the object manipulation problem to SE2. Extending it to SE3 with a more diverse set of tasks and objects will be an important future direction. In our setting robots have different capabilities if they have different reachable spaces. Of course, robots can differ in multiple other ways such as payload, gripper types, etc. We use instructions generated by templates, which are not as diverse as human natural language instructions. Leveraging LLMs to produce more diverse instructions can be a promising way to increase linguistic diversity. This work did not specifically focus on bimanual robot control, and one important future direction is to explore challenges inherent to bimanual control including dual-arm coordination and collision avoidance with real robot experiments. However, this study is the first of its kind to study vision-based languageconditioned multi-robot collaboration and our simplifications serve as a reasonable starting point for research in this area.\n\nFig. 1 :\n1Expert demonstrations and high-level instructions of tasks in LEMMA.\n\n\nthe pick-color cube on top of the place-color pad pick a pick-color cube from one side, put it at the center and put it on the place-color circlePass2place the pick-color1 cube on top of the place-color1 pad and place the pick-color2 cube on top of the place-color2 pad place the pick-color1 cube on the place-color1 pad and pick-color2 cube on the place-color2 pad in the opposite directionStack place the pick-color cube on top of the place-color cube take place-color block and place it in center under the pick-color block Stack2 place the pick-color1 cube on top of the place-color1 cube and place the pick-color2 cube on top of the place-color2 cube assemble two block towers by placing the place-color1 cube under the pick-color1 cube and the place-color2 cube under the pick-color2 cube Poke place the pick-color cube on top of the place-color pad use the L object to push the pick-color block to the other robot and place it on the place-color pad Poke&Stack place the pick-color cube on top of the place-color cube grab the brown L to push the pick-color and place-color cubes closer to the other robot, so it can grab and place the pick-color cube on top of the place-color cube Hook place the pick-color cube on top of the place-color pad fetch the pick-color cube using the L shaped object and place it on top of the place-color pad Hook&Stack place the pick-color cube on top of the place-color cube use the grippers to pick up the pick-color block from the hook and stack it on the place-color block\n\n( a )Fig. 2 :Fig. 3 :\na23Overall architecture of the baseline model.(b) The architecture of the Episodic Transformer model used as the high-level planner. Our baseline model involves a high-level task planning module and a low-level planning module. The high-level planning module takes as input the top-down fused scene image and human instruction to generate sub-instructions that assign the corresponding sub-tasks to robots based on their limitations. Each sub-task is specified by action primitives (in orange) and objects (in blue). The low-level planning module uses the sub-instruction and top-down projection of the scene to generate pick and place locations (visualized as purple and green stars respectively) of the gripper in the scene. Multi-Agent Cliport Model architecture. The output is extended to include robot assignment and action primitive for each predicted action.\n\n\nStack. Instruction: Place the pink cube on top of the red cube. Robots: UR5 and UR10. (d) Poke2stack. Instruction: Place the pink cube on top of the red cube. Robots: UR5 and UR5.arXiv:2308.00937v2 [cs.RO] 17 Sep 2023 \n(a) Pass. Instruction: Place the pink cube on the white pad. Robots: UR10 and UR10. \n\n(b) Hook. Instruction: Place the pink cube on the pink pad. Robots: UR5 and UR5. \n\n(c) \n\nTABLE I :\nIComparison with other benchmarks.\n\nTABLE II :\nIIThere are 8 types of tasks in LEMMA.\n\nTABLE III :\nIII\n\nTABLE IV :\nIVPerformance on the test set with high-level instructions. M-CLIPort: Multi-agent Cliport. Single-step: high-level planning generates each sub-instruction based on the new observation. Multi-step: high-level planning generates the complete sub-instruction sequences from the initial observation. GTA: replacing the robot task allocation results from either single-step or multi-step planning by the ground truth while preserving the predicted action primitives and objects. Single-step 44.44\u00b12.29 1.67\u00b10.00 21.67\u00b11.36 0.00\u00b10.00 13.88\u00b11.24Task Type \nPass \nPass2 \nStack \nStack2 \nPoke \nPoke&Stack \nHook \nHook&Stack \nAvg \nM-CLIPort 25.83\u00b11.86 0.00\u00b10.00 20.00\u00b12.36 0.00\u00b10.00 \n4.58\u00b11.38 \n1.25\u00b11.38 \n3.75\u00b10.72 \n0.00\u00b10.00 \n6.93\u00b10.47 \n2.50\u00b10.83 \n25.56\u00b11.57 \n9.17\u00b11.60 \n14.86\u00b10.26 \n+ GTA \n63.33\u00b10.00 \n5.56\u00b11.24 \n59.44\u00b11.24 \n1.39\u00b10.62 25.28\u00b10.62 \n9.17\u00b11.27 \n30.00\u00b10.00 \n19.72\u00b10.62 \n26.74\u00b10.20 \nMulti-step \n58.33\u00b11.18 0.00\u00b10.00 38.54\u00b10.99 0.00\u00b10.00 \n7.71\u00b12.92 \n2.92\u00b11.10 \n21.46\u00b10.99 \n3.96\u00b11.16 \n16.61\u00b10.39 \n+ GTA \n81.89\u00b10.55 \n5.00\u00b11.44 \n70.42\u00b10.72 \n0.00\u00b10.00 15.00\u00b10.00 \n8.33\u00b11.67 \n23.33\u00b10.00 \n14.17\u00b11.87 \n27.27\u00b10.30 \n\n\n\nTABLE V :\nVPerformance on the test set with human instructions.No. Distractors \n0 \n1 \n2 \nPass \n91.67\u00b10.00 \n87.06\u00b12.35 81.18\u00b13.50 \nPass-human \n75.00\u00b10.00 \n53.92\u00b12.19 37.25\u00b15.55 \nStack \n46.15\u00b10.00 \n40.00\u00b13.08 16.92\u00b12.86 \nStack-human \n16.67\u00b12.87 \n28.33\u00b12.36 15.38\u00b16.28 \nPoke \n33.08\u00b17.14 \n30.00\u00b14.44 17.50\u00b12.50 \nPoke-human \n16.03\u00b12.64 \n12.96\u00b12.62 11.46\u00b16.67 \nHook \n91.30\u00b13.89 \n83.16\u00b13.94 84.44\u00b17.37 \nHook-human \n26.09\u00b12.51 \n32.46\u00b13.62 \n17.59\u00b12.07 \n\n\n\nTABLE VI :\nVIImpact of distractors (single-step planning)Robot Type \nUR5&UR5 \nUR5&UR10 UR10&UR10 \nMulti-step \n38.60\u00b13.25 \n36.32\u00b10.47 \n46.89\u00b11.69 \nMulti-step-human \n15.71\u00b10.59 \n19.16\u00b10.83 \n13.06\u00b10.53 \nSingle-step \n37.44\u00b11.00 \n41.05\u00b11.30 \n51.94\u00b11.17 \nSingle-step-human 16.81\u00b10.40 \n15.57\u00b10.33 \n11.94\u00b10.96 \nM-CLIPort \n13.50\u00b10.84 \n11.27\u00b10.51 \n9.10\u00b12.23 \nM-CLIPort-human \n6.62\u00b10.93 \n8.52\u00b11.25 \n4.48\u00b11.06 \n\n\n\nTABLE VII :\nVIIImpact of robot types Episodic Transformer model for 30 epochs and save a checkpoint at the end of each epoch. We choose the best checkpoint\nCenter for Vision, Cognition, Learning, and Autonomy, UCLA. Email: nikepupu@ucla.edu 2 Amazon Alexa AI. Email: {gxiaofen, qzgao, ssshakia, thattg, sukhatme}@amazon.com 3 Department of Computer Science, USC Viterbi School of Engineering. Email: gaurav@usc.edu Digital Object Identifier (DOI): see top of this page.\n\nCliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLR, 2022. M. Shridhar, L. Manuelli, and D. Fox, \"Cliport: What and where pathways for robotic manipulation,\" in Conference on Robot Learning. PMLR, 2022, pp. 894-906.\n\nCalvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, IEEE Robotics and Automation Letters. 73O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, \"Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks,\" IEEE Robotics and Automation Letters, vol. 7, no. 3, pp. 7327-7334, 2022.\n\nVlmbench: A compositional benchmark for vision-and-language manipulation. K Zheng, X Chen, O C Jenkins, X E Wang, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and BenchmarksK. Zheng, X. Chen, O. C. Jenkins, and X. E. Wang, \"Vlmbench: A compositional benchmark for vision-and-language manipulation,\" in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2022.\n\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N S\u00fcnderhauf, I Reid, S Gould, A Van Den, Hengel, Conference on Computer Vision and Pattern Recognition (CVPR). P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. Van Den Hengel, \"Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,\" in Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nPerceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLR, 2023. M. Shridhar, L. Manuelli, and D. Fox, \"Perceiver-actor: A multi-task transformer for robotic manipulation,\" in Conference on Robot Learning. PMLR, 2023, pp. 785-799.\n\nAlfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition10M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox, \"Alfred: A benchmark for interpret- ing grounded instructions for everyday tasks,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 10 740-10 749.\n\nDialfred: Dialogue-enabled agents for embodied instruction following. X Gao, Q Gao, R Gong, K Lin, G Thattai, G S Sukhatme, IEEE Robotics and Automation Letters. 74X. Gao, Q. Gao, R. Gong, K. Lin, G. Thattai, and G. S. Sukhatme, \"Dialfred: Dialogue-enabled agents for embodied instruction following,\" IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 10 049-10 056, 2022.\n\nInner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, Conference on Robot Learning. PMLR, 2023. W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al., \"Inner monologue: Embod- ied reasoning through planning with language models,\" in Conference on Robot Learning. PMLR, 2023, pp. 1769-1782.\n\nLanguage conditioned imitation learning over unstructured data. C Lynch, P Sermanet, Robotics: Science and Systems. C. Lynch and P. Sermanet, \"Language conditioned imitation learning over unstructured data,\" Robotics: Science and Systems, 2021.\n\nSocratic models: Composing zero-shot multimodal reasoning with language. A Zeng, M Attarian, K M Choromanski, A Wong, S Welker, F Tombari, A Purohit, M S Ryoo, V Sindhwani, J Lee, The Eleventh International Conference on Learning Representations. A. Zeng, M. Attarian, K. M. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. S. Ryoo, V. Sindhwani, J. Lee, et al., \"Socratic models: Composing zero-shot multimodal reasoning with language,\" in The Eleventh International Conference on Learning Rep- resentations, 2022.\n\nLanguage-conditioned imitation learning for robot manipulation tasks. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H Ben Amor, Advances in Neural Information Processing Systems. 33S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor, \"Language-conditioned imitation learning for robot ma- nipulation tasks,\" Advances in Neural Information Processing Systems, vol. 33, pp. 13 139-13 150, 2020.\n\nLearning language-conditioned robot behavior from offline data and crowdsourced annotation. S Nair, E Mitchell, K Chen, S Savarese, C Finn, Conference on Robot Learning. PMLR, 2022. S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al., \"Learn- ing language-conditioned robot behavior from offline data and crowd- sourced annotation,\" in Conference on Robot Learning. PMLR, 2022, pp. 1303-1315.\n\nMulti-agent embodied question answering in interactive environments. S Tan, W Xiang, H Liu, D Guo, F Sun, SpringerS. Tan, W. Xiang, H. Liu, D. Guo, and F. Sun, \"Multi-agent embodied question answering in interactive environments,\" in European Confer- ence on Computer Vision. Springer, 2020, pp. 663-678.\n\nEmbodied multi-agent task planning from ambiguous instruction. X Liu, X Li, D Guo, S Tan, H Liu, F Sun, Proceedings of robotics: science and systems. robotics: science and systemsNew York City, NY, USAX. Liu, X. Li, D. Guo, S. Tan, H. Liu, and F. Sun, \"Embodied multi-agent task planning from ambiguous instruction,\" Proceedings of robotics: science and systems, New York City, NY, USA, pp. 1-14, 2022.\n\nA cordial sync: Going beyond marginal policies for multiagent embodied tasks. U Jain, L Weihs, E Kolve, A Farhadi, S Lazebnik, A Kembhavi, A Schwing, European Conference on Computer Vision. SpringerU. Jain, L. Weihs, E. Kolve, A. Farhadi, S. Lazebnik, A. Kembhavi, and A. Schwing, \"A cordial sync: Going beyond marginal policies for multi- agent embodied tasks,\" in European Conference on Computer Vision. Springer, 2020, pp. 471-490.\n\nTwo body problem: Collaborative visual task completion. U Jain, L Weihs, E Kolve, M Rastegari, S Lazebnik, A Farhadi, A G Schwing, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionU. Jain, L. Weihs, E. Kolve, M. Rastegari, S. Lazebnik, A. Farhadi, A. G. Schwing, and A. Kembhavi, \"Two body problem: Collaborative visual task completion,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6689-6699.\n\nCollaborative visual navigation. H Wang, W Wang, X Zhu, J Dai, L Wang, arXiv:2107.01151arXiv preprintH. Wang, W. Wang, X. Zhu, J. Dai, and L. Wang, \"Collaborative visual navigation,\" arXiv preprint arXiv:2107.01151, 2021.\n\nVisual hide and seek. B Chen, S Song, H Lipson, C Vondrick, Artificial Life Conference Proceedings. MIT PressB. Chen, S. Song, H. Lipson, and C. Vondrick, \"Visual hide and seek,\" in Artificial Life Conference Proceedings. MIT Press, 2020, pp. 645-655.\n\nMulti-agent embodied visual semantic navigation with scene prior knowledge. X Liu, D Guo, H Liu, F Sun, IEEE Robotics and Automation Letters. 72X. Liu, D. Guo, H. Liu, and F. Sun, \"Multi-agent embodied visual semantic navigation with scene prior knowledge,\" IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 3154-3161, 2022.\n\nHabitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D S Chaplot, O Maksymets, Advances in Neural Information Processing Systems. 34A. Szot, A. Clegg, E. Undersander, E. Wijmans, Y. Zhao, J. Turner, N. Maestre, M. Mukadam, D. S. Chaplot, O. Maksymets, et al., \"Habitat 2.0: Training home assistants to rearrange their habitat,\" Advances in Neural Information Processing Systems, vol. 34, 2021.\n\nMqa: Answering the question via robotic manipulation. Y Deng, RSSD Guo, RSSX Guo, RSSN Zhang, RSSH Liu, RSSF Sun, RSSRobotics: Science and Systems. 2020Y. Deng, D. Guo, X. Guo, N. Zhang, H. Liu, and F. Sun, \"Mqa: Answering the question via robotic manipulation,\" in Robotics: Science and Systems (RSS), 2020.\n\nTransporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, Conference on Robot Learning. PMLR, 2021. A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al., \"Transporter networks: Rearranging the visual world for robotic manipulation,\" in Conference on Robot Learning. PMLR, 2021, pp. 726-747.\n\nCh-marl: A multimodal benchmark for cooperative, heterogeneous multi-agent reinforcement learning. V Sharma, P Goyal, K Lin, G Thattai, Q Gao, G S Sukhatme, arXiv:2208.13626arXiv preprintV. Sharma, P. Goyal, K. Lin, G. Thattai, Q. Gao, and G. S. Sukhatme, \"Ch-marl: A multimodal benchmark for cooperative, heterogeneous multi-agent reinforcement learning,\" arXiv preprint arXiv:2208.13626, 2022.\n\nTowards human-level bimanual dexterous manipulation with reinforcement learning. Y Chen, T Wu, S Wang, X Feng, J Jiang, Z Lu, S Mcaleer, H Dong, S.-C Zhu, Y Yang, Advances in Neural Information Processing Systems. 35Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer, H. Dong, S.-C. Zhu, and Y. Yang, \"Towards human-level bimanual dexterous ma- nipulation with reinforcement learning,\" Advances in Neural Information Processing Systems, vol. 35, pp. 5150-5163, 2022.\n\nEfficient task/motion planning for a dual-arm robot from language instructions and cooking images. K Takata, T Kiyokawa, I G Ramirez-Alpizar, N Yamanobe, W Wan, K Harada, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). K. Takata, T. Kiyokawa, I. G. Ramirez-Alpizar, N. Yamanobe, W. Wan, and K. Harada, \"Efficient task/motion planning for a dual-arm robot from language instructions and cooking images,\" in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).\n\n. IEEE. 1265IEEE, 2022, pp. 12 058-12 065.\n\nPick-and-place in dynamic environments with a mobile dual-arm robot equipped with distributed distance sensors. S Stavridis, P Falco, Z Doulgeri, 2020 IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids). IEEES. Stavridis, P. Falco, and Z. Doulgeri, \"Pick-and-place in dynamic environments with a mobile dual-arm robot equipped with distributed distance sensors,\" in 2020 IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids). IEEE, 2021, pp. 76-82.\n\nDual arm manipulation-a survey. C Smith, Y Karayiannidis, L Nalpantidis, X Gratal, P Qi, D V Dimarogonas, D Kragic, Robotics and Autonomous systems. 6010C. Smith, Y. Karayiannidis, L. Nalpantidis, X. Gratal, P. Qi, D. V. Dimarogonas, and D. Kragic, \"Dual arm manipulation-a survey,\" Robotics and Autonomous systems, vol. 60, no. 10, pp. 1340-1353, 2012.\n\nLearning collaborative action plans from youtube videos. H Zhang, P.-J Lai, S Paul, S Kothawade, S Nikolaidis, The International Symposium of Robotics Research. SpringerH. Zhang, P.-J. Lai, S. Paul, S. Kothawade, and S. Nikolaidis, \"Learning collaborative action plans from youtube videos,\" in The International Symposium of Robotics Research. Springer, 2019, pp. 208-223.\n\nA system for imitation learning of contact-rich bimanual manipulation policies. S Stepputtis, M Bandari, S Schaal, H B Amor, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE11S. Stepputtis, M. Bandari, S. Schaal, and H. B. Amor, \"A system for imitation learning of contact-rich bimanual manipulation policies,\" in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. 11 810-11 817.\n\nA certified-complete bimanual manipulation planner. P Lertkultanon, Q.-C Pham, IEEE Transactions on Automation Science and Engineering. 153P. Lertkultanon and Q.-C. Pham, \"A certified-complete bimanual ma- nipulation planner,\" IEEE Transactions on Automation Science and Engineering, vol. 15, no. 3, pp. 1355-1368, 2018.\n\nDeep visual reasoning: Learning to predict action sequences for task and motion planning from an initial scene image. D Driess, RSS FoundationJ.-S Ha, RSS FoundationM Toussaint, RSS FoundationRobotics: Science and Systems 2020. RSS 2020D. Driess, J.-S. Ha, and M. Toussaint, \"Deep visual reasoning: Learning to predict action sequences for task and motion planning from an initial scene image,\" in Robotics: Science and Systems 2020 (RSS 2020). RSS Foundation, 2020.\n\nLearning to solve sequential physical reasoning problems from a scene image. D Driess, J.-S Ha, M Toussaint, The International Journal of Robotics Research. 2021D. Driess, J.-S. Ha, and M. Toussaint, \"Learning to solve sequential physical reasoning problems from a scene image,\" The International Journal of Robotics Research (IJRR), 2021.\n\nProgprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEEI. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, \"Progprompt: Generating situated robot task plans using large language models,\" in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 523-11 530.\n\nA comprehensive taxonomy for multi-robot task allocation. G A Korsah, A Stentz, M B Dias, The International Journal of Robotics Research. 3212G. A. Korsah, A. Stentz, and M. B. Dias, \"A comprehensive taxonomy for multi-robot task allocation,\" The International Journal of Robotics Research, vol. 32, no. 12, pp. 1495-1512, 2013.\n\nFilm: Following instructions in language with modular methods. S Y Min, D S Chaplot, P K Ravikumar, Y Bisk, R Salakhutdinov, International Conference on Learning Representations. S. Y. Min, D. S. Chaplot, P. K. Ravikumar, Y. Bisk, and R. Salakhutdi- nov, \"Film: Following instructions in language with modular methods,\" in International Conference on Learning Representations, 2021.\n\nEpisodic transformer for visionand-language navigation. A Pashevich, C Schmid, C Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionA. Pashevich, C. Schmid, and C. Sun, \"Episodic transformer for vision- and-language navigation,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 942-15 952.\n", "annotations": {"author": "[{\"end\":70,\"start\":65},{\"end\":79,\"start\":71},{\"end\":88,\"start\":80},{\"end\":96,\"start\":89}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":65},{\"end\":78,\"start\":71},{\"end\":87,\"start\":80},{\"end\":95,\"start\":89}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":62,\"start\":1},{\"end\":158,\"start\":97}]", "venue": null, "abstract": "[{\"end\":1248,\"start\":242}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1381,\"start\":1378},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1386,\"start\":1383},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1391,\"start\":1388},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1396,\"start\":1393},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1401,\"start\":1398},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1619,\"start\":1616},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1624,\"start\":1621},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1629,\"start\":1626},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1765,\"start\":1762},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1770,\"start\":1767},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1948,\"start\":1945},{\"end\":2449,\"start\":2440},{\"end\":4685,\"start\":4684},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5157,\"start\":5154},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5162,\"start\":5159},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5167,\"start\":5164},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5172,\"start\":5169},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5177,\"start\":5174},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5183,\"start\":5179},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5189,\"start\":5185},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5195,\"start\":5191},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5994,\"start\":5990},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6000,\"start\":5996},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6407,\"start\":6403},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6413,\"start\":6409},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6419,\"start\":6415},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6425,\"start\":6421},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6431,\"start\":6427},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6437,\"start\":6433},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6443,\"start\":6439},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6815,\"start\":6811},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7370,\"start\":7366},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7376,\"start\":7372},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7382,\"start\":7378},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7388,\"start\":7384},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7394,\"start\":7390},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7400,\"start\":7396},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7406,\"start\":7402},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8407,\"start\":8403},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8413,\"start\":8409},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9177,\"start\":9173},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10756,\"start\":10752},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11763,\"start\":11759},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11768,\"start\":11765},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11773,\"start\":11770},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12491,\"start\":12487},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16416,\"start\":16413},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21352,\"start\":21349},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23409,\"start\":23405},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24854,\"start\":24851}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31497,\"start\":31418},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33014,\"start\":31498},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33903,\"start\":33015},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34298,\"start\":33904},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34344,\"start\":34299},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34395,\"start\":34345},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34412,\"start\":34396},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35533,\"start\":34413},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35980,\"start\":35534},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36382,\"start\":35981},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":36539,\"start\":36383}]", "paragraph": "[{\"end\":1520,\"start\":1267},{\"end\":2231,\"start\":1522},{\"end\":3042,\"start\":2233},{\"end\":4481,\"start\":3044},{\"end\":5007,\"start\":4483},{\"end\":5926,\"start\":5028},{\"end\":8959,\"start\":5928},{\"end\":10030,\"start\":9148},{\"end\":10607,\"start\":10100},{\"end\":10658,\"start\":10647},{\"end\":11401,\"start\":10734},{\"end\":12816,\"start\":11403},{\"end\":13857,\"start\":12837},{\"end\":14186,\"start\":13859},{\"end\":14411,\"start\":14188},{\"end\":14589,\"start\":14413},{\"end\":14690,\"start\":14591},{\"end\":14974,\"start\":14692},{\"end\":15133,\"start\":14976},{\"end\":15284,\"start\":15135},{\"end\":15460,\"start\":15286},{\"end\":15815,\"start\":15462},{\"end\":16247,\"start\":15857},{\"end\":16627,\"start\":16249},{\"end\":18066,\"start\":16650},{\"end\":19062,\"start\":18094},{\"end\":20237,\"start\":19091},{\"end\":20699,\"start\":20263},{\"end\":21522,\"start\":20722},{\"end\":22616,\"start\":21547},{\"end\":24032,\"start\":22670},{\"end\":24768,\"start\":24034},{\"end\":25068,\"start\":24822},{\"end\":25455,\"start\":25094},{\"end\":25704,\"start\":25528},{\"end\":26301,\"start\":25731},{\"end\":27048,\"start\":26346},{\"end\":27819,\"start\":27104},{\"end\":28979,\"start\":27821},{\"end\":29683,\"start\":28981},{\"end\":31417,\"start\":29685}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9147,\"start\":8960},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10099,\"start\":10031},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10646,\"start\":10608},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10733,\"start\":10659},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25093,\"start\":25069},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25527,\"start\":25456}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":9395,\"start\":9387},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12815,\"start\":12808},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13580,\"start\":13572},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19716,\"start\":19706},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27133,\"start\":27125},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28634,\"start\":28625},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29323,\"start\":29313},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29504,\"start\":29493}]", "section_header": "[{\"end\":1265,\"start\":1250},{\"end\":5026,\"start\":5010},{\"end\":12835,\"start\":12819},{\"end\":15855,\"start\":15818},{\"end\":16648,\"start\":16630},{\"end\":18092,\"start\":18069},{\"end\":19089,\"start\":19065},{\"end\":20261,\"start\":20240},{\"end\":20720,\"start\":20702},{\"end\":21545,\"start\":21525},{\"end\":22668,\"start\":22619},{\"end\":24820,\"start\":24771},{\"end\":25729,\"start\":25707},{\"end\":26319,\"start\":26304},{\"end\":26344,\"start\":26322},{\"end\":27072,\"start\":27051},{\"end\":27102,\"start\":27075},{\"end\":31427,\"start\":31419},{\"end\":33037,\"start\":33016},{\"end\":34309,\"start\":34300},{\"end\":34356,\"start\":34346},{\"end\":34408,\"start\":34397},{\"end\":34424,\"start\":34414},{\"end\":35544,\"start\":35535},{\"end\":35992,\"start\":35982},{\"end\":36395,\"start\":36384}]", "table": "[{\"end\":34298,\"start\":34085},{\"end\":35533,\"start\":34964},{\"end\":35980,\"start\":35598},{\"end\":36382,\"start\":36039}]", "figure_caption": "[{\"end\":31497,\"start\":31429},{\"end\":33014,\"start\":31500},{\"end\":33903,\"start\":33041},{\"end\":34085,\"start\":33906},{\"end\":34344,\"start\":34311},{\"end\":34395,\"start\":34359},{\"end\":34964,\"start\":34427},{\"end\":35598,\"start\":35546},{\"end\":36039,\"start\":35995},{\"end\":36539,\"start\":36399}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12163,\"start\":12155},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17597,\"start\":17588},{\"end\":21284,\"start\":21275},{\"end\":23362,\"start\":23351},{\"end\":23635,\"start\":23626},{\"end\":25257,\"start\":25248},{\"end\":26300,\"start\":26292}]", "bib_author_first_name": "[{\"end\":36915,\"start\":36914},{\"end\":36927,\"start\":36926},{\"end\":36939,\"start\":36938},{\"end\":37250,\"start\":37249},{\"end\":37258,\"start\":37257},{\"end\":37269,\"start\":37268},{\"end\":37284,\"start\":37283},{\"end\":37643,\"start\":37642},{\"end\":37652,\"start\":37651},{\"end\":37660,\"start\":37659},{\"end\":37662,\"start\":37661},{\"end\":37673,\"start\":37672},{\"end\":37675,\"start\":37674},{\"end\":38184,\"start\":38183},{\"end\":38196,\"start\":38195},{\"end\":38202,\"start\":38201},{\"end\":38211,\"start\":38210},{\"end\":38220,\"start\":38219},{\"end\":38231,\"start\":38230},{\"end\":38245,\"start\":38244},{\"end\":38253,\"start\":38252},{\"end\":38262,\"start\":38261},{\"end\":38703,\"start\":38702},{\"end\":38715,\"start\":38714},{\"end\":38727,\"start\":38726},{\"end\":39022,\"start\":39021},{\"end\":39034,\"start\":39033},{\"end\":39046,\"start\":39045},{\"end\":39056,\"start\":39055},{\"end\":39064,\"start\":39063},{\"end\":39071,\"start\":39070},{\"end\":39083,\"start\":39082},{\"end\":39098,\"start\":39097},{\"end\":39616,\"start\":39615},{\"end\":39623,\"start\":39622},{\"end\":39630,\"start\":39629},{\"end\":39638,\"start\":39637},{\"end\":39645,\"start\":39644},{\"end\":39656,\"start\":39655},{\"end\":39658,\"start\":39657},{\"end\":40001,\"start\":40000},{\"end\":40010,\"start\":40009},{\"end\":40017,\"start\":40016},{\"end\":40025,\"start\":40024},{\"end\":40033,\"start\":40032},{\"end\":40042,\"start\":40041},{\"end\":40054,\"start\":40053},{\"end\":40062,\"start\":40061},{\"end\":40073,\"start\":40072},{\"end\":40085,\"start\":40084},{\"end\":40457,\"start\":40456},{\"end\":40466,\"start\":40465},{\"end\":40712,\"start\":40711},{\"end\":40720,\"start\":40719},{\"end\":40732,\"start\":40731},{\"end\":40734,\"start\":40733},{\"end\":40749,\"start\":40748},{\"end\":40757,\"start\":40756},{\"end\":40767,\"start\":40766},{\"end\":40778,\"start\":40777},{\"end\":40789,\"start\":40788},{\"end\":40791,\"start\":40790},{\"end\":40799,\"start\":40798},{\"end\":40812,\"start\":40811},{\"end\":41237,\"start\":41236},{\"end\":41251,\"start\":41250},{\"end\":41263,\"start\":41262},{\"end\":41275,\"start\":41274},{\"end\":41282,\"start\":41281},{\"end\":41291,\"start\":41290},{\"end\":41684,\"start\":41683},{\"end\":41692,\"start\":41691},{\"end\":41704,\"start\":41703},{\"end\":41712,\"start\":41711},{\"end\":41724,\"start\":41723},{\"end\":42063,\"start\":42062},{\"end\":42070,\"start\":42069},{\"end\":42079,\"start\":42078},{\"end\":42086,\"start\":42085},{\"end\":42093,\"start\":42092},{\"end\":42363,\"start\":42362},{\"end\":42370,\"start\":42369},{\"end\":42376,\"start\":42375},{\"end\":42383,\"start\":42382},{\"end\":42390,\"start\":42389},{\"end\":42397,\"start\":42396},{\"end\":42782,\"start\":42781},{\"end\":42790,\"start\":42789},{\"end\":42799,\"start\":42798},{\"end\":42808,\"start\":42807},{\"end\":42819,\"start\":42818},{\"end\":42831,\"start\":42830},{\"end\":42843,\"start\":42842},{\"end\":43196,\"start\":43195},{\"end\":43204,\"start\":43203},{\"end\":43213,\"start\":43212},{\"end\":43222,\"start\":43221},{\"end\":43235,\"start\":43234},{\"end\":43247,\"start\":43246},{\"end\":43258,\"start\":43257},{\"end\":43260,\"start\":43259},{\"end\":43271,\"start\":43270},{\"end\":43731,\"start\":43730},{\"end\":43739,\"start\":43738},{\"end\":43747,\"start\":43746},{\"end\":43754,\"start\":43753},{\"end\":43761,\"start\":43760},{\"end\":43943,\"start\":43942},{\"end\":43951,\"start\":43950},{\"end\":43959,\"start\":43958},{\"end\":43969,\"start\":43968},{\"end\":44250,\"start\":44249},{\"end\":44257,\"start\":44256},{\"end\":44264,\"start\":44263},{\"end\":44271,\"start\":44270},{\"end\":44573,\"start\":44572},{\"end\":44581,\"start\":44580},{\"end\":44590,\"start\":44589},{\"end\":44605,\"start\":44604},{\"end\":44616,\"start\":44615},{\"end\":44624,\"start\":44623},{\"end\":44634,\"start\":44633},{\"end\":44645,\"start\":44644},{\"end\":44656,\"start\":44655},{\"end\":44658,\"start\":44657},{\"end\":44669,\"start\":44668},{\"end\":45052,\"start\":45051},{\"end\":45063,\"start\":45062},{\"end\":45073,\"start\":45072},{\"end\":45083,\"start\":45082},{\"end\":45095,\"start\":45094},{\"end\":45105,\"start\":45104},{\"end\":45385,\"start\":45384},{\"end\":45393,\"start\":45392},{\"end\":45405,\"start\":45404},{\"end\":45416,\"start\":45415},{\"end\":45426,\"start\":45425},{\"end\":45435,\"start\":45434},{\"end\":45447,\"start\":45446},{\"end\":45460,\"start\":45459},{\"end\":45470,\"start\":45469},{\"end\":45479,\"start\":45478},{\"end\":45896,\"start\":45895},{\"end\":45906,\"start\":45905},{\"end\":45915,\"start\":45914},{\"end\":45922,\"start\":45921},{\"end\":45933,\"start\":45932},{\"end\":45940,\"start\":45939},{\"end\":45942,\"start\":45941},{\"end\":46275,\"start\":46274},{\"end\":46283,\"start\":46282},{\"end\":46289,\"start\":46288},{\"end\":46297,\"start\":46296},{\"end\":46305,\"start\":46304},{\"end\":46314,\"start\":46313},{\"end\":46320,\"start\":46319},{\"end\":46331,\"start\":46330},{\"end\":46342,\"start\":46338},{\"end\":46349,\"start\":46348},{\"end\":46772,\"start\":46771},{\"end\":46782,\"start\":46781},{\"end\":46794,\"start\":46793},{\"end\":46796,\"start\":46795},{\"end\":46815,\"start\":46814},{\"end\":46827,\"start\":46826},{\"end\":46834,\"start\":46833},{\"end\":47350,\"start\":47349},{\"end\":47363,\"start\":47362},{\"end\":47372,\"start\":47371},{\"end\":47754,\"start\":47753},{\"end\":47763,\"start\":47762},{\"end\":47780,\"start\":47779},{\"end\":47795,\"start\":47794},{\"end\":47805,\"start\":47804},{\"end\":47811,\"start\":47810},{\"end\":47813,\"start\":47812},{\"end\":47828,\"start\":47827},{\"end\":48134,\"start\":48133},{\"end\":48146,\"start\":48142},{\"end\":48153,\"start\":48152},{\"end\":48161,\"start\":48160},{\"end\":48174,\"start\":48173},{\"end\":48531,\"start\":48530},{\"end\":48545,\"start\":48544},{\"end\":48556,\"start\":48555},{\"end\":48566,\"start\":48565},{\"end\":48568,\"start\":48567},{\"end\":48967,\"start\":48966},{\"end\":48986,\"start\":48982},{\"end\":49355,\"start\":49354},{\"end\":49382,\"start\":49378},{\"end\":49402,\"start\":49401},{\"end\":49782,\"start\":49781},{\"end\":49795,\"start\":49791},{\"end\":49801,\"start\":49800},{\"end\":50124,\"start\":50123},{\"end\":50133,\"start\":50132},{\"end\":50143,\"start\":50142},{\"end\":50156,\"start\":50155},{\"end\":50165,\"start\":50164},{\"end\":50171,\"start\":50170},{\"end\":50183,\"start\":50182},{\"end\":50190,\"start\":50189},{\"end\":50202,\"start\":50201},{\"end\":50626,\"start\":50625},{\"end\":50628,\"start\":50627},{\"end\":50638,\"start\":50637},{\"end\":50648,\"start\":50647},{\"end\":50650,\"start\":50649},{\"end\":50961,\"start\":50960},{\"end\":50963,\"start\":50962},{\"end\":50970,\"start\":50969},{\"end\":50972,\"start\":50971},{\"end\":50983,\"start\":50982},{\"end\":50985,\"start\":50984},{\"end\":50998,\"start\":50997},{\"end\":51006,\"start\":51005},{\"end\":51338,\"start\":51337},{\"end\":51351,\"start\":51350},{\"end\":51361,\"start\":51360}]", "bib_author_last_name": "[{\"end\":36924,\"start\":36916},{\"end\":36936,\"start\":36928},{\"end\":36943,\"start\":36940},{\"end\":37255,\"start\":37251},{\"end\":37266,\"start\":37259},{\"end\":37281,\"start\":37270},{\"end\":37292,\"start\":37285},{\"end\":37649,\"start\":37644},{\"end\":37657,\"start\":37653},{\"end\":37670,\"start\":37663},{\"end\":37680,\"start\":37676},{\"end\":38193,\"start\":38185},{\"end\":38199,\"start\":38197},{\"end\":38208,\"start\":38203},{\"end\":38217,\"start\":38212},{\"end\":38228,\"start\":38221},{\"end\":38242,\"start\":38232},{\"end\":38250,\"start\":38246},{\"end\":38259,\"start\":38254},{\"end\":38270,\"start\":38263},{\"end\":38278,\"start\":38272},{\"end\":38712,\"start\":38704},{\"end\":38724,\"start\":38716},{\"end\":38731,\"start\":38728},{\"end\":39031,\"start\":39023},{\"end\":39043,\"start\":39035},{\"end\":39053,\"start\":39047},{\"end\":39061,\"start\":39057},{\"end\":39068,\"start\":39065},{\"end\":39080,\"start\":39072},{\"end\":39095,\"start\":39084},{\"end\":39102,\"start\":39099},{\"end\":39620,\"start\":39617},{\"end\":39627,\"start\":39624},{\"end\":39635,\"start\":39631},{\"end\":39642,\"start\":39639},{\"end\":39653,\"start\":39646},{\"end\":39667,\"start\":39659},{\"end\":40007,\"start\":40002},{\"end\":40014,\"start\":40011},{\"end\":40022,\"start\":40018},{\"end\":40030,\"start\":40026},{\"end\":40039,\"start\":40034},{\"end\":40051,\"start\":40043},{\"end\":40059,\"start\":40055},{\"end\":40070,\"start\":40063},{\"end\":40082,\"start\":40074},{\"end\":40094,\"start\":40086},{\"end\":40463,\"start\":40458},{\"end\":40475,\"start\":40467},{\"end\":40717,\"start\":40713},{\"end\":40729,\"start\":40721},{\"end\":40746,\"start\":40735},{\"end\":40754,\"start\":40750},{\"end\":40764,\"start\":40758},{\"end\":40775,\"start\":40768},{\"end\":40786,\"start\":40779},{\"end\":40796,\"start\":40792},{\"end\":40809,\"start\":40800},{\"end\":40816,\"start\":40813},{\"end\":41248,\"start\":41238},{\"end\":41260,\"start\":41252},{\"end\":41272,\"start\":41264},{\"end\":41279,\"start\":41276},{\"end\":41288,\"start\":41283},{\"end\":41300,\"start\":41292},{\"end\":41689,\"start\":41685},{\"end\":41701,\"start\":41693},{\"end\":41709,\"start\":41705},{\"end\":41721,\"start\":41713},{\"end\":41729,\"start\":41725},{\"end\":42067,\"start\":42064},{\"end\":42076,\"start\":42071},{\"end\":42083,\"start\":42080},{\"end\":42090,\"start\":42087},{\"end\":42097,\"start\":42094},{\"end\":42367,\"start\":42364},{\"end\":42373,\"start\":42371},{\"end\":42380,\"start\":42377},{\"end\":42387,\"start\":42384},{\"end\":42394,\"start\":42391},{\"end\":42401,\"start\":42398},{\"end\":42787,\"start\":42783},{\"end\":42796,\"start\":42791},{\"end\":42805,\"start\":42800},{\"end\":42816,\"start\":42809},{\"end\":42828,\"start\":42820},{\"end\":42840,\"start\":42832},{\"end\":42851,\"start\":42844},{\"end\":43201,\"start\":43197},{\"end\":43210,\"start\":43205},{\"end\":43219,\"start\":43214},{\"end\":43232,\"start\":43223},{\"end\":43244,\"start\":43236},{\"end\":43255,\"start\":43248},{\"end\":43268,\"start\":43261},{\"end\":43280,\"start\":43272},{\"end\":43736,\"start\":43732},{\"end\":43744,\"start\":43740},{\"end\":43751,\"start\":43748},{\"end\":43758,\"start\":43755},{\"end\":43766,\"start\":43762},{\"end\":43948,\"start\":43944},{\"end\":43956,\"start\":43952},{\"end\":43966,\"start\":43960},{\"end\":43978,\"start\":43970},{\"end\":44254,\"start\":44251},{\"end\":44261,\"start\":44258},{\"end\":44268,\"start\":44265},{\"end\":44275,\"start\":44272},{\"end\":44578,\"start\":44574},{\"end\":44587,\"start\":44582},{\"end\":44602,\"start\":44591},{\"end\":44613,\"start\":44606},{\"end\":44621,\"start\":44617},{\"end\":44631,\"start\":44625},{\"end\":44642,\"start\":44635},{\"end\":44653,\"start\":44646},{\"end\":44666,\"start\":44659},{\"end\":44679,\"start\":44670},{\"end\":45057,\"start\":45053},{\"end\":45067,\"start\":45064},{\"end\":45077,\"start\":45074},{\"end\":45089,\"start\":45084},{\"end\":45099,\"start\":45096},{\"end\":45109,\"start\":45106},{\"end\":45390,\"start\":45386},{\"end\":45402,\"start\":45394},{\"end\":45413,\"start\":45406},{\"end\":45423,\"start\":45417},{\"end\":45432,\"start\":45427},{\"end\":45444,\"start\":45436},{\"end\":45457,\"start\":45448},{\"end\":45467,\"start\":45461},{\"end\":45476,\"start\":45471},{\"end\":45489,\"start\":45480},{\"end\":45903,\"start\":45897},{\"end\":45912,\"start\":45907},{\"end\":45919,\"start\":45916},{\"end\":45930,\"start\":45923},{\"end\":45937,\"start\":45934},{\"end\":45951,\"start\":45943},{\"end\":46280,\"start\":46276},{\"end\":46286,\"start\":46284},{\"end\":46294,\"start\":46290},{\"end\":46302,\"start\":46298},{\"end\":46311,\"start\":46306},{\"end\":46317,\"start\":46315},{\"end\":46328,\"start\":46321},{\"end\":46336,\"start\":46332},{\"end\":46346,\"start\":46343},{\"end\":46354,\"start\":46350},{\"end\":46779,\"start\":46773},{\"end\":46791,\"start\":46783},{\"end\":46812,\"start\":46797},{\"end\":46824,\"start\":46816},{\"end\":46831,\"start\":46828},{\"end\":46841,\"start\":46835},{\"end\":47360,\"start\":47351},{\"end\":47369,\"start\":47364},{\"end\":47381,\"start\":47373},{\"end\":47760,\"start\":47755},{\"end\":47777,\"start\":47764},{\"end\":47792,\"start\":47781},{\"end\":47802,\"start\":47796},{\"end\":47808,\"start\":47806},{\"end\":47825,\"start\":47814},{\"end\":47835,\"start\":47829},{\"end\":48140,\"start\":48135},{\"end\":48150,\"start\":48147},{\"end\":48158,\"start\":48154},{\"end\":48171,\"start\":48162},{\"end\":48185,\"start\":48175},{\"end\":48542,\"start\":48532},{\"end\":48553,\"start\":48546},{\"end\":48563,\"start\":48557},{\"end\":48573,\"start\":48569},{\"end\":48980,\"start\":48968},{\"end\":48991,\"start\":48987},{\"end\":49362,\"start\":49356},{\"end\":49385,\"start\":49383},{\"end\":49412,\"start\":49403},{\"end\":49789,\"start\":49783},{\"end\":49798,\"start\":49796},{\"end\":49811,\"start\":49802},{\"end\":50130,\"start\":50125},{\"end\":50140,\"start\":50134},{\"end\":50153,\"start\":50144},{\"end\":50162,\"start\":50157},{\"end\":50168,\"start\":50166},{\"end\":50180,\"start\":50172},{\"end\":50187,\"start\":50184},{\"end\":50199,\"start\":50191},{\"end\":50207,\"start\":50203},{\"end\":50635,\"start\":50629},{\"end\":50645,\"start\":50639},{\"end\":50655,\"start\":50651},{\"end\":50967,\"start\":50964},{\"end\":50980,\"start\":50973},{\"end\":50995,\"start\":50986},{\"end\":51003,\"start\":50999},{\"end\":51020,\"start\":51007},{\"end\":51348,\"start\":51339},{\"end\":51358,\"start\":51352},{\"end\":51365,\"start\":51362}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":237396838},\"end\":37143,\"start\":36855},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":244908821},\"end\":37566,\"start\":37145},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":249848175},\"end\":38072,\"start\":37568},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4673790},\"end\":38632,\"start\":38074},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":252199474},\"end\":38940,\"start\":38634},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":208617407},\"end\":39543,\"start\":38942},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":247158852},\"end\":39923,\"start\":39545},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":250451569},\"end\":40390,\"start\":39925},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":235657751},\"end\":40636,\"start\":40392},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":247922520},\"end\":41164,\"start\":40638},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225062560},\"end\":41589,\"start\":41166},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":237385309},\"end\":41991,\"start\":41591},{\"attributes\":{\"id\":\"b12\"},\"end\":42297,\"start\":41993},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":248941435},\"end\":42701,\"start\":42299},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":220424783},\"end\":43137,\"start\":42703},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":109933186},\"end\":43695,\"start\":43139},{\"attributes\":{\"doi\":\"arXiv:2107.01151\",\"id\":\"b16\"},\"end\":43918,\"start\":43697},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204744022},\"end\":44171,\"start\":43920},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":237572234},\"end\":44504,\"start\":44173},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235658123},\"end\":44995,\"start\":44506},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":212644876},\"end\":45305,\"start\":44997},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":225076003},\"end\":45794,\"start\":45307},{\"attributes\":{\"doi\":\"arXiv:2208.13626\",\"id\":\"b22\"},\"end\":46191,\"start\":45796},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":249848184},\"end\":46670,\"start\":46193},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":255174843},\"end\":47191,\"start\":46672},{\"attributes\":{\"id\":\"b25\"},\"end\":47235,\"start\":47193},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":238752932},\"end\":47719,\"start\":47237},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4019175},\"end\":48074,\"start\":47721},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":219706788},\"end\":48448,\"start\":48076},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":251223661},\"end\":48912,\"start\":48450},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":19323588},\"end\":49234,\"start\":48914},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":219559269},\"end\":49702,\"start\":49236},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":245513370},\"end\":50043,\"start\":49704},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":252519594},\"end\":50565,\"start\":50045},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12515065},\"end\":50895,\"start\":50567},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":238857090},\"end\":51279,\"start\":50897},{\"attributes\":{\"id\":\"b36\"},\"end\":51693,\"start\":51281}]", "bib_title": "[{\"end\":36912,\"start\":36855},{\"end\":37247,\"start\":37145},{\"end\":37640,\"start\":37568},{\"end\":38181,\"start\":38074},{\"end\":38700,\"start\":38634},{\"end\":39019,\"start\":38942},{\"end\":39613,\"start\":39545},{\"end\":39998,\"start\":39925},{\"end\":40454,\"start\":40392},{\"end\":40709,\"start\":40638},{\"end\":41234,\"start\":41166},{\"end\":41681,\"start\":41591},{\"end\":42360,\"start\":42299},{\"end\":42779,\"start\":42703},{\"end\":43193,\"start\":43139},{\"end\":43940,\"start\":43920},{\"end\":44247,\"start\":44173},{\"end\":44570,\"start\":44506},{\"end\":45049,\"start\":44997},{\"end\":45382,\"start\":45307},{\"end\":46272,\"start\":46193},{\"end\":46769,\"start\":46672},{\"end\":47347,\"start\":47237},{\"end\":47751,\"start\":47721},{\"end\":48131,\"start\":48076},{\"end\":48528,\"start\":48450},{\"end\":48964,\"start\":48914},{\"end\":49352,\"start\":49236},{\"end\":49779,\"start\":49704},{\"end\":50121,\"start\":50045},{\"end\":50623,\"start\":50567},{\"end\":50958,\"start\":50897},{\"end\":51335,\"start\":51281}]", "bib_author": "[{\"end\":36926,\"start\":36914},{\"end\":36938,\"start\":36926},{\"end\":36945,\"start\":36938},{\"end\":37257,\"start\":37249},{\"end\":37268,\"start\":37257},{\"end\":37283,\"start\":37268},{\"end\":37294,\"start\":37283},{\"end\":37651,\"start\":37642},{\"end\":37659,\"start\":37651},{\"end\":37672,\"start\":37659},{\"end\":37682,\"start\":37672},{\"end\":38195,\"start\":38183},{\"end\":38201,\"start\":38195},{\"end\":38210,\"start\":38201},{\"end\":38219,\"start\":38210},{\"end\":38230,\"start\":38219},{\"end\":38244,\"start\":38230},{\"end\":38252,\"start\":38244},{\"end\":38261,\"start\":38252},{\"end\":38272,\"start\":38261},{\"end\":38280,\"start\":38272},{\"end\":38714,\"start\":38702},{\"end\":38726,\"start\":38714},{\"end\":38733,\"start\":38726},{\"end\":39033,\"start\":39021},{\"end\":39045,\"start\":39033},{\"end\":39055,\"start\":39045},{\"end\":39063,\"start\":39055},{\"end\":39070,\"start\":39063},{\"end\":39082,\"start\":39070},{\"end\":39097,\"start\":39082},{\"end\":39104,\"start\":39097},{\"end\":39622,\"start\":39615},{\"end\":39629,\"start\":39622},{\"end\":39637,\"start\":39629},{\"end\":39644,\"start\":39637},{\"end\":39655,\"start\":39644},{\"end\":39669,\"start\":39655},{\"end\":40009,\"start\":40000},{\"end\":40016,\"start\":40009},{\"end\":40024,\"start\":40016},{\"end\":40032,\"start\":40024},{\"end\":40041,\"start\":40032},{\"end\":40053,\"start\":40041},{\"end\":40061,\"start\":40053},{\"end\":40072,\"start\":40061},{\"end\":40084,\"start\":40072},{\"end\":40096,\"start\":40084},{\"end\":40465,\"start\":40456},{\"end\":40477,\"start\":40465},{\"end\":40719,\"start\":40711},{\"end\":40731,\"start\":40719},{\"end\":40748,\"start\":40731},{\"end\":40756,\"start\":40748},{\"end\":40766,\"start\":40756},{\"end\":40777,\"start\":40766},{\"end\":40788,\"start\":40777},{\"end\":40798,\"start\":40788},{\"end\":40811,\"start\":40798},{\"end\":40818,\"start\":40811},{\"end\":41250,\"start\":41236},{\"end\":41262,\"start\":41250},{\"end\":41274,\"start\":41262},{\"end\":41281,\"start\":41274},{\"end\":41290,\"start\":41281},{\"end\":41302,\"start\":41290},{\"end\":41691,\"start\":41683},{\"end\":41703,\"start\":41691},{\"end\":41711,\"start\":41703},{\"end\":41723,\"start\":41711},{\"end\":41731,\"start\":41723},{\"end\":42069,\"start\":42062},{\"end\":42078,\"start\":42069},{\"end\":42085,\"start\":42078},{\"end\":42092,\"start\":42085},{\"end\":42099,\"start\":42092},{\"end\":42369,\"start\":42362},{\"end\":42375,\"start\":42369},{\"end\":42382,\"start\":42375},{\"end\":42389,\"start\":42382},{\"end\":42396,\"start\":42389},{\"end\":42403,\"start\":42396},{\"end\":42789,\"start\":42781},{\"end\":42798,\"start\":42789},{\"end\":42807,\"start\":42798},{\"end\":42818,\"start\":42807},{\"end\":42830,\"start\":42818},{\"end\":42842,\"start\":42830},{\"end\":42853,\"start\":42842},{\"end\":43203,\"start\":43195},{\"end\":43212,\"start\":43203},{\"end\":43221,\"start\":43212},{\"end\":43234,\"start\":43221},{\"end\":43246,\"start\":43234},{\"end\":43257,\"start\":43246},{\"end\":43270,\"start\":43257},{\"end\":43282,\"start\":43270},{\"end\":43738,\"start\":43730},{\"end\":43746,\"start\":43738},{\"end\":43753,\"start\":43746},{\"end\":43760,\"start\":43753},{\"end\":43768,\"start\":43760},{\"end\":43950,\"start\":43942},{\"end\":43958,\"start\":43950},{\"end\":43968,\"start\":43958},{\"end\":43980,\"start\":43968},{\"end\":44256,\"start\":44249},{\"end\":44263,\"start\":44256},{\"end\":44270,\"start\":44263},{\"end\":44277,\"start\":44270},{\"end\":44580,\"start\":44572},{\"end\":44589,\"start\":44580},{\"end\":44604,\"start\":44589},{\"end\":44615,\"start\":44604},{\"end\":44623,\"start\":44615},{\"end\":44633,\"start\":44623},{\"end\":44644,\"start\":44633},{\"end\":44655,\"start\":44644},{\"end\":44668,\"start\":44655},{\"end\":44681,\"start\":44668},{\"end\":45062,\"start\":45051},{\"end\":45072,\"start\":45062},{\"end\":45082,\"start\":45072},{\"end\":45094,\"start\":45082},{\"end\":45104,\"start\":45094},{\"end\":45114,\"start\":45104},{\"end\":45392,\"start\":45384},{\"end\":45404,\"start\":45392},{\"end\":45415,\"start\":45404},{\"end\":45425,\"start\":45415},{\"end\":45434,\"start\":45425},{\"end\":45446,\"start\":45434},{\"end\":45459,\"start\":45446},{\"end\":45469,\"start\":45459},{\"end\":45478,\"start\":45469},{\"end\":45491,\"start\":45478},{\"end\":45905,\"start\":45895},{\"end\":45914,\"start\":45905},{\"end\":45921,\"start\":45914},{\"end\":45932,\"start\":45921},{\"end\":45939,\"start\":45932},{\"end\":45953,\"start\":45939},{\"end\":46282,\"start\":46274},{\"end\":46288,\"start\":46282},{\"end\":46296,\"start\":46288},{\"end\":46304,\"start\":46296},{\"end\":46313,\"start\":46304},{\"end\":46319,\"start\":46313},{\"end\":46330,\"start\":46319},{\"end\":46338,\"start\":46330},{\"end\":46348,\"start\":46338},{\"end\":46356,\"start\":46348},{\"end\":46781,\"start\":46771},{\"end\":46793,\"start\":46781},{\"end\":46814,\"start\":46793},{\"end\":46826,\"start\":46814},{\"end\":46833,\"start\":46826},{\"end\":46843,\"start\":46833},{\"end\":47362,\"start\":47349},{\"end\":47371,\"start\":47362},{\"end\":47383,\"start\":47371},{\"end\":47762,\"start\":47753},{\"end\":47779,\"start\":47762},{\"end\":47794,\"start\":47779},{\"end\":47804,\"start\":47794},{\"end\":47810,\"start\":47804},{\"end\":47827,\"start\":47810},{\"end\":47837,\"start\":47827},{\"end\":48142,\"start\":48133},{\"end\":48152,\"start\":48142},{\"end\":48160,\"start\":48152},{\"end\":48173,\"start\":48160},{\"end\":48187,\"start\":48173},{\"end\":48544,\"start\":48530},{\"end\":48555,\"start\":48544},{\"end\":48565,\"start\":48555},{\"end\":48575,\"start\":48565},{\"end\":48982,\"start\":48966},{\"end\":48993,\"start\":48982},{\"end\":49378,\"start\":49354},{\"end\":49401,\"start\":49378},{\"end\":49428,\"start\":49401},{\"end\":49791,\"start\":49781},{\"end\":49800,\"start\":49791},{\"end\":49813,\"start\":49800},{\"end\":50132,\"start\":50123},{\"end\":50142,\"start\":50132},{\"end\":50155,\"start\":50142},{\"end\":50164,\"start\":50155},{\"end\":50170,\"start\":50164},{\"end\":50182,\"start\":50170},{\"end\":50189,\"start\":50182},{\"end\":50201,\"start\":50189},{\"end\":50209,\"start\":50201},{\"end\":50637,\"start\":50625},{\"end\":50647,\"start\":50637},{\"end\":50657,\"start\":50647},{\"end\":50969,\"start\":50960},{\"end\":50982,\"start\":50969},{\"end\":50997,\"start\":50982},{\"end\":51005,\"start\":50997},{\"end\":51022,\"start\":51005},{\"end\":51350,\"start\":51337},{\"end\":51360,\"start\":51350},{\"end\":51367,\"start\":51360}]", "bib_venue": "[{\"end\":37847,\"start\":37773},{\"end\":39253,\"start\":39187},{\"end\":42500,\"start\":42449},{\"end\":43431,\"start\":43365},{\"end\":51496,\"start\":51440},{\"end\":36985,\"start\":36945},{\"end\":37330,\"start\":37294},{\"end\":37771,\"start\":37682},{\"end\":38340,\"start\":38280},{\"end\":38773,\"start\":38733},{\"end\":39185,\"start\":39104},{\"end\":39705,\"start\":39669},{\"end\":40136,\"start\":40096},{\"end\":40506,\"start\":40477},{\"end\":40883,\"start\":40818},{\"end\":41351,\"start\":41302},{\"end\":41771,\"start\":41731},{\"end\":42060,\"start\":41993},{\"end\":42447,\"start\":42403},{\"end\":42891,\"start\":42853},{\"end\":43363,\"start\":43282},{\"end\":43728,\"start\":43697},{\"end\":44018,\"start\":43980},{\"end\":44313,\"start\":44277},{\"end\":44730,\"start\":44681},{\"end\":45143,\"start\":45114},{\"end\":45531,\"start\":45491},{\"end\":45893,\"start\":45796},{\"end\":46405,\"start\":46356},{\"end\":46922,\"start\":46843},{\"end\":47199,\"start\":47195},{\"end\":47457,\"start\":47383},{\"end\":47868,\"start\":47837},{\"end\":48235,\"start\":48187},{\"end\":48654,\"start\":48575},{\"end\":49048,\"start\":48993},{\"end\":49462,\"start\":49428},{\"end\":49859,\"start\":49813},{\"end\":50277,\"start\":50209},{\"end\":50703,\"start\":50657},{\"end\":51074,\"start\":51022},{\"end\":51438,\"start\":51367}]"}}}, "year": 2023, "month": 12, "day": 17}