{"id": 253224416, "updated": "2023-10-05 09:04:34.036", "metadata": {"title": "QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation", "authors": "[{\"first\":\"Krishna\",\"last\":\"Srinivasan\",\"middle\":[]},{\"first\":\"Karthik\",\"last\":\"Raman\",\"middle\":[]},{\"first\":\"Anupam\",\"last\":\"Samanta\",\"middle\":[]},{\"first\":\"Lingrui\",\"last\":\"Liao\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Bertelli\",\"middle\":[]},{\"first\":\"Mike\",\"last\":\"Bendersky\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation. Thus, in this paper we make the following contributions: (1) We demonstrate that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding. While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy), (2) we provide a practical and effective way of distilling Retrieval Augmentation LLMs. Specifically, we use a novel two-stage distillation approach that allows us to carry over the gains of retrieval augmentation, without suffering the increased compute typically associated with it. (3) We demonstrate the benefits of the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains. Via extensive experiments, including on public benchmarks, we believe this work offers a recipe for practical use of retrieval-augmented query understanding.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.15718", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/SrinivasanRSLBB22", "doi": "10.18653/v1/2022.emnlp-industry.50"}}, "content": {"source": {"pdf_hash": "65bad077608a3c2ed8eac242e993aa40aa8c13e9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.15718v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5be2615f606a25a0b92ae9deb7826362c275b375", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/65bad077608a3c2ed8eac242e993aa40aa8c13e9.txt", "contents": "\nQUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation\n\n\nKrishna Srinivasan krishnaps@google.com \nGoogle Research\nGoogle Research\n\n\nKarthik Raman karthikraman@google.com \nGoogle Research\nGoogle Research\n\n\nAnupam Samanta Google \nGoogle Research\nGoogle Research\n\n\nLingrui Liao Google lingrui@google.com \nGoogle Research\nGoogle Research\n\n\nLuca Bertelli Google \nGoogle Research\nGoogle Research\n\n\nMike Bendersky \nGoogle Research\nGoogle Research\n\n\nGoogle Research \nGoogle Research\nGoogle Research\n\n\nQUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation\n\nLarge Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation. Thus, in this paper we make the following contributions: (1) We demonstrate that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding. While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy), (2) we provide a practical and effective way of distilling Retrieval Augmentation LLMs. Specifically, we use a novel two-stage distillation approach that allows us to carry over the gains of retrieval augmentation, without suffering the increased compute typically associated with it. (3) We demonstrate the benefits of the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains. Via extensive experiments, including on public benchmarks, we believe this work offers a recipe for practical use of retrievalaugmented query understanding.\n\nIntroduction\n\nThe recent advent of billion+ parameter Large Language Models (LLMs) -such as T5 (Raffel et al., 2019), mT5 (Xue et al., 2021), GPT-3 (Brown et al., 2020) and most recently PaLM (Chowdhery et al., 2022) -has disrupted many language understanding tasks -with new benchmarks set or eclipsed routinely by these Transformer models and their variants.\n\nQueries -especially keyword search onespresent a unique challenge though. Their short * Corresponding Authors length, inherent ambiguity and lack of grammar mean query understanding tasks typically require more memorization and world knowledge than other NLP tasks (Broder et al., 2007). Consequently, despite LLMs leading performance on language and query understanding tasks -like intent classification, query parsing and relevance prediction -there is significant room for further improvement.\n\nIn this paper we leverage Retrieval-Augmentation to provide LLMs more context and grounding for search queries. We show that the titles and URLs of documents retrieved for the query, greatly help improve LLMs query understanding capabilities.\n\nWhile different retrieval augmentation models exist, we show that even simple concatenation of these titles / urls with the query can help improve LLM performance considerably.\n\nHowever, the use of retrieval augmentation leads to a new challenge: Increased complexity of LLM inference. More specifically, the quadratic complexity of self-attention in Transformer models means that the latency of LLMs blows up given these (often 10x+) longer input sequences. This presents a significant problem as LLMs are impractical for online use and thus need to be distilled into smaller, more efficient models to be served online. However knowledge distillation (Gou et al., 2021) into these student models requires a lot of distillation data annotated by these LLMs -which may not be feasible for these retrieval augmented models.\n\nThus as a remedy we introduce a new two-stage distillation approach. In the first stage of this approach we distill the retrieval-augmented (long input) LLM (the Professor) into a non-retrieval augmented (short input) LLM (the Teacher) using a small distillation set. This second LLM Teacher is in turn distilled into the final Student using a large set.\n\nVia extensive experiments on a large-scale, realworld problem and data we demonstrate that the resulting QUILL system provides for an efficient and effective way of retaining the performance gains of retrieval augmented LLMs on query understanding tasks.\n\n\nRelated Work\n\nLarge language models (LLMs) such as mT5 (Xue et al., 2021) demonstrated significant performance improvements on a variety of natural language understanding (NLU) tasks. Specifically in the context of query understanding, researchers found that (a) model size significantly effects the quality of the resulting models (Nogueira et al., 2019;Han et al., 2020), and (b) using additional context in the form of query-associated documents is crucial to the model performance due to the paucity of context available in the query itself (Nogueira and Lin, 2019;Zhang et al., 2020). Retrieval augmentation of the query with the search results retrieved by it is a proven way to incorporate such context in LLM training for NLU tasks, as has been shown recently by models such as RAG (Lewis et al., 2020), REALM (Guu et al., 2020), and RETRO (Borgeaud et al., 2022).\n\nIn this paper, we leverage this insight to improve performance of query intent prediction (Broder, 2002) -a crucial query understanding task that is at the heart of modern search engines -using LLMs. Prior work by Broder et al. (2007) found importance of retrieval augmentation using statistical methods for this task. Statistical retrieval augmentation has also been found critical for other query understanding tasks including query expansion (Broder et al., 2008;Diaz and Metzler, 2006) and query tagging (Wang, 2020). We demonstrate similar benefits when using retrieval-augmented LLMs as well.\n\nWe also leverage Knowledge Distillation (Hinton et al., 2015;Mirzadeh et al., 2020;Gou et al., 2021) techniques to create a Student model that retains the LLMs gains.\n\n\nQuery Intent Understanding\n\nWhile the techniques described in this paper could be applied to any query understanding task, for the sake of brevity we focus on the task of query intent classification. Query intent (QI) classification is a classical IR task studied for over two decades (Kang and Kim, 2003; Baeza-Yates et al.,\n\n\nData\n\nTrain Val Test Unlabeled Orcas-I 1.28M 1K 1K 10.3M EComm 36K 4K 4K 128M  Kathuria et al., 2010;Lewandowski et al., 2012;Figueroa, 2015;Mohasseb et al., 2019). This task is particularly important in practice, as it is at the top of the search funnel, and the entire search engine behavior may vary based on the predicted query intent. Given the centrality of this task on overall retrieval, models for this task need to be both fast (i.e., low latency) and high efficacy. Thus even a single percentage point quality gain on the QI task can be considered a major accomplishment.\n\nIn this paper we tackle the QI task using LLMs. In particular we use two datasets in our study whose details are provided in Table 1: \u2022 EComm: Our main dataset will be a realworld dataset. Cast as a binary classification problem, this task involves identifying queries with a specific intent -where the required intent is similar to the transactional intent of the Broder taxonomy (Broder, 2002) in the context of e-commerce. As common in realworld applications, the human labeled data is accompanied by a large unlabeled set -that is used for knowledge distillation.\n\n\u2022 Orcas-I: The largest publicly available query intent dataset is ORCAS-I (Alexander et al., 2022). This comprises queries of the ORCAS dataset (Craswell et al., 2020) labeled with one of 5 intent classes. Note that while the test set is human-labeled, the training set labels are weak labels as detailed in ORCAS-I (Alexander et al., 2022) paper's Methodology section.\n\n\nQUILL Methodology\n\nThe keyword nature of queries and lack of context make the QI task (like other query understanding tasks) challenging for LLMs. Thus we propose QUILL as a solution. As seen in Figure 1, QUILL consists of two stages: (a) Retrieval Augmented LLM training, (b) Multi-Stage Distillation into efficient student.\n\nRetrieval Augmented (RA) LLM:\n\nThe key insight here is that titles / urls of related documents   could provide valuable context to help understand the intent of the query. For example, it may not be immediately apparent what a query like ua 1234 may mean. However, via the retrieved documents we can understand that the query is seeking information about a United Airlines flight. While there are multiple ways of augmenting the input via retrieved documents (example: the Fusion-in-Decoder architecture (Izacard and Grave, 2020)), we chose to study the most straightforward and popular approach of concatenating the titles / urls of the retrieved top-k documents with the original query as the input to our LLM. As shown empirically (Sec 5), this model outperforms all baselines -demonstrating the value of additional context.\n\nMulti-stage distillation: The drawback of RA is the additional sequence length of the input. As seen from Table 2, augmenting a query with (upto) 10 titles and urls increases the sequence length by an order of magnitude. Consequently, this makes distillation far more challenging given the quadratic complexity of sequence length (due to self-attention) in transformer models. This leaves us in a dichotomy between a more effective model with a much smaller distillation set, vs. a lower performing model with a larger distillation set. Given a large distillation set is required for training an effective student, this leaves us at risk of not being able to benefit from RA, given that a very large dataset with RA will incur very long and impractical inference times.\n\nTo get the best of both worlds we propose a twostage distillation approach. In the first stage we distill the Professor RA LLM into a Teacher LLM without RA. The Teacher model uses ExpandTerms which provide additional context to the queries. While this may not be as expressive as retrieval augmentation, this provides a good compromise of greatly reducing sequence length while giving up only a little in performance. We do so by using a small subset of the unlabeled data. As shown empirically, a LLM teacher trained in this manner performs significantly better than a non-RA LLM trained directly on the human data, while at the same time allowing us to efficient distillation.\n\nIn the second stage we use the Teacher LLM to annotate the entire unlabeled dataset. This is in turn used to train the final Student model that will be used in practice.\n\n\nExperiments and Results\n\nExperimental Setup: Our experiments were all conducted using the mT5 (Xue et al., 2021) checkpoints. We validate performance across three learning rates (1e-3, 1e-4, 5e-5) -selecting the best checkpoint using the validation set loss. For mod-els trained from the provided training sets, we used a batch size of 64 in our experiments and trained for 4K steps (EComm) / 20K steps (Orcas-I).\n\nFor distilled models, we used a batch size of 128 for Teacher models and 1024 for Student models. We use different batch sizes because of the model architectures, mT5 for the Teacher vs a BERTbased model for the Student. In both cases, we trained for 1 epoch, unless mentioned otherwise. We only use the encoder of the mT5 model with an additional layer added on top to predict the classification scores. The Professor, the Teacher and the Student fine-tuning experiments are all set up as a query intent classification task. Given that the Teacher and Student models are trained on millions of examples and this itself is a time and resource intensive step, we restrict our experiments to only one epoch. We demonstrate performance gains even with one epoch via the techniques elaborated in this paper.\n\nWe studied the effect of distillation data size, for both stages of distillation. For the EComm dataset, we used an in-house retriever to find related documents. For Orcas-I, we use the provided docids (aggregated at per-query level) for retrieval augmentation. Unless specified, we use (upto) the top-10 results for retrieval augmentation 1 . Sequence length for models are based on the training set and features (set to 99%-percentile of sequence lengths).\n\nStudents and Features: Our experiments demonstrate results for a fast, efficient 4-layer transformer student architecture, with hidden dimensionality of 256. We default to using the query as the only feature in the student for simplicity. To compare against query expansion techniques, we used a sophisticated in-house memorization-based query expansion model in our Professor / Teacher experiments on EComm. This expansion model -which we refer to as ExpandTerms -provides a list of related terms for a given query, which are concatenated with the query (and identifiers for start / end of each feature).\n\nMetrics: To compare performance of different models we use two metrics: MicroF1 and MacroF1 for Orcas-I, and AUC-PR and AUC-ROC for EComm. For EComm, we only report\n\n\nModel\n\nSize ROC PR query Base 0.0% 0.0% + RA (titles, urls) Base +4.3% +4.6% query XL +2.7% +3.1% + RA (titles, urls) XL +6.3% +6.7% query XXL +3.0% +3.3% + RA (titles, urls) XXL +6.4% +6.9%  performance of models relative to the mT5 queryonly Base-sized model 2 .\n\n\nEffect of Retrieval Augmentation\n\nWhile the use of retrieval augmentation (RA) has been known to improve query classification performance (Broder et al., 2007), the benefit of RA is unclear in the age of LLMs. Thus, we start by evaluating the first stage of QUILL i.e., the RA model. As seen in Table 3, RA improves performance significantly across all model sizes including the billion-parameter+ XL and XXL models. In fact the gains from RA on the Base-sized model exceed the gains obtained by increasing model size of a query-only model to XXL. Given the gains observed across all models sizes, we use Base-sized models in the rest of the paper to simplify experimentation. +8.2% +6.2% + RA (titles+urls) +9.0% +7.2%  A natural question that may arise though is how do these gains from RA compare to those obtained by powerful query expansion techniques. Thus, we performed an in-depth ablation of features for the EComm dataset (on a Base-sized model for ease of experimentation) as seen in Table 4 and for the Orcas-I dataset as seen in Table 5. These results clearly demonstrate the potency of powerful query expansion models (i.e., ExpandTerms) -as evidenced by the large \u223c2% gains over query-only models. However, we find that RA adds even more value over these highly sophisticated expansion models with an a nearly 5+% increase in performance. Furthermore, we find that RA techniques can still be combined with query expansion for further gains.\n\n\nEComm\n\nThe improvements on RA for Orcas-I (seen in Table 5) are even more substantial, with a nearly 9% improvement over the query-only baseline, via the use of the titles and urls of related documents. Interestingly, among RA features we find that urls tend to perform slightly better than titles on both datasets. We believe this to be because titles can have a higher variance of informativeness -with both highly verbose and very short titles commonly seen. Hence, given the simplicity and consistency of urls, we chose to use RA(urls) for subsequent experiments as the Professor model.\n\n\nDistilling gains from RA\n\nWe next focus on the second stage of QUILL: Distilling the RA model. Typically larger amounts of distillation data lead to better performance. However, given the increased sequence length of RA models and the cost of retrieval augmentation itself, annotating large distillation sets is highly challenging. Thus to capture such practical trade-offs, we  only used a small subset of the unlabeled data for the QUILL Professor to Teacher distillation. In particular, we used 4M examples for EComm (i.e., 3.1% of unlabeled data) and 2M for Orcas-I (19%) for this first stage of distillation -to represent a set that is small enough set to be practical, but large enough to learn from. However, we do share results for varying this size to understand its importance.\n\nQUILL Teacher models were thus trained by distilling the RA(urls) Professor models. Our Teacher models had the same capacity and architecture (i.e., mT5-Base as the Professor 3 -except it does not use RA (features).\n\nAs a realistic and competitive baseline, we chose a Baseline Teacher that resembles the QUILL Teacher in all aspects bar one -the data they are trained on. Specifically, the Baseline Teacher is directly trained from the gold-labeled training data, unlike the QUILL teacher. We believe this is representative of practical applications today, where LLMs are trained directly on gold-labeled sets (before being distilled into the final student models). To further challenge QUILL, we leverage the powerful ExpandTerms features (for the EComm dataset) in our Teacher models -both Baseline and QUILL. We believe this provides a more challenging but realistic evaluation setup, since many baseline models in use today avail of powerful features (along with the query).\n\nAs seen from the results in Table 6 and Table 7, we find the QUILL Teachers provide a significant performance improvement over the Baseline Teacher, despite having never directly seen the gold label data. On EComm, despite using an enhanced (realistic) baseline, QUILL teachers are \u223c1% better on all metrics. We find a similar gap on Orcas-I despite the Teacher there being trained on only 2M examples (just 1.5x the training set size). Put differently, we now have trained our non-retrieval aug-\n\n\nModel (# Distillation)\n\n\nROC PR\n\nNo Distillation Student -6.3% -7.3% Baseline Student -0.9% -1.6% QUILL Student +2.0% +1.5% QUILL 1-Stage Student(4M) +0.4% -0.2% QUILL 1-Stage Student(32M) +1.1% +0.6% Table 8: Performance of the different student models trained from different teachers and using differing amounts of distillation data (on EComm). mented language model to benefit from the gains of retrieval augmentation. Even though the student model does not have Retrieval Augmentation, because of the Teacher model's performance improvement, it is possible to annotate a considerably large number of training examples. We observe the Student models to close the gap (compared to the Teacher) given larger training datasets.\n\nTo test the robustness of QUILL teachers we also varied the amount of distillation data used -halving or doubling it. While there still exist distillation gaps to the professor (which can be narrowed via more distillation data) on both datasets, our proposed approach works well even when using small amounts of distillation data -which in turn allows us to save significant compute.\n\n\nQuery\n\nExample  \n\n\nFinal student training\n\nSo far, we have shown that QUILL can learn a better (non-RA) teacher. However, an important question remains unanswered: Can these Teacher gains be translated to the final student model? In particular, we postulate that the predictions of the QUILL Teacher may be more robust and easier to learn (for student models) than those of the Baseline. To verify this hypothesis we compared 4 fast student models (4-layer encoder-only models), with the only difference being the data they were trained on:\n\n\u2022 No Distillation Student: This is the simple solution of directly training the Student using the labeled data.\n\n\u2022 Baseline Student: This is the current standard involving distilling the Baseline Teacher model using the full unlabeled set.\n\n\u2022 QUILL Student: This is the proposed solution involving distilling the QUILL Teacher model using the full unlabeled set.\n\n\u2022 QUILL 1-Stage Student: Rather than the two stage distillation approach, this student is directly distilled from the Professor using a subset of the unlabeled data.\n\nAs seen from Table 8, all QUILL-based students significantly outperform the Baseline Student. In particular our proposed 2-stage approach leads to a \u223c3 point gain on both metrics. This is notable in that the gap between QUILL and Baseline students is even higher than the Teachers -which we attribute to the QUILL Teacher labels being more robust.\n\nComparing different QUILL students, we find that there is a notable performance gain by first distilling into a non-RA teacher, before distilling into the final student. While 1-stage distillation performance improves as more data is used, even when 1/4th of all unlabeled data is retrieval-augmented and annotated by the Professor for direct distillation, it still falls short of the 2-stage approach. Together, these results show: (1) QUILL students outperform the current state-of-the-art significantly, and (2) QUILL benefits from the 2 stage distillation of Professor to Teacher to final student.\n\n\nExamples of Wins/Losses from RA\n\nWhile the previous sections focused on demonstrating the efficacy (and efficiency) of QUILL, we wanted to also understand why and where are some of these gains from RA stem from. To do so we used the test-set of Orcas-I and sampled illustrative examples of wins / losses (Table 9) between the baseline and the retrieval-augmented professor models. One common win pattern we found for RA models is when the query is unclear, or uses technical terms / abbreviations. In these cases, the augmented urls / titles help provide additional context for the language model to understand what the query is about. On the flip side, we also found the biggest loss pattern to be when retrieval was inaccurate, which in turn misled the model regarding the query intent. For example, we found our retriever returned wikipedia results more often than it should, which mislead the model to believe the query had Factual intent.\n\n\nFuture Work\n\nWhile we studied the problem of query intent classification in this paper, the approach proposed in our paper is general and could be applied to any query understanding task. Following our approach, could enable myriad query understanding tasks use retrieval augmentation in a practically realistic and efficient manner. We leave this to future work though. We should also note that our experiments reveal non-trivial distillation gaps in both stages of distillation, which we believe is another open opportunity for future improvements.\n\n\nConclusion\n\nThis paper provides a practical recipe for combining Retrieval Augmentation and Large Language Models. In particular, we proposed QUILL as an approach to tackle the problem of query intent classification. Our empirical study demonstrates conclusively that Retrieval Augmentation can provide significant value over existing approaches. Furthermore we show that via our two-stage distillation approach, that QUILL not only learns better performing, more robust teachers, but also leads to even bigger gains when distilled into fast, realworld capable production student models.\n\n\nLimitations\n\nThis paper focuses on efficient and effective way of improving query intent classification using Retrieval Augmentation (RA) and Multi-stage distillation. While we have made the best attempts to ensure a robust and efficient method, we would be remiss to not point out some key limitations of our work:\n\n\u2022 Quality of Retrieval: A key reason for the gains seen in this paper is the use of Retrieval Augmentation. This additional context provided in the form of result titles / URLs are helpful, but are dependent on the quality of the retrieval system. While we did not get a chance to explore the dependence of performance gains on retrieval quality, we plan to explore this in future work.\n\n\u2022 Dependency of Retrieval: While our approach provides for a practical and lowcompute way of incorporating retrieval augmentation, it still does add some compute (to augment the datasets) and system complexity. While we considered this trade-off well worth it in our use case, this may depend on specific settings.\n\n\u2022 Retrieval-Augmentation techniques: As discussed in Section 4, we used a simple concatenation based retrieval augmentation. However, there do exist more sophisticated techniques for retrieval augmentation. For example, models built on a Fusion-in-Decoder (Izacard and Grave, 2020) backbone have demonstrated great performance (Hofst\u00e4tter et al., 2022b;Izacard et al., 2022) and improved efficiency (Hofst\u00e4tter et al., 2022a). We believe that these more sophisticated retrieval-augmentation technique may bring further improvements in our system and leave this for future work to follow up on.\n\n\u2022 Datasets: The lack of large public query sets means that we were very limited in terms of what public benchmarks we could study this problem on. While ORCAS-I is the largest such available set, they lack many alternatives that are large enough to study the effects of distillation. In the future though, we hope to use the (somewhat related) problem of questionanswering where larger datasets (with large enough unlabeled data) exist for a more thorough study.\n\n\u2022 Distillation gaps: Our results also clearly demonstrate large distillation gaps in both stages. While there have been innovative techniques proposed to improve distillation performance, we intentionally chose to keep things simple as those approaches are largely complementary to the problem we study in this work.\n\n\u2022 Limited \"Large\" Model Experiments: While our work is intended for and positioned in the context of \"Large\" Language Models, we realize that our most common model choice (mT5-Base), may not be the most representative model in that category. This was an intentional choice on our end as we hoped doing so would make the work more relevant to use cases and applications with more limited compute. For practitioners interested in models with tens of billions of parameters, we refer them to our analysis of mT5-XXL sized models in Table 3, that demonstrates the viability of our approach on models of that scale.\n\n\nEthics Statement\n\nIn this paper, we used only publicly available Language Model and Checkpoints that have been previously published -namely mT5. An important consideration when working with query datasets is data privacy. This is perhaps the biggest reason why there do not exist many large public query datasets. We intentionally chose ORCAS-I for this reason, as it is constructed from the ORCAS query set -which is widely regarded as a well-constructed, non-PII, sufficiently anonymized query dataset. While the EComm dataset used in this paper is proprietary, we should note that it too has been scrubbed of PII and aims to follow the same (if not higher) data privacy principles. Our data (and methodology) do not contain any information for or target any demographic or identity characteristics.\n\nThe task we focus on -query classificationis a general problem that benefits everyone. In fact, it can enable better IR systems thereby benefiting users who otherwise might not get answers. Thus, we do not anticipate any biases or misuse issues stemming from this. We believe that by using publicly available and vetted retrieval models, the resulting retrieval augmented models should not create any new or further any existing biases.\n\nIn many ways a goal of our work is making retrieval augmentation more practical and reducing compute needs for any such applications. While we did present results with XXL sized models, we focused most of our experiments on the smaller, more efficient Base-sized models so as to benefit a wider section of our community and to reduce the computational needs of our experiments.\n\nFigure 1 :\n1QUILL Architecture : Retrieval Augmentation and Multi-stage Distillation.\n\nTable 1 :\n1Statistics of datasets used.2006; Jansen et al., 2008; \n\nTable 2 :\n2mT5 sequence lengths by features.\n\nTable 3 :\n3Results demonstrating the benefit of Retrieval Augmentation (RA) across all model sizes.EComm \nROC PR \nquery \n0.0% 0.0% \n+ Terms \n+2.6% +1.9% \n+ RA (titles) \n+4.8% +4.8% \n+ RA (titles) + Terms +5.1% +5.2% \n+ RA (urls) \n+5.3% +5.7% \n\n\n\nTable 4 :\n4Analysis of the impact of different features \n(using Base-sized models) for the EComm dataset. Ex-\npandTerms abbreviated as Terms. \n\n\n\nTable 5 :\n5Analysis of the impact of different features (using Base-sized models) for the Orcas-I dataset.EComm \n\nROC PR \nBaseline Teacher \n+2.6% +1.9% \n(Finetuned on Training Set) \nQUILL Teacher \n+3.3% +2.8% \n(2M Prof Distilled Set) \nQUILL Teacher (4M) \n+3.4% +2.9% \nQUILL Teacher (8M) \n+3.5% +2.9% \n\nQUILL Professor \n+5.3% +5.7% \n\n\n\nTable 6 :\n6Comparison of different Teacher models \ntrained directly or via Professor-distillation for the \nEComm dataset. \n\n\n\nTable 7 :\n7Comparison of different Teacher models trained directly or via Professor-distillation for the Orcas-I dataset.\n\nTable 9 :\n9Wins/losses examples on Orcas-I.\nFor Orcas-I, nearly 2/3rd of the queries only have a single provided result, while some have upwards of 2000 results, which is why the lengths for RA features on Orcas-I inTable 2are smaller. The 10 results augmented are randomly chosen if more exist.\nFor a sense of scale, each 0.5% point increase in metrics on EComm is considered a significant gain.\nWe observed similar trends even if the Teacher had less capacity than the Professor.\nAcknowledgementsWe sincerely thank Jiecao Chen, William Dennis Kunz, Austin Tarango, Lee Gardner, Yang Zhang, Constance Wang, Derya Ozkan, Nitin Nalin, Raphael Hoffmann, Iftekhar Naim, Siddhartha Brahma, Siamak Shakeri, Hongkun Yu, John Nham, Ming-Wei Chang, Marc Najork, Corinna Cortes and many others for their insightful feedback and help. We also thank the EMNLP Reviewers for their thorough review, feedback and suggestions.\nORCAS-i. Daria Alexander, Wojciech Kusa, Arjen P De Vries, 10.1145/3477495.3531737Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalACMDaria Alexander, Wojciech Kusa, and Arjen P. de Vries. 2022. ORCAS-i. In Proceedings of the 45th Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval. ACM.\n\nThe intention behind web queries. Ricardo Baeza-Yates, Liliana Calder\u00f3n-Benavides, Cristina Gonz\u00e1lez-Caro, International symposium on string processing and information retrieval. SpringerRicardo Baeza-Yates, Liliana Calder\u00f3n-Benavides, and Cristina Gonz\u00e1lez-Caro. 2006. The intention behind web queries. In International symposium on string processing and information retrieval, pages 98-109. Springer.\n\nImproving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, PMLRInternational Conference on Machine Learning. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from tril- lions of tokens. In International Conference on Ma- chine Learning, pages 2206-2240. PMLR.\n\nA taxonomy of web search. Andrei Broder, ACM Sigir forum. New York, NY, USAACM36Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, volume 36, pages 3-10. ACM New York, NY, USA.\n\nSearch advertising using web relevance feedback. Andrei Z Broder, Peter Ciccolo, Marcus Fontoura, Evgeniy Gabrilovich, Vanja Josifovski, Lance Riedel, 10.1145/1458082.1458217Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM '08. the 17th ACM Conference on Information and Knowledge Management, CIKM '08New York, NY, USAAssociation for Computing MachineryAndrei Z. Broder, Peter Ciccolo, Marcus Fontoura, Evgeniy Gabrilovich, Vanja Josifovski, and Lance Riedel. 2008. Search advertising using web rele- vance feedback. In Proceedings of the 17th ACM Conference on Information and Knowledge Manage- ment, CIKM '08, page 1013-1022, New York, NY, USA. Association for Computing Machinery.\n\nRobust classification of rare queries using web knowledge. Z Andrei, Marcus Broder, Evgeniy Fontoura, Amruta Gabrilovich, Vanja Joshi, Tong Josifovski, Zhang, Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. the 30th annual international ACM SIGIR conference on Research and development in information retrievalAndrei Z Broder, Marcus Fontoura, Evgeniy Gabrilovich, Amruta Joshi, Vanja Josifovski, and Tong Zhang. 2007. Robust classification of rare queries using web knowledge. In Proceedings of the 30th annual international ACM SIGIR confer- ence on Research and development in information retrieval, pages 231-238.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\n\nOrcas: 20 million clicked query-document pairs for analyzing search. Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, Bodo Billerbeck, 10.1145/3340531.3412779Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM '20. the 29th ACM International Conference on Information & Knowledge Management, CIKM '20New York, NY, USAAssociation for Computing MachineryNick Craswell, Daniel Campos, Bhaskar Mitra, Em- ine Yilmaz, and Bodo Billerbeck. 2020. Orcas: 20 million clicked query-document pairs for analyz- ing search. In Proceedings of the 29th ACM Inter- national Conference on Information & Knowledge Management, CIKM '20, page 2983-2989, New York, NY, USA. Association for Computing Machin- ery.\n\nImproving the estimation of relevance models using large external corpora. Fernando Diaz, Donald Metzler, Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. the 29th annual international ACM SIGIR conference on Research and development in information retrievalFernando Diaz and Donald Metzler. 2006. Improving the estimation of relevance models using large exter- nal corpora. In Proceedings of the 29th annual inter- national ACM SIGIR conference on Research and development in information retrieval, pages 154- 161.\n\nExploring effective features for recognizing the user intent behind web queries. Alejandro Figueroa, Computers in Industry. 68Alejandro Figueroa. 2015. Exploring effective features for recognizing the user intent behind web queries. Computers in Industry, 68:162-169.\n\nKnowledge distillation: A survey. Jianping Gou, Baosheng Yu, J Stephen, Dacheng Maybank, Tao, International Journal of Computer Vision. 1296Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789-1819.\n\nPanupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, PMLRInternational Conference on Machine Learning. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa- supat, and Mingwei Chang. 2020. Retrieval aug- mented language model pre-training. In Inter- national Conference on Machine Learning, pages 3929-3938. PMLR.\n\nShuguang Han, Xuanhui Wang, 10.48550/ARXIV.2004.08476arXiv:2004.08476Mike Bendersky, and Marc Najork. 2020. Learning-to-rank with bert in tf-ranking. arXiv preprintShuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2020. Learning-to-rank with bert in tf-ranking. arXiv preprint arXiv:2004.08476.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 2arXiv preprintGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7).\n\nFid-light: Efficient and effective retrieval-augmented text generation. Sebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, Hamed Zamani, arXiv:2209.14290arXiv preprintSebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2022a. Fid-light: Efficient and ef- fective retrieval-augmented text generation. arXiv preprint arXiv:2209.14290.\n\nMulti-task retrievalaugmented text generation with relevance sampling. Sebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, Hamed Zamani, arXiv:2207.03030arXiv preprintSebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2022b. Multi-task retrieval- augmented text generation with relevance sampling. arXiv preprint arXiv:2207.03030.\n\nLeveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, arXiv:2007.01282arXiv preprintGautier Izacard and Edouard Grave. 2020. Lever- aging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, arXiv:2208.03299and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. arXiv preprintGautier Izacard, Patrick Lewis, Maria Lomeli, Lu- cas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299.\n\nDetermining the informational, navigational, and transactional intent of web queries. J Bernard, Danielle L Jansen, Amanda Booth, Spink, Information Processing & Management. 443Bernard J Jansen, Danielle L Booth, and Amanda Spink. 2008. Determining the informational, navi- gational, and transactional intent of web queries. In- formation Processing & Management, 44(3):1251- 1266.\n\nQuery type classification for web document retrieval. In-Ho Kang, Gilchang Kim, Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. the 26th annual international ACM SIGIR conference on Research and development in informaion retrievalIn-Ho Kang and GilChang Kim. 2003. Query type clas- sification for web document retrieval. In Proceed- ings of the 26th annual international ACM SIGIR conference on Research and development in infor- maion retrieval, pages 64-71.\n\nClassifying the user intent of web queries using k-means clustering. Ashish Kathuria, J Bernard, Carolyn Jansen, Amanda Hafernik, Spink, Internet ResearchAshish Kathuria, Bernard J Jansen, Carolyn Hafernik, and Amanda Spink. 2010. Classifying the user in- tent of web queries using k-means clustering. Inter- net Research.\n\nDeriving query intents from web search engine queries. Dirk Lewandowski, Jessica Drechsler, Sonja Von Mach, Journal of the American Society for Information Science and Technology. 639Dirk Lewandowski, Jessica Drechsler, and Sonja Von Mach. 2012. Deriving query intents from web search engine queries. Journal of the Ameri- can Society for Information Science and Technology, 63(9):1773-1788.\n\nRetrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, Advances in Neural Information Processing Systems. 33Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock- t\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459-9474.\n\nImproved knowledge distillation via teacher assistant. Mehrdad Seyed Iman Mirzadeh, Ang Farajtabar, Nir Li, Akihiro Levine, Hassan Matsukawa, Ghasemzadeh, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020. Improved knowledge distil- lation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, vol- ume 34, pages 5191-5198.\n\nA customised grammar framework for query classification. Alaa Mohasseb, Mohamed Bader-El-Den, Mihaela Cocea, Expert Systems with Applications. 135Alaa Mohasseb, Mohamed Bader-El-Den, and Mihaela Cocea. 2019. A customised grammar framework for query classification. Expert Systems with Applica- tions, 135:164-180.\n\nDocument expansion by query prediction. Rodrigo Nogueira, Jimmy Lin, arXiv:1904.083756. Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun ChoarXiv preprintFrom doc2query to doctttttqueryRodrigo Nogueira and Jimmy Lin. 2019. From doc2query to doctttttquery. Online preprint, 6. Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. arXiv preprint arXiv:1904.08375.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.\n\nQuery Segmentation and Tagging. Xuanhui Wang, 10.1007/978-3-030-58334-7_3Springer International PublishingChamXuanhui Wang. 2020. Query Segmentation and Tag- ging, pages 43-67. Springer International Publish- ing, Cham.\n\nAditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, 10.18653/v1/2021.naacl-main.41Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLinting Xue, Noah Constant, Adam Roberts, Mi- hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computa- tional Linguistics.\n\nQuery understanding via intent description generation. Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Xueqi Cheng, 10.1145/3340531.3411999Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM '20. the 29th ACM International Conference on Information & Knowledge Management, CIKM '20New York, NY, USAAssociation for Computing MachineryRuqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query understanding via intent description generation. In Proceedings of the 29th ACM International Conference on Informa- tion & Knowledge Management, CIKM '20, page 1823-1832, New York, NY, USA. Association for Computing Machinery.\n", "annotations": {"author": "[{\"end\":182,\"start\":108},{\"end\":255,\"start\":183},{\"end\":312,\"start\":256},{\"end\":386,\"start\":313},{\"end\":442,\"start\":387},{\"end\":492,\"start\":443},{\"end\":543,\"start\":493}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":116},{\"end\":196,\"start\":191},{\"end\":277,\"start\":271},{\"end\":332,\"start\":321},{\"end\":407,\"start\":401},{\"end\":457,\"start\":448},{\"end\":508,\"start\":500}]", "author_first_name": "[{\"end\":115,\"start\":108},{\"end\":190,\"start\":183},{\"end\":262,\"start\":256},{\"end\":270,\"start\":263},{\"end\":320,\"start\":313},{\"end\":391,\"start\":387},{\"end\":400,\"start\":392},{\"end\":447,\"start\":443},{\"end\":499,\"start\":493}]", "author_affiliation": "[{\"end\":181,\"start\":149},{\"end\":254,\"start\":222},{\"end\":311,\"start\":279},{\"end\":385,\"start\":353},{\"end\":441,\"start\":409},{\"end\":491,\"start\":459},{\"end\":542,\"start\":510}]", "title": "[{\"end\":105,\"start\":1},{\"end\":648,\"start\":544}]", "venue": null, "abstract": "[{\"end\":1921,\"start\":650}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2039,\"start\":2018},{\"end\":2063,\"start\":2045},{\"end\":2091,\"start\":2065},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2139,\"start\":2115},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2571,\"start\":2550},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3697,\"start\":3679},{\"end\":4535,\"start\":4518},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4818,\"start\":4795},{\"end\":4835,\"start\":4818},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5032,\"start\":5008},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5051,\"start\":5032},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5273,\"start\":5253},{\"end\":5299,\"start\":5281},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5334,\"start\":5311},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5440,\"start\":5427},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5571,\"start\":5551},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5803,\"start\":5782},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5826,\"start\":5803},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5857,\"start\":5845},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5998,\"start\":5977},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6020,\"start\":5998},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6037,\"start\":6020},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6410,\"start\":6391},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6535,\"start\":6513},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6560,\"start\":6535},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6575,\"start\":6560},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6597,\"start\":6575},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7413,\"start\":7399},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7685,\"start\":7661},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7754,\"start\":7731},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7927,\"start\":7903},{\"end\":10851,\"start\":10833},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13619,\"start\":13598},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24312,\"start\":24287},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24384,\"start\":24358},{\"end\":24405,\"start\":24384},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24456,\"start\":24430}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27726,\"start\":27640},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27794,\"start\":27727},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27840,\"start\":27795},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28086,\"start\":27841},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28232,\"start\":28087},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":28567,\"start\":28233},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":28693,\"start\":28568},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":28816,\"start\":28694},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":28861,\"start\":28817}]", "paragraph": "[{\"end\":2283,\"start\":1937},{\"end\":2781,\"start\":2285},{\"end\":3025,\"start\":2783},{\"end\":3203,\"start\":3027},{\"end\":3848,\"start\":3205},{\"end\":4204,\"start\":3850},{\"end\":4460,\"start\":4206},{\"end\":5335,\"start\":4477},{\"end\":5935,\"start\":5337},{\"end\":6103,\"start\":5937},{\"end\":6431,\"start\":6134},{\"end\":7016,\"start\":6440},{\"end\":7585,\"start\":7018},{\"end\":7956,\"start\":7587},{\"end\":8284,\"start\":7978},{\"end\":8315,\"start\":8286},{\"end\":9113,\"start\":8317},{\"end\":9884,\"start\":9115},{\"end\":10565,\"start\":9886},{\"end\":10736,\"start\":10567},{\"end\":11152,\"start\":10764},{\"end\":11957,\"start\":11154},{\"end\":12417,\"start\":11959},{\"end\":13024,\"start\":12419},{\"end\":13190,\"start\":13026},{\"end\":13457,\"start\":13200},{\"end\":14915,\"start\":13494},{\"end\":15508,\"start\":14925},{\"end\":16298,\"start\":15537},{\"end\":16515,\"start\":16300},{\"end\":17279,\"start\":16517},{\"end\":17777,\"start\":17281},{\"end\":18507,\"start\":17813},{\"end\":18892,\"start\":18509},{\"end\":18911,\"start\":18902},{\"end\":19435,\"start\":18938},{\"end\":19548,\"start\":19437},{\"end\":19676,\"start\":19550},{\"end\":19799,\"start\":19678},{\"end\":19966,\"start\":19801},{\"end\":20315,\"start\":19968},{\"end\":20918,\"start\":20317},{\"end\":21864,\"start\":20954},{\"end\":22417,\"start\":21880},{\"end\":23007,\"start\":22432},{\"end\":23325,\"start\":23023},{\"end\":23713,\"start\":23327},{\"end\":24029,\"start\":23715},{\"end\":24624,\"start\":24031},{\"end\":25088,\"start\":24626},{\"end\":25406,\"start\":25090},{\"end\":26018,\"start\":25408},{\"end\":26822,\"start\":26039},{\"end\":27260,\"start\":26824},{\"end\":27639,\"start\":27262}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7151,\"start\":7143},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":9228,\"start\":9221},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13762,\"start\":13755},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":14462,\"start\":14455},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":14509,\"start\":14502},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":14976,\"start\":14969},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":17328,\"start\":17309},{\"end\":17988,\"start\":17981},{\"end\":19988,\"start\":19981},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":21233,\"start\":21225},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25944,\"start\":25937}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1935,\"start\":1923},{\"attributes\":{\"n\":\"2\"},\"end\":4475,\"start\":4463},{\"attributes\":{\"n\":\"3\"},\"end\":6132,\"start\":6106},{\"end\":6438,\"start\":6434},{\"attributes\":{\"n\":\"4\"},\"end\":7976,\"start\":7959},{\"attributes\":{\"n\":\"5\"},\"end\":10762,\"start\":10739},{\"end\":13198,\"start\":13193},{\"attributes\":{\"n\":\"5.1\"},\"end\":13492,\"start\":13460},{\"end\":14923,\"start\":14918},{\"attributes\":{\"n\":\"5.2\"},\"end\":15535,\"start\":15511},{\"end\":17802,\"start\":17780},{\"end\":17811,\"start\":17805},{\"end\":18900,\"start\":18895},{\"attributes\":{\"n\":\"5.3\"},\"end\":18936,\"start\":18914},{\"attributes\":{\"n\":\"5.4\"},\"end\":20952,\"start\":20921},{\"attributes\":{\"n\":\"6\"},\"end\":21878,\"start\":21867},{\"attributes\":{\"n\":\"7\"},\"end\":22430,\"start\":22420},{\"end\":23021,\"start\":23010},{\"end\":26037,\"start\":26021},{\"end\":27651,\"start\":27641},{\"end\":27737,\"start\":27728},{\"end\":27805,\"start\":27796},{\"end\":27851,\"start\":27842},{\"end\":28097,\"start\":28088},{\"end\":28243,\"start\":28234},{\"end\":28578,\"start\":28569},{\"end\":28704,\"start\":28695},{\"end\":28827,\"start\":28818}]", "table": "[{\"end\":27794,\"start\":27767},{\"end\":28086,\"start\":27941},{\"end\":28232,\"start\":28099},{\"end\":28567,\"start\":28340},{\"end\":28693,\"start\":28580}]", "figure_caption": "[{\"end\":27726,\"start\":27653},{\"end\":27767,\"start\":27739},{\"end\":27840,\"start\":27807},{\"end\":27941,\"start\":27853},{\"end\":28340,\"start\":28245},{\"end\":28816,\"start\":28706},{\"end\":28861,\"start\":28829}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8162,\"start\":8154}]", "bib_author_first_name": "[{\"end\":29744,\"start\":29739},{\"end\":29764,\"start\":29756},{\"end\":29776,\"start\":29771},{\"end\":29778,\"start\":29777},{\"end\":30259,\"start\":30252},{\"end\":30280,\"start\":30273},{\"end\":30309,\"start\":30301},{\"end\":30697,\"start\":30688},{\"end\":30714,\"start\":30708},{\"end\":30729,\"start\":30723},{\"end\":30746,\"start\":30740},{\"end\":30757,\"start\":30752},{\"end\":30775,\"start\":30770},{\"end\":30792,\"start\":30786},{\"end\":30828,\"start\":30815},{\"end\":30844,\"start\":30838},{\"end\":30857,\"start\":30852},{\"end\":31284,\"start\":31278},{\"end\":31501,\"start\":31495},{\"end\":31503,\"start\":31502},{\"end\":31517,\"start\":31512},{\"end\":31533,\"start\":31527},{\"end\":31551,\"start\":31544},{\"end\":31570,\"start\":31565},{\"end\":31588,\"start\":31583},{\"end\":32227,\"start\":32226},{\"end\":32242,\"start\":32236},{\"end\":32258,\"start\":32251},{\"end\":32275,\"start\":32269},{\"end\":32294,\"start\":32289},{\"end\":32306,\"start\":32302},{\"end\":32900,\"start\":32897},{\"end\":32916,\"start\":32908},{\"end\":32927,\"start\":32923},{\"end\":32942,\"start\":32935},{\"end\":32957,\"start\":32952},{\"end\":32959,\"start\":32958},{\"end\":32976,\"start\":32968},{\"end\":32993,\"start\":32987},{\"end\":33013,\"start\":33007},{\"end\":33027,\"start\":33021},{\"end\":33042,\"start\":33036},{\"end\":33385,\"start\":33376},{\"end\":33403,\"start\":33397},{\"end\":33417,\"start\":33412},{\"end\":33433,\"start\":33426},{\"end\":33447,\"start\":33441},{\"end\":33460,\"start\":33456},{\"end\":33474,\"start\":33470},{\"end\":33501,\"start\":33494},{\"end\":33518,\"start\":33509},{\"end\":33941,\"start\":33937},{\"end\":33958,\"start\":33952},{\"end\":33974,\"start\":33967},{\"end\":33987,\"start\":33982},{\"end\":34000,\"start\":33996},{\"end\":34699,\"start\":34691},{\"end\":34712,\"start\":34706},{\"end\":35294,\"start\":35285},{\"end\":35515,\"start\":35507},{\"end\":35529,\"start\":35521},{\"end\":35535,\"start\":35534},{\"end\":35552,\"start\":35545},{\"end\":35875,\"start\":35869},{\"end\":35887,\"start\":35881},{\"end\":35897,\"start\":35893},{\"end\":36168,\"start\":36160},{\"end\":36181,\"start\":36174},{\"end\":36475,\"start\":36467},{\"end\":36489,\"start\":36484},{\"end\":36503,\"start\":36499},{\"end\":36810,\"start\":36801},{\"end\":36829,\"start\":36823},{\"end\":36843,\"start\":36836},{\"end\":36856,\"start\":36851},{\"end\":37158,\"start\":37149},{\"end\":37177,\"start\":37171},{\"end\":37191,\"start\":37184},{\"end\":37204,\"start\":37199},{\"end\":37520,\"start\":37513},{\"end\":37537,\"start\":37530},{\"end\":37747,\"start\":37740},{\"end\":37764,\"start\":37757},{\"end\":37777,\"start\":37772},{\"end\":37791,\"start\":37786},{\"end\":37807,\"start\":37802},{\"end\":37821,\"start\":37817},{\"end\":37834,\"start\":37830},{\"end\":37853,\"start\":37847},{\"end\":37871,\"start\":37862},{\"end\":38345,\"start\":38344},{\"end\":38363,\"start\":38355},{\"end\":38365,\"start\":38364},{\"end\":38380,\"start\":38374},{\"end\":38700,\"start\":38695},{\"end\":38715,\"start\":38707},{\"end\":39248,\"start\":39242},{\"end\":39260,\"start\":39259},{\"end\":39277,\"start\":39270},{\"end\":39292,\"start\":39286},{\"end\":39556,\"start\":39552},{\"end\":39577,\"start\":39570},{\"end\":39594,\"start\":39589},{\"end\":39598,\"start\":39595},{\"end\":39963,\"start\":39956},{\"end\":39976,\"start\":39971},{\"end\":39994,\"start\":39984},{\"end\":40008,\"start\":40003},{\"end\":40026,\"start\":40018},{\"end\":40043,\"start\":40038},{\"end\":40059,\"start\":40051},{\"end\":40073,\"start\":40069},{\"end\":40088,\"start\":40081},{\"end\":40097,\"start\":40094},{\"end\":40532,\"start\":40525},{\"end\":40557,\"start\":40554},{\"end\":40573,\"start\":40570},{\"end\":40585,\"start\":40578},{\"end\":40600,\"start\":40594},{\"end\":41061,\"start\":41057},{\"end\":41079,\"start\":41072},{\"end\":41101,\"start\":41094},{\"end\":41362,\"start\":41355},{\"end\":41378,\"start\":41373},{\"end\":41821,\"start\":41816},{\"end\":41834,\"start\":41830},{\"end\":41848,\"start\":41844},{\"end\":41867,\"start\":41858},{\"end\":41879,\"start\":41873},{\"end\":41895,\"start\":41888},{\"end\":41909,\"start\":41904},{\"end\":41919,\"start\":41916},{\"end\":41931,\"start\":41924},{\"end\":42256,\"start\":42249},{\"end\":42551,\"start\":42544},{\"end\":42561,\"start\":42557},{\"end\":42576,\"start\":42572},{\"end\":42591,\"start\":42586},{\"end\":42602,\"start\":42598},{\"end\":42618,\"start\":42612},{\"end\":43402,\"start\":43396},{\"end\":43417,\"start\":43410},{\"end\":43429,\"start\":43423},{\"end\":43441,\"start\":43435},{\"end\":43452,\"start\":43447}]", "bib_author_last_name": "[{\"end\":29754,\"start\":29745},{\"end\":29769,\"start\":29765},{\"end\":29787,\"start\":29779},{\"end\":30271,\"start\":30260},{\"end\":30299,\"start\":30281},{\"end\":30323,\"start\":30310},{\"end\":30706,\"start\":30698},{\"end\":30721,\"start\":30715},{\"end\":30738,\"start\":30730},{\"end\":30750,\"start\":30747},{\"end\":30768,\"start\":30758},{\"end\":30784,\"start\":30776},{\"end\":30813,\"start\":30793},{\"end\":30836,\"start\":30829},{\"end\":30850,\"start\":30845},{\"end\":30863,\"start\":30858},{\"end\":31291,\"start\":31285},{\"end\":31510,\"start\":31504},{\"end\":31525,\"start\":31518},{\"end\":31542,\"start\":31534},{\"end\":31563,\"start\":31552},{\"end\":31581,\"start\":31571},{\"end\":31595,\"start\":31589},{\"end\":32234,\"start\":32228},{\"end\":32249,\"start\":32243},{\"end\":32267,\"start\":32259},{\"end\":32287,\"start\":32276},{\"end\":32300,\"start\":32295},{\"end\":32317,\"start\":32307},{\"end\":32324,\"start\":32319},{\"end\":32906,\"start\":32901},{\"end\":32921,\"start\":32917},{\"end\":32933,\"start\":32928},{\"end\":32950,\"start\":32943},{\"end\":32966,\"start\":32960},{\"end\":32985,\"start\":32977},{\"end\":33005,\"start\":32994},{\"end\":33019,\"start\":33014},{\"end\":33034,\"start\":33028},{\"end\":33049,\"start\":33043},{\"end\":33395,\"start\":33386},{\"end\":33410,\"start\":33404},{\"end\":33424,\"start\":33418},{\"end\":33439,\"start\":33434},{\"end\":33454,\"start\":33448},{\"end\":33468,\"start\":33461},{\"end\":33481,\"start\":33475},{\"end\":33492,\"start\":33483},{\"end\":33507,\"start\":33502},{\"end\":33525,\"start\":33519},{\"end\":33535,\"start\":33527},{\"end\":33950,\"start\":33942},{\"end\":33965,\"start\":33959},{\"end\":33980,\"start\":33975},{\"end\":33994,\"start\":33988},{\"end\":34011,\"start\":34001},{\"end\":34704,\"start\":34700},{\"end\":34720,\"start\":34713},{\"end\":35303,\"start\":35295},{\"end\":35519,\"start\":35516},{\"end\":35532,\"start\":35530},{\"end\":35543,\"start\":35536},{\"end\":35560,\"start\":35553},{\"end\":35565,\"start\":35562},{\"end\":35879,\"start\":35876},{\"end\":35891,\"start\":35888},{\"end\":35902,\"start\":35898},{\"end\":36172,\"start\":36169},{\"end\":36186,\"start\":36182},{\"end\":36482,\"start\":36476},{\"end\":36497,\"start\":36490},{\"end\":36508,\"start\":36504},{\"end\":36821,\"start\":36811},{\"end\":36834,\"start\":36830},{\"end\":36849,\"start\":36844},{\"end\":36863,\"start\":36857},{\"end\":37169,\"start\":37159},{\"end\":37182,\"start\":37178},{\"end\":37197,\"start\":37192},{\"end\":37211,\"start\":37205},{\"end\":37528,\"start\":37521},{\"end\":37543,\"start\":37538},{\"end\":37755,\"start\":37748},{\"end\":37770,\"start\":37765},{\"end\":37784,\"start\":37778},{\"end\":37800,\"start\":37792},{\"end\":37815,\"start\":37808},{\"end\":37828,\"start\":37822},{\"end\":37845,\"start\":37835},{\"end\":37860,\"start\":37854},{\"end\":37878,\"start\":37872},{\"end\":38353,\"start\":38346},{\"end\":38372,\"start\":38366},{\"end\":38386,\"start\":38381},{\"end\":38393,\"start\":38388},{\"end\":38705,\"start\":38701},{\"end\":38719,\"start\":38716},{\"end\":39257,\"start\":39249},{\"end\":39268,\"start\":39261},{\"end\":39284,\"start\":39278},{\"end\":39301,\"start\":39293},{\"end\":39308,\"start\":39303},{\"end\":39568,\"start\":39557},{\"end\":39587,\"start\":39578},{\"end\":39603,\"start\":39599},{\"end\":39969,\"start\":39964},{\"end\":39982,\"start\":39977},{\"end\":40001,\"start\":39995},{\"end\":40016,\"start\":40009},{\"end\":40036,\"start\":40027},{\"end\":40049,\"start\":40044},{\"end\":40067,\"start\":40060},{\"end\":40079,\"start\":40074},{\"end\":40092,\"start\":40089},{\"end\":40109,\"start\":40098},{\"end\":40552,\"start\":40533},{\"end\":40568,\"start\":40558},{\"end\":40576,\"start\":40574},{\"end\":40592,\"start\":40586},{\"end\":40610,\"start\":40601},{\"end\":40623,\"start\":40612},{\"end\":41070,\"start\":41062},{\"end\":41092,\"start\":41080},{\"end\":41107,\"start\":41102},{\"end\":41371,\"start\":41363},{\"end\":41382,\"start\":41379},{\"end\":41828,\"start\":41822},{\"end\":41842,\"start\":41835},{\"end\":41856,\"start\":41849},{\"end\":41871,\"start\":41868},{\"end\":41886,\"start\":41880},{\"end\":41902,\"start\":41896},{\"end\":41914,\"start\":41910},{\"end\":41922,\"start\":41920},{\"end\":41935,\"start\":41932},{\"end\":42261,\"start\":42257},{\"end\":42555,\"start\":42552},{\"end\":42570,\"start\":42562},{\"end\":42584,\"start\":42577},{\"end\":42596,\"start\":42592},{\"end\":42610,\"start\":42603},{\"end\":42627,\"start\":42619},{\"end\":43408,\"start\":43403},{\"end\":43421,\"start\":43418},{\"end\":43433,\"start\":43430},{\"end\":43445,\"start\":43442},{\"end\":43458,\"start\":43453}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/3477495.3531737\",\"id\":\"b0\"},\"end\":30216,\"start\":29730},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":17356022},\"end\":30620,\"start\":30218},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":244954723},\"end\":31250,\"start\":30622},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207602540},\"end\":31444,\"start\":31252},{\"attributes\":{\"doi\":\"10.1145/1458082.1458217\",\"id\":\"b4\",\"matched_paper_id\":15428984},\"end\":32165,\"start\":31446},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1533857},\"end\":32856,\"start\":32167},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218971783},\"end\":33374,\"start\":32858},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b7\"},\"end\":33866,\"start\":33376},{\"attributes\":{\"doi\":\"10.1145/3340531.3412779\",\"id\":\"b8\",\"matched_paper_id\":219559030},\"end\":34614,\"start\":33868},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9130375},\"end\":35202,\"start\":34616},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":29997612},\"end\":35471,\"start\":35204},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219559263},\"end\":35775,\"start\":35473},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b12\",\"matched_paper_id\":225626501},\"end\":36158,\"start\":35777},{\"attributes\":{\"doi\":\"10.48550/ARXIV.2004.08476\",\"id\":\"b13\"},\"end\":36465,\"start\":36160},{\"attributes\":{\"id\":\"b14\"},\"end\":36727,\"start\":36467},{\"attributes\":{\"id\":\"b15\"},\"end\":37076,\"start\":36729},{\"attributes\":{\"id\":\"b16\"},\"end\":37423,\"start\":37078},{\"attributes\":{\"id\":\"b17\"},\"end\":37738,\"start\":37425},{\"attributes\":{\"id\":\"b18\"},\"end\":38256,\"start\":37740},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17036753},\"end\":38639,\"start\":38258},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18332366},\"end\":39171,\"start\":38641},{\"attributes\":{\"id\":\"b21\"},\"end\":39495,\"start\":39173},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":205439606},\"end\":39888,\"start\":39497},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":218869575},\"end\":40468,\"start\":39890},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":212908749},\"end\":40998,\"start\":40470},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":196194329},\"end\":41313,\"start\":41000},{\"attributes\":{\"id\":\"b26\"},\"end\":41731,\"start\":41315},{\"attributes\":{\"id\":\"b27\"},\"end\":42215,\"start\":41733},{\"attributes\":{\"id\":\"b28\"},\"end\":42436,\"start\":42217},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":225040574},\"end\":43339,\"start\":42438},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221293184},\"end\":44029,\"start\":43341}]", "bib_title": "[{\"end\":29737,\"start\":29730},{\"end\":30250,\"start\":30218},{\"end\":30686,\"start\":30622},{\"end\":31276,\"start\":31252},{\"end\":31493,\"start\":31446},{\"end\":32224,\"start\":32167},{\"end\":32895,\"start\":32858},{\"end\":33935,\"start\":33868},{\"end\":34689,\"start\":34616},{\"end\":35283,\"start\":35204},{\"end\":35505,\"start\":35473},{\"end\":35867,\"start\":35777},{\"end\":38342,\"start\":38258},{\"end\":38693,\"start\":38641},{\"end\":39550,\"start\":39497},{\"end\":39954,\"start\":39890},{\"end\":40523,\"start\":40470},{\"end\":41055,\"start\":41000},{\"end\":42542,\"start\":42438},{\"end\":43394,\"start\":43341}]", "bib_author": "[{\"end\":29756,\"start\":29739},{\"end\":29771,\"start\":29756},{\"end\":29789,\"start\":29771},{\"end\":30273,\"start\":30252},{\"end\":30301,\"start\":30273},{\"end\":30325,\"start\":30301},{\"end\":30708,\"start\":30688},{\"end\":30723,\"start\":30708},{\"end\":30740,\"start\":30723},{\"end\":30752,\"start\":30740},{\"end\":30770,\"start\":30752},{\"end\":30786,\"start\":30770},{\"end\":30815,\"start\":30786},{\"end\":30838,\"start\":30815},{\"end\":30852,\"start\":30838},{\"end\":30865,\"start\":30852},{\"end\":31293,\"start\":31278},{\"end\":31512,\"start\":31495},{\"end\":31527,\"start\":31512},{\"end\":31544,\"start\":31527},{\"end\":31565,\"start\":31544},{\"end\":31583,\"start\":31565},{\"end\":31597,\"start\":31583},{\"end\":32236,\"start\":32226},{\"end\":32251,\"start\":32236},{\"end\":32269,\"start\":32251},{\"end\":32289,\"start\":32269},{\"end\":32302,\"start\":32289},{\"end\":32319,\"start\":32302},{\"end\":32326,\"start\":32319},{\"end\":32908,\"start\":32897},{\"end\":32923,\"start\":32908},{\"end\":32935,\"start\":32923},{\"end\":32952,\"start\":32935},{\"end\":32968,\"start\":32952},{\"end\":32987,\"start\":32968},{\"end\":33007,\"start\":32987},{\"end\":33021,\"start\":33007},{\"end\":33036,\"start\":33021},{\"end\":33051,\"start\":33036},{\"end\":33397,\"start\":33376},{\"end\":33412,\"start\":33397},{\"end\":33426,\"start\":33412},{\"end\":33441,\"start\":33426},{\"end\":33456,\"start\":33441},{\"end\":33470,\"start\":33456},{\"end\":33483,\"start\":33470},{\"end\":33494,\"start\":33483},{\"end\":33509,\"start\":33494},{\"end\":33527,\"start\":33509},{\"end\":33537,\"start\":33527},{\"end\":33952,\"start\":33937},{\"end\":33967,\"start\":33952},{\"end\":33982,\"start\":33967},{\"end\":33996,\"start\":33982},{\"end\":34013,\"start\":33996},{\"end\":34706,\"start\":34691},{\"end\":34722,\"start\":34706},{\"end\":35305,\"start\":35285},{\"end\":35521,\"start\":35507},{\"end\":35534,\"start\":35521},{\"end\":35545,\"start\":35534},{\"end\":35562,\"start\":35545},{\"end\":35567,\"start\":35562},{\"end\":35881,\"start\":35869},{\"end\":35893,\"start\":35881},{\"end\":35904,\"start\":35893},{\"end\":36174,\"start\":36160},{\"end\":36188,\"start\":36174},{\"end\":36484,\"start\":36467},{\"end\":36499,\"start\":36484},{\"end\":36510,\"start\":36499},{\"end\":36823,\"start\":36801},{\"end\":36836,\"start\":36823},{\"end\":36851,\"start\":36836},{\"end\":36865,\"start\":36851},{\"end\":37171,\"start\":37149},{\"end\":37184,\"start\":37171},{\"end\":37199,\"start\":37184},{\"end\":37213,\"start\":37199},{\"end\":37530,\"start\":37513},{\"end\":37545,\"start\":37530},{\"end\":37757,\"start\":37740},{\"end\":37772,\"start\":37757},{\"end\":37786,\"start\":37772},{\"end\":37802,\"start\":37786},{\"end\":37817,\"start\":37802},{\"end\":37830,\"start\":37817},{\"end\":37847,\"start\":37830},{\"end\":37862,\"start\":37847},{\"end\":37880,\"start\":37862},{\"end\":38355,\"start\":38344},{\"end\":38374,\"start\":38355},{\"end\":38388,\"start\":38374},{\"end\":38395,\"start\":38388},{\"end\":38707,\"start\":38695},{\"end\":38721,\"start\":38707},{\"end\":39259,\"start\":39242},{\"end\":39270,\"start\":39259},{\"end\":39286,\"start\":39270},{\"end\":39303,\"start\":39286},{\"end\":39310,\"start\":39303},{\"end\":39570,\"start\":39552},{\"end\":39589,\"start\":39570},{\"end\":39605,\"start\":39589},{\"end\":39971,\"start\":39956},{\"end\":39984,\"start\":39971},{\"end\":40003,\"start\":39984},{\"end\":40018,\"start\":40003},{\"end\":40038,\"start\":40018},{\"end\":40051,\"start\":40038},{\"end\":40069,\"start\":40051},{\"end\":40081,\"start\":40069},{\"end\":40094,\"start\":40081},{\"end\":40111,\"start\":40094},{\"end\":40554,\"start\":40525},{\"end\":40570,\"start\":40554},{\"end\":40578,\"start\":40570},{\"end\":40594,\"start\":40578},{\"end\":40612,\"start\":40594},{\"end\":40625,\"start\":40612},{\"end\":41072,\"start\":41057},{\"end\":41094,\"start\":41072},{\"end\":41109,\"start\":41094},{\"end\":41373,\"start\":41355},{\"end\":41384,\"start\":41373},{\"end\":41830,\"start\":41816},{\"end\":41844,\"start\":41830},{\"end\":41858,\"start\":41844},{\"end\":41873,\"start\":41858},{\"end\":41888,\"start\":41873},{\"end\":41904,\"start\":41888},{\"end\":41916,\"start\":41904},{\"end\":41924,\"start\":41916},{\"end\":41937,\"start\":41924},{\"end\":42263,\"start\":42249},{\"end\":42557,\"start\":42544},{\"end\":42572,\"start\":42557},{\"end\":42586,\"start\":42572},{\"end\":42598,\"start\":42586},{\"end\":42612,\"start\":42598},{\"end\":42629,\"start\":42612},{\"end\":43410,\"start\":43396},{\"end\":43423,\"start\":43410},{\"end\":43435,\"start\":43423},{\"end\":43447,\"start\":43435},{\"end\":43460,\"start\":43447}]", "bib_venue": "[{\"end\":29923,\"start\":29812},{\"end\":30395,\"start\":30325},{\"end\":30913,\"start\":30869},{\"end\":31308,\"start\":31293},{\"end\":31708,\"start\":31620},{\"end\":32444,\"start\":32326},{\"end\":33100,\"start\":33051},{\"end\":33598,\"start\":33553},{\"end\":34136,\"start\":34036},{\"end\":34840,\"start\":34722},{\"end\":35326,\"start\":35305},{\"end\":35607,\"start\":35567},{\"end\":35952,\"start\":35908},{\"end\":36308,\"start\":36229},{\"end\":36570,\"start\":36526},{\"end\":36799,\"start\":36729},{\"end\":37147,\"start\":37078},{\"end\":37511,\"start\":37425},{\"end\":37979,\"start\":37896},{\"end\":38430,\"start\":38395},{\"end\":38838,\"start\":38721},{\"end\":39240,\"start\":39173},{\"end\":39675,\"start\":39605},{\"end\":40160,\"start\":40111},{\"end\":40686,\"start\":40625},{\"end\":41141,\"start\":41109},{\"end\":41353,\"start\":41315},{\"end\":41814,\"start\":41733},{\"end\":42247,\"start\":42217},{\"end\":42801,\"start\":42659},{\"end\":43583,\"start\":43483},{\"end\":30021,\"start\":29925},{\"end\":31327,\"start\":31310},{\"end\":31800,\"start\":31710},{\"end\":32549,\"start\":32446},{\"end\":34240,\"start\":34138},{\"end\":34945,\"start\":34842},{\"end\":38942,\"start\":38840},{\"end\":40734,\"start\":40688},{\"end\":42930,\"start\":42803},{\"end\":43687,\"start\":43585}]"}}}, "year": 2023, "month": 12, "day": 17}