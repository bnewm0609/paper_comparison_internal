{"id": 4619542, "updated": "2023-09-28 20:29:43.994", "metadata": {"title": "Maximum Classifier Discrepancy for Unsupervised Domain Adaptation", "authors": "[{\"first\":\"Kuniaki\",\"last\":\"Saito\",\"middle\":[]},{\"first\":\"Kohei\",\"last\":\"Watanabe\",\"middle\":[]},{\"first\":\"Yoshitaka\",\"last\":\"Ushiku\",\"middle\":[]},{\"first\":\"Tatsuya\",\"last\":\"Harada\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 12, "day": 7}, "abstract": "In this work, we present a method for unsupervised domain adaptation (UDA), where we aim to transfer knowledge from a label-rich domain (i.e., a source domain) to an unlabeled domain (i.e., a target domain). Many adversarial learning methods have been proposed for this task. These methods train domain classifier networks (i.e., a discriminator) to discriminate distinguish the features as either a source or target and train a feature generator network to mimic the discriminator.However, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. To solve the problem, we propose a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to utilize task-specific classifiers as discriminators that try to detect target samples that are far from the support of the source. A feature generator learns to generate target features inside the support to fool the classifiers. Since the generator uses feedback from task-specific classifiers, it avoids generating target features near class boundaries. Our method outperforms other methods on several datasets of image classification and semantic segmentation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1712.02560", "mag": "2962687275", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/SaitoWUH18", "doi": "10.1109/cvpr.2018.00392"}}, "content": {"source": {"pdf_hash": "bf42638b4e61d22e02d101b72b11a0989bb24554", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1712.02560v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1712.02560", "status": "GREEN"}}, "grobid": {"id": "2512fe43f55ce7d03dc73a22de0266126725b1eb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bf42638b4e61d22e02d101b72b11a0989bb24554.txt", "contents": "\nMaximum Classifier Discrepancy for Unsupervised Domain Adaptation\n\n\nKuniaki Saito k-saito@mi.t.u-tokyo.ac.jp \nThe University of Tokyo\n2 RIKEN\n\nKohei Watanabe watanabe@mi.t.u-tokyo.ac.jp \nThe University of Tokyo\n2 RIKEN\n\nYoshitaka Ushiku ushiku@mi.t.u-tokyo.ac.jp \nThe University of Tokyo\n2 RIKEN\n\nTatsuya Harada harada@mi.t.u-tokyo.ac.jp \nThe University of Tokyo\n2 RIKEN\n\nMaximum Classifier Discrepancy for Unsupervised Domain Adaptation\n\nIn this work, we present a method for unsupervised domain adaptation (UDA), where we aim to transfer knowledge from a label-rich domain (i.e., a source domain) to an unlabeled domain (i.e., a target domain). Many adversarial learning methods have been proposed for this task. These methods train domain classifier networks (i.e., a discriminator) to discriminate distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. However, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries.To solve the problem, we propose a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to utilize task-specific classifiers as discriminators that try to detect target samples that are far from the support of the source. A feature generator learns to generate target features inside the support to fool the classifiers. Since the generator uses feedback from task-specific classifiers, it avoids generating target features near class boundaries. Our method outperforms other methods on several datasets of image classification and semantic segmentation.\n\nIntroduction\n\nThe classification accuracy of images has improved substantially with the advent of deep convolutional neural networks (CNNs) which utilize numerous labeled samples [14]. However, collecting numerous labeled samples in various domains is expensive and time-consuming.\n\nDomain adaptation (DA) tackles this problem by transferring knowledge from a label-rich domain (i.e., source domain) to a label-scarce domain (i.e., target domain). DA aims to train a classifier using source samples that general- ize well to the target domain. However, each domain's samples have different characteristics, which makes the problem difficult to solve. Consider neural networks trained on labeled source images collected from the Web. Although such neural networks perform well on the source images, correctly recognizing target images collected from a real camera is difficult for them. This is because the target images can have different characteristics from the source images, such as change of light, noise, and angle in which the image is captured. Furthermore, regarding unsupervised DA (UDA), we have access to labeled source samples and only unlabeled target samples. We must construct a model that works well on target samples despite the absence of their labels during training. UDA is the most challenging situation and we propose a method for UDA in this study.\n\nMany UDA algorithms, particularly those for training neural networks, attempt to match the distribution of the source features with that of the target without considering the category of the samples [7,34,3,37]. In particular, domain classifier-based adaptation algorithms have been applied to many tasks [7,3]. The methods utilize two players to align distributions in an adversarial manner: domain clas-sifier (i.e., a discriminator) and feature generator. Source and target samples are input to the same feature generator. Features from the feature generator are shared by the discriminator and a task-specific classifier. The discriminator is trained to discriminate the domain labels of the features generated by the generator whereas the generator is trained to fool it. The generator aims to match distributions between the source and target because such distributions will mimic the discriminator. They assume that such target features are classified correctly by the task-specific classifier because they are aligned with the source samples.\n\nHowever, this method should fail to extract discriminative features because it does not consider the relationship between target samples and the task-specific decision boundary when aligning distributions. As shown in the left side of Fig. 1, the generator can generate ambiguous features near the boundary because it simply tries to make the two distributions similar.\n\nTo overcome both problems, we propose a method that learns to align distributions of features from source and target domain by using the classifier's output for the target samples.\n\nWe propose a new adversarial learning method that utilizes two types of players: task-specific classifiers and a feature generator. task-specific classifiers denotes the classifiers trained for each task such as object classification or semantic segmentation. Two classifiers take features from the generator. Two classifiers try to classify source samples correctly and, simultaneously, are trained to detect the target samples that are not included in support of the source. The samples existing outside the support do not have discriminative features because they are not clearly categorized into some classes. Thus, our method utilizes the taskspecific classifiers as a discriminator. Generator tries to fool the classifiers. In other words, it is trained to generate target features inside the support while considering classifiers' output for target samples. Thus, our method allows the generator to generate discriminative features for target samples because it considers the relationship between the decision boundary and target samples. This training is achieved in an adversarial manner. In addition, please note that we do not use domain labels in our method.\n\nOur proposed method considers decision boundary when aligning distributions. Thus, the generator can extract discriminative features for target samples. We evaluate our method on image recognition and semantic segmentation. In many settings, our method outperforms other methods by a large margin. The contributions of our paper are summarized as follows:\n\n\u2022 We propose a novel adversarial training method for domain adaptation that tries to align the distribution of a target domain by considering task-specific decision boundaries. We do not use a domain-discriminator that distinguishes the features as either source or target.\n\n\u2022 We confirm the behavior of our method through a toy problem.\n\n\u2022 We extensively evaluate our method on various tasks: digit classification, object classification, and semantic segmentation.\n\n\nRelated Work\n\nTraining CNNs for DA can be realized through various strategies. Ghifary et al. proposed using an autoencoder for the target domain to obtain domain-invariant features [8]. Sener et al. proposed using clustering techniques and pseudo-labels to obtain discriminative features [30]. Taigman et al. proposed cross-domain image translation methods [35]. Matching distributions of the middle features in CNNs is considered to be effective in realizing an accurate adaptation. To this end, numerous methods have been proposed [7,34,3,27,37,33].\n\nThe representative method of distribution matching involves training a domain classifier using the middle features and generating the features that deceive the domain classifier [7]. This method utilizes the techniques used in generative adversarial networks [9]. The domain classifier is trained to predict the domain of each input, and the category classifier is trained to predict the task-specific category labels. Feature extraction layers are shared by the two classifiers. The layers are trained to correctly predict the label of source samples as well as to deceive the domain classifier. Thus, the distributions of the middle features of the target and source samples are made similar. Some methods utilize maximum mean discrepancy (MMD) [19,18], which can be applied to measure the divergence in high-dimensional space between different domains. This approach can train the CNNs to simultaneously minimize both the divergence and category loss for the source domain. These methods are based on the theory proposed by [2], which states that the error on the target domain is bounded by the divergence of the distributions. To our understanding, these distribution aligning methods using GAN or MMD do not consider the relationship between target samples and decision boundaries. In order to tackle these problems, we propose a novel approach using task-specific classifiers as a discriminator.\n\nConsensus regularization is a technique used in multisource domain adaptation and multi-view learning, in which multiple classifiers are trained to maximize the consensus of their outputs [20]. In our method, we address a training step that minimizes the consensus of two classifiers, which is totally different from consensus regularization. Consensus regularization utilizes samples of multi-source domains to construct different classifiers as in [20]. In order to construct different classifiers, it relies on the different character-  Discrepancy refers to the disagreement between the predictions of two classifiers. First, we can see that the target samples outside the support of the source can be measured by two different classifiers (Leftmost, Two different classifiers). Second, regarding the training procedure, we solve a minimax problem in which we find two classifiers that maximize the discrepancy on the target sample, and then generate features that minimize this discrepancy.\n\nistics of samples in different source domains. By contrast, our method can construct different classifiers from only one source domain.\n\n\nMethod\n\nIn this section, we present the detail of our proposed method. First, we give the overall idea of our method in Section 3.1. Second, we explain about the loss function we used in experiments in Section 3.2. Finally, we provide an explanation of the entire training procedure of our method in Section 3.3.\n\n\nOverall Idea\n\nWe have access to a labeled source image x s and a corresponding label y s drawn from a set of labeled source images {X s , Y s }, as well as an unlabeled target image x t drawn from unlabeled target images X t . We train a feature generator network G, which takes inputs x s or x t , and classifier networks F 1 and F 2 , which take features from G. F 1 and F 2 classify them into K classes, that is, they output a Kdimensional vector of logits. We obtain class probabilities by applying the softmax function for the vector. We use the notation p 1 (y|x), p 2 (y|x) to denote the K-dimensional probabilistic outputs for input x obtained by F 1 and F 2 respectively.\n\nThe goal of our method is to align source and target features by utilizing the task-specific classifiers as a discriminator in order to consider the relationship between class boundaries and target samples. For this objective, we have to detect target samples far from the support of the source. The question is how to detect target samples far from the support. These target samples are likely to be misclassified by the classifier learned from source samples because they are near the class boundaries. Then, in order to detect these target samples, we propose to utilize the disagreement of the two classifiers on the prediction for target samples. Consider two classifiers (F 1 and F 2 ) that have different characteristics as shown in the leftmost side of Fig.  2. We assume that the two classifiers can classify source samples correctly. This assumption is realistic because we have access to source samples in the setting of UDA. Here, we have the key intuition that target samples outside the support of the source are likely to be classified differently by the two distinct classifiers. This region is denoted by black lines in the leftmost side of Fig. 2 (Discrepancy Region). Conversely, if we can measure the disagreement between the two classifiers and train the generator to minimize the disagreement, the generator will avoid generating target features outside the support of the source. Here, we consider measuring the difference for a target sample using the following equation, d(p 1 (y|x t ), p 2 (y|x t )) where d denotes the function measuring divergence between two probabilistic outputs. This term indicates how the two classifiers disagree on their predictions and, hereafter, we call the term as discrepancy. Our goal is to obtain a feature generator that can minimize the discrepancy on target samples.\n\nIn order to effectively detect target samples outside the support of the source, we propose to train discriminators (F 1 and F 2 ) to maximize the discrepancy given target features (Maximize Discrepancy in Fig. 2). Without this operation, the two classifiers can be very similar ones and cannot detect target samples outside the support of the source. We then train the generator to fool the discriminator, that is, by minimizing the discrepancy (Minimize Discrepancy in Fig. 2). This operation encourages the target samples to be generated inside the support of the source. This adversarial\n\nStep C : Minimize discrepancy on target (Fix F 1 , F 2 )\n\nStep B : Maximize discrepancy on target (Fix G)\nd(p1(y|xt), p2(y|xt)) G x t d(p1(y|xt), p2(y|xt)) F 1 F 2 Discrepancy Loss Discrepancy Loss Update ! Fix Class Predictions p1(y|xt) p2(y|xt) G x t F 1 F 2 Update ! Fix p1(y|xt) p2(y|xt)\nTarget sample\n\n\nClass Predictions\n\nTarget sample Figure 3. Adversarial training steps of our method. We separate the network into two modules: generator (G) and classifiers (F1 , F2 ).\n\nThe classifiers learn to maximize the discrepancy Step B on the target samples, and the generator learns to minimize the discrepancy\n\nStep C. Please note that we employ a training Step A to ensure the discriminative features for source samples.\n\nlearning steps are repeated in our method. Our goal is to obtain the features, in which the support of the target is included by that of the source (Obtained Distributions in Fig.  2). We show the loss function used for discrepancy loss in the next section. Then, we detail the training procedure.\n\n\nDiscrepancy Loss\n\nIn this study, we utilize the absolute values of the difference between the two classifiers' softmax output as discrepancy loss:\nd(p 1 , p 2 ) = 1 K K k=1 |p 1k \u2212 p 2k |,(1)\nwhere the p 1k and p 2k denote probability output of p 1 and p 2 for class k.\n\n\nTraining Steps\n\nTo sum up the previous discussion in Section 3.1, we need to train two classifiers, which take inputs from the generator and maximize d(p 1 (y|x t ), p 2 (y|x t )), and the generator which tries to mimic the classifiers. Both the classifiers and generator must classify source samples correctly. We will show the manner in which to achieve this. We solve this problem in three steps.\n\nStep A First, we train both classifiers and generator to classify the source samples correctly. In order to make classifiers and generator obtain task-specific discriminative features, this step is crucial. We train the networks to minimize softmax cross entropy. The objective is as follows:\nmin G,F1,F2 L(X s , Y s ).\n(2)\nL(X s , Y s ) = \u2212E (xs,ys)\u223c(Xs,Ys) K k=1 1l [k=ys] log p(y|x s )(3)\nStep B In this step, we train the classifiers (F 1 , F 2 ) as a discriminator for a fixed generator (G). By training the classifiers to increase the discrepancy, they can detect the target samples excluded by the support of the source. This step corresponds to Step B in Fig. 3. We add a classification loss on the source samples. Without this loss, we experimentally found that our algorithm's performance drops significantly. We use the same number of source and target samples to update the model. The objective is as follows:\nmin F1,F2 L(X s , Y s ) \u2212 L adv (X t ). (4) L adv (X t ) = E xt\u223cXt [d(p 1 (y|x t ), p 2 (y|x t ))](5)\nStep C We train the generator to minimize the discrepancy for fixed classifiers. This step corresponds to\n\nStep C in Fig. 3. The number n indicates the number of times we repeat this for the same mini-batch. This number is a hyperparameter of our method. This term denotes the trade-off between the generator and the classifiers. The objective is as follows: min\nG L adv (X t ).(6)\nThese three steps are repeated in our method. To our understanding, the order of the three steps is not important. Instead, our major concern is to train the classifiers and generator in an adversarial manner under the condition that they can classify source samples correctly.\n\n\nTheoretical Insight\n\nSince our method is motivated by the theory proposed by Ben-David et al. [1], we want to show the relationship between our method and the theory in this section.\n\nBen-David et al. [1] proposed the theory that bounds the expected error on the target samples, R T (h), by using three terms: (i) expected error on the source domain, R S (h); (ii) H\u2206H-distance (d H\u2206H (S, T )), which is measured as the discrepancy between two classifiers; and (iii) the shared error of the ideal joint hypothesis, \u03bb. S and T denote source and target domain respectively. Another theory [2] bounds the error on the target domain, which introduced H-distance (d H (S, T )) for domain divergence. The two theories and their relationships can be explained as follows.\n\nTheorem 1 Let H be the hypothesis class. Given two domains S and T , we have\n\u2200h \u2208 H, R T (h) \u2264 R S (h) + 1 2 d H\u2206H (S, T ) + \u03bb \u2264 R S (h) + 1 2 d H (S, T ) + \u03bb (7) where d H\u2206H (S, T ) = 2 sup (h,h )\u2208H 2 E x\u223cS I h(x) = h (x) \u2212 E x\u223cT I h(x) = h (x) d H (S, T ) = 2 sup h\u2208H E x\u223cS I h(x) = 1 \u2212 E x\u223cT I h(x) = 1 , \u03bb = min [R S (h) + R T (h)]\nHere, R T (h) is the error of hypothesis h on the target domain, and R S (h) is the corresponding error on the source domain. I[a] is the indicator function, which is 1 if predicate a is true and 0 otherwise.\n\nH-distance is shown to be empirically measured by the error of the domain classifier, which is trained to discriminate the domain of features. \u03bb is a constant-the shared error of the ideal joint hypothesis-which is considered sufficiently low to achieve an accurate adaptation. Earlier studies [7,34,3,27,37] attempted to measure and minimize Hdistance in order to realize the adaptation. As this inequality suggests, H-distance upper-bounds the H\u2206H-distance. We will show the relationship between our method and H\u2206Hdistance.\n\nRegarding d H\u2206H (S, T ), if we consider that h and h can classify source samples correctly, the term We assume that h and h share the feature extraction part. Then, we decompose the hypothesis h into G and F 1 , and h into G and F 2 . G, F 1 and F 2 correspond to the network in our method. If we substitute these notations into the sup\n(h,h )\u2208H 2 E x\u223cT I h(x) = h (x) and for fixed G, the term will become sup F1,F2 E x\u223cT I [F 1 \u2022 G(x) = F 2 \u2022 G(x)].(8)\nFurthermore, if we replace sup with max and minimize the term with respect to G, we obtain\nmin G max F1,F2 E x\u223cT I [F 1 \u2022 G(x) = F 2 \u2022 G(x)].(9)\nThis equation is very similar to the mini-max problem we solve in our method, in which classifiers are trained to maximize their discrepancy on target samples and generator tries  Fig. 4(a) is the model trained only on source samples. Fig. 4(b) is the model trained to increase discrepancy of the two classifiers on target samples without using\n\nStep C. Fig. 4(c) shows our proposed method.\n\nto minimize it. Although we must train all networks to minimize the classification loss on source samples, we can see the connection to the theory proposed by [1].\n\n\nExperiments on Classification\n\nFirst, we observed the behavior of our model on toy problem. Then, we performed an extensive evaluation of the proposed methods on the following datasets: digits, traffic signs, and object classification.\n\n\nExperiments on Toy Datasets\n\nIn the first experiment, we observed the behavior of the proposed method on inter twinning moons 2D problems, in which we used scikit-learn [25] to generate the target samples by rotating the source samples. The goal of the experiment was to observe the learned classifiers' boundary. For the source samples, we generated a lower moon and an upper moon, labeled 0 and 1, respectively. Target samples were generated by rotating the angle of the distribution of the source samples. We generated 300 source and target samples per class as the training samples. In this experiment, we compared the decision boundary obtained from our method with that obtained from both the model trained only on source samples and from that trained only to increase the discrepancy. In order to train the second comparable model, we simply skipped Step C in Section 3.3 during training. We tested the method on 1000 target samples and visualized the learned decision boundary with source and target samples. Other details including the network architecture used in this experiment are provided in our supplementary material.\n\nAs we expected, when we trained the two classifiers to increase the discrepancy on the target samples, two classifiers largely disagreed on their predictions on target samples ( Fig. 4(b)). This is clear when compared to the source- Relationship between discrepancy loss (blue line) and accuracy (red and green lines) during training. As discrepancy loss decreased, accuracy improved. Right: Visualization of features obtained from last pooling layer of the generator in adaptation from SYN SIGNS to GTSRB using t-SNE [21]. Red and blue points indicate the target and source samples, respectively. All samples are testing samples. We can see that applying our method makes the target samples discriminative.  [3]. Please note that \u2020 means that the method used a few labeled target samples as validation, which is different from our setting. We repeated each experiment 5 times and report the average and the standard deviation of the accuracy. The accuracy was obtained from classifier F1. Including the methods that used the labeled target samples for validation, our method achieved good performance.\n\n\nSVHN\n\nonly model (Fig. 4(a)). Two classifiers were trained on the source samples without adaptation, and the boundaries seemed to be nearly the same. Then, our proposed method attempted to generate target samples that reduce the discrepancy. Therefore, we could expect that the two classifiers will be similar. Fig. 4(c) demonstrates the assumption. The decision boundaries are drawn considering the target samples. The two classifiers output nearly the same prediction for target samples, and they classified most target samples correctly.\n\n\nExperiments on image datasets\n\nIn this experiment, we evaluate the adaptation of the model on three scenarios. The example datasets are presented in the supplementary material.\n\nWe assessed three types of adaptation scenarios by using the digits and traffic datasets, namely MNIST [15], Street View House Numbers (SVHN) [24], and Synthetic Digits (SYN DIGITS) [6]. We further evaluated our method on the traffic sign datasets, Synthetic Traffic Signs (SYN SIGNS) [22], and the German Traffic Signs Recognition Benchmark [32] (GTSRB). In this experiment, we employed the CNNs architecture used in [6]. We added batch normalization to each layer in these models. We used Adam [13] to optimize our model and set the learning rate as 2.0 \u00d7 10 \u22124 in all experiments. We set the batch size to 128 and report the accuracy after 50,000 iterations in all experiments. The hyper-parameter peculiar to our method was n, which denotes the number of times we update the feature generator to mimic classifiers. We varied the value of n from 2 to 4 in our experiment and observed the sensitivity to the hyper-parameter. Please note that we did not use validation samples as done in earlier studies [6,3,29]. Tuning possible hyper-parameters, such as the network architecture, batch size, or learning rate for each dataset should increase the accuracy. However, we fixed such parameters to follow the protocol of unsupervised domain adaptation.\n\nSVHN\u2192MNIST We examined the adaptation from Street View House Numbers (SVHN) [24] to MNIST [15] datasets. SVHN and MNIST have distinct properties because SVHN datasets contain images with colored background, multiple digits, and extremely blurred digits, meaning that the domain divergence is very large between these datasets.\n\nSYN DIGITS\u2192SVHN In this scenario, we evaluated the adaptation between synthetic images and real-world images. The datasets of synthetic numbers [6] consist of 500,000 images generated from Windows fonts by varying the text, positioning, orientation, background, stroke colors, and blur. SYN SIGNS\u2192GTSRB In this experiment, we evaluated the adaptation from synthesized traffic signs datasets to realworld signs datasets. We used the SYN SIGNS dataset [6] as the source dataset and the GTSRB dataset [32] as the target dataset, both of which consist of real images of traffic signs. These datasets contain 43 types of classes.\n\nResults Table 1 lists the accuracies for the target samples, and Fig. 5(a)5(b) shows the relationship between the discrepancy loss and accuracy during training. For the source only model, we used the same network architecture as used in our method. Details are provided in the supplementary material. We extensively compared our methods with distribution matching-based methods as shown in Table 1. The proposed method outperformed these methods in all settings. The performance slightly changed with the value of n. However, the change was not so obvious. Although other methods such as ATDA [29] performed better than our method in some situations, the method utilized a few labeled target samples to decide hyper-parameters for each dataset. The performance of our method will improve too if we can choose the best hyper-parameters for each dataset. As Fig. 5(a)5(b) shows, as the discrepancy loss diminishes, the accuracy improves, confirming that minimizing the discrepancy for target samples can result in accurate adaptation. In particular, the proposed method achieved a sensible increase in performance, thus demonstrating its suitability for adapting synthetic to real images. We visualized learned features as shown in Fig. 5(c)5(d). Our method did not match the distributions of source and target completely as shown in Fig. 5(d). However, the target samples seemed to be aligned with each class of source samples. Although the target samples did not separate well in the non-adapted situation, they did separate clearly as do source samples in the adapted situation.\n\n\nExperiments on VisDA Classification Dataset\n\nWe further evaluated our method on an object classification setting. The VisDA dataset [26] was used in this experiment, which evaluated adaptation from synthetic-object to real-object images. To date, this dataset represents the largest for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation, and testing domains. The source images were generated by rendering 3D models of the same object categories as in the real data from different angles and under different lighting conditions. It contains 152,397 synthetic images. The validation images were collected from MSCOCO [16] and they amount to 55,388 in total. In our experiment, we considered the validation images as the target domain and trained models in unsupervised domain adaptation settings. We evaluate the performance of ResNet101 [11] model pre-trained on Imagenet [5]. The final fully-connected layer was removed and all layers were updated with the same learning rate because this dataset has abundant source and target samples. We regarded the pre-trained model as a generator network and we used three-layered fully-connected networks for classification networks. The batch size was set to 32 and we used SGD with learning rate 1.0 \u00d7 10 \u22123 to optimize the model. We report the accuracy after 10 epochs. The training details for baseline methods are written in our supplementary material due to the limit of space.\n\nResults Our method achieved an accuracy accuracy much better than other distribution matching based methods ( Table 2). In addition, our method performed better than the source-only model in all classes, whereas MMD and DANN perform worse than the source only model in some classes such as car and plant. We can clearly see the clear effectiveness of our method in this regard. In this experiment, as the value of n increase, the performance improved. We think that it was because of the large domain difference between synthetic objects and real images. The generator had to be updated many times to align such distributions.\n\n\nExperiments on Semantic Segmentation\n\nWe further applied our method to semantic segmentation. Considering a huge annotation cost for semantic segmentation datasets, adaptation between different domains is an important problem in semantic segmentation.\n\nImplementation Detail We applied our method to VGG-16 [31] based FCN-8s [17] and DRN-D-105 [39] to evaluate our method. The details of models, including their   Table 3. Adaptation results on the semantic segmentation. We evaluate adaptation from GTA5 to Cityscapes dataset.\n\narchitecture and other hyper-parameters, are described in the supplementary material. We used Momentum SGD to optimize our model and set the momentum rate to 0.9 and the learning rate to 1.0 \u00d7 10 \u22123 in all experiments. The image size was resized to 1024 \u00d7 512. Here, we report the output of F 1 after 50,000 iterations.\n\nSetting We used the publicly available synthetic dataset GTA5 [28] as the source domain dataset and real dataset Cityscapes [4] as the target domain dataset. Following the work [12,40], the Cityscapes validation set was used as our test set. As our training set, the Cityscapes train set was used. During training, we randomly sampled just a single sample (setting the batch size to 1 because of the GPU memory limit) from both the images (and their labels) of the source dataset and the remaining images of the target dataset but with no labels. 19 common classes were considered. We used intersection-over-union (IoU) as an evaluation metric. (Details are provided in the supplementary material.) We compared our approach to two competing methods, namely, \"FCN Wld\" [12] and \"Crrclm DA\" [40]. \"FCN Wld\" introduced a pixel-level adversarial loss and \"Crrclm DA (I+SP)\" introduced curriculum learning approach.Zhang et al. [40] represented a superpixel using FCN-8s [17] pretrained on the PASCAL CONTEXT [23] dataset and we also considered their results without using the external dataset (\"Crrclm DA (I)\").\n\nResults Table 3 and Fig. 6 show quantitative and qualitative results, respectively. These results illustrate that even with a large domain difference between synthetic to real images, our method is capable of improving the performance. With regard to adaptation for VGG-16 based network, our method shows comparable or better performance. In addition, our method shows significant performance, 39.7%, in adaptation for DRN-D-105. Considering the fact that the mIoU of the model trained only on source samples is 22.2%, we can see clear effectiveness of our adaptation method. If we consider the case in which external data cannot be used, our method can improve the mean IoU by 12.6% compared with past work.\n\n\nConclusion\n\nIn this paper, we proposed a new approach for UDA, which utilizes task-specific classifiers to align distributions. We propose to utilize task-specific classifiers as discriminators that try to detect target samples that are far from the support of the source. A feature generator learns to generate target features inside the support to fool the classifiers. Since the generator uses feedback from task-specific classifiers, it will avoid generating target features near class boundaries. We extensively evaluated our method on image classification and semantic segmentation datasets. In almost all experiments, our method outperformed state-ofthe-art methods.\n\n\nAcknowledgements\n\nThe work was partially funded by the ImPACT Program of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan), and was partially supported by CREST, JST.\n\nWe would like to show supplementary information for our main paper.\n\n\nA. Toy Dataset Experiment\n\nWe show the detail of experiments on toy dataset in main paper.\n\n\nA.1. Detail on experimental setting\n\nThe detail of experiment on toy dataset is shown in this section. When generating target samples, we set the rotation angle 30 in experiments of our main paper. We used Adam with learning rate 2.0 \u00d7 10 \u22124 to optimizer the model. The batch size was set to 200. For a feature generator, we used 3-layered fully-connected networks with 15 neurons in hidden layer, in which ReLU is used as the activation function. For classifiers, we used three-layed fully-connected networks with 15 neurons in hidden layer and 2 neurons in output layer. The decision boundary shown in the main paper is obtained when we rotate the source samples 30 degrees to generate target samples. We set n to 3 in this experiment.\n\n\nB. Experiment on Digit Dataset\n\nThe detail of samples used for training and testing is shown in this section. We followed the protocol provided by [6].\n\nSVHN\u2192MNIST In this adaptation scenario, we used the standard training set as training samples, and testing set as testing samples both for source and target samples.\n\nSYN DIGITS\u2192SVHN We used 479400 source samples and 73257 target samples for training, 26032 samples for testing.\n\nSYN SIGNS\u2192GTSRB We randomly selected 31367 samples for target training and evaluated the accuracy on the rest.\n\nThe network architectures used in our experiment are shown in Fig. 8 and Fig. 9. Fig. 7 show supplemental results that we cannot show in our main paper due to a limit of space.\n\n\nC. Experiment on VisDA Classification Dataset\n\nThe detail of architecture we used and the detail of other methods are shown in this section.\n\nConditional Entropy Loss In addition to feature alignment loss, we used a conditional entropy loss to improve the accuracy in this experiment. Please note that we incorporated this loss in comparable methods too. We aimed to assign the target samples to each classes equally. Without this loss, the target samples can be aligned in an unbalanced way. The loss is calculated as follows:\nE xt\u223cXt K k=1 p(y = k|x t ) log p(y|x t )(10)\nThe constant term \u03bb = 0.01 was multiplied to the loss and add this loss in Step 2 and Step 3 of our method. This loss was also introduced in MMD and DANN too when updating parameters of the networks. For the fully-connected layers of classification networks, we set the number of neurons to 1000. In order to fairly compare our method with others, we used the exact the same architecture for other methods.\n\nMMD We calculated the maximum mean discrepancy (MMD) [18], namely the last layer of feature generator networks. We used RBF kernels to calculate the loss. We used the the following standard deviation parameters: \n\nWe changed the number of the kernels and their parameters, but we could not observe significant performance difference. We report the performance after 5 epochs. We could not see any improvement after the epoch. DANN To train a model ( [6]), we used two-layered domain classification networks. We set the number of neurons in the hidden layer as 100. We also used BatchNormalization, ReLU and dropout layer. Experimentally, we did not see any improvement when the network architecture is changed. According to the original method ( [6]), learning rate is decreased every iteration. However, in our experiment, we could not see improvement, thus, we fixed learning rate 1.0 \u00d7 10 \u22123 . In addition, we did not introduce gradient reversal layer for our model. We separately update discriminator and generator. We report the accuracy after 1 epoch.  Fig. (c):Relationship between discrepancy loss (blue line) and accuracy (red and green lines) in adaptation from SYNSIGN to GTSRB during training. As discrepancy loss decreased, accuracy improved as reported in our main paper.\n\n\nD. Experiments on Semantic Segmentation\n\nWe describe the details of our experiments on semantic segmentation.\n\n\nD.1. Details\n\nDatasets Both GTA [28] and Cityscapes [4] are vehicleegocentric image datasets but GTA is synthetic and Cityscapes is realworld dataset. GTA is collected from the open world in the realistically rendered computer game Grand Theft Auto V (GTA, or GTA5). It contains 24,996 images, whose semantic segmentation annotations are fully compatible with the classes used in Cityscapes. Cityscapes is collected in 50 cities in Germany and nearby countries. We only used dense pixel-level annotated dataset collected in 27 cities. It contains 2,975 training set, 500 validation set, and 1525 test set. We used training and validation set. Please note that the labels of Cityscapes are just used for evaluation and never used in training.\n\nTraining Details When training, we ignored the pixelwise loss that is annotated backward (void). Therefore, when testing, no predicted backward label existed. The weight decay ratio was set to 2 \u00d7 10 \u22125 and we used no augmentation methods.\n\nNetwork Architecture We applied our method to FCN-8s based on VGG-16 network. Convolution layers in original VGG-16 networks are used as generator and fullyconnected layers are used as classifiers. For DRN-D-105, we followed the implementation of https://github. com/fyu/drn. We applied our method to dilated residual networks [38,39] for base networks. We used DRN-D-105 model. We used the last convolution networks as classifier networks. All of lower layers are used as a generator. We will publicize our implementation.\n\nEvaluation Metrics As evaluation metrics, we use intersection-over-union (IoU) and pixel accuracy. We use the evaluation code 1 released along with VisDA challenge [26]. It calculates the PASCAL VOC intersection-overunion, i.e., IoU = TP TP+FP+FN , where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively, determined over the whole test set. For further discussing our result, we also compute pixel accuracy, pixelAcc. = \u03a3inii \u03a3iti , where n ii denotes number of pixels of class i predicted to belong to class j and t i denotes total number of pixels of class i in ground truth segmentation.   \n\nFigure 1 .\n1(Best viewed in color.) Comparison of previous and the proposed distribution matching methods.. Left: Previous methods try to match different distributions by mimicing the domain classifier. They do not consider the decision boundary. Right: Our proposed method tries to detect target samples outside the support of the source distribution using task-specific classifiers.\n\nFigure 2 .\n2(Best viewed in color.) Example of two classifiers with an overview of the proposed method.\n\n\n(x) = h (x) is assumed to be very low. h and h should agree on their predictions on source samples. Thus, d H\u2206H (S, T ) is approximately calculated as sup (h,h )\u2208H 2 E x\u223cT I h(x) = h (x) , which denotes the supremum of the expected disagreement of two classifiers' predictions on target samples.\n\nFigure 4 .\n4(Best viewed in color.) Red and green points indicate the source samples of class 0 and 1, respectively. Blue points are target samples generated by rotating source samples. The dashed and normal lines are two decision boundaries in our method. The pink and light green regions are where the results of both classifiers are class 0 and 1, respectively.\n\nFigure 5 .\n5(Best viewed in color.) Left:\n\nFigure 6 .\n6Qualitative results on adaptation from GTA5 to Cityscapes Network\n\nFigure 7 .\n7Fig. (b)(b): Visualization of last pooling layer in adaptation from SYNDIG to SVHN.\n\nFC\n\n\nFigure 9 .\n9Network Architecture used for experiments on GTSRB.\n\n\n93.2\u00b11.68 92.8\u00b10.58 93.1\u00b10.42 Ours (n = 3) 93.5\u00b13.66 92.5\u00b10.33 93.7\u00b10.16 Ours (n = 4) 93.6\u00b10.75 93.3\u00b10.40 93.6\u00b10.57 Table 1. Results of the visual DA experiment on the digits and traffic signs datasets. The results are cited from each study. The score of MMD is cited from DSNSYNDIG \nSYNSIG \nMETHOD \nto \nto \nto \nMNIST \nSVHN \nGTSRB \nSource Only \n67.1 \n87.5 \n85.1 \nDistribution Matching based Methods \nMMD  \u2020 [18] \n71.1 \n88.0 \n91.1 \nDANN  \u2020 [6] \n71.1 \n90.3 \n88.7 \nDTN [35] \n84.7 \n-\n-\nDSN  \u2020 [3] \n82.7 \n91.2 \n93.1 \nADDA [36] \n76.0\u00b11.8 \n-\n-\nOurs(n = 2) Other Methods \nATDA  \u2020 [29] \n86.2 \n93.1 \n96.2 \nASSC [10] \n95.7\u00b11.5 \n91.3\u00b10.2 82.8\u00b11.32 \n\n\n\n\nTable 2. Accuracy of ResNet101 model fine-tuned on the VisDA dataset. The reported accuracy was obtained after 10 epoch updates.Method \n\nplane bcycl bus \ncar horse knife mcycl person plant sktbrd train truck mean \nSource Only \n55.1 \n53.3 61.9 59.1 80.6 \n17.9 \n79.7 \n31.2 \n81.0 \n26.5 \n73.5 \n8.5 \n52.4 \nMMD [18] \n87.1 \n63.0 76.5 42.0 90.3 \n42.9 \n85.9 \n53.1 \n49.7 \n36.3 \n85.8 20.7 \n61.1 \nDANN [6] \n81.9 \n77.7 82.8 44.3 81.2 \n29.5 \n65.1 \n28.6 \n51.9 \n54.6 \n82.8 \n7.8 \n57.4 \nOurs (n = 2) \n81.1 \n55.3 83.6 65.7 87.6 \n72.7 \n83.1 \n73.9 \n85.3 \n47.7 \n73.2 27.1 \n69.7 \nOurs (n = 3) \n90.3 \n49.3 82.1 62.9 91.8 \n69.4 \n83.8 \n72.8 \n79.8 \n53.3 \n81.5 29.7 \n70.6 \nOurs (n = 4) \n87.0 \n60.9 83.7 64.0 88.9 \n79.6 \n84.7 \n76.9 \n88.6 \n40.3 \n83.0 25.8 \n71.9 \n\n\n\n\nFigure 8. Network Architecture used for experiments on SVHN.2048 \nunits \nBN+ReLU \n\nFC 10 units \nSoftmax \n\nF 2 : Classifier2 \n\nF 1 : Classifier1 \nG: Generator \n\nFC 2048 \nunits \nBN+ReLU \n\nFC 10 units \nSoftmax \n\nconv \n5x5x64 \nBN+ReLU \n\nconv \n5x5x64 \nBN+ReLU \n\nmax-pool 3x3 \n2x2 stride \n\nconv \n5x5x128 \nBN+ReLU \n\nFC 3072 \nunits \nBN+ReLU \n\nmax-pool 3x3 \n2x2 stride \n\nconv \n5x5x96 \nBN+ReLU \n\nconv \n3x3x144 \nBN+ReLU \n\nFC 43 units \nSoftmax \n\nFC 512 units \nBN+ReLU \n\nFC 43 units \nSoftmax \n\nF 2 : Classifier2 \n\nF 1 : Classifier1 \n\nG: Generator \n\nconv \n5x5x256 \nBN+ReLU \n\nFC 512 units \nBN+ReLU \n\nmax-pool 2x2 \n2x2 stride \n\nmax-pool 2x2 \n2x2 stride \n\nmax-pool 2x2 \n2x2 stride \n\n\nhttps://github.com/VisionLearningGroup/taskcv-2017public/blob/master/segmentation/eval.py\n\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, Machine learning. 791-25S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different do- mains. Machine learning, 79(1-2):151-175, 2010. 4, 5\n\nAnalysis of representations for domain adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, NIPS. 24S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. Anal- ysis of representations for domain adaptation. In NIPS, 2007. 2, 4\n\nDomain separation networks. K Bousmalis, G Trigeorgis, N Silberman, D Krishnan, D Erhan, NIPS. 6K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. In NIPS, 2016. 1, 2, 5, 6\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, CVPR. 811M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 8, 11\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 7\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, ICML. 710Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, 2014. 6, 7, 10\n\nDomainadversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, JMLR. 17595Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain- adversarial training of neural networks. JMLR, 17(59):1-35, 2016. 1, 2, 5\n\nDeep reconstruction-classification networks for unsupervised domain adaptation. M Ghifary, W B Kleijn, M Zhang, D Balduzzi, W Li, ECCV. M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li. Deep reconstruction-classification networks for un- supervised domain adaptation. In ECCV, 2016. 2\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NIPS. Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger2I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen- erative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, NIPS, 2014. 2\n\nAssociative domain adaptation. P Haeusser, T Frerix, A Mordvintsev, D Cremers, ICCV. P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. As- sociative domain adaptation. In ICCV, 2017. 6\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 7\n\nJ Hoffman, D Wang, F Yu, T Darrell, arXiv:1612.02649Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. J. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adapta- tion. arXiv:1612.02649, 2016. 8\n\nAdam: A method for stochastic optimization. D Kingma, J Ba, arXiv:1412.6980D. Kingma and J. Ba. Adam: A method for stochastic opti- mization. arXiv:1412.6980, 2014. 6\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998. 6\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCV. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In ECCV, 2014. 7\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. 7J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 7, 8\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M I Jordan, ICML. 710M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning trans- ferable features with deep adaptation networks. In ICML, 2015. 2, 6, 7, 10\n\nUnsupervised domain adaptation with residual transfer networks. M Long, H Zhu, J Wang, M I Jordan, NIPS. M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In NIPS, 2016. 2\n\nTransfer learning from multiple source domains via consensus regularization. P Luo, F Zhuang, H Xiong, Y Xiong, Q He, CIKM. P. Luo, F. Zhuang, H. Xiong, Y. Xiong, and Q. He. Transfer learning from multiple source domains via consensus regu- larization. In CIKM, 2008. 2\n\nVisualizing data using t-sne. L V D Maaten, G Hinton, JMLR. 911L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. JMLR, 9(11):2579-2605, 2008. 6\n\nEvaluation of traffic sign recognition methods trained on synthetically generated data. B Moiseev, A Konev, A Chigorin, A Konushin, ACIVS. B. Moiseev, A. Konev, A. Chigorin, and A. Konushin. Eval- uation of traffic sign recognition methods trained on synthet- ically generated data. In ACIVS, 2013. 6\n\nThe role of context for object detection and semantic segmentation in the wild. R Mottaghi, X Chen, X Liu, N.-G Cho, S.-W Lee, S Fidler, R Urtasun, A Yuille, CVPR. R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi- dler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 8\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, NIPS workshop on deep learning and unsupervised feature learning. Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised fea- ture learning. In NIPS workshop on deep learning and unsu- pervised feature learning, 2011. 6\n\nScikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, JMLR. 1210F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. JMLR, 12(10):2825-2830, 2011. 5\n\nX Peng, B Usman, N Kaushik, J Hoffman, D Wang, K Saenko, arXiv:1710.06924Visda: The visual domain adaptation challenge. 711X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. arXiv:1710.06924, 2017. 7, 11\n\nVariational recurrent adversarial deep domain adaptation. S Purushotham, W Carvalho, T Nilanon, Y Liu, ICLR. 25S. Purushotham, W. Carvalho, T. Nilanon, and Y. Liu. Vari- ational recurrent adversarial deep domain adaptation. In ICLR, 2017. 2, 5\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, ECCV. 811S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016. 8, 11\n\nAsymmetric tri-training for unsupervised domain adaptation. K Saito, Y Ushiku, T Harada, ICML. 67K. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training for unsupervised domain adaptation. In ICML, 2017. 6, 7\n\nLearning transferrable representations for unsupervised domain adaptation. O Sener, H O Song, A Saxena, S Savarese, NIPS. O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning transferrable representations for unsupervised domain adap- tation. In NIPS, 2016. 2\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556K. Simonyan and A. Zisserman. Very deep con- volutional networks for large-scale image recognition. arXiv:1409.1556, 2014. 7\n\nThe german traffic sign recognition benchmark: a multi-class classification competition. J Stallkamp, M Schlipsing, J Salmen, C Igel, IJCNN. 6J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The ger- man traffic sign recognition benchmark: a multi-class classi- fication competition. In IJCNN, 2011. 6, 7\n\nReturn of frustratingly easy domain adaptation. B Sun, J Feng, K Saenko, AAAI. B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, 2016. 2\n\nDeep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, ECCV Workshops. B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ECCV Workshops, 2016. 1, 2, 5\n\nUnsupervised crossdomain image generation. Y Taigman, A Polyak, L Wolf, ICLR. 26Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross- domain image generation. In ICLR, 2017. 2, 6\n\nAdversarial discriminative domain adaptation. E Tzeng, J Hoffman, K Saenko, T Darrell, CVPR. E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. In CVPR, 2017. 6\n\nE Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv:1412.3474Deep domain confusion: Maximizing for domain invariance. 15E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv:1412.3474, 2014. 1, 2, 5\n\nMulti-scale context aggregation by dilated convolutions. F Yu, V Koltun, ICLR. 11F. Yu and V. Koltun. Multi-scale context aggregation by di- lated convolutions. In ICLR, 2016. 11\n\nDilated residual networks. F Yu, V Koltun, T Funkhouser, CVPR. 711F. Yu, V. Koltun, and T. Funkhouser. Dilated residual net- works. In CVPR, 2017. 7, 11\n\nCurriculum domain adaptation for semantic segmentation of urban scenes. Y Zhang, P David, B Gong, ICCV. Y. Zhang, P. David, and B. Gong. Curriculum domain adap- tation for semantic segmentation of urban scenes. In ICCV, 2017. 8\n", "annotations": {"author": "[{\"end\":143,\"start\":69},{\"end\":220,\"start\":144},{\"end\":297,\"start\":221},{\"end\":372,\"start\":298}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":77},{\"end\":158,\"start\":150},{\"end\":237,\"start\":231},{\"end\":312,\"start\":306}]", "author_first_name": "[{\"end\":76,\"start\":69},{\"end\":149,\"start\":144},{\"end\":230,\"start\":221},{\"end\":305,\"start\":298}]", "author_affiliation": "[{\"end\":142,\"start\":111},{\"end\":219,\"start\":188},{\"end\":296,\"start\":265},{\"end\":371,\"start\":340}]", "title": "[{\"end\":66,\"start\":1},{\"end\":438,\"start\":373}]", "venue": null, "abstract": "[{\"end\":1813,\"start\":440}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1998,\"start\":1994},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3391,\"start\":3388},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3394,\"start\":3391},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3396,\"start\":3394},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3399,\"start\":3396},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3497,\"start\":3494},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3499,\"start\":3497},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6976,\"start\":6973},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7084,\"start\":7080},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7153,\"start\":7149},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7328,\"start\":7325},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7331,\"start\":7328},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7333,\"start\":7331},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7336,\"start\":7333},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7339,\"start\":7336},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7342,\"start\":7339},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7526,\"start\":7523},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7607,\"start\":7604},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8096,\"start\":8092},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8099,\"start\":8096},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8375,\"start\":8372},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8941,\"start\":8937},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9203,\"start\":9199},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16784,\"start\":16781},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16891,\"start\":16888},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17277,\"start\":17274},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18296,\"start\":18293},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18299,\"start\":18296},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18301,\"start\":18299},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18304,\"start\":18301},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18307,\"start\":18304},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19680,\"start\":19677},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20095,\"start\":20091},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21579,\"start\":21575},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21769,\"start\":21766},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22990,\"start\":22986},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23029,\"start\":23025},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23068,\"start\":23065},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23172,\"start\":23168},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23229,\"start\":23225},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23304,\"start\":23301},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23383,\"start\":23379},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23891,\"start\":23888},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23893,\"start\":23891},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23896,\"start\":23893},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24215,\"start\":24211},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24229,\"start\":24225},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24610,\"start\":24607},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24916,\"start\":24913},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24965,\"start\":24961},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25686,\"start\":25682},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26807,\"start\":26803},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27356,\"start\":27352},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27577,\"start\":27573},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27611,\"start\":27608},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29102,\"start\":29098},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29120,\"start\":29116},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29139,\"start\":29135},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29707,\"start\":29703},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29768,\"start\":29765},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29822,\"start\":29818},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29825,\"start\":29822},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30413,\"start\":30409},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30434,\"start\":30430},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30568,\"start\":30564},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30611,\"start\":30607},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30649,\"start\":30645},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33398,\"start\":33395},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35011,\"start\":35007},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35407,\"start\":35404},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35703,\"start\":35700},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36390,\"start\":36386},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36409,\"start\":36406},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":37669,\"start\":37665},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":37672,\"start\":37669},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38031,\"start\":38027}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38898,\"start\":38513},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39003,\"start\":38899},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39301,\"start\":39004},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39667,\"start\":39302},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39710,\"start\":39668},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39789,\"start\":39711},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39886,\"start\":39790},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39891,\"start\":39887},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39956,\"start\":39892},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40597,\"start\":39957},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41334,\"start\":40598},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42003,\"start\":41335}]", "paragraph": "[{\"end\":2096,\"start\":1829},{\"end\":3187,\"start\":2098},{\"end\":4239,\"start\":3189},{\"end\":4610,\"start\":4241},{\"end\":4792,\"start\":4612},{\"end\":5964,\"start\":4794},{\"end\":6321,\"start\":5966},{\"end\":6596,\"start\":6323},{\"end\":6660,\"start\":6598},{\"end\":6788,\"start\":6662},{\"end\":7343,\"start\":6805},{\"end\":8747,\"start\":7345},{\"end\":9744,\"start\":8749},{\"end\":9881,\"start\":9746},{\"end\":10196,\"start\":9892},{\"end\":10879,\"start\":10213},{\"end\":12709,\"start\":10881},{\"end\":13302,\"start\":12711},{\"end\":13360,\"start\":13304},{\"end\":13409,\"start\":13362},{\"end\":13609,\"start\":13596},{\"end\":13780,\"start\":13631},{\"end\":13914,\"start\":13782},{\"end\":14026,\"start\":13916},{\"end\":14325,\"start\":14028},{\"end\":14474,\"start\":14346},{\"end\":14597,\"start\":14520},{\"end\":14999,\"start\":14616},{\"end\":15293,\"start\":15001},{\"end\":15324,\"start\":15321},{\"end\":15922,\"start\":15393},{\"end\":16130,\"start\":16025},{\"end\":16387,\"start\":16132},{\"end\":16684,\"start\":16407},{\"end\":16869,\"start\":16708},{\"end\":17451,\"start\":16871},{\"end\":17529,\"start\":17453},{\"end\":17997,\"start\":17789},{\"end\":18524,\"start\":17999},{\"end\":18862,\"start\":18526},{\"end\":19071,\"start\":18981},{\"end\":19470,\"start\":19126},{\"end\":19516,\"start\":19472},{\"end\":19681,\"start\":19518},{\"end\":19919,\"start\":19715},{\"end\":21055,\"start\":19951},{\"end\":22159,\"start\":21057},{\"end\":22702,\"start\":22168},{\"end\":22881,\"start\":22736},{\"end\":24133,\"start\":22883},{\"end\":24461,\"start\":24135},{\"end\":25087,\"start\":24463},{\"end\":26668,\"start\":25089},{\"end\":28160,\"start\":26716},{\"end\":28788,\"start\":28162},{\"end\":29042,\"start\":28829},{\"end\":29318,\"start\":29044},{\"end\":29639,\"start\":29320},{\"end\":30748,\"start\":29641},{\"end\":31458,\"start\":30750},{\"end\":32134,\"start\":31473},{\"end\":32343,\"start\":32155},{\"end\":32412,\"start\":32345},{\"end\":32505,\"start\":32442},{\"end\":33245,\"start\":32545},{\"end\":33399,\"start\":33280},{\"end\":33566,\"start\":33401},{\"end\":33679,\"start\":33568},{\"end\":33791,\"start\":33681},{\"end\":33969,\"start\":33793},{\"end\":34112,\"start\":34019},{\"end\":34499,\"start\":34114},{\"end\":34952,\"start\":34546},{\"end\":35166,\"start\":34954},{\"end\":36239,\"start\":35168},{\"end\":36351,\"start\":36283},{\"end\":37095,\"start\":36368},{\"end\":37336,\"start\":37097},{\"end\":37861,\"start\":37338},{\"end\":38512,\"start\":37863}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13595,\"start\":13410},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14519,\"start\":14475},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15320,\"start\":15294},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15392,\"start\":15325},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16024,\"start\":15923},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16406,\"start\":16388},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17788,\"start\":17530},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18980,\"start\":18863},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19125,\"start\":19072},{\"attributes\":{\"id\":\"formula_9\"},\"end\":34545,\"start\":34500}]", "table_ref": "[{\"end\":25104,\"start\":25097},{\"end\":28279,\"start\":28272},{\"end\":29212,\"start\":29205},{\"end\":30765,\"start\":30758}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1827,\"start\":1815},{\"attributes\":{\"n\":\"2.\"},\"end\":6803,\"start\":6791},{\"attributes\":{\"n\":\"3.\"},\"end\":9890,\"start\":9884},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10211,\"start\":10199},{\"end\":13629,\"start\":13612},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14344,\"start\":14328},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14614,\"start\":14600},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16706,\"start\":16687},{\"attributes\":{\"n\":\"4.\"},\"end\":19713,\"start\":19684},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19949,\"start\":19922},{\"end\":22166,\"start\":22162},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22734,\"start\":22705},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26714,\"start\":26671},{\"attributes\":{\"n\":\"5.\"},\"end\":28827,\"start\":28791},{\"attributes\":{\"n\":\"6.\"},\"end\":31471,\"start\":31461},{\"attributes\":{\"n\":\"7.\"},\"end\":32153,\"start\":32137},{\"end\":32440,\"start\":32415},{\"end\":32543,\"start\":32508},{\"end\":33278,\"start\":33248},{\"end\":34017,\"start\":33972},{\"end\":36281,\"start\":36242},{\"end\":36366,\"start\":36354},{\"end\":38524,\"start\":38514},{\"end\":38910,\"start\":38900},{\"end\":39313,\"start\":39303},{\"end\":39679,\"start\":39669},{\"end\":39722,\"start\":39712},{\"end\":39801,\"start\":39791},{\"end\":39890,\"start\":39888},{\"end\":39903,\"start\":39893}]", "table": "[{\"end\":40597,\"start\":40235},{\"end\":41334,\"start\":40728},{\"end\":42003,\"start\":41397}]", "figure_caption": "[{\"end\":38898,\"start\":38526},{\"end\":39003,\"start\":38912},{\"end\":39301,\"start\":39006},{\"end\":39667,\"start\":39315},{\"end\":39710,\"start\":39681},{\"end\":39789,\"start\":39724},{\"end\":39886,\"start\":39803},{\"end\":39956,\"start\":39905},{\"end\":40235,\"start\":39959},{\"end\":40728,\"start\":40600},{\"end\":41397,\"start\":41337}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4482,\"start\":4476},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11649,\"start\":11642},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12045,\"start\":12039},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12923,\"start\":12917},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13188,\"start\":13182},{\"end\":13653,\"start\":13645},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14210,\"start\":14203},{\"end\":15670,\"start\":15664},{\"end\":16148,\"start\":16142},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19315,\"start\":19306},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19370,\"start\":19361},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19486,\"start\":19480},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21244,\"start\":21235},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22189,\"start\":22179},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22482,\"start\":22473},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25160,\"start\":25154},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25958,\"start\":25945},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26325,\"start\":26319},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26430,\"start\":26421},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30776,\"start\":30770},{\"end\":33861,\"start\":33855},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33872,\"start\":33866},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33880,\"start\":33874},{\"end\":36021,\"start\":36013}]", "bib_author_first_name": "[{\"end\":42141,\"start\":42140},{\"end\":42154,\"start\":42153},{\"end\":42165,\"start\":42164},{\"end\":42176,\"start\":42175},{\"end\":42187,\"start\":42186},{\"end\":42198,\"start\":42197},{\"end\":42200,\"start\":42199},{\"end\":42461,\"start\":42460},{\"end\":42474,\"start\":42473},{\"end\":42485,\"start\":42484},{\"end\":42496,\"start\":42495},{\"end\":42674,\"start\":42673},{\"end\":42687,\"start\":42686},{\"end\":42701,\"start\":42700},{\"end\":42714,\"start\":42713},{\"end\":42726,\"start\":42725},{\"end\":42930,\"start\":42929},{\"end\":42940,\"start\":42939},{\"end\":42949,\"start\":42948},{\"end\":42958,\"start\":42957},{\"end\":42969,\"start\":42968},{\"end\":42982,\"start\":42981},{\"end\":42994,\"start\":42993},{\"end\":43004,\"start\":43003},{\"end\":43012,\"start\":43011},{\"end\":43276,\"start\":43275},{\"end\":43284,\"start\":43283},{\"end\":43292,\"start\":43291},{\"end\":43305,\"start\":43301},{\"end\":43311,\"start\":43310},{\"end\":43317,\"start\":43316},{\"end\":43519,\"start\":43518},{\"end\":43528,\"start\":43527},{\"end\":43700,\"start\":43699},{\"end\":43709,\"start\":43708},{\"end\":43721,\"start\":43720},{\"end\":43731,\"start\":43730},{\"end\":43742,\"start\":43741},{\"end\":43756,\"start\":43755},{\"end\":43770,\"start\":43769},{\"end\":43782,\"start\":43781},{\"end\":44076,\"start\":44075},{\"end\":44087,\"start\":44086},{\"end\":44089,\"start\":44088},{\"end\":44099,\"start\":44098},{\"end\":44108,\"start\":44107},{\"end\":44120,\"start\":44119},{\"end\":44321,\"start\":44320},{\"end\":44335,\"start\":44334},{\"end\":44352,\"start\":44351},{\"end\":44361,\"start\":44360},{\"end\":44367,\"start\":44366},{\"end\":44383,\"start\":44382},{\"end\":44392,\"start\":44391},{\"end\":44405,\"start\":44404},{\"end\":44767,\"start\":44766},{\"end\":44779,\"start\":44778},{\"end\":44789,\"start\":44788},{\"end\":44804,\"start\":44803},{\"end\":44974,\"start\":44973},{\"end\":44980,\"start\":44979},{\"end\":44989,\"start\":44988},{\"end\":44996,\"start\":44995},{\"end\":45110,\"start\":45109},{\"end\":45121,\"start\":45120},{\"end\":45129,\"start\":45128},{\"end\":45135,\"start\":45134},{\"end\":45429,\"start\":45428},{\"end\":45439,\"start\":45438},{\"end\":45618,\"start\":45617},{\"end\":45632,\"start\":45631},{\"end\":45645,\"start\":45644},{\"end\":45647,\"start\":45646},{\"end\":45874,\"start\":45873},{\"end\":45883,\"start\":45882},{\"end\":45893,\"start\":45892},{\"end\":45903,\"start\":45902},{\"end\":46122,\"start\":46118},{\"end\":46129,\"start\":46128},{\"end\":46138,\"start\":46137},{\"end\":46150,\"start\":46149},{\"end\":46158,\"start\":46157},{\"end\":46168,\"start\":46167},{\"end\":46179,\"start\":46178},{\"end\":46189,\"start\":46188},{\"end\":46191,\"start\":46190},{\"end\":46425,\"start\":46424},{\"end\":46433,\"start\":46432},{\"end\":46446,\"start\":46445},{\"end\":46642,\"start\":46641},{\"end\":46650,\"start\":46649},{\"end\":46657,\"start\":46656},{\"end\":46665,\"start\":46664},{\"end\":46667,\"start\":46666},{\"end\":46886,\"start\":46885},{\"end\":46894,\"start\":46893},{\"end\":46901,\"start\":46900},{\"end\":46909,\"start\":46908},{\"end\":46911,\"start\":46910},{\"end\":47130,\"start\":47129},{\"end\":47137,\"start\":47136},{\"end\":47147,\"start\":47146},{\"end\":47156,\"start\":47155},{\"end\":47165,\"start\":47164},{\"end\":47354,\"start\":47353},{\"end\":47358,\"start\":47355},{\"end\":47368,\"start\":47367},{\"end\":47568,\"start\":47567},{\"end\":47579,\"start\":47578},{\"end\":47588,\"start\":47587},{\"end\":47600,\"start\":47599},{\"end\":47862,\"start\":47861},{\"end\":47874,\"start\":47873},{\"end\":47882,\"start\":47881},{\"end\":47892,\"start\":47888},{\"end\":47902,\"start\":47898},{\"end\":47909,\"start\":47908},{\"end\":47919,\"start\":47918},{\"end\":47930,\"start\":47929},{\"end\":48205,\"start\":48204},{\"end\":48215,\"start\":48214},{\"end\":48223,\"start\":48222},{\"end\":48233,\"start\":48232},{\"end\":48245,\"start\":48244},{\"end\":48251,\"start\":48250},{\"end\":48253,\"start\":48252},{\"end\":48583,\"start\":48582},{\"end\":48596,\"start\":48595},{\"end\":48609,\"start\":48608},{\"end\":48621,\"start\":48620},{\"end\":48631,\"start\":48630},{\"end\":48642,\"start\":48641},{\"end\":48652,\"start\":48651},{\"end\":48663,\"start\":48662},{\"end\":48679,\"start\":48678},{\"end\":48688,\"start\":48687},{\"end\":48917,\"start\":48916},{\"end\":48925,\"start\":48924},{\"end\":48934,\"start\":48933},{\"end\":48945,\"start\":48944},{\"end\":48956,\"start\":48955},{\"end\":48964,\"start\":48963},{\"end\":49243,\"start\":49242},{\"end\":49258,\"start\":49257},{\"end\":49270,\"start\":49269},{\"end\":49281,\"start\":49280},{\"end\":49482,\"start\":49481},{\"end\":49484,\"start\":49483},{\"end\":49495,\"start\":49494},{\"end\":49505,\"start\":49504},{\"end\":49513,\"start\":49512},{\"end\":49716,\"start\":49715},{\"end\":49725,\"start\":49724},{\"end\":49735,\"start\":49734},{\"end\":49945,\"start\":49944},{\"end\":49954,\"start\":49953},{\"end\":49956,\"start\":49955},{\"end\":49964,\"start\":49963},{\"end\":49974,\"start\":49973},{\"end\":50205,\"start\":50204},{\"end\":50217,\"start\":50216},{\"end\":50460,\"start\":50459},{\"end\":50473,\"start\":50472},{\"end\":50487,\"start\":50486},{\"end\":50497,\"start\":50496},{\"end\":50729,\"start\":50728},{\"end\":50736,\"start\":50735},{\"end\":50744,\"start\":50743},{\"end\":50920,\"start\":50919},{\"end\":50927,\"start\":50926},{\"end\":51114,\"start\":51113},{\"end\":51125,\"start\":51124},{\"end\":51135,\"start\":51134},{\"end\":51299,\"start\":51298},{\"end\":51308,\"start\":51307},{\"end\":51319,\"start\":51318},{\"end\":51329,\"start\":51328},{\"end\":51459,\"start\":51458},{\"end\":51468,\"start\":51467},{\"end\":51479,\"start\":51478},{\"end\":51488,\"start\":51487},{\"end\":51498,\"start\":51497},{\"end\":51788,\"start\":51787},{\"end\":51794,\"start\":51793},{\"end\":51938,\"start\":51937},{\"end\":51944,\"start\":51943},{\"end\":51954,\"start\":51953},{\"end\":52137,\"start\":52136},{\"end\":52146,\"start\":52145},{\"end\":52155,\"start\":52154}]", "bib_author_last_name": "[{\"end\":42151,\"start\":42142},{\"end\":42162,\"start\":42155},{\"end\":42173,\"start\":42166},{\"end\":42184,\"start\":42177},{\"end\":42195,\"start\":42188},{\"end\":42208,\"start\":42201},{\"end\":42471,\"start\":42462},{\"end\":42482,\"start\":42475},{\"end\":42493,\"start\":42486},{\"end\":42504,\"start\":42497},{\"end\":42684,\"start\":42675},{\"end\":42698,\"start\":42688},{\"end\":42711,\"start\":42702},{\"end\":42723,\"start\":42715},{\"end\":42732,\"start\":42727},{\"end\":42937,\"start\":42931},{\"end\":42946,\"start\":42941},{\"end\":42955,\"start\":42950},{\"end\":42966,\"start\":42959},{\"end\":42979,\"start\":42970},{\"end\":42991,\"start\":42983},{\"end\":43001,\"start\":42995},{\"end\":43009,\"start\":43005},{\"end\":43020,\"start\":43013},{\"end\":43281,\"start\":43277},{\"end\":43289,\"start\":43285},{\"end\":43299,\"start\":43293},{\"end\":43308,\"start\":43306},{\"end\":43314,\"start\":43312},{\"end\":43325,\"start\":43318},{\"end\":43525,\"start\":43520},{\"end\":43538,\"start\":43529},{\"end\":43706,\"start\":43701},{\"end\":43718,\"start\":43710},{\"end\":43728,\"start\":43722},{\"end\":43739,\"start\":43732},{\"end\":43753,\"start\":43743},{\"end\":43767,\"start\":43757},{\"end\":43779,\"start\":43771},{\"end\":43792,\"start\":43783},{\"end\":44084,\"start\":44077},{\"end\":44096,\"start\":44090},{\"end\":44105,\"start\":44100},{\"end\":44117,\"start\":44109},{\"end\":44123,\"start\":44121},{\"end\":44332,\"start\":44322},{\"end\":44349,\"start\":44336},{\"end\":44358,\"start\":44353},{\"end\":44364,\"start\":44362},{\"end\":44380,\"start\":44368},{\"end\":44389,\"start\":44384},{\"end\":44402,\"start\":44393},{\"end\":44412,\"start\":44406},{\"end\":44776,\"start\":44768},{\"end\":44786,\"start\":44780},{\"end\":44801,\"start\":44790},{\"end\":44812,\"start\":44805},{\"end\":44977,\"start\":44975},{\"end\":44986,\"start\":44981},{\"end\":44993,\"start\":44990},{\"end\":45000,\"start\":44997},{\"end\":45118,\"start\":45111},{\"end\":45126,\"start\":45122},{\"end\":45132,\"start\":45130},{\"end\":45143,\"start\":45136},{\"end\":45436,\"start\":45430},{\"end\":45442,\"start\":45440},{\"end\":45629,\"start\":45619},{\"end\":45642,\"start\":45633},{\"end\":45654,\"start\":45648},{\"end\":45880,\"start\":45875},{\"end\":45890,\"start\":45884},{\"end\":45900,\"start\":45894},{\"end\":45911,\"start\":45904},{\"end\":46126,\"start\":46123},{\"end\":46135,\"start\":46130},{\"end\":46147,\"start\":46139},{\"end\":46155,\"start\":46151},{\"end\":46165,\"start\":46159},{\"end\":46176,\"start\":46169},{\"end\":46186,\"start\":46180},{\"end\":46199,\"start\":46192},{\"end\":46430,\"start\":46426},{\"end\":46443,\"start\":46434},{\"end\":46454,\"start\":46447},{\"end\":46647,\"start\":46643},{\"end\":46654,\"start\":46651},{\"end\":46662,\"start\":46658},{\"end\":46674,\"start\":46668},{\"end\":46891,\"start\":46887},{\"end\":46898,\"start\":46895},{\"end\":46906,\"start\":46902},{\"end\":46918,\"start\":46912},{\"end\":47134,\"start\":47131},{\"end\":47144,\"start\":47138},{\"end\":47153,\"start\":47148},{\"end\":47162,\"start\":47157},{\"end\":47168,\"start\":47166},{\"end\":47365,\"start\":47359},{\"end\":47375,\"start\":47369},{\"end\":47576,\"start\":47569},{\"end\":47585,\"start\":47580},{\"end\":47597,\"start\":47589},{\"end\":47609,\"start\":47601},{\"end\":47871,\"start\":47863},{\"end\":47879,\"start\":47875},{\"end\":47886,\"start\":47883},{\"end\":47896,\"start\":47893},{\"end\":47906,\"start\":47903},{\"end\":47916,\"start\":47910},{\"end\":47927,\"start\":47920},{\"end\":47937,\"start\":47931},{\"end\":48212,\"start\":48206},{\"end\":48220,\"start\":48216},{\"end\":48230,\"start\":48224},{\"end\":48242,\"start\":48234},{\"end\":48248,\"start\":48246},{\"end\":48256,\"start\":48254},{\"end\":48593,\"start\":48584},{\"end\":48606,\"start\":48597},{\"end\":48618,\"start\":48610},{\"end\":48628,\"start\":48622},{\"end\":48639,\"start\":48632},{\"end\":48649,\"start\":48643},{\"end\":48660,\"start\":48653},{\"end\":48676,\"start\":48664},{\"end\":48685,\"start\":48680},{\"end\":48696,\"start\":48689},{\"end\":48922,\"start\":48918},{\"end\":48931,\"start\":48926},{\"end\":48942,\"start\":48935},{\"end\":48953,\"start\":48946},{\"end\":48961,\"start\":48957},{\"end\":48971,\"start\":48965},{\"end\":49255,\"start\":49244},{\"end\":49267,\"start\":49259},{\"end\":49278,\"start\":49271},{\"end\":49285,\"start\":49282},{\"end\":49492,\"start\":49485},{\"end\":49502,\"start\":49496},{\"end\":49510,\"start\":49506},{\"end\":49520,\"start\":49514},{\"end\":49722,\"start\":49717},{\"end\":49732,\"start\":49726},{\"end\":49742,\"start\":49736},{\"end\":49951,\"start\":49946},{\"end\":49961,\"start\":49957},{\"end\":49971,\"start\":49965},{\"end\":49983,\"start\":49975},{\"end\":50214,\"start\":50206},{\"end\":50227,\"start\":50218},{\"end\":50470,\"start\":50461},{\"end\":50484,\"start\":50474},{\"end\":50494,\"start\":50488},{\"end\":50502,\"start\":50498},{\"end\":50733,\"start\":50730},{\"end\":50741,\"start\":50737},{\"end\":50751,\"start\":50745},{\"end\":50924,\"start\":50921},{\"end\":50934,\"start\":50928},{\"end\":51122,\"start\":51115},{\"end\":51132,\"start\":51126},{\"end\":51140,\"start\":51136},{\"end\":51305,\"start\":51300},{\"end\":51316,\"start\":51309},{\"end\":51326,\"start\":51320},{\"end\":51337,\"start\":51330},{\"end\":51465,\"start\":51460},{\"end\":51476,\"start\":51469},{\"end\":51485,\"start\":51480},{\"end\":51495,\"start\":51489},{\"end\":51506,\"start\":51499},{\"end\":51791,\"start\":51789},{\"end\":51801,\"start\":51795},{\"end\":51941,\"start\":51939},{\"end\":51951,\"start\":51945},{\"end\":51965,\"start\":51955},{\"end\":52143,\"start\":52138},{\"end\":52152,\"start\":52147},{\"end\":52160,\"start\":52156}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8577357},\"end\":42407,\"start\":42095},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10908021},\"end\":42643,\"start\":42409},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2127515},\"end\":42864,\"start\":42645},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":502946},\"end\":43220,\"start\":42866},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57246310},\"end\":43465,\"start\":43222},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6755881},\"end\":43650,\"start\":43467},{\"attributes\":{\"id\":\"b6\"},\"end\":43993,\"start\":43652},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52459},\"end\":44289,\"start\":43995},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1033682},\"end\":44733,\"start\":44291},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":20186541},\"end\":44925,\"start\":44735},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":45107,\"start\":44927},{\"attributes\":{\"doi\":\"arXiv:1612.02649\",\"id\":\"b11\"},\"end\":45382,\"start\":45109},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b12\"},\"end\":45550,\"start\":45384},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":195908774},\"end\":45790,\"start\":45552},{\"attributes\":{\"id\":\"b14\"},\"end\":46073,\"start\":45792},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14113767},\"end\":46366,\"start\":46075},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1629541},\"end\":46577,\"start\":46368},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":556999},\"end\":46819,\"start\":46579},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":745350},\"end\":47050,\"start\":46821},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10440951},\"end\":47321,\"start\":47052},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5855042},\"end\":47477,\"start\":47323},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":34532173},\"end\":47779,\"start\":47479},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6529084},\"end\":48133,\"start\":47781},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16852518},\"end\":48538,\"start\":48135},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10659969},\"end\":48914,\"start\":48540},{\"attributes\":{\"doi\":\"arXiv:1710.06924\",\"id\":\"b25\"},\"end\":49182,\"start\":48916},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":51837620},\"end\":49427,\"start\":49184},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5844139},\"end\":49653,\"start\":49429},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":12570770},\"end\":49867,\"start\":49655},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2268966},\"end\":50134,\"start\":49869},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b30\"},\"end\":50368,\"start\":50136},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15926837},\"end\":50678,\"start\":50370},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16439870},\"end\":50855,\"start\":50680},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12453047},\"end\":51068,\"start\":50857},{\"attributes\":{\"id\":\"b34\"},\"end\":51250,\"start\":51070},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4357800},\"end\":51456,\"start\":51252},{\"attributes\":{\"doi\":\"arXiv:1412.3474\",\"id\":\"b36\"},\"end\":51728,\"start\":51458},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":17127188},\"end\":51908,\"start\":51730},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6592393},\"end\":52062,\"start\":51910},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11824004},\"end\":52291,\"start\":52064}]", "bib_title": "[{\"end\":42138,\"start\":42095},{\"end\":42458,\"start\":42409},{\"end\":42671,\"start\":42645},{\"end\":42927,\"start\":42866},{\"end\":43273,\"start\":43222},{\"end\":43516,\"start\":43467},{\"end\":43697,\"start\":43652},{\"end\":44073,\"start\":43995},{\"end\":44318,\"start\":44291},{\"end\":44764,\"start\":44735},{\"end\":44971,\"start\":44927},{\"end\":45615,\"start\":45552},{\"end\":46116,\"start\":46075},{\"end\":46422,\"start\":46368},{\"end\":46639,\"start\":46579},{\"end\":46883,\"start\":46821},{\"end\":47127,\"start\":47052},{\"end\":47351,\"start\":47323},{\"end\":47565,\"start\":47479},{\"end\":47859,\"start\":47781},{\"end\":48202,\"start\":48135},{\"end\":48580,\"start\":48540},{\"end\":49240,\"start\":49184},{\"end\":49479,\"start\":49429},{\"end\":49713,\"start\":49655},{\"end\":49942,\"start\":49869},{\"end\":50457,\"start\":50370},{\"end\":50726,\"start\":50680},{\"end\":50917,\"start\":50857},{\"end\":51111,\"start\":51070},{\"end\":51296,\"start\":51252},{\"end\":51785,\"start\":51730},{\"end\":51935,\"start\":51910},{\"end\":52134,\"start\":52064}]", "bib_author": "[{\"end\":42153,\"start\":42140},{\"end\":42164,\"start\":42153},{\"end\":42175,\"start\":42164},{\"end\":42186,\"start\":42175},{\"end\":42197,\"start\":42186},{\"end\":42210,\"start\":42197},{\"end\":42473,\"start\":42460},{\"end\":42484,\"start\":42473},{\"end\":42495,\"start\":42484},{\"end\":42506,\"start\":42495},{\"end\":42686,\"start\":42673},{\"end\":42700,\"start\":42686},{\"end\":42713,\"start\":42700},{\"end\":42725,\"start\":42713},{\"end\":42734,\"start\":42725},{\"end\":42939,\"start\":42929},{\"end\":42948,\"start\":42939},{\"end\":42957,\"start\":42948},{\"end\":42968,\"start\":42957},{\"end\":42981,\"start\":42968},{\"end\":42993,\"start\":42981},{\"end\":43003,\"start\":42993},{\"end\":43011,\"start\":43003},{\"end\":43022,\"start\":43011},{\"end\":43283,\"start\":43275},{\"end\":43291,\"start\":43283},{\"end\":43301,\"start\":43291},{\"end\":43310,\"start\":43301},{\"end\":43316,\"start\":43310},{\"end\":43327,\"start\":43316},{\"end\":43527,\"start\":43518},{\"end\":43540,\"start\":43527},{\"end\":43708,\"start\":43699},{\"end\":43720,\"start\":43708},{\"end\":43730,\"start\":43720},{\"end\":43741,\"start\":43730},{\"end\":43755,\"start\":43741},{\"end\":43769,\"start\":43755},{\"end\":43781,\"start\":43769},{\"end\":43794,\"start\":43781},{\"end\":44086,\"start\":44075},{\"end\":44098,\"start\":44086},{\"end\":44107,\"start\":44098},{\"end\":44119,\"start\":44107},{\"end\":44125,\"start\":44119},{\"end\":44334,\"start\":44320},{\"end\":44351,\"start\":44334},{\"end\":44360,\"start\":44351},{\"end\":44366,\"start\":44360},{\"end\":44382,\"start\":44366},{\"end\":44391,\"start\":44382},{\"end\":44404,\"start\":44391},{\"end\":44414,\"start\":44404},{\"end\":44778,\"start\":44766},{\"end\":44788,\"start\":44778},{\"end\":44803,\"start\":44788},{\"end\":44814,\"start\":44803},{\"end\":44979,\"start\":44973},{\"end\":44988,\"start\":44979},{\"end\":44995,\"start\":44988},{\"end\":45002,\"start\":44995},{\"end\":45120,\"start\":45109},{\"end\":45128,\"start\":45120},{\"end\":45134,\"start\":45128},{\"end\":45145,\"start\":45134},{\"end\":45438,\"start\":45428},{\"end\":45444,\"start\":45438},{\"end\":45631,\"start\":45617},{\"end\":45644,\"start\":45631},{\"end\":45656,\"start\":45644},{\"end\":45882,\"start\":45873},{\"end\":45892,\"start\":45882},{\"end\":45902,\"start\":45892},{\"end\":45913,\"start\":45902},{\"end\":46128,\"start\":46118},{\"end\":46137,\"start\":46128},{\"end\":46149,\"start\":46137},{\"end\":46157,\"start\":46149},{\"end\":46167,\"start\":46157},{\"end\":46178,\"start\":46167},{\"end\":46188,\"start\":46178},{\"end\":46201,\"start\":46188},{\"end\":46432,\"start\":46424},{\"end\":46445,\"start\":46432},{\"end\":46456,\"start\":46445},{\"end\":46649,\"start\":46641},{\"end\":46656,\"start\":46649},{\"end\":46664,\"start\":46656},{\"end\":46676,\"start\":46664},{\"end\":46893,\"start\":46885},{\"end\":46900,\"start\":46893},{\"end\":46908,\"start\":46900},{\"end\":46920,\"start\":46908},{\"end\":47136,\"start\":47129},{\"end\":47146,\"start\":47136},{\"end\":47155,\"start\":47146},{\"end\":47164,\"start\":47155},{\"end\":47170,\"start\":47164},{\"end\":47367,\"start\":47353},{\"end\":47377,\"start\":47367},{\"end\":47578,\"start\":47567},{\"end\":47587,\"start\":47578},{\"end\":47599,\"start\":47587},{\"end\":47611,\"start\":47599},{\"end\":47873,\"start\":47861},{\"end\":47881,\"start\":47873},{\"end\":47888,\"start\":47881},{\"end\":47898,\"start\":47888},{\"end\":47908,\"start\":47898},{\"end\":47918,\"start\":47908},{\"end\":47929,\"start\":47918},{\"end\":47939,\"start\":47929},{\"end\":48214,\"start\":48204},{\"end\":48222,\"start\":48214},{\"end\":48232,\"start\":48222},{\"end\":48244,\"start\":48232},{\"end\":48250,\"start\":48244},{\"end\":48258,\"start\":48250},{\"end\":48595,\"start\":48582},{\"end\":48608,\"start\":48595},{\"end\":48620,\"start\":48608},{\"end\":48630,\"start\":48620},{\"end\":48641,\"start\":48630},{\"end\":48651,\"start\":48641},{\"end\":48662,\"start\":48651},{\"end\":48678,\"start\":48662},{\"end\":48687,\"start\":48678},{\"end\":48698,\"start\":48687},{\"end\":48924,\"start\":48916},{\"end\":48933,\"start\":48924},{\"end\":48944,\"start\":48933},{\"end\":48955,\"start\":48944},{\"end\":48963,\"start\":48955},{\"end\":48973,\"start\":48963},{\"end\":49257,\"start\":49242},{\"end\":49269,\"start\":49257},{\"end\":49280,\"start\":49269},{\"end\":49287,\"start\":49280},{\"end\":49494,\"start\":49481},{\"end\":49504,\"start\":49494},{\"end\":49512,\"start\":49504},{\"end\":49522,\"start\":49512},{\"end\":49724,\"start\":49715},{\"end\":49734,\"start\":49724},{\"end\":49744,\"start\":49734},{\"end\":49953,\"start\":49944},{\"end\":49963,\"start\":49953},{\"end\":49973,\"start\":49963},{\"end\":49985,\"start\":49973},{\"end\":50216,\"start\":50204},{\"end\":50229,\"start\":50216},{\"end\":50472,\"start\":50459},{\"end\":50486,\"start\":50472},{\"end\":50496,\"start\":50486},{\"end\":50504,\"start\":50496},{\"end\":50735,\"start\":50728},{\"end\":50743,\"start\":50735},{\"end\":50753,\"start\":50743},{\"end\":50926,\"start\":50919},{\"end\":50936,\"start\":50926},{\"end\":51124,\"start\":51113},{\"end\":51134,\"start\":51124},{\"end\":51142,\"start\":51134},{\"end\":51307,\"start\":51298},{\"end\":51318,\"start\":51307},{\"end\":51328,\"start\":51318},{\"end\":51339,\"start\":51328},{\"end\":51467,\"start\":51458},{\"end\":51478,\"start\":51467},{\"end\":51487,\"start\":51478},{\"end\":51497,\"start\":51487},{\"end\":51508,\"start\":51497},{\"end\":51793,\"start\":51787},{\"end\":51803,\"start\":51793},{\"end\":51943,\"start\":51937},{\"end\":51953,\"start\":51943},{\"end\":51967,\"start\":51953},{\"end\":52145,\"start\":52136},{\"end\":52154,\"start\":52145},{\"end\":52162,\"start\":52154}]", "bib_venue": "[{\"end\":42226,\"start\":42210},{\"end\":42510,\"start\":42506},{\"end\":42738,\"start\":42734},{\"end\":43026,\"start\":43022},{\"end\":43331,\"start\":43327},{\"end\":43544,\"start\":43540},{\"end\":43798,\"start\":43794},{\"end\":44129,\"start\":44125},{\"end\":44418,\"start\":44414},{\"end\":44818,\"start\":44814},{\"end\":45006,\"start\":45002},{\"end\":45234,\"start\":45161},{\"end\":45426,\"start\":45384},{\"end\":45660,\"start\":45656},{\"end\":45871,\"start\":45792},{\"end\":46205,\"start\":46201},{\"end\":46460,\"start\":46456},{\"end\":46680,\"start\":46676},{\"end\":46924,\"start\":46920},{\"end\":47174,\"start\":47170},{\"end\":47381,\"start\":47377},{\"end\":47616,\"start\":47611},{\"end\":47943,\"start\":47939},{\"end\":48322,\"start\":48258},{\"end\":48702,\"start\":48698},{\"end\":49034,\"start\":48989},{\"end\":49291,\"start\":49287},{\"end\":49526,\"start\":49522},{\"end\":49748,\"start\":49744},{\"end\":49989,\"start\":49985},{\"end\":50202,\"start\":50136},{\"end\":50509,\"start\":50504},{\"end\":50757,\"start\":50753},{\"end\":50950,\"start\":50936},{\"end\":51146,\"start\":51142},{\"end\":51343,\"start\":51339},{\"end\":51578,\"start\":51523},{\"end\":51807,\"start\":51803},{\"end\":51971,\"start\":51967},{\"end\":52166,\"start\":52162}]"}}}, "year": 2023, "month": 12, "day": 17}