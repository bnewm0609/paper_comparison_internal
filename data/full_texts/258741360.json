{"id": 258741360, "updated": "2023-10-05 01:12:57.167", "metadata": {"title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering", "authors": "[{\"first\":\"Xiaoman\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chaoyi\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Ziheng\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Weixiong\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Ya\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yanfeng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Weidi\",\"last\":\"Xie\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images with vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin. Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.10415", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-10415", "doi": "10.48550/arxiv.2305.10415"}}, "content": {"source": {"pdf_hash": "f4793adffd6f67ffcb93ccfc5672ab301b8a2b96", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.10415v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d08b3424db6aa4080ba5a5d3fbfaaf2b315fc498", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96.txt", "contents": "\nPMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering\n\n\nXiaoman Zhang \nShanghai Jiao Tong University\n\n\nShanghai AI Laboratory\n\n\nChaoyi Wu \nShanghai Jiao Tong University\n\n\nShanghai AI Laboratory\n\n\nZiheng Zhao \nShanghai Jiao Tong University\n\n\nWeixiong Lin \nShanghai Jiao Tong University\n\n\nYa Zhang \nShanghai Jiao Tong University\n\n\nShanghai AI Laboratory\n\n\nYanfeng Wang \nShanghai Jiao Tong University\n\n\nShanghai AI Laboratory\n\n\nWeidi Xie \nShanghai Jiao Tong University\n\n\nShanghai AI Laboratory\n\n\nPMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering\n\nIn this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images with vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model. Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin. Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve.\n\nIntroduction\n\nLarge language models (LLMs), such as ChatGPT [1], PaLM [8], LLaMA [35], have recently demonstrated remarkable progress in a wide range of natural language processing (NLP) tasks, such as question answering, text classification, and interactive dialog. Notably, even in domains where expert knowledge is supposed to play a critical role, like medical diagnosis, these language models have also achieved impressive success, passing the United States Medical Licensing Examination (USMLE) [13,17,26,32]. While recent LLMs excel in language understanding in the medical domain, they are essentially \"blind\" to visual modalities such as images and videos, hindering the utilization of visual content as a means of communication with these models.\n\nIn this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which aims to develop models that can comprehend text-based queries and produce accurate answers by leveraging medical visual content [21]. Existing MedVQA methods [25,22,6,20] typically treat the problem as a retrieval task with a limited answer base and train multi-modal vision-language models with contrastive or classification objectives. Consequently, they are only useful for limited use cases where a finite set of outcomes is provided beforehand. We propose to develop the first open-ended MedVQA system with a generative model as the backend, capable of handling diverse questions that arise in clinical practice, generating answers in free form without being constrained by the vocabulary. While there has been promising research in visual-language representation learning, Table 1: Comparison of existing medical VQA datasets with PMC-VQA, demonstrating the significant increase in size and diversity achieved by our dataset. Dataset Modality Source Images QA pairs VQA-RAD [18] Radiology MedPix \u00ae database 0.3k 3.5k PathVQA [12] Pathology PEIR Digital Library [14] 5k 32.8k SLAKE [23] Radiology MSD [3], ChestX-ray8 [36], CHAOS [15] 0.7k 14k VQA-Med-2021 [  such as Flamingo [2] and BLIP [19], these models have primarily been trained on natural language and images, with very limited application in medical domain, due to the complex and nuanced visual concepts often found in medical scenarios.\n\nTo this end, we introduce a novel paradigm for MedVQA that harnesses the power of generative learning. Specifically, our proposed models start from the foundation models in medical domain, and train a bridge to align the pre-trained vision encoder and large language model via visual instruction tuning, we term the model as MedVInT (Medical Visual Instruction Tuning). To accommodate different architectures, we offer two variants, named as MedVInT-TE and MedVInT-TD, that are tailored for encoder-based and decoder-based language models, respectively.\n\nIn order to effectively train the generative-based MedVQA models, our study reveals that existing datasets are limited in size, making them insufficient for training high-performing models. To overcome this challenge, we leverage well-established medical visual-language datasets [20] and initiate a scalable, automatic pipeline for constructing a new large-scale medical visual questionanswering dataset. This new dataset, termed as PMC-VQA, contains 227k VQA pairs of 149k images, covering various modalities or diseases ( Fig. 1), surpassing existing datasets in terms of both amount and diversity, as illustrated in Tab. 1. In our experiments, we pre-trained MedVInT on the collected PMC-VQA dataset and fine-tuned it on the existing MedVQA datasets, e.g., VQA-RAD [18] and SLAKE [23], outperforming existing models by a large margin, achieving over 80% accuracy on multi-choice selection. However, while evaluating on our proposed challenging benchmark, even the state-of-the-art models struggle, showing that there is still ample room for development in this field.\n\nIn summary, our contributions are as follows: (i) We reframe the problem of MedVQA as a generative learning task and propose MedVInT, a model obtained by aligning a pre-trained vision encoder with large language model through visual instruction tuning; (ii) We introduce a scalable pipeline and construct a large-scale MedVQA dataset, PMC-VQA, which far exceeds the size and diversity of existing datasets, covering various modalities and diseases; (iii) We pre-train MedVInT on PMC-VQA and fine-tune it on VQA-RAD [18] and SLAKE [23], achieving state-of-the-art performance and significantly outperforming existing models; (iv) We propose a new test set and present a more challenging benchmark for MedVQA, to evaluate the performance of VQA methods thoroughly.\n\n\nMethod\n\nHere, we start with an introduction to the problem of generative medical visual question answering in Sec. What did the axial spin echo post-gadolinium T1WI show in the patient? A: Homogeneous enhancement of the brain tissue B: Heterogeneous enhancement of the brain tissue C: Homogeneous enhancement and extradural location of meningioma D: Homogeneous enhancement and intradural location of meningioma\n\n\nQuestion-Answer Generation\n\n(b) Pipeline for PMC-VQA generation Figure 2: (a) The proposed architecture of MedVInt, mainly consists of three components: a visual encoder to extract visual features, a language encoder to encode textual context, and a multimodal decoder to generate the answer; (b) The proposed question-answer pairs generation pipeline.\n\n\nProblem Formulation\n\nMedVQA is a task of answering natural language questions about medical visual content, typically images or videos obtained from medical devices like X-ray, CT, MRI, or microscopy, etc. Specifically, our goal is to train a model that can output the corresponding answer for a given question, which can be expressed as:\u00e2\ni = \u03a6 MedVQA (I i , q i ; \u0398) = \u03a6 dec (\u03a6 vis (I i ; \u03b8 vis ), \u03a6 text (q i ; \u03b8 text ); \u03b8 dec )(1)\nHere,\u00e2 i refers to the predicted answer, I i \u2208 R H\u00d7W \u00d7C refers to the visual image, H, W, C are height, width, channel respectively. The posed question and corresponding ground-truth answer in the form of natural language are denoted as q i and a i , respectively. \u0398 = {\u03b8 vis , \u03b8 text , \u03b8 dec } denote the trainable parameters.\n\nExisting approaches have primarily treated medical VQA as a classification problem, with the goal of selecting the correct answer from a candidate set, i.e., a i \u2208 \u2126 = {a 1 , a 2 , . . . , a N }, where N represents the total number of answers within the dataset. Consequently, this approach limits the system's utility to predefined outcomes, hampering its free-form user-machine interaction potential.\n\nIn this paper, we take an alternative approach, with the goal to generate an open-ended answer in natural language. Specifically, we train the system by maximizing the probability of generating the ground-truth answer given the input image and question. The loss function used to train the model is typically the negative log-likelihood of the correct next token in the sequence, summed over all time steps, which can be expressed as :\nL(\u0398) = \u2212 T t=1\nlog p(a t |I, q 1:T , a 1:t\u22121 ; \u0398)\n\nwhere T is the length of the ground-truth answer, and p(a t |I, q 1:T , a 1:t\u22121 ; \u0398) is the probability of generating the t-th token in the answer sequence given the input image I, the question sequence q 1:T , and the previous tokens in the answer sequence a 1:t\u22121 . This formulation allows the model to generate diverse and informative answers, which can be useful in a wider range of scenarios than traditional classification-based methods.\n\n\nArchitecture\n\nIn this section, we introduce our proposed architecture for generative MedVQA ( Fig. 2(a)). Specifically, we offer two model variants, which are tailored to encoder-based and decoder-based language models, respectively, denoted as MedVInT-TE (Sec. 2.2.1) and MedVInT-TD (Sec. 2.2.2).\n\n\nMedVInT-TE\n\nVisual Encoder. Given one specific image I, we can obtain the image embedding, i.e., v = \u03a6 vis (I) \u2208 R n\u00d7d , where d denotes the embedding dimension, n denotes the patch number. The vision encoder is based on a pre-trained ResNet-50 adopted from PMC-CLIP [20], with a trainable projection module. To produce a fixed shape of visual output, we add a trainable projection module on top of the ResNet-50, with the aim of bridging the gap between the pre-trained visual and language embeddings. We propose two distinct variants for this projection module. The first variant, MLP-based, employs a two-layer Multilayer Perceptron (MLP), while the second variant, transformer-based, employs a 12-layer transformer decoder supplemented with several learnable vectors as query input.\n\nLanguage Encoder. Given one question on the image, to guide the language model with desirable output, we append a fixed prompt with the question, i.e., \"Question: q, the answer is:\", and encode it with the language encoder: q = \u03a6 text (q) \u2208 R l\u00d7d , where q refers to the text embedding, l represents the sequential length for the question, and q is the prompted question. \u03a6 text is initialized with the pre-trained language model. Note that our model can also be applied to multiple-choice tasks, by providing options and training it to output the right choice as \"A/B/C/D\". The prompt is then modified as \"Question: q, the options are: a 1 , a 2 , a 3 , a 4 , the answer is:\", where a i refers to the i-th option.\n\nMultimodal Decoder. With encoded visual embeddings (v) and question embeddings (q), we concatenate them as the input to the multimodal decoder (\u03a6 dec ). The multimodal decoder is initialized from scratch with a 4-layer transformer structure. Additionally, acknowledging that the encoder-based language models lack casual masking, we reform the generation task as a mask language modeling task, i.e., the question input is padded with several '[MASK]' token and the decoder module learns to generate the prediction for the masked token.\n\n\nMedVInT-TD\n\nVisual Encoder. The visual encoder is the same as MedVInT-TE.\n\nText Encoder. We design \u03a6 text as a simple embedding layer similar to the primary GPT-like LLMs and initialized with their parameters. Same with MedVInT-TE, it also encodes the question input into embedding features q and can perform multi-choice or blank through different prompts.\n\nMultimodal Decoder. For the Transformer decoder-based language model, with its output format already being free-form text, we directly use its architecture as the multimodal decoder initialized with the pre-train weights. Specifically, we concatenate the image and text features as the input. However, directly using the text decoder as a multimodal decoder, may lead to significant mismatching between the image encoding space and the decoder input space. Therefore, to further fill the gap between the image embedding space, here, we pre-train the whole network using the PMC-OA [20] dataset in a caption-based manner, which is similar to BLIP-2 [19].\n\n\nThe PMC-VQA Dataset\n\nOur study has identified the lack of large-scale, multi-modal MedVQA datasets as a significant obstacle to the development of effective generative MedVQA models. To address this issue, we present a scalable and automatic pipeline for creating a new large MedVQA dataset. In this section, we provide a detailed description of our dataset collection process, starting with the source data and continuing with the question-answer generation and data filtering procedures. Finally, we analyze the collected data from various perspectives to gain insights into its properties and potential applications.\n\nSource Data. We start from PMC-OA [20], which is a comprehensive biomedical dataset comprising 1.6 million image-text pairs collected from PubMedCentral (PMC)'s OpenAccess subset [31], which covers 2.4 million papers. In order to maintain the diversity and complexity of PMC-VQA, we have used a version of 381K image-caption pairs obtained from the first stage of the medical figure collection process without subfigure auto-separation. We have opted not to use the final released version of the dataset, which only includes subfigure separation, subcaption separation, and alignment, in order to maintain a certain level of complexity and avoid oversimplifying the dataset.\n\nQuestion-Answer Generation. To automatically generate high-quality question-answer pairs within the constraints of an academic budget, we leverage the power of ChatGPT by inputting the image captions of PMC-OA as the content to the model. We use the following prompt to generate 5 question-answer pairs for each caption.  To answer questions related to these images, the network must acquire sufficient medical knowledge, for example, for the first two images, it is essential to recognize the anatomy structure and modalities; for the third image, recognizing the X-ray image pattern of pathologies is necessary; for the final two images, apart from the basic biomedical knowledge, the model is also required to discern colors, differentiate subfigures, and perform Optical Character Recognition (OCR). This approach allows us to generate a large volume of diverse and high-quality questions that cover a wide range of medical topics. After generating the question-answer pairs using ChatGPT, we applied a rigorous filtering process to ensure that the pairs met our formatting requirements. As a result, we obtained 1,497,808 question-answer pairs, and since the original captions are linked with images, the pairs can naturally find corresponding images, resulting in an average of 3.93 pairs per image.\n\nData Filtering. As the questions are sourced from image captions, some questions can be answered correctly using biomedical knowledge alone without the need for a specific image, for example, question: \"which type of MRI sequence shows high signal in the marrow edema?\". To address this issue, we trained a question-answer model using LLaMA-7B [35] with text data only and eliminated all questions that could be potentially answerable by the language model. This filtering process resulted in 848,433 high-quality question-answer pairs.\n\nFurthermore, some questions in our data rely on additional information in the caption that cannot be answered using only the corresponding image, such as \"How many patients were classified into the middle stage?\" To identify these questions, we trained a question classification model to determine whether a question is answerable given the image alone. Specifically, we manually annotated 2192 question-answer pairs and randomly split them into a training set of 1752 pairs and a testing set of 440 pairs. We fine-tuned LLaMA-7B [35] on this training set, and our model achieved an accuracy of 81.77% on the test set. We then used this model for data cleaning, resulting in a total of 226,946 question-answer pairs corresponding to 149,075 images. From this cleaned dataset, we randomly selected 50,000 image-question pairs to create our test set, namely, PMC-VQA-test. Additionally, we also provided a small clean test set of 2,000 samples, which were manually verified for quality, termed as PMC-VQA-test-clean. During this manual verification procedure, we have estimated that over 80% of PMC-VQA-test can be retained.\n\nData Analysis. This section provides an analysis on images, questions, and answers in the PMC-VQA dataset. In detail, the dataset comprises 227k image-question pairs, some examples are presented in Fig.3, which demonstrates the wide diversity of images within our dataset. As indicated in Table1, PMC-VQA outperforms existing MedVQA datasets in terms of data size and modality diversity. The questions in our dataset cover a range of difficulties, from simple questions such as identifying image modalities, perspectives, and organs to challenging questions that require specialized knowledge and judgment. Additionally, our dataset includes difficult questions that demand the ability to identify the specific target sub-figure from the compound figure.\n\nOur \n\n\nExperiments\n\nIn this section, we first introduce two existing primary MedVQA datasets, namely VQA-RAD and SLAKE (Sec There are 224 possible answers in total. We only use the \"English\" part, and follow the official split.\n\nBaselines and Metrics. We compare with various baselines on these two MedVQA datasets, namely, MEVF-BAN [25], CPRD-BAN [22], M3AE [6], PMC-CLIP [20]. PMC-CLIP [20] is the existing SOTA method on these two datasets. For evaluation, ACC scores are used. Note that, since our model is generative-based, we calculate ACC by matching the generative output with the options using difflib.SequenceMatcher and choosing the most similar one as the choice of the model, which is more difficult than the evaluation for retrieval-based methods due to the larger output space.\n\n\nPMC-VQA Dataset\n\nThe Multi-choice MedVQA. Four candidate answers are provided for each question as the prompt. The model is then trained to select the correct option among them. The accuracy (ACC) score can be used to evaluate the performance of the model on this task.\n\nOpen-ended MedVQA. The total possible answers for PMC-VQA are over 100K, which makes the traditional retrieval-based approach limited in usefulness for the answer set of such a level. Therefore, we provide another training style, called \"blank\", where the network is not provided with options in input and is required to directly generate answers based on the questions. For evaluation, we adopt two metrics. The first is Bleu scores, which are widely used to evaluate the quality of generated text against a set of references. The second is ACC scores, which can be computed by comparing the generated answer with the ground-truth answer using sentence similarity, as introduced in Sec. 4.1.\n\n\nPre-trained Backbones\n\nIn this section, we introduce the pre-trained models used in our experiments. We separate them into language and vision backbones. Notably, while all the following models can be used in our architecture, by default, we use the \"PMC-LLaMA\" (or \"PMC-LLaMA-ENC\") and \"PMC-CLIP\" as backbones, since they are known to be more suitable for medical data according to previous works.\n\n\nLanguage Backbone\n\nLLaMA [35] is a state-of-the-art large-scale language model, pre-trained on trillions of tokens and widely used in the research community. We adopt the 7B version, which consists of 32 transformer layers, as our language backbone.\n\nPMC-LLaMA [37] is an open-source language model that is acquired by fine-tuning LLaMA-7B on a total of 4.8 million biomedical academic papers with auto-regressive loss. Compared to LLaMA, PMC-LLaMA demonstrates stronger fitting capabilities and better performance on medical tasks.\n\nPubMedBERT [11] is an encoder-based BERT-like model that is trained from scratch using abstracts from PubMed and full-text articles from PubMedCentral in the corpus \"The Pile\" [10]. It has 12 transformer layers and 100 million parameters. Such domain-specific models proved to yield excellent text embedding capability before the era of large language models.\n\nLLaMA-ENC and PMC-LLaMA-ENC. While LLaMA and PMC-LLaMA are known for their performance in text generation tasks, we also experiment with them as encoder models by passing a full attention mask and sampling the embedding from the last token. This allows for a direct comparison to be made with the aforementioned BERT-like models, which are also encoder-based.\n\n\nVision Backbone\n\nCLIP [30] is a model trained from scratch on a dataset of 400 million image-text pairs collected from the internet with contrastive loss. We use its \"ViT-base-patch32\" version as our visual encoder with 12 transformer layers, which has been pre-trained on natural images.\n\n\nPMC-CLIP [20]\n\nis a medical-specific visual model based on CLIP architecture, which was trained on a dataset of 1.6 million biomedical image-text pairs collected from PubMed open-access papers using cross-modality contrastive loss. Compared to the pre-trained visual model on natural images, PMC-CLIP is specifically designed to handle medical images and text.\n\n\nResults\n\nIn this section, we begin by evaluating our model on two publicly-available datasets, VQA-RAD and SLAKE, and compare it with existing MedVQA models, showing state-of-the-art performance. However, these datasets have limited diversity and scope, which led us to propose a more challenging MedVQA benchmark in Sec. 5.2. Our benchmark covers significantly more diverse modalities and diseases, and we demonstrate that even state-of-the-art methods struggle to perform well on it.\n\n\nComparison on Existing Datasets\n\nAs shown in Tab. 2, comparing our model to existing ones, we can draw the following observations: State-of-the-art Performance of Generative MedVQA. As shown in Tab. 2, our MedVInT model outperforms the previous state-of-the-art (SOTA) methods on both the VQA-RAD and SLAKE datasets, regardless of whether the \"MedVInT-TE\" or \"MedVInT-TD\" variant is used. We improved the overall accuracy (ACC) scores from 77.6% to 81.6% on VQA-RAD and from 84.3% to 88.0% on SLAKE. Notably, since our model generates answers rather than retrieving one from a pre-defined answer basis, the evaluation metric is more challenging, further demonstrating our superiority.\n\nPre-training on PMC-VQA is Essential for Generative MedVQA. Comparing results using the same architecture, with and without PMC-VQA, it is clear that pre-training with PMC-VQA significantly outperforms. Specifically, \"MedVInT-TE\" boosts the final results by approximately 11% on VQA-RAD and 4% on SLAKE compared to \"MedVInT-TE-S\" that refers to training the model from scratch without pre-trained on PMC-VQA. Similar improvements are observed with 'MedVInT-TD'. These results highlight the critical role that our PMC-VQA plays in addressing the major challenges that hinder the development of a generative MedVQA system.\n\nBoth MedVInT-TE and MedVInT-TD Perform Well. The gap between the two training styles mainly exists in open-ended questions, with \"MedVInT-TD\" performing better on VQA-RAD and \"MedVInT-TE\" being more effective on SLAKE. This difference can be attributed to the fact that the VQA-RAD answers are typically longer than those in SLAKE, making the \"MedVInT-TD\" model more suitable for generating such answers. Conversely, SLAKE questions often require short and concise responses, making the MedVInT-TE\" model more appropriate for such retrieve-like tasks.\n\n\nBenchmark on PMC-VQA\n\nIn this section, we introduce our new MedVQA benchmark on PMC-VQA. We evaluate different methods for both open-ended and multiple-choice tasks. The results are summarized in Tab. 3 (See supplementary for more qualitative comparisons.).We can draw the following observations:\n\nMultimodal Understanding is Essential. As shown in Tab. 3, when using only language, the model struggles to provide accurate answers and produces nearly random outcomes, with accuracies of only 26.1% in Blanking and 30.6% in Choice. It is worth noting that around 30% of the questions have \"B\" answers, making the 30.6% score nearly equivalent to the highest possible score attainable through guessing. These observations highlight the crucial role of multimodal understanding in our dataset and emphasize the strong relationship between the images and the questions posed.\n\nGeneral Visual-language Models Struggle on MedVQA. We evaluated the zero-shot performance of existing SOTA multimodal models, BLIP-2 and open-source version of Flamingo [19,4]. As shown, even the best-performing models in natural images struggle to answer our MedVQA questions, demonstrating the challenging nature of our dataset and its strong biomedical relevance.\n\nPMC-VQA-test Presents a Significantly More Challenging Benchmark. Notably, the previous SOTA multimodal model for MedVQA, PMC-CLIP [20], struggles on our dataset. Not only does it fail to solve the blanking task, but it also significantly underperforms on multi-choice questions, with accuracy close to random. These findings underline the difficulty of our dataset and its potential to serve as a more robust benchmark for evaluating VQA models. Comparing Generative Model Backbones on PMC-VQA-test. To further assess the effectiveness of our proposed method, we compared it against various baselines that use different generative model backbones. Our results show that replacing the general visual backbone with a specialized medical one leads to improved performance, highlighting the importance of visual understanding in MedVQA. Additionally, we observed that replacing the language backbone with a domain-specific model also leads to some improvements, although not as significant as those achieved in the visual domain.\n\nDifferent Projection Modules Demonstrate Comparable Performance. We provide the comparison of baseline models using different projection modules (MLP or Transformer) on both open-ended and multiple-choice tasks. As shown, different projection modules demonstrate comparable performance across various evaluation tasks. Both architectures can effectively reconcile the diversity in the embedding dimensions arising from different pre-trained visual models, making our architecture adaptable to various visual model designs, regardless of whether they are based on ViT or ResNet.\n\n\nRelated Works\n\nInstruction Tuning with Large-language Models. Large Language Models (LLMs) have recently achieved tremendous success [28,27,1] in generating high-quality text for various tasks such as language translation, summarization, and question answering. Open-source models, e.g., Alpaca [34], have proposed instruction tuning to train models using examples generated from ChatGPT [1], effectively improving the performance of language models. In the visual-language domain, concurrent work to ours, Mini-GPT4 [39] generates a high-quality image-text dataset by prompting ChatGPT with well-designed inputs. In this paper, we focus on visual instruction tuning for MedVQA, which poses unique challenges due to the complexity of medical texts and the variability of medical images.\n\nMedical Visual Question Answering. The field of MedVQA has gained significant interest in recent years, with a growing number of studies [21]. Despite the increasing attention, building a robust and reliable MedVQA system remains challenging due to the complexity and variability of medical images, as well as the lack of large-scale and diverse MedVQA datasets. Existing publicly available MedVQA datasets have limitations on diversity, or dataset scale, for example, RadVisDial [16] only contains samples on chest x-ray images, VQA-Med [5], VQA-RAD [18], and SLAKE [23] have less than 10K images. To address these limitations, we propose the PMC-VQA dataset that includes 227k image-question pairs with various image modalities and question types.\n\n\nConclusion\n\nIn conclusion, this paper addresses the challenge of MedVQA, where even the strongest VQA models trained on natural images yield results that closely resemble random guesses. To overcome this, we propose MedVInT, a generative model tailored to advance this crucial medical task. MedVInT is trained by aligning visual data from a pre-trained vision encoder with language models. Additionally, we present a scalable pipeline for constructing PMC-VQA, a comprehensive MedVQA dataset comprising 227k VQA pairs across 149k images, spanning diverse modalities and diseases. Our proposed model delivers state-of-the-art performance on existing MedVQA datasets, providing a new and reliable benchmark for evaluating different methods in this field.  \n\n\nB Implementation Details\n\nOur models are trained using the AdamW optimizer [24] with a learning rate 2e-5. The max context length is set as 512, and the batch size is 128. To improve the training speed of our models, we adopt the Deepspeed acceleration strategy, together with Automatic Mixed Precision (AMP) and gradient checkpointing [9]. All models are implemented in PyTorch and trained on NVIDIA A100 GPU with 80 GB memory\n\n\nC Social Impact\n\nIn an era where the digitization of healthcare is rapidly advancing, and medical data is proliferating, multimodal tools such as Medical Visual Question Answering (MedVQA) present significant potential to revolutionize patient care, empower clinicians, and bolster research. Our contribution in this field is twofold: First, we introduce a scalable pipeline for the creation of a MedVQA dataset. This scalability ensures a continuous evolution and expansion of the dataset, maintaining its relevance in the everchanging landscape of healthcare. Second, we present the PMC-VQA dataset, crafted to overcome the limitations inherent in existing datasets. By encompassing a larger, more diverse selection of medical images, complemented by sophisticated questions and answers, we aim to significantly enhance the reliability and precision of medical multimodal models. This innovation holds the promise of equipping these models with the necessary tools to effectively navigate real-world scenarios.\n\n\nD Limitation\n\nThe proposed PMC-VQA has several limitations:\n\nInherent Biases: Despite efforts to construct a comprehensive MedVQA dataset with PMC-VQA, it is important to acknowledge the potential presence of biases in the dataset. Biases might arise from the data collection process, annotation methodology, or underlying distribution of the medical images and questions. Understanding and addressing these biases is crucial for ensuring fair and unbiased performance evaluation.\n\nPotential Annotation Biases: Despite efforts to ensure quality and accuracy during the annotation process of PMC-VQA-test-clean, the dataset may still be susceptible to annotation biases. The subjective nature of question-answer pairs and the involvement of human annotators introduces the possibility of inconsistencies or subjective interpretations, which could impact the dataset's reliability.\n\nLacking Comprehensive Evaluation Metrics: Although both the ACC score and Bleu score are utilized in our benchmark for assessing open-ended blanking results, these two metrics fail to capture the fluency of the generated sentence since they measure string similarity irrespective of word order. As exhibited in the third case of Fig.5, the encoder-based model significantly underperforms compared to the decoder-based model in this regard, a fact not reflected in the quantitative results. Indeed, finding an objective way to evaluate generative results comprehensively poses a significant challenge in the entire generative model community [7]. To address this issue, we plan to explore more evaluation metrics in our benchmark in future work.\n\nNeed for Continual Dataset Expansion and Updates: The medical field is dynamic, with ongoing advancements and new findings. To ensure the dataset's relevance and coverage of emerging medical knowledge, continual expansion and updates to the PMC-VQA dataset are necessary.\n\nFigure 1 :\n1The top 20 figure types in PMC-VQA, cover a wide range of diagnostic procedures.\n\nFigure 3 :\n3Several examples of challenging questions and answers along with their respective images.\n\n\nAsk 5 questions about the content and generate four options for each question. The questions should be answerable with the information provided in the caption, and the four options should include one correct and three incorrect options, with the position of the correct option randomized. The output should use the following template: i:'the question index' question:'the generate question' choice: 'A:option content B:option content C:option content D:option content' answer: The correct option(A\\B\\C\\D).\n\nFigure 4 :\n4analysis of the PMC-VQA dataset can be summarized in three aspects: (i) Images: We show the top 20 figure types in the PMC-VQA in Fig. 1. The images in the PMC-VQA are extremely diverse, ranging from Radiology to Signals. (ii) Questions: We clustered the questions into different Question distribution of the training set by their first four words. From left to right are all questions, questions started with \"What\" and questions started with \"Which\". The ordering of the words starts towards the center and radiates outwards. types based on the words that start the question, as shown in Fig. 4. We found a surprising variety of question types, including \"What is the difference...\", \"What type of imaging...\", and \"Which image shows...\". Most questions range from 5 to 15 words, and detailed information about the distribution of question lengths is shown in the supplementary materials. (iii) Answers: The words in answers primarily encompass positional descriptions, image modalities, and specific anatomical regions. Detailed information about the top 50 words that appeared in the answers is provided in the supplementary materials. Most answers are around 5 words, which is much shorter than the questions. The correct options were distributed as follows: A (24.07%), B (30.87%), C (29.09%), D (15.97 %).\n\n\n. 4.1). We then provide a detailed description of our proposed dataset, PMC-VQA, which can be used for both multiple-choice and open-ended answering tasks (Sec. 4.2). Finally, we discuss the primary pre-trained models we use for ablation in Sec. 4.3. The implementation details is provided in the supplementary materials.4.1 Existing MedVQA Datasets VQA-RAD [18] is a VQA dataset specifically designed for radiology, consisting of 315 images and 3,515 questions with 517 possible answers. The questions in VQA-RAD are categorized as either close-ended or open-ended, depending on whether answer choices are limited or not. We follow the official dataset split for our evaluation. SLAKE [23] is an English-Chinese bilingual VQA dataset composed of 642 images and 14k questions. The questions are categorized as close-ended if answer choices are limited, otherwise open-ended.\n\nFigure 5 :\n5Examples of image captions, images, the generated question-answer pairs, and model prediction. The wrong predictions are highlighted in red.A.2 Data AnalysisFig. 6 presents the top 50 words that appeared in the answers. As shown, words in answers primarily encompass positional descriptions such as left and right, image modality such as CT/MRI, and specific anatomical regions.Fig. 7shows the percentage of questions and answers with different word lengths. Most questions range from 5 to 15 words, and most answers are around 5 words.\n\nFigure 6 :Figure 7 :\n67Answer Percentage of questions and answers with different word lengths.\n\n\n2.1; then we present the architecture detail in Sec. 2.2.Image Caption: Anatomic and MRF data of subject excluded due to partial volume effects. a MRF T1-relaxometry map shows \u2026.. . b Axial spin echo post-gadolinium T1WI shows typical homogeneous enhancement and extradural location of meningioma.Visual Encoder \n\u03a6 !\"# \n\nProjection \n\nQuestion: What did the axial spin echo \npost-gadolinium T1WI show in the \npatient? \nThe answer is: \n\nMultimodal Decoder \u03a6 $%& \n\nTarget answer: Homogeneous enhancement and extradural loc\u2026 \n\nText Encoder \u03a6 '%(' \n\n(a) Overall architecture of MedVInT \n\n\n\n\nPMC-VQA dataset consists of a train set with 177K samples and a test set with 50K samples, which are respectively denoted as PMC-VQA-train and PMC-VQA-test. Additionally, the smaller clean test set with 2K samples that have been manually verified, is referred to as PMC-VQA-test-clean. The dataset can be used for both open-ended and multiple-choice tasks.https://docs.python.org/3/library/difflib.html \n\n\nTable 2 :\n2Comparison of ACC to SOTA approaches on VQA-RAD and SLAKE. We use the blank model for evaluation. Pre-training data indicates whether the model is pre-trained on the medical multi-modal dataset before training on the target dataset. The best result is in red, the second-best result is in blue. \"Overal\" refers to the micro-average ACC of all the Open and Close questions.Method \nPre-training Data \nVQA-RAD \nSLAKE \nOpen Close Overall Open Close Overall \n\nMEVF-BAN [25] -\n49.2 \n77.2 \n66.1 \n77.8 \n79.8 \n78.6 \nCPRD-BAN [22] -\n52.5 \n77.9 \n67.8 \n79.5 \n83.4 \n81.1 \nM3AE [6] \nROCO [29], MedICaT [33] \n67.2 \n83.5 \n77.0 \n80.3 \n87.8 \n83.3 \nPMC-CLIP [20] \nPMC-OA [20] \n67.0 \n84.0 \n77.6 \n81.9 \n88.0 \n84.3 \n\nMedVInT-TE-S \n-\n53.6 \n76.5 \n67.4 \n84.0 \n85.1 \n84.4 \nMedVInT-TD-S \n-\n55.3 \n80.5 \n70.5 \n79.7 \n85.1 \n81.8 \nMedVInT-TE \nPMC-VQA \n69.3 \n84.2 \n78.2 \n88.2 \n87.7 \n88.0 \nMedVInT-TD \nPMC-VQA \n73.7 \n86.8 \n81.6 \n84.5 \n86.3 \n85.2 \n\n\n\nTable 3 :\n3Comparison of baseline models using different pre-trained models on both open-ended and multiple-choice tasks. We reported the results on PMC-VQA-test / PMC-VQA-test-clean. \"Scratch\" means to train the vision model from scratch with the same architecture as \"PMC-CLIP\".Method \nLanguage Backbone \nVision Backbone \nBlanking \nChoice \nACC \nBleu-1 \nACC \n\nZero-shot \n\nPMC-CLIP [20] \nPMC-CLIP [20] \nPMC-CLIP [20] \n-\n-\n24.0 / 24.7 \nBLIP-2 [19] \nOPT-2.7B [38] \nCLIP [30] \n22.5 / 21.8 5.2 / 7.6 24.6 / 24.3 \nOpen-Flamingo [4] \nLLaMA[35] \nCLIP [30] \n26.1 / 26.5 4.1 / 4.1 25.0 / 26.4 \n\nTrained on PMC-VQA \n\nLLaMA [35] \nLLaMA [35] \n-\n26.1 / 27.2 14.2 / 14.6 30.6 / 30.8 \n\nMedVInT-TE-MLP \n\nPubMedBERT [11] \n\nScratch \n33.7 / 34.2 20.4 / 20.9 34.4 / 34.9 \nCLIP [30] \n33.7 / 34.4 20.4 / 20.8 34.5 / 34.3 \nPMC-CLIP [20] 35.2 / 36.4 22.0 / 23.2 37.1 / 37.6 \n\nLLaMA-ENC [35] \n\nScratch \n32.5 / 32.5 15.3 / 15.9 35.2 / 35.1 \nCLIP [30] \n32.3 / 33.4 15.6 / 15.1 35.3 / 36.1 \nPMC-CLIP [20] 35.4 / 36.8 18.2 / 18.4 36.9 / 37.1 \n\nPMC-LLaMA-ENC [37] \n\nScratch \n32.6 / 35.0 16.2 / 17.0 37.0 / 38.0 \nCLIP [30] \n33.0 / 34.4 16.6 / 16.5 37.1 / 38.5 \nPMC-CLIP [20] 34.8 / 35.3 18.1 / 18.6 38.2 / 39.2 \n\nMedVInT-TE-Transformer \n\nPubMedBERT [11] \n\nScratch \n34.1 / 36.2 21.0 / 21.9 39.8 / 40.6 \nCLIP [30] \n33.9 / 34.6 20.6 / 21.8 39.9 / 40.9 \nPMC-CLIP [20] 33.7 / 35.4 20.3 / 21.2 40.2 / 40.9 \n\nLLaMA-ENC [35] \n\nScratch \n32.0 / 33.5 15.1 / 15.3 38.4 / 39.7 \nCLIP [30] \n32.3 / 34.3 15.5 / 15.7 38.4 / 38.7 \nPMC-CLIP [20] 35.9 / 37.1 19.0 / 19.3 38.9 / 39.4 \n\nPMC-LLaMA-ENC [37] \n\nScratch \n33.2 / 34.7 16.6 / 16.5 38.1 /39.8 \nCLIP [30] \n33.6 / 35.1 16.7 / 17.2 38.7 / 38.9 \nPMC-CLIP [20] 35.5 / 36.0 18.4 /18.6 38.2 / 37.7 \n\nMedVInT-TD-MLP \n\nLLaMA[35] \n\nScratch \n28.1 / 30.6 16.5 / 16.9 35.8 / 37.4 \nCLIP [30] \n30.2 / 32.7 18.6 / 18.5 35.8 / 37.1 \nPMC-CLIP [20] 31.3 / 32.6 19.5 / 19.8 38.4 / 41.0 \n\nPMC-LLaMA [37] \n\nScratch \n28.3 / 30.6 16.4 / 17.3 35.8 / 37.0 \nCLIP [30] \n31.4 / 31.8 19.2 / 19.5 36.2 / 37.9 \nPMC-CLIP [20] 32.1 / 31.7 19.7 / 20.2 38.4 / 42.3 \n\nMedVInT-TD-Transformer \n\nLLaMA[35] \n\nScratch \n29.1 / 30.2 17.4 / 18.0 31.1 / 37.9 \nCLIP [30] \n31.3 / 32.2 19.5 / 20.0 38.2 / 38.3 \nPMC-CLIP [20] 31.9 / 33.4 20.0 / 21.3 37.3 / 39.5 \n\nPMC-LLaMA [37] \n\nScratch \n28.6 / 29.8 16.8 / 17.4 36.8 / 36.9 \nCLIP [30] \n31.4 / 32.6 19.5 / 20.4 36.8 / 36.9 \nPMC-CLIP [20] 32.7 / 33.6 20.3 / 21.5 39.4 / 40.3 \n\n\nA PMC-VQA DatasetA.1 ExamplesIn order to provide a more comprehensive understanding of the dataset, we offer additional examples illustrated inFig. 5. This figure showcases random instances of the original image and corresponding captions, along with multiple-choice questions generated from them. Additionally, we present the predictions of MedVInT-TE and MedVInT-TD models, with PMC-CLIP and PMC-LLAMA as their vision and language backbones.Magnetic resonance imaging coronal view of the brain showing T1 weighted image revealing hyperintensity in bilateral basal ganglia due to mineral deposition. Both the arrows point out to the hypertense foci at the basal ganglia bilaterally on a T1-weighted MRI image that suggests mineral deposition.MedVInT-TE Prediction: Left and and arteryMedVInT-TD Prediction:Left anterior descending artery and right circumflex artery Reformatted non-contrast whole-heart submillimeter isotropic CMRA (left) and CCTA (right) images along the LCX (top) and RCA (bottom) are shown for a 54 year-old male patient. The CMRA dataset was acquired in 9 min with 100% scan efficiency (heart rate of 57 bpm). The CCTA images demonstrate mild (25-49%) disease with a calcified plaque within the proximal RCA and severe disease (70-90%) with a partially calcified plaque in the midsegment of RCA (red arrows), and minimal (0-24%) disease with calcified plaque in the midsegment of the LCX.\n. Openai. introducing chatgpt. 10Openai. introducing chatgpt. https://openai.com/blog/chatgpt/, 2023. 1, 10\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Reynolds, Advances in Neural Information Processing Systems. 35Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716-23736, 2022. 2\n\nThe medical segmentation decathlon. Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Geert Bennett A Landman, Bjoern Litjens, Olaf Menze, Ronald M Ronneberger, Summers, Nature Communications. 1314128Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, et al. The medical segmentation decathlon. Nature Communications, 13(1):4128, 2022. 2\n\n. Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, 89Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, et al. Openflamingo, 2023. URL https: //github.com/mlfoundations/open_flamingo. 8, 9\n\nOverview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain. Asma Ben Abacha, Mourad Sarrouti, Dina Demner-Fushman, A Sadid, Henning Hasan, M\u00fcller, Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes. the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes210Asma Ben Abacha, Mourad Sarrouti, Dina Demner-Fushman, Sadid A Hasan, and Henning M\u00fcller. Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain. In Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes. 21-24 September 2021, 2021. 2, 10\n\nMulti-modal masked autoencoders for medical vision-and-language pre-training. Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, Tsung-Hui Chang, Medical Image Computing and Computer Assisted Intervention. Springer6Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, and Tsung-Hui Chang. Multi-modal masked autoencoders for medical vision-and-language pre-training. In Medical Image Computing and Computer Assisted Intervention, pages 679-689. Springer, 2022. 1, 6, 8\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. 16\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 1\n\nOptimal gradient checkpoint search for arbitrary computation graphs. Jianwei Feng, Dong Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition15Jianwei Feng and Dong Huang. Optimal gradient checkpoint search for arbitrary computa- tion graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11433-11442, 2021. 15\n\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, arXiv:2101.00027The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprintLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. 7\n\nDomain-specific language model pretraining for biomedical natural language processing. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon, ACM Transactions on Computing for Healthcare. 319Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23, 2021. 7, 9\n\nTowards visual question answering on pathology images. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, Pengtao Xie, 2020Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Towards visual question answering on pathology images. pages 708-718, 2020. 2\n\nWhat disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 11146421Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. 1\n\nPeir digital library: Online resources and authoring system. N Kristopher, Dwain E Jones, Kristina Woode, Peter G Panizzi, Anderson, Proceedings of the AMIA Symposium. the AMIA Symposium1075Kristopher N Jones, Dwain E Woode, Kristina Panizzi, and Peter G Anderson. Peir digital library: Online resources and authoring system. In Proceedings of the AMIA Symposium, page 1075. American Medical Informatics Association, 2001. 2\n\nChaos challengecombined (ct-mr) healthy abdominal organ segmentation. Sinem A Emre Kavur, Mustafa Gezer, Sinem Bar\u0131\u015f, Pierre-Henri Aslan, Vladimir Conze, Groza, Soumick Duc Duy Pham, Philipp Chatterjee, Sava\u015f Ernst, \u00d6zkan, Medical Image Analysis. 692101950A Emre Kavur, N Sinem Gezer, Mustafa Bar\u0131\u015f, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst, Sava\u015f \u00d6zkan, et al. Chaos challenge- combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis, 69:101950, 2021. 2\n\nTowards visual dialog for radiology. Olga Kovaleva, Chaitanya Shivade, Satyananda Kashyap, Karina Kanjaria, Joy Wu, Deddeh Ballah, Adam Coy, Alexandros Karargyris, Yufan Guo, David Beymer Beymer, Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing. the 19th SIGBioMed Workshop on Biomedical Language ProcessingOlga Kovaleva, Chaitanya Shivade, Satyananda Kashyap, Karina Kanjaria, Joy Wu, Deddeh Ballah, Adam Coy, Alexandros Karargyris, Yufan Guo, David Beymer Beymer, et al. Towards visual dialog for radiology. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, pages 60-69, 2020. 10\n\nPerformance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. H Tiffany, Morgan Kung, Arielle Cheatham, Czarina Medenilla, Lorie De Sillos, Camille Leon, Maria Elepa\u00f1o, Rimel Madriaga, Giezel Aggabao, James Diaz-Candido, Maningo, PLoS digital health. 22Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepa\u00f1o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Perfor- mance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198, 2023. 1\n\nAsma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data. J Jason, Soumya Lau, Gayen, 510Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1): 1-10, 2018. 2, 6, 10\n\nBlip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.1259789arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 2, 4, 8, 9\n\nPmc-clip: Contrastive language-image pre-training using biomedical documents. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie, arXiv:2303.0724089arXiv preprintWeixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-clip: Contrastive language-image pre-training using biomedical documents. arXiv preprint arXiv:2303.07240, 2023. 1, 2, 4, 6, 7, 8, 9\n\nMedical visual question answering: A survey. Zhihong Lin, Donghao Zhang, Qingyi Tac, Danli Shi, Gholamreza Haffari, Qi Wu, arXiv:2111.10056110arXiv preprintMingguang He, and Zongyuan GeZhihong Lin, Donghao Zhang, Qingyi Tac, Danli Shi, Gholamreza Haffari, Qi Wu, Ming- guang He, and Zongyuan Ge. Medical visual question answering: A survey. arXiv preprint arXiv:2111.10056, 2022. 1, 10\n\nContrastive pre-training and representation distillation for medical visual question answering based on radiology images. Bo Liu, Li-Ming Zhan, Xiao-Ming Wu, Medical Image Computing and Computer Assisted Intervention. Springer6Bo Liu, Li-Ming Zhan, and Xiao-Ming Wu. Contrastive pre-training and representation distillation for medical visual question answering based on radiology images. In Medical Image Computing and Computer Assisted Intervention, pages 210-220. Springer, 2021. 1, 6, 8\n\nSlake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, Xiao-Ming Wu, 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). 610Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically- labeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650-1654. IEEE, 2021. 2, 6, 10\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.0510115Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 15\n\nOvercoming data limitation in medical visual question answering. Thanh-Toan Binh D Nguyen, Do, X Binh, Tuong Nguyen, Erman Do, Quang D Tjiputra, Tran, Medical Image Computing and Computer Assisted Intervention. Springer6Binh D Nguyen, Thanh-Toan Do, Binh X Nguyen, Tuong Do, Erman Tjiputra, and Quang D Tran. Overcoming data limitation in medical visual question answering. In Medical Image Computing and Computer Assisted Intervention, pages 522-530. Springer, 2019. 1, 6, 8\n\nCapabilities of gpt-4 on medical challenge problems. Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, arXiv:2303.13375arXiv preprintHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Ca- pabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. 1\n\n. Openai, arXiv:2303.08774Gpt-4 technical report. arXiv preprintOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 10\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022. 10\n\nRadiology objects in context (roco): a multimodal image dataset. Obioma Pelka, Sven Koitka, Johannes R\u00fcckert, Felix Nensa, Christoph M Friedrich, MICCAI Workshop on Large-scale Annotation of Biomedical Data and Expert Label Synthesis (LABELS. SpringerObioma Pelka, Sven Koitka, Johannes R\u00fcckert, Felix Nensa, and Christoph M Friedrich. Radiology objects in context (roco): a multimodal image dataset. In MICCAI Workshop on Large-scale Annotation of Biomedical Data and Expert Label Synthesis (LABELS) 2018, pages 180-189. Springer, 2018. 8\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLRInternational conference on machine learning. 79Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 7, 9\n\nPubmed central: The genbank of the published literature. J Richard, Roberts, National Acad Sciences. 984Richard J Roberts. Pubmed central: The genbank of the published literature. volume 98, pages 381-382. National Acad Sciences, 2001. 4\n\nHyung Won Chung. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, arXiv:2212.13138Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen PfohlarXiv preprintet al. Large language models encode clinical knowledgeKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022. 1\n\nMedicat: A dataset of medical images, captions, and textual references. Sanjay Subramanian, Findings of EMNLP. Sanjay Subramanian et al. Medicat: A dataset of medical images, captions, and textual references. In Findings of EMNLP, 2020. 8\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023. 10Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 10\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. 79arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 5, 7, 9\n\nChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases. Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M Summers, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly- supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2097-2106, 2017. 2\n\nPmc-llama: Further finetuning llama on medical papers. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie, arXiv:2304.1445479arXiv preprintChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023. 7, 9\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, arXiv:2205.01068Opt: Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 9\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.10592arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 10\n", "annotations": {"author": "[{\"end\":147,\"start\":76},{\"end\":215,\"start\":148},{\"end\":260,\"start\":216},{\"end\":306,\"start\":261},{\"end\":373,\"start\":307},{\"end\":444,\"start\":374},{\"end\":512,\"start\":445}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":84},{\"end\":157,\"start\":155},{\"end\":227,\"start\":223},{\"end\":273,\"start\":270},{\"end\":315,\"start\":310},{\"end\":386,\"start\":382},{\"end\":454,\"start\":451}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":154,\"start\":148},{\"end\":222,\"start\":216},{\"end\":269,\"start\":261},{\"end\":309,\"start\":307},{\"end\":381,\"start\":374},{\"end\":450,\"start\":445}]", "author_affiliation": "[{\"end\":121,\"start\":91},{\"end\":146,\"start\":123},{\"end\":189,\"start\":159},{\"end\":214,\"start\":191},{\"end\":259,\"start\":229},{\"end\":305,\"start\":275},{\"end\":347,\"start\":317},{\"end\":372,\"start\":349},{\"end\":418,\"start\":388},{\"end\":443,\"start\":420},{\"end\":486,\"start\":456},{\"end\":511,\"start\":488}]", "title": "[{\"end\":73,\"start\":1},{\"end\":585,\"start\":513}]", "venue": null, "abstract": "[{\"end\":1602,\"start\":587}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1667,\"start\":1664},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1677,\"start\":1674},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1689,\"start\":1685},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2109,\"start\":2105},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2112,\"start\":2109},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2115,\"start\":2112},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2118,\"start\":2115},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2586,\"start\":2582},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2616,\"start\":2612},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2619,\"start\":2616},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2621,\"start\":2619},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2624,\"start\":2621},{\"end\":3393,\"start\":3386},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3438,\"start\":3434},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3489,\"start\":3485},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3525,\"start\":3521},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3545,\"start\":3541},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3563,\"start\":3560},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3581,\"start\":3577},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3593,\"start\":3589},{\"end\":3617,\"start\":3616},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3639,\"start\":3636},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3653,\"start\":3649},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4698,\"start\":4694},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5187,\"start\":5183},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5202,\"start\":5198},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6006,\"start\":6002},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6021,\"start\":6017},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9693,\"start\":9689},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12408,\"start\":12404},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12475,\"start\":12471},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13138,\"start\":13134},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13283,\"start\":13279},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15431,\"start\":15427},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16155,\"start\":16151},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17838,\"start\":17834},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17853,\"start\":17849},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17863,\"start\":17860},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17878,\"start\":17874},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17893,\"start\":17889},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19692,\"start\":19688},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19928,\"start\":19924},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20212,\"start\":20208},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20377,\"start\":20373},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20946,\"start\":20942},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24970,\"start\":24966},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24972,\"start\":24970},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25300,\"start\":25296},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26910,\"start\":26906},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26913,\"start\":26910},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26915,\"start\":26913},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27072,\"start\":27068},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27164,\"start\":27161},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27294,\"start\":27290},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27702,\"start\":27698},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28045,\"start\":28041},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28102,\"start\":28099},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28116,\"start\":28112},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28132,\"start\":28128},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29149,\"start\":29145},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29409,\"start\":29406},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32040,\"start\":32037}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32507,\"start\":32414},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32610,\"start\":32508},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33118,\"start\":32611},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34444,\"start\":33119},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35321,\"start\":34445},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35871,\"start\":35322},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35967,\"start\":35872},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36553,\"start\":35968},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36960,\"start\":36554},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37887,\"start\":36961},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40272,\"start\":37888}]", "paragraph": "[{\"end\":2360,\"start\":1618},{\"end\":3857,\"start\":2362},{\"end\":4412,\"start\":3859},{\"end\":5485,\"start\":4414},{\"end\":6249,\"start\":5487},{\"end\":6663,\"start\":6260},{\"end\":7018,\"start\":6694},{\"end\":7360,\"start\":7042},{\"end\":7783,\"start\":7456},{\"end\":8187,\"start\":7785},{\"end\":8624,\"start\":8189},{\"end\":8674,\"start\":8640},{\"end\":9119,\"start\":8676},{\"end\":9419,\"start\":9136},{\"end\":10208,\"start\":9434},{\"end\":10924,\"start\":10210},{\"end\":11461,\"start\":10926},{\"end\":11537,\"start\":11476},{\"end\":11821,\"start\":11539},{\"end\":12476,\"start\":11823},{\"end\":13098,\"start\":12500},{\"end\":13774,\"start\":13100},{\"end\":15081,\"start\":13776},{\"end\":15619,\"start\":15083},{\"end\":16743,\"start\":15621},{\"end\":17499,\"start\":16745},{\"end\":17505,\"start\":17501},{\"end\":17728,\"start\":17521},{\"end\":18293,\"start\":17730},{\"end\":18565,\"start\":18313},{\"end\":19259,\"start\":18567},{\"end\":19660,\"start\":19285},{\"end\":19912,\"start\":19682},{\"end\":20195,\"start\":19914},{\"end\":20556,\"start\":20197},{\"end\":20917,\"start\":20558},{\"end\":21208,\"start\":20937},{\"end\":21571,\"start\":21226},{\"end\":22059,\"start\":21583},{\"end\":22746,\"start\":22095},{\"end\":23368,\"start\":22748},{\"end\":23921,\"start\":23370},{\"end\":24220,\"start\":23946},{\"end\":24795,\"start\":24222},{\"end\":25163,\"start\":24797},{\"end\":26191,\"start\":25165},{\"end\":26770,\"start\":26193},{\"end\":27559,\"start\":26788},{\"end\":28310,\"start\":27561},{\"end\":29067,\"start\":28325},{\"end\":29497,\"start\":29096},{\"end\":30512,\"start\":29517},{\"end\":30574,\"start\":30529},{\"end\":30995,\"start\":30576},{\"end\":31394,\"start\":30997},{\"end\":32140,\"start\":31396},{\"end\":32413,\"start\":32142}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7455,\"start\":7361},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8639,\"start\":8625}]", "table_ref": "[{\"end\":3240,\"start\":3233}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1616,\"start\":1604},{\"attributes\":{\"n\":\"2\"},\"end\":6258,\"start\":6252},{\"end\":6692,\"start\":6666},{\"attributes\":{\"n\":\"2.1\"},\"end\":7040,\"start\":7021},{\"attributes\":{\"n\":\"2.2\"},\"end\":9134,\"start\":9122},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":9432,\"start\":9422},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":11474,\"start\":11464},{\"attributes\":{\"n\":\"3\"},\"end\":12498,\"start\":12479},{\"attributes\":{\"n\":\"4\"},\"end\":17519,\"start\":17508},{\"attributes\":{\"n\":\"4.2\"},\"end\":18311,\"start\":18296},{\"attributes\":{\"n\":\"4.3\"},\"end\":19283,\"start\":19262},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":19680,\"start\":19663},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":20935,\"start\":20920},{\"end\":21224,\"start\":21211},{\"attributes\":{\"n\":\"5\"},\"end\":21581,\"start\":21574},{\"attributes\":{\"n\":\"5.1\"},\"end\":22093,\"start\":22062},{\"attributes\":{\"n\":\"5.2\"},\"end\":23944,\"start\":23924},{\"attributes\":{\"n\":\"6\"},\"end\":26786,\"start\":26773},{\"attributes\":{\"n\":\"7\"},\"end\":28323,\"start\":28313},{\"end\":29094,\"start\":29070},{\"end\":29515,\"start\":29500},{\"end\":30527,\"start\":30515},{\"end\":32425,\"start\":32415},{\"end\":32519,\"start\":32509},{\"end\":33130,\"start\":33120},{\"end\":35333,\"start\":35323},{\"end\":35893,\"start\":35873},{\"end\":36971,\"start\":36962},{\"end\":37898,\"start\":37889}]", "table": "[{\"end\":36553,\"start\":36267},{\"end\":36960,\"start\":36912},{\"end\":37887,\"start\":37345},{\"end\":40272,\"start\":38169}]", "figure_caption": "[{\"end\":32507,\"start\":32427},{\"end\":32610,\"start\":32521},{\"end\":33118,\"start\":32613},{\"end\":34444,\"start\":33132},{\"end\":35321,\"start\":34447},{\"end\":35871,\"start\":35335},{\"end\":35967,\"start\":35896},{\"end\":36267,\"start\":35970},{\"end\":36912,\"start\":36556},{\"end\":37345,\"start\":36973},{\"end\":38169,\"start\":37900}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4946,\"start\":4939},{\"end\":6738,\"start\":6730},{\"end\":9225,\"start\":9216},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16948,\"start\":16943},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31730,\"start\":31725}]", "bib_author_first_name": "[{\"end\":41854,\"start\":41850},{\"end\":41885,\"start\":41878},{\"end\":41902,\"start\":41895},{\"end\":41912,\"start\":41908},{\"end\":41924,\"start\":41920},{\"end\":41936,\"start\":41931},{\"end\":41951,\"start\":41945},{\"end\":41967,\"start\":41958},{\"end\":41983,\"start\":41976},{\"end\":42395,\"start\":42388},{\"end\":42413,\"start\":42407},{\"end\":42430,\"start\":42422},{\"end\":42444,\"start\":42438},{\"end\":42462,\"start\":42455},{\"end\":42484,\"start\":42479},{\"end\":42510,\"start\":42504},{\"end\":42524,\"start\":42520},{\"end\":42538,\"start\":42532},{\"end\":42540,\"start\":42539},{\"end\":42861,\"start\":42857},{\"end\":42877,\"start\":42872},{\"end\":42889,\"start\":42883},{\"end\":42903,\"start\":42899},{\"end\":42917,\"start\":42912},{\"end\":42933,\"start\":42926},{\"end\":42946,\"start\":42939},{\"end\":42963,\"start\":42956},{\"end\":42977,\"start\":42972},{\"end\":42990,\"start\":42985},{\"end\":43345,\"start\":43341},{\"end\":43364,\"start\":43358},{\"end\":43379,\"start\":43375},{\"end\":43397,\"start\":43396},{\"end\":43412,\"start\":43405},{\"end\":44003,\"start\":43996},{\"end\":44015,\"start\":44010},{\"end\":44027,\"start\":44020},{\"end\":44036,\"start\":44032},{\"end\":44049,\"start\":44042},{\"end\":44059,\"start\":44054},{\"end\":44074,\"start\":44065},{\"end\":44504,\"start\":44497},{\"end\":44520,\"start\":44513},{\"end\":44527,\"start\":44525},{\"end\":44537,\"start\":44533},{\"end\":44553,\"start\":44545},{\"end\":44561,\"start\":44558},{\"end\":44576,\"start\":44569},{\"end\":44590,\"start\":44584},{\"end\":44606,\"start\":44599},{\"end\":44621,\"start\":44615},{\"end\":44623,\"start\":44622},{\"end\":44637,\"start\":44634},{\"end\":44650,\"start\":44646},{\"end\":44652,\"start\":44651},{\"end\":44974,\"start\":44965},{\"end\":44992,\"start\":44986},{\"end\":45006,\"start\":45001},{\"end\":45022,\"start\":45015},{\"end\":45036,\"start\":45030},{\"end\":45049,\"start\":45045},{\"end\":45063,\"start\":45059},{\"end\":45090,\"start\":45083},{\"end\":45107,\"start\":45098},{\"end\":45529,\"start\":45522},{\"end\":45540,\"start\":45536},{\"end\":45917,\"start\":45914},{\"end\":45929,\"start\":45923},{\"end\":45943,\"start\":45940},{\"end\":45959,\"start\":45951},{\"end\":45975,\"start\":45969},{\"end\":45990,\"start\":45983},{\"end\":46004,\"start\":45999},{\"end\":46018,\"start\":46012},{\"end\":46028,\"start\":46023},{\"end\":46039,\"start\":46036},{\"end\":46056,\"start\":46051},{\"end\":46072,\"start\":46066},{\"end\":46543,\"start\":46541},{\"end\":46554,\"start\":46548},{\"end\":46564,\"start\":46561},{\"end\":46579,\"start\":46572},{\"end\":46592,\"start\":46587},{\"end\":46610,\"start\":46602},{\"end\":46623,\"start\":46616},{\"end\":46641,\"start\":46633},{\"end\":46654,\"start\":46647},{\"end\":47060,\"start\":47054},{\"end\":47071,\"start\":47065},{\"end\":47086,\"start\":47079},{\"end\":47096,\"start\":47092},{\"end\":47110,\"start\":47103},{\"end\":47377,\"start\":47375},{\"end\":47389,\"start\":47383},{\"end\":47401,\"start\":47395},{\"end\":47421,\"start\":47413},{\"end\":47433,\"start\":47428},{\"end\":47445,\"start\":47440},{\"end\":47781,\"start\":47780},{\"end\":47799,\"start\":47794},{\"end\":47801,\"start\":47800},{\"end\":47817,\"start\":47809},{\"end\":47832,\"start\":47825},{\"end\":48220,\"start\":48215},{\"end\":48242,\"start\":48235},{\"end\":48255,\"start\":48250},{\"end\":48275,\"start\":48263},{\"end\":48291,\"start\":48283},{\"end\":48313,\"start\":48306},{\"end\":48335,\"start\":48328},{\"end\":48353,\"start\":48348},{\"end\":48720,\"start\":48716},{\"end\":48740,\"start\":48731},{\"end\":48760,\"start\":48750},{\"end\":48776,\"start\":48770},{\"end\":48790,\"start\":48787},{\"end\":48801,\"start\":48795},{\"end\":48814,\"start\":48810},{\"end\":48830,\"start\":48820},{\"end\":48848,\"start\":48843},{\"end\":48859,\"start\":48854},{\"end\":48866,\"start\":48860},{\"end\":49428,\"start\":49427},{\"end\":49444,\"start\":49438},{\"end\":49458,\"start\":49451},{\"end\":49476,\"start\":49469},{\"end\":49493,\"start\":49488},{\"end\":49496,\"start\":49494},{\"end\":49512,\"start\":49505},{\"end\":49524,\"start\":49519},{\"end\":49539,\"start\":49534},{\"end\":49556,\"start\":49550},{\"end\":49571,\"start\":49566},{\"end\":50092,\"start\":50091},{\"end\":50106,\"start\":50100},{\"end\":50432,\"start\":50426},{\"end\":50443,\"start\":50437},{\"end\":50454,\"start\":50448},{\"end\":50471,\"start\":50465},{\"end\":50806,\"start\":50798},{\"end\":50818,\"start\":50812},{\"end\":50832,\"start\":50825},{\"end\":50846,\"start\":50840},{\"end\":50853,\"start\":50851},{\"end\":50868,\"start\":50861},{\"end\":50880,\"start\":50875},{\"end\":51200,\"start\":51193},{\"end\":51213,\"start\":51206},{\"end\":51227,\"start\":51221},{\"end\":51238,\"start\":51233},{\"end\":51254,\"start\":51244},{\"end\":51266,\"start\":51264},{\"end\":51659,\"start\":51657},{\"end\":51672,\"start\":51665},{\"end\":51688,\"start\":51679},{\"end\":52124,\"start\":52122},{\"end\":52137,\"start\":52130},{\"end\":52146,\"start\":52144},{\"end\":52154,\"start\":52151},{\"end\":52162,\"start\":52159},{\"end\":52178,\"start\":52169},{\"end\":52534,\"start\":52530},{\"end\":52552,\"start\":52547},{\"end\":52823,\"start\":52813},{\"end\":52844,\"start\":52843},{\"end\":52856,\"start\":52851},{\"end\":52870,\"start\":52865},{\"end\":52882,\"start\":52875},{\"end\":53284,\"start\":53278},{\"end\":53299,\"start\":53291},{\"end\":53311,\"start\":53306},{\"end\":53317,\"start\":53312},{\"end\":53332,\"start\":53328},{\"end\":53347,\"start\":53343},{\"end\":53779,\"start\":53775},{\"end\":53795,\"start\":53788},{\"end\":53802,\"start\":53800},{\"end\":53815,\"start\":53810},{\"end\":53832,\"start\":53825},{\"end\":53851,\"start\":53845},{\"end\":53866,\"start\":53861},{\"end\":53882,\"start\":53874},{\"end\":53900,\"start\":53892},{\"end\":53912,\"start\":53908},{\"end\":54338,\"start\":54332},{\"end\":54350,\"start\":54346},{\"end\":54367,\"start\":54359},{\"end\":54382,\"start\":54377},{\"end\":54399,\"start\":54390},{\"end\":54401,\"start\":54400},{\"end\":54883,\"start\":54879},{\"end\":54897,\"start\":54893},{\"end\":54902,\"start\":54898},{\"end\":54913,\"start\":54908},{\"end\":54929,\"start\":54923},{\"end\":54945,\"start\":54938},{\"end\":54959,\"start\":54951},{\"end\":54975,\"start\":54969},{\"end\":54990,\"start\":54984},{\"end\":55005,\"start\":54999},{\"end\":55019,\"start\":55015},{\"end\":55447,\"start\":55446},{\"end\":55650,\"start\":55645},{\"end\":55669,\"start\":55660},{\"end\":55680,\"start\":55677},{\"end\":55689,\"start\":55685},{\"end\":55704,\"start\":55699},{\"end\":56180,\"start\":56174},{\"end\":56402,\"start\":56397},{\"end\":56416,\"start\":56410},{\"end\":56434,\"start\":56428},{\"end\":56446,\"start\":56442},{\"end\":56462,\"start\":56455},{\"end\":56473,\"start\":56467},{\"end\":56489,\"start\":56484},{\"end\":56506,\"start\":56497},{\"end\":56508,\"start\":56507},{\"end\":56771,\"start\":56767},{\"end\":56788,\"start\":56781},{\"end\":56804,\"start\":56797},{\"end\":56820,\"start\":56814},{\"end\":56841,\"start\":56831},{\"end\":56859,\"start\":56851},{\"end\":56874,\"start\":56869},{\"end\":56897,\"start\":56893},{\"end\":56911,\"start\":56905},{\"end\":57431,\"start\":57423},{\"end\":57443,\"start\":57438},{\"end\":57452,\"start\":57450},{\"end\":57464,\"start\":57457},{\"end\":57481,\"start\":57469},{\"end\":57499,\"start\":57491},{\"end\":58054,\"start\":58048},{\"end\":58066,\"start\":58059},{\"end\":58076,\"start\":58074},{\"end\":58091,\"start\":58084},{\"end\":58103,\"start\":58098},{\"end\":58311,\"start\":58306},{\"end\":58326,\"start\":58319},{\"end\":58340,\"start\":58335},{\"end\":58353,\"start\":58348},{\"end\":58367,\"start\":58363},{\"end\":58381,\"start\":58374},{\"end\":58399,\"start\":58388},{\"end\":58411,\"start\":58407},{\"end\":58422,\"start\":58418},{\"end\":58429,\"start\":58427},{\"end\":58449,\"start\":58444},{\"end\":58464,\"start\":58460},{\"end\":58473,\"start\":58470},{\"end\":58488,\"start\":58484},{\"end\":58504,\"start\":58498},{\"end\":58517,\"start\":58512},{\"end\":58537,\"start\":58531},{\"end\":58553,\"start\":58547},{\"end\":58564,\"start\":58560},{\"end\":59121,\"start\":59116},{\"end\":59130,\"start\":59127},{\"end\":59145,\"start\":59137},{\"end\":59157,\"start\":59152},{\"end\":59169,\"start\":59162}]", "bib_author_last_name": "[{\"end\":41876,\"start\":41855},{\"end\":41893,\"start\":41886},{\"end\":41906,\"start\":41903},{\"end\":41918,\"start\":41913},{\"end\":41929,\"start\":41925},{\"end\":41943,\"start\":41937},{\"end\":41956,\"start\":41952},{\"end\":41974,\"start\":41968},{\"end\":41992,\"start\":41984},{\"end\":42002,\"start\":41994},{\"end\":42405,\"start\":42396},{\"end\":42420,\"start\":42414},{\"end\":42436,\"start\":42431},{\"end\":42453,\"start\":42445},{\"end\":42477,\"start\":42463},{\"end\":42502,\"start\":42485},{\"end\":42518,\"start\":42511},{\"end\":42530,\"start\":42525},{\"end\":42552,\"start\":42541},{\"end\":42561,\"start\":42554},{\"end\":42870,\"start\":42862},{\"end\":42881,\"start\":42878},{\"end\":42897,\"start\":42890},{\"end\":42910,\"start\":42904},{\"end\":42924,\"start\":42918},{\"end\":42937,\"start\":42934},{\"end\":42954,\"start\":42947},{\"end\":42970,\"start\":42964},{\"end\":42983,\"start\":42978},{\"end\":42997,\"start\":42991},{\"end\":43356,\"start\":43346},{\"end\":43373,\"start\":43365},{\"end\":43394,\"start\":43380},{\"end\":43403,\"start\":43398},{\"end\":43418,\"start\":43413},{\"end\":43426,\"start\":43420},{\"end\":44008,\"start\":44004},{\"end\":44018,\"start\":44016},{\"end\":44030,\"start\":44028},{\"end\":44040,\"start\":44037},{\"end\":44052,\"start\":44050},{\"end\":44063,\"start\":44060},{\"end\":44080,\"start\":44075},{\"end\":44511,\"start\":44505},{\"end\":44523,\"start\":44521},{\"end\":44531,\"start\":44528},{\"end\":44543,\"start\":44538},{\"end\":44556,\"start\":44554},{\"end\":44567,\"start\":44562},{\"end\":44582,\"start\":44577},{\"end\":44597,\"start\":44591},{\"end\":44613,\"start\":44607},{\"end\":44632,\"start\":44624},{\"end\":44644,\"start\":44638},{\"end\":44657,\"start\":44653},{\"end\":44984,\"start\":44975},{\"end\":44999,\"start\":44993},{\"end\":45013,\"start\":45007},{\"end\":45028,\"start\":45023},{\"end\":45043,\"start\":45037},{\"end\":45057,\"start\":45050},{\"end\":45070,\"start\":45064},{\"end\":45081,\"start\":45072},{\"end\":45096,\"start\":45091},{\"end\":45114,\"start\":45108},{\"end\":45124,\"start\":45116},{\"end\":45534,\"start\":45530},{\"end\":45546,\"start\":45541},{\"end\":45921,\"start\":45918},{\"end\":45938,\"start\":45930},{\"end\":45949,\"start\":45944},{\"end\":45967,\"start\":45960},{\"end\":45981,\"start\":45976},{\"end\":45997,\"start\":45991},{\"end\":46010,\"start\":46005},{\"end\":46021,\"start\":46019},{\"end\":46034,\"start\":46029},{\"end\":46049,\"start\":46040},{\"end\":46064,\"start\":46057},{\"end\":46078,\"start\":46073},{\"end\":46546,\"start\":46544},{\"end\":46559,\"start\":46555},{\"end\":46570,\"start\":46565},{\"end\":46585,\"start\":46580},{\"end\":46600,\"start\":46593},{\"end\":46614,\"start\":46611},{\"end\":46631,\"start\":46624},{\"end\":46645,\"start\":46642},{\"end\":46659,\"start\":46655},{\"end\":47063,\"start\":47061},{\"end\":47077,\"start\":47072},{\"end\":47090,\"start\":47087},{\"end\":47101,\"start\":47097},{\"end\":47114,\"start\":47111},{\"end\":47381,\"start\":47378},{\"end\":47393,\"start\":47390},{\"end\":47411,\"start\":47402},{\"end\":47426,\"start\":47422},{\"end\":47438,\"start\":47434},{\"end\":47455,\"start\":47446},{\"end\":47792,\"start\":47782},{\"end\":47807,\"start\":47802},{\"end\":47823,\"start\":47818},{\"end\":47840,\"start\":47833},{\"end\":47850,\"start\":47842},{\"end\":48233,\"start\":48221},{\"end\":48248,\"start\":48243},{\"end\":48261,\"start\":48256},{\"end\":48281,\"start\":48276},{\"end\":48297,\"start\":48292},{\"end\":48304,\"start\":48299},{\"end\":48326,\"start\":48314},{\"end\":48346,\"start\":48336},{\"end\":48359,\"start\":48354},{\"end\":48366,\"start\":48361},{\"end\":48729,\"start\":48721},{\"end\":48748,\"start\":48741},{\"end\":48768,\"start\":48761},{\"end\":48785,\"start\":48777},{\"end\":48793,\"start\":48791},{\"end\":48808,\"start\":48802},{\"end\":48818,\"start\":48815},{\"end\":48841,\"start\":48831},{\"end\":48852,\"start\":48849},{\"end\":48873,\"start\":48867},{\"end\":49436,\"start\":49429},{\"end\":49449,\"start\":49445},{\"end\":49467,\"start\":49459},{\"end\":49486,\"start\":49477},{\"end\":49503,\"start\":49497},{\"end\":49517,\"start\":49513},{\"end\":49532,\"start\":49525},{\"end\":49548,\"start\":49540},{\"end\":49564,\"start\":49557},{\"end\":49584,\"start\":49572},{\"end\":49593,\"start\":49586},{\"end\":50098,\"start\":50093},{\"end\":50110,\"start\":50107},{\"end\":50117,\"start\":50112},{\"end\":50435,\"start\":50433},{\"end\":50446,\"start\":50444},{\"end\":50463,\"start\":50455},{\"end\":50475,\"start\":50472},{\"end\":50810,\"start\":50807},{\"end\":50823,\"start\":50819},{\"end\":50838,\"start\":50833},{\"end\":50849,\"start\":50847},{\"end\":50859,\"start\":50854},{\"end\":50873,\"start\":50869},{\"end\":50884,\"start\":50881},{\"end\":51204,\"start\":51201},{\"end\":51219,\"start\":51214},{\"end\":51231,\"start\":51228},{\"end\":51242,\"start\":51239},{\"end\":51262,\"start\":51255},{\"end\":51269,\"start\":51267},{\"end\":51663,\"start\":51660},{\"end\":51677,\"start\":51673},{\"end\":51691,\"start\":51689},{\"end\":52128,\"start\":52125},{\"end\":52142,\"start\":52138},{\"end\":52149,\"start\":52147},{\"end\":52157,\"start\":52155},{\"end\":52167,\"start\":52163},{\"end\":52181,\"start\":52179},{\"end\":52545,\"start\":52535},{\"end\":52559,\"start\":52553},{\"end\":52837,\"start\":52824},{\"end\":52841,\"start\":52839},{\"end\":52849,\"start\":52845},{\"end\":52863,\"start\":52857},{\"end\":52873,\"start\":52871},{\"end\":52891,\"start\":52883},{\"end\":52897,\"start\":52893},{\"end\":53289,\"start\":53285},{\"end\":53304,\"start\":53300},{\"end\":53326,\"start\":53318},{\"end\":53341,\"start\":53333},{\"end\":53355,\"start\":53348},{\"end\":53575,\"start\":53569},{\"end\":53786,\"start\":53780},{\"end\":53798,\"start\":53796},{\"end\":53808,\"start\":53803},{\"end\":53823,\"start\":53816},{\"end\":53843,\"start\":53833},{\"end\":53859,\"start\":53852},{\"end\":53872,\"start\":53867},{\"end\":53890,\"start\":53883},{\"end\":53906,\"start\":53901},{\"end\":53916,\"start\":53913},{\"end\":54344,\"start\":54339},{\"end\":54357,\"start\":54351},{\"end\":54375,\"start\":54368},{\"end\":54388,\"start\":54383},{\"end\":54411,\"start\":54402},{\"end\":54891,\"start\":54884},{\"end\":54906,\"start\":54903},{\"end\":54921,\"start\":54914},{\"end\":54936,\"start\":54930},{\"end\":54949,\"start\":54946},{\"end\":54967,\"start\":54960},{\"end\":54982,\"start\":54976},{\"end\":54997,\"start\":54991},{\"end\":55013,\"start\":55006},{\"end\":55025,\"start\":55020},{\"end\":55455,\"start\":55448},{\"end\":55464,\"start\":55457},{\"end\":55658,\"start\":55651},{\"end\":55675,\"start\":55670},{\"end\":55683,\"start\":55681},{\"end\":55697,\"start\":55690},{\"end\":55708,\"start\":55705},{\"end\":56192,\"start\":56181},{\"end\":56408,\"start\":56403},{\"end\":56426,\"start\":56417},{\"end\":56440,\"start\":56435},{\"end\":56453,\"start\":56447},{\"end\":56465,\"start\":56463},{\"end\":56482,\"start\":56474},{\"end\":56495,\"start\":56490},{\"end\":56518,\"start\":56509},{\"end\":56779,\"start\":56772},{\"end\":56795,\"start\":56789},{\"end\":56812,\"start\":56805},{\"end\":56829,\"start\":56821},{\"end\":56849,\"start\":56842},{\"end\":56867,\"start\":56860},{\"end\":56891,\"start\":56875},{\"end\":56903,\"start\":56898},{\"end\":56918,\"start\":56912},{\"end\":56925,\"start\":56920},{\"end\":57436,\"start\":57432},{\"end\":57448,\"start\":57444},{\"end\":57455,\"start\":57453},{\"end\":57467,\"start\":57465},{\"end\":57489,\"start\":57482},{\"end\":57507,\"start\":57500},{\"end\":58057,\"start\":58055},{\"end\":58072,\"start\":58067},{\"end\":58082,\"start\":58077},{\"end\":58096,\"start\":58092},{\"end\":58107,\"start\":58104},{\"end\":58317,\"start\":58312},{\"end\":58333,\"start\":58327},{\"end\":58346,\"start\":58341},{\"end\":58361,\"start\":58354},{\"end\":58372,\"start\":58368},{\"end\":58386,\"start\":58382},{\"end\":58405,\"start\":58400},{\"end\":58416,\"start\":58412},{\"end\":58425,\"start\":58423},{\"end\":58442,\"start\":58430},{\"end\":58458,\"start\":58450},{\"end\":58468,\"start\":58465},{\"end\":58482,\"start\":58474},{\"end\":58496,\"start\":58489},{\"end\":58510,\"start\":58505},{\"end\":58529,\"start\":58518},{\"end\":58545,\"start\":58538},{\"end\":58558,\"start\":58554},{\"end\":58576,\"start\":58565},{\"end\":59125,\"start\":59122},{\"end\":59135,\"start\":59131},{\"end\":59150,\"start\":59146},{\"end\":59160,\"start\":59158},{\"end\":59179,\"start\":59170}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":41791,\"start\":41684},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":248476411},\"end\":42350,\"start\":41793},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":235390655},\"end\":42853,\"start\":42352},{\"attributes\":{\"id\":\"b3\"},\"end\":43227,\"start\":42855},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":225073930},\"end\":43916,\"start\":43229},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":252280670},\"end\":44420,\"start\":43918},{\"attributes\":{\"id\":\"b6\"},\"end\":44963,\"start\":44422},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b7\"},\"end\":45451,\"start\":44965},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202734386},\"end\":45912,\"start\":45453},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b9\"},\"end\":46452,\"start\":45914},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220919723},\"end\":46997,\"start\":46454},{\"attributes\":{\"id\":\"b11\"},\"end\":47263,\"start\":46999},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":221970190},\"end\":47717,\"start\":47265},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7394670},\"end\":48143,\"start\":47719},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":210839730},\"end\":48677,\"start\":48145},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220059896},\"end\":49319,\"start\":48679},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":254876189},\"end\":49943,\"start\":49321},{\"attributes\":{\"id\":\"b17\"},\"end\":50321,\"start\":49945},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b18\"},\"end\":50718,\"start\":50323},{\"attributes\":{\"doi\":\"arXiv:2303.07240\",\"id\":\"b19\"},\"end\":51146,\"start\":50720},{\"attributes\":{\"doi\":\"arXiv:2111.10056\",\"id\":\"b20\"},\"end\":51533,\"start\":51148},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":238207364},\"end\":52025,\"start\":51535},{\"attributes\":{\"id\":\"b22\"},\"end\":52526,\"start\":52027},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b23\"},\"end\":52746,\"start\":52528},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202889111},\"end\":53223,\"start\":52748},{\"attributes\":{\"doi\":\"arXiv:2303.13375\",\"id\":\"b25\"},\"end\":53565,\"start\":53225},{\"attributes\":{\"doi\":\"arXiv:2303.08774\",\"id\":\"b26\"},\"end\":53704,\"start\":53567},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":246426909},\"end\":54265,\"start\":53706},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53087891},\"end\":54806,\"start\":54267},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b29\",\"matched_paper_id\":231591445},\"end\":55387,\"start\":54808},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":33355673},\"end\":55626,\"start\":55389},{\"attributes\":{\"doi\":\"arXiv:2212.13138\",\"id\":\"b31\"},\"end\":56100,\"start\":55628},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":222310689},\"end\":56340,\"start\":56102},{\"attributes\":{\"doi\":\"2023. 10\",\"id\":\"b33\"},\"end\":56765,\"start\":56342},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b34\"},\"end\":57278,\"start\":56767},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8945673},\"end\":57991,\"start\":57280},{\"attributes\":{\"doi\":\"arXiv:2304.14454\",\"id\":\"b36\"},\"end\":58304,\"start\":57993},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b37\"},\"end\":59026,\"start\":58306},{\"attributes\":{\"doi\":\"arXiv:2304.10592\",\"id\":\"b38\"},\"end\":59411,\"start\":59028}]", "bib_title": "[{\"end\":41848,\"start\":41793},{\"end\":42386,\"start\":42352},{\"end\":43339,\"start\":43229},{\"end\":43994,\"start\":43918},{\"end\":45520,\"start\":45453},{\"end\":46539,\"start\":46454},{\"end\":47373,\"start\":47265},{\"end\":47778,\"start\":47719},{\"end\":48213,\"start\":48145},{\"end\":48714,\"start\":48679},{\"end\":49425,\"start\":49321},{\"end\":51655,\"start\":51535},{\"end\":52120,\"start\":52027},{\"end\":52811,\"start\":52748},{\"end\":53773,\"start\":53706},{\"end\":54330,\"start\":54267},{\"end\":54877,\"start\":54808},{\"end\":55444,\"start\":55389},{\"end\":56172,\"start\":56102},{\"end\":57421,\"start\":57280}]", "bib_author": "[{\"end\":41878,\"start\":41850},{\"end\":41895,\"start\":41878},{\"end\":41908,\"start\":41895},{\"end\":41920,\"start\":41908},{\"end\":41931,\"start\":41920},{\"end\":41945,\"start\":41931},{\"end\":41958,\"start\":41945},{\"end\":41976,\"start\":41958},{\"end\":41994,\"start\":41976},{\"end\":42004,\"start\":41994},{\"end\":42407,\"start\":42388},{\"end\":42422,\"start\":42407},{\"end\":42438,\"start\":42422},{\"end\":42455,\"start\":42438},{\"end\":42479,\"start\":42455},{\"end\":42504,\"start\":42479},{\"end\":42520,\"start\":42504},{\"end\":42532,\"start\":42520},{\"end\":42554,\"start\":42532},{\"end\":42563,\"start\":42554},{\"end\":42872,\"start\":42857},{\"end\":42883,\"start\":42872},{\"end\":42899,\"start\":42883},{\"end\":42912,\"start\":42899},{\"end\":42926,\"start\":42912},{\"end\":42939,\"start\":42926},{\"end\":42956,\"start\":42939},{\"end\":42972,\"start\":42956},{\"end\":42985,\"start\":42972},{\"end\":42999,\"start\":42985},{\"end\":43358,\"start\":43341},{\"end\":43375,\"start\":43358},{\"end\":43396,\"start\":43375},{\"end\":43405,\"start\":43396},{\"end\":43420,\"start\":43405},{\"end\":43428,\"start\":43420},{\"end\":44010,\"start\":43996},{\"end\":44020,\"start\":44010},{\"end\":44032,\"start\":44020},{\"end\":44042,\"start\":44032},{\"end\":44054,\"start\":44042},{\"end\":44065,\"start\":44054},{\"end\":44082,\"start\":44065},{\"end\":44513,\"start\":44497},{\"end\":44525,\"start\":44513},{\"end\":44533,\"start\":44525},{\"end\":44545,\"start\":44533},{\"end\":44558,\"start\":44545},{\"end\":44569,\"start\":44558},{\"end\":44584,\"start\":44569},{\"end\":44599,\"start\":44584},{\"end\":44615,\"start\":44599},{\"end\":44634,\"start\":44615},{\"end\":44646,\"start\":44634},{\"end\":44659,\"start\":44646},{\"end\":44986,\"start\":44965},{\"end\":45001,\"start\":44986},{\"end\":45015,\"start\":45001},{\"end\":45030,\"start\":45015},{\"end\":45045,\"start\":45030},{\"end\":45059,\"start\":45045},{\"end\":45072,\"start\":45059},{\"end\":45083,\"start\":45072},{\"end\":45098,\"start\":45083},{\"end\":45116,\"start\":45098},{\"end\":45126,\"start\":45116},{\"end\":45536,\"start\":45522},{\"end\":45548,\"start\":45536},{\"end\":45923,\"start\":45914},{\"end\":45940,\"start\":45923},{\"end\":45951,\"start\":45940},{\"end\":45969,\"start\":45951},{\"end\":45983,\"start\":45969},{\"end\":45999,\"start\":45983},{\"end\":46012,\"start\":45999},{\"end\":46023,\"start\":46012},{\"end\":46036,\"start\":46023},{\"end\":46051,\"start\":46036},{\"end\":46066,\"start\":46051},{\"end\":46080,\"start\":46066},{\"end\":46548,\"start\":46541},{\"end\":46561,\"start\":46548},{\"end\":46572,\"start\":46561},{\"end\":46587,\"start\":46572},{\"end\":46602,\"start\":46587},{\"end\":46616,\"start\":46602},{\"end\":46633,\"start\":46616},{\"end\":46647,\"start\":46633},{\"end\":46661,\"start\":46647},{\"end\":47065,\"start\":47054},{\"end\":47079,\"start\":47065},{\"end\":47092,\"start\":47079},{\"end\":47103,\"start\":47092},{\"end\":47116,\"start\":47103},{\"end\":47383,\"start\":47375},{\"end\":47395,\"start\":47383},{\"end\":47413,\"start\":47395},{\"end\":47428,\"start\":47413},{\"end\":47440,\"start\":47428},{\"end\":47457,\"start\":47440},{\"end\":47794,\"start\":47780},{\"end\":47809,\"start\":47794},{\"end\":47825,\"start\":47809},{\"end\":47842,\"start\":47825},{\"end\":47852,\"start\":47842},{\"end\":48235,\"start\":48215},{\"end\":48250,\"start\":48235},{\"end\":48263,\"start\":48250},{\"end\":48283,\"start\":48263},{\"end\":48299,\"start\":48283},{\"end\":48306,\"start\":48299},{\"end\":48328,\"start\":48306},{\"end\":48348,\"start\":48328},{\"end\":48361,\"start\":48348},{\"end\":48368,\"start\":48361},{\"end\":48731,\"start\":48716},{\"end\":48750,\"start\":48731},{\"end\":48770,\"start\":48750},{\"end\":48787,\"start\":48770},{\"end\":48795,\"start\":48787},{\"end\":48810,\"start\":48795},{\"end\":48820,\"start\":48810},{\"end\":48843,\"start\":48820},{\"end\":48854,\"start\":48843},{\"end\":48875,\"start\":48854},{\"end\":49438,\"start\":49427},{\"end\":49451,\"start\":49438},{\"end\":49469,\"start\":49451},{\"end\":49488,\"start\":49469},{\"end\":49505,\"start\":49488},{\"end\":49519,\"start\":49505},{\"end\":49534,\"start\":49519},{\"end\":49550,\"start\":49534},{\"end\":49566,\"start\":49550},{\"end\":49586,\"start\":49566},{\"end\":49595,\"start\":49586},{\"end\":50100,\"start\":50091},{\"end\":50112,\"start\":50100},{\"end\":50119,\"start\":50112},{\"end\":50437,\"start\":50426},{\"end\":50448,\"start\":50437},{\"end\":50465,\"start\":50448},{\"end\":50477,\"start\":50465},{\"end\":50812,\"start\":50798},{\"end\":50825,\"start\":50812},{\"end\":50840,\"start\":50825},{\"end\":50851,\"start\":50840},{\"end\":50861,\"start\":50851},{\"end\":50875,\"start\":50861},{\"end\":50886,\"start\":50875},{\"end\":51206,\"start\":51193},{\"end\":51221,\"start\":51206},{\"end\":51233,\"start\":51221},{\"end\":51244,\"start\":51233},{\"end\":51264,\"start\":51244},{\"end\":51271,\"start\":51264},{\"end\":51665,\"start\":51657},{\"end\":51679,\"start\":51665},{\"end\":51693,\"start\":51679},{\"end\":52130,\"start\":52122},{\"end\":52144,\"start\":52130},{\"end\":52151,\"start\":52144},{\"end\":52159,\"start\":52151},{\"end\":52169,\"start\":52159},{\"end\":52183,\"start\":52169},{\"end\":52547,\"start\":52530},{\"end\":52561,\"start\":52547},{\"end\":52839,\"start\":52813},{\"end\":52843,\"start\":52839},{\"end\":52851,\"start\":52843},{\"end\":52865,\"start\":52851},{\"end\":52875,\"start\":52865},{\"end\":52893,\"start\":52875},{\"end\":52899,\"start\":52893},{\"end\":53291,\"start\":53278},{\"end\":53306,\"start\":53291},{\"end\":53328,\"start\":53306},{\"end\":53343,\"start\":53328},{\"end\":53357,\"start\":53343},{\"end\":53577,\"start\":53569},{\"end\":53788,\"start\":53775},{\"end\":53800,\"start\":53788},{\"end\":53810,\"start\":53800},{\"end\":53825,\"start\":53810},{\"end\":53845,\"start\":53825},{\"end\":53861,\"start\":53845},{\"end\":53874,\"start\":53861},{\"end\":53892,\"start\":53874},{\"end\":53908,\"start\":53892},{\"end\":53918,\"start\":53908},{\"end\":54346,\"start\":54332},{\"end\":54359,\"start\":54346},{\"end\":54377,\"start\":54359},{\"end\":54390,\"start\":54377},{\"end\":54413,\"start\":54390},{\"end\":54893,\"start\":54879},{\"end\":54908,\"start\":54893},{\"end\":54923,\"start\":54908},{\"end\":54938,\"start\":54923},{\"end\":54951,\"start\":54938},{\"end\":54969,\"start\":54951},{\"end\":54984,\"start\":54969},{\"end\":54999,\"start\":54984},{\"end\":55015,\"start\":54999},{\"end\":55027,\"start\":55015},{\"end\":55457,\"start\":55446},{\"end\":55466,\"start\":55457},{\"end\":55660,\"start\":55645},{\"end\":55677,\"start\":55660},{\"end\":55685,\"start\":55677},{\"end\":55699,\"start\":55685},{\"end\":55710,\"start\":55699},{\"end\":56194,\"start\":56174},{\"end\":56410,\"start\":56397},{\"end\":56428,\"start\":56410},{\"end\":56442,\"start\":56428},{\"end\":56455,\"start\":56442},{\"end\":56467,\"start\":56455},{\"end\":56484,\"start\":56467},{\"end\":56497,\"start\":56484},{\"end\":56520,\"start\":56497},{\"end\":56781,\"start\":56767},{\"end\":56797,\"start\":56781},{\"end\":56814,\"start\":56797},{\"end\":56831,\"start\":56814},{\"end\":56851,\"start\":56831},{\"end\":56869,\"start\":56851},{\"end\":56893,\"start\":56869},{\"end\":56905,\"start\":56893},{\"end\":56920,\"start\":56905},{\"end\":56927,\"start\":56920},{\"end\":57438,\"start\":57423},{\"end\":57450,\"start\":57438},{\"end\":57457,\"start\":57450},{\"end\":57469,\"start\":57457},{\"end\":57491,\"start\":57469},{\"end\":57509,\"start\":57491},{\"end\":58059,\"start\":58048},{\"end\":58074,\"start\":58059},{\"end\":58084,\"start\":58074},{\"end\":58098,\"start\":58084},{\"end\":58109,\"start\":58098},{\"end\":58319,\"start\":58306},{\"end\":58335,\"start\":58319},{\"end\":58348,\"start\":58335},{\"end\":58363,\"start\":58348},{\"end\":58374,\"start\":58363},{\"end\":58388,\"start\":58374},{\"end\":58407,\"start\":58388},{\"end\":58418,\"start\":58407},{\"end\":58427,\"start\":58418},{\"end\":58444,\"start\":58427},{\"end\":58460,\"start\":58444},{\"end\":58470,\"start\":58460},{\"end\":58484,\"start\":58470},{\"end\":58498,\"start\":58484},{\"end\":58512,\"start\":58498},{\"end\":58531,\"start\":58512},{\"end\":58547,\"start\":58531},{\"end\":58560,\"start\":58547},{\"end\":58578,\"start\":58560},{\"end\":59127,\"start\":59116},{\"end\":59137,\"start\":59127},{\"end\":59152,\"start\":59137},{\"end\":59162,\"start\":59152},{\"end\":59181,\"start\":59162}]", "bib_venue": "[{\"end\":43587,\"start\":43516},{\"end\":45697,\"start\":45631},{\"end\":47905,\"start\":47887},{\"end\":49014,\"start\":48953},{\"end\":57650,\"start\":57588},{\"end\":41713,\"start\":41686},{\"end\":42053,\"start\":42004},{\"end\":42584,\"start\":42563},{\"end\":43514,\"start\":43428},{\"end\":44140,\"start\":44082},{\"end\":44495,\"start\":44422},{\"end\":45181,\"start\":45142},{\"end\":45629,\"start\":45548},{\"end\":46160,\"start\":46096},{\"end\":46705,\"start\":46661},{\"end\":47052,\"start\":46999},{\"end\":47473,\"start\":47457},{\"end\":47885,\"start\":47852},{\"end\":48390,\"start\":48368},{\"end\":48951,\"start\":48875},{\"end\":49614,\"start\":49595},{\"end\":50089,\"start\":49945},{\"end\":50424,\"start\":50323},{\"end\":50796,\"start\":50720},{\"end\":51191,\"start\":51148},{\"end\":51751,\"start\":51693},{\"end\":52250,\"start\":52183},{\"end\":52957,\"start\":52899},{\"end\":53276,\"start\":53225},{\"end\":53967,\"start\":53918},{\"end\":54508,\"start\":54413},{\"end\":55075,\"start\":55031},{\"end\":55488,\"start\":55466},{\"end\":55643,\"start\":55628},{\"end\":56211,\"start\":56194},{\"end\":56395,\"start\":56342},{\"end\":56988,\"start\":56943},{\"end\":57586,\"start\":57509},{\"end\":58046,\"start\":57993},{\"end\":58643,\"start\":58594},{\"end\":59114,\"start\":59028}]"}}}, "year": 2023, "month": 12, "day": 17}