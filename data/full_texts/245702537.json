{"id": 245702537, "updated": "2022-09-29 22:23:24.631", "metadata": {"title": "STAnet: A Spatiotemporal Attention Network for Decoding Auditory Spatial Attention from EEG", "authors": "[{\"first\":\"Enze\",\"last\":\"Su\",\"middle\":[]},{\"first\":\"Siqi\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Longhan\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Haizhou\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Tanja\",\"last\":\"Schultz\",\"middle\":[]}]", "venue": "IEEE Transactions on Biomedical Engineering", "journal": "IEEE Transactions on Biomedical Engineering", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Objective: Humans are able to localize the source of a sound. This enables them to direct attention to a particular speaker in a cocktail party. Psycho-acoustic studies show that the sensory cortices of the human brain respond to the location of sound sources differently, and the auditory attention itself is a dynamic and temporally based brain activity. In this work, we seek to build a computational model which uses both spatial and temporal information manifested in EEG signals for auditory spatial attention detection (ASAD). Methods: We propose an end-to-end spatiotemporal attention network, denoted as STAnet, to detect auditory spatial attention from EEG. The STAnet is designed to assign differentiated weights dynamically to EEG channels through a spatial attention mechanism, and to temporal patterns in EEG signals through a temporal attention mechanism. Results: We report the ASAD experiments on two publicly available datasets. The STAnet outperforms other competitive models by a large margin under various experimental conditions. Its attention decision for 1-second decision window outperforms that of the state-of-the-art techniques for 10-second decision window. Experimental results also demonstrate that the STAnet achieves competitive performance on EEG signals ranging from 64 to as few as 16 channels. Conclusion: This study provides evidence suggesting that efficient low-density EEG online decoding is within reach. Significance: This study also marks an important step towards the practical implementation of ASAD in real life applications.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "34982671", "pubmedcentral": null, "dblp": "journals/tbe/SuCXLS22", "doi": "10.1109/tbme.2022.3140246"}}, "content": {"source": {"pdf_hash": "15a5cd7fd533f5f0d5e1ac1c3824be6061146a20", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://ieeexplore.ieee.org/ielx7/10/9799700/09669037.pdf", "status": "HYBRID"}}, "grobid": {"id": "b3ac1b1a408168772d9744a88e487333a40034b5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/15a5cd7fd533f5f0d5e1ac1c3824be6061146a20.txt", "contents": "\nSTAnet: A Spatiotemporal Attention Network for Decoding Auditory Spatial Attention from EEG\n\n\nEnze Su \nTransactions on Biomedical Engineering\n\n\nSiqi Cai \nTransactions on Biomedical Engineering\n\n\nStudent Member, IEEELonghan Xie \nTransactions on Biomedical Engineering\n\n\nMember, IEEEHaizhou Li \nTransactions on Biomedical Engineering\n\n\nFellow, IEEETanja Schultz \nTransactions on Biomedical Engineering\n\n\nSTAnet: A Spatiotemporal Attention Network for Decoding Auditory Spatial Attention from EEG\n10.1109/TBME.2022.3140246This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information:Index Terms-Auditory attentionbrain-computer inter- faceelectroencephalographyspatial attentiontemporal attention\nObjective: Humans are able to localize the source of a sound. This enables them to direct attention to a particular speaker in a cocktail party. Psycho-acoustic studies show that the sensory cortices of the human brain respond to the location of sound sources differently, and the auditory attention itself is a dynamic and temporally based brain activity. In this work, we seek to build a computational model which uses both spatial and temporal information manifested in EEG signals for auditory spatial attention detection (ASAD). Methods: We propose an endto-end spatiotemporal attention network, denoted as STAnet, to detect auditory spatial attention from EEG. The STAnet is designed to assign differentiated weights dynamically to EEG channels through a spatial attention mechanism, and to temporal patterns in EEG signals through a temporal attention mechanism. Results: We report the ASAD experiments on two publicly available datasets. The STAnet outperforms other competitive models by a large margin under various experimental conditions. Its attention decision for 1-second decision window outperforms that of the state-ofthe-art techniques for 10-second decision window. Experimental results also demonstrate that the STAnet achieves competitive performance on EEG signals ranging from 64 to as few as 16 channels. Conclusion: This study provides evidence suggesting that efficient low-density EEG online decoding is within reach.Significance: This study also marks an important step towards the practical implementation of ASAD in real life applications.Index Terms-Auditory attention, brain-computer interface, electroencephalography, spatial attention, temporal attention Manuscript received ; revised .\n\nI. INTRODUCTION\n\nHumans have the ability to focus the auditory attention on one speaker in a multi-speaker environment, or \"cocktail party scenario\" [1]. However, people with hearing loss will find such situations are particularly difficult. Modern hearing aids are developed for a better experience by applying noise suppression, however, these devices often fail in practice for unable to single out and enhance the attended speech stream. The studies in neuroscience show that auditory attention can be directly detected from neural activities [2]- [4], which is known as auditory attention detection (AAD). Such progress motivates us to develop engineering solutions to AAD, that in turn opens up many possibilities for the cognitive control of hearing aids, also called neuro-steered hearing aids [5], [6].\n\nTo develop a neurophysiologically plausible brain-computer interface (BCI), many studies have been devoted to discovering the relationship between neural responses and speech stimuli for AAD. Mesgarani and Chang [2] have demonstrated that speech spectrograms reconstructed from cortical responses to a mixture of speakers are dominated by the salient spectro-temporal features of the attended speaker. Along this line of thought, a stimulus-reconstruction method is studied, where neural responses are used to approximate the envelope of the speech stream heard by the subject. The reconstructed envelope is then compared with the original speech stimulus to detect the speaker's attention [4], [7]- [11]. Unfortunately, such correlation between the reconstructed and the attended speech envelopes is generally weak. This could be due to the over-simplified linear computational model. Considering the inherent non-linear processing of acoustic signals along the auditory pathway [12], [13], Taillez et al. [14] firstly studied a non-linear neural network to map the EEG signals to speech envelopes in a cocktail party scenario, that outperforms the linear model baseline. Recently convolutional neural network (CNN) models [15], [16] were studied to detect the attended speakers directly when both the EEG signals and speech stimuli are available.\n\nThe early studies of engineering solutions to AAD confronts two challenges. 1) There is a trade-off between the accuracy and the AAD decision window size. The decoding accuracy drops as the decision window narrows, i.e., temporal resolution increases, because the low-frequency envelopes of small window contain little information of speech [17], [18]. 2) The AAD methods require clean speech stimuli as the reference, which are not always available in real-world applications, such as hearing prostheses or robotic voice acquisition, where a system is expected to perform in a complex acoustic environment of multiple speakers. While speaker extraction and speech enhancement techniques can be explored to derive such speech stimuli [5], [19], [20], they add overhead to the AAD system, and their quality may impact AAD accuracy, that becomes another issue [17].\n\nInspired by the findings that the locus of the auditory attention is neurally encoded [21]- [23], auditory spatial attention detection (ASAD) from EEG signals has been studied recently [17], [24]- [27], where the spatial location of the attended speaker is decoded from brain activities alone, without the need of clean speech stimuli as the reference. This is highly desirable for neuro-steered hearing aids as clean speech stimuli are not always available. Moreover, the ASAD approach is based on brain lateralization [28], which is an instantaneous feature, as opposed to the low-frequency speech envelope, which requires a temporal observation window of reasonable size. We hypothesize that the ASAD approach will perform more accurately than the AAD approach in low-latency settings.\n\nVandecappelle et al. [24] developed a CNN-based ASAD model, which achieves a competitive accuracy of around 80% for a decision window of 1 second. Unfortunately, as the EEG signals are reduced from a time series to a single value in this approach, the temporal information is not exploited. We consider that the dynamics of the EEG signals contain valuable information for decoding auditory spatial attention [29], that will be a focus of this paper.\n\nEffective feature representation is a crucial step for pattern classification due to the low signal-to-noise ratio of raw EEG signals [30]- [33]. The EEG signals contain multivariate information in space and time. In space, the EEG channels reflect different functional roles of human brain in speech processing [34], [35], therefore the EEG signals are essentially non-linear time series data [36]. We consider that the EEG channels provide differentiated contributions to the encoding of spatial attention in human brain; in time, we hope to leverage the information encoded in the temporal progression of EEG signals. An early study by Bednar et al. [29] shows that the spatiotemporal pattern of the EEG features is critical for successfully decoding different spatial locations. In this paper, we propose a neural attention mechanism that is inspired by the findings in the spatiotemporal analysis of human AAD, and implement an engineering solution for the first time.\n\nFirst, previous studies show that the human responses to speech stimuli differ in different brain regions in a cocktail party task [21]- [23], [37]- [39]. The EEG signals are recorded from multiple sites of the scalp, therefore, some EEG channels are more informative than others in terms of informing the decision-making process in the brain [4], [17], [34], [35]. At the same time, the distribution of effective channels may vary from subject to subject [7], [28]. To extract discriminative features from the spatial information, some employ channel selection techniques to choose more relevant channels [40], [41]. Unlike the channel selection techniques, in this study, we propose a spatial attention mechanism, that derives weights dynamically from the input EEG channels across different spatial locations over the cortex, just like how human brains selectively attend to input acoustic stimuli.\n\nSecond, any attentional neural mechanism that respects the temporal structure of auditory sensory inputs must therefore be dynamic over time, including spatial attention [22]. In view of the fact that brain activity is a temporal process, and the EEG signals are essentially non-linear time series data [36], we propose a temporal attention mechanism to capture the temporal characteristics of EEG. The temporal attention mechanism is designed to assign differentiated weights temporally to EEG signals over a decision window to form a final representation [42].\n\nIn this paper, we propose an end-to-end spatiotemporal attention network, denoted as STAnet, to detect auditory spatial attention solely based on EEG signals. To the best of our knowledge, this is the first study of a spatiotemporal attention mechanism for EEGbased ASAD tasks. The main contributions of this work can be summarized as follows. (1) We propose a spatial attention mechanism to automatically assign differentiated modulation weights to EEG channels, and a temporal attention mechanism to learn temporal feature representation that is relevant to ASAD. (2) We propose an end-to-end pipeline architecture that optimizes spatial and temporal representation explicitly and in a logical order. (3) We validate the effectiveness of the spatiotemporal attention mechanism through extensive ablation study, data visualization, and comprehensive experiments on two publicly available EEG datasets.\n\nThe remainder of this paper is organized as follows. Section II elaborates on the proposed STAnet pipeline for decoding auditory spatial attention. In Section III, we describe: 1) the used datasets and processing; as well as 2) contrastive models and their application to the datasets. In Section IV, we report on experimental results and compare our proposed approach to competing ASAD models. In Section V, we discuss our findings and conclude in Section VI.\n\n\nII. METHODS\n\nA traditional EEG-based ASAD pipeline consists of a feature extraction frontend and a pattern classification backend. The STAnet is a novel end-to-end architecture, as illustrated in Fig. 1, that features a spatial attention and temporal attention mechanism. It is different from a traditional pipeline in many ways. The end-to-end architecture learns to automatically discover spatial and temporal representations needed for attention detection from raw EEG data, therefore, without the need of hand-crafted feature extraction. The attention mechanism learns to dynamically pay attention to specific channels and temporal patterns during run-time inference.\n\nBy applying a moving window to raw EEG data, we obtain a sequence of decision windows, each of which has a small duration, and is used for feature representation. Let E = [c 1 , \u00b7 \u00b7 \u00b7 , c i , \u00b7 \u00b7 \u00b7 , c N ] \u2208 R T \u00d7N be the EEG signals of a decision window, where c i \u2208 R T \u00d71 is a time series of T samples from the i-th EEG channel, and N is the number of the EEG channels. The STAnet takes E as the input and makes auditory spatial attention decision.\n\nAttention, as a human ability, plays a fundamental role in our everyday behavior in spatially and temporally fast-varying environments [22]. Auditory attention, which enhances the target speech and attenuates the interfering speech in a cocktail party, is a typical example [1], [2]. Inspired by the human cognitive process, computational attention mechanisms are employed widely in deep learning architecture [42]- [44]. Briefly, the attention mechanism can be interpreted as a means of dynamically assigning differentiated weights to the components of a signal at run-time. These differentiated weights form a receptive field, which can be in the form of an attention mask, or a regression function. The differentiated weights are dynamically generated by a neural attention mechanism, as opposed to a set of pre-trained weights. With the neural attention mechanism, we hope to extract salient information from EEG signals with respect to the ASAD task.\n\nWe employ feature representation not only for dimension reduction, but also to explicitly capture salient spatial and temporal information. The spatial and temporal attention mechanisms, as shown in Fig. 1 (a) and (b), learn \"where to attend\" and \"when to attend\" to the EEG signals in a decision window. As the temporal attention mechanism is expected to explore the interaction among the EEG signals across channels. It is logical to place spatial attention module in front of temporal attention module in a pipeline architecture.\n\n\nA. Spatial Feature Representation\n\nStudies show that several cortical regions of human brain are involved in spatial auditory processing [28], [29], [38]. The multichannel EEG recorded from different scalp regions reflect the brain responses to auditory stimulus [7], [40], [41]. We are motivated by this finding to design an attention mechanism, that learns to assign differentiated weights to EEG channels dynamically according to their individual contributions. Such attention mechanism is referred to as spatial attention. It is implemented in three steps as illustrated in Fig. 1 (a), First, we aggregate channel-wise EEG signals c i through a convolutional layer followed by a max pooling layer. This step is equivalent to a frequency analysis front-end that performs feature extraction from the time-domain signals c i as described next, Fig. 1. A schematic diagram of the proposed spatiotemporal attention network, i.e., STAnet, which mainly consists of three components: spatial feature representation, temporal feature representation, and classification module. Taking EEG data as input, the network is trained to detect auditory spatial attention by making a binary decision.\nr i = M ax(elu(Conv(c i )))(1)\nwhere Conv(\u00b7) denotes the convolution operation with an exponential linear unit elu(\u00b7) as the activation function [45]. M ax(\u00b7) denotes a max pooling layer. r i \u2208 R 1\u00d71 is the representation of the i-th channel c i and therefore Rs = [r 1 , \u00b7 \u00b7 \u00b7 , r i , \u00b7 \u00b7 \u00b7 , r N ] \u2208 R 1\u00d7N is the representation of the multi-channel EEG signals. Second, a gating mechanism, which models the interaction among the EEG channels, is adopted [43]. The gating mechanism learns to assign differentiated weights to EEG channels on the fly. As a trade-off between model complexity and generalizability, two fullyconnected (f c) layers are employed to parameterize the gating mechanism, and to achieve a non-linear mapping, as follows:\nMs = elu(w 2 (elu(w 1 Rs + b 1 )) + b 2 )(2)\nwhere w 1 and w 2 is the parameter of the first and the second f c layers, respectively. b 1 , b 2 are the bias terms of two f c layers. Ms is the attention mask generated by the spatial attention mechanism. The EEG signals E is then modulated by the attention mask Ms as follows,\nE \u2032 = Ms E(3)\nwhere denotes a point-wise multiplication. During the multiplication, the attention value is broadcast along the temporal dimension, i.e., Ms \u2208 R T \u00d7N .\n\nFinally, we employ a convolutional layer followed by a max pooling layer, that is referred to as the Conv block illustrated in Fig. 1, to extract the spatial feature representation from the masked EEG signals E \u2032 . Without spatial attention, the Conv block was first studied for ASAD in [24]. We believe that the same Conv block would work well for the masked EEG signals as the masked EEG signals carry the similar properties of original EEG signals except that the signals are weighted channel by channel. The height of the convolutional filter is set to N , the same as the number of EEG channels, and the width of the filter is extended to better explore the temporal dynamics. In this way, the masked EEG signals E \u2032 are encoded as E \u2032\u2032 = Conv(E \u2032 ). We adopt tanh function as the activation function in the convolutional layer, and apply max pooling to reduce the number of parameters. The spatial feature representation can be expressed as Es = M ax(E \u2032\u2032 ) \u2208 R Ts\u00d7Ns .\n\n\nB. Temporal Feature Representation\n\nPsycho-acoustic studies have provided convincing evidence that human attention itself is a dynamic and temporally based activity [46], [47], and the auditory system is sensitive to the temporal patterning [38], [48]. We believe that temporal patterns in EEG signals carry spatial attention information. As shown in Fig. 1  (b), a self-attention mechanism is adopted to explore the attentive temporal dynamics of EEG signals. Self-attention is an intra-attention mechanism that relates different positions of a single sequence to generate a representation of the sequence [42]. It is implemented in three steps as follows.\n\nFirst, the attention mechanism transforms EEG features Es into query Q, key K, and value V using linear projections.\nQ = EsWq K = EsW k V = EsWv(4)\nHere, the projections are the weight matrices Wq \u2208 R Ns\u00d7d k , W k \u2208 R Ns\u00d7d k , and Wv \u2208 R Ns\u00d7dv .\n\nThen, dot product is used to calculate the relationship between query and key. And temporal attention mask M t is calculated by using the sof tmax function.\nM t = sof tmax( QK T \u221a d k ) \u2208 R Ts\u00d7Ts(5)\nwhere \u221a d k is the scaling factor. The inner product values are proportional to the dimension of hidden feature space, thus need to be normalized by the square root of hidden dimension [42].\n\nFinally, the temporal attention mask assigns different weights over the time axis to an EEG sequence, that leads to an attention-weighted summation E t ,\nE t = M t V \u2208 R Ts\u00d7Ns (6)\nC. End-to-End Spatiotemporal Attention Network\n\nAn end-to-end neural architecture takes a window of EEG signals as input and makes spatial attention detection decision. It allows the spatial and temporal attention mechanisms to learn the respective feature representations in a way to optimize ASAD performance.\n\nFirst, we adopt a spatial attention mechanism to dynamically assign differentiated weights to individual EEG channels. The masked EEG signals are processed by a convolutional layer and a max pooling layer to derive spatial feature representation. Second, we adopt a self-attention mechanism to assign differentiated weights to the EEG signals temporally. In this way, we expect to generate a discriminative spatiotemporal EEG representation E t that is optimized for the ASAD task.\n\nSimilar to state-of-the-art ASAD approaches [17], [24], we treat the ASAD task as a classification problem. The extracted EEG feature E t is then transformed into a probability vector E \u2032 t by a f c layer with sigmoid activation function.\np = \u03c3(wE \u2032 t + b)(7)\nwhere w and b is the weight and the bias of the f c layer, respectively. p represents the predicted probability for a decision window. \u03c3(\u00b7) denotes the sigmoid activation function. Finally, we apply the binary cross-entropy loss to supervise the network training.\n\n\nIII. EXPERIMENTS\n\n\nA. Data Specifications\n\nWe conduct the auditory attention detection experiments on two publicly available datasets, which are denoted as KUL [49] and DTU datasets [50]. Details of the datasets are summarized in Table I. 1) KUL dataset: The EEG data were collected from 16 normalhearing subjects, who were instructed to attend to one particular speaker and ignore the other in the presence of two simultaneous speakers. The speech stimuli consist of four Dutch stories, narrated by three male Flemish speakers. All stimuli were normalized to have the same root mean squared (RMS) intensities and were perceived as equally loud. The stimuli were either presented dichotically (one speaker per ear) or after head-related transfer function (HRTF) filtering to simulate speech from 90\u00b0to the left and 90\u00b0to the right of the subject. Each subject listened to 8 trials of 6 minutes each. Throughout the experiments, the order of presentation of both conditions was randomized over the different subjects. The 64channel EEG data were collected using a BioSemi ActiveTwo device at a sampling rate of 8,192 Hz in an electromagnetically shielded and soundproof room. More details of the KUL dataset can be found in [24], [49].\n\n2) DTU dataset: The EEG data were collected from 18 normalhearing subjects, who selectively attended to one of the two simultaneous speakers. The speech stimuli consist of speech by a male and a female native speaker who simultaneously speak in anechoic or reverberant rooms. The two concurrent speech streams were normalized to have the same RMS value. The speech mixtures were presented to the subjects, with the two speech streams lateralized at respectively -60\u00b0and +60\u00b0along the azimuth direction. Each subject listened to 60 trials in total, and each trial contained auditory stimuli with a duration of 50 seconds. The position and the gender of the target speaker were randomized across the trials. 64-channel EEG data were recorded at a sample rate of 512 Hz using a BioSemi Active system. Further details of the DTU dataset can be found in [50], [51].\n\n\nB. Data Processing\n\nThe EEG data of each channel are firstly re-referenced to the average response of all electrodes. As the EEG signals analyzed are collected at different sampling frequencies, they are all bandpass filtered between 1 and 32 Hz by a 6th-order Chebyshev Type II bandpass filter, and downsampled to 128 Hz sampling rate. The frequency range is chosen based on previous non-linear AAD studies [14]- [16], [24]. Finally, EEG data channels are normalized to ensure zero mean and unit variance for each trial. As the proposed STAnet is a datadriven solution, that is expected to function in an end-to-end manner, no artifacts removal operation is involved in the data processing. The simplified end-to-end process greatly facilitates the implementation of real-time BCI systems, such as neuro-steered hearing aids.\n\nWe analyze seven decision window sizes in this study, i.e., 0.1, 0.2, 0.5, 1, 2, 5, and 10 seconds. After preprocessing, we obtain a total of 2,864 decision windows per subject, totaling 45,824 decision windows for 1-second case in the KUL dataset. In the DTU dataset, the test set results in 2,880 decision windows per subject, totaling 51,840 decision windows for 1-second case.\n\n\nC. Contrastive Models\n\nTo validate the effectiveness of the spatial attention and temporal attention mechanisms, we conduct experiments on four models, that include 1) a CNN model as in [24], that is a reduced version of STAnet (see Fig. 1) by only keeping the Conv block, and FC layers in the pipeline in Fig. 1, and removing the spatial attention mechanism and the temporal feature representation module; 2) a SAnet by removing the temporal feature representation module from the STAnet; 3) a TAnet by removing the spatial attention mechanism in spatial feature representation module from the STAnet; and 4) the proposed STAnet. The composition of the models are summarized in Table II.\n\n\nD. Network Configuration\n\nWe evaluate the performance of the proposed and baseline methods using 5-fold cross-validation (CV) over the collection of decision windows. We make a decision for each decision window and report the overall ASAD accuracy and by subject. For each subject, the feature data and attention labels are divided into five groups equally. Four of them are used to train the classifiers. The remaining group is used for evaluation. This process is repeated five times until all data are tested once. The models are implemented with the TensorFlow framework and trained on an NVIDIA TITAN Xp Pascal GPU.\n\nWe take 1-second decision window as an example to describe the network configuration. As the input to the systems, 1-second EEG signals are denoted as E \u2208 R 128\u00d764 with 128 samples and 64channel.\n\nIn the SAnet or STAnet, the spatial attention mechanism comprises a convolution layer with the size of 128 \u00d7 1 and two f c layers (input: 64, hidden: 8, output: 64). The output of the spatial attention mechanism is E \u2032 \u2208 R 128\u00d764 , that has the same dimension as the input EEG signals E.\n\nIn all models, we employ a convolutional layer with a kernel of 5 \u00d7 64, and a max pooling size of 4 \u00d7 1, as shown in the Conv block in Fig. 1. With or without the spatial attention mechanism, the Conv block produces an EEG feature representation Es \u2208 R 32\u00d75 .\n\nThe TAnet or STAnet involves a temporal attention mechanism. We set both Q and K to 32 \u00d7 50, and V to be of the same size as the input Es. Therefore, the size of the temporal mask is M t \u2208 R 32\u00d732 and the output is E t \u2208 R 32\u00d75 . We set both Q and K to 32 \u00d7 50, and V the same as the input Es. Therefore, the size of the temporal mask is M t \u2208 R 32\u00d732 and the output is E t \u2208 R 32\u00d75 .\n\nFor classification decision, we employ a f c layer of 160 \u00d7 2. To prevent overfitting, we apply dropout and batch normalization. The Adaptive Moment Estimation (Adam) optimizer [52] is employed to minimize the cross-entropy loss function with the learning rate of 10 \u22123 . All hyperparameters are chosen empirically with a grid search approach through a 5-fold CV experiment.\n\n\nIV. RESULTS\n\n\nA. Ablation Analysis\n\nWe conduct ablation analysis using 1-second decision window as a case study. The ASAD accuracy of the four models in Table II are reported across all subjects on KUL dataset in Fig. 2. Statistical analyses are performed using IBM SPSS statistics software at a significance level of 0.05. Descriptive statistics are used for means and standard deviations. The Kolmogorov-Smirnov test is used to confirm the normality of the distribution of the data, prior to selection of appropriate statistical tests. Paired t-tests are employed to compare the models to identify which one gives a significant improvement. We observe that SAnet attains a significantly higher average ASAD accuracy than CNN model (p <0.001), and that TAnet attains a significantly higher performance than CNN model (p <0.001). There is no statistically significant difference of ASAD accuracy between SAnet and TAnet (p = 0.15). Moreover, STAnet gains a significant increase of ASAD accuracy over SAnet (2.2%, p <0.001) and TAnet (1.8%, p <0.001).\n\nThe results on the DTU Dataset corroborate the findings on the KUL Dataset. Specifically, CNN model attains ASAD accuracy of 63.3% (SD: 5.96%). SAnet outperforms CNN model with an average improvement of 4.3% (67.6%, SD: 8.96%), and TAnet outperforms CNN model with an average improvement of 5.1% (68.4%, SD: 8.98%). STAnet achieves the best ASAD performance with an average accuracy of 71.9% (SD: 8.94%). \n\n\nModel\n\nSpatial Attention Temporal Attention CNN [24] \u00d7\n\u00d7 SAnet \u2713 \u00d7 TAnet \u00d7 \u2713 STAnet \u2713 \u2713\nIn sum, the spatial attention mechanism and the temporal attention mechanism are the contributing factors to the performance gains over the baseline CNN model. The fact that STAnet outperforms all competing models clearly confirms the advantage of the proposed spatiotemproal attention.\n\n\nB. Effect of Decision Windows\n\nWe further compare the ASAD performance of the STAnet for different detection window sizes ranging from 0.1-second to 10second, as shown in Fig. 3.\n\nOn the KUL dataset, the STAnet achieves an average decoding accuracy across all subjects of 90.1% (SD: 8.95%) for 1-second decision window, 91.4% (SD: 8.22%) for 2-second decision window, 92.6% (SD: 6.75%) for 5-second decision window, and 93.9% (SD: 6.54%) for 10-second decision window. In general, a larger decision window provides a better result, which corroborates with findings in previous studies [14], [15], [24]. It is worth noting that the STAnet is capable of decoding auditory spatial attention with a very short decision window (<1 s). For 0.5-second and 0.2-second decision windows, the STAnet obtains a high ASAD accuracy of 87.2% (SD: 9.77%) and 84.3% (SD: 9.73%), respectively. Although the accuracy for 0.1-second decision window is lower than that for 1-second, the STAnet maintains a competitive ASAD accuracy (80.8%, SD: 9.87%).\n\nOn the DTU dataset, the STAnet obtains an average accuracy of 65.7% (SD: 5.50%) for 0.1-second, 68.1% (SD: 7.08%) for 0.2second, 70.8% (SD: 8.04%) for 0.5-second, 71.9% (SD: 8.94%) for 1-second, 73.7% (SD: 9.59%) for 2-second, 76.1% (SD: 9.63%) for 5-second, and 75.8% (SD: 9.17%) for 10-second decision windows, respectively. The ASAD accuracy on the DTU dataset is significantly lower than that on the KUL dataset, which is consistent with the observations in [18], [27]. One of the possible reasons could be that the DTU dataset has two speech streams arriving 60\u00b0to the left and 60\u00b0to the right of the listening subjects [50], while the KUL dataset has the two speech streams arriving from \u00b1 90\u00b0instead [49]. Therefore, the DTU dataset presents a more challenging task than the KUL dataset. Another major difference between the DTU and the KUL dataset is that the former's auditory stimuli are presented to the listeners with room reverberation at various levels, which might reduce the cortical speech tracking accuracy in human brain [53], hence decrease the differential responses between the attended and unattended speakers [54]. However, the latter's auditory stimuli are presented to the listeners in a quiet acoustic environment.\n\nWe confirm that the STAnet performs reasonably well at a temporal resolution of 1 second, comparable to the time required for humans to switch their attention from one speaker to another. It is also encouraging to see that the STAnet still decodes well at a resolution of 100 milliseconds. We are not aware of other models that perform similarly in such low latency settings. With our results, we consider that real-time decoding of auditory attention is within reach.\n\n\nC. Low-Density EEG Signals\n\nThis work is motivated to achieve real-time EEG-based ASAD in daily-life applications. High-density EEG signals involve more Fig. 2. Accuracy of baseline model and attention-based models for decoding auditory spatial attention among all subjects in KUL dataset with 1-second decision window. These subjects are ranked according to the accuracy of the STAnet. The horizontal dotted line shows a reference accuracy at 80%. Statistically significant difference: * * * p <0.001. channels, therefore provide fine-grained spatial sampling attention detection. However, more channels also mean an increased setup time and effort [7], [41]. It is therefore desirable to reduce the number of channels required for an ASAD system. We further investigate how the STAnet performs in relatively low-density EEG systems.\n\nBoth KUL and DTU datasets are recorded with 64-channel with the BioSemi ActiveTwo system. We obtain 32-channel and 16 channel EEG following the electrode locations of the international 10/20 system [55]. Fig. 4 depicts the ASAD performance of the STAnet with 1-second decision windows based on 16-channel, 32-channel, and 64-channel EEG signals over all subjects in the KUL and DTU datasets. In general, more channels lead to better performance.\n\nOn the KUL dataset, the average accuracy of CNN model degrades from 64-channel (84.1%, SD: 10.16%) to 32-channel (79.9%, SD: 10.46%), and further to 16-channel (75.4%, SD: 11.01%). We observe an accuracy drop from 64-channel to 32-channel by 4.2%, and from 64-channel to 16-channel by 8.7%. We observe a modest accuracy drop of 3.4% and 5.7% for 32-channel and 16-channel over 64channel EEG, respectively. However, the average accuracy for the STAnet remains competitive (16-channel, 84.4%, SD: 9.94%; 32channel, 86.7%, SD: 9.85%), which significantly outperforms the CNN model (paired t-test: p <0.001).\n\nOn the DTU dataset, the average accuracy of CNN model decreases significantly from 64-channel EEG (63.3%, SD: 5.96%) to 32-channel   To conclude, the STAnet is more robust than the CNN model with low-density EEG signals. We believe that the improved robustness comes from the spatiotemproal attention mechanisms.\n\n\nD. STAnet vs Linear Decoder\n\nWe further compare the proposed STAnet with other competing models in the literature. We start by comparing the STAnet with the CCA model [9], which is considered to be one of the best linear AAD decoders to date [18]. It is noted that the CCA model requires the speech stimuli as the reference, that the STAnet doesn't require. In other words, the STAnet decodes the human attention purely from the brain signals themselves.\n\nWe re-implement the CCA model on both DTU and KUL datasets and report the performance for different decision windows, as summarized in Table III. The CCA model obtains an accuracy of 75.9% on the KUL dataset and 70.1% on the DTU dataset with 10second decision window, and 60.2% and 53.4% on the two datasets respectively with 1-second decision window. With decision windows of less than 1-second, the accuracy of the CCA model further degrades, and drops below the chance level on the DTU dataset.\n\nIt is observed in Table III that the STAnet clearly outperforms the CCA model by a large margin (>20%) across all decision window sizes. On the KUL dataset, the STAnet achieves a robust performance of above 80% accuracy even for 0.1-second decision window.\n\n\nE. STAnet vs CNN Decoder\n\nWe also compare the STAnet with CNN model by Vandecappelle et al. [24], that decodes the direction of auditory attention and achieves impressive results. For a fair comparison, we re-implement the CNN model with our experiment setup, and evaluate the ASAD accuracy for various decision windows.\n\nAs shown in Table III, the CNN model offers a significantly better accuracy than the CCA model on both KUL and DTU datasets. At the same time, the STAnet consistently outperforms the CNN model by average 6.1% on the KUL dataset and 8.8% on the DTU dataset respectively in terms of accuracy over various decision window sizes.\n\nTo summarize, the STAnet consistently outperforms the stateof-the-art linear and non-linear models on two publicly available datasets. These results confirm the effectiveness of the spatiotemporal attention network.\n\n\nV. DISCUSSIONS\n\nMulti-channel EEG signals are collected from multiple sites of the scalp. The signals acquired from various electrode positions are not equally informative as far as auditory attention detection is concerned [41]. To gain insight into how the spatially differentiated weights on channels contribute to the performance gain, we aggregate and visualize the masking weights generated by the spatial attention mechanism over all 1-second decision windows for individual subjects in Fig. 5 (a), and overall average in Fig. 5 (b) on the KUL dataset, and in Fig. 6 (a) and Fig. 6 (b) respectively on the DTU dataset.\n\n\nA. Subject-Independent Spatial Attention\n\nIt is expected that the locations indicative of neural activity contributing to speech processing have higher weights [56]. As illustrated in Fig. 5 (b) and Fig. 6 (b), in the topography with weights assigned by spatial attention mechanism, we see higher weights at electrodes placed over the frontal and temporal regions than elsewhere. These results are consistent with the findings by others that activations are observed prominently in the fronto-temporal cortex [15], [17], [24]. In addition, Fig. 5 (b) and Fig. 6 (b) also suggest that attentional modulation of speech tracking is mainly manifested in the auditoryspecific temporal regions, that corroborates previous studies [4], [7], [22], [38], [51]. Fig. 5 (a) and Fig. 6 (a) illustrate how the differentiated weights generated by the spatial attention mechanism are distributed across the scalp of different individuals. Without surprise, it is observed that the weights, that reflect an individual's attentional focus, vary across the subjects. Additionally, it is worth noting that the decoder activation pattern may vary across subjects as well, therefore, the spatial patterns of brain activity related to auditory attention differ considerably across the subjects. These findings support the claim that EEG signals exhibit subject-specific patterns due to the physiological and psychological individuality [28], [57], [58].\n\n\nB. Subject Individuality\n\nConsidering the individuality in the modulation of auditory attention, the pre-defined handcrafted EEG features find it hard to have one size fit all. In contrast, the data-driven end-to-end learning, and the spatial attention mechanism in this study learn the ability to assign differentiated weights dynamically to EEG channels subject by subject, that effectively addresses subject individuality issue.\n\n\nC. Data-Driven vs Handcrafted Features\n\nGenerally, most traditional auditory spatial attention decoding techniques involve a frontend process to remove artifacts, and to extract handcrafted features from EEG signals. In this study, we propose a data-driven solution to ASAD in an end-to-end manner. The raw EEG data are directly taken by the STAnet without any manipulation. The STAnet is capable of learning what is important for feature representation by itself as far as ASAD is concerned. As the end-to-end process is simple and straightforward, it facilitates the implementation in low-resource devices, such as neuro-steered hearing aids.\n\nWe consider that the data-driven approach is simple and effective, as can be seen in Table III where both STAnet and CNN [24] models clearly outperforms CCA [9]. The STAnet and CNN models employ data-driven frontend, while CCA involves handcrafted features. Besides the network architecture, we consider that the feature representation techniques deserve further exploration. For instance, the EEG characteristics in the frequency-domain may contribute to further enhancement of the ASAD performance [22], [23], [28], [38]. The fact that the time-domain frontend learns from the training data makes it less generalizable than other frequency-domain frontends across different recording conditions.\n\n\nD. Effect of EEG Signal Recording Conditions\n\nConsistent with the results of previous ASAD studies [17], [18], [24], the ASAD accuracy varies across subjects, which reflects the differences in recording conditions [49], [50], [59], as well as  the physiological and psychological characteristics of the individuals [60], [61]. In general, the content of the speech stimuli, the spatial origin of the brain responses, and the physical layout of the listening experiments, among others, all contribute to the variation of EEG signals. For example, as discussed in Section IV-B, the differences between the KUL and DTU datasets in terms of the physical layouts of the listening experiments possibly lead to the large performance difference in the ASAD experiments. Furthermore, we hypothesize that the variation across subjects observed in our study is partially due to the small size of the dataset. A sufficiently large dataset related to the selective auditory attention task will benefit for the non-linear ASAD decoders.\n\n\nVI. CONCLUSION\n\nIn this paper, we propose the STAnet, which incorporates two attention components into an end-to-end deep learning architecture. Our model infers attention maps along two separate dimensions, i.e., spatial and temporal, then the attention maps are multiplied to EEG signals feature map for adaptive feature refinement. This spatiotemporal encoding enables a high density of information, hence with high ASAD performance. Results indicate that the STAnet significantly outperformed conventional linear as well as the current state-of-theart non-linear approaches in two publicly available datasets. As it does not require clean speech envelopes, the STAnet has the potential to enhance the signal processing in realistic hearing aids and other BCIs by incorporating information about the attention of the user.\n\n\nFor 1second decision window, CNN model attains ASAD accuracy of 84.1%, with a standard deviation (SD) of 10.16%. SAnet outperforms CNN model with an average improvement of 3.8% (87.9%, SD: 10.10%). Similarly, TAnet outperforms CNN model with an average improvement of 4.2% (88.3%, SD: 9.39%). The proposed STAnet outperforms all others, with an average accuracy of 90.1% (SD: 8.95%).\n\nFig. 3 .\n3Auditory spatial attention detection accuracy of the STAnet for seven decision window sizes across all subjects in KUL and DTU datasets, respectively.\n\nFig. 4 .\n4Auditory spatial attention detection accuracy of the STAnet and CNN model for all subjects in the DTU and KUL datasets. (a) 16-channel EEG, (b) 32-channel EEG, and (c) 64-channel EEG.\n\n( 60 .\n602%, SD: 5.84%), and further to 16-channel (56.7%, SD: 5.18%). Relatively, the STAnet clearly reduces the performance gap over the CNN model between the performance of 64-channel (71.9%, SD: 8.94%) and 32-channel (69.1%, SD: 8.24%), as well as 16-channel (66.4%, SD: 7.66%).\n\nFig. 5 .\n5Topography maps of the decoder weights associated with the EEG electrodes on KUL dataset. We aggregate spatial attention weights for all 1-second decision windows and plot the average. (a) The decoder activation patterns for all individual subjects. (b) The decoder activation pattern averaged over all 16 subjects. Black dots represent all 64 EEG electrodes. The attention weights are denoted by colors, with red color corresponding to a higher weightage.\n\nFig. 6 .\n6Topography maps of the decoder weights associated with the EEG electrodes on DTU dataset. (a) The decoder activation patterns for all individual subjects. (b) The decoder activation pattern averaged over all 18 subjects.\n\nTABLE I DETAILS\nIOF TWO EEG DATASETS, KUL AND DTU, USED IN THE EXPERIMENTS.Dataset \n# subjects \nLanguage \nSpatial locus of the stimuli \nDuration per subject (minutes) \nTotal duration (hours) \nKUL [49] \n16 \nFlemish \n90\u00b0to the left and 90\u00b0to the right \n48 \n12.8 \nDTU [50] \n18 \nDanish \n60\u00b0to the left and 60\u00b0to the right \n50 \n15.0 \n\n\n\nTABLE II THE\nIIPROPOSED STANET AND THREE CONSTRASTIVE MODELS IN THE EXPERIMENTS\n\nTABLE III AUDITORY\nIIISPATIAL ATTENTION DETECTION ACCURACY (%) COMPARISON OF DIFFERENT MODELS ON KUL DATASET [49] AND DTU DATASET [50] FOR SEVEN DIFFERENT DECISION WINDOW LENGTHS. LINEAR MODEL DENOTES THE SETTING IN [9], WHILE CNN MODEL DENOTES THE SETTING IN [24]. NOTE THAT THE ACCURACY OF THE PROPOSED STANET SIGNIFICANTLY OUTPERFORMS BOTH THE LINEAR MODEL (p <0.001) AND NON-LINEAR MODEL (p <0.001).Dataset \nModel \nDecision window (second) \n0.1 \n0.2 \n0.5 \n1 \n2 \n5 \n10 \n\nKUL [49] \n\nlinear (CCA) [9] \n50.9 \n53.6 \n55.7 \n60.2 \n63.5 \n69.4 \n75.9 \nnon-linear (CNN) [24] \n74.3 \n78.2 \n80.6 \n84.1 \n85.7 \n86.9 \n87.9 \nSTAnet \n80.8 \n84.3 \n87.2 \n90.1 \n91.4 \n92.6 \n93.9 \n\nDTU [50] \n\nlinear (CCA) [9] \n-\n-\n-\n53.4 \n57.7 \n61.9 \n70.1 \nnon-linear (CNN) [24] \n56.7 \n58.4 \n61.7 \n63.3 \n65.2 \n67.4 \n67.8 \nSTAnet \n65.7 \n68.1 \n70.8 \n71.9 \n73.7 \n76.1 \n75.8 \n\n\nENZE et al.: STANET: A SPATIOTEMPORAL ATTENTION NETWORK FOR DECODING AUDITORY SPATIAL ATTENTION FROM EEG 3\nENZE et al.: STANET: A SPATIOTEMPORAL ATTENTION NETWORK FOR DECODING AUDITORY SPATIAL ATTENTION FROM EEG\nACKNOWLEDGMENTThe authors would like to thank Peiwen Li for useful discussions and technical assistance.\nSome experiments on the recognition of speech, with one and with two ears. E C Cherry, The Journal of the Acoustical Society of America. 255E. C. Cherry, \"Some experiments on the recognition of speech, with one and with two ears,\" The Journal of the Acoustical Society of America, vol. 25, no. 5, pp. 975-979, 1953.\n\nSelective cortical representation of attended speaker in multi-talker speech perception. N Mesgarani, E F Chang, Nature. 4857397233N. Mesgarani and E. F. Chang, \"Selective cortical representation of attended speaker in multi-talker speech perception,\" Nature, vol. 485, no. 7397, p. 233, 2012.\n\nEmergence of neural encoding of auditory objects while listening to competing speakers. N Ding, J Z Simon, Proceedings of the National Academy of Sciences. 10929N. Ding and J. Z. Simon, \"Emergence of neural encoding of auditory objects while listening to competing speakers,\" Proceedings of the National Academy of Sciences, vol. 109, no. 29, pp. 11 854-11 859, 2012.\n\nAttentional selection in a cocktail party environment can be decoded from single-trial EEG. J A O&apos;sullivan, A J Power, N Mesgarani, S Rajaram, J J Foxe, B G Shinn-Cunningham, M Slaney, S A Shamma, E C Lalor, Cerebral Cortex. 257J. A. O'Sullivan, A. J. Power, N. Mesgarani, S. Rajaram, J. J. Foxe, B. G. Shinn-Cunningham, M. Slaney, S. A. Shamma, and E. C. Lalor, \"Attentional selection in a cocktail party environment can be decoded from single-trial EEG,\" Cerebral Cortex, vol. 25, no. 7, pp. 1697-1706, 2015.\n\nEEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing prostheses. S Van Eyndhoven, T Francart, A Bertrand, IEEE Transactions on Biomedical Engineering. 645S. Van Eyndhoven, T. Francart, and A. Bertrand, \"EEG-informed attended speaker extraction from recorded speech mixtures with ap- plication in neuro-steered hearing prostheses,\" IEEE Transactions on Biomedical Engineering, vol. 64, no. 5, pp. 1045-1056, 2016.\n\nBrain-informed speech separation (BISS) for enhancement of target speaker in multitalker speech perception. E Ceolini, J Hjortkjaer, D D Wong, J O&apos;sullivan, V S Raghavan, J Herrero, A D Mehta, S.-C Liu, N Mesgarani, NeuroImage. 223117282E. Ceolini, J. Hjortkjaer, D. D. Wong, J. O'Sullivan, V. S. Raghavan, J. Herrero, A. D. Mehta, S.-C. Liu, and N. Mesgarani, \"Brain-informed speech separation (BISS) for enhancement of target speaker in mul- titalker speech perception,\" NeuroImage, vol. 223, p. 117282, 2020.\n\nDecoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. B Mirkovic, S Debener, M Jaeger, M. De Vos, Journal of Neural Engineering. 12446007B. Mirkovic, S. Debener, M. Jaeger, and M. De Vos, \"Decoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications,\" Journal of Neural Engineering, vol. 12, no. 4, p. 046007, 2015.\n\nAuditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario. W Biesmans, N Das, T Francart, A Bertrand, IEEE Transactions on Neural Systems and Rehabilitation Engineering. 255W. Biesmans, N. Das, T. Francart, and A. Bertrand, \"Auditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario,\" IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 5, pp. 402- 412, 2016.\n\nDecoding the auditory brain with canonical component analysis. A De Cheveign\u00e9, D D Wong, G M Di Liberto, J Hjortkjaer, M Slaney, E Lalor, NeuroImage. 172A. de Cheveign\u00e9, D. D. Wong, G. M. Di Liberto, J. Hjortkjaer, M. Slaney, and E. Lalor, \"Decoding the auditory brain with canonical component analysis,\" NeuroImage, vol. 172, pp. 206-216, 2018.\n\nToward decoding selective attention from single-trial EEG data in cochlear implant users. W Nogueira, G Cosatti, I Schierholz, M Egger, B Mirkovic, A B\u00fcchner, IEEE Transactions on Biomedical Engineering. 671W. Nogueira, G. Cosatti, I. Schierholz, M. Egger, B. Mirkovic, and A. B\u00fcchner, \"Toward decoding selective attention from single-trial EEG data in cochlear implant users,\" IEEE Transactions on Biomedical Engineering, vol. 67, no. 1, pp. 38-49, 2019.\n\nInference of the selective auditory attention using sequential LMMSE estimation. I Kuruvila, K C Demir, E Fischer, U Hoppe, IEEE Transactions on Biomedical Engineering. I. Kuruvila, K. C. Demir, E. Fischer, and U. Hoppe, \"Inference of the selective auditory attention using sequential LMMSE estimation,\" IEEE Transactions on Biomedical Engineering, pp. 1-14, 2021.\n\nIs there chaos in the brain? I. Concepts of nonlinear dynamics and methods of investigation. P Faure, H Korn, Comptes Rendus de l'Acad\u00e9mie des Sciences-Series III-Sciences de la Vie. 324P. Faure and H. Korn, \"Is there chaos in the brain? I. Concepts of nonlinear dynamics and methods of investigation,\" Comptes Rendus de l'Acad\u00e9mie des Sciences-Series III-Sciences de la Vie, vol. 324, no. 9, pp. 773-793, 2001.\n\nIs there chaos in the brain? II. Experimental evidence and related models. H Korn, P Faure, Comptes Rendus Biologies. 3269H. Korn and P. Faure, \"Is there chaos in the brain? II. Experimental evidence and related models,\" Comptes Rendus Biologies, vol. 326, no. 9, pp. 787-840, 2003.\n\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. T De Taillez, B Kollmeier, B T Meyer, European Journal of Neuroscience. 515T. de Taillez, B. Kollmeier, and B. T. Meyer, \"Machine learning for decoding listeners' attention from electroencephalography evoked by continuous speech,\" European Journal of Neuroscience, vol. 51, no. 5, pp. 1234-1241, 2020.\n\nComparison of two-talker attention decoding from EEG with nonlinear neural networks and linear methods. G Ciccarelli, M Nolan, J Perricone, P T Calamia, S Haro, J O&apos;sullivan, N Mesgarani, T F Quatieri, C J Smalt, Scientific Reports. 91G. Ciccarelli, M. Nolan, J. Perricone, P. T. Calamia, S. Haro, J. O'Sullivan, N. Mesgarani, T. F. Quatieri, and C. J. Smalt, \"Compar- ison of two-talker attention decoding from EEG with nonlinear neural networks and linear methods,\" Scientific Reports, vol. 9, no. 1, pp. 1-10, 2019.\n\nLow latency auditory attention detection with common spatial pattern analysis of EEG signals. S Cai, E Su, Y Song, L Xie, H Li, Proc. Interspeech 2020. Interspeech 2020S. Cai, E. Su, Y. Song, L. Xie, and H. Li, \"Low latency auditory attention detection with common spatial pattern analysis of EEG signals,\" Proc. Interspeech 2020, pp. 2772-2776, 2020.\n\nFast EEG-based decoding of the directional focus of auditory attention using common spatial patterns. S Geirnaert, T Francart, A Bertrand, IEEE Transactions on Biomedical Engineering. 685S. Geirnaert, T. Francart, and A. Bertrand, \"Fast EEG-based decoding of the directional focus of auditory attention using common spatial patterns,\" IEEE Transactions on Biomedical Engineering, vol. 68, no. 5, pp. 1557-1568, 2020.\n\nElectroencephalography-based auditory attention decoding: Toward neurosteered hearing devices. S Geirnaert, S Vandecappelle, E Alickovic, A De Cheveigne, E Lalor, B T Meyer, S Miran, T Francart, A Bertrand, IEEE Signal Processing Magazine. 384S.Geirnaert, S. Vandecappelle, E. Alickovic, A. de Cheveigne, E. Lalor, B. T. Meyer, S. Miran, T. Francart, and A. Bertrand, \"Electroencephalography-based auditory attention decoding: Toward neurosteered hearing devices,\" IEEE Signal Processing Magazine, vol. 38, no. 4, pp. 89-102, 2021.\n\nLinear versus deep learning methods for noisy speech separation for EEG-informed attention decoding. N Das, J Zegers, T Francart, A Bertrand, Journal of Neural Engineering. 17446039N. Das, J. Zegers, T. Francart, A. Bertrand et al., \"Linear versus deep learning methods for noisy speech separation for EEG-informed attention decoding,\" Journal of Neural Engineering, vol. 17, no. 4, p. 046039, 2020.\n\nCognitive-driven binaural beamforming using EEG-based auditory attention decoding. A Aroudi, S Doclo, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 28A. Aroudi and S. Doclo, \"Cognitive-driven binaural beamforming using EEG-based auditory attention decoding,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 862-875, 2020.\n\nSelective modulation of auditory cortical alpha activity in an audiovisual spatial attention task. J N Frey, N Mainy, J.-P Lachaux, N M\u00fcller, O Bertrand, N Weisz, Journal of Neuroscience. 3419J. N. Frey, N. Mainy, J.-P. Lachaux, N. M\u00fcller, O. Bertrand, and N. Weisz, \"Selective modulation of auditory cortical alpha activity in an audiovisual spatial attention task,\" Journal of Neuroscience, vol. 34, no. 19, pp. 6634-6639, 2014.\n\nSpatiotemporal dynamics of auditory attention synchronize with speech. M W\u00f6stmann, B Herrmann, B Maess, J Obleser, Proceedings of the National Academy of Sciences. 11314M. W\u00f6stmann, B. Herrmann, B. Maess, and J. Obleser, \"Spatiotemporal dynamics of auditory attention synchronize with speech,\" Proceedings of the National Academy of Sciences, vol. 113, no. 14, pp. 3873-3878, 2016.\n\nWhere is the cocktail party? Decoding locations of attended and unattended moving sound sources using EEG. A Bednar, E C Lalor, NeuroImage. 205116283A. Bednar and E. C. Lalor, \"Where is the cocktail party? Decoding locations of attended and unattended moving sound sources using EEG,\" NeuroImage, vol. 205, p. 116283, 2020.\n\nEEG-based detection of the locus of auditory attention with convolutional neural networks. S Vandecappelle, L Deckers, N Das, A H Ansari, A Bertrand, T Francart, Elife. 1056481S. Vandecappelle, L. Deckers, N. Das, A. H. Ansari, A. Bertrand, and T. Francart, \"EEG-based detection of the locus of auditory attention with convolutional neural networks,\" Elife, vol. 10, p. e56481, 2021.\n\nRiemannian geometry-based decoding of the directional focus of auditory attention using EEG. S Geirnaert, T Francart, A Bertrand, Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. the IEEE International Conference on Acoustics, Speech and Signal ProcessingS. Geirnaert, T. Francart, and A. Bertrand, \"Riemannian geometry-based decoding of the directional focus of auditory attention using EEG,\" in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 1115-1119.\n\nLow-latency auditory spatial attention detection based on spectro-spatial features from EEG. S Cai, P Sun, T Schultz, H Li, arXiv:2103.03621S. Cai, P. Sun, T. Schultz, and H. Li, \"Low-latency auditory spa- tial attention detection based on spectro-spatial features from EEG,\" arXiv:2103.03621, 2021.\n\nExtracting the locus of attention at a cocktail party from single-trial EEG using a joint CNN-LSTM model. I Kuruvila, J Muncke, E Fischer, U Hoppe, Frontiers in Physiology. 121178I. Kuruvila, J. Muncke, E. Fischer, and U. Hoppe, \"Extracting the locus of attention at a cocktail party from single-trial EEG using a joint CNN- LSTM model,\" Frontiers in Physiology, vol. 12, p. 1178, 2021.\n\nTopographic specificity of alpha power during auditory spatial attention. Y Deng, I Choi, B Shinn-Cunningham, NeuroImage. 207116360Y. Deng, I. Choi, and B. Shinn-Cunningham, \"Topographic specificity of alpha power during auditory spatial attention,\" NeuroImage, vol. 207, p. 116360, 2020.\n\nDifferent spatio-temporal electroencephalography features drive the successful decoding of binaural and monaural cues for sound localization. A Bednar, F M Boland, E C Lalor, European Journal of Neuroscience. 455A. Bednar, F. M. Boland, and E. C. Lalor, \"Different spatio-temporal electroencephalography features drive the successful decoding of bin- aural and monaural cues for sound localization,\" European Journal of Neuroscience, vol. 45, no. 5, pp. 679-689, 2017.\n\nOptimal spatial filtering of single trial EEG during imagined hand movement. H Ramoser, J Muller-Gerking, G Pfurtscheller, IEEE Transactions on Rehabilitation Engineering. 84H. Ramoser, J. Muller-Gerking, and G. Pfurtscheller, \"Optimal spatial filtering of single trial EEG during imagined hand movement,\" IEEE Transactions on Rehabilitation Engineering, vol. 8, no. 4, pp. 441-446, 2000.\n\nRegularizing common spatial patterns to improve BCI designs: Unified theory and new algorithms. F Lotte, C Guan, IEEE Transactions on Biomedical Engineering. 582F. Lotte and C. Guan, \"Regularizing common spatial patterns to improve BCI designs: Unified theory and new algorithms,\" IEEE Transactions on Biomedical Engineering, vol. 58, no. 2, pp. 355-362, 2010.\n\nBraincomputer interface-based soft robotic glove rehabilitation for stroke. N Cheng, K S Phua, H S Lai, P K Tam, K Y Tang, K K Cheng, R C Yeow, K K Ang, C Guan, J H Lim, IEEE Transactions on Biomedical Engineering. 6712N. Cheng, K. S. Phua, H. S. Lai, P. K. Tam, K. Y. Tang, K. K. Cheng, R. C.-H. Yeow, K. K. Ang, C. Guan, and J. H. Lim, \"Brain- computer interface-based soft robotic glove rehabilitation for stroke,\" IEEE Transactions on Biomedical Engineering, vol. 67, no. 12, pp. 3339-3351, 2020.\n\nDecoding single-hand and both-hand movement directions from noninvasive neural signals. J Wang, L Bi, W Fei, C Guan, IEEE Transactions on Biomedical Engineering. 686J. Wang, L. Bi, W. Fei, and C. Guan, \"Decoding single-hand and both-hand movement directions from noninvasive neural signals,\" IEEE Transactions on Biomedical Engineering, vol. 68, no. 6, pp. 1932-1940, 2020.\n\nAttentional gain control of ongoing cortical speech representations in a \"cocktail party. J R Kerlin, A J Shahin, L M Miller, Journal of Neuroscience. 302J. R. Kerlin, A. J. Shahin, and L. M. Miller, \"Attentional gain control of ongoing cortical speech representations in a \"cocktail party\",\" Journal of Neuroscience, vol. 30, no. 2, pp. 620-628, 2010.\n\nThe neural oscillations of speech processing and language comprehension: state of the art and emerging mechanisms. L Meyer, European Journal of Neuroscience. 487L. Meyer, \"The neural oscillations of speech processing and language comprehension: state of the art and emerging mechanisms,\" European Journal of Neuroscience, vol. 48, no. 7, pp. 2609-2621, 2018.\n\nNetwork neuroscience. D S Bassett, O Sporns, Nature Neuroscience. 203D. S. Bassett and O. Sporns, \"Network neuroscience,\" Nature Neuro- science, vol. 20, no. 3, pp. 353-364, 2017.\n\nGenerating natural, intelligible speech from brain activity in motor, premotor, and inferior frontal cortices. C Herff, L Diener, M Angrick, E Mugler, M C Tate, M A Goldrick, D J Krusienski, M W Slutzky, T Schultz, Frontiers in neuroscience. 131267C. Herff, L. Diener, M. Angrick, E. Mugler, M. C. Tate, M. A. Goldrick, D. J. Krusienski, M. W. Slutzky, and T. Schultz, \"Generating natural, intelligible speech from brain activity in motor, premotor, and inferior frontal cortices,\" Frontiers in neuroscience, vol. 13, p. 1267, 2019.\n\nMechanisms underlying selective neuronal tracking of attended speech at a \"cocktail party. E M Z Golumbic, N Ding, S Bickel, P Lakatos, C A Schevon, G M Mckhann, R R Goodman, R Emerson, A D Mehta, J Z Simon, Neuron. 775E. M. Z. Golumbic, N. Ding, S. Bickel, P. Lakatos, C. A. Schevon, G. M. McKhann, R. R. Goodman, R. Emerson, A. D. Mehta, J. Z. Simon et al., \"Mechanisms underlying selective neuronal tracking of attended speech at a \"cocktail party\",\" Neuron, vol. 77, no. 5, pp. 980-991, 2013.\n\nShared and connection-specific intrinsic interactions in the default mode network. J Samogin, Q Liu, M Marino, N Wenderoth, D Mantini, NeuroImage. 200J. Samogin, Q. Liu, M. Marino, N. Wenderoth, and D. Mantini, \"Shared and connection-specific intrinsic interactions in the default mode net- work,\" NeuroImage, vol. 200, pp. 474-481, 2019.\n\nOptimizing the channel selection and classification accuracy in EEG-based BCI. M Arvaneh, C Guan, K K Ang, C Quek, IEEE Transactions on Biomedical Engineering. 586M. Arvaneh, C. Guan, K. K. Ang, and C. Quek, \"Optimizing the channel selection and classification accuracy in EEG-based BCI,\" IEEE Transactions on Biomedical Engineering, vol. 58, no. 6, pp. 1865-1873, 2011.\n\nAnalysis of miniaturization effects and channel selection strategies for EEG sensor networks with application to auditory attention detection. A M Narayanan, A Bertrand, IEEE Transactions on Biomedical Engineering. 671A. M. Narayanan and A. Bertrand, \"Analysis of miniaturization effects and channel selection strategies for EEG sensor networks with applica- tion to auditory attention detection,\" IEEE Transactions on Biomedical Engineering, vol. 67, no. 1, pp. 234-244, 2019.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in Neural Information Processing Systems, 2017, pp. 5998-6008.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 7132-7141.\n\nCbam: Convolutional block attention module. S Woo, J Park, J.-Y. Lee, I So Kweon, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, \"Cbam: Convolutional block attention module,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 3-19.\n\nFast and accurate deep network learning by exponential linear units (elus). D.-A Clevert, T Unterthiner, S Hochreiter, arXiv:1511.07289D.-A. Clevert, T. Unterthiner, and S. Hochreiter, \"Fast and accurate deep network learning by exponential linear units (elus),\" arXiv:1511.07289, 2015.\n\nDynamic attending and responses to time. M R Jones, M Boltz, Psychological Review. 963459M. R. Jones and M. Boltz, \"Dynamic attending and responses to time.\" Psychological Review, vol. 96, no. 3, p. 459, 1989.\n\nTemporal aspects of stimulus-driven attending in dynamic arrays. M R Jones, H Moynihan, N Mackenzie, J Puente, Psychological Science. 134M. R. Jones, H. Moynihan, N. MacKenzie, and J. Puente, \"Temporal aspects of stimulus-driven attending in dynamic arrays,\" Psychological Science, vol. 13, no. 4, pp. 313-319, 2002.\n\nThe role of temporal regularity in auditory segregation. L.-V Andreou, M Kashino, M Chait, Hearing Research. 2801-2L.-V. Andreou, M. Kashino, and M. Chait, \"The role of temporal regularity in auditory segregation,\" Hearing Research, vol. 280, no. 1-2, pp. 228-235, 2011.\n\nAuditory Attention Detection Dataset KULeuven. N Das, T Francart, A Bertrand, 10.5281/zenodo.3997352Version 1.1.0. [OnlineN. Das, T. Francart, and A. Bertrand, \"Auditory Attention Detection Dataset KULeuven,\" Aug. 2020, Version 1.1.0. [Online]. Available: https://doi.org/10.5281/zenodo.3997352\n\nEEG and Audio Dataset for Auditory Attention Decoding. S A Fuglsang, D D Wong, J Hjortkjaer, 10.5281/zenodo.1199011S. A. Fuglsang, D. D. Wong, and J. Hjortkjaer, \"EEG and Audio Dataset for Auditory Attention Decoding,\" Mar. 2018. [Online]. Available: https://doi.org/10.5281/zenodo.1199011\n\nNoise-robust cortical tracking of attended speech in real-world acoustic scenes. S A Fuglsang, T Dau, J Hjortkjaer, NeuroImage. 156S. A. Fuglsang, T. Dau, and J. Hjortkjaer, \"Noise-robust cortical tracking of attended speech in real-world acoustic scenes,\" NeuroImage, vol. 156, pp. 435-444, 2017.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv:1412.6980, 2014.\n\nAdaptive temporal encoding leads to a background-insensitive cortical representation of speech. N Ding, J Z Simon, Journal of Neuroscience. 3313N. Ding and J. Z. Simon, \"Adaptive temporal encoding leads to a background-insensitive cortical representation of speech,\" Journal of Neuroscience, vol. 33, no. 13, pp. 5728-5735, 2013.\n\nThe effects of selective attention and speech acoustics on neural speechtracking in a multi-talker scene. J M Rimmele, E Z Golumbic, E Schr\u00f6ger, D Poeppel, Cortex. 68J. M. Rimmele, E. Z. Golumbic, E. Schr\u00f6ger, and D. Poeppel, \"The effects of selective attention and speech acoustics on neural speech- tracking in a multi-talker scene,\" Cortex, vol. 68, pp. 144-154, 2015.\n\nCerebral location of international 10-20 system electrode placement. R W Homan, J Herman, P Purdy, Electroencephalography and Clinical Neurophysiology. 664R. W. Homan, J. Herman, and P. Purdy, \"Cerebral location of interna- tional 10-20 system electrode placement,\" Electroencephalography and Clinical Neurophysiology, vol. 66, no. 4, pp. 376-382, 1987.\n\nDenoising based on spatial filtering. A De Cheveign\u00e9, J Z Simon, Journal of Neuroscience Methods. 1712A. de Cheveign\u00e9 and J. Z. Simon, \"Denoising based on spatial filtering,\" Journal of Neuroscience Methods, vol. 171, no. 2, pp. 331-339, 2008.\n\nIndividual differences in attentional modulation of cortical responses correlate with selective attention performance. I Choi, L Wang, H Bharadwaj, B Shinn-Cunningham, Hearing Research. 314I. Choi, L. Wang, H. Bharadwaj, and B. Shinn-Cunningham, \"Individual differences in attentional modulation of cortical responses correlate with selective attention performance,\" Hearing Research, vol. 314, pp. 10-19, 2014.\n\nElectroencephalographic signatures of the neural representation of speech during selective attention. V Viswanathan, H M Bharadwaj, B G Shinn-Cunningham, 6EneuroV. Viswanathan, H. M. Bharadwaj, and B. G. Shinn-Cunningham, \"Elec- troencephalographic signatures of the neural representation of speech during selective attention,\" Eneuro, vol. 6, no. 5, pp. 1-14, 2019.\n\nThe effect of headrelated filtering and ear-specific decoding bias on auditory attention detection. N Das, W Biesmans, A Bertrand, T Francart, Journal of neural engineering. 13556014N. Das, W. Biesmans, A. Bertrand, and T. Francart, \"The effect of head- related filtering and ear-specific decoding bias on auditory attention detection,\" Journal of neural engineering, vol. 13, no. 5, p. 056014, 2016.\n\nOptimizing spatial filters for robust EEG single-trial analysis. B Blankertz, R Tomioka, S Lemm, M Kawanabe, K.-R Muller, IEEE Signal Processing Magazine. 251B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. Muller, \"Optimizing spatial filters for robust EEG single-trial analysis,\" IEEE Signal Processing Magazine, vol. 25, no. 1, pp. 41-56, 2007.\n\nSubject-independent brain-computer interfaces based on deep convolutional neural networks. O.-Y Kwon, M.-H Lee, C Guan, S.-W Lee, IEEE Transactions on Neural Networks and Learning Systems. 3110O.-Y. Kwon, M.-H. Lee, C. Guan, and S.-W. Lee, \"Subject-independent brain-computer interfaces based on deep convolutional neural networks,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 10, pp. 3839-3852, 2019.\n", "annotations": {"author": "[{\"end\":144,\"start\":95},{\"end\":195,\"start\":145},{\"end\":269,\"start\":196},{\"end\":334,\"start\":270},{\"end\":402,\"start\":335}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":100},{\"end\":153,\"start\":150},{\"end\":227,\"start\":224},{\"end\":292,\"start\":290},{\"end\":360,\"start\":353}]", "author_first_name": "[{\"end\":99,\"start\":95},{\"end\":149,\"start\":145},{\"end\":223,\"start\":216},{\"end\":289,\"start\":282},{\"end\":352,\"start\":347}]", "author_affiliation": "[{\"end\":143,\"start\":104},{\"end\":194,\"start\":155},{\"end\":268,\"start\":229},{\"end\":333,\"start\":294},{\"end\":401,\"start\":362}]", "title": "[{\"end\":92,\"start\":1},{\"end\":494,\"start\":403}]", "venue": null, "abstract": "[{\"end\":2678,\"start\":958}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2832,\"start\":2829},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3230,\"start\":3227},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3235,\"start\":3232},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3485,\"start\":3482},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3490,\"start\":3487},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3708,\"start\":3705},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4186,\"start\":4183},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4191,\"start\":4188},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4197,\"start\":4193},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4477,\"start\":4473},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4483,\"start\":4479},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4504,\"start\":4500},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4721,\"start\":4717},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4727,\"start\":4723},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5188,\"start\":5184},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5194,\"start\":5190},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5580,\"start\":5577},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5586,\"start\":5582},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5592,\"start\":5588},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5705,\"start\":5701},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5798,\"start\":5794},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5804,\"start\":5800},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5897,\"start\":5893},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5903,\"start\":5899},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5909,\"start\":5905},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6232,\"start\":6228},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6523,\"start\":6519},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6911,\"start\":6907},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7088,\"start\":7084},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7094,\"start\":7090},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7266,\"start\":7262},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7272,\"start\":7268},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7348,\"start\":7344},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7607,\"start\":7603},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8060,\"start\":8056},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8066,\"start\":8062},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8072,\"start\":8068},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8078,\"start\":8074},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8271,\"start\":8268},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8277,\"start\":8273},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8283,\"start\":8279},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8289,\"start\":8285},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8384,\"start\":8381},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8390,\"start\":8386},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8535,\"start\":8531},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8541,\"start\":8537},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9002,\"start\":8998},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9135,\"start\":9131},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9389,\"start\":9385},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12024,\"start\":12020},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12162,\"start\":12159},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12167,\"start\":12164},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12299,\"start\":12295},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12305,\"start\":12301},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13518,\"start\":13514},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13524,\"start\":13520},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13530,\"start\":13526},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13643,\"start\":13640},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13649,\"start\":13645},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13655,\"start\":13651},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14713,\"start\":14709},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15024,\"start\":15020},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16094,\"start\":16090},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16950,\"start\":16946},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16956,\"start\":16952},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17026,\"start\":17022},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17032,\"start\":17028},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17392,\"start\":17388},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18075,\"start\":18071},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19102,\"start\":19098},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19108,\"start\":19104},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19744,\"start\":19740},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19766,\"start\":19762},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20692,\"start\":20690},{\"end\":20695,\"start\":20692},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20807,\"start\":20803},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20813,\"start\":20809},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21669,\"start\":21665},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21675,\"start\":21671},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22091,\"start\":22087},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22097,\"start\":22093},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22103,\"start\":22099},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23080,\"start\":23076},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25517,\"start\":25513},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27225,\"start\":27221},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28139,\"start\":28135},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28145,\"start\":28141},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28151,\"start\":28147},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29048,\"start\":29044},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29054,\"start\":29050},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29211,\"start\":29207},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29293,\"start\":29289},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":29626,\"start\":29622},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":29719,\"start\":29715},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30949,\"start\":30946},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30955,\"start\":30951},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":31334,\"start\":31330},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32670,\"start\":32667},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32746,\"start\":32742},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33810,\"start\":33806},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34809,\"start\":34805},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":35373,\"start\":35369},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35722,\"start\":35718},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35728,\"start\":35724},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":35734,\"start\":35730},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35936,\"start\":35933},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35941,\"start\":35938},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35947,\"start\":35943},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":35953,\"start\":35949},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":35959,\"start\":35955},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36627,\"start\":36623},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":36633,\"start\":36629},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":36639,\"start\":36635},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37848,\"start\":37844},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37883,\"start\":37880},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38227,\"start\":38223},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38233,\"start\":38229},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38239,\"start\":38235},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":38245,\"start\":38241},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38526,\"start\":38522},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38532,\"start\":38528},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38538,\"start\":38534},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":38641,\"start\":38637},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":38647,\"start\":38643},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":38653,\"start\":38649},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":38742,\"start\":38738},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":38748,\"start\":38744}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40659,\"start\":40274},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40821,\"start\":40660},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41016,\"start\":40822},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41300,\"start\":41017},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41768,\"start\":41301},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42000,\"start\":41769},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42332,\"start\":42001},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42413,\"start\":42333},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43251,\"start\":42414}]", "paragraph": "[{\"end\":3491,\"start\":2697},{\"end\":4841,\"start\":3493},{\"end\":5706,\"start\":4843},{\"end\":6496,\"start\":5708},{\"end\":6948,\"start\":6498},{\"end\":7923,\"start\":6950},{\"end\":8826,\"start\":7925},{\"end\":9390,\"start\":8828},{\"end\":10294,\"start\":9392},{\"end\":10756,\"start\":10296},{\"end\":11430,\"start\":10772},{\"end\":11883,\"start\":11432},{\"end\":12840,\"start\":11885},{\"end\":13374,\"start\":12842},{\"end\":14563,\"start\":13412},{\"end\":15308,\"start\":14595},{\"end\":15634,\"start\":15354},{\"end\":15801,\"start\":15649},{\"end\":16778,\"start\":15803},{\"end\":17438,\"start\":16817},{\"end\":17556,\"start\":17440},{\"end\":17685,\"start\":17588},{\"end\":17843,\"start\":17687},{\"end\":18076,\"start\":17886},{\"end\":18231,\"start\":18078},{\"end\":18304,\"start\":18258},{\"end\":18569,\"start\":18306},{\"end\":19052,\"start\":18571},{\"end\":19292,\"start\":19054},{\"end\":19577,\"start\":19314},{\"end\":20814,\"start\":19623},{\"end\":21676,\"start\":20816},{\"end\":22505,\"start\":21699},{\"end\":22887,\"start\":22507},{\"end\":23578,\"start\":22913},{\"end\":24201,\"start\":23607},{\"end\":24398,\"start\":24203},{\"end\":24687,\"start\":24400},{\"end\":24948,\"start\":24689},{\"end\":25334,\"start\":24950},{\"end\":25710,\"start\":25336},{\"end\":26763,\"start\":25749},{\"end\":27170,\"start\":26765},{\"end\":27227,\"start\":27180},{\"end\":27547,\"start\":27261},{\"end\":27728,\"start\":27581},{\"end\":28580,\"start\":27730},{\"end\":29823,\"start\":28582},{\"end\":30293,\"start\":29825},{\"end\":31130,\"start\":30324},{\"end\":31577,\"start\":31132},{\"end\":32183,\"start\":31579},{\"end\":32497,\"start\":32185},{\"end\":32954,\"start\":32529},{\"end\":33453,\"start\":32956},{\"end\":33711,\"start\":33455},{\"end\":34034,\"start\":33740},{\"end\":34361,\"start\":34036},{\"end\":34578,\"start\":34363},{\"end\":35206,\"start\":34597},{\"end\":36640,\"start\":35251},{\"end\":37074,\"start\":36669},{\"end\":37721,\"start\":37117},{\"end\":38420,\"start\":37723},{\"end\":39445,\"start\":38469},{\"end\":40273,\"start\":39464}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14594,\"start\":14564},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15353,\"start\":15309},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15648,\"start\":15635},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17587,\"start\":17557},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17885,\"start\":17844},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18257,\"start\":18232},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19313,\"start\":19293},{\"attributes\":{\"id\":\"formula_7\"},\"end\":27260,\"start\":27228}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19818,\"start\":19810},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23577,\"start\":23569},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25874,\"start\":25866},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33100,\"start\":33091},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33482,\"start\":33473},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34057,\"start\":34048},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":37817,\"start\":37808}]", "section_header": "[{\"end\":2695,\"start\":2680},{\"end\":10770,\"start\":10759},{\"end\":13410,\"start\":13377},{\"end\":16815,\"start\":16781},{\"end\":19596,\"start\":19580},{\"end\":19621,\"start\":19599},{\"end\":21697,\"start\":21679},{\"end\":22911,\"start\":22890},{\"end\":23605,\"start\":23581},{\"end\":25724,\"start\":25713},{\"end\":25747,\"start\":25727},{\"end\":27178,\"start\":27173},{\"end\":27579,\"start\":27550},{\"end\":30322,\"start\":30296},{\"end\":32527,\"start\":32500},{\"end\":33738,\"start\":33714},{\"end\":34595,\"start\":34581},{\"end\":35249,\"start\":35209},{\"end\":36667,\"start\":36643},{\"end\":37115,\"start\":37077},{\"end\":38467,\"start\":38423},{\"end\":39462,\"start\":39448},{\"end\":40669,\"start\":40661},{\"end\":40831,\"start\":40823},{\"end\":41024,\"start\":41018},{\"end\":41310,\"start\":41302},{\"end\":41778,\"start\":41770},{\"end\":42017,\"start\":42002},{\"end\":42346,\"start\":42334},{\"end\":42433,\"start\":42415}]", "table": "[{\"end\":42332,\"start\":42077},{\"end\":43251,\"start\":42818}]", "figure_caption": "[{\"end\":40659,\"start\":40276},{\"end\":40821,\"start\":40671},{\"end\":41016,\"start\":40833},{\"end\":41300,\"start\":41027},{\"end\":41768,\"start\":41312},{\"end\":42000,\"start\":41780},{\"end\":42077,\"start\":42019},{\"end\":42413,\"start\":42349},{\"end\":42818,\"start\":42437}]", "figure_ref": "[{\"end\":10961,\"start\":10955},{\"end\":13047,\"start\":13041},{\"end\":13965,\"start\":13955},{\"end\":14228,\"start\":14222},{\"end\":15936,\"start\":15930},{\"end\":17143,\"start\":17132},{\"end\":23129,\"start\":23123},{\"end\":23202,\"start\":23196},{\"end\":24830,\"start\":24824},{\"end\":25932,\"start\":25926},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27727,\"start\":27721},{\"end\":30455,\"start\":30449},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31342,\"start\":31336},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35085,\"start\":35075},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35120,\"start\":35110},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35154,\"start\":35148},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35169,\"start\":35163},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35403,\"start\":35393},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35418,\"start\":35408},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35755,\"start\":35749},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35770,\"start\":35764},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35971,\"start\":35961},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35986,\"start\":35976}]", "bib_author_first_name": "[{\"end\":43645,\"start\":43644},{\"end\":43647,\"start\":43646},{\"end\":43976,\"start\":43975},{\"end\":43989,\"start\":43988},{\"end\":43991,\"start\":43990},{\"end\":44270,\"start\":44269},{\"end\":44278,\"start\":44277},{\"end\":44280,\"start\":44279},{\"end\":44643,\"start\":44642},{\"end\":44645,\"start\":44644},{\"end\":44664,\"start\":44663},{\"end\":44666,\"start\":44665},{\"end\":44675,\"start\":44674},{\"end\":44688,\"start\":44687},{\"end\":44699,\"start\":44698},{\"end\":44701,\"start\":44700},{\"end\":44709,\"start\":44708},{\"end\":44711,\"start\":44710},{\"end\":44731,\"start\":44730},{\"end\":44741,\"start\":44740},{\"end\":44743,\"start\":44742},{\"end\":44753,\"start\":44752},{\"end\":44755,\"start\":44754},{\"end\":45193,\"start\":45192},{\"end\":45210,\"start\":45209},{\"end\":45222,\"start\":45221},{\"end\":45650,\"start\":45649},{\"end\":45661,\"start\":45660},{\"end\":45675,\"start\":45674},{\"end\":45677,\"start\":45676},{\"end\":45685,\"start\":45684},{\"end\":45704,\"start\":45703},{\"end\":45706,\"start\":45705},{\"end\":45718,\"start\":45717},{\"end\":45729,\"start\":45728},{\"end\":45731,\"start\":45730},{\"end\":45743,\"start\":45739},{\"end\":45750,\"start\":45749},{\"end\":46170,\"start\":46169},{\"end\":46182,\"start\":46181},{\"end\":46193,\"start\":46192},{\"end\":46207,\"start\":46202},{\"end\":46616,\"start\":46615},{\"end\":46628,\"start\":46627},{\"end\":46635,\"start\":46634},{\"end\":46647,\"start\":46646},{\"end\":47086,\"start\":47085},{\"end\":47102,\"start\":47101},{\"end\":47104,\"start\":47103},{\"end\":47112,\"start\":47111},{\"end\":47114,\"start\":47113},{\"end\":47128,\"start\":47127},{\"end\":47142,\"start\":47141},{\"end\":47152,\"start\":47151},{\"end\":47460,\"start\":47459},{\"end\":47472,\"start\":47471},{\"end\":47483,\"start\":47482},{\"end\":47497,\"start\":47496},{\"end\":47506,\"start\":47505},{\"end\":47518,\"start\":47517},{\"end\":47908,\"start\":47907},{\"end\":47920,\"start\":47919},{\"end\":47922,\"start\":47921},{\"end\":47931,\"start\":47930},{\"end\":47942,\"start\":47941},{\"end\":48286,\"start\":48285},{\"end\":48295,\"start\":48294},{\"end\":48681,\"start\":48680},{\"end\":48689,\"start\":48688},{\"end\":48998,\"start\":48997},{\"end\":49012,\"start\":49011},{\"end\":49025,\"start\":49024},{\"end\":49027,\"start\":49026},{\"end\":49405,\"start\":49404},{\"end\":49419,\"start\":49418},{\"end\":49428,\"start\":49427},{\"end\":49441,\"start\":49440},{\"end\":49443,\"start\":49442},{\"end\":49454,\"start\":49453},{\"end\":49462,\"start\":49461},{\"end\":49481,\"start\":49480},{\"end\":49494,\"start\":49493},{\"end\":49496,\"start\":49495},{\"end\":49508,\"start\":49507},{\"end\":49510,\"start\":49509},{\"end\":49920,\"start\":49919},{\"end\":49927,\"start\":49926},{\"end\":49933,\"start\":49932},{\"end\":49941,\"start\":49940},{\"end\":49948,\"start\":49947},{\"end\":50281,\"start\":50280},{\"end\":50294,\"start\":50293},{\"end\":50306,\"start\":50305},{\"end\":50692,\"start\":50691},{\"end\":50705,\"start\":50704},{\"end\":50722,\"start\":50721},{\"end\":50735,\"start\":50734},{\"end\":50751,\"start\":50750},{\"end\":50760,\"start\":50759},{\"end\":50762,\"start\":50761},{\"end\":50771,\"start\":50770},{\"end\":50780,\"start\":50779},{\"end\":50792,\"start\":50791},{\"end\":51231,\"start\":51230},{\"end\":51238,\"start\":51237},{\"end\":51248,\"start\":51247},{\"end\":51260,\"start\":51259},{\"end\":51614,\"start\":51613},{\"end\":51624,\"start\":51623},{\"end\":52002,\"start\":52001},{\"end\":52004,\"start\":52003},{\"end\":52012,\"start\":52011},{\"end\":52024,\"start\":52020},{\"end\":52035,\"start\":52034},{\"end\":52045,\"start\":52044},{\"end\":52057,\"start\":52056},{\"end\":52406,\"start\":52405},{\"end\":52418,\"start\":52417},{\"end\":52430,\"start\":52429},{\"end\":52439,\"start\":52438},{\"end\":52825,\"start\":52824},{\"end\":52835,\"start\":52834},{\"end\":52837,\"start\":52836},{\"end\":53134,\"start\":53133},{\"end\":53151,\"start\":53150},{\"end\":53162,\"start\":53161},{\"end\":53169,\"start\":53168},{\"end\":53171,\"start\":53170},{\"end\":53181,\"start\":53180},{\"end\":53193,\"start\":53192},{\"end\":53521,\"start\":53520},{\"end\":53534,\"start\":53533},{\"end\":53546,\"start\":53545},{\"end\":54086,\"start\":54085},{\"end\":54093,\"start\":54092},{\"end\":54100,\"start\":54099},{\"end\":54111,\"start\":54110},{\"end\":54400,\"start\":54399},{\"end\":54412,\"start\":54411},{\"end\":54422,\"start\":54421},{\"end\":54433,\"start\":54432},{\"end\":54756,\"start\":54755},{\"end\":54764,\"start\":54763},{\"end\":54772,\"start\":54771},{\"end\":55114,\"start\":55113},{\"end\":55124,\"start\":55123},{\"end\":55126,\"start\":55125},{\"end\":55136,\"start\":55135},{\"end\":55138,\"start\":55137},{\"end\":55519,\"start\":55518},{\"end\":55530,\"start\":55529},{\"end\":55548,\"start\":55547},{\"end\":55928,\"start\":55927},{\"end\":55937,\"start\":55936},{\"end\":56270,\"start\":56269},{\"end\":56279,\"start\":56278},{\"end\":56281,\"start\":56280},{\"end\":56289,\"start\":56288},{\"end\":56291,\"start\":56290},{\"end\":56298,\"start\":56297},{\"end\":56300,\"start\":56299},{\"end\":56307,\"start\":56306},{\"end\":56309,\"start\":56308},{\"end\":56317,\"start\":56316},{\"end\":56319,\"start\":56318},{\"end\":56328,\"start\":56327},{\"end\":56330,\"start\":56329},{\"end\":56338,\"start\":56337},{\"end\":56340,\"start\":56339},{\"end\":56347,\"start\":56346},{\"end\":56355,\"start\":56354},{\"end\":56357,\"start\":56356},{\"end\":56784,\"start\":56783},{\"end\":56792,\"start\":56791},{\"end\":56798,\"start\":56797},{\"end\":56805,\"start\":56804},{\"end\":57161,\"start\":57160},{\"end\":57163,\"start\":57162},{\"end\":57173,\"start\":57172},{\"end\":57175,\"start\":57174},{\"end\":57185,\"start\":57184},{\"end\":57187,\"start\":57186},{\"end\":57540,\"start\":57539},{\"end\":57807,\"start\":57806},{\"end\":57809,\"start\":57808},{\"end\":57820,\"start\":57819},{\"end\":58077,\"start\":58076},{\"end\":58086,\"start\":58085},{\"end\":58096,\"start\":58095},{\"end\":58107,\"start\":58106},{\"end\":58117,\"start\":58116},{\"end\":58119,\"start\":58118},{\"end\":58127,\"start\":58126},{\"end\":58129,\"start\":58128},{\"end\":58141,\"start\":58140},{\"end\":58143,\"start\":58142},{\"end\":58157,\"start\":58156},{\"end\":58159,\"start\":58158},{\"end\":58170,\"start\":58169},{\"end\":58591,\"start\":58590},{\"end\":58595,\"start\":58592},{\"end\":58607,\"start\":58606},{\"end\":58615,\"start\":58614},{\"end\":58625,\"start\":58624},{\"end\":58636,\"start\":58635},{\"end\":58638,\"start\":58637},{\"end\":58649,\"start\":58648},{\"end\":58651,\"start\":58650},{\"end\":58662,\"start\":58661},{\"end\":58664,\"start\":58663},{\"end\":58675,\"start\":58674},{\"end\":58686,\"start\":58685},{\"end\":58688,\"start\":58687},{\"end\":58697,\"start\":58696},{\"end\":58699,\"start\":58698},{\"end\":59081,\"start\":59080},{\"end\":59092,\"start\":59091},{\"end\":59099,\"start\":59098},{\"end\":59109,\"start\":59108},{\"end\":59122,\"start\":59121},{\"end\":59417,\"start\":59416},{\"end\":59428,\"start\":59427},{\"end\":59436,\"start\":59435},{\"end\":59438,\"start\":59437},{\"end\":59445,\"start\":59444},{\"end\":59853,\"start\":59852},{\"end\":59855,\"start\":59854},{\"end\":59868,\"start\":59867},{\"end\":60216,\"start\":60215},{\"end\":60227,\"start\":60226},{\"end\":60238,\"start\":60237},{\"end\":60248,\"start\":60247},{\"end\":60261,\"start\":60260},{\"end\":60270,\"start\":60269},{\"end\":60272,\"start\":60271},{\"end\":60281,\"start\":60280},{\"end\":60291,\"start\":60290},{\"end\":60596,\"start\":60595},{\"end\":60602,\"start\":60601},{\"end\":60610,\"start\":60609},{\"end\":60990,\"start\":60989},{\"end\":60997,\"start\":60996},{\"end\":61009,\"start\":61004},{\"end\":61016,\"start\":61015},{\"end\":61399,\"start\":61395},{\"end\":61410,\"start\":61409},{\"end\":61425,\"start\":61424},{\"end\":61649,\"start\":61648},{\"end\":61651,\"start\":61650},{\"end\":61660,\"start\":61659},{\"end\":61884,\"start\":61883},{\"end\":61886,\"start\":61885},{\"end\":61895,\"start\":61894},{\"end\":61907,\"start\":61906},{\"end\":61920,\"start\":61919},{\"end\":62197,\"start\":62193},{\"end\":62208,\"start\":62207},{\"end\":62219,\"start\":62218},{\"end\":62456,\"start\":62455},{\"end\":62463,\"start\":62462},{\"end\":62475,\"start\":62474},{\"end\":62760,\"start\":62759},{\"end\":62762,\"start\":62761},{\"end\":62774,\"start\":62773},{\"end\":62776,\"start\":62775},{\"end\":62784,\"start\":62783},{\"end\":63077,\"start\":63076},{\"end\":63079,\"start\":63078},{\"end\":63091,\"start\":63090},{\"end\":63098,\"start\":63097},{\"end\":63339,\"start\":63338},{\"end\":63341,\"start\":63340},{\"end\":63351,\"start\":63350},{\"end\":63562,\"start\":63561},{\"end\":63570,\"start\":63569},{\"end\":63572,\"start\":63571},{\"end\":63903,\"start\":63902},{\"end\":63905,\"start\":63904},{\"end\":63916,\"start\":63915},{\"end\":63918,\"start\":63917},{\"end\":63930,\"start\":63929},{\"end\":63942,\"start\":63941},{\"end\":64239,\"start\":64238},{\"end\":64241,\"start\":64240},{\"end\":64250,\"start\":64249},{\"end\":64260,\"start\":64259},{\"end\":64563,\"start\":64562},{\"end\":64579,\"start\":64578},{\"end\":64581,\"start\":64580},{\"end\":64889,\"start\":64888},{\"end\":64897,\"start\":64896},{\"end\":64905,\"start\":64904},{\"end\":64918,\"start\":64917},{\"end\":65285,\"start\":65284},{\"end\":65300,\"start\":65299},{\"end\":65302,\"start\":65301},{\"end\":65315,\"start\":65314},{\"end\":65317,\"start\":65316},{\"end\":65651,\"start\":65650},{\"end\":65658,\"start\":65657},{\"end\":65670,\"start\":65669},{\"end\":65682,\"start\":65681},{\"end\":66018,\"start\":66017},{\"end\":66031,\"start\":66030},{\"end\":66042,\"start\":66041},{\"end\":66050,\"start\":66049},{\"end\":66065,\"start\":66061},{\"end\":66405,\"start\":66401},{\"end\":66416,\"start\":66412},{\"end\":66423,\"start\":66422},{\"end\":66434,\"start\":66430}]", "bib_author_last_name": "[{\"end\":43654,\"start\":43648},{\"end\":43986,\"start\":43977},{\"end\":43997,\"start\":43992},{\"end\":44275,\"start\":44271},{\"end\":44286,\"start\":44281},{\"end\":44661,\"start\":44646},{\"end\":44672,\"start\":44667},{\"end\":44685,\"start\":44676},{\"end\":44696,\"start\":44689},{\"end\":44706,\"start\":44702},{\"end\":44728,\"start\":44712},{\"end\":44738,\"start\":44732},{\"end\":44750,\"start\":44744},{\"end\":44761,\"start\":44756},{\"end\":45207,\"start\":45194},{\"end\":45219,\"start\":45211},{\"end\":45231,\"start\":45223},{\"end\":45658,\"start\":45651},{\"end\":45672,\"start\":45662},{\"end\":45682,\"start\":45678},{\"end\":45701,\"start\":45686},{\"end\":45715,\"start\":45707},{\"end\":45726,\"start\":45719},{\"end\":45737,\"start\":45732},{\"end\":45747,\"start\":45744},{\"end\":45760,\"start\":45751},{\"end\":46179,\"start\":46171},{\"end\":46190,\"start\":46183},{\"end\":46200,\"start\":46194},{\"end\":46211,\"start\":46208},{\"end\":46625,\"start\":46617},{\"end\":46632,\"start\":46629},{\"end\":46644,\"start\":46636},{\"end\":46656,\"start\":46648},{\"end\":47099,\"start\":47087},{\"end\":47109,\"start\":47105},{\"end\":47125,\"start\":47115},{\"end\":47139,\"start\":47129},{\"end\":47149,\"start\":47143},{\"end\":47158,\"start\":47153},{\"end\":47469,\"start\":47461},{\"end\":47480,\"start\":47473},{\"end\":47494,\"start\":47484},{\"end\":47503,\"start\":47498},{\"end\":47515,\"start\":47507},{\"end\":47526,\"start\":47519},{\"end\":47917,\"start\":47909},{\"end\":47928,\"start\":47923},{\"end\":47939,\"start\":47932},{\"end\":47948,\"start\":47943},{\"end\":48292,\"start\":48287},{\"end\":48300,\"start\":48296},{\"end\":48686,\"start\":48682},{\"end\":48695,\"start\":48690},{\"end\":49009,\"start\":48999},{\"end\":49022,\"start\":49013},{\"end\":49033,\"start\":49028},{\"end\":49416,\"start\":49406},{\"end\":49425,\"start\":49420},{\"end\":49438,\"start\":49429},{\"end\":49451,\"start\":49444},{\"end\":49459,\"start\":49455},{\"end\":49478,\"start\":49463},{\"end\":49491,\"start\":49482},{\"end\":49505,\"start\":49497},{\"end\":49516,\"start\":49511},{\"end\":49924,\"start\":49921},{\"end\":49930,\"start\":49928},{\"end\":49938,\"start\":49934},{\"end\":49945,\"start\":49942},{\"end\":49951,\"start\":49949},{\"end\":50291,\"start\":50282},{\"end\":50303,\"start\":50295},{\"end\":50315,\"start\":50307},{\"end\":50702,\"start\":50693},{\"end\":50719,\"start\":50706},{\"end\":50732,\"start\":50723},{\"end\":50748,\"start\":50736},{\"end\":50757,\"start\":50752},{\"end\":50768,\"start\":50763},{\"end\":50777,\"start\":50772},{\"end\":50789,\"start\":50781},{\"end\":50801,\"start\":50793},{\"end\":51235,\"start\":51232},{\"end\":51245,\"start\":51239},{\"end\":51257,\"start\":51249},{\"end\":51269,\"start\":51261},{\"end\":51621,\"start\":51615},{\"end\":51630,\"start\":51625},{\"end\":52009,\"start\":52005},{\"end\":52018,\"start\":52013},{\"end\":52032,\"start\":52025},{\"end\":52042,\"start\":52036},{\"end\":52054,\"start\":52046},{\"end\":52063,\"start\":52058},{\"end\":52415,\"start\":52407},{\"end\":52427,\"start\":52419},{\"end\":52436,\"start\":52431},{\"end\":52447,\"start\":52440},{\"end\":52832,\"start\":52826},{\"end\":52843,\"start\":52838},{\"end\":53148,\"start\":53135},{\"end\":53159,\"start\":53152},{\"end\":53166,\"start\":53163},{\"end\":53178,\"start\":53172},{\"end\":53190,\"start\":53182},{\"end\":53202,\"start\":53194},{\"end\":53531,\"start\":53522},{\"end\":53543,\"start\":53535},{\"end\":53555,\"start\":53547},{\"end\":54090,\"start\":54087},{\"end\":54097,\"start\":54094},{\"end\":54108,\"start\":54101},{\"end\":54114,\"start\":54112},{\"end\":54409,\"start\":54401},{\"end\":54419,\"start\":54413},{\"end\":54430,\"start\":54423},{\"end\":54439,\"start\":54434},{\"end\":54761,\"start\":54757},{\"end\":54769,\"start\":54765},{\"end\":54789,\"start\":54773},{\"end\":55121,\"start\":55115},{\"end\":55133,\"start\":55127},{\"end\":55144,\"start\":55139},{\"end\":55527,\"start\":55520},{\"end\":55545,\"start\":55531},{\"end\":55562,\"start\":55549},{\"end\":55934,\"start\":55929},{\"end\":55942,\"start\":55938},{\"end\":56276,\"start\":56271},{\"end\":56286,\"start\":56282},{\"end\":56295,\"start\":56292},{\"end\":56304,\"start\":56301},{\"end\":56314,\"start\":56310},{\"end\":56325,\"start\":56320},{\"end\":56335,\"start\":56331},{\"end\":56344,\"start\":56341},{\"end\":56352,\"start\":56348},{\"end\":56361,\"start\":56358},{\"end\":56789,\"start\":56785},{\"end\":56795,\"start\":56793},{\"end\":56802,\"start\":56799},{\"end\":56810,\"start\":56806},{\"end\":57170,\"start\":57164},{\"end\":57182,\"start\":57176},{\"end\":57194,\"start\":57188},{\"end\":57546,\"start\":57541},{\"end\":57817,\"start\":57810},{\"end\":57827,\"start\":57821},{\"end\":58083,\"start\":58078},{\"end\":58093,\"start\":58087},{\"end\":58104,\"start\":58097},{\"end\":58114,\"start\":58108},{\"end\":58124,\"start\":58120},{\"end\":58138,\"start\":58130},{\"end\":58154,\"start\":58144},{\"end\":58167,\"start\":58160},{\"end\":58178,\"start\":58171},{\"end\":58604,\"start\":58596},{\"end\":58612,\"start\":58608},{\"end\":58622,\"start\":58616},{\"end\":58633,\"start\":58626},{\"end\":58646,\"start\":58639},{\"end\":58659,\"start\":58652},{\"end\":58672,\"start\":58665},{\"end\":58683,\"start\":58676},{\"end\":58694,\"start\":58689},{\"end\":58705,\"start\":58700},{\"end\":59089,\"start\":59082},{\"end\":59096,\"start\":59093},{\"end\":59106,\"start\":59100},{\"end\":59119,\"start\":59110},{\"end\":59130,\"start\":59123},{\"end\":59425,\"start\":59418},{\"end\":59433,\"start\":59429},{\"end\":59442,\"start\":59439},{\"end\":59450,\"start\":59446},{\"end\":59865,\"start\":59856},{\"end\":59877,\"start\":59869},{\"end\":60224,\"start\":60217},{\"end\":60235,\"start\":60228},{\"end\":60245,\"start\":60239},{\"end\":60258,\"start\":60249},{\"end\":60267,\"start\":60262},{\"end\":60278,\"start\":60273},{\"end\":60288,\"start\":60282},{\"end\":60302,\"start\":60292},{\"end\":60599,\"start\":60597},{\"end\":60607,\"start\":60603},{\"end\":60614,\"start\":60611},{\"end\":60994,\"start\":60991},{\"end\":61002,\"start\":60998},{\"end\":61013,\"start\":61010},{\"end\":61025,\"start\":61017},{\"end\":61407,\"start\":61400},{\"end\":61422,\"start\":61411},{\"end\":61436,\"start\":61426},{\"end\":61657,\"start\":61652},{\"end\":61666,\"start\":61661},{\"end\":61892,\"start\":61887},{\"end\":61904,\"start\":61896},{\"end\":61917,\"start\":61908},{\"end\":61927,\"start\":61921},{\"end\":62205,\"start\":62198},{\"end\":62216,\"start\":62209},{\"end\":62225,\"start\":62220},{\"end\":62460,\"start\":62457},{\"end\":62472,\"start\":62464},{\"end\":62484,\"start\":62476},{\"end\":62771,\"start\":62763},{\"end\":62781,\"start\":62777},{\"end\":62795,\"start\":62785},{\"end\":63088,\"start\":63080},{\"end\":63095,\"start\":63092},{\"end\":63109,\"start\":63099},{\"end\":63348,\"start\":63342},{\"end\":63354,\"start\":63352},{\"end\":63567,\"start\":63563},{\"end\":63578,\"start\":63573},{\"end\":63913,\"start\":63906},{\"end\":63927,\"start\":63919},{\"end\":63939,\"start\":63931},{\"end\":63950,\"start\":63943},{\"end\":64247,\"start\":64242},{\"end\":64257,\"start\":64251},{\"end\":64266,\"start\":64261},{\"end\":64576,\"start\":64564},{\"end\":64587,\"start\":64582},{\"end\":64894,\"start\":64890},{\"end\":64902,\"start\":64898},{\"end\":64915,\"start\":64906},{\"end\":64935,\"start\":64919},{\"end\":65297,\"start\":65286},{\"end\":65312,\"start\":65303},{\"end\":65334,\"start\":65318},{\"end\":65655,\"start\":65652},{\"end\":65667,\"start\":65659},{\"end\":65679,\"start\":65671},{\"end\":65691,\"start\":65683},{\"end\":66028,\"start\":66019},{\"end\":66039,\"start\":66032},{\"end\":66047,\"start\":66043},{\"end\":66059,\"start\":66051},{\"end\":66072,\"start\":66066},{\"end\":66410,\"start\":66406},{\"end\":66420,\"start\":66417},{\"end\":66428,\"start\":66424},{\"end\":66438,\"start\":66435}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16308483},\"end\":43884,\"start\":43569},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4320045},\"end\":44179,\"start\":43886},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15570759},\"end\":44548,\"start\":44181},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2934551},\"end\":45065,\"start\":44550},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3206129},\"end\":45539,\"start\":45067},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221198586},\"end\":46057,\"start\":45541},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":22076362},\"end\":46478,\"start\":46059},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":27835175},\"end\":47020,\"start\":46480},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3531854},\"end\":47367,\"start\":47022},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":89621357},\"end\":47824,\"start\":47369},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":231786459},\"end\":48190,\"start\":47826},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17481305},\"end\":48603,\"start\":48192},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8080272},\"end\":48887,\"start\":48605},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5275290},\"end\":49298,\"start\":48889},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":91320213},\"end\":49823,\"start\":49300},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":226201892},\"end\":50176,\"start\":49825},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219947628},\"end\":50594,\"start\":50178},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":235718402},\"end\":51127,\"start\":50596},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220603192},\"end\":51528,\"start\":51129},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":214048703},\"end\":51900,\"start\":51530},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":20140745},\"end\":52332,\"start\":51902},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":28105079},\"end\":52715,\"start\":52334},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":204742059},\"end\":53040,\"start\":52717},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":233460170},\"end\":53425,\"start\":53042},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":222341624},\"end\":53990,\"start\":53427},{\"attributes\":{\"doi\":\"arXiv:2103.03621\",\"id\":\"b25\"},\"end\":54291,\"start\":53992},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":231846490},\"end\":54679,\"start\":54293},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":196651196},\"end\":54969,\"start\":54681},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":35719008},\"end\":55439,\"start\":54971},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11661123},\"end\":55829,\"start\":55441},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16757203},\"end\":56191,\"start\":55831},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":214810317},\"end\":56693,\"start\":56193},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":225100809},\"end\":57068,\"start\":56695},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":24404227},\"end\":57422,\"start\":57070},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":34420853},\"end\":57782,\"start\":57424},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":205439817},\"end\":57963,\"start\":57784},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":208252106},\"end\":58497,\"start\":57965},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12700766},\"end\":58995,\"start\":58499},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":195828879},\"end\":59335,\"start\":58997},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":16655243},\"end\":59707,\"start\":59337},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":122543349},\"end\":60186,\"start\":59709},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13756489},\"end\":60560,\"start\":60188},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":140309863},\"end\":60943,\"start\":60562},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":49867180},\"end\":61317,\"start\":60945},{\"attributes\":{\"doi\":\"arXiv:1511.07289\",\"id\":\"b44\"},\"end\":61605,\"start\":61319},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":40980861},\"end\":61816,\"start\":61607},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":5110638},\"end\":62134,\"start\":61818},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11375499},\"end\":62406,\"start\":62136},{\"attributes\":{\"doi\":\"10.5281/zenodo.3997352\",\"id\":\"b48\"},\"end\":62702,\"start\":62408},{\"attributes\":{\"doi\":\"10.5281/zenodo.1199011\",\"id\":\"b49\"},\"end\":62993,\"start\":62704},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5319132},\"end\":63292,\"start\":62995},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b51\"},\"end\":63463,\"start\":63294},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":11814655},\"end\":63794,\"start\":63465},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":8288419},\"end\":64167,\"start\":63796},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":4551674},\"end\":64522,\"start\":64169},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":223009},\"end\":64767,\"start\":64524},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":11689536},\"end\":65180,\"start\":64769},{\"attributes\":{\"id\":\"b57\"},\"end\":65548,\"start\":65182},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":25324045},\"end\":65950,\"start\":65550},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":10908715},\"end\":66308,\"start\":65952},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":208037173},\"end\":66739,\"start\":66310}]", "bib_title": "[{\"end\":43642,\"start\":43569},{\"end\":43973,\"start\":43886},{\"end\":44267,\"start\":44181},{\"end\":44640,\"start\":44550},{\"end\":45190,\"start\":45067},{\"end\":45647,\"start\":45541},{\"end\":46167,\"start\":46059},{\"end\":46613,\"start\":46480},{\"end\":47083,\"start\":47022},{\"end\":47457,\"start\":47369},{\"end\":47905,\"start\":47826},{\"end\":48283,\"start\":48192},{\"end\":48678,\"start\":48605},{\"end\":48995,\"start\":48889},{\"end\":49402,\"start\":49300},{\"end\":49917,\"start\":49825},{\"end\":50278,\"start\":50178},{\"end\":50689,\"start\":50596},{\"end\":51228,\"start\":51129},{\"end\":51611,\"start\":51530},{\"end\":51999,\"start\":51902},{\"end\":52403,\"start\":52334},{\"end\":52822,\"start\":52717},{\"end\":53131,\"start\":53042},{\"end\":53518,\"start\":53427},{\"end\":54397,\"start\":54293},{\"end\":54753,\"start\":54681},{\"end\":55111,\"start\":54971},{\"end\":55516,\"start\":55441},{\"end\":55925,\"start\":55831},{\"end\":56267,\"start\":56193},{\"end\":56781,\"start\":56695},{\"end\":57158,\"start\":57070},{\"end\":57537,\"start\":57424},{\"end\":57804,\"start\":57784},{\"end\":58074,\"start\":57965},{\"end\":58588,\"start\":58499},{\"end\":59078,\"start\":58997},{\"end\":59414,\"start\":59337},{\"end\":59850,\"start\":59709},{\"end\":60213,\"start\":60188},{\"end\":60593,\"start\":60562},{\"end\":60987,\"start\":60945},{\"end\":61646,\"start\":61607},{\"end\":61881,\"start\":61818},{\"end\":62191,\"start\":62136},{\"end\":63074,\"start\":62995},{\"end\":63559,\"start\":63465},{\"end\":63900,\"start\":63796},{\"end\":64236,\"start\":64169},{\"end\":64560,\"start\":64524},{\"end\":64886,\"start\":64769},{\"end\":65648,\"start\":65550},{\"end\":66015,\"start\":65952},{\"end\":66399,\"start\":66310}]", "bib_author": "[{\"end\":43656,\"start\":43644},{\"end\":43988,\"start\":43975},{\"end\":43999,\"start\":43988},{\"end\":44277,\"start\":44269},{\"end\":44288,\"start\":44277},{\"end\":44663,\"start\":44642},{\"end\":44674,\"start\":44663},{\"end\":44687,\"start\":44674},{\"end\":44698,\"start\":44687},{\"end\":44708,\"start\":44698},{\"end\":44730,\"start\":44708},{\"end\":44740,\"start\":44730},{\"end\":44752,\"start\":44740},{\"end\":44763,\"start\":44752},{\"end\":45209,\"start\":45192},{\"end\":45221,\"start\":45209},{\"end\":45233,\"start\":45221},{\"end\":45660,\"start\":45649},{\"end\":45674,\"start\":45660},{\"end\":45684,\"start\":45674},{\"end\":45703,\"start\":45684},{\"end\":45717,\"start\":45703},{\"end\":45728,\"start\":45717},{\"end\":45739,\"start\":45728},{\"end\":45749,\"start\":45739},{\"end\":45762,\"start\":45749},{\"end\":46181,\"start\":46169},{\"end\":46192,\"start\":46181},{\"end\":46202,\"start\":46192},{\"end\":46213,\"start\":46202},{\"end\":46627,\"start\":46615},{\"end\":46634,\"start\":46627},{\"end\":46646,\"start\":46634},{\"end\":46658,\"start\":46646},{\"end\":47101,\"start\":47085},{\"end\":47111,\"start\":47101},{\"end\":47127,\"start\":47111},{\"end\":47141,\"start\":47127},{\"end\":47151,\"start\":47141},{\"end\":47160,\"start\":47151},{\"end\":47471,\"start\":47459},{\"end\":47482,\"start\":47471},{\"end\":47496,\"start\":47482},{\"end\":47505,\"start\":47496},{\"end\":47517,\"start\":47505},{\"end\":47528,\"start\":47517},{\"end\":47919,\"start\":47907},{\"end\":47930,\"start\":47919},{\"end\":47941,\"start\":47930},{\"end\":47950,\"start\":47941},{\"end\":48294,\"start\":48285},{\"end\":48302,\"start\":48294},{\"end\":48688,\"start\":48680},{\"end\":48697,\"start\":48688},{\"end\":49011,\"start\":48997},{\"end\":49024,\"start\":49011},{\"end\":49035,\"start\":49024},{\"end\":49418,\"start\":49404},{\"end\":49427,\"start\":49418},{\"end\":49440,\"start\":49427},{\"end\":49453,\"start\":49440},{\"end\":49461,\"start\":49453},{\"end\":49480,\"start\":49461},{\"end\":49493,\"start\":49480},{\"end\":49507,\"start\":49493},{\"end\":49518,\"start\":49507},{\"end\":49926,\"start\":49919},{\"end\":49932,\"start\":49926},{\"end\":49940,\"start\":49932},{\"end\":49947,\"start\":49940},{\"end\":49953,\"start\":49947},{\"end\":50293,\"start\":50280},{\"end\":50305,\"start\":50293},{\"end\":50317,\"start\":50305},{\"end\":50704,\"start\":50691},{\"end\":50721,\"start\":50704},{\"end\":50734,\"start\":50721},{\"end\":50750,\"start\":50734},{\"end\":50759,\"start\":50750},{\"end\":50770,\"start\":50759},{\"end\":50779,\"start\":50770},{\"end\":50791,\"start\":50779},{\"end\":50803,\"start\":50791},{\"end\":51237,\"start\":51230},{\"end\":51247,\"start\":51237},{\"end\":51259,\"start\":51247},{\"end\":51271,\"start\":51259},{\"end\":51623,\"start\":51613},{\"end\":51632,\"start\":51623},{\"end\":52011,\"start\":52001},{\"end\":52020,\"start\":52011},{\"end\":52034,\"start\":52020},{\"end\":52044,\"start\":52034},{\"end\":52056,\"start\":52044},{\"end\":52065,\"start\":52056},{\"end\":52417,\"start\":52405},{\"end\":52429,\"start\":52417},{\"end\":52438,\"start\":52429},{\"end\":52449,\"start\":52438},{\"end\":52834,\"start\":52824},{\"end\":52845,\"start\":52834},{\"end\":53150,\"start\":53133},{\"end\":53161,\"start\":53150},{\"end\":53168,\"start\":53161},{\"end\":53180,\"start\":53168},{\"end\":53192,\"start\":53180},{\"end\":53204,\"start\":53192},{\"end\":53533,\"start\":53520},{\"end\":53545,\"start\":53533},{\"end\":53557,\"start\":53545},{\"end\":54092,\"start\":54085},{\"end\":54099,\"start\":54092},{\"end\":54110,\"start\":54099},{\"end\":54116,\"start\":54110},{\"end\":54411,\"start\":54399},{\"end\":54421,\"start\":54411},{\"end\":54432,\"start\":54421},{\"end\":54441,\"start\":54432},{\"end\":54763,\"start\":54755},{\"end\":54771,\"start\":54763},{\"end\":54791,\"start\":54771},{\"end\":55123,\"start\":55113},{\"end\":55135,\"start\":55123},{\"end\":55146,\"start\":55135},{\"end\":55529,\"start\":55518},{\"end\":55547,\"start\":55529},{\"end\":55564,\"start\":55547},{\"end\":55936,\"start\":55927},{\"end\":55944,\"start\":55936},{\"end\":56278,\"start\":56269},{\"end\":56288,\"start\":56278},{\"end\":56297,\"start\":56288},{\"end\":56306,\"start\":56297},{\"end\":56316,\"start\":56306},{\"end\":56327,\"start\":56316},{\"end\":56337,\"start\":56327},{\"end\":56346,\"start\":56337},{\"end\":56354,\"start\":56346},{\"end\":56363,\"start\":56354},{\"end\":56791,\"start\":56783},{\"end\":56797,\"start\":56791},{\"end\":56804,\"start\":56797},{\"end\":56812,\"start\":56804},{\"end\":57172,\"start\":57160},{\"end\":57184,\"start\":57172},{\"end\":57196,\"start\":57184},{\"end\":57548,\"start\":57539},{\"end\":57819,\"start\":57806},{\"end\":57829,\"start\":57819},{\"end\":58085,\"start\":58076},{\"end\":58095,\"start\":58085},{\"end\":58106,\"start\":58095},{\"end\":58116,\"start\":58106},{\"end\":58126,\"start\":58116},{\"end\":58140,\"start\":58126},{\"end\":58156,\"start\":58140},{\"end\":58169,\"start\":58156},{\"end\":58180,\"start\":58169},{\"end\":58606,\"start\":58590},{\"end\":58614,\"start\":58606},{\"end\":58624,\"start\":58614},{\"end\":58635,\"start\":58624},{\"end\":58648,\"start\":58635},{\"end\":58661,\"start\":58648},{\"end\":58674,\"start\":58661},{\"end\":58685,\"start\":58674},{\"end\":58696,\"start\":58685},{\"end\":58707,\"start\":58696},{\"end\":59091,\"start\":59080},{\"end\":59098,\"start\":59091},{\"end\":59108,\"start\":59098},{\"end\":59121,\"start\":59108},{\"end\":59132,\"start\":59121},{\"end\":59427,\"start\":59416},{\"end\":59435,\"start\":59427},{\"end\":59444,\"start\":59435},{\"end\":59452,\"start\":59444},{\"end\":59867,\"start\":59852},{\"end\":59879,\"start\":59867},{\"end\":60226,\"start\":60215},{\"end\":60237,\"start\":60226},{\"end\":60247,\"start\":60237},{\"end\":60260,\"start\":60247},{\"end\":60269,\"start\":60260},{\"end\":60280,\"start\":60269},{\"end\":60290,\"start\":60280},{\"end\":60304,\"start\":60290},{\"end\":60601,\"start\":60595},{\"end\":60609,\"start\":60601},{\"end\":60616,\"start\":60609},{\"end\":60996,\"start\":60989},{\"end\":61004,\"start\":60996},{\"end\":61015,\"start\":61004},{\"end\":61027,\"start\":61015},{\"end\":61409,\"start\":61395},{\"end\":61424,\"start\":61409},{\"end\":61438,\"start\":61424},{\"end\":61659,\"start\":61648},{\"end\":61668,\"start\":61659},{\"end\":61894,\"start\":61883},{\"end\":61906,\"start\":61894},{\"end\":61919,\"start\":61906},{\"end\":61929,\"start\":61919},{\"end\":62207,\"start\":62193},{\"end\":62218,\"start\":62207},{\"end\":62227,\"start\":62218},{\"end\":62462,\"start\":62455},{\"end\":62474,\"start\":62462},{\"end\":62486,\"start\":62474},{\"end\":62773,\"start\":62759},{\"end\":62783,\"start\":62773},{\"end\":62797,\"start\":62783},{\"end\":63090,\"start\":63076},{\"end\":63097,\"start\":63090},{\"end\":63111,\"start\":63097},{\"end\":63350,\"start\":63338},{\"end\":63356,\"start\":63350},{\"end\":63569,\"start\":63561},{\"end\":63580,\"start\":63569},{\"end\":63915,\"start\":63902},{\"end\":63929,\"start\":63915},{\"end\":63941,\"start\":63929},{\"end\":63952,\"start\":63941},{\"end\":64249,\"start\":64238},{\"end\":64259,\"start\":64249},{\"end\":64268,\"start\":64259},{\"end\":64578,\"start\":64562},{\"end\":64589,\"start\":64578},{\"end\":64896,\"start\":64888},{\"end\":64904,\"start\":64896},{\"end\":64917,\"start\":64904},{\"end\":64937,\"start\":64917},{\"end\":65299,\"start\":65284},{\"end\":65314,\"start\":65299},{\"end\":65336,\"start\":65314},{\"end\":65657,\"start\":65650},{\"end\":65669,\"start\":65657},{\"end\":65681,\"start\":65669},{\"end\":65693,\"start\":65681},{\"end\":66030,\"start\":66017},{\"end\":66041,\"start\":66030},{\"end\":66049,\"start\":66041},{\"end\":66061,\"start\":66049},{\"end\":66074,\"start\":66061},{\"end\":66412,\"start\":66401},{\"end\":66422,\"start\":66412},{\"end\":66430,\"start\":66422},{\"end\":66440,\"start\":66430}]", "bib_venue": "[{\"end\":49993,\"start\":49977},{\"end\":53726,\"start\":53650},{\"end\":60771,\"start\":60702},{\"end\":61142,\"start\":61093},{\"end\":43704,\"start\":43656},{\"end\":44005,\"start\":43999},{\"end\":44335,\"start\":44288},{\"end\":44778,\"start\":44763},{\"end\":45276,\"start\":45233},{\"end\":45772,\"start\":45762},{\"end\":46242,\"start\":46213},{\"end\":46724,\"start\":46658},{\"end\":47170,\"start\":47160},{\"end\":47571,\"start\":47528},{\"end\":47993,\"start\":47950},{\"end\":48373,\"start\":48302},{\"end\":48721,\"start\":48697},{\"end\":49067,\"start\":49035},{\"end\":49536,\"start\":49518},{\"end\":49975,\"start\":49953},{\"end\":50360,\"start\":50317},{\"end\":50834,\"start\":50803},{\"end\":51300,\"start\":51271},{\"end\":51695,\"start\":51632},{\"end\":52088,\"start\":52065},{\"end\":52496,\"start\":52449},{\"end\":52855,\"start\":52845},{\"end\":53209,\"start\":53204},{\"end\":53648,\"start\":53557},{\"end\":54083,\"start\":53992},{\"end\":54464,\"start\":54441},{\"end\":54801,\"start\":54791},{\"end\":55178,\"start\":55146},{\"end\":55611,\"start\":55564},{\"end\":55987,\"start\":55944},{\"end\":56406,\"start\":56363},{\"end\":56855,\"start\":56812},{\"end\":57219,\"start\":57196},{\"end\":57580,\"start\":57548},{\"end\":57848,\"start\":57829},{\"end\":58205,\"start\":58180},{\"end\":58713,\"start\":58707},{\"end\":59142,\"start\":59132},{\"end\":59495,\"start\":59452},{\"end\":59922,\"start\":59879},{\"end\":60353,\"start\":60304},{\"end\":60700,\"start\":60616},{\"end\":61091,\"start\":61027},{\"end\":61393,\"start\":61319},{\"end\":61688,\"start\":61668},{\"end\":61950,\"start\":61929},{\"end\":62243,\"start\":62227},{\"end\":62453,\"start\":62408},{\"end\":62757,\"start\":62704},{\"end\":63121,\"start\":63111},{\"end\":63336,\"start\":63294},{\"end\":63603,\"start\":63580},{\"end\":63958,\"start\":63952},{\"end\":64319,\"start\":64268},{\"end\":64620,\"start\":64589},{\"end\":64953,\"start\":64937},{\"end\":65282,\"start\":65182},{\"end\":65722,\"start\":65693},{\"end\":66105,\"start\":66074},{\"end\":66497,\"start\":66440}]"}}}, "year": 2023, "month": 12, "day": 17}