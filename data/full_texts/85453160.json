{"id": 85453160, "updated": "2023-09-28 04:16:14.569", "metadata": {"title": "Learning Monocular Visual Odometry through Geometry-Aware Curriculum Learning", "authors": "[{\"first\":\"Muhamad\",\"last\":\"Saputra\",\"middle\":[\"Risqi\",\"U.\"]},{\"first\":\"Pedro\",\"last\":\"de Gusmao\",\"middle\":[\"P.\",\"B.\"]},{\"first\":\"Sen\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Markham\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]}]", "venue": "2019 International Conference on Robotics and Automation (ICRA)", "journal": "2019 International Conference on Robotics and Automation (ICRA)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Inspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.10543", "mag": "2964019655", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/SaputraGWMT19", "doi": "10.1109/icra.2019.8793581"}}, "content": {"source": {"pdf_hash": "53c9618eb5a1d00f27cf58121a485118d4b206db", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1903.10543", "status": "GREEN"}}, "grobid": {"id": "ed17358e96dddcf785a4daee71da40ce57ec6270", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/53c9618eb5a1d00f27cf58121a485118d4b206db.txt", "contents": "\nLearning Monocular Visual Odometry through Geometry-Aware Curriculum Learning\n\n\nMuhamad Risqi \nU Saputra \nPedro P B De Gusmao \nSen Wang \nAndrew Markham \nNiki Trigoni \nLearning Monocular Visual Odometry through Geometry-Aware Curriculum Learning\n\nInspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.\n\nI. INTRODUCTION\n\nVisual Odometry (VO) is the task of estimating an agent's pose and trajectory from a sequence of images. This problem has interested researchers from both robotics and computer vision communities for several decades. Conventional VO methods rely on finding feature correspondences between consecutive frames and leverage multi-view geometry technique [1]. Despite its good performances, these feature-based approaches are very sensitive to noise, outliers, and dynamic objects [2]. Typical approach to tackle these drawbacks is accomplished by manually fine-tuning the algorithm parameters for different cases. However, a new paradigm based on Deep Neural Networks (DNNs) has recently emerged to alleviate the manual tuning problems by directly learning the model parameters from the data. Results from [3], [4], [5], [6], [7] show that deep learning-based VO can yield comparable accuracy to the conventional approaches.\n\nState-of-the-art deep learning-based VO typically minimizes the relative transformation loss as the objective function [4]. Minimizing frame-to-frame relative loss generally can provide reasonable trajectory estimation, but it does not guarantee the consistency of the composed transformation when integrating those relative estimates. Adding the compositional transformation loss in the objective function is a natural way to introduce the consistency to the network. However, our experiments suggest that training deep learning-based VO using compositional transformation loss is hard to converge. Our intuition is that it is too difficult for 1 Muhamad Risqi U. Saputra the network to learn directly the complex geometry of composing the 6 Degree-of-Freedom (DoF) camera poses since the error of the predictions can be largely accumulated. An intuitive way to alleviate the difficulty of training complex geometry problem is by starting the learning process from an easier geometry task and then gradually increasing the difficulty of the task.\n\nThe idea of learning from small or easy tasks and progressively increasing the difficulties has been studied in the context of Curriculum Learning (CL). Inspired by the cognitive process of humans and animals, Bengio et al. [8] proposed CL as a strategy to improve the convergence speed and generalization ability of a machine learning model by learning through highly organized or meaningful order of examples. In this paper, we study whether a similar learning strategy can be applied for estimating the complex geometry of monocular VO. In particular, we propose a deep neural network framework with geometry-aware objective function for learning monocular VO in an end-to-end manner and employ the CL strategy to gradually learn the proposed objective from a simpler objective. Our specific contributions are listed as follows:\n\n\u2022 We present the first curriculum learning strategy for learning the geometry problem of monocular visual odometry, by gradually making the learning objective more difficult during training. \u2022 We propose a novel geometry-aware objective function by jointly optimizing relative transformation and its composition over small windows via bounded pose regression loss. \u2022 We design a network architecture, dubbed CL-VO, which consists of cascade optical flow network and recurrent networks with a differentiable windowed composition layer. \u2022 We evaluate the proposed approach on three datasets (2 public and 1 self-collected) and show that our method significantly outperforms state-of-the-art feature-based and learning-based VO approaches.\n\n\nII. RELATED WORK\n\n\nA. Feature-based VO\n\nThe feature-based VO pipeline generally starts by finding salient features, such as corners or blobs, and matching these features across frames. Using these feature correspondences, the camera ego motion can be estimated through the multiple view geometry principle. The first work on VO was proposed in 2004 by Nister in his landmark paper [9]. Subsequently, many variants were developed such as [10], [11], or VISO2 [12]. Estimating VO through feature-based approaches can be very accurate as it naturally follows the geometry of the camera and the captured scene. However, it can lack robustness due to noisy feature correspondences.\n\n\nB. Learning-based VO\n\nSince the advancement of DNNs, learning-based approaches are gaining more traction in solving computer vision tasks including VO. These approaches infer the camera pose by learning directly from real image data. Early works include Conda et al. [13] which fed stereo images to a Convolutional Neural Network (CNN) to estimate the velocities and orientations of the camera through softmax-based classification. Flowdometry [3] casted the VO problem as a regression problem by using FlowNet [14] to extract optical flow features and a fully connected layer to predict camera translation and rotation. The state-of-the-art approaches like DeepVO [4] and ESP-VO [5] do not only utilize CNNs as the main building blocks, but also incorporate Recurrent Neural Networks (RNNs), to implicitly model the sequential motion dynamics of the image sequences. By not relying on finding feature correspondences, these approaches can yield more robust results in a variety of VO dataset [4], [5], [7].\n\n\nC. Curriculum Learning\n\nCurriculum Learning (CL) was proposed by Bengio et al. [8] to formalize the idea of learning through a meaningful order of examples or concepts, which mimics how humans and animals learn. However, the basic idea of starting small or simple actually dates back to 1993 when Elman [15] successfully trained a DNN to recognize a simple grammar by increasing the complexity of the task. Bengio's work [8] confirmed Elman's findings and showed that a well chosen CL strategy can improve the generalization ability of a DNN model. This idea was further improved by [16] through Self-Paced Learning (SPL), in which the curriculum is learned during training rather than determined by prior knowledge. Jiang et al. [17] then combined both idea of CL and SPL through Self-Paced Curriculum Learning (SPCL). SPCL takes into account both prior knowledge and the learning progress during training in constructing the curriculum. The application of CL and its improvement includes action detection [18], dictionary learning [19], domain adaptation [20], and object tracking [21], but none of them tackle VO estimation where it is more difficult to differentiate between easy and hard examples or tasks.\n\n\nIII. PROPOSED APPROACH\n\n\nA. Learning Ego-motion with DNNs\n\nThe general approach to VO estimates a sequence of relative pose transformations {p t t\u22121 } \u2282 SE (3), from pairs of consecutive images {I t\u22121 , I t }. The cumulative composition of these estimations generates a global trajectory with respect to the starting position i.e.\np t =p t t\u22121 \u2295 ... \u2295p 2 1 \u2295p 1(1)\nwhere \u2295 represents the pose composition operation. While conventional methods require the use of handcrafted features and multiple view geometry techniques, DNN approaches work directly with raw image sequences by training the network in an end-to-end manner. Formally, given two concatenated images I t\u22121,t \u2208 IR 2\u00d7(w\u00d7h\u00d7c) at times t \u2212 1 and t, where w, h, and c are the image width, height, and channels respectively, DNNs learn the following mapping function to regress the 6-DoF camera pose:\nDNNs :{(IR 2\u00d7(w\u00d7h\u00d7c) ) 1:N } \u2192 {(IR 6 ) 1:N } (2)\nwhere N is the total number of consecutive image pairs.\n\n\nB. Enforcing Geometric Constraints\n\nDuring the training process, standard DNNs for VO estimation typically minimize relative transformation error between two consecutive frames. However, the ground truth pose is usually available as the composition of these relative transformations defining a sequence of global poses. In order to fully exploit both relative and composite transformation information, we need to jointly optimize these terms. Instead of directly placing relative and composite terms together, we propose to utilize the composed transformation as a constraint for the relative loss term. We only add the composite loss when its value at time t is larger than it was at time t\u22121. This means that the network does not have to minimize the composite loss when the integration of relative poses at time t yields more accurate absolute pose. Moreover, in order to reduce the accumulative errors, we only minimize the composite loss over small, bounded windows. We refer to this loss function as bounded pose regression loss.\n\nEquations (3)- (6) show this bounded loss, where N is the number of images. L rel is the relative loss that measures pose errors between consecutive frames, while L com is the composite loss which accounts for errors over a small window. The coefficients \u03b1 is used to balance both terms.\n\nThe pose error defined in Equation 6 compares the estimated translationt and rotationr vectors (encapsulated in p) with their respective ground truth values. We also use \u03b4 and \u03b6 to weigh the translation and rotation terms in relative loss as seen in [22], [4].\nL total = N t=1 \u03b1L rel + (1 \u2212 \u03b1)L com (3) L rel = L p t t\u22121 (4) L com = L p t t\u2212w , if L p t t\u2212w > L(p t\u22121 t\u2212w\u22121 ) 0, otherwise (5) L p j i = \u03b4 t j i \u2212 t j i 2 + \u03b6 r j i \u2212 r j i 2 (6)\n\nC. Geometry Aware Curriculum Learning\n\nThe bounded pose regression loss can blend together relative and composite transformation loss. However, it has been discovered by [23] and confirmed in our experiments that training VO using composite transformation loss is difficult to converge due to the accumulative error of predictions. Fig. 2 shows normalized translation and rotation errors for different value of \u03b1 in (3) in the first training stage. It can be seen that training a DNN using only composite loss (\u03b1 = 0) leads to very large translation and rotation errors compared to when relative loss is also incorporated (\u03b1 > 0). The best performance is even achieved by training using relative loss only (\u03b1 = 1), which indicates the difficulty in training with relative and composite losses right from the start. This motivates the utilization of Curriculum Learning (CL) where the learning process starts from the simplest objective and then increasing its difficulty. We refer to this mechanism as Geometry Aware Curriculum Learning (GA-CL).  In the first stage of GA-CL, we start the training process by predicting a reasonable relative transformation (as suggested from Fig. 2). This can be seen as minimizing the bounded pose regression loss from (3)-(6) with \u03b1 = 1. During the second stage, once the network has learned to produce reasonable relative transformations (as the validation loss no longer decreases), we may reveal more information to the network by gradually decreasing \u03b1 so as to equalize relative and composite loss (\u03b1 = 0.5). In the final stage, we put more emphasize on the composite loss 0 < \u03b1 < 0.5 such that the network can learn consistent composite transformation. \n\n\nD. Network Architecture\n\nThe network architecture, dubbed CL-VO, is depicted in Fig. 1 and is mainly composed of a feature extractor and a pose regressor. The feature extractor is essentially a CNN aimed to learn optical-flow like features for VO estimation. We construct a cascade optical flow network which refines optical flow estimation subsequently from the previous subnetwork for providing more detail flow estimation. We adopt FlowNet2-C [24] for the 1st network and Flownet2-S [24] for the 2nd and 3rd network. For producing the latent variables that can be directly consumed by the pose regressor, we remove the refinement part for the last optical flow network.\n\nThe pose regressor part consists of two recurrent layers, in particular two Long Short Term Memory (LSTM) [25] layers, followed by fully connected layers to estimate 6-DoF camera poses. Compared to directly using a fully connected layer for pose regressor, as seen in [3] and [22], LSTM is more suitable to learn the long term dependencies of camera pose since it can maintain its hidden state over time. The LSTM operation can be formulated as follows:\n\uf8ee \uf8ef \uf8ef \uf8f0 i f o g \uf8f9 \uf8fa \uf8fa \uf8fb = \uf8ee \uf8ef \uf8ef \uf8f0 sigm sigm sigm tanh \uf8f9 \uf8fa \uf8fa \uf8fb W (l) lstm h (l\u22121) t h (l) t\u22121 ,(7)\nc (l)\nt = f c (l) t\u22121 + i g,(8)h (l) t = o tanh(c (l) t ),(9)\nwhere W (l) lstm \u2208 IR 4n (l) \u00d7(n (l\u22121) +n (l) ) is the weight matrix for layer l, n is tensor dimension, t = 1, ..., T is the timestep, and the vector h\n(l) t \u2208 IR n (l)\nis its hidden state at step t and layer l. Vector h (0) t is equal to the input x t at step t. Operators sigm, tanh, and denote sigmoid function, hyperbolic tangent, and element-wise multiplication respectively.\n\nFor composing the relative transformation from a certain number of previous frames, we construct a differentiable custom windowed composition layer as seen in Fig. 3. A windowed composition layer concatenates the current frameto-frame camera ego motion with the previous ego motion for a predefined number of window w as follow\u015d\np t w =p t t\u22121 \u2295 ... \u2295p t\u2212w+1 t\u2212w \u2295p t\u2212w .(10)\nIV. EXPERIMENTS\n\n\nA. Datasets\n\nThree datasets, consist of two public datasets and one self-collected dataset, are used in our experiments. The first dataset is KITTI autonomous driving dataset [26], a wellknown public dataset for evaluating VO and SLAM algorithms. We use KITTI odometry data Sequences 00-10 for quantitative evaluation and Sequences 11-21 for qualitative evaluation. Although the dataset provides stereo imagery, we only use the left image for testing monocular VO algorithms. The second dataset is the Malaga urban dataset [27], which is also collected in a driving scenario. This dataset is only used to test a pre-trained model without training or finetuning. Similar to KITTI, we only utilize the left camera for testing monocular VO methods. The last dataset is our self-collected human motion data imitating firefighter walking pattern. This dataset is collected in an indoor environment that consists of a corridor and a large room for approximately 1.5 hours. We use uEye global shutter camera mounted in a helmet, with VGA resolution (640 \u00d7 480) which runs at 30 Hz. The ground truth is taken from a ViCon Motion Capture system with approximately 1cm accuracy. The firefighter walking motion contains sweeping hand and foot for inspecting obstacles in front of the user, which is very challenging for monocular VO since it creates a zigzag motion pattern. Moreover, the moving hand occasionally obstructs some parts of the image.\n\n\nB. Competing Approaches\n\nTo evaluate the performance of CL-VO, we compare our method with the state-of-the-art feature-based and learningbased VO methods, namely VISO2 [12], ORB-SLAM [28], and DeepVO [4]. For VISO2, we use the monocular version (VISO2-M) for quantitative evaluation while we utilize the stereo version (VISO2-S) for qualitative comparison. We set the height of the camera in VISO2-M as described on each dataset paper to estimate the scale of the prediction. For ORB-SLAM, we used the result from [5] for quantitative evaluation. As for DeepVO, we constructed the DeepVO model with the same architecture and parameters as described in the paper. For each dataset, we trained DeepVO with the same settings as CL-VO (e.g. total training sequences, validation data, total epochs, optimizer, learning rate, etc.). We also train DeepVO with GA-CL to see how much improvement GA-CL can bring to DeepVO.\n\n\nC. Implementation and Augmentation\n\nWe implemented CL-VO using Tensorflow and Keras, and ran the training code on a NVIDIA TITAN V GPU. Before training, we computed the dataset mean and used it to normalize the image intensity. In order to provide more trajectory variations, we generated sequences with random start and end points, and random lengths. In every epoch, we constructed 10 random trajectories for each training sequence. The training can extend to 200 epochs for each training stage which takes around 10 hours, or can be stopped earlier if the validation loss shows no improvement. We used the Adam optimizer with 1e \u2212 3 as the initial learning rate. We also applied Dropout [29] with 0.2 dropout rate for regularizing the network. For parameter in (3)-(6), we set [\u03b4; \u03b6] = [1; 100] for the KITTI dataset, and [\u03b4; \u03b6] = [1; 0.001] for the human motion dataset. For GA-CL setting, we mostly set the window w = 2 or 3 and \u03b1 = 1 for the 1st stage, \u03b1 = 0.5 for the 2nd stage, and \u03b1 = 0.1 for the 3rd stage as it get the best performance in KITTI dataset.\n\n\nD. Results\n\n\n1) Tests on KITTI Dataset:\n\nWe performed two experiments on the KITTI dataset. The first experiment is conducted for KITTI Sequences 00-10 where precise ground truth is available such that quantitative evaluation can be conducted. The second experiment is aimed to test further the generalization of the network on KITTI testing Sequences 11-20. Since there is no ground truth available for KITTI Sequences 11-20, no quantitative evaluation is performed.\n\nFor the first experiment, we trained CL-VO on KITTI Sequences 00, 01, 02, 08, and 09, and tested on KITTI Sequences 03, 04, 05, 06, 07, and 10 as seen in [4]. Fig. 4 (a) shows the qualitative results from Sequences 05 and 07. It can be seen that all CL-VO predictions are relatively accurate and consistent against the ground truth. CL-VO significantly outerforms VISO2-M and DeepVO. As for VISO2-M, the VO estimation in Fig. 4 (a) suggest that the scale estimation using fixed camera height is not robust against noise due to car jolts during driving [5]. Note that neither scale estimation nor post alignment to ground truth is conducted for CL-VO. The quantitative results can be seen in Fig. 5 where CL-VO consistently yields better performance for both translation and rotation against the path length compared to VISO2-M and DeepVO. Table I details the frame-to-frame relative transformation errors of the compared algorithms for each testing sequences. The result indicates that CL-VO achieves more robust outputs than VISO2-M, ORB-SLAM, and DeepVO, although the performance is, as expected, worse than the stereo algorithm, i.e. VISO2-S. The table also shows that GA-CL can boost the performance of DeepVO by 21% and 16% for translation and rotation respectively. CL-VO achieves higher accuracy than DeepVO+GA-CL as it estimates more accurate optical flow through the cascade optical flow networks.\n\nFor the second experiment, we trained CL-VO on KITTI Sequences 00-10 and tested on KITTI testing Sequences 2) Generalization in Malaga Dataset: In order to further test the generalization ability of the proposed framework, we tested CL-VO on the Malaga dataset without any furter training or fine-tuning. We used the CL-VO model which is trained on KITTI dataset Sequences 00-10 and tested directly on the Malaga image data. Since the image resolution in the Malaga dataset is different from KITTI, we cropped the images to the KITTI image size. Some image information is expected to lost during this cropping process which might affect the final predictions. Fig. 6 depicts the test results on Malaga dataset Sequences 03, 04, and 09, superimposed on Google Map. Since the Malaga dataset does not have ground truth, a quantitative evaluation cannot be conducted. However, since frequent GPS data is available, we still can perform qualitative comparison. As we can see from Fig. 6, CL-VO predictions are close to GPS and VISO2-S in those three sequences. It is significantly better than VISO2-M and DeepVO, although it suffers from drift. This experiment further confirms that CL-VO generalizes to other datasets which are collected with different cameras in different environments. This also shows that CL-VO generalizes better than DeepVO as the drift of DeepVO is larger on the test sequences.\n\n3) Tests on Human Motion Dataset: We divided the human motion dataset into 2 groups, 1 hour and 15 minutes for training and the remaining 15 minutes for testing. We subsample one frame for every six images to provide enough displacement between consecutive frames. Fig. 7 (a) shows the qualitative results on one of the test sequences. It can be seen that CL-VO performs better than DeepVO as the prediction is closer to the ground truth. While CL-VO successfully tracks the camera movement, DeepVO fails to perform turning accurately which leads to much larger drift. Fig. 7 (b) shows the 6-DoF translation (x, y, z) and orientation (roll, pitch, yaw) of CL-VO compared with DeepVO and ground truth. It is clear that CL-VO tracks the changes on translation and orientation accurately. Fig. 7 (c) illustrates the distribution of the absolute errors (RMSE). CL-VO significantly outperform DeepVO, achieving less than 2 meters errors during 100% of testing time.\n\n4) The Impact of Geometry-Aware Curriculum Learning: We performed an ablation study to understand the impact of the geometry-aware curriculum learning (GA-CL). We compare the performance of the proposed network when it is trained with the curriculum, reversed curriculum (anticurriculum), and without curriculum. For training without curriculum, we use two loss functions, namely the standard relative loss and the bounded pose regression loss with w = 2 and \u03b1 = 0.5. For the anti-curriculum, the stages described in Section III-C are reversed. All competing networks are trained with the same setting except GA-CL and anticurriculum changes the parameter of the objective function at the end of each training stage. Fig. 8 depicts the key results of this study. As expected, directly training the network with the bounded loss is more difficult to converge although the performance gradually improves in later stages of training. On the other hand, the network trained with the relative loss already reaches a stable state in the first stages of training. It only improves slightly afterwards or can even lead to overfitting as the accuracy of the rotation part decreases. The anti-curriculum gets very low accuracy in the beginning although the performance is improving after training with relative loss. Finally, the network trained with GA-CL can converge and generalize better which results in significantly lower translation and rotation errors in each training stages.\n\nOne possible explanation for this performance gain is GA-CL can be regarded as a special form of transfer learning, where the initial tasks (minimizing relative transformation loss) are used to guide the learner such that it can perform (b) Estimates of 6-DoF camera poses better on the final task (minimizing bounded pose regression loss). While the motivation of conventional transfer learning is to improve the generalization by sharing model weights across tasks, GA-CL introduces the idea of guiding the optimization process, either for faster convergence or better local minima [8]. Another perspective is GA-CL can be seen as a way to gradually injecting domain knowledge into DNNs by progressively reveals more information to the network over time via objective function alteration.\n\nV. CONCLUSION In this paper, we have presented a novel DNN framework (CL-VO) which is trained using a geometry-aware objective function and curriculum learning (GA-CL). We have shown that CL-VO performed significantly better than state-of-theart feature-based and learning-based approaches. We have also shown that GA-CL strategy can significantly improve the generalization ability of the network for both translation and rotation components, compared to a network that is   trained without GA-CL. We believe that CL-VO can be a viable complement to conventional VO approaches.\n\nFig. 1 .\n1CL-VO architecture consists of cascade optical-flow networks followed by recurrent networks and fully connected layers.\n\nFig. 2 .\n2Normalized translation and rotation errors for different value of \u03b1.\n\nFig. 3 .\n3CL-VO architecture with a windowed composition layer to integrate relative estimates over small windows w.\n\nFig. 4 .Fig. 5 .\n45(a) Qualitative results from Sequences 05 and 07 on KITTI dataset. (b) Qualitative results from Sequences 11 and 18 on KITTI dataset. Note that the ground truth is not available for KITTI 11-20. Qualitatively, we can see fromFig. 4 (b)that CL-VO predictions are more similar to the stereo algorithm (VISO2-S) estimation than VISO2-M and DeepVO. This confirms that CL-VO can generalize well in new scenarios with different motion patterns and environments although it suffers from drift over time. Translation and rotation errors against path length on KITTI dataset.\n\nFig. 7 .\n7(a) Test on human walking data in an office building. (b) 6-DoF camera poses compared to ground truth. (c) CDF of RMS absolute errors.\n\nFig. 8 .\n8The impact of GA-CL algorithm on translation and rotation errors.\n\n\n, Pedro P. B. de Gusmao, Andrew Markham, and Niki Trigoni are with Department of Computer Science, University of Oxford, UK firstname.lastname@cs.ox.ac.uk Sen Wang is with School of Engineering and Physical Sciences, Heriot Watt University, UK s.wang@hw.ac.uk2 \n\nTABLE I\nIFRAME-TO-FRAME RELATIVE TRANSLATION AND ROTATION ERRORS ON KITTI DATASET. %) rot( \u2022 ) trans(%) rot( \u2022 ) trans(%) rot( \u2022 ) trans(%) Fig. 6. Generalization tests on Malaga Dataset superimposed on Google Map. DeepVO and CL-VO are only trained on KITTI dataset Sequences 00-10.Monocular VO \nStereo VO \nSeq \nVISO2-M \nORB-SLAM \nDeepVO \nDeepVO+GA-CL (ours) \nCL-VO (ours) \nVISO2-S \ntrans(rot( \u2022 ) \ntrans(%) rot( \u2022 ) trans(%) rot( \u2022 ) \n03 \n28.14 \n0.0230 \n21.07 \n0.1836 \n10.71 \n0.0479 \n8.36 \n0.0353 \n8.12 \n0.0347 \n3.21 \n0.0325 \n04 \n33.92 \n0.0177 \n4.46 \n0.0560 \n9.95 \n0.0407 \n8.66 \n0.0308 \n7.57 \n0.0261 \n2.12 \n0.0212 \n05 \n14.65 \n0.0397 \n26.01 \n0.3427 \n8.02 \n0.0265 \n5.81 \n0.0210 \n5.77 \n0.0200 \n1.53 \n0.0160 \n06 \n19.54 \n0.0249 \n17.47 \n0.1717 \n7.10 \n0.0186 \n7.39 \n0.0183 \n7.66 \n0.0166 \n1.48 \n0.0158 \n07 \n12.69 \n0.0647 \n24.53 \n0.3890 \n16.20 \n0.0380 \n9.79 \n0.0413 \n6.79 \n0.0300 \n1.85 \n0.0191 \n10 \n30.39 \n0.0306 \n86.51 \n0.9890 \n9.04 \n0.0391 \n8.30 \n0.0303 \n8.29 \n0.0294 \n1.17 \n0.0130 \navg \n23.22 \n0.0334 \n30.01 \n0.3553 \n10.17 \n0.0351 \n8.05 \n0.0294 \n7.37 \n0.0267 \n1.89 \n0.0196 \n\n50 m \n\n-4.482 \n-4.4815 \n-4.481 \n-4.4805 \n-4.48 \n-4.4795 \n\n36.7188 \n\n36.719 \n\n36.7192 \n\n36.7194 \n\n36.7196 \n\n36.7198 \n\n36.72 \n\n36.7202 \n\n36.7204 \n\n36.7206 \n\nGPS \nVISO2-S \nVISO2-M \nDeepVO \nCL-VO \n\n20 m \n\n-4.481 \n-4.4805 \n-4.48 \n-4.4795 \n-4.479 \n\n36.7208 \n\n36.721 \n\n36.7212 \n\n36.7214 \n\n36.7216 \n\n36.7218 \n\n36.722 \n\n36.7222 \n\nGPS \nVISO2-S \nVISO2-M \nDeepVO \nCL-VO \n\n100 m \n\n-4.476 \n-4.475 \n-4.474 \n-4.473 \n-4.472 \n\n36.7155 \n\n36.716 \n\n36.7165 \n\n36.717 \n\n36.7175 \n\n36.718 \n\nGPS \nVISO2-S \nVISO2-M \nDeepVO \nCL-VO \n\n-12 \n-10 \n-8 \n-6 \n-4 \n-2 \n0 \n2 \n4 \n6 \n\nX [m] \n\n-10 \n\n-5 \n\n0 \n\n5 \n\nY [m] \n\nGT \nDeepVO \nCL-VO \nStart \nEnd GT \nEnd DeepVO \nEnd CL-VO \n\n(a) Test on human walking data \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n350 \n400 \n450 \n\nImage [n] \n\n-10 \n\n0 \n\n10 \n\nX [m] \n\nGT \nDeepVO \nCL-VO \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n350 \n400 \n450 \n\nImage [n] \n\n-10 \n\n0 \n\n10 \n\nY [m] \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n350 \n400 \n450 \n\nImage [n] \n\n-10 \n\n0 \n\n10 \n\nZ [m] \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n350 \n400 \n450 \n\nImage [n] \n\n-4 \n-2 \n0 \n2 \n4 \n\nRoll [rad] \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n350 \n400 \n450 \n\nImage [n] \n\n-4 \n-2 \n0 \n2 \n4 \n\nPitch [rad] \n\n0 \n50 \n100 \n150 \n200 \n250 \n300 \n350 \n400 \n450 \n\nImage [n] \n\n-4 \n-2 \n0 \n2 \n4 \n\nYaw [rad] \n\n\n\n\nTranslational Error [%] Average Rotational Error [deg/100m]End of 1st \ntraining stage \n\nEnd of 2nd \ntraining stage \n\nEnd of 3rd \ntraining stage \n\nBounded Loss \nRelative Loss \nAnti-Curriculum \nCurriculum \n\n1 \n1.5 \n2 \n2.5 \n3 \n3.5 \n\n2 \n\n3 \n\n4 \n\n5 \n\n6 \n\n7 \n\n8 \n\nEnd of 1st \ntraining stage \n\nEnd of 2nd \ntraining stage \n\nEnd of 3rd \ntraining stage \n\nBounded Loss \nRelative Loss \nAnti-Curriculum \nCurriculum \n\n\nAcknowledgement. This research is funded by the US National Institute of Standards and Technology (NIST) Grant No. 70NANB17H185. Muhamad Risqi U. Saputra was supported by Indonesia Endowment Fund for Education (LPDP).\nMultiple View Geometry in Computer Vision. R Hartley, A Zisserman, Cambridge University Press2nd edR. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision, 2nd ed. Cambridge University Press, 2004.\n\nM R U Saputra, A Markham, N Trigoni, Visual SLAM and Structure from Motion in Dynamic Environments : A Survey. 51M. R. U. Saputra, A. Markham, and N. Trigoni, \"Visual SLAM and Structure from Motion in Dynamic Environments : A Survey,\" ACM Computing Surveys, vol. 51, no. 2, 2018.\n\nFlowdometry: An Optical Flow and Deep Learning Based Approach to Visual Odometry. P Muller, A Savakis, IEEE Winter Conference on Applications of Computer Vision (WACV). P. Muller and A. Savakis, \"Flowdometry: An Optical Flow and Deep Learning Based Approach to Visual Odometry,\" in IEEE Winter Conference on Applications of Computer Vision (WACV), 2017.\n\nDeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks. S Wang, R Clark, H Wen, N Trigoni, IEEE International Conference on Robotics and Automation (ICRA). S. Wang, R. Clark, H. Wen, and N. Trigoni, \"DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks,\" in IEEE International Conference on Robotics and Automation (ICRA), 2017.\n\nEnd-to-end, sequence-tosequence probabilistic visual odometry through deep neural networks. S Wang, R Clark, H Wen, N Trigoni, The International Journal of Robotics Research. S. Wang, R. Clark, H. Wen, and N. Trigoni, \"End-to-end, sequence-to- sequence probabilistic visual odometry through deep neural networks,\" The International Journal of Robotics Research, pp. 1-30, 2017.\n\nLS-VO: Learning Dense Optical Subspace for Robust Visual Odometry Estimation. G Costante, T A Ciarfuglia, IEEE Robotics and Automation Letters. 33G. Costante and T. A. Ciarfuglia, \"LS-VO: Learning Dense Optical Subspace for Robust Visual Odometry Estimation,\" IEEE Robotics and Automation Letters, vol. 3, no. 3, 2018.\n\nGeometric correspondence network for camera motion estimation. J Tang, J Folkesson, P Jensfelt, IEEE Robotics and Automation Letters. 32J. Tang, J. Folkesson, and P. Jensfelt, \"Geometric correspondence net- work for camera motion estimation,\" IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 1010-1017, April 2018.\n\nCurriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningACMY. Bengio, J. Louradour, R. Collobert, and J. Weston, \"Curriculum learning,\" in Proceedings of the 26th annual international conference on machine learning. ACM, 2009, pp. 41-48.\n\nVisual Odometry. D Nist\u00e9r, O Naroditsky, J Bergen, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). D. Nist\u00e9r, O. Naroditsky, and J. Bergen, \"Visual Odometry,\" in IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2004, pp. 652-659.\n\nTwo years of visual odometry on the mars exploration rovers. M Maimone, Y Cheng, L Matthies, Journal of Field Robotics. 243M. Maimone, Y. Cheng, and L. Matthies, \"Two years of visual odometry on the mars exploration rovers,\" Journal of Field Robotics, vol. 24, no. 3, pp. 169-186, 2007.\n\nVisual odometry by multiframe feature integration. H Badino, A Yamamoto, T Kanade, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsH. Badino, A. Yamamoto, and T. Kanade, \"Visual odometry by multi- frame feature integration,\" in Proceedings of the IEEE International Conference on Computer Vision Workshops, 2013, pp. 222-229.\n\nStereoScan: Dense 3D Reconstruction in Real-time. A Geiger, J Ziegler, C Stiller, IEEE Intelligent Vehicles Symposium (IV). A. Geiger, J. Ziegler, and C. Stiller, \"StereoScan: Dense 3D Recon- struction in Real-time,\" in IEEE Intelligent Vehicles Symposium (IV), 2011, pp. 1-9.\n\nLearning Visual Odometry with a Convolutional Network. K Konda, R Memisevic, International Conference on Computer Vision Theory and Applications. K. Konda and R. Memisevic, \"Learning Visual Odometry with a Convolutional Network,\" in International Conference on Computer Vision Theory and Applications, 2015, pp. 486-490.\n\nFlowNet: Learning Optical Flow with Convolutional Networks. A Dosovitskiy, P Fischery, E Ilg, P Hausser, C Hazirbas, V Golkov, P V D Smagt, D Cremers, T Brox, IEEE International Conference on Computer Vision (ICCV). 11A. Dosovitskiy, P. Fischery, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. V. D. Smagt, D. Cremers, and T. Brox, \"FlowNet: Learning Optical Flow with Convolutional Networks,\" in IEEE International Conference on Computer Vision (ICCV), vol. 11-18-Dece, 2016, pp. 2758-2766.\n\nLearning and development in neural networks: The importance of starting small. J L Elman, Cognition. 481J. L. Elman, \"Learning and development in neural networks: The importance of starting small,\" Cognition, vol. 48, no. 1, pp. 71-99, 1993.\n\nSelf-paced learning for latent variable models. M P Kumar, B Packer, D Koller, Advances in Neural Information Processing Systems. M. P. Kumar, B. Packer, and D. Koller, \"Self-paced learning for latent variable models,\" in Advances in Neural Information Processing Systems, 2010, pp. 1189-1197.\n\nSelfpaced curriculum learning. L Jiang, D Meng, Q Zhao, S Shan, A G Hauptmann, AAAI. 256L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann, \"Self- paced curriculum learning.\" in AAAI, vol. 2, no. 5.4, 2015, p. 6.\n\nSelfpaced learning with diversity. L Jiang, D Meng, S.-I Yu, Z Lan, S Shan, A Hauptmann, Advances in Neural Information Processing Systems. L. Jiang, D. Meng, S.-I. Yu, Z. Lan, S. Shan, and A. Hauptmann, \"Self- paced learning with diversity,\" in Advances in Neural Information Processing Systems, 2014, pp. 2078-2086.\n\nSelf-paced dictionary learning for image classification. Y Tang, Y.-B Yang, Y Gao, Proceedings of the 20th ACM international conference on Multimedia. the 20th ACM international conference on MultimediaACMY. Tang, Y.-B. Yang, and Y. Gao, \"Self-paced dictionary learning for image classification,\" in Proceedings of the 20th ACM international conference on Multimedia. ACM, 2012, pp. 833-836.\n\nShifting weights: Adapting object detectors from image to video. K Tang, V Ramanathan, L Fei-Fei, D Koller, Advances in Neural Information Processing Systems. K. Tang, V. Ramanathan, L. Fei-Fei, and D. Koller, \"Shifting weights: Adapting object detectors from image to video,\" in Advances in Neural Information Processing Systems, 2012, pp. 638-646.\n\nSelf-paced learning for long-term tracking. J S Supancic, D Ramanan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. S. Supancic and D. Ramanan, \"Self-paced learning for long-term tracking,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2013, pp. 2379-2386.\n\nPoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. A Kendall, M Grimes, R Cipolla, IEEE International Conference on Computer Vision (ICCV). A. Kendall, M. Grimes, and R. Cipolla, \"PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization,\" in IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2938-2946. [Online]. Available: http://arxiv.org/abs/1505.07427\n\nVINet : Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem. R Clark, S Wang, H Wen, A Markham, N Trigoni, AAAI Conference on Artificial Intelligence. R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, \"VINet : Visual-Inertial Odometry as a Sequence-to-Sequence Learning Prob- lem,\" in AAAI Conference on Artificial Intelligence, 2017, pp. 3995- 4001.\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, IEEE conference on computer vision and pattern recognition (CVPR). 26E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, \"Flownet 2.0: Evolution of optical flow estimation with deep net- works,\" in IEEE conference on computer vision and pattern recog- nition (CVPR), vol. 2, 2017, p. 6.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n\nAre we ready for autonomous driving? the KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. the IEEE Computer Society Conference on Computer Vision and Pattern RecognitionA. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? the KITTI vision benchmark suite,\" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2012, pp. 3354-3361.\n\nThe m\u00e1laga urban dataset: High-rate stereo and lidar in a realistic urban scenario. J.-L Blanco-Claraco, F.-\u00c1 Moreno-Due\u00f1as, J Gonz\u00e1lez-Jim\u00e9nez, The International Journal of Robotics Research. 332J.-L. Blanco-Claraco, F.-\u00c1. Moreno-Due\u00f1as, and J. Gonz\u00e1lez-Jim\u00e9nez, \"The m\u00e1laga urban dataset: High-rate stereo and lidar in a realistic urban scenario,\" The International Journal of Robotics Research, vol. 33, no. 2, pp. 207-214, 2014.\n\nORB-SLAM: A Versatile and Accurate Monocular SLAM System. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. 315R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, \"ORB-SLAM: A Versatile and Accurate Monocular SLAM System,\" IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The Journal of Machine Learning Research. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut- dinov, \"Dropout: A simple way to prevent neural networks from overfitting,\" The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929-1958, 2014.\n", "annotations": {"author": "[{\"end\":95,\"start\":81},{\"end\":106,\"start\":96},{\"end\":127,\"start\":107},{\"end\":137,\"start\":128},{\"end\":153,\"start\":138},{\"end\":167,\"start\":154}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":89},{\"end\":105,\"start\":98},{\"end\":126,\"start\":117},{\"end\":136,\"start\":132},{\"end\":152,\"start\":145},{\"end\":166,\"start\":159}]", "author_first_name": "[{\"end\":88,\"start\":81},{\"end\":97,\"start\":96},{\"end\":112,\"start\":107},{\"end\":116,\"start\":113},{\"end\":131,\"start\":128},{\"end\":144,\"start\":138},{\"end\":158,\"start\":154}]", "author_affiliation": null, "title": "[{\"end\":78,\"start\":1},{\"end\":245,\"start\":168}]", "venue": null, "abstract": "[{\"end\":1194,\"start\":247}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1567,\"start\":1564},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1693,\"start\":1690},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2019,\"start\":2016},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2024,\"start\":2021},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2029,\"start\":2026},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2034,\"start\":2031},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2039,\"start\":2036},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2258,\"start\":2255},{\"end\":2808,\"start\":2801},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3412,\"start\":3409},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5141,\"start\":5138},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5198,\"start\":5194},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5204,\"start\":5200},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5219,\"start\":5215},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5707,\"start\":5703},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5883,\"start\":5880},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5951,\"start\":5947},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6104,\"start\":6101},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6119,\"start\":6116},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6432,\"start\":6429},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6437,\"start\":6434},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6442,\"start\":6439},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6528,\"start\":6525},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6753,\"start\":6749},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6870,\"start\":6867},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7033,\"start\":7029},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7180,\"start\":7176},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7457,\"start\":7453},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7483,\"start\":7479},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7507,\"start\":7503},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7533,\"start\":7529},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9683,\"start\":9680},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10208,\"start\":10204},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10213,\"start\":10210},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10574,\"start\":10570},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12548,\"start\":12544},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12588,\"start\":12584},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12882,\"start\":12878},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13043,\"start\":13040},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13052,\"start\":13048},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14342,\"start\":14338},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14690,\"start\":14686},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15775,\"start\":15771},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15790,\"start\":15786},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15806,\"start\":15803},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16120,\"start\":16117},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17213,\"start\":17209},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18212,\"start\":18209},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18610,\"start\":18607},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23888,\"start\":23885}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24802,\"start\":24672},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24882,\"start\":24803},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25000,\"start\":24883},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25587,\"start\":25001},{\"attributes\":{\"id\":\"fig_5\"},\"end\":25733,\"start\":25588},{\"attributes\":{\"id\":\"fig_7\"},\"end\":25810,\"start\":25734},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26074,\"start\":25811},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28358,\"start\":26075},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28765,\"start\":28359}]", "paragraph": "[{\"end\":2134,\"start\":1213},{\"end\":3183,\"start\":2136},{\"end\":4016,\"start\":3185},{\"end\":4754,\"start\":4018},{\"end\":5433,\"start\":4797},{\"end\":6443,\"start\":5458},{\"end\":7657,\"start\":6470},{\"end\":7990,\"start\":7719},{\"end\":8519,\"start\":8025},{\"end\":8625,\"start\":8570},{\"end\":9663,\"start\":8664},{\"end\":9952,\"start\":9665},{\"end\":10214,\"start\":9954},{\"end\":12095,\"start\":10439},{\"end\":12770,\"start\":12123},{\"end\":13225,\"start\":12772},{\"end\":13329,\"start\":13324},{\"end\":13538,\"start\":13386},{\"end\":13767,\"start\":13556},{\"end\":14097,\"start\":13769},{\"end\":14160,\"start\":14145},{\"end\":15600,\"start\":14176},{\"end\":16516,\"start\":15628},{\"end\":17583,\"start\":16555},{\"end\":18053,\"start\":17627},{\"end\":19461,\"start\":18055},{\"end\":20860,\"start\":19463},{\"end\":21822,\"start\":20862},{\"end\":23299,\"start\":21824},{\"end\":24091,\"start\":23301},{\"end\":24671,\"start\":24093}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8024,\"start\":7991},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8569,\"start\":8520},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10398,\"start\":10215},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13323,\"start\":13226},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13355,\"start\":13330},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13385,\"start\":13355},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13555,\"start\":13539},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14144,\"start\":14098}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18901,\"start\":18894}]", "section_header": "[{\"end\":1211,\"start\":1196},{\"end\":4773,\"start\":4757},{\"end\":4795,\"start\":4776},{\"end\":5456,\"start\":5436},{\"end\":6468,\"start\":6446},{\"end\":7682,\"start\":7660},{\"end\":7717,\"start\":7685},{\"end\":8662,\"start\":8628},{\"end\":10437,\"start\":10400},{\"end\":12121,\"start\":12098},{\"end\":14174,\"start\":14163},{\"end\":15626,\"start\":15603},{\"end\":16553,\"start\":16519},{\"end\":17596,\"start\":17586},{\"end\":17625,\"start\":17599},{\"end\":24681,\"start\":24673},{\"end\":24812,\"start\":24804},{\"end\":24892,\"start\":24884},{\"end\":25018,\"start\":25002},{\"end\":25597,\"start\":25589},{\"end\":25743,\"start\":25735},{\"end\":26083,\"start\":26076}]", "table": "[{\"end\":26074,\"start\":26072},{\"end\":28358,\"start\":26358},{\"end\":28765,\"start\":28420}]", "figure_caption": "[{\"end\":24802,\"start\":24683},{\"end\":24882,\"start\":24814},{\"end\":25000,\"start\":24894},{\"end\":25587,\"start\":25021},{\"end\":25733,\"start\":25599},{\"end\":25810,\"start\":25745},{\"end\":26072,\"start\":25813},{\"end\":26358,\"start\":26085},{\"end\":28420,\"start\":28361}]", "figure_ref": "[{\"end\":9990,\"start\":9980},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10738,\"start\":10732},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11582,\"start\":11576},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12184,\"start\":12178},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13934,\"start\":13928},{\"end\":18224,\"start\":18214},{\"end\":18486,\"start\":18476},{\"end\":18752,\"start\":18746},{\"end\":20129,\"start\":20123},{\"end\":20444,\"start\":20438},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21137,\"start\":21127},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21437,\"start\":21431},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21654,\"start\":21648},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":22547,\"start\":22541}]", "bib_author_first_name": "[{\"end\":29028,\"start\":29027},{\"end\":29039,\"start\":29038},{\"end\":29199,\"start\":29198},{\"end\":29203,\"start\":29200},{\"end\":29214,\"start\":29213},{\"end\":29225,\"start\":29224},{\"end\":29562,\"start\":29561},{\"end\":29572,\"start\":29571},{\"end\":29929,\"start\":29928},{\"end\":29937,\"start\":29936},{\"end\":29946,\"start\":29945},{\"end\":29953,\"start\":29952},{\"end\":30335,\"start\":30334},{\"end\":30343,\"start\":30342},{\"end\":30352,\"start\":30351},{\"end\":30359,\"start\":30358},{\"end\":30700,\"start\":30699},{\"end\":30712,\"start\":30711},{\"end\":30714,\"start\":30713},{\"end\":31005,\"start\":31004},{\"end\":31013,\"start\":31012},{\"end\":31026,\"start\":31025},{\"end\":31287,\"start\":31286},{\"end\":31297,\"start\":31296},{\"end\":31310,\"start\":31309},{\"end\":31323,\"start\":31322},{\"end\":31670,\"start\":31669},{\"end\":31680,\"start\":31679},{\"end\":31694,\"start\":31693},{\"end\":32016,\"start\":32015},{\"end\":32027,\"start\":32026},{\"end\":32036,\"start\":32035},{\"end\":32294,\"start\":32293},{\"end\":32304,\"start\":32303},{\"end\":32316,\"start\":32315},{\"end\":32713,\"start\":32712},{\"end\":32723,\"start\":32722},{\"end\":32734,\"start\":32733},{\"end\":32996,\"start\":32995},{\"end\":33005,\"start\":33004},{\"end\":33323,\"start\":33322},{\"end\":33338,\"start\":33337},{\"end\":33350,\"start\":33349},{\"end\":33357,\"start\":33356},{\"end\":33368,\"start\":33367},{\"end\":33380,\"start\":33379},{\"end\":33390,\"start\":33389},{\"end\":33394,\"start\":33391},{\"end\":33403,\"start\":33402},{\"end\":33414,\"start\":33413},{\"end\":33835,\"start\":33834},{\"end\":33837,\"start\":33836},{\"end\":34047,\"start\":34046},{\"end\":34049,\"start\":34048},{\"end\":34058,\"start\":34057},{\"end\":34068,\"start\":34067},{\"end\":34325,\"start\":34324},{\"end\":34334,\"start\":34333},{\"end\":34342,\"start\":34341},{\"end\":34350,\"start\":34349},{\"end\":34358,\"start\":34357},{\"end\":34360,\"start\":34359},{\"end\":34549,\"start\":34548},{\"end\":34558,\"start\":34557},{\"end\":34569,\"start\":34565},{\"end\":34575,\"start\":34574},{\"end\":34582,\"start\":34581},{\"end\":34590,\"start\":34589},{\"end\":34890,\"start\":34889},{\"end\":34901,\"start\":34897},{\"end\":34909,\"start\":34908},{\"end\":35291,\"start\":35290},{\"end\":35299,\"start\":35298},{\"end\":35313,\"start\":35312},{\"end\":35324,\"start\":35323},{\"end\":35621,\"start\":35620},{\"end\":35623,\"start\":35622},{\"end\":35635,\"start\":35634},{\"end\":36044,\"start\":36043},{\"end\":36055,\"start\":36054},{\"end\":36065,\"start\":36064},{\"end\":36462,\"start\":36461},{\"end\":36471,\"start\":36470},{\"end\":36479,\"start\":36478},{\"end\":36486,\"start\":36485},{\"end\":36497,\"start\":36496},{\"end\":36828,\"start\":36827},{\"end\":36835,\"start\":36834},{\"end\":36844,\"start\":36843},{\"end\":36854,\"start\":36853},{\"end\":36864,\"start\":36863},{\"end\":36879,\"start\":36878},{\"end\":37216,\"start\":37215},{\"end\":37230,\"start\":37229},{\"end\":37455,\"start\":37454},{\"end\":37465,\"start\":37464},{\"end\":37473,\"start\":37472},{\"end\":37976,\"start\":37972},{\"end\":37997,\"start\":37993},{\"end\":38014,\"start\":38013},{\"end\":38381,\"start\":38380},{\"end\":38394,\"start\":38393},{\"end\":38398,\"start\":38395},{\"end\":38409,\"start\":38408},{\"end\":38411,\"start\":38410},{\"end\":38701,\"start\":38700},{\"end\":38715,\"start\":38714},{\"end\":38725,\"start\":38724},{\"end\":38739,\"start\":38738},{\"end\":38752,\"start\":38751}]", "bib_author_last_name": "[{\"end\":29036,\"start\":29029},{\"end\":29049,\"start\":29040},{\"end\":29211,\"start\":29204},{\"end\":29222,\"start\":29215},{\"end\":29233,\"start\":29226},{\"end\":29569,\"start\":29563},{\"end\":29580,\"start\":29573},{\"end\":29934,\"start\":29930},{\"end\":29943,\"start\":29938},{\"end\":29950,\"start\":29947},{\"end\":29961,\"start\":29954},{\"end\":30340,\"start\":30336},{\"end\":30349,\"start\":30344},{\"end\":30356,\"start\":30353},{\"end\":30367,\"start\":30360},{\"end\":30709,\"start\":30701},{\"end\":30725,\"start\":30715},{\"end\":31010,\"start\":31006},{\"end\":31023,\"start\":31014},{\"end\":31035,\"start\":31027},{\"end\":31294,\"start\":31288},{\"end\":31307,\"start\":31298},{\"end\":31320,\"start\":31311},{\"end\":31330,\"start\":31324},{\"end\":31677,\"start\":31671},{\"end\":31691,\"start\":31681},{\"end\":31701,\"start\":31695},{\"end\":32024,\"start\":32017},{\"end\":32033,\"start\":32028},{\"end\":32045,\"start\":32037},{\"end\":32301,\"start\":32295},{\"end\":32313,\"start\":32305},{\"end\":32323,\"start\":32317},{\"end\":32720,\"start\":32714},{\"end\":32731,\"start\":32724},{\"end\":32742,\"start\":32735},{\"end\":33002,\"start\":32997},{\"end\":33015,\"start\":33006},{\"end\":33335,\"start\":33324},{\"end\":33347,\"start\":33339},{\"end\":33354,\"start\":33351},{\"end\":33365,\"start\":33358},{\"end\":33377,\"start\":33369},{\"end\":33387,\"start\":33381},{\"end\":33400,\"start\":33395},{\"end\":33411,\"start\":33404},{\"end\":33419,\"start\":33415},{\"end\":33843,\"start\":33838},{\"end\":34055,\"start\":34050},{\"end\":34065,\"start\":34059},{\"end\":34075,\"start\":34069},{\"end\":34331,\"start\":34326},{\"end\":34339,\"start\":34335},{\"end\":34347,\"start\":34343},{\"end\":34355,\"start\":34351},{\"end\":34370,\"start\":34361},{\"end\":34555,\"start\":34550},{\"end\":34563,\"start\":34559},{\"end\":34572,\"start\":34570},{\"end\":34579,\"start\":34576},{\"end\":34587,\"start\":34583},{\"end\":34600,\"start\":34591},{\"end\":34895,\"start\":34891},{\"end\":34906,\"start\":34902},{\"end\":34913,\"start\":34910},{\"end\":35296,\"start\":35292},{\"end\":35310,\"start\":35300},{\"end\":35321,\"start\":35314},{\"end\":35331,\"start\":35325},{\"end\":35632,\"start\":35624},{\"end\":35643,\"start\":35636},{\"end\":36052,\"start\":36045},{\"end\":36062,\"start\":36056},{\"end\":36073,\"start\":36066},{\"end\":36468,\"start\":36463},{\"end\":36476,\"start\":36472},{\"end\":36483,\"start\":36480},{\"end\":36494,\"start\":36487},{\"end\":36505,\"start\":36498},{\"end\":36832,\"start\":36829},{\"end\":36841,\"start\":36836},{\"end\":36851,\"start\":36845},{\"end\":36861,\"start\":36855},{\"end\":36876,\"start\":36865},{\"end\":36884,\"start\":36880},{\"end\":37227,\"start\":37217},{\"end\":37242,\"start\":37231},{\"end\":37462,\"start\":37456},{\"end\":37470,\"start\":37466},{\"end\":37481,\"start\":37474},{\"end\":37991,\"start\":37977},{\"end\":38011,\"start\":37998},{\"end\":38031,\"start\":38015},{\"end\":38391,\"start\":38382},{\"end\":38406,\"start\":38399},{\"end\":38418,\"start\":38412},{\"end\":38712,\"start\":38702},{\"end\":38722,\"start\":38716},{\"end\":38736,\"start\":38726},{\"end\":38749,\"start\":38740},{\"end\":38766,\"start\":38753}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":29196,\"start\":28984},{\"attributes\":{\"id\":\"b1\"},\"end\":29477,\"start\":29198},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206858678},\"end\":29832,\"start\":29479},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9114952},\"end\":30240,\"start\":29834},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":21689894},\"end\":30619,\"start\":30242},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2770584},\"end\":30939,\"start\":30621},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3740359},\"end\":31263,\"start\":30941},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":873046},\"end\":31650,\"start\":31265},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9224113},\"end\":31952,\"start\":31652},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":17544166},\"end\":32240,\"start\":31954},{\"attributes\":{\"id\":\"b10\"},\"end\":32660,\"start\":32242},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16284071},\"end\":32938,\"start\":32662},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9527002},\"end\":33260,\"start\":32940},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12552176},\"end\":33753,\"start\":33262},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2105042},\"end\":33996,\"start\":33755},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1977996},\"end\":34291,\"start\":33998},{\"attributes\":{\"id\":\"b16\"},\"end\":34511,\"start\":34293},{\"attributes\":{\"id\":\"b17\"},\"end\":34830,\"start\":34513},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2603360},\"end\":35223,\"start\":34832},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":813192},\"end\":35574,\"start\":35225},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10118550},\"end\":35965,\"start\":35576},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12888763},\"end\":36382,\"start\":35967},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17112447},\"end\":36755,\"start\":36384},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3759573},\"end\":37189,\"start\":36757},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1915014},\"end\":37381,\"start\":37191},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6724907},\"end\":37886,\"start\":37383},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16276115},\"end\":38320,\"start\":37888},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206775100},\"end\":38631,\"start\":38322},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6844431},\"end\":39039,\"start\":38633}]", "bib_title": "[{\"end\":29559,\"start\":29479},{\"end\":29926,\"start\":29834},{\"end\":30332,\"start\":30242},{\"end\":30697,\"start\":30621},{\"end\":31002,\"start\":30941},{\"end\":31284,\"start\":31265},{\"end\":31667,\"start\":31652},{\"end\":32013,\"start\":31954},{\"end\":32291,\"start\":32242},{\"end\":32710,\"start\":32662},{\"end\":32993,\"start\":32940},{\"end\":33320,\"start\":33262},{\"end\":33832,\"start\":33755},{\"end\":34044,\"start\":33998},{\"end\":34322,\"start\":34293},{\"end\":34546,\"start\":34513},{\"end\":34887,\"start\":34832},{\"end\":35288,\"start\":35225},{\"end\":35618,\"start\":35576},{\"end\":36041,\"start\":35967},{\"end\":36459,\"start\":36384},{\"end\":36825,\"start\":36757},{\"end\":37213,\"start\":37191},{\"end\":37452,\"start\":37383},{\"end\":37970,\"start\":37888},{\"end\":38378,\"start\":38322},{\"end\":38698,\"start\":38633}]", "bib_author": "[{\"end\":29038,\"start\":29027},{\"end\":29051,\"start\":29038},{\"end\":29213,\"start\":29198},{\"end\":29224,\"start\":29213},{\"end\":29235,\"start\":29224},{\"end\":29571,\"start\":29561},{\"end\":29582,\"start\":29571},{\"end\":29936,\"start\":29928},{\"end\":29945,\"start\":29936},{\"end\":29952,\"start\":29945},{\"end\":29963,\"start\":29952},{\"end\":30342,\"start\":30334},{\"end\":30351,\"start\":30342},{\"end\":30358,\"start\":30351},{\"end\":30369,\"start\":30358},{\"end\":30711,\"start\":30699},{\"end\":30727,\"start\":30711},{\"end\":31012,\"start\":31004},{\"end\":31025,\"start\":31012},{\"end\":31037,\"start\":31025},{\"end\":31296,\"start\":31286},{\"end\":31309,\"start\":31296},{\"end\":31322,\"start\":31309},{\"end\":31332,\"start\":31322},{\"end\":31679,\"start\":31669},{\"end\":31693,\"start\":31679},{\"end\":31703,\"start\":31693},{\"end\":32026,\"start\":32015},{\"end\":32035,\"start\":32026},{\"end\":32047,\"start\":32035},{\"end\":32303,\"start\":32293},{\"end\":32315,\"start\":32303},{\"end\":32325,\"start\":32315},{\"end\":32722,\"start\":32712},{\"end\":32733,\"start\":32722},{\"end\":32744,\"start\":32733},{\"end\":33004,\"start\":32995},{\"end\":33017,\"start\":33004},{\"end\":33337,\"start\":33322},{\"end\":33349,\"start\":33337},{\"end\":33356,\"start\":33349},{\"end\":33367,\"start\":33356},{\"end\":33379,\"start\":33367},{\"end\":33389,\"start\":33379},{\"end\":33402,\"start\":33389},{\"end\":33413,\"start\":33402},{\"end\":33421,\"start\":33413},{\"end\":33845,\"start\":33834},{\"end\":34057,\"start\":34046},{\"end\":34067,\"start\":34057},{\"end\":34077,\"start\":34067},{\"end\":34333,\"start\":34324},{\"end\":34341,\"start\":34333},{\"end\":34349,\"start\":34341},{\"end\":34357,\"start\":34349},{\"end\":34372,\"start\":34357},{\"end\":34557,\"start\":34548},{\"end\":34565,\"start\":34557},{\"end\":34574,\"start\":34565},{\"end\":34581,\"start\":34574},{\"end\":34589,\"start\":34581},{\"end\":34602,\"start\":34589},{\"end\":34897,\"start\":34889},{\"end\":34908,\"start\":34897},{\"end\":34915,\"start\":34908},{\"end\":35298,\"start\":35290},{\"end\":35312,\"start\":35298},{\"end\":35323,\"start\":35312},{\"end\":35333,\"start\":35323},{\"end\":35634,\"start\":35620},{\"end\":35645,\"start\":35634},{\"end\":36054,\"start\":36043},{\"end\":36064,\"start\":36054},{\"end\":36075,\"start\":36064},{\"end\":36470,\"start\":36461},{\"end\":36478,\"start\":36470},{\"end\":36485,\"start\":36478},{\"end\":36496,\"start\":36485},{\"end\":36507,\"start\":36496},{\"end\":36834,\"start\":36827},{\"end\":36843,\"start\":36834},{\"end\":36853,\"start\":36843},{\"end\":36863,\"start\":36853},{\"end\":36878,\"start\":36863},{\"end\":36886,\"start\":36878},{\"end\":37229,\"start\":37215},{\"end\":37244,\"start\":37229},{\"end\":37464,\"start\":37454},{\"end\":37472,\"start\":37464},{\"end\":37483,\"start\":37472},{\"end\":37993,\"start\":37972},{\"end\":38013,\"start\":37993},{\"end\":38033,\"start\":38013},{\"end\":38393,\"start\":38380},{\"end\":38408,\"start\":38393},{\"end\":38420,\"start\":38408},{\"end\":38714,\"start\":38700},{\"end\":38724,\"start\":38714},{\"end\":38738,\"start\":38724},{\"end\":38751,\"start\":38738},{\"end\":38768,\"start\":38751}]", "bib_venue": "[{\"end\":29025,\"start\":28984},{\"end\":29307,\"start\":29235},{\"end\":29646,\"start\":29582},{\"end\":30026,\"start\":29963},{\"end\":30415,\"start\":30369},{\"end\":30763,\"start\":30727},{\"end\":31073,\"start\":31037},{\"end\":31407,\"start\":31332},{\"end\":31785,\"start\":31703},{\"end\":32072,\"start\":32047},{\"end\":32402,\"start\":32325},{\"end\":32784,\"start\":32744},{\"end\":33084,\"start\":33017},{\"end\":33476,\"start\":33421},{\"end\":33854,\"start\":33845},{\"end\":34126,\"start\":34077},{\"end\":34376,\"start\":34372},{\"end\":34651,\"start\":34602},{\"end\":34981,\"start\":34915},{\"end\":35382,\"start\":35333},{\"end\":35722,\"start\":35645},{\"end\":36130,\"start\":36075},{\"end\":36549,\"start\":36507},{\"end\":36951,\"start\":36886},{\"end\":37262,\"start\":37244},{\"end\":37577,\"start\":37483},{\"end\":38079,\"start\":38033},{\"end\":38449,\"start\":38420},{\"end\":38808,\"start\":38768},{\"end\":31469,\"start\":31409},{\"end\":32466,\"start\":32404},{\"end\":35034,\"start\":34983},{\"end\":35786,\"start\":35724},{\"end\":37658,\"start\":37579}]"}}}, "year": 2023, "month": 12, "day": 17}