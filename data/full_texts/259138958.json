{"id": 259138958, "updated": "2023-10-04 23:55:46.685", "metadata": {"title": "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark", "authors": "[{\"first\":\"Zhenfei\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Jiong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jianjian\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Zhelun\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Dingning\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Mukai\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Sheng\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Bai\",\"middle\":[]},{\"first\":\"Xiaoshui\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Zhiyong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Wanli\",\"last\":\"Ouyang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities. Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Codes and datasets are now available at https://github.com/OpenLAMM/LAMM.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.06687", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-06687", "doi": "10.48550/arxiv.2306.06687"}}, "content": {"source": {"pdf_hash": "fd755dc7b5b206c17fd953db04e1c888d45b6e4e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.06687v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4d179b692ad7e4ed10265e031eae8a322f4e5ea1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fd755dc7b5b206c17fd953db04e1c888d45b6e4e.txt", "contents": "\nLAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark\n\n\nZhenfei Yin yinzhenfei@pjlab.org.cn \nShanghai AI Lab\n\n\nJiong Wang \nShanghai AI Lab\n\n\nThe Chinese University of Hong Kong (Shenzhen)\n\n\nJianjian Cao \nShanghai AI Lab\n\n\nFudan University\n\n\nZhelun Shi \nBeihang University\n\n\nDingning Liu \nShanghai AI Lab\n\n\nDalian University of Technology\n\n\nMukai Li \nLu Sheng \nShanghai AI Lab\n\n\nBeihang University\n\n\nLei Bai bailei@pjlab.org.cn \nShanghai AI Lab\n\n\n\u2020 \nXiaoshui Huang \nShanghai AI Lab\n\n\nZhiyong Wang \nThe University of Sydney\n\n\nJing Shao shaojing@pjlab.org.cn \nShanghai AI Lab\n\n\n\u2020 \nWanli Ouyang \nShanghai AI Lab\n\n\nLAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark\n\nLarge language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities. Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Codes and datasets are now available at https://github.com/OpenLAMM/LAMM.Recent works onMulti-modal Large Language Models (MLLM), including KOSMOS [6], LLaVA [7], and PaLM-E [8] demonstrate that by introducing other modalities, currently focusing on images, into LLMs through instruction tuning, MLLMs can effectively handle visual modalities and have preliminary abilities to interact with visual content through question-answering dialogue. In these * Equal Contribution \u2020 Corresponding Author Preprint. Under review. arXiv:2306.06687v2 [cs.CV] 18 Jun 2023 works, LLMs serve as the universal task interface, with inputs from vision tokens provided by pretrained multi-modal encoders and language instructions. The powerful modeling capability of LLMs, combined with a unified optimization objective, can help align the model to various modalities [6].While previous works mainly focused on the image modality, we aim to extend the research of MLLMs to additional modalities, such as point clouds, in this work. We present LAMM-Dataset, which emphasizes fine-grained and dense information, and factual knowledge. Additionally, we introduce the LAMM-Benchmark, which is the first attempt of a benchmark for MLLMs that offers a comprehensive evaluation of existing models on various computer vision tasks, with two new evaluation strategies designed explicitly for multi-modal language models. We conduct over 200 experiments to provide extensive results and valuable observations on the capabilities and limitations of existing MLLMs. Also, we establish an extensible framework to facilitate the extension of multimodal language models to additional modalities. Our baseline model surpasses existing multi-modal language models in downstream tasks related to images, demonstrating the effectiveness of our framework and dataset. Finally, we have open-sourced our codebase, baseline models, instruction tuning dataset, and multi-modal language model benchmark to promote the development of an open research community for MLLMs.LAMM-Dataset includes an image instruction-tuning dataset containing 186,098 image-language instruction-response pairs and a point cloud instruction-tuning dataset with 10,262 point cloudlanguage instruction-response pairs. We collect images and point clouds from publicly available datasets and use the GPT-API and self-instruction [9] methods to generate instructions and responses based on the original labels from these datasets. The resulting LAMM-Dataset has three appealing properties: 1) Existing multi-modal instruction tuning datasets mainly focus on holistic and rough information. To emphasize fine-grained and dense information, we add more visual information, such as visual relationships and fine-grained categories as input for the GPT-API. 2) We observe that existing MLLMs may struggle to understand vision task instructions. To address this, we designed a method to convert vision task annotations into instruction-response pairs, which enhances MLLMs' understanding and generalization of vision task instructions. 3) LAMM-Dataset also includes data pairs for commonsense knowledge question answering by incorporating a hierarchical knowledge graph label system from the Bamboo [10] dataset and the corresponding Wikipedia description.LAMM-Benchmark evaluates 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common point cloud tasks, by utilizing 3 datasets with over 12,788 data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies. 1) We are the very first attempt to establish a benchmark for MLLMs. We conducted a comprehensive benchmark to quantify the zero-shot and fine-tuning performance of existing multi-modal language models on various computer vision tasks and compare them against state-of-the-art methods of these tasks, including classification, object detection, pose estimation, visual question answering, facial classification, optical character recognition, object counting. 2) We also attempted two novel evaluation strategies designed explicitly for MLLMs. Specifically, as for text generation, we established a scoring logic based on the GPT-API. As for tasks involving interactions between points and images, such as object detection and pose estimation, we proposed an object-locating evaluation method.LAMM-FrameworkTo validate the effectiveness of LAMM-Dataset and LAMM-Benchmark, we propose a primary but potential MLLM training framework. To avoid modality conflicts caused by introducing multiple modalities, we differentiate the encoder, projector, and LLM finetuning blocks for different modalities in the framework design. Meanwhile, by adding encoders and decoders for other modalities, our framework can flexibly extend to cover more modalities and tasks, such as video understanding, image synthesis, and so on. We provide the results of our baseline models trained using this framework and various observations to accelerate future research.\n\nIntroduction\n\nLarge Language Models (LLM), notably the GPT series, have made remarkable progress toward achieving general artificial intelligence. The initial GPT-3 [1] model gains its generation ability, world knowledge, and in-context learning through pre-training and further acquires the ability to follow instructions and generalize to unseen tasks through instruction tuning [2]. The latest GPT-3.5 or GPT-4 [3] model has all these powerful abilities and can directly comprehend user intents and generalize to unknown real-world tasks [3,4]. LLM has become a ubiquitous model for natural language processing tasks. Almost all natural language understanding and generation tasks can be transformed into instruction inputs, enabling a single LLM to perform zero-shot generalization on various downstream applications [5].\n\n\nRelated Work\n\nMultimodal Large Language Model. With the rapid development of Large Language Models (LLM) such as ChatGPT, GPT-4 [3], many studies manage to explore incorporating other modalities based on LLM and they can be categorized into two perspectives. 1) System Design Perspective: Visual ChatGPT [11] and MMREACT [12] invoke various vision foundation models by processing user query to investigate the visual roles of ChatGPT with the help of Visual Foundation Models.\n\nViperGPT [13] instructs LLM to parse visual queries into interpretable steps expressed by Python code. HuggingGPT [14] extends its framework to more modalities by integrating more expert models on Huggingface. 2) End-to-End Trainable Model Perspective: The other methodology is to connect models for different modalities into an end-to-end trainable model, also known as multimodal large language model. Flamingo [15] proposes a unified architecture for language and vision modeling, while BLIP-2 [16] introduces a Querying Transformer to connect information from image to text modality. Kosmos [6] and PaLM-E [8] build an end-to-end trainable framework on web-scale multi-modal corpora. With the open-sourced LLaMA [17], Mini-GPT4 [18] optimizes a trainable projection matrix only, which connects pre-trained BLIP-2 style vision encoder and large language model, while LLaVA [7] and mPLUG-OwL [19] also finetune LLM. Besides feeding visual info to LLM as input only, LLaMA-Adapter [20], Multi-modal GPT [21] and Otter [22] also integrate multi modal information with intermediate features in LLM.\n\nInstruction Tuning. Instruction tuning [23] is a method proposed to improve the ability of large language models to follow instructions and enhance downstream task performance. Instruction-tuned models like InstructGPT [2], OPT-IML [24], Alpaca [25], have shown promising improvement compared to their based model. The existing instruction tuning datasets are primarily derived from collections of academic datasets like FLAN [23], chatbot data collected from ChatGPT usage such as ShareGPT, or constructed using self-instruction [9] methods like Alpaca. Apart from pure text instruction tuning datasets, Multi-Instruct [26] covers 47 multi-modal tasks. Mini-GPT4 [18] constructs instruction following dataset by composing image-text datasets and handwritten instruction templates. Moreover, LLaVA [7] feeds captions and bounding boxes as the context of COCO images to GPT-4 and therefore get 150K instruction data. Otter [22] builds such instruction tuning datasets from multi-modal MMC4 dataset [27] and incorporates in-contextual examples into instruction tuning by grouping similar instructions together.\n\nIn this work, LAMM follows the design of end-to-end trainable MLLM and employs a simple projection layer to connect vision encoder and LLM, which can be easily extended to more modalities. We extend existing vision datasets in 2D instruction datasets and build the first instruction tuning datasets for point cloud, to the best of our knowledge. Furthermore, we evaluated MLLMs on computer vision tasks with quantitative results rather than providing demonstrations only.\n\n\nLAMM-Dataset\n\nWe introduce a comprehensive multi-modal instruction tuning dataset, namely LAMM-Dataset. It involves images and point clouds from publicly available datasets for diverse vision tasks, as well as high-quality instructions and responses based on the GPT-API and self-instruction methods [9]. To be specific, LAMM-Dataset contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs. Figure 1 provides an overview of its construction process.\n\nWe design four kinds of multi-modal instruction-response pairs: 1) C1: n-round daily dialogue focuses on multi-modal daily conversations. 2) C2: n-round factual knowledge dialogue aims at factual knowledge reasoning. 3) C3: 1-round detailed description aims to elaborate images and 3D scenes in texts. 4) C4: 1-round visual task dialogue transfers various vision tasks into instructionresponse pairs, aiming at enhancing generalizability towards domain tasks in other modalities.\n\nWe include diverse 2D/3D vision tasks into LAMM-Dataset, such as captioning, scene graph recognition and VQA that are directly compatible with natural languages, as well as classification, detection, counting and OCR that output labels, bounding boxes, digits and a list of words instead. The former tasks are used in the first three categories, and the latter tasks are used in the fourth category. In LAMM-Dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets, which are referred in Figure 1.\n\nThe first three types of instruction-response pairs are generated by inputting several special designed signals to the GPT-API, namely system messages, in-context learning pairs and queries: (1) System messages are to inform the GPT-API about the task requirements. (2) Several in-context learning pairs are manually annotated to ensure that the rest instruction-response pairs can be generated by a similar fashion. (3) Queries include comprehensive annotations of captions, bounding boxes of objects, relations between objects, factual knowledges from the Bamboo's label system and their Wikipedia descriptions.  Figure 1: An overview of LAMM-Dataset. The above demonstrates the process of constructing our Instruction Tuning dataset using the GPT-API. By designing different system messages, in-context learning pairs, and queries, we have created the LAMM-Dataset that covers almost all high-level vision tasks for both 2D and 3D vision. The dataset includes four distinct groups: n-round Daily Dialogue, n-round Factual Knowledge Dialogue, 1-round Detailed Description, and 1-round Visual Dialogue. It is worth noting that for the introduction of vision tasks, we only used the GPT-API to generate instruction-response templates and did not directly generate dialogue data. Finally, some examples of the LAMM-Dataset are presented below, including 2D and 3D scenes and their corresponding instruction-response pairs.\n\n\nInputs GPT API Outputs\n\nThe last type of instruction-response pairs also apply the system messages and in-context learning pairs, but use GPT-API to generate a pool of templates of instruction-response pairs instead. In this way, ground-truth annotations of many vision tasks, such as object/keypoint detection, OCR, counting and etc., can be inserted into these templates, and thus are easier to be converted into reliable language responses, rather than aforementioned query-based conversion.\n\n\nLAMM-Benchmark\n\nDifferent from LLaVA [7], MiniGPT4 [18] and mPLUG-owl [19] that only provide demos and user studies to qualitatively evaluate the performances of their MLLMs, we propose the LAMM-Benchmark, which instead evaluates the quantitative performance of MLLMs on various 2D/3D vision tasks. It includes an inference pipeline and a set of evaluation metrics. To be specific, the LAMM-Benchmark-2D evaluates 9 common image tasks, using a total of 11 datasets with over 62,439 samples. The LAMM-Benchmark-3D evaluates 3 common point cloud tasks, by utilizing 3 datasets with over 12,788 data samples.\n\nQuery: <image> What is the category of the scene shown in the image?\n\nYou are performing an object detection task. Your goal is to locate all objects in the image, and give the corresponding coordinates.\n\nYour output should be a single class name. Your answer should be the object name and the bounding box, represented as [x1, y1, x2, y2] with floating numbers ranging from 0 to 1.\n\nQuery : <image> How many seashells are there in this image?\n\nYou are performing an object counting task. Your goal is to accurately count the number of objects in an image. Object counting is a computer vision task that involves detecting and counting the number of instances of specific objects within an image.\n\nThe answer needs to be an Arabic numeral.\n\nQuery : <point cloud> There is a TV in the room. It is on the wall. Give me the bounding box of it in the point cloud.\n\nYou are performing a point cloud grounding task, and our goal is to locate the position of objects in the question.\n\nYour answer should be a bounding box, represented as [x1, y1, z1, x2, y2, z2] with floating numbers ranging from 0 to 1.\n\nQuery : <point cloud> What is placed next to the fridge?\n\nYou are performing a visual question answering task, and your goal is to generate natural language answers that accurately solve the question.  Figure 2: An overview of LAMM Benchmark. It includes both 2D and 3D pipelines, covering multiple computer vision tasks. For each task, we provide the task definition, output structure, and a set of questions as instructions to the MLLM model. Then the entity extraction is applied on the output to extract the key answer. The LAMM Evaluation is used to evaluate the model's performance, which includes traditional metrics, binary-location metric and the GPT Metric.\n\nInference Pipeline. It ensures that the MLLMs can produce reasonable responses that can be fairly evaluated, which includes the way of processing input instructions and the extracting output entities. We construct the Inference Instruction to help the model better understand the task it is performing and the output structure that is required. Inference Instruction includes Task Definition, Output Structure and the usually employed Query Questions, as shown in Figure 2. Inspired by LLaVA [7], we also prompt the MLLM to perform complex reasoning followed by the final answer, so as to obtain a more reliable answer. Then, we employ the Natural Language Toolkit (NLTK) and regular expression matching to extract entities from the output text. These entities act as the results.\n\nEvaluation Metrics. The set of evaluation metrics includes Traditional Metrics, Binary Locating Metric, and GPT Metric. The Traditional Metrics are task-specific metrics from the listed 2D/3D vision tasks, which are the most rigorous to evaluate how MLLMs handle vision tasks. In the Binary Locating Metric, the model needs to output an approximated location of a recognized object through the instruction \"output the position of the object\", whose result is considered true if it is within the object's groundtruth bounding box. It is a straightforward metric to compare the localization ability of an MLLM model. To evaluate the understanding and question-answering abilitis of MLLM models, we utilize the GPT metric to evaluate the answers' relevance and accuracy to the groundtruth. To be specific, we prompt GPT to rank the responses of multiple MLLM models with the groundtruth results through the instruction described in Figure 2.\n\nEvaluation Settings. All 2D/3D vision tasks can be evaluated in a zero-shot manner, where the testing data have no intersection with MLLM's training data. Moreover, we also evaluate the finetuning ability of MLLMs on the test dataset about several mainstream tasks, such as detection, classification and VQA in 2D tasks, as well as detection, grounding and VQA in 3D tasks.\n\n\nExperiments and Results\n\n\nLAMM-Framework\n\nThe overall framework of our baseline MLLM is depicted in Figure 3. Each modality, image or point cloud, is processed by corresponding encoder, whose features are then projected to the same feature  Figure 3: Framework of multi-modality language model. Each modality is encoded by corresponding pre-trained encoder and decoded by LLM. LLM is shared among modalities and trainable projection layers and LoRA parameters are modality-specific.\n\nspace as the text embeddings by a trainable projection layer. Instructions are directly tokenized by SentencePiece tokenizer [28], then the vision and text tokens are concatenated to feed into the LLM model. To finetune LLM efficiently, we add LoRA [29] parameters to all projection layers in the self-attention layers. LoRA parameters for different vision modalities are not shared. Multimodal tokens are decoded by a shared LLM model and the corresponding LoRA parameters. As shown in Figure 3, only feature projectors and LoRA parameters are optimized during training. We use Vicuna-13B [30], as our LLM. Rank of LoRA modules are set to 32. We train all parameters including projection layers and LoRA modules in a one-stage end-to-end fashion with 4 A100 GPUs.\n\nInput images are resized to be 224\u00d7224 and split into 256 patches. We use CLIP [31] pre-trained ViT-L/14 and use image patch features output from transformer layers as image representations. We follow the design of FrozenCLIP [32] to encode point clouds, in which point cloud is tokenized to be 256 tokens by PointNet++ [33] and further encoded by CLIP pretrained ViT-L/14. Zero-shot Setting on Image Tasks. Table 1 shows the results of MLLM on 2D vision tasks by the Traditional Metrics. The \"SOTA\" column lists the results obtained by the best task-specific models. In contrast, the MLLM models were tested in a zero-shot setting. Although MLLM models demonstrated certain abilities of recognizing open-vocabulary classes, understanding images, and answering questions, they performed poorly on tasks involving object localization, including object detection, counting and keypoints detection. Localization-aware Tasks: In detection tasks, our LAMM baseline model demonstrated stronger localization ability, but there is still a significant gap between the predicted and the groundtruth bounding boxes. In counting tasks, the MLLM models showed a significant gap between the predicted and ground truth number of objects. MiniGPT4 failed in this task as it is unable to provide a specific number for most of the data. As for the keypoints detection task, we asked the MLLM models to predict the position of each human keypoint in turn. However, all the predicted positions were not in an acceptable range. The MLLM models showed a significant gap in this task, indicating that they have difficulty in accurately predicting keypoint locations. VQA Tasks: Our LAMM model demonstrated certain advantages in image understanding and multiple-choice question answering compared to other models. Note that the SOTA is the LLaVA trained on the ScienceQA dataset and evaluated with the GPT-4 answer generation component. But the LLaVA model we compared to was evaluated in the zero-shot setting. Additionally, we removed the random choice process from the LLaVA evaluation to obtain a more straightforward evaluation. Captioning Tasks: All MLLM models performed poorly on image captioning. We guess that BLEU4 is not an appropriate metric since longer captions may lead to lower scores. Classification Tasks: In fine-grained classification tasks and face classification tasks, all MLLMs performed poorly. Specifically, on the CelebA (Smile) dataset, the LLaVA model gave all \"yes\" answers, while the mPLUG model randomly gave predictions. However, in the CelebA (hair) dataset, the MLLM models can recognize hair color. These results suggest that the MLLM models may have difficulty in tasks that require fine-grained distinctions. OCR Tasks: In OCR tasks, LLaVA can recognize and extract text from images. However, the LAMM model performed poorly on this task.\n\n\nResults on Traditional Metrics\n\nFine-tuning Setting on Image Tasks. We also fine-tuned the LAMM baseline model on several vision datasets, including CIFAR10, VOC2012, and SQAimage. The results are shown in Table  2. The fine-tuned baseline achieved an accuracy of 91% on CIFAR10. It also achieved an mAP of 13% on VOC2012, in comparison with 4.8% in the zero-shot setting. These results indicate that our MLLM baseline model can receive the ability of localizing objects after being fine-tuned on detection data.\n\nZero-shot Setting on Point Cloud Tasks. Table 3 shows the result of LAMM model on 3D scene understanding tasks, under the zero-shot and fine-tuning settings, respectively. The colume \"SOTA\" presents the best performance on each task, and that for ScanQA is not available as we reformatted it to multiple-choice problem. The results after finetuning are significantly better than the zero-shot setting, in all test tasks. But these results are still inferior to the SOTA models. It is interesting to see that the LAMM finetuned on ScanQA multiple choice data almost achieves 100% accuracy, which may have an overfitting issue due to the narrow training/test gap and small scale of 3D dataset.    Table 4 shows the zero-shot results of the MLLM models on the proposed Binary Locating Metric and GPT Metric. The Binary Locating Metric covers the data from VOC2012, FSC147, and LSP. Since the LAMM baseline model has been trained on a small amount of data with detection instructions, it significant improvement in localizing accuracy.\n\nGPT Metric. We calculated GPT scores using a variety of tasks, including VQA, classification, captioning, as well as a small number of detection and counting tasks. As shown in Table 4, the LAMM model outperformed the LLAVA model overall. LLaVA's Response:\n\nThe image shows a group of people playing a game of volleyball on a beach. They are actively engaged in the game, with one man jumping in the air to hit the ball. The scene is lively and energetic, capturing the spirit of outdoor sports and beach activities.\n\n\nLAMM's Response:\n\nThe image captures an engaging scene from a professional sand volleyball game. Two men can be seen in the center of the court, fully immersed in the game. Both players are dressed in casual sportswear, with one wearing a white shirt and the other in a shirt of the same color. The sand court stretches across the image, providing ample space for the players to move around. There are multiple flags scattered across the scene, likely serving as markers or boundaries for the game. In the background, the sky provides a clear, light-colored backdrop to the lively action on the court. \n\n\nObservation and Analysis\n\nWe conducted dozens of experiments and observations on the MLLM model across various tasks to summarize its current capabilities and limitations.\n\nBetter Performance in Counting Tasks with Small Number of Objects. As shown in the Table 1, the recent MLLM models are not performing well on counting tasks. In the FSC147 dataset, there are data samples with dozens or even hundreds of objects, and these MLLM models would reply with \"I cannot accurately count the number\" for such data samples. Therefore, we conducted tests on the subset of the FSC147 dataset with less than 10 objects to evaluate the performance of the models on simple data, as shown in Figure 5 (b). The results show that the language model is able to roughly estimate the number of specified objects in the image, but it is still unable to provide an exact numerical value.\n\nGPT Metric is More Appropriate Than BLEU. Text with too much irrelevant information can lead to low BLEU4 scores, as shown in Table 1. Thus, we adopt the GPT Metric to assess the relevance and accuracy of the model's output captions to the ground truth captions. The results demonstrate that the LAMM outperforms LLaVA, which means the LAMM model is more able to generate high-quality, image-relevant text outputs.\n\n\nCapable of Object Localization but Struggles with Precise Bounding Box Prediction.\n\nWe visualize the results of LLaVA on VOC2012 dataset. Figure 4 (a) shows that the LAMM model was able to roughly point out a horse in the image, but was unable to accurately locate the entire object.\n\nLAMM Exhibits Fine-Grained Classification Ability on CIFAR10. As shown in Figure 4, when presented with a 32x32 pixel image of a car, the model's prediction was a more granular category: \"Fiat 500L 2012\", which accurately identifies the car's brand and model. The left sub figure in 4 (b) shows the image of Fiat 500L 2012 on Autoevolution [56], revealing that it has very similar features to the input image from CIFAR10. These results demonstrate that LAMM has the ability to perform more fine-grained classification, and is capable of recognizing subtle differences in images and assigning them to more specific categories.\n\nDifficulty in Comprehending Visual Information for Shifted Data. We conducted an analysis on several datasets that exhibit significant deviations from the training dataset, including UCMerced, CelebA, and LSP. The UCMerced dataset consists of top-down views of scenes, CelebA is a facial dataset that can describe the expressions and hair colors, and the LSP dataset involves 14 key points of the human body, they are significantly different from the COCO dataset during the training phase. These results suggest that the performance of the MLLM model may degrade significantly on datasets that exhibit significant deviations from the training dataset.\n\nDifficulty in Reading Text on SVT data. We analyzed the performance of our LAMM model on the SVT dataset and observed unsatisfactory results in Table 1. A possible explanation is that we used the TextVQA [57] dataset to generate visual task dialogue, which is more geared towards conversational text rather than OCR-related vision tasks. This mismatch in dataset characteristics may have resulted in suboptimal generalization of our model to the SVT dataset. To address this issue, we intend to conduct further investigations and incorporate more appropriate OCR data during the training process to improve our model's performance on OCR-related vision tasks.\n\nData volume validation on SQAimage data. As shown in Figure 5, our four types of image instruction tuning datasets outperform LLaVA [7] on all subsets, resulting in a 7% overall performance improvement for the complete LAMM-Dataset. Furthermore, we investigated the impact of sampling Daily Dialogue and Detailed Description data at different proportions. Notably, even with the small size of 10k examples, our LAMM-Dataset achieved comparable results to LLaVA-Dataset. As the dataset size increased, the overall performance of our model continuously improved, indicating that the LAMM-Dataset is scalable and can be further optimized by adding more data.\n\n\nConclusion\n\nWe present our research on extending multi-modal large language models (MLLMs) to point clouds and introduce the LAMM-Dataset and LAMM-Benchmark for image and point cloud understanding. Our contributions include presenting a large and comprehensive instruction-tuning dataset and benchmark, demonstrating the methods of constructing such datasets and benchmarks for MLLMs, and providing a primary but potential MLLM training framework optimized for modalities' extension. Our research shows that MLLMs can effectively handle visual modalities, including point clouds, and have the potential to generalize to various downstream applications through instruction tuning. By making our codebase, baseline model, instruction tuning dataset, and multi-modal language model benchmark publicly available, we hope to promote the development of an open research community for MLLMs. We believe that our research will facilitate future research on MLLMs and contribute to the development of artificial general intelligence.\n\n[62] Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico Tombari. Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d sequences.  \n\n\nB LAMM-Dataset\n\nThe paper introduces a novel method for constructing instruction tuning data, which is applied to generate the LAMM-dataset. This approach represents an innovative departure from traditional techniques that rely solely on daily dialogue and detailed description. Instead, the LAMM-dataset leverages additional factual knowledge extracted from Wikipedia to improve the quality and diversity of the training data. In addition, we also explore the use of traditional vision task data, covering common tasks in both 2D and 3D fields, which is converted into instruction tuning data for training purposes. By combining our new data construction method with traditional vision task data, we aim to improve the accuracy and effectiveness of instruction-tuning models in various vision-related applications. Specifically, we delve into the design of LAMM-Dataset-2D in Section B.1 and the design of LAMM-Dataset-3D in Section B.2. Finally, we provide a comprehensive explanation of the license and social impact information of our LAMM-Dataset in Section B.3.\n\n\nB.1 LAMM-Dataset-2D\n\nC1: n-round Daily Dialogue & C3: 1-round Detailed Description. The first step of our approach involves incorporating more visual information, such as visual relationships and fine-grained categories as input to GPT-API, providing dense visual context to the generated responses. To construct the C1: n-round Daily Dialogue and C3: 1-round Detailed Description data, we use the COCO images [58], similar to the LLAVA [7] approach. However, we further extract object attributes and relationships from the Visual Genome dataset [59] to emphasize fine-grained and dense information in the generated responses. Specifically, Our approach leverages image scene graph information to provide a structured representation of the objects and their relationships within the image. By doing so, we generated multi-modal dialogue data that enables us to capture the relationships between objects in the image and generate more accurate and natural language instructions. Figures 6 and 7 display the messages utilized to generate daily dialogue and detailed description data in the GPT-API. Additionally, Figure 8 provides detailed examples of the generated results for both types of data.\n\nC2: n-round Factual Knowledge Dialogue. In the second step of our approach, we expand the dataset by incorporating 42K classes of knowledge graph facts from Wikipedia using the Bamboo dataset. This addition enables MLLMs to generate question-answering data related to factual knowledge, which is a valuable addition to the dataset. To generate C2: n-round Factual Knowledge Dialogue data, we utilize the Bamboo dataset and Wikipedia to obtain relevant information, and then use GPT-API to generate a dialogue based on the given content. Specifically, we extract the QID labels and their corresponding Wikipedia descriptions from the Bamboo dataset to generate instruction-tuning training data. This approach allows us to incorporate common sense knowledge into the dataset, thereby enhancing the ability of MLLMs to generate responses that draw upon a broader range of factual knowledge. The messages used to generate factual knowledge data in the GPT-API are presented in Figure 9, while Figure 10 showcases detailed examples of the factual knowledge data generated by these messages.\n\nC4: 1-round Visual Task Dialogue. In addition to the three types of data discussed earlier, we also incorporate established computer vision tasks, such as image classification, object detection, keypoint detection, OCR, and object counting, into our dataset. This enables MLLMs to handle traditional computer vision tasks and generate responses that incorporate both language and visual information. The typical computer vision dataset consists of a set of images or videos, along with their corresponding labels or annotations that represent the desired output of the computer vision task, such as the class of objects present in the image or the location of an object. However, these discrete results are not suitable for large language model dialogues, as they do not allow for natural language messages = [ { \"You are an AI visual assistant that can analyze a single image. You receive five sentences, each describing the same image you are observing. In addition, specific object locations within the image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x, and bottom right y. It is worth noting that Attributes and Relationships of different objects are also given.\n\nYou need generate a n-round Daily Dialogue data between two people. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include at least one plausible question about the image, and provide the answer in detail. To answer such questions, one should require first understanding the visual content, then based on the background knowledge or reasoning, either explain why the things are happening that way, or provide guides and help to user's request. Make the question challenging by not including the visual content details in the question so that the user needs to reason about that first.\n\nInstead of directly mentioning the bounding box coordinates, the captions and relations, utilize those data to explain the scene using natural language. Include questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Always answer as if you are directly looking at the image. \"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample['context'] }) messages.append({\"role\":\"assistant\", \"content\":sample['response'] }) messages.append({\"role\":\"user\", \"content\":'\\n'.join(query)}) Instead of directly mentioning the bounding box coordinates, utilize this data to explain the scene using natural language. Include details like object counts, position of the objects, relative position between the objects. When using the information from the caption and coordinates, directly explain the scene, and do not mention that the information source is the caption, relationships or the bounding box. Always answer as if you are directly looking at the image. You can include multiple paragraphs if necessary.\"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample['context'] }) messages.append({\"role\":\"assistant\", \"content\":sample['response'] }) messages.append({\"role\":\"user\", \"content\":'\\n'.join(query)}) interactions. To address this issue, our proposed approach involves converting computer vision tasks, such as image classification, into natural language dialogues to enable large language models to perform these tasks through dialogue interactions. In detail, we first use GPT-API to generate a template pool of questions and answers for each task. Then, we randomly select a pair from the question template pool and answer template pool to combine with a piece of data from the computer vision dataset, creating the C4: 1-round Visual Task Dialogue data. Figure 27-30 provide some examples of the dialogues generated using our proposed approach for converting computer vision tasks into natural language dialogues. This approach allows us to leverage the rich visual information in traditional computer vision datasets and incorporate it into the instruction tuning process, thereby enhancing the ability of MLLMs to understand and respond to natural language instructions related to these tasks.\n\nIn summary, the construction of LAMM-Dataset-2D provides a comprehensive and diverse sample of real-world scenarios, incorporating fine-grained and dense information from object relationships and factual knowledge sources. The dataset contains 186K unique language-image instruction-following samples, including 49K in daily dialogues, 49K in detailed descriptions, 42K in factual Knowledge dialogues, and 46K in visual task dialogues. Our experiments showed that the use of GPT-API consistently provides higher-quality instruction-following data, such as spatial reasoning. These Instruction: <image> What kind of building has a clock tower in the image? Response: In the image, there is a church with a clock tower. Instruction : Is there anything special about the clock tower? Response : Yes, there is a large, fenced-in bell on top of the clock tower, which makes it unique. Instruction : What color is the clock on the tower? Response : The clock on the tower is white with black numerals and hands. Instruction : Can you describe the appearance of the sky in the image? Response : The sky in the image is blue. Instruction : What role might this clock tower and bell play in the local community? Response : The clock tower and bell might play a significant role in the local community by serving as a landmark and a focal point of the town or neighborhood. The clock tower, with its large, visible clock face, helps the residents keep track of time and maintain their daily schedules. In addition, the bell might be used to signal important events, such as religious services, weddings, or funerals. It could also serve as a reminder of community values and traditions, as well as a symbol of unity and shared history.\n\nInstruction : <image> What can you observe in this image? Response : The image presents a tall, white clock tower that stands out against a clear blue sky. The clock face is prominently visible, featuring white background and black hands. Above the clock, a fenced-in, rusty bell can be seen, adding an air of history to the tower. Furthermore, the tower also displays a cross at the top, signifying its connection to a church. The combination of the cross, bell, and clock creates a striking visual that showcases the tower's architectural and historical significance. You need to generate a n-round Factual Knowledge Dialogue data based on the received category lable and its English description. Please ask questions around the given category information and its corresponding description information, and don't ask questions that cannot be reasoned based on the given information. Questions and answers should be concise and effective, with reasoning logic. Answer all questions as you are seeing the image.\n\n\nC1: n-round Daily Dialogue\n\n\nC3: 1-round Detailed Description\n\nThe answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the image. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently. Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary \"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample['context'] }) messages.append({\"role\":\"assistant\", \"content\":sample['response'] }) messages.append({\"role\":\"user\", \"content\":'\\n'.join(query)}) features make LAMM-Dataset a valuable resource for researchers and practitioners working in the computer vision and natural language processing fields.\n\n\nB.2 LAMM-Dataset-3D\n\nThe construction pipeline of LAMM-Dataset-3D is similar to that of LAMM-Dataset-2D. However, due to the limited availability of 3D data, LAMM-Dataset-3D only consists of three major components: n-round plain conversation and 1-round detalied description data from GPT-API and 1-round visual dialogue data converted from datasets for existing 3D vision tasks.\n\nC1: n-round Daily Dialogue & C3: 1-round Detailed Description. To construct the C1: n-round Daily Dialogue and C3: 1-round Detailed Description data, we choose point clouds from 3RScan [61] as data source and use its original 3D bounding box annotations. Since there is no caption annotation for 3RScan, we input visual question answering (VQA) annotations from CLEVR3D [60] to GPT-API and ask it to convert the Q&A data into declarative sentences, which serves as point cloud captions in please help me to turn these questions and answer sentences into declarative sentences, there are many question-and-answer sentences, each will be separated by ***, these is a example: Are there the same number of sofas and wide sinks? no *** How many objects are tall beds or tall square beds? 1 *** There is a blue ottoman that is on the left side of the tv stand; is its shape the same as the brown tv stand? yes *** There is a couch that is close by the armchair that is lying on the low white cushion; what is its color? brown *** the results of the two question-answer sentences I hope you return is : There are not the same number of sofas and wide sinks. There is 1 object is tall bed or tall square bed. There is a blue ottoman that is on the left side of the tv stand and its shape is the same as the brown tv stand. There is a couch that is close by the armchair that is lying on the low white cushion and its color is brown. Please do not add 'yes or no' word at the first of the response sentence. \"\"\"} ] Figure 11: Message to transfer visual question answering annotations from CLEVR3D [60] to declarative sentences for 3D data.\n\nfurther steps. Figure 11 shows the corresponding prompts. Object attributes and relationships are extracted from scene graph annotation in 3DSSG [62]. Figure 12 and 13 show the prompts to let GPT-API generate daily dialogue and detailed description data for point clouds. Since full annotation of a scene point cloud may easily exceed input token limits of GPT-API, we randomly selected 10 captions and keep bounding box and relationships of corresponding objects as input contexts. For GPT-generated data, We limit the number of turns in each dialogue data to no more than 10, and any data exceeding this limit will be split into different samples. Figure 14 shows an example of GPT-generated data.\n\nC4: 1-round Visual Task Dialogue. On the other hand, we also leverage annotations for existing 3D vision tasks, such as point cloud classification, 3D object detection, and CLEVR3D for 3D VQA. Similar to 2D datasets, we designed 15 templates for instruction and response by sending definitions of the corresponding tasks to GPT-API. Then instruction data are formulated by replacing keywords with corresponding annotations. Templates of 3 tasks involved are presented in Figure 31, 32 and 33, respectively.\n\nFinally, the two subsets mentioned above of LAMM-Dataset-3D are combined. LAMM-Dataset-3D contains 10K samples in total, and the number of ShapeNet, 3RScan detection, CLEVR3D, and GPT-generated dialogue are 2K, 1.3K, 2K, and 4.9K, respectively. messages = [{\"\"\"You are an AI visual assistant that can analyze a single point cloud. You receive five sentences, each describing the same point cloud you are observing. In addition, specific object locations within the point cloud are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (cx, cy, cz, lx, ly, lz) with floating numbers in unit of meters. These values correspond to the top x, y, z coordinates of bounding box ceter and length of bounding box along x, y, z axis. It is worth noting that Attributes and Relationships of different objects are also given. Generate some questions and answers of the scene in a detailed manner. Instead of directly mentioning the bounding box coordinates or captions given, you should utilize this data to explain the question using natural language. Include details like the scenario, object counts, position of the objects, relative position between the objects. When using the information from the caption and coordinates, directly explain the scene, and do not mention that the information source is the caption, relationships or the bounding box. Answer questions or descriptions as if you really saw the whole scene, using the tone of seeing the scene to ask questions or answer. Please ask questions around the given category information and its corresponding description information, and don't ask questions that cannot be reasoned based on the given information. Questions and answers should be concise and effective, with reasoning logic. Answer all questions as you are seeing the point cloud. The answers should be in a tone that a visual AI assistant is seeing the point cloud and answering the question. You need use the provided caption, relationships and bounding box information to describe the scene in a detailed manner. Instead of directly mentioning the bounding box coordinates or captions given, you should utilize this data to explain the scene using natural language. Include details like the scenario, object counts, position of the objects, relative position between the objects. When using the information from the caption and coordinates, directly explain the scene, and do not mention that the information source is the caption, relationships or the bounding box. Answer questions or descriptions as if you really saw the whole scene, using the tone of seeing the scene to ask questions or answer. Always answer as if you are directly looking at the point cloud. You can include multiple paragraphs if necessary. Every question and answer must be related.\"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample['context'] }) messages.append({\"role\":\"assistant\", \"content\":sample['response'] }) messages.append({\"role\":\"user\", \"content\":'\\n'.join(query)}) Figure 13: Message to generate 1-round Detailed Description data in LAMM-Dataset-3D.\n\nCaptions: [There is a tall box and there are 6 boxes lower than it. The table that is made of the same material as the tall box is narrow. There are 9 boxes that are the same material as the door. The chair that is the same color as the glass window is tall. There are 7 objects that are either wide boxes higher than the low box or big shelves.]  \n\n\nObjects\n\n\nB.3 Social Impact\n\nThe LAMM-Dataset is a compilation of publicly available datasets that have been licensed under the Creative Commons license (CC-BY). We have taken great care to follow all necessary legal protocols to use this data in our research, and believe that transparency in data licensing is crucial for ensuring proper attribution and appropriate use of the data. Besides, the LAMM-Dataset includes images sourced from publicly available datasets and language data generated using the GPT-API. While we have taken steps to ensure appropriate content, we acknowledge that problematic content may exist. If you encounter any such content, please notify us immediately, and we will make necessary modifications to maintain a high-quality dataset that is free of inappropriate content. To protect the privacy of individuals and vehicles captured in the images, we plan to obfuscate sensitive information, such as faces and license plates, before publishing the LAMM-Dataset. We are committed to maintaining a dataset that is both high-quality and ethically responsible and pledge to uphold principles of privacy and transparency in our work.\n\n\nC LAMM-Benchmark\n\n\nC.1 LAMM-Benchmark on image tasks\n\nWe selected a set of nine commonly used CV tasks to evaluate the performance of MLLM models in our LAMM benchmark on image tasks. Our task selection criteria were based on widely studied tasks in the CV field that can showcase the MLLM model's abilities in visual interpretation, localization, and question-answering. Table 5 provides a summary of the tasks and the corresponding common evaluation metrics, which are based on the output that the MLLM models are required to generate for each task. We utilized a prompt-based approach to instruct the MLLM models to understand the task definition and generate the desired output. The ability of the models to understand and interpret the given instruction was also evaluated as part of the assessment criteria. As the models' outputs are text, we use different text-processing techniques for each task to extract entities as the final answers for evaluation. For each task, We selected datasets that are distinct from the training datasets, as our benchmark evaluation is conducted in an out-of-distribution zero-shot setting.\n\nClassification This task involves predicting the most likely category label for an image. For MLLM models, the task involves performing open-vocabulary classification. We selected CIFAR-10 [34] as the test dataset for the evaluation of classification. CIFAR10 contains 10000 test images across 10 common categories. We utilize NLTK to extract noun entities from the models' output text, and expand them to a synonym set for accuracy evaluation calculation.\n\n\nObject Detection\n\nWe selected the VOC 2012 [36] datasets to evaluate the model's ability to detect objects in images while considering both its visual interpretation and localization capabilities. To evaluate the accuracy of object category predictions, we employ a similar approach to classification tasks. We also use regular expression matching to extract the models' output bounding boxes for mAP50 calculation.\n\nVisual Question Answering We selected the ScienceQA [38] and AI2D [39] datasets to evaluate the MLLM model's ability to answer questions about images. The ScienceQA and AI2D datasets include over 2017 and 5793 multiple-choice questions with images, respectively. We extract the image-containing data from the ScienceQA dataset to create the SQAimage dataset. We then tested MLLM models on the SQAimage dataset to evaluate their multimodal understanding skills. As both ScienceQA and AI2D datasets are presented in a multiple-choice format, we evaluated the model's performance using the accuracy metric. Following LLaVA [7], we prompt the MLLM to output the complex reasoning procession, followed by the final option answer.\n\nImage Caption The image caption task involves generating a textual description of an image. We selected the Flickr30k [40] dataset to evaluate the MLLM model's ability to understand images and generate descriptive captions. Flickr30k contains a variety of objects and scenes with diverse captions, providing a challenging task for the MLLM model. To evaluate the quality of the models' text outputs, we split the generated text into sentences and calculate the BLEU-4 score for each. The highest score is selected as the final result.\n\nFine-grained classification Similar to the classification task, the fine-grained classification task requires the model to make predictions across a large number of fine-grained categories. We selected UCMerced Land Use dataset [42] as the test set. UCMerced Land Use contains 21 classes of land-use categories, including airports, forests, and residential areas. Similar to classification, we report Accuracy.\n\nObject counting We selected the FSC147 dataset for object counting evaluation. FSC147 [44] is a dataset of 1190 images containing various objects, including animals, vehicles, and household items. The images in this dataset are challenging and contain occlusions and overlapping objects, making it a suitable choice to test the model's object recognition and localization capabilities. We utilize regular expression matching to extract the numeric entity and evaluate the model's performance using the mean absolute error (MAE) metric.\n\n\nOptical Character Recognition\n\nThe OCR (Optical Character Recognition) task involves recognizing and transcribing text from images. To evaluate the MLLM model's ability to recognize text from images, we selected SVT dataset [46]. We extract the entities enclosed in quotation marks from the generated text as the predicted word list. Word Accuracy is adopted as the evaluation metric.\n\nFacial Classification Due to the difficulty of performing face recognition tasks using MLLM, we evaluated the model's performance on facial attribute classification tasks. We selected the CelebA [48] Figure 15: Template instructions for VQA inference. \"Response a\" is the generated reasoning process, which is the output of the first inference. \"Response b\" is the output answer, which is the ouput following the prompt \"### ANSWER\".\n\ndataset, which contains 19962 images for testing with annotations for 40 facial attributes, including hair color and facial expression. Specifically, we evaluated the model's ability to predict whether a person in an image is smiling, named CelebA(Smile) dataset, and the color of their hair, named CelebA(Hair) dataset. We aimed to evaluate the MLLM model's ability to understand facial images. Classification accuracy is used as the evaluation metric.\n\nKeypoints Detection To evaluate the models' ability to perform fine-grained point localization, we utilized the LSP [49] dataset for keypoint detection. To simplify the task difficulty for MLLM models, we employed a grounding approach, where we sequentially asked the model to predict the position of each human body keypoints in the image. The evaluation metric used for this task was PCK (Percentage of Correct Keypoints). Figure 17 shows the system messages defined for each image task. The system messages, which include the task definition and the output structure, is a part of the instruction that prompt the MLLM models to generated responses. This is designed to enable the model to better understand the task it is performing, focus on the critical aspects, and output the appropriate structure. Note that some tasks do not require a defined output structure. In such cases, the model can output any text as a response.\n\n\nC.2 Inference Details\n\n\nC.2.1 System messages for image tasks\n\n\nC.2.2 Instructions for VQA\n\nDifferent from other common image tasks, besides the system messages designed in C.2.1, we prompt MLLM to generate the reasoning process additionally, as figure 15 shows. To prompt the model to output its reasoning process, we first use conventional instruction texts to generate \"Response a\". We then combine the first instructions , the \"Response a\", and the prompt \"### ANSWER\" to make the model generate the option as the final answer.\n\n\nC.2.3 Metrics\n\nOur benchmark includes two evaluation settings. The first is a zero-shot setting, where we selected downstream tasks that have no intersection with the MLLM's training data. We provide the zero-shot results of the current MLLM models on these datasets. The second setting involves fine-tuning on mainstream task datasets, covering tasks such as detection, classification, and VQA.\n\n\nC.2.4 Binary Locating Metric\n\nThe ability to accurately localize objects in an image is a crucial component of MLLM models' visual understanding skills. In addition to using conventional detection tasks to calculate mAP, we attempted a more direct method for evaluating the models' localization ability, namely Binary Locating Metric. Distinct from object detection, which requires the model to output a bounding box, we instructed the model with \"output the position of the object\" instead of \"output the bounding box of the object\" to output the approximate position. During the evaluation phase, the model's predicted keypoint was considered correct as long as it was within the object's bounding box. Object locating is evaluated on all datasets involving object localization, including object detection, object counting, and keypoints detection. Compared to the traditional detection evaluation methods, the object locating evaluation method provides a more reasonable and direct approach for evaluating the localization ability of MLLM.\n\n\nC.2.5 GPT Metric\n\nTo evaluate the overall understanding and question-answering abilities of MLLM models, we utilized the GPT Metric. Unlike LLaVA [7] and Vicuna [30], we ranked the answers of multiple models using GPT. Similar to the pipeline approach, we give GPT an instruction, informing it of the task definition, the question, and the answer provided by each model. We then ranked each model's response based on its relevance and accuracy with the answer. Each model received a score based on its ranking, and the average score obtained on all test data served as a metric for measuring the model's overall ability. Our GPT evaluation datasets cover various visual tasks, including captioning and VQA tasks involving image description and answering, as well as a small number of detection and counting tasks related to object localization.\n\n\nC.3 LAMM-Benchmark on point cloud tasks\n\nFor LAMM-benchmark on point cloud tasks, we focus on three tasks of scene perception, including 3D object detection, visual grounding, and 3D visual question answering. Figure 18 presents system messages for point cloud tasks.\n\n3D Object Detection. As it's widely used in 3D object detection, we select ScanNetv2 [51] as the dataset to evaluate MLLM's ability to locate objects in a point cloud and identify semantics, whose validation set contains 312 scenes. In this task, MLLM is expected to list all objects along with bounding boxes, and we extract bounding boxes from the response text by entity extraction. Boxes whose IoU with ground truth is larger than 50% count for positive predictions and we use mean Average Precision (mAP) to evaluate performance.\n\nVisual Grounding. This task aims to locate the object described by a given caption and output the corresponding bounding box. We test on ScanRefer [53] in this task, which provides human-labeled captions towards each object in ScanNet and its test set contains 9508 samples. Similar with object detection, mean average precision (mAP) is reported to evaluate MLLM's capacity.\n\n3D Visual Question Answering. ScanQA [55] is proposed for 3D visual question answering before, and models are required to answer the given questions based on the point cloud. It has been formatted as an attribute classification task in previous work [55]. However, MLLM's output cannot be constrained with several classes consistently and is usually long text to explain details, so the original metrics in ScanQA, Exact Matching & BLEU, cannot be used for test, as long text is different from the style of given ground truth and the BLEU score inevitably decreases for long-text results. Following ScienceQA in 2D VQA task, we transfer this task to be a multiple-choice problem. First, we feed the original question-answer pairs to GPT-API and ask for 5 confusing options. Then MLLM is expected to choose the correct option or output the correct content. Thus, a metric of accuracy is used to evaluate model performance.\n\nEvaluation Settings Similar to evaluation for 2D tasks, our 3D benchmark includes two settings for evaluation. The first one is a zero-shot setting. MLLM is trained on instruction data from LAMM-Dataset-3D, where point clouds come from 3RScan or ShapeNet has no overlap with ones in downstream tasks. Furthermore, we finetune the models trained on LAMM-Dataset-3D by training a set of downstream tasks and reporting metrics on the corresponding test set.\n\n\nD Implementation Details\n\nIn our experiments, 2D and 3D models are trained independently, and only the feature projection layer and LoRA parameters are optimized during training while LLM can be shared among tasks.\n\nFor all experiments, trainable parameters are optimized by Adam optimizer with a learning rate initialized to be 5e-4, and scheduled using a linear decay scheduler. We For experiments on LAMM-Dataset-2D, models are trained for 2 epochs. For 3D experiments, we increase the number of iterations to 10,000 in case of too few samples. We use 4 A100-80GB to conduct experiments. Each GPU process 2 samples every iteration and the effective batch size are set to 64 by gradient accumulation. For reference, 2D experiments at most last for about 8 hours for 186K samples, while 3D experiments require about 3 hours.\n\nFollowing Vicuna [30], we format multi-modal training data as Figure 16.  Figure 16: Template for multi-modal data pairs. Bold words stand for corresponding text data and italic words indicate fixed templates. < vision > & < /vision > stand for start & end token for vision contents.\n\n[Response] refers to contents expected for LLM. The special tokens < vision > & < /vision > represents start and end positions for vision content. We use < Img >< /Img > and < P cl >< /P cl > in 2D and 3D datasets, respectively. The training objective used is next token prediction loss, and only text tokens of [Response] count for loss computation. As we use CLIP [31] pre-trained ViT-Large-14 as visual encoder, the number of vision tokens are 256 and length of text tokens after vision tokens are limited to 400 in training. \n\n\nE.2 More detailed information on image caption\n\nLAMM performed poorly on the Flickr30k dataset in terms of BLEU scores. This is because LAMM's responses include additional details that are not captured by the ground truth captions. Figure 20 illustrates this phenomenon, where the highlighted text in red represents the matching ground truth captions, while the text in orange is not matched but is still relevant to the image content. It is evident that LAMM is capable of providing more detailed descriptions of the image, which is not captured by the traditional BLEU metric.\n\n\nE.3 Comparison with LLaVA on detection and counting tasks\n\nWe compared the performance of our proposed LAMM with LLaVA on both object detection and counting tasks. Figure 22 illustrates the comparison results on detection, where the leftmost images represent the ground truth bounding box, and the rightmost images show the visualizations of the responses after entity extraction.\n\nAlthough LLaVA was able to identify the approximate location of the object, it was unable to provide precise bounding box coordinates. On the other hand, LAMM demonstrated superior detection capabilities after fine-tuning on detection-related data and was able to provide more accurate bounding box coordinates. Additionally, LAMM also exhibited better counting performance, as shown in Figure 23. It is worth noting that counting is essentially a task that tests the model's localization ability.\n\n\nE.4 Results of binary-loc metric and GPT metric\n\nWe present the results of LAMM and LLaVA on the binary locating metric in Figure 21 (a), where LAMM demonstrates more precise localization abilities. The green points in the image are the visualization of the predicted key points. In the second row of the figure, LAMM outputs a bounding box, which we break down into two position coordinates (top-left and bottom-right) during entity extraction.\n\nIn Figure 21 (b), we show the evaluation results of the two models' image captioning responses using the GPT metric. The GPT metric considers LAMM's responses to be more specific and accurate compared to LLaVA, resulting in a higher ranking for LAMM. These results further demonstrate the effectiveness of LAMM in accurately detecting, locating, and describing objects in images. Figure 24 shows the results of LAMM on VQA task and Figure 25 shows example results of LAMM on 3DVQA task. Figure 26 shows the results of LAMM on in-the-wild images.\n\n\nE.5 More demonstration examples\n\n\nDetection [System Message]\n\nYou are now performing an object detection task, and your goal is to locate all instances of objects in an image, such as people, cars, animals, or other objects, and give the corresponding coordinates.\n\nThese coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x, and bottom right y.\n\n\nClassification [System Message]\n\nYour primary objective as an AI assistant is to perform a classification task accurately and reliably, as this information is crucial for users to make informed decisions based on image data. To simply providing a class label for a given image, ensure that the classification is dependable and precise.\n\nPlease provide a label that accurately describes the subject of the image.\n\n\nVQA [System Message]\n\nAs an AI assistant, you are performing a visual question answering task, and your goal is to generate natural language answers that accurately solve the question. In order to generate accurate answers to questions about visual content, you must be able to understand the content of images, understand the meaning of questions, and perform complex reasoning processes.\n\n\nImage Caption [System Message]\n\nAs an AI assistant, your primary task is to perform image captioning, which requires you to generate clear and concise natural language descriptions of the visual content. To achieve this, you must be able to understand the visual content of the image, identify its salient features, and generate a coherent and contextually relevant caption that accurately conveys its meaning.\n\nGenerate descriptions of the visual context.\n\n\nFine-grained Classification [System Message]\n\nAs an AI assistant, your primary task is to perform image captioning, which requires you to generate clear and concise natural language descriptions of the visual content. To achieve this, you must be able to understand the visual content of the image, identify its salient features, and generate a coherent and contextually relevant caption that accurately conveys its meaning.\n\nPlease provide a fine-grained label that accurately describes the subject of the image.\n\n\nObject Counting [System Message]\n\nAs an AI assistant, you are performing an object counting task. Your goal is to accurately count the number of objects in an image. Object counting is a computer vision task that involves detecting and counting the number of instances of specific objects within an image. You need to analyze the input image and accurately count the number of objects in it.\n\nGive me a precise numerical result.\n\n\nOCR [System Message]\n\nYou are performing an Optical Character Recognition task, which involves recognizing and extracting text from images. To generate accurate answers to questions about the text content of images, you must be able to accurately recognize and extract text from images, and understand the meaning of questions.\n\nYour answer must be a list of words.\n\n\nFacial Classification [System Message]\n\nYou are performing an Optical Character Recognition task, which involves recognizing and extracting text from images. To generate accurate answers to questions about the text content of images, you must be able to accurately recognize and extract text from images, and understand the meaning of questions.\n\n\nKeypoints Detection [System Message]\n\nYou are an AI visual assistant that can analyze a single image and detect human key points. You will be provided with an image and specified which human body parts the user want you to detect. To generate accurate answers, you must be able to understand the content of images and comprehend the meaning of questions.\n\nPlease express the location as [x, y]. Both x and y are ratios between 0 and 1, indicating the position relative to the entire image. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language. You are now performing an visual ques\ufffdon and answering task, and your goal is to generate natural language answers that accurately solve the ques\ufffdon. In order to generate accurate answers to ques\ufffdons about visual content, you must be able to understand the content of point cloud, understand the meaning of ques\ufffdons, and perform complex reasoning processes.\n\n\n3D Object Detection [System Message]\n\nYou are now performing an object detec\ufffdon task, and your goal is to locate all instances of objects in a point cloud, such as furniture, transporta\ufffdons, or other objects, and give the corresponding coordinates.\n\n\n3D Visual Grounding [System Message]\n\nYou are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language. You are now performing an visual grounding task, and your goal is to locate the instances of objects I want with the given descrip\ufffdon in a point cloud, such as furniture, or other objects, and give the corresponding coordinates.\n\nThese coordinates are in the form of bounding boxes, represented as (cx, cy, cz, lx, ly, lz) with floa\ufffdng numbers in unit of meters. These values correspond to the x, y, z coordinates of bounding box center and length of bounding box along x, y, z axis.\n\nThese coordinates are in the form of bounding boxes, represented as (cx, cy, cz, lx, ly, lz) with floa\ufffdng numbers in unit of meters. These values correspond to x,y,z coordinates of center of bounding box and length of bounding box along x, y, z axis. \n\n\nLAMM's Response:\n\nThe caption of this image is \"This vibrant image features a man standing next to a slot machine in a casino. He appears to be checking his smartphone while wearing glasses. The slot machine is part of a row and the man is standing to the left of it. The background of the image is predominantly blue, with a white ceiling and lights installed on the machine. The slot machine itself has various buttons and symbols, including a circle, the number 7, and an orange circle. There is also a sign above the machine.\"\n\n\n(b)\n\nInstruction: What is the caption of this image?\n\n( \n\n\nLAMM's Response:\n\nThis image depicts a serene scene of two people sitting in a small boat on calm water, possibly at a lake. The boat, occupying the center of the image, has two people in it. One person is holding a paddle, indicating they may be in the middle of a paddle or rowing session. The water around the boat appears rippled, adding a sense of movement to the peaceful scene.\n\nLLaVA's Response:\n\nThe image captures a serene scene of two people riding in a boat on a large body of water, possibly a lake, during a beautiful sunset.\n\nLAMM has provided a more accurate and detailed image caption. The caption provided by LLaVA is more vague and lacks specific details such as the presence of a boat and the casual attire of the individuals in the image. Overall, LAMM's response is more helpful and relevant to the image.  LLaVA's Response: Unfortunately, I cannot provide exact coordinates for the hip key point in this image as it is a description and not a specific point on the image. However, I can tell you that the woman in the image is wearing shorts and is walking on a road. To determine the location of the hip key point, you would need to analyze the image yourself and identify the specific point or area on the image that corresponds to the hip region. LLaVA's Response: The image features a small bird with brown and orange feathers, which appears to be a finch, sitting on a wire. The bird is spreading its wings, possibly in flight or preparing to take off. There are no other objects in the image, so the only position to provide would be the coordinates of the bird, which is located at the top-left part of the image.  \n\n\nGround Truth\n\nLLaVA's Response: In the image, there is a grey and black cat sitting on a red couch, occupying the entire width of the couch. To provide the coordinates for this object, I would say the cat's position is at (0.125, 0.125) on the x-axis and (0.125, 0.875) on the y-axis. The cat is curled up, so its height might be slightly off the couch, but it is mostly lying on the red couch. LLaVA's Response:\n\nIn the image, there are several people on an airplane. One man is smiling and looking towards the camera, while another man is looking to his left. There are three people in the foreground, with one person on the left side, another in the middle, and the third person on the right side of the image. As for the objects in the scene, there are two ties visible, one located near the middle of the image and the other towards the right side. Additionally, there is a cup placed near the left side of the image. To provide the requested coordinates, I would need more information about the specific objects and their relative positions within the image.\n\n\nGround Truth\n\nInstruction: Detect all the objects in this image.   \n\n\nScanQA Instruction:\n\nWhat shape does the toilet have?\n\nOptions: (A) oval shape toilet. (B) a star shape. (C) a heart shape. (D) a triangular shape.\n\n(E) a diamond shape (F) a rectangular shape\n\n\nLAMM's Response:\n\nUpon careful examination of the point cloud, it becomes evident that the answer to this question is (A) oval shape toilet. The person in the image has a toe growth, which is a condition where an extra toe or a bony growth on the foot develops. This condition can be congenital, meaning it is present from birth, or it can develop later in life due to various causes.\n\n\nLAMM's Response:\n\nThe image shows a strawberry with a deformity, specifically a split in the fruit.\n\n\nInstruction:\n\nWhat's abnormal in this image?\n\nInstruction: What's abnormal in this image?\n\n\nLAMM's Response:\n\nThe snake in the image has two heads. This is considered abnormal because snakes typically have a single head. The presence of two heads in this snake indicates a genetic mutation or some other form of abnormal development.\n\nInstruction: What's the person in the picture doing?\n\nLAMM's Response:\n\nThe person in the picture is playing a guitar. The reasoning process is based on the fact that the instrument being held in the hand is a guitar, and the person is actively engaged in playing it.\n\n\nLAMM's Response:\n\nThe person in the picture is a woman who is riding a bicycle. She is dressed in a blue tank top and blue shorts, and she is wearing sunglasses. She is also carrying a backpack, which suggests that she might be on a journey or a long-distance ride.\n\nInstruction: What's the person in the picture doing?\n\nInstruction: What's the person in the picture doing?\n\nLAMM's Response:\n\nThe image portrays a young boy holding a pen and practicing writing with a quill. He is sitting on a chair and appears to be focusing on his work. This activity indicates a keen interest or an education in the art of calligraphy, a traditional form of handwriting.  14.Positioned at {P} within the image is an object that can be classified as belonging to the {C} category, and this object is also the only one in the image that has a specific type of texture and a distinctive shape that sets it apart from the other objects. 15.Upon careful examination of the image, it can be observed that there is an object positioned precisely at {P} that falls under the {C} category, and this object is also the only one in the image that has a specific type of pattern or design that makes it stand out from the rest of the objects. \n\n(Figure 4 :\n4Ground Truth: 1. \"A male volleyball player in red trunks tries to block the shot of the opposing player during a professional volleyball game.\" 2. \"Men playing volleyball, with one player missing the ball but hands still in the air.\" 3. \"A group of spectators watch a men's sand volleyball game.\" 4. \"Volleyball players compete on a sunny day at the beach.\" 5. \"Men playing volleyball in the sand.\") Observation and analysis on various tasks. (a) Visualization results on VOC2012. (b) Visualization results on CIFAR10. The left subfigure is from [56]. (c) Results on flickr30k.\n\nFigure 5 :\n5(a) Zero-shot Accuracy of LLaVA with different instructions in test. (b) Counting Performance on FSC147 of MLLMs. (c) Zero-shot accuracy of LAMM trained on various data combinations on SQAimage. (d) Zero-shot accuracy of LAMM model trained additional instruction data in LAMM-Dataset.\n\n\nLAMM-Dataset and code for LAMM-Benchmark has been open sourced at https://github. com/OpenLAMM/LAMM. In this Appendix, we present construction pipeline and more examples for LAMM-Dataset in Sec. B. Then, Sec. C shows details of LAMM-Benchmark and related evaluation metrics. Sec.D presents implementation details of LAMM-Framework and hyperparameter setting. Training a model based on LAMM-Framework takes about 20 A100 GPU hours. Finally, more examples and results are shown in Sec. E.\n\nFigure 6 :\n6Messages for construct n-round Daily Dialogue data in LAMM-Dataset-2D. messages = [ { \"You are an AI visual assistant that can analyze a single image. You receive five sentences, each describing the same image you are observing. In addition, specific object locations within the image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x, and bottom right y. It is worth noting that Attributes and Relationships of different objects are also given.You need generate a 1-round Detail Description data. You can use the provided caption, attributes, relationships and bounding box information to describe the scene in a detailed manner.\n\nFigure 7 :\n7Messages for construct 1-round Detailed Description data in LAMM-Dataset-2D.\n\nFigure 8 :\n8The example for generating n-round daily dialogue and 1-round detailed description data. messages = [ {\"\"\"You are an AI visual assistant that can analyze a single image. You receive a Category label for an image and its associated English description describing the same image you are looking at. The category information is the category label of the image you are seeing. In addition, The English description is a detailed semantic information for the given category.\n\nFigure 9 :\n9Messages for construct n-round Factual Knowledge Dialogue data in LAMM-Dataset-2D.\n\nFigure 10 :\n10Category: Women in Belarus.Description: The modern-day characteristics of women in Belarus evolved from the events that happened in the history of Belarus, particularly when the \"concept of equal rights for women was first developed and substantiated in the late 16th century\". The so-called Grand Duchy Charter of 1588 -one of the most important legal documents in Belarusian historyprotected the dignity of Belarusian women under the law. Women in Belarus and their contribution to Belarusian society is celebrated annually.(From Wikipedia page) Instruction : <image> What significant historical event marked the development of equal rights for women in Belarus? Response : The development of equal rights for women in Belarus was significantly marked by the \"Grand Duchy Charter of 1588\", one of the most important legal documents in Belarusian history, which protected the dignity of Belarusian women under the law. Instruction : When is the contribution of Belarusian women to their society celebrated annually? Response : The contribution of Belarusian women to their society is celebrated annually on the 8th of March, during International Women's Day. Instruction : How have the modern-day characteristics of women in Belarus been influenced? Response : The modern-day characteristics of women in Belarus have been influenced by the events that have occurred in the history of Belarus, particularly those related to the concept of equal rights for women. Instruction : Is there a specific legal document that stands out in the history of women's rights in Belarus ? Response : Yes, the Grand Duchy Charter of 1588 stands out as a significant legal document in the history of women's rights in Belarus. This charter protected the dignity of Belarusian women under the law. C2: n-round Factual Knowledge Dialogue The example for constructing n-round Factual Knowledge Dialogue data. messages = [{\"\"\"You are an expert in linguistics.\n\nFigure 12 :\n12Ask diverse questions and give corresponding answers. Include questions asking about the visual content of the point cloud. Only include questions that have definite answers: one can see the content in the point cloud that the question asks about and can answer confidently. All descriptions are attributes or relationships with other objects, which does not mean that there are people in the scene.\"\"\"} ] for sample in fewshot_samples: messages.append({\"role\":\"user\", \"content\":sample['context'] }) messages.append({\"role\":\"assistant\", \"content\":sample['response'] }) messages.append({\"role\":\"user\", \"content\":'\\n'.join(query)}) Message to generate n-round Daily Conversation Dialogue data in LAMM-Dataset-3D. messages = [{\"\"\"You are an AI visual assistant that can analyze a single point cloud. You receive five sentences, each describing the same point cloud you are observing.In addition, specific object locations within the point cloud are given, along with detailed coordinates. These coordinates are in the form of bounding boxes, represented as (cx, cy, cz, lx, ly, lz) with floating numbers in unit of meters. These values correspond to the top x, y, z coordinates of bounding box ceter and length of bounding box along x, y, z axis. It is worth noting that Attributes and Relationships of different objects are also given.\n\nFigure 14 :\n14Instruction: : <point cloud> What is the material of the tall box in the point cloud? Response: The tall box in the point cloud is made of the same material as the table, but the captions do not give information on the exact material. Instruction : \"How many boxes are in the point cloud? Response : There are a total of 16 boxes in the point cloud, with 9 of them made of the same material as the door and 6 of them lower than the tall box.. Instruction : Are there more desks or windows in the point cloud? Response : The captions mention that there aren't more desks being supported by the table than windows in the point cloud.\u2026 Instruction : <point cloud> Can you describe the scene in this point cloud? Response : The point cloud consists of a room with tiled flooring. There are four walls with two windows supported by the leftmost wall at different heights. Besides the walls, the point cloud shows six shelves on four walls, varying in size and position, with boxes placed on and behind them. There is also a narrow table made of the same material as the tall shelf with boxes placed on it. A single door hangs on the wall, and no other objects share the identical shape. There is no matching number of doors and shelves. Nine boxes are made of the same material as the door. C1: n-round Daily Dialogue C3: 1-round Detailed Description Example of GPT-generated n-round daily dialogue and 1-round detailed description data in LAMM-Dataset-3D.\n\n\n[SystemM esssage] specifies the corresponding task of sample, [Query] refers to position of texts from human and [System Message] ### Human: <vision>[Vision Tokens] </vision> [Query] ### Assistant: [Response]\n\nFigure 19\n19presents some examples of the LAMM model's responses on CIFAR10, where the model's answers were judged as incorrect in the evaluation, but in fact, LAMM provided a more granular classification result. The left column shows the test images from CIFAR10, and the right column displays the images of the objects that LAMM classified, including toad [63], Land Rover Series II [64], Mirage 2000D fighter aircraft [65] and police car [66]. It is evident that the fine-grained objects classified by LAMM have very similar features to the input images, demonstrating LAMM's ability to perform fine-grained classification.\n\nFigure 17 :\n17LAMM Benchmark system messages for image tasks 3D VQA [System Message]\n\nFigure 18 :Figure 19 :\n1819LAMM Benchmark system messages for point cloud tasks Instruction: What is the category for this image? LAMM's Response: Upon close inspection of the image, it can be concluded that the object of interest is a toad. (Ground Truth: \"frog\") Instruction: What is the category for this image? LAMM's Response: The image contains an object that can be classified as a police car based on the descriptions provided. (Ground Truth: \"automobile\") Instruction: What is the category for this image? LAMM's Response: Upon close inspection of the image, it can be concluded that the object of the image is a Mirage 2000D fighter aircraft. (Ground Truth: \"airplane\") Instruction: What is the category for this image? LAMM's Response: Upon close inspection of the image, it can be concluded that the object of the image is a Land Rover Series II. (Ground Truth: \"automobile\") LAMM's results on CIFAR10. (a) The images from CIFAT10 test set. (b) The instruction, response from LAMM and the ground truth. (c) The reference images.\n\n\nTell me the exact location of the hip key point for human body in the image.\n\n\nUpon careful examination of the image, it can be observed that the hip position is precisely located at [0.42, 0.39] position. Instruction: Tell me the exact location of the hip key point for human body in the image. Instruction: Tell me the exact location of the hip key point for human body in the image. LLaVA's Response: In the image, the hip key point for the human body can be located at approximately [0.224, 0.571]. This point is close to the center of the image, but slightly more towards the top. LAMM's Response: Upon close inspection of the image, it can be observed that the hip key point is positioned at [0.56, 0.38]. Instruction: Tell me the exact location of the hip key point for human body in the image. Instruction: Identify all the objects in the image and provide their positions. Instruction: Identify all the objects in the image and provide their positions.\n\n\nUpon close inspection of the image, it can be observed that there is an object positioned at [0.26, 0.29, 0.77, 0.96] that belongs to the bird category. LLaVA's Response: In the image, there is a small white dog wearing a red collar, with its mouth open, possibly barking or yawning. The dog is the main object of focus in the scene. However, there are no other objects or beings visible in the image. Therefore, I cannot provide the positions of any other objects or beings as there are none in the scene. LAMM's Response: There exists an object categorized as dog at the [0.58, 0.44, 1.0, 0.75] position of the image. Instruction: Identify all the objects in the image and provide their positions. Instruction: Identify all the objects in the image and provide their positions.\n\nFigure 21 :\n21Comparison of LAMM and LLaVA on binary-loc metric and GPT metric. (a) The comparison on binary-loc metric. (b) The results of GPT metric.\n\nFigure 22 :\n22Comparison of LAMM and LLaVA on VOC2012.\n\nFigure 23 :\n23Comparison of LAMM and LLaVA on FSC147.\n\n(Figure 25 :\n25Ground Truth: (A) oval shape toilet) Instruction: What is on the wall of the kitchen? Options: (A) a window. (B) a bookshelf. (C) a clock. (D) tv. (E) a family photo. (F) a painting. LAMM's Response: By closely observing the point cloud, one can derive that the answer to this question is (D) tv.(Ground Truth: (D) tv) Instruction: What objects are sitting on the black couch? Options: (A) dining table. (B) blue curtains. (C) ceiling fan. (D) 2 black cushions. (E) bicycle. (F) TV remote LAMM's Response: By carefully scrutinizing the details in the point cloud, it can be concluded that (D) 2 black cushions is the answer to this question.(Ground Truth: (D) 2 black cushions) Example results of LAMM on ScanQA. Instruction: What's abnormal in this image? LAMM's Response:\n\nFigure 26 :\n26Example results of LAMM on in-the-wild images.\n\nDetection Question Pool 1 .Figure 28 :\n128What is the identity of the objects visible in the image, and where are they located? 2.Can you name each object in the image and describe its position accurately? 3.What objects are visible in the image, and where can they be found? 4.From the visual information provided, can you identify all the objects present in the image and describe their positions? 5.What are the names of the objects present in the image, and where are they positioned? 6.Can you accurately describe the location of each object visible in the picture? 7.What is the identity of each object in the image, and where can they be located? 8.From the visual clues, can you name and locate all the objects present in the picture? 9.Can you identify and describe the positions of all the objects visible in the image? 10.What objects can you see in the picture, and where are they placed? 11.Based on the context of the image, can you identify all the objects present and describe their locations? 12.Can you accurately report the names and positions of all the objects visible in the image? 13.What are the objects visible in the picture, and where can they be found? 14.From the visual information provided, can you name all the objects in the image and describe their positions accurately? 15.What is the name of each object present in the image, and what is its location? 16.Can you locate and identify all the objects in the image and describe their positions accurately? 17.What objects are present in the image, and where are they positioned relative to each other? 18.Based on the visual clues, can you name and locate all the objects visible in the image? 19.Can you identify all the objects present in the image and describe their relative positions? 20.What is the identity of the objects visible in the image, and how are they positioned? Question template pool and Answer template pool for detection task in 2D vision.\n\nTable 1 :\n1Comparison of Multimodal Large Language Models on 2D computer vision tasks.Task \nDataset \nMetric \nSOTA LLaVA[7]MiniGPT4[18]mPLUG-owl[19]LAMM \n\nClassification \nCIFAR10 [34] \nAcc \u2191 \n99.5[35] 60.83 \n46.22 \n42.5 \n34.5 \nDetection \nVOC2012 [36] \nmAP \u2191 97.2[37] 1.42 \n0.92 \n0.158 \n4.82 \n\nVQA \nSQAimage [38] \nAcc \u2191 \n92.53 [7] 40.5 \n43.43 \n36.39 \n47.15 \nAI2D [39] \nN/A \n18.13 \nFailed \n19.31 \n19.5 \nImage Caption \nflickr30k [40] \nBLEU4 \u2191 30.1 [41] 6.65 \n5.1 \n2.74 \n0.70 \nF-g classification \nUCMerced [42] \nAcc \u2191 \n100[43] \n47 \n33.6 \n32.5 \n13 \nCounting \nFSC147 [44] \nMAE \u2193 10.79[45] 56.2 \nFailed \n60.67 \n53.97 \nOCR \nSVT [46] \nWord Acc \u2191 97.9 [47] 37.78 \n16.97 \n30.39 \n4.2 \n\nFacial Classification \nCelebA(Smile) [48] Acc \u2191 \nN/A \nFailed \n66.36 \nFailed \n51.3 \nCelebA(Hair) [48] \nN/A \n46.42 \n43.47 \n40.93 \n30.48 \nKeypoints Detection \nLSP [49] \nPCK \u2191 99.5 [50] Failed \nFailed \nFailed \nFailed \n\n\n\nTable 2 :\n2Results of LAMM model on selected 2D vision tasks. Both zero-shot test result and finetuned results reported. Metrics for classification and VQA is accuracy, and that for object detection is mAP@0.5.Task \nDataset \nLAMM (Zero-Shot) LAMM (Finetune) \nClassification \nCIFAR10 [34] \n34.5 \n91.2 \nObject Detection \nVOC2012 [36] \n4.82 \n13.48 \nVQA \nSQAimage [38] \n47.15 \n74.27 \n\n\n\nTable 3 :\n3Results of 3D tasks by LAMM. Metrics for 3D object detection and visual grounding is mAP@0.5, and that for 3D VQA is accuracy of multiple choice problem.Task \nDataset \nSOTA \nLAMM (Zero-Shot) LAMM (Finetune) \n3D Object Detection ScanNet[51] \n63.2[52] \n9.3 \n11.89 \nVisual Grounding \nScanRefer[53] 54.59[54] \nFailed \n3.38 \n3D VQA \nScanQA[55] \nN/A \n26.54 \n99.89 \n\n\n\nTable 4 :\n4Comparison of results of Binary Locating Metric and GPT Metric of existing MLLMs. The Binary-Locating Metric is the accuracy of the predicted position, and the GPT Metric is the score based on the given rank list from GPT.LLaVA MiniGPT4 mPLUG-owl LAMM \n\nBinary-Loc Metric 14.73 \n13.12 \n4.42 \n31.2 \n\nLLaVA LAMM \n\nGPT Metric 11 \n89 \n\n5.3 Results of Binary Locating Metric and GPT Metric \n\nBinary Locating Metric. \n\n\nhttps://en.wikipedia.org/wiki/Toad. 23 [64] https://robbreport.com/motors/cars. 23 [65] https://en.wikipedia.org/wiki/Dassault_Mirage_2000. 23 [66] https://en.wikipedia.org/wiki/Police_car. 23In Proceedings of \nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7515-7525, \n2021. 17 \n[63] A Appendix Overview \n\n\n\n\nCaptions: [a clock on a tower with a church steeple and bell. A clock tower has a fenced in bell on top. A very tall clock tower with a large bell sitting above it. The cross, bell and clock on the tower over a church. There is a tower clock on the wall.] Objects: bell: rusty, [0.4, 0.23, 0.57, 0.33]. clock: white, black, [0.18, 0.65, 0.77, 0.89]. building: white, tall, rusty, [0.14, 0.09, 0.9, 0.99]. sky: blue, [0.01, 0.0, 1.0, 0.7].\n\n\n: floor: brown,tiled,placing items on, same shape lab_name, [-2.25, 2.83, -1.59, 0.26, 7.81, 0.4]. shelf: placing items on,hanging,big, right box,same object type shelf, [-0.17, -2.12, -1.48, 0.16, 2.4, 1.33]. boxes: moving,placing items in,placing items on, [-1.21, 0.23, -1.46, 0.85, 0.52, 0.93] \u2026 \u2026\n\nTable 5 :\n5CV tasks for LAMM BenchmarkTask \nOutput \nMetrics \n\nClassification \nlabel name \nAcc \nDetection \nlist of object label and bbox mAP50 \nVQA \noption and answer \nAcc \nImage Caption \ncaptions \nBLEU4 \nFine-grained classification fine-grained label name \nAcc \nObject counting \nnumber \nMAE \nOCR \nlist of words \nword Acc \nFacial classification \nanswer \nAcc \nKeypoints detection \nkeypoints \nPCK \n3D Detection \nlist of object label and bbox mAP50 \n3D VQA \noption and answer \nAcc \n3D Visual Grounding \nbbox \nmAP50 \n\n\n\n\nInstruction: Detect all the objects in this image. LAMM's Response: There is an object at the [0.06, 0.1, 1.0, 1.0] location of the image that can be identified as belonging to the category of cat. Instruction: Detect all the objects in this image. Instruction: Detect all the objects in this image. ] position of the image, there is an item that falls under the category of human_face. You can spot an object belonging to the category of man at the [0.2, 0.1, 1.0, 1.0] position in the image. LLaVA's Response: In the image, there is a young girl posing for the picture. She is standing in front of a wall and leaning up against it. The girl is wearing a Batman shirt, which suggests she might be a fan of the Batman franchise. The bounding box for the girl would be around (0.25, 0.25, 0.75, 0.75) as she occupies most of the image. Instruction: Detect all the objects in this image. Instruction: Detect all the objects in this image. LAMM's Response: Scanning through the image, it becomes evident that there is an object at [0.0, 0.09, 0.93, 1.0] that falls under the man category. There is an object at the [0.39, 0.34, 0.56, 0.5] location of the image that can be identified as belonging to the category of human_face. An object that can be classified as person is located at the [0.0, 0.08, 0.92, 1.0] position of the image. At the [0.38, 0.34, 0.56, 0.5] position of the image, there is an item that falls under the category of human_beard. An object that can be classified as human_nose is located at the [0.46, 0.45, 0.5, 0.49] position of the image. At the [0.46, 0.43, 0.5, 0.47] position of the image, there is an item that falls under the category of human eye.LAMM's Response: \nAt the [0.46, 0.33, 0.75, 0.8Ground Truth \n\n\n\n\nInstruction: How many strawberries are there in this image? LLaVA's Response: There are six strawberries in the image. LAMM's Response: 10 Strawberries have been detected in the image according to the system's analysis. LLaVA's Response: There are hundreds of pairs of glasses in the image. LAMM's Response: 24 Sunglasses have been identified within the image by the system. LLaVA's Response: There are twelve comic books in the image. LAMM's Response: 18 Comic books have been identified in the image according to the system. LLaVA's Response: There are six elephants in this image. LAMM's Response: The system has identified 10 Elephant present in the image.(Ground Truth: 10) \n\nInstruction: How many glasses are there in this \nimage? \n\n(Ground Truth: 23) \n\nInstruction: How many comic books are \nthere in this image? \n\n(Ground Truth: 18) \n\nInstruction: How many elephants are \nthere in this image? \n\n(Ground Truth: 10) \n\n\n\n1 .\n1Can you identify the object in this image? 2.Based on the image's features, what could be the potential category label for this image? 3.What label would you assign to this image based on the object's shape and size? 4.According to the model's prediction, what is the label assigned to this image? 5.Can you provide the category label for this image based on the object's color and texture? 6.What label do you think best describes the image's content? 7.Based on the image's context, what category label would you assign to it? 8.Can you suggest any alternate labels for this image based on its content and features? 9.What is the most suitable category label for this image based on its shape, size, and context? 10.According to the model's classification, what is the category label assigned to this object? 11.Based on the image's visual cues, what category label do you think is the most appropriate? 12.Can you provide any additional labels that could be applied to this image based on its context and features? 13.What label would you assign to this image based on the object's function or purpose? 14.According to the image's features and context, what label do you think best represents it? 15.Can you suggest any potential alternate category labels that might be appropriate for this image based on its attributes? 16.What is the most accurate category label for this image based on its features, context, and meaning? 17.Based on the object's characteristics and the context of the image, what category label would you assign to it? 18.According to the image's attributes, what label would you assign to it? 19.Can you suggest any other category labels that could be applied to this image based on its features and meaning? 20.What label do you think best represents the object in this image based on its shape, color, and texture? 1.Upon close inspection of the image, it can be concluded that the majority of the objects in the image fall under the {C} category. 2.Through extensive analysis of the image, it can be confidently stated that the image belongs to the {C} category. 3.After careful examination of the image, it has been determined that the majority of the objects in the image can be classified as belonging to the {C} category. 4.Based on a detailed examination of the image, it can be concluded that the image primarily consists of objects that fall under the {C} category. 5.After closely analyzing the image, it has been determined that the main subject of the image belongs to the {C} category. 6.Through rigorous examination of the image, it has been concluded that the image is primarily focused on objects that can be classified as belonging to the {C} category. 7.Based on a thorough evaluation of the image, it can be confidently stated that the image is dominated by objects that fall under the {C} category. 8.After careful scrutiny of the image, it has been determined that the majority of the objects in the image can be identified as belonging to the {C} category. 9.Upon detailed analysis of the image, it can be concluded that the objects located at various positions in the image can be categorized as belonging to the {C} category. 10.After thorough analysis of the image, it can be confidently stated that the image falls under the {C} category. 11.Through careful scrutiny of the image, it can be confidently concluded that the image falls under the {C} category. 12.After meticulous examination of the image, it has been determined that the image can be classified as {C}. 13.After thorough inspection of the image, it can be confidently stated that the image falls under the {C} category. 14.After extensive scrutiny of the image, it has been determined that the majority of the objects in the image belong to the {C} category. 15.The image can be classified as {C} based on a close analysis of the objects and their characteristics.Figure 27: Question template pool and Answer template pool for classification task.1.An object that can be classified as {C} is located at the {P} position of the image. 2.The {C} object is present at the {P} coordinate in the image. 3.There is an object at the {P} location of the image that can be identified as belonging to the category of {C}. 4.An object categorized as {C} can be found at the {P} position in the image. 5.At the {P} position of the image, there is an item that falls under the category of {C}. 6.There exists an object categorized as {C} at the {P} position of the image. 7.The image contains an object that can be classified as {C} and is located at the {P} position. 8.You can spot an object belonging to the category of {C} at the {P} position in the image. 9.There is an object at the {P} position of the image, and its category is {C}. 10.Upon close inspection of the image, it can be observed that there is an object positioned at {P} that belongs to the {C} category.11.At the exact coordinates of {P} in the image, there is an object that can be identified as belonging to the {C} category, and this object stands out from the rest of the objects in the image due to its unique color and pattern. 12.Scanning through the image, it becomes evident that there is an object at {P} that falls under the {C} category.13.By carefully examining the image, one can spot an object at {P} that belongs to the {C} category.Classification Question Pool \n\nClassification Answer Pool \n\n\nInstruction:What is the caption of this image?(Ground Truth:A man standing in a bar with a neon beer sign hanging on the wall. 2. A man stands next to three video game machines and a beer sign. 3. A man stands next to a bank of computer gambling machines. 4. A man is standing by a group of video games in a bar. 5. A man stands next to three video machines. )Instruction: What is the caption of this image?LAMM's Response:The image depicts a young girl in a kitchen, seemingly learning about cooking. She is standing at a counter in front of a bowl of food. The bowl is filled with a variety of ingredients, including chopped onions and a dark purple fruit. The girl is holding a spatula, likely engaging in some form of cooking activity. The background reveals a sink, further suggesting the kitchen setting.Instruction: What is the caption of this image?(Ground Truth:A man sits on a bench holding his dog and looking at the water. 2. A man holding a dog sitting on a bench overlooking a lake. 3. A man and a dog sit on a bench near a body of water. 4. A man and his dog watch the sunset from a bench. 5. A man holds his dog near a pond. )LAMM's Response:This image captures a heartwarming scene of a man, seemingly lost in thought, holding a small black and white dog on his lap. The man and the dog are seated on a brown wooden bench, which occupies most of the lower portion of the frame. The bench is situated against a backdrop of grass, with a tree located slightly to the right of the bench. The image exudes a sense of tranquility and companionship between the man and his dog.\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 353Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022. 1, 3\n\n. abs/2303.08774OpenAI. Gpt-4 technical report. ArXiv. 12OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 1, 2\n\nHow does gpt obtain its ability? tracing emergent abilities of language models to their sources. Hao Fu, ; Yao, Tushar Peng, Khot, Yao Fu's NotionHao Fu, Yao; Peng and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion, Dec 2022. 1\n\nLanguage models are general-purpose interfaces. Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei, arXiv:2206.063362022arXiv preprintYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022. 1\n\nLanguage is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Qiang Owais Khan Mohammed, Liu, arXiv:2302.1404513arXiv preprintShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023. 1, 2, 3\n\nVisual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, 2022Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1, 3, 4, 5, 6, 9, 14, 20, 22\n\nPalm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.0337813arXiv preprintDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1, 3\n\nSelf-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, A Noah, Daniel Smith, Hannaneh Khashabi, Hajishirzi, arXiv:2212.1056023arXiv preprintYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- tions. arXiv preprint arXiv:2212.10560, 2022. 2, 3\n\nBamboo: Building mega-scale vision dataset continually with human-machine synergy. Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, Ziwei Liu, 2022Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang, Lu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu. Bamboo: Building mega-scale vision dataset continually with human-machine synergy, 2022. 2\n\nVisual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671arXiv preprintChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 2\n\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprintZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 2\n\nD\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick, Vipergpt, arXiv:2303.08128Visual inference via python execution for reasoning. arXiv preprintD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 3\n\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.17580arXiv preprintYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Reynolds, Advances in Neural Information Processing Systems. 35Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022. 3\n\nBlip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 3\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.105926arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3, 4, 6\n\n. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang, 6mplug-owl: Modularization empowers large language models with multimodality, 2023. 3, 4Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. 3, 4, 6\n\nLlama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.16199arXiv preprintRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 3\n\nMultimodal-gpt: A vision and language model for dialogue with humans. Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, Kai Chen, arXiv:2305.04790arXiv preprintTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023. 3\n\nOtter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, arXiv:2305.03726arXiv preprintBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. 3\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 3\n\nOpt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, O&apos; Brian, Gabriel Horo, Jeff Pereyra, Christopher Wang, Asli Dewan, Luke Celikyilmaz, Ves Zettlemoyer, Stoyanov, Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. Opt-iml: Scaling language model instruction meta learning through the lens of generalization, 2023. 3\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 3\n\nMultiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, arXiv:2212.107732022arXiv preprintZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022. 3\n\n. Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt, Openflamingo, Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 3\n\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2018 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsBrussels, BelgiumAssociation for Computational LinguisticsTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. 6\n\nJ Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, 622Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 6, 22\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLRInternational conference on machine learning. 623Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 6, 23\n\nFrozen clip model is efficient point cloud backbone. Xiaoshui Huang, Sheng Li, Wentao Qu, Tong He, Yifan Zuo, Wanli Ouyang, arXiv:2212.04098arXiv preprintXiaoshui Huang, Sheng Li, Wentao Qu, Tong He, Yifan Zuo, and Wanli Ouyang. Frozen clip model is efficient point cloud backbone. arXiv preprint arXiv:2212.04098, 2022. 6\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles R Qi, Hao Yi, Leonidas J Su, Guibas, arXiv:1706.02413arXiv preprintCharles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017. 6\n\nLearning multiple layers of features from tiny images. Handbook of Systemic Autoimmune Diseases. A Krizhevsky, G Hinton, 119A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Handbook of Systemic Autoimmune Diseases, 1(4), 2009. 6, 7, 19\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 6\n\nThe PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, 720M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal- network.org/challenges/VOC/voc2012/workshop/index.html. 6, 7, 20\n\nInternimage: Exploring large-scale vision foundation models with deformable convolutions. Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao, Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, and Yu Qiao. Internimage: Exploring large-scale vision foundation models with deformable convolutions, 2023. 6\n\nLearn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 6. 720Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Process- ing Systems (NeurIPS), 2022. 6, 7, 20\n\nA diagram is worth A dozen images. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, abs/1603.07396CoRR620Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth A dozen images. CoRR, abs/1603.07396, 2016. 6, 20\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, Transactions of the Association for Computational Linguistics. 220Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67-78, 2014. 6, 20\n\nUnified vision-language pre-training for image captioning and vqa. Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, Jianfeng Gao, Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa, 2019. 6\n\nBag-of-visual-words and spatial extensions for land-use classification. Yi Yang, Shawn Newsam, Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems. the 18th SIGSPATIAL international conference on advances in geographic information systems620Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas- sification. In Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270-279, 2010. 6, 20\n\nA continual development methodology for large-scale multitask dynamic ml systems. Andrea Gesmundo, Andrea Gesmundo. A continual development methodology for large-scale multitask dynamic ml systems, 2022. 6\n\nLearning to count everything. Udbhav Viresh Ranjan, Thu Sharma, Minh Nguyen, Hoai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)620Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai. Learning to count everything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6, 20\n\nA low-shot object counting network with iterative prototype adaptation. Nikola Djukic, Alan Lukezic, Vitjan Zavrtanik, Matej Kristan, Nikola Djukic, Alan Lukezic, Vitjan Zavrtanik, and Matej Kristan. A low-shot object counting network with iterative prototype adaptation, 2022. 6\n\nEnd-to-end scene text recognition. Kai Wang, Boris Babenko, Serge Belongie, 2011 International conference on computer vision. IEEE620Kai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. In 2011 International conference on computer vision, pages 1457-1464. IEEE, 2011. 6, 20\n\nScene text recognition with permuted autoregressive sequence models. Darwin Bautista, Rowel Atienza, Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models, 2022. 6\n\nLarge-scale celebfaces attributes (celeba) dataset. Retrieved. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, 1520Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018. 6, 20\n\nClustered pose and nonlinear appearance models for human pose estimation. Sam Johnson, Mark Everingham, British Machine Vision Conference. 621Sam Johnson and Mark Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In British Machine Vision Conference, 2010. 6, 21\n\nOmnipose: A multi-scale framework for multi-person pose estimation. Bruno Artacho, Andreas Savakis, Bruno Artacho and Andreas Savakis. Omnipose: A multi-scale framework for multi-person pose estimation, 2021. 6\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00dfner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEE722Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 7, 22\n\nSwin3d: A pretrained transformer backbone for 3d indoor scene understanding. Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, Baining Guo, arXiv:2304.06906arXiv preprintYu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3d: A pretrained transformer backbone for 3d indoor scene understanding. arXiv preprint arXiv:2304.06906, 2023. 7\n\nScanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu Chen, X Angel, Matthias Chang, Nie\u00dfner, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringer722Proceedings, Part XXDave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX, pages 202-221. Springer, 2020. 7, 22\n\nEda: Explicit text-decoupling and dense alignment for 3d visual grounding. Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, Jian Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and Jian Zhang. Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 7\n\nScanqa: 3d question answering for spatial scene understanding. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoki Kawanabe, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022722Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 7, 22\n\nTowards vqa models that can read. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 8\n\n. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in contextTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context, Jan 2014. 14\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei, abs/1602.07332CoRR14Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. CoRR, abs/1602.07332, 2016. 14\n\nClevr3d: Compositional language and elementary visual reasoning for question answering in 3d real-world scenes. Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, Shuguang Cui, arXiv:2112.116911617arXiv preprintXu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, and Shuguang Cui. Clevr3d: Compositional language and elementary visual reasoning for question answering in 3d real-world scenes. arXiv preprint arXiv:2112.11691, 2021. 16, 17\n\nRio: 3d object instance re-localization in changing indoor environments. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias Niessner, Proceedings IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)16Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Niessner. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings IEEE International Conference on Computer Vision (ICCV), 2019. 16\n\nThe system has identified a keypoint at {P} in the image that can be classified as {C}. The system has identified a keypoint at {P} in the image that can be classified as {C}.\n\nThere is a {C} keypoint located at the {P} position within the image according to the system's analysis. There is a {C} keypoint located at the {P} position within the image according to the system's analysis.\n\nThe image contains a keypoint that can be classified as {C} at the {P} position. The image contains a keypoint that can be classified as {C} at the {P} position.\n\nThe system has detected a {C} keypoint at the {P} coordinate in the image. The system has detected a {C} keypoint at the {P} coordinate in the image.\n\nThe {C} keypoint is present at the {P} position in the image, according to the system's analysis. The {C} keypoint is present at the {P} position in the image, according to the system's analysis.\n\nA keypoint that falls under the category of {C} is located at the {P} position of the image according to the system. 7.The system has identified a keypoint at the {P} position in the image that can be classified as {C}. A keypoint that falls under the category of {C} is located at the {P} position of the image according to the system. 7.The system has identified a keypoint at the {P} position in the image that can be classified as {C}.\n\nAt the {P} coordinate in the image, the system has detected a keypoint that falls under the category of {C}. At the {P} coordinate in the image, the system has detected a keypoint that falls under the category of {C}.\n\nThe image contains a keypoint that can be classified as {C} at the {P} position according to the system's analysis. 10.The system has located a {C} keypoint at the {P} position within the image. The image contains a keypoint that can be classified as {C} at the {P} position according to the system's analysis. 10.The system has located a {C} keypoint at the {P} position within the image.\n\nThere is a {C} keypoint present at the {P} position in the image according to the system's analysis. There is a {C} keypoint present at the {P} position in the image according to the system's analysis.\n\nA keypoint that falls under the category of {C} has been identified at the {P} position of the image by the system. 13.The system has classified a keypoint at the {P} position in the image as {C}. A keypoint that falls under the category of {C} has been identified at the {P} position of the image by the system. 13.The system has classified a keypoint at the {P} position in the image as {C}.\n\nAt the {P} coordinate in the image, the system has identified a keypoint that can be classified as {C}. At the {P} coordinate in the image, the system has identified a keypoint that can be classified as {C}.\n\nThe image contains a {C} keypoint at the {P} position according to the system's analysis. The image contains a {C} keypoint at the {P} position according to the system's analysis.\n\n{N} {C} have been identified in the image according to the system. {N} {C} have been identified in the image according to the system.\n\nThe system has detected {N} {C} in the image. The system has detected {N} {C} in the image.\n\nThe image contains {N} {C} according to the system's analysis. The image contains {N} {C} according to the system's analysis.\n\n{N} {C} have been detected within the image based on the system's analysis. {N} {C} have been detected within the image based on the system's analysis.\n\nThe system has identified {N} {C} present in the image. The system has identified {N} {C} present in the image.\n\nThere are {N} {C} visible in the image based on the system's analysis. There are {N} {C} visible in the image based on the system's analysis.\n\nThe image depicts {N} {C} according to the system. The image depicts {N} {C} according to the system.\n\n{N} {C} have been identified within the image by the system. {N} {C} have been identified within the image by the system.\n\nThe system has detected {N} {C} present in the image. The system has detected {N} {C} present in the image.\n\n{N} {C} have been detected in the image according to the system's analysis. {N} {C} have been detected in the image according to the system's analysis.\n\n{N} {C} have been detected within the image by the system's analysis. {N} {C} have been detected within the image by the system's analysis.\n\nAre you able to determine the exact count of objects that match a certain label in the image?. Are you able to determine the exact count of objects that match a certain label in the image?\n\nCan you identify the total number of objects with a certain label that are present in the image?. Can you identify the total number of objects with a certain label that are present in the image?\n\nQuestion template pool and Answer template pool for counting task in 2D vision. its scenario? 2.What is the best scenario label for this point cloud based on the model's output?. Figure. 30Figure 30: Question template pool and Answer template pool for counting task in 2D vision. its scenario? 2.What is the best scenario label for this point cloud based on the model's output?\n\nWhat are some other possible scenarios that could explain this point cloud?. What are some other possible scenarios that could explain this point cloud?\n\nWhat scenario does this point cloud belong to according to the model's prediction?. What scenario does this point cloud belong to according to the model's prediction?\n\nWhat is the most accurate point cloud scenario label for this point cloud based on its features, context, and meaning? 6.Can you suggest any other scenario labels that could be applied to this point cloud based on its features and meaning? 7.According to the point cloud's attributes. what scenario label would you assign to itWhat is the most accurate point cloud scenario label for this point cloud based on its features, context, and meaning? 6.Can you suggest any other scenario labels that could be applied to this point cloud based on its features and meaning? 7.According to the point cloud's attributes, what scenario label would you assign to it?\n\nWhat scenario label do you think best represents the object in this point cloud based on its features and objects? 10.Can you suggest any alternate scenario labels for this point cloud based on its content and features? 11.What is the most suitable scenario label for this point cloud based on its shape, size, and context? 12.According to the model's classification, what is the scenario label assigned to this point cloud? 13.Based on the point cloud's visual cues, what scenario label do you think is the most appropriate? 14.Can you provide any additional scenario labels that could be applied to this point cloud based on its context and features? 15. Based on the object's characteristics and the context of the point cloud. What scenario label would you assign to this point cloud based on the object's function or purpose? 3D Object Classification Answer PoolBased on the object's characteristics and the context of the point cloud, what point cloud scenario label would you assign to it? 9.What scenario label do you think best represents the object in this point cloud based on its features and objects? 10.Can you suggest any alternate scenario labels for this point cloud based on its content and features? 11.What is the most suitable scenario label for this point cloud based on its shape, size, and context? 12.According to the model's classification, what is the scenario label assigned to this point cloud? 13.Based on the point cloud's visual cues, what scenario label do you think is the most appropriate? 14.Can you provide any additional scenario labels that could be applied to this point cloud based on its context and features? 15.What scenario label would you assign to this point cloud based on the object's function or purpose? 3D Object Classification Answer Pool\n\nAfter conducting thorough analysis, it is evident that the point cloud in this scenario can be classified as {C}. After conducting thorough analysis, it is evident that the point cloud in this scenario can be classified as {C}.\n\nBy carefully examining the data, it becomes clear that {C} is the most appropriate classification for this point cloud scenario. By carefully examining the data, it becomes clear that {C} is the most appropriate classification for this point cloud scenario.\n\nTaking into account all the details, it can be determined that the point cloud falls under the scenario of {C}. Taking into account all the details, it can be determined that the point cloud falls under the scenario of {C}.\n\nThe analysis of this point cloud leads to the conclusion that it corresponds to the scenario of {C}. The analysis of this point cloud leads to the conclusion that it corresponds to the scenario of {C}.\n\nBased on a comprehensive assessment, it is evident that the point cloud can be accurately categorized as a scenario of {C}. Based on a comprehensive assessment, it is evident that the point cloud can be accurately categorized as a scenario of {C}.\n\nThis particular point cloud exhibits characteristics that align with the scenario of {C} upon closer examination. This particular point cloud exhibits characteristics that align with the scenario of {C} upon closer examination.\n\nConsidering the available information, it can be confidently stated that this point cloud conforms to the scenario of {C}. 8. The features present in this point cloud indicate that it can be classified as a scenario of {C}. Considering the available information, it can be confidently stated that this point cloud conforms to the scenario of {C}. 8. The features present in this point cloud indicate that it can be classified as a scenario of {C}.\n\nUpon careful scrutiny, it is apparent that the point cloud fits the description of {C} scenario. Upon careful scrutiny, it is apparent that the point cloud fits the description of {C} scenario.\n\nAnalyzing the data within this point cloud leads to the identification of {C} as the most suitable scenario. 11. The observed attributes of the point cloud confirm that it corresponds to the scenario of {C}. Analyzing the data within this point cloud leads to the identification of {C} as the most suitable scenario. 11. The observed attributes of the point cloud confirm that it corresponds to the scenario of {C}.\n\nThrough meticulous analysis, it becomes evident that the point cloud aligns with the characteristics of {C} scenario. 14. By thoroughly examining the point cloud, it becomes clear that the scenario it represents can be labeled as {C}. 15. The properties and structure of this point cloud provide strong evidence that it corresponds to the scenario of {C}. Figure 31: Question template pool and Answer template pool for object classification in 3D vision. Taking into account the available evidence, it is evident that this point cloud scenario can be characterized as {C}. 13. 3D Object Detection Question PoolTaking into account the available evidence, it is evident that this point cloud scenario can be characterized as {C}. 13. Through meticulous analysis, it becomes evident that the point cloud aligns with the characteristics of {C} scenario. 14. By thoroughly examining the point cloud, it becomes clear that the scenario it represents can be labeled as {C}. 15. The properties and structure of this point cloud provide strong evidence that it corresponds to the scenario of {C}. Figure 31: Question template pool and Answer template pool for object classification in 3D vision. 3D Object Detection Question Pool\n\nWhat is the identity of the objects visible in the point cloud, and where are they located? 2.Can you identify and describe the positions of all objects visible in the point cloud? 3.Analyzing the point cloud. please list all objects present and specify where they are locatedWhat is the identity of the objects visible in the point cloud, and where are they located? 2.Can you identify and describe the positions of all objects visible in the point cloud? 3.Analyzing the point cloud, please list all objects present and specify where they are located.\n\nBased on the visual data provided, name all the objects detected in the point cloud and describe their precise positions. 5.Can you accurately recognize and determine the locations of each object within the point cloud?. Based on the visual data provided, name all the objects detected in the point cloud and describe their precise positions. 5.Can you accurately recognize and determine the locations of each object within the point cloud?\n\nFrom the spatial information available, please identify all the objects present in the point cloud and provide details about their respective positions. From the spatial information available, please identify all the objects present in the point cloud and provide details about their respective positions.\n\nDescribe the objects captured in the point cloud and outline their exact coordinates within the scene. Describe the objects captured in the point cloud and outline their exact coordinates within the scene.\n\nBased on the given point cloud, identify and label the objects visible, specifying their spatial placements. Based on the given point cloud, identify and label the objects visible, specifying their spatial placements.\n\nProvide a detailed account of the objects observed in the point cloud. including their precise locationsProvide a detailed account of the objects observed in the point cloud, including their precise locations.\n\nAnalyze the point cloud and present a comprehensive inventory of the objects present, along with their respective positions. Analyze the point cloud and present a comprehensive inventory of the objects present, along with their respective positions.\n\nCan you precisely name and locate all the objects detected within the point cloud based on the provided data? 12.Based on the spatial context of the point cloud, identify all the objects present and describe where they are situated. Can you precisely name and locate all the objects detected within the point cloud based on the provided data? 12.Based on the spatial context of the point cloud, identify all the objects present and describe where they are situated.\n\nUtilizing the spatial cues within the point cloud, can you accurately detect and outline the positions of all objects? 15.Based on the given point cloud, determine the objects contained within and provide an overview of their locations. 14Please identify and describe the positions of each object visible in the point cloud based on the available visual information. 3D Object Detection Answer PoolPlease identify and describe the positions of each object visible in the point cloud based on the available visual information. 14.Utilizing the spatial cues within the point cloud, can you accurately detect and outline the positions of all objects? 15.Based on the given point cloud, determine the objects contained within and provide an overview of their locations. 3D Object Detection Answer Pool\n\nPositioned at the {P} location within the point cloud, an object within the {C} category can be observed. Positioned at the {P} location within the point cloud, an object within the {C} category can be observed.\n\nBy examining the {P} position of the point cloud, it is possible to identify an object categorized as {C}. By examining the {P} position of the point cloud, it is possible to identify an object categorized as {C}.\n\nThe point cloud includes an object at the {P} position, which can be classified under the category of {C}. The point cloud includes an object at the {P} position, which can be classified under the category of {C}.\n\nAt the {P} position in the point cloud, there is an item categorized as {C}. At the {P} position in the point cloud, there is an item categorized as {C}.\n\nThe {P} position of the point cloud allows for the identification of an object that belongs to the {C} category. The {P} position of the point cloud allows for the identification of an object that belongs to the {C} category.\n\nWithin the point cloud, an object classified as {C} is situated at the {P} position. Within the point cloud, an object classified as {C} is situated at the {P} position.\n\nAn object that can be classified as {C} is located at the {P} position within the point cloud. An object that can be classified as {C} is located at the {P} position within the point cloud.\n\nThe {P} position of the point cloud reveals the presence of an object categorized as {C}. The {P} position of the point cloud reveals the presence of an object categorized as {C}.\n\nAt the {P} position within the point cloud, there exists an object that falls under the {C} category. At the {P} position within the point cloud, there exists an object that falls under the {C} category.\n\nBy examining the {P} position of the point cloud, it is evident that an object belonging to the {C} category is present. 11.The point cloud contains an object at the {P} position. which can be identified as {C}By examining the {P} position of the point cloud, it is evident that an object belonging to the {C} category is present. 11.The point cloud contains an object at the {P} position, which can be identified as {C}.\n\nAn object within the {C} category can be observed at the {P} position in the point cloud. An object within the {C} category can be observed at the {P} position in the point cloud.\n\nThe {P} position of the point cloud displays the presence of an object that can be classified as {C}. The {P} position of the point cloud displays the presence of an object that can be classified as {C}.\n\nAt the {P} position of the point cloud, an object can be identified as belonging to the {C} category. At the {P} position of the point cloud, an object can be identified as belonging to the {C} category.\n\nBased on the visual content of the scenario, please answer the following questions. Based on the visual content of the scenario, please answer the following questions.\n\nUtilizing the details presented in the point cloud, please respond to the following questions. Utilizing the details presented in the point cloud, please respond to the following questions.\n\nAnalyze the visual elements of the point cloud and provide your answers to the following questions. Analyze the visual elements of the point cloud and provide your answers to the following questions.\n\nYour task is to examine the visual content of the point cloud and address the following questions. Your task is to examine the visual content of the point cloud and address the following questions.\n\nPlease utilize the details and visual cues depicted in the point cloud to answer the following questions. Please utilize the details and visual cues depicted in the point cloud to answer the following questions.\n\nGiven the information conveyed in the point cloud, please provide your responses to the following questions. Given the information conveyed in the point cloud, please provide your responses to the following questions.\n\nAnalyze the content of the point cloud and provide your answers to the following questions. 11.Use the visual elements. 10Based on the visual data presented, respond to the following questions using the point cloud as your reference. depicted in the point cloud to answer the following questions accuratelyBased on the visual data presented, respond to the following questions using the point cloud as your reference. 10.Analyze the content of the point cloud and provide your answers to the following questions. 11.Use the visual elements depicted in the point cloud to answer the following questions accurately.\n\nWithout any additional information, rely solely on the visual cues within the point cloud to address the following questions. Without any additional information, rely solely on the visual cues within the point cloud to address the following questions.\n\nUpon careful examination of the point cloud. it becomes evident that the answer to this question is {A}Upon careful examination of the point cloud, it becomes evident that the answer to this question is {A}.\n\nBy thoroughly analyzing the point cloud. one can reach the conclusion that {A} is the correct answer to this questionBy thoroughly analyzing the point cloud, one can reach the conclusion that {A} is the correct answer to this question.\n\nThe answer to this question can be deduced by closely observing the details within the point cloud. leading to the determination that {A} is the answerThe answer to this question can be deduced by closely observing the details within the point cloud, leading to the determination that {A} is the answer.\n\nBased on the analysis of the point cloud. it can be confidently stated that the answer to this question is {A}Based on the analysis of the point cloud, it can be confidently stated that the answer to this question is {A}.\n\nThe examination of the point cloud provides a clear indication that {A} is the answer to this question. The examination of the point cloud provides a clear indication that {A} is the answer to this question.\n\nBy closely studying the point cloud. one can ascertain that the correct answer to this question is {A}By closely studying the point cloud, one can ascertain that the correct answer to this question is {A}.\n\nAfter careful analysis of the point cloud. it is evident that {A} is the answer to this questionAfter careful analysis of the point cloud, it is evident that {A} is the answer to this question.\n\nThe answer to this question can be determined by carefully examining the details present in the point cloud. resulting in the conclusion that {A} is the answerThe answer to this question can be determined by carefully examining the details present in the point cloud, resulting in the conclusion that {A} is the answer.\n\nThrough a meticulous analysis of the point cloud. it becomes apparent that the answer to this question is {A}Through a meticulous analysis of the point cloud, it becomes apparent that the answer to this question is {A}.\n\nUpon thorough examination of the point cloud, it can be inferred that the answer to this question is {A}. 12.By closely observing the point cloud. one can derive that the answer to this question is {A}Upon thorough examination of the point cloud, it can be inferred that the answer to this question is {A}. 12.By closely observing the point cloud, one can derive that the answer to this question is {A}.\n\nAfter a detailed analysis of the point cloud, it is evident that the correct answer to this question is {A}. 14.The answer to this question can be revealed by closely inspecting the point cloud. and it is {A}After a detailed analysis of the point cloud, it is evident that the correct answer to this question is {A}. 14.The answer to this question can be revealed by closely inspecting the point cloud, and it is {A}.\n\nBy carefully scrutinizing the details in the point cloud, it can be concluded that {A} is the answer to this question. Figure 33: Question template pool and Answer template pool for visual question answering in 3D vision. By carefully scrutinizing the details in the point cloud, it can be concluded that {A} is the answer to this question. Figure 33: Question template pool and Answer template pool for visual question answering in 3D vision.\n", "annotations": {"author": "[{\"end\":146,\"start\":92},{\"end\":225,\"start\":147},{\"end\":276,\"start\":226},{\"end\":309,\"start\":277},{\"end\":375,\"start\":310},{\"end\":385,\"start\":376},{\"end\":434,\"start\":386},{\"end\":481,\"start\":435},{\"end\":484,\"start\":482},{\"end\":518,\"start\":485},{\"end\":559,\"start\":519},{\"end\":610,\"start\":560},{\"end\":613,\"start\":611},{\"end\":645,\"start\":614}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":100},{\"end\":157,\"start\":153},{\"end\":238,\"start\":235},{\"end\":287,\"start\":284},{\"end\":322,\"start\":319},{\"end\":384,\"start\":382},{\"end\":394,\"start\":389},{\"end\":442,\"start\":439},{\"end\":499,\"start\":494},{\"end\":531,\"start\":527},{\"end\":569,\"start\":565},{\"end\":626,\"start\":620}]", "author_first_name": "[{\"end\":99,\"start\":92},{\"end\":152,\"start\":147},{\"end\":234,\"start\":226},{\"end\":283,\"start\":277},{\"end\":318,\"start\":310},{\"end\":381,\"start\":376},{\"end\":388,\"start\":386},{\"end\":438,\"start\":435},{\"end\":483,\"start\":482},{\"end\":493,\"start\":485},{\"end\":526,\"start\":519},{\"end\":564,\"start\":560},{\"end\":612,\"start\":611},{\"end\":619,\"start\":614}]", "author_affiliation": "[{\"end\":145,\"start\":129},{\"end\":175,\"start\":159},{\"end\":224,\"start\":177},{\"end\":256,\"start\":240},{\"end\":275,\"start\":258},{\"end\":308,\"start\":289},{\"end\":340,\"start\":324},{\"end\":374,\"start\":342},{\"end\":412,\"start\":396},{\"end\":433,\"start\":414},{\"end\":480,\"start\":464},{\"end\":517,\"start\":501},{\"end\":558,\"start\":533},{\"end\":609,\"start\":593},{\"end\":644,\"start\":628}]", "title": "[{\"end\":89,\"start\":1},{\"end\":734,\"start\":646}]", "venue": null, "abstract": "[{\"end\":6990,\"start\":736}]", "bib_ref": "[{\"end\":7160,\"start\":7157},{\"end\":7376,\"start\":7373},{\"end\":7409,\"start\":7406},{\"end\":7536,\"start\":7533},{\"end\":7538,\"start\":7536},{\"end\":7816,\"start\":7813},{\"end\":7951,\"start\":7948},{\"end\":8128,\"start\":8124},{\"end\":8145,\"start\":8141},{\"end\":8311,\"start\":8307},{\"end\":8416,\"start\":8412},{\"end\":8715,\"start\":8711},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8799,\"start\":8795},{\"end\":8896,\"start\":8893},{\"end\":8911,\"start\":8908},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9018,\"start\":9014},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9034,\"start\":9030},{\"end\":9177,\"start\":9174},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9196,\"start\":9192},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9284,\"start\":9280},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9306,\"start\":9302},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9321,\"start\":9317},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9440,\"start\":9436},{\"end\":9619,\"start\":9616},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9633,\"start\":9629},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9646,\"start\":9642},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9827,\"start\":9823},{\"end\":9930,\"start\":9927},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10021,\"start\":10017},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10065,\"start\":10061},{\"end\":10198,\"start\":10195},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10323,\"start\":10319},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10398,\"start\":10394},{\"end\":11284,\"start\":11281},{\"end\":14465,\"start\":14462},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14480,\"start\":14476},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14499,\"start\":14495},{\"end\":17296,\"start\":17293},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19512,\"start\":19508},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19636,\"start\":19632},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19977,\"start\":19973},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20232,\"start\":20228},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20379,\"start\":20375},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20473,\"start\":20469},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":28739,\"start\":28735},{\"end\":29327,\"start\":29324},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":32528,\"start\":32524},{\"end\":32554,\"start\":32551},{\"end\":32664,\"start\":32660},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":43594,\"start\":43590},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":43779,\"start\":43775},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":44998,\"start\":44994},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":52230,\"start\":52226},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52543,\"start\":52539},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":52969,\"start\":52965},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":52983,\"start\":52979},{\"end\":53536,\"start\":53533},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":53761,\"start\":53757},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54407,\"start\":54403},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":54677,\"start\":54673},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":55353,\"start\":55349},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":55710,\"start\":55706},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":56521,\"start\":56517},{\"end\":59459,\"start\":59456},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":59475,\"start\":59471},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":60515,\"start\":60511},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":61113,\"start\":61109},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":61380,\"start\":61376},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":61593,\"start\":61589},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":63567,\"start\":63563},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":64201,\"start\":64197},{\"end\":70629,\"start\":70623},{\"end\":77978,\"start\":77976}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":78866,\"start\":78275},{\"attributes\":{\"id\":\"fig_1\"},\"end\":79164,\"start\":78867},{\"attributes\":{\"id\":\"fig_2\"},\"end\":79653,\"start\":79165},{\"attributes\":{\"id\":\"fig_3\"},\"end\":80489,\"start\":79654},{\"attributes\":{\"id\":\"fig_4\"},\"end\":80579,\"start\":80490},{\"attributes\":{\"id\":\"fig_5\"},\"end\":81061,\"start\":80580},{\"attributes\":{\"id\":\"fig_6\"},\"end\":81157,\"start\":81062},{\"attributes\":{\"id\":\"fig_7\"},\"end\":83111,\"start\":81158},{\"attributes\":{\"id\":\"fig_8\"},\"end\":84460,\"start\":83112},{\"attributes\":{\"id\":\"fig_9\"},\"end\":85927,\"start\":84461},{\"attributes\":{\"id\":\"fig_10\"},\"end\":86138,\"start\":85928},{\"attributes\":{\"id\":\"fig_11\"},\"end\":86766,\"start\":86139},{\"attributes\":{\"id\":\"fig_12\"},\"end\":86852,\"start\":86767},{\"attributes\":{\"id\":\"fig_13\"},\"end\":87894,\"start\":86853},{\"attributes\":{\"id\":\"fig_14\"},\"end\":87973,\"start\":87895},{\"attributes\":{\"id\":\"fig_15\"},\"end\":88858,\"start\":87974},{\"attributes\":{\"id\":\"fig_16\"},\"end\":89640,\"start\":88859},{\"attributes\":{\"id\":\"fig_17\"},\"end\":89793,\"start\":89641},{\"attributes\":{\"id\":\"fig_18\"},\"end\":89849,\"start\":89794},{\"attributes\":{\"id\":\"fig_19\"},\"end\":89904,\"start\":89850},{\"attributes\":{\"id\":\"fig_20\"},\"end\":90694,\"start\":89905},{\"attributes\":{\"id\":\"fig_21\"},\"end\":90756,\"start\":90695},{\"attributes\":{\"id\":\"fig_22\"},\"end\":92701,\"start\":90757},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":93591,\"start\":92702},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":93974,\"start\":93592},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":94347,\"start\":93975},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":94771,\"start\":94348},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":95108,\"start\":94772},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":95549,\"start\":95109},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":95853,\"start\":95550},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":96368,\"start\":95854},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":98108,\"start\":96369},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":99035,\"start\":98109},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":104427,\"start\":99036}]", "paragraph": "[{\"end\":7817,\"start\":7006},{\"end\":8296,\"start\":7834},{\"end\":9395,\"start\":8298},{\"end\":10505,\"start\":9397},{\"end\":10978,\"start\":10507},{\"end\":11477,\"start\":10995},{\"end\":11958,\"start\":11479},{\"end\":12502,\"start\":11960},{\"end\":13925,\"start\":12504},{\"end\":14422,\"start\":13952},{\"end\":15030,\"start\":14441},{\"end\":15100,\"start\":15032},{\"end\":15235,\"start\":15102},{\"end\":15414,\"start\":15237},{\"end\":15475,\"start\":15416},{\"end\":15728,\"start\":15477},{\"end\":15771,\"start\":15730},{\"end\":15891,\"start\":15773},{\"end\":16008,\"start\":15893},{\"end\":16130,\"start\":16010},{\"end\":16188,\"start\":16132},{\"end\":16799,\"start\":16190},{\"end\":17581,\"start\":16801},{\"end\":18521,\"start\":17583},{\"end\":18896,\"start\":18523},{\"end\":19381,\"start\":18941},{\"end\":20147,\"start\":19383},{\"end\":23002,\"start\":20149},{\"end\":23517,\"start\":23037},{\"end\":24550,\"start\":23519},{\"end\":24808,\"start\":24552},{\"end\":25068,\"start\":24810},{\"end\":25673,\"start\":25089},{\"end\":25847,\"start\":25702},{\"end\":26545,\"start\":25849},{\"end\":26961,\"start\":26547},{\"end\":27247,\"start\":27048},{\"end\":27875,\"start\":27249},{\"end\":28529,\"start\":27877},{\"end\":29190,\"start\":28531},{\"end\":29847,\"start\":29192},{\"end\":30874,\"start\":29862},{\"end\":31041,\"start\":30876},{\"end\":32111,\"start\":31060},{\"end\":33309,\"start\":32135},{\"end\":34396,\"start\":33311},{\"end\":35762,\"start\":34398},{\"end\":36462,\"start\":35764},{\"end\":38844,\"start\":36464},{\"end\":40571,\"start\":38846},{\"end\":41584,\"start\":40573},{\"end\":43021,\"start\":41650},{\"end\":43403,\"start\":43045},{\"end\":45036,\"start\":43405},{\"end\":45737,\"start\":45038},{\"end\":46245,\"start\":45739},{\"end\":49392,\"start\":46247},{\"end\":49742,\"start\":49394},{\"end\":50903,\"start\":49774},{\"end\":52035,\"start\":50960},{\"end\":52493,\"start\":52037},{\"end\":52911,\"start\":52514},{\"end\":53637,\"start\":52913},{\"end\":54173,\"start\":53639},{\"end\":54585,\"start\":54175},{\"end\":55122,\"start\":54587},{\"end\":55509,\"start\":55156},{\"end\":55944,\"start\":55511},{\"end\":56399,\"start\":55946},{\"end\":57330,\"start\":56401},{\"end\":57864,\"start\":57425},{\"end\":58262,\"start\":57882},{\"end\":59307,\"start\":58295},{\"end\":60154,\"start\":59328},{\"end\":60424,\"start\":60198},{\"end\":60960,\"start\":60426},{\"end\":61337,\"start\":60962},{\"end\":62260,\"start\":61339},{\"end\":62716,\"start\":62262},{\"end\":62933,\"start\":62745},{\"end\":63544,\"start\":62935},{\"end\":63829,\"start\":63546},{\"end\":64360,\"start\":63831},{\"end\":64941,\"start\":64411},{\"end\":65324,\"start\":65003},{\"end\":65823,\"start\":65326},{\"end\":66271,\"start\":65875},{\"end\":66818,\"start\":66273},{\"end\":67085,\"start\":66883},{\"end\":67305,\"start\":67087},{\"end\":67643,\"start\":67341},{\"end\":67719,\"start\":67645},{\"end\":68111,\"start\":67744},{\"end\":68524,\"start\":68146},{\"end\":68570,\"start\":68526},{\"end\":68997,\"start\":68619},{\"end\":69086,\"start\":68999},{\"end\":69480,\"start\":69123},{\"end\":69517,\"start\":69482},{\"end\":69847,\"start\":69542},{\"end\":69885,\"start\":69849},{\"end\":70233,\"start\":69928},{\"end\":70590,\"start\":70274},{\"end\":71221,\"start\":70592},{\"end\":71472,\"start\":71262},{\"end\":71879,\"start\":71513},{\"end\":72134,\"start\":71881},{\"end\":72387,\"start\":72136},{\"end\":72920,\"start\":72408},{\"end\":72975,\"start\":72928},{\"end\":72979,\"start\":72977},{\"end\":73366,\"start\":73000},{\"end\":73385,\"start\":73368},{\"end\":73521,\"start\":73387},{\"end\":74627,\"start\":73523},{\"end\":75042,\"start\":74644},{\"end\":75694,\"start\":75044},{\"end\":75764,\"start\":75711},{\"end\":75820,\"start\":75788},{\"end\":75914,\"start\":75822},{\"end\":75959,\"start\":75916},{\"end\":76346,\"start\":75980},{\"end\":76448,\"start\":76367},{\"end\":76495,\"start\":76465},{\"end\":76540,\"start\":76497},{\"end\":76784,\"start\":76561},{\"end\":76838,\"start\":76786},{\"end\":76856,\"start\":76840},{\"end\":77053,\"start\":76858},{\"end\":77321,\"start\":77074},{\"end\":77375,\"start\":77323},{\"end\":77429,\"start\":77377},{\"end\":77447,\"start\":77431},{\"end\":78274,\"start\":77449}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20564,\"start\":20557},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23219,\"start\":23211},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23566,\"start\":23559},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24221,\"start\":24214},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24736,\"start\":24729},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25939,\"start\":25932},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26680,\"start\":26673},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28682,\"start\":28675},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":51285,\"start\":51278}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":7004,\"start\":6992},{\"attributes\":{\"n\":\"2\"},\"end\":7832,\"start\":7820},{\"attributes\":{\"n\":\"3\"},\"end\":10993,\"start\":10981},{\"end\":13950,\"start\":13928},{\"attributes\":{\"n\":\"4\"},\"end\":14439,\"start\":14425},{\"attributes\":{\"n\":\"5\"},\"end\":18922,\"start\":18899},{\"attributes\":{\"n\":\"5.1\"},\"end\":18939,\"start\":18925},{\"attributes\":{\"n\":\"5.2\"},\"end\":23035,\"start\":23005},{\"end\":25087,\"start\":25071},{\"attributes\":{\"n\":\"5.4\"},\"end\":25700,\"start\":25676},{\"end\":27046,\"start\":26964},{\"attributes\":{\"n\":\"6\"},\"end\":29860,\"start\":29850},{\"end\":31058,\"start\":31044},{\"end\":32133,\"start\":32114},{\"end\":41613,\"start\":41587},{\"end\":41648,\"start\":41616},{\"end\":43043,\"start\":43024},{\"end\":49752,\"start\":49745},{\"end\":49772,\"start\":49755},{\"end\":50922,\"start\":50906},{\"end\":50958,\"start\":50925},{\"end\":52512,\"start\":52496},{\"end\":55154,\"start\":55125},{\"end\":57354,\"start\":57333},{\"end\":57394,\"start\":57357},{\"end\":57423,\"start\":57397},{\"end\":57880,\"start\":57867},{\"end\":58293,\"start\":58265},{\"end\":59326,\"start\":59310},{\"end\":60196,\"start\":60157},{\"end\":62743,\"start\":62719},{\"end\":64409,\"start\":64363},{\"end\":65001,\"start\":64944},{\"end\":65873,\"start\":65826},{\"end\":66852,\"start\":66821},{\"end\":66881,\"start\":66855},{\"end\":67339,\"start\":67308},{\"end\":67742,\"start\":67722},{\"end\":68144,\"start\":68114},{\"end\":68617,\"start\":68573},{\"end\":69121,\"start\":69089},{\"end\":69540,\"start\":69520},{\"end\":69926,\"start\":69888},{\"end\":70272,\"start\":70236},{\"end\":71260,\"start\":71224},{\"end\":71511,\"start\":71475},{\"end\":72406,\"start\":72390},{\"end\":72926,\"start\":72923},{\"end\":72998,\"start\":72982},{\"end\":74642,\"start\":74630},{\"end\":75709,\"start\":75697},{\"end\":75786,\"start\":75767},{\"end\":75978,\"start\":75962},{\"end\":76365,\"start\":76349},{\"end\":76463,\"start\":76451},{\"end\":76559,\"start\":76543},{\"end\":77072,\"start\":77056},{\"end\":78287,\"start\":78276},{\"end\":78878,\"start\":78868},{\"end\":79665,\"start\":79655},{\"end\":80501,\"start\":80491},{\"end\":80591,\"start\":80581},{\"end\":81073,\"start\":81063},{\"end\":81170,\"start\":81159},{\"end\":83124,\"start\":83113},{\"end\":84473,\"start\":84462},{\"end\":86149,\"start\":86140},{\"end\":86779,\"start\":86768},{\"end\":86876,\"start\":86854},{\"end\":89653,\"start\":89642},{\"end\":89806,\"start\":89795},{\"end\":89862,\"start\":89851},{\"end\":89918,\"start\":89906},{\"end\":90707,\"start\":90696},{\"end\":90796,\"start\":90758},{\"end\":92712,\"start\":92703},{\"end\":93602,\"start\":93593},{\"end\":93985,\"start\":93976},{\"end\":94358,\"start\":94349},{\"end\":95864,\"start\":95855},{\"end\":99040,\"start\":99037}]", "table": "[{\"end\":93591,\"start\":92789},{\"end\":93974,\"start\":93803},{\"end\":94347,\"start\":94140},{\"end\":94771,\"start\":94582},{\"end\":95108,\"start\":94966},{\"end\":96368,\"start\":95893},{\"end\":98108,\"start\":98046},{\"end\":99035,\"start\":98771},{\"end\":104427,\"start\":104367}]", "figure_caption": "[{\"end\":78866,\"start\":78289},{\"end\":79164,\"start\":78880},{\"end\":79653,\"start\":79167},{\"end\":80489,\"start\":79667},{\"end\":80579,\"start\":80503},{\"end\":81061,\"start\":80593},{\"end\":81157,\"start\":81075},{\"end\":83111,\"start\":81173},{\"end\":84460,\"start\":83127},{\"end\":85927,\"start\":84476},{\"end\":86138,\"start\":85930},{\"end\":86766,\"start\":86152},{\"end\":86852,\"start\":86782},{\"end\":87894,\"start\":86881},{\"end\":87973,\"start\":87897},{\"end\":88858,\"start\":87976},{\"end\":89640,\"start\":88861},{\"end\":89793,\"start\":89656},{\"end\":89849,\"start\":89809},{\"end\":89904,\"start\":89865},{\"end\":90694,\"start\":89921},{\"end\":90756,\"start\":90710},{\"end\":92701,\"start\":90800},{\"end\":92789,\"start\":92714},{\"end\":93803,\"start\":93604},{\"end\":94140,\"start\":93987},{\"end\":94582,\"start\":94360},{\"end\":94966,\"start\":94774},{\"end\":95549,\"start\":95111},{\"end\":95853,\"start\":95552},{\"end\":95893,\"start\":95866},{\"end\":98046,\"start\":96371},{\"end\":98771,\"start\":98111},{\"end\":104367,\"start\":99042}]", "figure_ref": "[{\"end\":11427,\"start\":11419},{\"end\":12501,\"start\":12493},{\"end\":13127,\"start\":13119},{\"end\":16342,\"start\":16334},{\"end\":17273,\"start\":17265},{\"end\":18520,\"start\":18512},{\"end\":19007,\"start\":18999},{\"end\":19148,\"start\":19140},{\"end\":19878,\"start\":19870},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26365,\"start\":26357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27110,\"start\":27102},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27331,\"start\":27323},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29253,\"start\":29245},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33233,\"start\":33225},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34292,\"start\":34284},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34309,\"start\":34300},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":38415,\"start\":38403},{\"end\":44921,\"start\":44912},{\"end\":45062,\"start\":45053},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45205,\"start\":45189},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45697,\"start\":45688},{\"end\":46219,\"start\":46210},{\"end\":49317,\"start\":49308},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":55720,\"start\":55711},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":56835,\"start\":56826},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":57588,\"start\":57579},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":60376,\"start\":60367},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":63617,\"start\":63608},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":63629,\"start\":63620},{\"end\":64604,\"start\":64595},{\"attributes\":{\"ref_id\":\"fig_18\"},\"end\":65117,\"start\":65108},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":65722,\"start\":65713},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":65958,\"start\":65949},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":66285,\"start\":66276},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":66662,\"start\":66653},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":66714,\"start\":66705},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":66769,\"start\":66760}]", "bib_author_first_name": "[{\"end\":106059,\"start\":106056},{\"end\":106075,\"start\":106067},{\"end\":106086,\"start\":106082},{\"end\":106101,\"start\":106094},{\"end\":106116,\"start\":106111},{\"end\":106118,\"start\":106117},{\"end\":106135,\"start\":106127},{\"end\":106152,\"start\":106146},{\"end\":106172,\"start\":106166},{\"end\":106186,\"start\":106180},{\"end\":106201,\"start\":106195},{\"end\":106610,\"start\":106606},{\"end\":106626,\"start\":106619},{\"end\":106633,\"start\":106631},{\"end\":106646,\"start\":106641},{\"end\":106663,\"start\":106656},{\"end\":106682,\"start\":106676},{\"end\":106697,\"start\":106692},{\"end\":106713,\"start\":106705},{\"end\":106731,\"start\":106723},{\"end\":106743,\"start\":106739},{\"end\":107325,\"start\":107322},{\"end\":107331,\"start\":107330},{\"end\":107343,\"start\":107337},{\"end\":107585,\"start\":107581},{\"end\":107596,\"start\":107591},{\"end\":107605,\"start\":107603},{\"end\":107619,\"start\":107612},{\"end\":107632,\"start\":107627},{\"end\":107644,\"start\":107638},{\"end\":107658,\"start\":107651},{\"end\":107667,\"start\":107663},{\"end\":107972,\"start\":107965},{\"end\":107982,\"start\":107980},{\"end\":107995,\"start\":107989},{\"end\":108006,\"start\":108002},{\"end\":108019,\"start\":108012},{\"end\":108036,\"start\":108029},{\"end\":108049,\"start\":108041},{\"end\":108057,\"start\":108054},{\"end\":108068,\"start\":108063},{\"end\":108418,\"start\":108411},{\"end\":108432,\"start\":108424},{\"end\":108445,\"start\":108437},{\"end\":108454,\"start\":108450},{\"end\":108458,\"start\":108455},{\"end\":108640,\"start\":108635},{\"end\":108652,\"start\":108649},{\"end\":108659,\"start\":108658},{\"end\":108661,\"start\":108660},{\"end\":108674,\"start\":108669},{\"end\":108693,\"start\":108684},{\"end\":108706,\"start\":108701},{\"end\":108724,\"start\":108718},{\"end\":108741,\"start\":108733},{\"end\":108753,\"start\":108749},{\"end\":108769,\"start\":108763},{\"end\":109136,\"start\":109129},{\"end\":109150,\"start\":109143},{\"end\":109165,\"start\":109158},{\"end\":109179,\"start\":109174},{\"end\":109186,\"start\":109185},{\"end\":109199,\"start\":109193},{\"end\":109215,\"start\":109207},{\"end\":109592,\"start\":109585},{\"end\":109608,\"start\":109600},{\"end\":109620,\"start\":109614},{\"end\":109632,\"start\":109627},{\"end\":109644,\"start\":109637},{\"end\":109653,\"start\":109650},{\"end\":109662,\"start\":109660},{\"end\":109672,\"start\":109670},{\"end\":109683,\"start\":109679},{\"end\":109695,\"start\":109690},{\"end\":110000,\"start\":109993},{\"end\":110014,\"start\":110005},{\"end\":110027,\"start\":110020},{\"end\":110040,\"start\":110032},{\"end\":110054,\"start\":110047},{\"end\":110064,\"start\":110061},{\"end\":110310,\"start\":110301},{\"end\":110323,\"start\":110317},{\"end\":110336,\"start\":110328},{\"end\":110348,\"start\":110343},{\"end\":110359,\"start\":110354},{\"end\":110377,\"start\":110371},{\"end\":110392,\"start\":110385},{\"end\":110400,\"start\":110398},{\"end\":110413,\"start\":110406},{\"end\":110426,\"start\":110420},{\"end\":110776,\"start\":110771},{\"end\":110790,\"start\":110784},{\"end\":110802,\"start\":110798},{\"end\":111140,\"start\":111131},{\"end\":111153,\"start\":111147},{\"end\":111162,\"start\":111160},{\"end\":111177,\"start\":111168},{\"end\":111189,\"start\":111182},{\"end\":111201,\"start\":111194},{\"end\":111500,\"start\":111496},{\"end\":111531,\"start\":111524},{\"end\":111548,\"start\":111541},{\"end\":111558,\"start\":111554},{\"end\":111570,\"start\":111566},{\"end\":111582,\"start\":111577},{\"end\":111597,\"start\":111591},{\"end\":111613,\"start\":111604},{\"end\":111629,\"start\":111622},{\"end\":112106,\"start\":112100},{\"end\":112117,\"start\":112111},{\"end\":112128,\"start\":112122},{\"end\":112145,\"start\":112139},{\"end\":112387,\"start\":112383},{\"end\":112404,\"start\":112397},{\"end\":112420,\"start\":112413},{\"end\":112436,\"start\":112430},{\"end\":112457,\"start\":112447},{\"end\":112475,\"start\":112467},{\"end\":112490,\"start\":112485},{\"end\":112513,\"start\":112509},{\"end\":112527,\"start\":112521},{\"end\":112978,\"start\":112973},{\"end\":112987,\"start\":112984},{\"end\":113002,\"start\":112994},{\"end\":113014,\"start\":113009},{\"end\":113026,\"start\":113019},{\"end\":113285,\"start\":113278},{\"end\":113297,\"start\":113290},{\"end\":113308,\"start\":113302},{\"end\":113318,\"start\":113313},{\"end\":113327,\"start\":113323},{\"end\":113339,\"start\":113333},{\"end\":113353,\"start\":113346},{\"end\":113365,\"start\":113360},{\"end\":113379,\"start\":113370},{\"end\":113389,\"start\":113385},{\"end\":113401,\"start\":113395},{\"end\":113418,\"start\":113409},{\"end\":113431,\"start\":113423},{\"end\":113442,\"start\":113436},{\"end\":113456,\"start\":113449},{\"end\":113467,\"start\":113463},{\"end\":113474,\"start\":113472},{\"end\":113485,\"start\":113482},{\"end\":113980,\"start\":113974},{\"end\":113995,\"start\":113988},{\"end\":114006,\"start\":114001},{\"end\":114021,\"start\":114013},{\"end\":114032,\"start\":114026},{\"end\":114041,\"start\":114038},{\"end\":114055,\"start\":114046},{\"end\":114064,\"start\":114060},{\"end\":114072,\"start\":114070},{\"end\":114415,\"start\":114412},{\"end\":114429,\"start\":114422},{\"end\":114442,\"start\":114435},{\"end\":114456,\"start\":114450},{\"end\":114467,\"start\":114463},{\"end\":114479,\"start\":114475},{\"end\":114492,\"start\":114486},{\"end\":114504,\"start\":114498},{\"end\":114516,\"start\":114512},{\"end\":114525,\"start\":114522},{\"end\":114863,\"start\":114861},{\"end\":114875,\"start\":114868},{\"end\":114890,\"start\":114883},{\"end\":114904,\"start\":114897},{\"end\":114919,\"start\":114911},{\"end\":114931,\"start\":114926},{\"end\":115207,\"start\":115202},{\"end\":115220,\"start\":115213},{\"end\":115229,\"start\":115228},{\"end\":115245,\"start\":115239},{\"end\":115257,\"start\":115252},{\"end\":115261,\"start\":115258},{\"end\":115272,\"start\":115267},{\"end\":115280,\"start\":115277},{\"end\":115294,\"start\":115293},{\"end\":115309,\"start\":115303},{\"end\":115664,\"start\":115654},{\"end\":115673,\"start\":115671},{\"end\":115697,\"start\":115688},{\"end\":115713,\"start\":115708},{\"end\":115730,\"start\":115724},{\"end\":115742,\"start\":115738},{\"end\":115751,\"start\":115747},{\"end\":115767,\"start\":115761},{\"end\":115778,\"start\":115774},{\"end\":115789,\"start\":115784},{\"end\":115807,\"start\":115803},{\"end\":115819,\"start\":115812},{\"end\":115834,\"start\":115827},{\"end\":115845,\"start\":115841},{\"end\":115866,\"start\":115855},{\"end\":115877,\"start\":115873},{\"end\":115889,\"start\":115885},{\"end\":115906,\"start\":115903},{\"end\":116366,\"start\":116361},{\"end\":116380,\"start\":116374},{\"end\":116398,\"start\":116392},{\"end\":116410,\"start\":116406},{\"end\":116426,\"start\":116419},{\"end\":116437,\"start\":116431},{\"end\":116453,\"start\":116448},{\"end\":116470,\"start\":116461},{\"end\":116472,\"start\":116471},{\"end\":116809,\"start\":116802},{\"end\":116818,\"start\":116814},{\"end\":116829,\"start\":116825},{\"end\":117038,\"start\":117034},{\"end\":117054,\"start\":117049},{\"end\":117066,\"start\":117060},{\"end\":117080,\"start\":117076},{\"end\":117094,\"start\":117089},{\"end\":117110,\"start\":117103},{\"end\":117123,\"start\":117116},{\"end\":117140,\"start\":117133},{\"end\":117154,\"start\":117149},{\"end\":117167,\"start\":117162},{\"end\":117181,\"start\":117176},{\"end\":117197,\"start\":117193},{\"end\":117201,\"start\":117198},{\"end\":117214,\"start\":117207},{\"end\":117232,\"start\":117224},{\"end\":117249,\"start\":117243},{\"end\":117646,\"start\":117642},{\"end\":117657,\"start\":117653},{\"end\":118283,\"start\":118282},{\"end\":118298,\"start\":118292},{\"end\":118310,\"start\":118303},{\"end\":118323,\"start\":118317},{\"end\":118339,\"start\":118332},{\"end\":118356,\"start\":118351},{\"end\":118363,\"start\":118361},{\"end\":118376,\"start\":118370},{\"end\":118757,\"start\":118750},{\"end\":118773,\"start\":118766},{\"end\":118780,\"start\":118778},{\"end\":118790,\"start\":118786},{\"end\":118806,\"start\":118798},{\"end\":118814,\"start\":118811},{\"end\":118829,\"start\":118822},{\"end\":118843,\"start\":118837},{\"end\":118859,\"start\":118852},{\"end\":118874,\"start\":118868},{\"end\":118876,\"start\":118875},{\"end\":118890,\"start\":118887},{\"end\":118903,\"start\":118899},{\"end\":118905,\"start\":118904},{\"end\":119252,\"start\":119248},{\"end\":119266,\"start\":119262},{\"end\":119271,\"start\":119267},{\"end\":119282,\"start\":119277},{\"end\":119298,\"start\":119292},{\"end\":119314,\"start\":119307},{\"end\":119328,\"start\":119320},{\"end\":119344,\"start\":119338},{\"end\":119359,\"start\":119353},{\"end\":119374,\"start\":119368},{\"end\":119388,\"start\":119384},{\"end\":119821,\"start\":119813},{\"end\":119834,\"start\":119829},{\"end\":119845,\"start\":119839},{\"end\":119854,\"start\":119850},{\"end\":119864,\"start\":119859},{\"end\":119875,\"start\":119870},{\"end\":120166,\"start\":120164},{\"end\":120184,\"start\":120181},{\"end\":120197,\"start\":120189},{\"end\":120199,\"start\":120198},{\"end\":120514,\"start\":120513},{\"end\":120528,\"start\":120527},{\"end\":120770,\"start\":120764},{\"end\":120789,\"start\":120784},{\"end\":120806,\"start\":120797},{\"end\":120823,\"start\":120819},{\"end\":120844,\"start\":120837},{\"end\":120857,\"start\":120851},{\"end\":120878,\"start\":120871},{\"end\":120897,\"start\":120889},{\"end\":120913,\"start\":120908},{\"end\":120930,\"start\":120923},{\"end\":120943,\"start\":120938},{\"end\":120959,\"start\":120955},{\"end\":121331,\"start\":121330},{\"end\":121345,\"start\":121344},{\"end\":121357,\"start\":121356},{\"end\":121361,\"start\":121358},{\"end\":121373,\"start\":121372},{\"end\":121381,\"start\":121380},{\"end\":121718,\"start\":121712},{\"end\":121731,\"start\":121725},{\"end\":121740,\"start\":121737},{\"end\":121755,\"start\":121747},{\"end\":121768,\"start\":121763},{\"end\":121779,\"start\":121773},{\"end\":121792,\"start\":121785},{\"end\":121801,\"start\":121797},{\"end\":121811,\"start\":121806},{\"end\":121825,\"start\":121816},{\"end\":121838,\"start\":121830},{\"end\":121847,\"start\":121845},{\"end\":122192,\"start\":122189},{\"end\":122204,\"start\":122197},{\"end\":122217,\"start\":122213},{\"end\":122228,\"start\":122223},{\"end\":122241,\"start\":122234},{\"end\":122258,\"start\":122249},{\"end\":122270,\"start\":122264},{\"end\":122285,\"start\":122280},{\"end\":122299,\"start\":122293},{\"end\":122742,\"start\":122733},{\"end\":122757,\"start\":122753},{\"end\":122771,\"start\":122767},{\"end\":122782,\"start\":122779},{\"end\":122787,\"start\":122783},{\"end\":122801,\"start\":122793},{\"end\":122817,\"start\":122814},{\"end\":123139,\"start\":123134},{\"end\":123152,\"start\":123147},{\"end\":123163,\"start\":123158},{\"end\":123177,\"start\":123172},{\"end\":123594,\"start\":123588},{\"end\":123606,\"start\":123601},{\"end\":123619,\"start\":123616},{\"end\":123634,\"start\":123627},{\"end\":123644,\"start\":123639},{\"end\":123646,\"start\":123645},{\"end\":123662,\"start\":123654},{\"end\":123903,\"start\":123901},{\"end\":123915,\"start\":123910},{\"end\":124450,\"start\":124444},{\"end\":124605,\"start\":124599},{\"end\":124624,\"start\":124621},{\"end\":124637,\"start\":124633},{\"end\":125089,\"start\":125083},{\"end\":125102,\"start\":125098},{\"end\":125118,\"start\":125112},{\"end\":125135,\"start\":125130},{\"end\":125330,\"start\":125327},{\"end\":125342,\"start\":125337},{\"end\":125357,\"start\":125352},{\"end\":125669,\"start\":125663},{\"end\":125685,\"start\":125680},{\"end\":125876,\"start\":125871},{\"end\":125886,\"start\":125882},{\"end\":125900,\"start\":125892},{\"end\":125913,\"start\":125907},{\"end\":126150,\"start\":126147},{\"end\":126164,\"start\":126160},{\"end\":126446,\"start\":126441},{\"end\":126463,\"start\":126456},{\"end\":126654,\"start\":126648},{\"end\":126665,\"start\":126660},{\"end\":126667,\"start\":126666},{\"end\":126682,\"start\":126675},{\"end\":126696,\"start\":126690},{\"end\":126711,\"start\":126705},{\"end\":126732,\"start\":126724},{\"end\":127169,\"start\":127164},{\"end\":127183,\"start\":127176},{\"end\":127196,\"start\":127189},{\"end\":127208,\"start\":127204},{\"end\":127217,\"start\":127214},{\"end\":127233,\"start\":127223},{\"end\":127243,\"start\":127240},{\"end\":127257,\"start\":127250},{\"end\":127599,\"start\":127588},{\"end\":127607,\"start\":127606},{\"end\":127623,\"start\":127615},{\"end\":128093,\"start\":128087},{\"end\":128104,\"start\":128098},{\"end\":128118,\"start\":128112},{\"end\":128131,\"start\":128126},{\"end\":128143,\"start\":128139},{\"end\":128616,\"start\":128610},{\"end\":128629,\"start\":128624},{\"end\":128647,\"start\":128641},{\"end\":128662,\"start\":128656},{\"end\":129130,\"start\":129121},{\"end\":129143,\"start\":129138},{\"end\":129159,\"start\":129155},{\"end\":129168,\"start\":129166},{\"end\":129182,\"start\":129176},{\"end\":129194,\"start\":129189},{\"end\":129206,\"start\":129202},{\"end\":129221,\"start\":129215},{\"end\":129661,\"start\":129653},{\"end\":129674,\"start\":129667},{\"end\":129687,\"start\":129682},{\"end\":129703,\"start\":129698},{\"end\":129716,\"start\":129710},{\"end\":129729,\"start\":129725},{\"end\":129744,\"start\":129739},{\"end\":129754,\"start\":129753},{\"end\":129763,\"start\":129755},{\"end\":130092,\"start\":130086},{\"end\":130106,\"start\":130102},{\"end\":130118,\"start\":130112},{\"end\":130132,\"start\":130126},{\"end\":130147,\"start\":130142},{\"end\":130160,\"start\":130154},{\"end\":130179,\"start\":130170},{\"end\":130192,\"start\":130186},{\"end\":130211,\"start\":130205},{\"end\":130221,\"start\":130216},{\"end\":130223,\"start\":130222},{\"end\":130239,\"start\":130232},{\"end\":130241,\"start\":130240},{\"end\":130255,\"start\":130253},{\"end\":130706,\"start\":130704},{\"end\":130718,\"start\":130712},{\"end\":130730,\"start\":130725},{\"end\":130743,\"start\":130735},{\"end\":130753,\"start\":130750},{\"end\":130763,\"start\":130759},{\"end\":130776,\"start\":130768},{\"end\":131137,\"start\":131130},{\"end\":131149,\"start\":131144},{\"end\":131167,\"start\":131161},{\"end\":131183,\"start\":131175},{\"end\":131201,\"start\":131193}]", "bib_author_last_name": "[{\"end\":106065,\"start\":106060},{\"end\":106080,\"start\":106076},{\"end\":106092,\"start\":106087},{\"end\":106109,\"start\":106102},{\"end\":106125,\"start\":106119},{\"end\":106144,\"start\":106136},{\"end\":106164,\"start\":106153},{\"end\":106178,\"start\":106173},{\"end\":106193,\"start\":106187},{\"end\":106208,\"start\":106202},{\"end\":106617,\"start\":106611},{\"end\":106629,\"start\":106627},{\"end\":106639,\"start\":106634},{\"end\":106654,\"start\":106647},{\"end\":106674,\"start\":106664},{\"end\":106690,\"start\":106683},{\"end\":106703,\"start\":106698},{\"end\":106721,\"start\":106714},{\"end\":106737,\"start\":106732},{\"end\":106747,\"start\":106744},{\"end\":107328,\"start\":107326},{\"end\":107335,\"start\":107332},{\"end\":107348,\"start\":107344},{\"end\":107354,\"start\":107350},{\"end\":107589,\"start\":107586},{\"end\":107601,\"start\":107597},{\"end\":107610,\"start\":107606},{\"end\":107625,\"start\":107620},{\"end\":107636,\"start\":107633},{\"end\":107649,\"start\":107645},{\"end\":107661,\"start\":107659},{\"end\":107671,\"start\":107668},{\"end\":107978,\"start\":107973},{\"end\":107987,\"start\":107983},{\"end\":108000,\"start\":107996},{\"end\":108010,\"start\":108007},{\"end\":108027,\"start\":108020},{\"end\":108039,\"start\":108037},{\"end\":108052,\"start\":108050},{\"end\":108061,\"start\":108058},{\"end\":108088,\"start\":108069},{\"end\":108093,\"start\":108090},{\"end\":108422,\"start\":108419},{\"end\":108435,\"start\":108433},{\"end\":108448,\"start\":108446},{\"end\":108462,\"start\":108459},{\"end\":108647,\"start\":108641},{\"end\":108656,\"start\":108653},{\"end\":108667,\"start\":108662},{\"end\":108682,\"start\":108675},{\"end\":108699,\"start\":108694},{\"end\":108716,\"start\":108707},{\"end\":108731,\"start\":108725},{\"end\":108747,\"start\":108742},{\"end\":108761,\"start\":108754},{\"end\":108775,\"start\":108770},{\"end\":108779,\"start\":108777},{\"end\":109141,\"start\":109137},{\"end\":109156,\"start\":109151},{\"end\":109172,\"start\":109166},{\"end\":109183,\"start\":109180},{\"end\":109191,\"start\":109187},{\"end\":109205,\"start\":109200},{\"end\":109224,\"start\":109216},{\"end\":109236,\"start\":109226},{\"end\":109598,\"start\":109593},{\"end\":109612,\"start\":109609},{\"end\":109625,\"start\":109621},{\"end\":109635,\"start\":109633},{\"end\":109648,\"start\":109645},{\"end\":109658,\"start\":109654},{\"end\":109668,\"start\":109663},{\"end\":109677,\"start\":109673},{\"end\":109688,\"start\":109684},{\"end\":109699,\"start\":109696},{\"end\":110003,\"start\":110001},{\"end\":110018,\"start\":110015},{\"end\":110030,\"start\":110028},{\"end\":110045,\"start\":110041},{\"end\":110059,\"start\":110055},{\"end\":110069,\"start\":110065},{\"end\":110315,\"start\":110311},{\"end\":110326,\"start\":110324},{\"end\":110341,\"start\":110337},{\"end\":110352,\"start\":110349},{\"end\":110369,\"start\":110360},{\"end\":110383,\"start\":110378},{\"end\":110396,\"start\":110393},{\"end\":110404,\"start\":110401},{\"end\":110418,\"start\":110414},{\"end\":110431,\"start\":110427},{\"end\":110782,\"start\":110777},{\"end\":110796,\"start\":110791},{\"end\":110811,\"start\":110803},{\"end\":110821,\"start\":110813},{\"end\":111145,\"start\":111141},{\"end\":111158,\"start\":111154},{\"end\":111166,\"start\":111163},{\"end\":111180,\"start\":111178},{\"end\":111192,\"start\":111190},{\"end\":111208,\"start\":111202},{\"end\":111522,\"start\":111501},{\"end\":111539,\"start\":111532},{\"end\":111552,\"start\":111549},{\"end\":111564,\"start\":111559},{\"end\":111575,\"start\":111571},{\"end\":111589,\"start\":111583},{\"end\":111602,\"start\":111598},{\"end\":111620,\"start\":111614},{\"end\":111638,\"start\":111630},{\"end\":111648,\"start\":111640},{\"end\":112109,\"start\":112107},{\"end\":112120,\"start\":112118},{\"end\":112137,\"start\":112129},{\"end\":112149,\"start\":112146},{\"end\":112395,\"start\":112388},{\"end\":112411,\"start\":112405},{\"end\":112428,\"start\":112421},{\"end\":112445,\"start\":112437},{\"end\":112465,\"start\":112458},{\"end\":112483,\"start\":112476},{\"end\":112507,\"start\":112491},{\"end\":112519,\"start\":112514},{\"end\":112534,\"start\":112528},{\"end\":112541,\"start\":112536},{\"end\":112982,\"start\":112979},{\"end\":112992,\"start\":112988},{\"end\":113007,\"start\":113003},{\"end\":113017,\"start\":113015},{\"end\":113036,\"start\":113027},{\"end\":113288,\"start\":113286},{\"end\":113300,\"start\":113298},{\"end\":113311,\"start\":113309},{\"end\":113321,\"start\":113319},{\"end\":113331,\"start\":113328},{\"end\":113344,\"start\":113340},{\"end\":113358,\"start\":113354},{\"end\":113368,\"start\":113366},{\"end\":113383,\"start\":113380},{\"end\":113393,\"start\":113390},{\"end\":113407,\"start\":113402},{\"end\":113421,\"start\":113419},{\"end\":113434,\"start\":113432},{\"end\":113447,\"start\":113443},{\"end\":113461,\"start\":113457},{\"end\":113470,\"start\":113468},{\"end\":113480,\"start\":113475},{\"end\":113491,\"start\":113486},{\"end\":113986,\"start\":113981},{\"end\":113999,\"start\":113996},{\"end\":114011,\"start\":114007},{\"end\":114024,\"start\":114022},{\"end\":114036,\"start\":114033},{\"end\":114044,\"start\":114042},{\"end\":114058,\"start\":114056},{\"end\":114068,\"start\":114065},{\"end\":114077,\"start\":114073},{\"end\":114420,\"start\":114416},{\"end\":114433,\"start\":114430},{\"end\":114448,\"start\":114443},{\"end\":114461,\"start\":114457},{\"end\":114473,\"start\":114468},{\"end\":114484,\"start\":114480},{\"end\":114496,\"start\":114493},{\"end\":114510,\"start\":114505},{\"end\":114520,\"start\":114517},{\"end\":114530,\"start\":114526},{\"end\":114866,\"start\":114864},{\"end\":114881,\"start\":114876},{\"end\":114895,\"start\":114891},{\"end\":114909,\"start\":114905},{\"end\":114924,\"start\":114920},{\"end\":114935,\"start\":114932},{\"end\":115211,\"start\":115208},{\"end\":115226,\"start\":115221},{\"end\":115237,\"start\":115230},{\"end\":115250,\"start\":115246},{\"end\":115265,\"start\":115262},{\"end\":115275,\"start\":115273},{\"end\":115287,\"start\":115281},{\"end\":115291,\"start\":115289},{\"end\":115301,\"start\":115295},{\"end\":115313,\"start\":115310},{\"end\":115317,\"start\":115315},{\"end\":115669,\"start\":115665},{\"end\":115686,\"start\":115674},{\"end\":115706,\"start\":115698},{\"end\":115722,\"start\":115714},{\"end\":115736,\"start\":115731},{\"end\":115745,\"start\":115743},{\"end\":115759,\"start\":115752},{\"end\":115772,\"start\":115768},{\"end\":115782,\"start\":115779},{\"end\":115801,\"start\":115790},{\"end\":115810,\"start\":115808},{\"end\":115825,\"start\":115820},{\"end\":115839,\"start\":115835},{\"end\":115853,\"start\":115846},{\"end\":115871,\"start\":115867},{\"end\":115883,\"start\":115878},{\"end\":115901,\"start\":115890},{\"end\":115918,\"start\":115907},{\"end\":115928,\"start\":115920},{\"end\":116372,\"start\":116367},{\"end\":116390,\"start\":116381},{\"end\":116404,\"start\":116399},{\"end\":116417,\"start\":116411},{\"end\":116429,\"start\":116427},{\"end\":116446,\"start\":116438},{\"end\":116459,\"start\":116454},{\"end\":116482,\"start\":116473},{\"end\":116812,\"start\":116810},{\"end\":116823,\"start\":116819},{\"end\":116835,\"start\":116830},{\"end\":117047,\"start\":117039},{\"end\":117058,\"start\":117055},{\"end\":117074,\"start\":117067},{\"end\":117087,\"start\":117081},{\"end\":117101,\"start\":117095},{\"end\":117114,\"start\":117111},{\"end\":117131,\"start\":117124},{\"end\":117147,\"start\":117141},{\"end\":117160,\"start\":117155},{\"end\":117174,\"start\":117168},{\"end\":117191,\"start\":117182},{\"end\":117205,\"start\":117202},{\"end\":117222,\"start\":117215},{\"end\":117241,\"start\":117233},{\"end\":117257,\"start\":117250},{\"end\":117271,\"start\":117259},{\"end\":117651,\"start\":117647},{\"end\":117668,\"start\":117658},{\"end\":118290,\"start\":118284},{\"end\":118301,\"start\":118299},{\"end\":118315,\"start\":118311},{\"end\":118330,\"start\":118324},{\"end\":118349,\"start\":118340},{\"end\":118359,\"start\":118357},{\"end\":118368,\"start\":118364},{\"end\":118381,\"start\":118377},{\"end\":118387,\"start\":118383},{\"end\":118393,\"start\":118389},{\"end\":118764,\"start\":118758},{\"end\":118776,\"start\":118774},{\"end\":118784,\"start\":118781},{\"end\":118796,\"start\":118791},{\"end\":118809,\"start\":118807},{\"end\":118820,\"start\":118815},{\"end\":118835,\"start\":118830},{\"end\":118850,\"start\":118844},{\"end\":118866,\"start\":118860},{\"end\":118885,\"start\":118877},{\"end\":118897,\"start\":118891},{\"end\":118910,\"start\":118906},{\"end\":119260,\"start\":119253},{\"end\":119275,\"start\":119272},{\"end\":119290,\"start\":119283},{\"end\":119305,\"start\":119299},{\"end\":119318,\"start\":119315},{\"end\":119336,\"start\":119329},{\"end\":119351,\"start\":119345},{\"end\":119366,\"start\":119360},{\"end\":119382,\"start\":119375},{\"end\":119394,\"start\":119389},{\"end\":119827,\"start\":119822},{\"end\":119837,\"start\":119835},{\"end\":119848,\"start\":119846},{\"end\":119857,\"start\":119855},{\"end\":119868,\"start\":119865},{\"end\":119882,\"start\":119876},{\"end\":120179,\"start\":120167},{\"end\":120187,\"start\":120185},{\"end\":120202,\"start\":120200},{\"end\":120210,\"start\":120204},{\"end\":120525,\"start\":120515},{\"end\":120535,\"start\":120529},{\"end\":120782,\"start\":120771},{\"end\":120795,\"start\":120790},{\"end\":120817,\"start\":120807},{\"end\":120835,\"start\":120824},{\"end\":120849,\"start\":120845},{\"end\":120869,\"start\":120858},{\"end\":120887,\"start\":120879},{\"end\":120906,\"start\":120898},{\"end\":120921,\"start\":120914},{\"end\":120936,\"start\":120931},{\"end\":120953,\"start\":120944},{\"end\":120967,\"start\":120960},{\"end\":121342,\"start\":121332},{\"end\":121354,\"start\":121346},{\"end\":121370,\"start\":121362},{\"end\":121378,\"start\":121374},{\"end\":121391,\"start\":121382},{\"end\":121723,\"start\":121719},{\"end\":121735,\"start\":121732},{\"end\":121745,\"start\":121741},{\"end\":121761,\"start\":121756},{\"end\":121771,\"start\":121769},{\"end\":121783,\"start\":121780},{\"end\":121795,\"start\":121793},{\"end\":121804,\"start\":121802},{\"end\":121814,\"start\":121812},{\"end\":121828,\"start\":121826},{\"end\":121843,\"start\":121839},{\"end\":121852,\"start\":121848},{\"end\":122195,\"start\":122193},{\"end\":122211,\"start\":122205},{\"end\":122221,\"start\":122218},{\"end\":122232,\"start\":122229},{\"end\":122247,\"start\":122242},{\"end\":122262,\"start\":122259},{\"end\":122278,\"start\":122271},{\"end\":122291,\"start\":122286},{\"end\":122306,\"start\":122300},{\"end\":122751,\"start\":122743},{\"end\":122765,\"start\":122758},{\"end\":122777,\"start\":122772},{\"end\":122791,\"start\":122788},{\"end\":122812,\"start\":122802},{\"end\":122825,\"start\":122818},{\"end\":123145,\"start\":123140},{\"end\":123156,\"start\":123153},{\"end\":123170,\"start\":123164},{\"end\":123189,\"start\":123178},{\"end\":123599,\"start\":123595},{\"end\":123614,\"start\":123607},{\"end\":123625,\"start\":123620},{\"end\":123637,\"start\":123635},{\"end\":123652,\"start\":123647},{\"end\":123666,\"start\":123663},{\"end\":123908,\"start\":123904},{\"end\":123922,\"start\":123916},{\"end\":124459,\"start\":124451},{\"end\":124619,\"start\":124606},{\"end\":124631,\"start\":124625},{\"end\":124644,\"start\":124638},{\"end\":124650,\"start\":124646},{\"end\":125096,\"start\":125090},{\"end\":125110,\"start\":125103},{\"end\":125128,\"start\":125119},{\"end\":125143,\"start\":125136},{\"end\":125335,\"start\":125331},{\"end\":125350,\"start\":125343},{\"end\":125366,\"start\":125358},{\"end\":125678,\"start\":125670},{\"end\":125693,\"start\":125686},{\"end\":125880,\"start\":125877},{\"end\":125890,\"start\":125887},{\"end\":125905,\"start\":125901},{\"end\":125918,\"start\":125914},{\"end\":126158,\"start\":126151},{\"end\":126175,\"start\":126165},{\"end\":126454,\"start\":126447},{\"end\":126471,\"start\":126464},{\"end\":126658,\"start\":126655},{\"end\":126673,\"start\":126668},{\"end\":126688,\"start\":126683},{\"end\":126703,\"start\":126697},{\"end\":126722,\"start\":126712},{\"end\":126740,\"start\":126733},{\"end\":127174,\"start\":127170},{\"end\":127187,\"start\":127184},{\"end\":127202,\"start\":127197},{\"end\":127212,\"start\":127209},{\"end\":127221,\"start\":127218},{\"end\":127238,\"start\":127234},{\"end\":127248,\"start\":127244},{\"end\":127261,\"start\":127258},{\"end\":127604,\"start\":127600},{\"end\":127613,\"start\":127608},{\"end\":127629,\"start\":127624},{\"end\":127638,\"start\":127631},{\"end\":128096,\"start\":128094},{\"end\":128110,\"start\":128105},{\"end\":128124,\"start\":128119},{\"end\":128137,\"start\":128132},{\"end\":128149,\"start\":128144},{\"end\":128622,\"start\":128617},{\"end\":128639,\"start\":128630},{\"end\":128654,\"start\":128648},{\"end\":128671,\"start\":128663},{\"end\":129136,\"start\":129131},{\"end\":129153,\"start\":129144},{\"end\":129164,\"start\":129160},{\"end\":129174,\"start\":129169},{\"end\":129187,\"start\":129183},{\"end\":129200,\"start\":129195},{\"end\":129213,\"start\":129207},{\"end\":129230,\"start\":129222},{\"end\":129665,\"start\":129662},{\"end\":129680,\"start\":129675},{\"end\":129696,\"start\":129688},{\"end\":129708,\"start\":129704},{\"end\":129723,\"start\":129717},{\"end\":129737,\"start\":129730},{\"end\":129751,\"start\":129745},{\"end\":129771,\"start\":129764},{\"end\":130100,\"start\":130093},{\"end\":130110,\"start\":130107},{\"end\":130124,\"start\":130119},{\"end\":130140,\"start\":130133},{\"end\":130152,\"start\":130148},{\"end\":130168,\"start\":130161},{\"end\":130184,\"start\":130180},{\"end\":130203,\"start\":130193},{\"end\":130214,\"start\":130212},{\"end\":130230,\"start\":130224},{\"end\":130251,\"start\":130242},{\"end\":130263,\"start\":130256},{\"end\":130710,\"start\":130707},{\"end\":130723,\"start\":130719},{\"end\":130733,\"start\":130731},{\"end\":130748,\"start\":130744},{\"end\":130757,\"start\":130754},{\"end\":130766,\"start\":130764},{\"end\":130780,\"start\":130777},{\"end\":131142,\"start\":131138},{\"end\":131159,\"start\":131150},{\"end\":131173,\"start\":131168},{\"end\":131191,\"start\":131184},{\"end\":131210,\"start\":131202}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218971783},\"end\":106535,\"start\":106017},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":246426909},\"end\":107099,\"start\":106537},{\"attributes\":{\"doi\":\"abs/2303.08774\",\"id\":\"b2\"},\"end\":107223,\"start\":107101},{\"attributes\":{\"id\":\"b3\"},\"end\":107531,\"start\":107225},{\"attributes\":{\"doi\":\"arXiv:2206.06336\",\"id\":\"b4\"},\"end\":107891,\"start\":107533},{\"attributes\":{\"doi\":\"arXiv:2302.14045\",\"id\":\"b5\"},\"end\":108382,\"start\":107893},{\"attributes\":{\"id\":\"b6\"},\"end\":108586,\"start\":108384},{\"attributes\":{\"doi\":\"arXiv:2303.03378\",\"id\":\"b7\"},\"end\":109054,\"start\":108588},{\"attributes\":{\"doi\":\"arXiv:2212.10560\",\"id\":\"b8\"},\"end\":109500,\"start\":109056},{\"attributes\":{\"id\":\"b9\"},\"end\":109915,\"start\":109502},{\"attributes\":{\"doi\":\"arXiv:2303.04671\",\"id\":\"b10\"},\"end\":110299,\"start\":109917},{\"attributes\":{\"doi\":\"arXiv:2303.11381\",\"id\":\"b11\"},\"end\":110769,\"start\":110301},{\"attributes\":{\"doi\":\"arXiv:2303.08128\",\"id\":\"b12\"},\"end\":111055,\"start\":110771},{\"attributes\":{\"doi\":\"arXiv:2303.17580\",\"id\":\"b13\"},\"end\":111437,\"start\":111057},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":248476411},\"end\":111995,\"start\":111439},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b15\"},\"end\":112381,\"start\":111997},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b16\"},\"end\":112883,\"start\":112383},{\"attributes\":{\"doi\":\"arXiv:2304.10592\",\"id\":\"b17\"},\"end\":113274,\"start\":112885},{\"attributes\":{\"id\":\"b18\"},\"end\":113890,\"start\":113276},{\"attributes\":{\"doi\":\"arXiv:2303.16199\",\"id\":\"b19\"},\"end\":114340,\"start\":113892},{\"attributes\":{\"doi\":\"arXiv:2305.04790\",\"id\":\"b20\"},\"end\":114796,\"start\":114342},{\"attributes\":{\"doi\":\"arXiv:2305.03726\",\"id\":\"b21\"},\"end\":115150,\"start\":114798},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b22\"},\"end\":115558,\"start\":115152},{\"attributes\":{\"id\":\"b23\"},\"end\":116304,\"start\":115560},{\"attributes\":{\"id\":\"b24\"},\"end\":116720,\"start\":116306},{\"attributes\":{\"doi\":\"arXiv:2212.10773\",\"id\":\"b25\"},\"end\":117030,\"start\":116722},{\"attributes\":{\"id\":\"b26\"},\"end\":117529,\"start\":117032},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52051958},\"end\":118280,\"start\":117531},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b28\"},\"end\":118673,\"start\":118282},{\"attributes\":{\"id\":\"b29\"},\"end\":119175,\"start\":118675},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b30\",\"matched_paper_id\":231591445},\"end\":119758,\"start\":119177},{\"attributes\":{\"doi\":\"arXiv:2212.04098\",\"id\":\"b31\"},\"end\":120082,\"start\":119760},{\"attributes\":{\"doi\":\"arXiv:1706.02413\",\"id\":\"b32\"},\"end\":120414,\"start\":120084},{\"attributes\":{\"id\":\"b33\"},\"end\":120686,\"start\":120416},{\"attributes\":{\"id\":\"b34\"},\"end\":121261,\"start\":120688},{\"attributes\":{\"id\":\"b35\"},\"end\":121620,\"start\":121263},{\"attributes\":{\"id\":\"b36\"},\"end\":122097,\"start\":121622},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":252383606},\"end\":122696,\"start\":122099},{\"attributes\":{\"doi\":\"abs/1603.07396\",\"id\":\"b38\"},\"end\":123014,\"start\":122698},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3104920},\"end\":123519,\"start\":123016},{\"attributes\":{\"id\":\"b40\"},\"end\":123827,\"start\":123521},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":993769},\"end\":124360,\"start\":123829},{\"attributes\":{\"id\":\"b42\"},\"end\":124567,\"start\":124362},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":233296193},\"end\":125009,\"start\":124569},{\"attributes\":{\"id\":\"b44\"},\"end\":125290,\"start\":125011},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14136313},\"end\":125592,\"start\":125292},{\"attributes\":{\"id\":\"b46\"},\"end\":125806,\"start\":125594},{\"attributes\":{\"id\":\"b47\"},\"end\":126071,\"start\":125808},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7318714},\"end\":126371,\"start\":126073},{\"attributes\":{\"id\":\"b49\"},\"end\":126583,\"start\":126373},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7684883},\"end\":127085,\"start\":126585},{\"attributes\":{\"doi\":\"arXiv:2304.06906\",\"id\":\"b51\"},\"end\":127513,\"start\":127087},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":209414687},\"end\":128010,\"start\":127515},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":253734729},\"end\":128545,\"start\":128012},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":245334889},\"end\":129085,\"start\":128547},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":85553602},\"end\":129649,\"start\":129087},{\"attributes\":{\"id\":\"b56\"},\"end\":129994,\"start\":129651},{\"attributes\":{\"doi\":\"abs/1602.07332\",\"id\":\"b57\"},\"end\":130590,\"start\":129996},{\"attributes\":{\"doi\":\"arXiv:2112.11691\",\"id\":\"b58\"},\"end\":131055,\"start\":130592},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":201070582},\"end\":131577,\"start\":131057},{\"attributes\":{\"id\":\"b60\"},\"end\":131754,\"start\":131579},{\"attributes\":{\"id\":\"b61\"},\"end\":131965,\"start\":131756},{\"attributes\":{\"id\":\"b62\"},\"end\":132128,\"start\":131967},{\"attributes\":{\"id\":\"b63\"},\"end\":132279,\"start\":132130},{\"attributes\":{\"id\":\"b64\"},\"end\":132476,\"start\":132281},{\"attributes\":{\"id\":\"b65\"},\"end\":132917,\"start\":132478},{\"attributes\":{\"id\":\"b66\"},\"end\":133136,\"start\":132919},{\"attributes\":{\"id\":\"b67\"},\"end\":133527,\"start\":133138},{\"attributes\":{\"id\":\"b68\"},\"end\":133730,\"start\":133529},{\"attributes\":{\"id\":\"b69\"},\"end\":134125,\"start\":133732},{\"attributes\":{\"id\":\"b70\"},\"end\":134334,\"start\":134127},{\"attributes\":{\"id\":\"b71\"},\"end\":134515,\"start\":134336},{\"attributes\":{\"id\":\"b72\"},\"end\":134650,\"start\":134517},{\"attributes\":{\"id\":\"b73\"},\"end\":134743,\"start\":134652},{\"attributes\":{\"id\":\"b74\"},\"end\":134870,\"start\":134745},{\"attributes\":{\"id\":\"b75\"},\"end\":135023,\"start\":134872},{\"attributes\":{\"id\":\"b76\"},\"end\":135136,\"start\":135025},{\"attributes\":{\"id\":\"b77\"},\"end\":135279,\"start\":135138},{\"attributes\":{\"id\":\"b78\"},\"end\":135382,\"start\":135281},{\"attributes\":{\"id\":\"b79\"},\"end\":135505,\"start\":135384},{\"attributes\":{\"id\":\"b80\"},\"end\":135614,\"start\":135507},{\"attributes\":{\"id\":\"b81\"},\"end\":135767,\"start\":135616},{\"attributes\":{\"id\":\"b82\"},\"end\":135908,\"start\":135769},{\"attributes\":{\"id\":\"b83\"},\"end\":136098,\"start\":135910},{\"attributes\":{\"id\":\"b84\"},\"end\":136294,\"start\":136100},{\"attributes\":{\"id\":\"b85\"},\"end\":136673,\"start\":136296},{\"attributes\":{\"id\":\"b86\"},\"end\":136827,\"start\":136675},{\"attributes\":{\"id\":\"b87\"},\"end\":136995,\"start\":136829},{\"attributes\":{\"id\":\"b88\"},\"end\":137652,\"start\":136997},{\"attributes\":{\"id\":\"b89\"},\"end\":139445,\"start\":137654},{\"attributes\":{\"id\":\"b90\"},\"end\":139674,\"start\":139447},{\"attributes\":{\"id\":\"b91\"},\"end\":139933,\"start\":139676},{\"attributes\":{\"id\":\"b92\"},\"end\":140158,\"start\":139935},{\"attributes\":{\"id\":\"b93\"},\"end\":140361,\"start\":140160},{\"attributes\":{\"id\":\"b94\"},\"end\":140610,\"start\":140363},{\"attributes\":{\"id\":\"b95\"},\"end\":140839,\"start\":140612},{\"attributes\":{\"id\":\"b96\"},\"end\":141288,\"start\":140841},{\"attributes\":{\"id\":\"b97\"},\"end\":141483,\"start\":141290},{\"attributes\":{\"id\":\"b98\"},\"end\":141900,\"start\":141485},{\"attributes\":{\"id\":\"b99\"},\"end\":143122,\"start\":141902},{\"attributes\":{\"id\":\"b100\"},\"end\":143677,\"start\":143124},{\"attributes\":{\"id\":\"b101\"},\"end\":144119,\"start\":143679},{\"attributes\":{\"id\":\"b102\"},\"end\":144426,\"start\":144121},{\"attributes\":{\"id\":\"b103\"},\"end\":144633,\"start\":144428},{\"attributes\":{\"id\":\"b104\"},\"end\":144852,\"start\":144635},{\"attributes\":{\"id\":\"b105\"},\"end\":145063,\"start\":144854},{\"attributes\":{\"id\":\"b106\"},\"end\":145314,\"start\":145065},{\"attributes\":{\"id\":\"b107\"},\"end\":145781,\"start\":145316},{\"attributes\":{\"id\":\"b108\"},\"end\":146580,\"start\":145783},{\"attributes\":{\"id\":\"b109\"},\"end\":146793,\"start\":146582},{\"attributes\":{\"id\":\"b110\"},\"end\":147008,\"start\":146795},{\"attributes\":{\"id\":\"b111\"},\"end\":147223,\"start\":147010},{\"attributes\":{\"id\":\"b112\"},\"end\":147378,\"start\":147225},{\"attributes\":{\"id\":\"b113\"},\"end\":147605,\"start\":147380},{\"attributes\":{\"id\":\"b114\"},\"end\":147776,\"start\":147607},{\"attributes\":{\"id\":\"b115\"},\"end\":147967,\"start\":147778},{\"attributes\":{\"id\":\"b116\"},\"end\":148148,\"start\":147969},{\"attributes\":{\"id\":\"b117\"},\"end\":148353,\"start\":148150},{\"attributes\":{\"id\":\"b118\"},\"end\":148776,\"start\":148355},{\"attributes\":{\"id\":\"b119\"},\"end\":148957,\"start\":148778},{\"attributes\":{\"id\":\"b120\"},\"end\":149162,\"start\":148959},{\"attributes\":{\"id\":\"b121\"},\"end\":149367,\"start\":149164},{\"attributes\":{\"id\":\"b122\"},\"end\":149536,\"start\":149369},{\"attributes\":{\"id\":\"b123\"},\"end\":149727,\"start\":149538},{\"attributes\":{\"id\":\"b124\"},\"end\":149928,\"start\":149729},{\"attributes\":{\"id\":\"b125\"},\"end\":150127,\"start\":149930},{\"attributes\":{\"id\":\"b126\"},\"end\":150340,\"start\":150129},{\"attributes\":{\"id\":\"b127\"},\"end\":150559,\"start\":150342},{\"attributes\":{\"id\":\"b128\"},\"end\":151174,\"start\":150561},{\"attributes\":{\"id\":\"b129\"},\"end\":151427,\"start\":151176},{\"attributes\":{\"id\":\"b130\"},\"end\":151636,\"start\":151429},{\"attributes\":{\"id\":\"b131\"},\"end\":151873,\"start\":151638},{\"attributes\":{\"id\":\"b132\"},\"end\":152178,\"start\":151875},{\"attributes\":{\"id\":\"b133\"},\"end\":152401,\"start\":152180},{\"attributes\":{\"id\":\"b134\"},\"end\":152610,\"start\":152403},{\"attributes\":{\"id\":\"b135\"},\"end\":152817,\"start\":152612},{\"attributes\":{\"id\":\"b136\"},\"end\":153012,\"start\":152819},{\"attributes\":{\"id\":\"b137\"},\"end\":153333,\"start\":153014},{\"attributes\":{\"id\":\"b138\"},\"end\":153554,\"start\":153335},{\"attributes\":{\"id\":\"b139\"},\"end\":153959,\"start\":153556},{\"attributes\":{\"id\":\"b140\"},\"end\":154378,\"start\":153961},{\"attributes\":{\"id\":\"b141\"},\"end\":154823,\"start\":154380}]", "bib_title": "[{\"end\":106054,\"start\":106017},{\"end\":106604,\"start\":106537},{\"end\":111494,\"start\":111439},{\"end\":117640,\"start\":117531},{\"end\":119246,\"start\":119177},{\"end\":122187,\"start\":122099},{\"end\":123132,\"start\":123016},{\"end\":123899,\"start\":123829},{\"end\":124597,\"start\":124569},{\"end\":125325,\"start\":125292},{\"end\":126145,\"start\":126073},{\"end\":126646,\"start\":126585},{\"end\":127586,\"start\":127515},{\"end\":128085,\"start\":128012},{\"end\":128608,\"start\":128547},{\"end\":129119,\"start\":129087},{\"end\":131128,\"start\":131057},{\"end\":136473,\"start\":136296}]", "bib_author": "[{\"end\":106067,\"start\":106056},{\"end\":106082,\"start\":106067},{\"end\":106094,\"start\":106082},{\"end\":106111,\"start\":106094},{\"end\":106127,\"start\":106111},{\"end\":106146,\"start\":106127},{\"end\":106166,\"start\":106146},{\"end\":106180,\"start\":106166},{\"end\":106195,\"start\":106180},{\"end\":106210,\"start\":106195},{\"end\":106619,\"start\":106606},{\"end\":106631,\"start\":106619},{\"end\":106641,\"start\":106631},{\"end\":106656,\"start\":106641},{\"end\":106676,\"start\":106656},{\"end\":106692,\"start\":106676},{\"end\":106705,\"start\":106692},{\"end\":106723,\"start\":106705},{\"end\":106739,\"start\":106723},{\"end\":106749,\"start\":106739},{\"end\":107330,\"start\":107322},{\"end\":107337,\"start\":107330},{\"end\":107350,\"start\":107337},{\"end\":107356,\"start\":107350},{\"end\":107591,\"start\":107581},{\"end\":107603,\"start\":107591},{\"end\":107612,\"start\":107603},{\"end\":107627,\"start\":107612},{\"end\":107638,\"start\":107627},{\"end\":107651,\"start\":107638},{\"end\":107663,\"start\":107651},{\"end\":107673,\"start\":107663},{\"end\":107980,\"start\":107965},{\"end\":107989,\"start\":107980},{\"end\":108002,\"start\":107989},{\"end\":108012,\"start\":108002},{\"end\":108029,\"start\":108012},{\"end\":108041,\"start\":108029},{\"end\":108054,\"start\":108041},{\"end\":108063,\"start\":108054},{\"end\":108090,\"start\":108063},{\"end\":108095,\"start\":108090},{\"end\":108424,\"start\":108411},{\"end\":108437,\"start\":108424},{\"end\":108450,\"start\":108437},{\"end\":108464,\"start\":108450},{\"end\":108649,\"start\":108635},{\"end\":108658,\"start\":108649},{\"end\":108669,\"start\":108658},{\"end\":108684,\"start\":108669},{\"end\":108701,\"start\":108684},{\"end\":108718,\"start\":108701},{\"end\":108733,\"start\":108718},{\"end\":108749,\"start\":108733},{\"end\":108763,\"start\":108749},{\"end\":108777,\"start\":108763},{\"end\":108781,\"start\":108777},{\"end\":109143,\"start\":109129},{\"end\":109158,\"start\":109143},{\"end\":109174,\"start\":109158},{\"end\":109185,\"start\":109174},{\"end\":109193,\"start\":109185},{\"end\":109207,\"start\":109193},{\"end\":109226,\"start\":109207},{\"end\":109238,\"start\":109226},{\"end\":109600,\"start\":109585},{\"end\":109614,\"start\":109600},{\"end\":109627,\"start\":109614},{\"end\":109637,\"start\":109627},{\"end\":109650,\"start\":109637},{\"end\":109660,\"start\":109650},{\"end\":109670,\"start\":109660},{\"end\":109679,\"start\":109670},{\"end\":109690,\"start\":109679},{\"end\":109701,\"start\":109690},{\"end\":110005,\"start\":109993},{\"end\":110020,\"start\":110005},{\"end\":110032,\"start\":110020},{\"end\":110047,\"start\":110032},{\"end\":110061,\"start\":110047},{\"end\":110071,\"start\":110061},{\"end\":110317,\"start\":110301},{\"end\":110328,\"start\":110317},{\"end\":110343,\"start\":110328},{\"end\":110354,\"start\":110343},{\"end\":110371,\"start\":110354},{\"end\":110385,\"start\":110371},{\"end\":110398,\"start\":110385},{\"end\":110406,\"start\":110398},{\"end\":110420,\"start\":110406},{\"end\":110433,\"start\":110420},{\"end\":110784,\"start\":110771},{\"end\":110798,\"start\":110784},{\"end\":110813,\"start\":110798},{\"end\":110823,\"start\":110813},{\"end\":111147,\"start\":111131},{\"end\":111160,\"start\":111147},{\"end\":111168,\"start\":111160},{\"end\":111182,\"start\":111168},{\"end\":111194,\"start\":111182},{\"end\":111210,\"start\":111194},{\"end\":111524,\"start\":111496},{\"end\":111541,\"start\":111524},{\"end\":111554,\"start\":111541},{\"end\":111566,\"start\":111554},{\"end\":111577,\"start\":111566},{\"end\":111591,\"start\":111577},{\"end\":111604,\"start\":111591},{\"end\":111622,\"start\":111604},{\"end\":111640,\"start\":111622},{\"end\":111650,\"start\":111640},{\"end\":112111,\"start\":112100},{\"end\":112122,\"start\":112111},{\"end\":112139,\"start\":112122},{\"end\":112151,\"start\":112139},{\"end\":112397,\"start\":112383},{\"end\":112413,\"start\":112397},{\"end\":112430,\"start\":112413},{\"end\":112447,\"start\":112430},{\"end\":112467,\"start\":112447},{\"end\":112485,\"start\":112467},{\"end\":112509,\"start\":112485},{\"end\":112521,\"start\":112509},{\"end\":112536,\"start\":112521},{\"end\":112543,\"start\":112536},{\"end\":112984,\"start\":112973},{\"end\":112994,\"start\":112984},{\"end\":113009,\"start\":112994},{\"end\":113019,\"start\":113009},{\"end\":113038,\"start\":113019},{\"end\":113290,\"start\":113278},{\"end\":113302,\"start\":113290},{\"end\":113313,\"start\":113302},{\"end\":113323,\"start\":113313},{\"end\":113333,\"start\":113323},{\"end\":113346,\"start\":113333},{\"end\":113360,\"start\":113346},{\"end\":113370,\"start\":113360},{\"end\":113385,\"start\":113370},{\"end\":113395,\"start\":113385},{\"end\":113409,\"start\":113395},{\"end\":113423,\"start\":113409},{\"end\":113436,\"start\":113423},{\"end\":113449,\"start\":113436},{\"end\":113463,\"start\":113449},{\"end\":113472,\"start\":113463},{\"end\":113482,\"start\":113472},{\"end\":113493,\"start\":113482},{\"end\":113988,\"start\":113974},{\"end\":114001,\"start\":113988},{\"end\":114013,\"start\":114001},{\"end\":114026,\"start\":114013},{\"end\":114038,\"start\":114026},{\"end\":114046,\"start\":114038},{\"end\":114060,\"start\":114046},{\"end\":114070,\"start\":114060},{\"end\":114079,\"start\":114070},{\"end\":114422,\"start\":114412},{\"end\":114435,\"start\":114422},{\"end\":114450,\"start\":114435},{\"end\":114463,\"start\":114450},{\"end\":114475,\"start\":114463},{\"end\":114486,\"start\":114475},{\"end\":114498,\"start\":114486},{\"end\":114512,\"start\":114498},{\"end\":114522,\"start\":114512},{\"end\":114532,\"start\":114522},{\"end\":114868,\"start\":114861},{\"end\":114883,\"start\":114868},{\"end\":114897,\"start\":114883},{\"end\":114911,\"start\":114897},{\"end\":114926,\"start\":114911},{\"end\":114937,\"start\":114926},{\"end\":115213,\"start\":115202},{\"end\":115228,\"start\":115213},{\"end\":115239,\"start\":115228},{\"end\":115252,\"start\":115239},{\"end\":115267,\"start\":115252},{\"end\":115277,\"start\":115267},{\"end\":115289,\"start\":115277},{\"end\":115293,\"start\":115289},{\"end\":115303,\"start\":115293},{\"end\":115315,\"start\":115303},{\"end\":115319,\"start\":115315},{\"end\":115671,\"start\":115654},{\"end\":115688,\"start\":115671},{\"end\":115708,\"start\":115688},{\"end\":115724,\"start\":115708},{\"end\":115738,\"start\":115724},{\"end\":115747,\"start\":115738},{\"end\":115761,\"start\":115747},{\"end\":115774,\"start\":115761},{\"end\":115784,\"start\":115774},{\"end\":115803,\"start\":115784},{\"end\":115812,\"start\":115803},{\"end\":115827,\"start\":115812},{\"end\":115841,\"start\":115827},{\"end\":115855,\"start\":115841},{\"end\":115873,\"start\":115855},{\"end\":115885,\"start\":115873},{\"end\":115903,\"start\":115885},{\"end\":115920,\"start\":115903},{\"end\":115930,\"start\":115920},{\"end\":116374,\"start\":116361},{\"end\":116392,\"start\":116374},{\"end\":116406,\"start\":116392},{\"end\":116419,\"start\":116406},{\"end\":116431,\"start\":116419},{\"end\":116448,\"start\":116431},{\"end\":116461,\"start\":116448},{\"end\":116484,\"start\":116461},{\"end\":116814,\"start\":116802},{\"end\":116825,\"start\":116814},{\"end\":116837,\"start\":116825},{\"end\":117049,\"start\":117034},{\"end\":117060,\"start\":117049},{\"end\":117076,\"start\":117060},{\"end\":117089,\"start\":117076},{\"end\":117103,\"start\":117089},{\"end\":117116,\"start\":117103},{\"end\":117133,\"start\":117116},{\"end\":117149,\"start\":117133},{\"end\":117162,\"start\":117149},{\"end\":117176,\"start\":117162},{\"end\":117193,\"start\":117176},{\"end\":117207,\"start\":117193},{\"end\":117224,\"start\":117207},{\"end\":117243,\"start\":117224},{\"end\":117259,\"start\":117243},{\"end\":117273,\"start\":117259},{\"end\":117653,\"start\":117642},{\"end\":117670,\"start\":117653},{\"end\":118292,\"start\":118282},{\"end\":118303,\"start\":118292},{\"end\":118317,\"start\":118303},{\"end\":118332,\"start\":118317},{\"end\":118351,\"start\":118332},{\"end\":118361,\"start\":118351},{\"end\":118370,\"start\":118361},{\"end\":118383,\"start\":118370},{\"end\":118389,\"start\":118383},{\"end\":118395,\"start\":118389},{\"end\":118766,\"start\":118750},{\"end\":118778,\"start\":118766},{\"end\":118786,\"start\":118778},{\"end\":118798,\"start\":118786},{\"end\":118811,\"start\":118798},{\"end\":118822,\"start\":118811},{\"end\":118837,\"start\":118822},{\"end\":118852,\"start\":118837},{\"end\":118868,\"start\":118852},{\"end\":118887,\"start\":118868},{\"end\":118899,\"start\":118887},{\"end\":118912,\"start\":118899},{\"end\":119262,\"start\":119248},{\"end\":119277,\"start\":119262},{\"end\":119292,\"start\":119277},{\"end\":119307,\"start\":119292},{\"end\":119320,\"start\":119307},{\"end\":119338,\"start\":119320},{\"end\":119353,\"start\":119338},{\"end\":119368,\"start\":119353},{\"end\":119384,\"start\":119368},{\"end\":119396,\"start\":119384},{\"end\":119829,\"start\":119813},{\"end\":119839,\"start\":119829},{\"end\":119850,\"start\":119839},{\"end\":119859,\"start\":119850},{\"end\":119870,\"start\":119859},{\"end\":119884,\"start\":119870},{\"end\":120181,\"start\":120164},{\"end\":120189,\"start\":120181},{\"end\":120204,\"start\":120189},{\"end\":120212,\"start\":120204},{\"end\":120527,\"start\":120513},{\"end\":120537,\"start\":120527},{\"end\":120784,\"start\":120764},{\"end\":120797,\"start\":120784},{\"end\":120819,\"start\":120797},{\"end\":120837,\"start\":120819},{\"end\":120851,\"start\":120837},{\"end\":120871,\"start\":120851},{\"end\":120889,\"start\":120871},{\"end\":120908,\"start\":120889},{\"end\":120923,\"start\":120908},{\"end\":120938,\"start\":120923},{\"end\":120955,\"start\":120938},{\"end\":120969,\"start\":120955},{\"end\":121344,\"start\":121330},{\"end\":121356,\"start\":121344},{\"end\":121372,\"start\":121356},{\"end\":121380,\"start\":121372},{\"end\":121393,\"start\":121380},{\"end\":121725,\"start\":121712},{\"end\":121737,\"start\":121725},{\"end\":121747,\"start\":121737},{\"end\":121763,\"start\":121747},{\"end\":121773,\"start\":121763},{\"end\":121785,\"start\":121773},{\"end\":121797,\"start\":121785},{\"end\":121806,\"start\":121797},{\"end\":121816,\"start\":121806},{\"end\":121830,\"start\":121816},{\"end\":121845,\"start\":121830},{\"end\":121854,\"start\":121845},{\"end\":122197,\"start\":122189},{\"end\":122213,\"start\":122197},{\"end\":122223,\"start\":122213},{\"end\":122234,\"start\":122223},{\"end\":122249,\"start\":122234},{\"end\":122264,\"start\":122249},{\"end\":122280,\"start\":122264},{\"end\":122293,\"start\":122280},{\"end\":122308,\"start\":122293},{\"end\":122753,\"start\":122733},{\"end\":122767,\"start\":122753},{\"end\":122779,\"start\":122767},{\"end\":122793,\"start\":122779},{\"end\":122814,\"start\":122793},{\"end\":122827,\"start\":122814},{\"end\":123147,\"start\":123134},{\"end\":123158,\"start\":123147},{\"end\":123172,\"start\":123158},{\"end\":123191,\"start\":123172},{\"end\":123601,\"start\":123588},{\"end\":123616,\"start\":123601},{\"end\":123627,\"start\":123616},{\"end\":123639,\"start\":123627},{\"end\":123654,\"start\":123639},{\"end\":123668,\"start\":123654},{\"end\":123910,\"start\":123901},{\"end\":123924,\"start\":123910},{\"end\":124461,\"start\":124444},{\"end\":124621,\"start\":124599},{\"end\":124633,\"start\":124621},{\"end\":124646,\"start\":124633},{\"end\":124652,\"start\":124646},{\"end\":125098,\"start\":125083},{\"end\":125112,\"start\":125098},{\"end\":125130,\"start\":125112},{\"end\":125145,\"start\":125130},{\"end\":125337,\"start\":125327},{\"end\":125352,\"start\":125337},{\"end\":125368,\"start\":125352},{\"end\":125680,\"start\":125663},{\"end\":125695,\"start\":125680},{\"end\":125882,\"start\":125871},{\"end\":125892,\"start\":125882},{\"end\":125907,\"start\":125892},{\"end\":125920,\"start\":125907},{\"end\":126160,\"start\":126147},{\"end\":126177,\"start\":126160},{\"end\":126456,\"start\":126441},{\"end\":126473,\"start\":126456},{\"end\":126660,\"start\":126648},{\"end\":126675,\"start\":126660},{\"end\":126690,\"start\":126675},{\"end\":126705,\"start\":126690},{\"end\":126724,\"start\":126705},{\"end\":126742,\"start\":126724},{\"end\":127176,\"start\":127164},{\"end\":127189,\"start\":127176},{\"end\":127204,\"start\":127189},{\"end\":127214,\"start\":127204},{\"end\":127223,\"start\":127214},{\"end\":127240,\"start\":127223},{\"end\":127250,\"start\":127240},{\"end\":127263,\"start\":127250},{\"end\":127606,\"start\":127588},{\"end\":127615,\"start\":127606},{\"end\":127631,\"start\":127615},{\"end\":127640,\"start\":127631},{\"end\":128098,\"start\":128087},{\"end\":128112,\"start\":128098},{\"end\":128126,\"start\":128112},{\"end\":128139,\"start\":128126},{\"end\":128151,\"start\":128139},{\"end\":128624,\"start\":128610},{\"end\":128641,\"start\":128624},{\"end\":128656,\"start\":128641},{\"end\":128673,\"start\":128656},{\"end\":129138,\"start\":129121},{\"end\":129155,\"start\":129138},{\"end\":129166,\"start\":129155},{\"end\":129176,\"start\":129166},{\"end\":129189,\"start\":129176},{\"end\":129202,\"start\":129189},{\"end\":129215,\"start\":129202},{\"end\":129232,\"start\":129215},{\"end\":129667,\"start\":129653},{\"end\":129682,\"start\":129667},{\"end\":129698,\"start\":129682},{\"end\":129710,\"start\":129698},{\"end\":129725,\"start\":129710},{\"end\":129739,\"start\":129725},{\"end\":129753,\"start\":129739},{\"end\":129773,\"start\":129753},{\"end\":130102,\"start\":130086},{\"end\":130112,\"start\":130102},{\"end\":130126,\"start\":130112},{\"end\":130142,\"start\":130126},{\"end\":130154,\"start\":130142},{\"end\":130170,\"start\":130154},{\"end\":130186,\"start\":130170},{\"end\":130205,\"start\":130186},{\"end\":130216,\"start\":130205},{\"end\":130232,\"start\":130216},{\"end\":130253,\"start\":130232},{\"end\":130265,\"start\":130253},{\"end\":130712,\"start\":130704},{\"end\":130725,\"start\":130712},{\"end\":130735,\"start\":130725},{\"end\":130750,\"start\":130735},{\"end\":130759,\"start\":130750},{\"end\":130768,\"start\":130759},{\"end\":130782,\"start\":130768},{\"end\":131144,\"start\":131130},{\"end\":131161,\"start\":131144},{\"end\":131175,\"start\":131161},{\"end\":131193,\"start\":131175},{\"end\":131212,\"start\":131193}]", "bib_venue": "[{\"end\":117892,\"start\":117781},{\"end\":124121,\"start\":124031},{\"end\":124815,\"start\":124742},{\"end\":126842,\"start\":126796},{\"end\":127704,\"start\":127693},{\"end\":128306,\"start\":128237},{\"end\":128848,\"start\":128769},{\"end\":129395,\"start\":129322},{\"end\":131336,\"start\":131281},{\"end\":106259,\"start\":106210},{\"end\":106798,\"start\":106749},{\"end\":107154,\"start\":107117},{\"end\":107320,\"start\":107225},{\"end\":107579,\"start\":107533},{\"end\":107963,\"start\":107893},{\"end\":108409,\"start\":108384},{\"end\":108633,\"start\":108588},{\"end\":109127,\"start\":109056},{\"end\":109583,\"start\":109502},{\"end\":109991,\"start\":109917},{\"end\":110512,\"start\":110449},{\"end\":110890,\"start\":110839},{\"end\":111129,\"start\":111057},{\"end\":111699,\"start\":111650},{\"end\":112098,\"start\":111997},{\"end\":112604,\"start\":112559},{\"end\":112971,\"start\":112885},{\"end\":113972,\"start\":113892},{\"end\":114410,\"start\":114342},{\"end\":114859,\"start\":114798},{\"end\":115200,\"start\":115152},{\"end\":115652,\"start\":115560},{\"end\":116359,\"start\":116306},{\"end\":116800,\"start\":116722},{\"end\":117779,\"start\":117670},{\"end\":118455,\"start\":118411},{\"end\":118748,\"start\":118675},{\"end\":119444,\"start\":119400},{\"end\":119811,\"start\":119760},{\"end\":120162,\"start\":120084},{\"end\":120511,\"start\":120416},{\"end\":120762,\"start\":120688},{\"end\":121328,\"start\":121263},{\"end\":121710,\"start\":121622},{\"end\":122387,\"start\":122308},{\"end\":122731,\"start\":122698},{\"end\":123252,\"start\":123191},{\"end\":123586,\"start\":123521},{\"end\":124029,\"start\":123924},{\"end\":124442,\"start\":124362},{\"end\":124740,\"start\":124652},{\"end\":125081,\"start\":125011},{\"end\":125416,\"start\":125368},{\"end\":125661,\"start\":125594},{\"end\":125869,\"start\":125808},{\"end\":126210,\"start\":126177},{\"end\":126439,\"start\":126373},{\"end\":126794,\"start\":126742},{\"end\":127162,\"start\":127087},{\"end\":127691,\"start\":127640},{\"end\":128235,\"start\":128151},{\"end\":128767,\"start\":128673},{\"end\":129320,\"start\":129232},{\"end\":130084,\"start\":129996},{\"end\":130702,\"start\":130592},{\"end\":131279,\"start\":131212},{\"end\":131665,\"start\":131579},{\"end\":131859,\"start\":131756},{\"end\":132046,\"start\":131967},{\"end\":132203,\"start\":132130},{\"end\":132377,\"start\":132281},{\"end\":132696,\"start\":132478},{\"end\":133026,\"start\":132919},{\"end\":133331,\"start\":133138},{\"end\":133628,\"start\":133529},{\"end\":133927,\"start\":133732},{\"end\":134229,\"start\":134127},{\"end\":134424,\"start\":134336},{\"end\":134582,\"start\":134517},{\"end\":134696,\"start\":134652},{\"end\":134806,\"start\":134745},{\"end\":134946,\"start\":134872},{\"end\":135079,\"start\":135025},{\"end\":135207,\"start\":135138},{\"end\":135330,\"start\":135281},{\"end\":135443,\"start\":135384},{\"end\":135559,\"start\":135507},{\"end\":135690,\"start\":135616},{\"end\":135837,\"start\":135769},{\"end\":136003,\"start\":135910},{\"end\":136196,\"start\":136100},{\"end\":136481,\"start\":136475},{\"end\":136750,\"start\":136675},{\"end\":136911,\"start\":136829},{\"end\":137280,\"start\":136997},{\"end\":138309,\"start\":137654},{\"end\":139559,\"start\":139447},{\"end\":139803,\"start\":139676},{\"end\":140045,\"start\":139935},{\"end\":140259,\"start\":140160},{\"end\":140485,\"start\":140363},{\"end\":140724,\"start\":140612},{\"end\":141063,\"start\":140841},{\"end\":141385,\"start\":141290},{\"end\":141691,\"start\":141485},{\"end\":142355,\"start\":141902},{\"end\":143332,\"start\":143124},{\"end\":143898,\"start\":143679},{\"end\":144272,\"start\":144121},{\"end\":144529,\"start\":144428},{\"end\":144742,\"start\":144635},{\"end\":144923,\"start\":144854},{\"end\":145188,\"start\":145065},{\"end\":145547,\"start\":145316},{\"end\":146018,\"start\":145783},{\"end\":146686,\"start\":146582},{\"end\":146900,\"start\":146795},{\"end\":147115,\"start\":147010},{\"end\":147300,\"start\":147225},{\"end\":147491,\"start\":147380},{\"end\":147690,\"start\":147607},{\"end\":147871,\"start\":147778},{\"end\":148057,\"start\":147969},{\"end\":148250,\"start\":148150},{\"end\":148533,\"start\":148355},{\"end\":148866,\"start\":148778},{\"end\":149059,\"start\":148959},{\"end\":149264,\"start\":149164},{\"end\":149451,\"start\":149369},{\"end\":149631,\"start\":149538},{\"end\":149827,\"start\":149729},{\"end\":150027,\"start\":149930},{\"end\":150233,\"start\":150129},{\"end\":150449,\"start\":150342},{\"end\":150679,\"start\":150561},{\"end\":151300,\"start\":151176},{\"end\":151472,\"start\":151429},{\"end\":151677,\"start\":151638},{\"end\":151973,\"start\":151875},{\"end\":152220,\"start\":152180},{\"end\":152505,\"start\":152403},{\"end\":152647,\"start\":152612},{\"end\":152860,\"start\":152819},{\"end\":153121,\"start\":153014},{\"end\":153383,\"start\":153335},{\"end\":153701,\"start\":153556},{\"end\":154154,\"start\":153961},{\"end\":154600,\"start\":154380}]"}}}, "year": 2023, "month": 12, "day": 17}