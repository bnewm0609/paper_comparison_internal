{"id": 4852047, "updated": "2023-11-08 01:55:18.132", "metadata": {"title": "Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly", "authors": "[{\"first\":\"Yongqin\",\"last\":\"Xian\",\"middle\":[]},{\"first\":\"Christoph\",\"last\":\"Lampert\",\"middle\":[\"H.\"]},{\"first\":\"Bernt\",\"last\":\"Schiele\",\"middle\":[]},{\"first\":\"Zeynep\",\"last\":\"Akata\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 7, "day": 3}, "abstract": "Due to the importance of zero-shot learning, i.e. classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1707.00600", "mag": "2963499153", "acl": null, "pubmed": "30028691", "pubmedcentral": null, "dblp": "journals/pami/XianLSA19", "doi": "10.1109/tpami.2018.2857768"}}, "content": {"source": {"pdf_hash": "c54c6ebba7f15002e7991ae7ba47a5b68adeb995", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1707.00600v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1707.00600", "status": "GREEN"}}, "grobid": {"id": "fe59520c310385dee065ad7ef9d7b5148c41a3bd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c54c6ebba7f15002e7991ae7ba47a5b68adeb995.txt", "contents": "\nZero-Shot Learning -A Comprehensive Evaluation of the Good, the Bad and the Ugly\n\n\nStudent Member, IEEEYongqin Xian \nChristoph H Lampert \nFellow, IEEEBernt Schiele \nMember, IEEEZeynep Akata \nZero-Shot Learning -A Comprehensive Evaluation of the Good, the Bad and the Ugly\n1Index Terms-Generalized Zero-shot LearningTransductive LearningImage classificationWeakly-Supervised Learning !\nDue to the importance of zero-shot learning, i.e. classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.\n\nINTRODUCTION\n\nZero-shot learning aims to recognize objects whose instances may not have been seen during training [1], [2], [3], [4], [5], [6]. The number of new zero-shot learning methods proposed every year has been increasing rapidly, i.e. the good aspects as our title suggests. Although each new method has been shown to make progress over the previous one, it is difficult to quantify this progress without an established evaluation protocol, i.e. the bad aspects. In fact, the quest for improving numbers has lead to even flawed evaluation protocols, i.e. the ugly aspects. Therefore, in this work, we propose to extensively evaluate a significant number of recent zero-shot learning methods in depth on several small to large-scale datasets using the same evaluation protocol both in zero-shot, i.e. training and test classes are disjoint, and the more realistic generalized zero-shot learning settings, i.e. training classes are present at test time. Figure 1 presents an illustration of zero-shot and generalized zero-shot learning tasks.\n\nWe benchmark and systematically evaluate zero-shot learning w.r.t. three aspects, i.e. methods, datasets and evaluation protocol. The crux of the matter for all zero-shot learning methods is to associate observed and non observed classes through some form of auxiliary information which encodes visually distinguishing properties of objects. Different flavors of zero-shot learning methods that we evaluate in this work are linear [7], [8], [9], [10] and nonlinear [11], [12] compatibility learning frameworks which have dominated the zero-shot learning literature in the past few years whereas an orthogonal direction is learning independent attribute [1] classifiers and finally others [13], [14], [15] propose a hybrid model between independent classifier learning and compatibility learning frameworks which have demonstrated improved results over the compatibility learning frameworks both for zeroshot and generalized zero-shot learning settings. We thoroughly evaluate the second aspect of zero-shot learning, by using multiple splits of several small, medium and largescale datasets [16], [17], [1], [18], [19]. Among these, the Animals with Attributes (AWA1) dataset [1] introduced as a zero-shot learning dataset with per-class attribute annotations, has been one of the most widely used datasets for zero-shot learning. However, as AWA1 images does not have the public copyright license, only some image features, i.e. SIFT [20], DECAF [21], VGG19 [22] of AWA1 dataset is publicly available, rather than the raw images. On the other hand, improving image features is a significant part of the progress both for supervised learning and for zero-shot learning. In fact, with the fast pace of deep learning, everyday new deep neural network models improve the ImageNet classification performance are being proposed. Without access to images, those new DNN models can not be evaluated on AWA1 dataset. Therefore, with this work, we introduce the Animals with Attributes 2 (AWA2) dataset that has roughly the same number of images all with public licenses, exactly the same number of classes and attributes as the AWA1 dataset. We will make both ResNet [23] features of AWA2 images and the images themselves publicly available.\n\u2022 Yongqin\nWe propose a unified evaluation protocol to address the third aspect of zero-shot learning which is one of the most important ones. We emphasize the necessity of tuning hyperparameters of the methods on a validation class split that is disjoint from training classes as improving zero-shot learning performance via tuning parameters on test classes violates the zero-shot assumption. We argue that per-class averaged top-1 accuracy is an important evaluation metric when the dataset is not well balanced with respect to the number of images per class. We point out that extracting image features via a pre-trained deep neural network Y tr  Fig. 1: Zero-shot learning (ZSL) vs generalized zero-shot learning (GZSL): At training time, for both cases the images and attributes of the seen classes (Y tr ) are available. At test time, in the ZSL setting, the learned model is evaluated only on unseen classes (Y ts ) whereas in GZSL setting, the search space contains both training and test classes (Y tr \u222a Y ts ). To facilitate classification without labels, both tasks use some form of side information, e.g. attributes. The attributes are annotated per class, therefore the labeling cost is significantly reduced.\n\n(DNN) on a large dataset that contains zero-shot test classes also violates the zero-shot learning idea as image feature extraction is a part of the training procedure. Moreover, we argue that demonstrating zero-shot performance on small-scale and coarse grained datasets, i.e. aPY [18] is not conclusive. On the other hand, with this work we emphasize that it is hard to obtain labeled training data for fine-grained classes of rare objects recognizing which requires expert opinion. Therefore, we argue that zero-shot learning methods should be also evaluated on least populated or rare classes. We recommend to abstract away from the restricted nature of zero-shot evaluation and make the task more practical by including training classes in the search space, i.e. generalized zero-shot learning setting. Therefore, we argue that our work plays an important role in advancing the zero-shot learning field by analyzing the good and bad aspects of the zero-shot learning task as well as proposing ways to eliminate the ugly ones.\n\n\nRELATED WORK\n\nEarly works of zero-shot learning [1], [24], [15], [25], [26] make use of the attributes within a two-stage approach to infer the label of an image that belong to one of the unseen classes. In the most general sense, the attributes of an input image are predicted in the first stage, then its class label is inferred by searching the class which attains the most similar set of attributes. For instance, DAP [1] first estimates the posterior of each attribute for an image by learning probabilistic attribute classifiers. It then calculates the class posteriors and predicts the class label using MAP estimate. Similarly, [24] first learns a probabilistic classifier for each attribute. It then estimates the class posteriors through random forest which is able to handle unreliable attributes. IAP [1] first predicts the class posterior of seen classes, then the probability of each class is used to calculate the attribute posteriors of an image. The class posterior of seen classes is predicted by a multi-class classifier. In addition, this two-stage approach have been extended to the case when attributes are not available. For example, following IAP [1], CONSE [15] first predicts seen class posteriors, then it projects image feature into the Word2vec [27] space by taking the convex combination of top T most possible seen classes. The two-stage models suffer from domain shift [28] between the intermediate task and target task, e.g. although the target task is to predict the class label, the intermediate task of DAP is to learn attribute classifiers.\n\nRecent advances in zero-shot learning directly learns a mapping from an image feature space to a semantic space. Among those, SOC [29] maps the image features into the semantic space and then searches the nearest class embedding vector. ALE [30] learns a bilinear compatibility function between the image and the attribute space using ranking loss. DeViSE [7] also learns a linear mapping between image and semantic space using an efficient ranking loss formulation, and it is evaluated on the large-scale ImageNet dataset. SJE [9] optimizes the structural SVM loss to learn the bilinear compatibility. On the other hand, ESZSL [10] uses the square loss to learn the bilinear compatibility and explicitly regularizes the objective w.r.t Frobenius norm. The l 2,1 -based objective function of [31] suppresses the noise in the semantic space. [32] embeds visual features into the attribute space, and then learns a metric to improve the consistency of the semantic embedding. Recently, SAE [33] proposed a semantic auto encoder to regularize the model by enforcing the image feature projected to the semantic space to be reconstructed.\n\nOther zero-shot learning approaches learn non-linear multimodal embeddings. LatEm [11] extends the bilinear compatibility model of SJE [9] to be a piecewise linear one by learning multiple linear mappings with the selection of which being a latent variable. CMT [12] uses a neural network with two hidden layers to learn a non-linear projection from image feature space to word2vec [27] space. Unlike other works which build their embedding on top of fixed image features, [34] trains a deep convolutional neural networks while learning a visual semantic embedding. Similarly, [35] argues that the visual feature space is more discriminative than the semantic space, thus it proposes an end-to-end deep embedding model which maps semantic features into the visual space. [36] proposes a simple model by projecting class semantic representations into the visual feature space and performing nearest neighbor classifiers among those projected representations. The projection is learned through support vector regressor with visual exemplars of seen classes, i.e. class centroid in the feature space.\n\nEmbedding both the image and semantic features into another common intermediate space is another direction that zero-shot learning approaches adapt. SSE [13] uses the mixture of seen class proportions as the common space and argues that images belong to the same class should have similar mixture pattern. JLSE [37] maps visual features and semantic features into two separate latent spaces, and measures their similarity by learning another bilinear compatibility function. Furthermore, hybrid models [38], [39], [14], [40] such as [39] jointly embeds multiple text representations and multiple visual parts to ground attributes on different image regions. SYNC [14] constructs the classifiers of unseen classes by taking the linear combinations of base classifiers, which are trained in a discriminative learning framework.\n\nWhile most of zero-shot learning methods learn the crossmodal mapping between the image and class embedding space with discriminative losses, there are a few generative models [41], [42], [43] that represent each class as a probability distribution. GFZSL [41] models each class-conditional distribution as a Gaussian and learns a regression function that maps a class embedding into the latent space. GLaP [42] assumes that each classconditional distribution follows a Gaussian and generates virtual instances of unseen classes from the learned distribution. [43] learns a multimodal mapping where class and image embeddings of categories are both represented by Gaussian distributions.\n\nApart from the inductive zero-shot learning set-up where the model has no access to neither visual nor side-information of unseen classes, transductive zero-shot learning approaches [44], [45], [28], [46], [47], [48], [49] use visual or semantic information of both seen and unseen classes without having access to the label information. [44] combines DAP and graph-based label propagation. [45] uses the idea of domain adaptation frameworks. [28] proposes hypergraph label propagation which allows to use multiple class embeddings. [46], [47], [48] use semi-supervised learning based on max-margin framework.\n\nIn zero-shot learning, some form of side information is required to share information between classes so that the knowledge learned from seen classes is transfered to unseen classes. One popular form of side information is attributes, i.e. shared and nameable visual properties of objects. However, attributes usually require costly manual annotation. Thus, there has been a large group of studies [50], [51], [9], [11], [7], [52], [31], [34], [53], [54] which exploit other auxiliary information that reduces this annotation effort. [55] does not use side information however it requires one-shot image of the novel class to perform nearest neighbor search with the learned metric. SJE [9] evaluates four different class embeddings including attributes, word2vec [27], glove [56] and wordnet hierarchy [57]. On ImageNet, [3] leverages the wordnet hierarchy. [52] leverages the rich information of detailed visual descriptions obtained from novice users and improves the performance of attributes obtained from experts. Recently, [58] took a different approach and learned class embeddings using human gaze tracks showing that human gaze is class-specific.\n\nZero-shot learning has been criticized for being a restrictive set up as it comes with a strong assumption of the image used at prediction time can only come from unseen classes. Therefore, generalized zero-shot learning setting [59] has been proposed to generalize the zero-shot learning task to the case where both seen and unseen classes are used at test time. [60] argues that although ImageNet classification challenge performance has reached beyond human performance, we do not observe similar behavior of the methods that compete at the detection challenge which involves rejecting unknown objects while detecting the position and label of a known object. [7] uses label embeddings to operate on the generalized zero-shot learning setting whereas [61] proposes to learn latent representations for images and classes through coupled linear regression of factorized joint embeddings. On the other hand, [62] introduces a new model layer to the deep net which estimates the probability of an input being from an unknown class and [12] proposes a novelty detection mechanism.\n\nAlthough zero-shot vs generalized zero-shot learning evaluation works exist [3], [63] in the literature, our work stands out in multiple aspects. For instance, [3] operates on the ImageNet 1K by using 800 classes for training and 200 for test. One of the most comprehensive works, [63] provides a comparison between five methods evaluated on three datasets including ImageNet with three standard splits and proposes a metric to evaluate generalized zero-shot learning performance. On the other hand, we evaluate ten zero-shot learning methods on five datasets with several splits both for zero-shot and generalized zero-shot learning settings, provide statistical significance and robustness tests, and present other valuable insights that emerge from our benchmark. In this sense, ours is the most extensive evaluation of zero-shot and generalized zero-shot learning tasks in the literature.\n\n\nEVALUATED METHODS\n\nWe start by formalizing the zero-shot learning task and then we describe the zero-shot learning methods that we evaluate in this work. Given a training set S = {(x n , y n ), n = 1...N }, with y n \u2208 Y tr belonging to training classes, the task is to learn f : X \u2192 Y by minimizing the regularized empirical risk:\n1 N N n=1 L(y n , f (x n ; W )) + \u2126(W )(1)\nwhere L(.) is the loss function and \u2126(.) is the regularization term.\n\nHere, the mapping f : X \u2192 Y from input to output embeddings is defined as:\nf (x; W ) = argmax y\u2208Y F (x, y; W )(2)\nAt test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label, i.e. Y ts \u2282 Y and in generalized zero-shot learning setting, the test image can be assigned either to seen or unseen classes, i.e. Y tr+ts \u2282 Y with the highest compatibility score.\n\n\nLearning Linear Compatibility\n\nAttribute Label Embedding (ALE) [30], Deep Visual Semantic Embedding (DEVISE) [7] and Structured Joint Embedding (SJE) [9] use bi-linear compatibility function to associate visual and auxiliary information:\nF (x, y; W ) = \u03b8(x) T W \u03c6(y)(3)\nwhere \u03b8(x) and \u03c6(y), i.e. image and class embeddings, both of which are given. F (.) is parameterized by the mapping W , that is to be learned. Given an image, compatibility learning frameworks predict the class which attains the maximum compatibility score with the image. Among the methods that are detailed below, ALE [30], DE-VISE [7] and SJE [9] do early stopping to implicitly regularize Stochastic Gradient Descent (SGD) while ESZSL [10] and SAE [33] explicitly regularize the embedding model as detailed below. In the following, we provide a unified formulation of these five zero-shot learning methods. DEVISE [7] uses pairwise ranking objective that is inspired from unregularized ranking SVM [64]:\ny\u2208Y tr [\u2206(y n , y) + F (x n , y; W ) \u2212 F (x n , y n ; W )] + (4)\nwhere \u2206(y n , y) is equal to 1 if y n = y, otherwise 0. The objective function is convex and is optimized by Stochastic Gradient Descent.\n\nALE [30] uses the weighted approximate ranking objective [65] for zero-shot learning in the following way: y\u2208Y tr l r \u2206(xn ,yn ) r \u2206(xn,yn)\n\n[\u2206(y n , y) + F (x n , y; W ) \u2212 F (x n , y n ; W )] + (5) where l k = k i=1 \u03b1 i and r \u2206(xn,yn) is defined as:\n\ny\u2208Y tr 1(F (x n , y; W ) + \u2206(y n , y) \u2265 F (x n , y n ; W ))\n\nFollowing the heuristic in [66], [30] selects \u03b1 i = 1/i which puts a high emphasis on the top of the rank list.\n\nSJE [9] gives the full weight to the top of the ranked list and is inspired from the structured SVM [67]:\n\n[ max y\u2208Y tr (\u2206(y n , y) + F (x n , y; W )) \u2212 F (x n , y n ; W )] +\n\nThe prediction can only be made after computing the score against all the classifiers, i.e. so as to find the maximum violating class, which makes SJE less efficient than DEVISE and ALE.\n\nESZSL [10] applies a square loss to the ranking formulation and adds the following implicit regularization term to the unregularized risk minimization formulation:\n\u03b3 W \u03c6(y) 2 + \u03bb \u03b8(x) T W 2 + \u03b2 W 2(8)\nwhere \u03b3, \u03bb, \u03b2 are regularization parameters. The first two terms bound the Euclidean norm of projected attributes in the feature space and projected image feature in the attribute space respectively. The advantage of this approach is that the objective function is convex and has a closed form solution.\n\nSAE [33] also learns the linear projection from image embedding space to class embedding space, but it further constrains that the projection must be able to reconstruct the original image embedding. Similar to the linear auto-encoder, SAE optimizes the following objective:\nmin W ||\u03b8(x) \u2212 W T \u03c6(y)|| 2 + \u03bb||W \u03b8(x) \u2212 \u03c6(y)|| 2 ,(9)\nwhere \u03bb is a hyperparameter to be tuned. The optimization problem can be transformed such that Bartels-Stewart algorithm [68] is able to solve it efficiently.\n\n\nLearning Nonlinear Compatibility\n\nLatent Embeddings (LATEM) [11] and Cross Modal Transfer (CMT) [12] encode an additional non-linearity component to linear compatibility learning framework.\n\nLATEM [11] constructs a piece-wise linear compatibility:\nF (x, y; W i ) = max 1\u2264i\u2264K \u03b8(x) T W i \u03c6(y)(10)\nwhere every W i models a different visual characteristic of the data and the selection of which matrix to use to do the mapping is a latent variable and K is a hyperparameter to be tuned. LATEM uses the ranking loss formulated in Equation 4 and Stochastic Gradient Descent as the optimizer.\n\nCMT [12] first maps images into a semantic space of words, i.e. class names, where a neural network with tanh nonlinearity learns the mapping:\ny\u2208Y tr x\u2208Xy \u03c6(y) \u2212 W 1 tanh(W 2 .\u03b8(x) 2(11)\nwhere (W 1 , W 2 ) are weights of the two layer neural network. This is followed by a novelty detection mechanism that assigns images to unseen or seen classes. The novelty is detected either via thresholds learned using the embedded images of the seen classes or the outlier probabilities are obtained in an unsupervised way. As zero-shot learning assumes that test images are only from unseen classes, in our experiments when we refer to CMT, that means we do not use the novelty detection component. On the other hand, we name the CMT with novelty detection as CMT* when we apply it to the generalized zero-shot learning setting.\n\n\nLearning Intermediate Attribute Classifiers\n\nAlthough Direct Attribute Prediction (DAP) [1] and Indirect Attribute Prediction (IAP) [1] have been shown to perform poorly compared to compatibility learning frameworks [30], we include them to our evaluation for being historically the most widely used methods in the literature.\n\nDAP [1] learns probabilistic attribute classifiers and makes a class prediction by combining scores of the learned attribute classifiers. A novel image is assigned to one of the unknown classes using:\nf (x) = argmax c M m=1 p(a c m |x) p(a c m ) .(12)\nwith M being the total number of attributes, a c m is the m-th attribute of class c, p(a c m |x) is the attribute probability given image x which is obtained from the attribute classifiers whereas p(a c m ) is the attribute prior estimated by the empirical mean of attributes over training classes. We train binary classifiers with logistic regression that gives probability scores of attributes with respect to training classes.\n\nIAP [1] indirectly estimates attributes probabilities of an image by first predicting the probabilities of each training class, then multiplying the class attribute matrix. Once the attributes probabilities are obtained by the following equation:\np(a m |x) = K k=1 p(a m |y k )p(y k |x),(13)\nwhere K is the number of training classes, p(a m |y k ) is the predefined class attribute and p(y k |x) is training class posterior from multi-class classifier, the Equation 12 is used to predict the class label for which we train a multi-class classifier on training classes with logistic regression.\n\n\nHybrid Models\n\nSemantic Similarity Embedding (SSE) [13], Convex Combination of Semantic Embeddings (CONSE) [15] and Synthesized Classifiers (SYNC) [14] express images and semantic class embeddings as a mixture of seen class proportions, hence we group them as hybrid models.\n\nSSE [13] leverages similar class relationships both in image and semantic embedding space. An image is labeled with:\nargmax u\u2208U \u03c0(\u03b8(x)) T \u03c8(\u03c6(y u ))(14)\nwhere \u03c0, \u03c8 are mappings of class and image embeddings into a common space defined by the mixture of seen classes proportions. Specifically, \u03c8 is learned by sparse coding and \u03c0 is by class dependent transformation.\n\nCONSE [15] learns the probability of a training image belonging to a training class:\nf (x, t) = argmax y\u2208Y tr p tr (y|x)(15)\nwhere y denotes the most likely training label (t=1) for image x. Combination of semantic embeddings (s) is used to assign an unknown image to an unseen class:\n1 Z T i=1 p tr (f (x, t)|x).s(f (x, t))(16)\nwhere Z = T i=1 p tr (f (x, t)|x), f (x, t) denotes the t th most likely label for image x and T controls the maximum number of semantic embedding vectors.\n\nSYNC [14] learns a mapping between the semantic class embedding space and a model space. In the model space, training classes and a set of phantom classes form a weighted bipartite graph. The objective is to minimize distortion error:\nmin wc w c \u2212 R r=1 s cr v r 2 2 .(17)\nSemantic and model spaces are aligned by embedding classifiers of real classes (w c ) and classifiers of phantom classes (v r ) in the weighted graph (s cr ). The classifiers for novel classes are constructed by linearly combining classifiers of phantom classes.\n\nGFZSL [41] proposes a generative framework for zero-shot learning by modeling each class-conditional distribution as a multivariate Gaussian with mean vector \u00b5 and diagonal covariance matrix \u03c3. While the parameters of seen classes can be estimated by MLE, that of unseen classes are computed by learning the following two regression functions:\n\u00b5 y = f \u00b5 (\u03c6(y)) and \u03c3 y = f \u03c3 (\u03c6(y))(18)\nwith an image x, its class is predicted by searching the class with the maximum probability, i.e. argmax y p(x|\u03c3 y , \u00b5 y ).\n\n\nTransductive Zero-Shot Learning Setting\n\nIn zero-shot learning, transductive setting [69], [70] implies that unlabeled images from unseen classes are available during training. Using unlabeled images are expected to improve performance as they possibly contain useful latent information of unseen classes. Here, we mainly focus on two state-of-the-art transductive approaches [41], [71] and show how to extend ALE [30] into the transductive learning setting.\n\nGFZSL-tran [41] uses an Expectation-Maximization (EM) based procedure that alternates between inferring the labels of unlabeled examples of unseen classes and using the inferred labels to update the parameter estimates of unseen class distributions. Since the class-conditional distribution is assumed to be Gaussian, this procedure is equivalent to repeatedly estimating a Gaussian Mixture Model (GMM) with the unlabeled data from unseen classes and use the inferred class labels to re-estimate the GMM.\n\nDSRL [71] proposes to simultaneously learn image features with non-negative matrix factorization and align them with their corresponding class attributes. This step gives us an initial prediction score matrix S 0 in which each row is one instance and indicates the prediction scores for all unseen classes. To improve the prediction score matrix by transductive learning, a graph-based label propagation algorithm is applied. Specifically, a KNN graph is constructed with the projected instances of unseen classes in the class embedding space,\nM ij = exp(\u2212 d(xi,xj ) 2\u03c3 2 ) if i \u2208 KNN(j) or j \u2208 KNN(i) 0\notherwise (19) where KNN(i) denotes the k-nearest neighbor of i-th instance and d(x i , x j ) measures the Euclidean distance between x i and x j . Given the affinity matrix M , a normalized Laplacian matrix L can be computed as\nL = Q \u22121/2 M Q \u22121/2 where Q is a diagonal matrix with Q ii = j M ij .\nFinally, the standard label propagation [72] gives the closed-form solution:\nS = (I \u2212 \u03b1L) \u22121 \u00d7 S 0(20)\nwhere \u03b1 \u2208 [0, 1] is a regularization trade-off parameter and S is the score matrix. The class label of an instance is predicted by searching the class with the highest score, i.e. argmax y S iy .\n\nALE-tran Any compatibility learning method that explicitly learns cross-modal mapping from image feature space to class embedding space can be extended to transductive setting following the label propagation procedure of DSRL [71]. Taking the ALE [30] as an example, after learning the linear mapping W , instances of unseen classes can be projected into the class embedding space and a score matrix S 0 can be computed similarly.\n\n\nDATASETS\n\nAmong the most widely used datasets for zero-shot learning, we select two coarse-grained, one small (aPY [18]) and one medium-scale (AWA1 [1]), and two fine-grained, both mediumscale, datasets (SUN [16], CUB [17]) with attributes and one large-scale dataset (ImageNet [19]) without. Here, we consider between 10K and 1M images, and, between 100 and 1K classes as medium-scale. Details of dataset statistics in terms of the number of images, classes, attributes for the attribute datasets are in Table 1. Furthermore, we introduce our Animals With Attributes 2 (AWA2) dataset and position it with respect to existing datasets.\n\n\nAttribute Datasets\n\nAttribute Pascal and Yahoo (aPY) [ Figure 1.\n\nIn Figure 2, we provide some statistics on the AWA2 dataset in comparison with the AWA1 dataset in terms of the number of images and also the distribution of the image features. Compared to AWA1, our proposed AWA2 dataset contains more images, e.g. horse and dolphin among the test classes, antelope and cow among the training classes. Moreover, the t-SNE embedding of these test classes with more training data, e.g. horse, dolphin, seal etc. shows that AWA2 leads to slightly more visible clusters of ResNet features. The images, their labels and ResNet features of our AWA2 are publicly available in http://cvml.ist.ac.at/AwA2.\n\n\nLarge-Scale ImageNet\n\nWe also evaluate the performance of methods on the large scale ImageNet [19] which contains a total of 14 million images from 21K classes, each one labeled with one label, and the classes are hierarchically related as ImageNet follows the WordNet [57].\n\nImageNet is a natural fit for zero-shot and generalized zeroshot learning as there is a large class imbalance problem. Moreover, ImageNet is diverse in terms of granularity, i.e. it contains a collection of fine-grained datasets, e.g. different vehicle types, as well as coarse-grained datasets. The highest populated class contains 3, 047 images whereas there are many classes that contains only a single image. A balanced subset of ImageNet with 1K classes containing about 1000 images each is used to train CNNs.\n\nPrevious works [3] proposed to split the balanced subset of 1K classes into 800 training and 200 test classes. In this work, from the total of 21K classes, we use 1K classes for training (among which we use 200 classes for validation) and the test split is either all the remaining 20K classes or a subset of it, e.g. we determine these subsets based on the hierarchical distance between classes and the population of classes. The details of these splits are provided in the following section.\n\n\nEVALUATION PROTOCOL\n\nIn this section, we provide several components of previously used and our proposed ZSL and GZSL evaluation protocols, e.g. image and class encodings, dataset splits and the evaluation criteria 1 .\n\n\nImage and Class Embedding\n\nWe extract image features, namely image embeddings, from the entire image for SUN, CUB, AWA1, our AWA2 and ImageNet, with no image pre-processing. For aPY, following the original publication in [18], we crop the images from bounding boxes. Our image embeddings are 2048-dim top-layer pooling units of the 101-layered ResNet [23] as we found that it performs better than 1, 024-dim top-layer pooling units of GoogleNet [73]. We use the original ResNet-101 that is pre-trained on ImageNet with 1K classes, i.e. the balanced subset, and we do not fine-tune it for any of the mentioned datasets. In addition to the ResNet features, we re-evaluate all methods with their published image features.\n\nIn zero-shot learning, class embeddings are as important as image features. As class embeddings, for aPY, AWA1, AWA2, CUB and SUN, we use the per-class attributes between values 0 and 1 that are provided with the datasets as binary attributes have been shown [30] to be weaker than continuous attributes. For ImageNet as attributes of 21K classes are not available, we use Word2Vec [27] trained on Wikipedia provided by [14]. Note that an evaluation of class embeddings is out of the scope of this paper. We refer the reader to [9] for more details on the topic.\n\n\nDataset Splits\n\nZero-shot learning assumes disjoint training and test classes. Hence, as deep neural network (DNN) training for image feature extraction is actually a part of model training, the dataset used to train DNNs, e.g. ImageNet, should not include any of the test classes. However, we notice from the standard splits (SS) of aPY and AWA1 datasets that 7 aPY test classes out of 12 (monkey, wolf, zebra, mug, building, bag, carriage), 6 AWA1 test classes out of 10 (chimpanzee, giant panda, leopard, persian cat, pig, hippopotamus), are among the 1K classes of ImageNet, i.e. are used to pre-train ResNet. On the other hand, the mostly widely used splits, i.e. we term them as standard splits (SS), for SUN from [1] and CUB from [8] shows us that 1 CUB test class out of 50 (Indigo Bunting), and 6 SUN test classes out of 72 (restaurant, supermarket, planetarium, tent, market, bridge), are also among the 1K classes of ImageNet.\n\nWe noticed that the accuracy for all methods on those overlapping test classes are higher than others. Therefore, we propose new dataset splits, i.e. proposed splits (PS), insuring that none of the test classes appear in ImageNet 1K, i.e. used to train the ResNet model. We present the differences between the standard splits (SS) and the proposed splits (PS) in Table 1 [16], CUB [17], AWA1 [1], proposed AWA2, aPY [18] in terms of size, granularity, number of attributes, number of classes in Y tr and Y ts , number of images at training and test time for standard split (SS) and our proposed splits (PS).\n\nway as evaluating accuracy on both training and test classes is crucial to show the generalization of the methods. For SUN, CUB, AWA1, aPY, and our proposed AWA2 dataset, for measuring the significance of the results, we propose 3 different splits of 580, 100, 27, 15 and 27 training classes respectively while keeping 72, 50, 10, 12 and 10 test classes the same. It is important to perform hyperparameter search on a disjoint set of validation set of 65, 50, 13, 5 and 13 classes respectively. We keep the number of classes the same for SS and PS, however we choose different classes while making sure that the test classes do not overlap with the 1K training classes of ImageNet.\n\nImageNet provides possibilities of constructing several zeroshot evaluation splits. Following [14], our first two standard splits consider all the classes that are 2-hops and 3-hops away from the original 1K classes according to the ImageNet label hierarchy, corresponding to 1509 and 7678 classes. This split measures the generalization ability of the models with respect to the hierarchical and semantic similarity between classes. As discussed in the previous section, another characteristic of ImageNet is the imbalanced sample size. Therefore, our proposed split considers 500, 1K and 5K most populated classes among the remaining 21K classes of ImageNet with approximately 1756, 1624 and 1335 images per class on average. Similarly, we consider 500, 1K and 5K least-populated classes in ImageNet which correspond to most fine-grained subsets of ImageNet with approximately 1, 3 and 51 images per class on average. We measure the generalization of methods to the entire ImageNet data distribution by considering a final split of all the remaining approximately 20K classes of ImageNet with at least 1 image per-class, i.e. approximately 631 images per class on average.\n\n\nEvaluation Criteria\n\nSingle label image classification accuracy has been measured with Top-1 accuracy, i.e. the prediction is accurate when the predicted class is the correct one. If the accuracy is averaged for all images, high performance on densely populated classes is encouraged. However, we are interested in having high performance also on sparsely populated classes. Therefore, we average the correct predictions independently for each class before dividing their cumulative sum w.r.t the number of classes, i.e. we measure average per-class top-1 accuracy in the following way:\nacc Y = 1 Y Y c=1 # correct predictions in c # samples in c(21)\nIn the generalized zero-shot learning setting, the search space at evaluation time is not restricted to only test classes (Y ts ), but includes also the training classes (Y tr ), hence this setting is more practical.  \nH = 2 * acc Y tr * acc Y ts acc Y tr + acc Y ts(22)\nwhere acc Y tr and acc Y ts represent the accuracy of images from seen (Y tr ), and images from unseen (Y ts ) classes respectively. We choose harmonic mean as our evaluation criteria and not arithmetic mean because in arithmetic mean if the seen class accuracy is much higher, it effects the overall results significantly. Instead, our aim is high accuracy on both seen and unseen classes.\n\n\nEXPERIMENTS\n\nWe first provide ZSL results on the attribute datasets SUN, CUB, AWA1, AWA2 and aPY and then on the large-scale ImageNet dataset. Finally, we present results for the GZSL setting.\n\n\nZero-Shot Learning Experiments\n\nOn attribute datasets, i.e. SUN, CUB, AWA1, AWA2, and aPY, we first reproduce the results of each method using their evaluation protocol, then provide a unified evaluation protocol using the same train/val/test class splits, followed by our proposed train/val/test class splits on SUN, CUB, AWA1, aPY and AWA2. We also evaluate the robustness of the methods to parameter tuning and visualize the ranking of different methods. Finally, we evaluate the methods on the large-scale ImageNet dataset.\n\nComparing State-of-The-Art Models. For sanity-check, we reevaluate methods [1], [13], [11], [9], [10], [14] and [33]    CIFAR dataset.). We observe from the results in Table 2 that our reproduced results of DAP [1], SYNC [14], GFZSL [41], GFZSLtran [41], DSRL [71] and SAE [33] are nearly identical to the reported number in their original publications. For LATEM [11], we obtain slightly different results which can be explained by the non-convexity and thus the sensibility to initialization. Similarly for SJE [9] random sampling in SGD might lead to slightly different results. ESZSL [10] has some variance because its algorithm randomly picks a validation set during each run, which leads to different hyperparameters. Notable observations on SSE [13] results are as follows. The published code has hard-coded hyperparameters operational on aPY, i.e. number of iterations, number of data points to train SVM, and one regularizer parameter \u03b3 which lead to inferior results than the ones reported here, therefore we set these parameters on validation sets. On SUN, SSE uses 10 classes (instead of 72) and our results with validated parameters got an improvement of 0.5% that may be due to random sampling of training images. On AWA1, our reproduced result being 64.9% is significantly lower than the reported result (76.3%). However, we could not reach the reported result even by tuning parameters on the test set (73.8%). In addition to [1], [13], [11], [9], [10], [14], [12], [33], we re-implement [15], [7], [30] based on the original publications. We use train, validation, test splits as provided in Table 1 and report results in Table 3 [10]). Second, after careful examination and correspondence with the authors of SYNC [14], we detected that SUN features were extracted with a MIT Places [74] pre-trained model. As the MIT Places dataset intersects with both training and test classes of SUN, it is expected to lead to significantly better results than ImageNet pre-trained models (62.8% vs 59.1%). In addition, while SAE [33] reported 84.7% on AWA1, we obtain only 80.7% on the standard split. This could be explained by two differences. First, we measure per-class accuracy but SAE [33] reports per-image accuracy which is typically higher when the dataset is class-imbalanced, e.g. AWA1. Indeed, their reported accuracy decreases to 82.0% if per-class accuracy is applied. Second, we confirmed with the authors of SAE [33] that they improved GoogleNet [75] by adding Batch Normalization and averaging 5 randomly cropped images to obtain better image features. Therefore, as expected, improving visual features lead to improved results in zero-shot learning.\n\nPromoting Our Proposed Splits (PS). We propose new dataset splits (see details in section 4) ensuring that test classes of any of the datasets do not overlap with the ImageNet1K used to pre-train ResNet. As training ResNet is a part of the training procedure, including test classes in the dataset used for pre-training ResNet would violate the zero-shot learning conditions. We compare the results obtained with our proposed split (PS) with previously published standard split (SS) results in Table 3.\n\nOur first observation is that the results on the PS are significantly lower than the SS for AWA1 and AWA2. This is expected as most of the test classes of AWA1 and AWA2 in SS overlaps with ImageNet 1K. On the other hand, for fine-grained datasets CUB and SUN, the results are not significantly effected as the overlap in that case was not as significant. Our second observation regarding the method ranking is as follows. On SS, SYNC [14] is the best performing method on SUN (59.1%) and aPY (39.7%) datasets whereas SJE [9] performs the best on CUB (55.3%) and SAE [33] performs the best on AWA1 (80.6%) and AWA2 (80.7%) dataset. On PS, ALE [30] performs the best on SUN (58.1%) and AWA2 (62.5%), SYNC [14] on CUB (55.6%), SJE [9] on AWA1 (65.6%) and DEVISE [7] on aPY (39.8%). ALE, SJE and DEVISE all use max-margin bi-linear compatibility learning framework which seem to perform better than others. It is also worth to note that SYNC and SAE perform well on SS, i.e. SYNC is the best performing model for SUN and aPY whereas SAE is for AWA1 and AWA2 on SS, while they perform significantly lower in PS which indicates that they do not generalize well in zero-shot learning task.\n\nEvaluating Robustness. We evaluate robustness of 13 methods, i.e. [1], [13], [11], [9], [10], [14], [12], [15], [7], [30], [33], [41], to hyperparameters by setting them on 3 different validation splits while keeping the test split intact. We report results on SS (Figure 3, top) and PS (Figure 3, bottom) for SUN, CUB, AWA1, AWA2 and aPY datasets. On SUN and CUB, the results are stable across methods and across dataset splits. This is expected as these datasets both have a balanced number of images across classes and they are fine-grained datasets. Therefore, the validation splits are similar. On the other hand, aPY being a small and coarsegrained dataset has several issues. First, many of the test classes of aPY are included in ImageNet1K. Second, it is not well balanced, i.e. different validation class splits contain significantly different number of images. Third, the class embeddings are far from each other, i.e. objects are semantically different, therefore different validation splits learn a different mapping between images and classes. On AWA1 and AWA2, on SS, the DEVISE method seems to show the largest variance. This might be due to the fact that AWA1 and AWA2 datasets are also coarse-grained and test classes overlap with ImageNet training classes. Indeed, AWA2 being slightly more balanced than AWA1, in the proposed split it does not lead to such a high variance for DEVISE.\n\nVisualizing Method Ranking. We first evaluate the 13 methods using three different validation splits as in the previous experiment. We then rank them based on their per-class top-1 accuracy using the non-parametric Friedman test [76], which does not assume a distribution on performance but rather uses algorithm ranking. Each entry of the rank matrix on Figure 4 indicates the number of times the method is ranked at the first to thirteenth rank. We then compute the mean rank of each method and order them based on the mean rank across datasets.\n\nOur general observation is that the highest ranked method on both splits is GFZSL, the second highest ranked method on the standard split (SS) is SYNC while it drops to the seventh rank     on the proposed split (PS). On the other hand, ALE ranks the second on the SS and the first on the PS. We reinforce our initial observation from numerical results and conclude that GFZSL and ALE seems to be the method that is the most robust in zero-shot learning setting for attribute datasets. These results also indicate the importance of choosing zero-shot splits carefully. On the PS, the two of three highest ranked methods are compatibility learning methods, i.e. ALE and DEVISE whereas the three lowest ranked methods are attribute classifier learning or hybrid methods, i.e. IAP, CMT and CONSE. Therefore, max-margin compatibility learning methods lead to consistently better results in the zeroshot learning task compared to learning independent classifiers. Finally, visualizing the method ranking in this way provides a visually interpretable way of how models compare across datasets.\n\nResults on Our Proposed AWA2. We introduce AWA2 which has the same classes and attributes as AWA1, but contains different images each coming with a public copyright license. In order to show that AWA1 and AWA2 images are not the same but similar in nature, we compare the zero-shot learning results on AWA1 and AWA2 in   5: ImageNet with different splits: 2/3 H = classes with 2/3 hops away from the Y tr of ImageNet1K, 500/1K/5K most populated classes, 500/1K/5K least populated classes, All = The remaining 20K categories of ImageNet (Y ts ). We measure top-1 accuracy in %. and AWA2 is within 2%. On the other hand, the same consistency is not observed for DEVISE [7], SJE [9] and SYNC [14]. For instance, SJE [9] obtains 65.6% on AWA1 and 61.9% on AWA2.\n\nAfter careful examination, we noticed that SJE [9] selects different hyperparameters for AWA1 and AWA2, which results in different performance on those two datasets. In our opinion, this does not indicate a possible dataset artifact, however shows that zero-shot learning is sensitive to parameter setting.\n\nCommonly, a model is trained and evaluated on the same dataset. Across dataset experiments are not easy as different datasets do not share the same attributes. However, AWA1 and AWA2 share both classes and attributes. In order to verify that AWA2 is a good replacement for AWA1, we conduct acrossdataset evaluation for 12 methods, i.e. [1], [13], [11], [9], [10], [14], [12], [15], [7], [30], [33]. In particular, with our Proposed Splits (PS), we train one model on the training set of AWA1 and evaluate it on the test set of AWA2 in the zero-shot learning setting, and vice versa. From Table. 4, we observe that all the models trained on AWA1 generalize well to AWA2 and vice versa.\n\nIn addition, we notice that the cross-dataset result is dependent on the training set. For instance, for all the methods, if we fix training set to be from AWA1, the results on the test set of AWA1 and AWA2 are close. To verify this hypothesis, we performed a paired t-test which determines if the mean difference between paired results is significantly higher than zero. To that end, we take the 24 pairs of results whose test sets are the same, i.e. the results obtained with 12 methods on AWA1:AWA2 and AWA2:AWA2 (2nd and 3rd column) as well as the results obtained with 12 methods on AWA1:AWA1 and AWA2:AWA1 (1st and 4th column). The paired t-test rejects the null hypothesis with p-value= 0.007, indicating that the results are significantly different if the test set is the same but the training set is different. As a conclusion, the training set is an important indicator of the final result and the two datasets, i.e. AWA1 and AWA2 are sufficiently similar. Therefore, our cross-dataset experimental results indicate that AWA2 is a good replacement for AWA1.\n\nZero-Shot Learning Results on ImageNet. ImageNet scales the methods to a truly large-scale setting, thus these experiments provide further insights on how to tackle the zero-shot learning problem from the practical point of view. Here, we evaluate 10 methods, i.e. [11], [9], [10], [14], [12], [15], [7], [30], [33], [41]. We exclude DAP and IAP as attributes are not available for all ImageNet classes as well as SSE [13] due to scalability issues of the public implementation of the method. Table 5 shows that the best performing method is SYNC [14] which may either indicate that it performs well in large-scale setting or it can learn under uncertainty due to usage of Word2Vec instead of attributes. Another possibility is Word2Vec may be tuned for SYNC as it is provided by the same authors. However, we refrain to make a strong claim as this would requires a full evaluation on class embeddings which is out of the scope of this paper. On the other hand, GFZSL [41] which is the best performing model for attribute datasets perform poorly on ImageNet which may indicate that generative models require a strong class embedding space such as attributes to perform well on ZSL task. Note that due to the computational issues, we were not able to obtain results for GFZSL for 3H, M5K, L5K and All 20K classes.\n\nMore detailed observations are as follows. The second highest  performing method is ESZSL [10] which is one of the linear embedding models that have an implicit regularization mechanism, which seems to be more effective than early stopping as an explicit regularizer. A general observation from the results of all the methods is that in the most populated classes, the results are higher than the least populated classes which indicates that zero-shot learning on fine-grained ImageNet subsets is a more difficult task. Moreover, we conclude that the nature of the test set, e.g. type of the classes being tested, is more important than the number of classes. Therefore, the selection of the test set is an important aspect of zero-shot learning on large-scale datasets. Furthermore, for all methods we consistently observe a large drop in accuracy between 1K and 5K most populated classes which is expected as 5K contains \u2248 6.6M images, making the problem much more difficult than 1K (\u2248 1624 images). It is worth to note that, measuring per-image accuracy in this case would lead to higher results if the labels of the highly populated class samples are predicted correctly. Finally, the largest test set, i.e. All 20K, the results are poor for all methods which indicates the difficulty of this problem where there is a large room for improvement. Several models in the literature evaluate Top-5 and Top-10 as well as Top-1 accuracy on ImageNet. Top-5 and Top-10 accuracy in this case is reasonable as an image usually contains multiple objects however by construction it is associated with a single label in ImageNet. Hence, we provide a comparison of the same 9 models according to all these three criteria in Figure 5. We observe that SYNC [14] performs significantly better than other methods when the number of images is higher, e.g. 2H, M500, M1K, whereas the gap reduces when the number of images and the number of classes increase, e.g. 3H, L5K and All. In fact, when for All, all the methods perform similarly and poorly which indicates that there is a large room for improvement in this task. In fact, this observation carries on for all three accuracy measures. For Top-5 (middle) and Top-10 (right) accuracy although the numbers are as expected in general higher, the winning model remains as SYNC, significantly for 2H, M500 and M1K whereas the difference is smaller with 3H, L5H, L1K. On the other hand, all methods perform similarly when all 20K classes are tested.\n\n\nGeneralized Zero-Shot Learning Results\n\nIn real world applications, image classification systems do not have access to whether a novel image belongs to a seen or unseen class in advance. Hence, generalized zero-shot learning is interesting from a practical point of view. Here, we use same models trained on ZSL setting on our proposed splits (PS). We evaluate performance on both Y tr and Y ts (using held-out images).\n\nAs shown in Table 6, generalized zero-shot learning results are significantly lower than zero-shot learning results. This is due to the fact that training classes are included in the search space which act as distractors for the images that come from test classes, e.g. most of the images that are being evaluated. An interesting observation is that compatibility learning frameworks, e.g. ALE, DEVISE, SJE, perform well on test classes. However, methods that learn independent attribute or object classifiers, e.g. DAP and CONSE, perform well on training classes. Due to this discrepancy, we evaluate the harmonic mean which takes a weighted average of training and test class accuracy as shown in Equation 17. The harmonic mean measure ranks ALE as the best performing method on SUN, CUB and AWA1 datasets whereas on our AWA2 dataset DEVISE performs the best and on aPY dataset CMT* performs the best. Note that CMT* has an integrated novelty detection phase for which the method receives another supervision signal determining if the image belongs to a training or a test class. Similar to the ImageNet results, GFZSL [41] performs poorly on GZSL setting.\n\nAs for the generalized zero-shot learning setting on ImageNet, we report results measured on unseen classes as no images are reserved from seen classes on Figure 6. Our first observation is that there is no winner model in all cases, the results diverge for different splits and different accuracy measures. For instance, when the performance is measured with Top-1 accuracy, in general the best performing model seems to be DEVISE, ALE and SJE which are all linear compatibility learning models. On the other hand, for Top-5 accuracy different models take the lead in different splits, e.g. CONSE works the best for 3H and M5K indicating that it performs better when the number of images that come from unseen classes is larger. Whereas SJE and ESZSL works better for 2H, M500, L5H settings. Finally, for Top-10 accuracy, the best performing model overall is ESZSL which is the model that learns a linear compatibility with an explicit regularization scheme. Finally, for Top-1, Top-5 and Top-10 results we observe the same trend for when all the unseen classes are included in the test set, i.e. the models perform similarly however CONSE slightly stands out for Top-5 and Top-10 accuracy plots.\n\nIn summary, generalized zero-shot learning setting provides one more level of detail on the performance of zero-shot learning methods. Our take-home message is that the accuracy of training classes is as important as the accuracy of test classes in real world scenarios. Therefore, methods should be designed in a way that Fig. 6: GZSL on Imagenet, measuring Top-1, Top-5 and Top-10 accuracy. 2/3H: classes with 2/3 hops away from ImageNet1K Y tr , M500/M1K/M5K: 500/1K/5K most populated classes, L500/L1K/L5K: 500/1K/5K least populated classes, All: Remaining 20K classes.     they are able to predict labels well both in train and test classes.\n\nVisualizing Method Ranking. Similar to the analysis in the previous section that was conducted for zero-shot learning setting, we rank the 13 methods, i.e. [1], [13], [11], [9], [10], [14], [12], [15], [7], [30], [33], [41], based on their results obtained on SUN, CUB, AWA1, AWA2 and aPY. The performance is measured on seen classes, unseen classes and the Harmonic mean of the two.\n\nThe rank matrix of test classes, i.e. Figure 7 top left, shows that highest ranked methods,i.e. ALE, DEVISE, SJE, although overall the absolute accuracy numbers are lower ( Table 6). Note that in Figure 4 GFZSL ranked highest which shows that GFZSL is not as strong for GZSL task. The rank matrix of the harmonic mean shows the same trend. However, the rank matrix of training classes, i.e. Figure 7 top right, shows that models that learn intermediate attribute classifiers perform well for the images that come from training classes. However, these models typically do not lead to a high accuracy for the images that belong to unseen classes as shown in Table 6. This eventually makes the harmonic mean, i.e. the overall accuracy on both training and test classes, lower. These results clearly suggest that one should not only optimize for test class accuracy but also for training class accuracy while evaluating generalized zero-shot learning.\n\nOur final observation from Figure 7 is that CMT* is better than CMT in all cases which supports the argument that a simple novelty detection scheme helps to improve results. However, it is important to note that the proposed novelty detection mechanism uses more supervision than classic zero-shot learning models. Although the label of test classes is not used, whether the sample comes from a seen or unseen class is an additional supervision.\n\n\nTransductive (Generalized) Zero-Shot Learning\n\nIn contrast to previous zero-shot learning approaches that learn only with data from training classes, transductive approaches use unlabaled images from test classes. In this section, we evaluate three state-of-the-art transductive ZSL approaches, i.e. DSRL [71], GFZSL-tran [41], and ALE-tran [30]. Similar to the previous section, we evaluate those approaches on our proposed splits in both zero-shot learning where test time search space is composed of only unseen classes and generalized zero-shot learning where it contains both seen and unseen classes. The performance is perclass averaged top-1 accuracy.\n\nOur transductive learning results are presented in Figure 8. We observe that in ZSL setting, transductive learning leads to accuracy improvement, e.g. ALE-tran and GFZSL-tran outperforms ALE and GFZSL respectively in almost all cases. In particular, on AWA2, GFZSL-tran achieves 78.6%, significantly improving GFZSL (63.8%). On APY, ALE-tran obtains 45.5% and significantly improves ALE (37.1%) as well. Moreover, GFZSL-tran outperforms ALE-tran and DSRL on SUN, AWA1 and AWA2. However, ALE-tran performs the best on CUB and APY. In GZSL setting we observe a different trend, i.e. transductive learning does not improve results for ALE in any of the datasets. Although, on AWA1 and AWA2 GFZSL results improve significantly for the transductive learning setting, on other datasets GFZSL model performs poorly both in inductive and in transductive settings.\n\n\nCONCLUSION\n\nIn this work, we evaluated a significant number of state-of-the-art zero-shot learning methods, i.e. [1], [13], [11], [9], [10], [14], [12], [15], [7], [30], [33], [41], [71], on several datasets, i.e. SUN, CUB, AWA1, AWA2, aPY and ImageNet, within a unified evaluation protocol both in zero-shot and generalized zero-shot settings.\n\nOur evaluation showed that generative models and compatibility learning frameworks have an edge over learning independent object or attribute classifiers and also over other hybrid models for the classic zero-shot learning setting. We observed that unlabeled data of unseen classes can further improve the zeroshot learning results, thus it is not fair to compare transductive learning approaches with inductive ones. We discovered that some standard zero-shot dataset splits may treat feature learning disjoint from the training stage as several test classes are included in the ImageNet1K dataset that is used to train the deep neural networks that act as feature extractor. Therefore, we proposed new dataset splits making sure that none of the test classes in none of the datasets belong to ImageNet1K. Moreover, disjoint training and validation class split is a necessary component of parameter tuning in zero-shot learning setting.\n\nIn addition, we introduced a new Animal with Attributes (AWA2) dataset. AWA2 inherits the same 50 classes and attributes annotations from the original Animal with Attributes (AWA1) dataset, but consists of different 37, 322 images with publicly available redistribution license. Our experimental results showed that the 12 methods that we evaluated perform similarly on AWA2 and AWA1. Moreover, our statistical consistency test indicated that AWA1 and AWA2 are compatible with each other.\n\nFinally, including training classes in the search space while evaluating the methods, i.e. generalized zero-shot learning, provides an interesting playground for future research. Although the generalized zero-shot learning accuracy obtained with 13 models compared to their zero-shot learning accuracy is significantly lower, the relative performance comparison of different models remain the same. Having noticed that some models perform well when the test set is composed only of seen classes, while some others perform well when the test set is composed of only of unseen classes, we proposed the Harmonic mean of seen and unseen class accuracy as a unified measure for performance in GZSL setting. The Harmonic mean encourages the models to perform well on both seen and unseen class samples, which is closer to a real world setting. In summary, our work extensively evaluated the good and bad aspects of zero-shot learning while sanitizing the ugly ones. \n\nFig. 3 :\n3Robustness of 10 methods evaluated on SUN, CUB, AWA1, aPY using 3 validation set splits (results are on the same test split). Top: original split, Bottom: proposed split (Image embeddings = ResNet). We measure top-1 accuracy in %.\n\nFig. 4 :\n4Ranking 12 models by setting parameters on three validation splits on the standard (SS, left) and proposed (PS, right) setting. Element (i, j) indicates number of times model i ranks at jth over all 4 \u00d7 3 observations. Models are ordered by their mean rank (displayed in brackets).\n\nFig. 5 :\n5Zero-Shot Learning experiments on Imagenet, measuring Top-1, Top-5 and Top-10 accuracy. 2/3 H = classes with 2/3 hops away from ImageNet1K training classes (Y tr ), M500/M1K/M5K denote 500, 1K and 5K most populated classes, L500/L1K/L5K denote 500, 1K and 5K least populated classes, All = The remaining 20K categories of ImageNet.\n\nFig. 7 :\n7Ranking 13 models on the proposed split (PS) in generalized zero-shot learning setting. Top-Left: Top-1 accuracy (T1) is measured on unseen classes (ts), Top-Right: T1 is measured on seen classes (tr), Bottom: T1 is measured on Harmonic mean (H).\n\nFig. 8 :\n8Zero-shot (left) and generalized zero-shot learning (right) results in the transductive learning setting on our Proposesd Split.\n\n\nXian received his B.Sc. degree in software engineering from Beijing Institute of Technology, China, in 2013 and his M.Sc. degree with honors in computer science from Saarland University, Germany in 2016. He is currently a PhD student in the Computer Vision and Multimodal Computing group at the Max-Planck Institute for Informatics in Saarbr\u00fccken, Germany. His research interests include machine learning and computer vision. Christoph Lampert received the PhD degree in mathematics from the University of Bonn in 2003. In 2010 he joined the Institute of Science and Technology Austria (IST Austria) first as an Assistant Professor and since 2015 as a Professor. His research on computer vision and machine learning has won several international and national awards, including the best paper prize at CVPR 2008. In 2012 he was awarded an ERC Starting Grant by the European Research Council. He is an Editor of the International Journal of Computer Vision (IJCV), Action Editor of the Journal for Machine Learning Research (JMLR), and Associate Editor in Chief of the IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). Bernt Schiele received masters degree in computer science from the University of Karlsruhe and INP Grenoble in 1994 and the PhD degree from INP Grenoble in computer vision in 1997. He was a postdoctoral associate and visiting assistant professor with MIT between 1997 and 2000. From 1999 until 2004, he was an assistant professor with ETH Zurich and, from 2004 to 2010, he was a full professor of computer science with TU Darmstadt. In 2010, he was appointed a scientific member of the Max Planck Society and director at the Max Planck Institute for Informatics. Since 2010, he has also been a professor at Saarland University. His main interests are computer vision, perceptual computing, statistical learning methods, wearable computers, and integration of multimodal sensor data. He is particularly interested in developing methods which work under real world conditions. Zeynep Akata holds a MSc degree from RWTH Aachen in 2010 and a PhD degree from INRIA of Universit\u00e9 de Grenoble in 2014. She was a postdoctoral researcher in Max Planck Institute for Informatics between 2014-2017 and a visiting researcher at UC Berkeley in 2016-2017. In 2014, she received Lise-Meitner Award for Excellent Women in Computer Science from Max Planck Society. She is currently an assistant professor at the University of Amsterdam, Scientific Manager of the Delta Lab and a Senior Researcher at Max Planck Institute for Informatics. Her research interests include machine learning combined with vision and language for the task of explainable artificial intelligence (XAI).\n\n\nXian and Bernt Schiele are with Max Planck Institute for Informatics, Germany. \u2022 Christoph H. Lampert is with IST Austria (Institute of Science and Technology, Austria). \u2022 Zeynep Akata is with University of Amsterdam, Netherlands and with Max Planck Institute for Informatics, Germany.\n\n\n18] is a small-scale coarsegrained dataset with 64 attributes. Among the total number of 32 classes, 20 Pascal classes are used for training (we randomly select 5 for validation) and 12 Yahoo classes are used for testing. The original Animals with Attributes (AWA1)[1] is a coarse-grained dataset that is medium-scale in terms of the number of images, i.e. 30, 475 and small-scale in terms of number of classes, i.e. 50 classes.[1] introduces a standard zero-shot split with 40 classes for training (we randomly select 13 classes for validation) and 10 classes for testing. AWA1 has 85 attributes. Caltech-UCSD-Birds 200-2011 (CUB)[17] is a fine-grained and medium scale dataset with respect to both number of images and number of classes, i.e. 11, 788 images from 200 different types of birds annotated with 312 attributes.[30] introduces the first zero-shot split of CUB with 150 training (50 validation classes) and 50 test classes. SUN[16] is a fine-grained and medium-scale dataset with respect to both number of images and number of classes, i.e. SUN contains 14340 images coming from 717 types of scenes annotated with 102 attributes. Following[1] we use 645 classes of SUN for training (we randomly select 65 classes for validation) and 72 classes for testing.Animals with Attributes2 (AWA2) Dataset. One disadvantage of AWA1 dataset is that the images are not publicly available. As having highly descriptive image features is an important component for zero-shot learning, in order to enable vision research on the objects of the AWA1 dataset, we introduce the Animals with Attributes2 (AWA2) dataset. Following[1], we collect 37, 322 images for the 50 classes of AWA1 dataset from public web sources, i.e. Flickr, Wikipedia, etc., making sure that all images of AWA2 have free-use and redistribution licenses and they do notFig. 2: Comparing AWA1 [1] and our AWA2 in terms of number of images (Left) and t-SNE embedding of the image features (the embedding is learned on AWA1 and AWA2 simultaneously, therefore the figures are comparable). AWA2 follows a similar distribution as AWA1 and it contains more examples. overlap with images of the original Animal with Attributes dataset. The AWA2 dataset uses the same 50 animal classes as AWA1 dataset, similarly the 85 binary and continuous class attributes are common. In total, AWA2 has 37, 322 images compared to 30, 475 images of AWA1. On average, each class includes 746 images where the least populated class, i.e. mole, has 100 and the most populated class, i.e. horse has 1645 examples. Some example images from polar bear, zebra, otter and tiger classes along with sample attributes from our AWA2 dataset are shown in\n\n\n. While in SS and PS no image from test classes is present at training time, at test time our PS includes images from training classes. We designed the PS this 1. Our benchmark is in: http://www.mpi-inf.mpg.de/zsl-benchmarkNumber of Classes \n\nNumber of Images \nAt Training Time \nAt Evaluation Time \nSS \nPS \nSS \nPS \nDataset \nSize \nGranularity \nAtt \nY \nY tr \nY ts \nTotal \nY tr \nY ts \nY tr \nY ts \nY tr \nY ts \nY tr \nY ts \nSUN [16] \nmedium \nfine \n102 717 580 + 65 \n72 \n14340 12900 \n0 \n10320 \n0 \n0 \n1440 2580 1440 \nCUB [17] \nmedium \nfine \n312 200 \n100 + 50 \n50 \n11788 \n8855 \n0 \n7057 \n0 \n0 \n2933 1764 2967 \nAWA1 [1] medium \ncoarse \n85 \n50 \n27 + 13 \n10 \n30475 24295 \n0 \n19832 \n0 \n0 \n6180 4958 5685 \nAWA2 \nmedium \ncoarse \n85 \n50 \n27 + 13 \n10 \n37322 30337 \n0 \n23527 \n0 \n0 \n6985 5882 7913 \naPY [18] \nsmall \ncoarse \n64 \n32 \n15 + 5 \n12 \n15339 12695 \n0 \n5932 \n0 \n0 \n2644 1483 7924 \n\n\n\nTABLE 1 :\n1Statistics for SUN\n\n\nAs with our proposed split at test time we have accessSUN \nCUB \nAWA1 \naPY \nModel \nR \nO \nR \nO \nR \nO \nR \nO \nDAP [1] \n22.1 22.2 \n\u2212 \n\u2212 \n41.4 41.4 19.1 19.1 \nSSE [13] \n83.0 82.5 44.2 30.4 64.9 76.3 45.7 46.2 \nLATEM [11] \n\u2212 \n\u2212 \n45.1 45.5 71.2 71.9 \n\u2212 \n\u2212 \nSJE [9] \n\u2212 \n\u2212 \n50.1 50.1 67.2 66.7 \n\u2212 \n\u2212 \nESZSL [10] \n64.3 65.8 \n\u2212 \n\u2212 \n48.0 49.3 14.3 15.1 \nSYNC [14] \n62.8 62.8 53.4 53.4 69.7 69.7 \n\u2212 \n\u2212 \nSAE [33] \n\u2212 \n\u2212 \n\u2212 \n\u2212 \n84.7 84.7 \n\u2212 \n\u2212 \nGFZSL [41] \n86.5 86.5 56.6 56.5 80.4 80.8 \n\u2212 \n\u2212 \nGFZSL-tran [41] 87.0 87.0 63.8 63.7 94.9 94.3 \n\u2212 \n\u2212 \nDSRL [71] \n86.0 85.4 57.6 57.1 87.7 87.2 47.8 51.3 \n\n\n\nTABLE 2 :\n2Reproducing zero-shot results with methods that have a public implementation: O = Original results, R = Reproduced using provided image features and code. We measure top-1 accuracy in %. \u2212: image features are not provided in the original paper for this dataset. Top: ZSL, Bottom: transductive ZSL.to some images from training classes, after having computed the average per-class top-1 accuracy on training and test classes, we compute the harmonic mean of training and test accuracies:\n\nTABLE 3 :\n3Zero-shot learning results on SUN, CUB, AWA1, AWA2 and aPY using SS = Standard Split, PS = Proposed Split with ResNet features. The results report top-1 accuracy in %.\n\n\nwith deep ResNet features. DAP [1] uses hand-crafted image features and thus reproduced results with those features are significantly lower than the results with deep features (22.1% vs 38.9%). When we investigate the results in detail, we noticed two irregularities with reported results on SUN. First, SSE [13] and ESZSL [10] report results on a test split with 10 classes whereas the standard split of SUN contains 72 test classes (74.5% vs 54.5% with SSE [13] and 64.3% vs 57.3% with ESZSL\n\nTABLE 4 :\n4Cross-dataset evaluation over AWA1 and AWA2 in \nzero-shot learning setting on the Proposed Splits: Left of the colon \nindicates the training set and right of the colon indicates the test \nset, e.g. AWA1:AWA2 means that the model is trained on the train \nset of AWA1 and evaluated on the test set of AWA2. We measure \ntop-1 accuracy in %. \n\n\n\nTable\n. 3. Under the Standard Splits (SS), \nSAE [33] is the best performing method on both AWA1 (80.6%) \nand AWA2 (80.7%). Similarly, for most of the methods, the results \non AWA1 are close to those on AWA2, for instance, DAP obtains \n57.1% on AWA1 and 58.7% on AWA2, SSE obtains 68.8% on \nAWA1 and 67.5% AWA2, etc. The results under the Proposed \nSplits (PS) are also consistent across AWA1 and AWA2. For 8 \nout of 12 methods, the performance difference between AWA1 \nHierarchy \nMost Populated \nLeast Populated \nAll \nMethod \n2 H \n3 H \n500 \n1K \n5K \n500 \n1K \n5K \n20K \nCONSE [15] \n7.63 \n2.18 \n12.33 \n8.31 \n3.22 \n3.53 \n2.69 \n1.05 \n0.95 \nCMT [12] \n2.88 \n0.67 \n5.10 \n3.04 \n1.04 \n1.87 \n1.08 \n0.33 \n0.29 \nLATEM [11] \n5.45 \n1.32 \n10.81 \n6.63 \n1.90 \n4.53 \n2.74 \n0.76 \n0.50 \nALE [30] \n5.38 \n1.32 \n10.40 \n6.77 \n2.00 \n4.27 \n2.85 \n0.79 \n0.50 \nDEVISE [7] \n5.25 \n1.29 \n10.36 \n6.68 \n1.94 \n4.23 \n2.86 \n0.78 \n0.49 \nSJE [9] \n5.31 \n1.33 \n9.88 \n6.53 \n1.99 \n4.93 \n2.93 \n0.78 \n0.52 \nESZSL [10] \n6.35 \n1.51 \n11.91 \n7.69 \n2.34 \n4.50 \n3.23 \n0.94 \n0.62 \nSYNC [14] \n9.26 2.29 \n15.83 10.75 3.42 \n5.83 3.52 1.26 \n0.96 \nSAE [33] \n4.89 \n1.26 \n9.96 \n6.57 \n2.09 \n2.50 \n2.17 \n0.72 \n0.56 \nGFZSL [41] \n1.45 \n\u2212\u2212 \n2.01 \n1.35 \n\u2212\u2212 \n1.40 \n1.11 \n0.13 \n\u2212\u2212 \n\n\n\nTABLE\n\n\nTABLE 6 :\n6Generalized Zero-Shot Learning on Proposed Split (PS) measuring ts = Top-1 accuracy on Y ts , tr=Top-1 accuracy on Y tr , \nH = harmonic mean (CMT*: CMT with novelty detection). We measure top-1 accuracy in %. \n\n\n\nAttribute-based classification for zero-shot visual object categorization. C Lampert, H Nickisch, S Harmeling, TPAMI. 1012C. Lampert, H. Nickisch, and S. Harmeling, \"Attribute-based classifica- tion for zero-shot visual object categorization,\" in TPAMI, 2013. 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\nZero-data learning of new tasks. H Larochelle, D Erhan, Y Bengio, AAAI. H. Larochelle, D. Erhan, and Y. Bengio, \"Zero-data learning of new tasks,\" in AAAI, 2008. 1\n\nEvaluating knowledge transfer and zero-shot learning in a large-scale setting. M Rohrbach, M Stark, B Schiele, CVPR. 6M. Rohrbach, M. Stark, and B.Schiele, \"Evaluating knowledge transfer and zero-shot learning in a large-scale setting,\" in CVPR, 2011. 1, 3, 6\n\nAttribute-based transfer learning for object categorization with zero or one training example. X Yu, Y Aloimonos, ECCV. 1X. Yu and Y. Aloimonos, \"Attribute-based transfer learning for object categorization with zero or one training example,\" in ECCV, 2010. 1\n\nMatrix trifactorization with manifold regularizations for zero-shot learning. X Xu, Y Yang, D Zhang, H T Shen, J Song, CVPR. X. Xu, Y. Yang, D. Zhang, H. T. Shen, and J. Song, \"Matrix tri- factorization with manifold regularizations for zero-shot learning,\" in CVPR, 2017. 1\n\nLow-rank embedded ensemble semantic dictionary for zero-shot learning. Z Ding, M Shao, Y Fu, CVPR. Z. Ding, M. Shao, and Y. Fu, \"Low-rank embedded ensemble semantic dictionary for zero-shot learning,\" in CVPR, 2017. 1\n\nDevise: A deep visual-semantic embedding model. A Frome, G S Corrado, J Shlens, S Bengio, J Dean, M A Ranzato, T Mikolov, NIPS. 1012A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A. Ranzato, and T. Mikolov, \"Devise: A deep visual-semantic embedding model,\" in NIPS, 2013. 1, 2, 3, 8, 9, 10, 11, 12\n\nLabel embedding for attribute-based classification. Z Akata, F Perronnin, Z Harchaoui, C Schmid, CVPR. 6Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid, \"Label embedding for attribute-based classification,\" in CVPR, 2013. 1, 6\n\nEvaluation of output embeddings for fine-grained image classification. Z Akata, S Reed, D Walter, H Lee, B Schiele, CVPR. 1012Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele, \"Evaluation of output embeddings for fine-grained image classification,\" in CVPR, 2015. 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12\n\nAn embarrassingly simple approach to zero-shot learning. B Romera-Paredes, P H Torr, ICML. 1012B. Romera-Paredes and P. H. Torr, \"An embarrassingly simple approach to zero-shot learning,\" ICML, 2015. 1, 2, 3, 4, 7, 8, 9, 10, 11, 12\n\nLatent embeddings for zero-shot classification. Y Xian, Z Akata, G Sharma, Q Nguyen, M Hein, B Schiele, CVPR. 1012Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and B. Schiele, \"Latent embeddings for zero-shot classification,\" in CVPR, 2016. 1, 2, 3, 4, 7, 8, 9, 10, 11, 12\n\nZero-shot learning through cross-modal transfer. R Socher, M Ganjoo, C D Manning, A Ng, NIPS. 1012R. Socher, M. Ganjoo, C. D. Manning, and A. Ng, \"Zero-shot learning through cross-modal transfer,\" in NIPS, 2013. 1, 2, 3, 4, 7, 8, 9, 10, 11, 12\n\nZero-shot learning via semantic similarity embedding. Z Zhang, V Saligrama, ICCV. 1012Z. Zhang and V. Saligrama, \"Zero-shot learning via semantic similarity embedding,\" in ICCV, 2015. 1, 2, 4, 7, 8, 9, 10, 11, 12\n\nSynthesized classifiers for zero-shot learning. S Changpinyo, W.-L Chao, B Gong, F Sha, CVPR. 1012S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha, \"Synthesized classifiers for zero-shot learning,\" in CVPR, 2016. 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\nZero-shot learning by convex combination of semantic embeddings. M Norouzi, T Mikolov, S Bengio, Y Singer, J Shlens, A Frome, G Corrado, J Dean, ICLR. 1012M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. Corrado, and J. Dean, \"Zero-shot learning by convex combination of semantic embeddings,\" in ICLR, 2014. 1, 2, 4, 8, 9, 10, 11, 12\n\nSun attribute database: Discovering, annotating, and recognizing scene attributes. G Patterson, J Hays, CVPR. 7G. Patterson and J. Hays, \"Sun attribute database: Discovering, annotat- ing, and recognizing scene attributes,\" in CVPR, 2012. 1, 5, 7\n\nCaltech-UCSD Birds 200. P Welinder, S Branson, T Mita, C Wah, F Schroff, S Belongie, P Perona, Caltech, Tech. Rep. CNS-TR. 7P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, \"Caltech-UCSD Birds 200,\" Caltech, Tech. Rep. CNS-TR- 2010-001, 2010. 1, 5, 7\n\nDescribing objects by their attributes. A Farhadi, I Endres, D Hoiem, D Forsyth, CVPR. 67A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth, \"Describing objects by their attributes,\" CVPR, 2009. 1, 2, 5, 6, 7\n\nImageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. 56J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"ImageNet: A Large-Scale Hierarchical Image Database,\" in CVPR, 2009. 1, 5, 6\n\nDistinctive image features from scale-invariant keypoints. D G Lowe, IJCV. 602D. G. Lowe, \"Distinctive image features from scale-invariant keypoints,\" IJCV, vol. 60, no. 2, pp. 91-110, 2004. 1\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, ICML. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, \"Decaf: A deep convolutional activation feature for generic visual recognition.\" in ICML, 2014. 1\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014. 1\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 16K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in CVPR, 2016. 1, 6\n\nRecovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning. Z Al-Halah, M Tapaswi, R Stiefelhagen, CVPR. 2Z. Al-Halah, M. Tapaswi, and R. Stiefelhagen, \"Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning,\" in CVPR, 2016. 2\n\nZero-shot recognition with unreliable attributes. D Jayaraman, K Grauman, NIPS. 2D. Jayaraman and K. Grauman, \"Zero-shot recognition with unreliable attributes,\" in NIPS, 2014. 2\n\nOnline incremental attribute-based zero-shot learning. P Kankuekul, A Kawewong, S Tangruamsub, O Hasegawa, CVPR. 2P. Kankuekul, A. Kawewong, S. Tangruamsub, and O. Hasegawa, \"On- line incremental attribute-based zero-shot learning,\" in CVPR, 2012. 2\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, NIPS. 326T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Dis- tributed representations of words and phrases and their compositionality,\" in NIPS, 2013. 2, 3, 6\n\nTransductive multiview zero-shot learning. Y Fu, T M Hospedales, T Xiang, S Gong, TPAMI. 23Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong, \"Transductive multi- view zero-shot learning,\" TPAMI, 2015. 2, 3\n\nZero-shot learning with semantic output codes. M Palatucci, D Pomerleau, G E Hinton, T M Mitchell, NIPS. 2M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M. Mitchell, \"Zero-shot learning with semantic output codes,\" in NIPS, 2009. 2\n\nLabel-embedding for image classification. Z Akata, F Perronnin, Z Harchaoui, C Schmid, TPAMI. 1012Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid, \"Label-embedding for image classification,\" TPAMI, 2016. 2, 3, 4, 5, 6, 8, 9, 10, 11, 12\n\nLess is more: Zeroshot learning from online textual documents with noise suppression. R Qiao, L Liu, C Shen, A Van Den, Hengel, CVPR. 23R. Qiao, L. Liu, C. Shen, and A. van den Hengel, \"Less is more: Zero- shot learning from online textual documents with noise suppression,\" in CVPR, 2016. 2, 3\n\nImproving semantic embedding consistency by metric learning for zero-shot classiffication. M Bucher, S Herbin, F Jurie, ECCV. M. Bucher, S. Herbin, and F. Jurie, \"Improving semantic embedding consistency by metric learning for zero-shot classiffication,\" in ECCV, 2016. 2\n\nSemantic autoencoder for zero-shot learning. E Kodirov, T Xiang, S Gong, CVPR. 1012E. Kodirov, T. Xiang, and S. Gong, \"Semantic autoencoder for zero-shot learning,\" in CVPR, 2017. 2, 3, 4, 7, 8, 9, 10, 11, 12\n\nPredicting deep zero-shot convolutional neural networks using textual descriptions. J Ba, K Swersky, S Fidler, ICCV. 23J. Lei Ba, K. Swersky, S. Fidler et al., \"Predicting deep zero-shot convolutional neural networks using textual descriptions,\" in ICCV, 2015. 2, 3\n\nLearning a deep embedding model for zero-shot learning. L Zhang, T Xiang, S Gong, CVPR. L. Zhang, T. Xiang, and S. Gong, \"Learning a deep embedding model for zero-shot learning,\" in CVPR, 2017. 2\n\nPredicting visual exemplars of unseen classes for zero-shot learning. S Changpinyo, W.-L Chao, F Sha, arXiv:1605.08151arXiv preprintS. Changpinyo, W.-L. Chao, and F. Sha, \"Predicting visual exemplars of unseen classes for zero-shot learning,\" arXiv preprint arXiv:1605.08151, 2016. 2\n\nZero-shot learning via joint semantic similarity embedding. Z Zhang, V Saligrama, CVPR. Z. Zhang and V. Saligrama, \"Zero-shot learning via joint semantic similarity embedding,\" in CVPR, 2016. 2\n\nZero-shot object recognition by semantic manifold distance. Z Fu, T Xiang, E Kodirov, S Gong, CVPR. Z. Fu, T. Xiang, E. Kodirov, and S. Gong, \"Zero-shot object recognition by semantic manifold distance,\" in CVPR, June 2015. 2\n\nMulti-cue zero-shot learning with strong supervision. Z Akata, M Malinowski, M Fritz, B Schiele, CVPR. 2Z. Akata, M. Malinowski, M. Fritz, and B. Schiele, \"Multi-cue zero-shot learning with strong supervision,\" in CVPR, 2016. 2\n\nFrom zero-shot learning to conventional supervised classification: Unseen visual data synthesis. Y Long, L Liu, L Shao, F Shen, G Ding, J Han, CVPR. 2Y. Long, L. Liu, L. Shao, F. Shen, G. Ding, and J. Han, \"From zero-shot learning to conventional supervised classification: Unseen visual data synthesis,\" in CVPR, 2017. 2\n\nA simple exponential family framework for zero-shot learning. V K Verm, P Rai, ECML. 1012V. K. Verm and P. Rai, \"A simple exponential family framework for zero-shot learning,\" in ECML, 2017. 2, 5, 7, 8, 9, 10, 11, 12\n\nZero-shot learning with generative latent prototype model. Y Li, D Wang, arXiv:1705.09474arXiv preprintY. Li and D. Wang, \"Zero-shot learning with generative latent prototype model,\" arXiv preprint arXiv:1705.09474, 2017. 2\n\nGaussian visual-linguistic embedding for zero-shot recognition. T Mukherjee, T Hospedales, EMNLP. 2T. Mukherjee and T. Hospedales, \"Gaussian visual-linguistic embedding for zero-shot recognition,\" in EMNLP, 2016. 2\n\nTransfer learning in a transductive setting. M Rohrbach, S Ebert, B Schiele, NIPS. M. Rohrbach, S. Ebert, and B. Schiele, \"Transfer learning in a transduc- tive setting,\" in NIPS, 2013. 3\n\nUnsupervised domain adaptation for zero-shot learning. E Kodirov, T Xiang, Z Fu, S Gong, ICCV. E. Kodirov, T. Xiang, Z. Fu, and S. Gong, \"Unsupervised domain adaptation for zero-shot learning,\" in ICCV, 2015. 3\n\nSemi-supervised zero-shot classification with label representation learning. X Li, Y Guo, D Schuurmans, ICCV. X. Li, Y. Guo, and D. Schuurmans, \"Semi-supervised zero-shot classifi- cation with label representation learning,\" in ICCV, 2015. 3\n\nMax-margin zero-shot learning for multi-class classification. X Li, Y Guo, AISTATS. X. Li and Y. Guo, \"Max-margin zero-shot learning for multi-class classification.\" in AISTATS, 2015. 3\n\nSemi-supervised vocabulary-informed learning. Y Fu, L Sigal, CVPR. Y. Fu and L. Sigal, \"Semi-supervised vocabulary-informed learning,\" in CVPR, 2016. 3\n\nZero-shot recognition via structured prediction. Z Zhang, V Saligrama, CVPR. Z. Zhang and V. Saligrama, \"Zero-shot recognition via structured predic- tion,\" in CVPR, 2016. 3\n\nCosta: Co-occurrence statistics for zero-shot classification. T Mensink, E Gavves, C G Snoek, CVPR. T. Mensink, E. Gavves, and C. G. Snoek, \"Costa: Co-occurrence statistics for zero-shot classification,\" in CVPR, 2014. 3\n\nWhat helps where-and why? semantic relatedness for knowledge transfer. M Rohrbach, M Stark, G Szarvas, I Gurevych, B Schiele, CVPR. 3M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele, \"What helps where-and why? semantic relatedness for knowledge transfer,\" in CVPR, 2010. 3\n\nLearning deep representations of fine-grained visual descriptions. S Reed, Z Akata, H Lee, B Schiele, CVPR. S. Reed, Z. Akata, H. Lee, and B. Schiele, \"Learning deep representations of fine-grained visual descriptions,\" in CVPR, 2016. 3\n\nWrite a classifier: Zero-shot learning using purely textual descriptions. M Elhoseiny, B Saleh, A Elgammal, ICCV. M. Elhoseiny, B. Saleh, and A. Elgammal, \"Write a classifier: Zero-shot learning using purely textual descriptions,\" in ICCV, 2013. 3\n\nZero-shot learning via visual abstraction. S Antol, C L Zitnick, D Parikh, European Conference on Computer Vision. SpringerS. Antol, C. L. Zitnick, and D. Parikh, \"Zero-shot learning via visual abstraction,\" in European Conference on Computer Vision. Springer, 2014, pp. 401-416. 3\n\nMetric learning for large scale image classification: Generalizing to new classes at near-zero cost. T Mensink, J Verbeek, F Perronnin, G Csurka, Computer Vision-ECCV 2012. T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka, \"Metric learning for large scale image classification: Generalizing to new classes at near-zero cost,\" Computer Vision-ECCV 2012, pp. 488-501, 2012. 3\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C D Manning, EMNLP. 3J. Pennington, R. Socher, and C. D. Manning, \"Glove: Global vectors for word representation,\" in EMNLP, 2014. 3\n\nWordnet: a lexical database for english. G A Miller, http:/doi.acm.org/10.1145/219717.219748CACM. 386G. A. Miller, \"Wordnet: a lexical database for english,\" CACM, vol. 38, pp. 39-41, 1995. [Online]. Available: http://doi.acm.org/10. 1145/219717.219748 3, 6\n\nGaze embeddings for zero-shot image classification. N Karessli, Z Akata, B Schiele, A Bulling, 2017. 3IEEE Computer Vision and Pattern Recognition. N. Karessli, Z. Akata, B. Schiele, and A. Bulling, \"Gaze embeddings for zero-shot image classification,\" in IEEE Computer Vision and Pattern Recognition (CVPR), 2017. 3\n\nTowards open set recognition. W J Scheirer, A Rocha, A Sapkota, T E Boult, TPAMI. 363W. J. Scheirer, A. Rocha, A. Sapkota, and T. E. Boult, \"Towards open set recognition,\" TPAMI, vol. 36, 2013. 3\n\nMulti-class open set recognition using probability of inclusion. L Jain, W Scheirer, T Boult, ECCV. L. Jain, W. Scheirer, and T. Boult, \"Multi-class open set recognition using probability of inclusion,\" in ECCV, 2014. 3\n\nOnline collaborative learning for open-vocabulary visual classifiers. H Zhang, X Shang, W Yang, H Xu, H Luan, T.-S Chua, CVPR. H. Zhang, X. Shang, W. Yang, H. Xu, H. Luan, and T.-S. Chua, \"Online collaborative learning for open-vocabulary visual classifiers,\" in CVPR, 2016. 3\n\nTowards open set deep networks. A Bendale, T E Boult, CVPR. A. Bendale and T. E. Boult, \"Towards open set deep networks,\" in CVPR, 2016. 3\n\nAn empirical study and analysis of generalized zero-shot learning for object recognition in the wild. W.-L Chao, S Changpinyo, B Gong, F Sha, ECCV. W.-L. Chao, S. Changpinyo, B. Gong, and F. Sha, \"An empirical study and analysis of generalized zero-shot learning for object recognition in the wild,\" in ECCV, 2016. 3\n\nOptimizing search engines using clickthrough data. T Joachims, KDD. ACMT. Joachims, \"Optimizing search engines using clickthrough data,\" in KDD. ACM, 2002. 3\n\nRanking with ordered weighted pairwise classification. N Usunier, D Buffoni, P Gallinari, ICML. N. Usunier, D. Buffoni, and P. Gallinari, \"Ranking with ordered weighted pairwise classification,\" in ICML, 2009. 3\n\nWsabie: Scaling up to large vocabulary image annotation. J Weston, S Bengio, N Usunier, IJCAI. J. Weston, S. Bengio, and N. Usunier, \"Wsabie: Scaling up to large vocabulary image annotation,\" in IJCAI, 2011. 3\n\nLarge margin methods for structured and interdependent output variables. I Tsochantaridis, T Joachims, T Hofmann, Y Altun, JMLR. 4I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, \"Large margin methods for structured and interdependent output variables,\" JMLR, 2005. 4\n\nSolution of the matrix equation ax+ xb= c [f4. R H Bartels, G Stewart, Commun. ACM. 159R. H. Bartels and G. Stewart, \"Solution of the matrix equation ax+ xb= c [f4],\" Commun. ACM, vol. 15, no. 9, pp. 820-826, 1972. 4\n\nSemi-supervised learning. O Chapelle, B Scholkopf, A Zien, IEEE Transactions on Neural Networks. 5O. Chapelle, B. Scholkopf, and A. Zien, \"Semi-supervised learning,\" IEEE Transactions on Neural Networks, 2009. 5\n\nLearning with local and global consistency. D Zhou, O Bousquet, T N Lal, J Weston, B Sch\u00f6lkopf, NIPS. D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch\u00f6lkopf, \"Learning with local and global consistency,\" in NIPS, 2004. 5\n\nZero-shot classification with discriminative semantic representation learning. M Ye, Y Guo, CVPR. 812M. Ye and Y. Guo, \"Zero-shot classification with discriminative semantic representation learning,\" in CVPR, 2017. 5, 7, 8, 12\n\nEfficient label propagation. Y Fujiwara, G Irie, ICML. Y. Fujiwara and G. Irie, \"Efficient label propagation,\" in ICML, 2014. 5\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" in CVPR, 2015. 6\n\nLearning deep features for scene recognition using places database. B Zhou, A Lapedriza, J Xiao, A Torralba, A Oliva, NIPS. B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, \"Learning deep features for scene recognition using places database,\" in NIPS, 2014. 8\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. 8C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" CVPR, 2015. 8\n\nAn extension on\"statistical comparisons of classifiers over multiple data sets\"for all pairwise comparisons. S Garcia, F Herrera, JLMR. 99S. Garcia and F. Herrera, \"An extension on\"statistical comparisons of classifiers over multiple data sets\"for all pairwise comparisons,\" JLMR, vol. 9, pp. 2677-2694, 2008. 9\n", "annotations": {"author": "[{\"end\":117,\"start\":84},{\"end\":138,\"start\":118},{\"end\":165,\"start\":139},{\"end\":191,\"start\":166}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":112},{\"end\":137,\"start\":130},{\"end\":164,\"start\":157},{\"end\":190,\"start\":185}]", "author_first_name": "[{\"end\":111,\"start\":104},{\"end\":127,\"start\":118},{\"end\":129,\"start\":128},{\"end\":156,\"start\":151},{\"end\":184,\"start\":178}]", "author_affiliation": null, "title": "[{\"end\":81,\"start\":1},{\"end\":272,\"start\":192}]", "venue": null, "abstract": "[{\"end\":1593,\"start\":386}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1712,\"start\":1709},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1717,\"start\":1714},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1722,\"start\":1719},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1727,\"start\":1724},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1732,\"start\":1729},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1737,\"start\":1734},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3079,\"start\":3076},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3084,\"start\":3081},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3089,\"start\":3086},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3095,\"start\":3091},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3114,\"start\":3110},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3120,\"start\":3116},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3301,\"start\":3298},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3337,\"start\":3333},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3343,\"start\":3339},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3349,\"start\":3345},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3740,\"start\":3736},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3746,\"start\":3742},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3751,\"start\":3748},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3757,\"start\":3753},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3763,\"start\":3759},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3824,\"start\":3821},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4084,\"start\":4080},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4096,\"start\":4092},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4108,\"start\":4104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4808,\"start\":4804},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6389,\"start\":6385},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7187,\"start\":7184},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7193,\"start\":7189},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7199,\"start\":7195},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7205,\"start\":7201},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7211,\"start\":7207},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7561,\"start\":7558},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7776,\"start\":7772},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7952,\"start\":7949},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8310,\"start\":8307},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8322,\"start\":8318},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8414,\"start\":8410},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8541,\"start\":8537},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8849,\"start\":8845},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8960,\"start\":8956},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9074,\"start\":9071},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9246,\"start\":9243},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9347,\"start\":9343},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9511,\"start\":9507},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9560,\"start\":9556},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9707,\"start\":9703},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9936,\"start\":9932},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9988,\"start\":9985},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10116,\"start\":10112},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10236,\"start\":10232},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10327,\"start\":10323},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10431,\"start\":10427},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10625,\"start\":10621},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11106,\"start\":11102},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11264,\"start\":11260},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11455,\"start\":11451},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11461,\"start\":11457},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11467,\"start\":11463},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11473,\"start\":11469},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11486,\"start\":11482},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11616,\"start\":11612},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11956,\"start\":11952},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11962,\"start\":11958},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11968,\"start\":11964},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12036,\"start\":12032},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12187,\"start\":12183},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12340,\"start\":12336},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12651,\"start\":12647},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12657,\"start\":12653},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12663,\"start\":12659},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12669,\"start\":12665},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12675,\"start\":12671},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12681,\"start\":12677},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12687,\"start\":12683},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12807,\"start\":12803},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12860,\"start\":12856},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12912,\"start\":12908},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13002,\"start\":12998},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13008,\"start\":13004},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13014,\"start\":13010},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13478,\"start\":13474},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13484,\"start\":13480},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13489,\"start\":13486},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13495,\"start\":13491},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13500,\"start\":13497},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13506,\"start\":13502},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13512,\"start\":13508},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13518,\"start\":13514},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13524,\"start\":13520},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13530,\"start\":13526},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13614,\"start\":13610},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13766,\"start\":13763},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13844,\"start\":13840},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13856,\"start\":13852},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13883,\"start\":13879},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13901,\"start\":13898},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13939,\"start\":13935},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14110,\"start\":14106},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":14467,\"start\":14463},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14602,\"start\":14598},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14900,\"start\":14897},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14992,\"start\":14988},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15146,\"start\":15142},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15272,\"start\":15268},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15393,\"start\":15390},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":15399,\"start\":15395},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15477,\"start\":15474},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":15599,\"start\":15595},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17122,\"start\":17118},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17167,\"start\":17164},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17208,\"start\":17205},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17650,\"start\":17646},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17663,\"start\":17660},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17675,\"start\":17672},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17769,\"start\":17765},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17782,\"start\":17778},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17947,\"start\":17944},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":18032,\"start\":18028},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18246,\"start\":18242},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18299,\"start\":18295},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":18582,\"start\":18578},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18588,\"start\":18584},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18671,\"start\":18668},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":18768,\"start\":18764},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19038,\"start\":19034},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19542,\"start\":19538},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":19990,\"start\":19986},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20090,\"start\":20086},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20126,\"start\":20122},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20227,\"start\":20223},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20561,\"start\":20560},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20621,\"start\":20617},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21526,\"start\":21523},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21570,\"start\":21567},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21655,\"start\":21651},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21770,\"start\":21767},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22453,\"start\":22450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23097,\"start\":23093},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23153,\"start\":23149},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23193,\"start\":23189},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23326,\"start\":23322},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23696,\"start\":23692},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24181,\"start\":24177},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24719,\"start\":24715},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":25310,\"start\":25306},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":25316,\"start\":25312},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25601,\"start\":25597},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":25607,\"start\":25603},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25639,\"start\":25635},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25696,\"start\":25692},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":26196,\"start\":26192},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26805,\"start\":26801},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":27134,\"start\":27130},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":27620,\"start\":27616},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27641,\"start\":27637},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27942,\"start\":27938},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27974,\"start\":27971},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28035,\"start\":28031},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28045,\"start\":28041},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28105,\"start\":28101},{\"end\":28515,\"start\":28514},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29258,\"start\":29254},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":29433,\"start\":29429},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29971,\"start\":29968},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30894,\"start\":30890},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31024,\"start\":31020},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":31118,\"start\":31114},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31652,\"start\":31648},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31775,\"start\":31771},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31813,\"start\":31809},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31920,\"start\":31917},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32677,\"start\":32674},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32694,\"start\":32691},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33268,\"start\":33264},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33278,\"start\":33274},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33288,\"start\":33285},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33313,\"start\":33309},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34283,\"start\":34279},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37479,\"start\":37476},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":37485,\"start\":37481},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37491,\"start\":37487},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37496,\"start\":37493},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37502,\"start\":37498},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37508,\"start\":37504},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37517,\"start\":37513},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37615,\"start\":37612},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":37626,\"start\":37622},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37638,\"start\":37634},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37654,\"start\":37650},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":37665,\"start\":37661},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37678,\"start\":37674},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37769,\"start\":37765},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37917,\"start\":37914},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37993,\"start\":37989},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38157,\"start\":38153},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38846,\"start\":38843},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38852,\"start\":38848},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38858,\"start\":38854},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38863,\"start\":38860},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38869,\"start\":38865},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":38875,\"start\":38871},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":38881,\"start\":38877},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38887,\"start\":38883},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38909,\"start\":38905},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38914,\"start\":38911},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":38920,\"start\":38916},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39052,\"start\":39048},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":39137,\"start\":39133},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":39206,\"start\":39202},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39440,\"start\":39436},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39602,\"start\":39598},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39839,\"start\":39835},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":39873,\"start\":39869},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41018,\"start\":41014},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41104,\"start\":41101},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41150,\"start\":41146},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41226,\"start\":41222},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41287,\"start\":41283},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41311,\"start\":41308},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":41342,\"start\":41339},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":41833,\"start\":41830},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41839,\"start\":41835},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":41845,\"start\":41841},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":41850,\"start\":41847},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41856,\"start\":41852},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41862,\"start\":41858},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41868,\"start\":41864},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41874,\"start\":41870},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":41879,\"start\":41876},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":41885,\"start\":41881},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41891,\"start\":41887},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41897,\"start\":41893},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":43402,\"start\":43398},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":45477,\"start\":45474},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":45486,\"start\":45483},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":45500,\"start\":45496},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":45523,\"start\":45520},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":45616,\"start\":45613},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46213,\"start\":46210},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46219,\"start\":46215},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":46225,\"start\":46221},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":46230,\"start\":46227},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46236,\"start\":46232},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":46242,\"start\":46238},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":46248,\"start\":46244},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":46254,\"start\":46250},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46259,\"start\":46256},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":46265,\"start\":46261},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":46271,\"start\":46267},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":47898,\"start\":47894},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47903,\"start\":47900},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47909,\"start\":47905},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":47915,\"start\":47911},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":47921,\"start\":47917},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47927,\"start\":47923},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47932,\"start\":47929},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":47938,\"start\":47934},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":47944,\"start\":47940},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47950,\"start\":47946},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48051,\"start\":48047},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":48180,\"start\":48176},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48601,\"start\":48597},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49037,\"start\":49033},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":50692,\"start\":50688},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":52559,\"start\":52557},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":52974,\"start\":52970},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":55015,\"start\":55012},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":55021,\"start\":55017},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":55027,\"start\":55023},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":55032,\"start\":55029},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":55038,\"start\":55034},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":55044,\"start\":55040},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":55050,\"start\":55046},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":55056,\"start\":55052},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":55061,\"start\":55058},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":55067,\"start\":55063},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":55073,\"start\":55069},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":55079,\"start\":55075},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":56947,\"start\":56943},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":56964,\"start\":56960},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":56983,\"start\":56979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":58272,\"start\":58269},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":58278,\"start\":58274},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":58284,\"start\":58280},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":58289,\"start\":58286},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":58295,\"start\":58291},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58301,\"start\":58297},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":58307,\"start\":58303},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":58313,\"start\":58309},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":58318,\"start\":58315},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":58324,\"start\":58320},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":58330,\"start\":58326},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":58336,\"start\":58332},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":58342,\"start\":58338},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":65163,\"start\":65161},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":65429,\"start\":65426},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":65592,\"start\":65589},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":65796,\"start\":65792},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":65989,\"start\":65985},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":66104,\"start\":66100},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":66315,\"start\":66312},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":66785,\"start\":66782}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61133,\"start\":60892},{\"attributes\":{\"id\":\"fig_2\"},\"end\":61426,\"start\":61134},{\"attributes\":{\"id\":\"fig_3\"},\"end\":61769,\"start\":61427},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62027,\"start\":61770},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62167,\"start\":62028},{\"attributes\":{\"id\":\"fig_8\"},\"end\":64870,\"start\":62168},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":65158,\"start\":64871},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":67845,\"start\":65159},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":68717,\"start\":67846},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":68748,\"start\":68718},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":69334,\"start\":68749},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69832,\"start\":69335},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":70012,\"start\":69833},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":70508,\"start\":70013},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":70861,\"start\":70509},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":72077,\"start\":70862},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":72085,\"start\":72078},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":72309,\"start\":72086}]", "paragraph": "[{\"end\":2643,\"start\":1609},{\"end\":4878,\"start\":2645},{\"end\":6101,\"start\":4889},{\"end\":7133,\"start\":6103},{\"end\":8713,\"start\":7150},{\"end\":9848,\"start\":8715},{\"end\":10947,\"start\":9850},{\"end\":11774,\"start\":10949},{\"end\":12463,\"start\":11776},{\"end\":13074,\"start\":12465},{\"end\":14232,\"start\":13076},{\"end\":15312,\"start\":14234},{\"end\":16206,\"start\":15314},{\"end\":16539,\"start\":16228},{\"end\":16651,\"start\":16583},{\"end\":16727,\"start\":16653},{\"end\":17052,\"start\":16767},{\"end\":17292,\"start\":17086},{\"end\":18033,\"start\":17325},{\"end\":18236,\"start\":18099},{\"end\":18377,\"start\":18238},{\"end\":18488,\"start\":18379},{\"end\":18549,\"start\":18490},{\"end\":18662,\"start\":18551},{\"end\":18769,\"start\":18664},{\"end\":18838,\"start\":18771},{\"end\":19026,\"start\":18840},{\"end\":19191,\"start\":19028},{\"end\":19532,\"start\":19229},{\"end\":19808,\"start\":19534},{\"end\":20023,\"start\":19865},{\"end\":20215,\"start\":20060},{\"end\":20273,\"start\":20217},{\"end\":20611,\"start\":20321},{\"end\":20755,\"start\":20613},{\"end\":21432,\"start\":20800},{\"end\":21761,\"start\":21480},{\"end\":21963,\"start\":21763},{\"end\":22444,\"start\":22015},{\"end\":22692,\"start\":22446},{\"end\":23039,\"start\":22738},{\"end\":23316,\"start\":23057},{\"end\":23434,\"start\":23318},{\"end\":23684,\"start\":23471},{\"end\":23770,\"start\":23686},{\"end\":23970,\"start\":23811},{\"end\":24170,\"start\":24015},{\"end\":24406,\"start\":24172},{\"end\":24707,\"start\":24445},{\"end\":25052,\"start\":24709},{\"end\":25218,\"start\":25095},{\"end\":25679,\"start\":25262},{\"end\":26185,\"start\":25681},{\"end\":26730,\"start\":26187},{\"end\":27019,\"start\":26791},{\"end\":27166,\"start\":27090},{\"end\":27388,\"start\":27193},{\"end\":27820,\"start\":27390},{\"end\":28458,\"start\":27833},{\"end\":28525,\"start\":28481},{\"end\":29157,\"start\":28527},{\"end\":29434,\"start\":29182},{\"end\":29951,\"start\":29436},{\"end\":30446,\"start\":29953},{\"end\":30666,\"start\":30470},{\"end\":31387,\"start\":30696},{\"end\":31951,\"start\":31389},{\"end\":32891,\"start\":31970},{\"end\":33500,\"start\":32893},{\"end\":34183,\"start\":33502},{\"end\":35359,\"start\":34185},{\"end\":35948,\"start\":35383},{\"end\":36231,\"start\":36013},{\"end\":36674,\"start\":36284},{\"end\":36869,\"start\":36690},{\"end\":37399,\"start\":36904},{\"end\":40074,\"start\":37401},{\"end\":40578,\"start\":40076},{\"end\":41762,\"start\":40580},{\"end\":43167,\"start\":41764},{\"end\":43716,\"start\":43169},{\"end\":44805,\"start\":43718},{\"end\":45564,\"start\":44807},{\"end\":45872,\"start\":45566},{\"end\":46558,\"start\":45874},{\"end\":47627,\"start\":46560},{\"end\":48941,\"start\":47629},{\"end\":51425,\"start\":48943},{\"end\":51847,\"start\":51468},{\"end\":53007,\"start\":51849},{\"end\":54206,\"start\":53009},{\"end\":54854,\"start\":54208},{\"end\":55239,\"start\":54856},{\"end\":56188,\"start\":55241},{\"end\":56635,\"start\":56190},{\"end\":57296,\"start\":56685},{\"end\":58153,\"start\":57298},{\"end\":58500,\"start\":58168},{\"end\":59439,\"start\":58502},{\"end\":59929,\"start\":59441},{\"end\":60891,\"start\":59931}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4888,\"start\":4879},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16582,\"start\":16540},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16766,\"start\":16728},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17324,\"start\":17293},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18098,\"start\":18034},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19228,\"start\":19192},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19864,\"start\":19809},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20320,\"start\":20274},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20799,\"start\":20756},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22014,\"start\":21964},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22737,\"start\":22693},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23470,\"start\":23435},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23810,\"start\":23771},{\"attributes\":{\"id\":\"formula_15\"},\"end\":24014,\"start\":23971},{\"attributes\":{\"id\":\"formula_16\"},\"end\":24444,\"start\":24407},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25094,\"start\":25053},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26790,\"start\":26731},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27089,\"start\":27020},{\"attributes\":{\"id\":\"formula_20\"},\"end\":27192,\"start\":27167},{\"attributes\":{\"id\":\"formula_21\"},\"end\":36012,\"start\":35949},{\"attributes\":{\"id\":\"formula_22\"},\"end\":36283,\"start\":36232}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28335,\"start\":28328},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33263,\"start\":33256},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37576,\"start\":37569},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":39017,\"start\":39010},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39047,\"start\":39040},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":40577,\"start\":40570},{\"end\":45129,\"start\":45128},{\"end\":46468,\"start\":46462},{\"end\":48129,\"start\":48122},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":51868,\"start\":51861},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":55421,\"start\":55414},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":55904,\"start\":55897}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1607,\"start\":1595},{\"attributes\":{\"n\":\"2\"},\"end\":7148,\"start\":7136},{\"attributes\":{\"n\":\"3\"},\"end\":16226,\"start\":16209},{\"attributes\":{\"n\":\"3.1\"},\"end\":17084,\"start\":17055},{\"attributes\":{\"n\":\"3.2\"},\"end\":20058,\"start\":20026},{\"attributes\":{\"n\":\"3.3\"},\"end\":21478,\"start\":21435},{\"attributes\":{\"n\":\"3.4\"},\"end\":23055,\"start\":23042},{\"attributes\":{\"n\":\"3.5\"},\"end\":25260,\"start\":25221},{\"attributes\":{\"n\":\"4\"},\"end\":27831,\"start\":27823},{\"attributes\":{\"n\":\"4.1\"},\"end\":28479,\"start\":28461},{\"attributes\":{\"n\":\"4.2\"},\"end\":29180,\"start\":29160},{\"attributes\":{\"n\":\"5\"},\"end\":30468,\"start\":30449},{\"attributes\":{\"n\":\"5.1\"},\"end\":30694,\"start\":30669},{\"attributes\":{\"n\":\"5.2\"},\"end\":31968,\"start\":31954},{\"attributes\":{\"n\":\"5.3\"},\"end\":35381,\"start\":35362},{\"attributes\":{\"n\":\"6\"},\"end\":36688,\"start\":36677},{\"attributes\":{\"n\":\"6.1\"},\"end\":36902,\"start\":36872},{\"attributes\":{\"n\":\"6.2\"},\"end\":51466,\"start\":51428},{\"attributes\":{\"n\":\"6.3\"},\"end\":56683,\"start\":56638},{\"attributes\":{\"n\":\"7\"},\"end\":58166,\"start\":58156},{\"end\":60901,\"start\":60893},{\"end\":61143,\"start\":61135},{\"end\":61436,\"start\":61428},{\"end\":61779,\"start\":61771},{\"end\":62037,\"start\":62029},{\"end\":68728,\"start\":68719},{\"end\":69345,\"start\":69336},{\"end\":69843,\"start\":69834},{\"end\":70519,\"start\":70510},{\"end\":70868,\"start\":70863},{\"end\":72084,\"start\":72079},{\"end\":72096,\"start\":72087}]", "table": "[{\"end\":68717,\"start\":68071},{\"end\":69334,\"start\":68805},{\"end\":70861,\"start\":70521},{\"end\":72077,\"start\":70869},{\"end\":72309,\"start\":72098}]", "figure_caption": "[{\"end\":61133,\"start\":60903},{\"end\":61426,\"start\":61145},{\"end\":61769,\"start\":61438},{\"end\":62027,\"start\":61781},{\"end\":62167,\"start\":62039},{\"end\":64870,\"start\":62170},{\"end\":65158,\"start\":64873},{\"end\":67845,\"start\":65161},{\"end\":68071,\"start\":67848},{\"end\":68748,\"start\":68730},{\"end\":68805,\"start\":68751},{\"end\":69832,\"start\":69347},{\"end\":70012,\"start\":69845},{\"end\":70508,\"start\":70015}]", "figure_ref": "[{\"end\":2563,\"start\":2555},{\"end\":5535,\"start\":5529},{\"end\":28524,\"start\":28516},{\"end\":28538,\"start\":28530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42037,\"start\":42028},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42069,\"start\":42051},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":43532,\"start\":43524},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":50665,\"start\":50657},{\"end\":53172,\"start\":53164},{\"end\":54537,\"start\":54531},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":55287,\"start\":55279},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":55445,\"start\":55437},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":55640,\"start\":55632},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":56225,\"start\":56217},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":57357,\"start\":57349}]", "bib_author_first_name": "[{\"end\":72387,\"start\":72386},{\"end\":72398,\"start\":72397},{\"end\":72410,\"start\":72409},{\"end\":72641,\"start\":72640},{\"end\":72655,\"start\":72654},{\"end\":72664,\"start\":72663},{\"end\":72852,\"start\":72851},{\"end\":72864,\"start\":72863},{\"end\":72873,\"start\":72872},{\"end\":73129,\"start\":73128},{\"end\":73135,\"start\":73134},{\"end\":73372,\"start\":73371},{\"end\":73378,\"start\":73377},{\"end\":73386,\"start\":73385},{\"end\":73395,\"start\":73394},{\"end\":73397,\"start\":73396},{\"end\":73405,\"start\":73404},{\"end\":73641,\"start\":73640},{\"end\":73649,\"start\":73648},{\"end\":73657,\"start\":73656},{\"end\":73837,\"start\":73836},{\"end\":73846,\"start\":73845},{\"end\":73848,\"start\":73847},{\"end\":73859,\"start\":73858},{\"end\":73869,\"start\":73868},{\"end\":73879,\"start\":73878},{\"end\":73887,\"start\":73886},{\"end\":73889,\"start\":73888},{\"end\":73900,\"start\":73899},{\"end\":74152,\"start\":74151},{\"end\":74161,\"start\":74160},{\"end\":74174,\"start\":74173},{\"end\":74187,\"start\":74186},{\"end\":74403,\"start\":74402},{\"end\":74412,\"start\":74411},{\"end\":74420,\"start\":74419},{\"end\":74430,\"start\":74429},{\"end\":74437,\"start\":74436},{\"end\":74693,\"start\":74692},{\"end\":74711,\"start\":74710},{\"end\":74713,\"start\":74712},{\"end\":74917,\"start\":74916},{\"end\":74925,\"start\":74924},{\"end\":74934,\"start\":74933},{\"end\":74944,\"start\":74943},{\"end\":74954,\"start\":74953},{\"end\":74962,\"start\":74961},{\"end\":75196,\"start\":75195},{\"end\":75206,\"start\":75205},{\"end\":75216,\"start\":75215},{\"end\":75218,\"start\":75217},{\"end\":75229,\"start\":75228},{\"end\":75446,\"start\":75445},{\"end\":75455,\"start\":75454},{\"end\":75654,\"start\":75653},{\"end\":75671,\"start\":75667},{\"end\":75679,\"start\":75678},{\"end\":75687,\"start\":75686},{\"end\":75918,\"start\":75917},{\"end\":75929,\"start\":75928},{\"end\":75940,\"start\":75939},{\"end\":75950,\"start\":75949},{\"end\":75960,\"start\":75959},{\"end\":75970,\"start\":75969},{\"end\":75979,\"start\":75978},{\"end\":75990,\"start\":75989},{\"end\":76292,\"start\":76291},{\"end\":76305,\"start\":76304},{\"end\":76481,\"start\":76480},{\"end\":76493,\"start\":76492},{\"end\":76504,\"start\":76503},{\"end\":76512,\"start\":76511},{\"end\":76519,\"start\":76518},{\"end\":76530,\"start\":76529},{\"end\":76542,\"start\":76541},{\"end\":76782,\"start\":76781},{\"end\":76793,\"start\":76792},{\"end\":76803,\"start\":76802},{\"end\":76812,\"start\":76811},{\"end\":77002,\"start\":77001},{\"end\":77010,\"start\":77009},{\"end\":77018,\"start\":77017},{\"end\":77031,\"start\":77027},{\"end\":77037,\"start\":77036},{\"end\":77043,\"start\":77042},{\"end\":77262,\"start\":77261},{\"end\":77264,\"start\":77263},{\"end\":77476,\"start\":77475},{\"end\":77487,\"start\":77486},{\"end\":77494,\"start\":77493},{\"end\":77505,\"start\":77504},{\"end\":77516,\"start\":77515},{\"end\":77525,\"start\":77524},{\"end\":77534,\"start\":77533},{\"end\":77798,\"start\":77797},{\"end\":77810,\"start\":77809},{\"end\":78039,\"start\":78038},{\"end\":78045,\"start\":78044},{\"end\":78054,\"start\":78053},{\"end\":78061,\"start\":78060},{\"end\":78288,\"start\":78287},{\"end\":78300,\"start\":78299},{\"end\":78311,\"start\":78310},{\"end\":78556,\"start\":78555},{\"end\":78569,\"start\":78568},{\"end\":78741,\"start\":78740},{\"end\":78754,\"start\":78753},{\"end\":78766,\"start\":78765},{\"end\":78781,\"start\":78780},{\"end\":79014,\"start\":79013},{\"end\":79025,\"start\":79024},{\"end\":79038,\"start\":79037},{\"end\":79046,\"start\":79045},{\"end\":79048,\"start\":79047},{\"end\":79059,\"start\":79058},{\"end\":79287,\"start\":79286},{\"end\":79293,\"start\":79292},{\"end\":79295,\"start\":79294},{\"end\":79309,\"start\":79308},{\"end\":79318,\"start\":79317},{\"end\":79496,\"start\":79495},{\"end\":79509,\"start\":79508},{\"end\":79522,\"start\":79521},{\"end\":79524,\"start\":79523},{\"end\":79534,\"start\":79533},{\"end\":79536,\"start\":79535},{\"end\":79726,\"start\":79725},{\"end\":79735,\"start\":79734},{\"end\":79748,\"start\":79747},{\"end\":79761,\"start\":79760},{\"end\":80011,\"start\":80010},{\"end\":80019,\"start\":80018},{\"end\":80026,\"start\":80025},{\"end\":80034,\"start\":80033},{\"end\":80312,\"start\":80311},{\"end\":80322,\"start\":80321},{\"end\":80332,\"start\":80331},{\"end\":80539,\"start\":80538},{\"end\":80550,\"start\":80549},{\"end\":80559,\"start\":80558},{\"end\":80788,\"start\":80787},{\"end\":80794,\"start\":80793},{\"end\":80805,\"start\":80804},{\"end\":81027,\"start\":81026},{\"end\":81036,\"start\":81035},{\"end\":81045,\"start\":81044},{\"end\":81238,\"start\":81237},{\"end\":81255,\"start\":81251},{\"end\":81263,\"start\":81262},{\"end\":81513,\"start\":81512},{\"end\":81522,\"start\":81521},{\"end\":81708,\"start\":81707},{\"end\":81714,\"start\":81713},{\"end\":81723,\"start\":81722},{\"end\":81734,\"start\":81733},{\"end\":81929,\"start\":81928},{\"end\":81938,\"start\":81937},{\"end\":81952,\"start\":81951},{\"end\":81961,\"start\":81960},{\"end\":82201,\"start\":82200},{\"end\":82209,\"start\":82208},{\"end\":82216,\"start\":82215},{\"end\":82224,\"start\":82223},{\"end\":82232,\"start\":82231},{\"end\":82240,\"start\":82239},{\"end\":82489,\"start\":82488},{\"end\":82491,\"start\":82490},{\"end\":82499,\"start\":82498},{\"end\":82704,\"start\":82703},{\"end\":82710,\"start\":82709},{\"end\":82934,\"start\":82933},{\"end\":82947,\"start\":82946},{\"end\":83131,\"start\":83130},{\"end\":83143,\"start\":83142},{\"end\":83152,\"start\":83151},{\"end\":83330,\"start\":83329},{\"end\":83341,\"start\":83340},{\"end\":83350,\"start\":83349},{\"end\":83356,\"start\":83355},{\"end\":83564,\"start\":83563},{\"end\":83570,\"start\":83569},{\"end\":83577,\"start\":83576},{\"end\":83792,\"start\":83791},{\"end\":83798,\"start\":83797},{\"end\":83963,\"start\":83962},{\"end\":83969,\"start\":83968},{\"end\":84119,\"start\":84118},{\"end\":84128,\"start\":84127},{\"end\":84307,\"start\":84306},{\"end\":84318,\"start\":84317},{\"end\":84328,\"start\":84327},{\"end\":84330,\"start\":84329},{\"end\":84538,\"start\":84537},{\"end\":84550,\"start\":84549},{\"end\":84559,\"start\":84558},{\"end\":84570,\"start\":84569},{\"end\":84582,\"start\":84581},{\"end\":84822,\"start\":84821},{\"end\":84830,\"start\":84829},{\"end\":84839,\"start\":84838},{\"end\":84846,\"start\":84845},{\"end\":85067,\"start\":85066},{\"end\":85080,\"start\":85079},{\"end\":85089,\"start\":85088},{\"end\":85285,\"start\":85284},{\"end\":85294,\"start\":85293},{\"end\":85296,\"start\":85295},{\"end\":85307,\"start\":85306},{\"end\":85626,\"start\":85625},{\"end\":85637,\"start\":85636},{\"end\":85648,\"start\":85647},{\"end\":85661,\"start\":85660},{\"end\":85950,\"start\":85949},{\"end\":85964,\"start\":85963},{\"end\":85974,\"start\":85973},{\"end\":85976,\"start\":85975},{\"end\":86149,\"start\":86148},{\"end\":86151,\"start\":86150},{\"end\":86419,\"start\":86418},{\"end\":86431,\"start\":86430},{\"end\":86440,\"start\":86439},{\"end\":86451,\"start\":86450},{\"end\":86715,\"start\":86714},{\"end\":86717,\"start\":86716},{\"end\":86729,\"start\":86728},{\"end\":86738,\"start\":86737},{\"end\":86749,\"start\":86748},{\"end\":86751,\"start\":86750},{\"end\":86947,\"start\":86946},{\"end\":86955,\"start\":86954},{\"end\":86967,\"start\":86966},{\"end\":87173,\"start\":87172},{\"end\":87182,\"start\":87181},{\"end\":87191,\"start\":87190},{\"end\":87199,\"start\":87198},{\"end\":87205,\"start\":87204},{\"end\":87216,\"start\":87212},{\"end\":87413,\"start\":87412},{\"end\":87424,\"start\":87423},{\"end\":87426,\"start\":87425},{\"end\":87626,\"start\":87622},{\"end\":87634,\"start\":87633},{\"end\":87648,\"start\":87647},{\"end\":87656,\"start\":87655},{\"end\":87890,\"start\":87889},{\"end\":88053,\"start\":88052},{\"end\":88064,\"start\":88063},{\"end\":88075,\"start\":88074},{\"end\":88268,\"start\":88267},{\"end\":88278,\"start\":88277},{\"end\":88288,\"start\":88287},{\"end\":88495,\"start\":88494},{\"end\":88513,\"start\":88512},{\"end\":88525,\"start\":88524},{\"end\":88536,\"start\":88535},{\"end\":88747,\"start\":88746},{\"end\":88749,\"start\":88748},{\"end\":88760,\"start\":88759},{\"end\":88944,\"start\":88943},{\"end\":88956,\"start\":88955},{\"end\":88969,\"start\":88968},{\"end\":89175,\"start\":89174},{\"end\":89183,\"start\":89182},{\"end\":89195,\"start\":89194},{\"end\":89197,\"start\":89196},{\"end\":89204,\"start\":89203},{\"end\":89214,\"start\":89213},{\"end\":89438,\"start\":89437},{\"end\":89444,\"start\":89443},{\"end\":89616,\"start\":89615},{\"end\":89628,\"start\":89627},{\"end\":89748,\"start\":89747},{\"end\":89759,\"start\":89758},{\"end\":89766,\"start\":89765},{\"end\":89773,\"start\":89772},{\"end\":89785,\"start\":89784},{\"end\":89793,\"start\":89792},{\"end\":89805,\"start\":89804},{\"end\":89814,\"start\":89813},{\"end\":89827,\"start\":89826},{\"end\":90073,\"start\":90072},{\"end\":90081,\"start\":90080},{\"end\":90094,\"start\":90093},{\"end\":90102,\"start\":90101},{\"end\":90114,\"start\":90113},{\"end\":90308,\"start\":90307},{\"end\":90319,\"start\":90318},{\"end\":90326,\"start\":90325},{\"end\":90333,\"start\":90332},{\"end\":90345,\"start\":90344},{\"end\":90353,\"start\":90352},{\"end\":90365,\"start\":90364},{\"end\":90374,\"start\":90373},{\"end\":90387,\"start\":90386},{\"end\":90672,\"start\":90671},{\"end\":90682,\"start\":90681}]", "bib_author_last_name": "[{\"end\":72395,\"start\":72388},{\"end\":72407,\"start\":72399},{\"end\":72420,\"start\":72411},{\"end\":72652,\"start\":72642},{\"end\":72661,\"start\":72656},{\"end\":72671,\"start\":72665},{\"end\":72861,\"start\":72853},{\"end\":72870,\"start\":72865},{\"end\":72881,\"start\":72874},{\"end\":73132,\"start\":73130},{\"end\":73145,\"start\":73136},{\"end\":73375,\"start\":73373},{\"end\":73383,\"start\":73379},{\"end\":73392,\"start\":73387},{\"end\":73402,\"start\":73398},{\"end\":73410,\"start\":73406},{\"end\":73646,\"start\":73642},{\"end\":73654,\"start\":73650},{\"end\":73660,\"start\":73658},{\"end\":73843,\"start\":73838},{\"end\":73856,\"start\":73849},{\"end\":73866,\"start\":73860},{\"end\":73876,\"start\":73870},{\"end\":73884,\"start\":73880},{\"end\":73897,\"start\":73890},{\"end\":73908,\"start\":73901},{\"end\":74158,\"start\":74153},{\"end\":74171,\"start\":74162},{\"end\":74184,\"start\":74175},{\"end\":74194,\"start\":74188},{\"end\":74409,\"start\":74404},{\"end\":74417,\"start\":74413},{\"end\":74427,\"start\":74421},{\"end\":74434,\"start\":74431},{\"end\":74445,\"start\":74438},{\"end\":74708,\"start\":74694},{\"end\":74718,\"start\":74714},{\"end\":74922,\"start\":74918},{\"end\":74931,\"start\":74926},{\"end\":74941,\"start\":74935},{\"end\":74951,\"start\":74945},{\"end\":74959,\"start\":74955},{\"end\":74970,\"start\":74963},{\"end\":75203,\"start\":75197},{\"end\":75213,\"start\":75207},{\"end\":75226,\"start\":75219},{\"end\":75232,\"start\":75230},{\"end\":75452,\"start\":75447},{\"end\":75465,\"start\":75456},{\"end\":75665,\"start\":75655},{\"end\":75676,\"start\":75672},{\"end\":75684,\"start\":75680},{\"end\":75691,\"start\":75688},{\"end\":75926,\"start\":75919},{\"end\":75937,\"start\":75930},{\"end\":75947,\"start\":75941},{\"end\":75957,\"start\":75951},{\"end\":75967,\"start\":75961},{\"end\":75976,\"start\":75971},{\"end\":75987,\"start\":75980},{\"end\":75995,\"start\":75991},{\"end\":76302,\"start\":76293},{\"end\":76310,\"start\":76306},{\"end\":76490,\"start\":76482},{\"end\":76501,\"start\":76494},{\"end\":76509,\"start\":76505},{\"end\":76516,\"start\":76513},{\"end\":76527,\"start\":76520},{\"end\":76539,\"start\":76531},{\"end\":76549,\"start\":76543},{\"end\":76790,\"start\":76783},{\"end\":76800,\"start\":76794},{\"end\":76809,\"start\":76804},{\"end\":76820,\"start\":76813},{\"end\":77007,\"start\":77003},{\"end\":77015,\"start\":77011},{\"end\":77025,\"start\":77019},{\"end\":77034,\"start\":77032},{\"end\":77040,\"start\":77038},{\"end\":77051,\"start\":77044},{\"end\":77269,\"start\":77265},{\"end\":77484,\"start\":77477},{\"end\":77491,\"start\":77488},{\"end\":77502,\"start\":77495},{\"end\":77513,\"start\":77506},{\"end\":77522,\"start\":77517},{\"end\":77531,\"start\":77526},{\"end\":77542,\"start\":77535},{\"end\":77807,\"start\":77799},{\"end\":77820,\"start\":77811},{\"end\":78042,\"start\":78040},{\"end\":78051,\"start\":78046},{\"end\":78058,\"start\":78055},{\"end\":78065,\"start\":78062},{\"end\":78297,\"start\":78289},{\"end\":78308,\"start\":78301},{\"end\":78324,\"start\":78312},{\"end\":78566,\"start\":78557},{\"end\":78577,\"start\":78570},{\"end\":78751,\"start\":78742},{\"end\":78763,\"start\":78755},{\"end\":78778,\"start\":78767},{\"end\":78790,\"start\":78782},{\"end\":79022,\"start\":79015},{\"end\":79035,\"start\":79026},{\"end\":79043,\"start\":79039},{\"end\":79056,\"start\":79049},{\"end\":79064,\"start\":79060},{\"end\":79290,\"start\":79288},{\"end\":79306,\"start\":79296},{\"end\":79315,\"start\":79310},{\"end\":79323,\"start\":79319},{\"end\":79506,\"start\":79497},{\"end\":79519,\"start\":79510},{\"end\":79531,\"start\":79525},{\"end\":79545,\"start\":79537},{\"end\":79732,\"start\":79727},{\"end\":79745,\"start\":79736},{\"end\":79758,\"start\":79749},{\"end\":79768,\"start\":79762},{\"end\":80016,\"start\":80012},{\"end\":80023,\"start\":80020},{\"end\":80031,\"start\":80027},{\"end\":80042,\"start\":80035},{\"end\":80050,\"start\":80044},{\"end\":80319,\"start\":80313},{\"end\":80329,\"start\":80323},{\"end\":80338,\"start\":80333},{\"end\":80547,\"start\":80540},{\"end\":80556,\"start\":80551},{\"end\":80564,\"start\":80560},{\"end\":80791,\"start\":80789},{\"end\":80802,\"start\":80795},{\"end\":80812,\"start\":80806},{\"end\":81033,\"start\":81028},{\"end\":81042,\"start\":81037},{\"end\":81050,\"start\":81046},{\"end\":81249,\"start\":81239},{\"end\":81260,\"start\":81256},{\"end\":81267,\"start\":81264},{\"end\":81519,\"start\":81514},{\"end\":81532,\"start\":81523},{\"end\":81711,\"start\":81709},{\"end\":81720,\"start\":81715},{\"end\":81731,\"start\":81724},{\"end\":81739,\"start\":81735},{\"end\":81935,\"start\":81930},{\"end\":81949,\"start\":81939},{\"end\":81958,\"start\":81953},{\"end\":81969,\"start\":81962},{\"end\":82206,\"start\":82202},{\"end\":82213,\"start\":82210},{\"end\":82221,\"start\":82217},{\"end\":82229,\"start\":82225},{\"end\":82237,\"start\":82233},{\"end\":82244,\"start\":82241},{\"end\":82496,\"start\":82492},{\"end\":82503,\"start\":82500},{\"end\":82707,\"start\":82705},{\"end\":82715,\"start\":82711},{\"end\":82944,\"start\":82935},{\"end\":82958,\"start\":82948},{\"end\":83140,\"start\":83132},{\"end\":83149,\"start\":83144},{\"end\":83160,\"start\":83153},{\"end\":83338,\"start\":83331},{\"end\":83347,\"start\":83342},{\"end\":83353,\"start\":83351},{\"end\":83361,\"start\":83357},{\"end\":83567,\"start\":83565},{\"end\":83574,\"start\":83571},{\"end\":83588,\"start\":83578},{\"end\":83795,\"start\":83793},{\"end\":83802,\"start\":83799},{\"end\":83966,\"start\":83964},{\"end\":83975,\"start\":83970},{\"end\":84125,\"start\":84120},{\"end\":84138,\"start\":84129},{\"end\":84315,\"start\":84308},{\"end\":84325,\"start\":84319},{\"end\":84336,\"start\":84331},{\"end\":84547,\"start\":84539},{\"end\":84556,\"start\":84551},{\"end\":84567,\"start\":84560},{\"end\":84579,\"start\":84571},{\"end\":84590,\"start\":84583},{\"end\":84827,\"start\":84823},{\"end\":84836,\"start\":84831},{\"end\":84843,\"start\":84840},{\"end\":84854,\"start\":84847},{\"end\":85077,\"start\":85068},{\"end\":85086,\"start\":85081},{\"end\":85098,\"start\":85090},{\"end\":85291,\"start\":85286},{\"end\":85304,\"start\":85297},{\"end\":85314,\"start\":85308},{\"end\":85634,\"start\":85627},{\"end\":85645,\"start\":85638},{\"end\":85658,\"start\":85649},{\"end\":85668,\"start\":85662},{\"end\":85961,\"start\":85951},{\"end\":85971,\"start\":85965},{\"end\":85984,\"start\":85977},{\"end\":86158,\"start\":86152},{\"end\":86428,\"start\":86420},{\"end\":86437,\"start\":86432},{\"end\":86448,\"start\":86441},{\"end\":86459,\"start\":86452},{\"end\":86726,\"start\":86718},{\"end\":86735,\"start\":86730},{\"end\":86746,\"start\":86739},{\"end\":86757,\"start\":86752},{\"end\":86952,\"start\":86948},{\"end\":86964,\"start\":86956},{\"end\":86973,\"start\":86968},{\"end\":87179,\"start\":87174},{\"end\":87188,\"start\":87183},{\"end\":87196,\"start\":87192},{\"end\":87202,\"start\":87200},{\"end\":87210,\"start\":87206},{\"end\":87221,\"start\":87217},{\"end\":87421,\"start\":87414},{\"end\":87432,\"start\":87427},{\"end\":87631,\"start\":87627},{\"end\":87645,\"start\":87635},{\"end\":87653,\"start\":87649},{\"end\":87660,\"start\":87657},{\"end\":87899,\"start\":87891},{\"end\":88061,\"start\":88054},{\"end\":88072,\"start\":88065},{\"end\":88085,\"start\":88076},{\"end\":88275,\"start\":88269},{\"end\":88285,\"start\":88279},{\"end\":88296,\"start\":88289},{\"end\":88510,\"start\":88496},{\"end\":88522,\"start\":88514},{\"end\":88533,\"start\":88526},{\"end\":88542,\"start\":88537},{\"end\":88757,\"start\":88750},{\"end\":88768,\"start\":88761},{\"end\":88953,\"start\":88945},{\"end\":88966,\"start\":88957},{\"end\":88974,\"start\":88970},{\"end\":89180,\"start\":89176},{\"end\":89192,\"start\":89184},{\"end\":89201,\"start\":89198},{\"end\":89211,\"start\":89205},{\"end\":89224,\"start\":89215},{\"end\":89441,\"start\":89439},{\"end\":89448,\"start\":89445},{\"end\":89625,\"start\":89617},{\"end\":89633,\"start\":89629},{\"end\":89756,\"start\":89749},{\"end\":89763,\"start\":89760},{\"end\":89770,\"start\":89767},{\"end\":89782,\"start\":89774},{\"end\":89790,\"start\":89786},{\"end\":89802,\"start\":89794},{\"end\":89811,\"start\":89806},{\"end\":89824,\"start\":89815},{\"end\":89838,\"start\":89828},{\"end\":90078,\"start\":90074},{\"end\":90091,\"start\":90082},{\"end\":90099,\"start\":90095},{\"end\":90111,\"start\":90103},{\"end\":90120,\"start\":90115},{\"end\":90316,\"start\":90309},{\"end\":90323,\"start\":90320},{\"end\":90330,\"start\":90327},{\"end\":90342,\"start\":90334},{\"end\":90350,\"start\":90346},{\"end\":90362,\"start\":90354},{\"end\":90371,\"start\":90366},{\"end\":90384,\"start\":90375},{\"end\":90398,\"start\":90388},{\"end\":90679,\"start\":90673},{\"end\":90690,\"start\":90683}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7016601},\"end\":72605,\"start\":72311},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7249642},\"end\":72770,\"start\":72607},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14700310},\"end\":73031,\"start\":72772},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":36135379},\"end\":73291,\"start\":73033},{\"attributes\":{\"id\":\"b4\"},\"end\":73567,\"start\":73293},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2142702},\"end\":73786,\"start\":73569},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":261138},\"end\":74097,\"start\":73788},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8288863},\"end\":74329,\"start\":74099},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11617315},\"end\":74633,\"start\":74331},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5891792},\"end\":74866,\"start\":74635},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2004024},\"end\":75144,\"start\":74868},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2808203},\"end\":75389,\"start\":75146},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":446581},\"end\":75603,\"start\":75391},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1580181},\"end\":75850,\"start\":75605},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1926319},\"end\":76206,\"start\":75852},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8724974},\"end\":76454,\"start\":76208},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7138640},\"end\":76739,\"start\":76456},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14940757},\"end\":76946,\"start\":76741},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":57246310},\"end\":77200,\"start\":76948},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":174065},\"end\":77394,\"start\":77202},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6161478},\"end\":77727,\"start\":77396},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b21\"},\"end\":77990,\"start\":77729},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206594692},\"end\":78179,\"start\":77992},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10169241},\"end\":78503,\"start\":78181},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":166607},\"end\":78683,\"start\":78505},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8220073},\"end\":78934,\"start\":78685},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16447573},\"end\":79241,\"start\":78936},{\"attributes\":{\"id\":\"b27\"},\"end\":79446,\"start\":79243},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7490338},\"end\":79681,\"start\":79448},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":523388},\"end\":79922,\"start\":79683},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13897053},\"end\":80218,\"start\":79924},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2120774},\"end\":80491,\"start\":80220},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2633144},\"end\":80701,\"start\":80493},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14149995},\"end\":80968,\"start\":80703},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206595438},\"end\":81165,\"start\":80970},{\"attributes\":{\"doi\":\"arXiv:1605.08151\",\"id\":\"b35\"},\"end\":81450,\"start\":81167},{\"attributes\":{\"id\":\"b36\"},\"end\":81645,\"start\":81452},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":217935609},\"end\":81872,\"start\":81647},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11621384},\"end\":82101,\"start\":81874},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":19798145},\"end\":82424,\"start\":82103},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":19288109},\"end\":82642,\"start\":82426},{\"attributes\":{\"doi\":\"arXiv:1705.09474\",\"id\":\"b41\"},\"end\":82867,\"start\":82644},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":9007950},\"end\":83083,\"start\":82869},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9584808},\"end\":83272,\"start\":83085},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1865113},\"end\":83484,\"start\":83274},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":15942776},\"end\":83727,\"start\":83486},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":9872454},\"end\":83914,\"start\":83729},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":13525770},\"end\":84067,\"start\":83916},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":34511677},\"end\":84242,\"start\":84069},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":9214350},\"end\":84464,\"start\":84244},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":8181220},\"end\":84752,\"start\":84466},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":7102424},\"end\":84990,\"start\":84754},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1923142},\"end\":85239,\"start\":84992},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":8505367},\"end\":85522,\"start\":85241},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":9296691},\"end\":85900,\"start\":85524},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":1957433},\"end\":86105,\"start\":85902},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/219717.219748\",\"id\":\"b56\",\"matched_paper_id\":1671874},\"end\":86364,\"start\":86107},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b57\",\"matched_paper_id\":16178753},\"end\":86682,\"start\":86366},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":12035411},\"end\":86879,\"start\":86684},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2284272},\"end\":87100,\"start\":86881},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":7652539},\"end\":87378,\"start\":87102},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":14240373},\"end\":87518,\"start\":87380},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":519822},\"end\":87836,\"start\":87520},{\"attributes\":{\"id\":\"b63\"},\"end\":87995,\"start\":87838},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":9359902},\"end\":88208,\"start\":87997},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":1337776},\"end\":88419,\"start\":88210},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":17671150},\"end\":88697,\"start\":88421},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":12957010},\"end\":88915,\"start\":88699},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":3869071},\"end\":89128,\"start\":88917},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":508435},\"end\":89356,\"start\":89130},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":24433423},\"end\":89584,\"start\":89358},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":18734348},\"end\":89713,\"start\":89586},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":206592484},\"end\":90002,\"start\":89715},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":1849990},\"end\":90273,\"start\":90004},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":206592484},\"end\":90560,\"start\":90275},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":17257979},\"end\":90873,\"start\":90562}]", "bib_title": "[{\"end\":72384,\"start\":72311},{\"end\":72638,\"start\":72607},{\"end\":72849,\"start\":72772},{\"end\":73126,\"start\":73033},{\"end\":73369,\"start\":73293},{\"end\":73638,\"start\":73569},{\"end\":73834,\"start\":73788},{\"end\":74149,\"start\":74099},{\"end\":74400,\"start\":74331},{\"end\":74690,\"start\":74635},{\"end\":74914,\"start\":74868},{\"end\":75193,\"start\":75146},{\"end\":75443,\"start\":75391},{\"end\":75651,\"start\":75605},{\"end\":75915,\"start\":75852},{\"end\":76289,\"start\":76208},{\"end\":76478,\"start\":76456},{\"end\":76779,\"start\":76741},{\"end\":76999,\"start\":76948},{\"end\":77259,\"start\":77202},{\"end\":77473,\"start\":77396},{\"end\":78036,\"start\":77992},{\"end\":78285,\"start\":78181},{\"end\":78553,\"start\":78505},{\"end\":78738,\"start\":78685},{\"end\":79011,\"start\":78936},{\"end\":79284,\"start\":79243},{\"end\":79493,\"start\":79448},{\"end\":79723,\"start\":79683},{\"end\":80008,\"start\":79924},{\"end\":80309,\"start\":80220},{\"end\":80536,\"start\":80493},{\"end\":80785,\"start\":80703},{\"end\":81024,\"start\":80970},{\"end\":81510,\"start\":81452},{\"end\":81705,\"start\":81647},{\"end\":81926,\"start\":81874},{\"end\":82198,\"start\":82103},{\"end\":82486,\"start\":82426},{\"end\":82931,\"start\":82869},{\"end\":83128,\"start\":83085},{\"end\":83327,\"start\":83274},{\"end\":83561,\"start\":83486},{\"end\":83789,\"start\":83729},{\"end\":83960,\"start\":83916},{\"end\":84116,\"start\":84069},{\"end\":84304,\"start\":84244},{\"end\":84535,\"start\":84466},{\"end\":84819,\"start\":84754},{\"end\":85064,\"start\":84992},{\"end\":85282,\"start\":85241},{\"end\":85623,\"start\":85524},{\"end\":85947,\"start\":85902},{\"end\":86146,\"start\":86107},{\"end\":86416,\"start\":86366},{\"end\":86712,\"start\":86684},{\"end\":86944,\"start\":86881},{\"end\":87170,\"start\":87102},{\"end\":87410,\"start\":87380},{\"end\":87620,\"start\":87520},{\"end\":88050,\"start\":87997},{\"end\":88265,\"start\":88210},{\"end\":88492,\"start\":88421},{\"end\":88744,\"start\":88699},{\"end\":88941,\"start\":88917},{\"end\":89172,\"start\":89130},{\"end\":89435,\"start\":89358},{\"end\":89613,\"start\":89586},{\"end\":89745,\"start\":89715},{\"end\":90070,\"start\":90004},{\"end\":90305,\"start\":90275},{\"end\":90669,\"start\":90562}]", "bib_author": "[{\"end\":72397,\"start\":72386},{\"end\":72409,\"start\":72397},{\"end\":72422,\"start\":72409},{\"end\":72654,\"start\":72640},{\"end\":72663,\"start\":72654},{\"end\":72673,\"start\":72663},{\"end\":72863,\"start\":72851},{\"end\":72872,\"start\":72863},{\"end\":72883,\"start\":72872},{\"end\":73134,\"start\":73128},{\"end\":73147,\"start\":73134},{\"end\":73377,\"start\":73371},{\"end\":73385,\"start\":73377},{\"end\":73394,\"start\":73385},{\"end\":73404,\"start\":73394},{\"end\":73412,\"start\":73404},{\"end\":73648,\"start\":73640},{\"end\":73656,\"start\":73648},{\"end\":73662,\"start\":73656},{\"end\":73845,\"start\":73836},{\"end\":73858,\"start\":73845},{\"end\":73868,\"start\":73858},{\"end\":73878,\"start\":73868},{\"end\":73886,\"start\":73878},{\"end\":73899,\"start\":73886},{\"end\":73910,\"start\":73899},{\"end\":74160,\"start\":74151},{\"end\":74173,\"start\":74160},{\"end\":74186,\"start\":74173},{\"end\":74196,\"start\":74186},{\"end\":74411,\"start\":74402},{\"end\":74419,\"start\":74411},{\"end\":74429,\"start\":74419},{\"end\":74436,\"start\":74429},{\"end\":74447,\"start\":74436},{\"end\":74710,\"start\":74692},{\"end\":74720,\"start\":74710},{\"end\":74924,\"start\":74916},{\"end\":74933,\"start\":74924},{\"end\":74943,\"start\":74933},{\"end\":74953,\"start\":74943},{\"end\":74961,\"start\":74953},{\"end\":74972,\"start\":74961},{\"end\":75205,\"start\":75195},{\"end\":75215,\"start\":75205},{\"end\":75228,\"start\":75215},{\"end\":75234,\"start\":75228},{\"end\":75454,\"start\":75445},{\"end\":75467,\"start\":75454},{\"end\":75667,\"start\":75653},{\"end\":75678,\"start\":75667},{\"end\":75686,\"start\":75678},{\"end\":75693,\"start\":75686},{\"end\":75928,\"start\":75917},{\"end\":75939,\"start\":75928},{\"end\":75949,\"start\":75939},{\"end\":75959,\"start\":75949},{\"end\":75969,\"start\":75959},{\"end\":75978,\"start\":75969},{\"end\":75989,\"start\":75978},{\"end\":75997,\"start\":75989},{\"end\":76304,\"start\":76291},{\"end\":76312,\"start\":76304},{\"end\":76492,\"start\":76480},{\"end\":76503,\"start\":76492},{\"end\":76511,\"start\":76503},{\"end\":76518,\"start\":76511},{\"end\":76529,\"start\":76518},{\"end\":76541,\"start\":76529},{\"end\":76551,\"start\":76541},{\"end\":76792,\"start\":76781},{\"end\":76802,\"start\":76792},{\"end\":76811,\"start\":76802},{\"end\":76822,\"start\":76811},{\"end\":77009,\"start\":77001},{\"end\":77017,\"start\":77009},{\"end\":77027,\"start\":77017},{\"end\":77036,\"start\":77027},{\"end\":77042,\"start\":77036},{\"end\":77053,\"start\":77042},{\"end\":77271,\"start\":77261},{\"end\":77486,\"start\":77475},{\"end\":77493,\"start\":77486},{\"end\":77504,\"start\":77493},{\"end\":77515,\"start\":77504},{\"end\":77524,\"start\":77515},{\"end\":77533,\"start\":77524},{\"end\":77544,\"start\":77533},{\"end\":77809,\"start\":77797},{\"end\":77822,\"start\":77809},{\"end\":78044,\"start\":78038},{\"end\":78053,\"start\":78044},{\"end\":78060,\"start\":78053},{\"end\":78067,\"start\":78060},{\"end\":78299,\"start\":78287},{\"end\":78310,\"start\":78299},{\"end\":78326,\"start\":78310},{\"end\":78568,\"start\":78555},{\"end\":78579,\"start\":78568},{\"end\":78753,\"start\":78740},{\"end\":78765,\"start\":78753},{\"end\":78780,\"start\":78765},{\"end\":78792,\"start\":78780},{\"end\":79024,\"start\":79013},{\"end\":79037,\"start\":79024},{\"end\":79045,\"start\":79037},{\"end\":79058,\"start\":79045},{\"end\":79066,\"start\":79058},{\"end\":79292,\"start\":79286},{\"end\":79308,\"start\":79292},{\"end\":79317,\"start\":79308},{\"end\":79325,\"start\":79317},{\"end\":79508,\"start\":79495},{\"end\":79521,\"start\":79508},{\"end\":79533,\"start\":79521},{\"end\":79547,\"start\":79533},{\"end\":79734,\"start\":79725},{\"end\":79747,\"start\":79734},{\"end\":79760,\"start\":79747},{\"end\":79770,\"start\":79760},{\"end\":80018,\"start\":80010},{\"end\":80025,\"start\":80018},{\"end\":80033,\"start\":80025},{\"end\":80044,\"start\":80033},{\"end\":80052,\"start\":80044},{\"end\":80321,\"start\":80311},{\"end\":80331,\"start\":80321},{\"end\":80340,\"start\":80331},{\"end\":80549,\"start\":80538},{\"end\":80558,\"start\":80549},{\"end\":80566,\"start\":80558},{\"end\":80793,\"start\":80787},{\"end\":80804,\"start\":80793},{\"end\":80814,\"start\":80804},{\"end\":81035,\"start\":81026},{\"end\":81044,\"start\":81035},{\"end\":81052,\"start\":81044},{\"end\":81251,\"start\":81237},{\"end\":81262,\"start\":81251},{\"end\":81269,\"start\":81262},{\"end\":81521,\"start\":81512},{\"end\":81534,\"start\":81521},{\"end\":81713,\"start\":81707},{\"end\":81722,\"start\":81713},{\"end\":81733,\"start\":81722},{\"end\":81741,\"start\":81733},{\"end\":81937,\"start\":81928},{\"end\":81951,\"start\":81937},{\"end\":81960,\"start\":81951},{\"end\":81971,\"start\":81960},{\"end\":82208,\"start\":82200},{\"end\":82215,\"start\":82208},{\"end\":82223,\"start\":82215},{\"end\":82231,\"start\":82223},{\"end\":82239,\"start\":82231},{\"end\":82246,\"start\":82239},{\"end\":82498,\"start\":82488},{\"end\":82505,\"start\":82498},{\"end\":82709,\"start\":82703},{\"end\":82717,\"start\":82709},{\"end\":82946,\"start\":82933},{\"end\":82960,\"start\":82946},{\"end\":83142,\"start\":83130},{\"end\":83151,\"start\":83142},{\"end\":83162,\"start\":83151},{\"end\":83340,\"start\":83329},{\"end\":83349,\"start\":83340},{\"end\":83355,\"start\":83349},{\"end\":83363,\"start\":83355},{\"end\":83569,\"start\":83563},{\"end\":83576,\"start\":83569},{\"end\":83590,\"start\":83576},{\"end\":83797,\"start\":83791},{\"end\":83804,\"start\":83797},{\"end\":83968,\"start\":83962},{\"end\":83977,\"start\":83968},{\"end\":84127,\"start\":84118},{\"end\":84140,\"start\":84127},{\"end\":84317,\"start\":84306},{\"end\":84327,\"start\":84317},{\"end\":84338,\"start\":84327},{\"end\":84549,\"start\":84537},{\"end\":84558,\"start\":84549},{\"end\":84569,\"start\":84558},{\"end\":84581,\"start\":84569},{\"end\":84592,\"start\":84581},{\"end\":84829,\"start\":84821},{\"end\":84838,\"start\":84829},{\"end\":84845,\"start\":84838},{\"end\":84856,\"start\":84845},{\"end\":85079,\"start\":85066},{\"end\":85088,\"start\":85079},{\"end\":85100,\"start\":85088},{\"end\":85293,\"start\":85284},{\"end\":85306,\"start\":85293},{\"end\":85316,\"start\":85306},{\"end\":85636,\"start\":85625},{\"end\":85647,\"start\":85636},{\"end\":85660,\"start\":85647},{\"end\":85670,\"start\":85660},{\"end\":85963,\"start\":85949},{\"end\":85973,\"start\":85963},{\"end\":85986,\"start\":85973},{\"end\":86160,\"start\":86148},{\"end\":86430,\"start\":86418},{\"end\":86439,\"start\":86430},{\"end\":86450,\"start\":86439},{\"end\":86461,\"start\":86450},{\"end\":86728,\"start\":86714},{\"end\":86737,\"start\":86728},{\"end\":86748,\"start\":86737},{\"end\":86759,\"start\":86748},{\"end\":86954,\"start\":86946},{\"end\":86966,\"start\":86954},{\"end\":86975,\"start\":86966},{\"end\":87181,\"start\":87172},{\"end\":87190,\"start\":87181},{\"end\":87198,\"start\":87190},{\"end\":87204,\"start\":87198},{\"end\":87212,\"start\":87204},{\"end\":87223,\"start\":87212},{\"end\":87423,\"start\":87412},{\"end\":87434,\"start\":87423},{\"end\":87633,\"start\":87622},{\"end\":87647,\"start\":87633},{\"end\":87655,\"start\":87647},{\"end\":87662,\"start\":87655},{\"end\":87901,\"start\":87889},{\"end\":88063,\"start\":88052},{\"end\":88074,\"start\":88063},{\"end\":88087,\"start\":88074},{\"end\":88277,\"start\":88267},{\"end\":88287,\"start\":88277},{\"end\":88298,\"start\":88287},{\"end\":88512,\"start\":88494},{\"end\":88524,\"start\":88512},{\"end\":88535,\"start\":88524},{\"end\":88544,\"start\":88535},{\"end\":88759,\"start\":88746},{\"end\":88770,\"start\":88759},{\"end\":88955,\"start\":88943},{\"end\":88968,\"start\":88955},{\"end\":88976,\"start\":88968},{\"end\":89182,\"start\":89174},{\"end\":89194,\"start\":89182},{\"end\":89203,\"start\":89194},{\"end\":89213,\"start\":89203},{\"end\":89226,\"start\":89213},{\"end\":89443,\"start\":89437},{\"end\":89450,\"start\":89443},{\"end\":89627,\"start\":89615},{\"end\":89635,\"start\":89627},{\"end\":89758,\"start\":89747},{\"end\":89765,\"start\":89758},{\"end\":89772,\"start\":89765},{\"end\":89784,\"start\":89772},{\"end\":89792,\"start\":89784},{\"end\":89804,\"start\":89792},{\"end\":89813,\"start\":89804},{\"end\":89826,\"start\":89813},{\"end\":89840,\"start\":89826},{\"end\":90080,\"start\":90072},{\"end\":90093,\"start\":90080},{\"end\":90101,\"start\":90093},{\"end\":90113,\"start\":90101},{\"end\":90122,\"start\":90113},{\"end\":90318,\"start\":90307},{\"end\":90325,\"start\":90318},{\"end\":90332,\"start\":90325},{\"end\":90344,\"start\":90332},{\"end\":90352,\"start\":90344},{\"end\":90364,\"start\":90352},{\"end\":90373,\"start\":90364},{\"end\":90386,\"start\":90373},{\"end\":90400,\"start\":90386},{\"end\":90681,\"start\":90671},{\"end\":90692,\"start\":90681}]", "bib_venue": "[{\"end\":72427,\"start\":72422},{\"end\":72677,\"start\":72673},{\"end\":72887,\"start\":72883},{\"end\":73151,\"start\":73147},{\"end\":73416,\"start\":73412},{\"end\":73666,\"start\":73662},{\"end\":73914,\"start\":73910},{\"end\":74200,\"start\":74196},{\"end\":74451,\"start\":74447},{\"end\":74724,\"start\":74720},{\"end\":74976,\"start\":74972},{\"end\":75238,\"start\":75234},{\"end\":75471,\"start\":75467},{\"end\":75697,\"start\":75693},{\"end\":76001,\"start\":75997},{\"end\":76316,\"start\":76312},{\"end\":76577,\"start\":76551},{\"end\":76826,\"start\":76822},{\"end\":77057,\"start\":77053},{\"end\":77275,\"start\":77271},{\"end\":77548,\"start\":77544},{\"end\":77795,\"start\":77729},{\"end\":78071,\"start\":78067},{\"end\":78330,\"start\":78326},{\"end\":78583,\"start\":78579},{\"end\":78796,\"start\":78792},{\"end\":79070,\"start\":79066},{\"end\":79330,\"start\":79325},{\"end\":79551,\"start\":79547},{\"end\":79775,\"start\":79770},{\"end\":80056,\"start\":80052},{\"end\":80344,\"start\":80340},{\"end\":80570,\"start\":80566},{\"end\":80818,\"start\":80814},{\"end\":81056,\"start\":81052},{\"end\":81235,\"start\":81167},{\"end\":81538,\"start\":81534},{\"end\":81745,\"start\":81741},{\"end\":81975,\"start\":81971},{\"end\":82250,\"start\":82246},{\"end\":82509,\"start\":82505},{\"end\":82701,\"start\":82644},{\"end\":82965,\"start\":82960},{\"end\":83166,\"start\":83162},{\"end\":83367,\"start\":83363},{\"end\":83594,\"start\":83590},{\"end\":83811,\"start\":83804},{\"end\":83981,\"start\":83977},{\"end\":84144,\"start\":84140},{\"end\":84342,\"start\":84338},{\"end\":84596,\"start\":84592},{\"end\":84860,\"start\":84856},{\"end\":85104,\"start\":85100},{\"end\":85354,\"start\":85316},{\"end\":85695,\"start\":85670},{\"end\":85991,\"start\":85986},{\"end\":86203,\"start\":86199},{\"end\":86512,\"start\":86468},{\"end\":86764,\"start\":86759},{\"end\":86979,\"start\":86975},{\"end\":87227,\"start\":87223},{\"end\":87438,\"start\":87434},{\"end\":87666,\"start\":87662},{\"end\":87887,\"start\":87838},{\"end\":88091,\"start\":88087},{\"end\":88303,\"start\":88298},{\"end\":88548,\"start\":88544},{\"end\":88781,\"start\":88770},{\"end\":89012,\"start\":88976},{\"end\":89230,\"start\":89226},{\"end\":89454,\"start\":89450},{\"end\":89639,\"start\":89635},{\"end\":89844,\"start\":89840},{\"end\":90126,\"start\":90122},{\"end\":90404,\"start\":90400},{\"end\":90696,\"start\":90692}]"}}}, "year": 2023, "month": 12, "day": 17}